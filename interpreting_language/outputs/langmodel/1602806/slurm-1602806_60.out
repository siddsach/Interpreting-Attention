Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'type': 'continuous', 'domain': [0, 30], 'name': 'lr'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'dropout'}, {'type': 'continuous', 'domain': [2, 8], 'name': 'anneal'}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 7.747554741775914, 'batch_size': 50, 'dropout': 0.8227747629004882, 'wordvec_source': 'glove', 'data': 'wikitext', 'lr': 4.8626202683209865, 'num_layers': 1, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.241873025894165 and batch: 50, loss is 7.592405176162719 and perplexity is 1983.0774329876951
At time: 3.867825508117676 and batch: 100, loss is 6.715592012405396 and perplexity is 825.1721344635838
At time: 5.447040557861328 and batch: 150, loss is 6.521908807754516 and perplexity is 679.8748980265025
At time: 7.036323547363281 and batch: 200, loss is 6.501244220733643 and perplexity is 665.9697311165695
At time: 8.617416381835938 and batch: 250, loss is 6.490060682296753 and perplexity is 658.5633251378443
At time: 10.198683500289917 and batch: 300, loss is 6.416111183166504 and perplexity is 611.6200052198274
At time: 11.780474185943604 and batch: 350, loss is 6.366901197433472 and perplexity is 582.2507503280211
At time: 13.362456560134888 and batch: 400, loss is 6.373136653900146 and perplexity is 585.8926923195885
At time: 14.94486403465271 and batch: 450, loss is 6.352926292419434 and perplexity is 574.170443662101
At time: 16.527995586395264 and batch: 500, loss is 6.339511451721191 and perplexity is 566.5194716495288
At time: 18.108492374420166 and batch: 550, loss is 6.275880689620972 and perplexity is 531.5943452849235
At time: 19.690593481063843 and batch: 600, loss is 6.291429386138916 and perplexity is 539.9245384266512
At time: 21.273183822631836 and batch: 650, loss is 6.296050434112549 and perplexity is 542.4253293080061
At time: 22.856135845184326 and batch: 700, loss is 6.250341806411743 and perplexity is 518.1899150367665
At time: 24.439252853393555 and batch: 750, loss is 6.2308092784881595 and perplexity is 508.1665654032683
At time: 26.041990995407104 and batch: 800, loss is 6.2388503456115725 and perplexity is 512.2692396991204
At time: 27.66121530532837 and batch: 850, loss is 6.238033094406128 and perplexity is 511.8507580710636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.551840464274089 and perplexity of 257.71142837628685
Finished 1 epochs...
Completing Train Step...
At time: 32.13518714904785 and batch: 50, loss is 5.7469868850708 and perplexity is 313.2453925319256
At time: 33.88430428504944 and batch: 100, loss is 5.487175951004028 and perplexity is 241.5740257586538
At time: 35.631564140319824 and batch: 150, loss is 5.376045551300049 and perplexity is 216.1657665962943
At time: 37.3794059753418 and batch: 200, loss is 5.335758323669434 and perplexity is 207.63013997330341
At time: 39.127678632736206 and batch: 250, loss is 5.3220228099822995 and perplexity is 204.79773017668916
At time: 40.875938177108765 and batch: 300, loss is 5.23405966758728 and perplexity is 187.55266155207246
At time: 42.62592530250549 and batch: 350, loss is 5.161768140792847 and perplexity is 174.47267529133086
At time: 44.38322639465332 and batch: 400, loss is 5.14977276802063 and perplexity is 172.39231281936256
At time: 46.13260078430176 and batch: 450, loss is 5.12551456451416 and perplexity is 168.26070036545008
At time: 47.881863594055176 and batch: 500, loss is 5.102619886398315 and perplexity is 164.45218946294133
At time: 49.67762494087219 and batch: 550, loss is 5.083095054626465 and perplexity is 161.27243126582817
At time: 51.42406868934631 and batch: 600, loss is 5.096628465652466 and perplexity is 163.46983299739222
At time: 53.17115616798401 and batch: 650, loss is 5.079085941314697 and perplexity is 160.6271661495808
At time: 54.92061996459961 and batch: 700, loss is 5.013768978118897 and perplexity is 150.47078987148328
At time: 56.667845010757446 and batch: 750, loss is 4.9866641330718995 and perplexity is 146.447079786555
At time: 58.41658091545105 and batch: 800, loss is 4.964423952102661 and perplexity is 143.22602145684596
At time: 60.16473841667175 and batch: 850, loss is 4.9538998031616215 and perplexity is 141.7265934162238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.6961164474487305 and perplexity of 109.5210148683363
Finished 2 epochs...
Completing Train Step...
At time: 64.63012146949768 and batch: 50, loss is 4.890490512847901 and perplexity is 133.01880548464945
At time: 66.3821337223053 and batch: 100, loss is 4.792833347320556 and perplexity is 120.64270756125109
At time: 68.13459849357605 and batch: 150, loss is 4.783911590576172 and perplexity is 119.5711498677337
At time: 69.88604545593262 and batch: 200, loss is 4.799058427810669 and perplexity is 121.3960605350117
At time: 71.63771748542786 and batch: 250, loss is 4.8043081569671635 and perplexity is 122.03503272145745
At time: 73.39141988754272 and batch: 300, loss is 4.765069770812988 and perplexity is 117.33930386083927
At time: 75.14476037025452 and batch: 350, loss is 4.713282165527343 and perplexity is 111.41725029247162
At time: 76.89891147613525 and batch: 400, loss is 4.730723609924317 and perplexity is 113.37757381053174
At time: 78.65123176574707 and batch: 450, loss is 4.730285520553589 and perplexity is 113.32791517882004
At time: 80.4041678905487 and batch: 500, loss is 4.719519348144531 and perplexity is 112.11435174386698
At time: 82.15712141990662 and batch: 550, loss is 4.725794305801392 and perplexity is 112.82007643439825
At time: 83.90979552268982 and batch: 600, loss is 4.748995809555054 and perplexity is 115.46827415108386
At time: 85.66225147247314 and batch: 650, loss is 4.732247438430786 and perplexity is 113.55047349077246
At time: 87.45321917533875 and batch: 700, loss is 4.687819089889526 and perplexity is 108.61603949186656
At time: 89.20403242111206 and batch: 750, loss is 4.6702393054962155 and perplexity is 106.72327884402866
At time: 90.953866481781 and batch: 800, loss is 4.6511414432525635 and perplexity is 104.70443156065785
At time: 92.70410132408142 and batch: 850, loss is 4.659714412689209 and perplexity is 105.60591813946918
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.555900891621907 and perplexity of 95.19247472074198
Finished 3 epochs...
Completing Train Step...
At time: 97.13052868843079 and batch: 50, loss is 4.625732822418213 and perplexity is 102.07755039743441
At time: 98.9045000076294 and batch: 100, loss is 4.5453619861602785 and perplexity is 94.19451815075476
At time: 100.66251707077026 and batch: 150, loss is 4.545582218170166 and perplexity is 94.21526508329293
At time: 102.41914129257202 and batch: 200, loss is 4.578110218048096 and perplexity is 97.33028728059543
At time: 104.16665816307068 and batch: 250, loss is 4.578953332901001 and perplexity is 97.41238249442327
At time: 105.91399216651917 and batch: 300, loss is 4.5553346824646 and perplexity is 95.13859112598028
At time: 107.66044569015503 and batch: 350, loss is 4.502565727233887 and perplexity is 90.2483872487959
At time: 109.40713810920715 and batch: 400, loss is 4.522227220535278 and perplexity is 92.0403640528011
At time: 111.15345191955566 and batch: 450, loss is 4.532955675125122 and perplexity is 93.0331308233343
At time: 112.89944982528687 and batch: 500, loss is 4.524604349136353 and perplexity is 92.25941608892393
At time: 114.6447594165802 and batch: 550, loss is 4.535181341171264 and perplexity is 93.24042209878114
At time: 116.39279699325562 and batch: 600, loss is 4.567844076156616 and perplexity is 96.33619223237211
At time: 118.14352631568909 and batch: 650, loss is 4.546293830871582 and perplexity is 94.28233372321122
At time: 119.89085388183594 and batch: 700, loss is 4.511548528671264 and perplexity is 91.06272262220838
At time: 121.63788151741028 and batch: 750, loss is 4.497857637405396 and perplexity is 89.82448839470914
At time: 123.3868055343628 and batch: 800, loss is 4.475735931396485 and perplexity is 87.85923499303058
At time: 125.13463973999023 and batch: 850, loss is 4.4901126194000245 and perplexity is 89.13148324761345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.495149930318196 and perplexity of 89.58159897597785
Finished 4 epochs...
Completing Train Step...
At time: 129.55878520011902 and batch: 50, loss is 4.46456208229065 and perplexity is 86.88297361489187
At time: 131.30297374725342 and batch: 100, loss is 4.395966958999634 and perplexity is 81.123035498789
At time: 133.02785921096802 and batch: 150, loss is 4.3995805263519285 and perplexity is 81.41670933692049
At time: 134.77563667297363 and batch: 200, loss is 4.436692571640014 and perplexity is 84.49501779678046
At time: 136.52215933799744 and batch: 250, loss is 4.434171667098999 and perplexity is 84.28228217847702
At time: 138.27014708518982 and batch: 300, loss is 4.419390029907227 and perplexity is 83.04561456486
At time: 140.01751160621643 and batch: 350, loss is 4.367251014709472 and perplexity is 78.82664030843252
At time: 141.7665560245514 and batch: 400, loss is 4.387452697753906 and perplexity is 80.43526486578342
At time: 143.5207600593567 and batch: 450, loss is 4.402718420028687 and perplexity is 81.67258756371011
At time: 145.2754979133606 and batch: 500, loss is 4.394236621856689 and perplexity is 80.98278667118768
At time: 147.02461910247803 and batch: 550, loss is 4.408972949981689 and perplexity is 82.18501202557312
At time: 148.7743275165558 and batch: 600, loss is 4.445156059265137 and perplexity is 85.21317510520154
At time: 150.52042388916016 and batch: 650, loss is 4.4202279949188235 and perplexity is 83.1152330490851
At time: 152.2692527770996 and batch: 700, loss is 4.39008936882019 and perplexity is 80.6476260416194
At time: 154.01813316345215 and batch: 750, loss is 4.381775608062744 and perplexity is 79.9799203914725
At time: 155.76904797554016 and batch: 800, loss is 4.355193815231323 and perplexity is 77.88191857619572
At time: 157.51704478263855 and batch: 850, loss is 4.372521142959595 and perplexity is 79.24316341342447
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.464156786600749 and perplexity of 86.84776745509433
Finished 5 epochs...
Completing Train Step...
At time: 161.93193435668945 and batch: 50, loss is 4.35491042137146 and perplexity is 77.85985044581086
At time: 163.64766645431519 and batch: 100, loss is 4.290437784194946 and perplexity is 72.99841905951776
At time: 165.37075448036194 and batch: 150, loss is 4.292809734344482 and perplexity is 73.17177318291502
At time: 167.094655752182 and batch: 200, loss is 4.336110172271728 and perplexity is 76.40973979476529
At time: 168.8412160873413 and batch: 250, loss is 4.329813270568848 and perplexity is 75.93010685968275
At time: 170.58388423919678 and batch: 300, loss is 4.317601871490479 and perplexity is 75.00853233186372
At time: 172.33365750312805 and batch: 350, loss is 4.2658624172210695 and perplexity is 71.22632029845931
At time: 174.07694029808044 and batch: 400, loss is 4.286521921157837 and perplexity is 72.71312619699894
At time: 175.86557173728943 and batch: 450, loss is 4.3071051025390625 and perplexity is 74.22530297884713
At time: 177.6189649105072 and batch: 500, loss is 4.298672370910644 and perplexity is 73.60201263245766
At time: 179.3732180595398 and batch: 550, loss is 4.314488372802734 and perplexity is 74.77535654948974
At time: 181.1261248588562 and batch: 600, loss is 4.353892526626587 and perplexity is 77.78063763519674
At time: 182.87953281402588 and batch: 650, loss is 4.3246974182128906 and perplexity is 75.54265157068095
At time: 184.63262104988098 and batch: 700, loss is 4.299267444610596 and perplexity is 73.64582428872808
At time: 186.3855495452881 and batch: 750, loss is 4.291791706085205 and perplexity is 73.09732015410721
At time: 188.13769793510437 and batch: 800, loss is 4.263651866912841 and perplexity is 71.06904483088057
At time: 189.89012598991394 and batch: 850, loss is 4.284568634033203 and perplexity is 72.57123520584297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.4504092534383135 and perplexity of 85.661994295183
Finished 6 epochs...
Completing Train Step...
At time: 194.29293513298035 and batch: 50, loss is 4.270995187759399 and perplexity is 71.59284850473824
At time: 195.99788069725037 and batch: 100, loss is 4.21013370513916 and perplexity is 67.36554632774067
At time: 197.7196385860443 and batch: 150, loss is 4.212096405029297 and perplexity is 67.49789451551807
At time: 199.45901608467102 and batch: 200, loss is 4.2578827667236325 and perplexity is 70.66022079819246
At time: 201.19736099243164 and batch: 250, loss is 4.248870515823365 and perplexity is 70.02627409386122
At time: 202.94555115699768 and batch: 300, loss is 4.237991046905518 and perplexity is 69.26855468759283
At time: 204.69464230537415 and batch: 350, loss is 4.188555288314819 and perplexity is 65.92747593076811
At time: 206.4443953037262 and batch: 400, loss is 4.209016542434693 and perplexity is 67.29033007403677
At time: 208.19487762451172 and batch: 450, loss is 4.234203977584839 and perplexity is 69.00672596393755
At time: 209.94351315498352 and batch: 500, loss is 4.223083457946777 and perplexity is 68.24358643129246
At time: 211.69203758239746 and batch: 550, loss is 4.2410988521575925 and perplexity is 69.48416272601499
At time: 213.43915820121765 and batch: 600, loss is 4.284076280593872 and perplexity is 72.53551330321532
At time: 215.1877841949463 and batch: 650, loss is 4.25152328491211 and perplexity is 70.21228424107949
At time: 216.9376459121704 and batch: 700, loss is 4.227168283462524 and perplexity is 68.52291969985636
At time: 218.68695402145386 and batch: 750, loss is 4.221397771835327 and perplexity is 68.12864606935281
At time: 220.46502351760864 and batch: 800, loss is 4.19281804561615 and perplexity is 66.20910859954337
At time: 222.21489024162292 and batch: 850, loss is 4.214497661590576 and perplexity is 67.66016903099512
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.446350733439128 and perplexity of 85.31503791910477
Finished 7 epochs...
Completing Train Step...
At time: 226.64349722862244 and batch: 50, loss is 4.204420590400696 and perplexity is 66.98177653641949
At time: 228.3629584312439 and batch: 100, loss is 4.143586564064026 and perplexity is 63.02847220529014
At time: 230.0888695716858 and batch: 150, loss is 4.144449214935303 and perplexity is 63.08286723033795
At time: 231.80866265296936 and batch: 200, loss is 4.19261137008667 and perplexity is 66.19542621092333
At time: 233.54639291763306 and batch: 250, loss is 4.184843230247497 and perplexity is 65.6832029700823
At time: 235.28465461730957 and batch: 300, loss is 4.175403780937195 and perplexity is 65.06610681116068
At time: 237.02317905426025 and batch: 350, loss is 4.124859938621521 and perplexity is 61.859144566528734
At time: 238.76360440254211 and batch: 400, loss is 4.1474047374725345 and perplexity is 63.269585855785856
At time: 240.50106978416443 and batch: 450, loss is 4.173295841217041 and perplexity is 64.92909583634861
At time: 242.2406041622162 and batch: 500, loss is 4.159088630676269 and perplexity is 64.01315638034524
At time: 243.98162078857422 and batch: 550, loss is 4.181544003486633 and perplexity is 65.46685627375179
At time: 245.73085498809814 and batch: 600, loss is 4.222830963134766 and perplexity is 68.2263574549706
At time: 247.47910356521606 and batch: 650, loss is 4.191211547851562 and perplexity is 66.10282920624421
At time: 249.2269525527954 and batch: 700, loss is 4.171283369064331 and perplexity is 64.79855923377268
At time: 250.97419667243958 and batch: 750, loss is 4.165053143501281 and perplexity is 64.39610458699589
At time: 252.7219319343567 and batch: 800, loss is 4.134493527412414 and perplexity is 62.457949818968515
At time: 254.4696502685547 and batch: 850, loss is 4.155870490074157 and perplexity is 63.807484161290205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.441252072652181 and perplexity of 84.88115253787169
Finished 8 epochs...
Completing Train Step...
At time: 258.8558785915375 and batch: 50, loss is 4.147514500617981 and perplexity is 63.27653090568878
At time: 260.5925965309143 and batch: 100, loss is 4.088711228370666 and perplexity is 59.662950217229714
At time: 262.33891797065735 and batch: 150, loss is 4.0910083246231075 and perplexity is 59.80015928747012
At time: 264.13251399993896 and batch: 200, loss is 4.138588509559631 and perplexity is 62.71423839886934
At time: 265.88573360443115 and batch: 250, loss is 4.130325975418091 and perplexity is 62.1981947130567
At time: 267.633663892746 and batch: 300, loss is 4.122574510574341 and perplexity is 61.717931370326646
At time: 269.38492250442505 and batch: 350, loss is 4.071554822921753 and perplexity is 58.64807911623278
At time: 271.1356372833252 and batch: 400, loss is 4.0949744033813475 and perplexity is 60.03780237305036
At time: 272.8824071884155 and batch: 450, loss is 4.123112750053406 and perplexity is 61.75115933905647
At time: 274.6294105052948 and batch: 500, loss is 4.106078791618347 and perplexity is 60.70820073258127
At time: 276.3790338039398 and batch: 550, loss is 4.128336863517761 and perplexity is 62.07459850786659
At time: 278.1380715370178 and batch: 600, loss is 4.173442373275757 and perplexity is 64.9386107275333
At time: 279.89104175567627 and batch: 650, loss is 4.140932235717774 and perplexity is 62.861395780833554
At time: 281.6464297771454 and batch: 700, loss is 4.123300204277038 and perplexity is 61.76273593969621
At time: 283.40166783332825 and batch: 750, loss is 4.116563696861267 and perplexity is 61.348069082348935
At time: 285.15886545181274 and batch: 800, loss is 4.084955849647522 and perplexity is 59.43931342667304
At time: 286.9063699245453 and batch: 850, loss is 4.10761360168457 and perplexity is 60.80144783015521
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.442885716756185 and perplexity of 85.01993145907508
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 291.3938412666321 and batch: 50, loss is 4.139954123497009 and perplexity is 62.799940341463724
At time: 293.15672039985657 and batch: 100, loss is 4.09330894947052 and perplexity is 59.93789539861431
At time: 294.8726885318756 and batch: 150, loss is 4.085143785476685 and perplexity is 59.4504852530871
At time: 296.59776878356934 and batch: 200, loss is 4.118199377059937 and perplexity is 61.4484970157738
At time: 298.32431721687317 and batch: 250, loss is 4.092420353889465 and perplexity is 59.88465850616203
At time: 300.0556824207306 and batch: 300, loss is 4.082088074684143 and perplexity is 59.269099036726004
At time: 301.80284905433655 and batch: 350, loss is 4.013562884330749 and perplexity is 55.34370292172861
At time: 303.54864621162415 and batch: 400, loss is 4.024604287147522 and perplexity is 55.958161036336286
At time: 305.29541516304016 and batch: 450, loss is 4.046836519241333 and perplexity is 57.21616819756654
At time: 307.0851380825043 and batch: 500, loss is 4.006796865463257 and perplexity is 54.970510321348925
At time: 308.8261103630066 and batch: 550, loss is 4.00987099647522 and perplexity is 55.13975688163542
At time: 310.5668842792511 and batch: 600, loss is 4.033860502243042 and perplexity is 56.478526402822794
At time: 312.3033998012543 and batch: 650, loss is 3.98255916595459 and perplexity is 53.65416859943987
At time: 314.02938294410706 and batch: 700, loss is 3.942859582901001 and perplexity is 51.56584748459578
At time: 315.778044462204 and batch: 750, loss is 3.915619373321533 and perplexity is 50.180142129193115
At time: 317.53367376327515 and batch: 800, loss is 3.8544820833206175 and perplexity is 47.20416278597075
At time: 319.2903892993927 and batch: 850, loss is 3.8596539068222047 and perplexity is 47.44892677694666
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.35763676961263 and perplexity of 78.07241314043146
Finished 10 epochs...
Completing Train Step...
At time: 323.7292547225952 and batch: 50, loss is 4.0589639806747435 and perplexity is 57.9142796745301
At time: 325.4591431617737 and batch: 100, loss is 4.010021471977234 and perplexity is 55.148054688525804
At time: 327.175085067749 and batch: 150, loss is 4.002911190986634 and perplexity is 54.75732726064325
At time: 328.914959192276 and batch: 200, loss is 4.043368721008301 and perplexity is 57.018097703338505
At time: 330.65430784225464 and batch: 250, loss is 4.023640570640564 and perplexity is 55.904259210058186
At time: 332.39613461494446 and batch: 300, loss is 4.020244145393372 and perplexity is 55.71470665541122
At time: 334.1410369873047 and batch: 350, loss is 3.9558270454406737 and perplexity is 52.23888001108936
At time: 335.8812747001648 and batch: 400, loss is 3.9691141414642335 and perplexity is 52.93761482431268
At time: 337.63168716430664 and batch: 450, loss is 3.996756420135498 and perplexity is 54.421343471172165
At time: 339.3721024990082 and batch: 500, loss is 3.961522755622864 and perplexity is 52.537266486652896
At time: 341.11223578453064 and batch: 550, loss is 3.9715264368057253 and perplexity is 53.06547013632887
At time: 342.852335691452 and batch: 600, loss is 4.002807884216309 and perplexity is 54.751670750195274
At time: 344.59413290023804 and batch: 650, loss is 3.959348826408386 and perplexity is 52.42317824300315
At time: 346.33216285705566 and batch: 700, loss is 3.9259257984161375 and perplexity is 50.69999431242362
At time: 348.0856008529663 and batch: 750, loss is 3.908300495147705 and perplexity is 49.81422048358795
At time: 349.8371157646179 and batch: 800, loss is 3.8562140226364137 and perplexity is 47.28598836938774
At time: 351.63469767570496 and batch: 850, loss is 3.872154769897461 and perplexity is 48.045802270190805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351113001505534 and perplexity of 77.56474457804093
Finished 11 epochs...
Completing Train Step...
At time: 356.07068037986755 and batch: 50, loss is 4.031533846855163 and perplexity is 56.347273084897644
At time: 357.7662396430969 and batch: 100, loss is 3.9798649883270265 and perplexity is 53.50980929090389
At time: 359.47812151908875 and batch: 150, loss is 3.9711994075775148 and perplexity is 53.04811901390229
At time: 361.19842076301575 and batch: 200, loss is 4.013770775794983 and perplexity is 55.35520960119389
At time: 362.9323182106018 and batch: 250, loss is 3.9941117477416994 and perplexity is 54.27760699817948
At time: 364.68240761756897 and batch: 300, loss is 3.992285385131836 and perplexity is 54.178566875323625
At time: 366.43488574028015 and batch: 350, loss is 3.9300756216049195 and perplexity is 50.91082748211706
At time: 368.1871156692505 and batch: 400, loss is 3.944530601501465 and perplexity is 51.65208700874845
At time: 369.9355618953705 and batch: 450, loss is 3.9745003271102903 and perplexity is 53.223515912395015
At time: 371.681969165802 and batch: 500, loss is 3.940941801071167 and perplexity is 51.46705020534376
At time: 373.42883348464966 and batch: 550, loss is 3.95342755317688 and perplexity is 52.1136834867482
At time: 375.17594814300537 and batch: 600, loss is 3.9880878496170045 and perplexity is 53.95162704393284
At time: 376.92167568206787 and batch: 650, loss is 3.9479935550689698 and perplexity is 51.8312658526728
At time: 378.66916394233704 and batch: 700, loss is 3.9174263763427732 and perplexity is 50.27089977259914
At time: 380.4178259372711 and batch: 750, loss is 3.904240074157715 and perplexity is 49.612363865894515
At time: 382.16731357574463 and batch: 800, loss is 3.8558466720581053 and perplexity is 47.268621024361785
At time: 383.916809797287 and batch: 850, loss is 3.8756090116500856 and perplexity is 48.212051052870365
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.348544438680013 and perplexity of 77.36577030689938
Finished 12 epochs...
Completing Train Step...
At time: 388.3394458293915 and batch: 50, loss is 4.011402292251587 and perplexity is 55.22425683916043
At time: 390.03385829925537 and batch: 100, loss is 3.959020972251892 and perplexity is 52.405993903252096
At time: 391.76148891448975 and batch: 150, loss is 3.949609899520874 and perplexity is 51.91511077455186
At time: 393.5151717662811 and batch: 200, loss is 3.9937555170059205 and perplexity is 54.2582750898166
At time: 395.31301641464233 and batch: 250, loss is 3.9738146305084228 and perplexity is 53.187033237846876
At time: 397.06619930267334 and batch: 300, loss is 3.973102579116821 and perplexity is 53.1491748169849
At time: 398.81852293014526 and batch: 350, loss is 3.91265634059906 and perplexity is 50.031676788499006
At time: 400.5711359977722 and batch: 400, loss is 3.928058376312256 and perplexity is 50.80823137058569
At time: 402.32324743270874 and batch: 450, loss is 3.95917142868042 and perplexity is 52.41387931511881
At time: 404.07617926597595 and batch: 500, loss is 3.9268394708633423 and perplexity is 50.746338668857085
At time: 405.8288471698761 and batch: 550, loss is 3.94080406665802 and perplexity is 51.45996190954965
At time: 407.58272910118103 and batch: 600, loss is 3.977574667930603 and perplexity is 53.38739492085154
At time: 409.33544421195984 and batch: 650, loss is 3.9393992185592652 and perplexity is 51.387719236771275
At time: 411.087769985199 and batch: 700, loss is 3.9105042457580566 and perplexity is 49.92411965308651
At time: 412.8400409221649 and batch: 750, loss is 3.900134024620056 and perplexity is 49.4090706937194
At time: 414.5986988544464 and batch: 800, loss is 3.8535651540756226 and perplexity is 47.16089974623113
At time: 416.34956097602844 and batch: 850, loss is 3.8754613733291627 and perplexity is 48.20493363201963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.347876866658528 and perplexity of 77.31414031850719
Finished 13 epochs...
Completing Train Step...
At time: 420.77657175064087 and batch: 50, loss is 3.994997978210449 and perplexity is 54.325730788507
At time: 422.4769368171692 and batch: 100, loss is 3.942390551567078 and perplexity is 51.54166715747402
At time: 424.1863031387329 and batch: 150, loss is 3.9328783845901487 and perplexity is 51.05371861638248
At time: 425.9062933921814 and batch: 200, loss is 3.9780844497680663 and perplexity is 53.41461778340257
At time: 427.6251871585846 and batch: 250, loss is 3.957908239364624 and perplexity is 52.347712462197606
At time: 429.35888934135437 and batch: 300, loss is 3.9582094383239745 and perplexity is 52.363481913467545
At time: 431.0975122451782 and batch: 350, loss is 3.899065055847168 and perplexity is 49.35628215972435
At time: 432.84729981422424 and batch: 400, loss is 3.9150565814971925 and perplexity is 50.15190910086251
At time: 434.596382856369 and batch: 450, loss is 3.9471174907684325 and perplexity is 51.78587821515495
At time: 436.3466477394104 and batch: 500, loss is 3.9155105638504026 and perplexity is 50.17468235150993
At time: 438.09523034095764 and batch: 550, loss is 3.9306444787979125 and perplexity is 50.93979671142752
At time: 439.88496804237366 and batch: 600, loss is 3.9687305641174317 and perplexity is 52.91731304837195
At time: 441.62572026252747 and batch: 650, loss is 3.9320199346542357 and perplexity is 51.00991036119538
At time: 443.3678822517395 and batch: 700, loss is 3.904417486190796 and perplexity is 49.62116647705542
At time: 445.1169674396515 and batch: 750, loss is 3.8956329154968263 and perplexity is 49.18717483821314
At time: 446.86630964279175 and batch: 800, loss is 3.850224161148071 and perplexity is 47.003598431331774
At time: 448.6169373989105 and batch: 850, loss is 3.873105216026306 and perplexity is 48.09148892487856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.347456614176433 and perplexity of 77.28165568548917
Finished 14 epochs...
Completing Train Step...
At time: 453.0346574783325 and batch: 50, loss is 3.9808114433288573 and perplexity is 53.56047789154722
At time: 454.7679133415222 and batch: 100, loss is 3.92839346408844 and perplexity is 50.82525944063722
At time: 456.504745721817 and batch: 150, loss is 3.9187776947021487 and perplexity is 50.33887768195832
At time: 458.24061942100525 and batch: 200, loss is 3.9648876667022703 and perplexity is 52.71434748054959
At time: 459.977899312973 and batch: 250, loss is 3.9444142150878907 and perplexity is 51.64607575740866
At time: 461.7147624492645 and batch: 300, loss is 3.9455987071990966 and perplexity is 51.70728637130604
At time: 463.4579474925995 and batch: 350, loss is 3.8873672914505004 and perplexity is 48.78228777041405
At time: 465.209796667099 and batch: 400, loss is 3.903942575454712 and perplexity is 49.597606447256815
At time: 466.96162581443787 and batch: 450, loss is 3.9367069721221926 and perplexity is 51.249556899728844
At time: 468.71564984321594 and batch: 500, loss is 3.9057109212875365 and perplexity is 49.68538976068892
At time: 470.4636447429657 and batch: 550, loss is 3.9215309953689577 and perplexity is 50.47766672375105
At time: 472.21283316612244 and batch: 600, loss is 3.960782051086426 and perplexity is 52.49836630358374
At time: 473.9613118171692 and batch: 650, loss is 3.9252180194854738 and perplexity is 50.66412262077579
At time: 475.7091860771179 and batch: 700, loss is 3.898490948677063 and perplexity is 49.32795449658205
At time: 477.45454239845276 and batch: 750, loss is 3.8909886837005616 and perplexity is 48.95926783298281
At time: 479.2016546726227 and batch: 800, loss is 3.8462014389038086 and perplexity is 46.81489581393998
At time: 480.94858479499817 and batch: 850, loss is 3.869842085838318 and perplexity is 47.934815896847624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.347404797871907 and perplexity of 77.27765133943005
Finished 15 epochs...
Completing Train Step...
At time: 485.3453731536865 and batch: 50, loss is 3.968314137458801 and perplexity is 52.89528145610506
At time: 487.08270287513733 and batch: 100, loss is 3.9163179540634157 and perplexity is 50.21520925729357
At time: 488.8008909225464 and batch: 150, loss is 3.906489806175232 and perplexity is 49.724104034937156
At time: 490.5392141342163 and batch: 200, loss is 3.953358039855957 and perplexity is 52.11006101744991
At time: 492.277498960495 and batch: 250, loss is 3.9326443767547605 and perplexity is 51.04177304393383
At time: 494.01514077186584 and batch: 300, loss is 3.934490513801575 and perplexity is 51.136090186483024
At time: 495.7554042339325 and batch: 350, loss is 3.8770856428146363 and perplexity is 48.28329505757549
At time: 497.503958940506 and batch: 400, loss is 3.8941274976730345 and perplexity is 49.11318329657633
At time: 499.25191354751587 and batch: 450, loss is 3.927496247291565 and perplexity is 50.779678615160385
At time: 500.9997570514679 and batch: 500, loss is 3.8969450998306274 and perplexity is 49.251759842908434
At time: 502.7481918334961 and batch: 550, loss is 3.9132382583618166 and perplexity is 50.060799582636506
At time: 504.496572971344 and batch: 600, loss is 3.95337375164032 and perplexity is 52.110879765923734
At time: 506.24361205101013 and batch: 650, loss is 3.918729348182678 and perplexity is 50.3364440312581
At time: 507.996426820755 and batch: 700, loss is 3.892295227050781 and perplexity is 49.023277045101224
At time: 509.74604868888855 and batch: 750, loss is 3.8861377906799315 and perplexity is 48.722346766316505
At time: 511.50229144096375 and batch: 800, loss is 3.84189377784729 and perplexity is 46.61366683466206
At time: 513.251871585846 and batch: 850, loss is 3.8660151863098147 and perplexity is 47.751724731689784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.34751033782959 and perplexity of 77.28580764988298
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 517.67595744133 and batch: 50, loss is 3.973392481803894 and perplexity is 53.16458513921905
At time: 519.4205162525177 and batch: 100, loss is 3.938686366081238 and perplexity is 51.35110042722841
At time: 521.1481831073761 and batch: 150, loss is 3.9284309434890745 and perplexity is 50.82716437659586
At time: 522.8921208381653 and batch: 200, loss is 3.9740657806396484 and perplexity is 53.20039284578749
At time: 524.6360325813293 and batch: 250, loss is 3.9461117219924926 and perplexity is 51.73381977957444
At time: 526.413268327713 and batch: 300, loss is 3.9395740365982057 and perplexity is 51.396703522358585
At time: 528.1694123744965 and batch: 350, loss is 3.8774322700500488 and perplexity is 48.300034263622656
At time: 529.9254200458527 and batch: 400, loss is 3.893181505203247 and perplexity is 49.0667445638194
At time: 531.6782088279724 and batch: 450, loss is 3.931558566093445 and perplexity is 50.986381420440104
At time: 533.4310760498047 and batch: 500, loss is 3.8954534196853636 and perplexity is 49.17834673867914
At time: 535.1842362880707 and batch: 550, loss is 3.9037841176986694 and perplexity is 49.58974794447089
At time: 536.9364533424377 and batch: 600, loss is 3.933867349624634 and perplexity is 51.10423393379863
At time: 538.6906747817993 and batch: 650, loss is 3.8959349632263183 and perplexity is 49.202033956661666
At time: 540.4440023899078 and batch: 700, loss is 3.8666788864135744 and perplexity is 47.78342807594158
At time: 542.1957938671112 and batch: 750, loss is 3.852609872817993 and perplexity is 47.11586933438927
At time: 543.9491651058197 and batch: 800, loss is 3.801776466369629 and perplexity is 44.78066522075311
At time: 545.7018573284149 and batch: 850, loss is 3.8229808712005617 and perplexity is 45.74035140374493
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.338234265645345 and perplexity of 76.57221371053996
Finished 17 epochs...
Completing Train Step...
At time: 550.1018307209015 and batch: 50, loss is 3.9639434719085695 and perplexity is 52.664598358232226
At time: 551.8008108139038 and batch: 100, loss is 3.9232502460479735 and perplexity is 50.56452513083383
At time: 553.5184409618378 and batch: 150, loss is 3.911447172164917 and perplexity is 49.971216624846306
At time: 555.2430834770203 and batch: 200, loss is 3.958014307022095 and perplexity is 52.353265155907835
At time: 556.9793744087219 and batch: 250, loss is 3.9327137422561647 and perplexity is 51.04531370491202
At time: 558.7164072990417 and batch: 300, loss is 3.9289877605438233 and perplexity is 50.85547368938792
At time: 560.4535677433014 and batch: 350, loss is 3.868372206687927 and perplexity is 47.864409267677544
At time: 562.1936378479004 and batch: 400, loss is 3.884359903335571 and perplexity is 48.63580087982807
At time: 563.9428536891937 and batch: 450, loss is 3.9236053037643432 and perplexity is 50.58248164326661
At time: 565.6903479099274 and batch: 500, loss is 3.8881500148773194 and perplexity is 48.82048575714513
At time: 567.438554763794 and batch: 550, loss is 3.8982272863388063 and perplexity is 49.31495028719353
At time: 569.1867299079895 and batch: 600, loss is 3.929984645843506 and perplexity is 50.90619604150029
At time: 570.9596836566925 and batch: 650, loss is 3.893690996170044 and perplexity is 49.09174999642491
At time: 572.7075822353363 and batch: 700, loss is 3.8659009885787965 and perplexity is 47.7462719044294
At time: 574.4548287391663 and batch: 750, loss is 3.853707971572876 and perplexity is 47.16763562889061
At time: 576.2009603977203 and batch: 800, loss is 3.8050708293914797 and perplexity is 44.928432253834224
At time: 577.947832107544 and batch: 850, loss is 3.827926926612854 and perplexity is 45.967146123642976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.337445576985677 and perplexity of 76.51184588278058
Finished 18 epochs...
Completing Train Step...
At time: 582.4047992229462 and batch: 50, loss is 3.960166697502136 and perplexity is 52.466071183187594
At time: 584.130462884903 and batch: 100, loss is 3.9183784914016724 and perplexity is 50.31878624639593
At time: 585.8598937988281 and batch: 150, loss is 3.905693001747131 and perplexity is 49.68449942931674
At time: 587.6088528633118 and batch: 200, loss is 3.9525394201278687 and perplexity is 52.06742014917841
At time: 589.3459324836731 and batch: 250, loss is 3.9276495456695555 and perplexity is 50.787463654228645
At time: 591.0838973522186 and batch: 300, loss is 3.924280734062195 and perplexity is 50.61665812452517
At time: 592.828592300415 and batch: 350, loss is 3.864120225906372 and perplexity is 47.661322785233445
At time: 594.5748016834259 and batch: 400, loss is 3.8803764247894286 and perplexity is 48.442446577486805
At time: 596.322735786438 and batch: 450, loss is 3.9200036334991455 and perplexity is 50.40062790837122
At time: 598.0756757259369 and batch: 500, loss is 3.8848762464523316 and perplexity is 48.66092012535718
At time: 599.8225746154785 and batch: 550, loss is 3.8957127714157105 and perplexity is 49.191102882093844
At time: 601.5686738491058 and batch: 600, loss is 3.9283823776245117 and perplexity is 50.82469597135523
At time: 603.3203296661377 and batch: 650, loss is 3.893015594482422 and perplexity is 49.05860454013755
At time: 605.0689222812653 and batch: 700, loss is 3.865917892456055 and perplexity is 47.74707900837079
At time: 606.8156752586365 and batch: 750, loss is 3.85467698097229 and perplexity is 47.21336366303245
At time: 608.565927028656 and batch: 800, loss is 3.806913065910339 and perplexity is 45.01127733915793
At time: 610.3172574043274 and batch: 850, loss is 3.8303740215301514 and perplexity is 46.07976983754193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.33714230855306 and perplexity of 76.48864577331128
Finished 19 epochs...
Completing Train Step...
At time: 614.7409837245941 and batch: 50, loss is 3.9570627784729004 and perplexity is 52.303473222448375
At time: 616.4610710144043 and batch: 100, loss is 3.9148884773254395 and perplexity is 50.14347906430329
At time: 618.1839225292206 and batch: 150, loss is 3.901703052520752 and perplexity is 49.48665575483119
At time: 619.9114592075348 and batch: 200, loss is 3.9487803173065186 and perplexity is 51.87206078122035
At time: 621.6527969837189 and batch: 250, loss is 3.924150514602661 and perplexity is 50.61006727979821
At time: 623.3940844535828 and batch: 300, loss is 3.9208484935760497 and perplexity is 50.44322737950422
At time: 625.1413488388062 and batch: 350, loss is 3.8610471153259276 and perplexity is 47.515079096511606
At time: 626.8923678398132 and batch: 400, loss is 3.8775812816619872 and perplexity is 48.307232065849696
At time: 628.6452660560608 and batch: 450, loss is 3.917531147003174 and perplexity is 50.276166963885984
At time: 630.3965644836426 and batch: 500, loss is 3.882636098861694 and perplexity is 48.552034487864105
At time: 632.1517941951752 and batch: 550, loss is 3.8939243602752684 and perplexity is 49.103207585579746
At time: 633.9052019119263 and batch: 600, loss is 3.9272196197509768 and perplexity is 50.76563350027556
At time: 635.6596240997314 and batch: 650, loss is 3.892511148452759 and perplexity is 49.03386336267548
At time: 637.4160852432251 and batch: 700, loss is 3.8658365964889527 and perplexity is 47.74319752118327
At time: 639.182933807373 and batch: 750, loss is 3.8552154016494753 and perplexity is 47.238791158998616
At time: 640.9372913837433 and batch: 800, loss is 3.8081529712677002 and perplexity is 45.06712167676467
At time: 642.6891994476318 and batch: 850, loss is 3.831851181983948 and perplexity is 46.1478873491271
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336962699890137 and perplexity of 76.4749089835753
Finished 20 epochs...
Completing Train Step...
At time: 647.0903813838959 and batch: 50, loss is 3.9542972326278685 and perplexity is 52.15902539999923
At time: 648.8348577022552 and batch: 100, loss is 3.9119732713699342 and perplexity is 49.997513358925296
At time: 650.5675983428955 and batch: 150, loss is 3.898443088531494 and perplexity is 49.32559370999347
At time: 652.3159081935883 and batch: 200, loss is 3.9457534313201905 and perplexity is 51.71528735470059
At time: 654.0648622512817 and batch: 250, loss is 3.9212907123565675 and perplexity is 50.465539255002945
At time: 655.8148546218872 and batch: 300, loss is 3.918002128601074 and perplexity is 50.29985169043629
At time: 657.5640468597412 and batch: 350, loss is 3.8585063123703005 and perplexity is 47.394505884358395
At time: 659.3566195964813 and batch: 400, loss is 3.875321068763733 and perplexity is 48.198170734198634
At time: 661.0956044197083 and batch: 450, loss is 3.915531210899353 and perplexity is 50.175718321327345
At time: 662.8343181610107 and batch: 500, loss is 3.8808207845687868 and perplexity is 48.46397723568453
At time: 664.5722751617432 and batch: 550, loss is 3.892412052154541 and perplexity is 49.0290045290791
At time: 666.3180854320526 and batch: 600, loss is 3.926163125038147 and perplexity is 50.71202819873399
At time: 668.0650305747986 and batch: 650, loss is 3.8919820594787597 and perplexity is 49.00792694815823
At time: 669.8116807937622 and batch: 700, loss is 3.865616340637207 and perplexity is 47.73268296053744
At time: 671.5594296455383 and batch: 750, loss is 3.8557038354873656 and perplexity is 47.261869818802076
At time: 673.3084893226624 and batch: 800, loss is 3.8086136627197265 and perplexity is 45.08788849766973
At time: 675.0584387779236 and batch: 850, loss is 3.832653865814209 and perplexity is 46.1849443826541
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336907068888347 and perplexity of 76.47065472611214
Finished 21 epochs...
Completing Train Step...
At time: 679.552234172821 and batch: 50, loss is 3.951820240020752 and perplexity is 52.02998775830706
At time: 681.2695121765137 and batch: 100, loss is 3.9094584512710573 and perplexity is 49.871936575128636
At time: 682.9931924343109 and batch: 150, loss is 3.895699920654297 and perplexity is 49.190470743028776
At time: 684.7116515636444 and batch: 200, loss is 3.9432242727279663 and perplexity is 51.5846564541037
At time: 686.4526040554047 and batch: 250, loss is 3.918891444206238 and perplexity is 50.344604030009506
At time: 688.1964013576508 and batch: 300, loss is 3.915567421913147 and perplexity is 50.17753526785213
At time: 689.9408655166626 and batch: 350, loss is 3.8563351440429687 and perplexity is 47.29171606167546
At time: 691.7004587650299 and batch: 400, loss is 3.873408489227295 and perplexity is 48.106075996487284
At time: 693.4584505558014 and batch: 450, loss is 3.9138586044311525 and perplexity is 50.091864237307405
At time: 695.2043402194977 and batch: 500, loss is 3.8792666673660277 and perplexity is 48.38871703168516
At time: 696.9497971534729 and batch: 550, loss is 3.8910793018341066 and perplexity is 48.963704631477725
At time: 698.6962349414825 and batch: 600, loss is 3.925184941291809 and perplexity is 50.66244677083309
At time: 700.4416396617889 and batch: 650, loss is 3.8914235496520995 and perplexity is 48.98056318155101
At time: 702.2331562042236 and batch: 700, loss is 3.865272765159607 and perplexity is 47.716285998150525
At time: 703.9749066829681 and batch: 750, loss is 3.855723023414612 and perplexity is 47.26277668482209
At time: 705.7201824188232 and batch: 800, loss is 3.8089203023910523 and perplexity is 45.10171635295558
At time: 707.4676198959351 and batch: 850, loss is 3.8331671667099 and perplexity is 46.20865724136833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336857795715332 and perplexity of 76.46688686713922
Finished 22 epochs...
Completing Train Step...
At time: 711.850937128067 and batch: 50, loss is 3.949514970779419 and perplexity is 51.91018277233176
At time: 713.5955967903137 and batch: 100, loss is 3.9071764278411867 and perplexity is 49.75825740596819
At time: 715.3176462650299 and batch: 150, loss is 3.8932378816604616 and perplexity is 49.069510851020965
At time: 717.0560507774353 and batch: 200, loss is 3.940958228111267 and perplexity is 51.46789566358547
At time: 718.8013546466827 and batch: 250, loss is 3.916733145713806 and perplexity is 50.236062521650865
At time: 720.5467805862427 and batch: 300, loss is 3.913353991508484 and perplexity is 50.06659361177104
At time: 722.2925276756287 and batch: 350, loss is 3.854365224838257 and perplexity is 47.19864690144171
At time: 724.0393691062927 and batch: 400, loss is 3.8716905546188354 and perplexity is 48.02350385073709
At time: 725.7873811721802 and batch: 450, loss is 3.9123453378677366 and perplexity is 50.01611921971367
At time: 727.5343859195709 and batch: 500, loss is 3.8778451681137085 and perplexity is 48.31998137202263
At time: 729.2818496227264 and batch: 550, loss is 3.8898267459869387 and perplexity is 48.90241325039476
At time: 731.0278835296631 and batch: 600, loss is 3.9242325830459595 and perplexity is 50.614220939674965
At time: 732.7744040489197 and batch: 650, loss is 3.890828585624695 and perplexity is 48.95143017582056
At time: 734.5204799175262 and batch: 700, loss is 3.8648403215408327 and perplexity is 47.695655855767235
At time: 736.2656300067902 and batch: 750, loss is 3.855615439414978 and perplexity is 47.25769223977991
At time: 738.0090658664703 and batch: 800, loss is 3.809036717414856 and perplexity is 45.10696717596981
At time: 739.7547314167023 and batch: 850, loss is 3.833439016342163 and perplexity is 46.221220755462724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336829503377278 and perplexity of 76.46472347072998
Finished 23 epochs...
Completing Train Step...
At time: 744.2046186923981 and batch: 50, loss is 3.9473364877700807 and perplexity is 51.797220409119696
At time: 745.9278903007507 and batch: 100, loss is 3.90506227016449 and perplexity is 49.65317172708313
At time: 747.6785933971405 and batch: 150, loss is 3.8909881210327146 and perplexity is 48.959240285184734
At time: 749.4263470172882 and batch: 200, loss is 3.9389189767837522 and perplexity is 51.363046632127364
At time: 751.1799192428589 and batch: 250, loss is 3.9147708892822264 and perplexity is 50.13758313737227
At time: 752.9281666278839 and batch: 300, loss is 3.91130793094635 and perplexity is 49.96425905615161
At time: 754.6779026985168 and batch: 350, loss is 3.8525405359268188 and perplexity is 47.11260257973925
At time: 756.4266755580902 and batch: 400, loss is 3.870097990036011 and perplexity is 47.9470841871378
At time: 758.1769924163818 and batch: 450, loss is 3.9109427547454834 and perplexity is 49.94601662890322
At time: 759.9269347190857 and batch: 500, loss is 3.8765125465393067 and perplexity is 48.25563200856966
At time: 761.6792988777161 and batch: 550, loss is 3.888628764152527 and perplexity is 48.84386412505845
At time: 763.4297676086426 and batch: 600, loss is 3.9232976961135866 and perplexity is 50.56692447779312
At time: 765.1797873973846 and batch: 650, loss is 3.890197238922119 and perplexity is 48.920534605727084
At time: 766.9299509525299 and batch: 700, loss is 3.8643410301208494 and perplexity is 47.671847768109515
At time: 768.679486989975 and batch: 750, loss is 3.8554070568084717 and perplexity is 47.24784558466497
At time: 770.4279155731201 and batch: 800, loss is 3.8090129947662352 and perplexity is 45.10589713192934
At time: 772.1763098239899 and batch: 850, loss is 3.8335496473312376 and perplexity is 46.22633453769731
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336814880371094 and perplexity of 76.46360533478105
Finished 24 epochs...
Completing Train Step...
At time: 776.5633690357208 and batch: 50, loss is 3.9452627992630003 and perplexity is 51.689920400306946
At time: 778.2673392295837 and batch: 100, loss is 3.903090295791626 and perplexity is 49.55535342419785
At time: 779.9808433055878 and batch: 150, loss is 3.8888930082321167 and perplexity is 48.856772532387794
At time: 781.7044057846069 and batch: 200, loss is 3.937014832496643 and perplexity is 51.26533703642119
At time: 783.4406228065491 and batch: 250, loss is 3.9129173946380615 and perplexity is 50.04473946476067
At time: 785.1755883693695 and batch: 300, loss is 3.909391412734985 and perplexity is 49.868593345573416
At time: 786.9158129692078 and batch: 350, loss is 3.8508268070220946 and perplexity is 47.031933493137146
At time: 788.6580495834351 and batch: 400, loss is 3.8686166620254516 and perplexity is 47.87611140826757
At time: 790.4317967891693 and batch: 450, loss is 3.9096301317214968 and perplexity is 49.880499346673346
At time: 792.1770188808441 and batch: 500, loss is 3.875248327255249 and perplexity is 48.19466485406632
At time: 793.9215471744537 and batch: 550, loss is 3.8874797773361207 and perplexity is 48.787775397891025
At time: 795.6668176651001 and batch: 600, loss is 3.9223668718338014 and perplexity is 50.519877456386965
At time: 797.4119129180908 and batch: 650, loss is 3.889546799659729 and perplexity is 48.88872511547456
At time: 799.158077955246 and batch: 700, loss is 3.8637968349456786 and perplexity is 47.645912036252774
At time: 800.9045219421387 and batch: 750, loss is 3.8551260232925415 and perplexity is 47.23456922213899
At time: 802.6516058444977 and batch: 800, loss is 3.8088830709457397 and perplexity is 45.10003718212886
At time: 804.3980252742767 and batch: 850, loss is 3.833539719581604 and perplexity is 46.22587561649958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336809158325195 and perplexity of 76.46316780777353
Finished 25 epochs...
Completing Train Step...
At time: 808.8129880428314 and batch: 50, loss is 3.9432831048965453 and perplexity is 51.58769138058308
At time: 810.498368024826 and batch: 100, loss is 3.9012264347076417 and perplexity is 49.46307515310098
At time: 812.1867444515228 and batch: 150, loss is 3.886933403015137 and perplexity is 48.76112629109245
At time: 813.8962626457214 and batch: 200, loss is 3.935215210914612 and perplexity is 51.17316179463441
At time: 815.636302947998 and batch: 250, loss is 3.911181216239929 and perplexity is 49.957928250845306
At time: 817.3806598186493 and batch: 300, loss is 3.907575778961182 and perplexity is 49.778132390076706
At time: 819.1203279495239 and batch: 350, loss is 3.8491974878311157 and perplexity is 46.95536585480776
At time: 820.8608481884003 and batch: 400, loss is 3.867206039428711 and perplexity is 47.80862389456777
At time: 822.6069695949554 and batch: 450, loss is 3.9083743572235106 and perplexity is 49.81790000120423
At time: 824.3598835468292 and batch: 500, loss is 3.8740283918380736 and perplexity is 48.135906323585374
At time: 826.1061713695526 and batch: 550, loss is 3.8863551235198974 and perplexity is 48.73293688305742
At time: 827.852637052536 and batch: 600, loss is 3.921438317298889 and perplexity is 50.47298876779284
At time: 829.5988805294037 and batch: 650, loss is 3.88887366771698 and perplexity is 48.85582762637662
At time: 831.3464732170105 and batch: 700, loss is 3.86321768283844 and perplexity is 47.61832579498164
At time: 833.0978894233704 and batch: 750, loss is 3.854785695075989 and perplexity is 47.21849670055742
At time: 834.8743698596954 and batch: 800, loss is 3.808669924736023 and perplexity is 45.09042530454943
At time: 836.6214053630829 and batch: 850, loss is 3.8334368801116945 and perplexity is 46.22112201638812
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336815516153972 and perplexity of 76.46365394904755
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 841.0004305839539 and batch: 50, loss is 3.9448953437805176 and perplexity is 51.670930144917165
At time: 842.7601661682129 and batch: 100, loss is 3.90857216835022 and perplexity is 49.82775551086636
At time: 844.4879758358002 and batch: 150, loss is 3.896866979598999 and perplexity is 49.24791243430358
At time: 846.2219595909119 and batch: 200, loss is 3.9453753852844238 and perplexity is 51.6957402904055
At time: 847.955917596817 and batch: 250, loss is 3.9203556632995604 and perplexity is 50.41837355466949
At time: 849.7039504051208 and batch: 300, loss is 3.9121489095687867 and perplexity is 50.00629560334498
At time: 851.4374680519104 and batch: 350, loss is 3.8547960329055786 and perplexity is 47.218984839852936
At time: 853.171409368515 and batch: 400, loss is 3.870190005302429 and perplexity is 47.951496253848894
At time: 854.9036538600922 and batch: 450, loss is 3.910902533531189 and perplexity is 49.94400777986469
At time: 856.6441283226013 and batch: 500, loss is 3.87606698513031 and perplexity is 48.23413595044249
At time: 858.3899374008179 and batch: 550, loss is 3.8864765262603758 and perplexity is 48.73885355428936
At time: 860.1359069347382 and batch: 600, loss is 3.9197460222244263 and perplexity is 50.38764581060827
At time: 861.8888313770294 and batch: 650, loss is 3.884416127204895 and perplexity is 48.63853544961452
At time: 863.6357202529907 and batch: 700, loss is 3.85672550201416 and perplexity is 47.3101803636207
At time: 865.3836245536804 and batch: 750, loss is 3.8481474924087524 and perplexity is 46.9060888104676
At time: 867.1381866931915 and batch: 800, loss is 3.7984824752807618 and perplexity is 44.63340078558717
At time: 868.8990526199341 and batch: 850, loss is 3.823349046707153 and perplexity is 45.757194981300756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3345387776692705 and perplexity of 76.28976423124085
Finished 27 epochs...
Completing Train Step...
At time: 873.4459991455078 and batch: 50, loss is 3.943467354774475 and perplexity is 51.597197282126345
At time: 875.2089629173279 and batch: 100, loss is 3.9045837688446046 and perplexity is 49.629418302350985
At time: 876.9300527572632 and batch: 150, loss is 3.8921116638183593 and perplexity is 49.01427899978332
At time: 878.7125089168549 and batch: 200, loss is 3.941246385574341 and perplexity is 51.48272865884615
At time: 880.4700040817261 and batch: 250, loss is 3.916111989021301 and perplexity is 50.204867744635706
At time: 882.2209932804108 and batch: 300, loss is 3.909119505882263 and perplexity is 49.855035576615826
At time: 883.9818637371063 and batch: 350, loss is 3.8518141889572144 and perplexity is 47.078394908448416
At time: 885.7337508201599 and batch: 400, loss is 3.8674118614196775 and perplexity is 47.81846497344361
At time: 887.4832608699799 and batch: 450, loss is 3.908763256072998 and perplexity is 49.83727789297425
At time: 889.2343089580536 and batch: 500, loss is 3.874013414382935 and perplexity is 48.135185375606845
At time: 890.9873971939087 and batch: 550, loss is 3.8851440572738647 and perplexity is 48.673953791553025
At time: 892.7426598072052 and batch: 600, loss is 3.9186733198165893 and perplexity is 50.33362384155035
At time: 894.4987804889679 and batch: 650, loss is 3.884211292266846 and perplexity is 48.62857359852131
At time: 896.2506504058838 and batch: 700, loss is 3.8569497299194335 and perplexity is 47.320789815684904
At time: 898.003755569458 and batch: 750, loss is 3.849032917022705 and perplexity is 46.94763900811396
At time: 899.7568171024323 and batch: 800, loss is 3.800234398841858 and perplexity is 44.71166362727618
At time: 901.5103595256805 and batch: 850, loss is 3.8255438137054445 and perplexity is 45.85773164970396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.333988507588704 and perplexity of 76.2477958045747
Finished 28 epochs...
Completing Train Step...
At time: 905.9590039253235 and batch: 50, loss is 3.942986207008362 and perplexity is 51.57237737741602
At time: 907.6544690132141 and batch: 100, loss is 3.9030220127105713 and perplexity is 49.55196974750853
At time: 909.3663811683655 and batch: 150, loss is 3.89012761592865 and perplexity is 48.91712873023072
At time: 911.0891349315643 and batch: 200, loss is 3.93937216758728 and perplexity is 51.38632916781927
At time: 912.8096730709076 and batch: 250, loss is 3.9142259883880617 and perplexity is 50.110270565487674
At time: 914.5551614761353 and batch: 300, loss is 3.9076788902282713 and perplexity is 49.783265341008786
At time: 916.3070666790009 and batch: 350, loss is 3.8505260133743286 and perplexity is 47.017788713736806
At time: 918.058375120163 and batch: 400, loss is 3.866165614128113 and perplexity is 47.75890845976387
At time: 919.8105747699738 and batch: 450, loss is 3.907796196937561 and perplexity is 49.78910559458741
At time: 921.5612440109253 and batch: 500, loss is 3.8731213712692263 and perplexity is 48.092265860840314
At time: 923.35320520401 and batch: 550, loss is 3.8846785402297974 and perplexity is 48.65130050961443
At time: 925.0958914756775 and batch: 600, loss is 3.918352608680725 and perplexity is 50.31748387614762
At time: 926.8463885784149 and batch: 650, loss is 3.8843108415603638 and perplexity is 48.633414779631906
At time: 928.6023187637329 and batch: 700, loss is 3.8573250818252562 and perplexity is 47.33855509823501
At time: 930.3519899845123 and batch: 750, loss is 3.849748911857605 and perplexity is 46.98126531184787
At time: 932.101665019989 and batch: 800, loss is 3.8013572883605957 and perplexity is 44.76189808432293
At time: 933.848991394043 and batch: 850, loss is 3.8267549085617065 and perplexity is 45.91330335713678
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3337663014729815 and perplexity of 76.23085496028585
Finished 29 epochs...
Completing Train Step...
At time: 938.3044970035553 and batch: 50, loss is 3.9425687551498414 and perplexity is 51.550852885663616
At time: 940.0172026157379 and batch: 100, loss is 3.9020680665969847 and perplexity is 49.504722377852644
At time: 941.7351050376892 and batch: 150, loss is 3.88894003868103 and perplexity is 48.85907034236555
At time: 943.4694969654083 and batch: 200, loss is 3.9382478475570677 and perplexity is 51.32858695510261
At time: 945.2226288318634 and batch: 250, loss is 3.91312819480896 and perplexity is 50.05529001638238
At time: 946.9650695323944 and batch: 300, loss is 3.9067683315277097 and perplexity is 49.73795538742699
At time: 948.7100162506104 and batch: 350, loss is 3.8497494792938234 and perplexity is 46.98129197072696
At time: 950.4659531116486 and batch: 400, loss is 3.865428953170776 and perplexity is 47.72373929200338
At time: 952.2164876461029 and batch: 450, loss is 3.907208204269409 and perplexity is 49.75983857078487
At time: 953.9642543792725 and batch: 500, loss is 3.8726063442230223 and perplexity is 48.067503420419385
At time: 955.7142972946167 and batch: 550, loss is 3.8844343423843384 and perplexity is 48.63942141733461
At time: 957.4644243717194 and batch: 600, loss is 3.9182142162323 and perplexity is 50.31052079818522
At time: 959.215677022934 and batch: 650, loss is 3.8844208908081055 and perplexity is 48.63876714484999
At time: 960.9649276733398 and batch: 700, loss is 3.8576157522201537 and perplexity is 47.35231701473325
At time: 962.7158811092377 and batch: 750, loss is 3.85024112701416 and perplexity is 47.00439589485327
At time: 964.4681298732758 and batch: 800, loss is 3.8020717525482177 and perplexity is 44.79389028475401
At time: 966.2202048301697 and batch: 850, loss is 3.8274682712554933 and perplexity is 45.946067880004975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.333659807840983 and perplexity of 76.22273729191848
Finished 30 epochs...
Completing Train Step...
At time: 970.7525889873505 and batch: 50, loss is 3.9421597623825075 and perplexity is 51.52977327068236
At time: 972.4670286178589 and batch: 100, loss is 3.9013643932342528 and perplexity is 49.469899476796755
At time: 974.1921837329865 and batch: 150, loss is 3.888102402687073 and perplexity is 48.818161362224544
At time: 975.9303410053253 and batch: 200, loss is 3.9374598407745363 and perplexity is 51.288155612622454
At time: 977.673942565918 and batch: 250, loss is 3.912372612953186 and perplexity is 50.01748343224366
At time: 979.4202411174774 and batch: 300, loss is 3.906100182533264 and perplexity is 49.704734122162485
At time: 981.1655144691467 and batch: 350, loss is 3.849192223548889 and perplexity is 46.95511866916048
At time: 982.911702632904 and batch: 400, loss is 3.864916067123413 and perplexity is 47.6992687278344
At time: 984.665771484375 and batch: 450, loss is 3.906792850494385 and perplexity is 49.739174925648484
At time: 986.4226758480072 and batch: 500, loss is 3.8722521114349364 and perplexity is 48.050479350086405
At time: 988.1794395446777 and batch: 550, loss is 3.8842623615264893 and perplexity is 48.631057087186925
At time: 989.9377498626709 and batch: 600, loss is 3.918127064704895 and perplexity is 50.30613635051155
At time: 991.6930890083313 and batch: 650, loss is 3.8844987106323243 and perplexity is 48.642552352439616
At time: 993.44842004776 and batch: 700, loss is 3.8578175258636476 and perplexity is 47.36187242824807
At time: 995.2084429264069 and batch: 750, loss is 3.8505764389038086 and perplexity is 47.02015967040555
At time: 996.9628627300262 and batch: 800, loss is 3.802543435096741 and perplexity is 44.815023764836845
At time: 998.7171807289124 and batch: 850, loss is 3.8279197072982787 and perplexity is 45.96681427355284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.333600680033366 and perplexity of 76.21823054181027
Finished 31 epochs...
Completing Train Step...
At time: 1003.0853817462921 and batch: 50, loss is 3.9417608261108397 and perplexity is 51.509220274994185
At time: 1004.8172383308411 and batch: 100, loss is 3.900793113708496 and perplexity is 49.44164640705295
At time: 1006.5538766384125 and batch: 150, loss is 3.8874485158920287 and perplexity is 48.786250245417406
At time: 1008.3029778003693 and batch: 200, loss is 3.936848120689392 and perplexity is 51.25679121179924
At time: 1010.0529255867004 and batch: 250, loss is 3.9117915296554564 and perplexity is 49.988427550788
At time: 1011.8496551513672 and batch: 300, loss is 3.9055633449554445 and perplexity is 49.6780579141263
At time: 1013.59965467453 and batch: 350, loss is 3.8487487125396727 and perplexity is 46.93429817449223
At time: 1015.3482007980347 and batch: 400, loss is 3.864517855644226 and perplexity is 47.68027811286953
At time: 1017.0957229137421 and batch: 450, loss is 3.9064690351486204 and perplexity is 49.72307122497531
At time: 1018.8474178314209 and batch: 500, loss is 3.87197660446167 and perplexity is 48.03724293140322
At time: 1020.5964176654816 and batch: 550, loss is 3.8841175508499144 and perplexity is 48.62401530078281
At time: 1022.3472340106964 and batch: 600, loss is 3.918053832054138 and perplexity is 50.30245243369041
At time: 1024.0979781150818 and batch: 650, loss is 3.8845426273345947 and perplexity is 48.64468861983751
At time: 1025.848376750946 and batch: 700, loss is 3.8579520750045777 and perplexity is 47.36824535622251
At time: 1027.5977983474731 and batch: 750, loss is 3.8508084774017335 and perplexity is 47.0310714235521
At time: 1029.3476345539093 and batch: 800, loss is 3.802866291999817 and perplexity is 44.82949494055463
At time: 1031.0964329242706 and batch: 850, loss is 3.828223400115967 and perplexity is 45.98077618485851
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.333566665649414 and perplexity of 76.21563806974339
Finished 32 epochs...
Completing Train Step...
At time: 1035.4604365825653 and batch: 50, loss is 3.941374831199646 and perplexity is 51.48934181482754
At time: 1037.198314666748 and batch: 100, loss is 3.9003020668029786 and perplexity is 49.417374199465016
At time: 1038.9297931194305 and batch: 150, loss is 3.8869037675857543 and perplexity is 48.75968125558987
At time: 1040.6710064411163 and batch: 200, loss is 3.936340413093567 and perplexity is 51.23077435460024
At time: 1042.4110896587372 and batch: 250, loss is 3.9113103199005126 and perplexity is 49.964378418618836
At time: 1044.15886592865 and batch: 300, loss is 3.9051057624816896 and perplexity is 49.655331305540045
At time: 1045.910481452942 and batch: 350, loss is 3.848372049331665 and perplexity is 46.916623080164065
At time: 1047.6601159572601 and batch: 400, loss is 3.8641842794418335 and perplexity is 47.66437575923845
At time: 1049.4119565486908 and batch: 450, loss is 3.9061981010437012 and perplexity is 49.709601373982544
At time: 1051.1622030735016 and batch: 500, loss is 3.8717435121536257 and perplexity is 48.026047124455175
At time: 1052.9126691818237 and batch: 550, loss is 3.883982930183411 and perplexity is 48.617469944014964
At time: 1054.688644170761 and batch: 600, loss is 3.9179814624786378 and perplexity is 50.2988121982839
At time: 1056.4367926120758 and batch: 650, loss is 3.8845588874816896 and perplexity is 48.64547959606054
At time: 1058.18652510643 and batch: 700, loss is 3.8580377817153932 and perplexity is 47.37230530670911
At time: 1059.937115430832 and batch: 750, loss is 3.8509716796875 and perplexity is 47.03874762828051
At time: 1061.687178850174 and batch: 800, loss is 3.8030941247940064 and perplexity is 44.83970973323723
At time: 1063.4379742145538 and batch: 850, loss is 3.8284376811981202 and perplexity is 45.990630051048505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.333544413248698 and perplexity of 76.21394210769392
Finished 33 epochs...
Completing Train Step...
At time: 1067.8280801773071 and batch: 50, loss is 3.941002359390259 and perplexity is 51.47016705776753
At time: 1069.5813374519348 and batch: 100, loss is 3.899864535331726 and perplexity is 49.39575727241388
At time: 1071.3147716522217 and batch: 150, loss is 3.8864293479919434 and perplexity is 48.736554193813646
At time: 1073.060926914215 and batch: 200, loss is 3.935899562835693 and perplexity is 51.20819423210706
At time: 1074.807886838913 and batch: 250, loss is 3.910891237258911 and perplexity is 49.943443601940714
At time: 1076.5534110069275 and batch: 300, loss is 3.9047002172470093 and perplexity is 49.635197905330806
At time: 1078.2988946437836 and batch: 350, loss is 3.848037905693054 and perplexity is 46.90094880789175
At time: 1080.053097486496 and batch: 400, loss is 3.8638902950286864 and perplexity is 47.650365235241594
At time: 1081.8087453842163 and batch: 450, loss is 3.9059593534469603 and perplexity is 49.697734742740764
At time: 1083.5645160675049 and batch: 500, loss is 3.871534810066223 and perplexity is 48.01602503402258
At time: 1085.3203339576721 and batch: 550, loss is 3.8838520145416258 and perplexity is 48.61110557334221
At time: 1087.0748455524445 and batch: 600, loss is 3.9179056739807128 and perplexity is 50.29500027131191
At time: 1088.8301289081573 and batch: 650, loss is 3.8845542287826538 and perplexity is 48.64525297193953
At time: 1090.5850355625153 and batch: 700, loss is 3.8580888414382937 and perplexity is 47.37472418524433
At time: 1092.3395869731903 and batch: 750, loss is 3.851088118553162 and perplexity is 47.04422508558477
At time: 1094.09334897995 and batch: 800, loss is 3.803258819580078 and perplexity is 44.84709520779714
At time: 1095.8454759120941 and batch: 850, loss is 3.828594365119934 and perplexity is 45.997836607892644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.333529154459636 and perplexity of 76.21277918410011
Finished 34 epochs...
Completing Train Step...
At time: 1100.2950427532196 and batch: 50, loss is 3.9406424713134767 and perplexity is 51.45164689112691
At time: 1102.0185406208038 and batch: 100, loss is 3.8994646739959715 and perplexity is 49.37600976732533
At time: 1103.7477612495422 and batch: 150, loss is 3.8860028743743897 and perplexity is 48.71577377070558
At time: 1105.4707000255585 and batch: 200, loss is 3.935504379272461 and perplexity is 51.187961593510146
At time: 1107.203578710556 and batch: 250, loss is 3.9105142307281495 and perplexity is 49.92461814641688
At time: 1108.956399679184 and batch: 300, loss is 3.9043305206298826 and perplexity is 49.61685133211678
At time: 1110.7081413269043 and batch: 350, loss is 3.847732496261597 and perplexity is 46.88662700290001
At time: 1112.4589099884033 and batch: 400, loss is 3.8636218452453615 and perplexity is 47.63757522183381
At time: 1114.2106401920319 and batch: 450, loss is 3.905741786956787 and perplexity is 49.68692335716349
At time: 1115.9737112522125 and batch: 500, loss is 3.8713413953781126 and perplexity is 48.00673892757996
At time: 1117.730971813202 and batch: 550, loss is 3.883722176551819 and perplexity is 48.60479441483521
At time: 1119.4770114421844 and batch: 600, loss is 3.9178251695632933 and perplexity is 50.29095146459156
At time: 1121.2235703468323 and batch: 650, loss is 3.884533157348633 and perplexity is 48.64422795750041
At time: 1122.9704768657684 and batch: 700, loss is 3.8581141328811643 and perplexity is 47.37592237552649
At time: 1124.7185921669006 and batch: 750, loss is 3.8511715173721313 and perplexity is 47.04814868200562
At time: 1126.4646334648132 and batch: 800, loss is 3.8033796405792235 and perplexity is 44.852514005994635
At time: 1128.214391708374 and batch: 850, loss is 3.8287121629714966 and perplexity is 46.00325537337478
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.333518664042155 and perplexity of 76.21197968442267
Finished 35 epochs...
Completing Train Step...
At time: 1132.653166770935 and batch: 50, loss is 3.9402942848205567 and perplexity is 51.433735241119194
At time: 1134.3772637844086 and batch: 100, loss is 3.8990929412841795 and perplexity is 49.35765850041148
At time: 1136.1063342094421 and batch: 150, loss is 3.885611205101013 and perplexity is 48.69669703512027
At time: 1137.8427517414093 and batch: 200, loss is 3.9351420783996582 and perplexity is 51.16941950945727
At time: 1139.5824573040009 and batch: 250, loss is 3.910166664123535 and perplexity is 49.90726903156218
At time: 1141.3265044689178 and batch: 300, loss is 3.903986482620239 and perplexity is 49.59978418538164
At time: 1143.119645357132 and batch: 350, loss is 3.8474477100372315 and perplexity is 46.873276238569744
At time: 1144.8723306655884 and batch: 400, loss is 3.8633711242675783 and perplexity is 47.62563297954287
At time: 1146.6197979450226 and batch: 450, loss is 3.9055385541915895 and perplexity is 49.676826372389264
At time: 1148.368216753006 and batch: 500, loss is 3.87115779876709 and perplexity is 47.99792586205585
At time: 1150.1156387329102 and batch: 550, loss is 3.8835922861099244 and perplexity is 48.598481526611266
At time: 1151.863686323166 and batch: 600, loss is 3.9177405309677122 and perplexity is 50.28669508921852
At time: 1153.6109030246735 and batch: 650, loss is 3.884500012397766 and perplexity is 48.64261567367448
At time: 1155.3576729297638 and batch: 700, loss is 3.8581205892562864 and perplexity is 47.376228253240534
At time: 1157.1037244796753 and batch: 750, loss is 3.851230812072754 and perplexity is 47.0509384706056
At time: 1158.8517470359802 and batch: 800, loss is 3.803469195365906 and perplexity is 44.8565309431839
At time: 1160.598090171814 and batch: 850, loss is 3.8288023853302002 and perplexity is 46.00740608282319
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.333511034647624 and perplexity of 76.2113982353797
Finished 36 epochs...
Completing Train Step...
At time: 1165.0472118854523 and batch: 50, loss is 3.939956340789795 and perplexity is 51.41635645400876
At time: 1166.7638981342316 and batch: 100, loss is 3.8987425136566163 and perplexity is 49.34036524343558
At time: 1168.4977309703827 and batch: 150, loss is 3.8852452516555784 and perplexity is 48.67887957143895
At time: 1170.2457270622253 and batch: 200, loss is 3.9348044776916504 and perplexity is 51.15214759287221
At time: 1171.9940774440765 and batch: 250, loss is 3.90984100818634 and perplexity is 49.891019079183216
At time: 1173.7415895462036 and batch: 300, loss is 3.903661680221558 and perplexity is 49.583676672525165
At time: 1175.4889016151428 and batch: 350, loss is 3.8471781539916994 and perplexity is 46.86064296634969
At time: 1177.2358798980713 and batch: 400, loss is 3.863133053779602 and perplexity is 47.614296071404425
At time: 1178.9833135604858 and batch: 450, loss is 3.905345792770386 and perplexity is 49.6672515195976
At time: 1180.7296690940857 and batch: 500, loss is 3.8709809398651123 and perplexity is 47.98943775221169
At time: 1182.4755520820618 and batch: 550, loss is 3.883462052345276 and perplexity is 48.59215277552334
At time: 1184.223039150238 and batch: 600, loss is 3.9176519393920897 and perplexity is 50.282240308998624
At time: 1185.9697210788727 and batch: 650, loss is 3.8844572734832763 and perplexity is 48.64053678550767
At time: 1187.7543511390686 and batch: 700, loss is 3.858112487792969 and perplexity is 47.37584443801995
At time: 1189.5058386325836 and batch: 750, loss is 3.851272120475769 and perplexity is 47.05288210987822
At time: 1191.257614850998 and batch: 800, loss is 3.803535623550415 and perplexity is 44.859510780069314
At time: 1193.0082168579102 and batch: 850, loss is 3.8288724517822263 and perplexity is 46.01062977146923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.333505312601726 and perplexity of 76.21096215150867
Finished 37 epochs...
Completing Train Step...
At time: 1197.378746509552 and batch: 50, loss is 3.9396275758743284 and perplexity is 51.39945533832473
At time: 1199.1097085475922 and batch: 100, loss is 3.8984089946746825 and perplexity is 49.32391203893187
At time: 1200.830506324768 and batch: 150, loss is 3.884899516105652 and perplexity is 48.66205246127322
At time: 1202.5739738941193 and batch: 200, loss is 3.9344858932495117 and perplexity is 51.13585391006187
At time: 1204.3183307647705 and batch: 250, loss is 3.9095321035385133 and perplexity is 49.8756098916122
At time: 1206.0687959194183 and batch: 300, loss is 3.9033516359329226 and perplexity is 49.56830591969351
At time: 1207.8251271247864 and batch: 350, loss is 3.846920008659363 and perplexity is 46.84854767133678
At time: 1209.5814099311829 and batch: 400, loss is 3.862904453277588 and perplexity is 47.6034126634431
At time: 1211.3368968963623 and batch: 450, loss is 3.9051605796813966 and perplexity is 49.65805334635939
At time: 1213.0932149887085 and batch: 500, loss is 3.8708091497421266 and perplexity is 47.98119434888614
At time: 1214.8480117321014 and batch: 550, loss is 3.8833311414718628 and perplexity is 48.585791950722125
At time: 1216.6024615764618 and batch: 600, loss is 3.917559976577759 and perplexity is 50.27761642528489
At time: 1218.3558285236359 and batch: 650, loss is 3.884407305717468 and perplexity is 48.63810638727808
At time: 1220.1106379032135 and batch: 700, loss is 3.8580931186676026 and perplexity is 47.374926818236474
At time: 1221.8643736839294 and batch: 750, loss is 3.851299514770508 and perplexity is 47.05417110805457
At time: 1223.6184878349304 and batch: 800, loss is 3.8035846281051637 and perplexity is 44.86170915428607
At time: 1225.3882048130035 and batch: 850, loss is 3.828926920890808 and perplexity is 46.013135997713505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.333499272664388 and perplexity of 76.21050184346294
Finished 38 epochs...
Completing Train Step...
At time: 1229.7997975349426 and batch: 50, loss is 3.93930655002594 and perplexity is 51.38295743283678
At time: 1231.5220346450806 and batch: 100, loss is 3.8980889987945555 and perplexity is 49.308131115337595
At time: 1233.2624316215515 and batch: 150, loss is 3.884569625854492 and perplexity is 48.64600197216033
At time: 1235.0187113285065 and batch: 200, loss is 3.9341825103759764 and perplexity is 51.12034252082585
At time: 1236.7686941623688 and batch: 250, loss is 3.909236259460449 and perplexity is 49.860856670220116
At time: 1238.5169875621796 and batch: 300, loss is 3.9030536794662476 and perplexity is 49.55353892247302
At time: 1240.2659437656403 and batch: 350, loss is 3.8466712427139282 and perplexity is 46.83689479756225
At time: 1242.0156965255737 and batch: 400, loss is 3.8626830911636354 and perplexity is 47.59287623761035
At time: 1243.7633066177368 and batch: 450, loss is 3.9049813461303713 and perplexity is 49.64915375469764
At time: 1245.510033607483 and batch: 500, loss is 3.8706409692764283 and perplexity is 47.973125527803845
At time: 1247.259696483612 and batch: 550, loss is 3.883199620246887 and perplexity is 48.57940230804437
At time: 1249.0093793869019 and batch: 600, loss is 3.9174650144577026 and perplexity is 50.27284218292744
At time: 1250.7584817409515 and batch: 650, loss is 3.8843514204025267 and perplexity is 48.63538830733555
At time: 1252.5086679458618 and batch: 700, loss is 3.8580646562576293 and perplexity is 47.37357843283616
At time: 1254.2591922283173 and batch: 750, loss is 3.851315989494324 and perplexity is 47.054946318913636
At time: 1256.008621454239 and batch: 800, loss is 3.8036199522018435 and perplexity is 44.863293881626824
At time: 1257.7596225738525 and batch: 850, loss is 3.82896924495697 and perplexity is 46.015083501938655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.333497047424316 and perplexity of 76.21033225698906
Finished 39 epochs...
Completing Train Step...
At time: 1262.1561377048492 and batch: 50, loss is 3.938992624282837 and perplexity is 51.36682953135601
At time: 1263.8714003562927 and batch: 100, loss is 3.8977801322937013 and perplexity is 49.29290383713546
At time: 1265.5926594734192 and batch: 150, loss is 3.8842528533935545 and perplexity is 48.63059469882961
At time: 1267.3240840435028 and batch: 200, loss is 3.9338913774490356 and perplexity is 51.105461872110055
At time: 1269.0748023986816 and batch: 250, loss is 3.908950996398926 and perplexity is 49.84663523811727
At time: 1270.8248543739319 and batch: 300, loss is 3.9027652311325074 and perplexity is 49.539247348029406
At time: 1272.5770597457886 and batch: 350, loss is 3.8464300298690794 and perplexity is 46.82559849938513
At time: 1274.3278815746307 and batch: 400, loss is 3.862467632293701 and perplexity is 47.582623034890666
At time: 1276.125895023346 and batch: 450, loss is 3.904806776046753 and perplexity is 49.64048725425297
At time: 1277.8769118785858 and batch: 500, loss is 3.870475759506226 and perplexity is 47.96520055341912
At time: 1279.6284387111664 and batch: 550, loss is 3.8830679082870483 and perplexity is 48.5730042411188
At time: 1281.3788661956787 and batch: 600, loss is 3.9173678493499757 and perplexity is 50.26795765410773
At time: 1283.1296005249023 and batch: 650, loss is 3.8842909717559815 and perplexity is 48.632448452794186
At time: 1284.8795018196106 and batch: 700, loss is 3.858028979301453 and perplexity is 47.371888317903746
At time: 1286.6298658847809 and batch: 750, loss is 3.8513234090805053 and perplexity is 47.05529544843831
At time: 1288.380853652954 and batch: 800, loss is 3.8036446619033812 and perplexity is 44.864402453924825
At time: 1290.1309463977814 and batch: 850, loss is 3.829001793861389 and perplexity is 46.016581266868556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.33349609375 and perplexity of 76.2102595771872
Finished 40 epochs...
Completing Train Step...
At time: 1294.5337479114532 and batch: 50, loss is 3.93868501663208 and perplexity is 51.351031131575944
At time: 1296.2436695098877 and batch: 100, loss is 3.8974807262420654 and perplexity is 49.278147452609566
At time: 1297.967264175415 and batch: 150, loss is 3.8839468336105347 and perplexity is 48.615715051641075
At time: 1299.7030379772186 and batch: 200, loss is 3.9336106061935423 and perplexity is 51.091114941614755
At time: 1301.4486513137817 and batch: 250, loss is 3.9086743640899657 and perplexity is 49.83284795540931
At time: 1303.1932964324951 and batch: 300, loss is 3.902484612464905 and perplexity is 49.52534766079164
At time: 1304.938406944275 and batch: 350, loss is 3.8461949253082275 and perplexity is 46.81459088163457
At time: 1306.6924448013306 and batch: 400, loss is 3.862256932258606 and perplexity is 47.57259843067661
At time: 1308.4477005004883 and batch: 450, loss is 3.9046360206604005 and perplexity is 49.63201159732578
At time: 1310.2020394802094 and batch: 500, loss is 3.8703128337860107 and perplexity is 47.957386425152215
At time: 1311.956791639328 and batch: 550, loss is 3.8829355525970457 and perplexity is 48.566575753059844
At time: 1313.710881471634 and batch: 600, loss is 3.9172687768936156 and perplexity is 50.26297773075743
At time: 1315.4656014442444 and batch: 650, loss is 3.884226670265198 and perplexity is 48.62932141439591
At time: 1317.221492767334 and batch: 700, loss is 3.8579872131347654 and perplexity is 47.36990981703745
At time: 1318.9771988391876 and batch: 750, loss is 3.8513235950469973 and perplexity is 47.05530419914734
At time: 1320.757426738739 and batch: 800, loss is 3.8036605405807493 and perplexity is 44.86511484695262
At time: 1322.5123190879822 and batch: 850, loss is 3.829026141166687 and perplexity is 46.017701660260656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.333495140075684 and perplexity of 76.21018689745465
Finished 41 epochs...
Completing Train Step...
At time: 1326.9495606422424 and batch: 50, loss is 3.938382773399353 and perplexity is 51.33551297516987
At time: 1328.6626181602478 and batch: 100, loss is 3.8971893978118897 and perplexity is 49.2637934182409
At time: 1330.3827693462372 and batch: 150, loss is 3.883649830818176 and perplexity is 48.60127819251809
At time: 1332.1279792785645 and batch: 200, loss is 3.93333824634552 and perplexity is 51.0772016681085
At time: 1333.878433227539 and batch: 250, loss is 3.9084052896499633 and perplexity is 49.81944101356555
At time: 1335.628629207611 and batch: 300, loss is 3.9022107887268067 and perplexity is 49.511788301486504
At time: 1337.3797090053558 and batch: 350, loss is 3.8459650230407716 and perplexity is 46.80382933813989
At time: 1339.1321187019348 and batch: 400, loss is 3.8620502281188966 and perplexity is 47.56276599388197
At time: 1340.8845419883728 and batch: 450, loss is 3.904468340873718 and perplexity is 49.62369000990902
At time: 1342.6364023685455 and batch: 500, loss is 3.870151677131653 and perplexity is 47.94965839593259
At time: 1344.3895945549011 and batch: 550, loss is 3.8828029155731203 and perplexity is 48.56013445417651
At time: 1346.143520116806 and batch: 600, loss is 3.9171678590774537 and perplexity is 50.25790555675171
At time: 1347.8990352153778 and batch: 650, loss is 3.8841590595245363 and perplexity is 48.626033661102184
At time: 1349.6511120796204 and batch: 700, loss is 3.85794050693512 and perplexity is 47.367697400239535
At time: 1351.4021377563477 and batch: 750, loss is 3.8513175773620607 and perplexity is 47.05502103600407
At time: 1353.1527037620544 and batch: 800, loss is 3.8036690855026247 and perplexity is 44.86549821749185
At time: 1354.9041199684143 and batch: 850, loss is 3.829043869972229 and perplexity is 46.01851750637684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.333494822184245 and perplexity of 76.21016267089256
Finished 42 epochs...
Completing Train Step...
At time: 1359.3111281394958 and batch: 50, loss is 3.938085775375366 and perplexity is 51.32026869312878
At time: 1361.0529255867004 and batch: 100, loss is 3.896904697418213 and perplexity is 49.249769993192764
At time: 1362.7691531181335 and batch: 150, loss is 3.8833608627319336 and perplexity is 48.58723600313986
At time: 1364.5355942249298 and batch: 200, loss is 3.9330734157562257 and perplexity is 51.06367665368905
At time: 1366.2724747657776 and batch: 250, loss is 3.9081423807144167 and perplexity is 49.806344758995756
At time: 1368.0139553546906 and batch: 300, loss is 3.901942925453186 and perplexity is 49.49852768788423
At time: 1369.7628145217896 and batch: 350, loss is 3.8457395935058596 and perplexity is 46.793279561820356
At time: 1371.5129544734955 and batch: 400, loss is 3.8618468379974367 and perplexity is 47.55309318084016
At time: 1373.2609026432037 and batch: 450, loss is 3.9043031692504884 and perplexity is 49.61549426135061
At time: 1375.0139191150665 and batch: 500, loss is 3.8699922227859496 and perplexity is 47.94201322407051
At time: 1376.7646293640137 and batch: 550, loss is 3.8826699304580687 and perplexity is 48.55367710848415
At time: 1378.514760017395 and batch: 600, loss is 3.9170655393600464 and perplexity is 50.252763445131855
At time: 1380.2648754119873 and batch: 650, loss is 3.8840888500213624 and perplexity is 48.62261977128267
At time: 1382.015280008316 and batch: 700, loss is 3.857889633178711 and perplexity is 47.365287688836396
At time: 1383.7655963897705 and batch: 750, loss is 3.8513060331344606 and perplexity is 47.054477825266986
At time: 1385.5164020061493 and batch: 800, loss is 3.8036713790893555 and perplexity is 44.86560112052124
At time: 1387.2657055854797 and batch: 850, loss is 3.8290559339523313 and perplexity is 46.019072676205155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3334957758585615 and perplexity of 76.210235350602
Annealing...
Finished 43 epochs...
Completing Train Step...
At time: 1391.6666026115417 and batch: 50, loss is 3.9382693195343017 and perplexity is 51.329689093185664
At time: 1393.4054358005524 and batch: 100, loss is 3.897742280960083 and perplexity is 49.2910380702984
At time: 1395.1312382221222 and batch: 150, loss is 3.884691581726074 and perplexity is 48.651934999506224
At time: 1396.8708441257477 and batch: 200, loss is 3.9344963884353636 and perplexity is 51.13639059316864
At time: 1398.6104884147644 and batch: 250, loss is 3.909349808692932 and perplexity is 49.86651865367681
At time: 1400.3575015068054 and batch: 300, loss is 3.9027678108215333 and perplexity is 49.539375144046986
At time: 1402.1080167293549 and batch: 350, loss is 3.8468512010574343 and perplexity is 46.84532424601703
At time: 1403.8587560653687 and batch: 400, loss is 3.8617520141601562 and perplexity is 47.548584227851684
At time: 1405.6100435256958 and batch: 450, loss is 3.903903431892395 and perplexity is 49.595665058254745
At time: 1407.3887493610382 and batch: 500, loss is 3.8696720361709596 and perplexity is 47.92666529037307
At time: 1409.140053987503 and batch: 550, loss is 3.8815891408920287 and perplexity is 48.50122914858173
At time: 1410.8921172618866 and batch: 600, loss is 3.9158531188964845 and perplexity is 50.19187288631912
At time: 1412.6420421600342 and batch: 650, loss is 3.882281656265259 and perplexity is 48.534828628122945
At time: 1414.393449306488 and batch: 700, loss is 3.8560916805267333 and perplexity is 47.280203655676566
At time: 1416.1436595916748 and batch: 750, loss is 3.8494065952301026 and perplexity is 46.96518559588461
At time: 1417.8940811157227 and batch: 800, loss is 3.8014543628692627 and perplexity is 44.766243534499274
At time: 1419.6448600292206 and batch: 850, loss is 3.826984152793884 and perplexity is 45.923829923642856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.333362897237142 and perplexity of 76.20010931237243
Finished 44 epochs...
Completing Train Step...
At time: 1424.0536625385284 and batch: 50, loss is 3.9381680297851562 and perplexity is 51.324490185156186
At time: 1425.7974758148193 and batch: 100, loss is 3.897496266365051 and perplexity is 49.278913247031745
At time: 1427.519728422165 and batch: 150, loss is 3.884370861053467 and perplexity is 48.636333820133636
At time: 1429.2455570697784 and batch: 200, loss is 3.9342614650726317 and perplexity is 51.12437887130481
At time: 1430.9724342823029 and batch: 250, loss is 3.9091243028640745 and perplexity is 49.8552747308883
At time: 1432.7116694450378 and batch: 300, loss is 3.902573628425598 and perplexity is 49.52975640341365
At time: 1434.4670240879059 and batch: 350, loss is 3.8466620445251465 and perplexity is 46.83646398494331
At time: 1436.2254283428192 and batch: 400, loss is 3.8616374731063843 and perplexity is 47.54313827480742
At time: 1437.9809799194336 and batch: 450, loss is 3.9038162517547605 and perplexity is 49.59134148981629
At time: 1439.7364945411682 and batch: 500, loss is 3.8695698404312133 and perplexity is 47.92176763962394
At time: 1441.4916543960571 and batch: 550, loss is 3.8815854501724245 and perplexity is 48.50105014447481
At time: 1443.2462272644043 and batch: 600, loss is 3.9158709239959717 and perplexity is 50.19276656556531
At time: 1445.0006637573242 and batch: 650, loss is 3.8823502588272096 and perplexity is 48.5381583559233
At time: 1446.755420923233 and batch: 700, loss is 3.8561675643920896 and perplexity is 47.28379159641647
At time: 1448.5110449790955 and batch: 750, loss is 3.8495338106155397 and perplexity is 46.971160670124945
At time: 1450.2659900188446 and batch: 800, loss is 3.8015795850753786 and perplexity is 44.77184961326964
At time: 1452.0665588378906 and batch: 850, loss is 3.8271134948730468 and perplexity is 45.9297701914433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.333272298177083 and perplexity of 76.1932059668154
Finished 45 epochs...
Completing Train Step...
At time: 1456.4799089431763 and batch: 50, loss is 3.9380851125717165 and perplexity is 51.32023467787866
At time: 1458.1889026165009 and batch: 100, loss is 3.897311406135559 and perplexity is 49.26980437777955
At time: 1459.9135611057281 and batch: 150, loss is 3.884126935005188 and perplexity is 48.624471598233406
At time: 1461.6519927978516 and batch: 200, loss is 3.9340668058395387 and perplexity is 51.11442800746655
At time: 1463.391891002655 and batch: 250, loss is 3.908947401046753 and perplexity is 49.84645602223113
At time: 1465.1405034065247 and batch: 300, loss is 3.902423357963562 and perplexity is 49.52231410322733
At time: 1466.8898468017578 and batch: 350, loss is 3.8465189027786253 and perplexity is 46.82976021149399
At time: 1468.6415991783142 and batch: 400, loss is 3.861546025276184 and perplexity is 47.538790756759845
At time: 1470.3897886276245 and batch: 450, loss is 3.903754267692566 and perplexity is 49.58826771228467
At time: 1472.1450753211975 and batch: 500, loss is 3.8695084857940674 and perplexity is 47.91882750715532
At time: 1473.8938581943512 and batch: 550, loss is 3.8815862131118775 and perplexity is 48.501087147853596
At time: 1475.6425168514252 and batch: 600, loss is 3.9158819198608397 and perplexity is 50.1933184814782
At time: 1477.3926010131836 and batch: 650, loss is 3.8824090433120726 and perplexity is 48.5410117304247
At time: 1479.1419467926025 and batch: 700, loss is 3.8562325048446655 and perplexity is 47.28686232694848
At time: 1480.8916189670563 and batch: 750, loss is 3.8496427965164184 and perplexity is 46.97628014335603
At time: 1482.6408820152283 and batch: 800, loss is 3.801686577796936 and perplexity is 44.77664013157965
At time: 1484.3887872695923 and batch: 850, loss is 3.827228569984436 and perplexity is 45.93505586898329
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.333208401997884 and perplexity of 76.1883376676077
Finished 46 epochs...
Completing Train Step...
At time: 1488.8033723831177 and batch: 50, loss is 3.9380111122131347 and perplexity is 51.31643710262269
At time: 1490.5147955417633 and batch: 100, loss is 3.897158327102661 and perplexity is 49.26226278101922
At time: 1492.2355766296387 and batch: 150, loss is 3.883925271034241 and perplexity is 48.61466678287792
At time: 1493.9717197418213 and batch: 200, loss is 3.9338996171951295 and perplexity is 51.10588296987476
At time: 1495.7446835041046 and batch: 250, loss is 3.908798155784607 and perplexity is 49.839017229951104
At time: 1497.4937734603882 and batch: 300, loss is 3.9022983360290526 and perplexity is 49.5161231147295
At time: 1499.2428212165833 and batch: 350, loss is 3.846401710510254 and perplexity is 46.82427244723557
At time: 1500.9938519001007 and batch: 400, loss is 3.8614677715301515 and perplexity is 47.53507081385291
At time: 1502.7433137893677 and batch: 450, loss is 3.903704218864441 and perplexity is 49.58578593970234
At time: 1504.4927682876587 and batch: 500, loss is 3.8694641971588135 and perplexity is 47.91670529467736
At time: 1506.2417228221893 and batch: 550, loss is 3.8815876960754396 and perplexity is 48.50115907325189
At time: 1507.9911172389984 and batch: 600, loss is 3.9158899450302123 and perplexity is 50.193721292976704
At time: 1509.7401258945465 and batch: 650, loss is 3.8824584341049193 and perplexity is 48.54340926868732
At time: 1511.488374710083 and batch: 700, loss is 3.856287980079651 and perplexity is 47.28948564951183
At time: 1513.233909368515 and batch: 750, loss is 3.849736633300781 and perplexity is 46.980688453253585
At time: 1514.982610464096 and batch: 800, loss is 3.8017804336547854 and perplexity is 44.78084287877394
At time: 1516.73086643219 and batch: 850, loss is 3.8273304319381714 and perplexity is 45.939735141834944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.33316167195638 and perplexity of 76.1847774666112
Finished 47 epochs...
Completing Train Step...
At time: 1521.1587407588959 and batch: 50, loss is 3.9379436683654787 and perplexity is 51.31297624136471
At time: 1522.88520860672 and batch: 100, loss is 3.8970254707336425 and perplexity is 49.25571841039678
At time: 1524.6124691963196 and batch: 150, loss is 3.883751802444458 and perplexity is 48.60623439658656
At time: 1526.3396308422089 and batch: 200, loss is 3.9337533378601073 and perplexity is 51.09840778204428
At time: 1528.0692658424377 and batch: 250, loss is 3.908667492866516 and perplexity is 49.83250554395226
At time: 1529.797015428543 and batch: 300, loss is 3.9021904563903806 and perplexity is 49.51078162138381
At time: 1531.531797170639 and batch: 350, loss is 3.8463014125823975 and perplexity is 46.81957630524632
At time: 1533.2753999233246 and batch: 400, loss is 3.8613985681533816 and perplexity is 47.531781340260245
At time: 1535.0195033550262 and batch: 450, loss is 3.903661584854126 and perplexity is 49.58367194385749
At time: 1536.7645111083984 and batch: 500, loss is 3.86942910194397 and perplexity is 47.91502367711898
At time: 1538.5109176635742 and batch: 550, loss is 3.88158896446228 and perplexity is 48.50122059152282
At time: 1540.310353755951 and batch: 600, loss is 3.915897054672241 and perplexity is 50.194078153635765
At time: 1542.0632359981537 and batch: 650, loss is 3.8825000143051147 and perplexity is 48.545427755327125
At time: 1543.8185360431671 and batch: 700, loss is 3.856335883140564 and perplexity is 47.29175101488198
At time: 1545.5732555389404 and batch: 750, loss is 3.8498182487487793 and perplexity is 46.98452295966431
At time: 1547.3282339572906 and batch: 800, loss is 3.8018633413314817 and perplexity is 44.78455570832643
At time: 1549.0838770866394 and batch: 850, loss is 3.8274201917648316 and perplexity is 45.94385886956787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.333127339680989 and perplexity of 76.18216191474981
Finished 48 epochs...
Completing Train Step...
At time: 1553.4884238243103 and batch: 50, loss is 3.937880778312683 and perplexity is 51.30974926705314
At time: 1555.2388837337494 and batch: 100, loss is 3.896907081604004 and perplexity is 49.249887413934566
At time: 1556.9622178077698 and batch: 150, loss is 3.883599281311035 and perplexity is 48.59882148395226
At time: 1558.689645767212 and batch: 200, loss is 3.9336233282089235 and perplexity is 51.09176492769944
At time: 1560.4401457309723 and batch: 250, loss is 3.908551073074341 and perplexity is 49.82670439170429
At time: 1562.1891498565674 and batch: 300, loss is 3.902094798088074 and perplexity is 49.50604573058528
At time: 1563.9376759529114 and batch: 350, loss is 3.8462134170532227 and perplexity is 46.81545657311526
At time: 1565.6881482601166 and batch: 400, loss is 3.8613359546661377 and perplexity is 47.52880530284663
At time: 1567.43834400177 and batch: 450, loss is 3.9036235570907594 and perplexity is 49.58178642356525
At time: 1569.1888732910156 and batch: 500, loss is 3.8693992614746096 and perplexity is 47.913593891655886
At time: 1570.9389951229095 and batch: 550, loss is 3.881589550971985 and perplexity is 48.50124903796773
At time: 1572.6899166107178 and batch: 600, loss is 3.9159034395217898 and perplexity is 50.19439863629612
At time: 1574.4398574829102 and batch: 650, loss is 3.8825355529785157 and perplexity is 48.54715302608597
At time: 1576.1900551319122 and batch: 700, loss is 3.8563774824142456 and perplexity is 47.29371835829509
At time: 1577.9396669864655 and batch: 750, loss is 3.8498896408081054 and perplexity is 46.98787740125368
At time: 1579.6889939308167 and batch: 800, loss is 3.8019373750686647 and perplexity is 44.78787139908857
At time: 1581.4370839595795 and batch: 850, loss is 3.8274994468688965 and perplexity is 45.94750029918276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.333101908365886 and perplexity of 76.18022452682015
Finished 49 epochs...
Completing Train Step...
At time: 1585.8164944648743 and batch: 50, loss is 3.937821617126465 and perplexity is 51.30671381121343
At time: 1587.5565996170044 and batch: 100, loss is 3.896799535751343 and perplexity is 49.24459107760378
At time: 1589.2760961055756 and batch: 150, loss is 3.8834629011154176 and perplexity is 48.59219401910924
At time: 1590.9977202415466 and batch: 200, loss is 3.9335063076019288 and perplexity is 51.085786488162306
At time: 1592.7359533309937 and batch: 250, loss is 3.9084459114074708 and perplexity is 49.821464807922325
At time: 1594.4796092510223 and batch: 300, loss is 3.9020085525512695 and perplexity is 49.50177623921112
At time: 1596.229118347168 and batch: 350, loss is 3.8461345481872558 and perplexity is 46.81176443674483
At time: 1597.9776690006256 and batch: 400, loss is 3.8612783432006834 and perplexity is 47.5260671775963
At time: 1599.7246611118317 and batch: 450, loss is 3.9035889673233033 and perplexity is 49.58007143076357
At time: 1601.4733142852783 and batch: 500, loss is 3.8693729305267333 and perplexity is 47.912332297922106
At time: 1603.221633195877 and batch: 550, loss is 3.8815891790390014 and perplexity is 48.50123099875683
At time: 1604.9694285392761 and batch: 600, loss is 3.9159091758728026 and perplexity is 50.194686569811424
At time: 1606.719483613968 and batch: 650, loss is 3.882565722465515 and perplexity is 48.548617690882025
At time: 1608.4696757793427 and batch: 700, loss is 3.856413803100586 and perplexity is 47.29543612980057
At time: 1610.2183725833893 and batch: 750, loss is 3.8499524879455564 and perplexity is 46.99083054764067
At time: 1611.96888422966 and batch: 800, loss is 3.8020036315917967 and perplexity is 44.79083898603587
At time: 1613.7190544605255 and batch: 850, loss is 3.827569718360901 and perplexity is 45.95072921203159
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.33308219909668 and perplexity of 76.178723085063
Finished Training.
Improved accuracyfrom -10000000 to -76.178723085063
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f37e976d898>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 7.4726475107090335, 'batch_size': 50, 'dropout': 0.8525503454773354, 'wordvec_source': 'glove', 'data': 'wikitext', 'lr': 21.82125557023854, 'num_layers': 1, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.055586814880371 and batch: 50, loss is 7.380263719558716 and perplexity is 1604.012721586587
At time: 3.638779401779175 and batch: 100, loss is 6.716642055511475 and perplexity is 826.0390558474621
At time: 5.223121881484985 and batch: 150, loss is 6.582112579345703 and perplexity is 722.0631347672033
At time: 6.808654308319092 and batch: 200, loss is 6.603258752822876 and perplexity is 737.4945901689065
At time: 8.396186113357544 and batch: 250, loss is 6.639359169006347 and perplexity is 764.6048535004935
At time: 10.01056694984436 and batch: 300, loss is 6.587770471572876 and perplexity is 726.1600692426299
At time: 11.59834098815918 and batch: 350, loss is 6.587019739151001 and perplexity is 725.615121915586
At time: 13.18572449684143 and batch: 400, loss is 6.6235016918182374 and perplexity is 752.5757769495164
At time: 14.773545742034912 and batch: 450, loss is 6.620344820022583 and perplexity is 750.2037377868838
At time: 16.361114501953125 and batch: 500, loss is 6.617504014968872 and perplexity is 748.0755794899051
At time: 17.94925284385681 and batch: 550, loss is 6.5963719367980955 and perplexity is 732.4330495616033
At time: 19.537591218948364 and batch: 600, loss is 6.61674506187439 and perplexity is 747.5080406088822
At time: 21.138219118118286 and batch: 650, loss is 6.640803337097168 and perplexity is 765.7098691541252
At time: 22.744401216506958 and batch: 700, loss is 6.61985445022583 and perplexity is 749.8359507156673
At time: 24.38795804977417 and batch: 750, loss is 6.5994554710388185 and perplexity is 734.6950175846218
At time: 26.13444948196411 and batch: 800, loss is 6.644839544296264 and perplexity is 768.8066783179776
At time: 27.895512342453003 and batch: 850, loss is 6.64379430770874 and perplexity is 768.003513270936
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.11587651570638 and perplexity of 452.9929288263655
Finished 1 epochs...
Completing Train Step...
At time: 32.408862829208374 and batch: 50, loss is 6.594359474182129 and perplexity is 730.9605376152574
At time: 34.160358905792236 and batch: 100, loss is 6.709429836273193 and perplexity is 820.1029131896757
At time: 35.91151738166809 and batch: 150, loss is 6.821946611404419 and perplexity is 917.7698135438425
At time: 37.663599729537964 and batch: 200, loss is 7.027589750289917 and perplexity is 1127.3102339457641
At time: 39.41472315788269 and batch: 250, loss is 7.329231662750244 and perplexity is 1524.2102144883079
At time: 41.1670081615448 and batch: 300, loss is 7.228466262817383 and perplexity is 1378.107228065648
At time: 42.919938802719116 and batch: 350, loss is 7.181517038345337 and perplexity is 1314.9015019486226
At time: 44.67198586463928 and batch: 400, loss is 7.267735214233398 and perplexity is 1433.3006554543
At time: 46.423664569854736 and batch: 450, loss is 7.2681930637359615 and perplexity is 1433.9570416980148
At time: 48.17548084259033 and batch: 500, loss is 7.202648983001709 and perplexity is 1342.9835973407548
At time: 49.92667865753174 and batch: 550, loss is 7.081142883300782 and perplexity is 1189.327004038095
At time: 51.67640423774719 and batch: 600, loss is 7.02931303024292 and perplexity is 1129.25457991778
At time: 53.427435636520386 and batch: 650, loss is 7.006146583557129 and perplexity is 1103.3944639458105
At time: 55.176695585250854 and batch: 700, loss is 6.842267208099365 and perplexity is 936.6102196260932
At time: 56.926369190216064 and batch: 750, loss is 6.864062929153443 and perplexity is 957.248409813829
At time: 58.67549991607666 and batch: 800, loss is 6.90671968460083 and perplexity is 998.9649416614974
At time: 60.42412877082825 and batch: 850, loss is 6.964404792785644 and perplexity is 1058.2848311624673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.564197540283203 and perplexity of 709.2425292213129
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 64.9274320602417 and batch: 50, loss is 6.805689706802368 and perplexity is 902.9703399941184
At time: 66.6780047416687 and batch: 100, loss is 6.594563245773315 and perplexity is 731.1095017839192
At time: 68.42853999137878 and batch: 150, loss is 6.478002853393555 and perplexity is 650.6701640670659
At time: 70.18107318878174 and batch: 200, loss is 6.489570980072021 and perplexity is 658.2409041639751
At time: 71.93414044380188 and batch: 250, loss is 6.525073881149292 and perplexity is 682.0301609614799
At time: 73.6853895187378 and batch: 300, loss is 6.478674650192261 and perplexity is 651.1074290604657
At time: 75.43280482292175 and batch: 350, loss is 6.445502967834472 and perplexity is 629.86339833448
At time: 77.18137001991272 and batch: 400, loss is 6.468740730285645 and perplexity is 644.6714004330317
At time: 78.93197870254517 and batch: 450, loss is 6.461385040283203 and perplexity is 639.9467951265236
At time: 80.68283772468567 and batch: 500, loss is 6.460201435089111 and perplexity is 639.189798856602
At time: 82.43399405479431 and batch: 550, loss is 6.430313177108765 and perplexity is 620.368202713158
At time: 84.1857590675354 and batch: 600, loss is 6.446226615905761 and perplexity is 630.319362726839
At time: 85.9363181591034 and batch: 650, loss is 6.478720102310181 and perplexity is 651.13702394468
At time: 87.68597793579102 and batch: 700, loss is 6.468566198348999 and perplexity is 644.5588945032383
At time: 89.43437671661377 and batch: 750, loss is 6.4484058856964115 and perplexity is 631.6944965220265
At time: 91.18346977233887 and batch: 800, loss is 6.416960563659668 and perplexity is 612.139724009745
At time: 92.93276238441467 and batch: 850, loss is 6.39626953125 and perplexity is 599.6040563206461
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.946205774943034 and perplexity of 382.30005132021046
Finished 3 epochs...
Completing Train Step...
At time: 97.41936588287354 and batch: 50, loss is 6.43601824760437 and perplexity is 623.9175620886037
At time: 99.17062950134277 and batch: 100, loss is 6.354186859130859 and perplexity is 574.8946801883914
At time: 100.92089486122131 and batch: 150, loss is 6.304577178955078 and perplexity is 547.0702264712231
At time: 102.6728196144104 and batch: 200, loss is 6.344926490783691 and perplexity is 569.5955176563394
At time: 104.46192622184753 and batch: 250, loss is 6.389317264556885 and perplexity is 595.4499061230754
At time: 106.21876573562622 and batch: 300, loss is 6.352569713592529 and perplexity is 573.9657431369621
At time: 107.97282481193542 and batch: 350, loss is 6.319654483795166 and perplexity is 555.3810661382776
At time: 109.72483325004578 and batch: 400, loss is 6.35335678100586 and perplexity is 574.4176706951507
At time: 111.47468400001526 and batch: 450, loss is 6.36252649307251 and perplexity is 579.7091388841749
At time: 113.22456192970276 and batch: 500, loss is 6.3512155723571775 and perplexity is 573.189038459104
At time: 114.97481632232666 and batch: 550, loss is 6.317082357406616 and perplexity is 553.954391422758
At time: 116.72538900375366 and batch: 600, loss is 6.320433588027954 and perplexity is 555.8139344805992
At time: 118.47495985031128 and batch: 650, loss is 6.346417093276978 and perplexity is 570.4451912605956
At time: 120.22466254234314 and batch: 700, loss is 6.324135723114014 and perplexity is 557.8754463919937
At time: 121.97581171989441 and batch: 750, loss is 6.322067461013794 and perplexity is 556.7228061415385
At time: 123.7316198348999 and batch: 800, loss is 6.325452270507813 and perplexity is 558.6103995511809
At time: 125.48795938491821 and batch: 850, loss is 6.3124597549438475 and perplexity is 551.3995899538314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.854922612508138 and perplexity of 348.94789480669505
Finished 4 epochs...
Completing Train Step...
At time: 129.9037425518036 and batch: 50, loss is 6.300390071868897 and perplexity is 544.7843737440069
At time: 131.60264205932617 and batch: 100, loss is 6.218860950469971 and perplexity is 502.13095412836833
At time: 133.3008120059967 and batch: 150, loss is 6.183155965805054 and perplexity is 484.5186703124889
At time: 135.0191786289215 and batch: 200, loss is 6.209650897979737 and perplexity is 497.52753309894473
At time: 136.74550700187683 and batch: 250, loss is 6.254288444519043 and perplexity is 520.2390640673079
At time: 138.46989846229553 and batch: 300, loss is 6.20706467628479 and perplexity is 496.2424790326251
At time: 140.19389462471008 and batch: 350, loss is 6.1801310062408445 and perplexity is 483.05523545823394
At time: 141.9196650981903 and batch: 400, loss is 6.198234119415283 and perplexity is 491.87967295452546
At time: 143.6722903251648 and batch: 450, loss is 6.208962593078613 and perplexity is 497.1852002876603
At time: 145.42651438713074 and batch: 500, loss is 6.209328231811523 and perplexity is 497.36702369312894
At time: 147.2092843055725 and batch: 550, loss is 6.170599765777588 and perplexity is 478.4729917745195
At time: 148.9629409313202 and batch: 600, loss is 6.168810434341431 and perplexity is 477.61761051749505
At time: 150.7164421081543 and batch: 650, loss is 6.198191318511963 and perplexity is 491.8586205107332
At time: 152.47226762771606 and batch: 700, loss is 6.1642653465271 and perplexity is 475.4517223405455
At time: 154.2255735397339 and batch: 750, loss is 6.155752325057984 and perplexity is 471.4213711940994
At time: 155.9806227684021 and batch: 800, loss is 6.16636510848999 and perplexity is 476.45110664969576
At time: 157.73505449295044 and batch: 850, loss is 6.160250806808472 and perplexity is 473.54682871084293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.747546513875325 and perplexity of 313.4207427374077
Finished 5 epochs...
Completing Train Step...
At time: 162.1820900440216 and batch: 50, loss is 6.179085645675659 and perplexity is 482.55053240857296
At time: 163.87690663337708 and batch: 100, loss is 6.120869064331055 and perplexity is 455.2601730042106
At time: 165.588609457016 and batch: 150, loss is 6.101298818588257 and perplexity is 446.43723467024796
At time: 167.31200408935547 and batch: 200, loss is 6.129614334106446 and perplexity is 459.25900598595194
At time: 169.03407907485962 and batch: 250, loss is 6.182773246765136 and perplexity is 484.33327127229916
At time: 170.75752449035645 and batch: 300, loss is 6.13523871421814 and perplexity is 461.84933086274646
At time: 172.4794991016388 and batch: 350, loss is 6.116995439529419 and perplexity is 453.50007708331896
At time: 174.20211577415466 and batch: 400, loss is 6.140726146697998 and perplexity is 464.39066420224657
At time: 175.93916487693787 and batch: 450, loss is 6.153602857589721 and perplexity is 470.4091545459731
At time: 177.67879939079285 and batch: 500, loss is 6.161252841949463 and perplexity is 474.02157709167295
At time: 179.418719291687 and batch: 550, loss is 6.126056680679321 and perplexity is 457.62802456295617
At time: 181.16854166984558 and batch: 600, loss is 6.1267918109893795 and perplexity is 457.96456447973526
At time: 182.91706156730652 and batch: 650, loss is 6.156441602706909 and perplexity is 471.7464234212373
At time: 184.66555070877075 and batch: 700, loss is 6.117404088973999 and perplexity is 453.68543750907406
At time: 186.41459465026855 and batch: 750, loss is 6.112673387527466 and perplexity is 451.5442557925204
At time: 188.1660385131836 and batch: 800, loss is 6.133998899459839 and perplexity is 461.2770780632673
At time: 189.91632533073425 and batch: 850, loss is 6.128544521331787 and perplexity is 458.76794755159904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.728076934814453 and perplexity of 307.3775924971628
Finished 6 epochs...
Completing Train Step...
At time: 194.3334834575653 and batch: 50, loss is 6.142790212631225 and perplexity is 465.3501870705076
At time: 196.05927801132202 and batch: 100, loss is 6.092413005828857 and perplexity is 442.4878497278518
At time: 197.7772753238678 and batch: 150, loss is 6.075236597061157 and perplexity is 434.9523988026687
At time: 199.49805784225464 and batch: 200, loss is 6.101878757476807 and perplexity is 446.696216073365
At time: 201.2178554534912 and batch: 250, loss is 6.155130977630615 and perplexity is 471.1285457204998
At time: 202.9499762058258 and batch: 300, loss is 6.110875120162964 and perplexity is 450.73298815038345
At time: 204.6875340938568 and batch: 350, loss is 6.090030527114868 and perplexity is 441.43488667377727
At time: 206.42836618423462 and batch: 400, loss is 6.111999082565307 and perplexity is 451.23987989282796
At time: 208.1701316833496 and batch: 450, loss is 6.126954593658447 and perplexity is 458.03911924182665
At time: 209.9200508594513 and batch: 500, loss is 6.134657535552979 and perplexity is 461.5809918691321
At time: 211.66742420196533 and batch: 550, loss is 6.101927480697632 and perplexity is 446.7179810819688
At time: 213.41626954078674 and batch: 600, loss is 6.1027507495880124 and perplexity is 447.0859015265148
At time: 215.16352486610413 and batch: 650, loss is 6.133336591720581 and perplexity is 460.97167183214276
At time: 216.91048908233643 and batch: 700, loss is 6.090353345870971 and perplexity is 441.5774131386643
At time: 218.6596167087555 and batch: 750, loss is 6.085221900939941 and perplexity is 439.31728677675517
At time: 220.40921235084534 and batch: 800, loss is 6.110935821533203 and perplexity is 450.76034909079084
At time: 222.1588282585144 and batch: 850, loss is 6.108655691146851 and perplexity is 449.7337275820981
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.71156120300293 and perplexity of 302.34271837606366
Finished 7 epochs...
Completing Train Step...
At time: 226.56835079193115 and batch: 50, loss is 6.118877000808716 and perplexity is 454.3541685292375
At time: 228.28961992263794 and batch: 100, loss is 6.073768367767334 and perplexity is 434.31425753281763
At time: 230.0088243484497 and batch: 150, loss is 6.056984004974365 and perplexity is 427.0854050152955
At time: 231.7299964427948 and batch: 200, loss is 6.081946582794189 and perplexity is 437.88073675904354
At time: 233.46975255012512 and batch: 250, loss is 6.138502216339111 and perplexity is 463.3590392619454
At time: 235.23842930793762 and batch: 300, loss is 6.08805172920227 and perplexity is 440.5622399220125
At time: 236.97861003875732 and batch: 350, loss is 6.048461894989014 and perplexity is 423.4612010872272
At time: 238.7188229560852 and batch: 400, loss is 6.071616115570069 and perplexity is 433.3805089094438
At time: 240.4588177204132 and batch: 450, loss is 6.086071462631225 and perplexity is 439.69067249856454
At time: 242.1959674358368 and batch: 500, loss is 6.0969500160217285 and perplexity is 444.4999826946237
At time: 243.93403029441833 and batch: 550, loss is 6.063815937042237 and perplexity is 430.0132133955022
At time: 245.6836211681366 and batch: 600, loss is 6.061527729034424 and perplexity is 429.0303786111922
At time: 247.4326844215393 and batch: 650, loss is 6.0943071651458744 and perplexity is 443.3267865002019
At time: 249.18406414985657 and batch: 700, loss is 6.052060079574585 and perplexity is 424.9876372061352
At time: 250.93849229812622 and batch: 750, loss is 6.050222854614258 and perplexity is 424.2075561228719
At time: 252.6957037448883 and batch: 800, loss is 6.072008314132691 and perplexity is 433.55051345769704
At time: 254.44435453414917 and batch: 850, loss is 6.067452182769776 and perplexity is 431.57969343263153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.666188557942708 and perplexity of 288.9311885743131
Finished 8 epochs...
Completing Train Step...
At time: 258.8374319076538 and batch: 50, loss is 6.069813776016235 and perplexity is 432.60011355633685
At time: 260.56996393203735 and batch: 100, loss is 6.02460940361023 and perplexity is 413.48010664061803
At time: 262.2878403663635 and batch: 150, loss is 6.010645818710327 and perplexity is 407.7465656215165
At time: 264.013925075531 and batch: 200, loss is 6.041391344070434 and perplexity is 420.47765718205187
At time: 265.73811054229736 and batch: 250, loss is 6.096807622909546 and perplexity is 444.43669346480505
At time: 267.46379947662354 and batch: 300, loss is 6.049464426040649 and perplexity is 423.885946965371
At time: 269.2012712955475 and batch: 350, loss is 6.025883588790894 and perplexity is 414.0072926599796
At time: 270.9564106464386 and batch: 400, loss is 6.051056251525879 and perplexity is 424.56123674773045
At time: 272.71149587631226 and batch: 450, loss is 6.0688285064697265 and perplexity is 432.1740957442851
At time: 274.4664969444275 and batch: 500, loss is 6.07493221282959 and perplexity is 434.82002629806436
At time: 276.2217712402344 and batch: 550, loss is 6.042815389633179 and perplexity is 421.0768630709779
At time: 277.97685408592224 and batch: 600, loss is 6.044838390350342 and perplexity is 421.92956408352217
At time: 279.7703263759613 and batch: 650, loss is 6.073687562942505 and perplexity is 434.2791642631886
At time: 281.5145151615143 and batch: 700, loss is 6.027679290771484 and perplexity is 414.7513942678044
At time: 283.2602505683899 and batch: 750, loss is 6.026082782745362 and perplexity is 414.0897686238671
At time: 285.0157582759857 and batch: 800, loss is 6.048648557662964 and perplexity is 423.5402528651147
At time: 286.77093720436096 and batch: 850, loss is 6.043393135070801 and perplexity is 421.3202085966199
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.6488081614176435 and perplexity of 283.95283812118225
Finished 9 epochs...
Completing Train Step...
At time: 291.2233917713165 and batch: 50, loss is 6.043467235565186 and perplexity is 421.3514297891099
At time: 292.93583941459656 and batch: 100, loss is 6.000851411819458 and perplexity is 403.77242380051894
At time: 294.651246547699 and batch: 150, loss is 5.982844362258911 and perplexity is 396.5667449665976
At time: 296.37153005599976 and batch: 200, loss is 6.011288366317749 and perplexity is 408.0086463923601
At time: 298.0912730693817 and batch: 250, loss is 6.065121717453003 and perplexity is 430.5750829852414
At time: 299.8112065792084 and batch: 300, loss is 6.02201169013977 and perplexity is 412.40739769629823
At time: 301.5307559967041 and batch: 350, loss is 5.996063718795776 and perplexity is 401.84390564418624
At time: 303.26001954078674 and batch: 400, loss is 6.020318078994751 and perplexity is 411.7095310554403
At time: 305.0004472732544 and batch: 450, loss is 6.034545841217041 and perplexity is 417.6091057205337
At time: 306.7416067123413 and batch: 500, loss is 6.042859239578247 and perplexity is 421.09532767312584
At time: 308.4903998374939 and batch: 550, loss is 6.0105713272094725 and perplexity is 407.7161930991364
At time: 310.2425272464752 and batch: 600, loss is 6.011157484054565 and perplexity is 407.9552487917973
At time: 311.994092464447 and batch: 650, loss is 6.035311737060547 and perplexity is 417.9290733140991
At time: 313.74596118927 and batch: 700, loss is 5.982900476455688 and perplexity is 396.5889986153269
At time: 315.498553276062 and batch: 750, loss is 5.977370882034302 and perplexity is 394.4020742756707
At time: 317.2506847381592 and batch: 800, loss is 6.000579338073731 and perplexity is 403.662582867849
At time: 319.00150179862976 and batch: 850, loss is 5.997648305892945 and perplexity is 402.4811670769252
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.617303212483724 and perplexity of 275.1463705142229
Finished 10 epochs...
Completing Train Step...
At time: 323.4255735874176 and batch: 50, loss is 6.002545928955078 and perplexity is 404.4572031128236
At time: 325.140216588974 and batch: 100, loss is 5.955370140075684 and perplexity is 385.8196915817654
At time: 326.8599305152893 and batch: 150, loss is 5.9373070430755615 and perplexity is 378.9131575502733
At time: 328.58549880981445 and batch: 200, loss is 5.96582516670227 and perplexity is 389.87460691820456
At time: 330.32576179504395 and batch: 250, loss is 6.0180982494354245 and perplexity is 410.79661969723116
At time: 332.0668251514435 and batch: 300, loss is 5.9699231624603275 and perplexity is 391.4755895730766
At time: 333.80739188194275 and batch: 350, loss is 5.946221618652344 and perplexity is 382.30610841907605
At time: 335.54890179634094 and batch: 400, loss is 5.972866830825805 and perplexity is 392.62966165134003
At time: 337.28969287872314 and batch: 450, loss is 5.988277893066407 and perplexity is 398.72736718056893
At time: 339.0300874710083 and batch: 500, loss is 5.996203126907349 and perplexity is 401.8999298492427
At time: 340.7720742225647 and batch: 550, loss is 5.965993137359619 and perplexity is 389.94009991250874
At time: 342.5173034667969 and batch: 600, loss is 5.965214443206787 and perplexity is 389.636574029005
At time: 344.2668921947479 and batch: 650, loss is 5.998537769317627 and perplexity is 402.8393186118966
At time: 346.015976190567 and batch: 700, loss is 5.948857297897339 and perplexity is 383.3150737646908
At time: 347.76695370674133 and batch: 750, loss is 5.947977409362793 and perplexity is 382.9779475646561
At time: 349.5180902481079 and batch: 800, loss is 5.973983287811279 and perplexity is 393.0682605726511
At time: 351.2682898044586 and batch: 850, loss is 5.969866847991943 and perplexity is 391.4535444540997
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.593608856201172 and perplexity of 268.703584637683
Finished 11 epochs...
Completing Train Step...
At time: 355.71317195892334 and batch: 50, loss is 5.973003396987915 and perplexity is 392.68328523886254
At time: 357.4014632701874 and batch: 100, loss is 5.928076839447021 and perplexity is 375.4318034686987
At time: 359.10408210754395 and batch: 150, loss is 5.91251311302185 and perplexity is 369.6339209512913
At time: 360.8225266933441 and batch: 200, loss is 5.943301773071289 and perplexity is 381.19146170784506
At time: 362.5496060848236 and batch: 250, loss is 5.994144868850708 and perplexity is 401.07356680659774
At time: 364.29520297050476 and batch: 300, loss is 5.946690273284912 and perplexity is 382.4853199387201
At time: 366.0401566028595 and batch: 350, loss is 5.92323865890503 and perplexity is 373.6197835965991
At time: 367.8116307258606 and batch: 400, loss is 5.9493543148040775 and perplexity is 383.50563518916164
At time: 369.5565719604492 and batch: 450, loss is 5.966046895980835 and perplexity is 389.9610631181083
At time: 371.30323243141174 and batch: 500, loss is 5.973931159973144 and perplexity is 393.04777130802324
At time: 373.0503396987915 and batch: 550, loss is 5.943509035110473 and perplexity is 381.27047641561023
At time: 374.7959382534027 and batch: 600, loss is 5.946204309463501 and perplexity is 382.29949106772017
At time: 376.54289722442627 and batch: 650, loss is 5.980333461761474 and perplexity is 395.5722543853322
At time: 378.29046845436096 and batch: 700, loss is 5.931619310379029 and perplexity is 376.7641181692082
At time: 380.036904335022 and batch: 750, loss is 5.930924272537231 and perplexity is 376.5023438317225
At time: 381.7857494354248 and batch: 800, loss is 5.958406524658203 and perplexity is 386.99296890407106
At time: 383.54227924346924 and batch: 850, loss is 5.95360107421875 and perplexity is 385.1377545120976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.587504704793294 and perplexity of 267.06837313991724
Finished 12 epochs...
Completing Train Step...
At time: 387.95735692977905 and batch: 50, loss is 5.959486856460571 and perplexity is 387.411275630016
At time: 389.6891939640045 and batch: 100, loss is 5.914791917800903 and perplexity is 370.4772049717554
At time: 391.40807056427 and batch: 150, loss is 5.9014496231079105 and perplexity is 365.5670183455726
At time: 393.1495473384857 and batch: 200, loss is 5.932724380493164 and perplexity is 377.18069906943384
At time: 394.88957023620605 and batch: 250, loss is 5.983297710418701 and perplexity is 396.74656852892286
At time: 396.63037872314453 and batch: 300, loss is 5.935409011840821 and perplexity is 378.1946506319784
At time: 398.37054777145386 and batch: 350, loss is 5.9130503368377685 and perplexity is 369.83255044626804
At time: 400.1215760707855 and batch: 400, loss is 5.940263538360596 and perplexity is 380.03507016147375
At time: 401.8720500469208 and batch: 450, loss is 5.9538042354583744 and perplexity is 385.21600752445073
At time: 403.6233100891113 and batch: 500, loss is 5.963271312713623 and perplexity is 388.8801944308799
At time: 405.3757674694061 and batch: 550, loss is 5.9348273181915285 and perplexity is 377.97472117749436
At time: 407.1265826225281 and batch: 600, loss is 5.935965480804444 and perplexity is 378.4051627835705
At time: 408.87808561325073 and batch: 650, loss is 5.970738744735717 and perplexity is 391.79500036036325
At time: 410.67710995674133 and batch: 700, loss is 5.921957063674927 and perplexity is 373.14126096572744
At time: 412.42762899398804 and batch: 750, loss is 5.921059217453003 and perplexity is 372.80638784910263
At time: 414.17908811569214 and batch: 800, loss is 5.950282211303711 and perplexity is 383.86165387434517
At time: 415.9283666610718 and batch: 850, loss is 5.945713367462158 and perplexity is 382.11185025461003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.579914093017578 and perplexity of 265.0488352625356
Finished 13 epochs...
Completing Train Step...
At time: 420.3427429199219 and batch: 50, loss is 5.950363140106202 and perplexity is 383.89272059539456
At time: 422.0804615020752 and batch: 100, loss is 5.904273138046265 and perplexity is 366.60066084866173
At time: 423.7931761741638 and batch: 150, loss is 5.89179069519043 and perplexity is 362.05303086198126
At time: 425.5045516490936 and batch: 200, loss is 5.9229716873168945 and perplexity is 373.5200510430851
At time: 427.21547746658325 and batch: 250, loss is 5.974076747894287 and perplexity is 393.10499848164926
At time: 428.9352366924286 and batch: 300, loss is 5.926501388549805 and perplexity is 374.84079477191375
At time: 430.66680550575256 and batch: 350, loss is 5.903524656295776 and perplexity is 366.3263696080986
At time: 432.41518235206604 and batch: 400, loss is 5.931833000183105 and perplexity is 376.8446374225682
At time: 434.1555314064026 and batch: 450, loss is 5.9480986976623536 and perplexity is 383.02440112576505
At time: 435.8997986316681 and batch: 500, loss is 5.958461675643921 and perplexity is 387.01431253632774
At time: 437.64235401153564 and batch: 550, loss is 5.925017280578613 and perplexity is 374.2849031640809
At time: 439.38542079925537 and batch: 600, loss is 5.926424598693847 and perplexity is 374.8120119064064
At time: 441.12815403938293 and batch: 650, loss is 5.961604146957398 and perplexity is 388.2324068221505
At time: 442.8723130226135 and batch: 700, loss is 5.912443933486938 and perplexity is 369.60835073303025
At time: 444.6163332462311 and batch: 750, loss is 5.909947357177734 and perplexity is 368.6867461876321
At time: 446.37244391441345 and batch: 800, loss is 5.938265495300293 and perplexity is 379.27650180534715
At time: 448.1252484321594 and batch: 850, loss is 5.934704828262329 and perplexity is 377.9284259160676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.574232737223308 and perplexity of 263.5472680343272
Finished 14 epochs...
Completing Train Step...
At time: 452.59175968170166 and batch: 50, loss is 5.941340036392212 and perplexity is 380.44439744692727
At time: 454.3335702419281 and batch: 100, loss is 5.893788814544678 and perplexity is 362.77717925674307
At time: 456.04802918434143 and batch: 150, loss is 5.877570858001709 and perplexity is 356.9411270661858
At time: 457.7721927165985 and batch: 200, loss is 5.908286819458008 and perplexity is 368.07503596333476
At time: 459.50438833236694 and batch: 250, loss is 5.961369037628174 and perplexity is 388.141140490603
At time: 461.2395474910736 and batch: 300, loss is 5.914423637390136 and perplexity is 370.3407905954419
At time: 462.9927179813385 and batch: 350, loss is 5.8903168678283695 and perplexity is 361.5198202253115
At time: 464.7414894104004 and batch: 400, loss is 5.918475008010864 and perplexity is 371.84422181626326
At time: 466.49741315841675 and batch: 450, loss is 5.935120906829834 and perplexity is 378.0857065524241
At time: 468.2507607936859 and batch: 500, loss is 5.945026626586914 and perplexity is 381.8495285119663
At time: 470.00337386131287 and batch: 550, loss is 5.9160921669006346 and perplexity is 370.95923093299075
At time: 471.75460839271545 and batch: 600, loss is 5.915954427719116 and perplexity is 370.9081388309183
At time: 473.5046751499176 and batch: 650, loss is 5.951849699020386 and perplexity is 384.4638241256365
At time: 475.2572603225708 and batch: 700, loss is 5.90333399772644 and perplexity is 366.25653300424415
At time: 477.0068702697754 and batch: 750, loss is 5.9009369850158695 and perplexity is 365.37966279367413
At time: 478.756635427475 and batch: 800, loss is 5.928782901763916 and perplexity is 375.69697532060167
At time: 480.50553941726685 and batch: 850, loss is 5.925641288757324 and perplexity is 374.5185328906725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.570483525594075 and perplexity of 262.5610235271174
Finished 15 epochs...
Completing Train Step...
At time: 484.9373767375946 and batch: 50, loss is 5.93307599067688 and perplexity is 377.3133429624327
At time: 486.6386671066284 and batch: 100, loss is 5.887502183914185 and perplexity is 360.50368692055315
At time: 488.33775544166565 and batch: 150, loss is 5.870338373184204 and perplexity is 354.36886890562573
At time: 490.054532289505 and batch: 200, loss is 5.9019935417175295 and perplexity is 365.7659111357551
At time: 491.78231048583984 and batch: 250, loss is 5.955188484191894 and perplexity is 385.7496115301273
At time: 493.5068678855896 and batch: 300, loss is 5.909363021850586 and perplexity is 368.47137242855956
At time: 495.2328395843506 and batch: 350, loss is 5.885951690673828 and perplexity is 359.94516149769805
At time: 496.97830605506897 and batch: 400, loss is 5.913031692504883 and perplexity is 369.82565522936414
At time: 498.76990151405334 and batch: 450, loss is 5.930155944824219 and perplexity is 376.21317774832556
At time: 500.5162875652313 and batch: 500, loss is 5.9392851543426515 and perplexity is 379.6634317547536
At time: 502.26420307159424 and batch: 550, loss is 5.909197292327881 and perplexity is 368.4103109038648
At time: 504.0112626552582 and batch: 600, loss is 5.910902061462402 and perplexity is 369.03890107906165
At time: 505.75861859321594 and batch: 650, loss is 5.947055978775024 and perplexity is 382.62522250011716
At time: 507.51435589790344 and batch: 700, loss is 5.900134696960449 and perplexity is 365.08664061433575
At time: 509.26959586143494 and batch: 750, loss is 5.895538873672486 and perplexity is 363.4126166349738
At time: 511.026474237442 and batch: 800, loss is 5.921351852416993 and perplexity is 372.9154999972221
At time: 512.7826180458069 and batch: 850, loss is 5.920888395309448 and perplexity is 372.742709701773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.566962560017903 and perplexity of 261.6381808037531
Finished 16 epochs...
Completing Train Step...
At time: 517.2298355102539 and batch: 50, loss is 5.92977234840393 and perplexity is 376.0688913957094
At time: 518.9396336078644 and batch: 100, loss is 5.880830402374268 and perplexity is 358.10649075377773
At time: 520.6510229110718 and batch: 150, loss is 5.861447801589966 and perplexity is 351.2322907523267
At time: 522.3688776493073 and batch: 200, loss is 5.89619589805603 and perplexity is 363.65146604174055
At time: 524.1036117076874 and batch: 250, loss is 5.948740911483765 and perplexity is 383.27046369403985
At time: 525.8528125286102 and batch: 300, loss is 5.90309549331665 and perplexity is 366.16918962231324
At time: 527.6017725467682 and batch: 350, loss is 5.878159399032593 and perplexity is 357.1512633959346
At time: 529.3528971672058 and batch: 400, loss is 5.907409458160401 and perplexity is 367.7522427959875
At time: 531.102597951889 and batch: 450, loss is 5.922933864593506 and perplexity is 373.50592376468205
At time: 532.8531651496887 and batch: 500, loss is 5.933271827697754 and perplexity is 377.3872421193139
At time: 534.6069033145905 and batch: 550, loss is 5.904550380706787 and perplexity is 366.70231228163044
At time: 536.3588752746582 and batch: 600, loss is 5.904511413574219 and perplexity is 366.68802322241817
At time: 538.1123430728912 and batch: 650, loss is 5.940090970993042 and perplexity is 379.96949416813885
At time: 539.8653843402863 and batch: 700, loss is 5.89195987701416 and perplexity is 362.11428883575115
At time: 541.6173758506775 and batch: 750, loss is 5.887672309875488 and perplexity is 360.56502317414055
At time: 543.4126198291779 and batch: 800, loss is 5.914510974884033 and perplexity is 370.373136644471
At time: 545.1650981903076 and batch: 850, loss is 5.913915700912476 and perplexity is 370.1527287645081
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.562925338745117 and perplexity of 260.584018948601
Finished 17 epochs...
Completing Train Step...
At time: 549.6145496368408 and batch: 50, loss is 5.921749572753907 and perplexity is 373.0638455735924
At time: 551.3084876537323 and batch: 100, loss is 5.874370279312134 and perplexity is 355.80053515220857
At time: 553.0026240348816 and batch: 150, loss is 5.855225505828858 and perplexity is 349.0536048019289
At time: 554.6979835033417 and batch: 200, loss is 5.887646417617798 and perplexity is 360.5556874525085
At time: 556.4176044464111 and batch: 250, loss is 5.93830267906189 and perplexity is 379.2906049945728
At time: 558.1366353034973 and batch: 300, loss is 5.8942390823364255 and perplexity is 362.94056291658296
At time: 559.8693194389343 and batch: 350, loss is 5.869401512145996 and perplexity is 354.03702998694257
At time: 561.6133062839508 and batch: 400, loss is 5.898938760757447 and perplexity is 364.6502812645736
At time: 563.3643038272858 and batch: 450, loss is 5.916990642547607 and perplexity is 371.29267854285075
At time: 565.1165626049042 and batch: 500, loss is 5.927359895706177 and perplexity is 375.16273645157
At time: 566.8683462142944 and batch: 550, loss is 5.897256870269775 and perplexity is 364.0374948894275
At time: 568.6212801933289 and batch: 600, loss is 5.896801490783691 and perplexity is 363.871757421675
At time: 570.3759768009186 and batch: 650, loss is 5.931983613967896 and perplexity is 376.90139969417146
At time: 572.130842924118 and batch: 700, loss is 5.886535863876343 and perplexity is 360.15549324452365
At time: 573.8834862709045 and batch: 750, loss is 5.879467086791992 and perplexity is 357.61861123725834
At time: 575.635335445404 and batch: 800, loss is 5.91046989440918 and perplexity is 368.8794490820893
At time: 577.3871581554413 and batch: 850, loss is 5.910302467346192 and perplexity is 368.81769384922575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.559194564819336 and perplexity of 259.61365012410704
Finished 18 epochs...
Completing Train Step...
At time: 581.7855837345123 and batch: 50, loss is 5.914270648956299 and perplexity is 370.2841370716798
At time: 583.5273833274841 and batch: 100, loss is 5.865038719177246 and perplexity is 352.4958041910068
At time: 585.2403807640076 and batch: 150, loss is 5.850405607223511 and perplexity is 347.3752498185877
At time: 587.0014545917511 and batch: 200, loss is 5.88620325088501 and perplexity is 360.03572076861536
At time: 588.7283391952515 and batch: 250, loss is 5.935061359405518 and perplexity is 378.06319319274115
At time: 590.4551928043365 and batch: 300, loss is 5.890112380981446 and perplexity is 361.4459017351103
At time: 592.1830821037292 and batch: 350, loss is 5.865657949447632 and perplexity is 352.71414785867324
At time: 593.9107778072357 and batch: 400, loss is 5.893732872009277 and perplexity is 362.75688514920716
At time: 595.6441423892975 and batch: 450, loss is 5.909625577926636 and perplexity is 368.56812952776727
At time: 597.4008665084839 and batch: 500, loss is 5.919255304336548 and perplexity is 372.1344837266968
At time: 599.1594383716583 and batch: 550, loss is 5.890402994155884 and perplexity is 361.55095794062044
At time: 600.9213066101074 and batch: 600, loss is 5.890440139770508 and perplexity is 361.5643882226076
At time: 602.6770422458649 and batch: 650, loss is 5.927196235656738 and perplexity is 375.10134232359485
At time: 604.4334585666656 and batch: 700, loss is 5.880408763885498 and perplexity is 357.955531101634
At time: 606.1893494129181 and batch: 750, loss is 5.873925228118896 and perplexity is 355.64222093106395
At time: 607.9477310180664 and batch: 800, loss is 5.902097110748291 and perplexity is 365.8037951184268
At time: 609.705982208252 and batch: 850, loss is 5.902848405838013 and perplexity is 366.07872497729545
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.555264155069987 and perplexity of 258.59526474950906
Finished 19 epochs...
Completing Train Step...
At time: 614.1353855133057 and batch: 50, loss is 5.907020578384399 and perplexity is 367.60925918970327
At time: 615.8864743709564 and batch: 100, loss is 5.858580837249756 and perplexity is 350.22676239532433
At time: 617.6000525951385 and batch: 150, loss is 5.841420707702636 and perplexity is 344.268097730511
At time: 619.3183181285858 and batch: 200, loss is 5.878549757003785 and perplexity is 357.29070745330137
At time: 621.0422823429108 and batch: 250, loss is 5.9275318622589115 and perplexity is 375.2272574416392
At time: 622.7658386230469 and batch: 300, loss is 5.882623281478882 and perplexity is 358.7491082939202
At time: 624.4896142482758 and batch: 350, loss is 5.858029203414917 and perplexity is 350.0336187405064
At time: 626.229656457901 and batch: 400, loss is 5.888326683044434 and perplexity is 360.80104446554395
At time: 627.9794704914093 and batch: 450, loss is 5.905357675552368 and perplexity is 366.9984686948865
At time: 629.7600948810577 and batch: 500, loss is 5.913725233078003 and perplexity is 370.0822332896093
At time: 631.5122647285461 and batch: 550, loss is 5.884789657592774 and perplexity is 359.52713623930083
At time: 633.2657806873322 and batch: 600, loss is 5.884614477157593 and perplexity is 359.4641596354105
At time: 635.020007610321 and batch: 650, loss is 5.922746086120606 and perplexity is 373.43579397733674
At time: 636.7739768028259 and batch: 700, loss is 5.875523271560669 and perplexity is 356.21100700122065
At time: 638.5274620056152 and batch: 750, loss is 5.872019872665406 and perplexity is 354.9652412342079
At time: 640.281033039093 and batch: 800, loss is 5.8991637134552 and perplexity is 364.7323195561023
At time: 642.0341460704803 and batch: 850, loss is 5.899270486831665 and perplexity is 364.77126533651636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.553160349527995 and perplexity of 258.05180246832253
Finished 20 epochs...
Completing Train Step...
At time: 646.4333894252777 and batch: 50, loss is 5.904499320983887 and perplexity is 366.6835890411842
At time: 648.1596102714539 and batch: 100, loss is 5.855373973846436 and perplexity is 349.10543194590366
At time: 649.8702111244202 and batch: 150, loss is 5.83603196144104 and perplexity is 342.41791386878765
At time: 651.5818390846252 and batch: 200, loss is 5.87562328338623 and perplexity is 356.2466340958514
At time: 653.2989339828491 and batch: 250, loss is 5.924383640289307 and perplexity is 374.0478162915867
At time: 655.0327775478363 and batch: 300, loss is 5.8807607269287105 and perplexity is 358.0815403937014
At time: 656.7752711772919 and batch: 350, loss is 5.855663175582886 and perplexity is 349.2064084436121
At time: 658.5174927711487 and batch: 400, loss is 5.885891103744507 and perplexity is 359.92335418626476
At time: 660.2675898075104 and batch: 450, loss is 5.902574491500855 and perplexity is 365.9784644980139
At time: 662.0209069252014 and batch: 500, loss is 5.911363658905029 and perplexity is 369.2092878140446
At time: 663.7740738391876 and batch: 550, loss is 5.8823457527160645 and perplexity is 358.64955891228095
At time: 665.523294210434 and batch: 600, loss is 5.881205921173096 and perplexity is 358.2409917252792
At time: 667.2728426456451 and batch: 650, loss is 5.918907117843628 and perplexity is 372.0049340809408
At time: 669.0266506671906 and batch: 700, loss is 5.8735317134857175 and perplexity is 355.50229804560854
At time: 670.7755897045135 and batch: 750, loss is 5.868215427398682 and perplexity is 353.6173609963842
At time: 672.5278217792511 and batch: 800, loss is 5.895156841278077 and perplexity is 363.27380775931414
At time: 674.3062720298767 and batch: 850, loss is 5.896561403274536 and perplexity is 363.7844068440946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.552642186482747 and perplexity of 257.9181241970848
Finished 21 epochs...
Completing Train Step...
At time: 678.6814467906952 and batch: 50, loss is 5.89943380355835 and perplexity is 364.8308434504786
At time: 680.3821604251862 and batch: 100, loss is 5.849198055267334 and perplexity is 346.9560293223487
At time: 682.1088297367096 and batch: 150, loss is 5.8318860912323 and perplexity is 341.00123235876185
At time: 683.8820900917053 and batch: 200, loss is 5.870934410095215 and perplexity is 354.5801487906568
At time: 685.6175529956818 and batch: 250, loss is 5.919785404205323 and perplexity is 372.3318044629145
At time: 687.3562748432159 and batch: 300, loss is 5.875996809005738 and perplexity is 356.37972619565664
At time: 689.0989453792572 and batch: 350, loss is 5.849326162338257 and perplexity is 347.0004796901466
At time: 690.8480031490326 and batch: 400, loss is 5.8799077415466305 and perplexity is 357.77623230433295
At time: 692.5959997177124 and batch: 450, loss is 5.895302639007569 and perplexity is 363.32677611690866
At time: 694.3412387371063 and batch: 500, loss is 5.904398393630982 and perplexity is 366.64658250470575
At time: 696.0936326980591 and batch: 550, loss is 5.8746864604949955 and perplexity is 355.91305037293614
At time: 697.8396108150482 and batch: 600, loss is 5.874194965362549 and perplexity is 355.73816382256837
At time: 699.5884022712708 and batch: 650, loss is 5.908931541442871 and perplexity is 368.3124185457619
At time: 701.3380930423737 and batch: 700, loss is 5.8608396530151365 and perplexity is 351.0187542728057
At time: 703.0871739387512 and batch: 750, loss is 5.858746690750122 and perplexity is 350.2848535469668
At time: 704.837813615799 and batch: 800, loss is 5.885260648727417 and perplexity is 359.69651021682324
At time: 706.5883855819702 and batch: 850, loss is 5.885083236694336 and perplexity is 359.6327013880478
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.539401372273763 and perplexity of 254.52558777029876
Finished 22 epochs...
Completing Train Step...
At time: 711.1072034835815 and batch: 50, loss is 5.8851051521301265 and perplexity is 359.6405829817872
At time: 712.8197686672211 and batch: 100, loss is 5.836598196029663 and perplexity is 342.61185763903796
At time: 714.5453433990479 and batch: 150, loss is 5.820540237426758 and perplexity is 337.1541477278082
At time: 716.2653806209564 and batch: 200, loss is 5.862634525299073 and perplexity is 351.6493538595588
At time: 718.0369429588318 and batch: 250, loss is 5.908327188491821 and perplexity is 368.0898950968299
At time: 719.7525398731232 and batch: 300, loss is 5.863016881942749 and perplexity is 351.7838350344985
At time: 721.4677259922028 and batch: 350, loss is 5.834019441604614 and perplexity is 341.72948399652466
At time: 723.1865994930267 and batch: 400, loss is 5.8622684574127195 and perplexity is 351.5206498824877
At time: 724.9100558757782 and batch: 450, loss is 5.877576999664306 and perplexity is 356.94331928488725
At time: 726.6350185871124 and batch: 500, loss is 5.887981090545654 and perplexity is 360.6763758745307
At time: 728.3615989685059 and batch: 550, loss is 5.855654945373535 and perplexity is 349.203534413591
At time: 730.0910358428955 and batch: 600, loss is 5.853934965133667 and perplexity is 348.6034274686939
At time: 731.8167405128479 and batch: 650, loss is 5.888218851089477 and perplexity is 360.7621406811422
At time: 733.5447700023651 and batch: 700, loss is 5.842588872909546 and perplexity is 344.670494731457
At time: 735.2719314098358 and batch: 750, loss is 5.836929855346679 and perplexity is 342.72550689912043
At time: 737.0168483257294 and batch: 800, loss is 5.8616579818725585 and perplexity is 351.30612061297137
At time: 738.7624433040619 and batch: 850, loss is 5.8643550777435305 and perplexity is 352.2549058074914
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.528176625569661 and perplexity of 251.68457715946022
Finished 23 epochs...
Completing Train Step...
At time: 743.2163348197937 and batch: 50, loss is 5.867374467849731 and perplexity is 353.3201081063262
At time: 744.9208979606628 and batch: 100, loss is 5.817059545516968 and perplexity is 335.9826579936175
At time: 746.6405317783356 and batch: 150, loss is 5.801477909088135 and perplexity is 330.7880735314885
At time: 748.3774404525757 and batch: 200, loss is 5.843056163787842 and perplexity is 344.8315937467815
At time: 750.1158747673035 and batch: 250, loss is 5.886227340698242 and perplexity is 360.04439406635424
At time: 751.8552901744843 and batch: 300, loss is 5.84278450012207 and perplexity is 344.73792825528176
At time: 753.5932250022888 and batch: 350, loss is 5.813835849761963 and perplexity is 334.9012960527161
At time: 755.3387303352356 and batch: 400, loss is 5.843008403778076 and perplexity is 344.815124979774
At time: 757.0800931453705 and batch: 450, loss is 5.860443143844605 and perplexity is 350.8795997075617
At time: 758.8285217285156 and batch: 500, loss is 5.873098001480103 and perplexity is 355.3481458621653
At time: 760.5691962242126 and batch: 550, loss is 5.841975088119507 and perplexity is 344.4590061350971
At time: 762.3510384559631 and batch: 600, loss is 5.841930408477783 and perplexity is 344.4436161739259
At time: 764.0991241931915 and batch: 650, loss is 5.875707712173462 and perplexity is 356.2767128368614
At time: 765.8404653072357 and batch: 700, loss is 5.826679410934449 and perplexity is 339.23036213885683
At time: 767.5811049938202 and batch: 750, loss is 5.822220506668091 and perplexity is 337.72113368295584
At time: 769.3208568096161 and batch: 800, loss is 5.845905990600586 and perplexity is 345.815705676765
At time: 771.0611777305603 and batch: 850, loss is 5.847906131744384 and perplexity is 346.5080780883111
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.515657424926758 and perplexity of 248.5533286998488
Finished 24 epochs...
Completing Train Step...
At time: 775.488109588623 and batch: 50, loss is 5.851848669052124 and perplexity is 347.8768956477263
At time: 777.2201926708221 and batch: 100, loss is 5.798330163955688 and perplexity is 329.7484740386684
At time: 778.9129602909088 and batch: 150, loss is 5.78584958076477 and perplexity is 325.6585958967161
At time: 780.6233022212982 and batch: 200, loss is 5.8252076816558835 and perplexity is 338.731474086844
At time: 782.3372325897217 and batch: 250, loss is 5.869900169372559 and perplexity is 354.2136171349862
At time: 784.0583899021149 and batch: 300, loss is 5.822333650588989 and perplexity is 337.75934693794886
At time: 785.777396440506 and batch: 350, loss is 5.7983203125 and perplexity is 329.74522555218925
At time: 787.4956424236298 and batch: 400, loss is 5.826124973297119 and perplexity is 339.04233218861964
At time: 789.2428886890411 and batch: 450, loss is 5.842701873779297 and perplexity is 344.7094449978048
At time: 790.9920542240143 and batch: 500, loss is 5.851845598220825 and perplexity is 347.87582737810703
At time: 792.7371137142181 and batch: 550, loss is 5.825384693145752 and perplexity is 338.7914387567987
At time: 794.4866917133331 and batch: 600, loss is 5.8249234867095945 and perplexity is 338.63522199158683
At time: 796.2369928359985 and batch: 650, loss is 5.860004711151123 and perplexity is 350.7257963382505
At time: 797.9965343475342 and batch: 700, loss is 5.81026439666748 and perplexity is 333.70734512155383
At time: 799.7550051212311 and batch: 750, loss is 5.808193807601929 and perplexity is 333.0170892064394
At time: 801.5055682659149 and batch: 800, loss is 5.83523229598999 and perplexity is 342.1442035461763
At time: 803.2604684829712 and batch: 850, loss is 5.834927320480347 and perplexity is 342.03987385313746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.50472895304362 and perplexity of 245.85180926245474
Finished 25 epochs...
Completing Train Step...
At time: 807.7386205196381 and batch: 50, loss is 5.838640413284302 and perplexity is 343.31226043025697
At time: 809.4658043384552 and batch: 100, loss is 5.784781150817871 and perplexity is 325.31083831078604
At time: 811.1697218418121 and batch: 150, loss is 5.773507280349731 and perplexity is 321.6639221150738
At time: 812.8836262226105 and batch: 200, loss is 5.815043392181397 and perplexity is 335.30594784195284
At time: 814.6006081104279 and batch: 250, loss is 5.8572610569000245 and perplexity is 349.7648448782443
At time: 816.3266308307648 and batch: 300, loss is 5.810942811965942 and perplexity is 333.9338141010176
At time: 818.0531413555145 and batch: 350, loss is 5.787754869461059 and perplexity is 326.2796610046137
At time: 819.7847921848297 and batch: 400, loss is 5.81535647392273 and perplexity is 335.41094244707176
At time: 821.5395624637604 and batch: 450, loss is 5.831624574661255 and perplexity is 340.91206654543043
At time: 823.2966465950012 and batch: 500, loss is 5.844220819473267 and perplexity is 345.23343778250086
At time: 825.0525343418121 and batch: 550, loss is 5.815286111831665 and perplexity is 335.387343062056
At time: 826.8067615032196 and batch: 600, loss is 5.817973051071167 and perplexity is 336.2897202479933
At time: 828.5759062767029 and batch: 650, loss is 5.849429321289063 and perplexity is 347.0362777419735
At time: 830.331663608551 and batch: 700, loss is 5.8022394943237305 and perplexity is 331.04009279929676
At time: 832.0886704921722 and batch: 750, loss is 5.800372333526611 and perplexity is 330.42256440755614
At time: 833.8457314968109 and batch: 800, loss is 5.827665328979492 and perplexity is 339.5649804003022
At time: 835.6091747283936 and batch: 850, loss is 5.830142421722412 and perplexity is 340.4071569931326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.5007890065511065 and perplexity of 244.88507198601303
Finished 26 epochs...
Completing Train Step...
At time: 840.0524425506592 and batch: 50, loss is 5.830537395477295 and perplexity is 340.5416354420999
At time: 841.7496869564056 and batch: 100, loss is 5.776439943313599 and perplexity is 322.6086385766751
At time: 843.4466428756714 and batch: 150, loss is 5.765372800827026 and perplexity is 319.05796694153634
At time: 845.1550319194794 and batch: 200, loss is 5.804130201339722 and perplexity is 331.66658469500754
At time: 846.8696014881134 and batch: 250, loss is 5.848268232345581 and perplexity is 346.63357159097154
At time: 848.5919494628906 and batch: 300, loss is 5.801644115447998 and perplexity is 330.8430571822661
At time: 850.3607075214386 and batch: 350, loss is 5.780181379318237 and perplexity is 323.81791896438455
At time: 852.1039893627167 and batch: 400, loss is 5.805754470825195 and perplexity is 332.20573835476245
At time: 853.848888874054 and batch: 450, loss is 5.8226831340789795 and perplexity is 337.87740888244207
At time: 855.6018991470337 and batch: 500, loss is 5.834138059616089 and perplexity is 341.7700216725806
At time: 857.3561058044434 and batch: 550, loss is 5.8067473030090335 and perplexity is 332.5357266878693
At time: 859.1087520122528 and batch: 600, loss is 5.8111483764648435 and perplexity is 334.002466094144
At time: 860.8630666732788 and batch: 650, loss is 5.84355525970459 and perplexity is 345.0037407425441
At time: 862.6151037216187 and batch: 700, loss is 5.796903171539307 and perplexity is 329.27826104188364
At time: 864.3690195083618 and batch: 750, loss is 5.7947717380523684 and perplexity is 328.5771737556582
At time: 866.1209452152252 and batch: 800, loss is 5.821845827102661 and perplexity is 337.5946201778434
At time: 867.8715279102325 and batch: 850, loss is 5.821715898513794 and perplexity is 337.55075983465275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.4956709543863935 and perplexity of 243.63493926434816
Finished 27 epochs...
Completing Train Step...
At time: 872.2878940105438 and batch: 50, loss is 5.823178930282593 and perplexity is 338.0449687533942
At time: 873.9813098907471 and batch: 100, loss is 5.769239807128907 and perplexity is 320.2941547429049
At time: 875.6802484989166 and batch: 150, loss is 5.756401557922363 and perplexity is 316.2084215132238
At time: 877.4002614021301 and batch: 200, loss is 5.7963283920288085 and perplexity is 329.0890530258388
At time: 879.121889591217 and batch: 250, loss is 5.841370248794556 and perplexity is 344.25072677647603
At time: 880.843777179718 and batch: 300, loss is 5.794690237045288 and perplexity is 328.5503954763368
At time: 882.5647432804108 and batch: 350, loss is 5.772512083053589 and perplexity is 321.3439622875037
At time: 884.3041574954987 and batch: 400, loss is 5.79837061882019 and perplexity is 329.7618142383415
At time: 886.0588572025299 and batch: 450, loss is 5.814783945083618 and perplexity is 335.2189649711318
At time: 887.8123955726624 and batch: 500, loss is 5.82497015953064 and perplexity is 338.65102742154164
At time: 889.5657067298889 and batch: 550, loss is 5.798373394012451 and perplexity is 329.7627293920462
At time: 891.3169128894806 and batch: 600, loss is 5.803235492706299 and perplexity is 331.36997244881843
At time: 893.0948777198792 and batch: 650, loss is 5.837210865020752 and perplexity is 342.8218296152841
At time: 894.8450267314911 and batch: 700, loss is 5.7883779907226565 and perplexity is 326.48303615572223
At time: 896.5949351787567 and batch: 750, loss is 5.787001104354858 and perplexity is 326.0338154478264
At time: 898.3441288471222 and batch: 800, loss is 5.814800481796265 and perplexity is 335.2245084366647
At time: 900.0974335670471 and batch: 850, loss is 5.816426811218261 and perplexity is 335.77013748373247
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.488699595133464 and perplexity of 241.94237915330802
Finished 28 epochs...
Completing Train Step...
At time: 904.5418694019318 and batch: 50, loss is 5.817127456665039 and perplexity is 336.0054757364347
At time: 906.26158452034 and batch: 100, loss is 5.763052911758423 and perplexity is 318.31864575492574
At time: 907.9778549671173 and batch: 150, loss is 5.749268798828125 and perplexity is 313.9610076780612
At time: 909.6975705623627 and batch: 200, loss is 5.790845174789428 and perplexity is 327.2895243689408
At time: 911.4393639564514 and batch: 250, loss is 5.8375306224823 and perplexity is 342.93146698103385
At time: 913.1808698177338 and batch: 300, loss is 5.788836574554443 and perplexity is 326.6327903322534
At time: 914.9206440448761 and batch: 350, loss is 5.765492467880249 and perplexity is 319.09614995282624
At time: 916.6630368232727 and batch: 400, loss is 5.788631000518799 and perplexity is 326.5656500127608
At time: 918.4056510925293 and batch: 450, loss is 5.806104974746704 and perplexity is 332.32219817742356
At time: 920.1486415863037 and batch: 500, loss is 5.817160997390747 and perplexity is 336.0167457929346
At time: 921.8932793140411 and batch: 550, loss is 5.791289615631103 and perplexity is 327.4350175297793
At time: 923.6369326114655 and batch: 600, loss is 5.796137170791626 and perplexity is 329.02613022625553
At time: 925.381135225296 and batch: 650, loss is 5.828533935546875 and perplexity is 339.86005690647426
At time: 927.1245403289795 and batch: 700, loss is 5.780367164611817 and perplexity is 323.87808516034977
At time: 928.866069316864 and batch: 750, loss is 5.778419160842896 and perplexity is 323.2477835440667
At time: 930.6096649169922 and batch: 800, loss is 5.807167692184448 and perplexity is 332.675550495956
At time: 932.3521053791046 and batch: 850, loss is 5.81100679397583 and perplexity is 333.9551805411398
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.483937581380208 and perplexity of 240.7929851034662
Finished 29 epochs...
Completing Train Step...
At time: 936.7272844314575 and batch: 50, loss is 5.809544639587402 and perplexity is 333.4672433150816
At time: 938.4747133255005 and batch: 100, loss is 5.7565672492980955 and perplexity is 316.26081886237716
At time: 940.2071695327759 and batch: 150, loss is 5.740381183624268 and perplexity is 311.18300624280135
At time: 941.954532623291 and batch: 200, loss is 5.784060535430908 and perplexity is 325.0764987596374
At time: 943.7007133960724 and batch: 250, loss is 5.827435531616211 and perplexity is 339.48695822813164
At time: 945.4583497047424 and batch: 300, loss is 5.776663198471069 and perplexity is 322.6806706595496
At time: 947.2264692783356 and batch: 350, loss is 5.757582263946533 and perplexity is 316.5819911959869
At time: 948.9863493442535 and batch: 400, loss is 5.783026762008667 and perplexity is 324.7406169577773
At time: 950.7433762550354 and batch: 450, loss is 5.798356447219849 and perplexity is 329.7571410188159
At time: 952.4994647502899 and batch: 500, loss is 5.8095982646942135 and perplexity is 333.48512601109877
At time: 954.2548935413361 and batch: 550, loss is 5.78498477935791 and perplexity is 325.37708762674714
At time: 956.0101730823517 and batch: 600, loss is 5.789409971237182 and perplexity is 326.82013419674604
At time: 957.7673954963684 and batch: 650, loss is 5.8231284809112545 and perplexity is 338.027915027415
At time: 959.5223846435547 and batch: 700, loss is 5.772379055023193 and perplexity is 321.30121737631976
At time: 961.2768361568451 and batch: 750, loss is 5.767530422210694 and perplexity is 319.74711642812184
At time: 963.034327507019 and batch: 800, loss is 5.79494083404541 and perplexity is 328.6327395369883
At time: 964.7912843227386 and batch: 850, loss is 5.799543218612671 and perplexity is 330.1487196715683
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.47061284383138 and perplexity of 237.6057633769696
Finished 30 epochs...
Completing Train Step...
At time: 969.2069234848022 and batch: 50, loss is 5.798009414672851 and perplexity is 329.6427244125689
At time: 970.9465146064758 and batch: 100, loss is 5.744891128540039 and perplexity is 312.58959389197014
At time: 972.6596143245697 and batch: 150, loss is 5.730810098648071 and perplexity is 308.2188549465925
At time: 974.3779048919678 and batch: 200, loss is 5.773620891571045 and perplexity is 321.70046882213643
At time: 976.0996339321136 and batch: 250, loss is 5.816986684799194 and perplexity is 335.9581789477878
At time: 977.8207111358643 and batch: 300, loss is 5.766057405471802 and perplexity is 319.2764702933279
At time: 979.5418977737427 and batch: 350, loss is 5.744135580062866 and perplexity is 312.3535064992909
At time: 981.2946689128876 and batch: 400, loss is 5.767927036285401 and perplexity is 319.8739577867155
At time: 983.0363526344299 and batch: 450, loss is 5.786930332183838 and perplexity is 326.0107421433646
At time: 984.7796382904053 and batch: 500, loss is 5.795418739318848 and perplexity is 328.78983239099904
At time: 986.5316071510315 and batch: 550, loss is 5.774336071014404 and perplexity is 321.9306246759147
At time: 988.2836844921112 and batch: 600, loss is 5.7776414585113525 and perplexity is 322.9964907173125
At time: 990.0393924713135 and batch: 650, loss is 5.80970685005188 and perplexity is 333.5213395788791
At time: 991.7937588691711 and batch: 700, loss is 5.759871501922607 and perplexity is 317.3075528875975
At time: 993.5463705062866 and batch: 750, loss is 5.756517295837402 and perplexity is 316.24502093457716
At time: 995.3039348125458 and batch: 800, loss is 5.7864509868621825 and perplexity is 325.85450786738505
At time: 997.0671648979187 and batch: 850, loss is 5.788329801559448 and perplexity is 326.4673035904808
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.463743845621745 and perplexity of 235.9792424936102
Finished 31 epochs...
Completing Train Step...
At time: 1001.4625852108002 and batch: 50, loss is 5.7900253200531 and perplexity is 327.0213044679385
At time: 1003.1822545528412 and batch: 100, loss is 5.733994064331054 and perplexity is 309.2017771685271
At time: 1004.894721031189 and batch: 150, loss is 5.720040311813355 and perplexity is 304.9172144749988
At time: 1006.6162693500519 and batch: 200, loss is 5.7656352329254155 and perplexity is 319.14170898112764
At time: 1008.3478767871857 and batch: 250, loss is 5.808798904418945 and perplexity is 333.21865776522685
At time: 1010.0886557102203 and batch: 300, loss is 5.75629222869873 and perplexity is 316.1738525817254
At time: 1011.8304505348206 and batch: 350, loss is 5.736230278015137 and perplexity is 309.8939920955597
At time: 1013.5765297412872 and batch: 400, loss is 5.76065185546875 and perplexity is 317.55526159932697
At time: 1015.3298263549805 and batch: 450, loss is 5.7780492305755615 and perplexity is 323.12822652032935
At time: 1017.0825951099396 and batch: 500, loss is 5.786314992904663 and perplexity is 325.8101966363828
At time: 1018.8357775211334 and batch: 550, loss is 5.765794553756714 and perplexity is 319.1925589541269
At time: 1020.5877344608307 and batch: 600, loss is 5.770714941024781 and perplexity is 320.7669801618779
At time: 1022.3384861946106 and batch: 650, loss is 5.804215316772461 and perplexity is 331.6948158413251
At time: 1024.0897107124329 and batch: 700, loss is 5.753370752334595 and perplexity is 315.2515061071834
At time: 1025.870487689972 and batch: 750, loss is 5.750111875534057 and perplexity is 314.2258124999092
At time: 1027.6236681938171 and batch: 800, loss is 5.780259799957276 and perplexity is 323.843313968255
At time: 1029.376706123352 and batch: 850, loss is 5.781633567810059 and perplexity is 324.28850522702805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.452559153238933 and perplexity of 233.35459256993192
Finished 32 epochs...
Completing Train Step...
At time: 1033.8089425563812 and batch: 50, loss is 5.782143297195435 and perplexity is 324.45384674361685
At time: 1035.50758934021 and batch: 100, loss is 5.725599756240845 and perplexity is 306.6171056307955
At time: 1037.2184562683105 and batch: 150, loss is 5.713368139266968 and perplexity is 302.8895262727961
At time: 1038.9448945522308 and batch: 200, loss is 5.757806577682495 and perplexity is 316.65301285043887
At time: 1040.6777272224426 and batch: 250, loss is 5.800227146148682 and perplexity is 330.3745947042032
At time: 1042.415355682373 and batch: 300, loss is 5.7499051284790035 and perplexity is 314.16085395377837
At time: 1044.1604809761047 and batch: 350, loss is 5.728970022201538 and perplexity is 307.6522301675599
At time: 1045.9071245193481 and batch: 400, loss is 5.754388856887817 and perplexity is 315.5726285413295
At time: 1047.6548492908478 and batch: 450, loss is 5.7736469078063966 and perplexity is 321.7088383661176
At time: 1049.4026362895966 and batch: 500, loss is 5.78075758934021 and perplexity is 324.00455986159994
At time: 1051.152503490448 and batch: 550, loss is 5.760034971237182 and perplexity is 317.3594271755893
At time: 1052.898760318756 and batch: 600, loss is 5.765631542205811 and perplexity is 319.1405311207392
At time: 1054.6480176448822 and batch: 650, loss is 5.798211336135864 and perplexity is 329.7092930743485
At time: 1056.3954701423645 and batch: 700, loss is 5.749011144638062 and perplexity is 313.8801247292792
At time: 1058.1434428691864 and batch: 750, loss is 5.744299020767212 and perplexity is 312.40456194855363
At time: 1059.8969419002533 and batch: 800, loss is 5.773716945648193 and perplexity is 321.7313709479011
At time: 1061.6558740139008 and batch: 850, loss is 5.775138349533081 and perplexity is 322.18900633376427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.449719746907552 and perplexity of 232.69294385164218
Finished 33 epochs...
Completing Train Step...
At time: 1066.1033198833466 and batch: 50, loss is 5.778526229858398 and perplexity is 323.28239521890004
At time: 1067.8182764053345 and batch: 100, loss is 5.721856651306152 and perplexity is 305.47155093288336
At time: 1069.5599319934845 and batch: 150, loss is 5.71070104598999 and perplexity is 302.08276798164604
At time: 1071.2790098190308 and batch: 200, loss is 5.754080009460449 and perplexity is 315.4751797960286
At time: 1073.021503686905 and batch: 250, loss is 5.794273138046265 and perplexity is 328.4133860104966
At time: 1074.7624814510345 and batch: 300, loss is 5.744445695877075 and perplexity is 312.45038728263495
At time: 1076.5043268203735 and batch: 350, loss is 5.7250893020629885 and perplexity is 306.46063158803605
At time: 1078.2465376853943 and batch: 400, loss is 5.749313459396363 and perplexity is 313.97502966818115
At time: 1079.9895975589752 and batch: 450, loss is 5.7704177093505855 and perplexity is 320.67165222328106
At time: 1081.7328186035156 and batch: 500, loss is 5.777765712738037 and perplexity is 323.03662688998133
At time: 1083.4742906093597 and batch: 550, loss is 5.758503580093384 and perplexity is 316.87379769865834
At time: 1085.2212624549866 and batch: 600, loss is 5.762584180831909 and perplexity is 318.16947492439647
At time: 1086.9745004177094 and batch: 650, loss is 5.7947994995117185 and perplexity is 328.5862956641289
At time: 1088.7289671897888 and batch: 700, loss is 5.746141672134399 and perplexity is 312.9807453312634
At time: 1090.4826636314392 and batch: 750, loss is 5.739879207611084 and perplexity is 311.02683903733185
At time: 1092.2357971668243 and batch: 800, loss is 5.770663251876831 and perplexity is 320.7504004184831
At time: 1093.9873337745667 and batch: 850, loss is 5.773529348373413 and perplexity is 321.67102068044983
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.453482309977214 and perplexity of 233.57011489961116
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 1098.4010231494904 and batch: 50, loss is 5.777310886383057 and perplexity is 322.88973472619375
At time: 1100.094453573227 and batch: 100, loss is 5.723534202575683 and perplexity is 305.98442518710806
At time: 1101.7958016395569 and batch: 150, loss is 5.708376874923706 and perplexity is 301.3814912119471
At time: 1103.5137243270874 and batch: 200, loss is 5.7417346858978275 and perplexity is 311.6044783166055
At time: 1105.2358553409576 and batch: 250, loss is 5.775660810470581 and perplexity is 322.3573814849552
At time: 1106.9590272903442 and batch: 300, loss is 5.712050323486328 and perplexity is 302.4906365645867
At time: 1108.6817648410797 and batch: 350, loss is 5.685950536727905 and perplexity is 294.69783307422693
At time: 1110.4044406414032 and batch: 400, loss is 5.709331274032593 and perplexity is 301.6692667431059
At time: 1112.1261174678802 and batch: 450, loss is 5.732810411453247 and perplexity is 308.83600611084586
At time: 1113.88338804245 and batch: 500, loss is 5.731406755447388 and perplexity is 308.402810695841
At time: 1115.6249270439148 and batch: 550, loss is 5.69749158859253 and perplexity is 298.11865803940276
At time: 1117.3666560649872 and batch: 600, loss is 5.685462875366211 and perplexity is 294.5541553635431
At time: 1119.1095077991486 and batch: 650, loss is 5.708287229537964 and perplexity is 301.35447496287094
At time: 1120.8533606529236 and batch: 700, loss is 5.653840780258179 and perplexity is 285.3854664365498
At time: 1122.5965030193329 and batch: 750, loss is 5.6321653175354 and perplexity is 279.26616334217493
At time: 1124.3404545783997 and batch: 800, loss is 5.649327192306519 and perplexity is 284.1002566692315
At time: 1126.0827991962433 and batch: 850, loss is 5.672930278778076 and perplexity is 290.8856628455747
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.383832295735677 and perplexity of 217.85556465113848
Finished 35 epochs...
Completing Train Step...
At time: 1130.4468703269958 and batch: 50, loss is 5.727958498001098 and perplexity is 307.34118983036126
At time: 1132.159380197525 and batch: 100, loss is 5.678990135192871 and perplexity is 292.65373993159045
At time: 1133.8632974624634 and batch: 150, loss is 5.665483150482178 and perplexity is 288.7274462274472
At time: 1135.5869274139404 and batch: 200, loss is 5.706289587020874 and perplexity is 300.7530773396941
At time: 1137.3100626468658 and batch: 250, loss is 5.743458690643311 and perplexity is 312.1421492563913
At time: 1139.0383443832397 and batch: 300, loss is 5.684959325790405 and perplexity is 294.40587008118683
At time: 1140.781537771225 and batch: 350, loss is 5.6596715450286865 and perplexity is 287.054342644386
At time: 1142.523434638977 and batch: 400, loss is 5.684575109481812 and perplexity is 294.2927762721901
At time: 1144.2659847736359 and batch: 450, loss is 5.710810279846191 and perplexity is 302.11576744958137
At time: 1146.0067629814148 and batch: 500, loss is 5.710406894683838 and perplexity is 301.99392300845136
At time: 1147.746749162674 and batch: 550, loss is 5.681373414993286 and perplexity is 293.35204747908796
At time: 1149.4893398284912 and batch: 600, loss is 5.674383182525634 and perplexity is 291.3085988835122
At time: 1151.2305476665497 and batch: 650, loss is 5.6993600940704345 and perplexity is 298.6762151220294
At time: 1152.9730033874512 and batch: 700, loss is 5.650942678451538 and perplexity is 284.55958761917327
At time: 1154.7157537937164 and batch: 750, loss is 5.636483697891236 and perplexity is 280.47474854311434
At time: 1156.4819197654724 and batch: 800, loss is 5.660021553039551 and perplexity is 287.1548315487998
At time: 1158.2083480358124 and batch: 850, loss is 5.681811208724976 and perplexity is 293.4805032831739
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.379200617472331 and perplexity of 216.84886093135043
Finished 36 epochs...
Completing Train Step...
At time: 1162.5895578861237 and batch: 50, loss is 5.71674147605896 and perplexity is 303.9129999458368
At time: 1164.3160429000854 and batch: 100, loss is 5.667472553253174 and perplexity is 289.3024131397891
At time: 1166.027979850769 and batch: 150, loss is 5.6519073867797855 and perplexity is 284.8342370802429
At time: 1167.7456860542297 and batch: 200, loss is 5.693965063095093 and perplexity is 297.06918657214476
At time: 1169.4753239154816 and batch: 250, loss is 5.731919412612915 and perplexity is 308.56095614034695
At time: 1171.2225325107574 and batch: 300, loss is 5.673521146774292 and perplexity is 291.05758866203763
At time: 1172.9685423374176 and batch: 350, loss is 5.648908061981201 and perplexity is 283.9812065867236
At time: 1174.715478181839 and batch: 400, loss is 5.675088386535645 and perplexity is 291.51410332835803
At time: 1176.4606330394745 and batch: 450, loss is 5.701889324188232 and perplexity is 299.4325921233197
At time: 1178.2063128948212 and batch: 500, loss is 5.702584209442139 and perplexity is 299.6407357257098
At time: 1179.952053308487 and batch: 550, loss is 5.676221885681152 and perplexity is 291.8447216577823
At time: 1181.697345495224 and batch: 600, loss is 5.671900405883789 and perplexity is 290.5862417953498
At time: 1183.4442579746246 and batch: 650, loss is 5.698007593154907 and perplexity is 298.27252832255084
At time: 1185.1920411586761 and batch: 700, loss is 5.652242155075073 and perplexity is 284.9296065146705
At time: 1186.939397096634 and batch: 750, loss is 5.641057376861572 and perplexity is 281.7604880407721
At time: 1188.686819076538 and batch: 800, loss is 5.6653954219818115 and perplexity is 288.7021177126076
At time: 1190.4325816631317 and batch: 850, loss is 5.6853039360046385 and perplexity is 294.50734283441636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.377981821695964 and perplexity of 216.5847274506193
Finished 37 epochs...
Completing Train Step...
At time: 1194.7875318527222 and batch: 50, loss is 5.7105457210540775 and perplexity is 302.03585063888005
At time: 1196.521871805191 and batch: 100, loss is 5.66099401473999 and perplexity is 287.4342144471441
At time: 1198.2402474880219 and batch: 150, loss is 5.643932685852051 and perplexity is 282.5718023358858
At time: 1200.0053465366364 and batch: 200, loss is 5.687209348678589 and perplexity is 295.0690358165481
At time: 1201.7283704280853 and batch: 250, loss is 5.725241174697876 and perplexity is 306.50717810612633
At time: 1203.4748752117157 and batch: 300, loss is 5.667126035690307 and perplexity is 289.2021821395603
At time: 1205.2193794250488 and batch: 350, loss is 5.6430555438995365 and perplexity is 282.3240554239486
At time: 1206.9597499370575 and batch: 400, loss is 5.67053915977478 and perplexity is 290.1909515090955
At time: 1208.7041656970978 and batch: 450, loss is 5.697771396636963 and perplexity is 298.20208570944004
At time: 1210.4461414813995 and batch: 500, loss is 5.699124774932861 and perplexity is 298.60593916163674
At time: 1212.1893317699432 and batch: 550, loss is 5.67460036277771 and perplexity is 291.37187222906135
At time: 1213.9374859333038 and batch: 600, loss is 5.672008543014527 and perplexity is 290.617666656836
At time: 1215.6905817985535 and batch: 650, loss is 5.6984286403656 and perplexity is 298.3981415813279
At time: 1217.443359375 and batch: 700, loss is 5.654090833663941 and perplexity is 285.45683696723745
At time: 1219.1964392662048 and batch: 750, loss is 5.644022045135498 and perplexity is 282.5970538778778
At time: 1220.9497547149658 and batch: 800, loss is 5.66819278717041 and perplexity is 289.5108536040062
At time: 1222.7030186653137 and batch: 850, loss is 5.687210607528686 and perplexity is 295.06940726446624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.377430597941081 and perplexity of 216.4653737022365
Finished 38 epochs...
Completing Train Step...
At time: 1227.1260664463043 and batch: 50, loss is 5.706727228164673 and perplexity is 300.88472806624816
At time: 1228.8221390247345 and batch: 100, loss is 5.657077465057373 and perplexity is 286.31066571919445
At time: 1230.5391416549683 and batch: 150, loss is 5.639280338287353 and perplexity is 281.26023340237225
At time: 1232.2526664733887 and batch: 200, loss is 5.68313835144043 and perplexity is 293.8702523643554
At time: 1233.9685776233673 and batch: 250, loss is 5.7213289833068846 and perplexity is 305.3104058900497
At time: 1235.711138010025 and batch: 300, loss is 5.663350419998169 and perplexity is 288.1123245788124
At time: 1237.4521102905273 and batch: 350, loss is 5.639662752151489 and perplexity is 281.3678117834766
At time: 1239.1928944587708 and batch: 400, loss is 5.6680167770385745 and perplexity is 289.4599012446928
At time: 1240.9345421791077 and batch: 450, loss is 5.695428075790406 and perplexity is 297.5041206429416
At time: 1242.6779899597168 and batch: 500, loss is 5.697465276718139 and perplexity is 298.11081408191404
At time: 1244.4502820968628 and batch: 550, loss is 5.673533239364624 and perplexity is 291.0611083235011
At time: 1246.1954894065857 and batch: 600, loss is 5.672162103652954 and perplexity is 290.6622975179501
At time: 1247.9423241615295 and batch: 650, loss is 5.699001369476318 and perplexity is 298.5690918330154
At time: 1249.6886568069458 and batch: 700, loss is 5.655262899398804 and perplexity is 285.79160729271905
At time: 1251.433117866516 and batch: 750, loss is 5.645894613265991 and perplexity is 283.1267318890456
At time: 1253.1757440567017 and batch: 800, loss is 5.669639987945557 and perplexity is 289.9301372563899
At time: 1254.9173729419708 and batch: 850, loss is 5.687979412078858 and perplexity is 295.2963451916652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.377041498819987 and perplexity of 216.38116359968262
Finished 39 epochs...
Completing Train Step...
At time: 1259.3164722919464 and batch: 50, loss is 5.703941946029663 and perplexity is 300.0478452269092
At time: 1261.02423620224 and batch: 100, loss is 5.654312353134156 and perplexity is 285.52007821885735
At time: 1262.7634444236755 and batch: 150, loss is 5.636316413879395 and perplexity is 280.4278335261335
At time: 1264.512549161911 and batch: 200, loss is 5.680276365280151 and perplexity is 293.03040216226367
At time: 1266.2658746242523 and batch: 250, loss is 5.718594751358032 and perplexity is 304.476756638522
At time: 1268.0136938095093 and batch: 300, loss is 5.660787296295166 and perplexity is 287.37480263431473
At time: 1269.7578020095825 and batch: 350, loss is 5.6374578952789305 and perplexity is 280.7481194474494
At time: 1271.5008172988892 and batch: 400, loss is 5.666318082809449 and perplexity is 288.9686147717697
At time: 1273.2445032596588 and batch: 450, loss is 5.693958835601807 and perplexity is 297.0673365815402
At time: 1274.989027261734 and batch: 500, loss is 5.696315011978149 and perplexity is 297.768104864817
At time: 1276.7385580539703 and batch: 550, loss is 5.672838888168335 and perplexity is 290.85907984221916
At time: 1278.4912984371185 and batch: 600, loss is 5.672076063156128 and perplexity is 290.63728986531396
At time: 1280.2475893497467 and batch: 650, loss is 5.6994763851165775 and perplexity is 298.7109505112122
At time: 1282.0150282382965 and batch: 700, loss is 5.65612608909607 and perplexity is 286.03840616549724
At time: 1283.7698106765747 and batch: 750, loss is 5.646984081268311 and perplexity is 283.4353574923732
At time: 1285.5250165462494 and batch: 800, loss is 5.6703102588653564 and perplexity is 290.1245341381769
At time: 1287.2838916778564 and batch: 850, loss is 5.688116121292114 and perplexity is 295.3367176822767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.3767515818278 and perplexity of 216.31844011630693
Finished 40 epochs...
Completing Train Step...
At time: 1291.8160407543182 and batch: 50, loss is 5.701570501327515 and perplexity is 299.33714138445424
At time: 1293.4954664707184 and batch: 100, loss is 5.652259855270386 and perplexity is 284.93464986899033
At time: 1295.1885430812836 and batch: 150, loss is 5.634098806381226 and perplexity is 279.8066436920007
At time: 1296.8855912685394 and batch: 200, loss is 5.678185720443725 and perplexity is 292.418419607056
At time: 1298.6048753261566 and batch: 250, loss is 5.716576118469238 and perplexity is 303.8627497794182
At time: 1300.335300207138 and batch: 300, loss is 5.658977918624878 and perplexity is 286.8553032104834
At time: 1302.076507806778 and batch: 350, loss is 5.635840673446655 and perplexity is 280.2944543966975
At time: 1303.8226726055145 and batch: 400, loss is 5.665055303573609 and perplexity is 288.60394150460064
At time: 1305.5630650520325 and batch: 450, loss is 5.692811574935913 and perplexity is 296.72671833747546
At time: 1307.303899526596 and batch: 500, loss is 5.695278015136719 and perplexity is 297.4594803295844
At time: 1309.04412150383 and batch: 550, loss is 5.672134237289429 and perplexity is 290.654197929558
At time: 1310.7825548648834 and batch: 600, loss is 5.671867160797119 and perplexity is 290.5765813911377
At time: 1312.5221281051636 and batch: 650, loss is 5.699678382873535 and perplexity is 298.77129554776997
At time: 1314.272747516632 and batch: 700, loss is 5.656564302444458 and perplexity is 286.1637794813547
At time: 1316.0227901935577 and batch: 750, loss is 5.647595081329346 and perplexity is 283.6085894300735
At time: 1317.772463798523 and batch: 800, loss is 5.670432176589966 and perplexity is 290.1599076175156
At time: 1319.5224850177765 and batch: 850, loss is 5.6878502368927 and perplexity is 295.25820269488855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.3765004475911455 and perplexity of 216.26412197083255
Finished 41 epochs...
Completing Train Step...
At time: 1323.9480786323547 and batch: 50, loss is 5.699674425125122 and perplexity is 298.770113088489
At time: 1325.6869013309479 and batch: 100, loss is 5.6504474449157716 and perplexity is 284.41869905771006
At time: 1327.3976011276245 and batch: 150, loss is 5.632433271408081 and perplexity is 279.34100381860105
At time: 1329.1094071865082 and batch: 200, loss is 5.676457395553589 and perplexity is 291.9134620651558
At time: 1330.8241429328918 and batch: 250, loss is 5.714932451248169 and perplexity is 303.3637107772788
At time: 1332.5730595588684 and batch: 300, loss is 5.657470760345459 and perplexity is 286.42329250128995
At time: 1334.2914519309998 and batch: 350, loss is 5.634422645568848 and perplexity is 279.89727072168506
At time: 1336.0216426849365 and batch: 400, loss is 5.66384711265564 and perplexity is 288.25546340001557
At time: 1337.7631735801697 and batch: 450, loss is 5.691782703399658 and perplexity is 296.4215816630688
At time: 1339.50315451622 and batch: 500, loss is 5.6943079948425295 and perplexity is 297.17107849739483
At time: 1341.2439818382263 and batch: 550, loss is 5.6714317321777346 and perplexity is 290.45008357386064
At time: 1342.9856204986572 and batch: 600, loss is 5.671640310287476 and perplexity is 290.5106714216964
At time: 1344.7252986431122 and batch: 650, loss is 5.6997819519042965 and perplexity is 298.80224060371626
At time: 1346.4644255638123 and batch: 700, loss is 5.656836910247803 and perplexity is 286.2418005947749
At time: 1348.20334649086 and batch: 750, loss is 5.647934408187866 and perplexity is 283.70484177135285
At time: 1349.9507360458374 and batch: 800, loss is 5.670185899734497 and perplexity is 290.0884567465934
At time: 1351.699936389923 and batch: 850, loss is 5.6875986480712895 and perplexity is 295.18392837534674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.376274108886719 and perplexity of 216.21517856875374
Finished 42 epochs...
Completing Train Step...
At time: 1356.0884156227112 and batch: 50, loss is 5.697985620498657 and perplexity is 298.26597455481925
At time: 1357.817953824997 and batch: 100, loss is 5.649040107727051 and perplexity is 284.01870757282296
At time: 1359.5293397903442 and batch: 150, loss is 5.630889596939087 and perplexity is 278.91012489682157
At time: 1361.2506439685822 and batch: 200, loss is 5.674898242950439 and perplexity is 291.4586790610646
At time: 1363.004609823227 and batch: 250, loss is 5.713505821228027 and perplexity is 302.9312315677253
At time: 1364.746927022934 and batch: 300, loss is 5.656065502166748 and perplexity is 286.0210765017805
At time: 1366.4860889911652 and batch: 350, loss is 5.633086471557617 and perplexity is 279.5235290103185
At time: 1368.2259328365326 and batch: 400, loss is 5.662763833999634 and perplexity is 287.94337148081433
At time: 1369.965814590454 and batch: 450, loss is 5.690778913497925 and perplexity is 296.12418595911305
At time: 1371.705855846405 and batch: 500, loss is 5.693389072418213 and perplexity is 296.89812675944864
At time: 1373.4442899227142 and batch: 550, loss is 5.670808324813843 and perplexity is 290.26907128097815
At time: 1375.2092015743256 and batch: 600, loss is 5.671308612823486 and perplexity is 290.41432574841497
At time: 1376.9289481639862 and batch: 650, loss is 5.699717540740966 and perplexity is 298.78299502361506
At time: 1378.6500506401062 and batch: 700, loss is 5.656883163452148 and perplexity is 286.25504050146174
At time: 1380.37233543396 and batch: 750, loss is 5.647926244735718 and perplexity is 283.7025257699062
At time: 1382.1193182468414 and batch: 800, loss is 5.669811944961548 and perplexity is 289.97999706438765
At time: 1383.8691523075104 and batch: 850, loss is 5.687023782730103 and perplexity is 295.0142861310436
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.375970204671224 and perplexity of 216.1494798480974
Finished 43 epochs...
Completing Train Step...
At time: 1388.289300441742 and batch: 50, loss is 5.696527328491211 and perplexity is 297.83133266245807
At time: 1390.0296084880829 and batch: 100, loss is 5.64771089553833 and perplexity is 283.641437236604
At time: 1391.7437977790833 and batch: 150, loss is 5.629598770141602 and perplexity is 278.5503324983076
At time: 1393.4669122695923 and batch: 200, loss is 5.673578958511353 and perplexity is 291.0744156932181
At time: 1395.1891617774963 and batch: 250, loss is 5.712270612716675 and perplexity is 302.55727933417535
At time: 1396.9295556545258 and batch: 300, loss is 5.65487380027771 and perplexity is 285.6804276608567
At time: 1398.683422088623 and batch: 350, loss is 5.631928339004516 and perplexity is 279.19999109807077
At time: 1400.4403884410858 and batch: 400, loss is 5.661716175079346 and perplexity is 287.64186300574414
At time: 1402.1976170539856 and batch: 450, loss is 5.689904689788818 and perplexity is 295.8654203009249
At time: 1403.952752828598 and batch: 500, loss is 5.692567300796509 and perplexity is 296.65424452581476
At time: 1405.7065889835358 and batch: 550, loss is 5.670281267166137 and perplexity is 290.11612305687345
At time: 1407.4594671726227 and batch: 600, loss is 5.6709404468536375 and perplexity is 290.3074247563791
At time: 1409.2134239673615 and batch: 650, loss is 5.699460048675537 and perplexity is 298.70607067724063
At time: 1410.9668691158295 and batch: 700, loss is 5.656894216537475 and perplexity is 286.25820452033577
At time: 1412.7201969623566 and batch: 750, loss is 5.647690839767456 and perplexity is 283.6357486459733
At time: 1414.4726810455322 and batch: 800, loss is 5.669406290054321 and perplexity is 289.8623891113152
At time: 1416.2254660129547 and batch: 850, loss is 5.686432180404663 and perplexity is 294.83980660966455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.375576655069987 and perplexity of 216.0644310430551
Finished 44 epochs...
Completing Train Step...
At time: 1420.6505374908447 and batch: 50, loss is 5.695120935440063 and perplexity is 297.4127591542171
At time: 1422.343591928482 and batch: 100, loss is 5.646622085571289 and perplexity is 283.33277368114653
At time: 1424.0378637313843 and batch: 150, loss is 5.6283590126037595 and perplexity is 278.2052116012454
At time: 1425.742458820343 and batch: 200, loss is 5.672328443527221 and perplexity is 290.71065026935736
At time: 1427.451732635498 and batch: 250, loss is 5.710900106430054 and perplexity is 302.1429066957971
At time: 1429.1649329662323 and batch: 300, loss is 5.6536636066436765 and perplexity is 285.3349081408651
At time: 1430.8845682144165 and batch: 350, loss is 5.63077151298523 and perplexity is 278.8771920309626
At time: 1432.602828502655 and batch: 400, loss is 5.660743789672852 and perplexity is 287.36230019928513
At time: 1434.3223366737366 and batch: 450, loss is 5.689070377349854 and perplexity is 295.61867904447064
At time: 1436.0438799858093 and batch: 500, loss is 5.691696710586548 and perplexity is 296.3960926333474
At time: 1437.7699377536774 and batch: 550, loss is 5.669654712677002 and perplexity is 289.9344064312302
At time: 1439.5102784633636 and batch: 600, loss is 5.670548658370972 and perplexity is 290.19370792885354
At time: 1441.249828338623 and batch: 650, loss is 5.699080829620361 and perplexity is 298.5928171186542
At time: 1442.993057012558 and batch: 700, loss is 5.656713542938232 and perplexity is 286.2064898920868
At time: 1444.7428028583527 and batch: 750, loss is 5.647400245666504 and perplexity is 283.55333774523837
At time: 1446.491013288498 and batch: 800, loss is 5.669165925979614 and perplexity is 289.79272497907687
At time: 1448.2409341335297 and batch: 850, loss is 5.685836629867554 and perplexity is 294.6642668810553
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.375234603881836 and perplexity of 215.9905385859214
Finished 45 epochs...
Completing Train Step...
At time: 1452.6451570987701 and batch: 50, loss is 5.693909273147583 and perplexity is 297.05261356012716
At time: 1454.3494050502777 and batch: 100, loss is 5.645385684967041 and perplexity is 282.98267734282496
At time: 1456.0653405189514 and batch: 150, loss is 5.627198123931885 and perplexity is 277.88243371348744
At time: 1457.7848091125488 and batch: 200, loss is 5.671323375701904 and perplexity is 290.4186131314439
At time: 1459.5148887634277 and batch: 250, loss is 5.709603443145752 and perplexity is 301.7513829741466
At time: 1461.2551567554474 and batch: 300, loss is 5.652552146911621 and perplexity is 285.0179460582421
At time: 1463.02094912529 and batch: 350, loss is 5.629734783172608 and perplexity is 278.58822154996244
At time: 1464.7631368637085 and batch: 400, loss is 5.659801416397094 and perplexity is 287.0916252055889
At time: 1466.5056471824646 and batch: 450, loss is 5.688355131149292 and perplexity is 295.40731450532155
At time: 1468.2525925636292 and batch: 500, loss is 5.691002855300903 and perplexity is 296.19050796908016
At time: 1470.0047109127045 and batch: 550, loss is 5.669190492630005 and perplexity is 289.7998443030857
At time: 1471.756599187851 and batch: 600, loss is 5.670190925598145 and perplexity is 290.0899146952866
At time: 1473.509033203125 and batch: 650, loss is 5.698655195236206 and perplexity is 298.46575279221827
At time: 1475.2618708610535 and batch: 700, loss is 5.656424856185913 and perplexity is 286.1238777951099
At time: 1477.0255012512207 and batch: 750, loss is 5.6471554470062255 and perplexity is 283.48393276351067
At time: 1478.783822774887 and batch: 800, loss is 5.668861389160156 and perplexity is 289.7044858610218
At time: 1480.5421175956726 and batch: 850, loss is 5.685426025390625 and perplexity is 294.54330125009744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.374900182088216 and perplexity of 215.91831871922426
Finished 46 epochs...
Completing Train Step...
At time: 1484.9820926189423 and batch: 50, loss is 5.692802906036377 and perplexity is 296.724146054514
At time: 1486.6821467876434 and batch: 100, loss is 5.64428144454956 and perplexity is 282.6703688965949
At time: 1488.3933551311493 and batch: 150, loss is 5.626080913543701 and perplexity is 277.57215392802186
At time: 1490.1113262176514 and batch: 200, loss is 5.670197563171387 and perplexity is 290.0918401947325
At time: 1491.8368728160858 and batch: 250, loss is 5.7083168220520015 and perplexity is 301.3633929313535
At time: 1493.5642144680023 and batch: 300, loss is 5.651470785140991 and perplexity is 284.70990512929063
At time: 1495.290183544159 and batch: 350, loss is 5.628755521774292 and perplexity is 278.3155443914713
At time: 1497.0177624225616 and batch: 400, loss is 5.659008016586304 and perplexity is 286.8639371002649
At time: 1498.7445466518402 and batch: 450, loss is 5.687634029388428 and perplexity is 295.19437255629407
At time: 1500.4714851379395 and batch: 500, loss is 5.690090579986572 and perplexity is 295.92042389456924
At time: 1502.2069120407104 and batch: 550, loss is 5.668569736480713 and perplexity is 289.6200050915938
At time: 1503.954270362854 and batch: 600, loss is 5.669584951400757 and perplexity is 289.9141809424962
At time: 1505.7013683319092 and batch: 650, loss is 5.6982808113098145 and perplexity is 298.35403292616036
At time: 1507.4773917198181 and batch: 700, loss is 5.656374397277832 and perplexity is 286.10944066090474
At time: 1509.2375779151917 and batch: 750, loss is 5.6471523189544675 and perplexity is 283.4830460124833
At time: 1510.9953932762146 and batch: 800, loss is 5.668279390335083 and perplexity is 289.53592724584826
At time: 1512.7533111572266 and batch: 850, loss is 5.684660720825195 and perplexity is 294.3179721506259
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.374490737915039 and perplexity of 215.8299303180353
Finished 47 epochs...
Completing Train Step...
At time: 1517.148000717163 and batch: 50, loss is 5.6913793182373045 and perplexity is 296.302033708781
At time: 1518.879545211792 and batch: 100, loss is 5.64287091255188 and perplexity is 282.2719343648585
At time: 1520.601580619812 and batch: 150, loss is 5.624753980636597 and perplexity is 277.2040785625438
At time: 1522.3344812393188 and batch: 200, loss is 5.669022598266602 and perplexity is 289.7511926269975
At time: 1524.0770859718323 and batch: 250, loss is 5.706794567108155 and perplexity is 300.9049900081472
At time: 1525.8184299468994 and batch: 300, loss is 5.650394229888916 and perplexity is 284.40356411170836
At time: 1527.5645916461945 and batch: 350, loss is 5.62769944190979 and perplexity is 278.0217760977549
At time: 1529.3057556152344 and batch: 400, loss is 5.658212299346924 and perplexity is 286.6357653123096
At time: 1531.0526013374329 and batch: 450, loss is 5.686756391525268 and perplexity is 294.93541245116563
At time: 1532.8048346042633 and batch: 500, loss is 5.689190368652344 and perplexity is 295.6541528430408
At time: 1534.5560851097107 and batch: 550, loss is 5.667920761108398 and perplexity is 289.43210981725724
At time: 1536.306747674942 and batch: 600, loss is 5.669031801223755 and perplexity is 289.7538592070787
At time: 1538.05552983284 and batch: 650, loss is 5.6977778244018555 and perplexity is 298.20400248849785
At time: 1539.803879737854 and batch: 700, loss is 5.6557756042480465 and perplexity is 285.938171604557
At time: 1541.5523512363434 and batch: 750, loss is 5.646514253616333 and perplexity is 283.30222300145664
At time: 1543.301805973053 and batch: 800, loss is 5.667399005889893 and perplexity is 289.281136492542
At time: 1545.0552191734314 and batch: 850, loss is 5.683986005783081 and perplexity is 294.1194583653007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.3736623128255205 and perplexity of 215.6512054290033
Finished 48 epochs...
Completing Train Step...
At time: 1549.4477014541626 and batch: 50, loss is 5.690018196105957 and perplexity is 295.8990048011422
At time: 1551.1671242713928 and batch: 100, loss is 5.641644496917724 and perplexity is 281.92596384666075
At time: 1552.8582129478455 and batch: 150, loss is 5.623519849777222 and perplexity is 276.8621834698818
At time: 1554.549371957779 and batch: 200, loss is 5.667871742248535 and perplexity is 289.4179225329514
At time: 1556.2624127864838 and batch: 250, loss is 5.705492992401123 and perplexity is 300.51359445446457
At time: 1557.979421377182 and batch: 300, loss is 5.649302082061768 and perplexity is 284.093122931818
At time: 1559.6956510543823 and batch: 350, loss is 5.626649522781372 and perplexity is 277.73002889919553
At time: 1561.4257662296295 and batch: 400, loss is 5.657174100875855 and perplexity is 286.33833492161267
At time: 1563.1642758846283 and batch: 450, loss is 5.686014051437378 and perplexity is 294.71655131591257
At time: 1564.9045884609222 and batch: 500, loss is 5.688808317184448 and perplexity is 295.5412193145415
At time: 1566.644809961319 and batch: 550, loss is 5.66698823928833 and perplexity is 289.1623338649551
At time: 1568.3853747844696 and batch: 600, loss is 5.668337965011597 and perplexity is 289.55288721583327
At time: 1570.1270668506622 and batch: 650, loss is 5.6971157360076905 and perplexity is 298.0066304254428
At time: 1571.8674368858337 and batch: 700, loss is 5.655283098220825 and perplexity is 285.7973800048307
At time: 1573.6077225208282 and batch: 750, loss is 5.645926809310913 and perplexity is 283.1358475967681
At time: 1575.3476750850677 and batch: 800, loss is 5.6666662311553955 and perplexity is 289.0692362316141
At time: 1577.0884928703308 and batch: 850, loss is 5.683096714019776 and perplexity is 293.85801661977405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.37306022644043 and perplexity of 215.52140385408777
Finished 49 epochs...
Completing Train Step...
At time: 1581.4990379810333 and batch: 50, loss is 5.688884592056274 and perplexity is 295.56376254289387
At time: 1583.210921049118 and batch: 100, loss is 5.640669164657592 and perplexity is 281.6511264098487
At time: 1584.936232805252 and batch: 150, loss is 5.622445325851441 and perplexity is 276.5648482051361
At time: 1586.676510810852 and batch: 200, loss is 5.666749973297119 and perplexity is 289.0934445221754
At time: 1588.4177629947662 and batch: 250, loss is 5.704358119964599 and perplexity is 300.17274330718647
At time: 1590.1669518947601 and batch: 300, loss is 5.648125143051147 and perplexity is 283.75895933644944
At time: 1591.9175617694855 and batch: 350, loss is 5.625788202285767 and perplexity is 277.49091732369243
At time: 1593.666868686676 and batch: 400, loss is 5.656470022201538 and perplexity is 286.13680116249145
At time: 1595.444462299347 and batch: 450, loss is 5.68546498298645 and perplexity is 294.5547761724967
At time: 1597.194857120514 and batch: 500, loss is 5.68777985572815 and perplexity is 295.2374228100046
At time: 1598.9451968669891 and batch: 550, loss is 5.666141948699951 and perplexity is 288.9177220242401
At time: 1600.6966047286987 and batch: 600, loss is 5.667822389602661 and perplexity is 289.4036393451705
At time: 1602.4467356204987 and batch: 650, loss is 5.6965688133239745 and perplexity is 297.84368840177143
At time: 1604.1963334083557 and batch: 700, loss is 5.654865951538086 and perplexity is 285.6781854383635
At time: 1605.9476997852325 and batch: 750, loss is 5.645342988967895 and perplexity is 282.97059537260253
At time: 1607.6978092193604 and batch: 800, loss is 5.666059312820434 and perplexity is 288.89384804061
At time: 1609.4470269680023 and batch: 850, loss is 5.68218300819397 and perplexity is 293.589639465585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.371986389160156 and perplexity of 215.2900931531869
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f37e976d898>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 4.002509243384625, 'batch_size': 50, 'dropout': 0.40333487804001933, 'wordvec_source': 'glove', 'data': 'wikitext', 'lr': 29.518967871004783, 'num_layers': 1, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.065471887588501 and batch: 50, loss is 7.710938272476196 and perplexity is 2232.63609673544
At time: 3.6620399951934814 and batch: 100, loss is 6.367485122680664 and perplexity is 582.5908405252973
At time: 5.252479314804077 and batch: 150, loss is 6.339196691513061 and perplexity is 566.3411819234504
At time: 6.843307256698608 and batch: 200, loss is 6.456621313095093 and perplexity is 636.9055128505084
At time: 8.433695316314697 and batch: 250, loss is 6.6219940090179445 and perplexity is 751.4419863077864
At time: 10.022003412246704 and batch: 300, loss is 6.66669080734253 and perplexity is 785.7909635233971
At time: 11.612702369689941 and batch: 350, loss is 6.7386003017425535 and perplexity is 844.3780338685441
At time: 13.200313329696655 and batch: 400, loss is 6.9917334461212155 and perplexity is 1087.6051480176736
At time: 14.78758955001831 and batch: 450, loss is 7.389512643814087 and perplexity is 1618.916931462052
At time: 16.403542518615723 and batch: 500, loss is 7.756604518890381 and perplexity is 2336.9560282431885
At time: 17.990949392318726 and batch: 550, loss is 7.519780006408691 and perplexity is 1844.161545702512
At time: 19.58084225654602 and batch: 600, loss is 7.445785837173462 and perplexity is 1712.6306122816447
At time: 21.185852527618408 and batch: 650, loss is 7.473541803359986 and perplexity is 1760.8321756525038
At time: 22.796942472457886 and batch: 700, loss is 7.165629796981811 and perplexity is 1294.1764125899076
At time: 24.46886134147644 and batch: 750, loss is 7.489002695083618 and perplexity is 1788.2677540042391
At time: 26.224875926971436 and batch: 800, loss is 7.52878589630127 and perplexity is 1860.8448728712492
At time: 27.985308408737183 and batch: 850, loss is 7.392306680679321 and perplexity is 1623.4465700921962
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.85700798034668 and perplexity of 950.5188375931601
Finished 1 epochs...
Completing Train Step...
At time: 32.53561210632324 and batch: 50, loss is 7.252991943359375 and perplexity is 1412.3241269285807
At time: 34.28401064872742 and batch: 100, loss is 6.8518756484985355 and perplexity is 945.6529568290674
At time: 36.032079458236694 and batch: 150, loss is 7.019110803604126 and perplexity is 1117.7922388828044
At time: 37.78161144256592 and batch: 200, loss is 7.061468019485473 and perplexity is 1166.1558493170908
At time: 39.527589559555054 and batch: 250, loss is 7.026583852767945 and perplexity is 1126.1768455068213
At time: 41.30222272872925 and batch: 300, loss is 6.833137083053589 and perplexity is 928.097770198104
At time: 43.05121350288391 and batch: 350, loss is 7.053502140045166 and perplexity is 1156.9032937109307
At time: 44.804452896118164 and batch: 400, loss is 6.9588649940490725 and perplexity is 1052.4383552937236
At time: 46.55159091949463 and batch: 450, loss is 6.949980716705323 and perplexity is 1043.129613010646
At time: 48.29924011230469 and batch: 500, loss is 6.772807779312134 and perplexity is 873.7617832658477
At time: 50.04669666290283 and batch: 550, loss is 6.6766602897644045 and perplexity is 793.6640729251036
At time: 51.79463863372803 and batch: 600, loss is 6.813711519241333 and perplexity is 910.2429293745773
At time: 53.55123043060303 and batch: 650, loss is 6.867662324905395 and perplexity is 960.7001340077665
At time: 55.30532646179199 and batch: 700, loss is 6.780756702423096 and perplexity is 880.7349262704327
At time: 57.058597803115845 and batch: 750, loss is 6.950793704986572 and perplexity is 1043.9780099835173
At time: 58.806294441223145 and batch: 800, loss is 7.025323839187622 and perplexity is 1124.7587409907312
At time: 60.55292463302612 and batch: 850, loss is 6.822399606704712 and perplexity is 918.1856531357079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.678487141927083 and perplexity of 795.115305042823
Finished 2 epochs...
Completing Train Step...
At time: 65.01956343650818 and batch: 50, loss is 6.835137357711792 and perplexity is 929.9560785921541
At time: 66.76429724693298 and batch: 100, loss is 6.703396415710449 and perplexity is 815.1697841953325
At time: 68.51285576820374 and batch: 150, loss is 6.549049339294434 and perplexity is 698.5797457291193
At time: 70.25835132598877 and batch: 200, loss is 6.757034778594971 and perplexity is 860.0880593468714
At time: 72.00486755371094 and batch: 250, loss is 7.020276327133178 and perplexity is 1119.0958115625601
At time: 73.77950382232666 and batch: 300, loss is 6.867691440582275 and perplexity is 960.7281058496544
At time: 75.52546525001526 and batch: 350, loss is 6.915338764190674 and perplexity is 1007.6123126572597
At time: 77.27070212364197 and batch: 400, loss is 6.792261829376221 and perplexity is 890.9264081370374
At time: 79.0155258178711 and batch: 450, loss is 6.7670989418029786 and perplexity is 868.7878304763614
At time: 80.75905728340149 and batch: 500, loss is 6.736159286499023 and perplexity is 842.3194078075697
At time: 82.50575590133667 and batch: 550, loss is 6.938206949234009 and perplexity is 1030.920064737699
At time: 84.2513976097107 and batch: 600, loss is 6.81391978263855 and perplexity is 910.4325194009875
At time: 85.99868631362915 and batch: 650, loss is 6.810756540298462 and perplexity is 907.5571508492005
At time: 87.74580502510071 and batch: 700, loss is 6.713914203643799 and perplexity is 823.7888142244723
At time: 89.49113774299622 and batch: 750, loss is 6.657187299728394 and perplexity is 778.358565980776
At time: 91.23904418945312 and batch: 800, loss is 6.9724190807342525 and perplexity is 1066.8003076933207
At time: 92.98644757270813 and batch: 850, loss is 6.8798158073425295 and perplexity is 972.447225656051
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 7.103953043619792 and perplexity of 1216.7675151282172
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 97.4690272808075 and batch: 50, loss is 6.226683826446533 and perplexity is 506.0744670015299
At time: 99.22332239151001 and batch: 100, loss is 5.956659135818481 and perplexity is 386.31733218110577
At time: 100.9762966632843 and batch: 150, loss is 5.929023342132568 and perplexity is 375.787318900536
At time: 102.72846746444702 and batch: 200, loss is 5.966452932357788 and perplexity is 390.1194336452512
At time: 104.48371648788452 and batch: 250, loss is 6.0154884147644045 and perplexity is 409.72590623670374
At time: 106.23718285560608 and batch: 300, loss is 5.994573745727539 and perplexity is 401.2456148763915
At time: 107.99027848243713 and batch: 350, loss is 5.9652956676483155 and perplexity is 389.66822332746034
At time: 109.74435758590698 and batch: 400, loss is 5.973472108840943 and perplexity is 392.8673836903327
At time: 111.49816942214966 and batch: 450, loss is 5.980779418945312 and perplexity is 395.74870201502205
At time: 113.25270009040833 and batch: 500, loss is 5.994804983139038 and perplexity is 401.33840860202776
At time: 115.00682973861694 and batch: 550, loss is 5.971984796524048 and perplexity is 392.28350150687896
At time: 116.76068997383118 and batch: 600, loss is 5.974600009918213 and perplexity is 393.31074922485055
At time: 118.53718876838684 and batch: 650, loss is 5.988128795623779 and perplexity is 398.6679223814605
At time: 120.28200697898865 and batch: 700, loss is 5.937637786865235 and perplexity is 379.03850145117434
At time: 122.0275228023529 and batch: 750, loss is 5.95833083152771 and perplexity is 386.9636773033763
At time: 123.77071261405945 and batch: 800, loss is 5.972639427185059 and perplexity is 392.5403863879602
At time: 125.51673936843872 and batch: 850, loss is 5.962766313552857 and perplexity is 388.6838598376287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.60374641418457 and perplexity of 271.44143692806455
Finished 4 epochs...
Completing Train Step...
At time: 129.9558343887329 and batch: 50, loss is 5.97152626991272 and perplexity is 392.1036703140976
At time: 131.67052936553955 and batch: 100, loss is 5.888674764633179 and perplexity is 360.926654526329
At time: 133.38155817985535 and batch: 150, loss is 5.901297445297241 and perplexity is 365.51139138976936
At time: 135.092942237854 and batch: 200, loss is 5.9269635868072506 and perplexity is 375.0140855783463
At time: 136.83659601211548 and batch: 250, loss is 5.969818601608276 and perplexity is 391.43465869179477
At time: 138.5860641002655 and batch: 300, loss is 5.944234504699707 and perplexity is 381.5471769082829
At time: 140.32768511772156 and batch: 350, loss is 5.917595796585083 and perplexity is 371.51743580587953
At time: 142.06795120239258 and batch: 400, loss is 5.9216227722167964 and perplexity is 373.0165438766009
At time: 143.818044424057 and batch: 450, loss is 5.929603986740112 and perplexity is 376.0055811411031
At time: 145.5673108100891 and batch: 500, loss is 5.946992511749268 and perplexity is 382.6009391858686
At time: 147.3165066242218 and batch: 550, loss is 5.921200046539306 and perplexity is 372.8588935291483
At time: 149.06696772575378 and batch: 600, loss is 5.919427404403686 and perplexity is 372.1985336076671
At time: 150.81697297096252 and batch: 650, loss is 5.934917287826538 and perplexity is 378.0087289550123
At time: 152.56921577453613 and batch: 700, loss is 5.8872528171539305 and perplexity is 360.4138004918956
At time: 154.3238229751587 and batch: 750, loss is 5.918511619567871 and perplexity is 371.8578358614023
At time: 156.07499432563782 and batch: 800, loss is 5.924757165908813 and perplexity is 374.18755883097674
At time: 157.82544589042664 and batch: 850, loss is 5.917171010971069 and perplexity is 371.3596540578749
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.541146596272786 and perplexity of 254.97017977791666
Finished 5 epochs...
Completing Train Step...
At time: 162.26224756240845 and batch: 50, loss is 5.909318447113037 and perplexity is 368.454948279893
At time: 163.96326971054077 and batch: 100, loss is 5.831738128662109 and perplexity is 340.95078067255645
At time: 165.67470359802246 and batch: 150, loss is 5.84607050895691 and perplexity is 345.8726033884846
At time: 167.39707255363464 and batch: 200, loss is 5.8730082321167 and perplexity is 355.3162479170753
At time: 169.13627552986145 and batch: 250, loss is 5.904586982727051 and perplexity is 366.71573457273524
At time: 170.8758590221405 and batch: 300, loss is 5.8760896587371825 and perplexity is 356.41281749376157
At time: 172.61695623397827 and batch: 350, loss is 5.86098970413208 and perplexity is 351.0714289808029
At time: 174.35636806488037 and batch: 400, loss is 5.872391242980957 and perplexity is 355.0970892685624
At time: 176.09430480003357 and batch: 450, loss is 5.8881982231140135 and perplexity is 360.75469896531
At time: 177.83877277374268 and batch: 500, loss is 5.899914779663086 and perplexity is 365.0063605748368
At time: 179.57711243629456 and batch: 550, loss is 5.868702392578125 and perplexity is 353.7896022723844
At time: 181.31679344177246 and batch: 600, loss is 5.876965408325195 and perplexity is 356.7250825848993
At time: 183.05778050422668 and batch: 650, loss is 5.897591571807862 and perplexity is 364.15935919183534
At time: 184.80104064941406 and batch: 700, loss is 5.850387849807739 and perplexity is 347.3690813866158
At time: 186.544189453125 and batch: 750, loss is 5.8721507549285885 and perplexity is 355.01170292877146
At time: 188.2846794128418 and batch: 800, loss is 5.885659646987915 and perplexity is 359.8400571342877
At time: 190.0265142917633 and batch: 850, loss is 5.8763431930542 and perplexity is 356.50319183003467
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.520592371622722 and perplexity of 249.7829577093957
Finished 6 epochs...
Completing Train Step...
At time: 194.43938612937927 and batch: 50, loss is 5.872062044143677 and perplexity is 354.98021095881114
At time: 196.1557924747467 and batch: 100, loss is 5.807016792297364 and perplexity is 332.6253535804001
At time: 197.87530636787415 and batch: 150, loss is 5.825588703155518 and perplexity is 338.8605626522715
At time: 199.6049063205719 and batch: 200, loss is 5.847301578521728 and perplexity is 346.2986588219509
At time: 201.35405349731445 and batch: 250, loss is 5.875776996612549 and perplexity is 356.30139812421345
At time: 203.10382342338562 and batch: 300, loss is 5.8510425090789795 and perplexity is 347.59656423003946
At time: 204.85211610794067 and batch: 350, loss is 5.836303014755249 and perplexity is 342.5107399590276
At time: 206.62776041030884 and batch: 400, loss is 5.844672651290893 and perplexity is 345.38946047955454
At time: 208.38555073738098 and batch: 450, loss is 5.861275644302368 and perplexity is 351.1718287584697
At time: 210.14659905433655 and batch: 500, loss is 5.8735608959198 and perplexity is 355.51267261936437
At time: 211.8948209285736 and batch: 550, loss is 5.843145275115967 and perplexity is 344.8623235172449
At time: 213.6485722064972 and batch: 600, loss is 5.848863801956177 and perplexity is 346.8400775004414
At time: 215.4044771194458 and batch: 650, loss is 5.86265606880188 and perplexity is 351.6569297000057
At time: 217.16144633293152 and batch: 700, loss is 5.825857057571411 and perplexity is 338.95150958309074
At time: 218.91293001174927 and batch: 750, loss is 5.846265134811401 and perplexity is 345.939925690582
At time: 220.66424560546875 and batch: 800, loss is 5.85740104675293 and perplexity is 349.8138118347888
At time: 222.41420030593872 and batch: 850, loss is 5.851981086730957 and perplexity is 347.9229637488157
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.509054819742839 and perplexity of 246.91763506518257
Finished 7 epochs...
Completing Train Step...
At time: 226.81327772140503 and batch: 50, loss is 5.845318660736084 and perplexity is 345.61265741919715
At time: 228.52487468719482 and batch: 100, loss is 5.7828482723236085 and perplexity is 324.6826592799084
At time: 230.207035779953 and batch: 150, loss is 5.793611898422241 and perplexity is 328.19629784816397
At time: 231.89244294166565 and batch: 200, loss is 5.815934247970581 and perplexity is 335.6047901796851
At time: 233.6077744960785 and batch: 250, loss is 5.845278882980347 and perplexity is 345.5989099967532
At time: 235.33062267303467 and batch: 300, loss is 5.821079034805297 and perplexity is 337.33585444592364
At time: 237.0534770488739 and batch: 350, loss is 5.8021765804290775 and perplexity is 331.01926643291324
At time: 238.7964949607849 and batch: 400, loss is 5.818829212188721 and perplexity is 336.57776171806125
At time: 240.54725623130798 and batch: 450, loss is 5.838516941070557 and perplexity is 343.2698735223124
At time: 242.29931211471558 and batch: 500, loss is 5.84864146232605 and perplexity is 346.76296977826576
At time: 244.05817556381226 and batch: 550, loss is 5.815897884368897 and perplexity is 335.59258660265596
At time: 245.8110568523407 and batch: 600, loss is 5.823059816360473 and perplexity is 338.0047052893304
At time: 247.56279349327087 and batch: 650, loss is 5.844903869628906 and perplexity is 345.46933008987725
At time: 249.36896586418152 and batch: 700, loss is 5.803173160552978 and perplexity is 331.34931808861194
At time: 251.1196506023407 and batch: 750, loss is 5.815795373916626 and perplexity is 335.55818661803374
At time: 252.87122225761414 and batch: 800, loss is 5.827787456512451 and perplexity is 339.6064531660694
At time: 254.62275075912476 and batch: 850, loss is 5.823040637969971 and perplexity is 337.9982229652612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.496637344360352 and perplexity of 243.8704994301177
Finished 8 epochs...
Completing Train Step...
At time: 259.03896713256836 and batch: 50, loss is 5.81824595451355 and perplexity is 336.3815073942314
At time: 260.8055398464203 and batch: 100, loss is 5.765521411895752 and perplexity is 319.10538601040093
At time: 262.54558515548706 and batch: 150, loss is 5.778497428894043 and perplexity is 323.2730845082379
At time: 264.2901656627655 and batch: 200, loss is 5.800228681564331 and perplexity is 330.37510196691534
At time: 266.03024911880493 and batch: 250, loss is 5.826424160003662 and perplexity is 339.1437843231739
At time: 267.7716040611267 and batch: 300, loss is 5.795491895675659 and perplexity is 328.8138863371322
At time: 269.510808467865 and batch: 350, loss is 5.780915260314941 and perplexity is 324.0556500039812
At time: 271.25041007995605 and batch: 400, loss is 5.792834053039551 and perplexity is 327.941111134037
At time: 272.9900367259979 and batch: 450, loss is 5.815154981613159 and perplexity is 335.3433665298653
At time: 274.72845911979675 and batch: 500, loss is 5.826612501144409 and perplexity is 339.20766506588785
At time: 276.47130036354065 and batch: 550, loss is 5.794694318771362 and perplexity is 328.5517365317894
At time: 278.2123591899872 and batch: 600, loss is 5.809227313995361 and perplexity is 333.36144241222513
At time: 279.952702999115 and batch: 650, loss is 5.8215764617919925 and perplexity is 337.5036961445393
At time: 281.6926529407501 and batch: 700, loss is 5.778361396789551 and perplexity is 323.2291119811322
At time: 283.4341621398926 and batch: 750, loss is 5.792975730895996 and perplexity is 327.98757641917615
At time: 285.1775381565094 and batch: 800, loss is 5.804490222930908 and perplexity is 331.7860133237253
At time: 286.9195673465729 and batch: 850, loss is 5.803928508758545 and perplexity is 331.59969675107806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.4849599202473955 and perplexity of 241.03928300959268
Finished 9 epochs...
Completing Train Step...
At time: 291.33343386650085 and batch: 50, loss is 5.8003816890716555 and perplexity is 330.4256557052015
At time: 293.03521966934204 and batch: 100, loss is 5.743124656677246 and perplexity is 312.0379005885664
At time: 294.7814543247223 and batch: 150, loss is 5.755824575424194 and perplexity is 316.0260274123992
At time: 296.49849343299866 and batch: 200, loss is 5.776086845397949 and perplexity is 322.4947462475785
At time: 298.2244610786438 and batch: 250, loss is 5.801640377044678 and perplexity is 330.8418203597945
At time: 299.9735064506531 and batch: 300, loss is 5.77573751449585 and perplexity is 322.3821085420106
At time: 301.7228982448578 and batch: 350, loss is 5.759054374694824 and perplexity is 317.04837815034017
At time: 303.4725377559662 and batch: 400, loss is 5.773095483779907 and perplexity is 321.53148928487474
At time: 305.2214002609253 and batch: 450, loss is 5.79104287147522 and perplexity is 327.35423481951295
At time: 306.9701738357544 and batch: 500, loss is 5.807360982894897 and perplexity is 332.7398598044561
At time: 308.71791648864746 and batch: 550, loss is 5.77532320022583 and perplexity is 322.2485686996839
At time: 310.46728348731995 and batch: 600, loss is 5.787960939407348 and perplexity is 326.346904365012
At time: 312.21567153930664 and batch: 650, loss is 5.80320634841919 and perplexity is 331.3603150479318
At time: 313.9656021595001 and batch: 700, loss is 5.762042140960693 and perplexity is 317.9970611149845
At time: 315.7168960571289 and batch: 750, loss is 5.772098360061645 and perplexity is 321.21104239988586
At time: 317.4666783809662 and batch: 800, loss is 5.782959060668945 and perplexity is 324.71863232715003
At time: 319.21582198143005 and batch: 850, loss is 5.776237964630127 and perplexity is 322.5434850886081
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.480928421020508 and perplexity of 240.06948950088324
Finished 10 epochs...
Completing Train Step...
At time: 323.6459698677063 and batch: 50, loss is 5.7783316802978515 and perplexity is 323.21950688862455
At time: 325.34279012680054 and batch: 100, loss is 5.722956485748291 and perplexity is 305.80770388810686
At time: 327.05620217323303 and batch: 150, loss is 5.727249431610107 and perplexity is 307.12334176558323
At time: 328.7762916088104 and batch: 200, loss is 5.750408048629761 and perplexity is 314.31889151461684
At time: 330.49756598472595 and batch: 250, loss is 5.775660581588745 and perplexity is 322.3573077032146
At time: 332.2417504787445 and batch: 300, loss is 5.751648321151733 and perplexity is 314.70897445346975
At time: 333.98469042778015 and batch: 350, loss is 5.735051965713501 and perplexity is 309.5290552394875
At time: 335.7269215583801 and batch: 400, loss is 5.7457765865325925 and perplexity is 312.86650142317205
At time: 337.49522709846497 and batch: 450, loss is 5.770429964065552 and perplexity is 320.67558198705586
At time: 339.23671436309814 and batch: 500, loss is 5.785768938064575 and perplexity is 325.6323349670917
At time: 340.9806399345398 and batch: 550, loss is 5.753469944000244 and perplexity is 315.2827779801015
At time: 342.7247726917267 and batch: 600, loss is 5.761988639831543 and perplexity is 317.98004836825305
At time: 344.46758818626404 and batch: 650, loss is 5.770587482452393 and perplexity is 320.7260982659466
At time: 346.2115659713745 and batch: 700, loss is 5.731177940368652 and perplexity is 308.3322515552348
At time: 347.954950094223 and batch: 750, loss is 5.747943849563598 and perplexity is 313.54530072797104
At time: 349.700448513031 and batch: 800, loss is 5.756494731903076 and perplexity is 316.23788528319824
At time: 351.444539308548 and batch: 850, loss is 5.752723302841186 and perplexity is 315.04746274027303
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.459940592447917 and perplexity of 235.08345821760744
Finished 11 epochs...
Completing Train Step...
At time: 355.9128632545471 and batch: 50, loss is 5.747091293334961 and perplexity is 313.278099647033
At time: 357.60712122917175 and batch: 100, loss is 5.699787511825561 and perplexity is 298.80390192526613
At time: 359.321848154068 and batch: 150, loss is 5.710899152755737 and perplexity is 302.1426185500045
At time: 361.0623621940613 and batch: 200, loss is 5.737903318405151 and perplexity is 310.41289120951035
At time: 362.80243849754333 and batch: 250, loss is 5.759552717208862 and perplexity is 317.2064162114509
At time: 364.5417640209198 and batch: 300, loss is 5.734513568878174 and perplexity is 309.3624506294267
At time: 366.2800371646881 and batch: 350, loss is 5.7180486679077145 and perplexity is 304.3105323110232
At time: 368.0205020904541 and batch: 400, loss is 5.731298446655273 and perplexity is 308.3694097687695
At time: 369.7699029445648 and batch: 450, loss is 5.7611408710479735 and perplexity is 317.71058904518844
At time: 371.51958537101746 and batch: 500, loss is 5.7700940704345705 and perplexity is 320.567887189466
At time: 373.2696874141693 and batch: 550, loss is 5.7375702476501464 and perplexity is 310.3095189695626
At time: 375.0190079212189 and batch: 600, loss is 5.754331226348877 and perplexity is 315.55444244471425
At time: 376.7695755958557 and batch: 650, loss is 5.761400499343872 and perplexity is 317.7930864128733
At time: 378.52876138687134 and batch: 700, loss is 5.723915367126465 and perplexity is 306.1010778335952
At time: 380.2870831489563 and batch: 750, loss is 5.738947191238403 and perplexity is 310.7370919764824
At time: 382.08249521255493 and batch: 800, loss is 5.74679310798645 and perplexity is 313.1846986338079
At time: 383.823796749115 and batch: 850, loss is 5.7435368633270265 and perplexity is 312.16655119966964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.466612497965495 and perplexity of 236.65715678598863
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 388.21247720718384 and batch: 50, loss is 5.729762172698974 and perplexity is 307.89603358647537
At time: 389.91833758354187 and batch: 100, loss is 5.65193359375 and perplexity is 284.84170182042374
At time: 391.62197732925415 and batch: 150, loss is 5.628194894790649 and perplexity is 278.1595569167975
At time: 393.34113025665283 and batch: 200, loss is 5.642728290557861 and perplexity is 282.2316790494389
At time: 395.06233263015747 and batch: 250, loss is 5.665545415878296 and perplexity is 288.7454245159636
At time: 396.78390860557556 and batch: 300, loss is 5.626075973510742 and perplexity is 277.57078271582
At time: 398.52301812171936 and batch: 350, loss is 5.585815315246582 and perplexity is 266.61757151906727
At time: 400.2749798297882 and batch: 400, loss is 5.611345796585083 and perplexity is 273.5120820536122
At time: 402.0268249511719 and batch: 450, loss is 5.642825069427491 and perplexity is 282.2589944340656
At time: 403.7779371738434 and batch: 500, loss is 5.6459269618988035 and perplexity is 283.13589079987315
At time: 405.52949118614197 and batch: 550, loss is 5.596437644958496 and perplexity is 269.4647664206095
At time: 407.2872622013092 and batch: 600, loss is 5.589410829544067 and perplexity is 267.57792425582505
At time: 409.04442286491394 and batch: 650, loss is 5.5784750747680665 and perplexity is 264.66769944804776
At time: 410.7958724498749 and batch: 700, loss is 5.539649782180786 and perplexity is 254.58882230160862
At time: 412.5462074279785 and batch: 750, loss is 5.525465393066407 and perplexity is 251.0031259570555
At time: 414.2974479198456 and batch: 800, loss is 5.531166105270386 and perplexity is 252.43810886654308
At time: 416.05632615089417 and batch: 850, loss is 5.557441654205323 and perplexity is 259.1589692251733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.316480000813802 and perplexity of 203.66571560875497
Finished 13 epochs...
Completing Train Step...
At time: 420.4525978565216 and batch: 50, loss is 5.596015243530274 and perplexity is 269.3509681543849
At time: 422.17005825042725 and batch: 100, loss is 5.548079624176025 and perplexity is 256.74403714481184
At time: 423.86844396591187 and batch: 150, loss is 5.537187290191651 and perplexity is 253.96267062931437
At time: 425.6051552295685 and batch: 200, loss is 5.565412063598632 and perplexity is 261.23282607300894
At time: 427.3241763114929 and batch: 250, loss is 5.5887441825866695 and perplexity is 267.39960369178795
At time: 429.0436100959778 and batch: 300, loss is 5.5532534885406495 and perplexity is 258.0758382777365
At time: 430.7627878189087 and batch: 350, loss is 5.522106475830078 and perplexity is 250.16144159630574
At time: 432.4816520214081 and batch: 400, loss is 5.546732692718506 and perplexity is 256.39845331576356
At time: 434.2196183204651 and batch: 450, loss is 5.581035013198853 and perplexity is 265.3461004251394
At time: 435.9716172218323 and batch: 500, loss is 5.581896905899048 and perplexity is 265.57489887781543
At time: 437.7125732898712 and batch: 550, loss is 5.545801839828491 and perplexity is 256.15989512301644
At time: 439.4529392719269 and batch: 600, loss is 5.554919586181641 and perplexity is 258.5061762160526
At time: 441.19362473487854 and batch: 650, loss is 5.550037746429443 and perplexity is 257.24726588828634
At time: 442.93995547294617 and batch: 700, loss is 5.5164688873291015 and perplexity is 248.75510223581338
At time: 444.6941487789154 and batch: 750, loss is 5.514652986526489 and perplexity is 248.30379753231063
At time: 446.44677925109863 and batch: 800, loss is 5.53084792137146 and perplexity is 252.3577999020136
At time: 448.2062339782715 and batch: 850, loss is 5.551845121383667 and perplexity is 257.712628569443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.307435989379883 and perplexity of 201.83206482588926
Finished 14 epochs...
Completing Train Step...
At time: 452.60625863075256 and batch: 50, loss is 5.563525514602661 and perplexity is 260.74046212764927
At time: 454.34155011177063 and batch: 100, loss is 5.511690731048584 and perplexity is 247.5693466005984
At time: 456.0592498779297 and batch: 150, loss is 5.501059818267822 and perplexity is 244.95139871337767
At time: 457.7825412750244 and batch: 200, loss is 5.539960803985596 and perplexity is 254.66801729165135
At time: 459.5092656612396 and batch: 250, loss is 5.565823755264282 and perplexity is 261.3403955915152
At time: 461.233699798584 and batch: 300, loss is 5.529471263885498 and perplexity is 252.01062867040474
At time: 462.9660782814026 and batch: 350, loss is 5.50073431968689 and perplexity is 244.8716803555091
At time: 464.70516538619995 and batch: 400, loss is 5.527110786437988 and perplexity is 251.416464795991
At time: 466.44519686698914 and batch: 450, loss is 5.558371114730835 and perplexity is 259.3999592349018
At time: 468.18529319763184 and batch: 500, loss is 5.557225561141967 and perplexity is 259.1029728200527
At time: 469.9683425426483 and batch: 550, loss is 5.531969833374023 and perplexity is 252.64108202575798
At time: 471.707882642746 and batch: 600, loss is 5.545499429702759 and perplexity is 256.0824414888967
At time: 473.4560797214508 and batch: 650, loss is 5.544146909713745 and perplexity is 255.7363189895107
At time: 475.1957838535309 and batch: 700, loss is 5.5087425327301025 and perplexity is 246.84053793337918
At time: 476.9468002319336 and batch: 750, loss is 5.510240507125855 and perplexity is 247.21057582348465
At time: 478.6892673969269 and batch: 800, loss is 5.526332902908325 and perplexity is 251.22096811562196
At time: 480.4334557056427 and batch: 850, loss is 5.543044891357422 and perplexity is 255.45464810335537
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.300699551900228 and perplexity of 200.47700500202663
Finished 15 epochs...
Completing Train Step...
At time: 484.8428897857666 and batch: 50, loss is 5.549081811904907 and perplexity is 257.00147184570426
At time: 486.53625869750977 and batch: 100, loss is 5.498243951797486 and perplexity is 244.2626184945579
At time: 488.2282803058624 and batch: 150, loss is 5.488067626953125 and perplexity is 241.7895275719627
At time: 489.92172026634216 and batch: 200, loss is 5.527199048995971 and perplexity is 251.43865643562404
At time: 491.63122391700745 and batch: 250, loss is 5.554283037185669 and perplexity is 258.3416767306753
At time: 493.3524980545044 and batch: 300, loss is 5.5174620151519775 and perplexity is 249.00227056348248
At time: 495.07464361190796 and batch: 350, loss is 5.489769859313965 and perplexity is 242.2014600332231
At time: 496.80555605888367 and batch: 400, loss is 5.517755794525146 and perplexity is 249.07543304073337
At time: 498.5450644493103 and batch: 450, loss is 5.548756151199341 and perplexity is 256.9177901917043
At time: 500.28223633766174 and batch: 500, loss is 5.547037467956543 and perplexity is 256.47660912477795
At time: 502.0183730125427 and batch: 550, loss is 5.520057191848755 and perplexity is 249.64931468725908
At time: 503.7545883655548 and batch: 600, loss is 5.5326364135742185 and perplexity is 252.80954370916587
At time: 505.4922664165497 and batch: 650, loss is 5.532235221862793 and perplexity is 252.7081389583934
At time: 507.2307596206665 and batch: 700, loss is 5.494702596664428 and perplexity is 243.39912768298092
At time: 508.9733610153198 and batch: 750, loss is 5.496051950454712 and perplexity is 243.7277809033395
At time: 510.7149028778076 and batch: 800, loss is 5.509804677963257 and perplexity is 247.10285772028863
At time: 512.5053539276123 and batch: 850, loss is 5.5240575885772705 and perplexity is 250.65001124610382
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.283311208089192 and perplexity of 197.0211745700813
Finished 16 epochs...
Completing Train Step...
At time: 516.9122998714447 and batch: 50, loss is 5.525459175109863 and perplexity is 251.00156523537828
At time: 518.6066200733185 and batch: 100, loss is 5.4732640361785885 and perplexity is 238.23653774110417
At time: 520.3263266086578 and batch: 150, loss is 5.463142042160034 and perplexity is 235.83727209200993
At time: 522.0682365894318 and batch: 200, loss is 5.501042337417602 and perplexity is 244.94711679209155
At time: 523.8121266365051 and batch: 250, loss is 5.529555015563965 and perplexity is 252.03173586741644
At time: 525.5572738647461 and batch: 300, loss is 5.492047233581543 and perplexity is 242.7536719638256
At time: 527.2973380088806 and batch: 350, loss is 5.464578628540039 and perplexity is 236.1763161797448
At time: 529.0374147891998 and batch: 400, loss is 5.490998611450196 and perplexity is 242.4992485112839
At time: 530.7768356800079 and batch: 450, loss is 5.52298599243164 and perplexity is 250.38155952175586
At time: 532.5168030261993 and batch: 500, loss is 5.521213884353638 and perplexity is 249.93824925041403
At time: 534.2608704566956 and batch: 550, loss is 5.497008562088013 and perplexity is 243.96104528783368
At time: 536.0094785690308 and batch: 600, loss is 5.51294095993042 and perplexity is 247.87905851302287
At time: 537.7596533298492 and batch: 650, loss is 5.509902448654175 and perplexity is 247.1270183184956
At time: 539.5166249275208 and batch: 700, loss is 5.473400745391846 and perplexity is 238.2691090970994
At time: 541.2675702571869 and batch: 750, loss is 5.476861248016357 and perplexity is 239.09506826716412
At time: 543.0206065177917 and batch: 800, loss is 5.491060829162597 and perplexity is 242.51433672915775
At time: 544.7772119045258 and batch: 850, loss is 5.50401478767395 and perplexity is 245.67629309545276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.271072069803874 and perplexity of 194.62450169246304
Finished 17 epochs...
Completing Train Step...
At time: 549.1987383365631 and batch: 50, loss is 5.505538473129272 and perplexity is 246.0509118180325
At time: 550.8963069915771 and batch: 100, loss is 5.454492969512939 and perplexity is 233.80629409176788
At time: 552.5947961807251 and batch: 150, loss is 5.445278739929199 and perplexity is 231.66184411939523
At time: 554.3024039268494 and batch: 200, loss is 5.482342157363892 and perplexity is 240.40912448371998
At time: 556.0234253406525 and batch: 250, loss is 5.510281629562378 and perplexity is 247.22074193372308
At time: 557.7939794063568 and batch: 300, loss is 5.474099626541138 and perplexity is 238.43568908893377
At time: 559.5186812877655 and batch: 350, loss is 5.447425746917725 and perplexity is 232.15975803875975
At time: 561.2623422145844 and batch: 400, loss is 5.474066171646118 and perplexity is 238.42771238141708
At time: 563.0077540874481 and batch: 450, loss is 5.507308902740479 and perplexity is 246.48691347942108
At time: 564.7537722587585 and batch: 500, loss is 5.50580057144165 and perplexity is 246.1154097988158
At time: 566.5020921230316 and batch: 550, loss is 5.484369516372681 and perplexity is 240.89701448502203
At time: 568.260843038559 and batch: 600, loss is 5.501431846618653 and perplexity is 245.04254453163762
At time: 570.0179288387299 and batch: 650, loss is 5.50036503791809 and perplexity is 244.78127040265963
At time: 571.7748117446899 and batch: 700, loss is 5.4645353698730466 and perplexity is 236.1660997281083
At time: 573.5305328369141 and batch: 750, loss is 5.467328634262085 and perplexity is 236.82669626525248
At time: 575.2871856689453 and batch: 800, loss is 5.481569499969482 and perplexity is 240.22344233958262
At time: 577.044506072998 and batch: 850, loss is 5.493504667282105 and perplexity is 243.1077272896916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.263708114624023 and perplexity of 193.1965596884022
Finished 18 epochs...
Completing Train Step...
At time: 581.5254366397858 and batch: 50, loss is 5.495018453598022 and perplexity is 243.47601912779902
At time: 583.2375514507294 and batch: 100, loss is 5.443280172348023 and perplexity is 231.1993146201297
At time: 584.9440412521362 and batch: 150, loss is 5.434804420471192 and perplexity is 229.24800767361842
At time: 586.6679079532623 and batch: 200, loss is 5.472420892715454 and perplexity is 238.03575481792714
At time: 588.4090955257416 and batch: 250, loss is 5.49890983581543 and perplexity is 244.42532323362528
At time: 590.1503214836121 and batch: 300, loss is 5.462866868972778 and perplexity is 235.77238492618818
At time: 591.8967380523682 and batch: 350, loss is 5.436998519897461 and perplexity is 229.7515528077649
At time: 593.648551940918 and batch: 400, loss is 5.463699951171875 and perplexity is 235.96888454191006
At time: 595.4024810791016 and batch: 450, loss is 5.498169994354248 and perplexity is 244.244554123821
At time: 597.1569125652313 and batch: 500, loss is 5.497136583328247 and perplexity is 243.99227948269788
At time: 598.9128088951111 and batch: 550, loss is 5.4768643474578855 and perplexity is 239.09580932949638
At time: 600.7058706283569 and batch: 600, loss is 5.494289855957032 and perplexity is 243.2986876841053
At time: 602.447957277298 and batch: 650, loss is 5.494585599899292 and perplexity is 243.37065243819322
At time: 604.1907653808594 and batch: 700, loss is 5.457793035507202 and perplexity is 234.57914481995815
At time: 605.9351651668549 and batch: 750, loss is 5.460488777160645 and perplexity is 235.21236270412396
At time: 607.67853474617 and batch: 800, loss is 5.474251775741577 and perplexity is 238.47196964834396
At time: 609.421713590622 and batch: 850, loss is 5.486757593154907 and perplexity is 241.47298250642652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.259738286336263 and perplexity of 192.43112285229927
Finished 19 epochs...
Completing Train Step...
At time: 613.803492307663 and batch: 50, loss is 5.487201690673828 and perplexity is 241.58024387433485
At time: 615.5208871364594 and batch: 100, loss is 5.4353757572174075 and perplexity is 229.37902290774397
At time: 617.2337219715118 and batch: 150, loss is 5.42689208984375 and perplexity is 227.44127878522366
At time: 618.9554796218872 and batch: 200, loss is 5.4647440910339355 and perplexity is 236.2153977351958
At time: 620.6770186424255 and batch: 250, loss is 5.491149415969849 and perplexity is 242.53582125156987
At time: 622.4024946689606 and batch: 300, loss is 5.45451244354248 and perplexity is 233.81084728678027
At time: 624.1406970024109 and batch: 350, loss is 5.429440135955811 and perplexity is 228.02154861454378
At time: 625.8934834003448 and batch: 400, loss is 5.4569243049621585 and perplexity is 234.37544724358688
At time: 627.642915725708 and batch: 450, loss is 5.49158408164978 and perplexity is 242.64126616420396
At time: 629.392373085022 and batch: 500, loss is 5.489818592071533 and perplexity is 242.2132634658621
At time: 631.1394555568695 and batch: 550, loss is 5.470297470092773 and perplexity is 237.53084057402214
At time: 632.8861217498779 and batch: 600, loss is 5.48822506904602 and perplexity is 241.82759841812205
At time: 634.6345882415771 and batch: 650, loss is 5.487950353622437 and perplexity is 241.76117377134364
At time: 636.3835425376892 and batch: 700, loss is 5.451848087310791 and perplexity is 233.18872104976214
At time: 638.132527589798 and batch: 750, loss is 5.455214042663574 and perplexity is 233.97494633088
At time: 639.881432056427 and batch: 800, loss is 5.467996501922608 and perplexity is 236.98491798654078
At time: 641.6323385238647 and batch: 850, loss is 5.481159629821778 and perplexity is 240.12500209702532
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.253946304321289 and perplexity of 191.3197867757376
Finished 20 epochs...
Completing Train Step...
At time: 646.0345168113708 and batch: 50, loss is 5.480507535934448 and perplexity is 239.96846909361702
At time: 647.7554860115051 and batch: 100, loss is 5.428729104995727 and perplexity is 227.85947586011397
At time: 649.4534387588501 and batch: 150, loss is 5.420525741577149 and perplexity is 225.9979077705836
At time: 651.1474380493164 and batch: 200, loss is 5.458972463607788 and perplexity is 234.85597727506786
At time: 652.8515241146088 and batch: 250, loss is 5.485536947250366 and perplexity is 241.17840932062467
At time: 654.5739736557007 and batch: 300, loss is 5.448558664321899 and perplexity is 232.42292491420946
At time: 656.2955901622772 and batch: 350, loss is 5.423837080001831 and perplexity is 226.74750372448386
At time: 658.0166699886322 and batch: 400, loss is 5.451438636779785 and perplexity is 233.0932613484501
At time: 659.7372422218323 and batch: 450, loss is 5.486353540420533 and perplexity is 241.37543439618466
At time: 661.457644701004 and batch: 500, loss is 5.483742570877075 and perplexity is 240.74603252056244
At time: 663.1776762008667 and batch: 550, loss is 5.464675006866455 and perplexity is 236.19907955476756
At time: 664.8967726230621 and batch: 600, loss is 5.484795007705689 and perplexity is 240.99953588627
At time: 666.6163523197174 and batch: 650, loss is 5.4866273307800295 and perplexity is 241.44152971085882
At time: 668.3376793861389 and batch: 700, loss is 5.448335666656494 and perplexity is 232.3711009230962
At time: 670.0584909915924 and batch: 750, loss is 5.4510384368896485 and perplexity is 232.9999961144799
At time: 671.7851383686066 and batch: 800, loss is 5.463902139663697 and perplexity is 236.01659955834356
At time: 673.5337626934052 and batch: 850, loss is 5.477214527130127 and perplexity is 239.17955048301081
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.252490043640137 and perplexity of 191.0413780597645
Finished 21 epochs...
Completing Train Step...
At time: 677.9900126457214 and batch: 50, loss is 5.476287631988526 and perplexity is 238.95795883167497
At time: 679.7040889263153 and batch: 100, loss is 5.4241735553741455 and perplexity is 226.8238115123467
At time: 681.4210317134857 and batch: 150, loss is 5.416917905807495 and perplexity is 225.18401351630004
At time: 683.1362361907959 and batch: 200, loss is 5.454928455352783 and perplexity is 233.90813559576807
At time: 684.855954170227 and batch: 250, loss is 5.479396009445191 and perplexity is 239.70188596818244
At time: 686.5777056217194 and batch: 300, loss is 5.444134836196899 and perplexity is 231.39699678008535
At time: 688.3282759189606 and batch: 350, loss is 5.420777463912964 and perplexity is 226.05480365249878
At time: 690.0515468120575 and batch: 400, loss is 5.449293308258056 and perplexity is 232.59373574150865
At time: 691.7746846675873 and batch: 450, loss is 5.483122053146363 and perplexity is 240.5966916778971
At time: 693.4985628128052 and batch: 500, loss is 5.480161685943603 and perplexity is 239.88549035070375
At time: 695.2286951541901 and batch: 550, loss is 5.461692895889282 and perplexity is 235.49575690115194
At time: 696.9655017852783 and batch: 600, loss is 5.481614198684692 and perplexity is 240.23418025880235
At time: 698.7136933803558 and batch: 650, loss is 5.4822164726257325 and perplexity is 240.37891062460844
At time: 700.4563300609589 and batch: 700, loss is 5.445963945388794 and perplexity is 231.8206344755624
At time: 702.1989736557007 and batch: 750, loss is 5.450487031936645 and perplexity is 232.8715541775871
At time: 703.9410722255707 and batch: 800, loss is 5.461988363265991 and perplexity is 235.56534849518627
At time: 705.6843819618225 and batch: 850, loss is 5.474249353408814 and perplexity is 238.4713919905783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.251528739929199 and perplexity of 190.8578175169443
Finished 22 epochs...
Completing Train Step...
At time: 710.1341373920441 and batch: 50, loss is 5.47229037284851 and perplexity is 238.00468845031364
At time: 711.8482303619385 and batch: 100, loss is 5.420450601577759 and perplexity is 225.98092692590998
At time: 713.575021982193 and batch: 150, loss is 5.4133380508422855 and perplexity is 224.37932859444476
At time: 715.301436662674 and batch: 200, loss is 5.4520657348632815 and perplexity is 233.23947952769598
At time: 717.0285537242889 and batch: 250, loss is 5.476538381576538 and perplexity is 239.01788495431558
At time: 718.7579956054688 and batch: 300, loss is 5.440820941925049 and perplexity is 230.6314407843514
At time: 720.4798645973206 and batch: 350, loss is 5.416234989166259 and perplexity is 225.0302841042909
At time: 722.2223136425018 and batch: 400, loss is 5.44560544013977 and perplexity is 231.73754045698186
At time: 723.9635970592499 and batch: 450, loss is 5.4793312454223635 and perplexity is 239.68636241245724
At time: 725.705552816391 and batch: 500, loss is 5.477540140151977 and perplexity is 239.2574431399519
At time: 727.4465517997742 and batch: 550, loss is 5.45896577835083 and perplexity is 234.85440720775986
At time: 729.185464143753 and batch: 600, loss is 5.478626880645752 and perplexity is 239.51759522514843
At time: 730.9386897087097 and batch: 650, loss is 5.479492988586426 and perplexity is 239.72513317846503
At time: 732.7339715957642 and batch: 700, loss is 5.443498497009277 and perplexity is 231.24979664271115
At time: 734.4727368354797 and batch: 750, loss is 5.447536735534668 and perplexity is 232.18552655919413
At time: 736.2135405540466 and batch: 800, loss is 5.458847532272339 and perplexity is 234.82663823690942
At time: 737.9546992778778 and batch: 850, loss is 5.470948305130005 and perplexity is 237.685484285805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.250620524088542 and perplexity of 190.68455611504666
Finished 23 epochs...
Completing Train Step...
At time: 742.38303565979 and batch: 50, loss is 5.468424129486084 and perplexity is 237.08628094085188
At time: 744.06542801857 and batch: 100, loss is 5.4170488929748535 and perplexity is 225.21351166426209
At time: 745.7718377113342 and batch: 150, loss is 5.409989004135132 and perplexity is 223.6291286713471
At time: 747.4850835800171 and batch: 200, loss is 5.449478425979614 and perplexity is 232.63679694949116
At time: 749.2053864002228 and batch: 250, loss is 5.473880987167359 and perplexity is 238.38356335776103
At time: 750.9266443252563 and batch: 300, loss is 5.438252773284912 and perplexity is 230.03990026373165
At time: 752.6666615009308 and batch: 350, loss is 5.413282241821289 and perplexity is 224.36680655320873
At time: 754.4060106277466 and batch: 400, loss is 5.443300743103027 and perplexity is 231.20407061350494
At time: 756.1467006206512 and batch: 450, loss is 5.47682819366455 and perplexity is 239.08716526527738
At time: 757.8876366615295 and batch: 500, loss is 5.474272680282593 and perplexity is 238.476954847521
At time: 759.6270124912262 and batch: 550, loss is 5.455899038314819 and perplexity is 234.1352730569031
At time: 761.3665516376495 and batch: 600, loss is 5.475265731811524 and perplexity is 238.7138923782643
At time: 763.1063804626465 and batch: 650, loss is 5.47681978225708 and perplexity is 239.08515421416737
At time: 764.8457798957825 and batch: 700, loss is 5.440325708389282 and perplexity is 230.5172526377064
At time: 766.5936069488525 and batch: 750, loss is 5.44464280128479 and perplexity is 231.5145682344613
At time: 768.3447158336639 and batch: 800, loss is 5.456092033386231 and perplexity is 234.1804643713568
At time: 770.093864440918 and batch: 850, loss is 5.468280277252197 and perplexity is 237.05217800266539
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.249828656514485 and perplexity of 190.53361896718948
Finished 24 epochs...
Completing Train Step...
At time: 774.5117344856262 and batch: 50, loss is 5.465134344100952 and perplexity is 236.30759950848076
At time: 776.2388479709625 and batch: 100, loss is 5.413581323623657 and perplexity is 224.43392061790175
At time: 777.9537973403931 and batch: 150, loss is 5.406295442581177 and perplexity is 222.804664261671
At time: 779.6759011745453 and batch: 200, loss is 5.445989437103272 and perplexity is 231.8265440563089
At time: 781.4013757705688 and batch: 250, loss is 5.470043544769287 and perplexity is 237.47053313560838
At time: 783.1456534862518 and batch: 300, loss is 5.434816970825195 and perplexity is 229.25088483532392
At time: 784.8886475563049 and batch: 350, loss is 5.410209903717041 and perplexity is 223.67853370894846
At time: 786.6336312294006 and batch: 400, loss is 5.439807319641114 and perplexity is 230.39778605542168
At time: 788.3785622119904 and batch: 450, loss is 5.473034553527832 and perplexity is 238.18187286147688
At time: 790.123279094696 and batch: 500, loss is 5.471069440841675 and perplexity is 237.71427823004993
At time: 791.8681757450104 and batch: 550, loss is 5.452893276214599 and perplexity is 233.4325747278767
At time: 793.6113133430481 and batch: 600, loss is 5.472015323638916 and perplexity is 237.9392344508238
At time: 795.355535030365 and batch: 650, loss is 5.474134006500244 and perplexity is 238.44388663908904
At time: 797.0992183685303 and batch: 700, loss is 5.439524993896485 and perplexity is 230.332748010302
At time: 798.8435521125793 and batch: 750, loss is 5.441937370300293 and perplexity is 230.88906805350413
At time: 800.5872888565063 and batch: 800, loss is 5.453099355697632 and perplexity is 233.4806853493354
At time: 802.3315470218658 and batch: 850, loss is 5.4656882572174075 and perplexity is 236.43852964597917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.248263676961263 and perplexity of 190.23567095139086
Finished 25 epochs...
Completing Train Step...
At time: 806.7192986011505 and batch: 50, loss is 5.46206298828125 and perplexity is 235.58292821884757
At time: 808.4547986984253 and batch: 100, loss is 5.410644130706787 and perplexity is 223.77568205600247
At time: 810.1800591945648 and batch: 150, loss is 5.403735876083374 and perplexity is 222.2351101242641
At time: 811.9218327999115 and batch: 200, loss is 5.443718795776367 and perplexity is 231.30074629966856
At time: 813.6615064144135 and batch: 250, loss is 5.468144121170044 and perplexity is 237.01990410403639
At time: 815.4037322998047 and batch: 300, loss is 5.432218370437622 and perplexity is 228.6559267610794
At time: 817.1452374458313 and batch: 350, loss is 5.406950244903564 and perplexity is 222.95060504923703
At time: 818.9143443107605 and batch: 400, loss is 5.437084674835205 and perplexity is 229.77134789120348
At time: 820.655412197113 and batch: 450, loss is 5.4711164951324465 and perplexity is 237.72546396998473
At time: 822.3954737186432 and batch: 500, loss is 5.469340686798096 and perplexity is 237.30368372106022
At time: 824.145266532898 and batch: 550, loss is 5.451974115371704 and perplexity is 233.21811122405737
At time: 825.8953008651733 and batch: 600, loss is 5.470228538513184 and perplexity is 237.51446776228858
At time: 827.6498699188232 and batch: 650, loss is 5.471724901199341 and perplexity is 237.87014159142788
At time: 829.4054574966431 and batch: 700, loss is 5.434458265304565 and perplexity is 229.1686660243765
At time: 831.1616513729095 and batch: 750, loss is 5.439408693313599 and perplexity is 230.30596173511057
At time: 832.9172508716583 and batch: 800, loss is 5.45088677406311 and perplexity is 232.96466135603816
At time: 834.6686317920685 and batch: 850, loss is 5.462949056625366 and perplexity is 235.79176330137093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.248270352681478 and perplexity of 190.236940915744
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 839.0847992897034 and batch: 50, loss is 5.461196746826172 and perplexity is 235.37894488248202
At time: 840.8006720542908 and batch: 100, loss is 5.407400751113892 and perplexity is 223.05106830937018
At time: 842.4907248020172 and batch: 150, loss is 5.392949485778809 and perplexity is 219.85087723666513
At time: 844.2023646831512 and batch: 200, loss is 5.427867021560669 and perplexity is 227.66312662732474
At time: 845.9241156578064 and batch: 250, loss is 5.445822257995605 and perplexity is 231.78779074100268
At time: 847.6497576236725 and batch: 300, loss is 5.4021973800659175 and perplexity is 221.89346516948868
At time: 849.3752899169922 and batch: 350, loss is 5.3781829357147215 and perplexity is 216.62829005593693
At time: 851.1254329681396 and batch: 400, loss is 5.403172874450684 and perplexity is 222.11002660883878
At time: 852.8770968914032 and batch: 450, loss is 5.43930383682251 and perplexity is 230.28181392613553
At time: 854.6290431022644 and batch: 500, loss is 5.434850234985351 and perplexity is 229.25851080030796
At time: 856.3799364566803 and batch: 550, loss is 5.405194683074951 and perplexity is 222.559544842969
At time: 858.1292548179626 and batch: 600, loss is 5.411717538833618 and perplexity is 224.01601365557863
At time: 859.8776223659515 and batch: 650, loss is 5.409976615905761 and perplexity is 223.62635831956715
At time: 861.6271879673004 and batch: 700, loss is 5.37644772529602 and perplexity is 216.252720330532
At time: 863.4041788578033 and batch: 750, loss is 5.375371198654175 and perplexity is 216.02004377944868
At time: 865.1540994644165 and batch: 800, loss is 5.38100646018982 and perplexity is 217.2408096600234
At time: 866.9057791233063 and batch: 850, loss is 5.410967292785645 and perplexity is 223.8480095567368
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.2136789957682295 and perplexity of 183.7689010698767
Finished 27 epochs...
Completing Train Step...
At time: 871.3242366313934 and batch: 50, loss is 5.433361749649048 and perplexity is 228.91751671406618
At time: 873.0176687240601 and batch: 100, loss is 5.383358459472657 and perplexity is 217.752361237197
At time: 874.7315847873688 and batch: 150, loss is 5.370402688980103 and perplexity is 214.94940803664284
At time: 876.4531123638153 and batch: 200, loss is 5.410323753356933 and perplexity is 223.70400087914948
At time: 878.1792256832123 and batch: 250, loss is 5.43036470413208 and perplexity is 228.23246757134905
At time: 879.9131052494049 and batch: 300, loss is 5.389694509506225 and perplexity is 219.13643123093985
At time: 881.6537721157074 and batch: 350, loss is 5.366386222839355 and perplexity is 214.0878024803465
At time: 883.3977401256561 and batch: 400, loss is 5.391182022094727 and perplexity is 219.46264199215167
At time: 885.1379306316376 and batch: 450, loss is 5.428696603775024 and perplexity is 227.85207026934575
At time: 886.8836400508881 and batch: 500, loss is 5.426025686264038 and perplexity is 227.24430818744676
At time: 888.6250700950623 and batch: 550, loss is 5.399980430603027 and perplexity is 221.40208345653085
At time: 890.366213798523 and batch: 600, loss is 5.412150926589966 and perplexity is 224.11312049406035
At time: 892.108081817627 and batch: 650, loss is 5.412840442657471 and perplexity is 224.26770337914667
At time: 893.8505811691284 and batch: 700, loss is 5.38134539604187 and perplexity is 217.3144528383973
At time: 895.5930852890015 and batch: 750, loss is 5.3808568096160885 and perplexity is 217.2083018806845
At time: 897.3364531993866 and batch: 800, loss is 5.386208486557007 and perplexity is 218.37384656874212
At time: 899.0797815322876 and batch: 850, loss is 5.413582143783569 and perplexity is 224.43410468968173
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.212389945983887 and perplexity of 183.53216642173808
Finished 28 epochs...
Completing Train Step...
At time: 903.5000240802765 and batch: 50, loss is 5.4260149002075195 and perplexity is 227.24185713071387
At time: 905.2162292003632 and batch: 100, loss is 5.375629158020019 and perplexity is 216.0757753608843
At time: 906.9660840034485 and batch: 150, loss is 5.361837034225464 and perplexity is 213.11608861756056
At time: 908.6940743923187 and batch: 200, loss is 5.403673944473266 and perplexity is 222.2213471722569
At time: 910.4365816116333 and batch: 250, loss is 5.42414478302002 and perplexity is 226.81728535120465
At time: 912.1787717342377 and batch: 300, loss is 5.383934259414673 and perplexity is 217.87777913851625
At time: 913.9225165843964 and batch: 350, loss is 5.361261539459228 and perplexity is 212.99347670860055
At time: 915.6763365268707 and batch: 400, loss is 5.387120056152344 and perplexity is 218.57300028513652
At time: 917.4428749084473 and batch: 450, loss is 5.4253991603851315 and perplexity is 227.10197833885573
At time: 919.2009723186493 and batch: 500, loss is 5.4232070255279545 and perplexity is 226.60468544169552
At time: 920.9546461105347 and batch: 550, loss is 5.398488521575928 and perplexity is 221.07201796466828
At time: 922.7076590061188 and batch: 600, loss is 5.4123133850097656 and perplexity is 224.14953251511176
At time: 924.4612231254578 and batch: 650, loss is 5.4136777591705325 and perplexity is 224.45556506940434
At time: 926.2209579944611 and batch: 700, loss is 5.3836789894104005 and perplexity is 217.82216857506108
At time: 927.9731681346893 and batch: 750, loss is 5.382840213775634 and perplexity is 217.63954124973097
At time: 929.7238259315491 and batch: 800, loss is 5.387974872589111 and perplexity is 218.75991995803616
At time: 931.4756779670715 and batch: 850, loss is 5.414018640518188 and perplexity is 224.53209082727477
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.2120615641276045 and perplexity of 183.4719076827208
Finished 29 epochs...
Completing Train Step...
At time: 935.8756682872772 and batch: 50, loss is 5.421689147949219 and perplexity is 226.26098818159812
At time: 937.5664165019989 and batch: 100, loss is 5.371576251983643 and perplexity is 215.2018127870173
At time: 939.2718350887299 and batch: 150, loss is 5.3578400230407714 and perplexity is 212.26596134363888
At time: 940.993246793747 and batch: 200, loss is 5.400338592529297 and perplexity is 221.48139545564317
At time: 942.7118759155273 and batch: 250, loss is 5.420962591171264 and perplexity is 226.09665643245017
At time: 944.4293491840363 and batch: 300, loss is 5.381086578369141 and perplexity is 217.25821529541219
At time: 946.1566257476807 and batch: 350, loss is 5.358522567749024 and perplexity is 212.4108918074301
At time: 947.9072024822235 and batch: 400, loss is 5.385617809295654 and perplexity is 218.2448961908771
At time: 949.6483039855957 and batch: 450, loss is 5.424155511856079 and perplexity is 226.81971884972896
At time: 951.4172255992889 and batch: 500, loss is 5.422468280792236 and perplexity is 226.43734424206193
At time: 953.1598663330078 and batch: 550, loss is 5.39833622932434 and perplexity is 221.03835297281324
At time: 954.9028446674347 and batch: 600, loss is 5.412877597808838 and perplexity is 224.27603623441576
At time: 956.6543910503387 and batch: 650, loss is 5.414693822860718 and perplexity is 224.68374212070927
At time: 958.4078238010406 and batch: 700, loss is 5.384454555511475 and perplexity is 217.99116959232921
At time: 960.1601617336273 and batch: 750, loss is 5.384238882064819 and perplexity is 217.94415975501178
At time: 961.9131808280945 and batch: 800, loss is 5.388938226699829 and perplexity is 218.97076476898954
At time: 963.663286447525 and batch: 850, loss is 5.4139759063720705 and perplexity is 224.52249584511537
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.211941719055176 and perplexity of 183.44992079619226
Finished 30 epochs...
Completing Train Step...
At time: 968.079656124115 and batch: 50, loss is 5.41907039642334 and perplexity is 225.66924202979564
At time: 969.8029749393463 and batch: 100, loss is 5.369169902801514 and perplexity is 214.68458464616623
At time: 971.5193185806274 and batch: 150, loss is 5.355171957015991 and perplexity is 211.70037658820883
At time: 973.2346704006195 and batch: 200, loss is 5.3981517314910885 and perplexity is 220.99757563740488
At time: 974.9673960208893 and batch: 250, loss is 5.418811769485473 and perplexity is 225.61088543137825
At time: 976.7079360485077 and batch: 300, loss is 5.3791107654571535 and perplexity is 216.8293774995327
At time: 978.4480547904968 and batch: 350, loss is 5.356977415084839 and perplexity is 212.08293798666102
At time: 980.1976182460785 and batch: 400, loss is 5.384553232192993 and perplexity is 218.01268129887933
At time: 981.9487633705139 and batch: 450, loss is 5.423123102188111 and perplexity is 226.5856688176495
At time: 983.699892282486 and batch: 500, loss is 5.421756286621093 and perplexity is 226.2761795538002
At time: 985.4501240253448 and batch: 550, loss is 5.397909259796142 and perplexity is 220.94399647663852
At time: 987.2009036540985 and batch: 600, loss is 5.412855596542358 and perplexity is 224.2711019318582
At time: 988.9511737823486 and batch: 650, loss is 5.415120306015015 and perplexity is 224.7795863882982
At time: 990.707795381546 and batch: 700, loss is 5.385219573974609 and perplexity is 218.1580006681577
At time: 992.4627890586853 and batch: 750, loss is 5.384686479568481 and perplexity is 218.04173285196413
At time: 994.2407686710358 and batch: 800, loss is 5.389059953689575 and perplexity is 218.99742104338833
At time: 995.9906015396118 and batch: 850, loss is 5.413745527267456 and perplexity is 224.4707765113124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.211424191792806 and perplexity of 183.355005023769
Finished 31 epochs...
Completing Train Step...
At time: 1000.3800566196442 and batch: 50, loss is 5.416945180892944 and perplexity is 225.19015551327178
At time: 1002.1014721393585 and batch: 100, loss is 5.366824569702149 and perplexity is 214.18166776820036
At time: 1003.7951655387878 and batch: 150, loss is 5.353064737319946 and perplexity is 211.25474706954827
At time: 1005.5159437656403 and batch: 200, loss is 5.396603012084961 and perplexity is 220.6555773013959
At time: 1007.2452986240387 and batch: 250, loss is 5.417193727493286 and perplexity is 225.24613271703873
At time: 1008.9957633018494 and batch: 300, loss is 5.377689018249511 and perplexity is 216.5213199793889
At time: 1010.7487318515778 and batch: 350, loss is 5.355508584976196 and perplexity is 211.77165285027255
At time: 1012.5019290447235 and batch: 400, loss is 5.383559055328369 and perplexity is 217.79604583976146
At time: 1014.2656610012054 and batch: 450, loss is 5.422269239425659 and perplexity is 226.39227832875915
At time: 1016.0204889774323 and batch: 500, loss is 5.421233673095703 and perplexity is 226.15795545733843
At time: 1017.7744822502136 and batch: 550, loss is 5.39740406036377 and perplexity is 220.8324038856627
At time: 1019.5270252227783 and batch: 600, loss is 5.412735319137573 and perplexity is 224.2441288079107
At time: 1021.2819120883942 and batch: 650, loss is 5.415496788024902 and perplexity is 224.8642277907462
At time: 1023.0349881649017 and batch: 700, loss is 5.386198053359985 and perplexity is 218.37156824326158
At time: 1024.7888996601105 and batch: 750, loss is 5.384928903579712 and perplexity is 218.09459781106662
At time: 1026.542245388031 and batch: 800, loss is 5.389066543579101 and perplexity is 218.99886421695476
At time: 1028.2954998016357 and batch: 850, loss is 5.413237609863281 and perplexity is 224.35679284678767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.211164474487305 and perplexity of 183.30739073930857
Finished 32 epochs...
Completing Train Step...
At time: 1032.70809340477 and batch: 50, loss is 5.414965238571167 and perplexity is 224.74473309478313
At time: 1034.4305217266083 and batch: 100, loss is 5.364844217300415 and perplexity is 213.75793229928863
At time: 1036.1535322666168 and batch: 150, loss is 5.351459169387818 and perplexity is 210.91583536782878
At time: 1037.92307639122 and batch: 200, loss is 5.395136547088623 and perplexity is 220.3322307671706
At time: 1039.6643228530884 and batch: 250, loss is 5.415731182098389 and perplexity is 224.9169408106479
At time: 1041.4045279026031 and batch: 300, loss is 5.376205358505249 and perplexity is 216.20031420371885
At time: 1043.1452333927155 and batch: 350, loss is 5.354261713027954 and perplexity is 211.5077652681012
At time: 1044.8838968276978 and batch: 400, loss is 5.382661771774292 and perplexity is 217.60070867920302
At time: 1046.624214887619 and batch: 450, loss is 5.421527595520019 and perplexity is 226.22443812177926
At time: 1048.3649730682373 and batch: 500, loss is 5.420615882873535 and perplexity is 226.01828043316596
At time: 1050.116590499878 and batch: 550, loss is 5.39695219039917 and perplexity is 220.7326388972348
At time: 1051.8697755336761 and batch: 600, loss is 5.412526168823242 and perplexity is 224.19723298219301
At time: 1053.622062921524 and batch: 650, loss is 5.415741548538208 and perplexity is 224.91927241066435
At time: 1055.3740348815918 and batch: 700, loss is 5.386784658432007 and perplexity is 218.49970369155608
At time: 1057.12589097023 and batch: 750, loss is 5.3849865913391115 and perplexity is 218.10717956265452
At time: 1058.8763802051544 and batch: 800, loss is 5.3885144519805905 and perplexity is 218.87799015378476
At time: 1060.6261913776398 and batch: 850, loss is 5.412565994262695 and perplexity is 224.20616191331877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.21088186899821 and perplexity of 183.2555943838059
Finished 33 epochs...
Completing Train Step...
At time: 1065.0667958259583 and batch: 50, loss is 5.413300342559815 and perplexity is 224.37086779486364
At time: 1066.7840592861176 and batch: 100, loss is 5.36316575050354 and perplexity is 213.3994476438131
At time: 1068.4949910640717 and batch: 150, loss is 5.350000419616699 and perplexity is 210.60838624145177
At time: 1070.2137024402618 and batch: 200, loss is 5.393838367462158 and perplexity is 220.04638553350827
At time: 1071.9545147418976 and batch: 250, loss is 5.414364957809449 and perplexity is 224.60986363902853
At time: 1073.699861049652 and batch: 300, loss is 5.374864530563355 and perplexity is 215.9106210391343
At time: 1075.4463274478912 and batch: 350, loss is 5.353116769790649 and perplexity is 211.26573946196416
At time: 1077.198302745819 and batch: 400, loss is 5.381793594360351 and perplexity is 217.4118746412544
At time: 1078.9484376907349 and batch: 450, loss is 5.420815324783325 and perplexity is 226.06336244613493
At time: 1080.6981143951416 and batch: 500, loss is 5.419926528930664 and perplexity is 225.86252753099498
At time: 1082.4937148094177 and batch: 550, loss is 5.39632924079895 and perplexity is 220.5951764086249
At time: 1084.2435472011566 and batch: 600, loss is 5.4120259189605715 and perplexity is 224.0851063951757
At time: 1085.9941146373749 and batch: 650, loss is 5.415637683868408 and perplexity is 224.89591245786167
At time: 1087.7458491325378 and batch: 700, loss is 5.386935377120972 and perplexity is 218.53263816229347
At time: 1089.4970149993896 and batch: 750, loss is 5.384606018066406 and perplexity is 218.02418959241174
At time: 1091.2482664585114 and batch: 800, loss is 5.387812643051148 and perplexity is 218.72443351584886
At time: 1093.0018093585968 and batch: 850, loss is 5.411898784637451 and perplexity is 224.056619297735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.210534413655599 and perplexity of 183.19193230897835
Finished 34 epochs...
Completing Train Step...
At time: 1097.4535377025604 and batch: 50, loss is 5.411755867004395 and perplexity is 224.02459994415406
At time: 1099.1655683517456 and batch: 100, loss is 5.36160364151001 and perplexity is 213.0663546789265
At time: 1100.8849596977234 and batch: 150, loss is 5.348628702163697 and perplexity is 210.31968909302216
At time: 1102.6023364067078 and batch: 200, loss is 5.392507314682007 and perplexity is 219.75368702207987
At time: 1104.3207035064697 and batch: 250, loss is 5.412916164398194 and perplexity is 224.28468596300178
At time: 1106.0592844486237 and batch: 300, loss is 5.373367481231689 and perplexity is 215.58763401228657
At time: 1107.8058562278748 and batch: 350, loss is 5.351853647232056 and perplexity is 210.99905340463704
At time: 1109.5555539131165 and batch: 400, loss is 5.380730428695679 and perplexity is 217.1808526301395
At time: 1111.2938632965088 and batch: 450, loss is 5.419702892303467 and perplexity is 225.81202204477648
At time: 1113.03231215477 and batch: 500, loss is 5.419059734344483 and perplexity is 225.66683593936847
At time: 1114.773329257965 and batch: 550, loss is 5.3955209350585935 and perplexity is 220.41694010565655
At time: 1116.5260438919067 and batch: 600, loss is 5.411393690109253 and perplexity is 223.9434781012262
At time: 1118.2767946720123 and batch: 650, loss is 5.415438280105591 and perplexity is 224.85107183751813
At time: 1120.0247230529785 and batch: 700, loss is 5.386788930892944 and perplexity is 218.50063722499908
At time: 1121.7715137004852 and batch: 750, loss is 5.384173183441162 and perplexity is 217.92984159402872
At time: 1123.5208721160889 and batch: 800, loss is 5.387299203872681 and perplexity is 218.612160647505
At time: 1125.2694566249847 and batch: 850, loss is 5.411256608963012 and perplexity is 223.91278177654675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.210166613260905 and perplexity of 183.12456663329093
Finished 35 epochs...
Completing Train Step...
At time: 1129.776977300644 and batch: 50, loss is 5.410373106002807 and perplexity is 223.71504153592605
At time: 1131.5026199817657 and batch: 100, loss is 5.360272617340088 and perplexity is 212.78294686421827
At time: 1133.2295277118683 and batch: 150, loss is 5.347369689941406 and perplexity is 210.05506065404063
At time: 1134.9561564922333 and batch: 200, loss is 5.391295509338379 and perplexity is 219.4875496157981
At time: 1136.6822752952576 and batch: 250, loss is 5.411697273254394 and perplexity is 224.0114738873072
At time: 1138.4087481498718 and batch: 300, loss is 5.3721867942810055 and perplexity is 215.33324271387494
At time: 1140.1340701580048 and batch: 350, loss is 5.350847673416138 and perplexity is 210.78690060969814
At time: 1141.8591389656067 and batch: 400, loss is 5.379918518066407 and perplexity is 217.00459275073203
At time: 1143.5988957881927 and batch: 450, loss is 5.418804330825806 and perplexity is 225.6092071950261
At time: 1145.360714673996 and batch: 500, loss is 5.418319854736328 and perplexity is 225.49993140146617
At time: 1147.1152696609497 and batch: 550, loss is 5.3948970413208 and perplexity is 220.2794662460204
At time: 1148.8688502311707 and batch: 600, loss is 5.41084547996521 and perplexity is 223.82074366004554
At time: 1150.6212558746338 and batch: 650, loss is 5.415164909362793 and perplexity is 224.78961253396142
At time: 1152.3737571239471 and batch: 700, loss is 5.386568088531494 and perplexity is 218.45238835618906
At time: 1154.1252427101135 and batch: 750, loss is 5.383677530288696 and perplexity is 217.8218507462391
At time: 1155.8780512809753 and batch: 800, loss is 5.386717166900635 and perplexity is 218.48495730958305
At time: 1157.630708694458 and batch: 850, loss is 5.410522375106812 and perplexity is 223.74843777217953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.209767977396647 and perplexity of 183.05158116168403
Finished 36 epochs...
Completing Train Step...
At time: 1162.0540664196014 and batch: 50, loss is 5.409063625335693 and perplexity is 223.42228273691958
At time: 1163.7916793823242 and batch: 100, loss is 5.359017419815063 and perplexity is 212.5160297878182
At time: 1165.513685464859 and batch: 150, loss is 5.346165752410888 and perplexity is 209.80231965575902
At time: 1167.2340240478516 and batch: 200, loss is 5.3899879646301265 and perplexity is 219.20074737600763
At time: 1168.954915523529 and batch: 250, loss is 5.410417184829712 and perplexity is 223.72490284985392
At time: 1170.7034618854523 and batch: 300, loss is 5.3709745025634765 and perplexity is 215.07235417568253
At time: 1172.4381017684937 and batch: 350, loss is 5.349767036437989 and perplexity is 210.5592395220395
At time: 1174.186290025711 and batch: 400, loss is 5.379110803604126 and perplexity is 216.8293857709172
At time: 1175.9389328956604 and batch: 450, loss is 5.417783060073853 and perplexity is 225.3789167248233
At time: 1177.6897161006927 and batch: 500, loss is 5.417547674179077 and perplexity is 225.32587195008728
At time: 1179.440839290619 and batch: 550, loss is 5.394215955734253 and perplexity is 220.12948815630304
At time: 1181.1932077407837 and batch: 600, loss is 5.410196800231933 and perplexity is 223.67560275981597
At time: 1182.944301366806 and batch: 650, loss is 5.414805736541748 and perplexity is 224.70888871245515
At time: 1184.703539609909 and batch: 700, loss is 5.386005048751831 and perplexity is 218.3294255912946
At time: 1186.4554586410522 and batch: 750, loss is 5.383013830184937 and perplexity is 217.6773303257119
At time: 1188.2123470306396 and batch: 800, loss is 5.386069383621216 and perplexity is 218.34347223821263
At time: 1189.9619472026825 and batch: 850, loss is 5.4098669528961185 and perplexity is 223.60183612469177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.209324836730957 and perplexity of 182.97048153275128
Finished 37 epochs...
Completing Train Step...
At time: 1194.3992178440094 and batch: 50, loss is 5.4078168773651125 and perplexity is 223.14390502881488
At time: 1196.143013715744 and batch: 100, loss is 5.357651968002319 and perplexity is 212.22604741324173
At time: 1197.8532757759094 and batch: 150, loss is 5.344647789001465 and perplexity is 209.48408900365826
At time: 1199.5721836090088 and batch: 200, loss is 5.388604707717896 and perplexity is 218.8977460396936
At time: 1201.3129000663757 and batch: 250, loss is 5.409041433334351 and perplexity is 223.4173246043368
At time: 1203.0540459156036 and batch: 300, loss is 5.369791679382324 and perplexity is 214.81811200094933
At time: 1204.8012745380402 and batch: 350, loss is 5.34859375 and perplexity is 210.31233809328776
At time: 1206.5455341339111 and batch: 400, loss is 5.378186502456665 and perplexity is 216.62906271452317
At time: 1208.300041437149 and batch: 450, loss is 5.416758813858032 and perplexity is 225.14819140218233
At time: 1210.0539438724518 and batch: 500, loss is 5.416641731262207 and perplexity is 225.1218320106306
At time: 1211.8038182258606 and batch: 550, loss is 5.393470287322998 and perplexity is 219.9654057337417
At time: 1213.5805876255035 and batch: 600, loss is 5.409534549713134 and perplexity is 223.52752251436738
At time: 1215.3305487632751 and batch: 650, loss is 5.414369335174561 and perplexity is 224.61084684056138
At time: 1217.0809397697449 and batch: 700, loss is 5.385468053817749 and perplexity is 218.2122152692822
At time: 1218.8317306041718 and batch: 750, loss is 5.382077569961548 and perplexity is 217.47362307617246
At time: 1220.5819764137268 and batch: 800, loss is 5.38494758605957 and perplexity is 218.09867239705898
At time: 1222.333423614502 and batch: 850, loss is 5.408881750106811 and perplexity is 223.38165145313354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.208692868550618 and perplexity of 182.85488654050698
Finished 38 epochs...
Completing Train Step...
At time: 1226.7418599128723 and batch: 50, loss is 5.406319169998169 and perplexity is 222.80995090356666
At time: 1228.4651093482971 and batch: 100, loss is 5.356167669296265 and perplexity is 211.9112742321788
At time: 1230.1792261600494 and batch: 150, loss is 5.342865238189697 and perplexity is 209.11100558961255
At time: 1231.9022154808044 and batch: 200, loss is 5.387074203491211 and perplexity is 218.56297836118932
At time: 1233.6297678947449 and batch: 250, loss is 5.40732253074646 and perplexity is 223.03362185519384
At time: 1235.373803138733 and batch: 300, loss is 5.3682174968719485 and perplexity is 214.48021511158947
At time: 1237.1290369033813 and batch: 350, loss is 5.347076692581177 and perplexity is 209.9935240912317
At time: 1238.8805575370789 and batch: 400, loss is 5.376882600784302 and perplexity is 216.34678378934174
At time: 1240.6244871616364 and batch: 450, loss is 5.415471248626709 and perplexity is 224.8584849670276
At time: 1242.3760123252869 and batch: 500, loss is 5.415474252700806 and perplexity is 224.85916045959243
At time: 1244.131577014923 and batch: 550, loss is 5.392372570037842 and perplexity is 219.7240783845656
At time: 1245.8854200839996 and batch: 600, loss is 5.408712024688721 and perplexity is 223.34374112621094
At time: 1247.6392407417297 and batch: 650, loss is 5.41358868598938 and perplexity is 224.43557298858855
At time: 1249.3918693065643 and batch: 700, loss is 5.38293791770935 and perplexity is 217.66080652787696
At time: 1251.1446735858917 and batch: 750, loss is 5.380897769927978 and perplexity is 217.2171989826874
At time: 1252.898273229599 and batch: 800, loss is 5.383365736007691 and perplexity is 217.75394572564707
At time: 1254.6514995098114 and batch: 850, loss is 5.407575187683105 and perplexity is 223.08997996619468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.208140055338542 and perplexity of 182.75382987863722
Finished 39 epochs...
Completing Train Step...
At time: 1259.0892226696014 and batch: 50, loss is 5.404469614028931 and perplexity is 222.3982322945419
At time: 1260.78183054924 and batch: 100, loss is 5.354625558853149 and perplexity is 211.58473548728762
At time: 1262.4861612319946 and batch: 150, loss is 5.340945768356323 and perplexity is 208.71000829677854
At time: 1264.2228755950928 and batch: 200, loss is 5.385351247787476 and perplexity is 218.18672825520696
At time: 1265.9744396209717 and batch: 250, loss is 5.405540590286255 and perplexity is 222.63654311083494
At time: 1267.7246270179749 and batch: 300, loss is 5.3662941265106205 and perplexity is 214.06808668760118
At time: 1269.478731393814 and batch: 350, loss is 5.345102319717407 and perplexity is 209.5793275994054
At time: 1271.2304286956787 and batch: 400, loss is 5.37522213935852 and perplexity is 215.98784638359575
At time: 1272.9823467731476 and batch: 450, loss is 5.414092779159546 and perplexity is 224.54873794851974
At time: 1274.7342221736908 and batch: 500, loss is 5.413902769088745 and perplexity is 224.50607548020153
At time: 1276.4862377643585 and batch: 550, loss is 5.390644111633301 and perplexity is 219.34462248594122
At time: 1278.2381000518799 and batch: 600, loss is 5.407309503555298 and perplexity is 223.0307163724915
At time: 1279.9905245304108 and batch: 650, loss is 5.412199554443359 and perplexity is 224.12401889900804
At time: 1281.7411313056946 and batch: 700, loss is 5.381171474456787 and perplexity is 217.27666045084942
At time: 1283.4928421974182 and batch: 750, loss is 5.379190444946289 and perplexity is 216.84665504188519
At time: 1285.2431387901306 and batch: 800, loss is 5.38140944480896 and perplexity is 217.3283720069205
At time: 1286.9947128295898 and batch: 850, loss is 5.406019248962402 and perplexity is 222.743135532481
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.20699405670166 and perplexity of 182.544514199361
Finished 40 epochs...
Completing Train Step...
At time: 1291.428381204605 and batch: 50, loss is 5.402726039886475 and perplexity is 222.01080234195953
At time: 1293.1114518642426 and batch: 100, loss is 5.352728252410889 and perplexity is 211.1836749932037
At time: 1294.7992277145386 and batch: 150, loss is 5.338890457153321 and perplexity is 208.28148480396632
At time: 1296.5302212238312 and batch: 200, loss is 5.383387479782105 and perplexity is 217.75868056979718
At time: 1298.2713088989258 and batch: 250, loss is 5.403680076599121 and perplexity is 222.22270986570356
At time: 1300.0143690109253 and batch: 300, loss is 5.364207487106324 and perplexity is 213.62186949181304
At time: 1301.7829804420471 and batch: 350, loss is 5.342950191497803 and perplexity is 209.12877101590382
At time: 1303.5237782001495 and batch: 400, loss is 5.373681230545044 and perplexity is 215.65528509661135
At time: 1305.2655301094055 and batch: 450, loss is 5.412602005004882 and perplexity is 224.21423588898628
At time: 1307.0068018436432 and batch: 500, loss is 5.412384185791016 and perplexity is 224.1654030389468
At time: 1308.7479684352875 and batch: 550, loss is 5.3891512966156006 and perplexity is 219.0174258222521
At time: 1310.4889855384827 and batch: 600, loss is 5.405775098800659 and perplexity is 222.68875939815473
At time: 1312.230955839157 and batch: 650, loss is 5.411104755401611 and perplexity is 223.87878240471898
At time: 1313.9793376922607 and batch: 700, loss is 5.381461114883423 and perplexity is 217.33960167020118
At time: 1315.7309439182281 and batch: 750, loss is 5.377801074981689 and perplexity is 216.5455840104012
At time: 1317.4830913543701 and batch: 800, loss is 5.380131454467773 and perplexity is 217.05080584784307
At time: 1319.2344734668732 and batch: 850, loss is 5.4045265197753904 and perplexity is 222.4108883920609
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.206015904744466 and perplexity of 182.36604522463995
Finished 41 epochs...
Completing Train Step...
At time: 1323.673087835312 and batch: 50, loss is 5.401047201156616 and perplexity is 221.63839470219446
At time: 1325.366715669632 and batch: 100, loss is 5.351035861968994 and perplexity is 210.82657202422095
At time: 1327.0881218910217 and batch: 150, loss is 5.337190885543823 and perplexity is 207.92779615034368
At time: 1328.8102300167084 and batch: 200, loss is 5.381526594161987 and perplexity is 217.3538333764579
At time: 1330.55291056633 and batch: 250, loss is 5.401863708496093 and perplexity is 221.819437979703
At time: 1332.2939791679382 and batch: 300, loss is 5.3618432235717775 and perplexity is 213.11740767092007
At time: 1334.0348994731903 and batch: 350, loss is 5.340818796157837 and perplexity is 208.68350961051289
At time: 1335.7817680835724 and batch: 400, loss is 5.372178192138672 and perplexity is 215.33139039463896
At time: 1337.5320417881012 and batch: 450, loss is 5.411179838180542 and perplexity is 223.89559247691102
At time: 1339.2828187942505 and batch: 500, loss is 5.410858135223389 and perplexity is 223.82357618726556
At time: 1341.032496213913 and batch: 550, loss is 5.3878685474395756 and perplexity is 218.73666151333498
At time: 1342.782769203186 and batch: 600, loss is 5.40435604095459 and perplexity is 222.37297527785793
At time: 1344.5326793193817 and batch: 650, loss is 5.4104887390136716 and perplexity is 223.74091187545827
At time: 1346.3257613182068 and batch: 700, loss is 5.380443601608277 and perplexity is 217.11856821159628
At time: 1348.0736746788025 and batch: 750, loss is 5.376596212387085 and perplexity is 216.28483345204168
At time: 1349.8159606456757 and batch: 800, loss is 5.378970422744751 and perplexity is 216.79894921181003
At time: 1351.5574870109558 and batch: 850, loss is 5.403316946029663 and perplexity is 222.14202865631674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.205299377441406 and perplexity of 182.23542177730732
Finished 42 epochs...
Completing Train Step...
At time: 1355.9447274208069 and batch: 50, loss is 5.399529142379761 and perplexity is 221.30218984576067
At time: 1357.664544582367 and batch: 100, loss is 5.3496012878417964 and perplexity is 210.52434251581823
At time: 1359.3850417137146 and batch: 150, loss is 5.335535945892334 and perplexity is 207.58397277778673
At time: 1361.1083793640137 and batch: 200, loss is 5.380039472579956 and perplexity is 217.0308420231381
At time: 1362.8318066596985 and batch: 250, loss is 5.4002299499511714 and perplexity is 221.45733445288232
At time: 1364.5554995536804 and batch: 300, loss is 5.3601171875 and perplexity is 212.74987661493222
At time: 1366.2788300514221 and batch: 350, loss is 5.33925235748291 and perplexity is 208.35687558311534
At time: 1368.0024318695068 and batch: 400, loss is 5.370738725662232 and perplexity is 215.02165106001823
At time: 1369.7569677829742 and batch: 450, loss is 5.409709692001343 and perplexity is 223.56667506467164
At time: 1371.511939048767 and batch: 500, loss is 5.409510116577149 and perplexity is 223.52206110273326
At time: 1373.2688355445862 and batch: 550, loss is 5.386716995239258 and perplexity is 218.4849198041577
At time: 1375.0236620903015 and batch: 600, loss is 5.403290920257568 and perplexity is 222.13624731373855
At time: 1376.783432006836 and batch: 650, loss is 5.409757146835327 and perplexity is 223.57728463585696
At time: 1378.541631937027 and batch: 700, loss is 5.3795637512207035 and perplexity is 216.92762037029038
At time: 1380.2939140796661 and batch: 750, loss is 5.375587530136109 and perplexity is 216.06678077080585
At time: 1382.0460646152496 and batch: 800, loss is 5.377922630310058 and perplexity is 216.57190787984337
At time: 1383.798975944519 and batch: 850, loss is 5.4021883296966555 and perplexity is 221.8914569607796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.204617818196614 and perplexity of 182.1112598575249
Finished 43 epochs...
Completing Train Step...
At time: 1388.206285238266 and batch: 50, loss is 5.398228273391724 and perplexity is 221.0144918592716
At time: 1389.9375066757202 and batch: 100, loss is 5.348407106399536 and perplexity is 210.27308830425898
At time: 1391.6526417732239 and batch: 150, loss is 5.333893222808838 and perplexity is 207.24324972737193
At time: 1393.3739001750946 and batch: 200, loss is 5.379138660430908 and perplexity is 216.8354260336888
At time: 1395.0934863090515 and batch: 250, loss is 5.399358148574829 and perplexity is 221.2643517774091
At time: 1396.8147203922272 and batch: 300, loss is 5.358238887786865 and perplexity is 212.35064363968317
At time: 1398.5353834629059 and batch: 350, loss is 5.337727031707764 and perplexity is 208.03930573067208
At time: 1400.2665615081787 and batch: 400, loss is 5.36937424659729 and perplexity is 214.72845859161902
At time: 1402.0072705745697 and batch: 450, loss is 5.408110322952271 and perplexity is 223.20939523148243
At time: 1403.7487313747406 and batch: 500, loss is 5.408051490783691 and perplexity is 223.19626372499494
At time: 1405.489646434784 and batch: 550, loss is 5.3852659034729005 and perplexity is 218.1681080530101
At time: 1407.2317509651184 and batch: 600, loss is 5.402152967453003 and perplexity is 221.88361051974886
At time: 1408.9782855510712 and batch: 650, loss is 5.4082506275177 and perplexity is 223.24071472576267
At time: 1410.7182054519653 and batch: 700, loss is 5.377193193435669 and perplexity is 216.41398994686284
At time: 1412.4583864212036 and batch: 750, loss is 5.374539184570312 and perplexity is 215.84038680955604
At time: 1414.202252149582 and batch: 800, loss is 5.376616659164429 and perplexity is 216.28925582508566
At time: 1415.9536459445953 and batch: 850, loss is 5.401217994689941 and perplexity is 221.67625233957395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.20393721262614 and perplexity of 181.98735608920012
Finished 44 epochs...
Completing Train Step...
At time: 1420.412350177765 and batch: 50, loss is 5.3968516540527345 and perplexity is 220.7104483596775
At time: 1422.1262419223785 and batch: 100, loss is 5.347266607284546 and perplexity is 210.03340873628923
At time: 1423.8479599952698 and batch: 150, loss is 5.332374334335327 and perplexity is 206.9287092805487
At time: 1425.567570924759 and batch: 200, loss is 5.377031011581421 and perplexity is 216.37889437069765
At time: 1427.2881398200989 and batch: 250, loss is 5.397434730529785 and perplexity is 220.8391769560164
At time: 1429.0079329013824 and batch: 300, loss is 5.356888647079468 and perplexity is 212.06411264283915
At time: 1430.7336885929108 and batch: 350, loss is 5.336174840927124 and perplexity is 207.716639522846
At time: 1432.4804470539093 and batch: 400, loss is 5.3681955242156985 and perplexity is 214.47550246332528
At time: 1434.268054485321 and batch: 450, loss is 5.407069149017334 and perplexity is 222.9771163694662
At time: 1436.0188972949982 and batch: 500, loss is 5.406977281570435 and perplexity is 222.95663297196145
At time: 1437.7699921131134 and batch: 550, loss is 5.384096965789795 and perplexity is 217.91323212631497
At time: 1439.5209670066833 and batch: 600, loss is 5.400968179702759 and perplexity is 221.6208812059947
At time: 1441.273066520691 and batch: 650, loss is 5.407181606292725 and perplexity is 223.00219317845594
At time: 1443.0286598205566 and batch: 700, loss is 5.376158590316773 and perplexity is 216.19020314311533
At time: 1444.7849667072296 and batch: 750, loss is 5.373364181518554 and perplexity is 215.5869226361126
At time: 1446.541217803955 and batch: 800, loss is 5.375571641921997 and perplexity is 216.0633478828019
At time: 1448.295799255371 and batch: 850, loss is 5.400525808334351 and perplexity is 221.5228641550746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.203437169392903 and perplexity of 181.89637729181487
Finished 45 epochs...
Completing Train Step...
At time: 1452.777797460556 and batch: 50, loss is 5.395751152038574 and perplexity is 220.4676896694258
At time: 1454.470984697342 and batch: 100, loss is 5.3460385131835935 and perplexity is 209.77562626898035
At time: 1456.1781084537506 and batch: 150, loss is 5.331162433624268 and perplexity is 206.67808412770802
At time: 1457.8910195827484 and batch: 200, loss is 5.37579927444458 and perplexity is 216.11253652597316
At time: 1459.609367132187 and batch: 250, loss is 5.396251173019409 and perplexity is 220.57795570522492
At time: 1461.3312752246857 and batch: 300, loss is 5.355649843215942 and perplexity is 211.80156945411596
At time: 1463.0539495944977 and batch: 350, loss is 5.334870300292969 and perplexity is 207.44584139818116
At time: 1464.7863261699677 and batch: 400, loss is 5.367096271514892 and perplexity is 214.23986922195164
At time: 1466.529699087143 and batch: 450, loss is 5.406086006164551 and perplexity is 222.75800573734804
At time: 1468.2810373306274 and batch: 500, loss is 5.405953350067139 and perplexity is 222.72845748956067
At time: 1470.0245759487152 and batch: 550, loss is 5.383017845153809 and perplexity is 217.67820429517178
At time: 1471.7688274383545 and batch: 600, loss is 5.3998070907592775 and perplexity is 221.36370897998756
At time: 1473.5132915973663 and batch: 650, loss is 5.405948095321655 and perplexity is 222.7272871112795
At time: 1475.2549691200256 and batch: 700, loss is 5.375030937194825 and perplexity is 215.94655298785437
At time: 1476.9970421791077 and batch: 750, loss is 5.3723304271698 and perplexity is 215.36417387089193
At time: 1478.7873497009277 and batch: 800, loss is 5.374739770889282 and perplexity is 215.88368578067661
At time: 1480.528237581253 and batch: 850, loss is 5.399794950485229 and perplexity is 221.36102158020913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.203057924906413 and perplexity of 181.82740717270974
Finished 46 epochs...
Completing Train Step...
At time: 1484.9397139549255 and batch: 50, loss is 5.394559888839722 and perplexity is 220.20521099581924
At time: 1486.650999546051 and batch: 100, loss is 5.3449109935760495 and perplexity is 209.53923343100107
At time: 1488.3694322109222 and batch: 150, loss is 5.329946784973145 and perplexity is 206.42698884626395
At time: 1490.0946691036224 and batch: 200, loss is 5.37447509765625 and perplexity is 215.8265547084684
At time: 1491.8125894069672 and batch: 250, loss is 5.395176372528076 and perplexity is 220.34100576981956
At time: 1493.5358412265778 and batch: 300, loss is 5.354542188644409 and perplexity is 211.56709635902294
At time: 1495.2554306983948 and batch: 350, loss is 5.333590812683106 and perplexity is 207.18058674560243
At time: 1496.978539943695 and batch: 400, loss is 5.365988235473633 and perplexity is 214.00261519266067
At time: 1498.718297958374 and batch: 450, loss is 5.405132427215576 and perplexity is 222.54568963853092
At time: 1500.4674699306488 and batch: 500, loss is 5.405049781799317 and perplexity is 222.52729801737615
At time: 1502.2167975902557 and batch: 550, loss is 5.382052593231201 and perplexity is 217.4681913639648
At time: 1503.9655666351318 and batch: 600, loss is 5.3987681102752685 and perplexity is 221.1338358440192
At time: 1505.7140641212463 and batch: 650, loss is 5.4049143409729 and perplexity is 222.4971607771849
At time: 1507.4608254432678 and batch: 700, loss is 5.374083795547485 and perplexity is 215.74211784372267
At time: 1509.2072186470032 and batch: 750, loss is 5.371311388015747 and perplexity is 215.14482112883567
At time: 1510.954591035843 and batch: 800, loss is 5.373862571716309 and perplexity is 215.6943958246857
At time: 1512.7020258903503 and batch: 850, loss is 5.399021110534668 and perplexity is 221.18978983973898
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.202788352966309 and perplexity of 181.77839821181115
Finished 47 epochs...
Completing Train Step...
At time: 1517.0875318050385 and batch: 50, loss is 5.393440313339234 and perplexity is 219.95881259305335
At time: 1518.8081991672516 and batch: 100, loss is 5.343933334350586 and perplexity is 209.33447557435375
At time: 1520.5146470069885 and batch: 150, loss is 5.32886212348938 and perplexity is 206.2032068280605
At time: 1522.2712135314941 and batch: 200, loss is 5.373334302902221 and perplexity is 215.5804812933944
At time: 1523.9800074100494 and batch: 250, loss is 5.3941879558563235 and perplexity is 220.12332464379526
At time: 1525.697870016098 and batch: 300, loss is 5.353576641082764 and perplexity is 211.36291685338387
At time: 1527.4163091182709 and batch: 350, loss is 5.332375640869141 and perplexity is 206.928979640081
At time: 1529.1393630504608 and batch: 400, loss is 5.3650522041320805 and perplexity is 213.80239575811765
At time: 1530.8742458820343 and batch: 450, loss is 5.4040897178649905 and perplexity is 222.31376010557236
At time: 1532.612016916275 and batch: 500, loss is 5.404089326858521 and perplexity is 222.3136731794708
At time: 1534.3588564395905 and batch: 550, loss is 5.381328010559082 and perplexity is 217.31067475455998
At time: 1536.1059715747833 and batch: 600, loss is 5.397907056808472 and perplexity is 220.94350974027455
At time: 1537.8525047302246 and batch: 650, loss is 5.404137916564942 and perplexity is 222.32447559802503
At time: 1539.603760957718 and batch: 700, loss is 5.374564590454102 and perplexity is 215.84587049499908
At time: 1541.3537786006927 and batch: 750, loss is 5.3704749584198 and perplexity is 214.9649428712651
At time: 1543.1048521995544 and batch: 800, loss is 5.373049459457397 and perplexity is 215.5190833512818
At time: 1544.855315208435 and batch: 850, loss is 5.398220367431641 and perplexity is 221.01274453442835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.2165524164835615 and perplexity of 184.29770581245384
Annealing...
Finished 48 epochs...
Completing Train Step...
At time: 1549.2491829395294 and batch: 50, loss is 5.395422496795654 and perplexity is 220.3952437128453
At time: 1550.9690737724304 and batch: 100, loss is 5.345722599029541 and perplexity is 209.70936564635207
At time: 1552.677653312683 and batch: 150, loss is 5.328474359512329 and perplexity is 206.12326415294683
At time: 1554.3993210792542 and batch: 200, loss is 5.3717920780181885 and perplexity is 215.24826395340335
At time: 1556.1251289844513 and batch: 250, loss is 5.388531322479248 and perplexity is 218.88168276577187
At time: 1557.8507916927338 and batch: 300, loss is 5.34558180809021 and perplexity is 209.6798425461175
At time: 1559.5678961277008 and batch: 350, loss is 5.326887559890747 and perplexity is 205.7964472004732
At time: 1561.2994329929352 and batch: 400, loss is 5.355544633865357 and perplexity is 211.7792871207162
At time: 1563.0431053638458 and batch: 450, loss is 5.395437965393066 and perplexity is 220.39865294450973
At time: 1564.8354306221008 and batch: 500, loss is 5.390872020721435 and perplexity is 219.39461881593172
At time: 1566.5835857391357 and batch: 550, loss is 5.365914258956909 and perplexity is 213.9867846101718
At time: 1568.3335008621216 and batch: 600, loss is 5.378194789886475 and perplexity is 216.63085802011443
At time: 1570.0828473567963 and batch: 650, loss is 5.383111066818238 and perplexity is 217.69849756555757
At time: 1571.8326969146729 and batch: 700, loss is 5.3523179626464845 and perplexity is 211.0970462656001
At time: 1573.582192182541 and batch: 750, loss is 5.3470065212249756 and perplexity is 209.97878907784673
At time: 1575.3301088809967 and batch: 800, loss is 5.350202722549438 and perplexity is 210.65099724566812
At time: 1577.0772256851196 and batch: 850, loss is 5.382614307403564 and perplexity is 217.5903806436044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.197776158650716 and perplexity of 180.86956907435462
Finished 49 epochs...
Completing Train Step...
At time: 1581.450936794281 and batch: 50, loss is 5.387415723800659 and perplexity is 218.63763480481347
At time: 1583.174510717392 and batch: 100, loss is 5.33993766784668 and perplexity is 208.49971364792793
At time: 1584.8708319664001 and batch: 150, loss is 5.322546739578247 and perplexity is 204.9050578823371
At time: 1586.5753858089447 and batch: 200, loss is 5.366630363464355 and perplexity is 214.14007639108192
At time: 1588.2940936088562 and batch: 250, loss is 5.384615535736084 and perplexity is 218.02626468450512
At time: 1590.0292117595673 and batch: 300, loss is 5.342394008636474 and perplexity is 209.0124895175399
At time: 1591.7694156169891 and batch: 350, loss is 5.321599283218384 and perplexity is 204.71101122196555
At time: 1593.5098276138306 and batch: 400, loss is 5.3531816291809085 and perplexity is 211.27944247338806
At time: 1595.2502131462097 and batch: 450, loss is 5.390928535461426 and perplexity is 219.4070181961401
At time: 1596.990306377411 and batch: 500, loss is 5.388960208892822 and perplexity is 218.9755782795061
At time: 1598.7305119037628 and batch: 550, loss is 5.365045118331909 and perplexity is 213.80088080243246
At time: 1600.4698674678802 and batch: 600, loss is 5.37856764793396 and perplexity is 216.71164563908184
At time: 1602.2097113132477 and batch: 650, loss is 5.384433851242066 and perplexity is 217.98665629114763
At time: 1603.949009180069 and batch: 700, loss is 5.354389333724976 and perplexity is 211.53475975902148
At time: 1605.6892368793488 and batch: 750, loss is 5.349222536087036 and perplexity is 210.44462114992726
At time: 1607.428442955017 and batch: 800, loss is 5.352234992980957 and perplexity is 211.07953234084994
At time: 1609.194175004959 and batch: 850, loss is 5.383869199752808 and perplexity is 217.86360454497975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.197663307189941 and perplexity of 180.84915883095948
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f37e976d898>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 3.7725787429032467, 'batch_size': 50, 'dropout': 0.525927678662764, 'wordvec_source': 'glove', 'data': 'wikitext', 'lr': 14.391874180920635, 'num_layers': 1, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.0392143726348877 and batch: 50, loss is 7.445717935562134 and perplexity is 1712.5143258515234
At time: 3.621216058731079 and batch: 100, loss is 6.495090446472168 and perplexity is 661.8840876832438
At time: 5.231611728668213 and batch: 150, loss is 6.197386074066162 and perplexity is 491.4627135107967
At time: 6.816622495651245 and batch: 200, loss is 6.190228357315063 and perplexity is 487.95752216225867
At time: 8.401483297348022 and batch: 250, loss is 6.2240572357177735 and perplexity is 504.74696066955687
At time: 9.985429286956787 and batch: 300, loss is 6.189498052597046 and perplexity is 487.60129457480826
At time: 11.575762033462524 and batch: 350, loss is 6.192708692550659 and perplexity is 489.1693226123733
At time: 13.168105363845825 and batch: 400, loss is 6.223204269409179 and perplexity is 504.316612080256
At time: 14.75304102897644 and batch: 450, loss is 6.237095537185669 and perplexity is 511.3710935886223
At time: 16.33857035636902 and batch: 500, loss is 6.262519235610962 and perplexity is 524.538713606448
At time: 17.935722589492798 and batch: 550, loss is 6.24922960281372 and perplexity is 517.6139027296607
At time: 19.53973698616028 and batch: 600, loss is 6.279702949523926 and perplexity is 533.630125197002
At time: 21.14193606376648 and batch: 650, loss is 6.3178293418884275 and perplexity is 554.3683413446178
At time: 22.745774745941162 and batch: 700, loss is 6.300637693405151 and perplexity is 544.9192907910546
At time: 24.42053246498108 and batch: 750, loss is 6.299399375915527 and perplexity is 544.2449253282824
At time: 26.175538539886475 and batch: 800, loss is 6.3398762130737305 and perplexity is 566.7261537507774
At time: 27.932708024978638 and batch: 850, loss is 6.315968170166015 and perplexity is 553.3375262234424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.712871551513672 and perplexity of 302.7391523834536
Finished 1 epochs...
Completing Train Step...
At time: 32.41052508354187 and batch: 50, loss is 6.028956460952759 and perplexity is 415.2814407889616
At time: 34.15831136703491 and batch: 100, loss is 5.903370094299317 and perplexity is 366.2697538484914
At time: 35.90745496749878 and batch: 150, loss is 5.879763717651367 and perplexity is 357.7247076882009
At time: 37.65620803833008 and batch: 200, loss is 5.880014142990112 and perplexity is 357.8143022372056
At time: 39.404980182647705 and batch: 250, loss is 5.929467191696167 and perplexity is 375.95414895902144
At time: 41.15146827697754 and batch: 300, loss is 5.880326280593872 and perplexity is 357.92600696881345
At time: 42.89664077758789 and batch: 350, loss is 5.848478603363037 and perplexity is 346.706500918951
At time: 44.64216923713684 and batch: 400, loss is 5.847263708114624 and perplexity is 346.2855445990836
At time: 46.388057231903076 and batch: 450, loss is 5.837264776229858 and perplexity is 342.84031205282764
At time: 48.13285183906555 and batch: 500, loss is 5.866071453094483 and perplexity is 352.8600266037265
At time: 49.87798070907593 and batch: 550, loss is 5.837562885284424 and perplexity is 342.9425310895738
At time: 51.622807025909424 and batch: 600, loss is 5.846633977890015 and perplexity is 346.0675467724187
At time: 53.41333031654358 and batch: 650, loss is 5.862043075561523 and perplexity is 351.44143243510626
At time: 55.15743017196655 and batch: 700, loss is 5.814777193069458 and perplexity is 335.2167015755748
At time: 56.900551319122314 and batch: 750, loss is 5.84507963180542 and perplexity is 345.5300558677436
At time: 58.64529514312744 and batch: 800, loss is 5.842056732177735 and perplexity is 344.4871303143216
At time: 60.3922643661499 and batch: 850, loss is 5.829526920318603 and perplexity is 340.1977003771699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.488992691040039 and perplexity of 242.0133018673358
Finished 2 epochs...
Completing Train Step...
At time: 64.88039946556091 and batch: 50, loss is 5.800757913589478 and perplexity is 330.54999332615944
At time: 66.63477802276611 and batch: 100, loss is 5.707686567306519 and perplexity is 301.1735170642121
At time: 68.3906729221344 and batch: 150, loss is 5.673975143432617 and perplexity is 291.1897578345815
At time: 70.14362668991089 and batch: 200, loss is 5.719670467376709 and perplexity is 304.8044633910534
At time: 71.8961091041565 and batch: 250, loss is 5.785511455535889 and perplexity is 325.5485011234004
At time: 73.6557047367096 and batch: 300, loss is 5.762877426147461 and perplexity is 318.26279031395137
At time: 75.40714979171753 and batch: 350, loss is 5.727652988433838 and perplexity is 307.24730849800767
At time: 77.15732908248901 and batch: 400, loss is 5.717916059494018 and perplexity is 304.2701808495932
At time: 78.90862011909485 and batch: 450, loss is 5.731198005676269 and perplexity is 308.3384383987808
At time: 80.65879988670349 and batch: 500, loss is 5.764663200378418 and perplexity is 318.831643574141
At time: 82.40950870513916 and batch: 550, loss is 5.740569915771484 and perplexity is 311.2417420222373
At time: 84.1597945690155 and batch: 600, loss is 5.758703050613403 and perplexity is 316.93701098424947
At time: 85.90931177139282 and batch: 650, loss is 5.777500410079956 and perplexity is 322.95093578175175
At time: 87.66078090667725 and batch: 700, loss is 5.724931926727295 and perplexity is 306.4124060381185
At time: 89.4096462726593 and batch: 750, loss is 5.764881944656372 and perplexity is 318.90139380025664
At time: 91.1596896648407 and batch: 800, loss is 5.76040696144104 and perplexity is 317.47750373390045
At time: 92.93670320510864 and batch: 850, loss is 5.73250002861023 and perplexity is 308.7401635879498
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.446832656860352 and perplexity of 232.02210721824622
Finished 3 epochs...
Completing Train Step...
At time: 97.34848713874817 and batch: 50, loss is 5.727688732147217 and perplexity is 307.2582908540131
At time: 99.1281623840332 and batch: 100, loss is 5.667329807281494 and perplexity is 289.2611193330484
At time: 100.87795305252075 and batch: 150, loss is 5.657076015472412 and perplexity is 286.31025068785993
At time: 102.63185334205627 and batch: 200, loss is 5.684298553466797 and perplexity is 294.21139908793015
At time: 104.37839984893799 and batch: 250, loss is 5.726438322067261 and perplexity is 306.8743320929591
At time: 106.12625360488892 and batch: 300, loss is 5.69027850151062 and perplexity is 295.9760389370924
At time: 107.87407398223877 and batch: 350, loss is 5.668914957046509 and perplexity is 289.7200051336598
At time: 109.62090516090393 and batch: 400, loss is 5.649974069595337 and perplexity is 284.28409412674574
At time: 111.36723041534424 and batch: 450, loss is 5.665008640289306 and perplexity is 288.5904746110346
At time: 113.1148796081543 and batch: 500, loss is 5.699466447830201 and perplexity is 298.7079821497018
At time: 114.86348915100098 and batch: 550, loss is 5.680994510650635 and perplexity is 293.24091616977
At time: 116.61128830909729 and batch: 600, loss is 5.700420703887939 and perplexity is 298.9931620968686
At time: 118.3588593006134 and batch: 650, loss is 5.716939096450806 and perplexity is 303.97306528684567
At time: 120.10639119148254 and batch: 700, loss is 5.658099088668823 and perplexity is 286.60331691973767
At time: 121.85470390319824 and batch: 750, loss is 5.704130573272705 and perplexity is 300.10444776295736
At time: 123.60207939147949 and batch: 800, loss is 5.709002408981323 and perplexity is 301.57007457554465
At time: 125.34814691543579 and batch: 850, loss is 5.695294914245605 and perplexity is 297.46450717220625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.417359670003255 and perplexity of 225.2835137271261
Finished 4 epochs...
Completing Train Step...
At time: 129.78067231178284 and batch: 50, loss is 5.681477918624878 and perplexity is 293.3827054352904
At time: 131.49039340019226 and batch: 100, loss is 5.619819526672363 and perplexity is 275.83959705240443
At time: 133.20879817008972 and batch: 150, loss is 5.599974699020386 and perplexity is 270.4195654596858
At time: 134.92731976509094 and batch: 200, loss is 5.625577440261841 and perplexity is 277.43243893906407
At time: 136.6840922832489 and batch: 250, loss is 5.679520778656006 and perplexity is 292.8090759360197
At time: 138.42161011695862 and batch: 300, loss is 5.659469585418702 and perplexity is 286.9963751150479
At time: 140.16503620147705 and batch: 350, loss is 5.617971668243408 and perplexity is 275.3303551763021
At time: 141.91179251670837 and batch: 400, loss is 5.610208740234375 and perplexity is 273.2012601482169
At time: 143.6596565246582 and batch: 450, loss is 5.639813480377197 and perplexity is 281.410225050876
At time: 145.4086470603943 and batch: 500, loss is 5.6555967617034915 and perplexity is 285.88703826690625
At time: 147.15828394889832 and batch: 550, loss is 5.635167722702026 and perplexity is 280.1058934881139
At time: 148.9080514907837 and batch: 600, loss is 5.65414981842041 and perplexity is 285.47367506583913
At time: 150.65646052360535 and batch: 650, loss is 5.6781236743927 and perplexity is 292.40027676172423
At time: 152.40521025657654 and batch: 700, loss is 5.629653768539429 and perplexity is 278.5656527415998
At time: 154.15328550338745 and batch: 750, loss is 5.666567573547363 and perplexity is 289.0407187589677
At time: 155.90210795402527 and batch: 800, loss is 5.644865684509277 and perplexity is 282.8355644737619
At time: 157.65040397644043 and batch: 850, loss is 5.635240383148194 and perplexity is 280.126246846742
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.429302215576172 and perplexity of 227.99010196460983
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 162.09872698783875 and batch: 50, loss is 5.574227380752563 and perplexity is 263.545856354877
At time: 163.80986666679382 and batch: 100, loss is 5.435973224639892 and perplexity is 229.51611034989511
At time: 165.52648067474365 and batch: 150, loss is 5.384828634262085 and perplexity is 218.07273071088397
At time: 167.24584460258484 and batch: 200, loss is 5.395235481262207 and perplexity is 220.3540302326738
At time: 168.96536231040955 and batch: 250, loss is 5.414996376037598 and perplexity is 224.75173118531626
At time: 170.68481922149658 and batch: 300, loss is 5.380181427001953 and perplexity is 217.06165269767715
At time: 172.41433310508728 and batch: 350, loss is 5.325802049636841 and perplexity is 205.5731742512816
At time: 174.15344882011414 and batch: 400, loss is 5.326338472366333 and perplexity is 205.6834779565863
At time: 175.89311742782593 and batch: 450, loss is 5.333845329284668 and perplexity is 207.23332435546453
At time: 177.63977122306824 and batch: 500, loss is 5.342424440383911 and perplexity is 209.0188502296153
At time: 179.43316674232483 and batch: 550, loss is 5.317684469223022 and perplexity is 203.91117232200085
At time: 181.1806070804596 and batch: 600, loss is 5.31962739944458 and perplexity is 204.3077426306838
At time: 182.92834901809692 and batch: 650, loss is 5.290901956558227 and perplexity is 198.52240326570245
At time: 184.67694902420044 and batch: 700, loss is 5.263914995193481 and perplexity is 193.2365324373378
At time: 186.4253511428833 and batch: 750, loss is 5.2576918792724605 and perplexity is 192.03773309827054
At time: 188.17629885673523 and batch: 800, loss is 5.244090442657471 and perplexity is 189.44342718280944
At time: 189.9379539489746 and batch: 850, loss is 5.258035955429077 and perplexity is 192.1038200722243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.105661074320476 and perplexity of 164.95308074150762
Finished 6 epochs...
Completing Train Step...
At time: 194.38148593902588 and batch: 50, loss is 5.321625213623047 and perplexity is 204.71631953014858
At time: 196.07706665992737 and batch: 100, loss is 5.253525247573853 and perplexity is 191.23924724564378
At time: 197.78381991386414 and batch: 150, loss is 5.229619045257568 and perplexity is 186.72165746838826
At time: 199.50480198860168 and batch: 200, loss is 5.265395202636719 and perplexity is 193.52277438742615
At time: 201.24840307235718 and batch: 250, loss is 5.28546627998352 and perplexity is 197.44622721097755
At time: 202.9916911125183 and batch: 300, loss is 5.254022636413574 and perplexity is 191.33439117274276
At time: 204.7352249622345 and batch: 350, loss is 5.21740912437439 and perplexity is 184.45566276310694
At time: 206.48591351509094 and batch: 400, loss is 5.235851640701294 and perplexity is 187.8890521905083
At time: 208.2353925704956 and batch: 450, loss is 5.253419389724732 and perplexity is 191.21900414172745
At time: 209.98544716835022 and batch: 500, loss is 5.259658632278442 and perplexity is 192.41579554296237
At time: 211.7359480857849 and batch: 550, loss is 5.243131494522094 and perplexity is 189.261847838055
At time: 213.49379634857178 and batch: 600, loss is 5.259928770065308 and perplexity is 192.46778134147652
At time: 215.24451279640198 and batch: 650, loss is 5.247987213134766 and perplexity is 190.1830849392635
At time: 216.99596405029297 and batch: 700, loss is 5.219953422546387 and perplexity is 184.9255705079038
At time: 218.75208520889282 and batch: 750, loss is 5.227261381149292 and perplexity is 186.28194906437787
At time: 220.5088346004486 and batch: 800, loss is 5.234070835113525 and perplexity is 187.55475606303793
At time: 222.2596938610077 and batch: 850, loss is 5.250974826812744 and perplexity is 190.7521281425113
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.090724945068359 and perplexity of 162.50762846506842
Finished 7 epochs...
Completing Train Step...
At time: 226.68418502807617 and batch: 50, loss is 5.275135612487793 and perplexity is 195.41697569793135
At time: 228.4177587032318 and batch: 100, loss is 5.208738479614258 and perplexity is 182.86322693657957
At time: 230.13540649414062 and batch: 150, loss is 5.187613115310669 and perplexity is 179.04069301989384
At time: 231.85528087615967 and batch: 200, loss is 5.228105564117431 and perplexity is 186.43927150817066
At time: 233.57499885559082 and batch: 250, loss is 5.2448657131195064 and perplexity is 189.59035402279156
At time: 235.29597759246826 and batch: 300, loss is 5.219430932998657 and perplexity is 184.8289740677182
At time: 237.0389609336853 and batch: 350, loss is 5.1820695686340335 and perplexity is 178.05091854633878
At time: 238.78985404968262 and batch: 400, loss is 5.200924310684204 and perplexity is 181.4398712039213
At time: 240.54026174545288 and batch: 450, loss is 5.222250213623047 and perplexity is 185.35079404597664
At time: 242.2885913848877 and batch: 500, loss is 5.227973318099975 and perplexity is 186.4146172872635
At time: 244.03818154335022 and batch: 550, loss is 5.215008249282837 and perplexity is 184.0133389514643
At time: 245.78636074066162 and batch: 600, loss is 5.235805416107178 and perplexity is 187.88036729606137
At time: 247.53321766853333 and batch: 650, loss is 5.226333541870117 and perplexity is 186.1091895139769
At time: 249.28207635879517 and batch: 700, loss is 5.197698640823364 and perplexity is 180.85554900173574
At time: 251.03050255775452 and batch: 750, loss is 5.206005706787109 and perplexity is 182.36418547297026
At time: 252.7793161869049 and batch: 800, loss is 5.208705968856812 and perplexity is 182.85728201120034
At time: 254.53057026863098 and batch: 850, loss is 5.225726709365845 and perplexity is 185.996286668459
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.082181930541992 and perplexity of 161.1252367383394
Finished 8 epochs...
Completing Train Step...
At time: 258.93120551109314 and batch: 50, loss is 5.2425314235687255 and perplexity is 189.148311368962
At time: 260.66914081573486 and batch: 100, loss is 5.175692129135132 and perplexity is 176.9190227205044
At time: 262.38623309135437 and batch: 150, loss is 5.155373744964599 and perplexity is 173.3605872989937
At time: 264.1056914329529 and batch: 200, loss is 5.200216541290283 and perplexity is 181.31149905054562
At time: 265.82501220703125 and batch: 250, loss is 5.215930738449097 and perplexity is 184.18316758358742
At time: 267.59423327445984 and batch: 300, loss is 5.19044189453125 and perplexity is 179.5478766289675
At time: 269.31875252723694 and batch: 350, loss is 5.154816884994506 and perplexity is 173.2640766015011
At time: 271.0675129890442 and batch: 400, loss is 5.1771440601348875 and perplexity is 177.1760835061413
At time: 272.8162350654602 and batch: 450, loss is 5.199559497833252 and perplexity is 181.19240864448034
At time: 274.5640308856964 and batch: 500, loss is 5.203016262054444 and perplexity is 181.81983188216594
At time: 276.32040786743164 and batch: 550, loss is 5.191426010131836 and perplexity is 179.72465946851767
At time: 278.0751414299011 and batch: 600, loss is 5.210735635757446 and perplexity is 183.22879828363915
At time: 279.8255088329315 and batch: 650, loss is 5.2028947353363035 and perplexity is 181.79773725727532
At time: 281.5823292732239 and batch: 700, loss is 5.176652250289917 and perplexity is 177.0889679878703
At time: 283.3324134349823 and batch: 750, loss is 5.1824880218505855 and perplexity is 178.12544011672256
At time: 285.08372712135315 and batch: 800, loss is 5.182770481109619 and perplexity is 178.1757604029338
At time: 286.8325946331024 and batch: 850, loss is 5.200722150802612 and perplexity is 181.40319504839115
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.067029635111491 and perplexity of 158.7022230183771
Finished 9 epochs...
Completing Train Step...
At time: 291.2680130004883 and batch: 50, loss is 5.210627040863037 and perplexity is 183.20890165199268
At time: 293.0193738937378 and batch: 100, loss is 5.138909139633179 and perplexity is 170.52964279260803
At time: 294.7355065345764 and batch: 150, loss is 5.122994709014892 and perplexity is 167.8372414660106
At time: 296.4568045139313 and batch: 200, loss is 5.167749242782593 and perplexity is 175.51934114403227
At time: 298.1821799278259 and batch: 250, loss is 5.183687324523926 and perplexity is 178.33919458577682
At time: 299.90710711479187 and batch: 300, loss is 5.159487676620484 and perplexity is 174.07524993564974
At time: 301.64917039871216 and batch: 350, loss is 5.12687201499939 and perplexity is 168.4892610296017
At time: 303.39443588256836 and batch: 400, loss is 5.146706495285034 and perplexity is 171.8645205617695
At time: 305.1398675441742 and batch: 450, loss is 5.17045862197876 and perplexity is 175.99553439820028
At time: 306.89354062080383 and batch: 500, loss is 5.170899677276611 and perplexity is 176.07317528174764
At time: 308.6478600502014 and batch: 550, loss is 5.161059265136719 and perplexity is 174.34903968547016
At time: 310.4010229110718 and batch: 600, loss is 5.183595275878906 and perplexity is 178.32277946006786
At time: 312.1817674636841 and batch: 650, loss is 5.172073755264282 and perplexity is 176.28002032340981
At time: 313.9356687068939 and batch: 700, loss is 5.15048731803894 and perplexity is 172.5155397703111
At time: 315.6976044178009 and batch: 750, loss is 5.155170927047729 and perplexity is 173.3254302311725
At time: 317.4601399898529 and batch: 800, loss is 5.1598616790771485 and perplexity is 174.14036668292295
At time: 319.2138023376465 and batch: 850, loss is 5.174816732406616 and perplexity is 176.764216155269
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.057843526204427 and perplexity of 157.2510426690614
Finished 10 epochs...
Completing Train Step...
At time: 323.64821314811707 and batch: 50, loss is 5.183562440872192 and perplexity is 178.31692432653432
At time: 325.3633625507355 and batch: 100, loss is 5.115614166259766 and perplexity is 166.60307155371643
At time: 327.07388257980347 and batch: 150, loss is 5.09902928352356 and perplexity is 163.8627657851303
At time: 328.79309344291687 and batch: 200, loss is 5.140043869018554 and perplexity is 170.72325761880376
At time: 330.54557967185974 and batch: 250, loss is 5.154851999282837 and perplexity is 173.27016075306386
At time: 332.29337215423584 and batch: 300, loss is 5.133043308258056 and perplexity is 169.5322727256314
At time: 334.0387794971466 and batch: 350, loss is 5.1006300735473635 and perplexity is 164.1252857294664
At time: 335.78048753738403 and batch: 400, loss is 5.124350938796997 and perplexity is 168.06502175772667
At time: 337.5185327529907 and batch: 450, loss is 5.148911027908325 and perplexity is 172.24381943889358
At time: 339.25885224342346 and batch: 500, loss is 5.154817419052124 and perplexity is 173.2641691345258
At time: 341.00502347946167 and batch: 550, loss is 5.143092937469483 and perplexity is 171.24459891619875
At time: 342.74722123146057 and batch: 600, loss is 5.16716477394104 and perplexity is 175.41678553124322
At time: 344.4962418079376 and batch: 650, loss is 5.154821605682373 and perplexity is 173.26489452905577
At time: 346.24758744239807 and batch: 700, loss is 5.133380460739136 and perplexity is 169.58944058860575
At time: 347.9986627101898 and batch: 750, loss is 5.136443691253662 and perplexity is 170.10972861219506
At time: 349.74851059913635 and batch: 800, loss is 5.1369130992889405 and perplexity is 170.1895982299382
At time: 351.50050616264343 and batch: 850, loss is 5.152190084457398 and perplexity is 172.80954367704712
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.051708221435547 and perplexity of 156.2892131727046
Finished 11 epochs...
Completing Train Step...
At time: 355.97152876853943 and batch: 50, loss is 5.1586691188812255 and perplexity is 173.93281759510188
At time: 357.6856439113617 and batch: 100, loss is 5.093484706878662 and perplexity is 162.95673023390395
At time: 359.40581011772156 and batch: 150, loss is 5.0812187767028805 and perplexity is 160.970123059156
At time: 361.1264307498932 and batch: 200, loss is 5.121448163986206 and perplexity is 167.5778742278455
At time: 362.8470506668091 and batch: 250, loss is 5.133060464859009 and perplexity is 169.53518134813407
At time: 364.57982301712036 and batch: 300, loss is 5.1133565425872805 and perplexity is 166.2273687727895
At time: 366.32302808761597 and batch: 350, loss is 5.082488718032837 and perplexity is 161.17467552864363
At time: 368.0740005970001 and batch: 400, loss is 5.105067510604858 and perplexity is 164.85519963021196
At time: 369.82577681541443 and batch: 450, loss is 5.130215902328491 and perplexity is 169.0536131737778
At time: 371.5752673149109 and batch: 500, loss is 5.134019336700439 and perplexity is 169.69782182288498
At time: 373.33037853240967 and batch: 550, loss is 5.126307821273803 and perplexity is 168.3942272569501
At time: 375.08116698265076 and batch: 600, loss is 5.147507705688477 and perplexity is 172.0022753815544
At time: 376.83249855041504 and batch: 650, loss is 5.137590589523316 and perplexity is 170.30493908747187
At time: 378.5827350616455 and batch: 700, loss is 5.1182863140106205 and perplexity is 167.04885491089686
At time: 380.33274960517883 and batch: 750, loss is 5.1183003807067875 and perplexity is 167.05120475291113
At time: 382.0828356742859 and batch: 800, loss is 5.119478359222412 and perplexity is 167.24810343155724
At time: 383.83332800865173 and batch: 850, loss is 5.134532251358032 and perplexity is 169.78488464905064
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.048139890034993 and perplexity of 155.73251529772875
Finished 12 epochs...
Completing Train Step...
At time: 388.29443192481995 and batch: 50, loss is 5.137851686477661 and perplexity is 170.34941099386452
At time: 390.00342631340027 and batch: 100, loss is 5.075166311264038 and perplexity is 159.9987993693385
At time: 391.7230398654938 and batch: 150, loss is 5.066504240036011 and perplexity is 158.6188635521795
At time: 393.44381380081177 and batch: 200, loss is 5.107352437973023 and perplexity is 165.23231246127347
At time: 395.18744802474976 and batch: 250, loss is 5.115554037094117 and perplexity is 166.59305415120136
At time: 396.93072724342346 and batch: 300, loss is 5.092593212127685 and perplexity is 162.8115199010061
At time: 398.67233443260193 and batch: 350, loss is 5.05972505569458 and perplexity is 157.54719366424774
At time: 400.4594826698303 and batch: 400, loss is 5.0861788082122805 and perplexity is 161.77052330587597
At time: 402.2130424976349 and batch: 450, loss is 5.1106296825408934 and perplexity is 165.7747074547885
At time: 403.96195220947266 and batch: 500, loss is 5.1187458419799805 and perplexity is 167.12563617219539
At time: 405.71103715896606 and batch: 550, loss is 5.108549289703369 and perplexity is 165.43018943142806
At time: 407.4596838951111 and batch: 600, loss is 5.132493114471435 and perplexity is 169.43902277766935
At time: 409.2094874382019 and batch: 650, loss is 5.124224472045898 and perplexity is 168.0437684643974
At time: 410.95850825309753 and batch: 700, loss is 5.103664855957032 and perplexity is 164.62412681379743
At time: 412.7079713344574 and batch: 750, loss is 5.107158985137939 and perplexity is 165.20035089360803
At time: 414.457594871521 and batch: 800, loss is 5.104023218154907 and perplexity is 164.68313244976898
At time: 416.2077717781067 and batch: 850, loss is 5.118315906524658 and perplexity is 167.05379837962522
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.047741254170735 and perplexity of 155.67044710402595
Finished 13 epochs...
Completing Train Step...
At time: 420.6166021823883 and batch: 50, loss is 5.122426681518554 and perplexity is 167.74193236960474
At time: 422.34632778167725 and batch: 100, loss is 5.060014095306396 and perplexity is 157.5927376256346
At time: 424.0633509159088 and batch: 150, loss is 5.051336317062378 and perplexity is 156.23109933791105
At time: 425.7913644313812 and batch: 200, loss is 5.087575769424438 and perplexity is 161.99666837337168
At time: 427.52449893951416 and batch: 250, loss is 5.09650369644165 and perplexity is 163.44943826768062
At time: 429.2485599517822 and batch: 300, loss is 5.076472644805908 and perplexity is 160.2079477466411
At time: 430.98454570770264 and batch: 350, loss is 5.042983074188232 and perplexity is 154.93149851809923
At time: 432.72935962677 and batch: 400, loss is 5.065088195800781 and perplexity is 158.39441117959757
At time: 434.4734892845154 and batch: 450, loss is 5.090999727249145 and perplexity is 162.5522888012637
At time: 436.2173352241516 and batch: 500, loss is 5.0991786670684816 and perplexity is 163.88724601439014
At time: 437.96682119369507 and batch: 550, loss is 5.096423454284668 and perplexity is 163.43632325839187
At time: 439.72310757637024 and batch: 600, loss is 5.118199911117554 and perplexity is 167.03442203007933
At time: 441.4795060157776 and batch: 650, loss is 5.110664663314819 and perplexity is 165.78050648377922
At time: 443.2747423648834 and batch: 700, loss is 5.088172159194946 and perplexity is 162.09331034452296
At time: 445.028596162796 and batch: 750, loss is 5.089742250442505 and perplexity is 162.34801153211163
At time: 446.7757291793823 and batch: 800, loss is 5.086816787719727 and perplexity is 161.87376251340163
At time: 448.52038764953613 and batch: 850, loss is 5.0968241882324214 and perplexity is 163.50183086610193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.032933870951335 and perplexity of 153.3823572562245
Finished 14 epochs...
Completing Train Step...
At time: 452.9440085887909 and batch: 50, loss is 5.100896997451782 and perplexity is 164.169100538897
At time: 454.6897306442261 and batch: 100, loss is 5.037702503204346 and perplexity is 154.11552803363836
At time: 456.4125590324402 and batch: 150, loss is 5.031363677978516 and perplexity is 153.14170634033871
At time: 458.15333223342896 and batch: 200, loss is 5.06662356376648 and perplexity is 158.63779167596599
At time: 459.8926260471344 and batch: 250, loss is 5.076615505218506 and perplexity is 160.23083675508528
At time: 461.64254450798035 and batch: 300, loss is 5.059497871398926 and perplexity is 157.51140548143334
At time: 463.3925633430481 and batch: 350, loss is 5.021344747543335 and perplexity is 151.61505073625918
At time: 465.1438195705414 and batch: 400, loss is 5.041621685028076 and perplexity is 154.7207199638239
At time: 466.89439630508423 and batch: 450, loss is 5.069117050170899 and perplexity is 159.03384642609953
At time: 468.6426479816437 and batch: 500, loss is 5.08361985206604 and perplexity is 161.3570888369257
At time: 470.3914518356323 and batch: 550, loss is 5.0786900806427 and perplexity is 160.5635927555813
At time: 472.1404845714569 and batch: 600, loss is 5.1009706687927245 and perplexity is 164.1811955421971
At time: 473.88967299461365 and batch: 650, loss is 5.094249610900879 and perplexity is 163.08142417567626
At time: 475.6394021511078 and batch: 700, loss is 5.073736591339111 and perplexity is 159.7702093467092
At time: 477.3888740539551 and batch: 750, loss is 5.074140357971191 and perplexity is 159.8347322512652
At time: 479.1435430049896 and batch: 800, loss is 5.06832109451294 and perplexity is 158.90731290044792
At time: 480.89829182624817 and batch: 850, loss is 5.082794303894043 and perplexity is 161.22393575690091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.02658748626709 and perplexity of 152.41201614910497
Finished 15 epochs...
Completing Train Step...
At time: 485.34431195259094 and batch: 50, loss is 5.084615039825439 and perplexity is 161.51774936706525
At time: 487.0746748447418 and batch: 100, loss is 5.019159736633301 and perplexity is 151.28413185861223
At time: 488.7854073047638 and batch: 150, loss is 5.015918979644775 and perplexity is 150.79465032478757
At time: 490.5117840766907 and batch: 200, loss is 5.056073961257934 and perplexity is 156.97302279581425
At time: 492.25076723098755 and batch: 250, loss is 5.064409761428833 and perplexity is 158.28698741083778
At time: 493.9919295310974 and batch: 300, loss is 5.043197193145752 and perplexity is 154.96467584086906
At time: 495.7322599887848 and batch: 350, loss is 5.007162160873413 and perplexity is 149.47993366918107
At time: 497.47178769111633 and batch: 400, loss is 5.030849313735962 and perplexity is 153.06295597747513
At time: 499.21911001205444 and batch: 450, loss is 5.053364601135254 and perplexity is 156.54830196816144
At time: 500.96941447257996 and batch: 500, loss is 5.06397852897644 and perplexity is 158.21874364059641
At time: 502.7183663845062 and batch: 550, loss is 5.058777141571045 and perplexity is 157.39792321321158
At time: 504.467796087265 and batch: 600, loss is 5.0810133934021 and perplexity is 160.93706587876767
At time: 506.2175226211548 and batch: 650, loss is 5.077166233062744 and perplexity is 160.3191046419525
At time: 507.96699810028076 and batch: 700, loss is 5.059625720977783 and perplexity is 157.53154453564667
At time: 509.7174987792969 and batch: 750, loss is 5.057599868774414 and perplexity is 157.21273195167282
At time: 511.4663333892822 and batch: 800, loss is 5.051068687438965 and perplexity is 156.18929286220506
At time: 513.216046333313 and batch: 850, loss is 5.063660211563111 and perplexity is 158.16838787436433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.021867116292317 and perplexity of 151.6942703897735
Finished 16 epochs...
Completing Train Step...
At time: 517.6503798961639 and batch: 50, loss is 5.065071115493774 and perplexity is 158.39170577753103
At time: 519.3467223644257 and batch: 100, loss is 5.001168842315674 and perplexity is 148.5867321031113
At time: 521.0496106147766 and batch: 150, loss is 5.001175441741943 and perplexity is 148.5877126935301
At time: 522.7644286155701 and batch: 200, loss is 5.040688238143921 and perplexity is 154.5763637747586
At time: 524.5021119117737 and batch: 250, loss is 5.046411256790162 and perplexity is 155.46354343831078
At time: 526.2455811500549 and batch: 300, loss is 5.026496210098267 and perplexity is 152.39810519906703
At time: 527.9885723590851 and batch: 350, loss is 4.992394561767578 and perplexity is 147.28869343540288
At time: 529.7368018627167 and batch: 400, loss is 5.0117784595489505 and perplexity is 150.1715728673068
At time: 531.5378122329712 and batch: 450, loss is 5.034771175384521 and perplexity is 153.66442638526647
At time: 533.2898142337799 and batch: 500, loss is 5.044668140411377 and perplexity is 155.19278843673496
At time: 535.0419704914093 and batch: 550, loss is 5.029488430023194 and perplexity is 152.85479676602353
At time: 536.7935733795166 and batch: 600, loss is 5.0568328285217286 and perplexity is 157.09218969432314
At time: 538.5451529026031 and batch: 650, loss is 5.051216068267823 and perplexity is 156.2123138660314
At time: 540.2966141700745 and batch: 700, loss is 5.032789487838745 and perplexity is 153.36021303273185
At time: 542.0482382774353 and batch: 750, loss is 5.04389063835144 and perplexity is 155.07217261963729
At time: 543.7997584342957 and batch: 800, loss is 5.037874307632446 and perplexity is 154.14200803841942
At time: 545.5559804439545 and batch: 850, loss is 5.050945100784301 and perplexity is 156.16999114274793
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.018406232198079 and perplexity of 151.17018153070822
Finished 17 epochs...
Completing Train Step...
At time: 549.995185136795 and batch: 50, loss is 5.04803240776062 and perplexity is 155.7157777123037
At time: 551.7001719474792 and batch: 100, loss is 4.985441007614136 and perplexity is 146.26806613544747
At time: 553.438257932663 and batch: 150, loss is 4.98108588218689 and perplexity is 145.6324354916895
At time: 555.1841044425964 and batch: 200, loss is 5.023235721588135 and perplexity is 151.90202210419437
At time: 556.9405660629272 and batch: 250, loss is 5.029098882675171 and perplexity is 152.79526418144377
At time: 558.6810233592987 and batch: 300, loss is 5.010229902267456 and perplexity is 149.93920354970476
At time: 560.4216740131378 and batch: 350, loss is 4.970006580352783 and perplexity is 144.02783512176703
At time: 562.1616911888123 and batch: 400, loss is 4.990208406448364 and perplexity is 146.96704918503553
At time: 563.8985810279846 and batch: 450, loss is 5.013521947860718 and perplexity is 150.43362362419577
At time: 565.6423587799072 and batch: 500, loss is 5.020811681747436 and perplexity is 151.53425147614257
At time: 567.3891425132751 and batch: 550, loss is 5.01026237487793 and perplexity is 149.94407254611042
At time: 569.1370687484741 and batch: 600, loss is 5.03929178237915 and perplexity is 154.36065536914552
At time: 570.8847801685333 and batch: 650, loss is 5.0358838367462155 and perplexity is 153.8354980099166
At time: 572.634352684021 and batch: 700, loss is 5.016313428878784 and perplexity is 150.8541428917282
At time: 574.3822963237762 and batch: 750, loss is 5.029077701568603 and perplexity is 152.7920278429449
At time: 576.1704905033112 and batch: 800, loss is 5.018799076080322 and perplexity is 151.22957947799634
At time: 577.919109582901 and batch: 850, loss is 5.031169366836548 and perplexity is 153.1119520913815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.015777587890625 and perplexity of 150.77333071190594
Finished 18 epochs...
Completing Train Step...
At time: 582.33056640625 and batch: 50, loss is 5.029779233932495 and perplexity is 152.89925400233435
At time: 584.0091323852539 and batch: 100, loss is 4.968771228790283 and perplexity is 143.85001996533504
At time: 585.6888027191162 and batch: 150, loss is 4.965606212615967 and perplexity is 143.39545206231463
At time: 587.3978672027588 and batch: 200, loss is 5.007882852554321 and perplexity is 149.5877014428435
At time: 589.1179478168488 and batch: 250, loss is 5.012620544433593 and perplexity is 150.29808333772306
At time: 590.8384008407593 and batch: 300, loss is 4.994982032775879 and perplexity is 147.6702921344023
At time: 592.5652248859406 and batch: 350, loss is 4.954841108322143 and perplexity is 141.86006419850256
At time: 594.2858679294586 and batch: 400, loss is 4.97680404663086 and perplexity is 145.0101944688442
At time: 596.0051550865173 and batch: 450, loss is 4.9970571804046635 and perplexity is 147.97704796270148
At time: 597.7256515026093 and batch: 500, loss is 5.002218265533447 and perplexity is 148.74274431673504
At time: 599.445992231369 and batch: 550, loss is 4.99403039932251 and perplexity is 147.52983098870115
At time: 601.1678240299225 and batch: 600, loss is 5.022910804748535 and perplexity is 151.8526745965957
At time: 602.8884122371674 and batch: 650, loss is 5.0199280834198 and perplexity is 151.4004152024201
At time: 604.6093509197235 and batch: 700, loss is 5.005452852249146 and perplexity is 149.22464457548745
At time: 606.3300716876984 and batch: 750, loss is 5.014551830291748 and perplexity is 150.58863237689715
At time: 608.050372838974 and batch: 800, loss is 5.004702262878418 and perplexity is 149.11268016832193
At time: 609.7704074382782 and batch: 850, loss is 5.0135673236846925 and perplexity is 150.44044982869235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.012132962544759 and perplexity of 150.2248185770977
Finished 19 epochs...
Completing Train Step...
At time: 614.1445009708405 and batch: 50, loss is 5.0130800533294675 and perplexity is 150.3671625140821
At time: 615.8816974163055 and batch: 100, loss is 4.953605623245239 and perplexity is 141.68490643086105
At time: 617.6072437763214 and batch: 150, loss is 4.9524991321563725 and perplexity is 141.52822004649863
At time: 619.3576800823212 and batch: 200, loss is 4.99528377532959 and perplexity is 147.71485726871867
At time: 621.0785069465637 and batch: 250, loss is 4.999628324508667 and perplexity is 148.358007818572
At time: 622.8115549087524 and batch: 300, loss is 4.980419292449951 and perplexity is 145.53539075293216
At time: 624.5519499778748 and batch: 350, loss is 4.941677350997924 and perplexity is 140.00489005980728
At time: 626.2914237976074 and batch: 400, loss is 4.960045671463012 and perplexity is 142.60030851298342
At time: 628.0294854640961 and batch: 450, loss is 4.978464908599854 and perplexity is 145.25123649931584
At time: 629.7687749862671 and batch: 500, loss is 4.99135160446167 and perplexity is 147.13515769603478
At time: 631.5078637599945 and batch: 550, loss is 4.982981643676758 and perplexity is 145.90878171507075
At time: 633.2471852302551 and batch: 600, loss is 5.018613796234131 and perplexity is 151.20156228036234
At time: 634.9862921237946 and batch: 650, loss is 5.008002090454101 and perplexity is 149.6055390296336
At time: 636.726104259491 and batch: 700, loss is 4.993329763412476 and perplexity is 147.4265024933633
At time: 638.4667439460754 and batch: 750, loss is 5.001133966445923 and perplexity is 148.58155010195964
At time: 640.2061107158661 and batch: 800, loss is 4.992888870239258 and perplexity is 147.36151748163934
At time: 641.9461793899536 and batch: 850, loss is 4.998107223510742 and perplexity is 148.13251184937943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.017111142476399 and perplexity of 150.97452930332432
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 646.3241050243378 and batch: 50, loss is 4.994598121643066 and perplexity is 147.6136107462703
At time: 648.0470132827759 and batch: 100, loss is 4.934699745178222 and perplexity is 139.03139141875363
At time: 649.7500250339508 and batch: 150, loss is 4.920932121276856 and perplexity is 137.13037577306707
At time: 651.4721348285675 and batch: 200, loss is 4.95586012840271 and perplexity is 142.00469613145782
At time: 653.1968564987183 and batch: 250, loss is 4.953130502700805 and perplexity is 141.61760501038938
At time: 654.9478445053101 and batch: 300, loss is 4.919263324737549 and perplexity is 136.90172391632078
At time: 656.6992743015289 and batch: 350, loss is 4.872859382629395 and perplexity is 130.6940875270331
At time: 658.4505829811096 and batch: 400, loss is 4.891873197555542 and perplexity is 133.20285576525345
At time: 660.2020225524902 and batch: 450, loss is 4.912411556243897 and perplexity is 135.96691121496877
At time: 662.0014567375183 and batch: 500, loss is 4.91648497581482 and perplexity is 136.52189105759953
At time: 663.745325088501 and batch: 550, loss is 4.888144617080688 and perplexity is 132.7071229621967
At time: 665.4905972480774 and batch: 600, loss is 4.914303045272828 and perplexity is 136.22433451575486
At time: 667.2356934547424 and batch: 650, loss is 4.904087314605713 and perplexity is 134.83978752466913
At time: 668.9794485569 and batch: 700, loss is 4.890412044525147 and perplexity is 133.00836813159444
At time: 670.7222738265991 and batch: 750, loss is 4.873672256469726 and perplexity is 130.8003685225381
At time: 672.4709661006927 and batch: 800, loss is 4.853280277252197 and perplexity is 128.16010175038446
At time: 674.2136545181274 and batch: 850, loss is 4.891341438293457 and perplexity is 133.13204274237316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.944114367167155 and perplexity of 140.34650032645615
Finished 21 epochs...
Completing Train Step...
At time: 678.5720646381378 and batch: 50, loss is 4.932711162567139 and perplexity is 138.7551907263493
At time: 680.2892291545868 and batch: 100, loss is 4.880848255157471 and perplexity is 131.74236764529977
At time: 681.9994337558746 and batch: 150, loss is 4.8714713287353515 and perplexity is 130.5128029354111
At time: 683.7187674045563 and batch: 200, loss is 4.914168367385864 and perplexity is 136.20598934560135
At time: 685.438319683075 and batch: 250, loss is 4.915026521682739 and perplexity is 136.32292526797426
At time: 687.1589825153351 and batch: 300, loss is 4.890011777877808 and perplexity is 132.95513997145366
At time: 688.8791069984436 and batch: 350, loss is 4.8437760639190675 and perplexity is 126.94781086367088
At time: 690.5981729030609 and batch: 400, loss is 4.862354927062988 and perplexity is 129.32840273726595
At time: 692.3380300998688 and batch: 450, loss is 4.8850454998016355 and perplexity is 132.2964846611638
At time: 694.0791354179382 and batch: 500, loss is 4.890160655975341 and perplexity is 132.97493555327765
At time: 695.819830417633 and batch: 550, loss is 4.87286566734314 and perplexity is 130.69490890454244
At time: 697.5602447986603 and batch: 600, loss is 4.903774108886719 and perplexity is 134.7975615451228
At time: 699.3154625892639 and batch: 650, loss is 4.897592439651489 and perplexity is 133.9668578190763
At time: 701.0575532913208 and batch: 700, loss is 4.884051256179809 and perplexity is 132.16501509231958
At time: 702.8032312393188 and batch: 750, loss is 4.877129535675049 and perplexity is 131.2533645323086
At time: 704.5535621643066 and batch: 800, loss is 4.858064365386963 and perplexity is 128.77469994626966
At time: 706.3512222766876 and batch: 850, loss is 4.892741794586182 and perplexity is 133.31860563296217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.941751162211101 and perplexity of 140.01522437198255
Finished 22 epochs...
Completing Train Step...
At time: 710.7546336650848 and batch: 50, loss is 4.917064304351807 and perplexity is 136.601004999282
At time: 712.4425938129425 and batch: 100, loss is 4.865435762405395 and perplexity is 129.72745664525115
At time: 714.1431510448456 and batch: 150, loss is 4.85666714668274 and perplexity is 128.59489916663102
At time: 715.8514335155487 and batch: 200, loss is 4.901681652069092 and perplexity is 134.5157983599226
At time: 717.5737781524658 and batch: 250, loss is 4.9032344722747805 and perplexity is 134.72483946922517
At time: 719.3123593330383 and batch: 300, loss is 4.87738145828247 and perplexity is 131.28643438748156
At time: 721.051509141922 and batch: 350, loss is 4.832764301300049 and perplexity is 125.55756030831225
At time: 722.7974066734314 and batch: 400, loss is 4.853236494064331 and perplexity is 128.15449061541014
At time: 724.5480363368988 and batch: 450, loss is 4.8776859951019285 and perplexity is 131.32642202919348
At time: 726.2988936901093 and batch: 500, loss is 4.8830219078063966 and perplexity is 132.02904124324843
At time: 728.0474956035614 and batch: 550, loss is 4.867346839904785 and perplexity is 129.975612916105
At time: 729.7960970401764 and batch: 600, loss is 4.9007952022552494 and perplexity is 134.3966096906992
At time: 731.5452253818512 and batch: 650, loss is 4.895681495666504 and perplexity is 133.71109910605637
At time: 733.2955930233002 and batch: 700, loss is 4.881841096878052 and perplexity is 131.87323191727089
At time: 735.0455455780029 and batch: 750, loss is 4.877359161376953 and perplexity is 131.2835071388927
At time: 736.7945415973663 and batch: 800, loss is 4.856977596282959 and perplexity is 128.6348275992298
At time: 738.5457427501678 and batch: 850, loss is 4.889698171615601 and perplexity is 132.91345094427768
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.939735094706218 and perplexity of 139.7332285847239
Finished 23 epochs...
Completing Train Step...
At time: 742.9612777233124 and batch: 50, loss is 4.907595043182373 and perplexity is 135.3135994165755
At time: 744.657476902008 and batch: 100, loss is 4.859317226409912 and perplexity is 128.93613785709044
At time: 746.3614366054535 and batch: 150, loss is 4.848986129760743 and perplexity is 127.61094329850242
At time: 748.0932431221008 and batch: 200, loss is 4.894083518981933 and perplexity is 133.4976025139356
At time: 749.8655023574829 and batch: 250, loss is 4.895146732330322 and perplexity is 133.63961442800988
At time: 751.6091771125793 and batch: 300, loss is 4.87042426109314 and perplexity is 130.37621872149717
At time: 753.3517787456512 and batch: 350, loss is 4.8256188488006595 and perplexity is 124.66359243133364
At time: 755.093962430954 and batch: 400, loss is 4.848127317428589 and perplexity is 127.50139649348657
At time: 756.8367486000061 and batch: 450, loss is 4.873644399642944 and perplexity is 130.79672489007936
At time: 758.5810370445251 and batch: 500, loss is 4.876908435821533 and perplexity is 131.22434764057044
At time: 760.325528383255 and batch: 550, loss is 4.863883409500122 and perplexity is 129.52623007876636
At time: 762.0852239131927 and batch: 600, loss is 4.898743515014648 and perplexity is 134.12115255398933
At time: 763.8441519737244 and batch: 650, loss is 4.893839225769043 and perplexity is 133.46499393889331
At time: 765.5983953475952 and batch: 700, loss is 4.881695804595947 and perplexity is 131.85407314630154
At time: 767.3533809185028 and batch: 750, loss is 4.874749813079834 and perplexity is 130.94138928952864
At time: 769.1072535514832 and batch: 800, loss is 4.85430344581604 and perplexity is 128.29129824425297
At time: 770.8615148067474 and batch: 850, loss is 4.886522436141968 and perplexity is 132.492022509991
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.937790234883626 and perplexity of 139.46173114038194
Finished 24 epochs...
Completing Train Step...
At time: 775.2965967655182 and batch: 50, loss is 4.899644594192505 and perplexity is 134.24206079763542
At time: 776.9953079223633 and batch: 100, loss is 4.852775897979736 and perplexity is 128.09547675063033
At time: 778.7092478275299 and batch: 150, loss is 4.843287487030029 and perplexity is 126.88580224639101
At time: 780.4306292533875 and batch: 200, loss is 4.88665512084961 and perplexity is 132.50960334159052
At time: 782.1514723300934 and batch: 250, loss is 4.8878774738311765 and perplexity is 132.6716758850697
At time: 783.8752353191376 and batch: 300, loss is 4.86491213798523 and perplexity is 129.65954596238333
At time: 785.6220681667328 and batch: 350, loss is 4.819409790039063 and perplexity is 123.89194693567318
At time: 787.3630709648132 and batch: 400, loss is 4.842034511566162 and perplexity is 126.72691700990143
At time: 789.1040787696838 and batch: 450, loss is 4.868944568634033 and perplexity is 130.18344467213475
At time: 790.8455247879028 and batch: 500, loss is 4.872740402221679 and perplexity is 130.67853841625126
At time: 792.5867948532104 and batch: 550, loss is 4.859666271209717 and perplexity is 128.9811502007216
At time: 794.35475730896 and batch: 600, loss is 4.894394445419311 and perplexity is 133.53911690150963
At time: 796.0945460796356 and batch: 650, loss is 4.889968547821045 and perplexity is 132.9493924374398
At time: 797.8346462249756 and batch: 700, loss is 4.877953758239746 and perplexity is 131.36159111231632
At time: 799.5735170841217 and batch: 750, loss is 4.872343311309814 and perplexity is 130.62665745768064
At time: 801.3150458335876 and batch: 800, loss is 4.850054159164428 and perplexity is 127.74730834615508
At time: 803.0662882328033 and batch: 850, loss is 4.882685432434082 and perplexity is 131.98462419548014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.935612042744954 and perplexity of 139.15828729344202
Finished 25 epochs...
Completing Train Step...
At time: 807.4442687034607 and batch: 50, loss is 4.892925491333008 and perplexity is 133.3430980766306
At time: 809.178032875061 and batch: 100, loss is 4.845718746185303 and perplexity is 127.19466983120492
At time: 810.9050941467285 and batch: 150, loss is 4.836882619857788 and perplexity is 126.07571256467571
At time: 812.6466071605682 and batch: 200, loss is 4.880027875900269 and perplexity is 131.63433326023235
At time: 814.3888087272644 and batch: 250, loss is 4.882093172073365 and perplexity is 131.90647807805314
At time: 816.131246805191 and batch: 300, loss is 4.859675254821777 and perplexity is 128.98230892254284
At time: 817.8758380413055 and batch: 350, loss is 4.812648992538453 and perplexity is 123.0571636505722
At time: 819.627320766449 and batch: 400, loss is 4.836541833877564 and perplexity is 126.03275504946207
At time: 821.3669877052307 and batch: 450, loss is 4.863112869262696 and perplexity is 129.42646334876466
At time: 823.1061935424805 and batch: 500, loss is 4.867807064056397 and perplexity is 130.03544459922625
At time: 824.8438513278961 and batch: 550, loss is 4.855959777832031 and perplexity is 128.5039673055752
At time: 826.5839359760284 and batch: 600, loss is 4.890117177963257 and perplexity is 132.96915419310463
At time: 828.3285293579102 and batch: 650, loss is 4.8854992485046385 and perplexity is 132.3565276406725
At time: 830.0783803462982 and batch: 700, loss is 4.873643035888672 and perplexity is 130.79654651560864
At time: 831.8287975788116 and batch: 750, loss is 4.86955454826355 and perplexity is 130.26287814542246
At time: 833.5853190422058 and batch: 800, loss is 4.845697116851807 and perplexity is 127.19191872502466
At time: 835.336701631546 and batch: 850, loss is 4.878402118682861 and perplexity is 131.42050165910825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9344174067179365 and perplexity of 138.99214305069168
Finished 26 epochs...
Completing Train Step...
At time: 839.775294303894 and batch: 50, loss is 4.887402048110962 and perplexity is 132.60861534949805
At time: 841.4955279827118 and batch: 100, loss is 4.838979368209839 and perplexity is 126.34033893773584
At time: 843.2129788398743 and batch: 150, loss is 4.831731081008911 and perplexity is 125.42789868534082
At time: 844.9332110881805 and batch: 200, loss is 4.874739580154419 and perplexity is 130.94004938291383
At time: 846.6725783348083 and batch: 250, loss is 4.876726741790772 and perplexity is 131.2005071258188
At time: 848.4122982025146 and batch: 300, loss is 4.85446759223938 and perplexity is 128.31235853044342
At time: 850.152426481247 and batch: 350, loss is 4.808519659042358 and perplexity is 122.55006728833085
At time: 851.8923180103302 and batch: 400, loss is 4.834035873413086 and perplexity is 125.71731735035515
At time: 853.6444125175476 and batch: 450, loss is 4.859087715148926 and perplexity is 128.9065489571269
At time: 855.3950092792511 and batch: 500, loss is 4.8640642166137695 and perplexity is 129.5496514598809
At time: 857.1466634273529 and batch: 550, loss is 4.853091640472412 and perplexity is 128.13592832156394
At time: 858.8961942195892 and batch: 600, loss is 4.8896387004852295 and perplexity is 132.90554666614898
At time: 860.6464819908142 and batch: 650, loss is 4.880967979431152 and perplexity is 131.75814134880798
At time: 862.3971683979034 and batch: 700, loss is 4.868847007751465 and perplexity is 130.17074447990782
At time: 864.1475515365601 and batch: 750, loss is 4.865131063461304 and perplexity is 129.6879348476224
At time: 865.8984818458557 and batch: 800, loss is 4.842750854492188 and perplexity is 126.81772946298439
At time: 867.6485257148743 and batch: 850, loss is 4.873241491317749 and perplexity is 130.74403641573818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.932058652242024 and perplexity of 138.66468106419308
Finished 27 epochs...
Completing Train Step...
At time: 872.080525636673 and batch: 50, loss is 4.880686740875245 and perplexity is 131.72109108963085
At time: 873.7769110202789 and batch: 100, loss is 4.834941730499268 and perplexity is 125.83125086908447
At time: 875.49453997612 and batch: 150, loss is 4.827522077560425 and perplexity is 124.90108169225267
At time: 877.2180731296539 and batch: 200, loss is 4.869804182052612 and perplexity is 130.2954002204036
At time: 878.9426033496857 and batch: 250, loss is 4.870872879028321 and perplexity is 130.4347209531307
At time: 880.6695516109467 and batch: 300, loss is 4.850024156570434 and perplexity is 127.74347565302459
At time: 882.4501178264618 and batch: 350, loss is 4.804724311828613 and perplexity is 122.08582876236821
At time: 884.1763684749603 and batch: 400, loss is 4.828330764770508 and perplexity is 125.00212845164683
At time: 885.9022400379181 and batch: 450, loss is 4.854621095657349 and perplexity is 128.3320564278541
At time: 887.6279113292694 and batch: 500, loss is 4.8590853309631346 and perplexity is 128.90624162033086
At time: 889.3546714782715 and batch: 550, loss is 4.848063554763794 and perplexity is 127.49326692386529
At time: 891.0924868583679 and batch: 600, loss is 4.884289083480835 and perplexity is 132.1964512791925
At time: 892.8464829921722 and batch: 650, loss is 4.8757986640930175 and perplexity is 131.07879934709473
At time: 894.5995953083038 and batch: 700, loss is 4.864769086837769 and perplexity is 129.64099934214343
At time: 896.3528625965118 and batch: 750, loss is 4.861567497253418 and perplexity is 129.22660578196502
At time: 898.1037905216217 and batch: 800, loss is 4.839123783111572 and perplexity is 126.35858568288769
At time: 899.8553562164307 and batch: 850, loss is 4.868903722763061 and perplexity is 130.1781273245476
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.931180318196614 and perplexity of 138.54294062613076
Finished 28 epochs...
Completing Train Step...
At time: 904.2915852069855 and batch: 50, loss is 4.875579080581665 and perplexity is 131.0500197639468
At time: 906.0001013278961 and batch: 100, loss is 4.830394020080567 and perplexity is 125.26030600846192
At time: 907.7196915149689 and batch: 150, loss is 4.823486595153809 and perplexity is 124.3980612222901
At time: 909.4390077590942 and batch: 200, loss is 4.864896669387817 and perplexity is 129.65754032657836
At time: 911.1584696769714 and batch: 250, loss is 4.866743650436401 and perplexity is 129.89723663549714
At time: 912.880145072937 and batch: 300, loss is 4.845732221603393 and perplexity is 127.1963838441083
At time: 914.6329863071442 and batch: 350, loss is 4.800902652740478 and perplexity is 121.62014874725132
At time: 916.393720626831 and batch: 400, loss is 4.825160245895386 and perplexity is 124.60643445307204
At time: 918.1563816070557 and batch: 450, loss is 4.850341119766235 and perplexity is 127.7839720509051
At time: 919.9150233268738 and batch: 500, loss is 4.854543142318725 and perplexity is 128.3220529055122
At time: 921.6655197143555 and batch: 550, loss is 4.843560714721679 and perplexity is 126.92047569789985
At time: 923.4155826568604 and batch: 600, loss is 4.87914213180542 and perplexity is 131.51779054799667
At time: 925.2265861034393 and batch: 650, loss is 4.8721202373504635 and perplexity is 130.59752130188443
At time: 926.9772880077362 and batch: 700, loss is 4.861443147659302 and perplexity is 129.21053750504961
At time: 928.7340619564056 and batch: 750, loss is 4.85711332321167 and perplexity is 128.65228799420123
At time: 930.4834804534912 and batch: 800, loss is 4.83408480644226 and perplexity is 125.72346923002661
At time: 932.2323133945465 and batch: 850, loss is 4.862768058776855 and perplexity is 129.38184344020505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.929396629333496 and perplexity of 138.2960433852613
Finished 29 epochs...
Completing Train Step...
At time: 936.6416008472443 and batch: 50, loss is 4.870311822891235 and perplexity is 130.36156027799296
At time: 938.3358342647552 and batch: 100, loss is 4.824769363403321 and perplexity is 124.55773749745933
At time: 940.0262205600739 and batch: 150, loss is 4.818566923141479 and perplexity is 123.7875665103061
At time: 941.731537103653 and batch: 200, loss is 4.859844779968261 and perplexity is 129.00417652085835
At time: 943.4476039409637 and batch: 250, loss is 4.862362241744995 and perplexity is 129.32934873686625
At time: 945.1815528869629 and batch: 300, loss is 4.840666265487671 and perplexity is 126.5536419711418
At time: 946.9213337898254 and batch: 350, loss is 4.796935300827027 and perplexity is 121.13859469623098
At time: 948.6670477390289 and batch: 400, loss is 4.820305700302124 and perplexity is 124.00299273855309
At time: 950.4077038764954 and batch: 450, loss is 4.84544153213501 and perplexity is 127.15941456845823
At time: 952.1486797332764 and batch: 500, loss is 4.851087741851806 and perplexity is 127.87941401172458
At time: 953.9017264842987 and batch: 550, loss is 4.838795232772827 and perplexity is 126.31707734592074
At time: 955.6498420238495 and batch: 600, loss is 4.87545075416565 and perplexity is 131.0332036635898
At time: 957.3970222473145 and batch: 650, loss is 4.868589038848877 and perplexity is 130.1371688067313
At time: 959.1515870094299 and batch: 700, loss is 4.860479927062988 and perplexity is 129.08613917519867
At time: 960.9053246974945 and batch: 750, loss is 4.855107107162476 and perplexity is 128.39444244268714
At time: 962.660523891449 and batch: 800, loss is 4.831126918792725 and perplexity is 125.3521427748332
At time: 964.4444711208344 and batch: 850, loss is 4.860034837722778 and perplexity is 129.02869709505998
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9285173416137695 and perplexity of 138.17449481853814
Finished 30 epochs...
Completing Train Step...
At time: 969.0951359272003 and batch: 50, loss is 4.865076904296875 and perplexity is 129.68091124763242
At time: 970.8467559814453 and batch: 100, loss is 4.821805200576782 and perplexity is 124.18907474036226
At time: 972.5669860839844 and batch: 150, loss is 4.814732799530029 and perplexity is 123.3138583868078
At time: 974.2899684906006 and batch: 200, loss is 4.8560240650177 and perplexity is 128.5122287295296
At time: 976.0167217254639 and batch: 250, loss is 4.8577016258239745 and perplexity is 128.72799673894977
At time: 977.7437119483948 and batch: 300, loss is 4.836922969818115 and perplexity is 126.08079981731039
At time: 979.4668831825256 and batch: 350, loss is 4.794262266159057 and perplexity is 120.81521942218409
At time: 981.191070318222 and batch: 400, loss is 4.816529207229614 and perplexity is 123.53557944255925
At time: 982.936252117157 and batch: 450, loss is 4.841449127197266 and perplexity is 126.65275476239349
At time: 984.7022614479065 and batch: 500, loss is 4.847193927764892 and perplexity is 127.38244353126149
At time: 986.4795382022858 and batch: 550, loss is 4.833318901062012 and perplexity is 125.62721381449552
At time: 988.2428441047668 and batch: 600, loss is 4.869792881011963 and perplexity is 130.29392775510954
At time: 990.0132837295532 and batch: 650, loss is 4.863019523620605 and perplexity is 129.41438251629515
At time: 991.7684099674225 and batch: 700, loss is 4.855626630783081 and perplexity is 128.46116371842922
At time: 993.5238976478577 and batch: 750, loss is 4.850520133972168 and perplexity is 127.80684924480298
At time: 995.2792658805847 and batch: 800, loss is 4.825997304916382 and perplexity is 124.71078105914638
At time: 997.0341892242432 and batch: 850, loss is 4.856557207107544 and perplexity is 128.58076227516048
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9268754323323565 and perplexity of 137.9478109811151
Finished 31 epochs...
Completing Train Step...
At time: 1001.4688928127289 and batch: 50, loss is 4.859629898071289 and perplexity is 128.97645883681113
At time: 1003.1997148990631 and batch: 100, loss is 4.817032995223999 and perplexity is 123.59783086380456
At time: 1004.9190189838409 and batch: 150, loss is 4.8105708026885985 and perplexity is 122.80169305226933
At time: 1006.648154258728 and batch: 200, loss is 4.852178831100463 and perplexity is 128.01901801183953
At time: 1008.3790028095245 and batch: 250, loss is 4.853743515014648 and perplexity is 128.21948410216683
At time: 1010.1215949058533 and batch: 300, loss is 4.833464469909668 and perplexity is 125.64550255434781
At time: 1011.8636481761932 and batch: 350, loss is 4.790650968551636 and perplexity is 120.3797065659231
At time: 1013.7491178512573 and batch: 400, loss is 4.813063850402832 and perplexity is 123.10822547364799
At time: 1015.498407125473 and batch: 450, loss is 4.838076477050781 and perplexity is 126.22631884428966
At time: 1017.2606012821198 and batch: 500, loss is 4.843617153167725 and perplexity is 126.92763909446317
At time: 1019.0177526473999 and batch: 550, loss is 4.829591722488403 and perplexity is 125.1598502696448
At time: 1020.7746911048889 and batch: 600, loss is 4.86565857887268 and perplexity is 129.75636527939096
At time: 1022.5310626029968 and batch: 650, loss is 4.8575872135162355 and perplexity is 128.7132695142762
At time: 1024.2884044647217 and batch: 700, loss is 4.85097716331482 and perplexity is 127.86527407501363
At time: 1026.0449323654175 and batch: 750, loss is 4.847618703842163 and perplexity is 127.43656403967272
At time: 1027.79767370224 and batch: 800, loss is 4.823196640014649 and perplexity is 124.36199659395197
At time: 1029.5491094589233 and batch: 850, loss is 4.852147130966187 and perplexity is 128.01495985610097
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.926265716552734 and perplexity of 137.86372766006426
Finished 32 epochs...
Completing Train Step...
At time: 1034.0056354999542 and batch: 50, loss is 4.85457103729248 and perplexity is 128.32563249573622
At time: 1035.7379930019379 and batch: 100, loss is 4.81315336227417 and perplexity is 123.11924561449652
At time: 1037.4477107524872 and batch: 150, loss is 4.806586666107178 and perplexity is 122.31340767842275
At time: 1039.1631758213043 and batch: 200, loss is 4.848492584228516 and perplexity is 127.54797702721325
At time: 1040.9016926288605 and batch: 250, loss is 4.84951602935791 and perplexity is 127.6785822052825
At time: 1042.6403074264526 and batch: 300, loss is 4.829594449996948 and perplexity is 125.16019164467149
At time: 1044.3788032531738 and batch: 350, loss is 4.786835565567016 and perplexity is 119.92128456265903
At time: 1046.1264159679413 and batch: 400, loss is 4.809549760818482 and perplexity is 122.67637137217064
At time: 1047.8679594993591 and batch: 450, loss is 4.834941511154175 and perplexity is 125.83122326862004
At time: 1049.616647720337 and batch: 500, loss is 4.839795627593994 and perplexity is 126.44350752542681
At time: 1051.3604996204376 and batch: 550, loss is 4.82521987915039 and perplexity is 124.61386536191539
At time: 1053.1008577346802 and batch: 600, loss is 4.8594786739349365 and perplexity is 128.9569559579039
At time: 1054.8422374725342 and batch: 650, loss is 4.853980026245117 and perplexity is 128.24981303654732
At time: 1056.5818729400635 and batch: 700, loss is 4.847594480514527 and perplexity is 127.43347713941681
At time: 1058.3830125331879 and batch: 750, loss is 4.8431836700439455 and perplexity is 126.87263002858681
At time: 1060.1074900627136 and batch: 800, loss is 4.8188270092010494 and perplexity is 123.81976611785656
At time: 1061.8422226905823 and batch: 850, loss is 4.847879819869995 and perplexity is 127.4698441138666
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.924242655436198 and perplexity of 137.58510284579745
Finished 33 epochs...
Completing Train Step...
At time: 1066.3121418952942 and batch: 50, loss is 4.850093660354614 and perplexity is 127.75235461654393
At time: 1068.0272057056427 and batch: 100, loss is 4.809592056274414 and perplexity is 122.68156013495971
At time: 1069.7511096000671 and batch: 150, loss is 4.80304482460022 and perplexity is 121.88095925820781
At time: 1071.4842731952667 and batch: 200, loss is 4.8447920513153075 and perplexity is 127.07685378137745
At time: 1073.235454082489 and batch: 250, loss is 4.84607268333435 and perplexity is 127.23969671789295
At time: 1074.9786503314972 and batch: 300, loss is 4.825719079971313 and perplexity is 124.67608823536503
At time: 1076.7227864265442 and batch: 350, loss is 4.783785800933838 and perplexity is 119.55611000150796
At time: 1078.4863541126251 and batch: 400, loss is 4.8061952304840085 and perplexity is 122.2655392227868
At time: 1080.2647626399994 and batch: 450, loss is 4.832534513473511 and perplexity is 125.52871202404276
At time: 1082.0142142772675 and batch: 500, loss is 4.836477346420288 and perplexity is 126.0246277796118
At time: 1083.7695207595825 and batch: 550, loss is 4.820168504714966 and perplexity is 123.98598124213272
At time: 1085.5338804721832 and batch: 600, loss is 4.854931192398071 and perplexity is 128.371857951125
At time: 1087.290024280548 and batch: 650, loss is 4.848452425003051 and perplexity is 127.54285490209729
At time: 1089.0433588027954 and batch: 700, loss is 4.843499660491943 and perplexity is 126.91272690256918
At time: 1090.7966322898865 and batch: 750, loss is 4.8384300708770756 and perplexity is 126.2709595832286
At time: 1092.5476443767548 and batch: 800, loss is 4.815373029708862 and perplexity is 123.39283291865709
At time: 1094.29851436615 and batch: 850, loss is 4.843720693588256 and perplexity is 126.940781915987
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.923434575398763 and perplexity of 137.47396797972016
Finished 34 epochs...
Completing Train Step...
At time: 1098.896418094635 and batch: 50, loss is 4.844443454742431 and perplexity is 127.03256294592262
At time: 1100.5867183208466 and batch: 100, loss is 4.804871788024903 and perplexity is 122.10383484371651
At time: 1102.3267540931702 and batch: 150, loss is 4.79973614692688 and perplexity is 121.47836085096758
At time: 1104.0735502243042 and batch: 200, loss is 4.840337886810302 and perplexity is 126.51209127612971
At time: 1105.8213782310486 and batch: 250, loss is 4.84140043258667 and perplexity is 126.64658760597426
At time: 1107.5676288604736 and batch: 300, loss is 4.821433877944946 and perplexity is 124.14296908685041
At time: 1109.3131611347198 and batch: 350, loss is 4.78048113822937 and perplexity is 119.16166948908689
At time: 1111.057808637619 and batch: 400, loss is 4.80145411491394 and perplexity is 121.68723615519997
At time: 1112.8020436763763 and batch: 450, loss is 4.827680311203003 and perplexity is 124.92084680908275
At time: 1114.5472235679626 and batch: 500, loss is 4.829846649169922 and perplexity is 125.19176092219904
At time: 1116.2934076786041 and batch: 550, loss is 4.814500970840454 and perplexity is 123.28527401007885
At time: 1118.03995347023 and batch: 600, loss is 4.850510959625244 and perplexity is 127.80567670580739
At time: 1119.7860028743744 and batch: 650, loss is 4.8435743713378905 and perplexity is 126.92220901396139
At time: 1121.5334072113037 and batch: 700, loss is 4.8393909358978275 and perplexity is 126.39234724066652
At time: 1123.28031873703 and batch: 750, loss is 4.83335431098938 and perplexity is 125.63166234377273
At time: 1125.0318365097046 and batch: 800, loss is 4.809496612548828 and perplexity is 122.66985150856607
At time: 1126.7798273563385 and batch: 850, loss is 4.838393678665161 and perplexity is 126.26636438732402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.921878814697266 and perplexity of 137.26025766701667
Finished 35 epochs...
Completing Train Step...
At time: 1131.2466292381287 and batch: 50, loss is 4.8394835090637205 and perplexity is 126.40404832198995
At time: 1132.9552698135376 and batch: 100, loss is 4.800682649612427 and perplexity is 121.59339487716832
At time: 1134.6984894275665 and batch: 150, loss is 4.795513343811035 and perplexity is 120.96646323237029
At time: 1136.441519021988 and batch: 200, loss is 4.835901250839234 and perplexity is 125.9520464573408
At time: 1138.1847591400146 and batch: 250, loss is 4.836155261993408 and perplexity is 125.9840437456941
At time: 1139.9350171089172 and batch: 300, loss is 4.816455965042114 and perplexity is 123.52653175782697
At time: 1141.6869564056396 and batch: 350, loss is 4.773741960525513 and perplexity is 118.36131770844611
At time: 1143.4395718574524 and batch: 400, loss is 4.796095190048217 and perplexity is 121.03686759409749
At time: 1145.1927223205566 and batch: 450, loss is 4.823031234741211 and perplexity is 124.34142816501026
At time: 1146.993414402008 and batch: 500, loss is 4.826643056869507 and perplexity is 124.79133929711146
At time: 1148.7481014728546 and batch: 550, loss is 4.8114973831176755 and perplexity is 122.91553142977075
At time: 1150.5025532245636 and batch: 600, loss is 4.846118774414062 and perplexity is 127.2455614680522
At time: 1152.2567732334137 and batch: 650, loss is 4.83901683807373 and perplexity is 126.34507298173129
At time: 1154.0108563899994 and batch: 700, loss is 4.832579288482666 and perplexity is 125.53433269910487
At time: 1155.7641353607178 and batch: 750, loss is 4.825318222045898 and perplexity is 124.62612085286587
At time: 1157.5183458328247 and batch: 800, loss is 4.802566413879394 and perplexity is 121.82266404627323
At time: 1159.2740886211395 and batch: 850, loss is 4.833125057220459 and perplexity is 125.60286411286228
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.918539683024089 and perplexity of 136.8026919544012
Finished 36 epochs...
Completing Train Step...
At time: 1163.6981446743011 and batch: 50, loss is 4.833973779678344 and perplexity is 125.70951133495402
At time: 1165.4222404956818 and batch: 100, loss is 4.794021520614624 and perplexity is 120.78613719725698
At time: 1167.1193263530731 and batch: 150, loss is 4.789194374084473 and perplexity is 120.20448979225135
At time: 1168.8310148715973 and batch: 200, loss is 4.829258661270142 and perplexity is 125.11817131864302
At time: 1170.5508530139923 and batch: 250, loss is 4.827575778961181 and perplexity is 124.90778923539615
At time: 1172.277203321457 and batch: 300, loss is 4.808255004882812 and perplexity is 122.51763819470693
At time: 1174.0263307094574 and batch: 350, loss is 4.763915939331055 and perplexity is 117.20399215644069
At time: 1175.7779698371887 and batch: 400, loss is 4.788601579666138 and perplexity is 120.13325435771375
At time: 1177.5277936458588 and batch: 450, loss is 4.814958438873291 and perplexity is 123.34168598421167
At time: 1179.2841429710388 and batch: 500, loss is 4.818220472335815 and perplexity is 123.7446876362965
At time: 1181.0375261306763 and batch: 550, loss is 4.80351336479187 and perplexity is 121.9380787665649
At time: 1182.7953443527222 and batch: 600, loss is 4.837930355072022 and perplexity is 126.20787575231196
At time: 1184.5516965389252 and batch: 650, loss is 4.831390533447266 and perplexity is 125.38519179255806
At time: 1186.302960395813 and batch: 700, loss is 4.822643604278564 and perplexity is 124.29323898008596
At time: 1188.0541942119598 and batch: 750, loss is 4.815413866043091 and perplexity is 123.39787193251033
At time: 1189.849523305893 and batch: 800, loss is 4.794906663894653 and perplexity is 120.89309756555998
At time: 1191.5901327133179 and batch: 850, loss is 4.823424587249756 and perplexity is 124.39034779839452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.912826855977376 and perplexity of 136.0233899639513
Finished 37 epochs...
Completing Train Step...
At time: 1195.9749522209167 and batch: 50, loss is 4.826046772003174 and perplexity is 124.71695029076264
At time: 1197.6969256401062 and batch: 100, loss is 4.783998298645019 and perplexity is 119.58151810072684
At time: 1199.4120354652405 and batch: 150, loss is 4.779256858825684 and perplexity is 119.01587157832302
At time: 1201.138151884079 and batch: 200, loss is 4.820773792266846 and perplexity is 124.0610511303287
At time: 1202.8758130073547 and batch: 250, loss is 4.816478004455567 and perplexity is 123.52925424013358
At time: 1204.6037650108337 and batch: 300, loss is 4.800580682754517 and perplexity is 121.58099701284645
At time: 1206.348709344864 and batch: 350, loss is 4.7569725227355955 and perplexity is 116.39301474741679
At time: 1208.107325553894 and batch: 400, loss is 4.782016038894653 and perplexity is 119.34471125438964
At time: 1209.8638937473297 and batch: 450, loss is 4.80820818901062 and perplexity is 122.51190255887636
At time: 1211.620194196701 and batch: 500, loss is 4.8118961143493655 and perplexity is 122.96455146327128
At time: 1213.3766973018646 and batch: 550, loss is 4.7981595611572265 and perplexity is 121.28699069130387
At time: 1215.1318173408508 and batch: 600, loss is 4.8316372108459475 and perplexity is 125.41612530064445
At time: 1216.8885843753815 and batch: 650, loss is 4.823611450195313 and perplexity is 124.41359391703445
At time: 1218.6456756591797 and batch: 700, loss is 4.815401840209961 and perplexity is 123.39638797921675
At time: 1220.4028701782227 and batch: 750, loss is 4.8091856956481935 and perplexity is 122.63171730712482
At time: 1222.1620054244995 and batch: 800, loss is 4.788191289901733 and perplexity is 120.08397502318071
At time: 1223.9191601276398 and batch: 850, loss is 4.817612037658692 and perplexity is 123.66941997727766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.91004753112793 and perplexity of 135.645861656045
Finished 38 epochs...
Completing Train Step...
At time: 1228.3188016414642 and batch: 50, loss is 4.820175743103027 and perplexity is 123.9868787040272
At time: 1230.0612511634827 and batch: 100, loss is 4.776416540145874 and perplexity is 118.67830819581425
At time: 1231.754956960678 and batch: 150, loss is 4.772433824539185 and perplexity is 118.20658623633092
At time: 1233.4916512966156 and batch: 200, loss is 4.815166797637939 and perplexity is 123.36738798305831
At time: 1235.2178206443787 and batch: 250, loss is 4.809221858978272 and perplexity is 122.63615215858485
At time: 1236.9711153507233 and batch: 300, loss is 4.794496603012085 and perplexity is 120.84353419796952
At time: 1238.7202882766724 and batch: 350, loss is 4.7506767272949215 and perplexity is 115.66253003984214
At time: 1240.4674854278564 and batch: 400, loss is 4.7766687870025635 and perplexity is 118.70824820199135
At time: 1242.2132561206818 and batch: 450, loss is 4.803825874328613 and perplexity is 121.976191534063
At time: 1243.9579911231995 and batch: 500, loss is 4.806229171752929 and perplexity is 122.26968914075964
At time: 1245.7121677398682 and batch: 550, loss is 4.793517026901245 and perplexity is 120.72521671866897
At time: 1247.4667212963104 and batch: 600, loss is 4.82614520072937 and perplexity is 124.72922662547693
At time: 1249.221384525299 and batch: 650, loss is 4.8185178565979 and perplexity is 123.78149283128779
At time: 1250.9759440422058 and batch: 700, loss is 4.810041160583496 and perplexity is 122.73666932622258
At time: 1252.7320764064789 and batch: 750, loss is 4.802872409820557 and perplexity is 121.85994699093695
At time: 1254.485557794571 and batch: 800, loss is 4.782822608947754 and perplexity is 119.4410099550868
At time: 1256.2377517223358 and batch: 850, loss is 4.811858510971069 and perplexity is 122.95992766766126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.907813390096028 and perplexity of 135.34314794918177
Finished 39 epochs...
Completing Train Step...
At time: 1260.6588218212128 and batch: 50, loss is 4.815318994522094 and perplexity is 123.38616554402257
At time: 1262.3594262599945 and batch: 100, loss is 4.771434764862061 and perplexity is 118.08854977500408
At time: 1264.0794026851654 and batch: 150, loss is 4.7669867897033695 and perplexity is 117.56446126947381
At time: 1265.821040391922 and batch: 200, loss is 4.809584045410157 and perplexity is 122.68057735357107
At time: 1267.5645365715027 and batch: 250, loss is 4.802922840118408 and perplexity is 121.86609257932048
At time: 1269.3072488307953 and batch: 300, loss is 4.789231519699097 and perplexity is 120.20895494483518
At time: 1271.051321029663 and batch: 350, loss is 4.746494455337524 and perplexity is 115.17980802493138
At time: 1272.794160604477 and batch: 400, loss is 4.7716145420074465 and perplexity is 118.10978130580017
At time: 1274.5380370616913 and batch: 450, loss is 4.798995780944824 and perplexity is 121.38845569050557
At time: 1276.2824981212616 and batch: 500, loss is 4.802234964370728 and perplexity is 121.78229267503579
At time: 1278.0547688007355 and batch: 550, loss is 4.788353404998779 and perplexity is 120.10344402651232
At time: 1279.8065257072449 and batch: 600, loss is 4.821827030181884 and perplexity is 124.19178576841207
At time: 1281.5603885650635 and batch: 650, loss is 4.813270301818847 and perplexity is 123.13364396487161
At time: 1283.315006017685 and batch: 700, loss is 4.804971780776977 and perplexity is 122.11604495265246
At time: 1285.0751922130585 and batch: 750, loss is 4.7975140476226805 and perplexity is 121.20872356121191
At time: 1286.828572511673 and batch: 800, loss is 4.778612995147705 and perplexity is 118.93926624585283
At time: 1288.5823860168457 and batch: 850, loss is 4.8063028526306155 and perplexity is 122.27869841067134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.904540379842122 and perplexity of 134.9008925861257
Finished 40 epochs...
Completing Train Step...
At time: 1293.0277614593506 and batch: 50, loss is 4.809374885559082 and perplexity is 122.6549201856003
At time: 1294.7188277244568 and batch: 100, loss is 4.765377769470215 and perplexity is 117.37544977502908
At time: 1296.4347369670868 and batch: 150, loss is 4.762186269760132 and perplexity is 117.0014431994989
At time: 1298.1575272083282 and batch: 200, loss is 4.80446792602539 and perplexity is 122.0545317013314
At time: 1299.8973989486694 and batch: 250, loss is 4.7967556285858155 and perplexity is 121.11683140861261
At time: 1301.641270160675 and batch: 300, loss is 4.783312377929687 and perplexity is 119.49952278464842
At time: 1303.3869524002075 and batch: 350, loss is 4.7407645416259765 and perplexity is 114.52172484406628
At time: 1305.1394460201263 and batch: 400, loss is 4.766555995941162 and perplexity is 117.51382614033359
At time: 1306.8935763835907 and batch: 450, loss is 4.794452590942383 and perplexity is 120.8382157409583
At time: 1308.6462354660034 and batch: 500, loss is 4.795601654052734 and perplexity is 120.97714628168019
At time: 1310.4000277519226 and batch: 550, loss is 4.783173131942749 and perplexity is 119.48288411412055
At time: 1312.1538121700287 and batch: 600, loss is 4.816981964111328 and perplexity is 123.59152368990421
At time: 1313.908338546753 and batch: 650, loss is 4.809269733428955 and perplexity is 122.64202343754441
At time: 1315.6616790294647 and batch: 700, loss is 4.801313695907592 and perplexity is 121.67015015404156
At time: 1317.414341211319 and batch: 750, loss is 4.7931272983551025 and perplexity is 120.67817582265985
At time: 1319.1675720214844 and batch: 800, loss is 4.77294264793396 and perplexity is 118.26674781733645
At time: 1320.9186570644379 and batch: 850, loss is 4.80112922668457 and perplexity is 121.64770782600071
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.901421229044597 and perplexity of 134.48077190991629
Finished 41 epochs...
Completing Train Step...
At time: 1325.3689770698547 and batch: 50, loss is 4.80415225982666 and perplexity is 122.0160092917026
At time: 1327.079231262207 and batch: 100, loss is 4.7596753883361815 and perplexity is 116.70803495999291
At time: 1328.7967524528503 and batch: 150, loss is 4.757674789428711 and perplexity is 116.47478239297166
At time: 1330.521162033081 and batch: 200, loss is 4.799261436462403 and perplexity is 121.4207074872743
At time: 1332.25594496727 and batch: 250, loss is 4.792043561935425 and perplexity is 120.5474633301982
At time: 1334.0110092163086 and batch: 300, loss is 4.77888298034668 and perplexity is 118.97138242256717
At time: 1335.7655506134033 and batch: 350, loss is 4.735177087783813 and perplexity is 113.88362433126099
At time: 1337.5216863155365 and batch: 400, loss is 4.762007417678833 and perplexity is 116.98051911908114
At time: 1339.278296470642 and batch: 450, loss is 4.789795026779175 and perplexity is 120.2767126312104
At time: 1341.0339710712433 and batch: 500, loss is 4.7914635181427006 and perplexity is 120.4775607975898
At time: 1342.7897155284882 and batch: 550, loss is 4.779006748199463 and perplexity is 118.98610816638035
At time: 1344.5438232421875 and batch: 600, loss is 4.812272281646728 and perplexity is 123.01081540721249
At time: 1346.2990357875824 and batch: 650, loss is 4.805262622833252 and perplexity is 122.15156659961458
At time: 1348.0592041015625 and batch: 700, loss is 4.796988554000855 and perplexity is 121.14504588264317
At time: 1349.8064966201782 and batch: 750, loss is 4.789017534255981 and perplexity is 120.18323473031869
At time: 1351.552487373352 and batch: 800, loss is 4.76831732749939 and perplexity is 117.72098933882563
At time: 1353.2984681129456 and batch: 850, loss is 4.795502996444702 and perplexity is 120.96521155453695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.898044268290202 and perplexity of 134.0274015587451
Finished 42 epochs...
Completing Train Step...
At time: 1357.6906926631927 and batch: 50, loss is 4.798983488082886 and perplexity is 121.38696348815061
At time: 1359.4167342185974 and batch: 100, loss is 4.755006237030029 and perplexity is 116.16437768285165
At time: 1361.1400918960571 and batch: 150, loss is 4.752131567001343 and perplexity is 115.83092294372958
At time: 1362.8637917041779 and batch: 200, loss is 4.794894475936889 and perplexity is 120.89162413457197
At time: 1364.5871000289917 and batch: 250, loss is 4.786926145553589 and perplexity is 119.93214752298047
At time: 1366.3487753868103 and batch: 300, loss is 4.774115571975708 and perplexity is 118.40554711379383
At time: 1368.0932812690735 and batch: 350, loss is 4.73087519645691 and perplexity is 113.39476162650728
At time: 1369.8360023498535 and batch: 400, loss is 4.757575569152832 and perplexity is 116.46322630623824
At time: 1371.5789020061493 and batch: 450, loss is 4.785096349716187 and perplexity is 119.71289683184695
At time: 1373.3213107585907 and batch: 500, loss is 4.788501482009888 and perplexity is 120.12122990233482
At time: 1375.0625553131104 and batch: 550, loss is 4.774147739410401 and perplexity is 118.40935597775824
At time: 1376.8164677619934 and batch: 600, loss is 4.809148063659668 and perplexity is 122.62710251857865
At time: 1378.5701220035553 and batch: 650, loss is 4.802138233184815 and perplexity is 121.7705130991775
At time: 1380.3230473995209 and batch: 700, loss is 4.792964067459106 and perplexity is 120.65847902349995
At time: 1382.075573682785 and batch: 750, loss is 4.784207887649536 and perplexity is 119.60658369870931
At time: 1383.8290491104126 and batch: 800, loss is 4.762850933074951 and perplexity is 117.07923561659344
At time: 1385.581591129303 and batch: 850, loss is 4.79046817779541 and perplexity is 120.35770427929428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.896965344746907 and perplexity of 133.8828742207506
Finished 43 epochs...
Completing Train Step...
At time: 1390.0139684677124 and batch: 50, loss is 4.794167966842651 and perplexity is 120.8038271667305
At time: 1391.779197216034 and batch: 100, loss is 4.750583400726319 and perplexity is 115.65173615648347
At time: 1393.4931750297546 and batch: 150, loss is 4.747821865081787 and perplexity is 115.33280034372875
At time: 1395.2068095207214 and batch: 200, loss is 4.791296520233154 and perplexity is 120.45744297665316
At time: 1396.9241490364075 and batch: 250, loss is 4.782428855895996 and perplexity is 119.39398895085144
At time: 1398.6468596458435 and batch: 300, loss is 4.770194387435913 and perplexity is 117.9421662077808
At time: 1400.3820271492004 and batch: 350, loss is 4.725413646697998 and perplexity is 112.7771386181117
At time: 1402.1256136894226 and batch: 400, loss is 4.753249168395996 and perplexity is 115.96044811002692
At time: 1403.8673286437988 and batch: 450, loss is 4.7812332344055175 and perplexity is 119.2513242353228
At time: 1405.6138167381287 and batch: 500, loss is 4.784385499954223 and perplexity is 119.62782918636992
At time: 1407.3692688941956 and batch: 550, loss is 4.76985803604126 and perplexity is 117.90250286647245
At time: 1409.150945186615 and batch: 600, loss is 4.8044948387146 and perplexity is 122.05781656121171
At time: 1410.90380525589 and batch: 650, loss is 4.796970977783203 and perplexity is 121.14291662966144
At time: 1412.6615335941315 and batch: 700, loss is 4.788386487960816 and perplexity is 120.10741746991778
At time: 1414.4143462181091 and batch: 750, loss is 4.779389591217041 and perplexity is 119.03166988801757
At time: 1416.16734790802 and batch: 800, loss is 4.758043575286865 and perplexity is 116.517744566984
At time: 1417.921842098236 and batch: 850, loss is 4.785946931838989 and perplexity is 119.81476579958063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.894588152567546 and perplexity of 133.5649868885422
Finished 44 epochs...
Completing Train Step...
At time: 1422.3541095256805 and batch: 50, loss is 4.790104780197144 and perplexity is 120.31397452475144
At time: 1424.096159696579 and batch: 100, loss is 4.746060457229614 and perplexity is 115.12983105191941
At time: 1425.8047256469727 and batch: 150, loss is 4.744102210998535 and perplexity is 114.9045990959669
At time: 1427.5280508995056 and batch: 200, loss is 4.787664480209351 and perplexity is 120.0207302816728
At time: 1429.2548305988312 and batch: 250, loss is 4.7784056472778325 and perplexity is 118.91460699896201
At time: 1430.9802422523499 and batch: 300, loss is 4.7658376884460445 and perplexity is 117.4294453875288
At time: 1432.705504655838 and batch: 350, loss is 4.721038551330566 and perplexity is 112.28480566852556
At time: 1434.4500994682312 and batch: 400, loss is 4.748195276260376 and perplexity is 115.37587494240294
At time: 1436.197387933731 and batch: 450, loss is 4.777331123352051 and perplexity is 118.78689903353651
At time: 1437.9443407058716 and batch: 500, loss is 4.780521421432495 and perplexity is 119.16646979950896
At time: 1439.694759130478 and batch: 550, loss is 4.76659441947937 and perplexity is 117.51834152407022
At time: 1441.4451222419739 and batch: 600, loss is 4.801368465423584 and perplexity is 121.67681415176645
At time: 1443.204297542572 and batch: 650, loss is 4.794125986099243 and perplexity is 120.79875583870934
At time: 1444.9630625247955 and batch: 700, loss is 4.784442243576049 and perplexity is 119.63461749526395
At time: 1446.7204995155334 and batch: 750, loss is 4.775430688858032 and perplexity is 118.56136668578095
At time: 1448.4790155887604 and batch: 800, loss is 4.754022426605225 and perplexity is 116.05015015542573
At time: 1450.2371575832367 and batch: 850, loss is 4.781444358825683 and perplexity is 119.27650375991873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.892020225524902 and perplexity of 133.22244175045813
Finished 45 epochs...
Completing Train Step...
At time: 1454.6770691871643 and batch: 50, loss is 4.785193948745728 and perplexity is 119.72458126458663
At time: 1456.3813326358795 and batch: 100, loss is 4.742940988540649 and perplexity is 114.7712467358448
At time: 1458.1053664684296 and batch: 150, loss is 4.7385640144348145 and perplexity is 114.26999374671064
At time: 1459.845784664154 and batch: 200, loss is 4.785693960189819 and perplexity is 119.78445989411048
At time: 1461.590209722519 and batch: 250, loss is 4.777372694015503 and perplexity is 118.79183718637917
At time: 1463.3343181610107 and batch: 300, loss is 4.762089605331421 and perplexity is 116.99013386844744
At time: 1465.0794558525085 and batch: 350, loss is 4.716414365768433 and perplexity is 111.76677854117258
At time: 1466.8233239650726 and batch: 400, loss is 4.744457235336304 and perplexity is 114.94540026744345
At time: 1468.567268371582 and batch: 450, loss is 4.773678102493286 and perplexity is 118.35375962893012
At time: 1470.3132810592651 and batch: 500, loss is 4.776953477859497 and perplexity is 118.74204816593907
At time: 1472.0577731132507 and batch: 550, loss is 4.762090463638305 and perplexity is 116.99023428192783
At time: 1473.8022677898407 and batch: 600, loss is 4.798209753036499 and perplexity is 121.2930784660751
At time: 1475.556767463684 and batch: 650, loss is 4.790064191818237 and perplexity is 120.30909127466794
At time: 1477.3115260601044 and batch: 700, loss is 4.780602560043335 and perplexity is 119.17613919360248
At time: 1479.0715296268463 and batch: 750, loss is 4.771456909179688 and perplexity is 118.09116479431219
At time: 1480.827027797699 and batch: 800, loss is 4.750351867675781 and perplexity is 115.62496205687529
At time: 1482.5815317630768 and batch: 850, loss is 4.777109270095825 and perplexity is 118.76054869625113
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.889668782552083 and perplexity of 132.90954479982474
Finished 46 epochs...
Completing Train Step...
At time: 1487.00457406044 and batch: 50, loss is 4.780487222671509 and perplexity is 119.16239452357576
At time: 1488.6990866661072 and batch: 100, loss is 4.737738237380982 and perplexity is 114.17567115800765
At time: 1490.412978887558 and batch: 150, loss is 4.734677925109863 and perplexity is 113.82679206227436
At time: 1492.1360440254211 and batch: 200, loss is 4.78093505859375 and perplexity is 119.21577167563463
At time: 1493.8837068080902 and batch: 250, loss is 4.769881391525269 and perplexity is 117.90525656864966
At time: 1495.6367440223694 and batch: 300, loss is 4.757380685806274 and perplexity is 116.4405317744098
At time: 1497.4370317459106 and batch: 350, loss is 4.713018884658814 and perplexity is 111.38792012325106
At time: 1499.1839637756348 and batch: 400, loss is 4.741439924240113 and perplexity is 114.59909695091369
At time: 1500.940309047699 and batch: 450, loss is 4.769513721466065 and perplexity is 117.86191430430128
At time: 1502.6948392391205 and batch: 500, loss is 4.772176303863525 and perplexity is 118.1761495155866
At time: 1504.4485929012299 and batch: 550, loss is 4.7584792995452885 and perplexity is 116.56852523721015
At time: 1506.2016847133636 and batch: 600, loss is 4.79431266784668 and perplexity is 120.82130886659131
At time: 1507.9532778263092 and batch: 650, loss is 4.78630220413208 and perplexity is 119.8573402284821
At time: 1509.7074983119965 and batch: 700, loss is 4.776646966934204 and perplexity is 118.70565800815997
At time: 1511.4595823287964 and batch: 750, loss is 4.766621580123902 and perplexity is 117.52153344131737
At time: 1513.20889544487 and batch: 800, loss is 4.744909706115723 and perplexity is 114.99742147044272
At time: 1514.9588086605072 and batch: 850, loss is 4.7723378086090085 and perplexity is 118.19523706585976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.886329968770345 and perplexity of 132.46652457249664
Finished 47 epochs...
Completing Train Step...
At time: 1519.3808550834656 and batch: 50, loss is 4.775986862182617 and perplexity is 118.62732569587868
At time: 1521.0741424560547 and batch: 100, loss is 4.733733835220337 and perplexity is 113.7193800500067
At time: 1522.776773929596 and batch: 150, loss is 4.731087894439697 and perplexity is 113.41888302875977
At time: 1524.4934508800507 and batch: 200, loss is 4.7762062454223635 and perplexity is 118.65335339782867
At time: 1526.216206073761 and batch: 250, loss is 4.766157388687134 and perplexity is 117.46699361129845
At time: 1527.945793390274 and batch: 300, loss is 4.752892608642578 and perplexity is 115.9191086516741
At time: 1529.687880039215 and batch: 350, loss is 4.707791585922241 and perplexity is 110.80718135899876
At time: 1531.4294848442078 and batch: 400, loss is 4.736285562515259 and perplexity is 114.00993144234519
At time: 1533.174721479416 and batch: 450, loss is 4.76541841506958 and perplexity is 117.3802206674932
At time: 1534.915678024292 and batch: 500, loss is 4.768258571624756 and perplexity is 117.71407274233155
At time: 1536.6562452316284 and batch: 550, loss is 4.755778732299805 and perplexity is 116.25414878453958
At time: 1538.406988143921 and batch: 600, loss is 4.790896625518799 and perplexity is 120.40928231214137
At time: 1540.1595413684845 and batch: 650, loss is 4.784938325881958 and perplexity is 119.69398083551145
At time: 1541.9427423477173 and batch: 700, loss is 4.773775787353515 and perplexity is 118.3653215641009
At time: 1543.6936349868774 and batch: 750, loss is 4.763020706176758 and perplexity is 117.09911420895867
At time: 1545.4476227760315 and batch: 800, loss is 4.740448131561279 and perplexity is 114.4854947497846
At time: 1547.1996097564697 and batch: 850, loss is 4.76768853187561 and perplexity is 117.64699016351929
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.883945782979329 and perplexity of 132.15107596030163
Finished 48 epochs...
Completing Train Step...
At time: 1551.635259628296 and batch: 50, loss is 4.770609798431397 and perplexity is 117.99117085828534
At time: 1553.3886609077454 and batch: 100, loss is 4.728467893600464 and perplexity is 113.12211439683354
At time: 1555.1144878864288 and batch: 150, loss is 4.726920394897461 and perplexity is 112.94719345139929
At time: 1556.8379695415497 and batch: 200, loss is 4.774230670928955 and perplexity is 118.41917625266008
At time: 1558.5691893100739 and batch: 250, loss is 4.764257354736328 and perplexity is 117.24401423661105
At time: 1560.3037855625153 and batch: 300, loss is 4.748490190505981 and perplexity is 115.4099059493903
At time: 1562.03342795372 and batch: 350, loss is 4.703735761642456 and perplexity is 110.35867704462598
At time: 1563.7862100601196 and batch: 400, loss is 4.7329673576354985 and perplexity is 113.63225009008288
At time: 1565.5344829559326 and batch: 450, loss is 4.7612340545654295 and perplexity is 116.89008567406017
At time: 1567.289375782013 and batch: 500, loss is 4.763961820602417 and perplexity is 117.20936974797402
At time: 1569.0361733436584 and batch: 550, loss is 4.753382930755615 and perplexity is 115.9759602906384
At time: 1570.7921702861786 and batch: 600, loss is 4.7872398853302 and perplexity is 119.96978091138952
At time: 1572.5504660606384 and batch: 650, loss is 4.780832500457763 and perplexity is 119.20354575525592
At time: 1574.3032648563385 and batch: 700, loss is 4.769706764221191 and perplexity is 117.88466888919632
At time: 1576.054791688919 and batch: 750, loss is 4.759483814239502 and perplexity is 116.68567886511292
At time: 1577.8108305931091 and batch: 800, loss is 4.737023239135742 and perplexity is 114.0940649311117
At time: 1579.5644090175629 and batch: 850, loss is 4.763077096939087 and perplexity is 117.10571770346326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.879837989807129 and perplexity of 131.60934010396798
Finished 49 epochs...
Completing Train Step...
At time: 1583.986997127533 and batch: 50, loss is 4.76620418548584 and perplexity is 117.47249081917798
At time: 1585.7349119186401 and batch: 100, loss is 4.723318891525269 and perplexity is 112.54114538394465
At time: 1587.4583439826965 and batch: 150, loss is 4.72273591041565 and perplexity is 112.47555514292434
At time: 1589.1808409690857 and batch: 200, loss is 4.769123001098633 and perplexity is 117.81587224920816
At time: 1590.9034898281097 and batch: 250, loss is 4.75864013671875 and perplexity is 116.58727529713676
At time: 1592.636152267456 and batch: 300, loss is 4.743908157348633 and perplexity is 114.88230360245274
At time: 1594.3802354335785 and batch: 350, loss is 4.699859504699707 and perplexity is 109.93172647618239
At time: 1596.124281167984 and batch: 400, loss is 4.729079103469848 and perplexity is 113.19127688384269
At time: 1597.8737835884094 and batch: 450, loss is 4.758075332641601 and perplexity is 116.52144492108769
At time: 1599.616904258728 and batch: 500, loss is 4.759899587631225 and perplexity is 116.7342037525597
At time: 1601.36256980896 and batch: 550, loss is 4.74969087600708 and perplexity is 115.54856017354776
At time: 1603.1149632930756 and batch: 600, loss is 4.784458923339844 and perplexity is 119.63661298906752
At time: 1604.8678493499756 and batch: 650, loss is 4.7770866012573245 and perplexity is 118.75785656306638
At time: 1606.6197304725647 and batch: 700, loss is 4.766173305511475 and perplexity is 117.46886332768159
At time: 1608.374234199524 and batch: 750, loss is 4.755658664703369 and perplexity is 116.24019126625892
At time: 1610.1265721321106 and batch: 800, loss is 4.7323438167572025 and perplexity is 113.56141782276103
At time: 1611.8782768249512 and batch: 850, loss is 4.759766645431519 and perplexity is 116.71868588224461
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.878126780192058 and perplexity of 131.38432151764212
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f37e976d898>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 7.503277863676992, 'batch_size': 50, 'dropout': 0.823769304253686, 'wordvec_source': 'glove', 'data': 'wikitext', 'lr': 2.862125934743378, 'num_layers': 1, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.2396905422210693 and batch: 50, loss is 7.732468433380127 and perplexity is 2281.2263117355133
At time: 3.819052219390869 and batch: 100, loss is 6.788782320022583 and perplexity is 887.831808333192
At time: 5.39741587638855 and batch: 150, loss is 6.598785419464111 and perplexity is 734.202898922014
At time: 6.976263523101807 and batch: 200, loss is 6.577354698181153 and perplexity is 718.6358040602806
At time: 8.558283567428589 and batch: 250, loss is 6.559121179580688 and perplexity is 705.6512812644556
At time: 10.14165472984314 and batch: 300, loss is 6.479320936203003 and perplexity is 651.5283666917572
At time: 11.758846521377563 and batch: 350, loss is 6.429596967697144 and perplexity is 619.9240482404971
At time: 13.34369421005249 and batch: 400, loss is 6.435625448226928 and perplexity is 623.6725357848809
At time: 14.933733701705933 and batch: 450, loss is 6.411415767669678 and perplexity is 608.7549267998016
At time: 16.519150495529175 and batch: 500, loss is 6.39293701171875 and perplexity is 597.6091899037858
At time: 18.11015224456787 and batch: 550, loss is 6.3257914066314695 and perplexity is 558.7998766441664
At time: 19.707484483718872 and batch: 600, loss is 6.331960496902465 and perplexity is 562.2578187383083
At time: 21.29625678062439 and batch: 650, loss is 6.337138528823853 and perplexity is 565.1767583310941
At time: 22.885233402252197 and batch: 700, loss is 6.29364203453064 and perplexity is 541.120524248362
At time: 24.496439218521118 and batch: 750, loss is 6.2689367866516115 and perplexity is 527.9157922707657
At time: 26.211034297943115 and batch: 800, loss is 6.282158231735229 and perplexity is 534.9419475388271
At time: 27.971830129623413 and batch: 850, loss is 6.274929866790772 and perplexity is 531.0891334665489
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.595225016276042 and perplexity of 269.13820375558083
Finished 1 epochs...
Completing Train Step...
At time: 32.492637157440186 and batch: 50, loss is 5.827079982757568 and perplexity is 339.36627548318694
At time: 34.24967360496521 and batch: 100, loss is 5.5986912822723385 and perplexity is 270.07272707707784
At time: 36.006080627441406 and batch: 150, loss is 5.491880989074707 and perplexity is 242.7133188536817
At time: 37.76214551925659 and batch: 200, loss is 5.4599550914764405 and perplexity is 235.0868667240835
At time: 39.51546287536621 and batch: 250, loss is 5.454944829940796 and perplexity is 233.9119657764802
At time: 41.268935680389404 and batch: 300, loss is 5.370755310058594 and perplexity is 215.025217093876
At time: 43.024723291397095 and batch: 350, loss is 5.297997522354126 and perplexity is 199.93604139015807
At time: 44.7959349155426 and batch: 400, loss is 5.28160306930542 and perplexity is 196.6849223250821
At time: 46.557049036026 and batch: 450, loss is 5.25749605178833 and perplexity is 192.0001305140696
At time: 48.31228470802307 and batch: 500, loss is 5.234201459884644 and perplexity is 187.5792569602981
At time: 50.079012870788574 and batch: 550, loss is 5.205992469787597 and perplexity is 182.3617715343129
At time: 51.83327293395996 and batch: 600, loss is 5.217265071868897 and perplexity is 184.42909337647285
At time: 53.58675289154053 and batch: 650, loss is 5.205965042114258 and perplexity is 182.3567698438063
At time: 55.33906006813049 and batch: 700, loss is 5.1379419040679934 and perplexity is 170.36478020051493
At time: 57.095760107040405 and batch: 750, loss is 5.110726518630981 and perplexity is 165.79076120657263
At time: 58.85083293914795 and batch: 800, loss is 5.085367984771729 and perplexity is 161.63940913599367
At time: 60.60200214385986 and batch: 850, loss is 5.079079446792602 and perplexity is 160.6261229562887
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.79761282602946 and perplexity of 121.2206969571598
Finished 2 epochs...
Completing Train Step...
At time: 65.07599687576294 and batch: 50, loss is 5.025443124771118 and perplexity is 152.23770146482994
At time: 66.82608485221863 and batch: 100, loss is 4.94397575378418 and perplexity is 140.32704777148052
At time: 68.58143091201782 and batch: 150, loss is 4.932574577331543 and perplexity is 138.7362401101505
At time: 70.33108353614807 and batch: 200, loss is 4.947574453353882 and perplexity is 140.83295241133595
At time: 72.08591341972351 and batch: 250, loss is 4.961303367614746 and perplexity is 142.7797692020893
At time: 73.83603358268738 and batch: 300, loss is 4.915434217453003 and perplexity is 136.37851487905064
At time: 75.58496332168579 and batch: 350, loss is 4.863951482772827 and perplexity is 129.53504765326613
At time: 77.33588123321533 and batch: 400, loss is 4.869638166427612 and perplexity is 130.27377094355
At time: 79.08796525001526 and batch: 450, loss is 4.874868593215942 and perplexity is 130.95694344931496
At time: 80.84281873703003 and batch: 500, loss is 4.863773221969605 and perplexity is 129.51195868961574
At time: 82.59801650047302 and batch: 550, loss is 4.861074161529541 and perplexity is 129.16286940386624
At time: 84.35275506973267 and batch: 600, loss is 4.886181077957153 and perplexity is 132.44680299214988
At time: 86.10314202308655 and batch: 650, loss is 4.877723340988159 and perplexity is 131.33132662239228
At time: 87.85308575630188 and batch: 700, loss is 4.827602910995483 and perplexity is 124.91117828379396
At time: 89.60270547866821 and batch: 750, loss is 4.810895862579346 and perplexity is 122.84161744575447
At time: 91.35046911239624 and batch: 800, loss is 4.787380151748657 and perplexity is 119.98660982311905
At time: 93.09919047355652 and batch: 850, loss is 4.792158355712891 and perplexity is 120.561302223172
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.625355084737142 and perplexity of 102.03899914184719
Finished 3 epochs...
Completing Train Step...
At time: 97.57247352600098 and batch: 50, loss is 4.760002365112305 and perplexity is 116.74620201654233
At time: 99.31895756721497 and batch: 100, loss is 4.699900274276733 and perplexity is 109.93620843753581
At time: 101.0724105834961 and batch: 150, loss is 4.700990695953369 and perplexity is 110.05615064416978
At time: 102.82275748252869 and batch: 200, loss is 4.724013566970825 and perplexity is 112.6193521152606
At time: 104.57601881027222 and batch: 250, loss is 4.735078830718994 and perplexity is 113.8724350103271
At time: 106.32538414001465 and batch: 300, loss is 4.701305084228515 and perplexity is 110.09075644708437
At time: 108.10492706298828 and batch: 350, loss is 4.652288475036621 and perplexity is 104.82459977679602
At time: 109.85374927520752 and batch: 400, loss is 4.66491888999939 and perplexity is 106.15697448050638
At time: 111.60303282737732 and batch: 450, loss is 4.68148832321167 and perplexity is 107.93058869223606
At time: 113.35647249221802 and batch: 500, loss is 4.6698174571990965 and perplexity is 106.67826730527334
At time: 115.10971426963806 and batch: 550, loss is 4.67384298324585 and perplexity is 107.10856896311768
At time: 116.86102557182312 and batch: 600, loss is 4.705255870819092 and perplexity is 110.52656185161271
At time: 118.611820936203 and batch: 650, loss is 4.695969362258911 and perplexity is 109.50490713370877
At time: 120.36593985557556 and batch: 700, loss is 4.653664398193359 and perplexity is 104.96892964165633
At time: 122.11926364898682 and batch: 750, loss is 4.64149489402771 and perplexity is 103.69925116345787
At time: 123.87616682052612 and batch: 800, loss is 4.617265872955322 and perplexity is 101.216913562743
At time: 125.6269474029541 and batch: 850, loss is 4.626752271652221 and perplexity is 102.1816663394381
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.541232426961263 and perplexity of 93.80633836916645
Finished 4 epochs...
Completing Train Step...
At time: 130.08441257476807 and batch: 50, loss is 4.600388050079346 and perplexity is 99.52292802979787
At time: 131.79920554161072 and batch: 100, loss is 4.547190885543824 and perplexity is 94.36694807736423
At time: 133.52143788337708 and batch: 150, loss is 4.553606567382812 and perplexity is 94.9743226700742
At time: 135.27008843421936 and batch: 200, loss is 4.582283515930175 and perplexity is 97.73732431500223
At time: 137.0201358795166 and batch: 250, loss is 4.58996353149414 and perplexity is 98.49083828273845
At time: 138.77096438407898 and batch: 300, loss is 4.561684074401856 and perplexity is 95.74458514027688
At time: 140.52546310424805 and batch: 350, loss is 4.51511022567749 and perplexity is 91.38763873163586
At time: 142.27803993225098 and batch: 400, loss is 4.5304951667785645 and perplexity is 92.80450341360647
At time: 144.02899765968323 and batch: 450, loss is 4.554090194702148 and perplexity is 95.0202659559715
At time: 145.77855038642883 and batch: 500, loss is 4.539027309417724 and perplexity is 93.5997122678813
At time: 147.5280978679657 and batch: 550, loss is 4.548965034484863 and perplexity is 94.5345177011465
At time: 149.27780628204346 and batch: 600, loss is 4.582379961013794 and perplexity is 97.74675105399248
At time: 151.0561327934265 and batch: 650, loss is 4.571755619049072 and perplexity is 96.71375332218803
At time: 152.79553771018982 and batch: 700, loss is 4.534804248809815 and perplexity is 93.20526847632719
At time: 154.53554320335388 and batch: 750, loss is 4.52363094329834 and perplexity is 92.16965392928401
At time: 156.27741932868958 and batch: 800, loss is 4.497202396392822 and perplexity is 89.76565098442674
At time: 158.01856541633606 and batch: 850, loss is 4.51006649017334 and perplexity is 90.92786411898727
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.499770482381185 and perplexity of 89.99647315368814
Finished 5 epochs...
Completing Train Step...
At time: 162.45331001281738 and batch: 50, loss is 4.488201379776001 and perplexity is 88.96129431280654
At time: 164.16154193878174 and batch: 100, loss is 4.4380780220031735 and perplexity is 84.61216258027555
At time: 165.88667726516724 and batch: 150, loss is 4.446113357543945 and perplexity is 85.29478858905267
At time: 167.62508392333984 and batch: 200, loss is 4.478930444717407 and perplexity is 88.14035126535626
At time: 169.3811388015747 and batch: 250, loss is 4.482967596054078 and perplexity is 88.49690645162644
At time: 171.1363124847412 and batch: 300, loss is 4.4579860210418705 and perplexity is 86.31350035665503
At time: 172.89260578155518 and batch: 350, loss is 4.412870635986328 and perplexity is 82.50596848415503
At time: 174.65049195289612 and batch: 400, loss is 4.431927976608276 and perplexity is 84.09339080954308
At time: 176.40678882598877 and batch: 450, loss is 4.457947988510131 and perplexity is 86.3102176981375
At time: 178.1633701324463 and batch: 500, loss is 4.4417840766906735 and perplexity is 84.9263216678282
At time: 179.91895031929016 and batch: 550, loss is 4.454514055252075 and perplexity is 86.01434246971152
At time: 181.67485427856445 and batch: 600, loss is 4.489197635650635 and perplexity is 89.04996668773381
At time: 183.43030834197998 and batch: 650, loss is 4.478164844512939 and perplexity is 88.07289681926933
At time: 185.18535447120667 and batch: 700, loss is 4.443724241256714 and perplexity is 85.09125265274201
At time: 186.94042134284973 and batch: 750, loss is 4.433635292053222 and perplexity is 84.237087387278
At time: 188.6950147151947 and batch: 800, loss is 4.404525594711304 and perplexity is 81.82031764304297
At time: 190.4503836631775 and batch: 850, loss is 4.420128498077393 and perplexity is 83.10696375731297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.468952178955078 and perplexity of 87.26523673964407
Finished 6 epochs...
Completing Train Step...
At time: 194.87718629837036 and batch: 50, loss is 4.40151891708374 and perplexity is 81.57467978647878
At time: 196.6068091392517 and batch: 100, loss is 4.352908201217652 and perplexity is 77.70411384558311
At time: 198.31625986099243 and batch: 150, loss is 4.362084083557129 and perplexity is 78.42039889947513
At time: 200.02983212471008 and batch: 200, loss is 4.3975489139556885 and perplexity is 81.2514700488982
At time: 201.76795935630798 and batch: 250, loss is 4.40018443107605 and perplexity is 81.46589212167476
At time: 203.50691151618958 and batch: 300, loss is 4.377093648910522 and perplexity is 79.60633291429822
At time: 205.24549436569214 and batch: 350, loss is 4.33321171760559 and perplexity is 76.188590278666
At time: 206.9839563369751 and batch: 400, loss is 4.353383722305298 and perplexity is 77.74107257694523
At time: 208.73152136802673 and batch: 450, loss is 4.381529903411865 and perplexity is 79.96027136708254
At time: 210.47963428497314 and batch: 500, loss is 4.363740377426147 and perplexity is 78.55039375049769
At time: 212.22629523277283 and batch: 550, loss is 4.379870138168335 and perplexity is 79.82766616499896
At time: 213.97514414787292 and batch: 600, loss is 4.417110433578491 and perplexity is 82.85651969864865
At time: 215.72091484069824 and batch: 650, loss is 4.4031980514526365 and perplexity is 81.71176969892149
At time: 217.47013330459595 and batch: 700, loss is 4.369457740783691 and perplexity is 79.00078118087916
At time: 219.21903443336487 and batch: 750, loss is 4.36134241104126 and perplexity is 78.36225820826947
At time: 220.97142362594604 and batch: 800, loss is 4.329406890869141 and perplexity is 75.89925667452744
At time: 222.72249674797058 and batch: 850, loss is 4.346659717559814 and perplexity is 77.22009472766405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.448629697163899 and perplexity of 85.50968951341468
Finished 7 epochs...
Completing Train Step...
At time: 227.12381148338318 and batch: 50, loss is 4.331135015487671 and perplexity is 76.03053344715559
At time: 228.86428570747375 and batch: 100, loss is 4.283245115280152 and perplexity is 72.47524934866848
At time: 230.57481288909912 and batch: 150, loss is 4.29262496471405 and perplexity is 73.15825451038447
At time: 232.30926036834717 and batch: 200, loss is 4.3313063621521 and perplexity is 76.04356214163629
At time: 234.05918550491333 and batch: 250, loss is 4.332259063720703 and perplexity is 76.11604348363265
At time: 235.810688495636 and batch: 300, loss is 4.310822162628174 and perplexity is 74.50171629434745
At time: 237.56160926818848 and batch: 350, loss is 4.267453260421753 and perplexity is 71.33972038277648
At time: 239.3389081954956 and batch: 400, loss is 4.28818943977356 and perplexity is 72.83447783848109
At time: 241.08321905136108 and batch: 450, loss is 4.317220153808594 and perplexity is 74.97990571277138
At time: 242.83401775360107 and batch: 500, loss is 4.300150156021118 and perplexity is 73.71086099826493
At time: 244.58739161491394 and batch: 550, loss is 4.317345819473267 and perplexity is 74.98932870452072
At time: 246.33772945404053 and batch: 600, loss is 4.356183204650879 and perplexity is 77.9590122539554
At time: 248.08700013160706 and batch: 650, loss is 4.341116952896118 and perplexity is 76.79326591387444
At time: 249.84447741508484 and batch: 700, loss is 4.308395671844482 and perplexity is 74.32115771684404
At time: 251.6022412776947 and batch: 750, loss is 4.301839075088501 and perplexity is 73.8354578642723
At time: 253.3525414466858 and batch: 800, loss is 4.267307996749878 and perplexity is 71.32935806569546
At time: 255.10298323631287 and batch: 850, loss is 4.285438957214356 and perplexity is 72.63442312709529
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.434878667195638 and perplexity of 84.34189082930013
Finished 8 epochs...
Completing Train Step...
At time: 259.5118818283081 and batch: 50, loss is 4.272887372970581 and perplexity is 71.7284436790404
At time: 261.2432470321655 and batch: 100, loss is 4.224833164215088 and perplexity is 68.3630971861118
At time: 262.9778039455414 and batch: 150, loss is 4.2338864660263065 and perplexity is 68.9848190088673
At time: 264.7339642047882 and batch: 200, loss is 4.274539403915405 and perplexity is 71.84703922240334
At time: 266.4891381263733 and batch: 250, loss is 4.275027694702149 and perplexity is 71.88213003627381
At time: 268.24578642845154 and batch: 300, loss is 4.25513111114502 and perplexity is 70.46605546801605
At time: 270.0025396347046 and batch: 350, loss is 4.21130136013031 and perplexity is 67.4442519857266
At time: 271.75891184806824 and batch: 400, loss is 4.233063287734986 and perplexity is 68.92805556984922
At time: 273.51447105407715 and batch: 450, loss is 4.2637084865570065 and perplexity is 71.0730688488282
At time: 275.27028584480286 and batch: 500, loss is 4.246660032272339 and perplexity is 69.87165312335266
At time: 277.0265874862671 and batch: 550, loss is 4.264835796356201 and perplexity is 71.15323539357624
At time: 278.7831914424896 and batch: 600, loss is 4.303984966278076 and perplexity is 73.99407084501472
At time: 280.5401291847229 and batch: 650, loss is 4.287559080123901 and perplexity is 72.78858039001732
At time: 282.2971158027649 and batch: 700, loss is 4.256139831542969 and perplexity is 70.53717187778604
At time: 284.10020208358765 and batch: 750, loss is 4.250623998641967 and perplexity is 70.14917168024921
At time: 285.85735845565796 and batch: 800, loss is 4.215125179290771 and perplexity is 67.70264030900594
At time: 287.6158854961395 and batch: 850, loss is 4.232940006256103 and perplexity is 68.919558540996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.426647186279297 and perplexity of 83.65048172889381
Finished 9 epochs...
Completing Train Step...
At time: 292.049414396286 and batch: 50, loss is 4.220993537902832 and perplexity is 68.10111172436983
At time: 293.7445755004883 and batch: 100, loss is 4.174723792076111 and perplexity is 65.02187762267695
At time: 295.4471764564514 and batch: 150, loss is 4.183736529350281 and perplexity is 65.61055151955121
At time: 297.1651930809021 and batch: 200, loss is 4.226013889312744 and perplexity is 68.44386288237223
At time: 298.88768196105957 and batch: 250, loss is 4.224959268569946 and perplexity is 68.37171861396692
At time: 300.6103820800781 and batch: 300, loss is 4.207393326759338 and perplexity is 67.18119195677554
At time: 302.33512234687805 and batch: 350, loss is 4.162947840690613 and perplexity is 64.26067389833997
At time: 304.0779585838318 and batch: 400, loss is 4.186207675933838 and perplexity is 65.77288530246211
At time: 305.8205590248108 and batch: 450, loss is 4.218120479583741 and perplexity is 67.90573405889917
At time: 307.56705379486084 and batch: 500, loss is 4.200880861282348 and perplexity is 66.74509832711462
At time: 309.31895542144775 and batch: 550, loss is 4.221632289886474 and perplexity is 68.14462534029683
At time: 311.07125544548035 and batch: 600, loss is 4.259907674789429 and perplexity is 70.80344620942451
At time: 312.82480096817017 and batch: 650, loss is 4.242365455627441 and perplexity is 69.57222736734002
At time: 314.57503867149353 and batch: 700, loss is 4.212258019447327 and perplexity is 67.5088040300021
At time: 316.3259575366974 and batch: 750, loss is 4.205362329483032 and perplexity is 67.04488560466199
At time: 318.0766854286194 and batch: 800, loss is 4.167777962684632 and perplexity is 64.57181160431986
At time: 319.8280897140503 and batch: 850, loss is 4.186106152534485 and perplexity is 65.76620815451005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.420878092447917 and perplexity of 83.1692836238893
Finished 10 epochs...
Completing Train Step...
At time: 324.2657480239868 and batch: 50, loss is 4.175719318389892 and perplexity is 65.08664084421866
At time: 325.97185587882996 and batch: 100, loss is 4.133708229064942 and perplexity is 62.40892094785453
At time: 327.7181804180145 and batch: 150, loss is 4.139865932464599 and perplexity is 62.79440219410076
At time: 329.437296628952 and batch: 200, loss is 4.183669815063476 and perplexity is 65.60617450440608
At time: 331.1790204048157 and batch: 250, loss is 4.180366234779358 and perplexity is 65.38979684706726
At time: 332.9297387599945 and batch: 300, loss is 4.1643735122680665 and perplexity is 64.35235385188825
At time: 334.6811800003052 and batch: 350, loss is 4.119833130836486 and perplexity is 61.548970782185975
At time: 336.4317181110382 and batch: 400, loss is 4.144760222434997 and perplexity is 63.10248952633124
At time: 338.18242502212524 and batch: 450, loss is 4.1763596343994145 and perplexity is 65.12833020811641
At time: 339.933958530426 and batch: 500, loss is 4.16191403388977 and perplexity is 64.19427510431323
At time: 341.6827473640442 and batch: 550, loss is 4.183711643218994 and perplexity is 65.6089187470691
At time: 343.431129693985 and batch: 600, loss is 4.218534669876099 and perplexity is 67.93386578028259
At time: 345.1804633140564 and batch: 650, loss is 4.20117338180542 and perplexity is 66.76462549409904
At time: 346.9297354221344 and batch: 700, loss is 4.173113465309143 and perplexity is 64.91725541328321
At time: 348.68022894859314 and batch: 750, loss is 4.1660867691040036 and perplexity is 64.46270046108367
At time: 350.43178844451904 and batch: 800, loss is 4.1263848447799685 and perplexity is 61.953545915340705
At time: 352.184285402298 and batch: 850, loss is 4.145078678131103 and perplexity is 63.122588073637935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.418466567993164 and perplexity of 82.9689605016808
Finished 11 epochs...
Completing Train Step...
At time: 356.74287009239197 and batch: 50, loss is 4.136366691589355 and perplexity is 62.57505345606304
At time: 358.44517397880554 and batch: 100, loss is 4.096216139793396 and perplexity is 60.11239980394421
At time: 360.1644628047943 and batch: 150, loss is 4.1004561424255375 and perplexity is 60.36781764083732
At time: 361.90055799484253 and batch: 200, loss is 4.146817808151245 and perplexity is 63.23246197631367
At time: 363.63913559913635 and batch: 250, loss is 4.1414046955108645 and perplexity is 62.89110227988819
At time: 365.3780105113983 and batch: 300, loss is 4.126308474540711 and perplexity is 61.94881468888102
At time: 367.1168804168701 and batch: 350, loss is 4.083543162345887 and perplexity is 59.35540354654313
At time: 368.8570020198822 and batch: 400, loss is 4.1080519962310795 and perplexity is 60.82810869686611
At time: 370.5972168445587 and batch: 450, loss is 4.1392265319824215 and perplexity is 62.754264256535585
At time: 372.3698561191559 and batch: 500, loss is 4.1257000827789305 and perplexity is 61.911137002931305
At time: 374.1201882362366 and batch: 550, loss is 4.145191254615784 and perplexity is 63.12969459271319
At time: 375.87194085121155 and batch: 600, loss is 4.183327560424805 and perplexity is 65.58372432891395
At time: 377.6225402355194 and batch: 650, loss is 4.165467910766601 and perplexity is 64.42281952305018
At time: 379.3748104572296 and batch: 700, loss is 4.138329353332519 and perplexity is 62.69798771928305
At time: 381.124951839447 and batch: 750, loss is 4.130878052711487 and perplexity is 62.232542404465825
At time: 382.8772020339966 and batch: 800, loss is 4.089399538040161 and perplexity is 59.70403093928203
At time: 384.626699924469 and batch: 850, loss is 4.110377187728882 and perplexity is 60.96971025961277
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.418569246927897 and perplexity of 82.97748010354539
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 389.05271196365356 and batch: 50, loss is 4.135468945503235 and perplexity is 62.51890215531059
At time: 390.787380695343 and batch: 100, loss is 4.107952308654785 and perplexity is 60.82204519237261
At time: 392.5074608325958 and batch: 150, loss is 4.109639668464661 and perplexity is 60.92476050145609
At time: 394.23955965042114 and batch: 200, loss is 4.146402096748352 and perplexity is 63.2061809838697
At time: 395.9846169948578 and batch: 250, loss is 4.130712723731994 and perplexity is 62.222254412213935
At time: 397.7304792404175 and batch: 300, loss is 4.110275244712829 and perplexity is 60.96349514026043
At time: 399.48649859428406 and batch: 350, loss is 4.065392508506775 and perplexity is 58.287782483612006
At time: 401.24140787124634 and batch: 400, loss is 4.068217549324036 and perplexity is 58.45268066064723
At time: 402.9961588382721 and batch: 450, loss is 4.0955590677261355 and perplexity is 60.072914598870355
At time: 404.750173330307 and batch: 500, loss is 4.068743605613708 and perplexity is 58.483438150333114
At time: 406.50508642196655 and batch: 550, loss is 4.077184705734253 and perplexity is 58.97919211796637
At time: 408.25877261161804 and batch: 600, loss is 4.10161217212677 and perplexity is 60.437644984493865
At time: 410.0136978626251 and batch: 650, loss is 4.070856680870056 and perplexity is 58.60714871523004
At time: 411.76822662353516 and batch: 700, loss is 4.031201758384705 and perplexity is 56.328563911886626
At time: 413.52570271492004 and batch: 750, loss is 4.008146486282349 and perplexity is 55.044749752787055
At time: 415.3118386268616 and batch: 800, loss is 3.947792444229126 and perplexity is 51.82084307136929
At time: 417.06703066825867 and batch: 850, loss is 3.9593915367126464 and perplexity is 52.42541730071129
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.358329137166341 and perplexity of 78.12648666334609
Finished 13 epochs...
Completing Train Step...
At time: 421.47497367858887 and batch: 50, loss is 4.091203656196594 and perplexity is 59.81184128757309
At time: 423.2016978263855 and batch: 100, loss is 4.056811633110047 and perplexity is 57.789762066366826
At time: 424.90818309783936 and batch: 150, loss is 4.057219729423523 and perplexity is 57.813350668106125
At time: 426.6353704929352 and batch: 200, loss is 4.0986184406280515 and perplexity is 60.25698146695894
At time: 428.3831191062927 and batch: 250, loss is 4.084586067199707 and perplexity is 59.4173378751947
At time: 430.13277673721313 and batch: 300, loss is 4.069392700195312 and perplexity is 58.52141175604766
At time: 431.88201212882996 and batch: 350, loss is 4.02559386253357 and perplexity is 56.01356326296515
At time: 433.63202691078186 and batch: 400, loss is 4.03186616897583 and perplexity is 56.36600164196753
At time: 435.3819901943207 and batch: 450, loss is 4.064325952529908 and perplexity is 58.22564844143282
At time: 437.1318473815918 and batch: 500, loss is 4.039518995285034 and perplexity is 56.79901563942006
At time: 438.88067626953125 and batch: 550, loss is 4.0530073595047 and perplexity is 57.57033165118892
At time: 440.635377407074 and batch: 600, loss is 4.081865458488465 and perplexity is 59.25590624389632
At time: 442.3839225769043 and batch: 650, loss is 4.055507407188416 and perplexity is 57.714440289658704
At time: 444.1375238895416 and batch: 700, loss is 4.020808691978455 and perplexity is 55.74616908295981
At time: 445.8862192630768 and batch: 750, loss is 4.003002700805664 and perplexity is 54.762338323028644
At time: 447.63489866256714 and batch: 800, loss is 3.950132632255554 and perplexity is 51.94225559650367
At time: 449.3846995830536 and batch: 850, loss is 3.967943844795227 and perplexity is 52.87569834740869
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.354111353556315 and perplexity of 77.79765999573029
Finished 14 epochs...
Completing Train Step...
At time: 453.88808393478394 and batch: 50, loss is 4.0747001314163205 and perplexity is 58.83283582377201
At time: 455.6125500202179 and batch: 100, loss is 4.037854709625244 and perplexity is 56.70456447086515
At time: 457.324102640152 and batch: 150, loss is 4.037176260948181 and perplexity is 56.66610638151153
At time: 459.0708475112915 and batch: 200, loss is 4.079831833839417 and perplexity is 59.13552441956756
At time: 460.80629873275757 and batch: 250, loss is 4.065890870094299 and perplexity is 58.31683811492774
At time: 462.55530738830566 and batch: 300, loss is 4.0517146158218384 and perplexity is 57.49595605327675
At time: 464.3039209842682 and batch: 350, loss is 4.0081853723526 and perplexity is 55.04689026841079
At time: 466.05378675460815 and batch: 400, loss is 4.016346640586853 and perplexity is 55.497980937539914
At time: 467.80349230766296 and batch: 450, loss is 4.050403389930725 and perplexity is 57.42061527225128
At time: 469.55255031585693 and batch: 500, loss is 4.026007981300354 and perplexity is 56.03676433437476
At time: 471.3024477958679 and batch: 550, loss is 4.0415611791610715 and perplexity is 56.915128194573555
At time: 473.0516834259033 and batch: 600, loss is 4.0723711061477665 and perplexity is 58.695972103915196
At time: 474.8032228946686 and batch: 650, loss is 4.047829976081848 and perplexity is 57.273038235537236
At time: 476.5538227558136 and batch: 700, loss is 4.0152263164520265 and perplexity is 55.435840025542724
At time: 478.30331563949585 and batch: 750, loss is 4.000314741134644 and perplexity is 54.615337021419336
At time: 480.052499294281 and batch: 800, loss is 3.9502246952056885 and perplexity is 51.94703777391782
At time: 481.80194783210754 and batch: 850, loss is 3.970382933616638 and perplexity is 53.0048242829693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3524274826049805 and perplexity of 77.66676900867469
Finished 15 epochs...
Completing Train Step...
At time: 486.2244327068329 and batch: 50, loss is 4.06170663356781 and perplexity is 58.07333646030213
At time: 487.9260697364807 and batch: 100, loss is 4.0241726303100585 and perplexity is 55.93401152603867
At time: 489.65144395828247 and batch: 150, loss is 4.023039450645447 and perplexity is 55.87066414038051
At time: 491.37508273124695 and batch: 200, loss is 4.066739649772644 and perplexity is 58.366357274480244
At time: 493.1213753223419 and batch: 250, loss is 4.05294545173645 and perplexity is 57.566767710757944
At time: 494.88308215141296 and batch: 300, loss is 4.039439940452576 and perplexity is 56.79452558023768
At time: 496.6392014026642 and batch: 350, loss is 3.9961055946350097 and perplexity is 54.38593619629879
At time: 498.39302039146423 and batch: 400, loss is 4.00521224975586 and perplexity is 54.88347216647653
At time: 500.1489496231079 and batch: 450, loss is 4.040982666015625 and perplexity is 56.88221156700716
At time: 501.90399074554443 and batch: 500, loss is 4.016930618286133 and perplexity is 55.530399985841775
At time: 503.70220494270325 and batch: 550, loss is 4.03318344116211 and perplexity is 56.440299932974845
At time: 505.44692945480347 and batch: 600, loss is 4.065353631973267 and perplexity is 58.28551650073024
At time: 507.19230604171753 and batch: 650, loss is 4.041702585220337 and perplexity is 56.92317690761913
At time: 508.9378561973572 and batch: 700, loss is 4.010816197395325 and perplexity is 55.19189966939775
At time: 510.682564496994 and batch: 750, loss is 3.996728367805481 and perplexity is 54.41981684709791
At time: 512.4255299568176 and batch: 800, loss is 3.9486425876617433 and perplexity is 51.864916952684965
At time: 514.1697719097137 and batch: 850, loss is 3.9698565006256104 and perplexity is 52.97692813815284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351430892944336 and perplexity of 77.5894056658618
Finished 16 epochs...
Completing Train Step...
At time: 518.5633115768433 and batch: 50, loss is 4.050723509788513 and perplexity is 57.438999693898566
At time: 520.2607243061066 and batch: 100, loss is 4.012938566207886 and perplexity is 55.309161628507816
At time: 521.9763159751892 and batch: 150, loss is 4.01169554233551 and perplexity is 55.24045373186713
At time: 523.7066698074341 and batch: 200, loss is 4.05623185634613 and perplexity is 57.756266616008666
At time: 525.4465134143829 and batch: 250, loss is 4.042765078544616 and perplexity is 56.98368954452393
At time: 527.185450553894 and batch: 300, loss is 4.029730911254883 and perplexity is 56.245774105829724
At time: 528.9389855861664 and batch: 350, loss is 3.986593818664551 and perplexity is 53.871081826700056
At time: 530.6903324127197 and batch: 400, loss is 3.9964282751083373 and perplexity is 54.403488307642476
At time: 532.4533317089081 and batch: 450, loss is 4.033345794677734 and perplexity is 56.449463957977564
At time: 534.2045447826385 and batch: 500, loss is 4.009187207221985 and perplexity is 55.10206579630192
At time: 535.9577448368073 and batch: 550, loss is 4.026277894973755 and perplexity is 56.05189146469959
At time: 537.7094988822937 and batch: 600, loss is 4.0590928220748905 and perplexity is 57.92174191212412
At time: 539.462732553482 and batch: 650, loss is 4.036263465881348 and perplexity is 56.614405438922255
At time: 541.2151529788971 and batch: 700, loss is 4.006169657707215 and perplexity is 54.93604320107428
At time: 542.9668292999268 and batch: 750, loss is 3.993301682472229 and perplexity is 54.233656397671204
At time: 544.7181980609894 and batch: 800, loss is 3.946325798034668 and perplexity is 51.744895936487076
At time: 546.4691827297211 and batch: 850, loss is 3.9684274291992185 and perplexity is 52.901274394069866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.350969314575195 and perplexity of 77.55360033864994
Finished 17 epochs...
Completing Train Step...
At time: 550.9058623313904 and batch: 50, loss is 4.041309022903443 and perplexity is 56.900778498104465
At time: 552.5877149105072 and batch: 100, loss is 4.003347225189209 and perplexity is 54.7812085343182
At time: 554.2754385471344 and batch: 150, loss is 4.001982522010803 and perplexity is 54.70649943438252
At time: 555.9801166057587 and batch: 200, loss is 4.047168369293213 and perplexity is 57.2351585367495
At time: 557.6908028125763 and batch: 250, loss is 4.033992776870727 and perplexity is 56.4859975729872
At time: 559.4113066196442 and batch: 300, loss is 4.021374807357788 and perplexity is 55.777736781253275
At time: 561.1369526386261 and batch: 350, loss is 3.9784356260299685 and perplexity is 53.433379023264905
At time: 562.8806662559509 and batch: 400, loss is 3.9887880229949952 and perplexity is 53.989415764672614
At time: 564.6187946796417 and batch: 450, loss is 4.026567845344544 and perplexity is 56.06814608781631
At time: 566.357409954071 and batch: 500, loss is 4.002564692497254 and perplexity is 54.738357216200626
At time: 568.0960478782654 and batch: 550, loss is 4.019834899902344 and perplexity is 55.691910327893844
At time: 569.8364334106445 and batch: 600, loss is 4.053405375480652 and perplexity is 57.59325012358346
At time: 571.5852019786835 and batch: 650, loss is 4.031211700439453 and perplexity is 56.32912393633683
At time: 573.3356783390045 and batch: 700, loss is 4.001696181297302 and perplexity is 54.6908369788077
At time: 575.0868763923645 and batch: 750, loss is 3.989707555770874 and perplexity is 54.03908363413603
At time: 576.8368670940399 and batch: 800, loss is 3.9436184644699095 and perplexity is 51.6049947080114
At time: 578.5866456031799 and batch: 850, loss is 3.9660210037231445 and perplexity is 52.77412446941937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.350705464680989 and perplexity of 77.53314052868333
Finished 18 epochs...
Completing Train Step...
At time: 582.9728517532349 and batch: 50, loss is 4.032617969512939 and perplexity is 56.408393565415416
At time: 584.6929919719696 and batch: 100, loss is 3.9948030138015747 and perplexity is 54.315140236941
At time: 586.3956582546234 and batch: 150, loss is 3.9934605407714843 and perplexity is 54.24227254844433
At time: 588.1232917308807 and batch: 200, loss is 4.039156427383423 and perplexity is 56.77842587232598
At time: 589.8641860485077 and batch: 250, loss is 4.026138329505921 and perplexity is 56.04406910212287
At time: 591.6311347484589 and batch: 300, loss is 4.013836269378662 and perplexity is 55.358835130969155
At time: 593.3818123340607 and batch: 350, loss is 3.971233458518982 and perplexity is 53.04992538305189
At time: 595.1351194381714 and batch: 400, loss is 3.981984920501709 and perplexity is 53.62336678184181
At time: 596.886084318161 and batch: 450, loss is 4.02036723613739 and perplexity is 55.72156504219948
At time: 598.6373569965363 and batch: 500, loss is 3.9964724254608153 and perplexity is 54.40589029385117
At time: 600.3905103206635 and batch: 550, loss is 4.013976945877075 and perplexity is 55.366623365849556
At time: 602.1439502239227 and batch: 600, loss is 4.048093099594116 and perplexity is 57.28811010131017
At time: 603.8939597606659 and batch: 650, loss is 4.026353077888489 and perplexity is 56.05610576769606
At time: 605.6449480056763 and batch: 700, loss is 3.9972351741790773 and perplexity is 54.44740414724526
At time: 607.3952107429504 and batch: 750, loss is 3.986125626564026 and perplexity is 53.84586571519479
At time: 609.1455307006836 and batch: 800, loss is 3.9404324960708617 and perplexity is 51.44084445324996
At time: 610.8969321250916 and batch: 850, loss is 3.9631239986419677 and perplexity is 52.621458806050356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.350512504577637 and perplexity of 77.51818116919986
Finished 19 epochs...
Completing Train Step...
At time: 615.2837867736816 and batch: 50, loss is 4.024696936607361 and perplexity is 55.96334576990757
At time: 616.9973073005676 and batch: 100, loss is 3.9869296979904174 and perplexity is 53.88917904841889
At time: 618.7115087509155 and batch: 150, loss is 3.985707697868347 and perplexity is 53.823366684591484
At time: 620.436705827713 and batch: 200, loss is 4.031731462478637 and perplexity is 56.358409286706866
At time: 622.1787571907043 and batch: 250, loss is 4.019074282646179 and perplexity is 55.64956620574613
At time: 623.9214560985565 and batch: 300, loss is 4.006977219581604 and perplexity is 54.98042537335635
At time: 625.6635792255402 and batch: 350, loss is 3.9645304918289184 and perplexity is 52.69552260225129
At time: 627.4152557849884 and batch: 400, loss is 3.975668077468872 and perplexity is 53.285703995197835
At time: 629.1674904823303 and batch: 450, loss is 4.014590196609497 and perplexity is 55.40058740135079
At time: 630.9209458827972 and batch: 500, loss is 3.9907322692871094 and perplexity is 54.094486594774274
At time: 632.6783936023712 and batch: 550, loss is 4.0084440135955814 and perplexity is 55.06112950587971
At time: 634.4591536521912 and batch: 600, loss is 4.043084440231323 and perplexity is 57.00189085797802
At time: 636.2058489322662 and batch: 650, loss is 4.021707720756531 and perplexity is 55.79630902848273
At time: 637.9583055973053 and batch: 700, loss is 3.9929341459274292 and perplexity is 54.21372720956375
At time: 639.7029240131378 and batch: 750, loss is 3.9822144651412965 and perplexity is 53.63567715107881
At time: 641.4484164714813 and batch: 800, loss is 3.9371168994903565 and perplexity is 51.27056980229599
At time: 643.193915605545 and batch: 850, loss is 3.9600785303115846 and perplexity is 52.461445601007384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.350543022155762 and perplexity of 77.52054687244738
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 647.5540285110474 and batch: 50, loss is 4.033040165901184 and perplexity is 56.43221401354517
At time: 649.2967391014099 and batch: 100, loss is 4.01306272983551 and perplexity is 55.31602944101388
At time: 651.0229949951172 and batch: 150, loss is 4.016288232803345 and perplexity is 55.49473951814714
At time: 652.7672801017761 and batch: 200, loss is 4.064789652824402 and perplexity is 58.25265395251006
At time: 654.5167877674103 and batch: 250, loss is 4.045761933326721 and perplexity is 57.15471753205643
At time: 656.2659647464752 and batch: 300, loss is 4.027438254356384 and perplexity is 56.116969552557734
At time: 658.0154900550842 and batch: 350, loss is 3.9800233459472656 and perplexity is 53.51828364793444
At time: 659.7651546001434 and batch: 400, loss is 3.9870902585983274 and perplexity is 53.897832222427525
At time: 661.5269482135773 and batch: 450, loss is 4.025975723266601 and perplexity is 56.03495672769455
At time: 663.2869832515717 and batch: 500, loss is 3.995700407028198 and perplexity is 54.36390415282411
At time: 665.0359904766083 and batch: 550, loss is 4.007566428184509 and perplexity is 55.01282985854097
At time: 666.7838315963745 and batch: 600, loss is 4.039222660064698 and perplexity is 56.7821865842497
At time: 668.5293700695038 and batch: 650, loss is 4.013955020904541 and perplexity is 55.36540946746038
At time: 670.2737247943878 and batch: 700, loss is 3.9788803911209105 and perplexity is 53.45714961071742
At time: 672.0180053710938 and batch: 750, loss is 3.9668996047973635 and perplexity is 52.820512247060755
At time: 673.7629442214966 and batch: 800, loss is 3.914453163146973 and perplexity is 50.121655647275
At time: 675.5102581977844 and batch: 850, loss is 3.9382296562194825 and perplexity is 51.327653227942434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.34020201365153 and perplexity of 76.72303687372138
Finished 21 epochs...
Completing Train Step...
At time: 679.9269454479218 and batch: 50, loss is 4.030473642349243 and perplexity is 56.28756510900206
At time: 681.6260216236115 and batch: 100, loss is 4.003385872840881 and perplexity is 54.78332574029606
At time: 683.3410565853119 and batch: 150, loss is 4.000438432693482 and perplexity is 54.622092895405665
At time: 685.0740485191345 and batch: 200, loss is 4.04784704208374 and perplexity is 57.27401566565649
At time: 686.8093311786652 and batch: 250, loss is 4.029954528808593 and perplexity is 56.25835305462628
At time: 688.5470063686371 and batch: 300, loss is 4.015897831916809 and perplexity is 55.47307855114666
At time: 690.291983127594 and batch: 350, loss is 3.9695132541656495 and perplexity is 52.95874711557414
At time: 692.0302495956421 and batch: 400, loss is 3.977545728683472 and perplexity is 53.385849952191464
At time: 693.7695667743683 and batch: 450, loss is 4.017457213401794 and perplexity is 55.55964972395374
At time: 695.5085370540619 and batch: 500, loss is 3.987858328819275 and perplexity is 53.93924544442657
At time: 697.2557713985443 and batch: 550, loss is 4.001791753768921 and perplexity is 54.69606416705633
At time: 698.9940242767334 and batch: 600, loss is 4.0351878356933595 and perplexity is 56.55354201449363
At time: 700.7445962429047 and batch: 650, loss is 4.011326966285705 and perplexity is 55.220097175341245
At time: 702.4981684684753 and batch: 700, loss is 3.978153738975525 and perplexity is 53.41831896816009
At time: 704.2503228187561 and batch: 750, loss is 3.968583731651306 and perplexity is 52.90954363921102
At time: 706.0009167194366 and batch: 800, loss is 3.9183266496658327 and perplexity is 50.31617770078792
At time: 707.7508888244629 and batch: 850, loss is 3.9440139961242675 and perplexity is 51.625410154152824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.339232126871745 and perplexity of 76.64866028881619
Finished 22 epochs...
Completing Train Step...
At time: 712.1615679264069 and batch: 50, loss is 4.028765935897827 and perplexity is 56.19152449885564
At time: 713.870320558548 and batch: 100, loss is 3.999908866882324 and perplexity is 54.593174560231006
At time: 715.5936269760132 and batch: 150, loss is 3.9956024885177612 and perplexity is 54.358581180920936
At time: 717.318038225174 and batch: 200, loss is 4.042755026817321 and perplexity is 56.983116762895094
At time: 719.0419914722443 and batch: 250, loss is 4.024921112060547 and perplexity is 55.97589278462113
At time: 720.7657158374786 and batch: 300, loss is 4.011832327842712 and perplexity is 55.24801034215451
At time: 722.5233089923859 and batch: 350, loss is 3.965523905754089 and perplexity is 52.74789707865837
At time: 724.2526333332062 and batch: 400, loss is 3.9738763046264647 and perplexity is 53.190313602368846
At time: 725.9788150787354 and batch: 450, loss is 4.014229402542115 and perplexity is 55.380602803465855
At time: 727.7240734100342 and batch: 500, loss is 3.9849150562286377 and perplexity is 53.78072094659456
At time: 729.4808053970337 and batch: 550, loss is 3.9996370840072633 and perplexity is 54.57833908639551
At time: 731.2368583679199 and batch: 600, loss is 4.033774709701538 and perplexity is 56.47368117434751
At time: 732.9927456378937 and batch: 650, loss is 4.010565013885498 and perplexity is 55.17803811529454
At time: 734.7484693527222 and batch: 700, loss is 3.978333101272583 and perplexity is 53.427901059862485
At time: 736.504337310791 and batch: 750, loss is 3.969735131263733 and perplexity is 52.970498752363596
At time: 738.2598242759705 and batch: 800, loss is 3.9202830505371096 and perplexity is 50.41471267020249
At time: 740.0153572559357 and batch: 850, loss is 3.9465708684921266 and perplexity is 51.75757863581937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.338853200276692 and perplexity of 76.61962157507551
Finished 23 epochs...
Completing Train Step...
At time: 744.442519903183 and batch: 50, loss is 4.026954865455627 and perplexity is 56.08984978755669
At time: 746.1584711074829 and batch: 100, loss is 3.99738263130188 and perplexity is 54.45543339677536
At time: 747.8791928291321 and batch: 150, loss is 3.9925590181350707 and perplexity is 54.193393947784344
At time: 749.6091854572296 and batch: 200, loss is 4.039607720375061 and perplexity is 56.80405536076768
At time: 751.3629031181335 and batch: 250, loss is 4.0218504571914675 and perplexity is 55.80427376313146
At time: 753.1185295581818 and batch: 300, loss is 4.009236426353454 and perplexity is 55.104777938866675
At time: 754.8714091777802 and batch: 350, loss is 3.9629965162277223 and perplexity is 52.614750923018285
At time: 756.6239187717438 and batch: 400, loss is 3.9715810060501098 and perplexity is 53.0683659579478
At time: 758.3807785511017 and batch: 450, loss is 4.012215628623962 and perplexity is 55.269191006704474
At time: 760.1327483654022 and batch: 500, loss is 3.9831008195877073 and perplexity is 53.68323844697719
At time: 761.884030342102 and batch: 550, loss is 3.9983080339431765 and perplexity is 54.505849922868954
At time: 763.6335332393646 and batch: 600, loss is 4.032895755767822 and perplexity is 56.42406521839204
At time: 765.3839910030365 and batch: 650, loss is 4.010050649642944 and perplexity is 55.149663803505064
At time: 767.1621265411377 and batch: 700, loss is 3.978357949256897 and perplexity is 53.429228652003886
At time: 768.912689447403 and batch: 750, loss is 3.9704477167129517 and perplexity is 53.00825821083495
At time: 770.6641757488251 and batch: 800, loss is 3.921304497718811 and perplexity is 50.466234945537565
At time: 772.4161806106567 and batch: 850, loss is 3.947821173667908 and perplexity is 51.8223318764941
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.338698387145996 and perplexity of 76.6077607697146
Finished 24 epochs...
Completing Train Step...
At time: 776.7983751296997 and batch: 50, loss is 4.025205054283142 and perplexity is 55.991788960721145
At time: 778.5393850803375 and batch: 100, loss is 3.9952762651443483 and perplexity is 54.34085103334567
At time: 780.2602844238281 and batch: 150, loss is 3.990187683105469 and perplexity is 54.06503550492797
At time: 781.9802327156067 and batch: 200, loss is 4.037240886688233 and perplexity is 56.66976858890745
At time: 783.703161239624 and batch: 250, loss is 4.019556579589843 and perplexity is 55.676412294816366
At time: 785.4497623443604 and batch: 300, loss is 4.007243041992187 and perplexity is 54.995042345237295
At time: 787.2001762390137 and batch: 350, loss is 3.9610580015182495 and perplexity is 52.512855249458745
At time: 788.9487457275391 and batch: 400, loss is 3.9698256731033323 and perplexity is 52.975295015893124
At time: 790.696811914444 and batch: 450, loss is 4.010712671279907 and perplexity is 55.18618616217614
At time: 792.445965051651 and batch: 500, loss is 3.981735553741455 and perplexity is 53.60999656370679
At time: 794.1938099861145 and batch: 550, loss is 3.9972727060317994 and perplexity is 54.44944769754769
At time: 795.9439573287964 and batch: 600, loss is 4.032180700302124 and perplexity is 56.38373330365724
At time: 797.6942114830017 and batch: 650, loss is 4.009556741714477 and perplexity is 55.12243167293641
At time: 799.4492266178131 and batch: 700, loss is 3.9782518100738526 and perplexity is 53.42355801826761
At time: 801.2009122371674 and batch: 750, loss is 3.9705941677093506 and perplexity is 53.01602189155281
At time: 802.9508526325226 and batch: 800, loss is 3.9217656898498534 and perplexity is 50.48951494384115
At time: 804.7004270553589 and batch: 850, loss is 3.948489384651184 and perplexity is 51.856971699899276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.338594118754069 and perplexity of 76.59977341811143
Finished 25 epochs...
Completing Train Step...
At time: 809.1045894622803 and batch: 50, loss is 4.023593869209289 and perplexity is 55.90164846210206
At time: 810.8520951271057 and batch: 100, loss is 3.993476986885071 and perplexity is 54.24316463035548
At time: 812.573855638504 and batch: 150, loss is 3.988228554725647 and perplexity is 53.959218847567755
At time: 814.2947778701782 and batch: 200, loss is 4.035307102203369 and perplexity is 56.5602873603173
At time: 816.0175082683563 and batch: 250, loss is 4.017691698074341 and perplexity is 55.57267913776527
At time: 817.7403547763824 and batch: 300, loss is 4.005569987297058 and perplexity is 54.903109557168285
At time: 819.4744827747345 and batch: 350, loss is 3.9594414901733397 and perplexity is 52.42803619714467
At time: 821.2167637348175 and batch: 400, loss is 3.9683689212799074 and perplexity is 52.8981793411196
At time: 822.9717364311218 and batch: 450, loss is 4.009455618858337 and perplexity is 55.11685781703511
At time: 824.7211520671844 and batch: 500, loss is 3.980585436820984 and perplexity is 53.54837424278214
At time: 826.4699485301971 and batch: 550, loss is 3.99635705947876 and perplexity is 54.399614066926326
At time: 828.2210881710052 and batch: 600, loss is 4.031514511108399 and perplexity is 56.34618357882764
At time: 829.9722344875336 and batch: 650, loss is 4.009044275283814 and perplexity is 55.094190514068494
At time: 831.723245382309 and batch: 700, loss is 3.978014922142029 and perplexity is 53.41090412093495
At time: 833.4744710922241 and batch: 750, loss is 3.9706791496276854 and perplexity is 53.02052748623996
At time: 835.2258565425873 and batch: 800, loss is 3.9219871664047243 and perplexity is 50.50069842606186
At time: 836.9768249988556 and batch: 850, loss is 3.9488057565689085 and perplexity is 51.87338038497007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.338539123535156 and perplexity of 76.5955609126385
Finished 26 epochs...
Completing Train Step...
At time: 841.3797249794006 and batch: 50, loss is 4.022073397636413 and perplexity is 55.8167161796615
At time: 843.0952832698822 and batch: 100, loss is 3.991857481002808 and perplexity is 54.155388602245885
At time: 844.822678565979 and batch: 150, loss is 3.986491503715515 and perplexity is 53.86557029166947
At time: 846.5677652359009 and batch: 200, loss is 4.033619337081909 and perplexity is 56.464907392183775
At time: 848.3126938343048 and batch: 250, loss is 4.016053576469421 and perplexity is 55.48171885377018
At time: 850.0607342720032 and batch: 300, loss is 4.0040894842147825 and perplexity is 54.82188547533034
At time: 851.8154821395874 and batch: 350, loss is 3.9580105781555175 and perplexity is 52.35306993793115
At time: 853.5702314376831 and batch: 400, loss is 3.9670832109451295 and perplexity is 52.83021130821385
At time: 855.3499863147736 and batch: 450, loss is 4.008342752456665 and perplexity is 55.05555423547979
At time: 857.1032350063324 and batch: 500, loss is 3.979659242630005 and perplexity is 53.49880101038584
At time: 858.8556046485901 and batch: 550, loss is 3.995490999221802 and perplexity is 54.35252111879804
At time: 860.6111137866974 and batch: 600, loss is 4.030862240791321 and perplexity is 56.30944261961976
At time: 862.3657915592194 and batch: 650, loss is 4.008497109413147 and perplexity is 55.06405309918152
At time: 864.119922876358 and batch: 700, loss is 3.9777078151702883 and perplexity is 53.39450377837112
At time: 865.8753869533539 and batch: 750, loss is 3.9705712842941283 and perplexity is 53.01480871779127
At time: 867.6305170059204 and batch: 800, loss is 3.9220140838623045 and perplexity is 50.50205779476481
At time: 869.386697769165 and batch: 850, loss is 3.948944330215454 and perplexity is 51.880569166525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.338463147481282 and perplexity of 76.5897417052391
Finished 27 epochs...
Completing Train Step...
At time: 873.8221197128296 and batch: 50, loss is 4.020619750022888 and perplexity is 55.73563728773826
At time: 875.5434033870697 and batch: 100, loss is 3.990387258529663 and perplexity is 54.07582663410922
At time: 877.2643666267395 and batch: 150, loss is 3.9849341678619385 and perplexity is 53.781748793833835
At time: 878.9862775802612 and batch: 200, loss is 4.032104473114014 and perplexity is 56.379435494019404
At time: 880.7213535308838 and batch: 250, loss is 4.014594769477844 and perplexity is 55.40084074152257
At time: 882.4664223194122 and batch: 300, loss is 4.002749085426331 and perplexity is 54.74845151285044
At time: 884.2139720916748 and batch: 350, loss is 3.956717600822449 and perplexity is 52.28542234800944
At time: 885.961487531662 and batch: 400, loss is 3.9659165716171265 and perplexity is 52.768613444226766
At time: 887.7109847068787 and batch: 450, loss is 4.00733757019043 and perplexity is 55.0002411732165
At time: 889.4596924781799 and batch: 500, loss is 3.9787129163742065 and perplexity is 53.448197637762476
At time: 891.208619594574 and batch: 550, loss is 3.994682788848877 and perplexity is 54.3086105942961
At time: 892.9577145576477 and batch: 600, loss is 4.030233745574951 and perplexity is 56.27406352325786
At time: 894.7066743373871 and batch: 650, loss is 4.007936568260193 and perplexity is 55.033196080493326
At time: 896.4549722671509 and batch: 700, loss is 3.977333912849426 and perplexity is 53.37454318137641
At time: 898.2039744853973 and batch: 750, loss is 3.970391659736633 and perplexity is 53.00528681144435
At time: 899.9778029918671 and batch: 800, loss is 3.9219287824630737 and perplexity is 50.49775008230044
At time: 901.7254304885864 and batch: 850, loss is 3.9489184999465943 and perplexity is 51.879229094782104
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.338414192199707 and perplexity of 76.58599232464489
Finished 28 epochs...
Completing Train Step...
At time: 906.1384932994843 and batch: 50, loss is 4.019232802391052 and perplexity is 55.65838846001562
At time: 907.8534314632416 and batch: 100, loss is 3.989019651412964 and perplexity is 54.00192269605768
At time: 909.5816411972046 and batch: 150, loss is 3.983496708869934 and perplexity is 53.704495273110766
At time: 911.3209421634674 and batch: 200, loss is 4.030718483924866 and perplexity is 56.301348332415536
At time: 913.0599782466888 and batch: 250, loss is 4.0132432508468625 and perplexity is 55.32601604796166
At time: 914.7994298934937 and batch: 300, loss is 4.001498808860779 and perplexity is 54.680043580252416
At time: 916.5370895862579 and batch: 350, loss is 3.9555140256881716 and perplexity is 52.22253076874869
At time: 918.2839815616608 and batch: 400, loss is 3.9648334121704103 and perplexity is 52.71148756588709
At time: 920.0360889434814 and batch: 450, loss is 4.00638252735138 and perplexity is 54.94773866180244
At time: 921.7832827568054 and batch: 500, loss is 3.977802109718323 and perplexity is 53.399538826357485
At time: 923.5307102203369 and batch: 550, loss is 3.9938859224319456 and perplexity is 54.26535112466172
At time: 925.2776255607605 and batch: 600, loss is 4.029599628448486 and perplexity is 56.238390487438835
At time: 927.0266976356506 and batch: 650, loss is 4.00736825466156 and perplexity is 55.00192885242157
At time: 928.7770013809204 and batch: 700, loss is 3.9769215822219848 and perplexity is 53.352539759151234
At time: 930.5262131690979 and batch: 750, loss is 3.970197010040283 and perplexity is 52.99497035254192
At time: 932.2769486904144 and batch: 800, loss is 3.9217800378799437 and perplexity is 50.49023937411787
At time: 934.0268201828003 and batch: 850, loss is 3.9487933254241945 and perplexity is 51.87273554347976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.338397979736328 and perplexity of 76.58475068711401
Finished 29 epochs...
Completing Train Step...
At time: 938.4088134765625 and batch: 50, loss is 4.017888159751892 and perplexity is 55.58359811207957
At time: 940.1359186172485 and batch: 100, loss is 3.987699074745178 and perplexity is 53.93065608379932
At time: 941.8597888946533 and batch: 150, loss is 3.9821166038513183 and perplexity is 53.6304285513455
At time: 943.6154725551605 and batch: 200, loss is 4.029408597946167 and perplexity is 56.2276482655331
At time: 945.3672804832458 and batch: 250, loss is 4.01196617603302 and perplexity is 55.2554056832724
At time: 947.1214737892151 and batch: 300, loss is 4.000317239761353 and perplexity is 54.61547348492963
At time: 948.8748090267181 and batch: 350, loss is 3.9543682289123536 and perplexity is 52.162728628461174
At time: 950.6297652721405 and batch: 400, loss is 3.9638049602508545 and perplexity is 52.65730420258519
At time: 952.3846535682678 and batch: 450, loss is 4.005493640899658 and perplexity is 54.89891806255235
At time: 954.14040350914 and batch: 500, loss is 3.976930012702942 and perplexity is 53.35298954861767
At time: 955.8955063819885 and batch: 550, loss is 3.9931075191497802 and perplexity is 54.22312723297836
At time: 957.6512789726257 and batch: 600, loss is 4.028978590965271 and perplexity is 56.203475181929356
At time: 959.4053041934967 and batch: 650, loss is 4.006832690238952 and perplexity is 54.972479662826444
At time: 961.1601715087891 and batch: 700, loss is 3.9765028142929078 and perplexity is 53.33020210403852
At time: 962.9137396812439 and batch: 750, loss is 3.969918999671936 and perplexity is 52.98023924910831
At time: 964.6674914360046 and batch: 800, loss is 3.9215505027771 and perplexity is 50.47865142180231
At time: 966.4223098754883 and batch: 850, loss is 3.948617181777954 and perplexity is 51.86359929537032
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3383487065633135 and perplexity of 76.58097720640957
Finished 30 epochs...
Completing Train Step...
At time: 970.8013021945953 and batch: 50, loss is 4.016618223190307 and perplexity is 55.513055270560024
At time: 972.5306258201599 and batch: 100, loss is 3.9864822673797606 and perplexity is 53.86507277347428
At time: 974.2529489994049 and batch: 150, loss is 3.9808506202697753 and perplexity is 53.56257626832884
At time: 975.9739634990692 and batch: 200, loss is 4.028187088966369 and perplexity is 56.159007619381455
At time: 977.7109911441803 and batch: 250, loss is 4.010755934715271 and perplexity is 55.18857375782157
At time: 979.4515552520752 and batch: 300, loss is 3.9991959381103515 and perplexity is 54.5542673860036
At time: 981.1917703151703 and batch: 350, loss is 3.9532853507995607 and perplexity is 52.106273323949395
At time: 982.9412779808044 and batch: 400, loss is 3.962825074195862 and perplexity is 52.60573131640615
At time: 984.6910898685455 and batch: 450, loss is 4.004619274139404 and perplexity is 54.85093725289396
At time: 986.4667267799377 and batch: 500, loss is 3.9760874366760253 and perplexity is 53.308054531901995
At time: 988.2167236804962 and batch: 550, loss is 3.9923487377166746 and perplexity is 54.18199933630452
At time: 989.967159986496 and batch: 600, loss is 4.028354015350342 and perplexity is 56.16838282191339
At time: 991.7170169353485 and batch: 650, loss is 4.006233277320862 and perplexity is 54.93953832209599
At time: 993.4674775600433 and batch: 700, loss is 3.9760341691970824 and perplexity is 53.30521502185717
At time: 995.2181363105774 and batch: 750, loss is 3.9695915460586546 and perplexity is 52.96289351844975
At time: 996.9682233333588 and batch: 800, loss is 3.9212854433059694 and perplexity is 50.46527335022369
At time: 998.7188045978546 and batch: 850, loss is 3.9483853816986083 and perplexity is 51.85157870217955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.338322639465332 and perplexity of 76.57898098859113
Finished 31 epochs...
Completing Train Step...
At time: 1003.1002674102783 and batch: 50, loss is 4.015379147529602 and perplexity is 55.44431299215995
At time: 1004.8434658050537 and batch: 100, loss is 3.9852981472015383 and perplexity is 53.80132780220345
At time: 1006.5834002494812 and batch: 150, loss is 3.9796262979507446 and perplexity is 53.49703853857795
At time: 1008.3231694698334 and batch: 200, loss is 4.02701340675354 and perplexity is 56.093133456277194
At time: 1010.0647065639496 and batch: 250, loss is 4.009600462913514 and perplexity is 55.12484174442817
At time: 1011.8142893314362 and batch: 300, loss is 3.9981249809265136 and perplexity is 54.49587337576118
At time: 1013.5638310909271 and batch: 350, loss is 3.9522449588775634 and perplexity is 52.052090568635606
At time: 1015.314032793045 and batch: 400, loss is 3.9618790483474733 and perplexity is 52.55598846752769
At time: 1017.0638859272003 and batch: 450, loss is 4.003773550987244 and perplexity is 54.80456815581474
At time: 1018.8129105567932 and batch: 500, loss is 3.975261383056641 and perplexity is 53.264037403270756
At time: 1020.5616137981415 and batch: 550, loss is 3.991593389511108 and perplexity is 54.14108851323538
At time: 1022.3109781742096 and batch: 600, loss is 4.02772361278534 and perplexity is 56.13298528783086
At time: 1024.060739517212 and batch: 650, loss is 4.005636649131775 and perplexity is 54.90676962117498
At time: 1025.810920715332 and batch: 700, loss is 3.9755520153045656 and perplexity is 53.27951989994225
At time: 1027.5594685077667 and batch: 750, loss is 3.969232077598572 and perplexity is 52.94385845013346
At time: 1029.3077335357666 and batch: 800, loss is 3.920988163948059 and perplexity is 50.45027329587909
At time: 1031.0827357769012 and batch: 850, loss is 3.948127479553223 and perplexity is 51.83820779305782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.338292121887207 and perplexity of 76.57664401921559
Finished 32 epochs...
Completing Train Step...
At time: 1035.4925656318665 and batch: 50, loss is 4.014173879623413 and perplexity is 55.377527996120676
At time: 1037.2142095565796 and batch: 100, loss is 3.9841507959365843 and perplexity is 53.739634179589224
At time: 1038.9360191822052 and batch: 150, loss is 3.9784444332122804 and perplexity is 53.43384962284783
At time: 1040.655283689499 and batch: 200, loss is 4.025878963470459 and perplexity is 56.02953505900873
At time: 1042.375351190567 and batch: 250, loss is 4.008488807678223 and perplexity is 55.0635959739063
At time: 1044.106793642044 and batch: 300, loss is 3.997091751098633 and perplexity is 54.439595692789936
At time: 1045.850557565689 and batch: 350, loss is 3.951235475540161 and perplexity is 51.99957136362079
At time: 1047.5992636680603 and batch: 400, loss is 3.960960283279419 and perplexity is 52.50772403643844
At time: 1049.3486971855164 and batch: 450, loss is 4.002969980239868 and perplexity is 54.760546497649365
At time: 1051.0972137451172 and batch: 500, loss is 3.9744576978683472 and perplexity is 53.221247082617715
At time: 1052.8450999259949 and batch: 550, loss is 3.9908548736572267 and perplexity is 54.10111922181627
At time: 1054.5928044319153 and batch: 600, loss is 4.027111921310425 and perplexity is 56.09865971866808
At time: 1056.3417506217957 and batch: 650, loss is 4.005033054351807 and perplexity is 54.87363818163458
At time: 1058.0914919376373 and batch: 700, loss is 3.9750602197647096 and perplexity is 53.25332371180202
At time: 1059.8431267738342 and batch: 750, loss is 3.9688676309585573 and perplexity is 52.92456675442228
At time: 1061.5949091911316 and batch: 800, loss is 3.920659523010254 and perplexity is 50.43369599488968
At time: 1063.3471081256866 and batch: 850, loss is 3.947820267677307 and perplexity is 51.822284925969775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.338273048400879 and perplexity of 76.57518344957195
Finished 33 epochs...
Completing Train Step...
At time: 1067.7769162654877 and batch: 50, loss is 4.013002796173096 and perplexity is 55.312714248126085
At time: 1069.5037150382996 and batch: 100, loss is 3.983046875 and perplexity is 53.680342604920575
At time: 1071.229070186615 and batch: 150, loss is 3.977309684753418 and perplexity is 53.37325003348514
At time: 1072.9635081291199 and batch: 200, loss is 4.024792995452881 and perplexity is 55.96872180249726
At time: 1074.7446727752686 and batch: 250, loss is 4.007409787178039 and perplexity is 55.004213268376475
At time: 1076.488706111908 and batch: 300, loss is 3.9960865783691406 and perplexity is 54.384901988709956
At time: 1078.2338016033173 and batch: 350, loss is 3.9502551317214967 and perplexity is 51.94861888481584
At time: 1079.9790647029877 and batch: 400, loss is 3.9600638341903687 and perplexity is 52.46067462690885
At time: 1081.7252118587494 and batch: 450, loss is 4.002153458595276 and perplexity is 54.715851575833085
At time: 1083.4802930355072 and batch: 500, loss is 3.9736628770828246 and perplexity is 53.17896253574946
At time: 1085.2351460456848 and batch: 550, loss is 3.9901188898086546 and perplexity is 54.061316320822144
At time: 1086.9896488189697 and batch: 600, loss is 4.026491541862487 and perplexity is 56.063868056253796
At time: 1088.7435626983643 and batch: 650, loss is 4.004425973892212 and perplexity is 54.840335577850425
At time: 1090.4989516735077 and batch: 700, loss is 3.97455913066864 and perplexity is 53.22664573654005
At time: 1092.2543609142303 and batch: 750, loss is 3.9685613775253294 and perplexity is 52.90836090582668
At time: 1094.0101907253265 and batch: 800, loss is 3.9203333711624144 and perplexity is 50.417249633898884
At time: 1095.767145872116 and batch: 850, loss is 3.9475002574920652 and perplexity is 51.805703920157825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.338279724121094 and perplexity of 76.57569464577836
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 1100.177695274353 and batch: 50, loss is 4.016074414253235 and perplexity is 55.482874981878815
At time: 1101.8910002708435 and batch: 100, loss is 3.9939281368255615 and perplexity is 54.2676419519064
At time: 1103.6196901798248 and batch: 150, loss is 3.9919070386886597 and perplexity is 54.15807248448437
At time: 1105.3586320877075 and batch: 200, loss is 4.042024102210998 and perplexity is 56.94148161863747
At time: 1107.1009294986725 and batch: 250, loss is 4.021660251617432 and perplexity is 55.793660488590675
At time: 1108.8498404026031 and batch: 300, loss is 4.006901106834412 and perplexity is 54.97624082139029
At time: 1110.5994429588318 and batch: 350, loss is 3.957593412399292 and perplexity is 52.33123458471726
At time: 1112.3481810092926 and batch: 400, loss is 3.9667320585250856 and perplexity is 52.81166310847474
At time: 1114.0968663692474 and batch: 450, loss is 4.011010398864746 and perplexity is 55.202619058240536
At time: 1115.845683336258 and batch: 500, loss is 3.980414662361145 and perplexity is 53.539230328891286
At time: 1117.5939180850983 and batch: 550, loss is 3.9931881618499756 and perplexity is 54.22750010868934
At time: 1119.3624629974365 and batch: 600, loss is 4.026217265129089 and perplexity is 56.048493150247545
At time: 1121.1004445552826 and batch: 650, loss is 4.00497766494751 and perplexity is 54.87059884767832
At time: 1122.8394565582275 and batch: 700, loss is 3.972658987045288 and perplexity is 53.12560349284035
At time: 1124.5803744792938 and batch: 750, loss is 3.962794318199158 and perplexity is 52.60411339958762
At time: 1126.326976776123 and batch: 800, loss is 3.9131846809387207 and perplexity is 50.058117525846235
At time: 1128.0768184661865 and batch: 850, loss is 3.940906829833984 and perplexity is 51.46525037039533
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336340268452962 and perplexity of 76.42732340698457
Finished 35 epochs...
Completing Train Step...
At time: 1132.4797549247742 and batch: 50, loss is 4.015339622497558 and perplexity is 55.44212159722006
At time: 1134.2199952602386 and batch: 100, loss is 3.9900316333770753 and perplexity is 54.05659932907044
At time: 1135.93896651268 and batch: 150, loss is 3.9861625146865847 and perplexity is 53.84785202472396
At time: 1137.6604278087616 and batch: 200, loss is 4.0353885269165035 and perplexity is 56.56489295299241
At time: 1139.400283575058 and batch: 250, loss is 4.0154420328140255 and perplexity is 55.447799733183324
At time: 1141.1407101154327 and batch: 300, loss is 4.001994285583496 and perplexity is 54.70714298205059
At time: 1142.8870480060577 and batch: 350, loss is 3.953994174003601 and perplexity is 52.14322055253701
At time: 1144.6377999782562 and batch: 400, loss is 3.9634169054031374 and perplexity is 52.63687424465014
At time: 1146.3881349563599 and batch: 450, loss is 4.007934064865112 and perplexity is 55.03305831083344
At time: 1148.1389064788818 and batch: 500, loss is 3.977767353057861 and perplexity is 53.397682868971316
At time: 1149.890741109848 and batch: 550, loss is 3.991407380104065 and perplexity is 54.13101869803356
At time: 1151.641151189804 and batch: 600, loss is 4.024845113754273 and perplexity is 55.97163887322441
At time: 1153.3909194469452 and batch: 650, loss is 4.004102292060852 and perplexity is 54.8225876300973
At time: 1155.1411666870117 and batch: 700, loss is 3.972973442077637 and perplexity is 53.14231173306266
At time: 1156.890876531601 and batch: 750, loss is 3.9643461561203 and perplexity is 52.685809830984184
At time: 1158.6409394741058 and batch: 800, loss is 3.9156167793273924 and perplexity is 50.180011962367274
At time: 1160.3940150737762 and batch: 850, loss is 3.943635458946228 and perplexity is 51.60587171532401
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.335946083068848 and perplexity of 76.3972028100901
Finished 36 epochs...
Completing Train Step...
At time: 1164.7789597511292 and batch: 50, loss is 4.015375618934631 and perplexity is 55.444117351981134
At time: 1166.5167043209076 and batch: 100, loss is 3.9887173318862916 and perplexity is 53.98559932790961
At time: 1168.2355020046234 and batch: 150, loss is 3.9838823890686035 and perplexity is 53.72521202828146
At time: 1169.958354473114 and batch: 200, loss is 4.032655472755432 and perplexity is 56.41050910274744
At time: 1171.7009737491608 and batch: 250, loss is 4.012896137237549 and perplexity is 55.30681496751327
At time: 1173.4628689289093 and batch: 300, loss is 4.000001134872437 and perplexity is 54.59821199511498
At time: 1175.2134323120117 and batch: 350, loss is 3.952492904663086 and perplexity is 52.064998265258104
At time: 1176.9665236473083 and batch: 400, loss is 3.962068781852722 and perplexity is 52.565961045477614
At time: 1178.7178828716278 and batch: 450, loss is 4.006699676513672 and perplexity is 54.96516805480148
At time: 1180.4697096347809 and batch: 500, loss is 3.9766626167297363 and perplexity is 53.33872508126944
At time: 1182.2209327220917 and batch: 550, loss is 3.9907933378219607 and perplexity is 54.09779016668527
At time: 1183.979777097702 and batch: 600, loss is 4.024514274597168 and perplexity is 55.953124326235255
At time: 1185.7434282302856 and batch: 650, loss is 4.004126343727112 and perplexity is 54.823906220535555
At time: 1187.500602722168 and batch: 700, loss is 3.9734892511367796 and perplexity is 53.169730089588796
At time: 1189.2541360855103 and batch: 750, loss is 3.9653675508499147 and perplexity is 52.73965033099175
At time: 1191.0070967674255 and batch: 800, loss is 3.9169759941101074 and perplexity is 50.24826375033519
At time: 1192.7596552371979 and batch: 850, loss is 3.9450894737243654 and perplexity is 51.68096199339434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3358198801676435 and perplexity of 76.3875618698217
Finished 37 epochs...
Completing Train Step...
At time: 1197.1662456989288 and batch: 50, loss is 4.015279774665832 and perplexity is 55.43880360574447
At time: 1198.8808736801147 and batch: 100, loss is 3.9879242372512818 and perplexity is 53.942800612673935
At time: 1200.598794221878 and batch: 150, loss is 3.982615132331848 and perplexity is 53.6571715129115
At time: 1202.3187148571014 and batch: 200, loss is 4.03119577884674 and perplexity is 56.32822709410725
At time: 1204.0385179519653 and batch: 250, loss is 4.011584019660949 and perplexity is 55.23429351223146
At time: 1205.7704901695251 and batch: 300, loss is 3.9989381074905395 and perplexity is 54.54020343856613
At time: 1207.54136967659 and batch: 350, loss is 3.95162486076355 and perplexity is 52.01982317095372
At time: 1209.282783985138 and batch: 400, loss is 3.9613198471069335 and perplexity is 52.52660730933455
At time: 1211.028267621994 and batch: 450, loss is 4.006016874313355 and perplexity is 54.92765052709571
At time: 1212.7726516723633 and batch: 500, loss is 3.976055927276611 and perplexity is 53.306374853582724
At time: 1214.5153532028198 and batch: 550, loss is 3.990493540763855 and perplexity is 54.08157423920841
At time: 1216.2718651294708 and batch: 600, loss is 4.024417276382446 and perplexity is 55.94769723628079
At time: 1218.0236885547638 and batch: 650, loss is 4.004266328811646 and perplexity is 54.83158128686718
At time: 1219.777719259262 and batch: 700, loss is 3.9738646125793458 and perplexity is 53.18969170235158
At time: 1221.5264978408813 and batch: 750, loss is 3.9660268068313598 and perplexity is 52.77443072426325
At time: 1223.2740182876587 and batch: 800, loss is 3.917784237861633 and perplexity is 50.28889301249827
At time: 1225.0207359790802 and batch: 850, loss is 3.945924620628357 and perplexity is 51.724141216786116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3357648849487305 and perplexity of 76.38336103464842
Finished 38 epochs...
Completing Train Step...
At time: 1229.488460302353 and batch: 50, loss is 4.015089263916016 and perplexity is 55.42824292369427
At time: 1231.2048189640045 and batch: 100, loss is 3.987331509590149 and perplexity is 53.91083669651628
At time: 1232.9214594364166 and batch: 150, loss is 3.9817757749557496 and perplexity is 53.61215286623118
At time: 1234.6436500549316 and batch: 200, loss is 4.03027503490448 and perplexity is 56.27638708957951
At time: 1236.3764610290527 and batch: 250, loss is 4.010767908096313 and perplexity is 55.189234555600336
At time: 1238.1270644664764 and batch: 300, loss is 3.9982559490203857 and perplexity is 54.50301106381559
At time: 1239.8745102882385 and batch: 350, loss is 3.9510435247421265 and perplexity is 51.98959096230373
At time: 1241.6201348304749 and batch: 400, loss is 3.9608173751831055 and perplexity is 52.50022079370447
At time: 1243.36692070961 and batch: 450, loss is 4.005569925308228 and perplexity is 54.90310615378886
At time: 1245.113275051117 and batch: 500, loss is 3.9756626796722414 and perplexity is 53.28541637058063
At time: 1246.8647804260254 and batch: 550, loss is 3.9903044939041137 and perplexity is 54.071351253770715
At time: 1248.6182177066803 and batch: 600, loss is 4.024374237060547 and perplexity is 55.94528933714746
At time: 1250.3973133563995 and batch: 650, loss is 4.004384722709656 and perplexity is 54.838073395815236
At time: 1252.1508004665375 and batch: 700, loss is 3.9741192054748535 and perplexity is 53.2032351439321
At time: 1253.9052815437317 and batch: 750, loss is 3.966460633277893 and perplexity is 52.79733063494644
At time: 1255.6547532081604 and batch: 800, loss is 3.918302698135376 and perplexity is 50.31497256575772
At time: 1257.4026708602905 and batch: 850, loss is 3.946442360877991 and perplexity is 51.75092782022481
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.335735003153483 and perplexity of 76.38107859679553
Finished 39 epochs...
Completing Train Step...
At time: 1261.8293719291687 and batch: 50, loss is 4.014861660003662 and perplexity is 55.415628674330655
At time: 1263.5486006736755 and batch: 100, loss is 3.986853880882263 and perplexity is 53.885093481582516
At time: 1265.2697191238403 and batch: 150, loss is 3.9811541843414306 and perplexity is 53.57883841024533
At time: 1266.995943069458 and batch: 200, loss is 4.029620885848999 and perplexity is 56.23958598213613
At time: 1268.7169151306152 and batch: 250, loss is 4.010188889503479 and perplexity is 55.15728821232426
At time: 1270.4373457431793 and batch: 300, loss is 3.9977600526809693 and perplexity is 54.47598992053864
At time: 1272.1636095046997 and batch: 350, loss is 3.95060564994812 and perplexity is 51.966831014238075
At time: 1273.8927702903748 and batch: 400, loss is 3.9604396772384645 and perplexity is 52.48039531247493
At time: 1275.6358132362366 and batch: 450, loss is 4.005244455337524 and perplexity is 54.885239749084285
At time: 1277.3764655590057 and batch: 500, loss is 3.975376958847046 and perplexity is 53.270193792251725
At time: 1279.1214973926544 and batch: 550, loss is 3.990159749984741 and perplexity is 54.06352532085624
At time: 1280.8726875782013 and batch: 600, loss is 4.0243378305435185 and perplexity is 55.94325260109399
At time: 1282.6258776187897 and batch: 650, loss is 4.004462699890137 and perplexity is 54.84234968088584
At time: 1284.3749976158142 and batch: 700, loss is 3.9742900133132935 and perplexity is 53.21232344967988
At time: 1286.123591184616 and batch: 750, loss is 3.9667554330825805 and perplexity is 52.81289757215794
At time: 1287.8757846355438 and batch: 800, loss is 3.9186558389663695 and perplexity is 50.33274397470139
At time: 1289.6288015842438 and batch: 850, loss is 3.946783103942871 and perplexity is 51.76856459461431
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.335716565450032 and perplexity of 76.37967031810184
Finished 40 epochs...
Completing Train Step...
At time: 1294.009267091751 and batch: 50, loss is 4.014624605178833 and perplexity is 55.40249368909978
At time: 1295.7536313533783 and batch: 100, loss is 3.986452827453613 and perplexity is 53.86348701305235
At time: 1297.4757311344147 and batch: 150, loss is 3.980658521652222 and perplexity is 53.55228795969167
At time: 1299.1980636119843 and batch: 200, loss is 4.029114789962769 and perplexity is 56.21113056022063
At time: 1300.9242498874664 and batch: 250, loss is 4.009738411903381 and perplexity is 55.13244668519832
At time: 1302.6769723892212 and batch: 300, loss is 3.997367434501648 and perplexity is 54.4546058547205
At time: 1304.4293830394745 and batch: 350, loss is 3.9502489519119264 and perplexity is 51.94829785323566
At time: 1306.1822752952576 and batch: 400, loss is 3.960132846832275 and perplexity is 52.46429520159232
At time: 1307.9347248077393 and batch: 450, loss is 4.004987583160401 and perplexity is 54.87114306865799
At time: 1309.6913301944733 and batch: 500, loss is 3.975150189399719 and perplexity is 53.25811510943634
At time: 1311.4437339305878 and batch: 550, loss is 3.990034375190735 and perplexity is 54.05674754239606
At time: 1313.1964194774628 and batch: 600, loss is 4.0242957544326785 and perplexity is 55.940898776117045
At time: 1314.9532573223114 and batch: 650, loss is 4.0045056867599484 and perplexity is 54.84470723250323
At time: 1316.705222606659 and batch: 700, loss is 3.974404420852661 and perplexity is 53.21841168893338
At time: 1318.4564654827118 and batch: 750, loss is 3.9669613647460937 and perplexity is 52.82377453992752
At time: 1320.2092378139496 and batch: 800, loss is 3.9189071035385132 and perplexity is 50.345392399064934
At time: 1321.9619140625 and batch: 850, loss is 3.9470168209075926 and perplexity is 51.7806652004027
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.335703214009603 and perplexity of 76.37865054629128
Finished 41 epochs...
Completing Train Step...
At time: 1326.3446927070618 and batch: 50, loss is 4.0143890285491945 and perplexity is 55.389443693560274
At time: 1328.075500011444 and batch: 100, loss is 3.9861048650741577 and perplexity is 53.844747806404065
At time: 1329.810426235199 and batch: 150, loss is 3.980242176055908 and perplexity is 53.52999634125772
At time: 1331.5484511852264 and batch: 200, loss is 4.028698291778564 and perplexity is 56.187723601226544
At time: 1333.2856514453888 and batch: 250, loss is 4.009364376068115 and perplexity is 55.11182903056336
At time: 1335.0291335582733 and batch: 300, loss is 3.997037672996521 and perplexity is 54.436651782376394
At time: 1336.770652770996 and batch: 350, loss is 3.9499431610107423 and perplexity is 51.93241496496526
At time: 1338.534654378891 and batch: 400, loss is 3.959869637489319 and perplexity is 52.45048792610532
At time: 1340.2854626178741 and batch: 450, loss is 4.00477303981781 and perplexity is 54.85937209294964
At time: 1342.033397436142 and batch: 500, loss is 3.974958448410034 and perplexity is 53.24790432468075
At time: 1343.7911710739136 and batch: 550, loss is 3.989918155670166 and perplexity is 54.05046545817064
At time: 1345.539939880371 and batch: 600, loss is 4.024245982170105 and perplexity is 55.938114540303985
At time: 1347.2862577438354 and batch: 650, loss is 4.004522371292114 and perplexity is 54.845622298418874
At time: 1349.032042980194 and batch: 700, loss is 3.974479818344116 and perplexity is 53.222424374945284
At time: 1350.7787628173828 and batch: 750, loss is 3.967108254432678 and perplexity is 52.83153437752
At time: 1352.5254607200623 and batch: 800, loss is 3.919091911315918 and perplexity is 50.35469747893587
At time: 1354.2726712226868 and batch: 850, loss is 3.9471825218200682 and perplexity is 51.78924601477975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.335692723592122 and perplexity of 76.37784930656314
Finished 42 epochs...
Completing Train Step...
At time: 1358.659239768982 and batch: 50, loss is 4.0141585302352905 and perplexity is 55.37667799147375
At time: 1360.3967382907867 and batch: 100, loss is 3.9857946968078615 and perplexity is 53.828049464109604
At time: 1362.1289427280426 and batch: 150, loss is 3.9798793363571168 and perplexity is 53.51057705676567
At time: 1363.867504119873 and batch: 200, loss is 4.028340487480164 and perplexity is 56.16762298846196
At time: 1365.6035702228546 and batch: 250, loss is 4.009039435386658 and perplexity is 55.09392386449779
At time: 1367.34091258049 and batch: 300, loss is 3.996748728752136 and perplexity is 54.42092489736617
At time: 1369.0788371562958 and batch: 350, loss is 3.9496714162826536 and perplexity is 51.91830452228765
At time: 1370.81724858284 and batch: 400, loss is 3.9596358156204223 and perplexity is 52.43822528868616
At time: 1372.5550515651703 and batch: 450, loss is 4.00458571434021 and perplexity is 54.84909649734197
At time: 1374.2990369796753 and batch: 500, loss is 3.9747883462905884 and perplexity is 53.238847513612306
At time: 1376.043211221695 and batch: 550, loss is 3.9898069715499878 and perplexity is 54.044456238794524
At time: 1377.7862675189972 and batch: 600, loss is 4.024189004898071 and perplexity is 55.93492742993205
At time: 1379.5319862365723 and batch: 650, loss is 4.004519882202149 and perplexity is 54.845485782900674
At time: 1381.2787749767303 and batch: 700, loss is 3.974527359008789 and perplexity is 53.22495466452092
At time: 1383.0597488880157 and batch: 750, loss is 3.9672139787673952 and perplexity is 52.837120251620355
At time: 1384.8100464344025 and batch: 800, loss is 3.919230718612671 and perplexity is 50.36168756349787
At time: 1386.564819097519 and batch: 850, loss is 3.9473023223876953 and perplexity is 51.79545076750833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.335685094197591 and perplexity of 76.3772665920402
Finished 43 epochs...
Completing Train Step...
At time: 1390.9632160663605 and batch: 50, loss is 4.013933873176574 and perplexity is 55.36423862722253
At time: 1392.6692605018616 and batch: 100, loss is 3.9855124807357787 and perplexity is 53.81286046681235
At time: 1394.3907942771912 and batch: 150, loss is 3.979553952217102 and perplexity is 53.49316839607301
At time: 1396.1282591819763 and batch: 200, loss is 4.0280226135253905 and perplexity is 56.14977160140821
At time: 1397.8706798553467 and batch: 250, loss is 4.008747816085815 and perplexity is 55.077859755355746
At time: 1399.6211473941803 and batch: 300, loss is 3.996487979888916 and perplexity is 54.40673655294152
At time: 1401.3654663562775 and batch: 350, loss is 3.9494240140914916 and perplexity is 51.90546140876014
At time: 1403.116140127182 and batch: 400, loss is 3.9594223499298096 and perplexity is 52.42703272136747
At time: 1404.8704841136932 and batch: 450, loss is 4.004416851997376 and perplexity is 54.83983533235812
At time: 1406.6236221790314 and batch: 500, loss is 3.9746327877044676 and perplexity is 53.23056639788242
At time: 1408.378517627716 and batch: 550, loss is 3.9896983909606933 and perplexity is 54.038588378461654
At time: 1410.1320917606354 and batch: 600, loss is 4.0241259622573855 and perplexity is 55.93140125555116
At time: 1411.890212059021 and batch: 650, loss is 4.004502902030945 and perplexity is 54.84455450506895
At time: 1413.6462981700897 and batch: 700, loss is 3.974554696083069 and perplexity is 53.226409698948224
At time: 1415.3997373580933 and batch: 750, loss is 3.9672900676727294 and perplexity is 52.841140723216014
At time: 1417.152316570282 and batch: 800, loss is 3.91933660030365 and perplexity is 50.367020226448346
At time: 1418.9056725502014 and batch: 850, loss is 3.9473897552490236 and perplexity is 51.799979589953786
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.335675239562988 and perplexity of 76.37651392569462
Finished 44 epochs...
Completing Train Step...
At time: 1423.3067610263824 and batch: 50, loss is 4.0137155866622924 and perplexity is 55.35215467948612
At time: 1425.0079724788666 and batch: 100, loss is 3.9852510023117067 and perplexity is 53.798791404320994
At time: 1426.750943660736 and batch: 150, loss is 3.9792567110061645 and perplexity is 53.47727038481118
At time: 1428.4885087013245 and batch: 200, loss is 4.027733778953552 and perplexity is 56.133555948102256
At time: 1430.2267417907715 and batch: 250, loss is 4.008479943275452 and perplexity is 55.06310787017695
At time: 1431.96812915802 and batch: 300, loss is 3.99624728679657 and perplexity is 54.393642803126916
At time: 1433.7094373703003 and batch: 350, loss is 3.9491943550109863 and perplexity is 51.89354221694751
At time: 1435.4468748569489 and batch: 400, loss is 3.9592239570617678 and perplexity is 52.41663260367165
At time: 1437.184579372406 and batch: 450, loss is 4.004260954856872 and perplexity is 54.831286625220926
At time: 1438.9235203266144 and batch: 500, loss is 3.9744868993759157 and perplexity is 53.22280124595905
At time: 1440.663497686386 and batch: 550, loss is 3.9895916414260864 and perplexity is 54.03282009218785
At time: 1442.409286737442 and batch: 600, loss is 4.024058265686035 and perplexity is 55.92761501961437
At time: 1444.1581237316132 and batch: 650, loss is 4.004475445747375 and perplexity is 54.84304869810025
At time: 1445.9069230556488 and batch: 700, loss is 3.9745669555664063 and perplexity is 53.22706223123089
At time: 1447.6558141708374 and batch: 750, loss is 3.9673439407348634 and perplexity is 52.843987513955405
At time: 1449.4114353656769 and batch: 800, loss is 3.9194178581237793 and perplexity is 50.371113107005385
At time: 1451.160721540451 and batch: 850, loss is 3.9474537849426268 and perplexity is 51.80329643296267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.335667610168457 and perplexity of 76.3759312213598
Finished 45 epochs...
Completing Train Step...
At time: 1455.5831949710846 and batch: 50, loss is 4.0135028409957885 and perplexity is 55.34038000099672
At time: 1457.2922413349152 and batch: 100, loss is 3.985005702972412 and perplexity is 53.78559621478646
At time: 1459.0094830989838 and batch: 150, loss is 3.978980460166931 and perplexity is 53.46249928434586
At time: 1460.7470092773438 and batch: 200, loss is 4.027466397285462 and perplexity is 56.118548870675134
At time: 1462.4874839782715 and batch: 250, loss is 4.00822998046875 and perplexity is 55.04934586125482
At time: 1464.233689069748 and batch: 300, loss is 3.996021718978882 and perplexity is 54.38137473151681
At time: 1465.9890894889832 and batch: 350, loss is 3.948978290557861 and perplexity is 51.88233107833556
At time: 1467.7417957782745 and batch: 400, loss is 3.9590368175506594 and perplexity is 52.4068242984616
At time: 1469.4995305538177 and batch: 450, loss is 4.0041143369674685 and perplexity is 54.8232479670226
At time: 1471.2750284671783 and batch: 500, loss is 3.9743478536605834 and perplexity is 53.2154013579611
At time: 1473.0249242782593 and batch: 550, loss is 3.989485764503479 and perplexity is 54.027099566317865
At time: 1474.7759418487549 and batch: 600, loss is 4.023986644744873 and perplexity is 55.923609574628315
At time: 1476.5269644260406 and batch: 650, loss is 4.004439973831177 and perplexity is 54.84110334457569
At time: 1478.27707862854 and batch: 700, loss is 3.9745674467086793 and perplexity is 53.22708837329764
At time: 1480.024974346161 and batch: 750, loss is 3.9673806381225587 and perplexity is 52.84592678583546
At time: 1481.7735583782196 and batch: 800, loss is 3.9194804430007935 and perplexity is 50.37426567557478
At time: 1483.521464586258 and batch: 850, loss is 3.9475005054473877 and perplexity is 51.80571676565944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.335659662882487 and perplexity of 76.37532424240511
Finished 46 epochs...
Completing Train Step...
At time: 1487.9059491157532 and batch: 50, loss is 4.0132954931259155 and perplexity is 55.328906480631666
At time: 1489.6391851902008 and batch: 100, loss is 3.9847730112075808 and perplexity is 53.77308220549066
At time: 1491.348380804062 and batch: 150, loss is 3.978720631599426 and perplexity is 53.44861000423558
At time: 1493.0587224960327 and batch: 200, loss is 4.027215766906738 and perplexity is 56.10448561993079
At time: 1494.7767021656036 and batch: 250, loss is 4.007993636131286 and perplexity is 55.03633679744928
At time: 1496.5026814937592 and batch: 300, loss is 3.99580756187439 and perplexity is 54.36972982073069
At time: 1498.2414419651031 and batch: 350, loss is 3.9487728309631347 and perplexity is 51.8716724506149
At time: 1499.979877948761 and batch: 400, loss is 3.958858413696289 and perplexity is 52.397475552962355
At time: 1501.7198138237 and batch: 450, loss is 4.003974599838257 and perplexity is 54.815587658964816
At time: 1503.4587049484253 and batch: 500, loss is 3.9742135572433472 and perplexity is 53.20825520007938
At time: 1505.2058668136597 and batch: 550, loss is 3.989380717277527 and perplexity is 54.0214244674641
At time: 1506.9544270038605 and batch: 600, loss is 4.023911581039429 and perplexity is 55.91941189882035
At time: 1508.7068314552307 and batch: 650, loss is 4.0043982934951785 and perplexity is 54.83881759659746
At time: 1510.4555547237396 and batch: 700, loss is 3.9745586776733397 and perplexity is 53.226621625125134
At time: 1512.2037370204926 and batch: 750, loss is 3.967404146194458 and perplexity is 52.84716910628415
At time: 1513.979387998581 and batch: 800, loss is 3.9195284414291383 and perplexity is 50.37668361918452
At time: 1515.7323460578918 and batch: 850, loss is 3.947535529136658 and perplexity is 51.8075312247602
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.335653940836589 and perplexity of 76.37488722054462
Finished 47 epochs...
Completing Train Step...
At time: 1520.1063215732574 and batch: 50, loss is 4.013094592094421 and perplexity is 55.31779196274469
At time: 1521.839806318283 and batch: 100, loss is 3.9845508527755737 and perplexity is 53.761137388733715
At time: 1523.5672917366028 and batch: 150, loss is 3.9784730863571167 and perplexity is 53.43538069261511
At time: 1525.2910890579224 and batch: 200, loss is 4.026977553367614 and perplexity is 56.09112236356805
At time: 1527.0158026218414 and batch: 250, loss is 4.007767119407654 and perplexity is 55.02387155860303
At time: 1528.7511870861053 and batch: 300, loss is 3.995601468086243 and perplexity is 54.3585257117397
At time: 1530.502718448639 and batch: 350, loss is 3.948575539588928 and perplexity is 51.86143962653188
At time: 1532.2534046173096 and batch: 400, loss is 3.9586866426467897 and perplexity is 52.388475956552696
At time: 1534.0033872127533 and batch: 450, loss is 4.003839921951294 and perplexity is 54.8082057085502
At time: 1535.7539224624634 and batch: 500, loss is 3.974082102775574 and perplexity is 53.201261196917464
At time: 1537.5058906078339 and batch: 550, loss is 3.9892755651474 and perplexity is 54.01574429825505
At time: 1539.2561602592468 and batch: 600, loss is 4.023833260536194 and perplexity is 55.915032433843024
At time: 1541.0118160247803 and batch: 650, loss is 4.004350872039795 and perplexity is 54.83621712171514
At time: 1542.7659068107605 and batch: 700, loss is 3.9745423078536986 and perplexity is 53.225750322060584
At time: 1544.5177431106567 and batch: 750, loss is 3.9674173688888548 and perplexity is 52.84786789287088
At time: 1546.2698471546173 and batch: 800, loss is 3.919566020965576 and perplexity is 50.37857678717417
At time: 1548.0261535644531 and batch: 850, loss is 3.947563762664795 and perplexity is 51.80899395479967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.33564821879069 and perplexity of 76.37445020118477
Finished 48 epochs...
Completing Train Step...
At time: 1552.396367073059 and batch: 50, loss is 4.012901225090027 and perplexity is 55.307096361144694
At time: 1554.1174938678741 and batch: 100, loss is 3.9843373012542727 and perplexity is 53.74965784183845
At time: 1555.8261287212372 and batch: 150, loss is 3.978233451843262 and perplexity is 53.422577265273
At time: 1557.5706100463867 and batch: 200, loss is 4.026747193336487 and perplexity is 56.07820269901868
At time: 1559.289577960968 and batch: 250, loss is 4.007546453475952 and perplexity is 55.011731004272356
At time: 1561.0079481601715 and batch: 300, loss is 3.9953999805450437 and perplexity is 54.3475742493795
At time: 1562.7266488075256 and batch: 350, loss is 3.9483836841583253 and perplexity is 51.85149068211067
At time: 1564.4614536762238 and batch: 400, loss is 3.9585187435150146 and perplexity is 52.37968071530192
At time: 1566.1987135410309 and batch: 450, loss is 4.003707766532898 and perplexity is 54.800962985786306
At time: 1567.9387037754059 and batch: 500, loss is 3.973951268196106 and perplexity is 53.194301087603
At time: 1569.6859149932861 and batch: 550, loss is 3.98916974067688 and perplexity is 54.01002841316052
At time: 1571.432671546936 and batch: 600, loss is 4.023750953674316 and perplexity is 55.910430432382334
At time: 1573.1799285411835 and batch: 650, loss is 4.004298152923584 and perplexity is 54.83332628101412
At time: 1574.9272561073303 and batch: 700, loss is 3.9745199918746947 and perplexity is 53.22456255058712
At time: 1576.6730089187622 and batch: 750, loss is 3.967423219680786 and perplexity is 52.84817709565448
At time: 1578.4219951629639 and batch: 800, loss is 3.91959716796875 and perplexity is 50.38014595330254
At time: 1580.1672792434692 and batch: 850, loss is 3.9475767040252685 and perplexity is 51.80966443800468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.335639953613281 and perplexity of 76.37381895541301
Finished 49 epochs...
Completing Train Step...
At time: 1584.6024527549744 and batch: 50, loss is 4.012696642875671 and perplexity is 55.29578267023096
At time: 1586.3119945526123 and batch: 100, loss is 3.9841293716430664 and perplexity is 53.738482858226185
At time: 1588.028885602951 and batch: 150, loss is 3.978010516166687 and perplexity is 53.41066879432683
At time: 1589.7505185604095 and batch: 200, loss is 4.026532235145569 and perplexity is 56.066149525527194
At time: 1591.4736487865448 and batch: 250, loss is 4.007340631484985 and perplexity is 55.00040954541308
At time: 1593.21040225029 and batch: 300, loss is 3.9952134609222414 and perplexity is 54.33743830563587
At time: 1594.952140569687 and batch: 350, loss is 3.9482018423080443 and perplexity is 51.84206276832583
At time: 1596.697677373886 and batch: 400, loss is 3.9583600091934206 and perplexity is 52.37136692207788
At time: 1598.4443635940552 and batch: 450, loss is 4.003582148551941 and perplexity is 54.794079431819675
At time: 1600.1907663345337 and batch: 500, loss is 3.9738288640975954 and perplexity is 53.18779028561504
At time: 1601.9619085788727 and batch: 550, loss is 3.9890660190582277 and perplexity is 54.00442669610466
At time: 1603.7082650661469 and batch: 600, loss is 4.02367178440094 and perplexity is 55.9060042194432
At time: 1605.45836186409 and batch: 650, loss is 4.004246654510498 and perplexity is 54.83050252443654
At time: 1607.2064638137817 and batch: 700, loss is 3.9744928884506225 and perplexity is 53.22312000224635
At time: 1608.9541747570038 and batch: 750, loss is 3.967416787147522 and perplexity is 52.847837149090715
At time: 1610.7036657333374 and batch: 800, loss is 3.919611358642578 and perplexity is 50.38086088659385
At time: 1612.4514570236206 and batch: 850, loss is 3.9475759744644163 and perplexity is 51.80962663971553
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.335633277893066 and perplexity of 76.37330910686774
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f37e976d898>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 2.0, 'batch_size': 50, 'dropout': 0.0, 'wordvec_source': 'glove', 'data': 'wikitext', 'lr': 4.333155033503922, 'num_layers': 1, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.0553910732269287 and batch: 50, loss is 7.438965187072754 and perplexity is 1700.9891045802885
At time: 3.634856939315796 and batch: 100, loss is 6.6140084552764895 and perplexity is 745.4652016711303
At time: 5.215991735458374 and batch: 150, loss is 6.280209865570068 and perplexity is 533.9006994435891
At time: 6.797231674194336 and batch: 200, loss is 6.11884485244751 and perplexity is 454.339562022101
At time: 8.378179550170898 and batch: 250, loss is 5.998042516708374 and perplexity is 402.63986078332607
At time: 9.959834814071655 and batch: 300, loss is 5.772395477294922 and perplexity is 321.30649391554465
At time: 11.542638778686523 and batch: 350, loss is 5.6489323806762695 and perplexity is 283.9881127230655
At time: 13.12646198272705 and batch: 400, loss is 5.5603125953674315 and perplexity is 259.9040684336676
At time: 14.711050748825073 and batch: 450, loss is 5.477505044937134 and perplexity is 239.24904649592384
At time: 16.33842444419861 and batch: 500, loss is 5.4115735912323 and perplexity is 223.9837694085528
At time: 17.923869609832764 and batch: 550, loss is 5.34606330871582 and perplexity is 209.78082783176933
At time: 19.52549958229065 and batch: 600, loss is 5.339005289077758 and perplexity is 208.30540354098213
At time: 21.12749934196472 and batch: 650, loss is 5.316941604614258 and perplexity is 203.75975017879034
At time: 22.73145866394043 and batch: 700, loss is 5.23636531829834 and perplexity is 187.98559138023546
At time: 24.37339949607849 and batch: 750, loss is 5.1932602214813235 and perplexity is 180.05461499027243
At time: 26.11493492126465 and batch: 800, loss is 5.171867065429687 and perplexity is 176.2435888003092
At time: 27.874241590499878 and batch: 850, loss is 5.138870439529419 and perplexity is 170.52304340543714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.873110771179199 and perplexity of 130.72694665419564
Finished 1 epochs...
Completing Train Step...
At time: 32.36909246444702 and batch: 50, loss is 5.125462341308594 and perplexity is 168.2519134817478
At time: 34.12639546394348 and batch: 100, loss is 5.0353317546844485 and perplexity is 153.7505916308033
At time: 35.87798595428467 and batch: 150, loss is 5.017796306610108 and perplexity is 151.0780070814894
At time: 37.629830837249756 and batch: 200, loss is 5.023404588699341 and perplexity is 151.92767552580216
At time: 39.407405853271484 and batch: 250, loss is 5.026395092010498 and perplexity is 152.3826957731887
At time: 41.15881323814392 and batch: 300, loss is 4.963911046981812 and perplexity is 143.15257893313523
At time: 42.91711783409119 and batch: 350, loss is 4.909031524658203 and perplexity is 135.50811457082503
At time: 44.67154908180237 and batch: 400, loss is 4.8984384346008305 and perplexity is 134.08024105823176
At time: 46.42497944831848 and batch: 450, loss is 4.892438716888428 and perplexity is 133.27820585934364
At time: 48.179121255874634 and batch: 500, loss is 4.873236789703369 and perplexity is 130.74342170914147
At time: 49.93248677253723 and batch: 550, loss is 4.86299126625061 and perplexity is 129.41072565787255
At time: 51.6885871887207 and batch: 600, loss is 4.881210880279541 and perplexity is 131.7901494003578
At time: 53.445401668548584 and batch: 650, loss is 4.869263124465943 and perplexity is 130.22492197372054
At time: 55.202993631362915 and batch: 700, loss is 4.812439556121826 and perplexity is 123.03139369785451
At time: 56.95981025695801 and batch: 750, loss is 4.797224683761597 and perplexity is 121.17365521096649
At time: 58.72480845451355 and batch: 800, loss is 4.773532962799072 and perplexity is 118.33658304698055
At time: 60.480117321014404 and batch: 850, loss is 4.764894437789917 and perplexity is 117.31873220946592
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.6329240798950195 and perplexity of 102.81426211188878
Finished 2 epochs...
Completing Train Step...
At time: 65.01270699501038 and batch: 50, loss is 4.760480632781983 and perplexity is 116.80205130491443
At time: 66.76316714286804 and batch: 100, loss is 4.690991201400757 and perplexity is 108.96112872243768
At time: 68.51333737373352 and batch: 150, loss is 4.691455011367798 and perplexity is 109.01167770161318
At time: 70.26372599601746 and batch: 200, loss is 4.718491067886353 and perplexity is 111.99912602163093
At time: 72.0158896446228 and batch: 250, loss is 4.723002490997314 and perplexity is 112.5055429387418
At time: 73.8270788192749 and batch: 300, loss is 4.6845998954772945 and perplexity is 108.26694554657246
At time: 75.57891035079956 and batch: 350, loss is 4.63551441192627 and perplexity is 103.08093041892727
At time: 77.32897591590881 and batch: 400, loss is 4.639906797409058 and perplexity is 103.53469743152542
At time: 79.08010220527649 and batch: 450, loss is 4.6511219882965085 and perplexity is 104.702394560358
At time: 80.83277678489685 and batch: 500, loss is 4.633446159362793 and perplexity is 102.86795334145646
At time: 82.58224654197693 and batch: 550, loss is 4.639227457046509 and perplexity is 103.46438601803057
At time: 84.33284831047058 and batch: 600, loss is 4.664759712219238 and perplexity is 106.14007799376927
At time: 86.08288931846619 and batch: 650, loss is 4.652419605255127 and perplexity is 104.83834635074547
At time: 87.83337378501892 and batch: 700, loss is 4.604721212387085 and perplexity is 99.9551127172559
At time: 89.5850441455841 and batch: 750, loss is 4.596648817062378 and perplexity is 99.15148350297332
At time: 91.33916234970093 and batch: 800, loss is 4.569498338699341 and perplexity is 96.49568947547517
At time: 93.09266662597656 and batch: 850, loss is 4.573975820541381 and perplexity is 96.9287158834393
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.539318402608235 and perplexity of 93.6269624727384
Finished 3 epochs...
Completing Train Step...
At time: 97.56671023368835 and batch: 50, loss is 4.564430866241455 and perplexity is 96.00793710636214
At time: 99.3465678691864 and batch: 100, loss is 4.507826671600342 and perplexity is 90.72443011281197
At time: 101.10180282592773 and batch: 150, loss is 4.510369729995728 and perplexity is 90.95544124938354
At time: 102.85471391677856 and batch: 200, loss is 4.54874397277832 and perplexity is 94.51362204903565
At time: 104.61781358718872 and batch: 250, loss is 4.547470483779907 and perplexity is 94.39333659851133
At time: 106.37103700637817 and batch: 300, loss is 4.516623620986938 and perplexity is 91.5260490637901
At time: 108.12043762207031 and batch: 350, loss is 4.47006742477417 and perplexity is 87.36261321933307
At time: 109.87112760543823 and batch: 400, loss is 4.483304777145386 and perplexity is 88.52675096634147
At time: 111.6229248046875 and batch: 450, loss is 4.503612098693847 and perplexity is 90.34287000893158
At time: 113.37841939926147 and batch: 500, loss is 4.484186000823975 and perplexity is 88.60479721853369
At time: 115.12913298606873 and batch: 550, loss is 4.498520460128784 and perplexity is 89.8840458425683
At time: 116.88090181350708 and batch: 600, loss is 4.526174249649048 and perplexity is 92.404367943768
At time: 118.671795129776 and batch: 650, loss is 4.512035503387451 and perplexity is 91.1070786649716
At time: 120.43688988685608 and batch: 700, loss is 4.472048311233521 and perplexity is 87.53584015171121
At time: 122.18756484985352 and batch: 750, loss is 4.46416428565979 and perplexity is 86.84841873407206
At time: 123.9388358592987 and batch: 800, loss is 4.435757131576538 and perplexity is 84.41601472904749
At time: 125.6933274269104 and batch: 850, loss is 4.4458655166625975 and perplexity is 85.27365167287857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.494968732198079 and perplexity of 89.56536842916302
Finished 4 epochs...
Completing Train Step...
At time: 130.19641494750977 and batch: 50, loss is 4.435136861801148 and perplexity is 84.3636702620758
At time: 131.91119861602783 and batch: 100, loss is 4.381796894073486 and perplexity is 79.98162286303642
At time: 133.6348066329956 and batch: 150, loss is 4.385135774612427 and perplexity is 80.24911826617868
At time: 135.36295557022095 and batch: 200, loss is 4.430869302749634 and perplexity is 84.00441044391265
At time: 137.08789539337158 and batch: 250, loss is 4.424947853088379 and perplexity is 83.50845240061001
At time: 138.82089352607727 and batch: 300, loss is 4.39987811088562 and perplexity is 81.44094129575247
At time: 140.54585576057434 and batch: 350, loss is 4.353640718460083 and perplexity is 77.76105430116951
At time: 142.27602243423462 and batch: 400, loss is 4.3709925937652585 and perplexity is 79.12212886702744
At time: 144.02065253257751 and batch: 450, loss is 4.397491073608398 and perplexity is 81.24677057156373
At time: 145.77018570899963 and batch: 500, loss is 4.376087303161621 and perplexity is 79.5262617159969
At time: 147.5268805027008 and batch: 550, loss is 4.396920566558838 and perplexity is 81.2004319357147
At time: 149.28302907943726 and batch: 600, loss is 4.42764458656311 and perplexity is 83.73395636500409
At time: 151.03968501091003 and batch: 650, loss is 4.409384517669678 and perplexity is 82.21884368249066
At time: 152.79646706581116 and batch: 700, loss is 4.37241979598999 and perplexity is 79.23513276589838
At time: 154.55336952209473 and batch: 750, loss is 4.366361818313599 and perplexity is 78.75657909767575
At time: 156.3091742992401 and batch: 800, loss is 4.33677456855774 and perplexity is 76.46052301032186
At time: 158.0650351047516 and batch: 850, loss is 4.348508243560791 and perplexity is 77.36297009429676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.469854354858398 and perplexity of 87.34400085763045
Finished 5 epochs...
Completing Train Step...
At time: 162.54599452018738 and batch: 50, loss is 4.337737674713135 and perplexity is 76.53419808344114
At time: 164.2625288963318 and batch: 100, loss is 4.287572774887085 and perplexity is 72.78957721921391
At time: 166.00916719436646 and batch: 150, loss is 4.2927530479431155 and perplexity is 73.16762545597263
At time: 167.76601004600525 and batch: 200, loss is 4.339504318237305 and perplexity is 76.66952623196447
At time: 169.51810455322266 and batch: 250, loss is 4.3335306930541995 and perplexity is 76.21289644475806
At time: 171.27273154258728 and batch: 300, loss is 4.309858226776123 and perplexity is 74.42993602031986
At time: 173.0233588218689 and batch: 350, loss is 4.264787473678589 and perplexity is 71.1497971617942
At time: 174.77508330345154 and batch: 400, loss is 4.285364093780518 and perplexity is 72.62898566830111
At time: 176.5269513130188 and batch: 450, loss is 4.3145643329620365 and perplexity is 74.78103671321546
At time: 178.278746843338 and batch: 500, loss is 4.291852903366089 and perplexity is 73.10179364822197
At time: 180.0305244922638 and batch: 550, loss is 4.316653633117676 and perplexity is 74.93744007474908
At time: 181.7886998653412 and batch: 600, loss is 4.34883544921875 and perplexity is 77.38828783765672
At time: 183.54033660888672 and batch: 650, loss is 4.329382448196411 and perplexity is 75.89740151650874
At time: 185.29214310646057 and batch: 700, loss is 4.294665794372559 and perplexity is 73.30771050118985
At time: 187.04412984848022 and batch: 750, loss is 4.29000581741333 and perplexity is 72.96689297697395
At time: 188.79631447792053 and batch: 800, loss is 4.260884313583374 and perplexity is 70.87262937972262
At time: 190.54768204689026 and batch: 850, loss is 4.271080436706543 and perplexity is 71.59895197985004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.455835024515788 and perplexity of 86.12803985119251
Finished 6 epochs...
Completing Train Step...
At time: 195.0284502506256 and batch: 50, loss is 4.2618092346191405 and perplexity is 70.9382112898648
At time: 196.7224462032318 and batch: 100, loss is 4.2147800064086915 and perplexity is 67.67927522625455
At time: 198.44197487831116 and batch: 150, loss is 4.220236711502075 and perplexity is 68.0495905038651
At time: 200.16460013389587 and batch: 200, loss is 4.2671049690246585 and perplexity is 71.31487769839377
At time: 201.8857138156891 and batch: 250, loss is 4.259184999465942 and perplexity is 70.7522967904687
At time: 203.6152319908142 and batch: 300, loss is 4.238241786956787 and perplexity is 69.28592526620542
At time: 205.33808588981628 and batch: 350, loss is 4.194965395927429 and perplexity is 66.35143550767893
At time: 207.13821029663086 and batch: 400, loss is 4.215721268653869 and perplexity is 67.74300916327478
At time: 208.8914361000061 and batch: 450, loss is 4.250308895111084 and perplexity is 70.12707091076237
At time: 210.64370822906494 and batch: 500, loss is 4.224756078720093 and perplexity is 68.35782758603328
At time: 212.39470839500427 and batch: 550, loss is 4.251505365371704 and perplexity is 70.21102608048794
At time: 214.14576530456543 and batch: 600, loss is 4.284580507278442 and perplexity is 72.57209686703123
At time: 215.89595770835876 and batch: 650, loss is 4.263458662033081 and perplexity is 71.05531527097101
At time: 217.64577674865723 and batch: 700, loss is 4.23084258556366 and perplexity is 68.77515672141762
At time: 219.39590096473694 and batch: 750, loss is 4.228937492370606 and perplexity is 68.64425836487044
At time: 221.14990639686584 and batch: 800, loss is 4.1980211353302 and perplexity is 66.55449829906765
At time: 222.90369844436646 and batch: 850, loss is 4.209344043731689 and perplexity is 67.31237135348813
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.450520833333333 and perplexity of 85.6715529847825
Finished 7 epochs...
Completing Train Step...
At time: 227.35189819335938 and batch: 50, loss is 4.200583372116089 and perplexity is 66.7252453366326
At time: 229.07769989967346 and batch: 100, loss is 4.155532941818238 and perplexity is 63.78594969096285
At time: 230.79972410202026 and batch: 150, loss is 4.160612449645996 and perplexity is 64.11077520016501
At time: 232.5259611606598 and batch: 200, loss is 4.209408845901489 and perplexity is 67.3167334825424
At time: 234.25244069099426 and batch: 250, loss is 4.199188766479492 and perplexity is 66.63225479102395
At time: 235.99712252616882 and batch: 300, loss is 4.179904541969299 and perplexity is 65.3596138161921
At time: 237.7517523765564 and batch: 350, loss is 4.136758341789245 and perplexity is 62.59956578807376
At time: 239.50639843940735 and batch: 400, loss is 4.1573129081726075 and perplexity is 63.89958764115518
At time: 241.26445293426514 and batch: 450, loss is 4.196002578735351 and perplexity is 66.4202897769281
At time: 243.02002358436584 and batch: 500, loss is 4.168592548370361 and perplexity is 64.62443230688018
At time: 244.77534866333008 and batch: 550, loss is 4.196730003356934 and perplexity is 66.46862310838088
At time: 246.53160333633423 and batch: 600, loss is 4.230444746017456 and perplexity is 68.74780068629461
At time: 248.28938603401184 and batch: 650, loss is 4.209451808929443 and perplexity is 67.31962567537302
At time: 250.07359147071838 and batch: 700, loss is 4.177434492111206 and perplexity is 65.1983715315817
At time: 251.83026790618896 and batch: 750, loss is 4.176640472412109 and perplexity is 65.146623287518
At time: 253.5862376689911 and batch: 800, loss is 4.14462459564209 and perplexity is 63.09393171839944
At time: 255.3415036201477 and batch: 850, loss is 4.155349702835083 and perplexity is 63.77426268919177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.448751131693522 and perplexity of 85.52007397284217
Finished 8 epochs...
Completing Train Step...
At time: 259.7659993171692 and batch: 50, loss is 4.149522294998169 and perplexity is 63.403704795627434
At time: 261.5095293521881 and batch: 100, loss is 4.107794280052185 and perplexity is 60.812434328979236
At time: 263.22918462753296 and batch: 150, loss is 4.108848896026611 and perplexity is 60.87660192380423
At time: 264.95245361328125 and batch: 200, loss is 4.15984354019165 and perplexity is 64.06149876597397
At time: 266.6858870983124 and batch: 250, loss is 4.148756837844848 and perplexity is 63.35519054640294
At time: 268.42792439460754 and batch: 300, loss is 4.127944321632385 and perplexity is 62.050236409819114
At time: 270.1708097457886 and batch: 350, loss is 4.087108507156372 and perplexity is 59.56740372885374
At time: 271.92208218574524 and batch: 400, loss is 4.109021706581116 and perplexity is 60.88712295218522
At time: 273.6741797924042 and batch: 450, loss is 4.1499926424026485 and perplexity is 63.43353357800771
At time: 275.42684531211853 and batch: 500, loss is 4.120792536735535 and perplexity is 61.60804956356882
At time: 277.17688846588135 and batch: 550, loss is 4.147837719917297 and perplexity is 63.29698640729965
At time: 278.92793369293213 and batch: 600, loss is 4.183643016815186 and perplexity is 65.60441639740952
At time: 280.67799067497253 and batch: 650, loss is 4.162177085876465 and perplexity is 64.21116375711328
At time: 282.42801332473755 and batch: 700, loss is 4.130902123451233 and perplexity is 62.234040405826725
At time: 284.1781735420227 and batch: 750, loss is 4.13164448261261 and perplexity is 62.28025756861608
At time: 285.9281485080719 and batch: 800, loss is 4.098968572616577 and perplexity is 60.27808305765792
At time: 287.6825647354126 and batch: 850, loss is 4.111331238746643 and perplexity is 61.02790623028683
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.4458268483479815 and perplexity of 85.27035434823863
Finished 9 epochs...
Completing Train Step...
At time: 292.0869851112366 and batch: 50, loss is 4.103911509513855 and perplexity is 60.57677140919279
At time: 293.8241198062897 and batch: 100, loss is 4.061788086891174 and perplexity is 58.078066919208865
At time: 295.5517489910126 and batch: 150, loss is 4.065924105644226 and perplexity is 58.31877633932126
At time: 297.289359331131 and batch: 200, loss is 4.11601176738739 and perplexity is 61.314218617251
At time: 299.039834022522 and batch: 250, loss is 4.103969674110413 and perplexity is 60.58029493513382
At time: 300.79325127601624 and batch: 300, loss is 4.085091552734375 and perplexity is 59.44738007230743
At time: 302.54482412338257 and batch: 350, loss is 4.04695246219635 and perplexity is 57.2228023937688
At time: 304.295667886734 and batch: 400, loss is 4.067972903251648 and perplexity is 58.43838219100659
At time: 306.04678225517273 and batch: 450, loss is 4.110216088294983 and perplexity is 60.95988886493677
At time: 307.7979907989502 and batch: 500, loss is 4.080692610740662 and perplexity is 59.1864488271635
At time: 309.5493996143341 and batch: 550, loss is 4.106325869560242 and perplexity is 60.72320224306925
At time: 311.30103182792664 and batch: 600, loss is 4.143094573020935 and perplexity is 62.99747038843891
At time: 313.0544812679291 and batch: 650, loss is 4.122469630241394 and perplexity is 61.71145871256889
At time: 314.80660939216614 and batch: 700, loss is 4.091895484924317 and perplexity is 59.85323515471602
At time: 316.56237030029297 and batch: 750, loss is 4.092176151275635 and perplexity is 59.87003630148947
At time: 318.3141324520111 and batch: 800, loss is 4.059804883003235 and perplexity is 57.96300040897989
At time: 320.06576442718506 and batch: 850, loss is 4.073477258682251 and perplexity is 58.76093472488201
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.451505343119304 and perplexity of 85.75593899968806
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 324.5350811481476 and batch: 50, loss is 4.066801943778992 and perplexity is 58.36999326195975
At time: 326.24101734161377 and batch: 100, loss is 4.02073718547821 and perplexity is 55.74218301202333
At time: 327.9615206718445 and batch: 150, loss is 4.018321647644043 and perplexity is 55.6076981520237
At time: 329.68582344055176 and batch: 200, loss is 4.062590107917786 and perplexity is 58.124665434059565
At time: 331.41691994667053 and batch: 250, loss is 4.040486874580384 and perplexity is 56.85401684362336
At time: 333.16440439224243 and batch: 300, loss is 4.018950624465942 and perplexity is 55.64268510711899
At time: 334.9072217941284 and batch: 350, loss is 3.9732292461395264 and perplexity is 53.15590749111311
At time: 336.6543276309967 and batch: 400, loss is 3.9846334886550903 and perplexity is 53.765580171169724
At time: 338.4354033470154 and batch: 450, loss is 4.021814489364624 and perplexity is 55.80226664077178
At time: 340.18789052963257 and batch: 500, loss is 3.9876398515701292 and perplexity is 53.927462233689475
At time: 341.94086623191833 and batch: 550, loss is 4.007960410118103 and perplexity is 55.03450818977612
At time: 343.69207859039307 and batch: 600, loss is 4.037178931236267 and perplexity is 56.666257696542296
At time: 345.4432454109192 and batch: 650, loss is 4.014431276321411 and perplexity is 55.39178382359294
At time: 347.194130897522 and batch: 700, loss is 3.9819493865966797 and perplexity is 53.62146136807284
At time: 348.9461693763733 and batch: 750, loss is 3.9725763463973998 and perplexity is 53.12121333995327
At time: 350.6987416744232 and batch: 800, loss is 3.934209589958191 and perplexity is 51.12172685708751
At time: 352.45168709754944 and batch: 850, loss is 3.9494556045532225 and perplexity is 51.90710115215239
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.408429145812988 and perplexity of 82.14033162322448
Finished 11 epochs...
Completing Train Step...
At time: 356.90696263313293 and batch: 50, loss is 4.004009351730347 and perplexity is 54.81749263745262
At time: 358.62864089012146 and batch: 100, loss is 3.960070638656616 and perplexity is 52.46103159501315
At time: 360.3542540073395 and batch: 150, loss is 3.960897488594055 and perplexity is 52.50442693394974
At time: 362.08056020736694 and batch: 200, loss is 4.011751985549926 and perplexity is 55.24357176863674
At time: 363.8169023990631 and batch: 250, loss is 3.992057914733887 and perplexity is 54.16624425672479
At time: 365.56256794929504 and batch: 300, loss is 3.974888114929199 and perplexity is 53.244159345922675
At time: 367.3178975582123 and batch: 350, loss is 3.9329272985458372 and perplexity is 51.05621591678854
At time: 369.0730369091034 and batch: 400, loss is 3.9473056602478027 and perplexity is 51.79562365376572
At time: 370.8277475833893 and batch: 450, loss is 3.9898280096054077 and perplexity is 54.045593241020136
At time: 372.58285999298096 and batch: 500, loss is 3.9570940732955933 and perplexity is 52.30511007598148
At time: 374.3393952846527 and batch: 550, loss is 3.978237729072571 and perplexity is 53.422805766374914
At time: 376.0944540500641 and batch: 600, loss is 4.011837601661682 and perplexity is 55.24830171092781
At time: 377.84915947914124 and batch: 650, loss is 3.9920782804489137 and perplexity is 54.16734740225253
At time: 379.60322284698486 and batch: 700, loss is 3.963328356742859 and perplexity is 52.63221352630776
At time: 381.35751962661743 and batch: 750, loss is 3.9551033210754394 and perplexity is 52.20108713827381
At time: 383.1485962867737 and batch: 800, loss is 3.9202369022369385 and perplexity is 50.41238617059156
At time: 384.891966342926 and batch: 850, loss is 3.9376912403106687 and perplexity is 51.300025041277486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.408166885375977 and perplexity of 82.1187922885378
Finished 12 epochs...
Completing Train Step...
At time: 389.2784023284912 and batch: 50, loss is 3.9726134586334227 and perplexity is 53.123184823543426
At time: 390.9796886444092 and batch: 100, loss is 3.9291321086883544 and perplexity is 50.86281511250189
At time: 392.7111177444458 and batch: 150, loss is 3.93138943195343 and perplexity is 50.9777586118923
At time: 394.4526934623718 and batch: 200, loss is 3.9841311740875245 and perplexity is 53.738579718944095
At time: 396.19411063194275 and batch: 250, loss is 3.9635840463638305 and perplexity is 52.64567275765475
At time: 397.9449725151062 and batch: 300, loss is 3.9489452171325685 and perplexity is 51.8806151803101
At time: 399.6988568305969 and batch: 350, loss is 3.907700529098511 and perplexity is 49.78434260628296
At time: 401.4569945335388 and batch: 400, loss is 3.924227638244629 and perplexity is 50.6139706630267
At time: 403.2133057117462 and batch: 450, loss is 3.9681872129440308 and perplexity is 52.88856817422164
At time: 404.9683039188385 and batch: 500, loss is 3.9351766967773436 and perplexity is 51.17119094240968
At time: 406.72219610214233 and batch: 550, loss is 3.9571532678604124 and perplexity is 52.308206345850515
At time: 408.47342324256897 and batch: 600, loss is 3.9921832704544067 and perplexity is 54.173034730904945
At time: 410.2238793373108 and batch: 650, loss is 3.9733557224273683 and perplexity is 53.16263087813501
At time: 411.9750626087189 and batch: 700, loss is 3.9464784479141235 and perplexity is 51.75279539152431
At time: 413.7254421710968 and batch: 750, loss is 3.9383381032943725 and perplexity is 51.3332198636332
At time: 415.4777281284332 and batch: 800, loss is 3.904461688995361 and perplexity is 49.62335992025731
At time: 417.23171305656433 and batch: 850, loss is 3.9229422426223755 and perplexity is 50.54895348206348
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.41172726949056 and perplexity of 82.41168783302915
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 421.725798368454 and batch: 50, loss is 3.9592637109756468 and perplexity is 52.41871641138949
At time: 423.45166635513306 and batch: 100, loss is 3.918791675567627 and perplexity is 50.33958146795528
At time: 425.1646044254303 and batch: 150, loss is 3.916141724586487 and perplexity is 50.20636063694906
At time: 426.9170491695404 and batch: 200, loss is 3.9627543926239013 and perplexity is 52.60201319202555
At time: 428.663596868515 and batch: 250, loss is 3.9388467693328857 and perplexity is 51.3593379713594
At time: 430.4063913822174 and batch: 300, loss is 3.9256168460845946 and perplexity is 50.68433285041876
At time: 432.1483931541443 and batch: 350, loss is 3.881998744010925 and perplexity is 48.52109947250682
At time: 433.90582966804504 and batch: 400, loss is 3.8912650966644287 and perplexity is 48.97280267983067
At time: 435.64694571495056 and batch: 450, loss is 3.931548194885254 and perplexity is 50.985852632805575
At time: 437.3960506916046 and batch: 500, loss is 3.8970400285720825 and perplexity is 49.25643547240707
At time: 439.1379690170288 and batch: 550, loss is 3.914708094596863 and perplexity is 50.13443486246277
At time: 440.8807604312897 and batch: 600, loss is 3.945779218673706 and perplexity is 51.71662097229294
At time: 442.6249623298645 and batch: 650, loss is 3.921482729911804 and perplexity is 50.475230454884866
At time: 444.3676517009735 and batch: 700, loss is 3.8941004419326783 and perplexity is 49.11185452101658
At time: 446.1116375923157 and batch: 750, loss is 3.884336161613464 and perplexity is 48.634646195866274
At time: 447.8545649051666 and batch: 800, loss is 3.841505208015442 and perplexity is 46.59555768854009
At time: 449.60022616386414 and batch: 850, loss is 3.8634330701828 and perplexity is 47.62858328434453
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.389208157857259 and perplexity of 80.57658977298333
Finished 14 epochs...
Completing Train Step...
At time: 454.0021855831146 and batch: 50, loss is 3.9313017654418947 and perplexity is 50.973289765515844
At time: 455.7207565307617 and batch: 100, loss is 3.88725501537323 and perplexity is 48.77681099396415
At time: 457.4339189529419 and batch: 150, loss is 3.883784432411194 and perplexity is 48.60782044227424
At time: 459.1572296619415 and batch: 200, loss is 3.934820294380188 and perplexity is 51.15295665685705
At time: 460.88043904304504 and batch: 250, loss is 3.913139042854309 and perplexity is 50.055833021383705
At time: 462.6032428741455 and batch: 300, loss is 3.9024925565719606 and perplexity is 49.52574109701818
At time: 464.32578706741333 and batch: 350, loss is 3.8612921142578127 and perplexity is 47.52672166628903
At time: 466.0571343898773 and batch: 400, loss is 3.872737855911255 and perplexity is 48.07382527463309
At time: 467.7823967933655 and batch: 450, loss is 3.9143700408935547 and perplexity is 50.11748959546076
At time: 469.56463623046875 and batch: 500, loss is 3.881575345993042 and perplexity is 48.50056008363975
At time: 471.28996419906616 and batch: 550, loss is 3.90143768787384 and perplexity is 49.4735254881313
At time: 473.0156321525574 and batch: 600, loss is 3.9342030239105226 and perplexity is 51.12139119049409
At time: 474.7435109615326 and batch: 650, loss is 3.911604852676392 and perplexity is 49.979096733096156
At time: 476.46981716156006 and batch: 700, loss is 3.8860818338394165 and perplexity is 48.71962049400649
At time: 478.19601464271545 and batch: 750, loss is 3.878010869026184 and perplexity is 48.32798870041927
At time: 479.92312145233154 and batch: 800, loss is 3.83637481212616 and perplexity is 46.357116203523816
At time: 481.658371925354 and batch: 850, loss is 3.8597312068939207 and perplexity is 47.452594724153826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.389721234639485 and perplexity of 80.61794235800531
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 486.04873037338257 and batch: 50, loss is 3.92425199508667 and perplexity is 50.61520347452884
At time: 487.75876688957214 and batch: 100, loss is 3.8869890832901 and perplexity is 48.763841399600146
At time: 489.43300199508667 and batch: 150, loss is 3.881157593727112 and perplexity is 48.480303096261714
At time: 491.12769746780396 and batch: 200, loss is 3.9293032026290895 and perplexity is 50.871518176475895
At time: 492.82373428344727 and batch: 250, loss is 3.9043301868438722 and perplexity is 49.61683477070869
At time: 494.5249345302582 and batch: 300, loss is 3.896143593788147 and perplexity is 49.21230007552499
At time: 496.24116563796997 and batch: 350, loss is 3.852806820869446 and perplexity is 47.12514962688494
At time: 497.9671585559845 and batch: 400, loss is 3.860858721733093 and perplexity is 47.50612840319991
At time: 499.69512009620667 and batch: 450, loss is 3.9005069875717164 and perplexity is 49.427501883425926
At time: 501.42853927612305 and batch: 500, loss is 3.86794659614563 and perplexity is 47.84404200506069
At time: 503.15133357048035 and batch: 550, loss is 3.886357989311218 and perplexity is 48.73307654168509
At time: 504.88625288009644 and batch: 600, loss is 3.9132201576232912 and perplexity is 50.05989345339372
At time: 506.61483120918274 and batch: 650, loss is 3.887145829200745 and perplexity is 48.771485531404416
At time: 508.3336670398712 and batch: 700, loss is 3.8596760034561157 and perplexity is 47.449975250094944
At time: 510.0532910823822 and batch: 750, loss is 3.856272039413452 and perplexity is 47.288731829614605
At time: 511.7724075317383 and batch: 800, loss is 3.806905884742737 and perplexity is 45.01095410679195
At time: 513.5326285362244 and batch: 850, loss is 3.8311167573928833 and perplexity is 46.11400764839774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.379177411397298 and perplexity of 79.77238655262134
Finished 16 epochs...
Completing Train Step...
At time: 517.865597486496 and batch: 50, loss is 3.908203573226929 and perplexity is 49.809392627623026
At time: 519.5140872001648 and batch: 100, loss is 3.868191442489624 and perplexity is 47.85575787806333
At time: 521.1775915622711 and batch: 150, loss is 3.8620885896682737 and perplexity is 47.56459061027549
At time: 522.8606100082397 and batch: 200, loss is 3.9124340057373046 and perplexity is 50.0205542390679
At time: 524.5478382110596 and batch: 250, loss is 3.888447289466858 and perplexity is 48.83500100440984
At time: 526.2249586582184 and batch: 300, loss is 3.8819634771347045 and perplexity is 48.519388315071396
At time: 527.9025745391846 and batch: 350, loss is 3.8394753456115724 and perplexity is 46.50107104768386
At time: 529.5796754360199 and batch: 400, loss is 3.848906021118164 and perplexity is 46.941681922968804
At time: 531.2688837051392 and batch: 450, loss is 3.8897434091567993 and perplexity is 48.898338048097884
At time: 532.9582257270813 and batch: 500, loss is 3.858685188293457 and perplexity is 47.40298437862766
At time: 534.6479487419128 and batch: 550, loss is 3.87878746509552 and perplexity is 48.36553460359633
At time: 536.3545346260071 and batch: 600, loss is 3.9071957635879517 and perplexity is 49.759219528334505
At time: 538.0455937385559 and batch: 650, loss is 3.8821805906295777 and perplexity is 48.52992367268036
At time: 539.7434296607971 and batch: 700, loss is 3.856243190765381 and perplexity is 47.28736763331002
At time: 541.4451947212219 and batch: 750, loss is 3.8545689725875856 and perplexity is 47.20826449926792
At time: 543.1407694816589 and batch: 800, loss is 3.805787739753723 and perplexity is 44.96065346095495
At time: 544.8341634273529 and batch: 850, loss is 3.8304775524139405 and perplexity is 46.084540763802856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.378796259562175 and perplexity of 79.74198695489392
Finished 17 epochs...
Completing Train Step...
At time: 549.1944105625153 and batch: 50, loss is 3.896884002685547 and perplexity is 49.24875079291496
At time: 550.8496382236481 and batch: 100, loss is 3.8574301290512083 and perplexity is 47.34352814332638
At time: 552.5196743011475 and batch: 150, loss is 3.851528878211975 and perplexity is 47.06496485247316
At time: 554.2001881599426 and batch: 200, loss is 3.9027967071533203 and perplexity is 49.5408066709505
At time: 555.9160721302032 and batch: 250, loss is 3.8791511392593385 and perplexity is 48.38312709772484
At time: 557.5883145332336 and batch: 300, loss is 3.8733477067947386 and perplexity is 48.10315208102972
At time: 559.2551815509796 and batch: 350, loss is 3.831196722984314 and perplexity is 46.11769532973443
At time: 560.9388921260834 and batch: 400, loss is 3.8414860439300536 and perplexity is 46.59466473585016
At time: 562.6255993843079 and batch: 450, loss is 3.882596526145935 and perplexity is 48.55011319001954
At time: 564.3240911960602 and batch: 500, loss is 3.8523788166046145 and perplexity is 47.10498417762079
At time: 566.0176961421967 and batch: 550, loss is 3.8729637670516968 and perplexity is 48.084686914162766
At time: 567.7111811637878 and batch: 600, loss is 3.901983766555786 and perplexity is 49.50054930361429
At time: 569.4044349193573 and batch: 650, loss is 3.8773555040359495 and perplexity is 48.296326604824294
At time: 571.0972974300385 and batch: 700, loss is 3.852133231163025 and perplexity is 47.09341729966702
At time: 572.7958130836487 and batch: 750, loss is 3.8514820337295532 and perplexity is 47.06276017019344
At time: 574.4909873008728 and batch: 800, loss is 3.8029322052001953 and perplexity is 44.83244989342163
At time: 576.1839764118195 and batch: 850, loss is 3.827905707359314 and perplexity is 45.9661707454633
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.37913195292155 and perplexity of 79.76876030394436
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 580.5021970272064 and batch: 50, loss is 3.89597806930542 and perplexity is 49.20415490914201
At time: 582.1563665866852 and batch: 100, loss is 3.862590847015381 and perplexity is 47.58848627575609
At time: 583.8347926139832 and batch: 150, loss is 3.8585881567001343 and perplexity is 47.39838501467055
At time: 585.5311496257782 and batch: 200, loss is 3.908583526611328 and perplexity is 49.82832147073804
At time: 587.2284262180328 and batch: 250, loss is 3.881069984436035 and perplexity is 48.476055957323396
At time: 588.9247970581055 and batch: 300, loss is 3.8769041109085083 and perplexity is 48.274530894501225
At time: 590.6206657886505 and batch: 350, loss is 3.830743908882141 and perplexity is 46.096817314216075
At time: 592.3177597522736 and batch: 400, loss is 3.8377041625976562 and perplexity is 46.41878203647706
At time: 594.0141429901123 and batch: 450, loss is 3.879243884086609 and perplexity is 48.38761459058298
At time: 595.7116572856903 and batch: 500, loss is 3.850428442955017 and perplexity is 47.01320139217399
At time: 597.4078676700592 and batch: 550, loss is 3.8722767353057863 and perplexity is 48.05166255345166
At time: 599.1249308586121 and batch: 600, loss is 3.8959780883789064 and perplexity is 49.2041558476368
At time: 600.8135120868683 and batch: 650, loss is 3.867312526702881 and perplexity is 47.813715175681644
At time: 602.4998972415924 and batch: 700, loss is 3.838349895477295 and perplexity is 46.44876584999606
At time: 604.1864423751831 and batch: 750, loss is 3.8418020820617675 and perplexity is 46.60939275382618
At time: 605.8734219074249 and batch: 800, loss is 3.789857130050659 and perplexity is 44.25007781797331
At time: 607.5606355667114 and batch: 850, loss is 3.813703680038452 and perplexity is 45.317971697970066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.373080889383952 and perplexity of 79.28753190719429
Finished 19 epochs...
Completing Train Step...
At time: 611.8707580566406 and batch: 50, loss is 3.8879201889038084 and perplexity is 48.80926683072844
At time: 613.5593430995941 and batch: 100, loss is 3.85108615398407 and perplexity is 47.044132664045
At time: 615.224406003952 and batch: 150, loss is 3.8470027017593384 and perplexity is 46.85242188315616
At time: 616.9046909809113 and batch: 200, loss is 3.8982304191589354 and perplexity is 49.31510478230446
At time: 618.5831875801086 and batch: 250, loss is 3.8720612573623656 and perplexity is 48.0413095954838
At time: 620.2728877067566 and batch: 300, loss is 3.869026427268982 and perplexity is 47.895733394647515
At time: 621.960946559906 and batch: 350, loss is 3.823247175216675 and perplexity is 45.75253386506937
At time: 623.6501276493073 and batch: 400, loss is 3.83088960647583 and perplexity is 46.103533998865984
At time: 625.338871717453 and batch: 450, loss is 3.8733005475997926 and perplexity is 48.100883628592825
At time: 627.0334711074829 and batch: 500, loss is 3.845365467071533 and perplexity is 46.77577623342054
At time: 628.7230267524719 and batch: 550, loss is 3.868185086250305 and perplexity is 47.855453696380195
At time: 630.4179601669312 and batch: 600, loss is 3.8928542566299438 and perplexity is 49.05069016869648
At time: 632.1123292446136 and batch: 650, loss is 3.8651215744018557 and perplexity is 47.709072282050734
At time: 633.8094365596771 and batch: 700, loss is 3.837243204116821 and perplexity is 46.39738983606421
At time: 635.5023078918457 and batch: 750, loss is 3.8416674518585205 and perplexity is 46.603118144191974
At time: 637.197883605957 and batch: 800, loss is 3.7902371406555178 and perplexity is 44.26689651224866
At time: 638.8944754600525 and batch: 850, loss is 3.81403781414032 and perplexity is 45.33311650779965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.372882843017578 and perplexity of 79.27183085442086
Finished 20 epochs...
Completing Train Step...
At time: 643.1734294891357 and batch: 50, loss is 3.8818791723251342 and perplexity is 48.51529806969513
At time: 644.8616313934326 and batch: 100, loss is 3.845013427734375 and perplexity is 46.75931221832077
At time: 646.5352354049683 and batch: 150, loss is 3.8411163663864136 and perplexity is 46.577442918099656
At time: 648.2283072471619 and batch: 200, loss is 3.892796039581299 and perplexity is 49.047834665401155
At time: 649.9211342334747 and batch: 250, loss is 3.866914792060852 and perplexity is 47.794701786184646
At time: 651.6373579502106 and batch: 300, loss is 3.864266972541809 and perplexity is 47.668317437200926
At time: 653.3404114246368 and batch: 350, loss is 3.8185482454299926 and perplexity is 45.53805023727698
At time: 655.0305466651917 and batch: 400, loss is 3.8265802669525146 and perplexity is 45.905285684085214
At time: 656.7241554260254 and batch: 450, loss is 3.8694396448135375 and perplexity is 47.91552884162687
At time: 658.416205406189 and batch: 500, loss is 3.84203230381012 and perplexity is 46.62012448500716
At time: 660.1087131500244 and batch: 550, loss is 3.865160336494446 and perplexity is 47.71092162136983
At time: 661.8036024570465 and batch: 600, loss is 3.8901504707336425 and perplexity is 48.91824673444448
At time: 663.497145652771 and batch: 650, loss is 3.8627685546875 and perplexity is 47.596943866338925
At time: 665.1988425254822 and batch: 700, loss is 3.835371799468994 and perplexity is 46.31064273985427
At time: 666.8919064998627 and batch: 750, loss is 3.8404254293441773 and perplexity is 46.54527195279056
At time: 668.5876305103302 and batch: 800, loss is 3.7891963958740233 and perplexity is 44.220849936233705
At time: 670.2818901538849 and batch: 850, loss is 3.8129721784591677 and perplexity is 45.28483365184724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.372915267944336 and perplexity of 79.27440127940297
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 674.5969874858856 and batch: 50, loss is 3.883211040496826 and perplexity is 48.579957100128546
At time: 676.2929615974426 and batch: 100, loss is 3.8510765552520754 and perplexity is 47.04368110219085
At time: 677.9481883049011 and batch: 150, loss is 3.8496136331558226 and perplexity is 46.97491017713527
At time: 679.6060426235199 and batch: 200, loss is 3.9017574644088744 and perplexity is 49.48934849046542
At time: 681.285279750824 and batch: 250, loss is 3.8737474250793458 and perplexity is 48.12238363380933
At time: 682.9927175045013 and batch: 300, loss is 3.870107684135437 and perplexity is 47.947548993192044
At time: 684.674887418747 and batch: 350, loss is 3.8228632497787474 and perplexity is 45.73497167497041
At time: 686.3566665649414 and batch: 400, loss is 3.827970061302185 and perplexity is 45.96912894497434
At time: 688.0488367080688 and batch: 450, loss is 3.86922833442688 and perplexity is 47.90540486238911
At time: 689.7344269752502 and batch: 500, loss is 3.842149353027344 and perplexity is 46.62558165345743
At time: 691.4193994998932 and batch: 550, loss is 3.8685549783706663 and perplexity is 47.873158325818395
At time: 693.1014277935028 and batch: 600, loss is 3.892507457733154 and perplexity is 49.03368239276863
At time: 694.787770986557 and batch: 650, loss is 3.863117604255676 and perplexity is 47.61356045888049
At time: 696.4820735454559 and batch: 700, loss is 3.8297962379455566 and perplexity is 46.05315339296073
At time: 698.1761286258698 and batch: 750, loss is 3.834538540840149 and perplexity is 46.27207006992077
At time: 699.8713059425354 and batch: 800, loss is 3.7827863454818726 and perplexity is 43.9382986113468
At time: 701.5653002262115 and batch: 850, loss is 3.8072926712036135 and perplexity is 45.02836710176986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.36715030670166 and perplexity of 78.81870223424507
Finished 22 epochs...
Completing Train Step...
At time: 705.8721945285797 and batch: 50, loss is 3.8801016283035277 and perplexity is 48.42913659225121
At time: 707.5231926441193 and batch: 100, loss is 3.8442955303192137 and perplexity is 46.725755875391634
At time: 709.2025325298309 and batch: 150, loss is 3.841582899093628 and perplexity is 46.59917788828237
At time: 710.8831343650818 and batch: 200, loss is 3.8943670177459717 and perplexity is 49.124948298742545
At time: 712.5647132396698 and batch: 250, loss is 3.8676775217056276 and perplexity is 47.83117012807494
At time: 714.2447643280029 and batch: 300, loss is 3.8651072216033935 and perplexity is 47.70838752826553
At time: 715.9265956878662 and batch: 350, loss is 3.8184331941604612 and perplexity is 45.532811328162545
At time: 717.6076138019562 and batch: 400, loss is 3.8239774227142336 and perplexity is 45.785956740431594
At time: 719.2886407375336 and batch: 450, loss is 3.865671739578247 and perplexity is 47.73532737387462
At time: 720.9700276851654 and batch: 500, loss is 3.8391644668579104 and perplexity is 46.48661709950171
At time: 722.663950920105 and batch: 550, loss is 3.866383056640625 and perplexity is 47.7692944059464
At time: 724.346512556076 and batch: 600, loss is 3.890893120765686 and perplexity is 48.95458936520695
At time: 726.075647354126 and batch: 650, loss is 3.8622307538986207 and perplexity is 47.57135307437023
At time: 727.7612359523773 and batch: 700, loss is 3.8296286249160767 and perplexity is 46.045434931278876
At time: 729.4419343471527 and batch: 750, loss is 3.834945321083069 and perplexity is 46.29089646266693
At time: 731.1351511478424 and batch: 800, loss is 3.7835571336746217 and perplexity is 43.97217878866884
At time: 732.8303077220917 and batch: 850, loss is 3.8079789972305296 and perplexity is 45.05928184964586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.367021560668945 and perplexity of 78.80855529223196
Finished 23 epochs...
Completing Train Step...
At time: 737.1396918296814 and batch: 50, loss is 3.8767301845550537 and perplexity is 48.266135411497054
At time: 738.7927134037018 and batch: 100, loss is 3.840793113708496 and perplexity is 46.56238906817557
At time: 740.4696519374847 and batch: 150, loss is 3.838092646598816 and perplexity is 46.436818493862276
At time: 742.1497256755829 and batch: 200, loss is 3.891157445907593 and perplexity is 48.96753100431284
At time: 743.830393075943 and batch: 250, loss is 3.864706792831421 and perplexity is 47.68928754158212
At time: 745.5113186836243 and batch: 300, loss is 3.862308158874512 and perplexity is 47.57503547632434
At time: 747.1915888786316 and batch: 350, loss is 3.815744605064392 and perplexity is 45.41055672794803
At time: 748.8719656467438 and batch: 400, loss is 3.821527905464172 and perplexity is 45.67394049845322
At time: 750.5532240867615 and batch: 450, loss is 3.863467226028442 and perplexity is 47.63021010666593
At time: 752.2431018352509 and batch: 500, loss is 3.837218403816223 and perplexity is 46.39623918111766
At time: 753.9439477920532 and batch: 550, loss is 3.864795126914978 and perplexity is 47.69350031715569
At time: 755.640163898468 and batch: 600, loss is 3.889510598182678 and perplexity is 48.88695530344935
At time: 757.3561055660248 and batch: 650, loss is 3.861117515563965 and perplexity is 47.51842428714009
At time: 759.0671889781952 and batch: 700, loss is 3.8288194370269775 and perplexity is 46.008190593849825
At time: 760.7779178619385 and batch: 750, loss is 3.8344400453567506 and perplexity is 46.267512704455044
At time: 762.489292383194 and batch: 800, loss is 3.783267593383789 and perplexity is 43.95944891422854
At time: 764.1992366313934 and batch: 850, loss is 3.807665829658508 and perplexity is 45.04517295309173
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.367042859395345 and perplexity of 78.81023383196437
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 768.5235939025879 and batch: 50, loss is 3.8783455419540407 and perplexity is 48.34416547670861
At time: 770.1619915962219 and batch: 100, loss is 3.846803126335144 and perplexity is 46.843072224196405
At time: 771.8081407546997 and batch: 150, loss is 3.8453649711608886 and perplexity is 46.775753036820944
At time: 773.46666264534 and batch: 200, loss is 3.899053897857666 and perplexity is 49.355731445918586
At time: 775.1436238288879 and batch: 250, loss is 3.8716502571105957 and perplexity is 48.02156866218687
At time: 776.8236238956451 and batch: 300, loss is 3.867766752243042 and perplexity is 47.8354383195143
At time: 778.5025639533997 and batch: 350, loss is 3.8220056295394897 and perplexity is 45.69576525213402
At time: 780.1836009025574 and batch: 400, loss is 3.8263796043395994 and perplexity is 45.89607513365069
At time: 781.8643248081207 and batch: 450, loss is 3.866020221710205 and perplexity is 47.751965181348524
At time: 783.5442941188812 and batch: 500, loss is 3.8368531799316408 and perplexity is 46.379297260399284
At time: 785.2242052555084 and batch: 550, loss is 3.864916625022888 and perplexity is 47.69929533923881
At time: 786.9166588783264 and batch: 600, loss is 3.891915593147278 and perplexity is 49.00466967929101
At time: 788.6094832420349 and batch: 650, loss is 3.863995060920715 and perplexity is 47.65535762977273
At time: 790.30073595047 and batch: 700, loss is 3.8298360204696653 and perplexity is 46.05498554008935
At time: 791.9949316978455 and batch: 750, loss is 3.8310009479522704 and perplexity is 46.10866752019216
At time: 793.6881988048553 and batch: 800, loss is 3.778252739906311 and perplexity is 43.73955055883641
At time: 795.3807196617126 and batch: 850, loss is 3.805350775718689 and perplexity is 44.94101156411467
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.363097190856934 and perplexity of 78.49988743646351
Finished 25 epochs...
Completing Train Step...
At time: 799.6899065971375 and batch: 50, loss is 3.877008934020996 and perplexity is 48.27959144631024
At time: 801.3649423122406 and batch: 100, loss is 3.842727084159851 and perplexity is 46.65252648623481
At time: 803.0390295982361 and batch: 150, loss is 3.8396342849731444 and perplexity is 46.50846248560824
At time: 804.709579706192 and batch: 200, loss is 3.893203167915344 and perplexity is 49.06780749409336
At time: 806.3934400081635 and batch: 250, loss is 3.866749782562256 and perplexity is 47.786815857051856
At time: 808.0881788730621 and batch: 300, loss is 3.863918433189392 and perplexity is 47.65170604774017
At time: 809.7857046127319 and batch: 350, loss is 3.8188007783889772 and perplexity is 45.549551548018684
At time: 811.5106790065765 and batch: 400, loss is 3.8235639429092405 and perplexity is 45.76702908533837
At time: 813.2144439220428 and batch: 450, loss is 3.8635894584655763 and perplexity is 47.63603241915895
At time: 814.9129791259766 and batch: 500, loss is 3.8350833463668823 and perplexity is 46.297286217752905
At time: 816.6102893352509 and batch: 550, loss is 3.864017481803894 and perplexity is 47.65642611695717
At time: 818.3156626224518 and batch: 600, loss is 3.891518177986145 and perplexity is 48.98519834995116
At time: 820.0136888027191 and batch: 650, loss is 3.8640352869033814 and perplexity is 47.65727465191949
At time: 821.7097015380859 and batch: 700, loss is 3.8302323150634767 and perplexity is 46.07324049880876
At time: 823.4071357250214 and batch: 750, loss is 3.831709632873535 and perplexity is 46.1413556190166
At time: 825.1039173603058 and batch: 800, loss is 3.7791522693634034 and perplexity is 43.7789132743109
At time: 826.8056349754333 and batch: 850, loss is 3.8061298513412476 and perplexity is 44.97603765289173
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.363003094991048 and perplexity of 78.49250126909261
Finished 26 epochs...
Completing Train Step...
At time: 831.0891470909119 and batch: 50, loss is 3.8750779581069947 and perplexity is 48.18645466946566
At time: 832.7534205913544 and batch: 100, loss is 3.8406289434432983 and perplexity is 46.55474553585153
At time: 834.3981947898865 and batch: 150, loss is 3.837339062690735 and perplexity is 46.401837636863725
At time: 836.0710351467133 and batch: 200, loss is 3.8910904264450075 and perplexity is 48.96424933666983
At time: 837.7530190944672 and batch: 250, loss is 3.864815187454224 and perplexity is 47.69445708408717
At time: 839.4326195716858 and batch: 300, loss is 3.862121095657349 and perplexity is 47.5661367694678
At time: 841.1134774684906 and batch: 350, loss is 3.8172058391571047 and perplexity is 45.47696068565591
At time: 842.7928371429443 and batch: 400, loss is 3.8221525287628175 and perplexity is 45.70247841762625
At time: 844.4773199558258 and batch: 450, loss is 3.8623040771484374 and perplexity is 47.57484128845785
At time: 846.1617484092712 and batch: 500, loss is 3.8339644622802735 and perplexity is 46.24551388996869
At time: 847.8429110050201 and batch: 550, loss is 3.863245811462402 and perplexity is 47.61966525180008
At time: 849.5499975681305 and batch: 600, loss is 3.8909703874588013 and perplexity is 48.958372070576694
At time: 851.2430255413055 and batch: 650, loss is 3.8636424207687377 and perplexity is 47.63855539996018
At time: 852.9453003406525 and batch: 700, loss is 3.830035157203674 and perplexity is 46.06415769272057
At time: 854.697491645813 and batch: 750, loss is 3.8316396999359132 and perplexity is 46.138128931299505
At time: 856.3912751674652 and batch: 800, loss is 3.7792015266418457 and perplexity is 43.78106975754277
At time: 858.0906040668488 and batch: 850, loss is 3.8061403322219847 and perplexity is 44.97650904384869
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.362999280293782 and perplexity of 78.49220184453375
Finished 27 epochs...
Completing Train Step...
At time: 862.3913457393646 and batch: 50, loss is 3.8733527517318724 and perplexity is 48.103394759020055
At time: 864.0666937828064 and batch: 100, loss is 3.8389604091644287 and perplexity is 46.47713211541352
At time: 865.706484079361 and batch: 150, loss is 3.835642309188843 and perplexity is 46.32317191340307
At time: 867.3739898204803 and batch: 200, loss is 3.8895346689224244 and perplexity is 48.88813206279012
At time: 869.0566976070404 and batch: 250, loss is 3.863346519470215 and perplexity is 47.62446117491017
At time: 870.742490530014 and batch: 300, loss is 3.860719952583313 and perplexity is 47.49953647554073
At time: 872.4266352653503 and batch: 350, loss is 3.8159081983566283 and perplexity is 45.41798619811461
At time: 874.1196472644806 and batch: 400, loss is 3.8209837675094604 and perplexity is 45.64909433437027
At time: 875.8225991725922 and batch: 450, loss is 3.8612098503112793 and perplexity is 47.5228120914097
At time: 877.5161912441254 and batch: 500, loss is 3.832948336601257 and perplexity is 46.19854650219138
At time: 879.2116987705231 and batch: 550, loss is 3.862448801994324 and perplexity is 47.58172704828964
At time: 880.9058797359467 and batch: 600, loss is 3.8903445291519163 and perplexity is 48.92774065318813
At time: 882.6073670387268 and batch: 650, loss is 3.8631140756607056 and perplexity is 47.61339245020695
At time: 884.3025012016296 and batch: 700, loss is 3.8296533012390137 and perplexity is 46.046571177320146
At time: 885.9962184429169 and batch: 750, loss is 3.8313472080230713 and perplexity is 46.12463587511454
At time: 887.6885731220245 and batch: 800, loss is 3.7790152406692505 and perplexity is 43.77291471799005
At time: 889.3806703090668 and batch: 850, loss is 3.805946183204651 and perplexity is 44.96777774642856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.363009452819824 and perplexity of 78.49300031256232
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 893.7088882923126 and batch: 50, loss is 3.874507007598877 and perplexity is 48.158950441211445
At time: 895.3694627285004 and batch: 100, loss is 3.8436205768585205 and perplexity is 46.69422880560934
At time: 897.0549595355988 and batch: 150, loss is 3.841131625175476 and perplexity is 46.57815363889857
At time: 898.7301361560822 and batch: 200, loss is 3.8949122333526613 and perplexity is 49.15173929000246
At time: 900.4208242893219 and batch: 250, loss is 3.868330955505371 and perplexity is 47.86243484491682
At time: 902.1086456775665 and batch: 300, loss is 3.865013265609741 and perplexity is 47.703905249881515
At time: 903.7953987121582 and batch: 350, loss is 3.821437201499939 and perplexity is 45.6697978788667
At time: 905.4816720485687 and batch: 400, loss is 3.82610098361969 and perplexity is 45.883289317435306
At time: 907.1678607463837 and batch: 450, loss is 3.8648242473602297 and perplexity is 47.69488919334278
At time: 908.8731722831726 and batch: 500, loss is 3.8334594678878786 and perplexity is 46.22216606053848
At time: 910.5697574615479 and batch: 550, loss is 3.85995924949646 and perplexity is 47.463417171294566
At time: 912.265477180481 and batch: 600, loss is 3.888882050514221 and perplexity is 48.85623717659023
At time: 913.9694323539734 and batch: 650, loss is 3.8629290008544923 and perplexity is 47.60458122621901
At time: 915.6836218833923 and batch: 700, loss is 3.8303879356384276 and perplexity is 46.08041100090939
At time: 917.3998260498047 and batch: 750, loss is 3.8300218105316164 and perplexity is 46.063542893617
At time: 919.113575220108 and batch: 800, loss is 3.7759472799301146 and perplexity is 43.638826927410044
At time: 920.8285064697266 and batch: 850, loss is 3.8047809696197508 and perplexity is 44.91541119594859
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.361115137736003 and perplexity of 78.34445058251468
Finished 29 epochs...
Completing Train Step...
At time: 925.1741228103638 and batch: 50, loss is 3.87347092628479 and perplexity is 48.10907969209011
At time: 926.8157567977905 and batch: 100, loss is 3.840445294380188 and perplexity is 46.54619658547793
At time: 928.483626127243 and batch: 150, loss is 3.837174792289734 and perplexity is 46.39421581442498
At time: 930.1511657238007 and batch: 200, loss is 3.8910953187942505 and perplexity is 48.964488887463986
At time: 931.8303558826447 and batch: 250, loss is 3.8648020458221435 and perplexity is 47.693830305198354
At time: 933.5106112957001 and batch: 300, loss is 3.8619266176223754 and perplexity is 47.55688710011552
At time: 935.1927025318146 and batch: 350, loss is 3.8186396884918214 and perplexity is 45.54221456641699
At time: 936.8751399517059 and batch: 400, loss is 3.8236714363098145 and perplexity is 45.77194900335355
At time: 938.5941936969757 and batch: 450, loss is 3.862906141281128 and perplexity is 47.603493018240016
At time: 940.278089761734 and batch: 500, loss is 3.83231575012207 and perplexity is 46.169331167902975
At time: 941.9616827964783 and batch: 550, loss is 3.859779648780823 and perplexity is 47.45489347305809
At time: 943.6460793018341 and batch: 600, loss is 3.8892382192611694 and perplexity is 48.87364134059169
At time: 945.3339636325836 and batch: 650, loss is 3.8635770988464357 and perplexity is 47.63544365957931
At time: 947.0191440582275 and batch: 700, loss is 3.8311978673934934 and perplexity is 46.1177481072785
At time: 948.7146995067596 and batch: 750, loss is 3.83088996887207 and perplexity is 46.10355070661639
At time: 950.4074733257294 and batch: 800, loss is 3.7768620538711546 and perplexity is 43.67876485340273
At time: 952.1004872322083 and batch: 850, loss is 3.805634651184082 and perplexity is 44.95377102565102
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.361061731974284 and perplexity of 78.34026664917896
Finished 30 epochs...
Completing Train Step...
At time: 956.4207944869995 and batch: 50, loss is 3.872388768196106 and perplexity is 48.05704622166051
At time: 958.080904006958 and batch: 100, loss is 3.839104623794556 and perplexity is 46.483835281166584
At time: 959.7484340667725 and batch: 150, loss is 3.835595421791077 and perplexity is 46.32099999133406
At time: 961.4338548183441 and batch: 200, loss is 3.889591083526611 and perplexity is 48.89089014520721
At time: 963.1220147609711 and batch: 250, loss is 3.8634365367889405 and perplexity is 47.62874839416998
At time: 964.8202300071716 and batch: 300, loss is 3.8606769847869873 and perplexity is 47.49749556897882
At time: 966.5106151103973 and batch: 350, loss is 3.817588849067688 and perplexity is 45.49438214838519
At time: 968.2011206150055 and batch: 400, loss is 3.822756361961365 and perplexity is 45.730083424921865
At time: 969.8905827999115 and batch: 450, loss is 3.8620981931686402 and perplexity is 47.565047399032224
At time: 971.5809171199799 and batch: 500, loss is 3.831723937988281 and perplexity is 46.14201568112439
At time: 973.2700545787811 and batch: 550, loss is 3.8595011186599733 and perplexity is 47.4416776964255
At time: 974.9593906402588 and batch: 600, loss is 3.889166598320007 and perplexity is 48.870141089747975
At time: 976.6487393379211 and batch: 650, loss is 3.8636331510543824 and perplexity is 47.63811380620606
At time: 978.348788022995 and batch: 700, loss is 3.831323642730713 and perplexity is 46.12354894739216
At time: 980.041008234024 and batch: 750, loss is 3.8310651063919066 and perplexity is 46.11162587525467
At time: 981.7778754234314 and batch: 800, loss is 3.7770624113082887 and perplexity is 43.68751709554506
At time: 983.4602158069611 and batch: 850, loss is 3.8057682514190674 and perplexity is 44.95977726123183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3610639572143555 and perplexity of 78.34044097527347
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 987.7318916320801 and batch: 50, loss is 3.8728926467895506 and perplexity is 48.08126724022971
At time: 989.408848285675 and batch: 100, loss is 3.841589822769165 and perplexity is 46.5995005269873
At time: 991.0731105804443 and batch: 150, loss is 3.8386628341674807 and perplexity is 46.46330374055747
At time: 992.7465901374817 and batch: 200, loss is 3.892438097000122 and perplexity is 49.03028149856022
At time: 994.4260652065277 and batch: 250, loss is 3.8661331653594972 and perplexity is 47.75735876713687
At time: 996.1072700023651 and batch: 300, loss is 3.8630918216705323 and perplexity is 47.61233287402919
At time: 997.7873561382294 and batch: 350, loss is 3.820692610740662 and perplexity is 45.63580522626606
At time: 999.4666576385498 and batch: 400, loss is 3.8259540748596192 and perplexity is 45.87654915540026
At time: 1001.1481897830963 and batch: 450, loss is 3.86496648311615 and perplexity is 47.70167359444151
At time: 1002.8404610157013 and batch: 500, loss is 3.8331262731552123 and perplexity is 46.20676764375282
At time: 1004.5335958003998 and batch: 550, loss is 3.857595705986023 and perplexity is 47.35136778861397
At time: 1006.2256166934967 and batch: 600, loss is 3.8866762638092043 and perplexity is 48.74858950571618
At time: 1007.9177973270416 and batch: 650, loss is 3.861722078323364 and perplexity is 47.547160842499764
At time: 1009.6101477146149 and batch: 700, loss is 3.830039744377136 and perplexity is 46.06436899748693
At time: 1011.3053805828094 and batch: 750, loss is 3.829319143295288 and perplexity is 46.03118692030891
At time: 1012.9968993663788 and batch: 800, loss is 3.7749238777160645 and perplexity is 43.59418970012851
At time: 1014.6895134449005 and batch: 850, loss is 3.8043935966491698 and perplexity is 44.898015549207955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.360337575276692 and perplexity of 78.28355655636885
Finished 32 epochs...
Completing Train Step...
At time: 1018.998893737793 and batch: 50, loss is 3.872240214347839 and perplexity is 48.04990769274905
At time: 1020.7140610218048 and batch: 100, loss is 3.839554719924927 and perplexity is 46.50476218475731
At time: 1022.3771026134491 and batch: 150, loss is 3.836443042755127 and perplexity is 46.360279286627836
At time: 1024.0760159492493 and batch: 200, loss is 3.8904629230499266 and perplexity is 48.93353374205132
At time: 1025.7449178695679 and batch: 250, loss is 3.864095778465271 and perplexity is 47.6601576020948
At time: 1027.4331443309784 and batch: 300, loss is 3.8610633754730226 and perplexity is 47.51585170496829
At time: 1029.1177978515625 and batch: 350, loss is 3.818716735839844 and perplexity is 45.54572360845186
At time: 1030.801460981369 and batch: 400, loss is 3.8242052745819093 and perplexity is 45.796390344801196
At time: 1032.501548051834 and batch: 450, loss is 3.863491725921631 and perplexity is 47.63137705602111
At time: 1034.1904408931732 and batch: 500, loss is 3.832168793678284 and perplexity is 46.16254678569909
At time: 1035.8824257850647 and batch: 550, loss is 3.857544193267822 and perplexity is 47.348928653772425
At time: 1037.5660562515259 and batch: 600, loss is 3.887111020088196 and perplexity is 48.76978786882262
At time: 1039.2523407936096 and batch: 650, loss is 3.8623441791534425 and perplexity is 47.57674917323607
At time: 1040.9568886756897 and batch: 700, loss is 3.8309194946289065 and perplexity is 46.104911968939895
At time: 1042.6734411716461 and batch: 750, loss is 3.8302498292922973 and perplexity is 46.07404744315185
At time: 1044.3856489658356 and batch: 800, loss is 3.7758159589767457 and perplexity is 43.633096611317555
At time: 1046.0992493629456 and batch: 850, loss is 3.8053211212158202 and perplexity is 44.93967888051844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3602720896403 and perplexity of 78.27843027569938
Finished 33 epochs...
Completing Train Step...
At time: 1050.5286853313446 and batch: 50, loss is 3.8716200590133667 and perplexity is 48.02011852408314
At time: 1052.1725215911865 and batch: 100, loss is 3.8386270999908447 and perplexity is 46.461643442319385
At time: 1053.8271765708923 and batch: 150, loss is 3.8353562259674074 and perplexity is 46.30992152660201
At time: 1055.492954492569 and batch: 200, loss is 3.889448528289795 and perplexity is 48.88392098954102
At time: 1057.174029827118 and batch: 250, loss is 3.8631353521347047 and perplexity is 47.61440550609052
At time: 1058.8536486625671 and batch: 300, loss is 3.8601404857635497 and perplexity is 47.472020043398686
At time: 1060.533002614975 and batch: 350, loss is 3.8178981161117553 and perplexity is 45.50845423737842
At time: 1062.216407775879 and batch: 400, loss is 3.8234962368011476 and perplexity is 45.76393048281837
At time: 1063.8951435089111 and batch: 450, loss is 3.8629027938842775 and perplexity is 47.60333367072412
At time: 1065.5998578071594 and batch: 500, loss is 3.831814398765564 and perplexity is 46.14618991252758
At time: 1067.3528397083282 and batch: 550, loss is 3.857480149269104 and perplexity is 47.34589633614833
At time: 1069.0548186302185 and batch: 600, loss is 3.887239193916321 and perplexity is 48.77603927985569
At time: 1070.747418642044 and batch: 650, loss is 3.8625876617431643 and perplexity is 47.58833469371434
At time: 1072.4533288478851 and batch: 700, loss is 3.8312461042404173 and perplexity is 46.11997273568852
At time: 1074.1651916503906 and batch: 750, loss is 3.8305754137039183 and perplexity is 46.08905087708919
At time: 1075.8739006519318 and batch: 800, loss is 3.776133885383606 and perplexity is 43.64697093033272
At time: 1077.5884017944336 and batch: 850, loss is 3.8055886363983156 and perplexity is 44.95170253509881
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.360269546508789 and perplexity of 78.27823120360986
Finished 34 epochs...
Completing Train Step...
At time: 1081.9602394104004 and batch: 50, loss is 3.871036477088928 and perplexity is 47.99210302636721
At time: 1083.613765001297 and batch: 100, loss is 3.8379440021514895 and perplexity is 46.42991643163098
At time: 1085.2686882019043 and batch: 150, loss is 3.8345949029922486 and perplexity is 46.27467813686946
At time: 1086.937712430954 and batch: 200, loss is 3.8887331676483154 and perplexity is 48.84896386143146
At time: 1088.6285622119904 and batch: 250, loss is 3.862473530769348 and perplexity is 47.58290370066161
At time: 1090.3125400543213 and batch: 300, loss is 3.8595152950286864 and perplexity is 47.44235025190808
At time: 1091.9931766986847 and batch: 350, loss is 3.8173638820648192 and perplexity is 45.48414856473839
At time: 1093.6746845245361 and batch: 400, loss is 3.823046488761902 and perplexity is 45.743352872532334
At time: 1095.3565509319305 and batch: 450, loss is 3.8625196027755737 and perplexity is 47.58509599099836
At time: 1097.0431213378906 and batch: 500, loss is 3.831549143791199 and perplexity is 46.13395102938935
At time: 1098.735699892044 and batch: 550, loss is 3.8573580265045164 and perplexity is 47.34011467743914
At time: 1100.427635192871 and batch: 600, loss is 3.8872291612625123 and perplexity is 48.775549929194185
At time: 1102.1276128292084 and batch: 650, loss is 3.862651481628418 and perplexity is 47.59137187268909
At time: 1103.8168897628784 and batch: 700, loss is 3.8313540172576905 and perplexity is 46.124949949651246
At time: 1105.5039360523224 and batch: 750, loss is 3.8306976747512818 and perplexity is 46.09468611719957
At time: 1107.1927411556244 and batch: 800, loss is 3.7762624788284302 and perplexity is 43.65258400557539
At time: 1108.8954935073853 and batch: 850, loss is 3.8056720972061155 and perplexity is 44.95545439706891
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.360274314880371 and perplexity of 78.27860446419295
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 1113.231598854065 and batch: 50, loss is 3.8712669372558595 and perplexity is 48.00316456901558
At time: 1114.8812000751495 and batch: 100, loss is 3.839171929359436 and perplexity is 46.48696400724713
At time: 1116.5565149784088 and batch: 150, loss is 3.836232476234436 and perplexity is 46.35051839161511
At time: 1118.2354052066803 and batch: 200, loss is 3.890193281173706 and perplexity is 48.92034099094202
At time: 1119.9224562644958 and batch: 250, loss is 3.8638126945495603 and perplexity is 47.6466676875364
At time: 1121.6231412887573 and batch: 300, loss is 3.860642852783203 and perplexity is 47.49587441194715
At time: 1123.3194267749786 and batch: 350, loss is 3.8186605644226073 and perplexity is 45.543165312459934
At time: 1125.0050280094147 and batch: 400, loss is 3.824440908432007 and perplexity is 45.80718279606221
At time: 1126.6868040561676 and batch: 450, loss is 3.864000291824341 and perplexity is 47.65560691100773
At time: 1128.3860182762146 and batch: 500, loss is 3.832665638923645 and perplexity is 46.18548812625939
At time: 1130.0827369689941 and batch: 550, loss is 3.8564112520217897 and perplexity is 47.295315475570206
At time: 1131.7793655395508 and batch: 600, loss is 3.8854532861709594 and perplexity is 48.68900751200396
At time: 1133.47616648674 and batch: 650, loss is 3.860874962806702 and perplexity is 47.5068999599936
At time: 1135.1749393939972 and batch: 700, loss is 3.829732675552368 and perplexity is 46.05022623734669
At time: 1136.8739819526672 and batch: 750, loss is 3.82877872467041 and perplexity is 46.006317530118025
At time: 1138.5747644901276 and batch: 800, loss is 3.774522342681885 and perplexity is 43.57668861956086
At time: 1140.279974937439 and batch: 850, loss is 3.8041142082214354 and perplexity is 44.88547331539435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.359969139099121 and perplexity of 78.25471937468123
Finished 36 epochs...
Completing Train Step...
At time: 1144.5499007701874 and batch: 50, loss is 3.8708783054351805 and perplexity is 47.984512636372806
At time: 1146.2144401073456 and batch: 100, loss is 3.838252820968628 and perplexity is 46.444257077719946
At time: 1147.864586353302 and batch: 150, loss is 3.835266785621643 and perplexity is 46.30577973643269
At time: 1149.541635274887 and batch: 200, loss is 3.8893850564956667 and perplexity is 48.88081833783827
At time: 1151.218981742859 and batch: 250, loss is 3.8629333066940306 and perplexity is 47.60478620434836
At time: 1152.9287087917328 and batch: 300, loss is 3.8596997928619383 and perplexity is 47.45110407023937
At time: 1154.6071705818176 and batch: 350, loss is 3.8177503538131714 and perplexity is 45.50173030035973
At time: 1156.2856187820435 and batch: 400, loss is 3.82361310005188 and perplexity is 45.7692789170125
At time: 1157.9652490615845 and batch: 450, loss is 3.8632626152038574 and perplexity is 47.620465447066266
At time: 1159.644555568695 and batch: 500, loss is 3.8321132183074953 and perplexity is 46.15998135633294
At time: 1161.3252625465393 and batch: 550, loss is 3.856343641281128 and perplexity is 47.2921179123571
At time: 1163.0039081573486 and batch: 600, loss is 3.885687761306763 and perplexity is 48.70042521218356
At time: 1164.6827673912048 and batch: 650, loss is 3.861209011077881 and perplexity is 47.52277220869534
At time: 1166.3611369132996 and batch: 700, loss is 3.830249629020691 and perplexity is 46.074038215829276
At time: 1168.0392260551453 and batch: 750, loss is 3.8293940353393556 and perplexity is 46.03463441908173
At time: 1169.718514919281 and batch: 800, loss is 3.7750969886779786 and perplexity is 43.60173698548143
At time: 1171.3979768753052 and batch: 850, loss is 3.8047345781326296 and perplexity is 44.913327551560606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.359912236531575 and perplexity of 78.2502666069144
Finished 37 epochs...
Completing Train Step...
At time: 1175.6700775623322 and batch: 50, loss is 3.8705512857437134 and perplexity is 47.96882332135256
At time: 1177.3643271923065 and batch: 100, loss is 3.8377077865600584 and perplexity is 46.41895025670273
At time: 1179.0423700809479 and batch: 150, loss is 3.8346633100509644 and perplexity is 46.27784375976807
At time: 1180.7204501628876 and batch: 200, loss is 3.888848752975464 and perplexity is 48.85461041122311
At time: 1182.408131122589 and batch: 250, loss is 3.8624002599716185 and perplexity is 47.57941739107307
At time: 1184.0988540649414 and batch: 300, loss is 3.8591444301605224 and perplexity is 47.42475881316145
At time: 1185.7887477874756 and batch: 350, loss is 3.8172355365753172 and perplexity is 45.47831125403053
At time: 1187.4779014587402 and batch: 400, loss is 3.823156623840332 and perplexity is 45.74839109772618
At time: 1189.1674513816833 and batch: 450, loss is 3.8628748559951784 and perplexity is 47.60200375264492
At time: 1190.8580167293549 and batch: 500, loss is 3.831863675117493 and perplexity is 46.148463884447935
At time: 1192.5473194122314 and batch: 550, loss is 3.856319603919983 and perplexity is 47.29098114830199
At time: 1194.2645108699799 and batch: 600, loss is 3.88580931186676 and perplexity is 48.706345135917935
At time: 1195.9438095092773 and batch: 650, loss is 3.861399183273315 and perplexity is 47.531810578015424
At time: 1197.6227045059204 and batch: 700, loss is 3.83053231716156 and perplexity is 46.08706464115607
At time: 1199.3066029548645 and batch: 750, loss is 3.8296957302093504 and perplexity is 46.048524927370245
At time: 1200.9890403747559 and batch: 800, loss is 3.7753822469711302 and perplexity is 43.61417651670772
At time: 1202.6695051193237 and batch: 850, loss is 3.805008316040039 and perplexity is 44.92562371474557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.359901746114095 and perplexity of 78.24944573325538
Finished 38 epochs...
Completing Train Step...
At time: 1206.9222133159637 and batch: 50, loss is 3.870237736701965 and perplexity is 47.95378510049943
At time: 1208.6334648132324 and batch: 100, loss is 3.83729163646698 and perplexity is 46.39963702511309
At time: 1210.2988874912262 and batch: 150, loss is 3.8342018604278563 and perplexity is 46.25649379255288
At time: 1211.97754073143 and batch: 200, loss is 3.8884263849258422 and perplexity is 48.83398014179872
At time: 1213.6544163227081 and batch: 250, loss is 3.8619996023178103 and perplexity is 47.56035815170163
At time: 1215.3331723213196 and batch: 300, loss is 3.8587433528900146 and perplexity is 47.40574163427621
At time: 1217.0109026432037 and batch: 350, loss is 3.816877746582031 and perplexity is 45.46204247992813
At time: 1218.6910016536713 and batch: 400, loss is 3.8228488540649415 and perplexity is 45.73431329214621
At time: 1220.3802077770233 and batch: 450, loss is 3.8626188325881956 and perplexity is 47.58981808543954
At time: 1222.0708045959473 and batch: 500, loss is 3.8317061281204223 and perplexity is 46.14119390524026
At time: 1223.7596154212952 and batch: 550, loss is 3.856285266876221 and perplexity is 47.28935734369123
At time: 1225.4485907554626 and batch: 600, loss is 3.8858629941940306 and perplexity is 48.708959876059716
At time: 1227.137670993805 and batch: 650, loss is 3.861502766609192 and perplexity is 47.53673433652062
At time: 1228.8266489505768 and batch: 700, loss is 3.830689697265625 and perplexity is 46.094318398968944
At time: 1230.51651096344 and batch: 750, loss is 3.829856581687927 and perplexity is 46.05593249643433
At time: 1232.2059652805328 and batch: 800, loss is 3.7755384349823 and perplexity is 43.62098906020164
At time: 1233.903029680252 and batch: 850, loss is 3.8051392221450806 and perplexity is 44.93150513811135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.359901746114095 and perplexity of 78.24944573325538
Finished 39 epochs...
Completing Train Step...
At time: 1238.2065584659576 and batch: 50, loss is 3.8699389934539794 and perplexity is 47.93946137065061
At time: 1239.864337682724 and batch: 100, loss is 3.8369408321380614 and perplexity is 46.38336268630557
At time: 1241.5366306304932 and batch: 150, loss is 3.8338193035125734 and perplexity is 46.23880143535837
At time: 1243.203543663025 and batch: 200, loss is 3.888070321083069 and perplexity is 48.81659522242579
At time: 1244.8715052604675 and batch: 250, loss is 3.8616677856445314 and perplexity is 47.54457944984273
At time: 1246.5510668754578 and batch: 300, loss is 3.8584197044372557 and perplexity is 47.39040132191247
At time: 1248.2507708072662 and batch: 350, loss is 3.8165954875946047 and perplexity is 45.44921222066458
At time: 1249.934071779251 and batch: 400, loss is 3.8226110458374025 and perplexity is 45.723438589262656
At time: 1251.6245875358582 and batch: 450, loss is 3.862421975135803 and perplexity is 47.58045059715161
At time: 1253.3092334270477 and batch: 500, loss is 3.8315804290771482 and perplexity is 46.13539436581676
At time: 1254.9905784130096 and batch: 550, loss is 3.8562353897094725 and perplexity is 47.28699874335022
At time: 1256.6706140041351 and batch: 600, loss is 3.885874352455139 and perplexity is 48.7095131282863
At time: 1258.3497078418732 and batch: 650, loss is 3.8615519618988037 and perplexity is 47.53907297745808
At time: 1260.0304148197174 and batch: 700, loss is 3.830774493217468 and perplexity is 46.09822717629406
At time: 1261.716099023819 and batch: 750, loss is 3.8299459171295167 and perplexity is 46.06004710728928
At time: 1263.403170824051 and batch: 800, loss is 3.775629539489746 and perplexity is 43.62496330995756
At time: 1265.1056442260742 and batch: 850, loss is 3.805204086303711 and perplexity is 44.934419676911666
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.359905242919922 and perplexity of 78.2497193568516
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 1269.4321248531342 and batch: 50, loss is 3.8700431108474733 and perplexity is 47.944452962265274
At time: 1271.0877878665924 and batch: 100, loss is 3.83752977848053 and perplexity is 46.41068804390593
At time: 1272.74143576622 and batch: 150, loss is 3.8346529817581176 and perplexity is 46.27736579111371
At time: 1274.4117143154144 and batch: 200, loss is 3.888809084892273 and perplexity is 48.8526724809103
At time: 1276.0907564163208 and batch: 250, loss is 3.862326645851135 and perplexity is 47.57591500302291
At time: 1277.7761859893799 and batch: 300, loss is 3.858925504684448 and perplexity is 47.4143774616732
At time: 1279.487783908844 and batch: 350, loss is 3.8171112155914306 and perplexity is 45.47265769706509
At time: 1281.1696319580078 and batch: 400, loss is 3.8231094741821288 and perplexity is 45.74623412757318
At time: 1282.8522913455963 and batch: 450, loss is 3.863019185066223 and perplexity is 47.6088746014461
At time: 1284.5465545654297 and batch: 500, loss is 3.8321636629104616 and perplexity is 46.16230993699707
At time: 1286.234060049057 and batch: 550, loss is 3.8557572603225707 and perplexity is 47.26439484385755
At time: 1287.9145166873932 and batch: 600, loss is 3.8848685693740843 and perplexity is 48.66054655309976
At time: 1289.6029410362244 and batch: 650, loss is 3.860397572517395 and perplexity is 47.484226039862904
At time: 1291.2890729904175 and batch: 700, loss is 3.8296494388580324 and perplexity is 46.04639332826284
At time: 1292.9817633628845 and batch: 750, loss is 3.8286149072647095 and perplexity is 45.998781511816745
At time: 1294.675235748291 and batch: 800, loss is 3.7744466352462767 and perplexity is 43.57338966509245
At time: 1296.3686468601227 and batch: 850, loss is 3.804071083068848 and perplexity is 44.88353766424658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.359757741292317 and perplexity of 78.23817824707403
Finished 41 epochs...
Completing Train Step...
At time: 1300.6724581718445 and batch: 50, loss is 3.8698295164108276 and perplexity is 47.93421338744155
At time: 1302.3272488117218 and batch: 100, loss is 3.837154369354248 and perplexity is 46.39326831802385
At time: 1303.9938790798187 and batch: 150, loss is 3.834238815307617 and perplexity is 46.25820322730492
At time: 1305.6764523983002 and batch: 200, loss is 3.8884858083724976 and perplexity is 48.836882111434306
At time: 1307.355697631836 and batch: 250, loss is 3.8619857835769653 and perplexity is 47.559700931978824
At time: 1309.0400230884552 and batch: 300, loss is 3.8585579347610475 and perplexity is 47.396952565211635
At time: 1310.7252011299133 and batch: 350, loss is 3.8167708778381346 and perplexity is 45.45718426815345
At time: 1312.4083602428436 and batch: 400, loss is 3.8228025388717652 and perplexity is 45.73219514764282
At time: 1314.0911936759949 and batch: 450, loss is 3.862741956710815 and perplexity is 47.59567790077185
At time: 1315.7776651382446 and batch: 500, loss is 3.8319442558288572 and perplexity is 46.152182710326926
At time: 1317.4611580371857 and batch: 550, loss is 3.855722122192383 and perplexity is 47.26273409057634
At time: 1319.1433210372925 and batch: 600, loss is 3.884960217475891 and perplexity is 48.665006404189555
At time: 1320.8276925086975 and batch: 650, loss is 3.8605452489852907 and perplexity is 47.49123886044677
At time: 1322.5584955215454 and batch: 700, loss is 3.8298708391189575 and perplexity is 46.05658914039646
At time: 1324.2482237815857 and batch: 750, loss is 3.8288874340057375 and perplexity is 46.01131911817232
At time: 1325.9322040081024 and batch: 800, loss is 3.7747160959243775 and perplexity is 43.58513256227222
At time: 1327.613784313202 and batch: 850, loss is 3.8043459367752077 and perplexity is 44.89587576643703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3597151438395185 and perplexity of 78.23484557095135
Finished 42 epochs...
Completing Train Step...
At time: 1331.8770163059235 and batch: 50, loss is 3.8696534633636475 and perplexity is 47.92577516591961
At time: 1333.543259382248 and batch: 100, loss is 3.8368790340423584 and perplexity is 46.38049637138657
At time: 1335.2080538272858 and batch: 150, loss is 3.8339393949508667 and perplexity is 46.244354652967964
At time: 1336.8771455287933 and batch: 200, loss is 3.8882304716110228 and perplexity is 48.82441385198568
At time: 1338.5600457191467 and batch: 250, loss is 3.8617284202575686 and perplexity is 47.547462384421614
At time: 1340.2454919815063 and batch: 300, loss is 3.858282070159912 and perplexity is 47.383879227115706
At time: 1341.929542541504 and batch: 350, loss is 3.8165184926986693 and perplexity is 45.445712998012176
At time: 1343.613464832306 and batch: 400, loss is 3.8225767183303834 and perplexity is 45.72186904454298
At time: 1345.29824757576 and batch: 450, loss is 3.862544846534729 and perplexity is 47.58629723286281
At time: 1346.9939427375793 and batch: 500, loss is 3.8318043041229246 and perplexity is 46.145724085582415
At time: 1348.6922717094421 and batch: 550, loss is 3.855702633857727 and perplexity is 47.26181302757265
At time: 1350.3896946907043 and batch: 600, loss is 3.88502272605896 and perplexity is 48.66804847986184
At time: 1352.094605922699 and batch: 650, loss is 3.860646662712097 and perplexity is 47.49605536819614
At time: 1353.7939794063568 and batch: 700, loss is 3.830027027130127 and perplexity is 46.06378318925301
At time: 1355.4918575286865 and batch: 750, loss is 3.8290693235397337 and perplexity is 46.01968885672612
At time: 1357.1893780231476 and batch: 800, loss is 3.774891061782837 and perplexity is 43.59275913958291
At time: 1358.8860800266266 and batch: 850, loss is 3.804518852233887 and perplexity is 44.903639628614535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.359701156616211 and perplexity of 78.2337512903489
Finished 43 epochs...
Completing Train Step...
At time: 1363.2063460350037 and batch: 50, loss is 3.869488821029663 and perplexity is 47.917885203966975
At time: 1364.8778145313263 and batch: 100, loss is 3.8366497278213503 and perplexity is 46.36986225431636
At time: 1366.5252015590668 and batch: 150, loss is 3.8336909437179565 and perplexity is 46.232866613207214
At time: 1368.198487997055 and batch: 200, loss is 3.888011441230774 and perplexity is 48.81372099312747
At time: 1369.880808353424 and batch: 250, loss is 3.861515212059021 and perplexity is 47.53732595624431
At time: 1371.5625882148743 and batch: 300, loss is 3.858058924674988 and perplexity is 47.373306908034486
At time: 1373.2532515525818 and batch: 350, loss is 3.816317024230957 and perplexity is 45.43655804209881
At time: 1374.9347460269928 and batch: 400, loss is 3.8223994064331053 and perplexity is 45.713762731889666
At time: 1376.619622707367 and batch: 450, loss is 3.862393970489502 and perplexity is 47.57911814211934
At time: 1378.3095309734344 and batch: 500, loss is 3.8317044878005984 and perplexity is 46.141118218987266
At time: 1379.9919066429138 and batch: 550, loss is 3.855685677528381 and perplexity is 47.26101164749972
At time: 1381.6753821372986 and batch: 600, loss is 3.885063691139221 and perplexity is 48.67004221121038
At time: 1383.3578057289124 and batch: 650, loss is 3.8607169246673583 and perplexity is 47.49939265115413
At time: 1385.0396671295166 and batch: 700, loss is 3.8301382637023926 and perplexity is 46.06890745159798
At time: 1386.7211184501648 and batch: 750, loss is 3.829193425178528 and perplexity is 46.0254003299243
At time: 1388.402927160263 and batch: 800, loss is 3.775010557174683 and perplexity is 43.59796858466409
At time: 1390.0889160633087 and batch: 850, loss is 3.804630255699158 and perplexity is 44.90864232832631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.35969607035319 and perplexity of 78.23335337392471
Finished 44 epochs...
Completing Train Step...
At time: 1394.4213047027588 and batch: 50, loss is 3.869331188201904 and perplexity is 47.91033236752517
At time: 1396.071784734726 and batch: 100, loss is 3.8364487409591677 and perplexity is 46.360543457711245
At time: 1397.732944726944 and batch: 150, loss is 3.8334740734100343 and perplexity is 46.22284116433907
At time: 1399.398808479309 and batch: 200, loss is 3.887816228866577 and perplexity is 48.804192881280045
At time: 1401.09374666214 and batch: 250, loss is 3.861329588890076 and perplexity is 47.528502746078594
At time: 1402.784590959549 and batch: 300, loss is 3.8578688859939576 and perplexity is 47.36430500265585
At time: 1404.4638950824738 and batch: 350, loss is 3.8161475372314455 and perplexity is 45.42885778877297
At time: 1406.142769575119 and batch: 400, loss is 3.8222524642944338 and perplexity is 45.707045947328545
At time: 1407.8849866390228 and batch: 450, loss is 3.862270932197571 and perplexity is 47.57326444881314
At time: 1409.5757558345795 and batch: 500, loss is 3.831626224517822 and perplexity is 46.13750720491123
At time: 1411.264924287796 and batch: 550, loss is 3.8556667518615724 and perplexity is 47.26011720980417
At time: 1412.9643819332123 and batch: 600, loss is 3.8850883293151854 and perplexity is 48.67124136704702
At time: 1414.654417514801 and batch: 650, loss is 3.86076518535614 and perplexity is 47.50168505987636
At time: 1416.3569736480713 and batch: 700, loss is 3.8302181911468507 and perplexity is 46.0725897687967
At time: 1418.0556843280792 and batch: 750, loss is 3.829280333518982 and perplexity is 46.029400494907044
At time: 1419.7462420463562 and batch: 800, loss is 3.7750949478149414 and perplexity is 43.60164800039886
At time: 1421.4510335922241 and batch: 850, loss is 3.8047034215927122 and perplexity is 44.911928229477056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.359695752461751 and perplexity of 78.23332850421535
Finished 45 epochs...
Completing Train Step...
At time: 1425.767368555069 and batch: 50, loss is 3.869179048538208 and perplexity is 47.903043860120796
At time: 1427.4242675304413 and batch: 100, loss is 3.836266961097717 and perplexity is 46.35211681046532
At time: 1429.0744824409485 and batch: 150, loss is 3.833279356956482 and perplexity is 46.21384169283545
At time: 1430.734914302826 and batch: 200, loss is 3.8876381969451903 and perplexity is 48.79550495043711
At time: 1432.4004926681519 and batch: 250, loss is 3.8611622428894044 and perplexity is 47.520549706699356
At time: 1434.076031923294 and batch: 300, loss is 3.8577006244659424 and perplexity is 47.35633608277284
At time: 1435.7570176124573 and batch: 350, loss is 3.8159995746612547 and perplexity is 45.42213651547441
At time: 1437.44296169281 and batch: 400, loss is 3.8221256828308103 and perplexity is 45.701251508466925
At time: 1439.1226603984833 and batch: 450, loss is 3.8621656703948974 and perplexity is 47.56825706478596
At time: 1440.812705039978 and batch: 500, loss is 3.8315600395202636 and perplexity is 46.1344536951589
At time: 1442.4964241981506 and batch: 550, loss is 3.85564444065094 and perplexity is 47.25906279113732
At time: 1444.174792766571 and batch: 600, loss is 3.8851002645492554 and perplexity is 48.67182227317183
At time: 1445.851140499115 and batch: 650, loss is 3.8607973194122316 and perplexity is 47.503211506213844
At time: 1447.52592253685 and batch: 700, loss is 3.830275363922119 and perplexity is 46.07522394191836
At time: 1449.2105023860931 and batch: 750, loss is 3.829341797828674 and perplexity is 46.032229747182136
At time: 1450.9358129501343 and batch: 800, loss is 3.7751560831069946 and perplexity is 43.60431368136612
At time: 1452.6352891921997 and batch: 850, loss is 3.8047520542144775 and perplexity is 44.91411246740756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.35969607035319 and perplexity of 78.23335337392471
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 1456.9900298118591 and batch: 50, loss is 3.869224796295166 and perplexity is 47.905235367056726
At time: 1458.6483550071716 and batch: 100, loss is 3.836544508934021 and perplexity is 46.36498352567606
At time: 1460.3187744617462 and batch: 150, loss is 3.8336853075027464 and perplexity is 46.23260603555554
At time: 1461.9844191074371 and batch: 200, loss is 3.8880060291290284 and perplexity is 48.81345680901778
At time: 1463.6544008255005 and batch: 250, loss is 3.8614883279800414 and perplexity is 47.536047976197565
At time: 1465.338061094284 and batch: 300, loss is 3.8579379415512083 and perplexity is 47.36757588406656
At time: 1467.0280373096466 and batch: 350, loss is 3.8162286853790284 and perplexity is 45.4325444060084
At time: 1468.7266416549683 and batch: 400, loss is 3.8223053073883055 and perplexity is 45.70946131286527
At time: 1470.4265851974487 and batch: 450, loss is 3.8623943376541137 and perplexity is 47.57913561149098
At time: 1472.122622013092 and batch: 500, loss is 3.8318314599990844 and perplexity is 46.14697723016603
At time: 1473.820689201355 and batch: 550, loss is 3.855381269454956 and perplexity is 47.24662720347751
At time: 1475.517050743103 and batch: 600, loss is 3.88455548286438 and perplexity is 48.64531397710059
At time: 1477.2150297164917 and batch: 650, loss is 3.8601455974578855 and perplexity is 47.47226270647486
At time: 1478.91699051857 and batch: 700, loss is 3.8296263074874877 and perplexity is 46.04532822439521
At time: 1480.6186182498932 and batch: 750, loss is 3.8285847187042235 and perplexity is 45.99739289577905
At time: 1482.3183827400208 and batch: 800, loss is 3.774454073905945 and perplexity is 43.5737137939143
At time: 1484.0168669223785 and batch: 850, loss is 3.804086046218872 and perplexity is 44.88420926837892
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.359651883443196 and perplexity of 78.22989656015416
Finished 47 epochs...
Completing Train Step...
At time: 1488.4784305095673 and batch: 50, loss is 3.8691270971298217 and perplexity is 47.90055529416909
At time: 1490.141175031662 and batch: 100, loss is 3.836391634941101 and perplexity is 46.357896067270644
At time: 1491.7706170082092 and batch: 150, loss is 3.8335049533843994 and perplexity is 46.22426854652796
At time: 1493.4581587314606 and batch: 200, loss is 3.887871780395508 and perplexity is 48.806904104118416
At time: 1495.1153481006622 and batch: 250, loss is 3.861350169181824 and perplexity is 47.52948090659684
At time: 1496.7791690826416 and batch: 300, loss is 3.8577950048446654 and perplexity is 47.36080580263088
At time: 1498.445648431778 and batch: 350, loss is 3.816096611022949 and perplexity is 45.426544328197856
At time: 1500.1251938343048 and batch: 400, loss is 3.8221935749053957 and perplexity is 45.70435436657152
At time: 1501.8048889636993 and batch: 450, loss is 3.8622917985916136 and perplexity is 47.574257141651955
At time: 1503.4847342967987 and batch: 500, loss is 3.831746063232422 and perplexity is 46.14303659578044
At time: 1505.1671075820923 and batch: 550, loss is 3.855367522239685 and perplexity is 47.245977698386966
At time: 1506.8561577796936 and batch: 600, loss is 3.8845914030075073 and perplexity is 48.64706135512398
At time: 1508.5460731983185 and batch: 650, loss is 3.860203895568848 and perplexity is 47.475030330386595
At time: 1510.235737323761 and batch: 700, loss is 3.8297121143341064 and perplexity is 46.04927939832814
At time: 1511.924558877945 and batch: 750, loss is 3.8286871004104612 and perplexity is 46.002102428427094
At time: 1513.6127898693085 and batch: 800, loss is 3.7745624017715453 and perplexity is 43.578434297002275
At time: 1515.3014407157898 and batch: 850, loss is 3.804186840057373 and perplexity is 44.88873354812509
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.359628041585286 and perplexity of 78.22803143631015
Finished 48 epochs...
Completing Train Step...
At time: 1519.5910980701447 and batch: 50, loss is 3.869038166999817 and perplexity is 47.896295680966254
At time: 1521.2767097949982 and batch: 100, loss is 3.8362629890441893 and perplexity is 46.351932697741866
At time: 1522.9392230510712 and batch: 150, loss is 3.8333618354797365 and perplexity is 46.217653499446136
At time: 1524.6020362377167 and batch: 200, loss is 3.8877529382705687 and perplexity is 48.80110413257028
At time: 1526.265299797058 and batch: 250, loss is 3.8612327480316164 and perplexity is 47.52390026792894
At time: 1527.9300045967102 and batch: 300, loss is 3.8576710176467897 and perplexity is 47.35493403304992
At time: 1529.6094489097595 and batch: 350, loss is 3.8159851121902464 and perplexity is 45.421479603892216
At time: 1531.29176902771 and batch: 400, loss is 3.822095718383789 and perplexity is 45.69988211625393
At time: 1532.9714331626892 and batch: 450, loss is 3.862205495834351 and perplexity is 47.570151529251284
At time: 1534.679783821106 and batch: 500, loss is 3.8316825342178347 and perplexity is 46.140105267248636
At time: 1536.3596954345703 and batch: 550, loss is 3.8553556776046753 and perplexity is 47.24541809033963
At time: 1538.039517402649 and batch: 600, loss is 3.8846164655685427 and perplexity is 48.6482805903469
At time: 1539.719046831131 and batch: 650, loss is 3.8602494287490843 and perplexity is 47.477192068714395
At time: 1541.3991978168488 and batch: 700, loss is 3.8297817945480346 and perplexity is 46.05248823376272
At time: 1543.0786893367767 and batch: 750, loss is 3.828769178390503 and perplexity is 46.0058783430297
At time: 1544.7589466571808 and batch: 800, loss is 3.774645471572876 and perplexity is 43.58205449924428
At time: 1546.445785999298 and batch: 850, loss is 3.804265365600586 and perplexity is 44.89225859871253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.359615325927734 and perplexity of 78.22703672177569
Finished 49 epochs...
Completing Train Step...
At time: 1550.7545976638794 and batch: 50, loss is 3.8689534854888916 and perplexity is 47.89223992200659
At time: 1552.431215763092 and batch: 100, loss is 3.83614764213562 and perplexity is 46.346586453941256
At time: 1554.0937798023224 and batch: 150, loss is 3.833236026763916 and perplexity is 46.21183928155843
At time: 1555.7788772583008 and batch: 200, loss is 3.8876447057724 and perplexity is 48.79582255298105
At time: 1557.463781118393 and batch: 250, loss is 3.861127552986145 and perplexity is 47.51890125201974
At time: 1559.1524896621704 and batch: 300, loss is 3.8575604009628295 and perplexity is 47.349696076986035
At time: 1560.8397629261017 and batch: 350, loss is 3.8158866357803345 and perplexity is 45.417006879880454
At time: 1562.52326130867 and batch: 400, loss is 3.822008843421936 and perplexity is 45.69591211318783
At time: 1564.2060105800629 and batch: 450, loss is 3.862130250930786 and perplexity is 47.56657225244976
At time: 1565.8897292613983 and batch: 500, loss is 3.8316299438476564 and perplexity is 46.13767880583738
At time: 1567.5735731124878 and batch: 550, loss is 3.8553446435928347 and perplexity is 47.24489678671305
At time: 1569.2662682533264 and batch: 600, loss is 3.8846356010437013 and perplexity is 48.64921150721838
At time: 1570.9505932331085 and batch: 650, loss is 3.8602856397628784 and perplexity is 47.47891129709861
At time: 1572.6440591812134 and batch: 700, loss is 3.829839310646057 and perplexity is 46.055137069364775
At time: 1574.341067790985 and batch: 750, loss is 3.8288358736038206 and perplexity is 46.00894681722478
At time: 1576.037302017212 and batch: 800, loss is 3.7747119188308718 and perplexity is 43.58495050347829
At time: 1577.775433063507 and batch: 850, loss is 3.804327464103699 and perplexity is 44.89504642733197
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.359609285990397 and perplexity of 78.22656423680273
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f37e976d898>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -76.178723085063, 'params': {'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 7.747554741775914, 'batch_size': 50, 'dropout': 0.8227747629004882, 'wordvec_source': 'glove', 'data': 'wikitext', 'lr': 4.8626202683209865, 'num_layers': 1, 'seq_len': 50}}, {'best_accuracy': -215.2900931531869, 'params': {'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 7.4726475107090335, 'batch_size': 50, 'dropout': 0.8525503454773354, 'wordvec_source': 'glove', 'data': 'wikitext', 'lr': 21.82125557023854, 'num_layers': 1, 'seq_len': 50}}, {'best_accuracy': -180.84915883095948, 'params': {'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 4.002509243384625, 'batch_size': 50, 'dropout': 0.40333487804001933, 'wordvec_source': 'glove', 'data': 'wikitext', 'lr': 29.518967871004783, 'num_layers': 1, 'seq_len': 50}}, {'best_accuracy': -131.38432151764212, 'params': {'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 3.7725787429032467, 'batch_size': 50, 'dropout': 0.525927678662764, 'wordvec_source': 'glove', 'data': 'wikitext', 'lr': 14.391874180920635, 'num_layers': 1, 'seq_len': 50}}, {'best_accuracy': -76.37330910686774, 'params': {'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 7.503277863676992, 'batch_size': 50, 'dropout': 0.823769304253686, 'wordvec_source': 'glove', 'data': 'wikitext', 'lr': 2.862125934743378, 'num_layers': 1, 'seq_len': 50}}, {'best_accuracy': -78.22656423680273, 'params': {'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 2.0, 'batch_size': 50, 'dropout': 0.0, 'wordvec_source': 'glove', 'data': 'wikitext', 'lr': 4.333155033503922, 'num_layers': 1, 'seq_len': 50}}]
