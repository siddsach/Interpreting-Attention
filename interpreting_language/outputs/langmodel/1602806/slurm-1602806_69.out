Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'type': 'continuous', 'domain': [0, 30], 'name': 'lr'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'dropout'}, {'type': 'continuous', 'domain': [2, 8], 'name': 'anneal'}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'anneal': 5.219921432741684, 'wordvec_source': '', 'data': 'wikitext', 'lr': 16.982241097578665, 'num_layers': 1, 'batch_size': 80, 'dropout': 0.7890174429212653, 'wordvec_dim': 200, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 3.2894351482391357 and batch: 50, loss is 7.338991270065308 and perplexity is 1539.1587348379892
At time: 5.769628286361694 and batch: 100, loss is 6.67291971206665 and perplexity is 790.700856320474
At time: 8.225772857666016 and batch: 150, loss is 6.6150940990448 and perplexity is 746.2749507918279
At time: 10.685329914093018 and batch: 200, loss is 6.563276643753052 and perplexity is 708.5896908815348
At time: 13.16685700416565 and batch: 250, loss is 6.47351565361023 and perplexity is 647.7570178703451
At time: 15.63613748550415 and batch: 300, loss is 6.458046569824218 and perplexity is 637.8139139171083
At time: 18.10071039199829 and batch: 350, loss is 6.408833703994751 and perplexity is 607.1851103715376
At time: 20.55953550338745 and batch: 400, loss is 6.419433612823486 and perplexity is 613.6554491011711
At time: 23.01905918121338 and batch: 450, loss is 6.404248199462891 and perplexity is 604.4072341457595
At time: 25.479838371276855 and batch: 500, loss is 6.363329038619995 and perplexity is 580.1745686117197
At time: 27.942029237747192 and batch: 550, loss is 6.331166496276856 and perplexity is 561.8115628656418
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.739204915364583 and perplexity of 310.8171867427172
Finished 1 epochs...
Completing Train Step...
At time: 31.935439348220825 and batch: 50, loss is 6.013260087966919 and perplexity is 408.81391949982276
At time: 34.39343214035034 and batch: 100, loss is 5.890892858505249 and perplexity is 361.72811225264275
At time: 36.84740662574768 and batch: 150, loss is 5.870879917144776 and perplexity is 354.56082719863207
At time: 39.301464557647705 and batch: 200, loss is 5.841149578094482 and perplexity is 344.1747691087214
At time: 41.75659418106079 and batch: 250, loss is 5.762621393203736 and perplexity is 318.18131498549826
At time: 44.215871810913086 and batch: 300, loss is 5.744672698974609 and perplexity is 312.52132253933087
At time: 46.67266774177551 and batch: 350, loss is 5.701584129333496 and perplexity is 299.3412207806045
At time: 49.130043506622314 and batch: 400, loss is 5.677935180664062 and perplexity is 292.34516633745045
At time: 51.58730173110962 and batch: 450, loss is 5.6570938873291015 and perplexity is 286.3153676293535
At time: 54.04370641708374 and batch: 500, loss is 5.649448480606079 and perplexity is 284.1347167960304
At time: 56.50064420700073 and batch: 550, loss is 5.617992792129517 and perplexity is 275.33617128479625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.390255737304687 and perplexity of 219.2594512056882
Finished 2 epochs...
Completing Train Step...
At time: 60.41968011856079 and batch: 50, loss is 5.64481125831604 and perplexity is 282.8201712295773
At time: 62.90178871154785 and batch: 100, loss is 5.5912044239044185 and perplexity is 268.05828116567216
At time: 65.36023950576782 and batch: 150, loss is 5.5883761978149415 and perplexity is 267.30122281210623
At time: 67.81728029251099 and batch: 200, loss is 5.601947193145752 and perplexity is 270.9534928751702
At time: 70.29912567138672 and batch: 250, loss is 5.588278551101684 and perplexity is 267.2751230005501
At time: 72.75656008720398 and batch: 300, loss is 5.594998359680176 and perplexity is 269.0772087192248
At time: 75.21314787864685 and batch: 350, loss is 5.546285009384155 and perplexity is 256.2836936911631
At time: 77.66874527931213 and batch: 400, loss is 5.529758806228638 and perplexity is 252.08310281626217
At time: 80.12835097312927 and batch: 450, loss is 5.511910429000855 and perplexity is 247.6237430542675
At time: 82.58492088317871 and batch: 500, loss is 5.50030481338501 and perplexity is 244.766529008844
At time: 85.0414526462555 and batch: 550, loss is 5.44252875328064 and perplexity is 231.0256523015183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.2651006062825525 and perplexity of 193.46577168045056
Finished 3 epochs...
Completing Train Step...
At time: 89.00708341598511 and batch: 50, loss is 5.508378314971924 and perplexity is 246.75065059630128
At time: 91.46631097793579 and batch: 100, loss is 5.468466911315918 and perplexity is 237.09642414274995
At time: 93.93417549133301 and batch: 150, loss is 5.518051977157593 and perplexity is 249.14921578421337
At time: 96.39294600486755 and batch: 200, loss is 5.505114450454712 and perplexity is 245.94660276858926
At time: 98.85159826278687 and batch: 250, loss is 5.443866348266601 and perplexity is 231.3348778188115
At time: 101.31114530563354 and batch: 300, loss is 5.47735366821289 and perplexity is 239.21283250003407
At time: 103.7694525718689 and batch: 350, loss is 5.480843906402588 and perplexity is 240.0492009770543
At time: 106.2285668849945 and batch: 400, loss is 5.509470834732055 and perplexity is 247.02037787226365
At time: 108.68600559234619 and batch: 450, loss is 5.500557870864868 and perplexity is 244.82847684767998
At time: 111.14305639266968 and batch: 500, loss is 5.445158452987671 and perplexity is 231.63397990058175
At time: 113.60010290145874 and batch: 550, loss is 5.463030014038086 and perplexity is 235.81085316519113
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.377907307942708 and perplexity of 216.56858951093582
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 117.54299640655518 and batch: 50, loss is 5.4612648391723635 and perplexity is 235.3949729327708
At time: 119.99948906898499 and batch: 100, loss is 5.369956226348877 and perplexity is 214.85346257797502
At time: 122.48006749153137 and batch: 150, loss is 5.364660444259644 and perplexity is 213.71865296343316
At time: 124.9343786239624 and batch: 200, loss is 5.368886976242066 and perplexity is 214.62385326693092
At time: 127.38994860649109 and batch: 250, loss is 5.3068016910552975 and perplexity is 201.70408367872716
At time: 129.84711718559265 and batch: 300, loss is 5.293122844696045 and perplexity is 198.9637892692538
At time: 132.30204844474792 and batch: 350, loss is 5.22689302444458 and perplexity is 186.21334349590953
At time: 134.75829648971558 and batch: 400, loss is 5.221208076477051 and perplexity is 185.15773371360615
At time: 137.213858127594 and batch: 450, loss is 5.226313953399658 and perplexity is 186.10554395532154
At time: 139.66806721687317 and batch: 500, loss is 5.1650010681152345 and perplexity is 175.03764553207358
At time: 142.12609314918518 and batch: 550, loss is 5.203237648010254 and perplexity is 181.86008869541456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.162885030110677 and perplexity of 174.66765082139193
Finished 5 epochs...
Completing Train Step...
At time: 146.03526544570923 and batch: 50, loss is 5.299286890029907 and perplexity is 200.19399872433723
At time: 148.52813959121704 and batch: 100, loss is 5.242277641296386 and perplexity is 189.1003149712687
At time: 150.98381304740906 and batch: 150, loss is 5.263551549911499 and perplexity is 193.16631429231657
At time: 153.44501209259033 and batch: 200, loss is 5.277093687057495 and perplexity is 195.79999157298064
At time: 155.90244221687317 and batch: 250, loss is 5.22046350479126 and perplexity is 185.01992181946164
At time: 158.35859727859497 and batch: 300, loss is 5.2191611003875735 and perplexity is 184.77910791110182
At time: 160.81242847442627 and batch: 350, loss is 5.165779781341553 and perplexity is 175.1740027464457
At time: 163.26769971847534 and batch: 400, loss is 5.175030889511109 and perplexity is 176.8020755217602
At time: 165.72525238990784 and batch: 450, loss is 5.179956369400024 and perplexity is 177.67505875483403
At time: 168.18183135986328 and batch: 500, loss is 5.140103693008423 and perplexity is 170.73347127074553
At time: 170.63541293144226 and batch: 550, loss is 5.156639537811279 and perplexity is 173.5801648308097
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.145652262369792 and perplexity of 171.68343079939697
Finished 6 epochs...
Completing Train Step...
At time: 174.53844571113586 and batch: 50, loss is 5.242280406951904 and perplexity is 189.10083795832142
At time: 177.02015089988708 and batch: 100, loss is 5.192762327194214 and perplexity is 179.964989140052
At time: 179.47676372528076 and batch: 150, loss is 5.216150121688843 and perplexity is 184.2235787161991
At time: 181.95930576324463 and batch: 200, loss is 5.229280700683594 and perplexity is 186.65849189520745
At time: 184.41886138916016 and batch: 250, loss is 5.179500408172608 and perplexity is 177.5940642835354
At time: 186.87890768051147 and batch: 300, loss is 5.180926876068115 and perplexity is 177.8475772857604
At time: 189.33436393737793 and batch: 350, loss is 5.1294755935668945 and perplexity is 168.92850761684943
At time: 191.79074931144714 and batch: 400, loss is 5.145670108795166 and perplexity is 171.68649476227299
At time: 194.24798464775085 and batch: 450, loss is 5.145800275802612 and perplexity is 171.70884413405904
At time: 196.70417952537537 and batch: 500, loss is 5.112729578018189 and perplexity is 166.12318276603585
At time: 199.16117334365845 and batch: 550, loss is 5.119502601623535 and perplexity is 167.2521579763134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.137713623046875 and perplexity of 170.32589359323424
Finished 7 epochs...
Completing Train Step...
At time: 203.0902693271637 and batch: 50, loss is 5.200500659942627 and perplexity is 181.3630203480445
At time: 205.55010056495667 and batch: 100, loss is 5.156347932815552 and perplexity is 173.52955536693077
At time: 208.01241159439087 and batch: 150, loss is 5.18047911643982 and perplexity is 177.76796214621444
At time: 210.472261428833 and batch: 200, loss is 5.19218560218811 and perplexity is 179.86122875407236
At time: 212.93149709701538 and batch: 250, loss is 5.146534748077393 and perplexity is 171.83500584487803
At time: 215.38929629325867 and batch: 300, loss is 5.14899094581604 and perplexity is 172.25758535462418
At time: 217.85048580169678 and batch: 350, loss is 5.101722602844238 and perplexity is 164.30469539979867
At time: 220.3105010986328 and batch: 400, loss is 5.118358554840088 and perplexity is 167.0609230946398
At time: 222.77004408836365 and batch: 450, loss is 5.115606718063354 and perplexity is 166.6018306659379
At time: 225.23161697387695 and batch: 500, loss is 5.087331609725952 and perplexity is 161.95712014390426
At time: 227.6931893825531 and batch: 550, loss is 5.087149238586425 and perplexity is 161.92758653247017
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.133524576822917 and perplexity of 169.613882915858
Finished 8 epochs...
Completing Train Step...
At time: 231.60825729370117 and batch: 50, loss is 5.165283164978027 and perplexity is 175.08703006803265
At time: 234.09159111976624 and batch: 100, loss is 5.124565477371216 and perplexity is 168.10108205589475
At time: 236.55036973953247 and batch: 150, loss is 5.149921598434449 and perplexity is 172.41797194800066
At time: 239.00841450691223 and batch: 200, loss is 5.15961709022522 and perplexity is 174.09777909899802
At time: 241.46555829048157 and batch: 250, loss is 5.117985677719116 and perplexity is 166.9986415110198
At time: 243.9483504295349 and batch: 300, loss is 5.121658973693847 and perplexity is 167.6132049944221
At time: 246.40592765808105 and batch: 350, loss is 5.077734289169311 and perplexity is 160.41020075974706
At time: 248.86433291435242 and batch: 400, loss is 5.09511664390564 and perplexity is 163.22288246854265
At time: 251.32343101501465 and batch: 450, loss is 5.089985160827637 and perplexity is 162.38745234021528
At time: 253.78253936767578 and batch: 500, loss is 5.064707193374634 and perplexity is 158.33407401968202
At time: 256.2400414943695 and batch: 550, loss is 5.059634246826172 and perplexity is 157.5328876314374
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1307729085286455 and perplexity of 169.147803314285
Finished 9 epochs...
Completing Train Step...
At time: 260.17479395866394 and batch: 50, loss is 5.135867729187011 and perplexity is 170.01178007137588
At time: 262.6316750049591 and batch: 100, loss is 5.098287601470947 and perplexity is 163.74127677133566
At time: 265.0883219242096 and batch: 150, loss is 5.123239259719849 and perplexity is 167.8782912008953
At time: 267.544739484787 and batch: 200, loss is 5.13279052734375 and perplexity is 169.48942361867378
At time: 270.0027222633362 and batch: 250, loss is 5.094087543487549 and perplexity is 163.05499613271814
At time: 272.46144914627075 and batch: 300, loss is 5.098406791687012 and perplexity is 163.76079429261847
At time: 274.9310224056244 and batch: 350, loss is 5.056131505966187 and perplexity is 156.9820560225193
At time: 277.38825726509094 and batch: 400, loss is 5.07380669593811 and perplexity is 159.7814103657843
At time: 279.8421640396118 and batch: 450, loss is 5.066590709686279 and perplexity is 158.6325798628506
At time: 282.2971279621124 and batch: 500, loss is 5.043657693862915 and perplexity is 155.03605361872633
At time: 284.75366950035095 and batch: 550, loss is 5.035726490020752 and perplexity is 153.81129440227386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.131124877929688 and perplexity of 169.20734864375743
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 288.6723401546478 and batch: 50, loss is 5.110204544067383 and perplexity is 165.70424522787758
At time: 291.1558828353882 and batch: 100, loss is 5.065082120895386 and perplexity is 158.39344895145717
At time: 293.61669850349426 and batch: 150, loss is 5.076502113342285 and perplexity is 160.21266890993977
At time: 296.07888102531433 and batch: 200, loss is 5.0685461807250975 and perplexity is 158.9430847713193
At time: 298.5366222858429 and batch: 250, loss is 5.020655288696289 and perplexity is 151.51055442527618
At time: 300.994083404541 and batch: 300, loss is 5.010532913208007 and perplexity is 149.98464365287438
At time: 303.451735496521 and batch: 350, loss is 4.949155931472778 and perplexity is 141.0558528536226
At time: 305.93404626846313 and batch: 400, loss is 4.9503845119476315 and perplexity is 141.2292578194684
At time: 308.3948464393616 and batch: 450, loss is 4.931080265045166 and perplexity is 138.52907966173834
At time: 310.85493564605713 and batch: 500, loss is 4.9140502548217775 and perplexity is 136.18990265699452
At time: 313.31480836868286 and batch: 550, loss is 4.952042093276978 and perplexity is 141.4635509266826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.0659428914388025 and perplexity of 158.52984806233138
Finished 11 epochs...
Completing Train Step...
At time: 317.2700126171112 and batch: 50, loss is 5.061158199310302 and perplexity is 157.7731432894809
At time: 319.73409152030945 and batch: 100, loss is 5.0229707622528075 and perplexity is 151.86177957693485
At time: 322.1905035972595 and batch: 150, loss is 5.03867202758789 and perplexity is 154.2650192519305
At time: 324.6472764015198 and batch: 200, loss is 5.034158620834351 and perplexity is 153.5703273650187
At time: 327.1050171852112 and batch: 250, loss is 4.994726200103759 and perplexity is 147.63251808109737
At time: 329.5624625682831 and batch: 300, loss is 4.988486452102661 and perplexity is 146.71419639897152
At time: 332.0175096988678 and batch: 350, loss is 4.933360376358032 and perplexity is 138.84530175722392
At time: 334.4794590473175 and batch: 400, loss is 4.945553474426269 and perplexity is 140.54861939449333
At time: 336.9446129798889 and batch: 450, loss is 4.9353759765625 and perplexity is 139.12544060495247
At time: 339.4073066711426 and batch: 500, loss is 4.920928621292115 and perplexity is 137.12989581968424
At time: 341.871440410614 and batch: 550, loss is 4.948698787689209 and perplexity is 140.99138478406587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.062182108561198 and perplexity of 157.93477140257727
Finished 12 epochs...
Completing Train Step...
At time: 345.82411527633667 and batch: 50, loss is 5.044593009948731 and perplexity is 155.18112916872886
At time: 348.3121132850647 and batch: 100, loss is 5.005675315856934 and perplexity is 149.25784532113244
At time: 350.7717659473419 and batch: 150, loss is 5.022312669754029 and perplexity is 151.76187335631693
At time: 353.2338602542877 and batch: 200, loss is 5.018828744888306 and perplexity is 151.234066345911
At time: 355.69320154190063 and batch: 250, loss is 4.983396425247192 and perplexity is 145.96931454177326
At time: 358.1550693511963 and batch: 300, loss is 4.979826364517212 and perplexity is 145.44912433196535
At time: 360.61645436286926 and batch: 350, loss is 4.928272094726562 and perplexity is 138.14061210877685
At time: 363.08147740364075 and batch: 400, loss is 4.945227479934692 and perplexity is 140.50280878618062
At time: 365.5382196903229 and batch: 450, loss is 4.936554756164551 and perplexity is 139.28953553334
At time: 368.019250869751 and batch: 500, loss is 4.922096185684204 and perplexity is 137.2900973077117
At time: 370.47393918037415 and batch: 550, loss is 4.944248666763306 and perplexity is 140.36535007049932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.060646565755208 and perplexity of 157.6924419018107
Finished 13 epochs...
Completing Train Step...
At time: 374.40719628334045 and batch: 50, loss is 5.03366569519043 and perplexity is 153.49464726637817
At time: 376.8652853965759 and batch: 100, loss is 4.994385671615601 and perplexity is 147.5822535616483
At time: 379.3236107826233 and batch: 150, loss is 5.011854267120361 and perplexity is 150.18295744105376
At time: 381.7825057506561 and batch: 200, loss is 5.009503879547119 and perplexity is 149.83038378884785
At time: 384.24145126342773 and batch: 250, loss is 4.976532783508301 and perplexity is 144.9708638853993
At time: 386.6997528076172 and batch: 300, loss is 4.974572219848633 and perplexity is 144.68691771617821
At time: 389.16182017326355 and batch: 350, loss is 4.924661340713501 and perplexity is 137.64271976357117
At time: 391.6193072795868 and batch: 400, loss is 4.943790283203125 and perplexity is 140.30102364582206
At time: 394.0795726776123 and batch: 450, loss is 4.935649881362915 and perplexity is 139.16355295033293
At time: 396.53870391845703 and batch: 500, loss is 4.920805053710938 and perplexity is 137.11295205702217
At time: 398.9945876598358 and batch: 550, loss is 4.9395320606231685 and perplexity is 139.70486085669197
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.059638977050781 and perplexity of 157.53363279914166
Finished 14 epochs...
Completing Train Step...
At time: 402.9234781265259 and batch: 50, loss is 5.0246694469451905 and perplexity is 152.11996408221046
At time: 405.41041469573975 and batch: 100, loss is 4.986159572601318 and perplexity is 146.3732070172695
At time: 407.8809463977814 and batch: 150, loss is 5.003788270950317 and perplexity is 148.97645464626783
At time: 410.34850692749023 and batch: 200, loss is 5.002302722930908 and perplexity is 148.75530727232112
At time: 412.8162319660187 and batch: 250, loss is 4.970778942108154 and perplexity is 144.13911968376163
At time: 415.29131031036377 and batch: 300, loss is 4.969882707595826 and perplexity is 144.0099951017209
At time: 417.7574985027313 and batch: 350, loss is 4.921201181411743 and perplexity is 137.16727705457902
At time: 420.21888422966003 and batch: 400, loss is 4.941519660949707 and perplexity is 139.98281442254296
At time: 422.67881202697754 and batch: 450, loss is 4.933555870056153 and perplexity is 138.872447792084
At time: 425.1387059688568 and batch: 500, loss is 4.918358993530274 and perplexity is 136.77797537831947
At time: 427.5988190174103 and batch: 550, loss is 4.934741907119751 and perplexity is 139.0372533757241
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.058970642089844 and perplexity of 157.42838273988332
Finished 15 epochs...
Completing Train Step...
At time: 431.5465805530548 and batch: 50, loss is 5.01740273475647 and perplexity is 151.0185587295648
At time: 434.0057454109192 and batch: 100, loss is 4.979174327850342 and perplexity is 145.35431708199735
At time: 436.46348214149475 and batch: 150, loss is 4.997168006896973 and perplexity is 147.99344864866796
At time: 438.9218635559082 and batch: 200, loss is 4.996022205352784 and perplexity is 147.82397463700983
At time: 441.38190937042236 and batch: 250, loss is 4.965978002548217 and perplexity is 143.44877495956587
At time: 443.84131026268005 and batch: 300, loss is 4.965851831436157 and perplexity is 143.43067700984875
At time: 446.3009281158447 and batch: 350, loss is 4.917713766098022 and perplexity is 136.68975094195164
At time: 448.76026487350464 and batch: 400, loss is 4.938760509490967 and perplexity is 139.59711298496526
At time: 451.21907019615173 and batch: 450, loss is 4.930942726135254 and perplexity is 138.51002783334448
At time: 453.6784644126892 and batch: 500, loss is 4.916048355102539 and perplexity is 136.46229578349636
At time: 456.1448941230774 and batch: 550, loss is 4.930443496704101 and perplexity is 138.44089680849717
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.05819346110026 and perplexity of 157.3060799254642
Finished 16 epochs...
Completing Train Step...
At time: 460.06591725349426 and batch: 50, loss is 5.010986108779907 and perplexity is 150.0526314339478
At time: 462.5579023361206 and batch: 100, loss is 4.973073253631592 and perplexity is 144.47019938178346
At time: 465.01706767082214 and batch: 150, loss is 4.991185111999512 and perplexity is 147.11066284052092
At time: 467.47956466674805 and batch: 200, loss is 4.990342245101929 and perplexity is 146.98672037337022
At time: 469.9400200843811 and batch: 250, loss is 4.9616061782836915 and perplexity is 142.82301098621969
At time: 472.39738059043884 and batch: 300, loss is 4.961497716903686 and perplexity is 142.80752104539698
At time: 474.85321950912476 and batch: 350, loss is 4.914173316955567 and perplexity is 136.20666350830794
At time: 477.3097040653229 and batch: 400, loss is 4.936116113662719 and perplexity is 139.22845062119353
At time: 479.767067193985 and batch: 450, loss is 4.92817364692688 and perplexity is 138.1270131388733
At time: 482.22407150268555 and batch: 500, loss is 4.9131475448608395 and perplexity is 136.06701814816668
At time: 484.68190717697144 and batch: 550, loss is 4.926250295639038 and perplexity is 137.86160169181042
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.0578770955403645 and perplexity of 157.25632157074344
Finished 17 epochs...
Completing Train Step...
At time: 488.6276521682739 and batch: 50, loss is 5.005021963119507 and perplexity is 149.16035914920596
At time: 491.0879535675049 and batch: 100, loss is 4.967350826263428 and perplexity is 143.64584007663154
At time: 493.58130621910095 and batch: 150, loss is 4.985625457763672 and perplexity is 146.2950477904272
At time: 496.03849840164185 and batch: 200, loss is 4.985026082992554 and perplexity is 146.2073885026491
At time: 498.49956607818604 and batch: 250, loss is 4.9573946380615235 and perplexity is 142.22277098546118
At time: 500.95757246017456 and batch: 300, loss is 4.9575992202758785 and perplexity is 142.25187021137165
At time: 503.41582202911377 and batch: 350, loss is 4.910780115127563 and perplexity is 135.74527005196504
At time: 505.8743438720703 and batch: 400, loss is 4.933021097183228 and perplexity is 138.7982024281808
At time: 508.3341417312622 and batch: 450, loss is 4.924898414611817 and perplexity is 137.67535512806393
At time: 510.7928681373596 and batch: 500, loss is 4.910208396911621 and perplexity is 135.66768418908717
At time: 513.252790927887 and batch: 550, loss is 4.9220969676971436 and perplexity is 137.2902046703863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.057389831542968 and perplexity of 157.17971489223297
Finished 18 epochs...
Completing Train Step...
At time: 517.1615340709686 and batch: 50, loss is 4.999585103988648 and perplexity is 148.35159584689046
At time: 519.6437995433807 and batch: 100, loss is 4.962374572753906 and perplexity is 142.932797572382
At time: 522.1031684875488 and batch: 150, loss is 4.9805335521698 and perplexity is 145.5520205359468
At time: 524.5601799488068 and batch: 200, loss is 4.980252637863159 and perplexity is 145.51113863344221
At time: 527.017364025116 and batch: 250, loss is 4.953327293395996 and perplexity is 141.64547677969313
At time: 529.4784109592438 and batch: 300, loss is 4.953583555221558 and perplexity is 141.6817797594905
At time: 531.9350309371948 and batch: 350, loss is 4.907218723297119 and perplexity is 135.26268779849775
At time: 534.391518831253 and batch: 400, loss is 4.930321931838989 and perplexity is 138.42406828245012
At time: 536.8469874858856 and batch: 450, loss is 4.921948080062866 and perplexity is 137.26976537822014
At time: 539.3028736114502 and batch: 500, loss is 4.907146396636963 and perplexity is 135.25290505382645
At time: 541.7593469619751 and batch: 550, loss is 4.9180468082427975 and perplexity is 136.7352819712291
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.057241821289063 and perplexity of 157.15645240430882
Finished 19 epochs...
Completing Train Step...
At time: 545.693436384201 and batch: 50, loss is 4.994494142532349 and perplexity is 147.59826281224125
At time: 548.1513721942902 and batch: 100, loss is 4.9573882293701175 and perplexity is 142.22185952653166
At time: 550.6107320785522 and batch: 150, loss is 4.975804414749145 and perplexity is 144.86531008287005
At time: 553.0704445838928 and batch: 200, loss is 4.975465641021729 and perplexity is 144.81624183378605
At time: 555.5638380050659 and batch: 250, loss is 4.949324979782104 and perplexity is 141.07970012268152
At time: 558.0217504501343 and batch: 300, loss is 4.949811162948609 and perplexity is 141.14830737451072
At time: 560.4788703918457 and batch: 350, loss is 4.903643112182618 and perplexity is 134.77990466536133
At time: 562.9354705810547 and batch: 400, loss is 4.927238349914551 and perplexity is 137.99788375274036
At time: 565.3941507339478 and batch: 450, loss is 4.9188117504119875 and perplexity is 136.83991656903098
At time: 567.8498265743256 and batch: 500, loss is 4.90409634590149 and perplexity is 134.84100530817187
At time: 570.3057012557983 and batch: 550, loss is 4.91395378112793 and perplexity is 136.1767645477716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.057318623860677 and perplexity of 157.16852288751542
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 574.213006734848 and batch: 50, loss is 4.990819339752197 and perplexity is 147.05686368248908
At time: 576.6952595710754 and batch: 100, loss is 4.951179857254028 and perplexity is 141.34162852763512
At time: 579.1537384986877 and batch: 150, loss is 4.965854167938232 and perplexity is 143.43101213631476
At time: 581.6116833686829 and batch: 200, loss is 4.961766633987427 and perplexity is 142.8459295916184
At time: 584.0709125995636 and batch: 250, loss is 4.930433073043823 and perplexity is 138.43945375514122
At time: 586.5303070545197 and batch: 300, loss is 4.92721453666687 and perplexity is 137.99459761408215
At time: 588.9926722049713 and batch: 350, loss is 4.874719095230103 and perplexity is 130.93736711338545
At time: 591.4516882896423 and batch: 400, loss is 4.89141884803772 and perplexity is 133.14234885864678
At time: 593.9109025001526 and batch: 450, loss is 4.881671772003174 and perplexity is 131.85090438913306
At time: 596.3706424236298 and batch: 500, loss is 4.8695463371276855 and perplexity is 130.26180854362323
At time: 598.8289325237274 and batch: 550, loss is 4.894307336807251 and perplexity is 133.52748500100728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.044598388671875 and perplexity of 155.18196384730462
Finished 21 epochs...
Completing Train Step...
At time: 602.7452938556671 and batch: 50, loss is 4.982379026412964 and perplexity is 145.82088105215868
At time: 605.2276117801666 and batch: 100, loss is 4.943959903717041 and perplexity is 140.32482359597867
At time: 607.6854565143585 and batch: 150, loss is 4.959927368164062 and perplexity is 142.58343942390914
At time: 610.1433911323547 and batch: 200, loss is 4.956816244125366 and perplexity is 142.14053398212434
At time: 612.602287530899 and batch: 250, loss is 4.927035236358643 and perplexity is 137.96985735823029
At time: 615.0995993614197 and batch: 300, loss is 4.924927263259888 and perplexity is 137.67932693322243
At time: 617.5580363273621 and batch: 350, loss is 4.873860311508179 and perplexity is 130.82496850387292
At time: 620.016717672348 and batch: 400, loss is 4.891785306930542 and perplexity is 133.19114899747353
At time: 622.4765374660492 and batch: 450, loss is 4.883125 and perplexity is 132.04265310835598
At time: 624.9436693191528 and batch: 500, loss is 4.871107683181763 and perplexity is 130.46535116327314
At time: 627.4081556797028 and batch: 550, loss is 4.893082962036133 and perplexity is 133.3640973613843
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.043507893880208 and perplexity of 155.01283095999563
Finished 22 epochs...
Completing Train Step...
At time: 631.3507678508759 and batch: 50, loss is 4.978267803192138 and perplexity is 145.22260951648371
At time: 633.8156290054321 and batch: 100, loss is 4.9400959300994876 and perplexity is 139.78365837709293
At time: 636.2781307697296 and batch: 150, loss is 4.9566741561889645 and perplexity is 142.12033896174017
At time: 638.7385990619659 and batch: 200, loss is 4.9542169189453125 and perplexity is 141.77154428290282
At time: 641.199517250061 and batch: 250, loss is 4.925380010604858 and perplexity is 137.7416749958436
At time: 643.6576557159424 and batch: 300, loss is 4.924099025726318 and perplexity is 137.56534295647944
At time: 646.1163105964661 and batch: 350, loss is 4.873889760971069 and perplexity is 130.82882128565893
At time: 648.5761430263519 and batch: 400, loss is 4.8921533298492434 and perplexity is 133.24017541374187
At time: 651.0359151363373 and batch: 450, loss is 4.8838099193573 and perplexity is 132.13312265609468
At time: 653.4960975646973 and batch: 500, loss is 4.871521759033203 and perplexity is 130.51938490090038
At time: 655.955730676651 and batch: 550, loss is 4.891786727905274 and perplexity is 133.19133825886522
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.043057759602864 and perplexity of 154.9430700734135
Finished 23 epochs...
Completing Train Step...
At time: 659.8703117370605 and batch: 50, loss is 4.9751444435119625 and perplexity is 144.76973468692503
At time: 662.3531019687653 and batch: 100, loss is 4.937264671325684 and perplexity is 139.38845439424745
At time: 664.8103456497192 and batch: 150, loss is 4.954265155792236 and perplexity is 141.77838306012177
At time: 667.2746143341064 and batch: 200, loss is 4.952323703765869 and perplexity is 141.50339415629176
At time: 669.736897945404 and batch: 250, loss is 4.924279279708863 and perplexity is 137.5901418923938
At time: 672.194349527359 and batch: 300, loss is 4.9236445808410645 and perplexity is 137.50284129284105
At time: 674.652782201767 and batch: 350, loss is 4.873973665237426 and perplexity is 130.83979884245272
At time: 677.1481733322144 and batch: 400, loss is 4.892437705993652 and perplexity is 133.2780711291697
At time: 679.6070609092712 and batch: 450, loss is 4.884211492538452 and perplexity is 132.18619442988262
At time: 682.0641376972198 and batch: 500, loss is 4.871688995361328 and perplexity is 130.54121430886454
At time: 684.5199987888336 and batch: 550, loss is 4.890527820587158 and perplexity is 133.02376820813416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.042798868815104 and perplexity of 154.90296193198233
Finished 24 epochs...
Completing Train Step...
At time: 688.4444296360016 and batch: 50, loss is 4.972574443817138 and perplexity is 144.3981541983519
At time: 690.9021620750427 and batch: 100, loss is 4.934911499023437 and perplexity is 139.06083496777447
At time: 693.3600907325745 and batch: 150, loss is 4.952374000549316 and perplexity is 141.51051150085289
At time: 695.818101644516 and batch: 200, loss is 4.9508360004425045 and perplexity is 141.2930356008972
At time: 698.2760908603668 and batch: 250, loss is 4.923414478302002 and perplexity is 137.47120517984578
At time: 700.7349436283112 and batch: 300, loss is 4.923222961425782 and perplexity is 137.44487964503207
At time: 703.1976456642151 and batch: 350, loss is 4.8739643573760985 and perplexity is 130.83858100941666
At time: 705.6533184051514 and batch: 400, loss is 4.892514057159424 and perplexity is 133.28824745375442
At time: 708.1106991767883 and batch: 450, loss is 4.88433500289917 and perplexity is 132.20252180271763
At time: 710.5680015087128 and batch: 500, loss is 4.8716119861602785 and perplexity is 130.5311618213185
At time: 713.0237762928009 and batch: 550, loss is 4.889433517456054 and perplexity is 132.87827950096528
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.042600504557291 and perplexity of 154.87223776829444
Finished 25 epochs...
Completing Train Step...
At time: 716.9385282993317 and batch: 50, loss is 4.970389804840088 and perplexity is 144.08304069242678
At time: 719.4098210334778 and batch: 100, loss is 4.932954387664795 and perplexity is 138.7889435757677
At time: 721.8670837879181 and batch: 150, loss is 4.950790939331054 and perplexity is 141.28666892311873
At time: 724.3239262104034 and batch: 200, loss is 4.9495746040344235 and perplexity is 141.11492143320584
At time: 726.7816293239594 and batch: 250, loss is 4.922705841064453 and perplexity is 137.37382247334594
At time: 729.2381665706635 and batch: 300, loss is 4.922829370498658 and perplexity is 137.3907932320831
At time: 731.695591211319 and batch: 350, loss is 4.873872900009156 and perplexity is 130.8266154044827
At time: 734.1571094989777 and batch: 400, loss is 4.892478075027466 and perplexity is 133.28345154473007
At time: 736.6135814189911 and batch: 450, loss is 4.884224920272827 and perplexity is 132.18796940290636
At time: 739.1074509620667 and batch: 500, loss is 4.8713281536102295 and perplexity is 130.4941180861555
At time: 741.5635282993317 and batch: 550, loss is 4.888305912017822 and perplexity is 132.7285296756029
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.042473856608073 and perplexity of 154.85262475899012
Finished 26 epochs...
Completing Train Step...
At time: 745.4927082061768 and batch: 50, loss is 4.96844449043274 and perplexity is 143.8030263238097
At time: 747.957373380661 and batch: 100, loss is 4.9311902141571045 and perplexity is 138.54431164838118
At time: 750.4165637493134 and batch: 150, loss is 4.949441127777099 and perplexity is 141.09608719862987
At time: 752.8775734901428 and batch: 200, loss is 4.9484624099731445 and perplexity is 140.95806150114174
At time: 755.3380768299103 and batch: 250, loss is 4.922051820755005 and perplexity is 137.28400657737336
At time: 757.7923998832703 and batch: 300, loss is 4.922456855773926 and perplexity is 137.33962267004722
At time: 760.2474310398102 and batch: 350, loss is 4.873747100830078 and perplexity is 130.8101585588139
At time: 762.7038209438324 and batch: 400, loss is 4.892316274642944 and perplexity is 133.2618879755637
At time: 765.1604025363922 and batch: 450, loss is 4.883971538543701 and perplexity is 132.15447962967687
At time: 767.6175334453583 and batch: 500, loss is 4.870988092422485 and perplexity is 130.44974964578535
At time: 770.0747113227844 and batch: 550, loss is 4.887247543334961 and perplexity is 132.58812826780445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.042420450846354 and perplexity of 154.8443549574402
Finished 27 epochs...
Completing Train Step...
At time: 773.9931480884552 and batch: 50, loss is 4.9666227531433105 and perplexity is 143.54129346506215
At time: 776.468896150589 and batch: 100, loss is 4.929549751281738 and perplexity is 138.31722116620875
At time: 778.928674697876 and batch: 150, loss is 4.948202972412109 and perplexity is 140.9214964288393
At time: 781.3894853591919 and batch: 200, loss is 4.947435398101806 and perplexity is 140.81337021117326
At time: 783.8514831066132 and batch: 250, loss is 4.921433362960816 and perplexity is 137.19912846295915
At time: 786.311925649643 and batch: 300, loss is 4.922045526504516 and perplexity is 137.2831424801673
At time: 788.7714929580688 and batch: 350, loss is 4.873541345596314 and perplexity is 130.78324645280912
At time: 791.2312185764313 and batch: 400, loss is 4.892122049331665 and perplexity is 133.23600765727775
At time: 793.6898519992828 and batch: 450, loss is 4.883608312606811 and perplexity is 132.10648641171764
At time: 796.1513233184814 and batch: 500, loss is 4.8705634593963625 and perplexity is 130.39436813307813
At time: 798.6095299720764 and batch: 550, loss is 4.886173028945922 and perplexity is 132.4457369306355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.042384847005208 and perplexity of 154.8388420017658
Finished 28 epochs...
Completing Train Step...
At time: 802.547116279602 and batch: 50, loss is 4.964925308227539 and perplexity is 143.2978467034544
At time: 805.0101747512817 and batch: 100, loss is 4.928070020675659 and perplexity is 138.11270029591535
At time: 807.470668554306 and batch: 150, loss is 4.947010316848755 and perplexity is 140.7535258075857
At time: 809.9328081607819 and batch: 200, loss is 4.9464399528503415 and perplexity is 140.67326795408616
At time: 812.3919484615326 and batch: 250, loss is 4.920793991088868 and perplexity is 137.1114352366427
At time: 814.8536658287048 and batch: 300, loss is 4.921542491912842 and perplexity is 137.21410167705795
At time: 817.3183538913727 and batch: 350, loss is 4.873298673629761 and perplexity is 130.75151287578078
At time: 819.7763555049896 and batch: 400, loss is 4.8918360900878906 and perplexity is 133.1979130362987
At time: 822.2344601154327 and batch: 450, loss is 4.883218688964844 and perplexity is 132.0550246273693
At time: 824.6963775157928 and batch: 500, loss is 4.870061168670654 and perplexity is 130.32888869751355
At time: 827.1548025608063 and batch: 550, loss is 4.885156288146972 and perplexity is 132.31114238173058
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.042347208658854 and perplexity of 154.83301423347592
Finished 29 epochs...
Completing Train Step...
At time: 831.0728194713593 and batch: 50, loss is 4.96339898109436 and perplexity is 143.07929414568406
At time: 833.5618324279785 and batch: 100, loss is 4.926726274490356 and perplexity is 137.9272365177799
At time: 836.0201885700226 and batch: 150, loss is 4.945921068191528 and perplexity is 140.60029368769062
At time: 838.4792540073395 and batch: 200, loss is 4.945502624511719 and perplexity is 140.54147269091314
At time: 840.9386785030365 and batch: 250, loss is 4.920210380554199 and perplexity is 137.03143890423675
At time: 843.3975071907043 and batch: 300, loss is 4.921052408218384 and perplexity is 137.14687175866536
At time: 845.8572664260864 and batch: 350, loss is 4.87299373626709 and perplexity is 130.7116479327434
At time: 848.3172047138214 and batch: 400, loss is 4.89151972770691 and perplexity is 133.15578089225295
At time: 850.7829248905182 and batch: 450, loss is 4.882838916778565 and perplexity is 132.00488332369633
At time: 853.2433135509491 and batch: 500, loss is 4.86956669807434 and perplexity is 130.26446082435956
At time: 855.7075347900391 and batch: 550, loss is 4.884141111373902 and perplexity is 132.17689133896616
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.0423227945963545 and perplexity of 154.82923417673288
Finished 30 epochs...
Completing Train Step...
At time: 859.6641669273376 and batch: 50, loss is 4.9619377326965335 and perplexity is 142.87037243678265
At time: 862.1216778755188 and batch: 100, loss is 4.925467891693115 and perplexity is 137.75378041605157
At time: 864.6059379577637 and batch: 150, loss is 4.944902811050415 and perplexity is 140.4571993004053
At time: 867.0681030750275 and batch: 200, loss is 4.9446078968048095 and perplexity is 140.41578257892454
At time: 869.5276472568512 and batch: 250, loss is 4.919619560241699 and perplexity is 136.95050185866077
At time: 871.9850680828094 and batch: 300, loss is 4.920591144561768 and perplexity is 137.08362547883104
At time: 874.445116519928 and batch: 350, loss is 4.872657594680786 and perplexity is 130.66771769586222
At time: 876.9022705554962 and batch: 400, loss is 4.891149578094482 and perplexity is 133.10650245232355
At time: 879.3604946136475 and batch: 450, loss is 4.882405395507813 and perplexity is 131.94766880168459
At time: 881.8220279216766 and batch: 500, loss is 4.869002065658569 and perplexity is 130.19093004803864
At time: 884.2824819087982 and batch: 550, loss is 4.883167791366577 and perplexity is 132.0483035148224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.042296346028646 and perplexity of 154.82513921940262
Finished 31 epochs...
Completing Train Step...
At time: 888.2062191963196 and batch: 50, loss is 4.9605301570892335 and perplexity is 142.66941305147873
At time: 890.6944208145142 and batch: 100, loss is 4.924247331619263 and perplexity is 137.58574622042963
At time: 893.1501188278198 and batch: 150, loss is 4.943954124450683 and perplexity is 140.3240126237899
At time: 895.6072652339935 and batch: 200, loss is 4.943681592941284 and perplexity is 140.28577511952366
At time: 898.0650162696838 and batch: 250, loss is 4.919012823104858 and perplexity is 136.86743410596765
At time: 900.5222673416138 and batch: 300, loss is 4.920100317001343 and perplexity is 137.01635756718602
At time: 902.9787213802338 and batch: 350, loss is 4.87225338935852 and perplexity is 130.614911781856
At time: 905.4355189800262 and batch: 400, loss is 4.890727262496949 and perplexity is 133.05030136833727
At time: 907.8961117267609 and batch: 450, loss is 4.881970548629761 and perplexity is 131.89030424314353
At time: 910.3536939620972 and batch: 500, loss is 4.86844235420227 and perplexity is 130.11808108208675
At time: 912.8183166980743 and batch: 550, loss is 4.882177753448486 and perplexity is 131.9176353811998
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.042297871907552 and perplexity of 154.8253754639969
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 916.7687797546387 and batch: 50, loss is 4.959400968551636 and perplexity is 142.50840330788185
At time: 919.2265868186951 and batch: 100, loss is 4.923000268936157 and perplexity is 137.41427511042426
At time: 921.6842038631439 and batch: 150, loss is 4.941972169876099 and perplexity is 140.0461722295161
At time: 924.1428127288818 and batch: 200, loss is 4.941166458129882 and perplexity is 139.9333808283183
At time: 926.6255753040314 and batch: 250, loss is 4.914201431274414 and perplexity is 136.2104929197053
At time: 929.0815875530243 and batch: 300, loss is 4.914227752685547 and perplexity is 136.214078219275
At time: 931.5385499000549 and batch: 350, loss is 4.8654594898223875 and perplexity is 129.73053477922824
At time: 934.0004687309265 and batch: 400, loss is 4.882567491531372 and perplexity is 131.9690587276794
At time: 936.4571433067322 and batch: 450, loss is 4.873552827835083 and perplexity is 130.78474814589333
At time: 938.9143562316895 and batch: 500, loss is 4.860730581283569 and perplexity is 129.11849921622246
At time: 941.3699567317963 and batch: 550, loss is 4.877724485397339 and perplexity is 131.33147691925407
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.042641194661458 and perplexity of 154.87853966399334
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 945.2674014568329 and batch: 50, loss is 4.958417139053345 and perplexity is 142.3682682827422
At time: 947.7543227672577 and batch: 100, loss is 4.922184467315674 and perplexity is 137.3022180364977
At time: 950.2104849815369 and batch: 150, loss is 4.9412689304351805 and perplexity is 139.94772085915537
At time: 952.6684002876282 and batch: 200, loss is 4.940528888702392 and perplexity is 139.84419201786744
At time: 955.1248383522034 and batch: 250, loss is 4.913158159255982 and perplexity is 136.0684624249282
At time: 957.5820014476776 and batch: 300, loss is 4.913164768218994 and perplexity is 136.06936169933516
At time: 960.0389380455017 and batch: 350, loss is 4.864012870788574 and perplexity is 129.54299979689196
At time: 962.49453997612 and batch: 400, loss is 4.880933103561401 and perplexity is 131.75354624916113
At time: 964.9527833461761 and batch: 450, loss is 4.871940412521362 and perplexity is 130.57403873637273
At time: 967.4188418388367 and batch: 500, loss is 4.85925874710083 and perplexity is 128.9285979812983
At time: 969.8766691684723 and batch: 550, loss is 4.876785192489624 and perplexity is 131.2081761112767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.043446858723958 and perplexity of 155.00337001636527
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 973.818149805069 and batch: 50, loss is 4.958145055770874 and perplexity is 142.32953752622282
At time: 976.2768752574921 and batch: 100, loss is 4.9219997596740725 and perplexity is 137.27685960963726
At time: 978.7351672649384 and batch: 150, loss is 4.9410961151123045 and perplexity is 139.92353783824782
At time: 981.1944909095764 and batch: 200, loss is 4.94026927947998 and perplexity is 139.80789188804698
At time: 983.653112411499 and batch: 250, loss is 4.912886037826538 and perplexity is 136.03144031791368
At time: 986.1110861301422 and batch: 300, loss is 4.9129439544677735 and perplexity is 136.03931903019142
At time: 988.5947711467743 and batch: 350, loss is 4.863658790588379 and perplexity is 129.49713930521511
At time: 991.0535695552826 and batch: 400, loss is 4.88059850692749 and perplexity is 131.7094693304859
At time: 993.5124552249908 and batch: 450, loss is 4.871612272262573 and perplexity is 130.5311991665888
At time: 995.9728050231934 and batch: 500, loss is 4.858928718566895 and perplexity is 128.8860548857281
At time: 998.4308342933655 and batch: 550, loss is 4.876612033843994 and perplexity is 131.18545824815965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.043516031901041 and perplexity of 155.01409246277646
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 1002.3291358947754 and batch: 50, loss is 4.958102703094482 and perplexity is 142.32350961702895
At time: 1004.8197522163391 and batch: 100, loss is 4.921960182189942 and perplexity is 137.27142664441678
At time: 1007.2763276100159 and batch: 150, loss is 4.9410481262207036 and perplexity is 139.91682322387283
At time: 1009.7356054782867 and batch: 200, loss is 4.94020676612854 and perplexity is 139.79915230134054
At time: 1012.1927938461304 and batch: 250, loss is 4.912828645706177 and perplexity is 136.02363340914772
At time: 1014.6505324840546 and batch: 300, loss is 4.912898807525635 and perplexity is 136.03317740956535
At time: 1017.1087188720703 and batch: 350, loss is 4.863591623306275 and perplexity is 129.4884416264307
At time: 1019.5708978176117 and batch: 400, loss is 4.880534763336182 and perplexity is 131.7010739634797
At time: 1022.0306234359741 and batch: 450, loss is 4.871544685363769 and perplexity is 130.5223772657658
At time: 1024.4871957302094 and batch: 500, loss is 4.858863182067871 and perplexity is 128.87760842169686
At time: 1026.9447162151337 and batch: 550, loss is 4.876579656600952 and perplexity is 131.18121089345354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.043531290690104 and perplexity of 155.01645780816116
Annealing...
Model not improving. Stopping early with 154.82513921940262loss at 35 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -154.82513921940262
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f6907359860>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'anneal': 4.812500434828028, 'wordvec_source': '', 'data': 'wikitext', 'lr': 1.2796573659970023, 'num_layers': 1, 'batch_size': 80, 'dropout': 0.8659211716877756, 'wordvec_dim': 200, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 3.196916103363037 and batch: 50, loss is 9.154707317352296 and perplexity is 9458.861607699388
At time: 5.665616989135742 and batch: 100, loss is 7.560664272308349 and perplexity is 1921.121237251515
At time: 8.135832071304321 and batch: 150, loss is 7.2363958835601805 and perplexity is 1389.0785373995702
At time: 10.605373620986938 and batch: 200, loss is 7.0562051486968995 and perplexity is 1160.0346434492312
At time: 13.077666521072388 and batch: 250, loss is 6.8757571029663085 and perplexity is 968.5083486214843
At time: 15.548464298248291 and batch: 300, loss is 6.815161323547363 and perplexity is 911.5635605902062
At time: 18.060500383377075 and batch: 350, loss is 6.720063915252686 and perplexity is 828.8704872551084
At time: 20.52909541130066 and batch: 400, loss is 6.7142377281188965 and perplexity is 824.0553731850347
At time: 22.99907159805298 and batch: 450, loss is 6.67912106513977 and perplexity is 795.6195068871942
At time: 25.47288489341736 and batch: 500, loss is 6.620188312530518 and perplexity is 750.0863344688335
At time: 27.942037343978882 and batch: 550, loss is 6.563273220062256 and perplexity is 708.5872648936851
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.951059977213542 and perplexity of 384.16032451568555
Finished 1 epochs...
Completing Train Step...
At time: 31.907948970794678 and batch: 50, loss is 6.184774045944214 and perplexity is 485.3032949715715
At time: 34.36679482460022 and batch: 100, loss is 5.993011331558227 and perplexity is 400.6191925353371
At time: 36.820828437805176 and batch: 150, loss is 5.935827360153199 and perplexity is 378.35290082536324
At time: 39.27487087249756 and batch: 200, loss is 5.870770521163941 and perplexity is 354.5220417906976
At time: 41.72830843925476 and batch: 250, loss is 5.764498271942139 and perplexity is 318.7790635058233
At time: 44.184086561203 and batch: 300, loss is 5.731584825515747 and perplexity is 308.4577328952786
At time: 46.63908934593201 and batch: 350, loss is 5.658626413345337 and perplexity is 286.75448977620573
At time: 49.12970995903015 and batch: 400, loss is 5.640279474258423 and perplexity is 281.54139105276784
At time: 51.585490703582764 and batch: 450, loss is 5.591169271469116 and perplexity is 268.0488584299033
At time: 54.039878368377686 and batch: 500, loss is 5.5596914482116695 and perplexity is 259.7426798889957
At time: 56.49525713920593 and batch: 550, loss is 5.508633689880371 and perplexity is 246.81367256787928
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.227921549479166 and perplexity of 186.4049671094183
Finished 2 epochs...
Completing Train Step...
At time: 60.43080282211304 and batch: 50, loss is 5.526868181228638 and perplexity is 251.35547725016235
At time: 62.890864849090576 and batch: 100, loss is 5.480135526657104 and perplexity is 239.87921519951186
At time: 65.34553742408752 and batch: 150, loss is 5.488776836395264 and perplexity is 241.96106780971513
At time: 67.80531764030457 and batch: 200, loss is 5.481861162185669 and perplexity is 240.29351665967437
At time: 70.26519584655762 and batch: 250, loss is 5.4275878810882565 and perplexity is 227.5995855034694
At time: 72.72071862220764 and batch: 300, loss is 5.417275514602661 and perplexity is 225.26455570050254
At time: 75.17635893821716 and batch: 350, loss is 5.360768547058106 and perplexity is 212.8884984219695
At time: 77.63299489021301 and batch: 400, loss is 5.3568566703796385 and perplexity is 212.0573316407822
At time: 80.08865213394165 and batch: 450, loss is 5.3177071475982665 and perplexity is 203.91579674852034
At time: 82.54294967651367 and batch: 500, loss is 5.306514892578125 and perplexity is 201.6462435493156
At time: 84.99702548980713 and batch: 550, loss is 5.275510950088501 and perplexity is 195.4903368034569
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.0533604939778645 and perplexity of 156.5476590009666
Finished 3 epochs...
Completing Train Step...
At time: 88.93141746520996 and batch: 50, loss is 5.310548763275147 and perplexity is 202.461301235767
At time: 91.41169452667236 and batch: 100, loss is 5.273738946914673 and perplexity is 195.14423404434564
At time: 93.89317321777344 and batch: 150, loss is 5.287672328948974 and perplexity is 197.88228406078562
At time: 96.34972310066223 and batch: 200, loss is 5.291483573913574 and perplexity is 198.63790092533276
At time: 98.80952548980713 and batch: 250, loss is 5.250675439834595 and perplexity is 190.69502798723917
At time: 101.264488697052 and batch: 300, loss is 5.245900430679321 and perplexity is 189.7866280178335
At time: 103.72101736068726 and batch: 350, loss is 5.190202159881592 and perplexity is 179.50483794080552
At time: 106.1763391494751 and batch: 400, loss is 5.193636302947998 and perplexity is 180.12234292877244
At time: 108.63057136535645 and batch: 450, loss is 5.154146728515625 and perplexity is 173.14800145660854
At time: 111.08698225021362 and batch: 500, loss is 5.151557331085205 and perplexity is 172.7002324427116
At time: 113.54142785072327 and batch: 550, loss is 5.128822202682495 and perplexity is 168.81816732145452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.948579406738281 and perplexity of 140.974554103129
Finished 4 epochs...
Completing Train Step...
At time: 117.50777673721313 and batch: 50, loss is 5.172480373382569 and perplexity is 176.35171354845951
At time: 119.9612169265747 and batch: 100, loss is 5.141321744918823 and perplexity is 170.94156020743193
At time: 122.41506171226501 and batch: 150, loss is 5.153638563156128 and perplexity is 173.06003599259597
At time: 124.86962151527405 and batch: 200, loss is 5.162373027801514 and perplexity is 174.5782434711733
At time: 127.3241229057312 and batch: 250, loss is 5.13059097290039 and perplexity is 169.11703210169586
At time: 129.77773475646973 and batch: 300, loss is 5.1263223361969 and perplexity is 168.39667150394757
At time: 132.2319004535675 and batch: 350, loss is 5.071009674072266 and perplexity is 159.3351226963753
At time: 134.68642497062683 and batch: 400, loss is 5.079301614761352 and perplexity is 160.66181290018844
At time: 137.14316868782043 and batch: 450, loss is 5.0357897758483885 and perplexity is 153.8210287853609
At time: 139.59646320343018 and batch: 500, loss is 5.038401432037354 and perplexity is 154.22328147140104
At time: 142.04880785942078 and batch: 550, loss is 5.0221568202972415 and perplexity is 151.7382231937737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.881125895182292 and perplexity of 131.77894967760602
Finished 5 epochs...
Completing Train Step...
At time: 145.9614179134369 and batch: 50, loss is 5.069597253799438 and perplexity is 159.11023339539597
At time: 148.4441225528717 and batch: 100, loss is 5.042588729858398 and perplexity is 154.87041420506378
At time: 150.90141105651855 and batch: 150, loss is 5.05128888130188 and perplexity is 156.22368857266952
At time: 153.38486790657043 and batch: 200, loss is 5.063103332519531 and perplexity is 158.08033173438545
At time: 155.84332609176636 and batch: 250, loss is 5.037617321014404 and perplexity is 154.1024006945721
At time: 158.30105876922607 and batch: 300, loss is 5.033904819488526 and perplexity is 153.531355954962
At time: 160.75902652740479 and batch: 350, loss is 4.979290285110474 and perplexity is 145.3711729476156
At time: 163.22825813293457 and batch: 400, loss is 4.990425872802734 and perplexity is 146.99901304884088
At time: 165.6874589920044 and batch: 450, loss is 4.943453893661499 and perplexity is 140.2538357859845
At time: 168.146244764328 and batch: 500, loss is 4.949763679504395 and perplexity is 141.14160532585106
At time: 170.60446548461914 and batch: 550, loss is 4.938298349380493 and perplexity is 139.53261167392088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.836107381184896 and perplexity of 125.97801167223139
Finished 6 epochs...
Completing Train Step...
At time: 174.56264853477478 and batch: 50, loss is 4.986742925643921 and perplexity is 146.45861918323985
At time: 177.01985335350037 and batch: 100, loss is 4.963036222457886 and perplexity is 143.0274003090698
At time: 179.4770004749298 and batch: 150, loss is 4.969145231246948 and perplexity is 143.9038302881626
At time: 181.93230271339417 and batch: 200, loss is 4.983131532669067 and perplexity is 145.93065347445815
At time: 184.39070677757263 and batch: 250, loss is 4.962220811843872 and perplexity is 142.91082178490544
At time: 186.84591841697693 and batch: 300, loss is 4.958963832855225 and perplexity is 142.44612141156924
At time: 189.3054494857788 and batch: 350, loss is 4.905598430633545 and perplexity is 135.04370011777868
At time: 191.76092314720154 and batch: 400, loss is 4.9179541110992435 and perplexity is 136.72260758861594
At time: 194.21627140045166 and batch: 450, loss is 4.868170604705811 and perplexity is 130.08272636310355
At time: 196.6746621131897 and batch: 500, loss is 4.8772930145263675 and perplexity is 131.27482343556466
At time: 199.1345293521881 and batch: 550, loss is 4.867916641235351 and perplexity is 130.0496942971161
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.802409362792969 and perplexity of 121.80353316683554
Finished 7 epochs...
Completing Train Step...
At time: 203.07367157936096 and batch: 50, loss is 4.919126768112182 and perplexity is 136.88303035528975
At time: 205.55883479118347 and batch: 100, loss is 4.89657169342041 and perplexity is 133.83018142169388
At time: 208.01735472679138 and batch: 150, loss is 4.900734071731567 and perplexity is 134.38839420667867
At time: 210.47627520561218 and batch: 200, loss is 4.91572787284851 and perplexity is 136.41856904654958
At time: 212.933819770813 and batch: 250, loss is 4.898807363510132 and perplexity is 134.12971626117942
At time: 215.41674971580505 and batch: 300, loss is 4.89514726638794 and perplexity is 133.63968579928306
At time: 217.87268710136414 and batch: 350, loss is 4.8424778747558594 and perplexity is 126.78311551730165
At time: 220.32989501953125 and batch: 400, loss is 4.85602388381958 and perplexity is 128.5122054433574
At time: 222.79120206832886 and batch: 450, loss is 4.80398736000061 and perplexity is 121.99589053183068
At time: 225.24833393096924 and batch: 500, loss is 4.816285667419433 and perplexity is 123.50549727424271
At time: 227.706636428833 and batch: 550, loss is 4.8080417346954345 and perplexity is 122.4915116211607
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.772437540690104 and perplexity of 118.20702551066125
Finished 8 epochs...
Completing Train Step...
At time: 231.66627955436707 and batch: 50, loss is 4.861851491928101 and perplexity is 129.2633106615829
At time: 234.12660217285156 and batch: 100, loss is 4.839378490447998 and perplexity is 126.39077424083844
At time: 236.58192658424377 and batch: 150, loss is 4.841868953704834 and perplexity is 126.7059381092075
At time: 239.03753423690796 and batch: 200, loss is 4.857150812149047 and perplexity is 128.65711112217573
At time: 241.4941132068634 and batch: 250, loss is 4.842695922851562 and perplexity is 126.81076334837638
At time: 243.962881565094 and batch: 300, loss is 4.839701452255249 and perplexity is 126.43160022596926
At time: 246.42044758796692 and batch: 350, loss is 4.78792589187622 and perplexity is 120.05210920206389
At time: 248.89470267295837 and batch: 400, loss is 4.801401824951172 and perplexity is 121.68087330051017
At time: 251.35872077941895 and batch: 450, loss is 4.748166465759278 and perplexity is 115.37255095351429
At time: 253.81468749046326 and batch: 500, loss is 4.7628722000122075 and perplexity is 117.08172555982797
At time: 256.2699944972992 and batch: 550, loss is 4.755539922714234 and perplexity is 116.22638949417163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.750016784667968 and perplexity of 115.5862245873074
Finished 9 epochs...
Completing Train Step...
At time: 260.1886479854584 and batch: 50, loss is 4.811493043899536 and perplexity is 122.91499807362432
At time: 262.6681797504425 and batch: 100, loss is 4.789334163665772 and perplexity is 120.22129430206999
At time: 265.12313628196716 and batch: 150, loss is 4.790765304565429 and perplexity is 120.39347108858883
At time: 267.57766604423523 and batch: 200, loss is 4.804882335662842 and perplexity is 122.10512275754964
At time: 270.03321981430054 and batch: 250, loss is 4.793025245666504 and perplexity is 120.66586091875722
At time: 272.49202370643616 and batch: 300, loss is 4.790479040145874 and perplexity is 120.35901165395971
At time: 274.9522612094879 and batch: 350, loss is 4.739970693588257 and perplexity is 114.43084807346804
At time: 277.4386029243469 and batch: 400, loss is 4.7524071788787845 and perplexity is 115.86285172164096
At time: 279.89483428001404 and batch: 450, loss is 4.69841477394104 and perplexity is 109.77301940169001
At time: 282.35455799102783 and batch: 500, loss is 4.715291938781738 and perplexity is 111.64139887064817
At time: 284.8116247653961 and batch: 550, loss is 4.708817119598389 and perplexity is 110.92087614399227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.728728739420573 and perplexity of 113.15162567631546
Finished 10 epochs...
Completing Train Step...
At time: 288.76558661460876 and batch: 50, loss is 4.766220626831054 and perplexity is 117.47442224083025
At time: 291.2207872867584 and batch: 100, loss is 4.7449438095092775 and perplexity is 115.00134333963902
At time: 293.67558097839355 and batch: 150, loss is 4.74484733581543 and perplexity is 114.99024927040111
At time: 296.12994360923767 and batch: 200, loss is 4.758194932937622 and perplexity is 116.53538175380106
At time: 298.5868077278137 and batch: 250, loss is 4.748348970413208 and perplexity is 115.39360890252748
At time: 301.0429685115814 and batch: 300, loss is 4.746248397827149 and perplexity is 115.15147065457758
At time: 303.5058982372284 and batch: 350, loss is 4.697092609405518 and perplexity is 109.62797731436115
At time: 305.9604892730713 and batch: 400, loss is 4.7081810474395756 and perplexity is 110.85034489670528
At time: 308.41658186912537 and batch: 450, loss is 4.653716611862182 and perplexity is 104.97441059767453
At time: 310.8710210323334 and batch: 500, loss is 4.672265100479126 and perplexity is 106.93969746273424
At time: 313.3258831501007 and batch: 550, loss is 4.666712827682495 and perplexity is 106.34758439738916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.711698913574219 and perplexity of 111.24098828373859
Finished 11 epochs...
Completing Train Step...
At time: 317.2313766479492 and batch: 50, loss is 4.7245943737030025 and perplexity is 112.68478119212746
At time: 319.7092490196228 and batch: 100, loss is 4.703852987289428 and perplexity is 110.37161467023752
At time: 322.16256070137024 and batch: 150, loss is 4.703562450408936 and perplexity is 110.33955230349184
At time: 324.6162371635437 and batch: 200, loss is 4.7159654331207275 and perplexity is 111.71661404643883
At time: 327.07119250297546 and batch: 250, loss is 4.707431087493896 and perplexity is 110.76724274360612
At time: 329.52629566192627 and batch: 300, loss is 4.706167192459106 and perplexity is 110.62733300971875
At time: 331.98063564300537 and batch: 350, loss is 4.658351469039917 and perplexity is 105.46208126705534
At time: 334.4356164932251 and batch: 400, loss is 4.668167085647583 and perplexity is 106.50235372905716
At time: 336.89055347442627 and batch: 450, loss is 4.613504056930542 and perplexity is 100.83686943155048
At time: 339.39066100120544 and batch: 500, loss is 4.633423891067505 and perplexity is 102.86566267300044
At time: 341.8650677204132 and batch: 550, loss is 4.62800742149353 and perplexity is 102.31000016397034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.6988774617513025 and perplexity of 109.82382179157985
Finished 12 epochs...
Completing Train Step...
At time: 345.8373303413391 and batch: 50, loss is 4.686788759231567 and perplexity is 108.50418668901305
At time: 348.29640221595764 and batch: 100, loss is 4.667155046463012 and perplexity is 106.39462369653647
At time: 350.7596318721771 and batch: 150, loss is 4.666143684387207 and perplexity is 106.28707460376435
At time: 353.21929240226746 and batch: 200, loss is 4.677654571533203 and perplexity is 107.51760176716597
At time: 355.677951335907 and batch: 250, loss is 4.6704092121124265 and perplexity is 106.74141337575283
At time: 358.1389582157135 and batch: 300, loss is 4.66986198425293 and perplexity is 106.68301747997936
At time: 360.59717059135437 and batch: 350, loss is 4.622089729309082 and perplexity is 101.70634894825804
At time: 363.06271147727966 and batch: 400, loss is 4.631287136077881 and perplexity is 102.6460986158593
At time: 365.52318382263184 and batch: 450, loss is 4.576442232131958 and perplexity is 97.1680770519943
At time: 367.98112416267395 and batch: 500, loss is 4.5974094009399415 and perplexity is 99.22692520899803
At time: 370.4377884864807 and batch: 550, loss is 4.5923765277862545 and perplexity is 98.72878327506012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.68820546468099 and perplexity of 108.65801409991784
Finished 13 epochs...
Completing Train Step...
At time: 374.366672039032 and batch: 50, loss is 4.652503223419189 and perplexity is 104.84711310731556
At time: 376.85113167762756 and batch: 100, loss is 4.633372058868408 and perplexity is 102.86033105766847
At time: 379.3091735839844 and batch: 150, loss is 4.632011404037476 and perplexity is 102.720468824993
At time: 381.7649471759796 and batch: 200, loss is 4.642506551742554 and perplexity is 103.8042123943885
At time: 384.220751285553 and batch: 250, loss is 4.636415119171143 and perplexity is 103.17381798573626
At time: 386.67960691452026 and batch: 300, loss is 4.636730279922485 and perplexity is 103.2063394482066
At time: 389.13312792778015 and batch: 350, loss is 4.589450330734253 and perplexity is 98.44030567748457
At time: 391.5871071815491 and batch: 400, loss is 4.597378034591674 and perplexity is 99.22381287151602
At time: 394.0522928237915 and batch: 450, loss is 4.542141742706299 and perplexity is 93.89167674351327
At time: 396.5070176124573 and batch: 500, loss is 4.563627138137817 and perplexity is 95.93080383038046
At time: 398.9612603187561 and batch: 550, loss is 4.559492101669312 and perplexity is 95.53494546652365
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.679142252604167 and perplexity of 107.67767270584073
Finished 14 epochs...
Completing Train Step...
At time: 402.89401173591614 and batch: 50, loss is 4.620033807754517 and perplexity is 101.49746347287146
At time: 405.37756419181824 and batch: 100, loss is 4.6019635581970215 and perplexity is 99.67985079488899
At time: 407.8367249965668 and batch: 150, loss is 4.6002295875549315 and perplexity is 99.50715862484803
At time: 410.29552125930786 and batch: 200, loss is 4.6098659706115725 and perplexity is 100.47068270976906
At time: 412.75374579429626 and batch: 250, loss is 4.605081281661987 and perplexity is 99.99110996257684
At time: 415.21189069747925 and batch: 300, loss is 4.605290880203247 and perplexity is 100.01207014989957
At time: 417.6706383228302 and batch: 350, loss is 4.558567609786987 and perplexity is 95.44666499853722
At time: 420.1287589073181 and batch: 400, loss is 4.5657876777648925 and perplexity is 96.13829019405757
At time: 422.5888283252716 and batch: 450, loss is 4.510706748962402 and perplexity is 90.98610012422779
At time: 425.0454602241516 and batch: 500, loss is 4.532343769073487 and perplexity is 92.9762207011807
At time: 427.5025999546051 and batch: 550, loss is 4.53101282119751 and perplexity is 92.85255651127265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.670776875813802 and perplexity of 106.78066553423938
Finished 15 epochs...
Completing Train Step...
At time: 431.4589509963989 and batch: 50, loss is 4.591755170822143 and perplexity is 98.66745651289241
At time: 433.9143805503845 and batch: 100, loss is 4.573647966384888 and perplexity is 96.89694260983784
At time: 436.3710126876831 and batch: 150, loss is 4.570885543823242 and perplexity is 96.62964167846033
At time: 438.8296046257019 and batch: 200, loss is 4.580148363113404 and perplexity is 97.52886281945788
At time: 441.2860074043274 and batch: 250, loss is 4.575743789672852 and perplexity is 97.1002344361511
At time: 443.7414798736572 and batch: 300, loss is 4.57528507232666 and perplexity is 97.0557030887276
At time: 446.19399523735046 and batch: 350, loss is 4.529298152923584 and perplexity is 92.69348159779804
At time: 448.6485710144043 and batch: 400, loss is 4.536313533782959 and perplexity is 93.34604799894143
At time: 451.1036765575409 and batch: 450, loss is 4.480984039306641 and perplexity is 88.32154179607757
At time: 453.5583338737488 and batch: 500, loss is 4.50312834739685 and perplexity is 90.29917709749368
At time: 456.01354002952576 and batch: 550, loss is 4.502667541503906 and perplexity is 90.25757629024388
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.664255777994792 and perplexity of 106.08660385073983
Finished 16 epochs...
Completing Train Step...
At time: 459.9459626674652 and batch: 50, loss is 4.562141237258911 and perplexity is 95.78836601509954
At time: 462.42897748947144 and batch: 100, loss is 4.545469284057617 and perplexity is 94.2046255667357
At time: 464.88523411750793 and batch: 150, loss is 4.5426782131195065 and perplexity is 93.94206036358496
At time: 467.34088587760925 and batch: 200, loss is 4.5516910648345945 and perplexity is 94.79257323930547
At time: 469.7960135936737 and batch: 250, loss is 4.547900533676147 and perplexity is 94.43393917306197
At time: 472.2509536743164 and batch: 300, loss is 4.547310876846313 and perplexity is 94.3782719697467
At time: 474.7028419971466 and batch: 350, loss is 4.502052488327027 and perplexity is 90.20208014949742
At time: 477.15588116645813 and batch: 400, loss is 4.5086125469207765 and perplexity is 90.7957562264434
At time: 479.6087899208069 and batch: 450, loss is 4.453310422897339 and perplexity is 85.91087510496887
At time: 482.06838178634644 and batch: 500, loss is 4.475526819229126 and perplexity is 87.84086447879466
At time: 484.5271759033203 and batch: 550, loss is 4.47463092803955 and perplexity is 87.7622038631609
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.659515380859375 and perplexity of 105.58490129192076
Finished 17 epochs...
Completing Train Step...
At time: 488.4676957130432 and batch: 50, loss is 4.535659570693969 and perplexity is 93.28502308524246
At time: 490.9218602180481 and batch: 100, loss is 4.519475593566894 and perplexity is 91.78745142507337
At time: 493.37739300727844 and batch: 150, loss is 4.516733331680298 and perplexity is 91.5360910009372
At time: 495.8386580944061 and batch: 200, loss is 4.525421762466431 and perplexity is 92.33486099609586
At time: 498.2960364818573 and batch: 250, loss is 4.521727523803711 and perplexity is 91.99438327289114
At time: 500.7531809806824 and batch: 300, loss is 4.521128358840943 and perplexity is 91.9392799712949
At time: 503.21987891197205 and batch: 350, loss is 4.477662181854248 and perplexity is 88.02863698760974
At time: 505.6728813648224 and batch: 400, loss is 4.483824214935303 and perplexity is 88.57274705123008
At time: 508.1249244213104 and batch: 450, loss is 4.428346033096314 and perplexity is 83.7927118629188
At time: 510.58107233047485 and batch: 500, loss is 4.449594125747681 and perplexity is 85.59219728220181
At time: 513.0338542461395 and batch: 550, loss is 4.448863172531128 and perplexity is 85.52965625035537
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.656050109863282 and perplexity of 105.2196542014167
Finished 18 epochs...
Completing Train Step...
At time: 516.9570610523224 and batch: 50, loss is 4.511002321243286 and perplexity is 91.01299706816992
At time: 519.434770822525 and batch: 100, loss is 4.494789085388184 and perplexity is 89.54927974163155
At time: 521.888022184372 and batch: 150, loss is 4.491707420349121 and perplexity is 89.27374363003558
At time: 524.367020368576 and batch: 200, loss is 4.500302505493164 and perplexity is 90.0443660963491
At time: 526.8197538852692 and batch: 250, loss is 4.496895847320556 and perplexity is 89.73813762470901
At time: 529.2715849876404 and batch: 300, loss is 4.496512279510498 and perplexity is 89.70372356426645
At time: 531.7241444587708 and batch: 350, loss is 4.454016771316528 and perplexity is 85.9715795525118
At time: 534.1757073402405 and batch: 400, loss is 4.457866172790528 and perplexity is 86.3031564544317
At time: 536.6273076534271 and batch: 450, loss is 4.403566827774048 and perplexity is 81.74190862168605
At time: 539.0815172195435 and batch: 500, loss is 4.42475248336792 and perplexity is 83.49213897123548
At time: 541.5385704040527 and batch: 550, loss is 4.424471855163574 and perplexity is 83.46871200948586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.651856486002604 and perplexity of 104.77932647868542
Finished 19 epochs...
Completing Train Step...
At time: 545.4796006679535 and batch: 50, loss is 4.4877784538269045 and perplexity is 88.92367822794634
At time: 547.9365518093109 and batch: 100, loss is 4.471001167297363 and perplexity is 87.4442255027333
At time: 550.3947243690491 and batch: 150, loss is 4.468073091506958 and perplexity is 87.18855667454855
At time: 552.8514380455017 and batch: 200, loss is 4.476376457214355 and perplexity is 87.91552912838087
At time: 555.3086423873901 and batch: 250, loss is 4.473254194259644 and perplexity is 87.6414618064087
At time: 557.7691066265106 and batch: 300, loss is 4.472905292510986 and perplexity is 87.61088888091325
At time: 560.2265322208405 and batch: 350, loss is 4.430321178436279 and perplexity is 83.9583782009676
At time: 562.6842687129974 and batch: 400, loss is 4.434402799606323 and perplexity is 84.30176480512733
At time: 565.1413097381592 and batch: 450, loss is 4.380161647796631 and perplexity is 79.8509400904123
At time: 567.5984959602356 and batch: 500, loss is 4.4011458396911625 and perplexity is 81.54425179399469
At time: 570.0551557540894 and batch: 550, loss is 4.401249847412109 and perplexity is 81.55273346685219
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.6485346476236975 and perplexity of 104.43184395059924
Finished 20 epochs...
Completing Train Step...
At time: 573.9689884185791 and batch: 50, loss is 4.465478219985962 and perplexity is 86.96260685403763
At time: 576.4826800823212 and batch: 100, loss is 4.448557243347168 and perplexity is 85.50349423448046
At time: 578.9385042190552 and batch: 150, loss is 4.446566953659057 and perplexity is 85.33348674980016
At time: 581.3949325084686 and batch: 200, loss is 4.453477735519409 and perplexity is 85.92525028128775
At time: 583.8526141643524 and batch: 250, loss is 4.451066646575928 and perplexity is 85.71832641653329
At time: 586.3399477005005 and batch: 300, loss is 4.4506506252288816 and perplexity is 85.68267317967896
At time: 588.7995295524597 and batch: 350, loss is 4.407866048812866 and perplexity is 82.09409166890998
At time: 591.2541353702545 and batch: 400, loss is 4.411787319183349 and perplexity is 82.41663677815563
At time: 593.7088820934296 and batch: 450, loss is 4.357499341964722 and perplexity is 78.06168456954511
At time: 596.1769142150879 and batch: 500, loss is 4.378568315505982 and perplexity is 79.72381231441786
At time: 598.6350238323212 and batch: 550, loss is 4.380561380386353 and perplexity is 79.88286549387338
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.643611145019531 and perplexity of 103.91893717996983
Finished 21 epochs...
Completing Train Step...
At time: 602.6225306987762 and batch: 50, loss is 4.444714441299438 and perplexity is 85.17555174434837
At time: 605.0797803401947 and batch: 100, loss is 4.427636661529541 and perplexity is 83.73329277321852
At time: 607.5417742729187 and batch: 150, loss is 4.424428396224975 and perplexity is 83.46508462667738
At time: 610.0009834766388 and batch: 200, loss is 4.431367616653443 and perplexity is 84.04628144120174
At time: 612.4593060016632 and batch: 250, loss is 4.429275484085083 and perplexity is 83.87062928641531
At time: 614.9169054031372 and batch: 300, loss is 4.428454704284668 and perplexity is 83.80181821128228
At time: 617.388815164566 and batch: 350, loss is 4.386272916793823 and perplexity is 80.34042482801148
At time: 619.8483979701996 and batch: 400, loss is 4.390621738433838 and perplexity is 80.6905718176356
At time: 622.3088927268982 and batch: 450, loss is 4.336410684585571 and perplexity is 76.43270531300884
At time: 624.7689316272736 and batch: 500, loss is 4.35705397605896 and perplexity is 78.02692629734072
At time: 627.2267377376556 and batch: 550, loss is 4.359531497955322 and perplexity is 78.22047938274753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.642713928222657 and perplexity of 103.8257411787793
Finished 22 epochs...
Completing Train Step...
At time: 631.1685221195221 and batch: 50, loss is 4.423322801589966 and perplexity is 83.3728570694442
At time: 633.6718807220459 and batch: 100, loss is 4.40660587310791 and perplexity is 81.99070384616165
At time: 636.1373674869537 and batch: 150, loss is 4.403903903961182 and perplexity is 81.76946651686748
At time: 638.5941741466522 and batch: 200, loss is 4.410652656555175 and perplexity is 82.32317473443513
At time: 641.049245595932 and batch: 250, loss is 4.408738470077514 and perplexity is 82.16574355095206
At time: 643.5041587352753 and batch: 300, loss is 4.407537527084351 and perplexity is 82.06712640559401
At time: 645.9611382484436 and batch: 350, loss is 4.366180267333984 and perplexity is 78.74228206144923
At time: 648.4437205791473 and batch: 400, loss is 4.370497999191284 and perplexity is 79.08300516739081
At time: 650.9002797603607 and batch: 450, loss is 4.316210222244263 and perplexity is 74.90421936475049
At time: 653.3571317195892 and batch: 500, loss is 4.3363472366333005 and perplexity is 76.42785596821234
At time: 655.8134818077087 and batch: 550, loss is 4.33882155418396 and perplexity is 76.61719690179197
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.640456136067709 and perplexity of 103.5915886681834
Finished 23 epochs...
Completing Train Step...
At time: 659.7653839588165 and batch: 50, loss is 4.402921895980835 and perplexity is 81.68920766206692
At time: 662.22429895401 and batch: 100, loss is 4.386397924423218 and perplexity is 80.35046862182614
At time: 664.6797513961792 and batch: 150, loss is 4.384705495834351 and perplexity is 80.21459620121801
At time: 667.1340732574463 and batch: 200, loss is 4.3901662921905515 and perplexity is 80.65382996743656
At time: 669.5870525836945 and batch: 250, loss is 4.389102125167847 and perplexity is 80.56804647340859
At time: 672.0416433811188 and batch: 300, loss is 4.387668905258178 and perplexity is 80.45265745379182
At time: 674.4968206882477 and batch: 350, loss is 4.346857852935791 and perplexity is 77.23539627600488
At time: 676.9524219036102 and batch: 400, loss is 4.3509934139251705 and perplexity is 77.55546935252725
At time: 679.4086661338806 and batch: 450, loss is 4.296431550979614 and perplexity is 73.43726842557354
At time: 681.8637273311615 and batch: 500, loss is 4.316810989379883 and perplexity is 74.94923287802999
At time: 684.3194694519043 and batch: 550, loss is 4.318899383544922 and perplexity is 75.1059199739072
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.639118957519531 and perplexity of 103.45316079006551
Finished 24 epochs...
Completing Train Step...
At time: 688.2474946975708 and batch: 50, loss is 4.384245748519898 and perplexity is 80.17772623211863
At time: 690.7296569347382 and batch: 100, loss is 4.367604560852051 and perplexity is 78.85451409008967
At time: 693.1894142627716 and batch: 150, loss is 4.365457286834717 and perplexity is 78.68537350141942
At time: 695.6447067260742 and batch: 200, loss is 4.3705575656890865 and perplexity is 79.087716005347
At time: 698.100572347641 and batch: 250, loss is 4.370057363510131 and perplexity is 79.04816604978416
At time: 700.5558960437775 and batch: 300, loss is 4.36858645439148 and perplexity is 78.93197885288082
At time: 703.0152757167816 and batch: 350, loss is 4.32796308517456 and perplexity is 75.78975196630955
At time: 705.4714732170105 and batch: 400, loss is 4.332236614227295 and perplexity is 76.11433473619653
At time: 707.926824092865 and batch: 450, loss is 4.277637720108032 and perplexity is 72.06998927394396
At time: 710.4098932743073 and batch: 500, loss is 4.29795657157898 and perplexity is 73.54934721219118
At time: 712.866024017334 and batch: 550, loss is 4.300378837585449 and perplexity is 73.72771924077738
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.638771565755208 and perplexity of 103.4172282557085
Finished 25 epochs...
Completing Train Step...
At time: 716.8019270896912 and batch: 50, loss is 4.365648193359375 and perplexity is 78.7003964865633
At time: 719.255264043808 and batch: 100, loss is 4.349293184280396 and perplexity is 77.42371927884854
At time: 721.7092230319977 and batch: 150, loss is 4.347427911758423 and perplexity is 77.27943754693634
At time: 724.1633081436157 and batch: 200, loss is 4.351764154434204 and perplexity is 77.61526753593532
At time: 726.6169924736023 and batch: 250, loss is 4.351813507080078 and perplexity is 77.61909814927311
At time: 729.0697622299194 and batch: 300, loss is 4.350718097686768 and perplexity is 77.53412001148254
At time: 731.5244917869568 and batch: 350, loss is 4.309670581817627 and perplexity is 74.41597092834493
At time: 733.9803879261017 and batch: 400, loss is 4.313558530807495 and perplexity is 74.7058595983602
At time: 736.4364376068115 and batch: 450, loss is 4.259218864440918 and perplexity is 70.75469285580009
At time: 738.8918209075928 and batch: 500, loss is 4.279767236709595 and perplexity is 72.22362704160545
At time: 741.346533536911 and batch: 550, loss is 4.2820030879974365 and perplexity is 72.38528898983051
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.636759440104167 and perplexity of 103.20934900769507
Finished 26 epochs...
Completing Train Step...
At time: 745.2818140983582 and batch: 50, loss is 4.347472648620606 and perplexity is 77.28289486381762
At time: 747.7670969963074 and batch: 100, loss is 4.331216087341309 and perplexity is 76.03669763330282
At time: 750.2254786491394 and batch: 150, loss is 4.3294555187225345 and perplexity is 75.90294758219343
At time: 752.6843686103821 and batch: 200, loss is 4.333860960006714 and perplexity is 76.23807120277563
At time: 755.1423029899597 and batch: 250, loss is 4.334750471115112 and perplexity is 76.30591598386316
At time: 757.6016478538513 and batch: 300, loss is 4.333834552764893 and perplexity is 76.23605799217522
At time: 760.0601634979248 and batch: 350, loss is 4.292781629562378 and perplexity is 73.16971673507167
At time: 762.5193951129913 and batch: 400, loss is 4.2961214160919186 and perplexity is 73.41449649794484
At time: 764.9784781932831 and batch: 450, loss is 4.241567678451538 and perplexity is 69.51674636595052
At time: 767.4366567134857 and batch: 500, loss is 4.261950531005859 and perplexity is 70.94823531096266
At time: 769.8944227695465 and batch: 550, loss is 4.264270467758179 and perplexity is 71.11302180278388
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.636216227213541 and perplexity of 103.1532995836441
Finished 27 epochs...
Completing Train Step...
At time: 773.8227851390839 and batch: 50, loss is 4.329589347839356 and perplexity is 75.91310628638254
At time: 776.278532743454 and batch: 100, loss is 4.314403057098389 and perplexity is 74.76897730940658
At time: 778.736243724823 and batch: 150, loss is 4.313031101226807 and perplexity is 74.66646790724201
At time: 781.191703081131 and batch: 200, loss is 4.316157550811767 and perplexity is 74.9002741561174
At time: 783.6507523059845 and batch: 250, loss is 4.317384843826294 and perplexity is 74.99225517165873
At time: 786.1064348220825 and batch: 300, loss is 4.316998968124389 and perplexity is 74.96332306501445
At time: 788.56716132164 and batch: 350, loss is 4.2761572647094725 and perplexity is 71.96337180992252
At time: 791.024507522583 and batch: 400, loss is 4.279102735519409 and perplexity is 72.17565029754392
At time: 793.4905877113342 and batch: 450, loss is 4.22514142036438 and perplexity is 68.3841737795319
At time: 795.9471044540405 and batch: 500, loss is 4.245081443786621 and perplexity is 69.7614415485023
At time: 798.4064257144928 and batch: 550, loss is 4.2475044059753415 and perplexity is 69.93067582493713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.637379964192708 and perplexity of 103.27341276941502
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 802.3354725837708 and batch: 50, loss is 4.3292440414428714 and perplexity is 75.8868975304908
At time: 804.816980600357 and batch: 100, loss is 4.319055004119873 and perplexity is 75.11760890985163
At time: 807.2737700939178 and batch: 150, loss is 4.312738790512085 and perplexity is 74.64464528829032
At time: 809.7329275608063 and batch: 200, loss is 4.3053382396697994 and perplexity is 74.09427283728748
At time: 812.1899671554565 and batch: 250, loss is 4.300976085662842 and perplexity is 73.77176613149554
At time: 814.6493036746979 and batch: 300, loss is 4.288783731460572 and perplexity is 72.87777562767211
At time: 817.1089489459991 and batch: 350, loss is 4.236427631378174 and perplexity is 69.16034376499724
At time: 819.5682899951935 and batch: 400, loss is 4.229159507751465 and perplexity is 68.65950013792653
At time: 822.0271120071411 and batch: 450, loss is 4.169528560638428 and perplexity is 64.68494988651638
At time: 824.4861423969269 and batch: 500, loss is 4.176504654884338 and perplexity is 65.13777583503361
At time: 826.9448668956757 and batch: 550, loss is 4.177174787521363 and perplexity is 65.18144141375446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.611589558919271 and perplexity of 100.64400212650675
Finished 29 epochs...
Completing Train Step...
At time: 830.8999488353729 and batch: 50, loss is 4.309088926315308 and perplexity is 74.3726990552756
At time: 833.353372335434 and batch: 100, loss is 4.295870018005371 and perplexity is 73.39604255374076
At time: 835.8243210315704 and batch: 150, loss is 4.291211633682251 and perplexity is 73.05493071161742
At time: 838.2801132202148 and batch: 200, loss is 4.286302404403687 and perplexity is 72.6971661993596
At time: 840.7359673976898 and batch: 250, loss is 4.284324073791504 and perplexity is 72.55348933707607
At time: 843.1951422691345 and batch: 300, loss is 4.2757074165344235 and perplexity is 71.93100649872734
At time: 845.6520688533783 and batch: 350, loss is 4.226686401367187 and perplexity is 68.48990768632073
At time: 848.1087279319763 and batch: 400, loss is 4.223305759429931 and perplexity is 68.25875876812587
At time: 850.5687592029572 and batch: 450, loss is 4.167534999847412 and perplexity is 64.55612495948108
At time: 853.0258011817932 and batch: 500, loss is 4.17850085735321 and perplexity is 65.26793389166792
At time: 855.4857637882233 and batch: 550, loss is 4.181484651565552 and perplexity is 65.46297080537096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.609439086914063 and perplexity of 100.42780256631085
Finished 30 epochs...
Completing Train Step...
At time: 859.4149954319 and batch: 50, loss is 4.299856967926026 and perplexity is 73.68925301909714
At time: 861.9060893058777 and batch: 100, loss is 4.285517797470093 and perplexity is 72.6401498693359
At time: 864.3602955341339 and batch: 150, loss is 4.281399946212769 and perplexity is 72.34164356095955
At time: 866.8167176246643 and batch: 200, loss is 4.277281360626221 and perplexity is 72.04431102552753
At time: 869.2715921401978 and batch: 250, loss is 4.2760561752319335 and perplexity is 71.95609743795174
At time: 871.7286112308502 and batch: 300, loss is 4.269125728607178 and perplexity is 71.45913362506336
At time: 874.1901326179504 and batch: 350, loss is 4.221243448257447 and perplexity is 68.1181330241615
At time: 876.6465172767639 and batch: 400, loss is 4.219689540863037 and perplexity is 68.01236595096277
At time: 879.1019551753998 and batch: 450, loss is 4.165423994064331 and perplexity is 64.41999034739027
At time: 881.5594758987427 and batch: 500, loss is 4.178435783386231 and perplexity is 65.26368678648254
At time: 884.016539812088 and batch: 550, loss is 4.181473646163941 and perplexity is 65.46225036305096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.608672078450521 and perplexity of 100.35080312515404
Finished 31 epochs...
Completing Train Step...
At time: 887.9272544384003 and batch: 50, loss is 4.2927477741241455 and perplexity is 73.16723958417901
At time: 890.4068627357483 and batch: 100, loss is 4.278096885681152 and perplexity is 72.1030889304035
At time: 892.8604729175568 and batch: 150, loss is 4.274568290710449 and perplexity is 71.84911468307644
At time: 895.3581783771515 and batch: 200, loss is 4.270678701400757 and perplexity is 71.57019392993121
At time: 897.8128907680511 and batch: 250, loss is 4.270111598968506 and perplexity is 71.52961780537377
At time: 900.2686569690704 and batch: 300, loss is 4.264139795303345 and perplexity is 71.10372989676559
At time: 902.7337446212769 and batch: 350, loss is 4.216887254714965 and perplexity is 67.82204263470491
At time: 905.1927411556244 and batch: 400, loss is 4.21640947341919 and perplexity is 67.78964627109731
At time: 907.6488790512085 and batch: 450, loss is 4.163097791671753 and perplexity is 64.270310571936
At time: 910.1042070388794 and batch: 500, loss is 4.176766123771667 and perplexity is 65.15480956360227
At time: 912.5564036369324 and batch: 550, loss is 4.180330972671509 and perplexity is 65.38749110565152
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.60848134358724 and perplexity of 100.33166455369422
Finished 32 epochs...
Completing Train Step...
At time: 916.4910991191864 and batch: 50, loss is 4.286665925979614 and perplexity is 72.72359799175419
At time: 918.9434792995453 and batch: 100, loss is 4.2718618202209475 and perplexity is 71.65492008400501
At time: 921.3997497558594 and batch: 150, loss is 4.268815383911133 and perplexity is 71.43696010285434
At time: 923.8537223339081 and batch: 200, loss is 4.265220055580139 and perplexity is 71.180581934333
At time: 926.3054440021515 and batch: 250, loss is 4.265041236877441 and perplexity is 71.16785465298614
At time: 928.757698059082 and batch: 300, loss is 4.25976372718811 and perplexity is 70.79325495669836
At time: 931.2146248817444 and batch: 350, loss is 4.212925100326538 and perplexity is 67.55385288629374
At time: 933.6677658557892 and batch: 400, loss is 4.213173685073852 and perplexity is 67.57064783114062
At time: 936.1198637485504 and batch: 450, loss is 4.160565147399902 and perplexity is 64.10774268822207
At time: 938.5715792179108 and batch: 500, loss is 4.174681768417359 and perplexity is 65.01914522289346
At time: 941.0239698886871 and batch: 550, loss is 4.178509693145752 and perplexity is 65.26851058813922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.608923848470052 and perplexity of 100.37607162960929
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 944.9394371509552 and batch: 50, loss is 4.2900917530059814 and perplexity is 72.97316369960116
At time: 947.4239492416382 and batch: 100, loss is 4.282124195098877 and perplexity is 72.39405589322342
At time: 949.8810687065125 and batch: 150, loss is 4.282240695953369 and perplexity is 72.40249035389655
At time: 952.3376705646515 and batch: 200, loss is 4.279568557739258 and perplexity is 72.20927915111007
At time: 954.7937045097351 and batch: 250, loss is 4.277242984771728 and perplexity is 72.04154631657998
At time: 957.2920591831207 and batch: 300, loss is 4.2681355381011965 and perplexity is 71.38841048984922
At time: 959.7484741210938 and batch: 350, loss is 4.211821603775024 and perplexity is 67.47934855778736
At time: 962.2065749168396 and batch: 400, loss is 4.20433892250061 and perplexity is 66.97630649875201
At time: 964.6637995243073 and batch: 450, loss is 4.148897724151611 and perplexity is 63.364117054009874
At time: 967.1214084625244 and batch: 500, loss is 4.160897564888001 and perplexity is 64.1290567654039
At time: 969.5788445472717 and batch: 550, loss is 4.166453828811646 and perplexity is 64.48636646422112
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.596122741699219 and perplexity of 99.09933606822474
Finished 34 epochs...
Completing Train Step...
At time: 973.5153107643127 and batch: 50, loss is 4.286573486328125 and perplexity is 72.71687575840605
At time: 975.9705801010132 and batch: 100, loss is 4.273355197906494 and perplexity is 71.76200788408156
At time: 978.4281044006348 and batch: 150, loss is 4.271239280700684 and perplexity is 71.61032594667802
At time: 980.8834779262543 and batch: 200, loss is 4.26889482498169 and perplexity is 71.44263535686335
At time: 983.3411042690277 and batch: 250, loss is 4.267823991775512 and perplexity is 71.3661731570242
At time: 985.7969920635223 and batch: 300, loss is 4.260234928131103 and perplexity is 70.82662066555089
At time: 988.2528076171875 and batch: 350, loss is 4.2070478820800785 and perplexity is 67.1579885794401
At time: 990.7095365524292 and batch: 400, loss is 4.203134698867798 and perplexity is 66.8957005911381
At time: 993.1653089523315 and batch: 450, loss is 4.150447092056274 and perplexity is 63.46236747665188
At time: 995.6209063529968 and batch: 500, loss is 4.164200177192688 and perplexity is 64.3412002984615
At time: 998.0746419429779 and batch: 550, loss is 4.1698519897460935 and perplexity is 64.70587426573182
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.595174153645833 and perplexity of 99.00537619358484
Finished 35 epochs...
Completing Train Step...
At time: 1001.9916536808014 and batch: 50, loss is 4.284693326950073 and perplexity is 72.58028488904561
At time: 1004.4656612873077 and batch: 100, loss is 4.269936408996582 and perplexity is 71.51708763125119
At time: 1006.9233248233795 and batch: 150, loss is 4.267522478103638 and perplexity is 71.34465852374905
At time: 1009.379298210144 and batch: 200, loss is 4.265286698341369 and perplexity is 71.18532576292826
At time: 1011.8374524116516 and batch: 250, loss is 4.264673919677734 and perplexity is 71.14171827637003
At time: 1014.2966957092285 and batch: 300, loss is 4.257844648361206 and perplexity is 70.65752739762127
At time: 1016.757045507431 and batch: 350, loss is 4.205572233200074 and perplexity is 67.0589600524468
At time: 1019.2588295936584 and batch: 400, loss is 4.202735805511475 and perplexity is 66.86902166198328
At time: 1021.7165148258209 and batch: 450, loss is 4.150979976654053 and perplexity is 63.49619460699666
At time: 1024.174131155014 and batch: 500, loss is 4.165292315483093 and perplexity is 64.41150817293057
At time: 1026.6309943199158 and batch: 550, loss is 4.170940084457397 and perplexity is 64.77631870347051
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.5948944091796875 and perplexity of 98.97768386104501
Finished 36 epochs...
Completing Train Step...
At time: 1030.5831499099731 and batch: 50, loss is 4.282844123840332 and perplexity is 72.4461932200996
At time: 1033.0475814342499 and batch: 100, loss is 4.267516050338745 and perplexity is 71.34419993853155
At time: 1035.5115723609924 and batch: 150, loss is 4.265019512176513 and perplexity is 71.16630856942227
At time: 1037.9698424339294 and batch: 200, loss is 4.262913117408752 and perplexity is 71.01656199747612
At time: 1040.4245743751526 and batch: 250, loss is 4.262677984237671 and perplexity is 70.99986561106843
At time: 1042.8787610530853 and batch: 300, loss is 4.256351623535156 and perplexity is 70.55211266805533
At time: 1045.3331480026245 and batch: 350, loss is 4.204517278671265 and perplexity is 66.98825320165615
At time: 1047.787407875061 and batch: 400, loss is 4.202333879470825 and perplexity is 66.84215066130376
At time: 1050.2408347129822 and batch: 450, loss is 4.151103925704956 and perplexity is 63.504065387831986
At time: 1052.6945445537567 and batch: 500, loss is 4.165694341659546 and perplexity is 64.43740849123503
At time: 1055.1535699367523 and batch: 550, loss is 4.171439590454102 and perplexity is 64.8086829455009
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.5947514851888025 and perplexity of 98.96353858633267
Finished 37 epochs...
Completing Train Step...
At time: 1059.062397480011 and batch: 50, loss is 4.28123423576355 and perplexity is 72.329656787902
At time: 1061.5328316688538 and batch: 100, loss is 4.265542163848877 and perplexity is 71.20351348136981
At time: 1063.9876914024353 and batch: 150, loss is 4.263038005828857 and perplexity is 71.02543169755516
At time: 1066.44327044487 and batch: 200, loss is 4.261062407493592 and perplexity is 70.88525248743218
At time: 1068.8973052501678 and batch: 250, loss is 4.2610906219482425 and perplexity is 70.88725250438847
At time: 1071.3503277301788 and batch: 300, loss is 4.255121717453003 and perplexity is 70.46539353470232
At time: 1073.8070147037506 and batch: 350, loss is 4.203550472259521 and perplexity is 66.92351982631271
At time: 1076.2595484256744 and batch: 400, loss is 4.2018419361114505 and perplexity is 66.80927619601005
At time: 1078.712457895279 and batch: 450, loss is 4.150976533889771 and perplexity is 63.49597600494209
At time: 1081.210337638855 and batch: 500, loss is 4.165772271156311 and perplexity is 64.44243026172109
At time: 1083.6726849079132 and batch: 550, loss is 4.171627759933472 and perplexity is 64.82087910906628
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.59468739827474 and perplexity of 98.95719652176379
Finished 38 epochs...
Completing Train Step...
At time: 1087.6042354106903 and batch: 50, loss is 4.279619398117066 and perplexity is 72.2129503914662
At time: 1090.0555579662323 and batch: 100, loss is 4.263799123764038 and perplexity is 71.07951100522892
At time: 1092.517835855484 and batch: 150, loss is 4.261371221542358 and perplexity is 70.90714622962426
At time: 1094.9726612567902 and batch: 200, loss is 4.259529447555542 and perplexity is 70.77667148159985
At time: 1097.4296844005585 and batch: 250, loss is 4.259778261184692 and perplexity is 70.79428387310105
At time: 1099.8834028244019 and batch: 300, loss is 4.254095344543457 and perplexity is 70.3931068666026
At time: 1102.3378427028656 and batch: 350, loss is 4.202692909240723 and perplexity is 66.86615329184676
At time: 1104.7906322479248 and batch: 400, loss is 4.201334753036499 and perplexity is 66.77540025325224
At time: 1107.24280834198 and batch: 450, loss is 4.150732421875 and perplexity is 63.48047776603984
At time: 1109.6957545280457 and batch: 500, loss is 4.165661292076111 and perplexity is 64.43527889691805
At time: 1112.1483120918274 and batch: 550, loss is 4.1715735912323 and perplexity is 64.8173679413347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.594655354817708 and perplexity of 98.95402564189231
Finished 39 epochs...
Completing Train Step...
At time: 1116.053150653839 and batch: 50, loss is 4.278147649765015 and perplexity is 72.10674927056284
At time: 1118.5298805236816 and batch: 100, loss is 4.262227897644043 and perplexity is 70.967916713832
At time: 1120.9810245037079 and batch: 150, loss is 4.259876871109009 and perplexity is 70.80126523628603
At time: 1123.4342617988586 and batch: 200, loss is 4.258142261505127 and perplexity is 70.67855913599742
At time: 1125.8865213394165 and batch: 250, loss is 4.258577575683594 and perplexity is 70.70933321261289
At time: 1128.3385639190674 and batch: 300, loss is 4.2531287956237795 and perplexity is 70.32510135582872
At time: 1130.7920815944672 and batch: 350, loss is 4.201867198944091 and perplexity is 66.81096400889282
At time: 1133.2496583461761 and batch: 400, loss is 4.200787086486816 and perplexity is 66.73883961262563
At time: 1135.7032358646393 and batch: 450, loss is 4.150409698486328 and perplexity is 63.459994436543184
At time: 1138.1567888259888 and batch: 500, loss is 4.165436625480652 and perplexity is 64.42080406824694
At time: 1140.609031677246 and batch: 550, loss is 4.1714235496521 and perplexity is 64.80764337058758
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.59463857014974 and perplexity of 98.95236474536662
Finished 40 epochs...
Completing Train Step...
At time: 1144.5735721588135 and batch: 50, loss is 4.27675573348999 and perplexity is 72.00645253123909
At time: 1147.03466963768 and batch: 100, loss is 4.260762729644775 and perplexity is 70.86401293012533
At time: 1149.4913046360016 and batch: 150, loss is 4.258488073348999 and perplexity is 70.70300484541873
At time: 1151.9457292556763 and batch: 200, loss is 4.256853685379029 and perplexity is 70.5875430852166
At time: 1154.4005303382874 and batch: 250, loss is 4.257448425292969 and perplexity is 70.62953680094797
At time: 1156.855664730072 and batch: 300, loss is 4.252211208343506 and perplexity is 70.26060153397428
At time: 1159.3187828063965 and batch: 350, loss is 4.201048002243042 and perplexity is 66.75625509932146
At time: 1161.7758944034576 and batch: 400, loss is 4.200222940444946 and perplexity is 66.70119977859913
At time: 1164.2315979003906 and batch: 450, loss is 4.150027704238892 and perplexity is 63.43575771316525
At time: 1166.688040971756 and batch: 500, loss is 4.1651448202133174 and perplexity is 64.402008480753
At time: 1169.1423761844635 and batch: 550, loss is 4.171218938827515 and perplexity is 64.7943843817507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.59463144938151 and perplexity of 98.95166013102023
Finished 41 epochs...
Completing Train Step...
At time: 1173.0455322265625 and batch: 50, loss is 4.275429067611694 and perplexity is 71.91098736683873
At time: 1175.524971485138 and batch: 100, loss is 4.2593949508666995 and perplexity is 70.76715289376193
At time: 1177.9776487350464 and batch: 150, loss is 4.2571979522705075 and perplexity is 70.61184822273869
At time: 1180.4315567016602 and batch: 200, loss is 4.255635089874268 and perplexity is 70.50157781160685
At time: 1182.8888971805573 and batch: 250, loss is 4.256385707855225 and perplexity is 70.55451742982713
At time: 1185.343241930008 and batch: 300, loss is 4.251325912475586 and perplexity is 70.1984276389656
At time: 1187.7988183498383 and batch: 350, loss is 4.200241479873657 and perplexity is 66.70243639220041
At time: 1190.256634235382 and batch: 400, loss is 4.199632196426392 and perplexity is 66.66180808015024
At time: 1192.71102643013 and batch: 450, loss is 4.149610900878907 and perplexity is 63.409322985631526
At time: 1195.1653280258179 and batch: 500, loss is 4.164798202514649 and perplexity is 64.37968947308876
At time: 1197.6189658641815 and batch: 550, loss is 4.170963497161865 and perplexity is 64.77783531003067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.594632975260416 and perplexity of 98.95181111938632
Annealing...
Finished 42 epochs...
Completing Train Step...
At time: 1201.579777956009 and batch: 50, loss is 4.277643003463745 and perplexity is 72.07037004633939
At time: 1204.0384662151337 and batch: 100, loss is 4.26488956451416 and perplexity is 71.15706127483054
At time: 1206.520676612854 and batch: 150, loss is 4.26343168258667 and perplexity is 71.05339826376044
At time: 1208.9768295288086 and batch: 200, loss is 4.262238516807556 and perplexity is 70.96867033774518
At time: 1211.432942867279 and batch: 250, loss is 4.26310887336731 and perplexity is 71.03046527342356
At time: 1213.8904113769531 and batch: 300, loss is 4.258107767105103 and perplexity is 70.67612116355387
At time: 1216.3484330177307 and batch: 350, loss is 4.204675483703613 and perplexity is 66.99885191878396
At time: 1218.8052489757538 and batch: 400, loss is 4.198845300674439 and perplexity is 66.60937281980193
At time: 1221.2614357471466 and batch: 450, loss is 4.143391885757446 and perplexity is 63.01620312335573
At time: 1223.718317747116 and batch: 500, loss is 4.157178530693054 and perplexity is 63.891001552524855
At time: 1226.1744134426117 and batch: 550, loss is 4.167134866714478 and perplexity is 64.53029908219014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.589641316731771 and perplexity of 98.45910819291181
Finished 43 epochs...
Completing Train Step...
At time: 1230.0776648521423 and batch: 50, loss is 4.277579431533813 and perplexity is 72.06578853945382
At time: 1232.5562477111816 and batch: 100, loss is 4.26226016998291 and perplexity is 70.97020705144595
At time: 1235.0228989124298 and batch: 150, loss is 4.259931879043579 and perplexity is 70.8051599747714
At time: 1237.4783709049225 and batch: 200, loss is 4.2582129287719725 and perplexity is 70.68355397307978
At time: 1239.932948589325 and batch: 250, loss is 4.2590671730041505 and perplexity is 70.74396078878468
At time: 1242.3897805213928 and batch: 300, loss is 4.255033941268921 and perplexity is 70.45920862279596
At time: 1244.8456494808197 and batch: 350, loss is 4.20287181854248 and perplexity is 66.87811733885114
At time: 1247.3013434410095 and batch: 400, loss is 4.198519687652588 and perplexity is 66.58768747134204
At time: 1249.7556567192078 and batch: 450, loss is 4.144345140457153 and perplexity is 63.07630225547983
At time: 1252.2099692821503 and batch: 500, loss is 4.15904972076416 and perplexity is 64.01066568251338
At time: 1254.664001941681 and batch: 550, loss is 4.169263744354248 and perplexity is 64.66782252634714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.589195251464844 and perplexity of 98.41519879849028
Finished 44 epochs...
Completing Train Step...
At time: 1258.6131346225739 and batch: 50, loss is 4.27774733543396 and perplexity is 72.07788968230287
At time: 1261.066980600357 and batch: 100, loss is 4.261427087783813 and perplexity is 70.9111076560304
At time: 1263.5227670669556 and batch: 150, loss is 4.258515787124634 and perplexity is 70.70496431978384
At time: 1265.9820318222046 and batch: 200, loss is 4.256547479629517 and perplexity is 70.56593208255548
At time: 1268.4621829986572 and batch: 250, loss is 4.257450895309448 and perplexity is 70.62971125728326
At time: 1270.9172577857971 and batch: 300, loss is 4.253782129287719 and perplexity is 70.37106212418821
At time: 1273.3741245269775 and batch: 350, loss is 4.201963605880738 and perplexity is 66.81740535975777
At time: 1275.8413138389587 and batch: 400, loss is 4.1985195350646975 and perplexity is 66.58767731086803
At time: 1278.2972118854523 and batch: 450, loss is 4.144892892837524 and perplexity is 63.11086191438893
At time: 1280.7519536018372 and batch: 500, loss is 4.160008549690247 and perplexity is 64.07207039395021
At time: 1283.2065470218658 and batch: 550, loss is 4.1703096294403075 and perplexity is 64.73549301908632
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.589049784342448 and perplexity of 98.40088366393701
Finished 45 epochs...
Completing Train Step...
At time: 1287.1103937625885 and batch: 50, loss is 4.277651538848877 and perplexity is 72.07098519732962
At time: 1289.589911699295 and batch: 100, loss is 4.260816488265991 and perplexity is 70.86782258415424
At time: 1292.0435223579407 and batch: 150, loss is 4.257594966888428 and perplexity is 70.63988772435921
At time: 1294.4960141181946 and batch: 200, loss is 4.255536870956421 and perplexity is 70.49465356297937
At time: 1296.9508728981018 and batch: 250, loss is 4.256520595550537 and perplexity is 70.56403500796475
At time: 1299.4070806503296 and batch: 300, loss is 4.253078670501709 and perplexity is 70.32157638988403
At time: 1301.862078666687 and batch: 350, loss is 4.201535081863403 and perplexity is 66.78877863084362
At time: 1304.3175809383392 and batch: 400, loss is 4.198511657714843 and perplexity is 66.58715277850385
At time: 1306.7748126983643 and batch: 450, loss is 4.145193891525269 and perplexity is 63.12986106022311
At time: 1309.23308467865 and batch: 500, loss is 4.160571780204773 and perplexity is 64.10816790378021
At time: 1311.6867787837982 and batch: 550, loss is 4.170864915847778 and perplexity is 64.77144974065618
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.588977559407552 and perplexity of 98.39377692316572
Finished 46 epochs...
Completing Train Step...
At time: 1315.6430110931396 and batch: 50, loss is 4.277458791732788 and perplexity is 72.05709506146395
At time: 1318.0990824699402 and batch: 100, loss is 4.260325574874878 and perplexity is 70.83304115908084
At time: 1320.5530879497528 and batch: 150, loss is 4.256908168792725 and perplexity is 70.59138904029773
At time: 1323.0078473091125 and batch: 200, loss is 4.2548162603378294 and perplexity is 70.44387266589224
At time: 1325.4624514579773 and batch: 250, loss is 4.255883884429932 and perplexity is 70.51912040249216
At time: 1327.9164326190948 and batch: 300, loss is 4.252597808837891 and perplexity is 70.28776956851249
At time: 1330.397551059723 and batch: 350, loss is 4.201234159469604 and perplexity is 66.76868341540094
At time: 1332.8507199287415 and batch: 400, loss is 4.198485956192017 and perplexity is 66.58544140926925
At time: 1335.3042621612549 and batch: 450, loss is 4.145370426177979 and perplexity is 63.14100665208367
At time: 1337.763236284256 and batch: 500, loss is 4.160913515090942 and perplexity is 64.1300796450313
At time: 1340.2177488803864 and batch: 550, loss is 4.171156749725342 and perplexity is 64.7903550024532
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.58893788655599 and perplexity of 98.3898734388909
Finished 47 epochs...
Completing Train Step...
At time: 1344.1247136592865 and batch: 50, loss is 4.277213554382325 and perplexity is 72.03942613701766
At time: 1346.6071286201477 and batch: 100, loss is 4.259881687164307 and perplexity is 70.80160621991566
At time: 1349.0639336109161 and batch: 150, loss is 4.2563388061523435 and perplexity is 70.55120838041432
At time: 1351.5218572616577 and batch: 200, loss is 4.254245138168335 and perplexity is 70.4036520950308
At time: 1353.985194683075 and batch: 250, loss is 4.255387668609619 and perplexity is 70.4841363798445
At time: 1356.4517288208008 and batch: 300, loss is 4.252226905822754 and perplexity is 70.26170445696535
At time: 1358.9089119434357 and batch: 350, loss is 4.200996131896972 and perplexity is 66.75279251907052
At time: 1361.3672082424164 and batch: 400, loss is 4.198465461730957 and perplexity is 66.58407679051675
At time: 1363.82461476326 and batch: 450, loss is 4.145470905303955 and perplexity is 63.14735132399352
At time: 1366.2813129425049 and batch: 500, loss is 4.16111430644989 and perplexity is 64.14295770373083
At time: 1368.7394518852234 and batch: 550, loss is 4.171362829208374 and perplexity is 64.80370834119489
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.5889027913411455 and perplexity of 98.38642048573547
Finished 48 epochs...
Completing Train Step...
At time: 1372.6929924488068 and batch: 50, loss is 4.276931848526001 and perplexity is 72.01913506697947
At time: 1375.148098707199 and batch: 100, loss is 4.259458274841308 and perplexity is 70.77163429304342
At time: 1377.6052796840668 and batch: 150, loss is 4.2558399677276615 and perplexity is 70.51602350328038
At time: 1380.0622148513794 and batch: 200, loss is 4.25376636505127 and perplexity is 70.36995278686963
At time: 1382.5204753875732 and batch: 250, loss is 4.25496997833252 and perplexity is 70.45470198904627
At time: 1384.9765906333923 and batch: 300, loss is 4.251921701431274 and perplexity is 70.2402635483084
At time: 1387.4315643310547 and batch: 350, loss is 4.200801649093628 and perplexity is 66.73981151118267
At time: 1389.88498878479 and batch: 400, loss is 4.198414306640625 and perplexity is 66.58067076317244
At time: 1392.3653981685638 and batch: 450, loss is 4.1455096340179445 and perplexity is 63.14979698706053
At time: 1394.819135427475 and batch: 500, loss is 4.161248846054077 and perplexity is 64.15158805242052
At time: 1397.2746789455414 and batch: 550, loss is 4.171478204727173 and perplexity is 64.81118553399901
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.588879903157552 and perplexity of 98.38416862505085
Finished 49 epochs...
Completing Train Step...
At time: 1401.186408996582 and batch: 50, loss is 4.276629295349121 and perplexity is 71.99734874479549
At time: 1403.6785242557526 and batch: 100, loss is 4.25906886100769 and perplexity is 70.74408020494168
At time: 1406.1382110118866 and batch: 150, loss is 4.255403184890747 and perplexity is 70.48523004000441
At time: 1408.5951373577118 and batch: 200, loss is 4.253359866142273 and perplexity is 70.34135329106137
At time: 1411.0571768283844 and batch: 250, loss is 4.254624576568603 and perplexity is 70.43037101292596
At time: 1413.5144159793854 and batch: 300, loss is 4.251670227050782 and perplexity is 70.22260214233559
At time: 1415.9707217216492 and batch: 350, loss is 4.2006319141387936 and perplexity is 66.72848439362124
At time: 1418.4268085956573 and batch: 400, loss is 4.198355703353882 and perplexity is 66.57676903136041
At time: 1420.883859872818 and batch: 450, loss is 4.145516519546509 and perplexity is 63.150231808288495
At time: 1423.3397529125214 and batch: 500, loss is 4.161319084167481 and perplexity is 64.15609409718336
At time: 1425.7956094741821 and batch: 550, loss is 4.171526489257812 and perplexity is 64.8143149872242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.588862609863281 and perplexity of 98.38246725338247
Finished Training.
Improved accuracyfrom -154.82513921940262 to -98.38246725338247
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f68feaca9b0>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'anneal': 3.351606636242914, 'wordvec_source': '', 'data': 'wikitext', 'lr': 13.95553065068248, 'num_layers': 1, 'batch_size': 80, 'dropout': 0.11615909844626904, 'wordvec_dim': 200, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.944606304168701 and batch: 50, loss is 6.844094848632812 and perplexity is 938.3235716464015
At time: 5.41327691078186 and batch: 100, loss is 5.8736175918579105 and perplexity is 355.5328293152447
At time: 7.884081840515137 and batch: 150, loss is 5.729989891052246 and perplexity is 307.9661551479028
At time: 10.352285861968994 and batch: 200, loss is 5.673124332427978 and perplexity is 290.9421157474227
At time: 12.821500301361084 and batch: 250, loss is 5.600323104858399 and perplexity is 270.5137976295707
At time: 15.291991472244263 and batch: 300, loss is 5.591526536941529 and perplexity is 268.1446401406701
At time: 17.762452602386475 and batch: 350, loss is 5.5379791164398195 and perplexity is 254.16384457486788
At time: 20.231817960739136 and batch: 400, loss is 5.536915187835693 and perplexity is 253.89357618911723
At time: 22.72455883026123 and batch: 450, loss is 5.5009315490722654 and perplexity is 244.9199810095191
At time: 25.194075107574463 and batch: 500, loss is 5.49811203956604 and perplexity is 244.2303993925871
At time: 27.661876678466797 and batch: 550, loss is 5.462707815170288 and perplexity is 235.73488741398342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.18561757405599 and perplexity of 178.68376618026866
Finished 1 epochs...
Completing Train Step...
At time: 31.618184804916382 and batch: 50, loss is 5.393153343200684 and perplexity is 219.89570003827066
At time: 34.07363748550415 and batch: 100, loss is 5.347997541427612 and perplexity is 210.1869854463122
At time: 36.527616024017334 and batch: 150, loss is 5.347607421875 and perplexity is 210.10500338602702
At time: 38.982792139053345 and batch: 200, loss is 5.330209684371948 and perplexity is 206.48126551186724
At time: 41.43742537498474 and batch: 250, loss is 5.306443653106689 and perplexity is 201.63187888917963
At time: 43.890950202941895 and batch: 300, loss is 5.306622533798218 and perplexity is 201.66795016524077
At time: 46.3448007106781 and batch: 350, loss is 5.269272327423096 and perplexity is 194.2745427408695
At time: 48.82037663459778 and batch: 400, loss is 5.275424394607544 and perplexity is 195.4734167756036
At time: 51.304598331451416 and batch: 450, loss is 5.237693586349487 and perplexity is 188.23545253990395
At time: 53.78007769584656 and batch: 500, loss is 5.237762031555175 and perplexity is 188.24833679509854
At time: 56.256354331970215 and batch: 550, loss is 5.220344877243042 and perplexity is 184.99797466155917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.130436197916667 and perplexity of 169.09085904132465
Finished 2 epochs...
Completing Train Step...
At time: 60.191627979278564 and batch: 50, loss is 5.247544002532959 and perplexity is 190.0988124563425
At time: 62.64510655403137 and batch: 100, loss is 5.2297525882720945 and perplexity is 186.74659450645055
At time: 65.09968638420105 and batch: 150, loss is 5.216442623138428 and perplexity is 184.27747226160716
At time: 67.55762934684753 and batch: 200, loss is 5.209000167846679 and perplexity is 182.9110863530614
At time: 70.01112365722656 and batch: 250, loss is 5.192737874984741 and perplexity is 179.96058865224074
At time: 72.46408891677856 and batch: 300, loss is 5.193225383758545 and perplexity is 180.0483424067723
At time: 74.91970562934875 and batch: 350, loss is 5.1555357837677 and perplexity is 173.38868071711494
At time: 77.37421941757202 and batch: 400, loss is 5.16138484954834 and perplexity is 174.4058142569291
At time: 79.82931017875671 and batch: 450, loss is 5.122622156143189 and perplexity is 167.77472486581615
At time: 82.28595638275146 and batch: 500, loss is 5.126172571182251 and perplexity is 168.37145346241036
At time: 84.74158692359924 and batch: 550, loss is 5.107885389328003 and perplexity is 165.32039671631284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.109661865234375 and perplexity of 165.61434543701594
Finished 3 epochs...
Completing Train Step...
At time: 88.6594021320343 and batch: 50, loss is 5.151092481613159 and perplexity is 172.6199714869152
At time: 91.13915586471558 and batch: 100, loss is 5.13742374420166 and perplexity is 170.27652687541854
At time: 93.59533739089966 and batch: 150, loss is 5.142911691665649 and perplexity is 171.21356436374217
At time: 96.05112051963806 and batch: 200, loss is 5.119702167510987 and perplexity is 167.28553913240745
At time: 98.50671315193176 and batch: 250, loss is 5.0971815967559815 and perplexity is 163.56027825824705
At time: 100.96657514572144 and batch: 300, loss is 5.10790771484375 and perplexity is 165.32408762063358
At time: 103.48000073432922 and batch: 350, loss is 5.074567394256592 and perplexity is 159.90300205740851
At time: 105.94392776489258 and batch: 400, loss is 5.080216569900513 and perplexity is 160.8088785205313
At time: 108.39956212043762 and batch: 450, loss is 5.032661247253418 and perplexity is 153.3405472902468
At time: 110.85500693321228 and batch: 500, loss is 5.025318431854248 and perplexity is 152.21871968524803
At time: 113.31305074691772 and batch: 550, loss is 5.040993738174438 and perplexity is 154.62359407267044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.143916829427083 and perplexity of 171.38574410023526
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 117.27229309082031 and batch: 50, loss is 5.035798225402832 and perplexity is 153.82232851000924
At time: 119.731276512146 and batch: 100, loss is 4.967885723114014 and perplexity is 143.72269633734172
At time: 122.19085121154785 and batch: 150, loss is 4.958418941497802 and perplexity is 142.3685248938696
At time: 124.65107917785645 and batch: 200, loss is 4.935977592468261 and perplexity is 139.20916586561546
At time: 127.11454439163208 and batch: 250, loss is 4.882757225036621 and perplexity is 131.99410005529055
At time: 129.57933568954468 and batch: 300, loss is 4.854345149993897 and perplexity is 128.29664863893856
At time: 132.0434935092926 and batch: 350, loss is 4.792804946899414 and perplexity is 120.6392813062024
At time: 134.50177359580994 and batch: 400, loss is 4.778146829605102 and perplexity is 118.88383377962322
At time: 136.9606101512909 and batch: 450, loss is 4.709882373809815 and perplexity is 111.03909803144192
At time: 139.420973777771 and batch: 500, loss is 4.680466985702514 and perplexity is 107.82041140729378
At time: 141.88112664222717 and batch: 550, loss is 4.7247058773040775 and perplexity is 112.6973466515507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.940430196126302 and perplexity of 139.83039111534046
Finished 5 epochs...
Completing Train Step...
At time: 145.7975594997406 and batch: 50, loss is 4.85572696685791 and perplexity is 128.47405365401664
At time: 148.28357887268066 and batch: 100, loss is 4.819611673355102 and perplexity is 123.91696117764525
At time: 150.74430418014526 and batch: 150, loss is 4.8264468383789065 and perplexity is 124.76685533105655
At time: 153.20332622528076 and batch: 200, loss is 4.813906707763672 and perplexity is 123.2120318885234
At time: 155.65956377983093 and batch: 250, loss is 4.778028087615967 and perplexity is 118.86971811480181
At time: 158.11704277992249 and batch: 300, loss is 4.756788234710694 and perplexity is 116.37156688496944
At time: 160.57407784461975 and batch: 350, loss is 4.705670623779297 and perplexity is 110.57241257802698
At time: 163.0697784423828 and batch: 400, loss is 4.704819593429566 and perplexity is 110.47835212890546
At time: 165.5276129245758 and batch: 450, loss is 4.65351710319519 and perplexity is 104.95346938199437
At time: 167.98518800735474 and batch: 500, loss is 4.63849666595459 and perplexity is 103.3888027875373
At time: 170.44201588630676 and batch: 550, loss is 4.669701776504517 and perplexity is 106.66592740297288
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.933919270833333 and perplexity of 138.92292331840784
Finished 6 epochs...
Completing Train Step...
At time: 174.43141841888428 and batch: 50, loss is 4.776103878021241 and perplexity is 118.64120778407487
At time: 176.88941717147827 and batch: 100, loss is 4.744437532424927 and perplexity is 114.9431355307198
At time: 179.3547761440277 and batch: 150, loss is 4.752591953277588 and perplexity is 115.88426218840367
At time: 181.81648683547974 and batch: 200, loss is 4.7466104793548585 and perplexity is 115.19317242425487
At time: 184.27644109725952 and batch: 250, loss is 4.717760076522827 and perplexity is 111.91728554377225
At time: 186.7386565208435 and batch: 300, loss is 4.6985582447052 and perplexity is 109.78876975049805
At time: 189.2000288963318 and batch: 350, loss is 4.654130058288574 and perplexity is 105.0178208658905
At time: 191.66107845306396 and batch: 400, loss is 4.656861906051636 and perplexity is 105.30510579558376
At time: 194.12240624427795 and batch: 450, loss is 4.608843383789062 and perplexity is 100.36799522596354
At time: 196.58279466629028 and batch: 500, loss is 4.5984347057342525 and perplexity is 99.32871522511809
At time: 199.04178857803345 and batch: 550, loss is 4.621437606811523 and perplexity is 101.64004557127586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.932063293457031 and perplexity of 138.66532463828523
Finished 7 epochs...
Completing Train Step...
At time: 202.94992637634277 and batch: 50, loss is 4.71744481086731 and perplexity is 111.88200742866277
At time: 205.43220782279968 and batch: 100, loss is 4.6899014091491695 and perplexity is 108.84244840882072
At time: 207.88873434066772 and batch: 150, loss is 4.696990509033203 and perplexity is 109.61678482844943
At time: 210.3479459285736 and batch: 200, loss is 4.695627765655518 and perplexity is 109.46750701761701
At time: 212.80883479118347 and batch: 250, loss is 4.6701373195648195 and perplexity is 106.7123951260366
At time: 215.26755690574646 and batch: 300, loss is 4.653188791275024 and perplexity is 104.91901756271443
At time: 217.73409986495972 and batch: 350, loss is 4.613540201187134 and perplexity is 100.84051417110096
At time: 220.19139528274536 and batch: 400, loss is 4.618165550231933 and perplexity is 101.30801709562823
At time: 222.64919257164001 and batch: 450, loss is 4.573044881820679 and perplexity is 96.83852317713053
At time: 225.15050554275513 and batch: 500, loss is 4.565161695480347 and perplexity is 96.07812815967802
At time: 227.6085810661316 and batch: 550, loss is 4.581258192062378 and perplexity is 97.6371632611418
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.9325922648111975 and perplexity of 138.7386940262765
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 231.55217361450195 and batch: 50, loss is 4.667793827056885 and perplexity is 106.46260822873441
At time: 234.01158928871155 and batch: 100, loss is 4.625103359222412 and perplexity is 102.01331655488254
At time: 236.47012662887573 and batch: 150, loss is 4.620565671920776 and perplexity is 101.55146069497974
At time: 238.93735599517822 and batch: 200, loss is 4.605584011077881 and perplexity is 100.04139107272049
At time: 241.39766883850098 and batch: 250, loss is 4.566785917282105 and perplexity is 96.23430715043334
At time: 243.85805439949036 and batch: 300, loss is 4.535321693420411 and perplexity is 93.25350952013709
At time: 246.31741523742676 and batch: 350, loss is 4.482365198135376 and perplexity is 88.44361215316734
At time: 248.7823760509491 and batch: 400, loss is 4.470770654678344 and perplexity is 87.42407082833725
At time: 251.24463748931885 and batch: 450, loss is 4.417939538955689 and perplexity is 82.92524497097615
At time: 253.7026867866516 and batch: 500, loss is 4.405329494476319 and perplexity is 81.8861194226435
At time: 256.15964579582214 and batch: 550, loss is 4.463348321914673 and perplexity is 86.77758247690332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.866952514648437 and perplexity of 129.9243703530053
Finished 9 epochs...
Completing Train Step...
At time: 260.06822967529297 and batch: 50, loss is 4.603713846206665 and perplexity is 99.85447201667421
At time: 262.55048632621765 and batch: 100, loss is 4.570144166946411 and perplexity is 96.55802924568444
At time: 265.0080006122589 and batch: 150, loss is 4.571432628631592 and perplexity is 96.68252075080858
At time: 267.465806722641 and batch: 200, loss is 4.563081874847412 and perplexity is 95.87851054273334
At time: 269.9241440296173 and batch: 250, loss is 4.533045692443848 and perplexity is 93.04150579326415
At time: 272.3822555541992 and batch: 300, loss is 4.506958818435669 and perplexity is 90.64572878456339
At time: 274.8424789905548 and batch: 350, loss is 4.459988069534302 and perplexity is 86.48647726622393
At time: 277.300931930542 and batch: 400, loss is 4.457750701904297 and perplexity is 86.29319152781193
At time: 279.75757694244385 and batch: 450, loss is 4.411970081329346 and perplexity is 82.43170079608323
At time: 282.2147581577301 and batch: 500, loss is 4.402249412536621 and perplexity is 81.63429148952122
At time: 284.6707375049591 and batch: 550, loss is 4.4507222175598145 and perplexity is 85.68880762155932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.863200887044271 and perplexity of 129.43785568030225
Finished 10 epochs...
Completing Train Step...
At time: 288.62301421165466 and batch: 50, loss is 4.574201774597168 and perplexity is 96.95061979446093
At time: 291.07942724227905 and batch: 100, loss is 4.543411855697632 and perplexity is 94.01100554641296
At time: 293.5382351875305 and batch: 150, loss is 4.548287296295166 and perplexity is 94.47046975457717
At time: 295.9956591129303 and batch: 200, loss is 4.540537729263305 and perplexity is 93.74119395230571
At time: 298.45118021965027 and batch: 250, loss is 4.5135266399383545 and perplexity is 91.24303309813617
At time: 300.90717577934265 and batch: 300, loss is 4.489954862594605 and perplexity is 89.1174232586126
At time: 303.3636152744293 and batch: 350, loss is 4.444265270233155 and perplexity is 85.13730194194544
At time: 305.8274095058441 and batch: 400, loss is 4.4456596946716305 and perplexity is 85.25610228620103
At time: 308.29341650009155 and batch: 450, loss is 4.402134351730346 and perplexity is 81.62489912247979
At time: 310.75155115127563 and batch: 500, loss is 4.392212810516358 and perplexity is 80.81905852247627
At time: 313.20640897750854 and batch: 550, loss is 4.436823511123658 and perplexity is 84.50608225515262
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.860698445638021 and perplexity of 129.11434997656752
Finished 11 epochs...
Completing Train Step...
At time: 317.1344954967499 and batch: 50, loss is 4.549883794784546 and perplexity is 94.62141217444373
At time: 319.62560057640076 and batch: 100, loss is 4.520177507400513 and perplexity is 91.85190092333234
At time: 322.0945885181427 and batch: 150, loss is 4.527844915390014 and perplexity is 92.55887378347678
At time: 324.5580151081085 and batch: 200, loss is 4.522452125549316 and perplexity is 92.06106672015005
At time: 327.0198726654053 and batch: 250, loss is 4.498303499221802 and perplexity is 89.86454663381855
At time: 329.4820101261139 and batch: 300, loss is 4.47648286819458 and perplexity is 87.92488480377686
At time: 331.94612741470337 and batch: 350, loss is 4.432468862533569 and perplexity is 84.1388880443399
At time: 334.4075093269348 and batch: 400, loss is 4.436304340362549 and perplexity is 84.46222055495122
At time: 336.8678493499756 and batch: 450, loss is 4.3931467628479 and perplexity is 80.8945749294894
At time: 339.3282730579376 and batch: 500, loss is 4.382789926528931 and perplexity is 80.0610866589127
At time: 341.7868938446045 and batch: 550, loss is 4.423613977432251 and perplexity is 83.39713676598414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.860400899251302 and perplexity of 129.07593818318702
Finished 12 epochs...
Completing Train Step...
At time: 345.7409493923187 and batch: 50, loss is 4.531193504333496 and perplexity is 92.86933491810939
At time: 348.19923424720764 and batch: 100, loss is 4.502384357452392 and perplexity is 90.23202040279082
At time: 350.68268632888794 and batch: 150, loss is 4.510993919372559 and perplexity is 91.01223239194643
At time: 353.1416425704956 and batch: 200, loss is 4.506842384338379 and perplexity is 90.63517514537281
At time: 355.59968852996826 and batch: 250, loss is 4.484696397781372 and perplexity is 88.6500323804073
At time: 358.05640721321106 and batch: 300, loss is 4.4641636371612545 and perplexity is 86.84836241301794
At time: 360.51405096054077 and batch: 350, loss is 4.420354118347168 and perplexity is 83.12571648831468
At time: 362.9711651802063 and batch: 400, loss is 4.42625054359436 and perplexity is 83.61730895652828
At time: 365.44633173942566 and batch: 450, loss is 4.383336658477783 and perplexity is 80.1048705807928
At time: 367.9068419933319 and batch: 500, loss is 4.373644313812256 and perplexity is 79.33221702669258
At time: 370.36148619651794 and batch: 550, loss is 4.411351337432861 and perplexity is 82.38071246032625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.860636393229167 and perplexity of 129.10633836870622
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 374.2754559516907 and batch: 50, loss is 4.517215824127197 and perplexity is 91.5802671299311
At time: 376.7609794139862 and batch: 100, loss is 4.486011152267456 and perplexity is 88.76666206106857
At time: 379.2220184803009 and batch: 150, loss is 4.489411296844483 and perplexity is 89.0689952426879
At time: 381.6847941875458 and batch: 200, loss is 4.47814772605896 and perplexity is 88.07138916034279
At time: 384.14782094955444 and batch: 250, loss is 4.45040041923523 and perplexity is 85.6612375430715
At time: 386.6162419319153 and batch: 300, loss is 4.4249707794189455 and perplexity is 83.51036696494172
At time: 389.07854986190796 and batch: 350, loss is 4.372841129302978 and perplexity is 79.26852420085967
At time: 391.5410454273224 and batch: 400, loss is 4.371083536148071 and perplexity is 79.1293247491602
At time: 394.00259256362915 and batch: 450, loss is 4.322480125427246 and perplexity is 75.37533695563786
At time: 396.4631578922272 and batch: 500, loss is 4.311104860305786 and perplexity is 74.52278073381575
At time: 398.9225559234619 and batch: 550, loss is 4.3708310794830325 and perplexity is 79.10935054514381
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.842461140950521 and perplexity of 126.78099397107717
Finished 14 epochs...
Completing Train Step...
At time: 402.8676128387451 and batch: 50, loss is 4.4983487033844 and perplexity is 89.8686089772131
At time: 405.32472491264343 and batch: 100, loss is 4.468987350463867 and perplexity is 87.26830604366074
At time: 407.7826313972473 and batch: 150, loss is 4.474173574447632 and perplexity is 87.72207468130165
At time: 410.2396574020386 and batch: 200, loss is 4.465193214416504 and perplexity is 86.93782555832107
At time: 412.73055362701416 and batch: 250, loss is 4.4408477020263675 and perplexity is 84.84683603186132
At time: 415.1878650188446 and batch: 300, loss is 4.417859792709351 and perplexity is 82.9186322576361
At time: 417.64592814445496 and batch: 350, loss is 4.36801570892334 and perplexity is 78.88694163727519
At time: 420.1049795150757 and batch: 400, loss is 4.36929144859314 and perplexity is 78.98764506016877
At time: 422.5652458667755 and batch: 450, loss is 4.322928590774536 and perplexity is 75.40914776322388
At time: 425.02314591407776 and batch: 500, loss is 4.313799409866333 and perplexity is 74.7238568430026
At time: 427.4867482185364 and batch: 550, loss is 4.368423538208008 and perplexity is 78.91912060356913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.840885925292969 and perplexity of 126.58144377285431
Finished 15 epochs...
Completing Train Step...
At time: 431.41159772872925 and batch: 50, loss is 4.4889742946624756 and perplexity is 89.03008040097234
At time: 433.8965358734131 and batch: 100, loss is 4.459696979522705 and perplexity is 86.46130558034442
At time: 436.35506987571716 and batch: 150, loss is 4.46567777633667 and perplexity is 86.97996252616917
At time: 438.8135905265808 and batch: 200, loss is 4.457619667053223 and perplexity is 86.28188485311173
At time: 441.2709927558899 and batch: 250, loss is 4.434876909255982 and perplexity is 84.34174256147588
At time: 443.7277591228485 and batch: 300, loss is 4.41312819480896 and perplexity is 82.52722136107336
At time: 446.18451619148254 and batch: 350, loss is 4.36468207359314 and perplexity is 78.62439919506109
At time: 448.6391429901123 and batch: 400, loss is 4.367123413085937 and perplexity is 78.81658254285861
At time: 451.0962014198303 and batch: 450, loss is 4.321829614639282 and perplexity is 75.3263204304195
At time: 453.55556631088257 and batch: 500, loss is 4.313987731933594 and perplexity is 74.73793031932843
At time: 456.0136501789093 and batch: 550, loss is 4.365447902679444 and perplexity is 78.68463510912134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.840411376953125 and perplexity of 126.52138900942808
Finished 16 epochs...
Completing Train Step...
At time: 459.92751574516296 and batch: 50, loss is 4.48176739692688 and perplexity is 88.39075625517236
At time: 462.4095618724823 and batch: 100, loss is 4.453077697753907 and perplexity is 85.89088381056635
At time: 464.865877866745 and batch: 150, loss is 4.459528036117554 and perplexity is 86.44669974678025
At time: 467.32193422317505 and batch: 200, loss is 4.451889448165893 and perplexity is 85.78888461551647
At time: 469.78151416778564 and batch: 250, loss is 4.430501689910889 and perplexity is 83.97353501957117
At time: 472.289320230484 and batch: 300, loss is 4.409534425735473 and perplexity is 82.23116987419372
At time: 474.7589511871338 and batch: 350, loss is 4.361373252868653 and perplexity is 78.36467508078141
At time: 477.22658491134644 and batch: 400, loss is 4.365029573440552 and perplexity is 78.65172590952379
At time: 479.69109416007996 and batch: 450, loss is 4.320867013931275 and perplexity is 75.25384614853536
At time: 482.1625769138336 and batch: 500, loss is 4.313123760223388 and perplexity is 74.67338674777803
At time: 484.63035559654236 and batch: 550, loss is 4.362484769821167 and perplexity is 78.45182717215266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.8405298868815105 and perplexity of 126.53638393868525
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 488.57116508483887 and batch: 50, loss is 4.476591215133667 and perplexity is 87.9344117120113
At time: 491.0263469219208 and batch: 100, loss is 4.447694139480591 and perplexity is 85.42972767668078
At time: 493.4812879562378 and batch: 150, loss is 4.452248039245606 and perplexity is 85.81965326062921
At time: 495.94291734695435 and batch: 200, loss is 4.442138442993164 and perplexity is 84.9564220273833
At time: 498.4118585586548 and batch: 250, loss is 4.418955345153808 and perplexity is 83.00952374700279
At time: 500.8793234825134 and batch: 300, loss is 4.395939807891846 and perplexity is 81.12083294840907
At time: 503.3525140285492 and batch: 350, loss is 4.345129814147949 and perplexity is 77.10204576605572
At time: 505.8220257759094 and batch: 400, loss is 4.344414768218994 and perplexity is 77.0469339681981
At time: 508.2887818813324 and batch: 450, loss is 4.298839893341064 and perplexity is 73.61434365333011
At time: 510.7664363384247 and batch: 500, loss is 4.290961389541626 and perplexity is 73.03665143049993
At time: 513.2365505695343 and batch: 550, loss is 4.348124895095825 and perplexity is 77.3333188022177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.834207661946615 and perplexity of 125.73891599908895
Finished 18 epochs...
Completing Train Step...
At time: 517.2186324596405 and batch: 50, loss is 4.471222877502441 and perplexity is 87.46361492923988
At time: 519.7208476066589 and batch: 100, loss is 4.442879705429077 and perplexity is 85.01942037799259
At time: 522.1926772594452 and batch: 150, loss is 4.448108797073364 and perplexity is 85.46515910735867
At time: 524.660165309906 and batch: 200, loss is 4.438774366378784 and perplexity is 84.67110230258697
At time: 527.1326718330383 and batch: 250, loss is 4.416595048904419 and perplexity is 82.8138277205896
At time: 529.59983086586 and batch: 300, loss is 4.394262886047363 and perplexity is 80.98491364646964
At time: 532.0719730854034 and batch: 350, loss is 4.344210929870606 and perplexity is 77.03123044897374
At time: 534.5843436717987 and batch: 400, loss is 4.344169111251831 and perplexity is 77.02800917666889
At time: 537.0574338436127 and batch: 450, loss is 4.299518384933472 and perplexity is 73.6643073146248
At time: 539.5276663303375 and batch: 500, loss is 4.291925382614136 and perplexity is 73.1070922032718
At time: 541.9954826831818 and batch: 550, loss is 4.3475432777404786 and perplexity is 77.2883534794294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.833858235677083 and perplexity of 125.69498719412263
Finished 19 epochs...
Completing Train Step...
At time: 546.0008013248444 and batch: 50, loss is 4.4681317996978756 and perplexity is 87.19367550723686
At time: 548.4707744121552 and batch: 100, loss is 4.4399188613891605 and perplexity is 84.76806343187586
At time: 550.9459652900696 and batch: 150, loss is 4.445410137176514 and perplexity is 85.2348286414802
At time: 553.4287695884705 and batch: 200, loss is 4.436590967178344 and perplexity is 84.48643316210939
At time: 555.8980276584625 and batch: 250, loss is 4.415000104904175 and perplexity is 82.681849579779
At time: 558.36878490448 and batch: 300, loss is 4.393105382919312 and perplexity is 80.89122758701248
At time: 560.8381354808807 and batch: 350, loss is 4.343579502105713 and perplexity is 76.98260614428794
At time: 563.3054940700531 and batch: 400, loss is 4.343805732727051 and perplexity is 77.000023937253
At time: 565.769341468811 and batch: 450, loss is 4.299645557403564 and perplexity is 73.67367598224934
At time: 568.2273662090302 and batch: 500, loss is 4.292184610366821 and perplexity is 73.12604604706401
At time: 570.6851060390472 and batch: 550, loss is 4.346661624908447 and perplexity is 77.22024201344662
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.833702087402344 and perplexity of 125.67536167101451
Finished 20 epochs...
Completing Train Step...
At time: 574.6138985157013 and batch: 50, loss is 4.465555353164673 and perplexity is 86.96931481503312
At time: 577.0886063575745 and batch: 100, loss is 4.437502727508545 and perplexity is 84.5634996680594
At time: 579.5482687950134 and batch: 150, loss is 4.443222150802613 and perplexity is 85.04853987079491
At time: 582.0066599845886 and batch: 200, loss is 4.434775905609131 and perplexity is 84.33322416809708
At time: 584.4675054550171 and batch: 250, loss is 4.4136644554138185 and perplexity is 82.57148932724006
At time: 586.9290692806244 and batch: 300, loss is 4.392120227813721 and perplexity is 80.81157642197559
At time: 589.3911261558533 and batch: 350, loss is 4.34295087814331 and perplexity is 76.93422824072529
At time: 591.8525984287262 and batch: 400, loss is 4.343346490859985 and perplexity is 76.96467042102466
At time: 594.3168749809265 and batch: 450, loss is 4.299572992324829 and perplexity is 73.66833004011767
At time: 596.8136332035065 and batch: 500, loss is 4.292202434539795 and perplexity is 73.12734946997384
At time: 599.2738580703735 and batch: 550, loss is 4.345685329437256 and perplexity is 77.14488903025796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.833649190266927 and perplexity of 125.6687139802131
Finished 21 epochs...
Completing Train Step...
At time: 603.2516267299652 and batch: 50, loss is 4.463285856246948 and perplexity is 86.77216202656815
At time: 605.719235420227 and batch: 100, loss is 4.43539496421814 and perplexity is 84.38544753953983
At time: 608.1832559108734 and batch: 150, loss is 4.441272792816162 and perplexity is 84.88291130749211
At time: 610.642242193222 and batch: 200, loss is 4.433131513595581 and perplexity is 84.19466124490339
At time: 613.1172409057617 and batch: 250, loss is 4.412422485351563 and perplexity is 82.46900166597713
At time: 615.58531498909 and batch: 300, loss is 4.391145839691162 and perplexity is 80.73287293184468
At time: 618.0534687042236 and batch: 350, loss is 4.342336702346802 and perplexity is 76.88699160710016
At time: 620.5173799991608 and batch: 400, loss is 4.342801141738891 and perplexity is 76.92270924843956
At time: 622.9891266822815 and batch: 450, loss is 4.299398498535156 and perplexity is 73.65547649549458
At time: 625.4501178264618 and batch: 500, loss is 4.29203766822815 and perplexity is 73.11530153889514
At time: 627.9111385345459 and batch: 550, loss is 4.34466028213501 and perplexity is 77.06585238494593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.8336644490559895 and perplexity of 125.67063154724131
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 631.8958268165588 and batch: 50, loss is 4.461611051559448 and perplexity is 86.6269572316384
At time: 634.38245844841 and batch: 100, loss is 4.4336847400665285 and perplexity is 84.24125284688199
At time: 636.8547785282135 and batch: 150, loss is 4.4390478515625 and perplexity is 84.69426176129572
At time: 639.3258686065674 and batch: 200, loss is 4.430527725219727 and perplexity is 83.97572132495004
At time: 641.8135440349579 and batch: 250, loss is 4.408639183044434 and perplexity is 82.15758596303209
At time: 644.2885911464691 and batch: 300, loss is 4.386407680511475 and perplexity is 80.35125253191346
At time: 646.7620949745178 and batch: 350, loss is 4.336971960067749 and perplexity is 76.47561715809428
At time: 649.2264230251312 and batch: 400, loss is 4.335965690612793 and perplexity is 76.39870078628724
At time: 651.6974165439606 and batch: 450, loss is 4.29197925567627 and perplexity is 73.11103081228401
At time: 654.1669633388519 and batch: 500, loss is 4.284543542861939 and perplexity is 72.56941433139559
At time: 656.6286947727203 and batch: 550, loss is 4.339712467193603 and perplexity is 76.68548657483629
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.833892313639323 and perplexity of 125.69927069613604
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 660.5673396587372 and batch: 50, loss is 4.460383443832398 and perplexity is 86.520678557178
At time: 663.0289855003357 and batch: 100, loss is 4.4324424362182615 and perplexity is 84.13666459293376
At time: 665.4868631362915 and batch: 150, loss is 4.438064889907837 and perplexity is 84.61105145258566
At time: 667.9459998607635 and batch: 200, loss is 4.429583044052124 and perplexity is 83.89642850159387
At time: 670.4037816524506 and batch: 250, loss is 4.407336978912354 and perplexity is 82.05066964365423
At time: 672.8599603176117 and batch: 300, loss is 4.384895658493042 and perplexity is 80.22985147254296
At time: 675.3165938854218 and batch: 350, loss is 4.335164394378662 and perplexity is 76.33750731537785
At time: 677.7723264694214 and batch: 400, loss is 4.33390043258667 and perplexity is 76.24108057553039
At time: 680.2300038337708 and batch: 450, loss is 4.289662837982178 and perplexity is 72.9418711247634
At time: 682.6953954696655 and batch: 500, loss is 4.282272911071777 and perplexity is 72.40482284626685
At time: 685.1619291305542 and batch: 550, loss is 4.338217229843139 and perplexity is 76.57090925256402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.834577433268229 and perplexity of 125.78541924148905
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 689.1171588897705 and batch: 50, loss is 4.459909229278565 and perplexity is 86.4796589190221
At time: 691.6041882038116 and batch: 100, loss is 4.432019338607788 and perplexity is 84.10107410084831
At time: 694.0650916099548 and batch: 150, loss is 4.4377525806427 and perplexity is 84.5846307632118
At time: 696.5248944759369 and batch: 200, loss is 4.429265384674072 and perplexity is 83.86978224673574
At time: 698.9879884719849 and batch: 250, loss is 4.406954183578491 and perplexity is 82.01926704094267
At time: 701.4565858840942 and batch: 300, loss is 4.384474534988403 and perplexity is 80.1960719094974
At time: 703.9143855571747 and batch: 350, loss is 4.33453145980835 and perplexity is 76.28920595539923
At time: 706.3708999156952 and batch: 400, loss is 4.333238315582276 and perplexity is 76.19061676796404
At time: 708.8381044864655 and batch: 450, loss is 4.288956804275513 and perplexity is 72.89038988100731
At time: 711.297614812851 and batch: 500, loss is 4.281595077514648 and perplexity is 72.35576105738562
At time: 713.7536001205444 and batch: 550, loss is 4.33775074005127 and perplexity is 76.53519803515033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.834835815429687 and perplexity of 125.81792414915594
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 717.7068371772766 and batch: 50, loss is 4.45976487159729 and perplexity is 86.46717581702065
At time: 720.1784081459045 and batch: 100, loss is 4.4318901062011715 and perplexity is 84.09020621889958
At time: 722.6745438575745 and batch: 150, loss is 4.437621307373047 and perplexity is 84.57352779094683
At time: 725.1454086303711 and batch: 200, loss is 4.429125108718872 and perplexity is 83.85801815804734
At time: 727.6213250160217 and batch: 250, loss is 4.406817731857299 and perplexity is 82.00807613431067
At time: 730.0927906036377 and batch: 300, loss is 4.3843552684783935 and perplexity is 80.18650777423615
At time: 732.5614576339722 and batch: 350, loss is 4.334335546493531 and perplexity is 76.27426134814716
At time: 735.0362346172333 and batch: 400, loss is 4.3330316162109375 and perplexity is 76.17486984287207
At time: 737.5179944038391 and batch: 450, loss is 4.288741540908814 and perplexity is 72.87470093896934
At time: 739.9865148067474 and batch: 500, loss is 4.281380968093872 and perplexity is 72.34027066567438
At time: 742.4634351730347 and batch: 550, loss is 4.337611989974976 and perplexity is 76.52457950726156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.834883117675782 and perplexity of 125.8238757603283
Annealing...
Model not improving. Stopping early with 125.6687139802131loss at 25 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f68feaca9b0>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'anneal': 4.062539135235045, 'wordvec_source': '', 'data': 'wikitext', 'lr': 7.877676270926454, 'num_layers': 1, 'batch_size': 80, 'dropout': 0.040193015000454024, 'wordvec_dim': 200, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.9920382499694824 and batch: 50, loss is 7.071790609359741 and perplexity is 1178.2559425847264
At time: 5.476838827133179 and batch: 100, loss is 5.938622817993164 and perplexity is 379.41205012211776
At time: 7.955329179763794 and batch: 150, loss is 5.68880368232727 and perplexity is 295.5398495263741
At time: 10.436634302139282 and batch: 200, loss is 5.583758659362793 and perplexity is 266.06979440947265
At time: 12.917308568954468 and batch: 250, loss is 5.467839908599854 and perplexity is 236.94781063624814
At time: 15.394941329956055 and batch: 300, loss is 5.424495849609375 and perplexity is 226.89692730097565
At time: 17.86857533454895 and batch: 350, loss is 5.34147536277771 and perplexity is 208.82056922650187
At time: 20.341861963272095 and batch: 400, loss is 5.323553915023804 and perplexity is 205.11153718829573
At time: 22.817102670669556 and batch: 450, loss is 5.25121428489685 and perplexity is 190.79781075097216
At time: 25.317254066467285 and batch: 500, loss is 5.234779644012451 and perplexity is 187.6877436690027
At time: 27.875004053115845 and batch: 550, loss is 5.211114501953125 and perplexity is 183.29823063333816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.989691670735677 and perplexity of 146.8911256800161
Finished 1 epochs...
Completing Train Step...
At time: 32.13086414337158 and batch: 50, loss is 5.194324407577515 and perplexity is 180.246328599486
At time: 34.5976676940918 and batch: 100, loss is 5.116442470550537 and perplexity is 166.74112676070706
At time: 37.06376361846924 and batch: 150, loss is 5.082823076248169 and perplexity is 161.2285746158092
At time: 39.53340291976929 and batch: 200, loss is 5.062754487991333 and perplexity is 158.02519589312155
At time: 42.00187611579895 and batch: 250, loss is 5.026355876922607 and perplexity is 152.3767201895482
At time: 44.46342182159424 and batch: 300, loss is 5.005105781555176 and perplexity is 149.17286206115355
At time: 46.92229604721069 and batch: 350, loss is 4.9435830783844 and perplexity is 140.2719556092732
At time: 49.38781142234802 and batch: 400, loss is 4.93069694519043 and perplexity is 138.47598889106905
At time: 51.84659504890442 and batch: 450, loss is 4.868957862854004 and perplexity is 130.18517537098896
At time: 54.318946838378906 and batch: 500, loss is 4.868615894317627 and perplexity is 130.14066374833035
At time: 56.77715444564819 and batch: 550, loss is 4.859527912139892 and perplexity is 128.96330572325607
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.844819132486979 and perplexity of 127.08029521806903
Finished 2 epochs...
Completing Train Step...
At time: 60.93060231208801 and batch: 50, loss is 4.897401609420776 and perplexity is 133.9412953318161
At time: 63.411141872406006 and batch: 100, loss is 4.8494259643554685 and perplexity is 127.66708335129434
At time: 65.86822724342346 and batch: 150, loss is 4.824705629348755 and perplexity is 124.54979918079435
At time: 68.32210326194763 and batch: 200, loss is 4.8155364894866945 and perplexity is 123.41300433227457
At time: 70.77715539932251 and batch: 250, loss is 4.795805997848511 and perplexity is 121.00186973690995
At time: 73.23195433616638 and batch: 300, loss is 4.783927564620972 and perplexity is 119.57305991789401
At time: 75.68619298934937 and batch: 350, loss is 4.729101495742798 and perplexity is 113.19381152218824
At time: 78.14371395111084 and batch: 400, loss is 4.720825424194336 and perplexity is 112.26087727943639
At time: 80.599524974823 and batch: 450, loss is 4.663831691741944 and perplexity is 106.04162351887972
At time: 83.05189180374146 and batch: 500, loss is 4.666418218612671 and perplexity is 106.31625804921153
At time: 85.50450992584229 and batch: 550, loss is 4.661427783966064 and perplexity is 105.78701538538196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.783390299479167 and perplexity of 119.50883473542196
Finished 3 epochs...
Completing Train Step...
At time: 89.42371940612793 and batch: 50, loss is 4.709482860565186 and perplexity is 110.99474530144819
At time: 91.91905927658081 and batch: 100, loss is 4.67448787689209 and perplexity is 107.17766487606278
At time: 94.37554979324341 and batch: 150, loss is 4.654025068283081 and perplexity is 105.0067956230812
At time: 96.83238530158997 and batch: 200, loss is 4.6449036788940425 and perplexity is 104.05334276965495
At time: 99.28973984718323 and batch: 250, loss is 4.631957807540894 and perplexity is 102.71496351527067
At time: 101.74684309959412 and batch: 300, loss is 4.624752244949341 and perplexity is 101.9775045108246
At time: 104.20417547225952 and batch: 350, loss is 4.575162029266357 and perplexity is 97.04376179266156
At time: 106.66411924362183 and batch: 400, loss is 4.571377267837525 and perplexity is 96.6771684778418
At time: 109.12763547897339 and batch: 450, loss is 4.515304460525512 and perplexity is 91.40539111976642
At time: 111.59433794021606 and batch: 500, loss is 4.524695663452149 and perplexity is 92.26784107903507
At time: 114.07437324523926 and batch: 550, loss is 4.514278078079224 and perplexity is 91.31162236036111
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.748034159342448 and perplexity of 115.35728743445019
Finished 4 epochs...
Completing Train Step...
At time: 118.03431582450867 and batch: 50, loss is 4.5718056583404545 and perplexity is 96.7185929309555
At time: 120.50448083877563 and batch: 100, loss is 4.538926076889038 and perplexity is 93.59023741191405
At time: 122.96093988418579 and batch: 150, loss is 4.521575517654419 and perplexity is 91.98040062368463
At time: 125.41858649253845 and batch: 200, loss is 4.514599943161011 and perplexity is 91.34101711347878
At time: 127.87997174263 and batch: 250, loss is 4.507004852294922 and perplexity is 90.64990165333006
At time: 130.3407278060913 and batch: 300, loss is 4.502289619445801 and perplexity is 90.22347240596362
At time: 132.7975378036499 and batch: 350, loss is 4.460412073135376 and perplexity is 86.5231556193564
At time: 135.25818848609924 and batch: 400, loss is 4.454873552322388 and perplexity is 86.04526993266732
At time: 137.7117199897766 and batch: 450, loss is 4.3998329544067385 and perplexity is 81.43726379263893
At time: 140.1737825870514 and batch: 500, loss is 4.406195602416992 and perplexity is 81.9570723629235
At time: 142.6260941028595 and batch: 550, loss is 4.398621768951416 and perplexity is 81.33868787212326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.750733439127604 and perplexity of 115.66908965989566
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 146.57246828079224 and batch: 50, loss is 4.461066303253173 and perplexity is 86.57978019438099
At time: 149.06782984733582 and batch: 100, loss is 4.4129547691345214 and perplexity is 82.51291026304109
At time: 151.52597618103027 and batch: 150, loss is 4.378572607040406 and perplexity is 79.72415445263695
At time: 153.9814383983612 and batch: 200, loss is 4.343903503417969 and perplexity is 77.00755265083178
At time: 156.45236110687256 and batch: 250, loss is 4.316198558807373 and perplexity is 74.90334572920995
At time: 158.9081573486328 and batch: 300, loss is 4.283822431564331 and perplexity is 72.5171025704282
At time: 161.36194586753845 and batch: 350, loss is 4.216181774139404 and perplexity is 67.77421237467468
At time: 163.817724943161 and batch: 400, loss is 4.185234565734863 and perplexity is 65.70891216846107
At time: 166.27219462394714 and batch: 450, loss is 4.096389427185058 and perplexity is 60.12281742750816
At time: 168.72620344161987 and batch: 500, loss is 4.067654881477356 and perplexity is 58.419800467868356
At time: 171.18125224113464 and batch: 550, loss is 4.06960207939148 and perplexity is 58.533666205068315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.630045572916667 and perplexity of 102.51873608185124
Finished 6 epochs...
Completing Train Step...
At time: 175.15137648582458 and batch: 50, loss is 4.31830322265625 and perplexity is 75.06115810587427
At time: 177.61079025268555 and batch: 100, loss is 4.2869361305236815 and perplexity is 72.7432508934246
At time: 180.07235479354858 and batch: 150, loss is 4.265601844787597 and perplexity is 71.20776310070397
At time: 182.53536534309387 and batch: 200, loss is 4.241854200363159 and perplexity is 69.53666729076338
At time: 184.99926590919495 and batch: 250, loss is 4.226185340881347 and perplexity is 68.45559869605773
At time: 187.46309185028076 and batch: 300, loss is 4.205291719436645 and perplexity is 67.04015172931099
At time: 189.92336916923523 and batch: 350, loss is 4.149368448257446 and perplexity is 63.39395109260182
At time: 192.38318300247192 and batch: 400, loss is 4.129475069046021 and perplexity is 62.145292383500696
At time: 194.84301257133484 and batch: 450, loss is 4.056287884712219 and perplexity is 57.75950269591384
At time: 197.30569076538086 and batch: 500, loss is 4.042518615722656 and perplexity is 56.96964691415733
At time: 199.76437616348267 and batch: 550, loss is 4.050990810394287 and perplexity is 57.45435522549182
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.623480733235677 and perplexity of 101.84792132003996
Finished 7 epochs...
Completing Train Step...
At time: 203.7128336429596 and batch: 50, loss is 4.255807905197144 and perplexity is 70.51376261736988
At time: 206.2154335975647 and batch: 100, loss is 4.2263257026672365 and perplexity is 68.46520792051301
At time: 208.67283058166504 and batch: 150, loss is 4.2088473033905025 and perplexity is 67.27894288649733
At time: 211.13830161094666 and batch: 200, loss is 4.1876696729660035 and perplexity is 65.86911539248379
At time: 213.59995532035828 and batch: 250, loss is 4.177809829711914 and perplexity is 65.22284752501542
At time: 216.05700135231018 and batch: 300, loss is 4.159982948303223 and perplexity is 64.07043008107588
At time: 218.51239728927612 and batch: 350, loss is 4.109304161071777 and perplexity is 60.90432322251897
At time: 220.968581199646 and batch: 400, loss is 4.094413070678711 and perplexity is 60.00411064820211
At time: 223.42472076416016 and batch: 450, loss is 4.025666046142578 and perplexity is 56.01760667004699
At time: 225.88343143463135 and batch: 500, loss is 4.0170904779434204 and perplexity is 55.53927776613311
At time: 228.33780789375305 and batch: 550, loss is 4.025934338569641 and perplexity is 56.032637785975886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.623864237467448 and perplexity of 101.88698791948751
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 232.30662512779236 and batch: 50, loss is 4.222043914794922 and perplexity is 68.17268113930042
At time: 234.7682647705078 and batch: 100, loss is 4.202314496040344 and perplexity is 66.84085504368001
At time: 237.25596356391907 and batch: 150, loss is 4.179492959976196 and perplexity is 65.33271851126044
At time: 239.71559238433838 and batch: 200, loss is 4.152464151382446 and perplexity is 63.590504022895495
At time: 242.17666697502136 and batch: 250, loss is 4.140386981964111 and perplexity is 62.827129711524
At time: 244.64232349395752 and batch: 300, loss is 4.107409472465515 and perplexity is 60.789037744765835
At time: 247.1033012866974 and batch: 350, loss is 4.043203358650207 and perplexity is 57.00866983577695
At time: 249.5661265850067 and batch: 400, loss is 4.0132881546020505 and perplexity is 55.32850044962086
At time: 252.02956223487854 and batch: 450, loss is 3.9339695024490355 and perplexity is 51.10945464228437
At time: 254.4933853149414 and batch: 500, loss is 3.9120714044570923 and perplexity is 50.00242001000991
At time: 256.95540738105774 and batch: 550, loss is 3.929665107727051 and perplexity is 50.889932170103094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.595772298177083 and perplexity of 99.06461343236847
Finished 9 epochs...
Completing Train Step...
At time: 260.8825616836548 and batch: 50, loss is 4.189834394454956 and perplexity is 66.01185812542164
At time: 263.3635678291321 and batch: 100, loss is 4.165671954154968 and perplexity is 64.43596591460533
At time: 265.82207345962524 and batch: 150, loss is 4.145264248847962 and perplexity is 63.13430286448419
At time: 268.2839186191559 and batch: 200, loss is 4.120982055664062 and perplexity is 61.6197265615819
At time: 270.74149346351624 and batch: 250, loss is 4.113138055801391 and perplexity is 61.138272167583246
At time: 273.19651103019714 and batch: 300, loss is 4.085361227989197 and perplexity is 59.46341372151899
At time: 275.6494154930115 and batch: 350, loss is 4.026895208358765 and perplexity is 56.08650372976395
At time: 278.10518312454224 and batch: 400, loss is 4.003889026641846 and perplexity is 54.81089711461101
At time: 280.5600516796112 and batch: 450, loss is 3.9299426460266114 and perplexity is 50.904058035485996
At time: 283.0217480659485 and batch: 500, loss is 3.9142036056518554 and perplexity is 50.109148973072585
At time: 285.4813289642334 and batch: 550, loss is 3.932689356803894 and perplexity is 51.04406895702795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.594183858235677 and perplexity of 98.90738015448207
Finished 10 epochs...
Completing Train Step...
At time: 289.43257880210876 and batch: 50, loss is 4.172312421798706 and perplexity is 64.86527468930936
At time: 291.88795924186707 and batch: 100, loss is 4.1468476390838624 and perplexity is 63.23434828776119
At time: 294.3449354171753 and batch: 150, loss is 4.1272696352005 and perplexity is 62.00838607673165
At time: 296.8027238845825 and batch: 200, loss is 4.104274339675904 and perplexity is 60.59875447679838
At time: 299.2854654788971 and batch: 250, loss is 4.099044127464294 and perplexity is 60.28263753109812
At time: 301.7408230304718 and batch: 300, loss is 4.0731054258346555 and perplexity is 58.73908954082607
At time: 304.1971507072449 and batch: 350, loss is 4.01694356918335 and perplexity is 55.53111915900136
At time: 306.65677785873413 and batch: 400, loss is 3.9970736932754516 and perplexity is 54.43861264107277
At time: 309.1240077018738 and batch: 450, loss is 3.925381031036377 and perplexity is 50.672382131158855
At time: 311.5820589065552 and batch: 500, loss is 3.912377128601074 and perplexity is 50.01770929409704
At time: 314.0397379398346 and batch: 550, loss is 3.9305587100982664 and perplexity is 50.935427858661434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.594103495279948 and perplexity of 98.89943198444293
Finished 11 epochs...
Completing Train Step...
At time: 317.95921182632446 and batch: 50, loss is 4.1577497005462645 and perplexity is 63.92750459022804
At time: 320.44122099876404 and batch: 100, loss is 4.13194372177124 and perplexity is 62.298897049182315
At time: 322.8972682952881 and batch: 150, loss is 4.113218021392822 and perplexity is 61.14316132115563
At time: 325.3526031970978 and batch: 200, loss is 4.091084942817688 and perplexity is 59.80474124323872
At time: 327.81370663642883 and batch: 250, loss is 4.088049077987671 and perplexity is 59.623457448411706
At time: 330.27703404426575 and batch: 300, loss is 4.063161129951477 and perplexity is 58.15786537677063
At time: 332.730819940567 and batch: 350, loss is 4.008302149772644 and perplexity is 55.05331887758863
At time: 335.18748593330383 and batch: 400, loss is 3.990431170463562 and perplexity is 54.07820126037074
At time: 337.64416694641113 and batch: 450, loss is 3.9201510524749756 and perplexity is 50.40805846500774
At time: 340.0980291366577 and batch: 500, loss is 3.908767080307007 and perplexity is 49.83746848275172
At time: 342.5512628555298 and batch: 550, loss is 3.926599941253662 and perplexity is 50.73418487381804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.59451904296875 and perplexity of 98.94053795498192
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 346.51960253715515 and batch: 50, loss is 4.151712388992309 and perplexity is 63.542717038080745
At time: 348.97389698028564 and batch: 100, loss is 4.130243821144104 and perplexity is 62.193085075418885
At time: 351.4307336807251 and batch: 150, loss is 4.112983946800232 and perplexity is 61.128850935493865
At time: 353.8868134021759 and batch: 200, loss is 4.090118765830994 and perplexity is 59.74698718336882
At time: 356.34166741371155 and batch: 250, loss is 4.088003959655762 and perplexity is 59.62076739815469
At time: 358.7954349517822 and batch: 300, loss is 4.057396478652954 and perplexity is 57.823570036393676
At time: 361.2756586074829 and batch: 350, loss is 3.99353479385376 and perplexity is 54.24630035390733
At time: 363.7312443256378 and batch: 400, loss is 3.9669399642944336 and perplexity is 52.82264409938999
At time: 366.18682074546814 and batch: 450, loss is 3.891242055892944 and perplexity is 48.97167432167434
At time: 368.63931131362915 and batch: 500, loss is 3.875732364654541 and perplexity is 48.21799852103018
At time: 371.09103751182556 and batch: 550, loss is 3.8969676017761232 and perplexity is 49.25286811579309
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.586241149902344 and perplexity of 98.12489930415425
Finished 13 epochs...
Completing Train Step...
At time: 375.01298475265503 and batch: 50, loss is 4.143815813064575 and perplexity is 63.042923075909876
At time: 377.4975218772888 and batch: 100, loss is 4.119410266876221 and perplexity is 61.52294944278182
At time: 379.95650148391724 and batch: 150, loss is 4.101493978500367 and perplexity is 60.43050206219413
At time: 382.4163954257965 and batch: 200, loss is 4.078846879005432 and perplexity is 59.077307274266204
At time: 384.8778462409973 and batch: 250, loss is 4.077309107780456 and perplexity is 58.9865297065453
At time: 387.3440566062927 and batch: 300, loss is 4.049261593818665 and perplexity is 57.355090052320755
At time: 389.831426858902 and batch: 350, loss is 3.9886061096191407 and perplexity is 53.979595261058016
At time: 392.3210723400116 and batch: 400, loss is 3.9653640365600586 and perplexity is 52.73946498889926
At time: 394.79894375801086 and batch: 450, loss is 3.892505655288696 and perplexity is 49.033594012359195
At time: 397.2579655647278 and batch: 500, loss is 3.879263825416565 and perplexity is 48.38857951359219
At time: 399.71510004997253 and batch: 550, loss is 3.8997894048690798 and perplexity is 49.39204628572306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.5853734334309895 and perplexity of 98.03979164277786
Finished 14 epochs...
Completing Train Step...
At time: 403.7014331817627 and batch: 50, loss is 4.139275550842285 and perplexity is 62.75734047441676
At time: 406.1566364765167 and batch: 100, loss is 4.1137368822097775 and perplexity is 61.174894343588996
At time: 408.61332988739014 and batch: 150, loss is 4.095717802047729 and perplexity is 60.082450989071546
At time: 411.0693554878235 and batch: 200, loss is 4.073436803817749 and perplexity is 58.75855760731302
At time: 413.52445220947266 and batch: 250, loss is 4.072479124069214 and perplexity is 58.70231266326005
At time: 415.98011207580566 and batch: 300, loss is 4.04546480178833 and perplexity is 57.13773758567352
At time: 418.4350600242615 and batch: 350, loss is 3.9859384155273436 and perplexity is 53.83578611838851
At time: 420.89004039764404 and batch: 400, loss is 3.96424307346344 and perplexity is 52.680379117638964
At time: 423.3908553123474 and batch: 450, loss is 3.8926100015640257 and perplexity is 49.03871075221214
At time: 425.8476574420929 and batch: 500, loss is 3.8804822683334352 and perplexity is 48.44757416906936
At time: 428.3019914627075 and batch: 550, loss is 3.9004844760894777 and perplexity is 49.42638920961918
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.585166931152344 and perplexity of 98.01954829262658
Finished 15 epochs...
Completing Train Step...
At time: 432.2467179298401 and batch: 50, loss is 4.135232896804809 and perplexity is 62.50414639143776
At time: 434.7437598705292 and batch: 100, loss is 4.109206147193909 and perplexity is 60.89835404615684
At time: 437.20623540878296 and batch: 150, loss is 4.091300320625305 and perplexity is 59.81762324449159
At time: 439.6701271533966 and batch: 200, loss is 4.069370489120484 and perplexity is 58.520111947027225
At time: 442.14519333839417 and batch: 250, loss is 4.068982710838318 and perplexity is 58.49742351786861
At time: 444.6052963733673 and batch: 300, loss is 4.042616372108459 and perplexity is 56.97521633315884
At time: 447.06885719299316 and batch: 350, loss is 3.983718433380127 and perplexity is 53.71640419622164
At time: 449.5305814743042 and batch: 400, loss is 3.9630176973342897 and perplexity is 52.61586537346721
At time: 451.9989402294159 and batch: 450, loss is 3.892120461463928 and perplexity is 49.014710211934606
At time: 454.4650876522064 and batch: 500, loss is 3.880784707069397 and perplexity is 48.46222880811502
At time: 456.92273330688477 and batch: 550, loss is 3.900400233268738 and perplexity is 49.42222556655417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.58515370686849 and perplexity of 98.0182520628676
Finished 16 epochs...
Completing Train Step...
At time: 460.8909811973572 and batch: 50, loss is 4.131515703201294 and perplexity is 62.27223767011977
At time: 463.35150504112244 and batch: 100, loss is 4.105232038497925 and perplexity is 60.65681763164637
At time: 465.80696749687195 and batch: 150, loss is 4.087498474121094 and perplexity is 59.590637578404696
At time: 468.2669644355774 and batch: 200, loss is 4.065885009765625 and perplexity is 58.31649636009057
At time: 470.723619222641 and batch: 250, loss is 4.066015129089355 and perplexity is 58.32408495686008
At time: 473.18046283721924 and batch: 300, loss is 4.040119791030884 and perplexity is 56.83315049939726
At time: 475.63664650917053 and batch: 350, loss is 3.981687517166138 and perplexity is 53.60742138492099
At time: 478.09028935432434 and batch: 400, loss is 3.9618325567245485 and perplexity is 52.55354511112766
At time: 480.5459985733032 and batch: 450, loss is 3.891340584754944 and perplexity is 48.97649968273025
At time: 483.0127160549164 and batch: 500, loss is 3.880632438659668 and perplexity is 48.454850103388566
At time: 485.51357340812683 and batch: 550, loss is 3.899941291809082 and perplexity is 49.399548862251116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.585208638509115 and perplexity of 98.0236365141516
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 489.45475339889526 and batch: 50, loss is 4.130372447967529 and perplexity is 62.201085288901155
At time: 491.9408664703369 and batch: 100, loss is 4.1060282468795775 and perplexity is 60.70513232998053
At time: 494.3977746963501 and batch: 150, loss is 4.088762335777282 and perplexity is 59.66599951380654
At time: 496.8550817966461 and batch: 200, loss is 4.06663462638855 and perplexity is 58.36022776399863
At time: 499.3126769065857 and batch: 250, loss is 4.0670543384552005 and perplexity is 58.38472739683979
At time: 501.7720685005188 and batch: 300, loss is 4.038600392341614 and perplexity is 56.74686385352962
At time: 504.2254238128662 and batch: 350, loss is 3.976693892478943 and perplexity is 53.340393315945576
At time: 506.68101382255554 and batch: 400, loss is 3.952824206352234 and perplexity is 52.08225034479261
At time: 509.1358916759491 and batch: 450, loss is 3.880598168373108 and perplexity is 48.45318957024393
At time: 511.5921323299408 and batch: 500, loss is 3.869041934013367 and perplexity is 47.89647610730091
At time: 514.0472657680511 and batch: 550, loss is 3.890588903427124 and perplexity is 48.93969879541946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.583905029296875 and perplexity of 97.89593525292938
Finished 18 epochs...
Completing Train Step...
At time: 517.990939617157 and batch: 50, loss is 4.128404712677002 and perplexity is 62.07881036006923
At time: 520.4717428684235 and batch: 100, loss is 4.103074684143066 and perplexity is 60.526100434332726
At time: 522.9261121749878 and batch: 150, loss is 4.085669894218444 and perplexity is 59.4817709021857
At time: 525.3816649913788 and batch: 200, loss is 4.063746037483216 and perplexity is 58.191892300591256
At time: 527.8354086875916 and batch: 250, loss is 4.06438117980957 and perplexity is 58.22886417440009
At time: 530.2932841777802 and batch: 300, loss is 4.036610617637634 and perplexity is 56.634062641020186
At time: 532.7466535568237 and batch: 350, loss is 3.97580331325531 and perplexity is 53.29291061656911
At time: 535.200846195221 and batch: 400, loss is 3.9529587411880494 and perplexity is 52.0892576931473
At time: 537.6553862094879 and batch: 450, loss is 3.8814871215820315 and perplexity is 48.49628133904032
At time: 540.1104536056519 and batch: 500, loss is 3.870419182777405 and perplexity is 47.96248691603966
At time: 542.5644352436066 and batch: 550, loss is 3.891557149887085 and perplexity is 48.987107433448855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.583512369791666 and perplexity of 97.85750302931379
Finished 19 epochs...
Completing Train Step...
At time: 546.5479521751404 and batch: 50, loss is 4.127112817764282 and perplexity is 61.998662843009306
At time: 549.0004570484161 and batch: 100, loss is 4.101279406547547 and perplexity is 60.41753676240135
At time: 551.4526987075806 and batch: 150, loss is 4.083781776428222 and perplexity is 59.36956827157628
At time: 553.9050281047821 and batch: 200, loss is 4.061967296600342 and perplexity is 58.08847600536726
At time: 556.3577029705048 and batch: 250, loss is 4.062829985618591 and perplexity is 58.13860991758241
At time: 558.8113467693329 and batch: 300, loss is 4.035452079772949 and perplexity is 56.56848792774838
At time: 561.263396024704 and batch: 350, loss is 3.9752396726608277 and perplexity is 53.26288103248881
At time: 563.7188901901245 and batch: 400, loss is 3.952987937927246 and perplexity is 52.09077855182106
At time: 566.1700592041016 and batch: 450, loss is 3.8819512844085695 and perplexity is 48.51879673506393
At time: 568.622475862503 and batch: 500, loss is 3.8711843967437742 and perplexity is 47.99920252674709
At time: 571.0746932029724 and batch: 550, loss is 3.892021656036377 and perplexity is 49.009867531781296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.583344014485677 and perplexity of 97.84102958618261
Finished 20 epochs...
Completing Train Step...
At time: 575.01917719841 and batch: 50, loss is 4.125978803634643 and perplexity is 61.928395333031446
At time: 577.5113666057587 and batch: 100, loss is 4.0998748016357425 and perplexity is 60.33273356494207
At time: 579.9733653068542 and batch: 150, loss is 4.082340359687805 and perplexity is 59.28405362792377
At time: 582.4326829910278 and batch: 200, loss is 4.060629358291626 and perplexity is 58.010809176323534
At time: 584.8947629928589 and batch: 250, loss is 4.061710348129273 and perplexity is 58.07355217768041
At time: 587.3552174568176 and batch: 300, loss is 4.034583301544189 and perplexity is 56.51936379907942
At time: 589.8155534267426 and batch: 350, loss is 3.9747543430328367 and perplexity is 53.2370372501355
At time: 592.2750797271729 and batch: 400, loss is 3.9528994226455687 and perplexity is 52.086167925943194
At time: 594.7332298755646 and batch: 450, loss is 3.882159538269043 and perplexity is 48.52890201398468
At time: 597.1918711662292 and batch: 500, loss is 3.8716241359710692 and perplexity is 48.02031430047431
At time: 599.6498565673828 and batch: 550, loss is 3.8922400856018067 and perplexity is 49.020573905099695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.58326416015625 and perplexity of 97.83321686831837
Finished 21 epochs...
Completing Train Step...
At time: 603.6017625331879 and batch: 50, loss is 4.124925184249878 and perplexity is 61.863180736955414
At time: 606.0644998550415 and batch: 100, loss is 4.0986606407165525 and perplexity is 60.25952437056465
At time: 608.5436522960663 and batch: 150, loss is 4.081125564575196 and perplexity is 59.21207937515732
At time: 611.0030252933502 and batch: 200, loss is 4.059509973526001 and perplexity is 57.94590909115008
At time: 613.4588930606842 and batch: 250, loss is 4.0607934665679934 and perplexity is 58.02033001143069
At time: 615.9154722690582 and batch: 300, loss is 4.033845143318176 and perplexity is 56.477658960040785
At time: 618.3699927330017 and batch: 350, loss is 3.97428946018219 and perplexity is 53.21229401629682
At time: 620.8298306465149 and batch: 400, loss is 3.952741003036499 and perplexity is 52.077917109145275
At time: 623.2874433994293 and batch: 450, loss is 3.8822207832336426 and perplexity is 48.53187425588708
At time: 625.7484283447266 and batch: 500, loss is 3.871880850791931 and perplexity is 48.03264340932247
At time: 628.2027368545532 and batch: 550, loss is 3.8923255825042724 and perplexity is 49.024765191494126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.58323008219401 and perplexity of 97.82988296845474
Finished 22 epochs...
Completing Train Step...
At time: 632.1496064662933 and batch: 50, loss is 4.123926200866699 and perplexity is 61.80141130582326
At time: 634.6365220546722 and batch: 100, loss is 4.097557058334351 and perplexity is 60.19305970247339
At time: 637.1003279685974 and batch: 150, loss is 4.08004135131836 and perplexity is 59.14791564360361
At time: 639.5633597373962 and batch: 200, loss is 4.058515100479126 and perplexity is 57.88828893514127
At time: 642.0226290225983 and batch: 250, loss is 4.0599927949905394 and perplexity is 57.97389337500428
At time: 644.4818272590637 and batch: 300, loss is 4.03317636013031 and perplexity is 56.4399002788312
At time: 646.9480936527252 and batch: 350, loss is 3.9738373613357543 and perplexity is 53.188242236856404
At time: 649.4082381725311 and batch: 400, loss is 3.952574677467346 and perplexity is 52.06925594024868
At time: 651.8650298118591 and batch: 450, loss is 3.8821909713745115 and perplexity is 48.53042745205446
At time: 654.3219478130341 and batch: 500, loss is 3.872025957107544 and perplexity is 48.0396137549451
At time: 656.7794060707092 and batch: 550, loss is 3.892330889701843 and perplexity is 49.02502537629928
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.583219401041666 and perplexity of 97.8288380381515
Finished 23 epochs...
Completing Train Step...
At time: 660.7516148090363 and batch: 50, loss is 4.122967681884766 and perplexity is 61.7422018611979
At time: 663.2082140445709 and batch: 100, loss is 4.096525726318359 and perplexity is 60.13101267390493
At time: 665.6750710010529 and batch: 150, loss is 4.07904004573822 and perplexity is 59.08872014695246
At time: 668.1334075927734 and batch: 200, loss is 4.057598309516907 and perplexity is 57.8352417953119
At time: 670.6330530643463 and batch: 250, loss is 4.0592660713195805 and perplexity is 57.93177767948098
At time: 673.0900001525879 and batch: 300, loss is 4.032548670768738 and perplexity is 56.40448467002123
At time: 675.5456240177155 and batch: 350, loss is 3.9733896160125735 and perplexity is 53.1644327808307
At time: 678.0057730674744 and batch: 400, loss is 3.9524065542221067 and perplexity is 52.06050262380141
At time: 680.4702682495117 and batch: 450, loss is 3.882101216316223 and perplexity is 48.52607179618374
At time: 682.9261026382446 and batch: 500, loss is 3.872097415924072 and perplexity is 48.04304673154726
At time: 685.3803210258484 and batch: 550, loss is 3.8922835874557493 and perplexity is 49.022706437330115
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.583222452799479 and perplexity of 97.82913658852783
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 689.3240211009979 and batch: 50, loss is 4.122683811187744 and perplexity is 61.72467754675726
At time: 691.8058683872223 and batch: 100, loss is 4.096674504280091 and perplexity is 60.139959508936855
At time: 694.2624790668488 and batch: 150, loss is 4.079269580841064 and perplexity is 59.1022846391123
At time: 696.7276248931885 and batch: 200, loss is 4.057471146583557 and perplexity is 57.82788776390425
At time: 699.1873016357422 and batch: 250, loss is 4.059292321205139 and perplexity is 57.933298401974575
At time: 701.6414844989777 and batch: 300, loss is 4.031723394393921 and perplexity is 56.357954584120534
At time: 704.0961837768555 and batch: 350, loss is 3.97138165473938 and perplexity is 53.05778776405924
At time: 706.5552067756653 and batch: 400, loss is 3.949425387382507 and perplexity is 51.9055326901129
At time: 709.0113348960876 and batch: 450, loss is 3.878561592102051 and perplexity is 48.35461136919296
At time: 711.4713227748871 and batch: 500, loss is 3.8684404420852663 and perplexity is 47.86767542609487
At time: 713.9311521053314 and batch: 550, loss is 3.889516386985779 and perplexity is 48.88723830122691
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.583594767252604 and perplexity of 97.86556657130026
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 717.9027984142303 and batch: 50, loss is 4.1224402236938475 and perplexity is 61.70964401830618
At time: 720.3674623966217 and batch: 100, loss is 4.096451997756958 and perplexity is 60.12657946427399
At time: 722.8240058422089 and batch: 150, loss is 4.079062266349792 and perplexity is 59.09003314903894
At time: 725.2834815979004 and batch: 200, loss is 4.057238459587097 and perplexity is 57.81443353176197
At time: 727.7375185489655 and batch: 250, loss is 4.059100699424744 and perplexity is 57.92219818374638
At time: 730.1914818286896 and batch: 300, loss is 4.031450862884522 and perplexity is 56.342597358450085
At time: 732.7008078098297 and batch: 350, loss is 3.970866808891296 and perplexity is 53.03047821302812
At time: 735.1710410118103 and batch: 400, loss is 3.948694338798523 and perplexity is 51.86760109054926
At time: 737.6343531608582 and batch: 450, loss is 3.877720341682434 and perplexity is 48.31395013762527
At time: 740.0913662910461 and batch: 500, loss is 3.8675471639633177 and perplexity is 47.824935371108246
At time: 742.5497131347656 and batch: 550, loss is 3.888834638595581 and perplexity is 48.85392086355901
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.583636983235677 and perplexity of 97.86969814961076
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 746.4767138957977 and batch: 50, loss is 4.122378034591675 and perplexity is 61.705806470277345
At time: 748.9563796520233 and batch: 100, loss is 4.09638557434082 and perplexity is 60.122585784103684
At time: 751.4112689495087 and batch: 150, loss is 4.078994827270508 and perplexity is 59.08604830597706
At time: 753.8652000427246 and batch: 200, loss is 4.0571702003479 and perplexity is 57.8104872971995
At time: 756.3187882900238 and batch: 250, loss is 4.059028511047363 and perplexity is 57.91801702516252
At time: 758.7706906795502 and batch: 300, loss is 4.031375799179077 and perplexity is 56.3383682330469
At time: 761.2233555316925 and batch: 350, loss is 3.9707406997680663 and perplexity is 53.02379100758388
At time: 763.6768500804901 and batch: 400, loss is 3.948515295982361 and perplexity is 51.85831540047528
At time: 766.1331965923309 and batch: 450, loss is 3.8775134086608887 and perplexity is 48.30395342030164
At time: 768.5875270366669 and batch: 500, loss is 3.8673264598846435 and perplexity is 47.814381377507075
At time: 771.0402657985687 and batch: 550, loss is 3.8886668252944947 and perplexity is 48.84572321368447
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.583646138509114 and perplexity of 97.8705941775602
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 775.0035588741302 and batch: 50, loss is 4.122362861633301 and perplexity is 61.70487021774721
At time: 777.4620418548584 and batch: 100, loss is 4.096369395256042 and perplexity is 60.12161306356011
At time: 779.9221441745758 and batch: 150, loss is 4.078977460861206 and perplexity is 59.085022202388046
At time: 782.3846626281738 and batch: 200, loss is 4.057153000831604 and perplexity is 57.80949299333192
At time: 784.8457720279694 and batch: 250, loss is 4.0590101146698 and perplexity is 57.91695155325403
At time: 787.3107352256775 and batch: 300, loss is 4.031356840133667 and perplexity is 56.33730012149046
At time: 789.7738330364227 and batch: 350, loss is 3.970709810256958 and perplexity is 53.02215315389891
At time: 792.2308804988861 and batch: 400, loss is 3.9484714460372925 and perplexity is 51.85604146604994
At time: 794.7342555522919 and batch: 450, loss is 3.8774623441696168 and perplexity is 48.301486866471
At time: 797.2087030410767 and batch: 500, loss is 3.867271966934204 and perplexity is 47.811775901783044
At time: 799.6794626712799 and batch: 550, loss is 3.8886255502700804 and perplexity is 48.843707146873186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.583649698893229 and perplexity of 97.87094263508934
Annealing...
Model not improving. Stopping early with 97.8288380381515loss at 27 epochs.
Finished Training.
Improved accuracyfrom -98.38246725338247 to -97.8288380381515
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f690564a9e8>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'anneal': 3.0553777196419674, 'wordvec_source': '', 'data': 'wikitext', 'lr': 21.4721337367448, 'num_layers': 1, 'batch_size': 80, 'dropout': 0.8539785317829272, 'wordvec_dim': 200, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 3.15434193611145 and batch: 50, loss is 7.51040114402771 and perplexity is 1826.9462644262558
At time: 5.6878674030303955 and batch: 100, loss is 6.892650899887085 and perplexity is 985.0091198764899
At time: 8.180578470230103 and batch: 150, loss is 6.82453426361084 and perplexity is 920.1477579465037
At time: 10.67773699760437 and batch: 200, loss is 6.776410503387451 and perplexity is 876.9153832427482
At time: 13.152682542800903 and batch: 250, loss is 6.670809640884399 and perplexity is 789.0341802507356
At time: 15.617632389068604 and batch: 300, loss is 6.6516753196716305 and perplexity is 774.0800714088585
At time: 18.081995725631714 and batch: 350, loss is 6.5944200611114505 and perplexity is 731.00482561131
At time: 20.548646926879883 and batch: 400, loss is 6.604226522445678 and perplexity is 738.2086605023095
At time: 23.017588138580322 and batch: 450, loss is 6.581162605285645 and perplexity is 721.3775192294991
At time: 25.500831127166748 and batch: 500, loss is 6.540466041564941 and perplexity is 692.6092875488897
At time: 27.995969533920288 and batch: 550, loss is 6.502894430160523 and perplexity is 667.0696279253136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.886980692545573 and perplexity of 360.31573637102554
Finished 1 epochs...
Completing Train Step...
At time: 32.23591756820679 and batch: 50, loss is 6.162359428405762 and perplexity is 474.54641328363016
At time: 34.72107934951782 and batch: 100, loss is 6.045522699356079 and perplexity is 422.2183930968877
At time: 37.236632108688354 and batch: 150, loss is 6.010665044784546 and perplexity is 407.75440506261026
At time: 39.721895933151245 and batch: 200, loss is 5.973886365890503 and perplexity is 393.03016548799445
At time: 42.20804762840271 and batch: 250, loss is 5.879857397079467 and perplexity is 357.75822070395
At time: 44.69281458854675 and batch: 300, loss is 5.851438217163086 and perplexity is 347.7341382182873
At time: 47.180612087249756 and batch: 350, loss is 5.79880732536316 and perplexity is 329.90585482968606
At time: 49.65389609336853 and batch: 400, loss is 5.797830934524536 and perplexity is 329.58389498032386
At time: 52.11385488510132 and batch: 450, loss is 5.7598056221008305 and perplexity is 317.2866494111312
At time: 54.57199430465698 and batch: 500, loss is 5.76700942993164 and perplexity is 319.5805740366303
At time: 57.0299973487854 and batch: 550, loss is 5.762447671890259 and perplexity is 318.1260449104692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.531649780273438 and perplexity of 252.5602364022567
Finished 2 epochs...
Completing Train Step...
At time: 61.07551717758179 and batch: 50, loss is 5.773814630508423 and perplexity is 321.76280076698754
At time: 63.56249523162842 and batch: 100, loss is 5.715643100738525 and perplexity is 303.5793726646643
At time: 66.04759931564331 and batch: 150, loss is 5.7046573448181155 and perplexity is 300.26257589173287
At time: 68.53297853469849 and batch: 200, loss is 5.749390907287598 and perplexity is 313.99934731379255
At time: 71.02139902114868 and batch: 250, loss is 5.699104433059692 and perplexity is 298.5998650192748
At time: 73.51627945899963 and batch: 300, loss is 5.672043933868408 and perplexity is 290.6279520462152
At time: 76.06267619132996 and batch: 350, loss is 5.62111967086792 and perplexity is 276.1984615406561
At time: 78.54749608039856 and batch: 400, loss is 5.661894998550415 and perplexity is 287.6933047214676
At time: 81.03259301185608 and batch: 450, loss is 5.620059909820557 and perplexity is 275.90591221334466
At time: 83.49209785461426 and batch: 500, loss is 5.622293519973755 and perplexity is 276.5228672221778
At time: 85.97818756103516 and batch: 550, loss is 5.607639303207398 and perplexity is 272.50018778093164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.431971740722656 and perplexity of 228.59954036860296
Finished 3 epochs...
Completing Train Step...
At time: 90.1368978023529 and batch: 50, loss is 5.665936632156372 and perplexity is 288.85840852540497
At time: 92.66371726989746 and batch: 100, loss is 5.673531332015991 and perplexity is 291.06055316902354
At time: 95.1540458202362 and batch: 150, loss is 5.658378114700318 and perplexity is 286.68329786373465
At time: 97.6298201084137 and batch: 200, loss is 5.665784378051757 and perplexity is 288.81443199494316
At time: 100.08915162086487 and batch: 250, loss is 5.6471364784240725 and perplexity is 283.4785555262424
At time: 102.54886412620544 and batch: 300, loss is 5.632943325042724 and perplexity is 279.4835190550212
At time: 105.00782060623169 and batch: 350, loss is 5.573534517288208 and perplexity is 263.3633183041021
At time: 107.46724486351013 and batch: 400, loss is 5.608633794784546 and perplexity is 272.77132172031435
At time: 109.93316864967346 and batch: 450, loss is 5.597723035812378 and perplexity is 269.81135667105843
At time: 112.40613031387329 and batch: 500, loss is 5.594181613922119 and perplexity is 268.85753077314075
At time: 114.88976645469666 and batch: 550, loss is 5.585807237625122 and perplexity is 266.61541789194814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.4830322265625 and perplexity of 240.57508066963106
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 119.12802529335022 and batch: 50, loss is 5.5786568069458005 and perplexity is 264.71580245624415
At time: 121.6143410205841 and batch: 100, loss is 5.525240669250488 and perplexity is 250.9467259142367
At time: 124.09890127182007 and batch: 150, loss is 5.509518241882324 and perplexity is 247.0320886820227
At time: 126.58310770988464 and batch: 200, loss is 5.506885442733765 and perplexity is 246.38255822608167
At time: 129.09555840492249 and batch: 250, loss is 5.4866422748565675 and perplexity is 241.44513785851842
At time: 131.5782127380371 and batch: 300, loss is 5.510179653167724 and perplexity is 247.19553253918045
At time: 134.06116938591003 and batch: 350, loss is 5.454468812942505 and perplexity is 233.8006462017737
At time: 136.53464221954346 and batch: 400, loss is 5.434583005905151 and perplexity is 229.19725444444308
At time: 138.99442839622498 and batch: 450, loss is 5.410399360656738 and perplexity is 223.72091517402552
At time: 141.45467138290405 and batch: 500, loss is 5.379995374679566 and perplexity is 217.02127162970368
At time: 143.9101116657257 and batch: 550, loss is 5.38350061416626 and perplexity is 217.78331795765945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.328489176432291 and perplexity of 206.12631828748047
Finished 5 epochs...
Completing Train Step...
At time: 147.94761633872986 and batch: 50, loss is 5.457330884933472 and perplexity is 234.4707589808125
At time: 150.44400644302368 and batch: 100, loss is 5.428047857284546 and perplexity is 227.7042999763225
At time: 152.90273237228394 and batch: 150, loss is 5.42029299736023 and perplexity is 225.9453141851781
At time: 155.38291192054749 and batch: 200, loss is 5.431476202011108 and perplexity is 228.48628850956587
At time: 157.87143921852112 and batch: 250, loss is 5.418635120391846 and perplexity is 225.57103499282988
At time: 160.35945630073547 and batch: 300, loss is 5.43956039428711 and perplexity is 230.34090202388208
At time: 162.84418892860413 and batch: 350, loss is 5.387341432571411 and perplexity is 218.6213925494999
At time: 165.33169627189636 and batch: 400, loss is 5.37629602432251 and perplexity is 216.21991707053976
At time: 167.81592988967896 and batch: 450, loss is 5.352086324691772 and perplexity is 211.04815384044662
At time: 170.30560564994812 and batch: 500, loss is 5.3303149318695064 and perplexity is 206.50299829199528
At time: 172.81093788146973 and batch: 550, loss is 5.329080886840821 and perplexity is 206.24832146719902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.314434305826823 and perplexity of 203.24950354193334
Finished 6 epochs...
Completing Train Step...
At time: 176.98890948295593 and batch: 50, loss is 5.397495431900024 and perplexity is 220.85258260352668
At time: 179.47605204582214 and batch: 100, loss is 5.376064548492431 and perplexity is 216.16987317795554
At time: 181.9653034210205 and batch: 150, loss is 5.367488050460816 and perplexity is 214.32382033615588
At time: 184.4542956352234 and batch: 200, loss is 5.375107727050781 and perplexity is 215.963136129253
At time: 186.94299268722534 and batch: 250, loss is 5.368381481170655 and perplexity is 214.5153893831862
At time: 189.43051600456238 and batch: 300, loss is 5.390688972473145 and perplexity is 219.35446269063942
At time: 191.9482123851776 and batch: 350, loss is 5.338080272674561 and perplexity is 208.11280671717572
At time: 194.4349443912506 and batch: 400, loss is 5.33354380607605 and perplexity is 207.17084811806342
At time: 196.91818165779114 and batch: 450, loss is 5.30650990486145 and perplexity is 201.64523779749237
At time: 199.40552353858948 and batch: 500, loss is 5.287910070419311 and perplexity is 197.929334478648
At time: 201.89604473114014 and batch: 550, loss is 5.278028450012207 and perplexity is 195.98310372152915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.30467529296875 and perplexity of 201.27563618754454
Finished 7 epochs...
Completing Train Step...
At time: 206.0024893283844 and batch: 50, loss is 5.3419280624389645 and perplexity is 208.91512362821166
At time: 208.5209379196167 and batch: 100, loss is 5.320798320770264 and perplexity is 204.54711103696914
At time: 211.00974225997925 and batch: 150, loss is 5.323198099136352 and perplexity is 205.03856822719217
At time: 213.49792885780334 and batch: 200, loss is 5.329369535446167 and perplexity is 206.30786335047296
At time: 215.98617482185364 and batch: 250, loss is 5.33239146232605 and perplexity is 206.93225358391481
At time: 218.47439813613892 and batch: 300, loss is 5.343081903457642 and perplexity is 209.15631759025993
At time: 220.96407389640808 and batch: 350, loss is 5.289919357299805 and perplexity is 198.3274311049109
At time: 223.4565567970276 and batch: 400, loss is 5.292472982406617 and perplexity is 198.8345322098392
At time: 225.9475474357605 and batch: 450, loss is 5.264741096496582 and perplexity is 193.396231343246
At time: 228.4371521472931 and batch: 500, loss is 5.244208297729492 and perplexity is 189.46575536728315
At time: 230.92436361312866 and batch: 550, loss is 5.232748289108276 and perplexity is 187.30687022598505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.303376261393229 and perplexity of 201.01434253185244
Finished 8 epochs...
Completing Train Step...
At time: 235.08145809173584 and batch: 50, loss is 5.295510129928589 and perplexity is 199.43933999694994
At time: 237.56629300117493 and batch: 100, loss is 5.276408853530884 and perplexity is 195.6659470785621
At time: 240.05418515205383 and batch: 150, loss is 5.283150386810303 and perplexity is 196.98949192050904
At time: 242.54079151153564 and batch: 200, loss is 5.2932307910919185 and perplexity is 198.98526785246173
At time: 245.02670526504517 and batch: 250, loss is 5.286079387664795 and perplexity is 197.5673201272247
At time: 247.52106356620789 and batch: 300, loss is 5.298451642990113 and perplexity is 200.0268570915122
At time: 250.0054075717926 and batch: 350, loss is 5.249821538925171 and perplexity is 190.5322628319653
At time: 252.48874187469482 and batch: 400, loss is 5.254062385559082 and perplexity is 191.34199670245374
At time: 254.9969310760498 and batch: 450, loss is 5.234041481018067 and perplexity is 187.54925064362848
At time: 257.4795389175415 and batch: 500, loss is 5.214132785797119 and perplexity is 183.852312489006
At time: 259.96345710754395 and batch: 550, loss is 5.202152795791626 and perplexity is 181.66290435198863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.303511047363282 and perplexity of 201.0414382710268
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 264.05089950561523 and batch: 50, loss is 5.243007555007934 and perplexity is 189.23839227015077
At time: 266.56294107437134 and batch: 100, loss is 5.193757095336914 and perplexity is 180.14410165098954
At time: 269.0474343299866 and batch: 150, loss is 5.183507833480835 and perplexity is 178.30718717032477
At time: 271.5288190841675 and batch: 200, loss is 5.184032640457153 and perplexity is 178.40078858526888
At time: 274.0135426521301 and batch: 250, loss is 5.163132238388061 and perplexity is 174.71083544804495
At time: 276.4974567890167 and batch: 300, loss is 5.157744226455688 and perplexity is 173.77202281995292
At time: 278.97988867759705 and batch: 350, loss is 5.105736532211304 and perplexity is 164.9655282226904
At time: 281.4630038738251 and batch: 400, loss is 5.093106727600098 and perplexity is 162.8951476057848
At time: 283.9455852508545 and batch: 450, loss is 5.056799449920654 and perplexity is 157.0869462643011
At time: 286.44377183914185 and batch: 500, loss is 5.04770336151123 and perplexity is 155.6645484485374
At time: 288.94422721862793 and batch: 550, loss is 5.072063102722168 and perplexity is 159.50305931865384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.194740804036458 and perplexity of 180.32139816071745
Finished 10 epochs...
Completing Train Step...
At time: 293.0535771846771 and batch: 50, loss is 5.160381841659546 and perplexity is 174.2309715483438
At time: 295.53613805770874 and batch: 100, loss is 5.126845207214355 and perplexity is 168.48474426625387
At time: 298.0256860256195 and batch: 150, loss is 5.12986162185669 and perplexity is 168.99373138804287
At time: 300.5098352432251 and batch: 200, loss is 5.137338542938233 and perplexity is 170.2620197182195
At time: 302.99603939056396 and batch: 250, loss is 5.124444580078125 and perplexity is 168.08076031855475
At time: 305.48095440864563 and batch: 300, loss is 5.125753364562988 and perplexity is 168.3008858268674
At time: 307.965119600296 and batch: 350, loss is 5.082329750061035 and perplexity is 161.14905595376885
At time: 310.44872760772705 and batch: 400, loss is 5.080210838317871 and perplexity is 160.80795683379594
At time: 312.93670296669006 and batch: 450, loss is 5.052843360900879 and perplexity is 156.46672395727037
At time: 315.42057824134827 and batch: 500, loss is 5.043640346527099 and perplexity is 155.03336417956803
At time: 317.94473481178284 and batch: 550, loss is 5.055804576873779 and perplexity is 156.930742409833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.189338175455729 and perplexity of 179.3498155345733
Finished 11 epochs...
Completing Train Step...
At time: 322.0359365940094 and batch: 50, loss is 5.132676591873169 and perplexity is 169.47011386148935
At time: 324.54648303985596 and batch: 100, loss is 5.103403654098511 and perplexity is 164.581132301284
At time: 327.0300660133362 and batch: 150, loss is 5.108854074478149 and perplexity is 165.48061771895252
At time: 329.512104511261 and batch: 200, loss is 5.117387351989746 and perplexity is 166.8987518133019
At time: 331.994380235672 and batch: 250, loss is 5.1064683628082275 and perplexity is 165.08629923025057
At time: 334.4775552749634 and batch: 300, loss is 5.110051670074463 and perplexity is 165.6789152944592
At time: 336.9619331359863 and batch: 350, loss is 5.069741811752319 and perplexity is 159.13323570756154
At time: 339.44722843170166 and batch: 400, loss is 5.071302518844605 and perplexity is 159.38178998690947
At time: 341.9336175918579 and batch: 450, loss is 5.0438611125946045 and perplexity is 155.0675940639695
At time: 344.42008543014526 and batch: 500, loss is 5.033977499008179 and perplexity is 153.54251494567467
At time: 346.90478897094727 and batch: 550, loss is 5.0397733783721925 and perplexity is 154.43501274592487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.187991841634115 and perplexity of 179.10851328515758
Finished 12 epochs...
Completing Train Step...
At time: 351.03787064552307 and batch: 50, loss is 5.113119125366211 and perplexity is 166.18790821732284
At time: 353.5214650630951 and batch: 100, loss is 5.085117521286011 and perplexity is 161.59892943570713
At time: 356.00638008117676 and batch: 150, loss is 5.092276353836059 and perplexity is 162.75993989316137
At time: 358.49149775505066 and batch: 200, loss is 5.102393312454224 and perplexity is 164.41493310257982
At time: 360.9786093235016 and batch: 250, loss is 5.091666603088379 and perplexity is 162.66072714871567
At time: 363.46403312683105 and batch: 300, loss is 5.0955729484558105 and perplexity is 163.29737880772169
At time: 365.949294090271 and batch: 350, loss is 5.05818470954895 and perplexity is 157.3047032592583
At time: 368.44115591049194 and batch: 400, loss is 5.061000909805298 and perplexity is 157.74832918142044
At time: 370.9296066761017 and batch: 450, loss is 5.034052515029908 and perplexity is 153.55403352634573
At time: 373.4153628349304 and batch: 500, loss is 5.021006479263305 and perplexity is 151.5637728471507
At time: 375.8944184780121 and batch: 550, loss is 5.023778457641601 and perplexity is 151.98448718454665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.185372416178385 and perplexity of 178.63996581661095
Finished 13 epochs...
Completing Train Step...
At time: 379.9785439968109 and batch: 50, loss is 5.095581912994385 and perplexity is 163.29884269993468
At time: 382.4961185455322 and batch: 100, loss is 5.069129838943481 and perplexity is 159.03588028679962
At time: 384.9819767475128 and batch: 150, loss is 5.077020225524902 and perplexity is 160.29569855298894
At time: 387.4663758277893 and batch: 200, loss is 5.088009405136108 and perplexity is 162.06693114707286
At time: 389.953577041626 and batch: 250, loss is 5.0766283130645755 and perplexity is 160.23288898012035
At time: 392.44082593917847 and batch: 300, loss is 5.082550630569458 and perplexity is 161.1846545705554
At time: 394.9287052154541 and batch: 350, loss is 5.046900033950806 and perplexity is 155.5395490410444
At time: 397.4176721572876 and batch: 400, loss is 5.051985015869141 and perplexity is 156.33247914455544
At time: 399.9059283733368 and batch: 450, loss is 5.026550979614258 and perplexity is 152.4064521981052
At time: 402.3944218158722 and batch: 500, loss is 5.012452249526977 and perplexity is 150.2727910641633
At time: 404.88181924819946 and batch: 550, loss is 5.012924118041992 and perplexity is 150.34371679542357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.187599690755208 and perplexity of 179.03828949431377
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 409.03034806251526 and batch: 50, loss is 5.079815683364868 and perplexity is 160.7444253263923
At time: 411.518976688385 and batch: 100, loss is 5.044545755386353 and perplexity is 155.17379632563723
At time: 414.01353907585144 and batch: 150, loss is 5.044652767181397 and perplexity is 155.19040264064574
At time: 416.5013666152954 and batch: 200, loss is 5.042022409439087 and perplexity is 154.78273275739286
At time: 418.98886466026306 and batch: 250, loss is 5.024638442993164 and perplexity is 152.11524783525334
At time: 421.47434401512146 and batch: 300, loss is 5.023686428070068 and perplexity is 151.97050076090352
At time: 423.96360182762146 and batch: 350, loss is 4.972564792633056 and perplexity is 144.3967605919096
At time: 426.4442241191864 and batch: 400, loss is 4.972730989456177 and perplexity is 144.42076086911857
At time: 428.9258646965027 and batch: 450, loss is 4.944957761764527 and perplexity is 140.4649177358738
At time: 431.40813398361206 and batch: 500, loss is 4.939108037948609 and perplexity is 139.64563538530126
At time: 433.8920564651489 and batch: 550, loss is 4.961020059585572 and perplexity is 142.73932427652733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.145436604817708 and perplexity of 171.6464099630324
Finished 15 epochs...
Completing Train Step...
At time: 437.99172711372375 and batch: 50, loss is 5.048273077011109 and perplexity is 155.75325822183711
At time: 440.50637555122375 and batch: 100, loss is 5.018250303268433 and perplexity is 151.1466115637753
At time: 442.99270510673523 and batch: 150, loss is 5.023298358917236 and perplexity is 151.91153713913914
At time: 445.50769686698914 and batch: 200, loss is 5.023481311798096 and perplexity is 151.9393323350227
At time: 447.992360830307 and batch: 250, loss is 5.012614326477051 and perplexity is 150.29714879367793
At time: 450.4772846698761 and batch: 300, loss is 5.013339939117432 and perplexity is 150.40624588098188
At time: 452.9631140232086 and batch: 350, loss is 4.967156343460083 and perplexity is 143.61790614738703
At time: 455.44282937049866 and batch: 400, loss is 4.973837366104126 and perplexity is 144.58063304953887
At time: 457.92490792274475 and batch: 450, loss is 4.947794742584229 and perplexity is 140.8639798114057
At time: 460.4134488105774 and batch: 500, loss is 4.940894994735718 and perplexity is 139.89539919334675
At time: 462.89833521842957 and batch: 550, loss is 4.955891675949097 and perplexity is 142.0091761018618
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.143521118164062 and perplexity of 171.31793824762815
Finished 16 epochs...
Completing Train Step...
At time: 467.02428340911865 and batch: 50, loss is 5.0366784286499025 and perplexity is 153.9577830280436
At time: 469.5112552642822 and batch: 100, loss is 5.007929029464722 and perplexity is 149.59460910021608
At time: 472.0023422241211 and batch: 150, loss is 5.014716529846192 and perplexity is 150.61343630009537
At time: 474.48758244514465 and batch: 200, loss is 5.015795450210572 and perplexity is 150.77602389743174
At time: 476.9712839126587 and batch: 250, loss is 5.007168779373169 and perplexity is 149.48092300535944
At time: 479.4544343948364 and batch: 300, loss is 5.008870792388916 and perplexity is 149.73555811658736
At time: 481.9392423629761 and batch: 350, loss is 4.964476232528686 and perplexity is 143.23350957000486
At time: 484.42637753486633 and batch: 400, loss is 4.97298342704773 and perplexity is 144.45722270013738
At time: 486.91095900535583 and batch: 450, loss is 4.9465182018280025 and perplexity is 140.6842759241634
At time: 489.39538502693176 and batch: 500, loss is 4.93909215927124 and perplexity is 139.64341801491557
At time: 491.880735874176 and batch: 550, loss is 4.9507262325286865 and perplexity is 141.2775270103307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1427861531575525 and perplexity of 171.19207181740538
Finished 17 epochs...
Completing Train Step...
At time: 495.9980905056 and batch: 50, loss is 5.029107484817505 and perplexity is 152.7965785537075
At time: 498.51261138916016 and batch: 100, loss is 5.00093864440918 and perplexity is 148.55253168503174
At time: 500.9966061115265 and batch: 150, loss is 5.00864351272583 and perplexity is 149.701530136467
At time: 503.4814577102661 and batch: 200, loss is 5.010340347290039 and perplexity is 149.95576450294743
At time: 505.964830160141 and batch: 250, loss is 5.003072204589844 and perplexity is 148.8698158033937
At time: 508.47987127304077 and batch: 300, loss is 5.005025930404663 and perplexity is 149.16095091205858
At time: 510.96675992012024 and batch: 350, loss is 4.961629695892334 and perplexity is 142.82636988139367
At time: 513.4527547359467 and batch: 400, loss is 4.9712607955932615 and perplexity is 144.20859035692035
At time: 515.9357733726501 and batch: 450, loss is 4.944575719833374 and perplexity is 140.41126449698885
At time: 518.4152870178223 and batch: 500, loss is 4.936411657333374 and perplexity is 139.26960478967712
At time: 520.898998260498 and batch: 550, loss is 4.94635274887085 and perplexity is 140.66100122017426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.142874654134115 and perplexity of 171.2072231533851
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 524.9872436523438 and batch: 50, loss is 5.023021602630616 and perplexity is 151.8695004834484
At time: 527.5012340545654 and batch: 100, loss is 4.993417730331421 and perplexity is 147.4394717189816
At time: 529.9846575260162 and batch: 150, loss is 4.995942258834839 and perplexity is 147.81215709736077
At time: 532.4699423313141 and batch: 200, loss is 4.993737516403198 and perplexity is 147.48662834808218
At time: 534.9538474082947 and batch: 250, loss is 4.982267227172851 and perplexity is 145.80457929974324
At time: 537.438588142395 and batch: 300, loss is 4.98264856338501 and perplexity is 145.86019046833107
At time: 539.9226307868958 and batch: 350, loss is 4.934006004333496 and perplexity is 138.93497311235302
At time: 542.4032785892487 and batch: 400, loss is 4.937268314361572 and perplexity is 139.38896219231412
At time: 544.8859369754791 and batch: 450, loss is 4.911048803329468 and perplexity is 135.78174810488412
At time: 547.3689556121826 and batch: 500, loss is 4.905938596725464 and perplexity is 135.08964521952575
At time: 549.8488788604736 and batch: 550, loss is 4.928477087020874 and perplexity is 138.16893277245038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.128565979003906 and perplexity of 168.7749176506423
Finished 19 epochs...
Completing Train Step...
At time: 553.9921464920044 and batch: 50, loss is 5.012626390457154 and perplexity is 150.29896198642763
At time: 556.4768526554108 and batch: 100, loss is 4.983984308242798 and perplexity is 146.05515264858212
At time: 558.9615042209625 and batch: 150, loss is 4.988107442855835 and perplexity is 146.65860089816562
At time: 561.4459614753723 and batch: 200, loss is 4.987993803024292 and perplexity is 146.64193558640474
At time: 563.931410074234 and batch: 250, loss is 4.978891630172729 and perplexity is 145.31323156179147
At time: 566.4186313152313 and batch: 300, loss is 4.981454591751099 and perplexity is 145.68614146385872
At time: 568.9333169460297 and batch: 350, loss is 4.934170246124268 and perplexity is 138.9577939151512
At time: 571.4187684059143 and batch: 400, loss is 4.937939586639405 and perplexity is 139.48256155028213
At time: 573.9032187461853 and batch: 450, loss is 4.913336210250854 and perplexity is 136.0926917069936
At time: 576.3867690563202 and batch: 500, loss is 4.907435522079468 and perplexity is 135.29201576352838
At time: 578.8693158626556 and batch: 550, loss is 4.926509637832641 and perplexity is 137.89735965858114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.12757314046224 and perplexity of 168.6074345631221
Finished 20 epochs...
Completing Train Step...
At time: 582.959100484848 and batch: 50, loss is 5.007351217269897 and perplexity is 149.50819647833544
At time: 585.4732520580292 and batch: 100, loss is 4.978997535705567 and perplexity is 145.32862185195242
At time: 587.9585676193237 and batch: 150, loss is 4.983672714233398 and perplexity is 146.0096498275462
At time: 590.4449565410614 and batch: 200, loss is 4.984611139297486 and perplexity is 146.14673325374443
At time: 592.9294302463531 and batch: 250, loss is 4.977452783584595 and perplexity is 145.10429846173295
At time: 595.4147346019745 and batch: 300, loss is 4.981131553649902 and perplexity is 145.63908688996887
At time: 597.899982213974 and batch: 350, loss is 4.934377679824829 and perplexity is 138.98662143436098
At time: 600.385014295578 and batch: 400, loss is 4.9381578826904295 and perplexity is 139.51301336629004
At time: 602.8704149723053 and batch: 450, loss is 4.914244928359985 and perplexity is 136.21641780802835
At time: 605.3542850017548 and batch: 500, loss is 4.907657585144043 and perplexity is 135.3220624591685
At time: 607.8401975631714 and batch: 550, loss is 4.924603509902954 and perplexity is 137.63476000368317
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.12723388671875 and perplexity of 168.5502435614456
Finished 21 epochs...
Completing Train Step...
At time: 611.9457263946533 and batch: 50, loss is 5.0033932876586915 and perplexity is 148.91762305534945
At time: 614.430278301239 and batch: 100, loss is 4.975098896026611 and perplexity is 144.7631409397205
At time: 616.9128897190094 and batch: 150, loss is 4.980435848236084 and perplexity is 145.53780022568156
At time: 619.3964855670929 and batch: 200, loss is 4.982206840515136 and perplexity is 145.79577491435614
At time: 621.881153345108 and batch: 250, loss is 4.976268367767334 and perplexity is 144.9325363744284
At time: 624.3654503822327 and batch: 300, loss is 4.9808852577209475 and perplexity is 145.6032209927647
At time: 626.852301120758 and batch: 350, loss is 4.934412078857422 and perplexity is 138.9914025219136
At time: 629.3370635509491 and batch: 400, loss is 4.937806510925293 and perplexity is 139.46400104380146
At time: 631.8527405261993 and batch: 450, loss is 4.914484729766846 and perplexity is 136.24908661351242
At time: 634.3416209220886 and batch: 500, loss is 4.907409753799438 and perplexity is 135.28852956589725
At time: 636.8238763809204 and batch: 550, loss is 4.922807388305664 and perplexity is 137.3877731143452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.127060953776041 and perplexity of 168.52109819200209
Finished 22 epochs...
Completing Train Step...
At time: 640.8709774017334 and batch: 50, loss is 5.000073499679566 and perplexity is 148.4240678231028
At time: 643.3827891349792 and batch: 100, loss is 4.971977195739746 and perplexity is 144.31193842704258
At time: 645.8648636341095 and batch: 150, loss is 4.977753772735595 and perplexity is 145.14797985481638
At time: 648.347799539566 and batch: 200, loss is 4.980083408355713 and perplexity is 145.48651593862547
At time: 650.8312518596649 and batch: 250, loss is 4.975282983779907 and perplexity is 144.78979251413514
At time: 653.3163256645203 and batch: 300, loss is 4.980497970581054 and perplexity is 145.54684165594793
At time: 655.8015556335449 and batch: 350, loss is 4.934110221862793 and perplexity is 138.9494533265166
At time: 658.2864353656769 and batch: 400, loss is 4.937263307571411 and perplexity is 139.38826430277675
At time: 660.772207736969 and batch: 450, loss is 4.914269456863403 and perplexity is 136.21975903387556
At time: 663.258451461792 and batch: 500, loss is 4.906862125396729 and perplexity is 135.21446200714664
At time: 665.7434582710266 and batch: 550, loss is 4.921079034805298 and perplexity is 137.15052356038353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.126889038085937 and perplexity of 168.49212926128752
Finished 23 epochs...
Completing Train Step...
At time: 669.891636133194 and batch: 50, loss is 4.997112874984741 and perplexity is 147.9852897117571
At time: 672.3759260177612 and batch: 100, loss is 4.969237279891968 and perplexity is 143.9170770504177
At time: 674.8614184856415 and batch: 150, loss is 4.975425338745117 and perplexity is 144.81040552715876
At time: 677.3455832004547 and batch: 200, loss is 4.978252420425415 and perplexity is 145.22037560814042
At time: 679.8305716514587 and batch: 250, loss is 4.974303245544434 and perplexity is 144.64800588653452
At time: 682.3149321079254 and batch: 300, loss is 4.97998929977417 and perplexity is 145.47282505320177
At time: 684.7984752655029 and batch: 350, loss is 4.933579483032227 and perplexity is 138.87572702258709
At time: 687.2838385105133 and batch: 400, loss is 4.9366082763671875 and perplexity is 139.29699053700182
At time: 689.7667520046234 and batch: 450, loss is 4.913925485610962 and perplexity is 136.17291141033323
At time: 692.25150847435 and batch: 500, loss is 4.906162919998169 and perplexity is 135.11995237002256
At time: 694.7673625946045 and batch: 550, loss is 4.919347620010376 and perplexity is 136.91326457089323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.12689208984375 and perplexity of 168.49264345924396
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 698.836225271225 and batch: 50, loss is 4.994798784255981 and perplexity is 147.64323425117004
At time: 701.3502447605133 and batch: 100, loss is 4.965523118972778 and perplexity is 143.3835373068132
At time: 703.8355596065521 and batch: 150, loss is 4.970754318237304 and perplexity is 144.13557046439206
At time: 706.3219437599182 and batch: 200, loss is 4.972367296218872 and perplexity is 144.3682455653733
At time: 708.8071093559265 and batch: 250, loss is 4.96646164894104 and perplexity is 143.51817022216142
At time: 711.2911174297333 and batch: 300, loss is 4.970795698165894 and perplexity is 144.14153490740833
At time: 713.7751502990723 and batch: 350, loss is 4.922629203796387 and perplexity is 137.36329492229422
At time: 716.2589585781097 and batch: 400, loss is 4.92336597442627 and perplexity is 137.46453745529956
At time: 718.7434134483337 and batch: 450, loss is 4.901129455566406 and perplexity is 134.44153971108224
At time: 721.2319419384003 and batch: 500, loss is 4.894893321990967 and perplexity is 133.60575305855815
At time: 723.7154536247253 and batch: 550, loss is 4.9125496196746825 and perplexity is 135.98568456913148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.124368286132812 and perplexity of 168.0679372633859
Finished 25 epochs...
Completing Train Step...
At time: 727.8355505466461 and batch: 50, loss is 4.991797733306885 and perplexity is 147.20081357843074
At time: 730.3119101524353 and batch: 100, loss is 4.963159017562866 and perplexity is 143.04496445207914
At time: 732.7709498405457 and batch: 150, loss is 4.969233541488648 and perplexity is 143.91653903134468
At time: 735.2317199707031 and batch: 200, loss is 4.9711899566650395 and perplexity is 144.19837513676097
At time: 737.7153556346893 and batch: 250, loss is 4.96550085067749 and perplexity is 143.38034443541497
At time: 740.1991519927979 and batch: 300, loss is 4.970187435150146 and perplexity is 144.05388560230878
At time: 742.6795980930328 and batch: 350, loss is 4.92241868019104 and perplexity is 137.33437974997483
At time: 745.1625678539276 and batch: 400, loss is 4.923679685592651 and perplexity is 137.50766838065346
At time: 747.6466906070709 and batch: 450, loss is 4.901627864837646 and perplexity is 134.50856332212118
At time: 750.1325709819794 and batch: 500, loss is 4.895181398391724 and perplexity is 133.64424726739006
At time: 752.6144530773163 and batch: 550, loss is 4.911740808486939 and perplexity is 135.87574229334376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.124114990234375 and perplexity of 168.0253717352827
Finished 26 epochs...
Completing Train Step...
At time: 756.7362039089203 and batch: 50, loss is 4.99006688117981 and perplexity is 146.94625110569262
At time: 759.2489175796509 and batch: 100, loss is 4.961827917098999 and perplexity is 142.85468390290217
At time: 761.732307434082 and batch: 150, loss is 4.968378915786743 and perplexity is 143.79359680043743
At time: 764.2166616916656 and batch: 200, loss is 4.970563201904297 and perplexity is 144.1080264348498
At time: 766.6993680000305 and batch: 250, loss is 4.964887628555298 and perplexity is 143.29244738928062
At time: 769.1819484233856 and batch: 300, loss is 4.969808893203735 and perplexity is 143.99936548379176
At time: 771.6629748344421 and batch: 350, loss is 4.922289867401123 and perplexity is 137.31669046469523
At time: 774.1442799568176 and batch: 400, loss is 4.923852472305298 and perplexity is 137.5314299314177
At time: 776.618420124054 and batch: 450, loss is 4.901889791488648 and perplexity is 134.5437993140644
At time: 779.1029672622681 and batch: 500, loss is 4.895235776901245 and perplexity is 133.6515148399606
At time: 781.584876537323 and batch: 550, loss is 4.910976181030273 and perplexity is 135.77188768019576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.124025472005209 and perplexity of 168.01033107476664
Finished 27 epochs...
Completing Train Step...
At time: 785.7086179256439 and batch: 50, loss is 4.9887282371521 and perplexity is 146.74967398700358
At time: 788.1907978057861 and batch: 100, loss is 4.960772085189819 and perplexity is 142.7039329670904
At time: 790.6752347946167 and batch: 150, loss is 4.96772931098938 and perplexity is 143.70021812302798
At time: 793.1595377922058 and batch: 200, loss is 4.9700963592529295 and perplexity is 144.0407663628623
At time: 795.6503031253815 and batch: 250, loss is 4.964422073364258 and perplexity is 143.2257523728719
At time: 798.1394186019897 and batch: 300, loss is 4.969478425979614 and perplexity is 143.95178627532255
At time: 800.6259715557098 and batch: 350, loss is 4.922160091400147 and perplexity is 137.2988712100203
At time: 803.110933303833 and batch: 400, loss is 4.923925008773804 and perplexity is 137.5414063374757
At time: 805.5961050987244 and batch: 450, loss is 4.90201847076416 and perplexity is 134.56111342664465
At time: 808.0786175727844 and batch: 500, loss is 4.895153503417969 and perplexity is 133.6405193166158
At time: 810.5620336532593 and batch: 550, loss is 4.910241041183472 and perplexity is 135.67211303413904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.123962910970052 and perplexity of 167.99982050331712
Finished 28 epochs...
Completing Train Step...
At time: 814.6720054149628 and batch: 50, loss is 4.987551021575928 and perplexity is 146.5770196306259
At time: 817.1837270259857 and batch: 100, loss is 4.959839162826538 and perplexity is 142.57086335815555
At time: 819.6647493839264 and batch: 150, loss is 4.967163372039795 and perplexity is 143.6189155808359
At time: 822.1677072048187 and batch: 200, loss is 4.969661684036255 and perplexity is 143.9781690172765
At time: 824.6502883434296 and batch: 250, loss is 4.963985595703125 and perplexity is 143.16325117264336
At time: 827.1322448253632 and batch: 300, loss is 4.969162502288818 and perplexity is 143.9063156787034
At time: 829.6149661540985 and batch: 350, loss is 4.921993160247803 and perplexity is 137.27595366411313
At time: 832.0987763404846 and batch: 400, loss is 4.923924827575684 and perplexity is 137.54138141523376
At time: 834.5831565856934 and batch: 450, loss is 4.902057685852051 and perplexity is 134.56639035600128
At time: 837.0670275688171 and batch: 500, loss is 4.895010023117066 and perplexity is 133.62134591022732
At time: 839.5505712032318 and batch: 550, loss is 4.909543504714966 and perplexity is 135.57750978598213
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.123936462402344 and perplexity of 167.9953772074492
Finished 29 epochs...
Completing Train Step...
At time: 843.6718242168427 and batch: 50, loss is 4.986481094360352 and perplexity is 146.420276754832
At time: 846.1565480232239 and batch: 100, loss is 4.958971872329712 and perplexity is 142.44726660813149
At time: 848.6405189037323 and batch: 150, loss is 4.966633358001709 and perplexity is 143.54281570822525
At time: 851.1271283626556 and batch: 200, loss is 4.969269037246704 and perplexity is 143.92164754865928
At time: 853.6103248596191 and batch: 250, loss is 4.963555192947387 and perplexity is 143.10164657316398
At time: 856.0888092517853 and batch: 300, loss is 4.968846225738526 and perplexity is 143.86080868239247
At time: 858.5714082717896 and batch: 350, loss is 4.921774835586548 and perplexity is 137.2459862094671
At time: 861.0549292564392 and batch: 400, loss is 4.9238402366638185 and perplexity is 137.52974715644382
At time: 863.5385091304779 and batch: 450, loss is 4.901978569030762 and perplexity is 134.55574431209013
At time: 866.0215978622437 and batch: 500, loss is 4.894784593582154 and perplexity is 133.5912271073259
At time: 868.503942489624 and batch: 550, loss is 4.908847541809082 and perplexity is 135.48318569514038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.12394053141276 and perplexity of 167.99606078377974
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 872.6156392097473 and batch: 50, loss is 4.985547971725464 and perplexity is 146.28371240593714
At time: 875.1282591819763 and batch: 100, loss is 4.958120946884155 and perplexity is 142.3261061608892
At time: 877.6068489551544 and batch: 150, loss is 4.965369510650635 and perplexity is 143.36151409374216
At time: 880.0856087207794 and batch: 200, loss is 4.9679327487945555 and perplexity is 143.72945515386397
At time: 882.5542035102844 and batch: 250, loss is 4.96061222076416 and perplexity is 142.68112150822628
At time: 885.0563886165619 and batch: 300, loss is 4.965320158004761 and perplexity is 143.35443899829392
At time: 887.5407829284668 and batch: 350, loss is 4.917625417709351 and perplexity is 136.67767515615398
At time: 890.0222144126892 and batch: 400, loss is 4.919382467269897 and perplexity is 136.91803570608567
At time: 892.5025811195374 and batch: 450, loss is 4.897423591613769 and perplexity is 133.9442396875814
At time: 894.987560749054 and batch: 500, loss is 4.890756902694702 and perplexity is 133.05424506402662
At time: 897.4666452407837 and batch: 550, loss is 4.906309547424317 and perplexity is 135.13976611344336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1236083984375 and perplexity of 167.9402730172709
Finished 31 epochs...
Completing Train Step...
At time: 901.5650215148926 and batch: 50, loss is 4.984856967926025 and perplexity is 146.18266472105645
At time: 904.0463409423828 and batch: 100, loss is 4.957466239929199 and perplexity is 142.23295476607427
At time: 906.5240910053253 and batch: 150, loss is 4.964867658615113 and perplexity is 143.2895858762495
At time: 909.0016496181488 and batch: 200, loss is 4.967433023452759 and perplexity is 143.65764784622095
At time: 911.4831655025482 and batch: 250, loss is 4.960249166488648 and perplexity is 142.62932991917563
At time: 913.9553971290588 and batch: 300, loss is 4.9650905895233155 and perplexity is 143.32153311464603
At time: 916.4349479675293 and batch: 350, loss is 4.917680902481079 and perplexity is 136.68525889614946
At time: 918.9171934127808 and batch: 400, loss is 4.919579105377197 and perplexity is 136.94496165672942
At time: 921.4009523391724 and batch: 450, loss is 4.89774079322815 and perplexity is 133.98673375588444
At time: 923.8828272819519 and batch: 500, loss is 4.89095106124878 and perplexity is 133.08008119193363
At time: 926.3672873973846 and batch: 550, loss is 4.906056814193725 and perplexity is 135.10561611937305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.123506164550781 and perplexity of 167.92310470803014
Finished 32 epochs...
Completing Train Step...
At time: 930.4889357089996 and batch: 50, loss is 4.984321575164795 and perplexity is 146.1044205281038
At time: 933.0224258899689 and batch: 100, loss is 4.957007265090942 and perplexity is 142.16768839762187
At time: 935.5034852027893 and batch: 150, loss is 4.964546213150024 and perplexity is 143.24353349073698
At time: 937.9814801216125 and batch: 200, loss is 4.96712737083435 and perplexity is 143.61374521982054
At time: 940.4615318775177 and batch: 250, loss is 4.959973573684692 and perplexity is 142.59002771816756
At time: 942.9420907497406 and batch: 300, loss is 4.964913053512573 and perplexity is 143.29609063994795
At time: 945.41947722435 and batch: 350, loss is 4.917712182998657 and perplexity is 136.68953454866497
At time: 947.9502120018005 and batch: 400, loss is 4.9197302913665775 and perplexity is 136.9656673814164
At time: 950.4315400123596 and batch: 450, loss is 4.897952823638916 and perplexity is 134.01514603010662
At time: 952.9122066497803 and batch: 500, loss is 4.8910519409179685 and perplexity is 133.0935069436812
At time: 955.3933401107788 and batch: 550, loss is 4.905802230834961 and perplexity is 135.07122485574098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.123466491699219 and perplexity of 167.91644285177148
Finished 33 epochs...
Completing Train Step...
At time: 959.5006582736969 and batch: 50, loss is 4.98383864402771 and perplexity is 146.0338791888405
At time: 961.9847283363342 and batch: 100, loss is 4.956601819992065 and perplexity is 142.11005888873285
At time: 964.4691009521484 and batch: 150, loss is 4.964263172149658 and perplexity is 143.20299543495832
At time: 966.9531977176666 and batch: 200, loss is 4.96685827255249 and perplexity is 143.57510420707862
At time: 969.4364883899689 and batch: 250, loss is 4.959711008071899 and perplexity is 142.55259339485963
At time: 971.9183962345123 and batch: 300, loss is 4.964748773574829 and perplexity is 143.27255190062291
At time: 974.4011046886444 and batch: 350, loss is 4.917715806961059 and perplexity is 136.69002990729652
At time: 976.8811540603638 and batch: 400, loss is 4.919862384796143 and perplexity is 136.98376084114076
At time: 979.3586194515228 and batch: 450, loss is 4.89812047958374 and perplexity is 134.03761634962373
At time: 981.8356258869171 and batch: 500, loss is 4.891100940704345 and perplexity is 133.10002865686957
At time: 984.315881729126 and batch: 550, loss is 4.905565204620362 and perplexity is 135.03921322855936
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.12344716389974 and perplexity of 167.91319742779837
Finished 34 epochs...
Completing Train Step...
At time: 988.4078919887543 and batch: 50, loss is 4.983399686813354 and perplexity is 145.96979063112667
At time: 990.91756772995 and batch: 100, loss is 4.956237907409668 and perplexity is 142.05835265906242
At time: 993.39710521698 and batch: 150, loss is 4.96402678489685 and perplexity is 143.16914807297368
At time: 995.8772437572479 and batch: 200, loss is 4.966625347137451 and perplexity is 143.5416658108192
At time: 998.355010509491 and batch: 250, loss is 4.959479722976685 and perplexity is 142.51962691719766
At time: 1000.8308663368225 and batch: 300, loss is 4.964599456787109 and perplexity is 143.251160500492
At time: 1003.3093435764313 and batch: 350, loss is 4.917738914489746 and perplexity is 136.69318851257745
At time: 1005.790712594986 and batch: 400, loss is 4.919966173171997 and perplexity is 136.99797890101883
At time: 1008.273030757904 and batch: 450, loss is 4.898260459899903 and perplexity is 134.05638029079856
At time: 1010.7822484970093 and batch: 500, loss is 4.891132335662842 and perplexity is 133.10420739234047
At time: 1013.2618448734283 and batch: 550, loss is 4.905334234237671 and perplexity is 135.0080267715143
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1234385172526045 and perplexity of 167.91174554790783
Finished 35 epochs...
Completing Train Step...
At time: 1017.3648307323456 and batch: 50, loss is 4.9829905128479 and perplexity is 145.9100758107657
At time: 1019.8417177200317 and batch: 100, loss is 4.955903511047364 and perplexity is 142.0108568043614
At time: 1022.3200199604034 and batch: 150, loss is 4.963796892166138 and perplexity is 143.13623830957204
At time: 1024.8008408546448 and batch: 200, loss is 4.966409578323364 and perplexity is 143.51069733695076
At time: 1027.2805426120758 and batch: 250, loss is 4.959262685775757 and perplexity is 142.4886982127554
At time: 1029.7612745761871 and batch: 300, loss is 4.964457950592041 and perplexity is 143.23089100799368
At time: 1032.2422540187836 and batch: 350, loss is 4.917748775482178 and perplexity is 136.6945364497208
At time: 1034.7213129997253 and batch: 400, loss is 4.920055465698242 and perplexity is 137.0102123428147
At time: 1037.2021112442017 and batch: 450, loss is 4.898375730514527 and perplexity is 134.07183394280784
At time: 1039.6820783615112 and batch: 500, loss is 4.891144504547119 and perplexity is 133.1058271318922
At time: 1042.1443784236908 and batch: 550, loss is 4.905104951858521 and perplexity is 134.97707535837415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.123432922363281 and perplexity of 167.91080610290342
Finished 36 epochs...
Completing Train Step...
At time: 1046.2251312732697 and batch: 50, loss is 4.982599802017212 and perplexity is 145.85307829933754
At time: 1048.757930278778 and batch: 100, loss is 4.955585899353028 and perplexity is 141.96575965758737
At time: 1051.240519285202 and batch: 150, loss is 4.96357871055603 and perplexity is 143.1050120212577
At time: 1053.720894575119 and batch: 200, loss is 4.966204223632812 and perplexity is 143.48122976786357
At time: 1056.202984571457 and batch: 250, loss is 4.9590543937683105 and perplexity is 142.45902204652765
At time: 1058.6844582557678 and batch: 300, loss is 4.96432168006897 and perplexity is 143.21137418936982
At time: 1061.1660861968994 and batch: 350, loss is 4.917747249603272 and perplexity is 136.69432787057022
At time: 1063.6470835208893 and batch: 400, loss is 4.920131340026855 and perplexity is 137.0206082950773
At time: 1066.125220298767 and batch: 450, loss is 4.898467683792115 and perplexity is 134.08416285420432
At time: 1068.6023635864258 and batch: 500, loss is 4.891139869689941 and perplexity is 133.10521020682356
At time: 1071.073918581009 and batch: 550, loss is 4.904877958297729 and perplexity is 134.9464399085698
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1234288533528645 and perplexity of 167.91012287347436
Finished 37 epochs...
Completing Train Step...
At time: 1075.1686370372772 and batch: 50, loss is 4.982223281860351 and perplexity is 145.79817201272812
At time: 1077.6524510383606 and batch: 100, loss is 4.9552816390991214 and perplexity is 141.9225716900321
At time: 1080.1328320503235 and batch: 150, loss is 4.963367977142334 and perplexity is 143.07485819087884
At time: 1082.6155729293823 and batch: 200, loss is 4.966007471084595 and perplexity is 143.45300224729473
At time: 1085.0975749492645 and batch: 250, loss is 4.958850212097168 and perplexity is 142.42993749470412
At time: 1087.5766577720642 and batch: 300, loss is 4.964189462661743 and perplexity is 143.19244040450462
At time: 1090.0583419799805 and batch: 350, loss is 4.917736167907715 and perplexity is 136.6928130740377
At time: 1092.5395514965057 and batch: 400, loss is 4.920190095901489 and perplexity is 137.0286592972801
At time: 1095.020067691803 and batch: 450, loss is 4.8985396671295165 and perplexity is 134.09381502713293
At time: 1097.4972143173218 and batch: 500, loss is 4.891123218536377 and perplexity is 133.1029938699805
At time: 1099.9769208431244 and batch: 550, loss is 4.904652252197265 and perplexity is 134.91598511089265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.123414611816406 and perplexity of 167.9077315923655
Finished 38 epochs...
Completing Train Step...
At time: 1104.1039073467255 and batch: 50, loss is 4.981858739852905 and perplexity is 145.74503214086687
At time: 1106.6155474185944 and batch: 100, loss is 4.954982147216797 and perplexity is 141.88007339615703
At time: 1109.0947370529175 and batch: 150, loss is 4.963165063858032 and perplexity is 143.04582934677092
At time: 1111.5778794288635 and batch: 200, loss is 4.965815372467041 and perplexity is 143.42544777055232
At time: 1114.059796333313 and batch: 250, loss is 4.958646545410156 and perplexity is 142.4009322150081
At time: 1116.5423407554626 and batch: 300, loss is 4.964055910110473 and perplexity is 143.17331796571975
At time: 1119.02516579628 and batch: 350, loss is 4.91771222114563 and perplexity is 136.689539762957
At time: 1121.506824016571 and batch: 400, loss is 4.92024748802185 and perplexity is 137.036523888268
At time: 1123.990569114685 and batch: 450, loss is 4.898599882125854 and perplexity is 134.1018897288203
At time: 1126.4744672775269 and batch: 500, loss is 4.891099042892456 and perplexity is 133.09977605829246
At time: 1128.9540724754333 and batch: 550, loss is 4.904429912567139 and perplexity is 134.88599127519765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.123412068684896 and perplexity of 167.90730458146552
Finished 39 epochs...
Completing Train Step...
At time: 1133.0952978134155 and batch: 50, loss is 4.981508741378784 and perplexity is 145.69403052777085
At time: 1135.5754506587982 and batch: 100, loss is 4.95470121383667 and perplexity is 141.84022014588143
At time: 1138.0886642932892 and batch: 150, loss is 4.962968597412109 and perplexity is 143.0177284016119
At time: 1140.571259021759 and batch: 200, loss is 4.965621643066406 and perplexity is 143.3976647358021
At time: 1143.0477449893951 and batch: 250, loss is 4.958457107543945 and perplexity is 142.37395864125168
At time: 1145.523423910141 and batch: 300, loss is 4.963929615020752 and perplexity is 143.1552370204729
At time: 1147.9973917007446 and batch: 350, loss is 4.917692880630494 and perplexity is 136.68689614240884
At time: 1150.478993654251 and batch: 400, loss is 4.920296773910523 and perplexity is 137.0432780215688
At time: 1152.9598052501678 and batch: 450, loss is 4.89865177154541 and perplexity is 134.10884837857827
At time: 1155.4382061958313 and batch: 500, loss is 4.891072721481323 and perplexity is 133.09627273047153
At time: 1157.9188606739044 and batch: 550, loss is 4.904205942153931 and perplexity is 134.85578418686913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.12340342203776 and perplexity of 167.90585275252798
Finished 40 epochs...
Completing Train Step...
At time: 1161.9893476963043 and batch: 50, loss is 4.9811670684814455 and perplexity is 145.64425932945446
At time: 1164.4994196891785 and batch: 100, loss is 4.954425868988037 and perplexity is 141.80117054823472
At time: 1166.981064081192 and batch: 150, loss is 4.962778339385986 and perplexity is 142.99052071922765
At time: 1169.4605145454407 and batch: 200, loss is 4.965438575744629 and perplexity is 143.37141571211234
At time: 1171.9422018527985 and batch: 250, loss is 4.958274936676025 and perplexity is 142.34802461592042
At time: 1174.4252824783325 and batch: 300, loss is 4.9638057041168215 and perplexity is 143.1374996246024
At time: 1176.9065868854523 and batch: 350, loss is 4.917674770355225 and perplexity is 136.68442072750938
At time: 1179.3896775245667 and batch: 400, loss is 4.9203445339202885 and perplexity is 137.04982336616703
At time: 1181.8718287944794 and batch: 450, loss is 4.898694744110108 and perplexity is 134.114611503569
At time: 1184.3514387607574 and batch: 500, loss is 4.891043758392334 and perplexity is 133.09241790710436
At time: 1186.8243894577026 and batch: 550, loss is 4.903986501693725 and perplexity is 134.82619461821878
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.12340342203776 and perplexity of 167.90585275252798
Finished 41 epochs...
Completing Train Step...
At time: 1190.9589974880219 and batch: 50, loss is 4.980835666656494 and perplexity is 145.596000553084
At time: 1193.4345152378082 and batch: 100, loss is 4.954151782989502 and perplexity is 141.76231015859926
At time: 1195.9162454605103 and batch: 150, loss is 4.9625953102111815 and perplexity is 142.96435167713767
At time: 1198.3981745243073 and batch: 200, loss is 4.9652559947967525 and perplexity is 143.34524121268845
At time: 1200.9305453300476 and batch: 250, loss is 4.958096628189087 and perplexity is 142.3226450177986
At time: 1203.4102189540863 and batch: 300, loss is 4.963680648803711 and perplexity is 143.119600638974
At time: 1205.8906240463257 and batch: 350, loss is 4.917657423019409 and perplexity is 136.68204963752834
At time: 1208.3735501766205 and batch: 400, loss is 4.920385751724243 and perplexity is 137.05547237533767
At time: 1210.8551111221313 and batch: 450, loss is 4.898731470108032 and perplexity is 134.11953708696066
At time: 1213.3364453315735 and batch: 500, loss is 4.891014451980591 and perplexity is 133.08851750305905
At time: 1215.8174002170563 and batch: 550, loss is 4.903768186569214 and perplexity is 134.7967632335284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.123405965169271 and perplexity of 167.9062797597359
Annealing...
Finished 42 epochs...
Completing Train Step...
At time: 1219.9122455120087 and batch: 50, loss is 4.980569076538086 and perplexity is 145.55719127137212
At time: 1222.4347281455994 and batch: 100, loss is 4.953761796951294 and perplexity is 141.70703561574388
At time: 1224.9014067649841 and batch: 150, loss is 4.962189502716065 and perplexity is 142.90634744176532
At time: 1227.3697164058685 and batch: 200, loss is 4.9648277282714846 and perplexity is 143.2838643880784
At time: 1229.8469247817993 and batch: 250, loss is 4.9569854736328125 and perplexity is 142.16459039014794
At time: 1232.3280818462372 and batch: 300, loss is 4.9625364112854005 and perplexity is 142.95593147837167
At time: 1234.8112361431122 and batch: 350, loss is 4.91612738609314 and perplexity is 136.47308096008777
At time: 1237.2930641174316 and batch: 400, loss is 4.91885968208313 and perplexity is 136.84647569210472
At time: 1239.7741618156433 and batch: 450, loss is 4.897164220809937 and perplexity is 133.9095029675051
At time: 1242.2561264038086 and batch: 500, loss is 4.889588003158569 and perplexity is 132.89880888102996
At time: 1244.7363920211792 and batch: 550, loss is 4.902844161987304 and perplexity is 134.6722652392006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.124156697591146 and perplexity of 168.03237977555062
Annealing...
Finished 43 epochs...
Completing Train Step...
At time: 1248.8796424865723 and batch: 50, loss is 4.980343179702759 and perplexity is 145.5243140760718
At time: 1251.3613538742065 and batch: 100, loss is 4.9535769271850585 and perplexity is 141.68084069059506
At time: 1253.8458960056305 and batch: 150, loss is 4.962052516937256 and perplexity is 142.88677264522923
At time: 1256.3278903961182 and batch: 200, loss is 4.964623212814331 and perplexity is 143.25456361938254
At time: 1258.8096685409546 and batch: 250, loss is 4.956632556915284 and perplexity is 142.11442698183197
At time: 1261.2927055358887 and batch: 300, loss is 4.962176113128662 and perplexity is 142.90443399754605
At time: 1263.804517507553 and batch: 350, loss is 4.915620822906494 and perplexity is 136.4039662282725
At time: 1266.2870151996613 and batch: 400, loss is 4.918312187194824 and perplexity is 136.77157345234835
At time: 1268.7714264392853 and batch: 450, loss is 4.8966115283966065 and perplexity is 133.8355126499692
At time: 1271.2529277801514 and batch: 500, loss is 4.8890626335144045 and perplexity is 132.82900621880367
At time: 1273.7322521209717 and batch: 550, loss is 4.902565326690674 and perplexity is 134.6347190930134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.124458821614583 and perplexity of 168.0831540638761
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 1277.770696401596 and batch: 50, loss is 4.980289392471313 and perplexity is 145.51648693661159
At time: 1280.301430940628 and batch: 100, loss is 4.95351655960083 and perplexity is 141.6722880186658
At time: 1282.7684350013733 and batch: 150, loss is 4.961969041824341 and perplexity is 142.8748456535591
At time: 1285.2488667964935 and batch: 200, loss is 4.96449758529663 and perplexity is 143.23656803454975
At time: 1287.73069190979 and batch: 250, loss is 4.956492300033569 and perplexity is 142.09449585322855
At time: 1290.2107992172241 and batch: 300, loss is 4.9620638275146485 and perplexity is 142.88838878626936
At time: 1292.689682483673 and batch: 350, loss is 4.915432004928589 and perplexity is 136.3782131385908
At time: 1295.1713309288025 and batch: 400, loss is 4.918127813339233 and perplexity is 136.74635867456004
At time: 1297.6525106430054 and batch: 450, loss is 4.896430044174195 and perplexity is 133.81122581992972
At time: 1300.1309943199158 and batch: 500, loss is 4.888875150680542 and perplexity is 132.80410539461016
At time: 1302.6098217964172 and batch: 550, loss is 4.902471561431884 and perplexity is 134.6220956255662
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.124507141113281 and perplexity of 168.09127595384132
Annealing...
Finished 45 epochs...
Completing Train Step...
At time: 1306.6978969573975 and batch: 50, loss is 4.980271511077881 and perplexity is 145.5138849223216
At time: 1309.1776094436646 and batch: 100, loss is 4.953493242263794 and perplexity is 141.66898463669054
At time: 1311.6594021320343 and batch: 150, loss is 4.961937646865845 and perplexity is 142.87036017412078
At time: 1314.1405563354492 and batch: 200, loss is 4.964451675415039 and perplexity is 143.2299922116205
At time: 1316.614339351654 and batch: 250, loss is 4.956444129943848 and perplexity is 142.08765131346675
At time: 1319.0891091823578 and batch: 300, loss is 4.962025623321534 and perplexity is 142.8829299549461
At time: 1321.5614466667175 and batch: 350, loss is 4.915369319915771 and perplexity is 136.36966453648955
At time: 1324.0320346355438 and batch: 400, loss is 4.918067255020142 and perplexity is 136.73807779567736
At time: 1326.5397055149078 and batch: 450, loss is 4.8963680076599125 and perplexity is 133.8029248953909
At time: 1329.017744064331 and batch: 500, loss is 4.8888118457794185 and perplexity is 132.79569850995063
At time: 1331.4952955245972 and batch: 550, loss is 4.902441453933716 and perplexity is 134.61804255208298
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.12452392578125 and perplexity of 168.09409733377447
Annealing...
Model not improving. Stopping early with 167.90585275252798loss at 45 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f690564a9e8>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'anneal': 2.0, 'wordvec_source': '', 'data': 'wikitext', 'lr': 4.744550879574482, 'num_layers': 1, 'batch_size': 80, 'dropout': 0.0, 'wordvec_dim': 200, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.9988977909088135 and batch: 50, loss is 7.263780002593994 and perplexity is 1427.6428443150735
At time: 5.470466613769531 and batch: 100, loss is 6.270086050033569 and perplexity is 528.5228553304346
At time: 7.943020343780518 and batch: 150, loss is 6.040081672668457 and perplexity is 419.92733007177094
At time: 10.416942119598389 and batch: 200, loss is 5.820208148956299 and perplexity is 337.04220131166636
At time: 12.893491983413696 and batch: 250, loss is 5.611219387054444 and perplexity is 273.4775097048788
At time: 15.370041608810425 and batch: 300, loss is 5.527757081985474 and perplexity is 251.57900665713603
At time: 17.847168445587158 and batch: 350, loss is 5.418314867019653 and perplexity is 225.49880667450296
At time: 20.32209587097168 and batch: 400, loss is 5.385477237701416 and perplexity is 218.21421931408437
At time: 22.799167156219482 and batch: 450, loss is 5.30626672744751 and perplexity is 201.59620819171948
At time: 25.274685859680176 and batch: 500, loss is 5.269653615951538 and perplexity is 194.34863151908792
At time: 27.7960946559906 and batch: 550, loss is 5.225257043838501 and perplexity is 185.90895113524684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.025109354654948 and perplexity of 152.1868975484127
Finished 1 epochs...
Completing Train Step...
At time: 31.900949001312256 and batch: 50, loss is 5.251908874511718 and perplexity is 190.930382965171
At time: 34.374282121658325 and batch: 100, loss is 5.191574583053589 and perplexity is 179.75136366999746
At time: 36.85023093223572 and batch: 150, loss is 5.163381586074829 and perplexity is 174.7544046224278
At time: 39.32488250732422 and batch: 200, loss is 5.136424207687378 and perplexity is 170.1064143003095
At time: 41.79964995384216 and batch: 250, loss is 5.074388742446899 and perplexity is 159.87443764833336
At time: 44.269858837127686 and batch: 300, loss is 5.045850191116333 and perplexity is 155.37634264552912
At time: 46.73975610733032 and batch: 350, loss is 4.980831432342529 and perplexity is 145.5953840552108
At time: 49.20237445831299 and batch: 400, loss is 4.971034774780273 and perplexity is 144.17599989728782
At time: 51.67566466331482 and batch: 450, loss is 4.906276636123657 and perplexity is 135.13531856115745
At time: 54.15018844604492 and batch: 500, loss is 4.900309495925903 and perplexity is 134.33134825695842
At time: 56.624250173568726 and batch: 550, loss is 4.8891307067871095 and perplexity is 132.83804863173697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.836946614583334 and perplexity of 126.08378100346523
Finished 2 epochs...
Completing Train Step...
At time: 60.69017791748047 and batch: 50, loss is 4.934251890182495 and perplexity is 138.96913945651053
At time: 63.21674418449402 and batch: 100, loss is 4.897635669708252 and perplexity is 133.9726493391271
At time: 65.69030117988586 and batch: 150, loss is 4.8805246257781985 and perplexity is 131.69973884297337
At time: 68.17026853561401 and batch: 200, loss is 4.873875198364257 and perplexity is 130.82691609084722
At time: 70.65040946006775 and batch: 250, loss is 4.838566913604736 and perplexity is 126.2882400280869
At time: 73.13139486312866 and batch: 300, loss is 4.819522190093994 and perplexity is 123.90587317995491
At time: 75.61070322990417 and batch: 350, loss is 4.764496841430664 and perplexity is 117.27209598047905
At time: 78.08362627029419 and batch: 400, loss is 4.756087913513183 and perplexity is 116.29009794043648
At time: 80.56248712539673 and batch: 450, loss is 4.70014121055603 and perplexity is 109.96269924972744
At time: 83.04322075843811 and batch: 500, loss is 4.702294034957886 and perplexity is 110.19968463439693
At time: 85.52306032180786 and batch: 550, loss is 4.7020064735412594 and perplexity is 110.16800001282738
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.7530370076497395 and perplexity of 115.9358484644515
Finished 3 epochs...
Completing Train Step...
At time: 89.55083632469177 and batch: 50, loss is 4.753917589187622 and perplexity is 116.03798439509556
At time: 92.0551404953003 and batch: 100, loss is 4.724472970962524 and perplexity is 112.67110178125631
At time: 94.53289008140564 and batch: 150, loss is 4.712820205688477 and perplexity is 111.36579188425365
At time: 97.00831508636475 and batch: 200, loss is 4.704809980392456 and perplexity is 110.47729010151131
At time: 99.48511695861816 and batch: 250, loss is 4.681086616516113 and perplexity is 107.88724095922325
At time: 101.96552419662476 and batch: 300, loss is 4.668415966033936 and perplexity is 106.52886337472937
At time: 104.44291377067566 and batch: 350, loss is 4.618058938980102 and perplexity is 101.29721709681606
At time: 106.91911768913269 and batch: 400, loss is 4.611858320236206 and perplexity is 100.67105497627679
At time: 109.39613580703735 and batch: 450, loss is 4.558024005889893 and perplexity is 95.39479391941705
At time: 111.87049722671509 and batch: 500, loss is 4.563629550933838 and perplexity is 95.93103529212144
At time: 114.39253234863281 and batch: 550, loss is 4.566499938964844 and perplexity is 96.20679016003054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.71318359375 and perplexity of 111.40626823735283
Finished 4 epochs...
Completing Train Step...
At time: 118.40159153938293 and batch: 50, loss is 4.624032983779907 and perplexity is 101.90418242370029
At time: 120.87369418144226 and batch: 100, loss is 4.597305965423584 and perplexity is 99.21666215154394
At time: 123.34307646751404 and batch: 150, loss is 4.592104892730713 and perplexity is 98.70196871858333
At time: 125.8138427734375 and batch: 200, loss is 4.579323997497559 and perplexity is 97.44849650856003
At time: 128.29291343688965 and batch: 250, loss is 4.5639622688293455 and perplexity is 95.96295857472691
At time: 130.76758217811584 and batch: 300, loss is 4.55510986328125 and perplexity is 95.11720454976466
At time: 133.23756098747253 and batch: 350, loss is 4.509627380371094 and perplexity is 90.88794556751033
At time: 135.70800304412842 and batch: 400, loss is 4.500460147857666 and perplexity is 90.05856202204083
At time: 138.18301844596863 and batch: 450, loss is 4.448521242141724 and perplexity is 85.50041606102762
At time: 140.65722560882568 and batch: 500, loss is 4.456524391174316 and perplexity is 86.18743412009248
At time: 143.13499808311462 and batch: 550, loss is 4.460555830001831 and perplexity is 86.53559481117226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.691674296061198 and perplexity of 109.03558491508218
Finished 5 epochs...
Completing Train Step...
At time: 147.20195198059082 and batch: 50, loss is 4.522524375915527 and perplexity is 92.06771840622481
At time: 149.70690655708313 and batch: 100, loss is 4.499371891021728 and perplexity is 89.96060848525586
At time: 152.18237018585205 and batch: 150, loss is 4.4937903022766115 and perplexity is 89.45988408424012
At time: 154.65912222862244 and batch: 200, loss is 4.478385648727417 and perplexity is 88.09234583320139
At time: 157.13676047325134 and batch: 250, loss is 4.470669984817505 and perplexity is 87.41527030227425
At time: 159.61432480812073 and batch: 300, loss is 4.4638029479980466 and perplexity is 86.81704279851553
At time: 162.09415078163147 and batch: 350, loss is 4.421159629821777 and perplexity is 83.19270218203161
At time: 164.57280445098877 and batch: 400, loss is 4.413282079696655 and perplexity is 82.53992203045982
At time: 167.05145907402039 and batch: 450, loss is 4.358248252868652 and perplexity is 78.1201677128968
At time: 169.5291564464569 and batch: 500, loss is 4.366901502609253 and perplexity is 78.7990942579445
At time: 172.00399613380432 and batch: 550, loss is 4.374841670989991 and perplexity is 79.42726291675912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.679569498697917 and perplexity of 107.72368739998761
Finished 6 epochs...
Completing Train Step...
At time: 176.03126883506775 and batch: 50, loss is 4.435183410644531 and perplexity is 84.36759738475088
At time: 178.4966757297516 and batch: 100, loss is 4.416463012695313 and perplexity is 82.80289401855393
At time: 180.97505688667297 and batch: 150, loss is 4.409974069595337 and perplexity is 82.26733025137975
At time: 183.45088601112366 and batch: 200, loss is 4.3948765563964844 and perplexity is 81.03462693892729
At time: 185.9283595085144 and batch: 250, loss is 4.387831230163574 and perplexity is 80.46571798379779
At time: 188.40484762191772 and batch: 300, loss is 4.3828552722930905 and perplexity is 80.06631848273679
At time: 190.87831807136536 and batch: 350, loss is 4.34604326248169 and perplexity is 77.17250667759052
At time: 193.34914541244507 and batch: 400, loss is 4.335322217941284 and perplexity is 76.34955612351197
At time: 195.82186436653137 and batch: 450, loss is 4.281828908920288 and perplexity is 72.37268208495657
At time: 198.29496312141418 and batch: 500, loss is 4.292363910675049 and perplexity is 73.13915874518004
At time: 200.7719430923462 and batch: 550, loss is 4.299947452545166 and perplexity is 73.69592106476455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.678584289550781 and perplexity of 107.61760930093504
Finished 7 epochs...
Completing Train Step...
At time: 204.77292013168335 and batch: 50, loss is 4.360121698379516 and perplexity is 78.26665876906611
At time: 207.26682686805725 and batch: 100, loss is 4.345217447280884 and perplexity is 77.10880275594567
At time: 209.74470233917236 and batch: 150, loss is 4.339613790512085 and perplexity is 76.67791987883484
At time: 212.21280884742737 and batch: 200, loss is 4.325043907165528 and perplexity is 75.56883080004806
At time: 214.68446111679077 and batch: 250, loss is 4.318648233413696 and perplexity is 75.0870594807567
At time: 217.1609296798706 and batch: 300, loss is 4.322703504562378 and perplexity is 75.39217611390559
At time: 219.63224005699158 and batch: 350, loss is 4.279099721908569 and perplexity is 72.17543278854954
At time: 222.1105251312256 and batch: 400, loss is 4.2694520664215085 and perplexity is 71.4824572480276
At time: 224.5820164680481 and batch: 450, loss is 4.214076461791993 and perplexity is 67.63167658235521
At time: 227.05179715156555 and batch: 500, loss is 4.228867835998535 and perplexity is 68.63947702139673
At time: 229.52405452728271 and batch: 550, loss is 4.232388820648193 and perplexity is 68.88158153937451
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.674566650390625 and perplexity of 107.18610796823154
Finished 8 epochs...
Completing Train Step...
At time: 233.67146277427673 and batch: 50, loss is 4.294329185485839 and perplexity is 73.28303862698908
At time: 236.13845300674438 and batch: 100, loss is 4.284310913085937 and perplexity is 72.5525344882483
At time: 238.64546155929565 and batch: 150, loss is 4.277809267044067 and perplexity is 72.0823537202951
At time: 241.1188769340515 and batch: 200, loss is 4.265304298400879 and perplexity is 71.1865786399232
At time: 243.5899658203125 and batch: 250, loss is 4.259186286926269 and perplexity is 70.7523878813025
At time: 246.05692791938782 and batch: 300, loss is 4.258675947189331 and perplexity is 70.71628933832733
At time: 248.52028393745422 and batch: 350, loss is 4.218884038925171 and perplexity is 67.95760391681368
At time: 250.98617100715637 and batch: 400, loss is 4.210333375930786 and perplexity is 67.37899860267277
At time: 253.44705939292908 and batch: 450, loss is 4.1550587463378905 and perplexity is 63.75570985227381
At time: 255.90250325202942 and batch: 500, loss is 4.169126081466675 and perplexity is 64.65892077989916
At time: 258.3742756843567 and batch: 550, loss is 4.175219116210937 and perplexity is 65.05409250570108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.677723185221354 and perplexity of 107.52497919945786
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 262.4574890136719 and batch: 50, loss is 4.233118848800659 and perplexity is 68.93188539246502
At time: 264.9663915634155 and batch: 100, loss is 4.212384843826294 and perplexity is 67.51736633509108
At time: 267.44221806526184 and batch: 150, loss is 4.198727254867554 and perplexity is 66.60151032671762
At time: 269.9190182685852 and batch: 200, loss is 4.1696072196960445 and perplexity is 64.69003814383258
At time: 272.3974087238312 and batch: 250, loss is 4.153898706436157 and perplexity is 63.681793566091585
At time: 274.87361669540405 and batch: 300, loss is 4.145172448158264 and perplexity is 63.12850735795745
At time: 277.35421347618103 and batch: 350, loss is 4.100736184120178 and perplexity is 60.38472551413547
At time: 279.8348705768585 and batch: 400, loss is 4.07680685043335 and perplexity is 58.95691072741747
At time: 282.3189079761505 and batch: 450, loss is 4.013420505523682 and perplexity is 55.33582371225743
At time: 284.79417157173157 and batch: 500, loss is 4.015770406723022 and perplexity is 55.46601033370638
At time: 287.2681393623352 and batch: 550, loss is 4.024195871353149 and perplexity is 55.935311505917184
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.644768269856771 and perplexity of 104.03925396058315
Finished 10 epochs...
Completing Train Step...
At time: 291.31695771217346 and batch: 50, loss is 4.160586786270142 and perplexity is 64.1091299223565
At time: 293.7877531051636 and batch: 100, loss is 4.144377794265747 and perplexity is 63.07836197060909
At time: 296.2612855434418 and batch: 150, loss is 4.137282476425171 and perplexity is 62.632384988785795
At time: 298.7348401546478 and batch: 200, loss is 4.11298490524292 and perplexity is 61.12890952402214
At time: 301.2380540370941 and batch: 250, loss is 4.104606981277466 and perplexity is 60.618915496551274
At time: 303.7083055973053 and batch: 300, loss is 4.100272560119629 and perplexity is 60.356736194882
At time: 306.18535900115967 and batch: 350, loss is 4.062258005142212 and perplexity is 58.105365276334965
At time: 308.65868949890137 and batch: 400, loss is 4.043449492454529 and perplexity is 57.02270332354993
At time: 311.1287910938263 and batch: 450, loss is 3.9853568267822266 and perplexity is 53.80448493418804
At time: 313.6095697879791 and batch: 500, loss is 3.9904055833816527 and perplexity is 54.076817574707896
At time: 316.07927107810974 and batch: 550, loss is 4.000455408096314 and perplexity is 54.62302013530619
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.646966044108073 and perplexity of 104.26816020404752
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 320.104510307312 and batch: 50, loss is 4.129760460853577 and perplexity is 62.163030671885814
At time: 322.6022651195526 and batch: 100, loss is 4.113923525810241 and perplexity is 61.18631331173279
At time: 325.06366991996765 and batch: 150, loss is 4.0999136066436765 and perplexity is 60.33507482257271
At time: 327.5391285419464 and batch: 200, loss is 4.0697347974777225 and perplexity is 58.541435196759274
At time: 330.0147695541382 and batch: 250, loss is 4.058257975578308 and perplexity is 57.87340632801673
At time: 332.4977397918701 and batch: 300, loss is 4.047172126770019 and perplexity is 57.23537359693424
At time: 334.9852864742279 and batch: 350, loss is 4.000694322586059 and perplexity is 54.63607192535898
At time: 337.4691460132599 and batch: 400, loss is 3.97870201587677 and perplexity is 53.4476150289965
At time: 339.94975304603577 and batch: 450, loss is 3.9136456871032714 and perplexity is 50.081199946771825
At time: 342.42524671554565 and batch: 500, loss is 3.914637956619263 and perplexity is 50.13091865790457
At time: 344.89850974082947 and batch: 550, loss is 3.925957889556885 and perplexity is 50.70162135918375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.618216959635417 and perplexity of 101.31322541423235
Finished 12 epochs...
Completing Train Step...
At time: 348.92683839797974 and batch: 50, loss is 4.091591958999634 and perplexity is 59.835070902970514
At time: 351.4075448513031 and batch: 100, loss is 4.072873883247375 and perplexity is 58.725490514496926
At time: 353.8868656158447 and batch: 150, loss is 4.06332923412323 and perplexity is 58.16764277834866
At time: 356.3630609512329 and batch: 200, loss is 4.038540906906128 and perplexity is 56.743488342018715
At time: 358.8411285877228 and batch: 250, loss is 4.029711704254151 and perplexity is 56.244693803580006
At time: 361.31932282447815 and batch: 300, loss is 4.023533511161804 and perplexity is 55.8982744495751
At time: 363.82626700401306 and batch: 350, loss is 3.98068838596344 and perplexity is 53.55388728576701
At time: 366.3034107685089 and batch: 400, loss is 3.963293766975403 and perplexity is 52.630393021766785
At time: 368.780681848526 and batch: 450, loss is 3.9024506187438965 and perplexity is 49.523664138555176
At time: 371.2582836151123 and batch: 500, loss is 3.907414298057556 and perplexity is 49.77009482125196
At time: 373.7355101108551 and batch: 550, loss is 3.917311906814575 and perplexity is 50.26514561576417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.618653869628906 and perplexity of 101.35749984615502
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 377.7815623283386 and batch: 50, loss is 4.075113749504089 and perplexity is 58.857175182075856
At time: 380.3026211261749 and batch: 100, loss is 4.062883334159851 and perplexity is 58.14171161034706
At time: 382.78112030029297 and batch: 150, loss is 4.05132640838623 and perplexity is 57.47364002752328
At time: 385.26077151298523 and batch: 200, loss is 4.02178213596344 and perplexity is 55.80046127685711
At time: 387.7379355430603 and batch: 250, loss is 4.011297841072082 and perplexity is 55.218488901635496
At time: 390.2182550430298 and batch: 300, loss is 4.000376725196839 and perplexity is 54.618722406784954
At time: 392.6972804069519 and batch: 350, loss is 3.9535233640670775 and perplexity is 52.11867678435687
At time: 395.1550226211548 and batch: 400, loss is 3.9317890405654907 and perplexity is 50.99813383404314
At time: 397.6262195110321 and batch: 450, loss is 3.866669774055481 and perplexity is 47.782992658217864
At time: 400.10131573677063 and batch: 500, loss is 3.8688758516311648 and perplexity is 47.88852200698618
At time: 402.5736961364746 and batch: 550, loss is 3.8788853740692137 and perplexity is 48.37027025527914
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.604322306315104 and perplexity of 99.91524796754138
Finished 14 epochs...
Completing Train Step...
At time: 406.65446162223816 and batch: 50, loss is 4.054326043128968 and perplexity is 57.64629878207831
At time: 409.1238577365875 and batch: 100, loss is 4.038909392356873 and perplexity is 56.764401344729215
At time: 411.5907905101776 and batch: 150, loss is 4.0286518621444705 and perplexity is 56.18511488634048
At time: 414.0624122619629 and batch: 200, loss is 4.001528306007385 and perplexity is 54.68165650930262
At time: 416.53775668144226 and batch: 250, loss is 3.996637463569641 and perplexity is 54.414870080077336
At time: 419.0126094818115 and batch: 300, loss is 3.9856095457077028 and perplexity is 53.81808406411278
At time: 421.48794627189636 and batch: 350, loss is 3.9446006774902345 and perplexity is 51.65570670664307
At time: 423.9627034664154 and batch: 400, loss is 3.9244936895370484 and perplexity is 50.627438366806636
At time: 426.4896116256714 and batch: 450, loss is 3.8627760219573974 and perplexity is 47.59729928689208
At time: 428.96580481529236 and batch: 500, loss is 3.8667639923095702 and perplexity is 47.787494900454654
At time: 431.4425780773163 and batch: 550, loss is 3.8769168043136597 and perplexity is 48.27514366656944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.604858907063802 and perplexity of 99.96887695179684
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 435.4829092025757 and batch: 50, loss is 4.047261576652527 and perplexity is 57.24049352336279
At time: 437.9852955341339 and batch: 100, loss is 4.036658806800842 and perplexity is 56.63679185486668
At time: 440.4604947566986 and batch: 150, loss is 4.026259651184082 and perplexity is 56.05086887510893
At time: 442.93600845336914 and batch: 200, loss is 3.996516327857971 and perplexity is 54.40827889528675
At time: 445.4029402732849 and batch: 250, loss is 3.990293416976929 and perplexity is 54.07075231266731
At time: 447.87884736061096 and batch: 300, loss is 3.975383563041687 and perplexity is 53.27054560014179
At time: 450.35194420814514 and batch: 350, loss is 3.9306958389282225 and perplexity is 50.94241305321184
At time: 452.8316614627838 and batch: 400, loss is 3.909046277999878 and perplexity is 49.851384931600435
At time: 455.3120174407959 and batch: 450, loss is 3.845462245941162 and perplexity is 46.780303359231944
At time: 457.7873318195343 and batch: 500, loss is 3.8458699703216555 and perplexity is 46.79938071832631
At time: 460.2634553909302 and batch: 550, loss is 3.859017848968506 and perplexity is 47.41875611058242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.596500651041667 and perplexity of 99.136793710506
Finished 16 epochs...
Completing Train Step...
At time: 464.34978222846985 and batch: 50, loss is 4.037280445098877 and perplexity is 56.67201039922533
At time: 466.8383102416992 and batch: 100, loss is 4.022271265983582 and perplexity is 55.82776163377317
At time: 469.3210792541504 and batch: 150, loss is 4.011748595237732 and perplexity is 55.24338447599919
At time: 471.7919690608978 and batch: 200, loss is 3.9836784410476684 and perplexity is 53.71425599488262
At time: 474.265442609787 and batch: 250, loss is 3.979212899208069 and perplexity is 53.47492750076173
At time: 476.7257971763611 and batch: 300, loss is 3.967453179359436 and perplexity is 52.849760433775536
At time: 479.2001795768738 and batch: 350, loss is 3.92511634349823 and perplexity is 50.65897155796461
At time: 481.685261964798 and batch: 400, loss is 3.9074381399154663 and perplexity is 49.77128144692649
At time: 484.1701819896698 and batch: 450, loss is 3.845382571220398 and perplexity is 46.77657630010271
At time: 486.65293192863464 and batch: 500, loss is 3.846197805404663 and perplexity is 46.81472571236507
At time: 489.1368319988251 and batch: 550, loss is 3.8587111711502073 and perplexity is 47.40421605958152
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.596540832519532 and perplexity of 99.14077725341987
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 493.2238507270813 and batch: 50, loss is 4.033847193717957 and perplexity is 56.47777476193905
At time: 495.73342728614807 and batch: 100, loss is 4.022160840034485 and perplexity is 55.821597140575086
At time: 498.20804166793823 and batch: 150, loss is 4.011478133201599 and perplexity is 55.22844525808768
At time: 500.6854798793793 and batch: 200, loss is 3.9823189115524293 and perplexity is 53.64127949763332
At time: 503.16551327705383 and batch: 250, loss is 3.97704562664032 and perplexity is 53.35915825439758
At time: 505.64502787590027 and batch: 300, loss is 3.9635268020629884 and perplexity is 52.64265917918129
At time: 508.1265432834625 and batch: 350, loss is 3.918802604675293 and perplexity is 50.340131637667426
At time: 510.60436034202576 and batch: 400, loss is 3.898409185409546 and perplexity is 49.323921446722395
At time: 513.0851066112518 and batch: 450, loss is 3.8361067008972167 and perplexity is 46.34468900613825
At time: 515.5660133361816 and batch: 500, loss is 3.8351752519607545 and perplexity is 46.30154139287162
At time: 518.0440816879272 and batch: 550, loss is 3.8476016759872436 and perplexity is 46.88049368268205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.591246541341146 and perplexity of 98.6172840963617
Finished 18 epochs...
Completing Train Step...
At time: 522.1600286960602 and batch: 50, loss is 4.028083615303039 and perplexity is 56.15319694176533
At time: 524.6452324390411 and batch: 100, loss is 4.013494577407837 and perplexity is 55.33992269278881
At time: 527.1336193084717 and batch: 150, loss is 4.002884321212768 and perplexity is 54.75585596340908
At time: 529.6205041408539 and batch: 200, loss is 3.9742874574661253 and perplexity is 53.212187447287484
At time: 532.1037566661835 and batch: 250, loss is 3.970340919494629 and perplexity is 53.00259737859586
At time: 534.5858128070831 and batch: 300, loss is 3.9587099170684814 and perplexity is 52.38969528222339
At time: 537.0709404945374 and batch: 350, loss is 3.915882968902588 and perplexity is 50.193371136392386
At time: 539.5469353199005 and batch: 400, loss is 3.897363929748535 and perplexity is 49.27239227387832
At time: 542.0349366664886 and batch: 450, loss is 3.83682192325592 and perplexity is 46.37784762040025
At time: 544.5099678039551 and batch: 500, loss is 3.8367076921463013 and perplexity is 46.3725501299797
At time: 546.9929900169373 and batch: 550, loss is 3.84850989818573 and perplexity is 46.92309092867479
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.59103749593099 and perplexity of 98.59667076039597
Finished 19 epochs...
Completing Train Step...
At time: 551.0260992050171 and batch: 50, loss is 4.023623757362365 and perplexity is 55.903319284097265
At time: 553.5545794963837 and batch: 100, loss is 4.0085474872589115 and perplexity is 55.066827177431165
At time: 556.0361132621765 and batch: 150, loss is 3.9981290245056154 and perplexity is 54.49609373458141
At time: 558.5103220939636 and batch: 200, loss is 3.969936113357544 and perplexity is 52.98114594402466
At time: 560.9846832752228 and batch: 250, loss is 3.9665169811248777 and perplexity is 52.800305734673714
At time: 563.4636855125427 and batch: 300, loss is 3.955648579597473 and perplexity is 52.2295579871765
At time: 565.9435517787933 and batch: 350, loss is 3.9134416580200195 and perplexity is 50.0709829677744
At time: 568.4252562522888 and batch: 400, loss is 3.8959362840652467 and perplexity is 49.202098944666396
At time: 570.9045734405518 and batch: 450, loss is 3.8363882446289064 and perplexity is 46.357738899796715
At time: 573.3864855766296 and batch: 500, loss is 3.836682367324829 and perplexity is 46.37137576829676
At time: 575.8712215423584 and batch: 550, loss is 3.8481517601013184 and perplexity is 46.90628899166127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.591197204589844 and perplexity of 98.61241875996308
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 580.0207579135895 and batch: 50, loss is 4.022546887397766 and perplexity is 55.84315108111862
At time: 582.5040619373322 and batch: 100, loss is 4.009811887741089 and perplexity is 55.13649773672875
At time: 584.980507850647 and batch: 150, loss is 3.9995302152633667 and perplexity is 54.57250667950994
At time: 587.4553565979004 and batch: 200, loss is 3.9709233045578003 and perplexity is 53.03347428987168
At time: 589.9267289638519 and batch: 250, loss is 3.966464385986328 and perplexity is 52.797528768306236
At time: 592.4021520614624 and batch: 300, loss is 3.954378900527954 and perplexity is 52.163285292020014
At time: 594.874921798706 and batch: 350, loss is 3.9085965251922605 and perplexity is 49.828969172417
At time: 597.3492078781128 and batch: 400, loss is 3.8889178037643433 and perplexity is 48.85798397708475
At time: 599.8283996582031 and batch: 450, loss is 3.829479007720947 and perplexity is 46.03854625780079
At time: 602.3013200759888 and batch: 500, loss is 3.8298800373077393 and perplexity is 46.05701277954637
At time: 604.7708365917206 and batch: 550, loss is 3.841267957687378 and perplexity is 46.58450418846727
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.5876408894856775 and perplexity of 98.26234478131728
Finished 21 epochs...
Completing Train Step...
At time: 608.8098006248474 and batch: 50, loss is 4.019219212532043 and perplexity is 55.657632075503386
At time: 611.306637763977 and batch: 100, loss is 4.004745383262634 and perplexity is 54.8578548926788
At time: 613.7828693389893 and batch: 150, loss is 3.9944876432418823 and perplexity is 54.29801354153442
At time: 616.2823491096497 and batch: 200, loss is 3.9663552951812746 and perplexity is 52.79176935754316
At time: 618.7578370571136 and batch: 250, loss is 3.9628419351577757 and perplexity is 52.606618307116065
At time: 621.2311375141144 and batch: 300, loss is 3.9519623517990112 and perplexity is 52.03738235780761
At time: 623.705316066742 and batch: 350, loss is 3.907419729232788 and perplexity is 49.77036513209229
At time: 626.1824898719788 and batch: 400, loss is 3.8889177417755127 and perplexity is 48.857980948435554
At time: 628.6510367393494 and batch: 450, loss is 3.8303219509124755 and perplexity is 46.07737049793222
At time: 631.1236011981964 and batch: 500, loss is 3.831217021942139 and perplexity is 46.118631480388316
At time: 633.5932953357697 and batch: 550, loss is 3.841959652900696 and perplexity is 46.616737613596996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.587433878580729 and perplexity of 98.24200550969987
Finished 22 epochs...
Completing Train Step...
At time: 637.6349558830261 and batch: 50, loss is 4.016763281822205 and perplexity is 55.5211085024913
At time: 640.1071639060974 and batch: 100, loss is 4.001886911392212 and perplexity is 54.7012691621694
At time: 642.5782914161682 and batch: 150, loss is 3.991711525917053 and perplexity is 54.14748492466336
At time: 645.055349111557 and batch: 200, loss is 3.963843641281128 and perplexity is 52.65934108075719
At time: 647.534337759018 and batch: 250, loss is 3.9607232856750487 and perplexity is 52.49528130613774
At time: 650.010514497757 and batch: 300, loss is 3.95041446685791 and perplexity is 51.95689678455497
At time: 652.4910888671875 and batch: 350, loss is 3.906405363082886 and perplexity is 49.71990535510555
At time: 654.967670917511 and batch: 400, loss is 3.888443956375122 and perplexity is 48.83483823314283
At time: 657.4385368824005 and batch: 450, loss is 3.830360689163208 and perplexity is 46.07915548923716
At time: 659.916202545166 and batch: 500, loss is 3.8315135765075685 and perplexity is 46.13231019924821
At time: 662.3966863155365 and batch: 550, loss is 3.841984624862671 and perplexity is 46.617901739531284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.587482198079427 and perplexity of 98.2467526288455
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 666.5204026699066 and batch: 50, loss is 4.016158671379089 and perplexity is 55.48755000640821
At time: 669.052009344101 and batch: 100, loss is 4.002395596504211 and perplexity is 54.7291019618649
At time: 671.5220403671265 and batch: 150, loss is 3.992253079414368 and perplexity is 54.176816626121
At time: 673.9992699623108 and batch: 200, loss is 3.9643987083435057 and perplexity is 52.688578660175615
At time: 676.4745192527771 and batch: 250, loss is 3.960719265937805 and perplexity is 52.49507028932449
At time: 678.9945273399353 and batch: 300, loss is 3.950043797492981 and perplexity is 51.93764152350831
At time: 681.4679319858551 and batch: 350, loss is 3.903688917160034 and perplexity is 49.58502719846813
At time: 683.9468669891357 and batch: 400, loss is 3.8835857820510866 and perplexity is 48.59816544025592
At time: 686.4196209907532 and batch: 450, loss is 3.8252866649627686 and perplexity is 45.845940907720475
At time: 688.8990259170532 and batch: 500, loss is 3.8266575145721435 and perplexity is 45.90883189509911
At time: 691.3776705265045 and batch: 550, loss is 3.8380687189102174 and perplexity is 46.43570738142308
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.586079406738281 and perplexity of 98.10902955591368
Finished 24 epochs...
Completing Train Step...
At time: 695.4351096153259 and batch: 50, loss is 4.014649600982666 and perplexity is 55.4038785362715
At time: 697.9131813049316 and batch: 100, loss is 3.999749808311462 and perplexity is 54.584491738463555
At time: 700.3924295902252 and batch: 150, loss is 3.9894796133041384 and perplexity is 54.02676723588076
At time: 702.869713306427 and batch: 200, loss is 3.9618170642852784 and perplexity is 52.5527309348284
At time: 705.3476617336273 and batch: 250, loss is 3.9586974287033083 and perplexity is 52.38904102466271
At time: 707.8261249065399 and batch: 300, loss is 3.948685956001282 and perplexity is 51.86716629678832
At time: 710.2952320575714 and batch: 350, loss is 3.9031717491149904 and perplexity is 49.55939003682027
At time: 712.762663602829 and batch: 400, loss is 3.8838208436965944 and perplexity is 48.609590347719134
At time: 715.2296714782715 and batch: 450, loss is 3.8260147905349733 and perplexity is 45.87933466562636
At time: 717.7006638050079 and batch: 500, loss is 3.827898955345154 and perplexity is 45.96586038227533
At time: 720.1743533611298 and batch: 550, loss is 3.83855194568634 and perplexity is 46.45815178102867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.585914611816406 and perplexity of 98.09286301817104
Finished 25 epochs...
Completing Train Step...
At time: 724.226254940033 and batch: 50, loss is 4.013335704803467 and perplexity is 55.33113139351177
At time: 726.7352015972137 and batch: 100, loss is 3.998128218650818 and perplexity is 54.496049818660524
At time: 729.2197413444519 and batch: 150, loss is 3.9878694915771487 and perplexity is 53.93984755852397
At time: 731.7002635002136 and batch: 200, loss is 3.960346374511719 and perplexity is 52.47549897691587
At time: 734.1838495731354 and batch: 250, loss is 3.957542200088501 and perplexity is 52.32855464989103
At time: 736.6603028774261 and batch: 300, loss is 3.9479085063934325 and perplexity is 51.82685785961026
At time: 739.1381134986877 and batch: 350, loss is 3.9027207469940186 and perplexity is 49.53704368630411
At time: 741.6679697036743 and batch: 400, loss is 3.883757448196411 and perplexity is 48.60650881610399
At time: 744.1513166427612 and batch: 450, loss is 3.826224126815796 and perplexity is 45.888939880236755
At time: 746.6343216896057 and batch: 500, loss is 3.828235139846802 and perplexity is 45.981315989963086
At time: 749.1158916950226 and batch: 550, loss is 3.8386804628372193 and perplexity is 46.46412283401382
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.585915120442708 and perplexity of 98.0929129107939
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 753.2405049800873 and batch: 50, loss is 4.012982530593872 and perplexity is 55.31159331529163
At time: 755.7195506095886 and batch: 100, loss is 3.998191165924072 and perplexity is 54.49948030436846
At time: 758.2016253471375 and batch: 150, loss is 3.9878449487686156 and perplexity is 53.93852373941823
At time: 760.6880097389221 and batch: 200, loss is 3.9604001235961914 and perplexity is 52.478319562744396
At time: 763.1715452671051 and batch: 250, loss is 3.9572607564926146 and perplexity is 52.31382918559384
At time: 765.6521327495575 and batch: 300, loss is 3.947600507736206 and perplexity is 51.810897714958955
At time: 768.1362762451172 and batch: 350, loss is 3.901290707588196 and perplexity is 49.46625438959039
At time: 770.615261554718 and batch: 400, loss is 3.880945839881897 and perplexity is 48.470038292508086
At time: 773.0958971977234 and batch: 450, loss is 3.8230795192718507 and perplexity is 45.7448638237581
At time: 775.5771667957306 and batch: 500, loss is 3.825104818344116 and perplexity is 45.83760473636272
At time: 778.0538024902344 and batch: 550, loss is 3.8363159322738647 and perplexity is 46.35438678372363
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.585783386230469 and perplexity of 98.07999156929542
Finished 27 epochs...
Completing Train Step...
At time: 782.1401028633118 and batch: 50, loss is 4.012213006019592 and perplexity is 55.269046057672675
At time: 784.6499679088593 and batch: 100, loss is 3.996980080604553 and perplexity is 54.43351673566781
At time: 787.1318118572235 and batch: 150, loss is 3.986640944480896 and perplexity is 53.87362060522908
At time: 789.6144535541534 and batch: 200, loss is 3.9592538547515868 and perplexity is 52.4181997633217
At time: 792.0993151664734 and batch: 250, loss is 3.9563643741607666 and perplexity is 52.26695700423666
At time: 794.5811724662781 and batch: 300, loss is 3.946991605758667 and perplexity is 51.77935955967923
At time: 797.0637497901917 and batch: 350, loss is 3.9010423707962034 and perplexity is 49.45397162385784
At time: 799.5372285842896 and batch: 400, loss is 3.8811217164993286 and perplexity is 48.47856378858548
At time: 802.0128448009491 and batch: 450, loss is 3.8234965658187865 and perplexity is 45.7639455399612
At time: 804.5357627868652 and batch: 500, loss is 3.825667142868042 and perplexity is 45.86338759411121
At time: 807.0033328533173 and batch: 550, loss is 3.8366150569915773 and perplexity is 46.368254600585054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.585686747233073 and perplexity of 98.07051367521997
Finished 28 epochs...
Completing Train Step...
At time: 811.0779342651367 and batch: 50, loss is 4.011540784835815 and perplexity is 55.231905518832654
At time: 813.5521128177643 and batch: 100, loss is 3.996088743209839 and perplexity is 54.38501972348659
At time: 816.022943019867 and batch: 150, loss is 3.9857467651367187 and perplexity is 53.825469457576936
At time: 818.5005006790161 and batch: 200, loss is 3.9584234285354616 and perplexity is 52.374688385031256
At time: 820.9804017543793 and batch: 250, loss is 3.955714430809021 and perplexity is 52.2329974800947
At time: 823.4590539932251 and batch: 300, loss is 3.9465549182891846 and perplexity is 51.7567530985201
At time: 825.9390897750854 and batch: 350, loss is 3.900838360786438 and perplexity is 49.443883547693154
At time: 828.4164373874664 and batch: 400, loss is 3.881195616722107 and perplexity is 48.48214649762931
At time: 830.9018211364746 and batch: 450, loss is 3.823711280822754 and perplexity is 45.77377280070178
At time: 833.3855242729187 and batch: 500, loss is 3.8259354305267332 and perplexity is 45.8756938257197
At time: 835.8696689605713 and batch: 550, loss is 3.8367139005661013 and perplexity is 46.37283803113181
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.585664367675781 and perplexity of 98.0683189250994
Finished 29 epochs...
Completing Train Step...
At time: 839.9435729980469 and batch: 50, loss is 4.010872535705566 and perplexity is 55.195009175355445
At time: 842.476804971695 and batch: 100, loss is 3.995308918952942 and perplexity is 54.34262549806548
At time: 844.9585208892822 and batch: 150, loss is 3.9849919509887695 and perplexity is 53.78485656123297
At time: 847.4394044876099 and batch: 200, loss is 3.957738242149353 and perplexity is 52.33881425321004
At time: 849.9196348190308 and batch: 250, loss is 3.9551782035827636 and perplexity is 52.20499623292336
At time: 852.3984634876251 and batch: 300, loss is 3.946197428703308 and perplexity is 51.73825390512011
At time: 854.8789229393005 and batch: 350, loss is 3.900655198097229 and perplexity is 49.434828102352796
At time: 857.3560791015625 and batch: 400, loss is 3.8811500787734987 and perplexity is 48.479938770401745
At time: 859.8308999538422 and batch: 450, loss is 3.82380567073822 and perplexity is 45.77809358716314
At time: 862.3069906234741 and batch: 500, loss is 3.826125407218933 and perplexity is 45.88440996619063
At time: 864.7788321971893 and batch: 550, loss is 3.836771230697632 and perplexity is 46.37549666824486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.58566640218099 and perplexity of 98.06851844580804
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 868.8382730484009 and batch: 50, loss is 4.010699539184571 and perplexity is 55.18546145667666
At time: 871.3209805488586 and batch: 100, loss is 3.995345115661621 and perplexity is 54.34459255784983
At time: 874.0967161655426 and batch: 150, loss is 3.9849782848358153 and perplexity is 53.7841215341791
At time: 876.5671513080597 and batch: 200, loss is 3.9577105903625487 and perplexity is 52.337367011486215
At time: 879.0445992946625 and batch: 250, loss is 3.9549160528182985 and perplexity is 52.19131244693784
At time: 881.5275957584381 and batch: 300, loss is 3.945925221443176 and perplexity is 51.724172293426
At time: 884.0112953186035 and batch: 350, loss is 3.899828066825867 and perplexity is 49.393955915796965
At time: 886.4958341121674 and batch: 400, loss is 3.8796135807037353 and perplexity is 48.405506635118314
At time: 888.9765462875366 and batch: 450, loss is 3.8220264863967897 and perplexity is 45.696718332128185
At time: 891.4606850147247 and batch: 500, loss is 3.8242879486083985 and perplexity is 45.80017667330303
At time: 893.9405777454376 and batch: 550, loss is 3.83544349193573 and perplexity is 46.31396298308522
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.585866292317708 and perplexity of 98.08812333471462
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 898.0078718662262 and batch: 50, loss is 4.010538158416748 and perplexity is 55.176556303113706
At time: 900.4856328964233 and batch: 100, loss is 3.995187463760376 and perplexity is 54.336025704818766
At time: 902.9610598087311 and batch: 150, loss is 3.9847921037673952 and perplexity is 53.774108881079975
At time: 905.4390251636505 and batch: 200, loss is 3.9575220727920533 and perplexity is 52.3275014281582
At time: 907.9204590320587 and batch: 250, loss is 3.9546699142456054 and perplexity is 52.178467732639845
At time: 910.403012752533 and batch: 300, loss is 3.9457237911224365 and perplexity is 51.71375452607328
At time: 912.8845973014832 and batch: 350, loss is 3.8993986225128174 and perplexity is 49.372748516354385
At time: 915.3702263832092 and batch: 400, loss is 3.878880362510681 and perplexity is 48.37002784544595
At time: 917.8480761051178 and batch: 450, loss is 3.8211841917037965 and perplexity is 45.65824443424447
At time: 920.3247001171112 and batch: 500, loss is 3.823407154083252 and perplexity is 45.759853889089094
At time: 922.8064823150635 and batch: 550, loss is 3.8347915840148925 and perplexity is 46.28378038297814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.585960388183594 and perplexity of 98.09735345586422
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 926.9677834510803 and batch: 50, loss is 4.010443067550659 and perplexity is 55.171309766039876
At time: 929.4509508609772 and batch: 100, loss is 3.9950807666778565 and perplexity is 54.33022851867729
At time: 931.9666264057159 and batch: 150, loss is 3.984670991897583 and perplexity is 53.76759659257159
At time: 934.4516496658325 and batch: 200, loss is 3.9574085664749146 and perplexity is 52.32156226325877
At time: 936.9321336746216 and batch: 250, loss is 3.9545177316665647 and perplexity is 52.17052768303384
At time: 939.4173014163971 and batch: 300, loss is 3.9456064748764037 and perplexity is 51.707688018380935
At time: 941.8995862007141 and batch: 350, loss is 3.8991788959503175 and perplexity is 49.3619012038067
At time: 944.382045507431 and batch: 400, loss is 3.8785139894485474 and perplexity is 48.352309616167865
At time: 946.8682827949524 and batch: 450, loss is 3.8207705211639404 and perplexity is 45.63936086967694
At time: 949.3568387031555 and batch: 500, loss is 3.822972445487976 and perplexity is 45.739966010314035
At time: 951.8415875434875 and batch: 550, loss is 3.834468421936035 and perplexity is 46.268825636825774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.585990397135417 and perplexity of 98.10029729878865
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 955.9431965351105 and batch: 50, loss is 4.010393261909485 and perplexity is 55.16856199201047
At time: 958.4264631271362 and batch: 100, loss is 3.9950201320648193 and perplexity is 54.326934326166885
At time: 960.9136147499084 and batch: 150, loss is 3.984601502418518 and perplexity is 53.76386044010693
At time: 963.4190628528595 and batch: 200, loss is 3.9573433589935303 and perplexity is 52.31815061719512
At time: 965.9200146198273 and batch: 250, loss is 3.9544307565689087 and perplexity is 52.16599034361447
At time: 968.4217877388 and batch: 300, loss is 3.9455432367324828 and perplexity is 51.704418223553176
At time: 970.921957731247 and batch: 350, loss is 3.899069347381592 and perplexity is 49.35649397436278
At time: 973.4223053455353 and batch: 400, loss is 3.8783297729492188 and perplexity is 48.34340314334073
At time: 975.925862789154 and batch: 450, loss is 3.820563349723816 and perplexity is 45.62990667691239
At time: 978.4724204540253 and batch: 500, loss is 3.8227543354034426 and perplexity is 45.72999075035292
At time: 980.9641184806824 and batch: 550, loss is 3.834306769371033 and perplexity is 46.261346766987465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.5860036214192705 and perplexity of 98.10159461354426
Annealing...
Model not improving. Stopping early with 98.0683189250994loss at 33 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f690564a9e8>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'tune_wordvecs': True, 'anneal': 5.219921432741684, 'wordvec_source': '', 'data': 'wikitext', 'lr': 16.982241097578665, 'num_layers': 1, 'batch_size': 80, 'dropout': 0.7890174429212653, 'wordvec_dim': 200, 'seq_len': 50}, 'best_accuracy': -154.82513921940262}, {'params': {'tune_wordvecs': True, 'anneal': 4.812500434828028, 'wordvec_source': '', 'data': 'wikitext', 'lr': 1.2796573659970023, 'num_layers': 1, 'batch_size': 80, 'dropout': 0.8659211716877756, 'wordvec_dim': 200, 'seq_len': 50}, 'best_accuracy': -98.38246725338247}, {'params': {'tune_wordvecs': True, 'anneal': 3.351606636242914, 'wordvec_source': '', 'data': 'wikitext', 'lr': 13.95553065068248, 'num_layers': 1, 'batch_size': 80, 'dropout': 0.11615909844626904, 'wordvec_dim': 200, 'seq_len': 50}, 'best_accuracy': -125.6687139802131}, {'params': {'tune_wordvecs': True, 'anneal': 4.062539135235045, 'wordvec_source': '', 'data': 'wikitext', 'lr': 7.877676270926454, 'num_layers': 1, 'batch_size': 80, 'dropout': 0.040193015000454024, 'wordvec_dim': 200, 'seq_len': 50}, 'best_accuracy': -97.8288380381515}, {'params': {'tune_wordvecs': True, 'anneal': 3.0553777196419674, 'wordvec_source': '', 'data': 'wikitext', 'lr': 21.4721337367448, 'num_layers': 1, 'batch_size': 80, 'dropout': 0.8539785317829272, 'wordvec_dim': 200, 'seq_len': 50}, 'best_accuracy': -167.90585275252798}, {'params': {'tune_wordvecs': True, 'anneal': 2.0, 'wordvec_source': '', 'data': 'wikitext', 'lr': 4.744550879574482, 'num_layers': 1, 'batch_size': 80, 'dropout': 0.0, 'wordvec_dim': 200, 'seq_len': 50}, 'best_accuracy': -98.0683189250994}]
