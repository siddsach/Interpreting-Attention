Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'domain': [0, 30], 'type': 'continuous', 'name': 'lr'}, {'domain': [0, 1], 'type': 'continuous', 'name': 'dropout'}, {'domain': [2, 8], 'type': 'continuous', 'name': 'anneal'}]
SETTINGS FOR THIS RUN
{'batch_size': 50, 'seq_len': 50, 'wordvec_source': 'glove', 'lr': 3.607194594002684, 'anneal': 5.151391374298254, 'dropout': 0.9606034870423327, 'wordvec_dim': 200, 'tune_wordvecs': True, 'data': 'wikitext', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.3067984580993652 and batch: 50, loss is 9.027154922485352 and perplexity is 8326.137336258225
At time: 3.846794366836548 and batch: 100, loss is 7.83500599861145 and perplexity is 2527.5506711121902
At time: 5.359730243682861 and batch: 150, loss is 7.493815717697143 and perplexity is 1796.8954731918545
At time: 6.870514631271362 and batch: 200, loss is 7.37094425201416 and perplexity is 1589.1336174412236
At time: 8.3829026222229 and batch: 250, loss is 7.28165117263794 and perplexity is 1453.3858358945029
At time: 9.896596193313599 and batch: 300, loss is 7.152813215255737 and perplexity is 1277.6953359671263
At time: 11.41073226928711 and batch: 350, loss is 7.0879452419281 and perplexity is 1197.4448116617802
At time: 12.92796802520752 and batch: 400, loss is 7.069020595550537 and perplexity is 1174.9966735471348
At time: 14.446502208709717 and batch: 450, loss is 7.030615825653076 and perplexity is 1130.7267263459787
At time: 15.965192794799805 and batch: 500, loss is 6.9950572204589845 and perplexity is 1091.226116405637
At time: 17.4818434715271 and batch: 550, loss is 6.915459690093994 and perplexity is 1007.734166453856
At time: 18.99936032295227 and batch: 600, loss is 6.910835447311402 and perplexity is 1003.0849169219696
At time: 20.526150941848755 and batch: 650, loss is 6.915009727478028 and perplexity is 1007.2808257529526
At time: 22.053303718566895 and batch: 700, loss is 6.8775208377838135 and perplexity is 970.2180478017729
At time: 23.581268548965454 and batch: 750, loss is 6.8368680381774904 and perplexity is 931.5669289428729
At time: 25.11019015312195 and batch: 800, loss is 6.856239624023438 and perplexity is 949.7887809416742
At time: 26.63778805732727 and batch: 850, loss is 6.849811687469482 and perplexity is 943.703178804253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.199078877766927 and perplexity of 492.295367972456
Finished 1 epochs...
Completing Train Step...
At time: 30.685578107833862 and batch: 50, loss is 6.345097274780273 and perplexity is 569.6928037624986
At time: 32.21784973144531 and batch: 100, loss is 5.961572437286377 and perplexity is 388.2200962954331
At time: 33.74793529510498 and batch: 150, loss is 5.779385004043579 and perplexity is 323.5601410378375
At time: 35.281243562698364 and batch: 200, loss is 5.695866928100586 and perplexity is 297.6347096661254
At time: 36.81298279762268 and batch: 250, loss is 5.661155967712403 and perplexity is 287.4807690422726
At time: 38.34630274772644 and batch: 300, loss is 5.5565330600738525 and perplexity is 258.9236058477011
At time: 39.8780152797699 and batch: 350, loss is 5.471899967193604 and perplexity is 237.911788209618
At time: 41.40778708457947 and batch: 400, loss is 5.442396364212036 and perplexity is 230.99506905507496
At time: 42.935555934906006 and batch: 450, loss is 5.40098575592041 and perplexity is 221.6247764970709
At time: 44.46383023262024 and batch: 500, loss is 5.3685486793518065 and perplexity is 214.55125896470096
At time: 46.04386115074158 and batch: 550, loss is 5.32383017539978 and perplexity is 205.16820920643252
At time: 47.57355284690857 and batch: 600, loss is 5.326610717773438 and perplexity is 205.7394819618479
At time: 49.10342049598694 and batch: 650, loss is 5.309094095230103 and perplexity is 202.1670013567125
At time: 50.63505935668945 and batch: 700, loss is 5.237276973724366 and perplexity is 188.15704760725674
At time: 52.166280031204224 and batch: 750, loss is 5.212014636993408 and perplexity is 183.46329807390143
At time: 53.701902627944946 and batch: 800, loss is 5.188603754043579 and perplexity is 179.2181455463272
At time: 55.23478412628174 and batch: 850, loss is 5.16953366279602 and perplexity is 175.83282097577535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.863481521606445 and perplexity of 129.4741855137249
Finished 2 epochs...
Completing Train Step...
At time: 59.24750328063965 and batch: 50, loss is 5.145791416168213 and perplexity is 171.70732286321584
At time: 60.78629517555237 and batch: 100, loss is 5.046508140563965 and perplexity is 155.47860606273812
At time: 62.321871757507324 and batch: 150, loss is 5.027031030654907 and perplexity is 152.47963263785235
At time: 63.85880255699158 and batch: 200, loss is 5.036627235412598 and perplexity is 153.94990163246098
At time: 65.39679336547852 and batch: 250, loss is 5.043470411300659 and perplexity is 155.00702078811892
At time: 66.9349913597107 and batch: 300, loss is 4.995502901077271 and perplexity is 147.74722894386554
At time: 68.4722969532013 and batch: 350, loss is 4.946711940765381 and perplexity is 140.71153458673368
At time: 70.00948524475098 and batch: 400, loss is 4.948779401779174 and perplexity is 141.00275113438101
At time: 71.54663372039795 and batch: 450, loss is 4.94236629486084 and perplexity is 140.1013788034395
At time: 73.0843403339386 and batch: 500, loss is 4.933421659469604 and perplexity is 138.85381089007316
At time: 74.62291526794434 and batch: 550, loss is 4.925701274871826 and perplexity is 137.7859335830627
At time: 76.16013813018799 and batch: 600, loss is 4.946331558227539 and perplexity is 140.65802055465082
At time: 77.69735097885132 and batch: 650, loss is 4.9332371520996094 and perplexity is 138.82819370196475
At time: 79.29785537719727 and batch: 700, loss is 4.881702909469604 and perplexity is 131.85500995616042
At time: 80.84722685813904 and batch: 750, loss is 4.8681596088409425 and perplexity is 130.08129599888682
At time: 82.38689875602722 and batch: 800, loss is 4.849348258972168 and perplexity is 127.65716331707272
At time: 83.9254822731018 and batch: 850, loss is 4.846731805801392 and perplexity is 127.3235909059341
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.671720186869304 and perplexity of 106.88144044012031
Finished 3 epochs...
Completing Train Step...
At time: 87.87910342216492 and batch: 50, loss is 4.842056465148926 and perplexity is 126.72969915030131
At time: 89.43712949752808 and batch: 100, loss is 4.766112546920777 and perplexity is 117.4617263019149
At time: 90.96262454986572 and batch: 150, loss is 4.764893312454223 and perplexity is 117.31860018658328
At time: 92.48977398872375 and batch: 200, loss is 4.786167631149292 and perplexity is 119.84121175395127
At time: 94.01674628257751 and batch: 250, loss is 4.798503761291504 and perplexity is 121.32874487527152
At time: 95.54367780685425 and batch: 300, loss is 4.76153606414795 and perplexity is 116.925392931332
At time: 97.07064414024353 and batch: 350, loss is 4.715312862396241 and perplexity is 111.64373483667899
At time: 98.59673953056335 and batch: 400, loss is 4.721038188934326 and perplexity is 112.28476497694155
At time: 100.1237940788269 and batch: 450, loss is 4.726153612136841 and perplexity is 112.86062068609402
At time: 101.65220093727112 and batch: 500, loss is 4.718280658721924 and perplexity is 111.97556285814768
At time: 103.18108820915222 and batch: 550, loss is 4.722129030227661 and perplexity is 112.40731666525953
At time: 104.70729160308838 and batch: 600, loss is 4.747947959899903 and perplexity is 115.3473441291396
At time: 106.24165844917297 and batch: 650, loss is 4.734795818328857 and perplexity is 113.840212260258
At time: 107.79719662666321 and batch: 700, loss is 4.692473497390747 and perplexity is 109.12276113054803
At time: 109.3572006225586 and batch: 750, loss is 4.683696060180664 and perplexity is 108.16913426903031
At time: 110.9135057926178 and batch: 800, loss is 4.665327301025391 and perplexity is 106.20033901404517
At time: 112.4620144367218 and batch: 850, loss is 4.668961534500122 and perplexity is 106.58699802009794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.575385093688965 and perplexity of 97.06541121787201
Finished 4 epochs...
Completing Train Step...
At time: 116.52847790718079 and batch: 50, loss is 4.661997861862183 and perplexity is 105.84733941761449
At time: 118.09963345527649 and batch: 100, loss is 4.5961603164672855 and perplexity is 99.10305977275131
At time: 119.63032174110413 and batch: 150, loss is 4.601072216033936 and perplexity is 99.5910415266679
At time: 121.1594352722168 and batch: 200, loss is 4.62901460647583 and perplexity is 102.41309716983731
At time: 122.69009852409363 and batch: 250, loss is 4.637262582778931 and perplexity is 103.2612911016617
At time: 124.2197916507721 and batch: 300, loss is 4.609286079406738 and perplexity is 100.4124375340867
At time: 125.74702215194702 and batch: 350, loss is 4.561065740585327 and perplexity is 95.685401325098
At time: 127.2748806476593 and batch: 400, loss is 4.569884281158448 and perplexity is 96.53293844668197
At time: 128.80429792404175 and batch: 450, loss is 4.584931669235229 and perplexity is 97.99649073816911
At time: 130.33468556404114 and batch: 500, loss is 4.572843532562256 and perplexity is 96.81902677516096
At time: 131.8633255958557 and batch: 550, loss is 4.583720016479492 and perplexity is 97.87782492551091
At time: 133.39071536064148 and batch: 600, loss is 4.610521955490112 and perplexity is 100.53661158017007
At time: 134.91967797279358 and batch: 650, loss is 4.597105083465576 and perplexity is 99.19673331592267
At time: 136.4505832195282 and batch: 700, loss is 4.561645212173462 and perplexity is 95.740864364641
At time: 137.98139667510986 and batch: 750, loss is 4.55310173034668 and perplexity is 94.92638821507092
At time: 139.51009607315063 and batch: 800, loss is 4.5352106857299805 and perplexity is 93.24315823796735
At time: 141.0382719039917 and batch: 850, loss is 4.54115740776062 and perplexity is 93.79930135660561
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5217084884643555 and perplexity of 91.99263214525347
Finished 5 epochs...
Completing Train Step...
At time: 145.04533696174622 and batch: 50, loss is 4.529987506866455 and perplexity is 92.75740224425945
At time: 146.57456827163696 and batch: 100, loss is 4.471330032348633 and perplexity is 87.47298758159859
At time: 148.1068377494812 and batch: 150, loss is 4.478425283432006 and perplexity is 88.09583741649857
At time: 149.63847613334656 and batch: 200, loss is 4.509962158203125 and perplexity is 90.91837793064126
At time: 151.1825053691864 and batch: 250, loss is 4.513512125015259 and perplexity is 91.24170872213938
At time: 152.72346091270447 and batch: 300, loss is 4.4904979228973385 and perplexity is 89.16583253685522
At time: 154.25681018829346 and batch: 350, loss is 4.445933780670166 and perplexity is 85.27947299277281
At time: 155.79188776016235 and batch: 400, loss is 4.457215023040772 and perplexity is 86.24697846782912
At time: 157.3565707206726 and batch: 450, loss is 4.477688293457032 and perplexity is 88.0309355864223
At time: 158.89423871040344 and batch: 500, loss is 4.462164258956909 and perplexity is 86.67489316323194
At time: 160.42584228515625 and batch: 550, loss is 4.478113422393799 and perplexity is 88.06836804071673
At time: 161.95928812026978 and batch: 600, loss is 4.505308437347412 and perplexity is 90.49625216866427
At time: 163.49595975875854 and batch: 650, loss is 4.49073896408081 and perplexity is 89.1873277651687
At time: 165.03236079216003 and batch: 700, loss is 4.459435148239136 and perplexity is 86.43867026917086
At time: 166.5678834915161 and batch: 750, loss is 4.4517653369903565 and perplexity is 85.77823791689964
At time: 168.10216188430786 and batch: 800, loss is 4.432802677154541 and perplexity is 84.16697952377385
At time: 169.63468480110168 and batch: 850, loss is 4.439672060012818 and perplexity is 84.74714513858892
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.48597526550293 and perplexity of 88.76347656992823
Finished 6 epochs...
Completing Train Step...
At time: 173.6302936077118 and batch: 50, loss is 4.428421354293823 and perplexity is 83.7990234680147
At time: 175.16134691238403 and batch: 100, loss is 4.3750020790100095 and perplexity is 79.4400047086546
At time: 176.69405007362366 and batch: 150, loss is 4.383231058120727 and perplexity is 80.09641192448397
At time: 178.22734880447388 and batch: 200, loss is 4.417790002822876 and perplexity is 82.91284557763245
At time: 179.76101636886597 and batch: 250, loss is 4.416453504562378 and perplexity is 82.80210672137312
At time: 181.29279613494873 and batch: 300, loss is 4.396769800186157 and perplexity is 81.18819056394847
At time: 182.82662796974182 and batch: 350, loss is 4.356987476348877 and perplexity is 78.0217377018852
At time: 184.3639316558838 and batch: 400, loss is 4.369336671829224 and perplexity is 78.9912172178607
At time: 185.89770913124084 and batch: 450, loss is 4.393899641036987 and perplexity is 80.95550162288066
At time: 187.4346318244934 and batch: 500, loss is 4.37526967048645 and perplexity is 79.46126502121535
At time: 188.96689748764038 and batch: 550, loss is 4.3947004795074465 and perplexity is 81.02035986999894
At time: 190.50210666656494 and batch: 600, loss is 4.422473669052124 and perplexity is 83.30209251222966
At time: 192.04037427902222 and batch: 650, loss is 4.407082366943359 and perplexity is 82.02978122043307
At time: 193.57618498802185 and batch: 700, loss is 4.378499841690063 and perplexity is 79.7183535076639
At time: 195.11010336875916 and batch: 750, loss is 4.371428699493408 and perplexity is 79.15664200579148
At time: 196.6927170753479 and batch: 800, loss is 4.349572315216064 and perplexity is 77.44533365053333
At time: 198.22740960121155 and batch: 850, loss is 4.358303155899048 and perplexity is 78.1244568645819
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.463255564371745 and perplexity of 86.76953357487552
Finished 7 epochs...
Completing Train Step...
At time: 202.2282965183258 and batch: 50, loss is 4.346453742980957 and perplexity is 77.20419098911238
At time: 203.7611129283905 and batch: 100, loss is 4.296934633255005 and perplexity is 73.47422270841453
At time: 205.30034732818604 and batch: 150, loss is 4.306196775436401 and perplexity is 74.15791273527395
At time: 206.84170198440552 and batch: 200, loss is 4.3437849807739255 and perplexity is 76.99842605294525
At time: 208.3808617591858 and batch: 250, loss is 4.338394279479981 and perplexity is 76.58446730442846
At time: 209.91549921035767 and batch: 300, loss is 4.321038103103637 and perplexity is 75.2667223682495
At time: 211.44854950904846 and batch: 350, loss is 4.284519758224487 and perplexity is 72.56768831471206
At time: 212.983638048172 and batch: 400, loss is 4.296638822555542 and perplexity is 73.45249146152875
At time: 214.51954412460327 and batch: 450, loss is 4.3259946155548095 and perplexity is 75.64070888359251
At time: 216.05285739898682 and batch: 500, loss is 4.304270496368408 and perplexity is 74.01520139530756
At time: 217.58711123466492 and batch: 550, loss is 4.324875440597534 and perplexity is 75.55610105077459
At time: 219.12335658073425 and batch: 600, loss is 4.354180469512939 and perplexity is 77.80303724124857
At time: 220.6589457988739 and batch: 650, loss is 4.337170858383178 and perplexity is 76.49082954232709
At time: 222.19462895393372 and batch: 700, loss is 4.3111214923858645 and perplexity is 74.52402021298012
At time: 223.72684168815613 and batch: 750, loss is 4.304184103012085 and perplexity is 74.00880724984991
At time: 225.2626941204071 and batch: 800, loss is 4.2828693866729735 and perplexity is 72.44802343927262
At time: 226.80395126342773 and batch: 850, loss is 4.291178274154663 and perplexity is 73.0524936742903
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.448680241902669 and perplexity of 85.51401168756406
Finished 8 epochs...
Completing Train Step...
At time: 230.7885479927063 and batch: 50, loss is 4.2790032005310055 and perplexity is 72.16846665254648
At time: 232.34513878822327 and batch: 100, loss is 4.230430459976196 and perplexity is 68.74681855939282
At time: 233.8794264793396 and batch: 150, loss is 4.241945772171021 and perplexity is 69.54303518065502
At time: 235.44404578208923 and batch: 200, loss is 4.283097515106201 and perplexity is 72.4645527786847
At time: 236.97846722602844 and batch: 250, loss is 4.2764268970489505 and perplexity is 71.98277807838157
At time: 238.51579427719116 and batch: 300, loss is 4.2585280227661135 and perplexity is 70.70582944567076
At time: 240.0559709072113 and batch: 350, loss is 4.223124303817749 and perplexity is 68.24637395694754
At time: 241.5938892364502 and batch: 400, loss is 4.2363158702850345 and perplexity is 69.15261476128518
At time: 243.13209795951843 and batch: 450, loss is 4.268564758300781 and perplexity is 71.41905841453134
At time: 244.66979789733887 and batch: 500, loss is 4.2457294750213626 and perplexity is 69.80666379273717
At time: 246.2044415473938 and batch: 550, loss is 4.266857452392578 and perplexity is 71.29722826440299
At time: 247.7391710281372 and batch: 600, loss is 4.2969725131988525 and perplexity is 73.47700596055934
At time: 249.27368330955505 and batch: 650, loss is 4.278289813995361 and perplexity is 72.11700099977298
At time: 250.80934286117554 and batch: 700, loss is 4.252377223968506 and perplexity is 70.27226685993722
At time: 252.3422532081604 and batch: 750, loss is 4.246466093063354 and perplexity is 69.8581035841514
At time: 253.8758111000061 and batch: 800, loss is 4.225112466812134 and perplexity is 68.38219384344684
At time: 255.4103488922119 and batch: 850, loss is 4.233755645751953 and perplexity is 68.97579498619375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.440909703572591 and perplexity of 84.85209682997458
Finished 9 epochs...
Completing Train Step...
At time: 259.556104183197 and batch: 50, loss is 4.2208232498168945 and perplexity is 68.08951590374717
At time: 261.1183593273163 and batch: 100, loss is 4.174060125350952 and perplexity is 64.97873908248165
At time: 262.65678095817566 and batch: 150, loss is 4.187809281349182 and perplexity is 65.87831191512602
At time: 264.1951837539673 and batch: 200, loss is 4.2289539289474485 and perplexity is 68.64538665077042
At time: 265.7337017059326 and batch: 250, loss is 4.222111053466797 and perplexity is 68.17725831622137
At time: 267.27136635780334 and batch: 300, loss is 4.2042686557769775 and perplexity is 66.97160045847428
At time: 268.8110439777374 and batch: 350, loss is 4.169966940879822 and perplexity is 64.71331270685798
At time: 270.3496196269989 and batch: 400, loss is 4.1822976922988895 and perplexity is 65.51621650969115
At time: 271.88782382011414 and batch: 450, loss is 4.218788795471191 and perplexity is 67.95113170811511
At time: 273.45002794265747 and batch: 500, loss is 4.194590320587158 and perplexity is 66.32655338705506
At time: 274.9949083328247 and batch: 550, loss is 4.215569305419922 and perplexity is 67.73271549867408
At time: 276.5343005657196 and batch: 600, loss is 4.246233253479004 and perplexity is 69.84183974585552
At time: 278.07355546951294 and batch: 650, loss is 4.226275711059571 and perplexity is 68.46178532025124
At time: 279.6104817390442 and batch: 700, loss is 4.200811386108398 and perplexity is 66.74046136087692
At time: 281.14773988723755 and batch: 750, loss is 4.197336053848266 and perplexity is 66.50891865940977
At time: 282.6881494522095 and batch: 800, loss is 4.176943435668945 and perplexity is 65.16636331078091
At time: 284.22834491729736 and batch: 850, loss is 4.185689325332642 and perplexity is 65.73880072246759
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.436288515726726 and perplexity of 84.46088398164555
Finished 10 epochs...
Completing Train Step...
At time: 288.2083430290222 and batch: 50, loss is 4.170659942626953 and perplexity is 64.75817468854692
At time: 289.77199244499207 and batch: 100, loss is 4.12491093158722 and perplexity is 61.86229902819277
At time: 291.30641508102417 and batch: 150, loss is 4.138860774040222 and perplexity is 62.731315583062525
At time: 292.8438172340393 and batch: 200, loss is 4.180713610649109 and perplexity is 65.41251563036994
At time: 294.37648010253906 and batch: 250, loss is 4.173436689376831 and perplexity is 64.93824162408252
At time: 295.9136390686035 and batch: 300, loss is 4.15584698677063 and perplexity is 63.8059844922463
At time: 297.4513256549835 and batch: 350, loss is 4.122390503883362 and perplexity is 61.70657590277415
At time: 298.9903054237366 and batch: 400, loss is 4.136206316947937 and perplexity is 62.56501880897625
At time: 300.5270972251892 and batch: 450, loss is 4.174376525878906 and perplexity is 64.99930164266408
At time: 302.06253004074097 and batch: 500, loss is 4.152374143600464 and perplexity is 63.58478064025161
At time: 303.5982141494751 and batch: 550, loss is 4.173516640663147 and perplexity is 64.94343372758638
At time: 305.1356670856476 and batch: 600, loss is 4.202604475021363 and perplexity is 66.86024029723818
At time: 306.674343585968 and batch: 650, loss is 4.1818803119659425 and perplexity is 65.48887703530725
At time: 308.2107434272766 and batch: 700, loss is 4.157697439193726 and perplexity is 63.92416373967316
At time: 309.74850487709045 and batch: 750, loss is 4.154409351348877 and perplexity is 63.71432065419822
At time: 311.2832729816437 and batch: 800, loss is 4.133960204124451 and perplexity is 62.42464842080653
At time: 312.8627440929413 and batch: 850, loss is 4.143645467758179 and perplexity is 63.03218492448524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.436280568440755 and perplexity of 84.46021274951445
Finished 11 epochs...
Completing Train Step...
At time: 316.9102644920349 and batch: 50, loss is 4.12840874671936 and perplexity is 62.079060789124924
At time: 318.44979977607727 and batch: 100, loss is 4.082964024543762 and perplexity is 59.32103854058649
At time: 319.9874403476715 and batch: 150, loss is 4.097346630096435 and perplexity is 60.18039471556764
At time: 321.5230700969696 and batch: 200, loss is 4.138506708145141 and perplexity is 62.70910849527919
At time: 323.0602958202362 and batch: 250, loss is 4.131372036933899 and perplexity is 62.26329189279
At time: 324.59703373908997 and batch: 300, loss is 4.113288369178772 and perplexity is 61.14746275847705
At time: 326.1357762813568 and batch: 350, loss is 4.080557851791382 and perplexity is 59.17847346089603
At time: 327.67258977890015 and batch: 400, loss is 4.095093932151794 and perplexity is 60.044979046649274
At time: 329.20445370674133 and batch: 450, loss is 4.1353966331481935 and perplexity is 62.51438142971524
At time: 330.73937916755676 and batch: 500, loss is 4.112828350067138 and perplexity is 61.1193402259274
At time: 332.27559447288513 and batch: 550, loss is 4.132358980178833 and perplexity is 62.3247725621061
At time: 333.8266611099243 and batch: 600, loss is 4.161056923866272 and perplexity is 64.13927712069858
At time: 335.3647005558014 and batch: 650, loss is 4.140271329879761 and perplexity is 62.819864043172075
At time: 336.9025340080261 and batch: 700, loss is 4.116732077598572 and perplexity is 61.35839978517454
At time: 338.44084119796753 and batch: 750, loss is 4.113623776435852 and perplexity is 61.16797550110735
At time: 339.9784119129181 and batch: 800, loss is 4.094597406387329 and perplexity is 60.015172567980535
At time: 341.5168385505676 and batch: 850, loss is 4.106647925376892 and perplexity is 60.74276165299909
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.434507052103679 and perplexity of 84.31055393277606
Finished 12 epochs...
Completing Train Step...
At time: 345.52221727371216 and batch: 50, loss is 4.0883963012695315 and perplexity is 59.644163695620314
At time: 347.0649492740631 and batch: 100, loss is 4.044592123031617 and perplexity is 57.087896446690564
At time: 348.60731267929077 and batch: 150, loss is 4.057422547340393 and perplexity is 57.825077440615495
At time: 350.1454482078552 and batch: 200, loss is 4.100349617004395 and perplexity is 60.3613872761444
At time: 351.71735405921936 and batch: 250, loss is 4.092518439292908 and perplexity is 59.89053260512863
At time: 353.25913763046265 and batch: 300, loss is 4.074618558883667 and perplexity is 58.828036876084525
At time: 354.80053305625916 and batch: 350, loss is 4.043452706336975 and perplexity is 57.02288658810969
At time: 356.33949184417725 and batch: 400, loss is 4.058999786376953 and perplexity is 57.916353373106695
At time: 357.8827528953552 and batch: 450, loss is 4.1002867603302 and perplexity is 60.357593279330715
At time: 359.4309585094452 and batch: 500, loss is 4.077182822227478 and perplexity is 58.97908103036303
At time: 360.9743323326111 and batch: 550, loss is 4.096240510940552 and perplexity is 60.11386482993786
At time: 362.51816511154175 and batch: 600, loss is 4.124279837608338 and perplexity is 61.82327042041134
At time: 364.0584852695465 and batch: 650, loss is 4.103996067047119 and perplexity is 60.581893848123606
At time: 365.5993142127991 and batch: 700, loss is 4.081666622161865 and perplexity is 59.244125188459186
At time: 367.1396746635437 and batch: 750, loss is 4.0777072763443 and perplexity is 59.01002096479459
At time: 368.6826298236847 and batch: 800, loss is 4.0592141485214235 and perplexity is 57.92876977757167
At time: 370.2263512611389 and batch: 850, loss is 4.077463641166687 and perplexity is 58.99564579907502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.4341433842976885 and perplexity of 84.27989847314525
Finished 13 epochs...
Completing Train Step...
At time: 374.2583701610565 and batch: 50, loss is 4.052495188713074 and perplexity is 57.540853358457625
At time: 375.8023087978363 and batch: 100, loss is 4.011101937294006 and perplexity is 55.20767245056642
At time: 377.34434175491333 and batch: 150, loss is 4.021834864616394 and perplexity is 55.80340363758714
At time: 378.8929669857025 and batch: 200, loss is 4.066019296646118 and perplexity is 58.324328026301274
At time: 380.43228912353516 and batch: 250, loss is 4.057246179580688 and perplexity is 57.81487986054113
At time: 381.97189712524414 and batch: 300, loss is 4.040260767936706 and perplexity is 56.841163225894206
At time: 383.5110659599304 and batch: 350, loss is 4.009760293960571 and perplexity is 55.1336531097492
At time: 385.0506272315979 and batch: 400, loss is 4.025922651290894 and perplexity is 56.03198292074593
At time: 386.5893416404724 and batch: 450, loss is 4.068638734817505 and perplexity is 58.47730526719536
At time: 388.12845277786255 and batch: 500, loss is 4.046221437454224 and perplexity is 57.180986395532045
At time: 389.66896200180054 and batch: 550, loss is 4.064343318939209 and perplexity is 58.22665962065576
At time: 391.23401403427124 and batch: 600, loss is 4.093248391151429 and perplexity is 59.93426577032228
At time: 392.77311062812805 and batch: 650, loss is 4.073111591339111 and perplexity is 58.73945169806078
At time: 394.31340503692627 and batch: 700, loss is 4.051431593894958 and perplexity is 57.4796857395431
At time: 395.8551745414734 and batch: 750, loss is 4.047648658752442 and perplexity is 57.26265458259391
At time: 397.394850730896 and batch: 800, loss is 4.028265089988708 and perplexity is 56.16338825023393
At time: 398.9317708015442 and batch: 850, loss is 4.050667181015014 and perplexity is 57.43576431661808
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.433736165364583 and perplexity of 84.24558508981063
Finished 14 epochs...
Completing Train Step...
At time: 402.90955328941345 and batch: 50, loss is 4.020554957389831 and perplexity is 55.732026146032226
At time: 404.46878838539124 and batch: 100, loss is 3.9813584327697753 and perplexity is 53.589782921444716
At time: 406.0048334598541 and batch: 150, loss is 3.9898902368545532 and perplexity is 54.04895645425663
At time: 407.5431809425354 and batch: 200, loss is 4.035108160972595 and perplexity is 56.54903630632331
At time: 409.08213543891907 and batch: 250, loss is 4.024692249298096 and perplexity is 55.96308345301321
At time: 410.61951541900635 and batch: 300, loss is 4.009965128898621 and perplexity is 55.144947564878386
At time: 412.1573405265808 and batch: 350, loss is 3.9797013568878175 and perplexity is 53.501054120127954
At time: 413.69586539268494 and batch: 400, loss is 3.995691113471985 and perplexity is 54.36339892117262
At time: 415.2336242198944 and batch: 450, loss is 4.039690098762512 and perplexity is 56.808734979996075
At time: 416.77159237861633 and batch: 500, loss is 4.0188664627075195 and perplexity is 55.63800231795566
At time: 418.31282925605774 and batch: 550, loss is 4.033650002479553 and perplexity is 56.466638937571034
At time: 419.85022616386414 and batch: 600, loss is 4.063004407882691 and perplexity is 58.14875146998639
At time: 421.3859226703644 and batch: 650, loss is 4.044864087104798 and perplexity is 57.10342441496658
At time: 422.92039704322815 and batch: 700, loss is 4.022915072441101 and perplexity is 55.86371547963907
At time: 424.4574885368347 and batch: 750, loss is 4.017987360954285 and perplexity is 55.58911234535039
At time: 425.99394822120667 and batch: 800, loss is 3.9992141485214234 and perplexity is 54.5552608506841
At time: 427.52915143966675 and batch: 850, loss is 4.021077370643615 and perplexity is 55.76114890154556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.43409538269043 and perplexity of 84.27585299965432
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 431.5142033100128 and batch: 50, loss is 4.015131831169128 and perplexity is 55.43060240195829
At time: 433.07576298713684 and batch: 100, loss is 3.978736610412598 and perplexity is 53.44946405641248
At time: 434.61039781570435 and batch: 150, loss is 3.977881383895874 and perplexity is 53.4037721986759
At time: 436.1509256362915 and batch: 200, loss is 4.024981265068054 and perplexity is 55.979260004193215
At time: 437.68949007987976 and batch: 250, loss is 4.003750619888305 and perplexity is 54.803311441249036
At time: 439.2272219657898 and batch: 300, loss is 4.005997262001038 and perplexity is 54.92657327942241
At time: 440.7616822719574 and batch: 350, loss is 3.9528824615478517 and perplexity is 52.0852844948513
At time: 442.30554485321045 and batch: 400, loss is 3.947069387435913 and perplexity is 51.78338720174886
At time: 443.8445568084717 and batch: 450, loss is 3.9830127334594727 and perplexity is 53.67850990661377
At time: 445.38057565689087 and batch: 500, loss is 3.9488405084609983 and perplexity is 51.87518311441149
At time: 446.91941952705383 and batch: 550, loss is 3.956016125679016 and perplexity is 52.24875828483634
At time: 448.45667243003845 and batch: 600, loss is 3.9737990427017214 and perplexity is 53.1862041751154
At time: 449.9945366382599 and batch: 650, loss is 3.9421022653579714 and perplexity is 51.526810547218986
At time: 451.53608775138855 and batch: 700, loss is 3.9008239793777464 and perplexity is 49.44317248010965
At time: 453.07520294189453 and batch: 750, loss is 3.880740532875061 and perplexity is 48.460088075484606
At time: 454.6146020889282 and batch: 800, loss is 3.849483723640442 and perplexity is 46.96880808568756
At time: 456.15200686454773 and batch: 850, loss is 3.868590121269226 and perplexity is 47.87484075692775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.365475972493489 and perplexity of 78.68684380319577
Finished 16 epochs...
Completing Train Step...
At time: 460.1643395423889 and batch: 50, loss is 3.9800563716888426 and perplexity is 53.52005115812635
At time: 461.73617029190063 and batch: 100, loss is 3.9360234260559084 and perplexity is 51.21453743678283
At time: 463.2842183113098 and batch: 150, loss is 3.9287287521362306 and perplexity is 50.842303399811755
At time: 464.83047556877136 and batch: 200, loss is 3.9737274265289306 and perplexity is 53.18239531911658
At time: 466.3787090778351 and batch: 250, loss is 3.9572457885742187 and perplexity is 52.313046162327744
At time: 467.94796085357666 and batch: 300, loss is 3.9602364826202394 and perplexity is 52.46973266191841
At time: 469.4870569705963 and batch: 350, loss is 3.9100660467147828 and perplexity is 49.902247744093025
At time: 471.0280933380127 and batch: 400, loss is 3.9088895750045776 and perplexity is 49.84357368230085
At time: 472.56787395477295 and batch: 450, loss is 3.9501033544540407 and perplexity is 51.94073486371612
At time: 474.111567735672 and batch: 500, loss is 3.9190417432785036 and perplexity is 50.352171345954844
At time: 475.65266704559326 and batch: 550, loss is 3.9324164485931394 and perplexity is 51.030140512179194
At time: 477.1933126449585 and batch: 600, loss is 3.9545600032806396 and perplexity is 52.17273306205827
At time: 478.735280752182 and batch: 650, loss is 3.9281575298309326 and perplexity is 50.8132694352706
At time: 480.27597188949585 and batch: 700, loss is 3.8930045413970946 and perplexity is 49.058062294192275
At time: 481.8195300102234 and batch: 750, loss is 3.878615221977234 and perplexity is 48.35720469050433
At time: 483.35940384864807 and batch: 800, loss is 3.8533229446411132 and perplexity is 47.149478314617724
At time: 484.9039537906647 and batch: 850, loss is 3.8764325094223024 and perplexity is 48.251769921461694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.36234696706136 and perplexity of 78.44101703870669
Finished 17 epochs...
Completing Train Step...
At time: 488.9054493904114 and batch: 50, loss is 3.9594845962524414 and perplexity is 52.43029621292999
At time: 490.45027446746826 and batch: 100, loss is 3.9151525115966797 and perplexity is 50.15672040926299
At time: 491.9969959259033 and batch: 150, loss is 3.9082633543014524 and perplexity is 49.81237037564127
At time: 493.5362620353699 and batch: 200, loss is 3.9539635610580444 and perplexity is 52.141624319397906
At time: 495.0860404968262 and batch: 250, loss is 3.9378822660446167 and perplexity is 51.309825602262414
At time: 496.6301817893982 and batch: 300, loss is 3.941807050704956 and perplexity is 51.51160132282571
At time: 498.1660158634186 and batch: 350, loss is 3.892404499053955 and perplexity is 49.0286342094748
At time: 499.70560574531555 and batch: 400, loss is 3.8927444124221804 and perplexity is 49.0453025304005
At time: 501.2472314834595 and batch: 450, loss is 3.9349406957626343 and perplexity is 51.15911591433887
At time: 502.7897753715515 and batch: 500, loss is 3.905124154090881 and perplexity is 49.65624455538574
At time: 504.327921628952 and batch: 550, loss is 3.920424427986145 and perplexity is 50.42184067753152
At time: 505.86561703681946 and batch: 600, loss is 3.944903039932251 and perplexity is 51.67132781376602
At time: 507.45024514198303 and batch: 650, loss is 3.9203533935546875 and perplexity is 50.41825911795449
At time: 508.98840832710266 and batch: 700, loss is 3.8879430294036865 and perplexity is 48.81038167151325
At time: 510.52522373199463 and batch: 750, loss is 3.8763196325302123 and perplexity is 48.2463237190161
At time: 512.0620882511139 and batch: 800, loss is 3.852550368309021 and perplexity is 47.11306581113169
At time: 513.5983762741089 and batch: 850, loss is 3.8774428796768188 and perplexity is 48.300546711677605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.361367861429851 and perplexity of 78.36425258357039
Finished 18 epochs...
Completing Train Step...
At time: 517.602109670639 and batch: 50, loss is 3.9440706157684327 and perplexity is 51.62833324925715
At time: 519.1381585597992 and batch: 100, loss is 3.9001871585845946 and perplexity is 49.41169606327704
At time: 520.6787467002869 and batch: 150, loss is 3.8937615156173706 and perplexity is 49.095212041572275
At time: 522.2179696559906 and batch: 200, loss is 3.939971914291382 and perplexity is 51.417157192952736
At time: 523.7588133811951 and batch: 250, loss is 3.9240775156021117 and perplexity is 50.6063729303126
At time: 525.2957632541656 and batch: 300, loss is 3.9291911125183105 and perplexity is 50.865816301935844
At time: 526.8336696624756 and batch: 350, loss is 3.8803507566452025 and perplexity is 48.441203165739495
At time: 528.3707950115204 and batch: 400, loss is 3.8813664054870607 and perplexity is 48.49042741067527
At time: 529.9097003936768 and batch: 450, loss is 3.9240141916275024 and perplexity is 50.60316843509985
At time: 531.4460165500641 and batch: 500, loss is 3.8948351526260376 and perplexity is 49.147950784235434
At time: 532.9876656532288 and batch: 550, loss is 3.911450729370117 and perplexity is 49.9713943830341
At time: 534.5265781879425 and batch: 600, loss is 3.937319917678833 and perplexity is 51.28097971716468
At time: 536.0654530525208 and batch: 650, loss is 3.913879632949829 and perplexity is 50.092917606085415
At time: 537.6044418811798 and batch: 700, loss is 3.8831361532211304 and perplexity is 48.57631921570523
At time: 539.1496210098267 and batch: 750, loss is 3.873176922798157 and perplexity is 48.0949375339457
At time: 540.6869490146637 and batch: 800, loss is 3.8501705598831175 and perplexity is 47.00107904652019
At time: 542.2228727340698 and batch: 850, loss is 3.8756348848342896 and perplexity is 48.213298468285345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.361258188883464 and perplexity of 78.35565864771189
Finished 19 epochs...
Completing Train Step...
At time: 546.2573797702789 and batch: 50, loss is 3.93109299659729 and perplexity is 50.962649241449405
At time: 547.8003218173981 and batch: 100, loss is 3.8875778102874756 and perplexity is 48.79255844194745
At time: 549.3546078205109 and batch: 150, loss is 3.8815664720535277 and perplexity is 48.50012969451278
At time: 550.9071891307831 and batch: 200, loss is 3.9286653280258177 and perplexity is 50.839078874204695
At time: 552.4505078792572 and batch: 250, loss is 3.9129185771942137 and perplexity is 50.04479864551019
At time: 553.992481470108 and batch: 300, loss is 3.9190733671188354 and perplexity is 50.35376370015989
At time: 555.5356061458588 and batch: 350, loss is 3.8706158924102785 and perplexity is 47.971922527240004
At time: 557.0856585502625 and batch: 400, loss is 3.8719386863708496 and perplexity is 48.03542148539607
At time: 558.6282818317413 and batch: 450, loss is 3.9148857736587526 and perplexity is 50.14334349323264
At time: 560.1745314598083 and batch: 500, loss is 3.8860787200927733 and perplexity is 48.719468793687895
At time: 561.7181084156036 and batch: 550, loss is 3.9036601972579956 and perplexity is 49.5836031417939
At time: 563.2606747150421 and batch: 600, loss is 3.930661497116089 and perplexity is 50.940663628472514
At time: 564.8034753799438 and batch: 650, loss is 3.9079191303253173 and perplexity is 49.795226714248784
At time: 566.3468379974365 and batch: 700, loss is 3.878300189971924 and perplexity is 48.3419730226969
At time: 567.8872632980347 and batch: 750, loss is 3.869519033432007 and perplexity is 47.91933294026391
At time: 569.4298949241638 and batch: 800, loss is 3.846617169380188 and perplexity is 46.83436223899106
At time: 570.9751815795898 and batch: 850, loss is 3.8726082038879395 and perplexity is 48.06759280995227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.361276308695476 and perplexity of 78.35707845037989
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 574.9893395900726 and batch: 50, loss is 3.94431893825531 and perplexity is 51.641155317300964
At time: 576.5540072917938 and batch: 100, loss is 3.915019783973694 and perplexity is 50.15006366876275
At time: 578.0939118862152 and batch: 150, loss is 3.9204095220565796 and perplexity is 50.421089098727336
At time: 579.6339228153229 and batch: 200, loss is 3.957552556991577 and perplexity is 52.32909661446619
At time: 581.1877710819244 and batch: 250, loss is 3.9316213655471803 and perplexity is 50.98958343788267
At time: 582.7334477901459 and batch: 300, loss is 3.938485207557678 and perplexity is 51.3407717545684
At time: 584.2749118804932 and batch: 350, loss is 3.8908994340896608 and perplexity is 48.954898432365276
At time: 585.8410198688507 and batch: 400, loss is 3.878701810836792 and perplexity is 48.36139206699736
At time: 587.3806004524231 and batch: 450, loss is 3.9149631786346437 and perplexity is 50.147224987748395
At time: 588.9202117919922 and batch: 500, loss is 3.8849761390686037 and perplexity is 48.665781234769135
At time: 590.4603822231293 and batch: 550, loss is 3.8937507486343383 and perplexity is 49.094683437102994
At time: 592.007269859314 and batch: 600, loss is 3.9204762268066404 and perplexity is 50.42445253705087
At time: 593.5512056350708 and batch: 650, loss is 3.892774920463562 and perplexity is 49.04679882934413
At time: 595.0926380157471 and batch: 700, loss is 3.862228636741638 and perplexity is 47.57125235845451
At time: 596.6339993476868 and batch: 750, loss is 3.845628867149353 and perplexity is 46.788098599305314
At time: 598.1739706993103 and batch: 800, loss is 3.8151778316497804 and perplexity is 45.38482652393849
At time: 599.7155539989471 and batch: 850, loss is 3.8432309675216674 and perplexity is 46.6760398416147
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.347784996032715 and perplexity of 77.30703774631577
Finished 21 epochs...
Completing Train Step...
At time: 603.7156286239624 and batch: 50, loss is 3.9405961084365844 and perplexity is 51.44926150005342
At time: 605.2800261974335 and batch: 100, loss is 3.9010724782943726 and perplexity is 49.455460581632245
At time: 606.8185875415802 and batch: 150, loss is 3.902128553390503 and perplexity is 49.507716850335406
At time: 608.3582487106323 and batch: 200, loss is 3.941578245162964 and perplexity is 51.49981653123039
At time: 609.8950834274292 and batch: 250, loss is 3.917803692817688 and perplexity is 50.28987139021898
At time: 611.4334523677826 and batch: 300, loss is 3.9250051164627076 and perplexity is 50.65333722408655
At time: 612.971866607666 and batch: 350, loss is 3.8789372730255125 and perplexity is 48.37278068696511
At time: 614.5112497806549 and batch: 400, loss is 3.869332594871521 and perplexity is 47.910399761581445
At time: 616.0495593547821 and batch: 450, loss is 3.907908101081848 and perplexity is 49.79467751359839
At time: 617.589433670044 and batch: 500, loss is 3.879477777481079 and perplexity is 48.3989334576624
At time: 619.1282992362976 and batch: 550, loss is 3.8890425157546997 and perplexity is 48.864077533473214
At time: 620.6656827926636 and batch: 600, loss is 3.9167224979400634 and perplexity is 50.235527622271164
At time: 622.2040643692017 and batch: 650, loss is 3.8902017974853518 and perplexity is 48.92075761358576
At time: 623.7945857048035 and batch: 700, loss is 3.8617046737670897 and perplexity is 47.54633331246461
At time: 625.3300654888153 and batch: 750, loss is 3.8462681674957278 and perplexity is 46.81801981024726
At time: 626.8685040473938 and batch: 800, loss is 3.8178846788406373 and perplexity is 45.50784273204916
At time: 628.4067804813385 and batch: 850, loss is 3.847584581375122 and perplexity is 46.87969228567629
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.347490628560384 and perplexity of 77.28428441810514
Finished 22 epochs...
Completing Train Step...
At time: 632.3951940536499 and batch: 50, loss is 3.9368399810791015 and perplexity is 51.256374003191986
At time: 633.9725968837738 and batch: 100, loss is 3.8960585975646973 and perplexity is 49.208117393629905
At time: 635.5168511867523 and batch: 150, loss is 3.8960511589050295 and perplexity is 49.207751352553146
At time: 637.0556390285492 and batch: 200, loss is 3.935955514907837 and perplexity is 51.21105951684366
At time: 638.5939409732819 and batch: 250, loss is 3.9123943138122557 and perplexity is 50.01856886638002
At time: 640.136283159256 and batch: 300, loss is 3.9195531892776487 and perplexity is 50.37793034914606
At time: 641.6752283573151 and batch: 350, loss is 3.8741819763183596 and perplexity is 48.14329981948976
At time: 643.2132804393768 and batch: 400, loss is 3.864860954284668 and perplexity is 47.69663995816889
At time: 644.7498800754547 and batch: 450, loss is 3.9042492437362672 and perplexity is 49.6128187924479
At time: 646.2955231666565 and batch: 500, loss is 3.8763766527175902 and perplexity is 48.24907481186803
At time: 647.8409116268158 and batch: 550, loss is 3.8864473056793214 and perplexity is 48.737429397476035
At time: 649.3814296722412 and batch: 600, loss is 3.914870834350586 and perplexity is 50.142594391967236
At time: 650.9196400642395 and batch: 650, loss is 3.8889160776138305 and perplexity is 48.85789964092344
At time: 652.4568359851837 and batch: 700, loss is 3.8615688562393187 and perplexity is 47.53987612552904
At time: 653.995002746582 and batch: 750, loss is 3.8467501401901245 and perplexity is 46.840590256133915
At time: 655.5353286266327 and batch: 800, loss is 3.818903503417969 and perplexity is 45.55423086735949
At time: 657.0763785839081 and batch: 850, loss is 3.8491259622573852 and perplexity is 46.95200746543207
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.347360610961914 and perplexity of 77.27423675424636
Finished 23 epochs...
Completing Train Step...
At time: 661.0849590301514 and batch: 50, loss is 3.9334163904190063 and perplexity is 51.08119320466456
At time: 662.6281042098999 and batch: 100, loss is 3.892085371017456 and perplexity is 49.012990294046084
At time: 664.1952660083771 and batch: 150, loss is 3.8916649532318117 and perplexity is 48.99238869214223
At time: 665.7336721420288 and batch: 200, loss is 3.932054080963135 and perplexity is 51.01165219108985
At time: 667.2759366035461 and batch: 250, loss is 3.9085236501693728 and perplexity is 49.82533801745994
At time: 668.8186013698578 and batch: 300, loss is 3.9156840944290163 and perplexity is 50.18338994866548
At time: 670.3605220317841 and batch: 350, loss is 3.8708816814422606 and perplexity is 47.98467463270065
At time: 671.9038510322571 and batch: 400, loss is 3.8617175149917604 and perplexity is 47.54694386953309
At time: 673.4453136920929 and batch: 450, loss is 3.9015172719955444 and perplexity is 49.47746295188261
At time: 674.9877836704254 and batch: 500, loss is 3.8740764474868774 and perplexity is 48.138219581376624
At time: 676.5311095714569 and batch: 550, loss is 3.884441967010498 and perplexity is 48.63979227615337
At time: 678.075877904892 and batch: 600, loss is 3.913385887145996 and perplexity is 50.06819054315978
At time: 679.6174001693726 and batch: 650, loss is 3.8877672243118284 and perplexity is 48.80180131213739
At time: 681.1581811904907 and batch: 700, loss is 3.8612313795089723 and perplexity is 47.523835230439786
At time: 682.7007119655609 and batch: 750, loss is 3.846752529144287 and perplexity is 46.84070215629064
At time: 684.2439413070679 and batch: 800, loss is 3.8191475248336793 and perplexity is 45.565348431674074
At time: 685.7843079566956 and batch: 850, loss is 3.8497029399871825 and perplexity is 46.979105544851315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3472849527994795 and perplexity of 77.26839054864938
Finished 24 epochs...
Completing Train Step...
At time: 689.8126695156097 and batch: 50, loss is 3.930330262184143 and perplexity is 50.923793095431435
At time: 691.3516640663147 and batch: 100, loss is 3.8886960554122925 and perplexity is 48.847151000795016
At time: 692.8886487483978 and batch: 150, loss is 3.888097953796387 and perplexity is 48.817944176044264
At time: 694.425961971283 and batch: 200, loss is 3.928943910598755 and perplexity is 50.85324372855241
At time: 695.9645121097565 and batch: 250, loss is 3.9053527975082396 and perplexity is 49.66759942689291
At time: 697.5025384426117 and batch: 300, loss is 3.9125033092498778 and perplexity is 50.024020959304075
At time: 699.0411133766174 and batch: 350, loss is 3.8681701850891113 and perplexity is 47.85474059986366
At time: 700.5760493278503 and batch: 400, loss is 3.8591554832458494 and perplexity is 47.4252830059642
At time: 702.1487526893616 and batch: 450, loss is 3.899255609512329 and perplexity is 49.365688076326094
At time: 703.6884756088257 and batch: 500, loss is 3.8721211671829225 and perplexity is 48.04418782793732
At time: 705.2270097732544 and batch: 550, loss is 3.8826503992080688 and perplexity is 48.552728803738944
At time: 706.7693109512329 and batch: 600, loss is 3.9119970083236693 and perplexity is 49.99870016167225
At time: 708.316499710083 and batch: 650, loss is 3.8866522693634034 and perplexity is 48.7474198243604
At time: 709.8552026748657 and batch: 700, loss is 3.860653557777405 and perplexity is 47.49638285772878
At time: 711.3915288448334 and batch: 750, loss is 3.8464767789840697 and perplexity is 46.827787605842865
At time: 712.9294612407684 and batch: 800, loss is 3.8189428091049193 and perplexity is 45.55602144288691
At time: 714.4658346176147 and batch: 850, loss is 3.849751992225647 and perplexity is 46.981410031659
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.347265561421712 and perplexity of 77.26689222262611
Finished 25 epochs...
Completing Train Step...
At time: 718.4798491001129 and batch: 50, loss is 3.927486081123352 and perplexity is 50.779162383029835
At time: 720.0344014167786 and batch: 100, loss is 3.8856812047958376 and perplexity is 48.70010590836035
At time: 721.5715148448944 and batch: 150, loss is 3.885033173561096 and perplexity is 48.66855694205838
At time: 723.1110286712646 and batch: 200, loss is 3.926294722557068 and perplexity is 50.71870221495676
At time: 724.6496546268463 and batch: 250, loss is 3.9025972843170167 and perplexity is 49.53092808781169
At time: 726.1887097358704 and batch: 300, loss is 3.9097185468673707 and perplexity is 49.88490973326896
At time: 727.7289276123047 and batch: 350, loss is 3.8657792949676515 and perplexity is 47.74046184171356
At time: 729.2665371894836 and batch: 400, loss is 3.856922078132629 and perplexity is 47.31948132938459
At time: 730.8032503128052 and batch: 450, loss is 3.8972436428070067 and perplexity is 49.266465804955466
At time: 732.3430082798004 and batch: 500, loss is 3.870347738265991 and perplexity is 47.95906038200074
At time: 733.8802828788757 and batch: 550, loss is 3.8809804677963258 and perplexity is 48.4717167379068
At time: 735.4173440933228 and batch: 600, loss is 3.9106464433670043 and perplexity is 49.931219248290766
At time: 736.959034204483 and batch: 650, loss is 3.8855080080032347 and perplexity is 48.691671936607115
At time: 738.5109932422638 and batch: 700, loss is 3.859982018470764 and perplexity is 47.46449787692376
At time: 740.0500581264496 and batch: 750, loss is 3.846028308868408 and perplexity is 46.80679145094501
At time: 741.6482646465302 and batch: 800, loss is 3.8185481071472167 and perplexity is 45.538043940149414
At time: 743.1980707645416 and batch: 850, loss is 3.8495541858673095 and perplexity is 46.972117729099644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.347263018290202 and perplexity of 77.2666957230077
Finished 26 epochs...
Completing Train Step...
At time: 747.1935849189758 and batch: 50, loss is 3.9248256397247316 and perplexity is 50.64424694412527
At time: 748.796064376831 and batch: 100, loss is 3.8829290437698365 and perplexity is 48.566259642638876
At time: 750.3386626243591 and batch: 150, loss is 3.8822867107391357 and perplexity is 48.53507394676634
At time: 751.8802981376648 and batch: 200, loss is 3.9239270448684693 and perplexity is 50.598758725122686
At time: 753.4242236614227 and batch: 250, loss is 3.900099711418152 and perplexity is 49.407375339387464
At time: 754.9668927192688 and batch: 300, loss is 3.907204785346985 and perplexity is 49.759668446047776
At time: 756.5078637599945 and batch: 350, loss is 3.8636234521865847 and perplexity is 47.637651772678716
At time: 758.0508921146393 and batch: 400, loss is 3.8549140882492066 and perplexity is 47.224559622393926
At time: 759.5939621925354 and batch: 450, loss is 3.895415620803833 and perplexity is 49.17648788730837
At time: 761.1348233222961 and batch: 500, loss is 3.8686918354034425 and perplexity is 47.87971055256542
At time: 762.6784036159515 and batch: 550, loss is 3.8793873023986816 and perplexity is 48.39455475825445
At time: 764.2200746536255 and batch: 600, loss is 3.909318289756775 and perplexity is 49.86494693882852
At time: 765.7640526294708 and batch: 650, loss is 3.884345703125 and perplexity is 48.63511024611787
At time: 767.3122601509094 and batch: 700, loss is 3.8592295980453493 and perplexity is 47.42879805156225
At time: 768.8638277053833 and batch: 750, loss is 3.845463738441467 and perplexity is 46.78037317890109
At time: 770.4059736728668 and batch: 800, loss is 3.8179628896713256 and perplexity is 45.51140207741993
At time: 771.9487211704254 and batch: 850, loss is 3.849141182899475 and perplexity is 46.95272211057176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.347262382507324 and perplexity of 77.26664659818114
Finished 27 epochs...
Completing Train Step...
At time: 775.9646620750427 and batch: 50, loss is 3.9223342084884645 and perplexity is 50.518227335132615
At time: 777.5298416614532 and batch: 100, loss is 3.8803828287124635 and perplexity is 48.44275680017963
At time: 779.0694394111633 and batch: 150, loss is 3.8797843933105467 and perplexity is 48.41377561209333
At time: 780.6350328922272 and batch: 200, loss is 3.9218119478225706 and perplexity is 50.4918505404655
At time: 782.1767060756683 and batch: 250, loss is 3.8977899169921875 and perplexity is 49.29338615569668
At time: 783.7169063091278 and batch: 300, loss is 3.904887194633484 and perplexity is 49.64447943260311
At time: 785.2553558349609 and batch: 350, loss is 3.8616422700881956 and perplexity is 47.54336633892398
At time: 786.7942645549774 and batch: 400, loss is 3.853062210083008 and perplexity is 47.13718641875512
At time: 788.333701133728 and batch: 450, loss is 3.8936839294433594 and perplexity is 49.091403079671
At time: 789.8731851577759 and batch: 500, loss is 3.8671189403533934 and perplexity is 47.80445998897315
At time: 791.4107003211975 and batch: 550, loss is 3.8778456926345823 and perplexity is 48.32000671686813
At time: 792.9490671157837 and batch: 600, loss is 3.908005404472351 and perplexity is 49.79952294028385
At time: 794.4896373748779 and batch: 650, loss is 3.883181390762329 and perplexity is 48.57851673865193
At time: 796.0329992771149 and batch: 700, loss is 3.858427381515503 and perplexity is 47.390765143128135
At time: 797.579151391983 and batch: 750, loss is 3.8448263025283813 and perplexity is 46.750563190997106
At time: 799.1200082302094 and batch: 800, loss is 3.81729775428772 and perplexity is 45.481140898546776
At time: 800.660548210144 and batch: 850, loss is 3.84867413520813 and perplexity is 46.93079807029238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.347301483154297 and perplexity of 77.26966783311828
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 804.7105767726898 and batch: 50, loss is 3.930199875831604 and perplexity is 50.91715376064101
At time: 806.2480471134186 and batch: 100, loss is 3.8996344232559204 and perplexity is 49.384392019863014
At time: 807.7884058952332 and batch: 150, loss is 3.9025048446655273 and perplexity is 49.52634967769788
At time: 809.3311376571655 and batch: 200, loss is 3.9513360834121705 and perplexity is 52.00480319301828
At time: 810.8714110851288 and batch: 250, loss is 3.921344938278198 and perplexity is 50.46827586957669
At time: 812.4107885360718 and batch: 300, loss is 3.914261121749878 and perplexity is 50.11203113868141
At time: 813.9474902153015 and batch: 350, loss is 3.863097152709961 and perplexity is 47.6125866979296
At time: 815.486115694046 and batch: 400, loss is 3.856452293395996 and perplexity is 47.29725658014337
At time: 817.0267477035522 and batch: 450, loss is 3.897038879394531 and perplexity is 49.256378868049694
At time: 818.5641241073608 and batch: 500, loss is 3.8702826642990114 and perplexity is 47.95593959723108
At time: 820.1293985843658 and batch: 550, loss is 3.879191198348999 and perplexity is 48.385065320572764
At time: 821.6685001850128 and batch: 600, loss is 3.9048750591278076 and perplexity is 49.64387697539673
At time: 823.208996295929 and batch: 650, loss is 3.881251502037048 and perplexity is 48.484856013365714
At time: 824.7482423782349 and batch: 700, loss is 3.85390962600708 and perplexity is 47.17714815085498
At time: 826.2950837612152 and batch: 750, loss is 3.843907299041748 and perplexity is 46.707618996377235
At time: 827.8316848278046 and batch: 800, loss is 3.811138744354248 and perplexity is 45.201882958945454
At time: 829.3695859909058 and batch: 850, loss is 3.8377499628067016 and perplexity is 46.42090807508409
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.344003041585286 and perplexity of 77.01521822299442
Finished 29 epochs...
Completing Train Step...
At time: 833.3950536251068 and batch: 50, loss is 3.9261599349975587 and perplexity is 50.71186642556376
At time: 834.9518191814423 and batch: 100, loss is 3.8915838098526 and perplexity is 48.98841344545274
At time: 836.4979107379913 and batch: 150, loss is 3.8928094482421876 and perplexity is 49.04849233559274
At time: 838.0357780456543 and batch: 200, loss is 3.9419455242156984 and perplexity is 51.51873480899288
At time: 839.5756192207336 and batch: 250, loss is 3.9134864807128906 and perplexity is 50.07322733436462
At time: 841.1119685173035 and batch: 300, loss is 3.9085939836502077 and perplexity is 49.82884253015733
At time: 842.6488790512085 and batch: 350, loss is 3.858860077857971 and perplexity is 47.41127539090692
At time: 844.1856672763824 and batch: 400, loss is 3.852814402580261 and perplexity is 47.12550691749597
At time: 845.7236042022705 and batch: 450, loss is 3.8941628742218017 and perplexity is 49.11492078223328
At time: 847.262589931488 and batch: 500, loss is 3.8681950044631956 and perplexity is 47.85592833931173
At time: 848.8007915019989 and batch: 550, loss is 3.878256359100342 and perplexity is 48.339854198320616
At time: 850.3411960601807 and batch: 600, loss is 3.904547996520996 and perplexity is 49.62764297449296
At time: 851.8781034946442 and batch: 650, loss is 3.8816030263900756 and perplexity is 48.501902616980054
At time: 853.4160118103027 and batch: 700, loss is 3.854291863441467 and perplexity is 47.195184469784394
At time: 854.9528987407684 and batch: 750, loss is 3.8456626176834106 and perplexity is 46.78967774926902
At time: 856.4911234378815 and batch: 800, loss is 3.8137518215179442 and perplexity is 45.320153424690545
At time: 858.0283968448639 and batch: 850, loss is 3.8405808067321776 and perplexity is 46.552504597450515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.343801498413086 and perplexity of 76.9996978956666
Finished 30 epochs...
Completing Train Step...
At time: 862.0475075244904 and batch: 50, loss is 3.924672908782959 and perplexity is 50.63651259124666
At time: 863.5862331390381 and batch: 100, loss is 3.8892724657058717 and perplexity is 48.87531511770755
At time: 865.1267960071564 and batch: 150, loss is 3.889740881919861 and perplexity is 48.8982144705679
At time: 866.6686849594116 and batch: 200, loss is 3.939255242347717 and perplexity is 51.38032116022177
At time: 868.2112064361572 and batch: 250, loss is 3.9112666988372804 and perplexity is 49.96219896684382
At time: 869.761960029602 and batch: 300, loss is 3.9069209051132203 and perplexity is 49.74554466456331
At time: 871.3019995689392 and batch: 350, loss is 3.8573264694213867 and perplexity is 47.33862078507646
At time: 872.844996213913 and batch: 400, loss is 3.8515278244018556 and perplexity is 47.064915254963054
At time: 874.3875031471252 and batch: 450, loss is 3.8931544065475463 and perplexity is 49.065414939017714
At time: 875.931812286377 and batch: 500, loss is 3.867438712120056 and perplexity is 47.81974894995694
At time: 877.4771084785461 and batch: 550, loss is 3.8779651498794556 and perplexity is 48.32577923652057
At time: 879.0200498104095 and batch: 600, loss is 3.9045513916015624 and perplexity is 49.627811464625196
At time: 880.5618069171906 and batch: 650, loss is 3.881833162307739 and perplexity is 48.51306593133777
At time: 882.1050779819489 and batch: 700, loss is 3.8546067667007446 and perplexity is 47.2100487274749
At time: 883.6587870121002 and batch: 750, loss is 3.846606149673462 and perplexity is 46.833846140898125
At time: 885.2098941802979 and batch: 800, loss is 3.814911541938782 and perplexity is 45.37274262058068
At time: 886.7533922195435 and batch: 850, loss is 3.841609363555908 and perplexity is 46.60041112678631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.343688011169434 and perplexity of 76.9909599080248
Finished 31 epochs...
Completing Train Step...
At time: 890.7499403953552 and batch: 50, loss is 3.9235964107513426 and perplexity is 50.58203181459992
At time: 892.3308718204498 and batch: 100, loss is 3.8879694032669065 and perplexity is 48.8116690068191
At time: 893.8671016693115 and batch: 150, loss is 3.888050036430359 and perplexity is 48.81560500478836
At time: 895.4053146839142 and batch: 200, loss is 3.937808966636658 and perplexity is 51.30606476025873
At time: 896.941810131073 and batch: 250, loss is 3.9100611543655397 and perplexity is 49.90200360546625
At time: 898.5206699371338 and batch: 300, loss is 3.905898036956787 and perplexity is 49.694687545499455
At time: 900.0598349571228 and batch: 350, loss is 3.8563207674026487 and perplexity is 47.291036170570806
At time: 901.5970549583435 and batch: 400, loss is 3.8506963300704955 and perplexity is 47.025797310151084
At time: 903.1336295604706 and batch: 450, loss is 3.892480506896973 and perplexity is 49.03236091183468
At time: 904.6689841747284 and batch: 500, loss is 3.8669228839874266 and perplexity is 47.795088538966944
At time: 906.209810256958 and batch: 550, loss is 3.877697229385376 and perplexity is 48.31283350416168
At time: 907.7585806846619 and batch: 600, loss is 3.9044592237472533 and perplexity is 49.62323758651395
At time: 909.2979674339294 and batch: 650, loss is 3.8818437910079955 and perplexity is 48.513581564914325
At time: 910.83562707901 and batch: 700, loss is 3.8547531747817994 and perplexity is 47.21696116612168
At time: 912.3693962097168 and batch: 750, loss is 3.8470986700057983 and perplexity is 46.856918443686745
At time: 913.9065296649933 and batch: 800, loss is 3.815466356277466 and perplexity is 45.397923053358674
At time: 915.4461967945099 and batch: 850, loss is 3.8420654201507567 and perplexity is 46.62166839849438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.343653043111165 and perplexity of 76.98826773072298
Finished 32 epochs...
Completing Train Step...
At time: 919.4591655731201 and batch: 50, loss is 3.9227134132385255 and perplexity is 50.537387719527544
At time: 921.0201051235199 and batch: 100, loss is 3.887033448219299 and perplexity is 48.766004851961675
At time: 922.5575692653656 and batch: 150, loss is 3.8868616914749143 and perplexity is 48.757629680998285
At time: 924.0974097251892 and batch: 200, loss is 3.936793341636658 and perplexity is 51.253983490233324
At time: 925.6373178958893 and batch: 250, loss is 3.9092054986953735 and perplexity is 49.85932293571116
At time: 927.1769690513611 and batch: 300, loss is 3.9050807476043703 and perplexity is 49.65408919905482
At time: 928.7226724624634 and batch: 350, loss is 3.8555184745788575 and perplexity is 47.25311012755192
At time: 930.2625567913055 and batch: 400, loss is 3.8500400495529177 and perplexity is 46.994945320440095
At time: 931.8021650314331 and batch: 450, loss is 3.891943264007568 and perplexity is 49.00602569942031
At time: 933.336505651474 and batch: 500, loss is 3.8664777135849 and perplexity is 47.773816315396715
At time: 934.8746435642242 and batch: 550, loss is 3.8774126625061034 and perplexity is 48.29908722786282
At time: 936.4580914974213 and batch: 600, loss is 3.904303970336914 and perplexity is 49.61553400766549
At time: 937.9965834617615 and batch: 650, loss is 3.8817545795440673 and perplexity is 48.50925379032894
At time: 939.534304857254 and batch: 700, loss is 3.85479606628418 and perplexity is 47.21898641595663
At time: 941.0708103179932 and batch: 750, loss is 3.8473707485198974 and perplexity is 46.86966893892141
At time: 942.6097986698151 and batch: 800, loss is 3.8157574939727783 and perplexity is 45.41114202422538
At time: 944.1504983901978 and batch: 850, loss is 3.842273178100586 and perplexity is 46.63135542698221
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.343636512756348 and perplexity of 76.98699509785918
Finished 33 epochs...
Completing Train Step...
At time: 948.1557745933533 and batch: 50, loss is 3.9219387912750245 and perplexity is 50.498255507314305
At time: 949.7410020828247 and batch: 100, loss is 3.886263761520386 and perplexity is 48.728484747882796
At time: 951.2859723567963 and batch: 150, loss is 3.885909276008606 and perplexity is 48.711214267277114
At time: 952.8284771442413 and batch: 200, loss is 3.9359695959091185 and perplexity is 51.21178062491529
At time: 954.3705050945282 and batch: 250, loss is 3.9084934997558594 and perplexity is 49.82383578556184
At time: 955.9138097763062 and batch: 300, loss is 3.904351773262024 and perplexity is 49.61790583201156
At time: 957.4588050842285 and batch: 350, loss is 3.8548201656341554 and perplexity is 47.22012437654776
At time: 959.007226228714 and batch: 400, loss is 3.8494725894927977 and perplexity is 46.968285130954996
At time: 960.5478353500366 and batch: 450, loss is 3.891475601196289 and perplexity is 48.9831127618541
At time: 962.0890347957611 and batch: 500, loss is 3.866066088676453 and perplexity is 47.75415546935431
At time: 963.637490272522 and batch: 550, loss is 3.877121386528015 and perplexity is 48.285020912679066
At time: 965.1820337772369 and batch: 600, loss is 3.9041103315353394 and perplexity is 49.60592744525239
At time: 966.7259931564331 and batch: 650, loss is 3.8816092824935913 and perplexity is 48.50220605085269
At time: 968.2669682502747 and batch: 700, loss is 3.8547693252563477 and perplexity is 47.21772374860926
At time: 969.8095321655273 and batch: 750, loss is 3.8475160694122312 and perplexity is 46.876480575959604
At time: 971.3517436981201 and batch: 800, loss is 3.8159072589874268 and perplexity is 45.41794353387722
At time: 972.895866394043 and batch: 850, loss is 3.8423627948760988 and perplexity is 46.635534565951055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3436323801676435 and perplexity of 76.98667694293027
Finished 34 epochs...
Completing Train Step...
At time: 976.9324977397919 and batch: 50, loss is 3.9212365674972536 and perplexity is 50.4628068794525
At time: 978.4738736152649 and batch: 100, loss is 3.8855903720855713 and perplexity is 48.69568254664645
At time: 980.0274693965912 and batch: 150, loss is 3.8850886392593384 and perplexity is 48.67125645241604
At time: 981.5665578842163 and batch: 200, loss is 3.9352498340606687 and perplexity is 51.174933601161996
At time: 983.1067681312561 and batch: 250, loss is 3.907860541343689 and perplexity is 49.792309348089226
At time: 984.6476728916168 and batch: 300, loss is 3.9036767530441283 and perplexity is 49.58442404411853
At time: 986.1897449493408 and batch: 350, loss is 3.8541816234588624 and perplexity is 47.18998196023697
At time: 987.7303974628448 and batch: 400, loss is 3.848955602645874 and perplexity is 46.94400942097181
At time: 989.2696390151978 and batch: 450, loss is 3.8910483837127687 and perplexity is 48.96219078911948
At time: 990.8086085319519 and batch: 500, loss is 3.865674638748169 and perplexity is 47.73546576690057
At time: 992.3487153053284 and batch: 550, loss is 3.8768267822265625 and perplexity is 48.27079803298609
At time: 993.8894772529602 and batch: 600, loss is 3.903894100189209 and perplexity is 49.59520224838853
At time: 995.430251121521 and batch: 650, loss is 3.881432409286499 and perplexity is 48.4936280687474
At time: 996.9696006774902 and batch: 700, loss is 3.854698224067688 and perplexity is 47.21436663167384
At time: 998.5109350681305 and batch: 750, loss is 3.8475866413116453 and perplexity is 46.87978885496609
At time: 1000.0515375137329 and batch: 800, loss is 3.815976390838623 and perplexity is 45.42108346892475
At time: 1001.5929551124573 and batch: 850, loss is 3.8423874235153197 and perplexity is 46.636683149850725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3436330159505205 and perplexity of 76.98672588975678
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 1005.624739408493 and batch: 50, loss is 3.9229667711257936 and perplexity is 50.550193387448196
At time: 1007.163872718811 and batch: 100, loss is 3.8911019706726075 and perplexity is 48.96481459437116
At time: 1008.7022469043732 and batch: 150, loss is 3.8922371006011964 and perplexity is 49.020427578875065
At time: 1010.240749835968 and batch: 200, loss is 3.9413679790496827 and perplexity is 51.48898900334466
At time: 1011.7773149013519 and batch: 250, loss is 3.911920828819275 and perplexity is 49.99489143054903
At time: 1013.324615240097 and batch: 300, loss is 3.9049293565750123 and perplexity is 49.64657258436752
At time: 1014.8940215110779 and batch: 350, loss is 3.854479808807373 and perplexity is 47.204055419599406
At time: 1016.4319446086884 and batch: 400, loss is 3.8496108198165895 and perplexity is 46.97477802096339
At time: 1017.9747450351715 and batch: 450, loss is 3.8917987060546877 and perplexity is 48.998942000681225
At time: 1019.5114824771881 and batch: 500, loss is 3.8658980417251585 and perplexity is 47.74613120336165
At time: 1021.0498857498169 and batch: 550, loss is 3.8762900924682615 and perplexity is 48.244898540674576
At time: 1022.5889301300049 and batch: 600, loss is 3.900911374092102 and perplexity is 49.44749374087031
At time: 1024.1258928775787 and batch: 650, loss is 3.8775216245651247 and perplexity is 48.30435028258745
At time: 1025.6618766784668 and batch: 700, loss is 3.8511339712142942 and perplexity is 47.046382237951065
At time: 1027.1989419460297 and batch: 750, loss is 3.8438759946823122 and perplexity is 46.70615686716942
At time: 1028.7379863262177 and batch: 800, loss is 3.812275414466858 and perplexity is 45.25329180025019
At time: 1030.2757325172424 and batch: 850, loss is 3.8384167098999025 and perplexity is 46.451869401146986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.343194961547852 and perplexity of 76.95300890101056
Finished 36 epochs...
Completing Train Step...
At time: 1034.2902081012726 and batch: 50, loss is 3.922021260261536 and perplexity is 50.502420218994
At time: 1035.8277924060822 and batch: 100, loss is 3.889348201751709 and perplexity is 48.879016880990314
At time: 1037.3652701377869 and batch: 150, loss is 3.8904863166809083 and perplexity is 48.934678488472144
At time: 1038.9021670818329 and batch: 200, loss is 3.9396937561035155 and perplexity is 51.40285707862184
At time: 1040.4394314289093 and batch: 250, loss is 3.9104384899139406 and perplexity is 49.92083695838634
At time: 1041.9770333766937 and batch: 300, loss is 3.9038242435455324 and perplexity is 49.59173781502524
At time: 1043.5152056217194 and batch: 350, loss is 3.8536400604248047 and perplexity is 47.16443252936755
At time: 1045.0526530742645 and batch: 400, loss is 3.848739185333252 and perplexity is 46.933851023875356
At time: 1046.5885078907013 and batch: 450, loss is 3.89116446018219 and perplexity is 48.96787447722625
At time: 1048.127604484558 and batch: 500, loss is 3.8654188919067383 and perplexity is 47.72325913327699
At time: 1049.6697239875793 and batch: 550, loss is 3.875948233604431 and perplexity is 48.22840841328255
At time: 1051.2116265296936 and batch: 600, loss is 3.9008639287948608 and perplexity is 49.445147745485606
At time: 1052.751694202423 and batch: 650, loss is 3.8777763509750365 and perplexity is 48.316656243578144
At time: 1054.336342573166 and batch: 700, loss is 3.8514883756637572 and perplexity is 47.06305864006834
At time: 1055.877194404602 and batch: 750, loss is 3.8446484565734864 and perplexity is 46.74224953174178
At time: 1057.4164218902588 and batch: 800, loss is 3.813303241729736 and perplexity is 45.29982827893135
At time: 1058.953712463379 and batch: 850, loss is 3.8393234539031984 and perplexity is 46.49400845694927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.343090057373047 and perplexity of 76.9449366325278
Finished 37 epochs...
Completing Train Step...
At time: 1062.9356226921082 and batch: 50, loss is 3.9213724279403688 and perplexity is 50.46966324449983
At time: 1064.5026881694794 and batch: 100, loss is 3.8882530164718627 and perplexity is 48.82551460400967
At time: 1066.0530626773834 and batch: 150, loss is 3.8892816257476808 and perplexity is 48.87576281968794
At time: 1067.6027834415436 and batch: 200, loss is 3.9386007165908814 and perplexity is 51.346702419993804
At time: 1069.1631462574005 and batch: 250, loss is 3.9095473432540895 and perplexity is 49.876369987512945
At time: 1070.7084293365479 and batch: 300, loss is 3.9031919479370116 and perplexity is 49.560391088229096
At time: 1072.2485485076904 and batch: 350, loss is 3.8531921100616455 and perplexity is 47.143309935977726
At time: 1073.7895815372467 and batch: 400, loss is 3.8482702493667604 and perplexity is 46.91184721267676
At time: 1075.3314909934998 and batch: 450, loss is 3.8907945203781127 and perplexity is 48.94976266168337
At time: 1076.8748998641968 and batch: 500, loss is 3.8651232528686523 and perplexity is 47.70915236021166
At time: 1078.4203674793243 and batch: 550, loss is 3.8757842445373534 and perplexity is 48.22050013003393
At time: 1079.9608845710754 and batch: 600, loss is 3.9009071159362794 and perplexity is 49.44728318618521
At time: 1081.5008616447449 and batch: 650, loss is 3.8780343532562256 and perplexity is 48.32912365935012
At time: 1083.0405781269073 and batch: 700, loss is 3.8517922830581663 and perplexity is 47.07736362517784
At time: 1084.5831739902496 and batch: 750, loss is 3.8452675342559814 and perplexity is 46.77119557425647
At time: 1086.1257286071777 and batch: 800, loss is 3.814037790298462 and perplexity is 45.33311542697395
At time: 1087.6671533584595 and batch: 850, loss is 3.839916820526123 and perplexity is 46.521604636249776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.343056360880534 and perplexity of 76.94234390172988
Finished 38 epochs...
Completing Train Step...
At time: 1091.6722860336304 and batch: 50, loss is 3.9208601617813112 and perplexity is 50.4438159648692
At time: 1093.2374079227448 and batch: 100, loss is 3.887461633682251 and perplexity is 48.78689021741137
At time: 1094.7776582241058 and batch: 150, loss is 3.8883704853057863 and perplexity is 48.83125041715897
At time: 1096.3187963962555 and batch: 200, loss is 3.937804388999939 and perplexity is 51.305829900270325
At time: 1097.8551619052887 and batch: 250, loss is 3.9089345073699953 and perplexity is 49.84581332228305
At time: 1099.394936800003 and batch: 300, loss is 3.902775983810425 and perplexity is 49.539780030464286
At time: 1100.929747581482 and batch: 350, loss is 3.852928409576416 and perplexity is 47.130879861252666
At time: 1102.4656074047089 and batch: 400, loss is 3.847991738319397 and perplexity is 46.89878356424546
At time: 1104.0027706623077 and batch: 450, loss is 3.890561304092407 and perplexity is 48.93834811093055
At time: 1105.5375926494598 and batch: 500, loss is 3.8648926639556884 and perplexity is 47.69815242691056
At time: 1107.0752882957458 and batch: 550, loss is 3.875653328895569 and perplexity is 48.21418772551749
At time: 1108.6076786518097 and batch: 600, loss is 3.900972089767456 and perplexity is 49.45049606999065
At time: 1110.1433849334717 and batch: 650, loss is 3.8782654190063477 and perplexity is 48.34029215483991
At time: 1111.6928486824036 and batch: 700, loss is 3.8520371437072756 and perplexity is 47.088892430411214
At time: 1113.2443583011627 and batch: 750, loss is 3.8457662010192872 and perplexity is 46.79452463119865
At time: 1114.7807970046997 and batch: 800, loss is 3.8145825481414795 and perplexity is 45.35781772492558
At time: 1116.3172450065613 and batch: 850, loss is 3.8402926445007326 and perplexity is 46.53909185646186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.343026796976726 and perplexity of 76.94006921930037
Finished 39 epochs...
Completing Train Step...
At time: 1120.3634359836578 and batch: 50, loss is 3.920424809455872 and perplexity is 50.421859911940984
At time: 1121.8988032341003 and batch: 100, loss is 3.8868421697616578 and perplexity is 48.75667785782322
At time: 1123.4340257644653 and batch: 150, loss is 3.887659258842468 and perplexity is 48.796532687173
At time: 1124.9714543819427 and batch: 200, loss is 3.9371986770629883 and perplexity is 51.27476275648434
At time: 1126.5149295330048 and batch: 250, loss is 3.908485827445984 and perplexity is 49.82345352312093
At time: 1128.0533001422882 and batch: 300, loss is 3.902508554458618 and perplexity is 49.52653341054854
At time: 1129.5899982452393 and batch: 350, loss is 3.8527136754989626 and perplexity is 47.12076034178783
At time: 1131.1286885738373 and batch: 400, loss is 3.847771325111389 and perplexity is 46.88844759204264
At time: 1132.6928935050964 and batch: 450, loss is 3.8903698205947874 and perplexity is 48.92897812199429
At time: 1134.2318406105042 and batch: 500, loss is 3.8647327995300294 and perplexity is 47.69052779863748
At time: 1135.769639492035 and batch: 550, loss is 3.875583095550537 and perplexity is 48.210801600746365
At time: 1137.306705713272 and batch: 600, loss is 3.90104576587677 and perplexity is 49.45413952436085
At time: 1138.8448219299316 and batch: 650, loss is 3.878444023132324 and perplexity is 48.34892670152955
At time: 1140.3840181827545 and batch: 700, loss is 3.8522282361984255 and perplexity is 47.09789162398301
At time: 1141.9237117767334 and batch: 750, loss is 3.84614595413208 and perplexity is 46.81229837219213
At time: 1143.4601726531982 and batch: 800, loss is 3.8149838495254516 and perplexity is 45.376023532716154
At time: 1144.9972898960114 and batch: 850, loss is 3.840575141906738 and perplexity is 46.55224088638514
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.343016942342122 and perplexity of 76.93931100676781
Finished 40 epochs...
Completing Train Step...
At time: 1149.0026626586914 and batch: 50, loss is 3.9200724506378175 and perplexity is 50.4040964547175
At time: 1150.5439298152924 and batch: 100, loss is 3.886363534927368 and perplexity is 48.73334679737076
At time: 1152.0910007953644 and batch: 150, loss is 3.8871023845672608 and perplexity is 48.7693667181169
At time: 1153.6406013965607 and batch: 200, loss is 3.9367214393615724 and perplexity is 51.25029834469994
At time: 1155.1883294582367 and batch: 250, loss is 3.9081254768371583 and perplexity is 49.80550284577308
At time: 1156.7296130657196 and batch: 300, loss is 3.9022680425643923 and perplexity is 49.5146231225239
At time: 1158.270616054535 and batch: 350, loss is 3.852533583641052 and perplexity is 47.11227504060148
At time: 1159.8123018741608 and batch: 400, loss is 3.8475999450683593 and perplexity is 46.880412536420465
At time: 1161.3550474643707 and batch: 450, loss is 3.8902173709869383 and perplexity is 48.92151948701457
At time: 1162.9028491973877 and batch: 500, loss is 3.864602065086365 and perplexity is 47.6842934115511
At time: 1164.4439470767975 and batch: 550, loss is 3.875521764755249 and perplexity is 48.20784488461252
At time: 1165.9846501350403 and batch: 600, loss is 3.901101007461548 and perplexity is 49.456871524861334
At time: 1167.5267827510834 and batch: 650, loss is 3.8785745191574095 and perplexity is 48.35523645597122
At time: 1169.0702056884766 and batch: 700, loss is 3.852372398376465 and perplexity is 47.10468184805558
At time: 1170.6121847629547 and batch: 750, loss is 3.8464404106140138 and perplexity is 46.82608458650252
At time: 1172.1980011463165 and batch: 800, loss is 3.815283007621765 and perplexity is 45.389600168213434
At time: 1173.7391295433044 and batch: 850, loss is 3.8407766723632815 and perplexity is 46.56162352615617
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.34300422668457 and perplexity of 76.93833267905683
Finished 41 epochs...
Completing Train Step...
At time: 1177.7570488452911 and batch: 50, loss is 3.919768271446228 and perplexity is 50.38876690898772
At time: 1179.294531583786 and batch: 100, loss is 3.8859709739685058 and perplexity is 48.71421974253654
At time: 1180.8358039855957 and batch: 150, loss is 3.8866437530517577 and perplexity is 48.74700467790901
At time: 1182.3739326000214 and batch: 200, loss is 3.936329107284546 and perplexity is 51.23019515252357
At time: 1183.9139869213104 and batch: 250, loss is 3.9078284025192263 and perplexity is 49.79070910751456
At time: 1185.4527769088745 and batch: 300, loss is 3.9020680570602417 and perplexity is 49.50472190573883
At time: 1186.9965915679932 and batch: 350, loss is 3.852378077507019 and perplexity is 47.10494936245311
At time: 1188.5358493328094 and batch: 400, loss is 3.8474557256698607 and perplexity is 46.873651959038014
At time: 1190.0755639076233 and batch: 450, loss is 3.890089473724365 and perplexity is 48.91526295869622
At time: 1191.6165285110474 and batch: 500, loss is 3.864490203857422 and perplexity is 47.67895968621292
At time: 1193.1547842025757 and batch: 550, loss is 3.87546591758728 and perplexity is 48.20515268817831
At time: 1194.695971250534 and batch: 600, loss is 3.9011400747299194 and perplexity is 49.458803707476314
At time: 1196.2362368106842 and batch: 650, loss is 3.878668384552002 and perplexity is 48.359775552350484
At time: 1197.7762677669525 and batch: 700, loss is 3.8524811697006225 and perplexity is 47.10980576533679
At time: 1199.3145649433136 and batch: 750, loss is 3.8466703605651857 and perplexity is 46.83685348047263
At time: 1200.8524658679962 and batch: 800, loss is 3.8155094385147095 and perplexity is 45.39987893958169
At time: 1202.392997264862 and batch: 850, loss is 3.840923647880554 and perplexity is 46.56846744779112
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.342995325724284 and perplexity of 76.93764785706097
Finished 42 epochs...
Completing Train Step...
At time: 1206.4071004390717 and batch: 50, loss is 3.919500150680542 and perplexity is 50.37525844525285
At time: 1207.9694557189941 and batch: 100, loss is 3.8856412076950075 and perplexity is 48.69815808426782
At time: 1209.5076615810394 and batch: 150, loss is 3.886257448196411 and perplexity is 48.728177110142894
At time: 1211.088970899582 and batch: 200, loss is 3.935998487472534 and perplexity is 51.213260234696854
At time: 1212.6288690567017 and batch: 250, loss is 3.907575497627258 and perplexity is 49.77811838580137
At time: 1214.1659979820251 and batch: 300, loss is 3.901893343925476 and perplexity is 49.496073536102855
At time: 1215.7074842453003 and batch: 350, loss is 3.8522367095947265 and perplexity is 47.09829070477446
At time: 1217.2467904090881 and batch: 400, loss is 3.847327461242676 and perplexity is 46.86764012248006
At time: 1218.7859172821045 and batch: 450, loss is 3.8899775075912477 and perplexity is 48.90978641245188
At time: 1220.3226544857025 and batch: 500, loss is 3.8643916177749635 and perplexity is 47.67425943605519
At time: 1221.860229253769 and batch: 550, loss is 3.875413155555725 and perplexity is 48.202609353487404
At time: 1223.3976678848267 and batch: 600, loss is 3.9011651706695556 and perplexity is 49.460044938203495
At time: 1224.9355020523071 and batch: 650, loss is 3.8787336587905883 and perplexity is 48.36293230290399
At time: 1226.4725139141083 and batch: 700, loss is 3.8525633001327515 and perplexity is 47.11367507293359
At time: 1228.0070106983185 and batch: 750, loss is 3.8468513011932375 and perplexity is 46.845328936911436
At time: 1229.543236732483 and batch: 800, loss is 3.815684838294983 and perplexity is 45.40784276677849
At time: 1231.0815353393555 and batch: 850, loss is 3.841032819747925 and perplexity is 46.57355169186615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.342988967895508 and perplexity of 76.93715870222442
Finished 43 epochs...
Completing Train Step...
At time: 1235.0530943870544 and batch: 50, loss is 3.9192598724365233 and perplexity is 50.36315582066855
At time: 1236.6240305900574 and batch: 100, loss is 3.88535813331604 and perplexity is 48.68437483434575
At time: 1238.160313129425 and batch: 150, loss is 3.8859255504608154 and perplexity is 48.71200702205658
At time: 1239.6985745429993 and batch: 200, loss is 3.935713448524475 and perplexity is 51.19866454114228
At time: 1241.2347145080566 and batch: 250, loss is 3.9073546361923217 and perplexity is 49.76712553313954
At time: 1242.7725965976715 and batch: 300, loss is 3.901734890937805 and perplexity is 49.488231356697874
At time: 1244.3097681999207 and batch: 350, loss is 3.8521041440963746 and perplexity is 47.0920475102209
At time: 1245.8484008312225 and batch: 400, loss is 3.8472090768814087 and perplexity is 46.86209205524873
At time: 1247.3877048492432 and batch: 450, loss is 3.8898764181137087 and perplexity is 48.90484239759505
At time: 1248.948900461197 and batch: 500, loss is 3.8643021154403687 and perplexity is 47.66999266948122
At time: 1250.4880349636078 and batch: 550, loss is 3.8753619480133055 and perplexity is 48.200141079521856
At time: 1252.0276038646698 and batch: 600, loss is 3.9011784315109255 and perplexity is 49.460700824362355
At time: 1253.5752565860748 and batch: 650, loss is 3.878777194023132 and perplexity is 48.3650378402405
At time: 1255.1298141479492 and batch: 700, loss is 3.8526246213912962 and perplexity is 47.11656423136624
At time: 1256.6862761974335 and batch: 750, loss is 3.846994662284851 and perplexity is 46.85204521581986
At time: 1258.2418117523193 and batch: 800, loss is 3.815823531150818 and perplexity is 45.41414094691532
At time: 1259.7806236743927 and batch: 850, loss is 3.841115117073059 and perplexity is 46.57738472831455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.342982292175293 and perplexity of 76.93664509299316
Finished 44 epochs...
Completing Train Step...
At time: 1263.774876832962 and batch: 50, loss is 3.919041175842285 and perplexity is 50.35214277431725
At time: 1265.3682799339294 and batch: 100, loss is 3.8851101493835447 and perplexity is 48.672303388447425
At time: 1266.9126768112183 and batch: 150, loss is 3.8856348466873167 and perplexity is 48.69784831589494
At time: 1268.4564046859741 and batch: 200, loss is 3.9354626417160032 and perplexity is 51.18582517765788
At time: 1269.9976003170013 and batch: 250, loss is 3.90715717792511 and perplexity is 49.75729957290813
At time: 1271.5374665260315 and batch: 300, loss is 3.9015874099731445 and perplexity is 49.48093332277181
At time: 1273.0842764377594 and batch: 350, loss is 3.851976933479309 and perplexity is 47.08605728281659
At time: 1274.6258399486542 and batch: 400, loss is 3.847097592353821 and perplexity is 46.85686794826314
At time: 1276.1697528362274 and batch: 450, loss is 3.889782862663269 and perplexity is 48.900267297052
At time: 1277.7149221897125 and batch: 500, loss is 3.864219102859497 and perplexity is 47.66603562460412
At time: 1279.2637741565704 and batch: 550, loss is 3.8753115558624267 and perplexity is 48.19771223193814
At time: 1280.824816942215 and batch: 600, loss is 3.9011819553375244 and perplexity is 49.46087511560261
At time: 1282.3664174079895 and batch: 650, loss is 3.878804020881653 and perplexity is 48.366335339671835
At time: 1283.9068467617035 and batch: 700, loss is 3.8526701021194456 and perplexity is 47.118707175746344
At time: 1285.448088645935 and batch: 750, loss is 3.8471092987060547 and perplexity is 46.857416474474526
At time: 1286.9891817569733 and batch: 800, loss is 3.815934567451477 and perplexity is 45.41918384509086
At time: 1288.5748782157898 and batch: 850, loss is 3.8411776494979857 and perplexity is 46.58029741619612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.34297529856364 and perplexity of 76.93610702985697
Finished 45 epochs...
Completing Train Step...
At time: 1292.5800819396973 and batch: 50, loss is 3.9188395309448243 and perplexity is 50.34199054525758
At time: 1294.1169068813324 and batch: 100, loss is 3.884889440536499 and perplexity is 48.66156216586854
At time: 1295.6588554382324 and batch: 150, loss is 3.8853760242462156 and perplexity is 48.68524585088818
At time: 1297.1970310211182 and batch: 200, loss is 3.935238003730774 and perplexity is 51.174328188396274
At time: 1298.7358345985413 and batch: 250, loss is 3.9069774961471557 and perplexity is 49.74835989602724
At time: 1300.2755391597748 and batch: 300, loss is 3.901447286605835 and perplexity is 49.47400037352246
At time: 1301.8152437210083 and batch: 350, loss is 3.8518533515930176 and perplexity is 47.08023865858523
At time: 1303.3570005893707 and batch: 400, loss is 3.8469905281066894 and perplexity is 46.851851521518086
At time: 1304.8942184448242 and batch: 450, loss is 3.889694447517395 and perplexity is 48.895943963912536
At time: 1306.430847644806 and batch: 500, loss is 3.8641407012939455 and perplexity is 47.6622986792806
At time: 1307.9681441783905 and batch: 550, loss is 3.8752614736557005 and perplexity is 48.19529844459481
At time: 1309.505709886551 and batch: 600, loss is 3.9011776781082155 and perplexity is 49.46066356055036
At time: 1311.043613433838 and batch: 650, loss is 3.878817777633667 and perplexity is 48.36700070792957
At time: 1312.580777168274 and batch: 700, loss is 3.8527029943466187 and perplexity is 47.12025704045598
At time: 1314.1202764511108 and batch: 750, loss is 3.8472012805938722 and perplexity is 46.861726706328696
At time: 1315.6582779884338 and batch: 800, loss is 3.8160243129730222 and perplexity is 45.423260196347606
At time: 1317.1967961788177 and batch: 850, loss is 3.8412254190444948 and perplexity is 46.58252258902728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.342971165974935 and perplexity of 76.93578908522701
Finished 46 epochs...
Completing Train Step...
At time: 1321.2322113513947 and batch: 50, loss is 3.918651275634766 and perplexity is 50.33251429022417
At time: 1322.772521018982 and batch: 100, loss is 3.8846896934509276 and perplexity is 48.651843131353274
At time: 1324.3122205734253 and batch: 150, loss is 3.8851420164108275 and perplexity is 48.67385445478123
At time: 1325.8542749881744 and batch: 200, loss is 3.935033793449402 and perplexity is 51.16387893139707
At time: 1327.4368288516998 and batch: 250, loss is 3.9068113470077517 and perplexity is 49.7400949354708
At time: 1328.976214170456 and batch: 300, loss is 3.90131254196167 and perplexity is 49.46733446605445
At time: 1330.5164706707 and batch: 350, loss is 3.8517322969436645 and perplexity is 47.07453972175132
At time: 1332.05526304245 and batch: 400, loss is 3.8468868207931517 and perplexity is 46.84699289380449
At time: 1333.5929853916168 and batch: 450, loss is 3.8896098566055297 and perplexity is 48.89180798636163
At time: 1335.1309752464294 and batch: 500, loss is 3.864065794944763 and perplexity is 47.65872860420521
At time: 1336.673677444458 and batch: 550, loss is 3.875211434364319 and perplexity is 48.19288684635056
At time: 1338.211982011795 and batch: 600, loss is 3.9011667156219483 and perplexity is 49.46012135167729
At time: 1339.7492530345917 and batch: 650, loss is 3.878821363449097 and perplexity is 48.36717414337796
At time: 1341.28795003891 and batch: 700, loss is 3.8527258968353273 and perplexity is 47.12133622396874
At time: 1342.8346207141876 and batch: 750, loss is 3.847275609970093 and perplexity is 46.865210038698756
At time: 1344.38303565979 and batch: 800, loss is 3.8160971546173097 and perplexity is 45.42656902181789
At time: 1345.9188921451569 and batch: 850, loss is 3.8412619733810427 and perplexity is 46.58422541335786
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.342967669169108 and perplexity of 76.93552005618187
Finished 47 epochs...
Completing Train Step...
At time: 1349.917392730713 and batch: 50, loss is 3.918474068641663 and perplexity is 50.32359580694363
At time: 1351.4622385501862 and batch: 100, loss is 3.8845066928863528 and perplexity is 48.64294063119875
At time: 1353.0048611164093 and batch: 150, loss is 3.884927806854248 and perplexity is 48.663429166639496
At time: 1354.551986694336 and batch: 200, loss is 3.934845681190491 and perplexity is 51.15425528374806
At time: 1356.0943279266357 and batch: 250, loss is 3.9066559314727782 and perplexity is 49.7323651526865
At time: 1357.634848833084 and batch: 300, loss is 3.9011817264556883 and perplexity is 49.46086379490799
At time: 1359.178103685379 and batch: 350, loss is 3.851613345146179 and perplexity is 47.06894045366373
At time: 1360.7171902656555 and batch: 400, loss is 3.8467856216430665 and perplexity is 46.842252257817805
At time: 1362.257182598114 and batch: 450, loss is 3.889528207778931 and perplexity is 48.887816190574185
At time: 1363.799524307251 and batch: 500, loss is 3.863993272781372 and perplexity is 47.655272415429025
At time: 1365.3423745632172 and batch: 550, loss is 3.875161156654358 and perplexity is 48.19046387927464
At time: 1366.942351102829 and batch: 600, loss is 3.9011501359939573 and perplexity is 49.45930132806276
At time: 1368.482177734375 and batch: 650, loss is 3.8788164854049683 and perplexity is 48.36693820674356
At time: 1370.029286146164 and batch: 700, loss is 3.8527403974533083 and perplexity is 47.12201951741816
At time: 1371.5727055072784 and batch: 750, loss is 3.8473357009887694 and perplexity is 46.868026301525674
At time: 1373.1154108047485 and batch: 800, loss is 3.816156620979309 and perplexity is 45.42927045493714
At time: 1374.6561217308044 and batch: 850, loss is 3.8412898683547976 and perplexity is 46.58552489722761
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.342962900797526 and perplexity of 76.93515319990904
Finished 48 epochs...
Completing Train Step...
At time: 1378.6367230415344 and batch: 50, loss is 3.918305778503418 and perplexity is 50.31512755462996
At time: 1380.2091617584229 and batch: 100, loss is 3.884336829185486 and perplexity is 48.6346786630062
At time: 1381.7485406398773 and batch: 150, loss is 3.884729151725769 and perplexity is 48.65376288702596
At time: 1383.2888250350952 and batch: 200, loss is 3.9346701097488403 and perplexity is 51.14527484577862
At time: 1384.8266718387604 and batch: 250, loss is 3.906508693695068 and perplexity is 49.725043208807755
At time: 1386.3685281276703 and batch: 300, loss is 3.901054105758667 and perplexity is 49.45455196776366
At time: 1387.91264128685 and batch: 350, loss is 3.8514959621429443 and perplexity is 47.06341568433754
At time: 1389.4638171195984 and batch: 400, loss is 3.8466864728927614 and perplexity is 46.837608137278146
At time: 1391.0047464370728 and batch: 450, loss is 3.88944890499115 and perplexity is 48.883939404183764
At time: 1392.5450539588928 and batch: 500, loss is 3.863922839164734 and perplexity is 47.65191600044454
At time: 1394.0845503807068 and batch: 550, loss is 3.8751109409332276 and perplexity is 48.18804402113731
At time: 1395.6232693195343 and batch: 600, loss is 3.9011288547515868 and perplexity is 49.45824878388348
At time: 1397.1670241355896 and batch: 650, loss is 3.878804621696472 and perplexity is 48.366364398891584
At time: 1398.7076954841614 and batch: 700, loss is 3.8527484130859375 and perplexity is 47.12239723172917
At time: 1400.2488901615143 and batch: 750, loss is 3.8473842620849608 and perplexity is 46.87030231952172
At time: 1401.7885236740112 and batch: 800, loss is 3.8162051391601564 and perplexity is 45.43147465396826
At time: 1403.3252947330475 and batch: 850, loss is 3.8413107013702392 and perplexity is 46.586495424296615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.342961311340332 and perplexity of 76.93503091487347
Finished 49 epochs...
Completing Train Step...
At time: 1407.3212895393372 and batch: 50, loss is 3.9181447982788087 and perplexity is 50.307028466009015
At time: 1408.8858046531677 and batch: 100, loss is 3.8841778802871705 and perplexity is 48.626948848751795
At time: 1410.42107796669 and batch: 150, loss is 3.8845434093475344 and perplexity is 48.644726660628336
At time: 1411.9594640731812 and batch: 200, loss is 3.934504957199097 and perplexity is 51.136828770695125
At time: 1413.4964582920074 and batch: 250, loss is 3.9063682985305785 and perplexity is 49.71806254322451
At time: 1415.0364785194397 and batch: 300, loss is 3.9009287548065186 and perplexity is 49.448353181106455
At time: 1416.574048757553 and batch: 350, loss is 3.851379871368408 and perplexity is 47.05795237308459
At time: 1418.1118845939636 and batch: 400, loss is 3.8465893125534056 and perplexity is 46.833057600446345
At time: 1419.6508257389069 and batch: 450, loss is 3.8893715381622314 and perplexity is 48.880157555103736
At time: 1421.1892223358154 and batch: 500, loss is 3.8638536071777345 and perplexity is 47.64861707781231
At time: 1422.727861881256 and batch: 550, loss is 3.8750606203079223 and perplexity is 48.18561922963895
At time: 1424.2644939422607 and batch: 600, loss is 3.901103386878967 and perplexity is 49.45698920354294
At time: 1425.8029761314392 and batch: 650, loss is 3.8787867641448974 and perplexity is 48.36550070175663
At time: 1427.3420023918152 and batch: 700, loss is 3.8527503919601442 and perplexity is 47.12249048111787
At time: 1428.8794212341309 and batch: 750, loss is 3.847422957420349 and perplexity is 46.87211601668031
At time: 1430.4154379367828 and batch: 800, loss is 3.816244444847107 and perplexity is 45.43326040438355
At time: 1431.9505994319916 and batch: 850, loss is 3.841326241493225 and perplexity is 46.58721938979023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.342960039774577 and perplexity of 76.934933086985
Finished Training.
Improved accuracyfrom -10000000 to -76.934933086985
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f777bec6898>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'seq_len': 50, 'wordvec_source': 'glove', 'lr': 14.952918635629437, 'anneal': 4.902308889829855, 'dropout': 0.672335060084884, 'wordvec_dim': 200, 'tune_wordvecs': True, 'data': 'wikitext', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.0031073093414307 and batch: 50, loss is 7.336346406936645 and perplexity is 1535.093249347335
At time: 3.523785352706909 and batch: 100, loss is 6.48508318901062 and perplexity is 655.293475187559
At time: 5.046739101409912 and batch: 150, loss is 6.336418781280518 and perplexity is 564.7701201037223
At time: 6.572010278701782 and batch: 200, loss is 6.357047843933105 and perplexity is 576.5418002004584
At time: 8.097869873046875 and batch: 250, loss is 6.415117139816284 and perplexity is 611.0123304983232
At time: 9.659188270568848 and batch: 300, loss is 6.389336910247803 and perplexity is 595.4616042627966
At time: 11.19088363647461 and batch: 350, loss is 6.398701629638672 and perplexity is 601.0641271781492
At time: 12.717609643936157 and batch: 400, loss is 6.435128555297852 and perplexity is 623.3627142922193
At time: 14.245866298675537 and batch: 450, loss is 6.462111921310425 and perplexity is 640.412129411175
At time: 15.77348279953003 and batch: 500, loss is 6.501632280349732 and perplexity is 666.2282172255199
At time: 17.30256223678589 and batch: 550, loss is 6.494605302810669 and perplexity is 661.5630566928289
At time: 18.83064031600952 and batch: 600, loss is 6.5307990837097165 and perplexity is 685.9461209208756
At time: 20.364675045013428 and batch: 650, loss is 6.56051342010498 and perplexity is 706.6344017855643
At time: 21.893124103546143 and batch: 700, loss is 6.5533177852630615 and perplexity is 701.5679686255044
At time: 23.418761730194092 and batch: 750, loss is 6.5633860778808595 and perplexity is 708.667239019459
At time: 24.94774556159973 and batch: 800, loss is 6.611310520172119 and perplexity is 743.4566955623261
At time: 26.477092027664185 and batch: 850, loss is 6.598754768371582 and perplexity is 734.1803951459085
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.09695307413737 and perplexity of 444.501342029052
Finished 1 epochs...
Completing Train Step...
At time: 30.50659203529358 and batch: 50, loss is 6.483634510040283 and perplexity is 654.3448526013556
At time: 32.039336919784546 and batch: 100, loss is 6.355868225097656 and perplexity is 575.8621016049677
At time: 33.57241725921631 and batch: 150, loss is 6.328976936340332 and perplexity is 560.582788504185
At time: 35.10737371444702 and batch: 200, loss is 6.33962308883667 and perplexity is 566.5827197795601
At time: 36.639291286468506 and batch: 250, loss is 6.383857116699219 and perplexity is 592.2075215987553
At time: 38.1711266040802 and batch: 300, loss is 6.333071651458741 and perplexity is 562.8829213038666
At time: 39.70394706726074 and batch: 350, loss is 6.298821353912354 and perplexity is 543.9304306874947
At time: 41.23759722709656 and batch: 400, loss is 6.2884509468078615 and perplexity is 538.3187984322337
At time: 42.770251750946045 and batch: 450, loss is 6.284247159957886 and perplexity is 536.0605708256304
At time: 44.30290412902832 and batch: 500, loss is 6.315558385848999 and perplexity is 553.1108236359344
At time: 45.83702993392944 and batch: 550, loss is 6.272467603683472 and perplexity is 529.783060893547
At time: 47.3708279132843 and batch: 600, loss is 6.305318078994751 and perplexity is 547.4757010132233
At time: 48.90545892715454 and batch: 650, loss is 6.313048391342163 and perplexity is 551.7242593692376
At time: 50.4403555393219 and batch: 700, loss is 6.282000312805176 and perplexity is 534.857476748775
At time: 51.97328782081604 and batch: 750, loss is 6.3232271766662596 and perplexity is 557.3688208181673
At time: 53.50774955749512 and batch: 800, loss is 6.303952760696411 and perplexity is 546.7287324616846
At time: 55.043046951293945 and batch: 850, loss is 6.305310611724853 and perplexity is 547.4716128796651
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.9073530832926435 and perplexity of 367.73151139630295
Finished 2 epochs...
Completing Train Step...
At time: 59.129390478134155 and batch: 50, loss is 6.306918125152588 and perplexity is 548.3523885883945
At time: 60.668455362319946 and batch: 100, loss is 6.255059881210327 and perplexity is 520.6405504103061
At time: 62.218082427978516 and batch: 150, loss is 6.241690435409546 and perplexity is 513.7261983079137
At time: 63.75211477279663 and batch: 200, loss is 6.254798555374146 and perplexity is 520.5045113591539
At time: 65.28399968147278 and batch: 250, loss is 6.310769090652466 and perplexity is 550.4681459582309
At time: 66.81705236434937 and batch: 300, loss is 6.296603651046753 and perplexity is 542.7254912053875
At time: 68.35212087631226 and batch: 350, loss is 6.252751960754394 and perplexity is 519.440338962845
At time: 69.88885045051575 and batch: 400, loss is 6.219467668533325 and perplexity is 502.4356984860127
At time: 71.42049717903137 and batch: 450, loss is 6.209200620651245 and perplexity is 497.30355815969284
At time: 72.95206308364868 and batch: 500, loss is 6.259865074157715 and perplexity is 523.1483491141889
At time: 74.48721528053284 and batch: 550, loss is 6.2225792121887205 and perplexity is 504.0014838373296
At time: 76.0220263004303 and batch: 600, loss is 6.232488069534302 and perplexity is 509.0203873770285
At time: 77.55614185333252 and batch: 650, loss is 6.240746946334839 and perplexity is 513.2417318327706
At time: 79.08847427368164 and batch: 700, loss is 6.2015077495574955 and perplexity is 493.49254360886454
At time: 80.6217315196991 and batch: 750, loss is 6.262936191558838 and perplexity is 524.7574687454363
At time: 82.15731072425842 and batch: 800, loss is 6.231448240280152 and perplexity is 508.49136817976023
At time: 83.69375538825989 and batch: 850, loss is 6.198873767852783 and perplexity is 492.1944036665418
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.857949574788411 and perplexity of 350.00574715391804
Finished 3 epochs...
Completing Train Step...
At time: 87.72691011428833 and batch: 50, loss is 6.211551532745362 and perplexity is 498.47405043214883
At time: 89.26470732688904 and batch: 100, loss is 6.13181432723999 and perplexity is 460.2704848609688
At time: 90.80221390724182 and batch: 150, loss is 6.142045793533325 and perplexity is 465.00390041123353
At time: 92.33658909797668 and batch: 200, loss is 6.163596563339233 and perplexity is 475.13385452617473
At time: 93.89593744277954 and batch: 250, loss is 6.222878522872925 and perplexity is 504.1523594445114
At time: 95.42686820030212 and batch: 300, loss is 6.180325012207032 and perplexity is 483.1489601471912
At time: 96.96522641181946 and batch: 350, loss is 6.144664239883423 and perplexity is 466.22308366342867
At time: 98.50264120101929 and batch: 400, loss is 6.127145261764526 and perplexity is 458.1264610195836
At time: 100.03720283508301 and batch: 450, loss is 6.1250621032714845 and perplexity is 457.1731043327246
At time: 101.57482099533081 and batch: 500, loss is 6.174251279830933 and perplexity is 480.2233363872465
At time: 103.11976408958435 and batch: 550, loss is 6.1503466033935545 and perplexity is 468.87987397697344
At time: 104.65705585479736 and batch: 600, loss is 6.1609391021728515 and perplexity is 473.8728809951268
At time: 106.19511985778809 and batch: 650, loss is 6.185119457244873 and perplexity is 485.4709531677529
At time: 107.73028135299683 and batch: 700, loss is 6.1265185356140135 and perplexity is 457.8394311401816
At time: 109.26780891418457 and batch: 750, loss is 6.195626230239868 and perplexity is 490.5985764846992
At time: 110.80399703979492 and batch: 800, loss is 6.162522974014283 and perplexity is 474.62402961229384
At time: 112.34042143821716 and batch: 850, loss is 6.138861265182495 and perplexity is 463.52543765985246
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.748599370320638 and perplexity of 313.75090356197063
Finished 4 epochs...
Completing Train Step...
At time: 116.34278345108032 and batch: 50, loss is 6.156589670181274 and perplexity is 471.8162788942281
At time: 117.88179922103882 and batch: 100, loss is 6.0975542163848875 and perplexity is 444.76863089608975
At time: 119.41922116279602 and batch: 150, loss is 6.106094884872436 and perplexity is 448.5835199886636
At time: 120.95650577545166 and batch: 200, loss is 6.131280698776245 and perplexity is 460.02493695072917
At time: 122.49457955360413 and batch: 250, loss is 6.189216842651367 and perplexity is 487.4641955189605
At time: 124.03226923942566 and batch: 300, loss is 6.1607006168365475 and perplexity is 473.75988273648676
At time: 125.57129716873169 and batch: 350, loss is 6.115254163742065 and perplexity is 452.71109549614994
At time: 127.11062264442444 and batch: 400, loss is 6.0692148113250735 and perplexity is 432.3410789469607
At time: 128.65075254440308 and batch: 450, loss is 6.087922067642212 and perplexity is 440.50511963791394
At time: 130.1904740333557 and batch: 500, loss is 6.1487594413757325 and perplexity is 468.13627591147787
At time: 131.7588677406311 and batch: 550, loss is 6.1010919952392575 and perplexity is 446.34491057397406
At time: 133.29648804664612 and batch: 600, loss is 6.1189247989654545 and perplexity is 454.3758863400311
At time: 134.83503890037537 and batch: 650, loss is 6.135758171081543 and perplexity is 462.089303989969
At time: 136.3728642463684 and batch: 700, loss is 6.093444423675537 and perplexity is 442.9444750382668
At time: 137.91669178009033 and batch: 750, loss is 6.150701837539673 and perplexity is 469.04646570642126
At time: 139.4517743587494 and batch: 800, loss is 6.1321438026428225 and perplexity is 460.4221576492347
At time: 140.99085140228271 and batch: 850, loss is 6.092226676940918 and perplexity is 442.40540913965543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.772642771402995 and perplexity of 321.3859609438345
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 145.0208580493927 and batch: 50, loss is 6.049377279281616 and perplexity is 423.84900828845963
At time: 146.559574842453 and batch: 100, loss is 5.904765672683716 and perplexity is 366.78126884643837
At time: 148.0997598171234 and batch: 150, loss is 5.863148508071899 and perplexity is 351.8301420265398
At time: 149.65073919296265 and batch: 200, loss is 5.859938259124756 and perplexity is 350.7024906727476
At time: 151.20122909545898 and batch: 250, loss is 5.900518455505371 and perplexity is 365.2267726190158
At time: 152.74100422859192 and batch: 300, loss is 5.867385330200196 and perplexity is 353.32394601401086
At time: 154.28012108802795 and batch: 350, loss is 5.818958330154419 and perplexity is 336.6212227596937
At time: 155.8200604915619 and batch: 400, loss is 5.810651054382324 and perplexity is 333.83640058958355
At time: 157.35832500457764 and batch: 450, loss is 5.807397203445435 and perplexity is 332.75191204363205
At time: 158.89983463287354 and batch: 500, loss is 5.819274778366089 and perplexity is 336.7277627999598
At time: 160.43988370895386 and batch: 550, loss is 5.7855477714538575 and perplexity is 325.56032393073883
At time: 161.9825463294983 and batch: 600, loss is 5.758534908294678 and perplexity is 316.8837249402778
At time: 163.5208990573883 and batch: 650, loss is 5.7632242202758786 and perplexity is 318.3731811212616
At time: 165.06109809875488 and batch: 700, loss is 5.711937952041626 and perplexity is 302.4566471645018
At time: 166.60458827018738 and batch: 750, loss is 5.712784156799317 and perplexity is 302.7126957378092
At time: 168.14798641204834 and batch: 800, loss is 5.679721574783326 and perplexity is 292.8678767678039
At time: 169.68807005882263 and batch: 850, loss is 5.691333160400391 and perplexity is 296.28835736346963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.417104721069336 and perplexity of 225.2260852564466
Finished 6 epochs...
Completing Train Step...
At time: 173.72946119308472 and batch: 50, loss is 5.780713453292846 and perplexity is 323.9902598965742
At time: 175.29674124717712 and batch: 100, loss is 5.714297246932984 and perplexity is 303.17107402744705
At time: 176.84139847755432 and batch: 150, loss is 5.7064377880096435 and perplexity is 300.79765254609464
At time: 178.38294315338135 and batch: 200, loss is 5.7316939163208005 and perplexity is 308.4913846331993
At time: 179.92075037956238 and batch: 250, loss is 5.772470769882202 and perplexity is 321.3306868235432
At time: 181.45807600021362 and batch: 300, loss is 5.741193046569824 and perplexity is 311.4357467763115
At time: 182.99340534210205 and batch: 350, loss is 5.697478103637695 and perplexity is 298.1146379498692
At time: 184.53201842308044 and batch: 400, loss is 5.70994875907898 and perplexity is 301.8556005275165
At time: 186.07041907310486 and batch: 450, loss is 5.712801074981689 and perplexity is 302.7178171297243
At time: 187.61148309707642 and batch: 500, loss is 5.728746356964112 and perplexity is 307.58342675320756
At time: 189.14810943603516 and batch: 550, loss is 5.708245544433594 and perplexity is 301.3419132319501
At time: 190.6873710155487 and batch: 600, loss is 5.695083074569702 and perplexity is 297.4014990614587
At time: 192.22527956962585 and batch: 650, loss is 5.715888891220093 and perplexity is 303.6539987556813
At time: 193.7700972557068 and batch: 700, loss is 5.6680332469940184 and perplexity is 289.4646686756287
At time: 195.31133103370667 and batch: 750, loss is 5.676599016189575 and perplexity is 291.95480596281186
At time: 196.8498456478119 and batch: 800, loss is 5.671016683578491 and perplexity is 290.32955768730363
At time: 198.38521099090576 and batch: 850, loss is 5.687249364852906 and perplexity is 295.08084358677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.404579162597656 and perplexity of 222.42259703711386
Finished 7 epochs...
Completing Train Step...
At time: 202.38723802566528 and batch: 50, loss is 5.731141490936279 and perplexity is 308.3210132245
At time: 203.95164251327515 and batch: 100, loss is 5.667142562866211 and perplexity is 289.2069618743938
At time: 205.486412525177 and batch: 150, loss is 5.665400705337524 and perplexity is 288.70364303262
At time: 207.02402806282043 and batch: 200, loss is 5.694145593643189 and perplexity is 297.12282147644254
At time: 208.5639295578003 and batch: 250, loss is 5.725093393325806 and perplexity is 306.4618854015879
At time: 210.12821745872498 and batch: 300, loss is 5.690460929870605 and perplexity is 296.03003828582723
At time: 211.66567635536194 and batch: 350, loss is 5.654625673294067 and perplexity is 285.60955143157145
At time: 213.2026653289795 and batch: 400, loss is 5.678725547790528 and perplexity is 292.5763176816836
At time: 214.7421371936798 and batch: 450, loss is 5.684803409576416 and perplexity is 294.35997101084513
At time: 216.28679752349854 and batch: 500, loss is 5.702262201309204 and perplexity is 299.54426450494
At time: 217.82451486587524 and batch: 550, loss is 5.686636257171631 and perplexity is 294.8999827042335
At time: 219.36319971084595 and batch: 600, loss is 5.678722972869873 and perplexity is 292.57556432185004
At time: 220.901136636734 and batch: 650, loss is 5.707488470077514 and perplexity is 301.11386133404466
At time: 222.44049835205078 and batch: 700, loss is 5.662293901443482 and perplexity is 287.8080893054661
At time: 223.97834753990173 and batch: 750, loss is 5.672097759246826 and perplexity is 290.6435956267201
At time: 225.51726055145264 and batch: 800, loss is 5.665385093688965 and perplexity is 288.69913592798883
At time: 227.05628299713135 and batch: 850, loss is 5.6782377815246585 and perplexity is 292.4336436223515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.398998260498047 and perplexity of 221.18473570268478
Finished 8 epochs...
Completing Train Step...
At time: 231.04601001739502 and batch: 50, loss is 5.707672243118286 and perplexity is 301.1692030289605
At time: 232.608252286911 and batch: 100, loss is 5.647971181869507 and perplexity is 283.7152748347141
At time: 234.14880633354187 and batch: 150, loss is 5.64832760810852 and perplexity is 283.8164164267618
At time: 235.68823289871216 and batch: 200, loss is 5.677655057907105 and perplexity is 292.2632852723831
At time: 237.226793050766 and batch: 250, loss is 5.7079606533050535 and perplexity is 301.25607582195175
At time: 238.7694010734558 and batch: 300, loss is 5.674753456115723 and perplexity is 291.4164827362826
At time: 240.32010507583618 and batch: 350, loss is 5.640522356033325 and perplexity is 281.60978063049
At time: 241.86325931549072 and batch: 400, loss is 5.661929559707642 and perplexity is 287.70324790682815
At time: 243.40239691734314 and batch: 450, loss is 5.675334482192993 and perplexity is 291.58585251146206
At time: 244.94267749786377 and batch: 500, loss is 5.693576726913452 and perplexity is 296.95384625540675
At time: 246.48196649551392 and batch: 550, loss is 5.675634040832519 and perplexity is 291.67321265683523
At time: 248.02203965187073 and batch: 600, loss is 5.670228548049927 and perplexity is 290.1008287944211
At time: 249.5882773399353 and batch: 650, loss is 5.699878244400025 and perplexity is 298.83101440252136
At time: 251.12992906570435 and batch: 700, loss is 5.6569312477111815 and perplexity is 286.26880519390863
At time: 252.66996717453003 and batch: 750, loss is 5.664073753356933 and perplexity is 288.3208012242177
At time: 254.21052145957947 and batch: 800, loss is 5.657253770828247 and perplexity is 286.36114839188605
At time: 255.750314950943 and batch: 850, loss is 5.670904321670532 and perplexity is 290.29693753693044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.39332898457845 and perplexity of 219.93432621406836
Finished 9 epochs...
Completing Train Step...
At time: 259.7772488594055 and batch: 50, loss is 5.6952965259552 and perplexity is 297.46498659899305
At time: 261.3178884983063 and batch: 100, loss is 5.636840105056763 and perplexity is 280.5747295691632
At time: 262.85771656036377 and batch: 150, loss is 5.637095432281495 and perplexity is 280.64637708258516
At time: 264.3962211608887 and batch: 200, loss is 5.6658494091033935 and perplexity is 288.8332145119003
At time: 265.9373507499695 and batch: 250, loss is 5.696489372253418 and perplexity is 297.8200283201099
At time: 267.4771885871887 and batch: 300, loss is 5.663333072662353 and perplexity is 288.1073266409158
At time: 269.0150957107544 and batch: 350, loss is 5.628156270980835 and perplexity is 278.14881354244943
At time: 270.5556700229645 and batch: 400, loss is 5.649727153778076 and perplexity is 284.2139085526309
At time: 272.0944240093231 and batch: 450, loss is 5.660905380249023 and perplexity is 287.40873899087876
At time: 273.6344413757324 and batch: 500, loss is 5.667630949020386 and perplexity is 289.348241046852
At time: 275.17191553115845 and batch: 550, loss is 5.6434463787078855 and perplexity is 282.4344190576125
At time: 276.7107970714569 and batch: 600, loss is 5.637706356048584 and perplexity is 280.81788300768113
At time: 278.2505257129669 and batch: 650, loss is 5.662745933532715 and perplexity is 287.93821720615085
At time: 279.7919011116028 and batch: 700, loss is 5.618394641876221 and perplexity is 275.4468372895493
At time: 281.3317093849182 and batch: 750, loss is 5.625233926773071 and perplexity is 277.33715352091644
At time: 282.87209129333496 and batch: 800, loss is 5.615206079483032 and perplexity is 274.5699566001718
At time: 284.413006067276 and batch: 850, loss is 5.624857606887818 and perplexity is 277.2328056704439
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.358646392822266 and perplexity of 212.43719523014354
Finished 10 epochs...
Completing Train Step...
At time: 288.4221570491791 and batch: 50, loss is 5.647625064849853 and perplexity is 283.6170931415157
At time: 289.96150612831116 and batch: 100, loss is 5.591445722579956 and perplexity is 268.12297107836554
At time: 291.5011475086212 and batch: 150, loss is 5.590830202102661 and perplexity is 267.9579866800804
At time: 293.0382525920868 and batch: 200, loss is 5.617830648422241 and perplexity is 275.2915308764155
At time: 294.5786955356598 and batch: 250, loss is 5.647546424865722 and perplexity is 283.5947903747679
At time: 296.1164333820343 and batch: 300, loss is 5.613608150482178 and perplexity is 274.1315636574374
At time: 297.654661655426 and batch: 350, loss is 5.579868373870849 and perplexity is 265.0367177329495
At time: 299.1970794200897 and batch: 400, loss is 5.603434419631958 and perplexity is 271.35676188815205
At time: 300.7377610206604 and batch: 450, loss is 5.615711841583252 and perplexity is 274.7088588008526
At time: 302.2776606082916 and batch: 500, loss is 5.63008599281311 and perplexity is 278.6860816027923
At time: 303.8153746128082 and batch: 550, loss is 5.61458044052124 and perplexity is 274.3982286630308
At time: 305.35472321510315 and batch: 600, loss is 5.6162597179412845 and perplexity is 274.85940652693586
At time: 306.8934328556061 and batch: 650, loss is 5.644846067428589 and perplexity is 282.83001612009366
At time: 308.4331965446472 and batch: 700, loss is 5.600633382797241 and perplexity is 270.59774511598073
At time: 309.97005319595337 and batch: 750, loss is 5.604283962249756 and perplexity is 271.5873889719013
At time: 311.50785517692566 and batch: 800, loss is 5.595352125167847 and perplexity is 269.1724157886833
At time: 313.04782032966614 and batch: 850, loss is 5.602860460281372 and perplexity is 271.20105882521057
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.34946886698405 and perplexity of 210.49646654752627
Finished 11 epochs...
Completing Train Step...
At time: 317.07261538505554 and batch: 50, loss is 5.626378040313721 and perplexity is 277.6546402994137
At time: 318.6130259037018 and batch: 100, loss is 5.573932752609253 and perplexity is 263.4682197660658
At time: 320.1547884941101 and batch: 150, loss is 5.575660238265991 and perplexity is 263.923750684999
At time: 321.6945688724518 and batch: 200, loss is 5.604207134246826 and perplexity is 271.56652425669307
At time: 323.23842191696167 and batch: 250, loss is 5.631734981536865 and perplexity is 279.1460109137278
At time: 324.7899613380432 and batch: 300, loss is 5.59815034866333 and perplexity is 269.92667516787975
At time: 326.33173632621765 and batch: 350, loss is 5.567590665817261 and perplexity is 261.8025688835979
At time: 327.9012417793274 and batch: 400, loss is 5.590026044845581 and perplexity is 267.7425929373272
At time: 329.4472508430481 and batch: 450, loss is 5.601071166992187 and perplexity is 270.71623446647055
At time: 330.98650884628296 and batch: 500, loss is 5.618709602355957 and perplexity is 275.53360582117574
At time: 332.52730321884155 and batch: 550, loss is 5.602843227386475 and perplexity is 271.19638528613734
At time: 334.0688827037811 and batch: 600, loss is 5.604926729202271 and perplexity is 271.76201248535193
At time: 335.61593317985535 and batch: 650, loss is 5.6349594783782955 and perplexity is 280.0475690988238
At time: 337.1581840515137 and batch: 700, loss is 5.590263156890869 and perplexity is 267.8060854582725
At time: 338.6978802680969 and batch: 750, loss is 5.591037826538086 and perplexity is 268.013627081736
At time: 340.239590883255 and batch: 800, loss is 5.580859603881836 and perplexity is 265.2995603287935
At time: 341.77933645248413 and batch: 850, loss is 5.585872745513916 and perplexity is 266.63288387716767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.344621658325195 and perplexity of 209.47861511426333
Finished 12 epochs...
Completing Train Step...
At time: 345.76321482658386 and batch: 50, loss is 5.614252882003784 and perplexity is 274.3083619051719
At time: 347.3236472606659 and batch: 100, loss is 5.563854179382324 and perplexity is 260.82617241838904
At time: 348.86183166503906 and batch: 150, loss is 5.560403795242309 and perplexity is 259.9277727330871
At time: 350.40198850631714 and batch: 200, loss is 5.591263093948364 and perplexity is 268.0740086181688
At time: 351.94175004959106 and batch: 250, loss is 5.620771856307983 and perplexity is 276.10241239891354
At time: 353.4830391407013 and batch: 300, loss is 5.586353845596314 and perplexity is 266.76119184158233
At time: 355.02245831489563 and batch: 350, loss is 5.556013317108154 and perplexity is 258.7890670907234
At time: 356.561931848526 and batch: 400, loss is 5.579180316925049 and perplexity is 264.85442010118913
At time: 358.10519671440125 and batch: 450, loss is 5.587580022811889 and perplexity is 267.0884889581439
At time: 359.64427423477173 and batch: 500, loss is 5.603653564453125 and perplexity is 271.4162348335645
At time: 361.18585753440857 and batch: 550, loss is 5.5909616565704345 and perplexity is 267.9932132699006
At time: 362.72630858421326 and batch: 600, loss is 5.5941715431213375 and perplexity is 268.85482317614355
At time: 364.26693773269653 and batch: 650, loss is 5.624417858123779 and perplexity is 277.1109196883686
At time: 365.832407951355 and batch: 700, loss is 5.579658403396606 and perplexity is 264.9810736896428
At time: 367.3706121444702 and batch: 750, loss is 5.582149677276611 and perplexity is 265.64203709580346
At time: 368.91167545318604 and batch: 800, loss is 5.567621068954468 and perplexity is 261.8105286240208
At time: 370.4552481174469 and batch: 850, loss is 5.572675704956055 and perplexity is 263.1372357336512
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.335448582967122 and perplexity of 207.56583842684418
Finished 13 epochs...
Completing Train Step...
At time: 374.43149995803833 and batch: 50, loss is 5.597029399871826 and perplexity is 269.624270709177
At time: 376.0105245113373 and batch: 100, loss is 5.5507901000976565 and perplexity is 257.4408796362893
At time: 377.54632568359375 and batch: 150, loss is 5.552070770263672 and perplexity is 257.7707876969671
At time: 379.0858633518219 and batch: 200, loss is 5.582268562316894 and perplexity is 265.6736198374051
At time: 380.63156032562256 and batch: 250, loss is 5.609816312789917 and perplexity is 273.09406950952393
At time: 382.18255710601807 and batch: 300, loss is 5.574950819015503 and perplexity is 263.7365844931015
At time: 383.72256088256836 and batch: 350, loss is 5.542502794265747 and perplexity is 255.31620440991597
At time: 385.2628803253174 and batch: 400, loss is 5.566899909973144 and perplexity is 261.6217896734729
At time: 386.8084514141083 and batch: 450, loss is 5.577753791809082 and perplexity is 264.4768679766597
At time: 388.36770725250244 and batch: 500, loss is 5.594704065322876 and perplexity is 268.9980324661545
At time: 389.9065980911255 and batch: 550, loss is 5.581491441726684 and perplexity is 265.4672395987066
At time: 391.44441175460815 and batch: 600, loss is 5.584342842102051 and perplexity is 266.22527320039575
At time: 392.98324632644653 and batch: 650, loss is 5.613917007446289 and perplexity is 274.21624417636644
At time: 394.5225920677185 and batch: 700, loss is 5.5718035221099855 and perplexity is 262.90783200602266
At time: 396.06148052215576 and batch: 750, loss is 5.574970664978028 and perplexity is 263.7418186514121
At time: 397.60245084762573 and batch: 800, loss is 5.558113594055175 and perplexity is 259.33316698269573
At time: 399.1440348625183 and batch: 850, loss is 5.5636221790313725 and perplexity is 260.76566767368297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.329346338907878 and perplexity of 206.30307777772595
Finished 14 epochs...
Completing Train Step...
At time: 403.14846658706665 and batch: 50, loss is 5.587327375411987 and perplexity is 267.0210182693707
At time: 404.7332646846771 and batch: 100, loss is 5.541271448135376 and perplexity is 255.0020152670855
At time: 406.27322363853455 and batch: 150, loss is 5.54187313079834 and perplexity is 255.15549172612472
At time: 407.81341671943665 and batch: 200, loss is 5.57248779296875 and perplexity is 263.0877937382679
At time: 409.35306453704834 and batch: 250, loss is 5.596578731536865 and perplexity is 269.50278696452585
At time: 410.8919825553894 and batch: 300, loss is 5.559700403213501 and perplexity is 259.7450058955846
At time: 412.42989563941956 and batch: 350, loss is 5.528464078903198 and perplexity is 251.7569351294362
At time: 413.97145557403564 and batch: 400, loss is 5.554150400161743 and perplexity is 258.3074133318656
At time: 415.5103905200958 and batch: 450, loss is 5.562481451034546 and perplexity is 260.4683745734757
At time: 417.04834604263306 and batch: 500, loss is 5.576380472183228 and perplexity is 264.1139059917374
At time: 418.5905330181122 and batch: 550, loss is 5.560515670776367 and perplexity is 259.9568539181844
At time: 420.1324484348297 and batch: 600, loss is 5.568136854171753 and perplexity is 261.945601455705
At time: 421.671662569046 and batch: 650, loss is 5.594314661026001 and perplexity is 268.8933038686675
At time: 423.2110118865967 and batch: 700, loss is 5.550834465026855 and perplexity is 257.4523012360446
At time: 424.7503089904785 and batch: 750, loss is 5.551803617477417 and perplexity is 257.701932710629
At time: 426.2909297943115 and batch: 800, loss is 5.534499959945679 and perplexity is 253.28110526896234
At time: 427.8324520587921 and batch: 850, loss is 5.539014863967895 and perplexity is 254.4272305257402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.310060818990071 and perplexity of 202.36253549892805
Finished 15 epochs...
Completing Train Step...
At time: 431.8702952861786 and batch: 50, loss is 5.562265434265137 and perplexity is 260.41211511337934
At time: 433.41071462631226 and batch: 100, loss is 5.516082019805908 and perplexity is 248.6588855782797
At time: 434.9542746543884 and batch: 150, loss is 5.513427476882935 and perplexity is 247.99968521825207
At time: 436.49385499954224 and batch: 200, loss is 5.5446598339080815 and perplexity is 255.86752598163474
At time: 438.03074622154236 and batch: 250, loss is 5.570559577941895 and perplexity is 262.58099266874297
At time: 439.56957173347473 and batch: 300, loss is 5.535285949707031 and perplexity is 253.48025988093795
At time: 441.1085662841797 and batch: 350, loss is 5.504423151016235 and perplexity is 245.77663877497554
At time: 442.65106439590454 and batch: 400, loss is 5.531019430160523 and perplexity is 252.40108519448341
At time: 444.23257422447205 and batch: 450, loss is 5.539104290008545 and perplexity is 254.44998396295938
At time: 445.7758147716522 and batch: 500, loss is 5.555213813781738 and perplexity is 258.58224705841286
At time: 447.31836891174316 and batch: 550, loss is 5.540965280532837 and perplexity is 254.92395386197495
At time: 448.85983538627625 and batch: 600, loss is 5.5467507266998295 and perplexity is 256.4030772423758
At time: 450.3989350795746 and batch: 650, loss is 5.572085790634155 and perplexity is 262.9820530864002
At time: 451.93735671043396 and batch: 700, loss is 5.52988374710083 and perplexity is 252.11460026661143
At time: 453.4782087802887 and batch: 750, loss is 5.527388362884522 and perplexity is 251.48626177143922
At time: 455.01615047454834 and batch: 800, loss is 5.513658437728882 and perplexity is 248.05697005036666
At time: 456.556823015213 and batch: 850, loss is 5.51898567199707 and perplexity is 249.38195375746506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.298043568929036 and perplexity of 199.9452479720294
Finished 16 epochs...
Completing Train Step...
At time: 460.58267641067505 and batch: 50, loss is 5.542887163162232 and perplexity is 255.41435888021385
At time: 462.12207293510437 and batch: 100, loss is 5.495759305953979 and perplexity is 243.6564657440859
At time: 463.6626284122467 and batch: 150, loss is 5.494479494094849 and perplexity is 243.3448307692771
At time: 465.1989960670471 and batch: 200, loss is 5.526425676345825 and perplexity is 251.24427582955784
At time: 466.73907589912415 and batch: 250, loss is 5.549184923171997 and perplexity is 257.02797295936847
At time: 468.2782552242279 and batch: 300, loss is 5.504046230316162 and perplexity is 245.68401792868062
At time: 469.81819581985474 and batch: 350, loss is 5.48042366027832 and perplexity is 239.9483424249023
At time: 471.3576924800873 and batch: 400, loss is 5.510982704162598 and perplexity is 247.39412288592968
At time: 472.8984999656677 and batch: 450, loss is 5.515561418533325 and perplexity is 248.5294671366343
At time: 474.43762588500977 and batch: 500, loss is 5.529071226119995 and perplexity is 251.90983506359706
At time: 475.98233556747437 and batch: 550, loss is 5.51527738571167 and perplexity is 248.45888663488304
At time: 477.5269441604614 and batch: 600, loss is 5.52093731880188 and perplexity is 249.869134498425
At time: 479.0698096752167 and batch: 650, loss is 5.544968099594116 and perplexity is 255.94641331857025
At time: 480.6236424446106 and batch: 700, loss is 5.5035285472869875 and perplexity is 245.55686439751173
At time: 482.1689021587372 and batch: 750, loss is 5.498130617141723 and perplexity is 244.23493664346137
At time: 483.73740577697754 and batch: 800, loss is 5.491782321929931 and perplexity is 242.6893722049046
At time: 485.28005266189575 and batch: 850, loss is 5.497658758163452 and perplexity is 244.11971938108636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.280778249104817 and perplexity of 196.522759514769
Finished 17 epochs...
Completing Train Step...
At time: 489.30806159973145 and batch: 50, loss is 5.518121891021728 and perplexity is 249.16663537756358
At time: 490.8502571582794 and batch: 100, loss is 5.466957721710205 and perplexity is 236.7388705598227
At time: 492.39225721359253 and batch: 150, loss is 5.470697584152222 and perplexity is 237.62589901873045
At time: 493.93249344825745 and batch: 200, loss is 5.50362271308899 and perplexity is 245.57998854531974
At time: 495.4721076488495 and batch: 250, loss is 5.524558401107788 and perplexity is 250.77557135092272
At time: 497.0133607387543 and batch: 300, loss is 5.4835583782196045 and perplexity is 240.7016929526938
At time: 498.5516905784607 and batch: 350, loss is 5.460583620071411 and perplexity is 235.23467198717287
At time: 500.08908462524414 and batch: 400, loss is 5.490659275054932 and perplexity is 242.41697365065667
At time: 501.6257996559143 and batch: 450, loss is 5.495993204116822 and perplexity is 243.71346320932946
At time: 503.1656286716461 and batch: 500, loss is 5.511399850845337 and perplexity is 247.49734405130732
At time: 504.7047643661499 and batch: 550, loss is 5.494663829803467 and perplexity is 243.3896920457361
At time: 506.243723154068 and batch: 600, loss is 5.5003267097473145 and perplexity is 244.7718885641204
At time: 507.78234219551086 and batch: 650, loss is 5.525200262069702 and perplexity is 250.93658606937763
At time: 509.3233993053436 and batch: 700, loss is 5.483261260986328 and perplexity is 240.63018695499693
At time: 510.86082553863525 and batch: 750, loss is 5.4800425624847415 and perplexity is 239.85691606333023
At time: 512.3985559940338 and batch: 800, loss is 5.474079904556274 and perplexity is 238.43098671025282
At time: 513.9364109039307 and batch: 850, loss is 5.473780994415283 and perplexity is 238.3597279209166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.2704010009765625 and perplexity of 194.49393906949905
Finished 18 epochs...
Completing Train Step...
At time: 517.9276793003082 and batch: 50, loss is 5.500133590698242 and perplexity is 244.72462301384724
At time: 519.4947998523712 and batch: 100, loss is 5.447011585235596 and perplexity is 232.0636262712691
At time: 521.0341017246246 and batch: 150, loss is 5.452381801605225 and perplexity is 233.31321042141073
At time: 522.59765458107 and batch: 200, loss is 5.4863230419158935 and perplexity is 241.36807291863656
At time: 524.1374137401581 and batch: 250, loss is 5.508439455032349 and perplexity is 246.76573740718825
At time: 525.6769351959229 and batch: 300, loss is 5.463414001464844 and perplexity is 235.90141895484896
At time: 527.2178483009338 and batch: 350, loss is 5.441248979568481 and perplexity is 230.73018085349193
At time: 528.7599902153015 and batch: 400, loss is 5.471376295089722 and perplexity is 237.78723305882326
At time: 530.2984216213226 and batch: 450, loss is 5.478285093307495 and perplexity is 239.43574513227009
At time: 531.8392949104309 and batch: 500, loss is 5.4933890533447265 and perplexity is 243.07962227282968
At time: 533.3827791213989 and batch: 550, loss is 5.480108413696289 and perplexity is 239.87271145191812
At time: 534.9209620952606 and batch: 600, loss is 5.488763494491577 and perplexity is 241.95783960998767
At time: 536.461993932724 and batch: 650, loss is 5.50977689743042 and perplexity is 247.0959931665866
At time: 538.00550532341 and batch: 700, loss is 5.467610626220703 and perplexity is 236.8934889062367
At time: 539.5452787876129 and batch: 750, loss is 5.471077489852905 and perplexity is 237.7161916026453
At time: 541.0858845710754 and batch: 800, loss is 5.4591310024261475 and perplexity is 234.89321401584618
At time: 542.6269896030426 and batch: 850, loss is 5.4599755859375 and perplexity is 235.09168475209052
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.265480041503906 and perplexity of 193.53919333685286
Finished 19 epochs...
Completing Train Step...
At time: 546.6076016426086 and batch: 50, loss is 5.486515655517578 and perplexity is 241.41456817015822
At time: 548.1750147342682 and batch: 100, loss is 5.434710178375244 and perplexity is 229.22640387889288
At time: 549.7207679748535 and batch: 150, loss is 5.438958024978637 and perplexity is 230.2021935150855
At time: 551.2610599994659 and batch: 200, loss is 5.472164583206177 and perplexity is 237.97475180857901
At time: 552.8013782501221 and batch: 250, loss is 5.4929854106903075 and perplexity is 242.98152476836077
At time: 554.3451743125916 and batch: 300, loss is 5.449771814346313 and perplexity is 232.70505989266928
At time: 555.8838937282562 and batch: 350, loss is 5.4282298755645755 and perplexity is 227.74575009358463
At time: 557.4229378700256 and batch: 400, loss is 5.457218132019043 and perplexity is 234.4443232097721
At time: 558.9626150131226 and batch: 450, loss is 5.467303295135498 and perplexity is 236.82069535964578
At time: 560.5472497940063 and batch: 500, loss is 5.480887823104858 and perplexity is 240.0597433778359
At time: 562.0882890224457 and batch: 550, loss is 5.4696486377716065 and perplexity is 237.37677287484024
At time: 563.6291720867157 and batch: 600, loss is 5.474235277175904 and perplexity is 238.46803523534757
At time: 565.1695692539215 and batch: 650, loss is 5.497222814559937 and perplexity is 244.01332014464845
At time: 566.711245059967 and batch: 700, loss is 5.454881019592285 and perplexity is 233.89704024862976
At time: 568.256245136261 and batch: 750, loss is 5.457017393112182 and perplexity is 234.39726583589223
At time: 569.7967667579651 and batch: 800, loss is 5.443525648117065 and perplexity is 231.25607541610316
At time: 571.338974237442 and batch: 850, loss is 5.445287628173828 and perplexity is 231.66390319568777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.257591247558594 and perplexity of 192.0184089843874
Finished 20 epochs...
Completing Train Step...
At time: 575.3259034156799 and batch: 50, loss is 5.473006763458252 and perplexity is 238.17525386262906
At time: 576.8883924484253 and batch: 100, loss is 5.4208753967285155 and perplexity is 226.07694291995205
At time: 578.4279069900513 and batch: 150, loss is 5.425790634155273 and perplexity is 227.19090021067728
At time: 579.9677586555481 and batch: 200, loss is 5.462366275787353 and perplexity is 235.6543884135719
At time: 581.5088295936584 and batch: 250, loss is 5.478188238143921 and perplexity is 239.41255566703774
At time: 583.0493814945221 and batch: 300, loss is 5.434972324371338 and perplexity is 229.2865025398362
At time: 584.5892291069031 and batch: 350, loss is 5.413754568099976 and perplexity is 224.47280592318222
At time: 586.1287424564362 and batch: 400, loss is 5.4494383335113525 and perplexity is 232.62747015306132
At time: 587.6720926761627 and batch: 450, loss is 5.452230978012085 and perplexity is 233.27802393823055
At time: 589.2130558490753 and batch: 500, loss is 5.467721319198608 and perplexity is 236.91971280334383
At time: 590.7517385482788 and batch: 550, loss is 5.458770084381103 and perplexity is 234.80845211321966
At time: 592.2907786369324 and batch: 600, loss is 5.464031620025635 and perplexity is 236.04716105159167
At time: 593.8305444717407 and batch: 650, loss is 5.485563554763794 and perplexity is 241.1848265637623
At time: 595.3718838691711 and batch: 700, loss is 5.444898014068603 and perplexity is 231.5736612522399
At time: 596.9143557548523 and batch: 750, loss is 5.444512100219726 and perplexity is 231.48431101118413
At time: 598.4656939506531 and batch: 800, loss is 5.431366672515869 and perplexity is 228.46126389220825
At time: 600.0356059074402 and batch: 850, loss is 5.435555505752563 and perplexity is 229.42025715689402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.245807647705078 and perplexity of 189.76901986689535
Finished 21 epochs...
Completing Train Step...
At time: 604.0447628498077 and batch: 50, loss is 5.4595399093627925 and perplexity is 234.98928312075122
At time: 605.5819356441498 and batch: 100, loss is 5.404813032150269 and perplexity is 222.4746209935465
At time: 607.122225522995 and batch: 150, loss is 5.413261394500733 and perplexity is 224.36212915522614
At time: 608.6611089706421 and batch: 200, loss is 5.450238218307495 and perplexity is 232.81361976880052
At time: 610.2025077342987 and batch: 250, loss is 5.4639315700531 and perplexity is 236.02354572098793
At time: 611.7414035797119 and batch: 300, loss is 5.421628494262695 and perplexity is 226.2472650347323
At time: 613.2808096408844 and batch: 350, loss is 5.400737352371216 and perplexity is 221.569730953037
At time: 614.8223359584808 and batch: 400, loss is 5.434820747375488 and perplexity is 229.251750614455
At time: 616.3614077568054 and batch: 450, loss is 5.436813650131225 and perplexity is 229.709082617753
At time: 617.9020767211914 and batch: 500, loss is 5.454039392471313 and perplexity is 233.700268971678
At time: 619.4424738883972 and batch: 550, loss is 5.445663070678711 and perplexity is 231.75089600117096
At time: 620.9834125041962 and batch: 600, loss is 5.45063099861145 and perplexity is 232.90508233430933
At time: 622.5237512588501 and batch: 650, loss is 5.47214225769043 and perplexity is 237.96943895881634
At time: 624.0648419857025 and batch: 700, loss is 5.429580574035644 and perplexity is 228.05357377171507
At time: 625.6046330928802 and batch: 750, loss is 5.428237113952637 and perplexity is 227.74739861166944
At time: 627.1437890529633 and batch: 800, loss is 5.419820957183838 and perplexity is 225.83868408804003
At time: 628.6865911483765 and batch: 850, loss is 5.416488933563232 and perplexity is 225.08743654055175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.240766843159993 and perplexity of 188.81483827095246
Finished 22 epochs...
Completing Train Step...
At time: 632.7140095233917 and batch: 50, loss is 5.447016448974609 and perplexity is 232.06475497092663
At time: 634.2520716190338 and batch: 100, loss is 5.395407123565674 and perplexity is 220.39185555212063
At time: 635.79523396492 and batch: 150, loss is 5.400549564361572 and perplexity is 221.52812672077428
At time: 637.3372490406036 and batch: 200, loss is 5.437732553482055 and perplexity is 229.9202600744633
At time: 638.9038372039795 and batch: 250, loss is 5.452709178924561 and perplexity is 233.38960437894642
At time: 640.4487292766571 and batch: 300, loss is 5.411395168304443 and perplexity is 223.94380913364307
At time: 641.9915807247162 and batch: 350, loss is 5.387577905654907 and perplexity is 218.67309673739769
At time: 643.5343475341797 and batch: 400, loss is 5.419873247146606 and perplexity is 225.85049349317669
At time: 645.0738112926483 and batch: 450, loss is 5.423428726196289 and perplexity is 226.65492942126187
At time: 646.6143600940704 and batch: 500, loss is 5.439345664978028 and perplexity is 230.29144639111408
At time: 648.1561768054962 and batch: 550, loss is 5.431492300033569 and perplexity is 228.48996671657613
At time: 649.7044589519501 and batch: 600, loss is 5.439033813476563 and perplexity is 230.21964085469662
At time: 651.2487359046936 and batch: 650, loss is 5.458690757751465 and perplexity is 234.7898262888746
At time: 652.7971160411835 and batch: 700, loss is 5.415898361206055 and perplexity is 224.95454536737208
At time: 654.3452455997467 and batch: 750, loss is 5.414968585968017 and perplexity is 224.74548540585397
At time: 655.8857786655426 and batch: 800, loss is 5.405177230834961 and perplexity is 222.55566071427378
At time: 657.4278879165649 and batch: 850, loss is 5.40568585395813 and perplexity is 222.66888646168093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.220787684122722 and perplexity of 185.07991117715375
Finished 23 epochs...
Completing Train Step...
At time: 661.4362926483154 and batch: 50, loss is 5.427547674179078 and perplexity is 227.59043461157165
At time: 662.9761612415314 and batch: 100, loss is 5.379683322906494 and perplexity is 216.95356032236367
At time: 664.51544713974 and batch: 150, loss is 5.383956365585327 and perplexity is 217.88259563512057
At time: 666.0598092079163 and batch: 200, loss is 5.41984206199646 and perplexity is 225.8434504214466
At time: 667.5987684726715 and batch: 250, loss is 5.432780542373657 and perplexity is 228.78450684478364
At time: 669.1396608352661 and batch: 300, loss is 5.389004640579223 and perplexity is 218.98530794988096
At time: 670.6821386814117 and batch: 350, loss is 5.365975980758667 and perplexity is 213.9999926676786
At time: 672.2227227687836 and batch: 400, loss is 5.397448024749756 and perplexity is 220.8421128601283
At time: 673.762617111206 and batch: 450, loss is 5.406686515808105 and perplexity is 222.8918142405995
At time: 675.3016746044159 and batch: 500, loss is 5.422845983505249 and perplexity is 226.52288639504437
At time: 676.8409924507141 and batch: 550, loss is 5.41188871383667 and perplexity is 224.05436287952043
At time: 678.4089674949646 and batch: 600, loss is 5.420346841812134 and perplexity is 225.95748041431978
At time: 679.951099395752 and batch: 650, loss is 5.440313777923584 and perplexity is 230.51450247593638
At time: 681.4912829399109 and batch: 700, loss is 5.403281593322754 and perplexity is 222.13417547310206
At time: 683.0309340953827 and batch: 750, loss is 5.396591510772705 and perplexity is 220.65303948729436
At time: 684.5715780258179 and batch: 800, loss is 5.384079542160034 and perplexity is 217.9094353199155
At time: 686.1146910190582 and batch: 850, loss is 5.382933530807495 and perplexity is 217.65985167337544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.216444651285808 and perplexity of 184.27784600385868
Finished 24 epochs...
Completing Train Step...
At time: 690.1360611915588 and batch: 50, loss is 5.4097300720214845 and perplexity is 223.57123140444148
At time: 691.7025299072266 and batch: 100, loss is 5.363196439743042 and perplexity is 213.4059968110655
At time: 693.2427449226379 and batch: 150, loss is 5.369452886581421 and perplexity is 214.74534549817943
At time: 694.7817554473877 and batch: 200, loss is 5.404926118850708 and perplexity is 222.4997813369893
At time: 696.3197524547577 and batch: 250, loss is 5.415463094711304 and perplexity is 224.85665149743807
At time: 697.8579170703888 and batch: 300, loss is 5.377297153472901 and perplexity is 216.43648952282317
At time: 699.3984704017639 and batch: 350, loss is 5.352271280288696 and perplexity is 211.08719198777055
At time: 700.9366250038147 and batch: 400, loss is 5.384792709350586 and perplexity is 218.0648966080534
At time: 702.4751603603363 and batch: 450, loss is 5.393264389038086 and perplexity is 219.92011989625917
At time: 704.0120198726654 and batch: 500, loss is 5.411018733978271 and perplexity is 223.8595248614934
At time: 705.5513024330139 and batch: 550, loss is 5.400733566284179 and perplexity is 221.56889207233894
At time: 707.0923755168915 and batch: 600, loss is 5.412765274047851 and perplexity is 224.25084612127736
At time: 708.6329882144928 and batch: 650, loss is 5.431799402236939 and perplexity is 228.56014726455152
At time: 710.1771614551544 and batch: 700, loss is 5.393834104537964 and perplexity is 220.04544749444696
At time: 711.7184159755707 and batch: 750, loss is 5.388128995895386 and perplexity is 218.79363855852483
At time: 713.2580554485321 and batch: 800, loss is 5.37517804145813 and perplexity is 215.97832198306506
At time: 714.7974383831024 and batch: 850, loss is 5.372758388519287 and perplexity is 215.45636113825958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.211393356323242 and perplexity of 183.34935127326074
Finished 25 epochs...
Completing Train Step...
At time: 718.7908809185028 and batch: 50, loss is 5.40228832244873 and perplexity is 221.91364560755645
At time: 720.3564982414246 and batch: 100, loss is 5.353901443481445 and perplexity is 211.43157918601156
At time: 721.9017765522003 and batch: 150, loss is 5.361926374435424 and perplexity is 213.13512930420046
At time: 723.4550342559814 and batch: 200, loss is 5.3967882251739505 and perplexity is 220.6964493873764
At time: 724.9970400333405 and batch: 250, loss is 5.407011384963989 and perplexity is 222.96423667941664
At time: 726.541003704071 and batch: 300, loss is 5.363529710769654 and perplexity is 213.47713069948134
At time: 728.0833818912506 and batch: 350, loss is 5.3450306224823 and perplexity is 209.5643018797386
At time: 729.6265954971313 and batch: 400, loss is 5.378992424011231 and perplexity is 216.80371911573596
At time: 731.1657848358154 and batch: 450, loss is 5.38681321144104 and perplexity is 218.50594260463868
At time: 732.7075493335724 and batch: 500, loss is 5.403624267578125 and perplexity is 222.21030817988895
At time: 734.2500479221344 and batch: 550, loss is 5.39028639793396 and perplexity is 219.2661739414973
At time: 735.7924854755402 and batch: 600, loss is 5.400959796905518 and perplexity is 221.6190234108699
At time: 737.33425116539 and batch: 650, loss is 5.419003801345825 and perplexity is 225.65421406952962
At time: 738.8752760887146 and batch: 700, loss is 5.383834266662598 and perplexity is 217.85599402895892
At time: 740.4173910617828 and batch: 750, loss is 5.375899152755737 and perplexity is 216.1341225591117
At time: 741.9615921974182 and batch: 800, loss is 5.365830364227295 and perplexity is 213.96883299976926
At time: 743.5069808959961 and batch: 850, loss is 5.363995189666748 and perplexity is 213.57652292953244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.203990936279297 and perplexity of 181.99713337743114
Finished 26 epochs...
Completing Train Step...
At time: 747.5402941703796 and batch: 50, loss is 5.389590625762939 and perplexity is 219.11366770057413
At time: 749.0825419425964 and batch: 100, loss is 5.3428729629516605 and perplexity is 209.11262092859369
At time: 750.623851776123 and batch: 150, loss is 5.347140855789185 and perplexity is 210.00699838167054
At time: 752.1665766239166 and batch: 200, loss is 5.384805631637573 and perplexity is 218.0677145234361
At time: 753.7078213691711 and batch: 250, loss is 5.394525842666626 and perplexity is 220.19771397871216
At time: 755.2519326210022 and batch: 300, loss is 5.356672267913819 and perplexity is 212.0182313511382
At time: 756.8205173015594 and batch: 350, loss is 5.332617483139038 and perplexity is 206.97902986611086
At time: 758.3628368377686 and batch: 400, loss is 5.366160888671875 and perplexity is 214.03956661840482
At time: 759.902111530304 and batch: 450, loss is 5.374901885986328 and perplexity is 215.9186866223534
At time: 761.4422006607056 and batch: 500, loss is 5.392565832138062 and perplexity is 219.7665468250609
At time: 762.9847266674042 and batch: 550, loss is 5.381862735748291 and perplexity is 217.42690731970575
At time: 764.525940656662 and batch: 600, loss is 5.391097841262817 and perplexity is 219.44416822195527
At time: 766.0652775764465 and batch: 650, loss is 5.409563522338868 and perplexity is 223.53399878743514
At time: 767.6054618358612 and batch: 700, loss is 5.3743994998931885 and perplexity is 215.81023932043445
At time: 769.1492891311646 and batch: 750, loss is 5.370308809280395 and perplexity is 214.92922959795177
At time: 770.6914892196655 and batch: 800, loss is 5.356905813217163 and perplexity is 212.0677529958423
At time: 772.2345831394196 and batch: 850, loss is 5.358293914794922 and perplexity is 212.36232898176337
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.204369862874349 and perplexity of 182.06610999920392
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 776.2517008781433 and batch: 50, loss is 5.383521614074707 and perplexity is 217.78789143541894
At time: 777.7914898395538 and batch: 100, loss is 5.332128076553345 and perplexity is 206.87775774943583
At time: 779.3459327220917 and batch: 150, loss is 5.332153749465943 and perplexity is 206.8830689722059
At time: 780.8967106342316 and batch: 200, loss is 5.363367614746093 and perplexity is 213.4425297098911
At time: 782.4362449645996 and batch: 250, loss is 5.365327091217041 and perplexity is 213.86117535395087
At time: 783.9769253730774 and batch: 300, loss is 5.317596807479858 and perplexity is 203.89329789664757
At time: 785.5184853076935 and batch: 350, loss is 5.2914292526245115 and perplexity is 198.6271109515631
At time: 787.0574605464935 and batch: 400, loss is 5.3225733757019045 and perplexity is 204.91051583148595
At time: 788.5994827747345 and batch: 450, loss is 5.333246030807495 and perplexity is 207.1091669471477
At time: 790.1409516334534 and batch: 500, loss is 5.335000047683716 and perplexity is 207.472758701011
At time: 791.6811757087708 and batch: 550, loss is 5.315565567016602 and perplexity is 203.47956192065502
At time: 793.2224628925323 and batch: 600, loss is 5.311155052185058 and perplexity is 202.5840884959083
At time: 794.7872135639191 and batch: 650, loss is 5.320251808166504 and perplexity is 204.43535400382004
At time: 796.3251864910126 and batch: 700, loss is 5.279388952255249 and perplexity is 196.24992063508952
At time: 797.8651735782623 and batch: 750, loss is 5.2622408676147465 and perplexity is 192.91330047043405
At time: 799.404699087143 and batch: 800, loss is 5.242316398620606 and perplexity is 189.10764413551448
At time: 800.9438586235046 and batch: 850, loss is 5.263800773620606 and perplexity is 193.21446191715455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.148378690083821 and perplexity of 172.15215193997244
Finished 28 epochs...
Completing Train Step...
At time: 804.9714257717133 and batch: 50, loss is 5.337141551971436 and perplexity is 207.9175385823843
At time: 806.5115447044373 and batch: 100, loss is 5.289929027557373 and perplexity is 198.32934899152568
At time: 808.0507748126984 and batch: 150, loss is 5.292384119033813 and perplexity is 198.81686388772223
At time: 809.5890374183655 and batch: 200, loss is 5.3306599140167235 and perplexity is 206.5742504294038
At time: 811.1343281269073 and batch: 250, loss is 5.334402885437012 and perplexity is 207.34890078761083
At time: 812.6746513843536 and batch: 300, loss is 5.2936693286895755 and perplexity is 199.0725495105404
At time: 814.2118031978607 and batch: 350, loss is 5.2667022132873536 and perplexity is 193.7758760819655
At time: 815.7515184879303 and batch: 400, loss is 5.29594988822937 and perplexity is 199.52706438956713
At time: 817.2929632663727 and batch: 450, loss is 5.309795017242432 and perplexity is 202.3087543312165
At time: 818.8348453044891 and batch: 500, loss is 5.311346092224121 and perplexity is 202.62279386510795
At time: 820.3820056915283 and batch: 550, loss is 5.297436447143554 and perplexity is 199.82389369822127
At time: 821.9190180301666 and batch: 600, loss is 5.3000870704650875 and perplexity is 200.3542541534357
At time: 823.4565999507904 and batch: 650, loss is 5.31549693107605 and perplexity is 203.4655963888136
At time: 824.9963576793671 and batch: 700, loss is 5.280829544067383 and perplexity is 196.53284040090873
At time: 826.5344696044922 and batch: 750, loss is 5.269789810180664 and perplexity is 194.37510248369483
At time: 828.0744400024414 and batch: 800, loss is 5.2500769233703615 and perplexity is 190.58092802210132
At time: 829.6188099384308 and batch: 850, loss is 5.268573055267334 and perplexity is 194.13873944982123
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.147664388020833 and perplexity of 172.02922721061438
Finished 29 epochs...
Completing Train Step...
At time: 833.6409299373627 and batch: 50, loss is 5.326186275482177 and perplexity is 205.65217595421296
At time: 835.2048635482788 and batch: 100, loss is 5.277598838806153 and perplexity is 195.8989252672714
At time: 836.743067741394 and batch: 150, loss is 5.281338472366333 and perplexity is 196.63288698117026
At time: 838.2811923027039 and batch: 200, loss is 5.321492538452149 and perplexity is 204.6891605591696
At time: 839.8225703239441 and batch: 250, loss is 5.327115926742554 and perplexity is 205.84344965393026
At time: 841.36505651474 and batch: 300, loss is 5.285640563964844 and perplexity is 197.48064192443266
At time: 842.9034414291382 and batch: 350, loss is 5.2587863636016845 and perplexity is 192.2480304503516
At time: 844.4422976970673 and batch: 400, loss is 5.289129705429077 and perplexity is 198.1708832951922
At time: 845.9833660125732 and batch: 450, loss is 5.301248655319214 and perplexity is 200.58711783980257
At time: 847.5252799987793 and batch: 500, loss is 5.304399280548096 and perplexity is 201.22008927815278
At time: 849.0668454170227 and batch: 550, loss is 5.294664192199707 and perplexity is 199.27069807496224
At time: 850.603723526001 and batch: 600, loss is 5.299325971603394 and perplexity is 200.20182277369707
At time: 852.1427750587463 and batch: 650, loss is 5.3150230979919435 and perplexity is 203.36921049498324
At time: 853.6857361793518 and batch: 700, loss is 5.2856694602966305 and perplexity is 197.48634847303197
At time: 855.2291178703308 and batch: 750, loss is 5.273965845108032 and perplexity is 195.18851694215957
At time: 856.771059513092 and batch: 800, loss is 5.253537130355835 and perplexity is 191.2415197134269
At time: 858.3112251758575 and batch: 850, loss is 5.271896715164185 and perplexity is 194.78506407920108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.147475560506185 and perplexity of 171.99674642592274
Finished 30 epochs...
Completing Train Step...
At time: 862.3173651695251 and batch: 50, loss is 5.320828666687012 and perplexity is 204.55331830075386
At time: 863.8802680969238 and batch: 100, loss is 5.2718601989746094 and perplexity is 194.77795140073948
At time: 865.4160599708557 and batch: 150, loss is 5.273409070968628 and perplexity is 195.079871271974
At time: 866.9544026851654 and batch: 200, loss is 5.31606125831604 and perplexity is 203.58044997171135
At time: 868.4989812374115 and batch: 250, loss is 5.320097913742066 and perplexity is 204.4038949634283
At time: 870.0471465587616 and batch: 300, loss is 5.280931634902954 and perplexity is 196.55290562702285
At time: 871.585880279541 and batch: 350, loss is 5.253987808227539 and perplexity is 191.3277274590152
At time: 873.1873707771301 and batch: 400, loss is 5.283533592224121 and perplexity is 197.06499382572366
At time: 874.7242739200592 and batch: 450, loss is 5.2972021484375 and perplexity is 199.7770807028153
At time: 876.2646811008453 and batch: 500, loss is 5.302135963439941 and perplexity is 200.76517940442858
At time: 877.802797794342 and batch: 550, loss is 5.291757822036743 and perplexity is 198.6923844675149
At time: 879.3412108421326 and batch: 600, loss is 5.297732610702514 and perplexity is 199.88308301817133
At time: 880.8811976909637 and batch: 650, loss is 5.314941272735596 and perplexity is 203.35257043799905
At time: 882.4212708473206 and batch: 700, loss is 5.284592752456665 and perplexity is 197.27382780521074
At time: 883.9625566005707 and batch: 750, loss is 5.2751288414001465 and perplexity is 195.41565251694104
At time: 885.5036392211914 and batch: 800, loss is 5.253539524078369 and perplexity is 191.24197749310994
At time: 887.0435583591461 and batch: 850, loss is 5.270160465240479 and perplexity is 194.4471619527108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.143519401550293 and perplexity of 171.31764416114882
Finished 31 epochs...
Completing Train Step...
At time: 891.0472011566162 and batch: 50, loss is 5.315061006546021 and perplexity is 203.37692007382546
At time: 892.609475851059 and batch: 100, loss is 5.265939083099365 and perplexity is 193.6280562712772
At time: 894.1477012634277 and batch: 150, loss is 5.267850866317749 and perplexity is 193.99858521251653
At time: 895.6868345737457 and batch: 200, loss is 5.31083945274353 and perplexity is 202.5201631586475
At time: 897.2250635623932 and batch: 250, loss is 5.31591537475586 and perplexity is 203.5507530970819
At time: 898.7638788223267 and batch: 300, loss is 5.276219348907471 and perplexity is 195.62887099010229
At time: 900.300057888031 and batch: 350, loss is 5.249216289520263 and perplexity is 190.41697818477195
At time: 901.8397755622864 and batch: 400, loss is 5.279222650527954 and perplexity is 196.21728664792585
At time: 903.3778841495514 and batch: 450, loss is 5.293758611679078 and perplexity is 199.09032409636077
At time: 904.916791677475 and batch: 500, loss is 5.298732261657715 and perplexity is 200.0829962381118
At time: 906.4539651870728 and batch: 550, loss is 5.288537006378174 and perplexity is 198.05346240181004
At time: 907.9920580387115 and batch: 600, loss is 5.295110301971436 and perplexity is 199.35961451236386
At time: 909.5321729183197 and batch: 650, loss is 5.313316602706909 and perplexity is 203.02245784618137
At time: 911.0720219612122 and batch: 700, loss is 5.283145151138306 and perplexity is 196.98846055084238
At time: 912.6547510623932 and batch: 750, loss is 5.274335107803345 and perplexity is 195.26060608911692
At time: 914.1932022571564 and batch: 800, loss is 5.252606067657471 and perplexity is 191.06354473383246
At time: 915.7322630882263 and batch: 850, loss is 5.268209562301636 and perplexity is 194.06818420760578
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.141346295674642 and perplexity of 170.94575700345285
Finished 32 epochs...
Completing Train Step...
At time: 919.7644512653351 and batch: 50, loss is 5.3109449195861815 and perplexity is 202.54152344721035
At time: 921.3035714626312 and batch: 100, loss is 5.262615575790405 and perplexity is 192.98560020611865
At time: 922.8453986644745 and batch: 150, loss is 5.264570274353027 and perplexity is 193.3631978059627
At time: 924.3841009140015 and batch: 200, loss is 5.308082132339478 and perplexity is 201.96251933518965
At time: 925.924727678299 and batch: 250, loss is 5.311879549026489 and perplexity is 202.73091320874582
At time: 927.4631636142731 and batch: 300, loss is 5.2738219833374025 and perplexity is 195.16043879624004
At time: 929.0023195743561 and batch: 350, loss is 5.246679916381836 and perplexity is 189.93462165286968
At time: 930.542368888855 and batch: 400, loss is 5.277037000656128 and perplexity is 195.7888926906514
At time: 932.0861959457397 and batch: 450, loss is 5.292173700332642 and perplexity is 198.77503350255398
At time: 933.6251811981201 and batch: 500, loss is 5.296689472198486 and perplexity is 199.67468599035024
At time: 935.1653738021851 and batch: 550, loss is 5.286824436187744 and perplexity is 197.71457221543218
At time: 936.7111611366272 and batch: 600, loss is 5.29408299446106 and perplexity is 199.1549160452485
At time: 938.2563147544861 and batch: 650, loss is 5.3126485061645505 and perplexity is 202.88686454382506
At time: 939.7947976589203 and batch: 700, loss is 5.283167028427124 and perplexity is 196.99277017142904
At time: 941.3338079452515 and batch: 750, loss is 5.273982925415039 and perplexity is 195.19185085042517
At time: 942.8752210140228 and batch: 800, loss is 5.251870012283325 and perplexity is 190.92296312919754
At time: 944.4168343544006 and batch: 850, loss is 5.267814483642578 and perplexity is 193.99152715340338
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.141180992126465 and perplexity of 170.91750139871462
Finished 33 epochs...
Completing Train Step...
At time: 948.4360568523407 and batch: 50, loss is 5.30828462600708 and perplexity is 202.00341960733138
At time: 949.9747922420502 and batch: 100, loss is 5.2597175884246825 and perplexity is 192.427139971152
At time: 951.5408580303192 and batch: 150, loss is 5.261519184112549 and perplexity is 192.77412834928236
At time: 953.0804748535156 and batch: 200, loss is 5.305650396347046 and perplexity is 201.47199646058368
At time: 954.6228768825531 and batch: 250, loss is 5.308839235305786 and perplexity is 202.115483655225
At time: 956.1639142036438 and batch: 300, loss is 5.2706941223144534 and perplexity is 194.55095774941842
At time: 957.7039034366608 and batch: 350, loss is 5.243755111694336 and perplexity is 189.379911585881
At time: 959.257800579071 and batch: 400, loss is 5.273478908538818 and perplexity is 195.0934956519179
At time: 960.8148674964905 and batch: 450, loss is 5.289863109588623 and perplexity is 198.3162759545754
At time: 962.3533201217651 and batch: 500, loss is 5.294724149703979 and perplexity is 199.28264620687997
At time: 963.8937017917633 and batch: 550, loss is 5.284488792419434 and perplexity is 197.25332027672755
At time: 965.4334056377411 and batch: 600, loss is 5.292726516723633 and perplexity is 198.88494997820945
At time: 966.9740550518036 and batch: 650, loss is 5.311593084335327 and perplexity is 202.6728462777643
At time: 968.5158576965332 and batch: 700, loss is 5.282619419097901 and perplexity is 196.88492462400254
At time: 970.0554766654968 and batch: 750, loss is 5.2726688480377195 and perplexity is 194.93552210977083
At time: 971.5966022014618 and batch: 800, loss is 5.251435747146607 and perplexity is 190.8400699426248
At time: 973.1386301517487 and batch: 850, loss is 5.266684694290161 and perplexity is 193.77248135267266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.139832496643066 and perplexity of 170.68717525183197
Finished 34 epochs...
Completing Train Step...
At time: 977.1828572750092 and batch: 50, loss is 5.305174360275268 and perplexity is 201.37611134701254
At time: 978.727028131485 and batch: 100, loss is 5.256746616363525 and perplexity is 191.85629271999667
At time: 980.2672333717346 and batch: 150, loss is 5.258957595825195 and perplexity is 192.28095232663372
At time: 981.8052217960358 and batch: 200, loss is 5.30310546875 and perplexity is 200.95991669610189
At time: 983.3434398174286 and batch: 250, loss is 5.306163463592529 and perplexity is 201.57539166492802
At time: 984.8835039138794 and batch: 300, loss is 5.268884887695313 and perplexity is 194.19928764426325
At time: 986.422446012497 and batch: 350, loss is 5.240705423355102 and perplexity is 188.8032416565599
At time: 987.962749004364 and batch: 400, loss is 5.271293840408325 and perplexity is 194.66766847222888
At time: 989.5107357501984 and batch: 450, loss is 5.287744188308716 and perplexity is 197.89650426594375
At time: 991.0906848907471 and batch: 500, loss is 5.292845687866211 and perplexity is 198.90865273725433
At time: 992.6306047439575 and batch: 550, loss is 5.282347764968872 and perplexity is 196.8314472852837
At time: 994.1743576526642 and batch: 600, loss is 5.29051721572876 and perplexity is 198.44603828290755
At time: 995.716267824173 and batch: 650, loss is 5.310033950805664 and perplexity is 202.35709845804922
At time: 997.2615718841553 and batch: 700, loss is 5.2810992240905765 and perplexity is 196.58584852916172
At time: 998.8016049861908 and batch: 750, loss is 5.271033973693847 and perplexity is 194.61708739726149
At time: 1000.3419075012207 and batch: 800, loss is 5.249453001022339 and perplexity is 190.46205740887362
At time: 1001.8841733932495 and batch: 850, loss is 5.264849395751953 and perplexity is 193.41717714527888
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.13861083984375 and perplexity of 170.4787814224168
Finished 35 epochs...
Completing Train Step...
At time: 1005.8730461597443 and batch: 50, loss is 5.303409652709961 and perplexity is 201.02105477749635
At time: 1007.4393498897552 and batch: 100, loss is 5.254339485168457 and perplexity is 191.39502484169614
At time: 1008.9775803089142 and batch: 150, loss is 5.255767917633056 and perplexity is 191.66861506480117
At time: 1010.5177390575409 and batch: 200, loss is 5.300400371551514 and perplexity is 200.41703519310252
At time: 1012.0596308708191 and batch: 250, loss is 5.304078788757324 and perplexity is 201.15561022445632
At time: 1013.6010551452637 and batch: 300, loss is 5.266596050262451 and perplexity is 193.75530534075287
At time: 1015.1427316665649 and batch: 350, loss is 5.238508949279785 and perplexity is 188.38899533811923
At time: 1016.6872260570526 and batch: 400, loss is 5.268464365005493 and perplexity is 194.1176396060916
At time: 1018.2416276931763 and batch: 450, loss is 5.285992650985718 and perplexity is 197.55018453713546
At time: 1019.7842881679535 and batch: 500, loss is 5.290462131500244 and perplexity is 198.4351073370509
At time: 1021.3287761211395 and batch: 550, loss is 5.280114841461182 and perplexity is 196.39242805018347
At time: 1022.8771767616272 and batch: 600, loss is 5.288631277084351 and perplexity is 198.07213392164618
At time: 1024.4211189746857 and batch: 650, loss is 5.308285074234009 and perplexity is 202.003510150724
At time: 1025.9670021533966 and batch: 700, loss is 5.279594469070434 and perplexity is 196.29025743856258
At time: 1027.508228302002 and batch: 750, loss is 5.269903936386108 and perplexity is 194.3972870424698
At time: 1029.0910408496857 and batch: 800, loss is 5.248047590255737 and perplexity is 190.19456799304294
At time: 1030.6356012821198 and batch: 850, loss is 5.263538856506347 and perplexity is 193.16386236958922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.137678782145183 and perplexity of 170.319959388897
Finished 36 epochs...
Completing Train Step...
At time: 1034.6486127376556 and batch: 50, loss is 5.300274600982666 and perplexity is 200.39183021363507
At time: 1036.2312994003296 and batch: 100, loss is 5.2520850658416744 and perplexity is 190.96402620701136
At time: 1037.777812242508 and batch: 150, loss is 5.254223709106445 and perplexity is 191.37286716212046
At time: 1039.3198804855347 and batch: 200, loss is 5.298940544128418 and perplexity is 200.1246743591742
At time: 1040.8622460365295 and batch: 250, loss is 5.301659116744995 and perplexity is 200.66946801381306
At time: 1042.407143831253 and batch: 300, loss is 5.264027509689331 and perplexity is 193.25827557155074
At time: 1043.9502704143524 and batch: 350, loss is 5.2361975765228275 and perplexity is 187.9540609879225
At time: 1045.493371963501 and batch: 400, loss is 5.266594104766845 and perplexity is 193.75492839102446
At time: 1047.031013727188 and batch: 450, loss is 5.284290885925293 and perplexity is 197.21428642630755
At time: 1048.5770072937012 and batch: 500, loss is 5.288484783172607 and perplexity is 198.043119685197
At time: 1050.1201722621918 and batch: 550, loss is 5.278024454116821 and perplexity is 195.98232059511386
At time: 1051.6625895500183 and batch: 600, loss is 5.287098255157471 and perplexity is 197.7687176285785
At time: 1053.204369544983 and batch: 650, loss is 5.307309885025024 and perplexity is 201.80661452829497
At time: 1054.746584892273 and batch: 700, loss is 5.278272943496704 and perplexity is 196.03102617158527
At time: 1056.2900664806366 and batch: 750, loss is 5.268860673904419 and perplexity is 194.19458540025025
At time: 1057.8317494392395 and batch: 800, loss is 5.2466939926147464 and perplexity is 189.93729523565884
At time: 1059.372437953949 and batch: 850, loss is 5.2623800277709964 and perplexity is 192.94014818349285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.136889139811198 and perplexity of 170.1855206248963
Finished 37 epochs...
Completing Train Step...
At time: 1063.3486323356628 and batch: 50, loss is 5.298186140060425 and perplexity is 199.97375642444143
At time: 1064.912697315216 and batch: 100, loss is 5.249675512313843 and perplexity is 190.5044420826096
At time: 1066.454850435257 and batch: 150, loss is 5.252169494628906 and perplexity is 190.98014974878487
At time: 1068.0165002346039 and batch: 200, loss is 5.296610841751098 and perplexity is 199.65898609771168
At time: 1069.555073261261 and batch: 250, loss is 5.299665441513062 and perplexity is 200.26979680530584
At time: 1071.0979974269867 and batch: 300, loss is 5.263397178649902 and perplexity is 193.13649726618644
At time: 1072.6383759975433 and batch: 350, loss is 5.234680414199829 and perplexity is 187.66912037337536
At time: 1074.176702260971 and batch: 400, loss is 5.265003633499146 and perplexity is 193.44701167569642
At time: 1075.7152953147888 and batch: 450, loss is 5.282742462158203 and perplexity is 196.9091514380958
At time: 1077.2552134990692 and batch: 500, loss is 5.2863170337677 and perplexity is 197.61427681022963
At time: 1078.7981531620026 and batch: 550, loss is 5.27627272605896 and perplexity is 195.6393133806748
At time: 1080.3392779827118 and batch: 600, loss is 5.285227088928223 and perplexity is 197.39900548725856
At time: 1081.8903467655182 and batch: 650, loss is 5.305536880493164 and perplexity is 201.4491274928919
At time: 1083.4309875965118 and batch: 700, loss is 5.2772430801391605 and perplexity is 195.82924492218137
At time: 1084.97323179245 and batch: 750, loss is 5.26784496307373 and perplexity is 193.99743999490894
At time: 1086.514078617096 and batch: 800, loss is 5.246441745758057 and perplexity is 189.88939019216988
At time: 1088.0561051368713 and batch: 850, loss is 5.2611227703094485 and perplexity is 192.69772516856227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.135987281799316 and perplexity of 170.03210663882894
Finished 38 epochs...
Completing Train Step...
At time: 1092.0555942058563 and batch: 50, loss is 5.2955397033691405 and perplexity is 199.4452381916295
At time: 1093.5953443050385 and batch: 100, loss is 5.247965440750122 and perplexity is 190.17894424506204
At time: 1095.1353521347046 and batch: 150, loss is 5.250275344848633 and perplexity is 190.61874712350667
At time: 1096.6738846302032 and batch: 200, loss is 5.295134525299073 and perplexity is 199.36444372411333
At time: 1098.2121431827545 and batch: 250, loss is 5.297604513168335 and perplexity is 199.85748012798115
At time: 1099.7517955303192 and batch: 300, loss is 5.2620631694793705 and perplexity is 192.8790231822465
At time: 1101.2904143333435 and batch: 350, loss is 5.2331132984161375 and perplexity is 187.3752514561775
At time: 1102.8277254104614 and batch: 400, loss is 5.263226442337036 and perplexity is 193.10352466765332
At time: 1104.3669414520264 and batch: 450, loss is 5.28156533241272 and perplexity is 196.67750018731778
At time: 1105.9092173576355 and batch: 500, loss is 5.285086498260498 and perplexity is 197.37125498004582
At time: 1107.4747545719147 and batch: 550, loss is 5.274691944122314 and perplexity is 195.33029459798956
At time: 1109.015116930008 and batch: 600, loss is 5.283630409240723 and perplexity is 197.08407399413016
At time: 1110.553740978241 and batch: 650, loss is 5.30442361831665 and perplexity is 201.22498658570862
At time: 1112.1047081947327 and batch: 700, loss is 5.276490125656128 and perplexity is 195.68184991213883
At time: 1113.6510162353516 and batch: 750, loss is 5.267009372711182 and perplexity is 193.83540531042837
At time: 1115.1902623176575 and batch: 800, loss is 5.244065608978271 and perplexity is 189.43872266392788
At time: 1116.7278809547424 and batch: 850, loss is 5.259553527832031 and perplexity is 192.39557285005716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.13602860768636 and perplexity of 170.03913351165664
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 1120.7237620353699 and batch: 50, loss is 5.295356159210205 and perplexity is 199.40863454242768
At time: 1122.267056941986 and batch: 100, loss is 5.248805685043335 and perplexity is 190.33880817062322
At time: 1123.8071126937866 and batch: 150, loss is 5.2495480823516845 and perplexity is 190.4801676554415
At time: 1125.3489286899567 and batch: 200, loss is 5.293352594375611 and perplexity is 199.00950638762913
At time: 1126.8915874958038 and batch: 250, loss is 5.292406606674194 and perplexity is 198.82133486012964
At time: 1128.4304676055908 and batch: 300, loss is 5.253472023010254 and perplexity is 191.22906889103777
At time: 1129.9713096618652 and batch: 350, loss is 5.22428677558899 and perplexity is 185.7286570636114
At time: 1131.5125224590302 and batch: 400, loss is 5.251039457321167 and perplexity is 190.76445694794018
At time: 1133.0525946617126 and batch: 450, loss is 5.27062991142273 and perplexity is 194.5384658599976
At time: 1134.594334602356 and batch: 500, loss is 5.271712560653686 and perplexity is 194.74919683373201
At time: 1136.1375358104706 and batch: 550, loss is 5.257075452804566 and perplexity is 191.91939243466015
At time: 1137.678935289383 and batch: 600, loss is 5.25879979133606 and perplexity is 192.25061192317023
At time: 1139.2270567417145 and batch: 650, loss is 5.276125783920288 and perplexity is 195.61056783357614
At time: 1140.7711374759674 and batch: 700, loss is 5.247795219421387 and perplexity is 190.14657448756483
At time: 1142.3132183551788 and batch: 750, loss is 5.235633172988892 and perplexity is 187.84800898256188
At time: 1143.8564519882202 and batch: 800, loss is 5.2121906661987305 and perplexity is 183.49559581505665
At time: 1145.3964114189148 and batch: 850, loss is 5.236075305938721 and perplexity is 187.9310811400087
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.125123341878255 and perplexity of 168.1948858456383
Finished 40 epochs...
Completing Train Step...
At time: 1149.4216058254242 and batch: 50, loss is 5.28639573097229 and perplexity is 197.62982911335507
At time: 1150.9631943702698 and batch: 100, loss is 5.240256900787354 and perplexity is 188.7185781299833
At time: 1152.5026123523712 and batch: 150, loss is 5.241566076278686 and perplexity is 188.9658056640333
At time: 1154.0449922084808 and batch: 200, loss is 5.284895458221436 and perplexity is 197.33355276921515
At time: 1155.588280916214 and batch: 250, loss is 5.285539569854737 and perplexity is 197.46069854983682
At time: 1157.1294295787811 and batch: 300, loss is 5.247570447921753 and perplexity is 190.1038397598213
At time: 1158.670948266983 and batch: 350, loss is 5.219613084793091 and perplexity is 184.86264406343994
At time: 1160.2114770412445 and batch: 400, loss is 5.246089563369751 and perplexity is 189.82252626805743
At time: 1161.7523934841156 and batch: 450, loss is 5.265880613327027 and perplexity is 193.61673521388173
At time: 1163.2949600219727 and batch: 500, loss is 5.268093366622924 and perplexity is 194.04563563327426
At time: 1164.8381011486053 and batch: 550, loss is 5.255506315231323 and perplexity is 191.6184806526906
At time: 1166.3758573532104 and batch: 600, loss is 5.259194316864014 and perplexity is 192.32647466124817
At time: 1167.915302991867 and batch: 650, loss is 5.2776034450531 and perplexity is 195.89982762817618
At time: 1169.473727464676 and batch: 700, loss is 5.250133857727051 and perplexity is 190.59177893352748
At time: 1171.0242326259613 and batch: 750, loss is 5.238594207763672 and perplexity is 188.40505778296287
At time: 1172.5688261985779 and batch: 800, loss is 5.215301237106323 and perplexity is 184.06726051793157
At time: 1174.1092953681946 and batch: 850, loss is 5.238190746307373 and perplexity is 188.32905893630922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.124547640482585 and perplexity of 168.09808368235628
Finished 41 epochs...
Completing Train Step...
At time: 1178.078197002411 and batch: 50, loss is 5.283401403427124 and perplexity is 197.0389457629287
At time: 1179.6397671699524 and batch: 100, loss is 5.236796464920044 and perplexity is 188.06665820747662
At time: 1181.1773328781128 and batch: 150, loss is 5.238075695037842 and perplexity is 188.30739268537775
At time: 1182.7159614562988 and batch: 200, loss is 5.2815573787689205 and perplexity is 196.67593589075884
At time: 1184.254734992981 and batch: 250, loss is 5.282143850326538 and perplexity is 196.7913145630743
At time: 1185.8205134868622 and batch: 300, loss is 5.244949989318847 and perplexity is 189.60633265055984
At time: 1187.3552103042603 and batch: 350, loss is 5.217551231384277 and perplexity is 184.48187706837317
At time: 1188.8952331542969 and batch: 400, loss is 5.244136209487915 and perplexity is 189.45209760642754
At time: 1190.4362354278564 and batch: 450, loss is 5.264329433441162 and perplexity is 193.31663364458453
At time: 1191.975760936737 and batch: 500, loss is 5.266746187210083 and perplexity is 193.78439735472276
At time: 1193.5155856609344 and batch: 550, loss is 5.255650854110717 and perplexity is 191.64617897484936
At time: 1195.0560727119446 and batch: 600, loss is 5.259986333847046 and perplexity is 192.4788608337189
At time: 1196.5965414047241 and batch: 650, loss is 5.279126462936401 and perplexity is 196.19841388737936
At time: 1198.1378128528595 and batch: 700, loss is 5.251925678253174 and perplexity is 190.9335913369185
At time: 1199.682113647461 and batch: 750, loss is 5.240286245346069 and perplexity is 188.72411607463383
At time: 1201.2268738746643 and batch: 800, loss is 5.216794662475586 and perplexity is 184.34235660104508
At time: 1202.7677371501923 and batch: 850, loss is 5.23907588005066 and perplexity is 188.49582913729316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.124375979105632 and perplexity of 168.0692302104324
Finished 42 epochs...
Completing Train Step...
At time: 1206.7357926368713 and batch: 50, loss is 5.281567974090576 and perplexity is 196.67801974660105
At time: 1208.2997195720673 and batch: 100, loss is 5.234617805480957 and perplexity is 187.65737101798675
At time: 1209.8381984233856 and batch: 150, loss is 5.235622205734253 and perplexity is 187.8459488169112
At time: 1211.3769977092743 and batch: 200, loss is 5.279291687011718 and perplexity is 196.2308332670499
At time: 1212.9203877449036 and batch: 250, loss is 5.280071983337402 and perplexity is 196.38401121955877
At time: 1214.4587817192078 and batch: 300, loss is 5.243373031616211 and perplexity is 189.30756711603746
At time: 1215.9968490600586 and batch: 350, loss is 5.216290140151978 and perplexity is 184.24937522451577
At time: 1217.5375978946686 and batch: 400, loss is 5.243206634521484 and perplexity is 189.27606950748748
At time: 1219.080897808075 and batch: 450, loss is 5.263338737487793 and perplexity is 193.12521047465023
At time: 1220.6240541934967 and batch: 500, loss is 5.26625075340271 and perplexity is 193.68841379164616
At time: 1222.1706149578094 and batch: 550, loss is 5.2557690238952635 and perplexity is 191.6688271006636
At time: 1223.7403283119202 and batch: 600, loss is 5.260652952194214 and perplexity is 192.6072135501774
At time: 1225.280415058136 and batch: 650, loss is 5.280142097473145 and perplexity is 196.3977809975015
At time: 1226.8212313652039 and batch: 700, loss is 5.253122749328614 and perplexity is 191.16228927297
At time: 1228.363203048706 and batch: 750, loss is 5.241386890411377 and perplexity is 188.9319486956893
At time: 1229.9022121429443 and batch: 800, loss is 5.217834978103638 and perplexity is 184.53423062299623
At time: 1231.4423170089722 and batch: 850, loss is 5.239479827880859 and perplexity is 188.57198699934082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.124306042989095 and perplexity of 168.05747651217092
Finished 43 epochs...
Completing Train Step...
At time: 1235.4147410392761 and batch: 50, loss is 5.280055656433105 and perplexity is 196.38080490277682
At time: 1236.9776494503021 and batch: 100, loss is 5.232952280044556 and perplexity is 187.34508302721412
At time: 1238.5190711021423 and batch: 150, loss is 5.23373592376709 and perplexity is 187.49195236457714
At time: 1240.0591943264008 and batch: 200, loss is 5.27754690170288 and perplexity is 195.88875110876933
At time: 1241.6016294956207 and batch: 250, loss is 5.27834457397461 and perplexity is 196.04506847059656
At time: 1243.1414575576782 and batch: 300, loss is 5.241962852478028 and perplexity is 189.04079767474886
At time: 1244.6810312271118 and batch: 350, loss is 5.2155064868927 and perplexity is 184.10504416124152
At time: 1246.2210342884064 and batch: 400, loss is 5.242772102355957 and perplexity is 189.1938408339176
At time: 1247.7637991905212 and batch: 450, loss is 5.262888765335083 and perplexity is 193.0383290565257
At time: 1249.3104395866394 and batch: 500, loss is 5.266274871826172 and perplexity is 193.69308530716424
At time: 1250.850597858429 and batch: 550, loss is 5.256020431518555 and perplexity is 191.71702016274176
At time: 1252.3900332450867 and batch: 600, loss is 5.261185865402222 and perplexity is 192.70988383298103
At time: 1253.9333448410034 and batch: 650, loss is 5.280635623931885 and perplexity is 196.49473242094012
At time: 1255.4754016399384 and batch: 700, loss is 5.253958892822266 and perplexity is 191.3221952202196
At time: 1257.0201003551483 and batch: 750, loss is 5.241878643035888 and perplexity is 189.02487932488216
At time: 1258.5615103244781 and batch: 800, loss is 5.218536777496338 and perplexity is 184.66378208823235
At time: 1260.1027381420135 and batch: 850, loss is 5.239727449417114 and perplexity is 188.6186872662136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.124316215515137 and perplexity of 168.05918608992263
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 1264.1232562065125 and batch: 50, loss is 5.279527673721313 and perplexity is 196.2771466001643
At time: 1265.6636373996735 and batch: 100, loss is 5.23302077293396 and perplexity is 187.35791527272016
At time: 1267.2034192085266 and batch: 150, loss is 5.233184041976929 and perplexity is 187.3885075175547
At time: 1268.7426598072052 and batch: 200, loss is 5.277525854110718 and perplexity is 195.88462816561594
At time: 1270.281289100647 and batch: 250, loss is 5.275719795227051 and perplexity is 195.5311682735104
At time: 1271.8205959796906 and batch: 300, loss is 5.239794778823852 and perplexity is 188.63138727806427
At time: 1273.3587355613708 and batch: 350, loss is 5.213337163925171 and perplexity is 183.70609374311096
At time: 1274.8991372585297 and batch: 400, loss is 5.239113788604737 and perplexity is 188.5029748770668
At time: 1276.4414529800415 and batch: 450, loss is 5.259792623519897 and perplexity is 192.4415793016431
At time: 1277.9809341430664 and batch: 500, loss is 5.262399139404297 and perplexity is 192.94383562009028
At time: 1279.5199236869812 and batch: 550, loss is 5.251380615234375 and perplexity is 190.8295488546649
At time: 1281.0602312088013 and batch: 600, loss is 5.255617418289185 and perplexity is 191.63977123453634
At time: 1282.6015326976776 and batch: 650, loss is 5.273470411300659 and perplexity is 195.09183790306514
At time: 1284.1453654766083 and batch: 700, loss is 5.2466831874847415 and perplexity is 189.93524294957865
At time: 1285.6833062171936 and batch: 750, loss is 5.233082094192505 and perplexity is 187.3694046481509
At time: 1287.221929550171 and batch: 800, loss is 5.211121597290039 and perplexity is 183.29953120065431
At time: 1288.7670359611511 and batch: 850, loss is 5.2331881427764895 and perplexity is 187.38927596183956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.12451966603597 and perplexity of 168.09338129726183
Annealing...
Finished 45 epochs...
Completing Train Step...
At time: 1292.807698726654 and batch: 50, loss is 5.278193197250366 and perplexity is 196.01539405639184
At time: 1294.3477437496185 and batch: 100, loss is 5.231158847808838 and perplexity is 187.00939342436337
At time: 1295.8872673511505 and batch: 150, loss is 5.232192354202271 and perplexity is 187.2027687381984
At time: 1297.427701473236 and batch: 200, loss is 5.276566343307495 and perplexity is 195.69676489153946
At time: 1298.9655520915985 and batch: 250, loss is 5.274882755279541 and perplexity is 195.36756935364977
At time: 1300.5034246444702 and batch: 300, loss is 5.238662071228028 and perplexity is 188.41784403674097
At time: 1302.0676267147064 and batch: 350, loss is 5.21229455947876 and perplexity is 183.51466076471982
At time: 1303.6080453395844 and batch: 400, loss is 5.237329778671264 and perplexity is 188.1669834924939
At time: 1305.1494088172913 and batch: 450, loss is 5.258786306381226 and perplexity is 192.24801944983145
At time: 1306.6879975795746 and batch: 500, loss is 5.261835556030274 and perplexity is 192.8351263184694
At time: 1308.2274947166443 and batch: 550, loss is 5.250713310241699 and perplexity is 190.7022498223279
At time: 1309.7685995101929 and batch: 600, loss is 5.254422740936279 and perplexity is 191.41096024479455
At time: 1311.3078026771545 and batch: 650, loss is 5.272152853012085 and perplexity is 194.83496229645564
At time: 1312.8483345508575 and batch: 700, loss is 5.245124855041504 and perplexity is 189.63949119800122
At time: 1314.3882946968079 and batch: 750, loss is 5.231115131378174 and perplexity is 187.00121821987892
At time: 1315.9275159835815 and batch: 800, loss is 5.209026956558228 and perplexity is 182.915986371025
At time: 1317.4674813747406 and batch: 850, loss is 5.2319989585876465 and perplexity is 187.1665680443195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.124112129211426 and perplexity of 168.0248910115258
Finished 46 epochs...
Completing Train Step...
At time: 1321.4869906902313 and batch: 50, loss is 5.27814211845398 and perplexity is 196.00538208169223
At time: 1323.031152009964 and batch: 100, loss is 5.231079063415527 and perplexity is 186.99447358855855
At time: 1324.575969696045 and batch: 150, loss is 5.232074251174927 and perplexity is 187.18066083001446
At time: 1326.1197860240936 and batch: 200, loss is 5.276215476989746 and perplexity is 195.62811353267557
At time: 1327.6601965427399 and batch: 250, loss is 5.274618530273438 and perplexity is 195.3159551756235
At time: 1329.204895734787 and batch: 300, loss is 5.2384499168396 and perplexity is 188.37787460426566
At time: 1330.7510159015656 and batch: 350, loss is 5.212028074264526 and perplexity is 183.46576333654102
At time: 1332.2965807914734 and batch: 400, loss is 5.237179946899414 and perplexity is 188.13879221198104
At time: 1333.8537929058075 and batch: 450, loss is 5.258765859603882 and perplexity is 192.24408863756926
At time: 1335.403493642807 and batch: 500, loss is 5.261818895339966 and perplexity is 192.83191357891258
At time: 1336.9428884983063 and batch: 550, loss is 5.250716209411621 and perplexity is 190.70280270135606
At time: 1338.484265089035 and batch: 600, loss is 5.254413328170776 and perplexity is 191.40915854679056
At time: 1340.0283977985382 and batch: 650, loss is 5.272173395156861 and perplexity is 194.83896466556695
At time: 1341.5953285694122 and batch: 700, loss is 5.245162515640259 and perplexity is 189.64663326927385
At time: 1343.1352059841156 and batch: 750, loss is 5.231290740966797 and perplexity is 187.03406031049107
At time: 1344.677502155304 and batch: 800, loss is 5.2092520236969 and perplexity is 182.95715938186697
At time: 1346.2192132472992 and batch: 850, loss is 5.232138938903809 and perplexity is 187.19276951349156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.124052365620931 and perplexity of 168.01484954080655
Finished 47 epochs...
Completing Train Step...
At time: 1350.1974911689758 and batch: 50, loss is 5.27792802810669 and perplexity is 195.9634237129727
At time: 1351.7606918811798 and batch: 100, loss is 5.230845308303833 and perplexity is 186.9507677829253
At time: 1353.3029301166534 and batch: 150, loss is 5.2318171119689945 and perplexity is 187.13253553123843
At time: 1354.844871520996 and batch: 200, loss is 5.275947465896606 and perplexity is 195.57569005346954
At time: 1356.3868350982666 and batch: 250, loss is 5.2743955421447755 and perplexity is 195.27240689183657
At time: 1357.9262030124664 and batch: 300, loss is 5.2382402420043945 and perplexity is 188.33838064504107
At time: 1359.4669744968414 and batch: 350, loss is 5.211821422576905 and perplexity is 183.4278537441023
At time: 1361.0193302631378 and batch: 400, loss is 5.237072563171386 and perplexity is 188.11859025178697
At time: 1362.56969165802 and batch: 450, loss is 5.258740844726563 and perplexity is 192.239279735424
At time: 1364.1112129688263 and batch: 500, loss is 5.261831274032593 and perplexity is 192.8343006006736
At time: 1365.6505436897278 and batch: 550, loss is 5.250749788284302 and perplexity is 190.7092063940016
At time: 1367.1926295757294 and batch: 600, loss is 5.254434213638306 and perplexity is 191.41315625830316
At time: 1368.7342958450317 and batch: 650, loss is 5.272226209640503 and perplexity is 194.84925525662294
At time: 1370.2727525234222 and batch: 700, loss is 5.245198907852173 and perplexity is 189.65353505532548
At time: 1371.8121411800385 and batch: 750, loss is 5.23142523765564 and perplexity is 187.0592174640427
At time: 1373.3568165302277 and batch: 800, loss is 5.209444065093994 and perplexity is 182.99229810429966
At time: 1374.8982620239258 and batch: 850, loss is 5.232232971191406 and perplexity is 187.21037250544242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.124030749003093 and perplexity of 168.0112176672675
Finished 48 epochs...
Completing Train Step...
At time: 1378.871149301529 and batch: 50, loss is 5.277735214233399 and perplexity is 195.9256428886739
At time: 1380.4350242614746 and batch: 100, loss is 5.230661907196045 and perplexity is 186.91648394895483
At time: 1381.973396062851 and batch: 150, loss is 5.231609916687011 and perplexity is 187.0937665692825
At time: 1383.510782957077 and batch: 200, loss is 5.275710411071778 and perplexity is 195.52933338727598
At time: 1385.048707485199 and batch: 250, loss is 5.274197254180908 and perplexity is 195.23369056249246
At time: 1386.5888285636902 and batch: 300, loss is 5.238060359954834 and perplexity is 188.30450499802146
At time: 1388.1285696029663 and batch: 350, loss is 5.211655702590942 and perplexity is 183.39745860136557
At time: 1389.670253753662 and batch: 400, loss is 5.236984453201294 and perplexity is 188.10201585862143
At time: 1391.2097585201263 and batch: 450, loss is 5.258724880218506 and perplexity is 192.23621075439127
At time: 1392.7485947608948 and batch: 500, loss is 5.261863822937012 and perplexity is 192.84057724804097
At time: 1394.2899713516235 and batch: 550, loss is 5.250804166793824 and perplexity is 190.71957715836822
At time: 1395.8305921554565 and batch: 600, loss is 5.254471158981323 and perplexity is 191.42022821365617
At time: 1397.3692436218262 and batch: 650, loss is 5.27227822303772 and perplexity is 194.85939029191053
At time: 1398.9109318256378 and batch: 700, loss is 5.245239219665527 and perplexity is 189.66118048733213
At time: 1400.4519786834717 and batch: 750, loss is 5.231533298492431 and perplexity is 187.07943233180953
At time: 1401.99129986763 and batch: 800, loss is 5.209617052078247 and perplexity is 183.02395612822446
At time: 1403.5321927070618 and batch: 850, loss is 5.232307958602905 and perplexity is 187.22441145304794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.124017079671224 and perplexity of 168.0089210818719
Finished 49 epochs...
Completing Train Step...
At time: 1407.5181398391724 and batch: 50, loss is 5.277563428878784 and perplexity is 195.8919886233698
At time: 1409.0654773712158 and batch: 100, loss is 5.2304884052276615 and perplexity is 186.88405638427085
At time: 1410.603414773941 and batch: 150, loss is 5.231417264938354 and perplexity is 187.05772609973133
At time: 1412.1422798633575 and batch: 200, loss is 5.275516624450684 and perplexity is 195.4914460895784
At time: 1413.6846005916595 and batch: 250, loss is 5.27402096748352 and perplexity is 195.1992764934247
At time: 1415.2277882099152 and batch: 300, loss is 5.237902278900147 and perplexity is 188.27473997597403
At time: 1416.7698636054993 and batch: 350, loss is 5.211508312225342 and perplexity is 183.3704295748501
At time: 1418.3114411830902 and batch: 400, loss is 5.236916790008545 and perplexity is 188.08928870625058
At time: 1419.8793156147003 and batch: 450, loss is 5.2587289714813235 and perplexity is 192.23699724486136
At time: 1421.4209282398224 and batch: 500, loss is 5.261907234191894 and perplexity is 192.84894888120178
At time: 1422.9651498794556 and batch: 550, loss is 5.250856065750122 and perplexity is 190.72947556222468
At time: 1424.503881931305 and batch: 600, loss is 5.254512529373169 and perplexity is 191.4281475073157
At time: 1426.0446112155914 and batch: 650, loss is 5.272329368591309 and perplexity is 194.8693567381665
At time: 1427.5850036144257 and batch: 700, loss is 5.24527795791626 and perplexity is 189.66852777200566
At time: 1429.1266312599182 and batch: 750, loss is 5.231635484695435 and perplexity is 187.0985502454364
At time: 1430.666464805603 and batch: 800, loss is 5.209771909713745 and perplexity is 183.05230097996167
At time: 1432.2090301513672 and batch: 850, loss is 5.232364740371704 and perplexity is 187.23504268811988
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.124011039733887 and perplexity of 168.00790632158106
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f777bec6898>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'seq_len': 50, 'wordvec_source': 'glove', 'lr': 29.273323309911518, 'anneal': 5.008675651095301, 'dropout': 0.2177491901710491, 'wordvec_dim': 200, 'tune_wordvecs': True, 'data': 'wikitext', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.999429702758789 and batch: 50, loss is 7.9019873046875 and perplexity is 2702.647979909401
At time: 3.526468276977539 and batch: 100, loss is 7.05224419593811 and perplexity is 1155.4488890018806
At time: 5.0536909103393555 and batch: 150, loss is 6.589826917648315 and perplexity is 727.654914774974
At time: 6.583878040313721 and batch: 200, loss is 6.281971206665039 and perplexity is 534.8419093386585
At time: 8.115291833877563 and batch: 250, loss is 6.3407997798919675 and perplexity is 567.2498049976448
At time: 9.645203590393066 and batch: 300, loss is 6.32206392288208 and perplexity is 556.720836386407
At time: 11.177913904190063 and batch: 350, loss is 6.390713653564453 and perplexity is 596.2819666314138
At time: 12.723818063735962 and batch: 400, loss is 6.49574993133545 and perplexity is 662.3207341853717
At time: 14.256194591522217 and batch: 450, loss is 6.686517992019653 and perplexity is 801.5264659429415
At time: 15.815363883972168 and batch: 500, loss is 6.827443065643311 and perplexity is 922.8281821367075
At time: 17.347843170166016 and batch: 550, loss is 6.993936052322388 and perplexity is 1090.0033540432867
At time: 18.881896018981934 and batch: 600, loss is 6.759727077484131 and perplexity is 862.4067934356301
At time: 20.415955781936646 and batch: 650, loss is 6.934324254989624 and perplexity is 1026.9250780107127
At time: 21.949652194976807 and batch: 700, loss is 7.015767517089844 and perplexity is 1114.061379306193
At time: 23.48467755317688 and batch: 750, loss is 6.985075750350952 and perplexity is 1080.3882544223106
At time: 25.01797103881836 and batch: 800, loss is 6.788559656143189 and perplexity is 887.6341422658625
At time: 26.551504135131836 and batch: 850, loss is 6.79406847000122 and perplexity is 892.5374468256384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.0646546681722 and perplexity of 430.37403015684566
Finished 1 epochs...
Completing Train Step...
At time: 30.5381338596344 and batch: 50, loss is 6.537399520874024 and perplexity is 690.4886400044868
At time: 32.0643744468689 and batch: 100, loss is 6.476593294143677 and perplexity is 649.7536520094579
At time: 33.59311008453369 and batch: 150, loss is 6.486440992355346 and perplexity is 656.1838391927664
At time: 35.12016415596008 and batch: 200, loss is 6.552351331710815 and perplexity is 700.8902633081896
At time: 36.64744782447815 and batch: 250, loss is 6.56507700920105 and perplexity is 709.8665603487462
At time: 38.201597929000854 and batch: 300, loss is 6.48317512512207 and perplexity is 654.044325478859
At time: 39.72954964637756 and batch: 350, loss is 6.508960247039795 and perplexity is 671.1282471099705
At time: 41.262622117996216 and batch: 400, loss is 6.54435715675354 and perplexity is 695.3095601963736
At time: 42.79327082633972 and batch: 450, loss is 6.529043312072754 and perplexity is 684.7428128534891
At time: 44.323978424072266 and batch: 500, loss is 6.585169191360474 and perplexity is 724.2735781332394
At time: 45.85096454620361 and batch: 550, loss is 6.614216403961182 and perplexity is 745.6202362983664
At time: 47.38155770301819 and batch: 600, loss is 6.532270565032959 and perplexity is 686.9562208158655
At time: 48.91219186782837 and batch: 650, loss is 6.578571348190308 and perplexity is 719.510664409379
At time: 50.44482469558716 and batch: 700, loss is 6.486306228637695 and perplexity is 656.0954153774292
At time: 51.97581720352173 and batch: 750, loss is 6.60917109489441 and perplexity is 741.8678257551119
At time: 53.50649809837341 and batch: 800, loss is 6.628724164962769 and perplexity is 756.5163645840386
At time: 55.03718018531799 and batch: 850, loss is 6.6497925186157225 and perplexity is 772.6240038060021
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.530722300211589 and perplexity of 685.8934536002006
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 59.04416012763977 and batch: 50, loss is 6.54296314239502 and perplexity is 694.3409639612274
At time: 60.5741024017334 and batch: 100, loss is 6.29009051322937 and perplexity is 539.2021318021966
At time: 62.10657835006714 and batch: 150, loss is 6.228338117599487 and perplexity is 506.9123543787786
At time: 63.638997316360474 and batch: 200, loss is 6.242572927474976 and perplexity is 514.1797577035721
At time: 65.17096138000488 and batch: 250, loss is 6.2909458637237545 and perplexity is 539.6635359151935
At time: 66.73144721984863 and batch: 300, loss is 6.297328386306763 and perplexity is 543.1189660707023
At time: 68.26103281974792 and batch: 350, loss is 6.2002134609222415 and perplexity is 492.8542349849948
At time: 69.79196453094482 and batch: 400, loss is 6.1384814167022705 and perplexity is 463.34940166243166
At time: 71.3257417678833 and batch: 450, loss is 6.133290719985962 and perplexity is 460.9505267469303
At time: 72.85752129554749 and batch: 500, loss is 6.156276369094849 and perplexity is 471.6684814952092
At time: 74.38979458808899 and batch: 550, loss is 6.093201494216919 and perplexity is 442.8368838458118
At time: 75.92332410812378 and batch: 600, loss is 6.10401780128479 and perplexity is 447.6527415085953
At time: 77.45597743988037 and batch: 650, loss is 6.09645206451416 and perplexity is 444.2786983571341
At time: 78.98896384239197 and batch: 700, loss is 6.043381633758545 and perplexity is 421.31536288920734
At time: 80.52035450935364 and batch: 750, loss is 6.0145659923553465 and perplexity is 409.3481401359634
At time: 82.04975938796997 and batch: 800, loss is 5.939209537506104 and perplexity is 379.6347238925039
At time: 83.58091497421265 and batch: 850, loss is 5.93557674407959 and perplexity is 378.25809138780073
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.62672742207845 and perplexity of 277.7516647159207
Finished 3 epochs...
Completing Train Step...
At time: 87.56762599945068 and batch: 50, loss is 5.984546813964844 and perplexity is 397.24245571733906
At time: 89.10157561302185 and batch: 100, loss is 5.929694871902466 and perplexity is 376.0397560223625
At time: 90.6357650756836 and batch: 150, loss is 5.9106920623779295 and perplexity is 368.9614113843674
At time: 92.16985130310059 and batch: 200, loss is 5.946977224349975 and perplexity is 382.59509025724924
At time: 93.70406675338745 and batch: 250, loss is 5.999225788116455 and perplexity is 403.11657500387633
At time: 95.23662424087524 and batch: 300, loss is 6.016965885162353 and perplexity is 410.33171155395075
At time: 96.76751804351807 and batch: 350, loss is 5.965683469772339 and perplexity is 389.8193667971161
At time: 98.30155873298645 and batch: 400, loss is 5.935356931686401 and perplexity is 378.1749547090607
At time: 99.83884239196777 and batch: 450, loss is 5.932722482681275 and perplexity is 377.17998325209777
At time: 101.37742161750793 and batch: 500, loss is 5.963956565856933 and perplexity is 389.14676713094394
At time: 102.91594076156616 and batch: 550, loss is 5.913826141357422 and perplexity is 370.11957953525524
At time: 104.45041966438293 and batch: 600, loss is 5.924896659851075 and perplexity is 374.23975936944794
At time: 106.01146626472473 and batch: 650, loss is 5.9217406749725345 and perplexity is 373.0605261478244
At time: 107.54678416252136 and batch: 700, loss is 5.889822492599487 and perplexity is 361.34113795312595
At time: 109.07738375663757 and batch: 750, loss is 5.896410980224609 and perplexity is 363.7296893995871
At time: 110.60875749588013 and batch: 800, loss is 5.885447664260864 and perplexity is 359.7637853421111
At time: 112.14097619056702 and batch: 850, loss is 5.89254789352417 and perplexity is 362.32728063130617
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.576091766357422 and perplexity of 264.0376657744282
Finished 4 epochs...
Completing Train Step...
At time: 116.1487991809845 and batch: 50, loss is 5.914194641113281 and perplexity is 370.2559936426915
At time: 117.68085432052612 and batch: 100, loss is 5.851644802093506 and perplexity is 347.80598227172965
At time: 119.21485590934753 and batch: 150, loss is 5.846513833999634 and perplexity is 346.0259713685802
At time: 120.7485134601593 and batch: 200, loss is 5.861190767288208 and perplexity is 351.1420236070916
At time: 122.28179097175598 and batch: 250, loss is 5.898274002075195 and perplexity is 364.4079573764954
At time: 123.81384921073914 and batch: 300, loss is 5.89125322341919 and perplexity is 361.85848986303176
At time: 125.34534215927124 and batch: 350, loss is 5.864754142761231 and perplexity is 352.3955064702496
At time: 126.89883065223694 and batch: 400, loss is 5.866060380935669 and perplexity is 352.85611970310185
At time: 128.45186305046082 and batch: 450, loss is 5.86201587677002 and perplexity is 351.4318737828522
At time: 129.98875308036804 and batch: 500, loss is 5.891564855575561 and perplexity is 361.97127417722595
At time: 131.5547637939453 and batch: 550, loss is 5.846425247192383 and perplexity is 345.9953193902513
At time: 133.09327387809753 and batch: 600, loss is 5.849322290420532 and perplexity is 346.99913613543976
At time: 134.64202332496643 and batch: 650, loss is 5.862098922729492 and perplexity is 351.4610599918814
At time: 136.17836618423462 and batch: 700, loss is 5.827349987030029 and perplexity is 339.45791819890223
At time: 137.70802998542786 and batch: 750, loss is 5.842542552947998 and perplexity is 344.65452997714084
At time: 139.24406504631042 and batch: 800, loss is 5.853842182159424 and perplexity is 348.5710845063233
At time: 140.776522397995 and batch: 850, loss is 5.856615753173828 and perplexity is 349.53921312888576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.54818852742513 and perplexity of 256.77199892718204
Finished 5 epochs...
Completing Train Step...
At time: 144.79841494560242 and batch: 50, loss is 5.866998605728149 and perplexity is 353.1873334149474
At time: 146.34337711334229 and batch: 100, loss is 5.803765363693238 and perplexity is 331.54560230963637
At time: 147.88410139083862 and batch: 150, loss is 5.794048614501953 and perplexity is 328.3396577502451
At time: 149.4180212020874 and batch: 200, loss is 5.811075983047485 and perplexity is 333.97828738941735
At time: 150.94813704490662 and batch: 250, loss is 5.8430040740966795 and perplexity is 344.8136320433741
At time: 152.47878766059875 and batch: 300, loss is 5.82743537902832 and perplexity is 339.48690642653673
At time: 154.0082688331604 and batch: 350, loss is 5.807539720535278 and perplexity is 332.799338257219
At time: 155.54170179367065 and batch: 400, loss is 5.823749160766601 and perplexity is 338.2377872698163
At time: 157.0747766494751 and batch: 450, loss is 5.821540584564209 and perplexity is 337.49158766476563
At time: 158.6052393913269 and batch: 500, loss is 5.846358804702759 and perplexity is 345.97233136353213
At time: 160.13726496696472 and batch: 550, loss is 5.801786623001099 and perplexity is 330.89020817641193
At time: 161.67765474319458 and batch: 600, loss is 5.8075888729095455 and perplexity is 332.81569653686984
At time: 163.2107765674591 and batch: 650, loss is 5.8282850551605225 and perplexity is 339.7754829290443
At time: 164.74216604232788 and batch: 700, loss is 5.7897452545166015 and perplexity is 326.9297298948956
At time: 166.27422714233398 and batch: 750, loss is 5.80606143951416 and perplexity is 332.30773076816956
At time: 167.80421090126038 and batch: 800, loss is 5.81232250213623 and perplexity is 334.39485727709393
At time: 169.33620190620422 and batch: 850, loss is 5.788458080291748 and perplexity is 326.50918508851794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.4790833791097 and perplexity of 239.6269595998926
Finished 6 epochs...
Completing Train Step...
At time: 173.3225371837616 and batch: 50, loss is 5.783643093109131 and perplexity is 324.9408263909045
At time: 174.87597131729126 and batch: 100, loss is 5.716735029220581 and perplexity is 303.9110406741605
At time: 176.40876364707947 and batch: 150, loss is 5.7108779144287105 and perplexity is 302.1362016144057
At time: 177.94295859336853 and batch: 200, loss is 5.731184520721436 and perplexity is 308.33428049690013
At time: 179.47268342971802 and batch: 250, loss is 5.757687158584595 and perplexity is 316.61520069108883
At time: 181.00494408607483 and batch: 300, loss is 5.7326923370361325 and perplexity is 308.7995426321846
At time: 182.53639578819275 and batch: 350, loss is 5.720543584823608 and perplexity is 305.0707097011682
At time: 184.0958330631256 and batch: 400, loss is 5.735412921905517 and perplexity is 309.64080183518485
At time: 185.62894129753113 and batch: 450, loss is 5.73562915802002 and perplexity is 309.707764598688
At time: 187.16055631637573 and batch: 500, loss is 5.75814642906189 and perplexity is 316.7606461022642
At time: 188.69291806221008 and batch: 550, loss is 5.713008718490601 and perplexity is 302.7806810458994
At time: 190.22705674171448 and batch: 600, loss is 5.717866258621216 and perplexity is 304.2550283063272
At time: 191.76261711120605 and batch: 650, loss is 5.7421377086639405 and perplexity is 311.7300873253347
At time: 193.2951319217682 and batch: 700, loss is 5.706227989196777 and perplexity is 300.73455217510036
At time: 194.82736945152283 and batch: 750, loss is 5.716887016296386 and perplexity is 303.9572347348986
At time: 196.35926342010498 and batch: 800, loss is 5.729311494827271 and perplexity is 307.7573029211213
At time: 197.89361667633057 and batch: 850, loss is 5.725381469726562 and perplexity is 306.5501825560559
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.457379659016927 and perplexity of 234.48219535607578
Finished 7 epochs...
Completing Train Step...
At time: 201.89367246627808 and batch: 50, loss is 5.7306671142578125 and perplexity is 308.17478761209736
At time: 203.4588978290558 and batch: 100, loss is 5.669004163742065 and perplexity is 289.74585125076067
At time: 204.99717450141907 and batch: 150, loss is 5.667026576995849 and perplexity is 289.17341989844147
At time: 206.53625512123108 and batch: 200, loss is 5.687603397369385 and perplexity is 295.1853302951446
At time: 208.07018995285034 and batch: 250, loss is 5.713529262542725 and perplexity is 302.93833275728645
At time: 209.60793113708496 and batch: 300, loss is 5.687625312805176 and perplexity is 295.1917994811845
At time: 211.14408493041992 and batch: 350, loss is 5.676934509277344 and perplexity is 292.05277121456675
At time: 212.68197560310364 and batch: 400, loss is 5.69379301071167 and perplexity is 297.0180795072257
At time: 214.21913313865662 and batch: 450, loss is 5.692235116958618 and perplexity is 296.55571714584914
At time: 215.75680804252625 and batch: 500, loss is 5.715816774368286 and perplexity is 303.63210097486154
At time: 217.29547762870789 and batch: 550, loss is 5.675550947189331 and perplexity is 291.6489774738839
At time: 218.83382034301758 and batch: 600, loss is 5.684353475570679 and perplexity is 294.2275582406973
At time: 220.37240958213806 and batch: 650, loss is 5.712853326797485 and perplexity is 302.73363509859837
At time: 221.94164443016052 and batch: 700, loss is 5.676955461502075 and perplexity is 292.05889043396803
At time: 223.47639417648315 and batch: 750, loss is 5.687438058853149 and perplexity is 295.1365288251116
At time: 225.0171127319336 and batch: 800, loss is 5.699288196563721 and perplexity is 298.65474181879523
At time: 226.55624651908875 and batch: 850, loss is 5.6990705013275145 and perplexity is 298.58973318052296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.4381866455078125 and perplexity of 230.02468873944073
Finished 8 epochs...
Completing Train Step...
At time: 230.5663845539093 and batch: 50, loss is 5.702364568710327 and perplexity is 299.57492964234706
At time: 232.1282274723053 and batch: 100, loss is 5.639253244400025 and perplexity is 281.2526130725312
At time: 233.66633868217468 and batch: 150, loss is 5.635529222488404 and perplexity is 280.20717001339113
At time: 235.20300769805908 and batch: 200, loss is 5.6563947391510006 and perplexity is 286.11526072205413
At time: 236.7375843524933 and batch: 250, loss is 5.683812923431397 and perplexity is 294.0685558830586
At time: 238.27357459068298 and batch: 300, loss is 5.659136285781861 and perplexity is 286.90073526669613
At time: 239.80895733833313 and batch: 350, loss is 5.643260917663574 and perplexity is 282.3820433322831
At time: 241.34758281707764 and batch: 400, loss is 5.655509881973266 and perplexity is 285.8622015570657
At time: 242.8883512020111 and batch: 450, loss is 5.658892345428467 and perplexity is 286.8307571355383
At time: 244.42637848854065 and batch: 500, loss is 5.685073156356811 and perplexity is 294.4393843755707
At time: 245.97331857681274 and batch: 550, loss is 5.646498394012451 and perplexity is 283.29772997604994
At time: 247.50822496414185 and batch: 600, loss is 5.6585352420806885 and perplexity is 286.72834719847333
At time: 249.04192447662354 and batch: 650, loss is 5.684222354888916 and perplexity is 294.1889814518301
At time: 250.57447266578674 and batch: 700, loss is 5.646300277709961 and perplexity is 283.2416096366438
At time: 252.11367559432983 and batch: 750, loss is 5.654837627410888 and perplexity is 285.6700939676941
At time: 253.64667677879333 and batch: 800, loss is 5.668148078918457 and perplexity is 289.49791036915695
At time: 255.18082404136658 and batch: 850, loss is 5.667688541412353 and perplexity is 289.3649057840419
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.421003341674805 and perplexity of 226.10587017277962
Finished 9 epochs...
Completing Train Step...
At time: 259.16886949539185 and batch: 50, loss is 5.664744167327881 and perplexity is 288.5141603255525
At time: 260.7021813392639 and batch: 100, loss is 5.605693788528442 and perplexity is 271.9705500416416
At time: 262.2645561695099 and batch: 150, loss is 5.603095769882202 and perplexity is 271.26488254698023
At time: 263.798463344574 and batch: 200, loss is 5.6246467971801755 and perplexity is 277.17436846351336
At time: 265.333692073822 and batch: 250, loss is 5.652076740264892 and perplexity is 284.8824788358095
At time: 266.869017124176 and batch: 300, loss is 5.6254383563995365 and perplexity is 277.39385524717454
At time: 268.40423011779785 and batch: 350, loss is 5.605197839736938 and perplexity is 271.8357000181409
At time: 269.9384198188782 and batch: 400, loss is 5.620239343643188 and perplexity is 275.95542350772723
At time: 271.47207498550415 and batch: 450, loss is 5.618762855529785 and perplexity is 275.5482792508819
At time: 273.0059208869934 and batch: 500, loss is 5.635716648101806 and perplexity is 280.2596929360275
At time: 274.54103684425354 and batch: 550, loss is 5.59098858833313 and perplexity is 268.00043089671567
At time: 276.0763051509857 and batch: 600, loss is 5.603801498413086 and perplexity is 271.456389482026
At time: 277.6103072166443 and batch: 650, loss is 5.626700277328491 and perplexity is 277.7441253187593
At time: 279.15136909484863 and batch: 700, loss is 5.586747360229492 and perplexity is 266.8661869313334
At time: 280.6856851577759 and batch: 750, loss is 5.602286987304687 and perplexity is 271.04557693331884
At time: 282.2208185195923 and batch: 800, loss is 5.614165620803833 and perplexity is 274.2844264726879
At time: 283.7545807361603 and batch: 850, loss is 5.611589450836181 and perplexity is 273.5787325546404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.379767735799153 and perplexity of 216.97187477294014
Finished 10 epochs...
Completing Train Step...
At time: 287.7625005245209 and batch: 50, loss is 5.610846643447876 and perplexity is 273.3755917074172
At time: 289.2974045276642 and batch: 100, loss is 5.54456805229187 and perplexity is 255.844043124228
At time: 290.8309829235077 and batch: 150, loss is 5.544527883529663 and perplexity is 255.8337663921008
At time: 292.3651111125946 and batch: 200, loss is 5.556396598815918 and perplexity is 258.88827521742417
At time: 293.899795293808 and batch: 250, loss is 5.5857346534729 and perplexity is 266.59606654018035
At time: 295.4334092140198 and batch: 300, loss is 5.557665634155273 and perplexity is 259.2170221392313
At time: 296.9680070877075 and batch: 350, loss is 5.54764892578125 and perplexity is 256.6334817098786
At time: 298.503130197525 and batch: 400, loss is 5.5571499443054195 and perplexity is 259.08338101365246
At time: 300.064270734787 and batch: 450, loss is 5.560273733139038 and perplexity is 259.8939681786602
At time: 301.598669052124 and batch: 500, loss is 5.591771831512451 and perplexity is 268.2104226328329
At time: 303.1342990398407 and batch: 550, loss is 5.552896804809571 and perplexity is 257.98380323951443
At time: 304.67032623291016 and batch: 600, loss is 5.565875511169434 and perplexity is 261.3539218502706
At time: 306.2048599720001 and batch: 650, loss is 5.590394678115845 and perplexity is 267.8413099590219
At time: 307.73905324935913 and batch: 700, loss is 5.544540910720825 and perplexity is 255.83709920918994
At time: 309.2734622955322 and batch: 750, loss is 5.55587028503418 and perplexity is 258.75205460078524
At time: 310.8093993663788 and batch: 800, loss is 5.571725072860718 and perplexity is 262.88720789295894
At time: 312.35327196121216 and batch: 850, loss is 5.579030237197876 and perplexity is 264.81467380471105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.362060546875 and perplexity of 213.1637280829995
Finished 11 epochs...
Completing Train Step...
At time: 316.35573530197144 and batch: 50, loss is 5.584571323394775 and perplexity is 266.28610764447416
At time: 317.89082312583923 and batch: 100, loss is 5.512721767425537 and perplexity is 247.82473123556298
At time: 319.4261074066162 and batch: 150, loss is 5.507705030441284 and perplexity is 246.58457311530265
At time: 320.95996284484863 and batch: 200, loss is 5.52231014251709 and perplexity is 250.2123963370491
At time: 322.49407625198364 and batch: 250, loss is 5.540564689636231 and perplexity is 254.82185409821426
At time: 324.02775478363037 and batch: 300, loss is 5.523448095321656 and perplexity is 250.49728830125977
At time: 325.5621600151062 and batch: 350, loss is 5.513461170196533 and perplexity is 248.00804129018937
At time: 327.0955488681793 and batch: 400, loss is 5.5310952091217045 and perplexity is 252.4202126112393
At time: 328.628050327301 and batch: 450, loss is 5.540864601135254 and perplexity is 254.8982895638253
At time: 330.16061210632324 and batch: 500, loss is 5.56374418258667 and perplexity is 260.79748395304887
At time: 331.6961603164673 and batch: 550, loss is 5.5239474487304685 and perplexity is 250.62240621249924
At time: 333.23372077941895 and batch: 600, loss is 5.533459205627441 and perplexity is 253.01763899054066
At time: 334.7666094303131 and batch: 650, loss is 5.549469261169434 and perplexity is 257.10106616958103
At time: 336.2999761104584 and batch: 700, loss is 5.508710355758667 and perplexity is 246.83259548022363
At time: 337.83518147468567 and batch: 750, loss is 5.521229791641235 and perplexity is 249.94222512164902
At time: 339.3978228569031 and batch: 800, loss is 5.53028546333313 and perplexity is 252.21589913927687
At time: 340.93270683288574 and batch: 850, loss is 5.536198244094849 and perplexity is 253.71161401494297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.34266726175944 and perplexity of 209.0696106369462
Finished 12 epochs...
Completing Train Step...
At time: 344.91636395454407 and batch: 50, loss is 5.543954620361328 and perplexity is 255.6871483459898
At time: 346.4920301437378 and batch: 100, loss is 5.478236999511719 and perplexity is 239.4242300353468
At time: 348.0260100364685 and batch: 150, loss is 5.4777681922912596 and perplexity is 239.31201253379612
At time: 349.56163024902344 and batch: 200, loss is 5.494876651763916 and perplexity is 243.44149622948888
At time: 351.09674525260925 and batch: 250, loss is 5.513748435974121 and perplexity is 248.07929574701228
At time: 352.6311583518982 and batch: 300, loss is 5.491034564971923 and perplexity is 242.50796737002014
At time: 354.1651248931885 and batch: 350, loss is 5.478407564163208 and perplexity is 239.4650708285998
At time: 355.6972303390503 and batch: 400, loss is 5.491594486236572 and perplexity is 242.64379075945078
At time: 357.2297441959381 and batch: 450, loss is 5.5014521026611325 and perplexity is 245.04750817410073
At time: 358.76329731941223 and batch: 500, loss is 5.531248388290405 and perplexity is 252.45888109109745
At time: 360.2961893081665 and batch: 550, loss is 5.493645868301392 and perplexity is 243.14205677220332
At time: 361.8311035633087 and batch: 600, loss is 5.499082050323486 and perplexity is 244.46742044519382
At time: 363.3625726699829 and batch: 650, loss is 5.513206920623779 and perplexity is 247.9449933669453
At time: 364.896427154541 and batch: 700, loss is 5.4769579124450685 and perplexity is 239.1181813724354
At time: 366.4328565597534 and batch: 750, loss is 5.486515550613404 and perplexity is 241.41454284476362
At time: 367.9686396121979 and batch: 800, loss is 5.494749565124511 and perplexity is 243.41056003367157
At time: 369.500869512558 and batch: 850, loss is 5.510267381668091 and perplexity is 247.21721958381943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.331827481587728 and perplexity of 206.81558068248296
Finished 13 epochs...
Completing Train Step...
At time: 373.4915859699249 and batch: 50, loss is 5.517817239761353 and perplexity is 249.09073800975372
At time: 375.0540060997009 and batch: 100, loss is 5.448681201934814 and perplexity is 232.45140720965586
At time: 376.59750485420227 and batch: 150, loss is 5.454861888885498 and perplexity is 233.89256567573528
At time: 378.18191504478455 and batch: 200, loss is 5.473027620315552 and perplexity is 238.18022150171572
At time: 379.7197468280792 and batch: 250, loss is 5.494262428283691 and perplexity is 243.29201465868852
At time: 381.26360273361206 and batch: 300, loss is 5.464938497543335 and perplexity is 236.2613240101746
At time: 382.80711340904236 and batch: 350, loss is 5.447136306762696 and perplexity is 232.0925714061252
At time: 384.36731934547424 and batch: 400, loss is 5.457322559356689 and perplexity is 234.46880688463148
At time: 385.9544770717621 and batch: 450, loss is 5.47168086051941 and perplexity is 237.85966585933772
At time: 387.5065999031067 and batch: 500, loss is 5.5047346782684325 and perplexity is 245.8532168233626
At time: 389.05585956573486 and batch: 550, loss is 5.470190744400025 and perplexity is 237.50549128324715
At time: 390.60654854774475 and batch: 600, loss is 5.481207695007324 and perplexity is 240.13654402718572
At time: 392.15784549713135 and batch: 650, loss is 5.4943279457092284 and perplexity is 243.30795504732362
At time: 393.70474076271057 and batch: 700, loss is 5.456288585662842 and perplexity is 234.2264975985864
At time: 395.2531385421753 and batch: 750, loss is 5.467090606689453 and perplexity is 236.77033169003371
At time: 396.80428433418274 and batch: 800, loss is 5.477128629684448 and perplexity is 239.15900645291816
At time: 398.35112142562866 and batch: 850, loss is 5.482184677124024 and perplexity is 240.3712677780496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.32456620534261 and perplexity of 205.31927473928323
Finished 14 epochs...
Completing Train Step...
At time: 402.53756284713745 and batch: 50, loss is 5.497881650924683 and perplexity is 244.17413796393873
At time: 404.23869705200195 and batch: 100, loss is 5.435533027648926 and perplexity is 229.4151002825358
At time: 405.78327536582947 and batch: 150, loss is 5.436457386016846 and perplexity is 229.62726009094976
At time: 407.3341076374054 and batch: 200, loss is 5.458815746307373 and perplexity is 234.81917416424037
At time: 408.889014005661 and batch: 250, loss is 5.474945449829102 and perplexity is 238.6374488619757
At time: 410.43981075286865 and batch: 300, loss is 5.450283746719361 and perplexity is 232.82421964446516
At time: 411.9901247024536 and batch: 350, loss is 5.423634824752807 and perplexity is 226.70164748914107
At time: 413.54006004333496 and batch: 400, loss is 5.4379572010040285 and perplexity is 229.97191689321136
At time: 415.09527111053467 and batch: 450, loss is 5.453280963897705 and perplexity is 233.5230912068627
At time: 416.6462404727936 and batch: 500, loss is 5.479785919189453 and perplexity is 239.79536629249986
At time: 418.24528789520264 and batch: 550, loss is 5.451107902526855 and perplexity is 233.01618216986
At time: 419.7908339500427 and batch: 600, loss is 5.458053731918335 and perplexity is 234.64030673313863
At time: 421.33666133880615 and batch: 650, loss is 5.46686487197876 and perplexity is 236.71689043970838
At time: 422.87554359436035 and batch: 700, loss is 5.4342022800445555 and perplexity is 229.11000973171164
At time: 424.4228301048279 and batch: 750, loss is 5.448248414993286 and perplexity is 232.3508270425371
At time: 425.9710237979889 and batch: 800, loss is 5.456149921417237 and perplexity is 234.1940210097191
At time: 427.51875400543213 and batch: 850, loss is 5.4614916515350345 and perplexity is 235.44836947801193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.3111467361450195 and perplexity of 202.58240380552212
Finished 15 epochs...
Completing Train Step...
At time: 431.6799054145813 and batch: 50, loss is 5.466577949523926 and perplexity is 236.6489807912703
At time: 433.2260220050812 and batch: 100, loss is 5.398518209457397 and perplexity is 221.07858122195796
At time: 434.769113779068 and batch: 150, loss is 5.403366241455078 and perplexity is 222.15297951203334
At time: 436.31169271469116 and batch: 200, loss is 5.426445350646973 and perplexity is 227.33969454354602
At time: 437.85100984573364 and batch: 250, loss is 5.4492503261566165 and perplexity is 232.5837385888156
At time: 439.38581466674805 and batch: 300, loss is 5.421691637039185 and perplexity is 226.26155136625445
At time: 440.91966485977173 and batch: 350, loss is 5.403465728759766 and perplexity is 222.17508201263422
At time: 442.4539086818695 and batch: 400, loss is 5.40838740348816 and perplexity is 223.271250779417
At time: 444.00934290885925 and batch: 450, loss is 5.428572320938111 and perplexity is 227.82375392731018
At time: 445.5599341392517 and batch: 500, loss is 5.459371528625488 and perplexity is 234.9497187830304
At time: 447.1052384376526 and batch: 550, loss is 5.427521657943726 and perplexity is 227.584513642282
At time: 448.6545968055725 and batch: 600, loss is 5.435689001083374 and perplexity is 229.45088573435848
At time: 450.20394563674927 and batch: 650, loss is 5.438532581329346 and perplexity is 230.10427628441045
At time: 451.75328540802 and batch: 700, loss is 5.409069395065307 and perplexity is 223.4235718267996
At time: 453.29799723625183 and batch: 750, loss is 5.42844087600708 and perplexity is 227.7938096177451
At time: 454.8430495262146 and batch: 800, loss is 5.427869167327881 and perplexity is 227.66361513992135
At time: 456.43954968452454 and batch: 850, loss is 5.439337940216064 and perplexity is 230.28966745137944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.2891050974528 and perplexity of 198.1660067707982
Finished 16 epochs...
Completing Train Step...
At time: 460.56248235702515 and batch: 50, loss is 5.441602573394776 and perplexity is 230.81178004661678
At time: 462.10706305503845 and batch: 100, loss is 5.369558200836182 and perplexity is 214.76796243512607
At time: 463.63876605033875 and batch: 150, loss is 5.372778491973877 and perplexity is 215.46069259897035
At time: 465.17494201660156 and batch: 200, loss is 5.39916145324707 and perplexity is 221.22083439318828
At time: 466.717342376709 and batch: 250, loss is 5.4291770553588865 and perplexity is 227.9615684595761
At time: 468.26908707618713 and batch: 300, loss is 5.396922826766968 and perplexity is 220.72615748037066
At time: 469.8328754901886 and batch: 350, loss is 5.375941753387451 and perplexity is 216.143330205392
At time: 471.38556361198425 and batch: 400, loss is 5.3862069225311275 and perplexity is 218.37350502666177
At time: 472.9243378639221 and batch: 450, loss is 5.404020757675171 and perplexity is 222.29842983507118
At time: 474.45950412750244 and batch: 500, loss is 5.430229549407959 and perplexity is 228.20162295960347
At time: 475.9944489002228 and batch: 550, loss is 5.403644485473633 and perplexity is 222.21480085009654
At time: 477.52938771247864 and batch: 600, loss is 5.414841661453247 and perplexity is 224.71696150440147
At time: 479.06548619270325 and batch: 650, loss is 5.425886878967285 and perplexity is 227.21276720843449
At time: 480.5999598503113 and batch: 700, loss is 5.389980726242065 and perplexity is 219.1991607216772
At time: 482.13894629478455 and batch: 750, loss is 5.400353336334229 and perplexity is 221.48466095820666
At time: 483.6740093231201 and batch: 800, loss is 5.413231334686279 and perplexity is 224.35538497261828
At time: 485.2112090587616 and batch: 850, loss is 5.423893127441406 and perplexity is 226.76021269764718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.269441604614258 and perplexity of 194.30743177338195
Finished 17 epochs...
Completing Train Step...
At time: 489.5649561882019 and batch: 50, loss is 5.419267826080322 and perplexity is 225.71380022925126
At time: 491.1059002876282 and batch: 100, loss is 5.358473777770996 and perplexity is 212.4005285374999
At time: 492.6582510471344 and batch: 150, loss is 5.353505325317383 and perplexity is 211.3478438826719
At time: 494.20180106163025 and batch: 200, loss is 5.383630619049073 and perplexity is 217.81163269287615
At time: 495.7400562763214 and batch: 250, loss is 5.412203741073609 and perplexity is 224.12495722536943
At time: 497.3060646057129 and batch: 300, loss is 5.384160795211792 and perplexity is 217.92714184588698
At time: 498.8454053401947 and batch: 350, loss is 5.360851669311524 and perplexity is 212.90619492916173
At time: 500.3869717121124 and batch: 400, loss is 5.374800367355347 and perplexity is 215.896767965479
At time: 501.9279477596283 and batch: 450, loss is 5.3938881778717045 and perplexity is 220.05734640707144
At time: 503.46373653411865 and batch: 500, loss is 5.418766345977783 and perplexity is 225.60063762633624
At time: 505.00077772140503 and batch: 550, loss is 5.397276525497436 and perplexity is 220.8042418504099
At time: 506.53880500793457 and batch: 600, loss is 5.405662221908569 and perplexity is 222.6636244016973
At time: 508.0780987739563 and batch: 650, loss is 5.412849054336548 and perplexity is 224.26963470895157
At time: 509.6163079738617 and batch: 700, loss is 5.379717502593994 and perplexity is 216.96097585398704
At time: 511.1572940349579 and batch: 750, loss is 5.394646396636963 and perplexity is 220.22426128755103
At time: 512.6958477497101 and batch: 800, loss is 5.402948226928711 and perplexity is 222.06013574589488
At time: 514.2348940372467 and batch: 850, loss is 5.412323417663575 and perplexity is 224.15178134105378
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.276482264200847 and perplexity of 195.68031157407316
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 518.293714761734 and batch: 50, loss is 5.404621772766113 and perplexity is 222.43207470336324
At time: 519.8593816757202 and batch: 100, loss is 5.321381607055664 and perplexity is 204.66645536412628
At time: 521.4057638645172 and batch: 150, loss is 5.30602910041809 and perplexity is 201.54830917490054
At time: 522.9446606636047 and batch: 200, loss is 5.33678635597229 and perplexity is 207.8437002188496
At time: 524.4865531921387 and batch: 250, loss is 5.345741567611694 and perplexity is 209.7133435734103
At time: 526.0245532989502 and batch: 300, loss is 5.293899564743042 and perplexity is 199.1183884653808
At time: 527.5641584396362 and batch: 350, loss is 5.264398059844971 and perplexity is 193.32990072517876
At time: 529.1036686897278 and batch: 400, loss is 5.287373027801514 and perplexity is 197.8230665284838
At time: 530.6416890621185 and batch: 450, loss is 5.307529354095459 and perplexity is 201.85090969892536
At time: 532.1831297874451 and batch: 500, loss is 5.312716798782349 and perplexity is 202.90072069205254
At time: 533.7346584796906 and batch: 550, loss is 5.273122959136963 and perplexity is 195.02406459653903
At time: 535.3513021469116 and batch: 600, loss is 5.2667794609069825 and perplexity is 193.7908453852984
At time: 536.8981523513794 and batch: 650, loss is 5.268331756591797 and perplexity is 194.09189968053605
At time: 538.4506454467773 and batch: 700, loss is 5.220800065994263 and perplexity is 185.08220282702626
At time: 540.0097434520721 and batch: 750, loss is 5.210142784118652 and perplexity is 183.12020298393378
At time: 541.5634689331055 and batch: 800, loss is 5.212872676849365 and perplexity is 183.62078445088602
At time: 543.1147663593292 and batch: 850, loss is 5.251844863891602 and perplexity is 190.91816178410517
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.13734753926595 and perplexity of 170.26355145803674
Finished 19 epochs...
Completing Train Step...
At time: 547.2393269538879 and batch: 50, loss is 5.3022332191467285 and perplexity is 200.7847059133667
At time: 548.8193504810333 and batch: 100, loss is 5.240467433929443 and perplexity is 188.75831382790162
At time: 550.3605024814606 and batch: 150, loss is 5.231492214202881 and perplexity is 187.0717464641279
At time: 551.9002487659454 and batch: 200, loss is 5.273184518814087 and perplexity is 195.03607058452573
At time: 553.4385211467743 and batch: 250, loss is 5.284777812957763 and perplexity is 197.31033877690314
At time: 554.975944519043 and batch: 300, loss is 5.2448569011688235 and perplexity is 189.5886833693028
At time: 556.5137703418732 and batch: 350, loss is 5.215717010498047 and perplexity is 184.1438066989723
At time: 558.0517921447754 and batch: 400, loss is 5.239886198043823 and perplexity is 188.64863260061622
At time: 559.5866062641144 and batch: 450, loss is 5.2642304229736325 and perplexity is 193.29749422182337
At time: 561.1231923103333 and batch: 500, loss is 5.2697602558135985 and perplexity is 194.36935793545626
At time: 562.6606693267822 and batch: 550, loss is 5.239794187545776 and perplexity is 188.63127574449342
At time: 564.1984462738037 and batch: 600, loss is 5.2454695129394535 and perplexity is 189.70486321124898
At time: 565.7389452457428 and batch: 650, loss is 5.247112150192261 and perplexity is 190.0167355630441
At time: 567.2744908332825 and batch: 700, loss is 5.208281717300415 and perplexity is 182.77972097856633
At time: 568.8115284442902 and batch: 750, loss is 5.2094448280334475 and perplexity is 182.9924377163969
At time: 570.3489756584167 and batch: 800, loss is 5.220591583251953 and perplexity is 185.04362040385257
At time: 571.8948633670807 and batch: 850, loss is 5.257414178848267 and perplexity is 191.98441154237992
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.133012135823567 and perplexity of 169.5269880743343
Finished 20 epochs...
Completing Train Step...
At time: 576.1368093490601 and batch: 50, loss is 5.282655019760131 and perplexity is 196.89193398247056
At time: 577.7264015674591 and batch: 100, loss is 5.220401124954224 and perplexity is 185.00838066686146
At time: 579.2798199653625 and batch: 150, loss is 5.208076591491699 and perplexity is 182.74223198559525
At time: 580.8169476985931 and batch: 200, loss is 5.2552080154418945 and perplexity is 191.56132942478467
At time: 582.3598334789276 and batch: 250, loss is 5.269307804107666 and perplexity is 194.28143507980033
At time: 583.8971316814423 and batch: 300, loss is 5.230342903137207 and perplexity is 186.85686634154635
At time: 585.4354982376099 and batch: 350, loss is 5.200469646453858 and perplexity is 181.35739573526968
At time: 586.9723620414734 and batch: 400, loss is 5.227628002166748 and perplexity is 186.35025646277634
At time: 588.5087885856628 and batch: 450, loss is 5.2525279712677 and perplexity is 191.0486239434094
At time: 590.0463120937347 and batch: 500, loss is 5.258672618865967 and perplexity is 192.2261644925282
At time: 591.5916197299957 and batch: 550, loss is 5.230685291290283 and perplexity is 186.92085487273494
At time: 593.130252122879 and batch: 600, loss is 5.240619306564331 and perplexity is 188.78698322737313
At time: 594.6655948162079 and batch: 650, loss is 5.244094305038452 and perplexity is 189.44415888691267
At time: 596.201619386673 and batch: 700, loss is 5.207307281494141 and perplexity is 182.60170062257487
At time: 597.7400281429291 and batch: 750, loss is 5.211379661560058 and perplexity is 183.34684036450832
At time: 599.2755680084229 and batch: 800, loss is 5.224584894180298 and perplexity is 185.7840344833298
At time: 600.8086218833923 and batch: 850, loss is 5.256441431045532 and perplexity is 191.79774992994837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.130687077840169 and perplexity of 169.1332858649025
Finished 21 epochs...
Completing Train Step...
At time: 604.8943827152252 and batch: 50, loss is 5.269939861297607 and perplexity is 194.40427087324844
At time: 606.4292650222778 and batch: 100, loss is 5.20893856048584 and perplexity is 182.89981803087224
At time: 607.9625990390778 and batch: 150, loss is 5.199161443710327 and perplexity is 181.12029861197678
At time: 609.4980471134186 and batch: 200, loss is 5.244736986160278 and perplexity is 189.56595020376764
At time: 611.0457262992859 and batch: 250, loss is 5.259983263015747 and perplexity is 192.47826976451628
At time: 612.5972888469696 and batch: 300, loss is 5.2225765609741215 and perplexity is 185.411292657877
At time: 614.2657923698425 and batch: 350, loss is 5.194447555541992 and perplexity is 180.2685269347694
At time: 615.8006451129913 and batch: 400, loss is 5.2222565650939945 and perplexity is 185.35197129989874
At time: 617.3366601467133 and batch: 450, loss is 5.245367641448975 and perplexity is 189.68553867840876
At time: 618.8913068771362 and batch: 500, loss is 5.251495132446289 and perplexity is 190.851403373886
At time: 620.4462304115295 and batch: 550, loss is 5.226188449859619 and perplexity is 186.0821885163601
At time: 621.9997889995575 and batch: 600, loss is 5.238080682754517 and perplexity is 188.30833191164263
At time: 623.5548331737518 and batch: 650, loss is 5.2422480869293215 and perplexity is 189.094726313733
At time: 625.1152272224426 and batch: 700, loss is 5.2069001960754395 and perplexity is 182.52738126101187
At time: 626.6667490005493 and batch: 750, loss is 5.210677194595337 and perplexity is 183.21809049262643
At time: 628.2077977657318 and batch: 800, loss is 5.224538507461548 and perplexity is 185.7754167714493
At time: 629.7498676776886 and batch: 850, loss is 5.253769340515137 and perplexity is 191.28593309360417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.128558158874512 and perplexity of 168.77359781410834
Finished 22 epochs...
Completing Train Step...
At time: 633.8657290935516 and batch: 50, loss is 5.261546354293824 and perplexity is 192.77936612845005
At time: 635.41770195961 and batch: 100, loss is 5.201467037200928 and perplexity is 181.53837015979443
At time: 636.9698348045349 and batch: 150, loss is 5.192062616348267 and perplexity is 179.83910972998996
At time: 638.5175786018372 and batch: 200, loss is 5.238592071533203 and perplexity is 188.40465530676784
At time: 640.0702481269836 and batch: 250, loss is 5.254094028472901 and perplexity is 191.34805141655923
At time: 641.6141791343689 and batch: 300, loss is 5.216692008972168 and perplexity is 184.32343418355458
At time: 643.1585612297058 and batch: 350, loss is 5.189278726577759 and perplexity is 179.33915370619556
At time: 644.7087948322296 and batch: 400, loss is 5.217350988388062 and perplexity is 184.44493956292334
At time: 646.2660677433014 and batch: 450, loss is 5.242008028030395 and perplexity is 189.04933789010678
At time: 647.8231854438782 and batch: 500, loss is 5.24872317314148 and perplexity is 190.32310360148008
At time: 649.3741588592529 and batch: 550, loss is 5.2232444477081295 and perplexity is 185.53516776324346
At time: 650.9230253696442 and batch: 600, loss is 5.235703239440918 and perplexity is 187.86117128718408
At time: 652.4727666378021 and batch: 650, loss is 5.239515552520752 and perplexity is 188.57872378600413
At time: 654.0943758487701 and batch: 700, loss is 5.204304933547974 and perplexity is 182.05428895309637
At time: 655.651686668396 and batch: 750, loss is 5.209641313552856 and perplexity is 183.02839661315502
At time: 657.2045114040375 and batch: 800, loss is 5.222594604492188 and perplexity is 185.414638160068
At time: 658.7597825527191 and batch: 850, loss is 5.250652389526367 and perplexity is 190.69063245872596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.127326011657715 and perplexity of 168.56577195761145
Finished 23 epochs...
Completing Train Step...
At time: 662.9433026313782 and batch: 50, loss is 5.254826889038086 and perplexity is 191.4883342552691
At time: 664.4906041622162 and batch: 100, loss is 5.195836315155029 and perplexity is 180.5190505027203
At time: 666.0415937900543 and batch: 150, loss is 5.186784000396728 and perplexity is 178.89230923321426
At time: 667.5980577468872 and batch: 200, loss is 5.232870817184448 and perplexity is 187.3298219825363
At time: 669.1551213264465 and batch: 250, loss is 5.247781925201416 and perplexity is 190.14404665397967
At time: 670.7051770687103 and batch: 300, loss is 5.212262802124023 and perplexity is 183.50883291707746
At time: 672.2586417198181 and batch: 350, loss is 5.185810518264771 and perplexity is 178.71824550434226
At time: 673.8088116645813 and batch: 400, loss is 5.214350919723511 and perplexity is 183.8924212901916
At time: 675.3622918128967 and batch: 450, loss is 5.2389146900177 and perplexity is 188.46544793702094
At time: 676.9193179607391 and batch: 500, loss is 5.245221548080444 and perplexity is 189.65782890325815
At time: 678.4687414169312 and batch: 550, loss is 5.2196918392181395 and perplexity is 184.87720338798425
At time: 680.0165009498596 and batch: 600, loss is 5.232943954467774 and perplexity is 187.3435232778336
At time: 681.5693714618683 and batch: 650, loss is 5.236142101287842 and perplexity is 187.9436344814319
At time: 683.1242496967316 and batch: 700, loss is 5.202179555892944 and perplexity is 181.66776573476008
At time: 684.6672270298004 and batch: 750, loss is 5.208046169281006 and perplexity is 182.73667264747525
At time: 686.203724861145 and batch: 800, loss is 5.219611015319824 and perplexity is 184.86226149553593
At time: 687.7410316467285 and batch: 850, loss is 5.2458192825317385 and perplexity is 189.7712278093915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.125707308451335 and perplexity of 168.29313472097266
Finished 24 epochs...
Completing Train Step...
At time: 691.7904119491577 and batch: 50, loss is 5.247909879684448 and perplexity is 190.16837799378973
At time: 693.3654198646545 and batch: 100, loss is 5.189523344039917 and perplexity is 179.3830285608998
At time: 694.905308008194 and batch: 150, loss is 5.180276641845703 and perplexity is 177.73197229387026
At time: 696.4440159797668 and batch: 200, loss is 5.227620697021484 and perplexity is 186.34889515205523
At time: 697.9835774898529 and batch: 250, loss is 5.242119817733765 and perplexity is 189.07047284082472
At time: 699.5213015079498 and batch: 300, loss is 5.206686382293701 and perplexity is 182.48835856329757
At time: 701.0691430568695 and batch: 350, loss is 5.180449075698853 and perplexity is 177.7626219451236
At time: 702.6093835830688 and batch: 400, loss is 5.2076996898651124 and perplexity is 182.67336911919213
At time: 704.1494114398956 and batch: 450, loss is 5.233317213058472 and perplexity is 187.41346390946745
At time: 705.6878569126129 and batch: 500, loss is 5.238920698165893 and perplexity is 188.46658026876304
At time: 707.2257225513458 and batch: 550, loss is 5.214549865722656 and perplexity is 183.92900959110773
At time: 708.764575958252 and batch: 600, loss is 5.227324533462524 and perplexity is 186.2937135718487
At time: 710.3116743564606 and batch: 650, loss is 5.231041450500488 and perplexity is 186.98744031358243
At time: 711.8515932559967 and batch: 700, loss is 5.195995903015136 and perplexity is 180.54786145057622
At time: 713.3899750709534 and batch: 750, loss is 5.201869068145752 and perplexity is 181.61136887515957
At time: 714.9296343326569 and batch: 800, loss is 5.213693923950196 and perplexity is 183.7716444259482
At time: 716.469527721405 and batch: 850, loss is 5.240275688171387 and perplexity is 188.72212369169068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.119808197021484 and perplexity of 167.30327727660176
Finished 25 epochs...
Completing Train Step...
At time: 720.4817705154419 and batch: 50, loss is 5.23978895187378 and perplexity is 188.63028813559077
At time: 722.0463147163391 and batch: 100, loss is 5.1811885643005375 and perplexity is 177.89412399399814
At time: 723.5848395824432 and batch: 150, loss is 5.172619771957398 and perplexity is 176.3762984395064
At time: 725.1225862503052 and batch: 200, loss is 5.21899034500122 and perplexity is 184.7475585768265
At time: 726.662050485611 and batch: 250, loss is 5.234163293838501 and perplexity is 187.57209793833812
At time: 728.1980390548706 and batch: 300, loss is 5.198561925888061 and perplexity is 181.01174630775657
At time: 729.7353835105896 and batch: 350, loss is 5.172925100326538 and perplexity is 176.43015934927746
At time: 731.3319940567017 and batch: 400, loss is 5.202263669967651 and perplexity is 181.68304719346284
At time: 732.888177394867 and batch: 450, loss is 5.227925949096679 and perplexity is 186.40578722178017
At time: 734.4432723522186 and batch: 500, loss is 5.234329776763916 and perplexity is 187.60332808950054
At time: 735.9947180747986 and batch: 550, loss is 5.21020697593689 and perplexity is 183.13195818000906
At time: 737.5488572120667 and batch: 600, loss is 5.22335919380188 and perplexity is 185.55645842048412
At time: 739.0970420837402 and batch: 650, loss is 5.225947504043579 and perplexity is 186.03735819265378
At time: 740.6363303661346 and batch: 700, loss is 5.191544151306152 and perplexity is 179.74589360512908
At time: 742.175329208374 and batch: 750, loss is 5.197787618637085 and perplexity is 180.8716418490277
At time: 743.714822769165 and batch: 800, loss is 5.210054044723511 and perplexity is 183.10395372886802
At time: 745.2659356594086 and batch: 850, loss is 5.23584361076355 and perplexity is 187.88754345917394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.117388725280762 and perplexity of 166.89898101401573
Finished 26 epochs...
Completing Train Step...
At time: 749.2670223712921 and batch: 50, loss is 5.232969884872436 and perplexity is 187.34838123418723
At time: 750.8310120105743 and batch: 100, loss is 5.175222272872925 and perplexity is 176.83591573547326
At time: 752.3687906265259 and batch: 150, loss is 5.167658596038819 and perplexity is 175.50343160837286
At time: 753.9238705635071 and batch: 200, loss is 5.213994731903076 and perplexity is 183.82693271326687
At time: 755.4626746177673 and batch: 250, loss is 5.227818422317505 and perplexity is 186.38574468543467
At time: 757.0001094341278 and batch: 300, loss is 5.192815380096436 and perplexity is 179.9745370582943
At time: 758.552677154541 and batch: 350, loss is 5.167927103042603 and perplexity is 175.5505618360628
At time: 760.0920383930206 and batch: 400, loss is 5.1968890571594235 and perplexity is 180.70919055642804
At time: 761.6346371173859 and batch: 450, loss is 5.222563343048096 and perplexity is 185.4088419213232
At time: 763.1716723442078 and batch: 500, loss is 5.228491039276123 and perplexity is 186.51115306932962
At time: 764.7163860797882 and batch: 550, loss is 5.204838266372681 and perplexity is 182.1514103779936
At time: 766.2551121711731 and batch: 600, loss is 5.218793210983276 and perplexity is 184.71114213787624
At time: 767.7942674160004 and batch: 650, loss is 5.221778163909912 and perplexity is 185.26331990451388
At time: 769.3315060138702 and batch: 700, loss is 5.186363849639893 and perplexity is 178.81716328151757
At time: 770.916974067688 and batch: 750, loss is 5.193377075195312 and perplexity is 180.07565627010675
At time: 772.4594116210938 and batch: 800, loss is 5.205346031188965 and perplexity is 182.2439239409985
At time: 774.0162556171417 and batch: 850, loss is 5.23133581161499 and perplexity is 187.04249024679336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.112722396850586 and perplexity of 166.1219898119011
Finished 27 epochs...
Completing Train Step...
At time: 778.2078192234039 and batch: 50, loss is 5.227084360122681 and perplexity is 186.24897616105002
At time: 779.7620694637299 and batch: 100, loss is 5.169038000106812 and perplexity is 175.7456888027469
At time: 781.3156952857971 and batch: 150, loss is 5.1619273853302 and perplexity is 174.50046132411663
At time: 782.8686158657074 and batch: 200, loss is 5.208426361083984 and perplexity is 182.8061608411032
At time: 784.4177582263947 and batch: 250, loss is 5.222152795791626 and perplexity is 185.3327384530511
At time: 785.9654850959778 and batch: 300, loss is 5.1881869029998775 and perplexity is 179.14345384404453
At time: 787.518060207367 and batch: 350, loss is 5.164636030197143 and perplexity is 174.97376181505987
At time: 789.0617964267731 and batch: 400, loss is 5.194297647476196 and perplexity is 180.24150525400722
At time: 790.6099026203156 and batch: 450, loss is 5.219613857269287 and perplexity is 184.8627868654873
At time: 792.1686203479767 and batch: 500, loss is 5.2250615501403805 and perplexity is 185.8726106591585
At time: 793.7256324291229 and batch: 550, loss is 5.2011325073242185 and perplexity is 181.4776503080754
At time: 795.2773001194 and batch: 600, loss is 5.21605393409729 and perplexity is 184.20585954605093
At time: 796.8291261196136 and batch: 650, loss is 5.218207683563232 and perplexity is 184.6030203565898
At time: 798.3808450698853 and batch: 700, loss is 5.183499975204468 and perplexity is 178.30578598867513
At time: 799.9374730587006 and batch: 750, loss is 5.1906880760192875 and perplexity is 179.59208343363522
At time: 801.4942226409912 and batch: 800, loss is 5.201279764175415 and perplexity is 181.5043761031522
At time: 803.0456418991089 and batch: 850, loss is 5.226977338790894 and perplexity is 186.22904461414672
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.1115922927856445 and perplexity of 165.93436071610944
Finished 28 epochs...
Completing Train Step...
At time: 807.1853017807007 and batch: 50, loss is 5.222127389907837 and perplexity is 185.32802997084755
At time: 808.7247796058655 and batch: 100, loss is 5.16460560798645 and perplexity is 174.96843880738146
At time: 810.3096182346344 and batch: 150, loss is 5.157736759185791 and perplexity is 173.7707252222028
At time: 811.8475940227509 and batch: 200, loss is 5.204353151321411 and perplexity is 182.06306741719166
At time: 813.3861644268036 and batch: 250, loss is 5.218131828308105 and perplexity is 184.58901777847473
At time: 814.925733089447 and batch: 300, loss is 5.184941129684448 and perplexity is 178.5629374239236
At time: 816.4652359485626 and batch: 350, loss is 5.161191816329956 and perplexity is 174.37215139042846
At time: 818.0032069683075 and batch: 400, loss is 5.190254421234131 and perplexity is 179.51421935156407
At time: 819.547610282898 and batch: 450, loss is 5.21599063873291 and perplexity is 184.19420053803452
At time: 821.092050075531 and batch: 500, loss is 5.221693716049194 and perplexity is 185.24767547405713
At time: 822.6318471431732 and batch: 550, loss is 5.197590436935425 and perplexity is 180.8359807868759
At time: 824.1708414554596 and batch: 600, loss is 5.213090677261352 and perplexity is 183.66081822107637
At time: 825.7080645561218 and batch: 650, loss is 5.21528468132019 and perplexity is 184.06421316495806
At time: 827.2453365325928 and batch: 700, loss is 5.181014451980591 and perplexity is 177.86315313164727
At time: 828.7838506698608 and batch: 750, loss is 5.188478679656982 and perplexity is 179.19573134845618
At time: 830.3224031925201 and batch: 800, loss is 5.198335838317871 and perplexity is 180.97082642777048
At time: 831.8755598068237 and batch: 850, loss is 5.223386869430542 and perplexity is 185.56159388318656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.109679222106934 and perplexity of 165.61722000905033
Finished 29 epochs...
Completing Train Step...
At time: 836.0493407249451 and batch: 50, loss is 5.217402429580688 and perplexity is 184.4544278746313
At time: 837.6019852161407 and batch: 100, loss is 5.160668725967407 and perplexity is 174.2809628505417
At time: 839.139059305191 and batch: 150, loss is 5.154369583129883 and perplexity is 173.18659258762884
At time: 840.6762266159058 and batch: 200, loss is 5.2005593204498295 and perplexity is 181.37365950685214
At time: 842.2152264118195 and batch: 250, loss is 5.214436149597168 and perplexity is 183.90809508595285
At time: 843.7540943622589 and batch: 300, loss is 5.181692094802856 and perplexity is 177.98372166730005
At time: 845.2933158874512 and batch: 350, loss is 5.157024726867676 and perplexity is 173.64703888946102
At time: 846.8299643993378 and batch: 400, loss is 5.185902194976807 and perplexity is 178.7346305565233
At time: 848.366779088974 and batch: 450, loss is 5.211707696914673 and perplexity is 183.40699447610305
At time: 849.9382750988007 and batch: 500, loss is 5.217869729995727 and perplexity is 184.54064364809736
At time: 851.4831612110138 and batch: 550, loss is 5.193972873687744 and perplexity is 180.18297704223858
At time: 853.0222265720367 and batch: 600, loss is 5.21037091255188 and perplexity is 183.1619826743193
At time: 854.5602171421051 and batch: 650, loss is 5.211912174224853 and perplexity is 183.44450087947527
At time: 856.0996823310852 and batch: 700, loss is 5.1778202247619625 and perplexity is 177.29592421803105
At time: 857.6392982006073 and batch: 750, loss is 5.1858297634124755 and perplexity is 178.7216849964712
At time: 859.1795358657837 and batch: 800, loss is 5.195564041137695 and perplexity is 180.4699065462494
At time: 860.7389011383057 and batch: 850, loss is 5.219646110534668 and perplexity is 184.86874939016593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.109286626180013 and perplexity of 165.55221212479708
Finished 30 epochs...
Completing Train Step...
At time: 864.8160297870636 and batch: 50, loss is 5.212866344451904 and perplexity is 183.61962169477837
At time: 866.4088268280029 and batch: 100, loss is 5.15697078704834 and perplexity is 173.63767265216384
At time: 867.9537818431854 and batch: 150, loss is 5.15077974319458 and perplexity is 172.56599503071283
At time: 869.4903740882874 and batch: 200, loss is 5.196979665756226 and perplexity is 180.72556510443988
At time: 871.033198595047 and batch: 250, loss is 5.211987867355346 and perplexity is 183.4583868935499
At time: 872.582412481308 and batch: 300, loss is 5.1791652584075925 and perplexity is 177.53455364766754
At time: 874.1337122917175 and batch: 350, loss is 5.154889879226684 and perplexity is 173.27672434133703
At time: 875.6689932346344 and batch: 400, loss is 5.1835924243927005 and perplexity is 178.3222709758471
At time: 877.2151265144348 and batch: 450, loss is 5.209408121109009 and perplexity is 182.98572075009315
At time: 878.7527785301208 and batch: 500, loss is 5.215233297348022 and perplexity is 184.05475545754106
At time: 880.2907974720001 and batch: 550, loss is 5.191282625198364 and perplexity is 179.69889150758945
At time: 881.8289246559143 and batch: 600, loss is 5.207382640838623 and perplexity is 182.61546188554843
At time: 883.3661031723022 and batch: 650, loss is 5.20854416847229 and perplexity is 182.82769802607297
At time: 884.9031133651733 and batch: 700, loss is 5.173747949600219 and perplexity is 176.57539452288734
At time: 886.4468476772308 and batch: 750, loss is 5.182308263778687 and perplexity is 178.09342350875912
At time: 888.0638499259949 and batch: 800, loss is 5.1925438785552975 and perplexity is 179.92568032675644
At time: 889.6205163002014 and batch: 850, loss is 5.216213445663453 and perplexity is 184.2352448547895
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.106452624003093 and perplexity of 165.08370098960324
Finished 31 epochs...
Completing Train Step...
At time: 893.8519396781921 and batch: 50, loss is 5.209025192260742 and perplexity is 182.91566365309484
At time: 895.4512476921082 and batch: 100, loss is 5.1532979011535645 and perplexity is 173.00109105489133
At time: 897.0073227882385 and batch: 150, loss is 5.147385368347168 and perplexity is 171.98123436756222
At time: 898.5628716945648 and batch: 200, loss is 5.193732070922851 and perplexity is 180.13959370680342
At time: 900.1087901592255 and batch: 250, loss is 5.20806715965271 and perplexity is 182.740508398415
At time: 901.6482951641083 and batch: 300, loss is 5.176812171936035 and perplexity is 177.1172906117803
At time: 903.1859509944916 and batch: 350, loss is 5.152031307220459 and perplexity is 172.7821076333527
At time: 904.7242665290833 and batch: 400, loss is 5.181370944976806 and perplexity is 177.92657140343672
At time: 906.2737500667572 and batch: 450, loss is 5.206245470046997 and perplexity is 182.40791494671714
At time: 907.8320732116699 and batch: 500, loss is 5.212900838851929 and perplexity is 183.62595565270382
At time: 909.3839018344879 and batch: 550, loss is 5.188148860931396 and perplexity is 179.1366389861319
At time: 910.9248034954071 and batch: 600, loss is 5.204511976242065 and perplexity is 182.09198586585265
At time: 912.4658718109131 and batch: 650, loss is 5.20640775680542 and perplexity is 182.43751973811234
At time: 914.0047280788422 and batch: 700, loss is 5.171682939529419 and perplexity is 176.21114077820718
At time: 915.5448541641235 and batch: 750, loss is 5.179391632080078 and perplexity is 177.5747473457957
At time: 917.082373380661 and batch: 800, loss is 5.189609651565552 and perplexity is 179.39851133436608
At time: 918.6344156265259 and batch: 850, loss is 5.212526874542236 and perplexity is 183.5572989373373
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.105382919311523 and perplexity of 164.90720459649285
Finished 32 epochs...
Completing Train Step...
At time: 922.7516701221466 and batch: 50, loss is 5.204600372314453 and perplexity is 182.10808279365864
At time: 924.3301546573639 and batch: 100, loss is 5.150111684799194 and perplexity is 172.45074936867672
At time: 925.8934869766235 and batch: 150, loss is 5.143524942398071 and perplexity is 171.31859340876662
At time: 927.4975395202637 and batch: 200, loss is 5.1905218029022215 and perplexity is 179.56222458055342
At time: 929.0479679107666 and batch: 250, loss is 5.205438966751099 and perplexity is 182.26086166956193
At time: 930.5974252223969 and batch: 300, loss is 5.173783025741577 and perplexity is 176.5815882150107
At time: 932.1548023223877 and batch: 350, loss is 5.148021516799926 and perplexity is 172.09067477018934
At time: 933.7162716388702 and batch: 400, loss is 5.178347826004028 and perplexity is 177.38949044852097
At time: 935.2746918201447 and batch: 450, loss is 5.2032128524780275 and perplexity is 181.8555794336296
At time: 936.8428952693939 and batch: 500, loss is 5.209964599609375 and perplexity is 183.08757670726123
At time: 938.3958101272583 and batch: 550, loss is 5.184704132080078 and perplexity is 178.52062344987817
At time: 939.9523544311523 and batch: 600, loss is 5.201618642807007 and perplexity is 181.56589448079703
At time: 941.5139038562775 and batch: 650, loss is 5.202731704711914 and perplexity is 181.76810107452593
At time: 943.0626120567322 and batch: 700, loss is 5.168546619415284 and perplexity is 175.65935197851132
At time: 944.6228582859039 and batch: 750, loss is 5.176397333145141 and perplexity is 177.0438307271549
At time: 946.1753733158112 and batch: 800, loss is 5.186467151641846 and perplexity is 178.83563640660714
At time: 947.724360704422 and batch: 850, loss is 5.209541063308716 and perplexity is 183.01004889140717
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.104587554931641 and perplexity of 164.7760954266642
Finished 33 epochs...
Completing Train Step...
At time: 951.8538227081299 and batch: 50, loss is 5.200387954711914 and perplexity is 181.34258093882923
At time: 953.4014918804169 and batch: 100, loss is 5.146551103591919 and perplexity is 171.83781631779547
At time: 954.9525270462036 and batch: 150, loss is 5.140840454101562 and perplexity is 170.8593073995642
At time: 956.5069282054901 and batch: 200, loss is 5.187289762496948 and perplexity is 178.98280906700913
At time: 958.0605185031891 and batch: 250, loss is 5.20233736038208 and perplexity is 181.6964359858119
At time: 959.6079678535461 and batch: 300, loss is 5.1707501792907715 and perplexity is 176.0468546641713
At time: 961.1546173095703 and batch: 350, loss is 5.14491307258606 and perplexity is 171.5565710538043
At time: 962.7068660259247 and batch: 400, loss is 5.175264129638672 and perplexity is 176.84331766988325
At time: 964.2612187862396 and batch: 450, loss is 5.200468978881836 and perplexity is 181.35727466618678
At time: 965.8090453147888 and batch: 500, loss is 5.206752767562866 and perplexity is 182.5004735042227
At time: 967.409786939621 and batch: 550, loss is 5.182160472869873 and perplexity is 178.06710486472102
At time: 968.962488412857 and batch: 600, loss is 5.199007749557495 and perplexity is 181.09246362021344
At time: 970.5158224105835 and batch: 650, loss is 5.199239444732666 and perplexity is 181.13442673143157
At time: 972.0661494731903 and batch: 700, loss is 5.166047124862671 and perplexity is 175.22084064177326
At time: 973.6087074279785 and batch: 750, loss is 5.173073711395264 and perplexity is 176.4563807721624
At time: 975.1455914974213 and batch: 800, loss is 5.18379430770874 and perplexity is 178.35827490140906
At time: 976.6829886436462 and batch: 850, loss is 5.206734094619751 and perplexity is 182.49706571507926
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.102901776631673 and perplexity of 164.49855346347763
Finished 34 epochs...
Completing Train Step...
At time: 980.8225078582764 and batch: 50, loss is 5.197142210006714 and perplexity is 180.75494339353506
At time: 982.373791217804 and batch: 100, loss is 5.14321722984314 and perplexity is 171.265884636673
At time: 983.9258124828339 and batch: 150, loss is 5.138301782608032 and perplexity is 170.4261018624007
At time: 985.4769790172577 and batch: 200, loss is 5.185082283020019 and perplexity is 178.58814395710164
At time: 987.0296573638916 and batch: 250, loss is 5.199001140594483 and perplexity is 181.09126679077445
At time: 988.5775628089905 and batch: 300, loss is 5.167782382965088 and perplexity is 175.52515798341423
At time: 990.1264209747314 and batch: 350, loss is 5.141929302215576 and perplexity is 171.04544855552064
At time: 991.6768944263458 and batch: 400, loss is 5.172739391326904 and perplexity is 176.39739772303798
At time: 993.2257220745087 and batch: 450, loss is 5.1986414813995365 and perplexity is 181.0261473626512
At time: 994.7796988487244 and batch: 500, loss is 5.203859386444091 and perplexity is 181.9731932592213
At time: 996.332622051239 and batch: 550, loss is 5.179822540283203 and perplexity is 177.65128224966952
At time: 997.8816208839417 and batch: 600, loss is 5.1967790412902835 and perplexity is 180.68931077133337
At time: 999.4283490180969 and batch: 650, loss is 5.195952463150024 and perplexity is 180.54001864617493
At time: 1000.9768218994141 and batch: 700, loss is 5.162704839706421 and perplexity is 174.63618022221507
At time: 1002.5215246677399 and batch: 750, loss is 5.169798707962036 and perplexity is 175.8794307915822
At time: 1004.0686302185059 and batch: 800, loss is 5.18036072731018 and perplexity is 177.74691759764568
At time: 1005.6065797805786 and batch: 850, loss is 5.202860937118531 and perplexity is 181.79159292159525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.1024274826049805 and perplexity of 164.42055128161707
Finished 35 epochs...
Completing Train Step...
At time: 1009.835364818573 and batch: 50, loss is 5.194119291305542 and perplexity is 180.20936093599119
At time: 1011.3753972053528 and batch: 100, loss is 5.139837799072265 and perplexity is 170.68808031089338
At time: 1012.9216058254242 and batch: 150, loss is 5.134965982437134 and perplexity is 169.85854160278487
At time: 1014.4692900180817 and batch: 200, loss is 5.182252054214477 and perplexity is 178.0834132363743
At time: 1016.0175204277039 and batch: 250, loss is 5.1956328105926515 and perplexity is 180.48231779011098
At time: 1017.5650851726532 and batch: 300, loss is 5.165086364746093 and perplexity is 175.05257629027358
At time: 1019.1099240779877 and batch: 350, loss is 5.138626937866211 and perplexity is 170.48152581575883
At time: 1020.6603388786316 and batch: 400, loss is 5.170186471939087 and perplexity is 175.94764372355164
At time: 1022.2107405662537 and batch: 450, loss is 5.195965137481689 and perplexity is 180.54230688475099
At time: 1023.7685401439667 and batch: 500, loss is 5.20065390586853 and perplexity is 181.39081562172427
At time: 1025.325846672058 and batch: 550, loss is 5.176984395980835 and perplexity is 177.14779709487297
At time: 1026.8809876441956 and batch: 600, loss is 5.19363730430603 and perplexity is 180.1225232958176
At time: 1028.4331917762756 and batch: 650, loss is 5.193206558227539 and perplexity is 180.04495293302415
At time: 1029.9826135635376 and batch: 700, loss is 5.160279350280762 and perplexity is 174.2131152909148
At time: 1031.5360498428345 and batch: 750, loss is 5.167015953063965 and perplexity is 175.3906817938055
At time: 1033.0890986919403 and batch: 800, loss is 5.177338037490845 and perplexity is 177.2104549878955
At time: 1034.6472737789154 and batch: 850, loss is 5.199127063751221 and perplexity is 181.1140718105564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.100520769755046 and perplexity of 164.1073471937138
Finished 36 epochs...
Completing Train Step...
At time: 1038.7304329872131 and batch: 50, loss is 5.190394639968872 and perplexity is 179.539392373093
At time: 1040.3140478134155 and batch: 100, loss is 5.136346797943116 and perplexity is 170.0932469159296
At time: 1041.8532464504242 and batch: 150, loss is 5.131757268905639 and perplexity is 169.31438768579932
At time: 1043.3902988433838 and batch: 200, loss is 5.179094886779785 and perplexity is 177.5220606917152
At time: 1044.9275832176208 and batch: 250, loss is 5.191886234283447 and perplexity is 179.80739213377026
At time: 1046.5207471847534 and batch: 300, loss is 5.16164384841919 and perplexity is 174.4509910160035
At time: 1048.060425043106 and batch: 350, loss is 5.13515347480774 and perplexity is 169.89039176915597
At time: 1049.5988550186157 and batch: 400, loss is 5.166506175994873 and perplexity is 175.3012944318441
At time: 1051.1433672904968 and batch: 450, loss is 5.192698812484741 and perplexity is 179.95355907904377
At time: 1052.6984641551971 and batch: 500, loss is 5.196855754852295 and perplexity is 180.70317262366922
At time: 1054.2558493614197 and batch: 550, loss is 5.173336658477783 and perplexity is 176.5027855634132
At time: 1055.8030602931976 and batch: 600, loss is 5.190724172592163 and perplexity is 179.59856620936506
At time: 1057.3396835327148 and batch: 650, loss is 5.18985050201416 and perplexity is 179.441724750077
At time: 1058.8836288452148 and batch: 700, loss is 5.156911764144898 and perplexity is 173.6274243550221
At time: 1060.4240155220032 and batch: 750, loss is 5.163473224639892 and perplexity is 174.77041959908945
At time: 1061.9747354984283 and batch: 800, loss is 5.173044872283936 and perplexity is 176.45129200033088
At time: 1063.514086008072 and batch: 850, loss is 5.195317668914795 and perplexity is 180.42544925095322
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.099570910135905 and perplexity of 163.9515422595323
Finished 37 epochs...
Completing Train Step...
At time: 1067.6157205104828 and batch: 50, loss is 5.186390590667725 and perplexity is 178.8219451001928
At time: 1069.205593585968 and batch: 100, loss is 5.13333987236023 and perplexity is 169.58255736782306
At time: 1070.7548484802246 and batch: 150, loss is 5.128646402359009 and perplexity is 168.78849164160258
At time: 1072.301679611206 and batch: 200, loss is 5.173741865158081 and perplexity is 176.57432016338473
At time: 1073.8490846157074 and batch: 250, loss is 5.188535861968994 and perplexity is 179.2059784676515
At time: 1075.4023370742798 and batch: 300, loss is 5.157547264099121 and perplexity is 173.73779964328233
At time: 1076.9580011367798 and batch: 350, loss is 5.1304749584198 and perplexity is 169.09741321511706
At time: 1078.5069253444672 and batch: 400, loss is 5.162887554168702 and perplexity is 174.66809169323406
At time: 1080.0547976493835 and batch: 450, loss is 5.18832706451416 and perplexity is 179.16856462155036
At time: 1081.6062178611755 and batch: 500, loss is 5.192928504943848 and perplexity is 179.9948978019684
At time: 1083.1614971160889 and batch: 550, loss is 5.168428192138672 and perplexity is 175.6385503516094
At time: 1084.7498152256012 and batch: 600, loss is 5.186290807723999 and perplexity is 178.80410261031088
At time: 1086.2974772453308 and batch: 650, loss is 5.1858243656158445 and perplexity is 178.72072029576566
At time: 1087.8511605262756 and batch: 700, loss is 5.15247875213623 and perplexity is 172.85943540761906
At time: 1089.4061481952667 and batch: 750, loss is 5.159458980560303 and perplexity is 174.07025473347332
At time: 1090.9572937488556 and batch: 800, loss is 5.167681198120118 and perplexity is 175.507398396031
At time: 1092.4985692501068 and batch: 850, loss is 5.190728044509887 and perplexity is 179.59926160158307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.096302032470703 and perplexity of 163.41647972829713
Finished 38 epochs...
Completing Train Step...
At time: 1096.7356848716736 and batch: 50, loss is 5.18163104057312 and perplexity is 177.9728553399891
At time: 1098.321598291397 and batch: 100, loss is 5.1282069206237795 and perplexity is 168.71432848028024
At time: 1099.8746654987335 and batch: 150, loss is 5.12343734741211 and perplexity is 167.91154911806322
At time: 1101.4291152954102 and batch: 200, loss is 5.168291959762573 and perplexity is 175.61462432434732
At time: 1102.9816143512726 and batch: 250, loss is 5.182520923614502 and perplexity is 178.13130085431447
At time: 1104.5323901176453 and batch: 300, loss is 5.152380418777466 and perplexity is 172.84243839444176
At time: 1106.0868966579437 and batch: 350, loss is 5.125447101593018 and perplexity is 168.24934938997933
At time: 1107.6420469284058 and batch: 400, loss is 5.156584768295288 and perplexity is 173.57065818953575
At time: 1109.1981434822083 and batch: 450, loss is 5.183202476501465 and perplexity is 178.25274813834258
At time: 1110.7503657341003 and batch: 500, loss is 5.1873246192932125 and perplexity is 178.98904794305258
At time: 1112.3032929897308 and batch: 550, loss is 5.164417657852173 and perplexity is 174.93555655602188
At time: 1113.8594086170197 and batch: 600, loss is 5.181605052947998 and perplexity is 177.96823030823967
At time: 1115.4102036952972 and batch: 650, loss is 5.180477886199951 and perplexity is 177.76774344911465
At time: 1116.9631972312927 and batch: 700, loss is 5.146617631912232 and perplexity is 171.8492487793684
At time: 1118.520204782486 and batch: 750, loss is 5.152932233810425 and perplexity is 172.93784177036878
At time: 1120.0694797039032 and batch: 800, loss is 5.163155117034912 and perplexity is 174.7148326412752
At time: 1121.622222661972 and batch: 850, loss is 5.184247512817382 and perplexity is 178.4391261024678
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.093045234680176 and perplexity of 162.8851310154952
Finished 39 epochs...
Completing Train Step...
At time: 1125.7839641571045 and batch: 50, loss is 5.174118824005127 and perplexity is 176.64089396253445
At time: 1127.3326938152313 and batch: 100, loss is 5.120802507400513 and perplexity is 167.4697113915152
At time: 1128.883537054062 and batch: 150, loss is 5.1161379146575925 and perplexity is 166.69035250014844
At time: 1130.4340677261353 and batch: 200, loss is 5.160397510528565 and perplexity is 174.2337015720042
At time: 1131.9887483119965 and batch: 250, loss is 5.175648231506347 and perplexity is 176.91125656537923
At time: 1133.5428586006165 and batch: 300, loss is 5.145996885299683 and perplexity is 171.74260704249227
At time: 1135.0934872627258 and batch: 350, loss is 5.118262519836426 and perplexity is 167.04488016863215
At time: 1136.6442139148712 and batch: 400, loss is 5.150769023895264 and perplexity is 172.56414525407453
At time: 1138.1969578266144 and batch: 450, loss is 5.176682634353638 and perplexity is 177.09434875210226
At time: 1139.7522790431976 and batch: 500, loss is 5.182454833984375 and perplexity is 178.11952861154288
At time: 1141.3042883872986 and batch: 550, loss is 5.157053861618042 and perplexity is 173.6520981262903
At time: 1142.850853919983 and batch: 600, loss is 5.1761485195159915 and perplexity is 176.99978528889298
At time: 1144.4013299942017 and batch: 650, loss is 5.173510980606079 and perplexity is 176.53355658659999
At time: 1145.9565796852112 and batch: 700, loss is 5.1416291236877445 and perplexity is 170.99411209001872
At time: 1147.5123317241669 and batch: 750, loss is 5.1479874610900875 and perplexity is 172.0848141998969
At time: 1149.0630149841309 and batch: 800, loss is 5.158037195205688 and perplexity is 173.822940050478
At time: 1150.6174499988556 and batch: 850, loss is 5.180330972671509 and perplexity is 177.74162888101995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.088983535766602 and perplexity of 162.2248824289938
Finished 40 epochs...
Completing Train Step...
At time: 1154.8660261631012 and batch: 50, loss is 5.168909358978271 and perplexity is 175.7230821331081
At time: 1156.4148139953613 and batch: 100, loss is 5.115982980728149 and perplexity is 166.6645285093931
At time: 1157.9643456935883 and batch: 150, loss is 5.1115077781677245 and perplexity is 165.9203374296084
At time: 1159.5112245082855 and batch: 200, loss is 5.155579090118408 and perplexity is 173.39618971072326
At time: 1161.0634505748749 and batch: 250, loss is 5.170589256286621 and perplexity is 176.01852695479866
At time: 1162.6153717041016 and batch: 300, loss is 5.14123501777649 and perplexity is 170.92673557726732
At time: 1164.190173625946 and batch: 350, loss is 5.113526048660279 and perplexity is 166.25554770947798
At time: 1165.7375001907349 and batch: 400, loss is 5.1463703918457036 and perplexity is 171.8067660116049
At time: 1167.283890247345 and batch: 450, loss is 5.1719598388671875 and perplexity is 176.25994028235945
At time: 1168.8315193653107 and batch: 500, loss is 5.177020435333252 and perplexity is 177.15418150180665
At time: 1170.375016450882 and batch: 550, loss is 5.152994127273559 and perplexity is 172.94854582355495
At time: 1171.9161596298218 and batch: 600, loss is 5.1725016212463375 and perplexity is 176.3554606854536
At time: 1173.464451789856 and batch: 650, loss is 5.16876669883728 and perplexity is 175.69801524150128
At time: 1175.017331123352 and batch: 700, loss is 5.1369054412841795 and perplexity is 170.1882949221751
At time: 1176.5675446987152 and batch: 750, loss is 5.143108530044556 and perplexity is 171.2472690812805
At time: 1178.1188576221466 and batch: 800, loss is 5.15410454750061 and perplexity is 173.140698052193
At time: 1179.6684799194336 and batch: 850, loss is 5.175352773666382 and perplexity is 176.85899446865216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.086047172546387 and perplexity of 161.74922993683276
Finished 41 epochs...
Completing Train Step...
At time: 1183.774789571762 and batch: 50, loss is 5.16514702796936 and perplexity is 175.0631958658981
At time: 1185.3215839862823 and batch: 100, loss is 5.1114962291717525 and perplexity is 165.9184212273649
At time: 1186.8643836975098 and batch: 150, loss is 5.106906366348267 and perplexity is 165.15862345170166
At time: 1188.4082653522491 and batch: 200, loss is 5.152230234146118 and perplexity is 172.8164820657198
At time: 1189.9481134414673 and batch: 250, loss is 5.166390285491944 and perplexity is 175.2809798538243
At time: 1191.4987165927887 and batch: 300, loss is 5.136624526977539 and perplexity is 170.1404933097015
At time: 1193.0480015277863 and batch: 350, loss is 5.109279794692993 and perplexity is 165.55108116087197
At time: 1194.5958893299103 and batch: 400, loss is 5.14221978187561 and perplexity is 171.09514099623965
At time: 1196.1428158283234 and batch: 450, loss is 5.167856149673462 and perplexity is 175.53810637412965
At time: 1197.6816580295563 and batch: 500, loss is 5.172734251022339 and perplexity is 176.39649098901964
At time: 1199.2181355953217 and batch: 550, loss is 5.148849906921387 and perplexity is 172.23329204838097
At time: 1200.7557320594788 and batch: 600, loss is 5.168309354782105 and perplexity is 175.61767917073692
At time: 1202.292670249939 and batch: 650, loss is 5.164280881881714 and perplexity is 174.91163121174955
At time: 1203.8724122047424 and batch: 700, loss is 5.1324098110198975 and perplexity is 169.42490851013855
At time: 1205.4097278118134 and batch: 750, loss is 5.138631858825684 and perplexity is 170.48236475050243
At time: 1206.9478449821472 and batch: 800, loss is 5.149547109603882 and perplexity is 172.35341543191868
At time: 1208.4869496822357 and batch: 850, loss is 5.1705144500732425 and perplexity is 176.0053601677976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.083487828572591 and perplexity of 161.33578731654623
Finished 42 epochs...
Completing Train Step...
At time: 1212.6951656341553 and batch: 50, loss is 5.160265159606934 and perplexity is 174.21064310696016
At time: 1214.2700054645538 and batch: 100, loss is 5.107293605804443 and perplexity is 165.22259177195937
At time: 1215.8220474720001 and batch: 150, loss is 5.102589120864868 and perplexity is 164.44713008143367
At time: 1217.37521982193 and batch: 200, loss is 5.147770490646362 and perplexity is 172.0474809316559
At time: 1218.9246172904968 and batch: 250, loss is 5.161901502609253 and perplexity is 174.49594483582084
At time: 1220.473577260971 and batch: 300, loss is 5.132369298934936 and perplexity is 169.41804489288108
At time: 1222.0249392986298 and batch: 350, loss is 5.1052352237701415 and perplexity is 164.88285033618428
At time: 1223.575672864914 and batch: 400, loss is 5.136959266662598 and perplexity is 170.19745561808838
At time: 1225.1280636787415 and batch: 450, loss is 5.162753305435181 and perplexity is 174.64464429706453
At time: 1226.6655535697937 and batch: 500, loss is 5.168181085586548 and perplexity is 175.59515427696007
At time: 1228.2036192417145 and batch: 550, loss is 5.143623161315918 and perplexity is 171.3354209619968
At time: 1229.742667913437 and batch: 600, loss is 5.163676538467407 and perplexity is 174.80595645447917
At time: 1231.281804561615 and batch: 650, loss is 5.1594413375854495 and perplexity is 174.06718364343797
At time: 1232.822450876236 and batch: 700, loss is 5.128226900100708 and perplexity is 168.71769933798754
At time: 1234.3626713752747 and batch: 750, loss is 5.133070335388184 and perplexity is 169.53685475834652
At time: 1235.9021430015564 and batch: 800, loss is 5.144124698638916 and perplexity is 171.42137362279286
At time: 1237.447508573532 and batch: 850, loss is 5.164130067825317 and perplexity is 174.8852540682075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.075846354166667 and perplexity of 160.10764242205693
Finished 43 epochs...
Completing Train Step...
At time: 1241.4925169944763 and batch: 50, loss is 5.154107723236084 and perplexity is 173.14124790212284
At time: 1243.0787239074707 and batch: 100, loss is 5.1000065517425535 and perplexity is 164.02298193273185
At time: 1244.6288902759552 and batch: 150, loss is 5.096575832366943 and perplexity is 163.46122926942093
At time: 1246.1817908287048 and batch: 200, loss is 5.140448875427246 and perplexity is 170.7924156360269
At time: 1247.7294301986694 and batch: 250, loss is 5.1551353931427 and perplexity is 173.31927141121966
At time: 1249.273981809616 and batch: 300, loss is 5.124743404388428 and perplexity is 168.13099444104694
At time: 1250.8218665122986 and batch: 350, loss is 5.097452144622803 and perplexity is 163.60453512915763
At time: 1252.371077299118 and batch: 400, loss is 5.128108549118042 and perplexity is 168.69773261404166
At time: 1253.9239914417267 and batch: 450, loss is 5.15456826210022 and perplexity is 173.22100453987275
At time: 1255.471764087677 and batch: 500, loss is 5.159975776672363 and perplexity is 174.16023681353673
At time: 1257.019020318985 and batch: 550, loss is 5.135285091400147 and perplexity is 169.91275363516732
At time: 1258.5684015750885 and batch: 600, loss is 5.157230634689331 and perplexity is 173.68279785437446
At time: 1260.1142513751984 and batch: 650, loss is 5.152005529403686 and perplexity is 172.7776537452465
At time: 1261.6529927253723 and batch: 700, loss is 5.120111436843872 and perplexity is 167.35401798560093
At time: 1263.1905825138092 and batch: 750, loss is 5.126435718536377 and perplexity is 168.4157657949801
At time: 1264.7277886867523 and batch: 800, loss is 5.137063789367676 and perplexity is 170.2152460462826
At time: 1266.2668669223785 and batch: 850, loss is 5.1578364753723145 and perplexity is 173.78805384020882
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.072142283121745 and perplexity of 159.5156893346428
Finished 44 epochs...
Completing Train Step...
At time: 1270.4415152072906 and batch: 50, loss is 5.1486976623535154 and perplexity is 172.20707246120546
At time: 1271.9972882270813 and batch: 100, loss is 5.093524303436279 and perplexity is 162.9631828872123
At time: 1273.5465834140778 and batch: 150, loss is 5.0915074634552 and perplexity is 162.6348434398812
At time: 1275.1015202999115 and batch: 200, loss is 5.1364585971832275 and perplexity is 170.11226427472627
At time: 1276.6538965702057 and batch: 250, loss is 5.150850667953491 and perplexity is 172.57823466634812
At time: 1278.2069854736328 and batch: 300, loss is 5.119515571594238 and perplexity is 167.25432724597016
At time: 1279.7553997039795 and batch: 350, loss is 5.09107084274292 and perplexity is 162.5638491986028
At time: 1281.3058931827545 and batch: 400, loss is 5.121994705200195 and perplexity is 167.66948747559107
At time: 1282.8955988883972 and batch: 450, loss is 5.148491640090942 and perplexity is 172.17159762493435
At time: 1284.4442529678345 and batch: 500, loss is 5.154861726760864 and perplexity is 173.27184624294318
At time: 1285.9903438091278 and batch: 550, loss is 5.130246648788452 and perplexity is 169.05881105383432
At time: 1287.5363266468048 and batch: 600, loss is 5.151427879333496 and perplexity is 172.67787754207308
At time: 1289.0870623588562 and batch: 650, loss is 5.147757081985474 and perplexity is 172.04517402079372
At time: 1290.6366646289825 and batch: 700, loss is 5.116499786376953 and perplexity is 166.75068394007104
At time: 1292.1846482753754 and batch: 750, loss is 5.122633743286133 and perplexity is 167.77666890679848
At time: 1293.734519958496 and batch: 800, loss is 5.132907266616821 and perplexity is 169.5092108457311
At time: 1295.2863981723785 and batch: 850, loss is 5.15261869430542 and perplexity is 172.8836274246771
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.0690053304036455 and perplexity of 159.01608019422954
Finished 45 epochs...
Completing Train Step...
At time: 1299.442841053009 and batch: 50, loss is 5.143562316894531 and perplexity is 171.3249964745845
At time: 1300.9977054595947 and batch: 100, loss is 5.087511901855469 and perplexity is 161.98632237037205
At time: 1302.5521609783173 and batch: 150, loss is 5.086852645874023 and perplexity is 161.87956711182488
At time: 1304.100023508072 and batch: 200, loss is 5.1307087230682376 and perplexity is 169.13694683307014
At time: 1305.6415922641754 and batch: 250, loss is 5.147214679718018 and perplexity is 171.95188163158892
At time: 1307.180772304535 and batch: 300, loss is 5.115248937606811 and perplexity is 166.54223444871795
At time: 1308.7254922389984 and batch: 350, loss is 5.087196464538574 and perplexity is 161.935233897499
At time: 1310.270634651184 and batch: 400, loss is 5.118423852920532 and perplexity is 167.07183220840332
At time: 1311.8128879070282 and batch: 450, loss is 5.144414472579956 and perplexity is 171.47105426753643
At time: 1313.360608100891 and batch: 500, loss is 5.149913806915283 and perplexity is 172.41662855530123
At time: 1314.9148650169373 and batch: 550, loss is 5.124903926849365 and perplexity is 168.15798540830465
At time: 1316.4728722572327 and batch: 600, loss is 5.148743324279785 and perplexity is 172.21493594738087
At time: 1318.0233464241028 and batch: 650, loss is 5.144077234268188 and perplexity is 171.4132374082562
At time: 1319.5746250152588 and batch: 700, loss is 5.112035074234009 and perplexity is 166.00784964124642
At time: 1321.1292743682861 and batch: 750, loss is 5.1206619167327885 and perplexity is 167.4461683679705
At time: 1322.7444696426392 and batch: 800, loss is 5.129089183807373 and perplexity is 168.86324460278976
At time: 1324.2987141609192 and batch: 850, loss is 5.148997869491577 and perplexity is 172.25877801438276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.067034721374512 and perplexity of 158.70303022167815
Finished 46 epochs...
Completing Train Step...
At time: 1328.3977468013763 and batch: 50, loss is 5.139272928237915 and perplexity is 170.5916908189239
At time: 1329.938717842102 and batch: 100, loss is 5.083617811203003 and perplexity is 161.3567595295434
At time: 1331.4853026866913 and batch: 150, loss is 5.082917280197144 and perplexity is 161.24376369965023
At time: 1333.0284028053284 and batch: 200, loss is 5.126980085372924 and perplexity is 168.5074707109235
At time: 1334.5734341144562 and batch: 250, loss is 5.142286367416382 and perplexity is 171.10653383802136
At time: 1336.11771941185 and batch: 300, loss is 5.110326499938965 and perplexity is 165.7244550658583
At time: 1337.663456439972 and batch: 350, loss is 5.082558603286743 and perplexity is 161.1859396553597
At time: 1339.2091665267944 and batch: 400, loss is 5.112355661392212 and perplexity is 166.06107815773586
At time: 1340.7546336650848 and batch: 450, loss is 5.139813251495362 and perplexity is 170.68389038354192
At time: 1342.3036921024323 and batch: 500, loss is 5.1451190567016605 and perplexity is 171.5919126221442
At time: 1343.8479299545288 and batch: 550, loss is 5.119926815032959 and perplexity is 167.32312363571003
At time: 1345.3940358161926 and batch: 600, loss is 5.144151735305786 and perplexity is 171.42600834801934
At time: 1346.9454016685486 and batch: 650, loss is 5.139111032485962 and perplexity is 170.56407498436533
At time: 1348.4880940914154 and batch: 700, loss is 5.1072672080993655 and perplexity is 165.21823033227585
At time: 1350.0331254005432 and batch: 750, loss is 5.115674562454224 and perplexity is 166.61313404908844
At time: 1351.5823197364807 and batch: 800, loss is 5.124039735794067 and perplexity is 168.01272755574655
At time: 1353.1285316944122 and batch: 850, loss is 5.1442756080627445 and perplexity is 171.44724467556188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.0648454030354815 and perplexity of 158.3559588306642
Finished 47 epochs...
Completing Train Step...
At time: 1357.190375328064 and batch: 50, loss is 5.1346228885650635 and perplexity is 169.8002741742126
At time: 1358.7767972946167 and batch: 100, loss is 5.079381008148193 and perplexity is 160.67456889201455
At time: 1360.3216876983643 and batch: 150, loss is 5.078330936431885 and perplexity is 160.5059376246484
At time: 1361.9084153175354 and batch: 200, loss is 5.123317308425904 and perplexity is 167.89139439563522
At time: 1363.4537224769592 and batch: 250, loss is 5.137123184204102 and perplexity is 170.2253562532228
At time: 1364.9980278015137 and batch: 300, loss is 5.106300506591797 and perplexity is 165.05859079425895
At time: 1366.5424189567566 and batch: 350, loss is 5.0784828662872314 and perplexity is 160.53032512108143
At time: 1368.0870270729065 and batch: 400, loss is 5.108161973953247 and perplexity is 165.36612812029384
At time: 1369.6302831172943 and batch: 450, loss is 5.136581048965454 and perplexity is 170.13309610008642
At time: 1371.173583984375 and batch: 500, loss is 5.140519409179688 and perplexity is 170.80446269084723
At time: 1372.7175829410553 and batch: 550, loss is 5.115360889434815 and perplexity is 166.56088019999777
At time: 1374.266625404358 and batch: 600, loss is 5.140136442184448 and perplexity is 170.73906274280714
At time: 1375.8106894493103 and batch: 650, loss is 5.135649261474609 and perplexity is 169.97464204357993
At time: 1377.3606033325195 and batch: 700, loss is 5.104429712295532 and perplexity is 164.7500887859268
At time: 1378.914668560028 and batch: 750, loss is 5.112246265411377 and perplexity is 166.04291273684242
At time: 1380.4632968902588 and batch: 800, loss is 5.120450887680054 and perplexity is 167.4108360898755
At time: 1382.0084731578827 and batch: 850, loss is 5.140054445266724 and perplexity is 170.725063239893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.062710762023926 and perplexity of 158.0182862396232
Finished 48 epochs...
Completing Train Step...
At time: 1386.0810115337372 and batch: 50, loss is 5.129970121383667 and perplexity is 169.01206812270613
At time: 1387.6685366630554 and batch: 100, loss is 5.075678577423096 and perplexity is 160.08078233650238
At time: 1389.2107095718384 and batch: 150, loss is 5.074804763793946 and perplexity is 159.94096266419598
At time: 1390.755928516388 and batch: 200, loss is 5.12026346206665 and perplexity is 167.37946195148135
At time: 1392.315812587738 and batch: 250, loss is 5.133418827056885 and perplexity is 169.59594723578758
At time: 1393.8665628433228 and batch: 300, loss is 5.102476005554199 and perplexity is 164.42852964524272
At time: 1395.4165952205658 and batch: 350, loss is 5.074951410293579 and perplexity is 159.9644191663836
At time: 1396.9719896316528 and batch: 400, loss is 5.104344644546509 and perplexity is 164.73607446281426
At time: 1398.5253698825836 and batch: 450, loss is 5.132829093933106 and perplexity is 169.49596037372402
At time: 1400.117348432541 and batch: 500, loss is 5.136267976760864 and perplexity is 170.0798404934767
At time: 1401.671198606491 and batch: 550, loss is 5.111222982406616 and perplexity is 165.8730907489733
At time: 1403.2181060314178 and batch: 600, loss is 5.135903587341309 and perplexity is 170.01787648932054
At time: 1404.7618639469147 and batch: 650, loss is 5.132049016952514 and perplexity is 169.3637920341751
At time: 1406.305871963501 and batch: 700, loss is 5.100910110473633 and perplexity is 164.17125330601417
At time: 1407.8516142368317 and batch: 750, loss is 5.109136924743653 and perplexity is 165.52743057581242
At time: 1409.399230480194 and batch: 800, loss is 5.116803703308105 and perplexity is 166.80136999798705
At time: 1410.942581653595 and batch: 850, loss is 5.136191711425782 and perplexity is 170.06686979206435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.061604817708333 and perplexity of 157.8436234156781
Finished 49 epochs...
Completing Train Step...
At time: 1415.0406849384308 and batch: 50, loss is 5.1258030605316165 and perplexity is 168.30924991023844
At time: 1416.6362671852112 and batch: 100, loss is 5.0723220729827885 and perplexity is 159.544371216541
At time: 1418.184891462326 and batch: 150, loss is 5.071628255844116 and perplexity is 159.4337149894447
At time: 1419.7339127063751 and batch: 200, loss is 5.116885299682617 and perplexity is 166.8149809403368
At time: 1421.2954082489014 and batch: 250, loss is 5.129449672698975 and perplexity is 168.92412890006594
At time: 1422.8561203479767 and batch: 300, loss is 5.099012842178345 and perplexity is 163.86007168297755
At time: 1424.4080801010132 and batch: 350, loss is 5.071292591094971 and perplexity is 159.38020769225662
At time: 1425.955988407135 and batch: 400, loss is 5.100829439163208 and perplexity is 164.15800993006334
At time: 1427.506276845932 and batch: 450, loss is 5.130168952941895 and perplexity is 169.04567639665225
At time: 1429.0601923465729 and batch: 500, loss is 5.134566125869751 and perplexity is 169.79063612652897
At time: 1430.6166143417358 and batch: 550, loss is 5.1093337440490725 and perplexity is 165.5600127760241
At time: 1432.164535522461 and batch: 600, loss is 5.13425181388855 and perplexity is 169.73727728140022
At time: 1433.7135169506073 and batch: 650, loss is 5.129177494049072 and perplexity is 168.87815761521085
At time: 1435.2645523548126 and batch: 700, loss is 5.097968854904175 and perplexity is 163.68909311864783
At time: 1436.816222190857 and batch: 750, loss is 5.106277542114258 and perplexity is 165.05480035348094
At time: 1438.367041349411 and batch: 800, loss is 5.1134114265441895 and perplexity is 166.236492238898
At time: 1440.0676295757294 and batch: 850, loss is 5.133803682327271 and perplexity is 169.66122969126008
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.059480349222819 and perplexity of 157.50864556302767
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f777bec6898>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'seq_len': 50, 'wordvec_source': 'glove', 'lr': 25.018820752002313, 'anneal': 4.547382136946763, 'dropout': 0.7545146846688054, 'wordvec_dim': 200, 'tune_wordvecs': True, 'data': 'wikitext', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.063769578933716 and batch: 50, loss is 7.4388065528869625 and perplexity is 1700.7192909600406
At time: 3.600137948989868 and batch: 100, loss is 6.568453273773193 and perplexity is 712.2673081667607
At time: 5.162789821624756 and batch: 150, loss is 6.503944053649902 and perplexity is 667.7701674628966
At time: 6.67951226234436 and batch: 200, loss is 6.580026884078979 and perplexity is 720.5587005457837
At time: 8.198698043823242 and batch: 250, loss is 6.670677623748779 and perplexity is 788.9300210939009
At time: 9.720341444015503 and batch: 300, loss is 6.632085208892822 and perplexity is 759.0633271480716
At time: 11.242039680480957 and batch: 350, loss is 6.662148189544678 and perplexity is 782.2295107907138
At time: 12.764329195022583 and batch: 400, loss is 6.721872711181641 and perplexity is 830.3711015614755
At time: 14.283103704452515 and batch: 450, loss is 6.767247371673584 and perplexity is 868.9167941124134
At time: 15.803849935531616 and batch: 500, loss is 6.812298812866211 and perplexity is 908.9579312615632
At time: 17.324580430984497 and batch: 550, loss is 7.031788911819458 and perplexity is 1132.053944545066
At time: 18.852073907852173 and batch: 600, loss is 7.064655961990357 and perplexity is 1169.879419226293
At time: 20.37504482269287 and batch: 650, loss is 7.1383397674560545 and perplexity is 1259.3358621561777
At time: 21.897123336791992 and batch: 700, loss is 7.345708446502686 and perplexity is 1549.5323372788573
At time: 23.419979095458984 and batch: 750, loss is 7.275969038009643 and perplexity is 1445.1509200149096
At time: 24.946693658828735 and batch: 800, loss is 7.343487215042114 and perplexity is 1546.0942870683405
At time: 26.47253704071045 and batch: 850, loss is 7.202095260620117 and perplexity is 1342.2401631118776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 7.307067235310872 and perplexity of 1490.798610136408
Finished 1 epochs...
Completing Train Step...
At time: 30.532870292663574 and batch: 50, loss is 7.0016944789886475 and perplexity is 1098.4929555222225
At time: 32.05783772468567 and batch: 100, loss is 7.081100444793702 and perplexity is 1189.2765318466036
At time: 33.581786155700684 and batch: 150, loss is 7.073441362380981 and perplexity is 1180.2025583906616
At time: 35.10780072212219 and batch: 200, loss is 7.32558316230774 and perplexity is 1518.6592653241644
At time: 36.63005232810974 and batch: 250, loss is 7.191716251373291 and perplexity is 1328.3810862036855
At time: 38.16338634490967 and batch: 300, loss is 7.319490089416504 and perplexity is 1509.4340970756389
At time: 39.68842601776123 and batch: 350, loss is 7.216794128417969 and perplexity is 1362.1152868859629
At time: 41.21253800392151 and batch: 400, loss is 7.036397962570191 and perplexity is 1137.283681431312
At time: 42.73961520195007 and batch: 450, loss is 7.161666374206543 and perplexity is 1289.0571958325384
At time: 44.27755904197693 and batch: 500, loss is 7.327466201782227 and perplexity is 1521.5216548194735
At time: 45.81181859970093 and batch: 550, loss is 7.116627817153931 and perplexity is 1232.2879188486227
At time: 47.33693766593933 and batch: 600, loss is 6.819133338928222 and perplexity is 915.1915054300429
At time: 48.922664403915405 and batch: 650, loss is 7.260510568618774 and perplexity is 1422.9828821656154
At time: 50.45040440559387 and batch: 700, loss is 7.134760589599609 and perplexity is 1254.8365318793954
At time: 51.99020552635193 and batch: 750, loss is 6.990409593582154 and perplexity is 1086.166271821073
At time: 53.528971433639526 and batch: 800, loss is 7.1532199001312256 and perplexity is 1278.2150610107406
At time: 55.068084478378296 and batch: 850, loss is 7.211423015594482 and perplexity is 1354.8188246037728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.908068339029948 and perplexity of 1000.313109056222
Finished 2 epochs...
Completing Train Step...
At time: 59.17971634864807 and batch: 50, loss is 7.17445294380188 and perplexity is 1305.6456440412078
At time: 60.71997117996216 and batch: 100, loss is 6.953473997116089 and perplexity is 1046.7799293308444
At time: 62.2540807723999 and batch: 150, loss is 7.144491834640503 and perplexity is 1267.1072615626472
At time: 63.78962469100952 and batch: 200, loss is 7.309973707199097 and perplexity is 1495.1378773121135
At time: 65.33182764053345 and batch: 250, loss is 7.413326253890991 and perplexity is 1657.9318877676517
At time: 66.87016797065735 and batch: 300, loss is 7.277913417816162 and perplexity is 1447.9635758306138
At time: 68.40044164657593 and batch: 350, loss is 6.992337102890015 and perplexity is 1088.261886429406
At time: 69.9309892654419 and batch: 400, loss is 7.152546195983887 and perplexity is 1277.3542122342024
At time: 71.46311211585999 and batch: 450, loss is 7.266169080734253 and perplexity is 1431.0576721475543
At time: 72.99695801734924 and batch: 500, loss is 6.887798070907593 and perplexity is 980.2406187927729
At time: 74.52895903587341 and batch: 550, loss is 7.028077402114868 and perplexity is 1127.8601029000365
At time: 76.05855536460876 and batch: 600, loss is 7.174773397445679 and perplexity is 1306.0641099912395
At time: 77.58882236480713 and batch: 650, loss is 7.119067764282226 and perplexity is 1235.2983073183125
At time: 79.12029027938843 and batch: 700, loss is 7.103354721069336 and perplexity is 1216.0397134370482
At time: 80.65297317504883 and batch: 750, loss is 7.059928874969483 and perplexity is 1164.3623475201139
At time: 82.18595099449158 and batch: 800, loss is 7.162187566757202 and perplexity is 1289.7292179516282
At time: 83.77252578735352 and batch: 850, loss is 7.178260335922241 and perplexity is 1310.6262244730224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.362742741902669 and perplexity of 579.8345138628988
Finished 3 epochs...
Completing Train Step...
At time: 87.85479688644409 and batch: 50, loss is 7.105253019332886 and perplexity is 1218.3503119221587
At time: 89.44196701049805 and batch: 100, loss is 6.9035705471038815 and perplexity is 995.824011911134
At time: 90.98524117469788 and batch: 150, loss is 6.912614622116089 and perplexity is 1004.8711688891734
At time: 92.52503538131714 and batch: 200, loss is 6.7437675666809085 and perplexity is 848.7524510305813
At time: 94.06325769424438 and batch: 250, loss is 6.986163091659546 and perplexity is 1081.5636441097377
At time: 95.60254120826721 and batch: 300, loss is 7.05983003616333 and perplexity is 1164.24726902296
At time: 97.14227724075317 and batch: 350, loss is 7.207532253265381 and perplexity is 1349.557787916734
At time: 98.68569540977478 and batch: 400, loss is 7.101890459060669 and perplexity is 1214.2604156805953
At time: 100.22886109352112 and batch: 450, loss is 7.2786713981628415 and perplexity is 1449.0615198212172
At time: 101.77164101600647 and batch: 500, loss is 7.42486644744873 and perplexity is 1677.1755669835088
At time: 103.31530833244324 and batch: 550, loss is 7.119798631668091 and perplexity is 1236.2014765712165
At time: 104.85755705833435 and batch: 600, loss is 7.25519100189209 and perplexity is 1415.4333277551943
At time: 106.39852714538574 and batch: 650, loss is 7.203857917785644 and perplexity is 1344.6081587223541
At time: 107.94005990028381 and batch: 700, loss is 7.0862765121459965 and perplexity is 1195.4482661529191
At time: 109.48725056648254 and batch: 750, loss is 7.284963693618774 and perplexity is 1458.2081896335626
At time: 111.02754592895508 and batch: 800, loss is 7.204771375656128 and perplexity is 1345.8369627725776
At time: 112.56796860694885 and batch: 850, loss is 7.256964998245239 and perplexity is 1417.946529863345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 7.181937535603841 and perplexity of 1315.4545306907808
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 116.66567635536194 and batch: 50, loss is 6.716393508911133 and perplexity is 825.8337721607196
At time: 118.20217037200928 and batch: 100, loss is 6.51472617149353 and perplexity is 675.0090994805083
At time: 119.74374151229858 and batch: 150, loss is 6.552764892578125 and perplexity is 701.1801840391746
At time: 121.28700757026672 and batch: 200, loss is 6.553967838287353 and perplexity is 702.0241732677946
At time: 122.8581063747406 and batch: 250, loss is 6.5015942573547365 and perplexity is 666.2028857149436
At time: 124.39818572998047 and batch: 300, loss is 6.466588010787964 and perplexity is 643.2850964369111
At time: 125.93953061103821 and batch: 350, loss is 6.446615133285523 and perplexity is 630.5643003322358
At time: 127.47948050498962 and batch: 400, loss is 6.479183778762818 and perplexity is 651.4390108568228
At time: 129.0186812877655 and batch: 450, loss is 6.490598554611206 and perplexity is 658.9176433981994
At time: 130.55822134017944 and batch: 500, loss is 6.508762435913086 and perplexity is 670.9955036047476
At time: 132.1130566596985 and batch: 550, loss is 6.455248937606812 and perplexity is 636.0320388405744
At time: 133.65816569328308 and batch: 600, loss is 6.4799411010742185 and perplexity is 651.9325470136447
At time: 135.20208835601807 and batch: 650, loss is 6.499082136154175 and perplexity is 664.531403683613
At time: 136.75114274024963 and batch: 700, loss is 6.463181715011597 and perplexity is 641.0976048666611
At time: 138.30105137825012 and batch: 750, loss is 6.4645873546600345 and perplexity is 641.9993907230762
At time: 139.8424153327942 and batch: 800, loss is 6.502639980316162 and perplexity is 666.8999137550979
At time: 141.3860728740692 and batch: 850, loss is 6.501292104721069 and perplexity is 666.0016211663062
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.054533004760742 and perplexity of 426.0399003858002
Finished 5 epochs...
Completing Train Step...
At time: 145.51250743865967 and batch: 50, loss is 6.4991482162475585 and perplexity is 664.5753174317211
At time: 147.04469633102417 and batch: 100, loss is 6.449200801849365 and perplexity is 632.1968403151892
At time: 148.57771921157837 and batch: 150, loss is 6.432415714263916 and perplexity is 621.6739220899768
At time: 150.1091513633728 and batch: 200, loss is 6.481019372940064 and perplexity is 652.6358866650513
At time: 151.64361929893494 and batch: 250, loss is 6.503449001312256 and perplexity is 667.439668094476
At time: 153.19296598434448 and batch: 300, loss is 6.472610950469971 and perplexity is 647.1712550728274
At time: 154.74072217941284 and batch: 350, loss is 6.442279033660888 and perplexity is 627.8360300069132
At time: 156.28348684310913 and batch: 400, loss is 6.473244714736938 and perplexity is 647.5815390868421
At time: 157.82752180099487 and batch: 450, loss is 6.460817289352417 and perplexity is 639.5835678590034
At time: 159.37408900260925 and batch: 500, loss is 6.465552654266357 and perplexity is 642.6194116870521
At time: 160.955393075943 and batch: 550, loss is 6.40048318862915 and perplexity is 602.1359128189664
At time: 162.5004243850708 and batch: 600, loss is 6.4231726264953615 and perplexity is 615.9542100864588
At time: 164.0483374595642 and batch: 650, loss is 6.455043659210205 and perplexity is 635.901488603481
At time: 165.59909653663635 and batch: 700, loss is 6.415590400695801 and perplexity is 611.3015671680522
At time: 167.14554023742676 and batch: 750, loss is 6.412958984375 and perplexity is 609.6950928256595
At time: 168.6876347064972 and batch: 800, loss is 6.432917261123658 and perplexity is 621.9857988972631
At time: 170.23029851913452 and batch: 850, loss is 6.425747718811035 and perplexity is 617.5423930208777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.982348759969075 and perplexity of 396.3702542743598
Finished 6 epochs...
Completing Train Step...
At time: 174.3496413230896 and batch: 50, loss is 6.41960337638855 and perplexity is 613.7596342811051
At time: 175.89545512199402 and batch: 100, loss is 6.352284202575683 and perplexity is 573.8018929856293
At time: 177.4465889930725 and batch: 150, loss is 6.339985637664795 and perplexity is 566.7881709214371
At time: 179.0012800693512 and batch: 200, loss is 6.3551786994934085 and perplexity is 575.4651668054757
At time: 180.54777932167053 and batch: 250, loss is 6.3829531002044675 and perplexity is 591.6723981475714
At time: 182.09325218200684 and batch: 300, loss is 6.355813159942627 and perplexity is 575.8303925421075
At time: 183.64170145988464 and batch: 350, loss is 6.330962247848511 and perplexity is 561.6968254547675
At time: 185.18818426132202 and batch: 400, loss is 6.359147386550903 and perplexity is 577.7535458916591
At time: 186.73304533958435 and batch: 450, loss is 6.365765399932862 and perplexity is 581.5898068011265
At time: 188.27931475639343 and batch: 500, loss is 6.368939018249511 and perplexity is 583.4384828091552
At time: 189.82334351539612 and batch: 550, loss is 6.305428371429444 and perplexity is 547.536086771208
At time: 191.36283707618713 and batch: 600, loss is 6.330743350982666 and perplexity is 561.5738852362255
At time: 192.90993070602417 and batch: 650, loss is 6.3597628688812256 and perplexity is 578.1092524447564
At time: 194.46443104743958 and batch: 700, loss is 6.31344783782959 and perplexity is 551.9446877084026
At time: 196.01407384872437 and batch: 750, loss is 6.321061410903931 and perplexity is 556.1629967466251
At time: 197.558984041214 and batch: 800, loss is 6.358642377853394 and perplexity is 577.461848986903
At time: 199.10680174827576 and batch: 850, loss is 6.344698476791382 and perplexity is 569.4656567139756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.930103937784831 and perplexity of 376.1936125235413
Finished 7 epochs...
Completing Train Step...
At time: 203.23876404762268 and batch: 50, loss is 6.356473159790039 and perplexity is 576.2105659567184
At time: 204.80354189872742 and batch: 100, loss is 6.304569082260132 and perplexity is 547.065797028417
At time: 206.34723544120789 and batch: 150, loss is 6.2890783882141115 and perplexity is 538.6566679217021
At time: 207.89208030700684 and batch: 200, loss is 6.32278073310852 and perplexity is 557.1200426356688
At time: 209.43959856033325 and batch: 250, loss is 6.363342428207398 and perplexity is 580.1823369618222
At time: 210.98967146873474 and batch: 300, loss is 6.3395336151123045 and perplexity is 566.532027781295
At time: 212.53417301177979 and batch: 350, loss is 6.316421728134156 and perplexity is 553.5885537910056
At time: 214.08163619041443 and batch: 400, loss is 6.348498315811157 and perplexity is 571.6336509427139
At time: 215.62572765350342 and batch: 450, loss is 6.356140956878662 and perplexity is 576.0191789205669
At time: 217.17444038391113 and batch: 500, loss is 6.358524351119995 and perplexity is 577.3936970731579
At time: 218.72241020202637 and batch: 550, loss is 6.303625526428223 and perplexity is 546.549853354303
At time: 220.26674032211304 and batch: 600, loss is 6.323402938842773 and perplexity is 557.4667937849538
At time: 221.8152039051056 and batch: 650, loss is 6.35495379447937 and perplexity is 575.3357563571317
At time: 223.37049460411072 and batch: 700, loss is 6.31716178894043 and perplexity is 553.9983946173289
At time: 224.914222240448 and batch: 750, loss is 6.319795532226562 and perplexity is 555.4594072913023
At time: 226.45684814453125 and batch: 800, loss is 6.350900650024414 and perplexity is 573.0085568503343
At time: 228.00249314308167 and batch: 850, loss is 6.347892770767212 and perplexity is 571.2876058019672
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.918824513753255 and perplexity of 371.97420622088765
Finished 8 epochs...
Completing Train Step...
At time: 232.08903193473816 and batch: 50, loss is 6.347955532073975 and perplexity is 571.3234616838142
At time: 233.66390919685364 and batch: 100, loss is 6.299858293533325 and perplexity is 544.4947462321352
At time: 235.2114498615265 and batch: 150, loss is 6.28856125831604 and perplexity is 538.378184466191
At time: 236.76018905639648 and batch: 200, loss is 6.320371074676514 and perplexity is 555.7791897747929
At time: 238.30264353752136 and batch: 250, loss is 6.351313743591309 and perplexity is 573.2453118965714
At time: 239.8756091594696 and batch: 300, loss is 6.331056432723999 and perplexity is 561.7497312917607
At time: 241.42757415771484 and batch: 350, loss is 6.309905424118042 and perplexity is 549.9929302857763
At time: 242.97756242752075 and batch: 400, loss is 6.336591691970825 and perplexity is 564.8677833383197
At time: 244.51822304725647 and batch: 450, loss is 6.351810750961303 and perplexity is 573.530289853605
At time: 246.05676889419556 and batch: 500, loss is 6.35212739944458 and perplexity is 573.7119261059046
At time: 247.5981981754303 and batch: 550, loss is 6.292640037536621 and perplexity is 540.5785946609136
At time: 249.13843655586243 and batch: 600, loss is 6.316321191787719 and perplexity is 553.5329008179986
At time: 250.67758440971375 and batch: 650, loss is 6.342182264328003 and perplexity is 568.0345613561345
At time: 252.21277403831482 and batch: 700, loss is 6.304149131774903 and perplexity is 546.8361047145777
At time: 253.76111364364624 and batch: 750, loss is 6.312172441482544 and perplexity is 551.2411881856503
At time: 255.3099389076233 and batch: 800, loss is 6.34537413597107 and perplexity is 569.8505514266318
At time: 256.8607714176178 and batch: 850, loss is 6.335992460250854 and perplexity is 564.5293980406815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.91856575012207 and perplexity of 371.877965276944
Finished 9 epochs...
Completing Train Step...
At time: 260.96362924575806 and batch: 50, loss is 6.343965749740601 and perplexity is 569.0485466553841
At time: 262.5350534915924 and batch: 100, loss is 6.294704265594483 and perplexity is 541.695624669192
At time: 264.0833296775818 and batch: 150, loss is 6.279277582168579 and perplexity is 533.4031846318894
At time: 265.6229827404022 and batch: 200, loss is 6.3109392738342285 and perplexity is 550.5618343506375
At time: 267.1679563522339 and batch: 250, loss is 6.3512788581848145 and perplexity is 573.2253143496582
At time: 268.71848011016846 and batch: 300, loss is 6.329384489059448 and perplexity is 560.8113021064364
At time: 270.2679922580719 and batch: 350, loss is 6.301916046142578 and perplexity is 545.6163352975656
At time: 271.8150315284729 and batch: 400, loss is 6.333258075714111 and perplexity is 562.9878661151567
At time: 273.35970520973206 and batch: 450, loss is 6.345404806137085 and perplexity is 569.8680291056686
At time: 274.9081115722656 and batch: 500, loss is 6.345784502029419 and perplexity is 570.0844467393355
At time: 276.45985674858093 and batch: 550, loss is 6.289385347366333 and perplexity is 538.8220388955946
At time: 278.008465051651 and batch: 600, loss is 6.309124422073364 and perplexity is 549.5635523770029
At time: 279.5830948352814 and batch: 650, loss is 6.33718843460083 and perplexity is 565.2049646201708
At time: 281.1310291290283 and batch: 700, loss is 6.300901823043823 and perplexity is 545.0632391361119
At time: 282.6824429035187 and batch: 750, loss is 6.3078199481964115 and perplexity is 548.8471284589366
At time: 284.2358021736145 and batch: 800, loss is 6.338337211608887 and perplexity is 565.8546321784963
At time: 285.7843132019043 and batch: 850, loss is 6.330313186645508 and perplexity is 561.3323681278654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.915534973144531 and perplexity of 370.75259233995905
Finished 10 epochs...
Completing Train Step...
At time: 289.88451051712036 and batch: 50, loss is 6.335609331130981 and perplexity is 564.3131518170254
At time: 291.4358160495758 and batch: 100, loss is 6.286551885604858 and perplexity is 537.2974681785604
At time: 292.9806077480316 and batch: 150, loss is 6.27459644317627 and perplexity is 530.9120853257015
At time: 294.52616357803345 and batch: 200, loss is 6.305450925827026 and perplexity is 547.548436257067
At time: 296.0776641368866 and batch: 250, loss is 6.338475894927979 and perplexity is 565.9331122188207
At time: 297.62445306777954 and batch: 300, loss is 6.321804466247559 and perplexity is 556.5764102088056
At time: 299.16923475265503 and batch: 350, loss is 6.298308658599853 and perplexity is 543.6516315809592
At time: 300.7122766971588 and batch: 400, loss is 6.326050701141358 and perplexity is 558.9447891710058
At time: 302.26052498817444 and batch: 450, loss is 6.336102743148803 and perplexity is 564.5916594117939
At time: 303.8077144622803 and batch: 500, loss is 6.3369737720489505 and perplexity is 565.0836493015241
At time: 305.35304021835327 and batch: 550, loss is 6.27952712059021 and perplexity is 533.5363058294114
At time: 306.89958095550537 and batch: 600, loss is 6.292115087509155 and perplexity is 540.2948923840729
At time: 308.44916439056396 and batch: 650, loss is 6.3123219871520995 and perplexity is 551.3236300824847
At time: 309.99410700798035 and batch: 700, loss is 6.27098536491394 and perplexity is 528.998377588922
At time: 311.53965282440186 and batch: 750, loss is 6.282761278152466 and perplexity is 535.2646396531026
At time: 313.08895468711853 and batch: 800, loss is 6.314469499588013 and perplexity is 552.5088766446546
At time: 314.6403684616089 and batch: 850, loss is 6.304220237731934 and perplexity is 546.8749894015925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.890359878540039 and perplexity of 361.53536978445845
Finished 11 epochs...
Completing Train Step...
At time: 318.76385712623596 and batch: 50, loss is 6.309459629058838 and perplexity is 549.747800797675
At time: 320.3115096092224 and batch: 100, loss is 6.260396566390991 and perplexity is 523.4264723022044
At time: 321.8583345413208 and batch: 150, loss is 6.245954017639161 and perplexity is 515.921188132645
At time: 323.401718378067 and batch: 200, loss is 6.273953695297241 and perplexity is 530.5709523518993
At time: 324.9457314014435 and batch: 250, loss is 6.312381572723389 and perplexity is 551.3564819946888
At time: 326.49165654182434 and batch: 300, loss is 6.294654445648193 and perplexity is 541.6686380945063
At time: 328.0438756942749 and batch: 350, loss is 6.265757856369018 and perplexity is 526.2402494000726
At time: 329.58529829978943 and batch: 400, loss is 6.29587423324585 and perplexity is 542.3297619146387
At time: 331.1285045146942 and batch: 450, loss is 6.310166091918945 and perplexity is 550.136314420428
At time: 332.67458319664 and batch: 500, loss is 6.30855544090271 and perplexity is 549.2509500044878
At time: 334.2194097042084 and batch: 550, loss is 6.2545316028594975 and perplexity is 520.3655799158348
At time: 335.7614006996155 and batch: 600, loss is 6.271838989257812 and perplexity is 529.4501362705727
At time: 337.2979507446289 and batch: 650, loss is 6.298489789962769 and perplexity is 543.7501128606938
At time: 338.84584498405457 and batch: 700, loss is 6.261406278610229 and perplexity is 523.9552493185431
At time: 340.3991711139679 and batch: 750, loss is 6.271923322677612 and perplexity is 529.4947884939886
At time: 341.9469473361969 and batch: 800, loss is 6.304321336746216 and perplexity is 546.9302807188583
At time: 343.49437618255615 and batch: 850, loss is 6.296446199417114 and perplexity is 542.6400449193559
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.88461176554362 and perplexity of 359.4631849086942
Finished 12 epochs...
Completing Train Step...
At time: 347.5882740020752 and batch: 50, loss is 6.302002115249634 and perplexity is 545.6632980293308
At time: 349.1317923069 and batch: 100, loss is 6.2530164051055905 and perplexity is 519.5777201903114
At time: 350.6784670352936 and batch: 150, loss is 6.240759363174439 and perplexity is 513.2481047125963
At time: 352.2262420654297 and batch: 200, loss is 6.268762559890747 and perplexity is 527.8238232242362
At time: 353.7712800502777 and batch: 250, loss is 6.304772176742554 and perplexity is 547.1769143565914
At time: 355.3194046020508 and batch: 300, loss is 6.28937873840332 and perplexity is 538.8184778524361
At time: 356.86758494377136 and batch: 350, loss is 6.263709907531738 and perplexity is 525.163639090883
At time: 358.4595994949341 and batch: 400, loss is 6.291777601242066 and perplexity is 540.1125810432318
At time: 360.0129544734955 and batch: 450, loss is 6.306354541778564 and perplexity is 548.0434333682699
At time: 361.5630249977112 and batch: 500, loss is 6.30272650718689 and perplexity is 546.0587153241388
At time: 363.11332535743713 and batch: 550, loss is 6.248001403808594 and perplexity is 516.9785600927523
At time: 364.65686440467834 and batch: 600, loss is 6.267223739624024 and perplexity is 527.012221842281
At time: 366.199720621109 and batch: 650, loss is 6.2903906440734865 and perplexity is 539.3639872808594
At time: 367.7470796108246 and batch: 700, loss is 6.24939564704895 and perplexity is 517.6998566701645
At time: 369.29951524734497 and batch: 750, loss is 6.262744760513305 and perplexity is 524.6570234890203
At time: 370.8508291244507 and batch: 800, loss is 6.298735618591309 and perplexity is 543.8837986364289
At time: 372.39705634117126 and batch: 850, loss is 6.289445381164551 and perplexity is 538.854387400145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.88232421875 and perplexity of 358.6418358480047
Finished 13 epochs...
Completing Train Step...
At time: 376.490225315094 and batch: 50, loss is 6.292805604934692 and perplexity is 540.6681042620157
At time: 378.05738639831543 and batch: 100, loss is 6.241195287704468 and perplexity is 513.4718909248372
At time: 379.60560035705566 and batch: 150, loss is 6.231765089035034 and perplexity is 508.65250856385325
At time: 381.15596079826355 and batch: 200, loss is 6.264337320327758 and perplexity is 525.4932368641713
At time: 382.6993842124939 and batch: 250, loss is 6.3022314453125 and perplexity is 545.7884493777063
At time: 384.2437517642975 and batch: 300, loss is 6.284532747268677 and perplexity is 536.2136847851349
At time: 385.8054111003876 and batch: 350, loss is 6.257204170227051 and perplexity is 521.7581520264218
At time: 387.35438895225525 and batch: 400, loss is 6.286958198547364 and perplexity is 537.5158234511395
At time: 388.9025881290436 and batch: 450, loss is 6.3001101779937745 and perplexity is 544.6319132719051
At time: 390.4485993385315 and batch: 500, loss is 6.298888120651245 and perplexity is 543.9667483609282
At time: 391.9924340248108 and batch: 550, loss is 6.2446519947052 and perplexity is 515.249884035082
At time: 393.53931283950806 and batch: 600, loss is 6.262184352874756 and perplexity is 524.3630840560828
At time: 395.08627700805664 and batch: 650, loss is 6.2900145053863525 and perplexity is 539.1611497687054
At time: 396.66241788864136 and batch: 700, loss is 6.248029766082763 and perplexity is 516.9932229883489
At time: 398.20701575279236 and batch: 750, loss is 6.257587823867798 and perplexity is 521.9583648447688
At time: 399.7506821155548 and batch: 800, loss is 6.293382129669189 and perplexity is 540.9799026683869
At time: 401.2933576107025 and batch: 850, loss is 6.286447381973266 and perplexity is 537.2413215757036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.878877639770508 and perplexity of 357.40787612673034
Finished 14 epochs...
Completing Train Step...
At time: 405.33828687667847 and batch: 50, loss is 6.286751585006714 and perplexity is 537.404776875959
At time: 406.9095721244812 and batch: 100, loss is 6.237594232559204 and perplexity is 511.62617558597395
At time: 408.45823311805725 and batch: 150, loss is 6.228341073989868 and perplexity is 506.91385301180213
At time: 410.0045485496521 and batch: 200, loss is 6.259173307418823 and perplexity is 522.7865776319318
At time: 411.549519777298 and batch: 250, loss is 6.293433303833008 and perplexity is 541.0075875709184
At time: 413.09377336502075 and batch: 300, loss is 6.2767230415344235 and perplexity is 532.0423234499978
At time: 414.6530292034149 and batch: 350, loss is 6.251955633163452 and perplexity is 519.0268589437045
At time: 416.19502997398376 and batch: 400, loss is 6.281298894882202 and perplexity is 534.4824496690802
At time: 417.7356560230255 and batch: 450, loss is 6.291771278381348 and perplexity is 540.1091659974064
At time: 419.2815487384796 and batch: 500, loss is 6.287941541671753 and perplexity is 538.0446459047297
At time: 420.8292474746704 and batch: 550, loss is 6.237334508895874 and perplexity is 511.4933114161261
At time: 422.3732614517212 and batch: 600, loss is 6.254734115600586 and perplexity is 520.4709712469752
At time: 423.9202620983124 and batch: 650, loss is 6.278753032684326 and perplexity is 533.1234616371823
At time: 425.4703152179718 and batch: 700, loss is 6.237079524993897 and perplexity is 511.36290548215976
At time: 427.01398253440857 and batch: 750, loss is 6.2481322097778325 and perplexity is 517.0461883973773
At time: 428.5572259426117 and batch: 800, loss is 6.281061544418335 and perplexity is 534.3556050656306
At time: 430.1040301322937 and batch: 850, loss is 6.268853979110718 and perplexity is 527.8720786721416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.873119990030925 and perplexity of 355.35595953883546
Finished 15 epochs...
Completing Train Step...
At time: 434.2149748802185 and batch: 50, loss is 6.273190298080444 and perplexity is 530.1660705260533
At time: 435.7773005962372 and batch: 100, loss is 6.2209927845001225 and perplexity is 503.202555816601
At time: 437.32043981552124 and batch: 150, loss is 6.212684936523438 and perplexity is 499.0393430960873
At time: 438.85730385780334 and batch: 200, loss is 6.244036750793457 and perplexity is 514.9329771784066
At time: 440.3939983844757 and batch: 250, loss is 6.276249351501465 and perplexity is 531.7903599852699
At time: 441.929114818573 and batch: 300, loss is 6.260300779342652 and perplexity is 523.3763372265836
At time: 443.46536684036255 and batch: 350, loss is 6.230763854980469 and perplexity is 508.14348321961756
At time: 445.0014703273773 and batch: 400, loss is 6.260884342193603 and perplexity is 523.6818493481484
At time: 446.5353169441223 and batch: 450, loss is 6.268814811706543 and perplexity is 527.8514036979788
At time: 448.06523156166077 and batch: 500, loss is 6.265148277282715 and perplexity is 525.9195621017327
At time: 449.5968642234802 and batch: 550, loss is 6.219117002487183 and perplexity is 502.2595422339969
At time: 451.1270434856415 and batch: 600, loss is 6.23745831489563 and perplexity is 511.5566412771421
At time: 452.659863948822 and batch: 650, loss is 6.2576838302612305 and perplexity is 522.008478590481
At time: 454.19149255752563 and batch: 700, loss is 6.221979389190674 and perplexity is 503.6992628048984
At time: 455.72254705429077 and batch: 750, loss is 6.230420408248901 and perplexity is 507.9689929669068
At time: 457.25312900543213 and batch: 800, loss is 6.265428123474121 and perplexity is 526.0667592834974
At time: 458.783917427063 and batch: 850, loss is 6.25852403640747 and perplexity is 522.4472576291917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.85897954305013 and perplexity of 350.3664276777666
Finished 16 epochs...
Completing Train Step...
At time: 463.000337600708 and batch: 50, loss is 6.261754179000855 and perplexity is 524.1375652665073
At time: 464.5511202812195 and batch: 100, loss is 6.213585567474365 and perplexity is 499.4889958294005
At time: 466.1001663208008 and batch: 150, loss is 6.201155443191528 and perplexity is 493.3187136666891
At time: 467.6493537425995 and batch: 200, loss is 6.230531196594239 and perplexity is 508.0252731286559
At time: 469.19505071640015 and batch: 250, loss is 6.260210180282593 and perplexity is 523.3289219702953
At time: 470.7422630786896 and batch: 300, loss is 6.246492538452149 and perplexity is 516.1990972535226
At time: 472.2884330749512 and batch: 350, loss is 6.22127589225769 and perplexity is 503.3450365315268
At time: 473.82800364494324 and batch: 400, loss is 6.249946727752685 and perplexity is 517.9852296960637
At time: 475.41891527175903 and batch: 450, loss is 6.2584600925445555 and perplexity is 522.4138514014426
At time: 476.9677677154541 and batch: 500, loss is 6.250354204177857 and perplexity is 518.1963394739596
At time: 478.517347574234 and batch: 550, loss is 6.208002338409424 and perplexity is 496.70800502878774
At time: 480.0632815361023 and batch: 600, loss is 6.225463752746582 and perplexity is 505.4573953671074
At time: 481.60923981666565 and batch: 650, loss is 6.243585786819458 and perplexity is 514.7008133093801
At time: 483.1574230194092 and batch: 700, loss is 6.202820320129394 and perplexity is 494.1407126898298
At time: 484.7066798210144 and batch: 750, loss is 6.215193901062012 and perplexity is 500.29298712784794
At time: 486.2549629211426 and batch: 800, loss is 6.25598837852478 and perplexity is 521.1241882566324
At time: 487.80788564682007 and batch: 850, loss is 6.24443886756897 and perplexity is 515.1400820041666
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.860389073689778 and perplexity of 350.8606281061834
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 491.9560241699219 and batch: 50, loss is 6.243001728057862 and perplexity is 514.4002855612414
At time: 493.50395941734314 and batch: 100, loss is 6.176190404891968 and perplexity is 481.1554529469402
At time: 495.06447863578796 and batch: 150, loss is 6.156900691986084 and perplexity is 471.9630468676652
At time: 496.6124050617218 and batch: 200, loss is 6.176801548004151 and perplexity is 481.4495976610007
At time: 498.15675950050354 and batch: 250, loss is 6.206823987960815 and perplexity is 496.12305363478777
At time: 499.70132303237915 and batch: 300, loss is 6.174101963043213 and perplexity is 480.15163633441296
At time: 501.24544286727905 and batch: 350, loss is 6.1409338855743405 and perplexity is 464.4871462181979
At time: 502.7939627170563 and batch: 400, loss is 6.169181489944458 and perplexity is 477.7948660917854
At time: 504.3381025791168 and batch: 450, loss is 6.18878625869751 and perplexity is 487.2543464403552
At time: 505.88963198661804 and batch: 500, loss is 6.172459402084351 and perplexity is 479.3636053738575
At time: 507.4677333831787 and batch: 550, loss is 6.114333200454712 and perplexity is 452.2943571272449
At time: 509.00904750823975 and batch: 600, loss is 6.113138637542725 and perplexity is 451.7543856420852
At time: 510.5446197986603 and batch: 650, loss is 6.119348802566528 and perplexity is 454.56858420148706
At time: 512.0786113739014 and batch: 700, loss is 6.077522897720337 and perplexity is 435.9479684108212
At time: 513.6131818294525 and batch: 750, loss is 6.065991840362549 and perplexity is 430.9498992736979
At time: 515.2193405628204 and batch: 800, loss is 6.090532751083374 and perplexity is 441.65664153504173
At time: 516.7539300918579 and batch: 850, loss is 6.1055403995513915 and perplexity is 448.3348559582157
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.732654571533203 and perplexity of 308.78788088236007
Finished 18 epochs...
Completing Train Step...
At time: 520.9063332080841 and batch: 50, loss is 6.143150930404663 and perplexity is 465.51807743255483
At time: 522.4386217594147 and batch: 100, loss is 6.096331911087036 and perplexity is 444.22531995579135
At time: 523.990617275238 and batch: 150, loss is 6.082493743896484 and perplexity is 438.1203936251208
At time: 525.526166677475 and batch: 200, loss is 6.117913751602173 and perplexity is 453.9167229552868
At time: 527.0607182979584 and batch: 250, loss is 6.145849409103394 and perplexity is 466.77596447577116
At time: 528.6043856143951 and batch: 300, loss is 6.114781379699707 and perplexity is 452.4971115024954
At time: 530.1435914039612 and batch: 350, loss is 6.085552730560303 and perplexity is 439.46264999186064
At time: 531.678352355957 and batch: 400, loss is 6.114347267150879 and perplexity is 452.3007194592931
At time: 533.2150156497955 and batch: 450, loss is 6.140214290618896 and perplexity is 464.1530238417168
At time: 534.7533342838287 and batch: 500, loss is 6.1270903301239015 and perplexity is 458.1012960726485
At time: 536.2905721664429 and batch: 550, loss is 6.079631252288818 and perplexity is 436.8680709119626
At time: 537.8269598484039 and batch: 600, loss is 6.091684627532959 and perplexity is 442.165668531051
At time: 539.361501455307 and batch: 650, loss is 6.100247888565064 and perplexity is 445.9683068251964
At time: 540.8986263275146 and batch: 700, loss is 6.0640515613555905 and perplexity is 430.1145469014917
At time: 542.434244632721 and batch: 750, loss is 6.064200429916382 and perplexity is 430.1785822013676
At time: 543.9686484336853 and batch: 800, loss is 6.0947091674804685 and perplexity is 443.50504073027076
At time: 545.5045449733734 and batch: 850, loss is 6.106297397613526 and perplexity is 448.6743730660464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.729608535766602 and perplexity of 307.84873301803555
Finished 19 epochs...
Completing Train Step...
At time: 549.5103623867035 and batch: 50, loss is 6.120767946243286 and perplexity is 455.21414029348824
At time: 551.072927236557 and batch: 100, loss is 6.073119831085205 and perplexity is 434.0326801217453
At time: 552.6069808006287 and batch: 150, loss is 6.060962219238281 and perplexity is 428.78782631855745
At time: 554.1863372325897 and batch: 200, loss is 6.101634874343872 and perplexity is 446.5872876841571
At time: 555.7211074829102 and batch: 250, loss is 6.128814554214477 and perplexity is 458.89184671063276
At time: 557.2567086219788 and batch: 300, loss is 6.096263246536255 and perplexity is 444.1948184709483
At time: 558.8052108287811 and batch: 350, loss is 6.067654600143433 and perplexity is 431.6670615028093
At time: 560.3558568954468 and batch: 400, loss is 6.099365510940552 and perplexity is 445.5749679322522
At time: 561.9041068553925 and batch: 450, loss is 6.123870983123779 and perplexity is 456.62888041954267
At time: 563.4533472061157 and batch: 500, loss is 6.1121063232421875 and perplexity is 451.288273757832
At time: 565.0009264945984 and batch: 550, loss is 6.073484735488892 and perplexity is 434.19108945844
At time: 566.5505814552307 and batch: 600, loss is 6.089738349914551 and perplexity is 441.3059283047319
At time: 568.1074552536011 and batch: 650, loss is 6.098617019653321 and perplexity is 445.24158373409506
At time: 569.6658403873444 and batch: 700, loss is 6.065135154724121 and perplexity is 430.5808687782407
At time: 571.2193179130554 and batch: 750, loss is 6.068869333267212 and perplexity is 432.1917403887554
At time: 572.7692973613739 and batch: 800, loss is 6.097288808822632 and perplexity is 444.6506016016223
At time: 574.3239638805389 and batch: 850, loss is 6.106783485412597 and perplexity is 448.89252121983253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.72883669535319 and perplexity of 307.6112145996235
Finished 20 epochs...
Completing Train Step...
At time: 578.5091505050659 and batch: 50, loss is 6.113963851928711 and perplexity is 452.1273337199448
At time: 580.1146953105927 and batch: 100, loss is 6.065663928985596 and perplexity is 430.8086090654287
At time: 581.6613297462463 and batch: 150, loss is 6.053926877975464 and perplexity is 425.7817444361005
At time: 583.2130174636841 and batch: 200, loss is 6.096393718719482 and perplexity is 444.25277731961893
At time: 584.7531092166901 and batch: 250, loss is 6.123451519012451 and perplexity is 456.43738115835237
At time: 586.3167490959167 and batch: 300, loss is 6.091492080688477 and perplexity is 442.08053912279956
At time: 587.8512840270996 and batch: 350, loss is 6.063455839157104 and perplexity is 429.85839442344457
At time: 589.3872184753418 and batch: 400, loss is 6.095859098434448 and perplexity is 444.0153342497775
At time: 590.9268238544464 and batch: 450, loss is 6.120411701202393 and perplexity is 455.052001395768
At time: 592.5193450450897 and batch: 500, loss is 6.109278554916382 and perplexity is 450.01393768424634
At time: 594.0570306777954 and batch: 550, loss is 6.072948446273804 and perplexity is 433.9582998867239
At time: 595.5930166244507 and batch: 600, loss is 6.08972318649292 and perplexity is 441.29923664760696
At time: 597.1307263374329 and batch: 650, loss is 6.099192790985107 and perplexity is 445.49801488949805
At time: 598.6686182022095 and batch: 700, loss is 6.065689153671265 and perplexity is 430.8194762142356
At time: 600.2050971984863 and batch: 750, loss is 6.068909549713135 and perplexity is 432.20912195402104
At time: 601.7415826320648 and batch: 800, loss is 6.09647123336792 and perplexity is 444.2872147521559
At time: 603.2796068191528 and batch: 850, loss is 6.1056819248199465 and perplexity is 448.39831115925904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.727896372477214 and perplexity of 307.32209669102355
Finished 21 epochs...
Completing Train Step...
At time: 607.3016033172607 and batch: 50, loss is 6.10998426437378 and perplexity is 450.33162886168026
At time: 608.8667137622833 and batch: 100, loss is 6.061319074630737 and perplexity is 428.940868872005
At time: 610.402271270752 and batch: 150, loss is 6.0497625541687015 and perplexity is 424.0123381286917
At time: 611.938488483429 and batch: 200, loss is 6.092388744354248 and perplexity is 442.47711445034827
At time: 613.4784324169159 and batch: 250, loss is 6.120040330886841 and perplexity is 454.88303996598876
At time: 615.0174016952515 and batch: 300, loss is 6.088600664138794 and perplexity is 440.80414631658016
At time: 616.5519108772278 and batch: 350, loss is 6.060125064849854 and perplexity is 428.4290149192749
At time: 618.0874373912811 and batch: 400, loss is 6.0933817195892335 and perplexity is 442.9167014804431
At time: 619.6246452331543 and batch: 450, loss is 6.118668460845948 and perplexity is 454.25942740684707
At time: 621.1608684062958 and batch: 500, loss is 6.108267831802368 and perplexity is 449.559327976771
At time: 622.693234205246 and batch: 550, loss is 6.072218780517578 and perplexity is 433.6417708699024
At time: 624.2283067703247 and batch: 600, loss is 6.0888559532165525 and perplexity is 440.91669316594476
At time: 625.7637686729431 and batch: 650, loss is 6.099047603607178 and perplexity is 445.43333889602627
At time: 627.3012840747833 and batch: 700, loss is 6.065256357192993 and perplexity is 430.6330594053382
At time: 628.8357479572296 and batch: 750, loss is 6.0686097240448 and perplexity is 432.0795539900418
At time: 630.3694381713867 and batch: 800, loss is 6.0954580497741695 and perplexity is 443.8372981977798
At time: 631.946742773056 and batch: 850, loss is 6.10424958229065 and perplexity is 447.7565109367245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.727156321207683 and perplexity of 307.09474671890626
Finished 22 epochs...
Completing Train Step...
At time: 635.9743466377258 and batch: 50, loss is 6.1064207553863525 and perplexity is 448.7297239513431
At time: 637.5077846050262 and batch: 100, loss is 6.0582746982574465 and perplexity is 427.63699717042147
At time: 639.0412385463715 and batch: 150, loss is 6.046839914321899 and perplexity is 422.7749119301217
At time: 640.574738740921 and batch: 200, loss is 6.089571189880371 and perplexity is 441.23216575591954
At time: 642.1102383136749 and batch: 250, loss is 6.117368965148926 and perplexity is 453.6695026209774
At time: 643.6600196361542 and batch: 300, loss is 6.086302604675293 and perplexity is 439.7923152458669
At time: 645.1983630657196 and batch: 350, loss is 6.057610483169555 and perplexity is 427.3530485366832
At time: 646.7321047782898 and batch: 400, loss is 6.092197666168213 and perplexity is 442.39257480305537
At time: 648.268824338913 and batch: 450, loss is 6.11776629447937 and perplexity is 453.8497946359777
At time: 649.805234670639 and batch: 500, loss is 6.10695216178894 and perplexity is 448.9682451699205
At time: 651.3498632907867 and batch: 550, loss is 6.071332597732544 and perplexity is 433.2576552211615
At time: 652.8925905227661 and batch: 600, loss is 6.088037395477295 and perplexity is 440.5559250692889
At time: 654.4329171180725 and batch: 650, loss is 6.098061981201172 and perplexity is 444.99452610421196
At time: 655.9683501720428 and batch: 700, loss is 6.064493455886841 and perplexity is 430.3046541681658
At time: 657.5084517002106 and batch: 750, loss is 6.068298215866089 and perplexity is 431.9449786368701
At time: 659.0409388542175 and batch: 800, loss is 6.094300689697266 and perplexity is 443.32391576967376
At time: 660.5755836963654 and batch: 850, loss is 6.103266620635987 and perplexity is 447.31659969920764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.7267608642578125 and perplexity of 306.97332797657276
Finished 23 epochs...
Completing Train Step...
At time: 664.6044614315033 and batch: 50, loss is 6.1037341976165775 and perplexity is 447.52580354990414
At time: 666.1400520801544 and batch: 100, loss is 6.055462570190429 and perplexity is 426.4361164747251
At time: 667.6770820617676 and batch: 150, loss is 6.044678277969361 and perplexity is 421.8620133444096
At time: 669.2153697013855 and batch: 200, loss is 6.087999782562256 and perplexity is 440.5393547883394
At time: 670.7968156337738 and batch: 250, loss is 6.115830802917481 and perplexity is 452.97222172953053
At time: 672.3318696022034 and batch: 300, loss is 6.085096864700318 and perplexity is 439.26235962922857
At time: 673.8736896514893 and batch: 350, loss is 6.056445045471191 and perplexity is 426.855285295771
At time: 675.4093849658966 and batch: 400, loss is 6.090957269668579 and perplexity is 441.84417279008807
At time: 676.9458727836609 and batch: 450, loss is 6.116512222290039 and perplexity is 453.2809909653696
At time: 678.4831988811493 and batch: 500, loss is 6.106468772888183 and perplexity is 448.7512713490064
At time: 680.0274586677551 and batch: 550, loss is 6.070354948043823 and perplexity is 432.83428799553775
At time: 681.5669133663177 and batch: 600, loss is 6.087299699783325 and perplexity is 440.2310487050928
At time: 683.1142017841339 and batch: 650, loss is 6.097425422668457 and perplexity is 444.7113511798779
At time: 684.6515347957611 and batch: 700, loss is 6.063653221130371 and perplexity is 429.9432490956772
At time: 686.1856637001038 and batch: 750, loss is 6.066935310363769 and perplexity is 431.3566794379585
At time: 687.721197605133 and batch: 800, loss is 6.0938112258911135 and perplexity is 443.1069778544626
At time: 689.25892329216 and batch: 850, loss is 6.102577905654908 and perplexity is 447.0086321188258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.72666613260905 and perplexity of 306.9442492644463
Finished 24 epochs...
Completing Train Step...
At time: 693.288241147995 and batch: 50, loss is 6.101932220458984 and perplexity is 446.72009842360876
At time: 694.8223204612732 and batch: 100, loss is 6.054133653640747 and perplexity is 425.8697948425988
At time: 696.354261636734 and batch: 150, loss is 6.043099279403687 and perplexity is 421.1964194546153
At time: 697.8883757591248 and batch: 200, loss is 6.086343040466309 and perplexity is 439.8100989555634
At time: 699.4227590560913 and batch: 250, loss is 6.113891305923462 and perplexity is 452.0945348777467
At time: 700.9566009044647 and batch: 300, loss is 6.082927188873291 and perplexity is 438.31033587075655
At time: 702.4891157150269 and batch: 350, loss is 6.053979530334472 and perplexity is 425.8041634395695
At time: 704.026435136795 and batch: 400, loss is 6.089808721542358 and perplexity is 441.3369848140032
At time: 705.5641882419586 and batch: 450, loss is 6.115331315994263 and perplexity is 452.74602452417156
At time: 707.0973999500275 and batch: 500, loss is 6.105062446594238 and perplexity is 448.1206241884478
At time: 708.635840177536 and batch: 550, loss is 6.069314565658569 and perplexity is 432.3842089943027
At time: 710.2120251655579 and batch: 600, loss is 6.085832967758178 and perplexity is 439.5858210312128
At time: 711.7555401325226 and batch: 650, loss is 6.0962125682830814 and perplexity is 444.17230802387934
At time: 713.2929425239563 and batch: 700, loss is 6.061809768676758 and perplexity is 429.151399251233
At time: 714.8266994953156 and batch: 750, loss is 6.06501522064209 and perplexity is 430.5292305536571
At time: 716.3650059700012 and batch: 800, loss is 6.091985483169555 and perplexity is 442.2987165778618
At time: 717.9056503772736 and batch: 850, loss is 6.101428375244141 and perplexity is 446.4950773323041
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.725234349568685 and perplexity of 306.50508616217655
Finished 25 epochs...
Completing Train Step...
At time: 721.8915085792542 and batch: 50, loss is 6.100032653808594 and perplexity is 445.8723292745064
At time: 723.4679353237152 and batch: 100, loss is 6.05249605178833 and perplexity is 425.1729604020887
At time: 725.0033676624298 and batch: 150, loss is 6.0408892250061035 and perplexity is 420.26658033154575
At time: 726.5392479896545 and batch: 200, loss is 6.084117422103882 and perplexity is 438.83233798832987
At time: 728.0722420215607 and batch: 250, loss is 6.111896333694458 and perplexity is 451.1935178865493
At time: 729.6140773296356 and batch: 300, loss is 6.081112003326416 and perplexity is 437.51544294137386
At time: 731.1558928489685 and batch: 350, loss is 6.051358652114868 and perplexity is 424.68964372997993
At time: 732.6905422210693 and batch: 400, loss is 6.086706037521362 and perplexity is 439.9697777059718
At time: 734.2277641296387 and batch: 450, loss is 6.113656272888184 and perplexity is 451.98829021297365
At time: 735.7624208927155 and batch: 500, loss is 6.103618783950806 and perplexity is 447.4741559368664
At time: 737.2947688102722 and batch: 550, loss is 6.06697449684143 and perplexity is 431.37358311803695
At time: 738.8309812545776 and batch: 600, loss is 6.082497434616089 and perplexity is 438.12201060763067
At time: 740.3668057918549 and batch: 650, loss is 6.092404003143311 and perplexity is 442.48386616681404
At time: 741.9024860858917 and batch: 700, loss is 6.058591690063476 and perplexity is 427.7725760820465
At time: 743.4347836971283 and batch: 750, loss is 6.061849822998047 and perplexity is 429.1685889635191
At time: 744.9719359874725 and batch: 800, loss is 6.088902235031128 and perplexity is 440.93710006281157
At time: 746.5132129192352 and batch: 850, loss is 6.098825435638428 and perplexity is 445.33438886808057
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.721494038899739 and perplexity of 305.3608032391664
Finished 26 epochs...
Completing Train Step...
At time: 750.5181567668915 and batch: 50, loss is 6.095868377685547 and perplexity is 444.0194543986717
At time: 752.0805342197418 and batch: 100, loss is 6.048767595291138 and perplexity is 423.5906730931447
At time: 753.6141088008881 and batch: 150, loss is 6.036525440216065 and perplexity is 418.436623094243
At time: 755.1490926742554 and batch: 200, loss is 6.0790162277221675 and perplexity is 436.59946892284404
At time: 756.6841382980347 and batch: 250, loss is 6.107273721694947 and perplexity is 449.1126385709573
At time: 758.2189373970032 and batch: 300, loss is 6.075714950561523 and perplexity is 435.16050957641875
At time: 759.7646253108978 and batch: 350, loss is 6.046940584182739 and perplexity is 422.8174747640334
At time: 761.3024888038635 and batch: 400, loss is 6.0829340362548825 and perplexity is 438.3133371591572
At time: 762.8449156284332 and batch: 450, loss is 6.109906272888184 and perplexity is 450.2965081985086
At time: 764.381297826767 and batch: 500, loss is 6.099833173751831 and perplexity is 445.7833955075052
At time: 765.9164931774139 and batch: 550, loss is 6.062929344177246 and perplexity is 429.6321357039974
At time: 767.4535756111145 and batch: 600, loss is 6.080126991271973 and perplexity is 437.0846971358047
At time: 768.9884119033813 and batch: 650, loss is 6.089592695236206 and perplexity is 441.24165471268117
At time: 770.5246829986572 and batch: 700, loss is 6.056212606430054 and perplexity is 426.7560789927082
At time: 772.0626475811005 and batch: 750, loss is 6.059580001831055 and perplexity is 428.19555773728007
At time: 773.6003963947296 and batch: 800, loss is 6.086405429840088 and perplexity is 439.8375392882027
At time: 775.1405293941498 and batch: 850, loss is 6.09751404762268 and perplexity is 444.75076544953697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.720268249511719 and perplexity of 304.9867245247372
Finished 27 epochs...
Completing Train Step...
At time: 779.1898839473724 and batch: 50, loss is 6.093400373458862 and perplexity is 442.9249636679095
At time: 780.7294840812683 and batch: 100, loss is 6.046282281875611 and perplexity is 422.53922464129954
At time: 782.2723815441132 and batch: 150, loss is 6.034760046005249 and perplexity is 417.6985691719858
At time: 783.8089809417725 and batch: 200, loss is 6.076365051269531 and perplexity is 435.44349970786703
At time: 785.3475770950317 and batch: 250, loss is 6.104190444946289 and perplexity is 447.7300325886847
At time: 786.8872361183167 and batch: 300, loss is 6.072863826751709 and perplexity is 433.9215800964057
At time: 788.466281414032 and batch: 350, loss is 6.044616098403931 and perplexity is 421.8357829632538
At time: 790.0035152435303 and batch: 400, loss is 6.07950611114502 and perplexity is 436.8134041624807
At time: 791.5394606590271 and batch: 450, loss is 6.106825370788574 and perplexity is 448.9113236456274
At time: 793.0816972255707 and batch: 500, loss is 6.0962590312957765 and perplexity is 444.1929460869154
At time: 794.6189861297607 and batch: 550, loss is 6.058036441802979 and perplexity is 427.5351220323629
At time: 796.1544575691223 and batch: 600, loss is 6.077061853408813 and perplexity is 435.74702340570025
At time: 797.692263841629 and batch: 650, loss is 6.086053037643433 and perplexity is 439.682571277924
At time: 799.2313520908356 and batch: 700, loss is 6.05243670463562 and perplexity is 425.1477283462125
At time: 800.7699766159058 and batch: 750, loss is 6.056101350784302 and perplexity is 426.70860261061796
At time: 802.3077981472015 and batch: 800, loss is 6.083295850753784 and perplexity is 438.47195397280456
At time: 803.8482336997986 and batch: 850, loss is 6.094149885177612 and perplexity is 443.2570655602885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.718380610148112 and perplexity of 304.4115625980774
Finished 28 epochs...
Completing Train Step...
At time: 807.8761138916016 and batch: 50, loss is 6.088669090270996 and perplexity is 440.8343098713472
At time: 809.4146451950073 and batch: 100, loss is 6.041396551132202 and perplexity is 420.479846640885
At time: 810.9534437656403 and batch: 150, loss is 6.029216270446778 and perplexity is 415.38934886713315
At time: 812.4973738193512 and batch: 200, loss is 6.071661605834961 and perplexity is 433.4002239520109
At time: 814.0315551757812 and batch: 250, loss is 6.1013583564758305 and perplexity is 446.4638153914067
At time: 815.5780439376831 and batch: 300, loss is 6.069195938110352 and perplexity is 432.3329193579438
At time: 817.1287429332733 and batch: 350, loss is 6.039850549697876 and perplexity is 419.8302864347555
At time: 818.6780850887299 and batch: 400, loss is 6.074350824356079 and perplexity is 434.5673004197981
At time: 820.2343502044678 and batch: 450, loss is 6.099606761932373 and perplexity is 445.6824763029256
At time: 821.8135187625885 and batch: 500, loss is 6.088523683547973 and perplexity is 440.77021425903206
At time: 823.3870799541473 and batch: 550, loss is 6.0505022621154785 and perplexity is 424.326099456304
At time: 824.9340393543243 and batch: 600, loss is 6.068904066085816 and perplexity is 432.2067518867705
At time: 826.547660112381 and batch: 650, loss is 6.078091096878052 and perplexity is 436.1957440655726
At time: 828.0976710319519 and batch: 700, loss is 6.04391227722168 and perplexity is 421.5389904604497
At time: 829.643988609314 and batch: 750, loss is 6.042808456420898 and perplexity is 421.07394366582014
At time: 831.1913878917694 and batch: 800, loss is 6.070252809524536 and perplexity is 432.7900811999121
At time: 832.7384421825409 and batch: 850, loss is 6.086219711303711 and perplexity is 439.7558608889935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.705230077107747 and perplexity of 300.43459522012375
Finished 29 epochs...
Completing Train Step...
At time: 837.0048975944519 and batch: 50, loss is 6.077757062911988 and perplexity is 436.0500642035677
At time: 838.5503933429718 and batch: 100, loss is 6.02681869506836 and perplexity is 414.39461454360145
At time: 840.096312046051 and batch: 150, loss is 6.012129421234131 and perplexity is 408.3519484180261
At time: 841.641693353653 and batch: 200, loss is 6.049791660308838 and perplexity is 424.02467967083123
At time: 843.1872770786285 and batch: 250, loss is 6.081225061416626 and perplexity is 437.5649103980859
At time: 844.7361037731171 and batch: 300, loss is 6.049229917526245 and perplexity is 423.7865537564035
At time: 846.28600025177 and batch: 350, loss is 6.0202224159240725 and perplexity is 411.67014754127604
At time: 847.8297929763794 and batch: 400, loss is 6.049315185546875 and perplexity is 423.82269073765445
At time: 849.3742527961731 and batch: 450, loss is 6.074213342666626 and perplexity is 434.507559479892
At time: 850.9235625267029 and batch: 500, loss is 6.065404348373413 and perplexity is 430.69679401609113
At time: 852.4760293960571 and batch: 550, loss is 6.027026739120483 and perplexity is 414.48083584699293
At time: 854.016036272049 and batch: 600, loss is 6.042424182891846 and perplexity is 420.9121671806969
At time: 855.5597305297852 and batch: 650, loss is 6.055977153778076 and perplexity is 426.6556099704746
At time: 857.1035494804382 and batch: 700, loss is 6.019464197158814 and perplexity is 411.3581298141095
At time: 858.6424386501312 and batch: 750, loss is 6.020140714645386 and perplexity is 411.63651493775694
At time: 860.1820633411407 and batch: 800, loss is 6.048348588943481 and perplexity is 423.4132230912453
At time: 861.719498872757 and batch: 850, loss is 6.057333574295044 and perplexity is 427.23472706788243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.683959325154622 and perplexity of 294.11161117799406
Finished 30 epochs...
Completing Train Step...
At time: 865.815934419632 and batch: 50, loss is 6.055936479568482 and perplexity is 426.63825644369297
At time: 867.3951890468597 and batch: 100, loss is 6.006109189987183 and perplexity is 405.9009604183083
At time: 868.9330823421478 and batch: 150, loss is 5.9953102684021 and perplexity is 401.5412502275213
At time: 870.472650051117 and batch: 200, loss is 6.032810583114624 and perplexity is 416.88507450838114
At time: 872.0116472244263 and batch: 250, loss is 6.0640991592407225 and perplexity is 430.13501993152124
At time: 873.5498468875885 and batch: 300, loss is 6.0312160396575925 and perplexity is 416.2208628384097
At time: 875.0911376476288 and batch: 350, loss is 6.004349346160889 and perplexity is 405.1872662983203
At time: 876.6308686733246 and batch: 400, loss is 6.035759696960449 and perplexity is 418.11633071871984
At time: 878.1710722446442 and batch: 450, loss is 6.063383045196534 and perplexity is 429.82710446730374
At time: 879.7140779495239 and batch: 500, loss is 6.0554233837127684 and perplexity is 426.4194062727821
At time: 881.2579300403595 and batch: 550, loss is 6.018198366165161 and perplexity is 410.837749370232
At time: 882.803689956665 and batch: 600, loss is 6.031479873657227 and perplexity is 416.3306905408875
At time: 884.3496990203857 and batch: 650, loss is 6.045955963134766 and perplexity is 422.40136466792694
At time: 885.891585111618 and batch: 700, loss is 6.011140413284302 and perplexity is 407.9482847409086
At time: 887.4341382980347 and batch: 750, loss is 6.012015972137451 and perplexity is 408.3056238861385
At time: 888.9741077423096 and batch: 800, loss is 6.040421571731567 and perplexity is 420.07008723797594
At time: 890.518581867218 and batch: 850, loss is 6.047710857391357 and perplexity is 423.1432852023966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.674383163452148 and perplexity of 291.3085933272418
Finished 31 epochs...
Completing Train Step...
At time: 894.626740694046 and batch: 50, loss is 6.04569031715393 and perplexity is 422.2891703457485
At time: 896.1882781982422 and batch: 100, loss is 5.994799547195434 and perplexity is 401.33622695500225
At time: 897.723014831543 and batch: 150, loss is 5.9863197803497314 and perplexity is 397.9473779550368
At time: 899.2620887756348 and batch: 200, loss is 6.022524013519287 and perplexity is 412.61873778063415
At time: 900.8009212017059 and batch: 250, loss is 6.053483343124389 and perplexity is 425.5929372678668
At time: 902.3402147293091 and batch: 300, loss is 6.020592584609985 and perplexity is 411.82256314682263
At time: 903.8785049915314 and batch: 350, loss is 5.99621732711792 and perplexity is 401.90563695339597
At time: 905.4806020259857 and batch: 400, loss is 6.028198070526123 and perplexity is 414.9666147155375
At time: 907.0160174369812 and batch: 450, loss is 6.055296020507813 and perplexity is 426.3650995889539
At time: 908.5540943145752 and batch: 500, loss is 6.04661750793457 and perplexity is 422.680894544727
At time: 910.1007590293884 and batch: 550, loss is 6.007825078964234 and perplexity is 406.59803928600445
At time: 911.6483352184296 and batch: 600, loss is 6.023384475708008 and perplexity is 412.9739333971357
At time: 913.2036120891571 and batch: 650, loss is 6.0374797916412355 and perplexity is 418.83614929571297
At time: 914.7605087757111 and batch: 700, loss is 6.001209182739258 and perplexity is 403.91690767649544
At time: 916.3003396987915 and batch: 750, loss is 6.00089464187622 and perplexity is 403.7898792826171
At time: 917.8528916835785 and batch: 800, loss is 6.029673986434936 and perplexity is 415.57952273290965
At time: 919.3894469738007 and batch: 850, loss is 6.037574844360352 and perplexity is 418.87596270272394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.669771194458008 and perplexity of 289.96818047425825
Finished 32 epochs...
Completing Train Step...
At time: 923.390864610672 and batch: 50, loss is 6.036055307388306 and perplexity is 418.2399485365954
At time: 924.9521231651306 and batch: 100, loss is 5.982868385314942 and perplexity is 396.5762718261635
At time: 926.4912438392639 and batch: 150, loss is 5.9741370964050295 and perplexity is 393.1287224987204
At time: 928.0280032157898 and batch: 200, loss is 6.009451389312744 and perplexity is 407.25983187918695
At time: 929.5634407997131 and batch: 250, loss is 6.042579393386841 and perplexity is 420.9775022367265
At time: 931.0980620384216 and batch: 300, loss is 6.0087353515625 and perplexity is 406.9683228435952
At time: 932.6329593658447 and batch: 350, loss is 5.984147644042968 and perplexity is 397.0839201207506
At time: 934.172164440155 and batch: 400, loss is 6.015156936645508 and perplexity is 409.5901135714332
At time: 935.7070662975311 and batch: 450, loss is 6.042211103439331 and perplexity is 420.82248900118503
At time: 937.2430334091187 and batch: 500, loss is 6.034598417282105 and perplexity is 417.6310625412437
At time: 938.7930884361267 and batch: 550, loss is 5.996921520233155 and perplexity is 402.1887558093876
At time: 940.3449859619141 and batch: 600, loss is 6.011961269378662 and perplexity is 408.2832890529768
At time: 941.9042956829071 and batch: 650, loss is 6.027333993911743 and perplexity is 414.6082066363328
At time: 943.4530363082886 and batch: 700, loss is 5.991168994903564 and perplexity is 399.88179658673295
At time: 945.0161125659943 and batch: 750, loss is 5.99267011642456 and perplexity is 400.48251852295334
At time: 946.5514767169952 and batch: 800, loss is 6.021591501235962 and perplexity is 412.2341450858992
At time: 948.0898401737213 and batch: 850, loss is 6.0280680847167964 and perplexity is 414.9126784498306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.661113739013672 and perplexity of 287.46862935980914
Finished 33 epochs...
Completing Train Step...
At time: 952.114250421524 and batch: 50, loss is 6.029603185653687 and perplexity is 415.5501004196026
At time: 953.6511039733887 and batch: 100, loss is 5.9753732490539555 and perplexity is 393.6149900989761
At time: 955.186066865921 and batch: 150, loss is 5.96896071434021 and perplexity is 391.09899588283486
At time: 956.7230162620544 and batch: 200, loss is 6.003944969177246 and perplexity is 405.02345101756083
At time: 958.2598187923431 and batch: 250, loss is 6.035684995651245 and perplexity is 418.0850980479907
At time: 959.7977085113525 and batch: 300, loss is 6.001763315200805 and perplexity is 404.14079317218847
At time: 961.3340067863464 and batch: 350, loss is 5.978270626068115 and perplexity is 394.7570948788082
At time: 962.8696427345276 and batch: 400, loss is 6.009308500289917 and perplexity is 407.2016430771425
At time: 964.4172110557556 and batch: 450, loss is 6.036849403381348 and perplexity is 418.5722031074104
At time: 965.9513945579529 and batch: 500, loss is 6.028476724624634 and perplexity is 415.08226297565545
At time: 967.4884748458862 and batch: 550, loss is 5.991594591140747 and perplexity is 400.05202099554276
At time: 969.0227224826813 and batch: 600, loss is 6.0058519077301025 and perplexity is 405.7965427360453
At time: 970.5605087280273 and batch: 650, loss is 6.022402439117432 and perplexity is 412.56857695359236
At time: 972.0978608131409 and batch: 700, loss is 5.987616214752197 and perplexity is 398.46362519422286
At time: 973.6363549232483 and batch: 750, loss is 5.988481693267822 and perplexity is 398.808636179348
At time: 975.1749687194824 and batch: 800, loss is 6.0172239017486575 and perplexity is 410.4375976010091
At time: 976.7134826183319 and batch: 850, loss is 6.023965845108032 and perplexity is 413.21409360915516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.658425649007161 and perplexity of 286.6969254794693
Finished 34 epochs...
Completing Train Step...
At time: 980.7245922088623 and batch: 50, loss is 6.025459060668945 and perplexity is 413.83157222332625
At time: 982.2814371585846 and batch: 100, loss is 5.972439079284668 and perplexity is 392.4617496233472
At time: 983.8476629257202 and batch: 150, loss is 5.964635200500489 and perplexity is 389.4109452386156
At time: 985.3842792510986 and batch: 200, loss is 5.999317836761475 and perplexity is 403.15368304623684
At time: 986.934175491333 and batch: 250, loss is 6.031389741897583 and perplexity is 416.2931676141846
At time: 988.4795925617218 and batch: 300, loss is 5.997873077392578 and perplexity is 402.5716435403075
At time: 990.0206878185272 and batch: 350, loss is 5.975030164718628 and perplexity is 393.47997012466664
At time: 991.5580065250397 and batch: 400, loss is 6.006027307510376 and perplexity is 405.867725603024
At time: 993.0979249477386 and batch: 450, loss is 6.032575626373291 and perplexity is 416.78713605586466
At time: 994.6338813304901 and batch: 500, loss is 6.024076538085938 and perplexity is 413.2598360393255
At time: 996.1708657741547 and batch: 550, loss is 5.986766262054443 and perplexity is 398.12509384922726
At time: 997.7069263458252 and batch: 600, loss is 6.001222229003906 and perplexity is 403.9221773177433
At time: 999.2446129322052 and batch: 650, loss is 6.018858299255371 and perplexity is 411.1089642777518
At time: 1000.7813878059387 and batch: 700, loss is 5.984019994735718 and perplexity is 397.0332358684005
At time: 1002.3189749717712 and batch: 750, loss is 5.984826154708863 and perplexity is 397.3534372205811
At time: 1003.8583536148071 and batch: 800, loss is 6.012913599014282 and perplexity is 408.6722945301924
At time: 1005.3971228599548 and batch: 850, loss is 6.020012874603271 and perplexity is 411.58389467191137
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.656391779581706 and perplexity of 286.11441394536683
Finished 35 epochs...
Completing Train Step...
At time: 1009.4233009815216 and batch: 50, loss is 6.022436828613281 and perplexity is 412.5827652229193
At time: 1010.9607856273651 and batch: 100, loss is 5.968977546691894 and perplexity is 391.10557905408183
At time: 1012.5013275146484 and batch: 150, loss is 5.961708116531372 and perplexity is 388.27277327849606
At time: 1014.0382370948792 and batch: 200, loss is 5.9958608436584475 and perplexity is 401.76238977569506
At time: 1015.5719530582428 and batch: 250, loss is 6.028668642044067 and perplexity is 415.16193213712387
At time: 1017.1098012924194 and batch: 300, loss is 5.994015054702759 and perplexity is 401.02150516249986
At time: 1018.6457772254944 and batch: 350, loss is 5.971025180816651 and perplexity is 391.9072406588639
At time: 1020.1801662445068 and batch: 400, loss is 6.003115167617798 and perplexity is 404.68750133135745
At time: 1021.7185173034668 and batch: 450, loss is 6.028827886581421 and perplexity is 415.2280496712224
At time: 1023.2794318199158 and batch: 500, loss is 6.020160398483276 and perplexity is 411.6446176039322
At time: 1024.8177347183228 and batch: 550, loss is 5.983624191284179 and perplexity is 396.8761198388529
At time: 1026.3527579307556 and batch: 600, loss is 5.997912588119507 and perplexity is 402.58754975281585
At time: 1027.8884601593018 and batch: 650, loss is 6.0151222133636475 and perplexity is 409.57589150539224
At time: 1029.427832365036 and batch: 700, loss is 5.980973272323609 and perplexity is 395.825426674291
At time: 1030.963100194931 and batch: 750, loss is 5.98199031829834 and perplexity is 396.2282041180566
At time: 1032.5047459602356 and batch: 800, loss is 6.009477252960205 and perplexity is 407.2703652401187
At time: 1034.0528223514557 and batch: 850, loss is 6.016743555068969 and perplexity is 410.2404926069369
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.653587977091472 and perplexity of 285.31332920554684
Finished 36 epochs...
Completing Train Step...
At time: 1038.1971518993378 and batch: 50, loss is 6.019355983734131 and perplexity is 411.31361775055586
At time: 1039.7919948101044 and batch: 100, loss is 5.966270494461059 and perplexity is 390.04826756819625
At time: 1041.3419528007507 and batch: 150, loss is 5.958462352752686 and perplexity is 387.01457458719955
At time: 1042.891253709793 and batch: 200, loss is 5.993001537322998 and perplexity is 400.61526879594317
At time: 1044.4388501644135 and batch: 250, loss is 6.025545930862426 and perplexity is 413.86752341359477
At time: 1045.9923191070557 and batch: 300, loss is 5.991197633743286 and perplexity is 399.8932489014028
At time: 1047.5498843193054 and batch: 350, loss is 5.968787050247192 and perplexity is 391.031081927713
At time: 1049.09925365448 and batch: 400, loss is 6.000241355895996 and perplexity is 403.5261751620065
At time: 1050.6446306705475 and batch: 450, loss is 6.02638536453247 and perplexity is 414.21508360414
At time: 1052.1916296482086 and batch: 500, loss is 6.018402719497681 and perplexity is 410.9217140123758
At time: 1053.74200963974 and batch: 550, loss is 5.980741786956787 and perplexity is 395.73380948462847
At time: 1055.2937915325165 and batch: 600, loss is 5.994655141830444 and perplexity is 401.27827603497764
At time: 1056.8495354652405 and batch: 650, loss is 6.0134926033020015 and perplexity is 408.9089860570896
At time: 1058.4000461101532 and batch: 700, loss is 5.97955132484436 and perplexity is 395.262983683574
At time: 1059.9507763385773 and batch: 750, loss is 5.979868412017822 and perplexity is 395.3883363786601
At time: 1061.5635211467743 and batch: 800, loss is 6.007617101669312 and perplexity is 406.51348491867225
At time: 1063.1164543628693 and batch: 850, loss is 6.014129104614258 and perplexity is 409.1693400123357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.651924769083659 and perplexity of 284.8391881985362
Finished 37 epochs...
Completing Train Step...
At time: 1067.1928565502167 and batch: 50, loss is 6.016634855270386 and perplexity is 410.1959019715602
At time: 1068.7585413455963 and batch: 100, loss is 5.96446231842041 and perplexity is 389.3436288834609
At time: 1070.2964570522308 and batch: 150, loss is 5.956479272842407 and perplexity is 386.2478542444723
At time: 1071.8321461677551 and batch: 200, loss is 5.990639429092408 and perplexity is 399.6700889202755
At time: 1073.3688523769379 and batch: 250, loss is 6.024709434509277 and perplexity is 413.5214694961709
At time: 1074.9073686599731 and batch: 300, loss is 5.989943208694458 and perplexity is 399.39192729405397
At time: 1076.4475688934326 and batch: 350, loss is 5.967416038513184 and perplexity is 390.495341063452
At time: 1077.9858601093292 and batch: 400, loss is 5.999150218963623 and perplexity is 403.0861129768203
At time: 1079.522751569748 and batch: 450, loss is 6.025300312042236 and perplexity is 413.76588224378196
At time: 1081.0619192123413 and batch: 500, loss is 6.016365261077881 and perplexity is 410.08533044398973
At time: 1082.6009631156921 and batch: 550, loss is 5.9779897308349605 and perplexity is 394.64622506473296
At time: 1084.1375761032104 and batch: 600, loss is 5.992077884674072 and perplexity is 400.2454102784141
At time: 1085.6764025688171 and batch: 650, loss is 6.0108214950561525 and perplexity is 407.81820334052975
At time: 1087.2139067649841 and batch: 700, loss is 5.976379747390747 and perplexity is 394.01136237240223
At time: 1088.7551972866058 and batch: 750, loss is 5.976249876022339 and perplexity is 393.9601949002694
At time: 1090.292690038681 and batch: 800, loss is 6.003796148300171 and perplexity is 404.9631795572815
At time: 1091.8303852081299 and batch: 850, loss is 6.010655479431152 and perplexity is 407.75050476628184
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.648286819458008 and perplexity of 283.804840174234
Finished 38 epochs...
Completing Train Step...
At time: 1095.8644807338715 and batch: 50, loss is 6.0123654556274415 and perplexity is 408.448344898423
At time: 1097.4604959487915 and batch: 100, loss is 5.960627784729004 and perplexity is 387.85353635179695
At time: 1099.0087487697601 and batch: 150, loss is 5.952401266098023 and perplexity is 384.6759402063124
At time: 1100.6053783893585 and batch: 200, loss is 5.987237377166748 and perplexity is 398.312700786289
At time: 1102.1551282405853 and batch: 250, loss is 6.020494871139526 and perplexity is 411.78232450092514
At time: 1103.704738855362 and batch: 300, loss is 5.986882638931275 and perplexity is 398.1714291003612
At time: 1105.2528471946716 and batch: 350, loss is 5.963242778778076 and perplexity is 388.86909830678536
At time: 1106.809547662735 and batch: 400, loss is 5.9940093231201175 and perplexity is 401.019206681189
At time: 1108.358516216278 and batch: 450, loss is 6.019499816894531 and perplexity is 411.3727825429403
At time: 1109.9076447486877 and batch: 500, loss is 6.010367155075073 and perplexity is 407.6329573112656
At time: 1111.4535942077637 and batch: 550, loss is 5.974084854125977 and perplexity is 393.1081850947609
At time: 1113.003945350647 and batch: 600, loss is 5.988534173965454 and perplexity is 398.82956648401006
At time: 1114.5539815425873 and batch: 650, loss is 6.0078978347778325 and perplexity is 406.6276227333312
At time: 1116.1013565063477 and batch: 700, loss is 5.973678760528564 and perplexity is 392.9485787874467
At time: 1117.6487045288086 and batch: 750, loss is 5.973330659866333 and perplexity is 392.81181693177524
At time: 1119.1954488754272 and batch: 800, loss is 6.001288757324219 and perplexity is 403.94905047564066
At time: 1120.745391368866 and batch: 850, loss is 6.0078271389007565 and perplexity is 406.5988768530183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.648206075032552 and perplexity of 283.78192544060346
Finished 39 epochs...
Completing Train Step...
At time: 1124.9307506084442 and batch: 50, loss is 6.009803886413574 and perplexity is 407.4034150940557
At time: 1126.4834215641022 and batch: 100, loss is 5.95773341178894 and perplexity is 386.73256660628823
At time: 1128.0323781967163 and batch: 150, loss is 5.949731378555298 and perplexity is 383.6502685288615
At time: 1129.5800874233246 and batch: 200, loss is 5.9841831684112545 and perplexity is 397.0980265267286
At time: 1131.1262512207031 and batch: 250, loss is 6.01630482673645 and perplexity is 410.06054795597817
At time: 1132.6841094493866 and batch: 300, loss is 5.9826399517059325 and perplexity is 396.48569082340947
At time: 1134.2326681613922 and batch: 350, loss is 5.958097343444824 and perplexity is 386.8733364433838
At time: 1135.781018257141 and batch: 400, loss is 5.989791393280029 and perplexity is 399.3312980454359
At time: 1137.3302917480469 and batch: 450, loss is 6.014936208724976 and perplexity is 409.49971557444195
At time: 1138.8639068603516 and batch: 500, loss is 6.003080682754517 and perplexity is 404.673545978828
At time: 1140.4342331886292 and batch: 550, loss is 5.965561275482178 and perplexity is 389.7717360064639
At time: 1141.9709236621857 and batch: 600, loss is 5.978982944488525 and perplexity is 395.0383878022494
At time: 1143.5088613033295 and batch: 650, loss is 5.998352670669556 and perplexity is 402.7647604991475
At time: 1145.0429468154907 and batch: 700, loss is 5.963508625030517 and perplexity is 388.9724914419902
At time: 1146.5842168331146 and batch: 750, loss is 5.962443265914917 and perplexity is 388.5583167140994
At time: 1148.1326932907104 and batch: 800, loss is 5.990001983642578 and perplexity is 399.41540222372237
At time: 1149.6657519340515 and batch: 850, loss is 5.994930362701416 and perplexity is 401.3887313907223
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.635251363118489 and perplexity of 280.1293226414974
Finished 40 epochs...
Completing Train Step...
At time: 1153.7211453914642 and batch: 50, loss is 5.995361557006836 and perplexity is 401.56184524613
At time: 1155.2687773704529 and batch: 100, loss is 5.941837091445922 and perplexity is 380.6335462621269
At time: 1156.8020436763763 and batch: 150, loss is 5.933057260513306 and perplexity is 377.3062758879844
At time: 1158.3407776355743 and batch: 200, loss is 5.965230731964112 and perplexity is 389.64292077629415
At time: 1159.8795340061188 and batch: 250, loss is 5.999825887680053 and perplexity is 403.3585576842041
At time: 1161.4173645973206 and batch: 300, loss is 5.9613519954681395 and perplexity is 388.1345257835355
At time: 1162.9562590122223 and batch: 350, loss is 5.933428211212158 and perplexity is 377.4462638774224
At time: 1164.4981632232666 and batch: 400, loss is 5.960831613540649 and perplexity is 387.9326001346686
At time: 1166.033554315567 and batch: 450, loss is 5.9820834827423095 and perplexity is 396.2651202179855
At time: 1167.5802438259125 and batch: 500, loss is 5.969134998321533 and perplexity is 391.1671641130716
At time: 1169.1268391609192 and batch: 550, loss is 5.931731662750244 and perplexity is 376.8064508893196
At time: 1170.6766130924225 and batch: 600, loss is 5.942961597442627 and perplexity is 381.06181171585223
At time: 1172.2175250053406 and batch: 650, loss is 5.964349575042725 and perplexity is 389.29973544205427
At time: 1173.754379272461 and batch: 700, loss is 5.92584566116333 and perplexity is 374.595081966326
At time: 1175.2942180633545 and batch: 750, loss is 5.927227592468261 and perplexity is 375.11310449009926
At time: 1176.8327770233154 and batch: 800, loss is 5.956018228530883 and perplexity is 386.06981791290724
At time: 1178.3694415092468 and batch: 850, loss is 5.959717292785644 and perplexity is 387.5005595473979
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.6047007242838545 and perplexity of 271.700599873929
Finished 41 epochs...
Completing Train Step...
At time: 1182.4106624126434 and batch: 50, loss is 5.960714750289917 and perplexity is 387.88726771885007
At time: 1183.960181236267 and batch: 100, loss is 5.907414388656616 and perplexity is 367.75405600149895
At time: 1185.5032269954681 and batch: 150, loss is 5.896250228881836 and perplexity is 363.6712240629262
At time: 1187.0386183261871 and batch: 200, loss is 5.92544545173645 and perplexity is 374.4451954782535
At time: 1188.5784394741058 and batch: 250, loss is 5.963658170700073 and perplexity is 389.03066494335104
At time: 1190.123411655426 and batch: 300, loss is 5.92659779548645 and perplexity is 374.8769337666643
At time: 1191.6742708683014 and batch: 350, loss is 5.905139465332031 and perplexity is 366.9183946149782
At time: 1193.2240326404572 and batch: 400, loss is 5.933557376861573 and perplexity is 377.4950201179631
At time: 1194.780944108963 and batch: 450, loss is 5.957183809280395 and perplexity is 386.52007581563004
At time: 1196.3321146965027 and batch: 500, loss is 5.949036359786987 and perplexity is 383.3837170316429
At time: 1197.8827064037323 and batch: 550, loss is 5.914703025817871 and perplexity is 370.4442739820094
At time: 1199.4282627105713 and batch: 600, loss is 5.92687572479248 and perplexity is 374.9811375326821
At time: 1200.9778685569763 and batch: 650, loss is 5.949479579925537 and perplexity is 383.55367807812036
At time: 1202.5309345722198 and batch: 700, loss is 5.912385005950927 and perplexity is 369.58657126534405
At time: 1204.0844507217407 and batch: 750, loss is 5.91188681602478 and perplexity is 369.4024928155223
At time: 1205.6332359313965 and batch: 800, loss is 5.94056544303894 and perplexity is 380.1498218482543
At time: 1207.1822752952576 and batch: 850, loss is 5.9442249584198 and perplexity is 381.5435345695198
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.588033676147461 and perplexity of 267.2096820297947
Finished 42 epochs...
Completing Train Step...
At time: 1211.3534059524536 and batch: 50, loss is 5.944414024353027 and perplexity is 381.6156782736936
At time: 1212.945595741272 and batch: 100, loss is 5.892374391555786 and perplexity is 362.2644215881591
At time: 1214.4860055446625 and batch: 150, loss is 5.882299633026123 and perplexity is 358.63301848724876
At time: 1216.0215346813202 and batch: 200, loss is 5.913461360931397 and perplexity is 369.9845917792945
At time: 1217.5584411621094 and batch: 250, loss is 5.9535235404968265 and perplexity is 385.1078945061306
At time: 1219.1360037326813 and batch: 300, loss is 5.913865718841553 and perplexity is 370.1342282269181
At time: 1220.6682031154633 and batch: 350, loss is 5.893771829605103 and perplexity is 362.77101756060244
At time: 1222.2062106132507 and batch: 400, loss is 5.922944993972778 and perplexity is 373.51008067690003
At time: 1223.7443413734436 and batch: 450, loss is 5.9454474449157715 and perplexity is 382.01025160767006
At time: 1225.2892463207245 and batch: 500, loss is 5.9370029926300045 and perplexity is 378.7979663487482
At time: 1226.8250777721405 and batch: 550, loss is 5.9035690402984615 and perplexity is 366.34262899949687
At time: 1228.3597502708435 and batch: 600, loss is 5.916349649429321 and perplexity is 371.0547587516554
At time: 1229.8977823257446 and batch: 650, loss is 5.93859938621521 and perplexity is 379.403159927363
At time: 1231.4362397193909 and batch: 700, loss is 5.901395082473755 and perplexity is 365.5470806322783
At time: 1232.974998474121 and batch: 750, loss is 5.902136678695679 and perplexity is 365.81826951010544
At time: 1234.5141429901123 and batch: 800, loss is 5.931122817993164 and perplexity is 376.5771040826469
At time: 1236.0516333580017 and batch: 850, loss is 5.933900423049927 and perplexity is 377.6245405602142
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.585041681925456 and perplexity of 266.41138704761477
Finished 43 epochs...
Completing Train Step...
At time: 1240.0587599277496 and batch: 50, loss is 5.939098138809204 and perplexity is 379.5924354344477
At time: 1241.6242849826813 and batch: 100, loss is 5.884676942825317 and perplexity is 359.4866145054871
At time: 1243.1570494174957 and batch: 150, loss is 5.874260082244873 and perplexity is 355.76132913693834
At time: 1244.6924068927765 and batch: 200, loss is 5.904784908294678 and perplexity is 366.78832417609055
At time: 1246.22953748703 and batch: 250, loss is 5.944310598373413 and perplexity is 381.5762113393203
At time: 1247.7651851177216 and batch: 300, loss is 5.906801633834839 and perplexity is 367.52878195638294
At time: 1249.3005414009094 and batch: 350, loss is 5.887476987838745 and perplexity is 360.4946037568918
At time: 1250.8363871574402 and batch: 400, loss is 5.91507194519043 and perplexity is 370.5809632632467
At time: 1252.3732664585114 and batch: 450, loss is 5.93796446800232 and perplexity is 379.1623464076315
At time: 1253.9206037521362 and batch: 500, loss is 5.93025686264038 and perplexity is 376.2511462764533
At time: 1255.4705274105072 and batch: 550, loss is 5.897034482955933 and perplexity is 363.95654657007447
At time: 1257.1572558879852 and batch: 600, loss is 5.908115816116333 and perplexity is 368.01209928354274
At time: 1258.703652381897 and batch: 650, loss is 5.930565977096558 and perplexity is 376.3674689225003
At time: 1260.2577788829803 and batch: 700, loss is 5.894236497879028 and perplexity is 362.9396249133724
At time: 1261.805725812912 and batch: 750, loss is 5.895527458190918 and perplexity is 363.40846812862566
At time: 1263.3487021923065 and batch: 800, loss is 5.926685085296631 and perplexity is 374.909658131285
At time: 1264.8971538543701 and batch: 850, loss is 5.930151681900025 and perplexity is 376.21157398348635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.581512451171875 and perplexity of 265.4728169767155
Finished 44 epochs...
Completing Train Step...
At time: 1269.0156195163727 and batch: 50, loss is 5.932917947769165 and perplexity is 377.2537159765267
At time: 1270.5915973186493 and batch: 100, loss is 5.878563480377197 and perplexity is 357.29561072074125
At time: 1272.1421964168549 and batch: 150, loss is 5.866663932800293 and perplexity is 353.06915095333005
At time: 1273.7015988826752 and batch: 200, loss is 5.9006884574890135 and perplexity is 365.2888671727894
At time: 1275.2542505264282 and batch: 250, loss is 5.940215721130371 and perplexity is 380.01689837149604
At time: 1276.79358792305 and batch: 300, loss is 5.901746349334717 and perplexity is 365.67550776260276
At time: 1278.3338856697083 and batch: 350, loss is 5.8814834785461425 and perplexity is 358.3404379542388
At time: 1279.8732116222382 and batch: 400, loss is 5.908902940750122 and perplexity is 368.3018847060816
At time: 1281.4110910892487 and batch: 450, loss is 5.933136053085327 and perplexity is 377.33600599114175
At time: 1282.949464559555 and batch: 500, loss is 5.925553216934204 and perplexity is 374.48554981314953
At time: 1284.4881663322449 and batch: 550, loss is 5.891947584152222 and perplexity is 362.1098374421529
At time: 1286.025202035904 and batch: 600, loss is 5.903254871368408 and perplexity is 366.2275536052144
At time: 1287.5654964447021 and batch: 650, loss is 5.926785907745361 and perplexity is 374.94745934664394
At time: 1289.1034207344055 and batch: 700, loss is 5.889970741271973 and perplexity is 361.3947102680561
At time: 1290.6415050029755 and batch: 750, loss is 5.89276593208313 and perplexity is 362.4062905627396
At time: 1292.1792576313019 and batch: 800, loss is 5.924066133499146 and perplexity is 373.9290724220593
At time: 1293.717514038086 and batch: 850, loss is 5.926479787826538 and perplexity is 374.8326980270848
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.579827626546224 and perplexity of 265.0259184157975
Finished 45 epochs...
Completing Train Step...
At time: 1297.8015196323395 and batch: 50, loss is 5.929831781387329 and perplexity is 376.09124295609183
At time: 1299.3429918289185 and batch: 100, loss is 5.872847175598144 and perplexity is 355.25902652726217
At time: 1300.879016160965 and batch: 150, loss is 5.862585840225219 and perplexity is 351.632234201535
At time: 1302.417163848877 and batch: 200, loss is 5.8960600852966305 and perplexity is 363.6020808863208
At time: 1303.976646900177 and batch: 250, loss is 5.935403070449829 and perplexity is 378.19240363636317
At time: 1305.5299470424652 and batch: 300, loss is 5.896036615371704 and perplexity is 363.5935472729214
At time: 1307.0752973556519 and batch: 350, loss is 5.876635932922364 and perplexity is 356.6075698044629
At time: 1308.6116197109222 and batch: 400, loss is 5.904223556518555 and perplexity is 366.58248467844214
At time: 1310.1483936309814 and batch: 450, loss is 5.928179874420166 and perplexity is 375.47048806738576
At time: 1311.6865804195404 and batch: 500, loss is 5.920413780212402 and perplexity is 372.56584235971025
At time: 1313.224993467331 and batch: 550, loss is 5.887336044311524 and perplexity is 360.4437979563524
At time: 1314.7632131576538 and batch: 600, loss is 5.898065109252929 and perplexity is 364.33184311996064
At time: 1316.3028979301453 and batch: 650, loss is 5.92153486251831 and perplexity is 372.9837535460136
At time: 1317.8408670425415 and batch: 700, loss is 5.886418800354004 and perplexity is 360.1133346415602
At time: 1319.37842130661 and batch: 750, loss is 5.8885290241241455 and perplexity is 360.8740567248829
At time: 1320.9189529418945 and batch: 800, loss is 5.919002819061279 and perplexity is 372.04053710970396
At time: 1322.456575870514 and batch: 850, loss is 5.922059650421143 and perplexity is 373.1795422771129
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.576571146647136 and perplexity of 264.1642705706399
Finished 46 epochs...
Completing Train Step...
At time: 1326.535166978836 and batch: 50, loss is 5.926135149002075 and perplexity is 374.7035383846586
At time: 1328.0714046955109 and batch: 100, loss is 5.868195838928223 and perplexity is 353.61043424099705
At time: 1329.6094186306 and batch: 150, loss is 5.857395629882813 and perplexity is 349.81191694393715
At time: 1331.1472749710083 and batch: 200, loss is 5.8914867115020755 and perplexity is 361.9429893725362
At time: 1332.6851909160614 and batch: 250, loss is 5.9301960659027095 and perplexity is 376.2282721295586
At time: 1334.221428155899 and batch: 300, loss is 5.892383069992065 and perplexity is 362.26756549050003
At time: 1335.7945528030396 and batch: 350, loss is 5.873421020507813 and perplexity is 355.46294861547887
At time: 1337.331698179245 and batch: 400, loss is 5.900879449844361 and perplexity is 365.3586412168553
At time: 1338.8675396442413 and batch: 450, loss is 5.925627136230469 and perplexity is 374.5132325445847
At time: 1340.3996241092682 and batch: 500, loss is 5.917137403488159 and perplexity is 371.34717380436365
At time: 1341.9501340389252 and batch: 550, loss is 5.8832475948333744 and perplexity is 358.9731500820036
At time: 1343.4981806278229 and batch: 600, loss is 5.895172624588013 and perplexity is 363.27954146766206
At time: 1345.0467157363892 and batch: 650, loss is 5.919647626876831 and perplexity is 372.28050911533217
At time: 1346.58593416214 and batch: 700, loss is 5.884458560943603 and perplexity is 359.4081177136109
At time: 1348.1305413246155 and batch: 750, loss is 5.885580043792725 and perplexity is 359.81141385604576
At time: 1349.6691884994507 and batch: 800, loss is 5.9166062068939205 and perplexity is 371.1499678325662
At time: 1351.208936214447 and batch: 850, loss is 5.918790616989136 and perplexity is 371.96159771265604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.575145085652669 and perplexity of 263.7878246894378
Finished 47 epochs...
Completing Train Step...
At time: 1355.4284925460815 and batch: 50, loss is 5.921969356536866 and perplexity is 373.1458479679259
At time: 1356.963193655014 and batch: 100, loss is 5.864962644577027 and perplexity is 352.4689892336055
At time: 1358.5000395774841 and batch: 150, loss is 5.853759384155273 and perplexity is 348.54222471100474
At time: 1360.0381140708923 and batch: 200, loss is 5.889862060546875 and perplexity is 361.3554357631272
At time: 1361.5742280483246 and batch: 250, loss is 5.92777642250061 and perplexity is 375.319034332443
At time: 1363.1186952590942 and batch: 300, loss is 5.890425939559936 and perplexity is 361.55925396861335
At time: 1364.6654980182648 and batch: 350, loss is 5.871441345214844 and perplexity is 354.7599434890753
At time: 1366.218477010727 and batch: 400, loss is 5.898094072341919 and perplexity is 364.3423954483679
At time: 1367.76029586792 and batch: 450, loss is 5.9225544929504395 and perplexity is 373.36425308330723
At time: 1369.2961211204529 and batch: 500, loss is 5.914022569656372 and perplexity is 370.19228863550006
At time: 1370.8312017917633 and batch: 550, loss is 5.882200918197632 and perplexity is 358.59761783765083
At time: 1372.3714501857758 and batch: 600, loss is 5.892658166885376 and perplexity is 362.36723788146764
At time: 1373.9067559242249 and batch: 650, loss is 5.91623851776123 and perplexity is 371.0135251085864
At time: 1375.4839181900024 and batch: 700, loss is 5.882155055999756 and perplexity is 358.5811721398646
At time: 1377.0215201377869 and batch: 750, loss is 5.884316530227661 and perplexity is 359.35707434628546
At time: 1378.5587759017944 and batch: 800, loss is 5.914692707061768 and perplexity is 370.44045147761824
At time: 1380.0967252254486 and batch: 850, loss is 5.91810622215271 and perplexity is 371.70711620867974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.575006484985352 and perplexity of 263.75126605448975
Finished 48 epochs...
Completing Train Step...
At time: 1384.0887343883514 and batch: 50, loss is 5.921377325057984 and perplexity is 372.9249992608588
At time: 1385.6530067920685 and batch: 100, loss is 5.862805299758911 and perplexity is 351.7094117160447
At time: 1387.192126274109 and batch: 150, loss is 5.852202749252319 and perplexity is 348.0000937783026
At time: 1388.7294878959656 and batch: 200, loss is 5.887478666305542 and perplexity is 360.4952088356224
At time: 1390.2656798362732 and batch: 250, loss is 5.925374460220337 and perplexity is 374.41861398966654
At time: 1391.801617383957 and batch: 300, loss is 5.886107997894287 and perplexity is 360.00142792271924
At time: 1393.3434298038483 and batch: 350, loss is 5.868768033981323 and perplexity is 353.8128262805341
At time: 1394.8844254016876 and batch: 400, loss is 5.8947764587402345 and perplexity is 363.13565102425764
At time: 1396.4255821704865 and batch: 450, loss is 5.9196889686584475 and perplexity is 372.29590017298483
At time: 1397.9622192382812 and batch: 500, loss is 5.911552982330322 and perplexity is 369.2791943983319
At time: 1399.4996497631073 and batch: 550, loss is 5.880208606719971 and perplexity is 357.8838909070322
At time: 1401.0374238491058 and batch: 600, loss is 5.891264896392823 and perplexity is 361.86271385229605
At time: 1402.574683189392 and batch: 650, loss is 5.914347105026245 and perplexity is 370.312448623845
At time: 1404.1138730049133 and batch: 700, loss is 5.880874996185303 and perplexity is 358.1224604430288
At time: 1405.652847290039 and batch: 750, loss is 5.882469673156738 and perplexity is 358.69400567754315
At time: 1407.190353155136 and batch: 800, loss is 5.912299146652222 and perplexity is 369.55484018374796
At time: 1408.7276740074158 and batch: 850, loss is 5.914538135528565 and perplexity is 370.383196354193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.573342005411784 and perplexity of 263.31262261743154
Finished 49 epochs...
Completing Train Step...
At time: 1412.7331075668335 and batch: 50, loss is 5.918172073364258 and perplexity is 371.7315943785727
At time: 1414.2945895195007 and batch: 100, loss is 5.861686286926269 and perplexity is 351.3160644923145
At time: 1415.83158993721 and batch: 150, loss is 5.85027419090271 and perplexity is 347.32960204081655
At time: 1417.3680889606476 and batch: 200, loss is 5.885745029449463 and perplexity is 359.8707824758134
At time: 1418.9059669971466 and batch: 250, loss is 5.922956104278565 and perplexity is 373.5142305111636
At time: 1420.4437446594238 and batch: 300, loss is 5.885934343338013 and perplexity is 359.9389174622654
At time: 1421.9817979335785 and batch: 350, loss is 5.868270530700683 and perplexity is 353.63684701748724
At time: 1423.5192606449127 and batch: 400, loss is 5.894711256027222 and perplexity is 363.1119743665187
At time: 1425.052949666977 and batch: 450, loss is 5.918473739624023 and perplexity is 371.8437501742445
At time: 1426.5883028507233 and batch: 500, loss is 5.909217376708984 and perplexity is 368.41771027125696
At time: 1428.1295051574707 and batch: 550, loss is 5.877148580551148 and perplexity is 356.79043069712634
At time: 1429.6770343780518 and batch: 600, loss is 5.88970838546753 and perplexity is 361.2999087045363
At time: 1431.2125341892242 and batch: 650, loss is 5.9124893474578855 and perplexity is 369.6251364970836
At time: 1432.7524900436401 and batch: 700, loss is 5.87640157699585 and perplexity is 356.52400649919986
At time: 1434.2890684604645 and batch: 750, loss is 5.879738416671753 and perplexity is 357.71565701715997
At time: 1435.826785326004 and batch: 800, loss is 5.91005687713623 and perplexity is 368.7271269559822
At time: 1437.3660142421722 and batch: 850, loss is 5.912696714401245 and perplexity is 369.7017924795116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.572854359944661 and perplexity of 263.18425071310844
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f777bec6898>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'seq_len': 50, 'wordvec_source': 'glove', 'lr': 7.703220052179859, 'anneal': 6.045581687791914, 'dropout': 0.3862150257859597, 'wordvec_dim': 200, 'tune_wordvecs': True, 'data': 'wikitext', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.040959119796753 and batch: 50, loss is 7.431589412689209 and perplexity is 1688.48914781504
At time: 3.565207004547119 and batch: 100, loss is 6.4804168224334715 and perplexity is 652.2427590324425
At time: 5.090057849884033 and batch: 150, loss is 6.090775527954102 and perplexity is 441.76387856921883
At time: 6.615225553512573 and batch: 200, loss is 5.953432512283325 and perplexity is 385.0728404179682
At time: 8.140260696411133 and batch: 250, loss is 5.888504209518433 and perplexity is 360.8651018885592
At time: 9.666743040084839 and batch: 300, loss is 5.759425916671753 and perplexity is 317.16619681747505
At time: 11.225752592086792 and batch: 350, loss is 5.677176904678345 and perplexity is 292.12357204370977
At time: 12.75378155708313 and batch: 400, loss is 5.65645694732666 and perplexity is 286.13305998407606
At time: 14.284553050994873 and batch: 450, loss is 5.617074308395385 and perplexity is 275.0833955930042
At time: 15.81498408317566 and batch: 500, loss is 5.59292329788208 and perplexity is 268.5194357894645
At time: 17.347119092941284 and batch: 550, loss is 5.560988855361939 and perplexity is 260.07989060160736
At time: 18.889044523239136 and batch: 600, loss is 5.571680994033813 and perplexity is 262.8756203886104
At time: 20.43666124343872 and batch: 650, loss is 5.560040054321289 and perplexity is 259.83324355874106
At time: 21.976603984832764 and batch: 700, loss is 5.503222332000733 and perplexity is 245.4816826434771
At time: 23.50861954689026 and batch: 750, loss is 5.478111906051636 and perplexity is 239.39428150320578
At time: 25.040170907974243 and batch: 800, loss is 5.4681738662719725 and perplexity is 237.0269543900982
At time: 26.575505018234253 and batch: 850, loss is 5.44912049293518 and perplexity is 232.55354345298926
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.932474454243978 and perplexity of 138.72235010480014
Finished 1 epochs...
Completing Train Step...
At time: 30.60932469367981 and batch: 50, loss is 5.19369779586792 and perplexity is 180.1334195181448
At time: 32.143622636795044 and batch: 100, loss is 5.064991054534912 and perplexity is 158.37902529330327
At time: 33.67748689651489 and batch: 150, loss is 5.033586473464966 and perplexity is 153.48248763723197
At time: 35.23100256919861 and batch: 200, loss is 5.027775268554688 and perplexity is 152.59315599835986
At time: 36.76454401016235 and batch: 250, loss is 5.018694505691529 and perplexity is 151.2137661688896
At time: 38.2978789806366 and batch: 300, loss is 4.963807401657104 and perplexity is 143.1377426064802
At time: 39.832178592681885 and batch: 350, loss is 4.904210243225098 and perplexity is 134.85636421244152
At time: 41.36659836769104 and batch: 400, loss is 4.9058275890350345 and perplexity is 135.07465006231334
At time: 42.901472091674805 and batch: 450, loss is 4.896183547973632 and perplexity is 133.77824592604878
At time: 44.43462610244751 and batch: 500, loss is 4.881533727645874 and perplexity is 131.832704372011
At time: 45.96787071228027 and batch: 550, loss is 4.881597261428833 and perplexity is 131.84108046851722
At time: 47.50290560722351 and batch: 600, loss is 4.906965618133545 and perplexity is 135.22845644602577
At time: 49.03946614265442 and batch: 650, loss is 4.8987659740447995 and perplexity is 134.1241648188246
At time: 50.57446813583374 and batch: 700, loss is 4.847111444473267 and perplexity is 127.37193704133465
At time: 52.10813021659851 and batch: 750, loss is 4.824709272384643 and perplexity is 124.55025292100912
At time: 53.64241290092468 and batch: 800, loss is 4.798246669769287 and perplexity is 121.29755629289473
At time: 55.17712092399597 and batch: 850, loss is 4.791588716506958 and perplexity is 120.49264533539139
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.645022074381511 and perplexity of 104.06566294520705
Finished 2 epochs...
Completing Train Step...
At time: 59.2367730140686 and batch: 50, loss is 4.756098489761353 and perplexity is 116.29132785987592
At time: 60.77115750312805 and batch: 100, loss is 4.668719387054443 and perplexity is 106.56119137541744
At time: 62.304332971572876 and batch: 150, loss is 4.670968723297119 and perplexity is 106.80115310137033
At time: 63.84489440917969 and batch: 200, loss is 4.685887947082519 and perplexity is 108.40648880974581
At time: 65.37636947631836 and batch: 250, loss is 4.683853321075439 and perplexity is 108.18614638150743
At time: 66.9130380153656 and batch: 300, loss is 4.651174898147583 and perplexity is 104.70793449501862
At time: 68.44674110412598 and batch: 350, loss is 4.6006108665466305 and perplexity is 99.54510584773509
At time: 69.98122882843018 and batch: 400, loss is 4.615276298522949 and perplexity is 101.01573517544489
At time: 71.5140609741211 and batch: 450, loss is 4.632436265945435 and perplexity is 102.76412011159226
At time: 73.04642701148987 and batch: 500, loss is 4.609769926071167 and perplexity is 100.46103351260778
At time: 74.58185338973999 and batch: 550, loss is 4.629378070831299 and perplexity is 102.45032744572097
At time: 76.12159276008606 and batch: 600, loss is 4.662489223480224 and perplexity is 105.89936151736102
At time: 77.65790486335754 and batch: 650, loss is 4.651444911956787 and perplexity is 104.7362109006037
At time: 79.1965503692627 and batch: 700, loss is 4.604218769073486 and perplexity is 99.90490355389655
At time: 80.74993300437927 and batch: 750, loss is 4.592348737716675 and perplexity is 98.72603963342655
At time: 82.2887716293335 and batch: 800, loss is 4.56239504814148 and perplexity is 95.81268123042027
At time: 83.82773089408875 and batch: 850, loss is 4.5669752216339115 and perplexity is 96.25252644801176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.587467193603516 and perplexity of 98.24527849887158
Finished 3 epochs...
Completing Train Step...
At time: 87.8355302810669 and batch: 50, loss is 4.553409194946289 and perplexity is 94.95557920638404
At time: 89.37410831451416 and batch: 100, loss is 4.481405391693115 and perplexity is 88.35876412979928
At time: 90.91502165794373 and batch: 150, loss is 4.484719457626343 and perplexity is 88.65207665998099
At time: 92.45824861526489 and batch: 200, loss is 4.511306438446045 and perplexity is 91.04067989545167
At time: 93.99726295471191 and batch: 250, loss is 4.504511604309082 and perplexity is 90.42417048743773
At time: 95.53829407691956 and batch: 300, loss is 4.483406705856323 and perplexity is 88.53577484383922
At time: 97.12017846107483 and batch: 350, loss is 4.433591108322144 and perplexity is 84.23336556068476
At time: 98.66040802001953 and batch: 400, loss is 4.450230407714844 and perplexity is 85.64667538374114
At time: 100.20170950889587 and batch: 450, loss is 4.474577102661133 and perplexity is 87.75748015645601
At time: 101.74619793891907 and batch: 500, loss is 4.451684112548828 and perplexity is 85.771270910379
At time: 103.28893184661865 and batch: 550, loss is 4.476638631820679 and perplexity is 87.93858136934386
At time: 104.83207631111145 and batch: 600, loss is 4.514516286849975 and perplexity is 91.3333761805514
At time: 106.37710809707642 and batch: 650, loss is 4.498504724502563 and perplexity is 89.88263147194775
At time: 107.91711664199829 and batch: 700, loss is 4.4593173694610595 and perplexity is 86.42849022771621
At time: 109.46156907081604 and batch: 750, loss is 4.448158216476441 and perplexity is 85.46938284887213
At time: 110.99886775016785 and batch: 800, loss is 4.416465940475464 and perplexity is 82.80313644757842
At time: 112.53878164291382 and batch: 850, loss is 4.427711935043335 and perplexity is 83.73959590961371
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.562618573506673 and perplexity of 95.8341001887335
Finished 4 epochs...
Completing Train Step...
At time: 116.81236481666565 and batch: 50, loss is 4.421292047500611 and perplexity is 83.20371909595143
At time: 118.35254096984863 and batch: 100, loss is 4.3562483310699465 and perplexity is 77.96408961059076
At time: 119.89139795303345 and batch: 150, loss is 4.365929079055786 and perplexity is 78.7225054071327
At time: 121.42719674110413 and batch: 200, loss is 4.398326396942139 and perplexity is 81.3146662482868
At time: 122.96675896644592 and batch: 250, loss is 4.383237190246582 and perplexity is 80.09690308726836
At time: 124.50469136238098 and batch: 300, loss is 4.366199169158936 and perplexity is 78.7437704483476
At time: 126.04489994049072 and batch: 350, loss is 4.316128854751587 and perplexity is 74.89812484418123
At time: 127.58179140090942 and batch: 400, loss is 4.337020320892334 and perplexity is 76.47931567143154
At time: 129.12162446975708 and batch: 450, loss is 4.370768718719482 and perplexity is 79.10441737945943
At time: 130.66666626930237 and batch: 500, loss is 4.3466292762756344 and perplexity is 77.21774408459454
At time: 132.21179175376892 and batch: 550, loss is 4.375522718429566 and perplexity is 79.48137507518295
At time: 133.753591299057 and batch: 600, loss is 4.416303768157959 and perplexity is 82.7897091598406
At time: 135.35608100891113 and batch: 650, loss is 4.395905132293701 and perplexity is 81.11802008377374
At time: 136.89766263961792 and batch: 700, loss is 4.358897018432617 and perplexity is 78.17086583137859
At time: 138.45180559158325 and batch: 750, loss is 4.345592851638794 and perplexity is 77.13775517062477
At time: 140.00489592552185 and batch: 800, loss is 4.313638687133789 and perplexity is 74.71184798561858
At time: 141.5434374809265 and batch: 850, loss is 4.336617708206177 and perplexity is 76.44853032641468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.543469746907552 and perplexity of 94.01644811480676
Finished 5 epochs...
Completing Train Step...
At time: 145.6000747680664 and batch: 50, loss is 4.323623476028442 and perplexity is 75.4615666784705
At time: 147.1418821811676 and batch: 100, loss is 4.266318035125733 and perplexity is 71.25877967924657
At time: 148.68841981887817 and batch: 150, loss is 4.272157235145569 and perplexity is 71.67609114378992
At time: 150.23443174362183 and batch: 200, loss is 4.3088868141174315 and perplexity is 74.35766894454305
At time: 151.7805733680725 and batch: 250, loss is 4.293430519104004 and perplexity is 73.21721120669127
At time: 153.32851338386536 and batch: 300, loss is 4.277243022918701 and perplexity is 72.04154906474695
At time: 154.8772964477539 and batch: 350, loss is 4.229258079528808 and perplexity is 68.666268360459
At time: 156.42605996131897 and batch: 400, loss is 4.250293369293213 and perplexity is 70.1259821390836
At time: 157.97412085533142 and batch: 450, loss is 4.288398351669311 and perplexity is 72.84969541683364
At time: 159.52212738990784 and batch: 500, loss is 4.268418521881103 and perplexity is 71.40861511074634
At time: 161.07063126564026 and batch: 550, loss is 4.294967412948608 and perplexity is 73.32982480331916
At time: 162.62757086753845 and batch: 600, loss is 4.3357825756073 and perplexity is 76.38471231856136
At time: 164.174551486969 and batch: 650, loss is 4.315012245178223 and perplexity is 74.81453955570448
At time: 165.71557760238647 and batch: 700, loss is 4.28378830909729 and perplexity is 72.51462815020277
At time: 167.25865507125854 and batch: 750, loss is 4.269486608505249 and perplexity is 71.4849264436972
At time: 168.80235195159912 and batch: 800, loss is 4.23731484413147 and perplexity is 69.22173093171979
At time: 170.34265422821045 and batch: 850, loss is 4.257346286773681 and perplexity is 70.62232317304206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.540734926859538 and perplexity of 93.75968131319274
Finished 6 epochs...
Completing Train Step...
At time: 174.3926510810852 and batch: 50, loss is 4.250527639389038 and perplexity is 70.14241248413325
At time: 175.95122456550598 and batch: 100, loss is 4.1952161693573 and perplexity is 66.36807677124419
At time: 177.48865246772766 and batch: 150, loss is 4.205529451370239 and perplexity is 67.05609120879662
At time: 179.02507638931274 and batch: 200, loss is 4.240972871780396 and perplexity is 69.4754096363559
At time: 180.56312584877014 and batch: 250, loss is 4.227688488960266 and perplexity is 68.55857497263591
At time: 182.1029074192047 and batch: 300, loss is 4.21163613319397 and perplexity is 67.46683428435185
At time: 183.63937306404114 and batch: 350, loss is 4.162207813262939 and perplexity is 64.21313682867152
At time: 185.17425537109375 and batch: 400, loss is 4.180425539016723 and perplexity is 65.39367485409073
At time: 186.71212196350098 and batch: 450, loss is 4.231966419219971 and perplexity is 68.85249200511697
At time: 188.2513952255249 and batch: 500, loss is 4.211876811981202 and perplexity is 67.48307407441385
At time: 189.79185247421265 and batch: 550, loss is 4.231169962882996 and perplexity is 68.7976758337842
At time: 191.33005356788635 and batch: 600, loss is 4.275041627883911 and perplexity is 71.88313159003445
At time: 192.8687345981598 and batch: 650, loss is 4.253611488342285 and perplexity is 70.35905496410184
At time: 194.40842199325562 and batch: 700, loss is 4.222733736038208 and perplexity is 68.21972432679165
At time: 195.94723629951477 and batch: 750, loss is 4.208570318222046 and perplexity is 67.26031019777669
At time: 197.4864320755005 and batch: 800, loss is 4.175452747344971 and perplexity is 65.06929294268525
At time: 199.02688145637512 and batch: 850, loss is 4.196223955154419 and perplexity is 66.4349952904993
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.539682070414226 and perplexity of 93.66101777679592
Finished 7 epochs...
Completing Train Step...
At time: 203.1069028377533 and batch: 50, loss is 4.189960527420044 and perplexity is 66.02018492194867
At time: 204.6722433567047 and batch: 100, loss is 4.137284665107727 and perplexity is 62.632522071344276
At time: 206.21064066886902 and batch: 150, loss is 4.15617458820343 and perplexity is 63.826890848476324
At time: 207.75126004219055 and batch: 200, loss is 4.185832514762878 and perplexity is 65.74821449784794
At time: 209.29246282577515 and batch: 250, loss is 4.167058405876159 and perplexity is 64.52536523005101
At time: 210.83600974082947 and batch: 300, loss is 4.156676254272461 and perplexity is 63.85891866686992
At time: 212.37581062316895 and batch: 350, loss is 4.110597500801086 and perplexity is 60.983144163569456
At time: 213.95923614501953 and batch: 400, loss is 4.134417586326599 and perplexity is 62.453206874535915
At time: 215.5017228126526 and batch: 450, loss is 4.1829673957824705 and perplexity is 65.5601076435009
At time: 217.0493483543396 and batch: 500, loss is 4.159190430641174 and perplexity is 64.01967324912108
At time: 218.59699392318726 and batch: 550, loss is 4.183847074508667 and perplexity is 65.61780484926359
At time: 220.14319849014282 and batch: 600, loss is 4.222508735656739 and perplexity is 68.20437658948251
At time: 221.68132042884827 and batch: 650, loss is 4.203099040985108 and perplexity is 66.89331527462187
At time: 223.22262954711914 and batch: 700, loss is 4.1733129644393925 and perplexity is 64.93020764121249
At time: 224.76232290267944 and batch: 750, loss is 4.158569583892822 and perplexity is 63.979939178814284
At time: 226.30005645751953 and batch: 800, loss is 4.1223157787322995 and perplexity is 61.701965041844055
At time: 227.83843278884888 and batch: 850, loss is 4.144512405395508 and perplexity is 63.08685359169796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.547279993693034 and perplexity of 94.37535731611433
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 231.8594946861267 and batch: 50, loss is 4.169216899871826 and perplexity is 64.66479326662413
At time: 233.42632007598877 and batch: 100, loss is 4.1110341310501095 and perplexity is 61.00977706293344
At time: 234.96872425079346 and batch: 150, loss is 4.125523862838745 and perplexity is 61.900227987293086
At time: 236.50944423675537 and batch: 200, loss is 4.155684251785278 and perplexity is 63.79560187112534
At time: 238.05335450172424 and batch: 250, loss is 4.09459165096283 and perplexity is 60.01482715617998
At time: 239.59667539596558 and batch: 300, loss is 4.0700385999679565 and perplexity is 58.559222932396295
At time: 241.14125156402588 and batch: 350, loss is 4.014196457862854 and perplexity is 55.37877833732265
At time: 242.68586492538452 and batch: 400, loss is 4.017170610427857 and perplexity is 55.5437284447639
At time: 244.23015904426575 and batch: 450, loss is 4.046473107337952 and perplexity is 57.19537893873851
At time: 245.78491640090942 and batch: 500, loss is 4.005457849502563 and perplexity is 54.89695318873889
At time: 247.33882927894592 and batch: 550, loss is 4.002929430007935 and perplexity is 54.75832598980946
At time: 248.88062739372253 and batch: 600, loss is 4.023011617660522 and perplexity is 55.86910911466836
At time: 250.42307090759277 and batch: 650, loss is 3.9801923179626466 and perplexity is 53.52732750423996
At time: 251.9665641784668 and batch: 700, loss is 3.9216259479522706 and perplexity is 50.48245993616643
At time: 253.53727889060974 and batch: 750, loss is 3.8925875997543335 and perplexity is 49.03761220865105
At time: 255.08255243301392 and batch: 800, loss is 3.822295422554016 and perplexity is 45.70900948464787
At time: 256.6294057369232 and batch: 850, loss is 3.817382802963257 and perplexity is 45.485009173835635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.416869481404622 and perplexity of 82.83655764515468
Finished 9 epochs...
Completing Train Step...
At time: 260.70508432388306 and batch: 50, loss is 4.044159331321716 and perplexity is 57.063194624130375
At time: 262.24592685699463 and batch: 100, loss is 3.989386057853699 and perplexity is 54.02171297376678
At time: 263.7851552963257 and batch: 150, loss is 4.017786812782288 and perplexity is 55.577965168296934
At time: 265.3255965709686 and batch: 200, loss is 4.048442029953003 and perplexity is 57.30810315001578
At time: 266.8670766353607 and batch: 250, loss is 3.994002170562744 and perplexity is 54.27165973697217
At time: 268.40870928764343 and batch: 300, loss is 3.9852792835235595 and perplexity is 53.80031292085319
At time: 269.9531066417694 and batch: 350, loss is 3.936383261680603 and perplexity is 51.23296956792467
At time: 271.5013792514801 and batch: 400, loss is 3.941304850578308 and perplexity is 51.48573868477115
At time: 273.0412085056305 and batch: 450, loss is 3.9786438274383547 and perplexity is 53.44450508622318
At time: 274.58267641067505 and batch: 500, loss is 3.9457772827148436 and perplexity is 51.71652085113914
At time: 276.12420439720154 and batch: 550, loss is 3.950128607749939 and perplexity is 51.94204655502501
At time: 277.6629240512848 and batch: 600, loss is 3.9778472852706908 and perplexity is 53.40195123451068
At time: 279.2026493549347 and batch: 650, loss is 3.943541741371155 and perplexity is 51.60103556478699
At time: 280.7440674304962 and batch: 700, loss is 3.8937874603271485 and perplexity is 49.096485819124
At time: 282.286719083786 and batch: 750, loss is 3.8770266246795653 and perplexity is 48.28044555163319
At time: 283.8299660682678 and batch: 800, loss is 3.818939595222473 and perplexity is 45.55587503142454
At time: 285.37092447280884 and batch: 850, loss is 3.826282482147217 and perplexity is 45.891617822667364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.409849166870117 and perplexity of 82.25705547932169
Finished 10 epochs...
Completing Train Step...
At time: 289.38257455825806 and batch: 50, loss is 3.9996695423126223 and perplexity is 54.580110635542155
At time: 290.9299433231354 and batch: 100, loss is 3.942840642929077 and perplexity is 51.56487083814103
At time: 292.52494621276855 and batch: 150, loss is 3.970403928756714 and perplexity is 53.00593713836206
At time: 294.0861530303955 and batch: 200, loss is 4.006863255500793 and perplexity is 54.97415993674024
At time: 295.63878202438354 and batch: 250, loss is 3.9548375368118287 and perplexity is 52.18721475438142
At time: 297.191025018692 and batch: 300, loss is 3.947681212425232 and perplexity is 51.81507926608031
At time: 298.7477502822876 and batch: 350, loss is 3.9001832246780395 and perplexity is 49.41150168266434
At time: 300.304484128952 and batch: 400, loss is 3.906451106071472 and perplexity is 49.722179744187
At time: 301.8603823184967 and batch: 450, loss is 3.947136845588684 and perplexity is 51.78688053121916
At time: 303.41538286209106 and batch: 500, loss is 3.9171721076965333 and perplexity is 50.25811908390176
At time: 304.96678161621094 and batch: 550, loss is 3.92441339969635 and perplexity is 50.62337366102468
At time: 306.5196385383606 and batch: 600, loss is 3.95550537109375 and perplexity is 52.222078805881
At time: 308.073361158371 and batch: 650, loss is 3.923611669540405 and perplexity is 50.58280364104229
At time: 309.61307191848755 and batch: 700, loss is 3.8784097719192503 and perplexity is 48.34727072049905
At time: 311.15261030197144 and batch: 750, loss is 3.866530375480652 and perplexity is 47.77633224137733
At time: 312.693332195282 and batch: 800, loss is 3.813220157623291 and perplexity is 45.296064739524006
At time: 314.23281264305115 and batch: 850, loss is 3.82562744140625 and perplexity is 45.86156678672559
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.407702128092448 and perplexity of 82.08063584913553
Finished 11 epochs...
Completing Train Step...
At time: 318.27041578292847 and batch: 50, loss is 3.9675865173339844 and perplexity is 52.856807783616375
At time: 319.82532501220703 and batch: 100, loss is 3.911502799987793 and perplexity is 49.973996492151876
At time: 321.3722529411316 and batch: 150, loss is 3.93931173324585 and perplexity is 51.38322376269499
At time: 322.9133462905884 and batch: 200, loss is 3.979627461433411 and perplexity is 53.497100781491184
At time: 324.46002292633057 and batch: 250, loss is 3.9284030961990357 and perplexity is 50.82574899751494
At time: 325.997127532959 and batch: 300, loss is 3.921805462837219 and perplexity is 50.49152310261609
At time: 327.5461027622223 and batch: 350, loss is 3.8749396800994873 and perplexity is 48.179792003185845
At time: 329.08471632003784 and batch: 400, loss is 3.8823893880844116 and perplexity is 48.54005765516456
At time: 330.624792098999 and batch: 450, loss is 3.924831314086914 and perplexity is 50.64453431874022
At time: 332.19068598747253 and batch: 500, loss is 3.8968614292144776 and perplexity is 49.24763909021128
At time: 333.7301001548767 and batch: 550, loss is 3.905473117828369 and perplexity is 49.67357580788884
At time: 335.269939661026 and batch: 600, loss is 3.9390415143966675 and perplexity is 51.36934092288953
At time: 336.80793952941895 and batch: 650, loss is 3.908210959434509 and perplexity is 49.80976053149513
At time: 338.35076355934143 and batch: 700, loss is 3.8665715408325196 and perplexity is 47.77829901138611
At time: 339.89071774482727 and batch: 750, loss is 3.857469205856323 and perplexity is 47.345378213296264
At time: 341.44489550590515 and batch: 800, loss is 3.805938754081726 and perplexity is 44.96744367652095
At time: 342.9974055290222 and batch: 850, loss is 3.8213215970993044 and perplexity is 45.664518554418265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.407236417134603 and perplexity of 82.04241889731101
Finished 12 epochs...
Completing Train Step...
At time: 347.17463207244873 and batch: 50, loss is 3.94257878780365 and perplexity is 51.55137008011857
At time: 348.7380106449127 and batch: 100, loss is 3.887001714706421 and perplexity is 48.76445735987249
At time: 350.2825028896332 and batch: 150, loss is 3.9154475212097166 and perplexity is 50.171519306743306
At time: 351.82533383369446 and batch: 200, loss is 3.9579656600952147 and perplexity is 52.35071839239247
At time: 353.3672571182251 and batch: 250, loss is 3.907603883743286 and perplexity is 49.77953141330061
At time: 354.9057147502899 and batch: 300, loss is 3.9015847396850587 and perplexity is 49.4808011946015
At time: 356.4473977088928 and batch: 350, loss is 3.854778652191162 and perplexity is 47.21816414729452
At time: 357.99229669570923 and batch: 400, loss is 3.8637267780303954 and perplexity is 47.64257422754932
At time: 359.5380024909973 and batch: 450, loss is 3.9074115419387816 and perplexity is 49.769957649148246
At time: 361.08366417884827 and batch: 500, loss is 3.8803471517562866 and perplexity is 48.44102854089788
At time: 362.6251046657562 and batch: 550, loss is 3.8903551149368285 and perplexity is 48.928258594468325
At time: 364.1684727668762 and batch: 600, loss is 3.925261363983154 and perplexity is 50.66631867931439
At time: 365.71218156814575 and batch: 650, loss is 3.894470148086548 and perplexity is 49.13001483264358
At time: 367.25629448890686 and batch: 700, loss is 3.855594696998596 and perplexity is 47.256712011216415
At time: 368.79762029647827 and batch: 750, loss is 3.84853431224823 and perplexity is 46.924236525933686
At time: 370.3638274669647 and batch: 800, loss is 3.798071026802063 and perplexity is 44.61504021820757
At time: 371.9053633213043 and batch: 850, loss is 3.8152766180038453 and perplexity is 45.3893101469374
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.407851537068685 and perplexity of 82.09290034909696
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 375.88226199150085 and batch: 50, loss is 3.943669362068176 and perplexity is 51.60762134514479
At time: 377.44366908073425 and batch: 100, loss is 3.901717114448547 and perplexity is 49.48735163750395
At time: 378.98404121398926 and batch: 150, loss is 3.923957028388977 and perplexity is 50.60027587678725
At time: 380.5253601074219 and batch: 200, loss is 3.9728517818450926 and perplexity is 53.135846820328645
At time: 382.0619521141052 and batch: 250, loss is 3.9288953733444214 and perplexity is 50.85077551162859
At time: 383.5993137359619 and batch: 300, loss is 3.9104800987243653 and perplexity is 49.92291414824198
At time: 385.1365776062012 and batch: 350, loss is 3.8544996309280397 and perplexity is 47.204991113355575
At time: 386.67770314216614 and batch: 400, loss is 3.854443573951721 and perplexity is 47.20234501845335
At time: 388.21743512153625 and batch: 450, loss is 3.898153419494629 and perplexity is 49.31130768198059
At time: 389.7549946308136 and batch: 500, loss is 3.8648550701141358 and perplexity is 47.69635930383127
At time: 391.2941393852234 and batch: 550, loss is 3.8617534112930296 and perplexity is 47.5486506595882
At time: 392.8398082256317 and batch: 600, loss is 3.892585277557373 and perplexity is 49.037498333789244
At time: 394.38154196739197 and batch: 650, loss is 3.8530466032028197 and perplexity is 47.13645076007496
At time: 395.9183967113495 and batch: 700, loss is 3.806866154670715 and perplexity is 45.00916585386747
At time: 397.456791639328 and batch: 750, loss is 3.789846601486206 and perplexity is 44.24961193062951
At time: 398.99644804000854 and batch: 800, loss is 3.736427249908447 and perplexity is 41.94785292789725
At time: 400.5373158454895 and batch: 850, loss is 3.7532339763641356 and perplexity is 42.658816769680435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.390908241271973 and perplexity of 80.7136932074858
Finished 14 epochs...
Completing Train Step...
At time: 404.54041481018066 and batch: 50, loss is 3.921936273574829 and perplexity is 50.49812836800659
At time: 406.1041307449341 and batch: 100, loss is 3.8753788614273073 and perplexity is 48.20095631535707
At time: 407.64438819885254 and batch: 150, loss is 3.8992074584960936 and perplexity is 49.363311125504815
At time: 409.20861411094666 and batch: 200, loss is 3.949543390274048 and perplexity is 51.91165805445554
At time: 410.7455379962921 and batch: 250, loss is 3.907168498039246 and perplexity is 49.757862834406474
At time: 412.28380012512207 and batch: 300, loss is 3.8910101509094237 and perplexity is 48.96031886309242
At time: 413.8281135559082 and batch: 350, loss is 3.8373487329483034 and perplexity is 46.402286356754935
At time: 415.36867666244507 and batch: 400, loss is 3.8380507230758667 and perplexity is 46.434871739644144
At time: 416.9076178073883 and batch: 450, loss is 3.8838476943969726 and perplexity is 48.61089556678801
At time: 418.4440724849701 and batch: 500, loss is 3.8520430278778077 and perplexity is 47.08916951029964
At time: 419.983186006546 and batch: 550, loss is 3.852137393951416 and perplexity is 47.09361334000588
At time: 421.5218245983124 and batch: 600, loss is 3.8865349292755127 and perplexity is 48.741700133414845
At time: 423.06238889694214 and batch: 650, loss is 3.849679093360901 and perplexity is 46.97798526503584
At time: 424.6006569862366 and batch: 700, loss is 3.8056034564971926 and perplexity is 44.95236872871159
At time: 426.1442883014679 and batch: 750, loss is 3.791761903762817 and perplexity is 44.33444452719888
At time: 427.68287348747253 and batch: 800, loss is 3.7421847343444825 and perplexity is 42.190063631353425
At time: 429.221638917923 and batch: 850, loss is 3.7607057762145994 and perplexity is 42.97874865552664
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.389495213826497 and perplexity of 80.5997230841772
Finished 15 epochs...
Completing Train Step...
At time: 433.25708174705505 and batch: 50, loss is 3.91380539894104 and perplexity is 50.08919914601939
At time: 434.8003797531128 and batch: 100, loss is 3.8661611032485963 and perplexity is 47.758693025567865
At time: 436.344158411026 and batch: 150, loss is 3.8889524841308596 and perplexity is 48.85967841925809
At time: 437.8850407600403 and batch: 200, loss is 3.9399755668640135 and perplexity is 51.417344998196874
At time: 439.4281280040741 and batch: 250, loss is 3.8977780294418336 and perplexity is 49.29280018156955
At time: 440.97118639945984 and batch: 300, loss is 3.882252087593079 and perplexity is 48.53339353890287
At time: 442.51346802711487 and batch: 350, loss is 3.82925754070282 and perplexity is 46.028351367199846
At time: 444.0577189922333 and batch: 400, loss is 3.8306630563735964 and perplexity is 46.093090421566636
At time: 445.5999722480774 and batch: 450, loss is 3.877199659347534 and perplexity is 48.288800465322616
At time: 447.1427674293518 and batch: 500, loss is 3.846119513511658 and perplexity is 46.81106064234304
At time: 448.73005294799805 and batch: 550, loss is 3.8475836610794065 and perplexity is 46.879649142516186
At time: 450.2735929489136 and batch: 600, loss is 3.883528208732605 and perplexity is 48.595367563141714
At time: 451.8157958984375 and batch: 650, loss is 3.8479284143447874 and perplexity is 46.89581384089415
At time: 453.3578419685364 and batch: 700, loss is 3.8051738119125367 and perplexity is 44.93305933530503
At time: 454.90062713623047 and batch: 750, loss is 3.7928320217132567 and perplexity is 44.38191300601595
At time: 456.44429445266724 and batch: 800, loss is 3.7448510932922363 and perplexity is 42.30270759290402
At time: 457.9872410297394 and batch: 850, loss is 3.763932695388794 and perplexity is 43.11766161354816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.388998667399089 and perplexity of 80.55971151425281
Finished 16 epochs...
Completing Train Step...
At time: 461.99576711654663 and batch: 50, loss is 3.9071654891967773 and perplexity is 49.75771312106087
At time: 463.5352690219879 and batch: 100, loss is 3.8593111658096313 and perplexity is 47.4326668703652
At time: 465.07542514801025 and batch: 150, loss is 3.8814460182189943 and perplexity is 48.494288019748836
At time: 466.61439180374146 and batch: 200, loss is 3.9329783153533935 and perplexity is 51.05882070837403
At time: 468.157146692276 and batch: 250, loss is 3.8911294412612913 and perplexity is 48.96615970512827
At time: 469.6985993385315 and batch: 300, loss is 3.875969786643982 and perplexity is 48.22944789327849
At time: 471.2405469417572 and batch: 350, loss is 3.823347945213318 and perplexity is 45.75714458006033
At time: 472.78112268447876 and batch: 400, loss is 3.825287413597107 and perplexity is 45.84597522957897
At time: 474.32819151878357 and batch: 450, loss is 3.8725580883026125 and perplexity is 48.06518393476494
At time: 475.8681764602661 and batch: 500, loss is 3.8419455099105835 and perplexity is 46.616078318200074
At time: 477.410293340683 and batch: 550, loss is 3.8441978073120118 and perplexity is 46.72118991711693
At time: 478.95686888694763 and batch: 600, loss is 3.8811537075042724 and perplexity is 48.48011469136665
At time: 480.4984097480774 and batch: 650, loss is 3.8464132595062255 and perplexity is 46.82481322369214
At time: 482.0374376773834 and batch: 700, loss is 3.8045099925994874 and perplexity is 44.903241800546525
At time: 483.58536982536316 and batch: 750, loss is 3.793070592880249 and perplexity is 44.39250251392055
At time: 485.12567257881165 and batch: 800, loss is 3.7460698461532593 and perplexity is 42.35429556892302
At time: 486.6657249927521 and batch: 850, loss is 3.765551218986511 and perplexity is 43.18750507274455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.388766924540202 and perplexity of 80.54104453944801
Finished 17 epochs...
Completing Train Step...
At time: 490.72008228302 and batch: 50, loss is 3.901377658843994 and perplexity is 49.47055572952994
At time: 492.2594244480133 and batch: 100, loss is 3.8534666872024537 and perplexity is 47.15625618851934
At time: 493.79618406295776 and batch: 150, loss is 3.8751471614837647 and perplexity is 48.189789450231075
At time: 495.3384122848511 and batch: 200, loss is 3.927277784347534 and perplexity is 50.76858634874188
At time: 496.8857161998749 and batch: 250, loss is 3.8858933019638062 and perplexity is 48.71043615837295
At time: 498.44012665748596 and batch: 300, loss is 3.8709330940246582 and perplexity is 47.98714171215794
At time: 499.9844117164612 and batch: 350, loss is 3.8185285091400147 and perplexity is 45.537151493981426
At time: 501.5217709541321 and batch: 400, loss is 3.8210158061981203 and perplexity is 45.650556894920385
At time: 503.0632677078247 and batch: 450, loss is 3.8689115047454834 and perplexity is 47.89022941237282
At time: 504.6176390647888 and batch: 500, loss is 3.83849636554718 and perplexity is 46.45556970224438
At time: 506.1696615219116 and batch: 550, loss is 3.841278147697449 and perplexity is 46.584978887452685
At time: 507.7090618610382 and batch: 600, loss is 3.8789800882339476 and perplexity is 48.374851821990525
At time: 509.24565958976746 and batch: 650, loss is 3.8448613405227663 and perplexity is 46.752201265664944
At time: 510.7856357097626 and batch: 700, loss is 3.8035810947418214 and perplexity is 44.86155064184751
At time: 512.3386328220367 and batch: 750, loss is 3.7928335857391358 and perplexity is 44.38198242053073
At time: 513.8791868686676 and batch: 800, loss is 3.7465719318389894 and perplexity is 42.375566393898886
At time: 515.4276614189148 and batch: 850, loss is 3.7663082456588746 and perplexity is 43.22021154427113
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.388662656148274 and perplexity of 80.53264709205148
Finished 18 epochs...
Completing Train Step...
At time: 519.4459974765778 and batch: 50, loss is 3.8960993766784666 and perplexity is 49.210124097963
At time: 521.0287826061249 and batch: 100, loss is 3.8482260417938234 and perplexity is 46.90977339960896
At time: 522.5652606487274 and batch: 150, loss is 3.8697034978866576 and perplexity is 47.9281731692109
At time: 524.1039915084839 and batch: 200, loss is 3.922274799346924 and perplexity is 50.5152261797634
At time: 525.6444180011749 and batch: 250, loss is 3.881334733963013 and perplexity is 48.48889166925721
At time: 527.2247493267059 and batch: 300, loss is 3.8665486574172974 and perplexity is 47.777205693240695
At time: 528.7673621177673 and batch: 350, loss is 3.8143461656570437 and perplexity is 45.34709719840544
At time: 530.3052687644958 and batch: 400, loss is 3.8172988748550414 and perplexity is 45.481191863255575
At time: 531.8445935249329 and batch: 450, loss is 3.8656478834152224 and perplexity is 47.73418860570613
At time: 533.38467669487 and batch: 500, loss is 3.8354359436035157 and perplexity is 46.31361339122588
At time: 534.9247691631317 and batch: 550, loss is 3.8385989618301393 and perplexity is 46.46033611552254
At time: 536.4631917476654 and batch: 600, loss is 3.876930150985718 and perplexity is 48.275787983380255
At time: 538.0014729499817 and batch: 650, loss is 3.843220672607422 and perplexity is 46.67555931826069
At time: 539.540830373764 and batch: 700, loss is 3.802489652633667 and perplexity is 44.81261356728982
At time: 541.0810823440552 and batch: 750, loss is 3.7922609567642214 and perplexity is 44.356575286557394
At time: 542.6238830089569 and batch: 800, loss is 3.746653218269348 and perplexity is 42.379011092427206
At time: 544.1674067974091 and batch: 850, loss is 3.76658607006073 and perplexity is 43.23222084185219
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.388642311096191 and perplexity of 80.53100866781898
Finished 19 epochs...
Completing Train Step...
At time: 548.1731865406036 and batch: 50, loss is 3.8912188720703127 and perplexity is 48.97053898422369
At time: 549.7407159805298 and batch: 100, loss is 3.843501214981079 and perplexity is 46.688655627412544
At time: 551.2836720943451 and batch: 150, loss is 3.8647713327407835 and perplexity is 47.69236550320223
At time: 552.8256487846375 and batch: 200, loss is 3.9177712869644163 and perplexity is 50.28824173043107
At time: 554.3675384521484 and batch: 250, loss is 3.877241768836975 and perplexity is 48.29083392486961
At time: 555.9101827144623 and batch: 300, loss is 3.862623543739319 and perplexity is 47.5900422887926
At time: 557.4599051475525 and batch: 350, loss is 3.8105590963363647 and perplexity is 45.17568936934825
At time: 559.0133364200592 and batch: 400, loss is 3.813911352157593 and perplexity is 45.32738395448555
At time: 560.571231842041 and batch: 450, loss is 3.8627272844314575 and perplexity is 47.59497956881254
At time: 562.1760656833649 and batch: 500, loss is 3.832594566345215 and perplexity is 46.182205721178086
At time: 563.7505662441254 and batch: 550, loss is 3.8360540437698365 and perplexity is 46.342248692196385
At time: 565.3786261081696 and batch: 600, loss is 3.874917812347412 and perplexity is 48.178738430958944
At time: 566.9312837123871 and batch: 650, loss is 3.8415409469604493 and perplexity is 46.59722299437187
At time: 568.488618850708 and batch: 700, loss is 3.801278829574585 and perplexity is 44.75838625790831
At time: 570.0486073493958 and batch: 750, loss is 3.791492476463318 and perplexity is 44.32250122653299
At time: 571.5957319736481 and batch: 800, loss is 3.7464617681503296 and perplexity is 42.37089840232219
At time: 573.1418008804321 and batch: 850, loss is 3.766540093421936 and perplexity is 43.23023321534283
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.388644218444824 and perplexity of 80.53116226867475
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 577.2363395690918 and batch: 50, loss is 3.8927886199951174 and perplexity is 49.04747075211488
At time: 578.82634973526 and batch: 100, loss is 3.8517890787124633 and perplexity is 47.07721277327169
At time: 580.3806521892548 and batch: 150, loss is 3.8725126934051515 and perplexity is 48.06300207019191
At time: 581.9350943565369 and batch: 200, loss is 3.925777568817139 and perplexity is 50.692479629560424
At time: 583.4875507354736 and batch: 250, loss is 3.8854126930236816 and perplexity is 48.68703111206563
At time: 585.0381064414978 and batch: 300, loss is 3.8701275110244753 and perplexity is 47.94849965334987
At time: 586.5960049629211 and batch: 350, loss is 3.818513717651367 and perplexity is 45.53647793670353
At time: 588.1581032276154 and batch: 400, loss is 3.8202709197998046 and perplexity is 45.61656507760672
At time: 589.7196824550629 and batch: 450, loss is 3.865643763542175 and perplexity is 47.733991947314166
At time: 591.2703211307526 and batch: 500, loss is 3.8359485912322997 and perplexity is 46.337362042135986
At time: 592.8170821666718 and batch: 550, loss is 3.83631112575531 and perplexity is 46.354163981038916
At time: 594.3635885715485 and batch: 600, loss is 3.8702992248535155 and perplexity is 47.95673378075862
At time: 595.9153509140015 and batch: 650, loss is 3.8339705085754394 and perplexity is 46.24579350484108
At time: 597.470174074173 and batch: 700, loss is 3.7956391286849978 and perplexity is 44.50667280856607
At time: 599.0201935768127 and batch: 750, loss is 3.7773125743865967 and perplexity is 43.69844746643593
At time: 600.5692460536957 and batch: 800, loss is 3.7273999452590942 and perplexity is 41.57088096054799
At time: 602.1207294464111 and batch: 850, loss is 3.7505119848251343 and perplexity is 42.54285772273922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.385538419087728 and perplexity of 80.28143663626628
Finished 21 epochs...
Completing Train Step...
At time: 606.356317281723 and batch: 50, loss is 3.888919653892517 and perplexity is 48.85807437070104
At time: 607.9065909385681 and batch: 100, loss is 3.8452894401550295 and perplexity is 46.772220150567165
At time: 609.4561061859131 and batch: 150, loss is 3.8660666036605833 and perplexity is 47.75418006199287
At time: 611.0046517848969 and batch: 200, loss is 3.918280873298645 and perplexity is 50.313874461679376
At time: 612.5536677837372 and batch: 250, loss is 3.8783354330062867 and perplexity is 48.34367677053575
At time: 614.1016492843628 and batch: 300, loss is 3.8645704889297487 and perplexity is 47.68278774860587
At time: 615.6517848968506 and batch: 350, loss is 3.812414708137512 and perplexity is 45.259595736413246
At time: 617.202311038971 and batch: 400, loss is 3.814222421646118 and perplexity is 45.34148611389049
At time: 618.7550928592682 and batch: 450, loss is 3.8607332611083987 and perplexity is 47.50016862851997
At time: 620.3076047897339 and batch: 500, loss is 3.8315538930892945 and perplexity is 46.134170133795415
At time: 621.8545114994049 and batch: 550, loss is 3.832775936126709 and perplexity is 46.19058253736634
At time: 623.4039082527161 and batch: 600, loss is 3.8680507564544677 and perplexity is 47.84902571479985
At time: 624.9528138637543 and batch: 650, loss is 3.8328881311416625 and perplexity is 46.195765181192805
At time: 626.5057520866394 and batch: 700, loss is 3.7953264474868775 and perplexity is 44.49275858425948
At time: 628.0550801753998 and batch: 750, loss is 3.7781425762176513 and perplexity is 43.734732314009214
At time: 629.6053631305695 and batch: 800, loss is 3.7304033756256105 and perplexity is 41.69592389165916
At time: 631.1575911045074 and batch: 850, loss is 3.7550057792663574 and perplexity is 42.734466783704924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3851369222005205 and perplexity of 80.24921035916418
Finished 22 epochs...
Completing Train Step...
At time: 635.4074058532715 and batch: 50, loss is 3.88797269821167 and perplexity is 48.81182983883724
At time: 636.961580991745 and batch: 100, loss is 3.8435072326660156 and perplexity is 46.68893658587758
At time: 638.5141727924347 and batch: 150, loss is 3.8637263917922975 and perplexity is 47.64255582617562
At time: 640.0689787864685 and batch: 200, loss is 3.915637421607971 and perplexity is 50.18104780294466
At time: 641.6200714111328 and batch: 250, loss is 3.8756261968612673 and perplexity is 48.212879594268514
At time: 643.1692695617676 and batch: 300, loss is 3.862281918525696 and perplexity is 47.573787107177395
At time: 644.7469992637634 and batch: 350, loss is 3.8101494979858397 and perplexity is 45.157189270562355
At time: 646.3023478984833 and batch: 400, loss is 3.8118917083740236 and perplexity is 45.23593116736893
At time: 647.8559830188751 and batch: 450, loss is 3.85882212638855 and perplexity is 47.409476097481836
At time: 649.4105460643768 and batch: 500, loss is 3.8299076414108275 and perplexity is 46.058284159622644
At time: 650.9668896198273 and batch: 550, loss is 3.831485080718994 and perplexity is 46.130995641420085
At time: 652.5238490104675 and batch: 600, loss is 3.8673676252365112 and perplexity is 47.816349713854166
At time: 654.0808637142181 and batch: 650, loss is 3.8327619552612306 and perplexity is 46.18993675755981
At time: 655.6345212459564 and batch: 700, loss is 3.7954022598266604 and perplexity is 44.496131812255676
At time: 657.1884295940399 and batch: 750, loss is 3.7787411785125733 and perplexity is 43.76091986232147
At time: 658.7422404289246 and batch: 800, loss is 3.7319459390640257 and perplexity is 41.76029213266927
At time: 660.2999589443207 and batch: 850, loss is 3.7569857311248778 and perplexity is 42.81916278997487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.385021527608235 and perplexity of 80.23995056852874
Finished 23 epochs...
Completing Train Step...
At time: 664.4714455604553 and batch: 50, loss is 3.887026605606079 and perplexity is 48.765671166193826
At time: 666.0281040668488 and batch: 100, loss is 3.8422735738754272 and perplexity is 46.63137388250316
At time: 667.5800635814667 and batch: 150, loss is 3.862119779586792 and perplexity is 47.56607416911689
At time: 669.133397102356 and batch: 200, loss is 3.913948974609375 and perplexity is 50.09639125255653
At time: 670.6833288669586 and batch: 250, loss is 3.8739269256591795 and perplexity is 48.131022404883595
At time: 672.2379515171051 and batch: 300, loss is 3.8608364868164062 and perplexity is 47.50507212013597
At time: 673.7889280319214 and batch: 350, loss is 3.8087827014923095 and perplexity is 45.095510743209104
At time: 675.3413541316986 and batch: 400, loss is 3.8105366945266725 and perplexity is 45.17467736348771
At time: 676.8929259777069 and batch: 450, loss is 3.8576872682571413 and perplexity is 47.35570358588395
At time: 678.4448463916779 and batch: 500, loss is 3.828874683380127 and perplexity is 46.01073244880861
At time: 679.9967131614685 and batch: 550, loss is 3.830746946334839 and perplexity is 46.096957331330835
At time: 681.5585675239563 and batch: 600, loss is 3.8669798374176025 and perplexity is 47.79781071072257
At time: 683.1094031333923 and batch: 650, loss is 3.832700181007385 and perplexity is 46.187083496811375
At time: 684.710999250412 and batch: 700, loss is 3.795478830337524 and perplexity is 44.499539034244705
At time: 686.2629613876343 and batch: 750, loss is 3.779074845314026 and perplexity is 43.77552386478046
At time: 687.8176679611206 and batch: 800, loss is 3.732785782814026 and perplexity is 41.79537898468942
At time: 689.3711268901825 and batch: 850, loss is 3.758001751899719 and perplexity is 42.862690057490404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3849843343098955 and perplexity of 80.23696623560741
Finished 24 epochs...
Completing Train Step...
At time: 693.4589178562164 and batch: 50, loss is 3.8860568761825562 and perplexity is 48.718404581609065
At time: 695.0375890731812 and batch: 100, loss is 3.8411881160736083 and perplexity is 46.580784954952975
At time: 696.589302778244 and batch: 150, loss is 3.8608013677597044 and perplexity is 47.50340381610936
At time: 698.1360504627228 and batch: 200, loss is 3.912644920349121 and perplexity is 50.03110541750782
At time: 699.6865696907043 and batch: 250, loss is 3.872643370628357 and perplexity is 48.06928322023404
At time: 701.2400615215302 and batch: 300, loss is 3.859693350791931 and perplexity is 47.45079838788964
At time: 702.788806438446 and batch: 350, loss is 3.807760486602783 and perplexity is 45.04943699332512
At time: 704.3368363380432 and batch: 400, loss is 3.8095539808273315 and perplexity is 45.13030539522077
At time: 705.8818809986115 and batch: 450, loss is 3.8568422412872314 and perplexity is 47.315703642070716
At time: 707.4305851459503 and batch: 500, loss is 3.828095932006836 and perplexity is 45.97491547579654
At time: 708.9746744632721 and batch: 550, loss is 3.830192003250122 and perplexity is 46.071383240372114
At time: 710.5266072750092 and batch: 600, loss is 3.86665207862854 and perplexity is 47.782147125243306
At time: 712.0750820636749 and batch: 650, loss is 3.832606554031372 and perplexity is 46.182759342284626
At time: 713.6308808326721 and batch: 700, loss is 3.795508794784546 and perplexity is 44.500872458302155
At time: 715.1841361522675 and batch: 750, loss is 3.779242362976074 and perplexity is 43.78285765244555
At time: 716.7308125495911 and batch: 800, loss is 3.733280301094055 and perplexity is 41.81605267495567
At time: 718.274053812027 and batch: 850, loss is 3.758591012954712 and perplexity is 42.887954814491145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384971300760905 and perplexity of 80.23592046999218
Finished 25 epochs...
Completing Train Step...
At time: 722.3508327007294 and batch: 50, loss is 3.885093717575073 and perplexity is 48.6715036210511
At time: 723.9392268657684 and batch: 100, loss is 3.8401808977127074 and perplexity is 46.5338915529933
At time: 725.4962866306305 and batch: 150, loss is 3.859640326499939 and perplexity is 47.44828240960523
At time: 727.0447969436646 and batch: 200, loss is 3.911544909477234 and perplexity is 49.976100915937295
At time: 728.593118429184 and batch: 250, loss is 3.8715746545791627 and perplexity is 48.01793824726863
At time: 730.1504039764404 and batch: 300, loss is 3.85872278213501 and perplexity is 47.40476647240917
At time: 731.7016763687134 and batch: 350, loss is 3.8069068384170532 and perplexity is 45.010997032603306
At time: 733.2418529987335 and batch: 400, loss is 3.8087432479858396 and perplexity is 45.09373160228114
At time: 734.7880597114563 and batch: 450, loss is 3.856131658554077 and perplexity is 47.28209386273425
At time: 736.3384647369385 and batch: 500, loss is 3.827438025474548 and perplexity is 45.94467822631627
At time: 737.8948509693146 and batch: 550, loss is 3.8297098016738893 and perplexity is 46.04917290211479
At time: 739.4436938762665 and batch: 600, loss is 3.866334104537964 and perplexity is 47.76695605577663
At time: 740.995519399643 and batch: 650, loss is 3.8324764728546143 and perplexity is 46.17675222531835
At time: 742.5534365177155 and batch: 700, loss is 3.7954958200454714 and perplexity is 44.50029507483912
At time: 744.1064386367798 and batch: 750, loss is 3.7793094396591185 and perplexity is 43.785794559808984
At time: 745.6593904495239 and batch: 800, loss is 3.7335901737213133 and perplexity is 41.82901233287755
At time: 747.2092485427856 and batch: 850, loss is 3.758965086936951 and perplexity is 42.90400108359746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384973208109538 and perplexity of 80.23607350801134
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 751.3226475715637 and batch: 50, loss is 3.885437693595886 and perplexity is 48.688248330917894
At time: 752.8644776344299 and batch: 100, loss is 3.8420648097991945 and perplexity is 46.62163994289492
At time: 754.4075334072113 and batch: 150, loss is 3.8613099479675292 and perplexity is 47.527569251604774
At time: 755.9490692615509 and batch: 200, loss is 3.9125537204742433 and perplexity is 50.026542795012205
At time: 757.5056524276733 and batch: 250, loss is 3.8719473028182985 and perplexity is 48.035835381864146
At time: 759.0485899448395 and batch: 300, loss is 3.859803442955017 and perplexity is 47.45602263649345
At time: 760.589391708374 and batch: 350, loss is 3.808180365562439 and perplexity is 45.068356275689325
At time: 762.1357645988464 and batch: 400, loss is 3.8089950704574584 and perplexity is 45.10508864714728
At time: 763.7238049507141 and batch: 450, loss is 3.8550020456314087 and perplexity is 47.22871355371574
At time: 765.2687654495239 and batch: 500, loss is 3.8267046451568603 and perplexity is 45.91099565617925
At time: 766.8118529319763 and batch: 550, loss is 3.829127297401428 and perplexity is 46.02235687313998
At time: 768.3548247814178 and batch: 600, loss is 3.863906421661377 and perplexity is 47.651133681375526
At time: 769.8972508907318 and batch: 650, loss is 3.8288957977294924 and perplexity is 46.011703945744316
At time: 771.4413254261017 and batch: 700, loss is 3.7919335889816286 and perplexity is 44.34205674944274
At time: 772.9842758178711 and batch: 750, loss is 3.7748858547210693 and perplexity is 43.59253214998443
At time: 774.5242340564728 and batch: 800, loss is 3.727862777709961 and perplexity is 41.590125766484455
At time: 776.0661907196045 and batch: 850, loss is 3.7537894821166993 and perplexity is 42.68252057098299
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384634017944336 and perplexity of 80.20886283604003
Finished 27 epochs...
Completing Train Step...
At time: 780.360472202301 and batch: 50, loss is 3.8849758243560792 and perplexity is 48.66576591904067
At time: 781.9012007713318 and batch: 100, loss is 3.841446270942688 and perplexity is 46.59281156369167
At time: 783.4465148448944 and batch: 150, loss is 3.8605473709106444 and perplexity is 47.491339633417496
At time: 785.0010900497437 and batch: 200, loss is 3.9116746854782103 and perplexity is 49.98258703532076
At time: 786.556492805481 and batch: 250, loss is 3.8712134408950805 and perplexity is 48.000596643093225
At time: 788.1088788509369 and batch: 300, loss is 3.8591545820236206 and perplexity is 47.4252402652642
At time: 789.6595368385315 and batch: 350, loss is 3.80746741771698 and perplexity is 45.03623633946439
At time: 791.2112135887146 and batch: 400, loss is 3.8083908271789553 and perplexity is 45.07784243300582
At time: 792.7670826911926 and batch: 450, loss is 3.854640965461731 and perplexity is 47.21166328025517
At time: 794.3207716941833 and batch: 500, loss is 3.826370234489441 and perplexity is 45.895645096318525
At time: 795.8707559108734 and batch: 550, loss is 3.8288548135757448 and perplexity is 46.00981823363803
At time: 797.4240658283234 and batch: 600, loss is 3.8638588619232177 and perplexity is 47.648867459825524
At time: 798.9793374538422 and batch: 650, loss is 3.829005813598633 and perplexity is 46.01676624180588
At time: 800.5315961837769 and batch: 700, loss is 3.792231731414795 and perplexity is 44.35527896908805
At time: 802.0828218460083 and batch: 750, loss is 3.7751590394973755 and perplexity is 43.60444259293022
At time: 803.6782884597778 and batch: 800, loss is 3.7282392883300783 and perplexity is 41.60578783881081
At time: 805.2320365905762 and batch: 850, loss is 3.7543365144729615 and perplexity is 42.7058756781994
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384574890136719 and perplexity of 80.20412040203531
Finished 28 epochs...
Completing Train Step...
At time: 809.3970143795013 and batch: 50, loss is 3.8847155475616457 and perplexity is 48.65310099775267
At time: 810.9505257606506 and batch: 100, loss is 3.8410739135742187 and perplexity is 46.57546561663424
At time: 812.500410079956 and batch: 150, loss is 3.8600379610061646 and perplexity is 47.46715323554957
At time: 814.0505397319794 and batch: 200, loss is 3.91109582901001 and perplexity is 49.95366266385538
At time: 815.6058897972107 and batch: 250, loss is 3.8706938457489013 and perplexity is 47.97566224452098
At time: 817.1605434417725 and batch: 300, loss is 3.858685779571533 and perplexity is 47.40301240698133
At time: 818.7116096019745 and batch: 350, loss is 3.806969165802002 and perplexity is 45.01380253777128
At time: 820.2642526626587 and batch: 400, loss is 3.8079602193832396 and perplexity is 45.05843574127637
At time: 821.8167712688446 and batch: 450, loss is 3.8543947649002077 and perplexity is 47.200041172988506
At time: 823.37779712677 and batch: 500, loss is 3.826160740852356 and perplexity is 45.88603125775511
At time: 824.9279673099518 and batch: 550, loss is 3.8286852836608887 and perplexity is 46.00201885420309
At time: 826.4781770706177 and batch: 600, loss is 3.8638553476333617 and perplexity is 47.6487000081882
At time: 828.0343816280365 and batch: 650, loss is 3.82911723613739 and perplexity is 46.021893832385224
At time: 829.5931551456451 and batch: 700, loss is 3.7924687385559084 and perplexity is 44.36579273281912
At time: 831.142171382904 and batch: 750, loss is 3.7753814125061034 and perplexity is 43.614140122217925
At time: 832.6950056552887 and batch: 800, loss is 3.728546781539917 and perplexity is 41.61858330321962
At time: 834.2548451423645 and batch: 850, loss is 3.7547585344314576 and perplexity is 42.72390221359197
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384545008341472 and perplexity of 80.20172379473912
Finished 29 epochs...
Completing Train Step...
At time: 838.4061591625214 and batch: 50, loss is 3.8844977521896364 and perplexity is 48.642505731363336
At time: 840.0028929710388 and batch: 100, loss is 3.8407736349105837 and perplexity is 46.561482097641964
At time: 841.5629844665527 and batch: 150, loss is 3.8596290731430054 and perplexity is 47.44774846015176
At time: 843.1605563163757 and batch: 200, loss is 3.9106458806991578 and perplexity is 49.93119115360706
At time: 844.7160730361938 and batch: 250, loss is 3.870274896621704 and perplexity is 47.95556709241408
At time: 846.2694623470306 and batch: 300, loss is 3.8583096647262574 and perplexity is 47.38518678275537
At time: 847.8263576030731 and batch: 350, loss is 3.8065802955627444 and perplexity is 44.99630141266242
At time: 849.3792102336884 and batch: 400, loss is 3.80762255191803 and perplexity is 45.04322354197022
At time: 850.9287073612213 and batch: 450, loss is 3.854199686050415 and perplexity is 47.19083434130458
At time: 852.4815850257874 and batch: 500, loss is 3.826008424758911 and perplexity is 45.87904260898557
At time: 854.0389287471771 and batch: 550, loss is 3.828562755584717 and perplexity is 45.99638266063589
At time: 855.596426486969 and batch: 600, loss is 3.863859429359436 and perplexity is 47.64889449752636
At time: 857.1522524356842 and batch: 650, loss is 3.8292096567153933 and perplexity is 46.02614739896952
At time: 858.709291934967 and batch: 700, loss is 3.7926501035690308 and perplexity is 44.373839865112366
At time: 860.2715215682983 and batch: 750, loss is 3.775558786392212 and perplexity is 43.62187681786429
At time: 861.830557346344 and batch: 800, loss is 3.728800268173218 and perplexity is 41.62913439500779
At time: 863.3861606121063 and batch: 850, loss is 3.7550927019119262 and perplexity is 42.73818153806054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384527206420898 and perplexity of 80.20029606273052
Finished 30 epochs...
Completing Train Step...
At time: 867.527019739151 and batch: 50, loss is 3.8842986154556276 and perplexity is 48.63282018604392
At time: 869.1178669929504 and batch: 100, loss is 3.8405138778686525 and perplexity is 46.54938899548657
At time: 870.6722459793091 and batch: 150, loss is 3.8592799282073975 and perplexity is 47.43118521072649
At time: 872.228435754776 and batch: 200, loss is 3.910272397994995 and perplexity is 49.91254619931366
At time: 873.7809672355652 and batch: 250, loss is 3.8699186277389526 and perplexity is 47.93848505918347
At time: 875.336891412735 and batch: 300, loss is 3.857991886138916 and perplexity is 47.37013117733986
At time: 876.8901274204254 and batch: 350, loss is 3.8062616920471193 and perplexity is 44.98196771634648
At time: 878.4543716907501 and batch: 400, loss is 3.8073432970047 and perplexity is 45.03064675663019
At time: 880.0041670799255 and batch: 450, loss is 3.8540336799621584 and perplexity is 47.18300102570104
At time: 881.6951069831848 and batch: 500, loss is 3.8258863925933837 and perplexity is 45.87344423166168
At time: 883.2457237243652 and batch: 550, loss is 3.828464860916138 and perplexity is 45.991880080392384
At time: 884.7969751358032 and batch: 600, loss is 3.863860683441162 and perplexity is 47.64895425317169
At time: 886.3503608703613 and batch: 650, loss is 3.829281520843506 and perplexity is 46.02945514677552
At time: 887.903712272644 and batch: 700, loss is 3.7927879524230956 and perplexity is 44.37995716971025
At time: 889.4565253257751 and batch: 750, loss is 3.7757001399993895 and perplexity is 43.62804336332581
At time: 891.0078465938568 and batch: 800, loss is 3.729010558128357 and perplexity is 41.6378895043357
At time: 892.5577433109283 and batch: 850, loss is 3.755362811088562 and perplexity is 42.749727072293865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384515126546224 and perplexity of 80.19932725905673
Finished 31 epochs...
Completing Train Step...
At time: 896.6391537189484 and batch: 50, loss is 3.884109477996826 and perplexity is 48.62362276783516
At time: 898.2162880897522 and batch: 100, loss is 3.840281157493591 and perplexity is 46.53855726465178
At time: 899.7711682319641 and batch: 150, loss is 3.858971133232117 and perplexity is 47.41654096021366
At time: 901.3243334293365 and batch: 200, loss is 3.9099494886398314 and perplexity is 49.89643157312766
At time: 902.8740940093994 and batch: 250, loss is 3.8696055221557617 and perplexity is 47.923477601443736
At time: 904.4240057468414 and batch: 300, loss is 3.8577143287658693 and perplexity is 47.356985072652925
At time: 905.9763510227203 and batch: 350, loss is 3.8059916591644285 and perplexity is 44.96982274577944
At time: 907.528256893158 and batch: 400, loss is 3.807103605270386 and perplexity is 45.0198545762617
At time: 909.0778839588165 and batch: 450, loss is 3.853886365890503 and perplexity is 47.17605081765131
At time: 910.6261208057404 and batch: 500, loss is 3.8257812547683714 and perplexity is 45.86862145104212
At time: 912.175509929657 and batch: 550, loss is 3.82838089466095 and perplexity is 45.98801847657746
At time: 913.7239427566528 and batch: 600, loss is 3.863855981826782 and perplexity is 47.648730226689814
At time: 915.2733445167542 and batch: 650, loss is 3.8293352270126344 and perplexity is 46.0319272788623
At time: 916.8248956203461 and batch: 700, loss is 3.792892518043518 and perplexity is 44.38459803009902
At time: 918.3757176399231 and batch: 750, loss is 3.7758128356933596 and perplexity is 43.63296033300465
At time: 919.9303166866302 and batch: 800, loss is 3.7291866445541384 and perplexity is 41.645222017034634
At time: 921.5088453292847 and batch: 850, loss is 3.755584797859192 and perplexity is 42.759217999543125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.38450304667155 and perplexity of 80.19835846708597
Finished 32 epochs...
Completing Train Step...
At time: 925.6446852684021 and batch: 50, loss is 3.883926982879639 and perplexity is 48.61475000374275
At time: 927.1958343982697 and batch: 100, loss is 3.840067129135132 and perplexity is 46.52859775948165
At time: 928.742844581604 and batch: 150, loss is 3.8586915349960327 and perplexity is 47.40328523222541
At time: 930.2921807765961 and batch: 200, loss is 3.9096624851226807 and perplexity is 49.882113176586266
At time: 931.8375377655029 and batch: 250, loss is 3.8693239974975584 and perplexity is 47.90998785972864
At time: 933.3872628211975 and batch: 300, loss is 3.857466096878052 and perplexity is 47.34523101777297
At time: 934.9390041828156 and batch: 350, loss is 3.805756516456604 and perplexity is 44.95924966302898
At time: 936.4865605831146 and batch: 400, loss is 3.8068921899795534 and perplexity is 45.010337696655604
At time: 938.0366303920746 and batch: 450, loss is 3.8537520122528077 and perplexity is 47.16971296937788
At time: 939.5856242179871 and batch: 500, loss is 3.825686206817627 and perplexity is 45.86426193975432
At time: 941.1403985023499 and batch: 550, loss is 3.8283053493499755 and perplexity is 45.98454442864622
At time: 942.690630197525 and batch: 600, loss is 3.8638455057144165 and perplexity is 47.64823105585258
At time: 944.2439434528351 and batch: 650, loss is 3.8293736600875854 and perplexity is 46.03369646137089
At time: 945.794370174408 and batch: 700, loss is 3.792971382141113 and perplexity is 44.388098519399435
At time: 947.3463654518127 and batch: 750, loss is 3.7759031581878664 and perplexity is 43.63690154881218
At time: 948.9009790420532 and batch: 800, loss is 3.72933566570282 and perplexity is 41.6514284982937
At time: 950.4474020004272 and batch: 850, loss is 3.7557700204849245 and perplexity is 42.76713870769981
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384496688842773 and perplexity of 80.19784858127557
Finished 33 epochs...
Completing Train Step...
At time: 954.5205409526825 and batch: 50, loss is 3.883749098777771 and perplexity is 48.606102981707494
At time: 956.065185546875 and batch: 100, loss is 3.839866781234741 and perplexity is 46.51927678636229
At time: 957.6070408821106 and batch: 150, loss is 3.85843373298645 and perplexity is 47.391066145151996
At time: 959.1533169746399 and batch: 200, loss is 3.909401879310608 and perplexity is 49.869115301708405
At time: 960.7393612861633 and batch: 250, loss is 3.869066262245178 and perplexity is 47.89764135804879
At time: 962.2923836708069 and batch: 300, loss is 3.8572397947311403 and perplexity is 47.334517902593824
At time: 963.8440942764282 and batch: 350, loss is 3.805547399520874 and perplexity is 44.94984890546979
At time: 965.3998436927795 and batch: 400, loss is 3.8067016410827637 and perplexity is 45.00176184354899
At time: 966.9518203735352 and batch: 450, loss is 3.853626918792725 and perplexity is 47.16381271582069
At time: 968.5068264007568 and batch: 500, loss is 3.8255975341796873 and perplexity is 45.86019521496723
At time: 970.0622227191925 and batch: 550, loss is 3.8282349491119385 and perplexity is 45.981307219723895
At time: 971.6143095493317 and batch: 600, loss is 3.8638292264938356 and perplexity is 47.64745538610259
At time: 973.1668524742126 and batch: 650, loss is 3.829399709701538 and perplexity is 46.03489563701148
At time: 974.725997209549 and batch: 700, loss is 3.7930309867858885 and perplexity is 44.3907443350946
At time: 976.2861242294312 and batch: 750, loss is 3.775975580215454 and perplexity is 43.640061936139446
At time: 977.8389511108398 and batch: 800, loss is 3.7294629621505737 and perplexity is 41.656730914667634
At time: 979.3934891223907 and batch: 850, loss is 3.755926375389099 and perplexity is 42.77382608236256
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.38448969523112 and perplexity of 80.19728771062842
Finished 34 epochs...
Completing Train Step...
At time: 983.5683019161224 and batch: 50, loss is 3.8835751533508303 and perplexity is 48.597648907667484
At time: 985.1205902099609 and batch: 100, loss is 3.839677166938782 and perplexity is 46.51045690266036
At time: 986.6750349998474 and batch: 150, loss is 3.8581931972503662 and perplexity is 47.379668271025956
At time: 988.231645822525 and batch: 200, loss is 3.909161672592163 and perplexity is 49.857137843760555
At time: 989.7817561626434 and batch: 250, loss is 3.8688273572921754 and perplexity is 47.8861997410751
At time: 991.3332664966583 and batch: 300, loss is 3.8570305585861204 and perplexity is 47.32461484661627
At time: 992.8815829753876 and batch: 350, loss is 3.805357508659363 and perplexity is 44.941314150298
At time: 994.4327847957611 and batch: 400, loss is 3.8065268850326537 and perplexity is 44.993898200530836
At time: 995.9849355220795 and batch: 450, loss is 3.8535087871551514 and perplexity is 47.15824150646488
At time: 997.5347988605499 and batch: 500, loss is 3.825512771606445 and perplexity is 45.856308151552504
At time: 999.083774805069 and batch: 550, loss is 3.8281676626205443 and perplexity is 45.97821340297857
At time: 1000.6642792224884 and batch: 600, loss is 3.863808455467224 and perplexity is 47.64646570981713
At time: 1002.2164413928986 and batch: 650, loss is 3.8294157552719117 and perplexity is 46.035634299095186
At time: 1003.7675096988678 and batch: 700, loss is 3.793075370788574 and perplexity is 44.39271461773458
At time: 1005.3153738975525 and batch: 750, loss is 3.77603374004364 and perplexity is 43.64260010845279
At time: 1006.8693141937256 and batch: 800, loss is 3.7295726776123046 and perplexity is 41.66130155286439
At time: 1008.4189710617065 and batch: 850, loss is 3.756060118675232 and perplexity is 42.779547176993766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384483973185222 and perplexity of 80.1968288193801
Finished 35 epochs...
Completing Train Step...
At time: 1012.4442136287689 and batch: 50, loss is 3.883404245376587 and perplexity is 48.58934389165664
At time: 1014.0206773281097 and batch: 100, loss is 3.8394959211349486 and perplexity is 46.50202784140145
At time: 1015.5705103874207 and batch: 150, loss is 3.8579659128189085 and perplexity is 47.3689008337421
At time: 1017.1200783252716 and batch: 200, loss is 3.9089374017715453 and perplexity is 49.845957596291186
At time: 1018.669753074646 and batch: 250, loss is 3.8686034202575685 and perplexity is 47.875477448110544
At time: 1020.2222406864166 and batch: 300, loss is 3.856834707260132 and perplexity is 47.315347165620096
At time: 1021.7705039978027 and batch: 350, loss is 3.805182523727417 and perplexity is 44.93345078550508
At time: 1023.3205494880676 and batch: 400, loss is 3.8063642740249635 and perplexity is 44.986582292244265
At time: 1024.8652951717377 and batch: 450, loss is 3.853395791053772 and perplexity is 47.15291311007643
At time: 1026.4042673110962 and batch: 500, loss is 3.825430655479431 and perplexity is 45.85254276372957
At time: 1027.9404456615448 and batch: 550, loss is 3.828102231025696 and perplexity is 45.97520507356829
At time: 1029.4804906845093 and batch: 600, loss is 3.8637834644317626 and perplexity is 47.645274990181676
At time: 1031.0197639465332 and batch: 650, loss is 3.8294235372543337 and perplexity is 46.03599254898603
At time: 1032.5631911754608 and batch: 700, loss is 3.7931080532073973 and perplexity is 44.394165502735504
At time: 1034.1020793914795 and batch: 750, loss is 3.7760802936553954 and perplexity is 43.64463187640693
At time: 1035.6404128074646 and batch: 800, loss is 3.72966814994812 and perplexity is 41.665279244513485
At time: 1037.1802196502686 and batch: 850, loss is 3.756175675392151 and perplexity is 42.784490926653056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384479522705078 and perplexity of 80.19647190578009
Finished 36 epochs...
Completing Train Step...
At time: 1041.2757458686829 and batch: 50, loss is 3.883235802650452 and perplexity is 48.581160059383265
At time: 1042.8582108020782 and batch: 100, loss is 3.8393211555480957 and perplexity is 46.493901597330485
At time: 1044.4178292751312 and batch: 150, loss is 3.8577497816085815 and perplexity is 47.358664042157976
At time: 1046.0059010982513 and batch: 200, loss is 3.9087259769439697 and perplexity is 49.8354200372911
At time: 1047.578087568283 and batch: 250, loss is 3.8683917999267576 and perplexity is 47.86534709566732
At time: 1049.1633768081665 and batch: 300, loss is 3.8566497325897218 and perplexity is 47.30659583428506
At time: 1050.7190401554108 and batch: 350, loss is 3.8050193691253664 and perplexity is 44.92612028424219
At time: 1052.2728135585785 and batch: 400, loss is 3.8062111234664915 and perplexity is 44.97969309959778
At time: 1053.8283479213715 and batch: 450, loss is 3.8532871198654175 and perplexity is 47.147789225388735
At time: 1055.3857276439667 and batch: 500, loss is 3.8253503370285036 and perplexity is 45.84886010641835
At time: 1056.9387435913086 and batch: 550, loss is 3.828037838935852 and perplexity is 45.97224472934501
At time: 1058.4928741455078 and batch: 600, loss is 3.8637550115585326 and perplexity is 47.64391936449819
At time: 1060.0466187000275 and batch: 650, loss is 3.829424624443054 and perplexity is 46.03604259882507
At time: 1061.6001403331757 and batch: 700, loss is 3.7931315422058107 and perplexity is 44.39520828946552
At time: 1063.159734249115 and batch: 750, loss is 3.776117358207703 and perplexity is 43.646249575127506
At time: 1064.714303970337 and batch: 800, loss is 3.7297517585754396 and perplexity is 41.6687629669506
At time: 1066.268346786499 and batch: 850, loss is 3.7562765789031984 and perplexity is 42.78880824981881
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384477933247884 and perplexity of 80.19634443702216
Finished 37 epochs...
Completing Train Step...
At time: 1070.4120450019836 and batch: 50, loss is 3.8830696678161623 and perplexity is 48.57308970680911
At time: 1071.9630210399628 and batch: 100, loss is 3.839151830673218 and perplexity is 46.486029689733634
At time: 1073.511709690094 and batch: 150, loss is 3.8575427198410033 and perplexity is 47.3488588886425
At time: 1075.0642654895782 and batch: 200, loss is 3.908524875640869 and perplexity is 49.82539907702889
At time: 1076.6084320545197 and batch: 250, loss is 3.8681901597976687 and perplexity is 47.855696493907054
At time: 1078.153401851654 and batch: 300, loss is 3.856473650932312 and perplexity is 47.298266743805684
At time: 1079.7515790462494 and batch: 350, loss is 3.8048655939102174 and perplexity is 44.919212291582305
At time: 1081.306012392044 and batch: 400, loss is 3.806065640449524 and perplexity is 44.97314979412482
At time: 1082.857327222824 and batch: 450, loss is 3.8531815910339358 and perplexity is 47.14281403680237
At time: 1084.4085710048676 and batch: 500, loss is 3.825271654129028 and perplexity is 45.845252727088926
At time: 1085.9655132293701 and batch: 550, loss is 3.8279743242263793 and perplexity is 45.96932490830399
At time: 1087.520271062851 and batch: 600, loss is 3.863723511695862 and perplexity is 47.642418611218
At time: 1089.0710508823395 and batch: 650, loss is 3.8294203090667724 and perplexity is 46.03584393640739
At time: 1090.620359659195 and batch: 700, loss is 3.793147497177124 and perplexity is 44.39591661939092
At time: 1092.1700625419617 and batch: 750, loss is 3.7761465549468993 and perplexity is 43.64752392189655
At time: 1093.7241382598877 and batch: 800, loss is 3.729825487136841 and perplexity is 41.671835258155944
At time: 1095.2799081802368 and batch: 850, loss is 3.756365418434143 and perplexity is 42.79260975633293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384474754333496 and perplexity of 80.1960895001142
Finished 38 epochs...
Completing Train Step...
At time: 1099.4075314998627 and batch: 50, loss is 3.8829053592681886 and perplexity is 48.56510938860399
At time: 1100.9585478305817 and batch: 100, loss is 3.838986873626709 and perplexity is 46.47836212399897
At time: 1102.5123126506805 and batch: 150, loss is 3.857343292236328 and perplexity is 47.33941716063214
At time: 1104.0635623931885 and batch: 200, loss is 3.9083326625823975 and perplexity is 49.81582290504487
At time: 1105.6174204349518 and batch: 250, loss is 3.86799711227417 and perplexity is 47.84645896188362
At time: 1107.1679773330688 and batch: 300, loss is 3.8563050127029417 and perplexity is 47.29029112036614
At time: 1108.7194159030914 and batch: 350, loss is 3.804719295501709 and perplexity is 44.91264116299714
At time: 1110.2711699008942 and batch: 400, loss is 3.8059265089035033 and perplexity is 44.9668930455302
At time: 1111.8180601596832 and batch: 450, loss is 3.853078818321228 and perplexity is 47.137969290877244
At time: 1113.367639541626 and batch: 500, loss is 3.8251938915252683 and perplexity is 45.84168781947689
At time: 1114.9200975894928 and batch: 550, loss is 3.8279111194610596 and perplexity is 45.96641951972942
At time: 1116.4739141464233 and batch: 600, loss is 3.8636895036697387 and perplexity is 47.640798414151305
At time: 1118.0727915763855 and batch: 650, loss is 3.8294112396240236 and perplexity is 46.035426418849745
At time: 1119.6215405464172 and batch: 700, loss is 3.7931576681137087 and perplexity is 44.3963681697398
At time: 1121.1716377735138 and batch: 750, loss is 3.7761692237854003 and perplexity is 43.6485133717821
At time: 1122.7228722572327 and batch: 800, loss is 3.729891047477722 and perplexity is 41.67456736743864
At time: 1124.2737865447998 and batch: 850, loss is 3.7564442443847654 and perplexity is 42.79598305742667
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.38447380065918 and perplexity of 80.19601301919984
Finished 39 epochs...
Completing Train Step...
At time: 1128.3851001262665 and batch: 50, loss is 3.8827431106567385 and perplexity is 48.55723040623503
At time: 1129.9300479888916 and batch: 100, loss is 3.8388258171081544 and perplexity is 46.47087708358062
At time: 1131.473420381546 and batch: 150, loss is 3.8571503925323487 and perplexity is 47.330286281775514
At time: 1133.0201354026794 and batch: 200, loss is 3.9081476640701296 and perplexity is 49.806607904327024
At time: 1134.568499326706 and batch: 250, loss is 3.867811245918274 and perplexity is 47.837566741321766
At time: 1136.1085913181305 and batch: 300, loss is 3.8561424016952515 and perplexity is 47.28260182367218
At time: 1137.6494653224945 and batch: 350, loss is 3.804579267501831 and perplexity is 44.90635257598525
At time: 1139.1895289421082 and batch: 400, loss is 3.8057923889160157 and perplexity is 44.96086249081558
At time: 1140.7659394741058 and batch: 450, loss is 3.852978296279907 and perplexity is 47.13323112412948
At time: 1142.3302767276764 and batch: 500, loss is 3.8251169443130495 and perplexity is 45.83816056510371
At time: 1143.9219226837158 and batch: 550, loss is 3.8278482437133787 and perplexity is 45.963529437592896
At time: 1145.4736819267273 and batch: 600, loss is 3.8636534547805788 and perplexity is 47.63908104724455
At time: 1147.0193960666656 and batch: 650, loss is 3.8293981981277465 and perplexity is 46.03482605192234
At time: 1148.5643556118011 and batch: 700, loss is 3.7931626510620116 and perplexity is 44.3965893950984
At time: 1150.1099381446838 and batch: 750, loss is 3.7761863040924073 and perplexity is 43.64925890815787
At time: 1151.664356470108 and batch: 800, loss is 3.729949517250061 and perplexity is 41.677004141143044
At time: 1153.2181839942932 and batch: 850, loss is 3.7565145349502562 and perplexity is 42.7989913170014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384474436442058 and perplexity of 80.196064006468
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 1157.3487186431885 and batch: 50, loss is 3.8827893781661986 and perplexity is 48.55947708032581
At time: 1158.952963590622 and batch: 100, loss is 3.8391643953323364 and perplexity is 46.48661377451986
At time: 1160.5044476985931 and batch: 150, loss is 3.857442307472229 and perplexity is 47.34410471625558
At time: 1162.0554225444794 and batch: 200, loss is 3.908219475746155 and perplexity is 49.81018472874511
At time: 1163.5987973213196 and batch: 250, loss is 3.8678151321411134 and perplexity is 47.83775264912746
At time: 1165.141041278839 and batch: 300, loss is 3.8562664699554445 and perplexity is 47.288468457741864
At time: 1166.6830546855927 and batch: 350, loss is 3.8047049713134764 and perplexity is 44.911997830478725
At time: 1168.2254729270935 and batch: 400, loss is 3.8056425714492796 and perplexity is 44.95412707284916
At time: 1169.767786026001 and batch: 450, loss is 3.8526260328292845 and perplexity is 47.1166307335218
At time: 1171.3321034908295 and batch: 500, loss is 3.8249445581436157 and perplexity is 45.83025938123698
At time: 1172.8839869499207 and batch: 550, loss is 3.827644262313843 and perplexity is 45.954154688700314
At time: 1174.4275596141815 and batch: 600, loss is 3.8630522537231444 and perplexity is 47.610448989017996
At time: 1175.9824333190918 and batch: 650, loss is 3.828580741882324 and perplexity is 45.9972099727034
At time: 1177.5343205928802 and batch: 700, loss is 3.7923505115509033 and perplexity is 44.36054780807163
At time: 1179.0768003463745 and batch: 750, loss is 3.775354609489441 and perplexity is 43.6129711473596
At time: 1180.6354780197144 and batch: 800, loss is 3.728879599571228 and perplexity is 41.632437023436616
At time: 1182.1889743804932 and batch: 850, loss is 3.7555346727371215 and perplexity is 42.75707474223723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384381930033366 and perplexity of 80.18864569972132
Finished 41 epochs...
Completing Train Step...
At time: 1186.259403705597 and batch: 50, loss is 3.8827236890792847 and perplexity is 48.55628735738153
At time: 1187.9468557834625 and batch: 100, loss is 3.8390690660476685 and perplexity is 46.48218245010295
At time: 1189.515272140503 and batch: 150, loss is 3.857357769012451 and perplexity is 47.3401024877368
At time: 1191.0645492076874 and batch: 200, loss is 3.9081540870666505 and perplexity is 49.80692781302369
At time: 1192.6057646274567 and batch: 250, loss is 3.867750015258789 and perplexity is 47.83463770523636
At time: 1194.1489915847778 and batch: 300, loss is 3.856201844215393 and perplexity is 47.28541250421957
At time: 1195.6880424022675 and batch: 350, loss is 3.804651627540588 and perplexity is 44.90960211896521
At time: 1197.2686381340027 and batch: 400, loss is 3.805597128868103 and perplexity is 44.95208428769549
At time: 1198.8111782073975 and batch: 450, loss is 3.852584710121155 and perplexity is 47.11468378696878
At time: 1200.3525710105896 and batch: 500, loss is 3.8249020338058473 and perplexity is 45.82831052124432
At time: 1201.8933222293854 and batch: 550, loss is 3.8276122331619264 and perplexity is 45.95268283966976
At time: 1203.4339990615845 and batch: 600, loss is 3.8630480909347535 and perplexity is 47.61025079720617
At time: 1204.9743461608887 and batch: 650, loss is 3.828585262298584 and perplexity is 45.997417899709234
At time: 1206.5158598423004 and batch: 700, loss is 3.792364912033081 and perplexity is 44.361186625949365
At time: 1208.0663208961487 and batch: 750, loss is 3.7753702640533446 and perplexity is 43.61365389474751
At time: 1209.611634016037 and batch: 800, loss is 3.7289140176773072 and perplexity is 41.633869957729736
At time: 1211.15429353714 and batch: 850, loss is 3.7555745315551756 and perplexity is 42.75877902266498
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384347279866536 and perplexity of 80.18586719790801
Finished 42 epochs...
Completing Train Step...
At time: 1215.157613992691 and batch: 50, loss is 3.8826807355880737 and perplexity is 48.55420174011188
At time: 1216.7210946083069 and batch: 100, loss is 3.8390093183517457 and perplexity is 46.47940532976419
At time: 1218.26260471344 and batch: 150, loss is 3.8572922277450563 and perplexity is 47.33699985909738
At time: 1219.8121931552887 and batch: 200, loss is 3.9081030321121215 and perplexity is 49.80438498750144
At time: 1221.3709383010864 and batch: 250, loss is 3.867696614265442 and perplexity is 47.83208335626951
At time: 1222.914234161377 and batch: 300, loss is 3.856155529022217 and perplexity is 47.283222521920145
At time: 1224.4555439949036 and batch: 350, loss is 3.8046066999435424 and perplexity is 44.90758448378181
At time: 1225.9960515499115 and batch: 400, loss is 3.8055617666244506 and perplexity is 44.95049470924393
At time: 1227.5349955558777 and batch: 450, loss is 3.8525585556030273 and perplexity is 47.11345154123207
At time: 1229.0834610462189 and batch: 500, loss is 3.82487407207489 and perplexity is 45.82702910027076
At time: 1230.6259264945984 and batch: 550, loss is 3.8275929498672485 and perplexity is 45.95179672908892
At time: 1232.1769955158234 and batch: 600, loss is 3.8630472946166994 and perplexity is 47.610212884319
At time: 1233.7302885055542 and batch: 650, loss is 3.8285932826995848 and perplexity is 45.99778681892522
At time: 1235.2819967269897 and batch: 700, loss is 3.7923801803588866 and perplexity is 44.361863952170694
At time: 1236.8557569980621 and batch: 750, loss is 3.7753866624832155 and perplexity is 43.614369096056386
At time: 1238.3971214294434 and batch: 800, loss is 3.7289418935775758 and perplexity is 41.63503055551275
At time: 1239.938117980957 and batch: 850, loss is 3.755608162879944 and perplexity is 42.760217081230756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384329795837402 and perplexity of 80.1844652381258
Finished 43 epochs...
Completing Train Step...
At time: 1243.9439949989319 and batch: 50, loss is 3.882642331123352 and perplexity is 48.552337077789964
At time: 1245.4873566627502 and batch: 100, loss is 3.8389612340927126 and perplexity is 46.477170455730175
At time: 1247.031572341919 and batch: 150, loss is 3.857235646247864 and perplexity is 47.33432153654522
At time: 1248.5775108337402 and batch: 200, loss is 3.9080549907684325 and perplexity is 49.80199237539765
At time: 1250.1298882961273 and batch: 250, loss is 3.8676484489440917 and perplexity is 47.82977956408568
At time: 1251.6769726276398 and batch: 300, loss is 3.8561142539978026 and perplexity is 47.28127094603211
At time: 1253.2260210514069 and batch: 350, loss is 3.804566330909729 and perplexity is 44.90577164457683
At time: 1254.7709209918976 and batch: 400, loss is 3.8055292892456056 and perplexity is 44.949034858704174
At time: 1256.3144752979279 and batch: 450, loss is 3.8525370073318483 and perplexity is 47.112436338740046
At time: 1257.858490228653 and batch: 500, loss is 3.8248531007766724 and perplexity is 45.826068058054254
At time: 1259.4058079719543 and batch: 550, loss is 3.827577223777771 and perplexity is 45.95107409270405
At time: 1260.9646112918854 and batch: 600, loss is 3.863046178817749 and perplexity is 47.61015976092306
At time: 1262.5081088542938 and batch: 650, loss is 3.828601012229919 and perplexity is 45.998142361587846
At time: 1264.0527877807617 and batch: 700, loss is 3.7923935508728026 and perplexity is 44.36245709705533
At time: 1265.5965843200684 and batch: 750, loss is 3.7754017019271853 and perplexity is 43.61502503684917
At time: 1267.1409952640533 and batch: 800, loss is 3.7289653539657595 and perplexity is 41.63600734094946
At time: 1268.6968476772308 and batch: 850, loss is 3.755638294219971 and perplexity is 42.76150552328241
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384317715962728 and perplexity of 80.18349662568522
Finished 44 epochs...
Completing Train Step...
At time: 1272.8673515319824 and batch: 50, loss is 3.882605972290039 and perplexity is 48.55057180355105
At time: 1274.422307729721 and batch: 100, loss is 3.838918533325195 and perplexity is 46.47518588725127
At time: 1276.0100984573364 and batch: 150, loss is 3.8571843910217285 and perplexity is 47.33189546736579
At time: 1277.560199022293 and batch: 200, loss is 3.908008418083191 and perplexity is 49.79967301689214
At time: 1279.1156833171844 and batch: 250, loss is 3.867603340148926 and perplexity is 47.82762206901786
At time: 1280.6661336421967 and batch: 300, loss is 3.856075472831726 and perplexity is 47.27943735876579
At time: 1282.2237663269043 and batch: 350, loss is 3.8045293140411376 and perplexity is 44.90410940429453
At time: 1283.7809417247772 and batch: 400, loss is 3.8054984426498413 and perplexity is 44.94764835538045
At time: 1285.3360936641693 and batch: 450, loss is 3.8525174617767335 and perplexity is 47.11151550901808
At time: 1286.8921966552734 and batch: 500, loss is 3.824835844039917 and perplexity is 45.82527725648458
At time: 1288.43630361557 and batch: 550, loss is 3.8275628185272215 and perplexity is 45.95041216073638
At time: 1289.9773559570312 and batch: 600, loss is 3.863044414520264 and perplexity is 47.61007576251203
At time: 1291.5202929973602 and batch: 650, loss is 3.828608093261719 and perplexity is 45.99846807704983
At time: 1293.0779387950897 and batch: 700, loss is 3.7924055814743043 and perplexity is 44.36299080730872
At time: 1294.6189732551575 and batch: 750, loss is 3.775415425300598 and perplexity is 43.6156235862312
At time: 1296.162430047989 and batch: 800, loss is 3.728986349105835 and perplexity is 41.636881503932315
At time: 1297.7060315608978 and batch: 850, loss is 3.7556660652160643 and perplexity is 42.76269306937483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384309768676758 and perplexity of 80.18285938703964
Finished 45 epochs...
Completing Train Step...
At time: 1301.7275471687317 and batch: 50, loss is 3.8825707817077637 and perplexity is 48.54886331072109
At time: 1303.2669830322266 and batch: 100, loss is 3.838878884315491 and perplexity is 46.47334322868505
At time: 1304.8095788955688 and batch: 150, loss is 3.857136106491089 and perplexity is 47.32961012418268
At time: 1306.352079153061 and batch: 200, loss is 3.907963433265686 and perplexity is 49.79743283807707
At time: 1307.8918809890747 and batch: 250, loss is 3.8675604391098024 and perplexity is 47.825570258345024
At time: 1309.4348678588867 and batch: 300, loss is 3.856038017272949 and perplexity is 47.277666514185064
At time: 1310.9747121334076 and batch: 350, loss is 3.8044945049285888 and perplexity is 44.902546359300636
At time: 1312.5168850421906 and batch: 400, loss is 3.8054685592651367 and perplexity is 44.94630518758238
At time: 1314.056991815567 and batch: 450, loss is 3.8524987936019897 and perplexity is 47.110636031223265
At time: 1315.624568939209 and batch: 500, loss is 3.8248204517364504 and perplexity is 45.82457190533912
At time: 1317.1643941402435 and batch: 550, loss is 3.827549123764038 and perplexity is 45.94978288503255
At time: 1318.7098381519318 and batch: 600, loss is 3.863042016029358 and perplexity is 47.609961570315235
At time: 1320.265739440918 and batch: 650, loss is 3.8286141204833983 and perplexity is 45.99874532084935
At time: 1321.8172342777252 and batch: 700, loss is 3.7924165773391723 and perplexity is 44.36347861944272
At time: 1323.3722376823425 and batch: 750, loss is 3.775427803993225 and perplexity is 43.61616349397098
At time: 1324.9282302856445 and batch: 800, loss is 3.7290053749084473 and perplexity is 41.63767368655714
At time: 1326.4823055267334 and batch: 850, loss is 3.755692195892334 and perplexity is 42.763810502063436
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384303092956543 and perplexity of 80.18232411049102
Finished 46 epochs...
Completing Train Step...
At time: 1330.6094918251038 and batch: 50, loss is 3.8825364446640016 and perplexity is 48.54719631489701
At time: 1332.198617219925 and batch: 100, loss is 3.8388412618637084 and perplexity is 46.471594820460176
At time: 1333.7529015541077 and batch: 150, loss is 3.857090268135071 and perplexity is 47.32744066238627
At time: 1335.3045721054077 and batch: 200, loss is 3.9079195261001587 and perplexity is 49.79524642195064
At time: 1336.8596215248108 and batch: 250, loss is 3.8675191593170166 and perplexity is 47.823596069462226
At time: 1338.4151029586792 and batch: 300, loss is 3.856001825332642 and perplexity is 47.27595547466382
At time: 1339.9692859649658 and batch: 350, loss is 3.804461088180542 and perplexity is 44.90104588729287
At time: 1341.5200588703156 and batch: 400, loss is 3.8054395008087156 and perplexity is 44.94499913630781
At time: 1343.070105791092 and batch: 450, loss is 3.8524805736541747 and perplexity is 47.10977768571278
At time: 1344.6122210025787 and batch: 500, loss is 3.824806122779846 and perplexity is 45.82391529174118
At time: 1346.1537783145905 and batch: 550, loss is 3.827536115646362 and perplexity is 45.94918516873719
At time: 1347.6959626674652 and batch: 600, loss is 3.863039422035217 and perplexity is 47.60983807051405
At time: 1349.2357020378113 and batch: 650, loss is 3.828619475364685 and perplexity is 45.99899163932939
At time: 1350.7784321308136 and batch: 700, loss is 3.79242666721344 and perplexity is 44.3639262436223
At time: 1352.319141626358 and batch: 750, loss is 3.7754394102096556 and perplexity is 43.61666971554202
At time: 1353.8946163654327 and batch: 800, loss is 3.7290233182907104 and perplexity is 41.63842081395562
At time: 1355.4377570152283 and batch: 850, loss is 3.7557170295715334 and perplexity is 42.76487249800136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384298960367839 and perplexity of 80.18199275060883
Finished 47 epochs...
Completing Train Step...
At time: 1359.4311516284943 and batch: 50, loss is 3.8825027751922607 and perplexity is 48.545561783959634
At time: 1361.0014226436615 and batch: 100, loss is 3.8388048219680786 and perplexity is 46.469901431248815
At time: 1362.5466639995575 and batch: 150, loss is 3.857045931816101 and perplexity is 47.325342384396336
At time: 1364.090838432312 and batch: 200, loss is 3.907876844406128 and perplexity is 49.793121121834695
At time: 1365.6348571777344 and batch: 250, loss is 3.8674791049957276 and perplexity is 47.82168056614241
At time: 1367.1803860664368 and batch: 300, loss is 3.8559664154052733 and perplexity is 47.274281466152615
At time: 1368.724962234497 and batch: 350, loss is 3.8044289493560792 and perplexity is 44.899602843649895
At time: 1370.27361702919 and batch: 400, loss is 3.805411238670349 and perplexity is 44.94372891247304
At time: 1371.822901725769 and batch: 450, loss is 3.852462658882141 and perplexity is 47.10893373234461
At time: 1373.3698329925537 and batch: 500, loss is 3.824792356491089 and perplexity is 45.823284470833336
At time: 1374.9154851436615 and batch: 550, loss is 3.827523283958435 and perplexity is 45.948595566915415
At time: 1376.461886882782 and batch: 600, loss is 3.8630366039276125 and perplexity is 47.609703901056385
At time: 1378.0036222934723 and batch: 650, loss is 3.828624167442322 and perplexity is 45.999207470675714
At time: 1379.548715353012 and batch: 700, loss is 3.79243595123291 and perplexity is 44.364338121089254
At time: 1381.092048883438 and batch: 750, loss is 3.775450139045715 and perplexity is 43.617137674151174
At time: 1382.643354177475 and batch: 800, loss is 3.72904025554657 and perplexity is 41.63912606051499
At time: 1384.1877510547638 and batch: 850, loss is 3.7557406425476074 and perplexity is 42.765882315834816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384294191996257 and perplexity of 80.18161041398477
Finished 48 epochs...
Completing Train Step...
At time: 1388.166425704956 and batch: 50, loss is 3.882469902038574 and perplexity is 48.54396596447626
At time: 1389.7310845851898 and batch: 100, loss is 3.8387693214416503 and perplexity is 46.468251754567305
At time: 1391.2818896770477 and batch: 150, loss is 3.8570028257369997 and perplexity is 47.32330241841181
At time: 1392.8548743724823 and batch: 200, loss is 3.90783531665802 and perplexity is 49.79105336857811
At time: 1394.396547794342 and batch: 250, loss is 3.8674399948120115 and perplexity is 47.81981028800356
At time: 1395.9389736652374 and batch: 300, loss is 3.8559319400787353 and perplexity is 47.27265169795578
At time: 1397.4802026748657 and batch: 350, loss is 3.8043978023529053 and perplexity is 44.89820437735674
At time: 1399.0211923122406 and batch: 400, loss is 3.8053835916519163 and perplexity is 44.942486369547744
At time: 1400.578263759613 and batch: 450, loss is 3.8524450778961183 and perplexity is 47.10810551811956
At time: 1402.1374354362488 and batch: 500, loss is 3.824779167175293 and perplexity is 45.82268009704929
At time: 1403.6889317035675 and batch: 550, loss is 3.827510871887207 and perplexity is 45.9480252532138
At time: 1405.2442331314087 and batch: 600, loss is 3.8630336236953737 and perplexity is 47.60956201329337
At time: 1406.785319328308 and batch: 650, loss is 3.8286281490325926 and perplexity is 45.99939062103726
At time: 1408.3265521526337 and batch: 700, loss is 3.7924445629119874 and perplexity is 44.36472017417669
At time: 1409.8689737319946 and batch: 750, loss is 3.775460095405579 and perplexity is 43.617571944231955
At time: 1411.4104731082916 and batch: 800, loss is 3.7290565013885497 and perplexity is 41.63980252867203
At time: 1412.949135541916 and batch: 850, loss is 3.755763278007507 and perplexity is 42.76685035220499
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384292284647624 and perplexity of 80.18145747984562
Finished 49 epochs...
Completing Train Step...
At time: 1416.94966340065 and batch: 50, loss is 3.8824373483657837 and perplexity is 48.542385705814056
At time: 1418.4915568828583 and batch: 100, loss is 3.8387347221374513 and perplexity is 46.466644013202774
At time: 1420.0331554412842 and batch: 150, loss is 3.8569607448577883 and perplexity is 47.32131105413832
At time: 1421.5737826824188 and batch: 200, loss is 3.9077947759628295 and perplexity is 49.789034845576715
At time: 1423.116576910019 and batch: 250, loss is 3.8674018001556396 and perplexity is 47.81798386166193
At time: 1424.6578421592712 and batch: 300, loss is 3.8558982229232788 and perplexity is 47.27105782548021
At time: 1426.1983559131622 and batch: 350, loss is 3.8043673849105835 and perplexity is 44.89683870958491
At time: 1427.738701581955 and batch: 400, loss is 3.8053563022613526 and perplexity is 44.941259933218724
At time: 1429.2802019119263 and batch: 450, loss is 3.852427611351013 and perplexity is 47.10728270945553
At time: 1430.8201220035553 and batch: 500, loss is 3.82476616859436 and perplexity is 45.82208447110464
At time: 1432.3866040706635 and batch: 550, loss is 3.8274989461898805 and perplexity is 45.94747729423928
At time: 1433.9268491268158 and batch: 600, loss is 3.8630303525924683 and perplexity is 47.609406277771456
At time: 1435.4680280685425 and batch: 650, loss is 3.8286315631866454 and perplexity is 45.99954767031127
At time: 1437.009145975113 and batch: 700, loss is 3.7924525594711302 and perplexity is 44.36507494070388
At time: 1438.5525000095367 and batch: 750, loss is 3.7754696130752565 and perplexity is 43.61798708384944
At time: 1440.0929725170135 and batch: 800, loss is 3.7290720558166504 and perplexity is 41.64045021702378
At time: 1441.6340198516846 and batch: 850, loss is 3.755784969329834 and perplexity is 42.76777803180214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384292284647624 and perplexity of 80.18145747984562
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f777bec6898>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'seq_len': 50, 'wordvec_source': 'glove', 'lr': 5.015641241631757, 'anneal': 7.593085859843627, 'dropout': 0.0, 'wordvec_dim': 200, 'tune_wordvecs': True, 'data': 'wikitext', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.1048429012298584 and batch: 50, loss is 7.46708574295044 and perplexity is 1749.5007542843111
At time: 3.6283047199249268 and batch: 100, loss is 6.580863094329834 and perplexity is 721.161491112223
At time: 5.1524152755737305 and batch: 150, loss is 6.169044752120971 and perplexity is 477.72953792823995
At time: 6.677147388458252 and batch: 200, loss is 5.985482406616211 and perplexity is 397.61428675378886
At time: 8.203169822692871 and batch: 250, loss is 5.8400962352752686 and perplexity is 343.8124259564076
At time: 9.731226205825806 and batch: 300, loss is 5.662374792098999 and perplexity is 287.8313712321056
At time: 11.25771689414978 and batch: 350, loss is 5.513554954528809 and perplexity is 248.03130164945247
At time: 12.787461757659912 and batch: 400, loss is 5.445493860244751 and perplexity is 231.71168464906657
At time: 14.31718397140503 and batch: 450, loss is 5.3646706867218015 and perplexity is 213.72084197985893
At time: 15.866683959960938 and batch: 500, loss is 5.315088224411011 and perplexity is 203.3824556347109
At time: 17.398555040359497 and batch: 550, loss is 5.2617559814453125 and perplexity is 192.8197821538375
At time: 18.93120813369751 and batch: 600, loss is 5.263416967391968 and perplexity is 193.14031923233009
At time: 20.467543601989746 and batch: 650, loss is 5.240796451568603 and perplexity is 188.8204288605995
At time: 22.000100135803223 and batch: 700, loss is 5.160611124038696 and perplexity is 174.27092422006885
At time: 23.534919023513794 and batch: 750, loss is 5.1251115894317625 and perplexity is 168.1929091558576
At time: 25.07030487060547 and batch: 800, loss is 5.100744276046753 and perplexity is 164.1440303176288
At time: 26.60609197616577 and batch: 850, loss is 5.073392553329468 and perplexity is 159.7152517761739
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.821005821228027 and perplexity of 124.08984022696848
Finished 1 epochs...
Completing Train Step...
At time: 30.6278395652771 and batch: 50, loss is 5.058679246902466 and perplexity is 157.38251554986005
At time: 32.160173654556274 and batch: 100, loss is 4.966811828613281 and perplexity is 143.56843616851307
At time: 33.69379186630249 and batch: 150, loss is 4.952242631912231 and perplexity is 141.49192267884465
At time: 35.22700214385986 and batch: 200, loss is 4.957492914199829 and perplexity is 142.23674877700319
At time: 36.784738063812256 and batch: 250, loss is 4.952917356491088 and perplexity is 141.58742297136013
At time: 38.31763005256653 and batch: 300, loss is 4.900342130661011 and perplexity is 134.33573219645945
At time: 39.859753131866455 and batch: 350, loss is 4.835921649932861 and perplexity is 125.95461579113503
At time: 41.39270734786987 and batch: 400, loss is 4.8335134887695315 and perplexity is 125.65166170458605
At time: 42.92467737197876 and batch: 450, loss is 4.826676616668701 and perplexity is 124.79552733967394
At time: 44.45799779891968 and batch: 500, loss is 4.806451969146728 and perplexity is 122.29693354371909
At time: 45.990906953811646 and batch: 550, loss is 4.80216537475586 and perplexity is 121.77381818706239
At time: 47.5267219543457 and batch: 600, loss is 4.825533723831176 and perplexity is 124.65298089849188
At time: 49.060441970825195 and batch: 650, loss is 4.807693319320679 and perplexity is 122.44884112919421
At time: 50.59949541091919 and batch: 700, loss is 4.756058235168457 and perplexity is 116.2866466940354
At time: 52.133076190948486 and batch: 750, loss is 4.741532793045044 and perplexity is 114.60974012629559
At time: 53.666768074035645 and batch: 800, loss is 4.71468035697937 and perplexity is 111.5731418971969
At time: 55.20142149925232 and batch: 850, loss is 4.71026538848877 and perplexity is 111.08163578169726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.6047665278116865 and perplexity of 99.95964232825959
Finished 2 epochs...
Completing Train Step...
At time: 59.216553688049316 and batch: 50, loss is 4.702217884063721 and perplexity is 110.19129314938895
At time: 60.74750590324402 and batch: 100, loss is 4.6344324970245365 and perplexity is 102.96946593263966
At time: 62.2799174785614 and batch: 150, loss is 4.641418294906616 and perplexity is 103.69130819617668
At time: 63.81173920631409 and batch: 200, loss is 4.663407964706421 and perplexity is 105.99670033436007
At time: 65.34334683418274 and batch: 250, loss is 4.6637351608276365 and perplexity is 106.03138771805037
At time: 66.90103340148926 and batch: 300, loss is 4.631136131286621 and perplexity is 102.6305997333964
At time: 68.43303728103638 and batch: 350, loss is 4.576281061172486 and perplexity is 97.1524176417407
At time: 69.964435338974 and batch: 400, loss is 4.5861117458343506 and perplexity is 98.11220236454838
At time: 71.50186944007874 and batch: 450, loss is 4.599428129196167 and perplexity is 99.42743973076546
At time: 73.0468864440918 and batch: 500, loss is 4.57778037071228 and perplexity is 97.29818843879153
At time: 74.59301018714905 and batch: 550, loss is 4.587363300323486 and perplexity is 98.23507200484316
At time: 76.1445541381836 and batch: 600, loss is 4.615886325836182 and perplexity is 101.07737633245131
At time: 77.6889476776123 and batch: 650, loss is 4.595884866714478 and perplexity is 99.075765618691
At time: 79.23195791244507 and batch: 700, loss is 4.555566701889038 and perplexity is 95.16066768813181
At time: 80.7759895324707 and batch: 750, loss is 4.544618320465088 and perplexity is 94.12449495908093
At time: 82.32706308364868 and batch: 800, loss is 4.518606338500977 and perplexity is 91.70769938538847
At time: 83.86543846130371 and batch: 850, loss is 4.520842771530152 and perplexity is 91.91302702850393
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5252126057942705 and perplexity of 92.31555056336883
Finished 3 epochs...
Completing Train Step...
At time: 87.85120415687561 and batch: 50, loss is 4.515343761444091 and perplexity is 91.40898350619209
At time: 89.4102692604065 and batch: 100, loss is 4.457164335250854 and perplexity is 86.24260690989678
At time: 90.94409775733948 and batch: 150, loss is 4.467633056640625 and perplexity is 87.1501991096184
At time: 92.47706627845764 and batch: 200, loss is 4.4953774738311765 and perplexity is 89.60198500697383
At time: 94.01358103752136 and batch: 250, loss is 4.4937101745605466 and perplexity is 89.45271615522773
At time: 95.5478937625885 and batch: 300, loss is 4.468328008651733 and perplexity is 87.21078536558996
At time: 97.08421540260315 and batch: 350, loss is 4.417620286941529 and perplexity is 82.89877514499192
At time: 98.61977767944336 and batch: 400, loss is 4.434874610900879 and perplexity is 84.34154871442426
At time: 100.15384578704834 and batch: 450, loss is 4.455653457641602 and perplexity is 86.11240327180232
At time: 101.69021391868591 and batch: 500, loss is 4.433903846740723 and perplexity is 84.2597126898868
At time: 103.2330687046051 and batch: 550, loss is 4.449507389068604 and perplexity is 85.58477362121113
At time: 104.77036786079407 and batch: 600, loss is 4.478607702255249 and perplexity is 88.11190922134793
At time: 106.33184361457825 and batch: 650, loss is 4.460064401626587 and perplexity is 86.49307921195499
At time: 107.8811731338501 and batch: 700, loss is 4.424302377700806 and perplexity is 83.45456714260587
At time: 109.42893409729004 and batch: 750, loss is 4.416233797073364 and perplexity is 82.78391647675993
At time: 110.97076988220215 and batch: 800, loss is 4.388114500045776 and perplexity is 80.48851472691533
At time: 112.50964093208313 and batch: 850, loss is 4.3954995918273925 and perplexity is 81.08513011364059
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.484201431274414 and perplexity of 88.60616444101424
Finished 4 epochs...
Completing Train Step...
At time: 116.56834030151367 and batch: 50, loss is 4.391590127944946 and perplexity is 80.76874956817647
At time: 118.1176381111145 and batch: 100, loss is 4.3356960105896 and perplexity is 76.37810036077376
At time: 119.66364526748657 and batch: 150, loss is 4.345980997085571 and perplexity is 77.16770165048781
At time: 121.20968985557556 and batch: 200, loss is 4.379644660949707 and perplexity is 79.80966887392836
At time: 122.75781726837158 and batch: 250, loss is 4.376478223800659 and perplexity is 79.55735625039887
At time: 124.31349301338196 and batch: 300, loss is 4.3565962028503415 and perplexity is 77.99121583520095
At time: 125.85547518730164 and batch: 350, loss is 4.308275299072266 and perplexity is 74.3122120114758
At time: 127.39698266983032 and batch: 400, loss is 4.325520210266113 and perplexity is 75.60483304177478
At time: 128.94297647476196 and batch: 450, loss is 4.353083209991455 and perplexity is 77.71771393729969
At time: 130.48769116401672 and batch: 500, loss is 4.330320215225219 and perplexity is 75.9686089800105
At time: 132.03271341323853 and batch: 550, loss is 4.350163383483887 and perplexity is 77.49112266062744
At time: 133.5775055885315 and batch: 600, loss is 4.38019268989563 and perplexity is 79.8534188696728
At time: 135.11973881721497 and batch: 650, loss is 4.3616409015655515 and perplexity is 78.38565209105919
At time: 136.66495752334595 and batch: 700, loss is 4.328487520217895 and perplexity is 75.82950919229641
At time: 138.2097818851471 and batch: 750, loss is 4.320010042190551 and perplexity is 75.18938335434693
At time: 139.75441002845764 and batch: 800, loss is 4.292246866226196 and perplexity is 73.13059871361699
At time: 141.29467010498047 and batch: 850, loss is 4.302579679489136 and perplexity is 73.8901609834745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.470923741658528 and perplexity of 87.43745533980685
Finished 5 epochs...
Completing Train Step...
At time: 145.3106291294098 and batch: 50, loss is 4.297973356246948 and perplexity is 73.55058172392383
At time: 146.85144424438477 and batch: 100, loss is 4.246190948486328 and perplexity is 69.83888514983411
At time: 148.38932418823242 and batch: 150, loss is 4.254627237319946 and perplexity is 70.43055841087953
At time: 149.92913222312927 and batch: 200, loss is 4.2934778213500975 and perplexity is 73.22067462714719
At time: 151.46759629249573 and batch: 250, loss is 4.286569223403931 and perplexity is 72.71656577253779
At time: 153.00975561141968 and batch: 300, loss is 4.269836149215698 and perplexity is 71.50991770314955
At time: 154.54559421539307 and batch: 350, loss is 4.224979572296142 and perplexity is 68.37310682871416
At time: 156.0835793018341 and batch: 400, loss is 4.244216003417969 and perplexity is 69.70109329848341
At time: 157.6208279132843 and batch: 450, loss is 4.2716876411437985 and perplexity is 71.64244038303454
At time: 159.1597456932068 and batch: 500, loss is 4.247290172576904 and perplexity is 69.91569594325291
At time: 160.6981999874115 and batch: 550, loss is 4.2721420001983645 and perplexity is 71.67499917064363
At time: 162.23738503456116 and batch: 600, loss is 4.305936880111695 and perplexity is 74.1386419447667
At time: 163.77595233917236 and batch: 650, loss is 4.283679552078247 and perplexity is 72.50674210424734
At time: 165.31402373313904 and batch: 700, loss is 4.259691123962402 and perplexity is 70.78811532460934
At time: 166.85283994674683 and batch: 750, loss is 4.244503164291382 and perplexity is 69.72111159941171
At time: 168.39228415489197 and batch: 800, loss is 4.217665100097657 and perplexity is 67.87481822037485
At time: 169.9299464225769 and batch: 850, loss is 4.228818693161011 and perplexity is 68.63610396561114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.464500109354655 and perplexity of 86.87758938876888
Finished 6 epochs...
Completing Train Step...
At time: 173.95429682731628 and batch: 50, loss is 4.22473373413086 and perplexity is 68.35630017551975
At time: 175.4915065765381 and batch: 100, loss is 4.173548989295959 and perplexity is 64.94553459285748
At time: 177.0302414894104 and batch: 150, loss is 4.180197153091431 and perplexity is 65.3787415644924
At time: 178.56882429122925 and batch: 200, loss is 4.223876085281372 and perplexity is 68.2976996062664
At time: 180.1075291633606 and batch: 250, loss is 4.2156331253051755 and perplexity is 67.737038330745
At time: 181.6487135887146 and batch: 300, loss is 4.199943361282348 and perplexity is 66.68255411961108
At time: 183.18359684944153 and batch: 350, loss is 4.1597004938125615 and perplexity is 64.052335655927
At time: 184.7464783191681 and batch: 400, loss is 4.178609881401062 and perplexity is 65.27505005392509
At time: 186.28584051132202 and batch: 450, loss is 4.207312870025635 and perplexity is 67.17578699494007
At time: 187.82481789588928 and batch: 500, loss is 4.182497863769531 and perplexity is 65.52933229976047
At time: 189.36384963989258 and batch: 550, loss is 4.209995241165161 and perplexity is 67.35621927223099
At time: 190.90108919143677 and batch: 600, loss is 4.245426435470581 and perplexity is 69.78551281765269
At time: 192.44074964523315 and batch: 650, loss is 4.219868021011353 and perplexity is 68.02450589146197
At time: 193.9803671836853 and batch: 700, loss is 4.197260546684265 and perplexity is 66.50389694917095
At time: 195.5187177658081 and batch: 750, loss is 4.183783397674561 and perplexity is 65.61362664821853
At time: 197.05529069900513 and batch: 800, loss is 4.157210063934326 and perplexity is 63.893016274657136
At time: 198.6026418209076 and batch: 850, loss is 4.170947580337525 and perplexity is 64.77680426081044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.466639518737793 and perplexity of 87.06365508306806
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 202.74979758262634 and batch: 50, loss is 4.198341541290283 and perplexity is 66.57582617359958
At time: 204.31540870666504 and batch: 100, loss is 4.15801094532013 and perplexity is 63.94420749838722
At time: 205.859037399292 and batch: 150, loss is 4.153201990127563 and perplexity is 63.6374408743695
At time: 207.41521215438843 and batch: 200, loss is 4.191171398162842 and perplexity is 66.10017525150613
At time: 208.97762751579285 and batch: 250, loss is 4.164840025901794 and perplexity is 64.38238210607302
At time: 210.53236770629883 and batch: 300, loss is 4.141340537071228 and perplexity is 62.88706741433561
At time: 212.07665014266968 and batch: 350, loss is 4.086845264434815 and perplexity is 59.55172510711214
At time: 213.6217451095581 and batch: 400, loss is 4.089167127609253 and perplexity is 59.69015671204448
At time: 215.1673150062561 and batch: 450, loss is 4.103538885116577 and perplexity is 60.55420323124535
At time: 216.7109067440033 and batch: 500, loss is 4.0681235885620115 and perplexity is 58.447188660250305
At time: 218.2531921863556 and batch: 550, loss is 4.076067361831665 and perplexity is 58.91332888006971
At time: 219.7996847629547 and batch: 600, loss is 4.0958274602890015 and perplexity is 60.08903988623518
At time: 221.3442780971527 and batch: 650, loss is 4.044476704597473 and perplexity is 57.081307831305296
At time: 222.9342746734619 and batch: 700, loss is 4.002754831314087 and perplexity is 54.74876609221145
At time: 224.47919487953186 and batch: 750, loss is 3.969667100906372 and perplexity is 52.96689527297828
At time: 226.02308344841003 and batch: 800, loss is 3.9125103044509886 and perplexity is 50.02437088861497
At time: 227.56700539588928 and batch: 850, loss is 3.9127071619033815 and perplexity is 50.0342195281829
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3664140701293945 and perplexity of 78.76069437945402
Finished 8 epochs...
Completing Train Step...
At time: 231.57543849945068 and batch: 50, loss is 4.106955413818359 and perplexity is 60.76144222199252
At time: 233.14117121696472 and batch: 100, loss is 4.067000565528869 and perplexity is 58.38158796360362
At time: 234.68223333358765 and batch: 150, loss is 4.065649852752686 and perplexity is 58.302784439291784
At time: 236.2242398262024 and batch: 200, loss is 4.112612752914429 and perplexity is 61.106164490575765
At time: 237.76726627349854 and batch: 250, loss is 4.091497178077698 and perplexity is 59.829399948539624
At time: 239.30925226211548 and batch: 300, loss is 4.074512104988099 and perplexity is 58.821774735710925
At time: 240.8504979610443 and batch: 350, loss is 4.024702334403992 and perplexity is 55.96364784948211
At time: 242.39313793182373 and batch: 400, loss is 4.030468740463257 and perplexity is 56.287289194451716
At time: 243.93696069717407 and batch: 450, loss is 4.052429971694946 and perplexity is 57.53710083794647
At time: 245.47914600372314 and batch: 500, loss is 4.020626854896546 and perplexity is 55.73603328380615
At time: 247.01843190193176 and batch: 550, loss is 4.03594829082489 and perplexity is 56.59656480210126
At time: 248.56124997138977 and batch: 600, loss is 4.062379980087281 and perplexity is 58.112453107332854
At time: 250.103839635849 and batch: 650, loss is 4.017556471824646 and perplexity is 55.56516476086193
At time: 251.6472725868225 and batch: 700, loss is 3.982697868347168 and perplexity is 53.6616110771292
At time: 253.1864447593689 and batch: 750, loss is 3.959392704963684 and perplexity is 52.425478546795226
At time: 254.72776627540588 and batch: 800, loss is 3.9125830221176146 and perplexity is 50.02800867640454
At time: 256.2711045742035 and batch: 850, loss is 3.923604531288147 and perplexity is 50.58244256951868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.358636538187663 and perplexity of 78.15050651681331
Finished 9 epochs...
Completing Train Step...
At time: 260.25726556777954 and batch: 50, loss is 4.074975152015686 and perplexity is 58.84901829069649
At time: 261.81838297843933 and batch: 100, loss is 4.032838959693908 and perplexity is 56.42086064401784
At time: 263.3594229221344 and batch: 150, loss is 4.030752143859863 and perplexity is 56.3032434640355
At time: 264.9020540714264 and batch: 200, loss is 4.080458989143372 and perplexity is 59.172623209494915
At time: 266.4396781921387 and batch: 250, loss is 4.059983129501343 and perplexity is 57.97333303167218
At time: 267.978688955307 and batch: 300, loss is 4.044725427627563 and perplexity is 57.09550703291163
At time: 269.5157196521759 and batch: 350, loss is 3.996606373786926 and perplexity is 54.41317835988783
At time: 271.05542755126953 and batch: 400, loss is 4.004349293708802 and perplexity is 54.83613057208242
At time: 272.59652304649353 and batch: 450, loss is 4.0289574193954465 and perplexity is 56.20228527872627
At time: 274.1360332965851 and batch: 500, loss is 3.9984486055374147 and perplexity is 54.51351243564112
At time: 275.67411828041077 and batch: 550, loss is 4.01629638671875 and perplexity is 55.495192019403426
At time: 277.2297456264496 and batch: 600, loss is 4.0458914041519165 and perplexity is 57.162117879553314
At time: 278.77481341362 and batch: 650, loss is 4.004036817550659 and perplexity is 54.81899826553193
At time: 280.31660294532776 and batch: 700, loss is 3.9723132276535034 and perplexity is 53.10723799169368
At time: 281.8556241989136 and batch: 750, loss is 3.9536023902893067 and perplexity is 52.122795689239325
At time: 283.39495062828064 and batch: 800, loss is 3.9105526208877563 and perplexity is 49.92653479726586
At time: 284.93451046943665 and batch: 850, loss is 3.925628538131714 and perplexity is 50.684925457491055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.35565725962321 and perplexity of 77.9180208796481
Finished 10 epochs...
Completing Train Step...
At time: 288.9625298976898 and batch: 50, loss is 4.0516909313201905 and perplexity is 57.49459430633707
At time: 290.5077803134918 and batch: 100, loss is 4.008998637199402 and perplexity is 55.09167617812637
At time: 292.04977107048035 and batch: 150, loss is 4.007257823944092 and perplexity is 54.99585528531662
At time: 293.58962988853455 and batch: 200, loss is 4.058322944641113 and perplexity is 57.877166431131066
At time: 295.128751039505 and batch: 250, loss is 4.038321204185486 and perplexity is 56.73102301263478
At time: 296.669043302536 and batch: 300, loss is 4.024314970970154 and perplexity is 55.94197377682362
At time: 298.209748506546 and batch: 350, loss is 3.9772218608856202 and perplexity is 53.36856279405903
At time: 299.749253988266 and batch: 400, loss is 3.9865416049957276 and perplexity is 53.86826909330661
At time: 301.3129870891571 and batch: 450, loss is 4.01270348072052 and perplexity is 55.2961607755064
At time: 302.8510239124298 and batch: 500, loss is 3.9828973388671876 and perplexity is 53.67231605422403
At time: 304.3944354057312 and batch: 550, loss is 4.002179317474365 and perplexity is 54.71726648471712
At time: 305.9336984157562 and batch: 600, loss is 4.0336888313293455 and perplexity is 56.46883151477997
At time: 307.47655630111694 and batch: 650, loss is 3.9936453199386595 and perplexity is 54.25229631645475
At time: 309.01768374443054 and batch: 700, loss is 3.9641649436950686 and perplexity is 52.676263372603984
At time: 310.55640864372253 and batch: 750, loss is 3.947822890281677 and perplexity is 51.82242083549891
At time: 312.0986874103546 and batch: 800, loss is 3.9063932943344115 and perplexity is 49.719305301694604
At time: 313.6414439678192 and batch: 850, loss is 3.9239026212692263 and perplexity is 50.59752293640865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.354217529296875 and perplexity of 77.80592065842737
Finished 11 epochs...
Completing Train Step...
At time: 317.68924498558044 and batch: 50, loss is 4.032859783172608 and perplexity is 56.42203553484029
At time: 319.23629212379456 and batch: 100, loss is 3.9901718378067015 and perplexity is 54.06417883507464
At time: 320.7849566936493 and batch: 150, loss is 3.988618812561035 and perplexity is 53.980280965075316
At time: 322.33206367492676 and batch: 200, loss is 4.040722823143005 and perplexity is 56.86743304988191
At time: 323.878217458725 and batch: 250, loss is 4.021081080436707 and perplexity is 55.761355764254226
At time: 325.425998210907 and batch: 300, loss is 4.00803699016571 and perplexity is 55.038722896412494
At time: 326.97467064857483 and batch: 350, loss is 3.9618234491348265 and perplexity is 52.55306647717996
At time: 328.5243790149689 and batch: 400, loss is 3.9724258041381835 and perplexity is 53.11321695439684
At time: 330.071182012558 and batch: 450, loss is 3.9995237398147583 and perplexity is 54.572153299191655
At time: 331.61877846717834 and batch: 500, loss is 3.9703278160095214 and perplexity is 53.001902864400726
At time: 333.1669075489044 and batch: 550, loss is 3.9904924488067626 and perplexity is 54.08151518448209
At time: 334.7132742404938 and batch: 600, loss is 4.023298115730285 and perplexity is 55.88511779970828
At time: 336.2568926811218 and batch: 650, loss is 3.9866523933410645 and perplexity is 53.87423740030894
At time: 337.80022072792053 and batch: 700, loss is 3.9566862392425537 and perplexity is 52.28378262027148
At time: 339.3459725379944 and batch: 750, loss is 3.9417553520202637 and perplexity is 51.50893830962866
At time: 340.93380069732666 and batch: 800, loss is 3.9013137292861937 and perplexity is 49.46739319986865
At time: 342.4819257259369 and batch: 850, loss is 3.9201545190811156 and perplexity is 50.408233210195604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.353473663330078 and perplexity of 77.74806500312856
Finished 12 epochs...
Completing Train Step...
At time: 346.5159785747528 and batch: 50, loss is 4.016474957466126 and perplexity is 55.50510272217255
At time: 348.0654761791229 and batch: 100, loss is 3.9739298009872437 and perplexity is 53.193159166688254
At time: 349.60741209983826 and batch: 150, loss is 3.9725027227401735 and perplexity is 53.11730250591761
At time: 351.15367007255554 and batch: 200, loss is 4.025576376914978 and perplexity is 56.01258383972493
At time: 352.7009229660034 and batch: 250, loss is 4.006289262771606 and perplexity is 54.94261422301466
At time: 354.2443344593048 and batch: 300, loss is 3.9941052055358885 and perplexity is 54.277251904065125
At time: 355.7846472263336 and batch: 350, loss is 3.9484264612197877 and perplexity is 51.8537087839561
At time: 357.3294303417206 and batch: 400, loss is 3.960179371833801 and perplexity is 52.466736159788994
At time: 358.87013506889343 and batch: 450, loss is 3.9878794384002685 and perplexity is 53.94038409131514
At time: 360.41089963912964 and batch: 500, loss is 3.9591559410095214 and perplexity is 52.41306755249163
At time: 361.9558095932007 and batch: 550, loss is 3.980149517059326 and perplexity is 53.52503653529856
At time: 363.49873661994934 and batch: 600, loss is 4.015378675460815 and perplexity is 55.44428681863656
At time: 365.0384063720703 and batch: 650, loss is 3.977953939437866 and perplexity is 53.4076470788825
At time: 366.577570438385 and batch: 700, loss is 3.9491508054733275 and perplexity is 51.891282326385436
At time: 368.1184597015381 and batch: 750, loss is 3.935425114631653 and perplexity is 51.18390435892049
At time: 369.6569240093231 and batch: 800, loss is 3.8956973266601564 and perplexity is 49.1903431434014
At time: 371.1978693008423 and batch: 850, loss is 3.915945067405701 and perplexity is 50.196488166386594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.353176116943359 and perplexity of 77.72493478863902
Finished 13 epochs...
Completing Train Step...
At time: 375.17765045166016 and batch: 50, loss is 4.002301621437073 and perplexity is 54.723959032490946
At time: 376.73917841911316 and batch: 100, loss is 3.9599949073791505 and perplexity is 52.45705880450745
At time: 378.2784729003906 and batch: 150, loss is 3.958560376167297 and perplexity is 52.38186146573071
At time: 379.84345626831055 and batch: 200, loss is 4.012421717643738 and perplexity is 55.28058255389893
At time: 381.3898894786835 and batch: 250, loss is 3.993339786529541 and perplexity is 54.23572295939474
At time: 382.93180108070374 and batch: 300, loss is 3.9819336128234863 and perplexity is 53.620615561973715
At time: 384.4772572517395 and batch: 350, loss is 3.9368059825897217 and perplexity is 51.25463139352801
At time: 386.01727628707886 and batch: 400, loss is 3.9494216728210447 and perplexity is 51.905339884179575
At time: 387.55916833877563 and batch: 450, loss is 3.977475714683533 and perplexity is 53.38211232614077
At time: 389.1008687019348 and batch: 500, loss is 3.949142212867737 and perplexity is 51.890836446978454
At time: 390.64072036743164 and batch: 550, loss is 3.9706806564331054 and perplexity is 53.020607377918346
At time: 392.1810448169708 and batch: 600, loss is 4.00648533821106 and perplexity is 54.953388176462475
At time: 393.7211740016937 and batch: 650, loss is 3.9694974184036256 and perplexity is 52.95790848009797
At time: 395.26202964782715 and batch: 700, loss is 3.942023506164551 and perplexity is 51.52275249698724
At time: 396.80224347114563 and batch: 750, loss is 3.9291732501983643 and perplexity is 50.86490772856537
At time: 398.3448226451874 and batch: 800, loss is 3.8899627447128298 and perplexity is 48.90906436855136
At time: 399.8857080936432 and batch: 850, loss is 3.910996503829956 and perplexity is 49.948701253717495
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.353318532307942 and perplexity of 77.73600480181541
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 403.8974800109863 and batch: 50, loss is 4.004564390182495 and perplexity is 54.84792689902817
At time: 405.46948051452637 and batch: 100, loss is 3.9768158054351805 and perplexity is 53.34689659739069
At time: 407.0138294696808 and batch: 150, loss is 3.972739119529724 and perplexity is 53.129860750005406
At time: 408.5581223964691 and batch: 200, loss is 4.026999373435974 and perplexity is 56.09234628904539
At time: 410.1040575504303 and batch: 250, loss is 4.005789275169373 and perplexity is 54.91515046341065
At time: 411.6499810218811 and batch: 300, loss is 3.9936351203918456 and perplexity is 54.25174297044067
At time: 413.19642090797424 and batch: 350, loss is 3.943642611503601 and perplexity is 51.60624083060229
At time: 414.74055528640747 and batch: 400, loss is 3.9557700157165527 and perplexity is 52.23590092712291
At time: 416.286851644516 and batch: 450, loss is 3.9860978746414184 and perplexity is 53.84437140963175
At time: 417.8572323322296 and batch: 500, loss is 3.949390411376953 and perplexity is 51.903717273661435
At time: 419.4034585952759 and batch: 550, loss is 3.961290383338928 and perplexity is 52.525059700361986
At time: 420.9478430747986 and batch: 600, loss is 3.9863686466217043 and perplexity is 53.85895293075017
At time: 422.4923369884491 and batch: 650, loss is 3.945915780067444 and perplexity is 51.72368394838627
At time: 424.03719115257263 and batch: 700, loss is 3.910402607917786 and perplexity is 49.91904573124315
At time: 425.5838348865509 and batch: 750, loss is 3.895621910095215 and perplexity is 49.186633516578645
At time: 427.1297423839569 and batch: 800, loss is 3.8466799545288084 and perplexity is 46.837302833696654
At time: 428.67426323890686 and batch: 850, loss is 3.8667645025253297 and perplexity is 47.78751928239387
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.340883890787761 and perplexity of 76.77537039887714
Finished 15 epochs...
Completing Train Step...
At time: 432.66754388809204 and batch: 50, loss is 3.996007399559021 and perplexity is 54.38059602735472
At time: 434.2536778450012 and batch: 100, loss is 3.9628489446640014 and perplexity is 52.606987054826966
At time: 435.7949550151825 and batch: 150, loss is 3.9584061765670775 and perplexity is 52.373784826357216
At time: 437.33518385887146 and batch: 200, loss is 4.011545009613037 and perplexity is 55.232138861821944
At time: 438.8778965473175 and batch: 250, loss is 3.990527811050415 and perplexity is 54.08342766201369
At time: 440.420485496521 and batch: 300, loss is 3.9797719287872315 and perplexity is 53.50482992436913
At time: 441.965451002121 and batch: 350, loss is 3.930657396316528 and perplexity is 50.9404547314498
At time: 443.5076494216919 and batch: 400, loss is 3.9438858032226562 and perplexity is 51.6187925672012
At time: 445.05028200149536 and batch: 450, loss is 3.9752035188674926 and perplexity is 53.26095541210498
At time: 446.59229373931885 and batch: 500, loss is 3.939726319313049 and perplexity is 51.40453094788065
At time: 448.1343548297882 and batch: 550, loss is 3.9545927667617797 and perplexity is 52.174442450416585
At time: 449.6765727996826 and batch: 600, loss is 3.9818365812301635 and perplexity is 53.615412920625154
At time: 451.2171423435211 and batch: 650, loss is 3.943104419708252 and perplexity is 51.57847424774221
At time: 452.75879859924316 and batch: 700, loss is 3.909945397377014 and perplexity is 49.89622743413003
At time: 454.3030662536621 and batch: 750, loss is 3.8972249937057497 and perplexity is 49.26554703821321
At time: 455.8460941314697 and batch: 800, loss is 3.850650749206543 and perplexity is 47.02365388253164
At time: 457.4211747646332 and batch: 850, loss is 3.8727061986923217 and perplexity is 48.07230341511045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.339738845825195 and perplexity of 76.68750945971546
Finished 16 epochs...
Completing Train Step...
At time: 461.4426255226135 and batch: 50, loss is 3.9921454620361327 and perplexity is 54.170986572867726
At time: 462.983763217926 and batch: 100, loss is 3.9578417539596558 and perplexity is 52.34423221902937
At time: 464.5238049030304 and batch: 150, loss is 3.9526975774765014 and perplexity is 52.07565564553432
At time: 466.0634734630585 and batch: 200, loss is 4.0057636785507205 and perplexity is 54.9137448392357
At time: 467.6050319671631 and batch: 250, loss is 3.9846843528747558 and perplexity is 53.768314985001496
At time: 469.14837288856506 and batch: 300, loss is 3.974175057411194 and perplexity is 53.20620673061812
At time: 470.68974328041077 and batch: 350, loss is 3.9251454973220827 and perplexity is 50.66044848222687
At time: 472.23094177246094 and batch: 400, loss is 3.938791332244873 and perplexity is 51.35649083813913
At time: 473.7716455459595 and batch: 450, loss is 3.970706372261047 and perplexity is 53.02197086426656
At time: 475.32204723358154 and batch: 500, loss is 3.935718283653259 and perplexity is 51.198912093877375
At time: 476.87931084632874 and batch: 550, loss is 3.9518734216690063 and perplexity is 52.0327548723939
At time: 478.43172240257263 and batch: 600, loss is 3.980067410469055 and perplexity is 53.5206419574689
At time: 479.98524355888367 and batch: 650, loss is 3.9420990324020386 and perplexity is 51.52664396358041
At time: 481.53571820259094 and batch: 700, loss is 3.910042157173157 and perplexity is 49.90105561650807
At time: 483.09326219558716 and batch: 750, loss is 3.8983855390548707 and perplexity is 49.32275512957152
At time: 484.6397421360016 and batch: 800, loss is 3.852802023887634 and perplexity is 47.124923568941504
At time: 486.1790335178375 and batch: 850, loss is 3.8755289268493653 and perplexity is 48.20819015497116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.339340527852376 and perplexity of 76.6569695291123
Finished 17 epochs...
Completing Train Step...
At time: 490.22639751434326 and batch: 50, loss is 3.9886499404907227 and perplexity is 53.98196128561803
At time: 491.7724528312683 and batch: 100, loss is 3.953938617706299 and perplexity is 52.14032374874229
At time: 493.3156533241272 and batch: 150, loss is 3.948357329368591 and perplexity is 51.85012416498356
At time: 494.85642743110657 and batch: 200, loss is 4.001625242233277 and perplexity is 54.68695739963013
At time: 496.4240336418152 and batch: 250, loss is 3.980653281211853 and perplexity is 53.55200732285534
At time: 497.9662456512451 and batch: 300, loss is 3.970330662727356 and perplexity is 53.00205374607763
At time: 499.5078225135803 and batch: 350, loss is 3.921425986289978 and perplexity is 50.47236638875586
At time: 501.05403757095337 and batch: 400, loss is 3.935319800376892 and perplexity is 51.17851424800992
At time: 502.59365367889404 and batch: 450, loss is 3.9677717876434326 and perplexity is 52.866601487964196
At time: 504.1389820575714 and batch: 500, loss is 3.9331054878234863 and perplexity is 51.06531439762404
At time: 505.687926530838 and batch: 550, loss is 3.950027513504028 and perplexity is 51.93679577841358
At time: 507.2458155155182 and batch: 600, loss is 3.978814935684204 and perplexity is 53.45365066415837
At time: 508.78501558303833 and batch: 650, loss is 3.94132342338562 and perplexity is 51.4866949283551
At time: 510.3367304801941 and batch: 700, loss is 3.9099500370025635 and perplexity is 49.896458934478694
At time: 511.87878918647766 and batch: 750, loss is 3.8990365266799927 and perplexity is 49.35487408618514
At time: 513.4195034503937 and batch: 800, loss is 3.854023060798645 and perplexity is 47.182499984358415
At time: 514.9651744365692 and batch: 850, loss is 3.877098789215088 and perplexity is 48.28392981327981
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.339144388834636 and perplexity of 76.64193558082603
Finished 18 epochs...
Completing Train Step...
At time: 518.986076593399 and batch: 50, loss is 3.9855152559280396 and perplexity is 53.813009808053486
At time: 520.5288510322571 and batch: 100, loss is 3.9506216478347778 and perplexity is 51.96766238036063
At time: 522.0753428936005 and batch: 150, loss is 3.94475670337677 and perplexity is 51.663766962864564
At time: 523.6215181350708 and batch: 200, loss is 3.998278818130493 and perplexity is 54.50425751342934
At time: 525.1744921207428 and batch: 250, loss is 3.9774430227279662 and perplexity is 53.38036718902268
At time: 526.7291660308838 and batch: 300, loss is 3.9672456550598145 and perplexity is 52.83879396220037
At time: 528.2793157100677 and batch: 350, loss is 3.9184096097946166 and perplexity is 50.32035211052228
At time: 529.8379673957825 and batch: 400, loss is 3.932638211250305 and perplexity is 51.04145834662485
At time: 531.3950366973877 and batch: 450, loss is 3.965491681098938 and perplexity is 52.74619732325221
At time: 532.9423787593842 and batch: 500, loss is 3.9310501337051393 and perplexity is 50.96046488172558
At time: 534.48863530159 and batch: 550, loss is 3.94849977016449 and perplexity is 51.857510263965494
At time: 536.0622291564941 and batch: 600, loss is 3.9776954460144043 and perplexity is 53.39384333751509
At time: 537.6067380905151 and batch: 650, loss is 3.940533800125122 and perplexity is 51.44605588331269
At time: 539.1529595851898 and batch: 700, loss is 3.9096634912490846 and perplexity is 49.882163364322665
At time: 540.6951155662537 and batch: 750, loss is 3.89932785987854 and perplexity is 49.36925489421825
At time: 542.2355811595917 and batch: 800, loss is 3.8546775817871093 and perplexity is 47.21339202952953
At time: 543.7808682918549 and batch: 850, loss is 3.8780018424987794 and perplexity is 48.327552468473684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.339034716288249 and perplexity of 76.63353052550121
Finished 19 epochs...
Completing Train Step...
At time: 547.7816982269287 and batch: 50, loss is 3.982650008201599 and perplexity is 53.659042886069074
At time: 549.3550143241882 and batch: 100, loss is 3.947681074142456 and perplexity is 51.815072100947816
At time: 550.8972902297974 and batch: 150, loss is 3.9416223526000977 and perplexity is 51.50208810624675
At time: 552.4347183704376 and batch: 200, loss is 3.995384702682495 and perplexity is 54.34674394095294
At time: 553.97318816185 and batch: 250, loss is 3.9746945667266846 and perplexity is 53.23385503180979
At time: 555.5091135501862 and batch: 300, loss is 3.9645973777770998 and perplexity is 52.699047310120854
At time: 557.0475373268127 and batch: 350, loss is 3.9158336973190306 and perplexity is 50.19089809043837
At time: 558.5873866081238 and batch: 400, loss is 3.9303626537323 and perplexity is 50.92544262264305
At time: 560.126115322113 and batch: 450, loss is 3.963536901473999 and perplexity is 52.64319084171777
At time: 561.6623060703278 and batch: 500, loss is 3.9292749309539796 and perplexity is 50.87007997377196
At time: 563.2015948295593 and batch: 550, loss is 3.9471163892745973 and perplexity is 51.78582117336076
At time: 564.7399718761444 and batch: 600, loss is 3.9766266679763795 and perplexity is 53.33680765506187
At time: 566.2789280414581 and batch: 650, loss is 3.9397274541854856 and perplexity is 51.404589285499036
At time: 567.815122127533 and batch: 700, loss is 3.90923526763916 and perplexity is 49.8608072171855
At time: 569.3567297458649 and batch: 750, loss is 3.8993762826919554 and perplexity is 49.37164555031713
At time: 570.8958759307861 and batch: 800, loss is 3.854988737106323 and perplexity is 47.22808501337914
At time: 572.4327428340912 and batch: 850, loss is 3.8785051774978636 and perplexity is 48.35188353987736
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.338970184326172 and perplexity of 76.62858537297743
Finished 20 epochs...
Completing Train Step...
At time: 576.469131231308 and batch: 50, loss is 3.979986367225647 and perplexity is 53.516304646812614
At time: 578.0359878540039 and batch: 100, loss is 3.9450144720077516 and perplexity is 51.67708597788431
At time: 579.578827381134 and batch: 150, loss is 3.9388106727600096 and perplexity is 51.3574841087327
At time: 581.1220796108246 and batch: 200, loss is 3.9928032541275025 and perplexity is 54.20663154162106
At time: 582.662398815155 and batch: 250, loss is 3.972242374420166 and perplexity is 53.10347530546917
At time: 584.2008211612701 and batch: 300, loss is 3.9622156858444213 and perplexity is 52.573683762217776
At time: 585.7387883663177 and batch: 350, loss is 3.913525075912476 and perplexity is 50.07515995786222
At time: 587.2777004241943 and batch: 400, loss is 3.928325681686401 and perplexity is 50.82181449922261
At time: 588.8147253990173 and batch: 450, loss is 3.9617856121063233 and perplexity is 52.55107806292381
At time: 590.3537633419037 and batch: 500, loss is 3.927662048339844 and perplexity is 50.78809863711098
At time: 591.891566991806 and batch: 550, loss is 3.9458193588256836 and perplexity is 51.71869692698281
At time: 593.4294517040253 and batch: 600, loss is 3.975567889213562 and perplexity is 53.28036566090527
At time: 594.9628837108612 and batch: 650, loss is 3.9388841247558593 and perplexity is 51.36125655698753
At time: 596.4957382678986 and batch: 700, loss is 3.9087021112442017 and perplexity is 49.83423069431099
At time: 598.0407407283783 and batch: 750, loss is 3.8992495155334472 and perplexity is 49.3653872437821
At time: 599.5950849056244 and batch: 800, loss is 3.8550601959228517 and perplexity is 47.23145999702584
At time: 601.1381549835205 and batch: 850, loss is 3.878742299079895 and perplexity is 48.36335017443607
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.338922500610352 and perplexity of 76.62493152440402
Finished 21 epochs...
Completing Train Step...
At time: 605.3214039802551 and batch: 50, loss is 3.977470579147339 and perplexity is 53.38183818107474
At time: 607.0111870765686 and batch: 100, loss is 3.9425341272354126 and perplexity is 51.54906781804793
At time: 608.5622668266296 and batch: 150, loss is 3.9362285947799682 and perplexity is 51.225046136073395
At time: 610.1083033084869 and batch: 200, loss is 3.9904333257675173 and perplexity is 54.07831781545742
At time: 611.6529684066772 and batch: 250, loss is 3.9699932765960693 and perplexity is 52.9841746044708
At time: 613.2441244125366 and batch: 300, loss is 3.960033402442932 and perplexity is 52.459078181199686
At time: 614.7941858768463 and batch: 350, loss is 3.9114010334014893 and perplexity is 49.96891106789244
At time: 616.3334224224091 and batch: 400, loss is 3.9264535808563235 and perplexity is 50.72675994173251
At time: 617.8700017929077 and batch: 450, loss is 3.9601680278778075 and perplexity is 52.4661409828187
At time: 619.4179475307465 and batch: 500, loss is 3.926160387992859 and perplexity is 50.711889397806125
At time: 620.9767763614655 and batch: 550, loss is 3.944568605422974 and perplexity is 51.65405002790988
At time: 622.5296313762665 and batch: 600, loss is 3.974515256881714 and perplexity is 53.224310533253686
At time: 624.0856580734253 and batch: 650, loss is 3.9380143308639526 and perplexity is 51.31660227258076
At time: 625.6230688095093 and batch: 700, loss is 3.9080877685546875 and perplexity is 49.803624801212315
At time: 627.1589195728302 and batch: 750, loss is 3.8989943647384644 and perplexity is 49.3527932327365
At time: 628.6957323551178 and batch: 800, loss is 3.8549704647064207 and perplexity is 47.22722205080737
At time: 630.2417845726013 and batch: 850, loss is 3.8787975406646726 and perplexity is 48.36602191633981
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.33889897664388 and perplexity of 76.62312902328502
Finished 22 epochs...
Completing Train Step...
At time: 634.3842372894287 and batch: 50, loss is 3.975085964202881 and perplexity is 53.25469470634941
At time: 635.9287707805634 and batch: 100, loss is 3.940208463668823 and perplexity is 51.42932132812909
At time: 637.4689972400665 and batch: 150, loss is 3.9338187074661253 and perplexity is 51.10174817400801
At time: 639.0078818798065 and batch: 200, loss is 3.9882205057144167 and perplexity is 53.95878453095718
At time: 640.5479724407196 and batch: 250, loss is 3.9679030847549437 and perplexity is 52.87354317573677
At time: 642.0925860404968 and batch: 300, loss is 3.9579942655563354 and perplexity is 52.352215930250864
At time: 643.6440560817719 and batch: 350, loss is 3.9094115018844606 and perplexity is 49.86959517326216
At time: 645.188295841217 and batch: 400, loss is 3.924706401824951 and perplexity is 50.638208590491146
At time: 646.7284126281738 and batch: 450, loss is 3.9586511421203614 and perplexity is 52.38661617108934
At time: 648.2686235904694 and batch: 500, loss is 3.9247358798980714 and perplexity is 50.63970132930808
At time: 649.8122234344482 and batch: 550, loss is 3.9433564138412476 and perplexity is 51.591473358422526
At time: 651.3638205528259 and batch: 600, loss is 3.973469223976135 and perplexity is 53.168665261525646
At time: 652.9495573043823 and batch: 650, loss is 3.937119970321655 and perplexity is 51.270727245808175
At time: 654.4889614582062 and batch: 700, loss is 3.9074117517471314 and perplexity is 49.769968091302026
At time: 656.0319583415985 and batch: 750, loss is 3.8986478424072266 and perplexity is 49.335694350515766
At time: 657.578850030899 and batch: 800, loss is 3.8547557544708253 and perplexity is 47.217082971355346
At time: 659.1262683868408 and batch: 850, loss is 3.8787136268615723 and perplexity is 48.36196350978052
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.338886896769206 and perplexity of 76.62220343107983
Finished 23 epochs...
Completing Train Step...
At time: 663.1836588382721 and batch: 50, loss is 3.9728054094314573 and perplexity is 53.13338283999183
At time: 664.729291677475 and batch: 100, loss is 3.9380069398880004 and perplexity is 51.316222994209035
At time: 666.2706425189972 and batch: 150, loss is 3.931546382904053 and perplexity is 50.98576024748279
At time: 667.8086769580841 and batch: 200, loss is 3.9861294555664064 and perplexity is 53.84607189153752
At time: 669.3488583564758 and batch: 250, loss is 3.9659231519699096 and perplexity is 52.76896068146158
At time: 670.8922166824341 and batch: 300, loss is 3.9560649394989014 and perplexity is 52.251308808562385
At time: 672.4384038448334 and batch: 350, loss is 3.9075249528884886 and perplexity is 49.77560242739589
At time: 673.9777193069458 and batch: 400, loss is 3.923045392036438 and perplexity is 50.5541678459213
At time: 675.5176465511322 and batch: 450, loss is 3.957202534675598 and perplexity is 52.31078346806803
At time: 677.0616829395294 and batch: 500, loss is 3.9233648061752318 and perplexity is 50.57031814108523
At time: 678.6095440387726 and batch: 550, loss is 3.942166028022766 and perplexity is 51.53009613871578
At time: 680.158572435379 and batch: 600, loss is 3.9724257612228393 and perplexity is 53.113214675024906
At time: 681.6982145309448 and batch: 650, loss is 3.9362105321884155 and perplexity is 51.22412088734399
At time: 683.2371196746826 and batch: 700, loss is 3.9066915702819824 and perplexity is 49.73413758654298
At time: 684.7789206504822 and batch: 750, loss is 3.8983215999603273 and perplexity is 49.31960157808684
At time: 686.3392570018768 and batch: 800, loss is 3.8544903087615965 and perplexity is 47.204551062622585
At time: 687.8800277709961 and batch: 850, loss is 3.878535022735596 and perplexity is 48.35332663487107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.338891983032227 and perplexity of 76.6225931527508
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 691.9857468605042 and batch: 50, loss is 3.97433304309845 and perplexity is 53.214613213789264
At time: 693.5318510532379 and batch: 100, loss is 3.9442609119415284 and perplexity is 51.63815885835606
At time: 695.0716261863708 and batch: 150, loss is 3.9406115341186525 and perplexity is 51.45005514612521
At time: 696.610030412674 and batch: 200, loss is 3.994802002906799 and perplexity is 54.31508533007726
At time: 698.1528098583221 and batch: 250, loss is 3.9733909606933593 and perplexity is 53.16450427007002
At time: 699.6980056762695 and batch: 300, loss is 3.9631979656219483 and perplexity is 52.625351200392956
At time: 701.2398359775543 and batch: 350, loss is 3.911916847229004 and perplexity is 49.99469237177201
At time: 702.7769467830658 and batch: 400, loss is 3.925182933807373 and perplexity is 50.66234506686179
At time: 704.3130457401276 and batch: 450, loss is 3.959028992652893 and perplexity is 52.40641422202362
At time: 705.8553357124329 and batch: 500, loss is 3.9255514907836915 and perplexity is 50.68102046883627
At time: 707.4097967147827 and batch: 550, loss is 3.9442337274551393 and perplexity is 51.63675512060944
At time: 708.9524054527283 and batch: 600, loss is 3.9704886054992676 and perplexity is 53.010425698490586
At time: 710.4951667785645 and batch: 650, loss is 3.9321139812469483 and perplexity is 51.01470789505175
At time: 712.0385391712189 and batch: 700, loss is 3.9008742141723634 and perplexity is 49.445656310111225
At time: 713.5907301902771 and batch: 750, loss is 3.8886602449417116 and perplexity is 48.845401792651316
At time: 715.134265422821 and batch: 800, loss is 3.843350214958191 and perplexity is 46.681606171591575
At time: 716.6753640174866 and batch: 850, loss is 3.868237533569336 and perplexity is 47.85796365244723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.337022463480632 and perplexity of 76.47947953529388
Finished 25 epochs...
Completing Train Step...
At time: 720.7260127067566 and batch: 50, loss is 3.973614602088928 and perplexity is 53.17639538362275
At time: 722.2968335151672 and batch: 100, loss is 3.941506838798523 and perplexity is 51.4961392478548
At time: 723.8403494358063 and batch: 150, loss is 3.9373330783843996 and perplexity is 51.28165461548097
At time: 725.3895320892334 and batch: 200, loss is 3.9909126567840576 and perplexity is 54.104245443970534
At time: 726.9405543804169 and batch: 250, loss is 3.9691444969177248 and perplexity is 52.939221794007445
At time: 728.4868893623352 and batch: 300, loss is 3.959080777168274 and perplexity is 52.409128133055624
At time: 730.0326476097107 and batch: 350, loss is 3.9086983394622803 and perplexity is 49.834042730815064
At time: 731.6090459823608 and batch: 400, loss is 3.9222068929672242 and perplexity is 50.511795990101014
At time: 733.1584980487823 and batch: 450, loss is 3.956607575416565 and perplexity is 52.27966993965512
At time: 734.7062165737152 and batch: 500, loss is 3.9235562229156495 and perplexity is 50.57999907306235
At time: 736.2523326873779 and batch: 550, loss is 3.9425424766540527 and perplexity is 51.54949822459246
At time: 737.8006756305695 and batch: 600, loss is 3.969443006515503 and perplexity is 52.95502701870012
At time: 739.3512480258942 and batch: 650, loss is 3.93144766330719 and perplexity is 50.98072720221959
At time: 740.9014656543732 and batch: 700, loss is 3.90084406375885 and perplexity is 49.44416552560103
At time: 742.4498884677887 and batch: 750, loss is 3.8896837854385375 and perplexity is 48.89542263428141
At time: 743.9949254989624 and batch: 800, loss is 3.8452607727050783 and perplexity is 46.770879329505945
At time: 745.5498945713043 and batch: 850, loss is 3.8708605003356933 and perplexity is 47.983658274957484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336572329203288 and perplexity of 76.44506124704844
Finished 26 epochs...
Completing Train Step...
At time: 749.6122856140137 and batch: 50, loss is 3.9733264017105103 and perplexity is 53.16107213453945
At time: 751.1759626865387 and batch: 100, loss is 3.9405484056472777 and perplexity is 51.446807285309006
At time: 752.7152028083801 and batch: 150, loss is 3.9361217641830444 and perplexity is 51.21957402611682
At time: 754.2560243606567 and batch: 200, loss is 3.9893778800964355 and perplexity is 54.0212711991175
At time: 755.8006739616394 and batch: 250, loss is 3.967499737739563 and perplexity is 52.852221090293064
At time: 757.346072435379 and batch: 300, loss is 3.9573627519607544 and perplexity is 52.319165231213425
At time: 758.8868861198425 and batch: 350, loss is 3.907351703643799 and perplexity is 49.76697958884308
At time: 760.4379360675812 and batch: 400, loss is 3.920964450836182 and perplexity is 50.44907697708847
At time: 761.9838678836823 and batch: 450, loss is 3.9556089782714845 and perplexity is 52.22748966837876
At time: 763.5294127464294 and batch: 500, loss is 3.9227724266052246 and perplexity is 50.54037018892297
At time: 765.068350315094 and batch: 550, loss is 3.9418799018859865 and perplexity is 51.51535414051579
At time: 766.6117987632751 and batch: 600, loss is 3.9691171550750735 and perplexity is 52.93777435792294
At time: 768.1605880260468 and batch: 650, loss is 3.931379795074463 and perplexity is 50.97726734776967
At time: 769.7025249004364 and batch: 700, loss is 3.9011090850830077 and perplexity is 49.45727102036158
At time: 771.2769658565521 and batch: 750, loss is 3.8904463815689088 and perplexity is 48.93272431562636
At time: 772.82031083107 and batch: 800, loss is 3.8463672256469725 and perplexity is 46.822657746443504
At time: 774.3650221824646 and batch: 850, loss is 3.8721820497512818 and perplexity is 48.04711297053122
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336429278055827 and perplexity of 76.4341264754538
Finished 27 epochs...
Completing Train Step...
At time: 778.352885723114 and batch: 50, loss is 3.972944655418396 and perplexity is 53.14078196546209
At time: 779.9271554946899 and batch: 100, loss is 3.939847149848938 and perplexity is 51.41074256017086
At time: 781.4714710712433 and batch: 150, loss is 3.9353092241287233 and perplexity is 51.17797297420465
At time: 783.014858007431 and batch: 200, loss is 3.9884050846099854 and perplexity is 53.96874510303955
At time: 784.5626440048218 and batch: 250, loss is 3.9665117645263672 and perplexity is 52.800030297395885
At time: 786.1117095947266 and batch: 300, loss is 3.956331100463867 and perplexity is 52.26521791828451
At time: 787.6523149013519 and batch: 350, loss is 3.9065185451507567 and perplexity is 49.72553307528048
At time: 789.1953756809235 and batch: 400, loss is 3.9202145195007323 and perplexity is 50.411257816078255
At time: 790.7413001060486 and batch: 450, loss is 3.955006413459778 and perplexity is 52.19602870048884
At time: 792.2935261726379 and batch: 500, loss is 3.9223283195495604 and perplexity is 50.517929837254265
At time: 793.8371171951294 and batch: 550, loss is 3.9415221977233887 and perplexity is 51.49693017926229
At time: 795.3812382221222 and batch: 600, loss is 3.96897066116333 and perplexity is 52.93001986428518
At time: 796.9307587146759 and batch: 650, loss is 3.9313889837265013 and perplexity is 50.977735762293236
At time: 798.4777565002441 and batch: 700, loss is 3.901313347816467 and perplexity is 49.4673743295593
At time: 800.0209736824036 and batch: 750, loss is 3.8909372806549074 and perplexity is 48.95675124218396
At time: 801.5692284107208 and batch: 800, loss is 3.847034912109375 and perplexity is 46.853931040370185
At time: 803.117443561554 and batch: 850, loss is 3.8729199361801148 and perplexity is 48.08257936661373
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336373964945476 and perplexity of 76.42989878310591
Finished 28 epochs...
Completing Train Step...
At time: 807.2097239494324 and batch: 50, loss is 3.972532649040222 and perplexity is 53.1188921340359
At time: 808.7631132602692 and batch: 100, loss is 3.9392461252212523 and perplexity is 51.37985272147135
At time: 810.336343050003 and batch: 150, loss is 3.9346561813354493 and perplexity is 51.144562478208655
At time: 811.886138677597 and batch: 200, loss is 3.9876680421829223 and perplexity is 53.92898250332478
At time: 813.4355795383453 and batch: 250, loss is 3.965793046951294 and perplexity is 52.76209562144886
At time: 814.9900166988373 and batch: 300, loss is 3.955597047805786 and perplexity is 52.22686657382167
At time: 816.5359179973602 and batch: 350, loss is 3.9059085512161253 and perplexity is 49.69521005107892
At time: 818.0936625003815 and batch: 400, loss is 3.919675498008728 and perplexity is 50.38409238670928
At time: 819.6524059772491 and batch: 450, loss is 3.954569263458252 and perplexity is 52.173216193069884
At time: 821.2013311386108 and batch: 500, loss is 3.9220214223861696 and perplexity is 50.50242840668104
At time: 822.7496778964996 and batch: 550, loss is 3.9412812995910644 and perplexity is 51.484526159074306
At time: 824.2999722957611 and batch: 600, loss is 3.968874797821045 and perplexity is 52.92494605887362
At time: 825.854819059372 and batch: 650, loss is 3.9313963413238526 and perplexity is 50.97811083732668
At time: 827.4041225910187 and batch: 700, loss is 3.9014407920837404 and perplexity is 49.47367906457729
At time: 828.9529128074646 and batch: 750, loss is 3.8912546157836916 and perplexity is 48.97228940441622
At time: 830.5074727535248 and batch: 800, loss is 3.8474653720855714 and perplexity is 46.87410412395157
At time: 832.0668787956238 and batch: 850, loss is 3.873376655578613 and perplexity is 48.10454462893918
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336347262064616 and perplexity of 76.4278579118733
Finished 29 epochs...
Completing Train Step...
At time: 836.1364133358002 and batch: 50, loss is 3.9721220922470093 and perplexity is 53.097088288187116
At time: 837.6830787658691 and batch: 100, loss is 3.938709955215454 and perplexity is 51.352311769515744
At time: 839.240710735321 and batch: 150, loss is 3.934095468521118 and perplexity is 51.11589310503735
At time: 840.7823903560638 and batch: 200, loss is 3.9870638036727906 and perplexity is 53.89640637814986
At time: 842.3250834941864 and batch: 250, loss is 3.965214238166809 and perplexity is 52.73156529347868
At time: 843.8717007637024 and batch: 300, loss is 3.9550196170806884 and perplexity is 52.19671788161466
At time: 845.4301173686981 and batch: 350, loss is 3.905418210029602 and perplexity is 49.670848416062576
At time: 846.9798526763916 and batch: 400, loss is 3.9192450571060182 and perplexity is 50.36240967939697
At time: 848.5728297233582 and batch: 450, loss is 3.954215807914734 and perplexity is 52.15477853922071
At time: 850.1233296394348 and batch: 500, loss is 3.9217644500732423 and perplexity is 50.48945234816022
At time: 851.6744232177734 and batch: 550, loss is 3.941090216636658 and perplexity is 51.47468928356916
At time: 853.2266047000885 and batch: 600, loss is 3.968789539337158 and perplexity is 52.92043395056336
At time: 854.7782843112946 and batch: 650, loss is 3.931380481719971 and perplexity is 50.977302351093314
At time: 856.3264887332916 and batch: 700, loss is 3.901514310836792 and perplexity is 49.47731644147706
At time: 857.8800964355469 and batch: 750, loss is 3.891463460922241 and perplexity is 48.98251809705116
At time: 859.4268593788147 and batch: 800, loss is 3.847759108543396 and perplexity is 46.88787477963344
At time: 860.9784502983093 and batch: 850, loss is 3.8736771821975706 and perplexity is 48.11900349762165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3363386789957685 and perplexity of 76.42720192912215
Finished 30 epochs...
Completing Train Step...
At time: 865.0155546665192 and batch: 50, loss is 3.971717805862427 and perplexity is 53.07562619703918
At time: 866.5577704906464 and batch: 100, loss is 3.938217992782593 and perplexity is 51.3270545745895
At time: 868.1044778823853 and batch: 150, loss is 3.9335898637771605 and perplexity is 51.09005519942607
At time: 869.6562440395355 and batch: 200, loss is 3.9865427732467653 and perplexity is 53.868332025004634
At time: 871.2016432285309 and batch: 250, loss is 3.9647166872024537 and perplexity is 52.705335178265635
At time: 872.7441501617432 and batch: 300, loss is 3.9545314836502077 and perplexity is 52.171245136210366
At time: 874.2871282100677 and batch: 350, loss is 3.9049967527389526 and perplexity is 49.64991868566842
At time: 875.8388900756836 and batch: 400, loss is 3.9188763427734377 and perplexity is 50.34384376009549
At time: 877.3841593265533 and batch: 450, loss is 3.9539105701446533 and perplexity is 52.13886136030604
At time: 878.92564702034 and batch: 500, loss is 3.921539125442505 and perplexity is 50.478077112562254
At time: 880.4686236381531 and batch: 550, loss is 3.9409269189834593 and perplexity is 51.46628427388788
At time: 882.0151739120483 and batch: 600, loss is 3.968704972267151 and perplexity is 52.91595881374792
At time: 883.566798210144 and batch: 650, loss is 3.9313466596603392 and perplexity is 50.975578222890285
At time: 885.1122839450836 and batch: 700, loss is 3.9015491437911987 and perplexity is 49.47903991260145
At time: 886.6572408676147 and batch: 750, loss is 3.891602358818054 and perplexity is 48.98932213826903
At time: 888.2308342456818 and batch: 800, loss is 3.8479671621322633 and perplexity is 46.89763098512729
At time: 889.7831470966339 and batch: 850, loss is 3.8738860511779785 and perplexity is 48.129055114519375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336337089538574 and perplexity of 76.42708045145274
Finished 31 epochs...
Completing Train Step...
At time: 893.8218655586243 and batch: 50, loss is 3.9713239669799805 and perplexity is 53.054727067447644
At time: 895.4184308052063 and batch: 100, loss is 3.937760286331177 and perplexity is 51.30356722614403
At time: 896.9687523841858 and batch: 150, loss is 3.9331235313415527 and perplexity is 51.06623580385962
At time: 898.5149738788605 and batch: 200, loss is 3.986073365211487 and perplexity is 53.843051730955835
At time: 900.0609946250916 and batch: 250, loss is 3.964272313117981 and perplexity is 52.68191949624542
At time: 901.610179901123 and batch: 300, loss is 3.954100012779236 and perplexity is 52.14873961921842
At time: 903.1570980548859 and batch: 350, loss is 3.9046189546585084 and perplexity is 49.63116458454953
At time: 904.704430103302 and batch: 400, loss is 3.918545894622803 and perplexity is 50.32721047839907
At time: 906.2493846416473 and batch: 450, loss is 3.9536350059509275 and perplexity is 52.12449573643019
At time: 907.7969305515289 and batch: 500, loss is 3.9213312911987304 and perplexity is 50.46758712970495
At time: 909.3421409130096 and batch: 550, loss is 3.9407773542404176 and perplexity is 51.458587307916865
At time: 910.8833208084106 and batch: 600, loss is 3.968616638183594 and perplexity is 52.911284737463724
At time: 912.4243037700653 and batch: 650, loss is 3.9312979125976564 and perplexity is 50.9730933737484
At time: 913.970983505249 and batch: 700, loss is 3.9015555286407473 and perplexity is 49.47935582983565
At time: 915.5243277549744 and batch: 750, loss is 3.8916940259933472 and perplexity is 48.99381305688073
At time: 917.0733187198639 and batch: 800, loss is 3.848118658065796 and perplexity is 46.90473632371522
At time: 918.6197030544281 and batch: 850, loss is 3.8740370321273803 and perplexity is 48.13632223353884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336339950561523 and perplexity of 76.42729911139665
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 922.6709196567535 and batch: 50, loss is 3.9715792083740236 and perplexity is 53.06827055830113
At time: 924.2483468055725 and batch: 100, loss is 3.938680067062378 and perplexity is 51.35077696669709
At time: 925.8001184463501 and batch: 150, loss is 3.9347920560836793 and perplexity is 51.15151220489421
At time: 927.3783473968506 and batch: 200, loss is 3.9875020599365234 and perplexity is 53.92003199249637
At time: 928.9338238239288 and batch: 250, loss is 3.9653217887878416 and perplexity is 52.73723691106155
At time: 930.4838464260101 and batch: 300, loss is 3.9549829864501955 and perplexity is 52.194805917947434
At time: 932.0343408584595 and batch: 350, loss is 3.9048748207092285 and perplexity is 49.643865139375535
At time: 933.590282201767 and batch: 400, loss is 3.918331460952759 and perplexity is 50.31641978693824
At time: 935.1413261890411 and batch: 450, loss is 3.953063292503357 and perplexity is 52.09470397825569
At time: 936.6915907859802 and batch: 500, loss is 3.92094575881958 and perplexity is 50.44813399091726
At time: 938.2417962551117 and batch: 550, loss is 3.9398863554000854 and perplexity is 51.41275818617943
At time: 939.7953433990479 and batch: 600, loss is 3.967105441093445 and perplexity is 52.83138574470067
At time: 941.3530976772308 and batch: 650, loss is 3.929677143096924 and perplexity is 50.8905446529453
At time: 942.9042172431946 and batch: 700, loss is 3.8999050664901733 and perplexity is 49.39775938025211
At time: 944.4585289955139 and batch: 750, loss is 3.8893651008605956 and perplexity is 48.87984289979832
At time: 946.0104382038116 and batch: 800, loss is 3.8454277181625365 and perplexity is 46.77868816715818
At time: 947.5653417110443 and batch: 850, loss is 3.8715752220153807 and perplexity is 48.01796549439364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336293856302897 and perplexity of 76.42377633289587
Finished 33 epochs...
Completing Train Step...
At time: 951.6613190174103 and batch: 50, loss is 3.971449604034424 and perplexity is 53.06139312582391
At time: 953.210854768753 and batch: 100, loss is 3.9384241819381716 and perplexity is 51.33763874776395
At time: 954.7599768638611 and batch: 150, loss is 3.9344495344161987 and perplexity is 51.13399470387272
At time: 956.3042089939117 and batch: 200, loss is 3.987250690460205 and perplexity is 53.90647984566073
At time: 957.849127292633 and batch: 250, loss is 3.9650414276123045 and perplexity is 52.7224535097693
At time: 959.401448726654 and batch: 300, loss is 3.9547527647018432 and perplexity is 52.182790921583894
At time: 960.9516742229462 and batch: 350, loss is 3.9047151708602907 and perplexity is 49.63594013643493
At time: 962.4945795536041 and batch: 400, loss is 3.9181909561157227 and perplexity is 50.309350583216116
At time: 964.0418772697449 and batch: 450, loss is 3.952938494682312 and perplexity is 52.088203078368736
At time: 965.5947499275208 and batch: 500, loss is 3.92082088470459 and perplexity is 50.44183471814848
At time: 967.1689035892487 and batch: 550, loss is 3.939869427680969 and perplexity is 51.41188789281593
At time: 968.7131817340851 and batch: 600, loss is 3.967154016494751 and perplexity is 52.83395211279545
At time: 970.2588684558868 and batch: 650, loss is 3.9296984481811523 and perplexity is 50.891628891835424
At time: 971.809798002243 and batch: 700, loss is 3.899971628189087 and perplexity is 49.401047488468805
At time: 973.3589980602264 and batch: 750, loss is 3.8894976568222046 and perplexity is 48.88632264383206
At time: 974.9048383235931 and batch: 800, loss is 3.8456142950057983 and perplexity is 46.78741680138353
At time: 976.4481489658356 and batch: 850, loss is 3.8717711067199705 and perplexity is 48.02737240068396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.33625602722168 and perplexity of 76.42088534633604
Finished 34 epochs...
Completing Train Step...
At time: 980.489305973053 and batch: 50, loss is 3.971357979774475 and perplexity is 53.05653163766542
At time: 982.0316596031189 and batch: 100, loss is 3.938251943588257 and perplexity is 51.32879719902626
At time: 983.5750498771667 and batch: 150, loss is 3.934204125404358 and perplexity is 51.12144750042234
At time: 985.1300988197327 and batch: 200, loss is 3.9870467662811278 and perplexity is 53.89548813178747
At time: 986.6760489940643 and batch: 250, loss is 3.9648250913619996 and perplexity is 52.71104896552277
At time: 988.2250537872314 and batch: 300, loss is 3.954570860862732 and perplexity is 52.17329953486574
At time: 989.7746343612671 and batch: 350, loss is 3.9045916557312013 and perplexity is 49.62980972548855
At time: 991.32976603508 and batch: 400, loss is 3.9180786180496217 and perplexity is 50.30369924550092
At time: 992.8808598518372 and batch: 450, loss is 3.952852849960327 and perplexity is 52.0837421897259
At time: 994.4284937381744 and batch: 500, loss is 3.9207550430297853 and perplexity is 50.43851365260387
At time: 995.9745309352875 and batch: 550, loss is 3.9398671054840086 and perplexity is 51.41176850442476
At time: 997.5214989185333 and batch: 600, loss is 3.9671942234039306 and perplexity is 52.83607644541578
At time: 999.0701961517334 and batch: 650, loss is 3.929729480743408 and perplexity is 50.89320821398239
At time: 1000.6324610710144 and batch: 700, loss is 3.9000325107574465 and perplexity is 49.40405524267853
At time: 1002.1859967708588 and batch: 750, loss is 3.8896132707595825 and perplexity is 48.891974910810966
At time: 1003.7363293170929 and batch: 800, loss is 3.845764169692993 and perplexity is 46.79442957634688
At time: 1005.2852954864502 and batch: 850, loss is 3.8719354009628297 and perplexity is 48.035263669696334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336220423380534 and perplexity of 76.41816451771022
Finished 35 epochs...
Completing Train Step...
At time: 1009.4485123157501 and batch: 50, loss is 3.971275734901428 and perplexity is 53.05216818939465
At time: 1011.007256269455 and batch: 100, loss is 3.9381102228164675 and perplexity is 51.32152335771157
At time: 1012.561910867691 and batch: 150, loss is 3.934005789756775 and perplexity is 51.111309300443445
At time: 1014.1149868965149 and batch: 200, loss is 3.986873207092285 and perplexity is 53.88613488627937
At time: 1015.6707000732422 and batch: 250, loss is 3.9646436643600462 and perplexity is 52.70148662539872
At time: 1017.2222518920898 and batch: 300, loss is 3.9544150829315186 and perplexity is 52.165172719205316
At time: 1018.7748017311096 and batch: 350, loss is 3.90448504447937 and perplexity is 49.62451891138075
At time: 1020.3283445835114 and batch: 400, loss is 3.917981128692627 and perplexity is 50.29879540924683
At time: 1021.8841242790222 and batch: 450, loss is 3.9527848243713377 and perplexity is 52.0801992829922
At time: 1023.4393308162689 and batch: 500, loss is 3.920710120201111 and perplexity is 50.43624786278969
At time: 1024.9916534423828 and batch: 550, loss is 3.9398678588867186 and perplexity is 51.411807238205064
At time: 1026.5407452583313 and batch: 600, loss is 3.9672278881073 and perplexity is 52.83785518619674
At time: 1028.093976020813 and batch: 650, loss is 3.92976104259491 and perplexity is 50.894814523211394
At time: 1029.6499826908112 and batch: 700, loss is 3.900087857246399 and perplexity is 49.406789659345705
At time: 1031.2018609046936 and batch: 750, loss is 3.8897146654129027 and perplexity is 48.89693254699184
At time: 1032.7561490535736 and batch: 800, loss is 3.8458902072906493 and perplexity is 46.800327805525896
At time: 1034.3150129318237 and batch: 850, loss is 3.872075471878052 and perplexity is 48.04199248428609
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336187998453776 and perplexity of 76.41568670449445
Finished 36 epochs...
Completing Train Step...
At time: 1038.3928673267365 and batch: 50, loss is 3.971197900772095 and perplexity is 53.04803908076926
At time: 1039.9797387123108 and batch: 100, loss is 3.937984976768494 and perplexity is 51.315095942247616
At time: 1041.5406031608582 and batch: 150, loss is 3.933836569786072 and perplexity is 51.102660977936104
At time: 1043.0903673171997 and batch: 200, loss is 3.9867212438583373 and perplexity is 53.87794679711711
At time: 1044.6366798877716 and batch: 250, loss is 3.9644861030578613 and perplexity is 52.693183564676644
At time: 1046.2140233516693 and batch: 300, loss is 3.9542773485183718 and perplexity is 52.15798827453813
At time: 1047.7708327770233 and batch: 350, loss is 3.9043890619277954 and perplexity is 49.61975605201432
At time: 1049.3233952522278 and batch: 400, loss is 3.9178940773010256 and perplexity is 50.294417019685795
At time: 1050.8740515708923 and batch: 450, loss is 3.952727117538452 and perplexity is 52.07719398634944
At time: 1052.421516418457 and batch: 500, loss is 3.9206747245788574 and perplexity is 50.43446267200661
At time: 1053.9673664569855 and batch: 550, loss is 3.9398691606521608 and perplexity is 51.4118741643626
At time: 1055.5172173976898 and batch: 600, loss is 3.9672563076019287 and perplexity is 52.83935683267631
At time: 1057.062698841095 and batch: 650, loss is 3.929790720939636 and perplexity is 50.896325019475995
At time: 1058.6077420711517 and batch: 700, loss is 3.9001382780075073 and perplexity is 49.4092808500876
At time: 1060.155190229416 and batch: 750, loss is 3.889803857803345 and perplexity is 48.90129397579124
At time: 1061.711751461029 and batch: 800, loss is 3.845998430252075 and perplexity is 46.80539294967421
At time: 1063.2638158798218 and batch: 850, loss is 3.8721961688995363 and perplexity is 48.047791359631574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336158434549968 and perplexity of 76.41342759187751
Finished 37 epochs...
Completing Train Step...
At time: 1067.3536367416382 and batch: 50, loss is 3.971123251914978 and perplexity is 53.04407925307975
At time: 1068.9297528266907 and batch: 100, loss is 3.9378702068328857 and perplexity is 51.30920684994244
At time: 1070.4766869544983 and batch: 150, loss is 3.9336877155303953 and perplexity is 51.095054695500934
At time: 1072.0254309177399 and batch: 200, loss is 3.98658570766449 and perplexity is 53.87064488012412
At time: 1073.5792424678802 and batch: 250, loss is 3.964346418380737 and perplexity is 52.6858236483895
At time: 1075.130048274994 and batch: 300, loss is 3.9541530227661132 and perplexity is 52.15150409649311
At time: 1076.6753895282745 and batch: 350, loss is 3.9043004322052 and perplexity is 49.615358461681666
At time: 1078.2264165878296 and batch: 400, loss is 3.9178149938583373 and perplexity is 50.29043972131118
At time: 1079.7727909088135 and batch: 450, loss is 3.952676200866699 and perplexity is 52.07454245646153
At time: 1081.3253462314606 and batch: 500, loss is 3.920644688606262 and perplexity is 50.43294784661767
At time: 1082.8753921985626 and batch: 550, loss is 3.939869465827942 and perplexity is 51.41188985402385
At time: 1084.4453139305115 and batch: 600, loss is 3.9672802686691284 and perplexity is 52.840622935224694
At time: 1085.9903750419617 and batch: 650, loss is 3.9298174381256104 and perplexity is 50.89768484422221
At time: 1087.535829782486 and batch: 700, loss is 3.900183539390564 and perplexity is 49.41151723308521
At time: 1089.0796930789948 and batch: 750, loss is 3.8898830604553223 and perplexity is 48.905167241343676
At time: 1090.6248648166656 and batch: 800, loss is 3.8460925340652468 and perplexity is 46.8097977228775
At time: 1092.1739115715027 and batch: 850, loss is 3.8723008918762205 and perplexity is 48.052823330842756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.33613395690918 and perplexity of 76.41155719433708
Finished 38 epochs...
Completing Train Step...
At time: 1096.2213275432587 and batch: 50, loss is 3.9710508298873903 and perplexity is 53.040237832412124
At time: 1097.7936329841614 and batch: 100, loss is 3.937763195037842 and perplexity is 51.30371645338898
At time: 1099.342125415802 and batch: 150, loss is 3.9335541009902952 and perplexity is 51.08822810934215
At time: 1100.89790391922 and batch: 200, loss is 3.9864622449874876 and perplexity is 53.86399427665447
At time: 1102.4471218585968 and batch: 250, loss is 3.9642203712463377 and perplexity is 52.67918316981059
At time: 1103.9972350597382 and batch: 300, loss is 3.9540393114089967 and perplexity is 52.14557421534044
At time: 1105.5495584011078 and batch: 350, loss is 3.904217829704285 and perplexity is 49.61126027825135
At time: 1107.0985646247864 and batch: 400, loss is 3.9177421855926515 and perplexity is 50.286778294907165
At time: 1108.6530559062958 and batch: 450, loss is 3.9526300191879273 and perplexity is 52.07213762219968
At time: 1110.203242778778 and batch: 500, loss is 3.9206179189682007 and perplexity is 50.431597792927754
At time: 1111.7545249462128 and batch: 550, loss is 3.939868493080139 and perplexity is 51.41183984324528
At time: 1113.3072707653046 and batch: 600, loss is 3.967300477027893 and perplexity is 52.841690768279854
At time: 1114.8580939769745 and batch: 650, loss is 3.9298412227630615 and perplexity is 50.89889544160009
At time: 1116.4100160598755 and batch: 700, loss is 3.9002243804931642 and perplexity is 49.41353529513982
At time: 1117.9609069824219 and batch: 750, loss is 3.889953474998474 and perplexity is 48.908610997596575
At time: 1119.5099897384644 and batch: 800, loss is 3.8461752843856813 and perplexity is 46.813671408910714
At time: 1121.0552144050598 and batch: 850, loss is 3.8723922681808474 and perplexity is 48.057214420883355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336113929748535 and perplexity of 76.4100269031298
Finished 39 epochs...
Completing Train Step...
At time: 1125.1437213420868 and batch: 50, loss is 3.970980429649353 and perplexity is 53.03650391847892
At time: 1126.6911671161652 and batch: 100, loss is 3.9376623582839967 and perplexity is 51.298543413982266
At time: 1128.2400312423706 and batch: 150, loss is 3.9334320259094238 and perplexity is 51.08199189041533
At time: 1129.7940332889557 and batch: 200, loss is 3.9863484954833983 and perplexity is 53.85786762247578
At time: 1131.354240655899 and batch: 250, loss is 3.9641053342819212 and perplexity is 52.6731234650425
At time: 1132.909265756607 and batch: 300, loss is 3.9539340019226072 and perplexity is 52.14008308084169
At time: 1134.46470952034 and batch: 350, loss is 3.904140410423279 and perplexity is 49.6074195588256
At time: 1136.0158166885376 and batch: 400, loss is 3.917674250602722 and perplexity is 50.28336217916832
At time: 1137.5665044784546 and batch: 450, loss is 3.9525871896743774 and perplexity is 52.069907445634854
At time: 1139.1113471984863 and batch: 500, loss is 3.920593090057373 and perplexity is 50.43034564682803
At time: 1140.6562237739563 and batch: 550, loss is 3.9398662090301513 and perplexity is 51.41172241616722
At time: 1142.2073969841003 and batch: 600, loss is 3.9673168659210205 and perplexity is 52.8425567921991
At time: 1143.7694718837738 and batch: 650, loss is 3.9298622179031373 and perplexity is 50.899964082257675
At time: 1145.3258213996887 and batch: 700, loss is 3.900260944366455 and perplexity is 49.41534207841451
At time: 1146.8779623508453 and batch: 750, loss is 3.890016407966614 and perplexity is 48.91168905850899
At time: 1148.4277038574219 and batch: 800, loss is 3.8462486600875856 and perplexity is 46.817106520934395
At time: 1149.9811279773712 and batch: 850, loss is 3.8724727725982664 and perplexity is 48.06108339466574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.33609676361084 and perplexity of 76.40871524934472
Finished 40 epochs...
Completing Train Step...
At time: 1154.057361125946 and batch: 50, loss is 3.9709115648269653 and perplexity is 53.03285169481282
At time: 1155.6071026325226 and batch: 100, loss is 3.9375667190551757 and perplexity is 51.29363749545339
At time: 1157.1552259922028 and batch: 150, loss is 3.9333194637298585 and perplexity is 51.07624231367013
At time: 1158.7060573101044 and batch: 200, loss is 3.9862424898147584 and perplexity is 53.85215868580211
At time: 1160.257523059845 and batch: 250, loss is 3.963999238014221 and perplexity is 52.66753533967961
At time: 1161.8092041015625 and batch: 300, loss is 3.9538357877731323 and perplexity is 52.134962438392236
At time: 1163.3992686271667 and batch: 350, loss is 3.9040669202804565 and perplexity is 49.60377403643379
At time: 1164.9444777965546 and batch: 400, loss is 3.917610402107239 and perplexity is 50.28015176463651
At time: 1166.4892592430115 and batch: 450, loss is 3.9525468492507936 and perplexity is 52.06780696587993
At time: 1168.0375905036926 and batch: 500, loss is 3.9205697059631346 and perplexity is 50.4291663926609
At time: 1169.593611240387 and batch: 550, loss is 3.939862461090088 and perplexity is 51.41152972847415
At time: 1171.140102148056 and batch: 600, loss is 3.9673302078247072 and perplexity is 52.84326181720556
At time: 1172.6858673095703 and batch: 650, loss is 3.9298804664611815 and perplexity is 50.90089294168183
At time: 1174.2309346199036 and batch: 700, loss is 3.900293493270874 and perplexity is 49.416950519837
At time: 1175.7855978012085 and batch: 750, loss is 3.8900728702545164 and perplexity is 48.91445080234486
At time: 1177.3339071273804 and batch: 800, loss is 3.8463140678405763 and perplexity is 46.82016882282151
At time: 1178.8832211494446 and batch: 850, loss is 3.8725438642501833 and perplexity is 48.064500257930966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336082140604655 and perplexity of 76.40759793239836
Finished 41 epochs...
Completing Train Step...
At time: 1182.9733974933624 and batch: 50, loss is 3.9708441066741944 and perplexity is 53.02927431726432
At time: 1184.5242545604706 and batch: 100, loss is 3.9374755144119264 and perplexity is 51.28895949087577
At time: 1186.0708413124084 and batch: 150, loss is 3.933214087486267 and perplexity is 51.070860374687584
At time: 1187.6149926185608 and batch: 200, loss is 3.9861430644989015 and perplexity is 53.84680468408127
At time: 1189.159637928009 and batch: 250, loss is 3.963900532722473 and perplexity is 52.662337031792724
At time: 1190.7084007263184 and batch: 300, loss is 3.953743658065796 and perplexity is 52.13015948081173
At time: 1192.2598295211792 and batch: 350, loss is 3.9039971446990966 and perplexity is 49.600313025011204
At time: 1193.8122951984406 and batch: 400, loss is 3.917550024986267 and perplexity is 50.27711608547464
At time: 1195.3564457893372 and batch: 450, loss is 3.952508406639099 and perplexity is 52.065805381868266
At time: 1196.9008231163025 and batch: 500, loss is 3.9205470085144043 and perplexity is 50.428021792232
At time: 1198.4475646018982 and batch: 550, loss is 3.9398573446273804 and perplexity is 51.41126668397248
At time: 1199.9997565746307 and batch: 600, loss is 3.9673407745361327 and perplexity is 52.84382019965408
At time: 1201.5540163516998 and batch: 650, loss is 3.9298960876464846 and perplexity is 50.90168808017305
At time: 1203.1265347003937 and batch: 700, loss is 3.900322561264038 and perplexity is 49.41838699229449
At time: 1204.6732022762299 and batch: 750, loss is 3.890123372077942 and perplexity is 48.91692113367982
At time: 1206.2233583927155 and batch: 800, loss is 3.8463730001449585 and perplexity is 46.82292812456701
At time: 1207.7755663394928 and batch: 850, loss is 3.87260715007782 and perplexity is 48.06754215586323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336069742838542 and perplexity of 76.406650654742
Finished 42 epochs...
Completing Train Step...
At time: 1211.8242824077606 and batch: 50, loss is 3.9707780027389528 and perplexity is 53.02576898940819
At time: 1213.4085710048676 and batch: 100, loss is 3.9373880434036255 and perplexity is 51.284473390079135
At time: 1214.9630300998688 and batch: 150, loss is 3.9331152486801146 and perplexity is 51.06581284126917
At time: 1216.5162637233734 and batch: 200, loss is 3.9860489654541014 and perplexity is 53.841737989584246
At time: 1218.0702724456787 and batch: 250, loss is 3.9638078355789186 and perplexity is 52.65745560982741
At time: 1219.622477054596 and batch: 300, loss is 3.9536564111709596 and perplexity is 52.125611484671865
At time: 1221.1751844882965 and batch: 350, loss is 3.9039305114746092 and perplexity is 49.59700810632868
At time: 1222.7181601524353 and batch: 400, loss is 3.917492446899414 and perplexity is 50.27422130865661
At time: 1224.2630093097687 and batch: 450, loss is 3.9524711656570433 and perplexity is 52.06386643624866
At time: 1225.824233531952 and batch: 500, loss is 3.920525336265564 and perplexity is 50.42692891543778
At time: 1227.376933813095 and batch: 550, loss is 3.939850912094116 and perplexity is 51.41093598035301
At time: 1228.9262619018555 and batch: 600, loss is 3.9673488473892213 and perplexity is 52.844246801773146
At time: 1230.4750893115997 and batch: 650, loss is 3.92990927696228 and perplexity is 50.90235944303906
At time: 1232.0353529453278 and batch: 700, loss is 3.900348505973816 and perplexity is 49.41966915463529
At time: 1233.5842974185944 and batch: 750, loss is 3.890169034004211 and perplexity is 48.9191548255229
At time: 1235.1337642669678 and batch: 800, loss is 3.8464261531829833 and perplexity is 46.82541697159035
At time: 1236.6906476020813 and batch: 850, loss is 3.872663655281067 and perplexity is 48.07025829883972
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336057980855306 and perplexity of 76.40575196628305
Finished 43 epochs...
Completing Train Step...
At time: 1240.7603027820587 and batch: 50, loss is 3.9707132053375243 and perplexity is 53.02233316868626
At time: 1242.3541197776794 and batch: 100, loss is 3.93730384349823 and perplexity is 51.28015542406015
At time: 1243.908236026764 and batch: 150, loss is 3.933021454811096 and perplexity is 51.06102340572156
At time: 1245.4806110858917 and batch: 200, loss is 3.985959377288818 and perplexity is 53.836914623123604
At time: 1247.0491034984589 and batch: 250, loss is 3.9637204122543337 and perplexity is 52.65285232121415
At time: 1248.5970766544342 and batch: 300, loss is 3.9535736227035523 and perplexity is 52.121296263812106
At time: 1250.1447083950043 and batch: 350, loss is 3.903866605758667 and perplexity is 49.59383867529051
At time: 1251.7160017490387 and batch: 400, loss is 3.917437391281128 and perplexity is 50.27145350651084
At time: 1253.305760383606 and batch: 450, loss is 3.9524350929260255 and perplexity is 52.06198838427239
At time: 1254.85657787323 and batch: 500, loss is 3.9205039310455323 and perplexity is 50.42584952748114
At time: 1256.409170627594 and batch: 550, loss is 3.939843468666077 and perplexity is 51.410553308174805
At time: 1257.959582567215 and batch: 600, loss is 3.9673548126220703 and perplexity is 52.84456203095026
At time: 1259.507069826126 and batch: 650, loss is 3.9299205493927003 and perplexity is 50.90293323957815
At time: 1261.0579755306244 and batch: 700, loss is 3.900371398925781 and perplexity is 49.42080052969759
At time: 1262.6141214370728 and batch: 750, loss is 3.8902101612091062 and perplexity is 48.92116677499935
At time: 1264.1632115840912 and batch: 800, loss is 3.8464744901657104 and perplexity is 46.82768042566553
At time: 1265.710785627365 and batch: 850, loss is 3.8727145910263063 and perplexity is 48.07270685562902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336050669352214 and perplexity of 76.40519332743355
Finished 44 epochs...
Completing Train Step...
At time: 1269.794541835785 and batch: 50, loss is 3.970649552345276 and perplexity is 53.0189582459372
At time: 1271.3463397026062 and batch: 100, loss is 3.937222428321838 and perplexity is 51.27598061110977
At time: 1272.9001083374023 and batch: 150, loss is 3.9329321241378783 and perplexity is 51.05646229385217
At time: 1274.447628736496 and batch: 200, loss is 3.985873680114746 and perplexity is 53.832301149363296
At time: 1275.9988341331482 and batch: 250, loss is 3.9636373138427734 and perplexity is 52.64847713461018
At time: 1277.550477027893 and batch: 300, loss is 3.9534947443008424 and perplexity is 52.11718518135558
At time: 1279.0954239368439 and batch: 350, loss is 3.9038050889968874 and perplexity is 49.590787916768335
At time: 1280.6453309059143 and batch: 400, loss is 3.9173843812942506 and perplexity is 50.268788688051764
At time: 1282.2417509555817 and batch: 450, loss is 3.9523999166488646 and perplexity is 52.06015706954905
At time: 1283.7889308929443 and batch: 500, loss is 3.9204828548431396 and perplexity is 50.42478675327033
At time: 1285.3412759304047 and batch: 550, loss is 3.9398348760604858 and perplexity is 51.410111559464895
At time: 1286.8911366462708 and batch: 600, loss is 3.9673588037490846 and perplexity is 52.844772940730216
At time: 1288.4419724941254 and batch: 650, loss is 3.9299297285079957 and perplexity is 50.90340048561567
At time: 1289.9879879951477 and batch: 700, loss is 3.9003918075561526 and perplexity is 49.42180915084052
At time: 1291.537880897522 and batch: 750, loss is 3.890247497558594 and perplexity is 48.92299334687796
At time: 1293.091067314148 and batch: 800, loss is 3.846518621444702 and perplexity is 46.82974703669568
At time: 1294.641176700592 and batch: 850, loss is 3.8727606439590456 and perplexity is 48.07492079574327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336042404174805 and perplexity of 76.40456182756546
Finished 45 epochs...
Completing Train Step...
At time: 1298.726350069046 and batch: 50, loss is 3.9705869150161743 and perplexity is 53.015637384006965
At time: 1300.2730696201324 and batch: 100, loss is 3.937143759727478 and perplexity is 51.271946960453555
At time: 1301.826013326645 and batch: 150, loss is 3.932846522331238 and perplexity is 51.05209195549625
At time: 1303.3677127361298 and batch: 200, loss is 3.9857914209365846 and perplexity is 53.82787313063729
At time: 1304.9095888137817 and batch: 250, loss is 3.9635581493377687 and perplexity is 52.644309408948715
At time: 1306.459121465683 and batch: 300, loss is 3.953419051170349 and perplexity is 52.11324041775435
At time: 1308.011263370514 and batch: 350, loss is 3.903745698928833 and perplexity is 49.58784280395517
At time: 1309.5584199428558 and batch: 400, loss is 3.9173331689834594 and perplexity is 50.266214373141246
At time: 1311.1084723472595 and batch: 450, loss is 3.9523653316497804 and perplexity is 52.05835660019927
At time: 1312.6607942581177 and batch: 500, loss is 3.9204619216918943 and perplexity is 50.42373121463063
At time: 1314.2089819908142 and batch: 550, loss is 3.939825358390808 and perplexity is 51.40962225733349
At time: 1315.7547833919525 and batch: 600, loss is 3.9673609590530394 and perplexity is 52.84488683740107
At time: 1317.3013753890991 and batch: 650, loss is 3.9299371814727784 and perplexity is 50.90377986828057
At time: 1318.8512296676636 and batch: 700, loss is 3.90040976524353 and perplexity is 49.42269666020766
At time: 1320.405398607254 and batch: 750, loss is 3.8902812576293946 and perplexity is 48.92464501847725
At time: 1321.9971776008606 and batch: 800, loss is 3.846559076309204 and perplexity is 46.83164156608791
At time: 1323.544296503067 and batch: 850, loss is 3.8728022003173828 and perplexity is 48.076918655890495
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336037317911784 and perplexity of 76.40417321485633
Finished 46 epochs...
Completing Train Step...
At time: 1327.6389379501343 and batch: 50, loss is 3.9705250692367553 and perplexity is 53.01235869197923
At time: 1329.1892938613892 and batch: 100, loss is 3.9370672845840455 and perplexity is 51.268026080882514
At time: 1330.7373249530792 and batch: 150, loss is 3.9327643299102784 and perplexity is 51.04789603290226
At time: 1332.2932682037354 and batch: 200, loss is 3.98571222782135 and perplexity is 53.823610502465264
At time: 1333.8494718074799 and batch: 250, loss is 3.963482060432434 and perplexity is 52.64030391346251
At time: 1335.4073526859283 and batch: 300, loss is 3.9533462381362914 and perplexity is 52.109446032746966
At time: 1336.9575867652893 and batch: 350, loss is 3.9036881017684935 and perplexity is 49.5849867672729
At time: 1338.5125613212585 and batch: 400, loss is 3.9172835159301758 and perplexity is 50.26371856408339
At time: 1340.0740251541138 and batch: 450, loss is 3.952331290245056 and perplexity is 52.05658449077568
At time: 1341.6327126026154 and batch: 500, loss is 3.920441164970398 and perplexity is 50.42268459414725
At time: 1343.1802036762238 and batch: 550, loss is 3.9398150730133055 and perplexity is 51.40909349268059
At time: 1344.7333431243896 and batch: 600, loss is 3.9673617696762085 and perplexity is 52.84492967470807
At time: 1346.2924008369446 and batch: 650, loss is 3.9299430227279664 and perplexity is 50.90407721111724
At time: 1347.8556225299835 and batch: 700, loss is 3.900425510406494 and perplexity is 49.423474834746926
At time: 1349.407558441162 and batch: 750, loss is 3.8903121614456175 and perplexity is 48.926157000078554
At time: 1350.9607229232788 and batch: 800, loss is 3.8465964126586916 and perplexity is 46.83339012126663
At time: 1352.5166342258453 and batch: 850, loss is 3.8728402519226073 and perplexity is 48.078748094625915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336031277974446 and perplexity of 76.4037117398314
Finished 47 epochs...
Completing Train Step...
At time: 1356.5816750526428 and batch: 50, loss is 3.9704644680023193 and perplexity is 53.00914617494437
At time: 1358.1630890369415 and batch: 100, loss is 3.9369926929473875 and perplexity is 51.26420205753077
At time: 1359.7226722240448 and batch: 150, loss is 3.9326849794387817 and perplexity is 51.04384551899035
At time: 1361.3061797618866 and batch: 200, loss is 3.98563551902771 and perplexity is 53.81948191658571
At time: 1362.8575189113617 and batch: 250, loss is 3.963408908843994 and perplexity is 52.63645333245505
At time: 1364.4079711437225 and batch: 300, loss is 3.9532762336730958 and perplexity is 52.10579826663147
At time: 1365.964213848114 and batch: 350, loss is 3.903632411956787 and perplexity is 49.58222546558526
At time: 1367.5121212005615 and batch: 400, loss is 3.9172352886199953 and perplexity is 50.261294538589965
At time: 1369.0634579658508 and batch: 450, loss is 3.9522978687286376 and perplexity is 52.05484470985565
At time: 1370.6059069633484 and batch: 500, loss is 3.9204204893112182 and perplexity is 50.42164208268299
At time: 1372.151959657669 and batch: 550, loss is 3.9398039531707765 and perplexity is 51.408521834834765
At time: 1373.6993086338043 and batch: 600, loss is 3.9673612976074217 and perplexity is 52.84490472827212
At time: 1375.2462825775146 and batch: 650, loss is 3.929947462081909 and perplexity is 50.90430319283471
At time: 1376.7863173484802 and batch: 700, loss is 3.9004393005371094 and perplexity is 49.424156395619754
At time: 1378.3360121250153 and batch: 750, loss is 3.8903401279449463 and perplexity is 48.92752531254882
At time: 1379.8793499469757 and batch: 800, loss is 3.8466307592391966 and perplexity is 46.834998715695455
At time: 1381.431795835495 and batch: 850, loss is 3.87287495136261 and perplexity is 48.0804164292058
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336027145385742 and perplexity of 76.40339599536773
Finished 48 epochs...
Completing Train Step...
At time: 1385.4883270263672 and batch: 50, loss is 3.9704043674468994 and perplexity is 53.005960391551554
At time: 1387.0665514469147 and batch: 100, loss is 3.936920094490051 and perplexity is 51.260480490636446
At time: 1388.6141674518585 and batch: 150, loss is 3.9326080322265624 and perplexity is 51.039917988484916
At time: 1390.1615571975708 and batch: 200, loss is 3.985561280250549 and perplexity is 53.81548657236736
At time: 1391.7079722881317 and batch: 250, loss is 3.963338303565979 and perplexity is 52.632737052229814
At time: 1393.2590579986572 and batch: 300, loss is 3.9532082891464233 and perplexity is 52.102258083100764
At time: 1394.8113312721252 and batch: 350, loss is 3.9035780096054076 and perplexity is 49.57952814930415
At time: 1396.3694338798523 and batch: 400, loss is 3.917188262939453 and perplexity is 50.25893102258277
At time: 1397.9193835258484 and batch: 450, loss is 3.952264652252197 and perplexity is 52.053115660049365
At time: 1399.5153064727783 and batch: 500, loss is 3.92039975643158 and perplexity is 50.42059670768337
At time: 1401.0736482143402 and batch: 550, loss is 3.9397922468185427 and perplexity is 51.40792003209282
At time: 1402.625595331192 and batch: 600, loss is 3.967359628677368 and perplexity is 52.84481653389603
At time: 1404.17453622818 and batch: 650, loss is 3.929950542449951 and perplexity is 50.904459997064976
At time: 1405.7201771736145 and batch: 700, loss is 3.9004514169692994 and perplexity is 49.424755243687216
At time: 1407.2683682441711 and batch: 750, loss is 3.890365629196167 and perplexity is 48.92877304157267
At time: 1408.8290638923645 and batch: 800, loss is 3.8466626596450806 and perplexity is 46.836492794994804
At time: 1410.3786203861237 and batch: 850, loss is 3.8729067707061766 and perplexity is 48.08194634083525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336023648579915 and perplexity of 76.40312882799452
Finished 49 epochs...
Completing Train Step...
At time: 1414.442540884018 and batch: 50, loss is 3.9703452348709107 and perplexity is 53.00282610524097
At time: 1416.0204379558563 and batch: 100, loss is 3.93684907913208 and perplexity is 51.256840338519524
At time: 1417.5805833339691 and batch: 150, loss is 3.9325335121154783 and perplexity is 51.036114629841805
At time: 1419.136155128479 and batch: 200, loss is 3.9854891204833987 and perplexity is 53.81160339949332
At time: 1420.692024230957 and batch: 250, loss is 3.9632698678970337 and perplexity is 52.629135218909575
At time: 1422.2522461414337 and batch: 300, loss is 3.953142595291138 and perplexity is 52.09883539732415
At time: 1423.8119323253632 and batch: 350, loss is 3.903524990081787 and perplexity is 49.57689953602488
At time: 1425.3730807304382 and batch: 400, loss is 3.9171423053741456 and perplexity is 50.25662129755309
At time: 1426.9297614097595 and batch: 450, loss is 3.9522318744659426 and perplexity is 52.05140950211256
At time: 1428.4852185249329 and batch: 500, loss is 3.9203791093826292 and perplexity is 50.419555681902125
At time: 1430.0394985675812 and batch: 550, loss is 3.9397799253463743 and perplexity is 51.407286614739235
At time: 1431.5988600254059 and batch: 600, loss is 3.9673566722869875 and perplexity is 52.84466030421971
At time: 1433.1557545661926 and batch: 650, loss is 3.929952311515808 and perplexity is 50.904550050486776
At time: 1434.7099151611328 and batch: 700, loss is 3.900461812019348 and perplexity is 49.42526901916197
At time: 1436.2656953334808 and batch: 750, loss is 3.890389165878296 and perplexity is 48.92992467610349
At time: 1437.8182651996613 and batch: 800, loss is 3.8466921138763426 and perplexity is 46.83787234820183
At time: 1439.4050681591034 and batch: 850, loss is 3.872936043739319 and perplexity is 48.0833538658452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.336019515991211 and perplexity of 76.40281308593978
Finished Training.
Improved accuracyfrom -76.934933086985 to -76.40281308593978
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f77b0541e48>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'batch_size': 50, 'seq_len': 50, 'wordvec_source': 'glove', 'lr': 3.607194594002684, 'anneal': 5.151391374298254, 'dropout': 0.9606034870423327, 'wordvec_dim': 200, 'tune_wordvecs': True, 'data': 'wikitext', 'num_layers': 1}, 'best_accuracy': -76.934933086985}, {'params': {'batch_size': 50, 'seq_len': 50, 'wordvec_source': 'glove', 'lr': 14.952918635629437, 'anneal': 4.902308889829855, 'dropout': 0.672335060084884, 'wordvec_dim': 200, 'tune_wordvecs': True, 'data': 'wikitext', 'num_layers': 1}, 'best_accuracy': -168.00790632158106}, {'params': {'batch_size': 50, 'seq_len': 50, 'wordvec_source': 'glove', 'lr': 29.273323309911518, 'anneal': 5.008675651095301, 'dropout': 0.2177491901710491, 'wordvec_dim': 200, 'tune_wordvecs': True, 'data': 'wikitext', 'num_layers': 1}, 'best_accuracy': -157.50864556302767}, {'params': {'batch_size': 50, 'seq_len': 50, 'wordvec_source': 'glove', 'lr': 25.018820752002313, 'anneal': 4.547382136946763, 'dropout': 0.7545146846688054, 'wordvec_dim': 200, 'tune_wordvecs': True, 'data': 'wikitext', 'num_layers': 1}, 'best_accuracy': -263.18425071310844}, {'params': {'batch_size': 50, 'seq_len': 50, 'wordvec_source': 'glove', 'lr': 7.703220052179859, 'anneal': 6.045581687791914, 'dropout': 0.3862150257859597, 'wordvec_dim': 200, 'tune_wordvecs': True, 'data': 'wikitext', 'num_layers': 1}, 'best_accuracy': -80.18145747984562}, {'params': {'batch_size': 50, 'seq_len': 50, 'wordvec_source': 'glove', 'lr': 5.015641241631757, 'anneal': 7.593085859843627, 'dropout': 0.0, 'wordvec_dim': 200, 'tune_wordvecs': True, 'data': 'wikitext', 'num_layers': 1}, 'best_accuracy': -76.40281308593978}]
