Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'type': 'continuous', 'name': 'lr', 'domain': [0, 30]}, {'type': 'continuous', 'name': 'dropout', 'domain': [0, 1]}, {'type': 'continuous', 'name': 'anneal', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'dropout': 0.9780999155560108, 'batch_size': 50, 'data': 'wikitext', 'lr': 25.20715218434082, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'anneal': 7.690045114466656, 'wordvec_source': '', 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.1112072467803955 and batch: 50, loss is 10.231368446350098 and perplexity is 27760.472804833287
At time: 3.2818799018859863 and batch: 100, loss is 8.82466453552246 and perplexity is 6799.90918224265
At time: 4.448062896728516 and batch: 150, loss is 8.337627201080322 and perplexity is 4178.164027170547
At time: 5.616817474365234 and batch: 200, loss is 8.160635185241699 and perplexity is 3500.409306105881
At time: 6.7882397174835205 and batch: 250, loss is 8.039861488342286 and perplexity is 3102.183471992833
At time: 7.96095085144043 and batch: 300, loss is 7.896625318527222 and perplexity is 2688.1952012720485
At time: 9.130802631378174 and batch: 350, loss is 7.815486612319947 and perplexity is 2478.6928220590053
At time: 10.30039644241333 and batch: 400, loss is 7.776104497909546 and perplexity is 2382.973836735771
At time: 11.46916937828064 and batch: 450, loss is 7.731712779998779 and perplexity is 2279.5031464992444
At time: 12.635003805160522 and batch: 500, loss is 7.667974290847778 and perplexity is 2138.7445796473585
At time: 13.833943128585815 and batch: 550, loss is 7.580646877288818 and perplexity is 1959.8963678657358
At time: 15.013733148574829 and batch: 600, loss is 7.560781383514405 and perplexity is 1921.3462352512245
At time: 16.180917263031006 and batch: 650, loss is 7.5516700935363765 and perplexity is 1903.9198017534252
At time: 17.350186347961426 and batch: 700, loss is 7.502934503555298 and perplexity is 1813.355913777723
At time: 18.519925355911255 and batch: 750, loss is 7.442385244369507 and perplexity is 1706.8165441875851
At time: 19.690775156021118 and batch: 800, loss is 7.443051700592041 and perplexity is 1707.9544418315484
At time: 20.861021518707275 and batch: 850, loss is 7.437438049316406 and perplexity is 1698.393442366888
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.6134033203125 and perplexity of 745.0142310759608
Finished 1 epochs...
Completing Train Step...
At time: 24.080560207366943 and batch: 50, loss is 6.646681451797486 and perplexity is 770.2240540425317
At time: 25.24862003326416 and batch: 100, loss is 6.076424264907837 and perplexity is 435.46928466516465
At time: 26.419404983520508 and batch: 150, loss is 5.910980758666992 and perplexity is 369.0679445517664
At time: 27.589173316955566 and batch: 200, loss is 5.868528051376343 and perplexity is 353.7279275443253
At time: 28.756778955459595 and batch: 250, loss is 5.866481666564941 and perplexity is 353.00480423266237
At time: 29.92666721343994 and batch: 300, loss is 5.818731708526611 and perplexity is 336.5449457535735
At time: 31.09649896621704 and batch: 350, loss is 5.7962676048278805 and perplexity is 329.0690492314428
At time: 32.315459966659546 and batch: 400, loss is 5.828704681396484 and perplexity is 339.91809155508577
At time: 33.48231053352356 and batch: 450, loss is 5.766924591064453 and perplexity is 319.5534623328333
At time: 34.65051770210266 and batch: 500, loss is 5.817310600280762 and perplexity is 336.0670186295856
At time: 35.82234573364258 and batch: 550, loss is 5.779161071777343 and perplexity is 323.4876935941542
At time: 36.994662284851074 and batch: 600, loss is 5.775689134597778 and perplexity is 322.3665121057392
At time: 38.167808055877686 and batch: 650, loss is 5.8006829833984375 and perplexity is 330.52522607992995
At time: 39.33884787559509 and batch: 700, loss is 5.748488206863403 and perplexity is 313.7160278653076
At time: 40.51129460334778 and batch: 750, loss is 5.79006817817688 and perplexity is 327.0353202878277
At time: 41.68025612831116 and batch: 800, loss is 5.8282809066772465 and perplexity is 339.77407337905953
At time: 42.84982657432556 and batch: 850, loss is 5.76915280342102 and perplexity is 320.2662891760494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.50010617574056 and perplexity of 244.7179139906276
Finished 2 epochs...
Completing Train Step...
At time: 46.07172513008118 and batch: 50, loss is 5.775799579620362 and perplexity is 322.40211784865124
At time: 47.24008846282959 and batch: 100, loss is 5.728483486175537 and perplexity is 307.5025826815024
At time: 48.408461809158325 and batch: 150, loss is 5.717735147476196 and perplexity is 304.2151396961657
At time: 49.57785606384277 and batch: 200, loss is 5.75180814743042 and perplexity is 314.7592772374724
At time: 50.746763706207275 and batch: 250, loss is 5.757888584136963 and perplexity is 316.67898150610347
At time: 51.91597008705139 and batch: 300, loss is 5.713130235671997 and perplexity is 302.8174763364261
At time: 53.085832595825195 and batch: 350, loss is 5.7236332321167 and perplexity is 306.0147281847137
At time: 54.254446268081665 and batch: 400, loss is 5.706114158630371 and perplexity is 300.7003213389832
At time: 55.42280387878418 and batch: 450, loss is 5.758307695388794 and perplexity is 316.8117330473601
At time: 56.61093759536743 and batch: 500, loss is 5.743588438034058 and perplexity is 312.1826515132737
At time: 57.778257608413696 and batch: 550, loss is 5.676022024154663 and perplexity is 291.7863989546402
At time: 58.947739362716675 and batch: 600, loss is 5.719176378250122 and perplexity is 304.65390001888227
At time: 60.13384938240051 and batch: 650, loss is 5.788073968887329 and perplexity is 326.3837932706334
At time: 61.30297017097473 and batch: 700, loss is 5.752408666610718 and perplexity is 314.94835298675497
At time: 62.47299838066101 and batch: 750, loss is 5.691433200836181 and perplexity is 296.3179996625494
At time: 63.642149209976196 and batch: 800, loss is 5.684236621856689 and perplexity is 294.19317866648845
At time: 64.8122673034668 and batch: 850, loss is 5.698632221221924 and perplexity is 298.45889591451623
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.39418093363444 and perplexity of 220.12177889439528
Finished 3 epochs...
Completing Train Step...
At time: 68.0108871459961 and batch: 50, loss is 5.675910949707031 and perplexity is 291.7539907414454
At time: 69.2065258026123 and batch: 100, loss is 5.615879764556885 and perplexity is 274.75499260265997
At time: 70.37032079696655 and batch: 150, loss is 5.6563725185394285 and perplexity is 286.10890313661577
At time: 71.53480672836304 and batch: 200, loss is 5.669174747467041 and perplexity is 289.7952813932323
At time: 72.69391703605652 and batch: 250, loss is 5.701705293655396 and perplexity is 299.3774924540088
At time: 73.86284232139587 and batch: 300, loss is 5.65566614151001 and perplexity is 285.90687374239013
At time: 75.03146147727966 and batch: 350, loss is 5.653384504318237 and perplexity is 285.25528161696474
At time: 76.20061492919922 and batch: 400, loss is 5.655180168151856 and perplexity is 285.76796437469795
At time: 77.37006688117981 and batch: 450, loss is 5.646722450256347 and perplexity is 283.3612117128068
At time: 78.5372805595398 and batch: 500, loss is 5.700097465515137 and perplexity is 298.8965316518487
At time: 79.70443630218506 and batch: 550, loss is 5.685619077682495 and perplexity is 294.6001689985066
At time: 80.87181544303894 and batch: 600, loss is 5.693856182098389 and perplexity is 297.03684314384503
At time: 82.03966116905212 and batch: 650, loss is 5.695419464111328 and perplexity is 297.50155864396197
At time: 83.20854187011719 and batch: 700, loss is 5.67365255355835 and perplexity is 291.0958381168031
At time: 84.37606024742126 and batch: 750, loss is 5.661112003326416 and perplexity is 287.46813040460563
At time: 85.54265093803406 and batch: 800, loss is 5.7236911392211915 and perplexity is 306.03244912463396
At time: 86.73898530006409 and batch: 850, loss is 5.72737117767334 and perplexity is 307.1607350995695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.506153742472331 and perplexity of 246.20234598252804
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 89.91665410995483 and batch: 50, loss is 5.678907308578491 and perplexity is 292.6295014169373
At time: 91.11693286895752 and batch: 100, loss is 5.562105484008789 and perplexity is 260.37046545983617
At time: 92.2821455001831 and batch: 150, loss is 5.5284714508056645 and perplexity is 251.75879106384804
At time: 93.44845747947693 and batch: 200, loss is 5.545120582580567 and perplexity is 255.98544376770852
At time: 94.61633968353271 and batch: 250, loss is 5.555754499435425 and perplexity is 258.72209657360173
At time: 95.80720901489258 and batch: 300, loss is 5.485368432998658 and perplexity is 241.13777074563552
At time: 96.98942470550537 and batch: 350, loss is 5.458215999603271 and perplexity is 234.67838436180566
At time: 98.15918231010437 and batch: 400, loss is 5.455866651535034 and perplexity is 234.127690292166
At time: 99.32594561576843 and batch: 450, loss is 5.457944412231445 and perplexity is 234.61465733029257
At time: 100.4899091720581 and batch: 500, loss is 5.463604164123535 and perplexity is 235.94628286145127
At time: 101.65550994873047 and batch: 550, loss is 5.425479383468628 and perplexity is 227.1201978906307
At time: 102.82399773597717 and batch: 600, loss is 5.436417074203491 and perplexity is 229.61800358627428
At time: 103.99538564682007 and batch: 650, loss is 5.427747640609741 and perplexity is 227.63594960901827
At time: 105.16575074195862 and batch: 700, loss is 5.371754035949707 and perplexity is 215.24007561995714
At time: 106.3370099067688 and batch: 750, loss is 5.31927526473999 and perplexity is 204.23581144956296
At time: 107.50974416732788 and batch: 800, loss is 5.303274984359741 and perplexity is 200.99398542642336
At time: 108.6801917552948 and batch: 850, loss is 5.318901510238647 and perplexity is 204.15949165901935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.232655843098958 and perplexity of 187.2895552536772
Finished 5 epochs...
Completing Train Step...
At time: 111.92836332321167 and batch: 50, loss is 5.462074584960938 and perplexity is 235.58566021444702
At time: 113.11258602142334 and batch: 100, loss is 5.394895238876343 and perplexity is 220.27906920487516
At time: 114.28546404838562 and batch: 150, loss is 5.378893013000488 and perplexity is 216.78216751013713
At time: 115.4512038230896 and batch: 200, loss is 5.412778730392456 and perplexity is 224.2538637382437
At time: 116.66439843177795 and batch: 250, loss is 5.426630859375 and perplexity is 227.38187195312867
At time: 117.83428835868835 and batch: 300, loss is 5.369684267044067 and perplexity is 214.7950391244174
At time: 119.00587344169617 and batch: 350, loss is 5.349635190963745 and perplexity is 210.53148006926776
At time: 120.1796805858612 and batch: 400, loss is 5.357055368423462 and perplexity is 212.09947120414347
At time: 121.3555998802185 and batch: 450, loss is 5.369568614959717 and perplexity is 214.7701990668645
At time: 122.52727627754211 and batch: 500, loss is 5.376813097000122 and perplexity is 216.33174739172262
At time: 123.69955396652222 and batch: 550, loss is 5.34703408241272 and perplexity is 209.98457642242752
At time: 124.87360382080078 and batch: 600, loss is 5.364857378005982 and perplexity is 213.76074552301023
At time: 126.06073760986328 and batch: 650, loss is 5.361107292175293 and perplexity is 212.96062557698644
At time: 127.22712659835815 and batch: 700, loss is 5.311545877456665 and perplexity is 202.66327895112556
At time: 128.3941934108734 and batch: 750, loss is 5.274139671325684 and perplexity is 195.22244877282415
At time: 129.56355714797974 and batch: 800, loss is 5.27306884765625 and perplexity is 195.0135118411444
At time: 130.73344612121582 and batch: 850, loss is 5.293913917541504 and perplexity is 199.12124639199004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.2139279047648115 and perplexity of 183.81464849588176
Finished 6 epochs...
Completing Train Step...
At time: 133.9944453239441 and batch: 50, loss is 5.391795310974121 and perplexity is 219.5972772708677
At time: 135.15852665901184 and batch: 100, loss is 5.332968711853027 and perplexity is 207.05173961272143
At time: 136.3262813091278 and batch: 150, loss is 5.317141275405884 and perplexity is 203.8004391114706
At time: 137.494323015213 and batch: 200, loss is 5.351776657104492 and perplexity is 210.98280918592982
At time: 138.6654417514801 and batch: 250, loss is 5.364759111404419 and perplexity is 213.739741013038
At time: 139.83580803871155 and batch: 300, loss is 5.3145772647857665 and perplexity is 203.27856195639637
At time: 141.00479817390442 and batch: 350, loss is 5.294759635925293 and perplexity is 199.2897181204442
At time: 142.17145490646362 and batch: 400, loss is 5.306337366104126 and perplexity is 201.61044918002116
At time: 143.3344371318817 and batch: 450, loss is 5.323717298507691 and perplexity is 205.14505176361646
At time: 144.49613285064697 and batch: 500, loss is 5.332496089935303 and perplexity is 206.95390554356123
At time: 145.71270036697388 and batch: 550, loss is 5.303099727630615 and perplexity is 200.95876296454034
At time: 146.8802616596222 and batch: 600, loss is 5.321768741607666 and perplexity is 204.74570415961728
At time: 148.0492513179779 and batch: 650, loss is 5.319607620239258 and perplexity is 204.3037016258574
At time: 149.2137532234192 and batch: 700, loss is 5.271891431808472 and perplexity is 194.78403496313857
At time: 150.37743401527405 and batch: 750, loss is 5.242293109893799 and perplexity is 189.10324011053544
At time: 151.5405535697937 and batch: 800, loss is 5.246404819488525 and perplexity is 189.88237841482658
At time: 152.70626068115234 and batch: 850, loss is 5.265123014450073 and perplexity is 193.47010694244435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.204833348592122 and perplexity of 182.1505145995315
Finished 7 epochs...
Completing Train Step...
At time: 155.91362524032593 and batch: 50, loss is 5.339237298965454 and perplexity is 208.35373806109052
At time: 157.0800426006317 and batch: 100, loss is 5.283139286041259 and perplexity is 196.9873051977924
At time: 158.24498772621155 and batch: 150, loss is 5.269269113540649 and perplexity is 194.27391836633012
At time: 159.41112995147705 and batch: 200, loss is 5.306216621398926 and perplexity is 201.5861072553786
At time: 160.57519245147705 and batch: 250, loss is 5.317857093811035 and perplexity is 203.94637544248891
At time: 161.7383530139923 and batch: 300, loss is 5.272920360565186 and perplexity is 194.98455700181594
At time: 162.90545225143433 and batch: 350, loss is 5.252647495269775 and perplexity is 191.07146020424756
At time: 164.07157254219055 and batch: 400, loss is 5.2669359397888185 and perplexity is 193.8211719327646
At time: 165.23719239234924 and batch: 450, loss is 5.285696878433227 and perplexity is 197.49176325494167
At time: 166.4004032611847 and batch: 500, loss is 5.298052778244019 and perplexity is 199.9470893392761
At time: 167.5564272403717 and batch: 550, loss is 5.269009971618653 and perplexity is 194.2235803723548
At time: 168.7178761959076 and batch: 600, loss is 5.2900526905059815 and perplexity is 198.35387650015832
At time: 169.88116145133972 and batch: 650, loss is 5.287868461608887 and perplexity is 197.92109904582668
At time: 171.07062482833862 and batch: 700, loss is 5.242029047012329 and perplexity is 189.05331155648554
At time: 172.233300447464 and batch: 750, loss is 5.216519088745117 and perplexity is 184.2915636890707
At time: 173.39535403251648 and batch: 800, loss is 5.225326709747314 and perplexity is 185.92190310243424
At time: 174.55891728401184 and batch: 850, loss is 5.244168558120728 and perplexity is 189.4582262218942
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.200107574462891 and perplexity of 181.2917431881107
Finished 8 epochs...
Completing Train Step...
At time: 177.74558806419373 and batch: 50, loss is 5.303664751052857 and perplexity is 201.07234145675255
At time: 178.9561984539032 and batch: 100, loss is 5.249774866104126 and perplexity is 190.52337036127875
At time: 180.12261414527893 and batch: 150, loss is 5.234903869628906 and perplexity is 187.71106074291998
At time: 181.28427577018738 and batch: 200, loss is 5.271342191696167 and perplexity is 194.677081132256
At time: 182.4466791152954 and batch: 250, loss is 5.282628755569458 and perplexity is 196.88676284308266
At time: 183.6097285747528 and batch: 300, loss is 5.240422706604004 and perplexity is 188.74987136217558
At time: 184.7724974155426 and batch: 350, loss is 5.218598670959473 and perplexity is 184.6752119229338
At time: 185.93816328048706 and batch: 400, loss is 5.236436891555786 and perplexity is 187.99904660287478
At time: 187.10147786140442 and batch: 450, loss is 5.256457462310791 and perplexity is 191.80082471519972
At time: 188.2673532962799 and batch: 500, loss is 5.272187690734864 and perplexity is 194.84175002109347
At time: 189.4439332485199 and batch: 550, loss is 5.241816644668579 and perplexity is 189.01316045426233
At time: 190.60687947273254 and batch: 600, loss is 5.26451548576355 and perplexity is 193.3526039993105
At time: 191.76887202262878 and batch: 650, loss is 5.261090440750122 and perplexity is 192.69149543672714
At time: 192.9411165714264 and batch: 700, loss is 5.215956201553345 and perplexity is 184.18785751849427
At time: 194.12817978858948 and batch: 750, loss is 5.195551595687866 and perplexity is 180.46766053105824
At time: 195.2949662208557 and batch: 800, loss is 5.20367202758789 and perplexity is 181.93910216360703
At time: 196.46092104911804 and batch: 850, loss is 5.220606145858764 and perplexity is 185.04631514096056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.197613716125488 and perplexity of 180.84019055104272
Finished 9 epochs...
Completing Train Step...
At time: 199.63456535339355 and batch: 50, loss is 5.27165150642395 and perplexity is 194.73730693449565
At time: 200.86151385307312 and batch: 100, loss is 5.218987216949463 and perplexity is 184.7469806778051
At time: 202.04934430122375 and batch: 150, loss is 5.204219808578491 and perplexity is 182.0387922468928
At time: 203.2150707244873 and batch: 200, loss is 5.24056679725647 and perplexity is 188.77707041381046
At time: 204.3814616203308 and batch: 250, loss is 5.251038618087769 and perplexity is 190.76429685210385
At time: 205.60052037239075 and batch: 300, loss is 5.212126140594482 and perplexity is 183.48375603284848
At time: 206.77849435806274 and batch: 350, loss is 5.187724657058716 and perplexity is 179.0606646455791
At time: 207.94496297836304 and batch: 400, loss is 5.208383178710937 and perplexity is 182.79826700770892
At time: 209.1107394695282 and batch: 450, loss is 5.2301489353179935 and perplexity is 186.8206256375558
At time: 210.27731680870056 and batch: 500, loss is 5.245266551971436 and perplexity is 189.66636443558048
At time: 211.4428071975708 and batch: 550, loss is 5.216233711242676 and perplexity is 184.23897852657228
At time: 212.6075370311737 and batch: 600, loss is 5.239985876083374 and perplexity is 188.66743766368805
At time: 213.77178859710693 and batch: 650, loss is 5.236491136550903 and perplexity is 188.0092448868402
At time: 214.9343068599701 and batch: 700, loss is 5.193248929977417 and perplexity is 180.05258191436224
At time: 216.10525631904602 and batch: 750, loss is 5.174495687484741 and perplexity is 176.7074760098583
At time: 217.27239537239075 and batch: 800, loss is 5.183553018569946 and perplexity is 178.31524417849317
At time: 218.4415307044983 and batch: 850, loss is 5.199326000213623 and perplexity is 181.1501055873908
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.19596004486084 and perplexity of 180.5413874535759
Finished 10 epochs...
Completing Train Step...
At time: 221.5774211883545 and batch: 50, loss is 5.244801721572876 and perplexity is 189.5782222309816
At time: 222.78747010231018 and batch: 100, loss is 5.195818567276001 and perplexity is 180.5158467008802
At time: 223.9597237110138 and batch: 150, loss is 5.181439771652221 and perplexity is 177.93881791925307
At time: 225.1319658756256 and batch: 200, loss is 5.216280136108399 and perplexity is 184.24753199495646
At time: 226.29801297187805 and batch: 250, loss is 5.225775299072265 and perplexity is 186.00532439299187
At time: 227.46312189102173 and batch: 300, loss is 5.188533163070678 and perplexity is 179.20549480959076
At time: 228.6295142173767 and batch: 350, loss is 5.163395471572876 and perplexity is 174.756831191219
At time: 229.79592776298523 and batch: 400, loss is 5.186795177459717 and perplexity is 178.89430873499705
At time: 230.9638693332672 and batch: 450, loss is 5.210627946853638 and perplexity is 183.20906763761073
At time: 232.13117671012878 and batch: 500, loss is 5.226257820129394 and perplexity is 186.09509753572365
At time: 233.29691529273987 and batch: 550, loss is 5.196346349716187 and perplexity is 180.61114494110086
At time: 234.4690399169922 and batch: 600, loss is 5.221696996688843 and perplexity is 185.24828320592303
At time: 235.70626306533813 and batch: 650, loss is 5.216306467056274 and perplexity is 184.25238347098932
At time: 236.8737063407898 and batch: 700, loss is 5.1734891128540035 and perplexity is 176.52969623676023
At time: 238.0558819770813 and batch: 750, loss is 5.158529243469238 and perplexity is 173.9084903720125
At time: 239.21844148635864 and batch: 800, loss is 5.167235631942749 and perplexity is 175.42921565451377
At time: 240.38532328605652 and batch: 850, loss is 5.182499036788941 and perplexity is 178.12740216827083
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.195037206013997 and perplexity of 180.3748537014975
Finished 11 epochs...
Completing Train Step...
At time: 243.5130786895752 and batch: 50, loss is 5.219825019836426 and perplexity is 184.90182708790195
At time: 244.67460536956787 and batch: 100, loss is 5.172166519165039 and perplexity is 176.29637350420114
At time: 245.83746695518494 and batch: 150, loss is 5.159639167785644 and perplexity is 174.10162279566535
At time: 247.00226378440857 and batch: 200, loss is 5.1940162658691404 and perplexity is 180.190795744297
At time: 248.1764588356018 and batch: 250, loss is 5.202699766159058 and perplexity is 181.7622957571295
At time: 249.33809161186218 and batch: 300, loss is 5.166572093963623 and perplexity is 175.3128503179584
At time: 250.50010561943054 and batch: 350, loss is 5.1413880443573 and perplexity is 170.95289391259107
At time: 251.65964937210083 and batch: 400, loss is 5.166148796081543 and perplexity is 175.23865646387583
At time: 252.8202521800995 and batch: 450, loss is 5.192817010879517 and perplexity is 179.97483055796366
At time: 254.0019896030426 and batch: 500, loss is 5.208817701339722 and perplexity is 182.87771425078884
At time: 255.16422820091248 and batch: 550, loss is 5.17835114479065 and perplexity is 177.39007916736557
At time: 256.3268232345581 and batch: 600, loss is 5.204511194229126 and perplexity is 182.09184346761933
At time: 257.4959716796875 and batch: 650, loss is 5.198520202636718 and perplexity is 181.0041940667225
At time: 258.65585803985596 and batch: 700, loss is 5.154971017837524 and perplexity is 173.2907843444378
At time: 259.8212375640869 and batch: 750, loss is 5.141943664550781 and perplexity is 171.04790518522947
At time: 260.9950132369995 and batch: 800, loss is 5.1509286975860595 and perplexity is 172.5917014079841
At time: 262.1897611618042 and batch: 850, loss is 5.165254306793213 and perplexity is 175.08197744706547
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.193900108337402 and perplexity of 180.169866441793
Finished 12 epochs...
Completing Train Step...
At time: 265.4049277305603 and batch: 50, loss is 5.197978963851929 and perplexity is 180.90625408353367
At time: 266.5762286186218 and batch: 100, loss is 5.150957374572754 and perplexity is 172.59665088887672
At time: 267.7464847564697 and batch: 150, loss is 5.140482549667358 and perplexity is 170.79816703767682
At time: 268.9158389568329 and batch: 200, loss is 5.174651803970337 and perplexity is 176.73506511349242
At time: 270.0884940624237 and batch: 250, loss is 5.183229055404663 and perplexity is 178.257485963843
At time: 271.25943183898926 and batch: 300, loss is 5.148204784393311 and perplexity is 172.12221630418063
At time: 272.43831992149353 and batch: 350, loss is 5.12042989730835 and perplexity is 167.40732211107868
At time: 273.6065855026245 and batch: 400, loss is 5.148048286437988 and perplexity is 172.09528163692855
At time: 274.77407121658325 and batch: 450, loss is 5.17490403175354 and perplexity is 176.77964822949406
At time: 275.95161962509155 and batch: 500, loss is 5.19152603149414 and perplexity is 179.7426366728347
At time: 277.11748147010803 and batch: 550, loss is 5.161114110946655 and perplexity is 174.35860226199455
At time: 278.288286447525 and batch: 600, loss is 5.1868674850463865 and perplexity is 178.9072446184062
At time: 279.4578323364258 and batch: 650, loss is 5.181102485656738 and perplexity is 177.8788117681021
At time: 280.62644720077515 and batch: 700, loss is 5.138593702316284 and perplexity is 170.4758598626521
At time: 281.7957215309143 and batch: 750, loss is 5.128359851837158 and perplexity is 168.74013214029125
At time: 282.96525979042053 and batch: 800, loss is 5.136450710296631 and perplexity is 170.11092262388004
At time: 284.1350827217102 and batch: 850, loss is 5.150305738449097 and perplexity is 172.48421731320354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.193846066792806 and perplexity of 180.160130047008
Finished 13 epochs...
Completing Train Step...
At time: 287.3325068950653 and batch: 50, loss is 5.177678871154785 and perplexity is 177.27086457078246
At time: 288.498904466629 and batch: 100, loss is 5.134413843154907 and perplexity is 169.76478191612927
At time: 289.6654734611511 and batch: 150, loss is 5.122798337936401 and perplexity is 167.8042863217174
At time: 290.8325951099396 and batch: 200, loss is 5.156354360580444 and perplexity is 173.53067077769927
At time: 291.99725008010864 and batch: 250, loss is 5.165025062561035 and perplexity is 175.04184551376034
At time: 293.18624114990234 and batch: 300, loss is 5.130520763397217 and perplexity is 169.1051588957047
At time: 294.3517246246338 and batch: 350, loss is 5.104103889465332 and perplexity is 164.69641818975037
At time: 295.56022572517395 and batch: 400, loss is 5.131099166870118 and perplexity is 169.20299819946422
At time: 296.7496237754822 and batch: 450, loss is 5.159059638977051 and perplexity is 174.00075512031734
At time: 297.91584849357605 and batch: 500, loss is 5.176265926361084 and perplexity is 177.0205674952285
At time: 299.0819489955902 and batch: 550, loss is 5.147021474838257 and perplexity is 171.91866289808772
At time: 300.24945640563965 and batch: 600, loss is 5.172043714523316 and perplexity is 176.2747248205225
At time: 301.4184868335724 and batch: 650, loss is 5.166074552536011 and perplexity is 175.22564660766062
At time: 302.58568692207336 and batch: 700, loss is 5.123260908126831 and perplexity is 167.88192553780527
At time: 303.7715799808502 and batch: 750, loss is 5.114526014328003 and perplexity is 166.42188069903557
At time: 304.9436845779419 and batch: 800, loss is 5.1230627632141115 and perplexity is 167.84866388374502
At time: 306.1293933391571 and batch: 850, loss is 5.13586992263794 and perplexity is 170.0121529842817
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.195068359375 and perplexity of 180.3804730719614
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 309.3198456764221 and batch: 50, loss is 5.170694065093994 and perplexity is 176.03697621348994
At time: 310.51114153862 and batch: 100, loss is 5.126200780868531 and perplexity is 168.3762032352854
At time: 311.67819809913635 and batch: 150, loss is 5.1110717296600345 and perplexity is 165.84800388568988
At time: 312.8583586215973 and batch: 200, loss is 5.1339692115783695 and perplexity is 169.6893159120328
At time: 314.041570186615 and batch: 250, loss is 5.131430702209473 and perplexity is 169.25910427295133
At time: 315.2087688446045 and batch: 300, loss is 5.0813569164276124 and perplexity is 160.99236096357913
At time: 316.3783504962921 and batch: 350, loss is 5.048547658920288 and perplexity is 155.79603112090402
At time: 317.5477328300476 and batch: 400, loss is 5.071680555343628 and perplexity is 159.44205351099228
At time: 318.7187361717224 and batch: 450, loss is 5.101116151809692 and perplexity is 164.20508285541098
At time: 319.88740515708923 and batch: 500, loss is 5.104830780029297 and perplexity is 164.81617798291
At time: 321.0554678440094 and batch: 550, loss is 5.064132575988769 and perplexity is 158.2431186427487
At time: 322.22548627853394 and batch: 600, loss is 5.073496799468995 and perplexity is 159.73190234245868
At time: 323.394727230072 and batch: 650, loss is 5.053011083602906 and perplexity is 156.492969179888
At time: 324.6147389411926 and batch: 700, loss is 5.006149663925171 and perplexity is 149.3286622863472
At time: 325.7831771373749 and batch: 750, loss is 4.976853561401367 and perplexity is 145.01737479310918
At time: 326.95337557792664 and batch: 800, loss is 4.966558713912963 and perplexity is 143.5321014854324
At time: 328.12249755859375 and batch: 850, loss is 5.001633367538452 and perplexity is 148.6557704217221
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.129157066345215 and perplexity of 168.87470785745455
Finished 15 epochs...
Completing Train Step...
At time: 331.32168412208557 and batch: 50, loss is 5.118350982666016 and perplexity is 167.05965808503885
At time: 332.52186346054077 and batch: 100, loss is 5.078993091583252 and perplexity is 160.61225265270875
At time: 333.69078493118286 and batch: 150, loss is 5.065318288803101 and perplexity is 158.43086081845917
At time: 334.8573591709137 and batch: 200, loss is 5.094915533065796 and perplexity is 163.1900598781677
At time: 336.0293164253235 and batch: 250, loss is 5.096534767150879 and perplexity is 163.45451683654755
At time: 337.19775223731995 and batch: 300, loss is 5.0529468059539795 and perplexity is 156.48291050303317
At time: 338.3678765296936 and batch: 350, loss is 5.019055004119873 and perplexity is 151.26828832092215
At time: 339.55123472213745 and batch: 400, loss is 5.0453111362457275 and perplexity is 155.29260884183412
At time: 340.7223906517029 and batch: 450, loss is 5.078044118881226 and perplexity is 160.4599083060764
At time: 341.8881366252899 and batch: 500, loss is 5.082787160873413 and perplexity is 161.22278413511478
At time: 343.0544979572296 and batch: 550, loss is 5.046998453140259 and perplexity is 155.5548578707179
At time: 344.22244477272034 and batch: 600, loss is 5.0618001651763915 and perplexity is 157.87446077976398
At time: 345.3905689716339 and batch: 650, loss is 5.044046964645386 and perplexity is 155.0964163725956
At time: 346.5605573654175 and batch: 700, loss is 5.0023690128326415 and perplexity is 148.76516857387122
At time: 347.75463604927063 and batch: 750, loss is 4.9809646320343015 and perplexity is 145.61477860713592
At time: 348.9380419254303 and batch: 800, loss is 4.979376316070557 and perplexity is 145.38367990717782
At time: 350.11196184158325 and batch: 850, loss is 5.015670127868653 and perplexity is 150.7571294769829
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.126394907633464 and perplexity of 168.40889273576204
Finished 16 epochs...
Completing Train Step...
At time: 353.28510332107544 and batch: 50, loss is 5.104861040115356 and perplexity is 164.8211654100993
At time: 354.49849009513855 and batch: 100, loss is 5.064814767837524 and perplexity is 158.35110763882673
At time: 355.66720724105835 and batch: 150, loss is 5.049091501235962 and perplexity is 155.8807826388836
At time: 356.83646059036255 and batch: 200, loss is 5.080403509140015 and perplexity is 160.83894281999824
At time: 358.003134727478 and batch: 250, loss is 5.083420581817627 and perplexity is 161.3249383731719
At time: 359.1726453304291 and batch: 300, loss is 5.040953006744385 and perplexity is 154.61729616082619
At time: 360.34160685539246 and batch: 350, loss is 5.007198724746704 and perplexity is 149.4853993344576
At time: 361.5104806423187 and batch: 400, loss is 5.035048999786377 and perplexity is 153.7071240435645
At time: 362.67981362342834 and batch: 450, loss is 5.068930807113648 and perplexity is 159.0042302343254
At time: 363.84594798088074 and batch: 500, loss is 5.0742024898529055 and perplexity is 159.8446633924603
At time: 365.015611410141 and batch: 550, loss is 5.040765953063965 and perplexity is 154.58837713131254
At time: 366.2077271938324 and batch: 600, loss is 5.058248052597046 and perplexity is 157.3146677342293
At time: 367.37706875801086 and batch: 650, loss is 5.042095909118652 and perplexity is 154.79410965674683
At time: 368.5464916229248 and batch: 700, loss is 5.002746181488037 and perplexity is 148.8212887151858
At time: 369.71579337120056 and batch: 750, loss is 4.985343160629273 and perplexity is 146.25375494635915
At time: 370.8861997127533 and batch: 800, loss is 4.985862970352173 and perplexity is 146.32979883265438
At time: 372.05580830574036 and batch: 850, loss is 5.021780614852905 and perplexity is 151.68114918454393
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.125912666320801 and perplexity of 168.32769858942441
Finished 17 epochs...
Completing Train Step...
At time: 375.2619047164917 and batch: 50, loss is 5.096316986083984 and perplexity is 163.41892341340963
At time: 376.42997550964355 and batch: 100, loss is 5.056056966781616 and perplexity is 156.97035514416356
At time: 377.5958032608032 and batch: 150, loss is 5.039446954727173 and perplexity is 154.38460973296083
At time: 378.762957572937 and batch: 200, loss is 5.071523323059082 and perplexity is 159.41698604342525
At time: 379.93187379837036 and batch: 250, loss is 5.075078678131104 and perplexity is 159.9847787876268
At time: 381.0957407951355 and batch: 300, loss is 5.0332373523712155 and perplexity is 153.42891301583597
At time: 382.26420736312866 and batch: 350, loss is 5.000172004699707 and perplexity is 148.43868905901397
At time: 383.4336223602295 and batch: 400, loss is 5.0294100952148435 and perplexity is 152.84282338378588
At time: 384.6350004673004 and batch: 450, loss is 5.063768720626831 and perplexity is 158.18555150923083
At time: 385.79952669143677 and batch: 500, loss is 5.069775447845459 and perplexity is 159.1385884179211
At time: 386.96395230293274 and batch: 550, loss is 5.037683172225952 and perplexity is 154.11254885849104
At time: 388.12425231933594 and batch: 600, loss is 5.0565007877349855 and perplexity is 157.04003733889954
At time: 389.2862515449524 and batch: 650, loss is 5.041285638809204 and perplexity is 154.6687353859985
At time: 390.4587821960449 and batch: 700, loss is 5.003167705535889 and perplexity is 148.88403369054043
At time: 391.6264123916626 and batch: 750, loss is 4.987724876403808 and perplexity is 146.60250496845538
At time: 392.7962443828583 and batch: 800, loss is 4.989156265258789 and perplexity is 146.81250041689717
At time: 393.9665336608887 and batch: 850, loss is 5.0243495750427245 and perplexity is 152.07131296137453
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.125836690266927 and perplexity of 168.3149102009397
Finished 18 epochs...
Completing Train Step...
At time: 397.17735528945923 and batch: 50, loss is 5.089934434890747 and perplexity is 162.37921529347386
At time: 398.3482279777527 and batch: 100, loss is 5.049643936157227 and perplexity is 155.96692041733522
At time: 399.51690912246704 and batch: 150, loss is 5.032805442810059 and perplexity is 153.36265991005138
At time: 400.6865077018738 and batch: 200, loss is 5.065278367996216 and perplexity is 158.42453625690143
At time: 401.85600423812866 and batch: 250, loss is 5.069167222976684 and perplexity is 159.04182580056172
At time: 403.0257556438446 and batch: 300, loss is 5.02765417098999 and perplexity is 152.5746784575941
At time: 404.1939654350281 and batch: 350, loss is 4.9952418231964115 and perplexity is 147.70866044534006
At time: 405.36454796791077 and batch: 400, loss is 5.025453910827637 and perplexity is 152.2393435181379
At time: 406.53207993507385 and batch: 450, loss is 5.0602956390380855 and perplexity is 157.6371131196008
At time: 407.70665192604065 and batch: 500, loss is 5.066745548248291 and perplexity is 158.65714420510946
At time: 408.8761315345764 and batch: 550, loss is 5.035595226287842 and perplexity is 153.791105882646
At time: 410.04210472106934 and batch: 600, loss is 5.055145902633667 and perplexity is 156.8274102071919
At time: 411.2109405994415 and batch: 650, loss is 5.0404535675048825 and perplexity is 154.54009349664247
At time: 412.3844816684723 and batch: 700, loss is 5.003286876678467 and perplexity is 148.90177742819674
At time: 413.5588436126709 and batch: 750, loss is 4.989013042449951 and perplexity is 146.79147502390884
At time: 414.77845335006714 and batch: 800, loss is 4.990929908752442 and perplexity is 147.07312451184083
At time: 415.9476854801178 and batch: 850, loss is 5.025520000457764 and perplexity is 152.24940529252757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.125894546508789 and perplexity of 168.32464855080275
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 419.1284215450287 and batch: 50, loss is 5.0883277320861815 and perplexity is 162.11852963113097
At time: 420.2967085838318 and batch: 100, loss is 5.0494453239440915 and perplexity is 155.9359465580904
At time: 421.4664840698242 and batch: 150, loss is 5.030999021530151 and perplexity is 153.08587241032822
At time: 422.6345844268799 and batch: 200, loss is 5.064145565032959 and perplexity is 158.2451740829585
At time: 423.7982988357544 and batch: 250, loss is 5.064328994750976 and perplexity is 158.274203612976
At time: 424.9633605480194 and batch: 300, loss is 5.02010136604309 and perplexity is 151.42665253670503
At time: 426.1273183822632 and batch: 350, loss is 4.986969509124756 and perplexity is 146.49180804685477
At time: 427.2886381149292 and batch: 400, loss is 5.01543719291687 and perplexity is 150.72201696192212
At time: 428.44736671447754 and batch: 450, loss is 5.049024801254273 and perplexity is 155.87038574027625
At time: 429.61171650886536 and batch: 500, loss is 5.055003252029419 and perplexity is 156.80504027794305
At time: 430.77488899230957 and batch: 550, loss is 5.019423933029175 and perplexity is 151.3241058612633
At time: 431.93781208992004 and batch: 600, loss is 5.032675943374634 and perplexity is 153.3428008180761
At time: 433.10238218307495 and batch: 650, loss is 5.015287666320801 and perplexity is 150.69948169662584
At time: 434.2741684913635 and batch: 700, loss is 4.978797330856323 and perplexity is 145.29952926945896
At time: 435.4353973865509 and batch: 750, loss is 4.9584565925598145 and perplexity is 142.3738853209412
At time: 436.6005599498749 and batch: 800, loss is 4.958066663742065 and perplexity is 142.3183804623349
At time: 437.76525807380676 and batch: 850, loss is 4.999811868667603 and perplexity is 148.3852405634677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.122598965962728 and perplexity of 167.77083418478267
Finished 20 epochs...
Completing Train Step...
At time: 440.97026109695435 and batch: 50, loss is 5.082669067382812 and perplexity is 161.20374589793997
At time: 442.1626195907593 and batch: 100, loss is 5.043625726699829 and perplexity is 155.03109763513083
At time: 443.32645297050476 and batch: 150, loss is 5.02498291015625 and perplexity is 152.16765556899077
At time: 444.5203547477722 and batch: 200, loss is 5.05847713470459 and perplexity is 157.35070983800435
At time: 445.6856327056885 and batch: 250, loss is 5.059788980484009 and perplexity is 157.55726515733346
At time: 446.8484597206116 and batch: 300, loss is 5.015852479934693 and perplexity is 150.7846228576745
At time: 448.010746717453 and batch: 350, loss is 4.983405075073242 and perplexity is 145.97057715641338
At time: 449.17207312583923 and batch: 400, loss is 5.011845874786377 and perplexity is 150.18169706080494
At time: 450.3291313648224 and batch: 450, loss is 5.046401338577271 and perplexity is 155.46200152543668
At time: 451.48589420318604 and batch: 500, loss is 5.052530136108398 and perplexity is 156.41772237478457
At time: 452.64601469039917 and batch: 550, loss is 5.017864217758179 and perplexity is 151.08826731078668
At time: 453.8060345649719 and batch: 600, loss is 5.032183361053467 and perplexity is 153.26728546560565
At time: 454.96870136260986 and batch: 650, loss is 5.015087337493896 and perplexity is 150.66929526994906
At time: 456.138831615448 and batch: 700, loss is 4.979440212249756 and perplexity is 145.3929696656287
At time: 457.29943442344666 and batch: 750, loss is 4.960260047912597 and perplexity is 142.6308819378262
At time: 458.4616765975952 and batch: 800, loss is 4.960745763778687 and perplexity is 142.70017684763394
At time: 459.6222815513611 and batch: 850, loss is 5.002411241531372 and perplexity is 148.77145086600214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.122204144795735 and perplexity of 167.70460778289552
Finished 21 epochs...
Completing Train Step...
At time: 462.755606174469 and batch: 50, loss is 5.080181083679199 and perplexity is 160.8031721223288
At time: 463.9435222148895 and batch: 100, loss is 5.041032505035401 and perplexity is 154.6295884602344
At time: 465.1051757335663 and batch: 150, loss is 5.022006597518921 and perplexity is 151.71543036835172
At time: 466.28787064552307 and batch: 200, loss is 5.055875425338745 and perplexity is 156.9418611059053
At time: 467.45148253440857 and batch: 250, loss is 5.057291030883789 and perplexity is 157.16418619992714
At time: 468.61347126960754 and batch: 300, loss is 5.013499202728272 and perplexity is 150.43020203041456
At time: 469.775985956192 and batch: 350, loss is 4.981437244415283 and perplexity is 145.68361421935958
At time: 470.9448781013489 and batch: 400, loss is 5.009987831115723 and perplexity is 149.90291198675894
At time: 472.1116075515747 and batch: 450, loss is 5.044923229217529 and perplexity is 155.2323814295106
At time: 473.30260181427 and batch: 500, loss is 5.0513184547424315 and perplexity is 156.22830871295264
At time: 474.4890375137329 and batch: 550, loss is 5.017327280044555 and perplexity is 151.0071640976169
At time: 475.6510238647461 and batch: 600, loss is 5.032196550369263 and perplexity is 153.26930696956595
At time: 476.81283497810364 and batch: 650, loss is 5.0154452419281 and perplexity is 150.72323013001164
At time: 477.97636795043945 and batch: 700, loss is 4.980331268310547 and perplexity is 145.52258068921398
At time: 479.1428859233856 and batch: 750, loss is 4.9618464374542235 and perplexity is 142.8573296468935
At time: 480.31062269210815 and batch: 800, loss is 4.96264949798584 and perplexity is 142.97209880710852
At time: 481.47905349731445 and batch: 850, loss is 5.004016742706299 and perplexity is 149.01049544699347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.1220658620198565 and perplexity of 167.68141872756402
Finished 22 epochs...
Completing Train Step...
At time: 484.6722197532654 and batch: 50, loss is 5.078380365371704 and perplexity is 160.51387145905866
At time: 485.8865602016449 and batch: 100, loss is 5.039070892333984 and perplexity is 154.32656240254627
At time: 487.07383036613464 and batch: 150, loss is 5.019794225692749 and perplexity is 151.3801504432941
At time: 488.2376127243042 and batch: 200, loss is 5.05395770072937 and perplexity is 156.641178242236
At time: 489.40047955513 and batch: 250, loss is 5.05540678024292 and perplexity is 156.86832830410762
At time: 490.5623803138733 and batch: 300, loss is 5.011737623214722 and perplexity is 150.16544053597735
At time: 491.7236886024475 and batch: 350, loss is 4.979966373443603 and perplexity is 145.46948993335735
At time: 492.88464403152466 and batch: 400, loss is 5.008677034378052 and perplexity is 149.70654846320818
At time: 494.06689643859863 and batch: 450, loss is 5.043870868682862 and perplexity is 155.0691069244828
At time: 495.230313539505 and batch: 500, loss is 5.050488338470459 and perplexity is 156.09867486477486
At time: 496.4133813381195 and batch: 550, loss is 5.016994380950928 and perplexity is 150.95690231607253
At time: 497.59559988975525 and batch: 600, loss is 5.032175703048706 and perplexity is 153.26611174849802
At time: 498.75707721710205 and batch: 650, loss is 5.015766105651855 and perplexity is 150.77159950646168
At time: 499.92097544670105 and batch: 700, loss is 4.981113185882569 and perplexity is 145.6364118496735
At time: 501.0836184024811 and batch: 750, loss is 4.963039388656616 and perplexity is 143.02785316296004
At time: 502.2683324813843 and batch: 800, loss is 4.96406195640564 and perplexity is 143.17418363647724
At time: 503.46025228500366 and batch: 850, loss is 5.005152235031128 and perplexity is 149.17979182006846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121999104817708 and perplexity of 167.67022515882732
Finished 23 epochs...
Completing Train Step...
At time: 506.6293761730194 and batch: 50, loss is 5.076913232803345 and perplexity is 160.27854899740203
At time: 507.7990746498108 and batch: 100, loss is 5.037471570968628 and perplexity is 154.07994189934158
At time: 508.96264839172363 and batch: 150, loss is 5.018034152984619 and perplexity is 151.1139447113902
At time: 510.12581396102905 and batch: 200, loss is 5.052468471527099 and perplexity is 156.40807723881127
At time: 511.28963136672974 and batch: 250, loss is 5.05388258934021 and perplexity is 156.62941314759044
At time: 512.452787399292 and batch: 300, loss is 5.010329627990723 and perplexity is 149.95415709083878
At time: 513.6165220737457 and batch: 350, loss is 4.978789176940918 and perplexity is 145.2983445142191
At time: 514.7886197566986 and batch: 400, loss is 5.00765305519104 and perplexity is 149.55333053279614
At time: 515.9965217113495 and batch: 450, loss is 5.04308666229248 and perplexity is 154.94754840959362
At time: 517.1644721031189 and batch: 500, loss is 5.049945573806763 and perplexity is 156.01397300867507
At time: 518.3287403583527 and batch: 550, loss is 5.01682596206665 and perplexity is 150.9314804638305
At time: 519.4920132160187 and batch: 600, loss is 5.032278995513916 and perplexity is 153.28194380066552
At time: 520.6609835624695 and batch: 650, loss is 5.016148176193237 and perplexity is 150.8292158991724
At time: 521.8223748207092 and batch: 700, loss is 4.981780843734741 and perplexity is 145.7336796107862
At time: 522.990003824234 and batch: 750, loss is 4.9640380382537845 and perplexity is 143.1707592155643
At time: 524.16850233078 and batch: 800, loss is 4.965176439285278 and perplexity is 143.33383776231602
At time: 525.3639430999756 and batch: 850, loss is 5.0061157703399655 and perplexity is 149.32360108838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121989885965983 and perplexity of 167.66867943900783
Finished 24 epochs...
Completing Train Step...
At time: 528.6012530326843 and batch: 50, loss is 5.075681571960449 and perplexity is 160.08126170510235
At time: 529.7633216381073 and batch: 100, loss is 5.0361206912994385 and perplexity is 153.87193896350007
At time: 530.9268064498901 and batch: 150, loss is 5.016540422439575 and perplexity is 150.88838969753837
At time: 532.0914354324341 and batch: 200, loss is 5.051215648651123 and perplexity is 156.21224831674957
At time: 533.3019390106201 and batch: 250, loss is 5.052574987411499 and perplexity is 156.42473807079153
At time: 534.4640846252441 and batch: 300, loss is 5.009108362197876 and perplexity is 149.77113499035175
At time: 535.6270422935486 and batch: 350, loss is 4.977786512374878 and perplexity is 145.15273202511113
At time: 536.7861502170563 and batch: 400, loss is 5.006776933670044 and perplexity is 149.42236102236066
At time: 537.9469542503357 and batch: 450, loss is 5.042430238723755 and perplexity is 154.84587056242003
At time: 539.107846736908 and batch: 500, loss is 5.049502649307251 and perplexity is 155.9448858990795
At time: 540.2703175544739 and batch: 550, loss is 5.016659755706787 and perplexity is 150.9063967764657
At time: 541.4350297451019 and batch: 600, loss is 5.032351121902466 and perplexity is 153.2929998724142
At time: 542.6141986846924 and batch: 650, loss is 5.016475515365601 and perplexity is 150.87859629150051
At time: 543.7894911766052 and batch: 700, loss is 4.982323703765869 and perplexity is 145.81281407816235
At time: 544.9695701599121 and batch: 750, loss is 4.964842281341553 and perplexity is 143.28594962336973
At time: 546.1660306453705 and batch: 800, loss is 4.966076946258545 and perplexity is 143.46296901579697
At time: 547.3433229923248 and batch: 850, loss is 5.006893033981323 and perplexity is 149.43971001207956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121982256571452 and perplexity of 167.66740023338164
Finished 25 epochs...
Completing Train Step...
At time: 550.6372413635254 and batch: 50, loss is 5.074580459594727 and perplexity is 159.9050912578472
At time: 551.8036398887634 and batch: 100, loss is 5.034918508529663 and perplexity is 153.68706791638527
At time: 552.9673140048981 and batch: 150, loss is 5.015209951400757 and perplexity is 150.68777055352626
At time: 554.1325786113739 and batch: 200, loss is 5.05011290550232 and perplexity is 156.0400812756185
At time: 555.3176414966583 and batch: 250, loss is 5.051405391693115 and perplexity is 156.2418913161291
At time: 556.4817385673523 and batch: 300, loss is 5.008040399551391 and perplexity is 149.61127039256436
At time: 557.6474807262421 and batch: 350, loss is 4.976911420822144 and perplexity is 145.02576565716015
At time: 558.8100290298462 and batch: 400, loss is 5.005996189117432 and perplexity is 149.30574585720515
At time: 559.9770867824554 and batch: 450, loss is 5.041834363937378 and perplexity is 154.75362929722868
At time: 561.1414008140564 and batch: 500, loss is 5.049107007980346 and perplexity is 155.883199861076
At time: 562.3048987388611 and batch: 550, loss is 5.016512002944946 and perplexity is 150.8841015866911
At time: 563.5449793338776 and batch: 600, loss is 5.032378015518188 and perplexity is 153.29712253088198
At time: 564.7087135314941 and batch: 650, loss is 5.0167593574523925 and perplexity is 150.92142806556663
At time: 565.8782541751862 and batch: 700, loss is 4.9827884006500245 and perplexity is 145.88058858461582
At time: 567.0761513710022 and batch: 750, loss is 4.965520515441894 and perplexity is 143.3831640038315
At time: 568.2431333065033 and batch: 800, loss is 4.966820526123047 and perplexity is 143.56968486181896
At time: 569.4142732620239 and batch: 850, loss is 5.007528057098389 and perplexity is 149.53463782003104
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.1219743092854815 and perplexity of 167.6660677378989
Finished 26 epochs...
Completing Train Step...
At time: 572.5966508388519 and batch: 50, loss is 5.07355598449707 and perplexity is 159.74135635934877
At time: 573.7920925617218 and batch: 100, loss is 5.033798389434814 and perplexity is 153.51501647402267
At time: 574.9621267318726 and batch: 150, loss is 5.013992137908936 and perplexity is 150.50437264838135
At time: 576.1346154212952 and batch: 200, loss is 5.049089193344116 and perplexity is 155.88042288331158
At time: 577.3039104938507 and batch: 250, loss is 5.050318298339843 and perplexity is 156.07213408227489
At time: 578.4994134902954 and batch: 300, loss is 5.0070582866668705 and perplexity is 149.46440736608227
At time: 579.703131198883 and batch: 350, loss is 4.976116037368774 and perplexity is 144.91046042485996
At time: 580.8729991912842 and batch: 400, loss is 5.005303955078125 and perplexity is 149.20242710216402
At time: 582.0417873859406 and batch: 450, loss is 5.041296901702881 and perplexity is 154.67047741333045
At time: 583.2117133140564 and batch: 500, loss is 5.048761644363403 and perplexity is 155.82937277084565
At time: 584.3804361820221 and batch: 550, loss is 5.016364965438843 and perplexity is 150.86191759566248
At time: 585.5503180027008 and batch: 600, loss is 5.032357635498047 and perplexity is 153.29399836427268
At time: 586.7194542884827 and batch: 650, loss is 5.0169687080383305 and perplexity is 150.95302686246066
At time: 587.8901982307434 and batch: 700, loss is 4.983159456253052 and perplexity is 145.9347284382099
At time: 589.0637671947479 and batch: 750, loss is 4.966018524169922 and perplexity is 143.4545878543319
At time: 590.2373003959656 and batch: 800, loss is 4.967419900894165 and perplexity is 143.65576270268866
At time: 591.4049489498138 and batch: 850, loss is 5.00801609992981 and perplexity is 149.60763493947974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121965408325195 and perplexity of 167.66457535553047
Finished 27 epochs...
Completing Train Step...
At time: 594.5869541168213 and batch: 50, loss is 5.072603979110718 and perplexity is 159.5893540926399
At time: 595.7799241542816 and batch: 100, loss is 5.032744207382202 and perplexity is 153.35326896948663
At time: 596.9467046260834 and batch: 150, loss is 5.012915315628052 and perplexity is 150.34239341361953
At time: 598.1133885383606 and batch: 200, loss is 5.0481478786468506 and perplexity is 155.7337593893163
At time: 599.2803089618683 and batch: 250, loss is 5.049341745376587 and perplexity is 155.91979577257553
At time: 600.4468219280243 and batch: 300, loss is 5.006167459487915 and perplexity is 149.33131969757144
At time: 601.6096465587616 and batch: 350, loss is 4.975428838729858 and perplexity is 144.8109123622554
At time: 602.7742278575897 and batch: 400, loss is 5.0046716403961184 and perplexity is 149.10811403782637
At time: 603.9412670135498 and batch: 450, loss is 5.040780782699585 and perplexity is 154.59066963761495
At time: 605.1077084541321 and batch: 500, loss is 5.0483917808532714 and perplexity is 155.77174782938945
At time: 606.3273158073425 and batch: 550, loss is 5.016195030212402 and perplexity is 150.83628301970498
At time: 607.5283162593842 and batch: 600, loss is 5.03232744216919 and perplexity is 153.28936997804172
At time: 608.7202672958374 and batch: 650, loss is 5.0171318531036375 and perplexity is 150.97765611290214
At time: 609.8878836631775 and batch: 700, loss is 4.983485898971558 and perplexity is 145.98237554428795
At time: 611.0542764663696 and batch: 750, loss is 4.966472082138061 and perplexity is 143.51966758331858
At time: 612.2225334644318 and batch: 800, loss is 4.9679376411437985 and perplexity is 143.7301583302752
At time: 613.4009349346161 and batch: 850, loss is 5.008438062667847 and perplexity is 149.67077710763425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.1219526926676435 and perplexity of 167.6624434037613
Finished 28 epochs...
Completing Train Step...
At time: 616.6154458522797 and batch: 50, loss is 5.071737957000733 and perplexity is 159.45120601175785
At time: 617.7833411693573 and batch: 100, loss is 5.03177812576294 and perplexity is 153.2051887353947
At time: 618.9531643390656 and batch: 150, loss is 5.0118940067291256 and perplexity is 150.18892577161435
At time: 620.1216282844543 and batch: 200, loss is 5.047294807434082 and perplexity is 155.60096405231795
At time: 621.2910895347595 and batch: 250, loss is 5.048448715209961 and perplexity is 155.780616846116
At time: 622.4607267379761 and batch: 300, loss is 5.005343008041382 and perplexity is 149.2082540128458
At time: 623.6572318077087 and batch: 350, loss is 4.974789218902588 and perplexity is 144.71831804724016
At time: 624.825975894928 and batch: 400, loss is 5.004114561080932 and perplexity is 149.0250721243832
At time: 625.9953503608704 and batch: 450, loss is 5.040325889587402 and perplexity is 154.52036339891328
At time: 627.1649401187897 and batch: 500, loss is 5.048082876205444 and perplexity is 155.72363664375177
At time: 628.3350346088409 and batch: 550, loss is 5.016054229736328 and perplexity is 150.81504669432402
At time: 629.5035705566406 and batch: 600, loss is 5.0322917938232425 and perplexity is 153.28390556294997
At time: 630.6737432479858 and batch: 650, loss is 5.017278299331665 and perplexity is 150.99976784020637
At time: 631.8429670333862 and batch: 700, loss is 4.983783378601074 and perplexity is 146.02580878721298
At time: 633.014128446579 and batch: 750, loss is 4.966874790191651 and perplexity is 143.5774757484284
At time: 634.1821944713593 and batch: 800, loss is 4.968394842147827 and perplexity is 143.7958869274183
At time: 635.351343870163 and batch: 850, loss is 5.008837938308716 and perplexity is 149.73063877336284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121962229410808 and perplexity of 167.66404236504673
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 638.739129781723 and batch: 50, loss is 5.071557674407959 and perplexity is 159.42246232598814
At time: 639.9075882434845 and batch: 100, loss is 5.031693229675293 and perplexity is 153.1921827663496
At time: 641.0958006381989 and batch: 150, loss is 5.012058410644531 and perplexity is 150.2136194488748
At time: 642.265139579773 and batch: 200, loss is 5.04742202758789 and perplexity is 155.62076089014906
At time: 643.4348697662354 and batch: 250, loss is 5.047469568252564 and perplexity is 155.62815938042192
At time: 644.6045100688934 and batch: 300, loss is 5.003832044601441 and perplexity is 148.9829760323505
At time: 645.7958979606628 and batch: 350, loss is 4.973690319061279 and perplexity is 144.5593744580408
At time: 646.9706525802612 and batch: 400, loss is 5.001562395095825 and perplexity is 148.64522033297192
At time: 648.1420328617096 and batch: 450, loss is 5.038052234649658 and perplexity is 154.16943650619714
At time: 649.3121235370636 and batch: 500, loss is 5.046313524246216 and perplexity is 155.44835033316224
At time: 650.4841945171356 and batch: 550, loss is 5.013565158843994 and perplexity is 150.44012414943637
At time: 651.6543505191803 and batch: 600, loss is 5.02863431930542 and perplexity is 152.72429758415078
At time: 652.8793947696686 and batch: 650, loss is 5.013299579620361 and perplexity is 150.40017568303776
At time: 654.050853729248 and batch: 700, loss is 4.979528522491455 and perplexity is 145.40580992087587
At time: 655.2265157699585 and batch: 750, loss is 4.9623262977600096 and perplexity is 142.92589765899953
At time: 656.4001700878143 and batch: 800, loss is 4.963580150604248 and perplexity is 143.10521809952357
At time: 657.5706269741058 and batch: 850, loss is 5.005094947814942 and perplexity is 149.17124596987023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121575991312663 and perplexity of 167.59929662863635
Finished 30 epochs...
Completing Train Step...
At time: 660.8037142753601 and batch: 50, loss is 5.0714125823974605 and perplexity is 159.39933307838572
At time: 661.9700751304626 and batch: 100, loss is 5.030821619033813 and perplexity is 153.0587170031982
At time: 663.1387495994568 and batch: 150, loss is 5.011332387924194 and perplexity is 150.10460052814628
At time: 664.307932138443 and batch: 200, loss is 5.046641836166382 and perplexity is 155.49939425825275
At time: 665.4779036045074 and batch: 250, loss is 5.046934108734131 and perplexity is 155.54484910777535
At time: 666.6484439373016 and batch: 300, loss is 5.003471755981446 and perplexity is 148.9293088299347
At time: 667.8167691230774 and batch: 350, loss is 4.973325576782226 and perplexity is 144.50665715704125
At time: 668.9883213043213 and batch: 400, loss is 5.001474418640137 and perplexity is 148.63214362856147
At time: 670.1596395969391 and batch: 450, loss is 5.038046283721924 and perplexity is 154.16851905775147
At time: 671.3289573192596 and batch: 500, loss is 5.0462171173095705 and perplexity is 155.43336475626842
At time: 672.498108625412 and batch: 550, loss is 5.013462677001953 and perplexity is 150.42470755837047
At time: 673.6669661998749 and batch: 600, loss is 5.028672389984131 and perplexity is 152.73011201249437
At time: 674.8368127346039 and batch: 650, loss is 5.013229322433472 and perplexity is 150.38960936097095
At time: 676.0064578056335 and batch: 700, loss is 4.979477071762085 and perplexity is 145.39832887835507
At time: 677.1805424690247 and batch: 750, loss is 4.962431631088257 and perplexity is 142.94095331240982
At time: 678.3516087532043 and batch: 800, loss is 4.963813695907593 and perplexity is 143.13864355412196
At time: 679.5211338996887 and batch: 850, loss is 5.005340795516968 and perplexity is 149.20792388630633
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121506373087565 and perplexity of 167.58762906921956
Finished 31 epochs...
Completing Train Step...
At time: 682.727153301239 and batch: 50, loss is 5.071174182891846 and perplexity is 159.36133688550126
At time: 683.9284813404083 and batch: 100, loss is 5.030514993667603 and perplexity is 153.01179251253262
At time: 685.0938785076141 and batch: 150, loss is 5.010976419448853 and perplexity is 150.05117753137
At time: 686.2617402076721 and batch: 200, loss is 5.046291599273681 and perplexity is 155.44494216971273
At time: 687.4304504394531 and batch: 250, loss is 5.046624422073364 and perplexity is 155.4966864009145
At time: 688.6000490188599 and batch: 300, loss is 5.003216104507446 and perplexity is 148.8912396990327
At time: 689.7698414325714 and batch: 350, loss is 4.973137760162354 and perplexity is 144.4795189537275
At time: 690.9373581409454 and batch: 400, loss is 5.001412143707276 and perplexity is 148.62288786000016
At time: 692.1065676212311 and batch: 450, loss is 5.038112354278565 and perplexity is 154.17870539412687
At time: 693.2730269432068 and batch: 500, loss is 5.046267366409301 and perplexity is 155.4411753391513
At time: 694.4413695335388 and batch: 550, loss is 5.013498888015747 and perplexity is 150.43015468815338
At time: 695.6335220336914 and batch: 600, loss is 5.028696203231812 and perplexity is 152.73374905578484
At time: 696.801896572113 and batch: 650, loss is 5.013259210586548 and perplexity is 150.3941042958089
At time: 697.98952293396 and batch: 700, loss is 4.979478588104248 and perplexity is 145.39854935213873
At time: 699.1560213565826 and batch: 750, loss is 4.962559595108032 and perplexity is 142.95924578175013
At time: 700.3244669437408 and batch: 800, loss is 4.96399941444397 and perplexity is 143.16522952217898
At time: 701.4948356151581 and batch: 850, loss is 5.005475997924805 and perplexity is 149.2280985206829
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121484756469727 and perplexity of 167.5840064306422
Finished 32 epochs...
Completing Train Step...
At time: 704.6751992702484 and batch: 50, loss is 5.070937690734863 and perplexity is 159.32365363527373
At time: 705.8940081596375 and batch: 100, loss is 5.030265512466431 and perplexity is 152.97362370814074
At time: 707.063802242279 and batch: 150, loss is 5.0106704139709475 and perplexity is 150.00526807371025
At time: 708.2333426475525 and batch: 200, loss is 5.046003208160401 and perplexity is 155.40011969329916
At time: 709.3976321220398 and batch: 250, loss is 5.046355199813843 and perplexity is 155.4548288663964
At time: 710.5706150531769 and batch: 300, loss is 5.002991399765015 and perplexity is 148.85778689000898
At time: 711.7408349514008 and batch: 350, loss is 4.972977333068847 and perplexity is 144.45634238355515
At time: 712.9601051807404 and batch: 400, loss is 5.001374588012696 and perplexity is 148.6173063290259
At time: 714.1326041221619 and batch: 450, loss is 5.038183546066284 and perplexity is 154.18968204251107
At time: 715.3028604984283 and batch: 500, loss is 5.0463578987121585 and perplexity is 155.45524842373834
At time: 716.4721624851227 and batch: 550, loss is 5.013578777313232 and perplexity is 150.44217292758992
At time: 717.6405119895935 and batch: 600, loss is 5.028740453720093 and perplexity is 152.74050774829414
At time: 718.8054804801941 and batch: 650, loss is 5.013304119110107 and perplexity is 150.4008584246427
At time: 719.9679231643677 and batch: 700, loss is 4.9794980239868165 and perplexity is 145.40137532873212
At time: 721.1324117183685 and batch: 750, loss is 4.962663917541504 and perplexity is 142.97416041610938
At time: 722.2962689399719 and batch: 800, loss is 4.964160165786743 and perplexity is 143.18824537492813
At time: 723.4620027542114 and batch: 850, loss is 5.0055836486816405 and perplexity is 149.24416390313957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121476809183757 and perplexity of 167.58267459791136
Finished 33 epochs...
Completing Train Step...
At time: 726.6417353153229 and batch: 50, loss is 5.070721273422241 and perplexity is 159.28917696912518
At time: 727.8584425449371 and batch: 100, loss is 5.030045356750488 and perplexity is 152.9399493974249
At time: 729.0223705768585 and batch: 150, loss is 5.010396280288696 and perplexity is 149.96415221309476
At time: 730.1850051879883 and batch: 200, loss is 5.045746536254883 and perplexity is 155.36023796693803
At time: 731.3484084606171 and batch: 250, loss is 5.04610821723938 and perplexity is 155.41643897356266
At time: 732.5082719326019 and batch: 300, loss is 5.002787675857544 and perplexity is 148.82746408885092
At time: 733.6899440288544 and batch: 350, loss is 4.972835283279419 and perplexity is 144.43582384789937
At time: 734.8603572845459 and batch: 400, loss is 5.001345872879028 and perplexity is 148.61303882448067
At time: 736.0274572372437 and batch: 450, loss is 5.038248748779297 and perplexity is 154.1997359558665
At time: 737.1910924911499 and batch: 500, loss is 5.04644289970398 and perplexity is 155.46846283564926
At time: 738.3549282550812 and batch: 550, loss is 5.013654403686523 and perplexity is 150.45355075374485
At time: 739.5184328556061 and batch: 600, loss is 5.028781700134277 and perplexity is 152.74680787646741
At time: 740.6847319602966 and batch: 650, loss is 5.013346147537232 and perplexity is 150.40717966899547
At time: 741.8520164489746 and batch: 700, loss is 4.979520292282104 and perplexity is 145.40461320554405
At time: 743.0652487277985 and batch: 750, loss is 4.962758045196534 and perplexity is 142.98761887195568
At time: 744.2295496463776 and batch: 800, loss is 4.964301986694336 and perplexity is 143.20855390189664
At time: 745.3957271575928 and batch: 850, loss is 5.005678157806397 and perplexity is 149.25826950498862
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121474901835124 and perplexity of 167.58235495963092
Finished 34 epochs...
Completing Train Step...
At time: 748.5769538879395 and batch: 50, loss is 5.070520257949829 and perplexity is 159.25716059796736
At time: 749.738481760025 and batch: 100, loss is 5.029842004776001 and perplexity is 152.908851918711
At time: 750.9006571769714 and batch: 150, loss is 5.010143413543701 and perplexity is 149.92623606012776
At time: 752.0588009357452 and batch: 200, loss is 5.04550989151001 and perplexity is 155.32347713286163
At time: 753.2200846672058 and batch: 250, loss is 5.045875158309936 and perplexity is 155.38022200518546
At time: 754.3807234764099 and batch: 300, loss is 5.002596426010132 and perplexity is 148.79900358067385
At time: 755.542370557785 and batch: 350, loss is 4.972705268859864 and perplexity is 144.4170463287993
At time: 756.7043559551239 and batch: 400, loss is 5.001321744918823 and perplexity is 148.60945313825178
At time: 757.8716266155243 and batch: 450, loss is 5.038310451507568 and perplexity is 154.20925079381644
At time: 759.0335969924927 and batch: 500, loss is 5.046524782180786 and perplexity is 155.48119349965353
At time: 760.1943652629852 and batch: 550, loss is 5.0137248229980464 and perplexity is 150.46414596225443
At time: 761.3543910980225 and batch: 600, loss is 5.028819551467896 and perplexity is 152.75258965627486
At time: 762.5163915157318 and batch: 650, loss is 5.013384399414062 and perplexity is 150.41293313594636
At time: 763.6787292957306 and batch: 700, loss is 4.979542903900146 and perplexity is 145.4079010762913
At time: 764.8383781909943 and batch: 750, loss is 4.962844924926758 and perplexity is 143.00004213736585
At time: 765.9987576007843 and batch: 800, loss is 4.964430685043335 and perplexity is 143.22698579239787
At time: 767.1587662696838 and batch: 850, loss is 5.005764350891114 and perplexity is 149.271135090111
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121476809183757 and perplexity of 167.58267459791136
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 770.4015345573425 and batch: 50, loss is 5.070474758148193 and perplexity is 159.24991459359822
At time: 771.5716269016266 and batch: 100, loss is 5.0297624874114994 and perplexity is 152.8966934932068
At time: 772.7659065723419 and batch: 150, loss is 5.010215082168579 and perplexity is 149.9369814523485
At time: 773.9311866760254 and batch: 200, loss is 5.045198392868042 and perplexity is 155.2751016155142
At time: 775.094211101532 and batch: 250, loss is 5.045660648345947 and perplexity is 155.34689497397648
At time: 776.257618188858 and batch: 300, loss is 5.002347240447998 and perplexity is 148.7619296366593
At time: 777.4206488132477 and batch: 350, loss is 4.972655286788941 and perplexity is 144.40982824613607
At time: 778.581475019455 and batch: 400, loss is 5.001045551300049 and perplexity is 148.56841382326473
At time: 779.7431902885437 and batch: 450, loss is 5.038050556182862 and perplexity is 154.1691777381341
At time: 780.9052138328552 and batch: 500, loss is 5.046616010665893 and perplexity is 155.4953784604256
At time: 782.0667695999146 and batch: 550, loss is 5.013558702468872 and perplexity is 150.43915285469703
At time: 783.2284464836121 and batch: 600, loss is 5.028247680664062 and perplexity is 152.6652598830943
At time: 784.3902015686035 and batch: 650, loss is 5.012675762176514 and perplexity is 150.3063826877838
At time: 785.5549590587616 and batch: 700, loss is 4.97895094871521 and perplexity is 145.321851586552
At time: 786.7281885147095 and batch: 750, loss is 4.9622291088104244 and perplexity is 142.9120075161327
At time: 787.9014875888824 and batch: 800, loss is 4.963773927688599 and perplexity is 143.13295129838474
At time: 789.0706508159637 and batch: 850, loss is 5.005114545822144 and perplexity is 149.17416945767025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121403694152832 and perplexity of 167.57042223339715
Finished 36 epochs...
Completing Train Step...
At time: 792.3175489902496 and batch: 50, loss is 5.070369291305542 and perplexity is 159.23311989357111
At time: 793.4861161708832 and batch: 100, loss is 5.0296536064147945 and perplexity is 152.88004685509412
At time: 794.6796796321869 and batch: 150, loss is 5.010056810379028 and perplexity is 149.91325253583278
At time: 795.8485908508301 and batch: 200, loss is 5.04518840789795 and perplexity is 155.27355120600885
At time: 797.0209345817566 and batch: 250, loss is 5.045576553344727 and perplexity is 155.33383162594288
At time: 798.195207118988 and batch: 300, loss is 5.002315292358398 and perplexity is 148.75717705312064
At time: 799.3688590526581 and batch: 350, loss is 4.972626848220825 and perplexity is 144.40572149579427
At time: 800.5423498153687 and batch: 400, loss is 5.000995388031006 and perplexity is 148.5609613328731
At time: 801.7177007198334 and batch: 450, loss is 5.038025588989258 and perplexity is 154.1653286144768
At time: 802.9288880825043 and batch: 500, loss is 5.04653395652771 and perplexity is 155.48261994460617
At time: 804.0906348228455 and batch: 550, loss is 5.013518056869507 and perplexity is 150.43303828942717
At time: 805.2838935852051 and batch: 600, loss is 5.028235559463501 and perplexity is 152.6634094080755
At time: 806.4711954593658 and batch: 650, loss is 5.012678365707398 and perplexity is 150.30677401560263
At time: 807.6449391841888 and batch: 700, loss is 4.978964557647705 and perplexity is 145.32382927527743
At time: 808.8165028095245 and batch: 750, loss is 4.962254648208618 and perplexity is 142.91565744940763
At time: 809.9849441051483 and batch: 800, loss is 4.963858985900879 and perplexity is 143.14512644913157
At time: 811.1548562049866 and batch: 850, loss is 5.005141983032226 and perplexity is 149.17826243684624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121334711710612 and perplexity of 167.55886321511693
Finished 37 epochs...
Completing Train Step...
At time: 814.3412282466888 and batch: 50, loss is 5.0703087902069095 and perplexity is 159.2234864063001
At time: 815.5438981056213 and batch: 100, loss is 5.029593152999878 and perplexity is 152.87080501354237
At time: 816.7122797966003 and batch: 150, loss is 5.009977827072143 and perplexity is 149.9014123589959
At time: 817.8824751377106 and batch: 200, loss is 5.0451560306549075 and perplexity is 155.26852395788808
At time: 819.0526938438416 and batch: 250, loss is 5.045535526275635 and perplexity is 155.32745886482914
At time: 820.2211813926697 and batch: 300, loss is 5.002294311523437 and perplexity is 148.75405603608053
At time: 821.3911244869232 and batch: 350, loss is 4.9726058387756344 and perplexity is 144.40268764357316
At time: 822.5605738162994 and batch: 400, loss is 5.000972785949707 and perplexity is 148.5576035838934
At time: 823.7291548252106 and batch: 450, loss is 5.03803165435791 and perplexity is 154.1662636868641
At time: 824.90021443367 and batch: 500, loss is 5.046532983779907 and perplexity is 155.4824686993029
At time: 826.0731754302979 and batch: 550, loss is 5.013520078659058 and perplexity is 150.43334243367954
At time: 827.2435894012451 and batch: 600, loss is 5.028228263854981 and perplexity is 152.66229563966795
At time: 828.4133853912354 and batch: 650, loss is 5.012682638168335 and perplexity is 150.30741619679503
At time: 829.5833656787872 and batch: 700, loss is 4.97897497177124 and perplexity is 145.3253427034686
At time: 830.7528584003448 and batch: 750, loss is 4.962290687561035 and perplexity is 142.9208081299654
At time: 831.9716515541077 and batch: 800, loss is 4.963912286758423 and perplexity is 143.15275641046443
At time: 833.1415588855743 and batch: 850, loss is 5.005157384872437 and perplexity is 149.18056007430104
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.1212921142578125 and perplexity of 167.55172578636916
Finished 38 epochs...
Completing Train Step...
At time: 836.3479080200195 and batch: 50, loss is 5.070257234573364 and perplexity is 159.21527775018617
At time: 837.55783867836 and batch: 100, loss is 5.02954626083374 and perplexity is 152.86363673842536
At time: 838.7476179599762 and batch: 150, loss is 5.009921226501465 and perplexity is 149.8929280936203
At time: 839.9187109470367 and batch: 200, loss is 5.0451192283630375 and perplexity is 155.2628098254984
At time: 841.0861718654633 and batch: 250, loss is 5.0455020236968995 and perplexity is 155.32225508157933
At time: 842.2734529972076 and batch: 300, loss is 5.002273864746094 and perplexity is 148.75101452611247
At time: 843.4410650730133 and batch: 350, loss is 4.972588777542114 and perplexity is 144.400223976615
At time: 844.6094737052917 and batch: 400, loss is 5.000957231521607 and perplexity is 148.55529287330066
At time: 845.7780027389526 and batch: 450, loss is 5.0380419921875 and perplexity is 154.16785743966454
At time: 846.945769071579 and batch: 500, loss is 5.046548519134522 and perplexity is 155.48488419335317
At time: 848.1073133945465 and batch: 550, loss is 5.013528547286987 and perplexity is 150.4346164030792
At time: 849.2949559688568 and batch: 600, loss is 5.028221864700317 and perplexity is 152.66131873315248
At time: 850.4701335430145 and batch: 650, loss is 5.012685394287109 and perplexity is 150.3078304624576
At time: 851.6525809764862 and batch: 700, loss is 4.978984107971192 and perplexity is 145.32667043092272
At time: 852.8215892314911 and batch: 750, loss is 4.962323484420776 and perplexity is 142.92549556052978
At time: 854.0041661262512 and batch: 800, loss is 4.963953227996826 and perplexity is 143.15861738156957
At time: 855.1679751873016 and batch: 850, loss is 5.0051695823669435 and perplexity is 149.18237971446058
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121267318725586 and perplexity of 167.54757130365934
Finished 39 epochs...
Completing Train Step...
At time: 858.4505953788757 and batch: 50, loss is 5.0702108669281 and perplexity is 159.20789548381737
At time: 859.6686153411865 and batch: 100, loss is 5.029505043029785 and perplexity is 152.85733616486326
At time: 860.836656332016 and batch: 150, loss is 5.009873571395874 and perplexity is 149.88578510050613
At time: 862.0333161354065 and batch: 200, loss is 5.045082206726074 and perplexity is 155.25706184851958
At time: 863.2046480178833 and batch: 250, loss is 5.045470399856567 and perplexity is 155.31734327305014
At time: 864.3742158412933 and batch: 300, loss is 5.002253303527832 and perplexity is 148.74795605547914
At time: 865.5651643276215 and batch: 350, loss is 4.972573499679566 and perplexity is 144.3980178666935
At time: 866.7415301799774 and batch: 400, loss is 5.000945167541504 and perplexity is 148.55350071601353
At time: 867.9268157482147 and batch: 450, loss is 5.038053426742554 and perplexity is 154.16962029059667
At time: 869.0908281803131 and batch: 500, loss is 5.046568031311035 and perplexity is 155.48791807145744
At time: 870.2596423625946 and batch: 550, loss is 5.0135384464263915 and perplexity is 150.43610558368897
At time: 871.4286479949951 and batch: 600, loss is 5.0282161426544185 and perplexity is 152.66044520057898
At time: 872.5959839820862 and batch: 650, loss is 5.012686796188355 and perplexity is 150.30804117934005
At time: 873.7642395496368 and batch: 700, loss is 4.97899284362793 and perplexity is 145.3279399603756
At time: 874.9336681365967 and batch: 750, loss is 4.962353467941284 and perplexity is 142.92978103430355
At time: 876.1224238872528 and batch: 800, loss is 4.963988666534424 and perplexity is 143.1636908035109
At time: 877.2985744476318 and batch: 850, loss is 5.0051807689666745 and perplexity is 149.18404856736373
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121253649393718 and perplexity of 167.5452810559566
Finished 40 epochs...
Completing Train Step...
At time: 880.6170914173126 and batch: 50, loss is 5.070168237686158 and perplexity is 159.20110871658
At time: 881.7991683483124 and batch: 100, loss is 5.02946683883667 and perplexity is 152.8514964852241
At time: 882.9706647396088 and batch: 150, loss is 5.009830312728882 and perplexity is 149.8793013814811
At time: 884.1509368419647 and batch: 200, loss is 5.0450459003448485 and perplexity is 155.25142512876917
At time: 885.3354675769806 and batch: 250, loss is 5.045439262390136 and perplexity is 155.31250715978038
At time: 886.5162494182587 and batch: 300, loss is 5.002233180999756 and perplexity is 148.74496290067222
At time: 887.7204332351685 and batch: 350, loss is 4.972559394836426 and perplexity is 144.39598116966548
At time: 888.8950927257538 and batch: 400, loss is 5.000934925079346 and perplexity is 148.55197917019615
At time: 890.0672504901886 and batch: 450, loss is 5.038064956665039 and perplexity is 154.1713978646158
At time: 891.2364485263824 and batch: 500, loss is 5.046588430404663 and perplexity is 155.49108991640742
At time: 892.4602525234222 and batch: 550, loss is 5.013548831939698 and perplexity is 150.4376679479782
At time: 893.6290369033813 and batch: 600, loss is 5.028210706710816 and perplexity is 152.65961534926402
At time: 894.80619597435 and batch: 650, loss is 5.012687129974365 and perplexity is 150.30809135006982
At time: 895.9971828460693 and batch: 700, loss is 4.979000902175903 and perplexity is 145.3291110972704
At time: 897.1760957241058 and batch: 750, loss is 4.962381162643433 and perplexity is 142.93373948683126
At time: 898.346798658371 and batch: 800, loss is 4.964020490646362 and perplexity is 143.16824693332958
At time: 899.5227715969086 and batch: 850, loss is 5.005191850662231 and perplexity is 149.1857017887321
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121245702107747 and perplexity of 167.54394953098603
Finished 41 epochs...
Completing Train Step...
At time: 902.7644741535187 and batch: 50, loss is 5.070128297805786 and perplexity is 159.19475037031944
At time: 903.9301233291626 and batch: 100, loss is 5.029430332183838 and perplexity is 152.84591649056122
At time: 905.0958881378174 and batch: 150, loss is 5.009789657592774 and perplexity is 149.87320814194518
At time: 906.2639365196228 and batch: 200, loss is 5.045010385513305 and perplexity is 155.24591149846728
At time: 907.4291296005249 and batch: 250, loss is 5.04540831565857 and perplexity is 155.307700819683
At time: 908.6187629699707 and batch: 300, loss is 5.002213172912597 and perplexity is 148.74198682826295
At time: 909.7844338417053 and batch: 350, loss is 4.972545490264893 and perplexity is 144.39397341937467
At time: 910.9519488811493 and batch: 400, loss is 5.000925960540772 and perplexity is 148.5506474762177
At time: 912.1314780712128 and batch: 450, loss is 5.038076000213623 and perplexity is 154.17310047333982
At time: 913.3076252937317 and batch: 500, loss is 5.046609096527099 and perplexity is 155.49430334751386
At time: 914.4777476787567 and batch: 550, loss is 5.013559226989746 and perplexity is 150.43923176319365
At time: 915.6580665111542 and batch: 600, loss is 5.028205327987671 and perplexity is 152.6587942376659
At time: 916.8246955871582 and batch: 650, loss is 5.012686614990234 and perplexity is 150.30801394380796
At time: 918.0059583187103 and batch: 700, loss is 4.979008893966675 and perplexity is 145.33027254176042
At time: 919.1858148574829 and batch: 750, loss is 4.962407159805298 and perplexity is 142.93745540669414
At time: 920.3577396869659 and batch: 800, loss is 4.964050273895264 and perplexity is 143.17251101236158
At time: 921.52912068367 and batch: 850, loss is 5.0052028465271 and perplexity is 149.18734222356827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.1212412516276045 and perplexity of 167.54320388162486
Finished 42 epochs...
Completing Train Step...
At time: 924.7638955116272 and batch: 50, loss is 5.07009030342102 and perplexity is 159.1887019786244
At time: 925.9305808544159 and batch: 100, loss is 5.029395112991333 and perplexity is 152.84053347559794
At time: 927.1017627716064 and batch: 150, loss is 5.009750127792358 and perplexity is 149.86728380103452
At time: 928.2704303264618 and batch: 200, loss is 5.044975786209107 and perplexity is 155.24054019087205
At time: 929.4391424655914 and batch: 250, loss is 5.045377473831177 and perplexity is 155.3029109202466
At time: 930.6139390468597 and batch: 300, loss is 5.002193269729614 and perplexity is 148.73902641874267
At time: 931.8071358203888 and batch: 350, loss is 4.972532186508179 and perplexity is 144.3920524498594
At time: 932.9791119098663 and batch: 400, loss is 5.000917692184448 and perplexity is 148.5494192116101
At time: 934.1472029685974 and batch: 450, loss is 5.038086776733398 and perplexity is 154.1747619317582
At time: 935.3149266242981 and batch: 500, loss is 5.046629648208619 and perplexity is 155.49749904975275
At time: 936.4895620346069 and batch: 550, loss is 5.0135695838928225 and perplexity is 150.4407898558044
At time: 937.6587319374084 and batch: 600, loss is 5.028200426101685 and perplexity is 152.65804592349585
At time: 938.8305525779724 and batch: 650, loss is 5.0126855754852295 and perplexity is 150.30785769795642
At time: 940.0002903938293 and batch: 700, loss is 4.979016723632813 and perplexity is 145.3314104337288
At time: 941.1715750694275 and batch: 750, loss is 4.9624317932128905 and perplexity is 142.94097648666138
At time: 942.341536283493 and batch: 800, loss is 4.9640783596038816 and perplexity is 143.17653217025617
At time: 943.5063674449921 and batch: 850, loss is 5.0052136707305905 and perplexity is 149.1889570664584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121238390604655 and perplexity of 167.54272453735928
Finished 43 epochs...
Completing Train Step...
At time: 946.6857285499573 and batch: 50, loss is 5.070053911209106 and perplexity is 159.1829088550606
At time: 947.8799619674683 and batch: 100, loss is 5.029360799789429 and perplexity is 152.83528911748957
At time: 949.0536870956421 and batch: 150, loss is 5.009712047576905 and perplexity is 149.861576931238
At time: 950.2333724498749 and batch: 200, loss is 5.044941663742065 and perplexity is 155.2352430910315
At time: 951.4060218334198 and batch: 250, loss is 5.045346965789795 and perplexity is 155.2981730048861
At time: 952.6252686977386 and batch: 300, loss is 5.0021737194061275 and perplexity is 148.7361185510861
At time: 953.7955777645111 and batch: 350, loss is 4.972518806457519 and perplexity is 144.39012048980769
At time: 954.9662208557129 and batch: 400, loss is 5.000909900665283 and perplexity is 148.5482617904724
At time: 956.1374032497406 and batch: 450, loss is 5.038097257614136 and perplexity is 154.17637782751876
At time: 957.3064527511597 and batch: 500, loss is 5.046650075912476 and perplexity is 155.50067553905797
At time: 958.474259853363 and batch: 550, loss is 5.013579521179199 and perplexity is 150.44228483644397
At time: 959.6442210674286 and batch: 600, loss is 5.0281955432891845 and perplexity is 152.6573005247008
At time: 960.8179762363434 and batch: 650, loss is 5.012684230804443 and perplexity is 150.30765558200406
At time: 961.9903903007507 and batch: 700, loss is 4.979024391174317 and perplexity is 145.3325247726222
At time: 963.1795959472656 and batch: 750, loss is 4.962455549240112 and perplexity is 142.94437223672452
At time: 964.3519721031189 and batch: 800, loss is 4.9641057395935055 and perplexity is 143.180452395889
At time: 965.5369191169739 and batch: 850, loss is 5.005224485397338 and perplexity is 149.19057050403595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121238390604655 and perplexity of 167.54272453735928
Finished 44 epochs...
Completing Train Step...
At time: 968.7265732288361 and batch: 50, loss is 5.0700189399719235 and perplexity is 159.17734212913783
At time: 969.9244832992554 and batch: 100, loss is 5.029327306747437 and perplexity is 152.8301702844564
At time: 971.094616651535 and batch: 150, loss is 5.009674425125122 and perplexity is 149.85593887734524
At time: 972.266744852066 and batch: 200, loss is 5.044908199310303 and perplexity is 155.23004831875247
At time: 973.43812251091 and batch: 250, loss is 5.04531662940979 and perplexity is 155.2934618919551
At time: 974.6282529830933 and batch: 300, loss is 5.00215443611145 and perplexity is 148.73325045633612
At time: 975.798751115799 and batch: 350, loss is 4.972505807876587 and perplexity is 144.3882436353389
At time: 976.9664359092712 and batch: 400, loss is 5.0009025287628175 and perplexity is 148.54716671121147
At time: 978.1377415657043 and batch: 450, loss is 5.038107376098633 and perplexity is 154.17793786670026
At time: 979.3087284564972 and batch: 500, loss is 5.046670198440552 and perplexity is 155.50380463724989
At time: 980.4779732227325 and batch: 550, loss is 5.013589496612549 and perplexity is 150.44378557091454
At time: 981.7003943920135 and batch: 600, loss is 5.028190879821778 and perplexity is 152.65658861401536
At time: 982.8715450763702 and batch: 650, loss is 5.0126824378967285 and perplexity is 150.30738609449034
At time: 984.0408945083618 and batch: 700, loss is 4.979031944274903 and perplexity is 145.3336224879458
At time: 985.2136416435242 and batch: 750, loss is 4.962478294372558 and perplexity is 142.94762356237922
At time: 986.3845884799957 and batch: 800, loss is 4.9641321182250975 and perplexity is 143.18422935010915
At time: 987.5579385757446 and batch: 850, loss is 5.00523530960083 and perplexity is 149.19218538187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121237436930339 and perplexity of 167.54256475624217
Finished 45 epochs...
Completing Train Step...
At time: 990.7169690132141 and batch: 50, loss is 5.069984731674194 and perplexity is 159.17189703636066
At time: 991.9066224098206 and batch: 100, loss is 5.029294195175171 and perplexity is 152.82510992100745
At time: 993.0734391212463 and batch: 150, loss is 5.009637422561646 and perplexity is 149.85039392604398
At time: 994.2490916252136 and batch: 200, loss is 5.044875173568726 and perplexity is 155.22492181594544
At time: 995.4284913539886 and batch: 250, loss is 5.045286493301392 and perplexity is 155.28878202187093
At time: 996.5976204872131 and batch: 300, loss is 5.002135133743286 and perplexity is 148.730379580085
At time: 997.7678980827332 and batch: 350, loss is 4.972493095397949 and perplexity is 144.38640811454317
At time: 998.9507262706757 and batch: 400, loss is 5.000895433425903 and perplexity is 148.5461127227552
At time: 1000.118800163269 and batch: 450, loss is 5.038117275238037 and perplexity is 154.17946410315446
At time: 1001.2868065834045 and batch: 500, loss is 5.046690263748169 and perplexity is 155.50692490023
At time: 1002.4483723640442 and batch: 550, loss is 5.013599300384522 and perplexity is 150.44526049471293
At time: 1003.6111948490143 and batch: 600, loss is 5.028186197280884 and perplexity is 152.65587379497006
At time: 1004.7800071239471 and batch: 650, loss is 5.0126806640625 and perplexity is 150.3071194743405
At time: 1005.9486742019653 and batch: 700, loss is 4.979039497375489 and perplexity is 145.33472021156055
At time: 1007.1168444156647 and batch: 750, loss is 4.9625004959106445 and perplexity is 142.95079725471837
At time: 1008.2874732017517 and batch: 800, loss is 4.964157800674439 and perplexity is 143.18790671904765
At time: 1009.454701423645 and batch: 850, loss is 5.005245885848999 and perplexity is 149.19376328379158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121237754821777 and perplexity of 167.54261801659757
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 1012.6918158531189 and batch: 50, loss is 5.069968299865723 and perplexity is 159.169281575723
At time: 1013.8526232242584 and batch: 100, loss is 5.02926872253418 and perplexity is 152.82121711142827
At time: 1015.0151772499084 and batch: 150, loss is 5.009624757766724 and perplexity is 149.8484961135536
At time: 1016.1779396533966 and batch: 200, loss is 5.044835605621338 and perplexity is 155.21878000591593
At time: 1017.3519213199615 and batch: 250, loss is 5.04524338722229 and perplexity is 155.28208827562108
At time: 1018.5299348831177 and batch: 300, loss is 5.002114667892456 and perplexity is 148.72733571747034
At time: 1019.6979002952576 and batch: 350, loss is 4.9725042247772215 and perplexity is 144.38801505458298
At time: 1020.8727259635925 and batch: 400, loss is 5.000845975875855 and perplexity is 148.5387661776233
At time: 1022.0580775737762 and batch: 450, loss is 5.038049755096435 and perplexity is 154.1690542353479
At time: 1023.2277235984802 and batch: 500, loss is 5.046653070449829 and perplexity is 155.50114119233652
At time: 1024.3977150917053 and batch: 550, loss is 5.013540306091309 and perplexity is 150.43638534469693
At time: 1025.5673568248749 and batch: 600, loss is 5.028104219436646 and perplexity is 152.64335990846376
At time: 1026.7387580871582 and batch: 650, loss is 5.012579069137574 and perplexity is 150.29184980949503
At time: 1027.9071946144104 and batch: 700, loss is 4.97895170211792 and perplexity is 145.32196107247003
At time: 1029.075900554657 and batch: 750, loss is 4.962438507080078 and perplexity is 142.94193617661477
At time: 1030.2464344501495 and batch: 800, loss is 4.964028396606445 and perplexity is 143.1693788202493
At time: 1031.4146404266357 and batch: 850, loss is 5.0051522541046145 and perplexity is 149.17979466544728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121238072713216 and perplexity of 167.54267127696988
Annealing...
Finished 47 epochs...
Completing Train Step...
At time: 1034.6207180023193 and batch: 50, loss is 5.069966564178467 and perplexity is 159.16900530786916
At time: 1035.789468050003 and batch: 100, loss is 5.029265813827514 and perplexity is 152.82077259998186
At time: 1036.9554243087769 and batch: 150, loss is 5.009622087478638 and perplexity is 149.84809597543403
At time: 1038.1247472763062 and batch: 200, loss is 5.044833393096924 and perplexity is 155.21843658095554
At time: 1039.294898033142 and batch: 250, loss is 5.045237846374512 and perplexity is 155.28122788359096
At time: 1040.4635798931122 and batch: 300, loss is 5.0021139335632325 and perplexity is 148.72722650268153
At time: 1041.6654586791992 and batch: 350, loss is 4.972508153915405 and perplexity is 144.38858237616074
At time: 1042.8357603549957 and batch: 400, loss is 5.0008389759063725 and perplexity is 148.53772641443226
At time: 1044.0059087276459 and batch: 450, loss is 5.03803973197937 and perplexity is 154.16750898861355
At time: 1045.174411058426 and batch: 500, loss is 5.046642599105835 and perplexity is 155.4995128949208
At time: 1046.3434324264526 and batch: 550, loss is 5.013528995513916 and perplexity is 150.43468383194036
At time: 1047.5155038833618 and batch: 600, loss is 5.028094806671143 and perplexity is 152.64192311907345
At time: 1048.685554265976 and batch: 650, loss is 5.012566337585449 and perplexity is 150.28993637315583
At time: 1049.8550772666931 and batch: 700, loss is 4.978941345214844 and perplexity is 145.32045599479844
At time: 1051.0250816345215 and batch: 750, loss is 4.962428722381592 and perplexity is 142.9405375397109
At time: 1052.1952636241913 and batch: 800, loss is 4.964012689590454 and perplexity is 143.16713007418736
At time: 1053.3636379241943 and batch: 850, loss is 5.005140333175659 and perplexity is 149.17801631431337
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121238072713216 and perplexity of 167.54267127696988
Annealing...
Finished 48 epochs...
Completing Train Step...
At time: 1056.577000617981 and batch: 50, loss is 5.0699665737152095 and perplexity is 159.16900682582306
At time: 1057.7429549694061 and batch: 100, loss is 5.029265470504761 and perplexity is 152.82072013314246
At time: 1058.9299447536469 and batch: 150, loss is 5.009621982574463 and perplexity is 149.84808025574395
At time: 1060.0944740772247 and batch: 200, loss is 5.044833335876465 and perplexity is 155.21842769928566
At time: 1061.2574472427368 and batch: 250, loss is 5.045237293243408 and perplexity is 155.28114199273776
At time: 1062.4244389533997 and batch: 300, loss is 5.002114200592041 and perplexity is 148.72726621714096
At time: 1063.5896933078766 and batch: 350, loss is 4.97250916481018 and perplexity is 144.388728337898
At time: 1064.7557380199432 and batch: 400, loss is 5.000838403701782 and perplexity is 148.5376414204877
At time: 1065.9220752716064 and batch: 450, loss is 5.038038673400879 and perplexity is 154.16734579029097
At time: 1067.1084201335907 and batch: 500, loss is 5.046641416549683 and perplexity is 155.49932900812396
At time: 1068.279133796692 and batch: 550, loss is 5.01352786064148 and perplexity is 150.43451310786114
At time: 1069.4466252326965 and batch: 600, loss is 5.0280939483642575 and perplexity is 152.6417921055161
At time: 1070.6126675605774 and batch: 650, loss is 5.0125651454925535 and perplexity is 150.2897572136972
At time: 1071.8286879062653 and batch: 700, loss is 4.978940620422363 and perplexity is 145.32035066766275
At time: 1072.9947443008423 and batch: 750, loss is 4.9624277782440185 and perplexity is 142.94040258424235
At time: 1074.323354959488 and batch: 800, loss is 4.96401120185852 and perplexity is 143.16691708003447
At time: 1075.488828420639 and batch: 850, loss is 5.005139179229737 and perplexity is 149.17784417104897
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121238390604655 and perplexity of 167.54272453735928
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 1078.6417589187622 and batch: 50, loss is 5.069966487884521 and perplexity is 159.16899316423823
At time: 1079.806143283844 and batch: 100, loss is 5.029265432357788 and perplexity is 152.82071430349473
At time: 1080.9704699516296 and batch: 150, loss is 5.009621868133545 and perplexity is 149.84806310699304
At time: 1082.1379570960999 and batch: 200, loss is 5.0448332691192626 and perplexity is 155.218417337338
At time: 1083.3056237697601 and batch: 250, loss is 5.045237302780151 and perplexity is 155.28114347361412
At time: 1084.4765906333923 and batch: 300, loss is 5.002114152908325 and perplexity is 148.72725912527235
At time: 1085.7094032764435 and batch: 350, loss is 4.972509117126465 and perplexity is 144.3887214529071
At time: 1086.8786234855652 and batch: 400, loss is 5.0008384704589846 and perplexity is 148.5376513364454
At time: 1088.0479757785797 and batch: 450, loss is 5.038038597106934 and perplexity is 154.16733402825636
At time: 1089.2177333831787 and batch: 500, loss is 5.046641254425049 and perplexity is 155.49930379785425
At time: 1090.3869695663452 and batch: 550, loss is 5.013527679443359 and perplexity is 150.43448584941257
At time: 1091.556372642517 and batch: 600, loss is 5.028093862533569 and perplexity is 152.64177900416658
At time: 1092.7254495620728 and batch: 650, loss is 5.0125651359558105 and perplexity is 150.2897557804224
At time: 1093.8954668045044 and batch: 700, loss is 4.97894058227539 and perplexity is 145.3203451241314
At time: 1095.0642824172974 and batch: 750, loss is 4.9624277687072755 and perplexity is 142.94040122105648
At time: 1096.2344317436218 and batch: 800, loss is 4.964011125564575 and perplexity is 143.16690615726594
At time: 1097.402278661728 and batch: 850, loss is 5.005139150619507 and perplexity is 149.17783990303664
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.121238390604655 and perplexity of 167.54272453735928
Annealing...
Model not improving. Stopping early with 167.54256475624217loss at 49 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -167.54256475624217
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fe9cde3e860>
SETTINGS FOR THIS RUN
{'dropout': 0.598377035716386, 'batch_size': 50, 'data': 'wikitext', 'lr': 11.942195201405458, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'anneal': 7.344128300137092, 'wordvec_source': '', 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.6423370838165283 and batch: 50, loss is 7.176869802474975 and perplexity is 1308.8050213863862
At time: 2.7998218536376953 and batch: 100, loss is 6.339042654037476 and perplexity is 566.2539508760644
At time: 3.9348785877227783 and batch: 150, loss is 6.179117631912232 and perplexity is 482.5659676309167
At time: 5.068319320678711 and batch: 200, loss is 6.158629550933838 and perplexity is 472.7797101486477
At time: 6.230194568634033 and batch: 250, loss is 6.1632487583160405 and perplexity is 474.9686293196256
At time: 7.363417148590088 and batch: 300, loss is 6.087828330993652 and perplexity is 440.4638300995274
At time: 8.49931287765503 and batch: 350, loss is 6.05038067817688 and perplexity is 424.2745113540877
At time: 9.633064985275269 and batch: 400, loss is 6.055967674255371 and perplexity is 426.6515654981027
At time: 10.768172979354858 and batch: 450, loss is 6.055027542114257 and perplexity is 426.2506451369095
At time: 11.937215566635132 and batch: 500, loss is 6.043089466094971 and perplexity is 421.192286144402
At time: 13.072206497192383 and batch: 550, loss is 5.996714162826538 and perplexity is 402.1053676378888
At time: 14.20948052406311 and batch: 600, loss is 6.014290714263916 and perplexity is 409.2354710695905
At time: 15.345164775848389 and batch: 650, loss is 6.02471996307373 and perplexity is 413.5258233065349
At time: 16.4797465801239 and batch: 700, loss is 5.976091461181641 and perplexity is 393.8977907017588
At time: 17.6153507232666 and batch: 750, loss is 5.967409391403198 and perplexity is 390.4927454065977
At time: 18.754648447036743 and batch: 800, loss is 5.977264137268066 and perplexity is 394.35997616536565
At time: 19.89408302307129 and batch: 850, loss is 5.97015061378479 and perplexity is 391.5646413415073
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.349674224853516 and perplexity of 210.53969809224358
Finished 1 epochs...
Completing Train Step...
At time: 23.00272798538208 and batch: 50, loss is 5.655320148468018 and perplexity is 285.8079690645694
At time: 24.1368191242218 and batch: 100, loss is 5.528101043701172 and perplexity is 251.66555508771953
At time: 25.270535945892334 and batch: 150, loss is 5.4863792896270756 and perplexity is 241.38164970211858
At time: 26.4059054851532 and batch: 200, loss is 5.468024206161499 and perplexity is 236.9914835642691
At time: 27.53766655921936 and batch: 250, loss is 5.4978162288665775 and perplexity is 244.15816411182394
At time: 28.67048692703247 and batch: 300, loss is 5.442862148284912 and perplexity is 231.10268794078948
At time: 29.822699069976807 and batch: 350, loss is 5.412522163391113 and perplexity is 224.1963349771912
At time: 30.960047483444214 and batch: 400, loss is 5.4150974082946775 and perplexity is 224.77443950711765
At time: 32.09492230415344 and batch: 450, loss is 5.416438617706299 and perplexity is 225.0761113582178
At time: 33.225796699523926 and batch: 500, loss is 5.423049144744873 and perplexity is 226.56891174057228
At time: 34.36170816421509 and batch: 550, loss is 5.388220405578613 and perplexity is 218.81363932983575
At time: 35.550567626953125 and batch: 600, loss is 5.409073286056518 and perplexity is 223.4244411676452
At time: 36.69762396812439 and batch: 650, loss is 5.4364598274230955 and perplexity is 229.627820705062
At time: 37.832035541534424 and batch: 700, loss is 5.383804302215577 and perplexity is 217.84946619236953
At time: 38.9694390296936 and batch: 750, loss is 5.3855832576751705 and perplexity is 218.23735560632184
At time: 40.12463688850403 and batch: 800, loss is 5.388407564163208 and perplexity is 218.85459601344067
At time: 41.262388706207275 and batch: 850, loss is 5.369843015670776 and perplexity is 214.829140248584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.116293907165527 and perplexity of 166.71635697448232
Finished 2 epochs...
Completing Train Step...
At time: 44.36498236656189 and batch: 50, loss is 5.358374090194702 and perplexity is 212.37935589894843
At time: 45.500136375427246 and batch: 100, loss is 5.268650817871094 and perplexity is 194.15383677068738
At time: 46.63799333572388 and batch: 150, loss is 5.2751641273498535 and perplexity is 195.422548065485
At time: 47.77385115623474 and batch: 200, loss is 5.288195238113404 and perplexity is 197.98578557918702
At time: 48.9093496799469 and batch: 250, loss is 5.316503620147705 and perplexity is 203.67052611410676
At time: 50.0455596446991 and batch: 300, loss is 5.2752511692047115 and perplexity is 195.43955874685957
At time: 51.17985463142395 and batch: 350, loss is 5.247010612487793 and perplexity is 189.99744267939874
At time: 52.31846308708191 and batch: 400, loss is 5.256817197799682 and perplexity is 191.86983469057287
At time: 53.45375037193298 and batch: 450, loss is 5.267805328369141 and perplexity is 193.98975111605787
At time: 54.58849000930786 and batch: 500, loss is 5.280060815811157 and perplexity is 196.3818181082052
At time: 55.72513818740845 and batch: 550, loss is 5.261552934646606 and perplexity is 192.78063468886216
At time: 56.86145877838135 and batch: 600, loss is 5.297087707519531 and perplexity is 199.7542193384733
At time: 57.99677300453186 and batch: 650, loss is 5.293184061050415 and perplexity is 198.97596947989467
At time: 59.13027858734131 and batch: 700, loss is 5.2596907138824465 and perplexity is 192.42196864934024
At time: 60.30510663986206 and batch: 750, loss is 5.2685340309143065 and perplexity is 194.1311634589417
At time: 61.44032120704651 and batch: 800, loss is 5.255740213394165 and perplexity is 191.66330510525597
At time: 62.57821011543274 and batch: 850, loss is 5.251570138931275 and perplexity is 190.86571900368347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.090993563334147 and perplexity of 162.5512868458608
Finished 3 epochs...
Completing Train Step...
At time: 65.65940809249878 and batch: 50, loss is 5.243348207473755 and perplexity is 189.3028677763521
At time: 66.81937408447266 and batch: 100, loss is 5.171979341506958 and perplexity is 176.2633778500015
At time: 67.97610330581665 and batch: 150, loss is 5.196548156738281 and perplexity is 180.64759721645729
At time: 69.1148316860199 and batch: 200, loss is 5.2060837554931645 and perplexity is 182.3784193171366
At time: 70.25375461578369 and batch: 250, loss is 5.224638090133667 and perplexity is 185.79391770503634
At time: 71.39145827293396 and batch: 300, loss is 5.190777616500855 and perplexity is 179.60816491523255
At time: 72.52991509437561 and batch: 350, loss is 5.154833278656006 and perplexity is 173.26691705740558
At time: 73.66858792304993 and batch: 400, loss is 5.160886163711548 and perplexity is 174.31886223017906
At time: 74.80753636360168 and batch: 450, loss is 5.183447437286377 and perplexity is 178.2964184199742
At time: 75.94675731658936 and batch: 500, loss is 5.196548070907593 and perplexity is 180.64758171135034
At time: 77.08378458023071 and batch: 550, loss is 5.16925009727478 and perplexity is 175.78296791888235
At time: 78.22516822814941 and batch: 600, loss is 5.197267932891846 and perplexity is 180.77766985510857
At time: 79.36333465576172 and batch: 650, loss is 5.207239236831665 and perplexity is 182.58927597420873
At time: 80.50202178955078 and batch: 700, loss is 5.173263072967529 and perplexity is 176.48979799373058
At time: 81.63836693763733 and batch: 750, loss is 5.176937913894653 and perplexity is 177.13956308706992
At time: 82.77744960784912 and batch: 800, loss is 5.175073862075806 and perplexity is 176.80967332363645
At time: 83.91790676116943 and batch: 850, loss is 5.173927621841431 and perplexity is 176.60712307004826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.061928431193034 and perplexity of 157.89471200672213
Finished 4 epochs...
Completing Train Step...
At time: 87.07575917243958 and batch: 50, loss is 5.168917083740235 and perplexity is 175.72443955733195
At time: 88.2127947807312 and batch: 100, loss is 5.087975635528564 and perplexity is 162.06145830282057
At time: 89.376544713974 and batch: 150, loss is 5.096821117401123 and perplexity is 163.50132878033324
At time: 90.51390528678894 and batch: 200, loss is 5.140072631835937 and perplexity is 170.72816817130598
At time: 91.65033602714539 and batch: 250, loss is 5.152456817626953 and perplexity is 172.85564386231243
At time: 92.78749704360962 and batch: 300, loss is 5.117304887771606 and perplexity is 166.88498920569432
At time: 93.9230580329895 and batch: 350, loss is 5.090676908493042 and perplexity is 162.49982234262518
At time: 95.06034278869629 and batch: 400, loss is 5.097438116073608 and perplexity is 163.60224001098663
At time: 96.19542646408081 and batch: 450, loss is 5.1190314197540285 and perplexity is 167.1733703549462
At time: 97.33063173294067 and batch: 500, loss is 5.133294916152954 and perplexity is 169.5749337505894
At time: 98.46824288368225 and batch: 550, loss is 5.112911033630371 and perplexity is 166.15332948492528
At time: 99.60594964027405 and batch: 600, loss is 5.132204198837281 and perplexity is 169.3900762659997
At time: 100.74105644226074 and batch: 650, loss is 5.133277959823609 and perplexity is 169.57205840654174
At time: 101.8774962425232 and batch: 700, loss is 5.11602900505066 and perplexity is 166.6721993079246
At time: 103.01349258422852 and batch: 750, loss is 5.115290937423706 and perplexity is 166.54922933896106
At time: 104.14950466156006 and batch: 800, loss is 5.110249938964844 and perplexity is 165.7117675258343
At time: 105.28486347198486 and batch: 850, loss is 5.128119316101074 and perplexity is 168.69954898944474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.081463813781738 and perplexity of 161.0095715408668
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 108.37797594070435 and batch: 50, loss is 5.12448600769043 and perplexity is 168.08772364736524
At time: 109.51195430755615 and batch: 100, loss is 5.022956428527832 and perplexity is 151.85960284755242
At time: 110.6459608078003 and batch: 150, loss is 5.000857925415039 and perplexity is 148.54054115803513
At time: 111.78066897392273 and batch: 200, loss is 4.991559438705444 and perplexity is 147.1657405982442
At time: 112.91320729255676 and batch: 250, loss is 4.9902808952331545 and perplexity is 146.97770303397286
At time: 114.0461175441742 and batch: 300, loss is 4.948353853225708 and perplexity is 140.94276038299503
At time: 115.1971116065979 and batch: 350, loss is 4.897078304290772 and perplexity is 133.8979984233493
At time: 116.34718823432922 and batch: 400, loss is 4.8922696208953855 and perplexity is 133.2556709541077
At time: 117.52811670303345 and batch: 450, loss is 4.904919443130493 and perplexity is 134.95203825518388
At time: 118.66032481193542 and batch: 500, loss is 4.900769910812378 and perplexity is 134.39321064950659
At time: 119.79153609275818 and batch: 550, loss is 4.873863906860351 and perplexity is 130.8254388665532
At time: 120.92137789726257 and batch: 600, loss is 4.882195968627929 and perplexity is 131.92003830648477
At time: 122.07720494270325 and batch: 650, loss is 4.841378507614135 and perplexity is 126.64381091345899
At time: 123.21842455863953 and batch: 700, loss is 4.788769388198853 and perplexity is 120.15341543441082
At time: 124.34249711036682 and batch: 750, loss is 4.758638715744018 and perplexity is 116.58710962968223
At time: 125.47829627990723 and batch: 800, loss is 4.713952703475952 and perplexity is 111.49198484029736
At time: 126.62322497367859 and batch: 850, loss is 4.735651216506958 and perplexity is 113.93763263108363
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.88886833190918 and perplexity of 132.80319983686684
Finished 6 epochs...
Completing Train Step...
At time: 129.7284061908722 and batch: 50, loss is 4.956156826019287 and perplexity is 142.04683483726035
At time: 130.8647186756134 and batch: 100, loss is 4.8811565780639645 and perplexity is 131.7829930975575
At time: 132.00040864944458 and batch: 150, loss is 4.875938348770141 and perplexity is 131.09711032571107
At time: 133.13345336914062 and batch: 200, loss is 4.881293478012085 and perplexity is 131.8010354174439
At time: 134.26977396011353 and batch: 250, loss is 4.88278151512146 and perplexity is 131.9973062421182
At time: 135.40890979766846 and batch: 300, loss is 4.85506459236145 and perplexity is 128.38898389454926
At time: 136.54653120040894 and batch: 350, loss is 4.8031731224060055 and perplexity is 121.8965973209908
At time: 137.68437385559082 and batch: 400, loss is 4.8082470703125 and perplexity is 122.51666607374891
At time: 138.8214557170868 and batch: 450, loss is 4.829074726104737 and perplexity is 125.0951598034846
At time: 139.9590346813202 and batch: 500, loss is 4.829690895080566 and perplexity is 125.17226331193744
At time: 141.0954225063324 and batch: 550, loss is 4.811828441619873 and perplexity is 122.95623039800076
At time: 142.23279309272766 and batch: 600, loss is 4.826982679367066 and perplexity is 124.8337284411833
At time: 143.36934185028076 and batch: 650, loss is 4.790297031402588 and perplexity is 120.33710725496199
At time: 144.50690245628357 and batch: 700, loss is 4.748220224380493 and perplexity is 115.37875338949533
At time: 145.64273881912231 and batch: 750, loss is 4.734072732925415 and perplexity is 113.75792581809574
At time: 146.8181254863739 and batch: 800, loss is 4.705294523239136 and perplexity is 110.53083405327237
At time: 147.94759106636047 and batch: 850, loss is 4.738201885223389 and perplexity is 114.22862073562561
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.878285090128581 and perplexity of 131.40512260770794
Finished 7 epochs...
Completing Train Step...
At time: 151.02335405349731 and batch: 50, loss is 4.90007022857666 and perplexity is 134.29921099619267
At time: 152.18265867233276 and batch: 100, loss is 4.827467250823974 and perplexity is 124.89423396128544
At time: 153.31410384178162 and batch: 150, loss is 4.823522834777832 and perplexity is 124.40256944294565
At time: 154.4458019733429 and batch: 200, loss is 4.832044448852539 and perplexity is 125.46720991459848
At time: 155.580796957016 and batch: 250, loss is 4.834017114639282 and perplexity is 125.71495906975507
At time: 156.73010182380676 and batch: 300, loss is 4.808803968429565 and perplexity is 122.58491437630617
At time: 157.8605227470398 and batch: 350, loss is 4.757810382843018 and perplexity is 116.49057667717328
At time: 158.9970474243164 and batch: 400, loss is 4.767224397659302 and perplexity is 117.59239883976934
At time: 160.13305592536926 and batch: 450, loss is 4.792706546783447 and perplexity is 120.62741097096242
At time: 161.2672884464264 and batch: 500, loss is 4.795088586807251 and perplexity is 120.91509279063908
At time: 162.4004089832306 and batch: 550, loss is 4.778692178726196 and perplexity is 118.94868465546399
At time: 163.53110456466675 and batch: 600, loss is 4.798890428543091 and perplexity is 121.37566779878527
At time: 164.6655569076538 and batch: 650, loss is 4.763093156814575 and perplexity is 117.10759842181054
At time: 165.7978630065918 and batch: 700, loss is 4.7254933166503905 and perplexity is 112.78612392530113
At time: 166.93002700805664 and batch: 750, loss is 4.7178194713592525 and perplexity is 111.92393305005209
At time: 168.06601738929749 and batch: 800, loss is 4.695211801528931 and perplexity is 109.42198193072872
At time: 169.20534563064575 and batch: 850, loss is 4.728529691696167 and perplexity is 113.12910534409654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.873022397359212 and perplexity of 130.7153943250138
Finished 8 epochs...
Completing Train Step...
At time: 172.30007410049438 and batch: 50, loss is 4.859915714263916 and perplexity is 129.0133276658176
At time: 173.46681237220764 and batch: 100, loss is 4.790482149124146 and perplexity is 120.3593858480935
At time: 174.61204075813293 and batch: 150, loss is 4.788553228378296 and perplexity is 120.12744590057724
At time: 175.77840304374695 and batch: 200, loss is 4.797849044799805 and perplexity is 121.24933494341718
At time: 176.91608500480652 and batch: 250, loss is 4.799750080108643 and perplexity is 121.48005344284115
At time: 178.05430150032043 and batch: 300, loss is 4.7756758880615235 and perplexity is 118.59044140286024
At time: 179.1934473514557 and batch: 350, loss is 4.726359004974365 and perplexity is 112.88380382996587
At time: 180.3332817554474 and batch: 400, loss is 4.737530136108399 and perplexity is 114.15191352762395
At time: 181.47245573997498 and batch: 450, loss is 4.765651426315308 and perplexity is 117.4075747657172
At time: 182.61306643486023 and batch: 500, loss is 4.769923753738404 and perplexity is 117.91025140205348
At time: 183.75466871261597 and batch: 550, loss is 4.755579538345337 and perplexity is 116.23099396714628
At time: 184.90382385253906 and batch: 600, loss is 4.778168058395385 and perplexity is 118.88635756638695
At time: 186.05708265304565 and batch: 650, loss is 4.7429423427581785 and perplexity is 114.77140216118421
At time: 187.19838738441467 and batch: 700, loss is 4.707809944152832 and perplexity is 110.80921560145775
At time: 188.337420463562 and batch: 750, loss is 4.704158039093017 and perplexity is 110.4052888662837
At time: 189.47688388824463 and batch: 800, loss is 4.683315181732178 and perplexity is 108.12794282196101
At time: 190.6156985759735 and batch: 850, loss is 4.7170861625671385 and perplexity is 111.84188833163176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.870494842529297 and perplexity of 130.38542118701304
Finished 9 epochs...
Completing Train Step...
At time: 193.6910297870636 and batch: 50, loss is 4.828842868804932 and perplexity is 125.06615893967482
At time: 194.847336769104 and batch: 100, loss is 4.76126877784729 and perplexity is 116.8941445519201
At time: 195.98337864875793 and batch: 150, loss is 4.761755771636963 and perplexity is 116.95108513809663
At time: 197.13084435462952 and batch: 200, loss is 4.771027164459229 and perplexity is 118.04042664271621
At time: 198.2718584537506 and batch: 250, loss is 4.773015069961548 and perplexity is 118.27531324516863
At time: 199.4065752029419 and batch: 300, loss is 4.750518379211425 and perplexity is 115.6442165498693
At time: 200.54186582565308 and batch: 350, loss is 4.7008395004272465 and perplexity is 110.03951190445333
At time: 201.68033623695374 and batch: 400, loss is 4.714634685516358 and perplexity is 111.56804630493603
At time: 202.8175904750824 and batch: 450, loss is 4.74402081489563 and perplexity is 114.8952466900245
At time: 203.95335125923157 and batch: 500, loss is 4.750138530731201 and perplexity is 115.6002976117651
At time: 205.13154983520508 and batch: 550, loss is 4.734861450195313 and perplexity is 113.84768405105741
At time: 206.26806497573853 and batch: 600, loss is 4.759288368225097 and perplexity is 116.662875342742
At time: 207.40546011924744 and batch: 650, loss is 4.725247631072998 and perplexity is 112.75841740500825
At time: 208.57885193824768 and batch: 700, loss is 4.691972351074218 and perplexity is 109.0680883614307
At time: 209.72722792625427 and batch: 750, loss is 4.691523666381836 and perplexity is 109.01916215679587
At time: 210.86151552200317 and batch: 800, loss is 4.671526422500611 and perplexity is 106.86073263157094
At time: 212.0297610759735 and batch: 850, loss is 4.704188089370728 and perplexity is 110.40860662572439
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.86970329284668 and perplexity of 130.28225548403083
Finished 10 epochs...
Completing Train Step...
At time: 215.13604760169983 and batch: 50, loss is 4.802179403305054 and perplexity is 121.77552650904403
At time: 216.27258396148682 and batch: 100, loss is 4.737141885757446 and perplexity is 114.10760260955631
At time: 217.4121115207672 and batch: 150, loss is 4.739564847946167 and perplexity is 114.38441623517376
At time: 218.54667162895203 and batch: 200, loss is 4.749316034317016 and perplexity is 115.50525587260354
At time: 219.68406438827515 and batch: 250, loss is 4.750357894897461 and perplexity is 115.62565895625353
At time: 220.82319808006287 and batch: 300, loss is 4.728311901092529 and perplexity is 113.104469570772
At time: 221.9615466594696 and batch: 350, loss is 4.679277219772339 and perplexity is 107.69220663718094
At time: 223.09598064422607 and batch: 400, loss is 4.6933584308624265 and perplexity is 109.21937025441363
At time: 224.22575974464417 and batch: 450, loss is 4.725123128890991 and perplexity is 112.7443796098879
At time: 225.36376929283142 and batch: 500, loss is 4.732876663208008 and perplexity is 113.6219447455422
At time: 226.50129890441895 and batch: 550, loss is 4.717514142990113 and perplexity is 111.8897647146527
At time: 227.6610713005066 and batch: 600, loss is 4.743730411529541 and perplexity is 114.86188556796367
At time: 228.80749702453613 and batch: 650, loss is 4.7092437839508055 and perplexity is 110.9682122253729
At time: 229.94240164756775 and batch: 700, loss is 4.677612400054931 and perplexity is 107.5130676865643
At time: 231.08939623832703 and batch: 750, loss is 4.678287343978882 and perplexity is 107.58565747260646
At time: 232.24295949935913 and batch: 800, loss is 4.659415559768677 and perplexity is 105.57436221793223
At time: 233.45544910430908 and batch: 850, loss is 4.6927081489562985 and perplexity is 109.14836996174328
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.868114789326985 and perplexity of 130.0754659490762
Finished 11 epochs...
Completing Train Step...
At time: 236.5680537223816 and batch: 50, loss is 4.778868703842163 and perplexity is 118.96968393921293
At time: 237.7031934261322 and batch: 100, loss is 4.715309534072876 and perplexity is 111.6433632508462
At time: 238.84229445457458 and batch: 150, loss is 4.718619518280029 and perplexity is 112.01351327746436
At time: 239.97908973693848 and batch: 200, loss is 4.7291647148132325 and perplexity is 113.20096775593447
At time: 241.1166696548462 and batch: 250, loss is 4.729919977188111 and perplexity is 113.28649648192905
At time: 242.25188660621643 and batch: 300, loss is 4.709148187637329 and perplexity is 110.95760458040499
At time: 243.39734768867493 and batch: 350, loss is 4.6597472763061525 and perplexity is 105.6093887889386
At time: 244.5451717376709 and batch: 400, loss is 4.675444660186767 and perplexity is 107.280259748082
At time: 245.67891836166382 and batch: 450, loss is 4.70747223854065 and perplexity is 110.77180102537939
At time: 246.81391096115112 and batch: 500, loss is 4.7167910289764405 and perplexity is 111.80888490398762
At time: 247.95131039619446 and batch: 550, loss is 4.702461757659912 and perplexity is 110.21816917336136
At time: 249.0902464389801 and batch: 600, loss is 4.728797645568847 and perplexity is 113.15942278764317
At time: 250.228187084198 and batch: 650, loss is 4.69391056060791 and perplexity is 109.27969016819102
At time: 251.36606192588806 and batch: 700, loss is 4.663785457611084 and perplexity is 106.03672088991664
At time: 252.50358772277832 and batch: 750, loss is 4.665973472595215 and perplexity is 106.26898482992048
At time: 253.64267945289612 and batch: 800, loss is 4.647420387268067 and perplexity is 104.31554449299007
At time: 254.78127813339233 and batch: 850, loss is 4.680184030532837 and perplexity is 107.78990738032897
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.867412249247233 and perplexity of 129.9841148135289
Finished 12 epochs...
Completing Train Step...
At time: 257.913437128067 and batch: 50, loss is 4.758008403778076 and perplexity is 116.51364653417431
At time: 259.04133319854736 and batch: 100, loss is 4.696184730529785 and perplexity is 109.52849355600272
At time: 260.176171541214 and batch: 150, loss is 4.7007413101196285 and perplexity is 110.02870762137593
At time: 261.32039642333984 and batch: 200, loss is 4.711112337112427 and perplexity is 111.17575607213516
At time: 262.48367381095886 and batch: 250, loss is 4.711951408386231 and perplexity is 111.26907960247014
At time: 263.6619589328766 and batch: 300, loss is 4.6918919563293455 and perplexity is 109.05932021275437
At time: 264.79567289352417 and batch: 350, loss is 4.6419821453094485 and perplexity is 103.74979106831937
At time: 265.9269518852234 and batch: 400, loss is 4.658777151107788 and perplexity is 105.50698414038706
At time: 267.05529022216797 and batch: 450, loss is 4.691985864639282 and perplexity is 109.06956227009798
At time: 268.18351554870605 and batch: 500, loss is 4.702263565063476 and perplexity is 110.19632691279715
At time: 269.3095817565918 and batch: 550, loss is 4.687523956298828 and perplexity is 108.58398798009546
At time: 270.444500207901 and batch: 600, loss is 4.7146682453155515 and perplexity is 111.5717905689945
At time: 271.57602405548096 and batch: 650, loss is 4.680302114486694 and perplexity is 107.80263639030942
At time: 272.7132227420807 and batch: 700, loss is 4.650385131835938 and perplexity is 104.62527234198092
At time: 273.83805441856384 and batch: 750, loss is 4.6536009311676025 and perplexity is 104.9622677873014
At time: 274.9660701751709 and batch: 800, loss is 4.634973049163818 and perplexity is 103.02514134410598
At time: 276.0957887172699 and batch: 850, loss is 4.668224201202393 and perplexity is 106.50843684379757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.867182731628418 and perplexity of 129.95428459242467
Finished 13 epochs...
Completing Train Step...
At time: 279.16747093200684 and batch: 50, loss is 4.7388263511657716 and perplexity is 114.29997489572301
At time: 280.3218340873718 and batch: 100, loss is 4.67809949874878 and perplexity is 107.5654499180286
At time: 281.4503827095032 and batch: 150, loss is 4.683667287826538 and perplexity is 108.16602203316701
At time: 282.58355593681335 and batch: 200, loss is 4.694435110092163 and perplexity is 109.33702781020867
At time: 283.7140738964081 and batch: 250, loss is 4.695143232345581 and perplexity is 109.41447921201778
At time: 284.84843730926514 and batch: 300, loss is 4.675942211151123 and perplexity is 107.33365042596624
At time: 285.980277299881 and batch: 350, loss is 4.626071004867554 and perplexity is 102.11207707127906
At time: 287.1153087615967 and batch: 400, loss is 4.642841014862061 and perplexity is 103.83893688179036
At time: 288.2479112148285 and batch: 450, loss is 4.677380790710449 and perplexity is 107.48816953886731
At time: 289.3879232406616 and batch: 500, loss is 4.687711229324341 and perplexity is 108.60432473625022
At time: 290.52280831336975 and batch: 550, loss is 4.673328075408936 and perplexity is 107.05343211797296
At time: 291.74676418304443 and batch: 600, loss is 4.70180606842041 and perplexity is 110.14592399361861
At time: 292.9016914367676 and batch: 650, loss is 4.666847534179688 and perplexity is 106.36191107289469
At time: 294.04054737091064 and batch: 700, loss is 4.638190479278564 and perplexity is 103.35715135954352
At time: 295.1796271800995 and batch: 750, loss is 4.641715049743652 and perplexity is 103.72208365960046
At time: 296.3169651031494 and batch: 800, loss is 4.624286212921143 and perplexity is 101.92999079988232
At time: 297.4560477733612 and batch: 850, loss is 4.657443046569824 and perplexity is 105.36632064482549
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.867594401041667 and perplexity of 130.00779380981038
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 300.5452847480774 and batch: 50, loss is 4.7295349597930905 and perplexity is 113.24288760578928
At time: 301.7108209133148 and batch: 100, loss is 4.674883680343628 and perplexity is 107.22009456210272
At time: 302.84702944755554 and batch: 150, loss is 4.677757158279419 and perplexity is 107.52863221387128
At time: 303.9844193458557 and batch: 200, loss is 4.68356987953186 and perplexity is 108.15548627856191
At time: 305.12147760391235 and batch: 250, loss is 4.677081727981568 and perplexity is 107.45602863987455
At time: 306.26059222221375 and batch: 300, loss is 4.6480491352081295 and perplexity is 104.38115330024635
At time: 307.39325737953186 and batch: 350, loss is 4.595255203247071 and perplexity is 99.01340086503953
At time: 308.5260765552521 and batch: 400, loss is 4.606982669830322 and perplexity is 100.18141273838843
At time: 309.66012048721313 and batch: 450, loss is 4.642095756530762 and perplexity is 103.76157887839472
At time: 310.79446625709534 and batch: 500, loss is 4.64306149482727 and perplexity is 103.86183381104577
At time: 311.9296383857727 and batch: 550, loss is 4.620294857025146 and perplexity is 101.52396277034244
At time: 313.06476497650146 and batch: 600, loss is 4.633184795379639 and perplexity is 102.84107087663922
At time: 314.19640278816223 and batch: 650, loss is 4.591257581710815 and perplexity is 98.61837287364413
At time: 315.33410930633545 and batch: 700, loss is 4.5604221057891845 and perplexity is 95.62383468666306
At time: 316.48759722709656 and batch: 750, loss is 4.5490507221221925 and perplexity is 94.54261848767808
At time: 317.6180262565613 and batch: 800, loss is 4.519770612716675 and perplexity is 91.8145344757662
At time: 318.7679944038391 and batch: 850, loss is 4.568277702331543 and perplexity is 96.37797518535321
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.843245188395183 and perplexity of 126.88043526368344
Finished 15 epochs...
Completing Train Step...
At time: 321.9545478820801 and batch: 50, loss is 4.703086309432983 and perplexity is 110.28702762693959
At time: 323.1173896789551 and batch: 100, loss is 4.649297323226929 and perplexity is 104.51152195055192
At time: 324.25565123558044 and batch: 150, loss is 4.652721748352051 and perplexity is 104.87002731924024
At time: 325.3942451477051 and batch: 200, loss is 4.66136830329895 and perplexity is 105.78072329026566
At time: 326.5327272415161 and batch: 250, loss is 4.656982822418213 and perplexity is 105.31783967621035
At time: 327.68626046180725 and batch: 300, loss is 4.630166854858398 and perplexity is 102.53117050724748
At time: 328.82253861427307 and batch: 350, loss is 4.578522624969483 and perplexity is 97.37043524281027
At time: 329.95990324020386 and batch: 400, loss is 4.591718225479126 and perplexity is 98.66381127720459
At time: 331.11588072776794 and batch: 450, loss is 4.6289589309692385 and perplexity is 102.40739542749593
At time: 332.2618901729584 and batch: 500, loss is 4.631275615692139 and perplexity is 102.64491610001984
At time: 333.43254828453064 and batch: 550, loss is 4.6104365253448485 and perplexity is 100.52802308970166
At time: 334.5788457393646 and batch: 600, loss is 4.626948852539062 and perplexity is 102.2017552765118
At time: 335.7178165912628 and batch: 650, loss is 4.587032127380371 and perplexity is 98.20254459332679
At time: 336.8544075489044 and batch: 700, loss is 4.558963966369629 and perplexity is 95.48450341075711
At time: 337.99340081214905 and batch: 750, loss is 4.550947780609131 and perplexity is 94.72214159353386
At time: 339.1321623325348 and batch: 800, loss is 4.525778760910034 and perplexity is 92.36783028240359
At time: 340.2911353111267 and batch: 850, loss is 4.57512939453125 and perplexity is 97.04059484687815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.841078122456868 and perplexity of 126.605774705443
Finished 16 epochs...
Completing Train Step...
At time: 343.41779685020447 and batch: 50, loss is 4.693072357177734 and perplexity is 109.18812993545482
At time: 344.5547606945038 and batch: 100, loss is 4.639163694381714 and perplexity is 103.45778906338859
At time: 345.7024521827698 and batch: 150, loss is 4.641931114196777 and perplexity is 103.74449673613069
At time: 346.83649706840515 and batch: 200, loss is 4.651665458679199 and perplexity is 104.75931267605246
At time: 347.96909832954407 and batch: 250, loss is 4.647464666366577 and perplexity is 104.3201635935249
At time: 349.1056079864502 and batch: 300, loss is 4.62138897895813 and perplexity is 101.63510315421152
At time: 350.26709938049316 and batch: 350, loss is 4.570207204818725 and perplexity is 96.56411625025683
At time: 351.41135120391846 and batch: 400, loss is 4.5842124938964846 and perplexity is 97.92603941521028
At time: 352.544397354126 and batch: 450, loss is 4.622375288009644 and perplexity is 101.73539622825692
At time: 353.6791989803314 and batch: 500, loss is 4.625333070755005 and perplexity is 102.03675288186749
At time: 354.8146686553955 and batch: 550, loss is 4.605434169769287 and perplexity is 100.02640186279805
At time: 355.96744561195374 and batch: 600, loss is 4.623996667861938 and perplexity is 101.90048174696786
At time: 357.0995464324951 and batch: 650, loss is 4.585244941711426 and perplexity is 98.0271951506538
At time: 358.2375953197479 and batch: 700, loss is 4.558710556030274 and perplexity is 95.46030971594031
At time: 359.3924238681793 and batch: 750, loss is 4.552369003295898 and perplexity is 94.85685855883354
At time: 360.5269253253937 and batch: 800, loss is 4.528759822845459 and perplexity is 92.64359533744984
At time: 361.6878046989441 and batch: 850, loss is 4.577878551483154 and perplexity is 97.30774171890364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.840290387471517 and perplexity of 126.5060821781609
Finished 17 epochs...
Completing Train Step...
At time: 364.7919292449951 and batch: 50, loss is 4.6860418701171875 and perplexity is 108.4231763497466
At time: 365.9310448169708 and batch: 100, loss is 4.631934032440186 and perplexity is 102.71252148569877
At time: 367.0671064853668 and batch: 150, loss is 4.634205160140991 and perplexity is 102.94605983578954
At time: 368.22467589378357 and batch: 200, loss is 4.644851722717285 and perplexity is 104.04793669622647
At time: 369.3812212944031 and batch: 250, loss is 4.640725002288819 and perplexity is 103.6194446917715
At time: 370.52023935317993 and batch: 300, loss is 4.615291624069214 and perplexity is 101.01728330863077
At time: 371.65853548049927 and batch: 350, loss is 4.564469928741455 and perplexity is 96.01168748965455
At time: 372.79560112953186 and batch: 400, loss is 4.579230546951294 and perplexity is 97.43939031882445
At time: 373.93110966682434 and batch: 450, loss is 4.618066082000732 and perplexity is 101.2979406675118
At time: 375.06644010543823 and batch: 500, loss is 4.621469993591308 and perplexity is 101.643337418355
At time: 376.2058491706848 and batch: 550, loss is 4.602085084915161 and perplexity is 99.6919652961236
At time: 377.3430483341217 and batch: 600, loss is 4.6218712043762205 and perplexity is 101.6841260034047
At time: 378.52097964286804 and batch: 650, loss is 4.584159345626831 and perplexity is 97.92083495396659
At time: 379.6642060279846 and batch: 700, loss is 4.5584829139709475 and perplexity is 95.4385814076849
At time: 380.8030800819397 and batch: 750, loss is 4.553109836578369 and perplexity is 94.9271577134861
At time: 381.96532917022705 and batch: 800, loss is 4.530089721679688 and perplexity is 92.7668839093809
At time: 383.0979106426239 and batch: 850, loss is 4.579168634414673 and perplexity is 97.43335778574965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.839842160542806 and perplexity of 126.44939145158719
Finished 18 epochs...
Completing Train Step...
At time: 386.22713923454285 and batch: 50, loss is 4.6803480052948 and perplexity is 107.80758365392538
At time: 387.35946464538574 and batch: 100, loss is 4.626316232681274 and perplexity is 102.13712086328538
At time: 388.49778604507446 and batch: 150, loss is 4.628252267837524 and perplexity is 102.33505346044342
At time: 389.6494162082672 and batch: 200, loss is 4.639669075012207 and perplexity is 103.51008784033614
At time: 390.7967493534088 and batch: 250, loss is 4.635678396224976 and perplexity is 103.09783545905469
At time: 391.93797993659973 and batch: 300, loss is 4.6105009460449216 and perplexity is 100.53449938392754
At time: 393.07598638534546 and batch: 350, loss is 4.55995882987976 and perplexity is 95.57954472771364
At time: 394.21230244636536 and batch: 400, loss is 4.575327978134156 and perplexity is 97.05986743137726
At time: 395.35056233406067 and batch: 450, loss is 4.61471022605896 and perplexity is 100.95856913092076
At time: 396.4887444972992 and batch: 500, loss is 4.618450765609741 and perplexity is 101.33691582098365
At time: 397.6266255378723 and batch: 550, loss is 4.599603176116943 and perplexity is 99.44484572131907
At time: 398.7657222747803 and batch: 600, loss is 4.62008207321167 and perplexity is 101.50236241256965
At time: 399.90402936935425 and batch: 650, loss is 4.583183851242065 and perplexity is 97.82536030438078
At time: 401.0419912338257 and batch: 700, loss is 4.557935562133789 and perplexity is 95.38635721862234
At time: 402.1805567741394 and batch: 750, loss is 4.553297033309937 and perplexity is 94.94492943049882
At time: 403.32539415359497 and batch: 800, loss is 4.530589065551758 and perplexity is 92.81321805176434
At time: 404.45952677726746 and batch: 850, loss is 4.579510221481323 and perplexity is 97.46664544562292
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.839554786682129 and perplexity of 126.41305842261697
Finished 19 epochs...
Completing Train Step...
At time: 407.5094439983368 and batch: 50, loss is 4.675458354949951 and perplexity is 107.28172893589364
At time: 408.6696050167084 and batch: 100, loss is 4.621549711227417 and perplexity is 101.65144050791552
At time: 409.8014461994171 and batch: 150, loss is 4.623322067260742 and perplexity is 101.83176280224595
At time: 410.93498611450195 and batch: 200, loss is 4.635340394973755 and perplexity is 103.06299415020608
At time: 412.090047121048 and batch: 250, loss is 4.631202383041382 and perplexity is 102.63739941596383
At time: 413.22944831848145 and batch: 300, loss is 4.60641918182373 and perplexity is 100.12497761557759
At time: 414.3653042316437 and batch: 350, loss is 4.556184167861939 and perplexity is 95.21944430680126
At time: 415.5015957355499 and batch: 400, loss is 4.572097997665406 and perplexity is 96.74687171242171
At time: 416.65670680999756 and batch: 450, loss is 4.611814842224121 and perplexity is 100.6666780940817
At time: 417.7893099784851 and batch: 500, loss is 4.616018953323365 and perplexity is 101.0907828599027
At time: 418.9420986175537 and batch: 550, loss is 4.59735915184021 and perplexity is 99.22193927060762
At time: 420.07790064811707 and batch: 600, loss is 4.618410568237305 and perplexity is 101.33284242510734
At time: 421.2139744758606 and batch: 650, loss is 4.582166242599487 and perplexity is 97.72586300551336
At time: 422.36691212654114 and batch: 700, loss is 4.557265558242798 and perplexity is 95.32246939307421
At time: 423.51961970329285 and batch: 750, loss is 4.5530571937561035 and perplexity is 94.92216061152631
At time: 424.6675012111664 and batch: 800, loss is 4.530653238296509 and perplexity is 92.81917432182895
At time: 425.8028085231781 and batch: 850, loss is 4.579378604888916 and perplexity is 97.4538180620428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.839345296223958 and perplexity of 126.38657886679322
Finished 20 epochs...
Completing Train Step...
At time: 428.90034079551697 and batch: 50, loss is 4.671058702468872 and perplexity is 106.81076341302607
At time: 430.08020424842834 and batch: 100, loss is 4.617458915710449 and perplexity is 101.23645464067367
At time: 431.23408913612366 and batch: 150, loss is 4.619280014038086 and perplexity is 101.42098415110937
At time: 432.3721454143524 and batch: 200, loss is 4.631578722000122 and perplexity is 102.67603313721908
At time: 433.50863218307495 and batch: 250, loss is 4.627360925674439 and perplexity is 102.24387855258875
At time: 434.6643261909485 and batch: 300, loss is 4.602688503265381 and perplexity is 99.75213941060939
At time: 435.8003947734833 and batch: 350, loss is 4.552668905258178 and perplexity is 94.88531058304675
At time: 436.96322083473206 and batch: 400, loss is 4.569274778366089 and perplexity is 96.47411927818875
At time: 438.0991401672363 and batch: 450, loss is 4.609184684753418 and perplexity is 100.4022567659404
At time: 439.23261976242065 and batch: 500, loss is 4.613699293136596 and perplexity is 100.85655836130213
At time: 440.3712923526764 and batch: 550, loss is 4.595226621627807 and perplexity is 99.01057094215611
At time: 441.5124318599701 and batch: 600, loss is 4.61656421661377 and perplexity is 101.14591898328051
At time: 442.66882038116455 and batch: 650, loss is 4.581017627716064 and perplexity is 97.61367806574957
At time: 443.835086107254 and batch: 700, loss is 4.556340026855469 and perplexity is 95.2342862701521
At time: 444.97152757644653 and batch: 750, loss is 4.552370176315308 and perplexity is 94.85696982783502
At time: 446.10917925834656 and batch: 800, loss is 4.530177240371704 and perplexity is 92.7750031010084
At time: 447.24674677848816 and batch: 850, loss is 4.578682975769043 and perplexity is 97.38604992183465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.838986714680989 and perplexity of 126.34126709680034
Finished 21 epochs...
Completing Train Step...
At time: 450.36782026290894 and batch: 50, loss is 4.666950025558472 and perplexity is 106.37281281046815
At time: 451.53013586997986 and batch: 100, loss is 4.61351411819458 and perplexity is 100.83788398302241
At time: 452.66583275794983 and batch: 150, loss is 4.615292491912842 and perplexity is 101.01737097587446
At time: 453.79783177375793 and batch: 200, loss is 4.628081502914429 and perplexity is 102.31757971490316
At time: 454.9366512298584 and batch: 250, loss is 4.623813972473145 and perplexity is 101.88186669933033
At time: 456.0755066871643 and batch: 300, loss is 4.5991632270812985 and perplexity is 99.40110467996413
At time: 457.2337019443512 and batch: 350, loss is 4.549585409164429 and perplexity is 94.59318271753716
At time: 458.38472843170166 and batch: 400, loss is 4.566568098068237 and perplexity is 96.21334775206546
At time: 459.5241439342499 and batch: 450, loss is 4.60682894706726 and perplexity is 100.16601375843203
At time: 460.660605430603 and batch: 500, loss is 4.611545085906982 and perplexity is 100.6395262840912
At time: 461.7993130683899 and batch: 550, loss is 4.593219289779663 and perplexity is 98.81202321206067
At time: 462.9391233921051 and batch: 600, loss is 4.61493595123291 and perplexity is 100.98136059370633
At time: 464.07704973220825 and batch: 650, loss is 4.579903469085694 and perplexity is 97.50498150773872
At time: 465.21369767189026 and batch: 700, loss is 4.555412673950196 and perplexity is 95.14601141539563
At time: 466.3769471645355 and batch: 750, loss is 4.55157205581665 and perplexity is 94.78129273950977
At time: 467.5134208202362 and batch: 800, loss is 4.529551105499268 and perplexity is 92.71693161846383
At time: 468.6517024040222 and batch: 850, loss is 4.577836008071899 and perplexity is 97.3036020036888
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.839021682739258 and perplexity of 126.34568508283364
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 471.81366777420044 and batch: 50, loss is 4.665085639953613 and perplexity is 106.1746776270999
At time: 472.95605754852295 and batch: 100, loss is 4.613615446090698 and perplexity is 100.84810219134145
At time: 474.0929524898529 and batch: 150, loss is 4.614472045898437 and perplexity is 100.93452566617086
At time: 475.2306263446808 and batch: 200, loss is 4.627280216217041 and perplexity is 102.23562683762877
At time: 476.3672251701355 and batch: 250, loss is 4.620421075820923 and perplexity is 101.53677781139832
At time: 477.5024859905243 and batch: 300, loss is 4.594531774520874 and perplexity is 98.94179762961683
At time: 478.6412446498871 and batch: 350, loss is 4.544294052124023 and perplexity is 94.09397831330543
At time: 479.7801237106323 and batch: 400, loss is 4.559427642822266 and perplexity is 95.52878759255316
At time: 480.92030692100525 and batch: 450, loss is 4.5989319896698 and perplexity is 99.37812208313832
At time: 482.0787672996521 and batch: 500, loss is 4.602539043426514 and perplexity is 99.73723158601473
At time: 483.24133491516113 and batch: 550, loss is 4.582323665618897 and perplexity is 97.74124851692714
At time: 484.391211271286 and batch: 600, loss is 4.6015581607818605 and perplexity is 99.63944903097178
At time: 485.5290513038635 and batch: 650, loss is 4.564868230819702 and perplexity is 96.04993676119413
At time: 486.6667640209198 and batch: 700, loss is 4.539473075866699 and perplexity is 93.64144518011899
At time: 487.8052453994751 and batch: 750, loss is 4.533504123687744 and perplexity is 93.0841687047576
At time: 488.9444670677185 and batch: 800, loss is 4.509577550888062 and perplexity is 90.88341678100333
At time: 490.09326457977295 and batch: 850, loss is 4.560289211273194 and perplexity is 95.61112764780178
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.837397575378418 and perplexity of 126.14065266807552
Finished 23 epochs...
Completing Train Step...
At time: 493.24410033226013 and batch: 50, loss is 4.662744598388672 and perplexity is 105.92640901059171
At time: 494.3772442340851 and batch: 100, loss is 4.610537300109863 and perplexity is 100.53815428808191
At time: 495.5396912097931 and batch: 150, loss is 4.611772327423096 and perplexity is 100.66239836126928
At time: 496.6759476661682 and batch: 200, loss is 4.624682321548462 and perplexity is 101.97037414618862
At time: 497.83150005340576 and batch: 250, loss is 4.618256759643555 and perplexity is 101.31725776167153
At time: 498.9638936519623 and batch: 300, loss is 4.592405595779419 and perplexity is 98.73165316436825
At time: 500.09742617607117 and batch: 350, loss is 4.5425732421875 and perplexity is 93.93219969550482
At time: 501.2348108291626 and batch: 400, loss is 4.55781361579895 and perplexity is 95.37472591117796
At time: 502.4096348285675 and batch: 450, loss is 4.597752828598022 and perplexity is 99.2610083317496
At time: 503.568710565567 and batch: 500, loss is 4.601524696350098 and perplexity is 99.63611470921973
At time: 504.7379050254822 and batch: 550, loss is 4.581790542602539 and perplexity is 97.68915429524336
At time: 505.897123336792 and batch: 600, loss is 4.601034498214721 and perplexity is 99.58728524060811
At time: 507.03462958335876 and batch: 650, loss is 4.564543628692627 and perplexity is 96.0187638070934
At time: 508.18813252449036 and batch: 700, loss is 4.539440250396728 and perplexity is 93.63837140612154
At time: 509.33494877815247 and batch: 750, loss is 4.5339938831329345 and perplexity is 93.12976872118632
At time: 510.48185300827026 and batch: 800, loss is 4.510484972000122 and perplexity is 90.96592374074375
At time: 511.61810851097107 and batch: 850, loss is 4.561363401412964 and perplexity is 95.71388736022726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.837241490681966 and perplexity of 126.12096557906017
Finished 24 epochs...
Completing Train Step...
At time: 514.7444450855255 and batch: 50, loss is 4.661213941574097 and perplexity is 105.76439605554486
At time: 515.8839237689972 and batch: 100, loss is 4.608974990844726 and perplexity is 100.38120523154403
At time: 517.0214021205902 and batch: 150, loss is 4.609943208694458 and perplexity is 100.4784431723855
At time: 518.1611866950989 and batch: 200, loss is 4.623369998931885 and perplexity is 101.83664388579083
At time: 519.2987687587738 and batch: 250, loss is 4.6168413162231445 and perplexity is 101.17395036148322
At time: 520.4371719360352 and batch: 300, loss is 4.591079425811768 and perplexity is 98.60080499371942
At time: 521.5767629146576 and batch: 350, loss is 4.5414386749267575 and perplexity is 93.82568773092027
At time: 522.7145416736603 and batch: 400, loss is 4.556800355911255 and perplexity is 95.27813547098565
At time: 523.8475620746613 and batch: 450, loss is 4.5970227146148686 and perplexity is 99.18856293150273
At time: 525.0158381462097 and batch: 500, loss is 4.600898475646972 and perplexity is 99.57374004360166
At time: 526.1522738933563 and batch: 550, loss is 4.581509809494019 and perplexity is 97.66173356442299
At time: 527.3088231086731 and batch: 600, loss is 4.600730524063111 and perplexity is 99.5570178805465
At time: 528.4537668228149 and batch: 650, loss is 4.564405603408813 and perplexity is 96.00551170455113
At time: 529.6111123561859 and batch: 700, loss is 4.53955189704895 and perplexity is 93.64882640043024
At time: 530.755925655365 and batch: 750, loss is 4.53454213142395 and perplexity is 93.18084095658402
At time: 531.9179630279541 and batch: 800, loss is 4.511232128143311 and perplexity is 91.03391488632556
At time: 533.071341753006 and batch: 850, loss is 4.562011795043945 and perplexity is 95.77596775928079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.837080001831055 and perplexity of 126.10060009369668
Finished 25 epochs...
Completing Train Step...
At time: 536.217104434967 and batch: 50, loss is 4.660112514495849 and perplexity is 105.64796841586254
At time: 537.3861937522888 and batch: 100, loss is 4.6079010391235355 and perplexity is 100.2734585311436
At time: 538.520339012146 and batch: 150, loss is 4.608729610443115 and perplexity is 100.3565766728965
At time: 539.6598372459412 and batch: 200, loss is 4.622323169708252 and perplexity is 101.73009409038447
At time: 540.7975747585297 and batch: 250, loss is 4.615687475204468 and perplexity is 101.05727903056246
At time: 541.9363009929657 and batch: 300, loss is 4.59001856803894 and perplexity is 98.49625902734012
At time: 543.0740694999695 and batch: 350, loss is 4.540582437515258 and perplexity is 93.74538505091095
At time: 544.2125887870789 and batch: 400, loss is 4.556093425750732 and perplexity is 95.2108042854098
At time: 545.3500752449036 and batch: 450, loss is 4.596432075500489 and perplexity is 99.1299955843219
At time: 546.4862856864929 and batch: 500, loss is 4.600471286773682 and perplexity is 99.531212334112
At time: 547.6258425712585 and batch: 550, loss is 4.581398906707764 and perplexity is 97.65090320662974
At time: 548.7652766704559 and batch: 600, loss is 4.600595760345459 and perplexity is 99.54360211069839
At time: 549.9030950069427 and batch: 650, loss is 4.564350481033325 and perplexity is 96.00021979853857
At time: 551.0421545505524 and batch: 700, loss is 4.5396913814544675 and perplexity is 93.66188986236153
At time: 552.1801905632019 and batch: 750, loss is 4.5350100040435795 and perplexity is 93.22444792119855
At time: 553.3664820194244 and batch: 800, loss is 4.51179349899292 and perplexity is 91.08503301924095
At time: 554.5142452716827 and batch: 850, loss is 4.562497787475586 and perplexity is 95.8225254671753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.837015151977539 and perplexity of 126.0924227534049
Finished 26 epochs...
Completing Train Step...
At time: 557.6657876968384 and batch: 50, loss is 4.659182329177856 and perplexity is 105.54974191827174
At time: 558.8732266426086 and batch: 100, loss is 4.606961736679077 and perplexity is 100.17931564767304
At time: 560.0317993164062 and batch: 150, loss is 4.607697401046753 and perplexity is 100.25304111584815
At time: 561.1691389083862 and batch: 200, loss is 4.6214331531524655 and perplexity is 101.63959290217431
At time: 562.329024553299 and batch: 250, loss is 4.614681892395019 and perplexity is 100.95570864527532
At time: 563.4641013145447 and batch: 300, loss is 4.589119005203247 and perplexity is 98.40769529355832
At time: 564.5963499546051 and batch: 350, loss is 4.539870481491089 and perplexity is 93.67866621254358
At time: 565.729789018631 and batch: 400, loss is 4.555511102676392 and perplexity is 95.1553769770145
At time: 566.8655042648315 and batch: 450, loss is 4.595923662185669 and perplexity is 99.07960938426166
At time: 568.0014142990112 and batch: 500, loss is 4.6001202011108395 and perplexity is 99.49627448590384
At time: 569.1426022052765 and batch: 550, loss is 4.581317996978759 and perplexity is 97.64300261813577
At time: 570.2870409488678 and batch: 600, loss is 4.600468559265137 and perplexity is 99.53094086225006
At time: 571.4213953018188 and batch: 650, loss is 4.5643166732788085 and perplexity is 95.99697430153586
At time: 572.5578939914703 and batch: 700, loss is 4.539796361923218 and perplexity is 93.67172304760047
At time: 573.6912114620209 and batch: 750, loss is 4.535399131774902 and perplexity is 93.2607311980787
At time: 574.8255379199982 and batch: 800, loss is 4.512230157852173 and perplexity is 91.12481479075355
At time: 575.9621624946594 and batch: 850, loss is 4.562876205444336 and perplexity is 95.8587932943891
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.836988766988118 and perplexity of 126.08909585005485
Finished 27 epochs...
Completing Train Step...
At time: 579.0833902359009 and batch: 50, loss is 4.658365383148193 and perplexity is 105.46354868808203
At time: 580.2497198581696 and batch: 100, loss is 4.60611891746521 and perplexity is 100.09491816651844
At time: 581.3851072788239 and batch: 150, loss is 4.606783485412597 and perplexity is 100.16146014921367
At time: 582.5660438537598 and batch: 200, loss is 4.620647554397583 and perplexity is 101.55977632055216
At time: 583.6991109848022 and batch: 250, loss is 4.613798761367798 and perplexity is 100.8665908837178
At time: 584.8369529247284 and batch: 300, loss is 4.588323011398315 and perplexity is 98.3293945453366
At time: 585.9743702411652 and batch: 350, loss is 4.5392430400848385 and perplexity is 93.61990677445942
At time: 587.1128613948822 and batch: 400, loss is 4.555001678466797 and perplexity is 95.10691486924283
At time: 588.2510621547699 and batch: 450, loss is 4.595466985702514 and perplexity is 99.03437238681836
At time: 589.3879895210266 and batch: 500, loss is 4.599820766448975 and perplexity is 99.46648631262472
At time: 590.5263335704803 and batch: 550, loss is 4.581242084503174 and perplexity is 97.63559057742016
At time: 591.6673564910889 and batch: 600, loss is 4.600340967178345 and perplexity is 99.51824231193957
At time: 592.807021856308 and batch: 650, loss is 4.564282550811767 and perplexity is 95.99369870383025
At time: 593.9409050941467 and batch: 700, loss is 4.539864978790283 and perplexity is 93.67815072828976
At time: 595.0780472755432 and batch: 750, loss is 4.535721054077149 and perplexity is 93.29075874038364
At time: 596.2152726650238 and batch: 800, loss is 4.512574100494385 and perplexity is 91.15616189081697
At time: 597.3544590473175 and batch: 850, loss is 4.563176965713501 and perplexity is 95.88762814683372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.83697509765625 and perplexity of 126.08737230813857
Finished 28 epochs...
Completing Train Step...
At time: 600.4829823970795 and batch: 50, loss is 4.657627334594727 and perplexity is 105.3857401852878
At time: 601.6198151111603 and batch: 100, loss is 4.605348472595215 and perplexity is 100.01783025011251
At time: 602.7550611495972 and batch: 150, loss is 4.605951833724975 and perplexity is 100.07819533030862
At time: 603.8935451507568 and batch: 200, loss is 4.619934787750244 and perplexity is 101.48741369117765
At time: 605.0307729244232 and batch: 250, loss is 4.612995557785034 and perplexity is 100.78560700416368
At time: 606.1688385009766 and batch: 300, loss is 4.587595329284668 and perplexity is 98.25786803111899
At time: 607.3064653873444 and batch: 350, loss is 4.538667078018189 and perplexity is 93.56600078486682
At time: 608.4604804515839 and batch: 400, loss is 4.554535331726075 and perplexity is 95.06257240975772
At time: 609.6072072982788 and batch: 450, loss is 4.595043621063232 and perplexity is 98.99245360956643
At time: 610.7469508647919 and batch: 500, loss is 4.59954683303833 and perplexity is 99.43924285040188
At time: 611.9095873832703 and batch: 550, loss is 4.581158018112182 and perplexity is 97.62738305068139
At time: 613.0473110675812 and batch: 600, loss is 4.6002086353302 and perplexity is 99.50507375033962
At time: 614.1855752468109 and batch: 650, loss is 4.5642369556427 and perplexity is 95.98932195468855
At time: 615.3252136707306 and batch: 700, loss is 4.539903478622437 and perplexity is 93.68175739079676
At time: 616.4630172252655 and batch: 750, loss is 4.5359865665435795 and perplexity is 93.31553188847629
At time: 617.5966386795044 and batch: 800, loss is 4.512847566604615 and perplexity is 91.18109342064157
At time: 618.7318065166473 and batch: 850, loss is 4.563418464660645 and perplexity is 95.91078770446714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.83696174621582 and perplexity of 126.08568887133644
Finished 29 epochs...
Completing Train Step...
At time: 621.8453917503357 and batch: 50, loss is 4.656944694519043 and perplexity is 105.31382420478967
At time: 622.9831056594849 and batch: 100, loss is 4.604633617401123 and perplexity is 99.94635753402123
At time: 624.1207783222198 and batch: 150, loss is 4.605182313919068 and perplexity is 100.001212800452
At time: 625.2601697444916 and batch: 200, loss is 4.619275321960449 and perplexity is 101.42050827709416
At time: 626.3986291885376 and batch: 250, loss is 4.61224407196045 and perplexity is 100.7098965004261
At time: 627.5366833209991 and batch: 300, loss is 4.5869170093536376 and perplexity is 98.19124036084597
At time: 628.6771192550659 and batch: 350, loss is 4.538123588562012 and perplexity is 93.51516246628009
At time: 629.8136298656464 and batch: 400, loss is 4.554095506668091 and perplexity is 95.02077070172871
At time: 630.9523239135742 and batch: 450, loss is 4.594640998840332 and perplexity is 98.95260507033584
At time: 632.1047122478485 and batch: 500, loss is 4.599286918640137 and perplexity is 99.41340051798223
At time: 633.2752134799957 and batch: 550, loss is 4.581064138412476 and perplexity is 97.6182182514785
At time: 634.4148044586182 and batch: 600, loss is 4.600065965652465 and perplexity is 99.49087840618134
At time: 635.5495841503143 and batch: 650, loss is 4.564176893234253 and perplexity is 95.98355677796363
At time: 636.6890008449554 and batch: 700, loss is 4.539917993545532 and perplexity is 93.68311718416936
At time: 637.8299236297607 and batch: 750, loss is 4.536204948425293 and perplexity is 93.33591253522415
At time: 638.9701502323151 and batch: 800, loss is 4.513067264556884 and perplexity is 91.20112792084055
At time: 640.1088824272156 and batch: 850, loss is 4.563612279891967 and perplexity is 95.92937847750173
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8369490305582685 and perplexity of 126.0840856190878
Finished 30 epochs...
Completing Train Step...
At time: 643.2684738636017 and batch: 50, loss is 4.6563016796112064 and perplexity is 105.24612761311603
At time: 644.4272708892822 and batch: 100, loss is 4.603962049484253 and perplexity is 99.87925929992585
At time: 645.5771477222443 and batch: 150, loss is 4.604461946487427 and perplexity is 99.92920112417319
At time: 646.7109706401825 and batch: 200, loss is 4.618654432296753 and perplexity is 101.35755687677732
At time: 647.8677976131439 and batch: 250, loss is 4.611530466079712 and perplexity is 100.63805496235561
At time: 649.0030059814453 and batch: 300, loss is 4.5862766456604005 and perplexity is 98.12838238365617
At time: 650.1383004188538 and batch: 350, loss is 4.53760142326355 and perplexity is 93.46634484010453
At time: 651.2726979255676 and batch: 400, loss is 4.553672828674316 and perplexity is 94.98061599985384
At time: 652.4060728549957 and batch: 450, loss is 4.594252014160157 and perplexity is 98.91412150814331
At time: 653.542998790741 and batch: 500, loss is 4.599037446975708 and perplexity is 99.388602784783
At time: 654.6942024230957 and batch: 550, loss is 4.580958948135376 and perplexity is 97.60795030410425
At time: 655.82981300354 and batch: 600, loss is 4.599914464950562 and perplexity is 99.47580660999246
At time: 656.9633662700653 and batch: 650, loss is 4.564103374481201 and perplexity is 95.97650044594539
At time: 658.0974893569946 and batch: 700, loss is 4.53991364479065 and perplexity is 93.68270978014195
At time: 659.2359247207642 and batch: 750, loss is 4.536384000778198 and perplexity is 93.35262604622623
At time: 660.3734786510468 and batch: 800, loss is 4.513246183395386 and perplexity is 91.21744698056823
At time: 661.5114958286285 and batch: 850, loss is 4.563769006729126 and perplexity is 95.9444143638138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.836933771769206 and perplexity of 126.08216174329924
Finished 31 epochs...
Completing Train Step...
At time: 664.6215710639954 and batch: 50, loss is 4.655688152313233 and perplexity is 105.18157604491881
At time: 665.7842442989349 and batch: 100, loss is 4.6033265590667725 and perplexity is 99.81580715148723
At time: 666.9191732406616 and batch: 150, loss is 4.603780784606934 and perplexity is 99.86115633900714
At time: 668.0559482574463 and batch: 200, loss is 4.61806324005127 and perplexity is 101.29765278429284
At time: 669.1928796768188 and batch: 250, loss is 4.6108514595031735 and perplexity is 100.56974425552029
At time: 670.3555657863617 and batch: 300, loss is 4.585667476654053 and perplexity is 98.06862381784609
At time: 671.4935967922211 and batch: 350, loss is 4.537099838256836 and perplexity is 93.41947527841772
At time: 672.6285617351532 and batch: 400, loss is 4.553263130187989 and perplexity is 94.94171055554324
At time: 673.7653384208679 and batch: 450, loss is 4.593874549865722 and perplexity is 98.87679200477919
At time: 674.9028084278107 and batch: 500, loss is 4.598795347213745 and perplexity is 99.36454374016895
At time: 676.0582418441772 and batch: 550, loss is 4.580835771560669 and perplexity is 97.59592803156798
At time: 677.2046213150024 and batch: 600, loss is 4.5997567558288575 and perplexity is 99.4601196049256
At time: 678.3433148860931 and batch: 650, loss is 4.564020309448242 and perplexity is 95.96852848587274
At time: 679.483035326004 and batch: 700, loss is 4.539892206192016 and perplexity is 93.68070137565687
At time: 680.6424443721771 and batch: 750, loss is 4.536531505584716 and perplexity is 93.36639702288683
At time: 681.7802426815033 and batch: 800, loss is 4.513390712738037 and perplexity is 91.2306315309729
At time: 682.9191665649414 and batch: 850, loss is 4.563896627426147 and perplexity is 95.95665963820879
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.836921374003093 and perplexity of 126.08059861583655
Finished 32 epochs...
Completing Train Step...
At time: 686.0198080539703 and batch: 50, loss is 4.655099897384644 and perplexity is 105.11972065956502
At time: 687.1791899204254 and batch: 100, loss is 4.602722444534302 and perplexity is 99.7555251822569
At time: 688.3227746486664 and batch: 150, loss is 4.603131713867188 and perplexity is 99.79636041523423
At time: 689.46040558815 and batch: 200, loss is 4.617482833862304 and perplexity is 101.23887605852688
At time: 690.6235930919647 and batch: 250, loss is 4.610201435089111 and perplexity is 100.50439270878974
At time: 691.7543253898621 and batch: 300, loss is 4.585078458786011 and perplexity is 98.01087665484809
At time: 692.887898683548 and batch: 350, loss is 4.536624946594238 and perplexity is 93.37512168089415
At time: 694.0204663276672 and batch: 400, loss is 4.552867403030396 and perplexity is 94.90414697524272
At time: 695.149361371994 and batch: 450, loss is 4.593506240844727 and perplexity is 98.84038149588778
At time: 696.2844684123993 and batch: 500, loss is 4.598549585342408 and perplexity is 99.34012672446343
At time: 697.4319307804108 and batch: 550, loss is 4.580704765319824 and perplexity is 97.58314319337988
At time: 698.5936496257782 and batch: 600, loss is 4.599592561721802 and perplexity is 99.44379018003377
At time: 699.7280647754669 and batch: 650, loss is 4.563927192687988 and perplexity is 95.95959262345949
At time: 700.8658695220947 and batch: 700, loss is 4.539855003356934 and perplexity is 93.67721625280177
At time: 702.013619184494 and batch: 750, loss is 4.53665018081665 and perplexity is 93.3774779592117
At time: 703.1673276424408 and batch: 800, loss is 4.513503751754761 and perplexity is 91.24094473474248
At time: 704.3066444396973 and batch: 850, loss is 4.563998174667359 and perplexity is 95.96640426703262
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.836909929911296 and perplexity of 126.07915574614837
Finished 33 epochs...
Completing Train Step...
At time: 707.418249130249 and batch: 50, loss is 4.654535589218139 and perplexity is 105.06041747694499
At time: 708.5847148895264 and batch: 100, loss is 4.602146015167237 and perplexity is 99.69803973775578
At time: 709.723459482193 and batch: 150, loss is 4.602510862350464 and perplexity is 99.73442092311026
At time: 710.8563323020935 and batch: 200, loss is 4.616915016174317 and perplexity is 101.1814071514639
At time: 711.9948115348816 and batch: 250, loss is 4.609578447341919 and perplexity is 100.44179920311116
At time: 713.1290299892426 and batch: 300, loss is 4.584503679275513 and perplexity is 97.95455819803894
At time: 714.2641034126282 and batch: 350, loss is 4.536172618865967 and perplexity is 93.33289507507948
At time: 715.3974075317383 and batch: 400, loss is 4.552487735748291 and perplexity is 94.86812181492026
At time: 716.53759765625 and batch: 450, loss is 4.593142375946045 and perplexity is 98.80442349281343
At time: 717.6750092506409 and batch: 500, loss is 4.598288974761963 and perplexity is 99.31424100956842
At time: 718.815299987793 and batch: 550, loss is 4.5805861759185795 and perplexity is 97.5715715530075
At time: 719.950558423996 and batch: 600, loss is 4.599423532485962 and perplexity is 99.426982692689
At time: 721.0871012210846 and batch: 650, loss is 4.563824052810669 and perplexity is 95.94969587323232
At time: 722.2212104797363 and batch: 700, loss is 4.539804000854492 and perplexity is 93.67243860218821
At time: 723.3572361469269 and batch: 750, loss is 4.536744995117187 and perplexity is 93.38633189920375
At time: 724.4909217357635 and batch: 800, loss is 4.51359377861023 and perplexity is 91.24915923984446
At time: 725.6256952285767 and batch: 850, loss is 4.564068088531494 and perplexity is 95.97311388372698
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.83689530690511 and perplexity of 126.07731210335388
Finished 34 epochs...
Completing Train Step...
At time: 728.8343117237091 and batch: 50, loss is 4.65399398803711 and perplexity is 105.00353203676134
At time: 729.9711151123047 and batch: 100, loss is 4.601591854095459 and perplexity is 99.64280627073272
At time: 731.1084930896759 and batch: 150, loss is 4.601905689239502 and perplexity is 99.67408259273935
At time: 732.2461910247803 and batch: 200, loss is 4.616371154785156 and perplexity is 101.12639345208281
At time: 733.3859729766846 and batch: 250, loss is 4.608992004394532 and perplexity is 100.38291308670708
At time: 734.5257360935211 and batch: 300, loss is 4.583944988250733 and perplexity is 97.89984715024615
At time: 735.6886396408081 and batch: 350, loss is 4.535725231170654 and perplexity is 93.29114842541999
At time: 736.8478529453278 and batch: 400, loss is 4.552113990783692 and perplexity is 94.83267195710654
At time: 737.9884712696075 and batch: 450, loss is 4.592775239944458 and perplexity is 98.76815548988527
At time: 739.1255073547363 and batch: 500, loss is 4.597991428375244 and perplexity is 99.28469481190646
At time: 740.2638204097748 and batch: 550, loss is 4.5804541015625 and perplexity is 97.558685701487
At time: 741.4119596481323 and batch: 600, loss is 4.599250230789185 and perplexity is 99.40975332086573
At time: 742.555903673172 and batch: 650, loss is 4.563697128295899 and perplexity is 95.9375182774753
At time: 743.6938095092773 and batch: 700, loss is 4.539735774993897 and perplexity is 93.66604793745732
At time: 744.8293359279633 and batch: 750, loss is 4.536795272827148 and perplexity is 93.39102726814848
At time: 745.9633271694183 and batch: 800, loss is 4.513666391372681 and perplexity is 91.25578533393478
At time: 747.0973515510559 and batch: 850, loss is 4.564036312103272 and perplexity is 95.97006424941591
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.83686637878418 and perplexity of 126.07366497637524
Finished 35 epochs...
Completing Train Step...
At time: 750.3078579902649 and batch: 50, loss is 4.6534690666198735 and perplexity is 104.94842789784644
At time: 751.4462459087372 and batch: 100, loss is 4.601056861877441 and perplexity is 99.58951240197007
At time: 752.5852649211884 and batch: 150, loss is 4.601305475234986 and perplexity is 99.61427476302384
At time: 753.7245316505432 and batch: 200, loss is 4.615849342346191 and perplexity is 101.07363820744034
At time: 754.8612585067749 and batch: 250, loss is 4.608410892486572 and perplexity is 100.32459632647969
At time: 755.9993760585785 and batch: 300, loss is 4.583397998809814 and perplexity is 97.84631161060969
At time: 757.1823697090149 and batch: 350, loss is 4.535279216766358 and perplexity is 93.24954850719988
At time: 758.3187108039856 and batch: 400, loss is 4.551714782714844 and perplexity is 94.79482154486848
At time: 759.4575247764587 and batch: 450, loss is 4.5924184703826905 and perplexity is 98.73292430341577
At time: 760.5957424640656 and batch: 500, loss is 4.5976513004302975 and perplexity is 99.25093105501956
At time: 761.7367770671844 and batch: 550, loss is 4.580255317687988 and perplexity is 97.53929453534037
At time: 762.8773295879364 and batch: 600, loss is 4.599070997238159 and perplexity is 99.39193735442859
At time: 764.0142061710358 and batch: 650, loss is 4.563530826568604 and perplexity is 95.92156502903653
At time: 765.1524386405945 and batch: 700, loss is 4.539661331176758 and perplexity is 93.65907533884918
At time: 766.2901380062103 and batch: 750, loss is 4.536753854751587 and perplexity is 93.38715927162738
At time: 767.4291853904724 and batch: 800, loss is 4.513723964691162 and perplexity is 91.26103938357217
At time: 768.5670671463013 and batch: 850, loss is 4.5639904117584225 and perplexity is 95.96565929146696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8368581136067705 and perplexity of 126.07262295947383
Finished 36 epochs...
Completing Train Step...
At time: 771.7113993167877 and batch: 50, loss is 4.652961091995239 and perplexity is 104.89513029763938
At time: 772.8585946559906 and batch: 100, loss is 4.600538721084595 and perplexity is 99.53792437913862
At time: 773.9952862262726 and batch: 150, loss is 4.600719718933106 and perplexity is 99.55594215983704
At time: 775.1333577632904 and batch: 200, loss is 4.615364990234375 and perplexity is 101.02469483119626
At time: 776.2912945747375 and batch: 250, loss is 4.607825603485107 and perplexity is 100.26589462407956
At time: 777.4273056983948 and batch: 300, loss is 4.582863426208496 and perplexity is 97.79401963145753
At time: 778.5811047554016 and batch: 350, loss is 4.534838266372681 and perplexity is 93.20843914633596
At time: 779.763747215271 and batch: 400, loss is 4.551332292556762 and perplexity is 94.75857039188708
At time: 780.8967339992523 and batch: 450, loss is 4.59208498954773 and perplexity is 98.7000042547887
At time: 782.0322108268738 and batch: 500, loss is 4.59737943649292 and perplexity is 99.22395197360039
At time: 783.168820142746 and batch: 550, loss is 4.58012095451355 and perplexity is 97.52618972651571
At time: 784.3108749389648 and batch: 600, loss is 4.598890132904053 and perplexity is 99.3739625234154
At time: 785.4483082294464 and batch: 650, loss is 4.563399810791015 and perplexity is 95.90899861382404
At time: 786.6294856071472 and batch: 700, loss is 4.539578037261963 and perplexity is 93.65127443269665
At time: 787.7674276828766 and batch: 750, loss is 4.53679181098938 and perplexity is 93.39070396412266
At time: 788.9055821895599 and batch: 800, loss is 4.51376314163208 and perplexity is 91.26461478195635
At time: 790.0428223609924 and batch: 850, loss is 4.5640249347686765 and perplexity is 95.96897237209514
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.836870829264323 and perplexity of 126.0742260659664
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 793.1324787139893 and batch: 50, loss is 4.652839136123657 and perplexity is 104.88233850063247
At time: 794.3011512756348 and batch: 100, loss is 4.600343894958496 and perplexity is 99.51853367990067
At time: 795.4364910125732 and batch: 150, loss is 4.600887222290039 and perplexity is 99.57261951106864
At time: 796.5721538066864 and batch: 200, loss is 4.614982051849365 and perplexity is 100.98601600398797
At time: 797.7260465621948 and batch: 250, loss is 4.607007656097412 and perplexity is 100.18391592919713
At time: 798.858963727951 and batch: 300, loss is 4.581939764022827 and perplexity is 97.70373269727197
At time: 799.9944586753845 and batch: 350, loss is 4.534170532226563 and perplexity is 93.14622146355897
At time: 801.157919883728 and batch: 400, loss is 4.549856986999512 and perplexity is 94.61887561796638
At time: 802.2940413951874 and batch: 450, loss is 4.590934619903565 and perplexity is 98.58652804832073
At time: 803.4314153194427 and batch: 500, loss is 4.5962261295318605 and perplexity is 99.10958226345392
At time: 804.563214302063 and batch: 550, loss is 4.578503303527832 and perplexity is 97.36855392380214
At time: 805.6965961456299 and batch: 600, loss is 4.596946020126342 and perplexity is 99.18095600710936
At time: 806.8304605484009 and batch: 650, loss is 4.560990362167359 and perplexity is 95.67818898275429
At time: 807.9647188186646 and batch: 700, loss is 4.537091655731201 and perplexity is 93.41871087429385
At time: 809.1021659374237 and batch: 750, loss is 4.533907747268676 and perplexity is 93.12174725354235
At time: 810.240326166153 and batch: 800, loss is 4.510820455551148 and perplexity is 90.99644643150668
At time: 811.377049446106 and batch: 850, loss is 4.56152226448059 and perplexity is 95.72909396984006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.83655579884847 and perplexity of 126.03451510550906
Finished 38 epochs...
Completing Train Step...
At time: 814.4894890785217 and batch: 50, loss is 4.65253095626831 and perplexity is 104.85002085680406
At time: 815.6560752391815 and batch: 100, loss is 4.600020809173584 and perplexity is 99.48638584986647
At time: 816.7921817302704 and batch: 150, loss is 4.600407524108887 and perplexity is 99.52486616110987
At time: 817.9299101829529 and batch: 200, loss is 4.614777908325196 and perplexity is 100.96540246692071
At time: 819.0893397331238 and batch: 250, loss is 4.60683653831482 and perplexity is 100.16677414632566
At time: 820.2325475215912 and batch: 300, loss is 4.581807031631469 and perplexity is 97.69076510781508
At time: 821.3744652271271 and batch: 350, loss is 4.534004497528076 and perplexity is 93.13075724259724
At time: 822.5197870731354 and batch: 400, loss is 4.549768047332764 and perplexity is 94.61046062091992
At time: 823.6679000854492 and batch: 450, loss is 4.590935077667236 and perplexity is 98.58657317766209
At time: 824.8101830482483 and batch: 500, loss is 4.596231136322022 and perplexity is 99.1100784855775
At time: 825.9640123844147 and batch: 550, loss is 4.57848048210144 and perplexity is 97.36633185987137
At time: 827.1085088253021 and batch: 600, loss is 4.596902647018433 and perplexity is 99.17665431409145
At time: 828.243358373642 and batch: 650, loss is 4.560924921035767 and perplexity is 95.67192789866705
At time: 829.3793556690216 and batch: 700, loss is 4.537063674926758 and perplexity is 93.41609698018313
At time: 830.518298625946 and batch: 750, loss is 4.533945341110229 and perplexity is 93.12524812355888
At time: 831.6712543964386 and batch: 800, loss is 4.510910873413086 and perplexity is 91.0046745076139
At time: 832.8093531131744 and batch: 850, loss is 4.561622533798218 and perplexity is 95.7386931420127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.836504618326823 and perplexity of 126.02806475834797
Finished 39 epochs...
Completing Train Step...
At time: 835.9449603557587 and batch: 50, loss is 4.652377681732178 and perplexity is 104.83395125005597
At time: 837.0843441486359 and batch: 100, loss is 4.599855651855469 and perplexity is 99.46995630195794
At time: 838.2230744361877 and batch: 150, loss is 4.600241813659668 and perplexity is 99.5083752172285
At time: 839.3580265045166 and batch: 200, loss is 4.614625444412232 and perplexity is 100.95001006000973
At time: 840.4964270591736 and batch: 250, loss is 4.606698923110962 and perplexity is 100.15299062371459
At time: 841.6336040496826 and batch: 300, loss is 4.581684217453003 and perplexity is 97.67876803347512
At time: 842.7690989971161 and batch: 350, loss is 4.533893175125122 and perplexity is 93.1203902799604
At time: 843.9046025276184 and batch: 400, loss is 4.549715738296509 and perplexity is 94.60551176834123
At time: 845.0794920921326 and batch: 450, loss is 4.590966062545776 and perplexity is 98.58962791798282
At time: 846.2097313404083 and batch: 500, loss is 4.596262969970703 and perplexity is 99.11323357121549
At time: 847.3396399021149 and batch: 550, loss is 4.578482875823974 and perplexity is 97.36656492813295
At time: 848.4723353385925 and batch: 600, loss is 4.596886281967163 and perplexity is 99.17503129633927
At time: 849.6028521060944 and batch: 650, loss is 4.560897178649903 and perplexity is 95.66927376794305
At time: 850.7330486774445 and batch: 700, loss is 4.537051773071289 and perplexity is 93.41498516191474
At time: 851.8657574653625 and batch: 750, loss is 4.533973569869995 and perplexity is 93.1278769709207
At time: 853.0241858959198 and batch: 800, loss is 4.510985803604126 and perplexity is 91.0114937607411
At time: 854.1643376350403 and batch: 850, loss is 4.561684770584106 and perplexity is 95.74465179598084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.836478233337402 and perplexity of 126.02473955306053
Finished 40 epochs...
Completing Train Step...
At time: 857.2959609031677 and batch: 50, loss is 4.65223783493042 and perplexity is 104.81929158233577
At time: 858.4323952198029 and batch: 100, loss is 4.599716491699219 and perplexity is 99.45611501039726
At time: 859.5672926902771 and batch: 150, loss is 4.600100717544556 and perplexity is 99.49433596252963
At time: 860.7114448547363 and batch: 200, loss is 4.614497194290161 and perplexity is 100.93706403907862
At time: 861.8609127998352 and batch: 250, loss is 4.606573400497436 and perplexity is 100.14041994754767
At time: 862.9924559593201 and batch: 300, loss is 4.581571063995361 and perplexity is 97.66771596843542
At time: 864.1275653839111 and batch: 350, loss is 4.5337959575653075 and perplexity is 93.11133778288635
At time: 865.2635629177094 and batch: 400, loss is 4.549673166275024 and perplexity is 94.60148430619084
At time: 866.3982946872711 and batch: 450, loss is 4.591001987457275 and perplexity is 98.59316980526111
At time: 867.5333027839661 and batch: 500, loss is 4.596301040649414 and perplexity is 99.11700695111388
At time: 868.6650116443634 and batch: 550, loss is 4.5784902667999265 and perplexity is 97.36728456473227
At time: 869.7983689308167 and batch: 600, loss is 4.596876211166382 and perplexity is 99.17403252938584
At time: 870.9356963634491 and batch: 650, loss is 4.560878944396973 and perplexity is 95.6675293261119
At time: 872.0698969364166 and batch: 700, loss is 4.537042179107666 and perplexity is 93.41408894624443
At time: 873.2025089263916 and batch: 750, loss is 4.533992624282837 and perplexity is 93.12965148484165
At time: 874.383407831192 and batch: 800, loss is 4.511049728393555 and perplexity is 91.01731183727304
At time: 875.5146019458771 and batch: 850, loss is 4.561728868484497 and perplexity is 95.74887402719378
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.836459159851074 and perplexity of 126.02233584483727
Finished 41 epochs...
Completing Train Step...
At time: 878.6261758804321 and batch: 50, loss is 4.652104921340943 and perplexity is 104.805360599874
At time: 879.7633833885193 and batch: 100, loss is 4.5995896053314205 and perplexity is 99.44349618580358
At time: 880.8965289592743 and batch: 150, loss is 4.599971961975098 and perplexity is 99.48152633731794
At time: 882.0293061733246 and batch: 200, loss is 4.614381322860718 and perplexity is 100.92536899475843
At time: 883.1656684875488 and batch: 250, loss is 4.606455574035644 and perplexity is 100.12862145128409
At time: 884.3039190769196 and batch: 300, loss is 4.581463766098023 and perplexity is 97.65723699007036
At time: 885.4421701431274 and batch: 350, loss is 4.53370659828186 and perplexity is 93.10301779220097
At time: 886.5800120830536 and batch: 400, loss is 4.549634761810303 and perplexity is 94.59785125658733
At time: 887.7174785137177 and batch: 450, loss is 4.5910373878479005 and perplexity is 98.59666010376378
At time: 888.8545467853546 and batch: 500, loss is 4.596340084075928 and perplexity is 99.1208768942385
At time: 889.9924936294556 and batch: 550, loss is 4.578498249053955 and perplexity is 97.36806177823368
At time: 891.1433334350586 and batch: 600, loss is 4.596867389678955 and perplexity is 99.17315767076362
At time: 892.2903809547424 and batch: 650, loss is 4.560863752365112 and perplexity is 95.6660759529982
At time: 893.4323515892029 and batch: 700, loss is 4.537032585144043 and perplexity is 93.41319273917233
At time: 894.5733933448792 and batch: 750, loss is 4.534005527496338 and perplexity is 93.13085316437079
At time: 895.7111170291901 and batch: 800, loss is 4.511105785369873 and perplexity is 91.02241413557559
At time: 896.851568698883 and batch: 850, loss is 4.561762847900391 and perplexity is 95.75212757328217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.836448669433594 and perplexity of 126.02101382485668
Finished 42 epochs...
Completing Train Step...
At time: 899.9426834583282 and batch: 50, loss is 4.651978178024292 and perplexity is 104.79207806262299
At time: 901.105021238327 and batch: 100, loss is 4.599470176696777 and perplexity is 99.43162049399291
At time: 902.2444105148315 and batch: 150, loss is 4.599851522445679 and perplexity is 99.46954555059463
At time: 903.426561832428 and batch: 200, loss is 4.614272718429565 and perplexity is 100.91440864765184
At time: 904.5646343231201 and batch: 250, loss is 4.60634295463562 and perplexity is 100.11734566095929
At time: 905.7032899856567 and batch: 300, loss is 4.581360845565796 and perplexity is 97.64718657246961
At time: 906.8405055999756 and batch: 350, loss is 4.533622531890869 and perplexity is 93.0951912864825
At time: 907.9840669631958 and batch: 400, loss is 4.549599094390869 and perplexity is 94.59447725552033
At time: 909.1227073669434 and batch: 450, loss is 4.591071109771729 and perplexity is 98.59998502888666
At time: 910.2595181465149 and batch: 500, loss is 4.596378879547119 and perplexity is 99.1247224099563
At time: 911.3982129096985 and batch: 550, loss is 4.578505191802979 and perplexity is 97.3687377825962
At time: 912.5358130931854 and batch: 600, loss is 4.596858167648316 and perplexity is 99.17224309708207
At time: 913.6735372543335 and batch: 650, loss is 4.560849437713623 and perplexity is 95.66470653626298
At time: 914.8117945194244 and batch: 700, loss is 4.537022581100464 and perplexity is 93.4122582341957
At time: 915.9493939876556 and batch: 750, loss is 4.534014158248901 and perplexity is 93.13165695718912
At time: 917.0873379707336 and batch: 800, loss is 4.511155776977539 and perplexity is 91.02696460613357
At time: 918.2260186672211 and batch: 850, loss is 4.5617906665802 and perplexity is 95.75479130811082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.836440404256185 and perplexity of 126.01997224312457
Finished 43 epochs...
Completing Train Step...
At time: 921.3294768333435 and batch: 50, loss is 4.651855897903443 and perplexity is 104.7792648580695
At time: 922.5000915527344 and batch: 100, loss is 4.599356431961059 and perplexity is 99.42031131378984
At time: 923.646904706955 and batch: 150, loss is 4.599737720489502 and perplexity is 99.45822636581586
At time: 924.7855734825134 and batch: 200, loss is 4.61416955947876 and perplexity is 100.90399896007003
At time: 925.923036813736 and batch: 250, loss is 4.606234035491943 and perplexity is 100.10644155924619
At time: 927.0601706504822 and batch: 300, loss is 4.581261358261108 and perplexity is 97.63747240029357
At time: 928.199373960495 and batch: 350, loss is 4.53354208946228 and perplexity is 93.0877027844061
At time: 929.3382234573364 and batch: 400, loss is 4.549564828872681 and perplexity is 94.5912359822717
At time: 930.4750995635986 and batch: 450, loss is 4.591103773117066 and perplexity is 98.60320568684632
At time: 931.6551430225372 and batch: 500, loss is 4.5964163875579835 and perplexity is 99.1284404508491
At time: 932.7932846546173 and batch: 550, loss is 4.578511161804199 and perplexity is 97.36931907581476
At time: 933.9316213130951 and batch: 600, loss is 4.5968479347229 and perplexity is 99.17122828010746
At time: 935.0877270698547 and batch: 650, loss is 4.560834999084473 and perplexity is 95.66332527901432
At time: 936.2225549221039 and batch: 700, loss is 4.53701156616211 and perplexity is 93.4112293095965
At time: 937.3586797714233 and batch: 750, loss is 4.53402003288269 and perplexity is 93.13220407317499
At time: 938.4940330982208 and batch: 800, loss is 4.511201114654541 and perplexity is 91.03109165080797
At time: 939.6332087516785 and batch: 850, loss is 4.561814403533935 and perplexity is 95.7570642621384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.836434682210286 and perplexity of 126.01925115312234
Finished 44 epochs...
Completing Train Step...
At time: 942.7303698062897 and batch: 50, loss is 4.651737642288208 and perplexity is 104.7668748542481
At time: 943.893149137497 and batch: 100, loss is 4.599247150421142 and perplexity is 99.40944710271013
At time: 945.0474891662598 and batch: 150, loss is 4.599628629684449 and perplexity is 99.44737697962732
At time: 946.1814818382263 and batch: 200, loss is 4.614070062637329 and perplexity is 100.8939598303249
At time: 947.3158481121063 and batch: 250, loss is 4.606128196716309 and perplexity is 100.09584697670716
At time: 948.4537408351898 and batch: 300, loss is 4.581164484024048 and perplexity is 97.62801430277666
At time: 949.5978336334229 and batch: 350, loss is 4.533464736938477 and perplexity is 93.0805024941446
At time: 950.735880613327 and batch: 400, loss is 4.549531631469726 and perplexity is 94.58809585101721
At time: 951.885062456131 and batch: 450, loss is 4.591134929656983 and perplexity is 98.60627786941929
At time: 953.025470495224 and batch: 500, loss is 4.596452951431274 and perplexity is 99.13206503684931
At time: 954.1607682704926 and batch: 550, loss is 4.578515872955323 and perplexity is 97.36977779847226
At time: 955.3149745464325 and batch: 600, loss is 4.596836833953858 and perplexity is 99.1701274093169
At time: 956.4500434398651 and batch: 650, loss is 4.56082049369812 and perplexity is 95.6619376555854
At time: 957.5850684642792 and batch: 700, loss is 4.537000074386596 and perplexity is 93.41015585488681
At time: 958.7177941799164 and batch: 750, loss is 4.5340236473083495 and perplexity is 93.13254069321142
At time: 959.8549113273621 and batch: 800, loss is 4.511242752075195 and perplexity is 91.03488202957391
At time: 961.0378198623657 and batch: 850, loss is 4.561835117340088 and perplexity is 95.7590477759483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.836430549621582 and perplexity of 126.01873036846462
Finished 45 epochs...
Completing Train Step...
At time: 964.1294960975647 and batch: 50, loss is 4.651622743606567 and perplexity is 104.75483796997202
At time: 965.2702269554138 and batch: 100, loss is 4.599141521453857 and perplexity is 99.39894714003417
At time: 966.4119317531586 and batch: 150, loss is 4.59952332496643 and perplexity is 99.43690525300761
At time: 967.5503220558167 and batch: 200, loss is 4.613973894119263 and perplexity is 100.88425747426426
At time: 968.686591386795 and batch: 250, loss is 4.606024580001831 and perplexity is 100.0854759112277
At time: 969.8224108219147 and batch: 300, loss is 4.581069717407226 and perplexity is 97.61876286452487
At time: 970.9712183475494 and batch: 350, loss is 4.5333899402618405 and perplexity is 93.07354064226332
At time: 972.1058945655823 and batch: 400, loss is 4.5494990730285645 and perplexity is 94.58501626019745
At time: 973.2446205615997 and batch: 450, loss is 4.591165409088135 and perplexity is 98.60928337847966
At time: 974.3822820186615 and batch: 500, loss is 4.59648829460144 and perplexity is 99.13556874020841
At time: 975.5215377807617 and batch: 550, loss is 4.578519325256348 and perplexity is 97.37011394883625
At time: 976.6602966785431 and batch: 600, loss is 4.596824884414673 and perplexity is 99.16894237907377
At time: 977.7970457077026 and batch: 650, loss is 4.560805501937867 and perplexity is 95.66050352550077
At time: 978.9359042644501 and batch: 700, loss is 4.536987714767456 and perplexity is 93.40900134807123
At time: 980.0742268562317 and batch: 750, loss is 4.534025745391846 and perplexity is 93.132736093263
At time: 981.223418712616 and batch: 800, loss is 4.511281328201294 and perplexity is 91.03839387039865
At time: 982.392392873764 and batch: 850, loss is 4.561853647232056 and perplexity is 95.76082219719837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.836427688598633 and perplexity of 126.01836982650076
Finished 46 epochs...
Completing Train Step...
At time: 985.5336232185364 and batch: 50, loss is 4.65151086807251 and perplexity is 104.74311912206745
At time: 986.6760230064392 and batch: 100, loss is 4.599038848876953 and perplexity is 99.38874211788664
At time: 987.8179862499237 and batch: 150, loss is 4.599421291351319 and perplexity is 99.4267598636833
At time: 988.9540526866913 and batch: 200, loss is 4.61388014793396 and perplexity is 100.87480040325806
At time: 990.1388022899628 and batch: 250, loss is 4.6059230518341066 and perplexity is 100.07531493206412
At time: 991.2781114578247 and batch: 300, loss is 4.58097653388977 and perplexity is 97.60966682863834
At time: 992.4141316413879 and batch: 350, loss is 4.53331693649292 and perplexity is 93.06674617102367
At time: 993.5538151264191 and batch: 400, loss is 4.549467144012451 and perplexity is 94.58199630190157
At time: 994.6923127174377 and batch: 450, loss is 4.59119439125061 and perplexity is 98.61214133016671
At time: 995.8308079242706 and batch: 500, loss is 4.596522750854493 and perplexity is 99.13898463930063
At time: 996.9688491821289 and batch: 550, loss is 4.578521871566773 and perplexity is 97.37036188368812
At time: 998.1076366901398 and batch: 600, loss is 4.596812038421631 and perplexity is 99.16766846371237
At time: 999.2456328868866 and batch: 650, loss is 4.560790185928345 and perplexity is 95.65903839953786
At time: 1000.3827440738678 and batch: 700, loss is 4.536974716186523 and perplexity is 93.40778717149868
At time: 1001.5205640792847 and batch: 750, loss is 4.534026460647583 and perplexity is 93.13280270701065
At time: 1002.6591291427612 and batch: 800, loss is 4.511317129135132 and perplexity is 91.04165318825727
At time: 1003.7970244884491 and batch: 850, loss is 4.5618704891204835 and perplexity is 95.76243500386289
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8364260991414385 and perplexity of 126.0181695258554
Finished 47 epochs...
Completing Train Step...
At time: 1006.9286203384399 and batch: 50, loss is 4.651401777267456 and perplexity is 104.73169323411952
At time: 1008.0647721290588 and batch: 100, loss is 4.59893871307373 and perplexity is 99.3787902446411
At time: 1009.1999068260193 and batch: 150, loss is 4.59932204246521 and perplexity is 99.416892358195
At time: 1010.3321523666382 and batch: 200, loss is 4.6137885475158695 and perplexity is 100.86556065255529
At time: 1011.4697988033295 and batch: 250, loss is 4.605822916030884 and perplexity is 100.06529431174044
At time: 1012.6225912570953 and batch: 300, loss is 4.580884828567505 and perplexity is 97.60071591311514
At time: 1013.7722244262695 and batch: 350, loss is 4.5332455062866215 and perplexity is 93.06009863156555
At time: 1014.9074037075043 and batch: 400, loss is 4.5494355201721195 and perplexity is 94.57900530324599
At time: 1016.0438623428345 and batch: 450, loss is 4.591222686767578 and perplexity is 98.61493165116154
At time: 1017.1792011260986 and batch: 500, loss is 4.596556253433228 and perplexity is 99.14230610657779
At time: 1018.3274447917938 and batch: 550, loss is 4.578523073196411 and perplexity is 97.37047888687115
At time: 1019.5094928741455 and batch: 600, loss is 4.596798677444458 and perplexity is 99.16634349560913
At time: 1020.6455340385437 and batch: 650, loss is 4.56077428817749 and perplexity is 95.65751764806672
At time: 1021.7838032245636 and batch: 700, loss is 4.5369612503051755 and perplexity is 93.40652936178844
At time: 1022.9182062149048 and batch: 750, loss is 4.534026174545288 and perplexity is 93.1327760615059
At time: 1024.0541560649872 and batch: 800, loss is 4.511350736618042 and perplexity is 91.04471292047559
At time: 1025.1888582706451 and batch: 850, loss is 4.561885709762573 and perplexity is 95.76389258070432
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.836424191792806 and perplexity of 126.01792916550127
Finished 48 epochs...
Completing Train Step...
At time: 1028.3236258029938 and batch: 50, loss is 4.651294641494751 and perplexity is 104.72047332427577
At time: 1029.4817583560944 and batch: 100, loss is 4.598840885162353 and perplexity is 99.36906870068321
At time: 1030.6212837696075 and batch: 150, loss is 4.599224796295166 and perplexity is 99.40722491624399
At time: 1031.7585186958313 and batch: 200, loss is 4.613698844909668 and perplexity is 100.8565131546869
At time: 1032.89559841156 and batch: 250, loss is 4.605724458694458 and perplexity is 100.05544263438674
At time: 1034.03551197052 and batch: 300, loss is 4.580794134140015 and perplexity is 97.591864473457
At time: 1035.170521736145 and batch: 350, loss is 4.533175621032715 and perplexity is 93.0535953301892
At time: 1036.304327249527 and batch: 400, loss is 4.549404344558716 and perplexity is 94.57605679070163
At time: 1037.4329493045807 and batch: 450, loss is 4.5912500095367434 and perplexity is 98.61762612098534
At time: 1038.5625913143158 and batch: 500, loss is 4.596588592529297 and perplexity is 99.14551233098244
At time: 1039.6917643547058 and batch: 550, loss is 4.578523597717285 and perplexity is 97.37052995973323
At time: 1040.8195433616638 and batch: 600, loss is 4.59678463935852 and perplexity is 99.16495139972827
At time: 1041.9490942955017 and batch: 650, loss is 4.560758113861084 and perplexity is 95.65597046562198
At time: 1043.076180934906 and batch: 700, loss is 4.536947374343872 and perplexity is 93.40523326539383
At time: 1044.2081258296967 and batch: 750, loss is 4.534024906158447 and perplexity is 93.1326579331932
At time: 1045.3367192745209 and batch: 800, loss is 4.511382465362549 and perplexity is 91.04760170073895
At time: 1046.465117931366 and batch: 850, loss is 4.56189980506897 and perplexity is 95.76524241162511
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.836423238118489 and perplexity of 126.01780898549613
Finished 49 epochs...
Completing Train Step...
At time: 1049.5932774543762 and batch: 50, loss is 4.651189575195312 and perplexity is 104.70947130964875
At time: 1050.7531776428223 and batch: 100, loss is 4.598744630813599 and perplexity is 99.35950445599654
At time: 1051.884194135666 and batch: 150, loss is 4.599129552841187 and perplexity is 99.39775747965528
At time: 1053.0193138122559 and batch: 200, loss is 4.613610429763794 and perplexity is 100.84759630556205
At time: 1054.1520674228668 and batch: 250, loss is 4.605626993179321 and perplexity is 100.04569115435237
At time: 1055.3080232143402 and batch: 300, loss is 4.580704536437988 and perplexity is 97.58312085837343
At time: 1056.444048166275 and batch: 350, loss is 4.533106861114502 and perplexity is 93.04719719255506
At time: 1057.5824630260468 and batch: 400, loss is 4.549373064041138 and perplexity is 94.5730984489642
At time: 1058.720549583435 and batch: 450, loss is 4.591276588439942 and perplexity is 98.62024730415759
At time: 1059.8534998893738 and batch: 500, loss is 4.596620121002197 and perplexity is 99.14863828685918
At time: 1060.9846441745758 and batch: 550, loss is 4.578523120880127 and perplexity is 97.3704835298575
At time: 1062.1135754585266 and batch: 600, loss is 4.596769971847534 and perplexity is 99.16349690738107
At time: 1063.2422726154327 and batch: 650, loss is 4.560741529464722 and perplexity is 95.65438408224803
At time: 1064.3832080364227 and batch: 700, loss is 4.536932764053344 and perplexity is 93.40386859776815
At time: 1065.5164771080017 and batch: 750, loss is 4.534022789001465 and perplexity is 93.13246075694491
At time: 1066.6474578380585 and batch: 800, loss is 4.511412229537964 and perplexity is 91.0503116978573
At time: 1067.7851431369781 and batch: 850, loss is 4.561912775039673 and perplexity is 95.7664844920684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.836423873901367 and perplexity of 126.01788910548686
Annealing...
Finished Training.
Improved accuracyfrom -167.54256475624217 to -126.01780898549613
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fe9bf57c6a0>
SETTINGS FOR THIS RUN
{'dropout': 0.0336618035998445, 'batch_size': 50, 'data': 'wikitext', 'lr': 15.53775482745188, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'anneal': 2.7899680959719806, 'wordvec_source': '', 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.6965417861938477 and batch: 50, loss is 6.861413631439209 and perplexity is 954.7157301822944
At time: 2.8334553241729736 and batch: 100, loss is 5.803325252532959 and perplexity is 331.39971749504093
At time: 3.97119140625 and batch: 150, loss is 5.639953546524048 and perplexity is 281.449643857339
At time: 5.110110521316528 and batch: 200, loss is 5.601570816040039 and perplexity is 270.8515313729302
At time: 6.248861074447632 and batch: 250, loss is 5.619677610397339 and perplexity is 275.80045370189157
At time: 7.460787773132324 and batch: 300, loss is 5.5433580112457275 and perplexity is 255.53464855844985
At time: 8.594776391983032 and batch: 350, loss is 5.507511224746704 and perplexity is 246.53678825147628
At time: 9.731089115142822 and batch: 400, loss is 5.513440093994141 and perplexity is 248.0028142775992
At time: 10.864564418792725 and batch: 450, loss is 5.506074151992798 and perplexity is 246.18275139953056
At time: 12.002853155136108 and batch: 500, loss is 5.513954048156738 and perplexity is 248.1303091167783
At time: 13.151813507080078 and batch: 550, loss is 5.478774728775025 and perplexity is 239.55301007147523
At time: 14.301570177078247 and batch: 600, loss is 5.516495914459228 and perplexity is 248.76182546318557
At time: 15.466188430786133 and batch: 650, loss is 5.522014274597168 and perplexity is 250.13837746625077
At time: 16.606995105743408 and batch: 700, loss is 5.474278812408447 and perplexity is 238.4784172227052
At time: 17.751922130584717 and batch: 750, loss is 5.475173664093018 and perplexity is 238.69191554651127
At time: 18.893068313598633 and batch: 800, loss is 5.484237365722656 and perplexity is 240.8651818913641
At time: 20.03187584877014 and batch: 850, loss is 5.473378076553344 and perplexity is 238.26370787436537
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.202358245849609 and perplexity of 181.70023084045445
Finished 1 epochs...
Completing Train Step...
At time: 23.185205221176147 and batch: 50, loss is 5.451488618850708 and perplexity is 233.104912123533
At time: 24.313713788986206 and batch: 100, loss is 5.370302181243897 and perplexity is 214.92780504389444
At time: 25.447346448898315 and batch: 150, loss is 5.372281265258789 and perplexity is 215.3535864167931
At time: 26.579599618911743 and batch: 200, loss is 5.3811657428741455 and perplexity is 217.2754151152829
At time: 27.712245225906372 and batch: 250, loss is 5.417841444015503 and perplexity is 225.39207561847687
At time: 28.84318780899048 and batch: 300, loss is 5.371841115951538 and perplexity is 215.2588195422347
At time: 29.977767944335938 and batch: 350, loss is 5.351721506118775 and perplexity is 210.97117359689395
At time: 31.110520362854004 and batch: 400, loss is 5.357009401321411 and perplexity is 212.08972183018264
At time: 32.24301242828369 and batch: 450, loss is 5.3714776134490965 and perplexity is 215.18058664244734
At time: 33.37390208244324 and batch: 500, loss is 5.380611248016358 and perplexity is 217.15497041094838
At time: 34.50860238075256 and batch: 550, loss is 5.367686672210693 and perplexity is 214.366393936273
At time: 35.639689922332764 and batch: 600, loss is 5.396759901046753 and perplexity is 220.69019844159862
At time: 36.78359389305115 and batch: 650, loss is 5.401979846954346 and perplexity is 221.84520124326423
At time: 37.91499352455139 and batch: 700, loss is 5.365581979751587 and perplexity is 213.91569306320756
At time: 39.04307317733765 and batch: 750, loss is 5.367913742065429 and perplexity is 214.41507560906538
At time: 40.17618775367737 and batch: 800, loss is 5.36753960609436 and perplexity is 214.33487022133644
At time: 41.32573080062866 and batch: 850, loss is 5.375519208908081 and perplexity is 216.05201932727329
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.2279008229573565 and perplexity of 186.40110362284048
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 44.513458490371704 and batch: 50, loss is 5.364918298721314 and perplexity is 213.77376837721414
At time: 45.64584684371948 and batch: 100, loss is 5.228642854690552 and perplexity is 186.53947048678435
At time: 46.78199100494385 and batch: 150, loss is 5.218456001281738 and perplexity is 184.6488662493722
At time: 47.92459845542908 and batch: 200, loss is 5.22252347946167 and perplexity is 185.40145100724433
At time: 49.062755823135376 and batch: 250, loss is 5.231487808227539 and perplexity is 187.07092223244157
At time: 50.20186138153076 and batch: 300, loss is 5.181926317214966 and perplexity is 178.02541432639222
At time: 51.35907006263733 and batch: 350, loss is 5.14570837020874 and perplexity is 171.69306385592483
At time: 52.49946093559265 and batch: 400, loss is 5.142955131530762 and perplexity is 171.22100201942786
At time: 53.648003816604614 and batch: 450, loss is 5.139521760940552 and perplexity is 170.63414489215958
At time: 54.78569054603577 and batch: 500, loss is 5.139920177459717 and perplexity is 170.70214189888418
At time: 55.924015045166016 and batch: 550, loss is 5.099183483123779 and perplexity is 163.88803530633015
At time: 57.0626757144928 and batch: 600, loss is 5.112972774505615 and perplexity is 166.16358825360186
At time: 58.20035099983215 and batch: 650, loss is 5.098081789016724 and perplexity is 163.70758024500705
At time: 59.33885478973389 and batch: 700, loss is 5.046007127761841 and perplexity is 155.4007288010258
At time: 60.47672772407532 and batch: 750, loss is 5.018862895965576 and perplexity is 151.23923124038953
At time: 61.615421295166016 and batch: 800, loss is 5.00166953086853 and perplexity is 148.66114640662204
At time: 62.75142550468445 and batch: 850, loss is 5.013581924438476 and perplexity is 150.44264638869512
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.990418752034505 and perplexity of 146.99796630666583
Finished 3 epochs...
Completing Train Step...
At time: 65.89589500427246 and batch: 50, loss is 5.126785774230957 and perplexity is 168.47473101280664
At time: 67.05818819999695 and batch: 100, loss is 5.030698623657226 and perplexity is 153.0398926463387
At time: 68.20388412475586 and batch: 150, loss is 5.045506019592285 and perplexity is 155.32287573430168
At time: 69.36615538597107 and batch: 200, loss is 5.053348741531372 and perplexity is 156.54581919379183
At time: 70.51511573791504 and batch: 250, loss is 5.061890363693237 and perplexity is 157.8887014642088
At time: 71.66369915008545 and batch: 300, loss is 5.027506999969482 and perplexity is 152.5522255387118
At time: 72.79812479019165 and batch: 350, loss is 4.989060382843018 and perplexity is 146.79842435452608
At time: 73.9383111000061 and batch: 400, loss is 4.993246583938599 and perplexity is 147.41424014444533
At time: 75.09646534919739 and batch: 450, loss is 5.001792573928833 and perplexity is 148.679439254405
At time: 76.23390698432922 and batch: 500, loss is 5.008812341690064 and perplexity is 149.72680622435198
At time: 77.37130570411682 and batch: 550, loss is 4.979685173034668 and perplexity is 145.42858960416467
At time: 78.50993084907532 and batch: 600, loss is 4.998696632385254 and perplexity is 148.2198482023495
At time: 79.65048885345459 and batch: 650, loss is 4.9872243785858155 and perplexity is 146.5291490933654
At time: 80.78946542739868 and batch: 700, loss is 4.9436270236969 and perplexity is 140.27812003964536
At time: 81.9274685382843 and batch: 750, loss is 4.931177501678467 and perplexity is 138.54255041797379
At time: 83.06673979759216 and batch: 800, loss is 4.918024959564209 and perplexity is 136.73229451863773
At time: 84.20532822608948 and batch: 850, loss is 4.937312221527099 and perplexity is 139.3950825009115
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.96886666615804 and perplexity of 143.8637492877271
Finished 4 epochs...
Completing Train Step...
At time: 87.31132340431213 and batch: 50, loss is 5.014078531265259 and perplexity is 150.5173757879601
At time: 88.44424700737 and batch: 100, loss is 4.9305969333648685 and perplexity is 138.4621403471442
At time: 89.57613396644592 and batch: 150, loss is 4.953038911819458 and perplexity is 141.60463472312134
At time: 90.71479892730713 and batch: 200, loss is 4.966884078979493 and perplexity is 143.5788094153336
At time: 91.85200190544128 and batch: 250, loss is 4.974952621459961 and perplexity is 144.74196732263118
At time: 92.98624157905579 and batch: 300, loss is 4.945307884216309 and perplexity is 140.51410626776305
At time: 94.12021827697754 and batch: 350, loss is 4.907144041061401 and perplexity is 135.25258645576383
At time: 95.2542290687561 and batch: 400, loss is 4.912670574188232 and perplexity is 136.0021336462333
At time: 96.40820026397705 and batch: 450, loss is 4.932895059585571 and perplexity is 138.78070973857916
At time: 97.54773902893066 and batch: 500, loss is 4.943135948181152 and perplexity is 140.20924980112676
At time: 98.73210573196411 and batch: 550, loss is 4.911876363754272 and perplexity is 135.89416221434496
At time: 99.86859726905823 and batch: 600, loss is 4.935352334976196 and perplexity is 139.12215149772138
At time: 101.0024676322937 and batch: 650, loss is 4.923795051574707 and perplexity is 137.52353300295772
At time: 102.14915418624878 and batch: 700, loss is 4.88658712387085 and perplexity is 132.5005933952347
At time: 103.29884219169617 and batch: 750, loss is 4.879105749130249 and perplexity is 131.51300566598798
At time: 104.43859052658081 and batch: 800, loss is 4.870422706604004 and perplexity is 130.37601605323914
At time: 105.57338213920593 and batch: 850, loss is 4.891419534683227 and perplexity is 133.14244028027386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.970052083333333 and perplexity of 144.0343889666558
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 108.70771551132202 and batch: 50, loss is 4.938177299499512 and perplexity is 139.51572229013433
At time: 109.84148907661438 and batch: 100, loss is 4.848262510299683 and perplexity is 127.51863493857813
At time: 110.98163771629333 and batch: 150, loss is 4.851752042770386 and perplexity is 127.96439264648747
At time: 112.11983323097229 and batch: 200, loss is 4.8612298583984375 and perplexity is 129.18298122384314
At time: 113.25855660438538 and batch: 250, loss is 4.856506814956665 and perplexity is 128.57428297724206
At time: 114.39752149581909 and batch: 300, loss is 4.822177820205688 and perplexity is 124.23535864991726
At time: 115.5581362247467 and batch: 350, loss is 4.7715655708312985 and perplexity is 118.10399747251677
At time: 116.69589757919312 and batch: 400, loss is 4.778137788772583 and perplexity is 118.88275897565136
At time: 117.84018468856812 and batch: 450, loss is 4.802548990249634 and perplexity is 121.82054147177
At time: 118.97914361953735 and batch: 500, loss is 4.799102554321289 and perplexity is 121.40141743775594
At time: 120.11415529251099 and batch: 550, loss is 4.765151491165161 and perplexity is 117.34889326189382
At time: 121.25189566612244 and batch: 600, loss is 4.782129430770874 and perplexity is 119.35824474239513
At time: 122.38985204696655 and batch: 650, loss is 4.757827806472778 and perplexity is 116.49260638353425
At time: 123.52790880203247 and batch: 700, loss is 4.715263290405273 and perplexity is 111.63820057163751
At time: 124.66550254821777 and batch: 750, loss is 4.703003091812134 and perplexity is 110.27785018475767
At time: 125.80612874031067 and batch: 800, loss is 4.684554824829101 and perplexity is 108.26206599512189
At time: 126.94524002075195 and batch: 850, loss is 4.723993797302246 and perplexity is 112.61712569000156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.886840184529622 and perplexity of 132.5341283256938
Finished 6 epochs...
Completing Train Step...
At time: 130.17270755767822 and batch: 50, loss is 4.846446142196656 and perplexity is 127.28722438453104
At time: 131.3331503868103 and batch: 100, loss is 4.772486429214478 and perplexity is 118.21280461897396
At time: 132.49768590927124 and batch: 150, loss is 4.781373977661133 and perplexity is 119.26810923609223
At time: 133.65242886543274 and batch: 200, loss is 4.798628311157227 and perplexity is 121.34385729525951
At time: 134.79939436912537 and batch: 250, loss is 4.795571517944336 and perplexity is 120.97350055622042
At time: 135.93770456314087 and batch: 300, loss is 4.766201009750366 and perplexity is 117.47211775821408
At time: 137.07738542556763 and batch: 350, loss is 4.7177345561981205 and perplexity is 111.91442941474969
At time: 138.21503567695618 and batch: 400, loss is 4.73037127494812 and perplexity is 113.3376339622779
At time: 139.35314559936523 and batch: 450, loss is 4.759320049285889 and perplexity is 116.66657140493514
At time: 140.4914412498474 and batch: 500, loss is 4.758361492156983 and perplexity is 116.55479341256436
At time: 141.62821006774902 and batch: 550, loss is 4.730166339874268 and perplexity is 113.31440948572781
At time: 142.76670384407043 and batch: 600, loss is 4.752799377441407 and perplexity is 115.90830187770784
At time: 143.90632128715515 and batch: 650, loss is 4.729805145263672 and perplexity is 113.27348832241542
At time: 145.04428601264954 and batch: 700, loss is 4.691284141540527 and perplexity is 108.99305248636281
At time: 146.1824643611908 and batch: 750, loss is 4.686962032318116 and perplexity is 108.52298917327927
At time: 147.3215515613556 and batch: 800, loss is 4.671167783737182 and perplexity is 106.82241510204732
At time: 148.4595241546631 and batch: 850, loss is 4.711309471130371 and perplexity is 111.19767475601579
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.882864634195964 and perplexity of 132.00827819203295
Finished 7 epochs...
Completing Train Step...
At time: 151.56645107269287 and batch: 50, loss is 4.802958431243897 and perplexity is 121.8704300079008
At time: 152.72848343849182 and batch: 100, loss is 4.732388238906861 and perplexity is 113.56646257710754
At time: 153.86540174484253 and batch: 150, loss is 4.743070011138916 and perplexity is 114.78605577563481
At time: 154.99923086166382 and batch: 200, loss is 4.761379404067993 and perplexity is 116.90707682466525
At time: 156.13046145439148 and batch: 250, loss is 4.7577500152587895 and perplexity is 116.48354463472977
At time: 157.29667806625366 and batch: 300, loss is 4.731367664337158 and perplexity is 113.45061865716464
At time: 158.43519067764282 and batch: 350, loss is 4.684047574996948 and perplexity is 108.2071640060069
At time: 159.57340264320374 and batch: 400, loss is 4.699325399398804 and perplexity is 109.87302703557064
At time: 160.71175074577332 and batch: 450, loss is 4.7296960830688475 and perplexity is 113.26113514080834
At time: 161.86855292320251 and batch: 500, loss is 4.730206661224365 and perplexity is 113.31897856781887
At time: 163.00970125198364 and batch: 550, loss is 4.704780788421631 and perplexity is 110.47406509875411
At time: 164.15345931053162 and batch: 600, loss is 4.729833679199219 and perplexity is 113.27672050694362
At time: 165.2922716140747 and batch: 650, loss is 4.706330146789551 and perplexity is 110.6453616815834
At time: 166.44358682632446 and batch: 700, loss is 4.670935955047607 and perplexity is 106.797653471876
At time: 167.58400440216064 and batch: 750, loss is 4.6689698696136475 and perplexity is 106.58788643852932
At time: 168.72341966629028 and batch: 800, loss is 4.654670648574829 and perplexity is 105.07460782759121
At time: 169.89099311828613 and batch: 850, loss is 4.694564867019653 and perplexity is 109.35121596748412
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.880485852559407 and perplexity of 131.6946325191794
Finished 8 epochs...
Completing Train Step...
At time: 173.04517555236816 and batch: 50, loss is 4.770566682815552 and perplexity is 117.9860837059582
At time: 174.22532749176025 and batch: 100, loss is 4.702809791564942 and perplexity is 110.25653550918956
At time: 175.35690808296204 and batch: 150, loss is 4.714304103851318 and perplexity is 111.53117005006602
At time: 176.49019122123718 and batch: 200, loss is 4.7314208984375 and perplexity is 113.45665825953708
At time: 177.63841438293457 and batch: 250, loss is 4.729210023880005 and perplexity is 113.20609690233874
At time: 178.76936841011047 and batch: 300, loss is 4.704042644500732 and perplexity is 110.39254942803412
At time: 179.89870977401733 and batch: 350, loss is 4.65658242225647 and perplexity is 105.27567883733622
At time: 181.03695440292358 and batch: 400, loss is 4.673998146057129 and perplexity is 107.12518951920286
At time: 182.17374682426453 and batch: 450, loss is 4.705786848068238 and perplexity is 110.58526452489502
At time: 183.31300711631775 and batch: 500, loss is 4.706909761428833 and perplexity is 110.70951194240195
At time: 184.44848322868347 and batch: 550, loss is 4.6833193969726565 and perplexity is 108.12839860820307
At time: 185.63095545768738 and batch: 600, loss is 4.710205097198486 and perplexity is 111.07493872843816
At time: 186.76519083976746 and batch: 650, loss is 4.686618375778198 and perplexity is 108.48570094585706
At time: 187.9020049571991 and batch: 700, loss is 4.651922121047973 and perplexity is 104.78620390023008
At time: 189.03969383239746 and batch: 750, loss is 4.651762342453003 and perplexity is 104.76946264528131
At time: 190.17761898040771 and batch: 800, loss is 4.638231143951416 and perplexity is 103.36135442974815
At time: 191.3167269229889 and batch: 850, loss is 4.67651671409607 and perplexity is 107.395331640551
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.87939453125 and perplexity of 131.55098975483784
Finished 9 epochs...
Completing Train Step...
At time: 194.42980408668518 and batch: 50, loss is 4.742226018905639 and perplexity is 114.68921810694016
At time: 195.61697554588318 and batch: 100, loss is 4.676245021820068 and perplexity is 107.36615712189081
At time: 196.7574508190155 and batch: 150, loss is 4.688999300003052 and perplexity is 108.74430491535257
At time: 197.89619708061218 and batch: 200, loss is 4.706733808517456 and perplexity is 110.69003399510959
At time: 199.03346061706543 and batch: 250, loss is 4.703345317840576 and perplexity is 110.31559659398498
At time: 200.1733865737915 and batch: 300, loss is 4.679614048004151 and perplexity is 107.72848652242426
At time: 201.31041479110718 and batch: 350, loss is 4.631864442825317 and perplexity is 102.70537400958432
At time: 202.44625282287598 and batch: 400, loss is 4.651209325790405 and perplexity is 104.71153940444195
At time: 203.58266592025757 and batch: 450, loss is 4.683702716827392 and perplexity is 108.16985431514055
At time: 204.7205331325531 and batch: 500, loss is 4.684334115982056 and perplexity is 108.23817423601561
At time: 205.85959839820862 and batch: 550, loss is 4.66366961479187 and perplexity is 106.02443800868411
At time: 206.998952627182 and batch: 600, loss is 4.691858692169189 and perplexity is 109.05569250639694
At time: 208.13725996017456 and batch: 650, loss is 4.667680931091309 and perplexity is 106.4505897082147
At time: 209.27604722976685 and batch: 700, loss is 4.634134721755982 and perplexity is 102.93880873697249
At time: 210.4197132587433 and batch: 750, loss is 4.634969701766968 and perplexity is 103.02479647864953
At time: 211.58532333374023 and batch: 800, loss is 4.622393741607666 and perplexity is 101.73727362968589
At time: 212.73170566558838 and batch: 850, loss is 4.6591993427276615 and perplexity is 105.55153770933916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.879642804463704 and perplexity of 131.58365439654023
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 216.00153017044067 and batch: 50, loss is 4.719965782165527 and perplexity is 112.16441457877224
At time: 217.14220356941223 and batch: 100, loss is 4.658142023086548 and perplexity is 105.43999497392315
At time: 218.27830362319946 and batch: 150, loss is 4.665162057876587 and perplexity is 106.18279158545852
At time: 219.4307415485382 and batch: 200, loss is 4.677525329589844 and perplexity is 107.50370688128857
At time: 220.60947227478027 and batch: 250, loss is 4.668070135116577 and perplexity is 106.49202876982297
At time: 221.7559871673584 and batch: 300, loss is 4.6367675304412845 and perplexity is 103.21018400949994
At time: 222.89171838760376 and batch: 350, loss is 4.585879497528076 and perplexity is 98.08941861757006
At time: 224.0309293270111 and batch: 400, loss is 4.602690954208374 and perplexity is 99.75238389771607
At time: 225.18660879135132 and batch: 450, loss is 4.634216938018799 and perplexity is 102.9472723290434
At time: 226.3188192844391 and batch: 500, loss is 4.631534605026245 and perplexity is 102.67150348126549
At time: 227.45526766777039 and batch: 550, loss is 4.604435186386109 and perplexity is 99.92652704440596
At time: 228.59358978271484 and batch: 600, loss is 4.623569669723511 and perplexity is 101.85697971926048
At time: 229.75522780418396 and batch: 650, loss is 4.5963596534729 and perplexity is 99.12281664900654
At time: 230.8939003944397 and batch: 700, loss is 4.5605214500427245 and perplexity is 95.6333348370255
At time: 232.0313515663147 and batch: 750, loss is 4.55546012878418 and perplexity is 95.15052666070588
At time: 233.1697268486023 and batch: 800, loss is 4.537322912216187 and perplexity is 93.44031705519095
At time: 234.30821299552917 and batch: 850, loss is 4.5830739307403565 and perplexity is 97.8146078826629
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.853983243306478 and perplexity of 128.25022562472898
Finished 11 epochs...
Completing Train Step...
At time: 237.49954628944397 and batch: 50, loss is 4.686034946441651 and perplexity is 108.42242566545163
At time: 238.64048528671265 and batch: 100, loss is 4.6281452560424805 and perplexity is 102.32410298860196
At time: 239.77423548698425 and batch: 150, loss is 4.636056318283081 and perplexity is 103.1368057686235
At time: 240.91140151023865 and batch: 200, loss is 4.6513909816741945 and perplexity is 104.73056259946075
At time: 242.05886912345886 and batch: 250, loss is 4.644091958999634 and perplexity is 103.96891487178698
At time: 243.1913161277771 and batch: 300, loss is 4.616327714920044 and perplexity is 101.12200063060446
At time: 244.3507068157196 and batch: 350, loss is 4.566111469268799 and perplexity is 96.16942399578078
At time: 245.48447060585022 and batch: 400, loss is 4.584441518783569 and perplexity is 97.94846948375388
At time: 246.61842703819275 and batch: 450, loss is 4.618696279525757 and perplexity is 101.36179849842065
At time: 247.74896121025085 and batch: 500, loss is 4.618160181045532 and perplexity is 101.30747315546076
At time: 248.88165831565857 and batch: 550, loss is 4.593671350479126 and perplexity is 98.85670234246788
At time: 250.01889276504517 and batch: 600, loss is 4.615074844360351 and perplexity is 100.99538718476843
At time: 251.16007351875305 and batch: 650, loss is 4.58977237701416 and perplexity is 98.47201311707842
At time: 252.2974705696106 and batch: 700, loss is 4.556378469467163 and perplexity is 95.23794739521037
At time: 253.4362404346466 and batch: 750, loss is 4.553487482070923 and perplexity is 94.9630132966411
At time: 254.57471013069153 and batch: 800, loss is 4.536897859573364 and perplexity is 93.40060844120397
At time: 255.7134895324707 and batch: 850, loss is 4.582499437332153 and perplexity is 97.75843017361517
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.852866490681966 and perplexity of 128.10708179167065
Finished 12 epochs...
Completing Train Step...
At time: 258.99249172210693 and batch: 50, loss is 4.670800409317017 and perplexity is 106.78317848694415
At time: 260.1316006183624 and batch: 100, loss is 4.61442702293396 and perplexity is 100.92998139690624
At time: 261.2693762779236 and batch: 150, loss is 4.622074375152588 and perplexity is 101.70478734504837
At time: 262.40725207328796 and batch: 200, loss is 4.6376842021942135 and perplexity is 103.30483724614224
At time: 263.54548382759094 and batch: 250, loss is 4.6311843204498295 and perplexity is 102.63554553528319
At time: 264.68352723121643 and batch: 300, loss is 4.604321413040161 and perplexity is 99.91515871579381
At time: 265.820529460907 and batch: 350, loss is 4.554729461669922 and perplexity is 95.08102869300225
At time: 266.9588224887848 and batch: 400, loss is 4.574179420471191 and perplexity is 96.94845257231577
At time: 268.10385155677795 and batch: 450, loss is 4.609645872116089 and perplexity is 100.44857169705404
At time: 269.2424771785736 and batch: 500, loss is 4.609896354675293 and perplexity is 100.47373546377183
At time: 270.3781077861786 and batch: 550, loss is 4.58645414352417 and perplexity is 98.14580150778737
At time: 271.5382127761841 and batch: 600, loss is 4.608451728820801 and perplexity is 100.32869329887875
At time: 272.6760971546173 and batch: 650, loss is 4.584160470962525 and perplexity is 97.92094514783935
At time: 273.8509261608124 and batch: 700, loss is 4.5518266963958744 and perplexity is 94.8054309759494
At time: 274.9893000125885 and batch: 750, loss is 4.549887752532959 and perplexity is 94.62178666292863
At time: 276.1459617614746 and batch: 800, loss is 4.5333757495880125 and perplexity is 93.07221987537736
At time: 277.314964056015 and batch: 850, loss is 4.578742504119873 and perplexity is 97.39184732533357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.852388064066569 and perplexity of 128.04580661312764
Finished 13 epochs...
Completing Train Step...
At time: 280.43934988975525 and batch: 50, loss is 4.658928241729736 and perplexity is 105.52292646057789
At time: 281.6108639240265 and batch: 100, loss is 4.6036491966247555 and perplexity is 99.84801667547637
At time: 282.74499559402466 and batch: 150, loss is 4.612018651962281 and perplexity is 100.6871970342943
At time: 283.8828504085541 and batch: 200, loss is 4.6270069408416745 and perplexity is 102.20769217543035
At time: 285.022438287735 and batch: 250, loss is 4.621115102767944 and perplexity is 101.60727153075393
At time: 286.16022276878357 and batch: 300, loss is 4.594347372055053 and perplexity is 98.92355420027957
At time: 287.29721760749817 and batch: 350, loss is 4.54563494682312 and perplexity is 94.22023305828466
At time: 288.43627524375916 and batch: 400, loss is 4.565552644729614 and perplexity is 96.1156971750637
At time: 289.5736811161041 and batch: 450, loss is 4.602140598297119 and perplexity is 99.69749968788621
At time: 290.71294116973877 and batch: 500, loss is 4.602609767913818 and perplexity is 99.74428570003012
At time: 291.84980940818787 and batch: 550, loss is 4.579567308425903 and perplexity is 97.4722096774309
At time: 292.98830556869507 and batch: 600, loss is 4.602361001968384 and perplexity is 99.71947580455279
At time: 294.1276412010193 and batch: 650, loss is 4.578512916564941 and perplexity is 97.36948993582325
At time: 295.26624155044556 and batch: 700, loss is 4.546659021377564 and perplexity is 94.3167710240737
At time: 296.4019570350647 and batch: 750, loss is 4.545190162658692 and perplexity is 94.17833470920341
At time: 297.54457545280457 and batch: 800, loss is 4.528821535110474 and perplexity is 92.64931275997294
At time: 298.67764258384705 and batch: 850, loss is 4.573945722579956 and perplexity is 96.92579857059043
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.852285703023274 and perplexity of 128.03270038156842
Finished 14 epochs...
Completing Train Step...
At time: 301.799991607666 and batch: 50, loss is 4.648392152786255 and perplexity is 104.41696401215407
At time: 302.9998769760132 and batch: 100, loss is 4.594175252914429 and perplexity is 98.9065290283641
At time: 304.1388680934906 and batch: 150, loss is 4.602709102630615 and perplexity is 99.75419426252623
At time: 305.2790741920471 and batch: 200, loss is 4.617431697845459 and perplexity is 101.23369923801742
At time: 306.4185252189636 and batch: 250, loss is 4.6123374080657955 and perplexity is 100.71929680862249
At time: 307.58004665374756 and batch: 300, loss is 4.5854806709289555 and perplexity is 98.05030574847785
At time: 308.7265236377716 and batch: 350, loss is 4.537227573394776 and perplexity is 93.4314089901396
At time: 309.86351346969604 and batch: 400, loss is 4.557914113998413 and perplexity is 95.38431138105945
At time: 311.002357006073 and batch: 450, loss is 4.595199298858643 and perplexity is 99.00786573613844
At time: 312.13879561424255 and batch: 500, loss is 4.595873718261719 and perplexity is 99.07466108335535
At time: 313.2775888442993 and batch: 550, loss is 4.572895765304565 and perplexity is 96.82408403051313
At time: 314.43059754371643 and batch: 600, loss is 4.5962687110900875 and perplexity is 99.1138025937554
At time: 315.59360933303833 and batch: 650, loss is 4.572680568695068 and perplexity is 96.80325005769264
At time: 316.7627203464508 and batch: 700, loss is 4.541551961898803 and perplexity is 93.8363175610827
At time: 317.9011752605438 and batch: 750, loss is 4.5403537940979 and perplexity is 93.72395323592642
At time: 319.03995633125305 and batch: 800, loss is 4.523894424438477 and perplexity is 92.19394209438354
At time: 320.17652678489685 and batch: 850, loss is 4.568635053634644 and perplexity is 96.41242213484045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.852164586385091 and perplexity of 128.01719443035392
Finished 15 epochs...
Completing Train Step...
At time: 323.41316270828247 and batch: 50, loss is 4.639166736602784 and perplexity is 103.45810380533307
At time: 324.5937924385071 and batch: 100, loss is 4.585577297210693 and perplexity is 98.05978044269044
At time: 325.7312214374542 and batch: 150, loss is 4.594672107696534 and perplexity is 98.95568342057943
At time: 326.8705415725708 and batch: 200, loss is 4.608862524032593 and perplexity is 100.36991631221981
At time: 328.0102000236511 and batch: 250, loss is 4.604170179367065 and perplexity is 99.90004932189665
At time: 329.1480162143707 and batch: 300, loss is 4.577575931549072 and perplexity is 97.27829891173384
At time: 330.2839229106903 and batch: 350, loss is 4.5294723796844485 and perplexity is 92.70963268978547
At time: 331.4236261844635 and batch: 400, loss is 4.550731887817383 and perplexity is 94.70169397327122
At time: 332.607773065567 and batch: 450, loss is 4.588479919433594 and perplexity is 98.34482442794946
At time: 333.7442502975464 and batch: 500, loss is 4.5891819190979 and perplexity is 98.41388669969376
At time: 334.8783497810364 and batch: 550, loss is 4.566438064575196 and perplexity is 96.20083760776653
At time: 336.01372027397156 and batch: 600, loss is 4.590121946334839 and perplexity is 98.50644192908722
At time: 337.16403317451477 and batch: 650, loss is 4.566901712417603 and perplexity is 96.24545126027378
At time: 338.3235821723938 and batch: 700, loss is 4.536040802001953 and perplexity is 93.3205930363672
At time: 339.46034836769104 and batch: 750, loss is 4.534980134963989 and perplexity is 93.22166343432909
At time: 340.59645438194275 and batch: 800, loss is 4.518882513046265 and perplexity is 91.73303021526985
At time: 341.74241399765015 and batch: 850, loss is 4.563351640701294 and perplexity is 95.90437878002551
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8523203531901045 and perplexity of 128.03713681285743
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 345.01963090896606 and batch: 50, loss is 4.63327561378479 and perplexity is 102.85041116280887
At time: 346.1584632396698 and batch: 100, loss is 4.581163702011108 and perplexity is 97.62793795643604
At time: 347.29670238494873 and batch: 150, loss is 4.588614253997803 and perplexity is 98.35803642447526
At time: 348.4345808029175 and batch: 200, loss is 4.601167364120483 and perplexity is 99.60051787452745
At time: 349.5733211040497 and batch: 250, loss is 4.593190259933472 and perplexity is 98.80915475586063
At time: 350.71347165107727 and batch: 300, loss is 4.563038339614868 and perplexity is 95.87433654033968
At time: 351.85159850120544 and batch: 350, loss is 4.513411283493042 and perplexity is 91.23250823324561
At time: 352.99696135520935 and batch: 400, loss is 4.532766103744507 and perplexity is 93.01549607585567
At time: 354.14994287490845 and batch: 450, loss is 4.569894161224365 and perplexity is 96.5338922031886
At time: 355.2893588542938 and batch: 500, loss is 4.568665084838867 and perplexity is 96.41531755945554
At time: 356.427038192749 and batch: 550, loss is 4.542560615539551 and perplexity is 93.93101365417603
At time: 357.5663456916809 and batch: 600, loss is 4.561563024520874 and perplexity is 95.7329959710889
At time: 358.7240595817566 and batch: 650, loss is 4.536219072341919 and perplexity is 93.33723081318067
At time: 359.86607599258423 and batch: 700, loss is 4.503782825469971 and perplexity is 90.35829527258926
At time: 361.05227184295654 and batch: 750, loss is 4.499309825897217 and perplexity is 89.9550252421532
At time: 362.1979992389679 and batch: 800, loss is 4.480320777893066 and perplexity is 88.26298094813903
At time: 363.3389894962311 and batch: 850, loss is 4.53030387878418 and perplexity is 92.78675272407962
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.845307350158691 and perplexity of 127.14235321159282
Finished 17 epochs...
Completing Train Step...
At time: 366.49013590812683 and batch: 50, loss is 4.6216325759887695 and perplexity is 101.65986417928227
At time: 367.62571692466736 and batch: 100, loss is 4.569018154144287 and perplexity is 96.44936485883265
At time: 368.7604093551636 and batch: 150, loss is 4.577074775695801 and perplexity is 97.22955953686953
At time: 369.88903284072876 and batch: 200, loss is 4.590787239074707 and perplexity is 98.57199935475454
At time: 371.02055835723877 and batch: 250, loss is 4.584109954833984 and perplexity is 97.91599868572655
At time: 372.152232170105 and batch: 300, loss is 4.555370483398438 and perplexity is 95.14199723735723
At time: 373.28024530410767 and batch: 350, loss is 4.506266384124756 and perplexity is 90.58298429754008
At time: 374.4078161716461 and batch: 400, loss is 4.526660041809082 and perplexity is 92.44926816647302
At time: 375.5359134674072 and batch: 450, loss is 4.564565305709839 and perplexity is 96.02084523004852
At time: 376.664671421051 and batch: 500, loss is 4.564226350784302 and perplexity is 95.98830400691905
At time: 377.79608821868896 and batch: 550, loss is 4.5391763401031495 and perplexity is 93.61366253663944
At time: 378.925217628479 and batch: 600, loss is 4.5595532894134525 and perplexity is 95.54079121316556
At time: 380.0577142238617 and batch: 650, loss is 4.535150213241577 and perplexity is 93.23751976265014
At time: 381.1886649131775 and batch: 700, loss is 4.503452768325806 and perplexity is 90.32847679287181
At time: 382.3232092857361 and batch: 750, loss is 4.500113563537598 and perplexity is 90.02735454487961
At time: 383.456743478775 and batch: 800, loss is 4.481916036605835 and perplexity is 88.40389560529488
At time: 384.59007382392883 and batch: 850, loss is 4.531718311309814 and perplexity is 92.91808618434382
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8450009028116865 and perplexity of 127.10339674411709
Finished 18 epochs...
Completing Train Step...
At time: 387.6943392753601 and batch: 50, loss is 4.615619831085205 and perplexity is 101.05044333112733
At time: 388.82190465927124 and batch: 100, loss is 4.563125429153442 and perplexity is 95.88268655566425
At time: 389.9731044769287 and batch: 150, loss is 4.571289777755737 and perplexity is 96.66871055446231
At time: 391.1093080043793 and batch: 200, loss is 4.585235452651977 and perplexity is 98.02626496918472
At time: 392.2458715438843 and batch: 250, loss is 4.578855247497558 and perplexity is 97.40282823016057
At time: 393.4199526309967 and batch: 300, loss is 4.550967750549316 and perplexity is 94.72403320792334
At time: 394.567587852478 and batch: 350, loss is 4.50237021446228 and perplexity is 90.2307442612427
At time: 395.72414684295654 and batch: 400, loss is 4.523250904083252 and perplexity is 92.13463250152928
At time: 396.8657238483429 and batch: 450, loss is 4.561407384872436 and perplexity is 95.71809728069562
At time: 398.0243649482727 and batch: 500, loss is 4.5614532947540285 and perplexity is 95.7224917880829
At time: 399.15803837776184 and batch: 550, loss is 4.536793928146363 and perplexity is 93.39090168711301
At time: 400.2914607524872 and batch: 600, loss is 4.5578671646118165 and perplexity is 95.37983325127276
At time: 401.42457580566406 and batch: 650, loss is 4.5337136936187745 and perplexity is 93.10367839182355
At time: 402.56085562705994 and batch: 700, loss is 4.502416763305664 and perplexity is 90.23494449578304
At time: 403.6974587440491 and batch: 750, loss is 4.499552803039551 and perplexity is 89.97688491271774
At time: 404.83357644081116 and batch: 800, loss is 4.481737232208252 and perplexity is 88.38809001309396
At time: 405.96729588508606 and batch: 850, loss is 4.5314999771118165 and perplexity is 92.89780120305045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.844583829243978 and perplexity of 127.05039633027323
Finished 19 epochs...
Completing Train Step...
At time: 409.07417345046997 and batch: 50, loss is 4.610512475967408 and perplexity is 100.53565854559511
At time: 410.24416184425354 and batch: 100, loss is 4.558308095932007 and perplexity is 95.4218984803247
At time: 411.39971256256104 and batch: 150, loss is 4.566597175598145 and perplexity is 96.21614543923693
At time: 412.53730034828186 and batch: 200, loss is 4.580784578323364 and perplexity is 97.59093190794918
At time: 413.67507457733154 and batch: 250, loss is 4.574625368118286 and perplexity is 96.99169614809868
At time: 414.8345670700073 and batch: 300, loss is 4.547061491012573 and perplexity is 94.35473830030843
At time: 415.9668529033661 and batch: 350, loss is 4.499021062850952 and perplexity is 89.92905330508576
At time: 417.1101403236389 and batch: 400, loss is 4.520349683761597 and perplexity is 91.8677170109311
At time: 418.2865090370178 and batch: 450, loss is 4.558636417388916 and perplexity is 95.45323268061853
At time: 419.46720576286316 and batch: 500, loss is 4.5589182472229 and perplexity is 95.48013804052661
At time: 420.60337233543396 and batch: 550, loss is 4.534518384933472 and perplexity is 93.17862826490345
At time: 421.7484302520752 and batch: 600, loss is 4.5561183738708495 and perplexity is 95.21317964562184
At time: 422.9188799858093 and batch: 650, loss is 4.5322276306152345 and perplexity is 92.96542321326828
At time: 424.077232837677 and batch: 700, loss is 4.50102611541748 and perplexity is 90.10954667309717
At time: 425.23471450805664 and batch: 750, loss is 4.498604364395142 and perplexity is 89.89158781388936
At time: 426.3852472305298 and batch: 800, loss is 4.480904960632325 and perplexity is 88.31455772178808
At time: 427.55593037605286 and batch: 850, loss is 4.530578546524048 and perplexity is 92.81224175208665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.844607988993327 and perplexity of 127.05346587308287
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 430.67350578308105 and batch: 50, loss is 4.607885904312134 and perplexity is 100.27194092274455
At time: 431.8402998447418 and batch: 100, loss is 4.556636085510254 and perplexity is 95.26248537892384
At time: 432.980838060379 and batch: 150, loss is 4.563608379364013 and perplexity is 95.92900430300911
At time: 434.1261110305786 and batch: 200, loss is 4.57854043006897 and perplexity is 97.37216894853124
At time: 435.26771116256714 and batch: 250, loss is 4.570342855453491 and perplexity is 96.57721612240468
At time: 436.40423822402954 and batch: 300, loss is 4.541285600662231 and perplexity is 93.81132653197021
At time: 437.54480838775635 and batch: 350, loss is 4.493106050491333 and perplexity is 89.3986919366503
At time: 438.6759305000305 and batch: 400, loss is 4.513315200805664 and perplexity is 91.22374278978876
At time: 439.8142132759094 and batch: 450, loss is 4.55054123878479 and perplexity is 94.68364090788472
At time: 440.9538607597351 and batch: 500, loss is 4.550401735305786 and perplexity is 94.67043313186007
At time: 442.09664964675903 and batch: 550, loss is 4.524527759552002 and perplexity is 92.25235024918152
At time: 443.23676562309265 and batch: 600, loss is 4.544096403121948 and perplexity is 94.07538257016608
At time: 444.37355971336365 and batch: 650, loss is 4.51945083618164 and perplexity is 91.78517903590628
At time: 445.52428007125854 and batch: 700, loss is 4.487515249252319 and perplexity is 88.90027618894476
At time: 446.67547035217285 and batch: 750, loss is 4.4837079429626465 and perplexity is 88.56244912189874
At time: 447.81380248069763 and batch: 800, loss is 4.464541053771972 and perplexity is 86.88114661386808
At time: 449.0068826675415 and batch: 850, loss is 4.516438999176025 and perplexity is 91.50915291861268
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.842331568400065 and perplexity of 126.76456769855906
Finished 21 epochs...
Completing Train Step...
At time: 452.1369261741638 and batch: 50, loss is 4.603722066879272 and perplexity is 99.85529289097113
At time: 453.3021516799927 and batch: 100, loss is 4.5523585510253906 and perplexity is 94.85586709446989
At time: 454.4374563694 and batch: 150, loss is 4.559488077163696 and perplexity is 95.53456098637278
At time: 455.5961980819702 and batch: 200, loss is 4.574851131439209 and perplexity is 97.01359578749762
At time: 456.7518754005432 and batch: 250, loss is 4.566954402923584 and perplexity is 96.25052261540407
At time: 457.899391412735 and batch: 300, loss is 4.538245134353637 and perplexity is 93.5265295315265
At time: 459.0373570919037 and batch: 350, loss is 4.490726976394654 and perplexity is 89.1862586218826
At time: 460.18681740760803 and batch: 400, loss is 4.511188039779663 and perplexity is 91.02990143845558
At time: 461.3390281200409 and batch: 450, loss is 4.548852186203003 and perplexity is 94.52385024516087
At time: 462.485063791275 and batch: 500, loss is 4.548950519561767 and perplexity is 94.53314554985057
At time: 463.62557554244995 and batch: 550, loss is 4.523521127700806 and perplexity is 92.15953281940133
At time: 464.7647349834442 and batch: 600, loss is 4.543578643798828 and perplexity is 94.02668677120408
At time: 465.9069697856903 and batch: 650, loss is 4.5191641044616695 and perplexity is 91.75886508635578
At time: 467.0457212924957 and batch: 700, loss is 4.487887001037597 and perplexity is 88.93333116907296
At time: 468.18627738952637 and batch: 750, loss is 4.484523029327392 and perplexity is 88.6346645935315
At time: 469.3265027999878 and batch: 800, loss is 4.465380773544312 and perplexity is 86.95413307031983
At time: 470.46555066108704 and batch: 850, loss is 4.5173194694519045 and perplexity is 91.58975948836539
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.842250823974609 and perplexity of 126.75433257959203
Finished 22 epochs...
Completing Train Step...
At time: 473.62920117378235 and batch: 50, loss is 4.601296300888062 and perplexity is 99.61336087130077
At time: 474.76710772514343 and batch: 100, loss is 4.549886302947998 and perplexity is 94.62164950070915
At time: 475.90353298187256 and batch: 150, loss is 4.55703444480896 and perplexity is 95.3004416354043
At time: 477.04056572914124 and batch: 200, loss is 4.572685251235962 and perplexity is 96.80370334393093
At time: 478.20572447776794 and batch: 250, loss is 4.564815053939819 and perplexity is 96.04482926104592
At time: 479.3421609401703 and batch: 300, loss is 4.536301307678222 and perplexity is 93.3449067473584
At time: 480.5017385482788 and batch: 350, loss is 4.489218282699585 and perplexity is 89.05180532573614
At time: 481.6489083766937 and batch: 400, loss is 4.509885597229004 and perplexity is 90.91141739751734
At time: 482.7934331893921 and batch: 450, loss is 4.547657775878906 and perplexity is 94.41101738033804
At time: 483.93969917297363 and batch: 500, loss is 4.547919225692749 and perplexity is 94.4357043503181
At time: 485.08144783973694 and batch: 550, loss is 4.5228071212768555 and perplexity is 92.09375380705323
At time: 486.21531438827515 and batch: 600, loss is 4.543162527084351 and perplexity is 93.98756883460979
At time: 487.3761270046234 and batch: 650, loss is 4.5189313793182375 and perplexity is 91.73751297600025
At time: 488.51453614234924 and batch: 700, loss is 4.488024005889892 and perplexity is 88.94551630166598
At time: 489.65530347824097 and batch: 750, loss is 4.484861268997192 and perplexity is 88.66464942396011
At time: 490.7895667552948 and batch: 800, loss is 4.465699348449707 and perplexity is 86.98183888799124
At time: 491.9257893562317 and batch: 850, loss is 4.517637510299682 and perplexity is 91.61889340576313
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.842290878295898 and perplexity of 126.75940974003439
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 495.1043667793274 and batch: 50, loss is 4.60000322341919 and perplexity is 99.48463632210279
At time: 496.25020122528076 and batch: 100, loss is 4.548880405426026 and perplexity is 94.52651767240805
At time: 497.3894383907318 and batch: 150, loss is 4.5554674625396725 and perplexity is 95.15122447396219
At time: 498.52777910232544 and batch: 200, loss is 4.571731414794922 and perplexity is 96.71141246625224
At time: 499.66644167900085 and batch: 250, loss is 4.56291163444519 and perplexity is 95.86218953582096
At time: 500.8055114746094 and batch: 300, loss is 4.534007959365844 and perplexity is 93.13107964672812
At time: 501.9676785469055 and batch: 350, loss is 4.4869742107391355 and perplexity is 88.85219072491081
At time: 503.103887796402 and batch: 400, loss is 4.506676988601685 and perplexity is 90.6201857134375
At time: 504.2374656200409 and batch: 450, loss is 4.544302892684937 and perplexity is 94.09481016052929
At time: 505.36832642555237 and batch: 500, loss is 4.544535913467407 and perplexity is 94.11673876162999
At time: 506.5706043243408 and batch: 550, loss is 4.519083433151245 and perplexity is 91.7514630770351
At time: 507.6977391242981 and batch: 600, loss is 4.538473443984985 and perplexity is 93.54788497673925
At time: 508.8488848209381 and batch: 650, loss is 4.513817596435547 and perplexity is 91.26958471393485
At time: 510.0046761035919 and batch: 700, loss is 4.482612886428833 and perplexity is 88.465521313738
At time: 511.1655647754669 and batch: 750, loss is 4.479155254364014 and perplexity is 88.16016829402217
At time: 512.3138825893402 and batch: 800, loss is 4.459392728805542 and perplexity is 86.43500366750561
At time: 513.4550933837891 and batch: 850, loss is 4.512051963806153 and perplexity is 91.10857833797571
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.842507044474284 and perplexity of 126.78681379902095
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 516.6147384643555 and batch: 50, loss is 4.599055347442627 and perplexity is 99.3903819031027
At time: 517.771228313446 and batch: 100, loss is 4.547624969482422 and perplexity is 94.40792014587426
At time: 518.9058113098145 and batch: 150, loss is 4.554446849822998 and perplexity is 95.05416146455327
At time: 520.058351278305 and batch: 200, loss is 4.57071270942688 and perplexity is 96.61294219583424
At time: 521.2012021541595 and batch: 250, loss is 4.56175142288208 and perplexity is 95.75103360972061
At time: 522.3463990688324 and batch: 300, loss is 4.5328222751617435 and perplexity is 93.02072103484052
At time: 523.4846813678741 and batch: 350, loss is 4.485980825424194 and perplexity is 88.7639700892411
At time: 524.6228559017181 and batch: 400, loss is 4.50518349647522 and perplexity is 90.48494619429206
At time: 525.7583167552948 and batch: 450, loss is 4.543160810470581 and perplexity is 93.98740749439338
At time: 526.8985726833344 and batch: 500, loss is 4.543468580245972 and perplexity is 94.01633842949286
At time: 528.037017583847 and batch: 550, loss is 4.517859449386597 and perplexity is 91.6392294759107
At time: 529.1759333610535 and batch: 600, loss is 4.536821508407593 and perplexity is 93.39347746809828
At time: 530.3142690658569 and batch: 650, loss is 4.512047080993653 and perplexity is 91.10813347295664
At time: 531.4685690402985 and batch: 700, loss is 4.480621728897095 and perplexity is 88.28954777833052
At time: 532.6029908657074 and batch: 750, loss is 4.477026004791259 and perplexity is 87.97265299762543
At time: 533.7414307594299 and batch: 800, loss is 4.45724193572998 and perplexity is 86.24929963719013
At time: 534.8785426616669 and batch: 850, loss is 4.510060148239136 and perplexity is 90.92728746228424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.842039744059245 and perplexity of 126.72758010935297
Finished 25 epochs...
Completing Train Step...
At time: 538.0126359462738 and batch: 50, loss is 4.598584079742432 and perplexity is 99.34355346163441
At time: 539.170768737793 and batch: 100, loss is 4.546963787078857 and perplexity is 94.34551992155501
At time: 540.3192400932312 and batch: 150, loss is 4.553833875656128 and perplexity is 94.99591357317217
At time: 541.4553189277649 and batch: 200, loss is 4.570130004882812 and perplexity is 96.55666179441624
At time: 542.592447757721 and batch: 250, loss is 4.561225872039795 and perplexity is 95.7007247944355
At time: 543.7266316413879 and batch: 300, loss is 4.532370729446411 and perplexity is 92.97872740855463
At time: 544.8649291992188 and batch: 350, loss is 4.485590772628784 and perplexity is 88.72935420602597
At time: 546.0015294551849 and batch: 400, loss is 4.504929141998291 and perplexity is 90.46193386990082
At time: 547.1342177391052 and batch: 450, loss is 4.543087091445923 and perplexity is 93.9804790897635
At time: 548.2743153572083 and batch: 500, loss is 4.543446073532104 and perplexity is 94.01422245447685
At time: 549.4134078025818 and batch: 550, loss is 4.517859239578247 and perplexity is 91.63921024923715
At time: 550.5526263713837 and batch: 600, loss is 4.536867847442627 and perplexity is 93.39780533199637
At time: 551.693158864975 and batch: 650, loss is 4.512077074050904 and perplexity is 91.11086612540002
At time: 552.8283605575562 and batch: 700, loss is 4.480628290176392 and perplexity is 88.29012707261293
At time: 553.9620878696442 and batch: 750, loss is 4.477193374633789 and perplexity is 87.98737819894747
At time: 555.0980536937714 and batch: 800, loss is 4.457506971359253 and perplexity is 86.27216180410437
At time: 556.230144739151 and batch: 850, loss is 4.5102691745758055 and perplexity is 90.94629564662216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.842001914978027 and perplexity of 126.72278621210742
Finished 26 epochs...
Completing Train Step...
At time: 559.3461191654205 and batch: 50, loss is 4.598141632080078 and perplexity is 99.2996088609452
At time: 560.5249176025391 and batch: 100, loss is 4.546498880386353 and perplexity is 94.30166825219281
At time: 561.6623065471649 and batch: 150, loss is 4.553356447219849 and perplexity is 94.95057064756432
At time: 562.8001782894135 and batch: 200, loss is 4.569694757461548 and perplexity is 96.51464490089992
At time: 563.9378318786621 and batch: 250, loss is 4.560793190002442 and perplexity is 95.6593257668089
At time: 565.0995042324066 and batch: 300, loss is 4.531987228393555 and perplexity is 92.94307680515912
At time: 566.2355115413666 and batch: 350, loss is 4.4852683734893795 and perplexity is 88.70075254941248
At time: 567.3687963485718 and batch: 400, loss is 4.5047170925140385 and perplexity is 90.44275349714516
At time: 568.5032205581665 and batch: 450, loss is 4.543034543991089 and perplexity is 93.97554078453211
At time: 569.6388850212097 and batch: 500, loss is 4.543438053131103 and perplexity is 94.01346842573678
At time: 570.7982671260834 and batch: 550, loss is 4.517866039276123 and perplexity is 91.63983337029903
At time: 571.968523979187 and batch: 600, loss is 4.536901407241821 and perplexity is 93.40093979618416
At time: 573.1049816608429 and batch: 650, loss is 4.512116384506226 and perplexity is 91.11444780543047
At time: 574.2423205375671 and batch: 700, loss is 4.480633611679077 and perplexity is 88.29059691001139
At time: 575.3801512718201 and batch: 750, loss is 4.4773182201385495 and perplexity is 87.99836371332307
At time: 576.515909910202 and batch: 800, loss is 4.457717161178589 and perplexity is 86.29029724008296
At time: 577.657233953476 and batch: 850, loss is 4.510426025390625 and perplexity is 90.96056176599616
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.841996510823567 and perplexity of 126.7221013844476
Finished 27 epochs...
Completing Train Step...
At time: 580.7374410629272 and batch: 50, loss is 4.597742128372192 and perplexity is 99.25994622222677
At time: 581.874477148056 and batch: 100, loss is 4.546088705062866 and perplexity is 94.26299596666293
At time: 583.0127108097076 and batch: 150, loss is 4.552928628921509 and perplexity is 94.90995774409458
At time: 584.1639928817749 and batch: 200, loss is 4.569313554763794 and perplexity is 96.47786026953676
At time: 585.300882101059 and batch: 250, loss is 4.5604033851623536 and perplexity is 95.6220445652939
At time: 586.4397044181824 and batch: 300, loss is 4.531636590957642 and perplexity is 92.91049319587273
At time: 587.5740752220154 and batch: 350, loss is 4.4849793815612795 and perplexity is 88.6751224515334
At time: 588.7106332778931 and batch: 400, loss is 4.504523363113403 and perplexity is 90.4252337738159
At time: 589.843864440918 and batch: 450, loss is 4.542986679077148 and perplexity is 93.97104276100953
At time: 590.9789478778839 and batch: 500, loss is 4.543430423736572 and perplexity is 94.01275116263105
At time: 592.1142311096191 and batch: 550, loss is 4.51787052154541 and perplexity is 91.6402441256301
At time: 593.2516458034515 and batch: 600, loss is 4.536925430297852 and perplexity is 93.40318359914559
At time: 594.4295461177826 and batch: 650, loss is 4.512149353027343 and perplexity is 91.11745176354482
At time: 595.5619549751282 and batch: 700, loss is 4.48063479423523 and perplexity is 88.2907013186617
At time: 596.698447227478 and batch: 750, loss is 4.4774181842803955 and perplexity is 88.0071608339265
At time: 597.8378438949585 and batch: 800, loss is 4.457892665863037 and perplexity is 86.30544292050104
At time: 598.9956834316254 and batch: 850, loss is 4.510551567077637 and perplexity is 90.97198182520351
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.842000325520833 and perplexity of 126.72258479182328
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 602.0805952548981 and batch: 50, loss is 4.597501544952393 and perplexity is 99.23606879728696
At time: 603.2127385139465 and batch: 100, loss is 4.545827360153198 and perplexity is 94.23836403135265
At time: 604.349750995636 and batch: 150, loss is 4.5527598094940185 and perplexity is 94.8939364517558
At time: 605.4868562221527 and batch: 200, loss is 4.568959579467774 and perplexity is 96.4437155339414
At time: 606.629371881485 and batch: 250, loss is 4.560011520385742 and perplexity is 95.58458099496738
At time: 607.7657120227814 and batch: 300, loss is 4.5312318515777585 and perplexity is 92.87289626946358
At time: 608.9027254581451 and batch: 350, loss is 4.484703416824341 and perplexity is 88.65065462097755
At time: 610.0409061908722 and batch: 400, loss is 4.5039842987060545 and perplexity is 90.37650188475578
At time: 611.1772747039795 and batch: 450, loss is 4.54260817527771 and perplexity is 93.93548109482477
At time: 612.3121902942657 and batch: 500, loss is 4.54328724861145 and perplexity is 93.99929183876345
At time: 613.4511911869049 and batch: 550, loss is 4.517523002624512 and perplexity is 91.60840293990846
At time: 614.5894932746887 and batch: 600, loss is 4.536268453598023 and perplexity is 93.34184003668315
At time: 615.7295002937317 and batch: 650, loss is 4.511394453048706 and perplexity is 91.04869315735554
At time: 616.8674762248993 and batch: 700, loss is 4.4798768043518065 and perplexity is 88.22380321750677
At time: 618.0056862831116 and batch: 750, loss is 4.476577272415161 and perplexity is 87.93318567580897
At time: 619.1449475288391 and batch: 800, loss is 4.457133951187134 and perplexity is 86.23998654884186
At time: 620.2837355136871 and batch: 850, loss is 4.509752597808838 and perplexity is 90.89932703574038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.841846466064453 and perplexity of 126.70308882367414
Finished 29 epochs...
Completing Train Step...
At time: 623.3724887371063 and batch: 50, loss is 4.59730523109436 and perplexity is 99.21658929387617
At time: 624.5348236560822 and batch: 100, loss is 4.545615539550782 and perplexity is 94.21840451830545
At time: 625.6658143997192 and batch: 150, loss is 4.552542142868042 and perplexity is 94.87328345659813
At time: 626.8231670856476 and batch: 200, loss is 4.5688026237487795 and perplexity is 96.42857932911534
At time: 627.9567828178406 and batch: 250, loss is 4.559859104156494 and perplexity is 95.57001346375023
At time: 629.0931830406189 and batch: 300, loss is 4.531108484268189 and perplexity is 92.86143949682901
At time: 630.227207660675 and batch: 350, loss is 4.484595441818238 and perplexity is 88.64108308275654
At time: 631.3593258857727 and batch: 400, loss is 4.503895835876465 and perplexity is 90.36850727728866
At time: 632.5127048492432 and batch: 450, loss is 4.54259295463562 and perplexity is 93.93405134736835
At time: 633.6678783893585 and batch: 500, loss is 4.543313617706299 and perplexity is 94.0017705476862
At time: 634.8131856918335 and batch: 550, loss is 4.517535276412964 and perplexity is 91.60952732896683
At time: 635.9534702301025 and batch: 600, loss is 4.536266002655029 and perplexity is 93.34161126143468
At time: 637.0985350608826 and batch: 650, loss is 4.5113801097869874 and perplexity is 91.04738723148607
At time: 638.236850976944 and batch: 700, loss is 4.479874954223633 and perplexity is 88.22363999231388
At time: 639.3739740848541 and batch: 750, loss is 4.476633338928223 and perplexity is 87.93811592112175
At time: 640.5118134021759 and batch: 800, loss is 4.45723292350769 and perplexity is 86.248522342832
At time: 641.6476094722748 and batch: 850, loss is 4.509810886383057 and perplexity is 90.90462558233162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.841823577880859 and perplexity of 126.7001888533029
Finished 30 epochs...
Completing Train Step...
At time: 644.7375011444092 and batch: 50, loss is 4.597148542404175 and perplexity is 99.20104439434182
At time: 645.91992020607 and batch: 100, loss is 4.545454711914062 and perplexity is 94.2032528134104
At time: 647.0589866638184 and batch: 150, loss is 4.552386140823364 and perplexity is 94.85848418478194
At time: 648.2005586624146 and batch: 200, loss is 4.568652877807617 and perplexity is 96.41414062184461
At time: 649.3357696533203 and batch: 250, loss is 4.5597154331207275 and perplexity is 95.55628380722823
At time: 650.4800925254822 and batch: 300, loss is 4.530987424850464 and perplexity is 92.85019842546691
At time: 651.6298146247864 and batch: 350, loss is 4.4844943618774415 and perplexity is 88.632123700141
At time: 652.7671120166779 and batch: 400, loss is 4.503819808959961 and perplexity is 90.36163709949386
At time: 653.950677394867 and batch: 450, loss is 4.542585411071777 and perplexity is 93.93334275252766
At time: 655.0870685577393 and batch: 500, loss is 4.543351621627807 and perplexity is 94.0053430514799
At time: 656.228485584259 and batch: 550, loss is 4.517553653717041 and perplexity is 91.61121088057641
At time: 657.375785112381 and batch: 600, loss is 4.536260404586792 and perplexity is 93.34108873018803
At time: 658.5208749771118 and batch: 650, loss is 4.511366987228394 and perplexity is 91.04619246465155
At time: 659.6574692726135 and batch: 700, loss is 4.479869832992554 and perplexity is 88.22318817982375
At time: 660.796085357666 and batch: 750, loss is 4.476681461334229 and perplexity is 87.94234781666317
At time: 661.9333171844482 and batch: 800, loss is 4.457316732406616 and perplexity is 86.25575103943385
At time: 663.0704429149628 and batch: 850, loss is 4.509858846664429 and perplexity is 90.90898549830312
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.84181563059489 and perplexity of 126.6991819346708
Finished 31 epochs...
Completing Train Step...
At time: 666.2010171413422 and batch: 50, loss is 4.5970006275177 and perplexity is 99.18637216826919
At time: 667.3376104831696 and batch: 100, loss is 4.5453054809570315 and perplexity is 94.18919582073298
At time: 668.4750142097473 and batch: 150, loss is 4.552240333557129 and perplexity is 94.84465413680901
At time: 669.612710237503 and batch: 200, loss is 4.568512458801269 and perplexity is 96.4006031944989
At time: 670.7490148544312 and batch: 250, loss is 4.559577207565308 and perplexity is 95.54307639964681
At time: 671.8869712352753 and batch: 300, loss is 4.5308700275421145 and perplexity is 92.8392987019037
At time: 673.0188248157501 and batch: 350, loss is 4.484398241043091 and perplexity is 88.62360471589317
At time: 674.1539902687073 and batch: 400, loss is 4.503747777938843 and perplexity is 90.35512849291726
At time: 675.303950548172 and batch: 450, loss is 4.542578086853028 and perplexity is 93.93265476669694
At time: 676.4396841526031 and batch: 500, loss is 4.543389081954956 and perplexity is 94.00886458834287
At time: 677.5842912197113 and batch: 550, loss is 4.517571220397949 and perplexity is 91.6128201996207
At time: 678.7357399463654 and batch: 600, loss is 4.536253004074097 and perplexity is 93.34039796083194
At time: 679.8836436271667 and batch: 650, loss is 4.511354293823242 and perplexity is 91.04503678577782
At time: 681.027881860733 and batch: 700, loss is 4.4798636150360105 and perplexity is 88.222639613579
At time: 682.2119584083557 and batch: 750, loss is 4.476723670959473 and perplexity is 87.94605990855004
At time: 683.3614635467529 and batch: 800, loss is 4.457393455505371 and perplexity is 86.26236910181493
At time: 684.4971611499786 and batch: 850, loss is 4.509900989532471 and perplexity is 90.91281674441214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.841813087463379 and perplexity of 126.69885972239855
Finished 32 epochs...
Completing Train Step...
At time: 687.6403474807739 and batch: 50, loss is 4.596858797073364 and perplexity is 99.17230551859559
At time: 688.7796053886414 and batch: 100, loss is 4.545163135528565 and perplexity is 94.17578937349282
At time: 689.9125773906708 and batch: 150, loss is 4.552099924087525 and perplexity is 94.83133798410573
At time: 691.0463175773621 and batch: 200, loss is 4.5683785247802735 and perplexity is 96.38769273868058
At time: 692.1818766593933 and batch: 250, loss is 4.55944354057312 and perplexity is 95.5303062974897
At time: 693.3171112537384 and batch: 300, loss is 4.530755596160889 and perplexity is 92.82867558054203
At time: 694.4529612064362 and batch: 350, loss is 4.484305953979492 and perplexity is 88.61542628103604
At time: 695.5844559669495 and batch: 400, loss is 4.5036776638031 and perplexity is 90.34879354326041
At time: 696.717206954956 and batch: 450, loss is 4.542570676803589 and perplexity is 93.93195872366005
At time: 697.8440058231354 and batch: 500, loss is 4.543424949645996 and perplexity is 94.0122365297245
At time: 698.9664075374603 and batch: 550, loss is 4.517587919235229 and perplexity is 91.61435003997123
At time: 700.0891101360321 and batch: 600, loss is 4.536243915557861 and perplexity is 93.33954963896463
At time: 701.2140312194824 and batch: 650, loss is 4.511341705322265 and perplexity is 91.04389067245727
At time: 702.3461549282074 and batch: 700, loss is 4.47985655784607 and perplexity is 88.22201701185108
At time: 703.4948568344116 and batch: 750, loss is 4.476761569976807 and perplexity is 87.9493930409598
At time: 704.6446115970612 and batch: 800, loss is 4.457464942932129 and perplexity is 86.26853599703315
At time: 705.778650522232 and batch: 850, loss is 4.509938793182373 and perplexity is 90.91625364567125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.841811498006185 and perplexity of 126.6986583401445
Finished 33 epochs...
Completing Train Step...
At time: 708.8744461536407 and batch: 50, loss is 4.596721506118774 and perplexity is 99.15869099269908
At time: 710.060802936554 and batch: 100, loss is 4.545025844573974 and perplexity is 94.1628607769804
At time: 711.1971542835236 and batch: 150, loss is 4.5519637107849125 and perplexity is 94.81842157408117
At time: 712.3802633285522 and batch: 200, loss is 4.568249206542969 and perplexity is 96.37522885807869
At time: 713.5189068317413 and batch: 250, loss is 4.559313507080078 and perplexity is 95.5178849656825
At time: 714.6569035053253 and batch: 300, loss is 4.530643682479859 and perplexity is 92.81828736305535
At time: 715.7948663234711 and batch: 350, loss is 4.484216527938843 and perplexity is 88.60750210864227
At time: 716.9332232475281 and batch: 400, loss is 4.503609504699707 and perplexity is 90.34263566036012
At time: 718.0727314949036 and batch: 450, loss is 4.542562961578369 and perplexity is 93.9312340202388
At time: 719.2042932510376 and batch: 500, loss is 4.543459396362305 and perplexity is 94.01547499834277
At time: 720.3415234088898 and batch: 550, loss is 4.517603034973145 and perplexity is 91.61573486894207
At time: 721.4804127216339 and batch: 600, loss is 4.536233797073364 and perplexity is 93.33860518895682
At time: 722.6196076869965 and batch: 650, loss is 4.511328954696655 and perplexity is 91.04272981329403
At time: 723.7557787895203 and batch: 700, loss is 4.479849090576172 and perplexity is 88.22135823669878
At time: 724.8890013694763 and batch: 750, loss is 4.47679630279541 and perplexity is 87.95244782432489
At time: 726.0215978622437 and batch: 800, loss is 4.457532157897949 and perplexity is 86.27433472861019
At time: 727.1630659103394 and batch: 850, loss is 4.5099731540679935 and perplexity is 90.91937766233548
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.841812451680501 and perplexity of 126.6987791694585
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 730.306426525116 and batch: 50, loss is 4.5966331672668455 and perplexity is 99.1499318146716
At time: 731.4485354423523 and batch: 100, loss is 4.544926290512085 and perplexity is 94.15348694832002
At time: 732.585134267807 and batch: 150, loss is 4.551908855438232 and perplexity is 94.81322041935094
At time: 733.7279989719391 and batch: 200, loss is 4.568117914199829 and perplexity is 96.36257635906752
At time: 734.8772737979889 and batch: 250, loss is 4.559177484512329 and perplexity is 95.50489326130605
At time: 736.0202157497406 and batch: 300, loss is 4.530522594451904 and perplexity is 92.80704886011823
At time: 737.1765151023865 and batch: 350, loss is 4.484161138534546 and perplexity is 88.60259432780501
At time: 738.3145945072174 and batch: 400, loss is 4.503405923843384 and perplexity is 90.32424550123558
At time: 739.4518699645996 and batch: 450, loss is 4.542387609481811 and perplexity is 93.91476442545229
At time: 740.5870776176453 and batch: 500, loss is 4.543429565429688 and perplexity is 94.01267047087414
At time: 741.7674171924591 and batch: 550, loss is 4.517488355636597 and perplexity is 91.60522903966223
At time: 742.9003489017487 and batch: 600, loss is 4.5359800434112545 and perplexity is 93.31492318089914
At time: 744.0367081165314 and batch: 650, loss is 4.511000347137451 and perplexity is 91.01281739905875
At time: 745.1748008728027 and batch: 700, loss is 4.479521188735962 and perplexity is 88.1924350332321
At time: 746.3129811286926 and batch: 750, loss is 4.476508255004883 and perplexity is 87.92711696448187
At time: 747.4502484798431 and batch: 800, loss is 4.45721508026123 and perplexity is 86.24698340292093
At time: 748.5885303020477 and batch: 850, loss is 4.509676170349121 and perplexity is 90.89238009655695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.841789881388347 and perplexity of 126.69591957326813
Finished 35 epochs...
Completing Train Step...
At time: 751.7212827205658 and batch: 50, loss is 4.596575555801391 and perplexity is 99.14421980634025
At time: 752.8577833175659 and batch: 100, loss is 4.544849576950074 and perplexity is 94.14626437599861
At time: 753.992317199707 and batch: 150, loss is 4.55181694984436 and perplexity is 94.80450695443558
At time: 755.1536378860474 and batch: 200, loss is 4.568080034255981 and perplexity is 96.35892621922002
At time: 756.2875854969025 and batch: 250, loss is 4.559121036529541 and perplexity is 95.49950235488943
At time: 757.4321799278259 and batch: 300, loss is 4.530481433868408 and perplexity is 92.80322894645005
At time: 758.5828049182892 and batch: 350, loss is 4.484130983352661 and perplexity is 88.5999225407419
At time: 759.7187030315399 and batch: 400, loss is 4.5033728218078615 and perplexity is 90.32125563433807
At time: 760.8551847934723 and batch: 450, loss is 4.542374486923218 and perplexity is 93.91353203153942
At time: 761.9896926879883 and batch: 500, loss is 4.543415718078613 and perplexity is 94.01136865343405
At time: 763.1292498111725 and batch: 550, loss is 4.517486019134521 and perplexity is 91.60501500410452
At time: 764.2706270217896 and batch: 600, loss is 4.535979976654053 and perplexity is 93.31491695145623
At time: 765.3973579406738 and batch: 650, loss is 4.5109948062896725 and perplexity is 91.01231311228871
At time: 766.525897026062 and batch: 700, loss is 4.479521112442017 and perplexity is 88.19242830468355
At time: 767.6545190811157 and batch: 750, loss is 4.476524925231933 and perplexity is 87.92858274170294
At time: 768.7864434719086 and batch: 800, loss is 4.457259302139282 and perplexity is 86.25079749083575
At time: 769.9226183891296 and batch: 850, loss is 4.509697971343994 and perplexity is 90.89436166246935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.84177303314209 and perplexity of 126.69378498719749
Finished 36 epochs...
Completing Train Step...
At time: 773.0482485294342 and batch: 50, loss is 4.596522274017334 and perplexity is 99.1389373661602
At time: 774.2076923847198 and batch: 100, loss is 4.544789972305298 and perplexity is 94.14065298858758
At time: 775.3604111671448 and batch: 150, loss is 4.5517551040649415 and perplexity is 94.79864387711575
At time: 776.4917266368866 and batch: 200, loss is 4.568034381866455 and perplexity is 96.35452730439717
At time: 777.6255745887756 and batch: 250, loss is 4.559073219299316 and perplexity is 95.4949359423764
At time: 778.7591905593872 and batch: 300, loss is 4.53044358253479 and perplexity is 92.79971628695024
At time: 779.8931963443756 and batch: 350, loss is 4.48410120010376 and perplexity is 88.5972837864918
At time: 781.0291199684143 and batch: 400, loss is 4.503344516754151 and perplexity is 90.3186991225274
At time: 782.1596398353577 and batch: 450, loss is 4.5423682498931885 and perplexity is 93.91294629184661
At time: 783.3141841888428 and batch: 500, loss is 4.543425102233886 and perplexity is 94.01225087485439
At time: 784.4480142593384 and batch: 550, loss is 4.51749207496643 and perplexity is 91.6055697503571
At time: 785.5797810554504 and batch: 600, loss is 4.535977230072022 and perplexity is 93.31466065473406
At time: 786.7181694507599 and batch: 650, loss is 4.510986404418945 and perplexity is 91.0115484418117
At time: 787.867192029953 and batch: 700, loss is 4.4795176219940185 and perplexity is 88.19212047413595
At time: 788.9966371059418 and batch: 750, loss is 4.476541528701782 and perplexity is 87.93004267339526
At time: 790.1241633892059 and batch: 800, loss is 4.457290544509887 and perplexity is 86.25349221231049
At time: 791.275298833847 and batch: 850, loss is 4.5097145748138425 and perplexity is 90.89587083679137
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.84176508585612 and perplexity of 126.69277811945855
Finished 37 epochs...
Completing Train Step...
At time: 794.3908190727234 and batch: 50, loss is 4.59647050857544 and perplexity is 99.13380552808555
At time: 795.5539910793304 and batch: 100, loss is 4.544735012054443 and perplexity is 94.13547913686304
At time: 796.6894958019257 and batch: 150, loss is 4.5517012405395505 and perplexity is 94.79353782547044
At time: 797.8272943496704 and batch: 200, loss is 4.567987661361695 and perplexity is 96.35002567740554
At time: 798.9864296913147 and batch: 250, loss is 4.559027099609375 and perplexity is 95.49053184709835
At time: 800.1671848297119 and batch: 300, loss is 4.530406551361084 and perplexity is 92.79627986816425
At time: 801.3045582771301 and batch: 350, loss is 4.484072370529175 and perplexity is 88.59472960130903
At time: 802.4420156478882 and batch: 400, loss is 4.503317365646362 and perplexity is 90.31624690308259
At time: 803.5787124633789 and batch: 450, loss is 4.542362833023072 and perplexity is 93.91243757899207
At time: 804.7063980102539 and batch: 500, loss is 4.543438444137573 and perplexity is 94.01350518561837
At time: 805.8633964061737 and batch: 550, loss is 4.5174991893768315 and perplexity is 91.60622147229365
At time: 806.9955558776855 and batch: 600, loss is 4.535973348617554 and perplexity is 93.31429845883045
At time: 808.1286182403564 and batch: 650, loss is 4.510977325439453 and perplexity is 91.01072215358073
At time: 809.2627005577087 and batch: 700, loss is 4.4795133495330814 and perplexity is 88.19174367755119
At time: 810.3972833156586 and batch: 750, loss is 4.47655668258667 and perplexity is 87.93137516523635
At time: 811.5335772037506 and batch: 800, loss is 4.457317962646484 and perplexity is 86.25585715476291
At time: 812.6721711158752 and batch: 850, loss is 4.509729509353638 and perplexity is 90.89722833492837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.841760953267415 and perplexity of 126.69225455139653
Finished 38 epochs...
Completing Train Step...
At time: 815.9173629283905 and batch: 50, loss is 4.596419925689697 and perplexity is 99.12879118094847
At time: 817.0580387115479 and batch: 100, loss is 4.544681901931763 and perplexity is 94.13047972277846
At time: 818.1996579170227 and batch: 150, loss is 4.551650152206421 and perplexity is 94.78869510533582
At time: 819.3398480415344 and batch: 200, loss is 4.567941474914551 and perplexity is 96.34557571480202
At time: 820.4868609905243 and batch: 250, loss is 4.558981561660767 and perplexity is 95.4861835031746
At time: 821.6259558200836 and batch: 300, loss is 4.530369644165039 and perplexity is 92.79285508087094
At time: 822.764820098877 and batch: 350, loss is 4.48404411315918 and perplexity is 88.59222618262525
At time: 823.9029290676117 and batch: 400, loss is 4.503291053771973 and perplexity is 90.3138705446021
At time: 825.0407218933105 and batch: 450, loss is 4.542357788085938 and perplexity is 93.9119637978435
At time: 826.1767146587372 and batch: 500, loss is 4.543452663421631 and perplexity is 94.0148419998581
At time: 827.3114349842072 and batch: 550, loss is 4.517506637573242 and perplexity is 91.6069037759646
At time: 828.4486346244812 and batch: 600, loss is 4.535968656539917 and perplexity is 93.31386062192465
At time: 829.6332068443298 and batch: 650, loss is 4.510967893600464 and perplexity is 91.00986375915124
At time: 830.7687885761261 and batch: 700, loss is 4.479508581161499 and perplexity is 88.19132314754948
At time: 831.9050707817078 and batch: 750, loss is 4.4765707206726075 and perplexity is 87.93260956210176
At time: 833.0403561592102 and batch: 800, loss is 4.457343339920044 and perplexity is 86.25804612102095
At time: 834.1757457256317 and batch: 850, loss is 4.509743576049805 and perplexity is 90.89850696761482
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.841759045918782 and perplexity of 126.69201290532848
Finished 39 epochs...
Completing Train Step...
At time: 837.273683309555 and batch: 50, loss is 4.596370601654053 and perplexity is 99.12390186950016
At time: 838.4072155952454 and batch: 100, loss is 4.544630355834961 and perplexity is 94.12562778900889
At time: 839.5385324954987 and batch: 150, loss is 4.551600427627563 and perplexity is 94.78398189457339
At time: 840.6692976951599 and batch: 200, loss is 4.567896041870117 and perplexity is 96.34119854141454
At time: 841.8008077144623 and batch: 250, loss is 4.558936395645142 and perplexity is 95.48187087011145
At time: 842.9345986843109 and batch: 300, loss is 4.530333347320557 and perplexity is 92.78948705416576
At time: 844.068253993988 and batch: 350, loss is 4.484016189575195 and perplexity is 88.5897524046956
At time: 845.2014796733856 and batch: 400, loss is 4.503265399932861 and perplexity is 90.31155367681599
At time: 846.3335144519806 and batch: 450, loss is 4.542352590560913 and perplexity is 93.91147568933003
At time: 847.4670786857605 and batch: 500, loss is 4.543466739654541 and perplexity is 94.01616538398521
At time: 848.5979120731354 and batch: 550, loss is 4.517513780593872 and perplexity is 91.60755812830514
At time: 849.7292668819427 and batch: 600, loss is 4.535963726043701 and perplexity is 93.31340053942219
At time: 850.8635954856873 and batch: 650, loss is 4.510958318710327 and perplexity is 91.0089923538762
At time: 851.9998853206635 and batch: 700, loss is 4.479503831863403 and perplexity is 88.19090430166098
At time: 853.1630086898804 and batch: 750, loss is 4.476583948135376 and perplexity is 87.93377269511353
At time: 854.2974452972412 and batch: 800, loss is 4.457367763519287 and perplexity is 86.26015287869814
At time: 855.4272272586823 and batch: 850, loss is 4.509757165908813 and perplexity is 90.89974227390242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.841757456461589 and perplexity of 126.69181153395725
Finished 40 epochs...
Completing Train Step...
At time: 858.7186810970306 and batch: 50, loss is 4.59632173538208 and perplexity is 99.11905817230007
At time: 859.9014201164246 and batch: 100, loss is 4.544579448699952 and perplexity is 94.12083624493009
At time: 861.0358152389526 and batch: 150, loss is 4.551551675796508 and perplexity is 94.77936111453802
At time: 862.1736323833466 and batch: 200, loss is 4.567851142883301 and perplexity is 96.33687301631791
At time: 863.3090434074402 and batch: 250, loss is 4.558891773223877 and perplexity is 95.47761033290479
At time: 864.4465434551239 and batch: 300, loss is 4.530297012329101 and perplexity is 92.78611561019754
At time: 865.5789413452148 and batch: 350, loss is 4.483988771438598 and perplexity is 88.58732347206166
At time: 866.7151954174042 and batch: 400, loss is 4.50323971748352 and perplexity is 90.3092342846978
At time: 867.8525927066803 and batch: 450, loss is 4.542347412109375 and perplexity is 93.91098937456351
At time: 868.9898951053619 and batch: 500, loss is 4.543480615615845 and perplexity is 94.01746995770915
At time: 870.129106760025 and batch: 550, loss is 4.517520990371704 and perplexity is 91.60821860082788
At time: 871.2646856307983 and batch: 600, loss is 4.535958614349365 and perplexity is 93.31292355106028
At time: 872.4016897678375 and batch: 650, loss is 4.510948390960693 and perplexity is 91.00808884387058
At time: 873.5376708507538 and batch: 700, loss is 4.479498767852784 and perplexity is 88.19045770311583
At time: 874.6735053062439 and batch: 750, loss is 4.47659670829773 and perplexity is 87.93489475148826
At time: 875.8053736686707 and batch: 800, loss is 4.457391319274902 and perplexity is 86.26218482571055
At time: 876.9820077419281 and batch: 850, loss is 4.509770336151123 and perplexity is 90.90093945341765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.841759363810222 and perplexity of 126.69205317964122
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 880.1620352268219 and batch: 50, loss is 4.596290254592896 and perplexity is 99.11593787524056
At time: 881.300567150116 and batch: 100, loss is 4.544539470672607 and perplexity is 94.11707355477796
At time: 882.4407227039337 and batch: 150, loss is 4.5515247821807865 and perplexity is 94.7768121890969
At time: 883.5756096839905 and batch: 200, loss is 4.567805652618408 and perplexity is 96.33249072612199
At time: 884.7137153148651 and batch: 250, loss is 4.558839988708496 and perplexity is 95.4726661991399
At time: 885.8506717681885 and batch: 300, loss is 4.530260171890259 and perplexity is 92.78269739194468
At time: 887.0036268234253 and batch: 350, loss is 4.483978309631348 and perplexity is 88.58639669340653
At time: 888.1442995071411 and batch: 400, loss is 4.503165073394776 and perplexity is 90.3024934857829
At time: 889.3302972316742 and batch: 450, loss is 4.54226881980896 and perplexity is 93.90360898389903
At time: 890.4668045043945 and batch: 500, loss is 4.543444690704345 and perplexity is 94.01409244909019
At time: 891.6001174449921 and batch: 550, loss is 4.5174631500244145 and perplexity is 91.6029201028844
At time: 892.7362408638 and batch: 600, loss is 4.535861921310425 and perplexity is 93.3039012771123
At time: 893.8735346794128 and batch: 650, loss is 4.510828475952149 and perplexity is 90.99717626242351
At time: 895.0074415206909 and batch: 700, loss is 4.479378509521484 and perplexity is 88.17985270351848
At time: 896.1444351673126 and batch: 750, loss is 4.476490697860718 and perplexity is 87.92557322896513
At time: 897.2801902294159 and batch: 800, loss is 4.457254877090454 and perplexity is 86.25041582768986
At time: 898.4172005653381 and batch: 850, loss is 4.509655523300171 and perplexity is 90.89050345650946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8417510986328125 and perplexity of 126.69100605167272
Finished 42 epochs...
Completing Train Step...
At time: 901.5351083278656 and batch: 50, loss is 4.59627332687378 and perplexity is 99.11426008268491
At time: 902.6694355010986 and batch: 100, loss is 4.54451768875122 and perplexity is 94.11502352640751
At time: 903.8021712303162 and batch: 150, loss is 4.551499681472778 and perplexity is 94.7744332538648
At time: 904.9340455532074 and batch: 200, loss is 4.567790813446045 and perplexity is 96.33106124229417
At time: 906.0695838928223 and batch: 250, loss is 4.558822202682495 and perplexity is 95.47096813491741
At time: 907.2074611186981 and batch: 300, loss is 4.5302466201782225 and perplexity is 92.78144003606737
At time: 908.345778465271 and batch: 350, loss is 4.483967723846436 and perplexity is 88.58545894182845
At time: 909.4829046726227 and batch: 400, loss is 4.503154792785645 and perplexity is 90.30156512591587
At time: 910.6261026859283 and batch: 450, loss is 4.5422656536102295 and perplexity is 93.90331166688215
At time: 911.7968063354492 and batch: 500, loss is 4.543443717956543 and perplexity is 94.01400099713285
At time: 912.9308369159698 and batch: 550, loss is 4.517464303970337 and perplexity is 91.60302580776155
At time: 914.0638737678528 and batch: 600, loss is 4.535861711502076 and perplexity is 93.30388170117685
At time: 915.1992971897125 and batch: 650, loss is 4.5108253765106205 and perplexity is 90.99689422243353
At time: 916.337812423706 and batch: 700, loss is 4.479376955032349 and perplexity is 88.179715629002
At time: 917.4753065109253 and batch: 750, loss is 4.476496162414551 and perplexity is 87.92605370430617
At time: 918.6754260063171 and batch: 800, loss is 4.457268323898315 and perplexity is 86.25157562825721
At time: 919.8335132598877 and batch: 850, loss is 4.509662475585937 and perplexity is 90.89113535545954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.841746012369792 and perplexity of 126.69036166953235
Finished 43 epochs...
Completing Train Step...
At time: 922.9301881790161 and batch: 50, loss is 4.596256265640259 and perplexity is 99.11256908557367
At time: 924.1284506320953 and batch: 100, loss is 4.5444970798492434 and perplexity is 94.11308393909955
At time: 925.2866303920746 and batch: 150, loss is 4.5514773178100585 and perplexity is 94.7723137741048
At time: 926.449316740036 and batch: 200, loss is 4.567775535583496 and perplexity is 96.32958952082369
At time: 927.587749004364 and batch: 250, loss is 4.558805322647094 and perplexity is 95.46935659519703
At time: 928.7254898548126 and batch: 300, loss is 4.530233507156372 and perplexity is 92.78022339899375
At time: 929.8607876300812 and batch: 350, loss is 4.483957595825196 and perplexity is 88.5845617509621
At time: 930.9908628463745 and batch: 400, loss is 4.503144807815552 and perplexity is 90.30066347199028
At time: 932.1425974369049 and batch: 450, loss is 4.542262678146362 and perplexity is 93.90303226138694
At time: 933.2776203155518 and batch: 500, loss is 4.543445558547973 and perplexity is 94.01417403865668
At time: 934.4121434688568 and batch: 550, loss is 4.517466125488281 and perplexity is 91.60319266446871
At time: 935.5441899299622 and batch: 600, loss is 4.53586106300354 and perplexity is 93.3038211937658
At time: 936.6975002288818 and batch: 650, loss is 4.510822143554687 and perplexity is 90.99660003396
At time: 937.8534324169159 and batch: 700, loss is 4.479375162124634 and perplexity is 88.1795575310513
At time: 938.9966206550598 and batch: 750, loss is 4.476501407623291 and perplexity is 87.92651489602108
At time: 940.147002696991 and batch: 800, loss is 4.457279739379882 and perplexity is 86.25256023714881
At time: 941.2938785552979 and batch: 850, loss is 4.5096684837341305 and perplexity is 90.8916814445107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.841743151346843 and perplexity of 126.68999920601868
Finished 44 epochs...
Completing Train Step...
At time: 944.3776395320892 and batch: 50, loss is 4.596239013671875 and perplexity is 99.11085921341474
At time: 945.551017999649 and batch: 100, loss is 4.5444772338867185 and perplexity is 94.11121619289626
At time: 946.6774988174438 and batch: 150, loss is 4.55145679473877 and perplexity is 94.77036877511176
At time: 947.833948135376 and batch: 200, loss is 4.567759923934936 and perplexity is 96.32808566886501
At time: 948.9618189334869 and batch: 250, loss is 4.5587888431549075 and perplexity is 95.46778332164435
At time: 950.0905582904816 and batch: 300, loss is 4.5302206230163575 and perplexity is 92.77902801330569
At time: 951.217999458313 and batch: 350, loss is 4.483947610855102 and perplexity is 88.58367724117818
At time: 952.3500869274139 and batch: 400, loss is 4.5031350803375245 and perplexity is 90.29978507854277
At time: 953.4872601032257 and batch: 450, loss is 4.542259922027588 and perplexity is 93.90277345383343
At time: 954.6216857433319 and batch: 500, loss is 4.543448543548584 and perplexity is 94.0144546714424
At time: 955.755574464798 and batch: 550, loss is 4.517468318939209 and perplexity is 91.60339359179707
At time: 956.8951196670532 and batch: 600, loss is 4.535860023498535 and perplexity is 93.30372420402712
At time: 958.0293054580688 and batch: 650, loss is 4.51081862449646 and perplexity is 90.99627981218946
At time: 959.166734457016 and batch: 700, loss is 4.479373397827149 and perplexity is 88.17940195621696
At time: 960.3066911697388 and batch: 750, loss is 4.476506595611572 and perplexity is 87.9269710589332
At time: 961.4447782039642 and batch: 800, loss is 4.457289915084839 and perplexity is 86.25343792221909
At time: 962.5824851989746 and batch: 850, loss is 4.509673957824707 and perplexity is 90.89217899516937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8417402903238935 and perplexity of 126.68963674354202
Finished 45 epochs...
Completing Train Step...
At time: 965.6946203708649 and batch: 50, loss is 4.59622181892395 and perplexity is 99.10915504182537
At time: 966.8639538288116 and batch: 100, loss is 4.544458112716675 and perplexity is 94.10941669353274
At time: 968.0187146663666 and batch: 150, loss is 4.5514372444152835 and perplexity is 94.76851600185651
At time: 969.1516010761261 and batch: 200, loss is 4.567744293212891 and perplexity is 96.3265800031001
At time: 970.2812972068787 and batch: 250, loss is 4.558772897720337 and perplexity is 95.46626105848836
At time: 971.4135637283325 and batch: 300, loss is 4.530208101272583 and perplexity is 92.77786626536285
At time: 972.550954580307 and batch: 350, loss is 4.483938131332398 and perplexity is 88.58283751417864
At time: 973.6927478313446 and batch: 400, loss is 4.503125610351563 and perplexity is 90.2989299448948
At time: 974.8346083164215 and batch: 450, loss is 4.542257308959961 and perplexity is 93.90252807985661
At time: 975.9708077907562 and batch: 500, loss is 4.54345232963562 and perplexity is 94.01481061902432
At time: 977.1485035419464 and batch: 550, loss is 4.517470636367798 and perplexity is 91.60360587636617
At time: 978.2826099395752 and batch: 600, loss is 4.535858755111694 and perplexity is 93.30360585888619
At time: 979.41930103302 and batch: 650, loss is 4.51081503868103 and perplexity is 90.9959535169102
At time: 980.5537912845612 and batch: 700, loss is 4.479371404647827 and perplexity is 88.17922619903152
At time: 981.6887345314026 and batch: 750, loss is 4.476511383056641 and perplexity is 87.92739200548485
At time: 982.8254325389862 and batch: 800, loss is 4.457299108505249 and perplexity is 86.25423088998075
At time: 983.96386551857 and batch: 850, loss is 4.509678974151611 and perplexity is 90.89263494119582
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.841738382975261 and perplexity of 126.68939510246703
Finished 46 epochs...
Completing Train Step...
At time: 987.0987467765808 and batch: 50, loss is 4.596204614639282 and perplexity is 99.10744995437628
At time: 988.2325086593628 and batch: 100, loss is 4.544439172744751 and perplexity is 94.10763428070223
At time: 989.3639621734619 and batch: 150, loss is 4.551418561935424 and perplexity is 94.76674550750369
At time: 990.4982867240906 and batch: 200, loss is 4.567728414535522 and perplexity is 96.32505047655768
At time: 991.6330893039703 and batch: 250, loss is 4.558757019042969 and perplexity is 95.46474519256452
At time: 992.7677035331726 and batch: 300, loss is 4.53019528388977 and perplexity is 92.77667710355534
At time: 993.9013540744781 and batch: 350, loss is 4.483928537368774 and perplexity is 88.5819876577346
At time: 995.0337133407593 and batch: 400, loss is 4.5031161403656 and perplexity is 90.2980748193448
At time: 996.1675269603729 and batch: 450, loss is 4.542254667282105 and perplexity is 93.9022800199552
At time: 997.3058347702026 and batch: 500, loss is 4.543456554412842 and perplexity is 94.01520781149372
At time: 998.4397268295288 and batch: 550, loss is 4.517473106384277 and perplexity is 91.6038321390617
At time: 999.5850877761841 and batch: 600, loss is 4.535857238769531 and perplexity is 93.30346437880193
At time: 1000.7176995277405 and batch: 650, loss is 4.510811424255371 and perplexity is 90.99562461939533
At time: 1001.854505777359 and batch: 700, loss is 4.47936939239502 and perplexity is 88.1790487603146
At time: 1002.995897769928 and batch: 750, loss is 4.476516151428223 and perplexity is 87.92781127696179
At time: 1004.1390783786774 and batch: 800, loss is 4.457307929992676 and perplexity is 86.25499178395015
At time: 1005.2902851104736 and batch: 850, loss is 4.50968379020691 and perplexity is 90.89307268620604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.841736475626628 and perplexity of 126.68915346185294
Finished 47 epochs...
Completing Train Step...
At time: 1008.4301714897156 and batch: 50, loss is 4.596187400817871 and perplexity is 99.10574395111573
At time: 1009.6020443439484 and batch: 100, loss is 4.544420642852783 and perplexity is 94.10589049256185
At time: 1010.7394342422485 and batch: 150, loss is 4.551400423049927 and perplexity is 94.7650265599479
At time: 1011.8773550987244 and batch: 200, loss is 4.567712507247925 and perplexity is 96.32351821846399
At time: 1013.0134332180023 and batch: 250, loss is 4.558741216659546 and perplexity is 95.46323663397703
At time: 1014.1469779014587 and batch: 300, loss is 4.530182733535766 and perplexity is 92.77551273072105
At time: 1015.2804691791534 and batch: 350, loss is 4.483919134140015 and perplexity is 88.58115470495694
At time: 1016.4366366863251 and batch: 400, loss is 4.503106803894043 and perplexity is 90.29723175787316
At time: 1017.5744659900665 and batch: 450, loss is 4.542252044677735 and perplexity is 93.90203375174819
At time: 1018.7125942707062 and batch: 500, loss is 4.543460760116577 and perplexity is 94.01560321243588
At time: 1019.8687825202942 and batch: 550, loss is 4.517475519180298 and perplexity is 91.60405316069
At time: 1020.9990694522858 and batch: 600, loss is 4.535855522155762 and perplexity is 93.30330421292773
At time: 1022.1335823535919 and batch: 650, loss is 4.510807647705078 and perplexity is 90.99528097049142
At time: 1023.2709341049194 and batch: 700, loss is 4.479367198944092 and perplexity is 88.17885534411043
At time: 1024.39471077919 and batch: 750, loss is 4.476520795822143 and perplexity is 87.9282196493022
At time: 1025.547515153885 and batch: 800, loss is 4.457316112518311 and perplexity is 86.2556975705191
At time: 1026.686308145523 and batch: 850, loss is 4.509688386917114 and perplexity is 90.89349049628105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.841735204060872 and perplexity of 126.68899236836619
Finished 48 epochs...
Completing Train Step...
At time: 1029.8176584243774 and batch: 50, loss is 4.596170253753662 and perplexity is 99.10404459313031
At time: 1030.9579124450684 and batch: 100, loss is 4.54440242767334 and perplexity is 94.10417635249162
At time: 1032.098884344101 and batch: 150, loss is 4.551382551193237 and perplexity is 94.76333294810806
At time: 1033.2449715137482 and batch: 200, loss is 4.567696886062622 and perplexity is 96.32201354268933
At time: 1034.3909475803375 and batch: 250, loss is 4.558725605010986 and perplexity is 95.46174630710959
At time: 1035.5656838417053 and batch: 300, loss is 4.530170316696167 and perplexity is 92.77436075921267
At time: 1036.7794902324677 and batch: 350, loss is 4.483909873962403 and perplexity is 88.58033443152922
At time: 1037.9195659160614 and batch: 400, loss is 4.50309739112854 and perplexity is 90.29638181520521
At time: 1039.0761442184448 and batch: 450, loss is 4.54224946975708 and perplexity is 93.90179196177328
At time: 1040.2143778800964 and batch: 500, loss is 4.5434652614593505 and perplexity is 94.01602640984444
At time: 1041.3505749702454 and batch: 550, loss is 4.517477874755859 and perplexity is 91.60426894121308
At time: 1042.4853253364563 and batch: 600, loss is 4.535853881835937 and perplexity is 93.30315116579362
At time: 1043.6406767368317 and batch: 650, loss is 4.510803747177124 and perplexity is 90.99492604154652
At time: 1044.7762689590454 and batch: 700, loss is 4.479365081787109 and perplexity is 88.1786686558287
At time: 1045.9115092754364 and batch: 750, loss is 4.476525220870972 and perplexity is 87.92860873682844
At time: 1047.049390554428 and batch: 800, loss is 4.457324209213257 and perplexity is 86.25639595941705
At time: 1048.1826393604279 and batch: 850, loss is 4.509693078994751 and perplexity is 90.89391697659565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.841735204060872 and perplexity of 126.68899236836619
Finished 49 epochs...
Completing Train Step...
At time: 1051.3843786716461 and batch: 50, loss is 4.596153507232666 and perplexity is 99.10238495906327
At time: 1052.518914937973 and batch: 100, loss is 4.54438422203064 and perplexity is 94.10246314107546
At time: 1053.6846063137054 and batch: 150, loss is 4.551365041732788 and perplexity is 94.76167370780406
At time: 1054.8209917545319 and batch: 200, loss is 4.5676813888549805 and perplexity is 96.32052083201145
At time: 1055.9571669101715 and batch: 250, loss is 4.558709869384765 and perplexity is 95.46024416856991
At time: 1057.0938429832458 and batch: 300, loss is 4.530157814025879 and perplexity is 92.77320083921994
At time: 1058.2317283153534 and batch: 350, loss is 4.48390061378479 and perplexity is 88.57951416569735
At time: 1059.3684194087982 and batch: 400, loss is 4.503088188171387 and perplexity is 90.2955508252961
At time: 1060.5058658123016 and batch: 450, loss is 4.542246923446656 and perplexity is 93.90155285896593
At time: 1061.661569595337 and batch: 500, loss is 4.543469848632813 and perplexity is 94.01645767865499
At time: 1062.8025228977203 and batch: 550, loss is 4.517480392456054 and perplexity is 91.6044995735892
At time: 1063.937261581421 and batch: 600, loss is 4.535852088928222 and perplexity is 93.30298388200406
At time: 1065.0759766101837 and batch: 650, loss is 4.510799913406372 and perplexity is 90.99457718852923
At time: 1066.2595963478088 and batch: 700, loss is 4.479363069534302 and perplexity is 88.1784912182337
At time: 1067.3959143161774 and batch: 750, loss is 4.476529607772827 and perplexity is 87.92899447185133
At time: 1068.531737089157 and batch: 800, loss is 4.457332162857056 and perplexity is 86.25708201479418
At time: 1069.6662831306458 and batch: 850, loss is 4.509697570800781 and perplexity is 90.89432525535699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8417361577351885 and perplexity of 126.68911318846199
Annealing...
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fe9bf57c6a0>
SETTINGS FOR THIS RUN
{'dropout': 0.28488080350002143, 'batch_size': 50, 'data': 'wikitext', 'lr': 14.935155926639776, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'anneal': 7.043080580538206, 'wordvec_source': '', 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.6799561977386475 and batch: 50, loss is 6.955013818740845 and perplexity is 1048.3930253237072
At time: 2.8416779041290283 and batch: 100, loss is 6.027770938873291 and perplexity is 414.7894071876903
At time: 3.994324207305908 and batch: 150, loss is 5.857153759002686 and perplexity is 349.727317859147
At time: 5.1296546459198 and batch: 200, loss is 5.826330966949463 and perplexity is 339.1121799507771
At time: 6.261823654174805 and batch: 250, loss is 5.85622278213501 and perplexity is 349.4018813266727
At time: 7.398120880126953 and batch: 300, loss is 5.781718740463257 and perplexity is 324.3161269157094
At time: 8.531870365142822 and batch: 350, loss is 5.742068061828613 and perplexity is 311.7083770673103
At time: 9.668713092803955 and batch: 400, loss is 5.757162094116211 and perplexity is 316.4490009356671
At time: 10.799969911575317 and batch: 450, loss is 5.749599399566651 and perplexity is 314.064820578433
At time: 11.933345794677734 and batch: 500, loss is 5.746306123733521 and perplexity is 313.0322197477364
At time: 13.073089361190796 and batch: 550, loss is 5.708463277816772 and perplexity is 301.4075325697125
At time: 14.209115982055664 and batch: 600, loss is 5.733604097366333 and perplexity is 309.0812221977542
At time: 15.344293117523193 and batch: 650, loss is 5.745073814392089 and perplexity is 312.64670480479145
At time: 16.47809410095215 and batch: 700, loss is 5.691774997711182 and perplexity is 296.4192975395032
At time: 17.614280939102173 and batch: 750, loss is 5.696093511581421 and perplexity is 297.7021564155299
At time: 18.746920108795166 and batch: 800, loss is 5.703031387329101 and perplexity is 299.7747584005144
At time: 19.882787704467773 and batch: 850, loss is 5.700309858322144 and perplexity is 298.9600218674047
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.220744768778483 and perplexity of 185.07196857948486
Finished 1 epochs...
Completing Train Step...
At time: 22.997957229614258 and batch: 50, loss is 5.525019683837891 and perplexity is 250.8912764754548
At time: 24.13170027732849 and batch: 100, loss is 5.438171129226685 and perplexity is 230.02111963939362
At time: 25.275550365447998 and batch: 150, loss is 5.427989463806153 and perplexity is 227.69100391840712
At time: 26.411216259002686 and batch: 200, loss is 5.432971181869507 and perplexity is 228.82812636549642
At time: 27.545534372329712 and batch: 250, loss is 5.477379941940308 and perplexity is 239.21911759535607
At time: 28.675776481628418 and batch: 300, loss is 5.43895622253418 and perplexity is 230.20177858879157
At time: 29.81128430366516 and batch: 350, loss is 5.410573987960816 and perplexity is 223.75998636565726
At time: 30.9517662525177 and batch: 400, loss is 5.422455997467041 and perplexity is 226.43456285560862
At time: 32.0990035533905 and batch: 450, loss is 5.418286418914795 and perplexity is 225.49239175205184
At time: 33.25453972816467 and batch: 500, loss is 5.438059587478637 and perplexity is 229.9954641124789
At time: 34.38993859291077 and batch: 550, loss is 5.41101110458374 and perplexity is 223.8578169553738
At time: 35.526607036590576 and batch: 600, loss is 5.45530797958374 and perplexity is 233.99692624908107
At time: 36.66055178642273 and batch: 650, loss is 5.457345294952392 and perplexity is 234.47413773322953
At time: 37.79472517967224 and batch: 700, loss is 5.418881282806397 and perplexity is 225.6265689383611
At time: 38.923524379730225 and batch: 750, loss is 5.422042675018311 and perplexity is 226.3409917064724
At time: 40.057799339294434 and batch: 800, loss is 5.428165407180786 and perplexity is 227.73106816642627
At time: 41.19223952293396 and batch: 850, loss is 5.412353010177612 and perplexity is 224.15841465393734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.242170651753743 and perplexity of 189.08008429731083
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 44.3084397315979 and batch: 50, loss is 5.427319355010987 and perplexity is 227.53847728452823
At time: 45.4369101524353 and batch: 100, loss is 5.313399114608765 and perplexity is 203.0392103064269
At time: 46.57030367851257 and batch: 150, loss is 5.287286930084228 and perplexity is 197.80603514722134
At time: 47.71917104721069 and batch: 200, loss is 5.28871973991394 and perplexity is 198.0896567181224
At time: 48.85440421104431 and batch: 250, loss is 5.300949039459229 and perplexity is 200.52702776040897
At time: 49.987409591674805 and batch: 300, loss is 5.249398574829102 and perplexity is 190.45169156622205
At time: 51.13244438171387 and batch: 350, loss is 5.199923086166382 and perplexity is 181.25830006826575
At time: 52.27118468284607 and batch: 400, loss is 5.21739649772644 and perplexity is 184.45333372109496
At time: 53.42168617248535 and batch: 450, loss is 5.215394735336304 and perplexity is 184.08447128556296
At time: 54.55257487297058 and batch: 500, loss is 5.20098087310791 and perplexity is 181.4501341730399
At time: 55.68651294708252 and batch: 550, loss is 5.154235677719116 and perplexity is 173.16340351841467
At time: 56.849689960479736 and batch: 600, loss is 5.154386529922485 and perplexity is 173.1895275697641
At time: 57.98417782783508 and batch: 650, loss is 5.134270935058594 and perplexity is 169.74052288777168
At time: 59.14039444923401 and batch: 700, loss is 5.070772008895874 and perplexity is 159.29725878597898
At time: 60.29328203201294 and batch: 750, loss is 5.0379540061950685 and perplexity is 154.1542934244571
At time: 61.43426847457886 and batch: 800, loss is 5.015983753204345 and perplexity is 150.80441814739817
At time: 62.568960666656494 and batch: 850, loss is 5.027759704589844 and perplexity is 152.59078106232622
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.967286427815755 and perplexity of 143.63658980537065
Finished 3 epochs...
Completing Train Step...
At time: 65.6446487903595 and batch: 50, loss is 5.2089306926727295 and perplexity is 182.898379014947
At time: 66.80182838439941 and batch: 100, loss is 5.133685064315796 and perplexity is 169.64110600710907
At time: 67.95792126655579 and batch: 150, loss is 5.116047096252442 and perplexity is 166.67521463558907
At time: 69.0889344215393 and batch: 200, loss is 5.137161111831665 and perplexity is 170.2318126195731
At time: 70.22095394134521 and batch: 250, loss is 5.1529625797271725 and perplexity is 172.94308980734547
At time: 71.35290718078613 and batch: 300, loss is 5.116932067871094 and perplexity is 166.82278275724312
At time: 72.49557065963745 and batch: 350, loss is 5.066404905319214 and perplexity is 158.60310797484075
At time: 73.62783789634705 and batch: 400, loss is 5.0903346061706545 and perplexity is 162.44420779508718
At time: 74.75916624069214 and batch: 450, loss is 5.098599014282226 and perplexity is 163.7922758431275
At time: 75.88670372962952 and batch: 500, loss is 5.085050640106201 and perplexity is 161.5881218700658
At time: 77.01833748817444 and batch: 550, loss is 5.049615726470948 and perplexity is 155.96252070149788
At time: 78.15157222747803 and batch: 600, loss is 5.064286699295044 and perplexity is 158.2675094749387
At time: 79.28333616256714 and batch: 650, loss is 5.049198293685913 and perplexity is 155.89743041847473
At time: 80.43122982978821 and batch: 700, loss is 4.995545701980591 and perplexity is 147.75355279405966
At time: 81.5696313381195 and batch: 750, loss is 4.980238218307495 and perplexity is 145.50904044260648
At time: 82.70199251174927 and batch: 800, loss is 4.969473848342895 and perplexity is 143.95112731784823
At time: 83.8327693939209 and batch: 850, loss is 4.993787527084351 and perplexity is 147.49400443926692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.941586494445801 and perplexity of 139.99217027606258
Finished 4 epochs...
Completing Train Step...
At time: 86.94708347320557 and batch: 50, loss is 5.119462013244629 and perplexity is 167.24536962011777
At time: 88.08199191093445 and batch: 100, loss is 5.052251768112183 and perplexity is 156.37418674657843
At time: 89.21539402008057 and batch: 150, loss is 5.033186168670654 and perplexity is 153.42106015726557
At time: 90.348623752594 and batch: 200, loss is 5.058163442611694 and perplexity is 157.30135790557688
At time: 91.48249506950378 and batch: 250, loss is 5.072928457260132 and perplexity is 159.64114575309875
At time: 92.61670780181885 and batch: 300, loss is 5.041290264129639 and perplexity is 154.6694507801154
At time: 93.75509858131409 and batch: 350, loss is 4.991470928192139 and perplexity is 147.15271545944228
At time: 94.88855695724487 and batch: 400, loss is 5.017962999343872 and perplexity is 151.10319278658025
At time: 96.02209997177124 and batch: 450, loss is 5.028533229827881 and perplexity is 152.7088595450253
At time: 97.15427279472351 and batch: 500, loss is 5.016278629302978 and perplexity is 150.84889332288893
At time: 98.28981685638428 and batch: 550, loss is 4.985639038085938 and perplexity is 146.29703453781238
At time: 99.42607045173645 and batch: 600, loss is 5.006915397644043 and perplexity is 149.44305206872139
At time: 100.56781959533691 and batch: 650, loss is 4.991353406906128 and perplexity is 147.1354228992233
At time: 101.70838809013367 and batch: 700, loss is 4.944485158920288 and perplexity is 140.39854930042705
At time: 102.86428928375244 and batch: 750, loss is 4.935242691040039 and perplexity is 139.1068984336428
At time: 104.00456762313843 and batch: 800, loss is 4.9234796905517575 and perplexity is 137.48017027872615
At time: 105.14382743835449 and batch: 850, loss is 4.951700677871704 and perplexity is 141.41526133498587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.92262617746989 and perplexity of 137.36287921674412
Finished 5 epochs...
Completing Train Step...
At time: 108.26355338096619 and batch: 50, loss is 5.052268476486206 and perplexity is 156.37679952680574
At time: 109.40145301818848 and batch: 100, loss is 4.987243146896362 and perplexity is 146.53189922374727
At time: 110.5379478931427 and batch: 150, loss is 4.968899154663086 and perplexity is 143.86842328179694
At time: 111.67228841781616 and batch: 200, loss is 4.992108993530273 and perplexity is 147.24663846791083
At time: 112.80543041229248 and batch: 250, loss is 5.005860805511475 and perplexity is 149.28553367517083
At time: 113.94183301925659 and batch: 300, loss is 4.978983983993531 and perplexity is 145.32665241366314
At time: 115.10364937782288 and batch: 350, loss is 4.926740188598632 and perplexity is 137.92915566563465
At time: 116.2406644821167 and batch: 400, loss is 4.956162738800049 and perplexity is 142.04767473153572
At time: 117.37923955917358 and batch: 450, loss is 4.968362684249878 and perplexity is 143.79126282831209
At time: 118.51455450057983 and batch: 500, loss is 4.959239568710327 and perplexity is 142.48540433026838
At time: 119.65574479103088 and batch: 550, loss is 4.934188032150269 and perplexity is 138.96026544406612
At time: 120.78883957862854 and batch: 600, loss is 4.9592776870727535 and perplexity is 142.49083574406876
At time: 121.92739605903625 and batch: 650, loss is 4.9430749797821045 and perplexity is 140.20070172821855
At time: 123.0616569519043 and batch: 700, loss is 4.898816003799438 and perplexity is 134.1308751857392
At time: 124.19486260414124 and batch: 750, loss is 4.8923617935180665 and perplexity is 133.26795404486032
At time: 125.32877993583679 and batch: 800, loss is 4.88353199005127 and perplexity is 132.0964040918302
At time: 126.46597361564636 and batch: 850, loss is 4.91345401763916 and perplexity is 136.1087253759949
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.908423105875651 and perplexity of 135.425693964397
Finished 6 epochs...
Completing Train Step...
At time: 129.63354396820068 and batch: 50, loss is 4.994478578567505 and perplexity is 147.59596561594458
At time: 130.76983308792114 and batch: 100, loss is 4.933837833404541 and perplexity is 138.9116102533923
At time: 131.90744495391846 and batch: 150, loss is 4.9191495800018314 and perplexity is 136.8861529514891
At time: 133.04472303390503 and batch: 200, loss is 4.94067232131958 and perplexity is 139.86425167489725
At time: 134.182776927948 and batch: 250, loss is 4.952808990478515 and perplexity is 141.57208053821637
At time: 135.32054352760315 and batch: 300, loss is 4.927117366790771 and perplexity is 137.98118934758
At time: 136.45901656150818 and batch: 350, loss is 4.87468544960022 and perplexity is 130.93296171730526
At time: 137.59704422950745 and batch: 400, loss is 4.9076003074646 and perplexity is 135.31431174742693
At time: 138.7325303554535 and batch: 450, loss is 4.91997597694397 and perplexity is 136.99932200454845
At time: 139.8725438117981 and batch: 500, loss is 4.914323844909668 and perplexity is 136.22716796190883
At time: 141.00748801231384 and batch: 550, loss is 4.89055534362793 and perplexity is 133.02742947711457
At time: 142.14325046539307 and batch: 600, loss is 4.919988164901733 and perplexity is 137.00099175667404
At time: 143.28412795066833 and batch: 650, loss is 4.902749509811401 and perplexity is 134.6595188195096
At time: 144.4935929775238 and batch: 700, loss is 4.860518474578857 and perplexity is 129.09111522110348
At time: 145.6319706439972 and batch: 750, loss is 4.8573455810546875 and perplexity is 128.68217196736927
At time: 146.76876831054688 and batch: 800, loss is 4.848230037689209 and perplexity is 127.51449414284926
At time: 147.90556263923645 and batch: 850, loss is 4.877047309875488 and perplexity is 131.24257256316008
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8972422281901045 and perplexity of 133.91994930445153
Finished 7 epochs...
Completing Train Step...
At time: 150.99770188331604 and batch: 50, loss is 4.944706993103027 and perplexity is 140.4296979526591
At time: 152.16122341156006 and batch: 100, loss is 4.887688655853271 and perplexity is 132.64662745237538
At time: 153.29858660697937 and batch: 150, loss is 4.875708084106446 and perplexity is 131.06692676892712
At time: 154.43547129631042 and batch: 200, loss is 4.898723840713501 and perplexity is 134.11851384000127
At time: 155.57183742523193 and batch: 250, loss is 4.906875066757202 and perplexity is 135.21621187756347
At time: 156.70713067054749 and batch: 300, loss is 4.883077478408813 and perplexity is 132.03637838046947
At time: 157.84504008293152 and batch: 350, loss is 4.83270414352417 and perplexity is 125.55000727192852
At time: 158.9836187362671 and batch: 400, loss is 4.8648358249664305 and perplexity is 129.64965162855285
At time: 160.12127041816711 and batch: 450, loss is 4.876606016159058 and perplexity is 131.184668817779
At time: 161.25891971588135 and batch: 500, loss is 4.8729381275177 and perplexity is 130.70437942356935
At time: 162.39140558242798 and batch: 550, loss is 4.853188943862915 and perplexity is 128.14839698844682
At time: 163.5292398929596 and batch: 600, loss is 4.885934867858887 and perplexity is 132.41419726586435
At time: 164.66596341133118 and batch: 650, loss is 4.867995882034302 and perplexity is 130.05999994710413
At time: 165.80450820922852 and batch: 700, loss is 4.826299352645874 and perplexity is 124.74845535684007
At time: 166.94229197502136 and batch: 750, loss is 4.824916086196899 and perplexity is 124.5760142974456
At time: 168.07809782028198 and batch: 800, loss is 4.814996709823609 and perplexity is 123.34640647807633
At time: 169.2083351612091 and batch: 850, loss is 4.8446534061431885 and perplexity is 127.05923641042253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.887739181518555 and perplexity of 132.6533296807909
Finished 8 epochs...
Completing Train Step...
At time: 172.35389947891235 and batch: 50, loss is 4.901448364257813 and perplexity is 134.48442112384637
At time: 173.51957750320435 and batch: 100, loss is 4.848891983032226 and perplexity is 127.59892971119683
At time: 174.65690541267395 and batch: 150, loss is 4.8370913314819335 and perplexity is 126.10202877756458
At time: 175.7937753200531 and batch: 200, loss is 4.860598630905152 and perplexity is 129.10146310537553
At time: 176.93032789230347 and batch: 250, loss is 4.868670530319214 and perplexity is 130.14777430808593
At time: 178.0672857761383 and batch: 300, loss is 4.847580661773682 and perplexity is 127.43171618138838
At time: 179.22061848640442 and batch: 350, loss is 4.796422357559204 and perplexity is 121.07647340331962
At time: 180.36012959480286 and batch: 400, loss is 4.8300261974334715 and perplexity is 125.21424090354373
At time: 181.51739263534546 and batch: 450, loss is 4.841825485229492 and perplexity is 126.70043051496535
At time: 182.65790367126465 and batch: 500, loss is 4.838611030578614 and perplexity is 126.29381160597296
At time: 183.79810571670532 and batch: 550, loss is 4.8197575950622555 and perplexity is 123.93504467152057
At time: 184.93830347061157 and batch: 600, loss is 4.856267366409302 and perplexity is 128.54349973760296
At time: 186.07292199134827 and batch: 650, loss is 4.836878652572632 and perplexity is 126.07521238736484
At time: 187.22020030021667 and batch: 700, loss is 4.797662506103515 and perplexity is 121.22671935995716
At time: 188.3638195991516 and batch: 750, loss is 4.794180917739868 and perplexity is 120.80539169481054
At time: 189.4994821548462 and batch: 800, loss is 4.784878168106079 and perplexity is 119.68678052841769
At time: 190.63689064979553 and batch: 850, loss is 4.81488582611084 and perplexity is 123.33273012882553
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.883376121520996 and perplexity of 132.07581602402757
Finished 9 epochs...
Completing Train Step...
At time: 193.77557516098022 and batch: 50, loss is 4.8683954524993895 and perplexity is 130.11197846562112
At time: 194.9430434703827 and batch: 100, loss is 4.815959577560425 and perplexity is 123.46522994978967
At time: 196.07691383361816 and batch: 150, loss is 4.808033142089844 and perplexity is 122.49045910443508
At time: 197.21407866477966 and batch: 200, loss is 4.8277793216705325 and perplexity is 124.93321589285378
At time: 198.35012888908386 and batch: 250, loss is 4.8360444355010985 and perplexity is 125.97008214971044
At time: 199.48603701591492 and batch: 300, loss is 4.8154175758361815 and perplexity is 123.39832971393214
At time: 200.62406373023987 and batch: 350, loss is 4.763263082504272 and perplexity is 117.12749970206261
At time: 201.76168370246887 and batch: 400, loss is 4.795668497085571 and perplexity is 120.98523303131002
At time: 202.94540977478027 and batch: 450, loss is 4.807564430236816 and perplexity is 122.43305982728164
At time: 204.0998363494873 and batch: 500, loss is 4.80506178855896 and perplexity is 122.12703684167344
At time: 205.23435592651367 and batch: 550, loss is 4.790511064529419 and perplexity is 120.36286613883044
At time: 206.36955285072327 and batch: 600, loss is 4.830170564651489 and perplexity is 125.23231904007288
At time: 207.50709676742554 and batch: 650, loss is 4.809111442565918 and perplexity is 122.62261186218785
At time: 208.64388799667358 and batch: 700, loss is 4.768865928649903 and perplexity is 117.78558892710045
At time: 209.77694582939148 and batch: 750, loss is 4.77038722038269 and perplexity is 117.96491153619384
At time: 210.91099953651428 and batch: 800, loss is 4.7578391838073735 and perplexity is 116.4939317664346
At time: 212.0469262599945 and batch: 850, loss is 4.785609331130981 and perplexity is 119.77432307694691
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.877067565917969 and perplexity of 131.24523104521026
Finished 10 epochs...
Completing Train Step...
At time: 215.17169547080994 and batch: 50, loss is 4.831009225845337 and perplexity is 125.33739057982034
At time: 216.30440592765808 and batch: 100, loss is 4.784284906387329 and perplexity is 119.61579600157292
At time: 217.4362006187439 and batch: 150, loss is 4.775431995391846 and perplexity is 118.5615215903167
At time: 218.57008147239685 and batch: 200, loss is 4.795652637481689 and perplexity is 120.98331426865406
At time: 219.70452737808228 and batch: 250, loss is 4.804160690307617 and perplexity is 122.0170379496814
At time: 220.83726716041565 and batch: 300, loss is 4.785592527389526 and perplexity is 119.77231043709895
At time: 221.97040247917175 and batch: 350, loss is 4.732899904251099 and perplexity is 113.62458546874257
At time: 223.10651803016663 and batch: 400, loss is 4.765238513946533 and perplexity is 117.35910573332879
At time: 224.24388647079468 and batch: 450, loss is 4.778358268737793 and perplexity is 118.90897313195626
At time: 225.3783540725708 and batch: 500, loss is 4.7752555561065675 and perplexity is 118.54060452553607
At time: 226.51479148864746 and batch: 550, loss is 4.762672157287597 and perplexity is 117.05830655492382
At time: 227.6648199558258 and batch: 600, loss is 4.80354525566101 and perplexity is 121.9419675398859
At time: 228.7975525856018 and batch: 650, loss is 4.7817848014831545 and perplexity is 119.31711748276223
At time: 229.9302806854248 and batch: 700, loss is 4.747403135299683 and perplexity is 115.28451717487725
At time: 231.10759377479553 and batch: 750, loss is 4.748704843521118 and perplexity is 115.43468169267787
At time: 232.25558829307556 and batch: 800, loss is 4.7337642288208 and perplexity is 113.72283644393471
At time: 233.41034483909607 and batch: 850, loss is 4.7625663948059085 and perplexity is 117.04592683258689
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.875521024068196 and perplexity of 131.04241167756092
Finished 11 epochs...
Completing Train Step...
At time: 236.51096630096436 and batch: 50, loss is 4.79971116065979 and perplexity is 121.47532559811766
At time: 237.64540910720825 and batch: 100, loss is 4.7562204933166505 and perplexity is 116.30551668085081
At time: 238.7809157371521 and batch: 150, loss is 4.749721326828003 and perplexity is 115.55207877563335
At time: 239.91446614265442 and batch: 200, loss is 4.770215196609497 and perplexity is 117.94462051232625
At time: 241.05750489234924 and batch: 250, loss is 4.775048999786377 and perplexity is 118.51612174309592
At time: 242.19136762619019 and batch: 300, loss is 4.758442087173462 and perplexity is 116.56418752661449
At time: 243.32895016670227 and batch: 350, loss is 4.7042584896087645 and perplexity is 110.41637969152175
At time: 244.4736728668213 and batch: 400, loss is 4.738950805664063 and perplexity is 114.31420092698175
At time: 245.63908863067627 and batch: 450, loss is 4.749247627258301 and perplexity is 115.49735476803187
At time: 246.78653001785278 and batch: 500, loss is 4.748860654830932 and perplexity is 115.45266912291655
At time: 247.92275857925415 and batch: 550, loss is 4.737391529083252 and perplexity is 114.13609236696236
At time: 249.0586564540863 and batch: 600, loss is 4.780193548202515 and perplexity is 119.12740470869916
At time: 250.1896104812622 and batch: 650, loss is 4.7567613983154295 and perplexity is 116.36844393350755
At time: 251.32453680038452 and batch: 700, loss is 4.7213037109375 and perplexity is 112.31458301116102
At time: 252.46187043190002 and batch: 750, loss is 4.724316644668579 and perplexity is 112.65348970213152
At time: 253.59579157829285 and batch: 800, loss is 4.709087362289429 and perplexity is 110.9508557507563
At time: 254.73097372055054 and batch: 850, loss is 4.739898452758789 and perplexity is 114.42258179267161
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.870939572652181 and perplexity of 130.44342040744408
Finished 12 epochs...
Completing Train Step...
At time: 257.8165476322174 and batch: 50, loss is 4.77259222984314 and perplexity is 118.22531226966548
At time: 258.95169854164124 and batch: 100, loss is 4.731440296173096 and perplexity is 113.45885908314098
At time: 260.12024426460266 and batch: 150, loss is 4.72548807144165 and perplexity is 112.78553234008963
At time: 261.2602508068085 and batch: 200, loss is 4.743590841293335 and perplexity is 114.84585538615796
At time: 262.39678978919983 and batch: 250, loss is 4.749209260940551 and perplexity is 115.49292364482311
At time: 263.5345296859741 and batch: 300, loss is 4.732451267242432 and perplexity is 113.57362070780064
At time: 264.66859316825867 and batch: 350, loss is 4.68088924407959 and perplexity is 107.86594909288823
At time: 265.8123571872711 and batch: 400, loss is 4.714229488372803 and perplexity is 111.52284840890889
At time: 266.9602508544922 and batch: 450, loss is 4.726752729415893 and perplexity is 112.92825769329838
At time: 268.1185312271118 and batch: 500, loss is 4.725676603317261 and perplexity is 112.80679801260904
At time: 269.25427055358887 and batch: 550, loss is 4.71543737411499 and perplexity is 111.65763665544283
At time: 270.4036886692047 and batch: 600, loss is 4.758617219924926 and perplexity is 116.58460352120072
At time: 271.57023572921753 and batch: 650, loss is 4.736214876174927 and perplexity is 114.00187278235195
At time: 272.705379486084 and batch: 700, loss is 4.701656675338745 and perplexity is 110.12947018367362
At time: 273.84172558784485 and batch: 750, loss is 4.706054410934448 and perplexity is 110.61485699397834
At time: 274.975389957428 and batch: 800, loss is 4.68889627456665 and perplexity is 108.73310206298176
At time: 276.12627625465393 and batch: 850, loss is 4.718410081863404 and perplexity is 111.99005602511708
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8701887130737305 and perplexity of 130.34551247793863
Finished 13 epochs...
Completing Train Step...
At time: 279.2188866138458 and batch: 50, loss is 4.750063123703003 and perplexity is 115.59158086551953
At time: 280.3770236968994 and batch: 100, loss is 4.7075217342376705 and perplexity is 110.77728388856936
At time: 281.51489901542664 and batch: 150, loss is 4.701961469650269 and perplexity is 110.16304213572685
At time: 282.6626374721527 and batch: 200, loss is 4.723033809661866 and perplexity is 112.50906651727789
At time: 283.7967936992645 and batch: 250, loss is 4.726373929977417 and perplexity is 112.88548863365529
At time: 284.9345586299896 and batch: 300, loss is 4.709388465881347 and perplexity is 110.98426848204355
At time: 286.0730221271515 and batch: 350, loss is 4.655239114761352 and perplexity is 105.13435617005062
At time: 287.21108961105347 and batch: 400, loss is 4.689391384124756 and perplexity is 108.78695019035689
At time: 288.34719038009644 and batch: 450, loss is 4.701919441223144 and perplexity is 110.15841225363268
At time: 289.5106029510498 and batch: 500, loss is 4.703042163848877 and perplexity is 110.28215904914953
At time: 290.6487364768982 and batch: 550, loss is 4.692878332138061 and perplexity is 109.16694675931203
At time: 291.7872905731201 and batch: 600, loss is 4.73885799407959 and perplexity is 114.30359173720144
At time: 292.92373871803284 and batch: 650, loss is 4.714024524688721 and perplexity is 111.49999261742332
At time: 294.0617642402649 and batch: 700, loss is 4.682988376617431 and perplexity is 108.09261183068854
At time: 295.1996650695801 and batch: 750, loss is 4.686899290084839 and perplexity is 108.51618041217738
At time: 296.33339285850525 and batch: 800, loss is 4.669660663604736 and perplexity is 106.66154214753546
At time: 297.50021743774414 and batch: 850, loss is 4.699739465713501 and perplexity is 109.91853117517542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.874237060546875 and perplexity of 130.87426597079775
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 300.6312036514282 and batch: 50, loss is 4.738206949234009 and perplexity is 114.22919919203875
At time: 301.7955183982849 and batch: 100, loss is 4.703082752227783 and perplexity is 110.28663531404922
At time: 302.92708373069763 and batch: 150, loss is 4.6931345176696775 and perplexity is 109.19491733427779
At time: 304.0592432022095 and batch: 200, loss is 4.710516948699951 and perplexity is 111.1095830165116
At time: 305.19652819633484 and batch: 250, loss is 4.704958152770996 and perplexity is 110.49366099718765
At time: 306.33347964286804 and batch: 300, loss is 4.676327447891236 and perplexity is 107.37500725713465
At time: 307.4657256603241 and batch: 350, loss is 4.6171238899230955 and perplexity is 101.20254349861955
At time: 308.59858417510986 and batch: 400, loss is 4.648612766265869 and perplexity is 104.44000234310529
At time: 309.73068380355835 and batch: 450, loss is 4.658699932098389 and perplexity is 105.49883731013617
At time: 310.86775970458984 and batch: 500, loss is 4.6482962799072265 and perplexity is 104.40695373705942
At time: 312.002667427063 and batch: 550, loss is 4.627958536148071 and perplexity is 102.30499882651549
At time: 313.13789319992065 and batch: 600, loss is 4.656123237609863 and perplexity is 105.22734895897128
At time: 314.27446818351746 and batch: 650, loss is 4.6249902629852295 and perplexity is 102.00177988502695
At time: 315.40946221351624 and batch: 700, loss is 4.586734352111816 and perplexity is 98.17330665762087
At time: 316.539941072464 and batch: 750, loss is 4.575777921676636 and perplexity is 97.10354871828154
At time: 317.673508644104 and batch: 800, loss is 4.540541639328003 and perplexity is 93.7415604871555
At time: 318.85322856903076 and batch: 850, loss is 4.58232385635376 and perplexity is 97.74126715959261
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.839398701985677 and perplexity of 126.39332881855017
Finished 15 epochs...
Completing Train Step...
At time: 321.9422903060913 and batch: 50, loss is 4.7028964138031 and perplexity is 110.26608659072902
At time: 323.10604310035706 and batch: 100, loss is 4.669747676849365 and perplexity is 106.67082351819009
At time: 324.24078011512756 and batch: 150, loss is 4.659778614044189 and perplexity is 105.61269840015626
At time: 325.3770477771759 and batch: 200, loss is 4.681369915008545 and perplexity is 107.91780958175747
At time: 326.5092444419861 and batch: 250, loss is 4.677299318313598 and perplexity is 107.47941257679201
At time: 327.6489987373352 and batch: 300, loss is 4.6525098323822025 and perplexity is 104.84780604029793
At time: 328.78730368614197 and batch: 350, loss is 4.5929453086853025 and perplexity is 98.78495429416026
At time: 329.92577624320984 and batch: 400, loss is 4.62596791267395 and perplexity is 102.10155065586527
At time: 331.06265592575073 and batch: 450, loss is 4.63955286026001 and perplexity is 103.49805914009767
At time: 332.19866609573364 and batch: 500, loss is 4.630560836791992 and perplexity is 102.57157389463728
At time: 333.33641839027405 and batch: 550, loss is 4.613791456222534 and perplexity is 100.8658540413105
At time: 334.47471284866333 and batch: 600, loss is 4.645736265182495 and perplexity is 104.14001223100855
At time: 335.61324739456177 and batch: 650, loss is 4.616920280456543 and perplexity is 101.1819398003492
At time: 336.7511553764343 and batch: 700, loss is 4.582178812026978 and perplexity is 97.7270913713823
At time: 337.88761377334595 and batch: 750, loss is 4.575589265823364 and perplexity is 97.08523129334142
At time: 339.0198812484741 and batch: 800, loss is 4.5451069259643555 and perplexity is 94.17049594218518
At time: 340.17172956466675 and batch: 850, loss is 4.589819211959838 and perplexity is 98.47662515646503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.836510340372722 and perplexity of 126.0287858987822
Finished 16 epochs...
Completing Train Step...
At time: 343.42734360694885 and batch: 50, loss is 4.689840660095215 and perplexity is 108.8358365338844
At time: 344.56625151634216 and batch: 100, loss is 4.656693325042725 and perplexity is 105.2873548508834
At time: 345.7090599536896 and batch: 150, loss is 4.645347833633423 and perplexity is 104.09956881999419
At time: 346.85024905204773 and batch: 200, loss is 4.66807975769043 and perplexity is 106.49305350216487
At time: 348.03434681892395 and batch: 250, loss is 4.66428882598877 and perplexity is 106.09010985811797
At time: 349.17231464385986 and batch: 300, loss is 4.640455560684204 and perplexity is 103.59152906330924
At time: 350.30650091171265 and batch: 350, loss is 4.580899858474732 and perplexity is 97.60218285384458
At time: 351.4436972141266 and batch: 400, loss is 4.61498875617981 and perplexity is 100.98669304987911
At time: 352.58067870140076 and batch: 450, loss is 4.62985445022583 and perplexity is 102.49914429742698
At time: 353.716260433197 and batch: 500, loss is 4.621219930648803 and perplexity is 101.6179233640032
At time: 354.85161304473877 and batch: 550, loss is 4.6065320301055905 and perplexity is 100.13627718482931
At time: 355.9868321418762 and batch: 600, loss is 4.640567321777343 and perplexity is 103.60310721281857
At time: 357.1210021972656 and batch: 650, loss is 4.613055524826049 and perplexity is 100.79165100001114
At time: 358.2592046260834 and batch: 700, loss is 4.580192708969117 and perplexity is 97.53318791623568
At time: 359.3971428871155 and batch: 750, loss is 4.575354471206665 and perplexity is 97.06243887954547
At time: 360.5329120159149 and batch: 800, loss is 4.5470948410034175 and perplexity is 94.35788508243917
At time: 361.66768622398376 and batch: 850, loss is 4.592308559417725 and perplexity is 98.72207306877745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.834981918334961 and perplexity of 125.83630785633345
Finished 17 epochs...
Completing Train Step...
At time: 364.8338224887848 and batch: 50, loss is 4.680016050338745 and perplexity is 107.7718023314534
At time: 365.9670994281769 and batch: 100, loss is 4.647565803527832 and perplexity is 104.33071477228151
At time: 367.09864234924316 and batch: 150, loss is 4.634952907562256 and perplexity is 103.02306627365579
At time: 368.2301323413849 and batch: 200, loss is 4.658346061706543 and perplexity is 105.46151099996543
At time: 369.3633575439453 and batch: 250, loss is 4.654654340744019 and perplexity is 105.07289430263629
At time: 370.49666690826416 and batch: 300, loss is 4.631425123214722 and perplexity is 102.66026343437412
At time: 371.62827491760254 and batch: 350, loss is 4.572330446243286 and perplexity is 96.76936299909815
At time: 372.77932262420654 and batch: 400, loss is 4.607077121734619 and perplexity is 100.19087551047811
At time: 373.90951561927795 and batch: 450, loss is 4.622672281265259 and perplexity is 101.76561544202326
At time: 375.04154348373413 and batch: 500, loss is 4.614656372070312 and perplexity is 100.95313225568498
At time: 376.2205419540405 and batch: 550, loss is 4.60124062538147 and perplexity is 99.60781500135695
At time: 377.3536322116852 and batch: 600, loss is 4.63652606010437 and perplexity is 103.18526482033778
At time: 378.5002455711365 and batch: 650, loss is 4.609529113769531 and perplexity is 100.43684417256506
At time: 379.63959550857544 and batch: 700, loss is 4.578143358230591 and perplexity is 97.33351287752636
At time: 380.77951526641846 and batch: 750, loss is 4.574380292892456 and perplexity is 96.96792879877471
At time: 381.91030263900757 and batch: 800, loss is 4.547491874694824 and perplexity is 94.39535577993921
At time: 383.04367446899414 and batch: 850, loss is 4.592734775543213 and perplexity is 98.76415897647013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.83393669128418 and perplexity of 125.7048490575061
Finished 18 epochs...
Completing Train Step...
At time: 386.17985105514526 and batch: 50, loss is 4.672260770797729 and perplexity is 106.93923444891794
At time: 387.31277418136597 and batch: 100, loss is 4.640133695602417 and perplexity is 103.55819193265177
At time: 388.44455194473267 and batch: 150, loss is 4.626875305175782 and perplexity is 102.19423888329742
At time: 389.5814461708069 and batch: 200, loss is 4.650845365524292 and perplexity is 104.67343549926957
At time: 390.71595573425293 and batch: 250, loss is 4.647375583648682 and perplexity is 104.31087088373673
At time: 391.8565218448639 and batch: 300, loss is 4.624321203231812 and perplexity is 101.93355742432522
At time: 392.98967933654785 and batch: 350, loss is 4.565140047073364 and perplexity is 96.07604824377101
At time: 394.1255648136139 and batch: 400, loss is 4.60057918548584 and perplexity is 99.54195220314097
At time: 395.26091265678406 and batch: 450, loss is 4.616874198913575 and perplexity is 101.17727728787139
At time: 396.3982570171356 and batch: 500, loss is 4.609707880020141 and perplexity is 100.4548004955654
At time: 397.53299260139465 and batch: 550, loss is 4.597335844039917 and perplexity is 99.2196266524136
At time: 398.6689932346344 and batch: 600, loss is 4.63344440460205 and perplexity is 102.8677728329686
At time: 399.80376625061035 and batch: 650, loss is 4.606969480514526 and perplexity is 100.18009142281258
At time: 400.9380679130554 and batch: 700, loss is 4.57657057762146 and perplexity is 97.18054893674547
At time: 402.0726363658905 and batch: 750, loss is 4.572970066070557 and perplexity is 96.83127840139333
At time: 403.2158205509186 and batch: 800, loss is 4.546650495529175 and perplexity is 94.31596689701135
At time: 404.36072182655334 and batch: 850, loss is 4.591963567733765 and perplexity is 98.68802064878403
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.833178202311198 and perplexity of 125.60953946585217
Finished 19 epochs...
Completing Train Step...
At time: 407.49006724357605 and batch: 50, loss is 4.665404310226441 and perplexity is 106.20851773221808
At time: 408.65153551101685 and batch: 100, loss is 4.632567358016968 and perplexity is 102.77759255602301
At time: 409.7900197505951 and batch: 150, loss is 4.619797086715698 and perplexity is 101.47343973145537
At time: 410.94847536087036 and batch: 200, loss is 4.6436007213592525 and perplexity is 103.91785396991763
At time: 412.08393597602844 and batch: 250, loss is 4.641045942306518 and perplexity is 103.65270565528651
At time: 413.22055411338806 and batch: 300, loss is 4.618325643539428 and perplexity is 101.32423712948564
At time: 414.35611176490784 and batch: 350, loss is 4.5593470764160156 and perplexity is 95.52109149147114
At time: 415.48953580856323 and batch: 400, loss is 4.595392541885376 and perplexity is 99.02700016452158
At time: 416.6229782104492 and batch: 450, loss is 4.612080717086792 and perplexity is 100.69344639164655
At time: 417.75754570961 and batch: 500, loss is 4.605234050750733 and perplexity is 100.00638668020387
At time: 418.8928790092468 and batch: 550, loss is 4.5934755325317385 and perplexity is 98.8373463211197
At time: 420.0262746810913 and batch: 600, loss is 4.630148630142212 and perplexity is 102.52930192279203
At time: 421.16804456710815 and batch: 650, loss is 4.603969297409058 and perplexity is 99.87998321991022
At time: 422.30880522727966 and batch: 700, loss is 4.573961067199707 and perplexity is 96.92728587152455
At time: 423.4499533176422 and batch: 750, loss is 4.570907697677613 and perplexity is 96.63178242118272
At time: 424.58500957489014 and batch: 800, loss is 4.545423126220703 and perplexity is 94.20027738534431
At time: 425.7175097465515 and batch: 850, loss is 4.590448293685913 and perplexity is 98.53859449164099
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.832256317138672 and perplexity of 125.49379525352462
Finished 20 epochs...
Completing Train Step...
At time: 428.80994176864624 and batch: 50, loss is 4.660191078186035 and perplexity is 105.65626883617344
At time: 429.9712588787079 and batch: 100, loss is 4.627722625732422 and perplexity is 102.28086685832254
At time: 431.11070227622986 and batch: 150, loss is 4.614308300018311 and perplexity is 100.91799940652092
At time: 432.2465178966522 and batch: 200, loss is 4.639415884017945 and perplexity is 103.48388333579197
At time: 433.3832862377167 and batch: 250, loss is 4.635640735626221 and perplexity is 103.09395280595292
At time: 434.54643535614014 and batch: 300, loss is 4.613098306655884 and perplexity is 100.79596314351298
At time: 435.68428325653076 and batch: 350, loss is 4.554132204055787 and perplexity is 95.02425777977324
At time: 436.82167887687683 and batch: 400, loss is 4.590371599197388 and perplexity is 98.53103741433324
At time: 437.96014046669006 and batch: 450, loss is 4.6077931785583495 and perplexity is 100.26264356249807
At time: 439.09695196151733 and batch: 500, loss is 4.601041860580445 and perplexity is 99.58801844132248
At time: 440.23673701286316 and batch: 550, loss is 4.589485063552856 and perplexity is 98.44372484614327
At time: 441.3748505115509 and batch: 600, loss is 4.626836175918579 and perplexity is 102.19024017687326
At time: 442.53252053260803 and batch: 650, loss is 4.601248445510865 and perplexity is 99.60859395040472
At time: 443.66729164123535 and batch: 700, loss is 4.571377468109131 and perplexity is 96.67718783953558
At time: 444.8041775226593 and batch: 750, loss is 4.569013156890869 and perplexity is 96.44888287811871
At time: 445.9415543079376 and batch: 800, loss is 4.543729810714722 and perplexity is 94.0409015698316
At time: 447.07398295402527 and batch: 850, loss is 4.588989171981812 and perplexity is 98.39491953484124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.832430203755696 and perplexity of 125.51561884239885
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 450.19115805625916 and batch: 50, loss is 4.657258768081665 and perplexity is 105.34690568748856
At time: 451.35290455818176 and batch: 100, loss is 4.627073020935058 and perplexity is 102.21444629242772
At time: 452.4905951023102 and batch: 150, loss is 4.612799510955811 and perplexity is 100.76585024216774
At time: 453.6446671485901 and batch: 200, loss is 4.638186273574829 and perplexity is 103.35671667090006
At time: 454.7854084968567 and batch: 250, loss is 4.630614643096924 and perplexity is 102.57709304050066
At time: 455.9257392883301 and batch: 300, loss is 4.606478910446167 and perplexity is 100.13095812116403
At time: 457.0645616054535 and batch: 350, loss is 4.547874155044556 and perplexity is 94.43144816782562
At time: 458.2039692401886 and batch: 400, loss is 4.581372938156128 and perplexity is 97.64836738703342
At time: 459.3433163166046 and batch: 450, loss is 4.597882070541382 and perplexity is 99.27383784640409
At time: 460.4812340736389 and batch: 500, loss is 4.589368696212769 and perplexity is 98.43226987823962
At time: 461.61866545677185 and batch: 550, loss is 4.575913619995117 and perplexity is 97.11672640063564
At time: 462.7546617984772 and batch: 600, loss is 4.608135757446289 and perplexity is 100.29699731153015
At time: 463.9151256084442 and batch: 650, loss is 4.580934104919433 and perplexity is 97.60552543883793
At time: 465.0512902736664 and batch: 700, loss is 4.550698146820069 and perplexity is 94.6984986975754
At time: 466.20525646209717 and batch: 750, loss is 4.544321641921997 and perplexity is 94.09657438297003
At time: 467.34513688087463 and batch: 800, loss is 4.517041959762573 and perplexity is 91.56434596907586
At time: 468.4716897010803 and batch: 850, loss is 4.565154886245727 and perplexity is 96.07747394338892
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.82906436920166 and perplexity of 125.09386421174835
Finished 22 epochs...
Completing Train Step...
At time: 471.5448853969574 and batch: 50, loss is 4.65302731513977 and perplexity is 104.90207701302779
At time: 472.66844630241394 and batch: 100, loss is 4.62197190284729 and perplexity is 101.69436595499033
At time: 473.799357175827 and batch: 150, loss is 4.608252267837525 and perplexity is 100.30868363470255
At time: 474.92977023124695 and batch: 200, loss is 4.633825445175171 and perplexity is 102.90697709681777
At time: 476.0570433139801 and batch: 250, loss is 4.627146224975586 and perplexity is 102.22192907677835
At time: 477.18691205978394 and batch: 300, loss is 4.603275232315063 and perplexity is 99.81068406181382
At time: 478.3218927383423 and batch: 350, loss is 4.545028858184814 and perplexity is 94.16314454762593
At time: 479.44931530952454 and batch: 400, loss is 4.578599996566773 and perplexity is 97.37796924036877
At time: 480.6047341823578 and batch: 450, loss is 4.595746107101441 and perplexity is 99.06201885756207
At time: 481.73562955856323 and batch: 500, loss is 4.587630891799927 and perplexity is 98.26136239018388
At time: 482.86292338371277 and batch: 550, loss is 4.574699487686157 and perplexity is 96.99888539713297
At time: 483.9919173717499 and batch: 600, loss is 4.607447643280029 and perplexity is 100.22800526677077
At time: 485.12258887290955 and batch: 650, loss is 4.5804292106628415 and perplexity is 97.5562574082517
At time: 486.2532262802124 and batch: 700, loss is 4.550737800598145 and perplexity is 94.7022539252809
At time: 487.38620710372925 and batch: 750, loss is 4.544957761764526 and perplexity is 94.15645012310317
At time: 488.51622200012207 and batch: 800, loss is 4.518403100967407 and perplexity is 91.6890628326437
At time: 489.64251828193665 and batch: 850, loss is 4.566793174743652 and perplexity is 96.2350055697514
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.828730583190918 and perplexity of 125.05211659761717
Finished 23 epochs...
Completing Train Step...
At time: 492.74825835227966 and batch: 50, loss is 4.650868616104126 and perplexity is 104.67586924563102
At time: 493.8921089172363 and batch: 100, loss is 4.6194404602050785 and perplexity is 101.43725806477762
At time: 495.03231930732727 and batch: 150, loss is 4.6058542346954345 and perplexity is 100.06842827220163
At time: 496.1822621822357 and batch: 200, loss is 4.631615753173828 and perplexity is 102.67983542163873
At time: 497.3289713859558 and batch: 250, loss is 4.625058488845825 and perplexity is 102.00873928164461
At time: 498.47300148010254 and batch: 300, loss is 4.601302280426025 and perplexity is 99.61395651495461
At time: 499.6065652370453 and batch: 350, loss is 4.543402833938599 and perplexity is 94.01015740560032
At time: 500.7460603713989 and batch: 400, loss is 4.577082843780517 and perplexity is 97.23034399635739
At time: 501.8853991031647 and batch: 450, loss is 4.594472017288208 and perplexity is 98.9358853182489
At time: 503.02167987823486 and batch: 500, loss is 4.5866481304168705 and perplexity is 98.164842353631
At time: 504.15903091430664 and batch: 550, loss is 4.574165449142456 and perplexity is 96.94709808307658
At time: 505.29636216163635 and batch: 600, loss is 4.6071358489990235 and perplexity is 100.19675961929224
At time: 506.4329686164856 and batch: 650, loss is 4.580353364944458 and perplexity is 97.54885846441844
At time: 507.5948190689087 and batch: 700, loss is 4.550952825546265 and perplexity is 94.72261946198878
At time: 508.73212027549744 and batch: 750, loss is 4.545576601028443 and perplexity is 94.21473586428282
At time: 509.8675081729889 and batch: 800, loss is 4.5193590259552 and perplexity is 91.77675260465736
At time: 511.00195932388306 and batch: 850, loss is 4.567754878997802 and perplexity is 96.32759970095444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8286237716674805 and perplexity of 125.03876030385004
Finished 24 epochs...
Completing Train Step...
At time: 514.1132340431213 and batch: 50, loss is 4.649157114028931 and perplexity is 104.49686950110582
At time: 515.2426836490631 and batch: 100, loss is 4.617588176727295 and perplexity is 101.24954141352895
At time: 516.3757817745209 and batch: 150, loss is 4.604001655578613 and perplexity is 99.88321520563272
At time: 517.507214307785 and batch: 200, loss is 4.629944725036621 and perplexity is 102.50839780595776
At time: 518.6399998664856 and batch: 250, loss is 4.6234112644195555 and perplexity is 101.84084631127048
At time: 519.7723727226257 and batch: 300, loss is 4.599797849655151 and perplexity is 99.46420688578422
At time: 520.9066824913025 and batch: 350, loss is 4.5421546459197994 and perplexity is 93.89288825568038
At time: 522.080961227417 and batch: 400, loss is 4.575956535339356 and perplexity is 97.12089428781293
At time: 523.2168598175049 and batch: 450, loss is 4.59348874092102 and perplexity is 98.83865181188719
At time: 524.3487963676453 and batch: 500, loss is 4.585902500152588 and perplexity is 98.09167495758588
At time: 525.4793653488159 and batch: 550, loss is 4.573809671401977 and perplexity is 96.91261259852223
At time: 526.6110737323761 and batch: 600, loss is 4.60688042640686 and perplexity is 100.17117037139906
At time: 527.7509870529175 and batch: 650, loss is 4.58031099319458 and perplexity is 97.54472523615333
At time: 528.8857207298279 and batch: 700, loss is 4.551112403869629 and perplexity is 94.73773634491855
At time: 530.0178370475769 and batch: 750, loss is 4.546008081436157 and perplexity is 94.25539644841811
At time: 531.1719613075256 and batch: 800, loss is 4.519994812011719 and perplexity is 91.83512153738656
At time: 532.3208339214325 and batch: 850, loss is 4.568393182754517 and perplexity is 96.38910559735291
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.828570048014323 and perplexity of 125.03204294530241
Finished 25 epochs...
Completing Train Step...
At time: 535.41153216362 and batch: 50, loss is 4.64767991065979 and perplexity is 104.34262033016095
At time: 536.5700018405914 and batch: 100, loss is 4.6160476016998295 and perplexity is 101.09367898819168
At time: 537.7012023925781 and batch: 150, loss is 4.602424001693725 and perplexity is 99.72575830203539
At time: 538.8651542663574 and batch: 200, loss is 4.6285241508483885 and perplexity is 102.36288040556057
At time: 540.0025300979614 and batch: 250, loss is 4.621989183425903 and perplexity is 101.69612330765975
At time: 541.1387617588043 and batch: 300, loss is 4.598535356521606 and perplexity is 99.33871324165798
At time: 542.2788197994232 and batch: 350, loss is 4.541057472229004 and perplexity is 93.7899279419357
At time: 543.416261434555 and batch: 400, loss is 4.575000600814819 and perplexity is 97.02809743283467
At time: 544.5529518127441 and batch: 450, loss is 4.5926471519470216 and perplexity is 98.75550528482513
At time: 545.6868917942047 and batch: 500, loss is 4.585281152725219 and perplexity is 98.03074487903879
At time: 546.8221719264984 and batch: 550, loss is 4.573531665802002 and perplexity is 96.88567409421262
At time: 547.9564666748047 and batch: 600, loss is 4.6066391658782955 and perplexity is 100.14700593696762
At time: 549.0922629833221 and batch: 650, loss is 4.580244636535644 and perplexity is 97.53825270883988
At time: 550.2708938121796 and batch: 700, loss is 4.551167049407959 and perplexity is 94.74291348097368
At time: 551.4060297012329 and batch: 750, loss is 4.546302719116211 and perplexity is 94.28317173138177
At time: 552.5439672470093 and batch: 800, loss is 4.5203930759429936 and perplexity is 91.87170343806139
At time: 553.6797759532928 and batch: 850, loss is 4.568833866119385 and perplexity is 96.43159203358944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.828529993693034 and perplexity of 125.02703497197899
Finished 26 epochs...
Completing Train Step...
At time: 556.7756214141846 and batch: 50, loss is 4.646342153549194 and perplexity is 104.20312857172473
At time: 557.9398367404938 and batch: 100, loss is 4.614645223617554 and perplexity is 100.95200679073282
At time: 559.0775723457336 and batch: 150, loss is 4.601047792434692 and perplexity is 99.5886091846848
At time: 560.2212212085724 and batch: 200, loss is 4.627335271835327 and perplexity is 102.24125563822227
At time: 561.3545820713043 and batch: 250, loss is 4.620715475082397 and perplexity is 101.5666745643735
At time: 562.4917590618134 and batch: 300, loss is 4.59740665435791 and perplexity is 99.22665267448251
At time: 563.6402363777161 and batch: 350, loss is 4.540103712081909 and perplexity is 93.70051749130582
At time: 564.7860336303711 and batch: 400, loss is 4.574194173812867 and perplexity is 96.94988289651248
At time: 565.9202117919922 and batch: 450, loss is 4.591902341842651 and perplexity is 98.68197857174525
At time: 567.0566351413727 and batch: 500, loss is 4.584749937057495 and perplexity is 97.97868324064365
At time: 568.1943657398224 and batch: 550, loss is 4.573273992538452 and perplexity is 96.86071246248817
At time: 569.3316991329193 and batch: 600, loss is 4.6063823223114015 and perplexity is 100.12128712574601
At time: 570.4636962413788 and batch: 650, loss is 4.580159616470337 and perplexity is 97.52996035273796
At time: 571.6056323051453 and batch: 700, loss is 4.551180095672607 and perplexity is 94.74414953015938
At time: 572.7506999969482 and batch: 750, loss is 4.546506147384644 and perplexity is 94.30235354474486
At time: 573.8915755748749 and batch: 800, loss is 4.520669441223145 and perplexity is 91.89709709592108
At time: 575.0442774295807 and batch: 850, loss is 4.56912935256958 and perplexity is 96.46009047265005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.82850964864095 and perplexity of 125.02449131631609
Finished 27 epochs...
Completing Train Step...
At time: 578.1850368976593 and batch: 50, loss is 4.645122842788696 and perplexity is 104.07615000467803
At time: 579.3677978515625 and batch: 100, loss is 4.613387069702148 and perplexity is 100.82507349567618
At time: 580.5037252902985 and batch: 150, loss is 4.599838037490844 and perplexity is 99.4682042173094
At time: 581.6409995555878 and batch: 200, loss is 4.626266136169433 and perplexity is 102.13200427796323
At time: 582.7978115081787 and batch: 250, loss is 4.619564781188965 and perplexity is 101.4498696284277
At time: 583.9301688671112 and batch: 300, loss is 4.596365566253662 and perplexity is 99.1234027422226
At time: 585.0646114349365 and batch: 350, loss is 4.539264688491821 and perplexity is 93.62193351824075
At time: 586.2087082862854 and batch: 400, loss is 4.573475952148438 and perplexity is 96.8802763896951
At time: 587.3482573032379 and batch: 450, loss is 4.591213293075562 and perplexity is 98.61400529721635
At time: 588.4860551357269 and batch: 500, loss is 4.584252233505249 and perplexity is 97.92993103502971
At time: 589.6233804225922 and batch: 550, loss is 4.572987546920777 and perplexity is 96.8329711092626
At time: 590.7625725269318 and batch: 600, loss is 4.606086921691895 and perplexity is 100.09171560344129
At time: 591.9084000587463 and batch: 650, loss is 4.580061082839966 and perplexity is 97.52035084511203
At time: 593.044243812561 and batch: 700, loss is 4.551180028915406 and perplexity is 94.7441432053053
At time: 594.1927161216736 and batch: 750, loss is 4.546640625 and perplexity is 94.31503595310294
At time: 595.3370521068573 and batch: 800, loss is 4.5208665466308595 and perplexity is 91.91521229595526
At time: 596.4753086566925 and batch: 850, loss is 4.569325942993164 and perplexity is 96.47905546680202
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8284807205200195 and perplexity of 125.020874645024
Finished 28 epochs...
Completing Train Step...
At time: 599.5923528671265 and batch: 50, loss is 4.6440254592895505 and perplexity is 103.96200119897148
At time: 600.726438999176 and batch: 100, loss is 4.612251415252685 and perplexity is 100.71063604534247
At time: 601.8605580329895 and batch: 150, loss is 4.598680639266968 and perplexity is 99.35314649106414
At time: 602.9978542327881 and batch: 200, loss is 4.6252428245544435 and perplexity is 102.02754486810265
At time: 604.1444182395935 and batch: 250, loss is 4.618492622375488 and perplexity is 101.34115754530248
At time: 605.2906684875488 and batch: 300, loss is 4.595391712188721 and perplexity is 99.02691800218484
At time: 606.4352653026581 and batch: 350, loss is 4.538483161926269 and perplexity is 93.54879407400993
At time: 607.5767261981964 and batch: 400, loss is 4.572800722122192 and perplexity is 96.81488199873863
At time: 608.7666280269623 and batch: 450, loss is 4.590554656982422 and perplexity is 98.54907593879524
At time: 609.9108970165253 and batch: 500, loss is 4.583777132034302 and perplexity is 97.88341543143605
At time: 611.0557401180267 and batch: 550, loss is 4.5726994609832765 and perplexity is 96.80507890986779
At time: 612.196818113327 and batch: 600, loss is 4.605775156021118 and perplexity is 100.06051530643049
At time: 613.34192943573 and batch: 650, loss is 4.579937210083008 and perplexity is 97.50827147856104
At time: 614.485818862915 and batch: 700, loss is 4.551149644851685 and perplexity is 94.741264536954
At time: 615.6228291988373 and batch: 750, loss is 4.546713075637817 and perplexity is 94.32186938515375
At time: 616.756560087204 and batch: 800, loss is 4.520987768173217 and perplexity is 91.92635507511471
At time: 617.8875274658203 and batch: 850, loss is 4.569461288452149 and perplexity is 96.49211435255715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.828439394632976 and perplexity of 125.01570815323593
Finished 29 epochs...
Completing Train Step...
At time: 621.0301089286804 and batch: 50, loss is 4.642984066009522 and perplexity is 103.85379222337393
At time: 622.1763355731964 and batch: 100, loss is 4.611186552047729 and perplexity is 100.60345007399704
At time: 623.3119697570801 and batch: 150, loss is 4.597588319778442 and perplexity is 99.24468036352283
At time: 624.4456994533539 and batch: 200, loss is 4.62427773475647 and perplexity is 101.92912662429856
At time: 625.5826919078827 and batch: 250, loss is 4.6174852085113525 and perplexity is 101.239116465613
At time: 626.7208547592163 and batch: 300, loss is 4.594471397399903 and perplexity is 98.93582398906965
At time: 627.8595695495605 and batch: 350, loss is 4.537751274108887 and perplexity is 93.48035190034805
At time: 628.9958639144897 and batch: 400, loss is 4.572151679992675 and perplexity is 96.75206544905598
At time: 630.1346106529236 and batch: 450, loss is 4.589918231964111 and perplexity is 98.48637679510446
At time: 631.2706241607666 and batch: 500, loss is 4.583316946029663 and perplexity is 97.83838121642074
At time: 632.410248041153 and batch: 550, loss is 4.572411985397339 and perplexity is 96.77725381279608
At time: 633.5464379787445 and batch: 600, loss is 4.605452508926391 and perplexity is 100.02823627951715
At time: 634.6822962760925 and batch: 650, loss is 4.5797943496704105 and perplexity is 97.49434240164648
At time: 635.8202767372131 and batch: 700, loss is 4.551085176467896 and perplexity is 94.7351569176275
At time: 636.9567725658417 and batch: 750, loss is 4.5467418193817135 and perplexity is 94.32458058777604
At time: 638.1295657157898 and batch: 800, loss is 4.521050243377686 and perplexity is 91.93209837234905
At time: 639.2663466930389 and batch: 850, loss is 4.569536800384522 and perplexity is 96.4994009336791
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.828397432963054 and perplexity of 125.01046239541651
Finished 30 epochs...
Completing Train Step...
At time: 642.4039742946625 and batch: 50, loss is 4.641986532211304 and perplexity is 103.7502462094686
At time: 643.5406737327576 and batch: 100, loss is 4.610178880691528 and perplexity is 100.50212591832086
At time: 644.6774442195892 and batch: 150, loss is 4.596547498703003 and perplexity is 99.14143814623331
At time: 645.8149795532227 and batch: 200, loss is 4.623354158401489 and perplexity is 101.83503075211443
At time: 646.9570496082306 and batch: 250, loss is 4.616523723602295 and perplexity is 101.14182336334507
At time: 648.0933709144592 and batch: 300, loss is 4.593590173721314 and perplexity is 98.8486778015912
At time: 649.2291011810303 and batch: 350, loss is 4.537046775817871 and perplexity is 93.41451834472733
At time: 650.3829238414764 and batch: 400, loss is 4.571521883010864 and perplexity is 96.69115047429925
At time: 651.539514541626 and batch: 450, loss is 4.589295539855957 and perplexity is 98.42506919537372
At time: 652.7270703315735 and batch: 500, loss is 4.5828665828704835 and perplexity is 97.79432833460918
At time: 653.875910282135 and batch: 550, loss is 4.5721225166320805 and perplexity is 96.74924387482648
At time: 655.0205583572388 and batch: 600, loss is 4.605119838714599 and perplexity is 99.99496539939106
At time: 656.191793680191 and batch: 650, loss is 4.579634714126587 and perplexity is 97.47878008146022
At time: 657.3305251598358 and batch: 700, loss is 4.550986623764038 and perplexity is 94.72582097181174
At time: 658.4892098903656 and batch: 750, loss is 4.546736192703247 and perplexity is 94.32404985518274
At time: 659.6483707427979 and batch: 800, loss is 4.52106348991394 and perplexity is 91.93331616228882
At time: 660.7875320911407 and batch: 850, loss is 4.569559984207153 and perplexity is 96.5016381846083
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.828351338704427 and perplexity of 125.00470026363341
Finished 31 epochs...
Completing Train Step...
At time: 663.895364522934 and batch: 50, loss is 4.641024484634399 and perplexity is 103.65048153337662
At time: 665.0920102596283 and batch: 100, loss is 4.609216804504395 and perplexity is 100.40548171321716
At time: 666.2440011501312 and batch: 150, loss is 4.595548305511475 and perplexity is 99.04242617052302
At time: 667.4244227409363 and batch: 200, loss is 4.622458000183105 and perplexity is 101.74381133200788
At time: 668.5674996376038 and batch: 250, loss is 4.615596523284912 and perplexity is 101.04808809502249
At time: 669.7017271518707 and batch: 300, loss is 4.592738561630249 and perplexity is 98.76453290687991
At time: 670.8389739990234 and batch: 350, loss is 4.536361303329468 and perplexity is 93.35050720382897
At time: 671.9700763225555 and batch: 400, loss is 4.570903425216675 and perplexity is 96.63136956654891
At time: 673.106034040451 and batch: 450, loss is 4.58867579460144 and perplexity is 98.36408962366623
At time: 674.2439796924591 and batch: 500, loss is 4.582418546676636 and perplexity is 97.75052274993877
At time: 675.3836512565613 and batch: 550, loss is 4.571831312179565 and perplexity is 96.72107416600404
At time: 676.5166964530945 and batch: 600, loss is 4.604778470993042 and perplexity is 99.96083617152532
At time: 677.6871314048767 and batch: 650, loss is 4.5794557762145995 and perplexity is 97.46133899257192
At time: 678.8229520320892 and batch: 700, loss is 4.550848894119262 and perplexity is 94.7127753165456
At time: 679.960024356842 and batch: 750, loss is 4.546694984436035 and perplexity is 94.32016300461754
At time: 681.1026968955994 and batch: 800, loss is 4.521028289794922 and perplexity is 91.93008015557237
At time: 682.237645149231 and batch: 850, loss is 4.569528703689575 and perplexity is 96.49861961063029
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.828306833902995 and perplexity of 124.9991370780653
Finished 32 epochs...
Completing Train Step...
At time: 685.3109858036041 and batch: 50, loss is 4.640091152191162 and perplexity is 103.55378630761935
At time: 686.4745602607727 and batch: 100, loss is 4.60828272819519 and perplexity is 100.31173911961837
At time: 687.6111009120941 and batch: 150, loss is 4.594581651687622 and perplexity is 98.9467326892279
At time: 688.7493343353271 and batch: 200, loss is 4.621583395004272 and perplexity is 101.65486457002191
At time: 689.8843336105347 and batch: 250, loss is 4.614698143005371 and perplexity is 100.95734925048966
At time: 691.0200138092041 and batch: 300, loss is 4.591903495788574 and perplexity is 98.68209244547778
At time: 692.1573331356049 and batch: 350, loss is 4.5356924438476565 and perplexity is 93.28808970854756
At time: 693.3066177368164 and batch: 400, loss is 4.570285863876343 and perplexity is 96.57171219138162
At time: 694.461835861206 and batch: 450, loss is 4.5880413913726805 and perplexity is 98.30170691760232
At time: 695.6226606369019 and batch: 500, loss is 4.581967039108276 and perplexity is 97.70639761127268
At time: 696.7592878341675 and batch: 550, loss is 4.5715478515625 and perplexity is 96.69366143603598
At time: 697.8962235450745 and batch: 600, loss is 4.604439325332642 and perplexity is 99.92694063581455
At time: 699.0468270778656 and batch: 650, loss is 4.579255495071411 and perplexity is 97.44182127876209
At time: 700.1833944320679 and batch: 700, loss is 4.550691032409668 and perplexity is 94.69782497598794
At time: 701.3183839321136 and batch: 750, loss is 4.546619119644165 and perplexity is 94.31300769650346
At time: 702.4771268367767 and batch: 800, loss is 4.520962791442871 and perplexity is 91.92405908400559
At time: 703.6133100986481 and batch: 850, loss is 4.5694834423065185 and perplexity is 96.49425204848535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.828282356262207 and perplexity of 124.9960774315357
Finished 33 epochs...
Completing Train Step...
At time: 706.6693818569183 and batch: 50, loss is 4.639194049835205 and perplexity is 103.46092961915907
At time: 707.8488178253174 and batch: 100, loss is 4.607375507354736 and perplexity is 100.22077548763714
At time: 708.9919300079346 and batch: 150, loss is 4.5936337757110595 and perplexity is 98.85298789459073
At time: 710.136470079422 and batch: 200, loss is 4.620732765197754 and perplexity is 101.56843067907484
At time: 711.2859060764313 and batch: 250, loss is 4.613832654953003 and perplexity is 100.87000967204744
At time: 712.430567741394 and batch: 300, loss is 4.591093721389771 and perplexity is 98.60221455929363
At time: 713.5736656188965 and batch: 350, loss is 4.5350534534454345 and perplexity is 93.22849855569719
At time: 714.7040774822235 and batch: 400, loss is 4.569691171646118 and perplexity is 96.51429881781756
At time: 715.8413662910461 and batch: 450, loss is 4.587436017990112 and perplexity is 98.2422156897931
At time: 716.9769690036774 and batch: 500, loss is 4.581538877487183 and perplexity is 97.66457243628653
At time: 718.115229845047 and batch: 550, loss is 4.571285123825073 and perplexity is 96.6682606660329
At time: 719.2515351772308 and batch: 600, loss is 4.604097919464111 and perplexity is 99.89283081483502
At time: 720.3869841098785 and batch: 650, loss is 4.579040069580078 and perplexity is 97.42083208742127
At time: 721.5217778682709 and batch: 700, loss is 4.550534524917603 and perplexity is 94.6830052166288
At time: 722.657701253891 and batch: 750, loss is 4.546520566940307 and perplexity is 94.30371335258486
At time: 723.7966618537903 and batch: 800, loss is 4.520857515335083 and perplexity is 91.91438218623519
At time: 724.9606964588165 and batch: 850, loss is 4.5694325828552245 and perplexity is 96.48934452857108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.828258832295735 and perplexity of 124.99313706258582
Finished 34 epochs...
Completing Train Step...
At time: 728.0486075878143 and batch: 50, loss is 4.6383260726928714 and perplexity is 103.37116685877267
At time: 729.1822938919067 and batch: 100, loss is 4.606514005661011 and perplexity is 100.13447230031686
At time: 730.3160462379456 and batch: 150, loss is 4.5927191257476805 and perplexity is 98.76261334967063
At time: 731.4497137069702 and batch: 200, loss is 4.61992377281189 and perplexity is 101.48629581972875
At time: 732.5868048667908 and batch: 250, loss is 4.612994394302368 and perplexity is 100.78548974192519
At time: 733.729775428772 and batch: 300, loss is 4.5903135108947755 and perplexity is 98.5253140798462
At time: 734.8644111156464 and batch: 350, loss is 4.5344371509552 and perplexity is 93.17105930167222
At time: 736.0195517539978 and batch: 400, loss is 4.56912091255188 and perplexity is 96.45927635121474
At time: 737.1547973155975 and batch: 450, loss is 4.586856775283813 and perplexity is 98.1853260809403
At time: 738.3028485774994 and batch: 500, loss is 4.581119737625122 and perplexity is 97.62364589843826
At time: 739.4433863162994 and batch: 550, loss is 4.571041803359986 and perplexity is 96.64474216127121
At time: 740.5943176746368 and batch: 600, loss is 4.603759174346924 and perplexity is 99.85899833677134
At time: 741.7416968345642 and batch: 650, loss is 4.578815813064575 and perplexity is 97.3989872805917
At time: 742.881231546402 and batch: 700, loss is 4.550374994277954 and perplexity is 94.6679015810211
At time: 744.0173780918121 and batch: 750, loss is 4.54641902923584 and perplexity is 94.29413845612301
At time: 745.1766374111176 and batch: 800, loss is 4.520744190216065 and perplexity is 91.90396656812113
At time: 746.3214502334595 and batch: 850, loss is 4.56937406539917 and perplexity is 96.48369838279359
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.828227361043294 and perplexity of 124.98920343391431
Finished 35 epochs...
Completing Train Step...
At time: 749.4246025085449 and batch: 50, loss is 4.637485332489014 and perplexity is 103.28429508628776
At time: 750.5806307792664 and batch: 100, loss is 4.605677661895752 and perplexity is 100.05076046953432
At time: 751.715414762497 and batch: 150, loss is 4.591834144592285 and perplexity is 98.67524896161905
At time: 752.8510723114014 and batch: 200, loss is 4.619154186248779 and perplexity is 101.4082233757312
At time: 754.0129747390747 and batch: 250, loss is 4.612181396484375 and perplexity is 100.70358465751843
At time: 755.1470301151276 and batch: 300, loss is 4.589557409286499 and perplexity is 98.45084708726858
At time: 756.2811942100525 and batch: 350, loss is 4.533842086791992 and perplexity is 93.11563303596144
At time: 757.4487638473511 and batch: 400, loss is 4.568568887710572 and perplexity is 96.4060431288766
At time: 758.6013379096985 and batch: 450, loss is 4.586297597885132 and perplexity is 98.1304384131155
At time: 759.734897851944 and batch: 500, loss is 4.580707092285156 and perplexity is 97.58337026623522
At time: 760.8700168132782 and batch: 550, loss is 4.570791759490967 and perplexity is 96.62057975697729
At time: 762.0040016174316 and batch: 600, loss is 4.60342776298523 and perplexity is 99.82590941348056
At time: 763.1380085945129 and batch: 650, loss is 4.578593950271607 and perplexity is 97.37738046620404
At time: 764.2698092460632 and batch: 700, loss is 4.550209751129151 and perplexity is 94.6522596512697
At time: 765.403778553009 and batch: 750, loss is 4.546310911178589 and perplexity is 94.28394410816945
At time: 766.5376269817352 and batch: 800, loss is 4.520631933212281 and perplexity is 91.89365028324676
At time: 767.670652627945 and batch: 850, loss is 4.569304399490356 and perplexity is 96.4769769923886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.828206698099772 and perplexity of 124.9866208157453
Finished 36 epochs...
Completing Train Step...
At time: 770.7755630016327 and batch: 50, loss is 4.6366655635833744 and perplexity is 103.19966052786447
At time: 771.9089436531067 and batch: 100, loss is 4.604857063293457 and perplexity is 99.96869263231599
At time: 773.0392003059387 and batch: 150, loss is 4.590980958938599 and perplexity is 98.59109655874721
At time: 774.2028546333313 and batch: 200, loss is 4.618415937423706 and perplexity is 101.33338650148751
At time: 775.3432862758636 and batch: 250, loss is 4.611384582519531 and perplexity is 100.6233745954462
At time: 776.5047452449799 and batch: 300, loss is 4.588826866149902 and perplexity is 98.37895076151804
At time: 777.6592175960541 and batch: 350, loss is 4.5332620906829835 and perplexity is 93.06164198992454
At time: 778.8259515762329 and batch: 400, loss is 4.5680238723754885 and perplexity is 96.35351467268401
At time: 779.9591906070709 and batch: 450, loss is 4.585759296417236 and perplexity is 98.07762886907523
At time: 781.1020810604095 and batch: 500, loss is 4.580296468734741 and perplexity is 97.54330846199815
At time: 782.2683086395264 and batch: 550, loss is 4.570521049499511 and perplexity is 96.59442714070384
At time: 783.4347269535065 and batch: 600, loss is 4.603086023330689 and perplexity is 99.79180077015332
At time: 784.5703611373901 and batch: 650, loss is 4.578366613388061 and perplexity is 97.35524551214209
At time: 785.7060587406158 and batch: 700, loss is 4.550040454864502 and perplexity is 94.63623673361849
At time: 786.8407030105591 and batch: 750, loss is 4.546196546554565 and perplexity is 94.27316197690902
At time: 787.9764535427094 and batch: 800, loss is 4.520509719848633 and perplexity is 91.88242033738648
At time: 789.114043712616 and batch: 850, loss is 4.569217882156372 and perplexity is 96.46863042261506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.828194300333659 and perplexity of 124.9850712704586
Finished 37 epochs...
Completing Train Step...
At time: 792.1687247753143 and batch: 50, loss is 4.635863161087036 and perplexity is 103.11688607629009
At time: 793.3256814479828 and batch: 100, loss is 4.6040518093109135 and perplexity is 99.88822484729454
At time: 794.4630582332611 and batch: 150, loss is 4.590157175064087 and perplexity is 98.50991224698625
At time: 795.5940217971802 and batch: 200, loss is 4.617699775695801 and perplexity is 101.26084138843348
At time: 796.7267265319824 and batch: 250, loss is 4.610601806640625 and perplexity is 100.54463986480306
At time: 797.860013961792 and batch: 300, loss is 4.588120336532593 and perplexity is 98.30946766790738
At time: 798.997784614563 and batch: 350, loss is 4.53269136428833 and perplexity is 93.00854440804811
At time: 800.1427736282349 and batch: 400, loss is 4.567482833862305 and perplexity is 96.30139781025166
At time: 801.2844183444977 and batch: 450, loss is 4.585237522125244 and perplexity is 98.0264678321294
At time: 802.4160232543945 and batch: 500, loss is 4.579890041351319 and perplexity is 97.50367224553702
At time: 803.5517299175262 and batch: 550, loss is 4.57023736000061 and perplexity is 96.56702820265092
At time: 804.6879920959473 and batch: 600, loss is 4.60273362159729 and perplexity is 99.75664016227637
At time: 805.8246581554413 and batch: 650, loss is 4.578129558563233 and perplexity is 97.33216971669346
At time: 806.9596238136292 and batch: 700, loss is 4.549859848022461 and perplexity is 94.6191463251282
At time: 808.0936713218689 and batch: 750, loss is 4.546070327758789 and perplexity is 94.26126368284027
At time: 809.2275347709656 and batch: 800, loss is 4.520371313095093 and perplexity is 91.86970406990913
At time: 810.3601291179657 and batch: 850, loss is 4.569115619659424 and perplexity is 96.45876580398978
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8281857172648115 and perplexity of 124.98399851959071
Finished 38 epochs...
Completing Train Step...
At time: 813.4137563705444 and batch: 50, loss is 4.635075922012329 and perplexity is 103.03574037903138
At time: 814.569682598114 and batch: 100, loss is 4.603264904022216 and perplexity is 99.80965319316316
At time: 815.6991209983826 and batch: 150, loss is 4.5893595504760745 and perplexity is 98.43136964673376
At time: 816.8271634578705 and batch: 200, loss is 4.617000341415405 and perplexity is 101.19004084775366
At time: 817.9610035419464 and batch: 250, loss is 4.609839305877686 and perplexity is 100.46800372146862
At time: 819.0893020629883 and batch: 300, loss is 4.587427854537964 and perplexity is 98.24141369743991
At time: 820.2214126586914 and batch: 350, loss is 4.5321282863616945 and perplexity is 92.95618809142988
At time: 821.357506275177 and batch: 400, loss is 4.56694917678833 and perplexity is 96.250019598469
At time: 822.4914696216583 and batch: 450, loss is 4.584726848602295 and perplexity is 97.97642109031996
At time: 823.6224541664124 and batch: 500, loss is 4.579488334655761 and perplexity is 97.46451223350067
At time: 824.7523634433746 and batch: 550, loss is 4.569943780899048 and perplexity is 96.5386823023562
At time: 825.8858118057251 and batch: 600, loss is 4.602373456954956 and perplexity is 99.72071781701949
At time: 827.021176815033 and batch: 650, loss is 4.5778843212127684 and perplexity is 97.30830315988239
At time: 828.1548008918762 and batch: 700, loss is 4.549668483734131 and perplexity is 94.6010413319091
At time: 829.2855830192566 and batch: 750, loss is 4.5459310340881345 and perplexity is 94.24813459984163
At time: 830.4202914237976 and batch: 800, loss is 4.520216598510742 and perplexity is 91.8554915862971
At time: 831.5552558898926 and batch: 850, loss is 4.568998460769653 and perplexity is 96.44746546406006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.82818063100179 and perplexity of 124.98336281971744
Finished 39 epochs...
Completing Train Step...
At time: 834.6421785354614 and batch: 50, loss is 4.634302940368652 and perplexity is 102.95612641710541
At time: 835.7747275829315 and batch: 100, loss is 4.602491283416748 and perplexity is 99.7324682486096
At time: 836.9077062606812 and batch: 150, loss is 4.588582487106323 and perplexity is 98.35491194503375
At time: 838.0425865650177 and batch: 200, loss is 4.616308088302612 and perplexity is 101.12001596726033
At time: 839.1937453746796 and batch: 250, loss is 4.609089012145996 and perplexity is 100.39265147973325
At time: 840.3373391628265 and batch: 300, loss is 4.586756324768066 and perplexity is 98.17546380964008
At time: 841.5052239894867 and batch: 350, loss is 4.531567363739014 and perplexity is 92.90406148347479
At time: 842.643828868866 and batch: 400, loss is 4.5664135932922365 and perplexity is 96.19848347865292
At time: 843.7803778648376 and batch: 450, loss is 4.584225883483887 and perplexity is 97.92735061325217
At time: 844.9155304431915 and batch: 500, loss is 4.579081115722656 and perplexity is 97.42483091885296
At time: 846.0465710163116 and batch: 550, loss is 4.569640340805054 and perplexity is 96.50939303951739
At time: 847.1800291538239 and batch: 600, loss is 4.602003402709961 and perplexity is 99.68382256912
At time: 848.3160285949707 and batch: 650, loss is 4.5776341438293455 and perplexity is 97.28396186815982
At time: 849.450888633728 and batch: 700, loss is 4.549465169906616 and perplexity is 94.58180958721515
At time: 850.5864946842194 and batch: 750, loss is 4.545773601531982 and perplexity is 94.23329804300816
At time: 851.7190985679626 and batch: 800, loss is 4.5200372982025145 and perplexity is 91.83902334476782
At time: 852.8521921634674 and batch: 850, loss is 4.5688651752471925 and perplexity is 96.4346112698937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.828183809916179 and perplexity of 124.98376013175935
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 855.9356024265289 and batch: 50, loss is 4.633935995101929 and perplexity is 102.91835408444967
At time: 857.0749938488007 and batch: 100, loss is 4.602187938690186 and perplexity is 99.70221951842765
At time: 858.2095928192139 and batch: 150, loss is 4.588477239608765 and perplexity is 98.34456088140031
At time: 859.3420701026917 and batch: 200, loss is 4.616064615249634 and perplexity is 101.09539896516549
At time: 860.4812655448914 and batch: 250, loss is 4.608271999359131 and perplexity is 100.31066289718783
At time: 861.6181190013885 and batch: 300, loss is 4.585665521621704 and perplexity is 98.06843209070148
At time: 862.7559525966644 and batch: 350, loss is 4.530656967163086 and perplexity is 92.81952043279111
At time: 863.9077088832855 and batch: 400, loss is 4.5644330883026125 and perplexity is 96.008150442107
At time: 865.0484185218811 and batch: 450, loss is 4.58243805885315 and perplexity is 97.75243009400111
At time: 866.2048819065094 and batch: 500, loss is 4.577233695983887 and perplexity is 97.24501251434495
At time: 867.3358047008514 and batch: 550, loss is 4.567233352661133 and perplexity is 96.27737541854363
At time: 868.4732842445374 and batch: 600, loss is 4.598947982788086 and perplexity is 99.37971146190942
At time: 869.6358332633972 and batch: 650, loss is 4.57405499458313 and perplexity is 96.93639042544552
At time: 870.7769641876221 and batch: 700, loss is 4.545782546997071 and perplexity is 94.23414100745634
At time: 871.9241058826447 and batch: 750, loss is 4.541639938354492 and perplexity is 93.84457331086705
At time: 873.0808100700378 and batch: 800, loss is 4.515670547485351 and perplexity is 91.43885956732167
At time: 874.2209796905518 and batch: 850, loss is 4.564794178009033 and perplexity is 96.04282425676617
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8278703689575195 and perplexity of 124.94459124105484
Finished 41 epochs...
Completing Train Step...
At time: 877.3345534801483 and batch: 50, loss is 4.633607482910156 and perplexity is 102.8845497032554
At time: 878.4741055965424 and batch: 100, loss is 4.601734590530396 and perplexity is 99.65702994476098
At time: 879.6144044399261 and batch: 150, loss is 4.5880414009094235 and perplexity is 98.30170785508044
At time: 880.7522735595703 and batch: 200, loss is 4.615592279434204 and perplexity is 101.04765926293223
At time: 881.8904101848602 and batch: 250, loss is 4.60790696144104 and perplexity is 100.27405238416083
At time: 883.0266149044037 and batch: 300, loss is 4.585381956100464 and perplexity is 98.04062720707796
At time: 884.1636679172516 and batch: 350, loss is 4.5303825092315675 and perplexity is 92.79404887480402
At time: 885.3015122413635 and batch: 400, loss is 4.564234828948974 and perplexity is 95.98911781501685
At time: 886.4384400844574 and batch: 450, loss is 4.582359991073608 and perplexity is 97.74479907671108
At time: 887.5796241760254 and batch: 500, loss is 4.577144784927368 and perplexity is 97.23636674189918
At time: 888.7321729660034 and batch: 550, loss is 4.567163114547729 and perplexity is 96.27061331481227
At time: 889.8619916439056 and batch: 600, loss is 4.598862829208374 and perplexity is 99.37124928402518
At time: 890.996680021286 and batch: 650, loss is 4.573959999084472 and perplexity is 96.92718234206916
At time: 892.1336619853973 and batch: 700, loss is 4.545747261047364 and perplexity is 94.23081592496077
At time: 893.2707109451294 and batch: 750, loss is 4.541705846786499 and perplexity is 93.85075866337756
At time: 894.4120986461639 and batch: 800, loss is 4.515810289382935 and perplexity is 91.45163829991183
At time: 895.5513575077057 and batch: 850, loss is 4.564915657043457 and perplexity is 96.05449215500836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.827776908874512 and perplexity of 124.93291445485127
Finished 42 epochs...
Completing Train Step...
At time: 898.6540381908417 and batch: 50, loss is 4.633373041152954 and perplexity is 102.86043209583161
At time: 899.8357121944427 and batch: 100, loss is 4.601447629928589 and perplexity is 99.62843640627946
At time: 900.9675076007843 and batch: 150, loss is 4.587779455184936 and perplexity is 98.27596151521705
At time: 902.1048295497894 and batch: 200, loss is 4.615320148468018 and perplexity is 101.02016480700226
At time: 903.2419211864471 and batch: 250, loss is 4.607665061950684 and perplexity is 100.24979907554285
At time: 904.3760588169098 and batch: 300, loss is 4.58518931388855 and perplexity is 98.02174226287245
At time: 905.5288803577423 and batch: 350, loss is 4.5302097034454345 and perplexity is 92.77801491166045
At time: 906.6620876789093 and batch: 400, loss is 4.564097423553466 and perplexity is 95.97592929842645
At time: 907.8016119003296 and batch: 450, loss is 4.582328643798828 and perplexity is 97.7417350916601
At time: 908.949045419693 and batch: 500, loss is 4.577133274078369 and perplexity is 97.23524747520624
At time: 910.0970599651337 and batch: 550, loss is 4.567133779525757 and perplexity is 96.26778925567749
At time: 911.236508846283 and batch: 600, loss is 4.598830184936523 and perplexity is 99.36800543489622
At time: 912.3847503662109 and batch: 650, loss is 4.573928546905518 and perplexity is 96.92413381892617
At time: 913.5334253311157 and batch: 700, loss is 4.5457338619232175 and perplexity is 94.22955332301868
At time: 914.6707837581635 and batch: 750, loss is 4.541759595870972 and perplexity is 93.85580319130096
At time: 915.8066077232361 and batch: 800, loss is 4.515910091400147 and perplexity is 91.46076581335608
At time: 916.940450668335 and batch: 850, loss is 4.564974889755249 and perplexity is 96.06018189156615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.827731132507324 and perplexity of 124.92719561078033
Finished 43 epochs...
Completing Train Step...
At time: 920.006995677948 and batch: 50, loss is 4.633162670135498 and perplexity is 102.8387955180099
At time: 921.1717989444733 and batch: 100, loss is 4.601214141845703 and perplexity is 99.60517706915672
At time: 922.3055109977722 and batch: 150, loss is 4.587556896209716 and perplexity is 98.25409175167982
At time: 923.4527344703674 and batch: 200, loss is 4.6150993061065675 and perplexity is 100.99785773851566
At time: 924.5985329151154 and batch: 250, loss is 4.607457447052002 and perplexity is 100.22898788409637
At time: 925.7361078262329 and batch: 300, loss is 4.585025138854981 and perplexity is 98.00565086098534
At time: 926.8736054897308 and batch: 350, loss is 4.530064573287964 and perplexity is 92.76455100077992
At time: 928.0367138385773 and batch: 400, loss is 4.563980684280396 and perplexity is 95.9647257921652
At time: 929.1736991405487 and batch: 450, loss is 4.58230562210083 and perplexity is 97.73948493685434
At time: 930.3086757659912 and batch: 500, loss is 4.5771408367156985 and perplexity is 97.23598283289917
At time: 931.4401814937592 and batch: 550, loss is 4.567114992141724 and perplexity is 96.26598065274021
At time: 932.5775496959686 and batch: 600, loss is 4.598808374404907 and perplexity is 99.3658381895065
At time: 933.7147924900055 and batch: 650, loss is 4.573908405303955 and perplexity is 96.92218163130114
At time: 934.8526952266693 and batch: 700, loss is 4.545722332000732 and perplexity is 94.22846686983642
At time: 935.9908564090729 and batch: 750, loss is 4.5417975902557375 and perplexity is 93.85936925254461
At time: 937.1255576610565 and batch: 800, loss is 4.515983762741089 and perplexity is 91.46750409882338
At time: 938.2634935379028 and batch: 850, loss is 4.565006093978882 and perplexity is 96.06317942173163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.827704111735026 and perplexity of 124.92382002707957
Finished 44 epochs...
Completing Train Step...
At time: 941.3610761165619 and batch: 50, loss is 4.632967891693116 and perplexity is 102.8187666882579
At time: 942.5188186168671 and batch: 100, loss is 4.6010091018676755 and perplexity is 99.58475611946614
At time: 943.6563975811005 and batch: 150, loss is 4.5873559474945065 and perplexity is 98.23434970181441
At time: 944.7928094863892 and batch: 200, loss is 4.61490327835083 and perplexity is 100.97806129551846
At time: 945.9280936717987 and batch: 250, loss is 4.607267551422119 and perplexity is 100.2099566443414
At time: 947.066123008728 and batch: 300, loss is 4.58487455368042 and perplexity is 97.99089377406963
At time: 948.1998143196106 and batch: 350, loss is 4.529934492111206 and perplexity is 92.75248486362997
At time: 949.3347880840302 and batch: 400, loss is 4.563874111175537 and perplexity is 95.95449907833664
At time: 950.4825441837311 and batch: 450, loss is 4.5822837543487545 and perplexity is 97.737347617399
At time: 951.6371731758118 and batch: 500, loss is 4.577153739929199 and perplexity is 97.23723749764017
At time: 952.7728967666626 and batch: 550, loss is 4.567098360061646 and perplexity is 96.26437956255602
At time: 953.9081826210022 and batch: 600, loss is 4.598787460327149 and perplexity is 99.36376006637116
At time: 955.0442147254944 and batch: 650, loss is 4.573889207839966 and perplexity is 96.92032098906938
At time: 956.1805920600891 and batch: 700, loss is 4.545708427429199 and perplexity is 94.22715667248725
At time: 957.3425216674805 and batch: 750, loss is 4.5418240928649904 and perplexity is 93.8618568036958
At time: 958.4773802757263 and batch: 800, loss is 4.51604076385498 and perplexity is 91.47271799703945
At time: 959.6416971683502 and batch: 850, loss is 4.565022087097168 and perplexity is 96.0647157838086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.827685991923015 and perplexity of 124.92155645145274
Finished 45 epochs...
Completing Train Step...
At time: 962.7395133972168 and batch: 50, loss is 4.632784900665283 and perplexity is 102.79995349783614
At time: 963.867582321167 and batch: 100, loss is 4.6008217144012455 and perplexity is 99.56609693262534
At time: 965.0007266998291 and batch: 150, loss is 4.587169637680054 and perplexity is 98.21604938316602
At time: 966.1323637962341 and batch: 200, loss is 4.614722776412964 and perplexity is 100.9598362046542
At time: 967.2663149833679 and batch: 250, loss is 4.607088279724121 and perplexity is 100.19199344545221
At time: 968.4075467586517 and batch: 300, loss is 4.584731588363647 and perplexity is 97.97688547627462
At time: 969.5464420318604 and batch: 350, loss is 4.529813928604126 and perplexity is 92.74130297284195
At time: 970.6763560771942 and batch: 400, loss is 4.56377326965332 and perplexity is 95.94482336845087
At time: 971.8208031654358 and batch: 450, loss is 4.582261552810669 and perplexity is 97.7351777220411
At time: 972.9514336585999 and batch: 500, loss is 4.577167577743531 and perplexity is 97.23858305778857
At time: 974.0863256454468 and batch: 550, loss is 4.567081527709961 and perplexity is 96.2627592203016
At time: 975.2185356616974 and batch: 600, loss is 4.598764448165894 and perplexity is 99.36147351781095
At time: 976.3500680923462 and batch: 650, loss is 4.573869018554688 and perplexity is 96.91836425681225
At time: 977.481537103653 and batch: 700, loss is 4.545691766738892 and perplexity is 94.22558679608903
At time: 978.6149592399597 and batch: 750, loss is 4.541842012405396 and perplexity is 93.8635387801014
At time: 979.7616045475006 and batch: 800, loss is 4.516086263656616 and perplexity is 91.4768800822497
At time: 980.9123930931091 and batch: 850, loss is 4.56502872467041 and perplexity is 96.06535342251178
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.827671368916829 and perplexity of 124.91972973611611
Finished 46 epochs...
Completing Train Step...
At time: 984.0182867050171 and batch: 50, loss is 4.632611112594605 and perplexity is 102.78208964455919
At time: 985.1501643657684 and batch: 100, loss is 4.600646238327027 and perplexity is 99.54862699763295
At time: 986.3085625171661 and batch: 150, loss is 4.586994295120239 and perplexity is 98.19882943939092
At time: 987.4417426586151 and batch: 200, loss is 4.614552783966064 and perplexity is 100.94267525371654
At time: 988.5770907402039 and batch: 250, loss is 4.60691764831543 and perplexity is 100.1748990029369
At time: 989.7174000740051 and batch: 300, loss is 4.584594039916992 and perplexity is 97.9634098346673
At time: 990.8843700885773 and batch: 350, loss is 4.529699392318726 and perplexity is 92.73068133678926
At time: 992.0182118415833 and batch: 400, loss is 4.563675994873047 and perplexity is 95.93549081075793
At time: 993.1508233547211 and batch: 450, loss is 4.582239179611206 and perplexity is 97.73299109787638
At time: 994.2896001338959 and batch: 500, loss is 4.577180871963501 and perplexity is 97.23987577749416
At time: 995.4263069629669 and batch: 550, loss is 4.567064189910889 and perplexity is 96.2610902503923
At time: 996.5665657520294 and batch: 600, loss is 4.598739185333252 and perplexity is 99.35896339724093
At time: 997.7018868923187 and batch: 650, loss is 4.573846387863159 and perplexity is 96.91617095202538
At time: 998.8376836776733 and batch: 700, loss is 4.545672369003296 and perplexity is 94.22375905079714
At time: 999.9711625576019 and batch: 750, loss is 4.5418532180786135 and perplexity is 93.86459059013713
At time: 1001.1050260066986 and batch: 800, loss is 4.516123113632202 and perplexity is 91.48025106515732
At time: 1002.2383894920349 and batch: 850, loss is 4.565029525756836 and perplexity is 96.06543037919329
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.827661196390788 and perplexity of 124.91845899337568
Finished 47 epochs...
Completing Train Step...
At time: 1005.3324172496796 and batch: 50, loss is 4.6324444770812985 and perplexity is 102.76496392520875
At time: 1006.4656457901001 and batch: 100, loss is 4.600479402542114 and perplexity is 99.53202010966095
At time: 1007.5969421863556 and batch: 150, loss is 4.58682770729065 and perplexity is 98.18247207203336
At time: 1008.7298924922943 and batch: 200, loss is 4.614390449523926 and perplexity is 100.92629011081377
At time: 1009.8646278381348 and batch: 250, loss is 4.606753520965576 and perplexity is 100.15845891141288
At time: 1011.0232374668121 and batch: 300, loss is 4.584460000991822 and perplexity is 97.95027980449423
At time: 1012.1599416732788 and batch: 350, loss is 4.529589357376099 and perplexity is 92.72047828294514
At time: 1013.2963962554932 and batch: 400, loss is 4.5635809803009035 and perplexity is 95.92637597417318
At time: 1014.4333817958832 and batch: 450, loss is 4.582216396331787 and perplexity is 97.73076444519701
At time: 1015.5959415435791 and batch: 500, loss is 4.57719310760498 and perplexity is 97.24106557703067
At time: 1016.7313876152039 and batch: 550, loss is 4.567046022415161 and perplexity is 96.25934144333216
At time: 1017.8656604290009 and batch: 600, loss is 4.598711824417114 and perplexity is 99.35624488216655
At time: 1018.9989976882935 and batch: 650, loss is 4.5738217163085935 and perplexity is 96.91377990892096
At time: 1020.1334068775177 and batch: 700, loss is 4.545651197433472 and perplexity is 94.22176420702034
At time: 1021.2689905166626 and batch: 750, loss is 4.541859521865844 and perplexity is 93.8651822944097
At time: 1022.4054141044617 and batch: 800, loss is 4.516153469085693 and perplexity is 91.48302803181173
At time: 1023.5417788028717 and batch: 850, loss is 4.565026063919067 and perplexity is 96.06509781683371
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.827651341756185 and perplexity of 124.9172279736727
Finished 48 epochs...
Completing Train Step...
At time: 1026.6289541721344 and batch: 50, loss is 4.632284183502197 and perplexity is 102.74849268148749
At time: 1027.786833524704 and batch: 100, loss is 4.6003192520141605 and perplexity is 99.5160812804321
At time: 1028.9220898151398 and batch: 150, loss is 4.586667747497558 and perplexity is 98.16676808015266
At time: 1030.0779538154602 and batch: 200, loss is 4.614233846664429 and perplexity is 100.91048600270055
At time: 1031.209985256195 and batch: 250, loss is 4.606595039367676 and perplexity is 100.14258689654575
At time: 1032.3445348739624 and batch: 300, loss is 4.584329013824463 and perplexity is 97.93745041506166
At time: 1033.4844081401825 and batch: 350, loss is 4.529482879638672 and perplexity is 92.71060614179537
At time: 1034.6235165596008 and batch: 400, loss is 4.563487854003906 and perplexity is 95.91744312194251
At time: 1035.759192943573 and batch: 450, loss is 4.5821937084197994 and perplexity is 97.72854716336767
At time: 1036.8970324993134 and batch: 500, loss is 4.577203874588013 and perplexity is 97.24211257557026
At time: 1038.0347998142242 and batch: 550, loss is 4.567026920318604 and perplexity is 96.25750270565932
At time: 1039.1732032299042 and batch: 600, loss is 4.598683185577393 and perplexity is 99.35339947533886
At time: 1040.3101227283478 and batch: 650, loss is 4.5737952136993405 and perplexity is 96.91121147491606
At time: 1041.4485161304474 and batch: 700, loss is 4.545629110336304 and perplexity is 94.21968314474135
At time: 1042.5848355293274 and batch: 750, loss is 4.541862287521362 and perplexity is 93.86544189352804
At time: 1043.7578732967377 and batch: 800, loss is 4.516178541183471 and perplexity is 91.48532173198942
At time: 1044.9052834510803 and batch: 850, loss is 4.565019512176514 and perplexity is 96.06446842510631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.82764212290446 and perplexity of 124.91607638557832
Finished 49 epochs...
Completing Train Step...
At time: 1048.0547959804535 and batch: 50, loss is 4.632128772735595 and perplexity is 102.73252570022525
At time: 1049.2143182754517 and batch: 100, loss is 4.600163984298706 and perplexity is 99.50063084534862
At time: 1050.348533630371 and batch: 150, loss is 4.586513090133667 and perplexity is 98.15158704053968
At time: 1051.5080761909485 and batch: 200, loss is 4.614081525802613 and perplexity is 100.89511640109153
At time: 1052.6668956279755 and batch: 250, loss is 4.606441297531128 and perplexity is 100.12719197477172
At time: 1053.7959463596344 and batch: 300, loss is 4.584200620651245 and perplexity is 97.92487672223146
At time: 1054.9305913448334 and batch: 350, loss is 4.529379634857178 and perplexity is 92.70103474962873
At time: 1056.069337129593 and batch: 400, loss is 4.563396043777466 and perplexity is 95.90863732400727
At time: 1057.2033932209015 and batch: 450, loss is 4.582171001434326 and perplexity is 97.72632806786149
At time: 1058.3379518985748 and batch: 500, loss is 4.577213220596313 and perplexity is 97.24302140540853
At time: 1059.4706840515137 and batch: 550, loss is 4.567006578445435 and perplexity is 96.25554466766286
At time: 1060.6023247241974 and batch: 600, loss is 4.598653268814087 and perplexity is 99.35042718766397
At time: 1061.7378113269806 and batch: 650, loss is 4.573766899108887 and perplexity is 96.90846751250005
At time: 1062.8724710941315 and batch: 700, loss is 4.54560661315918 and perplexity is 94.21756349168429
At time: 1064.0039014816284 and batch: 750, loss is 4.541861934661865 and perplexity is 93.86540877222126
At time: 1065.1348741054535 and batch: 800, loss is 4.516199064254761 and perplexity is 91.48719931103605
At time: 1066.269716501236 and batch: 850, loss is 4.565010433197021 and perplexity is 96.06359626172669
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.827635129292806 and perplexity of 124.91520277410557
Finished Training.
Improved accuracyfrom -126.01780898549613 to -124.91520277410557
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fe9c612d9e8>
SETTINGS FOR THIS RUN
{'dropout': 0.8517377223397841, 'batch_size': 50, 'data': 'wikitext', 'lr': 13.965449617430208, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'anneal': 3.476424992133352, 'wordvec_source': '', 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.652632713317871 and batch: 50, loss is 7.6574062252044675 and perplexity is 2116.2611987106407
At time: 2.7845213413238525 and batch: 100, loss is 6.916262102127075 and perplexity is 1008.5431089843722
At time: 3.9173054695129395 and batch: 150, loss is 6.744251136779785 and perplexity is 849.1629815893995
At time: 5.049376726150513 and batch: 200, loss is 6.731085605621338 and perplexity is 838.0565712160982
At time: 6.209020614624023 and batch: 250, loss is 6.730377044677734 and perplexity is 837.4629673882833
At time: 7.341574668884277 and batch: 300, loss is 6.648198432922364 and perplexity is 771.393356074761
At time: 8.475332021713257 and batch: 350, loss is 6.619373769760132 and perplexity is 749.4756058339908
At time: 9.61284589767456 and batch: 400, loss is 6.631781339645386 and perplexity is 758.832706187178
At time: 10.753401279449463 and batch: 450, loss is 6.615199508666993 and perplexity is 746.3536194985892
At time: 11.906554937362671 and batch: 500, loss is 6.602616510391235 and perplexity is 737.0210919166043
At time: 13.038801908493042 and batch: 550, loss is 6.539869470596313 and perplexity is 692.1962201795872
At time: 14.173742055892944 and batch: 600, loss is 6.547814283370972 and perplexity is 697.7174932507655
At time: 15.306441307067871 and batch: 650, loss is 6.562846126556397 and perplexity is 708.2846964916135
At time: 16.443562269210815 and batch: 700, loss is 6.5229647254943846 and perplexity is 680.5931691430897
At time: 17.577397346496582 and batch: 750, loss is 6.500547847747803 and perplexity is 665.5061292250842
At time: 18.71436595916748 and batch: 800, loss is 6.519076900482178 and perplexity is 677.9522789828286
At time: 19.849833488464355 and batch: 850, loss is 6.517317867279052 and perplexity is 676.7607866584692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.795891443888347 and perplexity of 328.9452895869438
Finished 1 epochs...
Completing Train Step...
At time: 22.989351749420166 and batch: 50, loss is 6.073248262405396 and perplexity is 434.08842709161087
At time: 24.11987566947937 and batch: 100, loss is 5.838789224624634 and perplexity is 343.3633529893658
At time: 25.24974751472473 and batch: 150, loss is 5.743793268203735 and perplexity is 312.2466024880743
At time: 26.381995916366577 and batch: 200, loss is 5.708840379714966 and perplexity is 301.52121535602436
At time: 27.531894207000732 and batch: 250, loss is 5.7268229103088375 and perplexity is 306.99237505023547
At time: 28.668813467025757 and batch: 300, loss is 5.662092180252075 and perplexity is 287.7500381700639
At time: 29.80192804336548 and batch: 350, loss is 5.6269598484039305 and perplexity is 277.8162290176777
At time: 30.935561656951904 and batch: 400, loss is 5.617139530181885 and perplexity is 275.1013376086
At time: 32.06025004386902 and batch: 450, loss is 5.611686038970947 and perplexity is 273.60515829031516
At time: 33.190804958343506 and batch: 500, loss is 5.61779676437378 and perplexity is 275.28220304287555
At time: 34.33665752410889 and batch: 550, loss is 5.57484094619751 and perplexity is 263.7076086032158
At time: 35.47081685066223 and batch: 600, loss is 5.5841026115417485 and perplexity is 266.16132543525947
At time: 36.60194420814514 and batch: 650, loss is 5.608078441619873 and perplexity is 272.61987935945444
At time: 37.73350238800049 and batch: 700, loss is 5.561596736907959 and perplexity is 260.23803642969057
At time: 38.88173031806946 and batch: 750, loss is 5.574054059982299 and perplexity is 263.5001823422956
At time: 40.05564045906067 and batch: 800, loss is 5.577104949951172 and perplexity is 264.3053199740484
At time: 41.199317932128906 and batch: 850, loss is 5.562576189041137 and perplexity is 260.4930519969895
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.249344189961751 and perplexity of 190.44133415788517
Finished 2 epochs...
Completing Train Step...
At time: 44.300426959991455 and batch: 50, loss is 5.536677017211914 and perplexity is 253.83311339821879
At time: 45.451887130737305 and batch: 100, loss is 5.460829353332519 and perplexity is 235.29248407313133
At time: 46.6045663356781 and batch: 150, loss is 5.46428539276123 and perplexity is 236.1070709868138
At time: 47.74751925468445 and batch: 200, loss is 5.474623985290528 and perplexity is 238.5607477136009
At time: 48.87442994117737 and batch: 250, loss is 5.511760120391846 and perplexity is 247.58652587099297
At time: 50.024951219558716 and batch: 300, loss is 5.445317277908325 and perplexity is 231.6707720707397
At time: 51.168283462524414 and batch: 350, loss is 5.433072643280029 and perplexity is 228.85134476783062
At time: 52.304683208465576 and batch: 400, loss is 5.436816539764404 and perplexity is 229.70974639369868
At time: 53.436129093170166 and batch: 450, loss is 5.453705062866211 and perplexity is 233.62214911265897
At time: 54.565422773361206 and batch: 500, loss is 5.481543979644775 and perplexity is 240.21731183755847
At time: 55.6932213306427 and batch: 550, loss is 5.4502753639221195 and perplexity is 232.82226793441944
At time: 56.824601888656616 and batch: 600, loss is 5.446872615814209 and perplexity is 232.03137876416193
At time: 57.95600962638855 and batch: 650, loss is 5.446864109039307 and perplexity is 232.029404933848
At time: 59.09293079376221 and batch: 700, loss is 5.410926380157471 and perplexity is 223.83885153369494
At time: 60.220543384552 and batch: 750, loss is 5.451621313095092 and perplexity is 233.13584585602828
At time: 61.350292682647705 and batch: 800, loss is 5.449731664657593 and perplexity is 232.6957170445084
At time: 62.48951816558838 and batch: 850, loss is 5.441984071731567 and perplexity is 230.899851155239
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.212338765462239 and perplexity of 183.5227733900937
Finished 3 epochs...
Completing Train Step...
At time: 65.58819675445557 and batch: 50, loss is 5.457945537567139 and perplexity is 234.61492135068934
At time: 66.76960802078247 and batch: 100, loss is 5.357018518447876 and perplexity is 212.09165548781306
At time: 67.90545678138733 and batch: 150, loss is 5.383065767288208 and perplexity is 217.6886361492902
At time: 69.04823088645935 and batch: 200, loss is 5.399112005233764 and perplexity is 221.20989573287534
At time: 70.18258118629456 and batch: 250, loss is 5.456726217269898 and perplexity is 234.3290249501141
At time: 71.31934452056885 and batch: 300, loss is 5.400612649917602 and perplexity is 221.54210238665138
At time: 72.45590543746948 and batch: 350, loss is 5.349262809753418 and perplexity is 210.45309669706072
At time: 73.5927939414978 and batch: 400, loss is 5.345657234191894 and perplexity is 209.69565847570203
At time: 74.73650860786438 and batch: 450, loss is 5.37481915473938 and perplexity is 215.90082413907265
At time: 75.8735044002533 and batch: 500, loss is 5.407621259689331 and perplexity is 223.10025840591302
At time: 77.01185154914856 and batch: 550, loss is 5.389291191101075 and perplexity is 219.04806729558393
At time: 78.15063834190369 and batch: 600, loss is 5.439168968200684 and perplexity is 230.25075822952627
At time: 79.28551888465881 and batch: 650, loss is 5.419294805526733 and perplexity is 225.719889944777
At time: 80.44132351875305 and batch: 700, loss is 5.369800243377686 and perplexity is 214.81995171014174
At time: 81.57687282562256 and batch: 750, loss is 5.393676977157593 and perplexity is 220.01087504592988
At time: 82.71115136146545 and batch: 800, loss is 5.395440835952758 and perplexity is 220.39928561290714
At time: 83.84291410446167 and batch: 850, loss is 5.3727768039703365 and perplexity is 215.46032890086536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.25849978129069 and perplexity of 192.19294345935668
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 86.9526379108429 and batch: 50, loss is 5.38948224067688 and perplexity is 219.08992033379712
At time: 88.0866150856018 and batch: 100, loss is 5.261931076049804 and perplexity is 192.85354681325052
At time: 89.2229654788971 and batch: 150, loss is 5.2509494876861575 and perplexity is 190.7472947114274
At time: 90.3584041595459 and batch: 200, loss is 5.245049886703491 and perplexity is 189.625274773422
At time: 91.4937515258789 and batch: 250, loss is 5.277873306274414 and perplexity is 195.95270052876702
At time: 92.63789701461792 and batch: 300, loss is 5.249978046417237 and perplexity is 190.5620848922058
At time: 93.78589487075806 and batch: 350, loss is 5.19335165977478 and perplexity is 180.071079629734
At time: 94.92151021957397 and batch: 400, loss is 5.18425500869751 and perplexity is 178.44046366578021
At time: 96.10371923446655 and batch: 450, loss is 5.199596376419067 and perplexity is 181.1990908874867
At time: 97.23800849914551 and batch: 500, loss is 5.247494802474976 and perplexity is 190.08945981382428
At time: 98.374431848526 and batch: 550, loss is 5.2267586040496825 and perplexity is 186.188314306996
At time: 99.50834631919861 and batch: 600, loss is 5.211201934814453 and perplexity is 183.31425762275148
At time: 100.65373611450195 and batch: 650, loss is 5.206949872970581 and perplexity is 182.5364488798149
At time: 101.81111240386963 and batch: 700, loss is 5.133419666290283 and perplexity is 169.59608956643046
At time: 102.94775700569153 and batch: 750, loss is 5.1300765228271485 and perplexity is 169.0300522074695
At time: 104.08647990226746 and batch: 800, loss is 5.114548511505127 and perplexity is 166.42562476367823
At time: 105.22346997261047 and batch: 850, loss is 5.117326898574829 and perplexity is 166.8886625187787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.079171816507976 and perplexity of 160.64096063081303
Finished 5 epochs...
Completing Train Step...
At time: 108.33578705787659 and batch: 50, loss is 5.235050830841065 and perplexity is 187.73864901509566
At time: 109.47147560119629 and batch: 100, loss is 5.140596027374268 and perplexity is 170.8175499217584
At time: 110.60600185394287 and batch: 150, loss is 5.139057121276855 and perplexity is 170.5548799167136
At time: 111.7401237487793 and batch: 200, loss is 5.150652847290039 and perplexity is 172.54409850199974
At time: 112.87655878067017 and batch: 250, loss is 5.179874248504639 and perplexity is 177.6604685190113
At time: 114.01439237594604 and batch: 300, loss is 5.155943489074707 and perplexity is 173.45938661502367
At time: 115.15173840522766 and batch: 350, loss is 5.10592490196228 and perplexity is 164.99660566509453
At time: 116.29019403457642 and batch: 400, loss is 5.102296915054321 and perplexity is 164.39908469440854
At time: 117.42581963539124 and batch: 450, loss is 5.124318056106567 and perplexity is 168.059495418505
At time: 118.5603654384613 and batch: 500, loss is 5.168223161697387 and perplexity is 175.60254279357315
At time: 119.69762754440308 and batch: 550, loss is 5.15172966003418 and perplexity is 172.7299962567659
At time: 120.83288860321045 and batch: 600, loss is 5.146383581161499 and perplexity is 171.80903204024122
At time: 121.96802997589111 and batch: 650, loss is 5.142841138839722 and perplexity is 171.20148518905418
At time: 123.10142827033997 and batch: 700, loss is 5.076611652374267 and perplexity is 160.23021941181833
At time: 124.23815131187439 and batch: 750, loss is 5.083445854187012 and perplexity is 161.3290154881244
At time: 125.41665625572205 and batch: 800, loss is 5.074808330535888 and perplexity is 159.9415331333532
At time: 126.55226254463196 and batch: 850, loss is 5.082157859802246 and perplexity is 161.12135838137695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.06809933980306 and perplexity of 158.87207836223462
Finished 6 epochs...
Completing Train Step...
At time: 129.69963574409485 and batch: 50, loss is 5.170663471221924 and perplexity is 176.03159064314315
At time: 130.83176064491272 and batch: 100, loss is 5.079605216979981 and perplexity is 160.71059758822884
At time: 131.96405935287476 and batch: 150, loss is 5.082103834152222 and perplexity is 161.11265393039162
At time: 133.0967824459076 and batch: 200, loss is 5.0952651309967045 and perplexity is 163.24712075904768
At time: 134.2316324710846 and batch: 250, loss is 5.1222539806365965 and perplexity is 167.71296569130064
At time: 135.36551785469055 and batch: 300, loss is 5.098232164382934 and perplexity is 163.73219968337
At time: 136.50051927566528 and batch: 350, loss is 5.053849239349365 and perplexity is 156.62418964519821
At time: 137.63253140449524 and batch: 400, loss is 5.056011667251587 and perplexity is 156.96324462189986
At time: 138.76707482337952 and batch: 450, loss is 5.077934322357177 and perplexity is 160.44229133305413
At time: 139.90104913711548 and batch: 500, loss is 5.1190463829040525 and perplexity is 167.17587181388166
At time: 141.03273725509644 and batch: 550, loss is 5.102883787155151 and perplexity is 164.4955942472297
At time: 142.16452193260193 and batch: 600, loss is 5.104832496643066 and perplexity is 164.81646090887338
At time: 143.29913640022278 and batch: 650, loss is 5.099833164215088 and perplexity is 163.99454485883524
At time: 144.43156719207764 and batch: 700, loss is 5.04042426109314 and perplexity is 154.53556454739564
At time: 145.5643012523651 and batch: 750, loss is 5.047358598709106 and perplexity is 155.61089035281216
At time: 146.69616866111755 and batch: 800, loss is 5.041332082748413 and perplexity is 154.6759189781582
At time: 147.831640958786 and batch: 850, loss is 5.050165395736695 and perplexity is 156.04827207102306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.0630489985148115 and perplexity of 158.07174283023184
Finished 7 epochs...
Completing Train Step...
At time: 150.86319518089294 and batch: 50, loss is 5.121821727752685 and perplexity is 167.64048694391357
At time: 152.02246499061584 and batch: 100, loss is 5.0342127799987795 and perplexity is 153.5786448308613
At time: 153.15836381912231 and batch: 150, loss is 5.040603818893433 and perplexity is 154.5633151047728
At time: 154.34366989135742 and batch: 200, loss is 5.054270639419555 and perplexity is 156.6902049981685
At time: 155.48137021064758 and batch: 250, loss is 5.0787318992614745 and perplexity is 160.57030744365426
At time: 156.61720085144043 and batch: 300, loss is 5.052001094818115 and perplexity is 156.3349928267192
At time: 157.78436017036438 and batch: 350, loss is 5.010383367538452 and perplexity is 149.96221577595404
At time: 158.93389058113098 and batch: 400, loss is 5.0147590160369875 and perplexity is 150.61983542722274
At time: 160.07023096084595 and batch: 450, loss is 5.038989772796631 and perplexity is 154.31404401095278
At time: 161.20437479019165 and batch: 500, loss is 5.078335638046265 and perplexity is 160.50669226344675
At time: 162.33832931518555 and batch: 550, loss is 5.063200359344482 and perplexity is 158.09567051118526
At time: 163.47423267364502 and batch: 600, loss is 5.069027595520019 and perplexity is 159.0196207451756
At time: 164.61317133903503 and batch: 650, loss is 5.063504114151001 and perplexity is 158.14370012523653
At time: 165.74910402297974 and batch: 700, loss is 5.008680429458618 and perplexity is 149.70705672986426
At time: 166.89228010177612 and batch: 750, loss is 5.018619985580444 and perplexity is 151.20249812209045
At time: 168.03010773658752 and batch: 800, loss is 5.01188515663147 and perplexity is 150.18759659083616
At time: 169.1670434474945 and batch: 850, loss is 5.021761159896851 and perplexity is 151.6781982631574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.0612227121988935 and perplexity of 157.78332201902273
Finished 8 epochs...
Completing Train Step...
At time: 172.2551429271698 and batch: 50, loss is 5.0811545848846436 and perplexity is 160.95979042591384
At time: 173.4133620262146 and batch: 100, loss is 4.998321552276611 and perplexity is 148.16426431044044
At time: 174.54313158988953 and batch: 150, loss is 5.0064763259887695 and perplexity is 149.37745026348094
At time: 175.67780423164368 and batch: 200, loss is 5.019225425720215 and perplexity is 151.29406990150548
At time: 176.8104064464569 and batch: 250, loss is 5.041820764541626 and perplexity is 154.75152475569013
At time: 177.94206285476685 and batch: 300, loss is 5.016206521987915 and perplexity is 150.8380164063684
At time: 179.07147192955017 and batch: 350, loss is 4.9765978527069095 and perplexity is 144.98029733024399
At time: 180.22066640853882 and batch: 400, loss is 4.98083119392395 and perplexity is 145.59534934257033
At time: 181.36139607429504 and batch: 450, loss is 5.006064662933349 and perplexity is 149.31596974139265
At time: 182.5602068901062 and batch: 500, loss is 5.045677127838135 and perplexity is 155.34945503301276
At time: 183.7087845802307 and batch: 550, loss is 5.0285984134674075 and perplexity is 152.71881398870832
At time: 184.8402738571167 and batch: 600, loss is 5.039642610549927 and perplexity is 154.41481893600556
At time: 185.97244334220886 and batch: 650, loss is 5.034675912857056 and perplexity is 153.64978862085377
At time: 187.10385727882385 and batch: 700, loss is 4.979536657333374 and perplexity is 145.40699277896488
At time: 188.23656725883484 and batch: 750, loss is 4.991526117324829 and perplexity is 147.1608369142875
At time: 189.37468600273132 and batch: 800, loss is 4.983968830108642 and perplexity is 146.0528920048306
At time: 190.51427364349365 and batch: 850, loss is 4.99544282913208 and perplexity is 147.73835374700417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.062230745951335 and perplexity of 157.94245312447805
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 193.61920714378357 and batch: 50, loss is 5.05076696395874 and perplexity is 156.14217399398063
At time: 194.77617979049683 and batch: 100, loss is 4.961975526809693 and perplexity is 142.87577219784455
At time: 195.90179896354675 and batch: 150, loss is 4.963552083969116 and perplexity is 143.10120167394578
At time: 197.03023672103882 and batch: 200, loss is 4.964686050415039 and perplexity is 143.26356567528165
At time: 198.15714049339294 and batch: 250, loss is 4.9762444496154785 and perplexity is 144.9290698974706
At time: 199.28815603256226 and batch: 300, loss is 4.941086978912353 and perplexity is 139.92225947466795
At time: 200.4297797679901 and batch: 350, loss is 4.8924175643920895 and perplexity is 133.2753867223982
At time: 201.55974292755127 and batch: 400, loss is 4.89791088104248 and perplexity is 134.0095252047972
At time: 202.6909053325653 and batch: 450, loss is 4.92583122253418 and perplexity is 137.80383970644135
At time: 203.82349371910095 and batch: 500, loss is 4.950771551132203 and perplexity is 141.28392965564137
At time: 204.9554364681244 and batch: 550, loss is 4.921886568069458 and perplexity is 137.26132190100725
At time: 206.08793210983276 and batch: 600, loss is 4.930518264770508 and perplexity is 138.45124815363334
At time: 207.21971154212952 and batch: 650, loss is 4.914978151321411 and perplexity is 136.316331438296
At time: 208.35422611236572 and batch: 700, loss is 4.855878887176513 and perplexity is 128.49357295583178
At time: 209.4834008216858 and batch: 750, loss is 4.854337959289551 and perplexity is 128.2957260989865
At time: 210.61693811416626 and batch: 800, loss is 4.84318829536438 and perplexity is 126.87321685651216
At time: 211.775235414505 and batch: 850, loss is 4.87561936378479 and perplexity is 131.05529898484377
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.996834754943848 and perplexity of 147.9441377597874
Finished 10 epochs...
Completing Train Step...
At time: 214.86156821250916 and batch: 50, loss is 4.987282247543335 and perplexity is 146.53762882782394
At time: 215.9944269657135 and batch: 100, loss is 4.910364103317261 and perplexity is 135.68881016123566
At time: 217.12808203697205 and batch: 150, loss is 4.914664640426635 and perplexity is 136.2736014817577
At time: 218.26173567771912 and batch: 200, loss is 4.923620376586914 and perplexity is 137.49951317940142
At time: 219.39418506622314 and batch: 250, loss is 4.939551696777344 and perplexity is 139.7076041498126
At time: 220.54266810417175 and batch: 300, loss is 4.909418649673462 and perplexity is 135.5605833070709
At time: 221.69062685966492 and batch: 350, loss is 4.860254220962524 and perplexity is 129.05700693388746
At time: 222.823388338089 and batch: 400, loss is 4.869432382583618 and perplexity is 130.24696546435547
At time: 223.9566411972046 and batch: 450, loss is 4.901739711761475 and perplexity is 134.5236085325219
At time: 225.0915229320526 and batch: 500, loss is 4.927521886825562 and perplexity is 138.03701679399435
At time: 226.2295801639557 and batch: 550, loss is 4.90372127532959 and perplexity is 134.79043989858675
At time: 227.39350008964539 and batch: 600, loss is 4.917683305740357 and perplexity is 136.68558738666076
At time: 228.53864455223083 and batch: 650, loss is 4.903222713470459 and perplexity is 134.72325527551476
At time: 229.67144298553467 and batch: 700, loss is 4.84873249053955 and perplexity is 127.57858026266967
At time: 230.80810022354126 and batch: 750, loss is 4.854461612701416 and perplexity is 128.31159128411832
At time: 231.94494485855103 and batch: 800, loss is 4.84602499961853 and perplexity is 127.23362960100609
At time: 233.08281922340393 and batch: 850, loss is 4.87672755241394 and perplexity is 131.20061348003273
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.993512471516927 and perplexity of 147.45344097104388
Finished 11 epochs...
Completing Train Step...
At time: 236.19222569465637 and batch: 50, loss is 4.966057424545288 and perplexity is 143.46016840018942
At time: 237.34662294387817 and batch: 100, loss is 4.891495122909546 and perplexity is 133.1525046615519
At time: 238.48077607154846 and batch: 150, loss is 4.895946588516235 and perplexity is 133.74654966099578
At time: 239.61582350730896 and batch: 200, loss is 4.9059779167175295 and perplexity is 135.0949570477338
At time: 240.7787902355194 and batch: 250, loss is 4.922206830978394 and perplexity is 137.30528865132752
At time: 241.91380071640015 and batch: 300, loss is 4.8935334014892575 and perplexity is 133.4241833439988
At time: 243.0436429977417 and batch: 350, loss is 4.8454134654998775 and perplexity is 127.15584568164945
At time: 244.1759693622589 and batch: 400, loss is 4.856697206497192 and perplexity is 128.5987647635388
At time: 245.31243801116943 and batch: 450, loss is 4.890869684219361 and perplexity is 133.0692519708823
At time: 246.45029854774475 and batch: 500, loss is 4.915368614196777 and perplexity is 136.369568297861
At time: 247.58890318870544 and batch: 550, loss is 4.893447732925415 and perplexity is 133.41275357542276
At time: 248.72911310195923 and batch: 600, loss is 4.909527168273926 and perplexity is 135.57529495007847
At time: 249.86555123329163 and batch: 650, loss is 4.895152883529663 and perplexity is 133.64043647444635
At time: 251.00322842597961 and batch: 700, loss is 4.842839632034302 and perplexity is 126.82898852907131
At time: 252.16185092926025 and batch: 750, loss is 4.850556812286377 and perplexity is 127.81153707054787
At time: 253.3182008266449 and batch: 800, loss is 4.8424043941497805 and perplexity is 126.77379975940083
At time: 254.4533622264862 and batch: 850, loss is 4.872131757736206 and perplexity is 130.59902584437324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.992515563964844 and perplexity of 147.30651676925046
Finished 12 epochs...
Completing Train Step...
At time: 257.60040521621704 and batch: 50, loss is 4.950499582290649 and perplexity is 141.24551005366698
At time: 258.74290895462036 and batch: 100, loss is 4.877645120620728 and perplexity is 131.32105423952865
At time: 259.88289046287537 and batch: 150, loss is 4.882877044677734 and perplexity is 132.00991648852897
At time: 261.01854491233826 and batch: 200, loss is 4.892922096252441 and perplexity is 133.34264536683807
At time: 262.1559612751007 and batch: 250, loss is 4.909904537200927 and perplexity is 135.6264665083324
At time: 263.2916896343231 and batch: 300, loss is 4.880940866470337 and perplexity is 131.7545690439126
At time: 264.42460441589355 and batch: 350, loss is 4.83334342956543 and perplexity is 125.63029529983089
At time: 265.55867075920105 and batch: 400, loss is 4.845826969146729 and perplexity is 127.20843595994386
At time: 266.6919376850128 and batch: 450, loss is 4.881464443206787 and perplexity is 131.8235707334484
At time: 267.8295404911041 and batch: 500, loss is 4.906053991317749 and perplexity is 135.10523473351333
At time: 268.9659216403961 and batch: 550, loss is 4.884344244003296 and perplexity is 132.2037435056323
At time: 270.15060806274414 and batch: 600, loss is 4.901798658370971 and perplexity is 134.53153847686153
At time: 271.28815841674805 and batch: 650, loss is 4.8868859767913815 and perplexity is 132.54019750214974
At time: 272.4265389442444 and batch: 700, loss is 4.836530246734619 and perplexity is 126.03129469837425
At time: 273.55582094192505 and batch: 750, loss is 4.845278606414795 and perplexity is 127.13869871687594
At time: 274.6938524246216 and batch: 800, loss is 4.837107315063476 and perplexity is 126.10404435573228
At time: 275.8289477825165 and batch: 850, loss is 4.865930290222168 and perplexity is 129.79162634670476
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.991221110026042 and perplexity of 147.11595862938313
Finished 13 epochs...
Completing Train Step...
At time: 278.90013670921326 and batch: 50, loss is 4.937524032592774 and perplexity is 139.42461104901204
At time: 280.07382893562317 and batch: 100, loss is 4.865949659347534 and perplexity is 129.79414032133369
At time: 281.22635865211487 and batch: 150, loss is 4.872114305496216 and perplexity is 130.5967466187206
At time: 282.3604736328125 and batch: 200, loss is 4.881634368896484 and perplexity is 131.84597284791707
At time: 283.4945299625397 and batch: 250, loss is 4.898619699478149 and perplexity is 134.10454729954543
At time: 284.62653636932373 and batch: 300, loss is 4.870067739486695 and perplexity is 130.32974506747948
At time: 285.75846099853516 and batch: 350, loss is 4.822733936309814 and perplexity is 124.30446714795761
At time: 286.8949315547943 and batch: 400, loss is 4.836283292770386 and perplexity is 126.00017461330334
At time: 288.02871918678284 and batch: 450, loss is 4.87324556350708 and perplexity is 130.74456883129236
At time: 289.1604378223419 and batch: 500, loss is 4.8966082668304445 and perplexity is 133.8350761373017
At time: 290.31541895866394 and batch: 550, loss is 4.874929161071777 and perplexity is 130.9648754707967
At time: 291.4778106212616 and batch: 600, loss is 4.894256982803345 and perplexity is 133.52076152678433
At time: 292.61860632896423 and batch: 650, loss is 4.87881986618042 and perplexity is 131.47541371369238
At time: 293.75231170654297 and batch: 700, loss is 4.829110631942749 and perplexity is 125.09965153066774
At time: 294.88574290275574 and batch: 750, loss is 4.840005388259888 and perplexity is 126.47003318168568
At time: 296.019823551178 and batch: 800, loss is 4.831354694366455 and perplexity is 125.38069818306084
At time: 297.1524655818939 and batch: 850, loss is 4.8599497413635255 and perplexity is 129.01771768985844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9913326899210615 and perplexity of 147.13237472843898
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 300.25058007240295 and batch: 50, loss is 4.931115999221801 and perplexity is 138.53402997278724
At time: 301.4281268119812 and batch: 100, loss is 4.861926069259644 and perplexity is 129.27295113383434
At time: 302.56525111198425 and batch: 150, loss is 4.863655967712402 and perplexity is 129.4967737513675
At time: 303.70518255233765 and batch: 200, loss is 4.869054899215699 and perplexity is 130.19780867968873
At time: 304.8479664325714 and batch: 250, loss is 4.882781505584717 and perplexity is 131.99730498329384
At time: 306.04166197776794 and batch: 300, loss is 4.847670297622681 and perplexity is 127.44313914340353
At time: 307.18274784088135 and batch: 350, loss is 4.801012954711914 and perplexity is 121.63356442929893
At time: 308.32689237594604 and batch: 400, loss is 4.809804286956787 and perplexity is 122.70759968928019
At time: 309.4659194946289 and batch: 450, loss is 4.848189868927002 and perplexity is 127.50937214632889
At time: 310.6391592025757 and batch: 500, loss is 4.8667659568786625 and perplexity is 129.90013421302325
At time: 311.8147656917572 and batch: 550, loss is 4.839673118591309 and perplexity is 126.42801800624595
At time: 312.9548122882843 and batch: 600, loss is 4.849864158630371 and perplexity is 127.7230385950508
At time: 314.10523867607117 and batch: 650, loss is 4.831221666336059 and perplexity is 125.36402014507978
At time: 315.2439000606537 and batch: 700, loss is 4.780708980560303 and perplexity is 119.18882265482395
At time: 316.4054591655731 and batch: 750, loss is 4.785784931182861 and perplexity is 119.79535730104446
At time: 317.54449486732483 and batch: 800, loss is 4.772365274429322 and perplexity is 118.19848343958485
At time: 318.6838569641113 and batch: 850, loss is 4.815279903411866 and perplexity is 123.38134233609748
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.972582817077637 and perplexity of 144.39936328677447
Finished 15 epochs...
Completing Train Step...
At time: 321.80286145210266 and batch: 50, loss is 4.912366275787353 and perplexity is 135.9607547105399
At time: 322.9853353500366 and batch: 100, loss is 4.843928413391113 and perplexity is 126.96715276896046
At time: 324.1226143836975 and batch: 150, loss is 4.84570390701294 and perplexity is 127.19278238157979
At time: 325.25888299942017 and batch: 200, loss is 4.854581136703491 and perplexity is 128.3269285155865
At time: 326.394896030426 and batch: 250, loss is 4.869092502593994 and perplexity is 130.2027046491937
At time: 327.5886001586914 and batch: 300, loss is 4.83634238243103 and perplexity is 126.00762014083708
At time: 328.72996068000793 and batch: 350, loss is 4.790396003723145 and perplexity is 120.34901788711785
At time: 329.88183403015137 and batch: 400, loss is 4.800205821990967 and perplexity is 121.5354296087226
At time: 331.0299611091614 and batch: 450, loss is 4.8402799797058105 and perplexity is 126.50476553934884
At time: 332.1777718067169 and batch: 500, loss is 4.860008459091187 and perplexity is 129.0252935394854
At time: 333.32152700424194 and batch: 550, loss is 4.8349371337890625 and perplexity is 125.83067246061883
At time: 334.4671540260315 and batch: 600, loss is 4.847509384155273 and perplexity is 127.42263347584993
At time: 335.61516523361206 and batch: 650, loss is 4.830165910720825 and perplexity is 125.23173621889939
At time: 336.75747299194336 and batch: 700, loss is 4.781465377807617 and perplexity is 119.27901085694458
At time: 337.9043490886688 and batch: 750, loss is 4.788126230239868 and perplexity is 120.07616265450808
At time: 339.04602813720703 and batch: 800, loss is 4.776289567947388 and perplexity is 118.66324030673178
At time: 340.18944454193115 and batch: 850, loss is 4.818203039169312 and perplexity is 123.74253039335689
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9714005788167315 and perplexity of 144.22874970746918
Finished 16 epochs...
Completing Train Step...
At time: 343.37237787246704 and batch: 50, loss is 4.9043614959716795 and perplexity is 134.8767631505815
At time: 344.5176260471344 and batch: 100, loss is 4.836148729324341 and perplexity is 125.98322073631681
At time: 345.6576683521271 and batch: 150, loss is 4.837512130737305 and perplexity is 126.15510358351123
At time: 346.79397797584534 and batch: 200, loss is 4.847707529067993 and perplexity is 127.44788412399959
At time: 347.9296417236328 and batch: 250, loss is 4.862288208007812 and perplexity is 129.31977435626985
At time: 349.0635802745819 and batch: 300, loss is 4.82994161605835 and perplexity is 125.20365055874257
At time: 350.1904327869415 and batch: 350, loss is 4.784576759338379 and perplexity is 119.65071131944961
At time: 351.33892726898193 and batch: 400, loss is 4.795550289154053 and perplexity is 120.97093246240617
At time: 352.4761142730713 and batch: 450, loss is 4.836203908920288 and perplexity is 125.99017263133271
At time: 353.621196269989 and batch: 500, loss is 4.85643141746521 and perplexity is 128.56458916428906
At time: 354.75509691238403 and batch: 550, loss is 4.832088670730591 and perplexity is 125.47275843293689
At time: 355.9064271450043 and batch: 600, loss is 4.845469236373901 and perplexity is 127.16293747205619
At time: 357.09054493904114 and batch: 650, loss is 4.829261817932129 and perplexity is 125.11856627504166
At time: 358.22537994384766 and batch: 700, loss is 4.781296377182007 and perplexity is 119.25885433276815
At time: 359.3599326610565 and batch: 750, loss is 4.788594169616699 and perplexity is 120.13236416765788
At time: 360.51565051078796 and batch: 800, loss is 4.776994485855102 and perplexity is 118.74691763917143
At time: 361.66301226615906 and batch: 850, loss is 4.818295536041259 and perplexity is 123.75397671971167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.970857620239258 and perplexity of 144.15046072645742
Finished 17 epochs...
Completing Train Step...
At time: 364.8167679309845 and batch: 50, loss is 4.89849440574646 and perplexity is 134.0877458929552
At time: 365.9829521179199 and batch: 100, loss is 4.8307998085021975 and perplexity is 125.31114550467194
At time: 367.1454129219055 and batch: 150, loss is 4.831723527908325 and perplexity is 125.42695131938355
At time: 368.3068130016327 and batch: 200, loss is 4.842798719406128 and perplexity is 126.82379972796639
At time: 369.4405515193939 and batch: 250, loss is 4.857157392501831 and perplexity is 128.65795773414052
At time: 370.57286739349365 and batch: 300, loss is 4.825149650573731 and perplexity is 124.60511421481284
At time: 371.7069802284241 and batch: 350, loss is 4.780155658721924 and perplexity is 119.12289111871989
At time: 372.84136414527893 and batch: 400, loss is 4.792345352172852 and perplexity is 120.5838488679017
At time: 373.9778859615326 and batch: 450, loss is 4.833316907882691 and perplexity is 125.62696341718032
At time: 375.1126797199249 and batch: 500, loss is 4.853666038513183 and perplexity is 128.20955048993468
At time: 376.2481529712677 and batch: 550, loss is 4.829505805969238 and perplexity is 125.1490974329002
At time: 377.3806245326996 and batch: 600, loss is 4.8435513877868654 and perplexity is 126.91929192441697
At time: 378.5352351665497 and batch: 650, loss is 4.827888164520264 and perplexity is 124.94681472015179
At time: 379.68677520751953 and batch: 700, loss is 4.780516233444214 and perplexity is 119.16585156686376
At time: 380.8247091770172 and batch: 750, loss is 4.78823489189148 and perplexity is 120.08921103757791
At time: 381.95780992507935 and batch: 800, loss is 4.7767417621612545 and perplexity is 118.7169112713324
At time: 383.0957000255585 and batch: 850, loss is 4.817599534988403 and perplexity is 123.66787378896068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9705454508463545 and perplexity of 144.10546838762662
Finished 18 epochs...
Completing Train Step...
At time: 386.21001863479614 and batch: 50, loss is 4.893517303466797 and perplexity is 133.42203549578662
At time: 387.3478043079376 and batch: 100, loss is 4.8263227653503415 and perplexity is 124.75137608974913
At time: 388.48427724838257 and batch: 150, loss is 4.8270050811767575 and perplexity is 124.83652497393452
At time: 389.62019205093384 and batch: 200, loss is 4.838582744598389 and perplexity is 126.29023931223851
At time: 390.7543647289276 and batch: 250, loss is 4.852871694564819 and perplexity is 128.1077484476516
At time: 391.88940382003784 and batch: 300, loss is 4.821220445632934 and perplexity is 124.11647579330125
At time: 393.02294301986694 and batch: 350, loss is 4.776370096206665 and perplexity is 118.67279643567788
At time: 394.1568195819855 and batch: 400, loss is 4.789309492111206 and perplexity is 120.21832829243587
At time: 395.2932515144348 and batch: 450, loss is 4.830702257156372 and perplexity is 125.29892183000871
At time: 396.4315483570099 and batch: 500, loss is 4.851101713180542 and perplexity is 127.88120066953724
At time: 397.56520199775696 and batch: 550, loss is 4.827069110870362 and perplexity is 124.84451847428716
At time: 398.7185890674591 and batch: 600, loss is 4.841619958877564 and perplexity is 126.67439291349211
At time: 399.8933274745941 and batch: 650, loss is 4.826442556381226 and perplexity is 124.7663210808152
At time: 401.03840351104736 and batch: 700, loss is 4.779273853302002 and perplexity is 119.01789420792082
At time: 402.1816575527191 and batch: 750, loss is 4.787170267105102 and perplexity is 119.96142911889898
At time: 403.3236720561981 and batch: 800, loss is 4.7759079742431645 and perplexity is 118.61796779970915
At time: 404.4630150794983 and batch: 850, loss is 4.816382789611817 and perplexity is 123.51749298142467
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.96996275583903 and perplexity of 144.02152331023296
Finished 19 epochs...
Completing Train Step...
At time: 407.670538187027 and batch: 50, loss is 4.889032707214356 and perplexity is 132.82503119758752
At time: 408.8522198200226 and batch: 100, loss is 4.822274360656738 and perplexity is 124.24735296643676
At time: 409.99042224884033 and batch: 150, loss is 4.8230571365356445 and perplexity is 124.34464887283306
At time: 411.1281313896179 and batch: 200, loss is 4.834758415222168 and perplexity is 125.8081861925819
At time: 412.26675724983215 and batch: 250, loss is 4.8491405010223385 and perplexity is 127.63064428140652
At time: 413.4134531021118 and batch: 300, loss is 4.817551717758179 and perplexity is 123.66196047514863
At time: 414.54551553726196 and batch: 350, loss is 4.773140373229981 and perplexity is 118.29013445704696
At time: 415.7153580188751 and batch: 400, loss is 4.786515121459961 and perplexity is 119.88286265007692
At time: 416.8488087654114 and batch: 450, loss is 4.82806694984436 and perplexity is 124.96915537394736
At time: 418.00042057037354 and batch: 500, loss is 4.848582601547241 and perplexity is 127.55945907089772
At time: 419.1369504928589 and batch: 550, loss is 4.82490982055664 and perplexity is 124.57523375140042
At time: 420.27301955223083 and batch: 600, loss is 4.839677124023438 and perplexity is 126.42852440610547
At time: 421.4072778224945 and batch: 650, loss is 4.824801721572876 and perplexity is 124.56176802305858
At time: 422.5470428466797 and batch: 700, loss is 4.777743968963623 and perplexity is 118.83594980801664
At time: 423.6807668209076 and batch: 750, loss is 4.786012763977051 and perplexity is 119.82265372142112
At time: 424.8170499801636 and batch: 800, loss is 4.77470211982727 and perplexity is 118.4750180050758
At time: 425.9536099433899 and batch: 850, loss is 4.814933223724365 and perplexity is 123.33857594444099
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.969850858052571 and perplexity of 144.00540852219348
Finished 20 epochs...
Completing Train Step...
At time: 429.0805835723877 and batch: 50, loss is 4.885177249908447 and perplexity is 132.31391588540635
At time: 430.2438905239105 and batch: 100, loss is 4.818702125549317 and perplexity is 123.80430401880886
At time: 431.37563848495483 and batch: 150, loss is 4.8195341682434085 and perplexity is 123.907357351906
At time: 432.5123620033264 and batch: 200, loss is 4.831344337463379 and perplexity is 125.37939963404659
At time: 433.647008895874 and batch: 250, loss is 4.845607280731201 and perplexity is 127.18049280971165
At time: 434.7799696922302 and batch: 300, loss is 4.814148969650269 and perplexity is 123.24188508381985
At time: 435.91143012046814 and batch: 350, loss is 4.7697859954833985 and perplexity is 117.89400941033303
At time: 437.06348419189453 and batch: 400, loss is 4.78388201713562 and perplexity is 119.56761378972865
At time: 438.19896602630615 and batch: 450, loss is 4.825663452148437 and perplexity is 124.66915296891047
At time: 439.35515117645264 and batch: 500, loss is 4.846274967193604 and perplexity is 127.26543785821609
At time: 440.4875280857086 and batch: 550, loss is 4.8224396324157714 and perplexity is 124.26788924200318
At time: 441.6226336956024 and batch: 600, loss is 4.837758045196534 and perplexity is 126.18613076244753
At time: 442.7570323944092 and batch: 650, loss is 4.822997589111328 and perplexity is 124.33724468971732
At time: 443.94245982170105 and batch: 700, loss is 4.776086540222168 and perplexity is 118.63915082448418
At time: 445.07539224624634 and batch: 750, loss is 4.784515314102173 and perplexity is 119.64335957909732
At time: 446.20870757102966 and batch: 800, loss is 4.773387269973755 and perplexity is 118.31934351172688
At time: 447.3409125804901 and batch: 850, loss is 4.813531332015991 and perplexity is 123.1657897595598
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.969649314880371 and perplexity of 143.9763881398741
Finished 21 epochs...
Completing Train Step...
At time: 450.4420483112335 and batch: 50, loss is 4.8816798973083495 and perplexity is 131.85197572232133
At time: 451.61812472343445 and batch: 100, loss is 4.815271997451783 and perplexity is 123.38036689198589
At time: 452.75528168678284 and batch: 150, loss is 4.8161443328857425 and perplexity is 123.48804291585807
At time: 453.88849806785583 and batch: 200, loss is 4.827882165908814 and perplexity is 124.94606521500634
At time: 455.0258913040161 and batch: 250, loss is 4.842384710311889 and perplexity is 126.77130438903683
At time: 456.163738489151 and batch: 300, loss is 4.811088418960571 and perplexity is 122.8652736605781
At time: 457.2958142757416 and batch: 350, loss is 4.766816539764404 and perplexity is 117.54444763082691
At time: 458.43241119384766 and batch: 400, loss is 4.781379203796387 and perplexity is 119.26873254899138
At time: 459.56854224205017 and batch: 450, loss is 4.82342493057251 and perplexity is 124.39039050443866
At time: 460.70157527923584 and batch: 500, loss is 4.84404881477356 and perplexity is 126.98244071000526
At time: 461.8417308330536 and batch: 550, loss is 4.820399265289307 and perplexity is 124.01459561978258
At time: 462.9781758785248 and batch: 600, loss is 4.835652017593384 and perplexity is 125.92065893155079
At time: 464.1338074207306 and batch: 650, loss is 4.820936222076416 and perplexity is 124.08120397990626
At time: 465.2693884372711 and batch: 700, loss is 4.77453616142273 and perplexity is 118.45535771155036
At time: 466.4171726703644 and batch: 750, loss is 4.78309458732605 and perplexity is 119.47349974533641
At time: 467.56493639945984 and batch: 800, loss is 4.771873989105225 and perplexity is 118.14042852127281
At time: 468.7110574245453 and batch: 850, loss is 4.812028198242188 and perplexity is 122.98079417258421
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.96946652730306 and perplexity of 143.95007344976847
Finished 22 epochs...
Completing Train Step...
At time: 471.8627758026123 and batch: 50, loss is 4.878149442672729 and perplexity is 131.38729904602803
At time: 472.99973726272583 and batch: 100, loss is 4.812047805786133 and perplexity is 122.98320554755082
At time: 474.1629250049591 and batch: 150, loss is 4.812960033416748 and perplexity is 123.09544541212234
At time: 475.30036759376526 and batch: 200, loss is 4.824997444152832 and perplexity is 124.58614995962981
At time: 476.43673491477966 and batch: 250, loss is 4.839235706329346 and perplexity is 126.37272893385499
At time: 477.57080364227295 and batch: 300, loss is 4.807881422042847 and perplexity is 122.47187625595404
At time: 478.70127868652344 and batch: 350, loss is 4.7637800693511965 and perplexity is 117.18806873416914
At time: 479.8394343852997 and batch: 400, loss is 4.778788375854492 and perplexity is 118.96012772772896
At time: 481.00296545028687 and batch: 450, loss is 4.821124343872071 and perplexity is 124.1045485545488
At time: 482.1674120426178 and batch: 500, loss is 4.841608715057373 and perplexity is 126.67296861740274
At time: 483.3278169631958 and batch: 550, loss is 4.818199615478516 and perplexity is 123.74210673791977
At time: 484.47548151016235 and batch: 600, loss is 4.833679122924805 and perplexity is 125.67247563513247
At time: 485.62375354766846 and batch: 650, loss is 4.819028148651123 and perplexity is 123.84467366238265
At time: 486.76538038253784 and batch: 700, loss is 4.772895336151123 and perplexity is 118.26115253900919
At time: 487.91635036468506 and batch: 750, loss is 4.781588754653931 and perplexity is 119.29372803299623
At time: 489.0645980834961 and batch: 800, loss is 4.770278558731079 and perplexity is 117.95209397047573
At time: 490.20124769210815 and batch: 850, loss is 4.810396604537964 and perplexity is 122.78030308754504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.969340006510417 and perplexity of 143.93186192446723
Finished 23 epochs...
Completing Train Step...
At time: 493.3456780910492 and batch: 50, loss is 4.874852924346924 and perplexity is 130.95489151819677
At time: 494.4839041233063 and batch: 100, loss is 4.80901629447937 and perplexity is 122.61094511034509
At time: 495.6182403564453 and batch: 150, loss is 4.809996147155761 and perplexity is 122.73114465237067
At time: 496.75082778930664 and batch: 200, loss is 4.822000770568848 and perplexity is 124.2133647718471
At time: 497.8805491924286 and batch: 250, loss is 4.8363403224945065 and perplexity is 126.00736057340545
At time: 499.01248145103455 and batch: 300, loss is 4.804852876663208 and perplexity is 122.10152571576596
At time: 500.1444365978241 and batch: 350, loss is 4.760919761657715 and perplexity is 116.8533537217491
At time: 501.27142333984375 and batch: 400, loss is 4.7763573169708256 and perplexity is 118.67127989771464
At time: 502.45337319374084 and batch: 450, loss is 4.8190320777893065 and perplexity is 123.84516026617474
At time: 503.5857410430908 and batch: 500, loss is 4.839459447860718 and perplexity is 126.40100692511292
At time: 504.7195963859558 and batch: 550, loss is 4.815880994796753 and perplexity is 123.45552809200728
At time: 505.84808444976807 and batch: 600, loss is 4.831598596572876 and perplexity is 125.41128254163495
At time: 506.97977018356323 and batch: 650, loss is 4.8169301414489745 and perplexity is 123.5851190140574
At time: 508.11294293403625 and batch: 700, loss is 4.771174192428589 and perplexity is 118.0577831628636
At time: 509.24582624435425 and batch: 750, loss is 4.779824066162109 and perplexity is 119.0833974025944
At time: 510.3781383037567 and batch: 800, loss is 4.768862085342407 and perplexity is 117.78513624173358
At time: 511.51239466667175 and batch: 850, loss is 4.808718948364258 and perplexity is 122.57449264191479
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.969079653422038 and perplexity of 143.89439369739503
Finished 24 epochs...
Completing Train Step...
At time: 514.6714859008789 and batch: 50, loss is 4.871798162460327 and perplexity is 130.55546589241828
At time: 515.8055381774902 and batch: 100, loss is 4.8062393093109135 and perplexity is 122.27092866310586
At time: 516.9401016235352 and batch: 150, loss is 4.807265634536743 and perplexity is 122.39648282025682
At time: 518.0735538005829 and batch: 200, loss is 4.819385719299317 and perplexity is 123.88896480075306
At time: 519.2109179496765 and batch: 250, loss is 4.833312005996704 and perplexity is 125.62634760963807
At time: 520.3461229801178 and batch: 300, loss is 4.801996641159057 and perplexity is 121.75327258613865
At time: 521.4825234413147 and batch: 350, loss is 4.758312816619873 and perplexity is 116.54912018346718
At time: 522.6322820186615 and batch: 400, loss is 4.773968706130981 and perplexity is 118.38815866001288
At time: 523.7929329872131 and batch: 450, loss is 4.8170093822479245 and perplexity is 123.59491238563858
At time: 524.9263205528259 and batch: 500, loss is 4.8372892570495605 and perplexity is 126.12699006334613
At time: 526.0631010532379 and batch: 550, loss is 4.8135202121734615 and perplexity is 123.16442018298741
At time: 527.1987066268921 and batch: 600, loss is 4.829521312713623 and perplexity is 125.15103810301076
At time: 528.3402993679047 and batch: 650, loss is 4.81509295463562 and perplexity is 123.35827850108127
At time: 529.4896397590637 and batch: 700, loss is 4.7694783210754395 and perplexity is 117.85774202034575
At time: 530.6220653057098 and batch: 750, loss is 4.777984676361084 and perplexity is 118.86455794317641
At time: 531.7980268001556 and batch: 800, loss is 4.76745903968811 and perplexity is 117.61999419619418
At time: 532.933278799057 and batch: 850, loss is 4.807084102630615 and perplexity is 122.37426597002164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9691057205200195 and perplexity of 143.8981446555426
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 536.0420117378235 and batch: 50, loss is 4.8709158515930175 and perplexity is 130.44032618805036
At time: 537.2019701004028 and batch: 100, loss is 4.806446218490601 and perplexity is 122.29623025813105
At time: 538.3385772705078 and batch: 150, loss is 4.805524940490723 and perplexity is 122.18361331544737
At time: 539.4739520549774 and batch: 200, loss is 4.817744674682618 and perplexity is 123.68582420897425
At time: 540.6106491088867 and batch: 250, loss is 4.828341341018676 and perplexity is 125.00345051217018
At time: 541.7474100589752 and batch: 300, loss is 4.795483102798462 and perplexity is 120.96280513934724
At time: 542.8808069229126 and batch: 350, loss is 4.750889129638672 and perplexity is 115.68709964153484
At time: 544.012476682663 and batch: 400, loss is 4.765836896896363 and perplexity is 117.42935243632546
At time: 545.1486613750458 and batch: 450, loss is 4.806812839508057 and perplexity is 122.34107484648129
At time: 546.2835459709167 and batch: 500, loss is 4.82613133430481 and perplexity is 124.7274970890567
At time: 547.4170799255371 and batch: 550, loss is 4.8003231811523435 and perplexity is 121.54969374181599
At time: 548.5487356185913 and batch: 600, loss is 4.81284532546997 and perplexity is 123.08132619613006
At time: 549.701996088028 and batch: 650, loss is 4.797335824966431 and perplexity is 121.18712334541927
At time: 550.8652241230011 and batch: 700, loss is 4.751723022460937 and perplexity is 115.78361051782547
At time: 552.0418000221252 and batch: 750, loss is 4.757397603988648 and perplexity is 116.44250175322618
At time: 553.1849608421326 and batch: 800, loss is 4.745541954040528 and perplexity is 115.07015134075733
At time: 554.327232837677 and batch: 850, loss is 4.790086660385132 and perplexity is 120.31179447790176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.96513557434082 and perplexity of 143.32798055267503
Finished 26 epochs...
Completing Train Step...
At time: 557.4899854660034 and batch: 50, loss is 4.8659342098236085 and perplexity is 129.79213507914739
At time: 558.687774181366 and batch: 100, loss is 4.80160493850708 and perplexity is 121.70559084552156
At time: 559.8176443576813 and batch: 150, loss is 4.800587205886841 and perplexity is 121.58179010436479
At time: 560.9946775436401 and batch: 200, loss is 4.813488912582398 and perplexity is 123.16056524733133
At time: 562.1212973594666 and batch: 250, loss is 4.824978437423706 and perplexity is 124.5837820069283
At time: 563.2537529468536 and batch: 300, loss is 4.7923515510559085 and perplexity is 120.58459635539614
At time: 564.3801460266113 and batch: 350, loss is 4.748587770462036 and perplexity is 115.42116819241681
At time: 565.5185296535492 and batch: 400, loss is 4.763187685012817 and perplexity is 117.11866891531848
At time: 566.647791147232 and batch: 450, loss is 4.804766254425049 and perplexity is 122.09094946640161
At time: 567.7823755741119 and batch: 500, loss is 4.824321680068969 and perplexity is 124.50198755430857
At time: 568.90958070755 and batch: 550, loss is 4.799348201751709 and perplexity is 121.43124304714104
At time: 570.041152715683 and batch: 600, loss is 4.812540073394775 and perplexity is 123.04376109959443
At time: 571.1779682636261 and batch: 650, loss is 4.797538375854492 and perplexity is 121.21167239100602
At time: 572.3158214092255 and batch: 700, loss is 4.752441453933716 and perplexity is 115.86682299530563
At time: 573.4498207569122 and batch: 750, loss is 4.758658466339111 and perplexity is 116.58941231721721
At time: 574.5869550704956 and batch: 800, loss is 4.746811561584472 and perplexity is 115.21633805322233
At time: 575.7240417003632 and batch: 850, loss is 4.791386852264404 and perplexity is 120.46832463363008
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9650163650512695 and perplexity of 143.3108955443067
Finished 27 epochs...
Completing Train Step...
At time: 578.8478891849518 and batch: 50, loss is 4.863575344085693 and perplexity is 129.4863336726847
At time: 579.9823751449585 and batch: 100, loss is 4.799166212081909 and perplexity is 121.40914582610947
At time: 581.1152334213257 and batch: 150, loss is 4.7979755878448485 and perplexity is 121.26467917430278
At time: 582.2504801750183 and batch: 200, loss is 4.811257266998291 and perplexity is 122.88602097246184
At time: 583.3857305049896 and batch: 250, loss is 4.822857055664063 and perplexity is 124.31977237584864
At time: 584.5204193592072 and batch: 300, loss is 4.790516271591186 and perplexity is 120.36349287734068
At time: 585.6528580188751 and batch: 350, loss is 4.7472773361206055 and perplexity is 115.2700153894318
At time: 586.785099029541 and batch: 400, loss is 4.761652364730835 and perplexity is 116.93899221347087
At time: 587.9148123264313 and batch: 450, loss is 4.803450727462769 and perplexity is 121.93044113019829
At time: 589.0404832363129 and batch: 500, loss is 4.82329249382019 and perplexity is 124.37391773592455
At time: 590.227127790451 and batch: 550, loss is 4.7986789989471434 and perplexity is 121.35000810308986
At time: 591.3680255413055 and batch: 600, loss is 4.812330808639526 and perplexity is 123.01801507100518
At time: 592.500738620758 and batch: 650, loss is 4.797669115066529 and perplexity is 121.22752054550914
At time: 593.6355113983154 and batch: 700, loss is 4.752861261367798 and perplexity is 115.91547496047635
At time: 594.7680039405823 and batch: 750, loss is 4.759240789413452 and perplexity is 116.65732479381539
At time: 595.9015562534332 and batch: 800, loss is 4.747411870956421 and perplexity is 115.28552426524533
At time: 597.0317430496216 and batch: 850, loss is 4.791939277648925 and perplexity is 120.53489277946233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.96493403116862 and perplexity of 143.29909668758006
Finished 28 epochs...
Completing Train Step...
At time: 600.1722209453583 and batch: 50, loss is 4.861770219802857 and perplexity is 129.25280558450012
At time: 601.3096635341644 and batch: 100, loss is 4.7973548412323 and perplexity is 121.18942789388865
At time: 602.4491097927094 and batch: 150, loss is 4.796003475189209 and perplexity is 121.02576722389347
At time: 603.5867133140564 and batch: 200, loss is 4.809623985290528 and perplexity is 122.68547729900537
At time: 604.7255103588104 and batch: 250, loss is 4.8212610912323 and perplexity is 124.1215206843766
At time: 605.8627543449402 and batch: 300, loss is 4.789139404296875 and perplexity is 120.1978823585854
At time: 606.9990270137787 and batch: 350, loss is 4.746297340393067 and perplexity is 115.15710660093839
At time: 608.1366577148438 and batch: 400, loss is 4.760533924102783 and perplexity is 116.8082760063605
At time: 609.2772402763367 and batch: 450, loss is 4.8024499893188475 and perplexity is 121.80848172174845
At time: 610.4181225299835 and batch: 500, loss is 4.822497568130493 and perplexity is 124.2750889995429
At time: 611.5558316707611 and batch: 550, loss is 4.79809799194336 and perplexity is 121.27952337651544
At time: 612.6928794384003 and batch: 600, loss is 4.812102041244507 and perplexity is 122.98987577895592
At time: 613.8315682411194 and batch: 650, loss is 4.797707738876343 and perplexity is 121.23220290463166
At time: 614.9694204330444 and batch: 700, loss is 4.7531679916381835 and perplexity is 115.95103519887482
At time: 616.1203939914703 and batch: 750, loss is 4.7595784378051755 and perplexity is 116.69672060250622
At time: 617.2617003917694 and batch: 800, loss is 4.747717676162719 and perplexity is 115.32078456989403
At time: 618.4009304046631 and batch: 850, loss is 4.792210931777954 and perplexity is 120.56764102867508
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.964939117431641 and perplexity of 143.29982554633
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 621.5332639217377 and batch: 50, loss is 4.861113605499267 and perplexity is 129.16796420065555
At time: 622.6646671295166 and batch: 100, loss is 4.797053785324096 and perplexity is 121.15294859203956
At time: 623.7971985340118 and batch: 150, loss is 4.794996948242187 and perplexity is 120.90401281272564
At time: 624.9533779621124 and batch: 200, loss is 4.809067468643189 and perplexity is 122.6172197834853
At time: 626.0858037471771 and batch: 250, loss is 4.819331274032593 and perplexity is 123.88221981663828
At time: 627.2239148616791 and batch: 300, loss is 4.786768455505371 and perplexity is 119.91323690790173
At time: 628.3776650428772 and batch: 350, loss is 4.744193954467773 and perplexity is 114.91514132610241
At time: 629.5252618789673 and batch: 400, loss is 4.756942911148071 and perplexity is 116.38956821650225
At time: 630.6823954582214 and batch: 450, loss is 4.798508138656616 and perplexity is 121.32927597664886
At time: 631.82590675354 and batch: 500, loss is 4.818883247375489 and perplexity is 123.82672971127056
At time: 632.9648687839508 and batch: 550, loss is 4.794065332412719 and perplexity is 120.79142917103717
At time: 634.0979874134064 and batch: 600, loss is 4.806943655014038 and perplexity is 122.35708000292813
At time: 635.2289175987244 and batch: 650, loss is 4.791844825744629 and perplexity is 120.52350856694392
At time: 636.3590886592865 and batch: 700, loss is 4.7473131370544435 and perplexity is 115.27414223749832
At time: 637.4903931617737 and batch: 750, loss is 4.753270177841187 and perplexity is 115.96288440029824
At time: 638.6215498447418 and batch: 800, loss is 4.740861797332764 and perplexity is 114.53286327698783
At time: 639.7526395320892 and batch: 850, loss is 4.786660833358765 and perplexity is 119.90033228236342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.965546290079753 and perplexity of 143.38685970058614
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 642.7983660697937 and batch: 50, loss is 4.8603167057037355 and perplexity is 129.0650712795139
At time: 643.9560720920563 and batch: 100, loss is 4.795801343917847 and perplexity is 121.00130660390838
At time: 645.0849022865295 and batch: 150, loss is 4.794258470535278 and perplexity is 120.81476085393467
At time: 646.2182879447937 and batch: 200, loss is 4.808092889785766 and perplexity is 122.49777784577583
At time: 647.3482708930969 and batch: 250, loss is 4.818336668014527 and perplexity is 123.7590670696612
At time: 648.5234341621399 and batch: 300, loss is 4.785550746917725 and perplexity is 119.76730639799644
At time: 649.661922454834 and batch: 350, loss is 4.743428058624268 and perplexity is 114.82716199280844
At time: 650.7957425117493 and batch: 400, loss is 4.755652952194214 and perplexity is 116.2395272449987
At time: 651.9237215518951 and batch: 450, loss is 4.79758955001831 and perplexity is 121.21787545570264
At time: 653.0503535270691 and batch: 500, loss is 4.81806435585022 and perplexity is 123.72537055844806
At time: 654.1796367168427 and batch: 550, loss is 4.793071260452271 and perplexity is 120.67141346024526
At time: 655.3086709976196 and batch: 600, loss is 4.805477418899536 and perplexity is 122.17780709368724
At time: 656.4472026824951 and batch: 650, loss is 4.790161619186401 and perplexity is 120.32081324380808
At time: 657.5827388763428 and batch: 700, loss is 4.745538101196289 and perplexity is 115.06970799424182
At time: 658.7144598960876 and batch: 750, loss is 4.751470422744751 and perplexity is 115.75436730423883
At time: 659.8468639850616 and batch: 800, loss is 4.73901743888855 and perplexity is 114.32181830457662
At time: 660.9775457382202 and batch: 850, loss is 4.785022172927857 and perplexity is 119.704017242972
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.965008735656738 and perplexity of 143.30980217311486
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 664.0465083122253 and batch: 50, loss is 4.860041484832764 and perplexity is 129.02955476585146
At time: 665.2100031375885 and batch: 100, loss is 4.795413093566895 and perplexity is 120.9543369227413
At time: 666.3439252376556 and batch: 150, loss is 4.79402153968811 and perplexity is 120.78613950106968
At time: 667.4780683517456 and batch: 200, loss is 4.807595958709717 and perplexity is 122.43692001554318
At time: 668.6307027339935 and batch: 250, loss is 4.817993965148926 and perplexity is 123.71666174935972
At time: 669.76549243927 and batch: 300, loss is 4.785197229385376 and perplexity is 119.72497403843904
At time: 670.9045207500458 and batch: 350, loss is 4.743212547302246 and perplexity is 114.80241810571303
At time: 672.0388028621674 and batch: 400, loss is 4.7552786064147945 and perplexity is 116.19602161215592
At time: 673.1726403236389 and batch: 450, loss is 4.797400302886963 and perplexity is 121.19493749104512
At time: 674.3267812728882 and batch: 500, loss is 4.818057289123535 and perplexity is 123.7244962281597
At time: 675.4598987102509 and batch: 550, loss is 4.792843551635742 and perplexity is 120.64393864374506
At time: 676.6456701755524 and batch: 600, loss is 4.8049791431427 and perplexity is 122.11694401894559
At time: 677.7793536186218 and batch: 650, loss is 4.789560461044312 and perplexity is 120.24850314434397
At time: 678.9130868911743 and batch: 700, loss is 4.744983530044555 and perplexity is 115.00591134527535
At time: 680.049268245697 and batch: 750, loss is 4.750941410064697 and perplexity is 115.6931479704929
At time: 681.1851184368134 and batch: 800, loss is 4.738538675308227 and perplexity is 114.2670982815584
At time: 682.3186361789703 and batch: 850, loss is 4.7845237445831295 and perplexity is 119.64436823441359
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.964846611022949 and perplexity of 143.28657000722322
Finished 32 epochs...
Completing Train Step...
At time: 685.3993530273438 and batch: 50, loss is 4.859875869750977 and perplexity is 129.00818729502248
At time: 686.5581846237183 and batch: 100, loss is 4.795210866928101 and perplexity is 120.9298792068221
At time: 687.6874737739563 and batch: 150, loss is 4.793784198760986 and perplexity is 120.75747540845248
At time: 688.8191409111023 and batch: 200, loss is 4.807493715286255 and perplexity is 122.42440228562134
At time: 689.9707963466644 and batch: 250, loss is 4.817866077423096 and perplexity is 123.70084091850795
At time: 691.1010205745697 and batch: 300, loss is 4.7851149940490725 and perplexity is 119.71512881975308
At time: 692.2318367958069 and batch: 350, loss is 4.74311731338501 and perplexity is 114.79148554231237
At time: 693.3594560623169 and batch: 400, loss is 4.755222244262695 and perplexity is 116.18947273886852
At time: 694.4904463291168 and batch: 450, loss is 4.797402963638306 and perplexity is 121.1952599610668
At time: 695.6202211380005 and batch: 500, loss is 4.818056840896606 and perplexity is 123.72444077152109
At time: 696.753232717514 and batch: 550, loss is 4.792847204208374 and perplexity is 120.64437930529833
At time: 697.8915481567383 and batch: 600, loss is 4.804973669052124 and perplexity is 122.11627554156289
At time: 699.0283744335175 and batch: 650, loss is 4.789549016952515 and perplexity is 120.24712701730984
At time: 700.1666815280914 and batch: 700, loss is 4.744983377456665 and perplexity is 115.00589379676735
At time: 701.3036274909973 and batch: 750, loss is 4.751001262664795 and perplexity is 115.70007271344237
At time: 702.4411325454712 and batch: 800, loss is 4.738646726608277 and perplexity is 114.2794456571436
At time: 703.576308965683 and batch: 850, loss is 4.784585857391358 and perplexity is 119.65179991291215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.964785893758138 and perplexity of 143.27787030272196
Finished 33 epochs...
Completing Train Step...
At time: 706.6947162151337 and batch: 50, loss is 4.859748334884643 and perplexity is 128.99173530222353
At time: 707.8366069793701 and batch: 100, loss is 4.79508111000061 and perplexity is 120.9141887352501
At time: 708.9720242023468 and batch: 150, loss is 4.793647146224975 and perplexity is 120.74092642427152
At time: 710.1071875095367 and batch: 200, loss is 4.807379951477051 and perplexity is 122.41047561146857
At time: 711.2354655265808 and batch: 250, loss is 4.817770462036133 and perplexity is 123.68901378017314
At time: 712.3664638996124 and batch: 300, loss is 4.785038785934448 and perplexity is 119.70600590311828
At time: 713.4928922653198 and batch: 350, loss is 4.7430402946472165 and perplexity is 114.7826447874427
At time: 714.6202504634857 and batch: 400, loss is 4.755178022384643 and perplexity is 116.18433473578105
At time: 715.7558159828186 and batch: 450, loss is 4.797420711517334 and perplexity is 121.19741093896691
At time: 716.8923804759979 and batch: 500, loss is 4.818088655471802 and perplexity is 123.72837707466113
At time: 718.0274577140808 and batch: 550, loss is 4.792863645553589 and perplexity is 120.646362877493
At time: 719.163102388382 and batch: 600, loss is 4.804966783523559 and perplexity is 122.11543470935418
At time: 720.2982406616211 and batch: 650, loss is 4.789537811279297 and perplexity is 120.24577957484861
At time: 721.4366464614868 and batch: 700, loss is 4.744985466003418 and perplexity is 115.00613399220418
At time: 722.5739195346832 and batch: 750, loss is 4.751055355072022 and perplexity is 115.7063313781633
At time: 723.7116279602051 and batch: 800, loss is 4.7387325477600095 and perplexity is 114.2892536716508
At time: 724.8500783443451 and batch: 850, loss is 4.784636650085449 and perplexity is 119.65787750452996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.964768091837565 and perplexity of 143.2753197041579
Finished 34 epochs...
Completing Train Step...
At time: 728.053867816925 and batch: 50, loss is 4.859631662368774 and perplexity is 128.97668638985382
At time: 729.1917059421539 and batch: 100, loss is 4.794967603683472 and perplexity is 120.90046498987762
At time: 730.3274416923523 and batch: 150, loss is 4.7935285568237305 and perplexity is 120.72660867908432
At time: 731.4603762626648 and batch: 200, loss is 4.807273588180542 and perplexity is 122.39745632215482
At time: 732.592787027359 and batch: 250, loss is 4.817680082321167 and perplexity is 123.67783530752442
At time: 733.7284483909607 and batch: 300, loss is 4.7849650573730464 and perplexity is 119.69718047685888
At time: 734.9131412506104 and batch: 350, loss is 4.74297022819519 and perplexity is 114.77460265651335
At time: 736.0474736690521 and batch: 400, loss is 4.755138731002807 and perplexity is 116.17976978240382
At time: 737.1717958450317 and batch: 450, loss is 4.797439880371094 and perplexity is 121.19973417668017
At time: 738.3065540790558 and batch: 500, loss is 4.8181235885620115 and perplexity is 123.73269936471402
At time: 739.4613263607025 and batch: 550, loss is 4.792882194519043 and perplexity is 120.64860076346537
At time: 740.6113202571869 and batch: 600, loss is 4.804960737228393 and perplexity is 122.11469636562373
At time: 741.7453351020813 and batch: 650, loss is 4.789527769088745 and perplexity is 120.24457204988015
At time: 742.8811492919922 and batch: 700, loss is 4.744989433288574 and perplexity is 115.00659025523753
At time: 744.0253648757935 and batch: 750, loss is 4.751104135513305 and perplexity is 115.7119757217329
At time: 745.166659116745 and batch: 800, loss is 4.738809309005737 and perplexity is 114.29802699385719
At time: 746.301864862442 and batch: 850, loss is 4.784682025909424 and perplexity is 119.66330720250437
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.964759508768718 and perplexity of 143.2740899675022
Finished 35 epochs...
Completing Train Step...
At time: 749.4183740615845 and batch: 50, loss is 4.859522151947021 and perplexity is 128.9625628718813
At time: 750.5529267787933 and batch: 100, loss is 4.794862194061279 and perplexity is 120.88772158919045
At time: 751.7051250934601 and batch: 150, loss is 4.793418073654175 and perplexity is 120.7132711575072
At time: 752.843727350235 and batch: 200, loss is 4.807172966003418 and perplexity is 122.38514104323076
At time: 754.000706911087 and batch: 250, loss is 4.8175927257537845 and perplexity is 123.6670317082602
At time: 755.1377985477448 and batch: 300, loss is 4.784893159866333 and perplexity is 119.68857485738695
At time: 756.2756023406982 and batch: 350, loss is 4.742904605865479 and perplexity is 114.76707112681628
At time: 757.4125661849976 and batch: 400, loss is 4.755102634429932 and perplexity is 116.17557616656524
At time: 758.5645308494568 and batch: 450, loss is 4.79745922088623 and perplexity is 121.20207826464136
At time: 759.7010941505432 and batch: 500, loss is 4.818159875869751 and perplexity is 123.73718937271792
At time: 760.8382496833801 and batch: 550, loss is 4.792901458740235 and perplexity is 120.65092498718404
At time: 761.9946434497833 and batch: 600, loss is 4.804955177307129 and perplexity is 122.11401741941413
At time: 763.1308634281158 and batch: 650, loss is 4.789518690109253 and perplexity is 120.24348035683215
At time: 764.3057582378387 and batch: 700, loss is 4.744994354248047 and perplexity is 115.00715619939976
At time: 765.4421956539154 and batch: 750, loss is 4.75114896774292 and perplexity is 115.71716346388581
At time: 766.5920822620392 and batch: 800, loss is 4.738880262374878 and perplexity is 114.30613711167527
At time: 767.7315919399261 and batch: 850, loss is 4.7847231578826905 and perplexity is 119.66822929168413
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.964754422505696 and perplexity of 143.27336123964972
Finished 36 epochs...
Completing Train Step...
At time: 770.8284015655518 and batch: 50, loss is 4.859418087005615 and perplexity is 128.94914308860902
At time: 771.9879379272461 and batch: 100, loss is 4.794762287139893 and perplexity is 120.87564467238688
At time: 773.1262707710266 and batch: 150, loss is 4.793312425613403 and perplexity is 120.70051871056164
At time: 774.2701759338379 and batch: 200, loss is 4.807076778411865 and perplexity is 122.37336967741066
At time: 775.425858259201 and batch: 250, loss is 4.817508153915405 and perplexity is 123.65657340228701
At time: 776.5697989463806 and batch: 300, loss is 4.7848231315612795 and perplexity is 119.68019356282278
At time: 777.7160651683807 and batch: 350, loss is 4.742842273712158 and perplexity is 114.75991767109011
At time: 778.8600599765778 and batch: 400, loss is 4.7550688648223876 and perplexity is 116.17165302919366
At time: 779.9936292171478 and batch: 450, loss is 4.797478170394897 and perplexity is 121.2043750062349
At time: 781.1519522666931 and batch: 500, loss is 4.818196887969971 and perplexity is 123.7417692307265
At time: 782.2855212688446 and batch: 550, loss is 4.792920656204224 and perplexity is 120.65324120120438
At time: 783.4177448749542 and batch: 600, loss is 4.804949731826782 and perplexity is 122.11335245174276
At time: 784.5513308048248 and batch: 650, loss is 4.78951024055481 and perplexity is 120.24246435729086
At time: 785.6920635700226 and batch: 700, loss is 4.745000009536743 and perplexity is 115.00780659990927
At time: 786.8286077976227 and batch: 750, loss is 4.751190805435181 and perplexity is 115.72200490423674
At time: 787.9640376567841 and batch: 800, loss is 4.738947086334228 and perplexity is 114.31377575555445
At time: 789.1234843730927 and batch: 850, loss is 4.784761095046997 and perplexity is 119.67276925107713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9647518793741865 and perplexity of 143.27299687711354
Finished 37 epochs...
Completing Train Step...
At time: 792.1861238479614 and batch: 50, loss is 4.859318504333496 and perplexity is 128.9363026277271
At time: 793.3471546173096 and batch: 100, loss is 4.794666776657104 and perplexity is 120.86410033251838
At time: 794.4775011539459 and batch: 150, loss is 4.7932106590271 and perplexity is 120.6882360557999
At time: 795.6085298061371 and batch: 200, loss is 4.806984596252441 and perplexity is 122.36208955585879
At time: 796.7401463985443 and batch: 250, loss is 4.817425928115845 and perplexity is 123.64640605968279
At time: 797.8726658821106 and batch: 300, loss is 4.784754867553711 and perplexity is 119.6720239920306
At time: 799.0040194988251 and batch: 350, loss is 4.742782936096192 and perplexity is 114.75310829319511
At time: 800.1355471611023 and batch: 400, loss is 4.755037078857422 and perplexity is 116.1679604597867
At time: 801.2653918266296 and batch: 450, loss is 4.797496786117554 and perplexity is 121.20663133426632
At time: 802.3956158161163 and batch: 500, loss is 4.8182339859008785 and perplexity is 123.74635987948308
At time: 803.5281603336334 and batch: 550, loss is 4.792939739227295 and perplexity is 120.65554365175862
At time: 804.6612231731415 and batch: 600, loss is 4.804944267272949 and perplexity is 122.11268515857783
At time: 805.7943794727325 and batch: 650, loss is 4.78950252532959 and perplexity is 120.24153666317608
At time: 806.9223396778107 and batch: 700, loss is 4.7450059795379635 and perplexity is 115.00849319870454
At time: 808.0551717281342 and batch: 750, loss is 4.751230049133301 and perplexity is 115.72654635277404
At time: 809.2036924362183 and batch: 800, loss is 4.7390103530883785 and perplexity is 114.32100824588683
At time: 810.3389031887054 and batch: 850, loss is 4.78479642868042 and perplexity is 119.67699779954107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.964752197265625 and perplexity of 143.27304242237986
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 813.422461271286 and batch: 50, loss is 4.859249258041382 and perplexity is 128.9273745759721
At time: 814.5551626682281 and batch: 100, loss is 4.794591865539551 and perplexity is 120.85504660680695
At time: 815.6886396408081 and batch: 150, loss is 4.793168506622314 and perplexity is 120.6831488636403
At time: 816.8248620033264 and batch: 200, loss is 4.806882371902466 and perplexity is 122.34958181010133
At time: 817.9596974849701 and batch: 250, loss is 4.817340421676636 and perplexity is 123.63583394777788
At time: 819.111311674118 and batch: 300, loss is 4.784681167602539 and perplexity is 119.66320449470798
At time: 820.2442419528961 and batch: 350, loss is 4.742764558792114 and perplexity is 114.75099945980756
At time: 821.3775219917297 and batch: 400, loss is 4.754911756515503 and perplexity is 116.15340293113675
At time: 822.5590205192566 and batch: 450, loss is 4.797411098480224 and perplexity is 121.19624586935792
At time: 823.7047665119171 and batch: 500, loss is 4.818236379623413 and perplexity is 123.74665609428786
At time: 824.8407456874847 and batch: 550, loss is 4.792874269485473 and perplexity is 120.6476446230429
At time: 825.9760389328003 and batch: 600, loss is 4.804786710739136 and perplexity is 122.0934470227564
At time: 827.11084151268 and batch: 650, loss is 4.789308729171753 and perplexity is 120.21823657316523
At time: 828.2510006427765 and batch: 700, loss is 4.744807081222534 and perplexity is 114.98562047789548
At time: 829.4048955440521 and batch: 750, loss is 4.751082601547242 and perplexity is 115.70948401080285
At time: 830.5682125091553 and batch: 800, loss is 4.738807935714721 and perplexity is 114.29787002951136
At time: 831.6999251842499 and batch: 850, loss is 4.784633178710937 and perplexity is 119.65746212794475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.964739163716634 and perplexity of 143.2711750783314
Finished 39 epochs...
Completing Train Step...
At time: 834.8197736740112 and batch: 50, loss is 4.859219732284546 and perplexity is 128.92356795385783
At time: 835.9753952026367 and batch: 100, loss is 4.794553985595703 and perplexity is 120.85046871113357
At time: 837.1107158660889 and batch: 150, loss is 4.793118534088134 and perplexity is 120.67711817154449
At time: 838.2468528747559 and batch: 200, loss is 4.806867179870605 and perplexity is 122.34772308547522
At time: 839.3804924488068 and batch: 250, loss is 4.817309312820434 and perplexity is 123.63198783822263
At time: 840.5295085906982 and batch: 300, loss is 4.78466233253479 and perplexity is 119.66095065136997
At time: 841.6634151935577 and batch: 350, loss is 4.742748126983643 and perplexity is 114.7491139088541
At time: 842.8231506347656 and batch: 400, loss is 4.754898595809936 and perplexity is 116.15187428045935
At time: 843.9695401191711 and batch: 450, loss is 4.797407913208008 and perplexity is 121.19585982693802
At time: 845.1271803379059 and batch: 500, loss is 4.8182254409790035 and perplexity is 123.7453024810233
At time: 846.2604911327362 and batch: 550, loss is 4.792872486114502 and perplexity is 120.64742946372752
At time: 847.3905708789825 and batch: 600, loss is 4.804786987304688 and perplexity is 122.09348078960261
At time: 848.531387090683 and batch: 650, loss is 4.789306926727295 and perplexity is 120.21801988666618
At time: 849.6759090423584 and batch: 700, loss is 4.7448091793060305 and perplexity is 114.9858617275812
At time: 850.8085055351257 and batch: 750, loss is 4.751094236373901 and perplexity is 115.71083027842396
At time: 851.9906826019287 and batch: 800, loss is 4.738840255737305 and perplexity is 114.30156419894946
At time: 853.1327681541443 and batch: 850, loss is 4.784651422500611 and perplexity is 119.65964515342998
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.964725812276204 and perplexity of 143.26926221454184
Finished 40 epochs...
Completing Train Step...
At time: 856.3941085338593 and batch: 50, loss is 4.859191370010376 and perplexity is 128.9199114401303
At time: 857.5285074710846 and batch: 100, loss is 4.794520969390869 and perplexity is 120.84647875317131
At time: 858.6619982719421 and batch: 150, loss is 4.793078927993775 and perplexity is 120.67233871686352
At time: 859.7998368740082 and batch: 200, loss is 4.8068458938598635 and perplexity is 122.34511881824476
At time: 860.9370934963226 and batch: 250, loss is 4.8172835350036625 and perplexity is 123.6288009165691
At time: 862.0741686820984 and batch: 300, loss is 4.784644536972046 and perplexity is 119.65882123636173
At time: 863.2085404396057 and batch: 350, loss is 4.742732172012329 and perplexity is 114.74728310463864
At time: 864.3528492450714 and batch: 400, loss is 4.754886837005615 and perplexity is 116.15050848132823
At time: 865.4992134571075 and batch: 450, loss is 4.797409200668335 and perplexity is 121.19601586189987
At time: 866.6556279659271 and batch: 500, loss is 4.818228340148925 and perplexity is 123.74566124020228
At time: 867.794025182724 and batch: 550, loss is 4.792876615524292 and perplexity is 120.64792766743254
At time: 868.9313900470734 and batch: 600, loss is 4.804786987304688 and perplexity is 122.09348078960261
At time: 870.068591594696 and batch: 650, loss is 4.789304399490357 and perplexity is 120.21771606762964
At time: 871.2033863067627 and batch: 700, loss is 4.744810419082642 and perplexity is 114.98600428445155
At time: 872.3408200740814 and batch: 750, loss is 4.7511070728302 and perplexity is 115.71231560497327
At time: 873.4769718647003 and batch: 800, loss is 4.738865346908569 and perplexity is 114.3044321950531
At time: 874.6117541790009 and batch: 850, loss is 4.7846653175354 and perplexity is 119.66130783991379
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.964715639750163 and perplexity of 143.2678048116538
Finished 41 epochs...
Completing Train Step...
At time: 877.6931793689728 and batch: 50, loss is 4.859162979125976 and perplexity is 128.91625134178472
At time: 878.86514544487 and batch: 100, loss is 4.794490356445312 and perplexity is 120.84277934312166
At time: 880.0072410106659 and batch: 150, loss is 4.793044185638427 and perplexity is 120.66814634841803
At time: 881.1858747005463 and batch: 200, loss is 4.8068221950531 and perplexity is 122.34221941927174
At time: 882.3191385269165 and batch: 250, loss is 4.817259769439698 and perplexity is 123.62586284330565
At time: 883.4515116214752 and batch: 300, loss is 4.784627180099488 and perplexity is 119.65674435147527
At time: 884.5827560424805 and batch: 350, loss is 4.742716493606568 and perplexity is 114.7454840642772
At time: 885.7151110172272 and batch: 400, loss is 4.754875631332397 and perplexity is 116.14920694397841
At time: 886.8474116325378 and batch: 450, loss is 4.797411861419678 and perplexity is 121.19633833479082
At time: 887.9751901626587 and batch: 500, loss is 4.818236284255981 and perplexity is 123.7466442928876
At time: 889.1066658496857 and batch: 550, loss is 4.792882461547851 and perplexity is 120.64863298012172
At time: 890.2401652336121 and batch: 600, loss is 4.804786367416382 and perplexity is 122.09340510530508
At time: 891.3719549179077 and batch: 650, loss is 4.7893015575408935 and perplexity is 120.21737441544145
At time: 892.5025238990784 and batch: 700, loss is 4.744811382293701 and perplexity is 114.98611504029589
At time: 893.6351521015167 and batch: 750, loss is 4.751119842529297 and perplexity is 115.71379322585975
At time: 894.769280910492 and batch: 800, loss is 4.738886795043945 and perplexity is 114.3068838382804
At time: 895.9188330173492 and batch: 850, loss is 4.784677114486694 and perplexity is 119.66271948686071
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.964708964029948 and perplexity of 143.26684839906548
Finished 42 epochs...
Completing Train Step...
At time: 899.0006766319275 and batch: 50, loss is 4.859134950637817 and perplexity is 128.9126380647981
At time: 900.1611588001251 and batch: 100, loss is 4.79446120262146 and perplexity is 120.8392563653733
At time: 901.2913093566895 and batch: 150, loss is 4.793011960983276 and perplexity is 120.66425792166613
At time: 902.430127620697 and batch: 200, loss is 4.806797752380371 and perplexity is 122.33922908498751
At time: 903.5676372051239 and batch: 250, loss is 4.8172370719909665 and perplexity is 123.6230568834661
At time: 904.7054979801178 and batch: 300, loss is 4.7846098804473876 and perplexity is 119.65467434933176
At time: 905.8410277366638 and batch: 350, loss is 4.7427014636993405 and perplexity is 114.7437594632573
At time: 906.9825716018677 and batch: 400, loss is 4.754864730834961 and perplexity is 116.14794086674635
At time: 908.1327614784241 and batch: 450, loss is 4.797415142059326 and perplexity is 121.19673593695575
At time: 909.314927816391 and batch: 500, loss is 4.818246593475342 and perplexity is 123.74792003076469
At time: 910.4487180709839 and batch: 550, loss is 4.792888879776001 and perplexity is 120.64940733305913
At time: 911.5958445072174 and batch: 600, loss is 4.804785547256469 and perplexity is 122.0933049692297
At time: 912.7351911067963 and batch: 650, loss is 4.789298458099365 and perplexity is 120.21700180929619
At time: 913.8857982158661 and batch: 700, loss is 4.7448123264312745 and perplexity is 114.98622360305878
At time: 915.0233924388885 and batch: 750, loss is 4.7511319732666015 and perplexity is 115.71519692800183
At time: 916.1606237888336 and batch: 800, loss is 4.7389062309265135 and perplexity is 114.30910551504148
At time: 917.2995038032532 and batch: 850, loss is 4.784688110351563 and perplexity is 119.66403528918815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.96470324198405 and perplexity of 143.26602862192863
Finished 43 epochs...
Completing Train Step...
At time: 920.3984010219574 and batch: 50, loss is 4.859107141494751 and perplexity is 128.9090531646498
At time: 921.5617060661316 and batch: 100, loss is 4.794433164596557 and perplexity is 120.8358683187914
At time: 922.6966516971588 and batch: 150, loss is 4.792981395721435 and perplexity is 120.66056984339178
At time: 923.8327076435089 and batch: 200, loss is 4.806773118972778 and perplexity is 122.33621549001057
At time: 924.970460653305 and batch: 250, loss is 4.817214813232422 and perplexity is 123.62030521831682
At time: 926.1140232086182 and batch: 300, loss is 4.784592981338501 and perplexity is 119.65265230904654
At time: 927.2625954151154 and batch: 350, loss is 4.742686958312988 and perplexity is 114.74209507276609
At time: 928.4355306625366 and batch: 400, loss is 4.754853887557983 and perplexity is 116.14668144928127
At time: 929.5763952732086 and batch: 450, loss is 4.7974186038970945 and perplexity is 121.19715550111987
At time: 930.7497839927673 and batch: 500, loss is 4.81825743675232 and perplexity is 123.74926187101195
At time: 931.9000098705292 and batch: 550, loss is 4.792895498275757 and perplexity is 120.65020585377466
At time: 933.0425491333008 and batch: 600, loss is 4.8047842693328855 and perplexity is 122.09314894341556
At time: 934.1877508163452 and batch: 650, loss is 4.789295358657837 and perplexity is 120.21662920430582
At time: 935.3321335315704 and batch: 700, loss is 4.744813089370727 and perplexity is 114.98631133061876
At time: 936.4767506122589 and batch: 750, loss is 4.751143760681153 and perplexity is 115.71656091903684
At time: 937.6135668754578 and batch: 800, loss is 4.7389246845245365 and perplexity is 114.31121494878829
At time: 938.7993366718292 and batch: 850, loss is 4.784698476791382 and perplexity is 119.66527578563829
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.964699745178223 and perplexity of 143.2655276493208
Finished 44 epochs...
Completing Train Step...
At time: 941.9301781654358 and batch: 50, loss is 4.859079885482788 and perplexity is 128.9055396658368
At time: 943.0726416110992 and batch: 100, loss is 4.7944056510925295 and perplexity is 120.83254374637721
At time: 944.2150838375092 and batch: 150, loss is 4.792951869964599 and perplexity is 120.65700730134054
At time: 945.3649990558624 and batch: 200, loss is 4.806748647689819 and perplexity is 122.333221802495
At time: 946.5168838500977 and batch: 250, loss is 4.817192850112915 and perplexity is 123.61759016059548
At time: 947.6600494384766 and batch: 300, loss is 4.7845762252807615 and perplexity is 119.65064741909283
At time: 948.7992920875549 and batch: 350, loss is 4.742672567367554 and perplexity is 114.74044383741831
At time: 949.937658071518 and batch: 400, loss is 4.754843082427978 and perplexity is 116.14542647606865
At time: 951.0773682594299 and batch: 450, loss is 4.797422323226929 and perplexity is 121.19760627415444
At time: 952.2316918373108 and batch: 500, loss is 4.81826865196228 and perplexity is 123.750649752749
At time: 953.3852002620697 and batch: 550, loss is 4.792902288436889 and perplexity is 120.6510250908944
At time: 954.5493807792664 and batch: 600, loss is 4.804782819747925 and perplexity is 122.09297195915134
At time: 955.6937165260315 and batch: 650, loss is 4.789292240142823 and perplexity is 120.21625430752724
At time: 956.8371407985687 and batch: 700, loss is 4.744813861846924 and perplexity is 114.98640015484153
At time: 957.9810569286346 and batch: 750, loss is 4.751155157089233 and perplexity is 115.71787967970127
At time: 959.1245634555817 and batch: 800, loss is 4.738942441940307 and perplexity is 114.31324483858214
At time: 960.2674548625946 and batch: 850, loss is 4.784708423614502 and perplexity is 119.66646608088992
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.964697519938151 and perplexity of 143.26520884948255
Finished 45 epochs...
Completing Train Step...
At time: 963.4831354618073 and batch: 50, loss is 4.859052839279175 and perplexity is 128.90205330751078
At time: 964.6199824810028 and batch: 100, loss is 4.794378852844238 and perplexity is 120.82930568925552
At time: 965.7581615447998 and batch: 150, loss is 4.792922992706298 and perplexity is 120.65352310808208
At time: 966.8957738876343 and batch: 200, loss is 4.806724214553833 and perplexity is 122.330232854766
At time: 968.1988089084625 and batch: 250, loss is 4.817171039581299 and perplexity is 123.61489402463924
At time: 969.3364996910095 and batch: 300, loss is 4.784559230804444 and perplexity is 119.64861403627705
At time: 970.4914107322693 and batch: 350, loss is 4.742658472061157 and perplexity is 114.73882654710447
At time: 971.6287257671356 and batch: 400, loss is 4.754832677841186 and perplexity is 116.14421803718504
At time: 972.7656064033508 and batch: 450, loss is 4.7974261379241945 and perplexity is 121.19806860721353
At time: 973.904440164566 and batch: 500, loss is 4.818279867172241 and perplexity is 123.75203765005152
At time: 975.046008348465 and batch: 550, loss is 4.792908811569214 and perplexity is 120.65181211606313
At time: 976.1795482635498 and batch: 600, loss is 4.804781532287597 and perplexity is 122.09281476939485
At time: 977.3139662742615 and batch: 650, loss is 4.789289026260376 and perplexity is 120.2158679472386
At time: 978.446117401123 and batch: 700, loss is 4.744814691543579 and perplexity is 114.98649555871263
At time: 979.5825898647308 and batch: 750, loss is 4.751166172027588 and perplexity is 115.71915431203253
At time: 980.714054107666 and batch: 800, loss is 4.73895941734314 and perplexity is 114.31518536843298
At time: 981.8488540649414 and batch: 850, loss is 4.784718332290649 and perplexity is 119.66765182302255
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9646962483723955 and perplexity of 143.26502667846478
Finished 46 epochs...
Completing Train Step...
At time: 985.0329995155334 and batch: 50, loss is 4.859026336669922 and perplexity is 128.89863711202932
At time: 986.1833491325378 and batch: 100, loss is 4.794352350234985 and perplexity is 120.8261034398147
At time: 987.3245947360992 and batch: 150, loss is 4.792894496917724 and perplexity is 120.65008503978234
At time: 988.4602568149567 and batch: 200, loss is 4.806700143814087 and perplexity is 122.32728831100665
At time: 989.6038157939911 and batch: 250, loss is 4.817149543762207 and perplexity is 123.61223684979942
At time: 990.7497482299805 and batch: 300, loss is 4.784542751312256 and perplexity is 119.64664230412339
At time: 991.8858728408813 and batch: 350, loss is 4.74264458656311 and perplexity is 114.73723335241377
At time: 993.0213847160339 and batch: 400, loss is 4.7548223400115965 and perplexity is 116.14301736425733
At time: 994.1460831165314 and batch: 450, loss is 4.797429742813111 and perplexity is 121.19850551357521
At time: 995.2771461009979 and batch: 500, loss is 4.818291044235229 and perplexity is 123.75342084210124
At time: 996.4127564430237 and batch: 550, loss is 4.792915344238281 and perplexity is 120.65260029699849
At time: 997.6954407691956 and batch: 600, loss is 4.804779939651489 and perplexity is 122.09262032012433
At time: 998.8280229568481 and batch: 650, loss is 4.789285764694214 and perplexity is 120.21547585587099
At time: 999.9649789333344 and batch: 700, loss is 4.744815568923951 and perplexity is 114.98659644565113
At time: 1001.1174168586731 and batch: 750, loss is 4.751176862716675 and perplexity is 115.72039143614552
At time: 1002.2521228790283 and batch: 800, loss is 4.73897611618042 and perplexity is 114.31709431505064
At time: 1003.3841245174408 and batch: 850, loss is 4.7847278022766115 and perplexity is 119.66878507937143
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.964694658915202 and perplexity of 143.26479896501854
Finished 47 epochs...
Completing Train Step...
At time: 1006.4729156494141 and batch: 50, loss is 4.858999948501587 and perplexity is 128.895235757973
At time: 1007.6314287185669 and batch: 100, loss is 4.79432632446289 and perplexity is 120.82295888810336
At time: 1008.7653760910034 and batch: 150, loss is 4.792866487503051 and perplexity is 120.64670574884633
At time: 1009.8981463909149 and batch: 200, loss is 4.806676473617554 and perplexity is 122.32439283431931
At time: 1011.0334515571594 and batch: 250, loss is 4.817128248214722 and perplexity is 123.6096044875687
At time: 1012.1683237552643 and batch: 300, loss is 4.784526290893555 and perplexity is 119.64467288650367
At time: 1013.3034429550171 and batch: 350, loss is 4.742630701065064 and perplexity is 114.7356401798452
At time: 1014.439893245697 and batch: 400, loss is 4.7548122215271 and perplexity is 116.14184217888231
At time: 1015.5768041610718 and batch: 450, loss is 4.797433567047119 and perplexity is 121.19896900590804
At time: 1016.7321536540985 and batch: 500, loss is 4.818302164077759 and perplexity is 123.75479696830462
At time: 1017.8642220497131 and batch: 550, loss is 4.792921781539917 and perplexity is 120.6533769766796
At time: 1019.0011584758759 and batch: 600, loss is 4.804778356552124 and perplexity is 122.09242703552756
At time: 1020.1392893791199 and batch: 650, loss is 4.789282569885254 and perplexity is 120.21509179100505
At time: 1021.2872173786163 and batch: 700, loss is 4.744816417694092 and perplexity is 114.98669404288226
At time: 1022.4416196346283 and batch: 750, loss is 4.751187314987183 and perplexity is 115.72160098330131
At time: 1023.5866813659668 and batch: 800, loss is 4.73899248123169 and perplexity is 114.31896513546809
At time: 1024.727528333664 and batch: 850, loss is 4.784737319946289 and perplexity is 119.66992405275867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.964694341023763 and perplexity of 143.26475342237262
Finished 48 epochs...
Completing Train Step...
At time: 1027.7990567684174 and batch: 50, loss is 4.858973894119263 and perplexity is 128.8918775159694
At time: 1028.9596598148346 and batch: 100, loss is 4.794300584793091 and perplexity is 120.81984898506153
At time: 1030.0913994312286 and batch: 150, loss is 4.792838850021362 and perplexity is 120.64337142380175
At time: 1031.225023984909 and batch: 200, loss is 4.806652412414551 and perplexity is 122.32144959768021
At time: 1032.3561232089996 and batch: 250, loss is 4.81710693359375 and perplexity is 123.60696982377905
At time: 1033.4855561256409 and batch: 300, loss is 4.784509754180908 and perplexity is 119.64269437328747
At time: 1034.618570804596 and batch: 350, loss is 4.742617120742798 and perplexity is 114.73408204345621
At time: 1035.7481446266174 and batch: 400, loss is 4.754802227020264 and perplexity is 116.14068140424736
At time: 1036.8815586566925 and batch: 450, loss is 4.7974374485015865 and perplexity is 121.19943943510069
At time: 1038.0212280750275 and batch: 500, loss is 4.818313236236572 and perplexity is 123.75616720865636
At time: 1039.1510429382324 and batch: 550, loss is 4.792928276062011 and perplexity is 120.65416056524664
At time: 1040.276409626007 and batch: 600, loss is 4.804776830673218 and perplexity is 122.09224073741072
At time: 1041.4067301750183 and batch: 650, loss is 4.78927921295166 and perplexity is 120.21468823760233
At time: 1042.541778087616 and batch: 700, loss is 4.744817161560059 and perplexity is 114.98677957760243
At time: 1043.6729183197021 and batch: 750, loss is 4.751197576522827 and perplexity is 115.72278847072734
At time: 1044.8054308891296 and batch: 800, loss is 4.739008445739746 and perplexity is 114.320790196076
At time: 1045.9407908916473 and batch: 850, loss is 4.784746637344361 and perplexity is 119.67103907027281
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.964695294698079 and perplexity of 143.26489005035356
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 1049.0408928394318 and batch: 50, loss is 4.858956260681152 and perplexity is 128.88960472906288
At time: 1050.1766204833984 and batch: 100, loss is 4.794279356002807 and perplexity is 120.81728415304953
At time: 1051.314376115799 and batch: 150, loss is 4.7928252696990965 and perplexity is 120.64173305906337
At time: 1052.451521396637 and batch: 200, loss is 4.806626682281494 and perplexity is 122.31830229099683
At time: 1053.586279630661 and batch: 250, loss is 4.817081394195557 and perplexity is 123.60381301646898
At time: 1054.7283835411072 and batch: 300, loss is 4.784492435455323 and perplexity is 119.64062233223795
At time: 1055.905826807022 and batch: 350, loss is 4.742616577148437 and perplexity is 114.73401967467319
At time: 1057.038518667221 and batch: 400, loss is 4.7547633075714115 and perplexity is 116.13616136089716
At time: 1058.1705377101898 and batch: 450, loss is 4.797402572631836 and perplexity is 121.1952125729453
At time: 1059.3298308849335 and batch: 500, loss is 4.818295431137085 and perplexity is 123.75396373740362
At time: 1060.4784774780273 and batch: 550, loss is 4.792899169921875 and perplexity is 120.65064883944784
At time: 1061.637849330902 and batch: 600, loss is 4.804730863571167 and perplexity is 122.08662863990803
At time: 1062.7956779003143 and batch: 650, loss is 4.789224014282227 and perplexity is 120.20805272990255
At time: 1063.9509499073029 and batch: 700, loss is 4.744760570526123 and perplexity is 114.98027254097883
At time: 1065.1006608009338 and batch: 750, loss is 4.751152439117432 and perplexity is 115.7175651621948
At time: 1066.2573959827423 and batch: 800, loss is 4.738940906524658 and perplexity is 114.31306932037188
At time: 1067.3949053287506 and batch: 850, loss is 4.784694509506226 and perplexity is 119.66480104030765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.964693387349446 and perplexity of 143.264616794522
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fe9c612d9e8>
SETTINGS FOR THIS RUN
{'dropout': 0.4690668651775487, 'batch_size': 50, 'data': 'wikitext', 'lr': 14.937532805173465, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'anneal': 7.048441954233849, 'wordvec_source': '', 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.751601219177246 and batch: 50, loss is 6.945235872268677 and perplexity is 1038.1918489945917
At time: 2.888821601867676 and batch: 100, loss is 6.182977466583252 and perplexity is 484.43219182529066
At time: 4.040428161621094 and batch: 150, loss is 6.0288121700286865 and perplexity is 415.2215237689652
At time: 5.175626754760742 and batch: 200, loss is 6.010419578552246 and perplexity is 407.6543274084405
At time: 6.309173107147217 and batch: 250, loss is 6.03636697769165 and perplexity is 418.3703218239122
At time: 7.442489862442017 and batch: 300, loss is 5.955725078582764 and perplexity is 385.9566581530147
At time: 8.57791805267334 and batch: 350, loss is 5.91981204032898 and perplexity is 372.3417220709823
At time: 9.74086332321167 and batch: 400, loss is 5.932254247665405 and perplexity is 377.00341571743115
At time: 10.876240730285645 and batch: 450, loss is 5.931993598937988 and perplexity is 376.90516306216375
At time: 12.012368202209473 and batch: 500, loss is 5.921107883453369 and perplexity is 372.824531286391
At time: 13.166081428527832 and batch: 550, loss is 5.8860245704650875 and perplexity is 359.971395181874
At time: 14.312207221984863 and batch: 600, loss is 5.90983063697815 and perplexity is 368.6437155083564
At time: 15.44168496131897 and batch: 650, loss is 5.922284708023072 and perplexity is 373.2635386216401
At time: 16.572062730789185 and batch: 700, loss is 5.870202198028564 and perplexity is 354.3206159552497
At time: 17.705783128738403 and batch: 750, loss is 5.870172080993652 and perplexity is 354.309945029578
At time: 18.835118532180786 and batch: 800, loss is 5.877318964004517 and perplexity is 356.8512270620378
At time: 19.96535587310791 and batch: 850, loss is 5.870113134384155 and perplexity is 354.2890602751564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.316513379414876 and perplexity of 203.67251379888518
Finished 1 epochs...
Completing Train Step...
At time: 23.013795852661133 and batch: 50, loss is 5.625090408325195 and perplexity is 277.29735337920033
At time: 24.20376181602478 and batch: 100, loss is 5.542949161529541 and perplexity is 255.4301946443425
At time: 25.34343957901001 and batch: 150, loss is 5.527770853042602 and perplexity is 251.58247118986412
At time: 26.48255944252014 and batch: 200, loss is 5.511041746139527 and perplexity is 247.4087299552363
At time: 27.625957489013672 and batch: 250, loss is 5.561921701431275 and perplexity is 260.322618301457
At time: 28.765527486801147 and batch: 300, loss is 5.519280948638916 and perplexity is 249.45560129597135
At time: 29.899558067321777 and batch: 350, loss is 5.4914772510528564 and perplexity is 242.61534603745733
At time: 31.04029631614685 and batch: 400, loss is 5.5002287578582765 and perplexity is 244.7479138694546
At time: 32.21017789840698 and batch: 450, loss is 5.499429941177368 and perplexity is 244.55248322026162
At time: 33.37339401245117 and batch: 500, loss is 5.514373731613159 and perplexity is 248.2344671577258
At time: 34.518449544906616 and batch: 550, loss is 5.4785268688201905 and perplexity is 239.49364183102713
At time: 35.689476013183594 and batch: 600, loss is 5.515614280700683 and perplexity is 248.5426052901721
At time: 36.819732427597046 and batch: 650, loss is 5.528362903594971 and perplexity is 251.7314648324304
At time: 37.95036816596985 and batch: 700, loss is 5.480336179733277 and perplexity is 239.92735253124175
At time: 39.078307151794434 and batch: 750, loss is 5.498469247817993 and perplexity is 244.31765609010623
At time: 40.20231628417969 and batch: 800, loss is 5.471185913085938 and perplexity is 237.74196695798148
At time: 41.33152747154236 and batch: 850, loss is 5.46932544708252 and perplexity is 237.30006730797186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.2377058664957685 and perplexity of 188.23776411298962
Finished 2 epochs...
Completing Train Step...
At time: 44.43773818016052 and batch: 50, loss is 5.469842720031738 and perplexity is 237.42284796645689
At time: 45.56768083572388 and batch: 100, loss is 5.39868317604065 and perplexity is 221.1150548085112
At time: 46.74000000953674 and batch: 150, loss is 5.403640279769897 and perplexity is 222.21386628244377
At time: 47.8722186088562 and batch: 200, loss is 5.418935012817383 and perplexity is 225.638692182077
At time: 49.003456592559814 and batch: 250, loss is 5.4551834297180175 and perplexity is 233.96778377822042
At time: 50.13078951835632 and batch: 300, loss is 5.429881248474121 and perplexity is 228.12215396158987
At time: 51.26238775253296 and batch: 350, loss is 5.362214221954345 and perplexity is 213.19648855299567
At time: 52.395079612731934 and batch: 400, loss is 5.393380374908447 and perplexity is 219.94562900209465
At time: 53.52650427818298 and batch: 450, loss is 5.426453285217285 and perplexity is 227.34149839349342
At time: 54.65934491157532 and batch: 500, loss is 5.438002672195434 and perplexity is 229.98237422801435
At time: 55.81289339065552 and batch: 550, loss is 5.3926779079437255 and perplexity is 219.7911787181481
At time: 56.94611406326294 and batch: 600, loss is 5.426772031784058 and perplexity is 227.41397426569546
At time: 58.07849597930908 and batch: 650, loss is 5.419549083709716 and perplexity is 225.77729288610388
At time: 59.21364784240723 and batch: 700, loss is 5.377191143035889 and perplexity is 216.41354621212022
At time: 60.34791159629822 and batch: 750, loss is 5.409186086654663 and perplexity is 223.44964499972602
At time: 61.484090089797974 and batch: 800, loss is 5.393772134780884 and perplexity is 220.0318117540252
At time: 62.622546434402466 and batch: 850, loss is 5.394632616043091 and perplexity is 220.22122648735612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.316298802693685 and perplexity of 203.62881510720607
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 65.70642733573914 and batch: 50, loss is 5.409521884918213 and perplexity is 223.52469160206198
At time: 66.87522482872009 and batch: 100, loss is 5.2875157165527344 and perplexity is 197.8512956687519
At time: 68.00927662849426 and batch: 150, loss is 5.262500801086426 and perplexity is 192.96345161205625
At time: 69.14438700675964 and batch: 200, loss is 5.265693187713623 and perplexity is 193.58044987902585
At time: 70.27955627441406 and batch: 250, loss is 5.273861713409424 and perplexity is 195.16819268855951
At time: 71.41474461555481 and batch: 300, loss is 5.236590929031372 and perplexity is 188.02800773191848
At time: 72.54881572723389 and batch: 350, loss is 5.159989500045777 and perplexity is 174.1626268959003
At time: 73.68117070198059 and batch: 400, loss is 5.164860630035401 and perplexity is 175.01306530727663
At time: 74.84211206436157 and batch: 450, loss is 5.170354194641114 and perplexity is 175.9771566126937
At time: 75.97549510002136 and batch: 500, loss is 5.170572414398193 and perplexity is 176.01556249537006
At time: 77.11181402206421 and batch: 550, loss is 5.13310866355896 and perplexity is 169.54335292039906
At time: 78.24695062637329 and batch: 600, loss is 5.149936103820801 and perplexity is 172.42047295543688
At time: 79.38036489486694 and batch: 650, loss is 5.149338254928589 and perplexity is 172.31742237409304
At time: 80.51357460021973 and batch: 700, loss is 5.1045671081542965 and perplexity is 164.77272632097316
At time: 81.6474974155426 and batch: 750, loss is 5.071824359893799 and perplexity is 159.46498365246558
At time: 82.78196024894714 and batch: 800, loss is 5.034751882553101 and perplexity is 153.6614617919907
At time: 83.91752457618713 and batch: 850, loss is 5.023590831756592 and perplexity is 151.95597363565457
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.00089963277181 and perplexity of 148.54673652057537
Finished 4 epochs...
Completing Train Step...
At time: 87.08471751213074 and batch: 50, loss is 5.2024365901947025 and perplexity is 181.71446658368063
At time: 88.25233817100525 and batch: 100, loss is 5.129375534057617 and perplexity is 168.91160555889425
At time: 89.385094165802 and batch: 150, loss is 5.120223922729492 and perplexity is 167.37284400933729
At time: 90.52056384086609 and batch: 200, loss is 5.141996850967407 and perplexity is 171.05700285231174
At time: 91.65486192703247 and batch: 250, loss is 5.156005945205688 and perplexity is 173.47022055551358
At time: 92.78851056098938 and batch: 300, loss is 5.128466234207154 and perplexity is 168.75808407032926
At time: 93.92338466644287 and batch: 350, loss is 5.058135843276977 and perplexity is 157.29701655265808
At time: 95.05588507652283 and batch: 400, loss is 5.074507007598877 and perplexity is 159.89334634109682
At time: 96.19232296943665 and batch: 450, loss is 5.091804666519165 and perplexity is 162.68318619711087
At time: 97.32664132118225 and batch: 500, loss is 5.094588537216186 and perplexity is 163.13670612959248
At time: 98.46089053153992 and batch: 550, loss is 5.066122417449951 and perplexity is 158.55831084843078
At time: 99.5932834148407 and batch: 600, loss is 5.088688220977783 and perplexity is 162.17698209528476
At time: 100.72674870491028 and batch: 650, loss is 5.0836992359161375 and perplexity is 161.3698984923113
At time: 101.86373901367188 and batch: 700, loss is 5.049173946380615 and perplexity is 155.8936347823481
At time: 103.02668285369873 and batch: 750, loss is 5.0331020927429195 and perplexity is 153.4081616815321
At time: 104.16207957267761 and batch: 800, loss is 5.010923252105713 and perplexity is 150.04319992100173
At time: 105.29959201812744 and batch: 850, loss is 5.013699636459351 and perplexity is 150.46035633894382
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.986076354980469 and perplexity of 146.3610266940412
Finished 5 epochs...
Completing Train Step...
At time: 108.39836525917053 and batch: 50, loss is 5.140193433761596 and perplexity is 170.74879370856254
At time: 109.53902292251587 and batch: 100, loss is 5.071391496658325 and perplexity is 159.395972061065
At time: 110.6784291267395 and batch: 150, loss is 5.06301134109497 and perplexity is 158.06579036832494
At time: 111.82167458534241 and batch: 200, loss is 5.086537818908692 and perplexity is 161.82861108054914
At time: 112.9688184261322 and batch: 250, loss is 5.100128927230835 and perplexity is 164.0430555534698
At time: 114.10046887397766 and batch: 300, loss is 5.071490716934204 and perplexity is 159.41178815801266
At time: 115.23342514038086 and batch: 350, loss is 5.004987030029297 and perplexity is 149.15514860793473
At time: 116.36645436286926 and batch: 400, loss is 5.026856117248535 and perplexity is 152.4529642383036
At time: 117.49863076210022 and batch: 450, loss is 5.048126583099365 and perplexity is 155.73044298896056
At time: 118.62975025177002 and batch: 500, loss is 5.053133544921875 and perplexity is 156.51213468879595
At time: 119.76280450820923 and batch: 550, loss is 5.027469844818115 and perplexity is 152.5465575429788
At time: 120.89761757850647 and batch: 600, loss is 5.0523500156402585 and perplexity is 156.38955087861277
At time: 122.03027820587158 and batch: 650, loss is 5.04497052192688 and perplexity is 155.23972296300647
At time: 123.1550600528717 and batch: 700, loss is 5.013420906066894 and perplexity is 150.41842430890938
At time: 124.29555487632751 and batch: 750, loss is 5.006192855834961 and perplexity is 149.33511221574915
At time: 125.4276692867279 and batch: 800, loss is 4.98683723449707 and perplexity is 146.47243217898247
At time: 126.57162833213806 and batch: 850, loss is 4.994296464920044 and perplexity is 147.56908882368538
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.976373354593913 and perplexity of 144.9477531802585
Finished 6 epochs...
Completing Train Step...
At time: 129.6439688205719 and batch: 50, loss is 5.094635419845581 and perplexity is 163.1443545866151
At time: 130.77271628379822 and batch: 100, loss is 5.0284312915802 and perplexity is 152.69329346488095
At time: 131.94630599021912 and batch: 150, loss is 5.02228627204895 and perplexity is 151.75786724401817
At time: 133.09453749656677 and batch: 200, loss is 5.04727536201477 and perplexity is 155.59793835574467
At time: 134.21791672706604 and batch: 250, loss is 5.05823971748352 and perplexity is 157.3133565040791
At time: 135.34451746940613 and batch: 300, loss is 5.02993577003479 and perplexity is 152.92319012898471
At time: 136.47090005874634 and batch: 350, loss is 4.9659502315521244 and perplexity is 143.4447912995123
At time: 137.59699296951294 and batch: 400, loss is 4.9905259227752685 and perplexity is 147.01372103181336
At time: 138.7255470752716 and batch: 450, loss is 5.013652477264404 and perplexity is 150.45326091697603
At time: 139.8778007030487 and batch: 500, loss is 5.019731454849243 and perplexity is 151.37064848178687
At time: 141.00409054756165 and batch: 550, loss is 4.995567150115967 and perplexity is 147.75672186624752
At time: 142.13275575637817 and batch: 600, loss is 5.023576650619507 and perplexity is 151.953818742441
At time: 143.28148913383484 and batch: 650, loss is 5.014848823547363 and perplexity is 150.63336282707763
At time: 144.4137887954712 and batch: 700, loss is 4.985537090301514 and perplexity is 146.2821206395058
At time: 145.54315662384033 and batch: 750, loss is 4.984302625656128 and perplexity is 146.1016519473524
At time: 146.67377948760986 and batch: 800, loss is 4.964558076858521 and perplexity is 143.24523290034296
At time: 147.8002691268921 and batch: 850, loss is 4.975414848327636 and perplexity is 144.80888641351734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.970987955729167 and perplexity of 144.16924987179647
Finished 7 epochs...
Completing Train Step...
At time: 150.9013180732727 and batch: 50, loss is 5.058779191970825 and perplexity is 157.39824594220968
At time: 152.07744908332825 and batch: 100, loss is 4.994667415618896 and perplexity is 147.62383983464855
At time: 153.2108175754547 and batch: 150, loss is 4.990287017822266 and perplexity is 146.97860292081185
At time: 154.34438037872314 and batch: 200, loss is 5.015225038528443 and perplexity is 150.69004401631125
At time: 155.4777798652649 and batch: 250, loss is 5.024739599227905 and perplexity is 152.13063601926325
At time: 156.6092507839203 and batch: 300, loss is 4.99626594543457 and perplexity is 147.8600096560887
At time: 157.7416181564331 and batch: 350, loss is 4.934516143798828 and perplexity is 139.0058674067037
At time: 158.8782238960266 and batch: 400, loss is 4.959891214370727 and perplexity is 142.57828458489126
At time: 160.01292061805725 and batch: 450, loss is 4.985693635940552 and perplexity is 146.30502226008883
At time: 161.21489000320435 and batch: 500, loss is 4.991998147964478 and perplexity is 147.23031773551548
At time: 162.35049057006836 and batch: 550, loss is 4.969277391433716 and perplexity is 143.92284990204024
At time: 163.48722314834595 and batch: 600, loss is 4.999427328109741 and perplexity is 148.32819138984647
At time: 164.6228895187378 and batch: 650, loss is 4.988849573135376 and perplexity is 146.76748108329113
At time: 165.75845098495483 and batch: 700, loss is 4.9607407760620115 and perplexity is 142.6994651013573
At time: 166.89366364479065 and batch: 750, loss is 4.964092836380005 and perplexity is 143.17860491986187
At time: 168.0278651714325 and batch: 800, loss is 4.943839340209961 and perplexity is 140.30790656292447
At time: 169.1618673801422 and batch: 850, loss is 4.956283359527588 and perplexity is 142.06480965880007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.965799013773601 and perplexity of 143.42310153681134
Finished 8 epochs...
Completing Train Step...
At time: 172.2219524383545 and batch: 50, loss is 5.027921905517578 and perplexity is 152.61553343595295
At time: 173.37996125221252 and batch: 100, loss is 4.966321668624878 and perplexity is 143.49808190933567
At time: 174.51393008232117 and batch: 150, loss is 4.964591369628907 and perplexity is 143.25000201037878
At time: 175.645911693573 and batch: 200, loss is 4.988206100463867 and perplexity is 146.6730705986889
At time: 176.7766239643097 and batch: 250, loss is 4.995700902938843 and perplexity is 147.77648606662538
At time: 177.9038064479828 and batch: 300, loss is 4.967125463485718 and perplexity is 143.61347129860118
At time: 179.06698751449585 and batch: 350, loss is 4.90645881652832 and perplexity is 135.15993981086376
At time: 180.1985731124878 and batch: 400, loss is 4.93343957901001 and perplexity is 138.85629910884163
At time: 181.34463477134705 and batch: 450, loss is 4.960558433532714 and perplexity is 142.67344729211007
At time: 182.47313332557678 and batch: 500, loss is 4.969037351608276 and perplexity is 143.88830683229506
At time: 183.61423325538635 and batch: 550, loss is 4.946308994293213 and perplexity is 140.6548467921191
At time: 184.754620552063 and batch: 600, loss is 4.978651838302612 and perplexity is 145.2783908076743
At time: 185.9114031791687 and batch: 650, loss is 4.96521954536438 and perplexity is 143.3400164552332
At time: 187.06548881530762 and batch: 700, loss is 4.938474473953247 and perplexity is 139.55718895980561
At time: 188.19524097442627 and batch: 750, loss is 4.943288011550903 and perplexity is 140.23057211325204
At time: 189.32685661315918 and batch: 800, loss is 4.924776926040649 and perplexity is 137.6586301618511
At time: 190.50567650794983 and batch: 850, loss is 4.939478588104248 and perplexity is 139.69739068560276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.962454795837402 and perplexity of 142.94426454208767
Finished 9 epochs...
Completing Train Step...
At time: 193.61960887908936 and batch: 50, loss is 5.001529026031494 and perplexity is 148.64026026380898
At time: 194.78321170806885 and batch: 100, loss is 4.941098737716675 and perplexity is 139.92390480281082
At time: 195.91609168052673 and batch: 150, loss is 4.9406330776214595 and perplexity is 139.8587629921255
At time: 197.0486295223236 and batch: 200, loss is 4.964601440429687 and perplexity is 143.25144465987518
At time: 198.17887020111084 and batch: 250, loss is 4.970823974609375 and perplexity is 144.14561077499863
At time: 199.31023001670837 and batch: 300, loss is 4.941700468063354 and perplexity is 140.0081265994208
At time: 200.44775533676147 and batch: 350, loss is 4.8826200389862064 and perplexity is 131.97599354803444
At time: 201.58652424812317 and batch: 400, loss is 4.910745038986206 and perplexity is 135.7405087151892
At time: 202.72728157043457 and batch: 450, loss is 4.939019222259521 and perplexity is 139.63323321272836
At time: 203.85922026634216 and batch: 500, loss is 4.947277660369873 and perplexity is 140.79116038124047
At time: 204.98792719841003 and batch: 550, loss is 4.925870370864868 and perplexity is 137.8092346023282
At time: 206.13451218605042 and batch: 600, loss is 4.958826007843018 and perplexity is 142.42649012601908
At time: 207.2617964744568 and batch: 650, loss is 4.944206676483154 and perplexity is 140.3594562138695
At time: 208.41252779960632 and batch: 700, loss is 4.9180111312866215 and perplexity is 136.7304037595869
At time: 209.56949663162231 and batch: 750, loss is 4.92478268623352 and perplexity is 137.659423104395
At time: 210.70235466957092 and batch: 800, loss is 4.9063406085968015 and perplexity is 135.14396377822
At time: 211.8354434967041 and batch: 850, loss is 4.923685750961304 and perplexity is 137.5085024178841
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.960412979125977 and perplexity of 142.6526963196737
Finished 10 epochs...
Completing Train Step...
At time: 214.87945008277893 and batch: 50, loss is 4.977287168502808 and perplexity is 145.0802689913518
At time: 216.00342345237732 and batch: 100, loss is 4.919025945663452 and perplexity is 136.8692301686757
At time: 217.16762042045593 and batch: 150, loss is 4.918623609542847 and perplexity is 136.81417380990297
At time: 218.2948112487793 and batch: 200, loss is 4.942646198272705 and perplexity is 140.1405991460714
At time: 219.4588303565979 and batch: 250, loss is 4.947742357254028 and perplexity is 140.85660079858758
At time: 220.58463144302368 and batch: 300, loss is 4.91902792930603 and perplexity is 136.86950166857758
At time: 221.71052432060242 and batch: 350, loss is 4.8605013179779055 and perplexity is 129.08890047535206
At time: 222.83760905265808 and batch: 400, loss is 4.8899639701843265 and perplexity is 132.94878384481223
At time: 223.96283745765686 and batch: 450, loss is 4.919362497329712 and perplexity is 136.91530148840354
At time: 225.09037733078003 and batch: 500, loss is 4.928167781829834 and perplexity is 138.12620301291227
At time: 226.21356177330017 and batch: 550, loss is 4.906923570632935 and perplexity is 135.2227705469606
At time: 227.36022663116455 and batch: 600, loss is 4.941315155029297 and perplexity is 139.95419003526604
At time: 228.50623321533203 and batch: 650, loss is 4.925506649017334 and perplexity is 137.75911948744485
At time: 229.63025951385498 and batch: 700, loss is 4.899299907684326 and perplexity is 134.19579734410044
At time: 230.75661730766296 and batch: 750, loss is 4.908541250228882 and perplexity is 135.44169469060031
At time: 231.8820309638977 and batch: 800, loss is 4.890661954879761 and perplexity is 133.04161245391802
At time: 233.01048517227173 and batch: 850, loss is 4.908833417892456 and perplexity is 135.48127215543482
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.959714253743489 and perplexity of 142.55305607451297
Finished 11 epochs...
Completing Train Step...
At time: 236.06324005126953 and batch: 50, loss is 4.955277967453003 and perplexity is 141.92205060152924
At time: 237.1930890083313 and batch: 100, loss is 4.89751012802124 and perplexity is 133.95583124242387
At time: 238.31923699378967 and batch: 150, loss is 4.898396100997925 and perplexity is 134.07456507869247
At time: 239.44551539421082 and batch: 200, loss is 4.923545932769775 and perplexity is 137.48927757177916
At time: 240.57262516021729 and batch: 250, loss is 4.926379680633545 and perplexity is 137.87944006837225
At time: 241.70186972618103 and batch: 300, loss is 4.897486066818237 and perplexity is 133.95260814275085
At time: 242.82638812065125 and batch: 350, loss is 4.839671583175659 and perplexity is 126.42782388683757
At time: 243.95531630516052 and batch: 400, loss is 4.870653305053711 and perplexity is 130.4060840271008
At time: 245.08007287979126 and batch: 450, loss is 4.90141902923584 and perplexity is 134.48047607826203
At time: 246.20588636398315 and batch: 500, loss is 4.910314168930054 and perplexity is 135.68203479281271
At time: 247.36019134521484 and batch: 550, loss is 4.889571809768677 and perplexity is 132.89665681622623
At time: 248.47936153411865 and batch: 600, loss is 4.923762655258178 and perplexity is 137.51907781921852
At time: 249.5960991382599 and batch: 650, loss is 4.907370204925537 and perplexity is 135.28317916270308
At time: 250.71054983139038 and batch: 700, loss is 4.881811151504516 and perplexity is 131.86928298320822
At time: 251.82487964630127 and batch: 750, loss is 4.893104047775268 and perplexity is 133.36690947159886
At time: 252.94248604774475 and batch: 800, loss is 4.875806474685669 and perplexity is 131.07982315419997
At time: 254.05780220031738 and batch: 850, loss is 4.894568271636963 and perplexity is 133.5623315187053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9582475026448565 and perplexity of 142.34411948934326
Finished 12 epochs...
Completing Train Step...
At time: 257.06964588165283 and batch: 50, loss is 4.934426727294922 and perplexity is 138.99343854369877
At time: 258.19591522216797 and batch: 100, loss is 4.87946268081665 and perplexity is 131.5599552032743
At time: 259.32051396369934 and batch: 150, loss is 4.8812133312225345 and perplexity is 131.7904724108969
At time: 260.44510293006897 and batch: 200, loss is 4.904938983917236 and perplexity is 134.95467534994938
At time: 261.56914496421814 and batch: 250, loss is 4.907700109481811 and perplexity is 135.3278170626145
At time: 262.6936790943146 and batch: 300, loss is 4.878449783325196 and perplexity is 131.4267659196059
At time: 263.81915950775146 and batch: 350, loss is 4.821151638031006 and perplexity is 124.10793593004927
At time: 264.94405341148376 and batch: 400, loss is 4.853499202728272 and perplexity is 128.1881623331501
At time: 266.0698947906494 and batch: 450, loss is 4.884969005584717 and perplexity is 132.28636513221616
At time: 267.1924340724945 and batch: 500, loss is 4.893732995986938 and perplexity is 133.45081673470253
At time: 268.31702852249146 and batch: 550, loss is 4.873427610397339 and perplexity is 130.76837264010535
At time: 269.4424605369568 and batch: 600, loss is 4.908524503707886 and perplexity is 135.43942653240842
At time: 270.5687370300293 and batch: 650, loss is 4.890117483139038 and perplexity is 132.96919477207635
At time: 271.6920371055603 and batch: 700, loss is 4.865794849395752 and perplexity is 129.7740484519797
At time: 272.8180663585663 and batch: 750, loss is 4.877046689987183 and perplexity is 131.24249120744943
At time: 273.9443299770355 and batch: 800, loss is 4.860652112960816 and perplexity is 129.10836790165138
At time: 275.0682761669159 and batch: 850, loss is 4.880716962814331 and perplexity is 131.7250720165775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.957205136617024 and perplexity of 142.19582211842405
Finished 13 epochs...
Completing Train Step...
At time: 278.1498312950134 and batch: 50, loss is 4.914864730834961 and perplexity is 136.3008712504408
At time: 279.29955768585205 and batch: 100, loss is 4.861862621307373 and perplexity is 129.26474928999875
At time: 280.4248399734497 and batch: 150, loss is 4.864411525726318 and perplexity is 129.59465304863897
At time: 281.5558571815491 and batch: 200, loss is 4.8878102684021 and perplexity is 132.66275992776923
At time: 282.6805212497711 and batch: 250, loss is 4.889387140274048 and perplexity is 132.87211712371007
At time: 283.8087680339813 and batch: 300, loss is 4.860619096755982 and perplexity is 129.10410530369873
At time: 284.9378402233124 and batch: 350, loss is 4.804087104797364 and perplexity is 122.00805959402679
At time: 286.0627362728119 and batch: 400, loss is 4.836176967620849 and perplexity is 125.98677833808908
At time: 287.19008779525757 and batch: 450, loss is 4.868475427627564 and perplexity is 130.12238460388647
At time: 288.3163080215454 and batch: 500, loss is 4.878148345947266 and perplexity is 131.38715495031062
At time: 289.4436502456665 and batch: 550, loss is 4.858406686782837 and perplexity is 128.8187898273328
At time: 290.5694386959076 and batch: 600, loss is 4.893178720474243 and perplexity is 133.37686871051997
At time: 291.6937425136566 and batch: 650, loss is 4.874427375793457 and perplexity is 130.8991757092735
At time: 292.82252645492554 and batch: 700, loss is 4.85008150100708 and perplexity is 127.75080124070988
At time: 293.9476845264435 and batch: 750, loss is 4.863488483428955 and perplexity is 129.47508689316166
At time: 295.08045530319214 and batch: 800, loss is 4.847237491607666 and perplexity is 127.38799292087916
At time: 296.20532989501953 and batch: 850, loss is 4.8671095752716065 and perplexity is 129.94477795815143
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.957796096801758 and perplexity of 142.27987902242694
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 299.2524399757385 and batch: 50, loss is 4.905249090194702 and perplexity is 134.9965321316387
At time: 300.4070975780487 and batch: 100, loss is 4.857385778427124 and perplexity is 128.6873447565274
At time: 301.53036618232727 and batch: 150, loss is 4.8532294654846195 and perplexity is 128.153589874523
At time: 302.6571435928345 and batch: 200, loss is 4.8693433856964115 and perplexity is 130.23537440565278
At time: 303.7821989059448 and batch: 250, loss is 4.864462432861328 and perplexity is 129.60125050906572
At time: 304.93297290802 and batch: 300, loss is 4.823698673248291 and perplexity is 124.42444612380234
At time: 306.062867641449 and batch: 350, loss is 4.766681814193726 and perplexity is 117.52861245476531
At time: 307.2102987766266 and batch: 400, loss is 4.792367706298828 and perplexity is 120.5865444445785
At time: 308.33454632759094 and batch: 450, loss is 4.825251302719116 and perplexity is 124.61778123580281
At time: 309.4594066143036 and batch: 500, loss is 4.825393581390381 and perplexity is 124.63551294952546
At time: 310.5877158641815 and batch: 550, loss is 4.7945941543579105 and perplexity is 120.85532322237306
At time: 311.715092420578 and batch: 600, loss is 4.814121322631836 and perplexity is 123.23847786025124
At time: 312.84205317497253 and batch: 650, loss is 4.78705771446228 and perplexity is 119.94792790282798
At time: 313.9677355289459 and batch: 700, loss is 4.758513126373291 and perplexity is 116.57246844735664
At time: 315.09342098236084 and batch: 750, loss is 4.755749320983886 and perplexity is 116.25072964732372
At time: 316.22004437446594 and batch: 800, loss is 4.72646198272705 and perplexity is 112.89542894895287
At time: 317.3451828956604 and batch: 850, loss is 4.768600625991821 and perplexity is 117.75434424210665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.921934445699056 and perplexity of 137.26789380505767
Finished 15 epochs...
Completing Train Step...
At time: 320.3883135318756 and batch: 50, loss is 4.873252515792847 and perplexity is 130.74547780805705
At time: 321.5370750427246 and batch: 100, loss is 4.826494960784912 and perplexity is 124.7728595567926
At time: 322.6565988063812 and batch: 150, loss is 4.822556648254395 and perplexity is 124.28243140409181
At time: 323.77700090408325 and batch: 200, loss is 4.8427276706695555 and perplexity is 126.81478937731923
At time: 324.9002606868744 and batch: 250, loss is 4.840070066452026 and perplexity is 126.47821329932627
At time: 326.0222706794739 and batch: 300, loss is 4.803226270675659 and perplexity is 121.90307608638113
At time: 327.1427175998688 and batch: 350, loss is 4.7464498519897464 and perplexity is 115.17467073446808
At time: 328.26406383514404 and batch: 400, loss is 4.774096212387085 and perplexity is 118.40325485329971
At time: 329.388156414032 and batch: 450, loss is 4.809193143844604 and perplexity is 122.63263069564304
At time: 330.5077414512634 and batch: 500, loss is 4.810585279464721 and perplexity is 122.80347083775541
At time: 331.63255167007446 and batch: 550, loss is 4.78221640586853 and perplexity is 119.36862638885341
At time: 332.76245498657227 and batch: 600, loss is 4.8063200569152835 and perplexity is 122.28080214630421
At time: 333.9129686355591 and batch: 650, loss is 4.781227169036865 and perplexity is 119.25060093427253
At time: 335.03482007980347 and batch: 700, loss is 4.7554316902160645 and perplexity is 116.21381070241279
At time: 336.1600441932678 and batch: 750, loss is 4.757379999160767 and perplexity is 116.44045182106922
At time: 337.2823224067688 and batch: 800, loss is 4.734094276428222 and perplexity is 113.76037658868897
At time: 338.4048125743866 and batch: 850, loss is 4.776618413925171 and perplexity is 118.70226865282291
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9195982615153 and perplexity of 136.947585018454
Finished 16 epochs...
Completing Train Step...
At time: 341.45839285850525 and batch: 50, loss is 4.862104234695434 and perplexity is 129.2959851573815
At time: 342.5834791660309 and batch: 100, loss is 4.8146994972229 and perplexity is 123.30975181920202
At time: 343.7076327800751 and batch: 150, loss is 4.8096214866638185 and perplexity is 122.68517075417795
At time: 344.832795381546 and batch: 200, loss is 4.831009359359741 and perplexity is 125.33740731416844
At time: 345.9567279815674 and batch: 250, loss is 4.828961925506592 and perplexity is 125.08104979045767
At time: 347.09661316871643 and batch: 300, loss is 4.793392705917358 and perplexity is 120.71020897385466
At time: 348.22640657424927 and batch: 350, loss is 4.7367715167999265 and perplexity is 114.06534852101709
At time: 349.3604598045349 and batch: 400, loss is 4.765464410781861 and perplexity is 117.3856197785175
At time: 350.4936740398407 and batch: 450, loss is 4.801705713272095 and perplexity is 121.71785631585453
At time: 351.61640548706055 and batch: 500, loss is 4.803415851593018 and perplexity is 121.92618877416746
At time: 352.7385959625244 and batch: 550, loss is 4.776742334365845 and perplexity is 118.71697920171341
At time: 353.8707506656647 and batch: 600, loss is 4.803099164962768 and perplexity is 121.88758249367451
At time: 355.0240225791931 and batch: 650, loss is 4.7793606185913085 and perplexity is 119.0282212779535
At time: 356.18567848205566 and batch: 700, loss is 4.75475435256958 and perplexity is 116.13512136601604
At time: 357.31618642807007 and batch: 750, loss is 4.758911561965943 and perplexity is 116.61892432213172
At time: 358.45299673080444 and batch: 800, loss is 4.73745644569397 and perplexity is 114.14350193573887
At time: 359.62122201919556 and batch: 850, loss is 4.779549961090088 and perplexity is 119.0507605125457
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.918855985005696 and perplexity of 136.84596976102281
Finished 17 epochs...
Completing Train Step...
At time: 362.967257976532 and batch: 50, loss is 4.854332828521729 and perplexity is 128.29506784509198
At time: 364.0988233089447 and batch: 100, loss is 4.806829204559326 and perplexity is 122.34307698082601
At time: 365.23451232910156 and batch: 150, loss is 4.800952520370483 and perplexity is 121.62621380705401
At time: 366.37220335006714 and batch: 200, loss is 4.823234977722168 and perplexity is 124.3667644391975
At time: 367.50477957725525 and batch: 250, loss is 4.821598997116089 and perplexity is 124.16346916344054
At time: 368.6368021965027 and batch: 300, loss is 4.786791133880615 and perplexity is 119.91595637612154
At time: 369.7658669948578 and batch: 350, loss is 4.730427665710449 and perplexity is 113.34402533806319
At time: 370.9018716812134 and batch: 400, loss is 4.7599577140808105 and perplexity is 116.7409892945768
At time: 372.0367684364319 and batch: 450, loss is 4.796779918670654 and perplexity is 121.11977338245316
At time: 373.1656265258789 and batch: 500, loss is 4.79902491569519 and perplexity is 121.39199236437923
At time: 374.29824447631836 and batch: 550, loss is 4.773235263824463 and perplexity is 118.30135961079934
At time: 375.42915391921997 and batch: 600, loss is 4.800986537933349 and perplexity is 121.63035130480176
At time: 376.56449270248413 and batch: 650, loss is 4.778179063796997 and perplexity is 118.88766596569779
At time: 377.6981108188629 and batch: 700, loss is 4.754097547531128 and perplexity is 116.05886827760918
At time: 378.82806491851807 and batch: 750, loss is 4.759371824264527 and perplexity is 116.67261197055109
At time: 379.9609990119934 and batch: 800, loss is 4.738936014175415 and perplexity is 114.31251006228176
At time: 381.09515404701233 and batch: 850, loss is 4.780587224960327 and perplexity is 119.17431163162833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.918595631917317 and perplexity of 136.810346127726
Finished 18 epochs...
Completing Train Step...
At time: 384.31852197647095 and batch: 50, loss is 4.848142137527466 and perplexity is 127.50328609079152
At time: 385.4514982700348 and batch: 100, loss is 4.800514039993286 and perplexity is 121.57289478947254
At time: 386.5844478607178 and batch: 150, loss is 4.794340162277222 and perplexity is 120.8246308253434
At time: 387.7141172885895 and batch: 200, loss is 4.817345848083496 and perplexity is 123.63650484793574
At time: 388.8469195365906 and batch: 250, loss is 4.815981531143189 and perplexity is 123.46794048368672
At time: 389.9875190258026 and batch: 300, loss is 4.7816531944274905 and perplexity is 119.30141554150579
At time: 391.1215407848358 and batch: 350, loss is 4.725609884262085 and perplexity is 112.79927190069857
At time: 392.28489422798157 and batch: 400, loss is 4.755851039886474 and perplexity is 116.26255514539665
At time: 393.4217963218689 and batch: 450, loss is 4.792992029190064 and perplexity is 120.66185289059749
At time: 394.5628471374512 and batch: 500, loss is 4.79553144454956 and perplexity is 120.96865283450819
At time: 395.6996223926544 and batch: 550, loss is 4.770369462966919 and perplexity is 117.96281680281182
At time: 396.83066964149475 and batch: 600, loss is 4.799041032791138 and perplexity is 121.39394886653396
At time: 397.9609463214874 and batch: 650, loss is 4.776827850341797 and perplexity is 118.72713183415189
At time: 399.0954225063324 and batch: 700, loss is 4.753053846359253 and perplexity is 115.93780069096248
At time: 400.2280912399292 and batch: 750, loss is 4.758802547454834 and perplexity is 116.60621186004471
At time: 401.3672626018524 and batch: 800, loss is 4.738995676040649 and perplexity is 114.31933036330558
At time: 402.5025267601013 and batch: 850, loss is 4.780466365814209 and perplexity is 119.1599091964358
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9184919993082685 and perplexity of 136.7961688492388
Finished 19 epochs...
Completing Train Step...
At time: 405.60601115226746 and batch: 50, loss is 4.842943563461303 and perplexity is 126.842170731845
At time: 406.76433086395264 and batch: 100, loss is 4.795127067565918 and perplexity is 120.9197457846688
At time: 407.89424896240234 and batch: 150, loss is 4.788936738967895 and perplexity is 120.1735248835058
At time: 409.0287621021271 and batch: 200, loss is 4.81232063293457 and perplexity is 123.01676328234846
At time: 410.1640908718109 and batch: 250, loss is 4.811318635940552 and perplexity is 122.89356258899565
At time: 411.31691575050354 and batch: 300, loss is 4.777362422943115 and perplexity is 118.79061707308637
At time: 412.4399473667145 and batch: 350, loss is 4.721530570983886 and perplexity is 112.34006559305338
At time: 413.56395053863525 and batch: 400, loss is 4.752460327148437 and perplexity is 115.86900979537106
At time: 414.6965637207031 and batch: 450, loss is 4.789662275314331 and perplexity is 120.26074678119043
At time: 415.8236954212189 and batch: 500, loss is 4.79259919166565 and perplexity is 120.61446169614595
At time: 416.9485533237457 and batch: 550, loss is 4.76788438796997 and perplexity is 117.67003430011961
At time: 418.1055865287781 and batch: 600, loss is 4.79713773727417 and perplexity is 121.16312004528172
At time: 419.2342963218689 and batch: 650, loss is 4.775420179367066 and perplexity is 118.56012067271631
At time: 420.420068025589 and batch: 700, loss is 4.751798315048218 and perplexity is 115.79232849362138
At time: 421.5455210208893 and batch: 750, loss is 4.758032875061035 and perplexity is 116.51649780747427
At time: 422.69990134239197 and batch: 800, loss is 4.7385755443573 and perplexity is 114.27131127847643
At time: 423.8227949142456 and batch: 850, loss is 4.779890909194946 and perplexity is 119.091357564075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.918383916219075 and perplexity of 136.78138429571425
Finished 20 epochs...
Completing Train Step...
At time: 426.88690662384033 and batch: 50, loss is 4.838082628250122 and perplexity is 126.22709528992698
At time: 428.0508596897125 and batch: 100, loss is 4.7904082298278805 and perplexity is 120.35048929581016
At time: 429.18299198150635 and batch: 150, loss is 4.784342279434204 and perplexity is 119.62265892111535
At time: 430.3191840648651 and batch: 200, loss is 4.808139715194702 and perplexity is 122.50351398861478
At time: 431.4479236602783 and batch: 250, loss is 4.807282218933105 and perplexity is 122.39851270887343
At time: 432.57667541503906 and batch: 300, loss is 4.773554019927978 and perplexity is 118.33907490189966
At time: 433.7104856967926 and batch: 350, loss is 4.7179568481445315 and perplexity is 111.93930985635421
At time: 434.8434727191925 and batch: 400, loss is 4.749524526596069 and perplexity is 115.52934033726822
At time: 435.9735288619995 and batch: 450, loss is 4.78687741279602 and perplexity is 119.92630304112053
At time: 437.1071026325226 and batch: 500, loss is 4.790023260116577 and perplexity is 120.3041669196187
At time: 438.2360441684723 and batch: 550, loss is 4.765604372024536 and perplexity is 117.4020503655298
At time: 439.3656413555145 and batch: 600, loss is 4.795187873840332 and perplexity is 120.92709868746209
At time: 440.5012927055359 and batch: 650, loss is 4.7739561080932615 and perplexity is 118.3866672109192
At time: 441.63269805908203 and batch: 700, loss is 4.750301189422608 and perplexity is 115.61910253425144
At time: 442.7624866962433 and batch: 750, loss is 4.756717281341553 and perplexity is 116.36331022314918
At time: 443.893269777298 and batch: 800, loss is 4.737589664459229 and perplexity is 114.15870900504011
At time: 445.0254862308502 and batch: 850, loss is 4.778922615051269 and perplexity is 118.97609791161199
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.918209711710612 and perplexity of 136.75755843724238
Finished 21 epochs...
Completing Train Step...
At time: 448.12920451164246 and batch: 50, loss is 4.833898830413818 and perplexity is 125.7000898526065
At time: 449.2939484119415 and batch: 100, loss is 4.786209516525268 and perplexity is 119.84623145328833
At time: 450.42958903312683 and batch: 150, loss is 4.780397615432739 and perplexity is 119.15171718882772
At time: 451.5679211616516 and batch: 200, loss is 4.804263982772827 and perplexity is 122.02964204127154
At time: 452.7070996761322 and batch: 250, loss is 4.80332010269165 and perplexity is 121.9145150344272
At time: 453.8437235355377 and batch: 300, loss is 4.7701625156402585 and perplexity is 117.93840723906327
At time: 454.99866676330566 and batch: 350, loss is 4.714729557037353 and perplexity is 111.57863143728936
At time: 456.1471321582794 and batch: 400, loss is 4.746799879074096 and perplexity is 115.21499204501994
At time: 457.2763321399689 and batch: 450, loss is 4.7842628860473635 and perplexity is 119.61316205008005
At time: 458.39932584762573 and batch: 500, loss is 4.787468166351318 and perplexity is 119.99717086166328
At time: 459.52618384361267 and batch: 550, loss is 4.7631791305541995 and perplexity is 117.11766703279716
At time: 460.65218710899353 and batch: 600, loss is 4.793093013763428 and perplexity is 120.67403849160145
At time: 461.7793891429901 and batch: 650, loss is 4.772371015548706 and perplexity is 118.19916203313727
At time: 462.90436363220215 and batch: 700, loss is 4.748919296264648 and perplexity is 115.45943963147448
At time: 464.03970193862915 and batch: 750, loss is 4.755572528839111 and perplexity is 116.23017924812578
At time: 465.1827235221863 and batch: 800, loss is 4.736668558120727 and perplexity is 114.05360510794449
At time: 466.32011246681213 and batch: 850, loss is 4.777812232971192 and perplexity is 118.84406230308629
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.918043772379558 and perplexity of 136.73486686224308
Finished 22 epochs...
Completing Train Step...
At time: 469.45279479026794 and batch: 50, loss is 4.83005500793457 and perplexity is 125.21784844053606
At time: 470.5905120372772 and batch: 100, loss is 4.7825095653533936 and perplexity is 119.40362556379384
At time: 471.72409868240356 and batch: 150, loss is 4.7767628479003905 and perplexity is 118.71941453154597
At time: 472.86209511756897 and batch: 200, loss is 4.800928926467895 and perplexity is 121.62334420386598
At time: 474.0007963180542 and batch: 250, loss is 4.799750003814697 and perplexity is 121.48004417464894
At time: 475.1391224861145 and batch: 300, loss is 4.766878709793091 and perplexity is 117.55175559967407
At time: 476.2769546508789 and batch: 350, loss is 4.711771039962769 and perplexity is 111.24901198383897
At time: 477.42818570137024 and batch: 400, loss is 4.744061908721924 and perplexity is 114.89996827234725
At time: 478.61070942878723 and batch: 450, loss is 4.781878995895386 and perplexity is 119.328357017855
At time: 479.73205757141113 and batch: 500, loss is 4.785194797515869 and perplexity is 119.72468288327951
At time: 480.85486125946045 and batch: 550, loss is 4.760934066772461 and perplexity is 116.85502533433883
At time: 481.99077129364014 and batch: 600, loss is 4.791236886978149 and perplexity is 120.45025992141548
At time: 483.12651348114014 and batch: 650, loss is 4.7708823680877686 and perplexity is 118.02333605461018
At time: 484.2799232006073 and batch: 700, loss is 4.747353925704956 and perplexity is 115.27884421009206
At time: 485.40792298316956 and batch: 750, loss is 4.754027156829834 and perplexity is 116.05069910000023
At time: 486.55427980422974 and batch: 800, loss is 4.735230093002319 and perplexity is 113.88966091763679
At time: 487.69845175743103 and batch: 850, loss is 4.776563196182251 and perplexity is 118.69571436242671
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.917915026346843 and perplexity of 136.71726392377934
Finished 23 epochs...
Completing Train Step...
At time: 490.8759081363678 and batch: 50, loss is 4.826187744140625 and perplexity is 124.73453314513861
At time: 492.0037188529968 and batch: 100, loss is 4.7788911819458 and perplexity is 118.97235818215404
At time: 493.12681436538696 and batch: 150, loss is 4.77329270362854 and perplexity is 118.3081550128788
At time: 494.25654435157776 and batch: 200, loss is 4.797758474349975 and perplexity is 121.23835383389932
At time: 495.38167691230774 and batch: 250, loss is 4.796392488479614 and perplexity is 121.07285701450841
At time: 496.52735805511475 and batch: 300, loss is 4.763821315765381 and perplexity is 117.19290242147508
At time: 497.65249943733215 and batch: 350, loss is 4.7089142417907714 and perplexity is 110.93164954582411
At time: 498.77516531944275 and batch: 400, loss is 4.741573781967163 and perplexity is 114.61443795228642
At time: 499.91084241867065 and batch: 450, loss is 4.779658050537109 and perplexity is 119.0636293388966
At time: 501.040691614151 and batch: 500, loss is 4.782828350067138 and perplexity is 119.44169568215275
At time: 502.1759445667267 and batch: 550, loss is 4.758701810836792 and perplexity is 116.59446593625151
At time: 503.3187279701233 and batch: 600, loss is 4.7891614055633545 and perplexity is 120.20052689331693
At time: 504.447208404541 and batch: 650, loss is 4.769323034286499 and perplexity is 117.83944169097211
At time: 505.58402252197266 and batch: 700, loss is 4.745781202316284 and perplexity is 115.09768496961189
At time: 506.7176446914673 and batch: 750, loss is 4.752536344528198 and perplexity is 115.87781818868241
At time: 507.9111566543579 and batch: 800, loss is 4.73387113571167 and perplexity is 113.73499484869686
At time: 509.0430178642273 and batch: 850, loss is 4.775277462005615 and perplexity is 118.543201292494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.917847951253255 and perplexity of 136.70809390804985
Finished 24 epochs...
Completing Train Step...
At time: 512.3360562324524 and batch: 50, loss is 4.82299469947815 and perplexity is 124.3368854012089
At time: 513.4607677459717 and batch: 100, loss is 4.77562442779541 and perplexity is 118.58433886420764
At time: 514.5965008735657 and batch: 150, loss is 4.770180864334106 and perplexity is 117.94057127464416
At time: 515.7197563648224 and batch: 200, loss is 4.794942741394043 and perplexity is 120.89745916489095
At time: 516.858677148819 and batch: 250, loss is 4.793417854309082 and perplexity is 120.7132446796464
At time: 517.993456363678 and batch: 300, loss is 4.760908317565918 and perplexity is 116.85201644889429
At time: 519.1156070232391 and batch: 350, loss is 4.706228761672974 and perplexity is 110.63414445733002
At time: 520.2432553768158 and batch: 400, loss is 4.739218587875366 and perplexity is 114.34481633543736
At time: 521.3661379814148 and batch: 450, loss is 4.777645502090454 and perplexity is 118.82424897970041
At time: 522.4903717041016 and batch: 500, loss is 4.780698528289795 and perplexity is 119.18757686751873
At time: 523.6160705089569 and batch: 550, loss is 4.7565617275238035 and perplexity is 116.34521087374597
At time: 524.7394251823425 and batch: 600, loss is 4.787221879959106 and perplexity is 119.96762083041078
At time: 525.8640127182007 and batch: 650, loss is 4.767676839828491 and perplexity is 117.64561463740827
At time: 526.9895353317261 and batch: 700, loss is 4.74410454750061 and perplexity is 114.9048675711148
At time: 528.112233877182 and batch: 750, loss is 4.751030721664429 and perplexity is 115.70348117204666
At time: 529.2395203113556 and batch: 800, loss is 4.7321697616577145 and perplexity is 113.54165359896577
At time: 530.3796484470367 and batch: 850, loss is 4.77375563621521 and perplexity is 118.36293639216753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.917697906494141 and perplexity of 136.68758311383752
Finished 25 epochs...
Completing Train Step...
At time: 533.4559397697449 and batch: 50, loss is 4.8197916316986085 and perplexity is 123.93926307535718
At time: 534.6210100650787 and batch: 100, loss is 4.772516851425171 and perplexity is 118.21640096852684
At time: 535.7506604194641 and batch: 150, loss is 4.767023315429688 and perplexity is 117.5687554752349
At time: 536.9109027385712 and batch: 200, loss is 4.792100124359131 and perplexity is 120.55428197973357
At time: 538.0370886325836 and batch: 250, loss is 4.790412273406982 and perplexity is 120.35097594351747
At time: 539.1658847332001 and batch: 300, loss is 4.758019943237304 and perplexity is 116.51499104640548
At time: 540.29145860672 and batch: 350, loss is 4.703555374145508 and perplexity is 110.33877151451578
At time: 541.4161598682404 and batch: 400, loss is 4.736961727142334 and perplexity is 114.08704699358762
At time: 542.5434684753418 and batch: 450, loss is 4.775524883270264 and perplexity is 118.57253503001888
At time: 543.6701099872589 and batch: 500, loss is 4.778536205291748 and perplexity is 118.93013326739467
At time: 544.7947297096252 and batch: 550, loss is 4.754487085342407 and perplexity is 116.10408640165033
At time: 545.9228749275208 and batch: 600, loss is 4.785333089828491 and perplexity is 119.74124103145928
At time: 547.048742055893 and batch: 650, loss is 4.765981035232544 and perplexity is 117.44627972771097
At time: 548.1850357055664 and batch: 700, loss is 4.742476329803467 and perplexity is 114.71792966135324
At time: 549.3436622619629 and batch: 750, loss is 4.749317855834961 and perplexity is 115.50546626769142
At time: 550.4783670902252 and batch: 800, loss is 4.730738887786865 and perplexity is 113.37930599075246
At time: 551.6076169013977 and batch: 850, loss is 4.772245483398438 and perplexity is 118.1843251694385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.917692502339681 and perplexity of 136.68684443502158
Finished 26 epochs...
Completing Train Step...
At time: 554.7147433757782 and batch: 50, loss is 4.816725959777832 and perplexity is 123.55988777389484
At time: 555.8655591011047 and batch: 100, loss is 4.769571132659912 and perplexity is 117.86868109175349
At time: 556.9881982803345 and batch: 150, loss is 4.764154052734375 and perplexity is 117.23190332077505
At time: 558.114862203598 and batch: 200, loss is 4.789435586929321 and perplexity is 120.23348815644606
At time: 559.2608675956726 and batch: 250, loss is 4.7874722576141355 and perplexity is 119.9976618026309
At time: 560.3855228424072 and batch: 300, loss is 4.755333652496338 and perplexity is 116.20241792388096
At time: 561.5087220668793 and batch: 350, loss is 4.70100507736206 and perplexity is 110.05773341803243
At time: 562.6332545280457 and batch: 400, loss is 4.734720811843872 and perplexity is 113.83167382630938
At time: 563.7594146728516 and batch: 450, loss is 4.773446283340454 and perplexity is 118.32632614056533
At time: 564.9114980697632 and batch: 500, loss is 4.776332111358642 and perplexity is 118.66828875315322
At time: 566.0393998622894 and batch: 550, loss is 4.752355918884278 and perplexity is 115.85691274471543
At time: 567.1624286174774 and batch: 600, loss is 4.783252019882202 and perplexity is 119.49231024444731
At time: 568.2913641929626 and batch: 650, loss is 4.764273920059204 and perplexity is 117.24595643764872
At time: 569.4103252887726 and batch: 700, loss is 4.740881748199463 and perplexity is 114.53514832967005
At time: 570.5336179733276 and batch: 750, loss is 4.7476387786865235 and perplexity is 115.31168640995419
At time: 571.6820602416992 and batch: 800, loss is 4.72920823097229 and perplexity is 113.2058939344362
At time: 572.8073506355286 and batch: 850, loss is 4.770456590652466 and perplexity is 117.97309507777511
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.917645454406738 and perplexity of 136.68041375280683
Finished 27 epochs...
Completing Train Step...
At time: 575.9248752593994 and batch: 50, loss is 4.813712911605835 and perplexity is 123.18815618372878
At time: 577.0750436782837 and batch: 100, loss is 4.7665953445434575 and perplexity is 117.51845023611789
At time: 578.1995959281921 and batch: 150, loss is 4.761423635482788 and perplexity is 116.91224790443398
At time: 579.3270878791809 and batch: 200, loss is 4.786798315048218 and perplexity is 119.91681751579452
At time: 580.4540429115295 and batch: 250, loss is 4.784776201248169 and perplexity is 119.67457706565878
At time: 581.5783588886261 and batch: 300, loss is 4.752671327590942 and perplexity is 115.89346078720482
At time: 582.7137503623962 and batch: 350, loss is 4.698414211273193 and perplexity is 109.77295763595895
At time: 583.8425288200378 and batch: 400, loss is 4.732429828643799 and perplexity is 113.57118587463077
At time: 584.9662997722626 and batch: 450, loss is 4.7712709426879885 and perplexity is 118.06920583657316
At time: 586.0930316448212 and batch: 500, loss is 4.774062433242798 and perplexity is 118.39925536022008
At time: 587.2190515995026 and batch: 550, loss is 4.750317192077636 and perplexity is 115.62095276166825
At time: 588.3439292907715 and batch: 600, loss is 4.781490201950073 and perplexity is 119.28197189285459
At time: 589.4705066680908 and batch: 650, loss is 4.762692728042603 and perplexity is 117.06071455743641
At time: 590.5961918830872 and batch: 700, loss is 4.73911208152771 and perplexity is 114.33263853519409
At time: 591.7230792045593 and batch: 750, loss is 4.745995960235596 and perplexity is 115.12240576335168
At time: 592.8521447181702 and batch: 800, loss is 4.727686290740967 and perplexity is 113.03373237306924
At time: 594.0070531368256 and batch: 850, loss is 4.768771953582764 and perplexity is 117.77452053855748
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.917459170023601 and perplexity of 136.6549546976297
Finished 28 epochs...
Completing Train Step...
At time: 597.0843646526337 and batch: 50, loss is 4.810795087814331 and perplexity is 122.8292387343636
At time: 598.2327332496643 and batch: 100, loss is 4.763912296295166 and perplexity is 117.20356517886869
At time: 599.3580222129822 and batch: 150, loss is 4.758736152648925 and perplexity is 116.59847007025071
At time: 600.4841537475586 and batch: 200, loss is 4.784256467819214 and perplexity is 119.61239434797994
At time: 601.6105995178223 and batch: 250, loss is 4.782101745605469 and perplexity is 119.35494033538866
At time: 602.7360551357269 and batch: 300, loss is 4.750288143157959 and perplexity is 115.61759414668073
At time: 603.8634057044983 and batch: 350, loss is 4.696020383834838 and perplexity is 109.51049438917663
At time: 604.9873676300049 and batch: 400, loss is 4.730200204849243 and perplexity is 113.31824694034667
At time: 606.1113328933716 and batch: 450, loss is 4.76915488243103 and perplexity is 117.81962843606881
At time: 607.2361347675323 and batch: 500, loss is 4.771780805587769 and perplexity is 118.1294202934901
At time: 608.3616719245911 and batch: 550, loss is 4.748242902755737 and perplexity is 115.38137002183029
At time: 609.4850594997406 and batch: 600, loss is 4.779380645751953 and perplexity is 119.03060509913279
At time: 610.6130270957947 and batch: 650, loss is 4.760840702056885 and perplexity is 116.84411570742984
At time: 611.737886428833 and batch: 700, loss is 4.737316389083862 and perplexity is 114.12751650325079
At time: 612.8678064346313 and batch: 750, loss is 4.744111633300781 and perplexity is 114.90568176692966
At time: 614.0006973743439 and batch: 800, loss is 4.7259852409362795 and perplexity is 112.84161980753827
At time: 615.1304671764374 and batch: 850, loss is 4.767292871475219 and perplexity is 117.60045111572272
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.917614936828613 and perplexity of 136.6762426612481
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 618.2488269805908 and batch: 50, loss is 4.810635690689087 and perplexity is 122.80966166711902
At time: 619.3703207969666 and batch: 100, loss is 4.765452928543091 and perplexity is 117.38427193654123
At time: 620.5029790401459 and batch: 150, loss is 4.758533344268799 and perplexity is 116.57482532116825
At time: 621.640950679779 and batch: 200, loss is 4.784015779495239 and perplexity is 119.58360850560479
At time: 622.8199555873871 and batch: 250, loss is 4.77780740737915 and perplexity is 118.84348881150878
At time: 623.940557718277 and batch: 300, loss is 4.74462854385376 and perplexity is 114.96509308027677
At time: 625.065046787262 and batch: 350, loss is 4.689529485702515 and perplexity is 108.8019748772604
At time: 626.185218334198 and batch: 400, loss is 4.721841287612915 and perplexity is 112.37497694302695
At time: 627.3083970546722 and batch: 450, loss is 4.759285202026367 and perplexity is 116.66250596547903
At time: 628.4280843734741 and batch: 500, loss is 4.760485849380493 and perplexity is 116.80266061591055
At time: 629.548623085022 and batch: 550, loss is 4.734510478973388 and perplexity is 113.80773380137683
At time: 630.6743512153625 and batch: 600, loss is 4.761813192367554 and perplexity is 116.9578007476544
At time: 631.7971079349518 and batch: 650, loss is 4.74096562385559 and perplexity is 114.54475544328169
At time: 632.9222931861877 and batch: 700, loss is 4.716766681671142 and perplexity is 111.80616269207123
At time: 634.0436227321625 and batch: 750, loss is 4.72083722114563 and perplexity is 112.26220162334947
At time: 635.196302652359 and batch: 800, loss is 4.700539493560791 and perplexity is 110.00650424681017
At time: 636.3218257427216 and batch: 850, loss is 4.746207246780395 and perplexity is 115.14673214852287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.915698051452637 and perplexity of 136.4145009150565
Finished 30 epochs...
Completing Train Step...
At time: 639.3625762462616 and batch: 50, loss is 4.807151355743408 and perplexity is 122.3824962970884
At time: 640.4957106113434 and batch: 100, loss is 4.761108121871948 and perplexity is 116.87536631757355
At time: 641.6278147697449 and batch: 150, loss is 4.754578065872193 and perplexity is 116.11465009347927
At time: 642.7583861351013 and batch: 200, loss is 4.780509128570556 and perplexity is 119.1650049115518
At time: 643.8896763324738 and batch: 250, loss is 4.774889011383056 and perplexity is 118.49716205471601
At time: 645.0265426635742 and batch: 300, loss is 4.741959409713745 and perplexity is 114.65864498290074
At time: 646.1569919586182 and batch: 350, loss is 4.687044353485107 and perplexity is 108.53192328012143
At time: 647.285320520401 and batch: 400, loss is 4.719709730148315 and perplexity is 112.13569833074344
At time: 648.4173533916473 and batch: 450, loss is 4.757641115188599 and perplexity is 116.47086025922019
At time: 649.5438313484192 and batch: 500, loss is 4.7591258144378665 and perplexity is 116.64391289177668
At time: 650.6778485774994 and batch: 550, loss is 4.7335637760162355 and perplexity is 113.70004266704102
At time: 651.8767766952515 and batch: 600, loss is 4.761237373352051 and perplexity is 116.89047360795644
At time: 653.0027897357941 and batch: 650, loss is 4.740645055770874 and perplexity is 114.50804193531877
At time: 654.1343603134155 and batch: 700, loss is 4.716872749328613 and perplexity is 111.81802233879009
At time: 655.2679243087769 and batch: 750, loss is 4.721663446426391 and perplexity is 112.35499382075557
At time: 656.393532037735 and batch: 800, loss is 4.701871795654297 and perplexity is 110.15316381845082
At time: 657.5292143821716 and batch: 850, loss is 4.747728328704834 and perplexity is 115.32201303595154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9154923756917315 and perplexity of 136.38644664392297
Finished 31 epochs...
Completing Train Step...
At time: 660.5353434085846 and batch: 50, loss is 4.8053751468658445 and perplexity is 122.1653123598256
At time: 661.6817636489868 and batch: 100, loss is 4.759227876663208 and perplexity is 116.6558184366418
At time: 662.8078830242157 and batch: 150, loss is 4.752563848495483 and perplexity is 115.88100533223228
At time: 663.9340088367462 and batch: 200, loss is 4.778830728530884 and perplexity is 118.96516611421596
At time: 665.0574276447296 and batch: 250, loss is 4.773279781341553 and perplexity is 118.30662621082463
At time: 666.1802067756653 and batch: 300, loss is 4.740387678146362 and perplexity is 114.47857391986457
At time: 667.3019371032715 and batch: 350, loss is 4.6857272720336915 and perplexity is 108.38907199112464
At time: 668.4274001121521 and batch: 400, loss is 4.718523931503296 and perplexity is 112.00280677848765
At time: 669.5576705932617 and batch: 450, loss is 4.756633892059326 and perplexity is 116.35360717480286
At time: 670.6950330734253 and batch: 500, loss is 4.758316688537597 and perplexity is 116.549571452945
At time: 671.8283586502075 and batch: 550, loss is 4.7331694221496585 and perplexity is 113.65521345545274
At time: 672.9648208618164 and batch: 600, loss is 4.760986394882202 and perplexity is 116.86114029691917
At time: 674.1052005290985 and batch: 650, loss is 4.740676174163818 and perplexity is 114.51160529700576
At time: 675.256144285202 and batch: 700, loss is 4.71714490890503 and perplexity is 111.84845882598843
At time: 676.3956155776978 and batch: 750, loss is 4.722411947250366 and perplexity is 112.43912310770058
At time: 677.5316920280457 and batch: 800, loss is 4.702810134887695 and perplexity is 110.25657336277341
At time: 678.6580212116241 and batch: 850, loss is 4.748667125701904 and perplexity is 115.4303278303324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.915432929992676 and perplexity of 136.37833929723632
Finished 32 epochs...
Completing Train Step...
At time: 681.6489417552948 and batch: 50, loss is 4.804015455245971 and perplexity is 121.99931808445697
At time: 682.812816619873 and batch: 100, loss is 4.757824392318725 and perplexity is 116.492208660509
At time: 683.9354665279388 and batch: 150, loss is 4.751058359146118 and perplexity is 115.70667896907824
At time: 685.0818319320679 and batch: 200, loss is 4.7776267623901365 and perplexity is 118.82202226974806
At time: 686.2048373222351 and batch: 250, loss is 4.772048006057739 and perplexity is 118.1609887474749
At time: 687.3257901668549 and batch: 300, loss is 4.739188117980957 and perplexity is 114.3413323140365
At time: 688.4491498470306 and batch: 350, loss is 4.684769210815429 and perplexity is 108.28527835303537
At time: 689.572555065155 and batch: 400, loss is 4.717676849365234 and perplexity is 111.90797137381222
At time: 690.6939923763275 and batch: 450, loss is 4.7558830451965335 and perplexity is 116.26627622406916
At time: 691.8162372112274 and batch: 500, loss is 4.757768125534057 and perplexity is 116.48565420288965
At time: 692.9410815238953 and batch: 550, loss is 4.732922010421753 and perplexity is 113.62709730098283
At time: 694.0624303817749 and batch: 600, loss is 4.760815296173096 and perplexity is 116.84114721711349
At time: 695.1858768463135 and batch: 650, loss is 4.740714454650879 and perplexity is 114.51598894093415
At time: 696.3094053268433 and batch: 700, loss is 4.71739351272583 and perplexity is 111.87626823682288
At time: 697.4317133426666 and batch: 750, loss is 4.722988805770874 and perplexity is 112.50400328544619
At time: 698.5543167591095 and batch: 800, loss is 4.703452749252319 and perplexity is 110.32744859089138
At time: 699.6784856319427 and batch: 850, loss is 4.7492936420440675 and perplexity is 115.50266947634478
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.91539986928304 and perplexity of 136.3738306070909
Finished 33 epochs...
Completing Train Step...
At time: 702.7123737335205 and batch: 50, loss is 4.802885999679566 and perplexity is 121.86160306168823
At time: 703.8337659835815 and batch: 100, loss is 4.756653747558594 and perplexity is 116.35591745670075
At time: 704.953118801117 and batch: 150, loss is 4.749829607009888 and perplexity is 115.56459145316562
At time: 706.0742123126984 and batch: 200, loss is 4.776652545928955 and perplexity is 118.70632026825024
At time: 707.1935975551605 and batch: 250, loss is 4.771032552719117 and perplexity is 118.04106267692582
At time: 708.3126094341278 and batch: 300, loss is 4.738206071853638 and perplexity is 114.22909896962554
At time: 709.4776794910431 and batch: 350, loss is 4.683986988067627 and perplexity is 108.2006082648073
At time: 710.5978534221649 and batch: 400, loss is 4.716984548568726 and perplexity is 111.83052420755516
At time: 711.7180840969086 and batch: 450, loss is 4.755245141983032 and perplexity is 116.19213324338098
At time: 712.8391189575195 and batch: 500, loss is 4.757336149215698 and perplexity is 116.43534602559832
At time: 713.9579586982727 and batch: 550, loss is 4.732703847885132 and perplexity is 113.60231082904488
At time: 715.0842225551605 and batch: 600, loss is 4.760611085891724 and perplexity is 116.817289489644
At time: 716.2039518356323 and batch: 650, loss is 4.7407710742950435 and perplexity is 114.52247297903934
At time: 717.3236682415009 and batch: 700, loss is 4.717572698593139 and perplexity is 111.89631667912334
At time: 718.4462399482727 and batch: 750, loss is 4.723435049057007 and perplexity is 112.55421864487559
At time: 719.5670335292816 and batch: 800, loss is 4.703916711807251 and perplexity is 110.37864827226775
At time: 720.688006401062 and batch: 850, loss is 4.7497524070739745 and perplexity is 115.5556702184754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.915385564168294 and perplexity of 136.37187977774914
Finished 34 epochs...
Completing Train Step...
At time: 723.7136216163635 and batch: 50, loss is 4.801912240982055 and perplexity is 121.74299702201763
At time: 724.8336458206177 and batch: 100, loss is 4.755644092559814 and perplexity is 116.23849740984652
At time: 725.9575145244598 and batch: 150, loss is 4.74874282836914 and perplexity is 115.43906654479693
At time: 727.0791029930115 and batch: 200, loss is 4.775808401107788 and perplexity is 118.60615722476115
At time: 728.2009375095367 and batch: 250, loss is 4.770196256637573 and perplexity is 117.9423866656797
At time: 729.3226916790009 and batch: 300, loss is 4.737372961044311 and perplexity is 114.13397310323015
At time: 730.4461138248444 and batch: 350, loss is 4.683318071365356 and perplexity is 108.1282552725035
At time: 731.5678055286407 and batch: 400, loss is 4.71638970375061 and perplexity is 111.76402218087706
At time: 732.6887261867523 and batch: 450, loss is 4.754675378799439 and perplexity is 116.12595009978546
At time: 733.8109328746796 and batch: 500, loss is 4.756971998214722 and perplexity is 116.39295369686698
At time: 734.9337000846863 and batch: 550, loss is 4.732530078887939 and perplexity is 113.58257198446266
At time: 736.0558235645294 and batch: 600, loss is 4.760455951690674 and perplexity is 116.79916853839622
At time: 737.2088558673859 and batch: 650, loss is 4.740792608261108 and perplexity is 114.52493912863899
At time: 738.3301708698273 and batch: 700, loss is 4.717718267440796 and perplexity is 111.91260648261456
At time: 739.452107667923 and batch: 750, loss is 4.723779392242432 and perplexity is 112.5929825967251
At time: 740.5742237567902 and batch: 800, loss is 4.704251308441162 and perplexity is 110.41558677584013
At time: 741.6974861621857 and batch: 850, loss is 4.750108070373535 and perplexity is 115.59677643898196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.915397008260091 and perplexity of 136.37344043898997
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 744.715430021286 and batch: 50, loss is 4.80155535697937 and perplexity is 121.69955664599021
At time: 745.8371813297272 and batch: 100, loss is 4.755198669433594 and perplexity is 116.18673362419253
At time: 746.9606544971466 and batch: 150, loss is 4.7486337184906 and perplexity is 115.4264716893915
At time: 748.082001209259 and batch: 200, loss is 4.775460615158081 and perplexity is 118.5649148419059
At time: 749.2036082744598 and batch: 250, loss is 4.769116764068603 and perplexity is 117.81513743036656
At time: 750.3262295722961 and batch: 300, loss is 4.736084280014038 and perplexity is 113.98698554756143
At time: 751.4475603103638 and batch: 350, loss is 4.682369108200073 and perplexity is 108.02569421215445
At time: 752.5701773166656 and batch: 400, loss is 4.714321613311768 and perplexity is 111.53312291777368
At time: 753.6914217472076 and batch: 450, loss is 4.752917652130127 and perplexity is 115.92201170677941
At time: 754.811350107193 and batch: 500, loss is 4.755354614257812 and perplexity is 116.20485375677778
At time: 755.9355289936066 and batch: 550, loss is 4.73031979560852 and perplexity is 113.33179956590642
At time: 757.0570802688599 and batch: 600, loss is 4.7576918792724605 and perplexity is 116.47677294581264
At time: 758.1772186756134 and batch: 650, loss is 4.737532625198364 and perplexity is 114.15219766236004
At time: 759.2987682819366 and batch: 700, loss is 4.7142949962615965 and perplexity is 111.5301542745537
At time: 760.4205052852631 and batch: 750, loss is 4.720177850723267 and perplexity is 112.18820364676915
At time: 761.5423240661621 and batch: 800, loss is 4.700381517410278 and perplexity is 109.98912721535227
At time: 762.6633155345917 and batch: 850, loss is 4.746655235290527 and perplexity is 115.19832811784244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.914770762125651 and perplexity of 136.28806383524918
Finished 36 epochs...
Completing Train Step...
At time: 765.6669676303864 and batch: 50, loss is 4.801244144439697 and perplexity is 121.6616881107765
At time: 766.8113956451416 and batch: 100, loss is 4.754700307846069 and perplexity is 116.12884504509448
At time: 767.9291319847107 and batch: 150, loss is 4.748187808990479 and perplexity is 115.37501340282176
At time: 769.0491752624512 and batch: 200, loss is 4.774983386993409 and perplexity is 118.5083458244392
At time: 770.1689629554749 and batch: 250, loss is 4.768798303604126 and perplexity is 117.77762394057676
At time: 771.2883450984955 and batch: 300, loss is 4.735832271575927 and perplexity is 113.9582634846218
At time: 772.4085223674774 and batch: 350, loss is 4.682114353179932 and perplexity is 107.99817762939225
At time: 773.5268001556396 and batch: 400, loss is 4.714186458587647 and perplexity is 111.5180497079461
At time: 774.6634495258331 and batch: 450, loss is 4.752892999649048 and perplexity is 115.91915397680442
At time: 775.7985072135925 and batch: 500, loss is 4.7553338527679445 and perplexity is 116.20244119592819
At time: 776.9163451194763 and batch: 550, loss is 4.7302712059021 and perplexity is 113.32629294082118
At time: 778.0362071990967 and batch: 600, loss is 4.757675399780274 and perplexity is 116.47485348355886
At time: 779.1541776657104 and batch: 650, loss is 4.737476739883423 and perplexity is 114.14581840909742
At time: 780.2730903625488 and batch: 700, loss is 4.714279747009277 and perplexity is 111.52845353605746
At time: 781.390679359436 and batch: 750, loss is 4.720293045043945 and perplexity is 112.20112783505867
At time: 782.5098340511322 and batch: 800, loss is 4.700574312210083 and perplexity is 110.01033459138468
At time: 783.6287562847137 and batch: 850, loss is 4.746787385940552 and perplexity is 115.21355265772924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.914698600769043 and perplexity of 136.27822945850852
Finished 37 epochs...
Completing Train Step...
At time: 786.6108858585358 and batch: 50, loss is 4.800998153686524 and perplexity is 121.6317641411466
At time: 787.7580714225769 and batch: 100, loss is 4.754425554275513 and perplexity is 116.096942613128
At time: 788.8785922527313 and batch: 150, loss is 4.74791838645935 and perplexity is 115.34393296175534
At time: 790.0002539157867 and batch: 200, loss is 4.774703493118286 and perplexity is 118.47518070586537
At time: 791.1244795322418 and batch: 250, loss is 4.7685647964477536 and perplexity is 117.75012523322351
At time: 792.2453918457031 and batch: 300, loss is 4.7356358242034915 and perplexity is 113.93587888196306
At time: 793.3659024238586 and batch: 350, loss is 4.681948175430298 and perplexity is 107.98023222637389
At time: 794.526563167572 and batch: 400, loss is 4.714100341796875 and perplexity is 111.50844654489491
At time: 795.6470019817352 and batch: 450, loss is 4.752925767898559 and perplexity is 115.9229525068003
At time: 796.767923116684 and batch: 500, loss is 4.755377273559571 and perplexity is 116.20748690745742
At time: 797.889271736145 and batch: 550, loss is 4.730276565551758 and perplexity is 113.32690033167611
At time: 799.0101218223572 and batch: 600, loss is 4.757679128646851 and perplexity is 116.47528780355687
At time: 800.1301312446594 and batch: 650, loss is 4.7374819469451905 and perplexity is 114.14641277497184
At time: 801.251505613327 and batch: 700, loss is 4.714286985397339 and perplexity is 111.52926082520585
At time: 802.3740441799164 and batch: 750, loss is 4.720388135910034 and perplexity is 112.21179764477333
At time: 803.4958140850067 and batch: 800, loss is 4.700734920501709 and perplexity is 110.02800458222016
At time: 804.6256597042084 and batch: 850, loss is 4.7468666744232175 and perplexity is 115.22268812766598
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.914675076802571 and perplexity of 136.27502369171427
Finished 38 epochs...
Completing Train Step...
At time: 807.8286130428314 and batch: 50, loss is 4.800775680541992 and perplexity is 121.60470734991951
At time: 809.0085852146149 and batch: 100, loss is 4.754204053878784 and perplexity is 116.0712299420687
At time: 810.1450476646423 and batch: 150, loss is 4.747685317993164 and perplexity is 115.31705306076638
At time: 811.286378622055 and batch: 200, loss is 4.77447512626648 and perplexity is 118.44812799091959
At time: 812.4244241714478 and batch: 250, loss is 4.768358135223389 and perplexity is 117.72579336248744
At time: 813.5619072914124 and batch: 300, loss is 4.7354632759094235 and perplexity is 113.91622113643285
At time: 814.7031238079071 and batch: 350, loss is 4.681805124282837 and perplexity is 107.96478663503193
At time: 815.8445312976837 and batch: 400, loss is 4.7140286254882815 and perplexity is 111.50044985748154
At time: 816.9996955394745 and batch: 450, loss is 4.752963323593139 and perplexity is 115.9273061555511
At time: 818.1222257614136 and batch: 500, loss is 4.755432147979736 and perplexity is 116.21386390088563
At time: 819.2420520782471 and batch: 550, loss is 4.730290355682373 and perplexity is 113.32846313520952
At time: 820.3632094860077 and batch: 600, loss is 4.757686405181885 and perplexity is 116.47613534315275
At time: 821.4845626354218 and batch: 650, loss is 4.737496194839477 and perplexity is 114.1480391325803
At time: 822.604926109314 and batch: 700, loss is 4.714296398162841 and perplexity is 111.5303106289254
At time: 823.7558381557465 and batch: 750, loss is 4.720464496612549 and perplexity is 112.22036654363134
At time: 824.8772795200348 and batch: 800, loss is 4.700868339538574 and perplexity is 110.04268539194763
At time: 825.9986717700958 and batch: 850, loss is 4.746924295425415 and perplexity is 115.22932756571545
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.914666493733724 and perplexity of 136.27385403882332
Finished 39 epochs...
Completing Train Step...
At time: 829.0451657772064 and batch: 50, loss is 4.800571813583374 and perplexity is 121.57991869495808
At time: 830.1691899299622 and batch: 100, loss is 4.754007587432861 and perplexity is 116.04842808002185
At time: 831.2900288105011 and batch: 150, loss is 4.74747254371643 and perplexity is 115.29251916838948
At time: 832.4115109443665 and batch: 200, loss is 4.774272727966308 and perplexity is 118.42415671711012
At time: 833.5347425937653 and batch: 250, loss is 4.7681669330596925 and perplexity is 117.70328608786201
At time: 834.67400431633 and batch: 300, loss is 4.735304069519043 and perplexity is 113.89808638968204
At time: 835.7978315353394 and batch: 350, loss is 4.681674900054932 and perplexity is 107.950727919464
At time: 836.9196963310242 and batch: 400, loss is 4.71396388053894 and perplexity is 111.49323100019878
At time: 838.0432784557343 and batch: 450, loss is 4.753000440597535 and perplexity is 115.9316091097391
At time: 839.1655275821686 and batch: 500, loss is 4.7554880714416505 and perplexity is 116.2203631642064
At time: 840.2854535579681 and batch: 550, loss is 4.73030387878418 and perplexity is 113.32999569791653
At time: 841.4079270362854 and batch: 600, loss is 4.7576932525634765 and perplexity is 116.47693290242833
At time: 842.5307493209839 and batch: 650, loss is 4.7375108432769775 and perplexity is 114.14971123524414
At time: 843.6527769565582 and batch: 700, loss is 4.714304962158203 and perplexity is 111.53126577807826
At time: 844.7737743854523 and batch: 750, loss is 4.720528974533081 and perplexity is 112.227602512785
At time: 845.8976225852966 and batch: 800, loss is 4.700982885360718 and perplexity is 110.05529104376537
At time: 847.0204465389252 and batch: 850, loss is 4.746969985961914 and perplexity is 115.23459257579202
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.914664586385091 and perplexity of 136.273594117322
Finished 40 epochs...
Completing Train Step...
At time: 850.0310935974121 and batch: 50, loss is 4.800381708145141 and perplexity is 121.55680788804897
At time: 851.1504955291748 and batch: 100, loss is 4.7538261985778805 and perplexity is 116.02738009752254
At time: 852.3100619316101 and batch: 150, loss is 4.747274093627929 and perplexity is 115.26964162785708
At time: 853.4296641349792 and batch: 200, loss is 4.774086370468139 and perplexity is 118.402089543797
At time: 854.5503821372986 and batch: 250, loss is 4.767985553741455 and perplexity is 117.68193908208825
At time: 855.6710002422333 and batch: 300, loss is 4.735153560638428 and perplexity is 113.88094500619286
At time: 856.7909142971039 and batch: 350, loss is 4.681553792953491 and perplexity is 107.9376551113282
At time: 857.9101614952087 and batch: 400, loss is 4.713903102874756 and perplexity is 111.48645490796585
At time: 859.0303318500519 and batch: 450, loss is 4.753036527633667 and perplexity is 115.93579281339424
At time: 860.1501972675323 and batch: 500, loss is 4.755543031692505 and perplexity is 116.22675084005282
At time: 861.2718341350555 and batch: 550, loss is 4.7303157234191895 and perplexity is 113.33133805830109
At time: 862.3911442756653 and batch: 600, loss is 4.7576988697052 and perplexity is 116.47758717170551
At time: 863.5106089115143 and batch: 650, loss is 4.737524137496949 and perplexity is 114.15122877670217
At time: 864.6325929164886 and batch: 700, loss is 4.714312314987183 and perplexity is 111.53208585141633
At time: 865.753842830658 and batch: 750, loss is 4.720585622787476 and perplexity is 112.23396019063625
At time: 866.8749573230743 and batch: 800, loss is 4.70108401298523 and perplexity is 110.06642123668907
At time: 867.9950904846191 and batch: 850, loss is 4.7470080852508545 and perplexity is 115.2389830154663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.914666493733724 and perplexity of 136.27385403882332
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 871.0463154315948 and batch: 50, loss is 4.800298624038696 and perplexity is 121.54670886882303
At time: 872.1681382656097 and batch: 100, loss is 4.753743457794189 and perplexity is 116.01778029831657
At time: 873.2927460670471 and batch: 150, loss is 4.747291498184204 and perplexity is 115.27164786228029
At time: 874.4147744178772 and batch: 200, loss is 4.773836898803711 and perplexity is 118.37255526158314
At time: 875.5366187095642 and batch: 250, loss is 4.767774314880371 and perplexity is 117.65708270871482
At time: 876.6579504013062 and batch: 300, loss is 4.734935874938965 and perplexity is 113.85615745106973
At time: 877.7820773124695 and batch: 350, loss is 4.681496543884277 and perplexity is 107.93147595791699
At time: 878.9026713371277 and batch: 400, loss is 4.713577928543091 and perplexity is 111.45020826805933
At time: 880.0250508785248 and batch: 450, loss is 4.752825078964233 and perplexity is 115.91128093585732
At time: 881.1713104248047 and batch: 500, loss is 4.755605764389038 and perplexity is 116.23404228624598
At time: 882.2902414798737 and batch: 550, loss is 4.730150661468506 and perplexity is 113.31263291036417
At time: 883.4142396450043 and batch: 600, loss is 4.757213859558106 and perplexity is 116.42110805759707
At time: 884.5346751213074 and batch: 650, loss is 4.7369207000732425 and perplexity is 114.08236643244369
At time: 885.6551337242126 and batch: 700, loss is 4.7137190628051755 and perplexity is 111.46593882099626
At time: 886.7765123844147 and batch: 750, loss is 4.720068769454956 and perplexity is 112.17596668264981
At time: 887.9000332355499 and batch: 800, loss is 4.700469541549682 and perplexity is 109.99880933974329
At time: 889.0244152545929 and batch: 850, loss is 4.746472339630127 and perplexity is 115.17726077016634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.914615948994954 and perplexity of 136.26696628654108
Finished 42 epochs...
Completing Train Step...
At time: 892.0111086368561 and batch: 50, loss is 4.800237588882446 and perplexity is 121.53929047284933
At time: 893.155472278595 and batch: 100, loss is 4.75366322517395 and perplexity is 116.008472261218
At time: 894.2769436836243 and batch: 150, loss is 4.747179927825928 and perplexity is 115.25878768065014
At time: 895.4303538799286 and batch: 200, loss is 4.773835144042969 and perplexity is 118.37234754625248
At time: 896.5533137321472 and batch: 250, loss is 4.767719535827637 and perplexity is 117.65063774170282
At time: 897.6738710403442 and batch: 300, loss is 4.734913034439087 and perplexity is 113.85355694921785
At time: 898.7956721782684 and batch: 350, loss is 4.681480741500854 and perplexity is 107.92977039682648
At time: 899.9178030490875 and batch: 400, loss is 4.713542575836182 and perplexity is 111.44626827115661
At time: 901.039092540741 and batch: 450, loss is 4.752792854309082 and perplexity is 115.90754579498315
At time: 902.1594388484955 and batch: 500, loss is 4.755527696609497 and perplexity is 116.22496850684709
At time: 903.279415845871 and batch: 550, loss is 4.730104875564575 and perplexity is 113.30744490780923
At time: 904.401153087616 and batch: 600, loss is 4.757204885482788 and perplexity is 116.42006329049272
At time: 905.5224940776825 and batch: 650, loss is 4.736915073394775 and perplexity is 114.0817245294549
At time: 906.6452348232269 and batch: 700, loss is 4.713724422454834 and perplexity is 111.46653624097814
At time: 907.7667248249054 and batch: 750, loss is 4.720071897506714 and perplexity is 112.17631757542841
At time: 908.9140794277191 and batch: 800, loss is 4.700535659790039 and perplexity is 110.00608250790003
At time: 910.0351622104645 and batch: 850, loss is 4.746504468917847 and perplexity is 115.1809613929652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.914568901062012 and perplexity of 136.2605553582606
Finished 43 epochs...
Completing Train Step...
At time: 913.0858261585236 and batch: 50, loss is 4.800198259353638 and perplexity is 121.53451048382136
At time: 914.2290589809418 and batch: 100, loss is 4.753616304397583 and perplexity is 116.00302918133208
At time: 915.3468332290649 and batch: 150, loss is 4.747117223739624 and perplexity is 115.25156071026281
At time: 916.465754032135 and batch: 200, loss is 4.773818426132202 and perplexity is 118.3703686244507
At time: 917.5826182365417 and batch: 250, loss is 4.767688055038452 and perplexity is 117.6469340650762
At time: 918.700537443161 and batch: 300, loss is 4.734897556304932 and perplexity is 113.85179472222741
At time: 919.8232460021973 and batch: 350, loss is 4.681466455459595 and perplexity is 107.92822851868718
At time: 920.9441421031952 and batch: 400, loss is 4.713523511886597 and perplexity is 111.4441436853684
At time: 922.0644738674164 and batch: 450, loss is 4.7527872085571286 and perplexity is 115.90689141157735
At time: 923.1852247714996 and batch: 500, loss is 4.755513134002686 and perplexity is 116.22327598065293
At time: 924.305296421051 and batch: 550, loss is 4.730093898773194 and perplexity is 113.30620116245066
At time: 925.4275920391083 and batch: 600, loss is 4.757202005386352 and perplexity is 116.4197279899662
At time: 926.547758102417 and batch: 650, loss is 4.736910676956176 and perplexity is 114.08122297726027
At time: 927.6688349246979 and batch: 700, loss is 4.713726472854614 and perplexity is 111.46676479217392
At time: 928.78822016716 and batch: 750, loss is 4.720084981918335 and perplexity is 112.17778534614412
At time: 929.921012878418 and batch: 800, loss is 4.700576400756836 and perplexity is 110.01056435335165
At time: 931.050329208374 and batch: 850, loss is 4.746523551940918 and perplexity is 115.1831594148812
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9145355224609375 and perplexity of 136.25600724744638
Finished 44 epochs...
Completing Train Step...
At time: 934.0529761314392 and batch: 50, loss is 4.80016261100769 and perplexity is 121.5301780567694
At time: 935.2050220966339 and batch: 100, loss is 4.753578090667725 and perplexity is 115.99859635761015
At time: 936.330671787262 and batch: 150, loss is 4.747070589065552 and perplexity is 115.24618611661491
At time: 937.481043100357 and batch: 200, loss is 4.773794040679932 and perplexity is 118.36748214467065
At time: 938.6035158634186 and batch: 250, loss is 4.767661533355713 and perplexity is 117.64381391179177
At time: 939.7257103919983 and batch: 300, loss is 4.734881706237793 and perplexity is 113.84999017793831
At time: 940.8480639457703 and batch: 350, loss is 4.681451597213745 and perplexity is 107.92662490644719
At time: 941.97039103508 and batch: 400, loss is 4.713508186340332 and perplexity is 111.4424357560759
At time: 943.0926694869995 and batch: 450, loss is 4.75278865814209 and perplexity is 115.90705942858584
At time: 944.2170724868774 and batch: 500, loss is 4.755517692565918 and perplexity is 116.22380579301309
At time: 945.3380763530731 and batch: 550, loss is 4.73009349822998 and perplexity is 113.30615577842981
At time: 946.4596242904663 and batch: 600, loss is 4.757199211120605 and perplexity is 116.41940268276254
At time: 947.5795304775238 and batch: 650, loss is 4.736905946731567 and perplexity is 114.08068334872816
At time: 948.7032999992371 and batch: 700, loss is 4.713726367950439 and perplexity is 111.46675309884552
At time: 949.8249487876892 and batch: 750, loss is 4.720099258422851 and perplexity is 112.17938686423525
At time: 950.9461290836334 and batch: 800, loss is 4.700606327056885 and perplexity is 110.01385661177143
At time: 952.0663003921509 and batch: 850, loss is 4.746537246704102 and perplexity is 115.18473683177331
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.914512634277344 and perplexity of 136.25288863062661
Finished 45 epochs...
Completing Train Step...
At time: 955.0859835147858 and batch: 50, loss is 4.800128412246704 and perplexity is 121.5260219463248
At time: 956.207765340805 and batch: 100, loss is 4.7535436153411865 and perplexity is 115.99459733705689
At time: 957.3281693458557 and batch: 150, loss is 4.747030849456787 and perplexity is 115.24160636926631
At time: 958.4530408382416 and batch: 200, loss is 4.773767013549804 and perplexity is 118.36428305455914
At time: 959.573695898056 and batch: 250, loss is 4.76763614654541 and perplexity is 117.6408273485145
At time: 960.6958951950073 and batch: 300, loss is 4.734865627288818 and perplexity is 113.84815960447233
At time: 961.8175823688507 and batch: 350, loss is 4.681437177658081 and perplexity is 107.9250686636919
At time: 962.9399056434631 and batch: 400, loss is 4.713494291305542 and perplexity is 111.4408872703122
At time: 964.0598428249359 and batch: 450, loss is 4.75279203414917 and perplexity is 115.90745073229962
At time: 965.1818506717682 and batch: 500, loss is 4.7555286407470705 and perplexity is 116.22507823925864
At time: 966.3301815986633 and batch: 550, loss is 4.730096712112426 and perplexity is 113.3065199316801
At time: 967.451584815979 and batch: 600, loss is 4.75719612121582 and perplexity is 116.41904295844883
At time: 968.5731267929077 and batch: 650, loss is 4.736901092529297 and perplexity is 114.0801295793601
At time: 969.6936542987823 and batch: 700, loss is 4.713725500106811 and perplexity is 111.46665636317607
At time: 970.8144869804382 and batch: 750, loss is 4.7201131820678714 and perplexity is 112.18094882107056
At time: 971.9368286132812 and batch: 800, loss is 4.700630750656128 and perplexity is 110.01654357892902
At time: 973.0582940578461 and batch: 850, loss is 4.74654842376709 and perplexity is 115.18602426602699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.914498011271159 and perplexity of 136.25089621836105
Finished 46 epochs...
Completing Train Step...
At time: 976.1036586761475 and batch: 50, loss is 4.80009557723999 and perplexity is 121.52203170408838
At time: 977.225091457367 and batch: 100, loss is 4.753511075973511 and perplexity is 115.9908230076132
At time: 978.347818851471 and batch: 150, loss is 4.746995058059692 and perplexity is 115.23748178498364
At time: 979.470157623291 and batch: 200, loss is 4.773739404678345 and perplexity is 118.36101519539407
At time: 980.5935862064362 and batch: 250, loss is 4.767611255645752 and perplexity is 117.6378991989276
At time: 981.7154128551483 and batch: 300, loss is 4.734849233627319 and perplexity is 113.84629323157982
At time: 982.8365938663483 and batch: 350, loss is 4.681423130035401 and perplexity is 107.92355258369827
At time: 983.9600465297699 and batch: 400, loss is 4.713480882644653 and perplexity is 111.43939300726366
At time: 985.0841565132141 and batch: 450, loss is 4.7527961158752445 and perplexity is 115.90792383572904
At time: 986.2061429023743 and batch: 500, loss is 4.755541667938233 and perplexity is 116.2265923354329
At time: 987.3282742500305 and batch: 550, loss is 4.730101041793823 and perplexity is 113.30701051387359
At time: 988.4515926837921 and batch: 600, loss is 4.75719292640686 and perplexity is 116.41867102244144
At time: 989.5730359554291 and batch: 650, loss is 4.736895790100098 and perplexity is 114.07952467915374
At time: 990.6944017410278 and batch: 700, loss is 4.713724184036255 and perplexity is 111.46650966528813
At time: 991.8166418075562 and batch: 750, loss is 4.720126438140869 and perplexity is 112.18243590977352
At time: 992.9407415390015 and batch: 800, loss is 4.700652704238892 and perplexity is 110.01895886273587
At time: 994.062319278717 and batch: 850, loss is 4.746558465957642 and perplexity is 115.18718099183963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.914489110310872 and perplexity of 136.2496834599421
Finished 47 epochs...
Completing Train Step...
At time: 997.1080114841461 and batch: 50, loss is 4.800063810348511 and perplexity is 121.51817138821036
At time: 998.2275738716125 and batch: 100, loss is 4.753479909896851 and perplexity is 115.98720808506327
At time: 999.347635269165 and batch: 150, loss is 4.7469613647460935 and perplexity is 115.23359911778186
At time: 1000.4678912162781 and batch: 200, loss is 4.773711891174316 and perplexity is 118.35775871392445
At time: 1001.5865292549133 and batch: 250, loss is 4.767586717605591 and perplexity is 117.63501263084804
At time: 1002.7072198390961 and batch: 300, loss is 4.734832601547241 and perplexity is 113.8443997466606
At time: 1003.8270871639252 and batch: 350, loss is 4.68140947341919 and perplexity is 107.92207872322447
At time: 1004.9474942684174 and batch: 400, loss is 4.71346827507019 and perplexity is 111.43798803567492
At time: 1006.0673606395721 and batch: 450, loss is 4.752800388336182 and perplexity is 115.9084190488638
At time: 1007.1875982284546 and batch: 500, loss is 4.755555238723755 and perplexity is 116.228169632292
At time: 1008.3082363605499 and batch: 550, loss is 4.730105752944946 and perplexity is 113.30754432158089
At time: 1009.4284920692444 and batch: 600, loss is 4.757189521789551 and perplexity is 116.41827466209362
At time: 1010.5483245849609 and batch: 650, loss is 4.736890716552734 and perplexity is 114.07894589275031
At time: 1011.669394493103 and batch: 700, loss is 4.713722467422485 and perplexity is 111.46631832050706
At time: 1012.7890148162842 and batch: 750, loss is 4.720138864517212 and perplexity is 112.18382993960256
At time: 1013.9118716716766 and batch: 800, loss is 4.7006729698181156 and perplexity is 110.02118848325502
At time: 1015.0416402816772 and batch: 850, loss is 4.746567850112915 and perplexity is 115.18826193130339
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.914484024047852 and perplexity of 136.24899045997796
Finished 48 epochs...
Completing Train Step...
At time: 1018.0306477546692 and batch: 50, loss is 4.800032730102539 and perplexity is 121.51439463224516
At time: 1019.1763963699341 and batch: 100, loss is 4.753449459075927 and perplexity is 115.98367623313464
At time: 1020.2984261512756 and batch: 150, loss is 4.746929159164429 and perplexity is 115.22988801245444
At time: 1021.4203486442566 and batch: 200, loss is 4.773684749603271 and perplexity is 118.35454634200221
At time: 1022.544162273407 and batch: 250, loss is 4.767562351226807 and perplexity is 117.63214632649289
At time: 1023.6906249523163 and batch: 300, loss is 4.73481632232666 and perplexity is 113.84254646365021
At time: 1024.8126575946808 and batch: 350, loss is 4.681396026611328 and perplexity is 107.92062752552484
At time: 1025.9377346038818 and batch: 400, loss is 4.713455781936646 and perplexity is 111.43659583470495
At time: 1027.05983543396 and batch: 450, loss is 4.752804899215699 and perplexity is 115.9089418989564
At time: 1028.1809372901917 and batch: 500, loss is 4.75556902885437 and perplexity is 116.22977244498385
At time: 1029.3023810386658 and batch: 550, loss is 4.7301109790802 and perplexity is 113.30813648368012
At time: 1030.4255013465881 and batch: 600, loss is 4.75718599319458 and perplexity is 116.4178638698799
At time: 1031.5473499298096 and batch: 650, loss is 4.736885356903076 and perplexity is 114.07833447120544
At time: 1032.6685512065887 and batch: 700, loss is 4.713720779418946 and perplexity is 111.46613016512597
At time: 1033.791172504425 and batch: 750, loss is 4.720150928497315 and perplexity is 112.18518333125846
At time: 1034.9140844345093 and batch: 800, loss is 4.700692119598389 and perplexity is 110.02329538501318
At time: 1036.0363783836365 and batch: 850, loss is 4.74657675743103 and perplexity is 115.18928795436508
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.914480209350586 and perplexity of 136.24847071231795
Finished 49 epochs...
Completing Train Step...
At time: 1039.0382087230682 and batch: 50, loss is 4.800002756118775 and perplexity is 121.51075241633944
At time: 1040.1853353977203 and batch: 100, loss is 4.753419847488403 and perplexity is 115.9802418232039
At time: 1041.3078272342682 and batch: 150, loss is 4.74689793586731 and perplexity is 115.22629021159189
At time: 1042.4307436943054 and batch: 200, loss is 4.773657808303833 and perplexity is 118.35135775968178
At time: 1043.5525929927826 and batch: 250, loss is 4.767538166046142 and perplexity is 117.62930140618464
At time: 1044.676017999649 and batch: 300, loss is 4.734799871444702 and perplexity is 113.84067366876114
At time: 1045.80131483078 and batch: 350, loss is 4.681382703781128 and perplexity is 107.91918972690704
At time: 1046.9231185913086 and batch: 400, loss is 4.713443593978882 and perplexity is 111.43523765845826
At time: 1048.0443632602692 and batch: 450, loss is 4.752809200286865 and perplexity is 115.90944043263649
At time: 1049.1682629585266 and batch: 500, loss is 4.7555827808380124 and perplexity is 116.23137084590385
At time: 1050.2897853851318 and batch: 550, loss is 4.730116024017334 and perplexity is 113.30870811754734
At time: 1051.4434368610382 and batch: 600, loss is 4.757182321548462 and perplexity is 116.41743642546669
At time: 1052.567758321762 and batch: 650, loss is 4.736879949569702 and perplexity is 114.07771761328799
At time: 1053.689623117447 and batch: 700, loss is 4.713718824386596 and perplexity is 111.46591224544868
At time: 1054.8106951713562 and batch: 750, loss is 4.720162296295166 and perplexity is 112.1864586369932
At time: 1055.9334511756897 and batch: 800, loss is 4.700710353851318 and perplexity is 110.02530159590013
At time: 1057.056390762329 and batch: 850, loss is 4.7465852355957034 and perplexity is 115.19026455225685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.914478302001953 and perplexity of 136.24821083923146
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fe9c612d9e8>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'dropout': 0.9780999155560108, 'batch_size': 50, 'data': 'wikitext', 'lr': 25.20715218434082, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'anneal': 7.690045114466656, 'wordvec_source': '', 'tune_wordvecs': True}, 'best_accuracy': -167.54256475624217}, {'params': {'dropout': 0.598377035716386, 'batch_size': 50, 'data': 'wikitext', 'lr': 11.942195201405458, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'anneal': 7.344128300137092, 'wordvec_source': '', 'tune_wordvecs': True}, 'best_accuracy': -126.01780898549613}, {'params': {'dropout': 0.0336618035998445, 'batch_size': 50, 'data': 'wikitext', 'lr': 15.53775482745188, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'anneal': 2.7899680959719806, 'wordvec_source': '', 'tune_wordvecs': True}, 'best_accuracy': -126.68899236836619}, {'params': {'dropout': 0.28488080350002143, 'batch_size': 50, 'data': 'wikitext', 'lr': 14.935155926639776, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'anneal': 7.043080580538206, 'wordvec_source': '', 'tune_wordvecs': True}, 'best_accuracy': -124.91520277410557}, {'params': {'dropout': 0.8517377223397841, 'batch_size': 50, 'data': 'wikitext', 'lr': 13.965449617430208, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'anneal': 3.476424992133352, 'wordvec_source': '', 'tune_wordvecs': True}, 'best_accuracy': -143.264616794522}, {'params': {'dropout': 0.4690668651775487, 'batch_size': 50, 'data': 'wikitext', 'lr': 14.937532805173465, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'anneal': 7.048441954233849, 'wordvec_source': '', 'tune_wordvecs': True}, 'best_accuracy': -136.24821083923146}]
