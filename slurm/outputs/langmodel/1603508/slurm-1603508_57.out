Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'name': 'lr', 'type': 'continuous', 'domain': [0, 30]}, {'name': 'dropout', 'type': 'continuous', 'domain': [0, 1]}, {'name': 'anneal', 'type': 'continuous', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'batch_size': 50, 'num_layers': 1, 'anneal': 5.65209007083128, 'wordvec_source': '', 'dropout': 0.509047440648295, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 9.721940560180443, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.475619077682495 and batch: 50, loss is 7.093319911956787 and perplexity is 1203.8980087700754
At time: 4.044721841812134 and batch: 100, loss is 6.234359397888183 and perplexity is 509.9738234784018
At time: 5.6179797649383545 and batch: 150, loss is 6.047939214706421 and perplexity is 423.23992410057235
At time: 7.189003229141235 and batch: 200, loss is 6.00325574874878 and perplexity is 404.7443967571067
At time: 8.762060642242432 and batch: 250, loss is 6.022468967437744 and perplexity is 412.59602536106877
At time: 10.3406400680542 and batch: 300, loss is 5.9402313137054445 and perplexity is 380.0228238597099
At time: 11.91555118560791 and batch: 350, loss is 5.899829359054565 and perplexity is 364.97518284103194
At time: 13.491297245025635 and batch: 400, loss is 5.907154178619384 and perplexity is 367.6583751539929
At time: 15.069513320922852 and batch: 450, loss is 5.895895862579346 and perplexity is 363.54237406732955
At time: 16.649495124816895 and batch: 500, loss is 5.888793439865112 and perplexity is 360.9694901224777
At time: 18.22994351387024 and batch: 550, loss is 5.842918701171875 and perplexity is 344.7841955516518
At time: 19.80984854698181 and batch: 600, loss is 5.862438907623291 and perplexity is 351.58057175798353
At time: 21.393033504486084 and batch: 650, loss is 5.878359994888306 and perplexity is 357.2229136453657
At time: 22.973920106887817 and batch: 700, loss is 5.821583042144775 and perplexity is 337.50591704523254
At time: 24.556421041488647 and batch: 750, loss is 5.814163818359375 and perplexity is 335.0111511745842
At time: 26.138880491256714 and batch: 800, loss is 5.824608821868896 and perplexity is 338.52868215645316
At time: 27.72250247001648 and batch: 850, loss is 5.8140511894226075 and perplexity is 334.9734213495972
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.240912437438965 and perplexity of 188.8423306325064
Finished 1 epochs...
Completing Train Step...
At time: 32.02799725532532 and batch: 50, loss is 5.520821313858033 and perplexity is 249.84015012470616
At time: 33.61617302894592 and batch: 100, loss is 5.385219841003418 and perplexity is 218.15805892263654
At time: 35.201948165893555 and batch: 150, loss is 5.353900117874145 and perplexity is 211.4312989109526
At time: 36.792487382888794 and batch: 200, loss is 5.339555349349975 and perplexity is 208.42001558683492
At time: 38.38647532463074 and batch: 250, loss is 5.371137657165527 and perplexity is 215.107447082747
At time: 39.98775839805603 and batch: 300, loss is 5.299248723983765 and perplexity is 200.1863582567487
At time: 41.59793710708618 and batch: 350, loss is 5.270152254104614 and perplexity is 194.4455653272006
At time: 43.20623469352722 and batch: 400, loss is 5.278766222000122 and perplexity is 196.1277479162162
At time: 44.8128821849823 and batch: 450, loss is 5.280936574935913 and perplexity is 196.5538766072531
At time: 46.473464250564575 and batch: 500, loss is 5.272061624526978 and perplexity is 194.817188608745
At time: 48.07823324203491 and batch: 550, loss is 5.249739952087403 and perplexity is 190.51671854126138
At time: 49.686718225479126 and batch: 600, loss is 5.279928398132324 and perplexity is 196.35581540533315
At time: 51.29327917098999 and batch: 650, loss is 5.275890235900879 and perplexity is 195.56449757783915
At time: 52.89996147155762 and batch: 700, loss is 5.217164468765259 and perplexity is 184.41054017054876
At time: 54.50658941268921 and batch: 750, loss is 5.206429615020752 and perplexity is 182.44150754028638
At time: 56.122377157211304 and batch: 800, loss is 5.204506025314331 and perplexity is 182.09090225282813
At time: 57.78456115722656 and batch: 850, loss is 5.197776842117309 and perplexity is 180.86969269270512
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9780731201171875 and perplexity of 145.19433988420943
Finished 2 epochs...
Completing Train Step...
At time: 62.202831745147705 and batch: 50, loss is 5.149068193435669 and perplexity is 172.27089235701638
At time: 63.871333599090576 and batch: 100, loss is 5.061072206497192 and perplexity is 157.75957651638706
At time: 65.55433821678162 and batch: 150, loss is 5.059747266769409 and perplexity is 157.55069299561714
At time: 67.2361330986023 and batch: 200, loss is 5.071015319824219 and perplexity is 159.33602226549482
At time: 68.91679978370667 and batch: 250, loss is 5.092866506576538 and perplexity is 162.85602146634275
At time: 70.6052258014679 and batch: 300, loss is 5.050747680664062 and perplexity is 156.13916308745812
At time: 72.29561424255371 and batch: 350, loss is 5.0137069797515865 and perplexity is 150.461461217367
At time: 73.9868574142456 and batch: 400, loss is 5.035578317642212 and perplexity is 153.78850550532016
At time: 75.67620015144348 and batch: 450, loss is 5.0457760524749755 and perplexity is 155.3648236815917
At time: 77.36546564102173 and batch: 500, loss is 5.046215839385987 and perplexity is 155.4331661244328
At time: 79.05488276481628 and batch: 550, loss is 5.025451707839966 and perplexity is 152.23900813711046
At time: 80.80301141738892 and batch: 600, loss is 5.057726802825928 and perplexity is 157.23268886726837
At time: 82.50772666931152 and batch: 650, loss is 5.058234415054321 and perplexity is 157.3125223633557
At time: 84.21436476707458 and batch: 700, loss is 5.015537128448487 and perplexity is 150.73708019947728
At time: 85.9214916229248 and batch: 750, loss is 5.009727745056153 and perplexity is 149.86392939869583
At time: 87.62857556343079 and batch: 800, loss is 5.000029373168945 and perplexity is 148.4175185313974
At time: 89.33411741256714 and batch: 850, loss is 4.988003349304199 and perplexity is 146.64333547804983
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.919806162516276 and perplexity of 136.9760595182966
Finished 3 epochs...
Completing Train Step...
At time: 93.7507061958313 and batch: 50, loss is 4.961417379379273 and perplexity is 142.79604870352452
At time: 95.43185901641846 and batch: 100, loss is 4.882666864395142 and perplexity is 131.98217352259073
At time: 97.07486653327942 and batch: 150, loss is 4.891410627365112 and perplexity is 133.14125434348543
At time: 98.74423480033875 and batch: 200, loss is 4.902276287078857 and perplexity is 134.5958099494825
At time: 100.42900109291077 and batch: 250, loss is 4.939815368652344 and perplexity is 139.74444597261328
At time: 102.12848663330078 and batch: 300, loss is 4.8914735221862795 and perplexity is 133.1496285022102
At time: 103.82575583457947 and batch: 350, loss is 4.857393369674683 and perplexity is 128.68832165772704
At time: 105.52555012702942 and batch: 400, loss is 4.870113945007324 and perplexity is 130.3357671603297
At time: 107.22676348686218 and batch: 450, loss is 4.890640640258789 and perplexity is 133.03877675259622
At time: 108.92586064338684 and batch: 500, loss is 4.901373519897461 and perplexity is 134.4743561000299
At time: 110.62353086471558 and batch: 550, loss is 4.8728485012054445 and perplexity is 130.69266539699635
At time: 112.32333374023438 and batch: 600, loss is 4.912539958953857 and perplexity is 135.98437085574236
At time: 114.02418112754822 and batch: 650, loss is 4.907885265350342 and perplexity is 135.35287612196348
At time: 115.72459554672241 and batch: 700, loss is 4.866264734268189 and perplexity is 129.83504164294877
At time: 117.42239904403687 and batch: 750, loss is 4.862174625396729 and perplexity is 129.30508671277997
At time: 119.12194657325745 and batch: 800, loss is 4.848816890716552 and perplexity is 127.58934837183428
At time: 120.82566499710083 and batch: 850, loss is 4.848808507919312 and perplexity is 127.58827882067976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.876076698303223 and perplexity of 131.11524880440905
Finished 4 epochs...
Completing Train Step...
At time: 125.23066067695618 and batch: 50, loss is 4.819831466674804 and perplexity is 123.9442002912878
At time: 126.9248480796814 and batch: 100, loss is 4.75557092666626 and perplexity is 116.22999302743727
At time: 128.6070532798767 and batch: 150, loss is 4.773171043395996 and perplexity is 118.2937624907446
At time: 130.3050138950348 and batch: 200, loss is 4.789316320419312 and perplexity is 120.21914918302403
At time: 132.00246238708496 and batch: 250, loss is 4.803010444641114 and perplexity is 121.87676906784237
At time: 133.70136880874634 and batch: 300, loss is 4.787928724288941 and perplexity is 120.05244923966666
At time: 135.40181851387024 and batch: 350, loss is 4.7470606994628906 and perplexity is 115.24504638326175
At time: 137.09910488128662 and batch: 400, loss is 4.756559181213379 and perplexity is 116.34491462309983
At time: 138.80266737937927 and batch: 450, loss is 4.778999834060669 and perplexity is 118.985285482758
At time: 140.51268339157104 and batch: 500, loss is 4.791667795181274 and perplexity is 120.50217411080587
At time: 142.22400641441345 and batch: 550, loss is 4.7768521404266355 and perplexity is 118.73001576128206
At time: 143.93481135368347 and batch: 600, loss is 4.819444751739502 and perplexity is 123.89627848452743
At time: 145.64403820037842 and batch: 650, loss is 4.805842981338501 and perplexity is 122.2224788755011
At time: 147.35411190986633 and batch: 700, loss is 4.7715370559692385 and perplexity is 118.10062980133466
At time: 149.0648775100708 and batch: 750, loss is 4.777504653930664 and perplexity is 118.80751398146766
At time: 150.77572512626648 and batch: 800, loss is 4.746309099197387 and perplexity is 115.15846071878241
At time: 152.48563027381897 and batch: 850, loss is 4.7481793022155765 and perplexity is 115.37403193772796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.870085080464681 and perplexity of 130.33200513231543
Finished 5 epochs...
Completing Train Step...
At time: 156.92441511154175 and batch: 50, loss is 4.729328832626343 and perplexity is 113.21954757580252
At time: 158.59269070625305 and batch: 100, loss is 4.670806617736816 and perplexity is 106.78384144380169
At time: 160.27368903160095 and batch: 150, loss is 4.68572732925415 and perplexity is 108.38907819319722
At time: 161.98547887802124 and batch: 200, loss is 4.697959041595459 and perplexity is 109.7230036838324
At time: 163.70531749725342 and batch: 250, loss is 4.706816320419311 and perplexity is 110.699167617142
At time: 165.4236307144165 and batch: 300, loss is 4.690382862091065 and perplexity is 108.8948635425079
At time: 167.16998481750488 and batch: 350, loss is 4.65167353630066 and perplexity is 104.7601588855424
At time: 168.8876006603241 and batch: 400, loss is 4.668540878295898 and perplexity is 106.54217096714122
At time: 170.60789275169373 and batch: 450, loss is 4.698761625289917 and perplexity is 109.81110092547195
At time: 172.32631826400757 and batch: 500, loss is 4.703505172729492 and perplexity is 110.33323249097884
At time: 174.04461240768433 and batch: 550, loss is 4.686137495040893 and perplexity is 108.43354480344637
At time: 175.7633843421936 and batch: 600, loss is 4.726270780563355 and perplexity is 112.87384516216561
At time: 177.48619389533997 and batch: 650, loss is 4.716525354385376 and perplexity is 111.77918406976657
At time: 179.20467495918274 and batch: 700, loss is 4.693162078857422 and perplexity is 109.19792691736885
At time: 180.92181611061096 and batch: 750, loss is 4.695050287246704 and perplexity is 109.40431014501864
At time: 182.64068388938904 and batch: 800, loss is 4.673883285522461 and perplexity is 107.11288576927942
At time: 184.36171889305115 and batch: 850, loss is 4.673266773223877 and perplexity is 107.04686970981311
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.880628903706868 and perplexity of 131.71347293501515
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 188.82417154312134 and batch: 50, loss is 4.667041969299317 and perplexity is 106.38259357443526
At time: 190.48326325416565 and batch: 100, loss is 4.5856137943267825 and perplexity is 98.0633594071915
At time: 192.1543254852295 and batch: 150, loss is 4.587135362625122 and perplexity is 98.21268308036865
At time: 193.85710310935974 and batch: 200, loss is 4.580252599716187 and perplexity is 97.53902942664857
At time: 195.56213927268982 and batch: 250, loss is 4.566911478042602 and perplexity is 96.24639116184804
At time: 197.26832461357117 and batch: 300, loss is 4.531602754592895 and perplexity is 92.90734949572217
At time: 198.9807710647583 and batch: 350, loss is 4.476629724502564 and perplexity is 87.93779807591355
At time: 200.6970920562744 and batch: 400, loss is 4.469812231063843 and perplexity is 87.34032167437378
At time: 202.4147880077362 and batch: 450, loss is 4.4851413822174075 and perplexity is 88.68948904322
At time: 204.13310956954956 and batch: 500, loss is 4.471625461578369 and perplexity is 87.49883347657094
At time: 205.8520803451538 and batch: 550, loss is 4.436343193054199 and perplexity is 84.46550220331258
At time: 207.56856274604797 and batch: 600, loss is 4.450173587799072 and perplexity is 85.64180908511237
At time: 209.31413793563843 and batch: 650, loss is 4.412389335632324 and perplexity is 82.46626788703824
At time: 211.03301787376404 and batch: 700, loss is 4.362220973968506 and perplexity is 78.43113463493368
At time: 212.75213050842285 and batch: 750, loss is 4.338035955429077 and perplexity is 76.55703016385488
At time: 214.4697196483612 and batch: 800, loss is 4.283938102722168 and perplexity is 72.52549119279696
At time: 216.1860156059265 and batch: 850, loss is 4.264782037734985 and perplexity is 71.14941039656063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.711798667907715 and perplexity of 111.25208560787613
Finished 7 epochs...
Completing Train Step...
At time: 220.64433574676514 and batch: 50, loss is 4.498559150695801 and perplexity is 89.887523574545
At time: 222.31895565986633 and batch: 100, loss is 4.434417209625244 and perplexity is 84.30297960390588
At time: 224.03482604026794 and batch: 150, loss is 4.440672988891602 and perplexity is 84.83201347004906
At time: 225.75236177444458 and batch: 200, loss is 4.454403104782105 and perplexity is 86.00479966738929
At time: 227.4698030948639 and batch: 250, loss is 4.4430629253387455 and perplexity is 85.03499905563265
At time: 229.18864250183105 and batch: 300, loss is 4.420978899002075 and perplexity is 83.17766805537897
At time: 230.91585230827332 and batch: 350, loss is 4.3691543769836425 and perplexity is 78.97681883853072
At time: 232.6453320980072 and batch: 400, loss is 4.370122833251953 and perplexity is 79.05334148222217
At time: 234.37502121925354 and batch: 450, loss is 4.394847803115844 and perplexity is 81.03229696105477
At time: 236.10091090202332 and batch: 500, loss is 4.386293697357178 and perplexity is 80.34209436464644
At time: 237.8282961845398 and batch: 550, loss is 4.361732387542725 and perplexity is 78.3928236070787
At time: 239.55854630470276 and batch: 600, loss is 4.391074266433716 and perplexity is 80.72709482392744
At time: 241.28861570358276 and batch: 650, loss is 4.359164867401123 and perplexity is 78.19180662151808
At time: 243.0156979560852 and batch: 700, loss is 4.313394155502319 and perplexity is 74.69358080908279
At time: 244.74221992492676 and batch: 750, loss is 4.302258415222168 and perplexity is 73.86642652779041
At time: 246.47137188911438 and batch: 800, loss is 4.268890810012818 and perplexity is 71.44234851748209
At time: 248.20201873779297 and batch: 850, loss is 4.2568573474884035 and perplexity is 70.5878015849932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.703973134358724 and perplexity of 110.38487629293014
Finished 8 epochs...
Completing Train Step...
At time: 252.68566823005676 and batch: 50, loss is 4.432521028518677 and perplexity is 84.14327734680573
At time: 254.39384579658508 and batch: 100, loss is 4.370536689758301 and perplexity is 79.08606499289495
At time: 256.0931942462921 and batch: 150, loss is 4.378554277420044 and perplexity is 79.72269315254474
At time: 257.80264616012573 and batch: 200, loss is 4.394227800369262 and perplexity is 80.98207228570426
At time: 259.5133385658264 and batch: 250, loss is 4.378395500183106 and perplexity is 79.71003600846451
At time: 261.2244896888733 and batch: 300, loss is 4.362939071655274 and perplexity is 78.48747607819455
At time: 262.9349648952484 and batch: 350, loss is 4.310686025619507 and perplexity is 74.49157454390003
At time: 264.64380288124084 and batch: 400, loss is 4.317514476776123 and perplexity is 75.0019772690494
At time: 266.35491704940796 and batch: 450, loss is 4.3442333316802975 and perplexity is 77.03295610726745
At time: 268.06519389152527 and batch: 500, loss is 4.340393543243408 and perplexity is 76.7377330129893
At time: 269.77598690986633 and batch: 550, loss is 4.319036931991577 and perplexity is 75.11625138705281
At time: 271.48454427719116 and batch: 600, loss is 4.3515514087677 and perplexity is 77.59875698044935
At time: 273.1942021846771 and batch: 650, loss is 4.324927310943604 and perplexity is 75.56002027352862
At time: 274.90570855140686 and batch: 700, loss is 4.283822002410889 and perplexity is 72.51707144947072
At time: 276.6165807247162 and batch: 750, loss is 4.279128713607788 and perplexity is 72.17752530732065
At time: 278.32568883895874 and batch: 800, loss is 4.244723711013794 and perplexity is 69.73649005782984
At time: 280.0359003543854 and batch: 850, loss is 4.240013570785522 and perplexity is 69.40879376421736
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.697395006815593 and perplexity of 109.66113354369907
Finished 9 epochs...
Completing Train Step...
At time: 284.46906661987305 and batch: 50, loss is 4.384374876022338 and perplexity is 80.18808005012531
At time: 286.1842794418335 and batch: 100, loss is 4.3219519710540775 and perplexity is 75.33553765280877
At time: 287.88486981391907 and batch: 150, loss is 4.333966388702392 and perplexity is 76.24610930689956
At time: 289.6042401790619 and batch: 200, loss is 4.352221574783325 and perplexity is 77.65077845979914
At time: 291.32268595695496 and batch: 250, loss is 4.3384318351745605 and perplexity is 76.58734354130131
At time: 293.0427014827728 and batch: 300, loss is 4.324156370162964 and perplexity is 75.5017904212817
At time: 294.76155853271484 and batch: 350, loss is 4.27369610786438 and perplexity is 71.78647643771691
At time: 296.51484847068787 and batch: 400, loss is 4.285519552230835 and perplexity is 72.64027733553102
At time: 298.23242568969727 and batch: 450, loss is 4.313011846542358 and perplexity is 74.66503024180452
At time: 299.94907236099243 and batch: 500, loss is 4.309543170928955 and perplexity is 74.40649012734906
At time: 301.6677477359772 and batch: 550, loss is 4.2902317237854 and perplexity is 72.9833785250727
At time: 303.3873805999756 and batch: 600, loss is 4.324035387039185 and perplexity is 75.49265653136005
At time: 305.1062924861908 and batch: 650, loss is 4.299533958435059 and perplexity is 73.66545453476476
At time: 306.8231680393219 and batch: 700, loss is 4.259169754981994 and perplexity is 70.75121821643717
At time: 308.54142570495605 and batch: 750, loss is 4.256585245132446 and perplexity is 70.56859709079262
At time: 310.26198863983154 and batch: 800, loss is 4.222188158035278 and perplexity is 68.18251529697007
At time: 311.98181104660034 and batch: 850, loss is 4.220593118667603 and perplexity is 68.07384818808139
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.696639696756999 and perplexity of 109.57833665909286
Finished 10 epochs...
Completing Train Step...
At time: 316.4322986602783 and batch: 50, loss is 4.3454885578155515 and perplexity is 77.12971059872856
At time: 318.1373882293701 and batch: 100, loss is 4.286969413757324 and perplexity is 72.745672064332
At time: 319.8423466682434 and batch: 150, loss is 4.297217111587525 and perplexity is 73.4949805160058
At time: 321.55390310287476 and batch: 200, loss is 4.318624525070191 and perplexity is 75.08527931206024
At time: 323.2656264305115 and batch: 250, loss is 4.301908187866211 and perplexity is 73.84056101420317
At time: 324.9791841506958 and batch: 300, loss is 4.2927297019958495 and perplexity is 73.16591730838638
At time: 326.69017028808594 and batch: 350, loss is 4.240759553909302 and perplexity is 69.46059087048891
At time: 328.40133786201477 and batch: 400, loss is 4.254766130447388 and perplexity is 70.4403414107847
At time: 330.1127152442932 and batch: 450, loss is 4.283172464370727 and perplexity is 72.46998414715569
At time: 331.82490038871765 and batch: 500, loss is 4.279538230895996 and perplexity is 72.20708930482496
At time: 333.5357155799866 and batch: 550, loss is 4.260648593902588 and perplexity is 70.85592527496922
At time: 335.2466311454773 and batch: 600, loss is 4.297300052642822 and perplexity is 73.50107652004988
At time: 336.95830392837524 and batch: 650, loss is 4.273512735366821 and perplexity is 71.77331397909492
At time: 338.67124938964844 and batch: 700, loss is 4.235439777374268 and perplexity is 69.09205717664048
At time: 340.4104492664337 and batch: 750, loss is 4.2344025611877445 and perplexity is 69.02043092894982
At time: 342.1217465400696 and batch: 800, loss is 4.20214168548584 and perplexity is 66.8293052364494
At time: 343.8333885669708 and batch: 850, loss is 4.201888475418091 and perplexity is 66.81238552575383
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.696077664693196 and perplexity of 109.51676742395513
Finished 11 epochs...
Completing Train Step...
At time: 348.2689485549927 and batch: 50, loss is 4.311085481643676 and perplexity is 74.5213365960212
At time: 349.9570310115814 and batch: 100, loss is 4.255352210998535 and perplexity is 70.48163722505646
At time: 351.66739535331726 and batch: 150, loss is 4.265274019241333 and perplexity is 71.18442320278362
At time: 353.38090920448303 and batch: 200, loss is 4.289805040359497 and perplexity is 72.95224436977604
At time: 355.0900218486786 and batch: 250, loss is 4.273785238265991 and perplexity is 71.79287508034463
At time: 356.800306558609 and batch: 300, loss is 4.264593944549561 and perplexity is 71.13602893583831
At time: 358.5115177631378 and batch: 350, loss is 4.21398802280426 and perplexity is 67.62569556982088
At time: 360.22290444374084 and batch: 400, loss is 4.228461675643921 and perplexity is 68.61160404790073
At time: 361.93219470977783 and batch: 450, loss is 4.2583707904815675 and perplexity is 70.69471308052495
At time: 363.6412761211395 and batch: 500, loss is 4.2565273094177245 and perplexity is 70.56450876711439
At time: 365.3518478870392 and batch: 550, loss is 4.240271649360657 and perplexity is 69.42670899848348
At time: 367.0634355545044 and batch: 600, loss is 4.278731813430786 and perplexity is 72.14888371905235
At time: 368.77251291275024 and batch: 650, loss is 4.251486978530884 and perplexity is 70.20973513339581
At time: 370.48206305503845 and batch: 700, loss is 4.214251756668091 and perplexity is 67.64353310788576
At time: 372.1934869289398 and batch: 750, loss is 4.217982234954834 and perplexity is 67.89634710475654
At time: 373.90536642074585 and batch: 800, loss is 4.1857936525344845 and perplexity is 65.74565942536789
At time: 375.6158666610718 and batch: 850, loss is 4.185713863372802 and perplexity is 65.74041384359117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.695974985758464 and perplexity of 109.50552293623659
Finished 12 epochs...
Completing Train Step...
At time: 380.1010093688965 and batch: 50, loss is 4.283344488143921 and perplexity is 72.48245177960585
At time: 381.7957956790924 and batch: 100, loss is 4.230081720352173 and perplexity is 68.72284799972056
At time: 383.5353047847748 and batch: 150, loss is 4.237828459739685 and perplexity is 69.25729342509801
At time: 385.2541518211365 and batch: 200, loss is 4.26269907951355 and perplexity is 71.00136338861883
At time: 386.9741201400757 and batch: 250, loss is 4.247007780075073 and perplexity is 69.89595506242776
At time: 388.6949164867401 and batch: 300, loss is 4.240691843032837 and perplexity is 69.45588779222788
At time: 390.4125678539276 and batch: 350, loss is 4.187727813720703 and perplexity is 65.8729451838965
At time: 392.131267786026 and batch: 400, loss is 4.20601318359375 and perplexity is 67.08853624761973
At time: 393.8510022163391 and batch: 450, loss is 4.232836818695068 and perplexity is 68.91244726674634
At time: 395.5710654258728 and batch: 500, loss is 4.231007051467896 and perplexity is 68.78646881995853
At time: 397.28810834884644 and batch: 550, loss is 4.216614484786987 and perplexity is 67.80354534388714
At time: 399.00602531433105 and batch: 600, loss is 4.25290657043457 and perplexity is 70.30947508321375
At time: 400.72482085227966 and batch: 650, loss is 4.229144172668457 and perplexity is 68.65844724686573
At time: 402.44515013694763 and batch: 700, loss is 4.192737498283386 and perplexity is 66.2037758472134
At time: 404.1620044708252 and batch: 750, loss is 4.197881593704223 and perplexity is 66.5452118240989
At time: 405.88186717033386 and batch: 800, loss is 4.164418277740478 and perplexity is 64.35523467988956
At time: 407.6037549972534 and batch: 850, loss is 4.169981226921082 and perplexity is 64.71423721051713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.698500315348308 and perplexity of 109.78240994188394
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 412.0819311141968 and batch: 50, loss is 4.272709579467773 and perplexity is 71.71569196140759
At time: 413.79107213020325 and batch: 100, loss is 4.224113178253174 and perplexity is 68.31389443059695
At time: 415.5102891921997 and batch: 150, loss is 4.230249319076538 and perplexity is 68.73436682662346
At time: 417.2296533584595 and batch: 200, loss is 4.254341287612915 and perplexity is 70.41042169251746
At time: 418.94614148139954 and batch: 250, loss is 4.232957773208618 and perplexity is 68.92078304239772
At time: 420.66461730003357 and batch: 300, loss is 4.214430470466613 and perplexity is 67.65562302091791
At time: 422.38496255874634 and batch: 350, loss is 4.15576545715332 and perplexity is 63.800782626804505
At time: 424.10495615005493 and batch: 400, loss is 4.16743549823761 and perplexity is 64.54970184068566
At time: 425.8225128650665 and batch: 450, loss is 4.19374789237976 and perplexity is 66.27070155646418
At time: 427.58555126190186 and batch: 500, loss is 4.186786689758301 and perplexity is 65.81097973986569
At time: 429.30423307418823 and batch: 550, loss is 4.1641303110122685 and perplexity is 64.33670518158318
At time: 431.0243384838104 and batch: 600, loss is 4.183434867858887 and perplexity is 65.59076232769729
At time: 432.74093103408813 and batch: 650, loss is 4.155317530632019 and perplexity is 63.77221096366676
At time: 434.4596209526062 and batch: 700, loss is 4.109457850456238 and perplexity is 60.91368428979594
At time: 436.1785626411438 and batch: 750, loss is 4.103809127807617 and perplexity is 60.570569773449954
At time: 437.8986692428589 and batch: 800, loss is 4.056823258399963 and perplexity is 57.79043389301014
At time: 439.61536407470703 and batch: 850, loss is 4.061145987510681 and perplexity is 58.040786998407675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.673564592997233 and perplexity of 107.07875513210756
Finished 14 epochs...
Completing Train Step...
At time: 444.0570557117462 and batch: 50, loss is 4.2431981563568115 and perplexity is 69.63018433887007
At time: 445.7738342285156 and batch: 100, loss is 4.195795888900757 and perplexity is 66.4065627968829
At time: 447.4811010360718 and batch: 150, loss is 4.200295262336731 and perplexity is 66.70602390999449
At time: 449.1917848587036 and batch: 200, loss is 4.2254128170013425 and perplexity is 68.40273553300196
At time: 450.9028332233429 and batch: 250, loss is 4.207044363021851 and perplexity is 67.15775224698365
At time: 452.61383390426636 and batch: 300, loss is 4.191552405357361 and perplexity is 66.12536469221025
At time: 454.3231644630432 and batch: 350, loss is 4.134588093757629 and perplexity is 62.46385651829559
At time: 456.03365778923035 and batch: 400, loss is 4.147687416076661 and perplexity is 63.28747334208493
At time: 457.74601340293884 and batch: 450, loss is 4.175901412963867 and perplexity is 65.0984938475057
At time: 459.456472158432 and batch: 500, loss is 4.170987453460693 and perplexity is 64.7793871657991
At time: 461.16511940956116 and batch: 550, loss is 4.151592841148377 and perplexity is 63.535121097309435
At time: 462.87555599212646 and batch: 600, loss is 4.174479084014893 and perplexity is 65.00596819172955
At time: 464.58795833587646 and batch: 650, loss is 4.149025082588196 and perplexity is 63.37218752280366
At time: 466.2983934879303 and batch: 700, loss is 4.106356220245361 and perplexity is 60.7250452618283
At time: 468.00731778144836 and batch: 750, loss is 4.105394721031189 and perplexity is 60.66668623910169
At time: 469.74642181396484 and batch: 800, loss is 4.062271943092346 and perplexity is 58.10617515166272
At time: 471.45803213119507 and batch: 850, loss is 4.069775757789611 and perplexity is 58.543833121312886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.672308603922526 and perplexity of 106.9443498090059
Finished 15 epochs...
Completing Train Step...
At time: 475.8960771560669 and batch: 50, loss is 4.231850328445435 and perplexity is 68.84449932993829
At time: 477.63666892051697 and batch: 100, loss is 4.183934135437012 and perplexity is 65.62351784495546
At time: 479.34890937805176 and batch: 150, loss is 4.1870483922958375 and perplexity is 65.82820489409636
At time: 481.06136894226074 and batch: 200, loss is 4.212771739959717 and perplexity is 67.54349359700738
At time: 482.76961874961853 and batch: 250, loss is 4.1951205348968506 and perplexity is 66.36172999952099
At time: 484.48083901405334 and batch: 300, loss is 4.180580058097839 and perplexity is 65.40378020535468
At time: 486.1912543773651 and batch: 350, loss is 4.123939743041992 and perplexity is 61.80224823703545
At time: 487.902738571167 and batch: 400, loss is 4.13759295463562 and perplexity is 62.65183399868391
At time: 489.6104784011841 and batch: 450, loss is 4.166977257728576 and perplexity is 64.5201293286352
At time: 491.31983518600464 and batch: 500, loss is 4.162968120574951 and perplexity is 64.26197711058857
At time: 493.0301353931427 and batch: 550, loss is 4.145940146446228 and perplexity is 63.176989612468645
At time: 494.7431585788727 and batch: 600, loss is 4.170031976699829 and perplexity is 64.71752152707583
At time: 496.45131516456604 and batch: 650, loss is 4.145615048408509 and perplexity is 63.15645423530289
At time: 498.16069293022156 and batch: 700, loss is 4.1042609167099 and perplexity is 60.59794106723638
At time: 499.8705823421478 and batch: 750, loss is 4.105578842163086 and perplexity is 60.677857286421876
At time: 501.5826961994171 and batch: 800, loss is 4.064546966552735 and perplexity is 58.23851854841125
At time: 503.29252099990845 and batch: 850, loss is 4.072888321876526 and perplexity is 58.726338436197565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.672113418579102 and perplexity of 106.92347787637502
Finished 16 epochs...
Completing Train Step...
At time: 507.7520432472229 and batch: 50, loss is 4.223029413223267 and perplexity is 68.23989832519463
At time: 509.49039793014526 and batch: 100, loss is 4.175072016716004 and perplexity is 65.04452378534437
At time: 511.2066390514374 and batch: 150, loss is 4.1775593042373655 and perplexity is 65.20650958680812
At time: 512.958357334137 and batch: 200, loss is 4.203936653137207 and perplexity is 66.94936940092153
At time: 514.6772394180298 and batch: 250, loss is 4.186685905456543 and perplexity is 65.80434736045011
At time: 516.3960542678833 and batch: 300, loss is 4.173010292053223 and perplexity is 64.91055803417798
At time: 518.1122896671295 and batch: 350, loss is 4.116202464103699 and perplexity is 61.325912152328264
At time: 519.8296127319336 and batch: 400, loss is 4.130455837249756 and perplexity is 62.20627240902924
At time: 521.5477020740509 and batch: 450, loss is 4.160568356513977 and perplexity is 64.10794841761155
At time: 523.2656145095825 and batch: 500, loss is 4.157348756790161 and perplexity is 63.901878394094275
At time: 524.9812817573547 and batch: 550, loss is 4.141172270774842 and perplexity is 62.87648653063923
At time: 526.6986768245697 and batch: 600, loss is 4.16654450416565 and perplexity is 64.4922140534408
At time: 528.4172220230103 and batch: 650, loss is 4.14256709575653 and perplexity is 62.96424941751769
At time: 530.1378762722015 and batch: 700, loss is 4.101904902458191 and perplexity is 60.45533950607572
At time: 531.8550474643707 and batch: 750, loss is 4.104793272018433 and perplexity is 60.630209291167745
At time: 533.5729882717133 and batch: 800, loss is 4.065261945724488 and perplexity is 58.28017276534141
At time: 535.2922580242157 and batch: 850, loss is 4.074017014503479 and perplexity is 58.79265984259881
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.672268231709798 and perplexity of 106.94003231611926
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 539.7339515686035 and batch: 50, loss is 4.220392799377441 and perplexity is 68.06021304887007
At time: 541.4306216239929 and batch: 100, loss is 4.176284856796265 and perplexity is 65.12346024977049
At time: 543.1414823532104 and batch: 150, loss is 4.179646320343018 and perplexity is 65.34273872926931
At time: 544.8528759479523 and batch: 200, loss is 4.206887311935425 and perplexity is 67.14720587721246
At time: 546.5631091594696 and batch: 250, loss is 4.1848677635192875 and perplexity is 65.68481441371974
At time: 548.2746515274048 and batch: 300, loss is 4.176045989990234 and perplexity is 65.10790627456109
At time: 549.9862062931061 and batch: 350, loss is 4.117917928695679 and perplexity is 61.431204870339776
At time: 551.6962194442749 and batch: 400, loss is 4.1262139272689815 and perplexity is 61.94295787434262
At time: 553.4060442447662 and batch: 450, loss is 4.155018854141235 and perplexity is 63.753166547687826
At time: 555.1157655715942 and batch: 500, loss is 4.149961643218994 and perplexity is 63.431567220728354
At time: 556.8721926212311 and batch: 550, loss is 4.129828629493713 and perplexity is 62.16726838559141
At time: 558.5827255249023 and batch: 600, loss is 4.150664615631103 and perplexity is 63.476173539210116
At time: 560.2916128635406 and batch: 650, loss is 4.125463342666626 and perplexity is 61.8964818881995
At time: 562.0018107891083 and batch: 700, loss is 4.081978931427002 and perplexity is 59.262630567210635
At time: 563.7130839824677 and batch: 750, loss is 4.080592350959778 and perplexity is 59.18051510423466
At time: 565.4248850345612 and batch: 800, loss is 4.038998112678528 and perplexity is 56.7694377240864
At time: 567.1347854137421 and batch: 850, loss is 4.050201144218445 and perplexity is 57.40900337328393
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.668406804402669 and perplexity of 106.5278874010346
Finished 18 epochs...
Completing Train Step...
At time: 571.5872206687927 and batch: 50, loss is 4.214860095977783 and perplexity is 67.68469584730853
At time: 573.2946488857269 and batch: 100, loss is 4.170356922149658 and perplexity is 64.73855460833718
At time: 575.0026593208313 and batch: 150, loss is 4.172953186035156 and perplexity is 64.90685135651594
At time: 576.7129461765289 and batch: 200, loss is 4.199627461433411 and perplexity is 66.66149243770417
At time: 578.424562215805 and batch: 250, loss is 4.179010639190674 and perplexity is 65.30121478121042
At time: 580.1335439682007 and batch: 300, loss is 4.169174213409423 and perplexity is 64.66203301427069
At time: 581.8428781032562 and batch: 350, loss is 4.109956660270691 and perplexity is 60.94407621261873
At time: 583.5522162914276 and batch: 400, loss is 4.11970465183258 and perplexity is 61.54106353969664
At time: 585.2631781101227 and batch: 450, loss is 4.150128016471863 and perplexity is 63.442121414844884
At time: 586.971441745758 and batch: 500, loss is 4.145952768325806 and perplexity is 63.17778702985608
At time: 588.6790142059326 and batch: 550, loss is 4.1275079679489135 and perplexity is 62.02316646706554
At time: 590.388338804245 and batch: 600, loss is 4.150154638290405 and perplexity is 63.44381038197073
At time: 592.1002929210663 and batch: 650, loss is 4.125555934906006 and perplexity is 61.90221328740476
At time: 593.8092904090881 and batch: 700, loss is 4.082861189842224 and perplexity is 59.31493859294176
At time: 595.5169067382812 and batch: 750, loss is 4.0818798542022705 and perplexity is 59.256759281103946
At time: 597.2270505428314 and batch: 800, loss is 4.041492214202881 and perplexity is 56.911203180482985
At time: 598.9384319782257 and batch: 850, loss is 4.053535885810852 and perplexity is 57.60076712818713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.668136278788249 and perplexity of 106.4990727765653
Finished 19 epochs...
Completing Train Step...
At time: 603.4288728237152 and batch: 50, loss is 4.212388000488281 and perplexity is 67.51757946493125
At time: 605.146675825119 and batch: 100, loss is 4.167536582946777 and perplexity is 64.55622715832243
At time: 606.8653955459595 and batch: 150, loss is 4.16940999507904 and perplexity is 64.67728093389495
At time: 608.5987496376038 and batch: 200, loss is 4.195969243049621 and perplexity is 66.41807564792695
At time: 610.3324625492096 and batch: 250, loss is 4.176115798950195 and perplexity is 65.11245154843193
At time: 612.0675418376923 and batch: 300, loss is 4.166517958641053 and perplexity is 64.49050209650882
At time: 613.8033788204193 and batch: 350, loss is 4.107577247619629 and perplexity is 60.79923749054991
At time: 615.5398142337799 and batch: 400, loss is 4.117287368774414 and perplexity is 61.3924810247691
At time: 617.2721529006958 and batch: 450, loss is 4.147977895736695 and perplexity is 63.30585973613321
At time: 619.0061182975769 and batch: 500, loss is 4.144199705123901 and perplexity is 63.06712939948586
At time: 620.7410504817963 and batch: 550, loss is 4.126174592971802 and perplexity is 61.94052143954745
At time: 622.4745707511902 and batch: 600, loss is 4.149560670852662 and perplexity is 63.406138013665064
At time: 624.2076394557953 and batch: 650, loss is 4.12552128314972 and perplexity is 61.900068304160264
At time: 625.941609621048 and batch: 700, loss is 4.083382592201233 and perplexity is 59.345873605942145
At time: 627.676661491394 and batch: 750, loss is 4.0826843738555905 and perplexity is 59.30445169070564
At time: 629.4128022193909 and batch: 800, loss is 4.042869663238525 and perplexity is 56.989649477906205
At time: 631.1459369659424 and batch: 850, loss is 4.055359568595886 and perplexity is 57.705908498717164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.668108940124512 and perplexity of 106.4961612740248
Finished 20 epochs...
Completing Train Step...
At time: 635.5779325962067 and batch: 50, loss is 4.210357875823974 and perplexity is 67.38064940116375
At time: 637.3135449886322 and batch: 100, loss is 4.165336041450501 and perplexity is 64.41432469001464
At time: 639.0295152664185 and batch: 150, loss is 4.167059979438782 and perplexity is 64.52546676483382
At time: 640.7480638027191 and batch: 200, loss is 4.19357850074768 and perplexity is 66.25947680488476
At time: 642.4668209552765 and batch: 250, loss is 4.173930244445801 and perplexity is 64.97030013307511
At time: 644.2117702960968 and batch: 300, loss is 4.164757475852967 and perplexity is 64.3770675566477
At time: 645.927928686142 and batch: 350, loss is 4.105989842414856 and perplexity is 60.70280102663408
At time: 647.6449589729309 and batch: 400, loss is 4.115498666763306 and perplexity is 61.282766323199105
At time: 649.3629231452942 and batch: 450, loss is 4.146404557228088 and perplexity is 63.206336501588545
At time: 651.0793850421906 and batch: 500, loss is 4.142866716384888 and perplexity is 62.983117632004216
At time: 652.795416355133 and batch: 550, loss is 4.125134959220886 and perplexity is 61.87615944515745
At time: 654.5138285160065 and batch: 600, loss is 4.148989052772522 and perplexity is 63.36990427570108
At time: 656.2324254512787 and batch: 650, loss is 4.125385775566101 and perplexity is 61.89168094376682
At time: 657.9500195980072 and batch: 700, loss is 4.083653869628907 and perplexity is 59.3619749857487
At time: 659.6656014919281 and batch: 750, loss is 4.083183841705322 and perplexity is 59.33407975618827
At time: 661.3835775852203 and batch: 800, loss is 4.0437317943573 and perplexity is 57.03880321360723
At time: 663.1021587848663 and batch: 850, loss is 4.056568579673767 and perplexity is 57.775717772940844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.668150901794434 and perplexity of 106.50063012455169
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 667.524875164032 and batch: 50, loss is 4.2096923828125 and perplexity is 67.33582296737231
At time: 669.2627928256989 and batch: 100, loss is 4.165731120109558 and perplexity is 64.43977844282342
At time: 670.9746642112732 and batch: 150, loss is 4.167536964416504 and perplexity is 64.55625178457348
At time: 672.6834855079651 and batch: 200, loss is 4.194571423530578 and perplexity is 66.32530002226544
At time: 674.3932712078094 and batch: 250, loss is 4.172999105453491 and perplexity is 64.90983190980832
At time: 676.1033184528351 and batch: 300, loss is 4.166381134986877 and perplexity is 64.48167887397891
At time: 677.814685344696 and batch: 350, loss is 4.1101962757110595 and perplexity is 60.95868110398713
At time: 679.5229105949402 and batch: 400, loss is 4.11621729850769 and perplexity is 61.32682189243199
At time: 681.2300653457642 and batch: 450, loss is 4.1467424535751345 and perplexity is 63.22769730046734
At time: 682.9399883747101 and batch: 500, loss is 4.143584117889405 and perplexity is 63.0283180268296
At time: 684.6503319740295 and batch: 550, loss is 4.123876981735229 and perplexity is 61.7983695688916
At time: 686.3998310565948 and batch: 600, loss is 4.145867533683777 and perplexity is 63.17240232327944
At time: 688.1088099479675 and batch: 650, loss is 4.120759620666504 and perplexity is 61.606021702131294
At time: 689.8176109790802 and batch: 700, loss is 4.0794635677337645 and perplexity is 59.11375081976935
At time: 691.5281932353973 and batch: 750, loss is 4.077568101882934 and perplexity is 59.001808848383355
At time: 693.2372138500214 and batch: 800, loss is 4.03726324558258 and perplexity is 56.67103567644131
At time: 694.9470617771149 and batch: 850, loss is 4.051396412849426 and perplexity is 57.47766357967299
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.667309761047363 and perplexity of 106.41108576994745
Finished 22 epochs...
Completing Train Step...
At time: 699.3674376010895 and batch: 50, loss is 4.209519014358521 and perplexity is 67.32415007173218
At time: 701.101541519165 and batch: 100, loss is 4.16484233379364 and perplexity is 64.38253069381915
At time: 702.810263633728 and batch: 150, loss is 4.166839609146118 and perplexity is 64.51124883550075
At time: 704.5212495326996 and batch: 200, loss is 4.193324146270752 and perplexity is 66.24262555350695
At time: 706.2323203086853 and batch: 250, loss is 4.171685762405396 and perplexity is 64.82463898932703
At time: 707.9409840106964 and batch: 300, loss is 4.1642419672012325 and perplexity is 64.3438891739554
At time: 709.6492483615875 and batch: 350, loss is 4.1078368663787845 and perplexity is 60.815024162314224
At time: 711.358743429184 and batch: 400, loss is 4.114505295753479 and perplexity is 61.22192002620893
At time: 713.0710172653198 and batch: 450, loss is 4.145360321998596 and perplexity is 63.1403686672492
At time: 714.7792329788208 and batch: 500, loss is 4.1422617053985595 and perplexity is 62.94502367867576
At time: 716.4886329174042 and batch: 550, loss is 4.1230505752563475 and perplexity is 61.74732009260986
At time: 718.1974778175354 and batch: 600, loss is 4.14552258014679 and perplexity is 63.15061453776094
At time: 719.9080963134766 and batch: 650, loss is 4.120902471542358 and perplexity is 61.61482280489701
At time: 721.6160671710968 and batch: 700, loss is 4.079669861793518 and perplexity is 59.12594689335875
At time: 723.3246011734009 and batch: 750, loss is 4.078118095397949 and perplexity is 59.034268386073485
At time: 725.0350878238678 and batch: 800, loss is 4.038091735839844 and perplexity is 56.71800653212972
At time: 726.7473554611206 and batch: 850, loss is 4.052515068054199 and perplexity is 57.54199724407999
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.6670716603597 and perplexity of 106.38575223333658
Finished 23 epochs...
Completing Train Step...
At time: 731.214878320694 and batch: 50, loss is 4.209377689361572 and perplexity is 67.31463615872143
At time: 732.9330523014069 and batch: 100, loss is 4.16436505317688 and perplexity is 64.35180949176134
At time: 734.6521656513214 and batch: 150, loss is 4.166215233802795 and perplexity is 64.47098217442421
At time: 736.3682255744934 and batch: 200, loss is 4.192388319969178 and perplexity is 66.18066295986317
At time: 738.085275888443 and batch: 250, loss is 4.170791578292847 and perplexity is 64.76669973508189
At time: 739.8045814037323 and batch: 300, loss is 4.162970414161682 and perplexity is 64.26212450117562
At time: 741.5241148471832 and batch: 350, loss is 4.1065108013153075 and perplexity is 60.73443292985659
At time: 743.240382194519 and batch: 400, loss is 4.1135482549667355 and perplexity is 61.16335618016576
At time: 744.9580926895142 and batch: 450, loss is 4.144641556739807 and perplexity is 63.09500186981614
At time: 746.6756465435028 and batch: 500, loss is 4.141565713882446 and perplexity is 62.90122971809421
At time: 748.4097850322723 and batch: 550, loss is 4.122789130210877 and perplexity is 61.73117867183974
At time: 750.1424298286438 and batch: 600, loss is 4.14568184375763 and perplexity is 63.16067293360593
At time: 751.8795676231384 and batch: 650, loss is 4.121261954307556 and perplexity is 61.636976253429
At time: 753.613596200943 and batch: 700, loss is 4.080071048736572 and perplexity is 59.1496722100735
At time: 755.3498499393463 and batch: 750, loss is 4.07853404045105 and perplexity is 59.05882850544808
At time: 757.0832087993622 and batch: 800, loss is 4.038668217658997 and perplexity is 56.75071285810947
At time: 758.8163704872131 and batch: 850, loss is 4.05316572189331 and perplexity is 57.5794493483534
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.66701062520345 and perplexity of 106.37925916048111
Finished 24 epochs...
Completing Train Step...
At time: 763.3226659297943 and batch: 50, loss is 4.209000082015991 and perplexity is 67.28922245614513
At time: 765.0279316902161 and batch: 100, loss is 4.1638329267501835 and perplexity is 64.31757530259122
At time: 766.7386956214905 and batch: 150, loss is 4.165553035736084 and perplexity is 64.428303747015
At time: 768.4507942199707 and batch: 200, loss is 4.191601347923279 and perplexity is 66.12860111642938
At time: 770.1626439094543 and batch: 250, loss is 4.17012610912323 and perplexity is 64.72361383095084
At time: 771.8721876144409 and batch: 300, loss is 4.162182321548462 and perplexity is 64.21149994658536
At time: 773.6125836372375 and batch: 350, loss is 4.105713877677918 and perplexity is 60.686051505362244
At time: 775.3240742683411 and batch: 400, loss is 4.1129310417175295 and perplexity is 61.125616994126354
At time: 777.0349130630493 and batch: 450, loss is 4.144159092903137 and perplexity is 63.06456815531298
At time: 778.7439193725586 and batch: 500, loss is 4.14114670753479 and perplexity is 62.87487922446449
At time: 780.4571511745453 and batch: 550, loss is 4.122633295059204 and perplexity is 61.72155953376787
At time: 782.1683418750763 and batch: 600, loss is 4.145799145698548 and perplexity is 63.168082237685105
At time: 783.8791809082031 and batch: 650, loss is 4.121511149406433 and perplexity is 61.65233779975247
At time: 785.5888421535492 and batch: 700, loss is 4.080391540527343 and perplexity is 59.16863223254712
At time: 787.2991445064545 and batch: 750, loss is 4.078845601081849 and perplexity is 59.07723177803021
At time: 789.0107073783875 and batch: 800, loss is 4.039097228050232 and perplexity is 56.77506472686447
At time: 790.723256111145 and batch: 850, loss is 4.053623366355896 and perplexity is 57.60580629510174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.666994094848633 and perplexity of 106.3775006881161
Finished 25 epochs...
Completing Train Step...
At time: 795.1867280006409 and batch: 50, loss is 4.208618745803833 and perplexity is 67.26356753082158
At time: 796.8923275470734 and batch: 100, loss is 4.1633721494674685 and perplexity is 64.28794605177956
At time: 798.6016082763672 and batch: 150, loss is 4.16499951839447 and perplexity is 64.39265143159793
At time: 800.3104619979858 and batch: 200, loss is 4.190986003875732 and perplexity is 66.0879217925333
At time: 802.0205714702606 and batch: 250, loss is 4.169592442512513 and perplexity is 64.68908221432923
At time: 803.7319974899292 and batch: 300, loss is 4.161658349037171 and perplexity is 64.17786369870878
At time: 805.4415047168732 and batch: 350, loss is 4.105209274291992 and perplexity is 60.65543684307492
At time: 807.1499714851379 and batch: 400, loss is 4.112482814788819 and perplexity is 61.09822498593197
At time: 808.8593916893005 and batch: 450, loss is 4.1437959957122805 and perplexity is 63.04167374447288
At time: 810.5703673362732 and batch: 500, loss is 4.140861368179321 and perplexity is 62.85694110629864
At time: 812.2800137996674 and batch: 550, loss is 4.122492089271545 and perplexity is 61.712844707644784
At time: 813.9893209934235 and batch: 600, loss is 4.14583598613739 and perplexity is 63.17040942042255
At time: 815.6983208656311 and batch: 650, loss is 4.121634263992309 and perplexity is 61.65992856904661
At time: 817.437427520752 and batch: 700, loss is 4.08059232711792 and perplexity is 59.180513693261226
At time: 819.1467185020447 and batch: 750, loss is 4.079064807891846 and perplexity is 59.09018332903396
At time: 820.8556110858917 and batch: 800, loss is 4.0394187879562375 and perplexity is 56.79332424694894
At time: 822.565173625946 and batch: 850, loss is 4.053966150283814 and perplexity is 57.625556024405974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.666995048522949 and perplexity of 106.37760213765473
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 826.9983882904053 and batch: 50, loss is 4.208649215698242 and perplexity is 67.26561707584632
At time: 828.7370882034302 and batch: 100, loss is 4.163869590759277 and perplexity is 64.31993348598697
At time: 830.454601764679 and batch: 150, loss is 4.165625810623169 and perplexity is 64.43299268016145
At time: 832.1750996112823 and batch: 200, loss is 4.191194825172424 and perplexity is 66.10172379908397
At time: 833.8931138515472 and batch: 250, loss is 4.169169020652771 and perplexity is 64.6616972409404
At time: 835.6106123924255 and batch: 300, loss is 4.161197419166565 and perplexity is 64.14828902074822
At time: 837.3279931545258 and batch: 350, loss is 4.105435824394226 and perplexity is 60.669179895178885
At time: 839.0468647480011 and batch: 400, loss is 4.112341384887696 and perplexity is 61.08958448104071
At time: 840.7636399269104 and batch: 450, loss is 4.143745036125183 and perplexity is 63.038461248663346
At time: 842.4800190925598 and batch: 500, loss is 4.14108097076416 and perplexity is 62.8707461687989
At time: 844.1979057788849 and batch: 550, loss is 4.122073030471801 and perplexity is 61.686988814960884
At time: 845.9170475006104 and batch: 600, loss is 4.145026316642761 and perplexity is 63.11928296750932
At time: 847.6332805156708 and batch: 650, loss is 4.120342984199524 and perplexity is 61.58035973313189
At time: 849.3495543003082 and batch: 700, loss is 4.079441695213318 and perplexity is 59.112457867186
At time: 851.0663187503815 and batch: 750, loss is 4.0777242517471315 and perplexity is 59.011022692173924
At time: 852.7852020263672 and batch: 800, loss is 4.038005328178405 and perplexity is 56.7131058735541
At time: 854.5027561187744 and batch: 850, loss is 4.052674040794373 and perplexity is 57.551145580205706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.66694704691569 and perplexity of 106.37249596432903
Finished 27 epochs...
Completing Train Step...
At time: 858.9606485366821 and batch: 50, loss is 4.2084844779968265 and perplexity is 67.25453680539917
At time: 860.7029221057892 and batch: 100, loss is 4.163652558326721 and perplexity is 64.30597548908445
At time: 862.4173717498779 and batch: 150, loss is 4.1654269313812256 and perplexity is 64.42017956959417
At time: 864.1337404251099 and batch: 200, loss is 4.1910857534408565 and perplexity is 66.09451436278961
At time: 865.8509697914124 and batch: 250, loss is 4.169018816947937 and perplexity is 64.65198554383866
At time: 867.5700607299805 and batch: 300, loss is 4.161035013198853 and perplexity is 64.13787180172488
At time: 869.2850842475891 and batch: 350, loss is 4.105272407531738 and perplexity is 60.65926633819399
At time: 871.0012674331665 and batch: 400, loss is 4.1121993827819825 and perplexity is 61.08091024730155
At time: 872.7182233333588 and batch: 450, loss is 4.143627529144287 and perplexity is 63.03105422459873
At time: 874.4365553855896 and batch: 500, loss is 4.140949516296387 and perplexity is 62.86248207151093
At time: 876.1519017219543 and batch: 550, loss is 4.122046899795532 and perplexity is 61.685376913286284
At time: 877.8675425052643 and batch: 600, loss is 4.145079274177551 and perplexity is 63.12262569764354
At time: 879.5894839763641 and batch: 650, loss is 4.12043249130249 and perplexity is 61.5858718594148
At time: 881.3255858421326 and batch: 700, loss is 4.07954936504364 and perplexity is 59.11882283814599
At time: 883.0575902462006 and batch: 750, loss is 4.077808694839478 and perplexity is 59.01600597581153
At time: 884.7920274734497 and batch: 800, loss is 4.038129191398621 and perplexity is 56.72013097654298
At time: 886.5265080928802 and batch: 850, loss is 4.052778263092041 and perplexity is 57.557144005410876
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.666916529337565 and perplexity of 106.36924978290614
Finished 28 epochs...
Completing Train Step...
At time: 891.0004324913025 and batch: 50, loss is 4.208359861373902 and perplexity is 67.24615629433245
At time: 892.7087516784668 and batch: 100, loss is 4.163496356010437 and perplexity is 64.29593153122735
At time: 894.4193239212036 and batch: 150, loss is 4.165287618637085 and perplexity is 64.411205642706
At time: 896.1290488243103 and batch: 200, loss is 4.190963406562805 and perplexity is 66.08642839995721
At time: 897.8554177284241 and batch: 250, loss is 4.168895602226257 and perplexity is 64.64401995818261
At time: 899.5836510658264 and batch: 300, loss is 4.160896544456482 and perplexity is 64.12899132612648
At time: 901.3100566864014 and batch: 350, loss is 4.105126557350158 and perplexity is 60.650419818333
At time: 903.0370125770569 and batch: 400, loss is 4.112080211639404 and perplexity is 61.0736315991487
At time: 904.798437833786 and batch: 450, loss is 4.143535985946655 and perplexity is 63.0252844244419
At time: 906.5242366790771 and batch: 500, loss is 4.14087103843689 and perplexity is 62.85754895204814
At time: 908.2505860328674 and batch: 550, loss is 4.122033972740173 and perplexity is 61.684579508158116
At time: 909.9776797294617 and batch: 600, loss is 4.1451265907287596 and perplexity is 63.125612513257146
At time: 911.7028684616089 and batch: 650, loss is 4.1205120944976805 and perplexity is 61.59077448672319
At time: 913.4294068813324 and batch: 700, loss is 4.079640107154846 and perplexity is 59.124187648345774
At time: 915.1581239700317 and batch: 750, loss is 4.0778866338729856 and perplexity is 59.0206058055296
At time: 916.8859705924988 and batch: 800, loss is 4.038225955963135 and perplexity is 56.72561974087077
At time: 918.6145484447479 and batch: 850, loss is 4.052866787910461 and perplexity is 57.56223946666683
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.666897773742676 and perplexity of 106.36725478305726
Finished 29 epochs...
Completing Train Step...
At time: 923.1210522651672 and batch: 50, loss is 4.208249616622925 and perplexity is 67.2387431672143
At time: 924.8163793087006 and batch: 100, loss is 4.163362655639649 and perplexity is 64.28733571598606
At time: 926.5242788791656 and batch: 150, loss is 4.165166177749634 and perplexity is 64.40338396367639
At time: 928.2339656352997 and batch: 200, loss is 4.190840964317322 and perplexity is 66.07833712463504
At time: 929.944358587265 and batch: 250, loss is 4.16878191947937 and perplexity is 64.63667146613022
At time: 931.6537680625916 and batch: 300, loss is 4.160770306587219 and perplexity is 64.1208963298618
At time: 933.3624494075775 and batch: 350, loss is 4.104995303153991 and perplexity is 60.64245971864223
At time: 935.0727307796478 and batch: 400, loss is 4.11197283744812 and perplexity is 61.067074219399935
At time: 936.7826652526855 and batch: 450, loss is 4.143455643653869 and perplexity is 63.02022103199277
At time: 938.4915959835052 and batch: 500, loss is 4.1408094215393065 and perplexity is 62.85367598421338
At time: 940.2001798152924 and batch: 550, loss is 4.1220230388641355 and perplexity is 61.68390506029952
At time: 941.909392118454 and batch: 600, loss is 4.145168437957763 and perplexity is 63.12825420049325
At time: 943.6199624538422 and batch: 650, loss is 4.120581722259521 and perplexity is 61.595063063801014
At time: 945.3312766551971 and batch: 700, loss is 4.079719285964966 and perplexity is 59.128869216511134
At time: 947.038501739502 and batch: 750, loss is 4.07795681476593 and perplexity is 59.02474806969934
At time: 948.7760746479034 and batch: 800, loss is 4.0383109188079835 and perplexity is 56.73043951564774
At time: 950.4913928508759 and batch: 850, loss is 4.052946467399597 and perplexity is 57.566826179232066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.666886329650879 and perplexity of 106.36603751339463
Finished 30 epochs...
Completing Train Step...
At time: 954.9856050014496 and batch: 50, loss is 4.208149099349976 and perplexity is 67.23198485178405
At time: 956.6955723762512 and batch: 100, loss is 4.16324257850647 and perplexity is 64.27961674045908
At time: 958.4147927761078 and batch: 150, loss is 4.1650538873672485 and perplexity is 64.39615248908432
At time: 960.1307184696198 and batch: 200, loss is 4.19072316646576 and perplexity is 66.07055369693148
At time: 961.8465330600739 and batch: 250, loss is 4.168674201965332 and perplexity is 64.6297093395424
At time: 963.5641520023346 and batch: 300, loss is 4.160653600692749 and perplexity is 64.11341347995622
At time: 965.2939553260803 and batch: 350, loss is 4.104876399040222 and perplexity is 60.63524950938286
At time: 967.0272197723389 and batch: 400, loss is 4.111874437332153 and perplexity is 61.06106550784876
At time: 968.7596955299377 and batch: 450, loss is 4.143382573127747 and perplexity is 63.01561627952351
At time: 970.4931712150574 and batch: 500, loss is 4.14075590133667 and perplexity is 62.850312132756045
At time: 972.2280225753784 and batch: 550, loss is 4.122012572288513 and perplexity is 61.68325944442124
At time: 973.9617035388947 and batch: 600, loss is 4.145205221176147 and perplexity is 63.130576303560666
At time: 975.6946115493774 and batch: 650, loss is 4.120642485618592 and perplexity is 61.598805900447445
At time: 977.4286644458771 and batch: 700, loss is 4.079789214134216 and perplexity is 59.1330041346572
At time: 979.1675341129303 and batch: 750, loss is 4.078019976615906 and perplexity is 59.028476299721504
At time: 980.905713558197 and batch: 800, loss is 4.038388671875 and perplexity is 56.734850652800795
At time: 982.639164686203 and batch: 850, loss is 4.053020005226135 and perplexity is 57.5710596741691
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.666882832845052 and perplexity of 106.36566557266514
Finished 31 epochs...
Completing Train Step...
At time: 987.1138472557068 and batch: 50, loss is 4.208056268692016 and perplexity is 67.22574395207221
At time: 988.8388435840607 and batch: 100, loss is 4.163132643699646 and perplexity is 64.2725505616266
At time: 990.5482904911041 and batch: 150, loss is 4.164947891235352 and perplexity is 64.38932710774985
At time: 992.2873833179474 and batch: 200, loss is 4.190610203742981 and perplexity is 66.06309060882371
At time: 993.9983055591583 and batch: 250, loss is 4.16857120513916 and perplexity is 64.62305302739941
At time: 995.7079620361328 and batch: 300, loss is 4.160544910430908 and perplexity is 64.10644535494812
At time: 997.4176983833313 and batch: 350, loss is 4.104768056869506 and perplexity is 60.628680510685335
At time: 999.1295139789581 and batch: 400, loss is 4.111782994270325 and perplexity is 61.05548215234367
At time: 1000.8416426181793 and batch: 450, loss is 4.143314733505249 and perplexity is 63.011341468906025
At time: 1002.5496203899384 and batch: 500, loss is 4.140707654953003 and perplexity is 62.84727990563089
At time: 1004.2591168880463 and batch: 550, loss is 4.12200168132782 and perplexity is 61.6825876581254
At time: 1005.9703245162964 and batch: 600, loss is 4.145237240791321 and perplexity is 63.13259775268243
At time: 1007.6826152801514 and batch: 650, loss is 4.120695343017578 and perplexity is 61.60206193916014
At time: 1009.3920681476593 and batch: 700, loss is 4.079851198196411 and perplexity is 59.13666955206081
At time: 1011.1014218330383 and batch: 750, loss is 4.078076877593994 and perplexity is 59.03183517331872
At time: 1012.8141071796417 and batch: 800, loss is 4.03846061706543 and perplexity is 56.73893259927143
At time: 1014.527307510376 and batch: 850, loss is 4.05308801651001 and perplexity is 57.57497528900306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.666880289713542 and perplexity of 106.36539507113342
Finished 32 epochs...
Completing Train Step...
At time: 1018.9434177875519 and batch: 50, loss is 4.207969751358032 and perplexity is 67.21992801152383
At time: 1020.6791086196899 and batch: 100, loss is 4.163031163215638 and perplexity is 64.26602848302463
At time: 1022.389271736145 and batch: 150, loss is 4.164847083091736 and perplexity is 64.3828364663757
At time: 1024.0986940860748 and batch: 200, loss is 4.190502061843872 and perplexity is 66.05594680702335
At time: 1025.8095681667328 and batch: 250, loss is 4.16847183227539 and perplexity is 64.6166315686192
At time: 1027.5203812122345 and batch: 300, loss is 4.160443167686463 and perplexity is 64.0999233210515
At time: 1029.2318258285522 and batch: 350, loss is 4.10466851234436 and perplexity is 60.62264555785189
At time: 1030.9396662712097 and batch: 400, loss is 4.111697196960449 and perplexity is 61.050243980935576
At time: 1032.6488556861877 and batch: 450, loss is 4.143250975608826 and perplexity is 63.0073241263932
At time: 1034.3911814689636 and batch: 500, loss is 4.140663003921508 and perplexity is 62.84447377240532
At time: 1036.101753950119 and batch: 550, loss is 4.121990175247192 and perplexity is 61.68187793738153
At time: 1037.8099553585052 and batch: 600, loss is 4.14526475906372 and perplexity is 63.134335076608636
At time: 1039.5188324451447 and batch: 650, loss is 4.120741114616394 and perplexity is 61.60488162855581
At time: 1041.2297458648682 and batch: 700, loss is 4.0799064493179324 and perplexity is 59.13993700964112
At time: 1042.941071987152 and batch: 750, loss is 4.078128485679627 and perplexity is 59.03488177193727
At time: 1044.649552822113 and batch: 800, loss is 4.038527746200561 and perplexity is 56.74274156258988
At time: 1046.3590519428253 and batch: 850, loss is 4.053151712417603 and perplexity is 57.578642696106904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.666881243387858 and perplexity of 106.36549650912723
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 1050.796644449234 and batch: 50, loss is 4.207951974868775 and perplexity is 67.21873308781646
At time: 1052.5329971313477 and batch: 100, loss is 4.163086786270141 and perplexity is 64.26960325524863
At time: 1054.2511370182037 and batch: 150, loss is 4.164936766624451 and perplexity is 64.38861080552391
At time: 1055.9701533317566 and batch: 200, loss is 4.190531907081604 and perplexity is 66.05791829187898
At time: 1057.6884953975677 and batch: 250, loss is 4.16839183807373 and perplexity is 64.61146281950064
At time: 1059.4055602550507 and batch: 300, loss is 4.160324168205261 and perplexity is 64.0922959172688
At time: 1061.124130487442 and batch: 350, loss is 4.104659872055054 and perplexity is 60.62212176291863
At time: 1062.842689037323 and batch: 400, loss is 4.111630868911743 and perplexity is 61.046194771668866
At time: 1064.5614864826202 and batch: 450, loss is 4.143209633827209 and perplexity is 63.00471934520243
At time: 1066.2774798870087 and batch: 500, loss is 4.140676326751709 and perplexity is 62.84531104423586
At time: 1067.9949736595154 and batch: 550, loss is 4.121882767677307 and perplexity is 61.675253192546556
At time: 1069.7130553722382 and batch: 600, loss is 4.145083780288696 and perplexity is 63.12291013585159
At time: 1071.4344987869263 and batch: 650, loss is 4.120486063957214 and perplexity is 61.58917126644201
At time: 1073.151032924652 and batch: 700, loss is 4.079681282043457 and perplexity is 59.12662213030581
At time: 1074.8681881427765 and batch: 750, loss is 4.077889046669006 and perplexity is 59.02074821038423
At time: 1076.5874519348145 and batch: 800, loss is 4.038234963417053 and perplexity is 56.72613069657775
At time: 1078.3339643478394 and batch: 850, loss is 4.052906632423401 and perplexity is 57.564533051755184
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.666880925496419 and perplexity of 106.36546269645181
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 1082.7921833992004 and batch: 50, loss is 4.207946624755859 and perplexity is 67.21837346096643
At time: 1084.506890296936 and batch: 100, loss is 4.163091192245483 and perplexity is 64.26988642615963
At time: 1086.2235374450684 and batch: 150, loss is 4.164947113990784 and perplexity is 64.38927706151455
At time: 1087.9394435882568 and batch: 200, loss is 4.190539011955261 and perplexity is 66.0583876267098
At time: 1089.6565046310425 and batch: 250, loss is 4.168376369476318 and perplexity is 64.61046337852409
At time: 1091.3753633499146 and batch: 300, loss is 4.160301952362061 and perplexity is 64.0908720686884
At time: 1093.092678785324 and batch: 350, loss is 4.104656071662903 and perplexity is 60.621891375520676
At time: 1094.8091192245483 and batch: 400, loss is 4.111618061065673 and perplexity is 61.045412906410085
At time: 1096.5265798568726 and batch: 450, loss is 4.143201313018799 and perplexity is 63.0041950971849
At time: 1098.2444550991058 and batch: 500, loss is 4.1406740474700925 and perplexity is 62.845167802236936
At time: 1099.9600493907928 and batch: 550, loss is 4.121863174438476 and perplexity is 61.67404478641916
At time: 1101.6759045124054 and batch: 600, loss is 4.145052781105042 and perplexity is 63.12095340749612
At time: 1103.393498659134 and batch: 650, loss is 4.1204416370391845 and perplexity is 61.586435110158575
At time: 1105.111191034317 and batch: 700, loss is 4.0796425151824955 and perplexity is 59.124330021195796
At time: 1106.8279948234558 and batch: 750, loss is 4.077846913337708 and perplexity is 59.018261522032994
At time: 1108.5446865558624 and batch: 800, loss is 4.038183364868164 and perplexity is 56.723203786062555
At time: 1110.2624771595001 and batch: 850, loss is 4.052862920761108 and perplexity is 57.56201686532015
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.666881561279297 and perplexity of 106.3655303218133
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 1114.750875711441 and batch: 50, loss is 4.207946195602417 and perplexity is 67.21834461397626
At time: 1116.4588990211487 and batch: 100, loss is 4.163092336654663 and perplexity is 64.26995997724973
At time: 1118.1687622070312 and batch: 150, loss is 4.164949598312378 and perplexity is 64.38943702538472
At time: 1119.8796632289886 and batch: 200, loss is 4.1905410194396975 and perplexity is 66.05852023802797
At time: 1121.6155107021332 and batch: 250, loss is 4.168374381065369 and perplexity is 64.61033490649898
At time: 1123.324190378189 and batch: 300, loss is 4.160298709869385 and perplexity is 64.09066425484203
At time: 1125.0345854759216 and batch: 350, loss is 4.104656081199646 and perplexity is 60.62189195365613
At time: 1126.7453515529633 and batch: 400, loss is 4.111616606712341 and perplexity is 61.04532412487499
At time: 1128.4533638954163 and batch: 450, loss is 4.143200359344482 and perplexity is 63.00413501173086
At time: 1130.1613709926605 and batch: 500, loss is 4.140674314498901 and perplexity is 62.84518458370948
At time: 1131.8724739551544 and batch: 550, loss is 4.12186053276062 and perplexity is 61.67388186367595
At time: 1133.5831608772278 and batch: 600, loss is 4.145048356056213 and perplexity is 63.12067409481317
At time: 1135.2916975021362 and batch: 650, loss is 4.12043475151062 and perplexity is 61.58601105646038
At time: 1137.000236749649 and batch: 700, loss is 4.079636611938477 and perplexity is 59.12398099687841
At time: 1138.708943605423 and batch: 750, loss is 4.07784038066864 and perplexity is 59.01787597652083
At time: 1140.4202716350555 and batch: 800, loss is 4.03817509174347 and perplexity is 56.722734509865745
At time: 1142.1294484138489 and batch: 850, loss is 4.0528560686111454 and perplexity is 57.561622443099736
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.666881561279297 and perplexity of 106.3655303218133
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 1146.5784583091736 and batch: 50, loss is 4.207946166992188 and perplexity is 67.21834269084405
At time: 1148.2838416099548 and batch: 100, loss is 4.163092741966247 and perplexity is 64.2699860266143
At time: 1149.9906697273254 and batch: 150, loss is 4.164950113296509 and perplexity is 64.38947018493153
At time: 1151.7001106739044 and batch: 200, loss is 4.1905414009094235 and perplexity is 66.05854543735839
At time: 1153.4102487564087 and batch: 250, loss is 4.168374214172363 and perplexity is 64.61032412348692
At time: 1155.1225516796112 and batch: 300, loss is 4.160298256874085 and perplexity is 64.09063522207892
At time: 1156.8304705619812 and batch: 350, loss is 4.104656157493591 and perplexity is 60.621896578739566
At time: 1158.5389232635498 and batch: 400, loss is 4.111616487503052 and perplexity is 61.0453168477057
At time: 1160.2485301494598 and batch: 450, loss is 4.143200387954712 and perplexity is 63.004136814293666
At time: 1161.9594459533691 and batch: 500, loss is 4.14067437171936 and perplexity is 62.845188179739864
At time: 1163.6674871444702 and batch: 550, loss is 4.1218602752685545 and perplexity is 61.67386598314275
At time: 1165.4220926761627 and batch: 600, loss is 4.145047750473022 and perplexity is 63.12063587000552
At time: 1167.132202386856 and batch: 650, loss is 4.1204339075088505 and perplexity is 61.58595907777998
At time: 1168.842780828476 and batch: 700, loss is 4.07963595867157 and perplexity is 59.12394237315087
At time: 1170.5507595539093 and batch: 750, loss is 4.077839584350586 and perplexity is 59.01782897953938
At time: 1172.2600648403168 and batch: 800, loss is 4.038174104690552 and perplexity is 56.72267852155278
At time: 1173.9708161354065 and batch: 850, loss is 4.052855219841003 and perplexity is 57.56157358653401
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.666881561279297 and perplexity of 106.3655303218133
Annealing...
Model not improving. Stopping early with 106.36539507113342loss at 36 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -106.36539507113342
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9fc4b2c860>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'num_layers': 1, 'anneal': 4.7874854656094, 'wordvec_source': '', 'dropout': 0.11864408616762978, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 13.47088802086473, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.1340012550354004 and batch: 50, loss is 6.868627882003784 and perplexity is 961.6281928163633
At time: 3.713571786880493 and batch: 100, loss is 5.842635622024536 and perplexity is 344.68660814869014
At time: 5.296594858169556 and batch: 150, loss is 5.681805181503296 and perplexity is 293.47873441645254
At time: 6.881304502487183 and batch: 200, loss is 5.64943263053894 and perplexity is 284.13021327738335
At time: 8.466078042984009 and batch: 250, loss is 5.665643930435181 and perplexity is 288.77387154471245
At time: 10.053129196166992 and batch: 300, loss is 5.589558687210083 and perplexity is 267.6174906282065
At time: 11.643289804458618 and batch: 350, loss is 5.546002025604248 and perplexity is 256.211179823402
At time: 13.232609987258911 and batch: 400, loss is 5.559365329742431 and perplexity is 259.657986814573
At time: 14.823982238769531 and batch: 450, loss is 5.554400682449341 and perplexity is 258.372071193199
At time: 16.418335676193237 and batch: 500, loss is 5.54869589805603 and perplexity is 256.9023105536258
At time: 18.054666757583618 and batch: 550, loss is 5.515883855819702 and perplexity is 248.60961522427402
At time: 19.647477626800537 and batch: 600, loss is 5.546685590744018 and perplexity is 256.3863767267746
At time: 21.24205255508423 and batch: 650, loss is 5.544528322219849 and perplexity is 255.83387862388787
At time: 22.839986085891724 and batch: 700, loss is 5.487042121887207 and perplexity is 241.54169828336651
At time: 24.43764853477478 and batch: 750, loss is 5.4860664749145505 and perplexity is 241.30615377949147
At time: 26.03655457496643 and batch: 800, loss is 5.491050834655762 and perplexity is 242.51191293007395
At time: 27.6349458694458 and batch: 850, loss is 5.480301771163941 and perplexity is 239.9190971163259
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.128133455912272 and perplexity of 168.70193438608118
Finished 1 epochs...
Completing Train Step...
At time: 31.86014699935913 and batch: 50, loss is 5.3894949150085445 and perplexity is 219.09269716970894
At time: 33.44116520881653 and batch: 100, loss is 5.3019068145751955 and perplexity is 200.71917956209475
At time: 35.02343273162842 and batch: 150, loss is 5.294182224273682 and perplexity is 199.17467913077783
At time: 36.60707449913025 and batch: 200, loss is 5.300708246231079 and perplexity is 200.47874802301345
At time: 38.189555644989014 and batch: 250, loss is 5.336460428237915 and perplexity is 207.77596923083652
At time: 39.77183651924133 and batch: 300, loss is 5.295159120559692 and perplexity is 199.3693472048659
At time: 41.41907596588135 and batch: 350, loss is 5.261219968795777 and perplexity is 192.71645600605729
At time: 43.11930274963379 and batch: 400, loss is 5.281660461425782 and perplexity is 196.69621081374962
At time: 44.82959532737732 and batch: 450, loss is 5.287480516433716 and perplexity is 197.8443314021686
At time: 46.538132429122925 and batch: 500, loss is 5.295295658111573 and perplexity is 199.39657046590972
At time: 48.24685454368591 and batch: 550, loss is 5.272163047790527 and perplexity is 194.836948605854
At time: 49.95696139335632 and batch: 600, loss is 5.306369085311889 and perplexity is 201.61684420516804
At time: 51.66809391975403 and batch: 650, loss is 5.31378005027771 and perplexity is 203.1165699174093
At time: 53.37603521347046 and batch: 700, loss is 5.2622700023651126 and perplexity is 192.91892103316198
At time: 55.086345911026 and batch: 750, loss is 5.271534013748169 and perplexity is 194.7144280713052
At time: 56.79891633987427 and batch: 800, loss is 5.26764705657959 and perplexity is 193.95905044058375
At time: 58.51194143295288 and batch: 850, loss is 5.267690277099609 and perplexity is 193.9674336327678
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.097282409667969 and perplexity of 163.57676807736308
Finished 2 epochs...
Completing Train Step...
At time: 62.96224570274353 and batch: 50, loss is 5.251994333267212 and perplexity is 190.94670033530622
At time: 64.63694047927856 and batch: 100, loss is 5.173664951324463 and perplexity is 176.56073967777283
At time: 66.31614184379578 and batch: 150, loss is 5.189884653091431 and perplexity is 179.44785298292686
At time: 67.99918460845947 and batch: 200, loss is 5.204864158630371 and perplexity is 182.15612675030818
At time: 69.69552493095398 and batch: 250, loss is 5.231135005950928 and perplexity is 187.00493482612833
At time: 71.3937041759491 and batch: 300, loss is 5.1935651493072506 and perplexity is 180.10952702424763
At time: 73.10038685798645 and batch: 350, loss is 5.15555661201477 and perplexity is 173.3922921370057
At time: 74.85539412498474 and batch: 400, loss is 5.174647750854493 and perplexity is 176.73434878725135
At time: 76.56490302085876 and batch: 450, loss is 5.195778121948242 and perplexity is 180.50854582593882
At time: 78.2745795249939 and batch: 500, loss is 5.2009304428100585 and perplexity is 181.44098381945784
At time: 79.98397517204285 and batch: 550, loss is 5.177245607376099 and perplexity is 177.19407616216682
At time: 81.69222235679626 and batch: 600, loss is 5.215289821624756 and perplexity is 184.06515931350515
At time: 83.4017162322998 and batch: 650, loss is 5.217927532196045 and perplexity is 184.55131081163807
At time: 85.1115996837616 and batch: 700, loss is 5.1784677314758305 and perplexity is 177.41076169430983
At time: 86.82238602638245 and batch: 750, loss is 5.173202676773071 and perplexity is 176.47913900345554
At time: 88.53155088424683 and batch: 800, loss is 5.162784509658813 and perplexity is 174.65009403262837
At time: 90.24164748191833 and batch: 850, loss is 5.168665523529053 and perplexity is 175.6802398398853
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.09238878885905 and perplexity of 162.7782408395596
Finished 3 epochs...
Completing Train Step...
At time: 94.65684294700623 and batch: 50, loss is 5.181472311019897 and perplexity is 177.9446080300759
At time: 96.35326075553894 and batch: 100, loss is 5.105316543579102 and perplexity is 164.89625912326707
At time: 98.03557252883911 and batch: 150, loss is 5.121397294998169 and perplexity is 167.56934992777926
At time: 99.72493004798889 and batch: 200, loss is 5.131613931655884 and perplexity is 169.29012036637133
At time: 101.42220306396484 and batch: 250, loss is 5.143033990859985 and perplexity is 171.23450492520408
At time: 103.13745665550232 and batch: 300, loss is 5.110511283874512 and perplexity is 165.7550811123954
At time: 104.85591959953308 and batch: 350, loss is 5.073419647216797 and perplexity is 159.7195791418326
At time: 106.57445502281189 and batch: 400, loss is 5.096794919967651 and perplexity is 163.49704552125547
At time: 108.29083395004272 and batch: 450, loss is 5.123677434921265 and perplexity is 167.95186742341647
At time: 110.00522780418396 and batch: 500, loss is 5.128356723785401 and perplexity is 168.73960431324994
At time: 111.72015523910522 and batch: 550, loss is 5.117013139724731 and perplexity is 166.83630793771854
At time: 113.4376494884491 and batch: 600, loss is 5.149332656860351 and perplexity is 172.31645773210417
At time: 115.1559579372406 and batch: 650, loss is 5.147264585494995 and perplexity is 171.9604632379786
At time: 116.8715591430664 and batch: 700, loss is 5.1150843524932865 and perplexity is 166.51482633170048
At time: 118.61473488807678 and batch: 750, loss is 5.114272289276123 and perplexity is 166.37966065508905
At time: 120.33212780952454 and batch: 800, loss is 5.091689329147339 and perplexity is 162.66442382799664
At time: 122.05064606666565 and batch: 850, loss is 5.1124872207641605 and perplexity is 166.08292648602804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.084118207295735 and perplexity of 161.43752202648173
Finished 4 epochs...
Completing Train Step...
At time: 126.50132131576538 and batch: 50, loss is 5.106013450622559 and perplexity is 165.0112165403539
At time: 128.1803343296051 and batch: 100, loss is 5.029888687133789 and perplexity is 152.91599023106048
At time: 129.8662486076355 and batch: 150, loss is 5.0530514144897465 and perplexity is 156.4992808073953
At time: 131.55526399612427 and batch: 200, loss is 5.081553764343262 and perplexity is 161.02405509362967
At time: 133.25803685188293 and batch: 250, loss is 5.0928003692626955 and perplexity is 162.8452509627099
At time: 134.96137380599976 and batch: 300, loss is 5.054371776580811 and perplexity is 156.70605300209613
At time: 136.67754435539246 and batch: 350, loss is 5.024323997497558 and perplexity is 152.0674234002417
At time: 138.395281791687 and batch: 400, loss is 5.048555030822754 and perplexity is 155.7971796382833
At time: 140.110107421875 and batch: 450, loss is 5.050912199020385 and perplexity is 156.16485295909672
At time: 141.8256480693817 and batch: 500, loss is 5.077069406509399 and perplexity is 160.3035822471168
At time: 143.54294276237488 and batch: 550, loss is 5.058565368652344 and perplexity is 157.36459412483902
At time: 145.26173305511475 and batch: 600, loss is 5.101686954498291 and perplexity is 164.29883831357466
At time: 146.97685980796814 and batch: 650, loss is 5.098013591766358 and perplexity is 163.69641621885253
At time: 148.69214963912964 and batch: 700, loss is 5.0677890205383305 and perplexity is 158.82278494443543
At time: 150.40846133232117 and batch: 750, loss is 5.059735832214355 and perplexity is 157.54889148384407
At time: 152.12697219848633 and batch: 800, loss is 5.03496994972229 and perplexity is 153.69497396579683
At time: 153.84335708618164 and batch: 850, loss is 5.06085825920105 and perplexity is 157.725827891893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.117578506469727 and perplexity of 166.93065830685168
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 158.29535388946533 and batch: 50, loss is 5.04528473854065 and perplexity is 155.28850952745162
At time: 159.97102999687195 and batch: 100, loss is 4.927301998138428 and perplexity is 138.00666735246733
At time: 161.6921021938324 and batch: 150, loss is 4.912387418746948 and perplexity is 135.96362935367233
At time: 163.40110874176025 and batch: 200, loss is 4.925361137390137 and perplexity is 137.7390753921669
At time: 165.1120264530182 and batch: 250, loss is 4.91305495262146 and perplexity is 136.05441998150587
At time: 166.82313537597656 and batch: 300, loss is 4.86030029296875 and perplexity is 129.06295298608666
At time: 168.5312774181366 and batch: 350, loss is 4.808745727539063 and perplexity is 122.57777512963081
At time: 170.2411527633667 and batch: 400, loss is 4.835157909393311 and perplexity is 125.85845587020093
At time: 171.95163917541504 and batch: 450, loss is 4.829304904937744 and perplexity is 125.12395737555164
At time: 173.66172194480896 and batch: 500, loss is 4.821141090393066 and perplexity is 124.10662689137924
At time: 175.36967873573303 and batch: 550, loss is 4.787152099609375 and perplexity is 119.95924973994501
At time: 177.07914471626282 and batch: 600, loss is 4.796770162582398 and perplexity is 121.11859173301859
At time: 178.7901258468628 and batch: 650, loss is 4.766536598205566 and perplexity is 117.51154666031374
At time: 180.50149154663086 and batch: 700, loss is 4.7242530155181885 and perplexity is 112.64632188433676
At time: 182.2084300518036 and batch: 750, loss is 4.694470338821411 and perplexity is 109.340879682606
At time: 183.91831469535828 and batch: 800, loss is 4.640708637237549 and perplexity is 103.61774896812193
At time: 185.62884330749512 and batch: 850, loss is 4.672188177108764 and perplexity is 106.93147161716391
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.896379152933757 and perplexity of 133.80441617394035
Finished 6 epochs...
Completing Train Step...
At time: 190.05913662910461 and batch: 50, loss is 4.852859334945679 and perplexity is 128.10616509445666
At time: 191.7432894706726 and batch: 100, loss is 4.781231689453125 and perplexity is 119.25113999784641
At time: 193.45399737358093 and batch: 150, loss is 4.7791839218139645 and perplexity is 119.00719123286602
At time: 195.16566014289856 and batch: 200, loss is 4.80229097366333 and perplexity is 121.78911380612155
At time: 196.87366604804993 and batch: 250, loss is 4.799807844161987 and perplexity is 121.48707082580276
At time: 198.58362889289856 and batch: 300, loss is 4.75746859550476 and perplexity is 116.45076847639575
At time: 200.2939739227295 and batch: 350, loss is 4.71169487953186 and perplexity is 111.24053953378495
At time: 202.00533533096313 and batch: 400, loss is 4.740666360855102 and perplexity is 114.5104815647852
At time: 203.71283650398254 and batch: 450, loss is 4.749137706756592 and perplexity is 115.48465993857353
At time: 205.46577262878418 and batch: 500, loss is 4.742278289794922 and perplexity is 114.69521317104405
At time: 207.17685532569885 and batch: 550, loss is 4.718487119674682 and perplexity is 111.99868382624743
At time: 208.88825345039368 and batch: 600, loss is 4.7383539009094235 and perplexity is 114.2459865976798
At time: 210.59692978858948 and batch: 650, loss is 4.707904787063598 and perplexity is 110.81972556839523
At time: 212.30557370185852 and batch: 700, loss is 4.6749387550354005 and perplexity is 107.22599983837668
At time: 214.01601243019104 and batch: 750, loss is 4.660063991546631 and perplexity is 105.64284218922687
At time: 215.72876930236816 and batch: 800, loss is 4.622110328674316 and perplexity is 101.70844405606552
At time: 217.43718695640564 and batch: 850, loss is 4.657993564605713 and perplexity is 105.42434267433714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.885393778483073 and perplexity of 132.34256873098414
Finished 7 epochs...
Completing Train Step...
At time: 221.86847829818726 and batch: 50, loss is 4.784372930526733 and perplexity is 119.62632554249518
At time: 223.59013295173645 and batch: 100, loss is 4.718774490356445 and perplexity is 112.03087358935066
At time: 225.30096697807312 and batch: 150, loss is 4.7197949504852295 and perplexity is 112.14525497993996
At time: 227.018226146698 and batch: 200, loss is 4.742171592712403 and perplexity is 114.68297617925703
At time: 228.73606729507446 and batch: 250, loss is 4.738656930923462 and perplexity is 114.2806118065761
At time: 230.45333170890808 and batch: 300, loss is 4.702708168029785 and perplexity is 110.24533141958622
At time: 232.16918516159058 and batch: 350, loss is 4.659014596939087 and perplexity is 105.53203930844894
At time: 233.88568997383118 and batch: 400, loss is 4.688400764465332 and perplexity is 108.67923705899078
At time: 235.60317707061768 and batch: 450, loss is 4.704312725067139 and perplexity is 110.42236833688332
At time: 237.32099318504333 and batch: 500, loss is 4.698069515228272 and perplexity is 109.7351258522302
At time: 239.0364408493042 and batch: 550, loss is 4.679588365554809 and perplexity is 107.72571982655444
At time: 240.75398993492126 and batch: 600, loss is 4.702182083129883 and perplexity is 110.18734826880903
At time: 242.471839427948 and batch: 650, loss is 4.671827325820923 and perplexity is 106.89289221905143
At time: 244.18955183029175 and batch: 700, loss is 4.644818496704102 and perplexity is 104.04447965554225
At time: 245.90560340881348 and batch: 750, loss is 4.634875278472901 and perplexity is 103.01506899725307
At time: 247.6506335735321 and batch: 800, loss is 4.601493816375733 and perplexity is 99.63303799605632
At time: 249.3691885471344 and batch: 850, loss is 4.638653984069824 and perplexity is 103.40506899857945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.88258171081543 and perplexity of 131.97093524655364
Finished 8 epochs...
Completing Train Step...
At time: 253.79893708229065 and batch: 50, loss is 4.7360696029663085 and perplexity is 113.98531256741128
At time: 255.5103006362915 and batch: 100, loss is 4.67291296005249 and perplexity is 107.00900181682876
At time: 257.21656942367554 and batch: 150, loss is 4.678098497390747 and perplexity is 107.56534220655519
At time: 258.92755937576294 and batch: 200, loss is 4.700346031188965 and perplexity is 109.98522418609413
At time: 260.6371331214905 and batch: 250, loss is 4.697624454498291 and perplexity is 109.68629792351662
At time: 262.35079050064087 and batch: 300, loss is 4.6637480926513675 and perplexity is 106.03275890613226
At time: 264.06330609321594 and batch: 350, loss is 4.62100492477417 and perplexity is 101.5960772621162
At time: 265.7743263244629 and batch: 400, loss is 4.6512187957763675 and perplexity is 104.71253102594554
At time: 267.4858455657959 and batch: 450, loss is 4.669685287475586 and perplexity is 106.66416859991052
At time: 269.19776225090027 and batch: 500, loss is 4.665117092132569 and perplexity is 106.17801710457782
At time: 270.9088144302368 and batch: 550, loss is 4.649420557022094 and perplexity is 104.52440209565871
At time: 272.6196324825287 and batch: 600, loss is 4.6734762859344485 and perplexity is 107.06929973925031
At time: 274.3286201953888 and batch: 650, loss is 4.643450927734375 and perplexity is 103.90228890368496
At time: 276.03984785079956 and batch: 700, loss is 4.618524808883667 and perplexity is 101.34441941579249
At time: 277.75147795677185 and batch: 750, loss is 4.612512693405152 and perplexity is 100.7369529721314
At time: 279.46311211586 and batch: 800, loss is 4.581729135513306 and perplexity is 97.6831556728093
At time: 281.17356753349304 and batch: 850, loss is 4.618612232208252 and perplexity is 101.35327966915679
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.882145881652832 and perplexity of 131.91343099628276
Finished 9 epochs...
Completing Train Step...
At time: 285.5980443954468 and batch: 50, loss is 4.697513027191162 and perplexity is 109.67407655561979
At time: 287.31808280944824 and batch: 100, loss is 4.636342496871948 and perplexity is 103.16632553792013
At time: 289.02655959129333 and batch: 150, loss is 4.644080181121826 and perplexity is 103.96769034582294
At time: 290.76381254196167 and batch: 200, loss is 4.664806184768676 and perplexity is 106.14501070840821
At time: 292.4752371311188 and batch: 250, loss is 4.663131809234619 and perplexity is 105.9674328069497
At time: 294.18652153015137 and batch: 300, loss is 4.633130016326905 and perplexity is 102.83543749449154
At time: 295.89630794525146 and batch: 350, loss is 4.589175920486451 and perplexity is 98.41329635479686
At time: 297.6057424545288 and batch: 400, loss is 4.619737014770508 and perplexity is 101.46734420763242
At time: 299.3171956539154 and batch: 450, loss is 4.640006771087647 and perplexity is 103.54504869350748
At time: 301.02755331993103 and batch: 500, loss is 4.635996828079223 and perplexity is 103.13067032152443
At time: 302.7355980873108 and batch: 550, loss is 4.62162691116333 and perplexity is 101.65928829552858
At time: 304.4457869529724 and batch: 600, loss is 4.647976055145263 and perplexity is 104.37352539772833
At time: 306.1576292514801 and batch: 650, loss is 4.618292245864868 and perplexity is 101.32085319209708
At time: 307.8693594932556 and batch: 700, loss is 4.595839786529541 and perplexity is 99.07129936552464
At time: 309.5775625705719 and batch: 750, loss is 4.592242803573608 and perplexity is 98.71558172895399
At time: 311.2885682582855 and batch: 800, loss is 4.560644922256469 and perplexity is 95.64514362559926
At time: 313.00012850761414 and batch: 850, loss is 4.597448978424072 and perplexity is 99.23085243877026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.882935206095378 and perplexity of 132.0175945956976
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 317.4247453212738 and batch: 50, loss is 4.6729910278320315 and perplexity is 107.01735609808738
At time: 319.12308979034424 and batch: 100, loss is 4.616000928878784 and perplexity is 101.08896077111056
At time: 320.84055376052856 and batch: 150, loss is 4.615972814559936 and perplexity is 101.08611876378616
At time: 322.55756092071533 and batch: 200, loss is 4.630325288772583 and perplexity is 102.54741620881792
At time: 324.2726135253906 and batch: 250, loss is 4.61969464302063 and perplexity is 101.463044949787
At time: 325.9896469116211 and batch: 300, loss is 4.577972497940063 and perplexity is 97.31688386589738
At time: 327.70692920684814 and batch: 350, loss is 4.5252226543426515 and perplexity is 92.3164782053057
At time: 329.4240517616272 and batch: 400, loss is 4.553152532577514 and perplexity is 94.93121080985556
At time: 331.13909935951233 and batch: 450, loss is 4.5730768489837645 and perplexity is 96.84161887947403
At time: 332.8552575111389 and batch: 500, loss is 4.560878839492798 and perplexity is 95.66751929018918
At time: 334.5999791622162 and batch: 550, loss is 4.536059236526489 and perplexity is 93.322313372986
At time: 336.3179814815521 and batch: 600, loss is 4.547145690917969 and perplexity is 94.36268329482614
At time: 338.0334577560425 and batch: 650, loss is 4.512090120315552 and perplexity is 91.11205478962562
At time: 339.7505226135254 and batch: 700, loss is 4.483231325149536 and perplexity is 88.52024873860059
At time: 341.46826362609863 and batch: 750, loss is 4.465021991729737 and perplexity is 86.92294110456169
At time: 343.18795228004456 and batch: 800, loss is 4.4206451797485355 and perplexity is 83.1499146972569
At time: 344.90465211868286 and batch: 850, loss is 4.472058162689209 and perplexity is 87.53670251140937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.836646715799968 and perplexity of 126.0459743003156
Finished 11 epochs...
Completing Train Step...
At time: 349.3539607524872 and batch: 50, loss is 4.625689888000489 and perplexity is 102.07316785132709
At time: 351.06064319610596 and batch: 100, loss is 4.572561855316162 and perplexity is 96.79175889887986
At time: 352.7767243385315 and batch: 150, loss is 4.57500810623169 and perplexity is 97.02882567188689
At time: 354.4948961734772 and batch: 200, loss is 4.5945188331604 and perplexity is 98.94051719643305
At time: 356.2144720554352 and batch: 250, loss is 4.587509670257568 and perplexity is 98.24945171821028
At time: 357.9315097332001 and batch: 300, loss is 4.549842386245728 and perplexity is 94.61749412114565
At time: 359.6479067802429 and batch: 350, loss is 4.498172445297241 and perplexity is 89.85277030398731
At time: 361.3642041683197 and batch: 400, loss is 4.528029928207397 and perplexity is 92.57599994572016
At time: 363.08172821998596 and batch: 450, loss is 4.551796751022339 and perplexity is 94.80259203441243
At time: 364.7969546318054 and batch: 500, loss is 4.542260265350341 and perplexity is 93.9028056927977
At time: 366.5126929283142 and batch: 550, loss is 4.521088600158691 and perplexity is 91.93562465934177
At time: 368.22922468185425 and batch: 600, loss is 4.536844263076782 and perplexity is 93.39560262996105
At time: 369.94742155075073 and batch: 650, loss is 4.5036188507080075 and perplexity is 90.34348000732852
At time: 371.665358543396 and batch: 700, loss is 4.478170671463013 and perplexity is 88.07341001713712
At time: 373.37976455688477 and batch: 750, loss is 4.465896778106689 and perplexity is 86.9990133779276
At time: 375.09722995758057 and batch: 800, loss is 4.426320915222168 and perplexity is 83.62319344972069
At time: 376.81532883644104 and batch: 850, loss is 4.478790340423584 and perplexity is 88.12800328870736
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.833553632100423 and perplexity of 125.65670588206328
Finished 12 epochs...
Completing Train Step...
At time: 381.28249311447144 and batch: 50, loss is 4.6071733951568605 and perplexity is 100.20052169326893
At time: 382.99015378952026 and batch: 100, loss is 4.554948587417602 and perplexity is 95.10186567738133
At time: 384.7024874687195 and batch: 150, loss is 4.557201385498047 and perplexity is 95.31635248484831
At time: 386.41697692871094 and batch: 200, loss is 4.577714853286743 and perplexity is 97.29181392079886
At time: 388.1270909309387 and batch: 250, loss is 4.571735019683838 and perplexity is 96.71176110077951
At time: 389.8383491039276 and batch: 300, loss is 4.534150094985962 and perplexity is 93.14431783127239
At time: 391.5493710041046 and batch: 350, loss is 4.483598041534424 and perplexity is 88.5527165170767
At time: 393.2613685131073 and batch: 400, loss is 4.514842615127564 and perplexity is 91.36318570746799
At time: 394.96967935562134 and batch: 450, loss is 4.540535507202148 and perplexity is 93.74098565387122
At time: 396.67872190475464 and batch: 500, loss is 4.532560482025146 and perplexity is 92.99637203585173
At time: 398.38788390159607 and batch: 550, loss is 4.512444124221802 and perplexity is 91.14431452262649
At time: 400.09951281547546 and batch: 600, loss is 4.530413417816162 and perplexity is 92.79691705183899
At time: 401.80636835098267 and batch: 650, loss is 4.498371276855469 and perplexity is 89.87063764655485
At time: 403.5163004398346 and batch: 700, loss is 4.475263404846191 and perplexity is 87.81772897892704
At time: 405.2260844707489 and batch: 750, loss is 4.465316734313965 and perplexity is 86.94856477285916
At time: 406.93726539611816 and batch: 800, loss is 4.427000494003296 and perplexity is 83.68004131171182
At time: 408.6451497077942 and batch: 850, loss is 4.4797814846038815 and perplexity is 88.21539414760468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.83281135559082 and perplexity of 125.5634684692646
Finished 13 epochs...
Completing Train Step...
At time: 413.0639235973358 and batch: 50, loss is 4.594274101257324 and perplexity is 98.91630625808357
At time: 414.8010923862457 and batch: 100, loss is 4.542597789764404 and perplexity is 93.9345055317018
At time: 416.50884318351746 and batch: 150, loss is 4.544785823822021 and perplexity is 94.14026244847305
At time: 418.219247341156 and batch: 200, loss is 4.566009378433227 and perplexity is 96.15960648007628
At time: 419.92992091178894 and batch: 250, loss is 4.560362644195557 and perplexity is 95.61814891010816
At time: 421.66764998435974 and batch: 300, loss is 4.523221378326416 and perplexity is 92.1319121969336
At time: 423.37571239471436 and batch: 350, loss is 4.4734575557708744 and perplexity is 87.65928651889692
At time: 425.08532428741455 and batch: 400, loss is 4.505860671997071 and perplexity is 90.54624113632569
At time: 426.7956032752991 and batch: 450, loss is 4.532798833847046 and perplexity is 93.0185405324024
At time: 428.50643157958984 and batch: 500, loss is 4.525442123413086 and perplexity is 92.33674104041467
At time: 430.2141408920288 and batch: 550, loss is 4.5064667797088624 and perplexity is 90.6011385465434
At time: 431.9235234260559 and batch: 600, loss is 4.525286912918091 and perplexity is 92.3224105212838
At time: 433.63346123695374 and batch: 650, loss is 4.4937069606781 and perplexity is 89.45242866517549
At time: 435.34439158439636 and batch: 700, loss is 4.472135887145996 and perplexity is 87.54350651847646
At time: 437.05201721191406 and batch: 750, loss is 4.46328351020813 and perplexity is 86.77195845594647
At time: 438.76207160949707 and batch: 800, loss is 4.425665473937988 and perplexity is 83.56840131489396
At time: 440.47323417663574 and batch: 850, loss is 4.478651285171509 and perplexity is 88.11574947899324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.832495371500651 and perplexity of 125.52379867876328
Finished 14 epochs...
Completing Train Step...
At time: 444.9157176017761 and batch: 50, loss is 4.583321113586425 and perplexity is 97.83878896427761
At time: 446.6579158306122 and batch: 100, loss is 4.5323795890808105 and perplexity is 92.9795511697355
At time: 448.3764109611511 and batch: 150, loss is 4.534673318862915 and perplexity is 93.19306591433266
At time: 450.0970993041992 and batch: 200, loss is 4.556359758377075 and perplexity is 95.2361654060684
At time: 451.8126382827759 and batch: 250, loss is 4.551291160583496 and perplexity is 94.75467286505997
At time: 453.5291781425476 and batch: 300, loss is 4.513993911743164 and perplexity is 91.28567835757595
At time: 455.24644112586975 and batch: 350, loss is 4.464875450134278 and perplexity is 86.91020421135535
At time: 456.96328592300415 and batch: 400, loss is 4.498201951980591 and perplexity is 89.85542160034407
At time: 458.67825627326965 and batch: 450, loss is 4.525954828262329 and perplexity is 92.38409467349076
At time: 460.39457607269287 and batch: 500, loss is 4.519478101730346 and perplexity is 91.7876816432931
At time: 462.11260056495667 and batch: 550, loss is 4.5011794471740725 and perplexity is 90.1233643874944
At time: 463.8585171699524 and batch: 600, loss is 4.520326833724976 and perplexity is 91.86561785421611
At time: 465.57452297210693 and batch: 650, loss is 4.489182415008545 and perplexity is 89.04861130037771
At time: 467.2913022041321 and batch: 700, loss is 4.4685265064239506 and perplexity is 87.22809823042405
At time: 469.0089633464813 and batch: 750, loss is 4.460758409500122 and perplexity is 86.5531269243168
At time: 470.72687554359436 and batch: 800, loss is 4.423497142791748 and perplexity is 83.38739366066831
At time: 472.44248723983765 and batch: 850, loss is 4.476550989151001 and perplexity is 87.93087453503365
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.832324981689453 and perplexity of 125.50241252444867
Finished 15 epochs...
Completing Train Step...
At time: 476.8743908405304 and batch: 50, loss is 4.57390362739563 and perplexity is 96.92171854710894
At time: 478.6085147857666 and batch: 100, loss is 4.523658199310303 and perplexity is 92.17216614071042
At time: 480.3178753852844 and batch: 150, loss is 4.525996427536011 and perplexity is 92.38793786466536
At time: 482.0288417339325 and batch: 200, loss is 4.548166799545288 and perplexity is 94.45908705581512
At time: 483.7401168346405 and batch: 250, loss is 4.543503656387329 and perplexity is 94.01963621770597
At time: 485.44936537742615 and batch: 300, loss is 4.506039953231811 and perplexity is 90.56247583348251
At time: 487.15920329093933 and batch: 350, loss is 4.457270011901856 and perplexity is 86.25172122134514
At time: 488.8693964481354 and batch: 400, loss is 4.491257276535034 and perplexity is 89.2335666499681
At time: 490.58111810684204 and batch: 450, loss is 4.520186347961426 and perplexity is 91.85271294924678
At time: 492.28986859321594 and batch: 500, loss is 4.513672161102295 and perplexity is 91.25631185666082
At time: 493.998987197876 and batch: 550, loss is 4.495973529815674 and perplexity is 89.65540872654228
At time: 495.70945048332214 and batch: 600, loss is 4.5157602024078365 and perplexity is 91.4470578786923
At time: 497.4211449623108 and batch: 650, loss is 4.484729862213134 and perplexity is 88.65299905300537
At time: 499.1309497356415 and batch: 700, loss is 4.464776515960693 and perplexity is 86.9016062474489
At time: 500.8395507335663 and batch: 750, loss is 4.457693862915039 and perplexity is 86.28828684941541
At time: 502.5503590106964 and batch: 800, loss is 4.4208761405944825 and perplexity is 83.16912128979585
At time: 504.26247096061707 and batch: 850, loss is 4.474215106964111 and perplexity is 87.72571807547308
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.83178170522054 and perplexity of 125.43424853455474
Finished 16 epochs...
Completing Train Step...
At time: 508.6956419944763 and batch: 50, loss is 4.564992437362671 and perplexity is 96.06186753271624
At time: 510.39716124534607 and batch: 100, loss is 4.515545301437378 and perplexity is 91.42740792868085
At time: 512.1085841655731 and batch: 150, loss is 4.51836742401123 and perplexity is 91.68579170431941
At time: 513.8178277015686 and batch: 200, loss is 4.54086501121521 and perplexity is 93.77187877425663
At time: 515.5274062156677 and batch: 250, loss is 4.536302900314331 and perplexity is 93.3450554119458
At time: 517.2382423877716 and batch: 300, loss is 4.498737030029297 and perplexity is 89.90351412948047
At time: 518.9502811431885 and batch: 350, loss is 4.45046817779541 and perplexity is 85.66704202184003
At time: 520.659077167511 and batch: 400, loss is 4.485259199142456 and perplexity is 88.69993878166916
At time: 522.3685564994812 and batch: 450, loss is 4.514508733749389 and perplexity is 91.3326863329795
At time: 524.0790102481842 and batch: 500, loss is 4.508385877609253 and perplexity is 90.77517794721062
At time: 525.7902457714081 and batch: 550, loss is 4.490988702774048 and perplexity is 89.20960407337039
At time: 527.4996712207794 and batch: 600, loss is 4.510971927642823 and perplexity is 91.01023089753724
At time: 529.208416223526 and batch: 650, loss is 4.480470609664917 and perplexity is 88.27620653774535
At time: 530.9190967082977 and batch: 700, loss is 4.460946588516236 and perplexity is 86.5694159391603
At time: 532.6293172836304 and batch: 750, loss is 4.454347696304321 and perplexity is 86.00003440437683
At time: 534.3389291763306 and batch: 800, loss is 4.417971220016479 and perplexity is 82.92787217231921
At time: 536.0480206012726 and batch: 850, loss is 4.471193828582764 and perplexity is 87.46107424261736
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.831991831461589 and perplexity of 125.46060833104356
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 540.5021295547485 and batch: 50, loss is 4.561093826293945 and perplexity is 95.68808875513757
At time: 542.2095258235931 and batch: 100, loss is 4.513262929916382 and perplexity is 91.21897456825019
At time: 543.9252355098724 and batch: 150, loss is 4.515173254013061 and perplexity is 91.39339892392266
At time: 545.6428725719452 and batch: 200, loss is 4.538377914428711 and perplexity is 93.53894881563757
At time: 547.3601741790771 and batch: 250, loss is 4.528190040588379 and perplexity is 92.59082369619459
At time: 549.0743849277496 and batch: 300, loss is 4.487449359893799 and perplexity is 88.89441879974632
At time: 550.8177745342255 and batch: 350, loss is 4.436655178070068 and perplexity is 84.49185828549544
At time: 552.5338997840881 and batch: 400, loss is 4.468915319442749 and perplexity is 87.26202024485411
At time: 554.2537472248077 and batch: 450, loss is 4.498502759933472 and perplexity is 89.88245489148156
At time: 555.9680573940277 and batch: 500, loss is 4.489632825851441 and perplexity is 89.08872879445113
At time: 557.6827054023743 and batch: 550, loss is 4.468728284835816 and perplexity is 87.24570075339986
At time: 559.3993511199951 and batch: 600, loss is 4.481889810562134 and perplexity is 88.40157715126745
At time: 561.123820066452 and batch: 650, loss is 4.448473834991455 and perplexity is 85.49636282603231
At time: 562.854341506958 and batch: 700, loss is 4.427415628433227 and perplexity is 83.71478698952298
At time: 564.5698618888855 and batch: 750, loss is 4.416252374649048 and perplexity is 82.78545441551923
At time: 566.2870874404907 and batch: 800, loss is 4.375800228118896 and perplexity is 79.50343498766578
At time: 568.0061631202698 and batch: 850, loss is 4.43581335067749 and perplexity is 84.42076065490613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.824012756347656 and perplexity of 124.46353187724367
Finished 18 epochs...
Completing Train Step...
At time: 572.4607148170471 and batch: 50, loss is 4.55161187171936 and perplexity is 94.78506661736988
At time: 574.1835608482361 and batch: 100, loss is 4.50364164352417 and perplexity is 90.34553921312727
At time: 575.9084231853485 and batch: 150, loss is 4.5055702209472654 and perplexity is 90.51994570448448
At time: 577.6421852111816 and batch: 200, loss is 4.52982494354248 and perplexity is 92.74232451820322
At time: 579.3758511543274 and batch: 250, loss is 4.520982580184937 and perplexity is 91.92587816349901
At time: 581.1128778457642 and batch: 300, loss is 4.480847768783569 and perplexity is 88.30950699339157
At time: 582.8490474224091 and batch: 350, loss is 4.4309152984619145 and perplexity is 84.0082743754672
At time: 584.5829784870148 and batch: 400, loss is 4.463586111068725 and perplexity is 86.7982196983877
At time: 586.3187730312347 and batch: 450, loss is 4.494130115509034 and perplexity is 89.49028890231007
At time: 588.0535705089569 and batch: 500, loss is 4.485793466567993 and perplexity is 88.74734093119197
At time: 589.7880158424377 and batch: 550, loss is 4.465948314666748 and perplexity is 87.00349712334301
At time: 591.5221562385559 and batch: 600, loss is 4.480537338256836 and perplexity is 88.28209728124587
At time: 593.257319688797 and batch: 650, loss is 4.448144969940185 and perplexity is 85.46825068309214
At time: 595.0268845558167 and batch: 700, loss is 4.4281884384155275 and perplexity is 83.77950761772728
At time: 596.7439551353455 and batch: 750, loss is 4.418185720443725 and perplexity is 82.94566214424086
At time: 598.459000825882 and batch: 800, loss is 4.378756294250488 and perplexity is 79.73880010521289
At time: 600.1756451129913 and batch: 850, loss is 4.438852090835571 and perplexity is 84.67768357377857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8235476811726885 and perplexity of 124.40566043670702
Finished 19 epochs...
Completing Train Step...
At time: 604.6020593643188 and batch: 50, loss is 4.547238035202026 and perplexity is 94.37139755160646
At time: 606.331832408905 and batch: 100, loss is 4.499252166748047 and perplexity is 89.94983866146254
At time: 608.0408148765564 and batch: 150, loss is 4.500940408706665 and perplexity is 90.10182401118544
At time: 609.7519958019257 and batch: 200, loss is 4.525789127349854 and perplexity is 92.36878781292054
At time: 611.4612052440643 and batch: 250, loss is 4.5170596313476565 and perplexity is 91.56596407050341
At time: 613.1707475185394 and batch: 300, loss is 4.477204055786133 and perplexity is 87.98831801055748
At time: 614.8813216686249 and batch: 350, loss is 4.427748861312867 and perplexity is 83.74268815759501
At time: 616.5923984050751 and batch: 400, loss is 4.4607332324981686 and perplexity is 86.55094780350312
At time: 618.301554441452 and batch: 450, loss is 4.491687173843384 and perplexity is 89.27193616697049
At time: 620.0098006725311 and batch: 500, loss is 4.483839530944824 and perplexity is 88.57410364265598
At time: 621.7187347412109 and batch: 550, loss is 4.464472045898438 and perplexity is 86.87515133755302
At time: 623.4293489456177 and batch: 600, loss is 4.479890127182006 and perplexity is 88.22497861608643
At time: 625.1392328739166 and batch: 650, loss is 4.448164539337158 and perplexity is 85.46992326158399
At time: 626.848258972168 and batch: 700, loss is 4.428807954788208 and perplexity is 83.83142647502214
At time: 628.5577402114868 and batch: 750, loss is 4.4191867637634275 and perplexity is 83.02873591851474
At time: 630.2692737579346 and batch: 800, loss is 4.38027437210083 and perplexity is 79.85994173941644
At time: 631.982096195221 and batch: 850, loss is 4.440276355743408 and perplexity is 84.79837295339185
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.823480288187663 and perplexity of 124.3972766504035
Finished 20 epochs...
Completing Train Step...
At time: 636.4014041423798 and batch: 50, loss is 4.543961343765258 and perplexity is 94.06267766749315
At time: 638.1303749084473 and batch: 100, loss is 4.496065702438354 and perplexity is 89.66367288156064
At time: 639.837991476059 and batch: 150, loss is 4.49763391494751 and perplexity is 89.80439488715541
At time: 641.5469145774841 and batch: 200, loss is 4.522913246154785 and perplexity is 92.10352776405493
At time: 643.258679151535 and batch: 250, loss is 4.514129123687744 and perplexity is 91.29802210615132
At time: 644.969135761261 and batch: 300, loss is 4.474452257156372 and perplexity is 87.74652471343248
At time: 646.6780438423157 and batch: 350, loss is 4.425177640914917 and perplexity is 83.52764383127885
At time: 648.3871765136719 and batch: 400, loss is 4.458301191329956 and perplexity is 86.3407080947382
At time: 650.096729516983 and batch: 450, loss is 4.489765453338623 and perplexity is 89.10054519225963
At time: 651.8074507713318 and batch: 500, loss is 4.482243156433105 and perplexity is 88.43281900280577
At time: 653.516211271286 and batch: 550, loss is 4.463297309875489 and perplexity is 86.77315588837133
At time: 655.2245013713837 and batch: 600, loss is 4.4792930793762205 and perplexity is 88.17231980766546
At time: 656.9339282512665 and batch: 650, loss is 4.448104391098022 and perplexity is 85.4647825508046
At time: 658.6613032817841 and batch: 700, loss is 4.429163093566895 and perplexity is 83.86120355262052
At time: 660.3901374340057 and batch: 750, loss is 4.419610967636109 and perplexity is 83.06396450135756
At time: 662.117689371109 and batch: 800, loss is 4.381061925888061 and perplexity is 79.922860511688
At time: 663.847359418869 and batch: 850, loss is 4.441011934280396 and perplexity is 84.86077176330697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8234907786051435 and perplexity of 124.39858163661391
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 668.3184657096863 and batch: 50, loss is 4.54219820022583 and perplexity is 93.89697778432716
At time: 670.0541529655457 and batch: 100, loss is 4.494956846237183 and perplexity is 89.5643038650229
At time: 671.7711770534515 and batch: 150, loss is 4.496016492843628 and perplexity is 89.65926067711877
At time: 673.4892718791962 and batch: 200, loss is 4.521885919570923 and perplexity is 92.00895594790339
At time: 675.2071406841278 and batch: 250, loss is 4.51116714477539 and perplexity is 91.02799938814785
At time: 676.9232785701752 and batch: 300, loss is 4.471377506256103 and perplexity is 87.47714036469081
At time: 678.6408371925354 and batch: 350, loss is 4.421812591552734 and perplexity is 83.24704157169165
At time: 680.3587787151337 and batch: 400, loss is 4.453502597808838 and perplexity is 85.92738660628636
At time: 682.100564956665 and batch: 450, loss is 4.485034065246582 and perplexity is 88.67997166660896
At time: 683.8167078495026 and batch: 500, loss is 4.477348022460937 and perplexity is 88.00098630800755
At time: 685.5326824188232 and batch: 550, loss is 4.457512950897216 and perplexity is 86.2726776733134
At time: 687.2506458759308 and batch: 600, loss is 4.472035551071167 and perplexity is 87.53472318730547
At time: 688.9664318561554 and batch: 650, loss is 4.440063371658325 and perplexity is 84.78031417269655
At time: 690.6867191791534 and batch: 700, loss is 4.420655889511108 and perplexity is 83.15080521786987
At time: 692.4026825428009 and batch: 750, loss is 4.410050964355468 and perplexity is 82.27365642122764
At time: 694.1207690238953 and batch: 800, loss is 4.370781383514404 and perplexity is 79.10541922702706
At time: 695.8368291854858 and batch: 850, loss is 4.431743841171265 and perplexity is 84.07790766181847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.824067751566569 and perplexity of 124.47037696464776
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 700.2880935668945 and batch: 50, loss is 4.541132574081421 and perplexity is 93.79697200376992
At time: 701.9780898094177 and batch: 100, loss is 4.493416538238526 and perplexity is 89.4264534446927
At time: 703.687762260437 and batch: 150, loss is 4.495006465911866 and perplexity is 89.56874812690437
At time: 705.3984396457672 and batch: 200, loss is 4.520594263076783 and perplexity is 91.89018870218871
At time: 707.1099143028259 and batch: 250, loss is 4.509918308258056 and perplexity is 90.9143912521717
At time: 708.8219137191772 and batch: 300, loss is 4.470125923156738 and perplexity is 87.36772394038634
At time: 710.5314512252808 and batch: 350, loss is 4.420863952636719 and perplexity is 83.16810763423551
At time: 712.2416586875916 and batch: 400, loss is 4.452070045471191 and perplexity is 85.80437925600559
At time: 713.9530560970306 and batch: 450, loss is 4.484016208648682 and perplexity is 88.58975409441109
At time: 715.6651375293732 and batch: 500, loss is 4.476559734344482 and perplexity is 87.93164351090687
At time: 717.3736162185669 and batch: 550, loss is 4.4564533519744876 and perplexity is 86.18131165120755
At time: 719.0831999778748 and batch: 600, loss is 4.470580139160156 and perplexity is 87.40741677265969
At time: 720.7944030761719 and batch: 650, loss is 4.438267679214477 and perplexity is 84.62821140891285
At time: 722.5060184001923 and batch: 700, loss is 4.418759279251098 and perplexity is 82.99325000521088
At time: 724.2418687343597 and batch: 750, loss is 4.407912673950196 and perplexity is 82.0979194064417
At time: 725.9519493579865 and batch: 800, loss is 4.368726034164428 and perplexity is 78.94299692949991
At time: 727.6637692451477 and batch: 850, loss is 4.42979100227356 and perplexity is 83.91387726789422
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.823605219523112 and perplexity of 124.41281873912841
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 732.1130530834198 and batch: 50, loss is 4.540803127288818 and perplexity is 93.76607598176456
At time: 733.8134055137634 and batch: 100, loss is 4.493070259094238 and perplexity is 89.39549228982767
At time: 735.5246460437775 and batch: 150, loss is 4.49470932006836 and perplexity is 89.54213709956431
At time: 737.2358322143555 and batch: 200, loss is 4.520205774307251 and perplexity is 91.85449732914546
At time: 738.9444007873535 and batch: 250, loss is 4.509636850357055 and perplexity is 90.8888062791544
At time: 740.6547451019287 and batch: 300, loss is 4.469879417419434 and perplexity is 87.34618994941502
At time: 742.3667075634003 and batch: 350, loss is 4.420695466995239 and perplexity is 83.15409618266749
At time: 744.0783448219299 and batch: 400, loss is 4.451731376647949 and perplexity is 85.77532490803254
At time: 745.787919998169 and batch: 450, loss is 4.483790864944458 and perplexity is 88.56979320018257
At time: 747.4974398612976 and batch: 500, loss is 4.4765576171875 and perplexity is 87.93145734601093
At time: 749.2078402042389 and batch: 550, loss is 4.456292324066162 and perplexity is 86.16743517213577
At time: 750.9180300235748 and batch: 600, loss is 4.470221834182739 and perplexity is 87.37610387028627
At time: 752.6266267299652 and batch: 650, loss is 4.437803268432617 and perplexity is 84.58891827986761
At time: 754.3355963230133 and batch: 700, loss is 4.41827018737793 and perplexity is 82.95266860593074
At time: 756.0466208457947 and batch: 750, loss is 4.407491035461426 and perplexity is 82.06331106039012
At time: 757.7591941356659 and batch: 800, loss is 4.368263301849365 and perplexity is 78.90647590415365
At time: 759.4681620597839 and batch: 850, loss is 4.429376697540283 and perplexity is 83.87911855220192
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.823544502258301 and perplexity of 124.40526496239174
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 763.9202687740326 and batch: 50, loss is 4.5407250785827635 and perplexity is 93.75875794644767
At time: 765.6158912181854 and batch: 100, loss is 4.492966365814209 and perplexity is 89.38620518135633
At time: 767.3302326202393 and batch: 150, loss is 4.494600057601929 and perplexity is 89.5323540392857
At time: 769.0737853050232 and batch: 200, loss is 4.520154972076416 and perplexity is 91.84983103429897
At time: 770.7911648750305 and batch: 250, loss is 4.509560279846191 and perplexity is 90.88184714326145
At time: 772.5079312324524 and batch: 300, loss is 4.469828071594239 and perplexity is 87.34170520235192
At time: 774.2234292030334 and batch: 350, loss is 4.42067437171936 and perplexity is 83.15234204257014
At time: 775.9406554698944 and batch: 400, loss is 4.451644067764282 and perplexity is 85.76783628708499
At time: 777.6575996875763 and batch: 450, loss is 4.483711347579956 and perplexity is 88.5627506436593
At time: 779.3737397193909 and batch: 500, loss is 4.47648962020874 and perplexity is 87.92547847584832
At time: 781.0881998538971 and batch: 550, loss is 4.4562313270568845 and perplexity is 86.16217937658871
At time: 782.8046007156372 and batch: 600, loss is 4.470146551132202 and perplexity is 87.3695261782403
At time: 784.5218758583069 and batch: 650, loss is 4.437710542678833 and perplexity is 84.58107507229758
At time: 786.2393641471863 and batch: 700, loss is 4.418170738220215 and perplexity is 82.94441944310061
At time: 787.9539206027985 and batch: 750, loss is 4.407400856018066 and perplexity is 82.05591097035145
At time: 789.6711077690125 and batch: 800, loss is 4.368155546188355 and perplexity is 78.89797374277086
At time: 791.3886032104492 and batch: 850, loss is 4.429286775588989 and perplexity is 83.8715763173002
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.823532422383626 and perplexity of 124.4037621714589
Annealing...
Model not improving. Stopping early with 124.3972766504035loss at 24 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9fc4b2c860>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'num_layers': 1, 'anneal': 3.3022335888493073, 'wordvec_source': '', 'dropout': 0.11820777557420015, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 11.163034786213991, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.1393613815307617 and batch: 50, loss is 7.007873907089233 and perplexity is 1105.3020301863135
At time: 3.7167744636535645 and batch: 100, loss is 5.914480228424072 and perplexity is 370.36174915671785
At time: 5.295010566711426 and batch: 150, loss is 5.6937405872344975 and perplexity is 297.00250919484336
At time: 6.877603769302368 and batch: 200, loss is 5.6380706310272215 and perplexity is 280.9201965700188
At time: 8.464083433151245 and batch: 250, loss is 5.644759559631348 and perplexity is 282.80555017666813
At time: 10.044984817504883 and batch: 300, loss is 5.558419704437256 and perplexity is 259.41256370898805
At time: 11.632246971130371 and batch: 350, loss is 5.508461122512817 and perplexity is 246.77108425691
At time: 13.243702411651611 and batch: 400, loss is 5.517082271575927 and perplexity is 248.90773150225075
At time: 14.833917617797852 and batch: 450, loss is 5.497611827850342 and perplexity is 244.10826303504643
At time: 16.423652172088623 and batch: 500, loss is 5.498835229873658 and perplexity is 244.40708833241698
At time: 18.012951135635376 and batch: 550, loss is 5.46227949142456 and perplexity is 235.63393818502692
At time: 19.60413932800293 and batch: 600, loss is 5.496600646972656 and perplexity is 243.86155018406052
At time: 21.19707679748535 and batch: 650, loss is 5.486199150085449 and perplexity is 241.338171238597
At time: 22.794134855270386 and batch: 700, loss is 5.433419141769409 and perplexity is 228.930655152756
At time: 24.388765811920166 and batch: 750, loss is 5.420601596832276 and perplexity is 226.01505154975035
At time: 25.98625612258911 and batch: 800, loss is 5.416842660903931 and perplexity is 225.1670702043756
At time: 27.58218741416931 and batch: 850, loss is 5.399156713485718 and perplexity is 221.219785861712
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.049293518066406 and perplexity of 155.9122763615434
Finished 1 epochs...
Completing Train Step...
At time: 31.799254655838013 and batch: 50, loss is 5.289283895492554 and perplexity is 198.20144163210222
At time: 33.408323526382446 and batch: 100, loss is 5.181907548904419 and perplexity is 178.0220731214853
At time: 35.012556314468384 and batch: 150, loss is 5.167975978851318 and perplexity is 175.55914222142798
At time: 36.617427349090576 and batch: 200, loss is 5.165471067428589 and perplexity is 175.11993244116547
At time: 38.22322750091553 and batch: 250, loss is 5.197122793197632 and perplexity is 180.75143374338188
At time: 39.8305287361145 and batch: 300, loss is 5.143410396575928 and perplexity is 171.29897070350714
At time: 41.49569511413574 and batch: 350, loss is 5.104435567855835 and perplexity is 164.75105349283098
At time: 43.200242042541504 and batch: 400, loss is 5.114089479446411 and perplexity is 166.3492475976449
At time: 44.90784692764282 and batch: 450, loss is 5.116175651550293 and perplexity is 166.69664299478603
At time: 46.61647701263428 and batch: 500, loss is 5.128385238647461 and perplexity is 168.74441596839253
At time: 48.325159311294556 and batch: 550, loss is 5.108428707122803 and perplexity is 165.4102426349252
At time: 50.03595852851868 and batch: 600, loss is 5.138953905105591 and perplexity is 170.53727680349675
At time: 51.74379539489746 and batch: 650, loss is 5.128910388946533 and perplexity is 168.83305542138564
At time: 53.45148158073425 and batch: 700, loss is 5.0755569362640385 and perplexity is 160.0613111088715
At time: 55.16046977043152 and batch: 750, loss is 5.068217077255249 and perplexity is 158.89078465715917
At time: 56.87197971343994 and batch: 800, loss is 5.054051523208618 and perplexity is 156.6558753953819
At time: 58.580846548080444 and batch: 850, loss is 5.051122684478759 and perplexity is 156.19772684937027
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.954253514607747 and perplexity of 141.77673260141464
Finished 2 epochs...
Completing Train Step...
At time: 63.03369379043579 and batch: 50, loss is 5.035509347915649 and perplexity is 153.77789911991093
At time: 64.69685912132263 and batch: 100, loss is 4.9557297706604 and perplexity is 141.9861859263732
At time: 66.40003967285156 and batch: 150, loss is 4.952629880905151 and perplexity is 141.54672589396847
At time: 68.11301755905151 and batch: 200, loss is 4.970935935974121 and perplexity is 144.16175041779456
At time: 69.82741832733154 and batch: 250, loss is 4.985193099975586 and perplexity is 146.231809658891
At time: 71.54797387123108 and batch: 300, loss is 4.9447642421722415 and perplexity is 140.43773765228735
At time: 73.27287101745605 and batch: 350, loss is 4.90874924659729 and perplexity is 135.46986900120265
At time: 74.99812388420105 and batch: 400, loss is 4.934572820663452 and perplexity is 139.0137460466988
At time: 76.72353148460388 and batch: 450, loss is 4.950744838714599 and perplexity is 141.28015567071816
At time: 78.4501895904541 and batch: 500, loss is 4.947428903579712 and perplexity is 140.8124556985989
At time: 80.17460131645203 and batch: 550, loss is 4.932296171188354 and perplexity is 138.6976204648436
At time: 81.90009999275208 and batch: 600, loss is 4.971881637573242 and perplexity is 144.2981489016632
At time: 83.62686777114868 and batch: 650, loss is 4.95963134765625 and perplexity is 142.5412380483098
At time: 85.35439157485962 and batch: 700, loss is 4.915310802459717 and perplexity is 136.3616847641182
At time: 87.079824924469 and batch: 750, loss is 4.914519290924073 and perplexity is 136.2537956209908
At time: 88.80568671226501 and batch: 800, loss is 4.880838699340821 and perplexity is 131.74110874540443
At time: 90.53191113471985 and batch: 850, loss is 4.888058252334595 and perplexity is 132.69566224012559
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.900797208150228 and perplexity of 134.39687927645403
Finished 3 epochs...
Completing Train Step...
At time: 94.97915482521057 and batch: 50, loss is 4.87689136505127 and perplexity is 131.22210755899897
At time: 96.67571759223938 and batch: 100, loss is 4.8097944259643555 and perplexity is 122.70638967653434
At time: 98.35183072090149 and batch: 150, loss is 4.8089174461364745 and perplexity is 122.59882582059575
At time: 100.03389072418213 and batch: 200, loss is 4.825584154129029 and perplexity is 124.6592673439591
At time: 101.721994638443 and batch: 250, loss is 4.8445524311065675 and perplexity is 127.04640724709647
At time: 103.43218040466309 and batch: 300, loss is 4.805546236038208 and perplexity is 122.18621531009202
At time: 105.14902663230896 and batch: 350, loss is 4.7670612621307376 and perplexity is 117.57321690629948
At time: 106.86639618873596 and batch: 400, loss is 4.788558568954468 and perplexity is 120.12808745206549
At time: 108.60878348350525 and batch: 450, loss is 4.812181634902954 and perplexity is 122.99966538271116
At time: 110.32384490966797 and batch: 500, loss is 4.80289475440979 and perplexity is 121.8626699318178
At time: 112.04063701629639 and batch: 550, loss is 4.801587600708007 and perplexity is 121.70348075673363
At time: 113.7588791847229 and batch: 600, loss is 4.835825119018555 and perplexity is 125.94245786372896
At time: 115.47325921058655 and batch: 650, loss is 4.813192644119263 and perplexity is 123.12408206062237
At time: 117.18878388404846 and batch: 700, loss is 4.796509666442871 and perplexity is 121.08704491653758
At time: 118.90593385696411 and batch: 750, loss is 4.795382041931152 and perplexity is 120.95058115104142
At time: 120.62445735931396 and batch: 800, loss is 4.767816953659057 and perplexity is 117.66209956998101
At time: 122.33952689170837 and batch: 850, loss is 4.773519210815429 and perplexity is 118.33495569541586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.888334274291992 and perplexity of 132.73229421193693
Finished 4 epochs...
Completing Train Step...
At time: 126.79751062393188 and batch: 50, loss is 4.7584121131896975 and perplexity is 116.56069368591248
At time: 128.4749903678894 and batch: 100, loss is 4.69937237739563 and perplexity is 109.87818877152904
At time: 130.15321731567383 and batch: 150, loss is 4.704211225509644 and perplexity is 110.41116108413482
At time: 131.833238363266 and batch: 200, loss is 4.719266757965088 and perplexity is 112.08603633588942
At time: 133.5340588092804 and batch: 250, loss is 4.736780061721801 and perplexity is 114.06632320467314
At time: 135.2449221611023 and batch: 300, loss is 4.7181217479705815 and perplexity is 111.95777015108554
At time: 136.95268511772156 and batch: 350, loss is 4.670076866149902 and perplexity is 106.70594419232965
At time: 138.6612548828125 and batch: 400, loss is 4.696750621795655 and perplexity is 109.5904923144934
At time: 140.37164044380188 and batch: 450, loss is 4.71532696723938 and perplexity is 111.645309565152
At time: 142.0813684463501 and batch: 500, loss is 4.707657337188721 and perplexity is 110.79230663371524
At time: 143.78800225257874 and batch: 550, loss is 4.700918960571289 and perplexity is 110.04825600731886
At time: 145.49690127372742 and batch: 600, loss is 4.741328496932983 and perplexity is 114.5863281935483
At time: 147.20699977874756 and batch: 650, loss is 4.722936687469482 and perplexity is 112.49813992069079
At time: 148.91739916801453 and batch: 700, loss is 4.703801736831665 and perplexity is 110.36595821941071
At time: 150.6501817703247 and batch: 750, loss is 4.700988121032715 and perplexity is 110.05586725867927
At time: 152.34884977340698 and batch: 800, loss is 4.668828735351562 and perplexity is 106.57284429733532
At time: 154.05889463424683 and batch: 850, loss is 4.672411727905273 and perplexity is 106.95537890496286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.893054326375325 and perplexity of 133.36027844700712
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 158.49509811401367 and batch: 50, loss is 4.65338059425354 and perplexity is 104.93914327280977
At time: 160.17850708961487 and batch: 100, loss is 4.567465000152588 and perplexity is 96.29968041439159
At time: 161.88808727264404 and batch: 150, loss is 4.545521249771118 and perplexity is 94.20952110451735
At time: 163.59689331054688 and batch: 200, loss is 4.555947761535645 and perplexity is 95.19693648838114
At time: 165.30458855628967 and batch: 250, loss is 4.542007913589478 and perplexity is 93.87911214411118
At time: 167.01300573349 and batch: 300, loss is 4.517951831817627 and perplexity is 91.64769572176759
At time: 168.72308039665222 and batch: 350, loss is 4.459243183135986 and perplexity is 86.4220786534731
At time: 170.43232107162476 and batch: 400, loss is 4.464015674591065 and perplexity is 86.83551305673386
At time: 172.1390745639801 and batch: 450, loss is 4.479460411071777 and perplexity is 88.1870750659168
At time: 173.8472192287445 and batch: 500, loss is 4.456720600128174 and perplexity is 86.20434652550529
At time: 175.56800413131714 and batch: 550, loss is 4.425811243057251 and perplexity is 83.58058389505229
At time: 177.2938690185547 and batch: 600, loss is 4.444847450256348 and perplexity is 85.18688160911015
At time: 179.01731157302856 and batch: 650, loss is 4.408616943359375 and perplexity is 82.15575882451265
At time: 180.74056673049927 and batch: 700, loss is 4.367582101821899 and perplexity is 78.85274311406737
At time: 182.46619844436646 and batch: 750, loss is 4.347960815429688 and perplexity is 77.3206310180195
At time: 184.19318437576294 and batch: 800, loss is 4.291018295288086 and perplexity is 73.04080775392673
At time: 185.91766691207886 and batch: 850, loss is 4.2848894405364994 and perplexity is 72.59452026484874
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.728742281595866 and perplexity of 113.1531580058406
Finished 6 epochs...
Completing Train Step...
At time: 190.42761778831482 and batch: 50, loss is 4.458187389373779 and perplexity is 86.33088291233243
At time: 192.11815929412842 and batch: 100, loss is 4.393153991699219 and perplexity is 80.89515970645765
At time: 193.8581395149231 and batch: 150, loss is 4.390526571273804 and perplexity is 80.68289309046129
At time: 195.5733301639557 and batch: 200, loss is 4.412532052993774 and perplexity is 82.47803809508629
At time: 197.28924179077148 and batch: 250, loss is 4.4085449123382565 and perplexity is 82.14984127443991
At time: 199.00485563278198 and batch: 300, loss is 4.3942777538299564 and perplexity is 80.98611772151003
At time: 200.71906208992004 and batch: 350, loss is 4.3346539497375485 and perplexity is 76.29855118717204
At time: 202.43429493904114 and batch: 400, loss is 4.349947443008423 and perplexity is 77.47439099734258
At time: 204.15081524848938 and batch: 450, loss is 4.374215383529663 and perplexity is 79.37753419185005
At time: 205.8654065132141 and batch: 500, loss is 4.351993417739868 and perplexity is 77.63306390869025
At time: 207.580228805542 and batch: 550, loss is 4.339094610214233 and perplexity is 76.63812054596221
At time: 209.29575371742249 and batch: 600, loss is 4.37152889251709 and perplexity is 79.16457334642443
At time: 211.01250386238098 and batch: 650, loss is 4.332465581893921 and perplexity is 76.13176445316252
At time: 212.7285578250885 and batch: 700, loss is 4.305385341644287 and perplexity is 74.09776290603028
At time: 214.44282031059265 and batch: 750, loss is 4.289934282302856 and perplexity is 72.96167346891521
At time: 216.15929913520813 and batch: 800, loss is 4.2458569526672365 and perplexity is 69.81556314912623
At time: 217.87715244293213 and batch: 850, loss is 4.251993865966797 and perplexity is 70.24533258720255
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.721927642822266 and perplexity of 112.38468152670126
Finished 7 epochs...
Completing Train Step...
At time: 222.3122456073761 and batch: 50, loss is 4.372046489715576 and perplexity is 79.20555931400382
At time: 224.03255581855774 and batch: 100, loss is 4.31584056854248 and perplexity is 74.87653585975346
At time: 225.74940824508667 and batch: 150, loss is 4.322615900039673 and perplexity is 75.38557170759357
At time: 227.46367144584656 and batch: 200, loss is 4.339000539779663 and perplexity is 76.63091150374201
At time: 229.17883467674255 and batch: 250, loss is 4.336516914367675 and perplexity is 76.44082517391777
At time: 230.89604234695435 and batch: 300, loss is 4.323826055526734 and perplexity is 75.4768551933061
At time: 232.61372923851013 and batch: 350, loss is 4.270653114318848 and perplexity is 71.56836268094513
At time: 234.32879042625427 and batch: 400, loss is 4.283965654373169 and perplexity is 72.52748941734609
At time: 236.04551887512207 and batch: 450, loss is 4.309156618118286 and perplexity is 74.3777336477654
At time: 237.79039978981018 and batch: 500, loss is 4.2929886436462406 and perplexity is 73.18486546489419
At time: 239.50587344169617 and batch: 550, loss is 4.2794898509979244 and perplexity is 72.20359601770745
At time: 241.22032690048218 and batch: 600, loss is 4.3157933235168455 and perplexity is 74.87299839946168
At time: 242.93445897102356 and batch: 650, loss is 4.286936540603637 and perplexity is 72.74328072397982
At time: 244.65079164505005 and batch: 700, loss is 4.2590538692474365 and perplexity is 70.74301963460181
At time: 246.37810063362122 and batch: 750, loss is 4.247154817581177 and perplexity is 69.9062331449602
At time: 248.11018204689026 and batch: 800, loss is 4.20892409324646 and perplexity is 67.2841094251969
At time: 249.84261107444763 and batch: 850, loss is 4.222873229980468 and perplexity is 68.22924122883757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.713379542032878 and perplexity of 111.42810024321794
Finished 8 epochs...
Completing Train Step...
At time: 254.3194556236267 and batch: 50, loss is 4.308887710571289 and perplexity is 74.3577356027921
At time: 256.0330374240875 and batch: 100, loss is 4.255435600280761 and perplexity is 70.48751488325821
At time: 257.74008226394653 and batch: 150, loss is 4.259825963973999 and perplexity is 70.7976610384582
At time: 259.4495122432709 and batch: 200, loss is 4.280873126983643 and perplexity is 72.30354262909803
At time: 261.15944623947144 and batch: 250, loss is 4.2816525650024415 and perplexity is 72.35992072787954
At time: 262.86616349220276 and batch: 300, loss is 4.277051420211792 and perplexity is 72.02774703123191
At time: 264.57423877716064 and batch: 350, loss is 4.219860525131225 and perplexity is 68.02399598983118
At time: 266.28374886512756 and batch: 400, loss is 4.235793724060058 and perplexity is 69.11651640966949
At time: 267.9939179420471 and batch: 450, loss is 4.26183819770813 and perplexity is 70.94026590934504
At time: 269.700492143631 and batch: 500, loss is 4.2421689796447755 and perplexity is 69.55855943835564
At time: 271.4084005355835 and batch: 550, loss is 4.233981075286866 and perplexity is 68.9913459203318
At time: 273.1168010234833 and batch: 600, loss is 4.273354253768921 and perplexity is 71.76194013090556
At time: 274.82680654525757 and batch: 650, loss is 4.2425888156890865 and perplexity is 69.5877687599322
At time: 276.533408164978 and batch: 700, loss is 4.218332271575928 and perplexity is 67.92011747268863
At time: 278.24071407318115 and batch: 750, loss is 4.206364803314209 and perplexity is 67.11213004776069
At time: 279.9498953819275 and batch: 800, loss is 4.165353541374206 and perplexity is 64.41545194564567
At time: 281.68912959098816 and batch: 850, loss is 4.183939018249512 and perplexity is 65.62383827307099
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.717252731323242 and perplexity of 111.86051924747261
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 286.12172293663025 and batch: 50, loss is 4.272485227584839 and perplexity is 71.69960421560559
At time: 287.85301327705383 and batch: 100, loss is 4.219024238586425 and perplexity is 67.96713221777917
At time: 289.5622773170471 and batch: 150, loss is 4.21382538318634 and perplexity is 67.6146978468888
At time: 291.27004957199097 and batch: 200, loss is 4.236489410400391 and perplexity is 69.16461655539428
At time: 292.9792482852936 and batch: 250, loss is 4.225491724014282 and perplexity is 68.40813320149392
At time: 294.6901857852936 and batch: 300, loss is 4.209513444900512 and perplexity is 67.32377511374955
At time: 296.40041947364807 and batch: 350, loss is 4.151330137252808 and perplexity is 63.51843236568513
At time: 298.1065933704376 and batch: 400, loss is 4.1577522706985475 and perplexity is 63.92766889386105
At time: 299.81460523605347 and batch: 450, loss is 4.179224920272827 and perplexity is 65.31520909548455
At time: 301.5231251716614 and batch: 500, loss is 4.152402324676514 and perplexity is 63.586572553039325
At time: 303.2324924468994 and batch: 550, loss is 4.1350172424316405 and perplexity is 62.49066855225697
At time: 304.9390079975128 and batch: 600, loss is 4.171326923370361 and perplexity is 64.80138155151346
At time: 306.6471276283264 and batch: 650, loss is 4.1309316539764405 and perplexity is 62.23587823686161
At time: 308.3553686141968 and batch: 700, loss is 4.092345399856567 and perplexity is 59.8801700777133
At time: 310.0662326812744 and batch: 750, loss is 4.069999265670776 and perplexity is 58.556919591819266
At time: 311.774156332016 and batch: 800, loss is 4.023593544960022 and perplexity is 55.90163033603643
At time: 313.4834225177765 and batch: 850, loss is 4.038143453598022 and perplexity is 56.72093993612978
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.672789573669434 and perplexity of 106.99579917766978
Finished 10 epochs...
Completing Train Step...
At time: 317.98137617111206 and batch: 50, loss is 4.217439880371094 and perplexity is 67.85953319368414
At time: 319.6865141391754 and batch: 100, loss is 4.164147658348083 and perplexity is 64.33782126169368
At time: 321.4023349285126 and batch: 150, loss is 4.158223805427551 and perplexity is 63.957820118004726
At time: 323.12006402015686 and batch: 200, loss is 4.185146369934082 and perplexity is 65.70311717388479
At time: 324.86510944366455 and batch: 250, loss is 4.179266257286072 and perplexity is 65.3179090869524
At time: 326.5793454647064 and batch: 300, loss is 4.167184052467346 and perplexity is 64.53347313159166
At time: 328.2955882549286 and batch: 350, loss is 4.110743646621704 and perplexity is 60.992057246506135
At time: 330.0130605697632 and batch: 400, loss is 4.120606627464294 and perplexity is 61.59659712056255
At time: 331.72975993156433 and batch: 450, loss is 4.145456881523132 and perplexity is 63.14646576558846
At time: 333.4436480998993 and batch: 500, loss is 4.123643178939819 and perplexity is 61.78392262626819
At time: 335.1596031188965 and batch: 550, loss is 4.111167573928833 and perplexity is 61.01791892643978
At time: 336.8761968612671 and batch: 600, loss is 4.151463060379029 and perplexity is 63.52687599545221
At time: 338.59333205223083 and batch: 650, loss is 4.113224506378174 and perplexity is 61.14355783494683
At time: 340.30858087539673 and batch: 700, loss is 4.079811458587646 and perplexity is 59.13431953064395
At time: 342.02398705482483 and batch: 750, loss is 4.06168505191803 and perplexity is 58.07208315541732
At time: 343.7421214580536 and batch: 800, loss is 4.019740204811097 and perplexity is 55.686636827054855
At time: 345.4603681564331 and batch: 850, loss is 4.0394131565094 and perplexity is 56.793004419263255
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.67214838663737 and perplexity of 106.92721684815157
Finished 11 epochs...
Completing Train Step...
At time: 349.9120922088623 and batch: 50, loss is 4.189498376846314 and perplexity is 65.98968070492286
At time: 351.59235310554504 and batch: 100, loss is 4.1366041469573975 and perplexity is 62.58991400270016
At time: 353.2913975715637 and batch: 150, loss is 4.131010675430298 and perplexity is 62.24079640075967
At time: 355.0000956058502 and batch: 200, loss is 4.160248355865479 and perplexity is 64.08743711453418
At time: 356.71115589141846 and batch: 250, loss is 4.154843211174011 and perplexity is 63.741969735694894
At time: 358.42245531082153 and batch: 300, loss is 4.144151749610901 and perplexity is 63.06410505545965
At time: 360.13285064697266 and batch: 350, loss is 4.088820996284485 and perplexity is 59.66949965425979
At time: 361.8417491912842 and batch: 400, loss is 4.098836646080017 and perplexity is 60.27013130346488
At time: 363.5525562763214 and batch: 450, loss is 4.127575149536133 and perplexity is 62.027333421802936
At time: 365.264728307724 and batch: 500, loss is 4.107332611083985 and perplexity is 60.78436559489911
At time: 367.0206415653229 and batch: 550, loss is 4.0954645013809206 and perplexity is 60.0672339914917
At time: 368.72945976257324 and batch: 600, loss is 4.136814918518066 and perplexity is 62.60310756692186
At time: 370.4388117790222 and batch: 650, loss is 4.100302472114563 and perplexity is 60.35854161227093
At time: 372.14937233924866 and batch: 700, loss is 4.068384766578674 and perplexity is 58.462455774694654
At time: 373.85959577560425 and batch: 750, loss is 4.052616267204285 and perplexity is 57.54782073995676
At time: 375.56787848472595 and batch: 800, loss is 4.012918438911438 and perplexity is 55.30804841581844
At time: 377.2779471874237 and batch: 850, loss is 4.03378888130188 and perplexity is 56.47448150245791
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.672780990600586 and perplexity of 106.99488082930016
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 381.7307071685791 and batch: 50, loss is 4.17669358253479 and perplexity is 65.15008332455373
At time: 383.4374303817749 and batch: 100, loss is 4.133551473617554 and perplexity is 62.39913877625474
At time: 385.14842081069946 and batch: 150, loss is 4.125898933410644 and perplexity is 61.923449295747474
At time: 386.85942792892456 and batch: 200, loss is 4.1515856456756595 and perplexity is 63.53466393372375
At time: 388.56747555732727 and batch: 250, loss is 4.143403458595276 and perplexity is 63.01693240387501
At time: 390.2753393650055 and batch: 300, loss is 4.126411628723145 and perplexity is 61.95520529781637
At time: 391.98559832572937 and batch: 350, loss is 4.068522672653199 and perplexity is 58.47051865842514
At time: 393.69531059265137 and batch: 400, loss is 4.079253787994385 and perplexity is 59.101351253163024
At time: 395.40242433547974 and batch: 450, loss is 4.108135619163513 and perplexity is 60.8331955343749
At time: 397.1096103191376 and batch: 500, loss is 4.081335611343384 and perplexity is 59.224517987367214
At time: 398.8180253505707 and batch: 550, loss is 4.064748344421386 and perplexity is 58.25024767810388
At time: 400.5282373428345 and batch: 600, loss is 4.097172193527221 and perplexity is 60.16989796951431
At time: 402.2368884086609 and batch: 650, loss is 4.057922706604004 and perplexity is 57.85400642271256
At time: 403.94482588768005 and batch: 700, loss is 4.0244993019104 and perplexity is 55.95228656390284
At time: 405.65242767333984 and batch: 750, loss is 4.003912725448608 and perplexity is 54.81219608286214
At time: 407.3633437156677 and batch: 800, loss is 3.9609473085403444 and perplexity is 52.50704276683932
At time: 409.0723683834076 and batch: 850, loss is 3.9782896423339844 and perplexity is 53.42557919044424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.660939534505208 and perplexity of 105.73537753925554
Finished 13 epochs...
Completing Train Step...
At time: 413.54162859916687 and batch: 50, loss is 4.158874626159668 and perplexity is 63.999458741488525
At time: 415.28805232048035 and batch: 100, loss is 4.1131144666671755 and perplexity is 61.136829985685345
At time: 417.0036242008209 and batch: 150, loss is 4.103885717391968 and perplexity is 60.57520902586975
At time: 418.7192599773407 and batch: 200, loss is 4.1316670179367065 and perplexity is 62.28166109021956
At time: 420.4357442855835 and batch: 250, loss is 4.124855570793152 and perplexity is 61.858874376992276
At time: 422.15391731262207 and batch: 300, loss is 4.110008125305176 and perplexity is 60.94721278231379
At time: 423.8684606552124 and batch: 350, loss is 4.054022421836853 and perplexity is 57.62879879517524
At time: 425.5837416648865 and batch: 400, loss is 4.0651967811584475 and perplexity is 58.27637508691279
At time: 427.30163383483887 and batch: 450, loss is 4.095216131210327 and perplexity is 60.05231693489077
At time: 429.0189847946167 and batch: 500, loss is 4.069322595596313 and perplexity is 58.51730927974656
At time: 430.7342495918274 and batch: 550, loss is 4.056796555519104 and perplexity is 57.78889074254245
At time: 432.44922947883606 and batch: 600, loss is 4.091670298576355 and perplexity is 59.839758540709546
At time: 434.16574811935425 and batch: 650, loss is 4.0557428646087645 and perplexity is 57.72803118286171
At time: 435.8841576576233 and batch: 700, loss is 4.023452143669129 and perplexity is 55.893726332175234
At time: 437.5986394882202 and batch: 750, loss is 4.004826374053955 and perplexity is 54.8622980536819
At time: 439.31394505500793 and batch: 800, loss is 3.9658483839035035 and perplexity is 52.76501539579769
At time: 441.03036284446716 and batch: 850, loss is 3.981955118179321 and perplexity is 53.62176870479078
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.661377588907878 and perplexity of 105.78170553325461
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 445.5062201023102 and batch: 50, loss is 4.153415040969849 and perplexity is 63.651000329124024
At time: 447.2382969856262 and batch: 100, loss is 4.110704865455627 and perplexity is 60.98969194926949
At time: 448.954204082489 and batch: 150, loss is 4.103720197677612 and perplexity is 60.565183464311666
At time: 450.67144227027893 and batch: 200, loss is 4.132836284637452 and perplexity is 62.354527554462734
At time: 452.38509368896484 and batch: 250, loss is 4.12512285232544 and perplexity is 61.87541032149923
At time: 454.1298842430115 and batch: 300, loss is 4.108166847229004 and perplexity is 60.8350952670514
At time: 455.84545040130615 and batch: 350, loss is 4.0489581060409545 and perplexity is 57.33768612457366
At time: 457.5625488758087 and batch: 400, loss is 4.059728331565857 and perplexity is 57.95856342781468
At time: 459.27546763420105 and batch: 450, loss is 4.093250832557678 and perplexity is 59.93441209439191
At time: 460.9903256893158 and batch: 500, loss is 4.064975891113281 and perplexity is 58.26350383740551
At time: 462.7063317298889 and batch: 550, loss is 4.047979135513305 and perplexity is 57.281581686510556
At time: 464.42300605773926 and batch: 600, loss is 4.077745366096496 and perplexity is 59.01226868467754
At time: 466.1370675563812 and batch: 650, loss is 4.038485698699951 and perplexity is 56.74035572228906
At time: 467.8521466255188 and batch: 700, loss is 4.0072607326507566 and perplexity is 54.99601525236008
At time: 469.567834854126 and batch: 750, loss is 3.9872584915161133 and perplexity is 53.90690037476411
At time: 471.2876422405243 and batch: 800, loss is 3.947951922416687 and perplexity is 51.82910802452254
At time: 473.01029419898987 and batch: 850, loss is 3.9666089057922362 and perplexity is 52.805159608306745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.656027793884277 and perplexity of 105.21730614802236
Finished 15 epochs...
Completing Train Step...
At time: 477.49200081825256 and batch: 50, loss is 4.147029819488526 and perplexity is 63.24586939634913
At time: 479.2591269016266 and batch: 100, loss is 4.10185884475708 and perplexity is 60.452555136239525
At time: 480.9818344116211 and batch: 150, loss is 4.093242993354798 and perplexity is 59.93394225821754
At time: 482.70650482177734 and batch: 200, loss is 4.123019394874572 and perplexity is 61.74539481761134
At time: 484.43250942230225 and batch: 250, loss is 4.11643126487732 and perplexity is 61.33994517379357
At time: 486.1587224006653 and batch: 300, loss is 4.100570363998413 and perplexity is 60.374713341730725
At time: 487.8831708431244 and batch: 350, loss is 4.042615027427673 and perplexity is 56.97513971973165
At time: 489.60888409614563 and batch: 400, loss is 4.0541700792312625 and perplexity is 57.63730874171108
At time: 491.338210105896 and batch: 450, loss is 4.087731800079346 and perplexity is 59.60454324323373
At time: 493.0616099834442 and batch: 500, loss is 4.059430861473084 and perplexity is 57.941325052652445
At time: 494.7852232456207 and batch: 550, loss is 4.04405385017395 and perplexity is 57.05717585030536
At time: 496.51096057891846 and batch: 600, loss is 4.0755655431747435 and perplexity is 58.883772489090795
At time: 498.3072648048401 and batch: 650, loss is 4.038893971443176 and perplexity is 56.763525992544444
At time: 500.0339632034302 and batch: 700, loss is 4.00923312664032 and perplexity is 55.104596109207115
At time: 501.75699830055237 and batch: 750, loss is 3.9890185165405274 and perplexity is 54.001861410798874
At time: 503.4833416938782 and batch: 800, loss is 3.950370774269104 and perplexity is 51.95462670282136
At time: 505.20835614204407 and batch: 850, loss is 3.9693433904647826 and perplexity is 52.94975211077985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655877749125163 and perplexity of 105.20152002700871
Finished 16 epochs...
Completing Train Step...
At time: 509.8370280265808 and batch: 50, loss is 4.143322629928589 and perplexity is 63.011839035097985
At time: 511.5313379764557 and batch: 100, loss is 4.097442026138306 and perplexity is 60.18613596086338
At time: 513.2479858398438 and batch: 150, loss is 4.088540806770324 and perplexity is 59.65278322813445
At time: 514.9721517562866 and batch: 200, loss is 4.118588404655457 and perplexity is 61.472406827302855
At time: 516.6964800357819 and batch: 250, loss is 4.112183403968811 and perplexity is 61.079934254646005
At time: 518.4221904277802 and batch: 300, loss is 4.096725063323975 and perplexity is 60.14300020465553
At time: 520.1497395038605 and batch: 350, loss is 4.039064335823059 and perplexity is 56.7731972992498
At time: 521.8743543624878 and batch: 400, loss is 4.05125048160553 and perplexity is 57.469276404720894
At time: 523.5986802577972 and batch: 450, loss is 4.085261778831482 and perplexity is 59.457500429150365
At time: 525.3247535228729 and batch: 500, loss is 4.0566956377029415 and perplexity is 57.78305910815312
At time: 527.0518054962158 and batch: 550, loss is 4.042196736335755 and perplexity is 56.95131251002828
At time: 528.775593996048 and batch: 600, loss is 4.0743503713607785 and perplexity is 58.812262045999155
At time: 530.5009281635284 and batch: 650, loss is 4.038627290725708 and perplexity is 56.74839027299871
At time: 532.2269425392151 and batch: 700, loss is 4.010037341117859 and perplexity is 55.14892984770482
At time: 533.9551992416382 and batch: 750, loss is 3.989838356971741 and perplexity is 54.046152473465376
At time: 535.6811356544495 and batch: 800, loss is 3.9514727210998535 and perplexity is 52.01190949455823
At time: 537.409072637558 and batch: 850, loss is 3.9705781364440917 and perplexity is 53.01517198445546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655967076619466 and perplexity of 105.21091783492463
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 541.8988387584686 and batch: 50, loss is 4.141586208343506 and perplexity is 62.90251885810735
At time: 543.5882246494293 and batch: 100, loss is 4.097083344459533 and perplexity is 60.16455216766511
At time: 545.3043742179871 and batch: 150, loss is 4.088924908638001 and perplexity is 59.6757003745621
At time: 547.0208561420441 and batch: 200, loss is 4.119054322242737 and perplexity is 61.50105457600288
At time: 548.739025592804 and batch: 250, loss is 4.113131003379822 and perplexity is 61.1378409962343
At time: 550.4531865119934 and batch: 300, loss is 4.097077240943909 and perplexity is 60.16418495350156
At time: 552.168942451477 and batch: 350, loss is 4.038393325805664 and perplexity is 56.735114693476376
At time: 553.8856773376465 and batch: 400, loss is 4.050043067932129 and perplexity is 57.39992908846311
At time: 555.6042668819427 and batch: 450, loss is 4.08590874671936 and perplexity is 59.495979968792575
At time: 557.3175852298737 and batch: 500, loss is 4.058926529884339 and perplexity is 57.91211077959423
At time: 559.0322117805481 and batch: 550, loss is 4.043782186508179 and perplexity is 57.04167759400721
At time: 560.7491414546967 and batch: 600, loss is 4.071116604804993 and perplexity is 58.62238409587377
At time: 562.467568397522 and batch: 650, loss is 4.032440896034241 and perplexity is 56.39840601922986
At time: 564.1822068691254 and batch: 700, loss is 4.003604831695557 and perplexity is 54.79532234788929
At time: 565.8973450660706 and batch: 750, loss is 3.9829855966567993 and perplexity is 53.67705326324704
At time: 567.6151189804077 and batch: 800, loss is 3.9440319204330443 and perplexity is 51.626335512238335
At time: 569.3340802192688 and batch: 850, loss is 3.964165472984314 and perplexity is 52.676291253591046
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655582110087077 and perplexity of 105.17042294780454
Finished 18 epochs...
Completing Train Step...
At time: 573.799587726593 and batch: 50, loss is 4.13955319404602 and perplexity is 62.774767042556135
At time: 575.5063014030457 and batch: 100, loss is 4.094455308914185 and perplexity is 60.00664516948353
At time: 577.2169990539551 and batch: 150, loss is 4.086270704269409 and perplexity is 59.51751888579149
At time: 578.9263792037964 and batch: 200, loss is 4.116457667350769 and perplexity is 61.34156472144735
At time: 580.635894536972 and batch: 250, loss is 4.1103034019470215 and perplexity is 60.965211727837335
At time: 582.3471474647522 and batch: 300, loss is 4.09436279296875 and perplexity is 60.0010938547698
At time: 584.0587198734283 and batch: 350, loss is 4.035826640129089 and perplexity is 56.58968020937941
At time: 585.7956449985504 and batch: 400, loss is 4.047580671310425 and perplexity is 57.25876157352513
At time: 587.5055854320526 and batch: 450, loss is 4.0837274932861325 and perplexity is 59.36634559233535
At time: 589.215948343277 and batch: 500, loss is 4.056778464317322 and perplexity is 57.78784528151613
At time: 590.9263377189636 and batch: 550, loss is 4.041885209083557 and perplexity is 56.933573387386524
At time: 592.6343586444855 and batch: 600, loss is 4.069934616088867 and perplexity is 58.55313403381845
At time: 594.3443350791931 and batch: 650, loss is 4.031838665008545 and perplexity is 56.36445137462173
At time: 596.0558803081512 and batch: 700, loss is 4.003879704475403 and perplexity is 54.81038616068681
At time: 597.7687516212463 and batch: 750, loss is 3.984238362312317 and perplexity is 53.744340170627815
At time: 599.4770801067352 and batch: 800, loss is 3.9460210514068605 and perplexity is 51.72912925648749
At time: 601.1870934963226 and batch: 850, loss is 3.965688910484314 and perplexity is 52.756601449297165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.6553850173950195 and perplexity of 105.1496966685871
Finished 19 epochs...
Completing Train Step...
At time: 605.6159279346466 and batch: 50, loss is 4.138145685195923 and perplexity is 62.68647315417203
At time: 607.349228143692 and batch: 100, loss is 4.092818493843079 and perplexity is 59.908505728272786
At time: 609.0582268238068 and batch: 150, loss is 4.0844864177703855 and perplexity is 59.41141726638117
At time: 610.768896818161 and batch: 200, loss is 4.114806704521179 and perplexity is 61.24037563088107
At time: 612.4785935878754 and batch: 250, loss is 4.10854588508606 and perplexity is 60.858158441827264
At time: 614.1862771511078 and batch: 300, loss is 4.092708106040955 and perplexity is 59.90189292499004
At time: 615.895534992218 and batch: 350, loss is 4.034331974983215 and perplexity is 56.505160766621756
At time: 617.605715751648 and batch: 400, loss is 4.046331553459168 and perplexity is 57.1872832840004
At time: 619.316465139389 and batch: 450, loss is 4.082746253013611 and perplexity is 59.308121513784755
At time: 621.0230619907379 and batch: 500, loss is 4.055979280471802 and perplexity is 57.74168061858325
At time: 622.7311544418335 and batch: 550, loss is 4.041317019462586 and perplexity is 56.901233510364264
At time: 624.4406881332397 and batch: 600, loss is 4.069591236114502 and perplexity is 58.53303151175422
At time: 626.1511623859406 and batch: 650, loss is 4.031869993209839 and perplexity is 56.366217199160154
At time: 627.9040713310242 and batch: 700, loss is 4.004294519424438 and perplexity is 54.83312704453187
At time: 629.6127772331238 and batch: 750, loss is 3.984866418838501 and perplexity is 53.778105256298744
At time: 631.3240892887115 and batch: 800, loss is 3.9469721698760987 and perplexity is 51.77835319190722
At time: 633.0351991653442 and batch: 850, loss is 3.9665452289581298 and perplexity is 52.801797249971706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655384063720703 and perplexity of 105.14959639006982
Finished 20 epochs...
Completing Train Step...
At time: 637.4743723869324 and batch: 50, loss is 4.136849250793457 and perplexity is 62.60525691094687
At time: 639.2149558067322 and batch: 100, loss is 4.091420459747314 and perplexity is 59.82481011293224
At time: 640.9302051067352 and batch: 150, loss is 4.083036284446717 and perplexity is 59.325325227950536
At time: 642.6459491252899 and batch: 200, loss is 4.1134885931015015 and perplexity is 61.15970716910657
At time: 644.3619449138641 and batch: 250, loss is 4.107197213172912 and perplexity is 60.77613607591412
At time: 646.0801255702972 and batch: 300, loss is 4.0914275598526 and perplexity is 59.82523487689068
At time: 647.7960186004639 and batch: 350, loss is 4.033166298866272 and perplexity is 56.439332424948894
At time: 649.5107321739197 and batch: 400, loss is 4.045390372276306 and perplexity is 57.133485010007405
At time: 651.2265527248383 and batch: 450, loss is 4.082055344581604 and perplexity is 59.26715918478307
At time: 652.9433043003082 and batch: 500, loss is 4.055498814582824 and perplexity is 57.71394437436697
At time: 654.6593794822693 and batch: 550, loss is 4.041064047813416 and perplexity is 56.88684093201725
At time: 656.3743484020233 and batch: 600, loss is 4.069343891143799 and perplexity is 58.51855545115396
At time: 658.0918509960175 and batch: 650, loss is 4.031830186843872 and perplexity is 56.36397350954701
At time: 659.8091275691986 and batch: 700, loss is 4.004491505622863 and perplexity is 54.84392947770632
At time: 661.5255353450775 and batch: 750, loss is 3.9852476644515993 and perplexity is 53.79861183178103
At time: 663.2412118911743 and batch: 800, loss is 3.947598071098328 and perplexity is 51.81077147071689
At time: 664.9581069946289 and batch: 850, loss is 3.9671489715576174 and perplexity is 52.83368556950086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655419985453288 and perplexity of 105.15337361359455
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 669.4034595489502 and batch: 50, loss is 4.136146459579468 and perplexity is 62.56127394367214
At time: 671.148449420929 and batch: 100, loss is 4.0910868740081785 and perplexity is 59.804856737697804
At time: 672.8649926185608 and batch: 150, loss is 4.083294773101807 and perplexity is 59.34066213360416
At time: 674.5839450359344 and batch: 200, loss is 4.114026703834534 and perplexity is 61.192626720350944
At time: 676.2997534275055 and batch: 250, loss is 4.1081860065460205 and perplexity is 60.83626083709308
At time: 678.0159270763397 and batch: 300, loss is 4.0921022319793705 and perplexity is 59.86561091410152
At time: 679.7323958873749 and batch: 350, loss is 4.032908315658569 and perplexity is 56.424773902937176
At time: 681.4499070644379 and batch: 400, loss is 4.044240708351135 and perplexity is 57.06783844634571
At time: 683.1649925708771 and batch: 450, loss is 4.081182436943054 and perplexity is 59.215447002078776
At time: 684.8797426223755 and batch: 500, loss is 4.055806760787964 and perplexity is 57.73171990133324
At time: 686.5962183475494 and batch: 550, loss is 4.04159574508667 and perplexity is 56.91709555266173
At time: 688.3141334056854 and batch: 600, loss is 4.0685348796844485 and perplexity is 58.47123241423001
At time: 690.0305123329163 and batch: 650, loss is 4.0301442718505855 and perplexity is 56.269028698454875
At time: 691.746221780777 and batch: 700, loss is 4.002507104873657 and perplexity is 54.73520505505255
At time: 693.4626324176788 and batch: 750, loss is 3.982348256111145 and perplexity is 53.64285360040469
At time: 695.1810586452484 and batch: 800, loss is 3.943953528404236 and perplexity is 51.62228857768337
At time: 696.8981897830963 and batch: 850, loss is 3.9645472288131716 and perplexity is 52.69640457376404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655399640401204 and perplexity of 105.15123428449404
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 701.3782682418823 and batch: 50, loss is 4.1360549640655515 and perplexity is 62.555550129617046
At time: 703.0872845649719 and batch: 100, loss is 4.090693187713623 and perplexity is 59.78131701918864
At time: 704.7946059703827 and batch: 150, loss is 4.082914113998413 and perplexity is 59.31807786908723
At time: 706.5030045509338 and batch: 200, loss is 4.113671083450317 and perplexity is 61.17086924385579
At time: 708.2151961326599 and batch: 250, loss is 4.108177728652954 and perplexity is 60.83575724311568
At time: 709.9432330131531 and batch: 300, loss is 4.092245106697082 and perplexity is 59.874164807414516
At time: 711.6674020290375 and batch: 350, loss is 4.032678780555725 and perplexity is 56.411823922950724
At time: 713.3936281204224 and batch: 400, loss is 4.043400568962097 and perplexity is 57.0199136419965
At time: 715.1555001735687 and batch: 450, loss is 4.080355777740478 and perplexity is 59.16651623520068
At time: 716.88214802742 and batch: 500, loss is 4.0554184103012085 and perplexity is 57.70930411268129
At time: 718.6059675216675 and batch: 550, loss is 4.041062345504761 and perplexity is 56.886744093138006
At time: 720.3314366340637 and batch: 600, loss is 4.067897825241089 and perplexity is 58.433994918227334
At time: 722.0580394268036 and batch: 650, loss is 4.029340448379517 and perplexity is 56.22381650623789
At time: 723.7856841087341 and batch: 700, loss is 4.0018077564239505 and perplexity is 54.696939456306126
At time: 725.5097239017487 and batch: 750, loss is 3.9816048669815065 and perplexity is 53.60299090473844
At time: 727.2364864349365 and batch: 800, loss is 3.942965044975281 and perplexity is 51.57128601260768
At time: 728.9641246795654 and batch: 850, loss is 3.9637307929992676 and perplexity is 52.653398899881076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.65530268351237 and perplexity of 105.14103964218914
Finished 23 epochs...
Completing Train Step...
At time: 733.4545087814331 and batch: 50, loss is 4.1359754657745365 and perplexity is 62.550577267957856
At time: 735.1626205444336 and batch: 100, loss is 4.090443058013916 and perplexity is 59.76636580626368
At time: 736.8892910480499 and batch: 150, loss is 4.082550787925721 and perplexity is 59.296529979507476
At time: 738.6166677474976 and batch: 200, loss is 4.113358383178711 and perplexity is 61.1517440868056
At time: 740.3424940109253 and batch: 250, loss is 4.1078414583206175 and perplexity is 60.81530342200892
At time: 742.0695645809174 and batch: 300, loss is 4.091964960098267 and perplexity is 59.85739361309299
At time: 743.7977073192596 and batch: 350, loss is 4.032434701919556 and perplexity is 56.39805668211684
At time: 745.5239934921265 and batch: 400, loss is 4.043172807693481 and perplexity is 57.00692819297634
At time: 747.2488720417023 and batch: 450, loss is 4.0801774549484255 and perplexity is 59.15596643749224
At time: 748.9745852947235 and batch: 500, loss is 4.05521210193634 and perplexity is 57.6973994285723
At time: 750.701092004776 and batch: 550, loss is 4.040960268974304 and perplexity is 56.88093758803099
At time: 752.4280443191528 and batch: 600, loss is 4.067868280410766 and perplexity is 58.43226852126558
At time: 754.153724193573 and batch: 650, loss is 4.029335355758667 and perplexity is 56.22353018038674
At time: 755.8799917697906 and batch: 700, loss is 4.001840901374817 and perplexity is 54.698752413721984
At time: 757.6071801185608 and batch: 750, loss is 3.9817192363739013 and perplexity is 53.60912179682528
At time: 759.3933753967285 and batch: 800, loss is 3.9431883907318115 and perplexity is 51.58280552686685
At time: 761.1185984611511 and batch: 850, loss is 3.964001522064209 and perplexity is 52.667655635100445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655239105224609 and perplexity of 105.1343551674113
Finished 24 epochs...
Completing Train Step...
At time: 765.6167960166931 and batch: 50, loss is 4.135926752090454 and perplexity is 62.54753027311345
At time: 767.3049418926239 and batch: 100, loss is 4.090259284973144 and perplexity is 59.755383368652474
At time: 769.0130577087402 and batch: 150, loss is 4.082266631126404 and perplexity is 59.279682861062916
At time: 770.7307369709015 and batch: 200, loss is 4.113083372116089 and perplexity is 61.13492899295748
At time: 772.4486277103424 and batch: 250, loss is 4.107557272911071 and perplexity is 60.798023055629564
At time: 774.1652715206146 and batch: 300, loss is 4.091723504066468 and perplexity is 59.84294242909033
At time: 775.8818097114563 and batch: 350, loss is 4.032227253913879 and perplexity is 56.3863582311862
At time: 777.5985291004181 and batch: 400, loss is 4.042981204986572 and perplexity is 56.99600655756285
At time: 779.3154897689819 and batch: 450, loss is 4.080033593177795 and perplexity is 59.14745676753996
At time: 781.0307931900024 and batch: 500, loss is 4.0550597333908085 and perplexity is 57.6886088294628
At time: 782.7456421852112 and batch: 550, loss is 4.04087131023407 and perplexity is 56.87587775654125
At time: 784.4623379707336 and batch: 600, loss is 4.067835764884949 and perplexity is 58.43036859621859
At time: 786.1800718307495 and batch: 650, loss is 4.029328331947327 and perplexity is 56.22313527830476
At time: 787.8970458507538 and batch: 700, loss is 4.001868214607239 and perplexity is 54.70024643386299
At time: 789.6121363639832 and batch: 750, loss is 3.981820592880249 and perplexity is 53.614555705495306
At time: 791.3288369178772 and batch: 800, loss is 3.9433823919296263 and perplexity is 51.5928136236857
At time: 793.0469861030579 and batch: 850, loss is 3.9642471170425413 and perplexity is 52.68059213534969
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655201594034831 and perplexity of 105.13041152662818
Finished 25 epochs...
Completing Train Step...
At time: 797.4713060855865 and batch: 50, loss is 4.1358835315704345 and perplexity is 62.54482699474806
At time: 799.1767511367798 and batch: 100, loss is 4.090097579956055 and perplexity is 59.745721404578745
At time: 800.8881599903107 and batch: 150, loss is 4.082018780708313 and perplexity is 59.26499218750148
At time: 802.6415295600891 and batch: 200, loss is 4.112838006019592 and perplexity is 61.119930394219935
At time: 804.3507330417633 and batch: 250, loss is 4.107305550575257 and perplexity is 60.78272076129852
At time: 806.0612120628357 and batch: 300, loss is 4.09150797367096 and perplexity is 59.830045845893
At time: 807.7730300426483 and batch: 350, loss is 4.032041392326355 and perplexity is 56.37587914699043
At time: 809.4814972877502 and batch: 400, loss is 4.04281455039978 and perplexity is 56.98650870309369
At time: 811.1903328895569 and batch: 450, loss is 4.079910426139832 and perplexity is 59.14017219910541
At time: 812.8997011184692 and batch: 500, loss is 4.054933938980103 and perplexity is 57.68135238133054
At time: 814.6097347736359 and batch: 550, loss is 4.040793347358703 and perplexity is 56.87144372241964
At time: 816.3183331489563 and batch: 600, loss is 4.067804999351502 and perplexity is 58.428570982411635
At time: 818.0270607471466 and batch: 650, loss is 4.029321813583374 and perplexity is 56.22276879664089
At time: 819.7370381355286 and batch: 700, loss is 4.0018962335586545 and perplexity is 54.70177909888199
At time: 821.447101354599 and batch: 750, loss is 3.9819123935699463 and perplexity is 53.61947778460857
At time: 823.1568040847778 and batch: 800, loss is 3.943555326461792 and perplexity is 51.601736574293746
At time: 824.865921497345 and batch: 850, loss is 3.9644683694839475 and perplexity is 52.69224913449657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655176162719727 and perplexity of 105.12773795600194
Finished 26 epochs...
Completing Train Step...
At time: 829.3116886615753 and batch: 50, loss is 4.135838403701782 and perplexity is 62.54200454369662
At time: 831.0476124286652 and batch: 100, loss is 4.089946947097778 and perplexity is 59.73672241358266
At time: 832.7562007904053 and batch: 150, loss is 4.081794738769531 and perplexity is 59.25171583103577
At time: 834.4666287899017 and batch: 200, loss is 4.112616181373596 and perplexity is 61.10637399092472
At time: 836.1780931949615 and batch: 250, loss is 4.107079195976257 and perplexity is 60.76896386994105
At time: 837.8864815235138 and batch: 300, loss is 4.091312837600708 and perplexity is 59.81837198489637
At time: 839.5953447818756 and batch: 350, loss is 4.031871271133423 and perplexity is 56.36628923092447
At time: 841.3047859668732 and batch: 400, loss is 4.0426667785644534 and perplexity is 56.97808832427653
At time: 843.0146882534027 and batch: 450, loss is 4.079802436828613 and perplexity is 59.13378603746908
At time: 844.7503294944763 and batch: 500, loss is 4.054827489852905 and perplexity is 57.67521257850811
At time: 846.4590218067169 and batch: 550, loss is 4.040726742744446 and perplexity is 56.86765594799126
At time: 848.1692187786102 and batch: 600, loss is 4.067777009010315 and perplexity is 58.42693556966275
At time: 849.8806531429291 and batch: 650, loss is 4.029316940307617 and perplexity is 56.22249480825233
At time: 851.5886390209198 and batch: 700, loss is 4.001926536560059 and perplexity is 54.703436752086645
At time: 853.2963972091675 and batch: 750, loss is 3.9819955015182495 and perplexity is 53.62393417457449
At time: 855.0066246986389 and batch: 800, loss is 3.943709568977356 and perplexity is 51.60969636980409
At time: 856.7182950973511 and batch: 850, loss is 3.96466570854187 and perplexity is 52.702648399357415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655160586039226 and perplexity of 105.12610042756967
Finished 27 epochs...
Completing Train Step...
At time: 861.1404254436493 and batch: 50, loss is 4.135788040161133 and perplexity is 62.53885478622564
At time: 862.8960945606232 and batch: 100, loss is 4.089802746772766 and perplexity is 59.72810897984036
At time: 864.613431930542 and batch: 150, loss is 4.08158878326416 and perplexity is 59.23951387053129
At time: 866.3287601470947 and batch: 200, loss is 4.112413649559021 and perplexity is 61.09399925929895
At time: 868.0446019172668 and batch: 250, loss is 4.1068730592727665 and perplexity is 60.75643844707339
At time: 869.7619128227234 and batch: 300, loss is 4.091134071350098 and perplexity is 59.80767943458105
At time: 871.4792561531067 and batch: 350, loss is 4.031713681221008 and perplexity is 56.35740717222124
At time: 873.1939389705658 and batch: 400, loss is 4.042533168792724 and perplexity is 56.97047600345351
At time: 874.9096274375916 and batch: 450, loss is 4.079706315994263 and perplexity is 59.12810232178297
At time: 876.6255507469177 and batch: 500, loss is 4.054736471176147 and perplexity is 57.66996329587246
At time: 878.3425936698914 and batch: 550, loss is 4.040669302940369 and perplexity is 56.864389574786124
At time: 880.0565927028656 and batch: 600, loss is 4.067751789093018 and perplexity is 58.425462065760684
At time: 881.7726893424988 and batch: 650, loss is 4.0293138265609745 and perplexity is 56.222319745920416
At time: 883.4896478652954 and batch: 700, loss is 4.001958751678467 and perplexity is 54.70519905816526
At time: 885.2070858478546 and batch: 750, loss is 3.9820711040496826 and perplexity is 53.627988432997654
At time: 886.9226055145264 and batch: 800, loss is 3.9438466930389406 and perplexity is 51.61677378621842
At time: 888.6674106121063 and batch: 850, loss is 3.9648407602310183 and perplexity is 52.71187489451552
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655150413513184 and perplexity of 105.1250310350146
Finished 28 epochs...
Completing Train Step...
At time: 893.1173515319824 and batch: 50, loss is 4.135731344223022 and perplexity is 62.53530918769665
At time: 894.8273594379425 and batch: 100, loss is 4.089662828445435 and perplexity is 59.719752507362685
At time: 896.5443484783173 and batch: 150, loss is 4.081396870613098 and perplexity is 59.22814614921641
At time: 898.2618126869202 and batch: 200, loss is 4.112226963043213 and perplexity is 61.082594897993815
At time: 899.9784653186798 and batch: 250, loss is 4.106683969497681 and perplexity is 60.74495111189546
At time: 901.6933109760284 and batch: 300, loss is 4.090968656539917 and perplexity is 59.797787176825544
At time: 903.4090638160706 and batch: 350, loss is 4.031566386222839 and perplexity is 56.34910661936495
At time: 905.1266505718231 and batch: 400, loss is 4.042410502433777 and perplexity is 56.96348807119551
At time: 906.8436470031738 and batch: 450, loss is 4.079618787765503 and perplexity is 59.12292717020603
At time: 908.5579733848572 and batch: 500, loss is 4.054658012390137 and perplexity is 57.665438758060205
At time: 910.2736384868622 and batch: 550, loss is 4.040620470046997 and perplexity is 56.861612789913146
At time: 911.9902422428131 and batch: 600, loss is 4.067729020118714 and perplexity is 58.42413179306073
At time: 913.7073669433594 and batch: 650, loss is 4.029312400817871 and perplexity is 56.22223958739294
At time: 915.42214012146 and batch: 700, loss is 4.0019924926757815 and perplexity is 54.707044897279815
At time: 917.1368689537048 and batch: 750, loss is 3.982140049934387 and perplexity is 53.63168598956928
At time: 918.8539929389954 and batch: 800, loss is 3.9439683389663696 and perplexity is 51.6230531384576
At time: 920.5713019371033 and batch: 850, loss is 3.9649964475631716 and perplexity is 52.720082104553384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.65514341990153 and perplexity of 105.12429583394335
Finished 29 epochs...
Completing Train Step...
At time: 925.0533928871155 and batch: 50, loss is 4.135667877197266 and perplexity is 62.53134038356316
At time: 926.7633028030396 and batch: 100, loss is 4.089525818824768 and perplexity is 59.711570887218485
At time: 928.470901966095 and batch: 150, loss is 4.081216654777527 and perplexity is 59.21747326110941
At time: 930.1794035434723 and batch: 200, loss is 4.1120535230636595 and perplexity is 61.07200165265632
At time: 931.9168508052826 and batch: 250, loss is 4.106508679389954 and perplexity is 60.734304056060246
At time: 933.6270847320557 and batch: 300, loss is 4.090814347267151 and perplexity is 59.78856053566974
At time: 935.3358464241028 and batch: 350, loss is 4.0314273929595945 and perplexity is 56.34127501743757
At time: 937.0440354347229 and batch: 400, loss is 4.042296452522278 and perplexity is 56.95699176088117
At time: 938.7524704933167 and batch: 450, loss is 4.079538369178772 and perplexity is 59.11817277913292
At time: 940.4615728855133 and batch: 500, loss is 4.0545892333984375 and perplexity is 57.66147272371801
At time: 942.1701295375824 and batch: 550, loss is 4.040578017234802 and perplexity is 56.8591989057827
At time: 943.8836371898651 and batch: 600, loss is 4.067708067893982 and perplexity is 58.422907690345504
At time: 945.6088881492615 and batch: 650, loss is 4.029312243461609 and perplexity is 56.22223074047216
At time: 947.3354120254517 and batch: 700, loss is 4.002026844024658 and perplexity is 54.708924190343026
At time: 949.061244726181 and batch: 750, loss is 3.9822027778625486 and perplexity is 53.63505029963221
At time: 950.7858943939209 and batch: 800, loss is 3.944076795578003 and perplexity is 51.62865230351091
At time: 952.5117008686066 and batch: 850, loss is 3.9651351881027224 and perplexity is 52.727397024615975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655141830444336 and perplexity of 105.12412874350782
Finished 30 epochs...
Completing Train Step...
At time: 956.9931621551514 and batch: 50, loss is 4.135598497390747 and perplexity is 62.527002121761626
At time: 958.6976850032806 and batch: 100, loss is 4.089391016960144 and perplexity is 59.70352219862458
At time: 960.406388759613 and batch: 150, loss is 4.081045651435852 and perplexity is 59.2073477410698
At time: 962.1167943477631 and batch: 200, loss is 4.111891074180603 and perplexity is 61.06208137999224
At time: 963.824099779129 and batch: 250, loss is 4.106345191001892 and perplexity is 60.72437551421281
At time: 965.5324814319611 and batch: 300, loss is 4.090669121742248 and perplexity is 59.779878341036174
At time: 967.2422337532043 and batch: 350, loss is 4.031295738220215 and perplexity is 56.33385790981822
At time: 968.9528169631958 and batch: 400, loss is 4.042189040184021 and perplexity is 56.9508742057724
At time: 970.6605195999146 and batch: 450, loss is 4.079462895393371 and perplexity is 59.113711075220245
At time: 972.3674530982971 and batch: 500, loss is 4.054528217315674 and perplexity is 57.65795455385962
At time: 974.0761625766754 and batch: 550, loss is 4.040540924072266 and perplexity is 56.857089857391884
At time: 975.8136661052704 and batch: 600, loss is 4.067688374519348 and perplexity is 58.42175715746617
At time: 977.5213391780853 and batch: 650, loss is 4.029312615394592 and perplexity is 56.222251651378045
At time: 979.229355096817 and batch: 700, loss is 4.002061228752137 and perplexity is 54.71080537413377
At time: 980.9378879070282 and batch: 750, loss is 3.9822602224349977 and perplexity is 53.63813143066124
At time: 982.6479125022888 and batch: 800, loss is 3.9441738080978395 and perplexity is 51.6336611721242
At time: 984.3570363521576 and batch: 850, loss is 3.965259733200073 and perplexity is 52.733964372368455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655138969421387 and perplexity of 105.12382798139322
Finished 31 epochs...
Completing Train Step...
At time: 988.8165192604065 and batch: 50, loss is 4.135523958206177 and perplexity is 62.52234158370835
At time: 990.5389497280121 and batch: 100, loss is 4.089258089065551 and perplexity is 59.695586462570965
At time: 992.252799987793 and batch: 150, loss is 4.0808825159072875 and perplexity is 59.19768970690681
At time: 993.9693484306335 and batch: 200, loss is 4.111737766265869 and perplexity is 61.05272079717062
At time: 995.6864469051361 and batch: 250, loss is 4.106191520690918 and perplexity is 60.715044697496964
At time: 997.4041223526001 and batch: 300, loss is 4.0905317783355715 and perplexity is 59.77166853268851
At time: 999.1179480552673 and batch: 350, loss is 4.03116997718811 and perplexity is 56.32677375116994
At time: 1000.8328194618225 and batch: 400, loss is 4.042086362838745 and perplexity is 56.94502694139374
At time: 1002.5487170219421 and batch: 450, loss is 4.079391312599182 and perplexity is 59.10947970205516
At time: 1004.2659029960632 and batch: 500, loss is 4.054473419189453 and perplexity is 57.65479509255537
At time: 1005.9804196357727 and batch: 550, loss is 4.040507726669311 and perplexity is 56.8552023809989
At time: 1007.6955099105835 and batch: 600, loss is 4.067669601440429 and perplexity is 58.42066041150315
At time: 1009.411634683609 and batch: 650, loss is 4.0293131875991826 and perplexity is 56.22228382201772
At time: 1011.1292164325714 and batch: 700, loss is 4.002094960212707 and perplexity is 54.71265088063367
At time: 1012.8437395095825 and batch: 750, loss is 3.9823125648498534 and perplexity is 53.6409390534669
At time: 1014.5591886043549 and batch: 800, loss is 3.9442613077163697 and perplexity is 51.63817929544423
At time: 1016.2767715454102 and batch: 850, loss is 3.9653724575042726 and perplexity is 52.739909106861745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655138333638509 and perplexity of 105.12376114548458
Finished 32 epochs...
Completing Train Step...
At time: 1020.6987521648407 and batch: 50, loss is 4.1354452323913575 and perplexity is 62.51741965516673
At time: 1022.4360046386719 and batch: 100, loss is 4.089126625061035 and perplexity is 59.687739157553025
At time: 1024.1455690860748 and batch: 150, loss is 4.080726146697998 and perplexity is 59.18843373466897
At time: 1025.8564894199371 and batch: 200, loss is 4.111592164039612 and perplexity is 61.04383203223118
At time: 1027.5638630390167 and batch: 250, loss is 4.106046323776245 and perplexity is 60.706229700303346
At time: 1029.2735707759857 and batch: 300, loss is 4.0904011011123655 and perplexity is 59.76385824734259
At time: 1030.9835987091064 and batch: 350, loss is 4.031049304008484 and perplexity is 56.31997703038248
At time: 1032.6946716308594 and batch: 400, loss is 4.041988306045532 and perplexity is 56.939443368420534
At time: 1034.4017407894135 and batch: 450, loss is 4.079322938919067 and perplexity is 59.10543830756233
At time: 1036.109992980957 and batch: 500, loss is 4.0544231367111205 and perplexity is 57.651896139454266
At time: 1037.819509267807 and batch: 550, loss is 4.040477485656738 and perplexity is 56.853483048106156
At time: 1039.530522108078 and batch: 600, loss is 4.0676511907577515 and perplexity is 58.41958485716336
At time: 1041.2379486560822 and batch: 650, loss is 4.029313688278198 and perplexity is 56.22231197134249
At time: 1042.9456968307495 and batch: 700, loss is 4.002127666473388 and perplexity is 54.71444035611929
At time: 1044.6554851531982 and batch: 750, loss is 3.982360563278198 and perplexity is 53.64351379602771
At time: 1046.3669657707214 and batch: 800, loss is 3.944340777397156 and perplexity is 51.6422831281322
At time: 1048.0758395195007 and batch: 850, loss is 3.9654755449295043 and perplexity is 52.74534620854212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655141194661458 and perplexity of 105.12406190740796
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 1052.5506162643433 and batch: 50, loss is 4.135438661575318 and perplexity is 62.51700886605251
At time: 1054.2585091590881 and batch: 100, loss is 4.089102592468262 and perplexity is 59.686304723660925
At time: 1055.966182231903 and batch: 150, loss is 4.0807056760787965 and perplexity is 59.18722212318211
At time: 1057.674694776535 and batch: 200, loss is 4.111577286720276 and perplexity is 61.04292387040404
At time: 1059.384323835373 and batch: 250, loss is 4.106145253181458 and perplexity is 60.712235628576856
At time: 1061.0935769081116 and batch: 300, loss is 4.0905375576019285 and perplexity is 59.77201397007975
At time: 1062.8304827213287 and batch: 350, loss is 4.031059145927429 and perplexity is 56.32053132975911
At time: 1064.5486767292023 and batch: 400, loss is 4.041803669929505 and perplexity is 56.92893126123541
At time: 1066.275024175644 and batch: 450, loss is 4.079087896347046 and perplexity is 59.091547645834446
At time: 1068.0008690357208 and batch: 500, loss is 4.054323072433472 and perplexity is 57.646127532732535
At time: 1069.7236034870148 and batch: 550, loss is 4.040305271148681 and perplexity is 56.84369289651898
At time: 1071.447265625 and batch: 600, loss is 4.067422819137573 and perplexity is 58.406245005199175
At time: 1073.173644065857 and batch: 650, loss is 4.029045457839966 and perplexity is 56.20723345831247
At time: 1074.9003579616547 and batch: 700, loss is 4.0018924045562745 and perplexity is 54.70156964604063
At time: 1076.6237149238586 and batch: 750, loss is 3.982070093154907 and perplexity is 53.62793422077173
At time: 1078.3490619659424 and batch: 800, loss is 3.943904719352722 and perplexity is 51.61976900423041
At time: 1080.0752103328705 and batch: 850, loss is 3.9651247549057005 and perplexity is 52.72684691216409
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655136426289876 and perplexity of 105.1235606380137
Finished 34 epochs...
Completing Train Step...
At time: 1084.573642730713 and batch: 50, loss is 4.135412826538086 and perplexity is 62.51539375766409
At time: 1086.2830703258514 and batch: 100, loss is 4.08905134677887 and perplexity is 59.68324613619851
At time: 1088.0002734661102 and batch: 150, loss is 4.080640454292297 and perplexity is 59.18336195270227
At time: 1089.7168157100677 and batch: 200, loss is 4.111527404785156 and perplexity is 61.039879007178506
At time: 1091.4333727359772 and batch: 250, loss is 4.10608829498291 and perplexity is 60.708777667486196
At time: 1093.14967918396 and batch: 300, loss is 4.090488905906677 and perplexity is 59.76910603101019
At time: 1094.8670001029968 and batch: 350, loss is 4.03101381778717 and perplexity is 56.31797848267387
At time: 1096.583164691925 and batch: 400, loss is 4.0417692089080814 and perplexity is 56.92696946591855
At time: 1098.2980906963348 and batch: 450, loss is 4.07906325340271 and perplexity is 59.090091474057346
At time: 1100.0135900974274 and batch: 500, loss is 4.054295196533203 and perplexity is 57.64452061742775
At time: 1101.7309758663177 and batch: 550, loss is 4.0402994632720945 and perplexity is 56.84336275632461
At time: 1103.4524455070496 and batch: 600, loss is 4.067424445152283 and perplexity is 58.40633997468989
At time: 1105.2204477787018 and batch: 650, loss is 4.029051632881164 and perplexity is 56.20758054136635
At time: 1106.952087879181 and batch: 700, loss is 4.00191192150116 and perplexity is 54.70263726397883
At time: 1108.683921098709 and batch: 750, loss is 3.982092609405518 and perplexity is 53.629141734372666
At time: 1110.417310476303 and batch: 800, loss is 3.9439404964447022 and perplexity is 51.621615842491124
At time: 1112.148909330368 and batch: 850, loss is 3.965160632133484 and perplexity is 52.72873863919581
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655130386352539 and perplexity of 105.1229257002123
Finished 35 epochs...
Completing Train Step...
At time: 1116.6800999641418 and batch: 50, loss is 4.135387506484985 and perplexity is 62.513810884613854
At time: 1118.3743314743042 and batch: 100, loss is 4.089004163742065 and perplexity is 59.68043016583298
At time: 1120.0789930820465 and batch: 150, loss is 4.080580744743347 and perplexity is 59.17982824635381
At time: 1121.796011686325 and batch: 200, loss is 4.111478567123413 and perplexity is 61.03689803500716
At time: 1123.5138444900513 and batch: 250, loss is 4.106034927368164 and perplexity is 60.7055378712788
At time: 1125.2288210391998 and batch: 300, loss is 4.090442605018616 and perplexity is 59.766338732387005
At time: 1126.9441838264465 and batch: 350, loss is 4.030972008705139 and perplexity is 56.31562392891287
At time: 1128.6608493328094 and batch: 400, loss is 4.041736273765564 and perplexity is 56.925094598940795
At time: 1130.3776552677155 and batch: 450, loss is 4.079040460586548 and perplexity is 59.08874465981426
At time: 1132.0926885604858 and batch: 500, loss is 4.054273357391358 and perplexity is 57.64326172431201
At time: 1133.807848930359 and batch: 550, loss is 4.040293536186218 and perplexity is 56.84302584183052
At time: 1135.524908065796 and batch: 600, loss is 4.067424473762512 and perplexity is 58.40634164570867
At time: 1137.2435307502747 and batch: 650, loss is 4.029057078361511 and perplexity is 56.20788661947489
At time: 1138.9588191509247 and batch: 700, loss is 4.001928796768189 and perplexity is 54.70356039337886
At time: 1140.6736934185028 and batch: 750, loss is 3.98211350440979 and perplexity is 53.63026232722567
At time: 1142.3907430171967 and batch: 800, loss is 3.943971900939941 and perplexity is 51.623237018736056
At time: 1144.1076128482819 and batch: 850, loss is 3.965195002555847 and perplexity is 52.730550979358796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.6551259358723955 and perplexity of 105.12245785375991
Finished 36 epochs...
Completing Train Step...
At time: 1148.5407741069794 and batch: 50, loss is 4.1353623485565185 and perplexity is 62.512238186414436
At time: 1150.279352903366 and batch: 100, loss is 4.088959679603577 and perplexity is 59.67777539236053
At time: 1152.009957075119 and batch: 150, loss is 4.080524826049805 and perplexity is 59.1765190801972
At time: 1153.7345249652863 and batch: 200, loss is 4.1114303541183475 and perplexity is 61.03395533367181
At time: 1155.4592008590698 and batch: 250, loss is 4.105983953475953 and perplexity is 60.70244355260013
At time: 1157.185621023178 and batch: 300, loss is 4.090398049354553 and perplexity is 59.76367586279956
At time: 1158.9136998653412 and batch: 350, loss is 4.030931916236877 and perplexity is 56.313366141808295
At time: 1160.638605594635 and batch: 400, loss is 4.041704287528992 and perplexity is 56.923273808518324
At time: 1162.3628442287445 and batch: 450, loss is 4.079018697738648 and perplexity is 59.08745873444439
At time: 1164.0877261161804 and batch: 500, loss is 4.054254536628723 and perplexity is 57.64217684437478
At time: 1165.8142023086548 and batch: 550, loss is 4.040287599563599 and perplexity is 56.84268838723923
At time: 1167.5386266708374 and batch: 600, loss is 4.067423157691955 and perplexity is 58.4062647788927
At time: 1169.263517856598 and batch: 650, loss is 4.029061813354492 and perplexity is 56.208152764053594
At time: 1170.9895157814026 and batch: 700, loss is 4.001944227218628 and perplexity is 54.70440450046881
At time: 1172.7173247337341 and batch: 750, loss is 3.9821327543258667 and perplexity is 53.631294715211304
At time: 1174.4420597553253 and batch: 800, loss is 3.9440003204345704 and perplexity is 51.62470414589066
At time: 1176.168091058731 and batch: 850, loss is 3.9652277994155884 and perplexity is 52.732280404203046
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655120213826497 and perplexity of 105.12185633995207
Finished 37 epochs...
Completing Train Step...
At time: 1180.6422033309937 and batch: 50, loss is 4.135337047576904 and perplexity is 62.51065658555853
At time: 1182.371282339096 and batch: 100, loss is 4.088916850090027 and perplexity is 59.67521947700543
At time: 1184.0796437263489 and batch: 150, loss is 4.0804712677001955 and perplexity is 59.17334976837194
At time: 1185.7898259162903 and batch: 200, loss is 4.111383275985718 and perplexity is 61.031082036662966
At time: 1187.498684167862 and batch: 250, loss is 4.105934844017029 and perplexity is 60.699462561639955
At time: 1189.2069330215454 and batch: 300, loss is 4.090354852676391 and perplexity is 59.76109432628487
At time: 1190.9160568714142 and batch: 350, loss is 4.030893235206604 and perplexity is 56.31118792491589
At time: 1192.6546711921692 and batch: 400, loss is 4.041672973632813 and perplexity is 56.92149134694016
At time: 1194.3642468452454 and batch: 450, loss is 4.078997850418091 and perplexity is 59.086226932091186
At time: 1196.0707268714905 and batch: 500, loss is 4.054237713813782 and perplexity is 57.641207148857454
At time: 1197.7802441120148 and batch: 550, loss is 4.0402813816070555 and perplexity is 56.842334942971874
At time: 1199.4907591342926 and batch: 600, loss is 4.067421183586121 and perplexity is 58.40614947885844
At time: 1201.2004146575928 and batch: 650, loss is 4.029065852165222 and perplexity is 56.20837977860254
At time: 1202.9075231552124 and batch: 700, loss is 4.001958227157592 and perplexity is 54.70517036415392
At time: 1204.615667104721 and batch: 750, loss is 3.9821508884429933 and perplexity is 53.6322672802096
At time: 1206.3256497383118 and batch: 800, loss is 3.9440264654159547 and perplexity is 51.62605389046397
At time: 1208.0367891788483 and batch: 850, loss is 3.965259509086609 and perplexity is 52.73395255397834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655118306477864 and perplexity of 105.12165583611431
Finished 38 epochs...
Completing Train Step...
At time: 1212.4812998771667 and batch: 50, loss is 4.135311603546143 and perplexity is 62.50906608272394
At time: 1214.2223958969116 and batch: 100, loss is 4.088874959945679 and perplexity is 59.67271972580538
At time: 1215.937898159027 and batch: 150, loss is 4.080419726371765 and perplexity is 59.170299973913124
At time: 1217.6542403697968 and batch: 200, loss is 4.111337132453919 and perplexity is 61.028265911961746
At time: 1219.3710842132568 and batch: 250, loss is 4.1058874559402465 and perplexity is 60.696586199000606
At time: 1221.0894720554352 and batch: 300, loss is 4.09031280040741 and perplexity is 59.75858128951144
At time: 1222.8063321113586 and batch: 350, loss is 4.03085566997528 and perplexity is 56.309072621846546
At time: 1224.521450996399 and batch: 400, loss is 4.041642141342163 and perplexity is 56.919736354030114
At time: 1226.2382123470306 and batch: 450, loss is 4.078977265357971 and perplexity is 59.0850106510762
At time: 1227.9557509422302 and batch: 500, loss is 4.054222121238708 and perplexity is 57.64030838101471
At time: 1229.6723182201385 and batch: 550, loss is 4.040274963378907 and perplexity is 56.841970117068456
At time: 1231.3876330852509 and batch: 600, loss is 4.06741849899292 and perplexity is 58.40599268231711
At time: 1233.1048879623413 and batch: 650, loss is 4.029069061279297 and perplexity is 56.20856015799465
At time: 1234.8377916812897 and batch: 700, loss is 4.001971383094787 and perplexity is 54.70589006667363
At time: 1236.6073942184448 and batch: 750, loss is 3.9821679830551147 and perplexity is 53.633184110852355
At time: 1238.339409828186 and batch: 800, loss is 3.944051213264465 and perplexity is 51.62733154003432
At time: 1240.0724279880524 and batch: 850, loss is 3.9652899312973022 and perplexity is 52.7355568617968
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655116081237793 and perplexity of 105.12142191545365
Finished 39 epochs...
Completing Train Step...
At time: 1244.5714104175568 and batch: 50, loss is 4.135286169052124 and perplexity is 62.50747621647537
At time: 1246.256031513214 and batch: 100, loss is 4.088833708763122 and perplexity is 59.670258206320945
At time: 1247.9589138031006 and batch: 150, loss is 4.080369434356689 and perplexity is 59.16732425512286
At time: 1249.6707320213318 and batch: 200, loss is 4.111291627883912 and perplexity is 61.025488910146755
At time: 1251.3797574043274 and batch: 250, loss is 4.105841274261475 and perplexity is 60.69378319347847
At time: 1253.0892388820648 and batch: 300, loss is 4.090271682739258 and perplexity is 59.75612420651185
At time: 1254.7999472618103 and batch: 350, loss is 4.030818934440613 and perplexity is 56.30700411595122
At time: 1256.5113279819489 and batch: 400, loss is 4.0416119050979615 and perplexity is 56.91801534100043
At time: 1258.2219293117523 and batch: 450, loss is 4.078957056999206 and perplexity is 59.08381665204775
At time: 1259.9307837486267 and batch: 500, loss is 4.054207220077514 and perplexity is 57.63944947988759
At time: 1261.6409747600555 and batch: 550, loss is 4.04026834487915 and perplexity is 56.84159390974806
At time: 1263.352177619934 and batch: 600, loss is 4.067415370941162 and perplexity is 58.40580998563477
At time: 1265.0616900920868 and batch: 650, loss is 4.029071788787842 and perplexity is 56.20871346753187
At time: 1266.770093679428 and batch: 700, loss is 4.001983799934387 and perplexity is 54.70656934515301
At time: 1268.4801399707794 and batch: 750, loss is 3.982184076309204 and perplexity is 53.63404725025721
At time: 1270.1919977664948 and batch: 800, loss is 3.9440748643875123 and perplexity is 51.62855259884481
At time: 1271.9018909931183 and batch: 850, loss is 3.96531973361969 and perplexity is 52.73712852728322
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655115127563477 and perplexity of 105.12132166390127
Finished 40 epochs...
Completing Train Step...
At time: 1276.3559799194336 and batch: 50, loss is 4.135260457992554 and perplexity is 62.50586910369115
At time: 1278.0667660236359 and batch: 100, loss is 4.088793053627014 and perplexity is 59.66783235316397
At time: 1279.8019437789917 and batch: 150, loss is 4.080320420265198 and perplexity is 59.16442429354859
At time: 1281.5117087364197 and batch: 200, loss is 4.111247148513794 and perplexity is 61.02277459520486
At time: 1283.221064567566 and batch: 250, loss is 4.105796175003052 and perplexity is 60.69104601058848
At time: 1284.9315950870514 and batch: 300, loss is 4.090231671333313 and perplexity is 59.753733327800056
At time: 1286.6392364501953 and batch: 350, loss is 4.030782985687256 and perplexity is 56.30497998573067
At time: 1288.3473064899445 and batch: 400, loss is 4.041582117080688 and perplexity is 56.91631989142846
At time: 1290.056470632553 and batch: 450, loss is 4.078936953544616 and perplexity is 59.08262887516194
At time: 1291.7669250965118 and batch: 500, loss is 4.0541928052902225 and perplexity is 57.638618625472034
At time: 1293.4740035533905 and batch: 550, loss is 4.04026153087616 and perplexity is 56.84120659227678
At time: 1295.1822881698608 and batch: 600, loss is 4.067411665916443 and perplexity is 58.40559359106591
At time: 1296.891449213028 and batch: 650, loss is 4.029074335098267 and perplexity is 56.20885659254716
At time: 1298.6017696857452 and batch: 700, loss is 4.0019956302642825 and perplexity is 54.70721654574409
At time: 1300.3095803260803 and batch: 750, loss is 3.9821998023986818 and perplexity is 53.634890710715474
At time: 1302.0176100730896 and batch: 800, loss is 3.944097456932068 and perplexity is 51.629719032396046
At time: 1303.7276933193207 and batch: 850, loss is 3.965348596572876 and perplexity is 52.73865069852214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655114809672038 and perplexity of 105.12128824673842
Finished 41 epochs...
Completing Train Step...
At time: 1308.2061908245087 and batch: 50, loss is 4.135234689712524 and perplexity is 62.50425845570449
At time: 1309.9163250923157 and batch: 100, loss is 4.088752737045288 and perplexity is 59.66542679861672
At time: 1311.6322991847992 and batch: 150, loss is 4.080272216796875 and perplexity is 59.161572431831644
At time: 1313.3501126766205 and batch: 200, loss is 4.111203474998474 and perplexity is 61.02010957431956
At time: 1315.064962387085 and batch: 250, loss is 4.10575222492218 and perplexity is 60.68837869282299
At time: 1316.7823369503021 and batch: 300, loss is 4.090192370414734 and perplexity is 59.751384997337816
At time: 1318.500257730484 and batch: 350, loss is 4.030747771263123 and perplexity is 56.30299727319489
At time: 1320.218433856964 and batch: 400, loss is 4.041552782058716 and perplexity is 56.914650274423096
At time: 1321.9323341846466 and batch: 450, loss is 4.078917088508606 and perplexity is 59.081455208269276
At time: 1323.6858222484589 and batch: 500, loss is 4.054178714752197 and perplexity is 57.63780647204642
At time: 1325.4173436164856 and batch: 550, loss is 4.040254654884339 and perplexity is 56.84081575394884
At time: 1327.1507759094238 and batch: 600, loss is 4.067407870292664 and perplexity is 58.405371905826776
At time: 1328.8808290958405 and batch: 650, loss is 4.029076299667358 and perplexity is 56.20896701883795
At time: 1330.6117715835571 and batch: 700, loss is 4.002007122039795 and perplexity is 54.707845232407905
At time: 1332.3439123630524 and batch: 750, loss is 3.9822144985198973 and perplexity is 53.63567894136269
At time: 1334.0782928466797 and batch: 800, loss is 3.9441192388534545 and perplexity is 51.630843639125224
At time: 1335.808609008789 and batch: 850, loss is 3.965376753807068 and perplexity is 52.7401356939674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655114809672038 and perplexity of 105.12128824673842
Finished 42 epochs...
Completing Train Step...
At time: 1340.2764930725098 and batch: 50, loss is 4.135209121704102 and perplexity is 62.502660366727916
At time: 1341.9937450885773 and batch: 100, loss is 4.088712849617004 and perplexity is 59.663046945647686
At time: 1343.708176612854 and batch: 150, loss is 4.0802250528335575 and perplexity is 59.158782203399326
At time: 1345.4240038394928 and batch: 200, loss is 4.111160607337951 and perplexity is 61.01749384104286
At time: 1347.1406269073486 and batch: 250, loss is 4.10570915222168 and perplexity is 60.68576473675916
At time: 1348.8570442199707 and batch: 300, loss is 4.090153965950012 and perplexity is 59.74909032144382
At time: 1350.571929216385 and batch: 350, loss is 4.030712847709656 and perplexity is 56.30103100679398
At time: 1352.2872762680054 and batch: 400, loss is 4.041523714065551 and perplexity is 56.912995903802695
At time: 1354.012459039688 and batch: 450, loss is 4.078897361755371 and perplexity is 59.08028973447714
At time: 1355.7443752288818 and batch: 500, loss is 4.054165167808533 and perplexity is 57.637025661218004
At time: 1357.4743919372559 and batch: 550, loss is 4.040247855186462 and perplexity is 56.84042925488868
At time: 1359.2061722278595 and batch: 600, loss is 4.067403821945191 and perplexity is 58.4051354610656
At time: 1360.9394795894623 and batch: 650, loss is 4.029077830314637 and perplexity is 56.20905305500618
At time: 1362.6724390983582 and batch: 700, loss is 4.002017908096313 and perplexity is 54.70843531750091
At time: 1364.4034559726715 and batch: 750, loss is 3.982228708267212 and perplexity is 53.636441096222505
At time: 1366.1956255435944 and batch: 800, loss is 3.9441402673721315 and perplexity is 51.631929370700625
At time: 1367.9288675785065 and batch: 850, loss is 3.965404233932495 and perplexity is 52.74158501942505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.6551157633463545 and perplexity of 105.12138849825894
Annealing...
Finished 43 epochs...
Completing Train Step...
At time: 1372.3923423290253 and batch: 50, loss is 4.135206661224365 and perplexity is 62.5025065803878
At time: 1374.1159038543701 and batch: 100, loss is 4.088705725669861 and perplexity is 59.6626219107688
At time: 1375.8260307312012 and batch: 150, loss is 4.080216226577758 and perplexity is 59.15826005515914
At time: 1377.535367488861 and batch: 200, loss is 4.111154298782349 and perplexity is 61.017108910004424
At time: 1379.2426979541779 and batch: 250, loss is 4.105737600326538 and perplexity is 60.68749115631438
At time: 1380.951334953308 and batch: 300, loss is 4.090196566581726 and perplexity is 59.75163572465335
At time: 1382.6613988876343 and batch: 350, loss is 4.030716438293457 and perplexity is 56.30123316072681
At time: 1384.3700518608093 and batch: 400, loss is 4.041467480659485 and perplexity is 56.9097955821769
At time: 1386.078241109848 and batch: 450, loss is 4.078821868896484 and perplexity is 59.07582976285134
At time: 1387.7864518165588 and batch: 500, loss is 4.054128036499024 and perplexity is 57.63488556271158
At time: 1389.4961364269257 and batch: 550, loss is 4.040188708305359 and perplexity is 56.837067420199695
At time: 1391.204297542572 and batch: 600, loss is 4.067329249382019 and perplexity is 58.40078020280529
At time: 1392.913066625595 and batch: 650, loss is 4.028991751670837 and perplexity is 56.20421486418539
At time: 1394.6222641468048 and batch: 700, loss is 4.001939854621887 and perplexity is 54.70416530069096
At time: 1396.3322553634644 and batch: 750, loss is 3.9821369791030885 and perplexity is 53.631521295962216
At time: 1398.0409293174744 and batch: 800, loss is 3.9440027570724485 and perplexity is 51.62482993675348
At time: 1399.7484693527222 and batch: 850, loss is 3.9652946043014525 and perplexity is 52.735803295848676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655117352803548 and perplexity of 105.12155558433885
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 1404.1817531585693 and batch: 50, loss is 4.1352064037323 and perplexity is 62.50249048649039
At time: 1405.911554813385 and batch: 100, loss is 4.08870334148407 and perplexity is 59.66247966416296
At time: 1407.6198160648346 and batch: 150, loss is 4.080212631225586 and perplexity is 59.158047360762694
At time: 1409.3586790561676 and batch: 200, loss is 4.111152563095093 and perplexity is 61.017003003477996
At time: 1411.0690083503723 and batch: 250, loss is 4.105745830535889 and perplexity is 60.68799062912693
At time: 1412.7765853404999 and batch: 300, loss is 4.090209612846374 and perplexity is 59.75241526539121
At time: 1414.4854114055634 and batch: 350, loss is 4.030717515945435 and perplexity is 56.3012938338948
At time: 1416.195235490799 and batch: 400, loss is 4.041450819969177 and perplexity is 56.908847433595675
At time: 1417.90554189682 and batch: 450, loss is 4.078798985481262 and perplexity is 59.074477921576715
At time: 1419.6132435798645 and batch: 500, loss is 4.05411584854126 and perplexity is 57.63418311544132
At time: 1421.3223507404327 and batch: 550, loss is 4.040171265602112 and perplexity is 56.83607603674548
At time: 1423.0319736003876 and batch: 600, loss is 4.067307243347168 and perplexity is 58.39949504734144
At time: 1424.7435171604156 and batch: 650, loss is 4.028966007232666 and perplexity is 56.202767936876086
At time: 1426.4519453048706 and batch: 700, loss is 4.001916279792786 and perplexity is 54.70287567454428
At time: 1428.160400390625 and batch: 750, loss is 3.982109479904175 and perplexity is 53.630046492368116
At time: 1429.8703498840332 and batch: 800, loss is 3.9439613580703736 and perplexity is 51.622692764550514
At time: 1431.5818855762482 and batch: 850, loss is 3.9652612447738647 and perplexity is 52.73404408370717
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655118306477864 and perplexity of 105.12165583611431
Annealing...
Finished 45 epochs...
Completing Train Step...
At time: 1436.0502412319183 and batch: 50, loss is 4.135207033157348 and perplexity is 62.50252982713585
At time: 1437.7652139663696 and batch: 100, loss is 4.088703188896179 and perplexity is 59.662470560391725
At time: 1439.4834079742432 and batch: 150, loss is 4.080212097167969 and perplexity is 59.15801576696534
At time: 1441.198008775711 and batch: 200, loss is 4.111152663230896 and perplexity is 61.01700911346488
At time: 1442.9152612686157 and batch: 250, loss is 4.1057489347457885 and perplexity is 60.68817901768064
At time: 1444.6318726539612 and batch: 300, loss is 4.090214381217956 and perplexity is 59.75270018778942
At time: 1446.349882364273 and batch: 350, loss is 4.0307187604904176 and perplexity is 56.30136390343116
At time: 1448.0658078193665 and batch: 400, loss is 4.041446499824524 and perplexity is 56.90860157967376
At time: 1449.7810816764832 and batch: 450, loss is 4.078792724609375 and perplexity is 59.07410806499649
At time: 1451.4976987838745 and batch: 500, loss is 4.0541129446029665 and perplexity is 57.634015749572974
At time: 1453.2435138225555 and batch: 550, loss is 4.0401664733886715 and perplexity is 56.83580366679063
At time: 1454.9590964317322 and batch: 600, loss is 4.067301068305969 and perplexity is 58.39913442916698
At time: 1456.6755802631378 and batch: 650, loss is 4.028958988189697 and perplexity is 56.20237344861747
At time: 1458.3930594921112 and batch: 700, loss is 4.001909809112549 and perplexity is 54.702521710872944
At time: 1460.1110653877258 and batch: 750, loss is 3.982101674079895 and perplexity is 53.629627867282935
At time: 1461.8272836208344 and batch: 800, loss is 3.9439492654800414 and perplexity is 51.62206851624947
At time: 1463.5432736873627 and batch: 850, loss is 3.9652516269683837 and perplexity is 52.733536900367945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655118306477864 and perplexity of 105.12165583611431
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 1468.0046381950378 and batch: 50, loss is 4.135207424163818 and perplexity is 62.502554266034174
At time: 1469.7135186195374 and batch: 100, loss is 4.0887033557891845 and perplexity is 59.66248051764156
At time: 1471.4233093261719 and batch: 150, loss is 4.080212073326111 and perplexity is 59.15801435652833
At time: 1473.1342086791992 and batch: 200, loss is 4.111152749061585 and perplexity is 61.01701435059705
At time: 1474.8449618816376 and batch: 250, loss is 4.105750126838684 and perplexity is 60.68825136367081
At time: 1476.5536572933197 and batch: 300, loss is 4.090215973854065 and perplexity is 59.752795352173095
At time: 1478.2645344734192 and batch: 350, loss is 4.030719327926636 and perplexity is 56.30139585087323
At time: 1479.975382566452 and batch: 400, loss is 4.041445789337158 and perplexity is 56.90856114684569
At time: 1481.6869781017303 and batch: 450, loss is 4.078791241645813 and perplexity is 59.074020460311736
At time: 1483.3947048187256 and batch: 500, loss is 4.054112424850464 and perplexity is 57.63398579415685
At time: 1485.1044442653656 and batch: 550, loss is 4.040165390968323 and perplexity is 56.83574214659349
At time: 1486.8150579929352 and batch: 600, loss is 4.067299728393555 and perplexity is 58.39905617949417
At time: 1488.5270736217499 and batch: 650, loss is 4.028957343101501 and perplexity is 56.20228099083236
At time: 1490.2354831695557 and batch: 700, loss is 4.0019084024429326 and perplexity is 54.702444762551835
At time: 1491.9448916912079 and batch: 750, loss is 3.982099676132202 and perplexity is 53.6295207181987
At time: 1493.6585850715637 and batch: 800, loss is 3.943946318626404 and perplexity is 51.62191639379322
At time: 1495.370637178421 and batch: 850, loss is 3.9652491426467895 and perplexity is 52.73340589346622
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.655118306477864 and perplexity of 105.12165583611431
Annealing...
Model not improving. Stopping early with 105.12128824673842loss at 46 epochs.
Finished Training.
Improved accuracyfrom -106.36539507113342 to -105.12128824673842
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9fbf6be518>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'num_layers': 1, 'anneal': 6.741463806971062, 'wordvec_source': '', 'dropout': 0.2727588808900182, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 11.43502763358037, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.1375417709350586 and batch: 50, loss is 6.94747917175293 and perplexity is 1040.5234384829555
At time: 3.7102341651916504 and batch: 100, loss is 6.039547357559204 and perplexity is 419.70301648693095
At time: 5.311974763870239 and batch: 150, loss is 5.8213136196136475 and perplexity is 337.41499759521577
At time: 6.888272523880005 and batch: 200, loss is 5.774659967422485 and perplexity is 322.034913737413
At time: 8.467541456222534 and batch: 250, loss is 5.791977882385254 and perplexity is 327.6604577390909
At time: 10.047382116317749 and batch: 300, loss is 5.709899978637695 and perplexity is 301.8408762372502
At time: 11.626107931137085 and batch: 350, loss is 5.666818494796753 and perplexity is 289.1132543172192
At time: 13.209545135498047 and batch: 400, loss is 5.673878021240235 and perplexity is 291.16147822021253
At time: 14.790472269058228 and batch: 450, loss is 5.662918663024902 and perplexity is 287.9879569238244
At time: 16.371421813964844 and batch: 500, loss is 5.655843439102173 and perplexity is 285.95756883661386
At time: 17.95508861541748 and batch: 550, loss is 5.613474731445312 and perplexity is 274.09499172798985
At time: 19.541565656661987 and batch: 600, loss is 5.645386409759522 and perplexity is 282.9828824466159
At time: 21.130292654037476 and batch: 650, loss is 5.6494248104095455 and perplexity is 284.1279913510385
At time: 22.715072870254517 and batch: 700, loss is 5.597610387802124 and perplexity is 269.78096467041684
At time: 24.301212072372437 and batch: 750, loss is 5.5927832889556885 and perplexity is 268.4818433032477
At time: 25.892075777053833 and batch: 800, loss is 5.591455659866333 and perplexity is 268.12563550635195
At time: 27.48050045967102 and batch: 850, loss is 5.5833875274658205 and perplexity is 265.97106574375806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.115777969360352 and perplexity of 166.63036388862756
Finished 1 epochs...
Completing Train Step...
At time: 31.72849416732788 and batch: 50, loss is 5.388135452270507 and perplexity is 218.79505117688979
At time: 33.31853985786438 and batch: 100, loss is 5.283257970809936 and perplexity is 197.0106859779859
At time: 34.908310413360596 and batch: 150, loss is 5.261158351898193 and perplexity is 192.70458178175514
At time: 36.49792194366455 and batch: 200, loss is 5.26841588973999 and perplexity is 194.10822993004297
At time: 38.0886013507843 and batch: 250, loss is 5.3034627151489255 and perplexity is 201.03172172795075
At time: 39.68013668060303 and batch: 300, loss is 5.254609413146973 and perplexity is 191.44669468710433
At time: 41.307366132736206 and batch: 350, loss is 5.2143709754943846 and perplexity is 183.89610943144248
At time: 43.008544921875 and batch: 400, loss is 5.230585069656372 and perplexity is 186.90212229796788
At time: 44.72299408912659 and batch: 450, loss is 5.23295672416687 and perplexity is 187.34591561352818
At time: 46.43896508216858 and batch: 500, loss is 5.232848987579346 and perplexity is 187.32573269113252
At time: 48.15539193153381 and batch: 550, loss is 5.203352203369141 and perplexity is 181.88092293645644
At time: 49.87151217460632 and batch: 600, loss is 5.233220834732055 and perplexity is 187.39540218386136
At time: 51.61490035057068 and batch: 650, loss is 5.232081899642944 and perplexity is 187.1820924807872
At time: 53.330822706222534 and batch: 700, loss is 5.178933124542237 and perplexity is 177.49334664845014
At time: 55.04724049568176 and batch: 750, loss is 5.19026026725769 and perplexity is 179.5152687989872
At time: 56.7648868560791 and batch: 800, loss is 5.1818956279754635 and perplexity is 178.01995094564836
At time: 58.47903227806091 and batch: 850, loss is 5.175283908843994 and perplexity is 176.84681552476562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.994711558024089 and perplexity of 147.63035644983108
Finished 2 epochs...
Completing Train Step...
At time: 62.926530838012695 and batch: 50, loss is 5.139177560806274 and perplexity is 170.5754227032478
At time: 64.59429597854614 and batch: 100, loss is 5.046741962432861 and perplexity is 155.51496461152735
At time: 66.27256155014038 and batch: 150, loss is 5.053220157623291 and perplexity is 156.52569121466098
At time: 67.95214581489563 and batch: 200, loss is 5.080601024627685 and perplexity is 160.8707141397863
At time: 69.63769841194153 and batch: 250, loss is 5.096521787643432 and perplexity is 163.45239529119743
At time: 71.328373670578 and batch: 300, loss is 5.064671821594239 and perplexity is 158.32847356063638
At time: 73.03000807762146 and batch: 350, loss is 5.020026082992554 and perplexity is 151.4152531054669
At time: 74.74477624893188 and batch: 400, loss is 5.058903017044067 and perplexity is 157.41773699825745
At time: 76.46026015281677 and batch: 450, loss is 5.06231162071228 and perplexity is 157.95522719916073
At time: 78.17500185966492 and batch: 500, loss is 5.06290675163269 and perplexity is 158.04925921681263
At time: 79.8882257938385 and batch: 550, loss is 5.036548757553101 and perplexity is 153.93782044777004
At time: 81.6030945777893 and batch: 600, loss is 5.075776653289795 and perplexity is 160.09648316789563
At time: 83.31916570663452 and batch: 650, loss is 5.064336585998535 and perplexity is 158.27540511619821
At time: 85.03467464447021 and batch: 700, loss is 5.0312630081176755 and perplexity is 153.1262903620478
At time: 86.74980306625366 and batch: 750, loss is 5.0414297389984135 and perplexity is 154.6910247859472
At time: 88.46516942977905 and batch: 800, loss is 5.018491344451904 and perplexity is 151.18304851313064
At time: 90.2176251411438 and batch: 850, loss is 5.01187180519104 and perplexity is 150.18559138347325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9667510986328125 and perplexity of 143.55971752493286
Finished 3 epochs...
Completing Train Step...
At time: 94.60880637168884 and batch: 50, loss is 5.004003057479858 and perplexity is 149.00845621857488
At time: 96.29775047302246 and batch: 100, loss is 4.924057064056396 and perplexity is 137.55957060607517
At time: 97.97003412246704 and batch: 150, loss is 4.9290953540802 and perplexity is 138.25438448547345
At time: 99.6524555683136 and batch: 200, loss is 4.947122583389282 and perplexity is 140.76932860604126
At time: 101.35925078392029 and batch: 250, loss is 4.962012920379639 and perplexity is 142.8811149329172
At time: 103.06721448898315 and batch: 300, loss is 4.944324951171875 and perplexity is 140.3760581666383
At time: 104.77572965621948 and batch: 350, loss is 4.884427833557129 and perplexity is 132.2147948194477
At time: 106.4823350906372 and batch: 400, loss is 4.92623161315918 and perplexity is 137.8590261192727
At time: 108.18840074539185 and batch: 450, loss is 4.939301624298095 and perplexity is 139.67267149090333
At time: 109.89515733718872 and batch: 500, loss is 4.950105867385864 and perplexity is 141.18991053702229
At time: 111.60334849357605 and batch: 550, loss is 4.934363050460815 and perplexity is 138.98458816335577
At time: 113.3099718093872 and batch: 600, loss is 4.965989875793457 and perplexity is 143.45047817216158
At time: 115.01742482185364 and batch: 650, loss is 4.95450876235962 and perplexity is 141.8129254125493
At time: 116.72460007667542 and batch: 700, loss is 4.9190567684173585 and perplexity is 136.873448920291
At time: 118.43288278579712 and batch: 750, loss is 4.942493867874146 and perplexity is 140.11925309861638
At time: 120.14036631584167 and batch: 800, loss is 4.888359460830689 and perplexity is 132.73563732110188
At time: 121.84715127944946 and batch: 850, loss is 4.896266345977783 and perplexity is 133.7893229563819
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.950460116068522 and perplexity of 141.2399357369921
Finished 4 epochs...
Completing Train Step...
At time: 126.26868271827698 and batch: 50, loss is 4.8856387424469 and perplexity is 132.3749918622907
At time: 127.93055987358093 and batch: 100, loss is 4.817507886886597 and perplexity is 123.65654038242403
At time: 129.61059021949768 and batch: 150, loss is 4.829122886657715 and perplexity is 125.10118460063329
At time: 131.29088521003723 and batch: 200, loss is 4.849867563247681 and perplexity is 127.7234734438591
At time: 133.0114188194275 and batch: 250, loss is 4.865968713760376 and perplexity is 129.79661349603012
At time: 134.71842765808105 and batch: 300, loss is 4.848870058059692 and perplexity is 127.59613213883596
At time: 136.42623805999756 and batch: 350, loss is 4.792226371765136 and perplexity is 120.56950260587847
At time: 138.1343195438385 and batch: 400, loss is 4.816773462295532 and perplexity is 123.56575731905879
At time: 139.8429296016693 and batch: 450, loss is 4.835531377792359 and perplexity is 125.90546880459767
At time: 141.54938554763794 and batch: 500, loss is 4.844429578781128 and perplexity is 127.0308002592244
At time: 143.25561261177063 and batch: 550, loss is 4.839724731445313 and perplexity is 126.43454348547918
At time: 144.96337485313416 and batch: 600, loss is 4.874674425125122 and perplexity is 130.9315182580861
At time: 146.67374515533447 and batch: 650, loss is 4.857458505630493 and perplexity is 128.6967041675583
At time: 148.38117456436157 and batch: 700, loss is 4.8409644222259525 and perplexity is 126.59138041795615
At time: 150.08716368675232 and batch: 750, loss is 4.8499456405639645 and perplexity is 127.73344613920757
At time: 151.79611468315125 and batch: 800, loss is 4.8047884082794186 and perplexity is 122.09365428147687
At time: 153.50579571723938 and batch: 850, loss is 4.816522340774537 and perplexity is 123.53473119396479
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.940545717875163 and perplexity of 139.84654549973945
Finished 5 epochs...
Completing Train Step...
At time: 157.98511242866516 and batch: 50, loss is 4.810847234725952 and perplexity is 122.83564406682814
At time: 159.66560101509094 and batch: 100, loss is 4.746256742477417 and perplexity is 115.1524315573373
At time: 161.35235977172852 and batch: 150, loss is 4.758018264770508 and perplexity is 116.51479548002582
At time: 163.04835319519043 and batch: 200, loss is 4.792396488189698 and perplexity is 120.59001520328832
At time: 164.7551031112671 and batch: 250, loss is 4.796938219070435 and perplexity is 121.13894820865221
At time: 166.47152733802795 and batch: 300, loss is 4.776353273391724 and perplexity is 118.67080004197743
At time: 168.1899390220642 and batch: 350, loss is 4.723966636657715 and perplexity is 112.6140669778211
At time: 169.90499758720398 and batch: 400, loss is 4.752329874038696 and perplexity is 115.85389530860795
At time: 171.6205017566681 and batch: 450, loss is 4.774452295303345 and perplexity is 118.44542373694648
At time: 173.33610272407532 and batch: 500, loss is 4.77419695854187 and perplexity is 118.41518412684432
At time: 175.10567259788513 and batch: 550, loss is 4.766693258285523 and perplexity is 117.52995747069123
At time: 176.83938908576965 and batch: 600, loss is 4.811810474395752 and perplexity is 122.95402123569845
At time: 178.572900056839 and batch: 650, loss is 4.7964222240448 and perplexity is 121.07645723786752
At time: 180.30858278274536 and batch: 700, loss is 4.775339479446411 and perplexity is 118.55055326643489
At time: 182.0475606918335 and batch: 750, loss is 4.789537496566773 and perplexity is 120.24574173200172
At time: 183.78254199028015 and batch: 800, loss is 4.7447497749328615 and perplexity is 114.97903126742358
At time: 185.51730012893677 and batch: 850, loss is 4.763988342285156 and perplexity is 117.21247837891539
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.953602472941081 and perplexity of 141.68446008101427
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 189.98530960083008 and batch: 50, loss is 4.748769292831421 and perplexity is 115.44212161804444
At time: 191.64668464660645 and batch: 100, loss is 4.6709401893615725 and perplexity is 106.79810568762898
At time: 193.32875967025757 and batch: 150, loss is 4.653493118286133 and perplexity is 104.95095211276437
At time: 195.03764128684998 and batch: 200, loss is 4.662443342208863 and perplexity is 105.8945028314804
At time: 196.74896907806396 and batch: 250, loss is 4.640567512512207 and perplexity is 103.60312697354497
At time: 198.45770478248596 and batch: 300, loss is 4.60687071800232 and perplexity is 100.17019787387449
At time: 200.16812562942505 and batch: 350, loss is 4.530687398910523 and perplexity is 92.82234513597422
At time: 201.87900066375732 and batch: 400, loss is 4.549798746109008 and perplexity is 94.61336509086253
At time: 203.59027862548828 and batch: 450, loss is 4.559813585281372 and perplexity is 95.56566332324941
At time: 205.29806566238403 and batch: 500, loss is 4.540942153930664 and perplexity is 93.77911287064381
At time: 207.00820922851562 and batch: 550, loss is 4.506675844192505 and perplexity is 90.62008200692442
At time: 208.71895956993103 and batch: 600, loss is 4.5153395938873295 and perplexity is 91.4086025548586
At time: 210.4303789138794 and batch: 650, loss is 4.4679460620880125 and perplexity is 87.1774818662836
At time: 212.1395845413208 and batch: 700, loss is 4.421889925003052 and perplexity is 83.25347960157949
At time: 213.84834814071655 and batch: 750, loss is 4.401065263748169 and perplexity is 81.53768155371591
At time: 215.5597105026245 and batch: 800, loss is 4.321335220336914 and perplexity is 75.28908873110878
At time: 217.2725715637207 and batch: 850, loss is 4.312352952957153 and perplexity is 74.61585013634627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.770232518513997 and perplexity of 117.94666355547372
Finished 7 epochs...
Completing Train Step...
At time: 221.74417185783386 and batch: 50, loss is 4.568458204269409 and perplexity is 96.3953731667792
At time: 223.4545180797577 and batch: 100, loss is 4.513551177978516 and perplexity is 91.24527205081644
At time: 225.15709781646729 and batch: 150, loss is 4.513425054550171 and perplexity is 91.23376460997935
At time: 226.8649046421051 and batch: 200, loss is 4.5344069385528565 and perplexity is 93.16824442266416
At time: 228.57311296463013 and batch: 250, loss is 4.517768621444702 and perplexity is 91.6309064512879
At time: 230.28287029266357 and batch: 300, loss is 4.4997147178649906 and perplexity is 89.99145468383114
At time: 231.99310946464539 and batch: 350, loss is 4.424804010391235 and perplexity is 83.49644118346606
At time: 233.70020413398743 and batch: 400, loss is 4.451902704238892 and perplexity is 85.79002184677097
At time: 235.40841031074524 and batch: 450, loss is 4.470056943893432 and perplexity is 87.36169758700133
At time: 237.11748957633972 and batch: 500, loss is 4.453997869491577 and perplexity is 85.96995454812212
At time: 238.82735633850098 and batch: 550, loss is 4.434760437011719 and perplexity is 84.33191966149344
At time: 240.53405666351318 and batch: 600, loss is 4.452478866577149 and perplexity is 85.83946506865072
At time: 242.24226593971252 and batch: 650, loss is 4.4115260982513425 and perplexity is 82.39511063914502
At time: 243.95182919502258 and batch: 700, loss is 4.3742312240600585 and perplexity is 79.37879158405201
At time: 245.66148161888123 and batch: 750, loss is 4.367293014526367 and perplexity is 78.82995108241735
At time: 247.3695306777954 and batch: 800, loss is 4.30344331741333 and perplexity is 73.95400289290357
At time: 249.078519821167 and batch: 850, loss is 4.312676181793213 and perplexity is 74.6399720289731
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.761401494344075 and perplexity of 116.90965936279265
Finished 8 epochs...
Completing Train Step...
At time: 253.49754071235657 and batch: 50, loss is 4.502268447875976 and perplexity is 90.22156225363831
At time: 255.22710919380188 and batch: 100, loss is 4.4464638233184814 and perplexity is 85.32468673203134
At time: 256.9441010951996 and batch: 150, loss is 4.453153886795044 and perplexity is 85.89742800394106
At time: 258.66254115104675 and batch: 200, loss is 4.472445430755616 and perplexity is 87.57060924600455
At time: 260.3799629211426 and batch: 250, loss is 4.459912586212158 and perplexity is 86.47994922598153
At time: 262.1244719028473 and batch: 300, loss is 4.44885835647583 and perplexity is 85.52924433579311
At time: 263.840993642807 and batch: 350, loss is 4.371497621536255 and perplexity is 79.16209783127461
At time: 265.55903005599976 and batch: 400, loss is 4.398454942703247 and perplexity is 81.32511957580004
At time: 267.27659344673157 and batch: 450, loss is 4.422950553894043 and perplexity is 83.34182749120039
At time: 268.9912929534912 and batch: 500, loss is 4.40783465385437 and perplexity is 82.0915143687666
At time: 270.70780205726624 and batch: 550, loss is 4.39405294418335 and perplexity is 80.96791330734563
At time: 272.4248957633972 and batch: 600, loss is 4.415513486862182 and perplexity is 82.72430784732965
At time: 274.1421835422516 and batch: 650, loss is 4.375867338180542 and perplexity is 79.50877064712506
At time: 275.8572406768799 and batch: 700, loss is 4.346297569274903 and perplexity is 77.1921346659475
At time: 277.5734131336212 and batch: 750, loss is 4.346172113418579 and perplexity is 77.18245106803626
At time: 279.2920722961426 and batch: 800, loss is 4.2853406715393065 and perplexity is 72.62728455460187
At time: 281.0098659992218 and batch: 850, loss is 4.301325874328613 and perplexity is 73.7975751727342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.756224314371745 and perplexity of 116.3059610914869
Finished 9 epochs...
Completing Train Step...
At time: 285.4569470882416 and batch: 50, loss is 4.455155153274536 and perplexity is 86.0695037745845
At time: 287.1804268360138 and batch: 100, loss is 4.403287239074707 and perplexity is 81.71905770235114
At time: 288.8956308364868 and batch: 150, loss is 4.412103080749512 and perplexity is 82.44266489358554
At time: 290.61131858825684 and batch: 200, loss is 4.431813688278198 and perplexity is 84.08378046552241
At time: 292.32738971710205 and batch: 250, loss is 4.419067497253418 and perplexity is 83.01883396144892
At time: 294.04521441459656 and batch: 300, loss is 4.411719207763672 and perplexity is 82.41102345518749
At time: 295.76067900657654 and batch: 350, loss is 4.333363027572632 and perplexity is 76.20011924395166
At time: 297.4761459827423 and batch: 400, loss is 4.361721820831299 and perplexity is 78.39199525711025
At time: 299.19256043434143 and batch: 450, loss is 4.389054956436158 and perplexity is 80.56424627046782
At time: 300.90969824790955 and batch: 500, loss is 4.376221742630005 and perplexity is 79.5369539030544
At time: 302.6251382827759 and batch: 550, loss is 4.365210237503052 and perplexity is 78.66593673350322
At time: 304.34004163742065 and batch: 600, loss is 4.389354686737061 and perplexity is 80.5883974354817
At time: 306.0852601528168 and batch: 650, loss is 4.350892419815064 and perplexity is 77.5476371024283
At time: 307.80382919311523 and batch: 700, loss is 4.323312997817993 and perplexity is 75.43814114303784
At time: 309.5200238227844 and batch: 750, loss is 4.3272825050354005 and perplexity is 75.7381885148902
At time: 311.23531913757324 and batch: 800, loss is 4.268589916229248 and perplexity is 71.42085519269567
At time: 312.9522497653961 and batch: 850, loss is 4.2886758518219 and perplexity is 72.86991402363131
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.754263242085774 and perplexity of 116.0781001933691
Finished 10 epochs...
Completing Train Step...
At time: 317.38794016838074 and batch: 50, loss is 4.416378755569458 and perplexity is 82.79591757860297
At time: 319.0792589187622 and batch: 100, loss is 4.367791547775268 and perplexity is 78.86926023168601
At time: 320.7868769168854 and batch: 150, loss is 4.3750966453552245 and perplexity is 79.44751741478274
At time: 322.4971845149994 and batch: 200, loss is 4.399684009552002 and perplexity is 81.42513503449365
At time: 324.20413303375244 and batch: 250, loss is 4.385920124053955 and perplexity is 80.31208630852015
At time: 325.9112193584442 and batch: 300, loss is 4.382450542449951 and perplexity is 80.03391981101345
At time: 327.6207060813904 and batch: 350, loss is 4.304305963516235 and perplexity is 74.01782654994997
At time: 329.3296744823456 and batch: 400, loss is 4.335556840896606 and perplexity is 76.36747158361409
At time: 331.0375008583069 and batch: 450, loss is 4.362053871154785 and perplexity is 78.41802966662193
At time: 332.7440598011017 and batch: 500, loss is 4.350843458175659 and perplexity is 77.54384033593256
At time: 334.45273637771606 and batch: 550, loss is 4.342204627990722 and perplexity is 76.87683747775759
At time: 336.16201758384705 and batch: 600, loss is 4.366649141311646 and perplexity is 78.77921092526462
At time: 337.87026810646057 and batch: 650, loss is 4.328506097793579 and perplexity is 75.83091793382798
At time: 339.57808804512024 and batch: 700, loss is 4.303812999725341 and perplexity is 73.98134743376063
At time: 341.28677129745483 and batch: 750, loss is 4.310564336776733 and perplexity is 74.48251030191712
At time: 342.99628734588623 and batch: 800, loss is 4.2536341571807865 and perplexity is 70.360649940234
At time: 344.7052266597748 and batch: 850, loss is 4.275769281387329 and perplexity is 71.9354566375159
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.754813194274902 and perplexity of 116.14195515565899
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 349.1763255596161 and batch: 50, loss is 4.397430057525635 and perplexity is 81.24181336312255
At time: 350.87146282196045 and batch: 100, loss is 4.358134908676147 and perplexity is 78.11131374735133
At time: 352.578088760376 and batch: 150, loss is 4.361946249008179 and perplexity is 78.40959060405976
At time: 354.2860691547394 and batch: 200, loss is 4.383708724975586 and perplexity is 80.13468046473238
At time: 355.9950215816498 and batch: 250, loss is 4.360702466964722 and perplexity is 78.3121267876686
At time: 357.7053654193878 and batch: 300, loss is 4.350023183822632 and perplexity is 77.48025919302522
At time: 359.41241908073425 and batch: 350, loss is 4.267925949096679 and perplexity is 71.37344983181008
At time: 361.1202292442322 and batch: 400, loss is 4.290836067199707 and perplexity is 73.02749887981825
At time: 362.82859992980957 and batch: 450, loss is 4.320665273666382 and perplexity is 75.2386659489594
At time: 364.53747940063477 and batch: 500, loss is 4.302206258773804 and perplexity is 73.86257401779689
At time: 366.2448670864105 and batch: 550, loss is 4.284057569503784 and perplexity is 72.53415609738876
At time: 367.9515595436096 and batch: 600, loss is 4.292716560363769 and perplexity is 73.16495579513824
At time: 369.6604585647583 and batch: 650, loss is 4.248483295440674 and perplexity is 69.99916374238728
At time: 371.3700649738312 and batch: 700, loss is 4.214697613716125 and perplexity is 67.67369917825367
At time: 373.0773115158081 and batch: 750, loss is 4.20988676071167 and perplexity is 67.34891283532885
At time: 374.78564167022705 and batch: 800, loss is 4.1419721031188965 and perplexity is 62.9267972956534
At time: 376.494740486145 and batch: 850, loss is 4.166356887817383 and perplexity is 64.48011539473701
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.730030695597331 and perplexity of 113.29904007699632
Finished 12 epochs...
Completing Train Step...
At time: 380.9615626335144 and batch: 50, loss is 4.370795211791992 and perplexity is 79.10651312628619
At time: 382.6731638908386 and batch: 100, loss is 4.330940742492675 and perplexity is 76.01576420239084
At time: 384.38977885246277 and batch: 150, loss is 4.333907804489136 and perplexity is 76.2416426194119
At time: 386.10930609703064 and batch: 200, loss is 4.359189386367798 and perplexity is 78.19372382732274
At time: 387.8247835636139 and batch: 250, loss is 4.338167161941528 and perplexity is 76.56707560378545
At time: 389.5414628982544 and batch: 300, loss is 4.329714984893799 and perplexity is 75.92264438460668
At time: 391.2591881752014 and batch: 350, loss is 4.248486614227295 and perplexity is 69.99939605506086
At time: 393.00573658943176 and batch: 400, loss is 4.273499956130982 and perplexity is 71.77239677684919
At time: 394.72021412849426 and batch: 450, loss is 4.305234498977661 and perplexity is 74.08658664483222
At time: 396.4354512691498 and batch: 500, loss is 4.2882034206390385 and perplexity is 72.83549613463627
At time: 398.15204548835754 and batch: 550, loss is 4.272580890655518 and perplexity is 71.70646354799851
At time: 399.87047266960144 and batch: 600, loss is 4.284542007446289 and perplexity is 72.56930290726667
At time: 401.5858151912689 and batch: 650, loss is 4.242065916061401 and perplexity is 69.55139085338195
At time: 403.30484414100647 and batch: 700, loss is 4.21309769153595 and perplexity is 67.56551309365948
At time: 405.02152705192566 and batch: 750, loss is 4.210792446136475 and perplexity is 67.40993739441578
At time: 406.74113845825195 and batch: 800, loss is 4.147364058494568 and perplexity is 63.267012166044765
At time: 408.4572820663452 and batch: 850, loss is 4.1747077941894535 and perplexity is 65.02083741836907
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.728179931640625 and perplexity of 113.08954422078162
Finished 13 epochs...
Completing Train Step...
At time: 412.9169993400574 and batch: 50, loss is 4.3594749164581295 and perplexity is 78.21605367612068
At time: 414.6438641548157 and batch: 100, loss is 4.319322347640991 and perplexity is 75.13769380057865
At time: 416.3513033390045 and batch: 150, loss is 4.321106157302856 and perplexity is 75.27184475906624
At time: 418.06216835975647 and batch: 200, loss is 4.347482404708862 and perplexity is 77.28364884623856
At time: 419.77348494529724 and batch: 250, loss is 4.326651849746704 and perplexity is 75.6904388841082
At time: 421.48469257354736 and batch: 300, loss is 4.319078340530395 and perplexity is 75.11936190566483
At time: 423.1928880214691 and batch: 350, loss is 4.238596124649048 and perplexity is 69.3104802311883
At time: 424.9028830528259 and batch: 400, loss is 4.264314317703247 and perplexity is 71.1161401732533
At time: 426.61290550231934 and batch: 450, loss is 4.296778383255005 and perplexity is 73.46274325797177
At time: 428.323543548584 and batch: 500, loss is 4.280739612579346 and perplexity is 72.29388970909163
At time: 430.03146171569824 and batch: 550, loss is 4.26643853187561 and perplexity is 71.26736664793869
At time: 431.74158930778503 and batch: 600, loss is 4.280139560699463 and perplexity is 72.25052263721537
At time: 433.4522054195404 and batch: 650, loss is 4.238845281600952 and perplexity is 69.32775157072551
At time: 435.1925506591797 and batch: 700, loss is 4.212831444740296 and perplexity is 67.54752638685935
At time: 436.90051198005676 and batch: 750, loss is 4.211314649581909 and perplexity is 67.44514828880466
At time: 438.6102395057678 and batch: 800, loss is 4.149800872802734 and perplexity is 63.42137012098021
At time: 440.321617603302 and batch: 850, loss is 4.178853559494018 and perplexity is 65.29095809178129
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.727789243062337 and perplexity of 113.04537005726321
Finished 14 epochs...
Completing Train Step...
At time: 444.74846506118774 and batch: 50, loss is 4.350749559402466 and perplexity is 77.53655940629685
At time: 446.479110956192 and batch: 100, loss is 4.310404071807861 and perplexity is 74.47057432120741
At time: 448.18839478492737 and batch: 150, loss is 4.312138237953186 and perplexity is 74.59983071365501
At time: 449.8973867893219 and batch: 200, loss is 4.33902678489685 and perplexity is 76.63292271738673
At time: 451.60443329811096 and batch: 250, loss is 4.318208694458008 and perplexity is 75.05406304518748
At time: 453.31334257125854 and batch: 300, loss is 4.311212091445923 and perplexity is 74.53077232502608
At time: 455.02278995513916 and batch: 350, loss is 4.231493530273437 and perplexity is 68.8199401200264
At time: 456.7320261001587 and batch: 400, loss is 4.257930660247803 and perplexity is 70.66360504622628
At time: 458.4386820793152 and batch: 450, loss is 4.290968017578125 and perplexity is 73.03713552169567
At time: 460.1480042934418 and batch: 500, loss is 4.2753967189788815 and perplexity is 71.90866118233785
At time: 461.8568580150604 and batch: 550, loss is 4.261933374404907 and perplexity is 70.94701809084286
At time: 463.56588101387024 and batch: 600, loss is 4.276665630340577 and perplexity is 71.99996481537413
At time: 465.27275109291077 and batch: 650, loss is 4.236143436431885 and perplexity is 69.14069153747975
At time: 466.9811661243439 and batch: 700, loss is 4.211279745101929 and perplexity is 67.44279419206094
At time: 468.6908280849457 and batch: 750, loss is 4.211428146362305 and perplexity is 67.4528035304032
At time: 470.401070356369 and batch: 800, loss is 4.150849919319153 and perplexity is 63.48793699814273
At time: 472.1085915565491 and batch: 850, loss is 4.180597972869873 and perplexity is 65.40495190966259
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.727734883626302 and perplexity of 113.0392251417191
Finished 15 epochs...
Completing Train Step...
At time: 476.56933856010437 and batch: 50, loss is 4.343370676040649 and perplexity is 76.96653184799295
At time: 478.313764333725 and batch: 100, loss is 4.303049335479736 and perplexity is 73.92487209072851
At time: 480.0293526649475 and batch: 150, loss is 4.30511589050293 and perplexity is 74.07779986889557
At time: 481.7461025714874 and batch: 200, loss is 4.332159194946289 and perplexity is 76.10844224722611
At time: 483.46323680877686 and batch: 250, loss is 4.311362218856812 and perplexity is 74.541962276845
At time: 485.17921590805054 and batch: 300, loss is 4.3047879028320315 and perplexity is 74.05350724790915
At time: 486.8941514492035 and batch: 350, loss is 4.2255271053314205 and perplexity is 68.41055361416801
At time: 488.6095018386841 and batch: 400, loss is 4.252579040527344 and perplexity is 70.28645039820493
At time: 490.3272490501404 and batch: 450, loss is 4.286367635726929 and perplexity is 72.70190848637792
At time: 492.0430941581726 and batch: 500, loss is 4.27094407081604 and perplexity is 71.5891889906887
At time: 493.7581148147583 and batch: 550, loss is 4.257921867370605 and perplexity is 70.66298371255641
At time: 495.4746415615082 and batch: 600, loss is 4.273523569107056 and perplexity is 71.77409155674638
At time: 497.1921737194061 and batch: 650, loss is 4.2336142635345455 and perplexity is 68.9660437246949
At time: 498.90923833847046 and batch: 700, loss is 4.209271850585938 and perplexity is 67.30751203705893
At time: 500.6236529350281 and batch: 750, loss is 4.210588026046753 and perplexity is 67.39615885731831
At time: 502.3403708934784 and batch: 800, loss is 4.151067056655884 and perplexity is 63.50172409648976
At time: 504.0579125881195 and batch: 850, loss is 4.1808979701995845 and perplexity is 65.42457616405017
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.727744420369466 and perplexity of 113.04030317291719
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 508.5339078903198 and batch: 50, loss is 4.339527139663696 and perplexity is 76.6712759598794
At time: 510.2507653236389 and batch: 100, loss is 4.301953649520874 and perplexity is 73.84391800459471
At time: 511.9679844379425 and batch: 150, loss is 4.302233395576477 and perplexity is 73.86457843908956
At time: 513.6834805011749 and batch: 200, loss is 4.329674921035767 and perplexity is 75.91960269149205
At time: 515.3980088233948 and batch: 250, loss is 4.3068985176086425 and perplexity is 74.20997073355578
At time: 517.1140096187592 and batch: 300, loss is 4.299117698669433 and perplexity is 73.6347969511274
At time: 518.8325803279877 and batch: 350, loss is 4.219550704956054 and perplexity is 68.0029240479027
At time: 520.547527551651 and batch: 400, loss is 4.2436726284027095 and perplexity is 69.66322975383247
At time: 522.2935621738434 and batch: 450, loss is 4.277884550094605 and perplexity is 72.08778050404295
At time: 524.0093097686768 and batch: 500, loss is 4.261094799041748 and perplexity is 70.88754860768897
At time: 525.726407289505 and batch: 550, loss is 4.246197853088379 and perplexity is 69.8393673612085
At time: 527.4433989524841 and batch: 600, loss is 4.258352003097534 and perplexity is 70.6933849242775
At time: 529.1588799953461 and batch: 650, loss is 4.216901645660401 and perplexity is 67.82301866504275
At time: 530.874924659729 and batch: 700, loss is 4.190975122451782 and perplexity is 66.08720266575085
At time: 532.5916404724121 and batch: 750, loss is 4.190213599205017 and perplexity is 66.0368948823117
At time: 534.3078577518463 and batch: 800, loss is 4.128302421569824 and perplexity is 62.07246057459406
At time: 536.0229842662811 and batch: 850, loss is 4.158458228111267 and perplexity is 63.972815039347616
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.725704828898112 and perplexity of 112.809982094943
Finished 17 epochs...
Completing Train Step...
At time: 540.4997487068176 and batch: 50, loss is 4.335947818756104 and perplexity is 76.3973354118565
At time: 542.1964039802551 and batch: 100, loss is 4.298043842315674 and perplexity is 73.55576619799649
At time: 543.9035015106201 and batch: 150, loss is 4.298348035812378 and perplexity is 73.57814478725709
At time: 545.6123094558716 and batch: 200, loss is 4.32623818397522 and perplexity is 75.65913481547048
At time: 547.3222208023071 and batch: 250, loss is 4.303762626647949 and perplexity is 73.97762085948109
At time: 549.0294115543365 and batch: 300, loss is 4.296269512176513 and perplexity is 73.42536970254832
At time: 550.7371492385864 and batch: 350, loss is 4.216958017349243 and perplexity is 67.82684207091219
At time: 552.4456126689911 and batch: 400, loss is 4.241495723724365 and perplexity is 69.51174448738836
At time: 554.1550908088684 and batch: 450, loss is 4.276064338684082 and perplexity is 71.95668485050766
At time: 555.8606467247009 and batch: 500, loss is 4.259534139633178 and perplexity is 70.7770035720164
At time: 557.5668346881866 and batch: 550, loss is 4.245201492309571 and perplexity is 69.76981680922785
At time: 559.2757954597473 and batch: 600, loss is 4.257670955657959 and perplexity is 70.64525576646007
At time: 560.9847762584686 and batch: 650, loss is 4.216566562652588 and perplexity is 67.80029613112964
At time: 562.6914122104645 and batch: 700, loss is 4.191061186790466 and perplexity is 66.09289066190706
At time: 564.3977689743042 and batch: 750, loss is 4.190898003578186 and perplexity is 66.08210629163803
At time: 566.1348841190338 and batch: 800, loss is 4.129562239646912 and perplexity is 62.15070986209933
At time: 567.8435668945312 and batch: 850, loss is 4.160033030509949 and perplexity is 64.07363894995316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.7254384358723955 and perplexity of 112.77993430492022
Finished 18 epochs...
Completing Train Step...
At time: 572.2902765274048 and batch: 50, loss is 4.333949060440063 and perplexity is 76.244788105763
At time: 573.9984028339386 and batch: 100, loss is 4.295922803878784 and perplexity is 73.39991693020727
At time: 575.7072646617889 and batch: 150, loss is 4.296064624786377 and perplexity is 73.41032731123093
At time: 577.4129691123962 and batch: 200, loss is 4.3242448139190675 and perplexity is 75.50846837852657
At time: 579.1205492019653 and batch: 250, loss is 4.301869096755982 and perplexity is 73.8376745611109
At time: 580.8295540809631 and batch: 300, loss is 4.294421176910401 and perplexity is 73.28978034819409
At time: 582.5387749671936 and batch: 350, loss is 4.215352897644043 and perplexity is 67.71805919828466
At time: 584.2439212799072 and batch: 400, loss is 4.240173854827881 and perplexity is 69.41991977789453
At time: 585.9504954814911 and batch: 450, loss is 4.274874248504639 and perplexity is 71.87110084296634
At time: 587.6582074165344 and batch: 500, loss is 4.258518152236938 and perplexity is 70.70513154516269
At time: 589.3675644397736 and batch: 550, loss is 4.244628620147705 and perplexity is 69.72985906987044
At time: 591.07368683815 and batch: 600, loss is 4.257320156097412 and perplexity is 70.62047778808855
At time: 592.781140089035 and batch: 650, loss is 4.216429920196533 and perplexity is 67.79103236507132
At time: 594.489551782608 and batch: 700, loss is 4.191214237213135 and perplexity is 66.10300698089198
At time: 596.1987299919128 and batch: 750, loss is 4.191484279632569 and perplexity is 66.12086000725766
At time: 597.9066474437714 and batch: 800, loss is 4.13043297290802 and perplexity is 62.204850119818644
At time: 599.6155664920807 and batch: 850, loss is 4.161025905609131 and perplexity is 64.13728766296296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.725354512532552 and perplexity of 112.77046983331658
Finished 19 epochs...
Completing Train Step...
At time: 604.0565001964569 and batch: 50, loss is 4.332292070388794 and perplexity is 76.11855586207872
At time: 605.8001832962036 and batch: 100, loss is 4.29420244216919 and perplexity is 73.27375108019899
At time: 607.5261836051941 and batch: 150, loss is 4.294238858222961 and perplexity is 73.2764194696442
At time: 609.2794494628906 and batch: 200, loss is 4.3226471042633055 and perplexity is 75.38792409253378
At time: 610.9999494552612 and batch: 250, loss is 4.300327501296997 and perplexity is 73.72393443046539
At time: 612.717009305954 and batch: 300, loss is 4.292917366027832 and perplexity is 73.1796492078837
At time: 614.4455854892731 and batch: 350, loss is 4.214073057174683 and perplexity is 67.6314463227704
At time: 616.1635007858276 and batch: 400, loss is 4.2391150760650635 and perplexity is 69.34645833768622
At time: 617.8810851573944 and batch: 450, loss is 4.273908281326294 and perplexity is 71.80170923888228
At time: 619.5955457687378 and batch: 500, loss is 4.257715015411377 and perplexity is 70.64836844758078
At time: 621.3114502429962 and batch: 550, loss is 4.244175853729248 and perplexity is 69.69829487746226
At time: 623.0291955471039 and batch: 600, loss is 4.2570140933990475 and perplexity is 70.59886680142405
At time: 624.7472608089447 and batch: 650, loss is 4.216307373046875 and perplexity is 67.78272527629801
At time: 626.4627273082733 and batch: 700, loss is 4.191309938430786 and perplexity is 66.10933342186966
At time: 628.1781115531921 and batch: 750, loss is 4.191924238204956 and perplexity is 66.14995684666413
At time: 629.8958835601807 and batch: 800, loss is 4.131056671142578 and perplexity is 62.243659276362486
At time: 631.6146626472473 and batch: 850, loss is 4.161720805168152 and perplexity is 64.18187212494409
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.725331942240397 and perplexity of 112.76792459958949
Finished 20 epochs...
Completing Train Step...
At time: 636.0248155593872 and batch: 50, loss is 4.330811777114868 and perplexity is 76.0059614327637
At time: 637.7571732997894 and batch: 100, loss is 4.292715272903442 and perplexity is 73.16486159822098
At time: 639.4678640365601 and batch: 150, loss is 4.292644915580749 and perplexity is 73.15971409552809
At time: 641.1770865917206 and batch: 200, loss is 4.321264419555664 and perplexity is 75.28375839350525
At time: 642.8870508670807 and batch: 250, loss is 4.298979206085205 and perplexity is 73.62459978394084
At time: 644.5988509654999 and batch: 300, loss is 4.291592683792114 and perplexity is 73.08277360542307
At time: 646.3091020584106 and batch: 350, loss is 4.212970209121704 and perplexity is 67.5569002279367
At time: 648.0185394287109 and batch: 400, loss is 4.238186502456665 and perplexity is 69.28209493434157
At time: 649.7289550304413 and batch: 450, loss is 4.273061904907227 and perplexity is 71.74096367576611
At time: 651.4668700695038 and batch: 500, loss is 4.257003803253173 and perplexity is 70.59814033252387
At time: 653.1763181686401 and batch: 550, loss is 4.243757429122925 and perplexity is 69.66913749637501
At time: 654.8850700855255 and batch: 600, loss is 4.2567081642150875 and perplexity is 70.57727185114618
At time: 656.5947196483612 and batch: 650, loss is 4.216171998977661 and perplexity is 67.77354987402472
At time: 658.3058128356934 and batch: 700, loss is 4.191346497535705 and perplexity is 66.11175036410674
At time: 660.0168759822845 and batch: 750, loss is 4.192249402999878 and perplexity is 66.1714699812837
At time: 661.727046251297 and batch: 800, loss is 4.131523580551147 and perplexity is 62.27272821225413
At time: 663.437691450119 and batch: 850, loss is 4.162237749099732 and perplexity is 64.21505913142829
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.725327491760254 and perplexity of 112.76742272929702
Finished 21 epochs...
Completing Train Step...
At time: 667.8691334724426 and batch: 50, loss is 4.329449777603149 and perplexity is 75.90251181556057
At time: 669.5819132328033 and batch: 100, loss is 4.291375722885132 and perplexity is 73.06691922052991
At time: 671.2797274589539 and batch: 150, loss is 4.291205973625183 and perplexity is 73.05451721771065
At time: 672.9981937408447 and batch: 200, loss is 4.32002142906189 and perplexity is 75.19023953105577
At time: 674.7252290248871 and batch: 250, loss is 4.297741422653198 and perplexity is 73.53352485128953
At time: 676.4524285793304 and batch: 300, loss is 4.290378036499024 and perplexity is 72.99405770246479
At time: 678.1813411712646 and batch: 350, loss is 4.2119716358184816 and perplexity is 67.48947338184769
At time: 679.9101610183716 and batch: 400, loss is 4.237338752746582 and perplexity is 69.22338594722648
At time: 681.6380341053009 and batch: 450, loss is 4.27227900505066 and perplexity is 71.68481966603066
At time: 683.3636426925659 and batch: 500, loss is 4.256346254348755 and perplexity is 70.55173386162836
At time: 685.0910503864288 and batch: 550, loss is 4.243349781036377 and perplexity is 69.64074279369949
At time: 686.8228316307068 and batch: 600, loss is 4.256389560699463 and perplexity is 70.55478926591677
At time: 688.5509965419769 and batch: 650, loss is 4.216013174057007 and perplexity is 67.76278660010428
At time: 690.277529001236 and batch: 700, loss is 4.19133599281311 and perplexity is 66.11105588215658
At time: 692.0058059692383 and batch: 750, loss is 4.192490153312683 and perplexity is 66.18740270120715
At time: 693.7353341579437 and batch: 800, loss is 4.131887879371643 and perplexity is 62.295418226412664
At time: 695.4975695610046 and batch: 850, loss is 4.162636790275574 and perplexity is 64.24068869741686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.725330988566081 and perplexity of 112.76781705576737
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 699.9781255722046 and batch: 50, loss is 4.3286361980438235 and perplexity is 75.84078419701514
At time: 701.6659173965454 and batch: 100, loss is 4.290922050476074 and perplexity is 73.03377829339526
At time: 703.3752241134644 and batch: 150, loss is 4.290763339996338 and perplexity is 73.02218798718121
At time: 705.0920922756195 and batch: 200, loss is 4.319482841491699 and perplexity is 75.14975390615054
At time: 706.8094143867493 and batch: 250, loss is 4.296791362762451 and perplexity is 73.46369677438294
At time: 708.5277671813965 and batch: 300, loss is 4.289411306381226 and perplexity is 72.92352624639797
At time: 710.2437858581543 and batch: 350, loss is 4.211201944351196 and perplexity is 67.43754729615017
At time: 711.9600718021393 and batch: 400, loss is 4.235434122085572 and perplexity is 69.0916664422154
At time: 713.676920413971 and batch: 450, loss is 4.270706090927124 and perplexity is 71.57215423049067
At time: 715.3942859172821 and batch: 500, loss is 4.254735040664673 and perplexity is 70.43815146991841
At time: 717.109100818634 and batch: 550, loss is 4.241257419586182 and perplexity is 69.49518152461432
At time: 718.8252472877502 and batch: 600, loss is 4.253733148574829 and perplexity is 70.36761538381114
At time: 720.5421545505524 and batch: 650, loss is 4.213098382949829 and perplexity is 67.56555980940912
At time: 722.2597005367279 and batch: 700, loss is 4.18818615436554 and perplexity is 65.90314435229388
At time: 723.974603176117 and batch: 750, loss is 4.189045748710632 and perplexity is 65.95981867747791
At time: 725.6904950141907 and batch: 800, loss is 4.127960233688355 and perplexity is 62.0512237645092
At time: 727.4074640274048 and batch: 850, loss is 4.158847441673279 and perplexity is 63.99771897272093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.725043932596843 and perplexity of 112.73545102639676
Finished 23 epochs...
Completing Train Step...
At time: 731.8714020252228 and batch: 50, loss is 4.328247718811035 and perplexity is 75.81132734941343
At time: 733.5674850940704 and batch: 100, loss is 4.2904102325439455 and perplexity is 72.99640786025829
At time: 735.2797162532806 and batch: 150, loss is 4.290321440696716 and perplexity is 72.9899266621063
At time: 736.9970011711121 and batch: 200, loss is 4.319011878967285 and perplexity is 75.11436952135544
At time: 738.7364008426666 and batch: 250, loss is 4.296424484252929 and perplexity is 73.43674946630277
At time: 740.4484317302704 and batch: 300, loss is 4.289093179702759 and perplexity is 72.90033101691738
At time: 742.1659870147705 and batch: 350, loss is 4.210880146026612 and perplexity is 67.415849497761
At time: 743.8829851150513 and batch: 400, loss is 4.235176877975464 and perplexity is 69.07389530382396
At time: 745.5975465774536 and batch: 450, loss is 4.27052077293396 and perplexity is 71.55889185141892
At time: 747.313773393631 and batch: 500, loss is 4.254641513824463 and perplexity is 70.43156392024238
At time: 749.0305633544922 and batch: 550, loss is 4.241230201721192 and perplexity is 69.49329003988733
At time: 750.7484965324402 and batch: 600, loss is 4.253654947280884 and perplexity is 70.36211276039522
At time: 752.4625771045685 and batch: 650, loss is 4.213052749633789 and perplexity is 67.56247663921312
At time: 754.1780028343201 and batch: 700, loss is 4.188220925331116 and perplexity is 65.90543590809709
At time: 755.8946797847748 and batch: 750, loss is 4.189175252914429 and perplexity is 65.9683613044194
At time: 757.6136481761932 and batch: 800, loss is 4.128149104118347 and perplexity is 62.062944512639696
At time: 759.3289670944214 and batch: 850, loss is 4.15902204990387 and perplexity is 64.0088944768317
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724970817565918 and perplexity of 112.72720867173233
Finished 24 epochs...
Completing Train Step...
At time: 763.7800116539001 and batch: 50, loss is 4.327946949005127 and perplexity is 75.78852901989735
At time: 765.4733755588531 and batch: 100, loss is 4.290078821182251 and perplexity is 72.97222002961287
At time: 767.1756224632263 and batch: 150, loss is 4.29001172542572 and perplexity is 72.9673240675552
At time: 768.8842227458954 and batch: 200, loss is 4.318699207305908 and perplexity is 75.09088705798561
At time: 770.5927803516388 and batch: 250, loss is 4.296135339736939 and perplexity is 73.4155187024498
At time: 772.3019399642944 and batch: 300, loss is 4.288828954696656 and perplexity is 72.88107147104839
At time: 774.0084054470062 and batch: 350, loss is 4.210651960372925 and perplexity is 67.40046792306858
At time: 775.7155058383942 and batch: 400, loss is 4.234986267089844 and perplexity is 69.06073032220112
At time: 777.4232521057129 and batch: 450, loss is 4.270408601760864 and perplexity is 71.55086545674799
At time: 779.1312277317047 and batch: 500, loss is 4.2545863628387455 and perplexity is 70.42767965717799
At time: 780.8367583751678 and batch: 550, loss is 4.241213274002075 and perplexity is 69.49211368694958
At time: 782.5719470977783 and batch: 600, loss is 4.253608903884888 and perplexity is 70.35887312435675
At time: 784.2804725170135 and batch: 650, loss is 4.21303695678711 and perplexity is 67.56140964380376
At time: 786.0035669803619 and batch: 700, loss is 4.1882560396194455 and perplexity is 65.90775017120765
At time: 787.7284302711487 and batch: 750, loss is 4.189276213645935 and perplexity is 65.97502185465433
At time: 789.455911397934 and batch: 800, loss is 4.128301272392273 and perplexity is 62.0723892423568
At time: 791.1817591190338 and batch: 850, loss is 4.1591527462005615 and perplexity is 64.01726074900365
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724942207336426 and perplexity of 112.72398356655793
Finished 25 epochs...
Completing Train Step...
At time: 795.582686662674 and batch: 50, loss is 4.327676229476928 and perplexity is 75.76801436206279
At time: 797.2999238967896 and batch: 100, loss is 4.289799308776855 and perplexity is 72.95182623915677
At time: 799.0095572471619 and batch: 150, loss is 4.289736795425415 and perplexity is 72.94726591854683
At time: 800.7184872627258 and batch: 200, loss is 4.318433895111084 and perplexity is 75.07096717254007
At time: 802.4255871772766 and batch: 250, loss is 4.295877256393433 and perplexity is 73.39657382470169
At time: 804.1343224048615 and batch: 300, loss is 4.288591384887695 and perplexity is 72.8637591853418
At time: 805.8437397480011 and batch: 350, loss is 4.210453386306763 and perplexity is 67.38708526686008
At time: 807.5545210838318 and batch: 400, loss is 4.234820261001587 and perplexity is 69.04926677204404
At time: 809.2613098621368 and batch: 450, loss is 4.270314826965332 and perplexity is 71.54415610355865
At time: 810.9685032367706 and batch: 500, loss is 4.254542064666748 and perplexity is 70.42455990881126
At time: 812.6767468452454 and batch: 550, loss is 4.241195583343506 and perplexity is 69.49088433656713
At time: 814.3863394260406 and batch: 600, loss is 4.253571033477783 and perplexity is 70.35620865564069
At time: 816.093456029892 and batch: 650, loss is 4.213023862838745 and perplexity is 67.56052500398616
At time: 817.8001351356506 and batch: 700, loss is 4.188283705711365 and perplexity is 65.90957360630566
At time: 819.5086438655853 and batch: 750, loss is 4.189357447624206 and perplexity is 65.98038148583525
At time: 821.2190632820129 and batch: 800, loss is 4.128430995941162 and perplexity is 62.08044201528322
At time: 822.9264488220215 and batch: 850, loss is 4.159258460998535 and perplexity is 64.02402867851937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724926312764485 and perplexity of 112.7221918813308
Finished 26 epochs...
Completing Train Step...
At time: 827.3595418930054 and batch: 50, loss is 4.327421760559082 and perplexity is 75.7487362103899
At time: 829.1011464595795 and batch: 100, loss is 4.289544744491577 and perplexity is 72.93325767319762
At time: 830.8154644966125 and batch: 150, loss is 4.2894798851013185 and perplexity is 72.92852741997768
At time: 832.5317137241364 and batch: 200, loss is 4.318192358016968 and perplexity is 75.05283693892686
At time: 834.248771905899 and batch: 250, loss is 4.29563738822937 and perplexity is 73.37897043462068
At time: 835.9672698974609 and batch: 300, loss is 4.288370275497437 and perplexity is 72.84765010497651
At time: 837.6831724643707 and batch: 350, loss is 4.21027096748352 and perplexity is 67.37479371520324
At time: 839.3993096351624 and batch: 400, loss is 4.234665985107422 and perplexity is 69.0386149563517
At time: 841.1158797740936 and batch: 450, loss is 4.270227737426758 and perplexity is 71.53792562732454
At time: 842.8327701091766 and batch: 500, loss is 4.254501237869262 and perplexity is 70.42168475825783
At time: 844.5503249168396 and batch: 550, loss is 4.24117506980896 and perplexity is 69.48945884753158
At time: 846.2664666175842 and batch: 600, loss is 4.253535394668579 and perplexity is 70.35370128882413
At time: 847.982589006424 and batch: 650, loss is 4.213008451461792 and perplexity is 67.55948381129129
At time: 849.6999678611755 and batch: 700, loss is 4.188304796218872 and perplexity is 65.91096368732127
At time: 851.4177539348602 and batch: 750, loss is 4.1894258117675784 and perplexity is 65.98489233228321
At time: 853.1334393024445 and batch: 800, loss is 4.1285454320907595 and perplexity is 62.087546668539574
At time: 854.8494226932526 and batch: 850, loss is 4.159348196983338 and perplexity is 64.02977419556986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724917093912761 and perplexity of 112.72115271694773
Finished 27 epochs...
Completing Train Step...
At time: 859.2900621891022 and batch: 50, loss is 4.3271777534484865 and perplexity is 75.73025523497236
At time: 860.9852659702301 and batch: 100, loss is 4.289305009841919 and perplexity is 72.91577513988913
At time: 862.693686246872 and batch: 150, loss is 4.289235825538635 and perplexity is 72.91073068728818
At time: 864.4048221111298 and batch: 200, loss is 4.317964887619018 and perplexity is 75.03576658181518
At time: 866.1156198978424 and batch: 250, loss is 4.295409784317017 and perplexity is 73.36227099436641
At time: 867.824164390564 and batch: 300, loss is 4.288160400390625 and perplexity is 72.83236280089814
At time: 869.5614354610443 and batch: 350, loss is 4.2100990104675295 and perplexity is 67.36320914277577
At time: 871.271808385849 and batch: 400, loss is 4.234518747329712 and perplexity is 69.02845061241544
At time: 872.9833338260651 and batch: 450, loss is 4.270143899917603 and perplexity is 71.53192831723308
At time: 874.6904497146606 and batch: 500, loss is 4.2544616603851315 and perplexity is 70.41889770029957
At time: 876.399222612381 and batch: 550, loss is 4.241151552200318 and perplexity is 69.48782464085006
At time: 878.1096427440643 and batch: 600, loss is 4.253500776290894 and perplexity is 70.35126579997792
At time: 879.82159948349 and batch: 650, loss is 4.212990045547485 and perplexity is 67.55824032866542
At time: 881.5303089618683 and batch: 700, loss is 4.188320426940918 and perplexity is 65.91199393132618
At time: 883.2391860485077 and batch: 750, loss is 4.189485421180725 and perplexity is 65.9888257702255
At time: 884.9499571323395 and batch: 800, loss is 4.128649325370788 and perplexity is 62.093997482504214
At time: 886.6620802879333 and batch: 850, loss is 4.15942717552185 and perplexity is 64.03483137325874
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724911053975423 and perplexity of 112.72047189030474
Finished 28 epochs...
Completing Train Step...
At time: 891.1197519302368 and batch: 50, loss is 4.326941833496094 and perplexity is 75.71239106410269
At time: 892.8291049003601 and batch: 100, loss is 4.289075603485108 and perplexity is 72.89904971609283
At time: 894.5382492542267 and batch: 150, loss is 4.289001202583313 and perplexity is 72.8936261628151
At time: 896.2456862926483 and batch: 200, loss is 4.317746973037719 and perplexity is 75.01941697563156
At time: 897.9537873268127 and batch: 250, loss is 4.295191221237182 and perplexity is 73.3462384625978
At time: 899.6643941402435 and batch: 300, loss is 4.287958383560181 and perplexity is 72.81765092388474
At time: 901.3740458488464 and batch: 350, loss is 4.209934606552124 and perplexity is 67.35213527775673
At time: 903.0809152126312 and batch: 400, loss is 4.234375991821289 and perplexity is 69.01859712418918
At time: 904.7896316051483 and batch: 450, loss is 4.270062017440796 and perplexity is 71.52607134556663
At time: 906.4985809326172 and batch: 500, loss is 4.254422607421875 and perplexity is 70.41614768737357
At time: 908.2084929943085 and batch: 550, loss is 4.241125459671021 and perplexity is 69.48601155140398
At time: 909.9147861003876 and batch: 600, loss is 4.253465623855591 and perplexity is 70.3487928251242
At time: 911.651529788971 and batch: 650, loss is 4.212969617843628 and perplexity is 67.5568602830345
At time: 913.3603365421295 and batch: 700, loss is 4.188331680297852 and perplexity is 65.91273566669358
At time: 915.0698318481445 and batch: 750, loss is 4.189538631439209 and perplexity is 65.99233714612156
At time: 916.7775189876556 and batch: 800, loss is 4.128744888305664 and perplexity is 62.099931650680546
At time: 918.503564119339 and batch: 850, loss is 4.159498496055603 and perplexity is 64.0393985344753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724905649820964 and perplexity of 112.71986273310995
Finished 29 epochs...
Completing Train Step...
At time: 922.9977831840515 and batch: 50, loss is 4.326712617874145 and perplexity is 75.69503859010074
At time: 924.6998805999756 and batch: 100, loss is 4.288854265213013 and perplexity is 72.88291615194481
At time: 926.4177298545837 and batch: 150, loss is 4.288774542808532 and perplexity is 72.87710598222705
At time: 928.1354360580444 and batch: 200, loss is 4.317536420822144 and perplexity is 75.00362313394868
At time: 929.8524188995361 and batch: 250, loss is 4.294979486465454 and perplexity is 73.33071015753889
At time: 931.5680174827576 and batch: 300, loss is 4.287762660980224 and perplexity is 72.80340026001339
At time: 933.2845094203949 and batch: 350, loss is 4.209776086807251 and perplexity is 67.34145948064054
At time: 935.0024998188019 and batch: 400, loss is 4.234236612319946 and perplexity is 69.00897801690738
At time: 936.7196543216705 and batch: 450, loss is 4.269981746673584 and perplexity is 71.52033012337331
At time: 938.4346697330475 and batch: 500, loss is 4.254383659362793 and perplexity is 70.4134051685013
At time: 940.1505441665649 and batch: 550, loss is 4.241096887588501 and perplexity is 69.48402621971064
At time: 941.8676583766937 and batch: 600, loss is 4.253428678512574 and perplexity is 70.34619381285346
At time: 943.5847733020782 and batch: 650, loss is 4.21294755935669 and perplexity is 67.55537009735002
At time: 945.2996380329132 and batch: 700, loss is 4.188339591026306 and perplexity is 65.91325708650955
At time: 947.0158426761627 and batch: 750, loss is 4.189586834907532 and perplexity is 65.99551828232501
At time: 948.7339634895325 and batch: 800, loss is 4.128833909034729 and perplexity is 62.10546007793962
At time: 950.4516224861145 and batch: 850, loss is 4.159563856124878 and perplexity is 64.043584290789
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724901835123698 and perplexity of 112.71943274177794
Finished 30 epochs...
Completing Train Step...
At time: 954.882737159729 and batch: 50, loss is 4.326488819122314 and perplexity is 75.67810003042943
At time: 956.6166484355927 and batch: 100, loss is 4.288639144897461 and perplexity is 72.86723924229518
At time: 958.3317494392395 and batch: 150, loss is 4.288554396629333 and perplexity is 72.86106413163482
At time: 960.0484390258789 and batch: 200, loss is 4.317331590652466 and perplexity is 74.98826170239172
At time: 961.7649986743927 and batch: 250, loss is 4.294773445129395 and perplexity is 73.31560255649244
At time: 963.4824266433716 and batch: 300, loss is 4.287571763992309 and perplexity is 72.78950363664775
At time: 965.1982951164246 and batch: 350, loss is 4.209622325897217 and perplexity is 67.33110579256444
At time: 966.9138827323914 and batch: 400, loss is 4.234100093841553 and perplexity is 68.99955765927413
At time: 968.6297886371613 and batch: 450, loss is 4.2699025535583495 and perplexity is 71.51466642989396
At time: 970.3465368747711 and batch: 500, loss is 4.254344625473022 and perplexity is 70.4106567130474
At time: 972.0621867179871 and batch: 550, loss is 4.241066646575928 and perplexity is 69.48192498417201
At time: 973.7773087024689 and batch: 600, loss is 4.253390579223633 and perplexity is 70.34351372394457
At time: 975.4933085441589 and batch: 650, loss is 4.2129235935211184 and perplexity is 67.55375109585876
At time: 977.2110319137573 and batch: 700, loss is 4.188344488143921 and perplexity is 65.91357987227221
At time: 978.9275689125061 and batch: 750, loss is 4.1896311378479005 and perplexity is 65.9984421426034
At time: 980.6429977416992 and batch: 800, loss is 4.128917407989502 and perplexity is 62.11064603544986
At time: 982.3588547706604 and batch: 850, loss is 4.15962435722351 and perplexity is 64.04745911521333
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724900881449382 and perplexity of 112.71932524420123
Finished 31 epochs...
Completing Train Step...
At time: 986.8002879619598 and batch: 50, loss is 4.3262697696685795 and perplexity is 75.66152459944391
At time: 988.5352087020874 and batch: 100, loss is 4.2884294795989994 and perplexity is 72.85196311232451
At time: 990.2437376976013 and batch: 150, loss is 4.288339724540711 and perplexity is 72.84542457356677
At time: 991.9537889957428 and batch: 200, loss is 4.3171313285827635 and perplexity is 74.97324590149768
At time: 993.660177230835 and batch: 250, loss is 4.294572010040283 and perplexity is 73.30083570888874
At time: 995.3686158657074 and batch: 300, loss is 4.2873849678039555 and perplexity is 72.77590810465257
At time: 997.0778369903564 and batch: 350, loss is 4.209472360610962 and perplexity is 67.32100922109694
At time: 998.8153209686279 and batch: 400, loss is 4.233965730667114 and perplexity is 68.99028728248472
At time: 1000.5215728282928 and batch: 450, loss is 4.269824094772339 and perplexity is 71.5090556960925
At time: 1002.2279653549194 and batch: 500, loss is 4.254305210113525 and perplexity is 70.40788150639388
At time: 1003.936342716217 and batch: 550, loss is 4.2410345554351805 and perplexity is 69.47969526571524
At time: 1005.645676612854 and batch: 600, loss is 4.2533512306213375 and perplexity is 70.34074585945513
At time: 1007.3528718948364 and batch: 650, loss is 4.212898359298706 and perplexity is 67.55204645098662
At time: 1009.059864282608 and batch: 700, loss is 4.188346982002258 and perplexity is 65.91374425160791
At time: 1010.7676677703857 and batch: 750, loss is 4.1896720266342165 and perplexity is 66.00114079397326
At time: 1012.4782605171204 and batch: 800, loss is 4.12899619102478 and perplexity is 62.1155394934258
At time: 1014.1867232322693 and batch: 850, loss is 4.1596808433532715 and perplexity is 64.05107701047928
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724897702534993 and perplexity of 112.71896691968588
Finished 32 epochs...
Completing Train Step...
At time: 1018.6048731803894 and batch: 50, loss is 4.326054916381836 and perplexity is 75.64527021841982
At time: 1020.3415186405182 and batch: 100, loss is 4.288224382400513 and perplexity is 72.837022910935
At time: 1022.0480105876923 and batch: 150, loss is 4.288129782676696 and perplexity is 72.83013287458695
At time: 1023.7560195922852 and batch: 200, loss is 4.316935243606568 and perplexity is 74.95854621560065
At time: 1025.4642000198364 and batch: 250, loss is 4.294374322891235 and perplexity is 73.28634650786623
At time: 1027.1742851734161 and batch: 300, loss is 4.287201271057129 and perplexity is 72.76254063490437
At time: 1028.8813123703003 and batch: 350, loss is 4.209325504302979 and perplexity is 67.311123432146
At time: 1030.5883121490479 and batch: 400, loss is 4.233833446502685 and perplexity is 68.9811615635852
At time: 1032.2977225780487 and batch: 450, loss is 4.269746513366699 and perplexity is 71.50350813823205
At time: 1034.0073595046997 and batch: 500, loss is 4.2542658615112305 and perplexity is 70.40511110917205
At time: 1035.7143671512604 and batch: 550, loss is 4.241000928878784 and perplexity is 69.47735894210547
At time: 1037.4223239421844 and batch: 600, loss is 4.253311042785644 and perplexity is 70.33791907391958
At time: 1039.1301789283752 and batch: 650, loss is 4.212871894836426 and perplexity is 67.5502587460568
At time: 1040.8396627902985 and batch: 700, loss is 4.1883473968505855 and perplexity is 65.91377159582012
At time: 1042.5756378173828 and batch: 750, loss is 4.189709935188294 and perplexity is 66.00364284921255
At time: 1044.2838716506958 and batch: 800, loss is 4.129070920944214 and perplexity is 62.12018155613607
At time: 1045.9990932941437 and batch: 850, loss is 4.1597335195541385 and perplexity is 64.05445106674308
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724899927775065 and perplexity of 112.71921774672704
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 1050.5054302215576 and batch: 50, loss is 4.325921440124512 and perplexity is 75.63517404468153
At time: 1052.2214500904083 and batch: 100, loss is 4.288120412826538 and perplexity is 72.82945047035194
At time: 1053.938626050949 and batch: 150, loss is 4.2880861282348635 and perplexity is 72.82695358518325
At time: 1055.655921459198 and batch: 200, loss is 4.316759519577026 and perplexity is 74.94537535506329
At time: 1057.3708379268646 and batch: 250, loss is 4.294220705032348 and perplexity is 73.27508928090775
At time: 1059.0875833034515 and batch: 300, loss is 4.287051954269409 and perplexity is 72.75167677716877
At time: 1060.8053147792816 and batch: 350, loss is 4.209272575378418 and perplexity is 67.30756082105519
At time: 1062.5237357616425 and batch: 400, loss is 4.233481988906861 and perplexity is 68.95692187024657
At time: 1064.2381949424744 and batch: 450, loss is 4.26949330329895 and perplexity is 71.48540502213444
At time: 1065.953292608261 and batch: 500, loss is 4.25414306640625 and perplexity is 70.39646623694712
At time: 1067.6703085899353 and batch: 550, loss is 4.240689888000488 and perplexity is 69.45575200385314
At time: 1069.388558626175 and batch: 600, loss is 4.252863426208496 and perplexity is 70.30644170076253
At time: 1071.1036927700043 and batch: 650, loss is 4.212334747314453 and perplexity is 67.51398403527041
At time: 1072.8193953037262 and batch: 700, loss is 4.187753849029541 and perplexity is 65.87466022869418
At time: 1074.5356752872467 and batch: 750, loss is 4.1891528987884525 and perplexity is 65.96688665584267
At time: 1076.254287481308 and batch: 800, loss is 4.1283998823165895 and perplexity is 62.07851049776546
At time: 1077.9698312282562 and batch: 850, loss is 4.159128551483154 and perplexity is 64.01571188820783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724893252054851 and perplexity of 112.7184652672782
Finished 34 epochs...
Completing Train Step...
At time: 1082.4241631031036 and batch: 50, loss is 4.325869884490967 and perplexity is 75.63127472588212
At time: 1084.126266002655 and batch: 100, loss is 4.288063669204712 and perplexity is 72.82531798080396
At time: 1085.8619346618652 and batch: 150, loss is 4.288009243011475 and perplexity is 72.82135448383495
At time: 1087.571745634079 and batch: 200, loss is 4.316736040115356 and perplexity is 74.94361569865328
At time: 1089.2819044589996 and batch: 250, loss is 4.2941718101501465 and perplexity is 73.27150659163733
At time: 1090.9920654296875 and batch: 300, loss is 4.28701810836792 and perplexity is 72.74921447275307
At time: 1092.7010514736176 and batch: 350, loss is 4.209252233505249 and perplexity is 67.30619167311522
At time: 1094.410441160202 and batch: 400, loss is 4.233448486328125 and perplexity is 68.95461167424125
At time: 1096.1218085289001 and batch: 450, loss is 4.269460916519165 and perplexity is 71.48308987755439
At time: 1097.8321073055267 and batch: 500, loss is 4.254088687896728 and perplexity is 70.39263828611762
At time: 1099.5394263267517 and batch: 550, loss is 4.240671520233154 and perplexity is 69.45447626847657
At time: 1101.2480821609497 and batch: 600, loss is 4.252858114242554 and perplexity is 70.30606823633065
At time: 1102.9584803581238 and batch: 650, loss is 4.2123245525360105 and perplexity is 67.51329574866986
At time: 1104.6697082519531 and batch: 700, loss is 4.18776228427887 and perplexity is 65.8752159002213
At time: 1106.3777375221252 and batch: 750, loss is 4.189163284301758 and perplexity is 65.96757175937931
At time: 1108.0880165100098 and batch: 800, loss is 4.128438410758972 and perplexity is 62.080902332156874
At time: 1109.7987804412842 and batch: 850, loss is 4.159152779579163 and perplexity is 64.0172628858103
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724883079528809 and perplexity of 112.71731864158691
Finished 35 epochs...
Completing Train Step...
At time: 1114.258406162262 and batch: 50, loss is 4.325828475952148 and perplexity is 75.6281430101471
At time: 1115.9657702445984 and batch: 100, loss is 4.28801854133606 and perplexity is 72.8220316035737
At time: 1117.6748349666595 and batch: 150, loss is 4.287954411506653 and perplexity is 72.81736168885192
At time: 1119.383484840393 and batch: 200, loss is 4.316708202362061 and perplexity is 74.94152946580654
At time: 1121.0899548530579 and batch: 250, loss is 4.294133806228638 and perplexity is 73.2687220399643
At time: 1122.7984824180603 and batch: 300, loss is 4.286988353729248 and perplexity is 72.74704987836628
At time: 1124.5076835155487 and batch: 350, loss is 4.209231357574463 and perplexity is 67.30478660838246
At time: 1126.215312719345 and batch: 400, loss is 4.233421707153321 and perplexity is 68.95276515136592
At time: 1127.9215610027313 and batch: 450, loss is 4.269438819885254 and perplexity is 71.4815103593376
At time: 1129.655448436737 and batch: 500, loss is 4.254061841964722 and perplexity is 70.39074855550234
At time: 1131.364322423935 and batch: 550, loss is 4.240662670135498 and perplexity is 69.45386159229889
At time: 1133.0730559825897 and batch: 600, loss is 4.252855052947998 and perplexity is 70.30585300907616
At time: 1134.7806932926178 and batch: 650, loss is 4.2123187637329105 and perplexity is 67.51290492862533
At time: 1136.4895651340485 and batch: 700, loss is 4.187767381668091 and perplexity is 65.8755516926926
At time: 1138.1985828876495 and batch: 750, loss is 4.18917552947998 and perplexity is 65.96837954899816
At time: 1139.907796382904 and batch: 800, loss is 4.128465466499328 and perplexity is 62.0825819996537
At time: 1141.6146502494812 and batch: 850, loss is 4.159171681404114 and perplexity is 64.01847294034333
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724874496459961 and perplexity of 112.71635118523257
Finished 36 epochs...
Completing Train Step...
At time: 1146.0409805774689 and batch: 50, loss is 4.325790033340454 and perplexity is 75.62523572269444
At time: 1147.7755076885223 and batch: 100, loss is 4.287977981567383 and perplexity is 72.81907801871603
At time: 1149.489964723587 and batch: 150, loss is 4.287909107208252 and perplexity is 72.81406282409615
At time: 1151.2062587738037 and batch: 200, loss is 4.31667742729187 and perplexity is 74.9392231704654
At time: 1152.9235248565674 and batch: 250, loss is 4.294099559783936 and perplexity is 73.26621288969153
At time: 1154.6390521526337 and batch: 300, loss is 4.286959209442139 and perplexity is 72.74492974835329
At time: 1156.3540360927582 and batch: 350, loss is 4.209209518432617 and perplexity is 67.30331674565113
At time: 1158.0713946819305 and batch: 400, loss is 4.2333972454071045 and perplexity is 68.95107846695359
At time: 1159.7881512641907 and batch: 450, loss is 4.269421138763428 and perplexity is 71.4802464972179
At time: 1161.5029554367065 and batch: 500, loss is 4.254046401977539 and perplexity is 70.38966173163713
At time: 1163.2170040607452 and batch: 550, loss is 4.240657176971435 and perplexity is 69.45348007189028
At time: 1164.9330296516418 and batch: 600, loss is 4.252851943969727 and perplexity is 70.3056344300466
At time: 1166.6505646705627 and batch: 650, loss is 4.212314147949218 and perplexity is 67.51259330437895
At time: 1168.3672983646393 and batch: 700, loss is 4.18777036190033 and perplexity is 65.87574801742804
At time: 1170.0818815231323 and batch: 750, loss is 4.189187302589416 and perplexity is 65.96915620652173
At time: 1171.8273046016693 and batch: 800, loss is 4.128486442565918 and perplexity is 62.08388426168589
At time: 1173.543983221054 and batch: 850, loss is 4.159187498092652 and perplexity is 64.0194855085982
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724865277608235 and perplexity of 112.71531207469366
Finished 37 epochs...
Completing Train Step...
At time: 1177.9801785945892 and batch: 50, loss is 4.325753087997437 and perplexity is 75.62244177403188
At time: 1179.7207119464874 and batch: 100, loss is 4.287939767837525 and perplexity is 72.81629538330792
At time: 1181.4379408359528 and batch: 150, loss is 4.287868452072144 and perplexity is 72.81110261863563
At time: 1183.1527206897736 and batch: 200, loss is 4.316645669937134 and perplexity is 74.93684333676033
At time: 1184.8671824932098 and batch: 250, loss is 4.2940670776367185 and perplexity is 73.2638330844292
At time: 1186.5836572647095 and batch: 300, loss is 4.286930389404297 and perplexity is 72.74283326693563
At time: 1188.3011090755463 and batch: 350, loss is 4.209187650680542 and perplexity is 67.30184498949878
At time: 1190.0165100097656 and batch: 400, loss is 4.23337381362915 and perplexity is 68.94946283952183
At time: 1191.730914592743 and batch: 450, loss is 4.269405031204224 and perplexity is 71.4790951341884
At time: 1193.4461407661438 and batch: 500, loss is 4.254036283493042 and perplexity is 70.3889494985395
At time: 1195.1624112129211 and batch: 550, loss is 4.240652990341187 and perplexity is 69.45318929645842
At time: 1196.8776950836182 and batch: 600, loss is 4.252848320007324 and perplexity is 70.30537964553235
At time: 1198.5921008586884 and batch: 650, loss is 4.212310009002685 and perplexity is 67.51231387394323
At time: 1200.306868314743 and batch: 700, loss is 4.187771921157837 and perplexity is 65.87585073476275
At time: 1202.023315191269 and batch: 750, loss is 4.189198231697082 and perplexity is 65.9698771944724
At time: 1203.738602399826 and batch: 800, loss is 4.1285041189193725 and perplexity is 62.08498168806716
At time: 1205.4542169570923 and batch: 850, loss is 4.159201464653015 and perplexity is 64.02037964685098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724859555562337 and perplexity of 112.71466711434975
Finished 38 epochs...
Completing Train Step...
At time: 1209.88529920578 and batch: 50, loss is 4.325716695785522 and perplexity is 75.61968975618169
At time: 1211.6184875965118 and batch: 100, loss is 4.287902736663819 and perplexity is 72.8135989603511
At time: 1213.3249480724335 and batch: 150, loss is 4.287830443382263 and perplexity is 72.80833521660931
At time: 1215.061441898346 and batch: 200, loss is 4.316613435745239 and perplexity is 74.93442784710315
At time: 1216.770669221878 and batch: 250, loss is 4.294035263061524 and perplexity is 73.2615022637797
At time: 1218.4766578674316 and batch: 300, loss is 4.2869017219543455 and perplexity is 72.74074794529417
At time: 1220.1837322711945 and batch: 350, loss is 4.209166021347046 and perplexity is 67.30038931119128
At time: 1221.8913578987122 and batch: 400, loss is 4.2333510208129885 and perplexity is 68.9478913050008
At time: 1223.6000368595123 and batch: 450, loss is 4.269389972686768 and perplexity is 71.4780187730908
At time: 1225.3054292201996 and batch: 500, loss is 4.254028549194336 and perplexity is 70.38840509148382
At time: 1227.0133893489838 and batch: 550, loss is 4.240649356842041 and perplexity is 69.4529369388129
At time: 1228.7209873199463 and batch: 600, loss is 4.2528440952301025 and perplexity is 70.30508262159331
At time: 1230.4299733638763 and batch: 650, loss is 4.212305727005005 and perplexity is 67.51202478699078
At time: 1232.135411977768 and batch: 700, loss is 4.187773103713989 and perplexity is 65.8759286367014
At time: 1233.8419983386993 and batch: 750, loss is 4.189208488464356 and perplexity is 65.97055383561994
At time: 1235.5502109527588 and batch: 800, loss is 4.128520007133484 and perplexity is 62.08596811538559
At time: 1237.259928226471 and batch: 850, loss is 4.159214386940002 and perplexity is 64.02120694191505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724855105082194 and perplexity of 112.71416548107823
Finished 39 epochs...
Completing Train Step...
At time: 1241.7287304401398 and batch: 50, loss is 4.325680932998657 and perplexity is 75.61698543369145
At time: 1243.437884569168 and batch: 100, loss is 4.287866764068603 and perplexity is 72.81097971334034
At time: 1245.1465332508087 and batch: 150, loss is 4.287794055938721 and perplexity is 72.80568595562245
At time: 1246.8533205986023 and batch: 200, loss is 4.316581392288208 and perplexity is 74.93202672745458
At time: 1248.5611729621887 and batch: 250, loss is 4.294004220962524 and perplexity is 73.25922810827103
At time: 1250.2697286605835 and batch: 300, loss is 4.286873283386231 and perplexity is 72.73867933199337
At time: 1251.9794249534607 and batch: 350, loss is 4.209144620895386 and perplexity is 67.29894906787415
At time: 1253.685392856598 and batch: 400, loss is 4.233328495025635 and perplexity is 68.94633821695511
At time: 1255.3924996852875 and batch: 450, loss is 4.269375553131104 and perplexity is 71.47698809925129
At time: 1257.1006665229797 and batch: 500, loss is 4.2540223598480225 and perplexity is 70.38796943461645
At time: 1258.8372023105621 and batch: 550, loss is 4.240645713806153 and perplexity is 69.45268391973198
At time: 1260.5428938865662 and batch: 600, loss is 4.252839727401733 and perplexity is 70.30477554172958
At time: 1262.249713420868 and batch: 650, loss is 4.212301530838013 and perplexity is 67.51174149585513
At time: 1263.9584646224976 and batch: 700, loss is 4.1877737760543825 and perplexity is 65.87597292776405
At time: 1265.6686851978302 and batch: 750, loss is 4.18921805858612 and perplexity is 65.97118518487405
At time: 1267.3755757808685 and batch: 800, loss is 4.128534712791443 and perplexity is 62.08688113711002
At time: 1269.0840365886688 and batch: 850, loss is 4.159226469993591 and perplexity is 64.02198051826294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724850336710612 and perplexity of 112.71362801933607
Finished 40 epochs...
Completing Train Step...
At time: 1273.5429055690765 and batch: 50, loss is 4.3256457328796385 and perplexity is 75.61432375365037
At time: 1275.2510254383087 and batch: 100, loss is 4.287831611633301 and perplexity is 72.8084202750722
At time: 1276.9666411876678 and batch: 150, loss is 4.287758603096008 and perplexity is 72.80310483284399
At time: 1278.6848125457764 and batch: 200, loss is 4.316549596786499 and perplexity is 74.92964426394667
At time: 1280.4015645980835 and batch: 250, loss is 4.293973302841186 and perplexity is 73.25696310558226
At time: 1282.1172392368317 and batch: 300, loss is 4.28684510231018 and perplexity is 72.73662950662258
At time: 1283.8335773944855 and batch: 350, loss is 4.209123487472534 and perplexity is 67.29752682575449
At time: 1285.5512657165527 and batch: 400, loss is 4.233306427001953 and perplexity is 68.94481672431877
At time: 1287.2714080810547 and batch: 450, loss is 4.269361715316773 and perplexity is 71.4759990208044
At time: 1288.9865469932556 and batch: 500, loss is 4.254016599655151 and perplexity is 70.38756398750444
At time: 1290.7030274868011 and batch: 550, loss is 4.240642280578613 and perplexity is 69.45244547327417
At time: 1292.4204444885254 and batch: 600, loss is 4.252834978103638 and perplexity is 70.3044416441859
At time: 1294.1378495693207 and batch: 650, loss is 4.212297143936158 and perplexity is 67.51144532912076
At time: 1295.8531246185303 and batch: 700, loss is 4.187774119377136 and perplexity is 65.87599554448835
At time: 1297.5686707496643 and batch: 750, loss is 4.189226994514465 and perplexity is 65.97177470129162
At time: 1299.2873895168304 and batch: 800, loss is 4.128548851013184 and perplexity is 62.08775894140802
At time: 1301.0060632228851 and batch: 850, loss is 4.159238109588623 and perplexity is 64.02272571252617
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724846839904785 and perplexity of 112.71323388235392
Finished 41 epochs...
Completing Train Step...
At time: 1305.4931230545044 and batch: 50, loss is 4.325610904693604 and perplexity is 75.61169028977542
At time: 1307.20454287529 and batch: 100, loss is 4.287797060012817 and perplexity is 72.8059046696262
At time: 1308.9137451648712 and batch: 150, loss is 4.2877239418029784 and perplexity is 72.80058142682643
At time: 1310.6238408088684 and batch: 200, loss is 4.316518058776856 and perplexity is 74.92728116936718
At time: 1312.3340928554535 and batch: 250, loss is 4.2939428615570066 and perplexity is 73.25473310349246
At time: 1314.0458569526672 and batch: 300, loss is 4.286817169189453 and perplexity is 72.73459777394571
At time: 1315.75581407547 and batch: 350, loss is 4.20910264968872 and perplexity is 67.29612450904992
At time: 1317.465285539627 and batch: 400, loss is 4.233284616470337 and perplexity is 68.94331301761227
At time: 1319.1756172180176 and batch: 450, loss is 4.269348020553589 and perplexity is 71.47502018062698
At time: 1320.8871762752533 and batch: 500, loss is 4.25401125907898 and perplexity is 70.38718807836122
At time: 1322.5978393554688 and batch: 550, loss is 4.240638694763184 and perplexity is 69.45219643007009
At time: 1324.3074288368225 and batch: 600, loss is 4.252830076217651 and perplexity is 70.30409702067324
At time: 1326.0181903839111 and batch: 650, loss is 4.212292833328247 and perplexity is 67.51115431437769
At time: 1327.7292733192444 and batch: 700, loss is 4.187774319648742 and perplexity is 65.87600873758112
At time: 1329.4398002624512 and batch: 750, loss is 4.189235634803772 and perplexity is 65.9723447189737
At time: 1331.1500852108002 and batch: 800, loss is 4.128562293052673 and perplexity is 62.088593533124815
At time: 1332.8602511882782 and batch: 850, loss is 4.159249143600464 and perplexity is 64.02343214393716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724845250447591 and perplexity of 112.71305472963581
Finished 42 epochs...
Completing Train Step...
At time: 1337.2839398384094 and batch: 50, loss is 4.325576496124268 and perplexity is 75.60908864444718
At time: 1338.999944448471 and batch: 100, loss is 4.287762708663941 and perplexity is 72.80340373155016
At time: 1340.7073407173157 and batch: 150, loss is 4.287689642906189 and perplexity is 72.79808449001919
At time: 1342.4177050590515 and batch: 200, loss is 4.316486949920654 and perplexity is 74.9249503036072
At time: 1344.1260113716125 and batch: 250, loss is 4.293912544250488 and perplexity is 73.25251225096032
At time: 1345.8619759082794 and batch: 300, loss is 4.286789216995239 and perplexity is 72.7325647107571
At time: 1347.5721147060394 and batch: 350, loss is 4.209081916809082 and perplexity is 67.29472928106397
At time: 1349.2824909687042 and batch: 400, loss is 4.233262910842895 and perplexity is 68.94181657598594
At time: 1350.9902353286743 and batch: 450, loss is 4.269334564208984 and perplexity is 71.47405839459587
At time: 1352.6975672245026 and batch: 500, loss is 4.254006071090698 and perplexity is 70.38682291140152
At time: 1354.4056062698364 and batch: 550, loss is 4.240635004043579 and perplexity is 69.45194010196016
At time: 1356.115582704544 and batch: 600, loss is 4.252825117111206 and perplexity is 70.30374837603705
At time: 1357.8236496448517 and batch: 650, loss is 4.212288179397583 and perplexity is 67.51084012287758
At time: 1359.5314474105835 and batch: 700, loss is 4.187774305343628 and perplexity is 65.87600779521728
At time: 1361.2403073310852 and batch: 750, loss is 4.189243836402893 and perplexity is 65.97288579991701
At time: 1362.9504630565643 and batch: 800, loss is 4.128575439453125 and perplexity is 62.08940978000423
At time: 1364.6598145961761 and batch: 850, loss is 4.159259805679321 and perplexity is 64.02411477045848
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724843660990397 and perplexity of 112.71287557720257
Finished 43 epochs...
Completing Train Step...
At time: 1369.110763311386 and batch: 50, loss is 4.325542325973511 and perplexity is 75.60650511462967
At time: 1370.8520276546478 and batch: 100, loss is 4.287728900909424 and perplexity is 72.8009424535542
At time: 1372.5663113594055 and batch: 150, loss is 4.2876558589935305 and perplexity is 72.79562512743476
At time: 1374.28218460083 and batch: 200, loss is 4.316455850601196 and perplexity is 74.92262022487442
At time: 1375.9992508888245 and batch: 250, loss is 4.293882417678833 and perplexity is 73.25030543714311
At time: 1377.7174174785614 and batch: 300, loss is 4.286761703491211 and perplexity is 72.73056361057374
At time: 1379.4332163333893 and batch: 350, loss is 4.209061374664307 and perplexity is 67.29334691719086
At time: 1381.1483812332153 and batch: 400, loss is 4.233241281509399 and perplexity is 68.94032542656976
At time: 1382.8637647628784 and batch: 450, loss is 4.269321279525757 and perplexity is 71.47310889067808
At time: 1384.5814790725708 and batch: 500, loss is 4.254001159667968 and perplexity is 70.38647721280853
At time: 1386.2965307235718 and batch: 550, loss is 4.240631456375122 and perplexity is 69.45169370994005
At time: 1388.0396738052368 and batch: 600, loss is 4.252819976806641 and perplexity is 70.30338699428714
At time: 1389.756466627121 and batch: 650, loss is 4.212283372879028 and perplexity is 67.51051563155173
At time: 1391.4747514724731 and batch: 700, loss is 4.18777400970459 and perplexity is 65.87598831970061
At time: 1393.1913847923279 and batch: 750, loss is 4.189251761436463 and perplexity is 65.97340863932342
At time: 1394.9066083431244 and batch: 800, loss is 4.128588228225708 and perplexity is 62.0902038324232
At time: 1396.622599363327 and batch: 850, loss is 4.159270272254944 and perplexity is 64.02478488720433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724843660990397 and perplexity of 112.71287557720257
Finished 44 epochs...
Completing Train Step...
At time: 1401.0660471916199 and batch: 50, loss is 4.325508432388306 and perplexity is 75.60394258253345
At time: 1402.8041393756866 and batch: 100, loss is 4.287695646286011 and perplexity is 72.79852152588252
At time: 1404.519511461258 and batch: 150, loss is 4.287622485160828 and perplexity is 72.79319569896016
At time: 1406.2383065223694 and batch: 200, loss is 4.316425094604492 and perplexity is 74.92031594044919
At time: 1407.9528594017029 and batch: 250, loss is 4.293852396011353 and perplexity is 73.24810637384041
At time: 1409.6684374809265 and batch: 300, loss is 4.286734209060669 and perplexity is 72.72856395263412
At time: 1411.3848528862 and batch: 350, loss is 4.209041109085083 and perplexity is 67.29198319235607
At time: 1413.1020419597626 and batch: 400, loss is 4.233219890594483 and perplexity is 68.93885074570667
At time: 1414.8159713745117 and batch: 450, loss is 4.269308252334595 and perplexity is 71.47217780289034
At time: 1416.5302336215973 and batch: 500, loss is 4.253996133804321 and perplexity is 70.38612346086042
At time: 1418.2460072040558 and batch: 550, loss is 4.2406277847290035 and perplexity is 69.45143870836654
At time: 1419.9633898735046 and batch: 600, loss is 4.2528147125244145 and perplexity is 70.3030168983907
At time: 1421.6771993637085 and batch: 650, loss is 4.212278566360474 and perplexity is 67.51019114178554
At time: 1423.392653465271 and batch: 700, loss is 4.187773747444153 and perplexity is 65.87597104303737
At time: 1425.1091990470886 and batch: 750, loss is 4.189259467124939 and perplexity is 65.97391701181675
At time: 1426.8268010616302 and batch: 800, loss is 4.128600878715515 and perplexity is 62.09098930888221
At time: 1428.5423848628998 and batch: 850, loss is 4.159280376434326 and perplexity is 64.02543180838404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724842707316081 and perplexity of 112.71276808587925
Finished 45 epochs...
Completing Train Step...
At time: 1433.0065512657166 and batch: 50, loss is 4.32547477722168 and perplexity is 75.60139816206494
At time: 1434.7146940231323 and batch: 100, loss is 4.2876624584198 and perplexity is 72.79610553838074
At time: 1436.4206278324127 and batch: 150, loss is 4.287589244842529 and perplexity is 72.79077607017997
At time: 1438.1285905838013 and batch: 200, loss is 4.316394653320312 and perplexity is 74.91803530453375
At time: 1439.8381249904633 and batch: 250, loss is 4.2938226699829105 and perplexity is 73.24592903090904
At time: 1441.5471329689026 and batch: 300, loss is 4.286706876754761 and perplexity is 72.72657614044175
At time: 1443.2536387443542 and batch: 350, loss is 4.2090209865570065 and perplexity is 67.29062912115866
At time: 1444.9616587162018 and batch: 400, loss is 4.233198585510254 and perplexity is 68.93738201333072
At time: 1446.669885635376 and batch: 450, loss is 4.269295244216919 and perplexity is 71.47124809043788
At time: 1448.3786687850952 and batch: 500, loss is 4.253991174697876 and perplexity is 70.3857744094474
At time: 1450.084618806839 and batch: 550, loss is 4.240624017715454 and perplexity is 69.4511770843487
At time: 1451.7924416065216 and batch: 600, loss is 4.252809391021729 and perplexity is 70.3026427816929
At time: 1453.5022757053375 and batch: 650, loss is 4.212273607254028 and perplexity is 67.50985635239164
At time: 1455.2124824523926 and batch: 700, loss is 4.187773270606995 and perplexity is 65.87593963093403
At time: 1456.918651342392 and batch: 750, loss is 4.189266920089722 and perplexity is 65.97440871492917
At time: 1458.626603603363 and batch: 800, loss is 4.128613038063049 and perplexity is 62.09174429939004
At time: 1460.3362169265747 and batch: 850, loss is 4.159290165901184 and perplexity is 64.02605858629468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724842707316081 and perplexity of 112.71276808587925
Finished 46 epochs...
Completing Train Step...
At time: 1464.7980663776398 and batch: 50, loss is 4.32544129371643 and perplexity is 75.59886680463231
At time: 1466.5069618225098 and batch: 100, loss is 4.287629661560058 and perplexity is 72.79371809386821
At time: 1468.2168354988098 and batch: 150, loss is 4.287556200027466 and perplexity is 72.78837075218831
At time: 1469.9257261753082 and batch: 200, loss is 4.316364259719848 and perplexity is 74.91575831030435
At time: 1471.6335508823395 and batch: 250, loss is 4.293793058395385 and perplexity is 73.24376013478306
At time: 1473.3418183326721 and batch: 300, loss is 4.286679773330689 and perplexity is 72.72460502791938
At time: 1475.0796945095062 and batch: 350, loss is 4.209000902175903 and perplexity is 67.28927764409055
At time: 1476.7886333465576 and batch: 400, loss is 4.2331774711608885 and perplexity is 68.93592646072909
At time: 1478.4957876205444 and batch: 450, loss is 4.269282464981079 and perplexity is 71.47033474833866
At time: 1480.2039067745209 and batch: 500, loss is 4.253986377716064 and perplexity is 70.38543677097756
At time: 1481.9132435321808 and batch: 550, loss is 4.240620193481445 and perplexity is 69.45091148730315
At time: 1483.622947692871 and batch: 600, loss is 4.252803964614868 and perplexity is 70.30226129198483
At time: 1485.3303396701813 and batch: 650, loss is 4.21226863861084 and perplexity is 67.50952092083703
At time: 1487.039071559906 and batch: 700, loss is 4.187772550582886 and perplexity is 65.87589219868639
At time: 1488.748619556427 and batch: 750, loss is 4.189273953437805 and perplexity is 65.97487273754209
At time: 1490.4593427181244 and batch: 800, loss is 4.128625121116638 and perplexity is 62.092494561796556
At time: 1492.166286945343 and batch: 850, loss is 4.159299912452698 and perplexity is 64.02668262261405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.7248430252075195 and perplexity of 112.71280391630893
Annealing...
Finished 47 epochs...
Completing Train Step...
At time: 1496.6489508152008 and batch: 50, loss is 4.325419616699219 and perplexity is 75.59722806445698
At time: 1498.361210823059 and batch: 100, loss is 4.287609481811524 and perplexity is 72.79224914976369
At time: 1500.0762829780579 and batch: 150, loss is 4.2875415182113645 and perplexity is 72.78730209455954
At time: 1501.7937343120575 and batch: 200, loss is 4.3163361072540285 and perplexity is 74.91364927666653
At time: 1503.511389017105 and batch: 250, loss is 4.293765707015991 and perplexity is 73.24175684430786
At time: 1505.2277677059174 and batch: 300, loss is 4.286658306121826 and perplexity is 72.72304385039088
At time: 1506.9440128803253 and batch: 350, loss is 4.2090003776550295 and perplexity is 67.28924234946909
At time: 1508.6612391471863 and batch: 400, loss is 4.2331225872039795 and perplexity is 68.93214308813593
At time: 1510.3787865638733 and batch: 450, loss is 4.269236526489258 and perplexity is 71.46705158436274
At time: 1512.0950183868408 and batch: 500, loss is 4.253950462341309 and perplexity is 70.38290889703367
At time: 1513.8101727962494 and batch: 550, loss is 4.240558843612671 and perplexity is 69.44665081369442
At time: 1515.5273478031158 and batch: 600, loss is 4.25272912979126 and perplexity is 70.29700043151203
At time: 1517.2453353404999 and batch: 650, loss is 4.212188968658447 and perplexity is 67.50414265476518
At time: 1518.9904398918152 and batch: 700, loss is 4.187681093215942 and perplexity is 65.86986763854023
At time: 1520.7055144309998 and batch: 750, loss is 4.189187164306641 and perplexity is 65.96914708412434
At time: 1522.4228343963623 and batch: 800, loss is 4.128512682914734 and perplexity is 62.08551338583909
At time: 1524.1412372589111 and batch: 850, loss is 4.159204454421997 and perplexity is 64.02057105328237
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.724843660990397 and perplexity of 112.71287557720257
Annealing...
Finished 48 epochs...
Completing Train Step...
At time: 1528.572735786438 and batch: 50, loss is 4.3254172897338865 and perplexity is 75.59705215253273
At time: 1530.306552886963 and batch: 100, loss is 4.287607192993164 and perplexity is 72.79208254171806
At time: 1532.01793384552 and batch: 150, loss is 4.287539358139038 and perplexity is 72.78714486889238
At time: 1533.7289640903473 and batch: 200, loss is 4.316333293914795 and perplexity is 74.91343851945436
At time: 1535.4380338191986 and batch: 250, loss is 4.293762283325195 and perplexity is 73.24150608760834
At time: 1537.1485333442688 and batch: 300, loss is 4.286656122207642 and perplexity is 72.72288502967736
At time: 1538.8600709438324 and batch: 350, loss is 4.209001226425171 and perplexity is 67.28929946259306
At time: 1540.571863412857 and batch: 400, loss is 4.233115482330322 and perplexity is 68.93165333570816
At time: 1542.280291557312 and batch: 450, loss is 4.269230308532715 and perplexity is 71.46660720672332
At time: 1543.9899866580963 and batch: 500, loss is 4.253944940567017 and perplexity is 70.38252025956972
At time: 1545.700587272644 and batch: 550, loss is 4.240550231933594 and perplexity is 69.44605276399972
At time: 1547.4131472110748 and batch: 600, loss is 4.252719135284424 and perplexity is 70.29629785117162
At time: 1549.121863603592 and batch: 650, loss is 4.212178316116333 and perplexity is 67.50342356787272
At time: 1550.8317799568176 and batch: 700, loss is 4.1876687860488895 and perplexity is 65.86905697206397
At time: 1552.5425164699554 and batch: 750, loss is 4.189175152778626 and perplexity is 65.9683546986249
At time: 1554.2554335594177 and batch: 800, loss is 4.128496952056885 and perplexity is 62.08453673513532
At time: 1555.9645900726318 and batch: 850, loss is 4.1591911172866824 and perplexity is 64.01971720795727
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.7248430252075195 and perplexity of 112.71280391630893
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 1560.4114346504211 and batch: 50, loss is 4.325417108535767 and perplexity is 75.59703845449027
At time: 1562.1478517055511 and batch: 100, loss is 4.287607049942016 and perplexity is 72.79207212872785
At time: 1563.8550124168396 and batch: 150, loss is 4.287539410591125 and perplexity is 72.78714868673015
At time: 1565.563913822174 and batch: 200, loss is 4.316333065032959 and perplexity is 74.91342137313096
At time: 1567.2736039161682 and batch: 250, loss is 4.293762187957764 and perplexity is 73.24149910275437
At time: 1568.9835357666016 and batch: 300, loss is 4.286656150817871 and perplexity is 72.72288711029577
At time: 1570.6914927959442 and batch: 350, loss is 4.20900164604187 and perplexity is 67.28932769831275
At time: 1572.4002616405487 and batch: 400, loss is 4.233114976882934 and perplexity is 68.93161849439285
At time: 1574.1103146076202 and batch: 450, loss is 4.269229955673218 and perplexity is 71.46658198905669
At time: 1575.8191108703613 and batch: 500, loss is 4.253944606781006 and perplexity is 70.38249676687293
At time: 1577.525175333023 and batch: 550, loss is 4.240549335479736 and perplexity is 69.44599050884571
At time: 1579.2355103492737 and batch: 600, loss is 4.252718105316162 and perplexity is 70.2962254482532
At time: 1580.944402217865 and batch: 650, loss is 4.212177515029907 and perplexity is 67.50336949181808
At time: 1582.6535377502441 and batch: 700, loss is 4.1876676607131955 and perplexity is 65.86898284730475
At time: 1584.3597733974457 and batch: 750, loss is 4.1891742706298825 and perplexity is 65.96829650474938
At time: 1586.0689163208008 and batch: 800, loss is 4.128495383262634 and perplexity is 62.08443933734742
At time: 1587.778553724289 and batch: 850, loss is 4.1591899824142455 and perplexity is 64.01964455378602
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.7248430252075195 and perplexity of 112.71280391630893
Annealing...
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9fbf6be518>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'num_layers': 1, 'anneal': 2.386890260775652, 'wordvec_source': '', 'dropout': 0.6838474312591215, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 18.074865051293788, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.1401097774505615 and batch: 50, loss is 7.20442084312439 and perplexity is 1345.3652858086002
At time: 3.710766315460205 and batch: 100, loss is 6.508929176330566 and perplexity is 671.107395003296
At time: 5.284096717834473 and batch: 150, loss is 6.378881807327271 and perplexity is 589.2684234903829
At time: 6.859081029891968 and batch: 200, loss is 6.368178730010986 and perplexity is 582.9950699749129
At time: 8.436391830444336 and batch: 250, loss is 6.382643184661865 and perplexity is 591.4890580866913
At time: 10.012852430343628 and batch: 300, loss is 6.30390510559082 and perplexity is 546.7026786670123
At time: 11.616636514663696 and batch: 350, loss is 6.272353658676147 and perplexity is 529.7226981978722
At time: 13.195683002471924 and batch: 400, loss is 6.284065570831299 and perplexity is 535.9632368924358
At time: 14.77574634552002 and batch: 450, loss is 6.272743005752563 and perplexity is 529.9289843375734
At time: 16.356037855148315 and batch: 500, loss is 6.267305908203125 and perplexity is 527.0555274668749
At time: 17.93637228012085 and batch: 550, loss is 6.221452903747559 and perplexity is 503.43414227249843
At time: 19.516575574874878 and batch: 600, loss is 6.2336718940734865 and perplexity is 509.62333502424474
At time: 21.09753155708313 and batch: 650, loss is 6.25689393043518 and perplexity is 521.5963069926132
At time: 22.68050479888916 and batch: 700, loss is 6.21098819732666 and perplexity is 498.1933214239576
At time: 24.263512134552002 and batch: 750, loss is 6.19969292640686 and perplexity is 492.5977541040001
At time: 25.845706701278687 and batch: 800, loss is 6.223754625320435 and perplexity is 504.59424209951226
At time: 27.43259358406067 and batch: 850, loss is 6.211247653961181 and perplexity is 498.32259775655103
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.556323369344075 and perplexity of 258.8693176598981
Finished 1 epochs...
Completing Train Step...
At time: 31.678082704544067 and batch: 50, loss is 5.896355056762696 and perplexity is 363.70934894491467
At time: 33.26722860336304 and batch: 100, loss is 5.806488933563233 and perplexity is 332.44982071467325
At time: 34.85639309883118 and batch: 150, loss is 5.750999402999878 and perplexity is 314.5048203342161
At time: 36.447641134262085 and batch: 200, loss is 5.781379203796387 and perplexity is 324.20602839126514
At time: 38.03791880607605 and batch: 250, loss is 5.8120947933197025 and perplexity is 334.3187212886372
At time: 39.62885880470276 and batch: 300, loss is 5.729574975967407 and perplexity is 307.8384018497301
At time: 41.27884292602539 and batch: 350, loss is 5.726543655395508 and perplexity is 306.90665789017413
At time: 42.99501276016235 and batch: 400, loss is 5.71844822883606 and perplexity is 304.43214720448054
At time: 44.70991587638855 and batch: 450, loss is 5.711044273376465 and perplexity is 302.18646885607154
At time: 46.4250750541687 and batch: 500, loss is 5.686717081069946 and perplexity is 294.9238186336922
At time: 48.1406512260437 and batch: 550, loss is 5.6766736030578615 and perplexity is 291.9765827695915
At time: 49.85762071609497 and batch: 600, loss is 5.669578142166138 and perplexity is 289.9122068555399
At time: 51.57422637939453 and batch: 650, loss is 5.743459148406982 and perplexity is 312.14229214376024
At time: 53.28914403915405 and batch: 700, loss is 5.6786433124542235 and perplexity is 292.5522585590729
At time: 55.00471615791321 and batch: 750, loss is 5.685648269653321 and perplexity is 294.6087690835713
At time: 56.72290778160095 and batch: 800, loss is 5.712968959808349 and perplexity is 302.7686431243174
At time: 58.44089484214783 and batch: 850, loss is 5.680829467773438 and perplexity is 293.1925228388506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.397886276245117 and perplexity of 220.9389184573775
Finished 2 epochs...
Completing Train Step...
At time: 62.883363246917725 and batch: 50, loss is 5.679643650054931 and perplexity is 292.84505600721366
At time: 64.55056071281433 and batch: 100, loss is 5.623689594268799 and perplexity is 276.9091832893193
At time: 66.22724652290344 and batch: 150, loss is 5.627683982849121 and perplexity is 278.017478175472
At time: 67.91551089286804 and batch: 200, loss is 5.631473369598389 and perplexity is 279.07299253635097
At time: 69.61238670349121 and batch: 250, loss is 5.665665702819824 and perplexity is 288.780158908964
At time: 71.31223607063293 and batch: 300, loss is 5.629258728027343 and perplexity is 278.4556297566903
At time: 73.02069759368896 and batch: 350, loss is 5.600454158782959 and perplexity is 270.54925184755433
At time: 74.7279269695282 and batch: 400, loss is 5.619826192855835 and perplexity is 275.84143585589607
At time: 76.43589806556702 and batch: 450, loss is 5.6274074268341066 and perplexity is 277.94060140046054
At time: 78.1449179649353 and batch: 500, loss is 5.6363543510437015 and perplexity is 280.4384723647326
At time: 79.85244941711426 and batch: 550, loss is 5.635541296005249 and perplexity is 280.2105531198013
At time: 81.56026816368103 and batch: 600, loss is 5.621521425247193 and perplexity is 276.3094477752386
At time: 83.26980376243591 and batch: 650, loss is 5.660783538818359 and perplexity is 287.3737228321876
At time: 84.97972178459167 and batch: 700, loss is 5.618561592102051 and perplexity is 275.4928270401317
At time: 86.68919897079468 and batch: 750, loss is 5.5967981719970705 and perplexity is 269.56193326943287
At time: 88.39636707305908 and batch: 800, loss is 5.594623756408692 and perplexity is 268.97643039368046
At time: 90.10497426986694 and batch: 850, loss is 5.61223258972168 and perplexity is 273.75473826778415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.441486994425456 and perplexity of 230.78510460056214
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 94.54143071174622 and batch: 50, loss is 5.6306620216369625 and perplexity is 278.84665906278457
At time: 96.23306655883789 and batch: 100, loss is 5.516266136169434 and perplexity is 248.70467196293274
At time: 97.90480518341064 and batch: 150, loss is 5.5079341506958 and perplexity is 246.64107710831215
At time: 99.58151960372925 and batch: 200, loss is 5.51570219039917 and perplexity is 248.5644555560754
At time: 101.27766227722168 and batch: 250, loss is 5.53335862159729 and perplexity is 252.9921907365771
At time: 103.01320028305054 and batch: 300, loss is 5.500075006484986 and perplexity is 244.71028643429617
At time: 104.72263669967651 and batch: 350, loss is 5.446979579925537 and perplexity is 232.0561991218116
At time: 106.43202447891235 and batch: 400, loss is 5.455766925811767 and perplexity is 234.10434290310178
At time: 108.14042472839355 and batch: 450, loss is 5.46516905784607 and perplexity is 236.3158027726416
At time: 109.84750509262085 and batch: 500, loss is 5.47029375076294 and perplexity is 237.52995712012336
At time: 111.5562653541565 and batch: 550, loss is 5.439018707275391 and perplexity is 230.2161631367558
At time: 113.26542925834656 and batch: 600, loss is 5.444457530975342 and perplexity is 231.47167943189638
At time: 114.97259974479675 and batch: 650, loss is 5.464198198318481 and perplexity is 236.08648465984976
At time: 116.6772723197937 and batch: 700, loss is 5.431586198806762 and perplexity is 228.511422651465
At time: 118.38348531723022 and batch: 750, loss is 5.4451859760284425 and perplexity is 231.64035525978892
At time: 120.08853721618652 and batch: 800, loss is 5.445399570465088 and perplexity is 231.68983763536687
At time: 121.79723978042603 and batch: 850, loss is 5.401109628677368 and perplexity is 221.6522314695722
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.2396189371744795 and perplexity of 188.59822093989905
Finished 4 epochs...
Completing Train Step...
At time: 126.26293277740479 and batch: 50, loss is 5.481890249252319 and perplexity is 240.30050619486116
At time: 127.94462037086487 and batch: 100, loss is 5.378697185516358 and perplexity is 216.73971976002292
At time: 129.62339687347412 and batch: 150, loss is 5.359122953414917 and perplexity is 212.53845855294168
At time: 131.3026430606842 and batch: 200, loss is 5.392517709732056 and perplexity is 219.75597138452798
At time: 132.9893763065338 and batch: 250, loss is 5.414846534729004 and perplexity is 224.71805661479058
At time: 134.6790268421173 and batch: 300, loss is 5.380681819915772 and perplexity is 217.17029599044875
At time: 136.37329983711243 and batch: 350, loss is 5.340736408233642 and perplexity is 208.66631731757076
At time: 138.07793736457825 and batch: 400, loss is 5.356208009719849 and perplexity is 211.91982299517272
At time: 139.79439520835876 and batch: 450, loss is 5.37614538192749 and perplexity is 216.18734763761142
At time: 141.51104068756104 and batch: 500, loss is 5.378927240371704 and perplexity is 216.78958752084043
At time: 143.22636675834656 and batch: 550, loss is 5.3581205749511716 and perplexity is 212.32552131904853
At time: 144.94275307655334 and batch: 600, loss is 5.387068119049072 and perplexity is 218.5616485314395
At time: 146.68796706199646 and batch: 650, loss is 5.408076524734497 and perplexity is 223.20185127922002
At time: 148.40451979637146 and batch: 700, loss is 5.356015939712524 and perplexity is 211.87912346192275
At time: 150.11826634407043 and batch: 750, loss is 5.367825136184693 and perplexity is 214.3960780141096
At time: 151.83285808563232 and batch: 800, loss is 5.3598457717895505 and perplexity is 212.69214079158186
At time: 153.5499939918518 and batch: 850, loss is 5.339761486053467 and perplexity is 208.46298303022107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.272015571594238 and perplexity of 194.80821691244958
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 158.02547335624695 and batch: 50, loss is 5.412586698532104 and perplexity is 224.21080398615342
At time: 159.69591093063354 and batch: 100, loss is 5.3118236637115475 and perplexity is 202.71958384438818
At time: 161.36765789985657 and batch: 150, loss is 5.273823194503784 and perplexity is 195.16067516814562
At time: 163.0482931137085 and batch: 200, loss is 5.270772161483765 and perplexity is 194.5661409369839
At time: 164.72691082954407 and batch: 250, loss is 5.282391786575317 and perplexity is 196.840112312515
At time: 166.41249561309814 and batch: 300, loss is 5.247579050064087 and perplexity is 190.1054750671428
At time: 168.12257409095764 and batch: 350, loss is 5.205436935424805 and perplexity is 182.2604914386573
At time: 169.85044646263123 and batch: 400, loss is 5.238962593078614 and perplexity is 188.47447622509313
At time: 171.57397174835205 and batch: 450, loss is 5.27414797782898 and perplexity is 195.2240703954733
At time: 173.29824423789978 and batch: 500, loss is 5.266850929260254 and perplexity is 193.80469579282428
At time: 175.02493834495544 and batch: 550, loss is 5.230939140319824 and perplexity is 186.96831057338244
At time: 176.75259613990784 and batch: 600, loss is 5.256239814758301 and perplexity is 191.7590842776517
At time: 178.47793841362 and batch: 650, loss is 5.263168601989746 and perplexity is 193.09235581573196
At time: 180.2027826309204 and batch: 700, loss is 5.204268493652344 and perplexity is 182.04765503467834
At time: 181.92797827720642 and batch: 750, loss is 5.199609880447388 and perplexity is 181.20153782166338
At time: 183.65576887130737 and batch: 800, loss is 5.185470466613769 and perplexity is 178.65748240177666
At time: 185.38148546218872 and batch: 850, loss is 5.196924600601196 and perplexity is 180.71561369717
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.133569717407227 and perplexity of 169.62153955845122
Finished 6 epochs...
Completing Train Step...
At time: 189.8806188106537 and batch: 50, loss is 5.2989483833312985 and perplexity is 200.12624318324708
At time: 191.57246446609497 and batch: 100, loss is 5.233189287185669 and perplexity is 187.3894904119699
At time: 193.28844141960144 and batch: 150, loss is 5.202953224182129 and perplexity is 181.80837070806723
At time: 195.01641535758972 and batch: 200, loss is 5.2104387378692625 and perplexity is 183.1744061152338
At time: 196.7440094947815 and batch: 250, loss is 5.221146354675293 and perplexity is 185.14630579735126
At time: 198.47445011138916 and batch: 300, loss is 5.192296562194824 and perplexity is 179.88118726450017
At time: 200.199725151062 and batch: 350, loss is 5.154434795379639 and perplexity is 173.19788684321693
At time: 201.9266655445099 and batch: 400, loss is 5.19364706993103 and perplexity is 180.12428231342312
At time: 203.65332794189453 and batch: 450, loss is 5.225399904251098 and perplexity is 185.93551206191887
At time: 205.38272881507874 and batch: 500, loss is 5.216862173080444 and perplexity is 184.3548020851366
At time: 207.10851979255676 and batch: 550, loss is 5.18952971458435 and perplexity is 179.38417133209384
At time: 208.83461260795593 and batch: 600, loss is 5.2223641967773435 and perplexity is 185.37192211823265
At time: 210.56307888031006 and batch: 650, loss is 5.229460945129395 and perplexity is 186.69213908390145
At time: 212.29269099235535 and batch: 700, loss is 5.176279497146607 and perplexity is 177.0229698196838
At time: 214.0186586380005 and batch: 750, loss is 5.176193542480469 and perplexity is 177.0077545233365
At time: 215.74668097496033 and batch: 800, loss is 5.162879705429077 and perplexity is 174.66672077424164
At time: 217.47443890571594 and batch: 850, loss is 5.174453229904175 and perplexity is 176.69997359722714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.126875559488933 and perplexity of 168.48985823908387
Finished 7 epochs...
Completing Train Step...
At time: 221.95391988754272 and batch: 50, loss is 5.25501787185669 and perplexity is 191.52490872951174
At time: 223.65343356132507 and batch: 100, loss is 5.1909997940063475 and perplexity is 179.64807424259283
At time: 225.33475518226624 and batch: 150, loss is 5.165554847717285 and perplexity is 175.13460465427437
At time: 227.037944316864 and batch: 200, loss is 5.17308274269104 and perplexity is 176.457974409125
At time: 228.75243544578552 and batch: 250, loss is 5.183964433670044 and perplexity is 178.38862085562735
At time: 230.46922874450684 and batch: 300, loss is 5.158704881668091 and perplexity is 173.93903802861576
At time: 232.18655586242676 and batch: 350, loss is 5.122073392868042 and perplexity is 167.682681515627
At time: 233.93180179595947 and batch: 400, loss is 5.163217649459839 and perplexity is 174.72575832503216
At time: 235.64548254013062 and batch: 450, loss is 5.1964346790313725 and perplexity is 180.62709890443935
At time: 237.361243724823 and batch: 500, loss is 5.192881240844726 and perplexity is 179.98639070631899
At time: 239.0771210193634 and batch: 550, loss is 5.164895753860474 and perplexity is 175.01921254352462
At time: 240.793705701828 and batch: 600, loss is 5.194169874191284 and perplexity is 180.21847667605334
At time: 242.50711369514465 and batch: 650, loss is 5.201639633178711 and perplexity is 181.56970565640995
At time: 244.22369647026062 and batch: 700, loss is 5.148899354934692 and perplexity is 172.2418088530656
At time: 245.9405357837677 and batch: 750, loss is 5.153291988372803 and perplexity is 173.0000681403925
At time: 247.65889883041382 and batch: 800, loss is 5.143316726684571 and perplexity is 171.28292589900101
At time: 249.37243723869324 and batch: 850, loss is 5.151550664901733 and perplexity is 172.69908119511373
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.120619773864746 and perplexity of 167.43911185488466
Finished 8 epochs...
Completing Train Step...
At time: 253.82078504562378 and batch: 50, loss is 5.216864643096923 and perplexity is 184.35525744509818
At time: 255.52973413467407 and batch: 100, loss is 5.1570805740356445 and perplexity is 173.65673685560853
At time: 257.2152943611145 and batch: 150, loss is 5.135623025894165 and perplexity is 169.97018271868188
At time: 258.90646862983704 and batch: 200, loss is 5.143216342926025 and perplexity is 171.2657327380961
At time: 260.6167814731598 and batch: 250, loss is 5.155171556472778 and perplexity is 173.32553932657424
At time: 262.3322534561157 and batch: 300, loss is 5.131752004623413 and perplexity is 169.31349636942366
At time: 264.04762268066406 and batch: 350, loss is 5.09785572052002 and perplexity is 163.6705753014256
At time: 265.76329255104065 and batch: 400, loss is 5.138362255096435 and perplexity is 170.436408264493
At time: 267.48010087013245 and batch: 450, loss is 5.170437250137329 and perplexity is 175.99177308973978
At time: 269.19548201560974 and batch: 500, loss is 5.16637001991272 and perplexity is 175.27742771923386
At time: 270.909795999527 and batch: 550, loss is 5.135452575683594 and perplexity is 169.9412137342016
At time: 272.62563610076904 and batch: 600, loss is 5.170325002670288 and perplexity is 175.97201956765252
At time: 274.34237480163574 and batch: 650, loss is 5.174220037460327 and perplexity is 176.65877330254168
At time: 276.08781003952026 and batch: 700, loss is 5.128233299255371 and perplexity is 168.71877899209446
At time: 277.80280351638794 and batch: 750, loss is 5.133861646652222 and perplexity is 169.6710642749343
At time: 279.51946115493774 and batch: 800, loss is 5.122674417495728 and perplexity is 167.78349322898075
At time: 281.2360017299652 and batch: 850, loss is 5.130862598419189 and perplexity is 169.1629748426062
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.120114008585612 and perplexity of 167.35444837746783
Finished 9 epochs...
Completing Train Step...
At time: 285.67507314682007 and batch: 50, loss is 5.188349838256836 and perplexity is 179.17264500679926
At time: 287.3837423324585 and batch: 100, loss is 5.130183925628662 and perplexity is 169.04820748356292
At time: 289.07031989097595 and batch: 150, loss is 5.109496173858642 and perplexity is 165.5869068415115
At time: 290.7758038043976 and batch: 200, loss is 5.116806983947754 and perplexity is 166.80191721407257
At time: 292.48316526412964 and batch: 250, loss is 5.1325531005859375 and perplexity is 169.44918707114604
At time: 294.1918394565582 and batch: 300, loss is 5.1105192756652835 and perplexity is 165.75640579761625
At time: 295.9014608860016 and batch: 350, loss is 5.075483350753784 and perplexity is 160.04953334896317
At time: 297.6112151145935 and batch: 400, loss is 5.1149606609344485 and perplexity is 166.4942311270146
At time: 299.3164713382721 and batch: 450, loss is 5.148900232315063 and perplexity is 172.24195997471406
At time: 301.02319073677063 and batch: 500, loss is 5.14546799659729 and perplexity is 171.65179833387484
At time: 302.7328157424927 and batch: 550, loss is 5.115177011489868 and perplexity is 166.53025614326867
At time: 304.4426302909851 and batch: 600, loss is 5.150410165786743 and perplexity is 172.50223032131248
At time: 306.14769434928894 and batch: 650, loss is 5.154307947158814 and perplexity is 173.1759183927791
At time: 307.855731010437 and batch: 700, loss is 5.108392333984375 and perplexity is 165.40422625469034
At time: 309.56452202796936 and batch: 750, loss is 5.113808851242066 and perplexity is 166.30257185656836
At time: 311.2751889228821 and batch: 800, loss is 5.101474552154541 and perplexity is 164.26394456113007
At time: 312.98347330093384 and batch: 850, loss is 5.111067667007446 and perplexity is 165.84733010423625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.118378639221191 and perplexity of 167.06427844358163
Finished 10 epochs...
Completing Train Step...
At time: 317.43041610717773 and batch: 50, loss is 5.160484857559204 and perplexity is 174.24892103315153
At time: 319.1111636161804 and batch: 100, loss is 5.103412427902222 and perplexity is 164.58257631016806
At time: 320.82264018058777 and batch: 150, loss is 5.0870810317993165 and perplexity is 161.91654234869694
At time: 322.52704310417175 and batch: 200, loss is 5.09242208480835 and perplexity is 162.78366078584435
At time: 324.2376537322998 and batch: 250, loss is 5.109048490524292 and perplexity is 165.5127929339504
At time: 325.94742155075073 and batch: 300, loss is 5.089754819869995 and perplexity is 162.3500521664955
At time: 327.65402364730835 and batch: 350, loss is 5.052745580673218 and perplexity is 156.45142535334307
At time: 329.361022233963 and batch: 400, loss is 5.092464971542358 and perplexity is 162.79064219510926
At time: 331.06777715682983 and batch: 450, loss is 5.128132305145264 and perplexity is 168.70174024957248
At time: 332.77649688720703 and batch: 500, loss is 5.118611907958984 and perplexity is 167.10325386264105
At time: 334.48344802856445 and batch: 550, loss is 5.085515632629394 and perplexity is 161.66327661041427
At time: 336.19303131103516 and batch: 600, loss is 5.125165615081787 and perplexity is 168.20199613256702
At time: 337.9026372432709 and batch: 650, loss is 5.127274484634399 and perplexity is 168.55708648891965
At time: 339.61313343048096 and batch: 700, loss is 5.08265645980835 and perplexity is 161.2017135225216
At time: 341.3207950592041 and batch: 750, loss is 5.08988881111145 and perplexity is 162.3718071089889
At time: 343.0296769142151 and batch: 800, loss is 5.080815525054931 and perplexity is 160.9052246778309
At time: 344.7394688129425 and batch: 850, loss is 5.089055423736572 and perplexity is 162.23654486565977
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.1090240478515625 and perplexity of 165.50874740836213
Finished 11 epochs...
Completing Train Step...
At time: 349.2112717628479 and batch: 50, loss is 5.131442470550537 and perplexity is 169.26109618353937
At time: 350.8904755115509 and batch: 100, loss is 5.076500253677368 and perplexity is 160.21237096833715
At time: 352.58111000061035 and batch: 150, loss is 5.061908988952637 and perplexity is 157.89164220961584
At time: 354.29109382629395 and batch: 200, loss is 5.068130350112915 and perplexity is 158.87700511100158
At time: 356.00676441192627 and batch: 250, loss is 5.082627544403076 and perplexity is 161.19705237703405
At time: 357.7237026691437 and batch: 300, loss is 5.067249622344971 and perplexity is 158.7371393217956
At time: 359.4414539337158 and batch: 350, loss is 5.0310879325866695 and perplexity is 153.09948404208552
At time: 361.1585123538971 and batch: 400, loss is 5.0754689025878905 and perplexity is 160.04722094345917
At time: 362.8993821144104 and batch: 450, loss is 5.111171398162842 and perplexity is 165.86453453170944
At time: 364.6150908470154 and batch: 500, loss is 5.10459186553955 and perplexity is 164.77680571333536
At time: 366.3318712711334 and batch: 550, loss is 5.072806415557861 and perplexity is 159.6216640647319
At time: 368.04895997047424 and batch: 600, loss is 5.109486799240113 and perplexity is 165.58535453470256
At time: 369.7639493942261 and batch: 650, loss is 5.108991355895996 and perplexity is 165.5033366921899
At time: 371.4804542064667 and batch: 700, loss is 5.0667039489746095 and perplexity is 158.65054432042226
At time: 373.1969656944275 and batch: 750, loss is 5.0748837947845455 and perplexity is 159.9536034564134
At time: 374.91522550582886 and batch: 800, loss is 5.064222650527954 and perplexity is 158.2573729607055
At time: 376.630215883255 and batch: 850, loss is 5.074204273223877 and perplexity is 159.8449484550472
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.110917727152507 and perplexity of 165.82246484381173
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 381.07641077041626 and batch: 50, loss is 5.111596298217774 and perplexity is 165.93502535626024
At time: 382.75507259368896 and batch: 100, loss is 5.049872779846192 and perplexity is 156.0026165470212
At time: 384.4588532447815 and batch: 150, loss is 5.032228307723999 and perplexity is 153.27417447460667
At time: 386.1702673435211 and batch: 200, loss is 5.02907096862793 and perplexity is 152.79099910674927
At time: 387.88233375549316 and batch: 250, loss is 5.0412044334411625 and perplexity is 154.6561759643701
At time: 389.5938711166382 and batch: 300, loss is 5.013074340820313 and perplexity is 150.36630354274314
At time: 391.3035876750946 and batch: 350, loss is 4.973410005569458 and perplexity is 144.51885819390404
At time: 393.01409697532654 and batch: 400, loss is 5.016091165542602 and perplexity is 150.8206172725482
At time: 394.7258839607239 and batch: 450, loss is 5.0545358562469485 and perplexity is 156.7317673885007
At time: 396.4370765686035 and batch: 500, loss is 5.035761528015136 and perplexity is 153.81668373595852
At time: 398.14612555503845 and batch: 550, loss is 5.003474426269531 and perplexity is 148.92970651462468
At time: 399.85663986206055 and batch: 600, loss is 5.037850513458252 and perplexity is 154.13834040026384
At time: 401.5671896934509 and batch: 650, loss is 5.029823932647705 and perplexity is 152.90608855529155
At time: 403.27868008613586 and batch: 700, loss is 4.9896229648590085 and perplexity is 146.88103374314304
At time: 404.98702239990234 and batch: 750, loss is 4.993905153274536 and perplexity is 147.51135461748197
At time: 406.7417163848877 and batch: 800, loss is 4.975870866775512 and perplexity is 144.87493699613574
At time: 408.45284938812256 and batch: 850, loss is 5.0025887775421145 and perplexity is 148.7978655006063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.063321113586426 and perplexity of 158.11476238672168
Finished 13 epochs...
Completing Train Step...
At time: 412.87664699554443 and batch: 50, loss is 5.068001842498779 and perplexity is 158.8565895179414
At time: 414.5910234451294 and batch: 100, loss is 5.0139905834198 and perplexity is 150.50413869115104
At time: 416.2898437976837 and batch: 150, loss is 4.998335876464844 and perplexity is 148.1663866584522
At time: 417.992595911026 and batch: 200, loss is 5.002295055389404 and perplexity is 148.7541666892014
At time: 419.70087337493896 and batch: 250, loss is 5.017311468124389 and perplexity is 151.00477640327077
At time: 421.4105124473572 and batch: 300, loss is 4.992938737869263 and perplexity is 147.368866234501
At time: 423.121160030365 and batch: 350, loss is 4.951374702453613 and perplexity is 141.3691709486223
At time: 424.82980966567993 and batch: 400, loss is 4.996719970703125 and perplexity is 147.92715707884116
At time: 426.53771710395813 and batch: 450, loss is 5.037407598495483 and perplexity is 154.07008533967394
At time: 428.2464485168457 and batch: 500, loss is 5.019692440032959 and perplexity is 151.36474289894886
At time: 429.95646238327026 and batch: 550, loss is 4.991625013351441 and perplexity is 147.17539125600257
At time: 431.6656081676483 and batch: 600, loss is 5.029608535766601 and perplexity is 152.8731566075617
At time: 433.37335181236267 and batch: 650, loss is 5.02265965461731 and perplexity is 151.8145415662021
At time: 435.0820186138153 and batch: 700, loss is 4.985713424682618 and perplexity is 146.3079174810835
At time: 436.79214429855347 and batch: 750, loss is 4.992492132186889 and perplexity is 147.30306515609718
At time: 438.5021779537201 and batch: 800, loss is 4.975399084091187 and perplexity is 144.80660362998515
At time: 440.2099783420563 and batch: 850, loss is 5.000253677368164 and perplexity is 148.45081293793967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.061827977498372 and perplexity of 157.8788516961602
Finished 14 epochs...
Completing Train Step...
At time: 444.66598105430603 and batch: 50, loss is 5.052984046936035 and perplexity is 156.4887381888089
At time: 446.3839907646179 and batch: 100, loss is 5.000680894851684 and perplexity is 148.51424726983257
At time: 448.0873622894287 and batch: 150, loss is 4.9859695148468015 and perplexity is 146.34539029769775
At time: 449.83202171325684 and batch: 200, loss is 4.991309537887573 and perplexity is 147.1289683542044
At time: 451.54874897003174 and batch: 250, loss is 5.005639629364014 and perplexity is 149.25251892713288
At time: 453.2641763687134 and batch: 300, loss is 4.981240911483765 and perplexity is 145.65501453592736
At time: 454.979291677475 and batch: 350, loss is 4.940839862823486 and perplexity is 139.88768670508082
At time: 456.6954970359802 and batch: 400, loss is 4.986087808609009 and perplexity is 146.36270306847393
At time: 458.41292428970337 and batch: 450, loss is 5.02816216468811 and perplexity is 152.65220512260467
At time: 460.12665343284607 and batch: 500, loss is 5.012114458084106 and perplexity is 150.22203877357273
At time: 461.84209275245667 and batch: 550, loss is 4.9838075351715085 and perplexity is 146.02933631255453
At time: 463.55878734588623 and batch: 600, loss is 5.0235788440704345 and perplexity is 151.9541520460512
At time: 465.2754671573639 and batch: 650, loss is 5.016970653533935 and perplexity is 150.9533205411966
At time: 466.9914743900299 and batch: 700, loss is 4.980426797866821 and perplexity is 145.53648306080822
At time: 468.7068884372711 and batch: 750, loss is 4.987990255355835 and perplexity is 146.64141535035824
At time: 470.42333912849426 and batch: 800, loss is 4.971080236434936 and perplexity is 144.18255452579686
At time: 472.14121103286743 and batch: 850, loss is 4.995112133026123 and perplexity is 147.68950532615602
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.060820579528809 and perplexity of 157.71988494635892
Finished 15 epochs...
Completing Train Step...
At time: 476.59267711639404 and batch: 50, loss is 5.040779685974121 and perplexity is 154.590500094184
At time: 478.30507707595825 and batch: 100, loss is 4.9899831199646 and perplexity is 146.93394322459818
At time: 479.9971513748169 and batch: 150, loss is 4.976452827453613 and perplexity is 144.9592730504633
At time: 481.70715165138245 and batch: 200, loss is 4.981335048675537 and perplexity is 145.66872673536722
At time: 483.4229187965393 and batch: 250, loss is 4.9961409091949465 and perplexity is 147.84152295226866
At time: 485.13817858695984 and batch: 300, loss is 4.971661396026612 and perplexity is 144.26637195360374
At time: 486.85620307922363 and batch: 350, loss is 4.931238851547241 and perplexity is 138.5510502459906
At time: 488.57029604911804 and batch: 400, loss is 4.977543325424194 and perplexity is 145.11743706663717
At time: 490.28667163848877 and batch: 450, loss is 5.020492095947265 and perplexity is 151.48583101878864
At time: 492.0026264190674 and batch: 500, loss is 5.005082950592041 and perplexity is 149.1694563399173
At time: 493.7484276294708 and batch: 550, loss is 4.977102537155151 and perplexity is 145.05348509841872
At time: 495.47828006744385 and batch: 600, loss is 5.017406930923462 and perplexity is 151.01919242998576
At time: 497.2132399082184 and batch: 650, loss is 5.010566539764405 and perplexity is 149.98968720475105
At time: 498.94846844673157 and batch: 700, loss is 4.975789413452149 and perplexity is 144.86313693162907
At time: 500.6850242614746 and batch: 750, loss is 4.983130741119385 and perplexity is 145.9305379631414
At time: 502.41885900497437 and batch: 800, loss is 4.9667222213745115 and perplexity is 143.55557197374463
At time: 504.15380811691284 and batch: 850, loss is 4.990010442733765 and perplexity is 146.93795792165753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.061296780904134 and perplexity of 157.7950092582172
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 508.6539731025696 and batch: 50, loss is 5.033406753540039 and perplexity is 153.45490625461247
At time: 510.32355523109436 and batch: 100, loss is 4.983583288192749 and perplexity is 145.996593346469
At time: 512.0224609375 and batch: 150, loss is 4.96432469367981 and perplexity is 143.2118057733698
At time: 513.7316777706146 and batch: 200, loss is 4.966699886322021 and perplexity is 143.55236568831577
At time: 515.441020488739 and batch: 250, loss is 4.977771263122559 and perplexity is 145.15051857135253
At time: 517.1488008499146 and batch: 300, loss is 4.94833888053894 and perplexity is 140.94065010698992
At time: 518.8571355342865 and batch: 350, loss is 4.906156692504883 and perplexity is 135.11911091404642
At time: 520.5664591789246 and batch: 400, loss is 4.948655118942261 and perplexity is 140.98522800139233
At time: 522.275933265686 and batch: 450, loss is 4.99469027519226 and perplexity is 147.627214491217
At time: 523.9820363521576 and batch: 500, loss is 4.976871585845947 and perplexity is 145.01998867430103
At time: 525.6893050670624 and batch: 550, loss is 4.9444072055816655 and perplexity is 140.3876051913418
At time: 527.3979403972626 and batch: 600, loss is 4.977966203689575 and perplexity is 145.178817053917
At time: 529.1079287528992 and batch: 650, loss is 4.9687832069396975 and perplexity is 143.851743032687
At time: 530.8141632080078 and batch: 700, loss is 4.934549827575683 and perplexity is 139.01054972818162
At time: 532.5246274471283 and batch: 750, loss is 4.937379808425903 and perplexity is 139.40450410063102
At time: 534.2336535453796 and batch: 800, loss is 4.918715515136719 and perplexity is 136.82674837562473
At time: 535.9897258281708 and batch: 850, loss is 4.954106063842773 and perplexity is 141.75582905489688
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.047087669372559 and perplexity of 155.5687365081437
Finished 17 epochs...
Completing Train Step...
At time: 540.4341406822205 and batch: 50, loss is 5.014605646133423 and perplexity is 150.5967366489623
At time: 542.1198880672455 and batch: 100, loss is 4.967105722427368 and perplexity is 143.61063624466803
At time: 543.8245258331299 and batch: 150, loss is 4.9489101314544675 and perplexity is 141.02118558319074
At time: 545.5312936306 and batch: 200, loss is 4.95422854423523 and perplexity is 141.7731924277872
At time: 547.2399044036865 and batch: 250, loss is 4.9666978931427 and perplexity is 143.55207956299412
At time: 548.9491112232208 and batch: 300, loss is 4.938591547012329 and perplexity is 139.5735283032639
At time: 550.6582343578339 and batch: 350, loss is 4.896402244567871 and perplexity is 133.80750597223562
At time: 552.3660106658936 and batch: 400, loss is 4.940119457244873 and perplexity is 139.7869471262334
At time: 554.07399725914 and batch: 450, loss is 4.987779130935669 and perplexity is 146.61045903450218
At time: 555.7828769683838 and batch: 500, loss is 4.970816640853882 and perplexity is 144.1445536502102
At time: 557.4921934604645 and batch: 550, loss is 4.94063479423523 and perplexity is 139.85900307581
At time: 559.1988890171051 and batch: 600, loss is 4.976239347457886 and perplexity is 144.92833044840268
At time: 560.9068248271942 and batch: 650, loss is 4.9673004245758055 and perplexity is 143.63860026632258
At time: 562.6164443492889 and batch: 700, loss is 4.9343240928649905 and perplexity is 138.9791737634109
At time: 564.327347278595 and batch: 750, loss is 4.938536109924317 and perplexity is 139.5657909677608
At time: 566.0345213413239 and batch: 800, loss is 4.919994487762451 and perplexity is 137.00185799760163
At time: 567.7433216571808 and batch: 850, loss is 4.954912843704224 and perplexity is 141.87024094942203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.046590805053711 and perplexity of 155.49145915361447
Finished 18 epochs...
Completing Train Step...
At time: 572.1903626918793 and batch: 50, loss is 5.006975307464599 and perplexity is 149.45200544334904
At time: 573.872168302536 and batch: 100, loss is 4.960336036682129 and perplexity is 142.64172069484857
At time: 575.586324930191 and batch: 150, loss is 4.941792840957642 and perplexity is 140.02106015264422
At time: 577.3044676780701 and batch: 200, loss is 4.947916536331177 and perplexity is 140.88113720812757
At time: 579.0194842815399 and batch: 250, loss is 4.961461582183838 and perplexity is 142.8023608288638
At time: 580.7640988826752 and batch: 300, loss is 4.932616319656372 and perplexity is 138.74203140422088
At time: 582.4810717105865 and batch: 350, loss is 4.890994644165039 and perplexity is 133.08588133636067
At time: 584.2121102809906 and batch: 400, loss is 4.935631017684937 and perplexity is 139.16092783864352
At time: 585.9462621212006 and batch: 450, loss is 4.984379568099976 and perplexity is 146.11289379798555
At time: 587.6808547973633 and batch: 500, loss is 4.967867641448975 and perplexity is 143.7200976151827
At time: 589.4145348072052 and batch: 550, loss is 4.938592777252198 and perplexity is 139.57370001228858
At time: 591.1557765007019 and batch: 600, loss is 4.9743164443969725 and perplexity is 144.64991508683386
At time: 592.8902027606964 and batch: 650, loss is 4.965471229553223 and perplexity is 143.3760974113161
At time: 594.6232509613037 and batch: 700, loss is 4.932976694107055 and perplexity is 138.79203949785332
At time: 596.3583009243011 and batch: 750, loss is 4.9374105262756345 and perplexity is 139.40878637301063
At time: 598.0945863723755 and batch: 800, loss is 4.918875722885132 and perplexity is 136.8486708369319
At time: 599.8303098678589 and batch: 850, loss is 4.953863182067871 and perplexity is 141.72140332839334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.046172142028809 and perplexity of 155.42637425424982
Finished 19 epochs...
Completing Train Step...
At time: 604.312668800354 and batch: 50, loss is 5.0011864757537845 and perplexity is 148.58935222115667
At time: 606.0417623519897 and batch: 100, loss is 4.95555998802185 and perplexity is 141.96208118343029
At time: 607.7587926387787 and batch: 150, loss is 4.936760139465332 and perplexity is 139.3181462158898
At time: 609.487152338028 and batch: 200, loss is 4.9433731174469 and perplexity is 140.2425070695982
At time: 611.214681148529 and batch: 250, loss is 4.9570697402954105 and perplexity is 142.1765706304801
At time: 612.9434494972229 and batch: 300, loss is 4.927415952682495 and perplexity is 138.02239473541013
At time: 614.667646408081 and batch: 350, loss is 4.886371746063232 and perplexity is 132.47205878089278
At time: 616.39373087883 and batch: 400, loss is 4.931806983947754 and perplexity is 138.6297879513634
At time: 618.1203782558441 and batch: 450, loss is 4.981238126754761 and perplexity is 145.6546089267486
At time: 619.8478949069977 and batch: 500, loss is 4.965521669387817 and perplexity is 143.38332946034444
At time: 621.572603225708 and batch: 550, loss is 4.936186666488648 and perplexity is 139.23827392836142
At time: 623.3336012363434 and batch: 600, loss is 4.972294263839721 and perplexity is 144.357702393927
At time: 625.0610530376434 and batch: 650, loss is 4.963204717636108 and perplexity is 143.05150176680826
At time: 626.7888724803925 and batch: 700, loss is 4.930880613327027 and perplexity is 138.50142485372845
At time: 628.5138738155365 and batch: 750, loss is 4.935915279388428 and perplexity is 139.2004915840127
At time: 630.2407541275024 and batch: 800, loss is 4.917029333114624 and perplexity is 136.59622797666646
At time: 631.9667489528656 and batch: 850, loss is 4.952068157196045 and perplexity is 141.46723806927542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.046148618062337 and perplexity of 155.4227180524374
Finished 20 epochs...
Completing Train Step...
At time: 636.4161002635956 and batch: 50, loss is 4.996406993865967 and perplexity is 147.88086654939846
At time: 638.1072609424591 and batch: 100, loss is 4.95152871131897 and perplexity is 141.39094473087124
At time: 639.8041741847992 and batch: 150, loss is 4.932553730010986 and perplexity is 138.73334786142797
At time: 641.5142760276794 and batch: 200, loss is 4.939159336090088 and perplexity is 139.6527991306041
At time: 643.2214357852936 and batch: 250, loss is 4.953023185729981 and perplexity is 141.60240785347523
At time: 644.9298148155212 and batch: 300, loss is 4.923362627029419 and perplexity is 137.46407730770989
At time: 646.639454126358 and batch: 350, loss is 4.882454738616944 and perplexity is 131.9541796705378
At time: 648.3500394821167 and batch: 400, loss is 4.928577699661255 and perplexity is 138.18283501295406
At time: 650.0581154823303 and batch: 450, loss is 4.978209705352783 and perplexity is 145.21417264172973
At time: 651.7661254405975 and batch: 500, loss is 4.962814474105835 and perplexity is 142.9956877349887
At time: 653.4751296043396 and batch: 550, loss is 4.933637838363648 and perplexity is 138.8838313981283
At time: 655.1862561702728 and batch: 600, loss is 4.9697582817077635 and perplexity is 143.9920776449114
At time: 656.8974924087524 and batch: 650, loss is 4.960815572738648 and perplexity is 142.71013894628376
At time: 658.6256082057953 and batch: 700, loss is 4.928761558532715 and perplexity is 138.20824348876997
At time: 660.3551499843597 and batch: 750, loss is 4.933658027648926 and perplexity is 138.88663539172617
At time: 662.0849924087524 and batch: 800, loss is 4.914489984512329 and perplexity is 136.24980256966597
At time: 663.8062582015991 and batch: 850, loss is 4.950149908065796 and perplexity is 141.19612877360845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.045767784118652 and perplexity of 155.36353907518023
Finished 21 epochs...
Completing Train Step...
At time: 668.2536053657532 and batch: 50, loss is 4.992177743911743 and perplexity is 147.25676207847266
At time: 669.9824883937836 and batch: 100, loss is 4.947348031997681 and perplexity is 140.8010684329962
At time: 671.6964945793152 and batch: 150, loss is 4.928666868209839 and perplexity is 138.19515712515565
At time: 673.4129841327667 and batch: 200, loss is 4.935253219604492 and perplexity is 139.10836303729891
At time: 675.1302442550659 and batch: 250, loss is 4.949585437774658 and perplexity is 141.11645024388923
At time: 676.8465542793274 and batch: 300, loss is 4.919533758163452 and perplexity is 136.9387517250845
At time: 678.5615994930267 and batch: 350, loss is 4.8787862491607665 and perplexity is 131.47099397641523
At time: 680.277510881424 and batch: 400, loss is 4.925332746505737 and perplexity is 137.73516491351134
At time: 681.994752407074 and batch: 450, loss is 4.975449905395508 and perplexity is 144.81396307746274
At time: 683.7122087478638 and batch: 500, loss is 4.960342893600464 and perplexity is 142.64269878083178
At time: 685.4263529777527 and batch: 550, loss is 4.9312764835357665 and perplexity is 138.55626429563057
At time: 687.1423437595367 and batch: 600, loss is 4.9674970817565915 and perplexity is 143.66685060622805
At time: 688.8755152225494 and batch: 650, loss is 4.9581891059875485 and perplexity is 142.33580731128168
At time: 690.6112279891968 and batch: 700, loss is 4.926649751663208 and perplexity is 137.91668233952382
At time: 692.3438258171082 and batch: 750, loss is 4.931739292144775 and perplexity is 138.62040416867654
At time: 694.0787343978882 and batch: 800, loss is 4.912495164871216 and perplexity is 135.97827969702126
At time: 695.8129887580872 and batch: 850, loss is 4.948265552520752 and perplexity is 140.93031558734498
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.04576047261556 and perplexity of 155.36240313833648
Finished 22 epochs...
Completing Train Step...
At time: 700.3050944805145 and batch: 50, loss is 4.988523607254028 and perplexity is 146.71964768841755
At time: 701.9974188804626 and batch: 100, loss is 4.94399998664856 and perplexity is 140.3304483390006
At time: 703.7103779315948 and batch: 150, loss is 4.924895315170288 and perplexity is 137.6749284120121
At time: 705.4260368347168 and batch: 200, loss is 4.931947813034058 and perplexity is 138.6493124325102
At time: 707.1413559913635 and batch: 250, loss is 4.946057081222534 and perplexity is 140.61941846037507
At time: 708.8580000400543 and batch: 300, loss is 4.9158977603912355 and perplexity is 136.44174683077964
At time: 710.6036837100983 and batch: 350, loss is 4.8751699829101565 and perplexity is 130.9964184708399
At time: 712.3196864128113 and batch: 400, loss is 4.921761674880981 and perplexity is 137.2441799673382
At time: 714.0343589782715 and batch: 450, loss is 4.9722195243835445 and perplexity is 144.34691358093517
At time: 715.7502670288086 and batch: 500, loss is 4.957157325744629 and perplexity is 142.18902377463627
At time: 717.4670534133911 and batch: 550, loss is 4.928038511276245 and perplexity is 138.1083485162389
At time: 719.1830506324768 and batch: 600, loss is 4.964213094711304 and perplexity is 143.19582437533793
At time: 720.897248506546 and batch: 650, loss is 4.954727401733399 and perplexity is 141.84393469155643
At time: 722.6135220527649 and batch: 700, loss is 4.923630123138428 and perplexity is 137.50085333202063
At time: 724.3309347629547 and batch: 750, loss is 4.928584327697754 and perplexity is 138.18375089686336
At time: 726.048258304596 and batch: 800, loss is 4.9097236633300785 and perplexity is 135.6019374427459
At time: 727.7637858390808 and batch: 850, loss is 4.946031322479248 and perplexity is 140.61579632752495
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.045608202616374 and perplexity of 155.3387479063739
Finished 23 epochs...
Completing Train Step...
At time: 732.2545330524445 and batch: 50, loss is 4.984281826019287 and perplexity is 146.09861311765337
At time: 733.935928106308 and batch: 100, loss is 4.94019040107727 and perplexity is 139.79686449976572
At time: 735.6375102996826 and batch: 150, loss is 4.919799461364746 and perplexity is 136.97514162404119
At time: 737.3460223674774 and batch: 200, loss is 4.927804594039917 and perplexity is 138.07604637120164
At time: 739.0558078289032 and batch: 250, loss is 4.942675075531006 and perplexity is 140.1446460807833
At time: 740.7628009319305 and batch: 300, loss is 4.912217464447021 and perplexity is 135.94052371373706
At time: 742.4713294506073 and batch: 350, loss is 4.871536626815796 and perplexity is 130.521325449165
At time: 744.1818840503693 and batch: 400, loss is 4.918609418869019 and perplexity is 136.81223233836283
At time: 745.8921029567719 and batch: 450, loss is 4.969344787597656 and perplexity is 143.9325500769006
At time: 747.5984854698181 and batch: 500, loss is 4.954510297775268 and perplexity is 141.81314315450135
At time: 749.305558681488 and batch: 550, loss is 4.925549545288086 and perplexity is 137.7650289666794
At time: 751.0142667293549 and batch: 600, loss is 4.96206072807312 and perplexity is 142.8879459127496
At time: 752.7232000827789 and batch: 650, loss is 4.951979951858521 and perplexity is 141.45476045409706
At time: 754.4598824977875 and batch: 700, loss is 4.920956354141236 and perplexity is 137.13369887512954
At time: 756.1679570674896 and batch: 750, loss is 4.9259420680999755 and perplexity is 137.81911549763362
At time: 757.8774974346161 and batch: 800, loss is 4.9070845890045165 and perplexity is 135.24454565032366
At time: 759.588410615921 and batch: 850, loss is 4.943473463058472 and perplexity is 140.25658049582995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.045185724894206 and perplexity of 155.27313460704855
Finished 24 epochs...
Completing Train Step...
At time: 764.0472946166992 and batch: 50, loss is 4.980259160995484 and perplexity is 145.51208782495013
At time: 765.7216022014618 and batch: 100, loss is 4.936655511856079 and perplexity is 139.30357045385256
At time: 767.4238014221191 and batch: 150, loss is 4.9164357757568355 and perplexity is 136.51517433787623
At time: 769.1315984725952 and batch: 200, loss is 4.924356489181519 and perplexity is 137.60076556481079
At time: 770.8397674560547 and batch: 250, loss is 4.939505014419556 and perplexity is 139.70108242167595
At time: 772.5492970943451 and batch: 300, loss is 4.908824005126953 and perplexity is 135.4799969079918
At time: 774.260285615921 and batch: 350, loss is 4.868244047164917 and perplexity is 130.09228030924325
At time: 775.9686965942383 and batch: 400, loss is 4.915239486694336 and perplexity is 136.35196037295592
At time: 777.6764280796051 and batch: 450, loss is 4.966443557739257 and perplexity is 143.51557382947044
At time: 779.3854532241821 and batch: 500, loss is 4.952196073532105 and perplexity is 141.4853351974762
At time: 781.0957703590393 and batch: 550, loss is 4.923197259902954 and perplexity is 137.44134714771587
At time: 782.8040516376495 and batch: 600, loss is 4.959913997650147 and perplexity is 142.581533022793
At time: 784.5133898258209 and batch: 650, loss is 4.949377584457397 and perplexity is 141.08712176970187
At time: 786.2232568264008 and batch: 700, loss is 4.9183047580718995 and perplexity is 136.7705573632909
At time: 787.9333391189575 and batch: 750, loss is 4.923540582656861 and perplexity is 137.48854199058735
At time: 789.642322063446 and batch: 800, loss is 4.904733505249023 and perplexity is 134.9269478917875
At time: 791.3513576984406 and batch: 850, loss is 4.941164503097534 and perplexity is 139.93310725429956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.045173009236653 and perplexity of 155.2711602195946
Finished 25 epochs...
Completing Train Step...
At time: 795.8568248748779 and batch: 50, loss is 4.976654958724976 and perplexity is 144.9885768141245
At time: 797.568727016449 and batch: 100, loss is 4.933393859863282 and perplexity is 138.84995086244788
At time: 799.2812125682831 and batch: 150, loss is 4.913287744522095 and perplexity is 136.08609603534453
At time: 800.9983322620392 and batch: 200, loss is 4.921501998901367 and perplexity is 137.20854557735663
At time: 802.7155261039734 and batch: 250, loss is 4.936455726623535 and perplexity is 139.2757424375411
At time: 804.4320251941681 and batch: 300, loss is 4.905645895004272 and perplexity is 135.05011003414555
At time: 806.1489069461823 and batch: 350, loss is 4.8651688861846925 and perplexity is 129.69284009127327
At time: 807.8662850856781 and batch: 400, loss is 4.912346811294555 and perplexity is 135.95810832916433
At time: 809.5844371318817 and batch: 450, loss is 4.963878965377807 and perplexity is 143.1479864424533
At time: 811.2990334033966 and batch: 500, loss is 4.949562950134277 and perplexity is 141.1132769035849
At time: 813.0154716968536 and batch: 550, loss is 4.9208261680603025 and perplexity is 137.11584713835813
At time: 814.7327678203583 and batch: 600, loss is 4.957477550506592 and perplexity is 142.2345635120149
At time: 816.4515285491943 and batch: 650, loss is 4.946951856613159 and perplexity is 140.74529756382097
At time: 818.1665852069855 and batch: 700, loss is 4.9159172248840335 and perplexity is 136.44440262602492
At time: 819.8822228908539 and batch: 750, loss is 4.9212996196746825 and perplexity is 137.18078022766792
At time: 821.600664138794 and batch: 800, loss is 4.9023317623138425 and perplexity is 134.6032768907807
At time: 823.322117805481 and batch: 850, loss is 4.938845539093018 and perplexity is 139.60898337659
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.044909159342448 and perplexity of 155.2301973446603
Finished 26 epochs...
Completing Train Step...
At time: 827.7785272598267 and batch: 50, loss is 4.973387060165405 and perplexity is 144.51554218835315
At time: 829.4942569732666 and batch: 100, loss is 4.930263223648072 and perplexity is 138.4159418943669
At time: 831.2052845954895 and batch: 150, loss is 4.910072383880615 and perplexity is 135.64923287098932
At time: 832.9146161079407 and batch: 200, loss is 4.918302421569824 and perplexity is 136.77023779897308
At time: 834.6256303787231 and batch: 250, loss is 4.933394289016723 and perplexity is 138.85001045039496
At time: 836.3369805812836 and batch: 300, loss is 4.902183294296265 and perplexity is 134.58329409254117
At time: 838.0482420921326 and batch: 350, loss is 4.862079629898071 and perplexity is 129.29280389500417
At time: 839.800244808197 and batch: 400, loss is 4.909217615127563 and perplexity is 135.53333368591333
At time: 841.5099215507507 and batch: 450, loss is 4.961088466644287 and perplexity is 142.74908898784787
At time: 843.2212359905243 and batch: 500, loss is 4.947124452590942 and perplexity is 140.7695917325499
At time: 844.932831287384 and batch: 550, loss is 4.918543710708618 and perplexity is 136.80324295359674
At time: 846.6420021057129 and batch: 600, loss is 4.955026426315308 and perplexity is 141.88635585699342
At time: 848.35218334198 and batch: 650, loss is 4.944417295455932 and perplexity is 140.38902169177294
At time: 850.0634298324585 and batch: 700, loss is 4.913290863037109 and perplexity is 136.08652042253996
At time: 851.7757253646851 and batch: 750, loss is 4.918861837387085 and perplexity is 136.84677063817284
At time: 853.4845309257507 and batch: 800, loss is 4.900041704177856 and perplexity is 134.29538024657435
At time: 855.1956870555878 and batch: 850, loss is 4.936609048843383 and perplexity is 139.2970981406527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.044802029927571 and perplexity of 155.21356851518183
Finished 27 epochs...
Completing Train Step...
At time: 859.6307835578918 and batch: 50, loss is 4.970180759429931 and perplexity is 144.05292394208254
At time: 861.3407602310181 and batch: 100, loss is 4.927182397842407 and perplexity is 137.9901627011994
At time: 863.046751499176 and batch: 150, loss is 4.90711817741394 and perplexity is 135.2490883757863
At time: 864.7576916217804 and batch: 200, loss is 4.915622692108155 and perplexity is 136.40422119503106
At time: 866.4662983417511 and batch: 250, loss is 4.930443315505982 and perplexity is 138.44087172326925
At time: 868.174896478653 and batch: 300, loss is 4.898881406784057 and perplexity is 134.13964803218798
At time: 869.8838891983032 and batch: 350, loss is 4.859191389083862 and perplexity is 128.91991389908245
At time: 871.5941917896271 and batch: 400, loss is 4.906903219223023 and perplexity is 135.22001860093084
At time: 873.3034014701843 and batch: 450, loss is 4.958687772750855 and perplexity is 142.40680314777697
At time: 875.0100286006927 and batch: 500, loss is 4.944837427139282 and perplexity is 140.44801595959288
At time: 876.7187991142273 and batch: 550, loss is 4.916223049163818 and perplexity is 136.4861370185587
At time: 878.4287614822388 and batch: 600, loss is 4.952721271514893 and perplexity is 141.55966252668918
At time: 880.1387114524841 and batch: 650, loss is 4.941944599151611 and perplexity is 140.04231110831324
At time: 881.8461284637451 and batch: 700, loss is 4.910921440124512 and perplexity is 135.7644556075083
At time: 883.5829622745514 and batch: 750, loss is 4.916775712966919 and perplexity is 136.56158881393148
At time: 885.2942464351654 and batch: 800, loss is 4.898107738494873 and perplexity is 134.0359085753217
At time: 887.0051715373993 and batch: 850, loss is 4.934494934082031 and perplexity is 139.00291916288867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.044954299926758 and perplexity of 155.23720468462744
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 891.4713230133057 and batch: 50, loss is 4.968433408737183 and perplexity is 143.80143275128157
At time: 893.1755068302155 and batch: 100, loss is 4.9253136157989506 and perplexity is 137.73252996766143
At time: 894.8841450214386 and batch: 150, loss is 4.904252405166626 and perplexity is 134.86205013844818
At time: 896.5999677181244 and batch: 200, loss is 4.911879653930664 and perplexity is 135.8946093308448
At time: 898.3162665367126 and batch: 250, loss is 4.92331374168396 and perplexity is 137.4573574930541
At time: 900.0339789390564 and batch: 300, loss is 4.889003601074219 and perplexity is 132.82116522987775
At time: 901.74893450737 and batch: 350, loss is 4.8490041065216065 and perplexity is 127.61323735053347
At time: 903.463259935379 and batch: 400, loss is 4.894709615707398 and perplexity is 133.58121109652564
At time: 905.1791064739227 and batch: 450, loss is 4.9460813999176025 and perplexity is 140.62283818271473
At time: 906.8952317237854 and batch: 500, loss is 4.931170730590821 and perplexity is 138.5416123373981
At time: 908.6109547615051 and batch: 550, loss is 4.899357652664184 and perplexity is 134.20354670145622
At time: 910.3254771232605 and batch: 600, loss is 4.931887426376343 and perplexity is 138.64094011672853
At time: 912.0418612957001 and batch: 650, loss is 4.919866075515747 and perplexity is 136.98426641072538
At time: 913.7594614028931 and batch: 700, loss is 4.888859548568726 and perplexity is 132.80203338627197
At time: 915.4753980636597 and batch: 750, loss is 4.893736953735352 and perplexity is 133.4513449005059
At time: 917.1914989948273 and batch: 800, loss is 4.873175249099732 and perplexity is 130.73537592762156
At time: 918.9087467193604 and batch: 850, loss is 4.915546236038208 and perplexity is 136.39379266302166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.0398054122924805 and perplexity of 154.43995998405214
Finished 29 epochs...
Completing Train Step...
At time: 923.3944618701935 and batch: 50, loss is 4.960176153182983 and perplexity is 142.61891646047866
At time: 925.0880739688873 and batch: 100, loss is 4.91751708984375 and perplexity is 136.66286995725136
At time: 926.8302073478699 and batch: 150, loss is 4.896840867996215 and perplexity is 133.8662099527728
At time: 928.5468866825104 and batch: 200, loss is 4.905570240020752 and perplexity is 135.0398932067783
At time: 930.2614979743958 and batch: 250, loss is 4.9177132415771485 and perplexity is 136.68967924534283
At time: 931.9773921966553 and batch: 300, loss is 4.884596796035766 and perplexity is 132.23713604625434
At time: 933.6928503513336 and batch: 350, loss is 4.845235862731934 and perplexity is 127.13326445679951
At time: 935.4099383354187 and batch: 400, loss is 4.891347761154175 and perplexity is 133.1328845203977
At time: 937.125216960907 and batch: 450, loss is 4.943245553970337 and perplexity is 140.22461838882998
At time: 938.8396084308624 and batch: 500, loss is 4.928955793380737 and perplexity is 138.23509095321162
At time: 940.5553367137909 and batch: 550, loss is 4.89787787437439 and perplexity is 134.0051020698747
At time: 942.2724294662476 and batch: 600, loss is 4.9319315528869625 and perplexity is 138.64705799262418
At time: 943.9873929023743 and batch: 650, loss is 4.920514669418335 and perplexity is 137.0731423897632
At time: 945.7021193504333 and batch: 700, loss is 4.889986238479614 and perplexity is 132.9517444205524
At time: 947.4178004264832 and batch: 750, loss is 4.894972486495972 and perplexity is 133.61633031053074
At time: 949.1359043121338 and batch: 800, loss is 4.874308786392212 and perplexity is 130.88365337481306
At time: 950.8516516685486 and batch: 850, loss is 4.916063623428345 and perplexity is 136.4643793501948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.03937307993571 and perplexity of 154.37320502337718
Finished 30 epochs...
Completing Train Step...
At time: 955.3097157478333 and batch: 50, loss is 4.956566104888916 and perplexity is 142.10498350395554
At time: 957.0032682418823 and batch: 100, loss is 4.9139465236663815 and perplexity is 136.1757762537254
At time: 958.7141919136047 and batch: 150, loss is 4.893413410186768 and perplexity is 133.4081745629322
At time: 960.4324703216553 and batch: 200, loss is 4.90247296333313 and perplexity is 134.62228435258206
At time: 962.1621470451355 and batch: 250, loss is 4.914801301956177 and perplexity is 136.29222611317869
At time: 963.891667842865 and batch: 300, loss is 4.882095775604248 and perplexity is 131.90682150108958
At time: 965.6178903579712 and batch: 350, loss is 4.843015928268432 and perplexity is 126.85134997318882
At time: 967.346293926239 and batch: 400, loss is 4.889624423980713 and perplexity is 132.903649253056
At time: 969.0755152702332 and batch: 450, loss is 4.941644144058228 and perplexity is 140.00024100305671
At time: 970.8402807712555 and batch: 500, loss is 4.927729215621948 and perplexity is 138.06563880952447
At time: 972.5476686954498 and batch: 550, loss is 4.897052450180054 and perplexity is 133.89453665442377
At time: 974.2557737827301 and batch: 600, loss is 4.931674509048462 and perplexity is 138.61142420056052
At time: 975.9654037952423 and batch: 650, loss is 4.9207690334320064 and perplexity is 137.10801329919227
At time: 977.6752283573151 and batch: 700, loss is 4.89029203414917 and perplexity is 132.99240670511452
At time: 979.3826024532318 and batch: 750, loss is 4.895275077819824 and perplexity is 133.65676757048118
At time: 981.0913302898407 and batch: 800, loss is 4.874589376449585 and perplexity is 130.92038317939065
At time: 982.8010778427124 and batch: 850, loss is 4.915893516540527 and perplexity is 136.4411677936044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.039280255635579 and perplexity of 154.3588761037082
Finished 31 epochs...
Completing Train Step...
At time: 987.2372694015503 and batch: 50, loss is 4.953848133087158 and perplexity is 141.71927058177587
At time: 988.9333748817444 and batch: 100, loss is 4.911306705474853 and perplexity is 135.81677102507356
At time: 990.6227676868439 and batch: 150, loss is 4.891026840209961 and perplexity is 133.0901662443529
At time: 992.332364320755 and batch: 200, loss is 4.900255823135376 and perplexity is 134.32413851212755
At time: 994.0400865077972 and batch: 250, loss is 4.91261209487915 and perplexity is 135.99418056797145
At time: 995.7496495246887 and batch: 300, loss is 4.880283823013306 and perplexity is 131.66802899978654
At time: 997.4587278366089 and batch: 350, loss is 4.841489572525024 and perplexity is 126.65787737816355
At time: 999.1684727668762 and batch: 400, loss is 4.888233423233032 and perplexity is 132.71890869449064
At time: 1000.8753082752228 and batch: 450, loss is 4.940355043411255 and perplexity is 139.8198828766684
At time: 1002.5828776359558 and batch: 500, loss is 4.926653423309326 and perplexity is 137.91718872170475
At time: 1004.2916417121887 and batch: 550, loss is 4.896208372116089 and perplexity is 133.78156689750318
At time: 1006.0019097328186 and batch: 600, loss is 4.931324672698975 and perplexity is 138.56294136694416
At time: 1007.7079555988312 and batch: 650, loss is 4.92061182975769 and perplexity is 137.0864611098095
At time: 1009.4165239334106 and batch: 700, loss is 4.890216131210327 and perplexity is 132.9823125736937
At time: 1011.1260814666748 and batch: 750, loss is 4.895102186203003 and perplexity is 133.63366143332308
At time: 1012.8660218715668 and batch: 800, loss is 4.874363632202148 and perplexity is 130.89083199164654
At time: 1014.5735461711884 and batch: 850, loss is 4.915473461151123 and perplexity is 136.38386698133613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.039165496826172 and perplexity of 154.34116307924734
Finished 32 epochs...
Completing Train Step...
At time: 1019.0105581283569 and batch: 50, loss is 4.951363859176635 and perplexity is 141.36763805185643
At time: 1020.7270195484161 and batch: 100, loss is 4.909023504257203 and perplexity is 135.50702774576578
At time: 1022.413300037384 and batch: 150, loss is 4.888944492340088 and perplexity is 132.81331457095882
At time: 1024.1128051280975 and batch: 200, loss is 4.898344068527222 and perplexity is 134.06758902930517
At time: 1025.8399910926819 and batch: 250, loss is 4.910628185272217 and perplexity is 135.72464785932513
At time: 1027.5760354995728 and batch: 300, loss is 4.878635396957398 and perplexity is 131.45116278312202
At time: 1029.310209274292 and batch: 350, loss is 4.840040664672852 and perplexity is 126.47449466949595
At time: 1031.0457277297974 and batch: 400, loss is 4.887001028060913 and perplexity is 132.5554472973737
At time: 1032.7812185287476 and batch: 450, loss is 4.939060926437378 and perplexity is 139.6390566233504
At time: 1034.5180461406708 and batch: 500, loss is 4.9255028820037845 and perplexity is 137.75860054795265
At time: 1036.252118587494 and batch: 550, loss is 4.895198879241943 and perplexity is 133.64658350287877
At time: 1037.9873542785645 and batch: 600, loss is 4.93064661026001 and perplexity is 138.46901888722215
At time: 1039.7239758968353 and batch: 650, loss is 4.920140438079834 and perplexity is 137.02185492150164
At time: 1041.4605758190155 and batch: 700, loss is 4.889749240875244 and perplexity is 132.92023890914075
At time: 1043.194653749466 and batch: 750, loss is 4.894478635787964 and perplexity is 133.55036008226824
At time: 1044.929969549179 and batch: 800, loss is 4.8739212608337406 and perplexity is 130.83294244047042
At time: 1046.6660585403442 and batch: 850, loss is 4.914784164428711 and perplexity is 136.28989042142425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.03905455271403 and perplexity of 154.324040785769
Finished 33 epochs...
Completing Train Step...
At time: 1051.1537957191467 and batch: 50, loss is 4.949312219619751 and perplexity is 141.07789993428855
At time: 1052.8604187965393 and batch: 100, loss is 4.907016763687134 and perplexity is 135.23537295716454
At time: 1054.5565712451935 and batch: 150, loss is 4.887246303558349 and perplexity is 132.5879638882459
At time: 1056.2837245464325 and batch: 200, loss is 4.896644763946533 and perplexity is 133.8399608207501
At time: 1057.9944360256195 and batch: 250, loss is 4.9088701725006105 and perplexity is 135.48625180801716
At time: 1059.7056312561035 and batch: 300, loss is 4.877132673263549 and perplexity is 131.25377635200184
At time: 1061.4181640148163 and batch: 350, loss is 4.838734970092774 and perplexity is 126.30946536966472
At time: 1063.1286721229553 and batch: 400, loss is 4.88589319229126 and perplexity is 132.4086789440218
At time: 1064.838828086853 and batch: 450, loss is 4.937850875854492 and perplexity is 139.4701884915854
At time: 1066.5500061511993 and batch: 500, loss is 4.9244717025756835 and perplexity is 137.6166199293512
At time: 1068.26198387146 and batch: 550, loss is 4.894128351211548 and perplexity is 133.50358764326688
At time: 1069.9720375537872 and batch: 600, loss is 4.93009168624878 and perplexity is 138.39220042000738
At time: 1071.6813101768494 and batch: 650, loss is 4.9196798038482665 and perplexity is 136.95875249933562
At time: 1073.3917338848114 and batch: 700, loss is 4.8891987609863286 and perplexity is 132.84708912638075
At time: 1075.1029329299927 and batch: 750, loss is 4.893896675109863 and perplexity is 133.47266163506754
At time: 1076.8147089481354 and batch: 800, loss is 4.873341970443725 and perplexity is 130.75717412226538
At time: 1078.525149345398 and batch: 850, loss is 4.9141176986694335 and perplexity is 136.1990881377902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.0390424728393555 and perplexity of 154.32217658195668
Finished 34 epochs...
Completing Train Step...
At time: 1082.9648475646973 and batch: 50, loss is 4.94765489578247 and perplexity is 140.84428181172933
At time: 1084.6525466442108 and batch: 100, loss is 4.905197038650512 and perplexity is 134.98950553655516
At time: 1086.3566393852234 and batch: 150, loss is 4.885776424407959 and perplexity is 132.39321876549465
At time: 1088.0665664672852 and batch: 200, loss is 4.895137939453125 and perplexity is 133.6384393564577
At time: 1089.7783317565918 and batch: 250, loss is 4.907257223129273 and perplexity is 135.2678954895216
At time: 1091.487471818924 and batch: 300, loss is 4.875793075561523 and perplexity is 131.0780668111432
At time: 1093.1961665153503 and batch: 350, loss is 4.837721996307373 and perplexity is 126.1815819745959
At time: 1094.905507326126 and batch: 400, loss is 4.8847951984405515 and perplexity is 132.26337481487914
At time: 1096.616358757019 and batch: 450, loss is 4.936734790802002 and perplexity is 139.314614731865
At time: 1098.3246660232544 and batch: 500, loss is 4.9234973907470705 and perplexity is 137.48260372612793
At time: 1100.0791809558868 and batch: 550, loss is 4.893227262496948 and perplexity is 133.38334325065165
At time: 1101.788791179657 and batch: 600, loss is 4.929320468902588 and perplexity is 138.28551110008092
At time: 1103.4994938373566 and batch: 650, loss is 4.9191347599029545 and perplexity is 136.8841243001999
At time: 1105.2086055278778 and batch: 700, loss is 4.888486309051514 and perplexity is 132.75247566847074
At time: 1106.9170725345612 and batch: 750, loss is 4.893195486068725 and perplexity is 133.37910487175935
At time: 1108.6272821426392 and batch: 800, loss is 4.872689180374145 and perplexity is 130.67184499150676
At time: 1110.338666677475 and batch: 850, loss is 4.913450527191162 and perplexity is 136.108250296396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.039008458455403 and perplexity of 154.31692749746279
Finished 35 epochs...
Completing Train Step...
At time: 1114.814757823944 and batch: 50, loss is 4.9459740352630615 and perplexity is 140.60774107073527
At time: 1116.5043518543243 and batch: 100, loss is 4.903603620529175 and perplexity is 134.77458208917463
At time: 1118.1963407993317 and batch: 150, loss is 4.884391756057739 and perplexity is 132.2100249263117
At time: 1119.9068388938904 and batch: 200, loss is 4.893735570907593 and perplexity is 133.4511603604093
At time: 1121.6235177516937 and batch: 250, loss is 4.905731287002563 and perplexity is 135.06164272530356
At time: 1123.3408071994781 and batch: 300, loss is 4.87437331199646 and perplexity is 130.8920989941097
At time: 1125.0593869686127 and batch: 350, loss is 4.836452302932739 and perplexity is 126.02147172293456
At time: 1126.7743272781372 and batch: 400, loss is 4.88359561920166 and perplexity is 132.10480954120516
At time: 1128.4898664951324 and batch: 450, loss is 4.935611934661865 and perplexity is 139.15827225278525
At time: 1130.2065391540527 and batch: 500, loss is 4.9223972415924075 and perplexity is 137.3314355248891
At time: 1131.9240493774414 and batch: 550, loss is 4.892147607803345 and perplexity is 133.23941300952387
At time: 1133.6391160488129 and batch: 600, loss is 4.92850703239441 and perplexity is 138.17307035470367
At time: 1135.3557813167572 and batch: 650, loss is 4.91851321220398 and perplexity is 136.79907072288074
At time: 1137.0728945732117 and batch: 700, loss is 4.887773685455322 and perplexity is 132.6579068218544
At time: 1138.7905025482178 and batch: 750, loss is 4.892369031906128 and perplexity is 133.26891869351908
At time: 1140.506532907486 and batch: 800, loss is 4.871950149536133 and perplexity is 130.5753101439064
At time: 1142.2224996089935 and batch: 850, loss is 4.912751655578614 and perplexity is 136.0131613353883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.039059003194173 and perplexity of 154.32472760337637
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 1146.7301850318909 and batch: 50, loss is 4.945123062133789 and perplexity is 140.48813855779633
At time: 1148.4240174293518 and batch: 100, loss is 4.903137865066529 and perplexity is 134.71182470727192
At time: 1150.134120941162 and batch: 150, loss is 4.882815618515014 and perplexity is 132.00180787496117
At time: 1151.8519382476807 and batch: 200, loss is 4.892539777755737 and perplexity is 133.2916757510492
At time: 1153.5689096450806 and batch: 250, loss is 4.902458276748657 and perplexity is 134.62030722554962
At time: 1155.2836475372314 and batch: 300, loss is 4.869656915664673 and perplexity is 130.27621350026217
At time: 1156.999969959259 and batch: 350, loss is 4.832173700332642 and perplexity is 125.48342778525362
At time: 1158.721762895584 and batch: 400, loss is 4.877607107162476 and perplexity is 131.31606236699545
At time: 1160.4389953613281 and batch: 450, loss is 4.92890025138855 and perplexity is 138.2274133140875
At time: 1162.153148651123 and batch: 500, loss is 4.915115747451782 and perplexity is 136.3350893284853
At time: 1163.8686842918396 and batch: 550, loss is 4.883978223800659 and perplexity is 132.1553631192845
At time: 1165.585872888565 and batch: 600, loss is 4.918705778121948 and perplexity is 136.82541609804102
At time: 1167.304202079773 and batch: 650, loss is 4.908138437271118 and perplexity is 135.38714800775037
At time: 1169.0184428691864 and batch: 700, loss is 4.877553462982178 and perplexity is 131.30901821341044
At time: 1170.7343711853027 and batch: 750, loss is 4.8806939220428465 and perplexity is 131.72203700425905
At time: 1172.4519894123077 and batch: 800, loss is 4.859888830184937 and perplexity is 129.00985930794917
At time: 1174.1694386005402 and batch: 850, loss is 4.903107814788818 and perplexity is 134.70777664035163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.036375681559245 and perplexity of 153.91117981199778
Finished 37 epochs...
Completing Train Step...
At time: 1178.5968523025513 and batch: 50, loss is 4.942322587966919 and perplexity is 140.0952555411533
At time: 1180.3229351043701 and batch: 100, loss is 4.900342655181885 and perplexity is 134.33580265837358
At time: 1182.0320394039154 and batch: 150, loss is 4.879980297088623 and perplexity is 131.6280704040748
At time: 1183.7396800518036 and batch: 200, loss is 4.890276145935059 and perplexity is 132.9902937100675
At time: 1185.4480080604553 and batch: 250, loss is 4.90027268409729 and perplexity is 134.32640336540484
At time: 1187.1861617565155 and batch: 300, loss is 4.867993040084839 and perplexity is 130.0596303236823
At time: 1188.8953392505646 and batch: 350, loss is 4.830816125869751 and perplexity is 125.31319026937419
At time: 1190.6024932861328 and batch: 400, loss is 4.876335887908936 and perplexity is 131.14923691854233
At time: 1192.3116555213928 and batch: 450, loss is 4.927757453918457 and perplexity is 138.0695376030183
At time: 1194.0207459926605 and batch: 500, loss is 4.914272327423095 and perplexity is 136.22015006138636
At time: 1195.7300510406494 and batch: 550, loss is 4.883447465896606 and perplexity is 132.08523922679726
At time: 1197.4368467330933 and batch: 600, loss is 4.918716535568238 and perplexity is 136.8268879980226
At time: 1199.1448092460632 and batch: 650, loss is 4.9083045387268065 and perplexity is 135.40963787786433
At time: 1200.8542652130127 and batch: 700, loss is 4.878130331039428 and perplexity is 131.38478804414302
At time: 1202.5642483234406 and batch: 750, loss is 4.881392860412598 and perplexity is 131.8141347716456
At time: 1204.271977186203 and batch: 800, loss is 4.860585765838623 and perplexity is 129.0998022171475
At time: 1205.9804413318634 and batch: 850, loss is 4.90361946105957 and perplexity is 134.77671700694773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.036366144816081 and perplexity of 153.9097120076049
Finished 38 epochs...
Completing Train Step...
At time: 1210.4366221427917 and batch: 50, loss is 4.9408946704864505 and perplexity is 139.8953538323734
At time: 1212.1528813838959 and batch: 100, loss is 4.898867721557617 and perplexity is 134.13781231329122
At time: 1213.8561463356018 and batch: 150, loss is 4.878519983291626 and perplexity is 131.43599239800693
At time: 1215.566617012024 and batch: 200, loss is 4.889113311767578 and perplexity is 132.83573793138387
At time: 1217.2751507759094 and batch: 250, loss is 4.898931589126587 and perplexity is 134.1463796428549
At time: 1218.9832804203033 and batch: 300, loss is 4.866883344650269 and perplexity is 129.9153837953496
At time: 1220.692274093628 and batch: 350, loss is 4.830027627944946 and perplexity is 125.21442002408024
At time: 1222.40247797966 and batch: 400, loss is 4.875464334487915 and perplexity is 131.03498314880594
At time: 1224.1119141578674 and batch: 450, loss is 4.926949720382691 and perplexity is 137.9580592356975
At time: 1225.8195850849152 and batch: 500, loss is 4.91366865158081 and perplexity is 136.13794206354956
At time: 1227.5273797512054 and batch: 550, loss is 4.883050508499146 and perplexity is 132.03281741929135
At time: 1229.2651798725128 and batch: 600, loss is 4.918647298812866 and perplexity is 136.81741487619794
At time: 1230.9745898246765 and batch: 650, loss is 4.908335361480713 and perplexity is 135.4138116401323
At time: 1232.6818985939026 and batch: 700, loss is 4.8784401416778564 and perplexity is 131.4254987551868
At time: 1234.3906416893005 and batch: 750, loss is 4.881696882247925 and perplexity is 131.8542152391809
At time: 1236.1013720035553 and batch: 800, loss is 4.86084921836853 and perplexity is 129.13381836727578
At time: 1237.8119988441467 and batch: 850, loss is 4.903755254745484 and perplexity is 134.795020076818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.036375681559245 and perplexity of 153.91117981199778
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 1242.2470273971558 and batch: 50, loss is 4.940294857025147 and perplexity is 139.81146787644866
At time: 1243.9624419212341 and batch: 100, loss is 4.898329133987427 and perplexity is 134.06558680651275
At time: 1245.6600849628448 and batch: 150, loss is 4.877374258041382 and perplexity is 131.28548909690545
At time: 1247.3688809871674 and batch: 200, loss is 4.88846676826477 and perplexity is 132.74988160599915
At time: 1249.0859065055847 and batch: 250, loss is 4.89714672088623 and perplexity is 133.90715958192416
At time: 1250.804675579071 and batch: 300, loss is 4.864719429016113 and perplexity is 129.63456181235705
At time: 1252.5206446647644 and batch: 350, loss is 4.828200073242187 and perplexity is 124.98579279992508
At time: 1254.2361340522766 and batch: 400, loss is 4.872373714447021 and perplexity is 130.63062897824324
At time: 1255.951907157898 and batch: 450, loss is 4.923559894561768 and perplexity is 137.4911971818744
At time: 1257.6695141792297 and batch: 500, loss is 4.910586023330689 and perplexity is 135.7189255652905
At time: 1259.4006490707397 and batch: 550, loss is 4.8797757530212404 and perplexity is 131.6011494165307
At time: 1261.1334965229034 and batch: 600, loss is 4.91441463470459 and perplexity is 136.23953656001365
At time: 1262.8690657615662 and batch: 650, loss is 4.903367118835449 and perplexity is 134.742711441119
At time: 1264.60524392128 and batch: 700, loss is 4.873740110397339 and perplexity is 130.80924414239496
At time: 1266.3402631282806 and batch: 750, loss is 4.876727437973022 and perplexity is 131.20059846531493
At time: 1268.073807001114 and batch: 800, loss is 4.855453186035156 and perplexity is 128.438884736412
At time: 1269.7942388057709 and batch: 850, loss is 4.899312915802002 and perplexity is 134.19754299017774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.037508010864258 and perplexity of 154.08555665864304
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 1274.2603685855865 and batch: 50, loss is 4.9396541309356685 and perplexity is 139.7219157136392
At time: 1275.9398832321167 and batch: 100, loss is 4.897364015579224 and perplexity is 133.93626005863024
At time: 1277.641809463501 and batch: 150, loss is 4.876528024673462 and perplexity is 131.17443792953677
At time: 1279.3541944026947 and batch: 200, loss is 4.887724113464356 and perplexity is 132.65133086828874
At time: 1281.063279390335 and batch: 250, loss is 4.895996980667114 and perplexity is 133.7532896071246
At time: 1282.7737379074097 and batch: 300, loss is 4.863473320007325 and perplexity is 129.47312362271344
At time: 1284.4860010147095 and batch: 350, loss is 4.827283401489257 and perplexity is 124.87127435008095
At time: 1286.1984791755676 and batch: 400, loss is 4.870913486480713 and perplexity is 130.44001768239477
At time: 1287.9076676368713 and batch: 450, loss is 4.922105665206909 and perplexity is 137.29139875847494
At time: 1289.6166684627533 and batch: 500, loss is 4.909413576126099 and perplexity is 135.5598955357756
At time: 1291.3273344039917 and batch: 550, loss is 4.878684911727905 and perplexity is 131.4576717184231
At time: 1293.0391716957092 and batch: 600, loss is 4.912690839767456 and perplexity is 136.00488983617507
At time: 1294.7486112117767 and batch: 650, loss is 4.901231384277343 and perplexity is 134.4552438623328
At time: 1296.4590656757355 and batch: 700, loss is 4.871509294509888 and perplexity is 130.51775804912324
At time: 1298.1732614040375 and batch: 750, loss is 4.874765777587891 and perplexity is 130.94347972107923
At time: 1299.902906179428 and batch: 800, loss is 4.853358116149902 and perplexity is 128.17007797969882
At time: 1301.6307890415192 and batch: 850, loss is 4.8975240516662595 and perplexity is 133.9576964088513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.03715705871582 and perplexity of 154.03148948954004
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 1306.1322491168976 and batch: 50, loss is 4.939357776641845 and perplexity is 139.6805146589694
At time: 1307.8270599842072 and batch: 100, loss is 4.89674825668335 and perplexity is 133.85381300137857
At time: 1309.5221810340881 and batch: 150, loss is 4.876211614608764 and perplexity is 131.13293958273832
At time: 1311.2308773994446 and batch: 200, loss is 4.887107667922973 and perplexity is 132.56958374572983
At time: 1312.9415476322174 and batch: 250, loss is 4.895343904495239 and perplexity is 133.66596703802054
At time: 1314.6532912254333 and batch: 300, loss is 4.86284140586853 and perplexity is 129.39133357016937
At time: 1316.4076335430145 and batch: 350, loss is 4.826907386779785 and perplexity is 124.82432974061977
At time: 1318.1164648532867 and batch: 400, loss is 4.870309762954712 and perplexity is 130.36129174173024
At time: 1319.826912164688 and batch: 450, loss is 4.921722450256348 and perplexity is 137.23879672147442
At time: 1321.5377197265625 and batch: 500, loss is 4.909066247940063 and perplexity is 135.5128199389741
At time: 1323.2452940940857 and batch: 550, loss is 4.878261632919312 and perplexity is 131.40204024639982
At time: 1324.9547572135925 and batch: 600, loss is 4.912023096084595 and perplexity is 135.91410374444172
At time: 1326.6644649505615 and batch: 650, loss is 4.900312538146973 and perplexity is 134.33175692323803
At time: 1328.3757057189941 and batch: 700, loss is 4.870633640289307 and perplexity is 130.40351964739557
At time: 1330.0838532447815 and batch: 750, loss is 4.873796291351319 and perplexity is 130.8165933369607
At time: 1331.7933530807495 and batch: 800, loss is 4.852478132247925 and perplexity is 128.05733998544667
At time: 1333.5037455558777 and batch: 850, loss is 4.896728582382202 and perplexity is 133.85117954705763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.036685625712077 and perplexity of 153.95889107576767
Annealing...
Finished 42 epochs...
Completing Train Step...
At time: 1337.9592263698578 and batch: 50, loss is 4.939125509262085 and perplexity is 139.64807519928596
At time: 1339.6408219337463 and batch: 100, loss is 4.896513385772705 and perplexity is 133.82237832611557
At time: 1341.34397315979 and batch: 150, loss is 4.876033163070678 and perplexity is 131.1095407958129
At time: 1343.0614280700684 and batch: 200, loss is 4.886739959716797 and perplexity is 132.52084578312073
At time: 1344.7775256633759 and batch: 250, loss is 4.895072832107544 and perplexity is 133.62973879564183
At time: 1346.4936065673828 and batch: 300, loss is 4.862593832015992 and perplexity is 129.35930362428445
At time: 1348.210384607315 and batch: 350, loss is 4.826794242858886 and perplexity is 124.81020742547145
At time: 1349.9276523590088 and batch: 400, loss is 4.870052852630615 and perplexity is 130.32780488176348
At time: 1351.642798423767 and batch: 450, loss is 4.921628589630127 and perplexity is 137.22591600657782
At time: 1353.3584778308868 and batch: 500, loss is 4.909098262786865 and perplexity is 135.51715843059185
At time: 1355.0754835605621 and batch: 550, loss is 4.878098192214966 and perplexity is 131.38056555935628
At time: 1356.7940390110016 and batch: 600, loss is 4.911641416549682 and perplexity is 135.86223801121056
At time: 1358.5071890354156 and batch: 650, loss is 4.899884901046753 and perplexity is 134.27432396134782
At time: 1360.251224040985 and batch: 700, loss is 4.870248317718506 and perplexity is 130.3532819074527
At time: 1361.9682216644287 and batch: 750, loss is 4.873368377685547 and perplexity is 130.760627104174
At time: 1363.6867861747742 and batch: 800, loss is 4.852168626785279 and perplexity is 128.01771167209526
At time: 1365.4006931781769 and batch: 850, loss is 4.896413316726685 and perplexity is 133.80898751839416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.036499659220378 and perplexity of 153.9302625429848
Annealing...
Model not improving. Stopping early with 153.9097120076049loss at 42 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9fbf6be518>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'num_layers': 1, 'anneal': 2.0, 'wordvec_source': '', 'dropout': 0.0, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 7.805828959670697, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.1071417331695557 and batch: 50, loss is 7.088152103424072 and perplexity is 1197.6925425089678
At time: 3.675748825073242 and batch: 100, loss is 5.978814973831176 and perplexity is 394.97203851709486
At time: 5.244929313659668 and batch: 150, loss is 5.645714902877808 and perplexity is 283.07585564578443
At time: 6.81746244430542 and batch: 200, loss is 5.547777423858642 and perplexity is 256.6664607376984
At time: 8.389387607574463 and batch: 250, loss is 5.52983772277832 and perplexity is 252.1029971299547
At time: 9.964197635650635 and batch: 300, loss is 5.43234395980835 and perplexity is 228.68464531836955
At time: 11.541122436523438 and batch: 350, loss is 5.365015745162964 and perplexity is 213.79460088527102
At time: 13.1179678440094 and batch: 400, loss is 5.35748327255249 and perplexity is 212.19024886431902
At time: 14.695371627807617 and batch: 450, loss is 5.312832202911377 and perplexity is 202.92413762418252
At time: 16.276057481765747 and batch: 500, loss is 5.285707111358643 and perplexity is 197.4937841837652
At time: 17.88665246963501 and batch: 550, loss is 5.259411706924438 and perplexity is 192.3682890700497
At time: 19.469390630722046 and batch: 600, loss is 5.28596453666687 and perplexity is 197.54463062633147
At time: 21.051536798477173 and batch: 650, loss is 5.277096366882324 and perplexity is 195.80051628336258
At time: 22.636682987213135 and batch: 700, loss is 5.21105541229248 and perplexity is 183.28739992308795
At time: 24.222116470336914 and batch: 750, loss is 5.188360729217529 and perplexity is 179.17459637965948
At time: 25.81041169166565 and batch: 800, loss is 5.174401769638061 and perplexity is 176.6908808035244
At time: 27.39620089530945 and batch: 850, loss is 5.15450457572937 and perplexity is 173.2099730740194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.935438791910808 and perplexity of 139.13418009244666
Finished 1 epochs...
Completing Train Step...
At time: 31.6201012134552 and batch: 50, loss is 5.13463436126709 and perplexity is 169.8022222533371
At time: 33.20641899108887 and batch: 100, loss is 5.027613801956177 and perplexity is 152.568519289561
At time: 34.7921187877655 and batch: 150, loss is 4.990082397460937 and perplexity is 146.9485311827245
At time: 36.377620697021484 and batch: 200, loss is 4.992115316390991 and perplexity is 147.24756949084036
At time: 37.96454882621765 and batch: 250, loss is 4.999066743850708 and perplexity is 148.27471622059204
At time: 39.55361199378967 and batch: 300, loss is 4.9581318283081055 and perplexity is 142.32765488001567
At time: 41.21874141693115 and batch: 350, loss is 4.913259029388428 and perplexity is 136.0821883610116
At time: 42.914448499679565 and batch: 400, loss is 4.926369361877441 and perplexity is 137.8780173313989
At time: 44.624191999435425 and batch: 450, loss is 4.917797021865844 and perplexity is 136.70113162586694
At time: 46.33537173271179 and batch: 500, loss is 4.902468013763428 and perplexity is 134.62161803185114
At time: 48.044724225997925 and batch: 550, loss is 4.89902102470398 and perplexity is 134.15837763828924
At time: 49.753297090530396 and batch: 600, loss is 4.9370082569122316 and perplexity is 139.35271776736084
At time: 51.46275305747986 and batch: 650, loss is 4.920120525360107 and perplexity is 137.01912647087372
At time: 53.17338013648987 and batch: 700, loss is 4.878613138198853 and perplexity is 131.44823687599285
At time: 54.88268423080444 and batch: 750, loss is 4.851517753601074 and perplexity is 127.9344154870319
At time: 56.59097504615784 and batch: 800, loss is 4.826839094161987 and perplexity is 124.8158054514532
At time: 58.30102586746216 and batch: 850, loss is 4.827620000839233 and perplexity is 124.9133130145545
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.783129056294759 and perplexity of 119.47761794463527
Finished 2 epochs...
Completing Train Step...
At time: 62.715620279312134 and batch: 50, loss is 4.8299061298370365 and perplexity is 125.19920763312138
At time: 64.38079881668091 and batch: 100, loss is 4.7479055309295655 and perplexity is 115.34245016392076
At time: 66.05579924583435 and batch: 150, loss is 4.728373403549194 and perplexity is 113.11142598742659
At time: 67.74249243736267 and batch: 200, loss is 4.744844188690186 and perplexity is 114.98988738225427
At time: 69.43935894966125 and batch: 250, loss is 4.760934715270996 and perplexity is 116.85510111467613
At time: 71.13638496398926 and batch: 300, loss is 4.718925971984863 and perplexity is 112.04784549394847
At time: 72.84363675117493 and batch: 350, loss is 4.676457557678223 and perplexity is 107.38897870534797
At time: 74.55563759803772 and batch: 400, loss is 4.69915060043335 and perplexity is 109.85382302258321
At time: 76.29364132881165 and batch: 450, loss is 4.697956676483154 and perplexity is 109.72274417691322
At time: 78.00087285041809 and batch: 500, loss is 4.686762084960938 and perplexity is 108.5012924575738
At time: 79.71054458618164 and batch: 550, loss is 4.701526956558228 and perplexity is 110.11518524963454
At time: 81.42197918891907 and batch: 600, loss is 4.733366203308106 and perplexity is 113.67758086068984
At time: 83.13138318061829 and batch: 650, loss is 4.717201852798462 and perplexity is 111.85482809405215
At time: 84.83972430229187 and batch: 700, loss is 4.687280931472778 and perplexity is 108.55760258158809
At time: 86.54972195625305 and batch: 750, loss is 4.647565298080444 and perplexity is 104.33066203860757
At time: 88.26101660728455 and batch: 800, loss is 4.623368616104126 and perplexity is 101.83650306335015
At time: 89.97138333320618 and batch: 850, loss is 4.641225147247314 and perplexity is 103.67128239673903
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.743897120157878 and perplexity of 114.88103563155092
Finished 3 epochs...
Completing Train Step...
At time: 94.43174171447754 and batch: 50, loss is 4.6441667366027835 and perplexity is 103.97668970873137
At time: 96.11605453491211 and batch: 100, loss is 4.569937419891358 and perplexity is 96.53806822100874
At time: 97.79588651657104 and batch: 150, loss is 4.5634017753601075 and perplexity is 95.90918703386347
At time: 99.480553150177 and batch: 200, loss is 4.584779376983643 and perplexity is 97.98156776830223
At time: 101.17603325843811 and batch: 250, loss is 4.589155387878418 and perplexity is 98.41127569390241
At time: 102.88382077217102 and batch: 300, loss is 4.5577734756469725 and perplexity is 95.37089763201945
At time: 104.60056781768799 and batch: 350, loss is 4.516704931259155 and perplexity is 91.53349137431843
At time: 106.31820702552795 and batch: 400, loss is 4.547317008972168 and perplexity is 94.37885071096287
At time: 108.03662014007568 and batch: 450, loss is 4.5561853122711184 and perplexity is 95.21955327686979
At time: 109.75587630271912 and batch: 500, loss is 4.54010534286499 and perplexity is 93.70067029664902
At time: 111.47301602363586 and batch: 550, loss is 4.553115568161011 and perplexity is 94.92770179789467
At time: 113.19096684455872 and batch: 600, loss is 4.585297546386719 and perplexity is 98.03235197505992
At time: 114.90933680534363 and batch: 650, loss is 4.566654624938965 and perplexity is 96.22167315214891
At time: 116.62882256507874 and batch: 700, loss is 4.540724687576294 and perplexity is 93.75872128617388
At time: 118.37462043762207 and batch: 750, loss is 4.503276853561402 and perplexity is 90.31258807772856
At time: 120.08177542686462 and batch: 800, loss is 4.478759050369263 and perplexity is 88.12524580183845
At time: 121.79803323745728 and batch: 850, loss is 4.498479633331299 and perplexity is 89.88037623974112
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.716773351033528 and perplexity of 111.80690837037379
Finished 4 epochs...
Completing Train Step...
At time: 126.2384581565857 and batch: 50, loss is 4.498261957168579 and perplexity is 89.86081355357993
At time: 127.92704391479492 and batch: 100, loss is 4.429494848251343 and perplexity is 83.88902951518408
At time: 129.6291422843933 and batch: 150, loss is 4.428479967117309 and perplexity is 83.80393530933259
At time: 131.3427493572235 and batch: 200, loss is 4.4535605907440186 and perplexity is 85.93236993214548
At time: 133.06574606895447 and batch: 250, loss is 4.460725994110107 and perplexity is 86.55032131642322
At time: 134.79013180732727 and batch: 300, loss is 4.426851205825805 and perplexity is 83.66754980329254
At time: 136.5164179801941 and batch: 350, loss is 4.388326978683471 and perplexity is 80.50561863391768
At time: 138.24222326278687 and batch: 400, loss is 4.425505695343017 and perplexity is 83.5550499398083
At time: 139.96709537506104 and batch: 450, loss is 4.438369054794311 and perplexity is 84.63679107779183
At time: 141.69177889823914 and batch: 500, loss is 4.426574573516846 and perplexity is 83.64440785685811
At time: 143.41715836524963 and batch: 550, loss is 4.438141956329345 and perplexity is 84.61757237481001
At time: 145.1437156200409 and batch: 600, loss is 4.471730833053589 and perplexity is 87.5080538435077
At time: 146.87048959732056 and batch: 650, loss is 4.453635740280151 and perplexity is 85.93882795254024
At time: 148.5946662425995 and batch: 700, loss is 4.426481637954712 and perplexity is 83.63663467800254
At time: 150.31967997550964 and batch: 750, loss is 4.3954400634765625 and perplexity is 81.08030339323287
At time: 152.04573678970337 and batch: 800, loss is 4.365754280090332 and perplexity is 78.70874599722991
At time: 153.7719211578369 and batch: 850, loss is 4.39222882270813 and perplexity is 80.82035262310087
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.6944014231363935 and perplexity of 109.33334464062655
Finished 5 epochs...
Completing Train Step...
At time: 158.20474004745483 and batch: 50, loss is 4.385593128204346 and perplexity is 80.28582888289237
At time: 159.9174997806549 and batch: 100, loss is 4.3136171531677245 and perplexity is 74.71023916054169
At time: 161.6243143081665 and batch: 150, loss is 4.324062786102295 and perplexity is 75.49472498775745
At time: 163.3620102405548 and batch: 200, loss is 4.351281547546387 and perplexity is 77.57781891043186
At time: 165.0730004310608 and batch: 250, loss is 4.358346853256226 and perplexity is 78.12787077146666
At time: 166.78534269332886 and batch: 300, loss is 4.3253626537323 and perplexity is 75.5929219447014
At time: 168.49579858779907 and batch: 350, loss is 4.280847082138061 and perplexity is 72.30165951901802
At time: 170.20610666275024 and batch: 400, loss is 4.32529130935669 and perplexity is 75.58752900726489
At time: 171.9166944026947 and batch: 450, loss is 4.340194110870361 and perplexity is 76.72243055074617
At time: 173.62823176383972 and batch: 500, loss is 4.329712448120117 and perplexity is 75.92245178628485
At time: 175.33828496932983 and batch: 550, loss is 4.345078792572021 and perplexity is 77.09811199852784
At time: 177.048406124115 and batch: 600, loss is 4.380370817184448 and perplexity is 79.86764420960195
At time: 178.75833797454834 and batch: 650, loss is 4.363241920471191 and perplexity is 78.5112495170864
At time: 180.47077631950378 and batch: 700, loss is 4.335675258636474 and perplexity is 76.37651538246101
At time: 182.18109917640686 and batch: 750, loss is 4.307462301254272 and perplexity is 74.25182089750851
At time: 183.89233541488647 and batch: 800, loss is 4.270889472961426 and perplexity is 71.58528048125532
At time: 185.60270261764526 and batch: 850, loss is 4.297468223571777 and perplexity is 73.51343830378464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.707103411356608 and perplexity of 110.73095290734726
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 190.03016662597656 and batch: 50, loss is 4.28719388961792 and perplexity is 72.76200354461622
At time: 191.76901412010193 and batch: 100, loss is 4.201081228256226 and perplexity is 66.75847318038227
At time: 193.48787832260132 and batch: 150, loss is 4.1997031974792485 and perplexity is 66.66654130673929
At time: 195.2090141773224 and batch: 200, loss is 4.225445404052734 and perplexity is 68.40496461277947
At time: 196.92605090141296 and batch: 250, loss is 4.210572156906128 and perplexity is 67.39508934668197
At time: 198.64305520057678 and batch: 300, loss is 4.176701617240906 and perplexity is 65.15060678842958
At time: 200.36183071136475 and batch: 350, loss is 4.119668941497803 and perplexity is 61.53886592695407
At time: 202.0816674232483 and batch: 400, loss is 4.152290124893188 and perplexity is 63.57943855360054
At time: 203.7984254360199 and batch: 450, loss is 4.1594275903701785 and perplexity is 64.034857938007
At time: 205.5431067943573 and batch: 500, loss is 4.134751949310303 and perplexity is 62.4740924066082
At time: 207.2613844871521 and batch: 550, loss is 4.141933860778809 and perplexity is 62.924390873684494
At time: 208.98162841796875 and batch: 600, loss is 4.173047780990601 and perplexity is 64.91299150763714
At time: 210.69948816299438 and batch: 650, loss is 4.142133736610413 and perplexity is 62.936969195647954
At time: 212.41748690605164 and batch: 700, loss is 4.112029123306274 and perplexity is 61.07051152881237
At time: 214.13629031181335 and batch: 750, loss is 4.076250915527344 and perplexity is 58.92414363182384
At time: 215.85863995552063 and batch: 800, loss is 4.029826970100403 and perplexity is 56.25117726946236
At time: 217.57758378982544 and batch: 850, loss is 4.053733396530151 and perplexity is 57.61214502072566
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.617172559102376 and perplexity of 101.20746906321334
Finished 7 epochs...
Completing Train Step...
At time: 222.01457476615906 and batch: 50, loss is 4.152035789489746 and perplexity is 63.563270107636676
At time: 223.7530255317688 and batch: 100, loss is 4.081468234062195 and perplexity is 59.23237302482964
At time: 225.468843460083 and batch: 150, loss is 4.08736174583435 and perplexity is 59.58249040960957
At time: 227.1882984638214 and batch: 200, loss is 4.118274784088134 and perplexity is 61.4531308390302
At time: 228.90786504745483 and batch: 250, loss is 4.108192472457886 and perplexity is 60.8366542002656
At time: 230.6283962726593 and batch: 300, loss is 4.0790376329422 and perplexity is 59.08857757809562
At time: 232.34457206726074 and batch: 350, loss is 4.027328743934631 and perplexity is 56.110824496034546
At time: 234.0626039505005 and batch: 400, loss is 4.067601375579834 and perplexity is 58.416674747634225
At time: 235.78926014900208 and batch: 450, loss is 4.083787713050842 and perplexity is 59.36992072734442
At time: 237.52471351623535 and batch: 500, loss is 4.065075798034668 and perplexity is 58.26932505548824
At time: 239.25618147850037 and batch: 550, loss is 4.0782065153121945 and perplexity is 59.03948842179588
At time: 240.98968052864075 and batch: 600, loss is 4.107053217887878 and perplexity is 60.76738522893205
At time: 242.72390151023865 and batch: 650, loss is 4.082799758911133 and perplexity is 59.311294932954226
At time: 244.46205019950867 and batch: 700, loss is 4.057243270874023 and perplexity is 57.81471169425932
At time: 246.19408655166626 and batch: 750, loss is 4.030342803001404 and perplexity is 56.28020096246234
At time: 247.92725467681885 and batch: 800, loss is 3.9848605394363403 and perplexity is 53.77778907411998
At time: 249.69566702842712 and batch: 850, loss is 4.0154428386688235 and perplexity is 55.447844416076784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.62592347462972 and perplexity of 102.09701356345175
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 254.17331552505493 and batch: 50, loss is 4.090398621559143 and perplexity is 59.763710059859015
At time: 255.86941361427307 and batch: 100, loss is 4.023932008743286 and perplexity is 55.92055421567391
At time: 257.58010840415955 and batch: 150, loss is 4.02554557800293 and perplexity is 56.0108587396474
At time: 259.2907679080963 and batch: 200, loss is 4.05337818145752 and perplexity is 57.59168395270269
At time: 260.9994034767151 and batch: 250, loss is 4.035619335174561 and perplexity is 56.57795010419328
At time: 262.70967745780945 and batch: 300, loss is 4.006793003082276 and perplexity is 54.970298004705356
At time: 264.421053647995 and batch: 350, loss is 3.947150287628174 and perplexity is 51.78757665719099
At time: 266.13156032562256 and batch: 400, loss is 3.978817868232727 and perplexity is 53.4538074198125
At time: 267.8406126499176 and batch: 450, loss is 3.992259397506714 and perplexity is 54.17715892133279
At time: 269.5495777130127 and batch: 500, loss is 3.9689704370498657 and perplexity is 52.930008001956395
At time: 271.26055121421814 and batch: 550, loss is 3.9761905097961425 and perplexity is 53.31354944259391
At time: 272.97128987312317 and batch: 600, loss is 4.000610094070435 and perplexity is 54.63147020392197
At time: 274.67936539649963 and batch: 650, loss is 3.9673304319381715 and perplexity is 52.84327366009335
At time: 276.3899185657501 and batch: 700, loss is 3.9424340772628783 and perplexity is 51.54391059322297
At time: 278.10008811950684 and batch: 750, loss is 3.91234845161438 and perplexity is 50.01627495747946
At time: 279.8118212223053 and batch: 800, loss is 3.85639506816864 and perplexity is 47.29455006132358
At time: 281.5204801559448 and batch: 850, loss is 3.889376139640808 and perplexity is 48.88038247661905
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.591460545857747 and perplexity of 98.63839089896824
Finished 9 epochs...
Completing Train Step...
At time: 285.9701325893402 and batch: 50, loss is 4.021686286926269 and perplexity is 55.79511311268236
At time: 287.68514108657837 and batch: 100, loss is 3.95895649433136 and perplexity is 52.40261498267585
At time: 289.41068983078003 and batch: 150, loss is 3.960111351013184 and perplexity is 52.46316745071492
At time: 291.1390743255615 and batch: 200, loss is 3.993195810317993 and perplexity is 54.227914867575734
At time: 292.90113949775696 and batch: 250, loss is 3.977839641571045 and perplexity is 53.401543047594984
At time: 294.62730050086975 and batch: 300, loss is 3.9544008684158327 and perplexity is 52.164431221809465
At time: 296.35297083854675 and batch: 350, loss is 3.900478482246399 and perplexity is 49.42609295648616
At time: 298.0791392326355 and batch: 400, loss is 3.9345782661437987 and perplexity is 51.14057769506087
At time: 299.80687975883484 and batch: 450, loss is 3.9528977489471435 and perplexity is 52.08608074947892
At time: 301.5333077907562 and batch: 500, loss is 3.9323365354537962 and perplexity is 51.02606269638744
At time: 303.25970125198364 and batch: 550, loss is 3.9435672092437746 and perplexity is 51.60234975012248
At time: 304.9869170188904 and batch: 600, loss is 3.9689911890029905 and perplexity is 52.93110641439841
At time: 306.7175416946411 and batch: 650, loss is 3.93949556350708 and perplexity is 51.392670422406496
At time: 308.4445974826813 and batch: 700, loss is 3.9184933567047118 and perplexity is 50.32456646099339
At time: 310.1694555282593 and batch: 750, loss is 3.8937993574142458 and perplexity is 49.09706992776655
At time: 311.8973696231842 and batch: 800, loss is 3.839871196746826 and perplexity is 46.519482193244606
At time: 313.62572598457336 and batch: 850, loss is 3.8754980897903444 and perplexity is 48.20670357908699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.596451123555501 and perplexity of 99.13188383591488
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 318.1167154312134 and batch: 50, loss is 3.9915472412109376 and perplexity is 54.138590051681426
At time: 319.8207232952118 and batch: 100, loss is 3.9371787786483763 and perplexity is 51.27374248014686
At time: 321.53984236717224 and batch: 150, loss is 3.9366266679763795 and perplexity is 51.245441513082234
At time: 323.25679326057434 and batch: 200, loss is 3.971295132637024 and perplexity is 53.05319729130707
At time: 324.97412633895874 and batch: 250, loss is 3.949583969116211 and perplexity is 51.9137646121747
At time: 326.69280672073364 and batch: 300, loss is 3.9258709478378297 and perplexity is 50.69721346468163
At time: 328.4128930568695 and batch: 350, loss is 3.8682905864715575 and perplexity is 47.86050272366536
At time: 330.12980818748474 and batch: 400, loss is 3.894668221473694 and perplexity is 49.13974714491622
At time: 331.8470690250397 and batch: 450, loss is 3.9139005851745607 and perplexity is 50.093967175147924
At time: 333.56562089920044 and batch: 500, loss is 3.8893069887161253 and perplexity is 48.87700246983858
At time: 335.2850375175476 and batch: 550, loss is 3.8942033767700197 and perplexity is 49.11691010196647
At time: 337.0295162200928 and batch: 600, loss is 3.916253700256348 and perplexity is 50.211982842581904
At time: 338.7467563152313 and batch: 650, loss is 3.8831386280059816 and perplexity is 48.57643943179291
At time: 340.4655029773712 and batch: 700, loss is 3.8590106773376465 and perplexity is 47.418416041987214
At time: 342.18494725227356 and batch: 750, loss is 3.8341372776031495 and perplexity is 46.253506513987205
At time: 343.9030487537384 and batch: 800, loss is 3.7737952947616575 and perplexity is 43.54501779322892
At time: 345.6214723587036 and batch: 850, loss is 3.8083404016494753 and perplexity is 45.075569416242836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.577222188313802 and perplexity of 97.24389345726237
Finished 11 epochs...
Completing Train Step...
At time: 350.0398347377777 and batch: 50, loss is 3.958764853477478 and perplexity is 52.392573463008354
At time: 351.77399802207947 and batch: 100, loss is 3.9009494590759277 and perplexity is 49.449376983731064
At time: 353.4853038787842 and batch: 150, loss is 3.89950581073761 and perplexity is 49.37804097726065
At time: 355.1976602077484 and batch: 200, loss is 3.9371593379974366 and perplexity is 51.272745694906035
At time: 356.91028904914856 and batch: 250, loss is 3.918113980293274 and perplexity is 50.305478128622724
At time: 358.6205427646637 and batch: 300, loss is 3.896890721321106 and perplexity is 49.24908167843482
At time: 360.33109402656555 and batch: 350, loss is 3.8418658208847045 and perplexity is 46.61236367633866
At time: 362.0433449745178 and batch: 400, loss is 3.87007239818573 and perplexity is 47.94585714823905
At time: 363.75772738456726 and batch: 450, loss is 3.89265962600708 and perplexity is 49.04114433130332
At time: 365.4672293663025 and batch: 500, loss is 3.8699181652069092 and perplexity is 47.93846288610315
At time: 367.1782886981964 and batch: 550, loss is 3.8781979370117186 and perplexity is 48.33703016556836
At time: 368.89104199409485 and batch: 600, loss is 3.90157564163208 and perplexity is 49.48035101769867
At time: 370.6049642562866 and batch: 650, loss is 3.8711234760284423 and perplexity is 47.99627847006252
At time: 372.3150384426117 and batch: 700, loss is 3.8491898822784423 and perplexity is 46.955008734657504
At time: 374.0255169868469 and batch: 750, loss is 3.827773971557617 and perplexity is 45.96011575394755
At time: 375.73871421813965 and batch: 800, loss is 3.7689804649353027 and perplexity is 43.33585987672463
At time: 377.45273661613464 and batch: 850, loss is 3.804194755554199 and perplexity is 44.88908886615927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.57963498433431 and perplexity of 97.47880642098296
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 381.8942153453827 and batch: 50, loss is 3.9451258325576783 and perplexity is 51.68284108703754
At time: 383.6335530281067 and batch: 100, loss is 3.893605499267578 and perplexity is 49.087552983281945
At time: 385.34447383880615 and batch: 150, loss is 3.89166944026947 and perplexity is 48.99260852332845
At time: 387.0549285411835 and batch: 200, loss is 3.933689889907837 and perplexity is 51.095165795556035
At time: 388.7652039527893 and batch: 250, loss is 3.9108772563934324 and perplexity is 49.942745354255244
At time: 390.4768579006195 and batch: 300, loss is 3.886533107757568 and perplexity is 48.741611349614274
At time: 392.1891224384308 and batch: 350, loss is 3.8315706491470336 and perplexity is 46.1349431670904
At time: 393.89802265167236 and batch: 400, loss is 3.8535176849365236 and perplexity is 47.15866111205448
At time: 395.6077206134796 and batch: 450, loss is 3.876224174499512 and perplexity is 48.24171843977429
At time: 397.3189036846161 and batch: 500, loss is 3.85193350315094 and perplexity is 47.08401236429325
At time: 399.0302107334137 and batch: 550, loss is 3.8554352045059206 and perplexity is 47.24917552144484
At time: 400.7396478652954 and batch: 600, loss is 3.8776529836654663 and perplexity is 48.31069591535234
At time: 402.4498538970947 and batch: 650, loss is 3.843900790214539 and perplexity is 46.70731498554522
At time: 404.1610186100006 and batch: 700, loss is 3.8191595792770388 and perplexity is 45.56589769989645
At time: 405.8732969760895 and batch: 750, loss is 3.7960070037841795 and perplexity is 44.5230487171996
At time: 407.582811832428 and batch: 800, loss is 3.7348301362991334 and perplexity is 41.88091091233614
At time: 409.303081035614 and batch: 850, loss is 3.7680765485763548 and perplexity is 43.29670558282234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.570103645324707 and perplexity of 96.554116637024
Finished 13 epochs...
Completing Train Step...
At time: 413.77799916267395 and batch: 50, loss is 3.929437184333801 and perplexity is 50.87833448582263
At time: 415.52263283729553 and batch: 100, loss is 3.873036708831787 and perplexity is 48.08819442474184
At time: 417.2414653301239 and batch: 150, loss is 3.870107774734497 and perplexity is 47.9475533371951
At time: 418.96245288848877 and batch: 200, loss is 3.9126159286499025 and perplexity is 50.02965495177381
At time: 420.6808247566223 and batch: 250, loss is 3.8916311931610106 and perplexity is 48.9907347335503
At time: 422.4265158176422 and batch: 300, loss is 3.869248642921448 and perplexity is 47.906377758922524
At time: 424.1452088356018 and batch: 350, loss is 3.815893931388855 and perplexity is 45.417338225791504
At time: 425.8646252155304 and batch: 400, loss is 3.8392363357543946 and perplexity is 46.489958161431716
At time: 427.58310651779175 and batch: 450, loss is 3.864110450744629 and perplexity is 47.66085689037143
At time: 429.29862427711487 and batch: 500, loss is 3.8415663623809815 and perplexity is 46.598407297439614
At time: 431.01622247695923 and batch: 550, loss is 3.847466344833374 and perplexity is 46.87414972065554
At time: 432.73527812957764 and batch: 600, loss is 3.8709854602813722 and perplexity is 47.98965468493673
At time: 434.4542758464813 and batch: 650, loss is 3.838921728134155 and perplexity is 46.47533436682875
At time: 436.17194080352783 and batch: 700, loss is 3.815759310722351 and perplexity is 45.41122452497318
At time: 437.89039635658264 and batch: 750, loss is 3.7945508670806887 and perplexity is 44.45826425077792
At time: 439.6103882789612 and batch: 800, loss is 3.7343585300445556 and perplexity is 41.8611642694886
At time: 441.330607175827 and batch: 850, loss is 3.76790678024292 and perplexity is 43.289355797170415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.571099599202474 and perplexity of 96.65032798698364
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 445.7790548801422 and batch: 50, loss is 3.923066830635071 and perplexity is 50.55525166805273
At time: 447.49834275245667 and batch: 100, loss is 3.8719597816467286 and perplexity is 48.036434816552486
At time: 449.2148473262787 and batch: 150, loss is 3.8693662738800048 and perplexity is 47.91201336351358
At time: 450.9322769641876 and batch: 200, loss is 3.9129022645950315 and perplexity is 50.043982291427184
At time: 452.65057396888733 and batch: 250, loss is 3.8914166116714477 and perplexity is 48.980223356530196
At time: 454.3702380657196 and batch: 300, loss is 3.86719030380249 and perplexity is 47.807871601850955
At time: 456.0880620479584 and batch: 350, loss is 3.813460474014282 and perplexity is 45.30695143440152
At time: 457.80550718307495 and batch: 400, loss is 3.833910984992981 and perplexity is 46.24304087146215
At time: 459.52373218536377 and batch: 450, loss is 3.8587682056427 and perplexity is 47.40691981208934
At time: 461.2416093349457 and batch: 500, loss is 3.8350252294540406 and perplexity is 46.294595640589755
At time: 462.9593710899353 and batch: 550, loss is 3.8370596313476564 and perplexity is 46.388873320454074
At time: 464.6758255958557 and batch: 600, loss is 3.858996696472168 and perplexity is 47.41775309612561
At time: 466.42426204681396 and batch: 650, loss is 3.824981303215027 and perplexity is 45.83194344833114
At time: 468.14363265037537 and batch: 700, loss is 3.800146498680115 and perplexity is 44.70773363753744
At time: 469.86054825782776 and batch: 750, loss is 3.7782642316818236 and perplexity is 43.740053206820605
At time: 471.57801032066345 and batch: 800, loss is 3.717477660179138 and perplexity is 41.16044243999693
At time: 473.29700994491577 and batch: 850, loss is 3.749491491317749 and perplexity is 42.49946515732896
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.566367785135905 and perplexity of 96.1940769044122
Finished 15 epochs...
Completing Train Step...
At time: 477.7691915035248 and batch: 50, loss is 3.91438817024231 and perplexity is 50.11839820114456
At time: 479.49608302116394 and batch: 100, loss is 3.85943922996521 and perplexity is 47.43874168376943
At time: 481.22376012802124 and batch: 150, loss is 3.8560804748535156 and perplexity is 47.279673852133136
At time: 482.95333337783813 and batch: 200, loss is 3.899214925765991 and perplexity is 49.363679736048276
At time: 484.67871260643005 and batch: 250, loss is 3.8793022203445435 and perplexity is 48.390437425284574
At time: 486.4060056209564 and batch: 300, loss is 3.8563242244720457 and perplexity is 47.2911996592473
At time: 488.1336233615875 and batch: 350, loss is 3.8033869647979737 and perplexity is 44.852842516821745
At time: 489.86238956451416 and batch: 400, loss is 3.825193819999695 and perplexity is 45.841684540624
At time: 491.58679127693176 and batch: 450, loss is 3.8514580583572386 and perplexity is 47.06163183652237
At time: 493.312593460083 and batch: 500, loss is 3.829130573272705 and perplexity is 46.0225076367039
At time: 495.0423746109009 and batch: 550, loss is 3.832744493484497 and perplexity is 46.18913020623875
At time: 496.77053236961365 and batch: 600, loss is 3.855829119682312 and perplexity is 47.26779135504369
At time: 498.49568009376526 and batch: 650, loss is 3.823032250404358 and perplexity is 45.74270156695567
At time: 500.2214813232422 and batch: 700, loss is 3.79934889793396 and perplexity is 44.67208893284044
At time: 501.9479458332062 and batch: 750, loss is 3.7789470529556275 and perplexity is 43.76993004477699
At time: 503.6758773326874 and batch: 800, loss is 3.718829894065857 and perplexity is 41.216138633709996
At time: 505.40154242515564 and batch: 850, loss is 3.750713210105896 and perplexity is 42.55141928260114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.566844622294108 and perplexity of 96.23995675241811
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 509.8771016597748 and batch: 50, loss is 3.911253128051758 and perplexity is 49.96152094515807
At time: 511.5882341861725 and batch: 100, loss is 3.8600282192230226 and perplexity is 47.46669082308874
At time: 513.296950340271 and batch: 150, loss is 3.8572589683532716 and perplexity is 47.33542548545466
At time: 515.0071845054626 and batch: 200, loss is 3.900605263710022 and perplexity is 49.43235966613527
At time: 516.7179493904114 and batch: 250, loss is 3.8798561811447145 and perplexity is 48.41725125694143
At time: 518.4294941425323 and batch: 300, loss is 3.856987509727478 and perplexity is 47.322577619811895
At time: 520.138176202774 and batch: 350, loss is 3.803423342704773 and perplexity is 44.85447419902492
At time: 521.8470706939697 and batch: 400, loss is 3.8245121145248415 and perplexity is 45.81044466270388
At time: 523.5572590827942 and batch: 450, loss is 3.849991478919983 and perplexity is 46.99266280163329
At time: 525.268566608429 and batch: 500, loss is 3.826567873954773 and perplexity is 45.9047167835086
At time: 526.9764499664307 and batch: 550, loss is 3.8273420619964598 and perplexity is 45.94026942673928
At time: 528.6860382556915 and batch: 600, loss is 3.8487360095977783 and perplexity is 46.93370197461642
At time: 530.3968162536621 and batch: 650, loss is 3.8149961280822753 and perplexity is 45.37658068822006
At time: 532.1085031032562 and batch: 700, loss is 3.789684257507324 and perplexity is 44.24242885564499
At time: 533.816458940506 and batch: 750, loss is 3.76953866481781 and perplexity is 43.36005670132021
At time: 535.5263912677765 and batch: 800, loss is 3.7107630825042723 and perplexity is 40.88499324949177
At time: 537.237731218338 and batch: 850, loss is 3.7412258195877075 and perplexity is 42.14962634780058
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.564672787984212 and perplexity of 96.03116632353844
Finished 17 epochs...
Completing Train Step...
At time: 541.6797506809235 and batch: 50, loss is 3.906056957244873 and perplexity is 49.70258566712979
At time: 543.4263091087341 and batch: 100, loss is 3.852673292160034 and perplexity is 47.11885748657446
At time: 545.1458992958069 and batch: 150, loss is 3.8490409517288207 and perplexity is 46.948016220111604
At time: 546.8652577400208 and batch: 200, loss is 3.8921484899520875 and perplexity is 49.016084039412476
At time: 548.5817182064056 and batch: 250, loss is 3.8725145149230955 and perplexity is 48.06308961789236
At time: 550.3011665344238 and batch: 300, loss is 3.8498955011367797 and perplexity is 46.988152766465824
At time: 552.0202970504761 and batch: 350, loss is 3.796947965621948 and perplexity is 44.56496292368537
At time: 553.7662625312805 and batch: 400, loss is 3.818836545944214 and perplexity is 45.55118077325621
At time: 555.4835906028748 and batch: 450, loss is 3.845338521003723 and perplexity is 46.77451582716386
At time: 557.2011172771454 and batch: 500, loss is 3.8228937196731567 and perplexity is 45.73636523595888
At time: 558.9193170070648 and batch: 550, loss is 3.825002703666687 and perplexity is 45.832924283116505
At time: 560.6378934383392 and batch: 600, loss is 3.8474025774002074 and perplexity is 46.87116077174579
At time: 562.3547575473785 and batch: 650, loss is 3.8146713066101072 and perplexity is 45.361843794038364
At time: 564.0727050304413 and batch: 700, loss is 3.790002856254578 and perplexity is 44.25652668370978
At time: 565.7910099029541 and batch: 750, loss is 3.770838112831116 and perplexity is 43.41643746485961
At time: 567.5113916397095 and batch: 800, loss is 3.712712736129761 and perplexity is 40.96478258031352
At time: 569.2280688285828 and batch: 850, loss is 3.743022928237915 and perplexity is 42.22544190990997
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.564827919006348 and perplexity of 96.04606489211231
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 573.6549830436707 and batch: 50, loss is 3.9043946361541746 and perplexity is 49.62003264453832
At time: 575.393559217453 and batch: 100, loss is 3.853360528945923 and perplexity is 47.151250428283944
At time: 577.1036734580994 and batch: 150, loss is 3.850314903259277 and perplexity is 47.007863830610454
At time: 578.8163571357727 and batch: 200, loss is 3.8930140686035157 and perplexity is 49.05852968270482
At time: 580.5292313098907 and batch: 250, loss is 3.872133207321167 and perplexity is 48.044766290083
At time: 582.2415542602539 and batch: 300, loss is 3.8497007656097413 and perplexity is 46.97900339465507
At time: 583.9531967639923 and batch: 350, loss is 3.7969530630111694 and perplexity is 44.56519008922601
At time: 585.6639089584351 and batch: 400, loss is 3.819213013648987 and perplexity is 45.56833255007406
At time: 587.3762338161469 and batch: 450, loss is 3.8449085998535155 and perplexity is 46.75441079561785
At time: 589.087902545929 and batch: 500, loss is 3.821615958213806 and perplexity is 45.6779623915675
At time: 590.7987303733826 and batch: 550, loss is 3.821809115409851 and perplexity is 45.686786270874556
At time: 592.5105619430542 and batch: 600, loss is 3.842497458457947 and perplexity is 46.64181509694702
At time: 594.2238039970398 and batch: 650, loss is 3.8086287641525267 and perplexity is 45.08856939452923
At time: 595.9662058353424 and batch: 700, loss is 3.7837438678741453 and perplexity is 43.980390664971466
At time: 597.677211523056 and batch: 750, loss is 3.764024496078491 and perplexity is 43.12162002631214
At time: 599.3891031742096 and batch: 800, loss is 3.706848392486572 and perplexity is 40.725254043279776
At time: 601.1029057502747 and batch: 850, loss is 3.7375711011886597 and perplexity is 41.99586248583352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.564667383829753 and perplexity of 96.03064735768504
Finished 19 epochs...
Completing Train Step...
At time: 605.5313234329224 and batch: 50, loss is 3.9015289878845216 and perplexity is 49.478042627741125
At time: 607.2698588371277 and batch: 100, loss is 3.849161357879639 and perplexity is 46.953669390364624
At time: 608.9812026023865 and batch: 150, loss is 3.845659279823303 and perplexity is 46.789521572130866
At time: 610.6908714771271 and batch: 200, loss is 3.8882484674453734 and perplexity is 48.82529249595557
At time: 612.39954662323 and batch: 250, loss is 3.8680461025238038 and perplexity is 47.848803029270016
At time: 614.1107165813446 and batch: 300, loss is 3.8457818984985352 and perplexity is 46.795259193043336
At time: 615.8235983848572 and batch: 350, loss is 3.7932094621658323 and perplexity is 44.398667697097295
At time: 617.5329596996307 and batch: 400, loss is 3.815626459121704 and perplexity is 45.40519197183375
At time: 619.2424063682556 and batch: 450, loss is 3.84209810256958 and perplexity is 46.62319213228673
At time: 620.9526989459991 and batch: 500, loss is 3.8193397235870363 and perplexity is 45.57410687649305
At time: 622.664214849472 and batch: 550, loss is 3.8206708431243896 and perplexity is 45.634811854381326
At time: 624.3737137317657 and batch: 600, loss is 3.842052140235901 and perplexity is 46.6210492708186
At time: 626.0836050510406 and batch: 650, loss is 3.8090229845046997 and perplexity is 45.10634773029557
At time: 627.7937121391296 and batch: 700, loss is 3.784450640678406 and perplexity is 44.01148579631641
At time: 629.5053617954254 and batch: 750, loss is 3.7653146409988403 and perplexity is 43.17728906819047
At time: 631.2153482437134 and batch: 800, loss is 3.7086763858795164 and perplexity is 40.799767623013295
At time: 632.9250288009644 and batch: 850, loss is 3.739211401939392 and perplexity is 42.06480485825136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.564695994059245 and perplexity of 96.03339485584732
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 637.3876175880432 and batch: 50, loss is 3.9006143951416017 and perplexity is 49.4328110564063
At time: 639.1036343574524 and batch: 100, loss is 3.8494445371627806 and perplexity is 46.96696757960044
At time: 640.8481647968292 and batch: 150, loss is 3.846330513954163 and perplexity is 46.820938838967976
At time: 642.5676128864288 and batch: 200, loss is 3.88856388092041 and perplexity is 48.840695080094996
At time: 644.2877583503723 and batch: 250, loss is 3.867699408531189 and perplexity is 47.83221701200836
At time: 646.0048699378967 and batch: 300, loss is 3.845188846588135 and perplexity is 46.767515402748295
At time: 647.7235519886017 and batch: 350, loss is 3.792636866569519 and perplexity is 44.373252492503276
At time: 649.4429099559784 and batch: 400, loss is 3.8152810096740724 and perplexity is 45.389509482257104
At time: 651.1630439758301 and batch: 450, loss is 3.84149423122406 and perplexity is 46.595046221631144
At time: 652.8798055648804 and batch: 500, loss is 3.8186573600769043 and perplexity is 45.54301937664768
At time: 654.5992555618286 and batch: 550, loss is 3.8186273527145387 and perplexity is 45.54165277126623
At time: 656.33336353302 and batch: 600, loss is 3.838843836784363 and perplexity is 46.471714481283584
At time: 658.0700657367706 and batch: 650, loss is 3.804968681335449 and perplexity is 44.92384313620771
At time: 659.8033425807953 and batch: 700, loss is 3.7803947496414185 and perplexity is 43.83334151665401
At time: 661.5378637313843 and batch: 750, loss is 3.760841293334961 and perplexity is 42.984573406448995
At time: 663.2736256122589 and batch: 800, loss is 3.704269304275513 and perplexity is 40.62035535032172
At time: 665.0106317996979 and batch: 850, loss is 3.7355393171310425 and perplexity is 41.91062258580746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.564892768859863 and perplexity of 96.0522936673167
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 669.5131874084473 and batch: 50, loss is 3.899805703163147 and perplexity is 49.392851298378154
At time: 671.2121193408966 and batch: 100, loss is 3.8487023401260374 and perplexity is 46.93212176826659
At time: 672.9303755760193 and batch: 150, loss is 3.8458216428756713 and perplexity is 46.79711907843263
At time: 674.6481709480286 and batch: 200, loss is 3.8877988243103028 and perplexity is 48.803343473350424
At time: 676.3777141571045 and batch: 250, loss is 3.866869740486145 and perplexity is 47.79254860810893
At time: 678.1126868724823 and batch: 300, loss is 3.844206166267395 and perplexity is 46.721580459091165
At time: 679.8487620353699 and batch: 350, loss is 3.791557569503784 and perplexity is 44.325386406800455
At time: 681.581324338913 and batch: 400, loss is 3.814122643470764 and perplexity is 45.336962248833466
At time: 683.349892616272 and batch: 450, loss is 3.840579242706299 and perplexity is 46.552431788185544
At time: 685.0853950977325 and batch: 500, loss is 3.8180018424987794 and perplexity is 45.51317490974042
At time: 686.8201398849487 and batch: 550, loss is 3.817435474395752 and perplexity is 45.487404997521054
At time: 688.5559737682343 and batch: 600, loss is 3.8370866346359254 and perplexity is 46.39012598948583
At time: 690.290851354599 and batch: 650, loss is 3.8029451036453246 and perplexity is 44.83302816604599
At time: 692.0256776809692 and batch: 700, loss is 3.7784082794189455 and perplexity is 43.746354316326034
At time: 693.7615385055542 and batch: 750, loss is 3.7585736751556396 and perplexity is 42.88721123819395
At time: 695.4940903186798 and batch: 800, loss is 3.702088165283203 and perplexity is 40.53185326214977
At time: 697.2281415462494 and batch: 850, loss is 3.733660035133362 and perplexity is 41.831934668821795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.564968427022298 and perplexity of 96.05956108226937
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 701.7122614383698 and batch: 50, loss is 3.899309573173523 and perplexity is 49.36835210147168
At time: 703.4142694473267 and batch: 100, loss is 3.8481818294525145 and perplexity is 46.907699454544016
At time: 705.1240043640137 and batch: 150, loss is 3.8454647159576414 and perplexity is 46.780418907494855
At time: 706.835946559906 and batch: 200, loss is 3.8873176622390746 and perplexity is 48.77986680401628
At time: 708.5458333492279 and batch: 250, loss is 3.8663023805618284 and perplexity is 47.76544072203899
At time: 710.2559101581573 and batch: 300, loss is 3.843615732192993 and perplexity is 46.69400258823669
At time: 711.9668855667114 and batch: 350, loss is 3.7909005165100096 and perplexity is 44.296271844914116
At time: 713.6782476902008 and batch: 400, loss is 3.813343915939331 and perplexity is 45.30167085111372
At time: 715.3884346485138 and batch: 450, loss is 3.839987626075745 and perplexity is 46.52489874065439
At time: 717.096480846405 and batch: 500, loss is 3.817644963264465 and perplexity is 45.49693510072509
At time: 718.8059306144714 and batch: 550, loss is 3.8168013525009155 and perplexity is 45.45856958162336
At time: 720.5173947811127 and batch: 600, loss is 3.836184368133545 and perplexity is 46.34828860983567
At time: 722.2276306152344 and batch: 650, loss is 3.8018919467926025 and perplexity is 44.78583680951672
At time: 723.9383535385132 and batch: 700, loss is 3.777408490180969 and perplexity is 43.70263903875329
At time: 725.647866487503 and batch: 750, loss is 3.7574224424362184 and perplexity is 42.83786648646804
At time: 727.3905811309814 and batch: 800, loss is 3.701028699874878 and perplexity is 40.488933905484714
At time: 729.1013784408569 and batch: 850, loss is 3.7326974296569824 and perplexity is 41.791686394134715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5649518966674805 and perplexity of 96.05797319676526
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 733.5284924507141 and batch: 50, loss is 3.8990087127685547 and perplexity is 49.3535013531789
At time: 735.262579202652 and batch: 100, loss is 3.847872772216797 and perplexity is 46.89320453061266
At time: 736.9717853069305 and batch: 150, loss is 3.845244526863098 and perplexity is 46.770119503363105
At time: 738.6815388202667 and batch: 200, loss is 3.886997256278992 and perplexity is 48.76423994756289
At time: 740.3916320800781 and batch: 250, loss is 3.86602520942688 and perplexity is 47.752203355215485
At time: 742.1035373210907 and batch: 300, loss is 3.843308854103088 and perplexity is 46.67967542037179
At time: 743.8145203590393 and batch: 350, loss is 3.790570387840271 and perplexity is 44.281650789163194
At time: 745.5242159366608 and batch: 400, loss is 3.812904920578003 and perplexity is 45.28178799231036
At time: 747.235048532486 and batch: 450, loss is 3.839649806022644 and perplexity is 46.50918435135866
At time: 748.9467475414276 and batch: 500, loss is 3.817471270561218 and perplexity is 45.48903330134031
At time: 750.6563577651978 and batch: 550, loss is 3.8164693307876587 and perplexity is 45.44347885483178
At time: 752.3660576343536 and batch: 600, loss is 3.8357173728942873 and perplexity is 46.32664923284343
At time: 754.0767893791199 and batch: 650, loss is 3.801358118057251 and perplexity is 44.76193522313546
At time: 755.7878980636597 and batch: 700, loss is 3.776879849433899 and perplexity is 43.67954214851945
At time: 757.4977779388428 and batch: 750, loss is 3.7568453407287596 and perplexity is 42.81315181270095
At time: 759.2073657512665 and batch: 800, loss is 3.700490465164185 and perplexity is 40.467147219559074
At time: 760.9175562858582 and batch: 850, loss is 3.7322173929214477 and perplexity is 41.771629663794414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.564935048421224 and perplexity of 96.05635480201151
Annealing...
Model not improving. Stopping early with 96.03064735768504loss at 23 epochs.
Finished Training.
Improved accuracyfrom -105.12128824673842 to -96.03064735768504
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9fb0e81b38>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -106.36539507113342, 'params': {'batch_size': 50, 'num_layers': 1, 'anneal': 5.65209007083128, 'wordvec_source': '', 'dropout': 0.509047440648295, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 9.721940560180443, 'seq_len': 50}}, {'best_accuracy': -124.3972766504035, 'params': {'batch_size': 50, 'num_layers': 1, 'anneal': 4.7874854656094, 'wordvec_source': '', 'dropout': 0.11864408616762978, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 13.47088802086473, 'seq_len': 50}}, {'best_accuracy': -105.12128824673842, 'params': {'batch_size': 50, 'num_layers': 1, 'anneal': 3.3022335888493073, 'wordvec_source': '', 'dropout': 0.11820777557420015, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 11.163034786213991, 'seq_len': 50}}, {'best_accuracy': -112.71276808587925, 'params': {'batch_size': 50, 'num_layers': 1, 'anneal': 6.741463806971062, 'wordvec_source': '', 'dropout': 0.2727588808900182, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 11.43502763358037, 'seq_len': 50}}, {'best_accuracy': -153.9097120076049, 'params': {'batch_size': 50, 'num_layers': 1, 'anneal': 2.386890260775652, 'wordvec_source': '', 'dropout': 0.6838474312591215, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 18.074865051293788, 'seq_len': 50}}, {'best_accuracy': -96.03064735768504, 'params': {'batch_size': 50, 'num_layers': 1, 'anneal': 2.0, 'wordvec_source': '', 'dropout': 0.0, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 7.805828959670697, 'seq_len': 50}}]
