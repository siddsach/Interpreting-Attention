Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 30], 'name': 'lr', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [2, 8], 'name': 'anneal', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'data': 'ptb', 'anneal': 4.140257380267493, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'lr': 11.870599401543625, 'dropout': 0.8000183658531081, 'seq_len': 35, 'batch_size': 80}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.4459874629974365 and batch: 50, loss is 6.637165231704712 and perplexity is 762.92919720594
At time: 2.3914144039154053 and batch: 100, loss is 6.03045612335205 and perplexity is 415.9046899657078
At time: 3.3161537647247314 and batch: 150, loss is 5.9580707836151126 and perplexity is 386.8630612899017
At time: 4.2413976192474365 and batch: 200, loss is 5.889136247634887 and perplexity is 361.0932544808233
At time: 5.168238639831543 and batch: 250, loss is 5.902208423614502 and perplexity is 365.8445160536721
At time: 6.09651780128479 and batch: 300, loss is 5.89721791267395 and perplexity is 364.0233131400817
At time: 7.027021169662476 and batch: 350, loss is 5.937511043548584 and perplexity is 378.9904638986457
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.958580806337554 and perplexity of 387.0604205662122
Finished 1 epochs...
Completing Train Step...
At time: 8.819956541061401 and batch: 50, loss is 5.627979125976562 and perplexity is 278.0995452336323
At time: 9.738941669464111 and batch: 100, loss is 5.431380987167358 and perplexity is 228.46453425898665
At time: 10.65944504737854 and batch: 150, loss is 5.346678123474121 and perplexity is 209.9098438371371
At time: 11.579185247421265 and batch: 200, loss is 5.271797742843628 and perplexity is 194.76578670337838
At time: 12.500418663024902 and batch: 250, loss is 5.2623771381378175 and perplexity is 192.93959065804464
At time: 13.427778244018555 and batch: 300, loss is 5.23139458656311 and perplexity is 187.05348398252883
At time: 14.35326600074768 and batch: 350, loss is 5.260005989074707 and perplexity is 192.48264408672884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.360744607859645 and perplexity of 212.8834021029569
Finished 2 epochs...
Completing Train Step...
At time: 16.17621898651123 and batch: 50, loss is 5.195573329925537 and perplexity is 180.47158290070882
At time: 17.100815057754517 and batch: 100, loss is 5.136276941299439 and perplexity is 170.08136518760162
At time: 18.03587818145752 and batch: 150, loss is 5.114442462921143 and perplexity is 166.40797649763667
At time: 18.96201252937317 and batch: 200, loss is 5.093097581863403 and perplexity is 162.8936578164685
At time: 19.88699984550476 and batch: 250, loss is 5.107057790756226 and perplexity is 165.18363439206283
At time: 20.810728073120117 and batch: 300, loss is 5.091834316253662 and perplexity is 162.68800978189736
At time: 21.759597063064575 and batch: 350, loss is 5.147312355041504 and perplexity is 171.96867790752884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.322624995790679 and perplexity of 204.92109360351435
Finished 3 epochs...
Completing Train Step...
At time: 23.665461778640747 and batch: 50, loss is 5.103895673751831 and perplexity is 164.66212937738456
At time: 24.70488977432251 and batch: 100, loss is 5.05746374130249 and perplexity is 157.19133243648199
At time: 25.728097438812256 and batch: 150, loss is 5.0410651588439945 and perplexity is 154.63463778765836
At time: 26.753726482391357 and batch: 200, loss is 5.0326301383972165 and perplexity is 153.33577711540906
At time: 27.78112483024597 and batch: 250, loss is 5.038800745010376 and perplexity is 154.2848771255881
At time: 28.810550928115845 and batch: 300, loss is 5.015572748184204 and perplexity is 150.74244951006293
At time: 29.83930277824402 and batch: 350, loss is 5.078769464492797 and perplexity is 160.5763394176925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.286260144463901 and perplexity of 197.60303499137373
Finished 4 epochs...
Completing Train Step...
At time: 31.79586124420166 and batch: 50, loss is 5.054149827957153 and perplexity is 156.6712761687914
At time: 32.83340835571289 and batch: 100, loss is 5.005161085128784 and perplexity is 149.18111208163666
At time: 33.85782027244568 and batch: 150, loss is 4.98493143081665 and perplexity is 146.19355031011204
At time: 34.91353225708008 and batch: 200, loss is 4.97778624534607 and perplexity is 145.1526932651553
At time: 35.95244836807251 and batch: 250, loss is 4.991861047744751 and perplexity is 147.21013381024548
At time: 36.99546551704407 and batch: 300, loss is 4.975889587402344 and perplexity is 144.87764917115527
At time: 38.027185916900635 and batch: 350, loss is 5.035498104095459 and perplexity is 153.77617007858447
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.259528850686961 and perplexity of 192.39082513517548
Finished 5 epochs...
Completing Train Step...
At time: 40.00685167312622 and batch: 50, loss is 5.011365184783935 and perplexity is 150.10952356840596
At time: 41.02288556098938 and batch: 100, loss is 4.964923677444458 and perplexity is 143.297613015941
At time: 42.044718980789185 and batch: 150, loss is 4.936504735946655 and perplexity is 139.28256841467189
At time: 43.06507349014282 and batch: 200, loss is 4.859411630630493 and perplexity is 128.94831054725722
At time: 44.0829496383667 and batch: 250, loss is 4.898570156097412 and perplexity is 134.09790347148015
At time: 45.10206699371338 and batch: 300, loss is 4.899074373245239 and perplexity is 134.1655349829492
At time: 46.12719249725342 and batch: 350, loss is 4.992386045455933 and perplexity is 147.287439084329
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.257186363483298 and perplexity of 191.9406795251979
Finished 6 epochs...
Completing Train Step...
At time: 48.03914165496826 and batch: 50, loss is 4.966938047409058 and perplexity is 143.58655834729356
At time: 49.06652045249939 and batch: 100, loss is 4.930438871383667 and perplexity is 138.4402564764691
At time: 50.08984017372131 and batch: 150, loss is 4.9063176345825195 and perplexity is 135.1408590145306
At time: 51.100959062576294 and batch: 200, loss is 4.903617334365845 and perplexity is 134.77643037845417
At time: 52.11604571342468 and batch: 250, loss is 4.915495843887329 and perplexity is 136.386919659617
At time: 53.13831067085266 and batch: 300, loss is 4.894261169433594 and perplexity is 133.52132053001353
At time: 54.15834593772888 and batch: 350, loss is 4.9622272109985355 and perplexity is 142.9117362962831
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.260045544854526 and perplexity of 192.49025803840416
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 56.21567988395691 and batch: 50, loss is 4.859379844665527 and perplexity is 128.94421186591637
At time: 57.23305583000183 and batch: 100, loss is 4.718001623153686 and perplexity is 111.94432205218752
At time: 58.24765110015869 and batch: 150, loss is 4.640895528793335 and perplexity is 103.63711606015022
At time: 59.261046171188354 and batch: 200, loss is 4.6006256866455075 and perplexity is 99.54658112697832
At time: 60.29545998573303 and batch: 250, loss is 4.575327100753785 and perplexity is 97.05978227299211
At time: 61.31074666976929 and batch: 300, loss is 4.554191455841065 and perplexity is 95.02988830349906
At time: 62.330421686172485 and batch: 350, loss is 4.656539850234985 and perplexity is 105.27119713427317
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.008715399380388 and perplexity of 149.71229206546562
Finished 8 epochs...
Completing Train Step...
At time: 64.30743861198425 and batch: 50, loss is 4.670398406982422 and perplexity is 106.74026002713545
At time: 65.31415724754333 and batch: 100, loss is 4.605555362701416 and perplexity is 100.03852509034003
At time: 66.33163619041443 and batch: 150, loss is 4.556664714813232 and perplexity is 95.26521271652089
At time: 67.33634948730469 and batch: 200, loss is 4.55456392288208 and perplexity is 95.06529039745078
At time: 68.33826518058777 and batch: 250, loss is 4.566477861404419 and perplexity is 96.2046661722538
At time: 69.34807634353638 and batch: 300, loss is 4.549497547149659 and perplexity is 94.58487193502644
At time: 70.35703778266907 and batch: 350, loss is 4.642115497589112 and perplexity is 103.76362726199638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.988371750404095 and perplexity of 146.6973689965251
Finished 9 epochs...
Completing Train Step...
At time: 72.33388948440552 and batch: 50, loss is 4.624110021591187 and perplexity is 101.91203320127393
At time: 73.37265300750732 and batch: 100, loss is 4.575808801651001 and perplexity is 97.10654731967485
At time: 74.38409638404846 and batch: 150, loss is 4.5320063400268555 and perplexity is 92.94485311613491
At time: 75.39173197746277 and batch: 200, loss is 4.53866286277771 and perplexity is 93.56560638250416
At time: 76.41603589057922 and batch: 250, loss is 4.551561851501464 and perplexity is 94.78032556625962
At time: 77.42859888076782 and batch: 300, loss is 4.532310199737549 and perplexity is 92.97309960358065
At time: 78.44350671768188 and batch: 350, loss is 4.6210051536560055 and perplexity is 101.59610051561549
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.979608996161099 and perplexity of 145.4175117308253
Finished 10 epochs...
Completing Train Step...
At time: 80.36190438270569 and batch: 50, loss is 4.5985638046264645 and perplexity is 99.34153927998632
At time: 81.37749862670898 and batch: 100, loss is 4.555366449356079 and perplexity is 95.14161343128443
At time: 82.37922835350037 and batch: 150, loss is 4.514705963134766 and perplexity is 91.35070159908128
At time: 83.3680956363678 and batch: 200, loss is 4.522030582427979 and perplexity is 92.02226718914343
At time: 84.36808037757874 and batch: 250, loss is 4.536396064758301 and perplexity is 93.35375225724269
At time: 85.39087772369385 and batch: 300, loss is 4.517382574081421 and perplexity is 91.59553940857334
At time: 86.41341733932495 and batch: 350, loss is 4.60413782119751 and perplexity is 99.89681679146176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.974230009933998 and perplexity of 144.63741288942217
Finished 11 epochs...
Completing Train Step...
At time: 88.36123299598694 and batch: 50, loss is 4.57779091835022 and perplexity is 97.29921471026775
At time: 89.38615155220032 and batch: 100, loss is 4.539477205276489 and perplexity is 93.64183186481786
At time: 90.40114092826843 and batch: 150, loss is 4.498489007949829 and perplexity is 89.88121883793126
At time: 91.40911960601807 and batch: 200, loss is 4.508198356628418 and perplexity is 90.75815729272121
At time: 92.41491079330444 and batch: 250, loss is 4.522540616989136 and perplexity is 92.06921369695895
At time: 93.42766213417053 and batch: 300, loss is 4.503282861709595 and perplexity is 90.31313069077149
At time: 94.42817521095276 and batch: 350, loss is 4.588550386428833 and perplexity is 98.35175473640034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.969537669214709 and perplexity of 143.96031469746242
Finished 12 epochs...
Completing Train Step...
At time: 96.32382106781006 and batch: 50, loss is 4.559529895782471 and perplexity is 95.53855619329494
At time: 97.31274008750916 and batch: 100, loss is 4.524818658828735 and perplexity is 92.27919029483165
At time: 98.30171370506287 and batch: 150, loss is 4.482179975509643 and perplexity is 88.42723191213747
At time: 99.29872298240662 and batch: 200, loss is 4.494352741241455 and perplexity is 89.51021396125468
At time: 100.29597902297974 and batch: 250, loss is 4.508893547058105 and perplexity is 90.82127343141302
At time: 101.29732871055603 and batch: 300, loss is 4.4897543144226075 and perplexity is 89.09955271429739
At time: 102.29954981803894 and batch: 350, loss is 4.573347454071045 and perplexity is 96.86782826023922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.966047089675377 and perplexity of 143.458685765673
Finished 13 epochs...
Completing Train Step...
At time: 104.2247519493103 and batch: 50, loss is 4.544182596206665 and perplexity is 94.08349156705042
At time: 105.23595905303955 and batch: 100, loss is 4.512220268249512 and perplexity is 91.12391360699887
At time: 106.2341480255127 and batch: 150, loss is 4.46944709777832 and perplexity is 87.30843663726064
At time: 107.24176979064941 and batch: 200, loss is 4.48235975265503 and perplexity is 88.4431305365269
At time: 108.25005793571472 and batch: 250, loss is 4.498253955841064 and perplexity is 89.8600945506564
At time: 109.27036905288696 and batch: 300, loss is 4.478861122131348 and perplexity is 88.13424136005005
At time: 110.28759026527405 and batch: 350, loss is 4.5609580993652346 and perplexity is 95.67510218607018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.961906170022899 and perplexity of 142.86586313700852
Finished 14 epochs...
Completing Train Step...
At time: 112.24794435501099 and batch: 50, loss is 4.53045729637146 and perplexity is 92.80098893582884
At time: 113.24983930587769 and batch: 100, loss is 4.4978141021728515 and perplexity is 89.82057794984038
At time: 114.25069451332092 and batch: 150, loss is 4.455651187896729 and perplexity is 86.1122078188383
At time: 115.25228333473206 and batch: 200, loss is 4.470231838226319 and perplexity is 87.37697798900953
At time: 116.25258469581604 and batch: 250, loss is 4.486855220794678 and perplexity is 88.8416188366582
At time: 117.25498366355896 and batch: 300, loss is 4.4657919979095455 and perplexity is 86.98989808171416
At time: 118.26244282722473 and batch: 350, loss is 4.547441673278809 and perplexity is 94.39061711835853
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.959108023807921 and perplexity of 142.46666233439544
Finished 15 epochs...
Completing Train Step...
At time: 120.1814911365509 and batch: 50, loss is 4.515565452575683 and perplexity is 91.42925031358595
At time: 121.17209315299988 and batch: 100, loss is 4.485584802627564 and perplexity is 88.72882449325427
At time: 122.16938877105713 and batch: 150, loss is 4.443764142990112 and perplexity is 85.09464800895974
At time: 123.16337156295776 and batch: 200, loss is 4.459526891708374 and perplexity is 86.44660081644011
At time: 124.16694450378418 and batch: 250, loss is 4.474366197586059 and perplexity is 87.73897361014622
At time: 125.17053961753845 and batch: 300, loss is 4.454364070892334 and perplexity is 86.00144263103881
At time: 126.17707133293152 and batch: 350, loss is 4.536124563217163 and perplexity is 93.32841001001908
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.956274098363416 and perplexity of 142.06349397936705
Finished 16 epochs...
Completing Train Step...
At time: 128.06795930862427 and batch: 50, loss is 4.502861156463623 and perplexity is 90.27505319908232
At time: 129.07039141654968 and batch: 100, loss is 4.473402147293091 and perplexity is 87.65442958582618
At time: 130.05145359039307 and batch: 150, loss is 4.429045829772949 and perplexity is 83.85137024628379
At time: 131.0491383075714 and batch: 200, loss is 4.446495094299316 and perplexity is 85.32735496039373
At time: 132.0479917526245 and batch: 250, loss is 4.461955919265747 and perplexity is 86.65683722370855
At time: 133.05732536315918 and batch: 300, loss is 4.439152774810791 and perplexity is 84.70314862456242
At time: 134.05997776985168 and batch: 350, loss is 4.519088077545166 and perplexity is 91.75188920796201
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.949506036166487 and perplexity of 141.10524581563118
Finished 17 epochs...
Completing Train Step...
At time: 135.98380827903748 and batch: 50, loss is 4.485863075256348 and perplexity is 88.75351873220097
At time: 136.9934904575348 and batch: 100, loss is 4.458166065216065 and perplexity is 86.32904199859763
At time: 137.98678970336914 and batch: 150, loss is 4.414172058105469 and perplexity is 82.61341347698158
At time: 138.97888207435608 and batch: 200, loss is 4.4302675247192385 and perplexity is 83.95387364274463
At time: 139.97785067558289 and batch: 250, loss is 4.443476886749267 and perplexity is 85.07020755076181
At time: 140.97692799568176 and batch: 300, loss is 4.423103866577148 and perplexity is 83.35460582990363
At time: 141.97584223747253 and batch: 350, loss is 4.501162729263306 and perplexity is 90.12185772572472
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.941131065631735 and perplexity of 139.92842832403784
Finished 18 epochs...
Completing Train Step...
At time: 143.87751865386963 and batch: 50, loss is 4.4690163040161135 and perplexity is 87.27083280769845
At time: 144.8624246120453 and batch: 100, loss is 4.44130693435669 and perplexity is 84.88580939032087
At time: 145.8463728427887 and batch: 150, loss is 4.398010168075562 and perplexity is 81.28895626887609
At time: 146.83595061302185 and batch: 200, loss is 4.4136708641052245 and perplexity is 82.57201850412976
At time: 147.82551670074463 and batch: 250, loss is 4.428349609375 and perplexity is 83.79301152954416
At time: 148.82062935829163 and batch: 300, loss is 4.407380437850952 and perplexity is 82.05423555615303
At time: 149.82001066207886 and batch: 350, loss is 4.484638967514038 and perplexity is 88.6449413315401
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.936682865537446 and perplexity of 139.30738097144766
Finished 19 epochs...
Completing Train Step...
At time: 151.7321217060089 and batch: 50, loss is 4.453007946014404 and perplexity is 85.88489298095098
At time: 152.7252230644226 and batch: 100, loss is 4.427586889266967 and perplexity is 83.72912528149807
At time: 153.7231240272522 and batch: 150, loss is 4.382557687759399 and perplexity is 80.04249552953372
At time: 154.71676468849182 and batch: 200, loss is 4.400437755584717 and perplexity is 81.48653204295816
At time: 155.70772743225098 and batch: 250, loss is 4.416150951385498 and perplexity is 82.77705847033727
At time: 156.69748783111572 and batch: 300, loss is 4.395241394042968 and perplexity is 81.06419681527744
At time: 157.68729972839355 and batch: 350, loss is 4.471044874191284 and perplexity is 87.4480475017446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.9365955221241915 and perplexity of 139.29521392066562
Finished 20 epochs...
Completing Train Step...
At time: 159.576997756958 and batch: 50, loss is 4.4402517986297605 and perplexity is 84.79629057567881
At time: 160.57407808303833 and batch: 100, loss is 4.414709806442261 and perplexity is 82.65785064961254
At time: 161.55813574790955 and batch: 150, loss is 4.370694808959961 and perplexity is 79.09857100704848
At time: 162.56712245941162 and batch: 200, loss is 4.389623851776123 and perplexity is 80.61009193419466
At time: 163.57281947135925 and batch: 250, loss is 4.40464220046997 and perplexity is 81.82985891952916
At time: 164.57570433616638 and batch: 300, loss is 4.382280292510987 and perplexity is 80.02029520087797
At time: 165.58042573928833 and batch: 350, loss is 4.457670087814331 and perplexity is 86.28623536109295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.93359375 and perplexity of 138.8777083722429
Finished 21 epochs...
Completing Train Step...
At time: 167.4963436126709 and batch: 50, loss is 4.426888818740845 and perplexity is 83.67069684291916
At time: 168.49036931991577 and batch: 100, loss is 4.400679340362549 and perplexity is 81.50622032679718
At time: 169.48139309883118 and batch: 150, loss is 4.355105333328247 and perplexity is 77.87502774068646
At time: 170.4826500415802 and batch: 200, loss is 4.376359996795654 and perplexity is 79.54795097843275
At time: 171.47351336479187 and batch: 250, loss is 4.3926622104644775 and perplexity is 80.85538676552721
At time: 172.46945476531982 and batch: 300, loss is 4.3691303920745845 and perplexity is 78.97492460942973
At time: 173.45892763137817 and batch: 350, loss is 4.4433002853393555 and perplexity is 85.05518535867606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.925959093817349 and perplexity of 137.82146198691805
Finished 22 epochs...
Completing Train Step...
At time: 175.36963820457458 and batch: 50, loss is 4.414018392562866 and perplexity is 82.6007196173028
At time: 176.36909770965576 and batch: 100, loss is 4.388190336227417 and perplexity is 80.49461889999384
At time: 177.3697350025177 and batch: 150, loss is 4.342499732971191 and perplexity is 76.89952756319393
At time: 178.35815286636353 and batch: 200, loss is 4.364911670684815 and perplexity is 78.64245320094525
At time: 179.34899520874023 and batch: 250, loss is 4.381779918670654 and perplexity is 79.98026515429304
At time: 180.33469676971436 and batch: 300, loss is 4.355832204818726 and perplexity is 77.93165345548715
At time: 181.32997846603394 and batch: 350, loss is 4.434563674926758 and perplexity is 84.31532796951355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.925024624528556 and perplexity of 137.6927322197292
Finished 23 epochs...
Completing Train Step...
At time: 183.22131299972534 and batch: 50, loss is 4.404050750732422 and perplexity is 81.78147498069198
At time: 184.21238112449646 and batch: 100, loss is 4.37788516998291 and perplexity is 79.66936794778312
At time: 185.18876028060913 and batch: 150, loss is 4.330429153442383 and perplexity is 75.97688531562962
At time: 186.17199230194092 and batch: 200, loss is 4.354757080078125 and perplexity is 77.84791223097868
At time: 187.15965342521667 and batch: 250, loss is 4.370011405944824 and perplexity is 79.04453327200795
At time: 188.1644241809845 and batch: 300, loss is 4.346100025177002 and perplexity is 77.17688732140093
At time: 189.15964150428772 and batch: 350, loss is 4.422360544204712 and perplexity is 83.29266950872307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.924031224744073 and perplexity of 137.55601620728913
Finished 24 epochs...
Completing Train Step...
At time: 191.05453824996948 and batch: 50, loss is 4.390675039291382 and perplexity is 80.69487280893141
At time: 192.0596137046814 and batch: 100, loss is 4.36891242980957 and perplexity is 78.95771293179878
At time: 193.05492901802063 and batch: 150, loss is 4.322775917053223 and perplexity is 75.39763564683498
At time: 194.041645526886 and batch: 200, loss is 4.347800235748291 and perplexity is 77.30821589256037
At time: 195.0318455696106 and batch: 250, loss is 4.3628518009185795 and perplexity is 78.48062671721468
At time: 196.03105664253235 and batch: 300, loss is 4.336366729736328 and perplexity is 76.42934579880358
At time: 197.02913522720337 and batch: 350, loss is 4.411534423828125 and perplexity is 82.39579662882079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.920976309940733 and perplexity of 137.13643551503444
Finished 25 epochs...
Completing Train Step...
At time: 198.93604850769043 and batch: 50, loss is 4.381343088150024 and perplexity is 79.9453349632678
At time: 199.91565704345703 and batch: 100, loss is 4.359019289016723 and perplexity is 78.18042441315869
At time: 200.8992371559143 and batch: 150, loss is 4.311008243560791 and perplexity is 74.51558093312954
At time: 201.88534212112427 and batch: 200, loss is 4.336041879653931 and perplexity is 76.40452175178842
At time: 202.8787488937378 and batch: 250, loss is 4.354038906097412 and perplexity is 77.7920239571158
At time: 203.87027287483215 and batch: 300, loss is 4.326397962570191 and perplexity is 75.67122449153801
At time: 204.86394500732422 and batch: 350, loss is 4.404381036758423 and perplexity is 81.8084907202789
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.921157310748923 and perplexity of 137.16125956721388
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 206.82388925552368 and batch: 50, loss is 4.359421167373657 and perplexity is 78.21184974782422
At time: 207.81771564483643 and batch: 100, loss is 4.310277338027954 and perplexity is 74.46113698185933
At time: 208.81103920936584 and batch: 150, loss is 4.236473932266235 and perplexity is 69.16354602446535
At time: 209.80841588974 and batch: 200, loss is 4.254588890075683 and perplexity is 70.42785764483638
At time: 210.80653309822083 and batch: 250, loss is 4.263194131851196 and perplexity is 71.03652148137039
At time: 211.79153513908386 and batch: 300, loss is 4.226791715621948 and perplexity is 68.4971210297346
At time: 212.77923893928528 and batch: 350, loss is 4.323003606796265 and perplexity is 75.41480486967518
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.871221081963901 and perplexity of 130.48014661408342
Finished 27 epochs...
Completing Train Step...
At time: 214.69621205329895 and batch: 50, loss is 4.321952590942383 and perplexity is 75.33558435244198
At time: 215.68428254127502 and batch: 100, loss is 4.283662796020508 and perplexity is 72.50552718726877
At time: 216.65749835968018 and batch: 150, loss is 4.215957441329956 and perplexity is 67.75901010045276
At time: 217.6315679550171 and batch: 200, loss is 4.242052927017212 and perplexity is 69.55048745315987
At time: 218.62159967422485 and batch: 250, loss is 4.259166965484619 and perplexity is 70.75102085637492
At time: 219.62003350257874 and batch: 300, loss is 4.228847713470459 and perplexity is 68.63809583548972
At time: 220.62068247795105 and batch: 350, loss is 4.322147607803345 and perplexity is 75.35027749427978
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.866503485317888 and perplexity of 129.86604359615595
Finished 28 epochs...
Completing Train Step...
At time: 222.5321865081787 and batch: 50, loss is 4.309606285095215 and perplexity is 74.4111863791362
At time: 223.51386523246765 and batch: 100, loss is 4.272982521057129 and perplexity is 71.73526882790226
At time: 224.50440573692322 and batch: 150, loss is 4.208624801635742 and perplexity is 67.26397486891351
At time: 225.4945900440216 and batch: 200, loss is 4.237776803970337 and perplexity is 69.25371597872183
At time: 226.48108315467834 and batch: 250, loss is 4.256557893753052 and perplexity is 70.56666696871615
At time: 227.46351528167725 and batch: 300, loss is 4.227232332229614 and perplexity is 68.52730864893243
At time: 228.44740343093872 and batch: 350, loss is 4.319017086029053 and perplexity is 75.11476064753548
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.863794261011584 and perplexity of 129.51468352581534
Finished 29 epochs...
Completing Train Step...
At time: 230.31745409965515 and batch: 50, loss is 4.301023797988892 and perplexity is 73.77528603802529
At time: 231.30682969093323 and batch: 100, loss is 4.265722332000732 and perplexity is 71.21634324252183
At time: 232.28795409202576 and batch: 150, loss is 4.203009719848633 and perplexity is 66.88734055451735
At time: 233.26613569259644 and batch: 200, loss is 4.2342533779144285 and perplexity is 69.01013500314721
At time: 234.24443316459656 and batch: 250, loss is 4.254199733734131 and perplexity is 70.40045552961122
At time: 235.22837114334106 and batch: 300, loss is 4.2252140140533445 and perplexity is 68.38913821916469
At time: 236.21775102615356 and batch: 350, loss is 4.315734605789185 and perplexity is 74.86860215620251
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.861059254613416 and perplexity of 129.16094399815645
Finished 30 epochs...
Completing Train Step...
At time: 238.0807740688324 and batch: 50, loss is 4.293931379318237 and perplexity is 73.25389197998497
At time: 239.06846690177917 and batch: 100, loss is 4.259153423309326 and perplexity is 70.75006274013582
At time: 240.04550671577454 and batch: 150, loss is 4.197871999740601 and perplexity is 66.54457339481993
At time: 241.0351860523224 and batch: 200, loss is 4.230860204696655 and perplexity is 68.7763684907258
At time: 242.0384979248047 and batch: 250, loss is 4.251504020690918 and perplexity is 70.21093166913366
At time: 243.04149341583252 and batch: 300, loss is 4.222543449401855 and perplexity is 68.20674425992239
At time: 244.04599380493164 and batch: 350, loss is 4.31229419708252 and perplexity is 74.61146614560388
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.859300284550108 and perplexity of 128.93395345762315
Finished 31 epochs...
Completing Train Step...
At time: 245.98981094360352 and batch: 50, loss is 4.287954225540161 and perplexity is 72.81734814726387
At time: 246.99690890312195 and batch: 100, loss is 4.253430433273316 and perplexity is 70.34631725370102
At time: 247.97469210624695 and batch: 150, loss is 4.193194360733032 and perplexity is 66.23402877661026
At time: 248.95254588127136 and batch: 200, loss is 4.227552270889282 and perplexity is 68.54923669183947
At time: 249.93672370910645 and batch: 250, loss is 4.248448429107666 and perplexity is 69.99672317078107
At time: 250.91669511795044 and batch: 300, loss is 4.219331169128418 and perplexity is 67.98799660830403
At time: 251.90681171417236 and batch: 350, loss is 4.308966312408447 and perplexity is 74.36358048712371
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.858038540544181 and perplexity of 128.7713744028302
Finished 32 epochs...
Completing Train Step...
At time: 253.83080339431763 and batch: 50, loss is 4.282625598907471 and perplexity is 72.43036365023097
At time: 254.81958389282227 and batch: 100, loss is 4.24820616722107 and perplexity is 69.97976768648691
At time: 255.8021595478058 and batch: 150, loss is 4.188791456222535 and perplexity is 65.94304772352612
At time: 256.7813436985016 and batch: 200, loss is 4.223931112289429 and perplexity is 68.30145792773654
At time: 257.7622549533844 and batch: 250, loss is 4.245000619888305 and perplexity is 69.7558033846965
At time: 258.7402415275574 and batch: 300, loss is 4.216104784011841 and perplexity is 67.76899463027844
At time: 259.7376971244812 and batch: 350, loss is 4.305626335144043 and perplexity is 74.11562213712996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.856420582738416 and perplexity of 128.56319620963623
Finished 33 epochs...
Completing Train Step...
At time: 261.609406709671 and batch: 50, loss is 4.277827968597412 and perplexity is 72.08370178488386
At time: 262.59634733200073 and batch: 100, loss is 4.243294172286987 and perplexity is 69.63687026676031
At time: 263.5971302986145 and batch: 150, loss is 4.1841560745239255 and perplexity is 65.63808388491306
At time: 264.6017544269562 and batch: 200, loss is 4.22004469871521 and perplexity is 68.03652536672428
At time: 265.6064064502716 and batch: 250, loss is 4.241958427429199 and perplexity is 69.54391527128861
At time: 266.6101496219635 and batch: 300, loss is 4.213137292861939 and perplexity is 67.56818883055014
At time: 267.6105320453644 and batch: 350, loss is 4.302616920471191 and perplexity is 73.8929127768732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.854808938914332 and perplexity of 128.3561650035547
Finished 34 epochs...
Completing Train Step...
At time: 269.520934343338 and batch: 50, loss is 4.272965517044067 and perplexity is 71.73404905082472
At time: 270.52877736091614 and batch: 100, loss is 4.2388897800445555 and perplexity is 69.3308366164083
At time: 271.5188114643097 and batch: 150, loss is 4.179489068984985 and perplexity is 65.33246430272149
At time: 272.5183918476105 and batch: 200, loss is 4.21632493019104 and perplexity is 67.78391535782478
At time: 273.51167726516724 and batch: 250, loss is 4.238542885780334 and perplexity is 69.30679031785506
At time: 274.5015332698822 and batch: 300, loss is 4.209856958389282 and perplexity is 67.3469057112247
At time: 275.4881670475006 and batch: 350, loss is 4.299591217041016 and perplexity is 73.66967263675879
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.853560349036908 and perplexity of 128.19600080573198
Finished 35 epochs...
Completing Train Step...
At time: 277.39563369750977 and batch: 50, loss is 4.2687833118438725 and perplexity is 71.43466900860528
At time: 278.35918045043945 and batch: 100, loss is 4.234367418289184 and perplexity is 69.01800539356753
At time: 279.31983852386475 and batch: 150, loss is 4.174955444335938 and perplexity is 65.03694183232818
At time: 280.28573536872864 and batch: 200, loss is 4.212915382385254 and perplexity is 67.55319640510767
At time: 281.25937700271606 and batch: 250, loss is 4.2350392293930055 and perplexity is 69.0643880343966
At time: 282.2377595901489 and batch: 300, loss is 4.2066552734375 and perplexity is 67.1316269479464
At time: 283.2164583206177 and batch: 350, loss is 4.296294803619385 and perplexity is 73.4272267595752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.852836871969289 and perplexity of 128.10328748101472
Finished 36 epochs...
Completing Train Step...
At time: 285.11105465888977 and batch: 50, loss is 4.264566888809204 and perplexity is 71.13410432394545
At time: 286.09337091445923 and batch: 100, loss is 4.230157823562622 and perplexity is 68.7280782281006
At time: 287.07545018196106 and batch: 150, loss is 4.170501251220703 and perplexity is 64.74789893809759
At time: 288.0553343296051 and batch: 200, loss is 4.209027433395386 and perplexity is 67.29106293436742
At time: 289.06821727752686 and batch: 250, loss is 4.231980919837952 and perplexity is 68.85349041603935
At time: 290.0447654724121 and batch: 300, loss is 4.203881778717041 and perplexity is 66.94569569389236
At time: 291.02614545822144 and batch: 350, loss is 4.293944473266602 and perplexity is 73.25485116894383
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.851467790274785 and perplexity of 127.9280236177681
Finished 37 epochs...
Completing Train Step...
At time: 292.86098766326904 and batch: 50, loss is 4.260977277755737 and perplexity is 70.87921830131816
At time: 293.83957743644714 and batch: 100, loss is 4.226425018310547 and perplexity is 68.4720079243498
At time: 294.81498980522156 and batch: 150, loss is 4.166531982421875 and perplexity is 64.49140650351696
At time: 295.79366064071655 and batch: 200, loss is 4.205440330505371 and perplexity is 67.05011537824294
At time: 296.7652027606964 and batch: 250, loss is 4.227789273262024 and perplexity is 68.5654849489468
At time: 297.74367690086365 and batch: 300, loss is 4.2002464771270756 and perplexity is 66.70276972201154
At time: 298.72183895111084 and batch: 350, loss is 4.291061239242554 and perplexity is 73.0439444824005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.851345193797145 and perplexity of 127.91234105401432
Finished 38 epochs...
Completing Train Step...
At time: 300.58090472221375 and batch: 50, loss is 4.25842511177063 and perplexity is 70.69855341277433
At time: 301.5933132171631 and batch: 100, loss is 4.223449811935425 and perplexity is 68.26859232160534
At time: 302.5842447280884 and batch: 150, loss is 4.162590885162354 and perplexity is 64.23773978901437
At time: 303.5697033405304 and batch: 200, loss is 4.2027007007598876 and perplexity is 66.86667428279127
At time: 304.5521020889282 and batch: 250, loss is 4.225631995201111 and perplexity is 68.4177295645577
At time: 305.5384006500244 and batch: 300, loss is 4.197676258087158 and perplexity is 66.53154912473369
At time: 306.5225028991699 and batch: 350, loss is 4.288523120880127 and perplexity is 72.85878538290109
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.850115019699623 and perplexity of 127.75508335230381
Finished 39 epochs...
Completing Train Step...
At time: 308.39734292030334 and batch: 50, loss is 4.255370931625366 and perplexity is 70.48295669783603
At time: 309.37406182289124 and batch: 100, loss is 4.220254011154175 and perplexity is 68.05076774828932
At time: 310.3488988876343 and batch: 150, loss is 4.158940563201904 and perplexity is 64.00367881563072
At time: 311.32853055000305 and batch: 200, loss is 4.1995697021484375 and perplexity is 66.65764222875943
At time: 312.3210551738739 and batch: 250, loss is 4.2228397321701046 and perplexity is 68.22695573693332
At time: 313.30833745002747 and batch: 300, loss is 4.1949347877502445 and perplexity is 66.34940464226514
At time: 314.3206036090851 and batch: 350, loss is 4.2854234790802 and perplexity is 72.63329889045036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.849248951879041 and perplexity of 127.64448668471697
Finished 40 epochs...
Completing Train Step...
At time: 316.19743943214417 and batch: 50, loss is 4.251964292526245 and perplexity is 70.24325522175283
At time: 317.1773991584778 and batch: 100, loss is 4.216366395950318 and perplexity is 67.78672612761687
At time: 318.1505365371704 and batch: 150, loss is 4.155133867263794 and perplexity is 63.760499420125704
At time: 319.130033493042 and batch: 200, loss is 4.19648549079895 and perplexity is 66.45237268211659
At time: 320.10785126686096 and batch: 250, loss is 4.219765367507935 and perplexity is 68.01752329601403
At time: 321.08541321754456 and batch: 300, loss is 4.192018642425537 and perplexity is 66.15620197656034
At time: 322.0642213821411 and batch: 350, loss is 4.282333106994629 and perplexity is 72.40918145258085
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.848212932718211 and perplexity of 127.51231302977914
Finished 41 epochs...
Completing Train Step...
At time: 323.91777777671814 and batch: 50, loss is 4.2487060070037845 and perplexity is 70.01475510168433
At time: 324.909353017807 and batch: 100, loss is 4.212649478912353 and perplexity is 67.53523616352915
At time: 325.8965528011322 and batch: 150, loss is 4.151157884597779 and perplexity is 63.507492089339706
At time: 326.8696277141571 and batch: 200, loss is 4.193017158508301 and perplexity is 66.22229299919178
At time: 327.84121584892273 and batch: 250, loss is 4.21662823677063 and perplexity is 67.80447778354505
At time: 328.8154389858246 and batch: 300, loss is 4.189175567626953 and perplexity is 65.96838206549218
At time: 329.7891845703125 and batch: 350, loss is 4.27929238319397 and perplexity is 72.18933953980823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.847494191136853 and perplexity of 127.42069755613865
Finished 42 epochs...
Completing Train Step...
At time: 331.66297030448914 and batch: 50, loss is 4.245552558898925 and perplexity is 69.79431496084462
At time: 332.6304295063019 and batch: 100, loss is 4.209076814651489 and perplexity is 67.29438593362588
At time: 333.6041169166565 and batch: 150, loss is 4.1473104429245 and perplexity is 63.26362016005396
At time: 334.58235359191895 and batch: 200, loss is 4.189816865921021 and perplexity is 66.01070104446734
At time: 335.56797909736633 and batch: 250, loss is 4.213601226806641 and perplexity is 67.59954327956488
At time: 336.55597162246704 and batch: 300, loss is 4.18630687713623 and perplexity is 65.7794103754111
At time: 337.5413341522217 and batch: 350, loss is 4.276188306808471 and perplexity is 71.96560573870728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.846871211610991 and perplexity of 127.34134179146496
Finished 43 epochs...
Completing Train Step...
At time: 339.4195935726166 and batch: 50, loss is 4.242171182632446 and perplexity is 69.55871267517325
At time: 340.40367698669434 and batch: 100, loss is 4.205431079864502 and perplexity is 67.04949512457421
At time: 341.3903625011444 and batch: 150, loss is 4.143668675422669 and perplexity is 63.03364777125959
At time: 342.3767442703247 and batch: 200, loss is 4.186620740890503 and perplexity is 65.80005938842424
At time: 343.366836309433 and batch: 250, loss is 4.210603404045105 and perplexity is 67.39719528330721
At time: 344.3572988510132 and batch: 300, loss is 4.183583059310913 and perplexity is 65.60048303825144
At time: 345.3450984954834 and batch: 350, loss is 4.273210620880127 and perplexity is 71.75163349634263
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.846417657260237 and perplexity of 127.28359866767696
Finished 44 epochs...
Completing Train Step...
At time: 347.2453737258911 and batch: 50, loss is 4.23901252746582 and perplexity is 69.33934732013986
At time: 348.2369418144226 and batch: 100, loss is 4.20194790840149 and perplexity is 66.81635650315594
At time: 349.228844165802 and batch: 150, loss is 4.13978138923645 and perplexity is 62.78909357703657
At time: 350.21546053886414 and batch: 200, loss is 4.1829143905639645 and perplexity is 65.55663270776566
At time: 351.1997594833374 and batch: 250, loss is 4.2068416166305544 and perplexity is 67.14413763527135
At time: 352.181693315506 and batch: 300, loss is 4.180434131622315 and perplexity is 65.39423675856104
At time: 353.1619863510132 and batch: 350, loss is 4.269944067001343 and perplexity is 71.51763531154583
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.845980940193965 and perplexity of 127.22802388399381
Finished 45 epochs...
Completing Train Step...
At time: 355.01746821403503 and batch: 50, loss is 4.235758504867554 and perplexity is 69.11408222463808
At time: 356.00245428085327 and batch: 100, loss is 4.198580799102783 and perplexity is 66.59175686583055
At time: 356.97722005844116 and batch: 150, loss is 4.135955348014831 and perplexity is 62.54931890313274
At time: 357.9527473449707 and batch: 200, loss is 4.180001106262207 and perplexity is 65.36592552582295
At time: 358.9319760799408 and batch: 250, loss is 4.2044744968414305 and perplexity is 66.9853873829097
At time: 359.91186141967773 and batch: 300, loss is 4.178185939788818 and perplexity is 65.24738310896493
At time: 360.9031095504761 and batch: 350, loss is 4.267949314117431 and perplexity is 71.37511749342893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.846348203461746 and perplexity of 127.274758645254
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 362.7957196235657 and batch: 50, loss is 4.233609247207641 and perplexity is 68.96569776934201
At time: 363.77579617500305 and batch: 100, loss is 4.190007953643799 and perplexity is 66.02331608426016
At time: 364.76477813720703 and batch: 150, loss is 4.120052366256714 and perplexity is 61.56246597591817
At time: 365.746942281723 and batch: 200, loss is 4.158633766174316 and perplexity is 63.984045689061816
At time: 366.73063802719116 and batch: 250, loss is 4.177265262603759 and perplexity is 65.18733897682432
At time: 367.7186062335968 and batch: 300, loss is 4.14825562953949 and perplexity is 63.32344435510545
At time: 368.7077920436859 and batch: 350, loss is 4.243843936920166 and perplexity is 69.67516468069098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.833899662412446 and perplexity of 125.70019443495234
Finished 47 epochs...
Completing Train Step...
At time: 370.6315746307373 and batch: 50, loss is 4.22432580947876 and perplexity is 68.32842164210894
At time: 371.6164016723633 and batch: 100, loss is 4.182946901321412 and perplexity is 65.55876403819606
At time: 372.6057162284851 and batch: 150, loss is 4.115226058959961 and perplexity is 61.266062439796805
At time: 373.58534026145935 and batch: 200, loss is 4.156035118103027 and perplexity is 63.817989526349855
At time: 374.5636863708496 and batch: 250, loss is 4.177211785316468 and perplexity is 65.18385302798045
At time: 375.5394835472107 and batch: 300, loss is 4.1502136659622195 and perplexity is 63.44755543291834
At time: 376.51650524139404 and batch: 350, loss is 4.243916673660278 and perplexity is 69.68023280935395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.832515321928879 and perplexity of 125.52630295728032
Finished 48 epochs...
Completing Train Step...
At time: 378.35080766677856 and batch: 50, loss is 4.220485229492187 and perplexity is 68.06650415291107
At time: 379.3338449001312 and batch: 100, loss is 4.180399332046509 and perplexity is 65.39196110645779
At time: 380.30373454093933 and batch: 150, loss is 4.1136686849594115 and perplexity is 61.17072252625818
At time: 381.2780203819275 and batch: 200, loss is 4.155593485832214 and perplexity is 63.78981166530098
At time: 382.25982880592346 and batch: 250, loss is 4.177640080451965 and perplexity is 65.21177693455515
At time: 383.25143575668335 and batch: 300, loss is 4.150978231430054 and perplexity is 63.49608379201068
At time: 384.25453996658325 and batch: 350, loss is 4.243285293579102 and perplexity is 69.63625198407594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.83196705785291 and perplexity of 125.45750025752739
Finished 49 epochs...
Completing Train Step...
At time: 386.1426830291748 and batch: 50, loss is 4.217734909057617 and perplexity is 67.87955665623302
At time: 387.12908720970154 and batch: 100, loss is 4.178661804199219 and perplexity is 65.27843940516526
At time: 388.11966276168823 and batch: 150, loss is 4.112698812484741 and perplexity is 61.11142348712548
At time: 389.10847544670105 and batch: 200, loss is 4.155340738296509 and perplexity is 63.77369098491642
At time: 390.10894560813904 and batch: 250, loss is 4.1778202247619625 and perplexity is 65.22352552330365
At time: 391.08764457702637 and batch: 300, loss is 4.151308174133301 and perplexity is 63.517037318084036
At time: 392.068852186203 and batch: 350, loss is 4.242508888244629 and perplexity is 69.58220700968091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.831648201778017 and perplexity of 125.41750374833323
Finished Training.
Improved accuracyfrom -10000000 to -125.41750374833323
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd1bf2898d0>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'anneal': 7.304921433418988, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'lr': 11.529068373196251, 'dropout': 0.37373393642136477, 'seq_len': 35, 'batch_size': 80}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.1365330219268799 and batch: 50, loss is 6.71735182762146 and perplexity is 826.625563449808
At time: 2.06950306892395 and batch: 100, loss is 5.754632940292359 and perplexity is 315.64966398403806
At time: 2.9963645935058594 and batch: 150, loss is 5.494543228149414 and perplexity is 243.36034061624704
At time: 3.924482583999634 and batch: 200, loss is 5.400744705200196 and perplexity is 221.57136012336522
At time: 4.853079080581665 and batch: 250, loss is 5.363366260528564 and perplexity is 213.4422406624716
At time: 5.780424118041992 and batch: 300, loss is 5.344894933700561 and perplexity is 209.53586828402427
At time: 6.707301616668701 and batch: 350, loss is 5.367248048782349 and perplexity is 214.27238843165833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.257619923558728 and perplexity of 192.02391538325963
Finished 1 epochs...
Completing Train Step...
At time: 8.464712619781494 and batch: 50, loss is 5.06668535232544 and perplexity is 158.64759397934222
At time: 9.396503686904907 and batch: 100, loss is 4.9614136600494385 and perplexity is 142.79551759890802
At time: 10.3127601146698 and batch: 150, loss is 4.902419872283936 and perplexity is 134.61513730398468
At time: 11.23770546913147 and batch: 200, loss is 4.886784925460815 and perplexity is 132.52680481552443
At time: 12.182978868484497 and batch: 250, loss is 4.878272123336792 and perplexity is 131.40341871589013
At time: 13.178024768829346 and batch: 300, loss is 4.848325805664063 and perplexity is 127.52670653248636
At time: 14.16100263595581 and batch: 350, loss is 4.896150274276733 and perplexity is 133.7737947032969
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.034051303205819 and perplexity of 153.55384744598183
Finished 2 epochs...
Completing Train Step...
At time: 16.02112627029419 and batch: 50, loss is 4.824315605163574 and perplexity is 124.50123121880995
At time: 16.989383220672607 and batch: 100, loss is 4.760056629180908 and perplexity is 116.7525373123456
At time: 17.960903882980347 and batch: 150, loss is 4.7077914810180665 and perplexity is 110.8071697348635
At time: 18.938486576080322 and batch: 200, loss is 4.727774677276611 and perplexity is 113.04372347461755
At time: 19.916739463806152 and batch: 250, loss is 4.739814500808716 and perplexity is 114.41297619700765
At time: 20.905459880828857 and batch: 300, loss is 4.708132343292236 and perplexity is 110.84494615664642
At time: 21.898733854293823 and batch: 350, loss is 4.766545629501342 and perplexity is 117.51260794664114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.9818572998046875 and perplexity of 145.74482226114424
Finished 3 epochs...
Completing Train Step...
At time: 23.78607988357544 and batch: 50, loss is 4.717095146179199 and perplexity is 111.84289308028178
At time: 24.768738985061646 and batch: 100, loss is 4.655890588760376 and perplexity is 105.20287078482129
At time: 25.75243878364563 and batch: 150, loss is 4.597108325958252 and perplexity is 99.1970549611254
At time: 26.735903024673462 and batch: 200, loss is 4.634019079208374 and perplexity is 102.92690531916655
At time: 27.724626779556274 and batch: 250, loss is 4.650014066696167 and perplexity is 104.58645675267724
At time: 28.713602304458618 and batch: 300, loss is 4.622439746856689 and perplexity is 101.74195418595846
At time: 29.701889514923096 and batch: 350, loss is 4.684206314086914 and perplexity is 108.22434207612855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.957308539028825 and perplexity of 142.21052626958837
Finished 4 epochs...
Completing Train Step...
At time: 31.577590942382812 and batch: 50, loss is 4.623214244842529 and perplexity is 101.82078364723992
At time: 32.580182790756226 and batch: 100, loss is 4.5802508735656735 and perplexity is 97.53886105974819
At time: 33.57245969772339 and batch: 150, loss is 4.526498384475708 and perplexity is 92.43432427223647
At time: 34.56761622428894 and batch: 200, loss is 4.55904146194458 and perplexity is 95.4919033239744
At time: 35.55659341812134 and batch: 250, loss is 4.58620680809021 and perplexity is 98.12152957515828
At time: 36.537824392318726 and batch: 300, loss is 4.555117492675781 and perplexity is 95.11793023921317
At time: 37.52509903907776 and batch: 350, loss is 4.620011072158814 and perplexity is 101.49515589380925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.923999128670528 and perplexity of 137.55160127012775
Finished 5 epochs...
Completing Train Step...
At time: 39.46497440338135 and batch: 50, loss is 4.554703321456909 and perplexity is 95.0785432871434
At time: 40.44198203086853 and batch: 100, loss is 4.528433961868286 and perplexity is 92.61341132310763
At time: 41.42515540122986 and batch: 150, loss is 4.471587858200073 and perplexity is 87.49554328669682
At time: 42.41155290603638 and batch: 200, loss is 4.519149551391601 and perplexity is 91.75752972287965
At time: 43.397074699401855 and batch: 250, loss is 4.53420859336853 and perplexity is 93.14976678258681
At time: 44.391563415527344 and batch: 300, loss is 4.499123106002807 and perplexity is 89.93823041735116
At time: 45.39480113983154 and batch: 350, loss is 4.57934160232544 and perplexity is 97.4502120876695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.952932423558728 and perplexity of 141.58955629471038
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 47.299039363861084 and batch: 50, loss is 4.479293937683106 and perplexity is 88.1723954866071
At time: 48.28685116767883 and batch: 100, loss is 4.348177547454834 and perplexity is 77.33739069107895
At time: 49.28623676300049 and batch: 150, loss is 4.230228385925293 and perplexity is 68.73292801478641
At time: 50.270161628723145 and batch: 200, loss is 4.238657207489013 and perplexity is 69.31471404146545
At time: 51.25247550010681 and batch: 250, loss is 4.189074745178223 and perplexity is 65.96173130695209
At time: 52.234780073165894 and batch: 300, loss is 4.108613862991333 and perplexity is 60.86229559257273
At time: 53.217652320861816 and batch: 350, loss is 4.204832601547241 and perplexity is 67.00937946093369
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.7118988037109375 and perplexity of 111.26322648261957
Finished 7 epochs...
Completing Train Step...
At time: 55.07433295249939 and batch: 50, loss is 4.284697074890136 and perplexity is 72.58055691611293
At time: 56.07839107513428 and batch: 100, loss is 4.215882368087769 and perplexity is 67.75392340281694
At time: 57.06387400627136 and batch: 150, loss is 4.124025421142578 and perplexity is 61.80754356311983
At time: 58.05057334899902 and batch: 200, loss is 4.156864929199219 and perplexity is 63.87096838037588
At time: 59.04707932472229 and batch: 250, loss is 4.1343673944473265 and perplexity is 62.45007230938182
At time: 60.04277563095093 and batch: 300, loss is 4.085684423446655 and perplexity is 59.48263513268781
At time: 61.03915476799011 and batch: 350, loss is 4.194166107177734 and perplexity is 66.29842274082792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.690132930360991 and perplexity of 108.86765066168944
Finished 8 epochs...
Completing Train Step...
At time: 62.89885926246643 and batch: 50, loss is 4.2286488056182865 and perplexity is 68.62444453699013
At time: 63.889270544052124 and batch: 100, loss is 4.1693925952911375 and perplexity is 64.67615557271512
At time: 64.8681378364563 and batch: 150, loss is 4.081469273567199 and perplexity is 59.232434597209846
At time: 65.8482825756073 and batch: 200, loss is 4.122086744308472 and perplexity is 61.68783478605094
At time: 66.82696104049683 and batch: 250, loss is 4.114265351295471 and perplexity is 61.20723192802441
At time: 67.80925273895264 and batch: 300, loss is 4.073349766731262 and perplexity is 58.753443656207615
At time: 68.7955949306488 and batch: 350, loss is 4.179549298286438 and perplexity is 65.33639934991001
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.68237409920528 and perplexity of 108.02623337030217
Finished 9 epochs...
Completing Train Step...
At time: 70.6673653125763 and batch: 50, loss is 4.19286958694458 and perplexity is 66.2125211928988
At time: 71.65054106712341 and batch: 100, loss is 4.139526562690735 and perplexity is 62.77309528769271
At time: 72.64908862113953 and batch: 150, loss is 4.055017209053039 and perplexity is 57.68615571173447
At time: 73.64711213111877 and batch: 200, loss is 4.100589790344238 and perplexity is 60.37588621318347
At time: 74.67176485061646 and batch: 250, loss is 4.098498859405518 and perplexity is 60.24977629425919
At time: 75.68295979499817 and batch: 300, loss is 4.061764121055603 and perplexity is 58.07667504648555
At time: 76.69380259513855 and batch: 350, loss is 4.1652873992919925 and perplexity is 64.4111915144257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.677630720467403 and perplexity of 107.5150373883532
Finished 10 epochs...
Completing Train Step...
At time: 78.5870897769928 and batch: 50, loss is 4.166618671417236 and perplexity is 64.49699744108904
At time: 79.57373547554016 and batch: 100, loss is 4.117467308044434 and perplexity is 61.40352893693623
At time: 80.55884838104248 and batch: 150, loss is 4.034831686019897 and perplexity is 56.53340407524515
At time: 81.54435420036316 and batch: 200, loss is 4.083445677757263 and perplexity is 59.34961759147111
At time: 82.53087520599365 and batch: 250, loss is 4.0847256565093994 and perplexity is 59.42563247927802
At time: 83.5264024734497 and batch: 300, loss is 4.0499712038040165 and perplexity is 57.39580424082152
At time: 84.51453399658203 and batch: 350, loss is 4.1512137174606325 and perplexity is 63.5110379934238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.673744990907866 and perplexity of 107.09807365826502
Finished 11 epochs...
Completing Train Step...
At time: 86.43017339706421 and batch: 50, loss is 4.144983649253845 and perplexity is 63.11658988996853
At time: 87.41944360733032 and batch: 100, loss is 4.098654460906983 and perplexity is 60.25915197932993
At time: 88.4014744758606 and batch: 150, loss is 4.017618083953858 and perplexity is 55.56858835443922
At time: 89.38644528388977 and batch: 200, loss is 4.068321108818054 and perplexity is 58.458734304131184
At time: 90.3748791217804 and batch: 250, loss is 4.072263832092285 and perplexity is 58.68967588666361
At time: 91.36725425720215 and batch: 300, loss is 4.038540654182434 and perplexity is 56.74347400159652
At time: 92.35408592224121 and batch: 350, loss is 4.137796897888183 and perplexity is 62.66461272051141
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.671095223262392 and perplexity of 106.81466429809366
Finished 12 epochs...
Completing Train Step...
At time: 94.24324107170105 and batch: 50, loss is 4.126889905929565 and perplexity is 61.98484414755757
At time: 95.23758292198181 and batch: 100, loss is 4.0825607872009275 and perplexity is 59.297122904793966
At time: 96.23781657218933 and batch: 150, loss is 4.001655278205871 and perplexity is 54.68860000025229
At time: 97.22949051856995 and batch: 200, loss is 4.05351646900177 and perplexity is 57.599648715946834
At time: 98.22983002662659 and batch: 250, loss is 4.059640574455261 and perplexity is 57.95347737492587
At time: 99.22909450531006 and batch: 300, loss is 4.0266197347640995 and perplexity is 56.07105550685576
At time: 100.24309992790222 and batch: 350, loss is 4.124620566368103 and perplexity is 61.844338975793846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.668676442113416 and perplexity of 106.55661520960031
Finished 13 epochs...
Completing Train Step...
At time: 102.13574457168579 and batch: 50, loss is 4.108807802200317 and perplexity is 60.87410032269995
At time: 103.12335085868835 and batch: 100, loss is 4.067529344558716 and perplexity is 58.41246708644475
At time: 104.12767767906189 and batch: 150, loss is 3.9864911890029906 and perplexity is 53.86555333950253
At time: 105.11678409576416 and batch: 200, loss is 4.040131340026855 and perplexity is 56.8338068690136
At time: 106.10323357582092 and batch: 250, loss is 4.0482044553756715 and perplexity is 57.29448981878697
At time: 107.10208702087402 and batch: 300, loss is 4.015899548530578 and perplexity is 55.47317377707886
At time: 108.09908294677734 and batch: 350, loss is 4.112729749679565 and perplexity is 61.11331413238538
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.667522561961207 and perplexity of 106.43373255578533
Finished 14 epochs...
Completing Train Step...
At time: 109.9836630821228 and batch: 50, loss is 4.092172288894654 and perplexity is 59.869805061046485
At time: 110.98677372932434 and batch: 100, loss is 4.052844710350037 and perplexity is 57.5609686468752
At time: 111.97518014907837 and batch: 150, loss is 3.972617840766907 and perplexity is 53.12341761694049
At time: 112.9637668132782 and batch: 200, loss is 4.0271848821640015 and perplexity is 56.1027528740839
At time: 113.95123744010925 and batch: 250, loss is 4.037106442451477 and perplexity is 56.662150177259626
At time: 114.94060349464417 and batch: 300, loss is 4.005692195892334 and perplexity is 54.90981959906762
At time: 115.9389488697052 and batch: 350, loss is 4.1011574077606205 and perplexity is 60.410166345807355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.666523900525323 and perplexity of 106.32749434843453
Finished 15 epochs...
Completing Train Step...
At time: 117.8160047531128 and batch: 50, loss is 4.07668562412262 and perplexity is 58.94976403183046
At time: 118.80999898910522 and batch: 100, loss is 4.039358215332031 and perplexity is 56.78988423044837
At time: 119.79004788398743 and batch: 150, loss is 3.959309720993042 and perplexity is 52.42112825292742
At time: 120.79047441482544 and batch: 200, loss is 4.015218424797058 and perplexity is 55.435402546746566
At time: 121.79806566238403 and batch: 250, loss is 4.027319164276123 and perplexity is 56.11028697607189
At time: 122.7942247390747 and batch: 300, loss is 3.9961941337585447 and perplexity is 54.390751692599004
At time: 123.78993153572083 and batch: 350, loss is 4.090682830810547 and perplexity is 59.78069787308871
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.666030357623923 and perplexity of 106.2750301161148
Finished 16 epochs...
Completing Train Step...
At time: 125.68134713172913 and batch: 50, loss is 4.063049669265747 and perplexity is 58.15138342246339
At time: 126.66781187057495 and batch: 100, loss is 4.026107859611511 and perplexity is 56.04236147127085
At time: 127.65236568450928 and batch: 150, loss is 3.946063070297241 and perplexity is 51.731302902765975
At time: 128.63692331314087 and batch: 200, loss is 4.003345427513122 and perplexity is 54.781110055538115
At time: 129.62302589416504 and batch: 250, loss is 4.017092771530152 and perplexity is 55.53940515042974
At time: 130.60917210578918 and batch: 300, loss is 3.986323595046997 and perplexity is 53.856526554765
At time: 131.59541583061218 and batch: 350, loss is 4.081454038619995 and perplexity is 59.231532201069975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.665445788153287 and perplexity of 106.21292313270982
Finished 17 epochs...
Completing Train Step...
At time: 133.4803867340088 and batch: 50, loss is 4.050293517112732 and perplexity is 57.414306654019605
At time: 134.46126127243042 and batch: 100, loss is 4.014292035102844 and perplexity is 55.384071541061026
At time: 135.44323539733887 and batch: 150, loss is 3.933522505760193 and perplexity is 51.08661399051887
At time: 136.42538046836853 and batch: 200, loss is 3.9915544891357424 and perplexity is 54.13898244553318
At time: 137.4109959602356 and batch: 250, loss is 4.0067075967788695 and perplexity is 54.9656033952331
At time: 138.3977541923523 and batch: 300, loss is 3.977488136291504 and perplexity is 53.3827754219311
At time: 139.38683581352234 and batch: 350, loss is 4.0709194612503055 and perplexity is 58.61082820981053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.6645181590113145 and perplexity of 106.11444261371938
Finished 18 epochs...
Completing Train Step...
At time: 141.2686643600464 and batch: 50, loss is 4.0384913396835325 and perplexity is 56.740675794606865
At time: 142.2633810043335 and batch: 100, loss is 4.00304609298706 and perplexity is 54.7647146319031
At time: 143.24248838424683 and batch: 150, loss is 3.9217951250076295 and perplexity is 50.491001132552555
At time: 144.2230794429779 and batch: 200, loss is 3.980590486526489 and perplexity is 53.54864464698508
At time: 145.21298718452454 and batch: 250, loss is 3.9953633451461794 and perplexity is 54.34558324079048
At time: 146.21046495437622 and batch: 300, loss is 3.9673197984695436 and perplexity is 52.842711755788194
At time: 147.21406865119934 and batch: 350, loss is 4.059533004760742 and perplexity is 57.94724367235309
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.665213223161368 and perplexity of 106.1882245972185
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 149.11108827590942 and batch: 50, loss is 4.027454504966736 and perplexity is 56.11788149497384
At time: 150.0960214138031 and batch: 100, loss is 3.983579955101013 and perplexity is 53.70896615602375
At time: 151.09596967697144 and batch: 150, loss is 3.8885306024551394 and perplexity is 48.83906976376415
At time: 152.0816912651062 and batch: 200, loss is 3.9329546117782592 and perplexity is 51.057610446124926
At time: 153.07664680480957 and batch: 250, loss is 3.937580041885376 and perplexity is 51.29432087642837
At time: 154.07736110687256 and batch: 300, loss is 3.8947327518463135 and perplexity is 49.14291825342522
At time: 155.07921075820923 and batch: 350, loss is 3.9930903720855713 and perplexity is 54.222197473505425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.642238222319504 and perplexity of 103.77636240661924
Finished 20 epochs...
Completing Train Step...
At time: 156.98015999794006 and batch: 50, loss is 4.010924572944641 and perplexity is 55.197881445975526
At time: 157.96311473846436 and batch: 100, loss is 3.965922961235046 and perplexity is 52.768950616582025
At time: 158.96189975738525 and batch: 150, loss is 3.873460831642151 and perplexity is 48.10859405057412
At time: 159.9487600326538 and batch: 200, loss is 3.9230366706848145 and perplexity is 50.5537269471701
At time: 160.93686199188232 and batch: 250, loss is 3.932074828147888 and perplexity is 51.01271055024138
At time: 161.93795442581177 and batch: 300, loss is 3.8943409299850464 and perplexity is 49.123666755552236
At time: 162.94611930847168 and batch: 350, loss is 3.995320520401001 and perplexity is 54.34325595486969
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.638872870083513 and perplexity of 103.4277053992331
Finished 21 epochs...
Completing Train Step...
At time: 164.90250396728516 and batch: 50, loss is 4.00371967792511 and perplexity is 54.801615745437665
At time: 165.90778589248657 and batch: 100, loss is 3.9591738557815552 and perplexity is 52.41400652905918
At time: 166.89923977851868 and batch: 150, loss is 3.8678010416030886 and perplexity is 47.837078594203646
At time: 167.8950071334839 and batch: 200, loss is 3.919382243156433 and perplexity is 50.369319173402516
At time: 168.88243198394775 and batch: 250, loss is 3.931024918556213 and perplexity is 50.95917992221453
At time: 169.87777543067932 and batch: 300, loss is 3.8946791887283325 and perplexity is 49.14028607599133
At time: 170.87419486045837 and batch: 350, loss is 3.9955745553970337 and perplexity is 54.35706279731692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.637620070884967 and perplexity of 103.29821238411098
Finished 22 epochs...
Completing Train Step...
At time: 172.76660299301147 and batch: 50, loss is 3.998513822555542 and perplexity is 54.51706776030239
At time: 173.7723250389099 and batch: 100, loss is 3.954572138786316 and perplexity is 52.173366208398264
At time: 174.7625138759613 and batch: 150, loss is 3.864077634811401 and perplexity is 47.659292880536505
At time: 175.7631335258484 and batch: 200, loss is 3.9171369218826295 and perplexity is 50.25635074218697
At time: 176.75959134101868 and batch: 250, loss is 3.9305479955673217 and perplexity is 50.93488211236718
At time: 177.7484540939331 and batch: 300, loss is 3.894619026184082 and perplexity is 49.137329760286434
At time: 178.74232721328735 and batch: 350, loss is 3.9951307153701783 and perplexity is 54.33294231031994
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.6369607859644395 and perplexity of 103.23013187506113
Finished 23 epochs...
Completing Train Step...
At time: 180.65111470222473 and batch: 50, loss is 3.9943081760406494 and perplexity is 54.28826970338744
At time: 181.63548398017883 and batch: 100, loss is 3.9509230518341063 and perplexity is 51.98332800235979
At time: 182.6228802204132 and batch: 150, loss is 3.861160817146301 and perplexity is 47.52048195465166
At time: 183.60513758659363 and batch: 200, loss is 3.9154064989089967 and perplexity is 50.1694611978052
At time: 184.59062814712524 and batch: 250, loss is 3.9300697898864745 and perplexity is 50.91053058537109
At time: 185.5782754421234 and batch: 300, loss is 3.8941996049880983 and perplexity is 49.11672484404229
At time: 186.57320308685303 and batch: 350, loss is 3.9943177127838134 and perplexity is 54.288787439141174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.636528804384429 and perplexity of 103.18554798999088
Finished 24 epochs...
Completing Train Step...
At time: 188.47393107414246 and batch: 50, loss is 3.9906553745269777 and perplexity is 54.09032717212408
At time: 189.46716499328613 and batch: 100, loss is 3.947767629623413 and perplexity is 51.81955717353535
At time: 190.459627866745 and batch: 150, loss is 3.8585826539993286 and perplexity is 47.39812419625675
At time: 191.4498791694641 and batch: 200, loss is 3.9138883256912234 and perplexity is 50.093353052756456
At time: 192.45124888420105 and batch: 250, loss is 3.929538516998291 and perplexity is 50.88349038424692
At time: 193.46665620803833 and batch: 300, loss is 3.893594059944153 and perplexity is 49.086991458098964
At time: 194.46888041496277 and batch: 350, loss is 3.993246941566467 and perplexity is 54.23068767945311
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.636193637190194 and perplexity of 103.15096937451803
Finished 25 epochs...
Completing Train Step...
At time: 196.36000204086304 and batch: 50, loss is 3.9873841762542725 and perplexity is 53.91367607521484
At time: 197.3603274822235 and batch: 100, loss is 3.944938268661499 and perplexity is 51.67314816104753
At time: 198.34636735916138 and batch: 150, loss is 3.856240429878235 and perplexity is 47.28723707840477
At time: 199.33286046981812 and batch: 200, loss is 3.912474513053894 and perplexity is 50.02258047853292
At time: 200.31844878196716 and batch: 250, loss is 3.928938765525818 and perplexity is 50.85298208557742
At time: 201.31820034980774 and batch: 300, loss is 3.892859334945679 and perplexity is 49.05093926422067
At time: 202.30390048027039 and batch: 350, loss is 3.9921113872528076 and perplexity is 54.16914073968611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.635954758216595 and perplexity of 103.12633171965409
Finished 26 epochs...
Completing Train Step...
At time: 204.18444967269897 and batch: 50, loss is 3.9844209957122803 and perplexity is 53.754156578576655
At time: 205.18449592590332 and batch: 100, loss is 3.942348155975342 and perplexity is 51.539482064315315
At time: 206.17253589630127 and batch: 150, loss is 3.854102830886841 and perplexity is 47.18626388666488
At time: 207.16471028327942 and batch: 200, loss is 3.9110977697372435 and perplexity is 49.95375961038301
At time: 208.16210103034973 and batch: 250, loss is 3.9282620191574096 and perplexity is 50.818579156969776
At time: 209.16211700439453 and batch: 300, loss is 3.891973166465759 and perplexity is 49.00749112196465
At time: 210.15807795524597 and batch: 350, loss is 3.990929117202759 and perplexity is 54.10513602983375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.635779019059806 and perplexity of 103.10820997747136
Finished 27 epochs...
Completing Train Step...
At time: 212.10786819458008 and batch: 50, loss is 3.9816806650161745 and perplexity is 53.60705406008899
At time: 213.08870935440063 and batch: 100, loss is 3.939933385848999 and perplexity is 51.41517620813669
At time: 214.0721082687378 and batch: 150, loss is 3.8520904207229614 and perplexity is 47.09140125290255
At time: 215.06098699569702 and batch: 200, loss is 3.909770493507385 and perplexity is 49.88750115402425
At time: 216.04966068267822 and batch: 250, loss is 3.9275037050247192 and perplexity is 50.78005731786528
At time: 217.03800749778748 and batch: 300, loss is 3.8909952545166018 and perplexity is 48.959589536382126
At time: 218.03055667877197 and batch: 350, loss is 3.989704813957214 and perplexity is 54.03893546924148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.635658527242726 and perplexity of 103.09578703034214
Finished 28 epochs...
Completing Train Step...
At time: 219.90806794166565 and batch: 50, loss is 3.979080982208252 and perplexity is 53.46787371402782
At time: 220.91123461723328 and batch: 100, loss is 3.937639346122742 and perplexity is 51.2973629372118
At time: 221.9001865386963 and batch: 150, loss is 3.8501877307891847 and perplexity is 47.001886104562494
At time: 222.88926005363464 and batch: 200, loss is 3.9084599828720092 and perplexity is 49.82216587383012
At time: 223.87761878967285 and batch: 250, loss is 3.9266920709609985 and perplexity is 50.738859214741936
At time: 224.86225032806396 and batch: 300, loss is 3.8899356174468993 and perplexity is 48.90773761735147
At time: 225.85203051567078 and batch: 350, loss is 3.98846200466156 and perplexity is 53.97181709422237
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.635516462654903 and perplexity of 103.08114181015955
Finished 29 epochs...
Completing Train Step...
At time: 227.7634837627411 and batch: 50, loss is 3.9766227912902834 and perplexity is 53.336600885402014
At time: 228.75633311271667 and batch: 100, loss is 3.935457806587219 and perplexity is 51.18557768819956
At time: 229.73552203178406 and batch: 150, loss is 3.848334746360779 and perplexity is 46.91487298338134
At time: 230.71440649032593 and batch: 200, loss is 3.9071625423431398 and perplexity is 49.757566492579
At time: 231.6941089630127 and batch: 250, loss is 3.9258149671554565 and perplexity is 50.69437547951438
At time: 232.67909240722656 and batch: 300, loss is 3.8888423442840576 and perplexity is 48.85429731810447
At time: 233.67526507377625 and batch: 350, loss is 3.9871986389160154 and perplexity is 53.903674003167794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.635365979424838 and perplexity of 103.06563099406935
Finished 30 epochs...
Completing Train Step...
At time: 235.57401394844055 and batch: 50, loss is 3.974311499595642 and perplexity is 53.21346679696903
At time: 236.55506300926208 and batch: 100, loss is 3.933362956047058 and perplexity is 51.07846378611034
At time: 237.5350317955017 and batch: 150, loss is 3.8465687036514282 and perplexity is 46.83209243249851
At time: 238.51243996620178 and batch: 200, loss is 3.9058449363708494 and perplexity is 49.69204879853292
At time: 239.50706028938293 and batch: 250, loss is 3.9248971271514894 and perplexity is 50.64786750042998
At time: 240.5009307861328 and batch: 300, loss is 3.8876227855682375 and perplexity is 48.79475295031141
At time: 241.4997375011444 and batch: 350, loss is 3.9858900499343872 and perplexity is 53.83318238163677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.635174455313847 and perplexity of 103.045893330899
Finished 31 epochs...
Completing Train Step...
At time: 243.39989924430847 and batch: 50, loss is 3.971987805366516 and perplexity is 53.08995852456519
At time: 244.3811001777649 and batch: 100, loss is 3.9312654399871825 and perplexity is 50.971438171217144
At time: 245.36468720436096 and batch: 150, loss is 3.8444124984741213 and perplexity is 46.731221620495866
At time: 246.34616589546204 and batch: 200, loss is 3.904400796890259 and perplexity is 49.620338341405606
At time: 247.325608253479 and batch: 250, loss is 3.923774037361145 and perplexity is 50.59101732743754
At time: 248.3221025466919 and batch: 300, loss is 3.8862369775772097 and perplexity is 48.7271796243945
At time: 249.3090419769287 and batch: 350, loss is 3.9844746494293215 and perplexity is 53.757040766256516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.6348682272023165 and perplexity of 103.01434261268827
Finished 32 epochs...
Completing Train Step...
At time: 251.17246198654175 and batch: 50, loss is 3.969577374458313 and perplexity is 52.96214295480822
At time: 252.1697814464569 and batch: 100, loss is 3.9293094730377196 and perplexity is 50.87183716268258
At time: 253.19008874893188 and batch: 150, loss is 3.84264449596405 and perplexity is 46.648673697342396
At time: 254.17779064178467 and batch: 200, loss is 3.90308301448822 and perplexity is 49.554992597947816
At time: 255.1628839969635 and batch: 250, loss is 3.922733955383301 and perplexity is 50.5384258765256
At time: 256.15613412857056 and batch: 300, loss is 3.885001440048218 and perplexity is 48.66701254228463
At time: 257.1581447124481 and batch: 350, loss is 3.9830751991271973 and perplexity is 53.681863075305415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.634760889513739 and perplexity of 103.00328588467434
Finished 33 epochs...
Completing Train Step...
At time: 259.06234645843506 and batch: 50, loss is 3.9673770904541015 and perplexity is 52.84573930634046
At time: 260.06147384643555 and batch: 100, loss is 3.927366738319397 and perplexity is 50.77310261701047
At time: 261.04733514785767 and batch: 150, loss is 3.840980610847473 and perplexity is 46.571120201415454
At time: 262.03275537490845 and batch: 200, loss is 3.9017682456970215 and perplexity is 49.489882052267944
At time: 263.02103328704834 and batch: 250, loss is 3.921693916320801 and perplexity is 50.48589126321721
At time: 264.01990818977356 and batch: 300, loss is 3.8837370920181273 and perplexity is 48.60551938341534
At time: 265.00628304481506 and batch: 350, loss is 3.9816924476623536 and perplexity is 53.60768569676086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.634574100889009 and perplexity of 102.98404783934119
Finished 34 epochs...
Completing Train Step...
At time: 266.87585067749023 and batch: 50, loss is 3.9652173137664795 and perplexity is 52.73172747491292
At time: 267.8444323539734 and batch: 100, loss is 3.9255009841918946 and perplexity is 50.67846080786403
At time: 268.8323407173157 and batch: 150, loss is 3.839348096847534 and perplexity is 46.495154220329056
At time: 269.8168432712555 and batch: 200, loss is 3.9004164361953735 and perplexity is 49.42302635773675
At time: 270.81590604782104 and batch: 250, loss is 3.920663819313049 and perplexity is 50.43391267378422
At time: 271.7966117858887 and batch: 300, loss is 3.8824935245513914 and perplexity is 48.54511270847889
At time: 272.78935265541077 and batch: 350, loss is 3.980298204421997 and perplexity is 53.53299562351112
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.634477812668373 and perplexity of 102.9741321660098
Finished 35 epochs...
Completing Train Step...
At time: 274.7284154891968 and batch: 50, loss is 3.9631393718719483 and perplexity is 52.6222677740567
At time: 275.7272734642029 and batch: 100, loss is 3.923631539344788 and perplexity is 50.58380872144111
At time: 276.70268845558167 and batch: 150, loss is 3.837705821990967 and perplexity is 46.41885906355737
At time: 277.6938319206238 and batch: 200, loss is 3.8990604162216185 and perplexity is 49.35605316558784
At time: 278.6787323951721 and batch: 250, loss is 3.9196355295181275 and perplexity is 50.38207865082957
At time: 279.66073870658875 and batch: 300, loss is 3.881202836036682 and perplexity is 48.48249650675956
At time: 280.6533784866333 and batch: 350, loss is 3.9788811302185056 and perplexity is 53.45718912078274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.634479391163793 and perplexity of 102.97429471033405
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 282.52519607543945 and batch: 50, loss is 3.961756329536438 and perplexity is 52.54953925483812
At time: 283.51528811454773 and batch: 100, loss is 3.9221713876724245 and perplexity is 50.510002585731385
At time: 284.5046818256378 and batch: 150, loss is 3.8334782028198244 and perplexity is 46.223032037786
At time: 285.48731207847595 and batch: 200, loss is 3.8922785758972167 and perplexity is 49.022460757782994
At time: 286.4732594490051 and batch: 250, loss is 3.9093264865875246 and perplexity is 49.86535567503408
At time: 287.45995450019836 and batch: 300, loss is 3.8675935077667236 and perplexity is 47.82715181186974
At time: 288.4503185749054 and batch: 350, loss is 3.966799464225769 and perplexity is 52.815223035609165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.632787770238416 and perplexity of 102.800248490244
Finished 37 epochs...
Completing Train Step...
At time: 290.3479425907135 and batch: 50, loss is 3.9590120697021485 and perplexity is 52.405527358361226
At time: 291.34028220176697 and batch: 100, loss is 3.9195265579223633 and perplexity is 50.37658873444898
At time: 292.3313100337982 and batch: 150, loss is 3.8310943841934204 and perplexity is 46.11297594204792
At time: 293.31800603866577 and batch: 200, loss is 3.8908101511001587 and perplexity is 48.950527787797476
At time: 294.3073375225067 and batch: 250, loss is 3.9086296653747556 and perplexity is 49.83062054091209
At time: 295.3044500350952 and batch: 300, loss is 3.8679123067855836 and perplexity is 47.84240149160454
At time: 296.2998173236847 and batch: 350, loss is 3.9674103260040283 and perplexity is 52.847495692734675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.632402091190733 and perplexity of 102.76060823300445
Finished 38 epochs...
Completing Train Step...
At time: 298.2201600074768 and batch: 50, loss is 3.957788701057434 and perplexity is 52.341455279258604
At time: 299.2022006511688 and batch: 100, loss is 3.918384118080139 and perplexity is 50.319069374823506
At time: 300.19089126586914 and batch: 150, loss is 3.8300685119628906 and perplexity is 46.06569417723331
At time: 301.1802456378937 and batch: 200, loss is 3.890376238822937 and perplexity is 48.929292160346776
At time: 302.1723942756653 and batch: 250, loss is 3.908608660697937 and perplexity is 49.82957387582442
At time: 303.1854567527771 and batch: 300, loss is 3.868284888267517 and perplexity is 47.860230005532365
At time: 304.1723356246948 and batch: 350, loss is 3.9676073837280272 and perplexity is 52.85791072610261
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.632180049501616 and perplexity of 102.73779362696827
Finished 39 epochs...
Completing Train Step...
At time: 306.04451608657837 and batch: 50, loss is 3.9569004201889038 and perplexity is 52.29498200961694
At time: 307.03075075149536 and batch: 100, loss is 3.917647261619568 and perplexity is 50.2820051006667
At time: 308.02532029151917 and batch: 150, loss is 3.8294180822372437 and perplexity is 46.03574142254487
At time: 309.00663781166077 and batch: 200, loss is 3.890193305015564 and perplexity is 48.92034215729385
At time: 309.98774814605713 and batch: 250, loss is 3.9086765241622925 and perplexity is 49.8329555980814
At time: 310.97608256340027 and batch: 300, loss is 3.8685333728790283 and perplexity is 47.87212401386996
At time: 311.9714000225067 and batch: 350, loss is 3.967647008895874 and perplexity is 52.860005271185244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.632061136179957 and perplexity of 102.72557746101502
Finished 40 epochs...
Completing Train Step...
At time: 313.8794593811035 and batch: 50, loss is 3.956182804107666 and perplexity is 52.25746775158611
At time: 314.87727546691895 and batch: 100, loss is 3.9170679664611816 and perplexity is 50.25288541381907
At time: 315.8648490905762 and batch: 150, loss is 3.8289057540893556 and perplexity is 46.012162057107275
At time: 316.86093640327454 and batch: 200, loss is 3.8900537586212156 and perplexity is 48.91351597623107
At time: 317.8508417606354 and batch: 250, loss is 3.908758935928345 and perplexity is 49.837062589189706
At time: 318.8500919342041 and batch: 300, loss is 3.868683514595032 and perplexity is 47.879312156324644
At time: 319.85768580436707 and batch: 350, loss is 3.967527804374695 and perplexity is 52.853704495115316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.631940118197737 and perplexity of 102.71314657110406
Finished 41 epochs...
Completing Train Step...
At time: 321.7713146209717 and batch: 50, loss is 3.9555519151687624 and perplexity is 52.22450949080079
At time: 322.7637755870819 and batch: 100, loss is 3.9166419458389283 and perplexity is 50.231481207945336
At time: 323.745888710022 and batch: 150, loss is 3.8284982347488405 and perplexity is 45.99341503131733
At time: 324.72705912590027 and batch: 200, loss is 3.8900055646896363 and perplexity is 48.91115869839251
At time: 325.7142255306244 and batch: 250, loss is 3.9087696409225465 and perplexity is 49.83759609751135
At time: 326.7138125896454 and batch: 300, loss is 3.8687638521194456 and perplexity is 47.88315881624742
At time: 327.7320468425751 and batch: 350, loss is 3.9673911809921263 and perplexity is 52.84648393648572
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.631850143958783 and perplexity of 102.70390544964839
Finished 42 epochs...
Completing Train Step...
At time: 329.6134719848633 and batch: 50, loss is 3.954959845542908 and perplexity is 52.19359809675788
At time: 330.6179234981537 and batch: 100, loss is 3.9162179327011106 and perplexity is 50.21018691483018
At time: 331.60326075553894 and batch: 150, loss is 3.8281330394744875 and perplexity is 45.976621520138636
At time: 332.58895087242126 and batch: 200, loss is 3.8899628639221193 and perplexity is 48.909070198966525
At time: 333.5890965461731 and batch: 250, loss is 3.908831224441528 and perplexity is 49.840665366563854
At time: 334.596360206604 and batch: 300, loss is 3.8688193702697755 and perplexity is 47.885817274452506
At time: 335.60140204429626 and batch: 350, loss is 3.967212953567505 and perplexity is 52.83706608303824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.631752803407866 and perplexity of 102.693908681464
Finished 43 epochs...
Completing Train Step...
At time: 337.4803190231323 and batch: 50, loss is 3.954406533241272 and perplexity is 52.16472672504347
At time: 338.4927382469177 and batch: 100, loss is 3.9158392000198363 and perplexity is 50.191174276693616
At time: 339.4865565299988 and batch: 150, loss is 3.827791919708252 and perplexity is 45.96094066043105
At time: 340.48194432258606 and batch: 200, loss is 3.88991081237793 and perplexity is 48.906524472592864
At time: 341.4790678024292 and batch: 250, loss is 3.908799395561218 and perplexity is 49.83907901923729
At time: 342.4762797355652 and batch: 300, loss is 3.8688250207901 and perplexity is 47.88608785500073
At time: 343.4829595088959 and batch: 350, loss is 3.9670331621170045 and perplexity is 52.82756728421398
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.6317117625269395 and perplexity of 102.68969411947117
Finished 44 epochs...
Completing Train Step...
At time: 345.3823266029358 and batch: 50, loss is 3.9538944578170776 and perplexity is 52.13802128866015
At time: 346.3632814884186 and batch: 100, loss is 3.915477933883667 and perplexity is 50.1730451800044
At time: 347.34571528434753 and batch: 150, loss is 3.8274754762649534 and perplexity is 45.94639892305129
At time: 348.3319263458252 and batch: 200, loss is 3.8898610782623293 and perplexity is 48.90409221033483
At time: 349.32994389533997 and batch: 250, loss is 3.908788447380066 and perplexity is 49.83853337495863
At time: 350.32812237739563 and batch: 300, loss is 3.868811902999878 and perplexity is 47.88545969946571
At time: 351.3259656429291 and batch: 350, loss is 3.9668329763412475 and perplexity is 52.816993015120275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.631657041352371 and perplexity of 102.68407497253743
Finished 45 epochs...
Completing Train Step...
At time: 353.234929561615 and batch: 50, loss is 3.9534010076522828 and perplexity is 52.11230012004254
At time: 354.2229471206665 and batch: 100, loss is 3.915136137008667 and perplexity is 50.15589912035437
At time: 355.2089149951935 and batch: 150, loss is 3.827169737815857 and perplexity is 45.932353489526925
At time: 356.2037925720215 and batch: 200, loss is 3.889813222885132 and perplexity is 48.901751942553254
At time: 357.19217896461487 and batch: 250, loss is 3.908767032623291 and perplexity is 49.83746610631608
At time: 358.19201731681824 and batch: 300, loss is 3.8687824487686155 and perplexity is 47.88404929083297
At time: 359.19094705581665 and batch: 350, loss is 3.966623935699463 and perplexity is 52.80595327092107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.631610738820043 and perplexity of 102.67932054990817
Finished 46 epochs...
Completing Train Step...
At time: 361.0791780948639 and batch: 50, loss is 3.9529279088974 and perplexity is 52.08765168677294
At time: 362.07976484298706 and batch: 100, loss is 3.914808945655823 and perplexity is 50.13949122827464
At time: 363.0713276863098 and batch: 150, loss is 3.8268743801116942 and perplexity is 45.91878901833761
At time: 364.07546615600586 and batch: 200, loss is 3.889762291908264 and perplexity is 48.89926139197989
At time: 365.083753824234 and batch: 250, loss is 3.9087344121932985 and perplexity is 49.83584041325751
At time: 366.08908867836 and batch: 300, loss is 3.8687391328811644 and perplexity is 47.88197519566415
At time: 367.0907506942749 and batch: 350, loss is 3.9664083337783813 and perplexity is 52.79456943318405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.631568119443696 and perplexity of 102.67494451455528
Finished 47 epochs...
Completing Train Step...
At time: 368.99320101737976 and batch: 50, loss is 3.952470407485962 and perplexity is 52.0638269629457
At time: 369.9817638397217 and batch: 100, loss is 3.914492630958557 and perplexity is 50.12363387837435
At time: 370.9741656780243 and batch: 150, loss is 3.826586675643921 and perplexity is 45.90557987783779
At time: 371.9655306339264 and batch: 200, loss is 3.889709177017212 and perplexity is 48.896664182014405
At time: 372.96596121788025 and batch: 250, loss is 3.9086924600601196 and perplexity is 49.83374973729787
At time: 373.9670763015747 and batch: 300, loss is 3.8686846399307253 and perplexity is 47.87936603665391
At time: 374.976194858551 and batch: 350, loss is 3.9661876916885377 and perplexity is 52.78292201405462
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.63152970938847 and perplexity of 102.671000840005
Finished 48 epochs...
Completing Train Step...
At time: 376.87778210639954 and batch: 50, loss is 3.95202582359314 and perplexity is 52.040685368650514
At time: 377.87290048599243 and batch: 100, loss is 3.9141851043701172 and perplexity is 50.10822189816584
At time: 378.87561655044556 and batch: 150, loss is 3.826305046081543 and perplexity is 45.89265332980056
At time: 379.8721287250519 and batch: 200, loss is 3.889653344154358 and perplexity is 48.89393421748069
At time: 380.87025332450867 and batch: 250, loss is 3.9086399984359743 and perplexity is 49.83113544642497
At time: 381.865980386734 and batch: 300, loss is 3.868620266914368 and perplexity is 47.87628399664202
At time: 382.87186098098755 and batch: 350, loss is 3.9659631633758545 and perplexity is 52.7710720840085
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.631493403993804 and perplexity of 102.66727339646225
Finished 49 epochs...
Completing Train Step...
At time: 384.7914180755615 and batch: 50, loss is 3.9515922117233275 and perplexity is 52.01812480137986
At time: 385.807177066803 and batch: 100, loss is 3.913884425163269 and perplexity is 50.093157662613606
At time: 386.8083670139313 and batch: 150, loss is 3.826026659011841 and perplexity is 45.879879186679844
At time: 387.8104639053345 and batch: 200, loss is 3.889590163230896 and perplexity is 48.890845151151204
At time: 388.81376338005066 and batch: 250, loss is 3.9085976600646974 and perplexity is 49.82902572197276
At time: 389.82507586479187 and batch: 300, loss is 3.868558979034424 and perplexity is 47.873349850610985
At time: 390.82814049720764 and batch: 350, loss is 3.9657315492630003 and perplexity is 52.75885097430877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.631449206122037 and perplexity of 102.66273582175432
Finished Training.
Improved accuracyfrom -125.41750374833323 to -102.66273582175432
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd1aa376c50>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'anneal': 2.0385820949287394, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'lr': 17.40382707577365, 'dropout': 0.5185250741563281, 'seq_len': 35, 'batch_size': 80}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.1752023696899414 and batch: 50, loss is 6.646001558303833 and perplexity is 769.7005616992184
At time: 2.1276772022247314 and batch: 100, loss is 5.813650951385498 and perplexity is 334.8393790711511
At time: 3.060584306716919 and batch: 150, loss is 5.749743146896362 and perplexity is 314.10996980270994
At time: 3.987809181213379 and batch: 200, loss is 5.762432355880737 and perplexity is 318.12117252624904
At time: 4.914879560470581 and batch: 250, loss is 5.822246685028076 and perplexity is 337.7299747840901
At time: 5.8366858959198 and batch: 300, loss is 5.841537199020386 and perplexity is 344.3082043108635
At time: 6.767123699188232 and batch: 350, loss is 5.901480875015259 and perplexity is 365.5784431906826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.910438800680226 and perplexity of 368.8679794228012
Finished 1 epochs...
Completing Train Step...
At time: 8.527494192123413 and batch: 50, loss is 5.708832740783691 and perplexity is 301.5189120649799
At time: 9.45319652557373 and batch: 100, loss is 5.666106195449829 and perplexity is 288.90739246131864
At time: 10.36696720123291 and batch: 150, loss is 5.673995313644409 and perplexity is 291.1956312529024
At time: 11.308566093444824 and batch: 200, loss is 5.643808526992798 and perplexity is 282.53672072114233
At time: 12.28506875038147 and batch: 250, loss is 5.651752367019653 and perplexity is 284.7900855674
At time: 13.279410123825073 and batch: 300, loss is 5.6279261016845705 and perplexity is 278.08479959308505
At time: 14.27561616897583 and batch: 350, loss is 5.665524673461914 and perplexity is 288.73943530025554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.817888457199623 and perplexity of 336.26127340182836
Finished 2 epochs...
Completing Train Step...
At time: 16.18022894859314 and batch: 50, loss is 5.660135278701782 and perplexity is 287.1874902792133
At time: 17.166368007659912 and batch: 100, loss is 5.636625070571899 and perplexity is 280.51440281312347
At time: 18.154277563095093 and batch: 150, loss is 5.645799617767334 and perplexity is 283.0998374014146
At time: 19.14668560028076 and batch: 200, loss is 5.633299179077149 and perplexity is 279.5829920907229
At time: 20.151544332504272 and batch: 250, loss is 5.640029306411743 and perplexity is 281.47096725846865
At time: 21.156076908111572 and batch: 300, loss is 5.645283470153808 and perplexity is 282.9537537995447
At time: 22.161890745162964 and batch: 350, loss is 5.7075149440765385 and perplexity is 301.1218331276379
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.821825224777748 and perplexity of 337.5876650154362
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 24.04725408554077 and batch: 50, loss is 5.564816160202026 and perplexity is 261.0772029174357
At time: 25.035462141036987 and batch: 100, loss is 5.415371704101562 and perplexity is 224.8361026499535
At time: 26.02330780029297 and batch: 150, loss is 5.390116147994995 and perplexity is 219.2288470663059
At time: 27.01730966567993 and batch: 200, loss is 5.328830919265747 and perplexity is 206.19677251747027
At time: 28.01626944541931 and batch: 250, loss is 5.323055057525635 and perplexity is 205.0092412776709
At time: 29.012441158294678 and batch: 300, loss is 5.345001802444458 and perplexity is 209.55826231565885
At time: 30.01634645462036 and batch: 350, loss is 5.38945818901062 and perplexity is 219.08465091952158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.5662384033203125 and perplexity of 261.4487823481354
Finished 4 epochs...
Completing Train Step...
At time: 31.916248559951782 and batch: 50, loss is 5.342393560409546 and perplexity is 209.0123958325347
At time: 32.932653188705444 and batch: 100, loss is 5.308781290054322 and perplexity is 202.10377236200992
At time: 33.933366537094116 and batch: 150, loss is 5.327136497497559 and perplexity is 205.84768405265476
At time: 34.933754205703735 and batch: 200, loss is 5.298946285247803 and perplexity is 200.12582330211973
At time: 35.93401312828064 and batch: 250, loss is 5.295692014694214 and perplexity is 199.47561827369455
At time: 36.93249988555908 and batch: 300, loss is 5.295004653930664 and perplexity is 199.33855367221946
At time: 37.9307005405426 and batch: 350, loss is 5.348163414001465 and perplexity is 210.2218525942503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.545403316103179 and perplexity of 256.05782966644017
Finished 5 epochs...
Completing Train Step...
At time: 39.826340675354004 and batch: 50, loss is 5.32534460067749 and perplexity is 205.4791565224456
At time: 40.82088351249695 and batch: 100, loss is 5.29506031036377 and perplexity is 199.34964845384243
At time: 41.83170533180237 and batch: 150, loss is 5.292742309570312 and perplexity is 198.88809096253374
At time: 42.83037257194519 and batch: 200, loss is 5.265000982284546 and perplexity is 193.44649880683463
At time: 43.83249878883362 and batch: 250, loss is 5.258071508407593 and perplexity is 192.11065005662456
At time: 44.83668780326843 and batch: 300, loss is 5.268731021881104 and perplexity is 194.16940931143702
At time: 45.8396635055542 and batch: 350, loss is 5.31951229095459 and perplexity is 204.28422642841946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.529020638301454 and perplexity of 251.89709181690156
Finished 6 epochs...
Completing Train Step...
At time: 47.750072717666626 and batch: 50, loss is 5.287217931747437 and perplexity is 197.79238733063258
At time: 48.760825872421265 and batch: 100, loss is 5.253839139938354 and perplexity is 191.29928520738318
At time: 49.76182699203491 and batch: 150, loss is 5.248279180526733 and perplexity is 190.238620305433
At time: 50.767582178115845 and batch: 200, loss is 5.232241201400757 and perplexity is 187.2119132923412
At time: 51.772982597351074 and batch: 250, loss is 5.227277431488037 and perplexity is 186.28493897675696
At time: 52.78207612037659 and batch: 300, loss is 5.228744163513183 and perplexity is 186.5583695382178
At time: 53.79551649093628 and batch: 350, loss is 5.278565120697022 and perplexity is 196.08831033614334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.502535589810075 and perplexity of 245.31315788812532
Finished 7 epochs...
Completing Train Step...
At time: 55.73403191566467 and batch: 50, loss is 5.2640744972229 and perplexity is 193.26735651460552
At time: 56.76015877723694 and batch: 100, loss is 5.234211750030518 and perplexity is 187.58118718814632
At time: 57.757362604141235 and batch: 150, loss is 5.227204542160035 and perplexity is 186.27136128757826
At time: 58.75769019126892 and batch: 200, loss is 5.194811544418335 and perplexity is 180.33415461647684
At time: 59.76190805435181 and batch: 250, loss is 5.197971572875977 and perplexity is 180.90491701470125
At time: 60.769510984420776 and batch: 300, loss is 5.223123474121094 and perplexity is 185.5127242660404
At time: 61.781110763549805 and batch: 350, loss is 5.260911645889283 and perplexity is 192.6570462673746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.478095745218211 and perplexity of 239.3904127233611
Finished 8 epochs...
Completing Train Step...
At time: 63.751317501068115 and batch: 50, loss is 5.226276750564575 and perplexity is 186.09862043024995
At time: 64.76497673988342 and batch: 100, loss is 5.190263662338257 and perplexity is 179.51587826882235
At time: 65.76368737220764 and batch: 150, loss is 5.196975660324097 and perplexity is 180.7248412219046
At time: 66.76742839813232 and batch: 200, loss is 5.1793900489807125 and perplexity is 177.57446622754838
At time: 67.76336359977722 and batch: 250, loss is 5.171691083908081 and perplexity is 176.2125759143063
At time: 68.75537014007568 and batch: 300, loss is 5.180134592056274 and perplexity is 177.70672729769677
At time: 69.75174498558044 and batch: 350, loss is 5.231387643814087 and perplexity is 187.05218532164383
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.466765305091595 and perplexity of 236.69332244910328
Finished 9 epochs...
Completing Train Step...
At time: 71.6462459564209 and batch: 50, loss is 5.204827795028686 and perplexity is 182.14950301790276
At time: 72.63715696334839 and batch: 100, loss is 5.171939201354981 and perplexity is 176.2563027532252
At time: 73.63556933403015 and batch: 150, loss is 5.1717053890228275 and perplexity is 176.2150966734543
At time: 74.6343743801117 and batch: 200, loss is 5.156423196792603 and perplexity is 173.54261638290936
At time: 75.63302445411682 and batch: 250, loss is 5.153940467834473 and perplexity is 173.1122915147931
At time: 76.63092708587646 and batch: 300, loss is 5.164915332794189 and perplexity is 175.02263926663156
At time: 77.62811326980591 and batch: 350, loss is 5.212888631820679 and perplexity is 183.62371413860603
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.473527579471983 and perplexity of 238.29933165696164
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 79.5194456577301 and batch: 50, loss is 5.140248241424561 and perplexity is 170.75815230735884
At time: 80.50521850585938 and batch: 100, loss is 5.037365388870239 and perplexity is 154.06358223635817
At time: 81.50189590454102 and batch: 150, loss is 4.995734739303589 and perplexity is 147.78148637030432
At time: 82.49965238571167 and batch: 200, loss is 4.971740522384644 and perplexity is 144.2777876778393
At time: 83.50093579292297 and batch: 250, loss is 4.974390811920166 and perplexity is 144.6606727427542
At time: 84.51169562339783 and batch: 300, loss is 4.9965048503875735 and perplexity is 147.89533836468038
At time: 85.52533745765686 and batch: 350, loss is 5.068276987075806 and perplexity is 158.90030406070616
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.320648193359375 and perplexity of 204.51640521373494
Finished 11 epochs...
Completing Train Step...
At time: 87.45535230636597 and batch: 50, loss is 5.024146661758423 and perplexity is 152.04045880227878
At time: 88.49153590202332 and batch: 100, loss is 4.986185607910156 and perplexity is 146.37701793852878
At time: 89.49732160568237 and batch: 150, loss is 4.97463397026062 and perplexity is 144.69585246881584
At time: 90.5035252571106 and batch: 200, loss is 4.970274515151978 and perplexity is 144.06643036112177
At time: 91.50943422317505 and batch: 250, loss is 4.9753555583953855 and perplexity is 144.80030095897058
At time: 92.52938318252563 and batch: 300, loss is 4.972651767730713 and perplexity is 144.40932006043593
At time: 93.54949116706848 and batch: 350, loss is 5.039134292602539 and perplexity is 154.3363470582194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.313931695346175 and perplexity of 203.14737387913604
Finished 12 epochs...
Completing Train Step...
At time: 95.50621056556702 and batch: 50, loss is 5.017527885437012 and perplexity is 151.03745998769233
At time: 96.49577403068542 and batch: 100, loss is 4.977523612976074 and perplexity is 145.1145764748824
At time: 97.49702548980713 and batch: 150, loss is 4.961445722579956 and perplexity is 142.80009605794694
At time: 98.49494290351868 and batch: 200, loss is 4.957614440917968 and perplexity is 142.2540353926524
At time: 99.49769973754883 and batch: 250, loss is 4.962963218688965 and perplexity is 143.01695915091491
At time: 100.49725794792175 and batch: 300, loss is 4.9636417007446285 and perplexity is 143.1140265168637
At time: 101.49565172195435 and batch: 350, loss is 5.024865446090698 and perplexity is 152.14978238727088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.301782016096444 and perplexity of 200.69413167683072
Finished 13 epochs...
Completing Train Step...
At time: 103.39471387863159 and batch: 50, loss is 4.992164182662964 and perplexity is 147.25476510642847
At time: 104.40163731575012 and batch: 100, loss is 4.961880731582641 and perplexity is 142.86222889850916
At time: 105.40079522132874 and batch: 150, loss is 4.944274616241455 and perplexity is 140.3689925253436
At time: 106.40050172805786 and batch: 200, loss is 4.942461605072022 and perplexity is 140.11473253180336
At time: 107.40652108192444 and batch: 250, loss is 4.948404779434204 and perplexity is 140.9499382461655
At time: 108.42074298858643 and batch: 300, loss is 4.945494012832642 and perplexity is 140.5402623980646
At time: 109.42925786972046 and batch: 350, loss is 5.006257600784302 and perplexity is 149.34478122303074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.289102488550647 and perplexity of 198.16548977575087
Finished 14 epochs...
Completing Train Step...
At time: 111.32814955711365 and batch: 50, loss is 4.975968704223633 and perplexity is 144.88911188367422
At time: 112.33136248588562 and batch: 100, loss is 4.942059221267701 and perplexity is 140.05836397434365
At time: 113.32631349563599 and batch: 150, loss is 4.93099606513977 and perplexity is 138.51741601738206
At time: 114.33109617233276 and batch: 200, loss is 4.928585958480835 and perplexity is 138.18397624477012
At time: 115.32896590232849 and batch: 250, loss is 4.935064916610718 and perplexity is 139.0821709821785
At time: 116.32668709754944 and batch: 300, loss is 4.929744968414306 and perplexity is 138.34422569329706
At time: 117.32734489440918 and batch: 350, loss is 4.993065633773804 and perplexity is 147.38756792664194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.281123194201239 and perplexity of 196.5905607701819
Finished 15 epochs...
Completing Train Step...
At time: 119.21823787689209 and batch: 50, loss is 4.961746377944946 and perplexity is 142.84303612770557
At time: 120.22789025306702 and batch: 100, loss is 4.928124294281006 and perplexity is 138.1201963735223
At time: 121.22612261772156 and batch: 150, loss is 4.917342119216919 and perplexity is 136.63896006105608
At time: 122.2261610031128 and batch: 200, loss is 4.914553556442261 and perplexity is 136.2584645078933
At time: 123.22419595718384 and batch: 250, loss is 4.922511701583862 and perplexity is 137.34715537945578
At time: 124.22444295883179 and batch: 300, loss is 4.917807531356812 and perplexity is 136.7025682927243
At time: 125.22565865516663 and batch: 350, loss is 4.980103826522827 and perplexity is 145.48948653694765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.280227134967673 and perplexity of 196.41448288286153
Finished 16 epochs...
Completing Train Step...
At time: 127.12356090545654 and batch: 50, loss is 4.949735174179077 and perplexity is 141.13758209581738
At time: 128.1132791042328 and batch: 100, loss is 4.916393842697143 and perplexity is 136.50944995894307
At time: 129.1136929988861 and batch: 150, loss is 4.904474048614502 and perplexity is 134.8919447410775
At time: 130.11276531219482 and batch: 200, loss is 4.904241724014282 and perplexity is 134.8606096640382
At time: 131.11196398735046 and batch: 250, loss is 4.909558963775635 and perplexity is 135.57960570313168
At time: 132.1104395389557 and batch: 300, loss is 4.905353164672851 and perplexity is 135.0105825564049
At time: 133.1101107597351 and batch: 350, loss is 4.970200023651123 and perplexity is 144.05569903620258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.271062916722791 and perplexity of 194.62272028677094
Finished 17 epochs...
Completing Train Step...
At time: 135.06491446495056 and batch: 50, loss is 4.937447147369385 and perplexity is 139.41389176872804
At time: 136.05174684524536 and batch: 100, loss is 4.905337715148926 and perplexity is 135.00849672329215
At time: 137.0514430999756 and batch: 150, loss is 4.894594564437866 and perplexity is 133.565843292663
At time: 138.05730152130127 and batch: 200, loss is 4.892074956893921 and perplexity is 133.22973339661817
At time: 139.06144857406616 and batch: 250, loss is 4.8993110179901125 and perplexity is 134.1972883087267
At time: 140.07051968574524 and batch: 300, loss is 4.895579833984375 and perplexity is 133.69750650173708
At time: 141.0849928855896 and batch: 350, loss is 4.962813262939453 and perplexity is 142.9955145435238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.268083769699623 and perplexity of 194.04377340112796
Finished 18 epochs...
Completing Train Step...
At time: 143.00212717056274 and batch: 50, loss is 4.928913021087647 and perplexity is 138.2291784478326
At time: 144.0170419216156 and batch: 100, loss is 4.895167942047119 and perplexity is 133.642448916444
At time: 145.0171501636505 and batch: 150, loss is 4.884874248504639 and perplexity is 132.2738306563967
At time: 146.017338514328 and batch: 200, loss is 4.885965328216553 and perplexity is 132.41823071110295
At time: 147.01726055145264 and batch: 250, loss is 4.890314903259277 and perplexity is 132.99544815788437
At time: 148.01726365089417 and batch: 300, loss is 4.8870276355743405 and perplexity is 132.5589743151399
At time: 149.0184428691864 and batch: 350, loss is 4.950207948684692 and perplexity is 141.20432412213756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.265219852842134 and perplexity of 193.48884318369159
Finished 19 epochs...
Completing Train Step...
At time: 150.9151210784912 and batch: 50, loss is 4.916895227432251 and perplexity is 136.57791087454524
At time: 151.91124439239502 and batch: 100, loss is 4.882053918838501 and perplexity is 131.90130042370964
At time: 152.90679574012756 and batch: 150, loss is 4.872598180770874 and perplexity is 130.65995444647945
At time: 153.90499663352966 and batch: 200, loss is 4.870146484375 and perplexity is 130.34000827277984
At time: 154.90331315994263 and batch: 250, loss is 4.873752775192261 and perplexity is 130.81090082513668
At time: 155.89906907081604 and batch: 300, loss is 4.871287355422973 and perplexity is 130.48879427129177
At time: 156.89567136764526 and batch: 350, loss is 4.940388135910034 and perplexity is 139.82450994253188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.259034781620421 and perplexity of 192.29579425763154
Finished 20 epochs...
Completing Train Step...
At time: 158.79193496704102 and batch: 50, loss is 4.903886346817017 and perplexity is 134.81269179351787
At time: 159.80547189712524 and batch: 100, loss is 4.870508289337158 and perplexity is 130.3871744665076
At time: 160.80836510658264 and batch: 150, loss is 4.86077730178833 and perplexity is 129.1245318386023
At time: 161.80812239646912 and batch: 200, loss is 4.862068424224853 and perplexity is 129.29135509021174
At time: 162.80898427963257 and batch: 250, loss is 4.8668237400054934 and perplexity is 129.90764046581913
At time: 163.81014275550842 and batch: 300, loss is 4.866112546920776 and perplexity is 129.81528389583767
At time: 164.8112347126007 and batch: 350, loss is 4.932151985168457 and perplexity is 138.6776236486449
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.256946432179418 and perplexity of 191.89463247197747
Finished 21 epochs...
Completing Train Step...
At time: 166.70460748672485 and batch: 50, loss is 4.893252048492432 and perplexity is 133.38664933056714
At time: 167.7182207107544 and batch: 100, loss is 4.863132839202881 and perplexity is 129.4290480133038
At time: 168.7139127254486 and batch: 150, loss is 4.853634147644043 and perplexity is 128.20546184113311
At time: 169.7224464416504 and batch: 200, loss is 4.854735689163208 and perplexity is 128.34676329075373
At time: 170.72098016738892 and batch: 250, loss is 4.855159616470337 and perplexity is 128.40118452300243
At time: 171.71913886070251 and batch: 300, loss is 4.853972272872925 and perplexity is 128.24881867186812
At time: 172.71667885780334 and batch: 350, loss is 4.91649824142456 and perplexity is 136.5237021157397
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.2510570657664335 and perplexity of 190.76781604301323
Finished 22 epochs...
Completing Train Step...
At time: 174.59477186203003 and batch: 50, loss is 4.877483148574829 and perplexity is 131.29978562221106
At time: 175.59922051429749 and batch: 100, loss is 4.849567852020264 and perplexity is 127.6851990207869
At time: 176.60267400741577 and batch: 150, loss is 4.836869878768921 and perplexity is 126.07410623305111
At time: 177.61181664466858 and batch: 200, loss is 4.840701293945313 and perplexity is 126.55807502767016
At time: 178.616854429245 and batch: 250, loss is 4.84483006477356 and perplexity is 127.08168450386916
At time: 179.62169098854065 and batch: 300, loss is 4.846801109313965 and perplexity is 127.3324151837905
At time: 180.62860894203186 and batch: 350, loss is 4.909882259368897 and perplexity is 135.62344507834823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.243145646720097 and perplexity of 189.2645263281532
Finished 23 epochs...
Completing Train Step...
At time: 182.56418204307556 and batch: 50, loss is 4.869144611358642 and perplexity is 130.2094895280611
At time: 183.5642900466919 and batch: 100, loss is 4.839772500991821 and perplexity is 126.44058335054417
At time: 184.56530022621155 and batch: 150, loss is 4.827390861511231 and perplexity is 124.88469374097755
At time: 185.57464480400085 and batch: 200, loss is 4.8322842502593994 and perplexity is 125.4973007358173
At time: 186.5764729976654 and batch: 250, loss is 4.83596438407898 and perplexity is 125.95999846910182
At time: 187.5812783241272 and batch: 300, loss is 4.835749397277832 and perplexity is 125.93292164264277
At time: 188.5814561843872 and batch: 350, loss is 4.898612670898437 and perplexity is 134.10360473835743
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.239323583142511 and perplexity of 188.54252592020154
Finished 24 epochs...
Completing Train Step...
At time: 190.47889924049377 and batch: 50, loss is 4.8593125152587895 and perplexity is 128.9355304208907
At time: 191.4698030948639 and batch: 100, loss is 4.830656280517578 and perplexity is 125.29316113916808
At time: 192.46097803115845 and batch: 150, loss is 4.8153957271575925 and perplexity is 123.39563365294059
At time: 193.4596471786499 and batch: 200, loss is 4.820474252700806 and perplexity is 124.02389550197887
At time: 194.46182680130005 and batch: 250, loss is 4.824754552841187 and perplexity is 124.55589274100983
At time: 195.48837614059448 and batch: 300, loss is 4.831974058151245 and perplexity is 125.45837850053161
At time: 196.49041152000427 and batch: 350, loss is 4.89091778755188 and perplexity is 133.0756531993169
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.234931156553071 and perplexity of 187.71618287028272
Finished 25 epochs...
Completing Train Step...
At time: 198.38294982910156 and batch: 50, loss is 4.84809944152832 and perplexity is 127.49784232681178
At time: 199.3870792388916 and batch: 100, loss is 4.820074977874756 and perplexity is 123.9743857673294
At time: 200.3818395137787 and batch: 150, loss is 4.8066425800323485 and perplexity is 122.32024689234893
At time: 201.38160467147827 and batch: 200, loss is 4.8127205944061275 and perplexity is 123.06597508877432
At time: 202.38133788108826 and batch: 250, loss is 4.817094354629517 and perplexity is 123.60541498590584
At time: 203.3810977935791 and batch: 300, loss is 4.819868764877319 and perplexity is 123.94882327338475
At time: 204.38383984565735 and batch: 350, loss is 4.882092628479004 and perplexity is 131.90640637445503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.238948427397629 and perplexity of 188.47180637469936
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 206.29153728485107 and batch: 50, loss is 4.817827625274658 and perplexity is 123.69608444685994
At time: 207.29097986221313 and batch: 100, loss is 4.758868761062622 and perplexity is 116.6139330336431
At time: 208.29879760742188 and batch: 150, loss is 4.714865427017212 and perplexity is 111.59379265364919
At time: 209.30392384529114 and batch: 200, loss is 4.71878833770752 and perplexity is 112.03242493092942
At time: 210.31147646903992 and batch: 250, loss is 4.734214429855347 and perplexity is 113.77404610901046
At time: 211.31548190116882 and batch: 300, loss is 4.740018100738525 and perplexity is 114.43627304246816
At time: 212.32008576393127 and batch: 350, loss is 4.806803808212281 and perplexity is 122.3399699530381
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.168565421268858 and perplexity of 175.66265473097505
Finished 27 epochs...
Completing Train Step...
At time: 214.30038189888 and batch: 50, loss is 4.77010009765625 and perplexity is 117.9310459911858
At time: 215.302983045578 and batch: 100, loss is 4.736768598556519 and perplexity is 114.06501565105141
At time: 216.2954580783844 and batch: 150, loss is 4.704814643859863 and perplexity is 110.47780530995423
At time: 217.29326605796814 and batch: 200, loss is 4.719257326126098 and perplexity is 112.08497916342723
At time: 218.29526495933533 and batch: 250, loss is 4.734548616409302 and perplexity is 113.8120742192968
At time: 219.3121075630188 and batch: 300, loss is 4.730086669921875 and perplexity is 113.30538209172948
At time: 220.33296966552734 and batch: 350, loss is 4.7955277729034425 and perplexity is 120.96820868123902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.166226091056035 and perplexity of 175.25220205485354
Finished 28 epochs...
Completing Train Step...
At time: 222.23057055473328 and batch: 50, loss is 4.762523174285889 and perplexity is 117.04086815608308
At time: 223.24463629722595 and batch: 100, loss is 4.733266582489014 and perplexity is 113.66625677103904
At time: 224.2451090812683 and batch: 150, loss is 4.701219844818115 and perplexity is 110.08137277584929
At time: 225.24513411521912 and batch: 200, loss is 4.715524339675904 and perplexity is 111.66734744669094
At time: 226.25366687774658 and batch: 250, loss is 4.729227428436279 and perplexity is 113.2080672213691
At time: 227.2580530643463 and batch: 300, loss is 4.724786996841431 and perplexity is 112.70648897897766
At time: 228.25835943222046 and batch: 350, loss is 4.790093393325805 and perplexity is 120.31260453280332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.165867246430496 and perplexity of 175.18932502624895
Finished 29 epochs...
Completing Train Step...
At time: 230.19488954544067 and batch: 50, loss is 4.755384616851806 and perplexity is 116.20834025612695
At time: 231.20410561561584 and batch: 100, loss is 4.727954730987549 and perplexity is 113.0640792490388
At time: 232.2001543045044 and batch: 150, loss is 4.695265035629273 and perplexity is 109.427807066541
At time: 233.2049741744995 and batch: 200, loss is 4.711053266525268 and perplexity is 111.16918904890704
At time: 234.21167826652527 and batch: 250, loss is 4.7249354648590085 and perplexity is 112.72322353020662
At time: 235.2083203792572 and batch: 300, loss is 4.720006465911865 and perplexity is 112.1689779401898
At time: 236.20840287208557 and batch: 350, loss is 4.7858336353302 and perplexity is 119.80119197386223
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.165033800848599 and perplexity of 175.04337508642504
Finished 30 epochs...
Completing Train Step...
At time: 238.16877436637878 and batch: 50, loss is 4.7503837299346925 and perplexity is 115.62864618804501
At time: 239.17563462257385 and batch: 100, loss is 4.722450656890869 and perplexity is 112.44347566997705
At time: 240.1872947216034 and batch: 150, loss is 4.691896562576294 and perplexity is 109.05982256807228
At time: 241.18642663955688 and batch: 200, loss is 4.706388111114502 and perplexity is 110.65177535116248
At time: 242.1858434677124 and batch: 250, loss is 4.720091028213501 and perplexity is 112.17846360819586
At time: 243.18535447120667 and batch: 300, loss is 4.714860372543335 and perplexity is 111.5932286071648
At time: 244.18565464019775 and batch: 350, loss is 4.780345497131347 and perplexity is 119.14550736554403
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.162163043844289 and perplexity of 174.54158868935662
Finished 31 epochs...
Completing Train Step...
At time: 246.08221554756165 and batch: 50, loss is 4.745313463211059 and perplexity is 115.04386186999629
At time: 247.084410905838 and batch: 100, loss is 4.716661357879639 and perplexity is 111.7943874632198
At time: 248.08400988578796 and batch: 150, loss is 4.687070121765137 and perplexity is 108.53471999714583
At time: 249.08709287643433 and batch: 200, loss is 4.702512636184692 and perplexity is 110.22377705387198
At time: 250.09080719947815 and batch: 250, loss is 4.715775604248047 and perplexity is 111.69540902026151
At time: 251.10487246513367 and batch: 300, loss is 4.71107961654663 and perplexity is 111.17211839800734
At time: 252.1050181388855 and batch: 350, loss is 4.774305381774902 and perplexity is 118.42802377999328
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.158800322433998 and perplexity of 173.95563969585172
Finished 32 epochs...
Completing Train Step...
At time: 253.99806904792786 and batch: 50, loss is 4.738962602615357 and perplexity is 114.31554949399678
At time: 255.00201606750488 and batch: 100, loss is 4.7114419269561765 and perplexity is 111.21240451135264
At time: 256.0044867992401 and batch: 150, loss is 4.6830410861969 and perplexity is 108.09830949696124
At time: 257.0041551589966 and batch: 200, loss is 4.697349519729614 and perplexity is 109.65614549173587
At time: 258.004212141037 and batch: 250, loss is 4.712187881469727 and perplexity is 111.29539485612347
At time: 259.0103280544281 and batch: 300, loss is 4.7066467761993405 and perplexity is 110.68040080406422
At time: 260.0101411342621 and batch: 350, loss is 4.770263395309448 and perplexity is 117.95030542670283
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.1569845265355605 and perplexity of 173.64005836113748
Finished 33 epochs...
Completing Train Step...
At time: 261.90290117263794 and batch: 50, loss is 4.733862752914429 and perplexity is 113.73404143529196
At time: 262.9009611606598 and batch: 100, loss is 4.706988344192505 and perplexity is 110.71821214365355
At time: 263.90123414993286 and batch: 150, loss is 4.677607231140136 and perplexity is 107.51251196211433
At time: 264.89853382110596 and batch: 200, loss is 4.69245228767395 and perplexity is 109.1204466922327
At time: 265.90262937545776 and batch: 250, loss is 4.706174793243409 and perplexity is 110.6281738674105
At time: 266.9111211299896 and batch: 300, loss is 4.702691965103149 and perplexity is 110.24354513704081
At time: 267.91670751571655 and batch: 350, loss is 4.766218347549438 and perplexity is 117.47415448384444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.1575927734375 and perplexity of 173.74570651551238
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 269.82995343208313 and batch: 50, loss is 4.721237144470215 and perplexity is 112.30710687497812
At time: 270.8323519229889 and batch: 100, loss is 4.6783974170684814 and perplexity is 107.59750041010348
At time: 271.84810876846313 and batch: 150, loss is 4.634462728500366 and perplexity is 102.97257889861471
At time: 272.850891828537 and batch: 200, loss is 4.644179010391236 and perplexity is 103.97796590445668
At time: 273.86004877090454 and batch: 250, loss is 4.662586841583252 and perplexity is 105.90969971673367
At time: 274.8610587120056 and batch: 300, loss is 4.66183648109436 and perplexity is 105.83025907096406
At time: 275.86242628097534 and batch: 350, loss is 4.730704288482666 and perplexity is 113.37538321351774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.131491825498384 and perplexity of 169.26945026226988
Finished 35 epochs...
Completing Train Step...
At time: 277.7731182575226 and batch: 50, loss is 4.702291831970215 and perplexity is 110.1994418661178
At time: 278.7743716239929 and batch: 100, loss is 4.663224573135376 and perplexity is 105.97726321531657
At time: 279.76057744026184 and batch: 150, loss is 4.624264392852783 and perplexity is 101.92776670478025
At time: 280.75111198425293 and batch: 200, loss is 4.642395133972168 and perplexity is 103.79264740477151
At time: 281.7581367492676 and batch: 250, loss is 4.666409597396851 and perplexity is 106.31534147775666
At time: 282.75836396217346 and batch: 300, loss is 4.660765113830567 and perplexity is 105.71693671164924
At time: 283.7632329463959 and batch: 350, loss is 4.72705979347229 and perplexity is 112.96293922664117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.128196979391164 and perplexity of 168.71265126023204
Finished 36 epochs...
Completing Train Step...
At time: 285.6403706073761 and batch: 50, loss is 4.698169994354248 and perplexity is 109.74615249572918
At time: 286.639976978302 and batch: 100, loss is 4.659747352600098 and perplexity is 105.60939684629585
At time: 287.63702869415283 and batch: 150, loss is 4.620578804016113 and perplexity is 101.55279428719965
At time: 288.63705801963806 and batch: 200, loss is 4.6398137664794925 and perplexity is 103.52506595040087
At time: 289.6391041278839 and batch: 250, loss is 4.665409631729126 and perplexity is 106.20908292263427
At time: 290.6449980735779 and batch: 300, loss is 4.6594781494140625 and perplexity is 105.58097028662144
At time: 291.65021896362305 and batch: 350, loss is 4.725551242828369 and perplexity is 112.79265738361647
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.127556636415679 and perplexity of 168.6046518811343
Finished 37 epochs...
Completing Train Step...
At time: 293.5731863975525 and batch: 50, loss is 4.695018224716186 and perplexity is 109.40080242221939
At time: 294.5779721736908 and batch: 100, loss is 4.655299978256226 and perplexity is 105.14075520913035
At time: 295.5776298046112 and batch: 150, loss is 4.616998748779297 and perplexity is 101.18987968896914
At time: 296.5813043117523 and batch: 200, loss is 4.636905765533447 and perplexity is 103.22445226496268
At time: 297.5982403755188 and batch: 250, loss is 4.664101305007935 and perplexity is 106.07021760182649
At time: 298.6042401790619 and batch: 300, loss is 4.658553686141968 and perplexity is 105.48340965991562
At time: 299.60938024520874 and batch: 350, loss is 4.723553266525268 and perplexity is 112.56752530617037
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.1263143605199355 and perplexity of 168.39532843184978
Finished 38 epochs...
Completing Train Step...
At time: 301.5006515979767 and batch: 50, loss is 4.69186770439148 and perplexity is 109.05667534496858
At time: 302.4903166294098 and batch: 100, loss is 4.651790809631348 and perplexity is 104.77244517871152
At time: 303.4785315990448 and batch: 150, loss is 4.613751916885376 and perplexity is 100.86186595114356
At time: 304.4759635925293 and batch: 200, loss is 4.634501399993897 and perplexity is 102.9765610790314
At time: 305.47624945640564 and batch: 250, loss is 4.6624476146698 and perplexity is 105.89495526257369
At time: 306.4760961532593 and batch: 300, loss is 4.656815986633301 and perplexity is 105.30027035739907
At time: 307.4749963283539 and batch: 350, loss is 4.722074680328369 and perplexity is 112.40120750493669
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.125580886314655 and perplexity of 168.27186008811324
Finished 39 epochs...
Completing Train Step...
At time: 309.3737952709198 and batch: 50, loss is 4.689828701019287 and perplexity is 108.83453496563443
At time: 310.3830099105835 and batch: 100, loss is 4.648677082061767 and perplexity is 104.44671970099337
At time: 311.3814871311188 and batch: 150, loss is 4.610174636840821 and perplexity is 100.50169940320772
At time: 312.38252687454224 and batch: 200, loss is 4.632233295440674 and perplexity is 102.74326414290677
At time: 313.3833453655243 and batch: 250, loss is 4.660876941680908 and perplexity is 105.72875947047093
At time: 314.38206815719604 and batch: 300, loss is 4.654980049133301 and perplexity is 105.10712299978115
At time: 315.38134503364563 and batch: 350, loss is 4.72012583732605 and perplexity is 112.18236850892382
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.124812159044989 and perplexity of 168.14255462723074
Finished 40 epochs...
Completing Train Step...
At time: 317.28979086875916 and batch: 50, loss is 4.687039594650269 and perplexity is 108.5314067958528
At time: 318.2786726951599 and batch: 100, loss is 4.645556678771973 and perplexity is 104.12131177924404
At time: 319.277232170105 and batch: 150, loss is 4.607377433776856 and perplexity is 100.22096855534186
At time: 320.2767803668976 and batch: 200, loss is 4.629856443405151 and perplexity is 102.49934859680543
At time: 321.27634620666504 and batch: 250, loss is 4.658609132766724 and perplexity is 105.48925852109738
At time: 322.2763259410858 and batch: 300, loss is 4.653090353012085 and perplexity is 104.90869002519703
At time: 323.30379915237427 and batch: 350, loss is 4.717720994949341 and perplexity is 111.91291172562124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.124401750235722 and perplexity of 168.0735616002405
Finished 41 epochs...
Completing Train Step...
At time: 325.21236419677734 and batch: 50, loss is 4.684525184631347 and perplexity is 108.25885713363242
At time: 326.2039704322815 and batch: 100, loss is 4.642446775436401 and perplexity is 103.79800754746171
At time: 327.19556641578674 and batch: 150, loss is 4.6037086009979244 and perplexity is 99.85394826049844
At time: 328.19676637649536 and batch: 200, loss is 4.627204694747925 and perplexity is 102.22790614443666
At time: 329.21072793006897 and batch: 250, loss is 4.65676983833313 and perplexity is 105.29541104103994
At time: 330.2211422920227 and batch: 300, loss is 4.651095905303955 and perplexity is 104.69966364419575
At time: 331.2334930896759 and batch: 350, loss is 4.715904703140259 and perplexity is 111.70982970465835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.124071318527748 and perplexity of 168.01803394077055
Finished 42 epochs...
Completing Train Step...
At time: 333.1260907649994 and batch: 50, loss is 4.682923460006714 and perplexity is 108.08559505244011
At time: 334.12740683555603 and batch: 100, loss is 4.640634050369263 and perplexity is 103.61002073294355
At time: 335.11664175987244 and batch: 150, loss is 4.602270345687867 and perplexity is 99.71043601754415
At time: 336.1275191307068 and batch: 200, loss is 4.624538831710815 and perplexity is 101.95574348345845
At time: 337.1274507045746 and batch: 250, loss is 4.654181432723999 and perplexity is 105.02321623573789
At time: 338.1273946762085 and batch: 300, loss is 4.649205570220947 and perplexity is 104.5019331441609
At time: 339.1271028518677 and batch: 350, loss is 4.713831262588501 and perplexity is 111.47844597681711
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.122468093345905 and perplexity of 167.74887901338306
Finished 43 epochs...
Completing Train Step...
At time: 341.0120995044708 and batch: 50, loss is 4.6806214237213135 and perplexity is 107.83706426390162
At time: 342.0136971473694 and batch: 100, loss is 4.637014608383179 and perplexity is 103.2356881199694
At time: 343.0082025527954 and batch: 150, loss is 4.5980352115631105 and perplexity is 99.28904190751582
At time: 344.0068039894104 and batch: 200, loss is 4.621532869338989 and perplexity is 101.64972852011255
At time: 345.007116317749 and batch: 250, loss is 4.651825294494629 and perplexity is 104.77605830445788
At time: 346.0089497566223 and batch: 300, loss is 4.647618045806885 and perplexity is 104.33616538897152
At time: 347.02017879486084 and batch: 350, loss is 4.711977605819702 and perplexity is 111.27199460496308
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.120576003502155 and perplexity of 167.43178314463813
Finished 44 epochs...
Completing Train Step...
At time: 348.9988594055176 and batch: 50, loss is 4.677978458404541 and perplexity is 107.55243094686865
At time: 349.98959469795227 and batch: 100, loss is 4.633713979721069 and perplexity is 102.89550716314794
At time: 350.989027261734 and batch: 150, loss is 4.594446392059326 and perplexity is 98.93335009602595
At time: 351.9782660007477 and batch: 200, loss is 4.617847557067871 and perplexity is 101.27580696029999
At time: 352.9793031215668 and batch: 250, loss is 4.648618478775024 and perplexity is 104.44059895927887
At time: 353.97856068611145 and batch: 300, loss is 4.644023933410645 and perplexity is 103.96184256566787
At time: 354.97634267807007 and batch: 350, loss is 4.708611841201782 and perplexity is 110.89810882128695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.11956471410291 and perplexity of 167.2625467451589
Finished 45 epochs...
Completing Train Step...
At time: 356.8820769786835 and batch: 50, loss is 4.6751438331604005 and perplexity is 107.24799180033288
At time: 357.8729622364044 and batch: 100, loss is 4.630299892425537 and perplexity is 102.54481191211718
At time: 358.8644504547119 and batch: 150, loss is 4.5912124538421635 and perplexity is 98.61392253708429
At time: 359.8639500141144 and batch: 200, loss is 4.615383234024048 and perplexity is 101.02653792129294
At time: 360.8707468509674 and batch: 250, loss is 4.646226959228516 and perplexity is 104.19112565446353
At time: 361.8732764720917 and batch: 300, loss is 4.642305765151978 and perplexity is 103.783371992801
At time: 362.87819147109985 and batch: 350, loss is 4.70660397529602 and perplexity is 110.67566368430718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.119185875202048 and perplexity of 167.19919318694843
Finished 46 epochs...
Completing Train Step...
At time: 364.8259620666504 and batch: 50, loss is 4.671834964752197 and perplexity is 106.89370876962758
At time: 365.84240651130676 and batch: 100, loss is 4.627211170196533 and perplexity is 102.22856811813253
At time: 366.846519947052 and batch: 150, loss is 4.587578248977661 and perplexity is 98.25618977089981
At time: 367.8579316139221 and batch: 200, loss is 4.612455968856811 and perplexity is 100.73123887603919
At time: 368.87674140930176 and batch: 250, loss is 4.644533281326294 and perplexity is 104.01480880146154
At time: 369.8931703567505 and batch: 300, loss is 4.640356884002686 and perplexity is 103.58130749931117
At time: 370.9033694267273 and batch: 350, loss is 4.7046776866912845 and perplexity is 110.4626756186317
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.117776804956897 and perplexity of 166.96376368612107
Finished 47 epochs...
Completing Train Step...
At time: 372.862361907959 and batch: 50, loss is 4.6689174938201905 and perplexity is 106.58230395959886
At time: 373.8531427383423 and batch: 100, loss is 4.623620529174804 and perplexity is 101.8621602410975
At time: 374.87067437171936 and batch: 150, loss is 4.582708883285522 and perplexity is 97.77890742558668
At time: 375.85617899894714 and batch: 200, loss is 4.609067401885986 and perplexity is 100.39048199187337
At time: 377.08138966560364 and batch: 250, loss is 4.641923303604126 and perplexity is 103.74368643329139
At time: 378.069144487381 and batch: 300, loss is 4.638395338058472 and perplexity is 103.37832714841485
At time: 379.05829191207886 and batch: 350, loss is 4.702973213195801 and perplexity is 110.27455528440574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.117171188880658 and perplexity of 166.86267835922249
Finished 48 epochs...
Completing Train Step...
At time: 381.0044140815735 and batch: 50, loss is 4.666506299972534 and perplexity is 106.32562294222622
At time: 381.9973695278168 and batch: 100, loss is 4.620817861557007 and perplexity is 101.57707415049937
At time: 382.9934067726135 and batch: 150, loss is 4.58000334739685 and perplexity is 97.51472062697637
At time: 384.00077533721924 and batch: 200, loss is 4.606258239746094 and perplexity is 100.10886459032349
At time: 385.0022511482239 and batch: 250, loss is 4.640603427886963 and perplexity is 103.60684798549649
At time: 386.05166888237 and batch: 300, loss is 4.636703567504883 and perplexity is 103.20358259418934
At time: 387.05220913887024 and batch: 350, loss is 4.701069593429565 and perplexity is 110.06483413924394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.11627512964709 and perplexity of 166.713226484407
Finished 49 epochs...
Completing Train Step...
At time: 388.97910356521606 and batch: 50, loss is 4.664272003173828 and perplexity is 106.0883251388447
At time: 389.980669260025 and batch: 100, loss is 4.618397426605225 and perplexity is 101.33151075492471
At time: 390.96534514427185 and batch: 150, loss is 4.576775178909302 and perplexity is 97.20043423642069
At time: 391.9544174671173 and batch: 200, loss is 4.6037790489196775 and perplexity is 99.86098301142111
At time: 392.9508285522461 and batch: 250, loss is 4.638305492401123 and perplexity is 103.36903947189155
At time: 393.9428668022156 and batch: 300, loss is 4.635003385543823 and perplexity is 103.0282668013511
At time: 394.9410789012909 and batch: 350, loss is 4.699834823608398 and perplexity is 109.92901327468606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.115854197535022 and perplexity of 166.64306630124614
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd1aa376c50>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'anneal': 3.4439614136649936, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'lr': 4.33658299020075, 'dropout': 0.20642903087480036, 'seq_len': 35, 'batch_size': 80}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.1739819049835205 and batch: 50, loss is 6.698588333129883 and perplexity is 811.2597878808175
At time: 2.095313787460327 and batch: 100, loss is 5.919830713272095 and perplexity is 372.34867485169224
At time: 3.0161924362182617 and batch: 150, loss is 5.5872612476348875 and perplexity is 267.0033613468067
At time: 3.938075065612793 and batch: 200, loss is 5.334757194519043 and perplexity is 207.4223794026108
At time: 4.859987020492554 and batch: 250, loss is 5.193519525527954 and perplexity is 180.10130993438622
At time: 5.782006502151489 and batch: 300, loss is 5.102519588470459 and perplexity is 164.43569607624684
At time: 6.737094402313232 and batch: 350, loss is 5.08282696723938 and perplexity is 161.2292019559965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.042275264345366 and perplexity of 154.82187527925862
Finished 1 epochs...
Completing Train Step...
At time: 8.501904249191284 and batch: 50, loss is 4.853873462677002 and perplexity is 128.2361470070234
At time: 9.414133548736572 and batch: 100, loss is 4.7248234462738035 and perplexity is 112.71059714139533
At time: 10.337690591812134 and batch: 150, loss is 4.638785009384155 and perplexity is 103.41861856789728
At time: 11.293309211730957 and batch: 200, loss is 4.621906185150147 and perplexity is 101.68768305504217
At time: 12.279775857925415 and batch: 250, loss is 4.593660764694214 and perplexity is 98.85565587221858
At time: 13.279708862304688 and batch: 300, loss is 4.540016832351685 and perplexity is 93.69237716924393
At time: 14.284288167953491 and batch: 350, loss is 4.5808588409423825 and perplexity is 97.59817953525571
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.7138477193898165 and perplexity of 111.48028057054924
Finished 2 epochs...
Completing Train Step...
At time: 16.26773953437805 and batch: 50, loss is 4.473691473007202 and perplexity is 87.67979393536343
At time: 17.25425410270691 and batch: 100, loss is 4.397969074249268 and perplexity is 81.28561586326308
At time: 18.24805188179016 and batch: 150, loss is 4.3227887630462645 and perplexity is 75.39860421055894
At time: 19.243398904800415 and batch: 200, loss is 4.3647809505462645 and perplexity is 78.6321737204491
At time: 20.24086880683899 and batch: 250, loss is 4.360764198303222 and perplexity is 78.31696124929339
At time: 21.23755645751953 and batch: 300, loss is 4.311900081634522 and perplexity is 74.58206640802405
At time: 22.234411001205444 and batch: 350, loss is 4.37394941329956 and perplexity is 79.35642493815706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.598555466224408 and perplexity of 99.3407109337444
Finished 3 epochs...
Completing Train Step...
At time: 24.117393732070923 and batch: 50, loss is 4.2843062114715575 and perplexity is 72.55219337501076
At time: 25.12109661102295 and batch: 100, loss is 4.221647109985351 and perplexity is 68.14563525786582
At time: 26.1285183429718 and batch: 150, loss is 4.149437651634217 and perplexity is 63.39833831988796
At time: 27.14236855506897 and batch: 200, loss is 4.208962349891663 and perplexity is 67.28668353873714
At time: 28.140522480010986 and batch: 250, loss is 4.21810049533844 and perplexity is 67.9043770276121
At time: 29.148199319839478 and batch: 300, loss is 4.168099279403687 and perplexity is 64.59256294066512
At time: 30.157216548919678 and batch: 350, loss is 4.241033010482788 and perplexity is 69.4795879229767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.550825974036908 and perplexity of 94.71060451681285
Finished 4 epochs...
Completing Train Step...
At time: 32.07542395591736 and batch: 50, loss is 4.160193901062012 and perplexity is 64.08394734075978
At time: 33.06321668624878 and batch: 100, loss is 4.102549929618835 and perplexity is 60.494347421293014
At time: 34.06282567977905 and batch: 150, loss is 4.0299837064743045 and perplexity is 56.259994565992784
At time: 35.06413769721985 and batch: 200, loss is 4.098031606674194 and perplexity is 60.22163099771948
At time: 36.06786274909973 and batch: 250, loss is 4.115190405845642 and perplexity is 61.26387815280728
At time: 37.07006883621216 and batch: 300, loss is 4.067099390029907 and perplexity is 58.38735777999882
At time: 38.071837186813354 and batch: 350, loss is 4.143897466659546 and perplexity is 63.048070967385556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.523963138974946 and perplexity of 92.20027737604035
Finished 5 epochs...
Completing Train Step...
At time: 40.005300521850586 and batch: 50, loss is 4.067685327529907 and perplexity is 58.421579147260175
At time: 41.01377034187317 and batch: 100, loss is 4.013065958023072 and perplexity is 55.31620801182032
At time: 42.02472734451294 and batch: 150, loss is 3.939199728965759 and perplexity is 51.377468943996895
At time: 43.027817249298096 and batch: 200, loss is 4.013482165336609 and perplexity is 55.33923581398941
At time: 44.03018498420715 and batch: 250, loss is 4.035815634727478 and perplexity is 56.58905742064867
At time: 45.03152680397034 and batch: 300, loss is 3.989582366943359 and perplexity is 54.032318968055044
At time: 46.04154181480408 and batch: 350, loss is 4.0681785345077515 and perplexity is 58.4504001845364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.511510388604526 and perplexity of 91.05924955012219
Finished 6 epochs...
Completing Train Step...
At time: 47.9962797164917 and batch: 50, loss is 3.9935236120224 and perplexity is 54.24569378431615
At time: 48.99909949302673 and batch: 100, loss is 3.9431867218017578 and perplexity is 51.582719438844286
At time: 49.9942786693573 and batch: 150, loss is 3.8677148389816285 and perplexity is 47.832955090356805
At time: 50.989238262176514 and batch: 200, loss is 3.9444281816482545 and perplexity is 51.64679708048047
At time: 51.98380517959595 and batch: 250, loss is 3.96999493598938 and perplexity is 52.98426252612867
At time: 52.99788689613342 and batch: 300, loss is 3.9254408502578735 and perplexity is 50.67541340427262
At time: 54.00707125663757 and batch: 350, loss is 4.0040570259094235 and perplexity is 54.82010607870948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.51440534920528 and perplexity of 91.32324443292234
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 55.90537142753601 and batch: 50, loss is 3.9232218647003174 and perplexity is 50.56309006183172
At time: 56.92300009727478 and batch: 100, loss is 3.8537922525405883 and perplexity is 47.171611130393316
At time: 57.91966199874878 and batch: 150, loss is 3.7562751245498656 and perplexity is 42.78874601981817
At time: 58.921271085739136 and batch: 200, loss is 3.823945178985596 and perplexity is 45.784480454267644
At time: 59.92045450210571 and batch: 250, loss is 3.8296604776382446 and perplexity is 46.04690162708385
At time: 60.92173171043396 and batch: 300, loss is 3.7651678943634033 and perplexity is 43.170953411171965
At time: 61.928184032440186 and batch: 350, loss is 3.8190483617782593 and perplexity is 45.56083025652405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433793166588092 and perplexity of 84.25038732810143
Finished 8 epochs...
Completing Train Step...
At time: 63.89671063423157 and batch: 50, loss is 3.853579697608948 and perplexity is 47.16158563733587
At time: 64.88692235946655 and batch: 100, loss is 3.79789746761322 and perplexity is 44.60729753994301
At time: 65.88187623023987 and batch: 150, loss is 3.705759296417236 and perplexity is 40.68092447314055
At time: 66.88394403457642 and batch: 200, loss is 3.7843907976150515 and perplexity is 44.008852092988796
At time: 67.88746070861816 and batch: 250, loss is 3.7991049098968506 and perplexity is 44.66119080710836
At time: 68.89371609687805 and batch: 300, loss is 3.74341025352478 and perplexity is 42.24180005906864
At time: 69.91652941703796 and batch: 350, loss is 3.8075128316879274 and perplexity is 45.0382816602358
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.431021854795259 and perplexity of 84.01722646609774
Finished 9 epochs...
Completing Train Step...
At time: 71.84289360046387 and batch: 50, loss is 3.818638505935669 and perplexity is 45.542160710222795
At time: 72.83961415290833 and batch: 100, loss is 3.7679698038101197 and perplexity is 43.292084132768345
At time: 73.84574055671692 and batch: 150, loss is 3.6763658046722414 and perplexity is 39.50257282801082
At time: 74.84479331970215 and batch: 200, loss is 3.759328217506409 and perplexity is 42.919583667027936
At time: 75.84929180145264 and batch: 250, loss is 3.7778538990020754 and perplexity is 43.7221089153923
At time: 76.84583592414856 and batch: 300, loss is 3.7256333160400392 and perplexity is 41.49750546031618
At time: 77.8423867225647 and batch: 350, loss is 3.7938737630844117 and perplexity is 44.4281715714726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.431234425511853 and perplexity of 84.03508796648262
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 79.74089288711548 and batch: 50, loss is 3.7985191440582273 and perplexity is 44.635037467835495
At time: 80.74319005012512 and batch: 100, loss is 3.7433982467651368 and perplexity is 42.241292874973254
At time: 81.73758387565613 and batch: 150, loss is 3.642916102409363 and perplexity is 38.203078539682195
At time: 82.7443459033966 and batch: 200, loss is 3.725774712562561 and perplexity is 41.50337347813047
At time: 83.74853301048279 and batch: 250, loss is 3.7323589897155762 and perplexity is 41.77754481141342
At time: 84.75680494308472 and batch: 300, loss is 3.675156707763672 and perplexity is 39.454839252398486
At time: 85.76327657699585 and batch: 350, loss is 3.7364311742782594 and perplexity is 41.94801754710799
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.415479988887392 and perplexity of 82.72153679696338
Finished 11 epochs...
Completing Train Step...
At time: 87.67347979545593 and batch: 50, loss is 3.7768653440475464 and perplexity is 43.67890856448009
At time: 88.67425632476807 and batch: 100, loss is 3.72598464012146 and perplexity is 41.51208709459292
At time: 89.67066693305969 and batch: 150, loss is 3.6277101278305053 and perplexity is 37.626557886401876
At time: 90.66657519340515 and batch: 200, loss is 3.7146073627471923 and perplexity is 41.042469118019845
At time: 91.66755604743958 and batch: 250, loss is 3.7243235731124877 and perplexity is 41.443189973455404
At time: 92.68430829048157 and batch: 300, loss is 3.6711289739608763 and perplexity is 39.29624526424228
At time: 93.69092750549316 and batch: 350, loss is 3.7360866260528565 and perplexity is 41.93356692171458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.414784924737338 and perplexity of 82.66405999964309
Finished 12 epochs...
Completing Train Step...
At time: 95.61374378204346 and batch: 50, loss is 3.7655772829055785 and perplexity is 43.188630723050856
At time: 96.61355686187744 and batch: 100, loss is 3.715808596611023 and perplexity is 41.09180034501791
At time: 97.61319255828857 and batch: 150, loss is 3.6180151224136354 and perplexity is 37.263530822052424
At time: 98.6225380897522 and batch: 200, loss is 3.707193446159363 and perplexity is 40.73930886645748
At time: 99.62499666213989 and batch: 250, loss is 3.718178811073303 and perplexity is 41.18931224087858
At time: 100.63879299163818 and batch: 300, loss is 3.666987752914429 and perplexity is 39.13384732128703
At time: 101.63856530189514 and batch: 350, loss is 3.733715043067932 and perplexity is 41.834235820437215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.414800183526401 and perplexity of 82.66532136272109
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 103.52374935150146 and batch: 50, loss is 3.7609954977035525 and perplexity is 42.9912023265414
At time: 104.52677273750305 and batch: 100, loss is 3.7099820137023927 and perplexity is 40.85307172487588
At time: 105.52141833305359 and batch: 150, loss is 3.6091348791122435 and perplexity is 36.93408653973436
At time: 106.5163323879242 and batch: 200, loss is 3.696707944869995 and perplexity is 40.31436854033666
At time: 107.51147723197937 and batch: 250, loss is 3.704226689338684 and perplexity is 40.61862435332793
At time: 108.50577926635742 and batch: 300, loss is 3.65034677028656 and perplexity is 38.48801023355288
At time: 109.50655055046082 and batch: 350, loss is 3.7146139335632324 and perplexity is 41.04273880142027
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.411877073090652 and perplexity of 82.42403432530764
Finished 14 epochs...
Completing Train Step...
At time: 111.4103615283966 and batch: 50, loss is 3.7537104988098147 and perplexity is 42.67914949749314
At time: 112.41658735275269 and batch: 100, loss is 3.703530960083008 and perplexity is 40.59037461626377
At time: 113.4105589389801 and batch: 150, loss is 3.604189476966858 and perplexity is 36.75188353383582
At time: 114.40609979629517 and batch: 200, loss is 3.693214282989502 and perplexity is 40.17376951347022
At time: 115.40497207641602 and batch: 250, loss is 3.70196044921875 and perplexity is 40.52667702391756
At time: 116.41296148300171 and batch: 300, loss is 3.649666452407837 and perplexity is 38.461835056804425
At time: 117.41933393478394 and batch: 350, loss is 3.7153993368148805 and perplexity is 41.074986564022566
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.411637141786772 and perplexity of 82.40426059154466
Finished 15 epochs...
Completing Train Step...
At time: 119.33240365982056 and batch: 50, loss is 3.7499639415740966 and perplexity is 42.51954878442099
At time: 120.3189606666565 and batch: 100, loss is 3.699905128479004 and perplexity is 40.44346724480192
At time: 121.30631899833679 and batch: 150, loss is 3.6009892272949218 and perplexity is 36.634456329020644
At time: 122.31098365783691 and batch: 200, loss is 3.6908043956756593 and perplexity is 40.07707181804332
At time: 123.31471395492554 and batch: 250, loss is 3.7002275657653807 and perplexity is 40.45650982922677
At time: 124.31877636909485 and batch: 300, loss is 3.648823928833008 and perplexity is 38.42944370119432
At time: 125.33630871772766 and batch: 350, loss is 3.7152691888809204 and perplexity is 41.069641087242736
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.411595574740706 and perplexity of 82.40083536103742
Finished 16 epochs...
Completing Train Step...
At time: 127.32649183273315 and batch: 50, loss is 3.7467716455459597 and perplexity is 42.38403022049133
At time: 128.32341742515564 and batch: 100, loss is 3.6968640279769898 and perplexity is 40.32066142332855
At time: 129.31909465789795 and batch: 150, loss is 3.5982351446151735 and perplexity is 36.53370081555578
At time: 130.33031797409058 and batch: 200, loss is 3.6886895895004272 and perplexity is 39.99240613638544
At time: 131.3398072719574 and batch: 250, loss is 3.698575077056885 and perplexity is 40.38971111081349
At time: 132.35490202903748 and batch: 300, loss is 3.6478274059295654 and perplexity is 38.391166955374054
At time: 133.35674047470093 and batch: 350, loss is 3.714781656265259 and perplexity is 41.04962317778758
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.411625566153691 and perplexity of 82.40330671558057
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 135.24926662445068 and batch: 50, loss is 3.746087131500244 and perplexity is 42.35502768394564
At time: 136.26342105865479 and batch: 100, loss is 3.6955428743362426 and perplexity is 40.26742680798791
At time: 137.25352573394775 and batch: 150, loss is 3.596739535331726 and perplexity is 36.47910151324657
At time: 138.25092673301697 and batch: 200, loss is 3.684547390937805 and perplexity is 39.82709226697621
At time: 139.25541734695435 and batch: 250, loss is 3.694300513267517 and perplexity is 40.2174311873208
At time: 140.2545201778412 and batch: 300, loss is 3.6417194318771364 and perplexity is 38.15738938423872
At time: 141.25547695159912 and batch: 350, loss is 3.707293515205383 and perplexity is 40.743385814216005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.410234911688443 and perplexity of 82.28879183291703
Finished 18 epochs...
Completing Train Step...
At time: 143.15966653823853 and batch: 50, loss is 3.7433909940719605 and perplexity is 42.240986512947636
At time: 144.14684629440308 and batch: 100, loss is 3.6931431913375854 and perplexity is 40.170913595348964
At time: 145.134281873703 and batch: 150, loss is 3.594795880317688 and perplexity is 36.40826758534199
At time: 146.1302309036255 and batch: 200, loss is 3.6834899473190306 and perplexity is 39.78499962162605
At time: 147.12585878372192 and batch: 250, loss is 3.693478698730469 and perplexity is 40.184393495015634
At time: 148.13706731796265 and batch: 300, loss is 3.641609344482422 and perplexity is 38.153188967862995
At time: 149.1476640701294 and batch: 350, loss is 3.7080871438980103 and perplexity is 40.77573376866749
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.410061803357355 and perplexity of 82.27454819037771
Finished 19 epochs...
Completing Train Step...
At time: 151.0441017150879 and batch: 50, loss is 3.7420666313171385 and perplexity is 42.1850811513435
At time: 152.03922295570374 and batch: 100, loss is 3.6918531894683837 and perplexity is 40.1191264516611
At time: 153.03384065628052 and batch: 150, loss is 3.5937204933166504 and perplexity is 36.369135652407614
At time: 154.03071284294128 and batch: 200, loss is 3.6827256965637205 and perplexity is 39.7546055214516
At time: 155.0523180961609 and batch: 250, loss is 3.6929663228988647 and perplexity is 40.16380925686452
At time: 156.0507833957672 and batch: 300, loss is 3.641487884521484 and perplexity is 38.148555164437795
At time: 157.05539107322693 and batch: 350, loss is 3.708340382575989 and perplexity is 40.78606106916144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.410022077889278 and perplexity of 82.27127986035845
Finished 20 epochs...
Completing Train Step...
At time: 158.95236587524414 and batch: 50, loss is 3.740955934524536 and perplexity is 42.13825232813909
At time: 159.961243391037 and batch: 100, loss is 3.690802979469299 and perplexity is 40.07701506067951
At time: 160.95799851417542 and batch: 150, loss is 3.5928358125686644 and perplexity is 36.33697480641327
At time: 161.9637668132782 and batch: 200, loss is 3.682043342590332 and perplexity is 39.72748806131904
At time: 162.9604036808014 and batch: 250, loss is 3.692499237060547 and perplexity is 40.14505369091835
At time: 163.9697687625885 and batch: 300, loss is 3.6413070440292357 and perplexity is 38.1416569846972
At time: 164.96546983718872 and batch: 350, loss is 3.708399930000305 and perplexity is 40.78848984635912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.410020499393858 and perplexity of 82.27114999562251
Finished 21 epochs...
Completing Train Step...
At time: 166.84183549880981 and batch: 50, loss is 3.7399486446380616 and perplexity is 42.095828262988206
At time: 167.8392369747162 and batch: 100, loss is 3.6898575401306153 and perplexity is 40.03914257995173
At time: 168.83275365829468 and batch: 150, loss is 3.5920290613174437 and perplexity is 36.30767172825942
At time: 169.82751059532166 and batch: 200, loss is 3.681402087211609 and perplexity is 39.702020762310504
At time: 170.82139039039612 and batch: 250, loss is 3.692041001319885 and perplexity is 40.12666200669174
At time: 171.82155990600586 and batch: 300, loss is 3.641083846092224 and perplexity is 38.133144795530875
At time: 172.81588625907898 and batch: 350, loss is 3.708367233276367 and perplexity is 40.787156218169514
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.41003444277007 and perplexity of 82.2722971412158
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 174.71027970314026 and batch: 50, loss is 3.7397162771224974 and perplexity is 42.086047696346114
At time: 175.69486737251282 and batch: 100, loss is 3.689409728050232 and perplexity is 40.021216582254965
At time: 176.68783593177795 and batch: 150, loss is 3.5914771795272826 and perplexity is 36.28763971354995
At time: 177.68300104141235 and batch: 200, loss is 3.6799218702316283 and perplexity is 39.64329662998974
At time: 178.6785135269165 and batch: 250, loss is 3.690585188865662 and perplexity is 40.06828761378992
At time: 179.67467641830444 and batch: 300, loss is 3.6387944269180297 and perplexity is 38.04594190274012
At time: 180.69747304916382 and batch: 350, loss is 3.705216097831726 and perplexity is 40.65883265317538
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409512486951105 and perplexity of 82.22936584207868
Finished 23 epochs...
Completing Train Step...
At time: 182.59638738632202 and batch: 50, loss is 3.739018745422363 and perplexity is 42.0567015800576
At time: 183.5875861644745 and batch: 100, loss is 3.6887530851364136 and perplexity is 39.99494556026801
At time: 184.57293105125427 and batch: 150, loss is 3.5909694385528566 and perplexity is 36.269219668703656
At time: 185.5595281124115 and batch: 200, loss is 3.6796335411071777 and perplexity is 39.63186796067037
At time: 186.55317997932434 and batch: 250, loss is 3.6903741550445557 and perplexity is 40.05983274211292
At time: 187.5462372303009 and batch: 300, loss is 3.638782000541687 and perplexity is 38.045469132485145
At time: 188.54967975616455 and batch: 350, loss is 3.7055303192138673 and perplexity is 40.67161053520466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409400676858836 and perplexity of 82.22017228307263
Finished 24 epochs...
Completing Train Step...
At time: 190.43245315551758 and batch: 50, loss is 3.7385773134231566 and perplexity is 42.03814050322711
At time: 191.43130588531494 and batch: 100, loss is 3.688326148986816 and perplexity is 39.97787391671734
At time: 192.4222218990326 and batch: 150, loss is 3.590611906051636 and perplexity is 36.25625456173986
At time: 193.4255759716034 and batch: 200, loss is 3.6794096326828 and perplexity is 39.62299504495751
At time: 194.43457412719727 and batch: 250, loss is 3.6902223110198973 and perplexity is 40.05375035768077
At time: 195.44074487686157 and batch: 300, loss is 3.6387761545181276 and perplexity is 38.045246718426384
At time: 196.4447202682495 and batch: 350, loss is 3.7057009506225587 and perplexity is 40.67855098151625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.40936016214305 and perplexity of 82.21684122363956
Finished 25 epochs...
Completing Train Step...
At time: 198.32292413711548 and batch: 50, loss is 3.7382068824768067 and perplexity is 42.02257115891922
At time: 199.30958819389343 and batch: 100, loss is 3.68797004699707 and perplexity is 39.963640250738585
At time: 200.30778741836548 and batch: 150, loss is 3.590313024520874 and perplexity is 36.2454198561041
At time: 201.298504114151 and batch: 200, loss is 3.6792043447494507 and perplexity is 39.61486175705311
At time: 202.29235816001892 and batch: 250, loss is 3.6900868606567383 and perplexity is 40.04832543006144
At time: 203.28894066810608 and batch: 300, loss is 3.638756079673767 and perplexity is 38.0444829736859
At time: 204.28444838523865 and batch: 350, loss is 3.7057982301712036 and perplexity is 40.682508365078384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.4093430617759966 and perplexity of 82.21543529749766
Finished 26 epochs...
Completing Train Step...
At time: 206.17923069000244 and batch: 50, loss is 3.7378711318969726 and perplexity is 42.00846442459113
At time: 207.16846585273743 and batch: 100, loss is 3.6876485538482666 and perplexity is 39.95079427925328
At time: 208.15711498260498 and batch: 150, loss is 3.5900447082519533 and perplexity is 36.23569592488566
At time: 209.15119862556458 and batch: 200, loss is 3.6790079641342164 and perplexity is 39.60708292995927
At time: 210.14970588684082 and batch: 250, loss is 3.689956965446472 and perplexity is 40.043123682257274
At time: 211.14928078651428 and batch: 300, loss is 3.6387230253219602 and perplexity is 38.04322545874467
At time: 212.15651774406433 and batch: 350, loss is 3.7058539295196535 and perplexity is 40.684774417395865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409337800124596 and perplexity of 82.21500270967542
Finished 27 epochs...
Completing Train Step...
At time: 214.0267457962036 and batch: 50, loss is 3.737556047439575 and perplexity is 41.995230295415496
At time: 215.02282786369324 and batch: 100, loss is 3.6873478746414183 and perplexity is 39.938783711870855
At time: 216.01082801818848 and batch: 150, loss is 3.5897943067550657 and perplexity is 36.22662358829604
At time: 217.00578594207764 and batch: 200, loss is 3.6788171911239624 and perplexity is 39.59952768821236
At time: 217.99952602386475 and batch: 250, loss is 3.6898291158676146 and perplexity is 40.038004513007145
At time: 218.99405193328857 and batch: 300, loss is 3.6386799669265746 and perplexity is 38.04158741376717
At time: 219.98927354812622 and batch: 350, loss is 3.705883297920227 and perplexity is 40.685969281693744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409337010876886 and perplexity of 82.21493782169843
Finished 28 epochs...
Completing Train Step...
At time: 221.86227941513062 and batch: 50, loss is 3.737255210876465 and perplexity is 41.98259849481541
At time: 222.8735375404358 and batch: 100, loss is 3.687061381340027 and perplexity is 39.927343156771144
At time: 223.86967492103577 and batch: 150, loss is 3.589555583000183 and perplexity is 36.21797646486428
At time: 224.86456537246704 and batch: 200, loss is 3.6786305284500123 and perplexity is 39.59213662432625
At time: 225.85938715934753 and batch: 250, loss is 3.6897017908096315 and perplexity is 40.03290699628869
At time: 226.8553102016449 and batch: 300, loss is 3.6386292552948 and perplexity is 38.039658311708486
At time: 227.8502655029297 and batch: 350, loss is 3.705895128250122 and perplexity is 40.686450612979584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409340694032866 and perplexity of 82.21524063269597
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 229.7426598072052 and batch: 50, loss is 3.7371756744384768 and perplexity is 41.979259481262034
At time: 230.72571444511414 and batch: 100, loss is 3.686890859603882 and perplexity is 39.9205352573603
At time: 231.7372760772705 and batch: 150, loss is 3.5892655944824217 and perplexity is 36.20747519025103
At time: 232.74062538146973 and batch: 200, loss is 3.678143916130066 and perplexity is 39.57287528965331
At time: 233.7467188835144 and batch: 250, loss is 3.689201693534851 and perplexity is 40.01289165382457
At time: 234.74899196624756 and batch: 300, loss is 3.6378358364105225 and perplexity is 38.009488898526264
At time: 235.74923276901245 and batch: 350, loss is 3.7048059272766114 and perplexity is 40.642159016966794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.4092693986563845 and perplexity of 82.20937927510887
Finished 30 epochs...
Completing Train Step...
At time: 237.6295268535614 and batch: 50, loss is 3.7370347070693968 and perplexity is 41.97334219257914
At time: 238.6115379333496 and batch: 100, loss is 3.6867621994018553 and perplexity is 39.915399403626154
At time: 239.6060016155243 and batch: 150, loss is 3.5891768741607666 and perplexity is 36.20426299390149
At time: 240.60029077529907 and batch: 200, loss is 3.678079614639282 and perplexity is 39.570330776586445
At time: 241.59537959098816 and batch: 250, loss is 3.689162502288818 and perplexity is 40.01132352947185
At time: 242.59594202041626 and batch: 300, loss is 3.637830958366394 and perplexity is 38.009303487014336
At time: 243.59741306304932 and batch: 350, loss is 3.7048628091812135 and perplexity is 40.64447088612995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409225990032327 and perplexity of 82.20581075652275
Finished 31 epochs...
Completing Train Step...
At time: 245.49880981445312 and batch: 50, loss is 3.736912236213684 and perplexity is 41.96820199621224
At time: 246.5188467502594 and batch: 100, loss is 3.6866482877731324 and perplexity is 39.91085283442742
At time: 247.52075910568237 and batch: 150, loss is 3.589092273712158 and perplexity is 36.20120022656825
At time: 248.52049016952515 and batch: 200, loss is 3.678019890785217 and perplexity is 39.56796755449691
At time: 249.52026748657227 and batch: 250, loss is 3.689125666618347 and perplexity is 40.00984971268788
At time: 250.52362513542175 and batch: 300, loss is 3.637826247215271 and perplexity is 38.00912441986333
At time: 251.52223086357117 and batch: 350, loss is 3.704906425476074 and perplexity is 40.64624368601778
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409200207940463 and perplexity of 82.2036913460796
Finished 32 epochs...
Completing Train Step...
At time: 253.40879130363464 and batch: 50, loss is 3.7368006992340086 and perplexity is 41.96352125076213
At time: 254.39842128753662 and batch: 100, loss is 3.6865434646606445 and perplexity is 39.906669473871546
At time: 255.38566398620605 and batch: 150, loss is 3.5890105390548706 and perplexity is 36.19824145479307
At time: 256.3686788082123 and batch: 200, loss is 3.677962512969971 and perplexity is 39.56569729609676
At time: 257.3667266368866 and batch: 250, loss is 3.689089732170105 and perplexity is 40.008412006645955
At time: 258.3592882156372 and batch: 300, loss is 3.637820439338684 and perplexity is 38.00890366820058
At time: 259.3533868789673 and batch: 350, loss is 3.7049403047561644 and perplexity is 40.647620774819494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409183633738551 and perplexity of 82.20232889679208
Finished 33 epochs...
Completing Train Step...
At time: 261.2331008911133 and batch: 50, loss is 3.7366962814331055 and perplexity is 41.95913974091277
At time: 262.22976183891296 and batch: 100, loss is 3.6864446210861206 and perplexity is 39.9027251509521
At time: 263.2286877632141 and batch: 150, loss is 3.58893141746521 and perplexity is 36.195377505687844
At time: 264.2383894920349 and batch: 200, loss is 3.6779062700271608 and perplexity is 39.56347206742378
At time: 265.23652505874634 and batch: 250, loss is 3.689054365158081 and perplexity is 40.00699705367894
At time: 266.2342326641083 and batch: 300, loss is 3.6378133487701416 and perplexity is 38.00863416441936
At time: 267.23237442970276 and batch: 350, loss is 3.7049667835235596 and perplexity is 40.64869708796481
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409173373518319 and perplexity of 82.20148548712083
Finished 34 epochs...
Completing Train Step...
At time: 269.1018750667572 and batch: 50, loss is 3.7365970325469973 and perplexity is 41.95497554968051
At time: 270.099080324173 and batch: 100, loss is 3.6863502216339112 and perplexity is 39.89895853334231
At time: 271.0908796787262 and batch: 150, loss is 3.5888545751571654 and perplexity is 36.19259627619918
At time: 272.0833332538605 and batch: 200, loss is 3.6778511810302734 and perplexity is 39.56129261546667
At time: 273.0763478279114 and batch: 250, loss is 3.6890191078186034 and perplexity is 40.00558653826799
At time: 274.07145953178406 and batch: 300, loss is 3.6378050088882445 and perplexity is 38.008317178221176
At time: 275.07166838645935 and batch: 350, loss is 3.7049875593185426 and perplexity is 40.649541605734576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409166796454068 and perplexity of 82.20094484444715
Finished 35 epochs...
Completing Train Step...
At time: 276.9553484916687 and batch: 50, loss is 3.7365012884140016 and perplexity is 41.95095879921487
At time: 277.95481514930725 and batch: 100, loss is 3.6862590646743776 and perplexity is 39.89532163136083
At time: 278.9408791065216 and batch: 150, loss is 3.5887797927856444 and perplexity is 36.18988980921741
At time: 279.93787598609924 and batch: 200, loss is 3.67779655456543 and perplexity is 39.55913158093182
At time: 280.9354021549225 and batch: 250, loss is 3.688983941078186 and perplexity is 40.004179696928105
At time: 281.94598937034607 and batch: 300, loss is 3.6377953100204468 and perplexity is 38.00794854236533
At time: 282.94139671325684 and batch: 350, loss is 3.7050040626525877 and perplexity is 40.650212464234166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409162587132947 and perplexity of 82.20059883500208
Finished 36 epochs...
Completing Train Step...
At time: 284.832542181015 and batch: 50, loss is 3.736408271789551 and perplexity is 41.947056844111046
At time: 285.81886553764343 and batch: 100, loss is 3.686170563697815 and perplexity is 39.891791012670076
At time: 286.8093099594116 and batch: 150, loss is 3.5887066173553466 and perplexity is 36.18724169534781
At time: 287.8120393753052 and batch: 200, loss is 3.6777424955368043 and perplexity is 39.55699311050763
At time: 288.8073048591614 and batch: 250, loss is 3.6889488077163697 and perplexity is 40.002774240298
At time: 289.80229020118713 and batch: 300, loss is 3.6377844429016113 and perplexity is 38.00753550771608
At time: 290.7972903251648 and batch: 350, loss is 3.7050169563293456 and perplexity is 40.65073659831282
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.4091602193898165 and perplexity of 82.20040420532929
Finished 37 epochs...
Completing Train Step...
At time: 292.6798791885376 and batch: 50, loss is 3.736317687034607 and perplexity is 41.94325725234136
At time: 293.6725640296936 and batch: 100, loss is 3.6860841131210327 and perplexity is 39.888342493393544
At time: 294.6618390083313 and batch: 150, loss is 3.588634901046753 and perplexity is 36.18464657301265
At time: 295.6510055065155 and batch: 200, loss is 3.6776888084411623 and perplexity is 39.55486946744181
At time: 296.645037651062 and batch: 250, loss is 3.6889135885238646 and perplexity is 40.00136539970056
At time: 297.63670015335083 and batch: 300, loss is 3.6377725315093996 and perplexity is 38.007082787749916
At time: 298.6267659664154 and batch: 350, loss is 3.7050269508361815 and perplexity is 40.651142884407946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409159167059537 and perplexity of 82.20031770340044
Finished 38 epochs...
Completing Train Step...
At time: 300.49088501930237 and batch: 50, loss is 3.7362288856506347 and perplexity is 41.93953279841982
At time: 301.49537920951843 and batch: 100, loss is 3.6859993362426757 and perplexity is 39.8849610275712
At time: 302.4950532913208 and batch: 150, loss is 3.5885644483566286 and perplexity is 36.18209735712099
At time: 303.49299931526184 and batch: 200, loss is 3.6776353216171263 and perplexity is 39.552753859677914
At time: 304.4911849498749 and batch: 250, loss is 3.6888782739639283 and perplexity is 39.999952794027536
At time: 305.4892256259918 and batch: 300, loss is 3.6377595329284667 and perplexity is 38.006588752819155
At time: 306.4865734577179 and batch: 350, loss is 3.7050343894958497 and perplexity is 40.651445275549676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409158640894397 and perplexity of 82.20027445247015
Finished 39 epochs...
Completing Train Step...
At time: 308.3673446178436 and batch: 50, loss is 3.73614155292511 and perplexity is 41.93587026464516
At time: 309.35005164146423 and batch: 100, loss is 3.685916085243225 and perplexity is 39.8816407029147
At time: 310.3342955112457 and batch: 150, loss is 3.5884951066970827 and perplexity is 36.179588517428954
At time: 311.33225417137146 and batch: 200, loss is 3.677582139968872 and perplexity is 39.55065043496696
At time: 312.33020186424255 and batch: 250, loss is 3.688842968940735 and perplexity is 39.99854061969498
At time: 313.32553815841675 and batch: 300, loss is 3.6377458000183105 and perplexity is 38.00606681533434
At time: 314.3331079483032 and batch: 350, loss is 3.7050399446487425 and perplexity is 40.65167110117074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409157851646686 and perplexity of 82.20020957611733
Finished 40 epochs...
Completing Train Step...
At time: 316.22504353523254 and batch: 50, loss is 3.736055736541748 and perplexity is 41.93227163433883
At time: 317.21432995796204 and batch: 100, loss is 3.6858340644836427 and perplexity is 39.87836971459716
At time: 318.2008812427521 and batch: 150, loss is 3.5884265661239625 and perplexity is 36.17710883267719
At time: 319.19111347198486 and batch: 200, loss is 3.6775293493270875 and perplexity is 39.54856258585743
At time: 320.1892042160034 and batch: 250, loss is 3.6888073444366456 and perplexity is 39.99711571690198
At time: 321.19509172439575 and batch: 300, loss is 3.637731227874756 and perplexity is 38.00551298950799
At time: 322.20837569236755 and batch: 350, loss is 3.7050435590744017 and perplexity is 40.6518180338794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409160745554957 and perplexity of 82.20044745632792
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 324.0811536312103 and batch: 50, loss is 3.7360317754745482 and perplexity is 41.93126690439762
At time: 325.0812613964081 and batch: 100, loss is 3.685781488418579 and perplexity is 39.87627312195219
At time: 326.0691990852356 and batch: 150, loss is 3.58832471370697 and perplexity is 36.17342429434554
At time: 327.06504678726196 and batch: 200, loss is 3.6773775100708006 and perplexity is 39.542558017403316
At time: 328.0599272251129 and batch: 250, loss is 3.688649911880493 and perplexity is 39.99081936437439
At time: 329.0554633140564 and batch: 300, loss is 3.6374861717224123 and perplexity is 37.996200645797096
At time: 330.0494875907898 and batch: 350, loss is 3.704709043502808 and perplexity is 40.63822164196266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409153379242996 and perplexity of 82.1998419444188
Finished 42 epochs...
Completing Train Step...
At time: 331.92971658706665 and batch: 50, loss is 3.7360006189346313 and perplexity is 41.9299604915583
At time: 332.9295151233673 and batch: 100, loss is 3.685753512382507 and perplexity is 39.87515755750153
At time: 333.916540145874 and batch: 150, loss is 3.588304386138916 and perplexity is 36.172688984075
At time: 334.90275835990906 and batch: 200, loss is 3.6773616886138916 and perplexity is 39.54193240147466
At time: 335.8942940235138 and batch: 250, loss is 3.6886398124694826 and perplexity is 39.99041548269246
At time: 336.89388155937195 and batch: 300, loss is 3.637483015060425 and perplexity is 37.99608070482416
At time: 337.89570713043213 and batch: 350, loss is 3.7047161626815797 and perplexity is 40.638510953757326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.4091470652613145 and perplexity of 82.19932293776104
Finished 43 epochs...
Completing Train Step...
At time: 339.81110858917236 and batch: 50, loss is 3.735970559120178 and perplexity is 41.9287001036695
At time: 340.8094208240509 and batch: 100, loss is 3.685725965499878 and perplexity is 39.87405913634558
At time: 341.8075065612793 and batch: 150, loss is 3.588283905982971 and perplexity is 36.17194816934969
At time: 342.8074951171875 and batch: 200, loss is 3.6773456811904905 and perplexity is 39.541299442086654
At time: 343.80593514442444 and batch: 250, loss is 3.6886297512054442 and perplexity is 39.99001313058738
At time: 344.8047399520874 and batch: 300, loss is 3.637479796409607 and perplexity is 37.995958408904734
At time: 345.80158615112305 and batch: 350, loss is 3.704722514152527 and perplexity is 40.638769068898696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409141540527344 and perplexity of 82.1988688096237
Finished 44 epochs...
Completing Train Step...
At time: 347.6852779388428 and batch: 50, loss is 3.735941095352173 and perplexity is 41.92746474437614
At time: 348.6702308654785 and batch: 100, loss is 3.6856989812850953 and perplexity is 39.87298318068657
At time: 349.660475730896 and batch: 150, loss is 3.588263454437256 and perplexity is 36.17120840466279
At time: 350.664532661438 and batch: 200, loss is 3.6773299598693847 and perplexity is 39.54067780550767
At time: 351.65988659858704 and batch: 250, loss is 3.688619785308838 and perplexity is 39.98961459623712
At time: 352.6577217578888 and batch: 300, loss is 3.637476501464844 and perplexity is 37.995833214526805
At time: 353.65470337867737 and batch: 350, loss is 3.70472825050354 and perplexity is 40.639002187811435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409137068123653 and perplexity of 82.19850118392152
Finished 45 epochs...
Completing Train Step...
At time: 355.53750801086426 and batch: 50, loss is 3.735912256240845 and perplexity is 41.92625561098793
At time: 356.5405032634735 and batch: 100, loss is 3.6856723165512086 and perplexity is 39.871919992375666
At time: 357.53071808815 and batch: 150, loss is 3.588242826461792 and perplexity is 36.17046227355894
At time: 358.5350511074066 and batch: 200, loss is 3.6773141384124757 and perplexity is 39.540052219326476
At time: 359.5257318019867 and batch: 250, loss is 3.688609700202942 and perplexity is 39.98921129877282
At time: 360.5256028175354 and batch: 300, loss is 3.6374730348587034 and perplexity is 37.99570149816638
At time: 361.5265152454376 and batch: 350, loss is 3.7047333240509035 and perplexity is 40.63920837223689
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.4091336480502425 and perplexity of 82.198220059494
Finished 46 epochs...
Completing Train Step...
At time: 363.4182755947113 and batch: 50, loss is 3.7358839321136474 and perplexity is 41.92506810320873
At time: 364.3982207775116 and batch: 100, loss is 3.685646023750305 and perplexity is 39.8708716617035
At time: 365.3817024230957 and batch: 150, loss is 3.588222374916077 and perplexity is 36.16972253926059
At time: 366.3675785064697 and batch: 200, loss is 3.6772985553741453 and perplexity is 39.5394360699779
At time: 367.36227583885193 and batch: 250, loss is 3.688599638938904 and perplexity is 39.9888089587833
At time: 368.3582787513733 and batch: 300, loss is 3.637469506263733 and perplexity is 37.99556742696172
At time: 369.35351753234863 and batch: 350, loss is 3.704738097190857 and perplexity is 40.63940234932898
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409130227976832 and perplexity of 82.19793893602794
Finished 47 epochs...
Completing Train Step...
At time: 371.25225377082825 and batch: 50, loss is 3.7358561038970945 and perplexity is 41.923901419568
At time: 372.2577531337738 and batch: 100, loss is 3.6856200170516966 and perplexity is 39.86983476544412
At time: 373.255197763443 and batch: 150, loss is 3.5882018756866456 and perplexity is 36.16898109541933
At time: 374.2525587081909 and batch: 200, loss is 3.6772827768325804 and perplexity is 39.53881220026432
At time: 375.25670552253723 and batch: 250, loss is 3.688589587211609 and perplexity is 39.98840700420097
At time: 376.25651931762695 and batch: 300, loss is 3.6374660968780517 and perplexity is 37.99543788563901
At time: 377.2566864490509 and batch: 350, loss is 3.704742226600647 and perplexity is 40.6395701664214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409128386398842 and perplexity of 82.1977875622521
Finished 48 epochs...
Completing Train Step...
At time: 379.13352489471436 and batch: 50, loss is 3.735828647613525 and perplexity is 41.922750360844255
At time: 380.14236330986023 and batch: 100, loss is 3.6855943489074705 and perplexity is 39.86881139390917
At time: 381.12573289871216 and batch: 150, loss is 3.5881814527511597 and perplexity is 36.16824242619475
At time: 382.1107907295227 and batch: 200, loss is 3.677267303466797 and perplexity is 39.538200406493765
At time: 383.1163148880005 and batch: 250, loss is 3.6885795259475707 and perplexity is 39.98800467230362
At time: 384.1217041015625 and batch: 300, loss is 3.6374624490737917 and perplexity is 37.99529928597162
At time: 385.1175878047943 and batch: 350, loss is 3.704746112823486 and perplexity is 40.63972810115404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409125755573141 and perplexity of 82.19757131448452
Finished 49 epochs...
Completing Train Step...
At time: 386.99612379074097 and batch: 50, loss is 3.735801658630371 and perplexity is 41.92161892370923
At time: 387.99242901802063 and batch: 100, loss is 3.6855689239501954 and perplexity is 39.86779774396893
At time: 388.97953152656555 and batch: 150, loss is 3.5881610918045044 and perplexity is 36.167506014037144
At time: 389.9723494052887 and batch: 200, loss is 3.677251658439636 and perplexity is 39.53758183511331
At time: 390.96689558029175 and batch: 250, loss is 3.688569312095642 and perplexity is 39.98759624283079
At time: 391.96255445480347 and batch: 300, loss is 3.637458667755127 and perplexity is 37.995155613908885
At time: 392.95900797843933 and batch: 350, loss is 3.7047494220733643 and perplexity is 40.63986258839183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.409123913995151 and perplexity of 82.1974199413857
Finished Training.
Improved accuracyfrom -102.66273582175432 to -82.1974199413857
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd1f2403cf8>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'anneal': 4.142212715602351, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'lr': 21.402157677253108, 'dropout': 0.4517921910286671, 'seq_len': 35, 'batch_size': 80}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.1545448303222656 and batch: 50, loss is 6.668359670639038 and perplexity is 787.1034360851779
At time: 2.1053521633148193 and batch: 100, loss is 5.810416755676269 and perplexity is 333.7581923152978
At time: 3.028218984603882 and batch: 150, loss is 5.775861530303955 and perplexity is 322.4220914989283
At time: 3.952667713165283 and batch: 200, loss is 5.766023359298706 and perplexity is 319.2656003363961
At time: 4.876276969909668 and batch: 250, loss is 5.831769361495971 and perplexity is 340.9614296979432
At time: 5.79997706413269 and batch: 300, loss is 5.864650659561157 and perplexity is 352.359041342345
At time: 6.721587181091309 and batch: 350, loss is 5.930450782775879 and perplexity is 376.3241160246439
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.908991846544989 and perplexity of 368.334630333509
Finished 1 epochs...
Completing Train Step...
At time: 8.484605073928833 and batch: 50, loss is 5.743407974243164 and perplexity is 312.1263189316788
At time: 9.408058166503906 and batch: 100, loss is 5.725979623794555 and perplexity is 306.73360164576445
At time: 10.31803560256958 and batch: 150, loss is 5.7606002140045165 and perplexity is 317.53886300407027
At time: 11.239436864852905 and batch: 200, loss is 5.752181777954101 and perplexity is 314.8769028839577
At time: 12.193676233291626 and batch: 250, loss is 5.790326166152954 and perplexity is 327.1197023525244
At time: 13.174881219863892 and batch: 300, loss is 5.792365055084229 and perplexity is 327.78734348462433
At time: 14.165283918380737 and batch: 350, loss is 5.858384761810303 and perplexity is 350.15809826087474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.911635826373923 and perplexity of 369.3097882472549
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 16.05909538269043 and batch: 50, loss is 5.6671219825744625 and perplexity is 289.20100997198904
At time: 17.051626682281494 and batch: 100, loss is 5.489968957901001 and perplexity is 242.24968680247542
At time: 18.05151104927063 and batch: 150, loss is 5.429330997467041 and perplexity is 227.99666404527784
At time: 19.040491342544556 and batch: 200, loss is 5.359305248260498 and perplexity is 212.57720675011473
At time: 20.037153720855713 and batch: 250, loss is 5.315735740661621 and perplexity is 203.51419172585045
At time: 21.03346824645996 and batch: 300, loss is 5.313308391571045 and perplexity is 203.02079080804077
At time: 22.03101658821106 and batch: 350, loss is 5.384701223373413 and perplexity is 218.04494764043932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.506326083479257 and perplexity of 246.2447803992322
Finished 3 epochs...
Completing Train Step...
At time: 23.911922454833984 and batch: 50, loss is 5.369273509979248 and perplexity is 214.7068286623844
At time: 24.891878843307495 and batch: 100, loss is 5.317394876480103 and perplexity is 203.8521296758698
At time: 25.875332355499268 and batch: 150, loss is 5.292496309280396 and perplexity is 198.83917045197265
At time: 26.864494562149048 and batch: 200, loss is 5.264775085449219 and perplexity is 193.40280479030486
At time: 27.857871770858765 and batch: 250, loss is 5.281968202590942 and perplexity is 196.75675164982405
At time: 28.852141618728638 and batch: 300, loss is 5.28935206413269 and perplexity is 198.2149532153792
At time: 29.845876216888428 and batch: 350, loss is 5.347038164138794 and perplexity is 209.9854335236974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.479866290914601 and perplexity of 239.81463983428483
Finished 4 epochs...
Completing Train Step...
At time: 31.705960035324097 and batch: 50, loss is 5.312985143661499 and perplexity is 202.95517536741602
At time: 32.711260080337524 and batch: 100, loss is 5.274818086624146 and perplexity is 195.35493560414005
At time: 33.707823276519775 and batch: 150, loss is 5.264829082489014 and perplexity is 193.41324825120702
At time: 34.70514416694641 and batch: 200, loss is 5.250330924987793 and perplexity is 190.62934203442896
At time: 35.70910048484802 and batch: 250, loss is 5.249138946533203 and perplexity is 190.4022513364087
At time: 36.7066707611084 and batch: 300, loss is 5.253291778564453 and perplexity is 191.19460401963863
At time: 37.70397067070007 and batch: 350, loss is 5.3095509052276615 and perplexity is 202.25937436095828
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.458269316574623 and perplexity of 234.69089703606753
Finished 5 epochs...
Completing Train Step...
At time: 39.64717197418213 and batch: 50, loss is 5.277249355316162 and perplexity is 195.83047378921094
At time: 40.626280069351196 and batch: 100, loss is 5.241406087875366 and perplexity is 188.93557574478575
At time: 41.6043381690979 and batch: 150, loss is 5.231254692077637 and perplexity is 187.0273180619078
At time: 42.583014726638794 and batch: 200, loss is 5.206955852508545 and perplexity is 182.53754036670415
At time: 43.58125448226929 and batch: 250, loss is 5.223110675811768 and perplexity is 185.51035003200445
At time: 44.579320192337036 and batch: 300, loss is 5.22779203414917 and perplexity is 186.38082637192161
At time: 45.58005499839783 and batch: 350, loss is 5.281180667877197 and perplexity is 196.6018598770655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.440195544012662 and perplexity of 230.48724945593167
Finished 6 epochs...
Completing Train Step...
At time: 47.46969699859619 and batch: 50, loss is 5.247820787429809 and perplexity is 190.15143621893498
At time: 48.45361828804016 and batch: 100, loss is 5.21388617515564 and perplexity is 183.80697814248154
At time: 49.447590827941895 and batch: 150, loss is 5.209102687835693 and perplexity is 182.92983935688721
At time: 50.44249391555786 and batch: 200, loss is 5.191234359741211 and perplexity is 179.69021846774663
At time: 51.440515995025635 and batch: 250, loss is 5.200501203536987 and perplexity is 181.3631189359864
At time: 52.44031548500061 and batch: 300, loss is 5.205854015350342 and perplexity is 182.33652448568148
At time: 53.442073583602905 and batch: 350, loss is 5.257084684371948 and perplexity is 191.9211641596413
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.4225179738011855 and perplexity of 226.44859687462213
Finished 7 epochs...
Completing Train Step...
At time: 55.31975317001343 and batch: 50, loss is 5.228665466308594 and perplexity is 186.54368849372852
At time: 56.33538842201233 and batch: 100, loss is 5.195176992416382 and perplexity is 180.40006941574308
At time: 57.330050468444824 and batch: 150, loss is 5.185727462768555 and perplexity is 178.70340258818052
At time: 58.325693130493164 and batch: 200, loss is 5.173087530136108 and perplexity is 176.45881919400648
At time: 59.321391582489014 and batch: 250, loss is 5.188642377853394 and perplexity is 179.2250677675765
At time: 60.31908464431763 and batch: 300, loss is 5.187838277816772 and perplexity is 179.0810108098856
At time: 61.32541036605835 and batch: 350, loss is 5.247183752059937 and perplexity is 190.0303416033032
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.417311569740033 and perplexity of 225.27267779142394
Finished 8 epochs...
Completing Train Step...
At time: 63.186014890670776 and batch: 50, loss is 5.213471879959107 and perplexity is 183.73084356653138
At time: 64.17766284942627 and batch: 100, loss is 5.175704898834229 and perplexity is 176.92128193761383
At time: 65.16465401649475 and batch: 150, loss is 5.163629360198975 and perplexity is 174.79770960667952
At time: 66.15860557556152 and batch: 200, loss is 5.153117237091064 and perplexity is 172.96983879812922
At time: 67.15166759490967 and batch: 250, loss is 5.163548946380615 and perplexity is 174.7836540205488
At time: 68.15794372558594 and batch: 300, loss is 5.164587793350219 and perplexity is 174.96532183605612
At time: 69.15015482902527 and batch: 350, loss is 5.2171924114227295 and perplexity is 184.4156931631005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.396160257273707 and perplexity of 220.55790260748162
Finished 9 epochs...
Completing Train Step...
At time: 71.01668643951416 and batch: 50, loss is 5.184134387969971 and perplexity is 178.41894134527558
At time: 72.02097058296204 and batch: 100, loss is 5.152731285095215 and perplexity is 172.90309362466837
At time: 73.01313924789429 and batch: 150, loss is 5.139799652099609 and perplexity is 170.68156920155135
At time: 73.99979996681213 and batch: 200, loss is 5.127972288131714 and perplexity is 168.67474726064341
At time: 74.98517346382141 and batch: 250, loss is 5.1440891456604 and perplexity is 171.41527919071754
At time: 75.97925734519958 and batch: 300, loss is 5.1489612579345705 and perplexity is 172.25247146775857
At time: 76.97558307647705 and batch: 350, loss is 5.203421249389648 and perplexity is 181.89348152394666
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.394482842807112 and perplexity of 220.18824571151433
Finished 10 epochs...
Completing Train Step...
At time: 78.86021065711975 and batch: 50, loss is 5.16599326133728 and perplexity is 175.21140288375204
At time: 79.84782075881958 and batch: 100, loss is 5.1349157619476316 and perplexity is 169.8500114378758
At time: 80.83144760131836 and batch: 150, loss is 5.1176787281036376 and perplexity is 166.94738920856216
At time: 81.81606101989746 and batch: 200, loss is 5.112578554153442 and perplexity is 166.09809609534122
At time: 82.8143982887268 and batch: 250, loss is 5.1234071254730225 and perplexity is 167.90647458213527
At time: 83.81161832809448 and batch: 300, loss is 5.130149669647217 and perplexity is 169.0424166704896
At time: 84.81594228744507 and batch: 350, loss is 5.182418050765992 and perplexity is 178.1129769225205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.381481302195582 and perplexity of 217.343989216871
Finished 11 epochs...
Completing Train Step...
At time: 86.69488906860352 and batch: 50, loss is 5.148859233856201 and perplexity is 172.23489846456016
At time: 87.69704174995422 and batch: 100, loss is 5.11965765953064 and perplexity is 167.27809375661002
At time: 88.68880248069763 and batch: 150, loss is 5.106229629516601 and perplexity is 165.0468923386886
At time: 89.69530463218689 and batch: 200, loss is 5.098289413452148 and perplexity is 163.7415734677199
At time: 90.7022807598114 and batch: 250, loss is 5.110867233276367 and perplexity is 165.81409203620024
At time: 91.69700741767883 and batch: 300, loss is 5.117236394882202 and perplexity is 166.8735591620282
At time: 92.69638967514038 and batch: 350, loss is 5.172355546951294 and perplexity is 176.32970156727515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.3760791647023165 and perplexity of 216.17303278528343
Finished 12 epochs...
Completing Train Step...
At time: 94.58153772354126 and batch: 50, loss is 5.137012004852295 and perplexity is 170.2064317604794
At time: 95.58069276809692 and batch: 100, loss is 5.106005878448486 and perplexity is 165.009967051429
At time: 96.5810272693634 and batch: 150, loss is 5.092161207199097 and perplexity is 162.7411997124056
At time: 97.5661256313324 and batch: 200, loss is 5.083528289794922 and perplexity is 161.3423152917708
At time: 98.55093145370483 and batch: 250, loss is 5.093937377929688 and perplexity is 163.03051272658996
At time: 99.54094958305359 and batch: 300, loss is 5.1062659072875975 and perplexity is 165.05287998066098
At time: 100.53573608398438 and batch: 350, loss is 5.163545160293579 and perplexity is 174.78299227567493
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.37317262846848 and perplexity of 215.54563025849222
Finished 13 epochs...
Completing Train Step...
At time: 102.41067552566528 and batch: 50, loss is 5.124314889907837 and perplexity is 168.05896330958646
At time: 103.40001654624939 and batch: 100, loss is 5.09316837310791 and perplexity is 162.9051896694
At time: 104.38460230827332 and batch: 150, loss is 5.080668992996216 and perplexity is 160.88164863136632
At time: 105.3685655593872 and batch: 200, loss is 5.076503963470459 and perplexity is 160.21296532418646
At time: 106.35808801651001 and batch: 250, loss is 5.08582649230957 and perplexity is 161.71353901675056
At time: 107.35303449630737 and batch: 300, loss is 5.094701948165894 and perplexity is 163.1552086675422
At time: 108.35362672805786 and batch: 350, loss is 5.149697742462158 and perplexity is 172.37937947498884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.368903850686961 and perplexity of 214.62747495587308
Finished 14 epochs...
Completing Train Step...
At time: 110.22828412055969 and batch: 50, loss is 5.115745620727539 and perplexity is 166.62497371135362
At time: 111.23235392570496 and batch: 100, loss is 5.0850972652435305 and perplexity is 161.59565611408004
At time: 112.22806668281555 and batch: 150, loss is 5.070360231399536 and perplexity is 159.2316772630069
At time: 113.22461271286011 and batch: 200, loss is 5.065311508178711 and perplexity is 158.42978656194222
At time: 114.2228786945343 and batch: 250, loss is 5.07324291229248 and perplexity is 159.6913536083933
At time: 115.22119760513306 and batch: 300, loss is 5.080419998168946 and perplexity is 160.8415949198448
At time: 116.22821044921875 and batch: 350, loss is 5.141391172409057 and perplexity is 170.95342866292762
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.359413410055226 and perplexity of 212.60020072582745
Finished 15 epochs...
Completing Train Step...
At time: 118.10191440582275 and batch: 50, loss is 5.105348024368286 and perplexity is 164.9014502693481
At time: 119.09666395187378 and batch: 100, loss is 5.0732275581359865 and perplexity is 159.68890170118283
At time: 120.08228850364685 and batch: 150, loss is 5.057631893157959 and perplexity is 157.2177666731155
At time: 121.06959176063538 and batch: 200, loss is 5.0525020980834965 and perplexity is 156.41333679227137
At time: 122.05925989151001 and batch: 250, loss is 5.063480558395386 and perplexity is 158.13997497475896
At time: 123.05317187309265 and batch: 300, loss is 5.07331488609314 and perplexity is 159.70284761567373
At time: 124.0581226348877 and batch: 350, loss is 5.12776515007019 and perplexity is 168.63981191881226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.361833243534483 and perplexity of 213.11528076185678
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 125.95672082901001 and batch: 50, loss is 5.061082324981689 and perplexity is 157.7611728122924
At time: 126.95103311538696 and batch: 100, loss is 4.982361383438111 and perplexity is 145.8183083607162
At time: 127.93976330757141 and batch: 150, loss is 4.930896072387696 and perplexity is 138.5035659722078
At time: 128.9362757205963 and batch: 200, loss is 4.9114369201660155 and perplexity is 135.83445751546267
At time: 129.9359576702118 and batch: 250, loss is 4.915197916030884 and perplexity is 136.34629224931487
At time: 130.9468913078308 and batch: 300, loss is 4.915687341690063 and perplexity is 136.41303995596337
At time: 131.95127844810486 and batch: 350, loss is 4.991706123352051 and perplexity is 147.18732913621133
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.264275912580819 and perplexity of 193.30628744890444
Finished 17 epochs...
Completing Train Step...
At time: 133.8389208316803 and batch: 50, loss is 4.985004835128784 and perplexity is 146.2042819409801
At time: 134.82035851478577 and batch: 100, loss is 4.936546001434326 and perplexity is 139.28831609637126
At time: 135.81224536895752 and batch: 150, loss is 4.892642812728882 and perplexity is 133.30541016283084
At time: 136.807758808136 and batch: 200, loss is 4.894092063903809 and perplexity is 133.4987432453934
At time: 137.804927110672 and batch: 250, loss is 4.9163820171356205 and perplexity is 136.50783566758906
At time: 138.8047640323639 and batch: 300, loss is 4.92161003112793 and perplexity is 137.22336932274547
At time: 139.80622005462646 and batch: 350, loss is 4.986559038162231 and perplexity is 146.43168975265496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.256647044214709 and perplexity of 191.83719012772556
Finished 18 epochs...
Completing Train Step...
At time: 141.70888209342957 and batch: 50, loss is 4.9692810440063475 and perplexity is 143.92337559166296
At time: 142.720360994339 and batch: 100, loss is 4.926079940795899 and perplexity is 137.83811830059074
At time: 143.7209119796753 and batch: 150, loss is 4.888475379943848 and perplexity is 132.75102481029955
At time: 144.73578262329102 and batch: 200, loss is 4.892875051498413 and perplexity is 133.33637244244056
At time: 145.74150681495667 and batch: 250, loss is 4.915185871124268 and perplexity is 136.34464998084778
At time: 146.7535915374756 and batch: 300, loss is 4.9186703395843505 and perplexity is 136.8205672913061
At time: 147.75378155708313 and batch: 350, loss is 4.981795043945312 and perplexity is 145.73574907441733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.253379032529634 and perplexity of 191.21128723478833
Finished 19 epochs...
Completing Train Step...
At time: 149.63983058929443 and batch: 50, loss is 4.963067836761475 and perplexity is 143.0319220922009
At time: 150.6159484386444 and batch: 100, loss is 4.921249151229858 and perplexity is 137.17385710173124
At time: 151.59411644935608 and batch: 150, loss is 4.883923015594482 and perplexity is 132.14806726014757
At time: 152.5715901851654 and batch: 200, loss is 4.8900587940216065 and perplexity is 132.96139115638547
At time: 153.5546247959137 and batch: 250, loss is 4.912471923828125 and perplexity is 135.9751194566872
At time: 154.5506670475006 and batch: 300, loss is 4.915363340377808 and perplexity is 136.36884911134132
At time: 155.54814529418945 and batch: 350, loss is 4.977267751693725 and perplexity is 145.07745202281555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.248882425242457 and perplexity of 190.35341536913546
Finished 20 epochs...
Completing Train Step...
At time: 157.43735766410828 and batch: 50, loss is 4.95615852355957 and perplexity is 142.0470759676893
At time: 158.41914772987366 and batch: 100, loss is 4.913500385284424 and perplexity is 136.11503656340676
At time: 159.40240502357483 and batch: 150, loss is 4.874991827011108 and perplexity is 130.97308276489366
At time: 160.3891077041626 and batch: 200, loss is 4.879373264312744 and perplexity is 131.5481920979417
At time: 161.37601566314697 and batch: 250, loss is 4.90004186630249 and perplexity is 134.29540201916544
At time: 162.36496257781982 and batch: 300, loss is 4.900518741607666 and perplexity is 134.35945945247713
At time: 163.36869835853577 and batch: 350, loss is 4.95997031211853 and perplexity is 142.58956265211654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.233667834051724 and perplexity of 187.47918652552343
Finished 21 epochs...
Completing Train Step...
At time: 165.24331998825073 and batch: 50, loss is 4.940117111206055 and perplexity is 139.78661918101383
At time: 166.2384593486786 and batch: 100, loss is 4.8981531143188475 and perplexity is 134.04199070310523
At time: 167.2261459827423 and batch: 150, loss is 4.863961181640625 and perplexity is 129.5363040026611
At time: 168.22103118896484 and batch: 200, loss is 4.869044675827026 and perplexity is 130.19647762369024
At time: 169.2179617881775 and batch: 250, loss is 4.888587875366211 and perplexity is 132.76595953293295
At time: 170.2308177947998 and batch: 300, loss is 4.888422756195069 and perplexity is 132.74403913752752
At time: 171.23452067375183 and batch: 350, loss is 4.949732942581177 and perplexity is 141.13726713383696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.230871792497306 and perplexity of 186.95571908879458
Finished 22 epochs...
Completing Train Step...
At time: 173.12817406654358 and batch: 50, loss is 4.9325572681427 and perplexity is 138.7338387191542
At time: 174.13846445083618 and batch: 100, loss is 4.890634822845459 and perplexity is 133.038002813294
At time: 175.14717936515808 and batch: 150, loss is 4.854830865859985 and perplexity is 128.35897949306545
At time: 176.14502835273743 and batch: 200, loss is 4.862286653518677 and perplexity is 129.31957333024184
At time: 177.14467668533325 and batch: 250, loss is 4.881968278884887 and perplexity is 131.8900048861413
At time: 178.14269852638245 and batch: 300, loss is 4.883091583251953 and perplexity is 132.03824074600942
At time: 179.14083218574524 and batch: 350, loss is 4.944690561294555 and perplexity is 140.42739045771674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.225694853683998 and perplexity of 185.99036172429822
Finished 23 epochs...
Completing Train Step...
At time: 181.10579538345337 and batch: 50, loss is 4.927123517990112 and perplexity is 137.9820380999914
At time: 182.09007740020752 and batch: 100, loss is 4.884349756240844 and perplexity is 132.20447224607977
At time: 183.07434844970703 and batch: 150, loss is 4.847959012985229 and perplexity is 127.47993924764769
At time: 184.058664560318 and batch: 200, loss is 4.858413391113281 and perplexity is 128.81965347396223
At time: 185.04505228996277 and batch: 250, loss is 4.8789297962188725 and perplexity is 131.48986760542135
At time: 186.03084874153137 and batch: 300, loss is 4.876881017684936 and perplexity is 131.2207497628058
At time: 187.03344058990479 and batch: 350, loss is 4.939446048736572 and perplexity is 139.69284509479962
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.219927031418373 and perplexity of 184.92069017789854
Finished 24 epochs...
Completing Train Step...
At time: 188.9903085231781 and batch: 50, loss is 4.920769443511963 and perplexity is 137.10806952445193
At time: 189.96916460990906 and batch: 100, loss is 4.880837440490723 and perplexity is 131.7409429032012
At time: 190.95016741752625 and batch: 150, loss is 4.844160661697388 and perplexity is 126.99664409966681
At time: 191.93150424957275 and batch: 200, loss is 4.853964347839355 and perplexity is 128.24780229970224
At time: 192.91253972053528 and batch: 250, loss is 4.872942104339599 and perplexity is 130.70489921264135
At time: 193.90117979049683 and batch: 300, loss is 4.871492214202881 and perplexity is 130.5155287847842
At time: 194.89683747291565 and batch: 350, loss is 4.934991731643676 and perplexity is 139.07199263053485
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.218716851596175 and perplexity of 184.69703824676088
Finished 25 epochs...
Completing Train Step...
At time: 196.79039025306702 and batch: 50, loss is 4.916920461654663 and perplexity is 136.58135735540927
At time: 197.8002381324768 and batch: 100, loss is 4.87563648223877 and perplexity is 131.05754246815067
At time: 198.7904188632965 and batch: 150, loss is 4.838760910034179 and perplexity is 126.31274187229134
At time: 199.77965807914734 and batch: 200, loss is 4.850740585327149 and perplexity is 127.83502754375276
At time: 200.7690031528473 and batch: 250, loss is 4.8686761379241945 and perplexity is 130.1485041274396
At time: 201.75992679595947 and batch: 300, loss is 4.868488740921021 and perplexity is 130.12411697290977
At time: 202.75843048095703 and batch: 350, loss is 4.931116981506348 and perplexity is 138.53416605269084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.216447501346983 and perplexity of 184.27837120774151
Finished 26 epochs...
Completing Train Step...
At time: 204.65838623046875 and batch: 50, loss is 4.911560916900635 and perplexity is 135.8513015889269
At time: 205.64401745796204 and batch: 100, loss is 4.870681781768798 and perplexity is 130.4097976168765
At time: 206.63015484809875 and batch: 150, loss is 4.834281301498413 and perplexity is 125.74817569744249
At time: 207.623539686203 and batch: 200, loss is 4.847116403579712 and perplexity is 127.3725686938947
At time: 208.63540983200073 and batch: 250, loss is 4.864553813934326 and perplexity is 129.6130941515333
At time: 209.64326429367065 and batch: 300, loss is 4.863611631393432 and perplexity is 129.4910324683755
At time: 210.63926768302917 and batch: 350, loss is 4.925954666137695 and perplexity is 137.82085175898587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.213111614358836 and perplexity of 183.66466358578674
Finished 27 epochs...
Completing Train Step...
At time: 212.5298674106598 and batch: 50, loss is 4.907014293670654 and perplexity is 135.23503892397727
At time: 213.51173400878906 and batch: 100, loss is 4.867306642532348 and perplexity is 129.97038834299266
At time: 214.49464464187622 and batch: 150, loss is 4.829382467269897 and perplexity is 125.13366265787126
At time: 215.48186659812927 and batch: 200, loss is 4.8434091091156 and perplexity is 126.90123530077074
At time: 216.46914410591125 and batch: 250, loss is 4.859799947738647 and perplexity is 128.99839310563806
At time: 217.45477676391602 and batch: 300, loss is 4.858944416046143 and perplexity is 128.88807808777958
At time: 218.4402198791504 and batch: 350, loss is 4.921084327697754 and perplexity is 137.1512494852761
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.209023837385507 and perplexity of 182.9154158253599
Finished 28 epochs...
Completing Train Step...
At time: 220.3264274597168 and batch: 50, loss is 4.902838935852051 and perplexity is 134.67156142556672
At time: 221.32812571525574 and batch: 100, loss is 4.862743616104126 and perplexity is 129.3786810407976
At time: 222.31603860855103 and batch: 150, loss is 4.824797010421753 and perplexity is 124.56118119512753
At time: 223.3121771812439 and batch: 200, loss is 4.83854868888855 and perplexity is 126.28593848172736
At time: 224.30982971191406 and batch: 250, loss is 4.854369115829468 and perplexity is 128.2997234121688
At time: 225.30563855171204 and batch: 300, loss is 4.8541281795501705 and perplexity is 128.26881507779402
At time: 226.30639338493347 and batch: 350, loss is 4.917092657089233 and perplexity is 136.60487806661467
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.206442471208243 and perplexity of 182.4438530575907
Finished 29 epochs...
Completing Train Step...
At time: 228.19081735610962 and batch: 50, loss is 4.8985093975067135 and perplexity is 134.08975611936296
At time: 229.1971399784088 and batch: 100, loss is 4.858400344848633 and perplexity is 128.81797286963396
At time: 230.18622469902039 and batch: 150, loss is 4.819827461242676 and perplexity is 123.9437038422001
At time: 231.17325139045715 and batch: 200, loss is 4.83358247756958 and perplexity is 125.66033056097474
At time: 232.15887212753296 and batch: 250, loss is 4.850231914520264 and perplexity is 127.77001813274047
At time: 233.14502882957458 and batch: 300, loss is 4.850551881790161 and perplexity is 127.81090689780154
At time: 234.1347939968109 and batch: 350, loss is 4.91329984664917 and perplexity is 136.08774297653758
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.204185222757274 and perplexity of 182.03249639464372
Finished 30 epochs...
Completing Train Step...
At time: 236.02602982521057 and batch: 50, loss is 4.894387845993042 and perplexity is 133.53823562286152
At time: 237.01319408416748 and batch: 100, loss is 4.854076414108277 and perplexity is 128.2621753577558
At time: 238.0005133152008 and batch: 150, loss is 4.815957565307617 and perplexity is 123.46498150678399
At time: 238.98979568481445 and batch: 200, loss is 4.82993366241455 and perplexity is 125.20265473746383
At time: 239.9817361831665 and batch: 250, loss is 4.846644945144654 and perplexity is 127.31253197550966
At time: 240.9811737537384 and batch: 300, loss is 4.846328516006469 and perplexity is 127.27225295380073
At time: 241.97929978370667 and batch: 350, loss is 4.9089500427246096 and perplexity is 135.49707355745988
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.199976427801724 and perplexity of 181.26796894028524
Finished 31 epochs...
Completing Train Step...
At time: 243.86400055885315 and batch: 50, loss is 4.889809923171997 and perplexity is 132.92830505925647
At time: 244.84466195106506 and batch: 100, loss is 4.850086231231689 and perplexity is 127.75140553212297
At time: 245.82737112045288 and batch: 150, loss is 4.810735263824463 and perplexity is 122.8218908190231
At time: 246.82740092277527 and batch: 200, loss is 4.825372066497803 and perplexity is 124.63283145869897
At time: 247.82291913032532 and batch: 250, loss is 4.842850675582886 and perplexity is 126.83038917890207
At time: 248.81896567344666 and batch: 300, loss is 4.841363363265991 and perplexity is 126.64189299001906
At time: 249.81469655036926 and batch: 350, loss is 4.9048888206481935 and perplexity is 134.94790575205525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.197469777074353 and perplexity of 180.8141624588821
Finished 32 epochs...
Completing Train Step...
At time: 251.68876147270203 and batch: 50, loss is 4.885901403427124 and perplexity is 132.40976617413796
At time: 252.70113968849182 and batch: 100, loss is 4.846355047225952 and perplexity is 127.27562968667213
At time: 253.69970679283142 and batch: 150, loss is 4.806141548156738 and perplexity is 122.25897590026511
At time: 254.68965649604797 and batch: 200, loss is 4.820445795059204 and perplexity is 124.02036612462967
At time: 255.68578672409058 and batch: 250, loss is 4.837562932968139 and perplexity is 126.16151270700193
At time: 256.68601393699646 and batch: 300, loss is 4.836880311965943 and perplexity is 126.07542159590251
At time: 257.68507742881775 and batch: 350, loss is 4.899662046432495 and perplexity is 134.2444036427411
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.193856601057382 and perplexity of 180.1620279114803
Finished 33 epochs...
Completing Train Step...
At time: 259.5715522766113 and batch: 50, loss is 4.881050157546997 and perplexity is 131.76896942952075
At time: 260.55402278900146 and batch: 100, loss is 4.839449214935303 and perplexity is 126.3997134796545
At time: 261.53642749786377 and batch: 150, loss is 4.799920845031738 and perplexity is 121.50079974614496
At time: 262.52250146865845 and batch: 200, loss is 4.814475603103638 and perplexity is 123.28214658136254
At time: 263.5148627758026 and batch: 250, loss is 4.8304251861572265 and perplexity is 125.26420994159625
At time: 264.5120804309845 and batch: 300, loss is 4.829235954284668 and perplexity is 125.1153302944
At time: 265.5084354877472 and batch: 350, loss is 4.893522109985351 and perplexity is 133.42267679281704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.189379988045528 and perplexity of 179.35731477162088
Finished 34 epochs...
Completing Train Step...
At time: 267.4004864692688 and batch: 50, loss is 4.874834699630737 and perplexity is 130.95250492421314
At time: 268.3854262828827 and batch: 100, loss is 4.833118486404419 and perplexity is 125.6020388022595
At time: 269.37156081199646 and batch: 150, loss is 4.792324342727661 and perplexity is 120.5813154947505
At time: 270.35750365257263 and batch: 200, loss is 4.80912034034729 and perplexity is 122.62370293623354
At time: 271.357994556427 and batch: 250, loss is 4.826796560287476 and perplexity is 124.81049666454953
At time: 272.34503698349 and batch: 300, loss is 4.823640184402466 and perplexity is 124.41716889437656
At time: 273.334308385849 and batch: 350, loss is 4.887933931350708 and perplexity is 132.6791664102485
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.186627618197737 and perplexity of 178.86433584784055
Finished 35 epochs...
Completing Train Step...
At time: 275.2110331058502 and batch: 50, loss is 4.867825775146485 and perplexity is 130.03787772690794
At time: 276.21002554893494 and batch: 100, loss is 4.826145534515381 and perplexity is 124.72926825835481
At time: 277.2001841068268 and batch: 150, loss is 4.786366186141968 and perplexity is 119.86500918734478
At time: 278.19771575927734 and batch: 200, loss is 4.804167203903198 and perplexity is 122.01783272190902
At time: 279.19326090812683 and batch: 250, loss is 4.821929864883423 and perplexity is 124.20455765032014
At time: 280.1912000179291 and batch: 300, loss is 4.817459087371827 and perplexity is 123.65050615049017
At time: 281.19383549690247 and batch: 350, loss is 4.881014947891235 and perplexity is 131.7643299711444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.182243084085399 and perplexity of 178.08181581233538
Finished 36 epochs...
Completing Train Step...
At time: 283.06502079963684 and batch: 50, loss is 4.860964260101318 and perplexity is 129.1486750000523
At time: 284.0626745223999 and batch: 100, loss is 4.820434226989746 and perplexity is 124.01893145671833
At time: 285.04699325561523 and batch: 150, loss is 4.778456478118897 and perplexity is 118.92065168207746
At time: 286.0336956977844 and batch: 200, loss is 4.79547423362732 and perplexity is 120.96173230428418
At time: 287.01993799209595 and batch: 250, loss is 4.815075435638428 and perplexity is 123.3561174066767
At time: 288.0075807571411 and batch: 300, loss is 4.810353841781616 and perplexity is 122.77505277561953
At time: 288.9981119632721 and batch: 350, loss is 4.874294557571411 and perplexity is 130.8817910680127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.175557629815463 and perplexity of 176.89522883247884
Finished 37 epochs...
Completing Train Step...
At time: 290.8881709575653 and batch: 50, loss is 4.853400506973267 and perplexity is 128.17551132999634
At time: 291.8784761428833 and batch: 100, loss is 4.811678047180176 and perplexity is 122.93773985509502
At time: 292.8686134815216 and batch: 150, loss is 4.76882625579834 and perplexity is 117.7809161296069
At time: 293.8662939071655 and batch: 200, loss is 4.787932386398316 and perplexity is 120.0528888856715
At time: 294.8694851398468 and batch: 250, loss is 4.804861574172974 and perplexity is 122.10258769959658
At time: 295.8731966018677 and batch: 300, loss is 4.799013118743897 and perplexity is 121.39056031740483
At time: 296.88759565353394 and batch: 350, loss is 4.8627876281738285 and perplexity is 129.3843753896347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.16455130741514 and perplexity of 174.95893817914003
Finished 38 epochs...
Completing Train Step...
At time: 298.77664828300476 and batch: 50, loss is 4.840817213058472 and perplexity is 126.57274637781899
At time: 299.75709199905396 and batch: 100, loss is 4.798589925765992 and perplexity is 121.33919955321842
At time: 300.73856258392334 and batch: 150, loss is 4.757518634796143 and perplexity is 116.456595736126
At time: 301.7202982902527 and batch: 200, loss is 4.778333959579467 and perplexity is 118.90608259003555
At time: 302.71213459968567 and batch: 250, loss is 4.794367389678955 and perplexity is 120.82792061089211
At time: 303.71041345596313 and batch: 300, loss is 4.791327247619629 and perplexity is 120.46114437592408
At time: 304.7080180644989 and batch: 350, loss is 4.855227346420288 and perplexity is 128.40988142332088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.160850261819774 and perplexity of 174.3126039655955
Finished 39 epochs...
Completing Train Step...
At time: 306.58541464805603 and batch: 50, loss is 4.831161880493164 and perplexity is 125.35652537550949
At time: 307.59056758880615 and batch: 100, loss is 4.788868837356567 and perplexity is 120.16536518455871
At time: 308.5795798301697 and batch: 150, loss is 4.7457051181793215 and perplexity is 115.08892819471467
At time: 309.57477164268494 and batch: 200, loss is 4.765512132644654 and perplexity is 117.39122177262617
At time: 310.56851029396057 and batch: 250, loss is 4.781946458816528 and perplexity is 119.33640752894703
At time: 311.55729722976685 and batch: 300, loss is 4.782324247360229 and perplexity is 119.3814999737261
At time: 312.554438829422 and batch: 350, loss is 4.845366411209106 and perplexity is 127.14986259427954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.153097744645743 and perplexity of 172.96646722586445
Finished 40 epochs...
Completing Train Step...
At time: 314.4414176940918 and batch: 50, loss is 4.820019721984863 and perplexity is 123.96753564157653
At time: 315.4287989139557 and batch: 100, loss is 4.778239517211914 and perplexity is 118.89485334834572
At time: 316.41583943367004 and batch: 150, loss is 4.737981052398681 and perplexity is 114.20339809174247
At time: 317.4031527042389 and batch: 200, loss is 4.757759971618652 and perplexity is 116.48470439259171
At time: 318.3961319923401 and batch: 250, loss is 4.774941568374634 and perplexity is 118.5033900727256
At time: 319.39873027801514 and batch: 300, loss is 4.772537078857422 and perplexity is 118.2187922069526
At time: 320.39854192733765 and batch: 350, loss is 4.835795459747314 and perplexity is 125.93872255760395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.144518622036638 and perplexity of 171.48891381469002
Finished 41 epochs...
Completing Train Step...
At time: 322.2753245830536 and batch: 50, loss is 4.811427392959595 and perplexity is 122.90692885334676
At time: 323.2563371658325 and batch: 100, loss is 4.770047073364258 and perplexity is 117.92479294675124
At time: 324.24214148521423 and batch: 150, loss is 4.7254587745666505 and perplexity is 112.78222812484871
At time: 325.23191928863525 and batch: 200, loss is 4.745981435775757 and perplexity is 115.12073368473571
At time: 326.21766996383667 and batch: 250, loss is 4.760493392944336 and perplexity is 116.80354172756128
At time: 327.205641746521 and batch: 300, loss is 4.759425916671753 and perplexity is 116.67892324368441
At time: 328.2029826641083 and batch: 350, loss is 4.822941932678223 and perplexity is 124.33032471474814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.132337372878502 and perplexity of 169.41263612915958
Finished 42 epochs...
Completing Train Step...
At time: 330.1186091899872 and batch: 50, loss is 4.798791217803955 and perplexity is 121.36362662638652
At time: 331.1337163448334 and batch: 100, loss is 4.756554450988769 and perplexity is 116.34436428682311
At time: 332.1281578540802 and batch: 150, loss is 4.712361059188843 and perplexity is 111.31467040775263
At time: 333.1171610355377 and batch: 200, loss is 4.732539176940918 and perplexity is 113.58360536942091
At time: 334.103502035141 and batch: 250, loss is 4.7482000255584715 and perplexity is 115.37642289812725
At time: 335.0925495624542 and batch: 300, loss is 4.74806040763855 and perplexity is 115.36031540642766
At time: 336.0878849029541 and batch: 350, loss is 4.80981032371521 and perplexity is 122.70834044765206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.122777478448276 and perplexity of 167.8007860467066
Finished 43 epochs...
Completing Train Step...
At time: 337.958398103714 and batch: 50, loss is 4.7861829853057865 and perplexity is 119.84305182879741
At time: 338.95717096328735 and batch: 100, loss is 4.743510570526123 and perplexity is 114.83663699122378
At time: 339.94612312316895 and batch: 150, loss is 4.699722013473511 and perplexity is 109.91661286732942
At time: 340.9336760044098 and batch: 200, loss is 4.721870613098145 and perplexity is 112.37827244207428
At time: 341.92806029319763 and batch: 250, loss is 4.737822341918945 and perplexity is 114.18527425390336
At time: 342.9242134094238 and batch: 300, loss is 4.740957889556885 and perplexity is 114.54386952335392
At time: 343.92127776145935 and batch: 350, loss is 4.803613758087158 and perplexity is 121.95032114662948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.1148655332368 and perplexity of 166.47839366743816
Finished 44 epochs...
Completing Train Step...
At time: 345.8073801994324 and batch: 50, loss is 4.776017503738403 and perplexity is 118.6309606773872
At time: 346.7895588874817 and batch: 100, loss is 4.734747486114502 and perplexity is 113.83471024368019
At time: 347.78658413887024 and batch: 150, loss is 4.691950626373291 and perplexity is 109.0657189155682
At time: 348.76782751083374 and batch: 200, loss is 4.713279352188111 and perplexity is 111.4169368383911
At time: 349.75366735458374 and batch: 250, loss is 4.728266735076904 and perplexity is 113.09936120789514
At time: 350.7444248199463 and batch: 300, loss is 4.730436630249024 and perplexity is 113.34504141950488
At time: 351.7437369823456 and batch: 350, loss is 4.791640968322754 and perplexity is 120.49894145939074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.1053882467335665 and perplexity of 164.90808312910983
Finished 45 epochs...
Completing Train Step...
At time: 353.6666479110718 and batch: 50, loss is 4.763389625549316 and perplexity is 117.14232231036256
At time: 354.6556532382965 and batch: 100, loss is 4.721635160446167 and perplexity is 112.3518157945692
At time: 355.6529321670532 and batch: 150, loss is 4.676791362762451 and perplexity is 107.42483167604892
At time: 356.650372505188 and batch: 200, loss is 4.701445322036744 and perplexity is 110.10619641608311
At time: 357.6493260860443 and batch: 250, loss is 4.718131351470947 and perplexity is 111.95884534273492
At time: 358.64834213256836 and batch: 300, loss is 4.720280656814575 and perplexity is 112.19973787036105
At time: 359.64609599113464 and batch: 350, loss is 4.781757068634033 and perplexity is 119.31380852502913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.099617793642241 and perplexity of 163.95922906289334
Finished 46 epochs...
Completing Train Step...
At time: 361.52714490890503 and batch: 50, loss is 4.753734312057495 and perplexity is 116.01671923509822
At time: 362.53098249435425 and batch: 100, loss is 4.714218254089356 and perplexity is 111.52159553665656
At time: 363.5213544368744 and batch: 150, loss is 4.667812967300415 and perplexity is 106.46464596848416
At time: 364.51043367385864 and batch: 200, loss is 4.6942720794677735 and perplexity is 109.31920397925046
At time: 365.5020091533661 and batch: 250, loss is 4.710106220245361 and perplexity is 111.06395651988079
At time: 366.50898599624634 and batch: 300, loss is 4.71207124710083 and perplexity is 111.28241474496161
At time: 367.5069930553436 and batch: 350, loss is 4.774003305435181 and perplexity is 118.39225487879072
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.095109610721983 and perplexity of 163.2217344960702
Finished 47 epochs...
Completing Train Step...
At time: 369.4338858127594 and batch: 50, loss is 4.747062873840332 and perplexity is 115.2452969697633
At time: 370.42006969451904 and batch: 100, loss is 4.705923986434937 and perplexity is 110.60043104738524
At time: 371.4040050506592 and batch: 150, loss is 4.661529960632325 and perplexity is 105.79782490217846
At time: 372.38492703437805 and batch: 200, loss is 4.685836725234985 and perplexity is 108.4009361713137
At time: 373.38525223731995 and batch: 250, loss is 4.703207483291626 and perplexity is 110.30039234134607
At time: 374.3709237575531 and batch: 300, loss is 4.705859861373901 and perplexity is 110.59333901538471
At time: 375.3566827774048 and batch: 350, loss is 4.768906288146972 and perplexity is 117.79034279016273
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.090547232792296 and perplexity of 162.47875143051394
Finished 48 epochs...
Completing Train Step...
At time: 377.24113941192627 and batch: 50, loss is 4.740148220062256 and perplexity is 114.45116438173122
At time: 378.2204473018646 and batch: 100, loss is 4.697397575378418 and perplexity is 109.66141521557184
At time: 379.2051913738251 and batch: 150, loss is 4.653927869796753 and perplexity is 104.99658961750465
At time: 380.1914565563202 and batch: 200, loss is 4.679868059158325 and perplexity is 107.75585423532831
At time: 381.1797790527344 and batch: 250, loss is 4.698533143997192 and perplexity is 109.78601400923164
At time: 382.16772532463074 and batch: 300, loss is 4.700543775558471 and perplexity is 110.00697529541465
At time: 383.15394926071167 and batch: 350, loss is 4.761257619857788 and perplexity is 116.89284025555901
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.0859527587890625 and perplexity of 161.73395930518032
Finished 49 epochs...
Completing Train Step...
At time: 385.0186080932617 and batch: 50, loss is 4.730595054626465 and perplexity is 113.36299945958595
At time: 386.01633381843567 and batch: 100, loss is 4.691090297698975 and perplexity is 108.9719269019647
At time: 387.0049705505371 and batch: 150, loss is 4.646697788238526 and perplexity is 104.24019340936204
At time: 387.9950473308563 and batch: 200, loss is 4.674145736694336 and perplexity is 107.14100136099611
At time: 388.9866626262665 and batch: 250, loss is 4.692455701828003 and perplexity is 109.12081924688401
At time: 389.98383474349976 and batch: 300, loss is 4.694798927307129 and perplexity is 109.3768137401235
At time: 390.9822995662689 and batch: 350, loss is 4.755515289306641 and perplexity is 116.22352647740924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.085444483263739 and perplexity of 161.65177478001254
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd1f2403cf8>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'anneal': 8.0, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'lr': 0.0, 'dropout': 0.0, 'seq_len': 35, 'batch_size': 80}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.1237547397613525 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 2.0478758811950684 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 2.957562208175659 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 3.8669028282165527 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 4.777123212814331 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 5.701053619384766 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 6.610041618347168 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 1 epochs...
Completing Train Step...
At time: 8.366796016693115 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 9.292815685272217 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 10.205620050430298 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 11.119845628738403 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 12.060220956802368 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 13.035535097122192 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 14.035800695419312 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 2 epochs...
Completing Train Step...
At time: 15.93390440940857 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 16.9453387260437 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 17.948195219039917 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 18.947965621948242 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 19.95529079437256 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 20.95374345779419 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 21.949185371398926 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 3 epochs...
Completing Train Step...
At time: 23.85689663887024 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 24.844855070114136 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 25.832441806793213 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 26.81779932975769 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 27.812381267547607 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 28.8197979927063 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 29.824333429336548 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 4 epochs...
Completing Train Step...
At time: 31.692896604537964 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 32.7000207901001 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 33.68753695487976 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 34.67101740837097 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 35.65529465675354 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 36.646881341934204 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 37.64469623565674 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 5 epochs...
Completing Train Step...
At time: 39.54648208618164 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 40.53744602203369 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 41.56082367897034 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 42.550485610961914 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 43.53998827934265 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 44.52715063095093 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 45.53470420837402 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 6 epochs...
Completing Train Step...
At time: 47.5372953414917 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 48.534470558166504 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 49.548314571380615 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 50.57870364189148 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 51.590097188949585 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 52.58441138267517 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 53.597198486328125 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 7 epochs...
Completing Train Step...
At time: 55.518654346466064 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 56.53995370864868 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 57.524810552597046 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 58.511313676834106 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 59.500571966171265 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 60.49833941459656 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 61.4960560798645 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 8 epochs...
Completing Train Step...
At time: 63.40967917442322 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 64.42247700691223 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 65.40892386436462 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 66.39640617370605 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 67.40140962600708 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 68.39121627807617 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 69.38357710838318 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 9 epochs...
Completing Train Step...
At time: 71.29526400566101 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 72.28732180595398 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 73.2770574092865 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 74.26335644721985 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 75.25270557403564 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 76.23872876167297 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 77.23075079917908 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 10 epochs...
Completing Train Step...
At time: 79.15337991714478 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 80.1388828754425 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 81.14386010169983 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 82.17061376571655 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 83.17172574996948 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 84.18867325782776 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 85.18840670585632 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 11 epochs...
Completing Train Step...
At time: 87.10020637512207 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 88.10402321815491 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 89.09557819366455 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 90.0820722579956 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 91.06937575340271 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 92.05950546264648 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 93.05190348625183 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 12 epochs...
Completing Train Step...
At time: 95.00795197486877 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 95.98896384239197 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 96.97858619689941 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 97.9632499217987 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 98.94833612442017 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 99.93240976333618 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 100.92359232902527 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 13 epochs...
Completing Train Step...
At time: 102.82250833511353 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 103.80833172798157 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 104.80195689201355 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 105.79756736755371 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 106.80656790733337 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 107.8170177936554 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 108.82839679718018 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 14 epochs...
Completing Train Step...
At time: 110.69708013534546 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 111.70451593399048 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 112.68939542770386 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 113.67539286613464 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 114.6609559059143 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 115.64467597007751 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 116.63006496429443 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 15 epochs...
Completing Train Step...
At time: 118.51299452781677 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 119.51427698135376 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 120.51132154464722 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 121.51224184036255 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 122.49975609779358 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 123.489737033844 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 124.48971176147461 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 16 epochs...
Completing Train Step...
At time: 126.48621106147766 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 127.46610307693481 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 128.4463813304901 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 129.42669868469238 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 130.40702605247498 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 131.39301443099976 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 132.4038245677948 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 17 epochs...
Completing Train Step...
At time: 134.3036506175995 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 135.29209303855896 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 136.28054404258728 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 137.26982069015503 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 138.26663064956665 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 139.26493096351624 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 140.26366114616394 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 18 epochs...
Completing Train Step...
At time: 142.13159036636353 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 143.12500023841858 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 144.11185455322266 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 145.10500121116638 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 146.09871554374695 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 147.0910530090332 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 148.0858314037323 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 19 epochs...
Completing Train Step...
At time: 149.96960163116455 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 150.9480335712433 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 151.92902946472168 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 152.913432598114 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 153.90464401245117 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 154.8986999988556 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 155.892395734787 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 20 epochs...
Completing Train Step...
At time: 157.76718950271606 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 158.74673342704773 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 159.7260718345642 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 160.70546793937683 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 161.68943524360657 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 162.6739432811737 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 163.66376495361328 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 21 epochs...
Completing Train Step...
At time: 165.5282962322235 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 166.5212345123291 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 167.50092005729675 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 168.48391103744507 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 169.4700517654419 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 170.46220445632935 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 171.46431231498718 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 22 epochs...
Completing Train Step...
At time: 173.33765840530396 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 174.3380982875824 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 175.32412958145142 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 176.3112871646881 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 177.31518721580505 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 178.32201170921326 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 179.32029938697815 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 23 epochs...
Completing Train Step...
At time: 181.28860664367676 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 182.26719689369202 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 183.26042318344116 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 184.2433626651764 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 185.23765206336975 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 186.22158551216125 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 187.20683026313782 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 24 epochs...
Completing Train Step...
At time: 189.07472801208496 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 190.05324172973633 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 191.03500413894653 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 192.02088022232056 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 193.0156490802765 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 194.01469731330872 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 195.01330542564392 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 25 epochs...
Completing Train Step...
At time: 196.87793588638306 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 197.87166357040405 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 198.8532154560089 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 199.8384828567505 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 200.82239437103271 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 201.80754566192627 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 202.7964780330658 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 26 epochs...
Completing Train Step...
At time: 204.67799639701843 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 205.66190028190613 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 206.6470673084259 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 207.63285398483276 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 208.6414020061493 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 209.6357138156891 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 210.63072681427002 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 27 epochs...
Completing Train Step...
At time: 212.5262885093689 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 213.51270055770874 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 214.49808645248413 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 215.48397135734558 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 216.46573400497437 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 217.44818592071533 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 218.433452129364 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 28 epochs...
Completing Train Step...
At time: 220.28795838356018 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 221.28081154823303 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 222.26891016960144 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 223.26294541358948 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 224.25776648521423 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 225.25225043296814 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 226.2499876022339 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 29 epochs...
Completing Train Step...
At time: 228.12329506874084 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 229.12301516532898 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 230.1041476726532 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 231.0870087146759 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 232.06903338432312 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 233.07765913009644 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 234.06526708602905 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 30 epochs...
Completing Train Step...
At time: 235.92790699005127 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 236.90323066711426 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 237.8806390762329 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 238.8605649471283 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 239.8449511528015 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 240.82956171035767 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 241.82679653167725 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 31 epochs...
Completing Train Step...
At time: 243.75165605545044 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 244.72683024406433 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 245.70544028282166 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 246.6972861289978 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 247.6950650215149 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 248.69222855567932 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 249.68932795524597 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 32 epochs...
Completing Train Step...
At time: 251.55242919921875 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 252.54531288146973 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 253.526207447052 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 254.51169347763062 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 255.49566531181335 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 256.48081827163696 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 257.46525859832764 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 33 epochs...
Completing Train Step...
At time: 259.33528447151184 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 260.3199541568756 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 261.3049614429474 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 262.2883298397064 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 263.2706632614136 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 264.2586438655853 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 265.2526104450226 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 34 epochs...
Completing Train Step...
At time: 267.1295883655548 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 268.10858178138733 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 269.08777809143066 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 270.0687291622162 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 271.05338644981384 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 272.037371635437 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 273.0226619243622 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 35 epochs...
Completing Train Step...
At time: 274.87601947784424 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 275.86145758628845 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 276.8392858505249 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 277.81969571113586 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 278.80385422706604 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 279.7975149154663 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 280.79168367385864 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 36 epochs...
Completing Train Step...
At time: 282.6592981815338 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 283.656259059906 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 284.6388142108917 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 285.6224055290222 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 286.60571575164795 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 287.6033887863159 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 288.5927050113678 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 37 epochs...
Completing Train Step...
At time: 290.48203110694885 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 291.4612102508545 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 292.4417316913605 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 293.4213721752167 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 294.4002294540405 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 295.3833613395691 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 296.3685233592987 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 38 epochs...
Completing Train Step...
At time: 298.25265431404114 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 299.2360053062439 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 300.22424125671387 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 301.21596908569336 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 302.2182261943817 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 303.2166802883148 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 304.2154006958008 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 39 epochs...
Completing Train Step...
At time: 306.0847508907318 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 307.08223509788513 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 308.06680274009705 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 309.06304454803467 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 310.04670000076294 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 311.02885699272156 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 312.01010847091675 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 40 epochs...
Completing Train Step...
At time: 313.88159823417664 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 314.8619773387909 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 315.8419985771179 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 316.820246219635 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 317.8037157058716 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 318.7942941188812 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 319.7892715930939 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 41 epochs...
Completing Train Step...
At time: 321.67112588882446 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 322.6562695503235 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 323.6408677101135 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 324.6252348423004 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 325.61570930480957 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 326.60678720474243 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 327.5918869972229 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 42 epochs...
Completing Train Step...
At time: 329.44696593284607 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 330.43956184387207 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 331.4275608062744 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 332.40748620033264 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 333.3866488933563 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 334.3792886734009 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 335.35743522644043 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 43 epochs...
Completing Train Step...
At time: 337.3427450656891 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 338.309287071228 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 339.2816915512085 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 340.261860370636 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 341.24437379837036 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 342.22708773612976 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 343.21382427215576 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 44 epochs...
Completing Train Step...
At time: 345.08796668052673 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 346.08269453048706 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 347.0666627883911 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 348.0505177974701 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 349.04326272010803 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 350.0375509262085 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 351.03166937828064 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 45 epochs...
Completing Train Step...
At time: 352.9225490093231 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 353.9056386947632 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 354.8898913860321 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 355.87878012657166 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 356.86608123779297 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 357.8603301048279 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 358.87489438056946 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 46 epochs...
Completing Train Step...
At time: 360.797714471817 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 361.7763042449951 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 362.7545020580292 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 363.73420548439026 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 364.71263241767883 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 365.69319772720337 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 366.67474365234375 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 47 epochs...
Completing Train Step...
At time: 368.55932211875916 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 369.5450928211212 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 370.5284638404846 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 371.5131230354309 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 372.49838423728943 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 373.4819459915161 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 374.4672939777374 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 48 epochs...
Completing Train Step...
At time: 376.3402395248413 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 377.3192882537842 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 378.3125891685486 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 379.29188895225525 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 380.27114939689636 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 381.2545702457428 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 382.23928570747375 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished 49 epochs...
Completing Train Step...
At time: 384.1130108833313 and batch: 50, loss is 9.175985164642334 and perplexity is 9662.282323204021
At time: 385.09185552597046 and batch: 100, loss is 9.17599920272827 and perplexity is 9662.417964105682
At time: 386.071825504303 and batch: 150, loss is 9.176263198852538 and perplexity is 9664.969141734948
At time: 387.0561718940735 and batch: 200, loss is 9.176428108215331 and perplexity is 9666.563117064641
At time: 388.0445032119751 and batch: 250, loss is 9.176046581268311 and perplexity is 9662.875766206995
At time: 389.0316152572632 and batch: 300, loss is 9.175838584899902 and perplexity is 9660.866132144769
At time: 390.0193626880646 and batch: 350, loss is 9.175958576202392 and perplexity is 9662.025421606088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 9.492145406788794 and perplexity of 13255.202644859444
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd1f2403cf8>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -125.41750374833323, 'params': {'data': 'ptb', 'anneal': 4.140257380267493, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'lr': 11.870599401543625, 'dropout': 0.8000183658531081, 'seq_len': 35, 'batch_size': 80}}, {'best_accuracy': -102.66273582175432, 'params': {'data': 'ptb', 'anneal': 7.304921433418988, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'lr': 11.529068373196251, 'dropout': 0.37373393642136477, 'seq_len': 35, 'batch_size': 80}}, {'best_accuracy': -166.64306630124614, 'params': {'data': 'ptb', 'anneal': 2.0385820949287394, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'lr': 17.40382707577365, 'dropout': 0.5185250741563281, 'seq_len': 35, 'batch_size': 80}}, {'best_accuracy': -82.1974199413857, 'params': {'data': 'ptb', 'anneal': 3.4439614136649936, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'lr': 4.33658299020075, 'dropout': 0.20642903087480036, 'seq_len': 35, 'batch_size': 80}}, {'best_accuracy': -161.65177478001254, 'params': {'data': 'ptb', 'anneal': 4.142212715602351, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'lr': 21.402157677253108, 'dropout': 0.4517921910286671, 'seq_len': 35, 'batch_size': 80}}, {'best_accuracy': -13255.202644859444, 'params': {'data': 'ptb', 'anneal': 8.0, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'lr': 0.0, 'dropout': 0.0, 'seq_len': 35, 'batch_size': 80}}]
