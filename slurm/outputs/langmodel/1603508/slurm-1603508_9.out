Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'type': 'continuous', 'domain': [0, 30], 'name': 'lr'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'dropout'}, {'type': 'continuous', 'domain': [2, 8], 'name': 'anneal'}]
SETTINGS FOR THIS RUN
{'data': 'ptb', 'batch_size': 20, 'lr': 2.9374378812766047, 'dropout': 0.21487660011599763, 'wordvec_source': '', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'anneal': 5.714280119200078, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.0072150230407715 and batch: 50, loss is 6.748823785781861 and perplexity is 853.0547970215396
At time: 1.5444445610046387 and batch: 100, loss is 5.922588243484497 and perplexity is 373.3768545388984
At time: 2.058824300765991 and batch: 150, loss is 5.662524671554565 and perplexity is 287.8745144743829
At time: 2.5818605422973633 and batch: 200, loss is 5.554224357604981 and perplexity is 258.32651779417495
At time: 3.0980844497680664 and batch: 250, loss is 5.5496639633178715 and perplexity is 257.1511291730589
At time: 3.606550931930542 and batch: 300, loss is 5.409669427871704 and perplexity is 223.55767352830475
At time: 4.113430976867676 and batch: 350, loss is 5.360345363616943 and perplexity is 212.7984265944206
At time: 4.6210832595825195 and batch: 400, loss is 5.329027223587036 and perplexity is 206.23725380814741
At time: 5.127502679824829 and batch: 450, loss is 5.318840951919555 and perplexity is 204.14712847772836
At time: 5.63476824760437 and batch: 500, loss is 5.325663747787476 and perplexity is 205.54474506705256
At time: 6.14126181602478 and batch: 550, loss is 5.230707235336304 and perplexity is 186.92495671758198
At time: 6.646499156951904 and batch: 600, loss is 5.119258689880371 and perplexity is 167.2113681856702
At time: 7.158710241317749 and batch: 650, loss is 5.0843889522552494 and perplexity is 161.48123633929472
At time: 7.666290760040283 and batch: 700, loss is 5.137255868911743 and perplexity is 170.24794405334512
At time: 8.179468631744385 and batch: 750, loss is 5.06598801612854 and perplexity is 158.53700183394403
At time: 8.691570281982422 and batch: 800, loss is 5.136467304229736 and perplexity is 170.11374545657137
At time: 9.203612089157104 and batch: 850, loss is 5.039891862869263 and perplexity is 154.4533119848068
At time: 9.715990781784058 and batch: 900, loss is 5.080680017471313 and perplexity is 160.88342227687198
At time: 10.228415727615356 and batch: 950, loss is 5.035014162063598 and perplexity is 153.70176933066153
At time: 10.737505435943604 and batch: 1000, loss is 4.92822190284729 and perplexity is 138.133678745852
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.983347822980183 and perplexity of 145.9622202745729
Finished 1 epochs...
Completing Train Step...
At time: 12.219525814056396 and batch: 50, loss is 4.930878915786743 and perplexity is 138.50118974218
At time: 12.752534866333008 and batch: 100, loss is 4.821790914535523 and perplexity is 124.18730058278943
At time: 13.260061025619507 and batch: 150, loss is 4.846261367797852 and perplexity is 127.26370713692954
At time: 13.76700496673584 and batch: 200, loss is 4.830433168411255 and perplexity is 125.26520983633137
At time: 14.274051666259766 and batch: 250, loss is 4.87242525100708 and perplexity is 130.63736140498065
At time: 14.780797004699707 and batch: 300, loss is 4.755455627441406 and perplexity is 116.21659257188234
At time: 15.287726163864136 and batch: 350, loss is 4.742003765106201 and perplexity is 114.66373082488883
At time: 15.79427170753479 and batch: 400, loss is 4.740672454833985 and perplexity is 114.51117939136793
At time: 16.300599575042725 and batch: 450, loss is 4.766205291748047 and perplexity is 117.47262077462685
At time: 16.807613372802734 and batch: 500, loss is 4.787088289260864 and perplexity is 119.95159534262947
At time: 17.31357717514038 and batch: 550, loss is 4.682774343490601 and perplexity is 108.06947890668545
At time: 17.81971049308777 and batch: 600, loss is 4.609769477844238 and perplexity is 100.4609884832773
At time: 18.3382465839386 and batch: 650, loss is 4.5792693996429445 and perplexity is 97.44317617495601
At time: 18.84314751625061 and batch: 700, loss is 4.642036600112915 and perplexity is 103.75544089663042
At time: 19.350303649902344 and batch: 750, loss is 4.600048961639405 and perplexity is 99.48918667636863
At time: 19.85604739189148 and batch: 800, loss is 4.696740446090698 and perplexity is 109.58937715965128
At time: 20.362942934036255 and batch: 850, loss is 4.574278440475464 and perplexity is 96.9580528838073
At time: 20.86769151687622 and batch: 900, loss is 4.588754329681397 and perplexity is 98.37181495866062
At time: 21.373432874679565 and batch: 950, loss is 4.58199257850647 and perplexity is 97.70889300573263
At time: 21.876275539398193 and batch: 1000, loss is 4.466724700927735 and perplexity is 87.07107167174851
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.715497644936166 and perplexity of 111.66436655570087
Finished 2 epochs...
Completing Train Step...
At time: 23.380035400390625 and batch: 50, loss is 4.572728185653687 and perplexity is 96.80785964379116
At time: 23.889806032180786 and batch: 100, loss is 4.466962232589721 and perplexity is 87.09175626463927
At time: 24.399309873580933 and batch: 150, loss is 4.545437135696411 and perplexity is 94.20159709108623
At time: 24.907888650894165 and batch: 200, loss is 4.554694910049438 and perplexity is 95.07774354613754
At time: 25.41716504096985 and batch: 250, loss is 4.589795570373536 and perplexity is 98.47429704035288
At time: 25.926547288894653 and batch: 300, loss is 4.463021621704102 and perplexity is 86.74923685294807
At time: 26.435871124267578 and batch: 350, loss is 4.4652981948852535 and perplexity is 86.94695281108348
At time: 26.944459199905396 and batch: 400, loss is 4.473016300201416 and perplexity is 87.62061490316609
At time: 27.454159021377563 and batch: 450, loss is 4.517751712799072 and perplexity is 91.62935710986059
At time: 27.96329140663147 and batch: 500, loss is 4.536484184265137 and perplexity is 93.36197890631172
At time: 28.471724033355713 and batch: 550, loss is 4.438095731735229 and perplexity is 84.61366105227225
At time: 28.982495307922363 and batch: 600, loss is 4.375731425285339 and perplexity is 79.49796511423469
At time: 29.490403413772583 and batch: 650, loss is 4.348517203330994 and perplexity is 77.3636632518363
At time: 29.999940156936646 and batch: 700, loss is 4.427380981445313 and perplexity is 83.71188657455552
At time: 30.506911277770996 and batch: 750, loss is 4.38341908454895 and perplexity is 80.11147358268211
At time: 31.03931450843811 and batch: 800, loss is 4.495959491729736 and perplexity is 89.65415014504381
At time: 31.548904180526733 and batch: 850, loss is 4.370108375549316 and perplexity is 79.0521985607806
At time: 32.05843162536621 and batch: 900, loss is 4.363544149398804 and perplexity is 78.53498147389475
At time: 32.56843447685242 and batch: 950, loss is 4.379790472984314 and perplexity is 79.82130693259202
At time: 33.07750701904297 and batch: 1000, loss is 4.2701680994033815 and perplexity is 71.53365937406038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.610696187833461 and perplexity of 100.55412983567703
Finished 3 epochs...
Completing Train Step...
At time: 34.56998085975647 and batch: 50, loss is 4.385678100585937 and perplexity is 80.29265125083292
At time: 35.07588481903076 and batch: 100, loss is 4.2779379034042355 and perplexity is 72.09162672832025
At time: 35.583178758621216 and batch: 150, loss is 4.379672412872314 and perplexity is 79.811883776416
At time: 36.08914852142334 and batch: 200, loss is 4.397202405929566 and perplexity is 81.22332063967174
At time: 36.59551167488098 and batch: 250, loss is 4.424664182662964 and perplexity is 83.48476688199116
At time: 37.101593017578125 and batch: 300, loss is 4.290973696708679 and perplexity is 73.03755031030143
At time: 37.605863094329834 and batch: 350, loss is 4.301746077537537 and perplexity is 73.82859166679134
At time: 38.110864877700806 and batch: 400, loss is 4.312774486541748 and perplexity is 74.64730985331967
At time: 38.617146492004395 and batch: 450, loss is 4.3633621978759765 and perplexity is 78.52069321434507
At time: 39.122464656829834 and batch: 500, loss is 4.384742937088013 and perplexity is 80.21759959248679
At time: 39.6295166015625 and batch: 550, loss is 4.286230998039246 and perplexity is 72.69197534434839
At time: 40.13538575172424 and batch: 600, loss is 4.228782229423523 and perplexity is 68.6336012823628
At time: 40.64107418060303 and batch: 650, loss is 4.201227660179138 and perplexity is 66.76824946774359
At time: 41.14729022979736 and batch: 700, loss is 4.285294508934021 and perplexity is 72.62393196731468
At time: 41.65231418609619 and batch: 750, loss is 4.2419694232940675 and perplexity is 69.54467997098762
At time: 42.15654921531677 and batch: 800, loss is 4.364759178161621 and perplexity is 78.63046172915463
At time: 42.65925621986389 and batch: 850, loss is 4.235961680412292 and perplexity is 69.12812594256489
At time: 43.16431450843811 and batch: 900, loss is 4.216739344596863 and perplexity is 67.81201181021734
At time: 43.692631483078 and batch: 950, loss is 4.2429017162323 and perplexity is 69.60954621748967
At time: 44.19782257080078 and batch: 1000, loss is 4.137574067115784 and perplexity is 62.65065067210158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.558441534274962 and perplexity of 95.4346322699086
Finished 4 epochs...
Completing Train Step...
At time: 45.66601014137268 and batch: 50, loss is 4.258858890533447 and perplexity is 70.72922759623037
At time: 46.18453550338745 and batch: 100, loss is 4.1489551687240604 and perplexity is 63.36775708317161
At time: 46.6905038356781 and batch: 150, loss is 4.260722126960754 and perplexity is 70.8611357194115
At time: 47.19771075248718 and batch: 200, loss is 4.282943134307861 and perplexity is 72.45336650667042
At time: 47.70453882217407 and batch: 250, loss is 4.305574364662171 and perplexity is 74.11177041262206
At time: 48.21061563491821 and batch: 300, loss is 4.165289125442505 and perplexity is 64.41130269793288
At time: 48.71559166908264 and batch: 350, loss is 4.184341468811035 and perplexity is 65.65025393877653
At time: 49.22188687324524 and batch: 400, loss is 4.196127395629883 and perplexity is 66.42858066864288
At time: 49.728596687316895 and batch: 450, loss is 4.246387400627136 and perplexity is 69.85260649608325
At time: 50.23497772216797 and batch: 500, loss is 4.268562812805175 and perplexity is 71.4189194692022
At time: 50.74108695983887 and batch: 550, loss is 4.173029417991638 and perplexity is 64.9117995213857
At time: 51.24739861488342 and batch: 600, loss is 4.118475675582886 and perplexity is 61.465477490468714
At time: 51.753050804138184 and batch: 650, loss is 4.094331550598144 and perplexity is 59.99921930764167
At time: 52.259824991226196 and batch: 700, loss is 4.177664165496826 and perplexity is 65.21334758204256
At time: 52.76627278327942 and batch: 750, loss is 4.135527067184448 and perplexity is 62.52253596461196
At time: 53.273415327072144 and batch: 800, loss is 4.264629802703857 and perplexity is 71.13857978827409
At time: 53.77879023551941 and batch: 850, loss is 4.133758821487427 and perplexity is 62.41207844622205
At time: 54.28206419944763 and batch: 900, loss is 4.104785528182983 and perplexity is 60.629739782621634
At time: 54.785545110702515 and batch: 950, loss is 4.137386088371277 and perplexity is 62.638874788288284
At time: 55.29173731803894 and batch: 1000, loss is 4.03787073135376 and perplexity is 56.70547298328064
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.528761700886052 and perplexity of 92.64376932604588
Finished 5 epochs...
Completing Train Step...
At time: 56.77468156814575 and batch: 50, loss is 4.1594220161437985 and perplexity is 64.03450099420748
At time: 57.285799741744995 and batch: 100, loss is 4.050037841796875 and perplexity is 57.39962910945402
At time: 57.79655909538269 and batch: 150, loss is 4.1656663036346435 and perplexity is 64.43560181889897
At time: 58.306514739990234 and batch: 200, loss is 4.192145686149598 and perplexity is 66.16460724073588
At time: 58.81716775894165 and batch: 250, loss is 4.211239290237427 and perplexity is 67.44006585814785
At time: 59.32869839668274 and batch: 300, loss is 4.064919805526733 and perplexity is 58.260236186253344
At time: 59.84018635749817 and batch: 350, loss is 4.090906953811645 and perplexity is 59.79409760404165
At time: 60.34859871864319 and batch: 400, loss is 4.101686415672302 and perplexity is 60.442132256114526
At time: 60.858741760253906 and batch: 450, loss is 4.153093571662903 and perplexity is 63.63054177473665
At time: 61.36889290809631 and batch: 500, loss is 4.17723780632019 and perplexity is 65.18554919933068
At time: 61.878990650177 and batch: 550, loss is 4.083810348510742 and perplexity is 59.371264608013895
At time: 62.38910222053528 and batch: 600, loss is 4.028411478996277 and perplexity is 56.17161055471437
At time: 62.89914321899414 and batch: 650, loss is 4.004800152778626 and perplexity is 54.86085951311968
At time: 63.40869164466858 and batch: 700, loss is 4.090549478530884 and perplexity is 59.77272651225886
At time: 63.91883111000061 and batch: 750, loss is 4.050304536819458 and perplexity is 57.41493934632683
At time: 64.42897748947144 and batch: 800, loss is 4.18085636138916 and perplexity is 65.4218539818972
At time: 64.93924760818481 and batch: 850, loss is 4.051559071540833 and perplexity is 57.48701358162491
At time: 65.44954824447632 and batch: 900, loss is 4.013136143684387 and perplexity is 55.32009055270886
At time: 65.95986866950989 and batch: 950, loss is 4.051671562194824 and perplexity is 57.493480697116894
At time: 66.46808671951294 and batch: 1000, loss is 3.9567726612091065 and perplexity is 52.28830128283738
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.513943928044017 and perplexity of 91.28111567572331
Finished 6 epochs...
Completing Train Step...
At time: 67.95936918258667 and batch: 50, loss is 4.076281809806824 and perplexity is 58.9259640789059
At time: 68.47836875915527 and batch: 100, loss is 3.969599289894104 and perplexity is 52.963303655970094
At time: 68.98669862747192 and batch: 150, loss is 4.087513203620911 and perplexity is 59.591515325154376
At time: 69.50659704208374 and batch: 200, loss is 4.115871963500976 and perplexity is 61.30564725036502
At time: 70.01371049880981 and batch: 250, loss is 4.1331557941436765 and perplexity is 62.37445360188382
At time: 70.52051091194153 and batch: 300, loss is 3.9823140621185305 and perplexity is 53.641019368424885
At time: 71.02643346786499 and batch: 350, loss is 4.011729383468628 and perplexity is 55.242323163047026
At time: 71.53239727020264 and batch: 400, loss is 4.022208962440491 and perplexity is 55.82428347477249
At time: 72.03900909423828 and batch: 450, loss is 4.074172053337097 and perplexity is 58.80177569464341
At time: 72.54529857635498 and batch: 500, loss is 4.100823192596436 and perplexity is 60.389979725668844
At time: 73.05156636238098 and batch: 550, loss is 4.008419785499573 and perplexity is 55.05979549570803
At time: 73.55873847007751 and batch: 600, loss is 3.954295334815979 and perplexity is 52.158926412074614
At time: 74.06555914878845 and batch: 650, loss is 3.931451578140259 and perplexity is 50.98092678364698
At time: 74.57216143608093 and batch: 700, loss is 4.017707500457764 and perplexity is 55.57355732548757
At time: 75.07880020141602 and batch: 750, loss is 3.978266649246216 and perplexity is 53.424350785535296
At time: 75.58452367782593 and batch: 800, loss is 4.108463835716248 and perplexity is 60.85316527312516
At time: 76.09042596817017 and batch: 850, loss is 3.9809894609451293 and perplexity is 53.57001344887127
At time: 76.59676170349121 and batch: 900, loss is 3.9371348667144774 and perplexity is 51.2714910003901
At time: 77.10347771644592 and batch: 950, loss is 3.9787097120285035 and perplexity is 53.44802637153444
At time: 77.61046051979065 and batch: 1000, loss is 3.8859360361099244 and perplexity is 48.71251780174753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5032958984375 and perplexity of 90.31430808615724
Finished 7 epochs...
Completing Train Step...
At time: 79.08222150802612 and batch: 50, loss is 4.005266380310059 and perplexity is 54.88644311965022
At time: 79.59927892684937 and batch: 100, loss is 3.9004558372497558 and perplexity is 49.42497371544973
At time: 80.10635757446289 and batch: 150, loss is 4.019562549591065 and perplexity is 55.67674468405795
At time: 80.612713098526 and batch: 200, loss is 4.048658113479615 and perplexity is 57.32048782506179
At time: 81.11915493011475 and batch: 250, loss is 4.06564564704895 and perplexity is 58.30253923556911
At time: 81.62639474868774 and batch: 300, loss is 3.9121517515182496 and perplexity is 50.006437718911855
At time: 82.13207292556763 and batch: 350, loss is 3.9434006071090697 and perplexity is 51.593753404602964
At time: 82.65237236022949 and batch: 400, loss is 3.9555848121643065 and perplexity is 52.226227548516114
At time: 83.15952372550964 and batch: 450, loss is 4.006198987960816 and perplexity is 54.93765451278317
At time: 83.66634821891785 and batch: 500, loss is 4.035902447700501 and perplexity is 56.59397029821158
At time: 84.17321753501892 and batch: 550, loss is 3.942654733657837 and perplexity is 51.55528534162586
At time: 84.68073892593384 and batch: 600, loss is 3.8892025423049925 and perplexity is 48.87189770893529
At time: 85.186767578125 and batch: 650, loss is 3.867650270462036 and perplexity is 47.82986668696679
At time: 85.69428563117981 and batch: 700, loss is 3.954164299964905 and perplexity is 52.152092222688296
At time: 86.20111560821533 and batch: 750, loss is 3.9159548330307006 and perplexity is 50.196978368859895
At time: 86.70852303504944 and batch: 800, loss is 4.046261734962464 and perplexity is 57.18329069323089
At time: 87.21558856964111 and batch: 850, loss is 3.9180180120468138 and perplexity is 50.30065063174628
At time: 87.72238278388977 and batch: 900, loss is 3.8710567140579224 and perplexity is 47.99307425089542
At time: 88.23141694068909 and batch: 950, loss is 3.9162126111984255 and perplexity is 50.209919721896625
At time: 88.73621559143066 and batch: 1000, loss is 3.8243740081787108 and perplexity is 45.80411838643639
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5028228759765625 and perplexity of 90.27159749221997
Finished 8 epochs...
Completing Train Step...
At time: 90.22631478309631 and batch: 50, loss is 3.943852353096008 and perplexity is 51.617065940930495
At time: 90.72959733009338 and batch: 100, loss is 3.8400216913223266 and perplexity is 46.52648364979719
At time: 91.23362183570862 and batch: 150, loss is 3.9597282123565676 and perplexity is 52.44307063339566
At time: 91.74005365371704 and batch: 200, loss is 3.9909994220733642 and perplexity is 54.10894001813929
At time: 92.24663877487183 and batch: 250, loss is 4.005390400886536 and perplexity is 54.893250590091164
At time: 92.75201654434204 and batch: 300, loss is 3.8482730865478514 and perplexity is 46.91198031027143
At time: 93.25747990608215 and batch: 350, loss is 3.8827448320388793 and perplexity is 48.5573139918562
At time: 93.77624177932739 and batch: 400, loss is 3.896605920791626 and perplexity is 49.23505751103635
At time: 94.28704977035522 and batch: 450, loss is 3.9459382629394533 and perplexity is 51.72484685842507
At time: 94.79178261756897 and batch: 500, loss is 3.975851583480835 and perplexity is 53.29548313947107
At time: 95.30959939956665 and batch: 550, loss is 3.883574538230896 and perplexity is 48.59761901429408
At time: 95.81444478034973 and batch: 600, loss is 3.832471795082092 and perplexity is 46.17653622148084
At time: 96.32050490379333 and batch: 650, loss is 3.810841054916382 and perplexity is 45.18842883849105
At time: 96.82559251785278 and batch: 700, loss is 3.897606954574585 and perplexity is 49.28436814359762
At time: 97.33122563362122 and batch: 750, loss is 3.8600002336502075 and perplexity is 47.46536245914402
At time: 97.83636832237244 and batch: 800, loss is 3.9898137712478636 and perplexity is 54.044823726018215
At time: 98.34255957603455 and batch: 850, loss is 3.8630615663528443 and perplexity is 47.610892369563786
At time: 98.84800124168396 and batch: 900, loss is 3.8129056310653686 and perplexity is 45.28182016446006
At time: 99.35343718528748 and batch: 950, loss is 3.859240517616272 and perplexity is 47.42931595651399
At time: 99.85827612876892 and batch: 1000, loss is 3.768792304992676 and perplexity is 43.32770657090338
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.50448906130907 and perplexity of 90.42213207833741
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 101.32630729675293 and batch: 50, loss is 3.9107608366012574 and perplexity is 49.93693136865851
At time: 101.84744834899902 and batch: 100, loss is 3.8041321849822998 and perplexity is 44.88628021806715
At time: 102.35241627693176 and batch: 150, loss is 3.921432762145996 and perplexity is 50.47270838340205
At time: 102.85908246040344 and batch: 200, loss is 3.948691806793213 and perplexity is 51.86746976167412
At time: 103.36682534217834 and batch: 250, loss is 3.9499478054046633 and perplexity is 51.93265616011772
At time: 103.87573409080505 and batch: 300, loss is 3.7849192094802855 and perplexity is 44.032113037748424
At time: 104.38419198989868 and batch: 350, loss is 3.8138408613204957 and perplexity is 45.32418890185946
At time: 104.89439177513123 and batch: 400, loss is 3.82036367893219 and perplexity is 45.620796626860106
At time: 105.40477252006531 and batch: 450, loss is 3.8632077980041504 and perplexity is 47.61785509804838
At time: 105.91463971138 and batch: 500, loss is 3.887918372154236 and perplexity is 48.809178156594335
At time: 106.42462515830994 and batch: 550, loss is 3.7810271120071413 and perplexity is 43.86106883812634
At time: 106.93403649330139 and batch: 600, loss is 3.7181964778900145 and perplexity is 41.19003993133637
At time: 107.44314980506897 and batch: 650, loss is 3.688630766868591 and perplexity is 39.99005374699058
At time: 107.9522488117218 and batch: 700, loss is 3.763969168663025 and perplexity is 43.11923428452443
At time: 108.48509120941162 and batch: 750, loss is 3.7180088329315186 and perplexity is 41.182311553121295
At time: 108.99477076530457 and batch: 800, loss is 3.837542734146118 and perplexity is 46.41128932915497
At time: 109.50454592704773 and batch: 850, loss is 3.6929820871353147 and perplexity is 40.164442413640984
At time: 110.01391267776489 and batch: 900, loss is 3.6326306295394897 and perplexity is 37.81215567143026
At time: 110.52369475364685 and batch: 950, loss is 3.659997501373291 and perplexity is 38.86124577146453
At time: 111.03323936462402 and batch: 1000, loss is 3.559758338928223 and perplexity is 35.15470059588136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.460212893602325 and perplexity of 86.50592369380246
Finished 10 epochs...
Completing Train Step...
At time: 112.51555323600769 and batch: 50, loss is 3.846669387817383 and perplexity is 46.83680792004848
At time: 113.03663277626038 and batch: 100, loss is 3.738049144744873 and perplexity is 42.01594313661295
At time: 113.5416932106018 and batch: 150, loss is 3.860092406272888 and perplexity is 47.46973766772249
At time: 114.04716682434082 and batch: 200, loss is 3.892780690193176 and perplexity is 49.04708181692819
At time: 114.55122947692871 and batch: 250, loss is 3.8927042150497435 and perplexity is 49.04333107773228
At time: 115.05753898620605 and batch: 300, loss is 3.729955930709839 and perplexity is 41.677271435789905
At time: 115.56452131271362 and batch: 350, loss is 3.7645299148559572 and perplexity is 43.14342001138503
At time: 116.07348275184631 and batch: 400, loss is 3.773351411819458 and perplexity is 43.52569319186049
At time: 116.59203314781189 and batch: 450, loss is 3.8194262409210205 and perplexity is 45.5780499972904
At time: 117.10062670707703 and batch: 500, loss is 3.8469500064849855 and perplexity is 46.8499530469795
At time: 117.60743260383606 and batch: 550, loss is 3.7438797760009765 and perplexity is 42.26163819049121
At time: 118.11388230323792 and batch: 600, loss is 3.685280022621155 and perplexity is 39.856281547817055
At time: 118.61957430839539 and batch: 650, loss is 3.659190273284912 and perplexity is 38.82988854025006
At time: 119.12506628036499 and batch: 700, loss is 3.737756595611572 and perplexity is 42.003653206655336
At time: 119.64309048652649 and batch: 750, loss is 3.6970470714569093 and perplexity is 40.32804253301953
At time: 120.16088724136353 and batch: 800, loss is 3.8204749155044557 and perplexity is 45.62587161015755
At time: 120.66748523712158 and batch: 850, loss is 3.6803770017623902 and perplexity is 39.661343650841985
At time: 121.18718981742859 and batch: 900, loss is 3.6234755659103395 and perplexity is 37.46756277210265
At time: 121.69254899024963 and batch: 950, loss is 3.657961483001709 and perplexity is 38.78220405363085
At time: 122.19900226593018 and batch: 1000, loss is 3.563464798927307 and perplexity is 35.285241860962124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.459292155940358 and perplexity of 86.42631108866065
Finished 11 epochs...
Completing Train Step...
At time: 123.68733596801758 and batch: 50, loss is 3.820886287689209 and perplexity is 45.64464468573892
At time: 124.19435238838196 and batch: 100, loss is 3.7110586977005005 and perplexity is 40.89708126140598
At time: 124.7014570236206 and batch: 150, loss is 3.833581395149231 and perplexity is 46.227802146249275
At time: 125.20535898208618 and batch: 200, loss is 3.8673207902908326 and perplexity is 47.81411029015482
At time: 125.71239590644836 and batch: 250, loss is 3.8661279821395875 and perplexity is 47.75711123088559
At time: 126.21834182739258 and batch: 300, loss is 3.703515043258667 and perplexity is 40.58972855154274
At time: 126.72314119338989 and batch: 350, loss is 3.7398699712753296 and perplexity is 42.09251657289437
At time: 127.23044466972351 and batch: 400, loss is 3.7499108791351317 and perplexity is 42.517292653317234
At time: 127.73729515075684 and batch: 450, loss is 3.7962176036834716 and perplexity is 44.53242625419514
At time: 128.24471426010132 and batch: 500, loss is 3.825326623916626 and perplexity is 45.84777290015976
At time: 128.75134944915771 and batch: 550, loss is 3.7239244318008424 and perplexity is 41.42665158504722
At time: 129.25731015205383 and batch: 600, loss is 3.667215895652771 and perplexity is 39.142776442895176
At time: 129.76482272148132 and batch: 650, loss is 3.642752151489258 and perplexity is 38.19681562322425
At time: 130.27127146720886 and batch: 700, loss is 3.7224999141693114 and perplexity is 41.3676806020238
At time: 130.7787365913391 and batch: 750, loss is 3.684257836341858 and perplexity is 39.81556181879479
At time: 131.2859914302826 and batch: 800, loss is 3.8094825315475465 and perplexity is 45.127080982596155
At time: 131.79283666610718 and batch: 850, loss is 3.671126308441162 and perplexity is 39.29614051946543
At time: 132.29878091812134 and batch: 900, loss is 3.615211248397827 and perplexity is 37.15919491696467
At time: 132.80885863304138 and batch: 950, loss is 3.6527694416046144 and perplexity is 38.581367072841246
At time: 133.3250834941864 and batch: 1000, loss is 3.5602721548080445 and perplexity is 35.17276828063207
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.460379995950839 and perplexity of 86.52038024464022
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 134.80313158035278 and batch: 50, loss is 3.8100337028503417 and perplexity is 45.15196059044577
At time: 135.3267207145691 and batch: 100, loss is 3.7033359527587892 and perplexity is 40.58245996765312
At time: 135.8377754688263 and batch: 150, loss is 3.827226023674011 and perplexity is 45.93493890422059
At time: 136.34722232818604 and batch: 200, loss is 3.8620494174957276 and perplexity is 47.5627274384175
At time: 136.85665082931519 and batch: 250, loss is 3.8579672145843507 and perplexity is 47.36896249698037
At time: 137.36988925933838 and batch: 300, loss is 3.691709032058716 and perplexity is 40.11334339915865
At time: 137.88061809539795 and batch: 350, loss is 3.7282332468032835 and perplexity is 41.60553647708807
At time: 138.38882684707642 and batch: 400, loss is 3.7371864986419676 and perplexity is 41.979713875768276
At time: 138.89810037612915 and batch: 450, loss is 3.7822174310684202 and perplexity is 43.91330858923015
At time: 139.40862226486206 and batch: 500, loss is 3.811961236000061 and perplexity is 45.23907642361442
At time: 139.91925811767578 and batch: 550, loss is 3.704783387184143 and perplexity is 40.641242949268594
At time: 140.4293475151062 and batch: 600, loss is 3.64325345993042 and perplexity is 38.215968809747615
At time: 140.94011402130127 and batch: 650, loss is 3.614140110015869 and perplexity is 37.119413586504976
At time: 141.45082116127014 and batch: 700, loss is 3.6923262643814088 and perplexity is 40.13811029395359
At time: 141.9617395401001 and batch: 750, loss is 3.6509134721755983 and perplexity is 38.50982764305725
At time: 142.47199368476868 and batch: 800, loss is 3.771521544456482 and perplexity is 43.446119773042554
At time: 142.982008934021 and batch: 850, loss is 3.6305460929870605 and perplexity is 37.733416946194346
At time: 143.49207186698914 and batch: 900, loss is 3.5693803930282595 and perplexity is 35.49459363912874
At time: 144.02205419540405 and batch: 950, loss is 3.603305792808533 and perplexity is 36.71942082207293
At time: 144.5489740371704 and batch: 1000, loss is 3.507097725868225 and perplexity is 33.35133247606322
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.453426081959794 and perplexity of 85.92081205087607
Finished 13 epochs...
Completing Train Step...
At time: 146.0597379207611 and batch: 50, loss is 3.7985978317260742 and perplexity is 44.6385498330262
At time: 146.58867835998535 and batch: 100, loss is 3.6893339014053343 and perplexity is 40.018182022728496
At time: 147.09425830841064 and batch: 150, loss is 3.813509702682495 and perplexity is 45.30918189018216
At time: 147.6140832901001 and batch: 200, loss is 3.8493150758743284 and perplexity is 46.960887569034384
At time: 148.11750173568726 and batch: 250, loss is 3.8450493955612184 and perplexity is 46.76099407941223
At time: 148.6281168460846 and batch: 300, loss is 3.67931902885437 and perplexity is 39.61940521254131
At time: 149.13629269599915 and batch: 350, loss is 3.716923265457153 and perplexity is 41.137629632188045
At time: 149.6433424949646 and batch: 400, loss is 3.7259065437316896 and perplexity is 41.508845277047776
At time: 150.1480393409729 and batch: 450, loss is 3.771656880378723 and perplexity is 43.45199999162326
At time: 150.65260648727417 and batch: 500, loss is 3.802083873748779 and perplexity is 44.79443324377274
At time: 151.16055059432983 and batch: 550, loss is 3.696349959373474 and perplexity is 40.29993916400564
At time: 151.66720128059387 and batch: 600, loss is 3.636107811927795 and perplexity is 37.94386428793051
At time: 152.1725254058838 and batch: 650, loss is 3.6086254262924196 and perplexity is 36.915275157361826
At time: 152.67829871177673 and batch: 700, loss is 3.6877212381362914 and perplexity is 39.95369817982419
At time: 153.1851360797882 and batch: 750, loss is 3.6483360147476196 and perplexity is 38.41069800783467
At time: 153.6905698776245 and batch: 800, loss is 3.7700121116638186 and perplexity is 43.38059024379511
At time: 154.19823932647705 and batch: 850, loss is 3.630387377738953 and perplexity is 37.72742855279902
At time: 154.70645260810852 and batch: 900, loss is 3.5707618379592896 and perplexity is 35.543661349957326
At time: 155.21279621124268 and batch: 950, loss is 3.6062484979629517 and perplexity is 36.827634393220386
At time: 155.71872425079346 and batch: 1000, loss is 3.5112840604782103 and perplexity is 33.49124496941843
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.452979855421113 and perplexity of 85.88248045723714
Finished 14 epochs...
Completing Train Step...
At time: 157.2172145843506 and batch: 50, loss is 3.7928101587295533 and perplexity is 44.38094269558215
At time: 157.7223732471466 and batch: 100, loss is 3.6824591684341432 and perplexity is 39.74401121270325
At time: 158.22773957252502 and batch: 150, loss is 3.806575675010681 and perplexity is 44.99609350538941
At time: 158.73259973526 and batch: 200, loss is 3.842459774017334 and perplexity is 46.640057459353926
At time: 159.23649430274963 and batch: 250, loss is 3.837946057319641 and perplexity is 46.43001185301457
At time: 159.74172925949097 and batch: 300, loss is 3.672311944961548 and perplexity is 39.34275908965408
At time: 160.29163646697998 and batch: 350, loss is 3.710538206100464 and perplexity is 40.87580021292734
At time: 160.80663800239563 and batch: 400, loss is 3.7195893239974978 and perplexity is 41.247451291434935
At time: 161.31258296966553 and batch: 450, loss is 3.7657828760147094 and perplexity is 43.197510920742765
At time: 161.81668615341187 and batch: 500, loss is 3.7966635179519654 and perplexity is 44.552288326534146
At time: 162.32345342636108 and batch: 550, loss is 3.691442623138428 and perplexity is 40.10265827002459
At time: 162.8280611038208 and batch: 600, loss is 3.631974720954895 and perplexity is 37.78736248584384
At time: 163.3346447944641 and batch: 650, loss is 3.6053278112411498 and perplexity is 36.793743283180696
At time: 163.83953189849854 and batch: 700, loss is 3.68491126537323 and perplexity is 39.84158696465451
At time: 164.3451087474823 and batch: 750, loss is 3.6465913009643556 and perplexity is 38.34374076119612
At time: 164.8512725830078 and batch: 800, loss is 3.7689710235595704 and perplexity is 43.335450728520314
At time: 165.35651564598083 and batch: 850, loss is 3.6300752782821655 and perplexity is 37.71565568009086
At time: 165.86174893379211 and batch: 900, loss is 3.5710786628723143 and perplexity is 35.55492425146279
At time: 166.36775922775269 and batch: 950, loss is 3.6073997449874877 and perplexity is 36.87005651222
At time: 166.87683963775635 and batch: 1000, loss is 3.5130295276641847 and perplexity is 33.549753886372585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.453139514457889 and perplexity of 85.89619346601665
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 168.35997009277344 and batch: 50, loss is 3.7897499990463257 and perplexity is 44.245337516616
At time: 168.8814458847046 and batch: 100, loss is 3.680475263595581 and perplexity is 39.66524103865502
At time: 169.39098238945007 and batch: 150, loss is 3.804336180686951 and perplexity is 44.8954377604473
At time: 169.90009331703186 and batch: 200, loss is 3.8407104206085205 and perplexity is 46.55853883907714
At time: 170.40745401382446 and batch: 250, loss is 3.8353487634658814 and perplexity is 46.30957594003142
At time: 170.9133038520813 and batch: 300, loss is 3.66816358089447 and perplexity is 39.17988905721007
At time: 171.42285633087158 and batch: 350, loss is 3.7070664978027343 and perplexity is 40.73413740640782
At time: 171.93132400512695 and batch: 400, loss is 3.7154470586776736 and perplexity is 41.076946785667936
At time: 172.44021606445312 and batch: 450, loss is 3.7615084409713746 and perplexity is 43.01326003104226
At time: 172.9490466117859 and batch: 500, loss is 3.793980188369751 and perplexity is 44.432900103922435
At time: 173.47103691101074 and batch: 550, loss is 3.688019275665283 and perplexity is 39.96560765595105
At time: 173.97901225090027 and batch: 600, loss is 3.6270296573638916 and perplexity is 37.60096283432599
At time: 174.48563408851624 and batch: 650, loss is 3.598276309967041 and perplexity is 36.53520476916006
At time: 174.9938952922821 and batch: 700, loss is 3.6781840229034426 and perplexity is 39.57446246182238
At time: 175.50202894210815 and batch: 750, loss is 3.6381732702255247 and perplexity is 38.02231674952521
At time: 176.00957417488098 and batch: 800, loss is 3.7596704816818236 and perplexity is 42.934276017129996
At time: 176.51724457740784 and batch: 850, loss is 3.6207131338119507 and perplexity is 37.36420400057448
At time: 177.02648615837097 and batch: 900, loss is 3.5599312353134156 and perplexity is 35.16077924200975
At time: 177.5342710018158 and batch: 950, loss is 3.5951309204101562 and perplexity is 36.42046785835656
At time: 178.0423755645752 and batch: 1000, loss is 3.5003002405166628 and perplexity is 33.12539605183304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.452425700862233 and perplexity of 85.83490147343683
Finished 16 epochs...
Completing Train Step...
At time: 179.52521562576294 and batch: 50, loss is 3.788358187675476 and perplexity is 44.18379918756038
At time: 180.0689799785614 and batch: 100, loss is 3.678294825553894 and perplexity is 39.57884766009464
At time: 180.58250784873962 and batch: 150, loss is 3.8021533727645873 and perplexity is 44.797546520980454
At time: 181.09541368484497 and batch: 200, loss is 3.838883090019226 and perplexity is 46.47353868220926
At time: 181.6084954738617 and batch: 250, loss is 3.8334641027450562 and perplexity is 46.22238029417308
At time: 182.11913442611694 and batch: 300, loss is 3.6664310359954833 and perplexity is 39.11206690970419
At time: 182.6312131881714 and batch: 350, loss is 3.7051856088638306 and perplexity is 40.65759302622953
At time: 183.14425039291382 and batch: 400, loss is 3.7134903049468995 and perplexity is 40.99664790498471
At time: 183.6575379371643 and batch: 450, loss is 3.7597802114486694 and perplexity is 42.93898744371419
At time: 184.17073822021484 and batch: 500, loss is 3.7922637176513674 and perplexity is 44.356697750224996
At time: 184.68358850479126 and batch: 550, loss is 3.6862900924682616 and perplexity is 39.89655951438163
At time: 185.1968915462494 and batch: 600, loss is 3.62585373878479 and perplexity is 37.55677315036425
At time: 185.70975542068481 and batch: 650, loss is 3.597552719116211 and perplexity is 36.50877779156855
At time: 186.2352259159088 and batch: 700, loss is 3.677454419136047 and perplexity is 39.545599315528094
At time: 186.74568152427673 and batch: 750, loss is 3.6380867767333984 and perplexity is 38.019028208791546
At time: 187.25863432884216 and batch: 800, loss is 3.759577407836914 and perplexity is 42.93028014494116
At time: 187.77060985565186 and batch: 850, loss is 3.6208072471618653 and perplexity is 37.367720636458465
At time: 188.28253412246704 and batch: 900, loss is 3.5602571821212767 and perplexity is 35.172241653732364
At time: 188.79270195960999 and batch: 950, loss is 3.5958053207397462 and perplexity is 36.44503811804843
At time: 189.30360174179077 and batch: 1000, loss is 3.501250076293945 and perplexity is 33.15687468553934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.452255621189025 and perplexity of 85.82030394285117
Finished 17 epochs...
Completing Train Step...
At time: 190.7875530719757 and batch: 50, loss is 3.7872369480133057 and perplexity is 44.13428632256713
At time: 191.29513335227966 and batch: 100, loss is 3.6769215631484986 and perplexity is 39.52453281935439
At time: 191.80463218688965 and batch: 150, loss is 3.8007007455825805 and perplexity is 44.73251962856718
At time: 192.31247067451477 and batch: 200, loss is 3.8374304723739625 and perplexity is 46.40607940800981
At time: 192.82068848609924 and batch: 250, loss is 3.832011046409607 and perplexity is 46.155265344357325
At time: 193.32687544822693 and batch: 300, loss is 3.6650307083129885 and perplexity is 39.05733552956794
At time: 193.83290243148804 and batch: 350, loss is 3.7038256168365478 and perplexity is 40.602336606527224
At time: 194.34058260917664 and batch: 400, loss is 3.7121074390411377 and perplexity is 40.93999421959969
At time: 194.84913754463196 and batch: 450, loss is 3.7585289335250853 and perplexity is 42.885292437358686
At time: 195.35726165771484 and batch: 500, loss is 3.7910345602035522 and perplexity is 44.30220987877148
At time: 195.86471009254456 and batch: 550, loss is 3.6852160930633544 and perplexity is 39.85373363480648
At time: 196.37267136573792 and batch: 600, loss is 3.6250068759918213 and perplexity is 37.52498118017761
At time: 196.88081693649292 and batch: 650, loss is 3.5969809436798097 and perplexity is 36.48790893593237
At time: 197.3894546031952 and batch: 700, loss is 3.676982197761536 and perplexity is 39.52692944676592
At time: 197.8962206840515 and batch: 750, loss is 3.638012580871582 and perplexity is 38.01620745887346
At time: 198.4029939174652 and batch: 800, loss is 3.75954291343689 and perplexity is 42.928799316224975
At time: 198.9095549583435 and batch: 850, loss is 3.6209367322921753 and perplexity is 37.37255951390904
At time: 199.44058275222778 and batch: 900, loss is 3.560551471710205 and perplexity is 35.1825940014897
At time: 199.9488823413849 and batch: 950, loss is 3.596292996406555 and perplexity is 36.46281581083628
At time: 200.45735716819763 and batch: 1000, loss is 3.5019176530838014 and perplexity is 33.17901683547395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.452219148961509 and perplexity of 85.81717394227972
Finished 18 epochs...
Completing Train Step...
At time: 201.9271628856659 and batch: 50, loss is 3.7861899089813233 and perplexity is 44.08810018570219
At time: 202.448233127594 and batch: 100, loss is 3.675696363449097 and perplexity is 39.47613702693425
At time: 202.9553928375244 and batch: 150, loss is 3.7994339513778685 and perplexity is 44.67588860943627
At time: 203.46380066871643 and batch: 200, loss is 3.836168427467346 and perplexity is 46.347549793126696
At time: 203.97214531898499 and batch: 250, loss is 3.8307415437698364 and perplexity is 46.09670829019517
At time: 204.48123025894165 and batch: 300, loss is 3.663801054954529 and perplexity is 39.00933806193303
At time: 204.98522305488586 and batch: 350, loss is 3.7026612424850462 and perplexity is 40.55508780015889
At time: 205.4920778274536 and batch: 400, loss is 3.710957922935486 and perplexity is 40.89296007531135
At time: 206.0007598400116 and batch: 450, loss is 3.757469096183777 and perplexity is 42.839865080097624
At time: 206.50857591629028 and batch: 500, loss is 3.790014486312866 and perplexity is 44.257041392687796
At time: 207.0166380405426 and batch: 550, loss is 3.6843338775634766 and perplexity is 39.818589557869956
At time: 207.52552914619446 and batch: 600, loss is 3.6242833232879637 and perplexity is 37.49783969891268
At time: 208.03404569625854 and batch: 650, loss is 3.5964683532714843 and perplexity is 36.469210376552475
At time: 208.54613542556763 and batch: 700, loss is 3.6765681219100954 and perplexity is 39.51056568795292
At time: 209.05340266227722 and batch: 750, loss is 3.637887406349182 and perplexity is 38.01144909608043
At time: 209.5626142024994 and batch: 800, loss is 3.759486885070801 and perplexity is 42.92639415312044
At time: 210.07012462615967 and batch: 850, loss is 3.62102059841156 and perplexity is 37.375693936881056
At time: 210.57739186286926 and batch: 900, loss is 3.5607678651809693 and perplexity is 35.190208108908024
At time: 211.08519172668457 and batch: 950, loss is 3.5966572904586793 and perplexity is 36.476101417546694
At time: 211.59256792068481 and batch: 1000, loss is 3.5024116230010987 and perplexity is 33.195410320286996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.452231058260289 and perplexity of 85.81819597073046
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 213.07783913612366 and batch: 50, loss is 3.7855589532852174 and perplexity is 44.060291321762655
At time: 213.6027717590332 and batch: 100, loss is 3.6753559589385985 and perplexity is 39.462701458726976
At time: 214.11581420898438 and batch: 150, loss is 3.798975739479065 and perplexity is 44.655422275002884
At time: 214.62846398353577 and batch: 200, loss is 3.8356534576416017 and perplexity is 46.3236883479755
At time: 215.14030027389526 and batch: 250, loss is 3.830323839187622 and perplexity is 46.07745750476789
At time: 215.65246748924255 and batch: 300, loss is 3.662725987434387 and perplexity is 38.9674229244389
At time: 216.16160774230957 and batch: 350, loss is 3.7017476654052732 and perplexity is 40.51805452042999
At time: 216.67216563224792 and batch: 400, loss is 3.7098190784454346 and perplexity is 40.846415861389005
At time: 217.18378138542175 and batch: 450, loss is 3.756300435066223 and perplexity is 42.78982903878005
At time: 217.69454526901245 and batch: 500, loss is 3.789097075462341 and perplexity is 44.21645812131515
At time: 218.2057044506073 and batch: 550, loss is 3.6832839679718017 and perplexity is 39.7768055773054
At time: 218.71788835525513 and batch: 600, loss is 3.623238582611084 and perplexity is 37.458684637488226
At time: 219.2302541732788 and batch: 650, loss is 3.5950872230529787 and perplexity is 36.41887641493517
At time: 219.7424600124359 and batch: 700, loss is 3.675028977394104 and perplexity is 39.44979999303962
At time: 220.2547447681427 and batch: 750, loss is 3.6360519695281983 and perplexity is 37.941745470659164
At time: 220.7664339542389 and batch: 800, loss is 3.757608399391174 and perplexity is 42.845833226389054
At time: 221.27760887145996 and batch: 850, loss is 3.6191242694854737 and perplexity is 37.3048844875591
At time: 221.78883981704712 and batch: 900, loss is 3.5587776470184327 and perplexity is 35.12024156502404
At time: 222.30060124397278 and batch: 950, loss is 3.5942358016967773 and perplexity is 36.387881802396535
At time: 222.81158018112183 and batch: 1000, loss is 3.499926195144653 and perplexity is 33.11300796774107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.452207983993903 and perplexity of 85.81621580166139
Finished 20 epochs...
Completing Train Step...
At time: 224.29773592948914 and batch: 50, loss is 3.785343413352966 and perplexity is 44.050795592947324
At time: 224.8057405948639 and batch: 100, loss is 3.6750590991973877 and perplexity is 39.45098831005163
At time: 225.32659649848938 and batch: 150, loss is 3.798697409629822 and perplexity is 44.6429950675648
At time: 225.83441615104675 and batch: 200, loss is 3.8353897857666017 and perplexity is 46.31147570434795
At time: 226.34213829040527 and batch: 250, loss is 3.8300288105010987 and perplexity is 46.063865338140054
At time: 226.84981203079224 and batch: 300, loss is 3.662508101463318 and perplexity is 38.95893339456327
At time: 227.35619473457336 and batch: 350, loss is 3.7015191507339478 and perplexity is 40.50879660834317
At time: 227.86091327667236 and batch: 400, loss is 3.7095862579345704 and perplexity is 40.836907084943235
At time: 228.36951899528503 and batch: 450, loss is 3.756091947555542 and perplexity is 42.780908823750316
At time: 228.87627482414246 and batch: 500, loss is 3.7888986444473267 and perplexity is 44.20768507510081
At time: 229.38433980941772 and batch: 550, loss is 3.6831152057647705 and perplexity is 39.770093322210954
At time: 229.89146542549133 and batch: 600, loss is 3.6231036472320555 and perplexity is 37.453630476678974
At time: 230.39942145347595 and batch: 650, loss is 3.59500009059906 and perplexity is 36.41570328710739
At time: 230.90739917755127 and batch: 700, loss is 3.6749385309219362 and perplexity is 39.44623205915847
At time: 231.4154121875763 and batch: 750, loss is 3.6360792541503906 and perplexity is 37.942780710972656
At time: 231.92310404777527 and batch: 800, loss is 3.7576114559173583 and perplexity is 42.84596418600034
At time: 232.4312446117401 and batch: 850, loss is 3.6191538190841674 and perplexity is 37.3059868482121
At time: 232.94032549858093 and batch: 900, loss is 3.5588364744186403 and perplexity is 35.12230765830083
At time: 233.44817543029785 and batch: 950, loss is 3.594323844909668 and perplexity is 36.39108564945714
At time: 233.95634031295776 and batch: 1000, loss is 3.5000428438186644 and perplexity is 33.116870781504694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.452190864376906 and perplexity of 85.81474667349023
Finished 21 epochs...
Completing Train Step...
At time: 235.4388473033905 and batch: 50, loss is 3.7851414346694945 and perplexity is 44.04189916972175
At time: 235.96022534370422 and batch: 100, loss is 3.6748116779327393 and perplexity is 39.441228504073884
At time: 236.4676239490509 and batch: 150, loss is 3.79844922542572 and perplexity is 44.63191675615305
At time: 236.97470474243164 and batch: 200, loss is 3.8351385498046877 and perplexity is 46.29984205765811
At time: 237.48181986808777 and batch: 250, loss is 3.829776601791382 and perplexity is 46.05224909501897
At time: 238.00140833854675 and batch: 300, loss is 3.6622930717468263 and perplexity is 38.95055696678333
At time: 238.50777411460876 and batch: 350, loss is 3.7013059854507446 and perplexity is 40.500162459525015
At time: 239.0119912624359 and batch: 400, loss is 3.7093726110458376 and perplexity is 40.82818333873286
At time: 239.51886367797852 and batch: 450, loss is 3.7559015703201295 and perplexity is 42.77276508781557
At time: 240.03938174247742 and batch: 500, loss is 3.7887150049209595 and perplexity is 44.19956754212459
At time: 240.54825019836426 and batch: 550, loss is 3.6829547023773195 and perplexity is 39.763710599751484
At time: 241.05554342269897 and batch: 600, loss is 3.6229727268218994 and perplexity is 37.44872735298162
At time: 241.56399059295654 and batch: 650, loss is 3.5949135971069337 and perplexity is 36.412553701973124
At time: 242.07205247879028 and batch: 700, loss is 3.674865756034851 and perplexity is 39.443361468529154
At time: 242.58011293411255 and batch: 750, loss is 3.6360919189453127 and perplexity is 37.9432612515521
At time: 243.08785343170166 and batch: 800, loss is 3.75761745929718 and perplexity is 42.84622140736927
At time: 243.5960099697113 and batch: 850, loss is 3.619182095527649 and perplexity is 37.30704174375501
At time: 244.10402131080627 and batch: 900, loss is 3.558890771865845 and perplexity is 35.12421476172156
At time: 244.6119191646576 and batch: 950, loss is 3.5943988227844237 and perplexity is 36.393814278011284
At time: 245.1195423603058 and batch: 1000, loss is 3.500148506164551 and perplexity is 33.12037017263358
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.452181188071647 and perplexity of 85.81391630782312
Finished 22 epochs...
Completing Train Step...
At time: 246.58588123321533 and batch: 50, loss is 3.784946155548096 and perplexity is 44.033299546037966
At time: 247.1064600944519 and batch: 100, loss is 3.67458270072937 and perplexity is 39.4321983957575
At time: 247.61379432678223 and batch: 150, loss is 3.798214945793152 and perplexity is 44.62146163185377
At time: 248.12126421928406 and batch: 200, loss is 3.8348989057540894 and perplexity is 46.28874790534225
At time: 248.62799787521362 and batch: 250, loss is 3.829543585777283 and perplexity is 46.04151943363456
At time: 249.13616800308228 and batch: 300, loss is 3.662081298828125 and perplexity is 38.94230916701058
At time: 249.642587184906 and batch: 350, loss is 3.7011009073257446 and perplexity is 40.49185761374582
At time: 250.14840841293335 and batch: 400, loss is 3.7091693925857543 and perplexity is 40.81988714118556
At time: 250.65362787246704 and batch: 450, loss is 3.755719013214111 and perplexity is 42.76495732830741
At time: 251.1746859550476 and batch: 500, loss is 3.7885391092300416 and perplexity is 44.19179371236512
At time: 251.68215537071228 and batch: 550, loss is 3.682799291610718 and perplexity is 39.757531371176015
At time: 252.18954133987427 and batch: 600, loss is 3.6228461503982543 and perplexity is 37.44398752698466
At time: 252.69748997688293 and batch: 650, loss is 3.594829788208008 and perplexity is 36.40950213381636
At time: 253.20557975769043 and batch: 700, loss is 3.674798483848572 and perplexity is 39.44070811661835
At time: 253.71348452568054 and batch: 750, loss is 3.636095986366272 and perplexity is 37.94341558308204
At time: 254.22149467468262 and batch: 800, loss is 3.7576207733154297 and perplexity is 42.846363400764226
At time: 254.72887349128723 and batch: 850, loss is 3.6192076778411866 and perplexity is 37.307996156402055
At time: 255.23699951171875 and batch: 900, loss is 3.5589399623870848 and perplexity is 35.12594258264969
At time: 255.744042634964 and batch: 950, loss is 3.5944665813446046 and perplexity is 36.396280354014195
At time: 256.25156354904175 and batch: 1000, loss is 3.500245213508606 and perplexity is 33.123573310548075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.452174489091083 and perplexity of 85.81334144399115
Finished 23 epochs...
Completing Train Step...
At time: 257.73318099975586 and batch: 50, loss is 3.7847557067871094 and perplexity is 44.024914257206554
At time: 258.2463355064392 and batch: 100, loss is 3.674362955093384 and perplexity is 39.423534294226805
At time: 258.7576081752777 and batch: 150, loss is 3.7979889631271364 and perplexity is 44.61137909427496
At time: 259.2674732208252 and batch: 200, loss is 3.8346688985824584 and perplexity is 46.27810238567807
At time: 259.777215719223 and batch: 250, loss is 3.8293215608596802 and perplexity is 46.03129820380186
At time: 260.2876949310303 and batch: 300, loss is 3.661873149871826 and perplexity is 38.93420420955007
At time: 260.7981700897217 and batch: 350, loss is 3.700901961326599 and perplexity is 40.48380272194642
At time: 261.3087406158447 and batch: 400, loss is 3.7089734983444216 and perplexity is 40.81189154353409
At time: 261.8162534236908 and batch: 450, loss is 3.7555415725708006 and perplexity is 42.75736975995934
At time: 262.3260087966919 and batch: 500, loss is 3.7883685350418093 and perplexity is 44.18425637588191
At time: 262.83640241622925 and batch: 550, loss is 3.682648401260376 and perplexity is 39.75153279591363
At time: 263.34706449508667 and batch: 600, loss is 3.6227233171463014 and perplexity is 37.439388442696654
At time: 263.8700957298279 and batch: 650, loss is 3.594748067855835 and perplexity is 36.40652685805148
At time: 264.3796706199646 and batch: 700, loss is 3.6747332334518434 and perplexity is 39.43813467872633
At time: 264.88938307762146 and batch: 750, loss is 3.6360939836502073 and perplexity is 37.9433395932702
At time: 265.3995008468628 and batch: 800, loss is 3.7576208829879763 and perplexity is 42.84636809983427
At time: 265.90858602523804 and batch: 850, loss is 3.619230418205261 and perplexity is 37.30884456346404
At time: 266.4188952445984 and batch: 900, loss is 3.558984761238098 and perplexity is 35.127516219766456
At time: 266.92941451072693 and batch: 950, loss is 3.5945296812057497 and perplexity is 36.39857702670983
At time: 267.4401857852936 and batch: 1000, loss is 3.5003350019454955 and perplexity is 33.12654755794434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.452170767435214 and perplexity of 85.81302207685962
Finished 24 epochs...
Completing Train Step...
At time: 268.90906143188477 and batch: 50, loss is 3.784569034576416 and perplexity is 44.01669679614624
At time: 269.4291932582855 and batch: 100, loss is 3.674149503707886 and perplexity is 39.41512018424414
At time: 269.93730306625366 and batch: 150, loss is 3.797769365310669 and perplexity is 44.60158360840928
At time: 270.44544768333435 and batch: 200, loss is 3.8344469451904297 and perplexity is 46.267831943698795
At time: 270.9525394439697 and batch: 250, loss is 3.829106650352478 and perplexity is 46.02140665709443
At time: 271.4588453769684 and batch: 300, loss is 3.6616685438156127 and perplexity is 38.926238850483095
At time: 271.96353244781494 and batch: 350, loss is 3.700707721710205 and perplexity is 40.47593992729335
At time: 272.4692208766937 and batch: 400, loss is 3.708783288002014 and perplexity is 40.80412943790909
At time: 272.97384095191956 and batch: 450, loss is 3.755368223190308 and perplexity is 42.749958438792476
At time: 273.47509145736694 and batch: 500, loss is 3.788202519416809 and perplexity is 44.1769217077961
At time: 273.97690320014954 and batch: 550, loss is 3.682501163482666 and perplexity is 39.745680299429054
At time: 274.4788546562195 and batch: 600, loss is 3.622603578567505 and perplexity is 37.434905771913186
At time: 274.98057985305786 and batch: 650, loss is 3.594667978286743 and perplexity is 36.403611191762074
At time: 275.4817228317261 and batch: 700, loss is 3.6746688747406004 and perplexity is 39.435596572880065
At time: 275.98421573638916 and batch: 750, loss is 3.6360873937606812 and perplexity is 37.94308955167791
At time: 276.48625779151917 and batch: 800, loss is 3.7576184129714965 and perplexity is 42.84626226872967
At time: 277.0005145072937 and batch: 850, loss is 3.61925048828125 and perplexity is 37.3095933623237
At time: 277.5065424442291 and batch: 900, loss is 3.5590254354476927 and perplexity is 35.12894503278144
At time: 278.0096855163574 and batch: 950, loss is 3.5945891094207765 and perplexity is 36.40074019344797
At time: 278.5194480419159 and batch: 1000, loss is 3.500419325828552 and perplexity is 33.129341034843435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.452169650938453 and perplexity of 85.81292626695193
Finished 25 epochs...
Completing Train Step...
At time: 279.9960525035858 and batch: 50, loss is 3.7843854904174803 and perplexity is 44.00861852993564
At time: 280.5173189640045 and batch: 100, loss is 3.6739410018920897 and perplexity is 39.40690291680326
At time: 281.0248818397522 and batch: 150, loss is 3.797555298805237 and perplexity is 44.592036925118514
At time: 281.53230381011963 and batch: 200, loss is 3.8342314624786376 and perplexity is 46.257863099898145
At time: 282.04031443595886 and batch: 250, loss is 3.828897228240967 and perplexity is 46.01176976606185
At time: 282.54902267456055 and batch: 300, loss is 3.6614674520492554 and perplexity is 38.9184118913499
At time: 283.05652928352356 and batch: 350, loss is 3.7005179262161256 and perplexity is 40.46825850524924
At time: 283.56463861465454 and batch: 400, loss is 3.7085978841781615 and perplexity is 40.79656489755134
At time: 284.0722391605377 and batch: 450, loss is 3.7551982975006104 and perplexity is 42.74269473978224
At time: 284.57975029945374 and batch: 500, loss is 3.7880403995513916 and perplexity is 44.16976033171061
At time: 285.08737778663635 and batch: 550, loss is 3.6823572540283203 and perplexity is 39.739960931809975
At time: 285.5952126979828 and batch: 600, loss is 3.622486753463745 and perplexity is 37.43053269060996
At time: 286.10250520706177 and batch: 650, loss is 3.5945887088775637 and perplexity is 36.40072561338147
At time: 286.6089811325073 and batch: 700, loss is 3.674604902267456 and perplexity is 39.433073860930286
At time: 287.1157805919647 and batch: 750, loss is 3.636077036857605 and perplexity is 37.942696580812
At time: 287.6230366230011 and batch: 800, loss is 3.7576137924194337 and perplexity is 42.84606429580153
At time: 288.1307604312897 and batch: 850, loss is 3.6192681407928466 and perplexity is 37.31025197616626
At time: 288.63909816741943 and batch: 900, loss is 3.5590626430511474 and perplexity is 35.13025212095466
At time: 289.1458947658539 and batch: 950, loss is 3.5946454334259035 and perplexity is 36.402790486665076
At time: 289.6658020019531 and batch: 1000, loss is 3.5004987955093383 and perplexity is 33.13197391761592
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4521703952696265 and perplexity of 85.81299014021181
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 291.1423044204712 and batch: 50, loss is 3.784269247055054 and perplexity is 44.00350311746411
At time: 291.65439915657043 and batch: 100, loss is 3.6738865900039674 and perplexity is 39.40475877114454
At time: 292.1676309108734 and batch: 150, loss is 3.797494640350342 and perplexity is 44.5893321230934
At time: 292.679575920105 and batch: 200, loss is 3.834149651527405 and perplexity is 46.254078854914404
At time: 293.1910574436188 and batch: 250, loss is 3.8288676500320435 and perplexity is 46.01040884044974
At time: 293.70360946655273 and batch: 300, loss is 3.6612522983551026 and perplexity is 38.910039351984594
At time: 294.21556282043457 and batch: 350, loss is 3.700323634147644 and perplexity is 40.46039660737334
At time: 294.7275252342224 and batch: 400, loss is 3.708340392112732 and perplexity is 40.78606145812763
At time: 295.2392120361328 and batch: 450, loss is 3.755007209777832 and perplexity is 42.73452791589374
At time: 295.7513027191162 and batch: 500, loss is 3.787851023674011 and perplexity is 44.161396436579096
At time: 296.2632145881653 and batch: 550, loss is 3.682123737335205 and perplexity is 39.73068207097527
At time: 296.7751142978668 and batch: 600, loss is 3.622283387184143 and perplexity is 37.422921356403734
At time: 297.2871530056 and batch: 650, loss is 3.5943069458007812 and perplexity is 36.390470677734484
At time: 297.7992351055145 and batch: 700, loss is 3.6743184232711794 and perplexity is 39.42177873149644
At time: 298.3118827342987 and batch: 750, loss is 3.6356969451904297 and perplexity is 37.928277618448774
At time: 298.82346081733704 and batch: 800, loss is 3.757278413772583 and perplexity is 42.831697050103514
At time: 299.33520007133484 and batch: 850, loss is 3.618913378715515 and perplexity is 37.2970180612538
At time: 299.8490047454834 and batch: 900, loss is 3.5586940574645998 and perplexity is 35.11730600239401
At time: 300.3590762615204 and batch: 950, loss is 3.5942004251480104 and perplexity is 36.38659454749088
At time: 300.87087965011597 and batch: 1000, loss is 3.500052876472473 and perplexity is 33.11720303327115
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.452173372594322 and perplexity of 85.81324563372688
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 302.3267855644226 and batch: 50, loss is 3.7842423915863037 and perplexity is 44.00232139862912
At time: 302.8465254306793 and batch: 100, loss is 3.673862271308899 and perplexity is 39.40380051048361
At time: 303.3535587787628 and batch: 150, loss is 3.797477025985718 and perplexity is 44.588546717256264
At time: 303.8611958026886 and batch: 200, loss is 3.8341328525543212 and perplexity is 46.25330184041525
At time: 304.3678801059723 and batch: 250, loss is 3.828856406211853 and perplexity is 46.009891510594244
At time: 304.8756914138794 and batch: 300, loss is 3.661216630935669 and perplexity is 38.90865155604055
At time: 305.3840079307556 and batch: 350, loss is 3.7002897214889527 and perplexity is 40.45902451101853
At time: 305.8918344974518 and batch: 400, loss is 3.708294377326965 and perplexity is 40.7841847394261
At time: 306.40046310424805 and batch: 450, loss is 3.7549720668792723 and perplexity is 42.73302612710296
At time: 306.90814900398254 and batch: 500, loss is 3.7878168869018554 and perplexity is 44.159888934781634
At time: 307.4162254333496 and batch: 550, loss is 3.682082562446594 and perplexity is 39.72904619824522
At time: 307.92376828193665 and batch: 600, loss is 3.6222477769851684 and perplexity is 37.42158874245549
At time: 308.43137550354004 and batch: 650, loss is 3.5942573881149293 and perplexity is 36.388667294906746
At time: 308.9380986690521 and batch: 700, loss is 3.674266805648804 and perplexity is 39.41974392552488
At time: 309.4459156990051 and batch: 750, loss is 3.6356296062469484 and perplexity is 37.92572365429747
At time: 309.9534602165222 and batch: 800, loss is 3.7572188186645508 and perplexity is 42.829144566549125
At time: 310.4617850780487 and batch: 850, loss is 3.618851089477539 and perplexity is 37.29469493077375
At time: 310.968777179718 and batch: 900, loss is 3.5586294174194335 and perplexity is 35.11503609151226
At time: 311.47628140449524 and batch: 950, loss is 3.5941226291656494 and perplexity is 36.383763926730175
At time: 311.9827377796173 and batch: 1000, loss is 3.4999743938446044 and perplexity is 33.114604010139864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.452174489091083 and perplexity of 85.81334144399115
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 313.44970512390137 and batch: 50, loss is 3.7842372179031374 and perplexity is 44.00209374514853
At time: 313.96931767463684 and batch: 100, loss is 3.6738579845428467 and perplexity is 39.4036315959713
At time: 314.4767339229584 and batch: 150, loss is 3.7974740266799927 and perplexity is 44.58841298277338
At time: 314.9853689670563 and batch: 200, loss is 3.8341301488876343 and perplexity is 46.25317678707295
At time: 315.4934527873993 and batch: 250, loss is 3.828854560852051 and perplexity is 46.00980660586828
At time: 316.01426887512207 and batch: 300, loss is 3.6612105131149293 and perplexity is 38.90841352061324
At time: 316.5216829776764 and batch: 350, loss is 3.700283923149109 and perplexity is 40.45878991652479
At time: 317.02909898757935 and batch: 400, loss is 3.7082864570617677 and perplexity is 40.78386171914631
At time: 317.5377547740936 and batch: 450, loss is 3.7549658966064454 and perplexity is 42.73276245348651
At time: 318.0451247692108 and batch: 500, loss is 3.787810983657837 and perplexity is 44.15962824895087
At time: 318.5529975891113 and batch: 550, loss is 3.6820752096176146 and perplexity is 39.728754078436964
At time: 319.0605206489563 and batch: 600, loss is 3.6222417306900025 and perplexity is 37.4213624811684
At time: 319.5667243003845 and batch: 650, loss is 3.5942486476898194 and perplexity is 36.38834924387536
At time: 320.07456254959106 and batch: 700, loss is 3.6742574119567872 and perplexity is 39.419373630330284
At time: 320.5832722187042 and batch: 750, loss is 3.635617551803589 and perplexity is 37.92526648356529
At time: 321.09113907814026 and batch: 800, loss is 3.7572080183029173 and perplexity is 42.82868199879731
At time: 321.5986831188202 and batch: 850, loss is 3.6188399362564088 and perplexity is 37.29427897711383
At time: 322.10687017440796 and batch: 900, loss is 3.5586178874969483 and perplexity is 35.11463122020213
At time: 322.61420130729675 and batch: 950, loss is 3.594108738899231 and perplexity is 36.38325855006585
At time: 323.12077260017395 and batch: 1000, loss is 3.4999600982666017 and perplexity is 33.114130621118896
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.452174116925495 and perplexity of 85.81330950722447
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 324.58380103111267 and batch: 50, loss is 3.784236626625061 and perplexity is 44.00206772768288
At time: 325.09109354019165 and batch: 100, loss is 3.6738574647903444 and perplexity is 39.4036111158405
At time: 325.5984396934509 and batch: 150, loss is 3.7974738454818726 and perplexity is 44.5884049034375
At time: 326.1053149700165 and batch: 200, loss is 3.834129934310913 and perplexity is 46.25316686221899
At time: 326.6127369403839 and batch: 250, loss is 3.828854489326477 and perplexity is 46.00980331499057
At time: 327.12006855010986 and batch: 300, loss is 3.661209864616394 and perplexity is 38.90838828857224
At time: 327.62663221359253 and batch: 350, loss is 3.7002833032608033 and perplexity is 40.45876483660184
At time: 328.1341972351074 and batch: 400, loss is 3.7082854652404786 and perplexity is 40.78382126886407
At time: 328.6601390838623 and batch: 450, loss is 3.7549651956558225 and perplexity is 42.732732499940546
At time: 329.1793386936188 and batch: 500, loss is 3.787810182571411 and perplexity is 44.159592873286286
At time: 329.6873354911804 and batch: 550, loss is 3.6820743703842163 and perplexity is 39.72872073675366
At time: 330.1961627006531 and batch: 600, loss is 3.6222409629821777 and perplexity is 37.421333752506634
At time: 330.70516324043274 and batch: 650, loss is 3.5942475318908693 and perplexity is 36.388308641816124
At time: 331.2128264904022 and batch: 700, loss is 3.674256205558777 and perplexity is 39.41932607490506
At time: 331.72032284736633 and batch: 750, loss is 3.635615921020508 and perplexity is 37.9252046357328
At time: 332.2283556461334 and batch: 800, loss is 3.757206583023071 and perplexity is 42.82862052769731
At time: 332.7355694770813 and batch: 850, loss is 3.6188384675979615 and perplexity is 37.29422420459619
At time: 333.2435758113861 and batch: 900, loss is 3.5586162424087524 and perplexity is 35.11457345358432
At time: 333.75057220458984 and batch: 950, loss is 3.594106845855713 and perplexity is 36.38318967503928
At time: 334.25816464424133 and batch: 1000, loss is 3.4999581718444825 and perplexity is 33.11406682938666
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.452174489091083 and perplexity of 85.81334144399115
Annealing...
Model not improving. Stopping early with 85.81292626695193loss at 29 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -85.81292626695193
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f46fd764898>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'batch_size': 20, 'lr': 19.749362416879595, 'dropout': 0.374282315851305, 'wordvec_source': '', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'anneal': 7.170494832615578, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.743065595626831 and batch: 50, loss is 6.347108192443848 and perplexity is 570.8395617158708
At time: 1.2691829204559326 and batch: 100, loss is 5.712646436691284 and perplexity is 302.6710089832675
At time: 1.7821543216705322 and batch: 150, loss is 5.617284803390503 and perplexity is 275.14130536566137
At time: 2.29541015625 and batch: 200, loss is 5.563877792358398 and perplexity is 260.8323313732732
At time: 2.8097500801086426 and batch: 250, loss is 5.6088754844665525 and perplexity is 272.837255701782
At time: 3.3227248191833496 and batch: 300, loss is 5.521373472213745 and perplexity is 249.97813954368948
At time: 3.8358359336853027 and batch: 350, loss is 5.500646514892578 and perplexity is 244.8501803918965
At time: 4.349010705947876 and batch: 400, loss is 5.4944983577728275 and perplexity is 243.34942119109851
At time: 4.862504005432129 and batch: 450, loss is 5.518702125549316 and perplexity is 249.31125241437417
At time: 5.375202655792236 and batch: 500, loss is 5.5258519649505615 and perplexity is 251.10017546548258
At time: 5.8875086307525635 and batch: 550, loss is 5.4657965183258055 and perplexity is 236.4641281289017
At time: 6.398951530456543 and batch: 600, loss is 5.368526134490967 and perplexity is 214.5464219909493
At time: 6.910756349563599 and batch: 650, loss is 5.360253648757935 and perplexity is 212.7789107116901
At time: 7.4238481521606445 and batch: 700, loss is 5.418068885803223 and perplexity is 225.4433450252752
At time: 7.936969518661499 and batch: 750, loss is 5.335991697311401 and perplexity is 207.67860102979793
At time: 8.46268081665039 and batch: 800, loss is 5.428483257293701 and perplexity is 227.8034640170624
At time: 8.975314617156982 and batch: 850, loss is 5.36687331199646 and perplexity is 214.19210772851847
At time: 9.487212657928467 and batch: 900, loss is 5.403945217132568 and perplexity is 222.28163792530472
At time: 9.999626159667969 and batch: 950, loss is 5.37391788482666 and perplexity is 215.70632688257308
At time: 10.513747215270996 and batch: 1000, loss is 5.2787930870056154 and perplexity is 196.13301696001744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.237156937762005 and perplexity of 188.13446335046083
Finished 1 epochs...
Completing Train Step...
At time: 12.004229307174683 and batch: 50, loss is 5.244722881317139 and perplexity is 189.56327642463205
At time: 12.508510828018188 and batch: 100, loss is 5.184661474227905 and perplexity is 178.51300830593718
At time: 13.014487743377686 and batch: 150, loss is 5.213816814422607 and perplexity is 183.79422959787036
At time: 13.518855810165405 and batch: 200, loss is 5.199696254730225 and perplexity is 181.21718965048973
At time: 14.0239896774292 and batch: 250, loss is 5.23905613899231 and perplexity is 188.49210806686054
At time: 14.529505252838135 and batch: 300, loss is 5.167014656066894 and perplexity is 175.39045431275252
At time: 15.034192323684692 and batch: 350, loss is 5.162948589324952 and perplexity is 174.67875291285372
At time: 15.537658452987671 and batch: 400, loss is 5.159895429611206 and perplexity is 174.14624411248207
At time: 16.041512489318848 and batch: 450, loss is 5.184462337493897 and perplexity is 178.47746334775601
At time: 16.546781063079834 and batch: 500, loss is 5.223698673248291 and perplexity is 185.61946171782384
At time: 17.053410291671753 and batch: 550, loss is 5.162053070068359 and perplexity is 174.52239474722623
At time: 17.560735940933228 and batch: 600, loss is 5.085436563491822 and perplexity is 161.65049453989585
At time: 18.067448616027832 and batch: 650, loss is 5.072735977172852 and perplexity is 159.61042096848013
At time: 18.576038122177124 and batch: 700, loss is 5.124002799987793 and perplexity is 168.00652198479457
At time: 19.08541774749756 and batch: 750, loss is 5.06285213470459 and perplexity is 158.04062728751256
At time: 19.597079753875732 and batch: 800, loss is 5.1914169979095455 and perplexity is 179.72303975723412
At time: 20.107304334640503 and batch: 850, loss is 5.098508529663086 and perplexity is 163.77745583193132
At time: 20.61768126487732 and batch: 900, loss is 5.1114100646972656 and perplexity is 165.90412556968988
At time: 21.1276695728302 and batch: 950, loss is 5.116146087646484 and perplexity is 166.69171486411508
At time: 21.65028405189514 and batch: 1000, loss is 5.022846240997314 and perplexity is 151.8428707347813
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.240493402248475 and perplexity of 188.76321562766051
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 23.127097129821777 and batch: 50, loss is 5.096773347854614 and perplexity is 163.49351858255014
At time: 23.65171718597412 and batch: 100, loss is 4.950875663757325 and perplexity is 141.2986398621912
At time: 24.162827491760254 and batch: 150, loss is 4.969633455276489 and perplexity is 143.97410474949677
At time: 24.674524545669556 and batch: 200, loss is 4.950865125656128 and perplexity is 141.29715085067102
At time: 25.18499445915222 and batch: 250, loss is 4.97341323852539 and perplexity is 144.51932541775923
At time: 25.695101022720337 and batch: 300, loss is 4.871891231536865 and perplexity is 130.56761713450953
At time: 26.20540499687195 and batch: 350, loss is 4.861646347045898 and perplexity is 129.23679567474403
At time: 26.71662473678589 and batch: 400, loss is 4.861295137405396 and perplexity is 129.19141443582618
At time: 27.226381063461304 and batch: 450, loss is 4.888805541992188 and perplexity is 132.79486139676052
At time: 27.737565994262695 and batch: 500, loss is 4.919884843826294 and perplexity is 136.98683739810383
At time: 28.24774670600891 and batch: 550, loss is 4.831967077255249 and perplexity is 125.45750269169639
At time: 28.757776498794556 and batch: 600, loss is 4.747131433486938 and perplexity is 115.2531984174537
At time: 29.264548301696777 and batch: 650, loss is 4.721923294067383 and perplexity is 112.38419279433144
At time: 29.775967836380005 and batch: 700, loss is 4.8030799198150635 and perplexity is 121.885236771718
At time: 30.28768563270569 and batch: 750, loss is 4.710400056838989 and perplexity is 111.096595969637
At time: 30.79873037338257 and batch: 800, loss is 4.8051053905487064 and perplexity is 122.13236193957322
At time: 31.310595989227295 and batch: 850, loss is 4.728283567428589 and perplexity is 113.10126495214053
At time: 31.82165265083313 and batch: 900, loss is 4.704504499435425 and perplexity is 110.44354654746985
At time: 32.33244037628174 and batch: 950, loss is 4.668485088348389 and perplexity is 106.53622715081953
At time: 32.842432498931885 and batch: 1000, loss is 4.555899305343628 and perplexity is 95.19232371910674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.9567677567644814 and perplexity of 142.13364212984203
Finished 3 epochs...
Completing Train Step...
At time: 34.321383476257324 and batch: 50, loss is 4.852302665710449 and perplexity is 128.03487217863284
At time: 34.84931039810181 and batch: 100, loss is 4.762295246124268 and perplexity is 117.01419428614827
At time: 35.360430002212524 and batch: 150, loss is 4.810611982345581 and perplexity is 122.80675008798883
At time: 35.87235164642334 and batch: 200, loss is 4.814284467697144 and perplexity is 123.2585852499363
At time: 36.383522272109985 and batch: 250, loss is 4.83867470741272 and perplexity is 126.30185385211287
At time: 36.89596104621887 and batch: 300, loss is 4.736359233856201 and perplexity is 114.01833101627543
At time: 37.40791726112366 and batch: 350, loss is 4.740143547058105 and perplexity is 114.45062955221464
At time: 37.91947889328003 and batch: 400, loss is 4.743499994277954 and perplexity is 114.83542245687471
At time: 38.430665731430054 and batch: 450, loss is 4.777553939819336 and perplexity is 118.81336965967517
At time: 38.94217872619629 and batch: 500, loss is 4.810826015472412 and perplexity is 122.83303761380652
At time: 39.453983545303345 and batch: 550, loss is 4.7315154552459715 and perplexity is 113.46738686626531
At time: 39.96731734275818 and batch: 600, loss is 4.65265266418457 and perplexity is 104.86278271095594
At time: 40.47707390785217 and batch: 650, loss is 4.63452938079834 and perplexity is 102.97944248636105
At time: 40.98661971092224 and batch: 700, loss is 4.720202074050904 and perplexity is 112.19092125129768
At time: 41.49866986274719 and batch: 750, loss is 4.634604034423828 and perplexity is 102.98713056206114
At time: 42.011242628097534 and batch: 800, loss is 4.7371971321105955 and perplexity is 114.1139068126076
At time: 42.52268099784851 and batch: 850, loss is 4.653897743225098 and perplexity is 104.99342647787152
At time: 43.03361415863037 and batch: 900, loss is 4.633288431167602 and perplexity is 102.85172944434956
At time: 43.54521298408508 and batch: 950, loss is 4.609110975265503 and perplexity is 100.39485643974913
At time: 44.0567352771759 and batch: 1000, loss is 4.50848913192749 and perplexity is 90.78455136023675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.924143349252096 and perplexity of 137.57144047263478
Finished 4 epochs...
Completing Train Step...
At time: 45.54384136199951 and batch: 50, loss is 4.770757064819336 and perplexity is 118.0085482713494
At time: 46.05302619934082 and batch: 100, loss is 4.681883764266968 and perplexity is 107.9732773179888
At time: 46.562209129333496 and batch: 150, loss is 4.730582647323608 and perplexity is 113.36159293924443
At time: 47.07154035568237 and batch: 200, loss is 4.742287216186523 and perplexity is 114.69623699000107
At time: 47.59782528877258 and batch: 250, loss is 4.762301044464111 and perplexity is 117.01487277618035
At time: 48.107975006103516 and batch: 300, loss is 4.662533559799194 and perplexity is 105.90405680931717
At time: 48.63526797294617 and batch: 350, loss is 4.666156816482544 and perplexity is 106.28847038492583
At time: 49.15838813781738 and batch: 400, loss is 4.6713435840606685 and perplexity is 106.8411961679882
At time: 49.673001527786255 and batch: 450, loss is 4.710771646499634 and perplexity is 111.13788598702874
At time: 50.19482660293579 and batch: 500, loss is 4.744415998458862 and perplexity is 114.94066037578999
At time: 50.70852756500244 and batch: 550, loss is 4.667481079101562 and perplexity is 106.429317471772
At time: 51.21843671798706 and batch: 600, loss is 4.590646390914917 and perplexity is 98.55811664773837
At time: 51.727505922317505 and batch: 650, loss is 4.574792261123657 and perplexity is 97.00788473460823
At time: 52.235732078552246 and batch: 700, loss is 4.6638803291320805 and perplexity is 106.04678123212138
At time: 52.7451274394989 and batch: 750, loss is 4.583114786148071 and perplexity is 97.81860421998384
At time: 53.25542640686035 and batch: 800, loss is 4.689781837463379 and perplexity is 108.82943471182921
At time: 53.764174461364746 and batch: 850, loss is 4.606000385284424 and perplexity is 100.08305440071545
At time: 54.27267336845398 and batch: 900, loss is 4.58507212638855 and perplexity is 98.01025601298663
At time: 54.78140640258789 and batch: 950, loss is 4.565693330764771 and perplexity is 96.12922026264808
At time: 55.29023313522339 and batch: 1000, loss is 4.470284051895142 and perplexity is 87.3815403807096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.905699194931403 and perplexity of 135.05730838700347
Finished 5 epochs...
Completing Train Step...
At time: 56.762651681900024 and batch: 50, loss is 4.714812526702881 and perplexity is 111.58788946308205
At time: 57.28379511833191 and batch: 100, loss is 4.628159599304199 and perplexity is 102.32557066051685
At time: 57.79272127151489 and batch: 150, loss is 4.670753269195557 and perplexity is 106.77814483358488
At time: 58.301857709884644 and batch: 200, loss is 4.690977144241333 and perplexity is 108.95959704924572
At time: 58.809998750686646 and batch: 250, loss is 4.708194541931152 and perplexity is 110.8518407758438
At time: 59.31958627700806 and batch: 300, loss is 4.606914710998535 and perplexity is 100.17460475794579
At time: 59.82712435722351 and batch: 350, loss is 4.613029184341431 and perplexity is 100.7889961340437
At time: 60.334975719451904 and batch: 400, loss is 4.616724824905395 and perplexity is 101.16216516113379
At time: 60.85593843460083 and batch: 450, loss is 4.659356641769409 and perplexity is 105.56814217097453
At time: 61.36386442184448 and batch: 500, loss is 4.695817584991455 and perplexity is 109.48828803936331
At time: 61.872713804244995 and batch: 550, loss is 4.6202061748504635 and perplexity is 101.51495980374878
At time: 62.38055968284607 and batch: 600, loss is 4.54395001411438 and perplexity is 94.0616119762314
At time: 62.88940453529358 and batch: 650, loss is 4.530219039916992 and perplexity is 92.77888113500158
At time: 63.3969087600708 and batch: 700, loss is 4.620400047302246 and perplexity is 101.53464266581922
At time: 63.90483069419861 and batch: 750, loss is 4.54272232055664 and perplexity is 93.94620399848856
At time: 64.41232419013977 and batch: 800, loss is 4.65090594291687 and perplexity is 104.67977653512403
At time: 64.92983055114746 and batch: 850, loss is 4.565577116012573 and perplexity is 96.11804927826546
At time: 65.4448401927948 and batch: 900, loss is 4.5457717800140385 and perplexity is 94.23312639552123
At time: 65.95331597328186 and batch: 950, loss is 4.527658252716065 and perplexity is 92.54159810901055
At time: 66.46417093276978 and batch: 1000, loss is 4.43510871887207 and perplexity is 84.3612960546956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.893827112709603 and perplexity of 133.46337727927994
Finished 6 epochs...
Completing Train Step...
At time: 67.93777656555176 and batch: 50, loss is 4.671873626708984 and perplexity is 106.89784156946739
At time: 68.4619951248169 and batch: 100, loss is 4.584098072052002 and perplexity is 97.91483517817444
At time: 68.97356271743774 and batch: 150, loss is 4.623744535446167 and perplexity is 101.87479257100982
At time: 69.48476839065552 and batch: 200, loss is 4.6464181327819825 and perplexity is 104.21104614626954
At time: 69.99518299102783 and batch: 250, loss is 4.662327861785888 and perplexity is 105.88227479556572
At time: 70.50646138191223 and batch: 300, loss is 4.561577949523926 and perplexity is 95.73442479700854
At time: 71.01733875274658 and batch: 350, loss is 4.570805034637451 and perplexity is 96.6218624178407
At time: 71.52783298492432 and batch: 400, loss is 4.5727169227600095 and perplexity is 96.80676931330102
At time: 72.03853607177734 and batch: 450, loss is 4.615411491394043 and perplexity is 101.02939270588845
At time: 72.54844999313354 and batch: 500, loss is 4.654613571166992 and perplexity is 105.06861061250135
At time: 73.05919480323792 and batch: 550, loss is 4.580538654327393 and perplexity is 97.566934906844
At time: 73.58306336402893 and batch: 600, loss is 4.506725597381592 and perplexity is 90.624590757161
At time: 74.09454393386841 and batch: 650, loss is 4.4932674789428715 and perplexity is 89.41312459394862
At time: 74.60562539100647 and batch: 700, loss is 4.58187707901001 and perplexity is 97.69760832949055
At time: 75.11325025558472 and batch: 750, loss is 4.508714084625244 and perplexity is 90.80497588716969
At time: 75.62430953979492 and batch: 800, loss is 4.616657285690308 and perplexity is 101.155332978625
At time: 76.13454127311707 and batch: 850, loss is 4.530616025924683 and perplexity is 92.8157203644669
At time: 76.64557361602783 and batch: 900, loss is 4.512163610458374 and perplexity is 91.11875087358997
At time: 77.15724563598633 and batch: 950, loss is 4.493762378692627 and perplexity is 89.45738607853043
At time: 77.66858243942261 and batch: 1000, loss is 4.406650152206421 and perplexity is 81.99433440099244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.886295225562119 and perplexity of 132.46192234039907
Finished 7 epochs...
Completing Train Step...
At time: 79.17128467559814 and batch: 50, loss is 4.636285429000854 and perplexity is 103.16043822334298
At time: 79.68734192848206 and batch: 100, loss is 4.54906852722168 and perplexity is 94.54430184339216
At time: 80.20113778114319 and batch: 150, loss is 4.583534126281738 and perplexity is 97.85963208826723
At time: 80.71501660346985 and batch: 200, loss is 4.609315671920776 and perplexity is 100.41540903452092
At time: 81.22881245613098 and batch: 250, loss is 4.622208423614502 and perplexity is 101.71842162916832
At time: 81.74277424812317 and batch: 300, loss is 4.520746879577636 and perplexity is 91.90421373144946
At time: 82.25551509857178 and batch: 350, loss is 4.533602209091186 and perplexity is 93.0932993507833
At time: 82.76931238174438 and batch: 400, loss is 4.536364908218384 and perplexity is 93.35084372264427
At time: 83.28354740142822 and batch: 450, loss is 4.578252248764038 and perplexity is 97.34411215273575
At time: 83.79754328727722 and batch: 500, loss is 4.620033340454102 and perplexity is 101.49741604307572
At time: 84.31095886230469 and batch: 550, loss is 4.545581302642822 and perplexity is 94.21517882668104
At time: 84.82420873641968 and batch: 600, loss is 4.474476079940796 and perplexity is 87.74861510487408
At time: 85.33796620368958 and batch: 650, loss is 4.460635967254639 and perplexity is 86.54252981388285
At time: 85.85167193412781 and batch: 700, loss is 4.5497502422332765 and perplexity is 94.60877608725272
At time: 86.37682580947876 and batch: 750, loss is 4.477990350723267 and perplexity is 88.05752998648333
At time: 86.89073514938354 and batch: 800, loss is 4.585355062484741 and perplexity is 98.03799057557906
At time: 87.4034972190857 and batch: 850, loss is 4.499140377044678 and perplexity is 89.93978375770831
At time: 87.91821599006653 and batch: 900, loss is 4.481720943450927 and perplexity is 88.38665029267105
At time: 88.43272352218628 and batch: 950, loss is 4.461284103393555 and perplexity is 86.59863933634887
At time: 88.94653654098511 and batch: 1000, loss is 4.378435564041138 and perplexity is 79.71322956400353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.879110941072789 and perplexity of 131.5136884757292
Finished 8 epochs...
Completing Train Step...
At time: 90.42441058158875 and batch: 50, loss is 4.603359155654907 and perplexity is 99.81906085927186
At time: 90.94779515266418 and batch: 100, loss is 4.518147144317627 and perplexity is 91.66559741049312
At time: 91.45778393745422 and batch: 150, loss is 4.548352870941162 and perplexity is 94.47666482530596
At time: 91.96842503547668 and batch: 200, loss is 4.5777382564544675 and perplexity is 97.29409088408225
At time: 92.47821187973022 and batch: 250, loss is 4.589277257919312 and perplexity is 98.42326981094254
At time: 92.98794507980347 and batch: 300, loss is 4.488399229049683 and perplexity is 88.9788969815523
At time: 93.49713063240051 and batch: 350, loss is 4.5029543590545655 and perplexity is 90.28346746004735
At time: 94.00565910339355 and batch: 400, loss is 4.502480974197388 and perplexity is 90.24073874805882
At time: 94.51450228691101 and batch: 450, loss is 4.54631199836731 and perplexity is 94.2840466126658
At time: 95.0236804485321 and batch: 500, loss is 4.590092496871948 and perplexity is 98.50354100999661
At time: 95.5341866016388 and batch: 550, loss is 4.516163177490235 and perplexity is 91.48391619036785
At time: 96.04462313652039 and batch: 600, loss is 4.447809209823609 and perplexity is 85.43955867036551
At time: 96.55379629135132 and batch: 650, loss is 4.432880935668945 and perplexity is 84.17356656429588
At time: 97.06278228759766 and batch: 700, loss is 4.521873216629029 and perplexity is 92.00778717090573
At time: 97.56988286972046 and batch: 750, loss is 4.453350706100464 and perplexity is 85.91433593990767
At time: 98.07741713523865 and batch: 800, loss is 4.559541416168213 and perplexity is 95.53965684065548
At time: 98.5941116809845 and batch: 850, loss is 4.4728853893280025 and perplexity is 87.60914516271333
At time: 99.11596655845642 and batch: 900, loss is 4.457291088104248 and perplexity is 86.25353909923518
At time: 99.6416244506836 and batch: 950, loss is 4.43419997215271 and perplexity is 84.28466782676374
At time: 100.15169382095337 and batch: 1000, loss is 4.3527817535400395 and perplexity is 77.69428896202638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.875466695645961 and perplexity of 131.03529254347595
Finished 9 epochs...
Completing Train Step...
At time: 101.62565660476685 and batch: 50, loss is 4.576611976623536 and perplexity is 97.1845721977715
At time: 102.14957237243652 and batch: 100, loss is 4.4896809768676755 and perplexity is 89.09301861055631
At time: 102.6609513759613 and batch: 150, loss is 4.51728588104248 and perplexity is 91.58668318568894
At time: 103.17208003997803 and batch: 200, loss is 4.55198112487793 and perplexity is 94.82007276527118
At time: 103.6937153339386 and batch: 250, loss is 4.561830997467041 and perplexity is 95.75865326164102
At time: 104.21165895462036 and batch: 300, loss is 4.460094060897827 and perplexity is 86.49564457169487
At time: 104.72331738471985 and batch: 350, loss is 4.474651699066162 and perplexity is 87.76402679316483
At time: 105.23442697525024 and batch: 400, loss is 4.474120111465454 and perplexity is 87.71738492295167
At time: 105.74545097351074 and batch: 450, loss is 4.519076414108277 and perplexity is 91.75081907183352
At time: 106.25596261024475 and batch: 500, loss is 4.56429009437561 and perplexity is 95.99442284115622
At time: 106.76598978042603 and batch: 550, loss is 4.489927930831909 and perplexity is 89.11502320163639
At time: 107.2885377407074 and batch: 600, loss is 4.422768020629883 and perplexity is 83.32661622371079
At time: 107.80765438079834 and batch: 650, loss is 4.407498054504394 and perplexity is 82.06388706831808
At time: 108.31799578666687 and batch: 700, loss is 4.496011953353882 and perplexity is 89.65885367074809
At time: 108.82929253578186 and batch: 750, loss is 4.431064968109131 and perplexity is 84.02084880523797
At time: 109.33836197853088 and batch: 800, loss is 4.535380554199219 and perplexity is 93.25899865587988
At time: 109.84839940071106 and batch: 850, loss is 4.449043016433716 and perplexity is 85.54503962078367
At time: 110.35981154441833 and batch: 900, loss is 4.436093444824219 and perplexity is 84.44440972765678
At time: 110.8704423904419 and batch: 950, loss is 4.413036289215088 and perplexity is 82.51963699631173
At time: 111.38169741630554 and batch: 1000, loss is 4.333204278945923 and perplexity is 76.18802353978025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.87385707948266 and perplexity of 130.82454567491948
Finished 10 epochs...
Completing Train Step...
At time: 112.87135863304138 and batch: 50, loss is 4.554020013809204 and perplexity is 95.01359758285783
At time: 113.38586831092834 and batch: 100, loss is 4.471004409790039 and perplexity is 87.44450904045375
At time: 113.90183615684509 and batch: 150, loss is 4.4908171558380126 and perplexity is 89.19430175169742
At time: 114.41662549972534 and batch: 200, loss is 4.526311025619507 and perplexity is 92.41700750524258
At time: 114.93303775787354 and batch: 250, loss is 4.536554040908814 and perplexity is 93.36850108861137
At time: 115.44773650169373 and batch: 300, loss is 4.4353531551361085 and perplexity is 84.38191953519323
At time: 115.96201968193054 and batch: 350, loss is 4.451678667068482 and perplexity is 85.77080384588065
At time: 116.47554421424866 and batch: 400, loss is 4.448631601333618 and perplexity is 85.50985233853115
At time: 116.9895875453949 and batch: 450, loss is 4.495402479171753 and perplexity is 89.6042255631381
At time: 117.50417113304138 and batch: 500, loss is 4.541279363632202 and perplexity is 93.8107414297342
At time: 118.01894760131836 and batch: 550, loss is 4.469210934638977 and perplexity is 87.28782003730956
At time: 118.53174376487732 and batch: 600, loss is 4.403828411102295 and perplexity is 81.76329373907373
At time: 119.0460855960846 and batch: 650, loss is 4.386912670135498 and perplexity is 80.3918393278122
At time: 119.5608298778534 and batch: 700, loss is 4.4744790744781495 and perplexity is 87.74887787177313
At time: 120.07557892799377 and batch: 750, loss is 4.41051308631897 and perplexity is 82.31168567127652
At time: 120.58750629425049 and batch: 800, loss is 4.514832229614258 and perplexity is 91.36223685881427
At time: 121.10129857063293 and batch: 850, loss is 4.4287180328369145 and perplexity is 83.82388852849226
At time: 121.61602902412415 and batch: 900, loss is 4.417857646942139 and perplexity is 82.91845433374463
At time: 122.13031148910522 and batch: 950, loss is 4.39077974319458 and perplexity is 80.70332231942324
At time: 122.6447696685791 and batch: 1000, loss is 4.310677504539489 and perplexity is 74.49093979793707
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.87244136159013 and perplexity of 130.63946606599458
Finished 11 epochs...
Completing Train Step...
At time: 124.12426948547363 and batch: 50, loss is 4.531540002822876 and perplexity is 92.90151957801424
At time: 124.6477952003479 and batch: 100, loss is 4.4490968608856205 and perplexity is 85.54964587056456
At time: 125.15984773635864 and batch: 150, loss is 4.4659678077697755 and perplexity is 87.00519310800615
At time: 125.68460750579834 and batch: 200, loss is 4.504081773757934 and perplexity is 90.38531176833068
At time: 126.19614315032959 and batch: 250, loss is 4.512837896347046 and perplexity is 91.1802116802461
At time: 126.70773148536682 and batch: 300, loss is 4.409407272338867 and perplexity is 82.22071456636533
At time: 127.21997213363647 and batch: 350, loss is 4.4281493663787845 and perplexity is 83.7762342456764
At time: 127.73059558868408 and batch: 400, loss is 4.425966339111328 and perplexity is 83.59354791912037
At time: 128.24315357208252 and batch: 450, loss is 4.4710800743103025 and perplexity is 87.45112573760143
At time: 128.75383853912354 and batch: 500, loss is 4.519278831481934 and perplexity is 91.76939291143144
At time: 129.2654311656952 and batch: 550, loss is 4.445790395736695 and perplexity is 85.26724607780992
At time: 129.77683115005493 and batch: 600, loss is 4.383539113998413 and perplexity is 80.12108989586078
At time: 130.28871130943298 and batch: 650, loss is 4.365142669677734 and perplexity is 78.66062162679866
At time: 130.7993185520172 and batch: 700, loss is 4.453994045257568 and perplexity is 85.96962577952687
At time: 131.31006717681885 and batch: 750, loss is 4.392477111816406 and perplexity is 80.84042192777599
At time: 131.81943464279175 and batch: 800, loss is 4.494514484405517 and perplexity is 89.52469279737146
At time: 132.32953906059265 and batch: 850, loss is 4.409403133392334 and perplexity is 82.2203742599281
At time: 132.84120559692383 and batch: 900, loss is 4.397193698883057 and perplexity is 81.22261342752022
At time: 133.35313868522644 and batch: 950, loss is 4.3724476146698 and perplexity is 79.23733701334594
At time: 133.86411786079407 and batch: 1000, loss is 4.293564062118531 and perplexity is 73.22698950668834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.872235926186166 and perplexity of 130.61263085105568
Finished 12 epochs...
Completing Train Step...
At time: 135.3403835296631 and batch: 50, loss is 4.511378078460694 and perplexity is 91.04720228472225
At time: 135.8647928237915 and batch: 100, loss is 4.429297285079956 and perplexity is 83.87245776950571
At time: 136.3756227493286 and batch: 150, loss is 4.443646516799927 and perplexity is 85.08463923836759
At time: 136.88704371452332 and batch: 200, loss is 4.483937587738037 and perplexity is 88.5827893610608
At time: 137.39855217933655 and batch: 250, loss is 4.490012617111206 and perplexity is 89.12257034094584
At time: 137.9106810092926 and batch: 300, loss is 4.388935413360596 and perplexity is 80.55461594831894
At time: 138.42135214805603 and batch: 350, loss is 4.407135429382325 and perplexity is 82.03413403617691
At time: 138.94599080085754 and batch: 400, loss is 4.4033611869812015 and perplexity is 81.72510087902629
At time: 139.46882319450378 and batch: 450, loss is 4.4503647994995115 and perplexity is 85.65818636677028
At time: 139.98812246322632 and batch: 500, loss is 4.499900817871094 and perplexity is 90.00820365254027
At time: 140.49899077415466 and batch: 550, loss is 4.425699210166931 and perplexity is 83.57122064516963
At time: 141.00991916656494 and batch: 600, loss is 4.364722566604614 and perplexity is 78.62758299822032
At time: 141.52017998695374 and batch: 650, loss is 4.3457554340362545 and perplexity is 77.15029743134292
At time: 142.03182983398438 and batch: 700, loss is 4.434206438064575 and perplexity is 84.2852128057594
At time: 142.54246139526367 and batch: 750, loss is 4.373996877670288 and perplexity is 79.3601916303211
At time: 143.0511884689331 and batch: 800, loss is 4.47580997467041 and perplexity is 87.86574061928002
At time: 143.56053924560547 and batch: 850, loss is 4.390640907287597 and perplexity is 80.6921185782313
At time: 144.0723135471344 and batch: 900, loss is 4.379384536743164 and perplexity is 79.78891114704868
At time: 144.58294224739075 and batch: 950, loss is 4.353657493591308 and perplexity is 77.76235876400087
At time: 145.09387254714966 and batch: 1000, loss is 4.2759967613220216 and perplexity is 71.9518223718614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8742448760242 and perplexity of 130.87528881965287
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 146.58402800559998 and batch: 50, loss is 4.492858333587646 and perplexity is 89.3765491121793
At time: 147.09479784965515 and batch: 100, loss is 4.390554790496826 and perplexity is 80.68516993114069
At time: 147.60595703125 and batch: 150, loss is 4.40537543296814 and perplexity is 81.88988123387618
At time: 148.1171612739563 and batch: 200, loss is 4.434518480300904 and perplexity is 84.31151745592021
At time: 148.62901854515076 and batch: 250, loss is 4.439009504318237 and perplexity is 84.69101403202345
At time: 149.1402006149292 and batch: 300, loss is 4.329869432449341 and perplexity is 75.93437135702
At time: 149.65210390090942 and batch: 350, loss is 4.340212392807007 and perplexity is 76.7238331981823
At time: 150.16353178024292 and batch: 400, loss is 4.33298734664917 and perplexity is 76.17149768940885
At time: 150.67451214790344 and batch: 450, loss is 4.379000930786133 and perplexity is 79.75830951528707
At time: 151.1860909461975 and batch: 500, loss is 4.41820647239685 and perplexity is 82.94738344659372
At time: 151.7206313610077 and batch: 550, loss is 4.341585602760315 and perplexity is 76.82926350198974
At time: 152.23162746429443 and batch: 600, loss is 4.274979104995728 and perplexity is 71.87863738953244
At time: 152.74295091629028 and batch: 650, loss is 4.247926526069641 and perplexity is 69.96020119959931
At time: 153.25416898727417 and batch: 700, loss is 4.335255403518676 and perplexity is 76.34445504241799
At time: 153.7648148536682 and batch: 750, loss is 4.266640210151673 and perplexity is 71.28174117705016
At time: 154.27486562728882 and batch: 800, loss is 4.358959884643554 and perplexity is 78.17578029199426
At time: 154.7823576927185 and batch: 850, loss is 4.2738307666778566 and perplexity is 71.79614377033886
At time: 155.2932460308075 and batch: 900, loss is 4.253035035133362 and perplexity is 70.31850794894228
At time: 155.80429601669312 and batch: 950, loss is 4.210280966758728 and perplexity is 67.37546741767594
At time: 156.31584310531616 and batch: 1000, loss is 4.131710433959961 and perplexity is 62.284365170965586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.821435323575648 and perplexity of 124.14314855186771
Finished 14 epochs...
Completing Train Step...
At time: 157.7974648475647 and batch: 50, loss is 4.449188451766968 and perplexity is 85.55748179687305
At time: 158.32481360435486 and batch: 100, loss is 4.353247919082642 and perplexity is 77.73051580559451
At time: 158.83913135528564 and batch: 150, loss is 4.3741778469085695 and perplexity is 79.37455468334646
At time: 159.3529872894287 and batch: 200, loss is 4.408883247375488 and perplexity is 82.1776401464471
At time: 159.866272687912 and batch: 250, loss is 4.416630096435547 and perplexity is 82.81673019165639
At time: 160.38083744049072 and batch: 300, loss is 4.309248924255371 and perplexity is 74.38459948591819
At time: 160.89475917816162 and batch: 350, loss is 4.3203962039947506 and perplexity is 75.21842422915708
At time: 161.4082329273224 and batch: 400, loss is 4.314056854248047 and perplexity is 74.74309655659864
At time: 161.9229714870453 and batch: 450, loss is 4.361417198181153 and perplexity is 78.36811891658647
At time: 162.43700313568115 and batch: 500, loss is 4.40256784439087 and perplexity is 81.66029058758129
At time: 162.95070958137512 and batch: 550, loss is 4.3279357481002805 and perplexity is 75.78768012454957
At time: 163.46466898918152 and batch: 600, loss is 4.263869791030884 and perplexity is 71.08453417748527
At time: 163.979665517807 and batch: 650, loss is 4.238889122009278 and perplexity is 69.33079099428697
At time: 164.4931607246399 and batch: 700, loss is 4.326443753242493 and perplexity is 75.67468960711577
At time: 165.0198633670807 and batch: 750, loss is 4.260825762748718 and perplexity is 70.86847984959857
At time: 165.5331916809082 and batch: 800, loss is 4.355091228485107 and perplexity is 77.87392933338214
At time: 166.04457473754883 and batch: 850, loss is 4.270573558807373 and perplexity is 71.56266924972157
At time: 166.55772352218628 and batch: 900, loss is 4.252166686058044 and perplexity is 70.2574734410515
At time: 167.0707700252533 and batch: 950, loss is 4.212958393096923 and perplexity is 67.55610197864556
At time: 167.58525824546814 and batch: 1000, loss is 4.1375249576568605 and perplexity is 62.647574008093144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.817347084603658 and perplexity of 123.63665772706123
Finished 15 epochs...
Completing Train Step...
At time: 169.07864904403687 and batch: 50, loss is 4.438201169967652 and perplexity is 84.6225830374833
At time: 169.60215544700623 and batch: 100, loss is 4.341191120147705 and perplexity is 76.79896167056438
At time: 170.11317467689514 and batch: 150, loss is 4.362054605484008 and perplexity is 78.41808725129388
At time: 170.62408113479614 and batch: 200, loss is 4.39840103149414 and perplexity is 81.32073535845325
At time: 171.13462328910828 and batch: 250, loss is 4.408174138069153 and perplexity is 82.11938787310211
At time: 171.64583945274353 and batch: 300, loss is 4.301154508590698 and perplexity is 73.78492988032423
At time: 172.15643453598022 and batch: 350, loss is 4.310661277770996 and perplexity is 74.48973106050909
At time: 172.66746163368225 and batch: 400, loss is 4.304868564605713 and perplexity is 74.05948077606557
At time: 173.1779134273529 and batch: 450, loss is 4.352303981781006 and perplexity is 77.65717769098669
At time: 173.68842673301697 and batch: 500, loss is 4.3942305374145505 and perplexity is 80.98229393760697
At time: 174.1992528438568 and batch: 550, loss is 4.320114979743957 and perplexity is 75.19727395828133
At time: 174.71007752418518 and batch: 600, loss is 4.258438196182251 and perplexity is 70.69947846780008
At time: 175.22084975242615 and batch: 650, loss is 4.233445038795471 and perplexity is 68.95437395137564
At time: 175.73094940185547 and batch: 700, loss is 4.320897564888001 and perplexity is 75.25614526065237
At time: 176.2415735721588 and batch: 750, loss is 4.257556447982788 and perplexity is 70.63716680559327
At time: 176.75290536880493 and batch: 800, loss is 4.352786021232605 and perplexity is 77.6946205380733
At time: 177.26130509376526 and batch: 850, loss is 4.268703212738037 and perplexity is 71.4289473846436
At time: 177.78351211547852 and batch: 900, loss is 4.251913585662842 and perplexity is 70.23969349690802
At time: 178.29450273513794 and batch: 950, loss is 4.213332977294922 and perplexity is 67.58141216702748
At time: 178.80470538139343 and batch: 1000, loss is 4.138697504997253 and perplexity is 62.721074337265044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.815661546660633 and perplexity of 123.42843897891011
Finished 16 epochs...
Completing Train Step...
At time: 180.2910873889923 and batch: 50, loss is 4.4306103515625 and perplexity is 83.9826602183486
At time: 180.80222272872925 and batch: 100, loss is 4.332812814712525 and perplexity is 76.15820449047757
At time: 181.31282997131348 and batch: 150, loss is 4.353812465667724 and perplexity is 77.77441069203758
At time: 181.82395386695862 and batch: 200, loss is 4.3910010719299315 and perplexity is 80.72118626051969
At time: 182.33493614196777 and batch: 250, loss is 4.402223243713379 and perplexity is 81.63215524412833
At time: 182.84626960754395 and batch: 300, loss is 4.294841351509095 and perplexity is 73.32058132268064
At time: 183.3579397201538 and batch: 350, loss is 4.303269920349121 and perplexity is 73.94118059762172
At time: 183.86941051483154 and batch: 400, loss is 4.298875522613526 and perplexity is 73.61696652556243
At time: 184.38039326667786 and batch: 450, loss is 4.34607479095459 and perplexity is 77.17493984723278
At time: 184.8907630443573 and batch: 500, loss is 4.388102893829346 and perplexity is 80.48758056521427
At time: 185.4015407562256 and batch: 550, loss is 4.3147253942489625 and perplexity is 74.79308201321588
At time: 185.91296005249023 and batch: 600, loss is 4.254354181289673 and perplexity is 70.41132954758795
At time: 186.4239857196808 and batch: 650, loss is 4.229264421463013 and perplexity is 68.6667038387959
At time: 186.93503189086914 and batch: 700, loss is 4.3170301055908205 and perplexity is 74.96565726931036
At time: 187.445716381073 and batch: 750, loss is 4.255325350761414 and perplexity is 70.47974409699293
At time: 187.9572958946228 and batch: 800, loss is 4.350940456390381 and perplexity is 77.55136231481117
At time: 188.46747732162476 and batch: 850, loss is 4.266962084770203 and perplexity is 71.30468865320663
At time: 188.97600483894348 and batch: 900, loss is 4.2509439134597775 and perplexity is 70.17161702983536
At time: 189.48711347579956 and batch: 950, loss is 4.212830853462219 and perplexity is 67.54748644749971
At time: 189.99803233146667 and batch: 1000, loss is 4.138510370254517 and perplexity is 62.70933814331386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.815181080887958 and perplexity of 123.3691500828892
Finished 17 epochs...
Completing Train Step...
At time: 191.47574400901794 and batch: 50, loss is 4.424397192001343 and perplexity is 83.46248020414589
At time: 192.0038356781006 and batch: 100, loss is 4.326013307571412 and perplexity is 75.64212277419294
At time: 192.5180583000183 and batch: 150, loss is 4.347198133468628 and perplexity is 77.2616824498865
At time: 193.03239631652832 and batch: 200, loss is 4.385090627670288 and perplexity is 80.24549534566215
At time: 193.546719789505 and batch: 250, loss is 4.397544522285461 and perplexity is 81.25111322001979
At time: 194.06131601333618 and batch: 300, loss is 4.290263638496399 and perplexity is 72.98570780567503
At time: 194.57533359527588 and batch: 350, loss is 4.297625494003296 and perplexity is 73.52500070313702
At time: 195.09052515029907 and batch: 400, loss is 4.293971757888794 and perplexity is 73.25684992714932
At time: 195.60428619384766 and batch: 450, loss is 4.340669813156128 and perplexity is 76.75893626856706
At time: 196.1178638935089 and batch: 500, loss is 4.38271424293518 and perplexity is 80.05502757744422
At time: 196.63296031951904 and batch: 550, loss is 4.310027384757996 and perplexity is 74.44252750303284
At time: 197.1485674381256 and batch: 600, loss is 4.250804100036621 and perplexity is 70.16180678166818
At time: 197.66326713562012 and batch: 650, loss is 4.225384359359741 and perplexity is 68.4007889801669
At time: 198.17795944213867 and batch: 700, loss is 4.3132164096832275 and perplexity is 74.68030551722975
At time: 198.6935577392578 and batch: 750, loss is 4.2528256940841676 and perplexity is 70.30378893941068
At time: 199.20933175086975 and batch: 800, loss is 4.349003329277038 and perplexity is 77.40128087853542
At time: 199.72450470924377 and batch: 850, loss is 4.265210824012756 and perplexity is 71.17992482902761
At time: 200.2363760471344 and batch: 900, loss is 4.24931640625 and perplexity is 70.05750510137882
At time: 200.75072050094604 and batch: 950, loss is 4.211435723304748 and perplexity is 67.45331461834942
At time: 201.26508355140686 and batch: 1000, loss is 4.137544765472412 and perplexity is 62.64881493197379
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.814387996022294 and perplexity of 123.27134666540017
Finished 18 epochs...
Completing Train Step...
At time: 202.74401187896729 and batch: 50, loss is 4.419079580307007 and perplexity is 83.01983708852896
At time: 203.26822113990784 and batch: 100, loss is 4.31981780052185 and perplexity is 75.17493021111497
At time: 203.77957010269165 and batch: 150, loss is 4.341014461517334 and perplexity is 76.78539566949259
At time: 204.3048357963562 and batch: 200, loss is 4.38000521659851 and perplexity is 79.83844988913697
At time: 204.8161323070526 and batch: 250, loss is 4.3931416273117065 and perplexity is 80.89415949353871
At time: 205.32771182060242 and batch: 300, loss is 4.286301345825195 and perplexity is 72.6970892437438
At time: 205.8386607170105 and batch: 350, loss is 4.292672748565674 and perplexity is 73.16175037708538
At time: 206.35061383247375 and batch: 400, loss is 4.289596347808838 and perplexity is 72.93702136834095
At time: 206.8628273010254 and batch: 450, loss is 4.335936546325684 and perplexity is 76.39647423306258
At time: 207.37375950813293 and batch: 500, loss is 4.378372268676758 and perplexity is 79.70818424576667
At time: 207.88415670394897 and batch: 550, loss is 4.30579053401947 and perplexity is 74.12779283811682
At time: 208.39513063430786 and batch: 600, loss is 4.247305960655212 and perplexity is 69.91679978644916
At time: 208.90598034858704 and batch: 650, loss is 4.2219704818725585 and perplexity is 68.1676752039015
At time: 209.41770815849304 and batch: 700, loss is 4.309632754325866 and perplexity is 74.41315601205875
At time: 209.9295835494995 and batch: 750, loss is 4.250620574951172 and perplexity is 70.14893151158694
At time: 210.4414324760437 and batch: 800, loss is 4.346940641403198 and perplexity is 77.24179074078243
At time: 210.95232820510864 and batch: 850, loss is 4.262954297065735 and perplexity is 71.01948649535379
At time: 211.46050381660461 and batch: 900, loss is 4.247757492065429 and perplexity is 69.94837654606752
At time: 211.97158575057983 and batch: 950, loss is 4.210155696868896 and perplexity is 67.36702782891936
At time: 212.4826681613922 and batch: 1000, loss is 4.136161751747132 and perplexity is 62.56223064847757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.813984940691692 and perplexity of 123.22167150360809
Finished 19 epochs...
Completing Train Step...
At time: 213.97424364089966 and batch: 50, loss is 4.414436960220337 and perplexity is 82.63530084380504
At time: 214.48638534545898 and batch: 100, loss is 4.314383325576782 and perplexity is 74.76750201827019
At time: 214.99684858322144 and batch: 150, loss is 4.335515451431275 and perplexity is 76.36431084020772
At time: 215.50736260414124 and batch: 200, loss is 4.37514370918274 and perplexity is 79.45125660702851
At time: 216.018413066864 and batch: 250, loss is 4.389137043952942 and perplexity is 80.57085986082862
At time: 216.53066658973694 and batch: 300, loss is 4.2827083778381345 and perplexity is 72.43635960644754
At time: 217.0650863647461 and batch: 350, loss is 4.2880589962005615 and perplexity is 72.8249776685859
At time: 217.57473373413086 and batch: 400, loss is 4.285319509506226 and perplexity is 72.62574762986586
At time: 218.0859637260437 and batch: 450, loss is 4.331544103622437 and perplexity is 76.06164299910915
At time: 218.5974428653717 and batch: 500, loss is 4.3739586925506595 and perplexity is 79.35716130976687
At time: 219.10833525657654 and batch: 550, loss is 4.301863403320312 and perplexity is 73.83725417225753
At time: 219.61812162399292 and batch: 600, loss is 4.2441463279724125 and perplexity is 69.69623701293598
At time: 220.12962174415588 and batch: 650, loss is 4.21868528842926 and perplexity is 67.94409865146784
At time: 220.63983583450317 and batch: 700, loss is 4.30603720664978 and perplexity is 74.14608039118191
At time: 221.1515588760376 and batch: 750, loss is 4.248274483680725 and perplexity is 69.98454861976913
At time: 221.6628348827362 and batch: 800, loss is 4.344395699501038 and perplexity is 77.04546479595257
At time: 222.17423629760742 and batch: 850, loss is 4.2603534936904905 and perplexity is 70.83501876130224
At time: 222.68287587165833 and batch: 900, loss is 4.245549693107605 and perplexity is 69.79411494518916
At time: 223.1926715373993 and batch: 950, loss is 4.208501076698303 and perplexity is 67.25565315264346
At time: 223.70440125465393 and batch: 1000, loss is 4.134110007286072 and perplexity is 62.4340005309734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.813737078410823 and perplexity of 123.1911332838495
Finished 20 epochs...
Completing Train Step...
At time: 225.1822395324707 and batch: 50, loss is 4.410152349472046 and perplexity is 82.28199816833254
At time: 225.70735931396484 and batch: 100, loss is 4.309176187515259 and perplexity is 74.37918918940309
At time: 226.22744226455688 and batch: 150, loss is 4.330598888397216 and perplexity is 75.9897823433343
At time: 226.73821759223938 and batch: 200, loss is 4.370717239379883 and perplexity is 79.10034524110975
At time: 227.24880266189575 and batch: 250, loss is 4.3853288269042965 and perplexity is 80.26461203788645
At time: 227.7600803375244 and batch: 300, loss is 4.27935788154602 and perplexity is 72.19406797743444
At time: 228.27502036094666 and batch: 350, loss is 4.2837913227081295 and perplexity is 72.51484668140144
At time: 228.78641843795776 and batch: 400, loss is 4.281470317840576 and perplexity is 72.34673453930327
At time: 229.294771194458 and batch: 450, loss is 4.327774085998535 and perplexity is 75.77542911918256
At time: 229.80831122398376 and batch: 500, loss is 4.369943046569825 and perplexity is 79.03913002179985
At time: 230.33013319969177 and batch: 550, loss is 4.2984217929840085 and perplexity is 73.58357190325233
At time: 230.83752584457397 and batch: 600, loss is 4.240845975875854 and perplexity is 69.46659405074939
At time: 231.34571170806885 and batch: 650, loss is 4.215804815292358 and perplexity is 67.74866910040258
At time: 231.85214257240295 and batch: 700, loss is 4.303427996635437 and perplexity is 73.9528698687304
At time: 232.3589470386505 and batch: 750, loss is 4.246082549095154 and perplexity is 69.83131506750715
At time: 232.8663866519928 and batch: 800, loss is 4.342285962104797 and perplexity is 76.8830904415129
At time: 233.37447786331177 and batch: 850, loss is 4.257894668579102 and perplexity is 70.66106179093248
At time: 233.88259935379028 and batch: 900, loss is 4.2434822463989255 and perplexity is 69.64996839096433
At time: 234.39009046554565 and batch: 950, loss is 4.206440029144287 and perplexity is 67.11717880334787
At time: 234.90045595169067 and batch: 1000, loss is 4.132369494438171 and perplexity is 62.32542786437299
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.813523083198361 and perplexity of 123.1647737916121
Finished 21 epochs...
Completing Train Step...
At time: 236.39742159843445 and batch: 50, loss is 4.4061816120147705 and perplexity is 81.95592575853698
At time: 236.92546391487122 and batch: 100, loss is 4.30472092628479 and perplexity is 74.0485475657756
At time: 237.44042944908142 and batch: 150, loss is 4.326128635406494 and perplexity is 75.65084691951228
At time: 237.95522499084473 and batch: 200, loss is 4.366408367156982 and perplexity is 78.76024521066758
At time: 238.46891450881958 and batch: 250, loss is 4.381908936500549 and perplexity is 79.99058470022607
At time: 238.98266625404358 and batch: 300, loss is 4.276126761436462 and perplexity is 71.96117672502426
At time: 239.49645566940308 and batch: 350, loss is 4.279696044921875 and perplexity is 72.21848549550083
At time: 240.01072931289673 and batch: 400, loss is 4.277528581619262 and perplexity is 72.0621240934337
At time: 240.52547669410706 and batch: 450, loss is 4.324198718070984 and perplexity is 75.50498783185918
At time: 241.03939414024353 and batch: 500, loss is 4.3662166500091555 and perplexity is 78.74514696843589
At time: 241.55497980117798 and batch: 550, loss is 4.295061240196228 and perplexity is 73.33670546174045
At time: 242.06895852088928 and batch: 600, loss is 4.238136997222901 and perplexity is 69.27866519293191
At time: 242.58332633972168 and batch: 650, loss is 4.213120088577271 and perplexity is 67.56702637819461
At time: 243.12215089797974 and batch: 700, loss is 4.300124096870422 and perplexity is 73.7089401808578
At time: 243.63656520843506 and batch: 750, loss is 4.243823509216309 and perplexity is 69.67374139159799
At time: 244.149578332901 and batch: 800, loss is 4.339995074272156 and perplexity is 76.70716149876377
At time: 244.66327571868896 and batch: 850, loss is 4.255870003700256 and perplexity is 70.51814155243846
At time: 245.17764019966125 and batch: 900, loss is 4.241224446296692 and perplexity is 69.49289007765424
At time: 245.69057393074036 and batch: 950, loss is 4.204409070014954 and perplexity is 66.98100488496097
At time: 246.2037317752838 and batch: 1000, loss is 4.130428671836853 and perplexity is 62.20458257290673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.813560671922637 and perplexity of 123.16940348534631
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 247.70641088485718 and batch: 50, loss is 4.401306800842285 and perplexity is 81.55737830703185
At time: 248.21554446220398 and batch: 100, loss is 4.299467067718506 and perplexity is 73.66052716452528
At time: 248.7385070323944 and batch: 150, loss is 4.322161560058594 and perplexity is 75.35132880791853
At time: 249.25394368171692 and batch: 200, loss is 4.3607094287872314 and perplexity is 78.31267198469344
At time: 249.76458430290222 and batch: 250, loss is 4.375153241157531 and perplexity is 79.45201393801308
At time: 250.28749895095825 and batch: 300, loss is 4.266416110992432 and perplexity is 71.26576878854941
At time: 250.80897307395935 and batch: 350, loss is 4.269459018707275 and perplexity is 71.48295421622522
At time: 251.32322430610657 and batch: 400, loss is 4.265917510986328 and perplexity is 71.23024453272949
At time: 251.83311343193054 and batch: 450, loss is 4.3141537857055665 and perplexity is 74.75034186503086
At time: 252.34606289863586 and batch: 500, loss is 4.3525341129302975 and perplexity is 77.6750510830727
At time: 252.85655236244202 and batch: 550, loss is 4.280704627037048 and perplexity is 72.29136051239826
At time: 253.39175868034363 and batch: 600, loss is 4.222252707481385 and perplexity is 68.18691658261538
At time: 253.92005133628845 and batch: 650, loss is 4.194618954658508 and perplexity is 66.32845261350828
At time: 254.4449439048767 and batch: 700, loss is 4.280922307968139 and perplexity is 72.30709867595134
At time: 254.95584273338318 and batch: 750, loss is 4.22428412437439 and perplexity is 68.32557342408587
At time: 255.46705627441406 and batch: 800, loss is 4.317738885879517 and perplexity is 75.01881028419142
At time: 255.97740840911865 and batch: 850, loss is 4.235360231399536 and perplexity is 69.0865614001925
At time: 256.5113980770111 and batch: 900, loss is 4.2135201263427735 and perplexity is 67.59406114755244
At time: 257.0195815563202 and batch: 950, loss is 4.177421021461487 and perplexity is 65.1974932730719
At time: 257.5298390388489 and batch: 1000, loss is 4.099830355644226 and perplexity is 60.33005207636902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.80682373046875 and perplexity of 122.34240726557427
Finished 23 epochs...
Completing Train Step...
At time: 259.00891757011414 and batch: 50, loss is 4.395596647262574 and perplexity is 81.09300024814398
At time: 259.53495693206787 and batch: 100, loss is 4.295494909286499 and perplexity is 73.36851622125455
At time: 260.04664182662964 and batch: 150, loss is 4.318469724655151 and perplexity is 75.07365697925356
At time: 260.55735754966736 and batch: 200, loss is 4.357429828643799 and perplexity is 78.05625843121042
At time: 261.06798005104065 and batch: 250, loss is 4.371925129890442 and perplexity is 79.19594752443017
At time: 261.57943415641785 and batch: 300, loss is 4.262773656845093 and perplexity is 71.0066586782881
At time: 262.09010100364685 and batch: 350, loss is 4.2659085083007815 and perplexity is 71.2296032721231
At time: 262.6014301776886 and batch: 400, loss is 4.263314189910889 and perplexity is 71.04505050028527
At time: 263.1117117404938 and batch: 450, loss is 4.311751532554626 and perplexity is 74.57098813353655
At time: 263.6211054325104 and batch: 500, loss is 4.35022608757019 and perplexity is 77.49598182301361
At time: 264.1323356628418 and batch: 550, loss is 4.27861490726471 and perplexity is 72.14044956268211
At time: 264.64304089546204 and batch: 600, loss is 4.220269684791565 and perplexity is 68.05183435970592
At time: 265.15538787841797 and batch: 650, loss is 4.1932745885849 and perplexity is 66.23934280362319
At time: 265.66766142845154 and batch: 700, loss is 4.280075879096985 and perplexity is 72.24592175465128
At time: 266.1791582107544 and batch: 750, loss is 4.223627886772156 and perplexity is 68.28075032252171
At time: 266.69115591049194 and batch: 800, loss is 4.317025060653687 and perplexity is 74.96527907323623
At time: 267.2021143436432 and batch: 850, loss is 4.23507285118103 and perplexity is 69.06671014164763
At time: 267.7126684188843 and batch: 900, loss is 4.213727631568909 and perplexity is 67.60808872384361
At time: 268.22176909446716 and batch: 950, loss is 4.178281660079956 and perplexity is 65.2536289063921
At time: 268.73002648353577 and batch: 1000, loss is 4.101095671653748 and perplexity is 60.4064369724413
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.806010176495808 and perplexity of 122.24291559049537
Finished 24 epochs...
Completing Train Step...
At time: 270.23618841171265 and batch: 50, loss is 4.393035516738892 and perplexity is 80.88557622333342
At time: 270.7630331516266 and batch: 100, loss is 4.2934804534912105 and perplexity is 73.22086735454884
At time: 271.27922916412354 and batch: 150, loss is 4.316778650283814 and perplexity is 74.94680912677879
At time: 271.79442620277405 and batch: 200, loss is 4.355672369003296 and perplexity is 77.91919818153522
At time: 272.3085572719574 and batch: 250, loss is 4.370265636444092 and perplexity is 79.06463135783103
At time: 272.8232400417328 and batch: 300, loss is 4.260694198608398 and perplexity is 70.8591567122801
At time: 273.336874961853 and batch: 350, loss is 4.263953094482422 and perplexity is 71.09045601118437
At time: 273.8505792617798 and batch: 400, loss is 4.26176305770874 and perplexity is 70.93493565806796
At time: 274.36493825912476 and batch: 450, loss is 4.310471167564392 and perplexity is 74.47557114836188
At time: 274.8795676231384 and batch: 500, loss is 4.348892984390258 and perplexity is 77.39274051416169
At time: 275.3927071094513 and batch: 550, loss is 4.27736834526062 and perplexity is 72.05057804614594
At time: 275.9077227115631 and batch: 600, loss is 4.219099035263062 and perplexity is 67.97221612351682
At time: 276.4232847690582 and batch: 650, loss is 4.1923910427093505 and perplexity is 66.18084315285819
At time: 276.9379427433014 and batch: 700, loss is 4.279586753845215 and perplexity is 72.21059309075885
At time: 277.45178294181824 and batch: 750, loss is 4.223325109481811 and perplexity is 68.26007959142827
At time: 277.96616888046265 and batch: 800, loss is 4.316568431854248 and perplexity is 74.93105558216389
At time: 278.4795880317688 and batch: 850, loss is 4.235039396286011 and perplexity is 69.06439956076083
At time: 278.9930124282837 and batch: 900, loss is 4.213807735443115 and perplexity is 67.61350461059196
At time: 279.5083386898041 and batch: 950, loss is 4.178907132148742 and perplexity is 65.2944559954309
At time: 280.02017307281494 and batch: 1000, loss is 4.10180805683136 and perplexity is 60.44948495432442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.805552040658346 and perplexity of 122.18692455671639
Finished 25 epochs...
Completing Train Step...
At time: 281.51123213768005 and batch: 50, loss is 4.391058387756348 and perplexity is 80.7258129946108
At time: 282.0241901874542 and batch: 100, loss is 4.291806268692016 and perplexity is 73.09838464939045
At time: 282.5580048561096 and batch: 150, loss is 4.315503444671631 and perplexity is 74.85129744662342
At time: 283.06917214393616 and batch: 200, loss is 4.354316463470459 and perplexity is 77.81361870368126
At time: 283.58097672462463 and batch: 250, loss is 4.369022598266602 and perplexity is 78.96641206037921
At time: 284.09280157089233 and batch: 300, loss is 4.259161005020141 and perplexity is 70.75059914868514
At time: 284.6065092086792 and batch: 350, loss is 4.262484941482544 and perplexity is 70.98616092423504
At time: 285.11820936203003 and batch: 400, loss is 4.260594034194947 and perplexity is 70.85205950186024
At time: 285.630003452301 and batch: 450, loss is 4.3095553636550905 and perplexity is 74.40739735083663
At time: 286.141321182251 and batch: 500, loss is 4.347864503860474 and perplexity is 77.31318450531194
At time: 286.6525721549988 and batch: 550, loss is 4.276404685974121 and perplexity is 71.98117928126683
At time: 287.16335701942444 and batch: 600, loss is 4.218160963058471 and perplexity is 67.90848317461452
At time: 287.6742036342621 and batch: 650, loss is 4.191732950210572 and perplexity is 66.13730436426286
At time: 288.1880714893341 and batch: 700, loss is 4.279138550758362 and perplexity is 72.17823533199747
At time: 288.70657539367676 and batch: 750, loss is 4.223137989044189 and perplexity is 68.24730793041967
At time: 289.2201108932495 and batch: 800, loss is 4.316201062202453 and perplexity is 74.90353324211186
At time: 289.7315435409546 and batch: 850, loss is 4.235072793960572 and perplexity is 69.06670618961891
At time: 290.24367666244507 and batch: 900, loss is 4.213840732574463 and perplexity is 67.615735699094
At time: 290.7554759979248 and batch: 950, loss is 4.1794083166122435 and perplexity is 65.32718876422098
At time: 291.26424765586853 and batch: 1000, loss is 4.102206072807312 and perplexity is 60.473549603814085
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.80530268971513 and perplexity of 122.15646093005438
Finished 26 epochs...
Completing Train Step...
At time: 292.7432863712311 and batch: 50, loss is 4.389530019760132 and perplexity is 80.60252848161181
At time: 293.26649022102356 and batch: 100, loss is 4.290515460968018 and perplexity is 73.00408956137991
At time: 293.7772374153137 and batch: 150, loss is 4.314537935256958 and perplexity is 74.77906269151777
At time: 294.2878427505493 and batch: 200, loss is 4.353255052566528 and perplexity is 77.7310702969542
At time: 294.79921889305115 and batch: 250, loss is 4.367980909347534 and perplexity is 78.88419645293548
At time: 295.32390236854553 and batch: 300, loss is 4.257955574989319 and perplexity is 70.66536563361281
At time: 295.83441138267517 and batch: 350, loss is 4.261291456222534 and perplexity is 70.90149052399504
At time: 296.34438729286194 and batch: 400, loss is 4.259623861312866 and perplexity is 70.78335408854538
At time: 296.8565273284912 and batch: 450, loss is 4.308796124458313 and perplexity is 74.35092577866595
At time: 297.3676788806915 and batch: 500, loss is 4.347035894393921 and perplexity is 77.24914860278288
At time: 297.8791000843048 and batch: 550, loss is 4.2756157922744755 and perplexity is 71.92441617541094
At time: 298.3903090953827 and batch: 600, loss is 4.217416710853577 and perplexity is 67.85796093925538
At time: 298.90160632133484 and batch: 650, loss is 4.191166110038758 and perplexity is 66.09982570650162
At time: 299.4124994277954 and batch: 700, loss is 4.2788099098205565 and perplexity is 72.15451850642218
At time: 299.9229121208191 and batch: 750, loss is 4.222967195510864 and perplexity is 68.23565272690333
At time: 300.43308687210083 and batch: 800, loss is 4.315938200950622 and perplexity is 74.88384659313924
At time: 300.9441726207733 and batch: 850, loss is 4.235043978691101 and perplexity is 69.06471604254205
At time: 301.4544589519501 and batch: 900, loss is 4.213793530464172 and perplexity is 67.61254416900422
At time: 301.96528911590576 and batch: 950, loss is 4.17975501537323 and perplexity is 65.34984154624308
At time: 302.47497630119324 and batch: 1000, loss is 4.102446298599244 and perplexity is 60.48807865521515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.805126655392531 and perplexity of 122.1349590927897
Finished 27 epochs...
Completing Train Step...
At time: 303.9487154483795 and batch: 50, loss is 4.388120470046997 and perplexity is 80.48899524488083
At time: 304.4733347892761 and batch: 100, loss is 4.289394817352295 and perplexity is 72.92232381817742
At time: 304.9836542606354 and batch: 150, loss is 4.313715438842774 and perplexity is 74.71758246769572
At time: 305.49367904663086 and batch: 200, loss is 4.352337245941162 and perplexity is 77.65976093474714
At time: 306.0057945251465 and batch: 250, loss is 4.367065472602844 and perplexity is 78.81201600428533
At time: 306.5169243812561 and batch: 300, loss is 4.256907501220703 and perplexity is 70.59134191547716
At time: 307.0280249118805 and batch: 350, loss is 4.260227613449096 and perplexity is 70.82610259323788
At time: 307.53840351104736 and batch: 400, loss is 4.258759393692016 and perplexity is 70.72219061157239
At time: 308.04904675483704 and batch: 450, loss is 4.308121690750122 and perplexity is 74.30079791394654
At time: 308.57337975502014 and batch: 500, loss is 4.346341848373413 and perplexity is 77.19555273975668
At time: 309.0844395160675 and batch: 550, loss is 4.274978351593018 and perplexity is 71.87858323599265
At time: 309.59554958343506 and batch: 600, loss is 4.2167495918273925 and perplexity is 67.81270669909539
At time: 310.1070787906647 and batch: 650, loss is 4.190624523162842 and perplexity is 66.06403660072844
At time: 310.6207387447357 and batch: 700, loss is 4.278506269454956 and perplexity is 72.13261280793863
At time: 311.1319947242737 and batch: 750, loss is 4.222796750068665 and perplexity is 68.22402326202332
At time: 311.6421790122986 and batch: 800, loss is 4.315653200149536 and perplexity is 74.86250767782019
At time: 312.1526572704315 and batch: 850, loss is 4.2349639940261845 and perplexity is 69.05919214528832
At time: 312.66365814208984 and batch: 900, loss is 4.21368968963623 and perplexity is 67.6055235909561
At time: 313.17537927627563 and batch: 950, loss is 4.179993605613708 and perplexity is 65.36543524083051
At time: 313.6860840320587 and batch: 1000, loss is 4.102544937133789 and perplexity is 60.49404540492206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.80499751393388 and perplexity of 122.1191874244302
Finished 28 epochs...
Completing Train Step...
At time: 315.1831958293915 and batch: 50, loss is 4.386892070770264 and perplexity is 80.39018332400843
At time: 315.6982944011688 and batch: 100, loss is 4.28841417312622 and perplexity is 72.85084801426837
At time: 316.2126348018646 and batch: 150, loss is 4.312990922927856 and perplexity is 74.6634679958389
At time: 316.7264618873596 and batch: 200, loss is 4.351475315093994 and perplexity is 77.59285243060798
At time: 317.24102783203125 and batch: 250, loss is 4.366244440078735 and perplexity is 78.74733533195646
At time: 317.75519704818726 and batch: 300, loss is 4.255965895652771 and perplexity is 70.5249039989466
At time: 318.26970958709717 and batch: 350, loss is 4.259271793365478 and perplexity is 70.75843792471092
At time: 318.7837507724762 and batch: 400, loss is 4.257998914718628 and perplexity is 70.66842831779832
At time: 319.29983830451965 and batch: 450, loss is 4.30753014087677 and perplexity is 74.2568582838732
At time: 319.81551790237427 and batch: 500, loss is 4.345727758407593 and perplexity is 77.14816227790607
At time: 320.33036398887634 and batch: 550, loss is 4.274365329742432 and perplexity is 71.83453359695493
At time: 320.8436427116394 and batch: 600, loss is 4.216123580932617 and perplexity is 67.7702684906739
At time: 321.3705587387085 and batch: 650, loss is 4.190088720321655 and perplexity is 66.02864878351062
At time: 321.8854215145111 and batch: 700, loss is 4.278218240737915 and perplexity is 72.11183953580777
At time: 322.3988559246063 and batch: 750, loss is 4.222602167129517 and perplexity is 68.21074932253946
At time: 322.9129285812378 and batch: 800, loss is 4.315362024307251 and perplexity is 74.8407126973314
At time: 323.4262113571167 and batch: 850, loss is 4.234829716682434 and perplexity is 69.04991968296022
At time: 323.9388539791107 and batch: 900, loss is 4.213556289672852 and perplexity is 67.59650561809686
At time: 324.4538073539734 and batch: 950, loss is 4.180149211883545 and perplexity is 65.37560730378243
At time: 324.9705243110657 and batch: 1000, loss is 4.102534837722779 and perplexity is 60.49343445377895
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8048899580792686 and perplexity of 122.10605349719039
Finished 29 epochs...
Completing Train Step...
At time: 326.438841342926 and batch: 50, loss is 4.385755462646484 and perplexity is 80.29886309605793
At time: 326.9602851867676 and batch: 100, loss is 4.287541942596436 and perplexity is 72.78733298441045
At time: 327.46823477745056 and batch: 150, loss is 4.312342023849487 and perplexity is 74.6150346561428
At time: 327.9767346382141 and batch: 200, loss is 4.3506747913360595 and perplexity is 77.53076236439372
At time: 328.48441433906555 and batch: 250, loss is 4.365493421554565 and perplexity is 78.68821682671813
At time: 328.9923429489136 and batch: 300, loss is 4.255114245414734 and perplexity is 70.46486701655228
At time: 329.5004668235779 and batch: 350, loss is 4.258410549163818 and perplexity is 70.69752386503528
At time: 330.00784516334534 and batch: 400, loss is 4.257331991195679 and perplexity is 70.62131359332872
At time: 330.5159115791321 and batch: 450, loss is 4.307007789611816 and perplexity is 74.21808024877673
At time: 331.02362418174744 and batch: 500, loss is 4.345147199630738 and perplexity is 77.10338623399765
At time: 331.53700137138367 and batch: 550, loss is 4.273800163269043 and perplexity is 71.7939465972205
At time: 332.0495204925537 and batch: 600, loss is 4.215565161705017 and perplexity is 67.73243483419284
At time: 332.55727982521057 and batch: 650, loss is 4.189553451538086 and perplexity is 65.99331516633035
At time: 333.06467270851135 and batch: 700, loss is 4.2779237651824955 and perplexity is 72.09060748812111
At time: 333.574588060379 and batch: 750, loss is 4.222398285865784 and perplexity is 68.1968438463486
At time: 334.0843117237091 and batch: 800, loss is 4.315080628395081 and perplexity is 74.81965578951733
At time: 334.6053819656372 and batch: 850, loss is 4.234674320220948 and perplexity is 69.03919040344321
At time: 335.11455821990967 and batch: 900, loss is 4.213424482345581 and perplexity is 67.58759649051557
At time: 335.62358713150024 and batch: 950, loss is 4.18025161743164 and perplexity is 65.38230247148577
At time: 336.1335153579712 and batch: 1000, loss is 4.102482538223267 and perplexity is 60.4902707601638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.804808453815739 and perplexity of 122.0961017387885
Finished 30 epochs...
Completing Train Step...
At time: 337.5939109325409 and batch: 50, loss is 4.38471586227417 and perplexity is 80.21542774531225
At time: 338.1176061630249 and batch: 100, loss is 4.286703271865845 and perplexity is 72.72631396968606
At time: 338.6282904148102 and batch: 150, loss is 4.311756982803344 and perplexity is 74.5713945650766
At time: 339.1392984390259 and batch: 200, loss is 4.349946556091308 and perplexity is 77.47432228400972
At time: 339.64999890327454 and batch: 250, loss is 4.364790110588074 and perplexity is 78.63289399774685
At time: 340.1598410606384 and batch: 300, loss is 4.254283328056335 and perplexity is 70.4063408539605
At time: 340.6669614315033 and batch: 350, loss is 4.25760498046875 and perplexity is 70.6405950860905
At time: 341.1741235256195 and batch: 400, loss is 4.256702680587768 and perplexity is 70.57688483275126
At time: 341.6819293498993 and batch: 450, loss is 4.306510815620422 and perplexity is 74.18120495699131
At time: 342.1900074481964 and batch: 500, loss is 4.344588556289673 and perplexity is 77.06032496976864
At time: 342.6976556777954 and batch: 550, loss is 4.273288383483886 and perplexity is 71.75721330713482
At time: 343.20519185066223 and batch: 600, loss is 4.215041036605835 and perplexity is 67.69694386673342
At time: 343.71432304382324 and batch: 650, loss is 4.189061946868897 and perplexity is 65.96088711371327
At time: 344.22362327575684 and batch: 700, loss is 4.277673306465149 and perplexity is 72.07255402795454
At time: 344.7339515686035 and batch: 750, loss is 4.222166495323181 and perplexity is 68.18103829476914
At time: 345.2455039024353 and batch: 800, loss is 4.314783172607422 and perplexity is 74.79740355956345
At time: 345.7567927837372 and batch: 850, loss is 4.234502305984497 and perplexity is 69.02731570115901
At time: 346.26765036582947 and batch: 900, loss is 4.213222661018372 and perplexity is 67.57395724848021
At time: 346.77909874916077 and batch: 950, loss is 4.180278453826904 and perplexity is 65.3840571203423
At time: 347.3029901981354 and batch: 1000, loss is 4.1023854970932 and perplexity is 60.484401000739844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.804725088724276 and perplexity of 122.08592361035598
Finished 31 epochs...
Completing Train Step...
At time: 348.797226190567 and batch: 50, loss is 4.383744373321533 and perplexity is 80.13753718446232
At time: 349.31103324890137 and batch: 100, loss is 4.2858791923522945 and perplexity is 72.66640639194169
At time: 349.82642126083374 and batch: 150, loss is 4.311159076690674 and perplexity is 74.52682119910767
At time: 350.34072637557983 and batch: 200, loss is 4.349193782806396 and perplexity is 77.41602362951548
At time: 350.85616278648376 and batch: 250, loss is 4.364091591835022 and perplexity is 78.57798662582972
At time: 351.36979365348816 and batch: 300, loss is 4.253400101661682 and perplexity is 70.34418356888541
At time: 351.8847568035126 and batch: 350, loss is 4.256801643371582 and perplexity is 70.58386966336067
At time: 352.3999807834625 and batch: 400, loss is 4.256037588119507 and perplexity is 70.5299602845272
At time: 352.9152104854584 and batch: 450, loss is 4.306032276153564 and perplexity is 74.14571481511436
At time: 353.42994141578674 and batch: 500, loss is 4.344043655395508 and perplexity is 77.01834616796995
At time: 353.9450788497925 and batch: 550, loss is 4.27280098438263 and perplexity is 71.72224742772156
At time: 354.45957922935486 and batch: 600, loss is 4.214551620483398 and perplexity is 67.66381999730437
At time: 354.97382378578186 and batch: 650, loss is 4.188575563430786 and perplexity is 65.92881263153883
At time: 355.48824405670166 and batch: 700, loss is 4.277446842193603 and perplexity is 72.05623401752752
At time: 356.0036940574646 and batch: 750, loss is 4.221893124580383 and perplexity is 68.16240214109139
At time: 356.51842069625854 and batch: 800, loss is 4.314446873664856 and perplexity is 74.77225350104578
At time: 357.0328278541565 and batch: 850, loss is 4.234300489425659 and perplexity is 69.01338625148246
At time: 357.5479316711426 and batch: 900, loss is 4.212953367233276 and perplexity is 67.5557624517417
At time: 358.06263160705566 and batch: 950, loss is 4.180274105072021 and perplexity is 65.38377278172287
At time: 358.57742953300476 and batch: 1000, loss is 4.102288794517517 and perplexity is 60.47855228617187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.804667775223895 and perplexity of 122.07892663923897
Finished 32 epochs...
Completing Train Step...
At time: 360.06446862220764 and batch: 50, loss is 4.382824335098267 and perplexity is 80.06384149375882
At time: 360.5887334346771 and batch: 100, loss is 4.285078144073486 and perplexity is 72.60822040017989
At time: 361.10094809532166 and batch: 150, loss is 4.310578317642212 and perplexity is 74.48355163915353
At time: 361.61299777030945 and batch: 200, loss is 4.348507518768311 and perplexity is 77.3629140222181
At time: 362.1244742870331 and batch: 250, loss is 4.363463497161865 and perplexity is 78.52864770738059
At time: 362.63819098472595 and batch: 300, loss is 4.252601561546325 and perplexity is 70.28803333851313
At time: 363.1502139568329 and batch: 350, loss is 4.2560621356964115 and perplexity is 70.53169164540164
At time: 363.6616003513336 and batch: 400, loss is 4.25543436050415 and perplexity is 70.48742749454006
At time: 364.17357897758484 and batch: 450, loss is 4.305595183372498 and perplexity is 74.11331334016295
At time: 364.6859917640686 and batch: 500, loss is 4.343528785705566 and perplexity is 76.97870196263429
At time: 365.1979205608368 and batch: 550, loss is 4.272293910980225 and perplexity is 71.68588820286725
At time: 365.7095808982849 and batch: 600, loss is 4.214074912071228 and perplexity is 67.63157177222291
At time: 366.2207307815552 and batch: 650, loss is 4.188102164268494 and perplexity is 65.89760937324905
At time: 366.73107743263245 and batch: 700, loss is 4.277217831611633 and perplexity is 72.03973426682147
At time: 367.24241161346436 and batch: 750, loss is 4.221599659919739 and perplexity is 68.14240181971566
At time: 367.7536916732788 and batch: 800, loss is 4.3141473245620725 and perplexity is 74.74985889390612
At time: 368.265704870224 and batch: 850, loss is 4.234095497131348 and perplexity is 68.99924048903225
At time: 368.77713441848755 and batch: 900, loss is 4.212664647102356 and perplexity is 67.53626055859223
At time: 369.28816866874695 and batch: 950, loss is 4.180227932929992 and perplexity is 65.38075394257328
At time: 369.8000154495239 and batch: 1000, loss is 4.102149128913879 and perplexity is 60.47010610249407
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.804632047327553 and perplexity of 122.07456509391736
Finished 33 epochs...
Completing Train Step...
At time: 371.2822334766388 and batch: 50, loss is 4.381959657669068 and perplexity is 79.99464201904766
At time: 371.80382013320923 and batch: 100, loss is 4.284244322776795 and perplexity is 72.54770335340282
At time: 372.31538462638855 and batch: 150, loss is 4.310027732849121 and perplexity is 74.44255341582047
At time: 372.8266317844391 and batch: 200, loss is 4.347869663238526 and perplexity is 77.31358339428822
At time: 373.3377137184143 and batch: 250, loss is 4.362824969291687 and perplexity is 78.47852098257057
At time: 373.8623204231262 and batch: 300, loss is 4.251899747848511 and perplexity is 70.23872153979565
At time: 374.372763633728 and batch: 350, loss is 4.255353136062622 and perplexity is 70.48170242511799
At time: 374.8846926689148 and batch: 400, loss is 4.254823608398437 and perplexity is 70.44439029363983
At time: 375.3959994316101 and batch: 450, loss is 4.305160603523254 and perplexity is 74.08111218511797
At time: 375.9066081047058 and batch: 500, loss is 4.3430353546142575 and perplexity is 76.94072764734172
At time: 376.4139099121094 and batch: 550, loss is 4.271800222396851 and perplexity is 71.65050643277908
At time: 376.9222927093506 and batch: 600, loss is 4.213620710372925 and perplexity is 67.60086037257793
At time: 377.43004989624023 and batch: 650, loss is 4.187626180648803 and perplexity is 65.86625065432095
At time: 377.9399583339691 and batch: 700, loss is 4.277006063461304 and perplexity is 72.02448016076949
At time: 378.4504373073578 and batch: 750, loss is 4.221346573829651 and perplexity is 68.12515810783377
At time: 378.9604015350342 and batch: 800, loss is 4.313840131759644 and perplexity is 74.72689980188582
At time: 379.47418427467346 and batch: 850, loss is 4.23382339477539 and perplexity is 68.9804681872455
At time: 379.9845769405365 and batch: 900, loss is 4.212343010902405 and perplexity is 67.51454194532063
At time: 380.49467301368713 and batch: 950, loss is 4.180138282775879 and perplexity is 65.37489281063588
At time: 381.004447221756 and batch: 1000, loss is 4.101995234489441 and perplexity is 60.460800806354655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.804546449242569 and perplexity of 122.06411619212938
Finished 34 epochs...
Completing Train Step...
At time: 382.5612041950226 and batch: 50, loss is 4.381129026412964 and perplexity is 79.92822355750964
At time: 383.072078704834 and batch: 100, loss is 4.283545846939087 and perplexity is 72.49704822827918
At time: 383.58271408081055 and batch: 150, loss is 4.309534015655518 and perplexity is 74.40580891870478
At time: 384.09278559684753 and batch: 200, loss is 4.347290115356445 and perplexity is 77.26878945214693
At time: 384.60417199134827 and batch: 250, loss is 4.362201442718506 and perplexity is 78.42960279179484
At time: 385.13345432281494 and batch: 300, loss is 4.251239743232727 and perplexity is 70.19237895421506
At time: 385.6524419784546 and batch: 350, loss is 4.254684543609619 and perplexity is 70.4345946405112
At time: 386.1704070568085 and batch: 400, loss is 4.254266300201416 and perplexity is 70.40514199521009
At time: 386.70174980163574 and batch: 450, loss is 4.304691438674927 and perplexity is 74.04636408328705
At time: 387.21181893348694 and batch: 500, loss is 4.342504968643189 and perplexity is 76.89993018495099
At time: 387.73333740234375 and batch: 550, loss is 4.271355595588684 and perplexity is 71.61865577815249
At time: 388.2547378540039 and batch: 600, loss is 4.213161969184876 and perplexity is 67.56985618557
At time: 388.7812328338623 and batch: 650, loss is 4.187185683250427 and perplexity is 65.83724313160513
At time: 389.2915463447571 and batch: 700, loss is 4.276760120391845 and perplexity is 72.00676841717215
At time: 389.80526995658875 and batch: 750, loss is 4.2210688829422 and perplexity is 68.10624299861935
At time: 390.31680250167847 and batch: 800, loss is 4.313523888587952 and perplexity is 74.70327166639692
At time: 390.8319230079651 and batch: 850, loss is 4.233560223579406 and perplexity is 68.96231690348543
At time: 391.3433277606964 and batch: 900, loss is 4.2120417928695675 and perplexity is 67.49420841037538
At time: 391.8561108112335 and batch: 950, loss is 4.180032925605774 and perplexity is 65.36800545975572
At time: 392.3672606945038 and batch: 1000, loss is 4.101827955245971 and perplexity is 60.45068781520654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.804460851157584 and perplexity of 122.0536681847091
Finished 35 epochs...
Completing Train Step...
At time: 393.9116761684418 and batch: 50, loss is 4.380331821441651 and perplexity is 79.8645297722158
At time: 394.47757863998413 and batch: 100, loss is 4.282867331504821 and perplexity is 72.44787454655516
At time: 394.99655199050903 and batch: 150, loss is 4.309048757553101 and perplexity is 74.36971165601301
At time: 395.52302074432373 and batch: 200, loss is 4.346717033386231 and perplexity is 77.22452078804977
At time: 396.0348334312439 and batch: 250, loss is 4.361600852012634 and perplexity is 78.38251284360094
At time: 396.5568115711212 and batch: 300, loss is 4.250613532066345 and perplexity is 70.14843746248137
At time: 397.0744802951813 and batch: 350, loss is 4.254050397872925 and perplexity is 70.38994300192364
At time: 397.59066104888916 and batch: 400, loss is 4.253698530197144 and perplexity is 70.36517941328988
At time: 398.11813950538635 and batch: 450, loss is 4.304183468818665 and perplexity is 74.00876031396618
At time: 398.64983892440796 and batch: 500, loss is 4.341950206756592 and perplexity is 76.85728086580463
At time: 399.16490387916565 and batch: 550, loss is 4.270931572914123 and perplexity is 71.5882942816174
At time: 399.68116426467896 and batch: 600, loss is 4.21270112991333 and perplexity is 67.53872451616587
At time: 400.22829270362854 and batch: 650, loss is 4.186737504005432 and perplexity is 65.80774285688506
At time: 400.7514181137085 and batch: 700, loss is 4.276492280960083 and perplexity is 71.98748474781506
At time: 401.2650854587555 and batch: 750, loss is 4.220779814720154 and perplexity is 68.08655849326504
At time: 401.787992477417 and batch: 800, loss is 4.313200254440307 and perplexity is 74.67909904849819
At time: 402.3050000667572 and batch: 850, loss is 4.23328727722168 and perplexity is 68.943496458869
At time: 402.82254338264465 and batch: 900, loss is 4.21174153804779 and perplexity is 67.47394599095492
At time: 403.3359303474426 and batch: 950, loss is 4.179899559020996 and perplexity is 65.35928813342677
At time: 403.8486614227295 and batch: 1000, loss is 4.101645269393921 and perplexity is 60.43964533847883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.804388651033727 and perplexity of 122.04485621286547
Finished 36 epochs...
Completing Train Step...
At time: 405.3340549468994 and batch: 50, loss is 4.379578304290772 and perplexity is 79.80437314665645
At time: 405.8574023246765 and batch: 100, loss is 4.282199382781982 and perplexity is 72.39949923919036
At time: 406.36763286590576 and batch: 150, loss is 4.308560371398926 and perplexity is 74.33339938647683
At time: 406.8778336048126 and batch: 200, loss is 4.346201591491699 and perplexity is 77.18472629150688
At time: 407.38785696029663 and batch: 250, loss is 4.361037592887879 and perplexity is 78.33837560953738
At time: 407.91927576065063 and batch: 300, loss is 4.249996948242187 and perplexity is 70.10519840227447
At time: 408.4377565383911 and batch: 350, loss is 4.253438272476196 and perplexity is 70.34686871491537
At time: 408.9471230506897 and batch: 400, loss is 4.253162488937378 and perplexity is 70.32747088144093
At time: 409.4565668106079 and batch: 450, loss is 4.3037025213241575 and perplexity is 73.97317454425092
At time: 409.9670624732971 and batch: 500, loss is 4.341412029266357 and perplexity is 76.81592913556528
At time: 410.47608971595764 and batch: 550, loss is 4.270511012077332 and perplexity is 71.55819337874397
At time: 410.9849524497986 and batch: 600, loss is 4.21225830078125 and perplexity is 67.50882302252145
At time: 411.49506068229675 and batch: 650, loss is 4.186292767524719 and perplexity is 65.77848226003296
At time: 412.0051679611206 and batch: 700, loss is 4.276224617958069 and perplexity is 71.968218940027
At time: 412.5150074958801 and batch: 750, loss is 4.220493927001953 and perplexity is 68.06709616457017
At time: 413.03875637054443 and batch: 800, loss is 4.3128919649124144 and perplexity is 74.65607981278603
At time: 413.5488169193268 and batch: 850, loss is 4.233024044036865 and perplexity is 68.92535063112065
At time: 414.05917286872864 and batch: 900, loss is 4.211450629234314 and perplexity is 67.45432008019972
At time: 414.5696601867676 and batch: 950, loss is 4.179796690940857 and perplexity is 65.35256509473619
At time: 415.0789701938629 and batch: 1000, loss is 4.101471648216248 and perplexity is 60.429152646980114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8043603664491235 and perplexity of 122.04140427362307
Finished 37 epochs...
Completing Train Step...
At time: 416.56731128692627 and batch: 50, loss is 4.3788526916503905 and perplexity is 79.74648708871104
At time: 417.07710886001587 and batch: 100, loss is 4.281591320037842 and perplexity is 72.35548918280244
At time: 417.58881545066833 and batch: 150, loss is 4.308119554519653 and perplexity is 74.3006391904877
At time: 418.10003876686096 and batch: 200, loss is 4.345702028274536 and perplexity is 77.1461772709629
At time: 418.61080265045166 and batch: 250, loss is 4.360472693443298 and perplexity is 78.29413480164656
At time: 419.1217477321625 and batch: 300, loss is 4.249407405853272 and perplexity is 70.06388059662861
At time: 419.63275599479675 and batch: 350, loss is 4.252836513519287 and perplexity is 70.30454959080866
At time: 420.14395689964294 and batch: 400, loss is 4.2526398944854735 and perplexity is 70.29072773705973
At time: 420.6541910171509 and batch: 450, loss is 4.303272876739502 and perplexity is 73.94139919693997
At time: 421.16531586647034 and batch: 500, loss is 4.340942602157593 and perplexity is 76.77987811836856
At time: 421.6775863170624 and batch: 550, loss is 4.270076994895935 and perplexity is 71.52714263211405
At time: 422.18878054618835 and batch: 600, loss is 4.21183560371399 and perplexity is 67.48029327116156
At time: 422.70024824142456 and batch: 650, loss is 4.185838775634766 and perplexity is 65.74862614028437
At time: 423.2112092971802 and batch: 700, loss is 4.275956168174743 and perplexity is 71.94890168021935
At time: 423.7220754623413 and batch: 750, loss is 4.220189871788025 and perplexity is 68.04640315515255
At time: 424.2330410480499 and batch: 800, loss is 4.31258339881897 and perplexity is 74.63304703163672
At time: 424.74395179748535 and batch: 850, loss is 4.232772874832153 and perplexity is 68.9080408795474
At time: 425.2556972503662 and batch: 900, loss is 4.211141605377197 and perplexity is 67.43347830649785
At time: 425.766197681427 and batch: 950, loss is 4.179699597358703 and perplexity is 65.34622008812289
At time: 426.2999892234802 and batch: 1000, loss is 4.101269068717957 and perplexity is 60.41691217943049
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.804330221036586 and perplexity of 122.03772534059631
Finished 38 epochs...
Completing Train Step...
At time: 427.78072118759155 and batch: 50, loss is 4.37818585395813 and perplexity is 79.69332685188965
At time: 428.3064196109772 and batch: 100, loss is 4.280959692001343 and perplexity is 72.30980185745668
At time: 428.82078552246094 and batch: 150, loss is 4.307668771743774 and perplexity is 74.26715329010447
At time: 429.3355176448822 and batch: 200, loss is 4.345212421417236 and perplexity is 77.10841521859084
At time: 429.8499610424042 and batch: 250, loss is 4.359919900894165 and perplexity is 78.25086634762874
At time: 430.36472749710083 and batch: 300, loss is 4.2488391304016115 and perplexity is 70.02407632420413
At time: 430.8780131340027 and batch: 350, loss is 4.252242641448975 and perplexity is 70.2628100775831
At time: 431.3917098045349 and batch: 400, loss is 4.25213963508606 and perplexity is 70.25557293381117
At time: 431.9070951938629 and batch: 450, loss is 4.30286211013794 and perplexity is 73.91103277685964
At time: 432.4201955795288 and batch: 500, loss is 4.340500545501709 and perplexity is 76.74594456303795
At time: 432.9350166320801 and batch: 550, loss is 4.269654111862183 and perplexity is 71.49690141174233
At time: 433.44938230514526 and batch: 600, loss is 4.211416144371032 and perplexity is 67.45199396730209
At time: 433.96423172950745 and batch: 650, loss is 4.185404162406922 and perplexity is 65.72005712633663
At time: 434.4787631034851 and batch: 700, loss is 4.275681080818177 and perplexity is 71.92911216909522
At time: 434.99270844459534 and batch: 750, loss is 4.219884581565857 and perplexity is 68.02563242432741
At time: 435.50718450546265 and batch: 800, loss is 4.312262787818908 and perplexity is 74.60912269119856
At time: 436.0225145816803 and batch: 850, loss is 4.232524442672729 and perplexity is 68.89092403242572
At time: 436.5373990535736 and batch: 900, loss is 4.210819506645203 and perplexity is 67.41176156629639
At time: 437.0521740913391 and batch: 950, loss is 4.179588823318482 and perplexity is 65.3389818242238
At time: 437.5671796798706 and batch: 1000, loss is 4.101041388511658 and perplexity is 60.40315801023698
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.804295981802592 and perplexity of 122.03354693389544
Finished 39 epochs...
Completing Train Step...
At time: 439.0466215610504 and batch: 50, loss is 4.377517318725586 and perplexity is 79.64006686016462
At time: 439.5705871582031 and batch: 100, loss is 4.280339617729187 and perplexity is 72.26497830810123
At time: 440.08212876319885 and batch: 150, loss is 4.307214298248291 and perplexity is 74.23340850598494
At time: 440.5942904949188 and batch: 200, loss is 4.3447256183624265 and perplexity is 77.07088774149722
At time: 441.106481552124 and batch: 250, loss is 4.359372496604919 and perplexity is 78.2080432096069
At time: 441.6181697845459 and batch: 300, loss is 4.24826714515686 and perplexity is 69.98403503837336
At time: 442.12975573539734 and batch: 350, loss is 4.251658630371094 and perplexity is 70.22178779803356
At time: 442.64148712158203 and batch: 400, loss is 4.251632957458496 and perplexity is 70.21998502335431
At time: 443.15362572669983 and batch: 450, loss is 4.302438149452209 and perplexity is 73.87970404626482
At time: 443.6664454936981 and batch: 500, loss is 4.340043535232544 and perplexity is 76.71087889155217
At time: 444.1782283782959 and batch: 550, loss is 4.269178185462952 and perplexity is 71.4628822448492
At time: 444.68949270248413 and batch: 600, loss is 4.210952425003052 and perplexity is 67.42072242246145
At time: 445.20086193084717 and batch: 650, loss is 4.184971561431885 and perplexity is 65.69163271420179
At time: 445.7120645046234 and batch: 700, loss is 4.275386614799499 and perplexity is 71.90793460799686
At time: 446.22237372398376 and batch: 750, loss is 4.2195735263824465 and perplexity is 68.004475989337
At time: 446.73395228385925 and batch: 800, loss is 4.311932458877563 and perplexity is 74.58448120880675
At time: 447.24453616142273 and batch: 850, loss is 4.232253217697144 and perplexity is 68.87224162691898
At time: 447.7559220790863 and batch: 900, loss is 4.2104536104202275 and perplexity is 67.38710036921493
At time: 448.26805210113525 and batch: 950, loss is 4.179440565109253 and perplexity is 65.32929550184187
At time: 448.77951073646545 and batch: 1000, loss is 4.100791916847229 and perplexity is 60.38809101334396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.804230852824886 and perplexity of 122.02559927255213
Finished 40 epochs...
Completing Train Step...
At time: 450.2682394981384 and batch: 50, loss is 4.376824312210083 and perplexity is 79.58489489440937
At time: 450.77670764923096 and batch: 100, loss is 4.2797118759155275 and perplexity is 72.21962879493609
At time: 451.2869303226471 and batch: 150, loss is 4.306751041412354 and perplexity is 74.19902733631044
At time: 451.7972204685211 and batch: 200, loss is 4.344207534790039 and perplexity is 77.03096892218416
At time: 452.3219392299652 and batch: 250, loss is 4.358809170722961 and perplexity is 78.16399900147555
At time: 452.832581281662 and batch: 300, loss is 4.247697467803955 and perplexity is 69.94417807243043
At time: 453.34394693374634 and batch: 350, loss is 4.2510362720489505 and perplexity is 70.17809828067921
At time: 453.85452103614807 and batch: 400, loss is 4.251120557785034 and perplexity is 70.18401354263237
At time: 454.36642026901245 and batch: 450, loss is 4.301993842124939 and perplexity is 73.84688604359992
At time: 454.878399848938 and batch: 500, loss is 4.3396123504638675 and perplexity is 76.6778094590125
At time: 455.38966846466064 and batch: 550, loss is 4.268750476837158 and perplexity is 71.43232348927653
At time: 455.9003322124481 and batch: 600, loss is 4.210471687316894 and perplexity is 67.38831852987522
At time: 456.41135334968567 and batch: 650, loss is 4.184543190002441 and perplexity is 65.66349832200862
At time: 456.92181372642517 and batch: 700, loss is 4.275088844299316 and perplexity is 71.88652573396524
At time: 457.43268847465515 and batch: 750, loss is 4.219297761917114 and perplexity is 67.9857253568735
At time: 457.9438316822052 and batch: 800, loss is 4.311610856056213 and perplexity is 74.56049848588347
At time: 458.45489501953125 and batch: 850, loss is 4.231971759796142 and perplexity is 68.85285971807703
At time: 458.9662344455719 and batch: 900, loss is 4.2100635480880735 and perplexity is 67.3608203254486
At time: 459.47766876220703 and batch: 950, loss is 4.179314432144165 and perplexity is 65.32105584375026
At time: 459.98852705955505 and batch: 1000, loss is 4.10058566570282 and perplexity is 60.375637184816085
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.804176888814786 and perplexity of 122.01901445955363
Finished 41 epochs...
Completing Train Step...
At time: 461.461416721344 and batch: 50, loss is 4.376188268661499 and perplexity is 79.53429153012476
At time: 461.9829022884369 and batch: 100, loss is 4.27911301612854 and perplexity is 72.17639231100759
At time: 462.49226450920105 and batch: 150, loss is 4.306385908126831 and perplexity is 74.17193974726769
At time: 463.00395941734314 and batch: 200, loss is 4.343701610565185 and perplexity is 76.99200694567743
At time: 463.5146381855011 and batch: 250, loss is 4.358285756111145 and perplexity is 78.12309752742854
At time: 464.02649092674255 and batch: 300, loss is 4.24714684009552 and perplexity is 69.90567547121238
At time: 464.53750252723694 and batch: 350, loss is 4.250457124710083 and perplexity is 70.13746658881698
At time: 465.0486407279968 and batch: 400, loss is 4.25062445640564 and perplexity is 70.149203791999
At time: 465.5727150440216 and batch: 450, loss is 4.301613869667054 and perplexity is 73.81883159109992
At time: 466.08416080474854 and batch: 500, loss is 4.339197473526001 and perplexity is 76.64600420231179
At time: 466.5949397087097 and batch: 550, loss is 4.268382396697998 and perplexity is 71.40603550804475
At time: 467.10574436187744 and batch: 600, loss is 4.210035448074341 and perplexity is 67.35892751206657
At time: 467.6166582107544 and batch: 650, loss is 4.184117231369019 and perplexity is 65.63553434416936
At time: 468.14428424835205 and batch: 700, loss is 4.274791307449341 and perplexity is 71.8651400252187
At time: 468.6617338657379 and batch: 750, loss is 4.219020867347718 and perplexity is 67.96690308473842
At time: 469.1725654602051 and batch: 800, loss is 4.311286363601685 and perplexity is 74.53630809173136
At time: 469.68397092819214 and batch: 850, loss is 4.2317146301269535 and perplexity is 68.8351578809663
At time: 470.1964168548584 and batch: 900, loss is 4.209737558364868 and perplexity is 67.3388649690805
At time: 470.70894598960876 and batch: 950, loss is 4.179185996055603 and perplexity is 65.31266680157637
At time: 471.2222046852112 and batch: 1000, loss is 4.10038209438324 and perplexity is 60.36334768762091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.804148232064596 and perplexity of 122.01551784123886
Finished 42 epochs...
Completing Train Step...
At time: 472.6981339454651 and batch: 50, loss is 4.37556830406189 and perplexity is 79.48499836650812
At time: 473.22293972969055 and batch: 100, loss is 4.278567070960999 and perplexity is 72.13699871276557
At time: 473.73194789886475 and batch: 150, loss is 4.30598816871643 and perplexity is 74.14244450978234
At time: 474.2436943054199 and batch: 200, loss is 4.343220100402832 and perplexity is 76.95494343585756
At time: 474.7554624080658 and batch: 250, loss is 4.357763667106628 and perplexity is 78.08232096263498
At time: 475.2677969932556 and batch: 300, loss is 4.246607484817505 and perplexity is 69.86798164228036
At time: 475.7793872356415 and batch: 350, loss is 4.24993013381958 and perplexity is 70.10051452039865
At time: 476.2908113002777 and batch: 400, loss is 4.250156545639038 and perplexity is 70.116387902329
At time: 476.801961183548 and batch: 450, loss is 4.301255569458008 and perplexity is 73.79238702613883
At time: 477.3270757198334 and batch: 500, loss is 4.338781461715699 and perplexity is 76.61412519083353
At time: 477.8519825935364 and batch: 550, loss is 4.268000183105468 and perplexity is 71.37874836577546
At time: 478.38351464271545 and batch: 600, loss is 4.209613151550293 and perplexity is 67.33048807647471
At time: 478.89503049850464 and batch: 650, loss is 4.183690218925476 and perplexity is 65.60751313739361
At time: 479.4060056209564 and batch: 700, loss is 4.274498949050903 and perplexity is 71.84413271895824
At time: 479.91777443885803 and batch: 750, loss is 4.2187389087677 and perplexity is 67.94774193470873
At time: 480.42998719215393 and batch: 800, loss is 4.310956311225891 and perplexity is 74.51171126550656
At time: 480.9416620731354 and batch: 850, loss is 4.2314361953735355 and perplexity is 68.8159944487618
At time: 481.452002286911 and batch: 900, loss is 4.209443874359131 and perplexity is 67.31909152518885
At time: 481.96300172805786 and batch: 950, loss is 4.179045977592469 and perplexity is 65.3035224625507
At time: 482.47454714775085 and batch: 1000, loss is 4.100161962509155 and perplexity is 60.35006125320592
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8041229248046875 and perplexity of 122.01243000188856
Finished 43 epochs...
Completing Train Step...
At time: 483.95853090286255 and batch: 50, loss is 4.374969425201416 and perplexity is 79.43741073229803
At time: 484.46584963798523 and batch: 100, loss is 4.278024168014526 and perplexity is 72.09784595265081
At time: 484.9730806350708 and batch: 150, loss is 4.305594882965088 and perplexity is 74.11329107597777
At time: 485.4804353713989 and batch: 200, loss is 4.342768592834473 and perplexity is 76.9202055392753
At time: 485.9889476299286 and batch: 250, loss is 4.357240076065064 and perplexity is 78.04144846004624
At time: 486.4973261356354 and batch: 300, loss is 4.246077847480774 and perplexity is 69.8309867483639
At time: 487.0053746700287 and batch: 350, loss is 4.24941987991333 and perplexity is 70.06475458313419
At time: 487.513014793396 and batch: 400, loss is 4.249680109024048 and perplexity is 70.08298984448203
At time: 488.0207345485687 and batch: 450, loss is 4.3009096002578735 and perplexity is 73.76686154879246
At time: 488.52852487564087 and batch: 500, loss is 4.33836724281311 and perplexity is 76.58239674368913
At time: 489.0365536212921 and batch: 550, loss is 4.267597293853759 and perplexity is 71.3499964275708
At time: 489.54438877105713 and batch: 600, loss is 4.209179997444153 and perplexity is 67.30132991454062
At time: 490.0532371997833 and batch: 650, loss is 4.183241763114929 and perplexity is 65.57809766317476
At time: 490.5614244937897 and batch: 700, loss is 4.274211411476135 and perplexity is 71.82347780094975
At time: 491.0924150943756 and batch: 750, loss is 4.218454556465149 and perplexity is 67.92842358457528
At time: 491.6007857322693 and batch: 800, loss is 4.310618948936463 and perplexity is 74.4865780637404
At time: 492.10822224617004 and batch: 850, loss is 4.231118659973145 and perplexity is 68.79414640335874
At time: 492.6159727573395 and batch: 900, loss is 4.209155464172364 and perplexity is 67.2996788129756
At time: 493.122642993927 and batch: 950, loss is 4.178906903266907 and perplexity is 65.29444105071768
At time: 493.63039994239807 and batch: 1000, loss is 4.099936118125916 and perplexity is 60.336433069826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.804103572194169 and perplexity of 122.01006876570042
Finished 44 epochs...
Completing Train Step...
At time: 495.10382890701294 and batch: 50, loss is 4.374401254653931 and perplexity is 79.39228955462735
At time: 495.62573432922363 and batch: 100, loss is 4.277493529319763 and perplexity is 72.05959819454695
At time: 496.1333158016205 and batch: 150, loss is 4.3052017116546635 and perplexity is 74.08415758380751
At time: 496.6395752429962 and batch: 200, loss is 4.342343130111694 and perplexity is 76.88748582019386
At time: 497.1483235359192 and batch: 250, loss is 4.356743230819702 and perplexity is 78.00268356831124
At time: 497.6577317714691 and batch: 300, loss is 4.245581712722778 and perplexity is 69.79634976166987
At time: 498.1659550666809 and batch: 350, loss is 4.2489114761352536 and perplexity is 70.02914245063252
At time: 498.6744222640991 and batch: 400, loss is 4.249214582443237 and perplexity is 70.05037194268593
At time: 499.1827232837677 and batch: 450, loss is 4.300575542449951 and perplexity is 73.74222326826026
At time: 499.70624566078186 and batch: 500, loss is 4.337960424423218 and perplexity is 76.5512479527319
At time: 500.2269308567047 and batch: 550, loss is 4.267197732925415 and perplexity is 71.32149345147782
At time: 500.74009013175964 and batch: 600, loss is 4.208759317398071 and perplexity is 67.27302354235124
At time: 501.2492849826813 and batch: 650, loss is 4.182810888290406 and perplexity is 65.54984779836637
At time: 501.75702691078186 and batch: 700, loss is 4.273921155929566 and perplexity is 71.80263366335373
At time: 502.27928400039673 and batch: 750, loss is 4.218148102760315 and perplexity is 67.90760985688914
At time: 502.7889151573181 and batch: 800, loss is 4.310289177894592 and perplexity is 74.46201859701002
At time: 503.2971661090851 and batch: 850, loss is 4.230831046104431 and perplexity is 68.77436309787967
At time: 503.8047456741333 and batch: 900, loss is 4.208877825737 and perplexity is 67.28099642904341
At time: 504.325026512146 and batch: 950, loss is 4.178762392997742 and perplexity is 65.28500601521263
At time: 504.8332567214966 and batch: 1000, loss is 4.099722456932068 and perplexity is 60.32354289261815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.804079381431022 and perplexity of 122.00711728472477
Finished 45 epochs...
Completing Train Step...
At time: 506.3103246688843 and batch: 50, loss is 4.373868007659912 and perplexity is 79.34996514056027
At time: 506.838947057724 and batch: 100, loss is 4.2769682455062865 and perplexity is 72.02175639372258
At time: 507.35336232185364 and batch: 150, loss is 4.3048100090026855 and perplexity is 74.05514430547238
At time: 507.86572074890137 and batch: 200, loss is 4.341894550323486 and perplexity is 76.85300338272926
At time: 508.3752746582031 and batch: 250, loss is 4.356265435218811 and perplexity is 77.96542313138987
At time: 508.8853769302368 and batch: 300, loss is 4.245092844963073 and perplexity is 69.76223691554104
At time: 509.3975532054901 and batch: 350, loss is 4.248378076553345 and perplexity is 69.9917988957316
At time: 509.9086136817932 and batch: 400, loss is 4.248757562637329 and perplexity is 70.0183648497917
At time: 510.41872692108154 and batch: 450, loss is 4.300164399147033 and perplexity is 73.71191087881621
At time: 510.9287545681 and batch: 500, loss is 4.337516660690308 and perplexity is 76.51728482154174
At time: 511.4377055168152 and batch: 550, loss is 4.266809177398682 and perplexity is 71.29378647422055
At time: 511.94838523864746 and batch: 600, loss is 4.208349142074585 and perplexity is 67.24543546651863
At time: 512.4607899188995 and batch: 650, loss is 4.18239342212677 and perplexity is 65.52248866603246
At time: 512.9775559902191 and batch: 700, loss is 4.273622455596924 and perplexity is 71.78118939565834
At time: 513.4901196956635 and batch: 750, loss is 4.217876725196838 and perplexity is 67.88918375551296
At time: 514.003383398056 and batch: 800, loss is 4.309970560073853 and perplexity is 74.43829745010751
At time: 514.5173230171204 and batch: 850, loss is 4.230563378334045 and perplexity is 68.75595688093497
At time: 515.0321979522705 and batch: 900, loss is 4.208596601486206 and perplexity is 67.26207804150933
At time: 515.5468916893005 and batch: 950, loss is 4.178610181808471 and perplexity is 65.27506966303669
At time: 516.0625312328339 and batch: 1000, loss is 4.099506649971008 and perplexity is 60.31052605675856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.804045886528201 and perplexity of 122.00303073662758
Finished 46 epochs...
Completing Train Step...
At time: 517.5563411712646 and batch: 50, loss is 4.3733248615264895 and perplexity is 79.30687821611444
At time: 518.0677511692047 and batch: 100, loss is 4.276446704864502 and perplexity is 71.98420391409391
At time: 518.5800313949585 and batch: 150, loss is 4.304414138793946 and perplexity is 74.02583388198242
At time: 519.091954946518 and batch: 200, loss is 4.341443567276001 and perplexity is 76.8183517952819
At time: 519.6035153865814 and batch: 250, loss is 4.355795111656189 and perplexity is 77.92876277761127
At time: 520.1148359775543 and batch: 300, loss is 4.2446083116531375 and perplexity is 69.72844297578573
At time: 520.6265320777893 and batch: 350, loss is 4.247866144180298 and perplexity is 69.955976998006
At time: 521.138266324997 and batch: 400, loss is 4.248315992355347 and perplexity is 69.98745364591782
At time: 521.6501996517181 and batch: 450, loss is 4.299853658676147 and perplexity is 73.68900916334901
At time: 522.1621868610382 and batch: 500, loss is 4.337110443115234 and perplexity is 76.48620846795816
At time: 522.6745216846466 and batch: 550, loss is 4.266422305107117 and perplexity is 71.26621021826155
At time: 523.1857678890228 and batch: 600, loss is 4.2079501247406 and perplexity is 67.21860872465959
At time: 523.6968307495117 and batch: 650, loss is 4.181997027397156 and perplexity is 65.4965210439083
At time: 524.208021402359 and batch: 700, loss is 4.273320641517639 and perplexity is 71.75952809107866
At time: 524.718380689621 and batch: 750, loss is 4.217597851753235 and perplexity is 67.87025390469465
At time: 525.2282972335815 and batch: 800, loss is 4.309633574485779 and perplexity is 74.4132170427713
At time: 525.7403347492218 and batch: 850, loss is 4.230290966033936 and perplexity is 68.73722946348006
At time: 526.2522251605988 and batch: 900, loss is 4.208306798934936 and perplexity is 67.24258814393664
At time: 526.7647018432617 and batch: 950, loss is 4.17844181060791 and perplexity is 65.26408014637582
At time: 527.2760233879089 and batch: 1000, loss is 4.09928343296051 and perplexity is 60.29706522382991
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8040034596512955 and perplexity of 121.99785463886398
Finished 47 epochs...
Completing Train Step...
At time: 528.7607443332672 and batch: 50, loss is 4.372766590118408 and perplexity is 79.26261580990969
At time: 529.2856135368347 and batch: 100, loss is 4.275915594100952 and perplexity is 71.94598247939571
At time: 529.7970616817474 and batch: 150, loss is 4.304014625549317 and perplexity is 73.9962654877777
At time: 530.3210463523865 and batch: 200, loss is 4.340981140136718 and perplexity is 76.78283711672526
At time: 530.8317017555237 and batch: 250, loss is 4.355331888198853 and perplexity is 77.89267270622067
At time: 531.3430647850037 and batch: 300, loss is 4.244133248329162 and perplexity is 69.69532541698166
At time: 531.8549828529358 and batch: 350, loss is 4.247363586425781 and perplexity is 69.92082891200218
At time: 532.3663868904114 and batch: 400, loss is 4.247867641448974 and perplexity is 69.95608174097748
At time: 532.8779094219208 and batch: 450, loss is 4.2995354318618775 and perplexity is 73.66556307550105
At time: 533.3888392448425 and batch: 500, loss is 4.33673098564148 and perplexity is 76.45719071036652
At time: 533.8993949890137 and batch: 550, loss is 4.266015028953552 and perplexity is 71.23719110008324
At time: 534.4104580879211 and batch: 600, loss is 4.207569088935852 and perplexity is 67.19300090704763
At time: 534.9213542938232 and batch: 650, loss is 4.181589217185974 and perplexity is 65.4698163394254
At time: 535.431619644165 and batch: 700, loss is 4.273025584220886 and perplexity is 71.73835804204619
At time: 535.9425072669983 and batch: 750, loss is 4.217320990562439 and perplexity is 67.85146586633834
At time: 536.4514670372009 and batch: 800, loss is 4.309298691749572 and perplexity is 74.38830151316131
At time: 536.9622495174408 and batch: 850, loss is 4.230018520355225 and perplexity is 68.7185048531816
At time: 537.4738492965698 and batch: 900, loss is 4.207997169494629 and perplexity is 67.22177108195874
At time: 537.9856216907501 and batch: 950, loss is 4.178265585899353 and perplexity is 65.25258001620608
At time: 538.4971764087677 and batch: 1000, loss is 4.099057278633118 and perplexity is 60.28343032345446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.803952472965892 and perplexity of 121.9916345312023
Finished 48 epochs...
Completing Train Step...
At time: 539.9745650291443 and batch: 50, loss is 4.372209072113037 and perplexity is 79.2184377906078
At time: 540.4987757205963 and batch: 100, loss is 4.275381426811219 and perplexity is 71.90756155144254
At time: 541.0092303752899 and batch: 150, loss is 4.303626918792725 and perplexity is 73.96758219639777
At time: 541.5194442272186 and batch: 200, loss is 4.34051103591919 and perplexity is 76.74674966425927
At time: 542.029217004776 and batch: 250, loss is 4.354864106178284 and perplexity is 77.856244435304
At time: 542.538444519043 and batch: 300, loss is 4.243672804832459 and perplexity is 69.6632420444997
At time: 543.0478665828705 and batch: 350, loss is 4.24686752319336 and perplexity is 69.88615236119057
At time: 543.5703954696655 and batch: 400, loss is 4.247407751083374 and perplexity is 69.9239170096616
At time: 544.0799043178558 and batch: 450, loss is 4.299203162193298 and perplexity is 73.64109030927638
At time: 544.5899519920349 and batch: 500, loss is 4.336341476440429 and perplexity is 76.42741573028917
At time: 545.0995562076569 and batch: 550, loss is 4.265601592063904 and perplexity is 71.20774510481735
At time: 545.6085245609283 and batch: 600, loss is 4.207191252708435 and perplexity is 67.16761775271573
At time: 546.1227068901062 and batch: 650, loss is 4.18116231918335 and perplexity is 65.44187337042112
At time: 546.6335031986237 and batch: 700, loss is 4.272690334320068 and perplexity is 71.71431179560382
At time: 547.1454293727875 and batch: 750, loss is 4.2170185422897335 and perplexity is 67.83094741072867
At time: 547.656058549881 and batch: 800, loss is 4.3089430618286135 and perplexity is 74.3618515108588
At time: 548.1664500236511 and batch: 850, loss is 4.229739241600036 and perplexity is 68.69931591434397
At time: 548.6794919967651 and batch: 900, loss is 4.207653765678406 and perplexity is 67.19869083238562
At time: 549.1910128593445 and batch: 950, loss is 4.178099040985107 and perplexity is 65.24171343577495
At time: 549.7110040187836 and batch: 1000, loss is 4.09878791809082 and perplexity is 60.267194532709844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.803887716153773 and perplexity of 121.98373499762195
Finished 49 epochs...
Completing Train Step...
At time: 551.2295618057251 and batch: 50, loss is 4.371655511856079 and perplexity is 79.17459774700099
At time: 551.744057893753 and batch: 100, loss is 4.274864635467529 and perplexity is 71.87040994672765
At time: 552.2591660022736 and batch: 150, loss is 4.303242616653442 and perplexity is 73.93916175768959
At time: 552.7736778259277 and batch: 200, loss is 4.340026235580444 and perplexity is 76.70955183151392
At time: 553.28795170784 and batch: 250, loss is 4.3544027042388915 and perplexity is 77.82032969932932
At time: 553.8021128177643 and batch: 300, loss is 4.243242092132569 and perplexity is 69.63324366224148
At time: 554.316502571106 and batch: 350, loss is 4.246375017166137 and perplexity is 69.85174148441098
At time: 554.8297853469849 and batch: 400, loss is 4.246947402954102 and perplexity is 69.89173507329023
At time: 555.3435366153717 and batch: 450, loss is 4.298863835334778 and perplexity is 73.61610614858185
At time: 555.8571598529816 and batch: 500, loss is 4.335953149795532 and perplexity is 76.39774269014943
At time: 556.3967392444611 and batch: 550, loss is 4.265218682289124 and perplexity is 71.18048418274651
At time: 556.9107427597046 and batch: 600, loss is 4.206792755126953 and perplexity is 67.14085695189868
At time: 557.4253158569336 and batch: 650, loss is 4.18075466632843 and perplexity is 65.41520124076466
At time: 557.939305305481 and batch: 700, loss is 4.272334170341492 and perplexity is 71.68877428903376
At time: 558.4526681900024 and batch: 750, loss is 4.216704850196838 and perplexity is 67.80967271589856
At time: 558.9634809494019 and batch: 800, loss is 4.308557915687561 and perplexity is 74.33321684532729
At time: 559.4896256923676 and batch: 850, loss is 4.229362311363221 and perplexity is 68.67342594459005
At time: 560.0071728229523 and batch: 900, loss is 4.207316179275512 and perplexity is 67.17600929677279
At time: 560.5216295719147 and batch: 950, loss is 4.17802857875824 and perplexity is 65.23711652131794
At time: 561.0363388061523 and batch: 1000, loss is 4.09853886127472 and perplexity is 60.25218644613493
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.803823331507241 and perplexity of 121.97588137079063
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f46fd764898>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'batch_size': 20, 'lr': 26.320530869097347, 'dropout': 0.07257977092906687, 'wordvec_source': '', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'anneal': 6.243356750075442, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7530276775360107 and batch: 50, loss is 6.4442909145355225 and perplexity is 629.1004327954946
At time: 1.2627134323120117 and batch: 100, loss is 5.51888277053833 and perplexity is 249.35629331091118
At time: 1.7737822532653809 and batch: 150, loss is 5.438484392166138 and perplexity is 230.0931880190549
At time: 2.2847840785980225 and batch: 200, loss is 5.392908306121826 and perplexity is 219.84182403935816
At time: 2.7928152084350586 and batch: 250, loss is 5.4362606716156 and perplexity is 229.58209354457205
At time: 3.3011868000030518 and batch: 300, loss is 5.3521253204345705 and perplexity is 211.056383980441
At time: 3.824092149734497 and batch: 350, loss is 5.322774429321289 and perplexity is 204.9517179741249
At time: 4.334479570388794 and batch: 400, loss is 5.3085700798034665 and perplexity is 202.0610904811346
At time: 4.845815420150757 and batch: 450, loss is 5.3443221092224125 and perplexity is 209.4158753803383
At time: 5.356494188308716 and batch: 500, loss is 5.354934473037719 and perplexity is 211.65010710990907
At time: 5.868358612060547 and batch: 550, loss is 5.300664825439453 and perplexity is 200.47004326605517
At time: 6.379549264907837 and batch: 600, loss is 5.218996143341064 and perplexity is 184.74862980906212
At time: 6.891475677490234 and batch: 650, loss is 5.210266466140747 and perplexity is 183.14285306160014
At time: 7.4034271240234375 and batch: 700, loss is 5.274369115829468 and perplexity is 195.26724662982483
At time: 7.9154016971588135 and batch: 750, loss is 5.202881841659546 and perplexity is 181.7953932311275
At time: 8.427114963531494 and batch: 800, loss is 5.305376558303833 and perplexity is 201.41683331653923
At time: 8.939380645751953 and batch: 850, loss is 5.225441541671753 and perplexity is 185.9432540982273
At time: 9.452944993972778 and batch: 900, loss is 5.25952826499939 and perplexity is 192.39071245429335
At time: 9.96613097190857 and batch: 950, loss is 5.254778814315796 and perplexity is 191.4791287280563
At time: 10.478707313537598 and batch: 1000, loss is 5.152129774093628 and perplexity is 172.7991217848823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.274772085794589 and perplexity of 195.345949321734
Finished 1 epochs...
Completing Train Step...
At time: 11.967551708221436 and batch: 50, loss is 5.275589046478271 and perplexity is 195.505604489164
At time: 12.47575068473816 and batch: 100, loss is 5.231303091049194 and perplexity is 187.03637021081076
At time: 12.983205795288086 and batch: 150, loss is 5.255690002441407 and perplexity is 191.6536817496988
At time: 13.491423606872559 and batch: 200, loss is 5.241133346557617 and perplexity is 188.884052233502
At time: 13.997132062911987 and batch: 250, loss is 5.257125759124756 and perplexity is 191.92904743591848
At time: 14.504363775253296 and batch: 300, loss is 5.224655227661133 and perplexity is 185.79710178068746
At time: 15.0133056640625 and batch: 350, loss is 5.206262254714966 and perplexity is 182.41097662869907
At time: 15.521096467971802 and batch: 400, loss is 5.207985734939575 and perplexity is 182.72562941070728
At time: 16.029945611953735 and batch: 450, loss is 5.2527188968658445 and perplexity is 191.08510349854015
At time: 16.54080820083618 and batch: 500, loss is 5.271680202484131 and perplexity is 194.74289520815518
At time: 17.051963329315186 and batch: 550, loss is 5.246264524459839 and perplexity is 189.85574072971102
At time: 17.564032793045044 and batch: 600, loss is 5.1652219772338865 and perplexity is 175.07631721538547
At time: 18.075042724609375 and batch: 650, loss is 5.132676286697388 and perplexity is 169.4700621433228
At time: 18.586086988449097 and batch: 700, loss is 5.225583515167236 and perplexity is 185.969654986042
At time: 19.097456455230713 and batch: 750, loss is 5.128198833465576 and perplexity is 168.71296406633172
At time: 19.622849941253662 and batch: 800, loss is 5.253590888977051 and perplexity is 191.25180087019385
At time: 20.134665727615356 and batch: 850, loss is 5.167094888687134 and perplexity is 175.40452691300035
At time: 20.64665937423706 and batch: 900, loss is 5.208800287246704 and perplexity is 182.87452962899064
At time: 21.161348581314087 and batch: 950, loss is 5.230046348571777 and perplexity is 186.8014613004659
At time: 21.672456741333008 and batch: 1000, loss is 5.102697639465332 and perplexity is 164.4649766221637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.372634143364139 and perplexity of 215.42959339215813
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 23.153205156326294 and batch: 50, loss is 5.194797382354737 and perplexity is 180.3316007307944
At time: 23.681946516036987 and batch: 100, loss is 5.057228517532349 and perplexity is 157.15436164700748
At time: 24.197885513305664 and batch: 150, loss is 5.053167095184326 and perplexity is 156.51738580007924
At time: 24.71255898475647 and batch: 200, loss is 5.048016166687011 and perplexity is 155.7132487414325
At time: 25.224973917007446 and batch: 250, loss is 5.078648271560669 and perplexity is 160.55687987949113
At time: 25.739367961883545 and batch: 300, loss is 4.9874390125274655 and perplexity is 146.56060259757595
At time: 26.25396227836609 and batch: 350, loss is 4.974006128311157 and perplexity is 144.60503485525967
At time: 26.76939296722412 and batch: 400, loss is 4.966787376403809 and perplexity is 143.56492564595845
At time: 27.282574892044067 and batch: 450, loss is 4.991996784210205 and perplexity is 147.23011694967747
At time: 27.796988010406494 and batch: 500, loss is 5.028991756439209 and perplexity is 152.77889667662944
At time: 28.31258201599121 and batch: 550, loss is 4.9381226062774655 and perplexity is 139.50809193442242
At time: 28.828072786331177 and batch: 600, loss is 4.857762794494629 and perplexity is 128.735871100216
At time: 29.342642784118652 and batch: 650, loss is 4.846905326843261 and perplexity is 127.34568614502167
At time: 29.85814094543457 and batch: 700, loss is 4.919431371688843 and perplexity is 136.9247317667972
At time: 30.37339448928833 and batch: 750, loss is 4.839105033874512 and perplexity is 126.35621657801971
At time: 30.888229608535767 and batch: 800, loss is 4.930469827651978 and perplexity is 138.44454213652725
At time: 31.40333867073059 and batch: 850, loss is 4.834804334640503 and perplexity is 125.81396336395484
At time: 31.91805648803711 and batch: 900, loss is 4.832092933654785 and perplexity is 125.47329331493458
At time: 32.447118520736694 and batch: 950, loss is 4.836703042984009 and perplexity is 126.05307431506775
At time: 32.962302446365356 and batch: 1000, loss is 4.728128347396851 and perplexity is 113.08371073262354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.083939342963986 and perplexity of 161.40864919422208
Finished 3 epochs...
Completing Train Step...
At time: 34.45704698562622 and batch: 50, loss is 4.957253475189209 and perplexity is 142.2026958275671
At time: 34.970036029815674 and batch: 100, loss is 4.881044034957886 and perplexity is 131.76816266473307
At time: 35.48291254043579 and batch: 150, loss is 4.920590457916259 and perplexity is 137.08353135100762
At time: 35.99588394165039 and batch: 200, loss is 4.935770282745361 and perplexity is 139.1803094432265
At time: 36.50584387779236 and batch: 250, loss is 4.959508953094482 and perplexity is 142.52379284356581
At time: 37.01643681526184 and batch: 300, loss is 4.8711429214477535 and perplexity is 130.46994861702265
At time: 37.52867364883423 and batch: 350, loss is 4.868715686798096 and perplexity is 130.1536514560026
At time: 38.040897846221924 and batch: 400, loss is 4.861139078140258 and perplexity is 129.17125449174122
At time: 38.5531702041626 and batch: 450, loss is 4.892220716476441 and perplexity is 133.24915432229596
At time: 39.06536555290222 and batch: 500, loss is 4.927738857269287 and perplexity is 138.06696999614095
At time: 39.57626414299011 and batch: 550, loss is 4.848056983947754 and perplexity is 127.49242919181432
At time: 40.088053941726685 and batch: 600, loss is 4.7738800144195555 and perplexity is 118.37765907722864
At time: 40.60044980049133 and batch: 650, loss is 4.769251480102539 and perplexity is 117.83101008754542
At time: 41.1131854057312 and batch: 700, loss is 4.8468004322052005 and perplexity is 127.33232896592536
At time: 41.626084327697754 and batch: 750, loss is 4.77022837638855 and perplexity is 117.94617500660902
At time: 42.138671875 and batch: 800, loss is 4.866670532226562 and perplexity is 129.8877391293159
At time: 42.650983572006226 and batch: 850, loss is 4.771957893371582 and perplexity is 118.15034142311656
At time: 43.16314744949341 and batch: 900, loss is 4.774472904205322 and perplexity is 118.44786479222765
At time: 43.67482113838196 and batch: 950, loss is 4.780200834274292 and perplexity is 119.12827268268254
At time: 44.18701696395874 and batch: 1000, loss is 4.680125112533569 and perplexity is 107.78355680172838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.059595619759908 and perplexity of 157.5268027156701
Finished 4 epochs...
Completing Train Step...
At time: 45.67920207977295 and batch: 50, loss is 4.888264322280884 and perplexity is 132.72300964575865
At time: 46.19187641143799 and batch: 100, loss is 4.808303508758545 and perplexity is 122.52358091912642
At time: 46.704521894454956 and batch: 150, loss is 4.84882960319519 and perplexity is 127.59097035900957
At time: 47.216851234436035 and batch: 200, loss is 4.866342344284058 and perplexity is 129.8451185336198
At time: 47.72873878479004 and batch: 250, loss is 4.888759613037109 and perplexity is 132.78876240759797
At time: 48.23948359489441 and batch: 300, loss is 4.800452756881714 and perplexity is 121.56544465248314
At time: 48.75122261047363 and batch: 350, loss is 4.809657964706421 and perplexity is 122.68964615068977
At time: 49.26272439956665 and batch: 400, loss is 4.800877208709717 and perplexity is 121.61705427981342
At time: 49.77483677864075 and batch: 450, loss is 4.8373618412017825 and perplexity is 126.13614521624768
At time: 50.28675031661987 and batch: 500, loss is 4.875926523208618 and perplexity is 131.09556003793398
At time: 50.799981355667114 and batch: 550, loss is 4.799198656082154 and perplexity is 121.41308488836547
At time: 51.31184387207031 and batch: 600, loss is 4.725598773956299 and perplexity is 112.79801867325715
At time: 51.82829475402832 and batch: 650, loss is 4.722794504165649 and perplexity is 112.48214570056429
At time: 52.36710476875305 and batch: 700, loss is 4.799707450866699 and perplexity is 121.47487495063
At time: 52.89788889884949 and batch: 750, loss is 4.726284332275391 and perplexity is 112.87537480637629
At time: 53.41567611694336 and batch: 800, loss is 4.819108180999756 and perplexity is 123.8545856391156
At time: 53.92786502838135 and batch: 850, loss is 4.727924585342407 and perplexity is 113.060670910801
At time: 54.43963694572449 and batch: 900, loss is 4.732018947601318 and perplexity is 113.52453121279729
At time: 54.95057964324951 and batch: 950, loss is 4.7301073741912845 and perplexity is 113.3077280211711
At time: 55.462815284729004 and batch: 1000, loss is 4.639972448348999 and perplexity is 103.54149480485286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.044351345155297 and perplexity of 155.14363188427276
Finished 5 epochs...
Completing Train Step...
At time: 56.94146966934204 and batch: 50, loss is 4.837709169387818 and perplexity is 126.17996346397487
At time: 57.465569734573364 and batch: 100, loss is 4.760374593734741 and perplexity is 116.78966638333245
At time: 57.97759985923767 and batch: 150, loss is 4.804732360839844 and perplexity is 122.08681143652983
At time: 58.512441873550415 and batch: 200, loss is 4.821110181808471 and perplexity is 124.10279099048462
At time: 59.02407503128052 and batch: 250, loss is 4.84579309463501 and perplexity is 127.20412690927306
At time: 59.53490424156189 and batch: 300, loss is 4.757907829284668 and perplexity is 116.50192882246034
At time: 60.04634642601013 and batch: 350, loss is 4.76387619972229 and perplexity is 117.19933460819193
At time: 60.55950570106506 and batch: 400, loss is 4.75120512008667 and perplexity is 115.72366143626287
At time: 61.07060098648071 and batch: 450, loss is 4.790659160614013 and perplexity is 120.38069272802765
At time: 61.582971811294556 and batch: 500, loss is 4.834841375350952 and perplexity is 125.8186236888526
At time: 62.095377922058105 and batch: 550, loss is 4.75908522605896 and perplexity is 116.63917860052231
At time: 62.6087965965271 and batch: 600, loss is 4.687461042404175 and perplexity is 108.57715675340646
At time: 63.120333671569824 and batch: 650, loss is 4.682821912765503 and perplexity is 108.07461981570978
At time: 63.630738258361816 and batch: 700, loss is 4.761790418624878 and perplexity is 116.95513721112573
At time: 64.14184355735779 and batch: 750, loss is 4.693224229812622 and perplexity is 109.20471388373866
At time: 64.65341067314148 and batch: 800, loss is 4.782704591751099 and perplexity is 119.4269146936842
At time: 65.16657543182373 and batch: 850, loss is 4.69022234916687 and perplexity is 108.8773859122612
At time: 65.67908668518066 and batch: 900, loss is 4.701420183181763 and perplexity is 110.1034285071701
At time: 66.19210720062256 and batch: 950, loss is 4.701823015213012 and perplexity is 110.14779062956521
At time: 66.70528626441956 and batch: 1000, loss is 4.615117177963257 and perplexity is 100.99966277388475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.040148107016959 and perplexity of 154.49289481331266
Finished 6 epochs...
Completing Train Step...
At time: 68.20081162452698 and batch: 50, loss is 4.790974349975586 and perplexity is 120.41864142191237
At time: 68.7147114276886 and batch: 100, loss is 4.71937593460083 and perplexity is 112.09827418028138
At time: 69.2292275428772 and batch: 150, loss is 4.762931566238404 and perplexity is 117.08867646635352
At time: 69.74438881874084 and batch: 200, loss is 4.7852386951446535 and perplexity is 119.72993862832159
At time: 70.25897860527039 and batch: 250, loss is 4.804860429763794 and perplexity is 122.1024479643543
At time: 70.77050948143005 and batch: 300, loss is 4.720725259780884 and perplexity is 112.24963329764286
At time: 71.284597158432 and batch: 350, loss is 4.7282380104064945 and perplexity is 113.09611251268001
At time: 71.81240630149841 and batch: 400, loss is 4.717468862533569 and perplexity is 111.88469840972706
At time: 72.32791996002197 and batch: 450, loss is 4.753443975448608 and perplexity is 115.98304022362287
At time: 72.84313583374023 and batch: 500, loss is 4.798319530487061 and perplexity is 121.30639444188347
At time: 73.3583493232727 and batch: 550, loss is 4.727493343353271 and perplexity is 113.0119249136002
At time: 73.87326622009277 and batch: 600, loss is 4.657638530731202 and perplexity is 105.38692010502267
At time: 74.3880763053894 and batch: 650, loss is 4.655323190689087 and perplexity is 105.14319581017766
At time: 74.90302109718323 and batch: 700, loss is 4.735728483200074 and perplexity is 113.94643655529913
At time: 75.41636371612549 and batch: 750, loss is 4.66906044960022 and perplexity is 106.59754160512756
At time: 75.93057560920715 and batch: 800, loss is 4.756073799133301 and perplexity is 116.28845658940091
At time: 76.44587731361389 and batch: 850, loss is 4.666442375183106 and perplexity is 106.31882631640758
At time: 76.96117997169495 and batch: 900, loss is 4.675212469100952 and perplexity is 107.25535311974501
At time: 77.47642993927002 and batch: 950, loss is 4.671909914016724 and perplexity is 106.90172067472189
At time: 77.99124264717102 and batch: 1000, loss is 4.58829623222351 and perplexity is 98.3267614005485
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.036609556616806 and perplexity of 153.94718000764544
Finished 7 epochs...
Completing Train Step...
At time: 79.48656678199768 and batch: 50, loss is 4.756971101760865 and perplexity is 116.39284935600149
At time: 79.99814987182617 and batch: 100, loss is 4.68841142654419 and perplexity is 108.68039581176389
At time: 80.51004672050476 and batch: 150, loss is 4.7318296718597415 and perplexity is 113.50304580636208
At time: 81.02384948730469 and batch: 200, loss is 4.75175479888916 and perplexity is 115.78728976587085
At time: 81.53641629219055 and batch: 250, loss is 4.7702361011505126 and perplexity is 117.94708611625438
At time: 82.04577827453613 and batch: 300, loss is 4.688868103027343 and perplexity is 108.73003892726989
At time: 82.5577232837677 and batch: 350, loss is 4.7003340148925785 and perplexity is 109.98390257898261
At time: 83.06866407394409 and batch: 400, loss is 4.687327289581299 and perplexity is 108.5626352233603
At time: 83.58015894889832 and batch: 450, loss is 4.7248311042785645 and perplexity is 112.7114602829898
At time: 84.09131026268005 and batch: 500, loss is 4.76821120262146 and perplexity is 117.70849687609451
At time: 84.61562323570251 and batch: 550, loss is 4.698385686874389 and perplexity is 109.76982647299494
At time: 85.12748169898987 and batch: 600, loss is 4.625140285491943 and perplexity is 102.01708359565635
At time: 85.6383707523346 and batch: 650, loss is 4.6245618724823 and perplexity is 101.95809264950877
At time: 86.14944243431091 and batch: 700, loss is 4.710299406051636 and perplexity is 111.085414572498
At time: 86.66038250923157 and batch: 750, loss is 4.645317440032959 and perplexity is 104.09640490737259
At time: 87.17149257659912 and batch: 800, loss is 4.728740921020508 and perplexity is 113.15300405254683
At time: 87.68289422988892 and batch: 850, loss is 4.642673635482788 and perplexity is 103.82155783947803
At time: 88.19408345222473 and batch: 900, loss is 4.6472500038146975 and perplexity is 104.29777236436057
At time: 88.70586800575256 and batch: 950, loss is 4.647245426177978 and perplexity is 104.2972949281408
At time: 89.21782302856445 and batch: 1000, loss is 4.5668088722229 and perplexity is 96.236516228611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.031526891196647 and perplexity of 153.16670313090842
Finished 8 epochs...
Completing Train Step...
At time: 90.69833493232727 and batch: 50, loss is 4.732742404937744 and perplexity is 113.60669108376467
At time: 91.22339081764221 and batch: 100, loss is 4.664493989944458 and perplexity is 106.11187795765422
At time: 91.73547410964966 and batch: 150, loss is 4.705626039505005 and perplexity is 110.5674828971494
At time: 92.24786186218262 and batch: 200, loss is 4.7300606632232665 and perplexity is 113.30243543112329
At time: 92.75998210906982 and batch: 250, loss is 4.7422154712677 and perplexity is 114.68800841297177
At time: 93.27090215682983 and batch: 300, loss is 4.654985141754151 and perplexity is 105.10765827187014
At time: 93.78215861320496 and batch: 350, loss is 4.665230121612549 and perplexity is 106.19001902890932
At time: 94.29514384269714 and batch: 400, loss is 4.655740652084351 and perplexity is 105.18709819854163
At time: 94.80732536315918 and batch: 450, loss is 4.696193637847901 and perplexity is 109.5294691654826
At time: 95.31912207603455 and batch: 500, loss is 4.745980129241944 and perplexity is 115.12058327570277
At time: 95.83337807655334 and batch: 550, loss is 4.680455017089844 and perplexity is 107.81912095427413
At time: 96.34581661224365 and batch: 600, loss is 4.610202198028564 and perplexity is 100.50446938758539
At time: 96.85826706886292 and batch: 650, loss is 4.6055311107635495 and perplexity is 100.03609899166416
At time: 97.37238454818726 and batch: 700, loss is 4.682395706176758 and perplexity is 108.02856751526225
At time: 97.89865064620972 and batch: 750, loss is 4.621347684860229 and perplexity is 101.6309063109647
At time: 98.40948867797852 and batch: 800, loss is 4.705302867889404 and perplexity is 110.53175639827475
At time: 98.92093014717102 and batch: 850, loss is 4.625565395355225 and perplexity is 102.06046128360435
At time: 99.43141674995422 and batch: 900, loss is 4.629047183990479 and perplexity is 102.41643358835638
At time: 99.94283246994019 and batch: 950, loss is 4.623046722412109 and perplexity is 101.80372781075354
At time: 100.45499968528748 and batch: 1000, loss is 4.546856355667114 and perplexity is 94.33538479358354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.032331513195503 and perplexity of 153.2899940243707
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 101.95027923583984 and batch: 50, loss is 4.710506982803345 and perplexity is 111.10847571541298
At time: 102.46609807014465 and batch: 100, loss is 4.612708292007446 and perplexity is 100.75665890649596
At time: 102.9820294380188 and batch: 150, loss is 4.63222261428833 and perplexity is 102.74216673231095
At time: 103.49685311317444 and batch: 200, loss is 4.6483834362030025 and perplexity is 104.41605385696101
At time: 104.01203775405884 and batch: 250, loss is 4.66290979385376 and perplexity is 105.94390901841444
At time: 104.52528405189514 and batch: 300, loss is 4.5725797843933105 and perplexity is 96.79349430134951
At time: 105.03986239433289 and batch: 350, loss is 4.571651601791382 and perplexity is 96.70369394597
At time: 105.555246591568 and batch: 400, loss is 4.557649278640747 and perplexity is 95.3590535875652
At time: 106.07103037834167 and batch: 450, loss is 4.595653867721557 and perplexity is 99.05288185977463
At time: 106.58671832084656 and batch: 500, loss is 4.630759286880493 and perplexity is 102.59193125244727
At time: 107.10129380226135 and batch: 550, loss is 4.560214185714722 and perplexity is 95.60395463863678
At time: 107.61638832092285 and batch: 600, loss is 4.48830548286438 and perplexity is 88.97055594036435
At time: 108.13133025169373 and batch: 650, loss is 4.477249240875244 and perplexity is 87.99229386037142
At time: 108.64776515960693 and batch: 700, loss is 4.55435320854187 and perplexity is 95.04526088783436
At time: 109.16311931610107 and batch: 750, loss is 4.478721179962158 and perplexity is 88.12190852609614
At time: 109.67938280105591 and batch: 800, loss is 4.553885831832885 and perplexity is 95.00084932586931
At time: 110.19529843330383 and batch: 850, loss is 4.473206272125244 and perplexity is 87.6372619411311
At time: 110.72444558143616 and batch: 900, loss is 4.47432147026062 and perplexity is 87.73504936828095
At time: 111.23899459838867 and batch: 950, loss is 4.443186693191528 and perplexity is 85.04552430620753
At time: 111.75407528877258 and batch: 1000, loss is 4.367771997451782 and perplexity is 78.86771832720777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.941799629025343 and perplexity of 140.02201062830613
Finished 10 epochs...
Completing Train Step...
At time: 113.24946904182434 and batch: 50, loss is 4.643876085281372 and perplexity is 103.94647313792736
At time: 113.76221013069153 and batch: 100, loss is 4.56177209854126 and perplexity is 95.75301334592379
At time: 114.27525925636292 and batch: 150, loss is 4.5929780769348145 and perplexity is 98.78819135722676
At time: 114.78752112388611 and batch: 200, loss is 4.616557130813598 and perplexity is 101.14520228604962
At time: 115.29935789108276 and batch: 250, loss is 4.635276031494141 and perplexity is 103.05636087075585
At time: 115.80768489837646 and batch: 300, loss is 4.548723154067993 and perplexity is 94.51165441779807
At time: 116.31997847557068 and batch: 350, loss is 4.547719793319702 and perplexity is 94.41687269158277
At time: 116.8318223953247 and batch: 400, loss is 4.536043472290039 and perplexity is 93.32084222956773
At time: 117.34481859207153 and batch: 450, loss is 4.574308128356933 and perplexity is 96.96093140571723
At time: 117.85681366920471 and batch: 500, loss is 4.6110456180572506 and perplexity is 100.58927262738771
At time: 118.3687629699707 and batch: 550, loss is 4.544896745681763 and perplexity is 94.15070524061667
At time: 118.88039302825928 and batch: 600, loss is 4.476342964172363 and perplexity is 87.91258461918255
At time: 119.3930184841156 and batch: 650, loss is 4.466411218643189 and perplexity is 87.04378071112284
At time: 119.90563535690308 and batch: 700, loss is 4.544789457321167 and perplexity is 94.1406045076577
At time: 120.4187581539154 and batch: 750, loss is 4.470168733596802 and perplexity is 87.37146427115765
At time: 120.9310736656189 and batch: 800, loss is 4.551320562362671 and perplexity is 94.75745886198378
At time: 121.44391918182373 and batch: 850, loss is 4.47298225402832 and perplexity is 87.6176318073261
At time: 121.95665240287781 and batch: 900, loss is 4.475949621200561 and perplexity is 87.87801162185818
At time: 122.46885704994202 and batch: 950, loss is 4.4495875358581545 and perplexity is 85.59163324093588
At time: 122.98122477531433 and batch: 1000, loss is 4.376407670974731 and perplexity is 79.55174345209376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.93849330995141 and perplexity of 139.5598176835177
Finished 11 epochs...
Completing Train Step...
At time: 124.45949983596802 and batch: 50, loss is 4.631383619308472 and perplexity is 102.65600272084363
At time: 124.98498392105103 and batch: 100, loss is 4.549839382171631 and perplexity is 94.61720988360938
At time: 125.4976418018341 and batch: 150, loss is 4.581311473846435 and perplexity is 97.64236568198658
At time: 126.00998377799988 and batch: 200, loss is 4.604830913543701 and perplexity is 99.96607851019971
At time: 126.5217456817627 and batch: 250, loss is 4.624782648086548 and perplexity is 101.98060499401824
At time: 127.03114867210388 and batch: 300, loss is 4.539062852859497 and perplexity is 93.60303918292891
At time: 127.54191040992737 and batch: 350, loss is 4.53772442817688 and perplexity is 93.47784236686246
At time: 128.05459117889404 and batch: 400, loss is 4.527196130752563 and perplexity is 92.49884248390447
At time: 128.56681966781616 and batch: 450, loss is 4.564658470153809 and perplexity is 96.02979137542877
At time: 129.07889771461487 and batch: 500, loss is 4.601733798980713 and perplexity is 99.65695106130177
At time: 129.59084224700928 and batch: 550, loss is 4.537881050109863 and perplexity is 93.49248419381081
At time: 130.10262966156006 and batch: 600, loss is 4.469926853179931 and perplexity is 87.35033338063445
At time: 130.61510848999023 and batch: 650, loss is 4.461166715621948 and perplexity is 86.58847431168942
At time: 131.12670469284058 and batch: 700, loss is 4.540035705566407 and perplexity is 93.69414546228266
At time: 131.63910126686096 and batch: 750, loss is 4.466853904724121 and perplexity is 87.08232231156096
At time: 132.14984679222107 and batch: 800, loss is 4.549897718429565 and perplexity is 94.62272965857012
At time: 132.66191935539246 and batch: 850, loss is 4.472514753341675 and perplexity is 87.57668007752069
At time: 133.17288279533386 and batch: 900, loss is 4.476790542602539 and perplexity is 87.95194120272106
At time: 133.68519520759583 and batch: 950, loss is 4.451288042068481 and perplexity is 85.73730616856936
At time: 134.19688510894775 and batch: 1000, loss is 4.378136949539185 and perplexity is 79.68942959134354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.937370486375762 and perplexity of 139.4032045709111
Finished 12 epochs...
Completing Train Step...
At time: 135.6994047164917 and batch: 50, loss is 4.622885913848877 and perplexity is 101.78735821577338
At time: 136.21131443977356 and batch: 100, loss is 4.541240396499634 and perplexity is 93.80708596535857
At time: 136.73642373085022 and batch: 150, loss is 4.572898254394532 and perplexity is 96.82432503466912
At time: 137.24988389015198 and batch: 200, loss is 4.596431894302368 and perplexity is 99.12997762215464
At time: 137.76296210289001 and batch: 250, loss is 4.617905378341675 and perplexity is 101.28166302576487
At time: 138.27247095108032 and batch: 300, loss is 4.532352266311645 and perplexity is 92.97701074562767
At time: 138.7837574481964 and batch: 350, loss is 4.5302966117858885 and perplexity is 92.78607844535614
At time: 139.29661083221436 and batch: 400, loss is 4.520385665893555 and perplexity is 91.87102266671914
At time: 139.80941557884216 and batch: 450, loss is 4.557533702850342 and perplexity is 95.34803302644178
At time: 140.33040237426758 and batch: 500, loss is 4.595102863311768 and perplexity is 98.99831831882385
At time: 140.85288095474243 and batch: 550, loss is 4.5323013591766355 and perplexity is 92.97227767286346
At time: 141.3884460926056 and batch: 600, loss is 4.465284700393677 and perplexity is 86.94577951407769
At time: 141.90846300125122 and batch: 650, loss is 4.457204570770264 and perplexity is 86.24607699579093
At time: 142.42763924598694 and batch: 700, loss is 4.535620956420899 and perplexity is 93.28142102143363
At time: 142.95650815963745 and batch: 750, loss is 4.464214363098145 and perplexity is 86.85276798930626
At time: 143.473974943161 and batch: 800, loss is 4.548247146606445 and perplexity is 94.46667687076524
At time: 143.9862072467804 and batch: 850, loss is 4.471577711105347 and perplexity is 87.49465546563538
At time: 144.49929118156433 and batch: 900, loss is 4.476687288284301 and perplexity is 87.94286025382614
At time: 145.0113821029663 and batch: 950, loss is 4.451328392028809 and perplexity is 85.74076573526807
At time: 145.5237443447113 and batch: 1000, loss is 4.377510089874267 and perplexity is 79.63949115604314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.9371464426924545 and perplexity of 139.37197566194465
Finished 13 epochs...
Completing Train Step...
At time: 147.0186653137207 and batch: 50, loss is 4.616609268188476 and perplexity is 101.1504758688525
At time: 147.53247737884521 and batch: 100, loss is 4.534063339233398 and perplexity is 93.13623737639999
At time: 148.04783010482788 and batch: 150, loss is 4.565918893814087 and perplexity is 96.15090590834713
At time: 148.56306314468384 and batch: 200, loss is 4.589695234298706 and perplexity is 98.46441701158615
At time: 149.0781569480896 and batch: 250, loss is 4.612206249237061 and perplexity is 100.70608744990304
At time: 149.59056401252747 and batch: 300, loss is 4.526827344894409 and perplexity is 92.46473650818805
At time: 150.11685013771057 and batch: 350, loss is 4.524443025588989 and perplexity is 92.24453367311712
At time: 150.63189244270325 and batch: 400, loss is 4.515558547973633 and perplexity is 91.42861903317618
At time: 151.1464865207672 and batch: 450, loss is 4.552003631591797 and perplexity is 94.82220687753357
At time: 151.6618525981903 and batch: 500, loss is 4.589725971221924 and perplexity is 98.46744355132458
At time: 152.17781448364258 and batch: 550, loss is 4.5274560928344725 and perplexity is 92.52289180139064
At time: 152.69284296035767 and batch: 600, loss is 4.461419906616211 and perplexity is 86.61040050923285
At time: 153.2074830532074 and batch: 650, loss is 4.4536654663085935 and perplexity is 85.94138261055399
At time: 153.72311282157898 and batch: 700, loss is 4.531863079071045 and perplexity is 92.93153870138052
At time: 154.23848628997803 and batch: 750, loss is 4.461734647750855 and perplexity is 86.63766465531015
At time: 154.7536289691925 and batch: 800, loss is 4.546353883743286 and perplexity is 94.28799581811316
At time: 155.26860761642456 and batch: 850, loss is 4.469960622787475 and perplexity is 87.35328321691868
At time: 155.7835931777954 and batch: 900, loss is 4.476033573150635 and perplexity is 87.88538946199012
At time: 156.29885530471802 and batch: 950, loss is 4.450198831558228 and perplexity is 85.64397103360233
At time: 156.81408286094666 and batch: 1000, loss is 4.376359844207764 and perplexity is 79.5479388403797
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.937069776581555 and perplexity of 139.36129096418458
Finished 14 epochs...
Completing Train Step...
At time: 158.29565691947937 and batch: 50, loss is 4.611568231582641 and perplexity is 100.64185568088266
At time: 158.8203570842743 and batch: 100, loss is 4.52845326423645 and perplexity is 92.61519899852306
At time: 159.33034300804138 and batch: 150, loss is 4.560271730422974 and perplexity is 95.60945629860836
At time: 159.84272742271423 and batch: 200, loss is 4.583970994949341 and perplexity is 97.90239323517224
At time: 160.35468769073486 and batch: 250, loss is 4.607444067001342 and perplexity is 100.22764682413268
At time: 160.86495184898376 and batch: 300, loss is 4.522129106521606 and perplexity is 92.0313340462564
At time: 161.37389612197876 and batch: 350, loss is 4.519084177017212 and perplexity is 91.7515313278513
At time: 161.8851625919342 and batch: 400, loss is 4.510909461975098 and perplexity is 91.00454606024988
At time: 162.39739894866943 and batch: 450, loss is 4.547159481048584 and perplexity is 94.36398457752638
At time: 162.92147612571716 and batch: 500, loss is 4.5847820377349855 and perplexity is 97.98182847323707
At time: 163.4329342842102 and batch: 550, loss is 4.523101043701172 and perplexity is 92.1208262048341
At time: 163.94387364387512 and batch: 600, loss is 4.457345714569092 and perplexity is 86.25825095385126
At time: 164.45525312423706 and batch: 650, loss is 4.450577936172485 and perplexity is 85.67644521337125
At time: 164.9670009613037 and batch: 700, loss is 4.528214044570923 and perplexity is 92.59304627138553
At time: 165.4784369468689 and batch: 750, loss is 4.458862285614014 and perplexity is 86.38916696625826
At time: 165.99013495445251 and batch: 800, loss is 4.54404200553894 and perplexity is 94.0702652359203
At time: 166.5027904510498 and batch: 850, loss is 4.468161029815674 and perplexity is 87.19622422589269
At time: 167.014657497406 and batch: 900, loss is 4.474551963806152 and perplexity is 87.75527406161834
At time: 167.52568888664246 and batch: 950, loss is 4.448488874435425 and perplexity is 85.49764865345966
At time: 168.0372724533081 and batch: 1000, loss is 4.374172477722168 and perplexity is 79.37412850771096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.936958499071075 and perplexity of 139.34578404947027
Finished 15 epochs...
Completing Train Step...
At time: 169.52989602088928 and batch: 50, loss is 4.606752014160156 and perplexity is 100.15830799221783
At time: 170.04162645339966 and batch: 100, loss is 4.523108968734741 and perplexity is 92.12155626836707
At time: 170.55299997329712 and batch: 150, loss is 4.554770374298096 and perplexity is 95.08491878735204
At time: 171.06475830078125 and batch: 200, loss is 4.578290443420411 and perplexity is 97.34783024865459
At time: 171.57486295700073 and batch: 250, loss is 4.603241090774536 and perplexity is 99.80727642947011
At time: 172.08723092079163 and batch: 300, loss is 4.517800445556641 and perplexity is 91.63382256991304
At time: 172.5974085330963 and batch: 350, loss is 4.514182500839233 and perplexity is 91.30289546456977
At time: 173.10914492607117 and batch: 400, loss is 4.506103973388672 and perplexity is 90.56827384298387
At time: 173.62081694602966 and batch: 450, loss is 4.542467975616455 and perplexity is 93.92231229534767
At time: 174.1324098110199 and batch: 500, loss is 4.580539712905884 and perplexity is 97.56703818915736
At time: 174.6443588733673 and batch: 550, loss is 4.519480304718018 and perplexity is 91.78788385064688
At time: 175.1557741165161 and batch: 600, loss is 4.4536686038970945 and perplexity is 85.94165225967085
At time: 175.6674768924713 and batch: 650, loss is 4.447460317611695 and perplexity is 85.40975467324962
At time: 176.19235157966614 and batch: 700, loss is 4.525117654800415 and perplexity is 92.30678552622555
At time: 176.7039713859558 and batch: 750, loss is 4.456690845489502 and perplexity is 86.20178158448209
At time: 177.2163119316101 and batch: 800, loss is 4.542121963500977 and perplexity is 93.88981965912677
At time: 177.72885942459106 and batch: 850, loss is 4.465991878509522 and perplexity is 87.00728741257167
At time: 178.24139165878296 and batch: 900, loss is 4.472671356201172 and perplexity is 87.59039590998735
At time: 178.7531383037567 and batch: 950, loss is 4.446242475509644 and perplexity is 85.30580238966446
At time: 179.26513981819153 and batch: 1000, loss is 4.371814193725586 and perplexity is 79.18716231704735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.936513016863567 and perplexity of 139.28372180682382
Finished 16 epochs...
Completing Train Step...
At time: 180.76115822792053 and batch: 50, loss is 4.602110328674317 and perplexity is 99.69448192784981
At time: 181.27605080604553 and batch: 100, loss is 4.518874883651733 and perplexity is 91.73233035046056
At time: 181.7916259765625 and batch: 150, loss is 4.5500412273406985 and perplexity is 94.63630983788696
At time: 182.30793952941895 and batch: 200, loss is 4.573167972564697 and perplexity is 96.85044383664437
At time: 182.82330012321472 and batch: 250, loss is 4.599257192611694 and perplexity is 99.41044539633312
At time: 183.33628153800964 and batch: 300, loss is 4.513951206207276 and perplexity is 91.2817800370033
At time: 183.84716701507568 and batch: 350, loss is 4.509629497528076 and perplexity is 90.88813799176259
At time: 184.36132669448853 and batch: 400, loss is 4.501920871734619 and perplexity is 90.19020884032781
At time: 184.87681460380554 and batch: 450, loss is 4.538091058731079 and perplexity is 93.51212048333326
At time: 185.39276146888733 and batch: 500, loss is 4.576140308380127 and perplexity is 97.13874412998832
At time: 185.9079942703247 and batch: 550, loss is 4.515770692825317 and perplexity is 91.44801720153865
At time: 186.4238522052765 and batch: 600, loss is 4.450248651504516 and perplexity is 85.64823791792624
At time: 186.9389786720276 and batch: 650, loss is 4.444191303253174 and perplexity is 85.13100482572926
At time: 187.45455026626587 and batch: 700, loss is 4.521533260345459 and perplexity is 91.9765138615998
At time: 187.97022938728333 and batch: 750, loss is 4.453972482681275 and perplexity is 85.9677720728975
At time: 188.48593401908875 and batch: 800, loss is 4.539789867401123 and perplexity is 93.67111469650101
At time: 189.01449036598206 and batch: 850, loss is 4.463985624313355 and perplexity is 86.8329036646581
At time: 189.53078055381775 and batch: 900, loss is 4.470921297073364 and perplexity is 87.43724159176196
At time: 190.04637098312378 and batch: 950, loss is 4.443606595993042 and perplexity is 85.0812426587133
At time: 190.56152081489563 and batch: 1000, loss is 4.369604797363281 and perplexity is 79.01239961980757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.936447887885861 and perplexity of 139.27465069581075
Finished 17 epochs...
Completing Train Step...
At time: 192.04365348815918 and batch: 50, loss is 4.598252143859863 and perplexity is 99.3105832438405
At time: 192.56994223594666 and batch: 100, loss is 4.514653367996216 and perplexity is 91.34589712262118
At time: 193.0824887752533 and batch: 150, loss is 4.5455506324768065 and perplexity is 94.21228927581696
At time: 193.59513926506042 and batch: 200, loss is 4.568586072921753 and perplexity is 96.40769990132287
At time: 194.10748386383057 and batch: 250, loss is 4.595802412033081 and perplexity is 99.06759669479042
At time: 194.61932158470154 and batch: 300, loss is 4.510343046188354 and perplexity is 90.95301424429067
At time: 195.1296353340149 and batch: 350, loss is 4.505566930770874 and perplexity is 90.5196478783861
At time: 195.6406865119934 and batch: 400, loss is 4.497646589279174 and perplexity is 89.80553310505418
At time: 196.1530466079712 and batch: 450, loss is 4.5338531017303465 and perplexity is 93.11665870456798
At time: 196.6648530960083 and batch: 500, loss is 4.5722354125976565 and perplexity is 96.76016709071439
At time: 197.1770703792572 and batch: 550, loss is 4.5121505832672115 and perplexity is 91.11756385993563
At time: 197.6896870136261 and batch: 600, loss is 4.447087326049805 and perplexity is 85.37790349593287
At time: 198.20333576202393 and batch: 650, loss is 4.44132794380188 and perplexity is 84.88759281281494
At time: 198.71572995185852 and batch: 700, loss is 4.518268003463745 and perplexity is 91.67667670582811
At time: 199.2278254032135 and batch: 750, loss is 4.451081972122193 and perplexity is 85.71964010677702
At time: 199.7399091720581 and batch: 800, loss is 4.53713680267334 and perplexity is 93.42292853863495
At time: 200.2531168460846 and batch: 850, loss is 4.461596908569336 and perplexity is 86.62573207610258
At time: 200.77254581451416 and batch: 900, loss is 4.468818073272705 and perplexity is 87.25353476019215
At time: 201.28513765335083 and batch: 950, loss is 4.440973949432373 and perplexity is 84.85754840100832
At time: 201.7985918521881 and batch: 1000, loss is 4.366872959136963 and perplexity is 78.79684509028071
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.936630993354611 and perplexity of 139.30015498092718
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 203.31349539756775 and batch: 50, loss is 4.595565757751465 and perplexity is 99.04415469779703
At time: 203.8258113861084 and batch: 100, loss is 4.50802077293396 and perplexity is 90.74204155483547
At time: 204.33845853805542 and batch: 150, loss is 4.537438325881958 and perplexity is 93.45110196706423
At time: 204.85097575187683 and batch: 200, loss is 4.559363508224488 and perplexity is 95.5226610886472
At time: 205.3625943660736 and batch: 250, loss is 4.586955461502075 and perplexity is 98.19501609758774
At time: 205.87641286849976 and batch: 300, loss is 4.495864562988281 and perplexity is 89.6456397933492
At time: 206.38558387756348 and batch: 350, loss is 4.489920501708984 and perplexity is 89.11436115763378
At time: 206.89759850502014 and batch: 400, loss is 4.479280004501343 and perplexity is 88.17116697315288
At time: 207.40902376174927 and batch: 450, loss is 4.518394079208374 and perplexity is 91.6882356397442
At time: 207.92044162750244 and batch: 500, loss is 4.552238941192627 and perplexity is 94.84452207857133
At time: 208.43290829658508 and batch: 550, loss is 4.491975059509278 and perplexity is 89.29763997746132
At time: 208.9451551437378 and batch: 600, loss is 4.422484827041626 and perplexity is 83.30302200128902
At time: 209.457777261734 and batch: 650, loss is 4.4141126537323 and perplexity is 82.60850602470214
At time: 209.97056221961975 and batch: 700, loss is 4.487145347595215 and perplexity is 88.8673979107051
At time: 210.48299908638 and batch: 750, loss is 4.423079452514648 and perplexity is 83.35257083018868
At time: 210.99497199058533 and batch: 800, loss is 4.503396825790405 and perplexity is 90.32342373020303
At time: 211.50759935379028 and batch: 850, loss is 4.432123403549195 and perplexity is 84.10982652959942
At time: 212.0198884010315 and batch: 900, loss is 4.428699111938476 and perplexity is 83.82230252021512
At time: 212.53197360038757 and batch: 950, loss is 4.400083131790161 and perplexity is 81.45764010294742
At time: 213.0438003540039 and batch: 1000, loss is 4.32571964263916 and perplexity is 75.61991259666785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.921893608279344 and perplexity of 137.26228825292435
Finished 19 epochs...
Completing Train Step...
At time: 214.53598475456238 and batch: 50, loss is 4.582543687820435 and perplexity is 97.76275612759606
At time: 215.0476038455963 and batch: 100, loss is 4.500432109832763 and perplexity is 90.05603699323473
At time: 215.57257533073425 and batch: 150, loss is 4.5301123046875 and perplexity is 92.76897888830031
At time: 216.1008460521698 and batch: 200, loss is 4.553274240493774 and perplexity is 94.94276539283909
At time: 216.61640071868896 and batch: 250, loss is 4.581171178817749 and perplexity is 97.62866790437978
At time: 217.12910771369934 and batch: 300, loss is 4.489171628952026 and perplexity is 89.04765082220318
At time: 217.6379554271698 and batch: 350, loss is 4.484230489730835 and perplexity is 88.60873923679188
At time: 218.1502285003662 and batch: 400, loss is 4.4752687644958495 and perplexity is 87.81819965244947
At time: 218.6629822254181 and batch: 450, loss is 4.515126075744629 and perplexity is 91.3890872433249
At time: 219.17533564567566 and batch: 500, loss is 4.548924798965454 and perplexity is 94.53071413224455
At time: 219.68641424179077 and batch: 550, loss is 4.488362035751343 and perplexity is 88.97558762443425
At time: 220.1985468864441 and batch: 600, loss is 4.420137166976929 and perplexity is 83.10768420635527
At time: 220.71105194091797 and batch: 650, loss is 4.411860485076904 and perplexity is 82.422667085633
At time: 221.2223834991455 and batch: 700, loss is 4.485897712707519 and perplexity is 88.75659298111411
At time: 221.73532485961914 and batch: 750, loss is 4.423098707199097 and perplexity is 83.35417577308928
At time: 222.2476418018341 and batch: 800, loss is 4.503381853103638 and perplexity is 90.32207135599609
At time: 222.75995993614197 and batch: 850, loss is 4.432540864944458 and perplexity is 84.14494646523642
At time: 223.27205324172974 and batch: 900, loss is 4.429410228729248 and perplexity is 83.88193116593118
At time: 223.78462314605713 and batch: 950, loss is 4.402017393112183 and perplexity is 81.61535294533539
At time: 224.29744386672974 and batch: 1000, loss is 4.328198394775391 and perplexity is 75.80758812101868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.92088764469798 and perplexity of 137.12427681872404
Finished 20 epochs...
Completing Train Step...
At time: 225.77970480918884 and batch: 50, loss is 4.578501205444336 and perplexity is 97.3683496366605
At time: 226.308851480484 and batch: 100, loss is 4.4974516582489015 and perplexity is 89.78802892607105
At time: 226.8243052959442 and batch: 150, loss is 4.527516355514527 and perplexity is 92.52846764682296
At time: 227.3396201133728 and batch: 200, loss is 4.550576467514038 and perplexity is 94.68697655098931
At time: 227.8549325466156 and batch: 250, loss is 4.578336896896363 and perplexity is 97.35235249878234
At time: 228.38339495658875 and batch: 300, loss is 4.486133270263672 and perplexity is 88.77750272987711
At time: 228.89632749557495 and batch: 350, loss is 4.481349334716797 and perplexity is 88.35381114347702
At time: 229.4100160598755 and batch: 400, loss is 4.473294715881348 and perplexity is 87.64501325252432
At time: 229.92544984817505 and batch: 450, loss is 4.513743600845337 and perplexity is 91.26283141700591
At time: 230.44031262397766 and batch: 500, loss is 4.547530565261841 and perplexity is 94.39900806043204
At time: 230.9551706314087 and batch: 550, loss is 4.486458778381348 and perplexity is 88.80640523142695
At time: 231.46872758865356 and batch: 600, loss is 4.418926401138306 and perplexity is 83.00712115283112
At time: 231.98632764816284 and batch: 650, loss is 4.410603742599488 and perplexity is 82.31914808079469
At time: 232.50764441490173 and batch: 700, loss is 4.485446214675903 and perplexity is 88.71652859926556
At time: 233.02295660972595 and batch: 750, loss is 4.423460512161255 and perplexity is 83.38433918380738
At time: 233.53812861442566 and batch: 800, loss is 4.503524742126465 and perplexity is 90.33497831062098
At time: 234.05366396903992 and batch: 850, loss is 4.432977666854859 and perplexity is 84.18170916702769
At time: 234.56847167015076 and batch: 900, loss is 4.429837923049927 and perplexity is 83.91781466453575
At time: 235.08411717414856 and batch: 950, loss is 4.403128538131714 and perplexity is 81.70608983986648
At time: 235.59981298446655 and batch: 1000, loss is 4.329282894134521 and perplexity is 75.88984599799838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.920451466630145 and perplexity of 137.06447925874815
Finished 21 epochs...
Completing Train Step...
At time: 237.0816125869751 and batch: 50, loss is 4.575784387588501 and perplexity is 97.10417658329924
At time: 237.61014103889465 and batch: 100, loss is 4.495364856719971 and perplexity is 89.60085449589666
At time: 238.12780475616455 and batch: 150, loss is 4.525868873596192 and perplexity is 92.3761541707432
At time: 238.6397635936737 and batch: 200, loss is 4.54883002281189 and perplexity is 94.52175529931397
At time: 239.15145087242126 and batch: 250, loss is 4.576339244842529 and perplexity is 97.15807049040286
At time: 239.66360664367676 and batch: 300, loss is 4.483959531784057 and perplexity is 88.58473324719543
At time: 240.17248368263245 and batch: 350, loss is 4.479216356277465 and perplexity is 88.16555521356902
At time: 240.68288683891296 and batch: 400, loss is 4.472051820755005 and perplexity is 87.53614736116194
At time: 241.1941273212433 and batch: 450, loss is 4.512911109924317 and perplexity is 91.18688755409886
At time: 241.71888947486877 and batch: 500, loss is 4.546616516113281 and perplexity is 94.31276214999478
At time: 242.22967100143433 and batch: 550, loss is 4.485089082717895 and perplexity is 88.68485074862232
At time: 242.7416410446167 and batch: 600, loss is 4.418056392669678 and perplexity is 82.93493566001999
At time: 243.25299263000488 and batch: 650, loss is 4.409730787277222 and perplexity is 82.24731849892254
At time: 243.76390624046326 and batch: 700, loss is 4.485103702545166 and perplexity is 88.68614731529955
At time: 244.27637124061584 and batch: 750, loss is 4.423676671981812 and perplexity is 83.40236547581243
At time: 244.78755617141724 and batch: 800, loss is 4.503717155456543 and perplexity is 90.35236163695846
At time: 245.29943919181824 and batch: 850, loss is 4.4332350158691405 and perplexity is 84.20337603475633
At time: 245.81099271774292 and batch: 900, loss is 4.43000171661377 and perplexity is 83.9315609882185
At time: 246.3220820426941 and batch: 950, loss is 4.4038511085510255 and perplexity is 81.76514957830271
At time: 246.83393168449402 and batch: 1000, loss is 4.329719591140747 and perplexity is 75.92299410386114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.920230400271532 and perplexity of 137.03418226236988
Finished 22 epochs...
Completing Train Step...
At time: 248.32675528526306 and batch: 50, loss is 4.573655881881714 and perplexity is 96.89770960031507
At time: 248.83875250816345 and batch: 100, loss is 4.493609027862549 and perplexity is 89.4436687659262
At time: 249.3509771823883 and batch: 150, loss is 4.524554204940796 and perplexity is 92.25478993071007
At time: 249.86239576339722 and batch: 200, loss is 4.547507047653198 and perplexity is 94.39678804760901
At time: 250.37506651878357 and batch: 250, loss is 4.574724159240723 and perplexity is 97.00127853994799
At time: 250.88596200942993 and batch: 300, loss is 4.482127771377564 and perplexity is 88.42261576573532
At time: 251.39651823043823 and batch: 350, loss is 4.477463178634643 and perplexity is 88.01112074838363
At time: 251.90734839439392 and batch: 400, loss is 4.471092824935913 and perplexity is 87.45224080127379
At time: 252.41846084594727 and batch: 450, loss is 4.512276029586792 and perplexity is 91.12899493994942
At time: 252.93014287948608 and batch: 500, loss is 4.545852870941162 and perplexity is 94.24076815694094
At time: 253.44243836402893 and batch: 550, loss is 4.4839961719512935 and perplexity is 88.58797906609949
At time: 253.95411252975464 and batch: 600, loss is 4.417373847961426 and perplexity is 82.87834817249794
At time: 254.47914695739746 and batch: 650, loss is 4.408956546783447 and perplexity is 82.18366393958479
At time: 254.99074268341064 and batch: 700, loss is 4.484717264175415 and perplexity is 88.6518822062144
At time: 255.5018174648285 and batch: 750, loss is 4.423852586746216 and perplexity is 83.41703847384652
At time: 256.01393723487854 and batch: 800, loss is 4.50377872467041 and perplexity is 90.35792473209149
At time: 256.52647709846497 and batch: 850, loss is 4.433379516601563 and perplexity is 84.21554436341081
At time: 257.0383520126343 and batch: 900, loss is 4.429966192245484 and perplexity is 83.9285794254945
At time: 257.55014634132385 and batch: 950, loss is 4.404300384521484 and perplexity is 81.80189294856783
At time: 258.06236028671265 and batch: 1000, loss is 4.329851026535034 and perplexity is 75.93297372835023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.920155967154154 and perplexity of 137.0239827605919
Finished 23 epochs...
Completing Train Step...
At time: 259.54341983795166 and batch: 50, loss is 4.571840600967407 and perplexity is 96.72197259171445
At time: 260.07214736938477 and batch: 100, loss is 4.492189016342163 and perplexity is 89.31674786175334
At time: 260.59086537361145 and batch: 150, loss is 4.523492746353149 and perplexity is 92.15691724477743
At time: 261.1113212108612 and batch: 200, loss is 4.546424112319946 and perplexity is 94.29461776237771
At time: 261.62666845321655 and batch: 250, loss is 4.5733251953125 and perplexity is 96.86567212663574
At time: 262.1419951915741 and batch: 300, loss is 4.480504989624023 and perplexity is 88.2792415222872
At time: 262.6552917957306 and batch: 350, loss is 4.475923051834107 and perplexity is 87.87567678978174
At time: 263.16986751556396 and batch: 400, loss is 4.470284175872803 and perplexity is 87.38155121406926
At time: 263.68430399894714 and batch: 450, loss is 4.511724863052368 and perplexity is 91.0787815268705
At time: 264.1992688179016 and batch: 500, loss is 4.545216131210327 and perplexity is 94.18078041590684
At time: 264.7156388759613 and batch: 550, loss is 4.483007440567016 and perplexity is 88.50043263800971
At time: 265.2303419113159 and batch: 600, loss is 4.416795740127563 and perplexity is 82.83044939682456
At time: 265.7449474334717 and batch: 650, loss is 4.408204555511475 and perplexity is 82.12188577283587
At time: 266.25978326797485 and batch: 700, loss is 4.484362077713013 and perplexity is 88.6203998491728
At time: 266.7754032611847 and batch: 750, loss is 4.423949737548828 and perplexity is 83.4251428997551
At time: 267.29057121276855 and batch: 800, loss is 4.503780784606934 and perplexity is 90.35811086387254
At time: 267.8268885612488 and batch: 850, loss is 4.433404502868652 and perplexity is 84.21764862178401
At time: 268.3417057991028 and batch: 900, loss is 4.429837789535522 and perplexity is 83.91780346029942
At time: 268.85741209983826 and batch: 950, loss is 4.404572887420654 and perplexity is 81.82418723904534
At time: 269.3728611469269 and batch: 1000, loss is 4.329754028320313 and perplexity is 75.92560872266206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.920044317478087 and perplexity of 137.008684931318
Finished 24 epochs...
Completing Train Step...
At time: 270.85715913772583 and batch: 50, loss is 4.5701956367492675 and perplexity is 96.562999196314
At time: 271.38385939598083 and batch: 100, loss is 4.490955324172973 and perplexity is 89.20662643127908
At time: 271.89723229408264 and batch: 150, loss is 4.522573642730713 and perplexity is 92.0722544012282
At time: 272.41026854515076 and batch: 200, loss is 4.545460195541382 and perplexity is 94.20376939035746
At time: 272.92294454574585 and batch: 250, loss is 4.572060403823852 and perplexity is 96.74323469422092
At time: 273.43569135665894 and batch: 300, loss is 4.479161424636841 and perplexity is 88.16071226799124
At time: 273.94631123542786 and batch: 350, loss is 4.474521350860596 and perplexity is 87.75258765531083
At time: 274.45912408828735 and batch: 400, loss is 4.4695742893218995 and perplexity is 87.31954223833833
At time: 274.971773147583 and batch: 450, loss is 4.511236371994019 and perplexity is 91.0343012214895
At time: 275.4841721057892 and batch: 500, loss is 4.544676609039307 and perplexity is 94.12998150159098
At time: 275.9961938858032 and batch: 550, loss is 4.482173881530762 and perplexity is 88.42669304009559
At time: 276.50863671302795 and batch: 600, loss is 4.416280975341797 and perplexity is 82.78782217072464
At time: 277.0210566520691 and batch: 650, loss is 4.407474231719971 and perplexity is 82.06193210131397
At time: 277.53354930877686 and batch: 700, loss is 4.484010171890259 and perplexity is 88.58921930108109
At time: 278.0467960834503 and batch: 750, loss is 4.423930521011353 and perplexity is 83.42353977277352
At time: 278.5595703125 and batch: 800, loss is 4.503757972717285 and perplexity is 90.3560496481288
At time: 279.07221007347107 and batch: 850, loss is 4.433385572433472 and perplexity is 84.21605436013581
At time: 279.5836055278778 and batch: 900, loss is 4.429676837921143 and perplexity is 83.90429784126205
At time: 280.0949878692627 and batch: 950, loss is 4.404752082824707 and perplexity is 81.83885107114538
At time: 280.621120929718 and batch: 1000, loss is 4.329545669555664 and perplexity is 75.90979060460238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.920017521555831 and perplexity of 137.0050137064353
Finished 25 epochs...
Completing Train Step...
At time: 282.1146237850189 and batch: 50, loss is 4.5687459945678714 and perplexity is 96.42311881226554
At time: 282.6272714138031 and batch: 100, loss is 4.489842329025269 and perplexity is 89.10739512114493
At time: 283.13978695869446 and batch: 150, loss is 4.52173957824707 and perplexity is 91.99549222065752
At time: 283.65238189697266 and batch: 200, loss is 4.544487733840942 and perplexity is 94.11220436154605
At time: 284.1648473739624 and batch: 250, loss is 4.570961618423462 and perplexity is 96.63699301944206
At time: 284.6775782108307 and batch: 300, loss is 4.477940807342529 and perplexity is 88.05316742681727
At time: 285.1881699562073 and batch: 350, loss is 4.473251428604126 and perplexity is 87.64121942065142
At time: 285.69900918006897 and batch: 400, loss is 4.4689410781860355 and perplexity is 87.26426803378227
At time: 286.2111141681671 and batch: 450, loss is 4.510793342590332 and perplexity is 90.9939792818662
At time: 286.72305822372437 and batch: 500, loss is 4.544105110168457 and perplexity is 94.07620169246351
At time: 287.23429012298584 and batch: 550, loss is 4.48142373085022 and perplexity is 88.36038456991497
At time: 287.7462694644928 and batch: 600, loss is 4.415819253921509 and perplexity is 82.74960608316142
At time: 288.25834560394287 and batch: 650, loss is 4.406734304428101 and perplexity is 82.00123469673099
At time: 288.7703101634979 and batch: 700, loss is 4.483480224609375 and perplexity is 88.5422841228803
At time: 289.28333735466003 and batch: 750, loss is 4.423900156021118 and perplexity is 83.42100665626225
At time: 289.7952127456665 and batch: 800, loss is 4.503664379119873 and perplexity is 90.3475932961307
At time: 290.3087012767792 and batch: 850, loss is 4.433303384780884 and perplexity is 84.20913312474171
At time: 290.8214781284332 and batch: 900, loss is 4.429455766677856 and perplexity is 83.88575106397629
At time: 291.33360266685486 and batch: 950, loss is 4.404782657623291 and perplexity is 81.84135331578581
At time: 291.84899401664734 and batch: 1000, loss is 4.329278774261475 and perplexity is 75.88953334211142
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.920008589581745 and perplexity of 137.00378998666838
Finished 26 epochs...
Completing Train Step...
At time: 293.3397297859192 and batch: 50, loss is 4.567398138046265 and perplexity is 96.29324183017194
At time: 293.86536955833435 and batch: 100, loss is 4.488840599060058 and perplexity is 89.01817826638911
At time: 294.37775444984436 and batch: 150, loss is 4.52096378326416 and perplexity is 91.9241502562896
At time: 294.89084243774414 and batch: 200, loss is 4.543611841201782 and perplexity is 94.02980826482576
At time: 295.4034595489502 and batch: 250, loss is 4.569926080703735 and perplexity is 96.53697356394673
At time: 295.91660928726196 and batch: 300, loss is 4.476777925491333 and perplexity is 87.95083151029864
At time: 296.42787528038025 and batch: 350, loss is 4.472042474746704 and perplexity is 87.53532925142513
At time: 296.93796277046204 and batch: 400, loss is 4.468378429412842 and perplexity is 87.21518271062277
At time: 297.44999599456787 and batch: 450, loss is 4.510373134613037 and perplexity is 90.9557509183803
At time: 297.96248602867126 and batch: 500, loss is 4.543574123382569 and perplexity is 94.02626173240108
At time: 298.4743621349335 and batch: 550, loss is 4.480678730010986 and perplexity is 88.29458052433354
At time: 298.9868793487549 and batch: 600, loss is 4.41535641670227 and perplexity is 82.71131534746117
At time: 299.5166826248169 and batch: 650, loss is 4.406084985733032 and perplexity is 81.94800704474858
At time: 300.03456830978394 and batch: 700, loss is 4.4831107711791995 and perplexity is 88.50957791437804
At time: 300.5467686653137 and batch: 750, loss is 4.423799753189087 and perplexity is 83.41263137140123
At time: 301.05833983421326 and batch: 800, loss is 4.5035177516937255 and perplexity is 90.33434683223825
At time: 301.5709979534149 and batch: 850, loss is 4.433236865997315 and perplexity is 84.20353182193881
At time: 302.0836229324341 and batch: 900, loss is 4.429242639541626 and perplexity is 83.8678746391248
At time: 302.5963251590729 and batch: 950, loss is 4.40479868888855 and perplexity is 81.84266534674673
At time: 303.1081864833832 and batch: 1000, loss is 4.329023447036743 and perplexity is 75.87015915166137
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.920003007097942 and perplexity of 137.0030251673646
Finished 27 epochs...
Completing Train Step...
At time: 304.5876770019531 and batch: 50, loss is 4.56620924949646 and perplexity is 96.17882792370467
At time: 305.1154274940491 and batch: 100, loss is 4.4879436206817624 and perplexity is 88.9383666851914
At time: 305.63058853149414 and batch: 150, loss is 4.520259675979614 and perplexity is 91.85944857360452
At time: 306.14539074897766 and batch: 200, loss is 4.542888498306274 and perplexity is 93.96181706449181
At time: 306.6598889827728 and batch: 250, loss is 4.5689597415924075 and perplexity is 96.44373116984472
At time: 307.1876916885376 and batch: 300, loss is 4.475753755569458 and perplexity is 87.86080102518896
At time: 307.7029564380646 and batch: 350, loss is 4.470903463363648 and perplexity is 87.43568227528127
At time: 308.2158958911896 and batch: 400, loss is 4.4678354072570805 and perplexity is 87.1678357904689
At time: 308.73072123527527 and batch: 450, loss is 4.509966459274292 and perplexity is 90.91876897789606
At time: 309.2461631298065 and batch: 500, loss is 4.543093852996826 and perplexity is 93.98111454570511
At time: 309.76156425476074 and batch: 550, loss is 4.480011892318726 and perplexity is 88.23572199674102
At time: 310.2767553329468 and batch: 600, loss is 4.414929294586182 and perplexity is 82.67599505900067
At time: 310.79267501831055 and batch: 650, loss is 4.4055224609375 and perplexity is 81.90192222198448
At time: 311.3075442314148 and batch: 700, loss is 4.4827217102050785 and perplexity is 88.47514898968541
At time: 311.8228702545166 and batch: 750, loss is 4.423709726333618 and perplexity is 83.40512233250512
At time: 312.33875155448914 and batch: 800, loss is 4.503366994857788 and perplexity is 90.32072933842427
At time: 312.85406494140625 and batch: 850, loss is 4.433135614395142 and perplexity is 84.19500651104121
At time: 313.3696618080139 and batch: 900, loss is 4.429037933349609 and perplexity is 83.85070812298089
At time: 313.88523030281067 and batch: 950, loss is 4.404710645675659 and perplexity is 81.83545997273484
At time: 314.400931596756 and batch: 1000, loss is 4.328697414398193 and perplexity is 75.84542703544359
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.9199940751238564 and perplexity of 137.0018014653592
Finished 28 epochs...
Completing Train Step...
At time: 315.8918447494507 and batch: 50, loss is 4.565015735626221 and perplexity is 96.06410563349499
At time: 316.40361428260803 and batch: 100, loss is 4.487101860046387 and perplexity is 88.86353336942959
At time: 316.91633105278015 and batch: 150, loss is 4.519542760848999 and perplexity is 91.79361674576855
At time: 317.42807364463806 and batch: 200, loss is 4.542215194702148 and perplexity is 93.8985735278519
At time: 317.9398491382599 and batch: 250, loss is 4.5680281352996825 and perplexity is 96.3539254212884
At time: 318.45203924179077 and batch: 300, loss is 4.474735956192017 and perplexity is 87.77142184935461
At time: 318.9630551338196 and batch: 350, loss is 4.469831552505493 and perplexity is 87.34200923160569
At time: 319.47062611579895 and batch: 400, loss is 4.467339134216308 and perplexity is 87.12458747591754
At time: 319.9956440925598 and batch: 450, loss is 4.50960054397583 and perplexity is 90.8855064954065
At time: 320.50722002983093 and batch: 500, loss is 4.5426270866394045 and perplexity is 93.93725755948132
At time: 321.0286250114441 and batch: 550, loss is 4.479364547729492 and perplexity is 88.17862156335164
At time: 321.54181480407715 and batch: 600, loss is 4.414519453048706 and perplexity is 82.64211794467188
At time: 322.05418515205383 and batch: 650, loss is 4.4049562644958495 and perplexity is 81.85556277057475
At time: 322.56566882133484 and batch: 700, loss is 4.482390174865722 and perplexity is 88.44582121300637
At time: 323.07791113853455 and batch: 750, loss is 4.423538885116577 and perplexity is 83.39087451699012
At time: 323.5892918109894 and batch: 800, loss is 4.503193674087524 and perplexity is 90.30507623658755
At time: 324.1000485420227 and batch: 850, loss is 4.432977342605591 and perplexity is 84.18168187117458
At time: 324.61185932159424 and batch: 900, loss is 4.428790864944458 and perplexity is 83.82999382128422
At time: 325.1236414909363 and batch: 950, loss is 4.404585733413696 and perplexity is 81.82523835873658
At time: 325.63543486595154 and batch: 1000, loss is 4.328329963684082 and perplexity is 75.81756269881606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.919992958627096 and perplexity of 137.00164850337703
Finished 29 epochs...
Completing Train Step...
At time: 327.11407256126404 and batch: 50, loss is 4.563944101333618 and perplexity is 95.9612151839236
At time: 327.63871908187866 and batch: 100, loss is 4.486310548782349 and perplexity is 88.79324246917055
At time: 328.1512897014618 and batch: 150, loss is 4.518845434188843 and perplexity is 91.72962892238004
At time: 328.6643784046173 and batch: 200, loss is 4.541595497131348 and perplexity is 93.84040283591486
At time: 329.17681431770325 and batch: 250, loss is 4.56713041305542 and perplexity is 96.26746517356604
At time: 329.6899449825287 and batch: 300, loss is 4.473792963027954 and perplexity is 87.68869301104552
At time: 330.20138692855835 and batch: 350, loss is 4.468820314407349 and perplexity is 87.25373030733084
At time: 330.7110815048218 and batch: 400, loss is 4.46682144165039 and perplexity is 87.07949539759655
At time: 331.22237968444824 and batch: 450, loss is 4.509273357391358 and perplexity is 90.85577484112264
At time: 331.7348952293396 and batch: 500, loss is 4.542160339355469 and perplexity is 93.89342283032127
At time: 332.24735951423645 and batch: 550, loss is 4.478716173171997 and perplexity is 88.12146731929607
At time: 332.7590742111206 and batch: 600, loss is 4.414098138809204 and perplexity is 82.60730697729223
At time: 333.28422451019287 and batch: 650, loss is 4.404419469833374 and perplexity is 81.81163493255487
At time: 333.79704570770264 and batch: 700, loss is 4.481998281478882 and perplexity is 88.41116667146565
At time: 334.30907464027405 and batch: 750, loss is 4.423392820358276 and perplexity is 83.37869493858454
At time: 334.82007789611816 and batch: 800, loss is 4.5030210494995115 and perplexity is 90.28948870544106
At time: 335.33247423171997 and batch: 850, loss is 4.432764329910278 and perplexity is 84.16375201393464
At time: 335.845577955246 and batch: 900, loss is 4.428529109954834 and perplexity is 83.80805377370511
At time: 336.35814023017883 and batch: 950, loss is 4.404401941299438 and perplexity is 81.81020090710385
At time: 336.8708438873291 and batch: 1000, loss is 4.327907485961914 and perplexity is 75.78553823291477
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.919930062642911 and perplexity of 136.9930319208366
Finished 30 epochs...
Completing Train Step...
At time: 338.3639106750488 and batch: 50, loss is 4.562919692993164 and perplexity is 95.86296204898687
At time: 338.8926742076874 and batch: 100, loss is 4.485579032897949 and perplexity is 88.72831255340476
At time: 339.4081063270569 and batch: 150, loss is 4.518157844543457 and perplexity is 91.66657825833387
At time: 339.92380833625793 and batch: 200, loss is 4.540921926498413 and perplexity is 93.77721597917636
At time: 340.4398822784424 and batch: 250, loss is 4.566298379898071 and perplexity is 96.18740076330866
At time: 340.9548382759094 and batch: 300, loss is 4.4729464244842525 and perplexity is 87.61449256376534
At time: 341.4699692726135 and batch: 350, loss is 4.4677734375 and perplexity is 87.16243418822945
At time: 341.98321557044983 and batch: 400, loss is 4.466284627914429 and perplexity is 87.03276247290671
At time: 342.49850153923035 and batch: 450, loss is 4.508948183059692 and perplexity is 90.82623567820978
At time: 343.0138490200043 and batch: 500, loss is 4.541715240478515 and perplexity is 93.85164027264071
At time: 343.52990531921387 and batch: 550, loss is 4.477835426330566 and perplexity is 88.04388878383227
At time: 344.04616141319275 and batch: 600, loss is 4.413597249984742 and perplexity is 82.56594026133608
At time: 344.5613946914673 and batch: 650, loss is 4.403882846832276 and perplexity is 81.76774470479872
At time: 345.07718443870544 and batch: 700, loss is 4.481601371765136 and perplexity is 88.37608238371807
At time: 345.5930218696594 and batch: 750, loss is 4.423231782913208 and perplexity is 83.3652689276528
At time: 346.12166571617126 and batch: 800, loss is 4.502866497039795 and perplexity is 90.2755353211678
At time: 346.63666701316833 and batch: 850, loss is 4.432527074813843 and perplexity is 84.14378610343485
At time: 347.1519157886505 and batch: 900, loss is 4.428259601593018 and perplexity is 83.78546984584096
At time: 347.667822599411 and batch: 950, loss is 4.404305114746093 and perplexity is 81.8022798908101
At time: 348.18417716026306 and batch: 1000, loss is 4.327502040863037 and perplexity is 75.7548175860612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.919901405892721 and perplexity of 136.9891062019924
Finished 31 epochs...
Completing Train Step...
At time: 349.6813631057739 and batch: 50, loss is 4.561871709823609 and perplexity is 95.76255190143793
At time: 350.19438457489014 and batch: 100, loss is 4.484832630157471 and perplexity is 88.66211020763663
At time: 350.70834493637085 and batch: 150, loss is 4.517523708343506 and perplexity is 91.60846758972127
At time: 351.2207043170929 and batch: 200, loss is 4.540342588424682 and perplexity is 93.72290300181808
At time: 351.7327857017517 and batch: 250, loss is 4.565545644760132 and perplexity is 96.11502437047152
At time: 352.2484951019287 and batch: 300, loss is 4.472110290527343 and perplexity is 87.54126572940298
At time: 352.76101422309875 and batch: 350, loss is 4.466763944625854 and perplexity is 87.0744887296487
At time: 353.26995062828064 and batch: 400, loss is 4.4658527851104735 and perplexity is 86.9951861148483
At time: 353.78203320503235 and batch: 450, loss is 4.50855354309082 and perplexity is 90.7903990871297
At time: 354.2954161167145 and batch: 500, loss is 4.541331129074097 and perplexity is 93.81559770991144
At time: 354.80910634994507 and batch: 550, loss is 4.477398300170899 and perplexity is 88.00541090730191
At time: 355.3221209049225 and batch: 600, loss is 4.413166275024414 and perplexity is 82.5303640752809
At time: 355.8349800109863 and batch: 650, loss is 4.403271703720093 and perplexity is 81.71778817767228
At time: 356.3482847213745 and batch: 700, loss is 4.481210632324219 and perplexity is 88.34155710832962
At time: 356.8611719608307 and batch: 750, loss is 4.423014793395996 and perplexity is 83.34718150065792
At time: 357.37359142303467 and batch: 800, loss is 4.502621650695801 and perplexity is 90.25343439216853
At time: 357.8860776424408 and batch: 850, loss is 4.432218494415284 and perplexity is 84.11782498613475
At time: 358.39934635162354 and batch: 900, loss is 4.42795350074768 and perplexity is 83.75982696754774
At time: 358.91306376457214 and batch: 950, loss is 4.4041845321655275 and perplexity is 81.79241655548985
At time: 359.4397692680359 and batch: 1000, loss is 4.32717264175415 and perplexity is 75.72986812604499
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.91987163264577 and perplexity of 136.98502765222005
Finished 32 epochs...
Completing Train Step...
At time: 360.92176246643066 and batch: 50, loss is 4.560853681564331 and perplexity is 95.66511252385705
At time: 361.44631457328796 and batch: 100, loss is 4.484093513488769 and perplexity is 88.59660277589944
At time: 361.9584493637085 and batch: 150, loss is 4.516885986328125 and perplexity is 91.5500654772797
At time: 362.47092032432556 and batch: 200, loss is 4.539709014892578 and perplexity is 93.66354145806136
At time: 362.9834280014038 and batch: 250, loss is 4.5646821403503415 and perplexity is 96.03206444636552
At time: 363.4944770336151 and batch: 300, loss is 4.471260766983033 and perplexity is 87.46692894296292
At time: 364.00660014152527 and batch: 350, loss is 4.465842819213867 and perplexity is 86.99431913413834
At time: 364.51538825035095 and batch: 400, loss is 4.46537841796875 and perplexity is 86.95392824353019
At time: 365.0274302959442 and batch: 450, loss is 4.508161001205444 and perplexity is 90.75476704668968
At time: 365.53934264183044 and batch: 500, loss is 4.540872688293457 and perplexity is 93.7725986710707
At time: 366.0512344837189 and batch: 550, loss is 4.476745262145996 and perplexity is 87.94795878883296
At time: 366.5632076263428 and batch: 600, loss is 4.41274356842041 and perplexity is 82.49548531761536
At time: 367.076069355011 and batch: 650, loss is 4.402721509933472 and perplexity is 81.67283992461915
At time: 367.5877616405487 and batch: 700, loss is 4.480780448913574 and perplexity is 88.30356220896475
At time: 368.09963035583496 and batch: 750, loss is 4.4228199863433835 and perplexity is 83.33094646328743
At time: 368.6118354797363 and batch: 800, loss is 4.502362661361694 and perplexity is 90.23006274192916
At time: 369.12421774864197 and batch: 850, loss is 4.432004470825195 and perplexity is 84.09982371365797
At time: 369.63672494888306 and batch: 900, loss is 4.4276015663146975 and perplexity is 83.73035418688455
At time: 370.1496305465698 and batch: 950, loss is 4.404006900787354 and perplexity is 81.77788894613079
At time: 370.6621038913727 and batch: 1000, loss is 4.32672378540039 and perplexity is 75.69588392115094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.919869027486661 and perplexity of 136.98467078489233
Finished 33 epochs...
Completing Train Step...
At time: 372.14181113243103 and batch: 50, loss is 4.559899702072143 and perplexity is 95.57389348585525
At time: 372.66778588294983 and batch: 100, loss is 4.483341903686523 and perplexity is 88.53003771941549
At time: 373.17992973327637 and batch: 150, loss is 4.516320352554321 and perplexity is 91.49829631082653
At time: 373.69276762008667 and batch: 200, loss is 4.539134092330933 and perplexity is 93.60970765149135
At time: 374.2050530910492 and batch: 250, loss is 4.563725366592407 and perplexity is 95.9402274278134
At time: 374.7183060646057 and batch: 300, loss is 4.470539274215699 and perplexity is 87.40384494640982
At time: 375.23098969459534 and batch: 350, loss is 4.46493070602417 and perplexity is 86.91500664470989
At time: 375.7396640777588 and batch: 400, loss is 4.46493537902832 and perplexity is 86.91541279984568
At time: 376.2507243156433 and batch: 450, loss is 4.507853689193726 and perplexity is 90.72688130168721
At time: 376.7627966403961 and batch: 500, loss is 4.540510196685791 and perplexity is 93.73861305114649
At time: 377.27586460113525 and batch: 550, loss is 4.476178102493286 and perplexity is 87.89809239750905
At time: 377.79011964797974 and batch: 600, loss is 4.412352905273438 and perplexity is 82.46326366602588
At time: 378.3025150299072 and batch: 650, loss is 4.402174806594848 and perplexity is 81.62820131350816
At time: 378.81482696533203 and batch: 700, loss is 4.480445489883423 and perplexity is 88.27398908657706
At time: 379.32731771469116 and batch: 750, loss is 4.422661762237549 and perplexity is 83.31776254182957
At time: 379.8403036594391 and batch: 800, loss is 4.502167835235595 and perplexity is 90.21248528067744
At time: 380.35284900665283 and batch: 850, loss is 4.431774272918701 and perplexity is 84.08046633840168
At time: 380.86664724349976 and batch: 900, loss is 4.427254476547241 and perplexity is 83.70129728068991
At time: 381.386182308197 and batch: 950, loss is 4.403815650939942 and perplexity is 81.76225043282746
At time: 381.90004777908325 and batch: 1000, loss is 4.326284656524658 and perplexity is 75.66265097005534
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.9198854027724845 and perplexity of 136.98691396639617
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 383.3936975002289 and batch: 50, loss is 4.558775863647461 and perplexity is 95.46654420488139
At time: 383.90810918807983 and batch: 100, loss is 4.482809915542602 and perplexity is 88.48295331425099
At time: 384.4233570098877 and batch: 150, loss is 4.514849214553833 and perplexity is 91.36378865406536
At time: 384.93840289115906 and batch: 200, loss is 4.538103170394898 and perplexity is 93.5132530775583
At time: 385.46748781204224 and batch: 250, loss is 4.561940097808838 and perplexity is 95.76910113336473
At time: 385.98291397094727 and batch: 300, loss is 4.467506160736084 and perplexity is 87.13914080791292
At time: 386.4980254173279 and batch: 350, loss is 4.462212171554565 and perplexity is 86.67904608200276
At time: 387.01071333885193 and batch: 400, loss is 4.460741081237793 and perplexity is 86.55162712202258
At time: 387.52276134490967 and batch: 450, loss is 4.504730682373047 and perplexity is 90.44398260976527
At time: 388.036901473999 and batch: 500, loss is 4.536326866149903 and perplexity is 93.34729253100237
At time: 388.55183005332947 and batch: 550, loss is 4.472023315429688 and perplexity is 87.53365215036797
At time: 389.0669436454773 and batch: 600, loss is 4.407369680404663 and perplexity is 82.05335286686905
At time: 389.5826098918915 and batch: 650, loss is 4.3965856647491455 and perplexity is 81.17324231729198
At time: 390.0985209941864 and batch: 700, loss is 4.475857591629028 and perplexity is 87.86992461822886
At time: 390.6283805370331 and batch: 750, loss is 4.416608476638794 and perplexity is 82.81493973013669
At time: 391.1497893333435 and batch: 800, loss is 4.49578293800354 and perplexity is 89.63832276799907
At time: 391.66464924812317 and batch: 850, loss is 4.425389146804809 and perplexity is 83.54531228834345
At time: 392.1799385547638 and batch: 900, loss is 4.419616947174072 and perplexity is 83.06446118697166
At time: 392.6962900161743 and batch: 950, loss is 4.396470432281494 and perplexity is 81.16388906318211
At time: 393.2126166820526 and batch: 1000, loss is 4.317510747909546 and perplexity is 75.00169759720464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.918722385313453 and perplexity of 136.8276884027971
Finished 35 epochs...
Completing Train Step...
At time: 394.69369101524353 and batch: 50, loss is 4.5578296089172365 and perplexity is 95.37625126264854
At time: 395.21935200691223 and batch: 100, loss is 4.482272253036499 and perplexity is 88.43539213490186
At time: 395.7312867641449 and batch: 150, loss is 4.514616622924804 and perplexity is 91.34254067277514
At time: 396.2431857585907 and batch: 200, loss is 4.537485475540161 and perplexity is 93.45550825845747
At time: 396.75449085235596 and batch: 250, loss is 4.561336164474487 and perplexity is 95.71128044246822
At time: 397.2643840312958 and batch: 300, loss is 4.466757698059082 and perplexity is 87.07394481473949
At time: 397.7760772705078 and batch: 350, loss is 4.461516819000244 and perplexity is 86.6187945363643
At time: 398.2859399318695 and batch: 400, loss is 4.460134420394898 and perplexity is 86.49913556285543
At time: 398.80989599227905 and batch: 450, loss is 4.504247341156006 and perplexity is 90.40027786813897
At time: 399.32098174095154 and batch: 500, loss is 4.535743083953857 and perplexity is 93.29281394693463
At time: 399.8313045501709 and batch: 550, loss is 4.471563663482666 and perplexity is 87.49342638236165
At time: 400.3427541255951 and batch: 600, loss is 4.407108449935913 and perplexity is 82.03192083050946
At time: 400.85489797592163 and batch: 650, loss is 4.396292562484741 and perplexity is 81.14945374257262
At time: 401.36626148223877 and batch: 700, loss is 4.475721454620361 and perplexity is 87.85796308376179
At time: 401.8776535987854 and batch: 750, loss is 4.416587409973144 and perplexity is 82.81319511386724
At time: 402.3879199028015 and batch: 800, loss is 4.49567834854126 and perplexity is 89.62894803427879
At time: 402.9112946987152 and batch: 850, loss is 4.425583868026734 and perplexity is 83.56158191760794
At time: 403.4321115016937 and batch: 900, loss is 4.419796466827393 and perplexity is 83.0793742287992
At time: 403.9435954093933 and batch: 950, loss is 4.396644382476807 and perplexity is 81.17800876556389
At time: 404.45566725730896 and batch: 1000, loss is 4.3179143524169925 and perplexity is 75.03197473000363
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.9185002024580795 and perplexity of 136.79729101330807
Finished 36 epochs...
Completing Train Step...
At time: 405.94563388824463 and batch: 50, loss is 4.557348403930664 and perplexity is 95.33036677574773
At time: 406.4713349342346 and batch: 100, loss is 4.48205945968628 and perplexity is 88.4165756736114
At time: 406.9833116531372 and batch: 150, loss is 4.514360561370849 and perplexity is 91.3191543541646
At time: 407.4946892261505 and batch: 200, loss is 4.537106885910034 and perplexity is 93.42013366880147
At time: 408.0070860385895 and batch: 250, loss is 4.5609746837615965 and perplexity is 95.67668891304422
At time: 408.51986360549927 and batch: 300, loss is 4.466291017532349 and perplexity is 87.0333185807821
At time: 409.0320601463318 and batch: 350, loss is 4.461125755310059 and perplexity is 86.58492769341142
At time: 409.5419087409973 and batch: 400, loss is 4.459754123687744 and perplexity is 86.46624648063045
At time: 410.0524024963379 and batch: 450, loss is 4.503967227935791 and perplexity is 90.37495910142322
At time: 410.5644631385803 and batch: 500, loss is 4.535386390686035 and perplexity is 93.25954296238446
At time: 411.0758876800537 and batch: 550, loss is 4.47134108543396 and perplexity is 87.47395443333748
At time: 411.60006833076477 and batch: 600, loss is 4.406906700134277 and perplexity is 82.0153725761137
At time: 412.1156280040741 and batch: 650, loss is 4.396117544174194 and perplexity is 81.1352523450653
At time: 412.6294550895691 and batch: 700, loss is 4.4756560707092286 and perplexity is 87.852218774306
At time: 413.14113783836365 and batch: 750, loss is 4.416604042053223 and perplexity is 82.81457248101414
At time: 413.6526622772217 and batch: 800, loss is 4.49558650970459 and perplexity is 89.62071699392976
At time: 414.1646101474762 and batch: 850, loss is 4.425776090621948 and perplexity is 83.57764588562372
At time: 414.6764199733734 and batch: 900, loss is 4.419969120025635 and perplexity is 83.09371938680083
At time: 415.1888835430145 and batch: 950, loss is 4.3967702674865725 and perplexity is 81.1882285032325
At time: 415.70052123069763 and batch: 1000, loss is 4.318139390945435 and perplexity is 75.04886171522286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.918410882717225 and perplexity of 136.78507286039445
Finished 37 epochs...
Completing Train Step...
At time: 417.1964099407196 and batch: 50, loss is 4.557021446228028 and perplexity is 95.29920287295188
At time: 417.71131443977356 and batch: 100, loss is 4.481923189163208 and perplexity is 88.40452792149152
At time: 418.2269330024719 and batch: 150, loss is 4.514138240814209 and perplexity is 91.29885448555927
At time: 418.7424702644348 and batch: 200, loss is 4.536819019317627 and perplexity is 93.39324500361992
At time: 419.25803661346436 and batch: 250, loss is 4.560708017349243 and perplexity is 95.65117855519489
At time: 419.7720682621002 and batch: 300, loss is 4.4659433269500735 and perplexity is 87.0030631756319
At time: 420.28743624687195 and batch: 350, loss is 4.460831842422485 and perplexity is 86.55948300673653
At time: 420.8264112472534 and batch: 400, loss is 4.459446811676026 and perplexity is 86.4396784470258
At time: 421.34437918663025 and batch: 450, loss is 4.503747987747192 and perplexity is 90.35514745017962
At time: 421.8603880405426 and batch: 500, loss is 4.535136737823486 and perplexity is 93.23626335655491
At time: 422.3758566379547 and batch: 550, loss is 4.471179857254028 and perplexity is 87.45985230373361
At time: 422.8903741836548 and batch: 600, loss is 4.406748447418213 and perplexity is 82.00239444758367
At time: 423.40460300445557 and batch: 650, loss is 4.395974378585816 and perplexity is 81.12363740037515
At time: 423.9190113544464 and batch: 700, loss is 4.47560453414917 and perplexity is 87.84769128982335
At time: 424.4486210346222 and batch: 750, loss is 4.416624994277954 and perplexity is 82.81630764872553
At time: 424.9636859893799 and batch: 800, loss is 4.495504846572876 and perplexity is 89.61339858433979
At time: 425.47912669181824 and batch: 850, loss is 4.425935716629028 and perplexity is 83.59098811637277
At time: 425.9946143627167 and batch: 900, loss is 4.420114736557007 and perplexity is 83.10582008700639
At time: 426.508829832077 and batch: 950, loss is 4.396874942779541 and perplexity is 81.19672734963852
At time: 427.02362060546875 and batch: 1000, loss is 4.3182961368560795 and perplexity is 75.06062623939171
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.918372921827363 and perplexity of 136.77988047586314
Finished 38 epochs...
Completing Train Step...
At time: 428.50587797164917 and batch: 50, loss is 4.556756134033203 and perplexity is 95.2739221860582
At time: 429.0316150188446 and batch: 100, loss is 4.481815176010132 and perplexity is 88.39497958536677
At time: 429.5444676876068 and batch: 150, loss is 4.513953285217285 and perplexity is 91.28196981293496
At time: 430.0570366382599 and batch: 200, loss is 4.536572036743164 and perplexity is 93.37018134780924
At time: 430.56911039352417 and batch: 250, loss is 4.560493726730346 and perplexity is 95.63068360096074
At time: 431.08192682266235 and batch: 300, loss is 4.465671014785767 and perplexity is 86.9793744087133
At time: 431.5949330329895 and batch: 350, loss is 4.460586452484131 and perplexity is 86.5382447864669
At time: 432.1062572002411 and batch: 400, loss is 4.459169960021972 and perplexity is 86.41575079143024
At time: 432.6173987388611 and batch: 450, loss is 4.503552360534668 and perplexity is 90.33747325338
At time: 433.1299133300781 and batch: 500, loss is 4.5349288654327395 and perplexity is 93.21688412585998
At time: 433.6430640220642 and batch: 550, loss is 4.471038188934326 and perplexity is 87.44746289103072
At time: 434.15586161613464 and batch: 600, loss is 4.406621465682983 and perplexity is 81.99198230233384
At time: 434.6691393852234 and batch: 650, loss is 4.3958585071563725 and perplexity is 81.11423803311749
At time: 435.18089842796326 and batch: 700, loss is 4.475561790466308 and perplexity is 87.84393643621543
At time: 435.6929888725281 and batch: 750, loss is 4.416637878417969 and perplexity is 82.81737467250265
At time: 436.2050347328186 and batch: 800, loss is 4.495436449050903 and perplexity is 89.60726945955189
At time: 436.7179911136627 and batch: 850, loss is 4.426067161560058 and perplexity is 83.60197645020504
At time: 437.2313437461853 and batch: 900, loss is 4.420227575302124 and perplexity is 83.11519817255264
At time: 437.7568521499634 and batch: 950, loss is 4.396956253051758 and perplexity is 81.2033297460601
At time: 438.2680654525757 and batch: 1000, loss is 4.318407373428345 and perplexity is 75.06897619056802
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.918348358898628 and perplexity of 136.77652080266856
Finished 39 epochs...
Completing Train Step...
At time: 439.7477533817291 and batch: 50, loss is 4.556528244018555 and perplexity is 95.2522126843189
At time: 440.28064465522766 and batch: 100, loss is 4.4817229270935055 and perplexity is 88.3868256203678
At time: 440.79648542404175 and batch: 150, loss is 4.513793363571167 and perplexity is 91.26737301726443
At time: 441.30824089050293 and batch: 200, loss is 4.536359920501709 and perplexity is 93.35037811624554
At time: 441.8205463886261 and batch: 250, loss is 4.560303554534912 and perplexity is 95.61249903306391
At time: 442.332133769989 and batch: 300, loss is 4.465432510375977 and perplexity is 86.95863191804213
At time: 442.8433380126953 and batch: 350, loss is 4.460361528396606 and perplexity is 86.51878243957965
At time: 443.3547511100769 and batch: 400, loss is 4.458910140991211 and perplexity is 86.39330125135227
At time: 443.864385843277 and batch: 450, loss is 4.50337215423584 and perplexity is 90.32119533841498
At time: 444.37648463249207 and batch: 500, loss is 4.5347463893890385 and perplexity is 93.19987582948906
At time: 444.88817977905273 and batch: 550, loss is 4.470902709960938 and perplexity is 87.43561640102611
At time: 445.40023469924927 and batch: 600, loss is 4.406501817703247 and perplexity is 81.98217271415719
At time: 445.91182494163513 and batch: 650, loss is 4.3957618045806885 and perplexity is 81.10639445662818
At time: 446.42439365386963 and batch: 700, loss is 4.47552381515503 and perplexity is 87.84060059872546
At time: 446.9365804195404 and batch: 750, loss is 4.4166427326202395 and perplexity is 82.81777668576653
At time: 447.4479055404663 and batch: 800, loss is 4.495372371673584 and perplexity is 89.60152784469199
At time: 447.959436416626 and batch: 850, loss is 4.426178245544434 and perplexity is 83.61126380667953
At time: 448.47126269340515 and batch: 900, loss is 4.420315008163453 and perplexity is 83.12246548984531
At time: 448.98297119140625 and batch: 950, loss is 4.397019004821777 and perplexity is 81.2084255586171
At time: 449.4954092502594 and batch: 1000, loss is 4.318485097885132 and perplexity is 75.07481111271915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.918330494950458 and perplexity of 136.77407745581397
Finished 40 epochs...
Completing Train Step...
At time: 450.9886062145233 and batch: 50, loss is 4.5563247108459475 and perplexity is 95.23282767208684
At time: 451.5016417503357 and batch: 100, loss is 4.481638851165772 and perplexity is 88.37939472838828
At time: 452.01496291160583 and batch: 150, loss is 4.5136473274230955 and perplexity is 91.25404565482644
At time: 452.5277290344238 and batch: 200, loss is 4.536173868179321 and perplexity is 93.33301167718452
At time: 453.04029512405396 and batch: 250, loss is 4.560126438140869 and perplexity is 95.59556599161357
At time: 453.55292987823486 and batch: 300, loss is 4.4652151775360105 and perplexity is 86.93973500514204
At time: 454.0653064250946 and batch: 350, loss is 4.460149030685425 and perplexity is 86.50039934958852
At time: 454.57844066619873 and batch: 400, loss is 4.458664665222168 and perplexity is 86.37209639203346
At time: 455.08785676956177 and batch: 450, loss is 4.503202257156372 and perplexity is 90.30585133460055
At time: 455.5993061065674 and batch: 500, loss is 4.534579429626465 and perplexity is 93.18431649927588
At time: 456.1114339828491 and batch: 550, loss is 4.470772142410278 and perplexity is 87.42420089201599
At time: 456.6238498687744 and batch: 600, loss is 4.406394052505493 and perplexity is 81.97333836512858
At time: 457.13626289367676 and batch: 650, loss is 4.395679655075074 and perplexity is 81.0997318800888
At time: 457.6486146450043 and batch: 700, loss is 4.475488805770874 and perplexity is 87.83752540722526
At time: 458.1620569229126 and batch: 750, loss is 4.416643323898316 and perplexity is 82.81782565411669
At time: 458.6742935180664 and batch: 800, loss is 4.495307655334472 and perplexity is 89.59572934946178
At time: 459.1866366863251 and batch: 850, loss is 4.426276664733887 and perplexity is 83.61949316444927
At time: 459.6988525390625 and batch: 900, loss is 4.420383682250977 and perplexity is 83.12817404532827
At time: 460.2104504108429 and batch: 950, loss is 4.397064123153687 and perplexity is 81.21208962997308
At time: 460.72294664382935 and batch: 1000, loss is 4.318536615371704 and perplexity is 75.07867887792044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.918314491830221 and perplexity of 136.77188866132096
Finished 41 epochs...
Completing Train Step...
At time: 462.2058687210083 and batch: 50, loss is 4.556136255264282 and perplexity is 95.21488220516908
At time: 462.735023021698 and batch: 100, loss is 4.481559724807739 and perplexity is 88.37240186542182
At time: 463.25072836875916 and batch: 150, loss is 4.513509225845337 and perplexity is 91.24144419730528
At time: 463.77921772003174 and batch: 200, loss is 4.536005210876465 and perplexity is 93.31727171053507
At time: 464.294203042984 and batch: 250, loss is 4.5599599456787105 and perplexity is 95.5796513753288
At time: 464.809433221817 and batch: 300, loss is 4.465015735626221 and perplexity is 86.92239730734524
At time: 465.324383020401 and batch: 350, loss is 4.459957427978516 and perplexity is 86.4838272266069
At time: 465.8399133682251 and batch: 400, loss is 4.458431749343872 and perplexity is 86.35198130199495
At time: 466.35235619544983 and batch: 450, loss is 4.503041439056396 and perplexity is 90.29132968687544
At time: 466.86673283576965 and batch: 500, loss is 4.534424123764038 and perplexity is 93.1698455523778
At time: 467.3821818828583 and batch: 550, loss is 4.47065167427063 and perplexity is 87.41366969552381
At time: 467.8970777988434 and batch: 600, loss is 4.406292963027954 and perplexity is 81.96505214201316
At time: 468.41156673431396 and batch: 650, loss is 4.395597038269043 and perplexity is 81.09303195603786
At time: 468.92531847953796 and batch: 700, loss is 4.475468416213989 and perplexity is 87.83573445726269
At time: 469.44062209129333 and batch: 750, loss is 4.416639404296875 and perplexity is 82.81750104188411
At time: 469.95604705810547 and batch: 800, loss is 4.495249004364013 and perplexity is 89.59047462708524
At time: 470.47131848335266 and batch: 850, loss is 4.426362390518189 and perplexity is 83.62666181834852
At time: 470.98567938804626 and batch: 900, loss is 4.42043999671936 and perplexity is 83.13285549607279
At time: 471.5004246234894 and batch: 950, loss is 4.397101430892945 and perplexity is 81.21511952595652
At time: 472.01638889312744 and batch: 1000, loss is 4.318577299118042 and perplexity is 75.08173342198204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.918300721703506 and perplexity of 136.77000530805014
Finished 42 epochs...
Completing Train Step...
At time: 473.512362241745 and batch: 50, loss is 4.555963449478149 and perplexity is 95.19842994416193
At time: 474.0387668609619 and batch: 100, loss is 4.4814885711669925 and perplexity is 88.36611407098997
At time: 474.55135202407837 and batch: 150, loss is 4.51338038444519 and perplexity is 91.22968927915979
At time: 475.0633854866028 and batch: 200, loss is 4.535847520828247 and perplexity is 93.30255766561915
At time: 475.57497787475586 and batch: 250, loss is 4.559808578491211 and perplexity is 95.56518484722436
At time: 476.08599734306335 and batch: 300, loss is 4.464822444915772 and perplexity is 86.90559763907798
At time: 476.59773206710815 and batch: 350, loss is 4.459766416549683 and perplexity is 86.46730940479401
At time: 477.1236193180084 and batch: 400, loss is 4.458208236694336 and perplexity is 86.33268269868276
At time: 477.6324691772461 and batch: 450, loss is 4.502891969680786 and perplexity is 90.27783490675746
At time: 478.1446359157562 and batch: 500, loss is 4.534277353286743 and perplexity is 93.15617197314005
At time: 478.65666222572327 and batch: 550, loss is 4.470531177520752 and perplexity is 87.40313726700508
At time: 479.16897892951965 and batch: 600, loss is 4.406193780899048 and perplexity is 81.95692307678146
At time: 479.6800720691681 and batch: 650, loss is 4.395509119033814 and perplexity is 81.08590263209284
At time: 480.19157671928406 and batch: 700, loss is 4.475453777313232 and perplexity is 87.83444864807453
At time: 480.70330691337585 and batch: 750, loss is 4.416633462905883 and perplexity is 82.8170089921912
At time: 481.2166180610657 and batch: 800, loss is 4.4951925945281985 and perplexity is 89.58542098565984
At time: 481.72887682914734 and batch: 850, loss is 4.426435203552246 and perplexity is 83.6327511510123
At time: 482.2416512966156 and batch: 900, loss is 4.4204834365844725 and perplexity is 83.13646685453983
At time: 482.75469422340393 and batch: 950, loss is 4.397124576568603 and perplexity is 81.21699932652619
At time: 483.2665832042694 and batch: 1000, loss is 4.318604612350464 and perplexity is 75.08378417482389
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.918286207245617 and perplexity of 136.7680201799742
Finished 43 epochs...
Completing Train Step...
At time: 484.7563967704773 and batch: 50, loss is 4.555800561904907 and perplexity is 95.1829245657829
At time: 485.26805806159973 and batch: 100, loss is 4.481417798995972 and perplexity is 88.35986043054699
At time: 485.7807710170746 and batch: 150, loss is 4.513251571655274 and perplexity is 91.21793848520306
At time: 486.2929985523224 and batch: 200, loss is 4.535693111419678 and perplexity is 93.288151985087
At time: 486.8051040172577 and batch: 250, loss is 4.559668760299683 and perplexity is 95.55182402997042
At time: 487.3181674480438 and batch: 300, loss is 4.464633188247681 and perplexity is 86.88915173152829
At time: 487.83087396621704 and batch: 350, loss is 4.459585380554199 and perplexity is 86.4516571262148
At time: 488.34330105781555 and batch: 400, loss is 4.457991228103638 and perplexity is 86.3139497975529
At time: 488.8527023792267 and batch: 450, loss is 4.502751703262329 and perplexity is 90.26517284624073
At time: 489.3653061389923 and batch: 500, loss is 4.534136514663697 and perplexity is 93.14305291000814
At time: 489.89074516296387 and batch: 550, loss is 4.470407571792602 and perplexity is 87.3923344062421
At time: 490.40290927886963 and batch: 600, loss is 4.406095523834228 and perplexity is 81.94887062568988
At time: 490.915589094162 and batch: 650, loss is 4.395428829193115 and perplexity is 81.07939251923906
At time: 491.42776012420654 and batch: 700, loss is 4.475444107055664 and perplexity is 87.83359927043962
At time: 491.9403576850891 and batch: 750, loss is 4.416621799468994 and perplexity is 82.81604306686648
At time: 492.4533324241638 and batch: 800, loss is 4.495132627487183 and perplexity is 89.58004897411863
At time: 492.9662027359009 and batch: 850, loss is 4.426496829986572 and perplexity is 83.63790529807287
At time: 493.4783661365509 and batch: 900, loss is 4.420512170791626 and perplexity is 83.13885574932175
At time: 493.9901921749115 and batch: 950, loss is 4.397132272720337 and perplexity is 81.21762438728165
At time: 494.50220370292664 and batch: 1000, loss is 4.318618469238281 and perplexity is 75.08482460960663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.918271692787728 and perplexity of 136.7660350807111
Finished 44 epochs...
Completing Train Step...
At time: 495.98497200012207 and batch: 50, loss is 4.555638589859009 and perplexity is 95.1675088412483
At time: 496.5132110118866 and batch: 100, loss is 4.481348075866699 and perplexity is 88.35369991934321
At time: 497.0289657115936 and batch: 150, loss is 4.513117065429688 and perplexity is 91.20566992970872
At time: 497.54468631744385 and batch: 200, loss is 4.535535364151001 and perplexity is 93.27343719455095
At time: 498.0601625442505 and batch: 250, loss is 4.559531354904175 and perplexity is 95.53869559577758
At time: 498.5756108760834 and batch: 300, loss is 4.464437189102173 and perplexity is 86.87212320087828
At time: 499.09057545661926 and batch: 350, loss is 4.45940372467041 and perplexity is 86.43595410035117
At time: 499.6058168411255 and batch: 400, loss is 4.457787265777588 and perplexity is 86.29634679881694
At time: 500.1186170578003 and batch: 450, loss is 4.502619771957398 and perplexity is 90.2532648297346
At time: 500.6456415653229 and batch: 500, loss is 4.533994922637939 and perplexity is 93.12986553009732
At time: 501.16081166267395 and batch: 550, loss is 4.47029667854309 and perplexity is 87.38264372362293
At time: 501.6768527030945 and batch: 600, loss is 4.406003684997558 and perplexity is 81.94134488232827
At time: 502.1921920776367 and batch: 650, loss is 4.395333232879639 and perplexity is 81.07164199868186
At time: 502.706666469574 and batch: 700, loss is 4.475437450408935 and perplexity is 87.83301459514433
At time: 503.23413848876953 and batch: 750, loss is 4.416593170166015 and perplexity is 82.81367213521723
At time: 503.7494156360626 and batch: 800, loss is 4.49506667137146 and perplexity is 89.57414081688373
At time: 504.2650954723358 and batch: 850, loss is 4.426554880142212 and perplexity is 83.64276063241789
At time: 504.77978801727295 and batch: 900, loss is 4.420532732009888 and perplexity is 83.14056520305505
At time: 505.29506397247314 and batch: 950, loss is 4.397129392623901 and perplexity is 81.21739047302796
At time: 505.810183763504 and batch: 1000, loss is 4.318630886077881 and perplexity is 75.08575693161843
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.918259783488948 and perplexity of 136.7644063028352
Finished 45 epochs...
Completing Train Step...
At time: 507.2908766269684 and batch: 50, loss is 4.55547191619873 and perplexity is 95.15164824601858
At time: 507.81601452827454 and batch: 100, loss is 4.481283025741577 and perplexity is 88.34795268703958
At time: 508.3277225494385 and batch: 150, loss is 4.512976293563843 and perplexity is 91.1928316410332
At time: 508.84047842025757 and batch: 200, loss is 4.535372085571289 and perplexity is 93.25820888346333
At time: 509.3538508415222 and batch: 250, loss is 4.5593922996520995 and perplexity is 95.52541136202117
At time: 509.8671705722809 and batch: 300, loss is 4.464238443374634 and perplexity is 86.8548594531547
At time: 510.380832195282 and batch: 350, loss is 4.4592148876190185 and perplexity is 86.41963333067616
At time: 510.8935971260071 and batch: 400, loss is 4.457596893310547 and perplexity is 86.2799199140431
At time: 511.4037561416626 and batch: 450, loss is 4.502499904632568 and perplexity is 90.2424470606838
At time: 511.9118866920471 and batch: 500, loss is 4.533849849700927 and perplexity is 93.11635588694685
At time: 512.4193599224091 and batch: 550, loss is 4.470209274291992 and perplexity is 87.37500644285956
At time: 512.9274234771729 and batch: 600, loss is 4.405925531387329 and perplexity is 81.93494112064045
At time: 513.4382057189941 and batch: 650, loss is 4.3952412509918215 and perplexity is 81.06418521895193
At time: 513.9464645385742 and batch: 700, loss is 4.47542064666748 and perplexity is 87.8315386842763
At time: 514.4543268680573 and batch: 750, loss is 4.41657338142395 and perplexity is 82.81203337303444
At time: 514.9621770381927 and batch: 800, loss is 4.495002155303955 and perplexity is 89.5683620319823
At time: 515.4708912372589 and batch: 850, loss is 4.426608581542968 and perplexity is 83.64725248643533
At time: 515.9950132369995 and batch: 900, loss is 4.420560255050659 and perplexity is 83.14285351571137
At time: 516.505918264389 and batch: 950, loss is 4.397124404907227 and perplexity is 81.21698538470548
At time: 517.0192747116089 and batch: 1000, loss is 4.31863205909729 and perplexity is 75.0858450087203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.918252340177211 and perplexity of 136.76338832651302
Finished 46 epochs...
Completing Train Step...
At time: 518.5110981464386 and batch: 50, loss is 4.555322132110596 and perplexity is 95.13739711047488
At time: 519.0238757133484 and batch: 100, loss is 4.481219558715821 and perplexity is 88.34234568318266
At time: 519.5360040664673 and batch: 150, loss is 4.512854290008545 and perplexity is 91.18170647002435
At time: 520.0469710826874 and batch: 200, loss is 4.535231504440308 and perplexity is 93.24509946047552
At time: 520.5599436759949 and batch: 250, loss is 4.559268188476563 and perplexity is 95.5135563266098
At time: 521.0724382400513 and batch: 300, loss is 4.464067792892456 and perplexity is 86.84003889411325
At time: 521.5841038227081 and batch: 350, loss is 4.459060230255127 and perplexity is 86.40626893147478
At time: 522.0953483581543 and batch: 400, loss is 4.45740382194519 and perplexity is 86.26326334010793
At time: 522.606616973877 and batch: 450, loss is 4.502377490997315 and perplexity is 90.23140083080332
At time: 523.1184375286102 and batch: 500, loss is 4.533717451095581 and perplexity is 93.10402822739292
At time: 523.6305794715881 and batch: 550, loss is 4.470123929977417 and perplexity is 87.3675498010192
At time: 524.1422245502472 and batch: 600, loss is 4.405843391418457 and perplexity is 81.92821126352618
At time: 524.654604434967 and batch: 650, loss is 4.395167722702026 and perplexity is 81.05822492717685
At time: 525.1666307449341 and batch: 700, loss is 4.4754111766815186 and perplexity is 87.83070692477635
At time: 525.6796126365662 and batch: 750, loss is 4.416561012268066 and perplexity is 82.81100906441954
At time: 526.1914863586426 and batch: 800, loss is 4.494942502975464 and perplexity is 89.56301922998485
At time: 526.7037537097931 and batch: 850, loss is 4.4266476058959965 and perplexity is 83.65051683004023
At time: 527.2162082195282 and batch: 900, loss is 4.420581493377686 and perplexity is 83.14461934957589
At time: 527.7287662029266 and batch: 950, loss is 4.397124452590942 and perplexity is 81.21698925743321
At time: 528.2416019439697 and batch: 1000, loss is 4.318628997802734 and perplexity is 75.0856151491836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.918247874190167 and perplexity of 136.76277754435665
Finished 47 epochs...
Completing Train Step...
At time: 529.7127358913422 and batch: 50, loss is 4.5551848793029786 and perplexity is 95.12434013168608
At time: 530.2398915290833 and batch: 100, loss is 4.481160392761231 and perplexity is 88.33711897859261
At time: 530.7507171630859 and batch: 150, loss is 4.5127497577667235 and perplexity is 91.17217553998734
At time: 531.2595298290253 and batch: 200, loss is 4.535105533599854 and perplexity is 93.23335403673424
At time: 531.7678787708282 and batch: 250, loss is 4.559150905609131 and perplexity is 95.50235487972694
At time: 532.2811853885651 and batch: 300, loss is 4.463901233673096 and perplexity is 86.82557608951603
At time: 532.7936644554138 and batch: 350, loss is 4.458901519775391 and perplexity is 86.39255643926734
At time: 533.3053431510925 and batch: 400, loss is 4.457214307785034 and perplexity is 86.24691677920501
At time: 533.8178901672363 and batch: 450, loss is 4.502255363464355 and perplexity is 90.22038176530371
At time: 534.3305542469025 and batch: 500, loss is 4.533585462570191 and perplexity is 93.0917403749449
At time: 534.8418402671814 and batch: 550, loss is 4.4700320434570315 and perplexity is 87.35952226968999
At time: 535.3544595241547 and batch: 600, loss is 4.40575927734375 and perplexity is 81.92132023766361
At time: 535.8672726154327 and batch: 650, loss is 4.395091896057129 and perplexity is 81.05207878696284
At time: 536.3798789978027 and batch: 700, loss is 4.475397815704346 and perplexity is 87.82953342854562
At time: 536.8914422988892 and batch: 750, loss is 4.41655327796936 and perplexity is 82.8103685818161
At time: 537.402473449707 and batch: 800, loss is 4.4948805809021 and perplexity is 89.55747347384151
At time: 537.9143800735474 and batch: 850, loss is 4.426678161621094 and perplexity is 83.65307287128742
At time: 538.4264600276947 and batch: 900, loss is 4.420596361160278 and perplexity is 83.14585553488976
At time: 538.9390015602112 and batch: 950, loss is 4.397116575241089 and perplexity is 81.21634948531465
At time: 539.4509930610657 and batch: 1000, loss is 4.318615045547485 and perplexity is 75.08456754282378
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.918246013362233 and perplexity of 136.76252305259658
Finished 48 epochs...
Completing Train Step...
At time: 540.9365720748901 and batch: 50, loss is 4.555045185089111 and perplexity is 95.11105273987918
At time: 541.4641456604004 and batch: 100, loss is 4.4810997486114506 and perplexity is 88.33176201155412
At time: 541.9794960021973 and batch: 150, loss is 4.512648191452026 and perplexity is 91.16291598835204
At time: 542.507874250412 and batch: 200, loss is 4.534987726211548 and perplexity is 93.22237110574008
At time: 543.0228731632233 and batch: 250, loss is 4.559034910202026 and perplexity is 95.49127768765736
At time: 543.5379819869995 and batch: 300, loss is 4.463737211227417 and perplexity is 86.81133591406447
At time: 544.0522646903992 and batch: 350, loss is 4.458747148513794 and perplexity is 86.3792209406726
At time: 544.5662250518799 and batch: 400, loss is 4.45702712059021 and perplexity is 86.23077397170141
At time: 545.0809104442596 and batch: 450, loss is 4.502128744125367 and perplexity is 90.20895884339795
At time: 545.5965940952301 and batch: 500, loss is 4.533450479507446 and perplexity is 93.0791754147604
At time: 546.1115729808807 and batch: 550, loss is 4.469938163757324 and perplexity is 87.3513213689278
At time: 546.6270506381989 and batch: 600, loss is 4.405678749084473 and perplexity is 81.9147235219617
At time: 547.1423194408417 and batch: 650, loss is 4.395029983520508 and perplexity is 81.0470608025064
At time: 547.6566083431244 and batch: 700, loss is 4.4753929615020756 and perplexity is 87.82910708725981
At time: 548.170937538147 and batch: 750, loss is 4.416545772552491 and perplexity is 82.8097470578112
At time: 548.686840057373 and batch: 800, loss is 4.4948157119750975 and perplexity is 89.55166416505607
At time: 549.2011778354645 and batch: 850, loss is 4.426698513031006 and perplexity is 83.65477534658764
At time: 549.716150522232 and batch: 900, loss is 4.420604219436646 and perplexity is 83.1465089205686
At time: 550.2324192523956 and batch: 950, loss is 4.397098731994629 and perplexity is 81.21490033490302
At time: 550.7478713989258 and batch: 1000, loss is 4.318590412139892 and perplexity is 75.0827179768482
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.918245269031059 and perplexity of 136.7624212560252
Finished 49 epochs...
Completing Train Step...
At time: 552.2352228164673 and batch: 50, loss is 4.554904861450195 and perplexity is 95.09770734721656
At time: 552.7474961280823 and batch: 100, loss is 4.481034917831421 and perplexity is 88.32603558014803
At time: 553.2598094940186 and batch: 150, loss is 4.512545042037964 and perplexity is 91.15351307194459
At time: 553.7716267108917 and batch: 200, loss is 4.534883060455322 and perplexity is 93.21261442637518
At time: 554.2838339805603 and batch: 250, loss is 4.558918218612671 and perplexity is 95.48013530881805
At time: 554.7956366539001 and batch: 300, loss is 4.463572959899903 and perplexity is 86.79707820785295
At time: 555.320389509201 and batch: 350, loss is 4.458603820800781 and perplexity is 86.36684129167793
At time: 555.8313314914703 and batch: 400, loss is 4.456840324401855 and perplexity is 86.21466789612823
At time: 556.3408617973328 and batch: 450, loss is 4.5020043087005615 and perplexity is 90.19773435165972
At time: 556.851948261261 and batch: 500, loss is 4.5333189773559575 and perplexity is 93.06693610769979
At time: 557.3645734786987 and batch: 550, loss is 4.46984790802002 and perplexity is 87.34343776678865
At time: 557.8754215240479 and batch: 600, loss is 4.405601863861084 and perplexity is 81.90842573225149
At time: 558.3869390487671 and batch: 650, loss is 4.39496787071228 and perplexity is 81.04202689829796
At time: 558.8984432220459 and batch: 700, loss is 4.475409774780274 and perplexity is 87.83058379488529
At time: 559.4101152420044 and batch: 750, loss is 4.416525421142578 and perplexity is 82.80806177985299
At time: 559.9217596054077 and batch: 800, loss is 4.494750747680664 and perplexity is 89.54584669334425
At time: 560.4391841888428 and batch: 850, loss is 4.426708202362061 and perplexity is 83.6555859093272
At time: 560.9617805480957 and batch: 900, loss is 4.420606632232666 and perplexity is 83.14670953637642
At time: 561.4744784832001 and batch: 950, loss is 4.397080602645874 and perplexity is 81.21342797499727
At time: 561.986834526062 and batch: 1000, loss is 4.318566551208496 and perplexity is 75.08092645463931
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.918243036037538 and perplexity of 136.76211586676547
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f46fd764898>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'batch_size': 20, 'lr': 8.771703379866045, 'dropout': 0.044677723951818016, 'wordvec_source': '', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'anneal': 7.420550692577278, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.73154616355896 and batch: 50, loss is 6.385197649002075 and perplexity is 593.001927255711
At time: 1.2555177211761475 and batch: 100, loss is 5.4804556179046635 and perplexity is 239.95601072690098
At time: 1.76707124710083 and batch: 150, loss is 5.2859718608856205 and perplexity is 197.54607749171777
At time: 2.2794132232666016 and batch: 200, loss is 5.17200626373291 and perplexity is 176.26812331636594
At time: 2.7906181812286377 and batch: 250, loss is 5.193708009719849 and perplexity is 180.1352593836152
At time: 3.3020079135894775 and batch: 300, loss is 5.062895946502685 and perplexity is 158.04755148324574
At time: 3.8129003047943115 and batch: 350, loss is 5.012472181320191 and perplexity is 150.27578630021048
At time: 4.324245452880859 and batch: 400, loss is 4.9942018413543705 and perplexity is 147.55512597093553
At time: 4.8349621295928955 and batch: 450, loss is 4.994208402633667 and perplexity is 147.55609412450485
At time: 5.344406366348267 and batch: 500, loss is 5.000280904769897 and perplexity is 148.45485492288728
At time: 5.854354619979858 and batch: 550, loss is 4.901838293075562 and perplexity is 134.536870700317
At time: 6.378712177276611 and batch: 600, loss is 4.815770416259766 and perplexity is 123.44187731508323
At time: 6.890034914016724 and batch: 650, loss is 4.801496725082398 and perplexity is 121.69242137930254
At time: 7.401349067687988 and batch: 700, loss is 4.84722939491272 and perplexity is 127.38696150333617
At time: 7.912510395050049 and batch: 750, loss is 4.781827964782715 and perplexity is 119.3222677143965
At time: 8.4240562915802 and batch: 800, loss is 4.889395055770874 and perplexity is 132.873168876694
At time: 8.934912204742432 and batch: 850, loss is 4.768395605087281 and perplexity is 117.73020461458525
At time: 9.451369285583496 and batch: 900, loss is 4.768789262771606 and perplexity is 117.77655913761748
At time: 9.966435670852661 and batch: 950, loss is 4.7676084613800045 and perplexity is 117.6375704878345
At time: 10.480535984039307 and batch: 1000, loss is 4.648211851119995 and perplexity is 104.39813915668424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8245790062881095 and perplexity of 124.5340293024555
Finished 1 epochs...
Completing Train Step...
At time: 11.977418661117554 and batch: 50, loss is 4.729062786102295 and perplexity is 113.18942991524284
At time: 12.485126495361328 and batch: 100, loss is 4.590568885803223 and perplexity is 98.550478185913
At time: 12.993365287780762 and batch: 150, loss is 4.658558139801025 and perplexity is 105.48387944810462
At time: 13.500863552093506 and batch: 200, loss is 4.653197002410889 and perplexity is 104.91987907055939
At time: 14.009128093719482 and batch: 250, loss is 4.683355302810669 and perplexity is 108.13228111867
At time: 14.517736673355103 and batch: 300, loss is 4.558613166809082 and perplexity is 95.45101336341199
At time: 15.026026248931885 and batch: 350, loss is 4.543525238037109 and perplexity is 94.02166533846308
At time: 15.533015489578247 and batch: 400, loss is 4.551806554794312 and perplexity is 94.80352146196311
At time: 16.04108476638794 and batch: 450, loss is 4.594279432296753 and perplexity is 98.91683358621805
At time: 16.548076391220093 and batch: 500, loss is 4.604053010940552 and perplexity is 99.88834487601815
At time: 17.054553508758545 and batch: 550, loss is 4.516813077926636 and perplexity is 91.54339095166695
At time: 17.56219744682312 and batch: 600, loss is 4.4543990850448605 and perplexity is 86.00445395138772
At time: 18.07108998298645 and batch: 650, loss is 4.426572313308716 and perplexity is 83.64421880330109
At time: 18.579951524734497 and batch: 700, loss is 4.508066310882568 and perplexity is 90.74617385534793
At time: 19.089624881744385 and batch: 750, loss is 4.4460903787612915 and perplexity is 85.2928286411629
At time: 19.613290548324585 and batch: 800, loss is 4.573493518829346 and perplexity is 96.88197826954487
At time: 20.12316918373108 and batch: 850, loss is 4.441425647735596 and perplexity is 84.89588706974061
At time: 20.633063554763794 and batch: 900, loss is 4.420583324432373 and perplexity is 83.14477159206027
At time: 21.142651081085205 and batch: 950, loss is 4.449605913162231 and perplexity is 85.59320619885969
At time: 21.652571439743042 and batch: 1000, loss is 4.333412599563599 and perplexity is 76.20389672920248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6794493140243905 and perplexity of 107.71074144175488
Finished 2 epochs...
Completing Train Step...
At time: 23.12782335281372 and batch: 50, loss is 4.453575372695923 and perplexity is 85.93364018969325
At time: 23.648542881011963 and batch: 100, loss is 4.3224933099746705 and perplexity is 75.37633075189395
At time: 24.15516972541809 and batch: 150, loss is 4.421833353042603 and perplexity is 83.24876992224334
At time: 24.661760568618774 and batch: 200, loss is 4.44484658241272 and perplexity is 85.18680768024983
At time: 25.168659448623657 and batch: 250, loss is 4.451659255027771 and perplexity is 85.76913887570488
At time: 25.67581081390381 and batch: 300, loss is 4.32250825881958 and perplexity is 75.37745754939435
At time: 26.18265175819397 and batch: 350, loss is 4.318266258239746 and perplexity is 75.05838356524275
At time: 26.68961453437805 and batch: 400, loss is 4.334013929367066 and perplexity is 76.24973418377861
At time: 27.19590139389038 and batch: 450, loss is 4.380739235877991 and perplexity is 79.89707436371458
At time: 27.701574325561523 and batch: 500, loss is 4.3941864490509035 and perplexity is 80.97872363948777
At time: 28.206066131591797 and batch: 550, loss is 4.311409931182862 and perplexity is 74.54551893210103
At time: 28.712002754211426 and batch: 600, loss is 4.2556986808776855 and perplexity is 70.50606122023314
At time: 29.21798086166382 and batch: 650, loss is 4.2337832403182984 and perplexity is 68.97769836960614
At time: 29.72279381752014 and batch: 700, loss is 4.308162350654602 and perplexity is 74.3038190387114
At time: 30.22998046875 and batch: 750, loss is 4.254465079307556 and perplexity is 70.41913845745957
At time: 30.735026836395264 and batch: 800, loss is 4.386878166198731 and perplexity is 80.389065540725
At time: 31.24045157432556 and batch: 850, loss is 4.253199596405029 and perplexity is 70.33008060421142
At time: 31.74706506729126 and batch: 900, loss is 4.218318433761596 and perplexity is 67.91917761321646
At time: 32.26629114151001 and batch: 950, loss is 4.257837681770325 and perplexity is 70.65703515724984
At time: 32.77208495140076 and batch: 1000, loss is 4.154225511550903 and perplexity is 63.70260850297384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.630262421398628 and perplexity of 102.5409695247002
Finished 3 epochs...
Completing Train Step...
At time: 34.238959312438965 and batch: 50, loss is 4.275284028053283 and perplexity is 71.90055818532639
At time: 34.75827646255493 and batch: 100, loss is 4.159383072853088 and perplexity is 64.03200732857584
At time: 35.26465463638306 and batch: 150, loss is 4.258649640083313 and perplexity is 70.71442902187638
At time: 35.770806074142456 and batch: 200, loss is 4.289595470428467 and perplexity is 72.93695737485815
At time: 36.27691173553467 and batch: 250, loss is 4.290372061729431 and perplexity is 72.99362158109123
At time: 36.783607959747314 and batch: 300, loss is 4.155338220596313 and perplexity is 63.773530422084264
At time: 37.28959918022156 and batch: 350, loss is 4.1608079195022585 and perplexity is 64.1233081490452
At time: 37.795820236206055 and batch: 400, loss is 4.180864586830139 and perplexity is 65.42239210770903
At time: 38.301392555236816 and batch: 450, loss is 4.22393832206726 and perplexity is 68.30195036784896
At time: 38.80911922454834 and batch: 500, loss is 4.244193396568298 and perplexity is 69.69951759415628
At time: 39.315547943115234 and batch: 550, loss is 4.162177872657776 and perplexity is 64.21121427727678
At time: 39.81815814971924 and batch: 600, loss is 4.107662138938903 and perplexity is 60.80439903711353
At time: 40.32428526878357 and batch: 650, loss is 4.093732752799988 and perplexity is 59.96330266170658
At time: 40.83076214790344 and batch: 700, loss is 4.174176836013794 and perplexity is 64.98632323675788
At time: 41.335689067840576 and batch: 750, loss is 4.1187421989440915 and perplexity is 61.481861659412495
At time: 41.84132528305054 and batch: 800, loss is 4.250901741981506 and perplexity is 70.16865785140944
At time: 42.347076177597046 and batch: 850, loss is 4.126433501243591 and perplexity is 61.95656042913102
At time: 42.85296034812927 and batch: 900, loss is 4.08101453781128 and perplexity is 59.205505614538005
At time: 43.35859179496765 and batch: 950, loss is 4.119125337600708 and perplexity is 61.50542225049334
At time: 43.86438512802124 and batch: 1000, loss is 4.022256150245666 and perplexity is 55.82691776233779
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.602638058546113 and perplexity of 99.74710756885638
Finished 4 epochs...
Completing Train Step...
At time: 45.35599970817566 and batch: 50, loss is 4.142416906356812 and perplexity is 62.954793564796134
At time: 45.86185932159424 and batch: 100, loss is 4.0269582653045655 and perplexity is 56.090040484897116
At time: 46.3684401512146 and batch: 150, loss is 4.127263684272766 and perplexity is 62.00801707040513
At time: 46.873406410217285 and batch: 200, loss is 4.166731090545654 and perplexity is 64.50424854490068
At time: 47.37909293174744 and batch: 250, loss is 4.165021586418152 and perplexity is 64.39407246583428
At time: 47.884663343429565 and batch: 300, loss is 4.026197147369385 and perplexity is 56.04736559147258
At time: 48.392956495285034 and batch: 350, loss is 4.037132644653321 and perplexity is 56.663634869806494
At time: 48.90011525154114 and batch: 400, loss is 4.05549156665802 and perplexity is 57.7135260695539
At time: 49.40683102607727 and batch: 450, loss is 4.105074486732483 and perplexity is 60.64726179573159
At time: 49.913285970687866 and batch: 500, loss is 4.129714460372925 and perplexity is 62.160171208364964
At time: 50.41884088516235 and batch: 550, loss is 4.04470419883728 and perplexity is 57.09429497723201
At time: 50.92233872413635 and batch: 600, loss is 3.9903392887115476 and perplexity is 54.07323268875718
At time: 51.42691493034363 and batch: 650, loss is 3.9751485347747804 and perplexity is 53.258026987303786
At time: 51.94290351867676 and batch: 700, loss is 4.0501925659179685 and perplexity is 57.40851090371524
At time: 52.449320554733276 and batch: 750, loss is 4.004029588699341 and perplexity is 54.81860198857636
At time: 52.95452094078064 and batch: 800, loss is 4.14222852230072 and perplexity is 62.9429350024511
At time: 53.459912061691284 and batch: 850, loss is 4.0098567295074465 and perplexity is 55.138970210112674
At time: 53.966527223587036 and batch: 900, loss is 3.9682875967025755 and perplexity is 52.89387759396431
At time: 54.47143483161926 and batch: 950, loss is 4.00866777420044 and perplexity is 55.07345139604752
At time: 54.97829079627991 and batch: 1000, loss is 3.9156654357910154 and perplexity is 50.18245360369426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.610175528177401 and perplexity of 100.50178898408873
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 56.4507999420166 and batch: 50, loss is 4.050923328399659 and perplexity is 57.45047822181654
At time: 56.97362494468689 and batch: 100, loss is 3.9184290313720704 and perplexity is 50.32132942062872
At time: 57.482462644577026 and batch: 150, loss is 3.9983692598342895 and perplexity is 54.509187194264015
At time: 58.00494837760925 and batch: 200, loss is 4.021085510253906 and perplexity is 55.76160277741416
At time: 58.51308059692383 and batch: 250, loss is 4.010375423431396 and perplexity is 55.167577877603776
At time: 59.02171754837036 and batch: 300, loss is 3.8477087688446043 and perplexity is 46.885514517547996
At time: 59.530773401260376 and batch: 350, loss is 3.8545566511154177 and perplexity is 47.207682827534335
At time: 60.03967308998108 and batch: 400, loss is 3.8557639741897582 and perplexity is 47.26471217179248
At time: 60.548535108566284 and batch: 450, loss is 3.8884141349792483 and perplexity is 48.83338193181404
At time: 61.05673289299011 and batch: 500, loss is 3.9020161247253418 and perplexity is 49.502151076696855
At time: 61.56571960449219 and batch: 550, loss is 3.7962837982177735 and perplexity is 44.53537415497875
At time: 62.07388353347778 and batch: 600, loss is 3.7245431566238403 and perplexity is 41.45229121383602
At time: 62.57981872558594 and batch: 650, loss is 3.6891500186920165 and perplexity is 40.01082404735908
At time: 63.08576679229736 and batch: 700, loss is 3.745286340713501 and perplexity is 42.32112374479693
At time: 63.59487724304199 and batch: 750, loss is 3.683728561401367 and perplexity is 39.79449401550451
At time: 64.10430932044983 and batch: 800, loss is 3.7889380502700805 and perplexity is 44.20942714962695
At time: 64.6207275390625 and batch: 850, loss is 3.6321679306030275 and perplexity is 37.79466407419953
At time: 65.1384539604187 and batch: 900, loss is 3.5634009313583372 and perplexity is 35.28298835030784
At time: 65.6466760635376 and batch: 950, loss is 3.5598373651504516 and perplexity is 35.157478848839006
At time: 66.15500473976135 and batch: 1000, loss is 3.43788827419281 and perplexity is 31.1211693523161
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4847564697265625 and perplexity of 88.65535792024977
Finished 6 epochs...
Completing Train Step...
At time: 67.62372493743896 and batch: 50, loss is 3.8901423597335816 and perplexity is 48.91784996015136
At time: 68.1431736946106 and batch: 100, loss is 3.7656188774108887 and perplexity is 43.19042717014172
At time: 68.64916515350342 and batch: 150, loss is 3.8638006496429442 and perplexity is 47.646093791329804
At time: 69.1552939414978 and batch: 200, loss is 3.8978954696655275 and perplexity is 49.298589478990905
At time: 69.66107368469238 and batch: 250, loss is 3.889343934059143 and perplexity is 48.87880828081846
At time: 70.16684675216675 and batch: 300, loss is 3.729586639404297 and perplexity is 41.661883223351374
At time: 70.67179942131042 and batch: 350, loss is 3.74352991104126 and perplexity is 42.246854910374786
At time: 71.19010496139526 and batch: 400, loss is 3.7512643051147463 and perplexity is 42.57487562012889
At time: 71.69541525840759 and batch: 450, loss is 3.792982511520386 and perplexity is 44.38859253413209
At time: 72.20149445533752 and batch: 500, loss is 3.8073995161056517 and perplexity is 45.03317841026911
At time: 72.70718502998352 and batch: 550, loss is 3.7052392959594727 and perplexity is 40.65977587290973
At time: 73.21296119689941 and batch: 600, loss is 3.644647397994995 and perplexity is 38.26927664862731
At time: 73.71753096580505 and batch: 650, loss is 3.6141006565093994 and perplexity is 37.117949124370156
At time: 74.2207567691803 and batch: 700, loss is 3.674594702720642 and perplexity is 39.43267166349855
At time: 74.72477555274963 and batch: 750, loss is 3.623031759262085 and perplexity is 37.4509381079916
At time: 75.23017811775208 and batch: 800, loss is 3.737311758995056 and perplexity is 41.98497259889879
At time: 75.73614430427551 and batch: 850, loss is 3.5878165435791014 and perplexity is 36.155046710561464
At time: 76.2418897151947 and batch: 900, loss is 3.52731264591217 and perplexity is 34.0323875400509
At time: 76.74669027328491 and batch: 950, loss is 3.5374417543411254 and perplexity is 34.37885703480459
At time: 77.25086283683777 and batch: 1000, loss is 3.4338276958465577 and perplexity is 30.995055626549377
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.477594515172447 and perplexity of 88.02268058336867
Finished 7 epochs...
Completing Train Step...
At time: 78.72964310646057 and batch: 50, loss is 3.8296590423583985 and perplexity is 46.046835536941394
At time: 79.23590230941772 and batch: 100, loss is 3.70408257484436 and perplexity is 40.61277104260271
At time: 79.74211120605469 and batch: 150, loss is 3.804656343460083 and perplexity is 44.909813909532744
At time: 80.2491660118103 and batch: 200, loss is 3.8394386100769045 and perplexity is 46.499362837352486
At time: 80.75484657287598 and batch: 250, loss is 3.8300321435928346 and perplexity is 46.06401887348481
At time: 81.26100850105286 and batch: 300, loss is 3.6701635837554933 and perplexity is 39.25832735968757
At time: 81.76783585548401 and batch: 350, loss is 3.6865454626083376 and perplexity is 39.9067492053894
At time: 82.27363729476929 and batch: 400, loss is 3.6966058254241942 and perplexity is 40.31025186956316
At time: 82.77963972091675 and batch: 450, loss is 3.7417033815383913 and perplexity is 42.169760212781085
At time: 83.28755164146423 and batch: 500, loss is 3.7557325649261473 and perplexity is 42.76553687062125
At time: 83.80633974075317 and batch: 550, loss is 3.6560255098342895 and perplexity is 38.707195378028935
At time: 84.31247329711914 and batch: 600, loss is 3.5984405088424682 and perplexity is 36.541204301241386
At time: 84.81889653205872 and batch: 650, loss is 3.5711888599395754 and perplexity is 35.55884251572864
At time: 85.32190704345703 and batch: 700, loss is 3.6341468286514282 and perplexity is 37.869529912675304
At time: 85.8268473148346 and batch: 750, loss is 3.5879906463623046 and perplexity is 36.16134195281431
At time: 86.3312873840332 and batch: 800, loss is 3.7058400869369508 and perplexity is 40.68421123893918
At time: 86.83715534210205 and batch: 850, loss is 3.5600603199005127 and perplexity is 35.16531824963188
At time: 87.34369802474976 and batch: 900, loss is 3.5035611963272095 and perplexity is 33.23359282112784
At time: 87.8502459526062 and batch: 950, loss is 3.5182573652267455 and perplexity is 33.725605812099055
At time: 88.3580813407898 and batch: 1000, loss is 3.422874188423157 and perplexity is 30.657403667207927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.478086145912728 and perplexity of 88.0659658783044
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 89.83397436141968 and batch: 50, loss is 3.796825342178345 and perplexity is 44.55949854950466
At time: 90.35869884490967 and batch: 100, loss is 3.6744374418258667 and perplexity is 39.42647093384829
At time: 90.87280368804932 and batch: 150, loss is 3.7762728834152224 and perplexity is 43.653038195037205
At time: 91.38743472099304 and batch: 200, loss is 3.8130656051635743 and perplexity is 45.28906466225653
At time: 91.90152215957642 and batch: 250, loss is 3.799243230819702 and perplexity is 44.66736881150024
At time: 92.4151074886322 and batch: 300, loss is 3.6318831920623778 and perplexity is 37.78390400868207
At time: 92.92880201339722 and batch: 350, loss is 3.649460964202881 and perplexity is 38.45393241533691
At time: 93.44256925582886 and batch: 400, loss is 3.6599858236312866 and perplexity is 38.860791962512174
At time: 93.95308828353882 and batch: 450, loss is 3.6956481742858887 and perplexity is 40.27166718925521
At time: 94.46230554580688 and batch: 500, loss is 3.7091265296936036 and perplexity is 40.8181375202626
At time: 94.97224521636963 and batch: 550, loss is 3.6062938499450685 and perplexity is 36.82930463731095
At time: 95.48132514953613 and batch: 600, loss is 3.545195631980896 and perplexity is 34.646462635428
At time: 95.99119973182678 and batch: 650, loss is 3.5131571531295775 and perplexity is 33.55403596257132
At time: 96.4998722076416 and batch: 700, loss is 3.570726795196533 and perplexity is 35.542415823688614
At time: 97.02053713798523 and batch: 750, loss is 3.5193653774261473 and perplexity is 33.762994904731265
At time: 97.52961158752441 and batch: 800, loss is 3.6271111440658568 and perplexity is 37.604026937618215
At time: 98.03965520858765 and batch: 850, loss is 3.4765771579742433 and perplexity is 32.34880750481384
At time: 98.54886293411255 and batch: 900, loss is 3.4137037229537963 and perplexity is 30.377546178487915
At time: 99.05829238891602 and batch: 950, loss is 3.4191153717041014 and perplexity is 30.54238440931829
At time: 99.58536005020142 and batch: 1000, loss is 3.316654930114746 and perplexity is 27.567979333082363
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.462481615020008 and perplexity of 86.70240433128284
Finished 9 epochs...
Completing Train Step...
At time: 101.11179900169373 and batch: 50, loss is 3.7754160261154173 and perplexity is 43.61564979115207
At time: 101.63414287567139 and batch: 100, loss is 3.6517891454696656 and perplexity is 38.54356443973356
At time: 102.14254808425903 and batch: 150, loss is 3.7540864753723144 and perplexity is 42.69519887431733
At time: 102.65064215660095 and batch: 200, loss is 3.790246901512146 and perplexity is 44.26732859718765
At time: 103.15888261795044 and batch: 250, loss is 3.7783946990966797 and perplexity is 43.74576023077041
At time: 103.66731429100037 and batch: 300, loss is 3.611027579307556 and perplexity is 37.004057889046784
At time: 104.17537903785706 and batch: 350, loss is 3.6307092332839965 and perplexity is 37.73957328919952
At time: 104.68420171737671 and batch: 400, loss is 3.6413416147232054 and perplexity is 38.14297559104075
At time: 105.19230270385742 and batch: 450, loss is 3.6795895862579346 and perplexity is 39.63012598617333
At time: 105.70059704780579 and batch: 500, loss is 3.693641047477722 and perplexity is 40.1909179105597
At time: 106.20844554901123 and batch: 550, loss is 3.5910469675064087 and perplexity is 36.27203169235594
At time: 106.71610474586487 and batch: 600, loss is 3.531283779144287 and perplexity is 34.1678033844015
At time: 107.22547268867493 and batch: 650, loss is 3.5015445375442504 and perplexity is 33.16663953793023
At time: 107.73243451118469 and batch: 700, loss is 3.5609868478775026 and perplexity is 35.19791499937831
At time: 108.23789525032043 and batch: 750, loss is 3.5119859790802 and perplexity is 33.51476134959309
At time: 108.74324250221252 and batch: 800, loss is 3.6224958658218385 and perplexity is 37.43087377258149
At time: 109.24878311157227 and batch: 850, loss is 3.4742972803115846 and perplexity is 32.27514018931276
At time: 109.77686095237732 and batch: 900, loss is 3.4136850214004517 and perplexity is 30.376978076499793
At time: 110.28302359580994 and batch: 950, loss is 3.4218166780471804 and perplexity is 30.625000281207612
At time: 110.78890776634216 and batch: 1000, loss is 3.3235992860794066 and perplexity is 27.76008745637445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4610536156631095 and perplexity of 86.57868171259061
Finished 10 epochs...
Completing Train Step...
At time: 112.27548885345459 and batch: 50, loss is 3.7647761726379394 and perplexity is 43.154045722582495
At time: 112.78069972991943 and batch: 100, loss is 3.6402375650405885 and perplexity is 38.10088708911922
At time: 113.28620171546936 and batch: 150, loss is 3.742506489753723 and perplexity is 42.20364069667299
At time: 113.79210162162781 and batch: 200, loss is 3.777929286956787 and perplexity is 43.72540516000612
At time: 114.29812622070312 and batch: 250, loss is 3.7668010663986204 and perplexity is 43.24151661025434
At time: 114.80412244796753 and batch: 300, loss is 3.5991589975357057 and perplexity is 36.56746817738829
At time: 115.31056594848633 and batch: 350, loss is 3.6195779132843016 and perplexity is 37.32181145618976
At time: 115.81676816940308 and batch: 400, loss is 3.630591926574707 and perplexity is 37.73514644370139
At time: 116.33282423019409 and batch: 450, loss is 3.6698360204696656 and perplexity is 39.24546987891582
At time: 116.84530425071716 and batch: 500, loss is 3.684334921836853 and perplexity is 39.81863113938462
At time: 117.35166049003601 and batch: 550, loss is 3.5821303510665894 and perplexity is 35.950045545067084
At time: 117.85751509666443 and batch: 600, loss is 3.523153557777405 and perplexity is 33.89113777954845
At time: 118.36297225952148 and batch: 650, loss is 3.494760093688965 and perplexity is 32.94238391912597
At time: 118.86808562278748 and batch: 700, loss is 3.554943685531616 and perplexity is 34.98584970268992
At time: 119.37388610839844 and batch: 750, loss is 3.507470269203186 and perplexity is 33.36375960736799
At time: 119.87873721122742 and batch: 800, loss is 3.619472007751465 and perplexity is 37.317859079154054
At time: 120.38356852531433 and batch: 850, loss is 3.4726520204544067 and perplexity is 32.22208285529371
At time: 120.90008735656738 and batch: 900, loss is 3.4132345962524413 and perplexity is 30.363298602674366
At time: 121.41007089614868 and batch: 950, loss is 3.422799310684204 and perplexity is 30.65510819608008
At time: 121.91585755348206 and batch: 1000, loss is 3.3266254663467407 and perplexity is 27.844221723777487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.460931917516197 and perplexity of 86.56814588857218
Finished 11 epochs...
Completing Train Step...
At time: 123.3857729434967 and batch: 50, loss is 3.755908203125 and perplexity is 42.773048792161056
At time: 123.91134357452393 and batch: 100, loss is 3.631074757575989 and perplexity is 37.753370541467966
At time: 124.42500901222229 and batch: 150, loss is 3.733425850868225 and perplexity is 41.82213943493172
At time: 124.93196415901184 and batch: 200, loss is 3.7686541223526 and perplexity is 43.32171984766112
At time: 125.44785022735596 and batch: 250, loss is 3.757710018157959 and perplexity is 42.85018738835193
At time: 125.96067523956299 and batch: 300, loss is 3.5900306272506715 and perplexity is 36.23518569359719
At time: 126.46577286720276 and batch: 350, loss is 3.610912322998047 and perplexity is 36.999793183669446
At time: 126.97090125083923 and batch: 400, loss is 3.6223477029800417 and perplexity is 37.42532831877765
At time: 127.47628688812256 and batch: 450, loss is 3.662141480445862 and perplexity is 38.944652848697224
At time: 127.98215770721436 and batch: 500, loss is 3.6771139764785765 and perplexity is 39.53213859803707
At time: 128.4880654811859 and batch: 550, loss is 3.5751611471176146 and perplexity is 35.700373364405536
At time: 128.99341917037964 and batch: 600, loss is 3.5167987871170046 and perplexity is 33.67645023905985
At time: 129.49881267547607 and batch: 650, loss is 3.48928795337677 and perplexity is 32.762610892314044
At time: 130.00483751296997 and batch: 700, loss is 3.549934959411621 and perplexity is 34.81105328258262
At time: 130.5141315460205 and batch: 750, loss is 3.5036390924453737 and perplexity is 33.23618168983136
At time: 131.0187509059906 and batch: 800, loss is 3.616695914268494 and perplexity is 37.214404879471566
At time: 131.52710270881653 and batch: 850, loss is 3.4706507444381716 and perplexity is 32.15766205708248
At time: 132.03615880012512 and batch: 900, loss is 3.4120880794525146 and perplexity is 30.328506519390388
At time: 132.54554390907288 and batch: 950, loss is 3.4226678562164308 and perplexity is 30.651078710000405
At time: 133.05404710769653 and batch: 1000, loss is 3.327784523963928 and perplexity is 27.876513491455633
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.461247886099467 and perplexity of 86.59550302475508
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 134.52751183509827 and batch: 50, loss is 3.749760980606079 and perplexity is 42.51091985133797
At time: 135.05173063278198 and batch: 100, loss is 3.62653196811676 and perplexity is 37.58225389544658
At time: 135.56216716766357 and batch: 150, loss is 3.728976454734802 and perplexity is 41.63646953521616
At time: 136.0855734348297 and batch: 200, loss is 3.7635369873046876 and perplexity is 43.100602981621776
At time: 136.5954144001007 and batch: 250, loss is 3.752156572341919 and perplexity is 42.612880739185336
At time: 137.1065969467163 and batch: 300, loss is 3.5830796718597413 and perplexity is 35.984189875214796
At time: 137.61749243736267 and batch: 350, loss is 3.6054762029647827 and perplexity is 36.79920357528645
At time: 138.1287977695465 and batch: 400, loss is 3.616167049407959 and perplexity is 37.19472869190581
At time: 138.63897800445557 and batch: 450, loss is 3.654717512130737 and perplexity is 38.65659955218904
At time: 139.14965415000916 and batch: 500, loss is 3.6688782596588134 and perplexity is 39.20790010016511
At time: 139.6611111164093 and batch: 550, loss is 3.5668663692474367 and perplexity is 35.405471461206695
At time: 140.17244052886963 and batch: 600, loss is 3.508148913383484 and perplexity is 33.386409413368604
At time: 140.6829810142517 and batch: 650, loss is 3.478774938583374 and perplexity is 32.41998127018827
At time: 141.1943962574005 and batch: 700, loss is 3.538334050178528 and perplexity is 34.40954683598608
At time: 141.70540046691895 and batch: 750, loss is 3.4899154901504517 and perplexity is 32.78317708779785
At time: 142.2160358428955 and batch: 800, loss is 3.6017522478103636 and perplexity is 36.66241983778048
At time: 142.7250726222992 and batch: 850, loss is 3.4557080936431883 and perplexity is 31.680713653797895
At time: 143.23843502998352 and batch: 900, loss is 3.3948213911056517 and perplexity is 29.8093287881779
At time: 143.748188495636 and batch: 950, loss is 3.404889144897461 and perplexity is 30.110957585605657
At time: 144.25944328308105 and batch: 1000, loss is 3.308266143798828 and perplexity is 27.337684741870785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.460219964748475 and perplexity of 86.50653539199448
Finished 13 epochs...
Completing Train Step...
At time: 145.74731040000916 and batch: 50, loss is 3.747555470466614 and perplexity is 42.41726490300054
At time: 146.25564622879028 and batch: 100, loss is 3.6242830896377565 and perplexity is 37.49783093753569
At time: 146.7645788192749 and batch: 150, loss is 3.726700143814087 and perplexity is 41.54179977469632
At time: 147.27168583869934 and batch: 200, loss is 3.7614148378372194 and perplexity is 43.00923404351858
At time: 147.78027844429016 and batch: 250, loss is 3.7500624656677246 and perplexity is 42.523738190801794
At time: 148.2868983745575 and batch: 300, loss is 3.580867328643799 and perplexity is 35.90466849359229
At time: 148.80790400505066 and batch: 350, loss is 3.6032327604293823 and perplexity is 36.71673921333259
At time: 149.3144142627716 and batch: 400, loss is 3.613952283859253 and perplexity is 37.112442244435854
At time: 149.82216572761536 and batch: 450, loss is 3.6528338241577147 and perplexity is 38.58385111971928
At time: 150.33672380447388 and batch: 500, loss is 3.667335648536682 and perplexity is 39.14746418393815
At time: 150.84767770767212 and batch: 550, loss is 3.5652236223220823 and perplexity is 35.34735697858596
At time: 151.35517501831055 and batch: 600, loss is 3.5067488670349123 and perplexity is 33.33969959836025
At time: 151.86276507377625 and batch: 650, loss is 3.47754852771759 and perplexity is 32.3802454241072
At time: 152.37010645866394 and batch: 700, loss is 3.5373166513442995 and perplexity is 34.374556405778485
At time: 152.87703132629395 and batch: 750, loss is 3.4895410633087156 and perplexity is 32.7709044840783
At time: 153.3843207359314 and batch: 800, loss is 3.601548933982849 and perplexity is 36.65496661857425
At time: 153.8915615081787 and batch: 850, loss is 3.455741581916809 and perplexity is 31.68177460396983
At time: 154.39903712272644 and batch: 900, loss is 3.395258936882019 and perplexity is 29.82237458794468
At time: 154.9060342311859 and batch: 950, loss is 3.4055385303497316 and perplexity is 30.130517553707683
At time: 155.4137532711029 and batch: 1000, loss is 3.3092801761627197 and perplexity is 27.365420098851107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.459886504382622 and perplexity of 86.47769370010178
Finished 14 epochs...
Completing Train Step...
At time: 156.94076371192932 and batch: 50, loss is 3.7459502029418945 and perplexity is 42.34922846811421
At time: 157.46900987625122 and batch: 100, loss is 3.6226095151901245 and perplexity is 37.435128009481225
At time: 157.97679543495178 and batch: 150, loss is 3.7249740171432495 and perplexity is 41.470155217729754
At time: 158.4854121208191 and batch: 200, loss is 3.7596695756912233 and perplexity is 42.93423711909711
At time: 158.99333786964417 and batch: 250, loss is 3.7483787059783937 and perplexity is 42.452198679170884
At time: 159.50719118118286 and batch: 300, loss is 3.579119687080383 and perplexity is 35.84197480162001
At time: 160.0155792236328 and batch: 350, loss is 3.601518678665161 and perplexity is 36.653857627690876
At time: 160.5231704711914 and batch: 400, loss is 3.612302656173706 and perplexity is 37.05127100099152
At time: 161.03207325935364 and batch: 450, loss is 3.651397581100464 and perplexity is 38.52847510765213
At time: 161.53984236717224 and batch: 500, loss is 3.66608323097229 and perplexity is 39.09846590175208
At time: 162.06909155845642 and batch: 550, loss is 3.563966417312622 and perplexity is 35.30294602700623
At time: 162.5769100189209 and batch: 600, loss is 3.5056416320800783 and perplexity is 33.30280514676379
At time: 163.08495783805847 and batch: 650, loss is 3.4766529035568237 and perplexity is 32.35125787688533
At time: 163.59278678894043 and batch: 700, loss is 3.5365971755981445 and perplexity is 34.349833640941
At time: 164.09981775283813 and batch: 750, loss is 3.4892218589782713 and perplexity is 32.76044553881353
At time: 164.6073293685913 and batch: 800, loss is 3.601409049034119 and perplexity is 36.64983949905989
At time: 165.11634016036987 and batch: 850, loss is 3.455799493789673 and perplexity is 31.683609408000756
At time: 165.62570905685425 and batch: 900, loss is 3.395563259124756 and perplexity is 29.8314515809584
At time: 166.13573217391968 and batch: 950, loss is 3.406025915145874 and perplexity is 30.14520628910564
At time: 166.6461353302002 and batch: 1000, loss is 3.3100189542770386 and perplexity is 27.385644542077433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.459734288657584 and perplexity of 86.46453143703275
Finished 15 epochs...
Completing Train Step...
At time: 168.17359685897827 and batch: 50, loss is 3.7445065116882326 and perplexity is 42.28813336921685
At time: 168.73289704322815 and batch: 100, loss is 3.621120686531067 and perplexity is 37.37943498701668
At time: 169.25375247001648 and batch: 150, loss is 3.723437876701355 and perplexity is 41.40650013925574
At time: 169.77385067939758 and batch: 200, loss is 3.7581159925460814 and perplexity is 42.867586998616936
At time: 170.30897521972656 and batch: 250, loss is 3.7468875885009765 and perplexity is 42.38894463509099
At time: 170.82065868377686 and batch: 300, loss is 3.5775903129577635 and perplexity is 35.787200908418534
At time: 171.33228063583374 and batch: 350, loss is 3.6000357484817505 and perplexity is 36.5995427983798
At time: 171.84329867362976 and batch: 400, loss is 3.6108950090408327 and perplexity is 36.99915257637907
At time: 172.35785818099976 and batch: 450, loss is 3.6501505899429323 and perplexity is 38.48046038307005
At time: 172.88407015800476 and batch: 500, loss is 3.6649673891067507 and perplexity is 39.05486252837951
At time: 173.42682456970215 and batch: 550, loss is 3.562880263328552 and perplexity is 35.26462240797297
At time: 173.96030807495117 and batch: 600, loss is 3.5046703004837036 and perplexity is 33.27047278514114
At time: 174.47957301139832 and batch: 650, loss is 3.475880069732666 and perplexity is 32.32626538930784
At time: 175.01216006278992 and batch: 700, loss is 3.535965332984924 and perplexity is 34.32813680750342
At time: 175.52814102172852 and batch: 750, loss is 3.4888986110687257 and perplexity is 32.74985750464947
At time: 176.03962898254395 and batch: 800, loss is 3.601251745223999 and perplexity is 36.6440747930834
At time: 176.5515444278717 and batch: 850, loss is 3.4558135747909544 and perplexity is 31.684055548086473
At time: 177.0635483264923 and batch: 900, loss is 3.3957677555084227 and perplexity is 29.837552628726076
At time: 177.5755259990692 and batch: 950, loss is 3.4063770008087157 and perplexity is 30.155791696920886
At time: 178.08714079856873 and batch: 1000, loss is 3.3105788087844847 and perplexity is 27.400980811252765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.459663949361661 and perplexity of 86.45844979666082
Finished 16 epochs...
Completing Train Step...
At time: 179.6671895980835 and batch: 50, loss is 3.7431574010849 and perplexity is 42.23112046709612
At time: 180.1940484046936 and batch: 100, loss is 3.6197415685653684 and perplexity is 37.3279198675568
At time: 180.71272134780884 and batch: 150, loss is 3.722019295692444 and perplexity is 41.34780330746146
At time: 181.22162580490112 and batch: 200, loss is 3.7566876363754274 and perplexity is 42.80640052464796
At time: 181.73004412651062 and batch: 250, loss is 3.745519757270813 and perplexity is 42.33100334878973
At time: 182.23909831047058 and batch: 300, loss is 3.576192183494568 and perplexity is 35.73720072992941
At time: 182.7562129497528 and batch: 350, loss is 3.598694839477539 and perplexity is 36.550499030854716
At time: 183.26790857315063 and batch: 400, loss is 3.609623308181763 and perplexity is 36.95213062752629
At time: 183.7761025428772 and batch: 450, loss is 3.649012598991394 and perplexity is 38.436694874442615
At time: 184.28474354743958 and batch: 500, loss is 3.6639339447021486 and perplexity is 39.014522347484466
At time: 184.7929093837738 and batch: 550, loss is 3.5618900203704835 and perplexity is 35.22971914817027
At time: 185.30698776245117 and batch: 600, loss is 3.5037755489349367 and perplexity is 33.240717291960365
At time: 185.81623125076294 and batch: 650, loss is 3.4751681518554687 and perplexity is 32.30325993304361
At time: 186.324631690979 and batch: 700, loss is 3.5353687381744385 and perplexity is 34.30766292714307
At time: 186.83253931999207 and batch: 750, loss is 3.4885599851608275 and perplexity is 32.73876943187873
At time: 187.34004426002502 and batch: 800, loss is 3.6010630750656127 and perplexity is 36.637161801846354
At time: 187.84817719459534 and batch: 850, loss is 3.4557765197753905 and perplexity is 31.68288151666702
At time: 188.39833784103394 and batch: 900, loss is 3.395889916419983 and perplexity is 29.841197834000194
At time: 188.9307427406311 and batch: 950, loss is 3.406627368927002 and perplexity is 30.163342690968122
At time: 189.44759607315063 and batch: 1000, loss is 3.3110171270370485 and perplexity is 27.412993793842926
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.459642363757622 and perplexity of 86.45658355893966
Finished 17 epochs...
Completing Train Step...
At time: 191.00556254386902 and batch: 50, loss is 3.741875023841858 and perplexity is 42.17699894877954
At time: 191.52802443504333 and batch: 100, loss is 3.6184420728683473 and perplexity is 37.27944390028734
At time: 192.03795862197876 and batch: 150, loss is 3.7206870794296263 and perplexity is 41.29275576721918
At time: 192.54629135131836 and batch: 200, loss is 3.755352177619934 and perplexity is 42.74927249682906
At time: 193.05525088310242 and batch: 250, loss is 3.7442362689971924 and perplexity is 42.2767068542917
At time: 193.56451058387756 and batch: 300, loss is 3.574883666038513 and perplexity is 35.69046856054144
At time: 194.07219099998474 and batch: 350, loss is 3.5974484348297118 and perplexity is 36.504970698239056
At time: 194.5804419517517 and batch: 400, loss is 3.6084407329559327 and perplexity is 36.908457781607
At time: 195.08952045440674 and batch: 450, loss is 3.647946252822876 and perplexity is 38.39572989743765
At time: 195.59781551361084 and batch: 500, loss is 3.6629568004608153 and perplexity is 38.97641815132403
At time: 196.10650372505188 and batch: 550, loss is 3.560960602760315 and perplexity is 35.196991238096565
At time: 196.61408114433289 and batch: 600, loss is 3.5029304361343385 and perplexity is 33.21263700345278
At time: 197.12284445762634 and batch: 650, loss is 3.4744900846481324 and perplexity is 32.281363576232664
At time: 197.63158345222473 and batch: 700, loss is 3.5347885036468507 and perplexity is 34.28776221065165
At time: 198.14136695861816 and batch: 750, loss is 3.488205370903015 and perplexity is 32.7271618556826
At time: 198.6497242450714 and batch: 800, loss is 3.600845122337341 and perplexity is 36.62917750260702
At time: 199.1553909778595 and batch: 850, loss is 3.455694193840027 and perplexity is 31.680273301174513
At time: 199.6631715297699 and batch: 900, loss is 3.3959486627578737 and perplexity is 29.842950946585184
At time: 200.17244672775269 and batch: 950, loss is 3.406803231239319 and perplexity is 30.16864775262749
At time: 200.68123960494995 and batch: 1000, loss is 3.3113655042648316 and perplexity is 27.42254552033145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.459653156559642 and perplexity of 86.4575166727648
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 202.16967630386353 and batch: 50, loss is 3.7408796167373657 and perplexity is 42.13503655267817
At time: 202.6920268535614 and batch: 100, loss is 3.617784242630005 and perplexity is 37.254928419217585
At time: 203.20218706130981 and batch: 150, loss is 3.719966311454773 and perplexity is 41.26300399461919
At time: 203.71406817436218 and batch: 200, loss is 3.7544652318954466 and perplexity is 42.71137302223605
At time: 204.2230293750763 and batch: 250, loss is 3.7433589410781862 and perplexity is 42.23963258456876
At time: 204.73214626312256 and batch: 300, loss is 3.5735635423660277 and perplexity is 35.643383813794955
At time: 205.24081254005432 and batch: 350, loss is 3.59642231464386 and perplexity is 36.467531422804846
At time: 205.74946641921997 and batch: 400, loss is 3.6073198652267457 and perplexity is 36.86711145855392
At time: 206.25882411003113 and batch: 450, loss is 3.646685757637024 and perplexity is 38.34736275442411
At time: 206.76799488067627 and batch: 500, loss is 3.6616114473342893 and perplexity is 38.9240163626623
At time: 207.27735662460327 and batch: 550, loss is 3.5594673013687133 and perplexity is 35.14447074632208
At time: 207.78635430335999 and batch: 600, loss is 3.501621265411377 and perplexity is 33.16918444107297
At time: 208.29329657554626 and batch: 650, loss is 3.4728826522827148 and perplexity is 32.22951515020346
At time: 208.80144572257996 and batch: 700, loss is 3.5330003309249878 and perplexity is 34.226504555529715
At time: 209.31006264686584 and batch: 750, loss is 3.4860904455184936 and perplexity is 32.65801949155349
At time: 209.81943202018738 and batch: 800, loss is 3.598532404899597 and perplexity is 36.54456244813728
At time: 210.3364508152008 and batch: 850, loss is 3.4532931423187256 and perplexity is 31.60429857888058
At time: 210.85616254806519 and batch: 900, loss is 3.3932618713378906 and perplexity is 29.76287678149505
At time: 211.36456656455994 and batch: 950, loss is 3.404095344543457 and perplexity is 30.087064981048385
At time: 211.8740222454071 and batch: 1000, loss is 3.3084561347961428 and perplexity is 27.342879149288862
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.459634176114711 and perplexity of 86.45587568620407
Finished 19 epochs...
Completing Train Step...
At time: 213.3620948791504 and batch: 50, loss is 3.740679769515991 and perplexity is 42.12661682405831
At time: 213.87357139587402 and batch: 100, loss is 3.6175844192504885 and perplexity is 37.24748475725083
At time: 214.39876508712769 and batch: 150, loss is 3.7197317361831663 and perplexity is 41.253325849420996
At time: 214.9097888469696 and batch: 200, loss is 3.754272575378418 and perplexity is 42.70314519047024
At time: 215.42149353027344 and batch: 250, loss is 3.7431562662124636 and perplexity is 42.231072540188734
At time: 215.9329297542572 and batch: 300, loss is 3.573365364074707 and perplexity is 35.63632076878807
At time: 216.44443655014038 and batch: 350, loss is 3.5962187957763674 and perplexity is 36.46011034729949
At time: 216.95716500282288 and batch: 400, loss is 3.607112741470337 and perplexity is 36.859476194690245
At time: 217.46902108192444 and batch: 450, loss is 3.6465188169479372 and perplexity is 38.34096155358653
At time: 217.98089933395386 and batch: 500, loss is 3.661466999053955 and perplexity is 38.9183942614962
At time: 218.49294090270996 and batch: 550, loss is 3.5593280506134035 and perplexity is 35.1395771929491
At time: 219.00377750396729 and batch: 600, loss is 3.501500086784363 and perplexity is 33.16516528836585
At time: 219.5147466659546 and batch: 650, loss is 3.472781023979187 and perplexity is 32.226239885687825
At time: 220.02527451515198 and batch: 700, loss is 3.532897720336914 and perplexity is 34.22299273394768
At time: 220.5356330871582 and batch: 750, loss is 3.486071047782898 and perplexity is 32.657386006070425
At time: 221.0454330444336 and batch: 800, loss is 3.598501806259155 and perplexity is 36.54344425131856
At time: 221.55486845970154 and batch: 850, loss is 3.4533063840866087 and perplexity is 31.604717078437304
At time: 222.06448698043823 and batch: 900, loss is 3.3933018398284913 and perplexity is 29.764066382529062
At time: 222.57573461532593 and batch: 950, loss is 3.404156608581543 and perplexity is 30.08890829260705
At time: 223.0867681503296 and batch: 1000, loss is 3.3085285949707033 and perplexity is 27.34486049086832
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.45962226681593 and perplexity of 86.4548460634802
Finished 20 epochs...
Completing Train Step...
At time: 224.55954480171204 and batch: 50, loss is 3.7404872703552248 and perplexity is 42.11850826614409
At time: 225.08019518852234 and batch: 100, loss is 3.61739173412323 and perplexity is 37.24030841231998
At time: 225.5879077911377 and batch: 150, loss is 3.719512219429016 and perplexity is 41.2442710471094
At time: 226.0953962802887 and batch: 200, loss is 3.7540794467926024 and perplexity is 42.694898788763304
At time: 226.60190987586975 and batch: 250, loss is 3.742965292930603 and perplexity is 42.22300830372061
At time: 227.12260222434998 and batch: 300, loss is 3.5731755971908568 and perplexity is 35.62955881686145
At time: 227.6301646232605 and batch: 350, loss is 3.5960274171829223 and perplexity is 36.453133330311516
At time: 228.13711619377136 and batch: 400, loss is 3.6069251918792724 and perplexity is 36.85256386322581
At time: 228.64462971687317 and batch: 450, loss is 3.646362290382385 and perplexity is 38.3349606442177
At time: 229.152517080307 and batch: 500, loss is 3.6613298511505126 and perplexity is 38.91305705131985
At time: 229.6609878540039 and batch: 550, loss is 3.559194803237915 and perplexity is 35.1348952484477
At time: 230.16955256462097 and batch: 600, loss is 3.5013797426223756 and perplexity is 33.16117429449327
At time: 230.67721891403198 and batch: 650, loss is 3.4726844739913942 and perplexity is 32.223128592820316
At time: 231.18487763404846 and batch: 700, loss is 3.53280659198761 and perplexity is 34.21987419120757
At time: 231.6987464427948 and batch: 750, loss is 3.48604868888855 and perplexity is 32.656655831190015
At time: 232.20830845832825 and batch: 800, loss is 3.5984788370132446 and perplexity is 36.542604885600966
At time: 232.71841144561768 and batch: 850, loss is 3.4533158349990845 and perplexity is 31.6050157732637
At time: 233.22247123718262 and batch: 900, loss is 3.393333730697632 and perplexity is 29.765015599610756
At time: 233.73084568977356 and batch: 950, loss is 3.4042060708999635 and perplexity is 30.09039659657713
At time: 234.23862266540527 and batch: 1000, loss is 3.3085972452163697 and perplexity is 27.346737786696405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.459613707007431 and perplexity of 86.45410602972142
Finished 21 epochs...
Completing Train Step...
At time: 235.72159790992737 and batch: 50, loss is 3.74029953956604 and perplexity is 42.11060206748969
At time: 236.2437949180603 and batch: 100, loss is 3.617203617095947 and perplexity is 37.23330353509537
At time: 236.75250101089478 and batch: 150, loss is 3.719302978515625 and perplexity is 41.23564196097376
At time: 237.27787470817566 and batch: 200, loss is 3.7538878107070923 and perplexity is 42.686717689410145
At time: 237.79096841812134 and batch: 250, loss is 3.7427805280685424 and perplexity is 42.215207696076874
At time: 238.29941272735596 and batch: 300, loss is 3.5729898834228515 and perplexity is 35.62294253162785
At time: 238.80800533294678 and batch: 350, loss is 3.5958423900604246 and perplexity is 36.446389135894066
At time: 239.31672286987305 and batch: 400, loss is 3.6067476224899293 and perplexity is 36.846020556927535
At time: 239.82528734207153 and batch: 450, loss is 3.6462109088897705 and perplexity is 38.32915787988271
At time: 240.3463990688324 and batch: 500, loss is 3.661195311546326 and perplexity is 38.907822056191456
At time: 240.8573579788208 and batch: 550, loss is 3.5590648460388183 and perplexity is 35.13032951255221
At time: 241.36680626869202 and batch: 600, loss is 3.5012609243392943 and perplexity is 33.157234374770525
At time: 241.87545251846313 and batch: 650, loss is 3.472590503692627 and perplexity is 32.22010071806661
At time: 242.38363981246948 and batch: 700, loss is 3.5327212238311767 and perplexity is 34.21695302832342
At time: 242.89199995994568 and batch: 750, loss is 3.486022539138794 and perplexity is 32.65580187897752
At time: 243.40047979354858 and batch: 800, loss is 3.598457179069519 and perplexity is 36.54181345649116
At time: 243.90784883499146 and batch: 850, loss is 3.4533220386505126 and perplexity is 31.6052118403731
At time: 244.4268434047699 and batch: 900, loss is 3.393360905647278 and perplexity is 29.765824473401395
At time: 244.94143533706665 and batch: 950, loss is 3.4042487049102785 and perplexity is 30.091679498203437
At time: 245.45254158973694 and batch: 1000, loss is 3.3086619567871094 and perplexity is 27.34850749431285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.459607008026867 and perplexity of 86.45352687728531
Finished 22 epochs...
Completing Train Step...
At time: 246.95691561698914 and batch: 50, loss is 3.7401156520843504 and perplexity is 42.10285916685612
At time: 247.47064471244812 and batch: 100, loss is 3.6170191097259523 and perplexity is 37.226434349911024
At time: 247.98485016822815 and batch: 150, loss is 3.719101252555847 and perplexity is 41.22732450047419
At time: 248.49930334091187 and batch: 200, loss is 3.753698878288269 and perplexity is 42.67865354639847
At time: 249.01408529281616 and batch: 250, loss is 3.7426000595092774 and perplexity is 42.207589865775
At time: 249.52878642082214 and batch: 300, loss is 3.5728069305419923 and perplexity is 35.61642580781188
At time: 250.0435779094696 and batch: 350, loss is 3.5956618118286134 and perplexity is 36.43980830558324
At time: 250.55788493156433 and batch: 400, loss is 3.6065760231018067 and perplexity is 36.83969834480459
At time: 251.0721309185028 and batch: 450, loss is 3.646062607765198 and perplexity is 38.32347404413523
At time: 251.5863835811615 and batch: 500, loss is 3.661062355041504 and perplexity is 38.902649352040584
At time: 252.10065603256226 and batch: 550, loss is 3.5589368963241577 and perplexity is 35.12583488446446
At time: 252.6140639781952 and batch: 600, loss is 3.5011435747146606 and perplexity is 33.15334361405688
At time: 253.14111948013306 and batch: 650, loss is 3.4724983406066894 and perplexity is 32.217131350990336
At time: 253.65505123138428 and batch: 700, loss is 3.5326391887664794 and perplexity is 34.21414615350064
At time: 254.16952395439148 and batch: 750, loss is 3.485993285179138 and perplexity is 32.65484658144002
At time: 254.68334436416626 and batch: 800, loss is 3.598435039520264 and perplexity is 36.54100444616786
At time: 255.19691276550293 and batch: 850, loss is 3.4533255958557127 and perplexity is 31.605324266796973
At time: 255.71001839637756 and batch: 900, loss is 3.393384323120117 and perplexity is 29.76652152194906
At time: 256.21834921836853 and batch: 950, loss is 3.4042868709564207 and perplexity is 30.092828000548415
At time: 256.72934341430664 and batch: 1000, loss is 3.308723201751709 and perplexity is 27.350182503978626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.459602169874238 and perplexity of 86.45310860293874
Finished 23 epochs...
Completing Train Step...
At time: 258.20527839660645 and batch: 50, loss is 3.7399347496032713 and perplexity is 42.095243344053706
At time: 258.726845741272 and batch: 100, loss is 3.616837248802185 and perplexity is 37.21966493173656
At time: 259.2351462841034 and batch: 150, loss is 3.7189056777954104 and perplexity is 41.21926226477216
At time: 259.7440767288208 and batch: 200, loss is 3.7535123872756957 and perplexity is 42.67069510319546
At time: 260.2535910606384 and batch: 250, loss is 3.7424226570129395 and perplexity is 42.20010279810028
At time: 260.7628436088562 and batch: 300, loss is 3.5726260566711425 and perplexity is 35.60998430957711
At time: 261.2708613872528 and batch: 350, loss is 3.5954847240448 and perplexity is 36.4333558320318
At time: 261.77997756004333 and batch: 400, loss is 3.6064087104797364 and perplexity is 36.83353511388579
At time: 262.2895007133484 and batch: 450, loss is 3.6459166288375853 and perplexity is 38.3178800328057
At time: 262.797739982605 and batch: 500, loss is 3.6609303617477416 and perplexity is 38.89751480208706
At time: 263.3085799217224 and batch: 550, loss is 3.5588105869293214 and perplexity is 35.12139844170518
At time: 263.8158884048462 and batch: 600, loss is 3.5010277891159056 and perplexity is 33.149505156539135
At time: 264.3243796825409 and batch: 650, loss is 3.472407522201538 and perplexity is 32.21420557536163
At time: 264.8328857421875 and batch: 700, loss is 3.532559313774109 and perplexity is 34.211413407978135
At time: 265.34199237823486 and batch: 750, loss is 3.4859611892700197 and perplexity is 32.65379851127134
At time: 265.8510196208954 and batch: 800, loss is 3.5984120178222656 and perplexity is 36.54016321988222
At time: 266.37382984161377 and batch: 850, loss is 3.453326768875122 and perplexity is 31.605361340477515
At time: 266.88243865966797 and batch: 900, loss is 3.3934050607681274 and perplexity is 29.767138815995462
At time: 267.3882625102997 and batch: 950, loss is 3.4043216133117675 and perplexity is 30.093873514433906
At time: 267.89582228660583 and batch: 1000, loss is 3.3087813425064088 and perplexity is 27.351772710458032
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.45959956471513 and perplexity of 86.45288337912882
Finished 24 epochs...
Completing Train Step...
At time: 269.37513041496277 and batch: 50, loss is 3.7397563219070435 and perplexity is 42.08773305680323
At time: 269.89703822135925 and batch: 100, loss is 3.6166576957702636 and perplexity is 37.212982627982875
At time: 270.40611147880554 and batch: 150, loss is 3.71871479511261 and perplexity is 41.21139497229684
At time: 270.91625690460205 and batch: 200, loss is 3.7533282899856566 and perplexity is 42.6628402669121
At time: 271.4251570701599 and batch: 250, loss is 3.7422474908828733 and perplexity is 42.192711416783474
At time: 271.9336223602295 and batch: 300, loss is 3.5724472856521605 and perplexity is 35.60361884539336
At time: 272.44151186943054 and batch: 350, loss is 3.5953106117248534 and perplexity is 36.42701288813281
At time: 272.95064759254456 and batch: 400, loss is 3.6062447118759153 and perplexity is 36.827494960855184
At time: 273.45942306518555 and batch: 450, loss is 3.645772433280945 and perplexity is 38.312355163105494
At time: 273.9683086872101 and batch: 500, loss is 3.660799379348755 and perplexity is 38.89242024593954
At time: 274.4765212535858 and batch: 550, loss is 3.5586856412887573 and perplexity is 35.11701045021529
At time: 274.9846031665802 and batch: 600, loss is 3.500913395881653 and perplexity is 33.145713294316224
At time: 275.49311900138855 and batch: 650, loss is 3.472317657470703 and perplexity is 32.21131078452029
At time: 276.00126123428345 and batch: 700, loss is 3.5324808454513548 and perplexity is 34.20872900107079
At time: 276.50929045677185 and batch: 750, loss is 3.485927200317383 and perplexity is 32.65268866172175
At time: 277.01832818984985 and batch: 800, loss is 3.598387999534607 and perplexity is 36.53928559827043
At time: 277.5263903141022 and batch: 850, loss is 3.453325934410095 and perplexity is 31.60533496691982
At time: 278.0361385345459 and batch: 900, loss is 3.3934231567382813 and perplexity is 29.76767748612491
At time: 278.54410767555237 and batch: 950, loss is 3.4043534231185912 and perplexity is 30.094830809962588
At time: 279.06315326690674 and batch: 1000, loss is 3.308836784362793 and perplexity is 27.353289185550206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.459598820383956 and perplexity of 86.45281902957663
Finished 25 epochs...
Completing Train Step...
At time: 280.5486867427826 and batch: 50, loss is 3.739579930305481 and perplexity is 42.08030978888351
At time: 281.05742478370667 and batch: 100, loss is 3.616480140686035 and perplexity is 37.20637586026792
At time: 281.565806388855 and batch: 150, loss is 3.718527874946594 and perplexity is 41.20369245140755
At time: 282.0750062465668 and batch: 200, loss is 3.7531462955474852 and perplexity is 42.65507657376285
At time: 282.5838432312012 and batch: 250, loss is 3.7420742464065553 and perplexity is 42.18540239573168
At time: 283.0922350883484 and batch: 300, loss is 3.5722701597213744 and perplexity is 35.59731307973981
At time: 283.6014230251312 and batch: 350, loss is 3.5951390266418457 and perplexity is 36.42076309230388
At time: 284.1093680858612 and batch: 400, loss is 3.6060834074020387 and perplexity is 36.821555000240316
At time: 284.618616104126 and batch: 450, loss is 3.64562967300415 and perplexity is 38.30688607107159
At time: 285.12771439552307 and batch: 500, loss is 3.660669403076172 and perplexity is 38.887365482631004
At time: 285.63587975502014 and batch: 550, loss is 3.558562035560608 and perplexity is 35.11267005482256
At time: 286.1440691947937 and batch: 600, loss is 3.5008001565933227 and perplexity is 33.14196010983956
At time: 286.652626991272 and batch: 650, loss is 3.4722286224365235 and perplexity is 32.20844297703319
At time: 287.1604416370392 and batch: 700, loss is 3.5324032926559448 and perplexity is 34.20607612137979
At time: 287.66910004615784 and batch: 750, loss is 3.4858913660049438 and perplexity is 32.651518596038635
At time: 288.1778154373169 and batch: 800, loss is 3.5983630657196044 and perplexity is 36.53837454584106
At time: 288.6874122619629 and batch: 850, loss is 3.453323335647583 and perplexity is 31.605252832266856
At time: 289.19656014442444 and batch: 900, loss is 3.3934389781951904 and perplexity is 29.76814845787726
At time: 289.70537185668945 and batch: 950, loss is 3.404383130073547 and perplexity is 30.09572484902539
At time: 290.2101752758026 and batch: 1000, loss is 3.3088896751403807 and perplexity is 27.354735960545003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.459597703887195 and perplexity of 86.4527225053381
Finished 26 epochs...
Completing Train Step...
At time: 291.6850485801697 and batch: 50, loss is 3.739405426979065 and perplexity is 42.072967275513875
At time: 292.21427273750305 and batch: 100, loss is 3.6163045024871825 and perplexity is 37.19984157327799
At time: 292.7252550125122 and batch: 150, loss is 3.7183442401885984 and perplexity is 41.19612671600296
At time: 293.23636627197266 and batch: 200, loss is 3.7529664754867555 and perplexity is 42.647407024892956
At time: 293.74727416038513 and batch: 250, loss is 3.74190260887146 and perplexity is 42.178162418591164
At time: 294.25895404815674 and batch: 300, loss is 3.572094717025757 and perplexity is 35.59106833898946
At time: 294.77001190185547 and batch: 350, loss is 3.5949697208404543 and perplexity is 36.41459736778244
At time: 295.2811748981476 and batch: 400, loss is 3.605924434661865 and perplexity is 36.8157018420031
At time: 295.79258918762207 and batch: 450, loss is 3.6454882860183715 and perplexity is 38.30147035878007
At time: 296.303427696228 and batch: 500, loss is 3.660540189743042 and perplexity is 38.88234104113975
At time: 296.814560174942 and batch: 550, loss is 3.5584393072128297 and perplexity is 35.10836099926769
At time: 297.325074672699 and batch: 600, loss is 3.5006879281997683 and perplexity is 33.138240849604394
At time: 297.8367841243744 and batch: 650, loss is 3.472140417098999 and perplexity is 32.205602145739334
At time: 298.3474009037018 and batch: 700, loss is 3.5323263692855833 and perplexity is 34.20344497591728
At time: 298.85933208465576 and batch: 750, loss is 3.4858541250228883 and perplexity is 32.65030264406228
At time: 299.3700656890869 and batch: 800, loss is 3.5983373403549193 and perplexity is 36.5374345949212
At time: 299.8826537132263 and batch: 850, loss is 3.4533189725875855 and perplexity is 31.60511493695333
At time: 300.39506483078003 and batch: 900, loss is 3.393452887535095 and perplexity is 29.768562516052125
At time: 300.9063284397125 and batch: 950, loss is 3.4044106435775756 and perplexity is 30.096552899263493
At time: 301.4158499240875 and batch: 1000, loss is 3.308940372467041 and perplexity is 27.356122807684127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.459598076052782 and perplexity of 86.45275468007225
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 302.89861702919006 and batch: 50, loss is 3.7392643308639526 and perplexity is 42.06703136205706
At time: 303.42001461982727 and batch: 100, loss is 3.6162144994735717 and perplexity is 37.196493626095474
At time: 303.9267590045929 and batch: 150, loss is 3.7182442951202392 and perplexity is 41.19200957204974
At time: 304.43466210365295 and batch: 200, loss is 3.752850351333618 and perplexity is 42.64245491840384
At time: 304.9427990913391 and batch: 250, loss is 3.7418030595779417 and perplexity is 42.17396382130765
At time: 305.4642996788025 and batch: 300, loss is 3.571896505355835 and perplexity is 35.58401447300206
At time: 305.9723560810089 and batch: 350, loss is 3.594797649383545 and perplexity is 36.408331994022056
At time: 306.4807434082031 and batch: 400, loss is 3.6057427978515624 and perplexity is 36.809015362625246
At time: 306.9892635345459 and batch: 450, loss is 3.6453316164016725 and perplexity is 38.29547015213726
At time: 307.49725699424744 and batch: 500, loss is 3.6603454637527464 and perplexity is 38.874770375902074
At time: 308.0057625770569 and batch: 550, loss is 3.558210825920105 and perplexity is 35.10034031188454
At time: 308.51382422447205 and batch: 600, loss is 3.500497694015503 and perplexity is 33.131937422971006
At time: 309.0216464996338 and batch: 650, loss is 3.471888403892517 and perplexity is 32.19748693128947
At time: 309.5314393043518 and batch: 700, loss is 3.532089715003967 and perplexity is 34.19535154192838
At time: 310.03952717781067 and batch: 750, loss is 3.485536198616028 and perplexity is 32.63992390058548
At time: 310.54780554771423 and batch: 800, loss is 3.598030252456665 and perplexity is 36.52621611354194
At time: 311.05531072616577 and batch: 850, loss is 3.452986297607422 and perplexity is 31.594602454685482
At time: 311.5627841949463 and batch: 900, loss is 3.3930788612365723 and perplexity is 29.75743037278885
At time: 312.07054591178894 and batch: 950, loss is 3.404026231765747 and perplexity is 30.08498565226932
At time: 312.5776278972626 and batch: 1000, loss is 3.308542847633362 and perplexity is 27.34525023071775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.459599936880717 and perplexity of 86.45291555392292
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 314.0633680820465 and batch: 50, loss is 3.7392436027526856 and perplexity is 42.066159400987395
At time: 314.58953881263733 and batch: 100, loss is 3.616198081970215 and perplexity is 37.19588295754935
At time: 315.1019546985626 and batch: 150, loss is 3.7182286882400515 and perplexity is 41.19136669830829
At time: 315.6097340583801 and batch: 200, loss is 3.75283447265625 and perplexity is 42.64177781799575
At time: 316.11798620224 and batch: 250, loss is 3.7417878246307374 and perplexity is 42.17332130808978
At time: 316.62678122520447 and batch: 300, loss is 3.5718700456619263 and perplexity is 35.58307294332741
At time: 317.1365797519684 and batch: 350, loss is 3.59477436542511 and perplexity is 36.407484273802396
At time: 317.6450357437134 and batch: 400, loss is 3.6057174682617186 and perplexity is 36.80808301717157
At time: 318.1670916080475 and batch: 450, loss is 3.645309524536133 and perplexity is 38.294624143104876
At time: 318.67584252357483 and batch: 500, loss is 3.660318875312805 and perplexity is 38.87373677014575
At time: 319.18384051322937 and batch: 550, loss is 3.5581795740127564 and perplexity is 35.099243376441954
At time: 319.69233083724976 and batch: 600, loss is 3.500472192764282 and perplexity is 33.131092527884334
At time: 320.20096707344055 and batch: 650, loss is 3.471853985786438 and perplexity is 32.19637877383924
At time: 320.7093412876129 and batch: 700, loss is 3.532057046890259 and perplexity is 34.19423446254245
At time: 321.2181031703949 and batch: 750, loss is 3.4854928779602052 and perplexity is 32.638509948302996
At time: 321.72590160369873 and batch: 800, loss is 3.597988247871399 and perplexity is 36.52468187720546
At time: 322.2523236274719 and batch: 850, loss is 3.4529412603378296 and perplexity is 31.59317955209913
At time: 322.77222657203674 and batch: 900, loss is 3.393028221130371 and perplexity is 29.755923491509133
At time: 323.2888243198395 and batch: 950, loss is 3.4039739847183226 and perplexity is 30.08341384165877
At time: 323.80459547042847 and batch: 1000, loss is 3.308488426208496 and perplexity is 27.34376210373025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.459601425543064 and perplexity of 86.4530442532189
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 325.29253029823303 and batch: 50, loss is 3.73924129486084 and perplexity is 42.06606231695317
At time: 325.81762051582336 and batch: 100, loss is 3.616196608543396 and perplexity is 37.19582815217823
At time: 326.33030796051025 and batch: 150, loss is 3.718227505683899 and perplexity is 41.19131798723298
At time: 326.8431589603424 and batch: 200, loss is 3.7528329944610594 and perplexity is 42.641714785171445
At time: 327.3558716773987 and batch: 250, loss is 3.7417862749099733 and perplexity is 42.1732559512687
At time: 327.8678529262543 and batch: 300, loss is 3.5718671321868896 and perplexity is 35.58296927308368
At time: 328.3797550201416 and batch: 350, loss is 3.5947718620300293 and perplexity is 36.40739313159945
At time: 328.89217019081116 and batch: 400, loss is 3.6057146787643433 and perplexity is 36.80798034126381
At time: 329.4049050807953 and batch: 450, loss is 3.6453071069717407 and perplexity is 38.294531563497046
At time: 329.9171230792999 and batch: 500, loss is 3.660315909385681 and perplexity is 38.87362147364644
At time: 330.4300148487091 and batch: 550, loss is 3.558176064491272 and perplexity is 35.099120195109386
At time: 330.9415657520294 and batch: 600, loss is 3.5004693841934205 and perplexity is 33.130999476993914
At time: 331.4669644832611 and batch: 650, loss is 3.471849994659424 and perplexity is 32.19625027425859
At time: 331.97981786727905 and batch: 700, loss is 3.532053289413452 and perplexity is 34.19410597874092
At time: 332.49249148368835 and batch: 750, loss is 3.4854875373840333 and perplexity is 32.63833564031993
At time: 333.00394797325134 and batch: 800, loss is 3.5979831552505495 and perplexity is 36.52449587132264
At time: 333.51631355285645 and batch: 850, loss is 3.4529357814788817 and perplexity is 31.59300645799883
At time: 334.0283238887787 and batch: 900, loss is 3.3930218744277956 and perplexity is 29.755734640112163
At time: 334.5407292842865 and batch: 950, loss is 3.4039674282073973 and perplexity is 30.083216600073857
At time: 335.0528736114502 and batch: 1000, loss is 3.308481593132019 and perplexity is 27.343575262350974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.459601053377477 and perplexity of 86.45301207837691
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 336.5237045288086 and batch: 50, loss is 3.739241237640381 and perplexity is 42.06605990991384
At time: 337.04647636413574 and batch: 100, loss is 3.6161965465545656 and perplexity is 37.195825846452415
At time: 337.5565905570984 and batch: 150, loss is 3.7182274866104126 and perplexity is 41.19131720157094
At time: 338.0651307106018 and batch: 200, loss is 3.75283296585083 and perplexity is 42.64171356518223
At time: 338.57371520996094 and batch: 250, loss is 3.741786298751831 and perplexity is 42.173256956757484
At time: 339.0838544368744 and batch: 300, loss is 3.5718670177459715 and perplexity is 35.582965200936236
At time: 339.59265065193176 and batch: 350, loss is 3.5947717237472534 and perplexity is 36.407388097084414
At time: 340.10058641433716 and batch: 400, loss is 3.6057145977020264 and perplexity is 36.80797735752376
At time: 340.60899448394775 and batch: 450, loss is 3.6453070974349977 and perplexity is 38.29453119829194
At time: 341.1313762664795 and batch: 500, loss is 3.66031578540802 and perplexity is 38.873616654186065
At time: 341.6447958946228 and batch: 550, loss is 3.558175835609436 and perplexity is 35.09911216155925
At time: 342.15390396118164 and batch: 600, loss is 3.500469274520874 and perplexity is 33.13099584343303
At time: 342.6626732349396 and batch: 650, loss is 3.471849784851074 and perplexity is 32.196243519217155
At time: 343.17152166366577 and batch: 700, loss is 3.5320531368255614 and perplexity is 34.19410076113481
At time: 343.6795127391815 and batch: 750, loss is 3.4854871892929076 and perplexity is 32.63832427920692
At time: 344.20201778411865 and batch: 800, loss is 3.5979828882217406 and perplexity is 36.524486118231316
At time: 344.71099162101746 and batch: 850, loss is 3.452935438156128 and perplexity is 31.592995611402714
At time: 345.2194347381592 and batch: 900, loss is 3.393021569252014 and perplexity is 29.755725559383976
At time: 345.72798252105713 and batch: 950, loss is 3.4039670848846435 and perplexity is 30.083206271822867
At time: 346.2369029521942 and batch: 1000, loss is 3.308481240272522 and perplexity is 27.343565613912457
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.459601425543064 and perplexity of 86.4530442532189
Annealing...
Model not improving. Stopping early with 86.4527225053381loss at 30 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f46fd764898>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'batch_size': 20, 'lr': 17.388210957216543, 'dropout': 0.6548801797608832, 'wordvec_source': '', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'anneal': 3.598069729244449, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7499797344207764 and batch: 50, loss is 6.582654266357422 and perplexity is 722.4543729437005
At time: 1.2619211673736572 and batch: 100, loss is 6.100818653106689 and perplexity is 446.22292237727316
At time: 1.7729392051696777 and batch: 150, loss is 5.935456857681275 and perplexity is 378.2127461057862
At time: 2.283581018447876 and batch: 200, loss is 5.900921640396118 and perplexity is 365.374056224699
At time: 2.7941579818725586 and batch: 250, loss is 5.95380877494812 and perplexity is 385.2177562125359
At time: 3.305161476135254 and batch: 300, loss is 5.860291528701782 and perplexity is 350.82640507959593
At time: 3.8165769577026367 and batch: 350, loss is 5.8434481525421145 and perplexity is 344.96679034969105
At time: 4.329277515411377 and batch: 400, loss is 5.837087841033935 and perplexity is 342.77965690121806
At time: 4.863232851028442 and batch: 450, loss is 5.85509843826294 and perplexity is 349.00925422781114
At time: 5.372641086578369 and batch: 500, loss is 5.858408651351929 and perplexity is 350.1664634772589
At time: 5.881717205047607 and batch: 550, loss is 5.803142518997192 and perplexity is 331.3391651855482
At time: 6.393207788467407 and batch: 600, loss is 5.706958618164062 and perplexity is 300.9543578387976
At time: 6.90418267250061 and batch: 650, loss is 5.691062526702881 and perplexity is 296.20818259927296
At time: 7.414980888366699 and batch: 700, loss is 5.75967845916748 and perplexity is 317.2463048752983
At time: 7.926418781280518 and batch: 750, loss is 5.654762392044067 and perplexity is 285.6486022818622
At time: 8.43801999092102 and batch: 800, loss is 5.743211908340454 and perplexity is 312.0651276021606
At time: 8.948949337005615 and batch: 850, loss is 5.70294734954834 and perplexity is 299.7495670536173
At time: 9.459979772567749 and batch: 900, loss is 5.7379881954193115 and perplexity is 310.4392392470286
At time: 9.970036506652832 and batch: 950, loss is 5.698017377853393 and perplexity is 298.27544684358554
At time: 10.480734825134277 and batch: 1000, loss is 5.615781145095825 and perplexity is 274.7278977494279
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.478636950981326 and perplexity of 239.52000725985326
Finished 1 epochs...
Completing Train Step...
At time: 11.968607425689697 and batch: 50, loss is 5.43208981513977 and perplexity is 228.62653371966522
At time: 12.475137710571289 and batch: 100, loss is 5.348630228042603 and perplexity is 210.32001001564407
At time: 12.977927446365356 and batch: 150, loss is 5.308548212051392 and perplexity is 202.0566719076163
At time: 13.480375528335571 and batch: 200, loss is 5.271524810791016 and perplexity is 194.7126361310122
At time: 13.983935356140137 and batch: 250, loss is 5.308432722091675 and perplexity is 202.0333377381743
At time: 14.490159749984741 and batch: 300, loss is 5.231815948486328 and perplexity is 187.13231780590363
At time: 14.996824264526367 and batch: 350, loss is 5.197235279083252 and perplexity is 180.77176687205704
At time: 15.502692699432373 and batch: 400, loss is 5.213488636016845 and perplexity is 183.73392219694733
At time: 16.009001970291138 and batch: 450, loss is 5.21619875907898 and perplexity is 184.2325390881723
At time: 16.51604676246643 and batch: 500, loss is 5.253434514999389 and perplexity is 191.22189640355793
At time: 17.02076506614685 and batch: 550, loss is 5.18286940574646 and perplexity is 178.19338724717562
At time: 17.528048515319824 and batch: 600, loss is 5.10473087310791 and perplexity is 164.79971252849316
At time: 18.034945487976074 and batch: 650, loss is 5.088357019424438 and perplexity is 162.12327772087508
At time: 18.54210615158081 and batch: 700, loss is 5.159609804153442 and perplexity is 174.09651061470424
At time: 19.049938201904297 and batch: 750, loss is 5.0863910675048825 and perplexity is 161.80486424717975
At time: 19.556829690933228 and batch: 800, loss is 5.1978585910797115 and perplexity is 180.8844792067956
At time: 20.06486201286316 and batch: 850, loss is 5.12034951210022 and perplexity is 167.3938655795086
At time: 20.57236886024475 and batch: 900, loss is 5.127501163482666 and perplexity is 168.59529914598912
At time: 21.080917358398438 and batch: 950, loss is 5.132819671630859 and perplexity is 169.49436333907963
At time: 21.588591814041138 and batch: 1000, loss is 5.039027166366577 and perplexity is 154.3198144718396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.218803591844512 and perplexity of 184.71305960856407
Finished 2 epochs...
Completing Train Step...
At time: 23.06616759300232 and batch: 50, loss is 5.1447885131835935 and perplexity is 171.53520340062337
At time: 23.590158939361572 and batch: 100, loss is 5.080537986755371 and perplexity is 160.86057351187443
At time: 24.10189962387085 and batch: 150, loss is 5.118984384536743 and perplexity is 167.16550750406412
At time: 24.6130690574646 and batch: 200, loss is 5.127946214675903 and perplexity is 168.67034938440855
At time: 25.123108386993408 and batch: 250, loss is 5.162743692398071 and perplexity is 174.64296543968743
At time: 25.636639833450317 and batch: 300, loss is 5.072206201553345 and perplexity is 159.5258856531835
At time: 26.150214433670044 and batch: 350, loss is 5.056451110839844 and perplexity is 157.03223627119965
At time: 26.66404366493225 and batch: 400, loss is 5.106793098449707 and perplexity is 165.13991734091513
At time: 27.17766046524048 and batch: 450, loss is 5.125778312683106 and perplexity is 168.30508466995923
At time: 27.6917245388031 and batch: 500, loss is 5.1352795886993405 and perplexity is 169.9118186586934
At time: 28.203794717788696 and batch: 550, loss is 5.09211109161377 and perplexity is 162.7330440462899
At time: 28.71616291999817 and batch: 600, loss is 5.013756151199341 and perplexity is 150.4688598071445
At time: 29.2302348613739 and batch: 650, loss is 4.97470687866211 and perplexity is 144.706402396706
At time: 29.7450270652771 and batch: 700, loss is 5.062841377258301 and perplexity is 158.0389271830974
At time: 30.25964593887329 and batch: 750, loss is 5.01602988243103 and perplexity is 150.81137479903919
At time: 30.775906324386597 and batch: 800, loss is 5.118124942779541 and perplexity is 167.02190020644494
At time: 31.290359020233154 and batch: 850, loss is 5.064878540039063 and perplexity is 158.36120635958227
At time: 31.804604530334473 and batch: 900, loss is 5.05547571182251 and perplexity is 156.8791418584587
At time: 32.3195538520813 and batch: 950, loss is 5.048281488418579 and perplexity is 155.75456833146674
At time: 32.83334755897522 and batch: 1000, loss is 4.9493342304229735 and perplexity is 141.08100520635773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.180982636242378 and perplexity of 177.85749437414879
Finished 3 epochs...
Completing Train Step...
At time: 34.330363035202026 and batch: 50, loss is 5.057472629547119 and perplexity is 157.1927295977074
At time: 34.84034705162048 and batch: 100, loss is 5.014758977890015 and perplexity is 150.61982968153208
At time: 35.35197949409485 and batch: 150, loss is 5.041397762298584 and perplexity is 154.686078356567
At time: 35.86307883262634 and batch: 200, loss is 5.07501615524292 and perplexity is 159.97477638988414
At time: 36.37381386756897 and batch: 250, loss is 5.106829519271851 and perplexity is 165.1459319820019
At time: 36.88481855392456 and batch: 300, loss is 5.036284532546997 and perplexity is 153.89715159932254
At time: 37.39514088630676 and batch: 350, loss is 5.029627857208252 and perplexity is 152.8761103658723
At time: 37.90735983848572 and batch: 400, loss is 5.04609414100647 and perplexity is 155.4142513109667
At time: 38.41847896575928 and batch: 450, loss is 5.090775814056396 and perplexity is 162.51589527393705
At time: 38.930135011672974 and batch: 500, loss is 5.096056537628174 and perplexity is 163.37636674930573
At time: 39.44008207321167 and batch: 550, loss is 5.036026792526245 and perplexity is 153.85749125552417
At time: 39.950193643569946 and batch: 600, loss is 4.988186702728272 and perplexity is 146.67022550084076
At time: 40.46110725402832 and batch: 650, loss is 4.942606744766235 and perplexity is 140.13507021710484
At time: 40.97243642807007 and batch: 700, loss is 5.025584831237793 and perplexity is 152.25927606019295
At time: 41.4845335483551 and batch: 750, loss is 4.949125061035156 and perplexity is 141.05149846492714
At time: 41.99595618247986 and batch: 800, loss is 5.072377271652222 and perplexity is 159.55317809660957
At time: 42.507489919662476 and batch: 850, loss is 4.994625415802002 and perplexity is 147.6176397906075
At time: 43.019179821014404 and batch: 900, loss is 4.988854789733887 and perplexity is 146.76824671231145
At time: 43.5305700302124 and batch: 950, loss is 5.022060966491699 and perplexity is 151.72367920469378
At time: 44.042760372161865 and batch: 1000, loss is 4.948112535476684 and perplexity is 140.90875249683657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.222949516482469 and perplexity of 185.48045571735392
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 45.53512644767761 and batch: 50, loss is 5.00323260307312 and perplexity is 148.89369621119351
At time: 46.047213077545166 and batch: 100, loss is 4.880904378890992 and perplexity is 131.74976172632472
At time: 46.55839014053345 and batch: 150, loss is 4.904444465637207 and perplexity is 134.88795429476383
At time: 47.09264850616455 and batch: 200, loss is 4.922098159790039 and perplexity is 137.29036833316144
At time: 47.6301474571228 and batch: 250, loss is 4.942634153366089 and perplexity is 140.1389111758073
At time: 48.14545559883118 and batch: 300, loss is 4.839828805923462 and perplexity is 126.44770277937393
At time: 48.65717124938965 and batch: 350, loss is 4.840904378890992 and perplexity is 126.58377967748773
At time: 49.1698899269104 and batch: 400, loss is 4.836344375610351 and perplexity is 126.00787129687015
At time: 49.68081474304199 and batch: 450, loss is 4.876209726333618 and perplexity is 131.13269196790148
At time: 50.19203424453735 and batch: 500, loss is 4.913261623382568 and perplexity is 136.08254135786862
At time: 50.70409178733826 and batch: 550, loss is 4.8308141231536865 and perplexity is 125.31293930288629
At time: 51.211604595184326 and batch: 600, loss is 4.7635069179534915 and perplexity is 117.15606302080171
At time: 51.72275996208191 and batch: 650, loss is 4.716723966598511 and perplexity is 111.80138698570919
At time: 52.23297190666199 and batch: 700, loss is 4.814797058105468 and perplexity is 123.32178261427671
At time: 52.743510007858276 and batch: 750, loss is 4.738648090362549 and perplexity is 114.2796015063321
At time: 53.254392862319946 and batch: 800, loss is 4.8333804893493655 and perplexity is 125.63495121770362
At time: 53.76584196090698 and batch: 850, loss is 4.738285932540894 and perplexity is 114.23822174824474
At time: 54.277344703674316 and batch: 900, loss is 4.74120418548584 and perplexity is 114.57208468660156
At time: 54.78936815261841 and batch: 950, loss is 4.722508821487427 and perplexity is 112.45001608958354
At time: 55.30051064491272 and batch: 1000, loss is 4.636768465042114 and perplexity is 103.21028046986862
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.014165366568217 and perplexity of 150.53044657738184
Finished 5 epochs...
Completing Train Step...
At time: 56.796040773391724 and batch: 50, loss is 4.8483357238769536 and perplexity is 127.52797137578351
At time: 57.34270453453064 and batch: 100, loss is 4.765846109390258 and perplexity is 117.43043425850108
At time: 57.86091899871826 and batch: 150, loss is 4.802676610946655 and perplexity is 121.83608928627444
At time: 58.37789463996887 and batch: 200, loss is 4.835286502838135 and perplexity is 125.87464148326109
At time: 58.895604848861694 and batch: 250, loss is 4.850934066772461 and perplexity is 127.85976364255438
At time: 59.4173309803009 and batch: 300, loss is 4.748594055175781 and perplexity is 115.42189358369845
At time: 59.93372893333435 and batch: 350, loss is 4.753575096130371 and perplexity is 115.99824899600044
At time: 60.46579432487488 and batch: 400, loss is 4.752975549697876 and perplexity is 115.9287235036023
At time: 60.98226499557495 and batch: 450, loss is 4.801497421264648 and perplexity is 121.69250609943582
At time: 61.4962260723114 and batch: 500, loss is 4.834322719573975 and perplexity is 125.75338405274461
At time: 62.01298975944519 and batch: 550, loss is 4.760103092193604 and perplexity is 116.75796211299435
At time: 62.52619028091431 and batch: 600, loss is 4.696315298080444 and perplexity is 109.54279535678896
At time: 63.03940415382385 and batch: 650, loss is 4.650812835693359 and perplexity is 104.67003054549119
At time: 63.55385684967041 and batch: 700, loss is 4.7539395904541015 and perplexity is 116.04053740579666
At time: 64.06866312026978 and batch: 750, loss is 4.6837348461151125 and perplexity is 108.17332979134449
At time: 64.5833842754364 and batch: 800, loss is 4.776387414932251 and perplexity is 118.67485171507124
At time: 65.09831809997559 and batch: 850, loss is 4.687342987060547 and perplexity is 108.56433939644941
At time: 65.61350083351135 and batch: 900, loss is 4.686380920410156 and perplexity is 108.4599434920555
At time: 66.12880682945251 and batch: 950, loss is 4.683362836837769 and perplexity is 108.1330957932752
At time: 66.6441740989685 and batch: 1000, loss is 4.601442852020264 and perplexity is 99.62796039188092
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.9979467624571265 and perplexity of 148.10874425739073
Finished 6 epochs...
Completing Train Step...
At time: 68.14164304733276 and batch: 50, loss is 4.786769638061523 and perplexity is 119.91337871212166
At time: 68.65294885635376 and batch: 100, loss is 4.708873453140259 and perplexity is 110.92712488581806
At time: 69.16360902786255 and batch: 150, loss is 4.747128839492798 and perplexity is 115.25289945172011
At time: 69.67488074302673 and batch: 200, loss is 4.780848894119263 and perplexity is 119.20549995385275
At time: 70.20510268211365 and batch: 250, loss is 4.797064580917358 and perplexity is 121.15425651705495
At time: 70.72388362884521 and batch: 300, loss is 4.687720937728882 and perplexity is 108.6053791160878
At time: 71.24775004386902 and batch: 350, loss is 4.704223365783691 and perplexity is 110.41250151402485
At time: 71.77746295928955 and batch: 400, loss is 4.701762866973877 and perplexity is 110.14116563315714
At time: 72.3026671409607 and batch: 450, loss is 4.7506607627868656 and perplexity is 115.66068355918866
At time: 72.81753063201904 and batch: 500, loss is 4.781014432907105 and perplexity is 119.22523472120886
At time: 73.33133316040039 and batch: 550, loss is 4.715577745437622 and perplexity is 111.6733112856907
At time: 73.86324286460876 and batch: 600, loss is 4.65841305732727 and perplexity is 105.46857669604044
At time: 74.37137269973755 and batch: 650, loss is 4.614090032577515 and perplexity is 100.89597469678615
At time: 74.89306378364563 and batch: 700, loss is 4.70782169342041 and perplexity is 110.81051753623038
At time: 75.41474223136902 and batch: 750, loss is 4.645653562545776 and perplexity is 104.13139993354392
At time: 75.92655897140503 and batch: 800, loss is 4.7387409496307376 and perplexity is 114.29021391921968
At time: 76.43751215934753 and batch: 850, loss is 4.649677276611328 and perplexity is 104.551239001862
At time: 76.9508707523346 and batch: 900, loss is 4.650818777084351 and perplexity is 104.6706524329152
At time: 77.48878264427185 and batch: 950, loss is 4.649120969772339 and perplexity is 104.49309260769842
At time: 78.01411366462708 and batch: 1000, loss is 4.571875429153442 and perplexity is 96.7253413012322
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.989124205054306 and perplexity of 146.80779365356352
Finished 7 epochs...
Completing Train Step...
At time: 79.58339428901672 and batch: 50, loss is 4.74286563873291 and perplexity is 114.76259907027334
At time: 80.11568593978882 and batch: 100, loss is 4.6678080558776855 and perplexity is 106.4641230768861
At time: 80.64454960823059 and batch: 150, loss is 4.709366445541382 and perplexity is 110.98182459762842
At time: 81.16754627227783 and batch: 200, loss is 4.74718132019043 and perplexity is 115.25894816300637
At time: 81.69044518470764 and batch: 250, loss is 4.7610604858398435 and perplexity is 116.86979897147559
At time: 82.22909545898438 and batch: 300, loss is 4.653468084335327 and perplexity is 104.94832480867821
At time: 82.75920414924622 and batch: 350, loss is 4.665487251281738 and perplexity is 106.2173271440863
At time: 83.2944815158844 and batch: 400, loss is 4.665649967193604 and perplexity is 106.23461179953455
At time: 83.82038927078247 and batch: 450, loss is 4.718441276550293 and perplexity is 111.99354957433934
At time: 84.34597826004028 and batch: 500, loss is 4.750634813308716 and perplexity is 115.65768226374908
At time: 84.88818502426147 and batch: 550, loss is 4.680647811889648 and perplexity is 107.83990992405188
At time: 85.4156174659729 and batch: 600, loss is 4.6254056930542 and perplexity is 102.04416329454146
At time: 85.9399299621582 and batch: 650, loss is 4.586568336486817 and perplexity is 98.15700970756947
At time: 86.46774864196777 and batch: 700, loss is 4.681776161193848 and perplexity is 107.96165968659216
At time: 87.03456664085388 and batch: 750, loss is 4.618015880584717 and perplexity is 101.29285549509335
At time: 87.5490870475769 and batch: 800, loss is 4.703576402664185 and perplexity is 110.34109179982933
At time: 88.05977821350098 and batch: 850, loss is 4.619131927490234 and perplexity is 101.4059661796939
At time: 88.58094072341919 and batch: 900, loss is 4.619103450775146 and perplexity is 101.40307851200261
At time: 89.10757231712341 and batch: 950, loss is 4.623079099655151 and perplexity is 101.80702398815176
At time: 89.6310179233551 and batch: 1000, loss is 4.5455908203125 and perplexity is 94.2160755398991
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.9837519948075455 and perplexity of 146.02122601529297
Finished 8 epochs...
Completing Train Step...
At time: 91.19002294540405 and batch: 50, loss is 4.705241661071778 and perplexity is 110.52499130825586
At time: 91.72326254844666 and batch: 100, loss is 4.625050497055054 and perplexity is 102.007924052401
At time: 92.24157667160034 and batch: 150, loss is 4.672198724746704 and perplexity is 106.93259949755914
At time: 92.77421975135803 and batch: 200, loss is 4.712089939117432 and perplexity is 111.28449485714613
At time: 93.29286456108093 and batch: 250, loss is 4.716435050964355 and perplexity is 111.76909048279572
At time: 93.81496262550354 and batch: 300, loss is 4.616815843582153 and perplexity is 101.17137322659136
At time: 94.33157753944397 and batch: 350, loss is 4.634865074157715 and perplexity is 103.0140178043835
At time: 94.85370469093323 and batch: 400, loss is 4.63339825630188 and perplexity is 102.86302576964532
At time: 95.37529134750366 and batch: 450, loss is 4.683288478851319 and perplexity is 108.1250555329359
At time: 95.8965003490448 and batch: 500, loss is 4.717921724319458 and perplexity is 111.935378188664
At time: 96.41929793357849 and batch: 550, loss is 4.650856161117554 and perplexity is 104.67456551720414
At time: 96.94169592857361 and batch: 600, loss is 4.594109582901001 and perplexity is 98.90003404853972
At time: 97.4695372581482 and batch: 650, loss is 4.558997840881347 and perplexity is 95.48773795647084
At time: 97.9941258430481 and batch: 700, loss is 4.655181665420532 and perplexity is 105.12831644408186
At time: 98.51358270645142 and batch: 750, loss is 4.595210494995118 and perplexity is 99.00897424792082
At time: 99.03320932388306 and batch: 800, loss is 4.6838839149475096 and perplexity is 108.18945626526043
At time: 99.55558562278748 and batch: 850, loss is 4.592758846282959 and perplexity is 98.76653633144934
At time: 100.08589029312134 and batch: 900, loss is 4.59100998878479 and perplexity is 98.59395868465944
At time: 100.62430143356323 and batch: 950, loss is 4.595971393585205 and perplexity is 99.0843387055507
At time: 101.14263606071472 and batch: 1000, loss is 4.521087303161621 and perplexity is 91.93550541918323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.982261843797637 and perplexity of 145.8037943810977
Finished 9 epochs...
Completing Train Step...
At time: 102.73993062973022 and batch: 50, loss is 4.682436199188232 and perplexity is 108.03294200585378
At time: 103.27963495254517 and batch: 100, loss is 4.6028549861907955 and perplexity is 99.76874782106484
At time: 103.81252598762512 and batch: 150, loss is 4.638079595565796 and perplexity is 103.34569137023446
At time: 104.33066749572754 and batch: 200, loss is 4.68309232711792 and perplexity is 108.10384869581618
At time: 104.86180138587952 and batch: 250, loss is 4.6908108997344975 and perplexity is 108.9414846203569
At time: 105.38476848602295 and batch: 300, loss is 4.589477376937866 and perplexity is 98.44296815004044
At time: 105.90455317497253 and batch: 350, loss is 4.614432344436645 and perplexity is 100.93051849750239
At time: 106.4321768283844 and batch: 400, loss is 4.606574058532715 and perplexity is 100.14048584349851
At time: 106.95624852180481 and batch: 450, loss is 4.6590923309326175 and perplexity is 105.54024305416067
At time: 107.47021126747131 and batch: 500, loss is 4.694789390563965 and perplexity is 109.37577064651661
At time: 107.99370861053467 and batch: 550, loss is 4.625811614990234 and perplexity is 102.08559366704667
At time: 108.50704574584961 and batch: 600, loss is 4.57254472732544 and perplexity is 96.79010106472914
At time: 109.02698922157288 and batch: 650, loss is 4.539503574371338 and perplexity is 93.64430114772037
At time: 109.54937219619751 and batch: 700, loss is 4.63254412651062 and perplexity is 102.77520490546358
At time: 110.07623600959778 and batch: 750, loss is 4.574658422470093 and perplexity is 96.99490219873216
At time: 110.60388803482056 and batch: 800, loss is 4.657774868011475 and perplexity is 105.40128925058903
At time: 111.11937594413757 and batch: 850, loss is 4.571611404418945 and perplexity is 96.69980678969569
At time: 111.64108085632324 and batch: 900, loss is 4.570692739486694 and perplexity is 96.61101286042182
At time: 112.1596052646637 and batch: 950, loss is 4.572860345840454 and perplexity is 96.82065463407778
At time: 112.69718480110168 and batch: 1000, loss is 4.502229204177857 and perplexity is 90.2180216953581
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.9813425947980186 and perplexity of 145.66982597356838
Finished 10 epochs...
Completing Train Step...
At time: 114.35321927070618 and batch: 50, loss is 4.662080354690552 and perplexity is 105.85607142417741
At time: 114.86706352233887 and batch: 100, loss is 4.587812986373901 and perplexity is 98.27925688030177
At time: 115.38840055465698 and batch: 150, loss is 4.622031936645508 and perplexity is 101.70047123729579
At time: 115.92406678199768 and batch: 200, loss is 4.661056709289551 and perplexity is 105.74776778521414
At time: 116.44385838508606 and batch: 250, loss is 4.6639063262939455 and perplexity is 106.04953818329463
At time: 116.95538687705994 and batch: 300, loss is 4.563710432052613 and perplexity is 95.93879461536824
At time: 117.46917033195496 and batch: 350, loss is 4.589998044967651 and perplexity is 98.49423760233744
At time: 117.98312020301819 and batch: 400, loss is 4.587373933792114 and perplexity is 98.2361165899533
At time: 118.5153694152832 and batch: 450, loss is 4.640338401794434 and perplexity is 103.57939310570613
At time: 119.03224849700928 and batch: 500, loss is 4.67043321609497 and perplexity is 106.74397562552818
At time: 119.54932594299316 and batch: 550, loss is 4.604465847015381 and perplexity is 99.92959090157575
At time: 120.06547951698303 and batch: 600, loss is 4.553642597198486 and perplexity is 94.97774463905986
At time: 120.58282208442688 and batch: 650, loss is 4.522179822921753 and perplexity is 92.03600166258131
At time: 121.09696340560913 and batch: 700, loss is 4.612707996368409 and perplexity is 100.75662911889867
At time: 121.61646866798401 and batch: 750, loss is 4.556833438873291 and perplexity is 95.28128760606495
At time: 122.13476991653442 and batch: 800, loss is 4.6348684120178225 and perplexity is 103.0143616513379
At time: 122.65566086769104 and batch: 850, loss is 4.551414613723755 and perplexity is 94.76637134907182
At time: 123.1742537021637 and batch: 900, loss is 4.55285490989685 and perplexity is 94.90296133246676
At time: 123.68897342681885 and batch: 950, loss is 4.554562015533447 and perplexity is 95.06510907497204
At time: 124.20594358444214 and batch: 1000, loss is 4.485679502487183 and perplexity is 88.73722749835335
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.984043772627667 and perplexity of 146.06383798662296
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 125.77948760986328 and batch: 50, loss is 4.615814275741577 and perplexity is 101.07009396027516
At time: 126.32249927520752 and batch: 100, loss is 4.514718084335327 and perplexity is 91.3518088859676
At time: 126.84766840934753 and batch: 150, loss is 4.539913187026977 and perplexity is 93.68266689561051
At time: 127.38170456886292 and batch: 200, loss is 4.5666849899291995 and perplexity is 96.22459496667472
At time: 127.8949294090271 and batch: 250, loss is 4.570767726898193 and perplexity is 96.61825774183258
At time: 128.4073555469513 and batch: 300, loss is 4.463748302459717 and perplexity is 86.81229876409694
At time: 128.9251787662506 and batch: 350, loss is 4.48345703125 and perplexity is 88.54023055367949
At time: 129.44514846801758 and batch: 400, loss is 4.480230188369751 and perplexity is 88.25498560892098
At time: 129.97980070114136 and batch: 450, loss is 4.536816024780274 and perplexity is 93.39296533447796
At time: 130.50779628753662 and batch: 500, loss is 4.555488109588623 and perplexity is 95.15318908623327
At time: 131.0271978378296 and batch: 550, loss is 4.487394618988037 and perplexity is 88.88955277193062
At time: 131.54176807403564 and batch: 600, loss is 4.427803020477295 and perplexity is 83.74722371443251
At time: 132.0552089214325 and batch: 650, loss is 4.394386501312256 and perplexity is 80.9949252368023
At time: 132.56738948822021 and batch: 700, loss is 4.483771638870239 and perplexity is 88.5680903671344
At time: 133.0829725265503 and batch: 750, loss is 4.422223348617553 and perplexity is 83.28124290588147
At time: 133.59914803504944 and batch: 800, loss is 4.495723867416382 and perplexity is 89.63302793602729
At time: 134.11659336090088 and batch: 850, loss is 4.4087936973571775 and perplexity is 82.1702814667571
At time: 134.62910079956055 and batch: 900, loss is 4.404815406799316 and perplexity is 81.84403359655995
At time: 135.1421287059784 and batch: 950, loss is 4.398825473785401 and perplexity is 81.35525864374864
At time: 135.66691255569458 and batch: 1000, loss is 4.326865644454956 and perplexity is 75.70662282936615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.906062800709794 and perplexity of 135.1064249337359
Finished 12 epochs...
Completing Train Step...
At time: 137.2252917289734 and batch: 50, loss is 4.555347328186035 and perplexity is 95.13979422970846
At time: 137.75725555419922 and batch: 100, loss is 4.471302795410156 and perplexity is 87.47060511766308
At time: 138.27475428581238 and batch: 150, loss is 4.506982297897339 and perplexity is 90.64785712246395
At time: 138.79136896133423 and batch: 200, loss is 4.53942530632019 and perplexity is 93.63697207758821
At time: 139.31549787521362 and batch: 250, loss is 4.547778167724609 and perplexity is 94.42238438120857
At time: 139.8391580581665 and batch: 300, loss is 4.4432953929901124 and perplexity is 85.05476924002224
At time: 140.36819005012512 and batch: 350, loss is 4.460997381210327 and perplexity is 86.5738131446938
At time: 140.91737508773804 and batch: 400, loss is 4.461554107666015 and perplexity is 86.62202449586333
At time: 141.44198274612427 and batch: 450, loss is 4.517509565353394 and perplexity is 91.60717198123184
At time: 141.96214604377747 and batch: 500, loss is 4.540217123031616 and perplexity is 93.7111447585954
At time: 142.48860955238342 and batch: 550, loss is 4.47548189163208 and perplexity is 87.83691808848283
At time: 143.02031445503235 and batch: 600, loss is 4.416648511886597 and perplexity is 82.81825531314016
At time: 143.53686237335205 and batch: 650, loss is 4.385528917312622 and perplexity is 80.28067382373426
At time: 144.05400109291077 and batch: 700, loss is 4.477014360427856 and perplexity is 87.9716286180485
At time: 144.57042264938354 and batch: 750, loss is 4.414682168960571 and perplexity is 82.65556622634669
At time: 145.10278868675232 and batch: 800, loss is 4.494176006317138 and perplexity is 89.49439577821852
At time: 145.63119077682495 and batch: 850, loss is 4.408362646102905 and perplexity is 82.1348694966023
At time: 146.14920258522034 and batch: 900, loss is 4.4073828125 and perplexity is 82.05443040639673
At time: 146.68769907951355 and batch: 950, loss is 4.401234912872314 and perplexity is 81.55151552340357
At time: 147.211852312088 and batch: 1000, loss is 4.327855100631714 and perplexity is 75.78156828645446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.904416712318978 and perplexity of 134.88421075839113
Finished 13 epochs...
Completing Train Step...
At time: 148.77665734291077 and batch: 50, loss is 4.539961500167847 and perplexity is 93.68719310883016
At time: 149.2995946407318 and batch: 100, loss is 4.456610689163208 and perplexity is 86.19487224326771
At time: 149.81446957588196 and batch: 150, loss is 4.491807641983033 and perplexity is 89.28269123885211
At time: 150.33146238327026 and batch: 200, loss is 4.524961729049682 and perplexity is 92.2923936434564
At time: 150.85813426971436 and batch: 250, loss is 4.5363193798065184 and perplexity is 93.34659370373231
At time: 151.38464045524597 and batch: 300, loss is 4.432441101074219 and perplexity is 84.13655225844226
At time: 151.9010238647461 and batch: 350, loss is 4.449102325439453 and perplexity is 85.5501133624871
At time: 152.4134395122528 and batch: 400, loss is 4.451262512207031 and perplexity is 85.73511733496223
At time: 152.9264624118805 and batch: 450, loss is 4.507153053283691 and perplexity is 90.66333705393214
At time: 153.45676350593567 and batch: 500, loss is 4.531068677902222 and perplexity is 92.85774309395484
At time: 154.00451970100403 and batch: 550, loss is 4.46684380531311 and perplexity is 87.08144283583728
At time: 154.5233371257782 and batch: 600, loss is 4.4107576179504395 and perplexity is 82.33181594320656
At time: 155.0480465888977 and batch: 650, loss is 4.379679183959961 and perplexity is 79.81242419150591
At time: 155.56368780136108 and batch: 700, loss is 4.471753349304199 and perplexity is 87.51002421896114
At time: 156.08689975738525 and batch: 750, loss is 4.410413122177124 and perplexity is 82.30345786550411
At time: 156.60661602020264 and batch: 800, loss is 4.490584259033203 and perplexity is 89.17353110261503
At time: 157.11724495887756 and batch: 850, loss is 4.406286993026733 and perplexity is 81.96456281201249
At time: 157.6305706501007 and batch: 900, loss is 4.406353902816773 and perplexity is 81.97004722717938
At time: 158.14197063446045 and batch: 950, loss is 4.399438953399658 and perplexity is 81.40518374889051
At time: 158.65319561958313 and batch: 1000, loss is 4.3239560890197755 and perplexity is 75.48667035056644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.904376146270008 and perplexity of 134.87873914987367
Finished 14 epochs...
Completing Train Step...
At time: 160.2271625995636 and batch: 50, loss is 4.528421535491943 and perplexity is 92.61226048115452
At time: 160.76721334457397 and batch: 100, loss is 4.446395177841186 and perplexity is 85.31882977921451
At time: 161.284832239151 and batch: 150, loss is 4.481549921035767 and perplexity is 88.3715354867922
At time: 161.8080551624298 and batch: 200, loss is 4.514968299865723 and perplexity is 91.37466938718755
At time: 162.32761573791504 and batch: 250, loss is 4.5270837306976315 and perplexity is 92.4884461932109
At time: 162.84576106071472 and batch: 300, loss is 4.423323373794556 and perplexity is 83.37290477578937
At time: 163.3636016845703 and batch: 350, loss is 4.439489622116088 and perplexity is 84.73168545793472
At time: 163.88076758384705 and batch: 400, loss is 4.44295030593872 and perplexity is 85.02542300429256
At time: 164.40647411346436 and batch: 450, loss is 4.49961088180542 and perplexity is 89.98211081090564
At time: 164.91793584823608 and batch: 500, loss is 4.523313970565796 and perplexity is 92.14044329195275
At time: 165.43425297737122 and batch: 550, loss is 4.460381631851196 and perplexity is 86.52052178347695
At time: 165.9471161365509 and batch: 600, loss is 4.404390020370483 and perplexity is 81.80922565932397
At time: 166.46766686439514 and batch: 650, loss is 4.3747053050994875 and perplexity is 79.41643248578872
At time: 166.9979829788208 and batch: 700, loss is 4.4674506092071535 and perplexity is 87.13430022986334
At time: 167.52760577201843 and batch: 750, loss is 4.404578723907471 and perplexity is 81.8246648062291
At time: 168.05043292045593 and batch: 800, loss is 4.486878843307495 and perplexity is 88.84371752372587
At time: 168.5656235218048 and batch: 850, loss is 4.402493982315064 and perplexity is 81.65425921175529
At time: 169.0818293094635 and batch: 900, loss is 4.404399452209472 and perplexity is 81.80999727440708
At time: 169.61494398117065 and batch: 950, loss is 4.396378583908081 and perplexity is 81.15643463433562
At time: 170.13716769218445 and batch: 1000, loss is 4.31926007270813 and perplexity is 75.13301475143713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.903901262981136 and perplexity of 134.81470269675143
Finished 15 epochs...
Completing Train Step...
At time: 171.64689540863037 and batch: 50, loss is 4.518703775405884 and perplexity is 91.71663553512106
At time: 172.1701967716217 and batch: 100, loss is 4.438220367431641 and perplexity is 84.62420759206744
At time: 172.69652223587036 and batch: 150, loss is 4.473496522903442 and perplexity is 87.66270241648894
At time: 173.23411560058594 and batch: 200, loss is 4.507144680023194 and perplexity is 90.66257790937168
At time: 173.74992322921753 and batch: 250, loss is 4.51939884185791 and perplexity is 91.7804068516582
At time: 174.272057056427 and batch: 300, loss is 4.415525703430176 and perplexity is 82.7253184606343
At time: 174.79525113105774 and batch: 350, loss is 4.432074041366577 and perplexity is 84.10567478745268
At time: 175.3330957889557 and batch: 400, loss is 4.435520668029785 and perplexity is 84.396055778677
At time: 175.85278964042664 and batch: 450, loss is 4.492672557830811 and perplexity is 89.3599466583391
At time: 176.36978936195374 and batch: 500, loss is 4.516893348693848 and perplexity is 91.55073950482486
At time: 176.89433526992798 and batch: 550, loss is 4.454232969284058 and perplexity is 85.99016844264408
At time: 177.42356300354004 and batch: 600, loss is 4.398800430297851 and perplexity is 81.35322124985353
At time: 177.95104789733887 and batch: 650, loss is 4.369358987808227 and perplexity is 78.99298000387466
At time: 178.46338844299316 and batch: 700, loss is 4.462513399124146 and perplexity is 86.70516013332623
At time: 178.98920392990112 and batch: 750, loss is 4.399992599487304 and perplexity is 81.45026588901092
At time: 179.51613187789917 and batch: 800, loss is 4.483234510421753 and perplexity is 88.52053070013865
At time: 180.0363998413086 and batch: 850, loss is 4.3984753513336186 and perplexity is 81.32677932704195
At time: 180.58121371269226 and batch: 900, loss is 4.401187295913696 and perplexity is 81.54763238071612
At time: 181.10504722595215 and batch: 950, loss is 4.391984548568725 and perplexity is 80.80061271209051
At time: 181.6153404712677 and batch: 1000, loss is 4.314165420532227 and perplexity is 74.75121157736072
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.904183736661586 and perplexity of 134.85278968103302
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 183.1629056930542 and batch: 50, loss is 4.507693710327149 and perplexity is 90.7123680789839
At time: 183.69277811050415 and batch: 100, loss is 4.422679796218872 and perplexity is 83.31926510635165
At time: 184.22064089775085 and batch: 150, loss is 4.45166446685791 and perplexity is 85.76958589105276
At time: 184.75765991210938 and batch: 200, loss is 4.4850419235229495 and perplexity is 88.68066854107269
At time: 185.28043007850647 and batch: 250, loss is 4.4960210990905765 and perplexity is 89.65967367076581
At time: 185.80363011360168 and batch: 300, loss is 4.387045354843139 and perplexity is 80.40250680319996
At time: 186.3274028301239 and batch: 350, loss is 4.40114013671875 and perplexity is 81.54378675070238
At time: 186.84830737113953 and batch: 400, loss is 4.403695087432862 and perplexity is 81.75239348337466
At time: 187.37182903289795 and batch: 450, loss is 4.4595875072479245 and perplexity is 86.45184098260704
At time: 187.88984608650208 and batch: 500, loss is 4.479321727752685 and perplexity is 88.17484583766024
At time: 188.40635299682617 and batch: 550, loss is 4.417042260169983 and perplexity is 82.8508712798218
At time: 188.92519974708557 and batch: 600, loss is 4.359400815963745 and perplexity is 78.21025804260681
At time: 189.44957399368286 and batch: 650, loss is 4.324563503265381 and perplexity is 75.5325359577921
At time: 189.9739122390747 and batch: 700, loss is 4.41645809173584 and perplexity is 82.8024865498708
At time: 190.5023627281189 and batch: 750, loss is 4.35377537727356 and perplexity is 77.77152621752849
At time: 191.03081965446472 and batch: 800, loss is 4.433227596282959 and perplexity is 84.20275128286873
At time: 191.5450463294983 and batch: 850, loss is 4.350591058731079 and perplexity is 77.52427078347712
At time: 192.06837058067322 and batch: 900, loss is 4.348105993270874 and perplexity is 77.33185707517748
At time: 192.601646900177 and batch: 950, loss is 4.3322493267059325 and perplexity is 76.11530234420123
At time: 193.11418342590332 and batch: 1000, loss is 4.255179195404053 and perplexity is 70.46944385754367
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.883522591939786 and perplexity of 132.09516264093162
Finished 17 epochs...
Completing Train Step...
At time: 194.62465476989746 and batch: 50, loss is 4.488757772445679 and perplexity is 89.01080549740003
At time: 195.1476936340332 and batch: 100, loss is 4.407468214035034 and perplexity is 82.0614382799471
At time: 195.6619212627411 and batch: 150, loss is 4.438781642913819 and perplexity is 84.67171841707092
At time: 196.17731142044067 and batch: 200, loss is 4.475122108459472 and perplexity is 87.8053215277171
At time: 196.69677329063416 and batch: 250, loss is 4.489005928039551 and perplexity is 89.03289676762205
At time: 197.2105541229248 and batch: 300, loss is 4.38015061378479 and perplexity is 79.8500590190547
At time: 197.7348051071167 and batch: 350, loss is 4.3931963253021244 and perplexity is 80.89858436251417
At time: 198.2510871887207 and batch: 400, loss is 4.396325168609619 and perplexity is 81.15209975493295
At time: 198.7715322971344 and batch: 450, loss is 4.4525800228118895 and perplexity is 85.84814870492379
At time: 199.29816269874573 and batch: 500, loss is 4.474421844482422 and perplexity is 87.74385614756545
At time: 199.82816791534424 and batch: 550, loss is 4.412401099205017 and perplexity is 82.46723799068117
At time: 200.34601020812988 and batch: 600, loss is 4.356075010299683 and perplexity is 77.950577985483
At time: 200.8605260848999 and batch: 650, loss is 4.321266622543335 and perplexity is 75.28392424287946
At time: 201.3809540271759 and batch: 700, loss is 4.413210067749024 and perplexity is 82.5339783839264
At time: 201.9035828113556 and batch: 750, loss is 4.3531398010253906 and perplexity is 77.7221121875361
At time: 202.41875648498535 and batch: 800, loss is 4.43452094078064 and perplexity is 84.31172490295566
At time: 202.93506383895874 and batch: 850, loss is 4.351767635345459 and perplexity is 77.61553770826382
At time: 203.44824171066284 and batch: 900, loss is 4.349437265396118 and perplexity is 77.43487537837797
At time: 203.96197986602783 and batch: 950, loss is 4.334272193908691 and perplexity is 76.2694293295961
At time: 204.47322607040405 and batch: 1000, loss is 4.25793791770935 and perplexity is 70.66411788648365
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.882027974942836 and perplexity of 131.89787843452336
Finished 18 epochs...
Completing Train Step...
At time: 206.04412364959717 and batch: 50, loss is 4.4833703708648684 and perplexity is 88.53255795566
At time: 206.55993700027466 and batch: 100, loss is 4.401802845001221 and perplexity is 81.59784440381492
At time: 207.08128929138184 and batch: 150, loss is 4.433747310638427 and perplexity is 84.24652403515901
At time: 207.63110995292664 and batch: 200, loss is 4.471132860183716 and perplexity is 87.45574204349128
At time: 208.1483747959137 and batch: 250, loss is 4.4859567070007325 and perplexity is 88.76182926803905
At time: 208.66723704338074 and batch: 300, loss is 4.377368659973144 and perplexity is 79.62822854713707
At time: 209.2008318901062 and batch: 350, loss is 4.388664493560791 and perplexity is 80.53279506388117
At time: 209.7259817123413 and batch: 400, loss is 4.392578048706055 and perplexity is 80.84858212034854
At time: 210.25668025016785 and batch: 450, loss is 4.448619041442871 and perplexity is 85.50877835087257
At time: 210.78390789031982 and batch: 500, loss is 4.471378326416016 and perplexity is 87.47721210996401
At time: 211.2996325492859 and batch: 550, loss is 4.410072817802429 and perplexity is 82.27545440386048
At time: 211.8232171535492 and batch: 600, loss is 4.354052581787109 and perplexity is 77.79308782397088
At time: 212.34254479408264 and batch: 650, loss is 4.318934497833252 and perplexity is 75.10855731114026
At time: 212.8687572479248 and batch: 700, loss is 4.410774717330932 and perplexity is 82.33322377829056
At time: 213.38257932662964 and batch: 750, loss is 4.352360572814941 and perplexity is 77.66157251531737
At time: 213.8932638168335 and batch: 800, loss is 4.435285129547119 and perplexity is 84.37617960065023
At time: 214.41253781318665 and batch: 850, loss is 4.352256240844727 and perplexity is 77.65347035311153
At time: 214.93960666656494 and batch: 900, loss is 4.349910135269165 and perplexity is 77.47150065688045
At time: 215.46063804626465 and batch: 950, loss is 4.334912128448487 and perplexity is 76.31825239187037
At time: 215.98134446144104 and batch: 1000, loss is 4.258683786392212 and perplexity is 70.71684369983934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.881778251834032 and perplexity of 131.86494459861711
Finished 19 epochs...
Completing Train Step...
At time: 217.60728311538696 and batch: 50, loss is 4.479704580307007 and perplexity is 88.20861026560242
At time: 218.137939453125 and batch: 100, loss is 4.397721910476685 and perplexity is 81.26552748645135
At time: 218.66754698753357 and batch: 150, loss is 4.4298991680145265 and perplexity is 83.92295436551294
At time: 219.1906123161316 and batch: 200, loss is 4.468079986572266 and perplexity is 87.1891578474135
At time: 219.7146360874176 and batch: 250, loss is 4.484148654937744 and perplexity is 88.60148825564575
At time: 220.25183486938477 and batch: 300, loss is 4.375248718261719 and perplexity is 79.45960014837466
At time: 220.79544830322266 and batch: 350, loss is 4.385160474777222 and perplexity is 80.25110045710464
At time: 221.31592679023743 and batch: 400, loss is 4.389861268997192 and perplexity is 80.62923243026361
At time: 221.83042645454407 and batch: 450, loss is 4.445815086364746 and perplexity is 85.26935140565861
At time: 222.3511836528778 and batch: 500, loss is 4.469081449508667 and perplexity is 87.27651829427714
At time: 222.87489247322083 and batch: 550, loss is 4.408101921081543 and perplexity is 82.1134576724187
At time: 223.40088033676147 and batch: 600, loss is 4.352539710998535 and perplexity is 77.6754859145261
At time: 223.92852306365967 and batch: 650, loss is 4.316947412490845 and perplexity is 74.95945838302515
At time: 224.45069575309753 and batch: 700, loss is 4.4086721229553225 and perplexity is 82.16029227116508
At time: 224.9732325077057 and batch: 750, loss is 4.351404581069946 and perplexity is 77.58736417002387
At time: 225.49337220191956 and batch: 800, loss is 4.435509796142578 and perplexity is 84.3951382392656
At time: 226.0083725452423 and batch: 850, loss is 4.3522083568573 and perplexity is 77.64975208433698
At time: 226.52131962776184 and batch: 900, loss is 4.349499683380127 and perplexity is 77.43970885803753
At time: 227.03764176368713 and batch: 950, loss is 4.334511547088623 and perplexity is 76.28768684494774
At time: 227.55513286590576 and batch: 1000, loss is 4.258252382278442 and perplexity is 70.68634274214678
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.881597007193217 and perplexity of 131.8410469498231
Finished 20 epochs...
Completing Train Step...
At time: 229.08603501319885 and batch: 50, loss is 4.47676176071167 and perplexity is 87.94940981597688
At time: 229.61956238746643 and batch: 100, loss is 4.394227075576782 and perplexity is 80.98201359052851
At time: 230.14360928535461 and batch: 150, loss is 4.426727390289306 and perplexity is 83.6571911020234
At time: 230.67893242835999 and batch: 200, loss is 4.465541896820068 and perplexity is 86.96814453383679
At time: 231.19891214370728 and batch: 250, loss is 4.48247727394104 and perplexity is 88.45352509774546
At time: 231.715074300766 and batch: 300, loss is 4.373558568954468 and perplexity is 79.32541498864965
At time: 232.23467779159546 and batch: 350, loss is 4.382211589813233 and perplexity is 80.0147977795686
At time: 232.76507019996643 and batch: 400, loss is 4.3873114585876465 and perplexity is 80.42390505827969
At time: 233.30579352378845 and batch: 450, loss is 4.443285760879516 and perplexity is 85.05394998702376
At time: 233.82742071151733 and batch: 500, loss is 4.4669156169891355 and perplexity is 87.08769652473927
At time: 234.3724844455719 and batch: 550, loss is 4.406167697906494 and perplexity is 81.95478542284549
At time: 234.89019584655762 and batch: 600, loss is 4.351011581420899 and perplexity is 77.5568783539844
At time: 235.40957522392273 and batch: 650, loss is 4.31519645690918 and perplexity is 74.82832254099162
At time: 235.94636845588684 and batch: 700, loss is 4.4068285083770755 and perplexity is 82.00895990072688
At time: 236.46295166015625 and batch: 750, loss is 4.350558967590332 and perplexity is 77.52178298111049
At time: 236.97975730895996 and batch: 800, loss is 4.435426349639893 and perplexity is 84.38809605396283
At time: 237.50230073928833 and batch: 850, loss is 4.352102422714234 and perplexity is 77.64152676006962
At time: 238.04011130332947 and batch: 900, loss is 4.348764171600342 and perplexity is 77.38277198138783
At time: 238.55825090408325 and batch: 950, loss is 4.334043226242065 and perplexity is 76.25196809543291
At time: 239.08862829208374 and batch: 1000, loss is 4.2576214838027955 and perplexity is 70.64176090104824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.881490195669779 and perplexity of 131.8269655587876
Finished 21 epochs...
Completing Train Step...
At time: 240.62014985084534 and batch: 50, loss is 4.474219799041748 and perplexity is 87.7261296923187
At time: 241.15011358261108 and batch: 100, loss is 4.391336317062378 and perplexity is 80.74825218190455
At time: 241.68454909324646 and batch: 150, loss is 4.423673810958863 and perplexity is 83.40212686007213
At time: 242.20319175720215 and batch: 200, loss is 4.463558654785157 and perplexity is 86.79583657457043
At time: 242.71452736854553 and batch: 250, loss is 4.480973682403564 and perplexity is 88.32062706316654
At time: 243.23334908485413 and batch: 300, loss is 4.371809568405151 and perplexity is 79.18679605189433
At time: 243.7592694759369 and batch: 350, loss is 4.379570713043213 and perplexity is 79.80376733420307
At time: 244.29034399986267 and batch: 400, loss is 4.385166349411011 and perplexity is 80.2515719043158
At time: 244.8082058429718 and batch: 450, loss is 4.44116886138916 and perplexity is 84.87408976381764
At time: 245.32248091697693 and batch: 500, loss is 4.465041189193726 and perplexity is 86.92460982061088
At time: 245.84614944458008 and batch: 550, loss is 4.404573564529419 and perplexity is 81.82424264293844
At time: 246.36564135551453 and batch: 600, loss is 4.349883432388306 and perplexity is 77.46943197224846
At time: 246.8838438987732 and batch: 650, loss is 4.313421258926391 and perplexity is 74.69560528831397
At time: 247.43004727363586 and batch: 700, loss is 4.4051628589630125 and perplexity is 81.87247542391981
At time: 247.94405841827393 and batch: 750, loss is 4.349543447494507 and perplexity is 77.44309801247464
At time: 248.46454691886902 and batch: 800, loss is 4.435378322601318 and perplexity is 84.3840432409415
At time: 248.9773256778717 and batch: 850, loss is 4.352010498046875 and perplexity is 77.63438991657998
At time: 249.5082609653473 and batch: 900, loss is 4.348161010742188 and perplexity is 77.33611179544667
At time: 250.0192060470581 and batch: 950, loss is 4.332931079864502 and perplexity is 76.16721188472589
At time: 250.5466890335083 and batch: 1000, loss is 4.256752462387085 and perplexity is 70.58039836452264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8816919094178735 and perplexity of 131.8535595522072
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 252.08851432800293 and batch: 50, loss is 4.4706409740448 and perplexity is 87.41273435452165
At time: 252.60909485816956 and batch: 100, loss is 4.38701174736023 and perplexity is 80.39980472273201
At time: 253.12253761291504 and batch: 150, loss is 4.419937696456909 and perplexity is 83.09110832662363
At time: 253.6450390815735 and batch: 200, loss is 4.458636093139648 and perplexity is 86.36962859662316
At time: 254.16834211349487 and batch: 250, loss is 4.475501279830933 and perplexity is 87.83862110462648
At time: 254.6855571269989 and batch: 300, loss is 4.362954626083374 and perplexity is 78.48869691549267
At time: 255.2008605003357 and batch: 350, loss is 4.369864025115967 and perplexity is 79.03288448160296
At time: 255.7238118648529 and batch: 400, loss is 4.376403408050537 and perplexity is 79.55140432976474
At time: 256.24421763420105 and batch: 450, loss is 4.43313009262085 and perplexity is 84.19454160650228
At time: 256.76148676872253 and batch: 500, loss is 4.4546991634368895 and perplexity is 86.03026590224745
At time: 257.28468227386475 and batch: 550, loss is 4.3936790132522585 and perplexity is 80.93764256007118
At time: 257.8000657558441 and batch: 600, loss is 4.336807146072387 and perplexity is 76.46301394470422
At time: 258.31474804878235 and batch: 650, loss is 4.298137159347534 and perplexity is 73.56263052404856
At time: 258.84090995788574 and batch: 700, loss is 4.389668035507202 and perplexity is 80.6136536675037
At time: 259.3565261363983 and batch: 750, loss is 4.335222325325012 and perplexity is 76.34192974751525
At time: 259.87195348739624 and batch: 800, loss is 4.418479642868042 and perplexity is 82.97004531755
At time: 260.4178876876831 and batch: 850, loss is 4.337467279434204 and perplexity is 76.51350639519609
At time: 260.94961428642273 and batch: 900, loss is 4.327039756774902 and perplexity is 75.71980543269582
At time: 261.48406195640564 and batch: 950, loss is 4.313630647659302 and perplexity is 74.71124734403725
At time: 262.0071985721588 and batch: 1000, loss is 4.236299772262573 and perplexity is 69.15150154989973
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8748414574599845 and perplexity of 130.95338988187115
Finished 23 epochs...
Completing Train Step...
At time: 263.6463189125061 and batch: 50, loss is 4.464822406768799 and perplexity is 86.90559432389259
At time: 264.208340883255 and batch: 100, loss is 4.383591051101685 and perplexity is 80.12525126124461
At time: 264.7405483722687 and batch: 150, loss is 4.416405782699585 and perplexity is 82.79815534488147
At time: 265.2598762512207 and batch: 200, loss is 4.455275545120239 and perplexity is 86.07986646478189
At time: 265.77388858795166 and batch: 250, loss is 4.472487230300903 and perplexity is 87.5742697341544
At time: 266.2902171611786 and batch: 300, loss is 4.359479284286499 and perplexity is 78.21639531116494
At time: 266.8072898387909 and batch: 350, loss is 4.366990842819214 and perplexity is 78.80613450005389
At time: 267.3366582393646 and batch: 400, loss is 4.374839086532592 and perplexity is 79.4270576406471
At time: 267.86096024513245 and batch: 450, loss is 4.432039070129394 and perplexity is 84.10273355938078
At time: 268.39644503593445 and batch: 500, loss is 4.453221940994263 and perplexity is 85.90327388353492
At time: 268.927729845047 and batch: 550, loss is 4.391908030509949 and perplexity is 80.79443024259615
At time: 269.43936038017273 and batch: 600, loss is 4.33540102481842 and perplexity is 76.35557323069239
At time: 269.95345640182495 and batch: 650, loss is 4.297311482429504 and perplexity is 73.50191662647647
At time: 270.4727826118469 and batch: 700, loss is 4.38942587852478 and perplexity is 80.5941348717913
At time: 270.99217081069946 and batch: 750, loss is 4.335686950683594 and perplexity is 76.37740838549821
At time: 271.5117220878601 and batch: 800, loss is 4.418475122451782 and perplexity is 82.96967025925578
At time: 272.02945137023926 and batch: 850, loss is 4.337825727462769 and perplexity is 76.54093742672791
At time: 272.55472135543823 and batch: 900, loss is 4.327517051696777 and perplexity is 75.75595473756773
At time: 273.0760757923126 and batch: 950, loss is 4.314751129150391 and perplexity is 74.79500683057641
At time: 273.5920603275299 and batch: 1000, loss is 4.237521467208862 and perplexity is 69.23603521653406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.874446589772294 and perplexity of 130.901690827428
Finished 24 epochs...
Completing Train Step...
At time: 275.14648056030273 and batch: 50, loss is 4.4623627090454105 and perplexity is 86.69209551029826
At time: 275.6813426017761 and batch: 100, loss is 4.381959800720215 and perplexity is 79.99465346237376
At time: 276.1987051963806 and batch: 150, loss is 4.414977617263794 and perplexity is 82.67999028098532
At time: 276.71878600120544 and batch: 200, loss is 4.453729524612426 and perplexity is 85.94688804608562
At time: 277.24127101898193 and batch: 250, loss is 4.470724964141846 and perplexity is 87.4200764668912
At time: 277.76019978523254 and batch: 300, loss is 4.35756552696228 and perplexity is 78.0668512529243
At time: 278.2765054702759 and batch: 350, loss is 4.365393838882446 and perplexity is 78.68038123397335
At time: 278.7914459705353 and batch: 400, loss is 4.374106502532959 and perplexity is 79.36889195730741
At time: 279.3063781261444 and batch: 450, loss is 4.431605491638184 and perplexity is 84.06627632716442
At time: 279.82009100914 and batch: 500, loss is 4.452521753311157 and perplexity is 85.84314652189875
At time: 280.33552503585815 and batch: 550, loss is 4.39085765838623 and perplexity is 80.7096105792209
At time: 280.85056948661804 and batch: 600, loss is 4.3345523452758785 and perplexity is 76.2907993077719
At time: 281.37780928611755 and batch: 650, loss is 4.296733598709107 and perplexity is 73.4594533360439
At time: 281.8906354904175 and batch: 700, loss is 4.38939884185791 and perplexity is 80.59195590447125
At time: 282.41323733329773 and batch: 750, loss is 4.336053457260132 and perplexity is 76.40540633837392
At time: 282.9292883872986 and batch: 800, loss is 4.4184023380279545 and perplexity is 82.96363157937438
At time: 283.44567251205444 and batch: 850, loss is 4.338009157180786 and perplexity is 76.55497859704147
At time: 283.9753842353821 and batch: 900, loss is 4.327717924118042 and perplexity is 75.77117354808962
At time: 284.4902501106262 and batch: 950, loss is 4.315338611602783 and perplexity is 74.83896049435485
At time: 285.01218938827515 and batch: 1000, loss is 4.238024697303772 and perplexity is 69.2708856412631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8742430151962655 and perplexity of 130.8750452834861
Finished 25 epochs...
Completing Train Step...
At time: 286.5555398464203 and batch: 50, loss is 4.460489883422851 and perplexity is 86.52988827289988
At time: 287.0755865573883 and batch: 100, loss is 4.3807202816009525 and perplexity is 79.89555998678452
At time: 287.61079621315 and batch: 150, loss is 4.413992977142334 and perplexity is 82.5986203119549
At time: 288.1271114349365 and batch: 200, loss is 4.452592067718506 and perplexity is 85.84918274408555
At time: 288.6588068008423 and batch: 250, loss is 4.469367771148682 and perplexity is 87.30151102793947
At time: 289.1937756538391 and batch: 300, loss is 4.356126613616944 and perplexity is 77.95460059767863
At time: 289.7118923664093 and batch: 350, loss is 4.364076108932495 and perplexity is 78.57677001994034
At time: 290.2258050441742 and batch: 400, loss is 4.37340051651001 and perplexity is 79.31287840364814
At time: 290.7407853603363 and batch: 450, loss is 4.431152868270874 and perplexity is 84.02823457603822
At time: 291.25297594070435 and batch: 500, loss is 4.4519632625579835 and perplexity is 85.79521730360133
At time: 291.776823759079 and batch: 550, loss is 4.3900535202026365 and perplexity is 80.64473498753729
At time: 292.2894058227539 and batch: 600, loss is 4.3338747549057 and perplexity is 76.23912290652086
At time: 292.8076093196869 and batch: 650, loss is 4.2960629510879516 and perplexity is 73.41020444458454
At time: 293.32413816452026 and batch: 700, loss is 4.388980808258057 and perplexity is 80.55827279985054
At time: 293.85257506370544 and batch: 750, loss is 4.335961360931396 and perplexity is 76.39837000496982
At time: 294.3665030002594 and batch: 800, loss is 4.418244228363037 and perplexity is 82.95051526432047
At time: 294.8911626338959 and batch: 850, loss is 4.337866601943969 and perplexity is 76.54406606177606
At time: 295.4228129386902 and batch: 900, loss is 4.327504281997681 and perplexity is 75.75498736299753
At time: 295.9386420249939 and batch: 950, loss is 4.315651769638062 and perplexity is 74.86240058622055
At time: 296.455712556839 and batch: 1000, loss is 4.238063311576843 and perplexity is 69.27356053780137
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.873927418778583 and perplexity of 130.8337481049941
Finished 26 epochs...
Completing Train Step...
At time: 298.00268745422363 and batch: 50, loss is 4.458804664611816 and perplexity is 86.38418927928964
At time: 298.5530798435211 and batch: 100, loss is 4.379501008987427 and perplexity is 79.79820488181788
At time: 299.0802173614502 and batch: 150, loss is 4.41313910484314 and perplexity is 82.52812174079078
At time: 299.6042037010193 and batch: 200, loss is 4.4515821075439455 and perplexity is 85.76252225768175
At time: 300.11896562576294 and batch: 250, loss is 4.468256216049195 and perplexity is 87.2045245010837
At time: 300.64959239959717 and batch: 300, loss is 4.354982805252075 and perplexity is 77.86548644790567
At time: 301.1765546798706 and batch: 350, loss is 4.363081493377686 and perplexity is 78.49865519578107
At time: 301.70179414749146 and batch: 400, loss is 4.372922801971436 and perplexity is 79.2749985371404
At time: 302.23351192474365 and batch: 450, loss is 4.430921678543091 and perplexity is 84.00881035678698
At time: 302.7486038208008 and batch: 500, loss is 4.451547470092773 and perplexity is 85.75955171395098
At time: 303.2643747329712 and batch: 550, loss is 4.389376173019409 and perplexity is 80.59012899914532
At time: 303.7995367050171 and batch: 600, loss is 4.333272047042847 and perplexity is 76.19318683209524
At time: 304.3196699619293 and batch: 650, loss is 4.295567893981934 and perplexity is 73.37387119547867
At time: 304.840886592865 and batch: 700, loss is 4.388783731460571 and perplexity is 80.54239819774541
At time: 305.36768865585327 and batch: 750, loss is 4.336002798080444 and perplexity is 76.40153580120507
At time: 305.8887631893158 and batch: 800, loss is 4.418040933609009 and perplexity is 82.933653573728
At time: 306.4137291908264 and batch: 850, loss is 4.337991676330566 and perplexity is 76.55364036262381
At time: 306.94127559661865 and batch: 900, loss is 4.327379121780395 and perplexity is 75.74550644564793
At time: 307.46045088768005 and batch: 950, loss is 4.315861988067627 and perplexity is 74.87813969677285
At time: 307.97565937042236 and batch: 1000, loss is 4.238009405136109 and perplexity is 69.26982634736514
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.873860428972942 and perplexity of 130.8249838711978
Finished 27 epochs...
Completing Train Step...
At time: 309.50103974342346 and batch: 50, loss is 4.457452745437622 and perplexity is 86.26748374345665
At time: 310.03433895111084 and batch: 100, loss is 4.378539342880249 and perplexity is 79.72150253970193
At time: 310.5538556575775 and batch: 150, loss is 4.412449979782105 and perplexity is 82.47126913538656
At time: 311.06845331192017 and batch: 200, loss is 4.450767707824707 and perplexity is 85.69270571677839
At time: 311.58347272872925 and batch: 250, loss is 4.467364253997803 and perplexity is 87.12677605400596
At time: 312.0990459918976 and batch: 300, loss is 4.353891038894654 and perplexity is 77.78052191854442
At time: 312.6104156970978 and batch: 350, loss is 4.362202835083008 and perplexity is 78.42971199446569
At time: 313.13920545578003 and batch: 400, loss is 4.372496891021728 and perplexity is 79.2412416364525
At time: 313.668265581131 and batch: 450, loss is 4.4305667972564695 and perplexity is 83.97900249151968
At time: 314.2187783718109 and batch: 500, loss is 4.4511476993560795 and perplexity is 85.72527440677244
At time: 314.73839020729065 and batch: 550, loss is 4.388790497779846 and perplexity is 80.5429431751705
At time: 315.2551155090332 and batch: 600, loss is 4.332729606628418 and perplexity is 76.15186777582953
At time: 315.77142214775085 and batch: 650, loss is 4.2951582384109495 and perplexity is 73.34381933625478
At time: 316.3004071712494 and batch: 700, loss is 4.388512678146363 and perplexity is 80.52056987223322
At time: 316.8276295661926 and batch: 750, loss is 4.336036248207092 and perplexity is 76.40409148499747
At time: 317.3507037162781 and batch: 800, loss is 4.417935762405396 and perplexity is 82.92493180020932
At time: 317.86996364593506 and batch: 850, loss is 4.337978181838989 and perplexity is 76.55260731713894
At time: 318.38665294647217 and batch: 900, loss is 4.3272115516662595 and perplexity is 75.73281482588536
At time: 318.9187331199646 and batch: 950, loss is 4.315993232727051 and perplexity is 74.88796769763785
At time: 319.43666315078735 and batch: 1000, loss is 4.237828350067138 and perplexity is 69.25728582947468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.873773714391197 and perplexity of 130.81363992928925
Finished 28 epochs...
Completing Train Step...
At time: 320.9896774291992 and batch: 50, loss is 4.456232814788819 and perplexity is 86.16230756290983
At time: 321.5197374820709 and batch: 100, loss is 4.377674655914307 and perplexity is 79.65259819019022
At time: 322.04737615585327 and batch: 150, loss is 4.411865024566651 and perplexity is 82.42304124333437
At time: 322.5704436302185 and batch: 200, loss is 4.450021781921387 and perplexity is 85.6288091418765
At time: 323.0919156074524 and batch: 250, loss is 4.466548728942871 and perplexity is 87.05575095049062
At time: 323.61562180519104 and batch: 300, loss is 4.352910566329956 and perplexity is 77.7042976247562
At time: 324.1339862346649 and batch: 350, loss is 4.361344785690307 and perplexity is 78.3624442913522
At time: 324.64702558517456 and batch: 400, loss is 4.372170400619507 and perplexity is 79.21537435453752
At time: 325.1681740283966 and batch: 450, loss is 4.430320825576782 and perplexity is 83.95834857546171
At time: 325.68372201919556 and batch: 500, loss is 4.450767879486084 and perplexity is 85.69272042690748
At time: 326.21088671684265 and batch: 550, loss is 4.388230428695679 and perplexity is 80.49784619264196
At time: 326.72642850875854 and batch: 600, loss is 4.332155270576477 and perplexity is 76.10814357013896
At time: 327.25754952430725 and batch: 650, loss is 4.2947167825698855 and perplexity is 73.31144842449261
At time: 327.7747800350189 and batch: 700, loss is 4.38823618888855 and perplexity is 80.4983098770972
At time: 328.29441499710083 and batch: 750, loss is 4.336077871322632 and perplexity is 76.40727172751042
At time: 328.80637645721436 and batch: 800, loss is 4.417863206863403 and perplexity is 82.91891535510375
At time: 329.32271456718445 and batch: 850, loss is 4.337828369140625 and perplexity is 76.54113962349446
At time: 329.8601908683777 and batch: 900, loss is 4.326999626159668 and perplexity is 75.71676681128966
At time: 330.3824384212494 and batch: 950, loss is 4.31600401878357 and perplexity is 74.88877544784624
At time: 330.9095537662506 and batch: 1000, loss is 4.237596697807312 and perplexity is 69.24124408082827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.873723844202553 and perplexity of 130.80711639105522
Finished 29 epochs...
Completing Train Step...
At time: 332.424364566803 and batch: 50, loss is 4.45507080078125 and perplexity is 86.0622439035437
At time: 332.9549653530121 and batch: 100, loss is 4.376894578933716 and perplexity is 79.59048726070064
At time: 333.4669086933136 and batch: 150, loss is 4.411328020095826 and perplexity is 82.37879158388355
At time: 333.97834944725037 and batch: 200, loss is 4.449310207366944 and perplexity is 85.56789953359765
At time: 334.4948048591614 and batch: 250, loss is 4.46572961807251 and perplexity is 86.98447183529404
At time: 335.00642943382263 and batch: 300, loss is 4.35211597442627 and perplexity is 77.64257894281175
At time: 335.51803827285767 and batch: 350, loss is 4.360507555007935 and perplexity is 78.29686430526475
At time: 336.0305573940277 and batch: 400, loss is 4.371813898086548 and perplexity is 79.18713890623427
At time: 336.5459659099579 and batch: 450, loss is 4.43004921913147 and perplexity is 83.93554804337674
At time: 337.0601146221161 and batch: 500, loss is 4.450444993972778 and perplexity is 85.66505595535476
At time: 337.57861399650574 and batch: 550, loss is 4.38758526802063 and perplexity is 80.44592889715184
At time: 338.1037051677704 and batch: 600, loss is 4.3316102218627925 and perplexity is 76.06667222736284
At time: 338.6151773929596 and batch: 650, loss is 4.294212779998779 and perplexity is 73.27450857566649
At time: 339.14089012145996 and batch: 700, loss is 4.387927103042602 and perplexity is 80.47343283366052
At time: 339.67140650749207 and batch: 750, loss is 4.336000204086304 and perplexity is 76.40133761632593
At time: 340.1950681209564 and batch: 800, loss is 4.417694358825684 and perplexity is 82.90491584088471
At time: 340.7497351169586 and batch: 850, loss is 4.337710905075073 and perplexity is 76.53214931808157
At time: 341.26483726501465 and batch: 900, loss is 4.32672010421753 and perplexity is 75.6956052712733
At time: 341.78098487854004 and batch: 950, loss is 4.316091117858886 and perplexity is 74.89529847501004
At time: 342.30422472953796 and batch: 1000, loss is 4.237385601997375 and perplexity is 69.22662908696424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.873741708150724 and perplexity of 130.8094531434745
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 343.897958278656 and batch: 50, loss is 4.453952989578247 and perplexity is 85.96609631059238
At time: 344.43845200538635 and batch: 100, loss is 4.376156063079834 and perplexity is 79.5317301232497
At time: 344.9540550708771 and batch: 150, loss is 4.4103617668151855 and perplexity is 82.29923125016728
At time: 345.4706461429596 and batch: 200, loss is 4.4486064529418945 and perplexity is 85.50770193030806
At time: 346.0048952102661 and batch: 250, loss is 4.463553247451782 and perplexity is 86.79536724181546
At time: 346.53043389320374 and batch: 300, loss is 4.349239339828491 and perplexity is 77.41955055335195
At time: 347.05644130706787 and batch: 350, loss is 4.358302822113037 and perplexity is 78.12443078773548
At time: 347.58350443840027 and batch: 400, loss is 4.368184442520142 and perplexity is 78.90025363773759
At time: 348.1063003540039 and batch: 450, loss is 4.427419452667237 and perplexity is 83.71510713507064
At time: 348.64474296569824 and batch: 500, loss is 4.446770505905151 and perplexity is 85.35085834064832
At time: 349.1804049015045 and batch: 550, loss is 4.383913059234619 and perplexity is 80.15105639831314
At time: 349.7248387336731 and batch: 600, loss is 4.327314252853394 and perplexity is 75.74059307528388
At time: 350.2513530254364 and batch: 650, loss is 4.288989968299866 and perplexity is 72.89280725975723
At time: 350.76932740211487 and batch: 700, loss is 4.384055385589599 and perplexity is 80.1624648178583
At time: 351.28559136390686 and batch: 750, loss is 4.331018862724304 and perplexity is 76.02170280345716
At time: 351.8181436061859 and batch: 800, loss is 4.4126057434082036 and perplexity is 82.48411615983963
At time: 352.3406927585602 and batch: 850, loss is 4.332405490875244 and perplexity is 76.12718975533339
At time: 352.86117339134216 and batch: 900, loss is 4.320487146377563 and perplexity is 75.22526508294497
At time: 353.3786573410034 and batch: 950, loss is 4.310261278152466 and perplexity is 74.45994115487314
At time: 353.93720173835754 and batch: 1000, loss is 4.2305516386032105 and perplexity is 68.75514970924593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.872654240305831 and perplexity of 130.66727938807773
Finished 31 epochs...
Completing Train Step...
At time: 355.4664011001587 and batch: 50, loss is 4.453134679794312 and perplexity is 85.89577818782257
At time: 355.97879338264465 and batch: 100, loss is 4.375615100860596 and perplexity is 79.48871809701193
At time: 356.4934732913971 and batch: 150, loss is 4.410098571777343 and perplexity is 82.27757335113482
At time: 357.00546956062317 and batch: 200, loss is 4.447932987213135 and perplexity is 85.45013481043001
At time: 357.5246298313141 and batch: 250, loss is 4.463024158477783 and perplexity is 86.74945691640808
At time: 358.043616771698 and batch: 300, loss is 4.3485518741607665 and perplexity is 77.36634556073415
At time: 358.5561559200287 and batch: 350, loss is 4.357670917510986 and perplexity is 78.07507919477962
At time: 359.0724301338196 and batch: 400, loss is 4.367558059692382 and perplexity is 78.85084734899388
At time: 359.5994346141815 and batch: 450, loss is 4.42695086479187 and perplexity is 83.67588844030216
At time: 360.11967945098877 and batch: 500, loss is 4.446168994903564 and perplexity is 85.29953429789715
At time: 360.63672494888306 and batch: 550, loss is 4.3834853887557985 and perplexity is 80.11678548649631
At time: 361.15838074684143 and batch: 600, loss is 4.326978015899658 and perplexity is 75.71513056995161
At time: 361.67158913612366 and batch: 650, loss is 4.288821449279785 and perplexity is 72.88052447027778
At time: 362.19206619262695 and batch: 700, loss is 4.383955841064453 and perplexity is 80.1544854805197
At time: 362.7194793224335 and batch: 750, loss is 4.331051230430603 and perplexity is 76.02416349142906
At time: 363.2456133365631 and batch: 800, loss is 4.412502212524414 and perplexity is 82.47557694843869
At time: 363.754358291626 and batch: 850, loss is 4.332615795135498 and perplexity is 76.14320131125032
At time: 364.26680994033813 and batch: 900, loss is 4.320611438751221 and perplexity is 75.2346155907875
At time: 364.7885265350342 and batch: 950, loss is 4.310506153106689 and perplexity is 74.47817676218553
At time: 365.3025426864624 and batch: 1000, loss is 4.230845351219177 and perplexity is 68.77534693007227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.872527704006288 and perplexity of 130.6507462801138
Finished 32 epochs...
Completing Train Step...
At time: 366.83835196495056 and batch: 50, loss is 4.452740249633789 and perplexity is 85.86190498298964
At time: 367.3687026500702 and batch: 100, loss is 4.375321960449218 and perplexity is 79.46542015643985
At time: 367.8952763080597 and batch: 150, loss is 4.409901180267334 and perplexity is 82.26133405949301
At time: 368.4180543422699 and batch: 200, loss is 4.447552490234375 and perplexity is 85.41762747716324
At time: 368.9358093738556 and batch: 250, loss is 4.462713871002197 and perplexity is 86.72254382202708
At time: 369.4760355949402 and batch: 300, loss is 4.348124599456787 and perplexity is 77.33329593947306
At time: 370.00163674354553 and batch: 350, loss is 4.357284078598022 and perplexity is 78.04488255700818
At time: 370.52775597572327 and batch: 400, loss is 4.367133836746216 and perplexity is 78.81740410442121
At time: 371.04333448410034 and batch: 450, loss is 4.42664231300354 and perplexity is 83.65007407802241
At time: 371.5704925060272 and batch: 500, loss is 4.445764350891113 and perplexity is 85.26502533447224
At time: 372.084894657135 and batch: 550, loss is 4.383188877105713 and perplexity is 80.0930334477843
At time: 372.6094551086426 and batch: 600, loss is 4.326714968681335 and perplexity is 75.69521653475084
At time: 373.1258223056793 and batch: 650, loss is 4.288683152198791 and perplexity is 72.87044600340955
At time: 373.64512372016907 and batch: 700, loss is 4.383893671035767 and perplexity is 80.1495024287579
At time: 374.16610622406006 and batch: 750, loss is 4.3310667467117305 and perplexity is 76.02534311287393
At time: 374.69866466522217 and batch: 800, loss is 4.412396583557129 and perplexity is 82.46686559851294
At time: 375.23169565200806 and batch: 850, loss is 4.332754583358764 and perplexity is 76.15376982424971
At time: 375.7641713619232 and batch: 900, loss is 4.320687990188599 and perplexity is 75.24037512919924
At time: 376.2753677368164 and batch: 950, loss is 4.310670747756958 and perplexity is 74.49043648055668
At time: 376.7866280078888 and batch: 1000, loss is 4.230969491004944 and perplexity is 68.78388521686583
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.872481183307927 and perplexity of 130.6446684575288
Finished 33 epochs...
Completing Train Step...
At time: 378.301296710968 and batch: 50, loss is 4.452432765960693 and perplexity is 85.83550790760931
At time: 378.8406140804291 and batch: 100, loss is 4.375086898803711 and perplexity is 79.44674307923519
At time: 379.36643385887146 and batch: 150, loss is 4.409775171279907 and perplexity is 82.25096904514
At time: 379.8939530849457 and batch: 200, loss is 4.44726788520813 and perplexity is 85.39332065013997
At time: 380.410612821579 and batch: 250, loss is 4.462454290390014 and perplexity is 86.7000352525322
At time: 380.94462728500366 and batch: 300, loss is 4.34777756690979 and perplexity is 77.30646342496276
At time: 381.46508598327637 and batch: 350, loss is 4.3569880294799805 and perplexity is 78.02178085814703
At time: 381.98474526405334 and batch: 400, loss is 4.366780557632446 and perplexity is 78.7895644796181
At time: 382.50610637664795 and batch: 450, loss is 4.426377182006836 and perplexity is 83.62789879031573
At time: 383.0253596305847 and batch: 500, loss is 4.445433158874511 and perplexity is 85.23679091455142
At time: 383.5531265735626 and batch: 550, loss is 4.382931060791016 and perplexity is 80.07238681870115
At time: 384.06815099716187 and batch: 600, loss is 4.326486434936523 and perplexity is 75.67791959999373
At time: 384.5884873867035 and batch: 650, loss is 4.28854199886322 and perplexity is 72.86016082280247
At time: 385.10766553878784 and batch: 700, loss is 4.383831663131714 and perplexity is 80.14453268018482
At time: 385.6278111934662 and batch: 750, loss is 4.3310590267181395 and perplexity is 76.02475619997783
At time: 386.15329337120056 and batch: 800, loss is 4.412296266555786 and perplexity is 82.45859318478473
At time: 386.6763391494751 and batch: 850, loss is 4.332851161956787 and perplexity is 76.16112500374423
At time: 387.2010507583618 and batch: 900, loss is 4.320736198425293 and perplexity is 75.24400242244442
At time: 387.7257716655731 and batch: 950, loss is 4.3108056640625 and perplexity is 74.50048713302809
At time: 388.24729084968567 and batch: 1000, loss is 4.231068787574768 and perplexity is 68.7907155598362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8724584812071265 and perplexity of 130.6417025827624
Finished 34 epochs...
Completing Train Step...
At time: 389.7885375022888 and batch: 50, loss is 4.452173147201538 and perplexity is 85.81322629204276
At time: 390.300119638443 and batch: 100, loss is 4.374886693954468 and perplexity is 79.43083904809937
At time: 390.8248028755188 and batch: 150, loss is 4.409683303833008 and perplexity is 82.24341320568197
At time: 391.3512372970581 and batch: 200, loss is 4.447035884857177 and perplexity is 85.37351166771442
At time: 391.86714148521423 and batch: 250, loss is 4.462226495742798 and perplexity is 86.68028769786723
At time: 392.38610315322876 and batch: 300, loss is 4.347481632232666 and perplexity is 77.28358914648256
At time: 392.8994822502136 and batch: 350, loss is 4.356733951568604 and perplexity is 78.00195976518226
At time: 393.41273760795593 and batch: 400, loss is 4.366461610794067 and perplexity is 78.7644388042204
At time: 393.94186520576477 and batch: 450, loss is 4.426150512695313 and perplexity is 83.60894506026843
At time: 394.45599031448364 and batch: 500, loss is 4.445138645172119 and perplexity is 85.21169120796435
At time: 394.97008228302 and batch: 550, loss is 4.382694706916809 and perplexity is 80.05346363623141
At time: 395.49303460121155 and batch: 600, loss is 4.326272192001343 and perplexity is 75.66170787705587
At time: 396.016854763031 and batch: 650, loss is 4.28839337348938 and perplexity is 72.8493327588446
At time: 396.5361649990082 and batch: 700, loss is 4.38376238822937 and perplexity is 80.13898086781289
At time: 397.06060433387756 and batch: 750, loss is 4.331038951873779 and perplexity is 76.02323003014845
At time: 397.5738434791565 and batch: 800, loss is 4.412197799682617 and perplexity is 82.45047414468272
At time: 398.10385274887085 and batch: 850, loss is 4.332916593551635 and perplexity is 76.16610851065627
At time: 398.62041187286377 and batch: 900, loss is 4.320773410797119 and perplexity is 75.24680248233834
At time: 399.13967514038086 and batch: 950, loss is 4.310906524658203 and perplexity is 74.50800167549474
At time: 399.65148520469666 and batch: 1000, loss is 4.231149830818176 and perplexity is 68.79629080845663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.872461830697408 and perplexity of 130.6421401666084
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 401.23774337768555 and batch: 50, loss is 4.451981611251831 and perplexity is 85.79679154821987
At time: 401.7796869277954 and batch: 100, loss is 4.375188913345337 and perplexity is 79.45484821572795
At time: 402.29182291030884 and batch: 150, loss is 4.409244060516357 and perplexity is 82.20729626873512
At time: 402.8066589832306 and batch: 200, loss is 4.446690301895142 and perplexity is 85.34401313406183
At time: 403.32685375213623 and batch: 250, loss is 4.4617668151855465 and perplexity is 86.64045161155418
At time: 403.84124875068665 and batch: 300, loss is 4.346418552398681 and perplexity is 77.20147417648073
At time: 404.3647212982178 and batch: 350, loss is 4.356197395324707 and perplexity is 77.96011855271979
At time: 404.89521884918213 and batch: 400, loss is 4.365132188796997 and perplexity is 78.65979719852503
At time: 405.40992045402527 and batch: 450, loss is 4.4252286052703855 and perplexity is 83.53190087228825
At time: 405.9268717765808 and batch: 500, loss is 4.443939113616944 and perplexity is 85.10953837551227
At time: 406.44432258605957 and batch: 550, loss is 4.381556029319763 and perplexity is 79.96236042905701
At time: 406.96836948394775 and batch: 600, loss is 4.325096487998962 and perplexity is 75.57280437661929
At time: 407.51316237449646 and batch: 650, loss is 4.286946730613709 and perplexity is 72.74402198251973
At time: 408.0274991989136 and batch: 700, loss is 4.382330226898193 and perplexity is 80.02429106504759
At time: 408.53966999053955 and batch: 750, loss is 4.329554038047791 and perplexity is 75.91042585774547
At time: 409.0600748062134 and batch: 800, loss is 4.41042839050293 and perplexity is 82.3047145111072
At time: 409.58234095573425 and batch: 850, loss is 4.331463375091553 and perplexity is 76.05550290225705
At time: 410.10364842414856 and batch: 900, loss is 4.318952579498291 and perplexity is 75.10991541119344
At time: 410.61894392967224 and batch: 950, loss is 4.308861637115479 and perplexity is 74.35579686493361
At time: 411.1305875778198 and batch: 1000, loss is 4.2289530515670775 and perplexity is 68.64532642268203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8721838230040015 and perplexity of 130.60582569464418
Finished 36 epochs...
Completing Train Step...
At time: 412.6675133705139 and batch: 50, loss is 4.451590843200684 and perplexity is 85.76327145290958
At time: 413.2102828025818 and batch: 100, loss is 4.37497875213623 and perplexity is 79.43815164330492
At time: 413.7349100112915 and batch: 150, loss is 4.409007844924926 and perplexity is 82.18787991693985
At time: 414.25060963630676 and batch: 200, loss is 4.446452617645264 and perplexity is 85.32373061683138
At time: 414.7619397640228 and batch: 250, loss is 4.461567859649659 and perplexity is 86.62321572871825
At time: 415.28176283836365 and batch: 300, loss is 4.346224355697632 and perplexity is 77.18648336051011
At time: 415.80316138267517 and batch: 350, loss is 4.356096096038819 and perplexity is 77.95222164836477
At time: 416.3137803077698 and batch: 400, loss is 4.365009622573853 and perplexity is 78.65015675507756
At time: 416.83079981803894 and batch: 450, loss is 4.42513216972351 and perplexity is 83.52384581614922
At time: 417.366174697876 and batch: 500, loss is 4.443753499984741 and perplexity is 85.09374235098338
At time: 417.88832116127014 and batch: 550, loss is 4.3813896751403805 and perplexity is 79.94905946257268
At time: 418.4014492034912 and batch: 600, loss is 4.325023331642151 and perplexity is 75.56727594779939
At time: 418.91560983657837 and batch: 650, loss is 4.286893448829651 and perplexity is 72.7401461545054
At time: 419.42836904525757 and batch: 700, loss is 4.382221736907959 and perplexity is 80.01560970142049
At time: 419.95680832862854 and batch: 750, loss is 4.329638991355896 and perplexity is 75.91687497347475
At time: 420.49348998069763 and batch: 800, loss is 4.410372428894043 and perplexity is 82.30010873573866
At time: 421.0226650238037 and batch: 850, loss is 4.331569919586181 and perplexity is 76.06360662907356
At time: 421.54182720184326 and batch: 900, loss is 4.319077844619751 and perplexity is 75.11932465318293
At time: 422.05561566352844 and batch: 950, loss is 4.3089111328125 and perplexity is 74.3594772480078
At time: 422.57702350616455 and batch: 1000, loss is 4.229076957702636 and perplexity is 68.6538325267716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.872152933260289 and perplexity of 130.601791376471
Finished 37 epochs...
Completing Train Step...
At time: 424.093816280365 and batch: 50, loss is 4.451430139541626 and perplexity is 85.74949008876176
At time: 424.6171143054962 and batch: 100, loss is 4.37494083404541 and perplexity is 79.43513955736297
At time: 425.14208340644836 and batch: 150, loss is 4.40886625289917 and perplexity is 82.1762435923546
At time: 425.6648485660553 and batch: 200, loss is 4.446282329559327 and perplexity is 85.30920223909956
At time: 426.1934497356415 and batch: 250, loss is 4.461435718536377 and perplexity is 86.61176999679834
At time: 426.7107238769531 and batch: 300, loss is 4.346091032028198 and perplexity is 77.17619326128995
At time: 427.22892928123474 and batch: 350, loss is 4.356007261276245 and perplexity is 77.9452970888379
At time: 427.7532105445862 and batch: 400, loss is 4.364921989440918 and perplexity is 78.64326469742606
At time: 428.27786588668823 and batch: 450, loss is 4.425044507980346 and perplexity is 83.51652429114273
At time: 428.8072712421417 and batch: 500, loss is 4.443634786605835 and perplexity is 85.0836411848888
At time: 429.324031829834 and batch: 550, loss is 4.381301598548889 and perplexity is 79.94201813201504
At time: 429.84031796455383 and batch: 600, loss is 4.324982628822327 and perplexity is 75.56420020917794
At time: 430.3667333126068 and batch: 650, loss is 4.286875658035278 and perplexity is 72.73885206103402
At time: 430.88910603523254 and batch: 700, loss is 4.382156009674072 and perplexity is 80.01035066955976
At time: 431.412317276001 and batch: 750, loss is 4.32969901561737 and perplexity is 75.9214319645921
At time: 431.93308901786804 and batch: 800, loss is 4.4103472042083744 and perplexity is 82.29803276754826
At time: 432.4606912136078 and batch: 850, loss is 4.331648597717285 and perplexity is 76.06959140692061
At time: 432.9842767715454 and batch: 900, loss is 4.319166202545166 and perplexity is 75.12596233410936
At time: 433.50687074661255 and batch: 950, loss is 4.3089486026763915 and perplexity is 74.3622635397
At time: 434.0443158149719 and batch: 1000, loss is 4.229152750968933 and perplexity is 68.65903622218265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.872135069312119 and perplexity of 130.59945833367763
Finished 38 epochs...
Completing Train Step...
At time: 435.55646109580994 and batch: 50, loss is 4.451302213668823 and perplexity is 85.73852121201632
At time: 436.0810487270355 and batch: 100, loss is 4.37490873336792 and perplexity is 79.43258967649331
At time: 436.5874514579773 and batch: 150, loss is 4.408749961853028 and perplexity is 82.1666877866573
At time: 437.1277105808258 and batch: 200, loss is 4.44615008354187 and perplexity is 85.29792118280484
At time: 437.65935826301575 and batch: 250, loss is 4.461319942474365 and perplexity is 86.60174300759807
At time: 438.18007373809814 and batch: 300, loss is 4.345977182388306 and perplexity is 77.1674072796288
At time: 438.70695209503174 and batch: 350, loss is 4.355933532714844 and perplexity is 77.93955050606182
At time: 439.2347297668457 and batch: 400, loss is 4.364840631484985 and perplexity is 78.6368667024298
At time: 439.74983406066895 and batch: 450, loss is 4.424960651397705 and perplexity is 83.50952117445442
At time: 440.26359009742737 and batch: 500, loss is 4.443533964157105 and perplexity is 85.0750632762678
At time: 440.785120010376 and batch: 550, loss is 4.381229486465454 and perplexity is 79.93625355438387
At time: 441.3046133518219 and batch: 600, loss is 4.324944133758545 and perplexity is 75.5612914164587
At time: 441.83348059654236 and batch: 650, loss is 4.286857862472534 and perplexity is 72.7375576437457
At time: 442.3553249835968 and batch: 700, loss is 4.382099599838257 and perplexity is 80.00583742611178
At time: 442.8809652328491 and batch: 750, loss is 4.329744381904602 and perplexity is 75.92487631620979
At time: 443.3959815502167 and batch: 800, loss is 4.41031849861145 and perplexity is 82.29567038729891
At time: 443.9080171585083 and batch: 850, loss is 4.331712512969971 and perplexity is 76.07445356945863
At time: 444.42503023147583 and batch: 900, loss is 4.319239358901978 and perplexity is 75.13145847685219
At time: 444.94523906707764 and batch: 950, loss is 4.3089753532409665 and perplexity is 74.36425279883954
At time: 445.46167945861816 and batch: 1000, loss is 4.229209146499634 and perplexity is 68.66290839415335
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.872123160013339 and perplexity of 130.59790299496933
Finished 39 epochs...
Completing Train Step...
At time: 447.02581429481506 and batch: 50, loss is 4.451189241409302 and perplexity is 85.72883568465522
At time: 447.56310200691223 and batch: 100, loss is 4.374883155822754 and perplexity is 79.43055801182585
At time: 448.08990836143494 and batch: 150, loss is 4.408645677566528 and perplexity is 82.158119539022
At time: 448.60244131088257 and batch: 200, loss is 4.446036510467529 and perplexity is 85.28823418576266
At time: 449.1120066642761 and batch: 250, loss is 4.461212873458862 and perplexity is 86.59247114060715
At time: 449.62772965431213 and batch: 300, loss is 4.345873651504516 and perplexity is 77.1594184833041
At time: 450.1419517993927 and batch: 350, loss is 4.355868597030639 and perplexity is 77.93448961234117
At time: 450.6727030277252 and batch: 400, loss is 4.364762172698975 and perplexity is 78.630697191362
At time: 451.192720413208 and batch: 450, loss is 4.4248805809021 and perplexity is 83.5028347934007
At time: 451.7051944732666 and batch: 500, loss is 4.443440914154053 and perplexity is 85.0671474096617
At time: 452.22694301605225 and batch: 550, loss is 4.381164536476136 and perplexity is 79.93106186417128
At time: 452.7506947517395 and batch: 600, loss is 4.324906711578369 and perplexity is 75.55846380110508
At time: 453.267564535141 and batch: 650, loss is 4.286836643218994 and perplexity is 72.73601422344335
At time: 453.7814407348633 and batch: 700, loss is 4.382046480178833 and perplexity is 80.00158765614995
At time: 454.2929701805115 and batch: 750, loss is 4.329780178070068 and perplexity is 75.92759418428976
At time: 454.80836820602417 and batch: 800, loss is 4.4102877998352055 and perplexity is 82.29314404970572
At time: 455.32992696762085 and batch: 850, loss is 4.331766099929809 and perplexity is 76.078530277375
At time: 455.857670545578 and batch: 900, loss is 4.31930365562439 and perplexity is 75.1362893386849
At time: 456.3836615085602 and batch: 950, loss is 4.30899598121643 and perplexity is 74.36578679864326
At time: 456.9031722545624 and batch: 1000, loss is 4.229254961013794 and perplexity is 68.66605422400404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.872114228039253 and perplexity of 130.59673650309367
Finished 40 epochs...
Completing Train Step...
At time: 458.45211243629456 and batch: 50, loss is 4.451084880828858 and perplexity is 85.71988944042812
At time: 458.96735167503357 and batch: 100, loss is 4.374862623214722 and perplexity is 79.42892711205577
At time: 459.4823637008667 and batch: 150, loss is 4.408549537658692 and perplexity is 82.15022124465823
At time: 459.9992368221283 and batch: 200, loss is 4.445934104919433 and perplexity is 85.27950064458393
At time: 460.5326097011566 and batch: 250, loss is 4.461111793518066 and perplexity is 86.58371882110029
At time: 461.0517084598541 and batch: 300, loss is 4.345776643753052 and perplexity is 77.15193378465548
At time: 461.57270312309265 and batch: 350, loss is 4.3558094024658205 and perplexity is 77.92987645068258
At time: 462.0920569896698 and batch: 400, loss is 4.364685859680176 and perplexity is 78.6246968744431
At time: 462.6106536388397 and batch: 450, loss is 4.424803667068481 and perplexity is 83.49641251724283
At time: 463.1306302547455 and batch: 500, loss is 4.443353195190429 and perplexity is 85.0596857349225
At time: 463.66221809387207 and batch: 550, loss is 4.381103472709656 and perplexity is 79.92618112149492
At time: 464.18167662620544 and batch: 600, loss is 4.32487012386322 and perplexity is 75.55569934012738
At time: 464.70283126831055 and batch: 650, loss is 4.286812033653259 and perplexity is 72.73422424374544
At time: 465.2221751213074 and batch: 700, loss is 4.381995096206665 and perplexity is 79.99747696240922
At time: 465.7374472618103 and batch: 750, loss is 4.329809165000915 and perplexity is 75.92979512411083
At time: 466.2532157897949 and batch: 800, loss is 4.410255746841431 and perplexity is 82.2905063503451
At time: 466.77926659584045 and batch: 850, loss is 4.331812133789063 and perplexity is 76.08203254634086
At time: 467.30278062820435 and batch: 900, loss is 4.319361457824707 and perplexity is 75.14063250705345
At time: 467.8234016895294 and batch: 950, loss is 4.309012603759766 and perplexity is 74.36702295743103
At time: 468.3614664077759 and batch: 1000, loss is 4.2292930698394775 and perplexity is 68.66867105655673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8721067847275155 and perplexity of 130.59576443448967
Finished 41 epochs...
Completing Train Step...
At time: 469.88017225265503 and batch: 50, loss is 4.450986108779907 and perplexity is 85.71142312943661
At time: 470.41685342788696 and batch: 100, loss is 4.374845724105835 and perplexity is 79.42758484530935
At time: 470.943724155426 and batch: 150, loss is 4.408459568023682 and perplexity is 82.14283055171084
At time: 471.4757716655731 and batch: 200, loss is 4.445839300155639 and perplexity is 85.2714161249004
At time: 471.9968316555023 and batch: 250, loss is 4.461014776229859 and perplexity is 86.57531911096248
At time: 472.50971841812134 and batch: 300, loss is 4.345683917999268 and perplexity is 77.14478014510786
At time: 473.02223086357117 and batch: 350, loss is 4.355754261016846 and perplexity is 77.92557940285047
At time: 473.5376582145691 and batch: 400, loss is 4.364611368179322 and perplexity is 78.6188402209069
At time: 474.0779285430908 and batch: 450, loss is 4.424729261398316 and perplexity is 83.49020014183391
At time: 474.5965702533722 and batch: 500, loss is 4.44326979637146 and perplexity is 85.0525921533926
At time: 475.1215114593506 and batch: 550, loss is 4.381044526100158 and perplexity is 79.92146988296486
At time: 475.6482906341553 and batch: 600, loss is 4.324833736419678 and perplexity is 75.55295011140238
At time: 476.1929154396057 and batch: 650, loss is 4.286784362792969 and perplexity is 72.73221165303326
At time: 476.72134232521057 and batch: 700, loss is 4.381944837570191 and perplexity is 79.99345649932809
At time: 477.2460663318634 and batch: 750, loss is 4.32983274936676 and perplexity is 75.93158590129467
At time: 477.7828640937805 and batch: 800, loss is 4.410222673416138 and perplexity is 82.28778476643733
At time: 478.3008351325989 and batch: 850, loss is 4.331852912902832 and perplexity is 76.0851351674625
At time: 478.82946729660034 and batch: 900, loss is 4.319414548873901 and perplexity is 75.14462190797018
At time: 479.35479283332825 and batch: 950, loss is 4.309026975631713 and perplexity is 74.36809175844242
At time: 479.8699097633362 and batch: 1000, loss is 4.229324951171875 and perplexity is 68.67086034018241
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.872100457912538 and perplexity of 130.59493818186502
Finished 42 epochs...
Completing Train Step...
At time: 481.4029788970947 and batch: 50, loss is 4.450891361236573 and perplexity is 85.70330256736712
At time: 481.9339852333069 and batch: 100, loss is 4.374831552505493 and perplexity is 79.42645923729664
At time: 482.448100566864 and batch: 150, loss is 4.408375062942505 and perplexity is 82.13588935843414
At time: 482.96881556510925 and batch: 200, loss is 4.44575005531311 and perplexity is 85.26380643036411
At time: 483.4972062110901 and batch: 250, loss is 4.460920906066894 and perplexity is 86.56719265307076
At time: 484.02099323272705 and batch: 300, loss is 4.345594472885132 and perplexity is 77.13788023002925
At time: 484.546110868454 and batch: 350, loss is 4.355702171325683 and perplexity is 77.92152038920304
At time: 485.0705418586731 and batch: 400, loss is 4.364538345336914 and perplexity is 78.6130994593331
At time: 485.5964105129242 and batch: 450, loss is 4.424656991958618 and perplexity is 83.48416656987348
At time: 486.1139118671417 and batch: 500, loss is 4.443189115524292 and perplexity is 85.04573031501698
At time: 486.62822103500366 and batch: 550, loss is 4.380986728668213 and perplexity is 79.91685076073631
At time: 487.1645784378052 and batch: 600, loss is 4.324797148704529 and perplexity is 75.5501858521543
At time: 487.6782295703888 and batch: 650, loss is 4.286754369735718 and perplexity is 72.73003022435917
At time: 488.1907618045807 and batch: 700, loss is 4.381895389556885 and perplexity is 79.98950107962133
At time: 488.7036418914795 and batch: 750, loss is 4.329852123260498 and perplexity is 75.9330570060217
At time: 489.2367579936981 and batch: 800, loss is 4.410188426971436 and perplexity is 82.28496675062043
At time: 489.7639493942261 and batch: 850, loss is 4.331889848709107 and perplexity is 76.0879454851758
At time: 490.2896378040314 and batch: 900, loss is 4.319463605880737 and perplexity is 75.14830836862346
At time: 490.81362080574036 and batch: 950, loss is 4.309040489196778 and perplexity is 74.36909674327956
At time: 491.32855248451233 and batch: 1000, loss is 4.229351258277893 and perplexity is 68.67266689554825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.872095991925495 and perplexity of 130.59435494786564
Finished 43 epochs...
Completing Train Step...
At time: 492.8608272075653 and batch: 50, loss is 4.450799484252929 and perplexity is 85.69542876815495
At time: 493.3793845176697 and batch: 100, loss is 4.374819078445435 and perplexity is 79.42546847305331
At time: 493.894677400589 and batch: 150, loss is 4.408295583724976 and perplexity is 82.12936152163398
At time: 494.4134180545807 and batch: 200, loss is 4.445665369033813 and perplexity is 85.25658606157626
At time: 494.9361457824707 and batch: 250, loss is 4.460828876495361 and perplexity is 86.55922627799873
At time: 495.4516098499298 and batch: 300, loss is 4.345507745742798 and perplexity is 77.13119057220281
At time: 495.97333002090454 and batch: 350, loss is 4.355651645660401 and perplexity is 77.91758345200455
At time: 496.4999611377716 and batch: 400, loss is 4.364466581344605 and perplexity is 78.60745807189474
At time: 497.0171802043915 and batch: 450, loss is 4.424586315155029 and perplexity is 83.4782663843356
At time: 497.5297636985779 and batch: 500, loss is 4.443109807968139 and perplexity is 85.03898581343299
At time: 498.0476915836334 and batch: 550, loss is 4.380928993225098 and perplexity is 79.91223685913937
At time: 498.5665740966797 and batch: 600, loss is 4.324760317802429 and perplexity is 75.54740332189743
At time: 499.09408593177795 and batch: 650, loss is 4.286722240447998 and perplexity is 72.72769349783108
At time: 499.623108625412 and batch: 700, loss is 4.381846122741699 and perplexity is 79.98556034872927
At time: 500.1678593158722 and batch: 750, loss is 4.329868230819702 and perplexity is 75.93428011208361
At time: 500.6877408027649 and batch: 800, loss is 4.410153732299805 and perplexity is 82.28211195024234
At time: 501.20344138145447 and batch: 850, loss is 4.331923522949219 and perplexity is 76.09050773206229
At time: 501.72597312927246 and batch: 900, loss is 4.319508819580078 and perplexity is 75.15170617845722
At time: 502.2525329589844 and batch: 950, loss is 4.309053926467896 and perplexity is 74.37009606770937
At time: 502.7879021167755 and batch: 1000, loss is 4.229372501373291 and perplexity is 68.6741257310574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.872091898104039 and perplexity of 130.59382031898767
Finished 44 epochs...
Completing Train Step...
At time: 504.34528160095215 and batch: 50, loss is 4.4507089042663575 and perplexity is 85.68766682891135
At time: 504.8882987499237 and batch: 100, loss is 4.37480749130249 and perplexity is 79.42454816412855
At time: 505.4057478904724 and batch: 150, loss is 4.408220739364624 and perplexity is 82.12321483213017
At time: 505.93684935569763 and batch: 200, loss is 4.445583267211914 and perplexity is 85.24958662786875
At time: 506.4555461406708 and batch: 250, loss is 4.460737323760986 and perplexity is 86.55130190690205
At time: 506.972695350647 and batch: 300, loss is 4.345423078536987 and perplexity is 77.12466036626725
At time: 507.4906632900238 and batch: 350, loss is 4.3556019115448 and perplexity is 77.91370838626409
At time: 508.014436006546 and batch: 400, loss is 4.364395561218262 and perplexity is 78.60187555852876
At time: 508.549720287323 and batch: 450, loss is 4.424516696929931 and perplexity is 83.47245497788794
At time: 509.0796766281128 and batch: 500, loss is 4.443031120300293 and perplexity is 85.03229455722644
At time: 509.6043622493744 and batch: 550, loss is 4.380870013237 and perplexity is 79.90752377535071
At time: 510.12752079963684 and batch: 600, loss is 4.324722418785095 and perplexity is 75.5445402038044
At time: 510.65671014785767 and batch: 650, loss is 4.2866885948181155 and perplexity is 72.72524656993792
At time: 511.17290019989014 and batch: 700, loss is 4.381796493530273 and perplexity is 79.98159082694664
At time: 511.71094965934753 and batch: 750, loss is 4.329882626533508 and perplexity is 75.93537324811638
At time: 512.2265672683716 and batch: 800, loss is 4.410118942260742 and perplexity is 82.27924940214778
At time: 512.7498235702515 and batch: 850, loss is 4.33195424079895 and perplexity is 76.09284510474414
At time: 513.2794246673584 and batch: 900, loss is 4.3195497512817385 and perplexity is 75.15478232862937
At time: 513.8256559371948 and batch: 950, loss is 4.309066495895386 and perplexity is 74.37103086311423
At time: 514.3444178104401 and batch: 1000, loss is 4.229388132095337 and perplexity is 68.67519916561768
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.872086315620236 and perplexity of 130.59309128313583
Finished 45 epochs...
Completing Train Step...
At time: 515.9097857475281 and batch: 50, loss is 4.450617895126343 and perplexity is 85.67986882289371
At time: 516.4483344554901 and batch: 100, loss is 4.374795026779175 and perplexity is 79.42355818116603
At time: 516.9652590751648 and batch: 150, loss is 4.408151111602783 and perplexity is 82.11749697554934
At time: 517.4827587604523 and batch: 200, loss is 4.445501670837403 and perplexity is 85.24263085445813
At time: 517.9974472522736 and batch: 250, loss is 4.460642747879028 and perplexity is 86.54311662826068
At time: 518.5069830417633 and batch: 300, loss is 4.345339088439942 and perplexity is 77.11818293058253
At time: 519.0199301242828 and batch: 350, loss is 4.3555505752563475 and perplexity is 77.9097086883217
At time: 519.5350115299225 and batch: 400, loss is 4.364324655532837 and perplexity is 78.59630243625192
At time: 520.0638275146484 and batch: 450, loss is 4.42444694519043 and perplexity is 83.46663283200752
At time: 520.574346780777 and batch: 500, loss is 4.442951698303222 and perplexity is 85.02554139075573
At time: 521.0953903198242 and batch: 550, loss is 4.380806198120117 and perplexity is 79.90242463008423
At time: 521.6123471260071 and batch: 600, loss is 4.324681959152222 and perplexity is 75.54148376127385
At time: 522.1286692619324 and batch: 650, loss is 4.286653213500976 and perplexity is 72.72267350044456
At time: 522.6522798538208 and batch: 700, loss is 4.38174427986145 and perplexity is 79.97741480367495
At time: 523.1840975284576 and batch: 750, loss is 4.329897179603576 and perplexity is 75.9364783489652
At time: 523.6961431503296 and batch: 800, loss is 4.410087432861328 and perplexity is 82.2766568732596
At time: 524.2228319644928 and batch: 850, loss is 4.33198148727417 and perplexity is 76.09491839480751
At time: 524.7346270084381 and batch: 900, loss is 4.319585371017456 and perplexity is 75.15745936979127
At time: 525.2551648616791 and batch: 950, loss is 4.309075517654419 and perplexity is 74.37170182366035
At time: 525.7690515518188 and batch: 1000, loss is 4.229395818710327 and perplexity is 68.67572704746189
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.872079988805259 and perplexity of 130.59226504742375
Finished 46 epochs...
Completing Train Step...
At time: 527.3143591880798 and batch: 50, loss is 4.4505206489562985 and perplexity is 85.67153718891707
At time: 527.8368353843689 and batch: 100, loss is 4.374778881072998 and perplexity is 79.42227584208425
At time: 528.3620500564575 and batch: 150, loss is 4.408089580535889 and perplexity is 82.11244435379797
At time: 528.8848464488983 and batch: 200, loss is 4.445416765213013 and perplexity is 85.23539358290768
At time: 529.4089088439941 and batch: 250, loss is 4.460536794662476 and perplexity is 86.53394759243643
At time: 529.9391767978668 and batch: 300, loss is 4.345254793167114 and perplexity is 77.11168250629379
At time: 530.4643723964691 and batch: 350, loss is 4.355494947433471 and perplexity is 77.90537486138825
At time: 530.9924178123474 and batch: 400, loss is 4.364252500534057 and perplexity is 78.59063152474033
At time: 531.509119272232 and batch: 450, loss is 4.424377908706665 and perplexity is 83.46087078806306
At time: 532.0254144668579 and batch: 500, loss is 4.442869205474853 and perplexity is 85.01852768265708
At time: 532.5431087017059 and batch: 550, loss is 4.380733647346497 and perplexity is 79.89662785764584
At time: 533.0558495521545 and batch: 600, loss is 4.324636507034302 and perplexity is 75.53805031887529
At time: 533.5775804519653 and batch: 650, loss is 4.286616635322571 and perplexity is 72.72001348616868
At time: 534.0907015800476 and batch: 700, loss is 4.381689100265503 and perplexity is 79.97300180399611
At time: 534.6083252429962 and batch: 750, loss is 4.329915475845337 and perplexity is 75.93786771384157
At time: 535.1356160640717 and batch: 800, loss is 4.410072288513184 and perplexity is 82.27541085635883
At time: 535.6593487262726 and batch: 850, loss is 4.332007665634155 and perplexity is 76.09691046104865
At time: 536.1729271411896 and batch: 900, loss is 4.319618558883667 and perplexity is 75.15995372688855
At time: 536.6842210292816 and batch: 950, loss is 4.309080657958984 and perplexity is 74.3720841178413
At time: 537.2101383209229 and batch: 1000, loss is 4.229400267601013 and perplexity is 68.67603257894395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.872072173327934 and perplexity of 130.59124441052586
Finished 47 epochs...
Completing Train Step...
At time: 538.7631771564484 and batch: 50, loss is 4.450422620773315 and perplexity is 85.66313937541096
At time: 539.3199861049652 and batch: 100, loss is 4.374761600494384 and perplexity is 79.4209033910613
At time: 539.8423583507538 and batch: 150, loss is 4.4080362796783445 and perplexity is 82.10806780673673
At time: 540.3741092681885 and batch: 200, loss is 4.445338649749756 and perplexity is 85.22873564069955
At time: 540.8904242515564 and batch: 250, loss is 4.460430860519409 and perplexity is 86.52478117837876
At time: 541.4047377109528 and batch: 300, loss is 4.345173416137695 and perplexity is 77.1054076419563
At time: 541.9366202354431 and batch: 350, loss is 4.355440979003906 and perplexity is 77.90117054410351
At time: 542.4584450721741 and batch: 400, loss is 4.364181613922119 and perplexity is 78.58506069859226
At time: 542.9731783866882 and batch: 450, loss is 4.424315032958984 and perplexity is 83.4556232883821
At time: 543.4956486225128 and batch: 500, loss is 4.442791242599487 and perplexity is 85.01189965215306
At time: 544.0138404369354 and batch: 550, loss is 4.380670523643493 and perplexity is 79.89158464581276
At time: 544.5293288230896 and batch: 600, loss is 4.324594850540161 and perplexity is 75.5349037340631
At time: 545.0492947101593 and batch: 650, loss is 4.286582474708557 and perplexity is 72.71752936828663
At time: 545.5778830051422 and batch: 700, loss is 4.381641902923584 and perplexity is 79.96922737995776
At time: 546.1115646362305 and batch: 750, loss is 4.32993335723877 and perplexity is 75.93922560087101
At time: 546.6376433372498 and batch: 800, loss is 4.410061702728272 and perplexity is 82.27453991116582
At time: 547.1597473621368 and batch: 850, loss is 4.332033624649048 and perplexity is 76.09888588752058
At time: 547.675530910492 and batch: 900, loss is 4.319652881622314 and perplexity is 75.16253346660869
At time: 548.1913876533508 and batch: 950, loss is 4.30909107208252 and perplexity is 74.37285864194591
At time: 548.7134599685669 and batch: 1000, loss is 4.229409818649292 and perplexity is 68.6766885101791
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.872065846512958 and perplexity of 130.59041818649857
Finished 48 epochs...
Completing Train Step...
At time: 550.2892167568207 and batch: 50, loss is 4.450339107513428 and perplexity is 85.6559856661085
At time: 550.822811126709 and batch: 100, loss is 4.374749450683594 and perplexity is 79.41993844797426
At time: 551.3516499996185 and batch: 150, loss is 4.407978067398071 and perplexity is 82.1032882479967
At time: 551.8807039260864 and batch: 200, loss is 4.445270519256592 and perplexity is 85.22292916271
At time: 552.3984713554382 and batch: 250, loss is 4.460344200134277 and perplexity is 86.51728323241052
At time: 552.920892238617 and batch: 300, loss is 4.345094947814942 and perplexity is 77.09935754731693
At time: 553.4363396167755 and batch: 350, loss is 4.355391855239868 and perplexity is 77.89734383937527
At time: 553.9729428291321 and batch: 400, loss is 4.364113664627075 and perplexity is 78.5797210805305
At time: 554.495138168335 and batch: 450, loss is 4.42425199508667 and perplexity is 83.45036258927074
At time: 555.0225963592529 and batch: 500, loss is 4.442720565795899 and perplexity is 85.00589149513984
At time: 555.5469491481781 and batch: 550, loss is 4.380615272521973 and perplexity is 79.88717066810075
At time: 556.0790519714355 and batch: 600, loss is 4.324557666778564 and perplexity is 75.53209511442826
At time: 556.6028668880463 and batch: 650, loss is 4.286548895835876 and perplexity is 72.71508763662186
At time: 557.1155664920807 and batch: 700, loss is 4.381597719192505 and perplexity is 79.96569411917748
At time: 557.6301355361938 and batch: 750, loss is 4.329946746826172 and perplexity is 75.94024240257673
At time: 558.1502721309662 and batch: 800, loss is 4.410039110183716 and perplexity is 82.27268114095429
At time: 558.6687421798706 and batch: 850, loss is 4.332057018280029 and perplexity is 76.10066613759827
At time: 559.1909594535828 and batch: 900, loss is 4.319684867858887 and perplexity is 75.16493767163607
At time: 559.702760219574 and batch: 950, loss is 4.309105110168457 and perplexity is 74.37390270185522
At time: 560.2256534099579 and batch: 1000, loss is 4.2294200420379635 and perplexity is 68.67739062224739
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.872061008360328 and perplexity of 130.58978637165183
Finished 49 epochs...
Completing Train Step...
At time: 561.7535960674286 and batch: 50, loss is 4.450262784957886 and perplexity is 85.64944843185737
At time: 562.2677369117737 and batch: 100, loss is 4.3747389698028565 and perplexity is 79.41910606143328
At time: 562.7916648387909 and batch: 150, loss is 4.407916650772095 and perplexity is 82.09824589589468
At time: 563.3166122436523 and batch: 200, loss is 4.44520583152771 and perplexity is 85.21741646327773
At time: 563.8383293151855 and batch: 250, loss is 4.460265007019043 and perplexity is 86.51043193052128
At time: 564.3620662689209 and batch: 300, loss is 4.345018148422241 and perplexity is 77.09343659084554
At time: 564.8901612758636 and batch: 350, loss is 4.355343647003174 and perplexity is 77.8935886363022
At time: 565.4168815612793 and batch: 400, loss is 4.364047203063965 and perplexity is 78.5744987229837
At time: 565.9390151500702 and batch: 450, loss is 4.424188327789307 and perplexity is 83.44504969935117
At time: 566.4643111228943 and batch: 500, loss is 4.442652654647827 and perplexity is 85.00011884347151
At time: 567.0038805007935 and batch: 550, loss is 4.380561256408692 and perplexity is 79.88285559018317
At time: 567.5249259471893 and batch: 600, loss is 4.3245215368270875 and perplexity is 75.52936619279504
At time: 568.0392782688141 and batch: 650, loss is 4.286515831947327 and perplexity is 72.71268343281473
At time: 568.5604317188263 and batch: 700, loss is 4.381553831100464 and perplexity is 79.96218465444632
At time: 569.0837135314941 and batch: 750, loss is 4.3299582862854 and perplexity is 75.9411187169638
At time: 569.6036946773529 and batch: 800, loss is 4.410011377334595 and perplexity is 82.27039951673953
At time: 570.1241512298584 and batch: 850, loss is 4.332077722549439 and perplexity is 76.10224176260328
At time: 570.6404778957367 and batch: 900, loss is 4.3197148895263675 and perplexity is 75.1671942822745
At time: 571.1579523086548 and batch: 950, loss is 4.3091198825836186 and perplexity is 74.37500139213823
At time: 571.6856019496918 and batch: 1000, loss is 4.229428768157959 and perplexity is 68.67798991201367
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.872056542373285 and perplexity of 130.58920316066022
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f46fd764898>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'batch_size': 20, 'lr': 4.57052781456207, 'dropout': 0.0, 'wordvec_source': '', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'anneal': 8.0, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7581393718719482 and batch: 50, loss is 6.589087858200073 and perplexity is 727.1173332119857
At time: 1.295365571975708 and batch: 100, loss is 5.726297283172608 and perplexity is 306.8310539283873
At time: 1.8054299354553223 and batch: 150, loss is 5.447058830261231 and perplexity is 232.074590382239
At time: 2.318434238433838 and batch: 200, loss is 5.297764854431152 and perplexity is 199.8895280979657
At time: 2.8395614624023438 and batch: 250, loss is 5.279930114746094 and perplexity is 196.3561524727189
At time: 3.3513729572296143 and batch: 300, loss is 5.141133852005005 and perplexity is 170.9094445168418
At time: 3.8616225719451904 and batch: 350, loss is 5.074172954559327 and perplexity is 159.83994240311824
At time: 4.379924058914185 and batch: 400, loss is 5.040453729629516 and perplexity is 154.54011855140055
At time: 4.894601345062256 and batch: 450, loss is 5.028463497161865 and perplexity is 152.69821112040083
At time: 5.406466484069824 and batch: 500, loss is 5.023806858062744 and perplexity is 151.98880366928458
At time: 5.9167540073394775 and batch: 550, loss is 4.92776424407959 and perplexity is 138.0704751206091
At time: 6.439605951309204 and batch: 600, loss is 4.828345050811768 and perplexity is 125.00391424996747
At time: 6.960509300231934 and batch: 650, loss is 4.7889251708984375 and perplexity is 120.17213471586375
At time: 7.47018837928772 and batch: 700, loss is 4.8366439914703365 and perplexity is 126.04563091000068
At time: 7.994096994400024 and batch: 750, loss is 4.774974784851074 and perplexity is 118.50732640316522
At time: 8.518521785736084 and batch: 800, loss is 4.857845306396484 and perplexity is 128.74649378002016
At time: 9.033276557922363 and batch: 850, loss is 4.738405113220215 and perplexity is 114.25183754847117
At time: 9.558459520339966 and batch: 900, loss is 4.752884006500244 and perplexity is 115.918111503291
At time: 10.086288213729858 and batch: 950, loss is 4.727175827026367 and perplexity is 112.97604747844244
At time: 10.618438005447388 and batch: 1000, loss is 4.605632858276367 and perplexity is 100.0462779337608
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.799645400628811 and perplexity of 121.46733763959026
Finished 1 epochs...
Completing Train Step...
At time: 12.185517072677612 and batch: 50, loss is 4.727326326370239 and perplexity is 112.99305157898205
At time: 12.707293272018433 and batch: 100, loss is 4.596399202346801 and perplexity is 99.12673692230359
At time: 13.217541694641113 and batch: 150, loss is 4.650950736999512 and perplexity is 104.68446567470718
At time: 13.732450008392334 and batch: 200, loss is 4.645985326766968 and perplexity is 104.16595273771959
At time: 14.248778581619263 and batch: 250, loss is 4.671582221984863 and perplexity is 106.86669557170158
At time: 14.76991581916809 and batch: 300, loss is 4.54702784538269 and perplexity is 94.35156372911139
At time: 15.294328212738037 and batch: 350, loss is 4.529701938629151 and perplexity is 92.73091745819046
At time: 15.805787086486816 and batch: 400, loss is 4.536892404556275 and perplexity is 93.40009894067842
At time: 16.31884503364563 and batch: 450, loss is 4.579026117324829 and perplexity is 97.41947285658763
At time: 16.82934284210205 and batch: 500, loss is 4.587392950057984 and perplexity is 98.23798469182647
At time: 17.34115958213806 and batch: 550, loss is 4.5012917852401735 and perplexity is 90.13348924065278
At time: 17.853848457336426 and batch: 600, loss is 4.431956954002381 and perplexity is 84.09582765217668
At time: 18.37870454788208 and batch: 650, loss is 4.401192817687988 and perplexity is 81.54808266957937
At time: 18.90093493461609 and batch: 700, loss is 4.477104930877686 and perplexity is 87.97959660885151
At time: 19.419780015945435 and batch: 750, loss is 4.4271499538421635 and perplexity is 83.69254905188185
At time: 19.952446937561035 and batch: 800, loss is 4.53629391670227 and perplexity is 93.34421683994691
At time: 20.471178770065308 and batch: 850, loss is 4.413700819015503 and perplexity is 82.57449197858173
At time: 20.986536264419556 and batch: 900, loss is 4.398152189254761 and perplexity is 81.30050184213982
At time: 21.50490927696228 and batch: 950, loss is 4.417215495109558 and perplexity is 82.86522518876512
At time: 22.012925148010254 and batch: 1000, loss is 4.303269553184509 and perplexity is 73.94115344904185
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.629267994950458 and perplexity of 102.43905075634203
Finished 2 epochs...
Completing Train Step...
At time: 23.526694297790527 and batch: 50, loss is 4.438790016174316 and perplexity is 84.67242739839422
At time: 24.049737215042114 and batch: 100, loss is 4.315788269042969 and perplexity is 74.87261995680362
At time: 24.553531408309937 and batch: 150, loss is 4.409005098342895 and perplexity is 82.18765418149569
At time: 25.07922887802124 and batch: 200, loss is 4.420694808959961 and perplexity is 83.1540414643567
At time: 25.60148859024048 and batch: 250, loss is 4.432951421737671 and perplexity is 84.1794998371977
At time: 26.13131618499756 and batch: 300, loss is 4.296542930603027 and perplexity is 73.44544829640228
At time: 26.644301891326904 and batch: 350, loss is 4.302381324768066 and perplexity is 73.87550597469598
At time: 27.154444932937622 and batch: 400, loss is 4.314198112487793 and perplexity is 74.75365538059424
At time: 27.668286561965942 and batch: 450, loss is 4.361409683227539 and perplexity is 78.36752998602088
At time: 28.19136691093445 and batch: 500, loss is 4.374442281723023 and perplexity is 79.39554685439458
At time: 28.712268114089966 and batch: 550, loss is 4.289189562797547 and perplexity is 72.90735771505388
At time: 29.2238986492157 and batch: 600, loss is 4.226952891349793 and perplexity is 68.50816199281559
At time: 29.738786458969116 and batch: 650, loss is 4.1980483341217045 and perplexity is 66.55630852560843
At time: 30.262216329574585 and batch: 700, loss is 4.285664076805115 and perplexity is 72.65077639935659
At time: 30.772560119628906 and batch: 750, loss is 4.239914517402649 and perplexity is 69.40191892888737
At time: 31.28462529182434 and batch: 800, loss is 4.358189697265625 and perplexity is 78.1155934732928
At time: 31.80115509033203 and batch: 850, loss is 4.231252093315124 and perplexity is 68.80332644867039
At time: 32.324183225631714 and batch: 900, loss is 4.20007700920105 and perplexity is 66.69146669974351
At time: 32.86785697937012 and batch: 950, loss is 4.23189311504364 and perplexity is 68.84744501488721
At time: 33.375643730163574 and batch: 1000, loss is 4.127075023651123 and perplexity is 61.99631970280883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.563082439143483 and perplexity of 95.87856464661542
Finished 3 epochs...
Completing Train Step...
At time: 34.86405825614929 and batch: 50, loss is 4.264774928092956 and perplexity is 71.14890455152033
At time: 35.39130473136902 and batch: 100, loss is 4.1448580837249756 and perplexity is 63.108665119527956
At time: 35.900402545928955 and batch: 150, loss is 4.252778348922729 and perplexity is 70.3004604739676
At time: 36.40970301628113 and batch: 200, loss is 4.273851566314697 and perplexity is 71.79763711958633
At time: 36.925620794296265 and batch: 250, loss is 4.275331926345825 and perplexity is 71.90400218177639
At time: 37.437193870544434 and batch: 300, loss is 4.134204258918762 and perplexity is 62.43988531477972
At time: 37.952691078186035 and batch: 350, loss is 4.150551981925965 and perplexity is 63.46902438522177
At time: 38.46817708015442 and batch: 400, loss is 4.159687333106994 and perplexity is 64.05149268754359
At time: 38.98629689216614 and batch: 450, loss is 4.21344162940979 and perplexity is 67.58875542930845
At time: 39.507588148117065 and batch: 500, loss is 4.231078028678894 and perplexity is 68.79135126493891
At time: 40.01914572715759 and batch: 550, loss is 4.142619333267212 and perplexity is 62.96753859907777
At time: 40.52774143218994 and batch: 600, loss is 4.085522222518921 and perplexity is 59.47298777651141
At time: 41.044458627700806 and batch: 650, loss is 4.0601660251617435 and perplexity is 57.983937072389594
At time: 41.54925537109375 and batch: 700, loss is 4.153060870170593 and perplexity is 63.628460995086634
At time: 42.059792041778564 and batch: 750, loss is 4.108829388618469 and perplexity is 60.87541439066708
At time: 42.569212913513184 and batch: 800, loss is 4.232551336288452 and perplexity is 68.8927767833769
At time: 43.080058574676514 and batch: 850, loss is 4.100198616981507 and perplexity is 60.352273393399706
At time: 43.59446859359741 and batch: 900, loss is 4.064221420288086 and perplexity is 58.21956230197416
At time: 44.11560297012329 and batch: 950, loss is 4.103152418136597 and perplexity is 60.53080555270379
At time: 44.62667465209961 and batch: 1000, loss is 4.000067391395569 and perplexity is 54.601829602654725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.534031937762005 and perplexity of 93.13331280742442
Finished 4 epochs...
Completing Train Step...
At time: 46.177574157714844 and batch: 50, loss is 4.13794234752655 and perplexity is 62.67372792865582
At time: 46.69797658920288 and batch: 100, loss is 4.02411500453949 and perplexity is 55.93078837839256
At time: 47.217650175094604 and batch: 150, loss is 4.13104896068573 and perplexity is 62.24317935116379
At time: 47.72960305213928 and batch: 200, loss is 4.163323516845703 and perplexity is 64.28481963643866
At time: 48.25558423995972 and batch: 250, loss is 4.158482642173767 and perplexity is 63.97437689471781
At time: 48.770241498947144 and batch: 300, loss is 4.012753591537476 and perplexity is 55.298931780725454
At time: 49.28644132614136 and batch: 350, loss is 4.0365515279769895 and perplexity is 56.63071625234727
At time: 49.80602741241455 and batch: 400, loss is 4.043586711883545 and perplexity is 57.03052848321068
At time: 50.3214795589447 and batch: 450, loss is 4.100585508346557 and perplexity is 60.37562768433224
At time: 50.837327003479004 and batch: 500, loss is 4.119801878929138 and perplexity is 61.54704728951028
At time: 51.370564460754395 and batch: 550, loss is 4.034009385108948 and perplexity is 56.48693571368141
At time: 51.89003300666809 and batch: 600, loss is 3.976710920333862 and perplexity is 53.341301596157315
At time: 52.4050452709198 and batch: 650, loss is 3.9537186098098753 and perplexity is 52.128853727589366
At time: 52.914310932159424 and batch: 700, loss is 4.042491884231567 and perplexity is 56.96812405090279
At time: 53.42910838127136 and batch: 750, loss is 4.005314474105835 and perplexity is 54.88908288051404
At time: 53.94351005554199 and batch: 800, loss is 4.130984754562378 and perplexity is 62.239183086206246
At time: 54.46059012413025 and batch: 850, loss is 3.998391718864441 and perplexity is 54.51041143149029
At time: 54.970970153808594 and batch: 900, loss is 3.954534845352173 and perplexity is 52.17142052068247
At time: 55.48180270195007 and batch: 950, loss is 3.9981400203704833 and perplexity is 54.496692969558495
At time: 55.99821877479553 and batch: 1000, loss is 3.9006739616394044 and perplexity is 49.43575568353717
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.520995349418826 and perplexity of 91.92705199403224
Finished 5 epochs...
Completing Train Step...
At time: 57.50574564933777 and batch: 50, loss is 4.036674728393555 and perplexity is 56.637693609977816
At time: 58.028767347335815 and batch: 100, loss is 3.9247091245651244 and perplexity is 50.63834646536368
At time: 58.54475498199463 and batch: 150, loss is 4.035604109764099 and perplexity is 56.5770886882376
At time: 59.07059407234192 and batch: 200, loss is 4.069267601966858 and perplexity is 58.514091289008604
At time: 59.591548681259155 and batch: 250, loss is 4.061621990203857 and perplexity is 58.068421145775496
At time: 60.127952575683594 and batch: 300, loss is 3.91472776889801 and perplexity is 50.135421232135144
At time: 60.64892888069153 and batch: 350, loss is 3.942883234024048 and perplexity is 51.5670670892221
At time: 61.15754985809326 and batch: 400, loss is 3.951411714553833 and perplexity is 52.00873652439501
At time: 61.66646981239319 and batch: 450, loss is 4.007264218330383 and perplexity is 54.99620695118409
At time: 62.17537212371826 and batch: 500, loss is 4.031104416847229 and perplexity is 56.32308106973011
At time: 62.6889591217041 and batch: 550, loss is 3.9463208723068237 and perplexity is 51.744641055840106
At time: 63.195526123046875 and batch: 600, loss is 3.883416175842285 and perplexity is 48.58992358861525
At time: 63.70641779899597 and batch: 650, loss is 3.8661270141601562 and perplexity is 47.7570650030066
At time: 64.22112894058228 and batch: 700, loss is 3.9537243461608886 and perplexity is 52.12915275784994
At time: 64.7326865196228 and batch: 750, loss is 3.920458197593689 and perplexity is 50.423543432053364
At time: 65.23984551429749 and batch: 800, loss is 4.042703523635864 and perplexity is 56.98018202666722
At time: 65.7565586566925 and batch: 850, loss is 3.9110893487930296 and perplexity is 49.953338954331215
At time: 66.2760398387909 and batch: 900, loss is 3.8625518894195556 and perplexity is 47.586632378853736
At time: 66.79718804359436 and batch: 950, loss is 3.911029348373413 and perplexity is 49.95034182294817
At time: 67.30650401115417 and batch: 1000, loss is 3.8156255006790163 and perplexity is 45.405148453580374
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.520641419945694 and perplexity of 91.8945220579445
Finished 6 epochs...
Completing Train Step...
At time: 68.79637265205383 and batch: 50, loss is 3.953087763786316 and perplexity is 52.09597881809583
At time: 69.34052801132202 and batch: 100, loss is 3.844283843040466 and perplexity is 46.72520978164921
At time: 69.85930252075195 and batch: 150, loss is 3.948854146003723 and perplexity is 51.875890569261536
At time: 70.36563873291016 and batch: 200, loss is 3.989937653541565 and perplexity is 54.05151933746938
At time: 70.87539458274841 and batch: 250, loss is 3.977103762626648 and perplexity is 53.36226043186553
At time: 71.38102507591248 and batch: 300, loss is 3.8303319358825685 and perplexity is 46.07783058139556
At time: 71.88962173461914 and batch: 350, loss is 3.8639542198181154 and perplexity is 47.65341137216627
At time: 72.41477179527283 and batch: 400, loss is 3.87216646194458 and perplexity is 48.04636402725885
At time: 72.93145895004272 and batch: 450, loss is 3.9274485063552858 and perplexity is 50.777254403626856
At time: 73.44507026672363 and batch: 500, loss is 3.954706616401672 and perplexity is 52.18038283004978
At time: 73.96643900871277 and batch: 550, loss is 3.865612812042236 and perplexity is 47.73251453152956
At time: 74.48089098930359 and batch: 600, loss is 3.8096218395233152 and perplexity is 45.133367982804664
At time: 74.99747967720032 and batch: 650, loss is 3.7965581703186033 and perplexity is 44.5475950956128
At time: 75.51599788665771 and batch: 700, loss is 3.8796525144577028 and perplexity is 48.407391279892224
At time: 76.02369546890259 and batch: 750, loss is 3.845512843132019 and perplexity is 46.78267037105093
At time: 76.55315017700195 and batch: 800, loss is 3.969777750968933 and perplexity is 52.97275638751426
At time: 77.06588840484619 and batch: 850, loss is 3.8355615758895873 and perplexity is 46.31943224186256
At time: 77.5785768032074 and batch: 900, loss is 3.785621819496155 and perplexity is 44.06306131239935
At time: 78.09199070930481 and batch: 950, loss is 3.8355048513412475 and perplexity is 46.316804867508296
At time: 78.6108808517456 and batch: 1000, loss is 3.743423161506653 and perplexity is 42.242345318977165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.520441567025533 and perplexity of 91.87615850443048
Finished 7 epochs...
Completing Train Step...
At time: 80.15622425079346 and batch: 50, loss is 3.8780555009841917 and perplexity is 48.330145721317216
At time: 80.67306518554688 and batch: 100, loss is 3.7752290391921997 and perplexity is 43.60749499743714
At time: 81.18721151351929 and batch: 150, loss is 3.873611903190613 and perplexity is 48.11586243938122
At time: 81.69803500175476 and batch: 200, loss is 3.917083053588867 and perplexity is 50.253643591237214
At time: 82.20835280418396 and batch: 250, loss is 3.907002911567688 and perplexity is 49.74962428758223
At time: 82.72657632827759 and batch: 300, loss is 3.7563739681243895 and perplexity is 42.79297562145519
At time: 83.23887920379639 and batch: 350, loss is 3.789791631698608 and perplexity is 44.24717960571321
At time: 83.76320266723633 and batch: 400, loss is 3.800128655433655 and perplexity is 44.70693591354449
At time: 84.2838761806488 and batch: 450, loss is 3.855997910499573 and perplexity is 47.275770397552236
At time: 84.79991221427917 and batch: 500, loss is 3.88319785118103 and perplexity is 48.57931636795842
At time: 85.33802914619446 and batch: 550, loss is 3.79604887008667 and perplexity is 44.52491277164526
At time: 85.85759925842285 and batch: 600, loss is 3.743870530128479 and perplexity is 42.26124744657936
At time: 86.38198733329773 and batch: 650, loss is 3.729024248123169 and perplexity is 41.63845953072882
At time: 86.89478302001953 and batch: 700, loss is 3.81311550617218 and perplexity is 45.29132468865031
At time: 87.41040301322937 and batch: 750, loss is 3.7790487957000733 and perplexity is 43.77438354413573
At time: 87.9255359172821 and batch: 800, loss is 3.904895238876343 and perplexity is 49.64487878645853
At time: 88.4357533454895 and batch: 850, loss is 3.7699338722229006 and perplexity is 43.377196303439476
At time: 88.94784212112427 and batch: 900, loss is 3.717328877449036 and perplexity is 41.154318932545856
At time: 89.4602255821228 and batch: 950, loss is 3.770611939430237 and perplexity is 43.406618931931476
At time: 89.9759430885315 and batch: 1000, loss is 3.6777718925476073 and perplexity is 39.558155984953856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.531615094440739 and perplexity of 92.90849596535215
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 91.47573137283325 and batch: 50, loss is 3.8403881549835206 and perplexity is 46.54353703987101
At time: 91.99740648269653 and batch: 100, loss is 3.737132692337036 and perplexity is 41.977455163249566
At time: 92.52395415306091 and batch: 150, loss is 3.8292904138565063 and perplexity is 46.029864489138674
At time: 93.03874015808105 and batch: 200, loss is 3.867804179191589 and perplexity is 47.83722868750681
At time: 93.56325554847717 and batch: 250, loss is 3.844245963096619 and perplexity is 46.72343986684877
At time: 94.08292698860168 and batch: 300, loss is 3.6788235473632813 and perplexity is 39.59977939308721
At time: 94.58914875984192 and batch: 350, loss is 3.707331404685974 and perplexity is 40.744929589188246
At time: 95.10570859909058 and batch: 400, loss is 3.7088279056549074 and perplexity is 40.805950063008666
At time: 95.6254198551178 and batch: 450, loss is 3.757571120262146 and perplexity is 42.84423600081566
At time: 96.14017176628113 and batch: 500, loss is 3.7781523275375366 and perplexity is 43.73515878745344
At time: 96.6541645526886 and batch: 550, loss is 3.668378176689148 and perplexity is 39.18829779884572
At time: 97.18506813049316 and batch: 600, loss is 3.594540710449219 and perplexity is 36.398978477691635
At time: 97.70163154602051 and batch: 650, loss is 3.566067280769348 and perplexity is 35.37719065783769
At time: 98.21699547767639 and batch: 700, loss is 3.6330003213882445 and perplexity is 37.82613710141695
At time: 98.76665902137756 and batch: 750, loss is 3.584866247177124 and perplexity is 36.0485358029698
At time: 99.28202748298645 and batch: 800, loss is 3.685775818824768 and perplexity is 39.876047040321794
At time: 99.79407024383545 and batch: 850, loss is 3.5275562047958373 and perplexity is 34.040677439867075
At time: 100.30827069282532 and batch: 900, loss is 3.4564463567733763 and perplexity is 31.704110992260013
At time: 100.82026505470276 and batch: 950, loss is 3.475644030570984 and perplexity is 32.31863602517482
At time: 101.33620238304138 and batch: 1000, loss is 3.363250937461853 and perplexity is 28.882935033998724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.465578032702934 and perplexity of 86.97128726130305
Finished 9 epochs...
Completing Train Step...
At time: 102.88806819915771 and batch: 50, loss is 3.752822914123535 and perplexity is 42.641284944460274
At time: 103.4266083240509 and batch: 100, loss is 3.644537425041199 and perplexity is 38.26506829464141
At time: 103.94454598426819 and batch: 150, loss is 3.741233410835266 and perplexity is 42.149946317263165
At time: 104.45670890808105 and batch: 200, loss is 3.786266918182373 and perplexity is 44.09149550580706
At time: 104.97861218452454 and batch: 250, loss is 3.7651829385757445 and perplexity is 43.171602889047485
At time: 105.50272679328918 and batch: 300, loss is 3.6010129165649416 and perplexity is 36.63532418282802
At time: 106.02477312088013 and batch: 350, loss is 3.6355409002304078 and perplexity is 37.92235956363743
At time: 106.54129076004028 and batch: 400, loss is 3.6394368171691895 and perplexity is 38.070390096714625
At time: 107.04967188835144 and batch: 450, loss is 3.6929559898376465 and perplexity is 40.16339424390889
At time: 107.55956649780273 and batch: 500, loss is 3.7154782438278198 and perplexity is 41.07822779639515
At time: 108.07442092895508 and batch: 550, loss is 3.611554193496704 and perplexity is 37.02354988291701
At time: 108.58562350273132 and batch: 600, loss is 3.542990345954895 and perplexity is 34.57014146146791
At time: 109.09067440032959 and batch: 650, loss is 3.519127883911133 and perplexity is 33.75497736448852
At time: 109.59522104263306 and batch: 700, loss is 3.5911828517913817 and perplexity is 36.27696082633531
At time: 110.10522222518921 and batch: 750, loss is 3.54960054397583 and perplexity is 34.79941387533597
At time: 110.61942195892334 and batch: 800, loss is 3.6583264827728272 and perplexity is 38.79636213292455
At time: 111.14176774024963 and batch: 850, loss is 3.5047073984146118 and perplexity is 33.27170707373644
At time: 111.68503594398499 and batch: 900, loss is 3.4421802520751954 and perplexity is 31.255027776062665
At time: 112.20368647575378 and batch: 950, loss is 3.4715313482284547 and perplexity is 32.18599268837646
At time: 112.72025418281555 and batch: 1000, loss is 3.3697754383087157 and perplexity is 29.071997867711342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.463845601895961 and perplexity of 86.82074596277289
Finished 10 epochs...
Completing Train Step...
At time: 114.26067996025085 and batch: 50, loss is 3.719163889884949 and perplexity is 41.229906950844956
At time: 114.77638864517212 and batch: 100, loss is 3.6081636142730713 and perplexity is 36.898231175457376
At time: 115.29418683052063 and batch: 150, loss is 3.7054260110855104 and perplexity is 40.66736837688211
At time: 115.81252098083496 and batch: 200, loss is 3.750461902618408 and perplexity is 42.54072713589672
At time: 116.3316080570221 and batch: 250, loss is 3.7295284175872805 and perplexity is 41.65945766342072
At time: 116.84188485145569 and batch: 300, loss is 3.5650723600387573 and perplexity is 35.342010661018165
At time: 117.35928606987 and batch: 350, loss is 3.601131739616394 and perplexity is 36.639677562474176
At time: 117.8850827217102 and batch: 400, loss is 3.6062164068222047 and perplexity is 36.8264525713848
At time: 118.40543746948242 and batch: 450, loss is 3.6601453161239625 and perplexity is 38.866990461383516
At time: 118.93153834342957 and batch: 500, loss is 3.6844707536697388 and perplexity is 39.824040144384504
At time: 119.44769549369812 and batch: 550, loss is 3.5828661346435546 and perplexity is 35.97650673182983
At time: 119.95440721511841 and batch: 600, loss is 3.5169214820861816 and perplexity is 33.68058242357786
At time: 120.47534537315369 and batch: 650, loss is 3.4948284339904787 and perplexity is 32.944635288504344
At time: 120.99776363372803 and batch: 700, loss is 3.569423236846924 and perplexity is 35.49611439563945
At time: 121.52346086502075 and batch: 750, loss is 3.531413960456848 and perplexity is 34.17225168342983
At time: 122.03731274604797 and batch: 800, loss is 3.643385720252991 and perplexity is 38.221023600376505
At time: 122.55172991752625 and batch: 850, loss is 3.492047233581543 and perplexity is 32.8531369519521
At time: 123.06397771835327 and batch: 900, loss is 3.4333347034454347 and perplexity is 30.979779065576743
At time: 123.57383179664612 and batch: 950, loss is 3.46687077999115 and perplexity is 32.03633668506312
At time: 124.08096408843994 and batch: 1000, loss is 3.369768385887146 and perplexity is 29.071792840449472
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.465700103015434 and perplexity of 86.98190452153052
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 125.6511332988739 and batch: 50, loss is 3.7027716493606566 and perplexity is 40.55956560787884
At time: 126.19417238235474 and batch: 100, loss is 3.5959778213500977 and perplexity is 36.451325451636926
At time: 126.7373685836792 and batch: 150, loss is 3.6963769245147704 and perplexity is 40.301025872210985
At time: 127.27370882034302 and batch: 200, loss is 3.741858763694763 and perplexity is 42.17631315014822
At time: 127.79869675636292 and batch: 250, loss is 3.7181212854385377 and perplexity is 41.18694286769687
At time: 128.3154273033142 and batch: 300, loss is 3.55019953250885 and perplexity is 34.82026456924298
At time: 128.85540413856506 and batch: 350, loss is 3.5845909690856934 and perplexity is 36.03861379655324
At time: 129.37986087799072 and batch: 400, loss is 3.5897508573532106 and perplexity is 36.22504959736463
At time: 129.89741563796997 and batch: 450, loss is 3.642263765335083 and perplexity is 38.178165381970814
At time: 130.41028380393982 and batch: 500, loss is 3.6644484615325927 and perplexity is 39.03460114085959
At time: 130.92377257347107 and batch: 550, loss is 3.560355334281921 and perplexity is 35.175694054672924
At time: 131.45563006401062 and batch: 600, loss is 3.4892822027206423 and perplexity is 32.762422486346686
At time: 131.9760546684265 and batch: 650, loss is 3.460875334739685 and perplexity is 31.84483921233727
At time: 132.49378514289856 and batch: 700, loss is 3.532790002822876 and perplexity is 34.21930651678608
At time: 133.013277053833 and batch: 750, loss is 3.491029825210571 and perplexity is 32.81972889310565
At time: 133.54270339012146 and batch: 800, loss is 3.5955995893478394 and perplexity is 36.43754100085093
At time: 134.0565550327301 and batch: 850, loss is 3.44102011680603 and perplexity is 31.218788741156725
At time: 134.58745050430298 and batch: 900, loss is 3.3774695205688476 and perplexity is 29.296542937155564
At time: 135.10711574554443 and batch: 950, loss is 3.405630431175232 and perplexity is 30.133286700385103
At time: 135.62543725967407 and batch: 1000, loss is 3.303610668182373 and perplexity is 27.210710608965076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.45867919921875 and perplexity of 86.37335173290975
Finished 12 epochs...
Completing Train Step...
At time: 137.19637632369995 and batch: 50, loss is 3.6922673273086546 and perplexity is 40.13574474093705
At time: 137.73145508766174 and batch: 100, loss is 3.583988218307495 and perplexity is 36.01689803929443
At time: 138.24489831924438 and batch: 150, loss is 3.683345651626587 and perplexity is 39.77925923172351
At time: 138.80301928520203 and batch: 200, loss is 3.728887529373169 and perplexity is 41.63276716172551
At time: 139.3134949207306 and batch: 250, loss is 3.7059609270095826 and perplexity is 40.689127819034226
At time: 139.8251702785492 and batch: 300, loss is 3.538192534446716 and perplexity is 34.4046776883229
At time: 140.3477554321289 and batch: 350, loss is 3.5732020854949953 and perplexity is 35.63050259595121
At time: 140.86972284317017 and batch: 400, loss is 3.5783193206787107 and perplexity is 35.813299566097484
At time: 141.38534212112427 and batch: 450, loss is 3.631788425445557 and perplexity is 37.780323525586695
At time: 141.91442942619324 and batch: 500, loss is 3.6546854162216187 and perplexity is 38.65535885339377
At time: 142.43669295310974 and batch: 550, loss is 3.551196608543396 and perplexity is 34.85500033478346
At time: 142.95674991607666 and batch: 600, loss is 3.48132719039917 and perplexity is 32.5028309078488
At time: 143.46972179412842 and batch: 650, loss is 3.454867901802063 and perplexity is 31.65410695559551
At time: 143.98532032966614 and batch: 700, loss is 3.5275937604904173 and perplexity is 34.0419558851586
At time: 144.4969892501831 and batch: 750, loss is 3.487714171409607 and perplexity is 32.71109023786681
At time: 145.01269388198853 and batch: 800, loss is 3.593608250617981 and perplexity is 36.365053711562034
At time: 145.53760862350464 and batch: 850, loss is 3.4406113052368164 and perplexity is 31.206028747536717
At time: 146.0597414970398 and batch: 900, loss is 3.3790948820114135 and perplexity is 29.3441991272215
At time: 146.58447313308716 and batch: 950, loss is 3.4090356492996214 and perplexity is 30.23607201829775
At time: 147.10256600379944 and batch: 1000, loss is 3.309098525047302 and perplexity is 27.360449591229077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.457774092511433 and perplexity of 86.29521000156001
Finished 13 epochs...
Completing Train Step...
At time: 148.65959668159485 and batch: 50, loss is 3.686773271560669 and perplexity is 39.91584135571414
At time: 149.1739320755005 and batch: 100, loss is 3.5772614669799805 and perplexity is 35.77543436613977
At time: 149.68666553497314 and batch: 150, loss is 3.6761384868621825 and perplexity is 39.493594210202396
At time: 150.21131229400635 and batch: 200, loss is 3.7214197969436644 and perplexity is 41.32302277980369
At time: 150.7387056350708 and batch: 250, loss is 3.6987153911590576 and perplexity is 40.39537875448089
At time: 151.27281093597412 and batch: 300, loss is 3.5309808969497682 and perplexity is 34.15745613220887
At time: 151.7929186820984 and batch: 350, loss is 3.5663931274414065 and perplexity is 35.38872007598957
At time: 152.30785727500916 and batch: 400, loss is 3.5715090322494505 and perplexity is 35.570229295241866
At time: 152.83022737503052 and batch: 450, loss is 3.6254477453231813 and perplexity is 37.54152844086167
At time: 153.3419587612152 and batch: 500, loss is 3.648779134750366 and perplexity is 38.42772232807125
At time: 153.85615348815918 and batch: 550, loss is 3.5457154655456544 and perplexity is 34.664477711621004
At time: 154.37350392341614 and batch: 600, loss is 3.4765600633621214 and perplexity is 32.34825451922349
At time: 154.90945076942444 and batch: 650, loss is 3.4511751985549926 and perplexity is 31.537433285258825
At time: 155.42644381523132 and batch: 700, loss is 3.5245418787002563 and perplexity is 33.93822223182947
At time: 155.94177842140198 and batch: 750, loss is 3.4857660579681395 and perplexity is 32.647427354682755
At time: 156.45270037651062 and batch: 800, loss is 3.592618713378906 and perplexity is 36.32908693488076
At time: 156.96216249465942 and batch: 850, loss is 3.4406658744812013 and perplexity is 31.207731683409264
At time: 157.47762608528137 and batch: 900, loss is 3.3801811122894287 and perplexity is 29.37609100262218
At time: 158.0030059814453 and batch: 950, loss is 3.4111201572418213 and perplexity is 30.29916508671607
At time: 158.5129578113556 and batch: 1000, loss is 3.3123570013046266 and perplexity is 27.449748376585713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.457723477991616 and perplexity of 86.29084232147821
Finished 14 epochs...
Completing Train Step...
At time: 160.06435585021973 and batch: 50, loss is 3.682191586494446 and perplexity is 39.733377855802175
At time: 160.58801674842834 and batch: 100, loss is 3.571915225982666 and perplexity is 35.58468063429361
At time: 161.0998830795288 and batch: 150, loss is 3.6705988073349 and perplexity is 39.27541722812966
At time: 161.60707664489746 and batch: 200, loss is 3.715701684951782 and perplexity is 41.08740738729522
At time: 162.1380364894867 and batch: 250, loss is 3.6931229925155638 and perplexity is 40.17010219840947
At time: 162.6518452167511 and batch: 300, loss is 3.525408596992493 and perplexity is 33.96764986073375
At time: 163.17329359054565 and batch: 350, loss is 3.5611509704589843 and perplexity is 35.20369224612614
At time: 163.69121551513672 and batch: 400, loss is 3.566343388557434 and perplexity is 35.386959924322134
At time: 164.2005114555359 and batch: 450, loss is 3.6205541706085205 and perplexity is 37.35826493907156
At time: 164.70829916000366 and batch: 500, loss is 3.644264259338379 and perplexity is 38.254617017897424
At time: 165.2450578212738 and batch: 550, loss is 3.54151584148407 and perplexity is 34.519205195430565
At time: 165.76074767112732 and batch: 600, loss is 3.472890543937683 and perplexity is 32.22976949542041
At time: 166.27120447158813 and batch: 650, loss is 3.448212938308716 and perplexity is 31.444149434103945
At time: 166.78513622283936 and batch: 700, loss is 3.5221295738220215 and perplexity is 33.85645156032931
At time: 167.29735016822815 and batch: 750, loss is 3.4841660928726195 and perplexity is 32.59523437506356
At time: 167.81037092208862 and batch: 800, loss is 3.591731376647949 and perplexity is 36.29686509956452
At time: 168.3301088809967 and batch: 850, loss is 3.440499634742737 and perplexity is 31.202544149455242
At time: 168.84490656852722 and batch: 900, loss is 3.380662441253662 and perplexity is 29.390233969514448
At time: 169.38394689559937 and batch: 950, loss is 3.4123121166229247 and perplexity is 30.335301993364713
At time: 169.89643955230713 and batch: 1000, loss is 3.314365668296814 and perplexity is 27.504941193531007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.457969851610137 and perplexity of 86.31210472768666
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 171.40454363822937 and batch: 50, loss is 3.6793199014663696 and perplexity is 39.61943978492479
At time: 171.94045853614807 and batch: 100, loss is 3.569551696777344 and perplexity is 35.50067451691506
At time: 172.46204495429993 and batch: 150, loss is 3.6687626361846926 and perplexity is 39.20336700861349
At time: 172.98491215705872 and batch: 200, loss is 3.7135922956466674 and perplexity is 41.000829395025605
At time: 173.50786352157593 and batch: 250, loss is 3.6906174707412718 and perplexity is 40.06958111414477
At time: 174.02495884895325 and batch: 300, loss is 3.5222021627426146 and perplexity is 33.85890925280273
At time: 174.53766751289368 and batch: 350, loss is 3.557618565559387 and perplexity is 35.079557926569095
At time: 175.05070686340332 and batch: 400, loss is 3.5620819759368896 and perplexity is 35.236482337958876
At time: 175.56519770622253 and batch: 450, loss is 3.6163888645172118 and perplexity is 37.202979959808346
At time: 176.07846641540527 and batch: 500, loss is 3.640096092224121 and perplexity is 38.09549723058104
At time: 176.6037836074829 and batch: 550, loss is 3.5370073413848875 and perplexity is 34.36392565731475
At time: 177.1207242012024 and batch: 600, loss is 3.4676326227188112 and perplexity is 32.0607526345634
At time: 177.64044284820557 and batch: 650, loss is 3.442286744117737 and perplexity is 31.258356365041205
At time: 178.18966674804688 and batch: 700, loss is 3.516088147163391 and perplexity is 33.6525269094678
At time: 178.7070152759552 and batch: 750, loss is 3.476664743423462 and perplexity is 32.35164091373172
At time: 179.22362184524536 and batch: 800, loss is 3.583816661834717 and perplexity is 36.01071963729398
At time: 179.73496198654175 and batch: 850, loss is 3.4319807052612306 and perplexity is 30.93786088594855
At time: 180.25133299827576 and batch: 900, loss is 3.3710914945602415 and perplexity is 29.110283439706773
At time: 180.7665228843689 and batch: 950, loss is 3.402031202316284 and perplexity is 30.025025051376314
At time: 181.28708052635193 and batch: 1000, loss is 3.3033383512496948 and perplexity is 27.203301680550165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.457515065262958 and perplexity of 86.27286008550031
Finished 16 epochs...
Completing Train Step...
At time: 182.91758728027344 and batch: 50, loss is 3.678448758125305 and perplexity is 39.584940602827594
At time: 183.42672324180603 and batch: 100, loss is 3.5686138582229616 and perplexity is 35.46739622291922
At time: 183.95063638687134 and batch: 150, loss is 3.6675955295562743 and perplexity is 39.157639188932784
At time: 184.47091460227966 and batch: 200, loss is 3.712531576156616 and perplexity is 40.95736207357107
At time: 184.98834371566772 and batch: 250, loss is 3.6896076726913454 and perplexity is 40.02913935171727
At time: 185.50593948364258 and batch: 300, loss is 3.521193242073059 and perplexity is 33.8247655264609
At time: 186.03657937049866 and batch: 350, loss is 3.556746392250061 and perplexity is 35.04897581083374
At time: 186.558687210083 and batch: 400, loss is 3.5611680269241335 and perplexity is 35.204292701796874
At time: 187.08763098716736 and batch: 450, loss is 3.615545654296875 and perplexity is 37.1716232488867
At time: 187.60335063934326 and batch: 500, loss is 3.6393457746505735 and perplexity is 38.06692423028852
At time: 188.12086963653564 and batch: 550, loss is 3.5362585496902468 and perplexity is 34.33820386652079
At time: 188.6300323009491 and batch: 600, loss is 3.467112684249878 and perplexity is 32.04408734876276
At time: 189.1467764377594 and batch: 650, loss is 3.4418425798416137 and perplexity is 31.244475602711102
At time: 189.654310464859 and batch: 700, loss is 3.515660219192505 and perplexity is 33.638129132737646
At time: 190.1657464504242 and batch: 750, loss is 3.4765656852722167 and perplexity is 32.34843637871333
At time: 190.67105984687805 and batch: 800, loss is 3.58373507976532 and perplexity is 36.00778192809933
At time: 191.18749499320984 and batch: 850, loss is 3.4321284770965574 and perplexity is 30.942432968236925
At time: 191.72620010375977 and batch: 900, loss is 3.3713807010650636 and perplexity is 29.11870354055003
At time: 192.24150896072388 and batch: 950, loss is 3.4023611545562744 and perplexity is 30.03493351021698
At time: 192.7536916732788 and batch: 1000, loss is 3.303919439315796 and perplexity is 27.21911378818387
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4573755031678735 and perplexity of 86.26082050455199
Finished 17 epochs...
Completing Train Step...
At time: 194.3356602191925 and batch: 50, loss is 3.6777210664749145 and perplexity is 39.556145450336395
At time: 194.85830879211426 and batch: 100, loss is 3.567807397842407 and perplexity is 35.438804703577404
At time: 195.39313554763794 and batch: 150, loss is 3.66670147895813 and perplexity is 39.12264592339991
At time: 195.91243815422058 and batch: 200, loss is 3.7116001510620116 and perplexity is 40.9192311195474
At time: 196.42653965950012 and batch: 250, loss is 3.6887479496002196 and perplexity is 39.99474016530491
At time: 196.93977999687195 and batch: 300, loss is 3.5203478956222534 and perplexity is 33.7961839633373
At time: 197.4492359161377 and batch: 350, loss is 3.5559631538391114 and perplexity is 35.0215348545226
At time: 197.9592261314392 and batch: 400, loss is 3.5603970861434937 and perplexity is 35.17716273604169
At time: 198.47255158424377 and batch: 450, loss is 3.614823031425476 and perplexity is 37.144771886632725
At time: 198.99700570106506 and batch: 500, loss is 3.638694019317627 and perplexity is 38.042121992785816
At time: 199.52339029312134 and batch: 550, loss is 3.5356403255462645 and perplexity is 34.31698172052697
At time: 200.05191278457642 and batch: 600, loss is 3.4666478776931764 and perplexity is 32.02919650780789
At time: 200.57845759391785 and batch: 650, loss is 3.4414721298217774 and perplexity is 31.232903229729548
At time: 201.09303975105286 and batch: 700, loss is 3.5153130960464476 and perplexity is 33.626454585895544
At time: 201.61556720733643 and batch: 750, loss is 3.476476106643677 and perplexity is 32.34553877993052
At time: 202.13277864456177 and batch: 800, loss is 3.5836731672286986 and perplexity is 36.00555266399246
At time: 202.66535663604736 and batch: 850, loss is 3.4322511339187622 and perplexity is 30.946228501505363
At time: 203.1818425655365 and batch: 900, loss is 3.371613883972168 and perplexity is 29.125494316208357
At time: 203.69483852386475 and batch: 950, loss is 3.402657699584961 and perplexity is 30.04384154118724
At time: 204.21020650863647 and batch: 1000, loss is 3.304401240348816 and perplexity is 27.232231145049205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4573207948266 and perplexity of 86.2561014472324
Finished 18 epochs...
Completing Train Step...
At time: 205.76522421836853 and batch: 50, loss is 3.677047653198242 and perplexity is 39.52951678387124
At time: 206.3017020225525 and batch: 100, loss is 3.567047438621521 and perplexity is 35.41188288820235
At time: 206.81845617294312 and batch: 150, loss is 3.665891695022583 and perplexity is 39.090977857091254
At time: 207.33262467384338 and batch: 200, loss is 3.7107537984848022 and perplexity is 40.8846136741804
At time: 207.8500599861145 and batch: 250, loss is 3.6879530429840086 and perplexity is 39.96296071425521
At time: 208.3748562335968 and batch: 300, loss is 3.5195704460144044 and perplexity is 33.76991934439998
At time: 208.8938775062561 and batch: 350, loss is 3.5552363634109496 and perplexity is 34.99609078558369
At time: 209.4147207736969 and batch: 400, loss is 3.559692406654358 and perplexity is 35.15238284294223
At time: 209.93857526779175 and batch: 450, loss is 3.614162950515747 and perplexity is 37.120261422148914
At time: 210.45376658439636 and batch: 500, loss is 3.6380965328216552 and perplexity is 38.01939912759555
At time: 210.97990083694458 and batch: 550, loss is 3.5350809335708617 and perplexity is 34.297790444555794
At time: 211.4948229789734 and batch: 600, loss is 3.466203923225403 and perplexity is 32.014980158858364
At time: 212.00655913352966 and batch: 650, loss is 3.4411305570602417 and perplexity is 31.222236742517044
At time: 212.52703499794006 and batch: 700, loss is 3.515000276565552 and perplexity is 33.61593722093171
At time: 213.03972458839417 and batch: 750, loss is 3.4763761615753173 and perplexity is 32.34230616439071
At time: 213.57835698127747 and batch: 800, loss is 3.583616380691528 and perplexity is 36.003508091390415
At time: 214.09662675857544 and batch: 850, loss is 3.4323473262786863 and perplexity is 30.94920543543253
At time: 214.61449480056763 and batch: 900, loss is 3.3718078422546385 and perplexity is 29.13114399494532
At time: 215.13303875923157 and batch: 950, loss is 3.402922339439392 and perplexity is 30.051793391181047
At time: 215.65702986717224 and batch: 1000, loss is 3.3048163843154907 and perplexity is 27.243538788495744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.457304047375191 and perplexity of 86.25465688946099
Finished 19 epochs...
Completing Train Step...
At time: 217.19945359230042 and batch: 50, loss is 3.6764077091217042 and perplexity is 39.50422819626095
At time: 217.71708750724792 and batch: 100, loss is 3.5663238334655762 and perplexity is 35.386267935836216
At time: 218.24296021461487 and batch: 150, loss is 3.6651337337493897 and perplexity is 39.06135963589464
At time: 218.76563262939453 and batch: 200, loss is 3.709964756965637 and perplexity is 40.852366740254354
At time: 219.2819139957428 and batch: 250, loss is 3.687202868461609 and perplexity is 39.93299276129052
At time: 219.80051112174988 and batch: 300, loss is 3.5188366413116454 and perplexity is 33.74514790858287
At time: 220.31710958480835 and batch: 350, loss is 3.554549617767334 and perplexity is 34.972065623225134
At time: 220.82479596138 and batch: 400, loss is 3.5590301370620727 and perplexity is 35.12911019592283
At time: 221.33822989463806 and batch: 450, loss is 3.613542938232422 and perplexity is 37.09725353742956
At time: 221.84986758232117 and batch: 500, loss is 3.637534465789795 and perplexity is 37.99803568118167
At time: 222.35783910751343 and batch: 550, loss is 3.534555878639221 and perplexity is 34.2797869473646
At time: 222.87116408348083 and batch: 600, loss is 3.465774292945862 and perplexity is 32.00122850825748
At time: 223.39414310455322 and batch: 650, loss is 3.44080370426178 and perplexity is 31.212033334660557
At time: 223.90546441078186 and batch: 700, loss is 3.514706783294678 and perplexity is 33.60607261723146
At time: 224.4199936389923 and batch: 750, loss is 3.476265196800232 and perplexity is 32.33871750677241
At time: 224.94889998435974 and batch: 800, loss is 3.5835586738586427 and perplexity is 36.001430502911795
At time: 225.47680616378784 and batch: 850, loss is 3.432421817779541 and perplexity is 30.951510974066128
At time: 225.99113202095032 and batch: 900, loss is 3.3719719743728636 and perplexity is 29.135925743724073
At time: 226.51967692375183 and batch: 950, loss is 3.4031595468521116 and perplexity is 30.0589227448733
At time: 227.0282564163208 and batch: 1000, loss is 3.3051827144622803 and perplexity is 27.253520746293603
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.457309257693407 and perplexity of 86.25510630484179
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 228.55271863937378 and batch: 50, loss is 3.6759584617614744 and perplexity is 39.48648501186365
At time: 229.08731818199158 and batch: 100, loss is 3.5659593534469605 and perplexity is 35.37337269841116
At time: 229.59904766082764 and batch: 150, loss is 3.664852123260498 and perplexity is 39.05036109603558
At time: 230.11976170539856 and batch: 200, loss is 3.7095508861541746 and perplexity is 40.83546263638029
At time: 230.63057374954224 and batch: 250, loss is 3.6868501710891723 and perplexity is 39.91891098311914
At time: 231.18217706680298 and batch: 300, loss is 3.5182744121551512 and perplexity is 33.72618073498709
At time: 231.69503927230835 and batch: 350, loss is 3.5540026092529295 and perplexity is 34.952940836750315
At time: 232.20688939094543 and batch: 400, loss is 3.558331251144409 and perplexity is 35.10456753276665
At time: 232.72721314430237 and batch: 450, loss is 3.612910199165344 and perplexity is 37.07378808037373
At time: 233.25016117095947 and batch: 500, loss is 3.6369343614578247 and perplexity is 37.97523973601987
At time: 233.7604057788849 and batch: 550, loss is 3.5338174057006837 and perplexity is 34.25448159717331
At time: 234.27536487579346 and batch: 600, loss is 3.465017485618591 and perplexity is 31.977018906198403
At time: 234.79025411605835 and batch: 650, loss is 3.439963598251343 and perplexity is 31.185822929159308
At time: 235.30788159370422 and batch: 700, loss is 3.513892993927002 and perplexity is 33.57873547749171
At time: 235.81564140319824 and batch: 750, loss is 3.4752416467666625 and perplexity is 32.30563414550886
At time: 236.32938933372498 and batch: 800, loss is 3.5825650548934935 and perplexity is 35.96567656462877
At time: 236.8503143787384 and batch: 850, loss is 3.4312171125411988 and perplexity is 30.914245977836075
At time: 237.37075185775757 and batch: 900, loss is 3.370654525756836 and perplexity is 29.09756593276458
At time: 237.9041497707367 and batch: 950, loss is 3.4017912483215333 and perplexity is 30.01782129099285
At time: 238.4168553352356 and batch: 1000, loss is 3.3037330961227416 and perplexity is 27.214042164154456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.457296976229039 and perplexity of 86.2540469723323
Finished 21 epochs...
Completing Train Step...
At time: 239.92969346046448 and batch: 50, loss is 3.675871877670288 and perplexity is 39.483066258451714
At time: 240.4691174030304 and batch: 100, loss is 3.5658532857894896 and perplexity is 35.36962092660654
At time: 240.98585391044617 and batch: 150, loss is 3.6647381830215453 and perplexity is 39.045911942034756
At time: 241.505624294281 and batch: 200, loss is 3.7094521284103394 and perplexity is 40.83143001735131
At time: 242.03035473823547 and batch: 250, loss is 3.6867430210113525 and perplexity is 39.914633897849924
At time: 242.5540006160736 and batch: 300, loss is 3.518179483413696 and perplexity is 33.722979303052185
At time: 243.07147455215454 and batch: 350, loss is 3.5539163827896116 and perplexity is 34.94992709821327
At time: 243.59641408920288 and batch: 400, loss is 3.558244228363037 and perplexity is 35.10151276858005
At time: 244.11058354377747 and batch: 450, loss is 3.612825746536255 and perplexity is 37.07065723370603
At time: 244.66597509384155 and batch: 500, loss is 3.6368593883514406 and perplexity is 37.9723927210573
At time: 245.19074964523315 and batch: 550, loss is 3.5337513637542726 and perplexity is 34.25221943923481
At time: 245.70378422737122 and batch: 600, loss is 3.4649644184112547 and perplexity is 31.975322020131006
At time: 246.21346640586853 and batch: 650, loss is 3.439927167892456 and perplexity is 31.184686839132024
At time: 246.75384879112244 and batch: 700, loss is 3.513850030899048 and perplexity is 33.577292864330474
At time: 247.2721652984619 and batch: 750, loss is 3.4752384710311888 and perplexity is 32.30553155152341
At time: 247.78657054901123 and batch: 800, loss is 3.582557406425476 and perplexity is 35.96540148335381
At time: 248.29514265060425 and batch: 850, loss is 3.4312297201156614 and perplexity is 30.914635733951133
At time: 248.81301498413086 and batch: 900, loss is 3.3706847476959227 and perplexity is 29.098445330918267
At time: 249.32867670059204 and batch: 950, loss is 3.4018232154846193 and perplexity is 30.018780890919306
At time: 249.84965538978577 and batch: 1000, loss is 3.3037843561172484 and perplexity is 27.215437191560543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.457287672089367 and perplexity of 86.25324445636535
Finished 22 epochs...
Completing Train Step...
At time: 251.46919441223145 and batch: 50, loss is 3.675786504745483 and perplexity is 39.47969561748776
At time: 252.00643110275269 and batch: 100, loss is 3.5657527112960814 and perplexity is 35.36606382377971
At time: 252.54122281074524 and batch: 150, loss is 3.6646304178237914 and perplexity is 39.04170437833138
At time: 253.0606415271759 and batch: 200, loss is 3.709352369308472 and perplexity is 40.827356913732764
At time: 253.5740294456482 and batch: 250, loss is 3.6866400623321534 and perplexity is 39.910524551413154
At time: 254.09105610847473 and batch: 300, loss is 3.518085103034973 and perplexity is 33.71979666568552
At time: 254.6078667640686 and batch: 350, loss is 3.553829894065857 and perplexity is 34.946904454337414
At time: 255.13285040855408 and batch: 400, loss is 3.5581590509414673 and perplexity is 35.09852303955974
At time: 255.64762902259827 and batch: 450, loss is 3.612743763923645 and perplexity is 37.06761820895013
At time: 256.1684648990631 and batch: 500, loss is 3.636785640716553 and perplexity is 37.96959245016104
At time: 256.6887192726135 and batch: 550, loss is 3.533685870170593 and perplexity is 34.24997621209405
At time: 257.2065827846527 and batch: 600, loss is 3.4649114561080934 and perplexity is 31.973628578277182
At time: 257.74372959136963 and batch: 650, loss is 3.4398895740509032 and perplexity is 31.18351450899246
At time: 258.25933170318604 and batch: 700, loss is 3.513809094429016 and perplexity is 33.575918356621315
At time: 258.7775573730469 and batch: 750, loss is 3.4752332401275634 and perplexity is 32.30536256484328
At time: 259.2871654033661 and batch: 800, loss is 3.5825507116317747 and perplexity is 35.96516070321649
At time: 259.815970659256 and batch: 850, loss is 3.4312425327301024 and perplexity is 30.915031833796906
At time: 260.35243344306946 and batch: 900, loss is 3.370712676048279 and perplexity is 29.09925801390088
At time: 260.8615446090698 and batch: 950, loss is 3.4018546676635744 and perplexity is 30.01972506183594
At time: 261.3860342502594 and batch: 1000, loss is 3.3038348150253296 and perplexity is 27.216810487451397
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.457278740115282 and perplexity of 86.2524740480617
Finished 23 epochs...
Completing Train Step...
At time: 262.8946373462677 and batch: 50, loss is 3.675701880455017 and perplexity is 39.47635481761671
At time: 263.43252873420715 and batch: 100, loss is 3.5656556940078734 and perplexity is 35.36263287060652
At time: 263.95244121551514 and batch: 150, loss is 3.6645269060134886 and perplexity is 39.037663309986634
At time: 264.47271180152893 and batch: 200, loss is 3.709252338409424 and perplexity is 40.82327312077108
At time: 264.9924998283386 and batch: 250, loss is 3.686539959907532 and perplexity is 39.90652961109258
At time: 265.5044012069702 and batch: 300, loss is 3.517991347312927 and perplexity is 33.716635389998046
At time: 266.0131504535675 and batch: 350, loss is 3.5537435388565064 and perplexity is 34.94388673738678
At time: 266.52234053611755 and batch: 400, loss is 3.5580751943588256 and perplexity is 35.095579920763534
At time: 267.0383059978485 and batch: 450, loss is 3.6126637411117555 and perplexity is 37.064652072591876
At time: 267.5485026836395 and batch: 500, loss is 3.6367128038406373 and perplexity is 37.966826964383074
At time: 268.0652594566345 and batch: 550, loss is 3.533620948791504 and perplexity is 34.24775272858118
At time: 268.57143092155457 and batch: 600, loss is 3.464858822822571 and perplexity is 31.971945745441936
At time: 269.0861072540283 and batch: 650, loss is 3.43985134601593 and perplexity is 31.182322447294457
At time: 269.6077184677124 and batch: 700, loss is 3.513769659996033 and perplexity is 33.574594335425225
At time: 270.11338448524475 and batch: 750, loss is 3.4752263975143434 and perplexity is 32.3051415124986
At time: 270.62147760391235 and batch: 800, loss is 3.5825445461273193 and perplexity is 35.96493896054151
At time: 271.1765675544739 and batch: 850, loss is 3.431255130767822 and perplexity is 30.915421304987337
At time: 271.6920886039734 and batch: 900, loss is 3.37073869228363 and perplexity is 29.100015076893843
At time: 272.2080535888672 and batch: 950, loss is 3.4018852376937865 and perplexity is 30.020642779765296
At time: 272.7420392036438 and batch: 1000, loss is 3.3038844585418703 and perplexity is 27.218161659171194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.457273901962653 and perplexity of 86.25205674643713
Finished 24 epochs...
Completing Train Step...
At time: 274.25258898735046 and batch: 50, loss is 3.6756181478500367 and perplexity is 39.47304949797615
At time: 274.7870020866394 and batch: 100, loss is 3.565561146736145 and perplexity is 35.359289588199076
At time: 275.3167531490326 and batch: 150, loss is 3.6644263792037965 and perplexity is 39.03373917547991
At time: 275.8314836025238 and batch: 200, loss is 3.7091524696350096 and perplexity is 40.81919635409116
At time: 276.3484466075897 and batch: 250, loss is 3.6864418506622316 and perplexity is 39.902614603642235
At time: 276.87555742263794 and batch: 300, loss is 3.517898244857788 and perplexity is 33.7134964345887
At time: 277.39417695999146 and batch: 350, loss is 3.5536574840545656 and perplexity is 34.94087977751808
At time: 277.90691661834717 and batch: 400, loss is 3.557992248535156 and perplexity is 35.092669009705475
At time: 278.42359733581543 and batch: 450, loss is 3.612584767341614 and perplexity is 37.06172505285913
At time: 278.9359633922577 and batch: 500, loss is 3.636640911102295 and perplexity is 37.964097523341096
At time: 279.46062779426575 and batch: 550, loss is 3.5335564756393434 and perplexity is 34.24554473918713
At time: 279.9769997596741 and batch: 600, loss is 3.4648061037063598 and perplexity is 31.9702602571478
At time: 280.5067768096924 and batch: 650, loss is 3.4398126888275145 and perplexity is 31.181117049679166
At time: 281.02735114097595 and batch: 700, loss is 3.513731198310852 and perplexity is 33.57330302478097
At time: 281.5492014884949 and batch: 750, loss is 3.475218300819397 and perplexity is 32.304879948681474
At time: 282.0619652271271 and batch: 800, loss is 3.5825387144088747 and perplexity is 35.96472922375518
At time: 282.5727016925812 and batch: 850, loss is 3.4312673139572145 and perplexity is 30.91579795571464
At time: 283.09482741355896 and batch: 900, loss is 3.370763411521912 and perplexity is 29.100734415991248
At time: 283.6174371242523 and batch: 950, loss is 3.401915283203125 and perplexity is 30.021544778818736
At time: 284.15110969543457 and batch: 1000, loss is 3.303932981491089 and perplexity is 27.21948239668999
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.457270180306783 and perplexity of 86.2517357465612
Finished 25 epochs...
Completing Train Step...
At time: 285.69783639907837 and batch: 50, loss is 3.675535202026367 and perplexity is 39.46977550915652
At time: 286.22358536720276 and batch: 100, loss is 3.5654684829711916 and perplexity is 35.35601321510263
At time: 286.74779438972473 and batch: 150, loss is 3.6643281173706055 and perplexity is 39.029903837149
At time: 287.259464263916 and batch: 200, loss is 3.7090527057647704 and perplexity is 40.81512427620933
At time: 287.78276228904724 and batch: 250, loss is 3.6863455247879027 and perplexity is 39.89877113451826
At time: 288.29689025878906 and batch: 300, loss is 3.517805733680725 and perplexity is 33.71037770361159
At time: 288.8214647769928 and batch: 350, loss is 3.553571791648865 and perplexity is 34.93788573775771
At time: 289.3334140777588 and batch: 400, loss is 3.5579102993011475 and perplexity is 35.08979331019311
At time: 289.84615993499756 and batch: 450, loss is 3.6125069427490235 and perplexity is 37.05884085143855
At time: 290.37588906288147 and batch: 500, loss is 3.6365696716308595 and perplexity is 37.96139307743281
At time: 290.9013352394104 and batch: 550, loss is 3.533492422103882 and perplexity is 34.243351261223545
At time: 291.42558121681213 and batch: 600, loss is 3.4647536087036133 and perplexity is 31.96858202229765
At time: 291.9494047164917 and batch: 650, loss is 3.4397736740112306 and perplexity is 31.1799005478569
At time: 292.4723310470581 and batch: 700, loss is 3.5136936283111573 and perplexity is 33.57204169949072
At time: 292.9874076843262 and batch: 750, loss is 3.4752093076705934 and perplexity is 32.30458942739537
At time: 293.5005531311035 and batch: 800, loss is 3.582532539367676 and perplexity is 35.964507140756204
At time: 294.0141625404358 and batch: 850, loss is 3.431279182434082 and perplexity is 30.91616488132494
At time: 294.53025817871094 and batch: 900, loss is 3.370787091255188 and perplexity is 29.101423521779243
At time: 295.04118752479553 and batch: 950, loss is 3.40194456577301 and perplexity is 30.022423899673168
At time: 295.56084871292114 and batch: 1000, loss is 3.3039805841445924 and perplexity is 27.220778147119397
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.457267202982089 and perplexity of 86.25147894752072
Finished 26 epochs...
Completing Train Step...
At time: 297.1133232116699 and batch: 50, loss is 3.6754530000686647 and perplexity is 39.46653114968776
At time: 297.6388168334961 and batch: 100, loss is 3.5653770303726198 and perplexity is 35.352779963665846
At time: 298.15461134910583 and batch: 150, loss is 3.6642314434051513 and perplexity is 39.02613084395181
At time: 298.66183733940125 and batch: 200, loss is 3.7089537143707276 and perplexity is 40.81108413013234
At time: 299.173495054245 and batch: 250, loss is 3.6862505865097046 and perplexity is 39.89498339368818
At time: 299.6827416419983 and batch: 300, loss is 3.5177138566970827 and perplexity is 33.70728063806744
At time: 300.1936206817627 and batch: 350, loss is 3.5534866189956666 and perplexity is 34.93491011205505
At time: 300.71000027656555 and batch: 400, loss is 3.557829008102417 and perplexity is 35.08694093476979
At time: 301.2176721096039 and batch: 450, loss is 3.612429995536804 and perplexity is 37.055989386654495
At time: 301.72753953933716 and batch: 500, loss is 3.636499056816101 and perplexity is 37.95871253533677
At time: 302.24367570877075 and batch: 550, loss is 3.5334287595748903 and perplexity is 34.24117131227238
At time: 302.754741191864 and batch: 600, loss is 3.4647011518478394 and perplexity is 31.966905094984764
At time: 303.26880979537964 and batch: 650, loss is 3.4397347259521482 and perplexity is 31.178686174897067
At time: 303.7787826061249 and batch: 700, loss is 3.513656663894653 and perplexity is 33.57080075149405
At time: 304.2976167201996 and batch: 750, loss is 3.4751994609832764 and perplexity is 32.30427133577045
At time: 304.8061192035675 and batch: 800, loss is 3.5825264692306518 and perplexity is 35.964288831932436
At time: 305.337361574173 and batch: 850, loss is 3.4312904691696167 and perplexity is 30.91651382587092
At time: 305.8587806224823 and batch: 900, loss is 3.3708097648620607 and perplexity is 29.102083363496085
At time: 306.37114334106445 and batch: 950, loss is 3.401973385810852 and perplexity is 30.023289159534418
At time: 306.8904013633728 and batch: 1000, loss is 3.3040272617340087 and perplexity is 27.222048777080083
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4572642256573936 and perplexity of 86.25122214924474
Finished 27 epochs...
Completing Train Step...
At time: 308.406742811203 and batch: 50, loss is 3.6753714275360108 and perplexity is 39.463311896089934
At time: 308.94857835769653 and batch: 100, loss is 3.565286626815796 and perplexity is 35.349584091074824
At time: 309.4683177471161 and batch: 150, loss is 3.6641361236572267 and perplexity is 39.02241106028453
At time: 309.98716139793396 and batch: 200, loss is 3.7088553380966185 and perplexity is 40.80706948520941
At time: 310.51593470573425 and batch: 250, loss is 3.6861567878723145 and perplexity is 39.89124147410337
At time: 311.06114625930786 and batch: 300, loss is 3.517622594833374 and perplexity is 33.70420458918096
At time: 311.5853395462036 and batch: 350, loss is 3.553401951789856 and perplexity is 34.93195239604316
At time: 312.1039328575134 and batch: 400, loss is 3.557748408317566 and perplexity is 35.08411304884441
At time: 312.6230103969574 and batch: 450, loss is 3.612353677749634 and perplexity is 37.05316146345489
At time: 313.1384539604187 and batch: 500, loss is 3.6364290952682494 and perplexity is 37.95605697794789
At time: 313.6544156074524 and batch: 550, loss is 3.533365459442139 and perplexity is 34.239003910181886
At time: 314.16440081596375 and batch: 600, loss is 3.464648756980896 and perplexity is 31.965230237123084
At time: 314.6802291870117 and batch: 650, loss is 3.4396956968307495 and perplexity is 31.1774693219158
At time: 315.1983063220978 and batch: 700, loss is 3.5136200618743896 and perplexity is 33.569572014851936
At time: 315.72447896003723 and batch: 750, loss is 3.475188970565796 and perplexity is 32.30393245225525
At time: 316.2353551387787 and batch: 800, loss is 3.5825202322006224 and perplexity is 35.96406452228252
At time: 316.74291253089905 and batch: 850, loss is 3.431301517486572 and perplexity is 30.91685540320175
At time: 317.26422023773193 and batch: 900, loss is 3.3708317613601686 and perplexity is 29.102723514458237
At time: 317.7830584049225 and batch: 950, loss is 3.4020015478134153 and perplexity is 30.024134687386507
At time: 318.29500675201416 and batch: 1000, loss is 3.3040729188919067 and perplexity is 27.22329168683305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4572627369950455 and perplexity of 86.25109375039342
Finished 28 epochs...
Completing Train Step...
At time: 319.79222989082336 and batch: 50, loss is 3.6752905654907226 and perplexity is 39.460120940991494
At time: 320.30527997016907 and batch: 100, loss is 3.565197033882141 and perplexity is 35.34641716000206
At time: 320.8261544704437 and batch: 150, loss is 3.6640419149398804 and perplexity is 39.01873498215281
At time: 321.34912037849426 and batch: 200, loss is 3.7087577772140503 and perplexity is 40.80308850569251
At time: 321.8612575531006 and batch: 250, loss is 3.6860639810562135 and perplexity is 39.887539466780254
At time: 322.37682056427 and batch: 300, loss is 3.5175318908691406 and perplexity is 33.701147622854975
At time: 322.89160418510437 and batch: 350, loss is 3.5533177709579467 and perplexity is 34.92901191899788
At time: 323.4025807380676 and batch: 400, loss is 3.5576684045791627 and perplexity is 35.08130630091859
At time: 323.9319381713867 and batch: 450, loss is 3.612278046607971 and perplexity is 37.05035919652185
At time: 324.43722677230835 and batch: 500, loss is 3.63635968208313 and perplexity is 37.95342241857612
At time: 324.95402908325195 and batch: 550, loss is 3.5333023118972777 and perplexity is 34.23684186941097
At time: 325.4713888168335 and batch: 600, loss is 3.4645965623855592 and perplexity is 31.963561868406302
At time: 325.98226261138916 and batch: 650, loss is 3.439656615257263 and perplexity is 31.176250881166826
At time: 326.49203157424927 and batch: 700, loss is 3.513583974838257 and perplexity is 33.56836061035181
At time: 327.011700630188 and batch: 750, loss is 3.475177960395813 and perplexity is 32.303576782425836
At time: 327.5257742404938 and batch: 800, loss is 3.582513828277588 and perplexity is 35.96383421191876
At time: 328.03776597976685 and batch: 850, loss is 3.4313120460510254 and perplexity is 30.917180915020136
At time: 328.5463399887085 and batch: 900, loss is 3.3708530139923094 and perplexity is 29.10334203050791
At time: 329.07145142555237 and batch: 950, loss is 3.4020290899276735 and perplexity is 30.024961626922348
At time: 329.5873222351074 and batch: 1000, loss is 3.3041178131103517 and perplexity is 27.22451388267139
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.457261992663872 and perplexity of 86.25102955103948
Finished 29 epochs...
Completing Train Step...
At time: 331.10768723487854 and batch: 50, loss is 3.6752102994918823 and perplexity is 39.45695376207989
At time: 331.6399600505829 and batch: 100, loss is 3.5651083040237426 and perplexity is 35.3432810165494
At time: 332.1586585044861 and batch: 150, loss is 3.663948664665222 and perplexity is 39.015096644039545
At time: 332.68372440338135 and batch: 200, loss is 3.7086607694625853 and perplexity is 40.799130481806344
At time: 333.2137522697449 and batch: 250, loss is 3.6859719944000244 and perplexity is 39.88387051415103
At time: 333.7455470561981 and batch: 300, loss is 3.5174418830871583 and perplexity is 33.69811439381634
At time: 334.27496004104614 and batch: 350, loss is 3.55323410987854 and perplexity is 34.92608984239187
At time: 334.7993278503418 and batch: 400, loss is 3.557588858604431 and perplexity is 35.078515835200655
At time: 335.3186824321747 and batch: 450, loss is 3.612202911376953 and perplexity is 37.04757551380195
At time: 335.85018372535706 and batch: 500, loss is 3.636290874481201 and perplexity is 37.95081102443741
At time: 336.37614274024963 and batch: 550, loss is 3.5332396173477174 and perplexity is 34.23469547331597
At time: 336.8950288295746 and batch: 600, loss is 3.464544486999512 and perplexity is 31.96189739692193
At time: 337.444762468338 and batch: 650, loss is 3.43961763381958 and perplexity is 31.175035609772575
At time: 337.95769000053406 and batch: 700, loss is 3.513548340797424 and perplexity is 33.567164455331174
At time: 338.47920751571655 and batch: 750, loss is 3.475166563987732 and perplexity is 32.303208639780095
At time: 339.00799012184143 and batch: 800, loss is 3.5825072813034056 and perplexity is 35.96359875839543
At time: 339.5331711769104 and batch: 850, loss is 3.431322078704834 and perplexity is 30.917491097948968
At time: 340.0449631214142 and batch: 900, loss is 3.370873556137085 and perplexity is 29.103939881713902
At time: 340.56242084503174 and batch: 950, loss is 3.402056269645691 and perplexity is 30.025777708003236
At time: 341.0724790096283 and batch: 1000, loss is 3.304161829948425 and perplexity is 27.225712246064482
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.457260504001525 and perplexity of 86.25090115247495
Finished 30 epochs...
Completing Train Step...
At time: 342.59987020492554 and batch: 50, loss is 3.675130624771118 and perplexity is 39.45381016554095
At time: 343.1352906227112 and batch: 100, loss is 3.5650199842453003 and perplexity is 35.34015964364211
At time: 343.64565086364746 and batch: 150, loss is 3.6638561725616454 and perplexity is 39.01148822255752
At time: 344.1660089492798 and batch: 200, loss is 3.7085645818710327 and perplexity is 40.79520630043966
At time: 344.6777763366699 and batch: 250, loss is 3.6858807754516603 and perplexity is 39.880232515355765
At time: 345.1927092075348 and batch: 300, loss is 3.517352342605591 and perplexity is 33.69509718350884
At time: 345.7031536102295 and batch: 350, loss is 3.5531509494781495 and perplexity is 34.9231854955415
At time: 346.21646785736084 and batch: 400, loss is 3.5575096797943115 and perplexity is 35.07573847001175
At time: 346.72403025627136 and batch: 450, loss is 3.6121282291412355 and perplexity is 37.044808821347345
At time: 347.23316740989685 and batch: 500, loss is 3.636222448348999 and perplexity is 37.94821428606846
At time: 347.7456338405609 and batch: 550, loss is 3.5331772422790526 and perplexity is 34.23256014843132
At time: 348.28285551071167 and batch: 600, loss is 3.464492564201355 and perplexity is 31.9602378888582
At time: 348.80482292175293 and batch: 650, loss is 3.4395787286758424 and perplexity is 31.173822764124267
At time: 349.3246901035309 and batch: 700, loss is 3.5135128545761107 and perplexity is 33.56597330463929
At time: 349.84266471862793 and batch: 750, loss is 3.4751546812057494 and perplexity is 32.3028247900751
At time: 350.38965916633606 and batch: 800, loss is 3.5825005960464478 and perplexity is 35.963358333300256
At time: 350.91474986076355 and batch: 850, loss is 3.431331763267517 and perplexity is 30.9177905217794
At time: 351.430073261261 and batch: 900, loss is 3.370893716812134 and perplexity is 29.10452664270323
At time: 351.95925402641296 and batch: 950, loss is 3.402083115577698 and perplexity is 30.02658378880998
At time: 352.47331404685974 and batch: 1000, loss is 3.3042051124572756 and perplexity is 27.22689066869809
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4572601318359375 and perplexity of 86.25086905286365
Finished 31 epochs...
Completing Train Step...
At time: 354.00451493263245 and batch: 50, loss is 3.6750515031814577 and perplexity is 39.45068864085411
At time: 354.5208089351654 and batch: 100, loss is 3.5649322509765624 and perplexity is 35.337059271923714
At time: 355.0556392669678 and batch: 150, loss is 3.663764500617981 and perplexity is 39.0079121275232
At time: 355.57117319107056 and batch: 200, loss is 3.708469200134277 and perplexity is 40.7913153683763
At time: 356.09098052978516 and batch: 250, loss is 3.6857901191711426 and perplexity is 39.876617285683864
At time: 356.6037971973419 and batch: 300, loss is 3.517263298034668 and perplexity is 33.692096951617096
At time: 357.1120238304138 and batch: 350, loss is 3.5530682754516603 and perplexity is 34.920298374525366
At time: 357.6288778781891 and batch: 400, loss is 3.557431206703186 and perplexity is 35.07298607638632
At time: 358.15142488479614 and batch: 450, loss is 3.6120540618896486 and perplexity is 37.042061411576675
At time: 358.66972494125366 and batch: 500, loss is 3.636154417991638 and perplexity is 37.94563274330201
At time: 359.19387793540955 and batch: 550, loss is 3.533115129470825 and perplexity is 34.23043393402094
At time: 359.70358204841614 and batch: 600, loss is 3.4644408226013184 and perplexity is 31.958584257793408
At time: 360.21844506263733 and batch: 650, loss is 3.4395399475097657 and perplexity is 31.172613830368487
At time: 360.7454080581665 and batch: 700, loss is 3.5134777450561523 and perplexity is 33.564794840117415
At time: 361.26936411857605 and batch: 750, loss is 3.47514244556427 and perplexity is 32.30242954671023
At time: 361.7832417488098 and batch: 800, loss is 3.582493805885315 and perplexity is 35.963114137131356
At time: 362.29962730407715 and batch: 850, loss is 3.4313410568237304 and perplexity is 30.9180778593388
At time: 362.82112073898315 and batch: 900, loss is 3.3709131193161013 and perplexity is 29.105091348875227
At time: 363.33076190948486 and batch: 950, loss is 3.402109298706055 and perplexity is 30.02736998899999
At time: 363.8618929386139 and batch: 1000, loss is 3.304247694015503 and perplexity is 27.228050056812553
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.457261992663872 and perplexity of 86.25102955103948
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 365.37692523002625 and batch: 50, loss is 3.67499370098114 and perplexity is 39.44840837014959
At time: 365.9291055202484 and batch: 100, loss is 3.564886360168457 and perplexity is 35.33543766292641
At time: 366.45375299453735 and batch: 150, loss is 3.663729901313782 and perplexity is 39.00656250425348
At time: 366.973340511322 and batch: 200, loss is 3.7084192037582397 and perplexity is 40.78927600141499
At time: 367.4911570549011 and batch: 250, loss is 3.6857516670227053 and perplexity is 39.87508397355658
At time: 368.00419878959656 and batch: 300, loss is 3.517189631462097 and perplexity is 33.68961506172923
At time: 368.5182194709778 and batch: 350, loss is 3.5529957008361817 and perplexity is 34.91776413926012
At time: 369.0309431552887 and batch: 400, loss is 3.5573362636566164 and perplexity is 35.06965629830808
At time: 369.56450486183167 and batch: 450, loss is 3.611976828575134 and perplexity is 37.03920064087224
At time: 370.0875549316406 and batch: 500, loss is 3.6360739135742186 and perplexity is 37.94257807520319
At time: 370.60776591300964 and batch: 550, loss is 3.5330158948898314 and perplexity is 34.22703725978882
At time: 371.1234564781189 and batch: 600, loss is 3.4643431425094606 and perplexity is 31.955462692807323
At time: 371.6500201225281 and batch: 650, loss is 3.4394240617752074 and perplexity is 31.169001578424933
At time: 372.17837476730347 and batch: 700, loss is 3.513374285697937 and perplexity is 33.561322427614506
At time: 372.69663286209106 and batch: 750, loss is 3.475000162124634 and perplexity is 32.29783377288473
At time: 373.2148492336273 and batch: 800, loss is 3.5823659658432008 and perplexity is 35.95851690496695
At time: 373.7342269420624 and batch: 850, loss is 3.4311896419525145 and perplexity is 30.913396756964687
At time: 374.2530686855316 and batch: 900, loss is 3.3707452058792113 and perplexity is 29.10020462324077
At time: 374.7697846889496 and batch: 950, loss is 3.401935453414917 and perplexity is 30.02215032584223
At time: 375.2880263328552 and batch: 1000, loss is 3.3040639305114747 and perplexity is 27.223046994630455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4572642256573936 and perplexity of 86.25122214924474
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 376.82452607154846 and batch: 50, loss is 3.674986138343811 and perplexity is 39.44811003727198
At time: 377.3492171764374 and batch: 100, loss is 3.564880232810974 and perplexity is 35.335221150731364
At time: 377.86013555526733 and batch: 150, loss is 3.6637251234054564 and perplexity is 39.00637613491897
At time: 378.3670401573181 and batch: 200, loss is 3.7084134244918823 and perplexity is 40.78904027000563
At time: 378.8762331008911 and batch: 250, loss is 3.685746726989746 and perplexity is 39.87488698981405
At time: 379.3928482532501 and batch: 300, loss is 3.51718062877655 and perplexity is 33.689311766083875
At time: 379.9111382961273 and batch: 350, loss is 3.552986879348755 and perplexity is 34.91745611400142
At time: 380.4388732910156 and batch: 400, loss is 3.557324390411377 and perplexity is 35.06923991015033
At time: 380.9554398059845 and batch: 450, loss is 3.6119672870635986 and perplexity is 37.03884723259808
At time: 381.4669499397278 and batch: 500, loss is 3.6360640048980715 and perplexity is 37.94220211634748
At time: 381.9804308414459 and batch: 550, loss is 3.533003396987915 and perplexity is 34.22660949630733
At time: 382.4962913990021 and batch: 600, loss is 3.464330940246582 and perplexity is 31.95507276623013
At time: 383.01219844818115 and batch: 650, loss is 3.4394094562530517 and perplexity is 31.168546342206298
At time: 383.54549074172974 and batch: 700, loss is 3.5133612537384034 and perplexity is 33.56088506066862
At time: 384.06741762161255 and batch: 750, loss is 3.474982113838196 and perplexity is 32.2972508575898
At time: 384.580904006958 and batch: 800, loss is 3.5823495626449584 and perplexity is 35.95792707512322
At time: 385.09526443481445 and batch: 850, loss is 3.4311701965332033 and perplexity is 30.91279563884693
At time: 385.612708568573 and batch: 900, loss is 3.370723729133606 and perplexity is 29.099579652260207
At time: 386.1308765411377 and batch: 950, loss is 3.4019132089614867 and perplexity is 30.021482506945087
At time: 386.63791131973267 and batch: 1000, loss is 3.3040401792526244 and perplexity is 27.222400420673093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.457263481326219 and perplexity of 86.25115794979514
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 388.19584679603577 and batch: 50, loss is 3.674985704421997 and perplexity is 39.448092919880224
At time: 388.70716857910156 and batch: 100, loss is 3.564879846572876 and perplexity is 35.33520750292539
At time: 389.22459840774536 and batch: 150, loss is 3.663725037574768 and perplexity is 39.00637278697501
At time: 389.73756551742554 and batch: 200, loss is 3.708413004875183 and perplexity is 40.78902315424678
At time: 390.2659864425659 and batch: 250, loss is 3.6857465219497683 and perplexity is 39.87487881386894
At time: 390.77875447273254 and batch: 300, loss is 3.51717990398407 and perplexity is 33.68928734833289
At time: 391.30458068847656 and batch: 350, loss is 3.5529861068725586 and perplexity is 34.91742914110815
At time: 391.8232772350311 and batch: 400, loss is 3.557323484420776 and perplexity is 35.06920813776299
At time: 392.3444833755493 and batch: 450, loss is 3.6119664907455444 and perplexity is 37.03881773790707
At time: 392.8637607097626 and batch: 500, loss is 3.636063189506531 and perplexity is 37.94217117860946
At time: 393.38283133506775 and batch: 550, loss is 3.5330024099349977 and perplexity is 34.22657571284926
At time: 393.8961546421051 and batch: 600, loss is 3.464329948425293 and perplexity is 31.9550410725244
At time: 394.4280014038086 and batch: 650, loss is 3.4394082117080687 and perplexity is 31.168507551572457
At time: 394.9409565925598 and batch: 700, loss is 3.5133600997924805 and perplexity is 33.56084633324448
At time: 395.4587371349335 and batch: 750, loss is 3.474980506896973 and perplexity is 32.2971989578477
At time: 395.9802634716034 and batch: 800, loss is 3.5823482131958007 and perplexity is 35.95787855176155
At time: 396.5000092983246 and batch: 850, loss is 3.4311685037612913 and perplexity is 30.912743310579042
At time: 397.01489639282227 and batch: 900, loss is 3.3707218313217164 and perplexity is 29.099524426784367
At time: 397.53059101104736 and batch: 950, loss is 3.4019112253189085 and perplexity is 30.02142295511319
At time: 398.04642033576965 and batch: 1000, loss is 3.3040380764007566 and perplexity is 27.222343176057713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4572642256573936 and perplexity of 86.25122214924474
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 399.6354892253876 and batch: 50, loss is 3.674985632896423 and perplexity is 39.44809009833284
At time: 400.1623933315277 and batch: 100, loss is 3.5648798608779906 and perplexity is 35.33520800839958
At time: 400.68260860443115 and batch: 150, loss is 3.663725004196167 and perplexity is 39.00637148499686
At time: 401.20143866539 and batch: 200, loss is 3.708412961959839 and perplexity is 40.78902140377185
At time: 401.73112511634827 and batch: 250, loss is 3.6857465887069703 and perplexity is 39.87488147580437
At time: 402.2455415725708 and batch: 300, loss is 3.5171799278259277 and perplexity is 33.6892881515481
At time: 402.7554693222046 and batch: 350, loss is 3.5529860925674437 and perplexity is 34.91742864161032
At time: 403.264865398407 and batch: 400, loss is 3.557323527336121 and perplexity is 35.069209642770176
At time: 403.8082435131073 and batch: 450, loss is 3.611966509819031 and perplexity is 37.03881844436646
At time: 404.3247182369232 and batch: 500, loss is 3.636063199043274 and perplexity is 37.9421715404542
At time: 404.8499972820282 and batch: 550, loss is 3.533002347946167 and perplexity is 34.22657359118392
At time: 405.3780109882355 and batch: 600, loss is 3.464329996109009 and perplexity is 31.955042596259524
At time: 405.90472435951233 and batch: 650, loss is 3.4394082164764406 and perplexity is 31.168507700195494
At time: 406.4277853965759 and batch: 700, loss is 3.5133600759506227 and perplexity is 33.56084553309156
At time: 406.94264125823975 and batch: 750, loss is 3.474980478286743 and perplexity is 32.29719803381743
At time: 407.45916843414307 and batch: 800, loss is 3.582348141670227 and perplexity is 35.95787597985376
At time: 407.9818596839905 and batch: 850, loss is 3.4311684703826906 and perplexity is 30.91274227875494
At time: 408.4961893558502 and batch: 900, loss is 3.3707217931747437 and perplexity is 29.099523316725627
At time: 409.009938955307 and batch: 950, loss is 3.401911149024963 and perplexity is 30.021420664660475
At time: 409.5275423526764 and batch: 1000, loss is 3.3040380096435547 and perplexity is 27.22234135877031
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4572642256573936 and perplexity of 86.25122214924474
Annealing...
Model not improving. Stopping early with 86.25086905286365loss at 35 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f46fd764898>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -85.81292626695193, 'params': {'data': 'ptb', 'batch_size': 20, 'lr': 2.9374378812766047, 'dropout': 0.21487660011599763, 'wordvec_source': '', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'anneal': 5.714280119200078, 'seq_len': 50}}, {'best_accuracy': -121.97588137079063, 'params': {'data': 'ptb', 'batch_size': 20, 'lr': 19.749362416879595, 'dropout': 0.374282315851305, 'wordvec_source': '', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'anneal': 7.170494832615578, 'seq_len': 50}}, {'best_accuracy': -136.76211586676547, 'params': {'data': 'ptb', 'batch_size': 20, 'lr': 26.320530869097347, 'dropout': 0.07257977092906687, 'wordvec_source': '', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'anneal': 6.243356750075442, 'seq_len': 50}}, {'best_accuracy': -86.4527225053381, 'params': {'data': 'ptb', 'batch_size': 20, 'lr': 8.771703379866045, 'dropout': 0.044677723951818016, 'wordvec_source': '', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'anneal': 7.420550692577278, 'seq_len': 50}}, {'best_accuracy': -130.58920316066022, 'params': {'data': 'ptb', 'batch_size': 20, 'lr': 17.388210957216543, 'dropout': 0.6548801797608832, 'wordvec_source': '', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'anneal': 3.598069729244449, 'seq_len': 50}}, {'best_accuracy': -86.25086905286365, 'params': {'data': 'ptb', 'batch_size': 20, 'lr': 4.57052781456207, 'dropout': 0.0, 'wordvec_source': '', 'wordvec_dim': 200, 'num_layers': 1, 'tune_wordvecs': True, 'anneal': 8.0, 'seq_len': 50}}]
