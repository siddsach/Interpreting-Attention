Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'type': 'continuous', 'domain': [0, 30], 'name': 'lr'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'dropout'}, {'type': 'continuous', 'domain': [2, 8], 'name': 'anneal'}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'wordvec_source': '', 'anneal': 3.791756496647735, 'lr': 10.618319537787965, 'dropout': 0.3990264700264041, 'seq_len': 50, 'batch_size': 50, 'data': 'ptb', 'num_layers': 1, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 418 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.263355016708374 and batch: 50, loss is 6.3633245182037355 and perplexity is 580.1719459870939
At time: 1.990854024887085 and batch: 100, loss is 5.5798021411895755 and perplexity is 265.01916422181256
At time: 2.7069761753082275 and batch: 150, loss is 5.378622560501099 and perplexity is 216.72354615861377
At time: 3.418572425842285 and batch: 200, loss is 5.310023450851441 and perplexity is 202.35497372893343
At time: 4.135611057281494 and batch: 250, loss is 5.320862293243408 and perplexity is 204.5601968400979
At time: 4.851158142089844 and batch: 300, loss is 5.27930097579956 and perplexity is 196.23265602209375
At time: 5.5700531005859375 and batch: 350, loss is 5.238327112197876 and perplexity is 188.35474234726948
At time: 6.287444353103638 and batch: 400, loss is 5.260728960037231 and perplexity is 192.62185376540754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 34 batches
Traceback (most recent call last):
  File "tune_models.py", line 172, in <module>
    seq_len = args.seq_len)
  File "tune_models.py", line 147, in tuneModels
    opt = Optimizer(dataset, vectors, tune_wordvecs, wordvec_dim, choices, trainerclass, max_time, num_layers, batch_size, seq_len)
  File "tune_models.py", line 35, in __init__
    exact_feval = True
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 117, in __init__
    self._init_design_chooser()
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 192, in _init_design_chooser
    self.Y, _ = self.objective.evaluate(self.X)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 50, in evaluate
    f_evals, cost_evals = self._eval_func(x)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 74, in _eval_func
    rlt = self.func(np.atleast_2d(x[i]))
  File "tune_models.py", line 73, in getError
    trainer.train()
  File "/home-nfs/siddsach/Interpreting-Attention/interpreting_language/pretraining/langmodel/trainer.py", line 408, in train
    this_perplexity = self.evaluate()
  File "/home-nfs/siddsach/Interpreting-Attention/interpreting_language/pretraining/langmodel/trainer.py", line 348, in evaluate
    for i, batch in enumerate(self.valid_iterator):
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torchtext/data/iterator.py", line 246, in __iter__
    text=data[i:i + seq_len],
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/autograd/variable.py", line 76, in __getitem__
    return Index.apply(self, key)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/autograd/_functions/tensor.py", line 16, in forward
    result = i.index(ctx.index)
ValueError: result of slicing is an empty tensor
