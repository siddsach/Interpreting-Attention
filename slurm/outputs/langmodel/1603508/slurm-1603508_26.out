Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'type': 'continuous', 'name': 'lr', 'domain': [0, 30]}, {'type': 'continuous', 'name': 'dropout', 'domain': [0, 1]}, {'type': 'continuous', 'name': 'anneal', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'seq_len': 20, 'anneal': 7.417179330718226, 'data': 'ptb', 'dropout': 0.9588655967622915, 'batch_size': 80, 'num_layers': 1, 'lr': 27.24601701842465}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.0754778385162354 and batch: 50, loss is 7.725847969055176 and perplexity is 2266.1734178762604
At time: 1.661128044128418 and batch: 100, loss is 6.999255647659302 and perplexity is 1095.8171806952382
At time: 2.2411305904388428 and batch: 150, loss is 6.820363264083863 and perplexity is 916.3178149810425
At time: 2.8175737857818604 and batch: 200, loss is 6.729895315170288 and perplexity is 837.0596339221149
At time: 3.39092755317688 and batch: 250, loss is 6.659046564102173 and perplexity is 779.8070865065561
At time: 3.983184814453125 and batch: 300, loss is 6.633389177322388 and perplexity is 760.0537673737668
At time: 4.546513795852661 and batch: 350, loss is 6.559063663482666 and perplexity is 705.6106961233537
At time: 5.119216680526733 and batch: 400, loss is 6.595076389312744 and perplexity is 731.4847621743628
At time: 5.702773571014404 and batch: 450, loss is 6.563883657455444 and perplexity is 709.0199451051675
At time: 6.269684553146362 and batch: 500, loss is 6.5576732730865475 and perplexity is 704.630303510242
At time: 6.850179195404053 and batch: 550, loss is 6.545622987747192 and perplexity is 696.190261879999
At time: 7.433587074279785 and batch: 600, loss is 6.548157730102539 and perplexity is 697.957163197954
At time: 8.006492376327515 and batch: 650, loss is 6.540257310867309 and perplexity is 692.4647338160389
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 6.422937131395527 and perplexity of 615.8091729666966
Finished 1 epochs...
Completing Train Step...
At time: 9.118144989013672 and batch: 50, loss is 6.269532480239868 and perplexity is 528.2303620076465
At time: 9.681528568267822 and batch: 100, loss is 6.280556526184082 and perplexity is 534.0858138719675
At time: 10.280880689620972 and batch: 150, loss is 6.321214122772217 and perplexity is 556.2479359223748
At time: 10.857164859771729 and batch: 200, loss is 6.670760269165039 and perplexity is 788.9952252382689
At time: 11.427894592285156 and batch: 250, loss is 7.029934749603272 and perplexity is 1129.9568776463057
At time: 12.007720470428467 and batch: 300, loss is 7.001812133789063 and perplexity is 1098.6222060949879
At time: 12.570204973220825 and batch: 350, loss is 6.884674034118652 and perplexity is 977.1830894414287
At time: 13.136211633682251 and batch: 400, loss is 6.9527559661865235 and perplexity is 1046.0285787438977
At time: 13.695643901824951 and batch: 450, loss is 6.761557893753052 and perplexity is 863.9871480515196
At time: 14.273034811019897 and batch: 500, loss is 6.797354278564453 and perplexity is 895.4749774501345
At time: 14.829068660736084 and batch: 550, loss is 6.469627122879029 and perplexity is 645.243085718913
At time: 15.39321494102478 and batch: 600, loss is 6.469060363769532 and perplexity is 644.877491933832
At time: 15.965998411178589 and batch: 650, loss is 6.508823757171631 and perplexity is 671.0366511551033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 6.933968936695772 and perplexity of 1026.5602575616076
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 17.049150705337524 and batch: 50, loss is 6.128110914230347 and perplexity is 458.5690656330496
At time: 17.60724425315857 and batch: 100, loss is 5.872079343795776 and perplexity is 354.986352046082
At time: 18.177860260009766 and batch: 150, loss is 5.853362379074096 and perplexity is 348.4038791405541
At time: 18.7684965133667 and batch: 200, loss is 5.840008430480957 and perplexity is 343.7822389023681
At time: 19.339630126953125 and batch: 250, loss is 5.795702781677246 and perplexity is 328.88323589505654
At time: 19.90347671508789 and batch: 300, loss is 5.819637985229492 and perplexity is 336.85008684762914
At time: 20.46906852722168 and batch: 350, loss is 5.744253883361816 and perplexity is 312.390461135433
At time: 21.047003507614136 and batch: 400, loss is 5.803985958099365 and perplexity is 331.61874748229343
At time: 21.615812301635742 and batch: 450, loss is 5.769167652130127 and perplexity is 320.2710447523211
At time: 22.183762311935425 and batch: 500, loss is 5.763427476882935 and perplexity is 318.43789915079486
At time: 22.751312255859375 and batch: 550, loss is 5.7716896343231205 and perplexity is 321.07978200593675
At time: 23.312896251678467 and batch: 600, loss is 5.798038358688355 and perplexity is 329.65226573477634
At time: 23.87431001663208 and batch: 650, loss is 5.758693828582763 and perplexity is 316.9340881949004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.831918155445772 and perplexity of 341.0121664703663
Finished 3 epochs...
Completing Train Step...
At time: 24.94989037513733 and batch: 50, loss is 5.778063192367553 and perplexity is 323.13273800090883
At time: 25.53555130958557 and batch: 100, loss is 5.750173263549804 and perplexity is 314.24510279112536
At time: 26.127057552337646 and batch: 150, loss is 5.745276803970337 and perplexity is 312.71017526924373
At time: 26.687142372131348 and batch: 200, loss is 5.749628400802612 and perplexity is 314.0739289784785
At time: 27.27741551399231 and batch: 250, loss is 5.72256181716919 and perplexity is 305.68703500980877
At time: 27.845568656921387 and batch: 300, loss is 5.7372717189788816 and perplexity is 310.2168965071019
At time: 28.41562056541443 and batch: 350, loss is 5.648761806488037 and perplexity is 283.93967581243083
At time: 28.98001980781555 and batch: 400, loss is 5.692799406051636 and perplexity is 296.72310752634786
At time: 29.55617117881775 and batch: 450, loss is 5.6550186824798585 and perplexity is 285.7218206688096
At time: 30.128760814666748 and batch: 500, loss is 5.654978713989258 and perplexity is 285.7104010271202
At time: 30.68661618232727 and batch: 550, loss is 5.6660566234588625 and perplexity is 288.89307110164157
At time: 31.261514902114868 and batch: 600, loss is 5.693034572601318 and perplexity is 296.7928950812835
At time: 31.8268563747406 and batch: 650, loss is 5.6537943363189695 and perplexity is 285.3722123190845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.741553811465993 and perplexity of 311.548122130483
Finished 4 epochs...
Completing Train Step...
At time: 32.878283977508545 and batch: 50, loss is 5.670641412734986 and perplexity is 290.220625910034
At time: 33.441038370132446 and batch: 100, loss is 5.621570310592651 and perplexity is 276.32295558821096
At time: 34.008209228515625 and batch: 150, loss is 5.577058734893799 and perplexity is 264.2931053707733
At time: 34.57574009895325 and batch: 200, loss is 5.558925533294678 and perplexity is 259.54381526230304
At time: 35.15401792526245 and batch: 250, loss is 5.518835124969482 and perplexity is 249.34441287149804
At time: 35.71731925010681 and batch: 300, loss is 5.5511063385009765 and perplexity is 257.5223052032818
At time: 36.291502714157104 and batch: 350, loss is 5.462147378921509 and perplexity is 235.60281005190257
At time: 36.85029983520508 and batch: 400, loss is 5.51575364112854 and perplexity is 248.5772447076118
At time: 37.42355966567993 and batch: 450, loss is 5.480171060562133 and perplexity is 239.88773919620775
At time: 37.992539167404175 and batch: 500, loss is 5.474991645812988 and perplexity is 238.64847320835628
At time: 38.55989098548889 and batch: 550, loss is 5.494190311431884 and perplexity is 243.27446983716786
At time: 39.13678789138794 and batch: 600, loss is 5.524418420791626 and perplexity is 250.7404701639544
At time: 39.70325303077698 and batch: 650, loss is 5.477475099563598 and perplexity is 239.2418822011278
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.5678734872855395 and perplexity of 261.8766227420456
Finished 5 epochs...
Completing Train Step...
At time: 40.78390336036682 and batch: 50, loss is 5.486816263198852 and perplexity is 241.48715015252583
At time: 41.3700966835022 and batch: 100, loss is 5.444310960769653 and perplexity is 231.43775506644224
At time: 41.94208645820618 and batch: 150, loss is 5.424625968933105 and perplexity is 226.9264528965961
At time: 42.49652361869812 and batch: 200, loss is 5.422742214202881 and perplexity is 226.49938149271736
At time: 43.06993532180786 and batch: 250, loss is 5.406502704620362 and perplexity is 222.85084799662795
At time: 43.6390380859375 and batch: 300, loss is 5.4587759590148925 and perplexity is 234.8098315309382
At time: 44.216140270233154 and batch: 350, loss is 5.373619136810302 and perplexity is 215.64189467027805
At time: 44.808621883392334 and batch: 400, loss is 5.425473976135254 and perplexity is 227.11896977932514
At time: 45.374202489852905 and batch: 450, loss is 5.385399808883667 and perplexity is 218.1973238991714
At time: 45.95069670677185 and batch: 500, loss is 5.373585948944092 and perplexity is 215.63473809468468
At time: 46.51709985733032 and batch: 550, loss is 5.385173387527466 and perplexity is 218.14792495787339
At time: 47.08440709114075 and batch: 600, loss is 5.4170785236358645 and perplexity is 225.22018498834834
At time: 47.649322509765625 and batch: 650, loss is 5.359389410018921 and perplexity is 212.59509837452015
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.462769751455269 and perplexity of 235.74948840931145
Finished 6 epochs...
Completing Train Step...
At time: 48.741485595703125 and batch: 50, loss is 5.377752981185913 and perplexity is 216.53516976173998
At time: 49.30765104293823 and batch: 100, loss is 5.342907152175903 and perplexity is 209.1197704491002
At time: 49.90811276435852 and batch: 150, loss is 5.323716497421264 and perplexity is 205.14488742476584
At time: 50.47314953804016 and batch: 200, loss is 5.316643371582031 and perplexity is 203.6989913512437
At time: 51.04283928871155 and batch: 250, loss is 5.3000133514404295 and perplexity is 200.3394847776321
At time: 51.61402916908264 and batch: 300, loss is 5.341348638534546 and perplexity is 208.79410827456704
At time: 52.17880153656006 and batch: 350, loss is 5.268139219284057 and perplexity is 194.0545333460396
At time: 52.75017213821411 and batch: 400, loss is 5.3190905952453615 and perplexity is 204.19809880777194
At time: 53.320600271224976 and batch: 450, loss is 5.288013010025025 and perplexity is 197.9497102950196
At time: 53.900758266448975 and batch: 500, loss is 5.28447774887085 and perplexity is 197.25114191213018
At time: 54.46209192276001 and batch: 550, loss is 5.308387823104859 and perplexity is 202.0242668496451
At time: 55.03233742713928 and batch: 600, loss is 5.347514028549194 and perplexity is 210.08538189726474
At time: 55.60141158103943 and batch: 650, loss is 5.2970876789093015 and perplexity is 199.75421362345926
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.409318512561274 and perplexity of 223.47923748090935
Finished 7 epochs...
Completing Train Step...
At time: 56.65342450141907 and batch: 50, loss is 5.309598150253296 and perplexity is 202.2689303360192
At time: 57.22307300567627 and batch: 100, loss is 5.278863649368287 and perplexity is 196.1468570573812
At time: 57.81730246543884 and batch: 150, loss is 5.260581111907959 and perplexity is 192.59337708983475
At time: 58.38670206069946 and batch: 200, loss is 5.250184755325318 and perplexity is 190.60147984419902
At time: 58.94773197174072 and batch: 250, loss is 5.2348086643219 and perplexity is 187.69319050443778
At time: 59.51779127120972 and batch: 300, loss is 5.2745588016510006 and perplexity is 195.30428957106955
At time: 60.09555220603943 and batch: 350, loss is 5.205355157852173 and perplexity is 182.2455872275042
At time: 60.66856288909912 and batch: 400, loss is 5.251683073043823 and perplexity is 190.8872754714825
At time: 61.24057388305664 and batch: 450, loss is 5.217842426300049 and perplexity is 184.535605075309
At time: 61.803616762161255 and batch: 500, loss is 5.205120611190796 and perplexity is 182.2028471459369
At time: 62.36626315116882 and batch: 550, loss is 5.22815936088562 and perplexity is 186.4493016082323
At time: 62.929224491119385 and batch: 600, loss is 5.263989009857178 and perplexity is 193.25083530360442
At time: 63.4947829246521 and batch: 650, loss is 5.212929611206055 and perplexity is 183.63123907973454
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.331773346545649 and perplexity of 206.80438501536193
Finished 8 epochs...
Completing Train Step...
At time: 64.593994140625 and batch: 50, loss is 5.224636249542236 and perplexity is 185.79357573465813
At time: 65.14760947227478 and batch: 100, loss is 5.1930237865448 and perplexity is 180.01204882106953
At time: 65.70406317710876 and batch: 150, loss is 5.16585069656372 and perplexity is 175.1864256902511
At time: 66.27124071121216 and batch: 200, loss is 5.161108903884887 and perplexity is 174.35769436834659
At time: 66.83211183547974 and batch: 250, loss is 5.148686561584473 and perplexity is 172.20516084087683
At time: 67.3939483165741 and batch: 300, loss is 5.190853672027588 and perplexity is 179.6218256283006
At time: 67.96506476402283 and batch: 350, loss is 5.126663751602173 and perplexity is 168.45417453744798
At time: 68.5223319530487 and batch: 400, loss is 5.173914299011231 and perplexity is 176.60477017900914
At time: 69.09595322608948 and batch: 450, loss is 5.138536148071289 and perplexity is 170.4660485355922
At time: 69.65978026390076 and batch: 500, loss is 5.129430427551269 and perplexity is 168.92087796153672
At time: 70.2317681312561 and batch: 550, loss is 5.160000276565552 and perplexity is 174.16450377300623
At time: 70.80937838554382 and batch: 600, loss is 5.197448558807373 and perplexity is 180.81032593641163
At time: 71.37230634689331 and batch: 650, loss is 5.152065105438233 and perplexity is 172.7879474593412
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.278296377144608 and perplexity of 196.03561994745507
Finished 9 epochs...
Completing Train Step...
At time: 72.42935824394226 and batch: 50, loss is 5.161494054794312 and perplexity is 174.42486132677493
At time: 73.00853419303894 and batch: 100, loss is 5.136421623229981 and perplexity is 170.10597466809682
At time: 73.57757568359375 and batch: 150, loss is 5.106908512115479 and perplexity is 165.15897784404086
At time: 74.1466896533966 and batch: 200, loss is 5.105402812957764 and perplexity is 164.91048523471542
At time: 74.7140281200409 and batch: 250, loss is 5.09410927772522 and perplexity is 163.05854004726953
At time: 75.27919101715088 and batch: 300, loss is 5.141677923202515 and perplexity is 171.0024567233227
At time: 75.84955525398254 and batch: 350, loss is 5.080105266571045 and perplexity is 160.7909809529919
At time: 76.41700315475464 and batch: 400, loss is 5.123551483154297 and perplexity is 167.9307149210744
At time: 76.98103904724121 and batch: 450, loss is 5.085845108032227 and perplexity is 161.71654945916345
At time: 77.5403048992157 and batch: 500, loss is 5.077683172225952 and perplexity is 160.4020012901683
At time: 78.09899020195007 and batch: 550, loss is 5.104489526748657 and perplexity is 164.75994351711452
At time: 78.69420266151428 and batch: 600, loss is 5.149279193878174 and perplexity is 172.30724542665644
At time: 79.25607800483704 and batch: 650, loss is 5.100174751281738 and perplexity is 164.0505728430328
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.236799651501226 and perplexity of 188.06725749810548
Finished 10 epochs...
Completing Train Step...
At time: 80.30073714256287 and batch: 50, loss is 5.104333152770996 and perplexity is 164.7341813637053
At time: 80.85589218139648 and batch: 100, loss is 5.074238195419311 and perplexity is 159.85037083859686
At time: 81.4095196723938 and batch: 150, loss is 5.041324806213379 and perplexity is 154.6747934775097
At time: 81.96312832832336 and batch: 200, loss is 5.036496849060058 and perplexity is 153.9298299748768
At time: 82.5172016620636 and batch: 250, loss is 5.021531953811645 and perplexity is 151.64343668105772
At time: 83.0806245803833 and batch: 300, loss is 5.073045253753662 and perplexity is 159.65979236805424
At time: 83.64523315429688 and batch: 350, loss is 5.017889604568482 and perplexity is 151.09210300865573
At time: 84.20379042625427 and batch: 400, loss is 5.061608200073242 and perplexity is 157.84415730131397
At time: 84.77189040184021 and batch: 450, loss is 5.024283094406128 and perplexity is 152.06120349972622
At time: 85.3275089263916 and batch: 500, loss is 5.0133396244049075 and perplexity is 150.40619854626001
At time: 85.89898824691772 and batch: 550, loss is 5.042799510955811 and perplexity is 154.90306140151174
At time: 86.46526980400085 and batch: 600, loss is 5.0949639892578125 and perplexity is 163.19796763863272
At time: 87.0283579826355 and batch: 650, loss is 5.037393054962158 and perplexity is 154.06784463254732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.185491225298713 and perplexity of 178.6611911346607
Finished 11 epochs...
Completing Train Step...
At time: 88.0832109451294 and batch: 50, loss is 5.046733331680298 and perplexity is 155.51362240614003
At time: 88.66772770881653 and batch: 100, loss is 5.021878852844238 and perplexity is 151.6960507679017
At time: 89.22722434997559 and batch: 150, loss is 4.991302490234375 and perplexity is 147.12793144391398
At time: 89.7866370677948 and batch: 200, loss is 4.981792430877686 and perplexity is 145.73536825754698
At time: 90.35336875915527 and batch: 250, loss is 4.971693897247315 and perplexity is 144.27106086299588
At time: 90.9279510974884 and batch: 300, loss is 5.029236001968384 and perplexity is 152.8162167965457
At time: 91.51189613342285 and batch: 350, loss is 4.971365909576416 and perplexity is 144.22374949296255
At time: 92.07952618598938 and batch: 400, loss is 5.01712197303772 and perplexity is 150.97616445107656
At time: 92.64824533462524 and batch: 450, loss is 4.979605474472046 and perplexity is 145.41699961646785
At time: 93.23388957977295 and batch: 500, loss is 4.967349185943603 and perplexity is 143.64560445170557
At time: 93.80226683616638 and batch: 550, loss is 5.001154985427856 and perplexity is 148.5846731676986
At time: 94.37334418296814 and batch: 600, loss is 5.0552136516571045 and perplexity is 156.83803547100322
At time: 94.94930386543274 and batch: 650, loss is 4.998450689315796 and perplexity is 148.18339904032106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.157685522939644 and perplexity of 173.76182209063563
Finished 12 epochs...
Completing Train Step...
At time: 96.08084559440613 and batch: 50, loss is 5.007918891906738 and perplexity is 149.5930925838791
At time: 96.6539957523346 and batch: 100, loss is 4.9824473094940185 and perplexity is 145.83083849115744
At time: 97.22455906867981 and batch: 150, loss is 4.952490177154541 and perplexity is 141.5269526667036
At time: 97.78322052955627 and batch: 200, loss is 4.937965936660767 and perplexity is 139.48623696718207
At time: 98.35370564460754 and batch: 250, loss is 4.930247001647949 and perplexity is 138.41369652915273
At time: 98.91793084144592 and batch: 300, loss is 4.988271656036377 and perplexity is 146.68268615097685
At time: 99.47425127029419 and batch: 350, loss is 4.936951847076416 and perplexity is 139.34485712514194
At time: 100.03791213035583 and batch: 400, loss is 4.979884099960327 and perplexity is 145.45752214403277
At time: 100.6138026714325 and batch: 450, loss is 4.946589479446411 and perplexity is 140.69430392168042
At time: 101.19506335258484 and batch: 500, loss is 4.929951219558716 and perplexity is 138.37276229092058
At time: 101.76123785972595 and batch: 550, loss is 4.964936094284058 and perplexity is 143.2993923304636
At time: 102.31982064247131 and batch: 600, loss is 5.022580919265747 and perplexity is 151.80258886545576
At time: 102.8809871673584 and batch: 650, loss is 4.966484813690186 and perplexity is 143.52149482307908
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.132335588043811 and perplexity of 169.41233375587942
Finished 13 epochs...
Completing Train Step...
At time: 104.07793760299683 and batch: 50, loss is 4.974068632125855 and perplexity is 144.61407350403462
At time: 104.66470170021057 and batch: 100, loss is 4.950899877548218 and perplexity is 141.3020612793329
At time: 105.23484325408936 and batch: 150, loss is 4.920063953399659 and perplexity is 137.01137524952327
At time: 105.8069441318512 and batch: 200, loss is 4.907654285430908 and perplexity is 135.3216159359183
At time: 106.38479351997375 and batch: 250, loss is 4.903041048049927 and perplexity is 134.69878294164533
At time: 106.95731353759766 and batch: 300, loss is 4.964288320541382 and perplexity is 143.20659680526768
At time: 107.52493190765381 and batch: 350, loss is 4.910026626586914 and perplexity is 135.64302607120484
At time: 108.1217954158783 and batch: 400, loss is 4.953930521011353 and perplexity is 141.73094701928963
At time: 108.6849091053009 and batch: 450, loss is 4.918641729354858 and perplexity is 136.81665287947303
At time: 109.25243306159973 and batch: 500, loss is 4.9071215057373045 and perplexity is 135.24953852923628
At time: 109.82844519615173 and batch: 550, loss is 4.940628290176392 and perplexity is 139.85809342758316
At time: 110.39639401435852 and batch: 600, loss is 4.998626070022583 and perplexity is 148.2093898286537
At time: 110.96079421043396 and batch: 650, loss is 4.942601499557495 and perplexity is 140.13433518133743
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.1253009870940565 and perplexity of 168.22476751652042
Finished 14 epochs...
Completing Train Step...
At time: 112.05482888221741 and batch: 50, loss is 4.948618202209473 and perplexity is 140.98002338347248
At time: 112.61616158485413 and batch: 100, loss is 4.924981260299683 and perplexity is 137.68676141003587
At time: 113.18156933784485 and batch: 150, loss is 4.89714337348938 and perplexity is 133.90671134227014
At time: 113.74574136734009 and batch: 200, loss is 4.88262903213501 and perplexity is 131.9771804331198
At time: 114.30657172203064 and batch: 250, loss is 4.878382720947266 and perplexity is 131.41795242369
At time: 114.87452387809753 and batch: 300, loss is 4.938201684951782 and perplexity is 139.51912448560302
At time: 115.44302129745483 and batch: 350, loss is 4.885763540267944 and perplexity is 132.39151300371566
At time: 116.0026216506958 and batch: 400, loss is 4.932077369689941 and perplexity is 138.66727653742905
At time: 116.5655369758606 and batch: 450, loss is 4.89577431678772 and perplexity is 133.72351089622362
At time: 117.15417170524597 and batch: 500, loss is 4.882706604003906 and perplexity is 131.98741854674734
At time: 117.71618151664734 and batch: 550, loss is 4.915743551254272 and perplexity is 136.42070788899534
At time: 118.275545835495 and batch: 600, loss is 4.977994632720947 and perplexity is 145.18294440572967
At time: 118.84835028648376 and batch: 650, loss is 4.91969518661499 and perplexity is 136.96085932008037
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.102834065755208 and perplexity of 164.48741549933035
Finished 15 epochs...
Completing Train Step...
At time: 119.93495535850525 and batch: 50, loss is 4.925128917694092 and perplexity is 137.70709337951618
At time: 120.5135018825531 and batch: 100, loss is 4.900805702209473 and perplexity is 134.3980208563573
At time: 121.07523083686829 and batch: 150, loss is 4.872818202972412 and perplexity is 130.68870570015068
At time: 121.6459732055664 and batch: 200, loss is 4.860328235626221 and perplexity is 129.06655939836023
At time: 122.20949411392212 and batch: 250, loss is 4.855787944793701 and perplexity is 128.4818879754692
At time: 122.7908296585083 and batch: 300, loss is 4.916936674118042 and perplexity is 136.58357169361352
At time: 123.35679650306702 and batch: 350, loss is 4.868124475479126 and perplexity is 130.07672588593113
At time: 123.92467403411865 and batch: 400, loss is 4.914689846038819 and perplexity is 136.2770363845967
At time: 124.48203945159912 and batch: 450, loss is 4.875284090042114 and perplexity is 131.01136694929608
At time: 125.04305624961853 and batch: 500, loss is 4.866307048797608 and perplexity is 129.8405356678757
At time: 125.59920120239258 and batch: 550, loss is 4.897566547393799 and perplexity is 133.96338915957764
At time: 126.17175507545471 and batch: 600, loss is 4.961646041870117 and perplexity is 142.82870453714364
At time: 126.73198938369751 and batch: 650, loss is 4.903911771774292 and perplexity is 134.81611944402067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.095599006204044 and perplexity of 163.30163402514634
Finished 16 epochs...
Completing Train Step...
At time: 127.81857562065125 and batch: 50, loss is 4.906617708206177 and perplexity is 135.18141730673182
At time: 128.38277888298035 and batch: 100, loss is 4.883643703460693 and perplexity is 132.1111618558086
At time: 128.94331240653992 and batch: 150, loss is 4.856331748962402 and perplexity is 128.55177596271992
At time: 129.51270937919617 and batch: 200, loss is 4.842467641830444 and perplexity is 126.78181816177451
At time: 130.07729625701904 and batch: 250, loss is 4.83745774269104 and perplexity is 126.14824244048475
At time: 130.6495065689087 and batch: 300, loss is 4.898609218597412 and perplexity is 134.10314177314442
At time: 131.21880269050598 and batch: 350, loss is 4.849954404830933 and perplexity is 127.73456563413608
At time: 131.78719782829285 and batch: 400, loss is 4.894463911056518 and perplexity is 133.54839360355953
At time: 132.34691452980042 and batch: 450, loss is 4.857498579025268 and perplexity is 128.70186158472754
At time: 132.91383147239685 and batch: 500, loss is 4.845979061126709 and perplexity is 127.22778481420464
At time: 133.47916555404663 and batch: 550, loss is 4.880018491744995 and perplexity is 131.63309798900562
At time: 134.0402262210846 and batch: 600, loss is 4.941253805160523 and perplexity is 139.94560412744778
At time: 134.60635209083557 and batch: 650, loss is 4.885450935363769 and perplexity is 132.35013323557578
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.090406829235601 and perplexity of 162.45594043733553
Finished 17 epochs...
Completing Train Step...
At time: 135.68042635917664 and batch: 50, loss is 4.890114212036133 and perplexity is 132.96875981686847
At time: 136.25468850135803 and batch: 100, loss is 4.869205846786499 and perplexity is 130.2174632059966
At time: 136.82252287864685 and batch: 150, loss is 4.839332332611084 and perplexity is 126.38494045073224
At time: 137.40683603286743 and batch: 200, loss is 4.825440587997437 and perplexity is 124.64137177980865
At time: 137.9781768321991 and batch: 250, loss is 4.820159549713135 and perplexity is 123.98487095241504
At time: 138.5528118610382 and batch: 300, loss is 4.883226823806763 and perplexity is 132.0560988784887
At time: 139.12354969978333 and batch: 350, loss is 4.831689929962158 and perplexity is 125.42273730221673
At time: 139.69255328178406 and batch: 400, loss is 4.880855331420898 and perplexity is 131.74329989229622
At time: 140.25141096115112 and batch: 450, loss is 4.8375019836425786 and perplexity is 126.15382348221964
At time: 140.81787300109863 and batch: 500, loss is 4.827491149902344 and perplexity is 124.89721885403875
At time: 141.38319063186646 and batch: 550, loss is 4.857054653167725 and perplexity is 128.64474018022844
At time: 141.9438717365265 and batch: 600, loss is 4.923925085067749 and perplexity is 137.54141683105263
At time: 142.50367188453674 and batch: 650, loss is 4.867412195205689 and perplexity is 129.98410778898673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.077480540556066 and perplexity of 160.36950205759192
Finished 18 epochs...
Completing Train Step...
At time: 143.60711479187012 and batch: 50, loss is 4.8740735912323 and perplexity is 130.85287379277392
At time: 144.18287134170532 and batch: 100, loss is 4.852076177597046 and perplexity is 128.00587708562915
At time: 144.74749851226807 and batch: 150, loss is 4.821682825088501 and perplexity is 124.17387797157664
At time: 145.30991578102112 and batch: 200, loss is 4.805243310928344 and perplexity is 122.14920764295418
At time: 145.86906242370605 and batch: 250, loss is 4.8013340091705325 and perplexity is 121.67262169689607
At time: 146.4282386302948 and batch: 300, loss is 4.865002613067627 and perplexity is 129.6712774511836
At time: 146.99250769615173 and batch: 350, loss is 4.814295406341553 and perplexity is 123.25993353914491
At time: 147.55852055549622 and batch: 400, loss is 4.859421634674073 and perplexity is 128.94960055822807
At time: 148.12550020217896 and batch: 450, loss is 4.820867557525634 and perplexity is 124.0726842922782
At time: 148.68474650382996 and batch: 500, loss is 4.809627809524536 and perplexity is 122.68594647787714
At time: 149.25371050834656 and batch: 550, loss is 4.837205963134766 and perplexity is 126.11648489009706
At time: 149.81987714767456 and batch: 600, loss is 4.908874244689941 and perplexity is 135.48680353480958
At time: 150.38535904884338 and batch: 650, loss is 4.851092138290405 and perplexity is 127.87997622695222
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.07341033337163 and perplexity of 159.71809154532994
Finished 19 epochs...
Completing Train Step...
At time: 151.43258547782898 and batch: 50, loss is 4.857346439361573 and perplexity is 128.68228241621085
At time: 152.00821805000305 and batch: 100, loss is 4.836958990097046 and perplexity is 126.08534136468108
At time: 152.5760841369629 and batch: 150, loss is 4.806616439819336 and perplexity is 122.31704945683039
At time: 153.14126634597778 and batch: 200, loss is 4.790084981918335 and perplexity is 120.31159253871895
At time: 153.70725321769714 and batch: 250, loss is 4.784355964660644 and perplexity is 119.62429599549188
At time: 154.2677104473114 and batch: 300, loss is 4.851249189376831 and perplexity is 127.9000614933159
At time: 154.82733869552612 and batch: 350, loss is 4.7971372890472415 and perplexity is 121.16306573672074
At time: 155.40449810028076 and batch: 400, loss is 4.843299074172974 and perplexity is 126.88727249883729
At time: 155.97479939460754 and batch: 450, loss is 4.802686185836792 and perplexity is 121.83725585902896
At time: 156.54189491271973 and batch: 500, loss is 4.793075723648071 and perplexity is 120.67195204159295
At time: 157.1035599708557 and batch: 550, loss is 4.822094221115112 and perplexity is 124.22497312103314
At time: 157.67169904708862 and batch: 600, loss is 4.895732898712158 and perplexity is 133.71797244044177
At time: 158.23626685142517 and batch: 650, loss is 4.838569164276123 and perplexity is 126.28852426173512
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.059812657973346 and perplexity of 157.5609957619647
Finished 20 epochs...
Completing Train Step...
At time: 159.32658314704895 and batch: 50, loss is 4.839674081802368 and perplexity is 126.42813978316975
At time: 159.89698386192322 and batch: 100, loss is 4.822797412872315 and perplexity is 124.3123578186693
At time: 160.47166991233826 and batch: 150, loss is 4.787889938354493 and perplexity is 120.04779298353931
At time: 161.0321388244629 and batch: 200, loss is 4.770454931259155 and perplexity is 117.97289931417272
At time: 161.5889174938202 and batch: 250, loss is 4.767322587966919 and perplexity is 117.60394584047737
At time: 162.15425992012024 and batch: 300, loss is 4.835538969039917 and perplexity is 125.90642458780813
At time: 162.71452617645264 and batch: 350, loss is 4.78251130104065 and perplexity is 119.40383281132492
At time: 163.2790994644165 and batch: 400, loss is 4.830380563735962 and perplexity is 125.25862047395957
At time: 163.84449672698975 and batch: 450, loss is 4.7897920894622805 and perplexity is 120.27635934090922
At time: 164.4105863571167 and batch: 500, loss is 4.77851619720459 and perplexity is 118.92775372672769
At time: 164.97237300872803 and batch: 550, loss is 4.803496570587158 and perplexity is 121.93603093070382
At time: 165.54106783866882 and batch: 600, loss is 4.878770198822021 and perplexity is 131.468883839361
At time: 166.11952185630798 and batch: 650, loss is 4.817874956130981 and perplexity is 123.70193922701542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.050230138442096 and perplexity of 156.05837538538455
Finished 21 epochs...
Completing Train Step...
At time: 167.17896795272827 and batch: 50, loss is 4.821756048202515 and perplexity is 124.18297070249588
At time: 167.76991176605225 and batch: 100, loss is 4.802421884536743 and perplexity is 121.80505836901773
At time: 168.33429050445557 and batch: 150, loss is 4.7674630260467525 and perplexity is 117.62046307260911
At time: 168.90543913841248 and batch: 200, loss is 4.748456239700317 and perplexity is 115.40598775661503
At time: 169.4889636039734 and batch: 250, loss is 4.74584776878357 and perplexity is 115.10534687090312
At time: 170.06018733978271 and batch: 300, loss is 4.813346481323242 and perplexity is 123.143024582144
At time: 170.6167232990265 and batch: 350, loss is 4.761168003082275 and perplexity is 116.88236516551373
At time: 171.18223643302917 and batch: 400, loss is 4.807772092819214 and perplexity is 122.45848723272516
At time: 171.74446988105774 and batch: 450, loss is 4.76791223526001 and perplexity is 117.673311137319
At time: 172.31171584129333 and batch: 500, loss is 4.75665602684021 and perplexity is 116.35618266490656
At time: 172.87469506263733 and batch: 550, loss is 4.781924858093261 and perplexity is 119.33382980407279
At time: 173.4371738433838 and batch: 600, loss is 4.859705238342285 and perplexity is 128.9861763242265
At time: 174.0053367614746 and batch: 650, loss is 4.797578239440918 and perplexity is 121.21650441929462
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.037168615004596 and perplexity of 154.03326953219846
Finished 22 epochs...
Completing Train Step...
At time: 175.09533047676086 and batch: 50, loss is 4.803466339111328 and perplexity is 121.93234468025257
At time: 175.6510307788849 and batch: 100, loss is 4.784118890762329 and perplexity is 119.59593955872532
At time: 176.21543908119202 and batch: 150, loss is 4.7490051746368405 and perplexity is 115.4693555259786
At time: 176.77872443199158 and batch: 200, loss is 4.729688844680786 and perplexity is 113.26031531572703
At time: 177.3496870994568 and batch: 250, loss is 4.727144117355347 and perplexity is 112.97246510194199
At time: 177.91280436515808 and batch: 300, loss is 4.792708864212036 and perplexity is 120.62769051669719
At time: 178.47447514533997 and batch: 350, loss is 4.741101922988892 and perplexity is 114.56036885819613
At time: 179.0387454032898 and batch: 400, loss is 4.78617488861084 and perplexity is 119.84208150009353
At time: 179.59819793701172 and batch: 450, loss is 4.748774185180664 and perplexity is 115.4426864026023
At time: 180.17334389686584 and batch: 500, loss is 4.736824731826783 and perplexity is 114.07141867311222
At time: 180.74904251098633 and batch: 550, loss is 4.764280786514282 and perplexity is 117.24676150450568
At time: 181.31203269958496 and batch: 600, loss is 4.842995042800903 and perplexity is 126.84870065110424
At time: 181.87178492546082 and batch: 650, loss is 4.7816320991516115 and perplexity is 119.2988988717772
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.025517482383578 and perplexity of 152.249021917713
Finished 23 epochs...
Completing Train Step...
At time: 182.97977375984192 and batch: 50, loss is 4.783918333053589 and perplexity is 119.57195607622936
At time: 183.5515923500061 and batch: 100, loss is 4.7643811321258545 and perplexity is 117.25852729280642
At time: 184.11419534683228 and batch: 150, loss is 4.72859335899353 and perplexity is 113.13630819777758
At time: 184.6783435344696 and batch: 200, loss is 4.709890270233155 and perplexity is 111.03997484662912
At time: 185.24597144126892 and batch: 250, loss is 4.710400142669678 and perplexity is 111.0966055051347
At time: 185.8142340183258 and batch: 300, loss is 4.773933830261231 and perplexity is 118.38402984200994
At time: 186.37599849700928 and batch: 350, loss is 4.7181119537353515 and perplexity is 111.95667361571874
At time: 186.93688821792603 and batch: 400, loss is 4.764513940811157 and perplexity is 117.27410127781404
At time: 187.50582551956177 and batch: 450, loss is 4.727892637252808 and perplexity is 113.05705889605534
At time: 188.063538312912 and batch: 500, loss is 4.716649074554443 and perplexity is 111.79301426483724
At time: 188.6333978176117 and batch: 550, loss is 4.743025302886963 and perplexity is 114.78092400644957
At time: 189.19919514656067 and batch: 600, loss is 4.825146379470826 and perplexity is 124.6047066193284
At time: 189.7565085887909 and batch: 650, loss is 4.757779941558838 and perplexity is 116.48703060839813
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.0203103458180145 and perplexity of 151.45830095129375
Finished 24 epochs...
Completing Train Step...
At time: 190.84928584098816 and batch: 50, loss is 4.765600833892822 and perplexity is 117.40163498234698
At time: 191.4122998714447 and batch: 100, loss is 4.744833393096924 and perplexity is 114.98864600490157
At time: 191.9687044620514 and batch: 150, loss is 4.708933181762696 and perplexity is 110.93375060804898
At time: 192.53101563453674 and batch: 200, loss is 4.688519983291626 and perplexity is 108.69219444244223
At time: 193.09251618385315 and batch: 250, loss is 4.690138988494873 and perplexity is 108.8683101984903
At time: 193.65482187271118 and batch: 300, loss is 4.753239984512329 and perplexity is 115.95938314765176
At time: 194.2147352695465 and batch: 350, loss is 4.699982204437256 and perplexity is 109.94521589772664
At time: 194.769681930542 and batch: 400, loss is 4.750532312393188 and perplexity is 115.64582785298359
At time: 195.35295820236206 and batch: 450, loss is 4.709649429321289 and perplexity is 111.01323509797554
At time: 195.914315700531 and batch: 500, loss is 4.697231683731079 and perplexity is 109.64322481161194
At time: 196.47245860099792 and batch: 550, loss is 4.726802644729614 and perplexity is 112.93389468339394
At time: 197.03163886070251 and batch: 600, loss is 4.804149255752564 and perplexity is 122.01564274712034
At time: 197.5962474346161 and batch: 650, loss is 4.740825958251953 and perplexity is 114.52875859799977
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.004338881548713 and perplexity of 149.05850524799993
Finished 25 epochs...
Completing Train Step...
At time: 198.68336081504822 and batch: 50, loss is 4.74443341255188 and perplexity is 114.94266198056926
At time: 199.2626645565033 and batch: 100, loss is 4.725389471054077 and perplexity is 112.77441219112279
At time: 199.8262755870819 and batch: 150, loss is 4.690838861465454 and perplexity is 108.94453085542867
At time: 200.39368200302124 and batch: 200, loss is 4.671286745071411 and perplexity is 106.83512359496805
At time: 200.95126676559448 and batch: 250, loss is 4.672795133590698 and perplexity is 106.99639406754264
At time: 201.51765537261963 and batch: 300, loss is 4.740045785903931 and perplexity is 114.43944127347203
At time: 202.0843002796173 and batch: 350, loss is 4.687569484710694 and perplexity is 108.58893174916233
At time: 202.64703369140625 and batch: 400, loss is 4.735225944519043 and perplexity is 113.88918844926317
At time: 203.21048498153687 and batch: 450, loss is 4.694997758865356 and perplexity is 109.39856346462749
At time: 203.7692244052887 and batch: 500, loss is 4.682667579650879 and perplexity is 108.05794161005463
At time: 204.33125233650208 and batch: 550, loss is 4.7116647148132325 and perplexity is 111.23718404481886
At time: 204.89670181274414 and batch: 600, loss is 4.793169441223145 and perplexity is 120.68326165426416
At time: 205.46225833892822 and batch: 650, loss is 4.728055944442749 and perplexity is 113.07552343430162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.004431331858916 and perplexity of 149.0722863900741
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 206.55226254463196 and batch: 50, loss is 4.72146369934082 and perplexity is 112.33255347946147
At time: 207.12322568893433 and batch: 100, loss is 4.676329326629639 and perplexity is 107.37520898687382
At time: 207.69406962394714 and batch: 150, loss is 4.6309823799133305 and perplexity is 102.61482135075082
At time: 208.25517749786377 and batch: 200, loss is 4.600581903457641 and perplexity is 99.54222275572778
At time: 208.81069135665894 and batch: 250, loss is 4.5927650928497314 and perplexity is 98.76715328514032
At time: 209.36607360839844 and batch: 300, loss is 4.649210395812989 and perplexity is 104.50243742907453
At time: 209.949383020401 and batch: 350, loss is 4.591504878997803 and perplexity is 98.64276394550157
At time: 210.51513242721558 and batch: 400, loss is 4.626745662689209 and perplexity is 102.18099102681627
At time: 211.087176322937 and batch: 450, loss is 4.577156181335449 and perplexity is 97.23747489352937
At time: 211.65433835983276 and batch: 500, loss is 4.550446176528931 and perplexity is 94.6746404951936
At time: 212.21599316596985 and batch: 550, loss is 4.567752075195313 and perplexity is 96.3273296177692
At time: 212.7836833000183 and batch: 600, loss is 4.648802280426025 and perplexity is 104.45979707806808
At time: 213.34829449653625 and batch: 650, loss is 4.610833988189698 and perplexity is 100.56798718534141
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.925202911975337 and perplexity of 137.71728329390612
Finished 27 epochs...
Completing Train Step...
At time: 214.42150473594666 and batch: 50, loss is 4.659731845855713 and perplexity is 105.60775920107166
At time: 215.0028247833252 and batch: 100, loss is 4.633965940475464 and perplexity is 102.92143605915152
At time: 215.57477974891663 and batch: 150, loss is 4.597389106750488 and perplexity is 99.22491149941233
At time: 216.13895058631897 and batch: 200, loss is 4.571134166717529 and perplexity is 96.65366900639651
At time: 216.70604729652405 and batch: 250, loss is 4.567353982925415 and perplexity is 96.28899008431073
At time: 217.27656269073486 and batch: 300, loss is 4.627291431427002 and perplexity is 102.2367734380789
At time: 217.8399031162262 and batch: 350, loss is 4.570693597793579 and perplexity is 96.61109578235492
At time: 218.4060459136963 and batch: 400, loss is 4.609883470535278 and perplexity is 100.47244095443568
At time: 218.96659588813782 and batch: 450, loss is 4.566723318099975 and perplexity is 96.22828315006365
At time: 219.53259563446045 and batch: 500, loss is 4.545378379821777 and perplexity is 94.19606235645789
At time: 220.096581697464 and batch: 550, loss is 4.568941850662231 and perplexity is 96.44200571721943
At time: 220.66356682777405 and batch: 600, loss is 4.653016967773437 and perplexity is 104.90099155842368
At time: 221.2383587360382 and batch: 650, loss is 4.606165933609009 and perplexity is 100.099624354217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.918209898705576 and perplexity of 136.7575840102194
Finished 28 epochs...
Completing Train Step...
At time: 222.35061597824097 and batch: 50, loss is 4.644665212631225 and perplexity is 104.02853251619281
At time: 222.91556024551392 and batch: 100, loss is 4.621063013076782 and perplexity is 101.601978977205
At time: 223.47869729995728 and batch: 150, loss is 4.5853299713134765 and perplexity is 98.0355307184276
At time: 224.04579257965088 and batch: 200, loss is 4.559360637664795 and perplexity is 95.52238688554006
At time: 224.62106800079346 and batch: 250, loss is 4.556419286727905 and perplexity is 95.24183482667836
At time: 225.18030333518982 and batch: 300, loss is 4.617659730911255 and perplexity is 101.25678650104621
At time: 225.7434365749359 and batch: 350, loss is 4.5609496974945065 and perplexity is 95.67429833960664
At time: 226.30618047714233 and batch: 400, loss is 4.602721214294434 and perplexity is 99.75540245910825
At time: 226.86963725090027 and batch: 450, loss is 4.562873315811157 and perplexity is 95.85851629803972
At time: 227.43499970436096 and batch: 500, loss is 4.5438024520874025 and perplexity is 94.04773307813181
At time: 227.99779391288757 and batch: 550, loss is 4.569751396179199 and perplexity is 96.52011152143137
At time: 228.5639226436615 and batch: 600, loss is 4.653821239471435 and perplexity is 104.98539439388226
At time: 229.12461853027344 and batch: 650, loss is 4.6017320728302 and perplexity is 99.65677903855303
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.914226157992494 and perplexity of 136.21386099980393
Finished 29 epochs...
Completing Train Step...
At time: 230.16002869606018 and batch: 50, loss is 4.635090894699097 and perplexity is 103.03728311244737
At time: 230.72795939445496 and batch: 100, loss is 4.612583894729614 and perplexity is 100.74412583196083
At time: 231.29277968406677 and batch: 150, loss is 4.577265396118164 and perplexity is 97.24809524316059
At time: 231.8567566871643 and batch: 200, loss is 4.55116904258728 and perplexity is 94.74310232077785
At time: 232.4228286743164 and batch: 250, loss is 4.548494663238525 and perplexity is 94.49006183843292
At time: 232.97875547409058 and batch: 300, loss is 4.610505237579345 and perplexity is 100.5349308321182
At time: 233.5462098121643 and batch: 350, loss is 4.55406044960022 and perplexity is 95.01743961051056
At time: 234.10337233543396 and batch: 400, loss is 4.597944021224976 and perplexity is 99.27998811902648
At time: 234.68488478660583 and batch: 450, loss is 4.560015220642089 and perplexity is 95.58493468307431
At time: 235.255921125412 and batch: 500, loss is 4.542607889175415 and perplexity is 93.93545421967191
At time: 235.82072567939758 and batch: 550, loss is 4.5693770790100094 and perplexity is 96.48398914755096
At time: 236.3815402984619 and batch: 600, loss is 4.653208208084107 and perplexity is 104.92105477502557
At time: 236.9454848766327 and batch: 650, loss is 4.5973301601409915 and perplexity is 99.21906269968697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.911497826669731 and perplexity of 135.842730969305
Finished 30 epochs...
Completing Train Step...
At time: 238.0387623310089 and batch: 50, loss is 4.627734775543213 and perplexity is 102.2821095590517
At time: 238.6083264350891 and batch: 100, loss is 4.60600998878479 and perplexity is 100.08401555298028
At time: 239.17956161499023 and batch: 150, loss is 4.571211242675782 and perplexity is 96.66111896765653
At time: 239.7394859790802 and batch: 200, loss is 4.544771289825439 and perplexity is 94.1388942241633
At time: 240.3081874847412 and batch: 250, loss is 4.5425065231323245 and perplexity is 93.92593283695233
At time: 240.8672502040863 and batch: 300, loss is 4.604938116073608 and perplexity is 99.97679570116566
At time: 241.4290828704834 and batch: 350, loss is 4.548802394866943 and perplexity is 94.51914389353632
At time: 241.98888635635376 and batch: 400, loss is 4.593768215179443 and perplexity is 98.86627853110443
At time: 242.5515034198761 and batch: 450, loss is 4.557604103088379 and perplexity is 95.35474578695043
At time: 243.11042523384094 and batch: 500, loss is 4.5411092472076415 and perplexity is 93.79478403916241
At time: 243.6736454963684 and batch: 550, loss is 4.568025693893433 and perplexity is 96.35369018249985
At time: 244.23087239265442 and batch: 600, loss is 4.651875953674317 and perplexity is 104.7813663080709
At time: 244.791113615036 and batch: 650, loss is 4.593333168029785 and perplexity is 98.82327639308922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9093909170113355 and perplexity of 135.55682390334573
Finished 31 epochs...
Completing Train Step...
At time: 245.87043523788452 and batch: 50, loss is 4.622007560729981 and perplexity is 101.69799222541407
At time: 246.4600965976715 and batch: 100, loss is 4.601058053970337 and perplexity is 99.58963112199103
At time: 247.03606057167053 and batch: 150, loss is 4.56640362739563 and perplexity is 96.19752477929002
At time: 247.60255599021912 and batch: 200, loss is 4.539873714447022 and perplexity is 93.6789690720328
At time: 248.16319942474365 and batch: 250, loss is 4.537780265808106 and perplexity is 93.48306209387961
At time: 248.73037695884705 and batch: 300, loss is 4.600688753128051 and perplexity is 99.55285937767079
At time: 249.29460668563843 and batch: 350, loss is 4.544841222763061 and perplexity is 94.14547786378483
At time: 249.8542993068695 and batch: 400, loss is 4.590579423904419 and perplexity is 98.55151672629715
At time: 250.4196014404297 and batch: 450, loss is 4.555488920211792 and perplexity is 95.15326621964417
At time: 250.99360156059265 and batch: 500, loss is 4.539706029891968 and perplexity is 93.66326187275023
At time: 251.56192803382874 and batch: 550, loss is 4.566770133972168 and perplexity is 96.23278826652353
At time: 252.13337111473083 and batch: 600, loss is 4.650362586975097 and perplexity is 104.62291360636436
At time: 252.690842628479 and batch: 650, loss is 4.589625511169434 and perplexity is 98.45755200363787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.907666973039215 and perplexity of 135.32333285446853
Finished 32 epochs...
Completing Train Step...
At time: 253.76918935775757 and batch: 50, loss is 4.61676775932312 and perplexity is 101.16650859303148
At time: 254.33927416801453 and batch: 100, loss is 4.596289949417114 and perplexity is 99.11590762746137
At time: 254.9108784198761 and batch: 150, loss is 4.561974544525146 and perplexity is 95.77240012124184
At time: 255.48647737503052 and batch: 200, loss is 4.53494743347168 and perplexity is 93.2186149966637
At time: 256.0576617717743 and batch: 250, loss is 4.5329932117462155 and perplexity is 93.0366230382577
At time: 256.6163821220398 and batch: 300, loss is 4.596404428482056 and perplexity is 99.12725497339177
At time: 257.1811201572418 and batch: 350, loss is 4.541016292572022 and perplexity is 93.78606578439637
At time: 257.74487924575806 and batch: 400, loss is 4.587028436660766 and perplexity is 98.20218215593907
At time: 258.30430459976196 and batch: 450, loss is 4.553038330078125 and perplexity is 94.92037004734385
At time: 258.86707854270935 and batch: 500, loss is 4.538002672195435 and perplexity is 93.50385563621897
At time: 259.4348051548004 and batch: 550, loss is 4.564704637527466 and perplexity is 96.03422492103124
At time: 260.0089547634125 and batch: 600, loss is 4.648379707336426 and perplexity is 104.41566450415367
At time: 260.5743200778961 and batch: 650, loss is 4.586111192703247 and perplexity is 98.11214809565261
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9061653286803 and perplexity of 135.12027783142935
Finished 33 epochs...
Completing Train Step...
At time: 261.63254499435425 and batch: 50, loss is 4.6123180103302 and perplexity is 100.71734310128244
At time: 262.2113974094391 and batch: 100, loss is 4.592336912155151 and perplexity is 98.72487214947395
At time: 262.77361941337585 and batch: 150, loss is 4.5579228019714355 and perplexity is 95.38514008098338
At time: 263.3315153121948 and batch: 200, loss is 4.530668411254883 and perplexity is 92.82058267398166
At time: 263.89950489997864 and batch: 250, loss is 4.528551692962647 and perplexity is 92.62431544323955
At time: 264.46947383880615 and batch: 300, loss is 4.5925176811218265 and perplexity is 98.74272015573182
At time: 265.038610458374 and batch: 350, loss is 4.537363080978394 and perplexity is 93.44407051245265
At time: 265.6059260368347 and batch: 400, loss is 4.583946466445923 and perplexity is 97.89999186543629
At time: 266.17202639579773 and batch: 450, loss is 4.550597686767578 and perplexity is 94.68898575926858
At time: 266.73831033706665 and batch: 500, loss is 4.535934114456177 and perplexity is 93.31063742240528
At time: 267.30590772628784 and batch: 550, loss is 4.562512884140014 and perplexity is 95.82397207860647
At time: 267.9023220539093 and batch: 600, loss is 4.646152410507202 and perplexity is 104.1833586287883
At time: 268.47641944885254 and batch: 650, loss is 4.582549133300781 and perplexity is 97.76328849421269
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9040078555836395 and perplexity of 134.82907371267603
Finished 34 epochs...
Completing Train Step...
At time: 269.6007990837097 and batch: 50, loss is 4.6079379749298095 and perplexity is 100.2771622805824
At time: 270.17122316360474 and batch: 100, loss is 4.588338251113892 and perplexity is 98.3308930687608
At time: 270.72911977767944 and batch: 150, loss is 4.554060039520263 and perplexity is 95.01740064577103
At time: 271.298198223114 and batch: 200, loss is 4.526736059188843 and perplexity is 92.45629618472222
At time: 271.88164591789246 and batch: 250, loss is 4.524724912643433 and perplexity is 92.27053987823682
At time: 272.44618582725525 and batch: 300, loss is 4.588925056457519 and perplexity is 98.38861109522416
At time: 273.0183804035187 and batch: 350, loss is 4.533932819366455 and perplexity is 93.12408204036366
At time: 273.579402923584 and batch: 400, loss is 4.580849475860596 and perplexity is 97.59726552460204
At time: 274.1463499069214 and batch: 450, loss is 4.548122501373291 and perplexity is 94.45490278360852
At time: 274.71034717559814 and batch: 500, loss is 4.533419990539551 and perplexity is 93.07633757003116
At time: 275.2767403125763 and batch: 550, loss is 4.5601480770111085 and perplexity is 95.59763459404257
At time: 275.83577489852905 and batch: 600, loss is 4.64378026008606 and perplexity is 103.93651292406426
At time: 276.4087748527527 and batch: 650, loss is 4.579185562133789 and perplexity is 97.43500712422252
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.903078864602482 and perplexity of 134.7038768815666
Finished 35 epochs...
Completing Train Step...
At time: 277.4794225692749 and batch: 50, loss is 4.603623704910278 and perplexity is 99.84547141078589
At time: 278.05754828453064 and batch: 100, loss is 4.584296731948853 and perplexity is 97.93428886150052
At time: 278.62736201286316 and batch: 150, loss is 4.550199565887451 and perplexity is 94.65129560003727
At time: 279.19282841682434 and batch: 200, loss is 4.523245468139648 and perplexity is 92.13413166422431
At time: 279.7626049518585 and batch: 250, loss is 4.521009578704834 and perplexity is 91.92836005965327
At time: 280.326376914978 and batch: 300, loss is 4.585354747772217 and perplexity is 98.03795972180049
At time: 280.89366388320923 and batch: 350, loss is 4.530448875427246 and perplexity is 92.80020746716838
At time: 281.4672064781189 and batch: 400, loss is 4.57770604133606 and perplexity is 97.2909565939101
At time: 282.0358655452728 and batch: 450, loss is 4.545489950180054 and perplexity is 94.20657243117874
At time: 282.6199812889099 and batch: 500, loss is 4.530794458389282 and perplexity is 92.83228317983313
At time: 283.19377732276917 and batch: 550, loss is 4.55791669845581 and perplexity is 95.38455789806716
At time: 283.7575454711914 and batch: 600, loss is 4.641302633285522 and perplexity is 103.67931578492158
At time: 284.32683205604553 and batch: 650, loss is 4.5760037708282475 and perplexity is 97.1254819490856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.901691212373621 and perplexity of 134.51708437806627
Finished 36 epochs...
Completing Train Step...
At time: 285.4128408432007 and batch: 50, loss is 4.599728860855103 and perplexity is 99.45734520619563
At time: 285.9732983112335 and batch: 100, loss is 4.580281162261963 and perplexity is 97.54181542942905
At time: 286.53854751586914 and batch: 150, loss is 4.546802034378052 and perplexity is 94.33026051305738
At time: 287.1073868274689 and batch: 200, loss is 4.520051488876343 and perplexity is 91.8403266116399
At time: 287.6725523471832 and batch: 250, loss is 4.517649707794189 and perplexity is 91.62001093352768
At time: 288.23201155662537 and batch: 300, loss is 4.5820880794525145 and perplexity is 97.71822474303944
At time: 288.7955889701843 and batch: 350, loss is 4.527038745880127 and perplexity is 92.48428571091758
At time: 289.35506653785706 and batch: 400, loss is 4.574781675338745 and perplexity is 97.00685783544088
At time: 289.92046785354614 and batch: 450, loss is 4.543125305175781 and perplexity is 93.98407050302364
At time: 290.4925775527954 and batch: 500, loss is 4.528302526473999 and perplexity is 92.60123944279954
At time: 291.06094121932983 and batch: 550, loss is 4.555737295150757 and perplexity is 95.17690284158456
At time: 291.6260299682617 and batch: 600, loss is 4.638879508972168 and perplexity is 103.42839204653235
At time: 292.1971535682678 and batch: 650, loss is 4.572720050811768 and perplexity is 96.80707213035957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.899773092830882 and perplexity of 134.25931182800403
Finished 37 epochs...
Completing Train Step...
At time: 293.2304253578186 and batch: 50, loss is 4.595977096557617 and perplexity is 99.08490378241217
At time: 293.8096089363098 and batch: 100, loss is 4.576837244033814 and perplexity is 97.20646718068996
At time: 294.37480878829956 and batch: 150, loss is 4.543487033843994 and perplexity is 94.01807338521779
At time: 294.9504852294922 and batch: 200, loss is 4.516225576400757 and perplexity is 91.48962486517412
At time: 295.5099685192108 and batch: 250, loss is 4.514018793106079 and perplexity is 91.287949697925
At time: 296.08178877830505 and batch: 300, loss is 4.578621616363526 and perplexity is 97.3800745550301
At time: 296.64975452423096 and batch: 350, loss is 4.523687009811401 and perplexity is 92.17482170525815
At time: 297.24673795700073 and batch: 400, loss is 4.571621360778809 and perplexity is 96.70076957256373
At time: 297.81535363197327 and batch: 450, loss is 4.540427503585815 and perplexity is 93.73086183513634
At time: 298.38282561302185 and batch: 500, loss is 4.525755586624146 and perplexity is 92.36568974870049
At time: 298.963095664978 and batch: 550, loss is 4.553629512786865 and perplexity is 94.97650191928433
At time: 299.54958271980286 and batch: 600, loss is 4.636541728973389 and perplexity is 103.18688162940362
At time: 300.1239972114563 and batch: 650, loss is 4.56979603767395 and perplexity is 96.52442041966039
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.898113774318321 and perplexity of 134.0367175948431
Finished 38 epochs...
Completing Train Step...
At time: 301.25034403800964 and batch: 50, loss is 4.592518091201782 and perplexity is 98.74276064815044
At time: 301.8193814754486 and batch: 100, loss is 4.573348340988159 and perplexity is 96.867914174012
At time: 302.3846879005432 and batch: 150, loss is 4.540160455703735 and perplexity is 93.70583454888839
At time: 302.9438135623932 and batch: 200, loss is 4.512748184204102 and perplexity is 91.17203207497266
At time: 303.50299167633057 and batch: 250, loss is 4.510670490264893 and perplexity is 90.98280114655451
At time: 304.0709352493286 and batch: 300, loss is 4.575060081481934 and perplexity is 97.03386890044249
At time: 304.6424262523651 and batch: 350, loss is 4.520486516952515 and perplexity is 91.88028842386568
At time: 305.21080493927 and batch: 400, loss is 4.568683032989502 and perplexity is 96.41704805162814
At time: 305.77573585510254 and batch: 450, loss is 4.537895650863647 and perplexity is 93.49384926451866
At time: 306.33451223373413 and batch: 500, loss is 4.52335018157959 and perplexity is 92.14377985122556
At time: 306.89508605003357 and batch: 550, loss is 4.551369867324829 and perplexity is 94.7621309900858
At time: 307.46669340133667 and batch: 600, loss is 4.63411919593811 and perplexity is 102.93721054018283
At time: 308.035667181015 and batch: 650, loss is 4.566778593063354 and perplexity is 96.23360231189766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.896351234585631 and perplexity of 133.80068062781424
Finished 39 epochs...
Completing Train Step...
At time: 309.16216492652893 and batch: 50, loss is 4.5890442943573 and perplexity is 98.40034344603002
At time: 309.77268266677856 and batch: 100, loss is 4.570081214904786 and perplexity is 96.55195091193184
At time: 310.33910942077637 and batch: 150, loss is 4.536982593536377 and perplexity is 93.40852298021535
At time: 310.89896965026855 and batch: 200, loss is 4.509466714859009 and perplexity is 90.87334418219433
At time: 311.46845626831055 and batch: 250, loss is 4.507339105606079 and perplexity is 90.68020674762069
At time: 312.05127477645874 and batch: 300, loss is 4.571765489578247 and perplexity is 96.71470794282315
At time: 312.61451745033264 and batch: 350, loss is 4.517660884857178 and perplexity is 91.62103498188385
At time: 313.1772918701172 and batch: 400, loss is 4.566130151748657 and perplexity is 96.17122069589091
At time: 313.74977946281433 and batch: 450, loss is 4.53560320854187 and perplexity is 93.27976548874886
At time: 314.3105413913727 and batch: 500, loss is 4.521026620864868 and perplexity is 91.92992673082678
At time: 314.8789460659027 and batch: 550, loss is 4.549243326187134 and perplexity is 94.5608295340135
At time: 315.43405508995056 and batch: 600, loss is 4.632071504592895 and perplexity is 102.72664256774316
At time: 315.9975211620331 and batch: 650, loss is 4.564103755950928 and perplexity is 95.97653705808179
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.894770902745864 and perplexity of 133.58939814422587
Finished 40 epochs...
Completing Train Step...
At time: 317.12239813804626 and batch: 50, loss is 4.585784969329834 and perplexity is 98.08014683979064
At time: 317.6917769908905 and batch: 100, loss is 4.566952676773071 and perplexity is 96.25035647265844
At time: 318.2543315887451 and batch: 150, loss is 4.5338412857055665 and perplexity is 93.11555844232167
At time: 318.818382024765 and batch: 200, loss is 4.506517934799194 and perplexity is 90.60577337451642
At time: 319.3865671157837 and batch: 250, loss is 4.504180374145508 and perplexity is 90.3942242344811
At time: 319.9498872756958 and batch: 300, loss is 4.56876919746399 and perplexity is 96.42535613383082
At time: 320.5115294456482 and batch: 350, loss is 4.514803237915039 and perplexity is 91.35958815071882
At time: 321.07023096084595 and batch: 400, loss is 4.563300638198853 and perplexity is 95.89948754144645
At time: 321.62991189956665 and batch: 450, loss is 4.533119163513184 and perplexity is 93.04834190331249
At time: 322.19411277770996 and batch: 500, loss is 4.518368740081787 and perplexity is 91.68591236936973
At time: 322.76725912094116 and batch: 550, loss is 4.546653327941894 and perplexity is 94.31623403913396
At time: 323.3375129699707 and batch: 600, loss is 4.629345951080322 and perplexity is 102.44703681956321
At time: 323.9097578525543 and batch: 650, loss is 4.5609998035430905 and perplexity is 95.6790923207502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.892718446020987 and perplexity of 133.31549287114623
Finished 41 epochs...
Completing Train Step...
At time: 324.97167444229126 and batch: 50, loss is 4.582183799743652 and perplexity is 97.72757880764107
At time: 325.5551266670227 and batch: 100, loss is 4.563885383605957 and perplexity is 95.9555807248473
At time: 326.1211676597595 and batch: 150, loss is 4.530668125152588 and perplexity is 92.82055611780376
At time: 326.6991493701935 and batch: 200, loss is 4.50336633682251 and perplexity is 90.3206699042176
At time: 327.27049255371094 and batch: 250, loss is 4.500759868621826 and perplexity is 90.08555848856795
At time: 327.8482036590576 and batch: 300, loss is 4.565506887435913 and perplexity is 96.11129928149744
At time: 328.42066955566406 and batch: 350, loss is 4.511710929870605 and perplexity is 91.0775125184934
At time: 328.9841032028198 and batch: 400, loss is 4.559511547088623 and perplexity is 95.53680320165928
At time: 329.5481607913971 and batch: 450, loss is 4.529791717529297 and perplexity is 92.73924311169782
At time: 330.1196689605713 and batch: 500, loss is 4.515468273162842 and perplexity is 91.4203657044315
At time: 330.6847610473633 and batch: 550, loss is 4.543866033554077 and perplexity is 94.05371296104121
At time: 331.2523329257965 and batch: 600, loss is 4.6263839149475094 and perplexity is 102.14403396903681
At time: 331.8167655467987 and batch: 650, loss is 4.557680416107178 and perplexity is 95.36202287312287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.89072343414905 and perplexity of 133.04979200647068
Finished 42 epochs...
Completing Train Step...
At time: 332.9045433998108 and batch: 50, loss is 4.578397941589356 and perplexity is 97.35829552464601
At time: 333.4738562107086 and batch: 100, loss is 4.560351810455322 and perplexity is 95.61711301353247
At time: 334.04637718200684 and batch: 150, loss is 4.527310304641723 and perplexity is 92.5094040394091
At time: 334.6237597465515 and batch: 200, loss is 4.499598360061645 and perplexity is 89.98098408502402
At time: 335.18566513061523 and batch: 250, loss is 4.497255144119262 and perplexity is 89.77038604330927
At time: 335.7651135921478 and batch: 300, loss is 4.561857442855835 and perplexity is 95.76118566994207
At time: 336.32502031326294 and batch: 350, loss is 4.508418340682983 and perplexity is 90.77812483633743
At time: 336.87777948379517 and batch: 400, loss is 4.555697641372681 and perplexity is 95.17312879262944
At time: 337.44215965270996 and batch: 450, loss is 4.5265028762817385 and perplexity is 92.43473947022413
At time: 338.011164188385 and batch: 500, loss is 4.512203350067138 and perplexity is 91.12237196905079
At time: 338.57677817344666 and batch: 550, loss is 4.541170492172241 and perplexity is 93.80052867330365
At time: 339.1436355113983 and batch: 600, loss is 4.623859882354736 and perplexity is 101.88654419113755
At time: 339.7168674468994 and batch: 650, loss is 4.554616079330445 and perplexity is 95.07024879466574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.888756247127757 and perplexity of 132.78831545341293
Finished 43 epochs...
Completing Train Step...
At time: 340.83210802078247 and batch: 50, loss is 4.574800729751587 and perplexity is 97.00870626176888
At time: 341.40859937667847 and batch: 100, loss is 4.556431932449341 and perplexity is 95.24303923600586
At time: 341.96814012527466 and batch: 150, loss is 4.523581399917602 and perplexity is 92.16508764614244
At time: 342.5368971824646 and batch: 200, loss is 4.496369905471802 and perplexity is 89.69095299197707
At time: 343.12084197998047 and batch: 250, loss is 4.493989381790161 and perplexity is 89.47769548732973
At time: 343.6919059753418 and batch: 300, loss is 4.5586930274963375 and perplexity is 95.45863645132687
At time: 344.2662715911865 and batch: 350, loss is 4.50519063949585 and perplexity is 90.48559253243783
At time: 344.82847929000854 and batch: 400, loss is 4.552405710220337 and perplexity is 94.8603405262789
At time: 345.4003338813782 and batch: 450, loss is 4.523496561050415 and perplexity is 92.15726879618819
At time: 345.9705340862274 and batch: 500, loss is 4.509379568099976 and perplexity is 90.86542520982776
At time: 346.54053592681885 and batch: 550, loss is 4.538752164840698 and perplexity is 93.57396235727617
At time: 347.10678362846375 and batch: 600, loss is 4.621379117965699 and perplexity is 101.63410093616999
At time: 347.67478036880493 and batch: 650, loss is 4.551321039199829 and perplexity is 94.75750404587195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.8863483503753065 and perplexity of 132.46895954220122
Finished 44 epochs...
Completing Train Step...
At time: 348.77583384513855 and batch: 50, loss is 4.571494035720825 and perplexity is 96.68845792527804
At time: 349.34683537483215 and batch: 100, loss is 4.553407468795776 and perplexity is 94.95541529890382
At time: 349.9072439670563 and batch: 150, loss is 4.520717582702637 and perplexity is 91.90152126462844
At time: 350.4749753475189 and batch: 200, loss is 4.493414745330811 and perplexity is 89.42629311145815
At time: 351.03351283073425 and batch: 250, loss is 4.491464042663575 and perplexity is 89.25201903667872
At time: 351.59206438064575 and batch: 300, loss is 4.555625581741333 and perplexity is 95.16627089914608
At time: 352.1591999530792 and batch: 350, loss is 4.502508420944213 and perplexity is 90.24321559675914
At time: 352.721971988678 and batch: 400, loss is 4.548924608230591 and perplexity is 94.53069610194342
At time: 353.2883970737457 and batch: 450, loss is 4.520591478347779 and perplexity is 91.88993281327134
At time: 353.8476300239563 and batch: 500, loss is 4.506856260299682 and perplexity is 90.63643280428147
At time: 354.4114816188812 and batch: 550, loss is 4.536503648757934 and perplexity is 93.36379616756365
At time: 354.96887254714966 and batch: 600, loss is 4.619046907424927 and perplexity is 101.39734500431842
At time: 355.55589842796326 and batch: 650, loss is 4.548571357727051 and perplexity is 94.49730898330083
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.884970870672488 and perplexity of 132.28661185812817
Finished 45 epochs...
Completing Train Step...
At time: 356.6242411136627 and batch: 50, loss is 4.5685687828063966 and perplexity is 96.40603301548072
At time: 357.21823716163635 and batch: 100, loss is 4.550616064071655 and perplexity is 94.6907259035422
At time: 357.78289246559143 and batch: 150, loss is 4.517743673324585 and perplexity is 91.62862046094298
At time: 358.347128868103 and batch: 200, loss is 4.490836896896362 and perplexity is 89.19606255899282
At time: 358.907723903656 and batch: 250, loss is 4.488725481033325 and perplexity is 89.00793125918263
At time: 359.4685080051422 and batch: 300, loss is 4.553202085494995 and perplexity is 94.93591504486446
At time: 360.02790784835815 and batch: 350, loss is 4.5001351928710935 and perplexity is 90.02930179761363
At time: 360.58656096458435 and batch: 400, loss is 4.546079196929932 and perplexity is 94.26209970582738
At time: 361.1634681224823 and batch: 450, loss is 4.518259391784668 and perplexity is 91.67588721910893
At time: 361.73124051094055 and batch: 500, loss is 4.504405546188354 and perplexity is 90.41458077838975
At time: 362.29570150375366 and batch: 550, loss is 4.534133577346802 and perplexity is 93.14277931974702
At time: 362.8654386997223 and batch: 600, loss is 4.616324186325073 and perplexity is 101.12164381265112
At time: 363.42719554901123 and batch: 650, loss is 4.5457542037963865 and perplexity is 94.23147014813699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.883402506510417 and perplexity of 132.07930088900997
Finished 46 epochs...
Completing Train Step...
At time: 364.51275515556335 and batch: 50, loss is 4.565872144699097 and perplexity is 96.14641104365673
At time: 365.0835819244385 and batch: 100, loss is 4.547878799438476 and perplexity is 94.43188674568783
At time: 365.6565833091736 and batch: 150, loss is 4.515227460861206 and perplexity is 91.3983532062964
At time: 366.22251558303833 and batch: 200, loss is 4.488043050765992 and perplexity is 88.94721027413422
At time: 366.79193782806396 and batch: 250, loss is 4.486355228424072 and perplexity is 88.7972098080605
At time: 367.36395359039307 and batch: 300, loss is 4.550491647720337 and perplexity is 94.67894556177036
At time: 367.93565678596497 and batch: 350, loss is 4.497437047958374 and perplexity is 89.78671710646559
At time: 368.50555992126465 and batch: 400, loss is 4.543055238723755 and perplexity is 93.97748560334949
At time: 369.07100772857666 and batch: 450, loss is 4.515478162765503 and perplexity is 91.42126981999414
At time: 369.63843035697937 and batch: 500, loss is 4.501680250167847 and perplexity is 90.1685097417094
At time: 370.2204096317291 and batch: 550, loss is 4.531601457595825 and perplexity is 92.90722899524026
At time: 370.7934341430664 and batch: 600, loss is 4.613983221054077 and perplexity is 100.88519841954562
At time: 371.3711440563202 and batch: 650, loss is 4.543325386047363 and perplexity is 94.00287679909201
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.881758446786918 and perplexity of 131.8623330330379
Finished 47 epochs...
Completing Train Step...
At time: 372.50535321235657 and batch: 50, loss is 4.563314228057862 and perplexity is 95.90079081081676
At time: 373.09337091445923 and batch: 100, loss is 4.544979610443115 and perplexity is 94.15850733959265
At time: 373.6708137989044 and batch: 150, loss is 4.5120174598693845 and perplexity is 91.10543478758244
At time: 374.239874124527 and batch: 200, loss is 4.48552565574646 and perplexity is 88.72357661522081
At time: 374.80267333984375 and batch: 250, loss is 4.4837375926971434 and perplexity is 88.56507501392991
At time: 375.3588945865631 and batch: 300, loss is 4.548138256072998 and perplexity is 94.45639090396018
At time: 375.92005705833435 and batch: 350, loss is 4.4947927379608155 and perplexity is 89.5496068274773
At time: 376.48767280578613 and batch: 400, loss is 4.54016152381897 and perplexity is 93.70593463757129
At time: 377.04552960395813 and batch: 450, loss is 4.513127841949463 and perplexity is 91.20665281471037
At time: 377.60706210136414 and batch: 500, loss is 4.498936595916748 and perplexity is 89.92145759445496
At time: 378.1694452762604 and batch: 550, loss is 4.529056634902954 and perplexity is 92.67109715482924
At time: 378.73722648620605 and batch: 600, loss is 4.611515016555786 and perplexity is 100.63650016432808
At time: 379.3104455471039 and batch: 650, loss is 4.540785322189331 and perplexity is 93.76440648231602
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.880259794347427 and perplexity of 131.66486523072146
Finished 48 epochs...
Completing Train Step...
At time: 380.4073176383972 and batch: 50, loss is 4.560605363845825 and perplexity is 95.64136013056662
At time: 380.9775869846344 and batch: 100, loss is 4.5424036693573 and perplexity is 93.91627269698675
At time: 381.54819917678833 and batch: 150, loss is 4.509819574356079 and perplexity is 90.90541536269711
At time: 382.1331512928009 and batch: 200, loss is 4.4829558086395265 and perplexity is 88.4958633080516
At time: 382.69876527786255 and batch: 250, loss is 4.48141544342041 and perplexity is 88.35965229246422
At time: 383.2673099040985 and batch: 300, loss is 4.545717487335205 and perplexity is 94.22801036553709
At time: 383.8397579193115 and batch: 350, loss is 4.491999025344849 and perplexity is 89.29978009566267
At time: 384.4052917957306 and batch: 400, loss is 4.537107753753662 and perplexity is 93.42021474290438
At time: 384.97649812698364 and batch: 450, loss is 4.510295448303222 and perplexity is 90.94868517619433
At time: 385.5447220802307 and batch: 500, loss is 4.496129789352417 and perplexity is 89.66941933379337
At time: 386.1027572154999 and batch: 550, loss is 4.526521110534668 and perplexity is 92.43642496400993
At time: 386.6678307056427 and batch: 600, loss is 4.609116868972778 and perplexity is 100.39544813938858
At time: 387.23481798171997 and batch: 650, loss is 4.537994632720947 and perplexity is 93.50310391737878
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.878751567765778 and perplexity of 131.46643445800927
Finished 49 epochs...
Completing Train Step...
At time: 388.319500207901 and batch: 50, loss is 4.557497415542603 and perplexity is 95.34457316579993
At time: 388.89782762527466 and batch: 100, loss is 4.539285326004029 and perplexity is 93.6238656619739
At time: 389.46870970726013 and batch: 150, loss is 4.506940660476684 and perplexity is 90.64408285808138
At time: 390.0380322933197 and batch: 200, loss is 4.48056414604187 and perplexity is 88.28446396045474
At time: 390.6030604839325 and batch: 250, loss is 4.4787936115264895 and perplexity is 88.1282915649465
At time: 391.1670467853546 and batch: 300, loss is 4.543618707656861 and perplexity is 94.0304539184969
At time: 391.7411081790924 and batch: 350, loss is 4.489318752288819 and perplexity is 89.06075277350344
At time: 392.3033564090729 and batch: 400, loss is 4.534339723587036 and perplexity is 93.16198233275516
At time: 392.8681471347809 and batch: 450, loss is 4.507821521759033 and perplexity is 90.72396289759709
At time: 393.43233704566956 and batch: 500, loss is 4.493312301635743 and perplexity is 89.41713242079106
At time: 393.9944632053375 and batch: 550, loss is 4.523955020904541 and perplexity is 92.19952889073537
At time: 394.5614082813263 and batch: 600, loss is 4.6066646480560305 and perplexity is 100.14955793328761
At time: 395.1213617324829 and batch: 650, loss is 4.535580434799194 and perplexity is 93.27764118356205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.876927095301011 and perplexity of 131.226796241437
Finished Training.
Improved accuracyfrom -10000000 to -131.226796241437
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa09ad998d0>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'seq_len': 20, 'anneal': 2.8637643628573874, 'data': 'ptb', 'dropout': 0.4074046926609942, 'batch_size': 80, 'num_layers': 1, 'lr': 15.763860501961515}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.8326396942138672 and batch: 50, loss is 6.630364952087402 and perplexity is 757.7586657918272
At time: 1.415968894958496 and batch: 100, loss is 5.81681643486023 and perplexity is 335.90098695693194
At time: 1.9801654815673828 and batch: 150, loss is 5.683466348648071 and perplexity is 293.9666567958676
At time: 2.5547375679016113 and batch: 200, loss is 5.674615840911866 and perplexity is 291.3763821568915
At time: 3.128270149230957 and batch: 250, loss is 5.659549016952514 and perplexity is 287.0191725827289
At time: 3.6987760066986084 and batch: 300, loss is 5.70768518447876 and perplexity is 301.1731005934004
At time: 4.276705741882324 and batch: 350, loss is 5.678964614868164 and perplexity is 292.64627140842066
At time: 4.851322889328003 and batch: 400, loss is 5.7434539794921875 and perplexity is 312.1406787110181
At time: 5.427666902542114 and batch: 450, loss is 5.726779766082764 and perplexity is 306.97913038752085
At time: 6.014336824417114 and batch: 500, loss is 5.719085569381714 and perplexity is 304.6262359990537
At time: 6.579578399658203 and batch: 550, loss is 5.7451119804382325 and perplexity is 312.65863752106435
At time: 7.15011739730835 and batch: 600, loss is 5.800695686340332 and perplexity is 330.5294247493393
At time: 7.715766429901123 and batch: 650, loss is 5.81255352973938 and perplexity is 334.47212064411775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.694683000153186 and perplexity of 297.28254012805047
Finished 1 epochs...
Completing Train Step...
At time: 8.788769483566284 and batch: 50, loss is 5.592087917327881 and perplexity is 268.29521354287346
At time: 9.358391046524048 and batch: 100, loss is 5.565666379928589 and perplexity is 261.29927029516733
At time: 9.92052960395813 and batch: 150, loss is 5.541839399337769 and perplexity is 255.14688510387379
At time: 10.476558685302734 and batch: 200, loss is 5.512753601074219 and perplexity is 247.83262052656352
At time: 11.046841382980347 and batch: 250, loss is 5.486057605743408 and perplexity is 241.3040136034067
At time: 11.612842559814453 and batch: 300, loss is 5.529707908630371 and perplexity is 252.07027271827855
At time: 12.180038452148438 and batch: 350, loss is 5.4809126377105715 and perplexity is 240.06570043962623
At time: 12.75057601928711 and batch: 400, loss is 5.51195517539978 and perplexity is 247.6348235729625
At time: 13.306547164916992 and batch: 450, loss is 5.488865156173706 and perplexity is 241.98243870133794
At time: 13.872304916381836 and batch: 500, loss is 5.501808977127075 and perplexity is 245.1349749791172
At time: 14.440502405166626 and batch: 550, loss is 5.505924701690674 and perplexity is 246.14596206211345
At time: 15.008317708969116 and batch: 600, loss is 5.546538791656494 and perplexity is 256.3487422030419
At time: 15.57480001449585 and batch: 650, loss is 5.549993963241577 and perplexity is 257.2360030294796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.660669064989277 and perplexity of 287.34082794459505
Finished 2 epochs...
Completing Train Step...
At time: 16.65027379989624 and batch: 50, loss is 5.5336143493652346 and perplexity is 253.05689613795292
At time: 17.232088088989258 and batch: 100, loss is 5.499411907196045 and perplexity is 244.54807300511362
At time: 17.795671939849854 and batch: 150, loss is 5.496539211273193 and perplexity is 243.84656883935227
At time: 18.357685804367065 and batch: 200, loss is 5.469059066772461 and perplexity is 237.23686366094265
At time: 18.942749500274658 and batch: 250, loss is 5.433919630050659 and perplexity is 229.04526093991325
At time: 19.51111149787903 and batch: 300, loss is 5.487025938034058 and perplexity is 241.5377892396239
At time: 20.068885326385498 and batch: 350, loss is 5.445491619110108 and perplexity is 231.7111653525648
At time: 20.648737907409668 and batch: 400, loss is 5.485801343917847 and perplexity is 241.24218451892077
At time: 21.21966791152954 and batch: 450, loss is 5.459534387588501 and perplexity is 234.9879855665514
At time: 21.778777599334717 and batch: 500, loss is 5.461359968185425 and perplexity is 235.41736688936615
At time: 22.34934639930725 and batch: 550, loss is 5.449729280471802 and perplexity is 232.69516225534755
At time: 22.914308309555054 and batch: 600, loss is 5.515373163223266 and perplexity is 248.48268454841323
At time: 23.47382664680481 and batch: 650, loss is 5.499537954330444 and perplexity is 244.57889953169564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.647744571461397 and perplexity of 283.6509892846591
Finished 3 epochs...
Completing Train Step...
At time: 24.53596258163452 and batch: 50, loss is 5.499076681137085 and perplexity is 244.46610785756818
At time: 25.092597246170044 and batch: 100, loss is 5.484281692504883 and perplexity is 240.875858906465
At time: 25.646895170211792 and batch: 150, loss is 5.476019191741943 and perplexity is 238.8938215071881
At time: 26.203715085983276 and batch: 200, loss is 5.43529486656189 and perplexity is 229.36046903864636
At time: 26.777360916137695 and batch: 250, loss is 5.40486406326294 and perplexity is 222.48597441068307
At time: 27.338297128677368 and batch: 300, loss is 5.465796747207642 and perplexity is 236.4641822512517
At time: 27.897032260894775 and batch: 350, loss is 5.413300542831421 and perplexity is 224.37091272998228
At time: 28.472947359085083 and batch: 400, loss is 5.464232301712036 and perplexity is 236.09453614743975
At time: 29.033809423446655 and batch: 450, loss is 5.43922122001648 and perplexity is 230.26278956405895
At time: 29.6113543510437 and batch: 500, loss is 5.433201923370361 and perplexity is 228.8809326028725
At time: 30.182600259780884 and batch: 550, loss is 5.445944185256958 and perplexity is 231.81605371452127
At time: 30.747217893600464 and batch: 600, loss is 5.498346910476685 and perplexity is 244.28776874583195
At time: 31.31145429611206 and batch: 650, loss is 5.500734462738037 and perplexity is 244.87171538468635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.652912214690564 and perplexity of 285.12059031537405
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 32.382424116134644 and batch: 50, loss is 5.376486101150513 and perplexity is 216.26101937270138
At time: 32.955063581466675 and batch: 100, loss is 5.237648029327392 and perplexity is 188.2268772885664
At time: 33.50991749763489 and batch: 150, loss is 5.178676891326904 and perplexity is 177.44787278374434
At time: 34.07892370223999 and batch: 200, loss is 5.152699584960938 and perplexity is 172.89761266025758
At time: 34.64244246482849 and batch: 250, loss is 5.106036701202393 and perplexity is 165.01505319141955
At time: 35.226794958114624 and batch: 300, loss is 5.1592615127563475 and perplexity is 174.03588485612087
At time: 35.7866415977478 and batch: 350, loss is 5.080543279647827 and perplexity is 160.86142493184366
At time: 36.348904609680176 and batch: 400, loss is 5.114120788574219 and perplexity is 166.35445592903238
At time: 36.9117476940155 and batch: 450, loss is 5.070538558959961 and perplexity is 159.260075191547
At time: 37.47216868400574 and batch: 500, loss is 5.054528017044067 and perplexity is 156.730538741194
At time: 38.033466815948486 and batch: 550, loss is 5.078480739593505 and perplexity is 160.5299837226092
At time: 38.609264850616455 and batch: 600, loss is 5.14683539390564 and perplexity is 171.8866750892143
At time: 39.1789767742157 and batch: 650, loss is 5.118562631607055 and perplexity is 167.0950198267686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.313786525352328 and perplexity of 203.11788511661362
Finished 5 epochs...
Completing Train Step...
At time: 40.25612211227417 and batch: 50, loss is 5.139075584411621 and perplexity is 170.55802892351667
At time: 40.82361173629761 and batch: 100, loss is 5.098046865463257 and perplexity is 163.70186309440763
At time: 41.39229440689087 and batch: 150, loss is 5.06987566947937 and perplexity is 159.15453834652124
At time: 41.95770454406738 and batch: 200, loss is 5.06389102935791 and perplexity is 158.2049001665417
At time: 42.52743458747864 and batch: 250, loss is 5.032164039611817 and perplexity is 153.2643241493012
At time: 43.08897829055786 and batch: 300, loss is 5.089462852478027 and perplexity is 162.30265816426683
At time: 43.656790256500244 and batch: 350, loss is 5.025449676513672 and perplexity is 152.23869889032437
At time: 44.21211838722229 and batch: 400, loss is 5.081313018798828 and perplexity is 160.98529393579443
At time: 44.77797198295593 and batch: 450, loss is 5.044059505462647 and perplexity is 155.0983614206074
At time: 45.34451341629028 and batch: 500, loss is 5.0333531379699705 and perplexity is 153.44667890289293
At time: 45.90905570983887 and batch: 550, loss is 5.06052059173584 and perplexity is 157.6725780022743
At time: 46.47749090194702 and batch: 600, loss is 5.13794469833374 and perplexity is 170.36525624564982
At time: 47.04735541343689 and batch: 650, loss is 5.099405279159546 and perplexity is 163.92438905428028
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.308758305568321 and perplexity of 202.0991271640673
Finished 6 epochs...
Completing Train Step...
At time: 48.12749242782593 and batch: 50, loss is 5.110669002532959 and perplexity is 165.78122584312098
At time: 48.70922541618347 and batch: 100, loss is 5.072299308776856 and perplexity is 159.54073935695757
At time: 49.27686333656311 and batch: 150, loss is 5.047900543212891 and perplexity is 155.695245675455
At time: 49.84687948226929 and batch: 200, loss is 5.043711261749268 and perplexity is 155.0443587948705
At time: 50.4057674407959 and batch: 250, loss is 5.011721305847168 and perplexity is 150.16299025128137
At time: 50.9737663269043 and batch: 300, loss is 5.067893142700195 and perplexity is 158.83932277711975
At time: 51.53781008720398 and batch: 350, loss is 5.0100525379180905 and perplexity is 149.9126120386855
At time: 52.11885476112366 and batch: 400, loss is 5.065504703521729 and perplexity is 158.46039741574327
At time: 52.67611217498779 and batch: 450, loss is 5.027146062850952 and perplexity is 152.49717371372174
At time: 53.251001596450806 and batch: 500, loss is 5.017537536621094 and perplexity is 151.03891768505622
At time: 53.806615352630615 and batch: 550, loss is 5.044820289611817 and perplexity is 155.21640269180904
At time: 54.36461329460144 and batch: 600, loss is 5.121536750793457 and perplexity is 167.59272007425258
At time: 54.93251371383667 and batch: 650, loss is 5.076682138442993 and perplexity is 160.24151380811986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.295371859681373 and perplexity of 199.4117653765226
Finished 7 epochs...
Completing Train Step...
At time: 56.03738760948181 and batch: 50, loss is 5.084698123931885 and perplexity is 161.53116948243792
At time: 56.60632824897766 and batch: 100, loss is 5.048704338073731 and perplexity is 155.8204430235379
At time: 57.16712713241577 and batch: 150, loss is 5.026822843551636 and perplexity is 152.44789164897253
At time: 57.73774433135986 and batch: 200, loss is 5.018835411071778 and perplexity is 151.23507450330476
At time: 58.3090386390686 and batch: 250, loss is 4.9899326419830325 and perplexity is 146.92652648291315
At time: 58.875903606414795 and batch: 300, loss is 5.042864751815796 and perplexity is 154.91316774012148
At time: 59.44396257400513 and batch: 350, loss is 4.988772420883179 and perplexity is 146.75615807837957
At time: 60.00590968132019 and batch: 400, loss is 5.049037065505981 and perplexity is 155.87229738564326
At time: 60.574867963790894 and batch: 450, loss is 5.009722843170166 and perplexity is 149.86319478460086
At time: 61.13907504081726 and batch: 500, loss is 5.000161380767822 and perplexity is 148.4371120648692
At time: 61.697752952575684 and batch: 550, loss is 5.028086051940918 and perplexity is 152.64058678606526
At time: 62.25602173805237 and batch: 600, loss is 5.104195690155029 and perplexity is 164.7115381285306
At time: 62.81501817703247 and batch: 650, loss is 5.062066116333008 and perplexity is 157.91645325893523
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.289452646292892 and perplexity of 198.23489110623913
Finished 8 epochs...
Completing Train Step...
At time: 63.87478017807007 and batch: 50, loss is 5.066669626235962 and perplexity is 158.64509909270134
At time: 64.45574688911438 and batch: 100, loss is 5.0286375999450685 and perplexity is 152.72479861835862
At time: 65.01916718482971 and batch: 150, loss is 5.006931810379029 and perplexity is 149.44550485805883
At time: 65.58174347877502 and batch: 200, loss is 5.002948493957519 and perplexity is 148.8514001634753
At time: 66.14772129058838 and batch: 250, loss is 4.971991930007935 and perplexity is 144.3140647735112
At time: 66.71397995948792 and batch: 300, loss is 5.025071172714234 and perplexity is 152.1810868682453
At time: 67.275381565094 and batch: 350, loss is 4.975216808319092 and perplexity is 144.78021129991834
At time: 67.83694815635681 and batch: 400, loss is 5.033531923294067 and perplexity is 153.47411536965782
At time: 68.40086483955383 and batch: 450, loss is 4.993522109985352 and perplexity is 147.4548622032281
At time: 68.96038699150085 and batch: 500, loss is 4.984311323165894 and perplexity is 146.10292267342308
At time: 69.51951551437378 and batch: 550, loss is 5.012228555679322 and perplexity is 150.23917972479722
At time: 70.08198118209839 and batch: 600, loss is 5.089487085342407 and perplexity is 162.30659127022565
At time: 70.64423155784607 and batch: 650, loss is 5.044661331176758 and perplexity is 155.19173169622502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.282234939874387 and perplexity of 196.8092410113822
Finished 9 epochs...
Completing Train Step...
At time: 71.71655702590942 and batch: 50, loss is 5.049985256195068 and perplexity is 156.02016413857163
At time: 72.28201818466187 and batch: 100, loss is 5.011628751754761 and perplexity is 150.14909269515329
At time: 72.84365725517273 and batch: 150, loss is 4.990660524368286 and perplexity is 147.03351064468646
At time: 73.41406512260437 and batch: 200, loss is 4.988056058883667 and perplexity is 146.65106519030783
At time: 73.98025321960449 and batch: 250, loss is 4.958207540512085 and perplexity is 142.3384312283992
At time: 74.54341912269592 and batch: 300, loss is 5.01189697265625 and perplexity is 150.18937122168364
At time: 75.10812258720398 and batch: 350, loss is 4.962174282073975 and perplexity is 142.90417233195188
At time: 75.67378497123718 and batch: 400, loss is 5.018404893875122 and perplexity is 151.1699792163192
At time: 76.2403609752655 and batch: 450, loss is 4.977388744354248 and perplexity is 145.0950063916711
At time: 76.80762028694153 and batch: 500, loss is 4.9678418254852295 and perplexity is 143.71638739024493
At time: 77.37244057655334 and batch: 550, loss is 4.998051490783691 and perplexity is 148.12425625058475
At time: 77.93394422531128 and batch: 600, loss is 5.074380474090576 and perplexity is 159.87311575498086
At time: 78.50932884216309 and batch: 650, loss is 5.028577365875244 and perplexity is 152.7155996592229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.274938845166973 and perplexity of 195.37852780594906
Finished 10 epochs...
Completing Train Step...
At time: 79.57676529884338 and batch: 50, loss is 5.031803855895996 and perplexity is 153.20913077600014
At time: 80.1468403339386 and batch: 100, loss is 4.992207450866699 and perplexity is 147.2611366934487
At time: 80.7108325958252 and batch: 150, loss is 4.971802492141723 and perplexity is 144.28672881433084
At time: 81.27107882499695 and batch: 200, loss is 4.967354440689087 and perplexity is 143.64635927478005
At time: 81.82846260070801 and batch: 250, loss is 4.935023775100708 and perplexity is 139.07644904935418
At time: 82.38426804542542 and batch: 300, loss is 4.987661628723145 and perplexity is 146.5932329932552
At time: 82.94479632377625 and batch: 350, loss is 4.941327142715454 and perplexity is 139.95586777222857
At time: 83.50690364837646 and batch: 400, loss is 4.996252059936523 and perplexity is 147.8579565604676
At time: 84.08367562294006 and batch: 450, loss is 4.957638092041016 and perplexity is 142.25739990013454
At time: 84.65528726577759 and batch: 500, loss is 4.947500953674316 and perplexity is 140.8226016148562
At time: 85.2238039970398 and batch: 550, loss is 4.973650732040405 and perplexity is 144.55365189633696
At time: 85.79759383201599 and batch: 600, loss is 5.053401260375977 and perplexity is 156.55404101524198
At time: 86.36174249649048 and batch: 650, loss is 5.00494912147522 and perplexity is 149.14949445908883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.261897068397672 and perplexity of 192.84698842843625
Finished 11 epochs...
Completing Train Step...
At time: 87.4430603981018 and batch: 50, loss is 5.012323484420777 and perplexity is 150.2534424180048
At time: 88.00445795059204 and batch: 100, loss is 4.971622371673584 and perplexity is 144.26074216162488
At time: 88.56159281730652 and batch: 150, loss is 4.95365309715271 and perplexity is 141.69163292666423
At time: 89.12003016471863 and batch: 200, loss is 4.947621307373047 and perplexity is 140.83955115577493
At time: 89.67983746528625 and batch: 250, loss is 4.91581654548645 and perplexity is 136.43066617726524
At time: 90.23914241790771 and batch: 300, loss is 4.970109691619873 and perplexity is 144.04268678001614
At time: 90.80770516395569 and batch: 350, loss is 4.926148929595947 and perplexity is 137.84762791499793
At time: 91.37117600440979 and batch: 400, loss is 4.979429264068603 and perplexity is 145.39137788577202
At time: 91.9271309375763 and batch: 450, loss is 4.939052267074585 and perplexity is 139.6378474433344
At time: 92.4869704246521 and batch: 500, loss is 4.930559883117676 and perplexity is 138.45701038565124
At time: 93.06296157836914 and batch: 550, loss is 4.956409187316894 and perplexity is 142.08268648441475
At time: 93.62250399589539 and batch: 600, loss is 5.036320676803589 and perplexity is 153.90271419798614
At time: 94.18451476097107 and batch: 650, loss is 4.98704758644104 and perplexity is 146.5032461806083
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.255015055338542 and perplexity of 191.52436929689026
Finished 12 epochs...
Completing Train Step...
At time: 95.23613953590393 and batch: 50, loss is 4.992647123336792 and perplexity is 147.32589759691734
At time: 95.80899500846863 and batch: 100, loss is 4.953127603530884 and perplexity is 141.6171944374838
At time: 96.37957096099854 and batch: 150, loss is 4.934340782165528 and perplexity is 138.9814932479655
At time: 96.9519510269165 and batch: 200, loss is 4.927706108093262 and perplexity is 138.06244849067548
At time: 97.52644896507263 and batch: 250, loss is 4.897015752792359 and perplexity is 133.88962316485862
At time: 98.09378409385681 and batch: 300, loss is 4.951532287597656 and perplexity is 141.39145038519752
At time: 98.65917110443115 and batch: 350, loss is 4.907525596618652 and perplexity is 135.30420267836067
At time: 99.21600770950317 and batch: 400, loss is 4.959927005767822 and perplexity is 142.58338775221617
At time: 99.77623128890991 and batch: 450, loss is 4.920420942306518 and perplexity is 137.0602955220787
At time: 100.33827662467957 and batch: 500, loss is 4.910640106201172 and perplexity is 135.72626583285358
At time: 100.90192151069641 and batch: 550, loss is 4.937679061889648 and perplexity is 139.44622762398788
At time: 101.46970868110657 and batch: 600, loss is 5.02069598197937 and perplexity is 151.5167200126053
At time: 102.02594590187073 and batch: 650, loss is 4.97140474319458 and perplexity is 144.22935033173025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.250906551585478 and perplexity of 190.739104942197
Finished 13 epochs...
Completing Train Step...
At time: 103.14945387840271 and batch: 50, loss is 4.977747364044189 and perplexity is 145.147049649186
At time: 103.70261788368225 and batch: 100, loss is 4.9385274887084964 and perplexity is 139.56458774614234
At time: 104.26206636428833 and batch: 150, loss is 4.917570810317994 and perplexity is 136.6702117486374
At time: 104.82023024559021 and batch: 200, loss is 4.911883707046509 and perplexity is 135.89516012855532
At time: 105.37698793411255 and batch: 250, loss is 4.883650512695312 and perplexity is 132.11206143476818
At time: 105.93912363052368 and batch: 300, loss is 4.936576843261719 and perplexity is 139.29261206882157
At time: 106.49613308906555 and batch: 350, loss is 4.892915830612183 and perplexity is 133.34180989240855
At time: 107.05806303024292 and batch: 400, loss is 4.947324743270874 and perplexity is 140.79778939356186
At time: 107.6302855014801 and batch: 450, loss is 4.905600471496582 and perplexity is 135.04397572375584
At time: 108.1896460056305 and batch: 500, loss is 4.899604206085205 and perplexity is 134.23663912436598
At time: 108.75680994987488 and batch: 550, loss is 4.92492280960083 and perplexity is 137.67871375780643
At time: 109.325430393219 and batch: 600, loss is 5.005717945098877 and perplexity is 149.26420820555413
At time: 109.8981294631958 and batch: 650, loss is 4.957114400863648 and perplexity is 142.1829204587069
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.246534160539215 and perplexity of 189.90693958950814
Finished 14 epochs...
Completing Train Step...
At time: 110.98561906814575 and batch: 50, loss is 4.962626295089722 and perplexity is 142.96878147883805
At time: 111.56655645370483 and batch: 100, loss is 4.92303318977356 and perplexity is 137.41879897789633
At time: 112.1266930103302 and batch: 150, loss is 4.902486515045166 and perplexity is 134.6241087273749
At time: 112.68980717658997 and batch: 200, loss is 4.8965811347961425 and perplexity is 133.83144496868587
At time: 113.24785304069519 and batch: 250, loss is 4.868608655929566 and perplexity is 130.1397217431129
At time: 113.81444120407104 and batch: 300, loss is 4.922354669570923 and perplexity is 137.32558917251177
At time: 114.38727736473083 and batch: 350, loss is 4.877412281036377 and perplexity is 131.29048105930443
At time: 114.9482045173645 and batch: 400, loss is 4.934355545043945 and perplexity is 138.98354502999766
At time: 115.51704549789429 and batch: 450, loss is 4.892403020858764 and perplexity is 133.27344844146472
At time: 116.09004044532776 and batch: 500, loss is 4.88201340675354 and perplexity is 131.89595693525902
At time: 116.64596652984619 and batch: 550, loss is 4.912974634170532 and perplexity is 136.04349274008655
At time: 117.21404027938843 and batch: 600, loss is 4.993033237457276 and perplexity is 147.38279318968145
At time: 117.77586126327515 and batch: 650, loss is 4.942886657714844 and perplexity is 140.17430132820468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.248758353439032 and perplexity of 190.32979934267428
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 118.85850787162781 and batch: 50, loss is 4.9218863677978515 and perplexity is 137.26129441146458
At time: 119.41331028938293 and batch: 100, loss is 4.849863481521607 and perplexity is 127.72295211269119
At time: 119.97900795936584 and batch: 150, loss is 4.809667892456055 and perplexity is 122.69086418882561
At time: 120.54292392730713 and batch: 200, loss is 4.794709091186523 and perplexity is 120.86921474825388
At time: 121.1126594543457 and batch: 250, loss is 4.761483001708984 and perplexity is 116.91918874941204
At time: 121.68107056617737 and batch: 300, loss is 4.812650051116943 and perplexity is 123.05729391630771
At time: 122.26349759101868 and batch: 350, loss is 4.754062557220459 and perplexity is 116.0548074127982
At time: 122.8306622505188 and batch: 400, loss is 4.788176832199096 and perplexity is 120.0822388973286
At time: 123.39774680137634 and batch: 450, loss is 4.747954769134521 and perplexity is 115.34812955894252
At time: 123.96106338500977 and batch: 500, loss is 4.743110589981079 and perplexity is 114.7907137553817
At time: 124.52894878387451 and batch: 550, loss is 4.768754119873047 and perplexity is 117.7724202006746
At time: 125.09972739219666 and batch: 600, loss is 4.862619752883911 and perplexity is 129.36265677316595
At time: 125.66668438911438 and batch: 650, loss is 4.821399660110473 and perplexity is 124.13872125595947
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.101310580384498 and perplexity of 164.23701211952644
Finished 16 epochs...
Completing Train Step...
At time: 126.72295618057251 and batch: 50, loss is 4.841860942840576 and perplexity is 126.7049230892022
At time: 127.30609965324402 and batch: 100, loss is 4.808440494537353 and perplexity is 122.54036605691734
At time: 127.86996674537659 and batch: 150, loss is 4.779253187179566 and perplexity is 119.0154345949624
At time: 128.43930196762085 and batch: 200, loss is 4.770128307342529 and perplexity is 117.9343728359201
At time: 129.0045120716095 and batch: 250, loss is 4.7383277893066404 and perplexity is 114.24300349080522
At time: 129.56277799606323 and batch: 300, loss is 4.792328214645385 and perplexity is 120.58178237658704
At time: 130.13207840919495 and batch: 350, loss is 4.73943585395813 and perplexity is 114.36966228476022
At time: 130.69930863380432 and batch: 400, loss is 4.782929620742798 and perplexity is 119.45379223587916
At time: 131.25682830810547 and batch: 450, loss is 4.747654819488526 and perplexity is 115.31353611671979
At time: 131.82003784179688 and batch: 500, loss is 4.742693634033203 and perplexity is 114.74286106144642
At time: 132.38360571861267 and batch: 550, loss is 4.765978260040283 and perplexity is 117.44595379215667
At time: 132.94124341011047 and batch: 600, loss is 4.855339546203613 and perplexity is 128.42428979248615
At time: 133.49954462051392 and batch: 650, loss is 4.807056217193604 and perplexity is 122.37085355771468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.095994537951899 and perplexity of 163.36623778145594
Finished 17 epochs...
Completing Train Step...
At time: 134.5851309299469 and batch: 50, loss is 4.8255504131317135 and perplexity is 124.65506128691301
At time: 135.14710760116577 and batch: 100, loss is 4.796860980987549 and perplexity is 121.12959202986106
At time: 135.70520520210266 and batch: 150, loss is 4.768582210540772 and perplexity is 117.75217576271123
At time: 136.27872467041016 and batch: 200, loss is 4.760381336212158 and perplexity is 116.79045383767529
At time: 136.85722661018372 and batch: 250, loss is 4.729543542861938 and perplexity is 113.24385958146136
At time: 137.42979741096497 and batch: 300, loss is 4.784198017120361 and perplexity is 119.60540312426181
At time: 137.98699283599854 and batch: 350, loss is 4.7320746898651125 and perplexity is 113.53085950353824
At time: 138.55156421661377 and batch: 400, loss is 4.776360368728637 and perplexity is 118.67164205427271
At time: 139.12177729606628 and batch: 450, loss is 4.743786745071411 and perplexity is 114.8683563270622
At time: 139.69380569458008 and batch: 500, loss is 4.7378844165802 and perplexity is 114.19236248612049
At time: 140.26290035247803 and batch: 550, loss is 4.7604814147949215 and perplexity is 116.80214264566555
At time: 140.82425618171692 and batch: 600, loss is 4.849028453826905 and perplexity is 127.61634442680763
At time: 141.38337683677673 and batch: 650, loss is 4.797311296463013 and perplexity is 121.18415084310577
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.091883042279412 and perplexity of 162.69593711518343
Finished 18 epochs...
Completing Train Step...
At time: 142.47099995613098 and batch: 50, loss is 4.816025838851929 and perplexity is 123.47341118642868
At time: 143.05023217201233 and batch: 100, loss is 4.788096399307251 and perplexity is 120.07258072401741
At time: 143.61468744277954 and batch: 150, loss is 4.760282735824585 and perplexity is 116.77893882136395
At time: 144.17391800880432 and batch: 200, loss is 4.753264646530152 and perplexity is 115.96224297529007
At time: 144.74059414863586 and batch: 250, loss is 4.722436456680298 and perplexity is 112.44187896028204
At time: 145.3066487312317 and batch: 300, loss is 4.777012853622437 and perplexity is 118.74909877495757
At time: 145.87242555618286 and batch: 350, loss is 4.725581703186035 and perplexity is 112.79609314062934
At time: 146.43612408638 and batch: 400, loss is 4.770144453048706 and perplexity is 117.936276985024
At time: 147.006822347641 and batch: 450, loss is 4.738755264282227 and perplexity is 114.29184995551017
At time: 147.57923769950867 and batch: 500, loss is 4.7324051189422605 and perplexity is 113.56837959919572
At time: 148.14113998413086 and batch: 550, loss is 4.753583545684815 and perplexity is 115.99922913366156
At time: 148.70332217216492 and batch: 600, loss is 4.840802755355835 and perplexity is 126.5709164399183
At time: 149.26225900650024 and batch: 650, loss is 4.785910682678223 and perplexity is 119.81042269359043
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.085287655101103 and perplexity of 161.6264252170016
Finished 19 epochs...
Completing Train Step...
At time: 150.33259630203247 and batch: 50, loss is 4.803369436264038 and perplexity is 121.92052966134011
At time: 150.88934183120728 and batch: 100, loss is 4.778070850372314 and perplexity is 118.87480142028195
At time: 151.46788024902344 and batch: 150, loss is 4.750035047531128 and perplexity is 115.58833554198614
At time: 152.03666186332703 and batch: 200, loss is 4.7434092712402345 and perplexity is 114.82500471108352
At time: 152.6042628288269 and batch: 250, loss is 4.712272653579712 and perplexity is 111.30483000148998
At time: 153.17121839523315 and batch: 300, loss is 4.768051338195801 and perplexity is 117.6896809788826
At time: 153.74152064323425 and batch: 350, loss is 4.716076736450195 and perplexity is 111.72904916956153
At time: 154.30440878868103 and batch: 400, loss is 4.761145668029785 and perplexity is 116.87975462090601
At time: 154.8756628036499 and batch: 450, loss is 4.7302029418945315 and perplexity is 113.31855709794499
At time: 155.4404957294464 and batch: 500, loss is 4.724752159118652 and perplexity is 112.70256260995255
At time: 156.00421833992004 and batch: 550, loss is 4.743916816711426 and perplexity is 114.88329841430534
At time: 156.56236457824707 and batch: 600, loss is 4.83111086845398 and perplexity is 125.35013084662533
At time: 157.1287658214569 and batch: 650, loss is 4.7751753044128415 and perplexity is 118.53109182295795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.080442540785846 and perplexity of 160.84522075116843
Finished 20 epochs...
Completing Train Step...
At time: 158.2154507637024 and batch: 50, loss is 4.79411548614502 and perplexity is 120.79748746396174
At time: 158.7959849834442 and batch: 100, loss is 4.7706887149810795 and perplexity is 118.00048268180615
At time: 159.3567600250244 and batch: 150, loss is 4.742911729812622 and perplexity is 114.76788872427704
At time: 159.92274284362793 and batch: 200, loss is 4.736369543075561 and perplexity is 114.01950646231988
At time: 160.48474144935608 and batch: 250, loss is 4.704822387695312 and perplexity is 110.47866083521187
At time: 161.04918432235718 and batch: 300, loss is 4.7603496742248534 and perplexity is 116.78675607834806
At time: 161.62445878982544 and batch: 350, loss is 4.708811931610107 and perplexity is 110.92030068927937
At time: 162.19431567192078 and batch: 400, loss is 4.7542306995391845 and perplexity is 116.07432277784918
At time: 162.76254773139954 and batch: 450, loss is 4.723771677017212 and perplexity is 112.59211391985724
At time: 163.3281729221344 and batch: 500, loss is 4.717380199432373 and perplexity is 111.87477880514761
At time: 163.8989601135254 and batch: 550, loss is 4.735896263122559 and perplexity is 113.96555608348346
At time: 164.47315645217896 and batch: 600, loss is 4.823308143615723 and perplexity is 124.37586417753735
At time: 165.04481315612793 and batch: 650, loss is 4.766819715499878 and perplexity is 117.54482092149173
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.07649201037837 and perplexity of 160.21105029530344
Finished 21 epochs...
Completing Train Step...
At time: 166.1186945438385 and batch: 50, loss is 4.78539083480835 and perplexity is 119.74815568665234
At time: 166.6782443523407 and batch: 100, loss is 4.763400974273682 and perplexity is 117.14365173383362
At time: 167.24302053451538 and batch: 150, loss is 4.736122007369995 and perplexity is 113.99128605626261
At time: 167.79885864257812 and batch: 200, loss is 4.729096069335937 and perplexity is 113.19319728817935
At time: 168.3645281791687 and batch: 250, loss is 4.69825439453125 and perplexity is 109.75541548131841
At time: 168.92237782478333 and batch: 300, loss is 4.753752956390381 and perplexity is 116.0188823095964
At time: 169.47946524620056 and batch: 350, loss is 4.703366117477417 and perplexity is 110.31789114219487
At time: 170.04422783851624 and batch: 400, loss is 4.74821102142334 and perplexity is 115.3776915686575
At time: 170.6001534461975 and batch: 450, loss is 4.718377094268799 and perplexity is 111.98636180348129
At time: 171.1776671409607 and batch: 500, loss is 4.7103512001037595 and perplexity is 111.0911682852533
At time: 171.748539686203 and batch: 550, loss is 4.729509382247925 and perplexity is 113.23999116775892
At time: 172.31683778762817 and batch: 600, loss is 4.816345901489258 and perplexity is 123.51293673704153
At time: 172.87383460998535 and batch: 650, loss is 4.759546241760254 and perplexity is 116.6929634901299
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.074311499502144 and perplexity of 159.8620889529093
Finished 22 epochs...
Completing Train Step...
At time: 173.95626997947693 and batch: 50, loss is 4.778498907089233 and perplexity is 118.92569746992308
At time: 174.53952169418335 and batch: 100, loss is 4.756581764221192 and perplexity is 116.34754207088349
At time: 175.10296201705933 and batch: 150, loss is 4.729783782958984 and perplexity is 113.2710685654907
At time: 175.67206239700317 and batch: 200, loss is 4.723084344863891 and perplexity is 112.51475232934968
At time: 176.2369954586029 and batch: 250, loss is 4.692596521377563 and perplexity is 109.13618667348969
At time: 176.80522966384888 and batch: 300, loss is 4.747780494689941 and perplexity is 115.3280290792811
At time: 177.36267042160034 and batch: 350, loss is 4.696981658935547 and perplexity is 109.61581471349182
At time: 177.92712688446045 and batch: 400, loss is 4.742265644073487 and perplexity is 114.69376277649899
At time: 178.48726320266724 and batch: 450, loss is 4.713166828155518 and perplexity is 111.40440046069399
At time: 179.04796147346497 and batch: 500, loss is 4.704306135177612 and perplexity is 110.42164066807237
At time: 179.61086010932922 and batch: 550, loss is 4.722323455810547 and perplexity is 112.42917364803226
At time: 180.1895613670349 and batch: 600, loss is 4.809287157058716 and perplexity is 122.64416032536997
At time: 180.75352668762207 and batch: 650, loss is 4.751788873672485 and perplexity is 115.79123525990192
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.070422602634804 and perplexity of 159.24160904913728
Finished 23 epochs...
Completing Train Step...
At time: 181.8013985157013 and batch: 50, loss is 4.771067943572998 and perplexity is 118.04524032485183
At time: 182.3648383617401 and batch: 100, loss is 4.7504281902313235 and perplexity is 115.63378718623788
At time: 182.92255973815918 and batch: 150, loss is 4.72243935585022 and perplexity is 112.44220494886802
At time: 183.48561000823975 and batch: 200, loss is 4.716093168258667 and perplexity is 111.730885094982
At time: 184.05852699279785 and batch: 250, loss is 4.685229034423828 and perplexity is 108.33508193001681
At time: 184.62616181373596 and batch: 300, loss is 4.741190710067749 and perplexity is 114.57054079026122
At time: 185.18558764457703 and batch: 350, loss is 4.690841836929321 and perplexity is 108.94485501642602
At time: 185.74826741218567 and batch: 400, loss is 4.735759077072143 and perplexity is 113.94992267132851
At time: 186.3190541267395 and batch: 450, loss is 4.705999898910522 and perplexity is 110.60882731859371
At time: 186.88412189483643 and batch: 500, loss is 4.6972012710571285 and perplexity is 109.63989031867052
At time: 187.4443187713623 and batch: 550, loss is 4.71429970741272 and perplexity is 111.530679711203
At time: 188.01440834999084 and batch: 600, loss is 4.801741781234742 and perplexity is 121.72224651012179
At time: 188.59127259254456 and batch: 650, loss is 4.7436245250701905 and perplexity is 114.8497238934762
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.068063773360907 and perplexity of 158.8664279481331
Finished 24 epochs...
Completing Train Step...
At time: 189.68649768829346 and batch: 50, loss is 4.764128637313843 and perplexity is 117.2289238605147
At time: 190.26033091545105 and batch: 100, loss is 4.742784481048584 and perplexity is 114.7532855814213
At time: 190.82320761680603 and batch: 150, loss is 4.716413335800171 and perplexity is 111.76666342499725
At time: 191.38814115524292 and batch: 200, loss is 4.708654489517212 and perplexity is 110.90283853966903
At time: 191.95004415512085 and batch: 250, loss is 4.6778837585449216 and perplexity is 107.5422462290095
At time: 192.52293252944946 and batch: 300, loss is 4.734658727645874 and perplexity is 113.82460689750762
At time: 193.08708000183105 and batch: 350, loss is 4.684163913726807 and perplexity is 108.21975342233311
At time: 193.65471053123474 and batch: 400, loss is 4.729250078201294 and perplexity is 113.21063138652819
At time: 194.23169350624084 and batch: 450, loss is 4.699299011230469 and perplexity is 109.87012772589166
At time: 194.81769609451294 and batch: 500, loss is 4.68999981880188 and perplexity is 108.85316008342622
At time: 195.38074159622192 and batch: 550, loss is 4.707515916824341 and perplexity is 110.77663945319591
At time: 195.9475224018097 and batch: 600, loss is 4.795773639678955 and perplexity is 120.99795440123927
At time: 196.52740359306335 and batch: 650, loss is 4.736117429733277 and perplexity is 113.99076424676029
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.064097086588542 and perplexity of 158.23750278903017
Finished 25 epochs...
Completing Train Step...
At time: 197.65227437019348 and batch: 50, loss is 4.755079879760742 and perplexity is 116.17293265983781
At time: 198.21978950500488 and batch: 100, loss is 4.735086870193482 and perplexity is 113.87335048854321
At time: 198.78666949272156 and batch: 150, loss is 4.709972019195557 and perplexity is 111.04905262040224
At time: 199.35589408874512 and batch: 200, loss is 4.700821371078491 and perplexity is 110.03751697784855
At time: 199.92420172691345 and batch: 250, loss is 4.6692432689666745 and perplexity is 106.61703148165856
At time: 200.50223422050476 and batch: 300, loss is 4.727409324645996 and perplexity is 113.0024301966348
At time: 201.0724856853485 and batch: 350, loss is 4.677215957641602 and perplexity is 107.47045339416078
At time: 201.6443829536438 and batch: 400, loss is 4.721943893432617 and perplexity is 112.38650786120972
At time: 202.20795154571533 and batch: 450, loss is 4.691624355316162 and perplexity is 109.0301397327055
At time: 202.77935004234314 and batch: 500, loss is 4.681451654434204 and perplexity is 107.92663108205834
At time: 203.3464217185974 and batch: 550, loss is 4.698355093002319 and perplexity is 109.76646824033757
At time: 203.91511273384094 and batch: 600, loss is 4.7879301452636716 and perplexity is 120.0526198312846
At time: 204.47981476783752 and batch: 650, loss is 4.726160650253296 and perplexity is 112.86141501508088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.0593830183440565 and perplexity of 157.4933158541814
Finished 26 epochs...
Completing Train Step...
At time: 205.55353093147278 and batch: 50, loss is 4.74497727394104 and perplexity is 115.0051918586397
At time: 206.15187215805054 and batch: 100, loss is 4.7266480922698975 and perplexity is 112.91644182091143
At time: 206.717627286911 and batch: 150, loss is 4.701207704544068 and perplexity is 110.08003636592848
At time: 207.2793674468994 and batch: 200, loss is 4.690710096359253 and perplexity is 108.93050350447912
At time: 207.84979605674744 and batch: 250, loss is 4.658488597869873 and perplexity is 105.4765441504807
At time: 208.41909837722778 and batch: 300, loss is 4.718199310302734 and perplexity is 111.966454193614
At time: 208.97962832450867 and batch: 350, loss is 4.668355655670166 and perplexity is 106.52243877396421
At time: 209.5637674331665 and batch: 400, loss is 4.711550941467285 and perplexity is 111.2245289381169
At time: 210.13288521766663 and batch: 450, loss is 4.682037849426269 and perplexity is 107.98991567945399
At time: 210.69278144836426 and batch: 500, loss is 4.6720403289794925 and perplexity is 106.91566316777866
At time: 211.26162123680115 and batch: 550, loss is 4.688606300354004 and perplexity is 108.70157683829449
At time: 211.83513069152832 and batch: 600, loss is 4.777844104766846 and perplexity is 118.84785013711695
At time: 212.40654373168945 and batch: 650, loss is 4.715052881240845 and perplexity is 111.61471334218774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.054589365042892 and perplexity of 156.74015414104028
Finished 27 epochs...
Completing Train Step...
At time: 213.47931337356567 and batch: 50, loss is 4.733122911453247 and perplexity is 113.64992739525425
At time: 214.0354573726654 and batch: 100, loss is 4.714466228485107 and perplexity is 111.54925346601044
At time: 214.59278464317322 and batch: 150, loss is 4.6891483974456785 and perplexity is 108.76051962187128
At time: 215.16087651252747 and batch: 200, loss is 4.6781526470184325 and perplexity is 107.57116698749101
At time: 215.726007938385 and batch: 250, loss is 4.646095924377441 and perplexity is 104.17747388027874
At time: 216.29284191131592 and batch: 300, loss is 4.707483234405518 and perplexity is 110.77301906383137
At time: 216.8518090248108 and batch: 350, loss is 4.658247671127319 and perplexity is 105.45113509126655
At time: 217.41721558570862 and batch: 400, loss is 4.700434255599975 and perplexity is 109.99492799576795
At time: 217.98615837097168 and batch: 450, loss is 4.670167093276977 and perplexity is 106.71557239747212
At time: 218.55683040618896 and batch: 500, loss is 4.659876823425293 and perplexity is 105.62307106724113
At time: 219.12323641777039 and batch: 550, loss is 4.675926418304443 and perplexity is 107.33195533546662
At time: 219.68133115768433 and batch: 600, loss is 4.764728641510009 and perplexity is 117.29928281246549
At time: 220.23728156089783 and batch: 650, loss is 4.70323938369751 and perplexity is 110.30391102475434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.048177083333333 and perplexity of 155.7383076113484
Finished 28 epochs...
Completing Train Step...
At time: 221.2917766571045 and batch: 50, loss is 4.721804618835449 and perplexity is 112.37085636555341
At time: 221.88002610206604 and batch: 100, loss is 4.703920602798462 and perplexity is 110.37907775545362
At time: 222.43950939178467 and batch: 150, loss is 4.67857873916626 and perplexity is 107.61701198347977
At time: 223.00670433044434 and batch: 200, loss is 4.668666753768921 and perplexity is 106.55558285740487
At time: 223.57564544677734 and batch: 250, loss is 4.637265214920044 and perplexity is 103.2615629003091
At time: 224.16436433792114 and batch: 300, loss is 4.697227687835693 and perplexity is 109.64278668963115
At time: 224.72952270507812 and batch: 350, loss is 4.649517765045166 and perplexity is 104.53456320001165
At time: 225.29257440567017 and batch: 400, loss is 4.691274108886719 and perplexity is 108.99195900228501
At time: 225.8501341342926 and batch: 450, loss is 4.662237558364868 and perplexity is 105.87271369563281
At time: 226.41808676719666 and batch: 500, loss is 4.650980720520019 and perplexity is 104.68760453058728
At time: 226.97936582565308 and batch: 550, loss is 4.66488941192627 and perplexity is 106.15384522357287
At time: 227.54211711883545 and batch: 600, loss is 4.755724039077759 and perplexity is 116.24779064444145
At time: 228.09907722473145 and batch: 650, loss is 4.695676069259644 and perplexity is 109.47279482044964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.04362278358609 and perplexity of 155.03064136164892
Finished 29 epochs...
Completing Train Step...
At time: 229.12369847297668 and batch: 50, loss is 4.713312082290649 and perplexity is 111.42058358583722
At time: 229.68550515174866 and batch: 100, loss is 4.695960540771484 and perplexity is 109.50394114180807
At time: 230.2462272644043 and batch: 150, loss is 4.67078127861023 and perplexity is 106.78113566880711
At time: 230.80645155906677 and batch: 200, loss is 4.660241746902466 and perplexity is 105.66162243932726
At time: 231.3697648048401 and batch: 250, loss is 4.629935684204102 and perplexity is 102.50747104889074
At time: 231.93825221061707 and batch: 300, loss is 4.690064392089844 and perplexity is 108.86018931682597
At time: 232.50495314598083 and batch: 350, loss is 4.6411878490448 and perplexity is 103.66741571636386
At time: 233.06469178199768 and batch: 400, loss is 4.6835205936431885 and perplexity is 108.15015587066398
At time: 233.63531112670898 and batch: 450, loss is 4.653538827896118 and perplexity is 104.95574948949528
At time: 234.1952896118164 and batch: 500, loss is 4.6435676860809325 and perplexity is 103.91442107139301
At time: 234.75191712379456 and batch: 550, loss is 4.657823524475098 and perplexity is 105.40641782935347
At time: 235.3240509033203 and batch: 600, loss is 4.748334760665894 and perplexity is 115.39196920015323
At time: 235.8860423564911 and batch: 650, loss is 4.687323055267334 and perplexity is 108.5621755360511
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.038781259574142 and perplexity of 154.28187084674238
Finished 30 epochs...
Completing Train Step...
At time: 236.94860219955444 and batch: 50, loss is 4.705724611282348 and perplexity is 110.57838226763128
At time: 237.52009296417236 and batch: 100, loss is 4.6883673572540285 and perplexity is 108.67560644939871
At time: 238.07518410682678 and batch: 150, loss is 4.663003082275391 and perplexity is 105.95379281948314
At time: 238.65979957580566 and batch: 200, loss is 4.652686138153076 and perplexity is 104.86629294319218
At time: 239.22577261924744 and batch: 250, loss is 4.623767700195312 and perplexity is 101.87715250235748
At time: 239.8000545501709 and batch: 300, loss is 4.68233943939209 and perplexity is 108.02248926611925
At time: 240.36529922485352 and batch: 350, loss is 4.633840579986572 and perplexity is 102.90853458629407
At time: 240.9305019378662 and batch: 400, loss is 4.67584924697876 and perplexity is 107.32367270578006
At time: 241.4858546257019 and batch: 450, loss is 4.647129850387573 and perplexity is 104.28524138240486
At time: 242.04697251319885 and batch: 500, loss is 4.636131658554077 and perplexity is 103.14457641623746
At time: 242.60397028923035 and batch: 550, loss is 4.650356388092041 and perplexity is 104.622265063168
At time: 243.17021656036377 and batch: 600, loss is 4.74197380065918 and perplexity is 114.66029504107719
At time: 243.72765922546387 and batch: 650, loss is 4.679297399520874 and perplexity is 107.69437986075758
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.03431911094516 and perplexity of 153.59497586174155
Finished 31 epochs...
Completing Train Step...
At time: 244.78277134895325 and batch: 50, loss is 4.6973024845123295 and perplexity is 109.65098791240096
At time: 245.33965063095093 and batch: 100, loss is 4.681311426162719 and perplexity is 107.91149777821788
At time: 245.90350461006165 and batch: 150, loss is 4.656788558959961 and perplexity is 105.29738225558819
At time: 246.46509528160095 and batch: 200, loss is 4.646092958450318 and perplexity is 104.17716489794151
At time: 247.02942514419556 and batch: 250, loss is 4.6169110202789305 and perplexity is 101.1810028419537
At time: 247.58375477790833 and batch: 300, loss is 4.67632116317749 and perplexity is 107.37433243807111
At time: 248.15426468849182 and batch: 350, loss is 4.62785174369812 and perplexity is 102.29407400840294
At time: 248.72905373573303 and batch: 400, loss is 4.669833517074585 and perplexity is 106.67998055872083
At time: 249.3041591644287 and batch: 450, loss is 4.642038717269897 and perplexity is 103.75566056341908
At time: 249.87295699119568 and batch: 500, loss is 4.6311869621276855 and perplexity is 102.63581666568919
At time: 250.43175625801086 and batch: 550, loss is 4.643428792953491 and perplexity is 103.89998907473995
At time: 250.98419666290283 and batch: 600, loss is 4.735731801986694 and perplexity is 113.94681471983577
At time: 251.53786873817444 and batch: 650, loss is 4.673006944656372 and perplexity is 107.019059488102
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.032746558095894 and perplexity of 153.35362945953946
Finished 32 epochs...
Completing Train Step...
At time: 252.58889055252075 and batch: 50, loss is 4.6914178657531735 and perplexity is 109.00762847104978
At time: 253.17108416557312 and batch: 100, loss is 4.676024656295777 and perplexity is 107.34249992909606
At time: 253.73791790008545 and batch: 150, loss is 4.650859088897705 and perplexity is 104.67487198176799
At time: 254.30181574821472 and batch: 200, loss is 4.639419984817505 and perplexity is 103.48430770332538
At time: 254.85670447349548 and batch: 250, loss is 4.610440797805786 and perplexity is 100.528452592671
At time: 255.40571117401123 and batch: 300, loss is 4.671456422805786 and perplexity is 106.8532526746984
At time: 255.96738147735596 and batch: 350, loss is 4.622091741561889 and perplexity is 101.70655360735012
At time: 256.51893734931946 and batch: 400, loss is 4.664204082489014 and perplexity is 106.08111979184929
At time: 257.06762528419495 and batch: 450, loss is 4.637977361679077 and perplexity is 103.3351264785842
At time: 257.6157531738281 and batch: 500, loss is 4.62588249206543 and perplexity is 102.092829451768
At time: 258.16335129737854 and batch: 550, loss is 4.637413167953492 and perplexity is 103.27684189203748
At time: 258.714497089386 and batch: 600, loss is 4.730706148147583 and perplexity is 113.37559405393631
At time: 259.26623344421387 and batch: 650, loss is 4.667462434768677 and perplexity is 106.42733318664617
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.029427921070772 and perplexity of 152.8455479622191
Finished 33 epochs...
Completing Train Step...
At time: 260.3616542816162 and batch: 50, loss is 4.6847302341461186 and perplexity is 108.28105783580116
At time: 260.9173147678375 and batch: 100, loss is 4.670683822631836 and perplexity is 106.7707297158259
At time: 261.46775364875793 and batch: 150, loss is 4.646389169692993 and perplexity is 104.20802791617517
At time: 262.01669430732727 and batch: 200, loss is 4.635000553131103 and perplexity is 103.02797498319102
At time: 262.56601190567017 and batch: 250, loss is 4.604413623809815 and perplexity is 99.92437239427274
At time: 263.127051115036 and batch: 300, loss is 4.66588791847229 and perplexity is 106.2598934690365
At time: 263.68934631347656 and batch: 350, loss is 4.617728872299194 and perplexity is 101.26378777783938
At time: 264.25067710876465 and batch: 400, loss is 4.659028434753418 and perplexity is 105.53349965131889
At time: 264.8053147792816 and batch: 450, loss is 4.632051658630371 and perplexity is 102.72460387887448
At time: 265.3583197593689 and batch: 500, loss is 4.620290269851685 and perplexity is 101.52349706338285
At time: 265.91295289993286 and batch: 550, loss is 4.630768566131592 and perplexity is 102.59288323315492
At time: 266.4671165943146 and batch: 600, loss is 4.723295841217041 and perplexity is 112.53855130575253
At time: 267.0434019565582 and batch: 650, loss is 4.660581026077271 and perplexity is 105.6974773094584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.02861052868413 and perplexity of 152.72066422144533
Finished 34 epochs...
Completing Train Step...
At time: 268.08929920196533 and batch: 50, loss is 4.677946586608886 and perplexity is 107.54900311239322
At time: 268.6717371940613 and batch: 100, loss is 4.663892059326172 and perplexity is 106.04802518874394
At time: 269.2263402938843 and batch: 150, loss is 4.640021953582764 and perplexity is 103.54662077763773
At time: 269.78870582580566 and batch: 200, loss is 4.62696177482605 and perplexity is 102.2030759654573
At time: 270.35463309288025 and batch: 250, loss is 4.59813081741333 and perplexity is 99.29853497457404
At time: 270.9084641933441 and batch: 300, loss is 4.660917301177978 and perplexity is 105.73302671614033
At time: 271.4625017642975 and batch: 350, loss is 4.6110263919830325 and perplexity is 100.58733870915754
At time: 272.0257215499878 and batch: 400, loss is 4.65140284538269 and perplexity is 104.73180509969633
At time: 272.58076071739197 and batch: 450, loss is 4.624551258087158 and perplexity is 101.95701043176908
At time: 273.1441378593445 and batch: 500, loss is 4.614200897216797 and perplexity is 100.90716111270346
At time: 273.700884103775 and batch: 550, loss is 4.623113651275634 and perplexity is 101.81054164657714
At time: 274.2539517879486 and batch: 600, loss is 4.715935344696045 and perplexity is 111.71325272008009
At time: 274.8070707321167 and batch: 650, loss is 4.65200436592102 and perplexity is 104.79482238267508
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.025118958716299 and perplexity of 152.1883591677324
Finished 35 epochs...
Completing Train Step...
At time: 275.8509647846222 and batch: 50, loss is 4.670834932327271 and perplexity is 106.78686502734489
At time: 276.4007182121277 and batch: 100, loss is 4.656294784545898 and perplexity is 105.24540193669452
At time: 276.95062732696533 and batch: 150, loss is 4.633512649536133 and perplexity is 102.87479327689694
At time: 277.502037525177 and batch: 200, loss is 4.6199872684478756 and perplexity is 101.49273996121092
At time: 278.06329560279846 and batch: 250, loss is 4.590499706268311 and perplexity is 98.54366074548311
At time: 278.632390499115 and batch: 300, loss is 4.651799507141114 and perplexity is 104.77335644203949
At time: 279.18573665618896 and batch: 350, loss is 4.602849960327148 and perplexity is 99.7682463982021
At time: 279.73749470710754 and batch: 400, loss is 4.644370069503784 and perplexity is 103.9978337402521
At time: 280.3076066970825 and batch: 450, loss is 4.616984052658081 and perplexity is 101.18839260115864
At time: 280.86122608184814 and batch: 500, loss is 4.606358785629272 and perplexity is 100.11893063056868
At time: 281.45248889923096 and batch: 550, loss is 4.615383882522583 and perplexity is 101.026603436876
At time: 282.01005601882935 and batch: 600, loss is 4.708574161529541 and perplexity is 110.89393029561765
At time: 282.5642147064209 and batch: 650, loss is 4.644558553695679 and perplexity is 104.01743753534808
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.025200338924632 and perplexity of 152.20074479207284
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 283.58907675743103 and batch: 50, loss is 4.6599927997589115 and perplexity is 105.63532155413859
At time: 284.16212463378906 and batch: 100, loss is 4.63234203338623 and perplexity is 102.75443684180655
At time: 284.7230670452118 and batch: 150, loss is 4.603818016052246 and perplexity is 99.86487438340187
At time: 285.28048515319824 and batch: 200, loss is 4.580206489562988 and perplexity is 97.53453199074838
At time: 285.8331069946289 and batch: 250, loss is 4.550644779205323 and perplexity is 94.69344499943298
At time: 286.38515305519104 and batch: 300, loss is 4.608499965667725 and perplexity is 100.33353295542342
At time: 286.9499864578247 and batch: 350, loss is 4.555624847412109 and perplexity is 95.16620101579788
At time: 287.51440048217773 and batch: 400, loss is 4.5935795116424565 and perplexity is 98.84762387481209
At time: 288.0778341293335 and batch: 450, loss is 4.561729183197022 and perplexity is 95.74890416056837
At time: 288.6406693458557 and batch: 500, loss is 4.5493520164489745 and perplexity is 94.57110793390628
At time: 289.2050402164459 and batch: 550, loss is 4.555935554504394 and perplexity is 95.19577442349515
At time: 289.7582895755768 and batch: 600, loss is 4.65561282157898 and perplexity is 105.17365293799593
At time: 290.3120241165161 and batch: 650, loss is 4.600660161972046 and perplexity is 99.5500130870271
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.991888906441483 and perplexity of 147.2142349498451
Finished 37 epochs...
Completing Train Step...
At time: 291.3743073940277 and batch: 50, loss is 4.63299090385437 and perplexity is 102.82113279752149
At time: 291.93000626564026 and batch: 100, loss is 4.613213968276978 and perplexity is 100.80762204224045
At time: 292.49836468696594 and batch: 150, loss is 4.5885982322692875 and perplexity is 98.35646057134234
At time: 293.0646598339081 and batch: 200, loss is 4.5665437030792235 and perplexity is 96.21100065713286
At time: 293.6211402416229 and batch: 250, loss is 4.540977849960327 and perplexity is 93.78246047238629
At time: 294.18445444107056 and batch: 300, loss is 4.5996208763122555 and perplexity is 99.44660593008909
At time: 294.74290013313293 and batch: 350, loss is 4.548860483169555 and perplexity is 94.52463450963823
At time: 295.307124376297 and batch: 400, loss is 4.5902266025543215 and perplexity is 98.51675178037982
At time: 295.9012806415558 and batch: 450, loss is 4.560309886932373 and perplexity is 95.613104491327
At time: 296.46698927879333 and batch: 500, loss is 4.550048065185547 and perplexity is 94.63695694850308
At time: 297.02597737312317 and batch: 550, loss is 4.557314281463623 and perplexity is 95.32711392394738
At time: 297.5776438713074 and batch: 600, loss is 4.653814334869384 and perplexity is 104.98466951401531
At time: 298.13214468955994 and batch: 650, loss is 4.5943813896179195 and perplexity is 98.92691939574107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.988877539541207 and perplexity of 146.77158569962532
Finished 38 epochs...
Completing Train Step...
At time: 299.17485547065735 and batch: 50, loss is 4.62417067527771 and perplexity is 101.91821472925298
At time: 299.7410695552826 and batch: 100, loss is 4.605653476715088 and perplexity is 100.04834075307761
At time: 300.29351592063904 and batch: 150, loss is 4.580282926559448 and perplexity is 97.5419875223605
At time: 300.8453223705292 and batch: 200, loss is 4.559471111297608 and perplexity is 95.5329401735536
At time: 301.40876388549805 and batch: 250, loss is 4.535725517272949 and perplexity is 93.29117511623545
At time: 301.97534823417664 and batch: 300, loss is 4.594900455474853 and perplexity is 98.97828231114568
At time: 302.5396137237549 and batch: 350, loss is 4.544673643112183 and perplexity is 94.12970231933964
At time: 303.1049840450287 and batch: 400, loss is 4.587363996505737 and perplexity is 98.23514039438056
At time: 303.670627117157 and batch: 450, loss is 4.558022546768188 and perplexity is 95.39465472690432
At time: 304.225647687912 and batch: 500, loss is 4.548193283081055 and perplexity is 94.46158869955173
At time: 304.790390253067 and batch: 550, loss is 4.555566873550415 and perplexity is 95.16068402354443
At time: 305.34866666793823 and batch: 600, loss is 4.650806179046631 and perplexity is 104.66933379639384
At time: 305.901976108551 and batch: 650, loss is 4.589694128036499 and perplexity is 98.46430808418319
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.986641678155637 and perplexity of 146.44379136656227
Finished 39 epochs...
Completing Train Step...
At time: 306.94985580444336 and batch: 50, loss is 4.6179600429534915 and perplexity is 101.28719969988701
At time: 307.5034658908844 and batch: 100, loss is 4.600050458908081 and perplexity is 99.48933563852297
At time: 308.0578291416168 and batch: 150, loss is 4.574769020080566 and perplexity is 97.00563019637796
At time: 308.6093444824219 and batch: 200, loss is 4.553976430892944 and perplexity is 95.00945670342728
At time: 309.1631484031677 and batch: 250, loss is 4.531100511550903 and perplexity is 92.8606991417765
At time: 309.71547985076904 and batch: 300, loss is 4.5906832408905025 and perplexity is 98.56174857884848
At time: 310.2824306488037 and batch: 350, loss is 4.541635932922364 and perplexity is 93.84419742355085
At time: 310.8401532173157 and batch: 400, loss is 4.584729843139648 and perplexity is 97.97671448481194
At time: 311.3931736946106 and batch: 450, loss is 4.555537919998169 and perplexity is 95.15792882359445
At time: 311.946896314621 and batch: 500, loss is 4.5457608127594 and perplexity is 94.23209292249584
At time: 312.5009939670563 and batch: 550, loss is 4.553157176971435 and perplexity is 94.93165170881781
At time: 313.05355525016785 and batch: 600, loss is 4.64756031036377 and perplexity is 104.33014166812261
At time: 313.6067340373993 and batch: 650, loss is 4.585848188400268 and perplexity is 98.08634757150213
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.984466851926317 and perplexity of 146.1256476470284
Finished 40 epochs...
Completing Train Step...
At time: 314.64674162864685 and batch: 50, loss is 4.613132648468017 and perplexity is 100.79942471898111
At time: 315.21379375457764 and batch: 100, loss is 4.595351657867432 and perplexity is 99.0229516256307
At time: 315.765896320343 and batch: 150, loss is 4.569907302856445 and perplexity is 96.53516082441908
At time: 316.3257930278778 and batch: 200, loss is 4.549223394393921 and perplexity is 94.55894478589641
At time: 316.8834488391876 and batch: 250, loss is 4.527412748336792 and perplexity is 92.51888153003377
At time: 317.4428114891052 and batch: 300, loss is 4.58734691619873 and perplexity is 98.23346252235308
At time: 317.9960775375366 and batch: 350, loss is 4.538693037033081 and perplexity is 93.56842969760056
At time: 318.5504376888275 and batch: 400, loss is 4.582579021453857 and perplexity is 97.7662105020109
At time: 319.1049690246582 and batch: 450, loss is 4.553275814056397 and perplexity is 94.94291479134354
At time: 319.6670045852661 and batch: 500, loss is 4.543343372344971 and perplexity is 94.00456757801548
At time: 320.2342596054077 and batch: 550, loss is 4.550944118499756 and perplexity is 94.72179471132628
At time: 320.7894141674042 and batch: 600, loss is 4.64472562789917 and perplexity is 104.03481761771482
At time: 321.34586238861084 and batch: 650, loss is 4.582252244949341 and perplexity is 97.7342680207937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.982525096220129 and perplexity of 145.84218263585578
Finished 41 epochs...
Completing Train Step...
At time: 322.3891589641571 and batch: 50, loss is 4.609008750915527 and perplexity is 100.38459416534475
At time: 322.9407160282135 and batch: 100, loss is 4.590607175827026 and perplexity is 98.55425175831317
At time: 323.50109243392944 and batch: 150, loss is 4.565154266357422 and perplexity is 96.07741438610489
At time: 324.06273221969604 and batch: 200, loss is 4.544461669921875 and perplexity is 94.10975146063464
At time: 324.6398572921753 and batch: 250, loss is 4.523214950561523 and perplexity is 92.13131999656613
At time: 325.1993668079376 and batch: 300, loss is 4.58351692199707 and perplexity is 97.8579484977818
At time: 325.7509756088257 and batch: 350, loss is 4.534834508895874 and perplexity is 93.20808891844564
At time: 326.30872416496277 and batch: 400, loss is 4.5792293357849125 and perplexity is 97.43927230358214
At time: 326.8704617023468 and batch: 450, loss is 4.550103940963745 and perplexity is 94.64224500985478
At time: 327.4232077598572 and batch: 500, loss is 4.540241966247558 and perplexity is 93.71347287371964
At time: 327.97524428367615 and batch: 550, loss is 4.547729835510254 and perplexity is 94.41782084857044
At time: 328.52679085731506 and batch: 600, loss is 4.641408996582031 and perplexity is 103.69034404521894
At time: 329.08432841300964 and batch: 650, loss is 4.578389263153076 and perplexity is 97.35745061054827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.980127671185662 and perplexity of 145.49295572606087
Finished 42 epochs...
Completing Train Step...
At time: 330.12008714675903 and batch: 50, loss is 4.60436939239502 and perplexity is 99.91995269565473
At time: 330.6883170604706 and batch: 100, loss is 4.585449142456055 and perplexity is 98.04721442080282
At time: 331.24535512924194 and batch: 150, loss is 4.560167589187622 and perplexity is 95.59949993016139
At time: 331.806355714798 and batch: 200, loss is 4.539581546783447 and perplexity is 93.65160310443304
At time: 332.36461210250854 and batch: 250, loss is 4.519250411987304 and perplexity is 91.76678490872166
At time: 332.916983127594 and batch: 300, loss is 4.5797781562805175 and perplexity is 97.4927636505303
At time: 333.4715542793274 and batch: 350, loss is 4.531411981582641 and perplexity is 92.88962697152755
At time: 334.03016567230225 and batch: 400, loss is 4.575830783843994 and perplexity is 97.10868195800084
At time: 334.599817276001 and batch: 450, loss is 4.546668653488159 and perplexity is 94.31767949801846
At time: 335.1658511161804 and batch: 500, loss is 4.537131395339966 and perplexity is 93.42242337108138
At time: 335.7326760292053 and batch: 550, loss is 4.545075063705444 and perplexity is 94.16749550526228
At time: 336.2885682582855 and batch: 600, loss is 4.637734413146973 and perplexity is 103.31002441067014
At time: 336.8498320579529 and batch: 650, loss is 4.574161634445191 and perplexity is 96.946728259952
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.976983463062959 and perplexity of 145.0362140146511
Finished 43 epochs...
Completing Train Step...
At time: 337.9176695346832 and batch: 50, loss is 4.5992600536346435 and perplexity is 99.41072981230565
At time: 338.4887204170227 and batch: 100, loss is 4.580315885543823 and perplexity is 97.54520246018338
At time: 339.06929421424866 and batch: 150, loss is 4.554681129455567 and perplexity is 95.07643332739535
At time: 339.62717294692993 and batch: 200, loss is 4.5348331737518315 and perplexity is 93.20796447230408
At time: 340.1778054237366 and batch: 250, loss is 4.514854183197022 and perplexity is 91.36424260925929
At time: 340.729704618454 and batch: 300, loss is 4.575626459121704 and perplexity is 97.08884228046514
At time: 341.28294944763184 and batch: 350, loss is 4.5272039031982425 and perplexity is 92.49956142892704
At time: 341.83588886260986 and batch: 400, loss is 4.5720447826385495 and perplexity is 96.74172346202863
At time: 342.38936138153076 and batch: 450, loss is 4.542836437225342 and perplexity is 93.95692543806184
At time: 342.9426155090332 and batch: 500, loss is 4.532822761535645 and perplexity is 93.0207662777025
At time: 343.4967110157013 and batch: 550, loss is 4.5410247421264645 and perplexity is 93.78685823821313
At time: 344.0498106479645 and batch: 600, loss is 4.6341228199005124 and perplexity is 102.93758358143957
At time: 344.6027228832245 and batch: 650, loss is 4.569958505630493 and perplexity is 96.54010381899283
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.973936791513481 and perplexity of 144.59500875280781
Finished 44 epochs...
Completing Train Step...
At time: 345.62449312210083 and batch: 50, loss is 4.5943528842926025 and perplexity is 98.92409949191241
At time: 346.1996257305145 and batch: 100, loss is 4.575516233444214 and perplexity is 97.07814118682651
At time: 346.7639162540436 and batch: 150, loss is 4.550096397399902 and perplexity is 94.64153107273013
At time: 347.33106565475464 and batch: 200, loss is 4.530114374160767 and perplexity is 92.76917087142073
At time: 347.88476610183716 and batch: 250, loss is 4.510068521499634 and perplexity is 90.9280488233361
At time: 348.4400496482849 and batch: 300, loss is 4.5717290496826175 and perplexity is 96.71118373317115
At time: 349.00071024894714 and batch: 350, loss is 4.523141489028931 and perplexity is 92.12455213719116
At time: 349.559118270874 and batch: 400, loss is 4.5670716381073 and perplexity is 96.26180722456928
At time: 350.1247310638428 and batch: 450, loss is 4.5385981369018555 and perplexity is 93.55955046267064
At time: 350.69059205055237 and batch: 500, loss is 4.528557958602906 and perplexity is 92.62489579569751
At time: 351.2559132575989 and batch: 550, loss is 4.537534475326538 and perplexity is 93.46008767059207
At time: 351.8209731578827 and batch: 600, loss is 4.630279941558838 and perplexity is 102.54276607465222
At time: 352.3783800601959 and batch: 650, loss is 4.565435094833374 and perplexity is 96.10439944887003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.971882539636948 and perplexity of 144.29827906784493
Finished 45 epochs...
Completing Train Step...
At time: 353.43287229537964 and batch: 50, loss is 4.589385032653809 and perplexity is 98.43387792434775
At time: 353.9981243610382 and batch: 100, loss is 4.570684280395508 and perplexity is 96.61019562251096
At time: 354.5661823749542 and batch: 150, loss is 4.545418157577514 and perplexity is 94.1998093389405
At time: 355.13064432144165 and batch: 200, loss is 4.52617259979248 and perplexity is 92.4042154899404
At time: 355.68358516693115 and batch: 250, loss is 4.506354093551636 and perplexity is 90.59092962761295
At time: 356.23584175109863 and batch: 300, loss is 4.568500080108643 and perplexity is 96.39940988844874
At time: 356.8022999763489 and batch: 350, loss is 4.520430107116699 and perplexity is 91.87510561806296
At time: 357.357896566391 and batch: 400, loss is 4.564573402404785 and perplexity is 96.02162268468825
At time: 357.9160315990448 and batch: 450, loss is 4.536169338226318 and perplexity is 93.33258888398561
At time: 358.47116923332214 and batch: 500, loss is 4.524855928421021 and perplexity is 92.28262956672008
At time: 359.0243492126465 and batch: 550, loss is 4.53431321144104 and perplexity is 93.15951244141948
At time: 359.5772178173065 and batch: 600, loss is 4.6265746688842775 and perplexity is 102.16352020411271
At time: 360.13043880462646 and batch: 650, loss is 4.561190824508667 and perplexity is 95.69737077907963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.969006407494638 and perplexity of 143.88385440506588
Finished 46 epochs...
Completing Train Step...
At time: 361.16130232810974 and batch: 50, loss is 4.584624509811402 and perplexity is 97.96639481489673
At time: 361.72770857810974 and batch: 100, loss is 4.565959205627442 and perplexity is 96.15478200384571
At time: 362.29037380218506 and batch: 150, loss is 4.5411187934875485 and perplexity is 93.7956794346985
At time: 362.84900403022766 and batch: 200, loss is 4.521958646774292 and perplexity is 92.01564774528927
At time: 363.40160393714905 and batch: 250, loss is 4.502564029693604 and perplexity is 90.24823404865305
At time: 363.9523310661316 and batch: 300, loss is 4.565271320343018 and perplexity is 96.08866128861908
At time: 364.5056240558624 and batch: 350, loss is 4.517046794891358 and perplexity is 91.564788695551
At time: 365.06296038627625 and batch: 400, loss is 4.561375608444214 and perplexity is 95.71505574977257
At time: 365.62479400634766 and batch: 450, loss is 4.531850881576538 and perplexity is 92.93040517636078
At time: 366.1778197288513 and batch: 500, loss is 4.5207365608215335 and perplexity is 91.90326539917595
At time: 366.72948718070984 and batch: 550, loss is 4.53008469581604 and perplexity is 92.76641767684293
At time: 367.29776525497437 and batch: 600, loss is 4.621659193038941 and perplexity is 101.66257010099918
At time: 367.8523097038269 and batch: 650, loss is 4.5559303760528564 and perplexity is 95.19528145806709
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.965244966394761 and perplexity of 143.34366035246197
Finished 47 epochs...
Completing Train Step...
At time: 368.88983178138733 and batch: 50, loss is 4.579190034866333 and perplexity is 97.43544292592436
At time: 369.4439432621002 and batch: 100, loss is 4.560684404373169 and perplexity is 95.64891997287025
At time: 369.9989731311798 and batch: 150, loss is 4.535531625747681 and perplexity is 93.27308850147543
At time: 370.55393528938293 and batch: 200, loss is 4.517038993835449 and perplexity is 91.56407439630127
At time: 371.10916113853455 and batch: 250, loss is 4.4976701736450195 and perplexity is 89.80765113657796
At time: 371.6595914363861 and batch: 300, loss is 4.5608617782592775 and perplexity is 95.66588709822591
At time: 372.21116638183594 and batch: 350, loss is 4.512938461303711 and perplexity is 91.18938167536481
At time: 372.7682328224182 and batch: 400, loss is 4.557328090667725 and perplexity is 95.32843032460922
At time: 373.32109665870667 and batch: 450, loss is 4.528121490478515 and perplexity is 92.58447680260042
At time: 373.8735158443451 and batch: 500, loss is 4.51709243774414 and perplexity is 91.56896806910005
At time: 374.435400724411 and batch: 550, loss is 4.526544694900513 and perplexity is 92.43860504418143
At time: 374.9989902973175 and batch: 600, loss is 4.617495527267456 and perplexity is 101.24016113275378
At time: 375.55754804611206 and batch: 650, loss is 4.551202325820923 and perplexity is 94.74625573006598
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.962692859126072 and perplexity of 142.97829837474336
Finished 48 epochs...
Completing Train Step...
At time: 376.5848820209503 and batch: 50, loss is 4.574534845352173 and perplexity is 96.98291658885434
At time: 377.1505877971649 and batch: 100, loss is 4.555744199752808 and perplexity is 95.17756000249184
At time: 377.70904898643494 and batch: 150, loss is 4.53066725730896 and perplexity is 92.82047556411054
At time: 378.27251863479614 and batch: 200, loss is 4.511000776290894 and perplexity is 91.01285645753103
At time: 378.83133459091187 and batch: 250, loss is 4.492596187591553 and perplexity is 89.35312247841817
At time: 379.3909697532654 and batch: 300, loss is 4.556389570236206 and perplexity is 95.23900461553647
At time: 379.95248794555664 and batch: 350, loss is 4.508346138000488 and perplexity is 90.77157064883018
At time: 380.51076555252075 and batch: 400, loss is 4.553753051757813 and perplexity is 94.98823594338532
At time: 381.06381845474243 and batch: 450, loss is 4.524432973861694 and perplexity is 92.24360646088023
At time: 381.63012194633484 and batch: 500, loss is 4.513375129699707 and perplexity is 91.22920989162165
At time: 382.18352794647217 and batch: 550, loss is 4.523387517929077 and perplexity is 92.14722022781828
At time: 382.73491764068604 and batch: 600, loss is 4.614345493316651 and perplexity is 100.92175294958359
At time: 383.28702449798584 and batch: 650, loss is 4.548610353469849 and perplexity is 94.50099404790753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.960822011910233 and perplexity of 142.711057884314
Finished 49 epochs...
Completing Train Step...
At time: 384.34341192245483 and batch: 50, loss is 4.571278743743896 and perplexity is 96.66764391665005
At time: 384.89765071868896 and batch: 100, loss is 4.552216529846191 and perplexity is 94.84239650894804
At time: 385.451012134552 and batch: 150, loss is 4.527013483047486 and perplexity is 92.48194932539772
At time: 386.0039174556732 and batch: 200, loss is 4.5073856925964355 and perplexity is 90.68443136394325
At time: 386.56041193008423 and batch: 250, loss is 4.488633260726929 and perplexity is 88.99972329896637
At time: 387.1112275123596 and batch: 300, loss is 4.552122039794922 and perplexity is 94.83343526942004
At time: 387.66688299179077 and batch: 350, loss is 4.504547615051269 and perplexity is 90.4274267875592
At time: 388.2215156555176 and batch: 400, loss is 4.549533367156982 and perplexity is 94.58826002651224
At time: 388.7836163043976 and batch: 450, loss is 4.520468339920044 and perplexity is 91.87861832805835
At time: 389.3412311077118 and batch: 500, loss is 4.510398607254029 and perplexity is 90.95806783107844
At time: 389.89620780944824 and batch: 550, loss is 4.520543327331543 and perplexity is 91.8855083261472
At time: 390.44899463653564 and batch: 600, loss is 4.610727491378785 and perplexity is 100.55727758570546
At time: 391.0191066265106 and batch: 650, loss is 4.543676681518555 and perplexity is 94.0359053850472
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.956908880495558 and perplexity of 142.15370197515767
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa09ad998d0>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'seq_len': 20, 'anneal': 3.8249845852050006, 'data': 'ptb', 'dropout': 0.6607439212537519, 'batch_size': 80, 'num_layers': 1, 'lr': 23.07657387936441}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.8391096591949463 and batch: 50, loss is 6.577095279693603 and perplexity is 718.4494008261605
At time: 1.4037258625030518 and batch: 100, loss is 5.970471115112304 and perplexity is 391.69015844198907
At time: 1.9632856845855713 and batch: 150, loss is 5.992136602401733 and perplexity is 400.26891246940346
At time: 2.5365588665008545 and batch: 200, loss is 6.038379287719726 and perplexity is 419.2130602590883
At time: 3.097046375274658 and batch: 250, loss is 6.047819585800171 and perplexity is 423.18929539975915
At time: 3.6565186977386475 and batch: 300, loss is 6.109313220977783 and perplexity is 450.02953816544346
At time: 4.215697765350342 and batch: 350, loss is 6.077532348632812 and perplexity is 435.95208853638366
At time: 4.774449110031128 and batch: 400, loss is 6.134256086349487 and perplexity is 461.3957277371567
At time: 5.331603288650513 and batch: 450, loss is 6.129202938079834 and perplexity is 459.07010751440805
At time: 5.900830030441284 and batch: 500, loss is 6.163887596130371 and perplexity is 475.2721541819127
At time: 6.4635255336761475 and batch: 550, loss is 6.160861082077027 and perplexity is 473.8359108297695
At time: 7.023207664489746 and batch: 600, loss is 6.201228866577148 and perplexity is 493.354936126608
At time: 7.583535432815552 and batch: 650, loss is 6.246327962875366 and perplexity is 516.1141504796313
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 6.2311748429840685 and perplexity of 508.3523670167552
Finished 1 epochs...
Completing Train Step...
At time: 8.629455089569092 and batch: 50, loss is 6.165006303787232 and perplexity is 475.8041422939938
At time: 9.180052757263184 and batch: 100, loss is 6.298041582107544 and perplexity is 543.5064543977278
At time: 9.74753451347351 and batch: 150, loss is 6.383794450759888 and perplexity is 592.1704115209166
At time: 10.299648523330688 and batch: 200, loss is 6.488685684204102 and perplexity is 657.6584240830705
At time: 10.847643375396729 and batch: 250, loss is 6.515826683044434 and perplexity is 675.7523637018669
At time: 11.395827293395996 and batch: 300, loss is 6.477089128494263 and perplexity is 650.0759020742664
At time: 11.948000192642212 and batch: 350, loss is 6.384749345779419 and perplexity is 592.7361421612894
At time: 12.503175258636475 and batch: 400, loss is 6.440011234283447 and perplexity is 626.4138370825605
At time: 13.0512056350708 and batch: 450, loss is 6.438285160064697 and perplexity is 625.333532918902
At time: 13.600051879882812 and batch: 500, loss is 6.477801885604858 and perplexity is 650.5394134617925
At time: 14.150062084197998 and batch: 550, loss is 6.561119403839111 and perplexity is 707.0627405090653
At time: 14.711867570877075 and batch: 600, loss is 6.431476917266846 and perplexity is 621.0905703460386
At time: 15.269566774368286 and batch: 650, loss is 6.372120752334594 and perplexity is 585.2977852508917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 6.130433325674019 and perplexity of 459.6352893045866
Finished 2 epochs...
Completing Train Step...
At time: 16.353209733963013 and batch: 50, loss is 6.355231800079346 and perplexity is 575.4957251543456
At time: 16.911288022994995 and batch: 100, loss is 6.459315156936645 and perplexity is 638.6235498667158
At time: 17.470492362976074 and batch: 150, loss is 6.542022876739502 and perplexity is 693.688405836669
At time: 18.026034593582153 and batch: 200, loss is 6.434617567062378 and perplexity is 623.0442646477228
At time: 18.590875387191772 and batch: 250, loss is 6.671405801773071 and perplexity is 789.5047118111472
At time: 19.17411518096924 and batch: 300, loss is 6.339875030517578 and perplexity is 566.7254835656737
At time: 19.736022233963013 and batch: 350, loss is 6.19669285774231 and perplexity is 491.1221415938709
At time: 20.298961400985718 and batch: 400, loss is 6.309244012832641 and perplexity is 549.6292790295743
At time: 20.853296279907227 and batch: 450, loss is 6.402824983596802 and perplexity is 603.5476440167943
At time: 21.40941071510315 and batch: 500, loss is 6.305646533966065 and perplexity is 547.6555516636977
At time: 21.964230060577393 and batch: 550, loss is 6.262528638839722 and perplexity is 524.5436459871557
At time: 22.514503717422485 and batch: 600, loss is 6.2922260951995845 and perplexity is 540.3548726012982
At time: 23.063098430633545 and batch: 650, loss is 6.39081485748291 and perplexity is 596.3423157566747
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 6.236296410654106 and perplexity of 510.96260661838716
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 24.10299777984619 and batch: 50, loss is 5.974338569641113 and perplexity is 393.2079353940109
At time: 24.653274297714233 and batch: 100, loss is 5.8413192176818844 and perplexity is 344.23315972708747
At time: 25.20448350906372 and batch: 150, loss is 5.816137819290161 and perplexity is 335.67311664409766
At time: 25.75448489189148 and batch: 200, loss is 5.815317440032959 and perplexity is 335.3978503088365
At time: 26.305848836898804 and batch: 250, loss is 5.807575740814209 and perplexity is 332.8113259981107
At time: 26.85642433166504 and batch: 300, loss is 5.846968784332275 and perplexity is 346.1834318149822
At time: 27.41210150718689 and batch: 350, loss is 5.7800204467773435 and perplexity is 323.7658103169997
At time: 27.970181941986084 and batch: 400, loss is 5.831434755325318 and perplexity is 340.84736098472325
At time: 28.520920991897583 and batch: 450, loss is 5.801431360244751 and perplexity is 330.77267608763
At time: 29.069132804870605 and batch: 500, loss is 5.799717140197754 and perplexity is 330.20614465378395
At time: 29.629446744918823 and batch: 550, loss is 5.813100175857544 and perplexity is 334.65500851341966
At time: 30.184576749801636 and batch: 600, loss is 5.819766349792481 and perplexity is 336.8933292371559
At time: 30.74115824699402 and batch: 650, loss is 5.791371011734009 and perplexity is 327.46167054887235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.89183074352788 and perplexity of 362.0675307742827
Finished 4 epochs...
Completing Train Step...
At time: 31.793606758117676 and batch: 50, loss is 5.779014921188354 and perplexity is 323.440419131884
At time: 32.37322998046875 and batch: 100, loss is 5.749218425750732 and perplexity is 313.9451928942461
At time: 32.939003229141235 and batch: 150, loss is 5.736426076889038 and perplexity is 309.9546749308622
At time: 33.49870800971985 and batch: 200, loss is 5.74476131439209 and perplexity is 312.5490179739023
At time: 34.06354284286499 and batch: 250, loss is 5.715442209243775 and perplexity is 303.5183922761401
At time: 34.631728649139404 and batch: 300, loss is 5.749256610870361 and perplexity is 313.9571811578789
At time: 35.190770387649536 and batch: 350, loss is 5.6867617416381835 and perplexity is 294.93699039314606
At time: 35.74232244491577 and batch: 400, loss is 5.731948738098144 and perplexity is 308.5700049727887
At time: 36.29332518577576 and batch: 450, loss is 5.691935710906982 and perplexity is 296.4669398603799
At time: 36.842665910720825 and batch: 500, loss is 5.687970647811889 and perplexity is 295.2937571470024
At time: 37.402875900268555 and batch: 550, loss is 5.709722166061401 and perplexity is 301.78720990483134
At time: 37.967811822891235 and batch: 600, loss is 5.73538293838501 and perplexity is 309.63151785303705
At time: 38.52036213874817 and batch: 650, loss is 5.709318161010742 and perplexity is 301.66531097335553
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.809047923368566 and perplexity of 333.30164585775253
Finished 5 epochs...
Completing Train Step...
At time: 39.56891679763794 and batch: 50, loss is 5.71292739868164 and perplexity is 302.7560599798636
At time: 40.121407985687256 and batch: 100, loss is 5.692858152389526 and perplexity is 296.74053943430783
At time: 40.68724989891052 and batch: 150, loss is 5.676417627334595 and perplexity is 291.90185341749816
At time: 41.24649214744568 and batch: 200, loss is 5.686361503601074 and perplexity is 294.81896901093893
At time: 41.79771971702576 and batch: 250, loss is 5.671259632110596 and perplexity is 290.4001013960686
At time: 42.35561442375183 and batch: 300, loss is 5.7080186748504635 and perplexity is 301.2735556721342
At time: 42.909130811691284 and batch: 350, loss is 5.651335573196411 and perplexity is 284.6714115518312
At time: 43.45987057685852 and batch: 400, loss is 5.700376615524292 and perplexity is 298.9799802681949
At time: 44.01131582260132 and batch: 450, loss is 5.664275007247925 and perplexity is 288.3788327466568
At time: 44.571887254714966 and batch: 500, loss is 5.663542127609253 and perplexity is 288.16756319894387
At time: 45.132813692092896 and batch: 550, loss is 5.680193376541138 and perplexity is 293.0060849477371
At time: 45.684537172317505 and batch: 600, loss is 5.697053499221802 and perplexity is 297.98808402773176
At time: 46.26709485054016 and batch: 650, loss is 5.675379619598389 and perplexity is 291.5990142373353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.790783452052696 and perplexity of 327.26932378721693
Finished 6 epochs...
Completing Train Step...
At time: 47.31363320350647 and batch: 50, loss is 5.6843134307861325 and perplexity is 294.2157761974263
At time: 47.878052949905396 and batch: 100, loss is 5.65670711517334 and perplexity is 286.2046502299717
At time: 48.428444623947144 and batch: 150, loss is 5.641350631713867 and perplexity is 281.84312778773614
At time: 48.98208498954773 and batch: 200, loss is 5.648338174819946 and perplexity is 283.819415448777
At time: 49.54311966896057 and batch: 250, loss is 5.630499639511108 and perplexity is 278.8013830256077
At time: 50.104276180267334 and batch: 300, loss is 5.657535266876221 and perplexity is 286.4417692701484
At time: 50.658153772354126 and batch: 350, loss is 5.60075644493103 and perplexity is 270.63104750098245
At time: 51.211183309555054 and batch: 400, loss is 5.640992813110351 and perplexity is 281.74229711396185
At time: 51.771302938461304 and batch: 450, loss is 5.605218172073364 and perplexity is 271.8412271292357
At time: 52.321120262145996 and batch: 500, loss is 5.611092386245727 and perplexity is 273.44278004537455
At time: 52.876073598861694 and batch: 550, loss is 5.625565519332886 and perplexity is 277.42913170638235
At time: 53.438345432281494 and batch: 600, loss is 5.651858997344971 and perplexity is 284.8204544459643
At time: 54.00499749183655 and batch: 650, loss is 5.6265716552734375 and perplexity is 277.70840359593166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.744396733302696 and perplexity of 312.43508928182575
Finished 7 epochs...
Completing Train Step...
At time: 55.08143329620361 and batch: 50, loss is 5.631114177703857 and perplexity is 278.9727697801275
At time: 55.63289284706116 and batch: 100, loss is 5.603802347183228 and perplexity is 271.4566198862021
At time: 56.20202374458313 and batch: 150, loss is 5.5877794170379635 and perplexity is 267.1417501705057
At time: 56.769750356674194 and batch: 200, loss is 5.601421356201172 and perplexity is 270.81105297171734
At time: 57.32794737815857 and batch: 250, loss is 5.586327171325683 and perplexity is 266.7540762762591
At time: 57.890740394592285 and batch: 300, loss is 5.615758695602417 and perplexity is 274.7217303165264
At time: 58.46043562889099 and batch: 350, loss is 5.55886305809021 and perplexity is 259.52760071588483
At time: 59.018195152282715 and batch: 400, loss is 5.605441970825195 and perplexity is 271.902071664775
At time: 59.56830716133118 and batch: 450, loss is 5.571434392929077 and perplexity is 262.8108029625672
At time: 60.11832547187805 and batch: 500, loss is 5.571256523132324 and perplexity is 262.76406101557336
At time: 60.68768000602722 and batch: 550, loss is 5.589583034515381 and perplexity is 267.62400647227525
At time: 61.24749302864075 and batch: 600, loss is 5.619650335311889 and perplexity is 275.792931323537
At time: 61.8059766292572 and batch: 650, loss is 5.60084981918335 and perplexity is 270.65631865251635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.722586837469363 and perplexity of 305.6946834868669
Finished 8 epochs...
Completing Train Step...
At time: 62.85987043380737 and batch: 50, loss is 5.60713342666626 and perplexity is 272.36237119050674
At time: 63.42760896682739 and batch: 100, loss is 5.577483024597168 and perplexity is 264.40526600667226
At time: 63.98188781738281 and batch: 150, loss is 5.56939058303833 and perplexity is 262.27421617155653
At time: 64.5346028804779 and batch: 200, loss is 5.579975519180298 and perplexity is 265.06511669546694
At time: 65.08484029769897 and batch: 250, loss is 5.565795822143555 and perplexity is 261.3330956406498
At time: 65.63825154304504 and batch: 300, loss is 5.59908634185791 and perplexity is 270.17944297489225
At time: 66.18796944618225 and batch: 350, loss is 5.544581079483033 and perplexity is 255.84737607519494
At time: 66.74031829833984 and batch: 400, loss is 5.584415130615234 and perplexity is 266.2445189251813
At time: 67.3035614490509 and batch: 450, loss is 5.553619518280029 and perplexity is 258.17031899988365
At time: 67.85787963867188 and batch: 500, loss is 5.553329200744629 and perplexity is 258.0953785079539
At time: 68.40895104408264 and batch: 550, loss is 5.5757568836212155 and perplexity is 263.94925892224256
At time: 68.98040652275085 and batch: 600, loss is 5.60539981842041 and perplexity is 271.8906105801461
At time: 69.54178237915039 and batch: 650, loss is 5.585418472290039 and perplexity is 266.51178720498757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.719636804917279 and perplexity of 304.7942030958649
Finished 9 epochs...
Completing Train Step...
At time: 70.59405279159546 and batch: 50, loss is 5.595459775924683 and perplexity is 269.2013939626955
At time: 71.14551854133606 and batch: 100, loss is 5.563507604598999 and perplexity is 260.735792306836
At time: 71.69727754592896 and batch: 150, loss is 5.556159420013428 and perplexity is 258.8268796874762
At time: 72.2525417804718 and batch: 200, loss is 5.569094438552856 and perplexity is 262.1965566085486
At time: 72.8164427280426 and batch: 250, loss is 5.554702463150025 and perplexity is 258.4500546642423
At time: 73.37989902496338 and batch: 300, loss is 5.585293788909912 and perplexity is 266.47855968601783
At time: 73.94030475616455 and batch: 350, loss is 5.5353356456756595 and perplexity is 253.4928571409948
At time: 74.4913592338562 and batch: 400, loss is 5.572440967559815 and perplexity is 263.0754748331614
At time: 75.07913160324097 and batch: 450, loss is 5.538870220184326 and perplexity is 254.39043186988647
At time: 75.64593815803528 and batch: 500, loss is 5.539787282943726 and perplexity is 254.62383086570807
At time: 76.20971751213074 and batch: 550, loss is 5.567512655258179 and perplexity is 261.78214631542863
At time: 76.7627625465393 and batch: 600, loss is 5.595331172943116 and perplexity is 269.16677608681863
At time: 77.32294583320618 and batch: 650, loss is 5.5743194103240965 and perplexity is 263.5701114831941
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.711465274586397 and perplexity of 302.31371650891344
Finished 10 epochs...
Completing Train Step...
At time: 78.37890291213989 and batch: 50, loss is 5.58391110420227 and perplexity is 266.1103584683833
At time: 78.94909596443176 and batch: 100, loss is 5.55315676689148 and perplexity is 258.0508779641682
At time: 79.51700806617737 and batch: 150, loss is 5.545978641510009 and perplexity is 256.2051886270776
At time: 80.08427619934082 and batch: 200, loss is 5.557365026473999 and perplexity is 259.1391112221559
At time: 80.65610003471375 and batch: 250, loss is 5.545842943191528 and perplexity is 256.17042437257345
At time: 81.22285890579224 and batch: 300, loss is 5.576571702957153 and perplexity is 264.1644175279108
At time: 81.77682590484619 and batch: 350, loss is 5.526377153396607 and perplexity is 251.23208501208987
At time: 82.34079575538635 and batch: 400, loss is 5.563051462173462 and perplexity is 260.6168867711003
At time: 82.89436388015747 and batch: 450, loss is 5.530597610473633 and perplexity is 252.29463989969472
At time: 83.44470286369324 and batch: 500, loss is 5.5303664398193355 and perplexity is 252.23632352349057
At time: 83.9944589138031 and batch: 550, loss is 5.559682598114014 and perplexity is 259.74038115108544
At time: 84.5487289428711 and batch: 600, loss is 5.585922565460205 and perplexity is 266.64616784402193
At time: 85.11564064025879 and batch: 650, loss is 5.563468675613404 and perplexity is 260.7256423244987
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.702702540977328 and perplexity of 299.67619477189373
Finished 11 epochs...
Completing Train Step...
At time: 86.19748282432556 and batch: 50, loss is 5.571399307250976 and perplexity is 262.8015822290919
At time: 86.75999975204468 and batch: 100, loss is 5.543373212814331 and perplexity is 255.53853311546618
At time: 87.32257080078125 and batch: 150, loss is 5.537271957397461 and perplexity is 253.98417384931275
At time: 87.88559985160828 and batch: 200, loss is 5.548780326843262 and perplexity is 256.92400141979675
At time: 88.44874668121338 and batch: 250, loss is 5.53545578956604 and perplexity is 253.52331458863685
At time: 89.00553441047668 and batch: 300, loss is 5.563526029586792 and perplexity is 260.7405964048841
At time: 89.58579540252686 and batch: 350, loss is 5.517014722824097 and perplexity is 248.8909186635164
At time: 90.15255928039551 and batch: 400, loss is 5.554258642196655 and perplexity is 258.3353745651806
At time: 90.71075558662415 and batch: 450, loss is 5.52288215637207 and perplexity is 250.35556223697725
At time: 91.26130795478821 and batch: 500, loss is 5.51938159942627 and perplexity is 249.480710462259
At time: 91.81232929229736 and batch: 550, loss is 5.550131292343139 and perplexity is 257.2713314444194
At time: 92.36866903305054 and batch: 600, loss is 5.576074495315551 and perplexity is 264.0331056082268
At time: 92.92593097686768 and batch: 650, loss is 5.557165975570679 and perplexity is 259.0875344813503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.696770761527267 and perplexity of 297.903843473406
Finished 12 epochs...
Completing Train Step...
At time: 93.97582674026489 and batch: 50, loss is 5.565531005859375 and perplexity is 261.26389954386013
At time: 94.54682993888855 and batch: 100, loss is 5.537059478759765 and perplexity is 253.93021337098438
At time: 95.1031277179718 and batch: 150, loss is 5.530945615768433 and perplexity is 252.38245504941173
At time: 95.66412782669067 and batch: 200, loss is 5.543874025344849 and perplexity is 255.66654206644822
At time: 96.22288584709167 and batch: 250, loss is 5.523804445266723 and perplexity is 250.58656890280946
At time: 96.7853422164917 and batch: 300, loss is 5.555659122467041 and perplexity is 258.69742162110646
At time: 97.3433210849762 and batch: 350, loss is 5.509285831451416 and perplexity is 246.97468251899807
At time: 97.89466953277588 and batch: 400, loss is 5.545357141494751 and perplexity is 256.04600656938607
At time: 98.44368100166321 and batch: 450, loss is 5.513754415512085 and perplexity is 248.08077915101418
At time: 98.99512600898743 and batch: 500, loss is 5.512516574859619 and perplexity is 247.77388465991083
At time: 99.5451512336731 and batch: 550, loss is 5.543288946151733 and perplexity is 255.51700064336288
At time: 100.10027980804443 and batch: 600, loss is 5.571323881149292 and perplexity is 262.78176087776086
At time: 100.6631441116333 and batch: 650, loss is 5.551259593963623 and perplexity is 257.56177492770576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.692790311925552 and perplexity of 296.7204091012659
Finished 13 epochs...
Completing Train Step...
At time: 101.70390701293945 and batch: 50, loss is 5.5582014846801755 and perplexity is 259.35596093849796
At time: 102.26494860649109 and batch: 100, loss is 5.529934511184693 and perplexity is 252.12739895817654
At time: 102.82972717285156 and batch: 150, loss is 5.52774130821228 and perplexity is 251.5750383382426
At time: 103.3813591003418 and batch: 200, loss is 5.537103471755981 and perplexity is 253.94138476763004
At time: 103.9469702243805 and batch: 250, loss is 5.517602958679199 and perplexity is 249.03736829512286
At time: 104.49682283401489 and batch: 300, loss is 5.553241529464722 and perplexity is 258.0727519476463
At time: 105.04541039466858 and batch: 350, loss is 5.504483995437622 and perplexity is 245.79159336729927
At time: 105.5954270362854 and batch: 400, loss is 5.54041051864624 and perplexity is 254.7825709889325
At time: 106.14521837234497 and batch: 450, loss is 5.508416843414307 and perplexity is 246.76015769767153
At time: 106.69540810585022 and batch: 500, loss is 5.5058315563201905 and perplexity is 246.12303577303965
At time: 107.24566960334778 and batch: 550, loss is 5.536214618682862 and perplexity is 253.71576847211014
At time: 107.80241107940674 and batch: 600, loss is 5.56562313079834 and perplexity is 261.2879695733672
At time: 108.35744309425354 and batch: 650, loss is 5.545361843109131 and perplexity is 256.0472104018024
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.696950276692708 and perplexity of 297.95732653151913
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 109.40231013298035 and batch: 50, loss is 5.513047380447388 and perplexity is 247.9054393342807
At time: 109.96715307235718 and batch: 100, loss is 5.43140941619873 and perplexity is 228.47102937672292
At time: 110.51894807815552 and batch: 150, loss is 5.414961833953857 and perplexity is 224.7439679262771
At time: 111.06990170478821 and batch: 200, loss is 5.412174520492553 and perplexity is 224.11840825957307
At time: 111.62117099761963 and batch: 250, loss is 5.404313755035401 and perplexity is 222.3635722310051
At time: 112.17323160171509 and batch: 300, loss is 5.42474781036377 and perplexity is 226.9541036247406
At time: 112.72604298591614 and batch: 350, loss is 5.36655107498169 and perplexity is 214.12309822244245
At time: 113.2806613445282 and batch: 400, loss is 5.391413469314575 and perplexity is 219.51344188902198
At time: 113.83326148986816 and batch: 450, loss is 5.3574106407165525 and perplexity is 212.1748376566549
At time: 114.39932584762573 and batch: 500, loss is 5.355098752975464 and perplexity is 211.68487983248093
At time: 114.96232843399048 and batch: 550, loss is 5.3779223346710205 and perplexity is 216.57184385274238
At time: 115.52322149276733 and batch: 600, loss is 5.419848175048828 and perplexity is 225.8448310185059
At time: 116.0808937549591 and batch: 650, loss is 5.401535844802856 and perplexity is 221.74672336042835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.547533820657169 and perplexity of 256.6039435811509
Finished 15 epochs...
Completing Train Step...
At time: 117.11513471603394 and batch: 50, loss is 5.423103494644165 and perplexity is 226.58122607274646
At time: 117.66529178619385 and batch: 100, loss is 5.3902405834198 and perplexity is 219.25612859837943
At time: 118.2291476726532 and batch: 150, loss is 5.388969888687134 and perplexity is 218.97769792832202
At time: 118.77890920639038 and batch: 200, loss is 5.3865891456604 and perplexity is 218.45698838472208
At time: 119.3301911354065 and batch: 250, loss is 5.380593929290772 and perplexity is 217.15120959617255
At time: 119.88693141937256 and batch: 300, loss is 5.40712537765503 and perplexity is 222.98965442145237
At time: 120.44260096549988 and batch: 350, loss is 5.349816379547119 and perplexity is 210.56962942590656
At time: 121.0055160522461 and batch: 400, loss is 5.3845224285125735 and perplexity is 218.0059658093486
At time: 121.5678813457489 and batch: 450, loss is 5.3557971096038814 and perplexity is 211.83276300303362
At time: 122.1299729347229 and batch: 500, loss is 5.3597705078125 and perplexity is 212.67613333757834
At time: 122.68574738502502 and batch: 550, loss is 5.381204462051391 and perplexity is 217.28382800346088
At time: 123.23769092559814 and batch: 600, loss is 5.418747301101685 and perplexity is 225.59634113105815
At time: 123.79032373428345 and batch: 650, loss is 5.393738050460815 and perplexity is 220.0243122471366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.5442851945465685 and perplexity of 255.77168588892565
Finished 16 epochs...
Completing Train Step...
At time: 124.81933760643005 and batch: 50, loss is 5.413088045120239 and perplexity is 224.32323948997794
At time: 125.39090538024902 and batch: 100, loss is 5.382466878890991 and perplexity is 217.55830398200123
At time: 125.95566916465759 and batch: 150, loss is 5.383002786636353 and perplexity is 217.67492640881272
At time: 126.50940728187561 and batch: 200, loss is 5.382589702606201 and perplexity is 217.58502694224413
At time: 127.06052350997925 and batch: 250, loss is 5.378417425155639 and perplexity is 216.6790930587102
At time: 127.61454343795776 and batch: 300, loss is 5.403119831085205 and perplexity is 222.0982454579796
At time: 128.17766785621643 and batch: 350, loss is 5.3479150390625 and perplexity is 210.16964523821105
At time: 128.73133754730225 and batch: 400, loss is 5.38245717048645 and perplexity is 217.5561918482277
At time: 129.28418469429016 and batch: 450, loss is 5.35538691520691 and perplexity is 211.7458882095496
At time: 129.84284710884094 and batch: 500, loss is 5.359779443740845 and perplexity is 212.6780338047577
At time: 130.39575934410095 and batch: 550, loss is 5.380969476699829 and perplexity is 217.23277548528063
At time: 130.9579701423645 and batch: 600, loss is 5.417559356689453 and perplexity is 225.32850433730073
At time: 131.51221227645874 and batch: 650, loss is 5.389658718109131 and perplexity is 219.12858817226973
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.5407876407398895 and perplexity of 254.87867324553233
Finished 17 epochs...
Completing Train Step...
At time: 132.56846976280212 and batch: 50, loss is 5.408738021850586 and perplexity is 223.3495475050749
At time: 133.12229657173157 and batch: 100, loss is 5.378991537094116 and perplexity is 216.80352682889216
At time: 133.68206810951233 and batch: 150, loss is 5.38027606010437 and perplexity is 217.08219488725783
At time: 134.24739861488342 and batch: 200, loss is 5.381224603652954 and perplexity is 217.28820449182513
At time: 134.80786204338074 and batch: 250, loss is 5.375652770996094 and perplexity is 216.0808776132377
At time: 135.36782145500183 and batch: 300, loss is 5.40190019607544 and perplexity is 221.8275317817076
At time: 135.9288580417633 and batch: 350, loss is 5.3458856582641605 and perplexity is 209.7435634830677
At time: 136.4833641052246 and batch: 400, loss is 5.380872344970703 and perplexity is 217.21167631489084
At time: 137.03267288208008 and batch: 450, loss is 5.354607305526733 and perplexity is 211.5808733972943
At time: 137.60015845298767 and batch: 500, loss is 5.359311771392822 and perplexity is 212.57859342388616
At time: 138.1605896949768 and batch: 550, loss is 5.380738344192505 and perplexity is 217.18257173129294
At time: 138.7129089832306 and batch: 600, loss is 5.414843244552612 and perplexity is 224.71731725396222
At time: 139.2656319141388 and batch: 650, loss is 5.386576557159424 and perplexity is 218.45423835602003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.541514078776042 and perplexity of 255.06389407596484
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 140.30734086036682 and batch: 50, loss is 5.39921968460083 and perplexity is 221.23371675692988
At time: 140.88449478149414 and batch: 100, loss is 5.359470472335816 and perplexity is 212.6123325242654
At time: 141.44238352775574 and batch: 150, loss is 5.356292839050293 and perplexity is 211.93780077437603
At time: 142.00741314888 and batch: 200, loss is 5.34766806602478 and perplexity is 210.11774541168268
At time: 142.5688488483429 and batch: 250, loss is 5.342095308303833 and perplexity is 208.950066740711
At time: 143.127379655838 and batch: 300, loss is 5.36716323852539 and perplexity is 214.25421670592155
At time: 143.686669588089 and batch: 350, loss is 5.304529237747192 and perplexity is 201.2462409766212
At time: 144.2384490966797 and batch: 400, loss is 5.340161247253418 and perplexity is 208.54633510181463
At time: 144.7900550365448 and batch: 450, loss is 5.306689777374268 and perplexity is 201.6815114953355
At time: 145.34055638313293 and batch: 500, loss is 5.308385944366455 and perplexity is 202.02388729925298
At time: 145.89231419563293 and batch: 550, loss is 5.328080215454102 and perplexity is 206.04203790159218
At time: 146.45663213729858 and batch: 600, loss is 5.36833498954773 and perplexity is 214.50541644642223
At time: 147.02066946029663 and batch: 650, loss is 5.352502632141113 and perplexity is 211.13603305017568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.5187539493336395 and perplexity of 249.32417300174268
Finished 19 epochs...
Completing Train Step...
At time: 148.05836176872253 and batch: 50, loss is 5.378033647537231 and perplexity is 216.59595242719453
At time: 148.60714149475098 and batch: 100, loss is 5.343953704833984 and perplexity is 209.3387398622457
At time: 149.1575379371643 and batch: 150, loss is 5.345517835617065 and perplexity is 209.66642923706712
At time: 149.706889629364 and batch: 200, loss is 5.338696050643921 and perplexity is 208.24099746316176
At time: 150.2654755115509 and batch: 250, loss is 5.3337764549255375 and perplexity is 207.21905178457177
At time: 150.8160274028778 and batch: 300, loss is 5.359647779464722 and perplexity is 212.6500335487468
At time: 151.3654761314392 and batch: 350, loss is 5.298929433822632 and perplexity is 200.12245092519836
At time: 151.9184057712555 and batch: 400, loss is 5.33746711730957 and perplexity is 207.98524034621283
At time: 152.47122645378113 and batch: 450, loss is 5.306801290512085 and perplexity is 201.7040028875416
At time: 153.0222930908203 and batch: 500, loss is 5.308990631103516 and perplexity is 202.14608540653632
At time: 153.57533597946167 and batch: 550, loss is 5.325833740234375 and perplexity is 205.57968909123977
At time: 154.12499403953552 and batch: 600, loss is 5.365280265808106 and perplexity is 213.85116145141393
At time: 154.6771969795227 and batch: 650, loss is 5.342194843292236 and perplexity is 208.97086561827177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.507154277726715 and perplexity of 246.44880338349475
Finished 20 epochs...
Completing Train Step...
At time: 155.69270634651184 and batch: 50, loss is 5.363153409957886 and perplexity is 213.39681419443608
At time: 156.25811171531677 and batch: 100, loss is 5.3301389694213865 and perplexity is 206.466664715638
At time: 156.81247448921204 and batch: 150, loss is 5.332131404876709 and perplexity is 206.87844630665623
At time: 157.3624312877655 and batch: 200, loss is 5.321738328933716 and perplexity is 204.73947738996068
At time: 157.9133231639862 and batch: 250, loss is 5.314828109741211 and perplexity is 203.3295597542159
At time: 158.4635260105133 and batch: 300, loss is 5.344382562637329 and perplexity is 209.42853566781775
At time: 159.01285099983215 and batch: 350, loss is 5.28636791229248 and perplexity is 197.62433138888807
At time: 159.56147575378418 and batch: 400, loss is 5.329775848388672 and perplexity is 206.39170593750342
At time: 160.11136531829834 and batch: 450, loss is 5.297969903945923 and perplexity is 199.930519551205
At time: 160.67452239990234 and batch: 500, loss is 5.2983633518219 and perplexity is 200.0091972662404
At time: 161.22464561462402 and batch: 550, loss is 5.317878303527832 and perplexity is 203.95070113322686
At time: 161.7759461402893 and batch: 600, loss is 5.360024957656861 and perplexity is 212.73025563201804
At time: 162.333181142807 and batch: 650, loss is 5.33333423614502 and perplexity is 207.12743588681985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.500999899471507 and perplexity of 244.9367219601222
Finished 21 epochs...
Completing Train Step...
At time: 163.3710811138153 and batch: 50, loss is 5.3539277362823485 and perplexity is 211.437138387511
At time: 163.9211230278015 and batch: 100, loss is 5.322363977432251 and perplexity is 204.86761241614425
At time: 164.471449136734 and batch: 150, loss is 5.325007343292237 and perplexity is 205.4098688439401
At time: 165.02207231521606 and batch: 200, loss is 5.314235010147095 and perplexity is 203.20900083011236
At time: 165.57320737838745 and batch: 250, loss is 5.307740440368653 and perplexity is 201.89352215248815
At time: 166.1220531463623 and batch: 300, loss is 5.339687566757203 and perplexity is 208.44757416273163
At time: 166.67128372192383 and batch: 350, loss is 5.282475843429565 and perplexity is 196.85665876855754
At time: 167.23075795173645 and batch: 400, loss is 5.327217082977295 and perplexity is 205.86427305543415
At time: 167.7947540283203 and batch: 450, loss is 5.295573835372925 and perplexity is 199.45204577343173
At time: 168.34683179855347 and batch: 500, loss is 5.2960554790496825 and perplexity is 199.54813372831532
At time: 168.90785789489746 and batch: 550, loss is 5.316154708862305 and perplexity is 203.59947556493225
At time: 169.4655454158783 and batch: 600, loss is 5.3577228355407716 and perplexity is 212.2410878837517
At time: 170.01614546775818 and batch: 650, loss is 5.329843769073486 and perplexity is 206.40572467958654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.497962502872243 and perplexity of 244.1938807166462
Finished 22 epochs...
Completing Train Step...
At time: 171.03149271011353 and batch: 50, loss is 5.350225601196289 and perplexity is 210.65581671057407
At time: 171.59594774246216 and batch: 100, loss is 5.319138507843018 and perplexity is 204.20788270350639
At time: 172.14657521247864 and batch: 150, loss is 5.32218108177185 and perplexity is 204.83014644516288
At time: 172.69790387153625 and batch: 200, loss is 5.310818042755127 and perplexity is 202.51582725071881
At time: 173.24925208091736 and batch: 250, loss is 5.30491940498352 and perplexity is 201.32477598618203
At time: 173.79703950881958 and batch: 300, loss is 5.337509746551514 and perplexity is 207.99410678832774
At time: 174.35683417320251 and batch: 350, loss is 5.280864315032959 and perplexity is 196.53967415634432
At time: 174.9305877685547 and batch: 400, loss is 5.325865821838379 and perplexity is 205.58628452321182
At time: 175.48351073265076 and batch: 450, loss is 5.294261798858643 and perplexity is 199.19052900381965
At time: 176.0349156856537 and batch: 500, loss is 5.2949149036407475 and perplexity is 199.32066378205928
At time: 176.59088373184204 and batch: 550, loss is 5.315120706558227 and perplexity is 203.3890620408706
At time: 177.14438676834106 and batch: 600, loss is 5.356113843917846 and perplexity is 211.89986833461828
At time: 177.69536757469177 and batch: 650, loss is 5.327695960998535 and perplexity is 205.96288053975297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.496242747587316 and perplexity of 243.77428790163174
Finished 23 epochs...
Completing Train Step...
At time: 178.73970413208008 and batch: 50, loss is 5.347984504699707 and perplexity is 210.18424531363496
At time: 179.29329085350037 and batch: 100, loss is 5.316910638809204 and perplexity is 203.75344069177808
At time: 179.84563422203064 and batch: 150, loss is 5.3202409744262695 and perplexity is 204.4331392162972
At time: 180.4064381122589 and batch: 200, loss is 5.308533792495727 and perplexity is 202.05375836119447
At time: 180.96779608726501 and batch: 250, loss is 5.303006010055542 and perplexity is 200.93993047906687
At time: 181.52133178710938 and batch: 300, loss is 5.33607159614563 and perplexity is 207.6951949708239
At time: 182.09131479263306 and batch: 350, loss is 5.2796744537353515 and perplexity is 196.30595827693506
At time: 182.65964078903198 and batch: 400, loss is 5.324975252151489 and perplexity is 205.4032771126968
At time: 183.21637773513794 and batch: 450, loss is 5.293360557556152 and perplexity is 199.01109114256784
At time: 183.7737522125244 and batch: 500, loss is 5.294044094085693 and perplexity is 199.1471689949409
At time: 184.33598113059998 and batch: 550, loss is 5.314218034744263 and perplexity is 203.2055513047428
At time: 184.89062428474426 and batch: 600, loss is 5.354948511123657 and perplexity is 211.6530782931563
At time: 185.4439263343811 and batch: 650, loss is 5.326193761825562 and perplexity is 205.65371554278283
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.494639677159927 and perplexity of 243.38381361225288
Finished 24 epochs...
Completing Train Step...
At time: 186.49749565124512 and batch: 50, loss is 5.3462418270111085 and perplexity is 209.8182808904667
At time: 187.0696120262146 and batch: 100, loss is 5.3151830959320066 and perplexity is 203.40175175293237
At time: 187.63224840164185 and batch: 150, loss is 5.3186570835113525 and perplexity is 204.1095957208258
At time: 188.1968822479248 and batch: 200, loss is 5.306661176681518 and perplexity is 201.67574334687896
At time: 188.76671695709229 and batch: 250, loss is 5.301349620819092 and perplexity is 200.60737124085455
At time: 189.33704590797424 and batch: 300, loss is 5.334772539138794 and perplexity is 207.42556224457024
At time: 189.90282654762268 and batch: 350, loss is 5.278744897842407 and perplexity is 196.12356570177857
At time: 190.47086000442505 and batch: 400, loss is 5.324151792526245 and perplexity is 205.23420542850826
At time: 191.0313322544098 and batch: 450, loss is 5.292605447769165 and perplexity is 198.86087264279448
At time: 191.59834241867065 and batch: 500, loss is 5.29318172454834 and perplexity is 198.97550457267235
At time: 192.16077876091003 and batch: 550, loss is 5.313399925231933 and perplexity is 203.03937489478162
At time: 192.71731758117676 and batch: 600, loss is 5.353979187011719 and perplexity is 211.4480172623576
At time: 193.27906703948975 and batch: 650, loss is 5.324852209091187 and perplexity is 205.37800521968214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.493060542087929 and perplexity of 242.99978099571217
Finished 25 epochs...
Completing Train Step...
At time: 194.40764832496643 and batch: 50, loss is 5.344588146209717 and perplexity is 209.47159516035083
At time: 194.9622619152069 and batch: 100, loss is 5.313776388168335 and perplexity is 203.1158260836762
At time: 195.52104902267456 and batch: 150, loss is 5.317386875152588 and perplexity is 203.850498594741
At time: 196.07828569412231 and batch: 200, loss is 5.305091190338135 and perplexity is 201.35936360495555
At time: 196.63356685638428 and batch: 250, loss is 5.299984321594239 and perplexity is 200.33366903761848
At time: 197.18647837638855 and batch: 300, loss is 5.333769330978393 and perplexity is 207.21757557225783
At time: 197.74181604385376 and batch: 350, loss is 5.277879610061645 and perplexity is 195.95393577679184
At time: 198.30103015899658 and batch: 400, loss is 5.323444633483887 and perplexity is 205.0891235083801
At time: 198.85534238815308 and batch: 450, loss is 5.291931657791138 and perplexity is 198.7269273103649
At time: 199.4072675704956 and batch: 500, loss is 5.292395753860474 and perplexity is 198.81917710092762
At time: 199.9605040550232 and batch: 550, loss is 5.312666397094727 and perplexity is 202.89049441102307
At time: 200.51080226898193 and batch: 600, loss is 5.353072395324707 and perplexity is 211.25636486560163
At time: 201.0760247707367 and batch: 650, loss is 5.323680534362793 and perplexity is 205.13750991984384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.492197672526042 and perplexity of 242.79019431713238
Finished 26 epochs...
Completing Train Step...
At time: 202.12961316108704 and batch: 50, loss is 5.343242101669311 and perplexity is 209.1898267422787
At time: 202.6959195137024 and batch: 100, loss is 5.312620944976807 and perplexity is 202.8812728179183
At time: 203.2488408088684 and batch: 150, loss is 5.316357679367066 and perplexity is 203.64080444738687
At time: 203.8164963722229 and batch: 200, loss is 5.303905982971191 and perplexity is 201.12085237436173
At time: 204.36908721923828 and batch: 250, loss is 5.298795566558838 and perplexity is 200.09566287333087
At time: 204.9213273525238 and batch: 300, loss is 5.33274715423584 and perplexity is 207.00587080413786
At time: 205.47414231300354 and batch: 350, loss is 5.277204380035401 and perplexity is 195.82166645672834
At time: 206.02714157104492 and batch: 400, loss is 5.322795095443726 and perplexity is 204.9559535751889
At time: 206.57968306541443 and batch: 450, loss is 5.291334829330444 and perplexity is 198.60835681088383
At time: 207.13299751281738 and batch: 500, loss is 5.291616182327271 and perplexity is 198.66424372887465
At time: 207.6862952709198 and batch: 550, loss is 5.311902341842651 and perplexity is 202.73553406984186
At time: 208.23738265037537 and batch: 600, loss is 5.3522409820556645 and perplexity is 211.08079651572393
At time: 208.7860608100891 and batch: 650, loss is 5.322696933746338 and perplexity is 204.93583573831276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.491449692670037 and perplexity of 242.6086600430014
Finished 27 epochs...
Completing Train Step...
At time: 209.8321647644043 and batch: 50, loss is 5.34228404045105 and perplexity is 208.9895060570845
At time: 210.39093041419983 and batch: 100, loss is 5.311604537963867 and perplexity is 202.6751676305546
At time: 210.95424604415894 and batch: 150, loss is 5.315478830337525 and perplexity is 203.46191354458574
At time: 211.50617694854736 and batch: 200, loss is 5.302868328094482 and perplexity is 200.912266579837
At time: 212.05842399597168 and batch: 250, loss is 5.2978300285339355 and perplexity is 199.90255614315618
At time: 212.6091697216034 and batch: 300, loss is 5.331905288696289 and perplexity is 206.83167303086276
At time: 213.16054558753967 and batch: 350, loss is 5.276616201400757 and perplexity is 195.7065222023329
At time: 213.71164679527283 and batch: 400, loss is 5.322246561050415 and perplexity is 204.84355901449828
At time: 214.26351976394653 and batch: 450, loss is 5.29077826499939 and perplexity is 198.49784923877266
At time: 214.81464743614197 and batch: 500, loss is 5.29088828086853 and perplexity is 198.51968835348165
At time: 215.36694765090942 and batch: 550, loss is 5.311299724578857 and perplexity is 202.61339894109162
At time: 215.91734051704407 and batch: 600, loss is 5.351580162048339 and perplexity is 210.9413561797804
At time: 216.47315120697021 and batch: 650, loss is 5.321847257614135 and perplexity is 204.76178060577047
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.490836349188113 and perplexity of 242.45990322688087
Finished 28 epochs...
Completing Train Step...
At time: 217.51154685020447 and batch: 50, loss is 5.341406316757202 and perplexity is 208.80615149494574
At time: 218.08102703094482 and batch: 100, loss is 5.310699043273925 and perplexity is 202.49172940618485
At time: 218.63633179664612 and batch: 150, loss is 5.314628019332885 and perplexity is 203.2888795295771
At time: 219.19156742095947 and batch: 200, loss is 5.302030963897705 and perplexity is 200.74410025916387
At time: 219.75348114967346 and batch: 250, loss is 5.296985149383545 and perplexity is 199.73373396857116
At time: 220.31952333450317 and batch: 300, loss is 5.331120138168335 and perplexity is 206.66934276876964
At time: 220.88557410240173 and batch: 350, loss is 5.276075267791748 and perplexity is 195.6006865945708
At time: 221.44177174568176 and batch: 400, loss is 5.321698875427246 and perplexity is 204.73139985900937
At time: 222.00251579284668 and batch: 450, loss is 5.290269641876221 and perplexity is 198.39691431384486
At time: 222.5632209777832 and batch: 500, loss is 5.290254850387573 and perplexity is 198.3939797498423
At time: 223.12447714805603 and batch: 550, loss is 5.310765399932861 and perplexity is 202.50516652662662
At time: 223.68726468086243 and batch: 600, loss is 5.350994701385498 and perplexity is 210.817894458088
At time: 224.25094628334045 and batch: 650, loss is 5.321108512878418 and perplexity is 204.61056977825007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.490361232383578 and perplexity of 242.3447338140601
Finished 29 epochs...
Completing Train Step...
At time: 225.32658433914185 and batch: 50, loss is 5.34055986404419 and perplexity is 208.6294817433629
At time: 225.88779544830322 and batch: 100, loss is 5.309871807098388 and perplexity is 202.32429018781082
At time: 226.45897245407104 and batch: 150, loss is 5.313882722854614 and perplexity is 203.13742548968386
At time: 227.02354288101196 and batch: 200, loss is 5.301274242401123 and perplexity is 200.5922503444793
At time: 227.58734321594238 and batch: 250, loss is 5.296226425170898 and perplexity is 199.58224862359347
At time: 228.15738081932068 and batch: 300, loss is 5.330401840209961 and perplexity is 206.52094590476312
At time: 228.72357058525085 and batch: 350, loss is 5.275570926666259 and perplexity is 195.50206199645817
At time: 229.28681898117065 and batch: 400, loss is 5.321164932250976 and perplexity is 204.62211410387457
At time: 229.8419451713562 and batch: 450, loss is 5.289740076065064 and perplexity is 198.2918779052853
At time: 230.40571117401123 and batch: 500, loss is 5.289673528671265 and perplexity is 198.27868253666273
At time: 230.965895652771 and batch: 550, loss is 5.310207195281983 and perplexity is 202.39215874451372
At time: 231.5252296924591 and batch: 600, loss is 5.3505392837524415 and perplexity is 210.72190613063538
At time: 232.0903034210205 and batch: 650, loss is 5.320443878173828 and perplexity is 204.47462367490306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.4898334577971815 and perplexity of 242.2168641685487
Finished 30 epochs...
Completing Train Step...
At time: 233.127108335495 and batch: 50, loss is 5.339711112976074 and perplexity is 208.4524823727208
At time: 233.707102060318 and batch: 100, loss is 5.309134159088135 and perplexity is 202.1751011090062
At time: 234.25755739212036 and batch: 150, loss is 5.313167991638184 and perplexity is 202.9922887035355
At time: 234.8165533542633 and batch: 200, loss is 5.300594263076782 and perplexity is 200.45589812522067
At time: 235.3721444606781 and batch: 250, loss is 5.295454015731812 and perplexity is 199.42814893257196
At time: 235.93430876731873 and batch: 300, loss is 5.329666185379028 and perplexity is 206.36907364285025
At time: 236.48548913002014 and batch: 350, loss is 5.275116186141968 and perplexity is 195.41317949705476
At time: 237.03703212738037 and batch: 400, loss is 5.320715684890747 and perplexity is 204.53020880492096
At time: 237.58819389343262 and batch: 450, loss is 5.289279155731201 and perplexity is 198.20050220679533
At time: 238.1604516506195 and batch: 500, loss is 5.289202938079834 and perplexity is 198.185396405689
At time: 238.72783017158508 and batch: 550, loss is 5.3097788429260255 and perplexity is 202.3054821518749
At time: 239.2875919342041 and batch: 600, loss is 5.35005630493164 and perplexity is 210.62015648633496
At time: 239.85431289672852 and batch: 650, loss is 5.3198814868927 and perplexity is 204.35966125929656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.489376890893076 and perplexity of 242.10630120636839
Finished 31 epochs...
Completing Train Step...
At time: 240.90612721443176 and batch: 50, loss is 5.3390911674499515 and perplexity is 208.3232932381153
At time: 241.46088242530823 and batch: 100, loss is 5.308480091094971 and perplexity is 202.04290808268257
At time: 242.0166883468628 and batch: 150, loss is 5.312602214813232 and perplexity is 202.87747285407923
At time: 242.57088589668274 and batch: 200, loss is 5.299950160980225 and perplexity is 200.3268256333646
At time: 243.12522983551025 and batch: 250, loss is 5.294708909988404 and perplexity is 199.27960921917406
At time: 243.6911392211914 and batch: 300, loss is 5.328914222717285 and perplexity is 206.21395013578442
At time: 244.25633430480957 and batch: 350, loss is 5.274680719375611 and perplexity is 195.32810207721442
At time: 244.8140368461609 and batch: 400, loss is 5.320256004333496 and perplexity is 204.43621185050426
At time: 245.3656759262085 and batch: 450, loss is 5.288840980529785 and perplexity is 198.11367468604254
At time: 245.9175136089325 and batch: 500, loss is 5.288765439987182 and perplexity is 198.0987096368009
At time: 246.50034260749817 and batch: 550, loss is 5.309407176971436 and perplexity is 202.23030606279406
At time: 247.05751395225525 and batch: 600, loss is 5.349666051864624 and perplexity is 210.53797736066156
At time: 247.60931587219238 and batch: 650, loss is 5.319361181259155 and perplexity is 204.25335943339158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.488963407628677 and perplexity of 242.00621499602642
Finished 32 epochs...
Completing Train Step...
At time: 248.63205742835999 and batch: 50, loss is 5.33850251197815 and perplexity is 208.20069867816625
At time: 249.19809556007385 and batch: 100, loss is 5.307883129119873 and perplexity is 201.92233214242552
At time: 249.75362730026245 and batch: 150, loss is 5.312055139541626 and perplexity is 202.76651395971592
At time: 250.30611944198608 and batch: 200, loss is 5.299395141601562 and perplexity is 200.21567121235435
At time: 250.85759234428406 and batch: 250, loss is 5.294074125289917 and perplexity is 199.15314971404723
At time: 251.4092836380005 and batch: 300, loss is 5.328268995285034 and perplexity is 206.08093815434833
At time: 251.9693946838379 and batch: 350, loss is 5.274244260787964 and perplexity is 195.2428680515684
At time: 252.53140544891357 and batch: 400, loss is 5.319843730926514 and perplexity is 204.35194560849303
At time: 253.0849552154541 and batch: 450, loss is 5.288447046279908 and perplexity is 198.03564629425173
At time: 253.6379692554474 and batch: 500, loss is 5.288346643447876 and perplexity is 198.015763952659
At time: 254.19255089759827 and batch: 550, loss is 5.309069194793701 and perplexity is 202.16196737282715
At time: 254.74628138542175 and batch: 600, loss is 5.349271450042725 and perplexity is 210.45491508055744
At time: 255.29925537109375 and batch: 650, loss is 5.318868780136109 and perplexity is 204.15280960727586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.488575654871323 and perplexity of 241.91239460959736
Finished 33 epochs...
Completing Train Step...
At time: 256.3373427391052 and batch: 50, loss is 5.33791298866272 and perplexity is 208.07799568369842
At time: 256.89028906822205 and batch: 100, loss is 5.307324981689453 and perplexity is 201.80966115803093
At time: 257.4405632019043 and batch: 150, loss is 5.31153980255127 and perplexity is 202.66204779461833
At time: 258.0000319480896 and batch: 200, loss is 5.298843755722046 and perplexity is 200.10530554822049
At time: 258.55886363983154 and batch: 250, loss is 5.293501720428467 and perplexity is 199.039186102752
At time: 259.1192469596863 and batch: 300, loss is 5.3276642799377445 and perplexity is 205.95635552057428
At time: 259.67139506340027 and batch: 350, loss is 5.273828554153442 and perplexity is 195.16172116379468
At time: 260.226642370224 and batch: 400, loss is 5.319444570541382 and perplexity is 204.2703926846126
At time: 260.8057932853699 and batch: 450, loss is 5.288093481063843 and perplexity is 197.96564015477838
At time: 261.35846400260925 and batch: 500, loss is 5.2879709625244145 and perplexity is 197.9413871794395
At time: 261.9216797351837 and batch: 550, loss is 5.308740262985229 and perplexity is 202.09548080666767
At time: 262.485515832901 and batch: 600, loss is 5.348926801681518 and perplexity is 210.38239463671096
At time: 263.04345774650574 and batch: 650, loss is 5.318372459411621 and perplexity is 204.05150947766165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.488180123123468 and perplexity of 241.8167294978711
Finished 34 epochs...
Completing Train Step...
At time: 264.07310700416565 and batch: 50, loss is 5.337314929962158 and perplexity is 207.95359003263337
At time: 264.65285301208496 and batch: 100, loss is 5.3067898941040035 and perplexity is 201.7017041995115
At time: 265.21139907836914 and batch: 150, loss is 5.311046028137207 and perplexity is 202.56200316248837
At time: 265.76420426368713 and batch: 200, loss is 5.298333511352539 and perplexity is 200.00322898696615
At time: 266.316917181015 and batch: 250, loss is 5.292920408248901 and perplexity is 198.92351582318804
At time: 266.86950874328613 and batch: 300, loss is 5.327150096893311 and perplexity is 205.85048347580994
At time: 267.4216961860657 and batch: 350, loss is 5.273461332321167 and perplexity is 195.0900666763103
At time: 267.9745888710022 and batch: 400, loss is 5.31911410331726 and perplexity is 204.2028991677837
At time: 268.52742099761963 and batch: 450, loss is 5.28774847984314 and perplexity is 197.89735354742652
At time: 269.07907795906067 and batch: 500, loss is 5.28762529373169 and perplexity is 197.87297684344327
At time: 269.6315336227417 and batch: 550, loss is 5.3084015369415285 and perplexity is 202.0270373964414
At time: 270.18375849723816 and batch: 600, loss is 5.348529863357544 and perplexity is 210.29890237332594
At time: 270.7350318431854 and batch: 650, loss is 5.3178668785095216 and perplexity is 203.94837100604295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.487810920266544 and perplexity of 241.72746654957035
Finished 35 epochs...
Completing Train Step...
At time: 271.77910447120667 and batch: 50, loss is 5.3367735958099365 and perplexity is 207.84104811641134
At time: 272.34308075904846 and batch: 100, loss is 5.306294612884521 and perplexity is 201.60182986846579
At time: 272.9005069732666 and batch: 150, loss is 5.310611772537231 and perplexity is 202.47405857486956
At time: 273.45189595222473 and batch: 200, loss is 5.297849073410034 and perplexity is 199.90636329882295
At time: 274.0043110847473 and batch: 250, loss is 5.292380638122559 and perplexity is 198.8161718250677
At time: 274.55823159217834 and batch: 300, loss is 5.326653957366943 and perplexity is 205.74837824575047
At time: 275.1298816204071 and batch: 350, loss is 5.273054437637329 and perplexity is 195.0107017129959
At time: 275.69060945510864 and batch: 400, loss is 5.3187659740447994 and perplexity is 204.13182253370832
At time: 276.24335956573486 and batch: 450, loss is 5.287392416000366 and perplexity is 197.8269019986166
At time: 276.8091719150543 and batch: 500, loss is 5.287312698364258 and perplexity is 197.8111323341992
At time: 277.3680076599121 and batch: 550, loss is 5.308111267089844 and perplexity is 201.9684035484909
At time: 277.9329581260681 and batch: 600, loss is 5.348178825378418 and perplexity is 210.2250924274295
At time: 278.5001528263092 and batch: 650, loss is 5.317442464828491 and perplexity is 203.8618308928671
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.487660725911458 and perplexity of 241.6911631749745
Finished 36 epochs...
Completing Train Step...
At time: 279.53354811668396 and batch: 50, loss is 5.336378726959229 and perplexity is 207.75899436191372
At time: 280.0981459617615 and batch: 100, loss is 5.305848846435547 and perplexity is 201.51198256360334
At time: 280.649986743927 and batch: 150, loss is 5.310182781219482 and perplexity is 202.38721759001757
At time: 281.20317029953003 and batch: 200, loss is 5.297443790435791 and perplexity is 199.8253610688562
At time: 281.7555365562439 and batch: 250, loss is 5.29187014579773 and perplexity is 198.7147035968785
At time: 282.3224196434021 and batch: 300, loss is 5.3262364959716795 and perplexity is 205.66250416649834
At time: 282.8828139305115 and batch: 350, loss is 5.272651500701905 and perplexity is 194.93214052713734
At time: 283.4347834587097 and batch: 400, loss is 5.3184464740753175 and perplexity is 204.0666128404407
At time: 283.997136592865 and batch: 450, loss is 5.2870156192779545 and perplexity is 197.75237551188837
At time: 284.54969811439514 and batch: 500, loss is 5.286987190246582 and perplexity is 197.74675368331287
At time: 285.1017060279846 and batch: 550, loss is 5.307788581848144 and perplexity is 201.90324183930255
At time: 285.6653504371643 and batch: 600, loss is 5.347822818756104 and perplexity is 210.1502642228072
At time: 286.22473311424255 and batch: 650, loss is 5.317000102996826 and perplexity is 203.77167014325423
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.487406412760417 and perplexity of 241.62970574873577
Finished 37 epochs...
Completing Train Step...
At time: 287.3118848800659 and batch: 50, loss is 5.335948266983032 and perplexity is 207.6695816758181
At time: 287.8725595474243 and batch: 100, loss is 5.305347480773926 and perplexity is 201.410976697693
At time: 288.4335799217224 and batch: 150, loss is 5.309776887893677 and perplexity is 202.30508663849966
At time: 288.987361907959 and batch: 200, loss is 5.296922740936279 and perplexity is 199.72126928532234
At time: 289.5536193847656 and batch: 250, loss is 5.291370439529419 and perplexity is 198.6154294199156
At time: 290.1064772605896 and batch: 300, loss is 5.325725088119507 and perplexity is 205.55735363666508
At time: 290.6595675945282 and batch: 350, loss is 5.272281446456909 and perplexity is 194.86001840642075
At time: 291.22200560569763 and batch: 400, loss is 5.318088340759277 and perplexity is 203.99354287286693
At time: 291.77917098999023 and batch: 450, loss is 5.286625385284424 and perplexity is 197.67522086782373
At time: 292.33265209198 and batch: 500, loss is 5.286662607192993 and perplexity is 197.68257885375962
At time: 292.8838515281677 and batch: 550, loss is 5.30750449180603 and perplexity is 201.8458912855718
At time: 293.43582105636597 and batch: 600, loss is 5.347548112869263 and perplexity is 210.09254263669683
At time: 293.9985990524292 and batch: 650, loss is 5.316535081863403 and perplexity is 203.67693403909715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.487118590111826 and perplexity of 241.5601692544179
Finished 38 epochs...
Completing Train Step...
At time: 295.0599904060364 and batch: 50, loss is 5.335476655960083 and perplexity is 207.57166550295653
At time: 295.6286623477936 and batch: 100, loss is 5.304882621765136 and perplexity is 201.31737074917586
At time: 296.18706488609314 and batch: 150, loss is 5.309321403503418 and perplexity is 202.2129608119965
At time: 296.7483296394348 and batch: 200, loss is 5.296450557708741 and perplexity is 199.6269865129068
At time: 297.31099152565 and batch: 250, loss is 5.290900630950928 and perplexity is 198.52214010313
At time: 297.864862203598 and batch: 300, loss is 5.32531476020813 and perplexity is 205.47302501945512
At time: 298.4166579246521 and batch: 350, loss is 5.271880407333374 and perplexity is 194.7818875832325
At time: 298.96948742866516 and batch: 400, loss is 5.317756328582764 and perplexity is 203.92582577477566
At time: 299.53359174728394 and batch: 450, loss is 5.2862723922729495 and perplexity is 197.6054552104349
At time: 300.08885526657104 and batch: 500, loss is 5.286329784393311 and perplexity is 197.6167965319525
At time: 300.64684772491455 and batch: 550, loss is 5.307216100692749 and perplexity is 201.7876891171701
At time: 301.2050726413727 and batch: 600, loss is 5.347236003875732 and perplexity is 210.02698109637157
At time: 301.7585778236389 and batch: 650, loss is 5.3160817527771 and perplexity is 203.58462228607036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.486898983226103 and perplexity of 241.50712680239036
Finished 39 epochs...
Completing Train Step...
At time: 302.8299036026001 and batch: 50, loss is 5.335089187622071 and perplexity is 207.49125363423835
At time: 303.38828468322754 and batch: 100, loss is 5.304485521316528 and perplexity is 201.23744340158186
At time: 303.96869111061096 and batch: 150, loss is 5.3089332771301265 and perplexity is 202.13449185780436
At time: 304.53013157844543 and batch: 200, loss is 5.296007490158081 and perplexity is 199.53855786432587
At time: 305.08147978782654 and batch: 250, loss is 5.29042384147644 and perplexity is 198.427509397531
At time: 305.63281750679016 and batch: 300, loss is 5.324900054931641 and perplexity is 205.38783193803457
At time: 306.1847767829895 and batch: 350, loss is 5.271463289260864 and perplexity is 194.70065748017373
At time: 306.73705983161926 and batch: 400, loss is 5.3173902797698975 and perplexity is 203.8511926288586
At time: 307.29320669174194 and batch: 450, loss is 5.285907106399536 and perplexity is 197.53328591115238
At time: 307.8510503768921 and batch: 500, loss is 5.285988264083862 and perplexity is 197.54931790576518
At time: 308.40295004844666 and batch: 550, loss is 5.306930160522461 and perplexity is 201.7299981594561
At time: 308.9544243812561 and batch: 600, loss is 5.3468922519683835 and perplexity is 209.9547963285616
At time: 309.5037579536438 and batch: 650, loss is 5.31564661026001 and perplexity is 203.4960532325659
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.486685360179228 and perplexity of 241.45554082429388
Finished 40 epochs...
Completing Train Step...
At time: 310.5293004512787 and batch: 50, loss is 5.33469479560852 and perplexity is 207.40943687592195
At time: 311.1051254272461 and batch: 100, loss is 5.304128980636596 and perplexity is 201.165706855942
At time: 311.6557903289795 and batch: 150, loss is 5.308526268005371 and perplexity is 202.0522380153582
At time: 312.2091987133026 and batch: 200, loss is 5.295559482574463 and perplexity is 199.4491830989597
At time: 312.7630863189697 and batch: 250, loss is 5.289850578308106 and perplexity is 198.3137908132613
At time: 313.315149307251 and batch: 300, loss is 5.324445533752441 and perplexity is 205.29450003073987
At time: 313.86898279190063 and batch: 350, loss is 5.271049680709839 and perplexity is 194.6201442749727
At time: 314.42227363586426 and batch: 400, loss is 5.316974668502808 and perplexity is 203.76648737983962
At time: 314.97452116012573 and batch: 450, loss is 5.285510873794555 and perplexity is 197.45503228704786
At time: 315.5240478515625 and batch: 500, loss is 5.2856275177001955 and perplexity is 197.47806555652127
At time: 316.07627272605896 and batch: 550, loss is 5.306576337814331 and perplexity is 201.65863413104773
At time: 316.62815260887146 and batch: 600, loss is 5.346491279602051 and perplexity is 209.8706271329434
At time: 317.1803729534149 and batch: 650, loss is 5.315309982299805 and perplexity is 203.4275622998848
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.4864950741038605 and perplexity of 241.40959956818392
Finished 41 epochs...
Completing Train Step...
At time: 318.2316746711731 and batch: 50, loss is 5.3343112182617185 and perplexity is 207.32989457071193
At time: 318.78237104415894 and batch: 100, loss is 5.30377272605896 and perplexity is 201.09405341620192
At time: 319.3335950374603 and batch: 150, loss is 5.308024368286133 and perplexity is 201.95085349838533
At time: 319.884818315506 and batch: 200, loss is 5.295178546905517 and perplexity is 199.3732202603712
At time: 320.4411826133728 and batch: 250, loss is 5.2892631149291995 and perplexity is 198.19732293728197
At time: 320.99546337127686 and batch: 300, loss is 5.323903512954712 and perplexity is 205.18325629299704
At time: 321.54805970191956 and batch: 350, loss is 5.270596837997436 and perplexity is 194.53203191297717
At time: 322.1017277240753 and batch: 400, loss is 5.3165256690979 and perplexity is 203.67501688490154
At time: 322.6563878059387 and batch: 450, loss is 5.284979209899903 and perplexity is 197.35008047757944
At time: 323.20866441726685 and batch: 500, loss is 5.285225887298584 and perplexity is 197.3987682869053
At time: 323.76124382019043 and batch: 550, loss is 5.306216669082642 and perplexity is 201.58611686775345
At time: 324.31346321105957 and batch: 600, loss is 5.346254777908325 and perplexity is 209.82099824305277
At time: 324.86595344543457 and batch: 650, loss is 5.3149622631073 and perplexity is 203.35683892883793
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.486084582758885 and perplexity of 241.31052335332265
Finished 42 epochs...
Completing Train Step...
At time: 325.90341663360596 and batch: 50, loss is 5.333948812484741 and perplexity is 207.25477063267618
At time: 326.46865463256836 and batch: 100, loss is 5.303405599594116 and perplexity is 201.02024001752525
At time: 327.01819682121277 and batch: 150, loss is 5.307670269012451 and perplexity is 201.87935550728264
At time: 327.57038474082947 and batch: 200, loss is 5.294680986404419 and perplexity is 199.2740446959607
At time: 328.12222385406494 and batch: 250, loss is 5.2888140296936035 and perplexity is 198.10833542879985
At time: 328.67359495162964 and batch: 300, loss is 5.323507766723633 and perplexity is 205.10207185791526
At time: 329.2245924472809 and batch: 350, loss is 5.270213947296143 and perplexity is 194.4575616647466
At time: 329.77659606933594 and batch: 400, loss is 5.316138076782226 and perplexity is 203.59608931031102
At time: 330.3282175064087 and batch: 450, loss is 5.284465351104736 and perplexity is 197.2486964537663
At time: 330.88023805618286 and batch: 500, loss is 5.2847255611419675 and perplexity is 197.30002922277555
At time: 331.4319214820862 and batch: 550, loss is 5.305606126785278 and perplexity is 201.46307758101085
At time: 331.99826288223267 and batch: 600, loss is 5.345564203262329 and perplexity is 209.67615120107226
At time: 332.5491349697113 and batch: 650, loss is 5.3141724967956545 and perplexity is 203.19629795148154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.485666312423407 and perplexity of 241.20961142546963
Finished 43 epochs...
Completing Train Step...
At time: 333.6006591320038 and batch: 50, loss is 5.332880697250366 and perplexity is 207.033516838076
At time: 334.15396094322205 and batch: 100, loss is 5.302173976898193 and perplexity is 200.77281132825138
At time: 334.70721101760864 and batch: 150, loss is 5.3065994834899906 and perplexity is 201.66330171040423
At time: 335.26093435287476 and batch: 200, loss is 5.293927316665649 and perplexity is 199.12391446016528
At time: 335.8165476322174 and batch: 250, loss is 5.287921590805054 and perplexity is 197.9316147140655
At time: 336.3715648651123 and batch: 300, loss is 5.322700996398925 and perplexity is 204.9366683231074
At time: 336.9277672767639 and batch: 350, loss is 5.2691907787323 and perplexity is 194.25870055221768
At time: 337.4870271682739 and batch: 400, loss is 5.3150831317901615 and perplexity is 203.38141988761427
At time: 338.04295229911804 and batch: 450, loss is 5.283687257766724 and perplexity is 197.0952782517048
At time: 338.59855937957764 and batch: 500, loss is 5.284004344940185 and perplexity is 197.15778454583653
At time: 339.16292572021484 and batch: 550, loss is 5.305042448043824 and perplexity is 201.3495491267845
At time: 339.7150435447693 and batch: 600, loss is 5.344828004837036 and perplexity is 209.5218447557869
At time: 340.2829341888428 and batch: 650, loss is 5.313402080535889 and perplexity is 203.0398125068211
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.485100839652267 and perplexity of 241.07325251533007
Finished 44 epochs...
Completing Train Step...
At time: 341.3261241912842 and batch: 50, loss is 5.33202543258667 and perplexity is 206.85652408553605
At time: 341.89177560806274 and batch: 100, loss is 5.301450462341308 and perplexity is 200.62760181356197
At time: 342.4437484741211 and batch: 150, loss is 5.305965957641601 and perplexity is 201.53558325684165
At time: 342.9956431388855 and batch: 200, loss is 5.293210506439209 and perplexity is 198.9812315463467
At time: 343.5474007129669 and batch: 250, loss is 5.286868896484375 and perplexity is 197.723362859377
At time: 344.09852409362793 and batch: 300, loss is 5.321961688995361 and perplexity is 204.78521311982936
At time: 344.6499035358429 and batch: 350, loss is 5.26832407951355 and perplexity is 194.09040962755475
At time: 345.2026193141937 and batch: 400, loss is 5.313832845687866 and perplexity is 203.1272938231115
At time: 345.75512170791626 and batch: 450, loss is 5.2827819728851315 and perplexity is 196.9169316155072
At time: 346.32161355018616 and batch: 500, loss is 5.282877626419068 and perplexity is 196.93576831679226
At time: 346.87703037261963 and batch: 550, loss is 5.3038288021087645 and perplexity is 201.1053302925351
At time: 347.4325008392334 and batch: 600, loss is 5.343888731002807 and perplexity is 209.32513876416567
At time: 347.986478805542 and batch: 650, loss is 5.312280597686768 and perplexity is 202.81223447568783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.484293619791667 and perplexity of 240.87873191904708
Finished 45 epochs...
Completing Train Step...
At time: 349.0324721336365 and batch: 50, loss is 5.330798816680908 and perplexity is 206.60294613604938
At time: 349.5872001647949 and batch: 100, loss is 5.300012550354004 and perplexity is 200.3393242884546
At time: 350.1416003704071 and batch: 150, loss is 5.3048912525177006 and perplexity is 201.31910827708776
At time: 350.6930503845215 and batch: 200, loss is 5.291046504974365 and perplexity is 198.55110143875018
At time: 351.24559140205383 and batch: 250, loss is 5.2843898487091066 and perplexity is 197.23380426685415
At time: 351.79758167266846 and batch: 300, loss is 5.3195035362243654 and perplexity is 204.28243798295668
At time: 352.34980869293213 and batch: 350, loss is 5.265411338806152 and perplexity is 193.5258971288974
At time: 352.9035179615021 and batch: 400, loss is 5.310891752243042 and perplexity is 202.5307551387969
At time: 353.4601435661316 and batch: 450, loss is 5.278636960983277 and perplexity is 196.10239788250976
At time: 354.0197253227234 and batch: 500, loss is 5.276636295318603 and perplexity is 195.710454752622
At time: 354.57360768318176 and batch: 550, loss is 5.29312539100647 and perplexity is 198.96429589346957
At time: 355.12648248672485 and batch: 600, loss is 5.33191481590271 and perplexity is 206.83364356829298
At time: 355.67867970466614 and batch: 650, loss is 5.296857843399048 and perplexity is 199.708308287386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.466955446729473 and perplexity of 236.73833198406632
Finished 46 epochs...
Completing Train Step...
At time: 356.72678232192993 and batch: 50, loss is 5.311797399520874 and perplexity is 202.71425964850292
At time: 357.2949137687683 and batch: 100, loss is 5.280657024383545 and perplexity is 196.49893754195827
At time: 357.8489673137665 and batch: 150, loss is 5.281679296493531 and perplexity is 196.69991563509643
At time: 358.4021942615509 and batch: 200, loss is 5.264476156234741 and perplexity is 193.34499968203934
At time: 358.9576632976532 and batch: 250, loss is 5.257759304046631 and perplexity is 192.050681635578
At time: 359.51097869873047 and batch: 300, loss is 5.289475326538086 and perplexity is 198.23938717316116
At time: 360.0636320114136 and batch: 350, loss is 5.231763935089111 and perplexity is 187.12258467145438
At time: 360.63030886650085 and batch: 400, loss is 5.2747878074646 and perplexity is 195.3490205104293
At time: 361.18127155303955 and batch: 450, loss is 5.240542135238647 and perplexity is 188.7724148477435
At time: 361.7345063686371 and batch: 500, loss is 5.241642208099365 and perplexity is 188.98019252250646
At time: 362.28754210472107 and batch: 550, loss is 5.262106342315674 and perplexity is 192.88735049649628
At time: 362.8402271270752 and batch: 600, loss is 5.298581743240357 and perplexity is 200.05288232858356
At time: 363.39364552497864 and batch: 650, loss is 5.265680847167968 and perplexity is 193.5780610053863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.436383415670956 and perplexity of 229.61027511129515
Finished 47 epochs...
Completing Train Step...
At time: 364.43495655059814 and batch: 50, loss is 5.2841669368743895 and perplexity is 197.18984341755564
At time: 364.98570108413696 and batch: 100, loss is 5.251977319717407 and perplexity is 190.9434516817457
At time: 365.53636050224304 and batch: 150, loss is 5.250375747680664 and perplexity is 190.63788674637627
At time: 366.0858426094055 and batch: 200, loss is 5.238164472579956 and perplexity is 188.32411089495207
At time: 366.6497845649719 and batch: 250, loss is 5.233265705108643 and perplexity is 187.4038108747773
At time: 367.2114768028259 and batch: 300, loss is 5.265684356689453 and perplexity is 193.57874037294255
At time: 367.7744436264038 and batch: 350, loss is 5.207536869049072 and perplexity is 182.64362851342383
At time: 368.3316550254822 and batch: 400, loss is 5.253435077667237 and perplexity is 191.22200399800104
At time: 368.8840973377228 and batch: 450, loss is 5.219764633178711 and perplexity is 184.89066182167863
At time: 369.4367895126343 and batch: 500, loss is 5.221739282608032 and perplexity is 185.2561167654801
At time: 369.9891126155853 and batch: 550, loss is 5.243434991836548 and perplexity is 189.31929701799726
At time: 370.5408601760864 and batch: 600, loss is 5.281558818817139 and perplexity is 196.67621911379385
At time: 371.09142994880676 and batch: 650, loss is 5.2496801948547365 and perplexity is 190.50533412953857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.420610016467524 and perplexity of 226.01695452205624
Finished 48 epochs...
Completing Train Step...
At time: 372.1060781478882 and batch: 50, loss is 5.2699493980407714 and perplexity is 194.40612486569026
At time: 372.6767063140869 and batch: 100, loss is 5.23589560508728 and perplexity is 187.89731279890628
At time: 373.23454880714417 and batch: 150, loss is 5.2333105850219725 and perplexity is 187.41222173030482
At time: 373.7859055995941 and batch: 200, loss is 5.221085557937622 and perplexity is 185.13504984813292
At time: 374.33456349372864 and batch: 250, loss is 5.217947216033935 and perplexity is 184.5549435254753
At time: 374.898485660553 and batch: 300, loss is 5.25026840209961 and perplexity is 190.61742370998041
At time: 375.4596893787384 and batch: 350, loss is 5.193464937210083 and perplexity is 180.0914787751663
At time: 376.02232813835144 and batch: 400, loss is 5.240917644500732 and perplexity is 188.8433139487464
At time: 376.5712306499481 and batch: 450, loss is 5.20879900932312 and perplexity is 182.8742959294657
At time: 377.12904477119446 and batch: 500, loss is 5.209592552185058 and perplexity is 183.01947211577743
At time: 377.6892035007477 and batch: 550, loss is 5.2325474071502684 and perplexity is 187.2692474341424
At time: 378.2412521839142 and batch: 600, loss is 5.272440729141235 and perplexity is 194.8910587052457
At time: 378.7921016216278 and batch: 650, loss is 5.241715879440307 and perplexity is 188.99411545955556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.411343443627451 and perplexity of 223.9322260120204
Finished 49 epochs...
Completing Train Step...
At time: 379.8346781730652 and batch: 50, loss is 5.260633974075318 and perplexity is 192.60355826226376
At time: 380.3872311115265 and batch: 100, loss is 5.226500577926636 and perplexity is 186.14027905554016
At time: 380.94187688827515 and batch: 150, loss is 5.224849262237549 and perplexity is 185.83315634043328
At time: 381.4935405254364 and batch: 200, loss is 5.211557273864746 and perplexity is 183.3794079114956
At time: 382.04527592658997 and batch: 250, loss is 5.210142135620117 and perplexity is 183.12008423078896
At time: 382.59733033180237 and batch: 300, loss is 5.243145513534546 and perplexity is 189.26450112085456
At time: 383.15796756744385 and batch: 350, loss is 5.187408351898194 and perplexity is 179.00403578977694
At time: 383.7142233848572 and batch: 400, loss is 5.235274562835693 and perplexity is 187.78065685658254
At time: 384.26962018013 and batch: 450, loss is 5.202032632827759 and perplexity is 181.6410765104523
At time: 384.8337278366089 and batch: 500, loss is 5.202658653259277 and perplexity is 181.75482313569177
At time: 385.38698649406433 and batch: 550, loss is 5.226840982437134 and perplexity is 186.20365283186365
At time: 385.93516993522644 and batch: 600, loss is 5.26568039894104 and perplexity is 193.57797423850604
At time: 386.48908019065857 and batch: 650, loss is 5.2353683185577395 and perplexity is 187.79826319298695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.402979233685662 and perplexity of 222.06702121730007
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa09ad998d0>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'seq_len': 20, 'anneal': 5.209266033185621, 'data': 'ptb', 'dropout': 0.6712772295736538, 'batch_size': 80, 'num_layers': 1, 'lr': 22.05395899225164}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7963519096374512 and batch: 50, loss is 6.612345886230469 and perplexity is 744.2268440147492
At time: 1.3704893589019775 and batch: 100, loss is 5.982430810928345 and perplexity is 396.4027781682391
At time: 1.9280507564544678 and batch: 150, loss is 5.980910549163818 and perplexity is 395.80060003141546
At time: 2.4868805408477783 and batch: 200, loss is 6.017182884216308 and perplexity is 410.4207628088353
At time: 3.042465925216675 and batch: 250, loss is 6.035497350692749 and perplexity is 418.00665384717934
At time: 3.605661630630493 and batch: 300, loss is 6.074443349838257 and perplexity is 434.607510829023
At time: 4.172919750213623 and batch: 350, loss is 6.039299831390381 and perplexity is 419.5991418635906
At time: 4.754100799560547 and batch: 400, loss is 6.1081561183929445 and perplexity is 449.5091089766247
At time: 5.313048362731934 and batch: 450, loss is 6.115478105545044 and perplexity is 452.8124877876664
At time: 5.869854211807251 and batch: 500, loss is 6.132609195709229 and perplexity is 460.6364847983406
At time: 6.425579309463501 and batch: 550, loss is 6.153198099136352 and perplexity is 470.2187909923561
At time: 6.982474327087402 and batch: 600, loss is 6.183811101913452 and perplexity is 484.8361999898253
At time: 7.5481226444244385 and batch: 650, loss is 6.198824110031128 and perplexity is 492.16996297146596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 6.197180056104473 and perplexity of 491.3614737932715
Finished 1 epochs...
Completing Train Step...
At time: 8.68763017654419 and batch: 50, loss is 6.130499668121338 and perplexity is 459.6657836460766
At time: 9.240957021713257 and batch: 100, loss is 6.220818843841553 and perplexity is 503.11503604449223
At time: 9.792352199554443 and batch: 150, loss is 6.32554874420166 and perplexity is 558.6642933594725
At time: 10.342232465744019 and batch: 200, loss is 6.389759588241577 and perplexity is 595.7133459781497
At time: 10.892145156860352 and batch: 250, loss is 6.3565825176239015 and perplexity is 576.2735825416758
At time: 11.442104578018188 and batch: 300, loss is 6.474983892440796 and perplexity is 648.7087784116683
At time: 11.989886045455933 and batch: 350, loss is 6.4408924102783205 and perplexity is 626.966061186234
At time: 12.53849983215332 and batch: 400, loss is 6.452493600845337 and perplexity is 634.2819685062383
At time: 13.087991952896118 and batch: 450, loss is 6.303266544342041 and perplexity is 546.3536869599606
At time: 13.638368844985962 and batch: 500, loss is 6.328595542907715 and perplexity is 560.3690266764811
At time: 14.187164783477783 and batch: 550, loss is 6.618675327301025 and perplexity is 748.9523230115548
At time: 14.741067886352539 and batch: 600, loss is 6.499950408935547 and perplexity is 665.1086487806363
At time: 15.295496463775635 and batch: 650, loss is 6.764605875015259 and perplexity is 866.6245820724515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 7.020348044002757 and perplexity of 1119.1760724889211
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 16.347707271575928 and batch: 50, loss is 6.108530826568604 and perplexity is 449.6775752756702
At time: 16.91814875602722 and batch: 100, loss is 5.920676364898681 and perplexity is 372.66368529000994
At time: 17.478147745132446 and batch: 150, loss is 5.910351686477661 and perplexity is 368.83584718252945
At time: 18.028762102127075 and batch: 200, loss is 5.9076974582672115 and perplexity is 367.85817073408924
At time: 18.586843013763428 and batch: 250, loss is 5.872376098632812 and perplexity is 355.0917115953382
At time: 19.15149760246277 and batch: 300, loss is 5.904603967666626 and perplexity is 366.721963270225
At time: 19.703670978546143 and batch: 350, loss is 5.846804828643799 and perplexity is 346.12667772478494
At time: 20.26989221572876 and batch: 400, loss is 5.907036457061768 and perplexity is 367.615096384871
At time: 20.829254150390625 and batch: 450, loss is 5.878735942840576 and perplexity is 357.3572361158015
At time: 21.382981538772583 and batch: 500, loss is 5.8623262310028075 and perplexity is 351.5409590790834
At time: 21.935674905776978 and batch: 550, loss is 5.86030686378479 and perplexity is 350.8317850728904
At time: 22.488704442977905 and batch: 600, loss is 5.892017278671265 and perplexity is 362.1350753925774
At time: 23.0395290851593 and batch: 650, loss is 5.873250150680542 and perplexity is 355.402215911689
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.929886163449755 and perplexity of 376.1116961296819
Finished 3 epochs...
Completing Train Step...
At time: 24.103594541549683 and batch: 50, loss is 5.854524354934693 and perplexity is 348.80895133433216
At time: 24.67345142364502 and batch: 100, loss is 5.823973140716553 and perplexity is 338.31355423730474
At time: 25.233599424362183 and batch: 150, loss is 5.81185133934021 and perplexity is 334.2373399722253
At time: 25.78430151939392 and batch: 200, loss is 5.812350769042968 and perplexity is 334.40430971893323
At time: 26.33452844619751 and batch: 250, loss is 5.791629276275635 and perplexity is 327.5462532089974
At time: 26.884398460388184 and batch: 300, loss is 5.825767831802368 and perplexity is 338.92126772317675
At time: 27.434848070144653 and batch: 350, loss is 5.769422664642334 and perplexity is 320.35272829074984
At time: 27.992693424224854 and batch: 400, loss is 5.8278971290588375 and perplexity is 339.64370071303676
At time: 28.54175877571106 and batch: 450, loss is 5.790237598419189 and perplexity is 327.09073138478266
At time: 29.098145484924316 and batch: 500, loss is 5.779562530517578 and perplexity is 323.6175866277081
At time: 29.658275365829468 and batch: 550, loss is 5.787171297073364 and perplexity is 326.08930875134644
At time: 30.209380388259888 and batch: 600, loss is 5.8244252777099605 and perplexity is 338.4665528961171
At time: 30.76324701309204 and batch: 650, loss is 5.806170597076416 and perplexity is 332.3440066498364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.882484585631127 and perplexity of 358.6993547326002
Finished 4 epochs...
Completing Train Step...
At time: 31.847143411636353 and batch: 50, loss is 5.797372732162476 and perplexity is 329.4329134538344
At time: 32.41336154937744 and batch: 100, loss is 5.767768354415893 and perplexity is 319.823203616077
At time: 32.964548110961914 and batch: 150, loss is 5.755837297439575 and perplexity is 316.03004792595505
At time: 33.530662059783936 and batch: 200, loss is 5.763212060928344 and perplexity is 318.3693099346424
At time: 34.083269357681274 and batch: 250, loss is 5.740729656219482 and perplexity is 311.2914638887345
At time: 34.63393521308899 and batch: 300, loss is 5.769977264404297 and perplexity is 320.5304451139028
At time: 35.18621563911438 and batch: 350, loss is 5.707638854980469 and perplexity is 301.1591477179685
At time: 35.73750710487366 and batch: 400, loss is 5.766777305603028 and perplexity is 319.50640021955115
At time: 36.28531551361084 and batch: 450, loss is 5.728251390457153 and perplexity is 307.43122093036885
At time: 36.83673143386841 and batch: 500, loss is 5.718603296279907 and perplexity is 304.479358379736
At time: 37.39000344276428 and batch: 550, loss is 5.7314084815979 and perplexity is 308.4033430459702
At time: 37.940858125686646 and batch: 600, loss is 5.768222217559814 and perplexity is 319.9683925262244
At time: 38.489667654037476 and batch: 650, loss is 5.743319845199585 and perplexity is 312.09881274977994
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.82908660290288 and perplexity of 340.0479383787325
Finished 5 epochs...
Completing Train Step...
At time: 39.559563636779785 and batch: 50, loss is 5.738912687301636 and perplexity is 310.7263705084771
At time: 40.1183648109436 and batch: 100, loss is 5.715421714782715 and perplexity is 303.5121718940106
At time: 40.66839027404785 and batch: 150, loss is 5.706263341903687 and perplexity is 300.7451841435143
At time: 41.21970319747925 and batch: 200, loss is 5.7185006523132325 and perplexity is 304.4481070145309
At time: 41.77025866508484 and batch: 250, loss is 5.696024866104126 and perplexity is 297.68172121031114
At time: 42.32045340538025 and batch: 300, loss is 5.731579208374024 and perplexity is 308.4560002493435
At time: 42.87044835090637 and batch: 350, loss is 5.6772489261627195 and perplexity is 292.14461197464357
At time: 43.42072105407715 and batch: 400, loss is 5.730044374465942 and perplexity is 307.9829346524363
At time: 43.97179865837097 and batch: 450, loss is 5.694092721939087 and perplexity is 297.1071125018273
At time: 44.522372007369995 and batch: 500, loss is 5.68452317237854 and perplexity is 294.27749195479186
At time: 45.08577036857605 and batch: 550, loss is 5.699472579956055 and perplexity is 298.7098138702582
At time: 45.644349575042725 and batch: 600, loss is 5.734580574035644 and perplexity is 309.383180203633
At time: 46.19860625267029 and batch: 650, loss is 5.703130540847778 and perplexity is 299.80448359627474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.7899822160309435 and perplexity of 327.00720883817627
Finished 6 epochs...
Completing Train Step...
At time: 47.24245810508728 and batch: 50, loss is 5.702003059387207 and perplexity is 299.4666500855221
At time: 47.81491756439209 and batch: 100, loss is 5.682783060073852 and perplexity is 293.7658613465081
At time: 48.366716623306274 and batch: 150, loss is 5.677919206619262 and perplexity is 292.340496439965
At time: 48.92294931411743 and batch: 200, loss is 5.686499347686768 and perplexity is 294.85961086322243
At time: 49.4851975440979 and batch: 250, loss is 5.6624588203430175 and perplexity is 287.8555582129843
At time: 50.03682613372803 and batch: 300, loss is 5.693221588134765 and perplexity is 296.8484051533415
At time: 50.600361347198486 and batch: 350, loss is 5.636252346038819 and perplexity is 280.4098676959242
At time: 51.15567493438721 and batch: 400, loss is 5.691488018035889 and perplexity is 296.33424343072875
At time: 51.70759963989258 and batch: 450, loss is 5.648825473785401 and perplexity is 283.95775405969465
At time: 52.25839447975159 and batch: 500, loss is 5.630973434448242 and perplexity is 278.93350900718997
At time: 52.80815100669861 and batch: 550, loss is 5.629279623031616 and perplexity is 278.4614481490513
At time: 53.35894179344177 and batch: 600, loss is 5.654777536392212 and perplexity is 285.65292827649927
At time: 53.909286975860596 and batch: 650, loss is 5.61525818824768 and perplexity is 274.5842644741993
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.699595133463542 and perplexity of 298.7464240489759
Finished 7 epochs...
Completing Train Step...
At time: 54.952165842056274 and batch: 50, loss is 5.611705369949341 and perplexity is 273.61044739684024
At time: 55.504103660583496 and batch: 100, loss is 5.586856203079224 and perplexity is 266.8952349884239
At time: 56.065656900405884 and batch: 150, loss is 5.5764938163757325 and perplexity is 264.1438434657286
At time: 56.6270055770874 and batch: 200, loss is 5.579496059417725 and perplexity is 264.9380590994861
At time: 57.19055891036987 and batch: 250, loss is 5.561104459762573 and perplexity is 260.1099587194474
At time: 57.748764753341675 and batch: 300, loss is 5.584454069137573 and perplexity is 266.2548862951728
At time: 58.298333406448364 and batch: 350, loss is 5.528863687515258 and perplexity is 251.8575594726978
At time: 58.85413861274719 and batch: 400, loss is 5.5767153453826905 and perplexity is 264.20236547098716
At time: 59.412269115448 and batch: 450, loss is 5.540227746963501 and perplexity is 254.73600820500894
At time: 59.96395206451416 and batch: 500, loss is 5.534781579971313 and perplexity is 253.35244434510568
At time: 60.52027916908264 and batch: 550, loss is 5.556060104370117 and perplexity is 258.8011754058538
At time: 61.08731460571289 and batch: 600, loss is 5.591968402862549 and perplexity is 268.2631502999247
At time: 61.6780686378479 and batch: 650, loss is 5.564545278549194 and perplexity is 261.0064914708418
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.657449759689032 and perplexity of 286.41727748729147
Finished 8 epochs...
Completing Train Step...
At time: 62.73373222351074 and batch: 50, loss is 5.5683871841430665 and perplexity is 262.01118249871104
At time: 63.301586866378784 and batch: 100, loss is 5.546553955078125 and perplexity is 256.3526293565758
At time: 63.854859828948975 and batch: 150, loss is 5.540529251098633 and perplexity is 254.8128237443693
At time: 64.41047048568726 and batch: 200, loss is 5.54982780456543 and perplexity is 257.19326458653876
At time: 64.96149682998657 and batch: 250, loss is 5.534790382385254 and perplexity is 253.35467446800897
At time: 65.52155876159668 and batch: 300, loss is 5.558070278167724 and perplexity is 259.32193397970786
At time: 66.07866859436035 and batch: 350, loss is 5.506733922958374 and perplexity is 246.3452292243115
At time: 66.63863515853882 and batch: 400, loss is 5.559041194915771 and perplexity is 259.573836256826
At time: 67.19052076339722 and batch: 450, loss is 5.525423355102539 and perplexity is 250.992574518498
At time: 67.74129462242126 and batch: 500, loss is 5.5208448219299315 and perplexity is 249.8460234539536
At time: 68.30073523521423 and batch: 550, loss is 5.543424758911133 and perplexity is 255.55170546891947
At time: 68.8563711643219 and batch: 600, loss is 5.578753423690796 and perplexity is 264.74137967100006
At time: 69.40652751922607 and batch: 650, loss is 5.547552423477173 and perplexity is 256.60871718252656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.646835027956495 and perplexity of 283.39311366202287
Finished 9 epochs...
Completing Train Step...
At time: 70.4489336013794 and batch: 50, loss is 5.552044525146484 and perplexity is 257.76402256121264
At time: 71.00573945045471 and batch: 100, loss is 5.529769525527954 and perplexity is 252.08580498497656
At time: 71.5596911907196 and batch: 150, loss is 5.524817733764649 and perplexity is 250.84061407960428
At time: 72.12044477462769 and batch: 200, loss is 5.536008749008179 and perplexity is 253.6635414655558
At time: 72.6815767288208 and batch: 250, loss is 5.5207359313964846 and perplexity is 249.818819068357
At time: 73.23516821861267 and batch: 300, loss is 5.541601524353028 and perplexity is 255.08619926058117
At time: 73.79036331176758 and batch: 350, loss is 5.491329116821289 and perplexity is 242.57940906141948
At time: 74.36162161827087 and batch: 400, loss is 5.5445715427398685 and perplexity is 255.84493613611468
At time: 74.9249017238617 and batch: 450, loss is 5.510455799102783 and perplexity is 247.26380400666616
At time: 75.49082374572754 and batch: 500, loss is 5.506693449020386 and perplexity is 246.33525886455132
At time: 76.05610775947571 and batch: 550, loss is 5.529481391906739 and perplexity is 252.01318105232997
At time: 76.60650205612183 and batch: 600, loss is 5.566841630935669 and perplexity is 261.6065430516718
At time: 77.16006326675415 and batch: 650, loss is 5.535527353286743 and perplexity is 253.54145830952214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.637031704771752 and perplexity of 280.62849275772334
Finished 10 epochs...
Completing Train Step...
At time: 78.21485185623169 and batch: 50, loss is 5.539988231658936 and perplexity is 254.67500233863103
At time: 78.7814085483551 and batch: 100, loss is 5.520470533370972 and perplexity is 249.7525264443952
At time: 79.3446102142334 and batch: 150, loss is 5.514540090560913 and perplexity is 248.2757666176504
At time: 79.90220189094543 and batch: 200, loss is 5.5208750915527345 and perplexity is 249.85358631330422
At time: 80.4525933265686 and batch: 250, loss is 5.508915557861328 and perplexity is 246.88325124497845
At time: 81.01240038871765 and batch: 300, loss is 5.535264587402343 and perplexity is 253.47484501623123
At time: 81.56807851791382 and batch: 350, loss is 5.484564428329468 and perplexity is 240.94397276971637
At time: 82.11732029914856 and batch: 400, loss is 5.538318767547607 and perplexity is 254.25018626843317
At time: 82.67104148864746 and batch: 450, loss is 5.5018240833282475 and perplexity is 245.1386780653334
At time: 83.22764182090759 and batch: 500, loss is 5.499655046463013 and perplexity is 244.6075394733463
At time: 83.78765630722046 and batch: 550, loss is 5.519656066894531 and perplexity is 249.54919419908867
At time: 84.3466215133667 and batch: 600, loss is 5.559335956573486 and perplexity is 259.6503599486715
At time: 84.91063714027405 and batch: 650, loss is 5.528337917327881 and perplexity is 251.72517508139416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.631452971813726 and perplexity of 279.0673001236004
Finished 11 epochs...
Completing Train Step...
At time: 85.95485258102417 and batch: 50, loss is 5.530925493240357 and perplexity is 252.3773765274707
At time: 86.50666785240173 and batch: 100, loss is 5.5116282558441165 and perplexity is 247.55388013818987
At time: 87.06887197494507 and batch: 150, loss is 5.5029869556427 and perplexity is 245.423908858611
At time: 87.62338447570801 and batch: 200, loss is 5.5107825756072994 and perplexity is 247.34461721144197
At time: 88.17544960975647 and batch: 250, loss is 5.4987900352478025 and perplexity is 244.39604269510707
At time: 88.72793006896973 and batch: 300, loss is 5.522846841812134 and perplexity is 250.34672119657898
At time: 89.28582549095154 and batch: 350, loss is 5.470001001358032 and perplexity is 237.46043054395707
At time: 89.83747744560242 and batch: 400, loss is 5.510567140579224 and perplexity is 247.29133625638678
At time: 90.41411304473877 and batch: 450, loss is 5.462301273345947 and perplexity is 235.6390708008434
At time: 90.96655344963074 and batch: 500, loss is 5.449098701477051 and perplexity is 232.5484758273999
At time: 91.51520037651062 and batch: 550, loss is 5.4699256801605225 and perplexity is 237.4425454135406
At time: 92.06644201278687 and batch: 600, loss is 5.5065868377685545 and perplexity is 246.3089981541021
At time: 92.61729001998901 and batch: 650, loss is 5.470911626815796 and perplexity is 237.6767665428065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.586072734757965 and perplexity of 266.68621291849263
Finished 12 epochs...
Completing Train Step...
At time: 93.64882612228394 and batch: 50, loss is 5.477556772232056 and perplexity is 239.26142252199818
At time: 94.21300888061523 and batch: 100, loss is 5.451284236907959 and perplexity is 233.057274557024
At time: 94.77209234237671 and batch: 150, loss is 5.447193336486817 and perplexity is 232.10580795887788
At time: 95.33382940292358 and batch: 200, loss is 5.451973123550415 and perplexity is 233.21787991348435
At time: 95.88757634162903 and batch: 250, loss is 5.441049470901489 and perplexity is 230.68415277432797
At time: 96.4386773109436 and batch: 300, loss is 5.464158334732056 and perplexity is 236.07707359344528
At time: 96.99718761444092 and batch: 350, loss is 5.413052034378052 and perplexity is 224.31516158908053
At time: 97.56025171279907 and batch: 400, loss is 5.465616807937622 and perplexity is 236.42163688681745
At time: 98.11905312538147 and batch: 450, loss is 5.427745208740235 and perplexity is 227.63539602876693
At time: 98.68008089065552 and batch: 500, loss is 5.421849422454834 and perplexity is 226.29725495586086
At time: 99.24387860298157 and batch: 550, loss is 5.4447391319274905 and perplexity is 231.53687125582888
At time: 99.79877376556396 and batch: 600, loss is 5.4830896759033205 and perplexity is 240.58890194644118
At time: 100.36352586746216 and batch: 650, loss is 5.447666358947754 and perplexity is 232.21562519031735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.567789115157782 and perplexity of 261.85452858625354
Finished 13 epochs...
Completing Train Step...
At time: 101.44720602035522 and batch: 50, loss is 5.454941072463989 and perplexity is 233.91108685934515
At time: 102.0014717578888 and batch: 100, loss is 5.428071918487549 and perplexity is 227.70977888162324
At time: 102.56444692611694 and batch: 150, loss is 5.426031188964844 and perplexity is 227.24555864832507
At time: 103.12843799591064 and batch: 200, loss is 5.428369979858399 and perplexity is 227.7776604864121
At time: 103.68568587303162 and batch: 250, loss is 5.419215183258057 and perplexity is 225.70191833053414
At time: 104.2483081817627 and batch: 300, loss is 5.445257625579834 and perplexity is 231.65695278192285
At time: 104.8294928073883 and batch: 350, loss is 5.398978757858276 and perplexity is 221.18042205852544
At time: 105.39528584480286 and batch: 400, loss is 5.448959321975708 and perplexity is 232.51606559551442
At time: 105.95867943763733 and batch: 450, loss is 5.412527618408203 and perplexity is 224.19755797536567
At time: 106.51554274559021 and batch: 500, loss is 5.407839012145996 and perplexity is 223.14884432492315
At time: 107.06400036811829 and batch: 550, loss is 5.434074211120605 and perplexity is 229.0806697381099
At time: 107.61509084701538 and batch: 600, loss is 5.470547676086426 and perplexity is 237.5902796497075
At time: 108.16885757446289 and batch: 650, loss is 5.438085212707519 and perplexity is 230.00135787440277
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.555589264514399 and perplexity of 258.6793501801025
Finished 14 epochs...
Completing Train Step...
At time: 109.21159267425537 and batch: 50, loss is 5.444645042419434 and perplexity is 231.51508709036221
At time: 109.77844738960266 and batch: 100, loss is 5.421700086593628 and perplexity is 226.263463183628
At time: 110.32920598983765 and batch: 150, loss is 5.417002620697022 and perplexity is 225.2030907631802
At time: 110.88591456413269 and batch: 200, loss is 5.42092456817627 and perplexity is 226.08805972385176
At time: 111.44313979148865 and batch: 250, loss is 5.409609432220459 and perplexity is 223.54426144242797
At time: 112.00219988822937 and batch: 300, loss is 5.434386434555054 and perplexity is 229.15220525853314
At time: 112.5573000907898 and batch: 350, loss is 5.389609470367431 and perplexity is 219.1177968498867
At time: 113.10895371437073 and batch: 400, loss is 5.440598573684692 and perplexity is 230.5801613783603
At time: 113.6614363193512 and batch: 450, loss is 5.404818925857544 and perplexity is 222.4759321977027
At time: 114.22843170166016 and batch: 500, loss is 5.403512630462647 and perplexity is 222.1855026466891
At time: 114.79624009132385 and batch: 550, loss is 5.430949592590332 and perplexity is 228.36599715357917
At time: 115.35085368156433 and batch: 600, loss is 5.467389678955078 and perplexity is 236.84115371948948
At time: 115.91115403175354 and batch: 650, loss is 5.434252653121948 and perplexity is 229.12155099864546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.553168801700368 and perplexity of 258.05398357585557
Finished 15 epochs...
Completing Train Step...
At time: 117.00267791748047 and batch: 50, loss is 5.43805965423584 and perplexity is 229.99547946633317
At time: 117.5661268234253 and batch: 100, loss is 5.417204551696777 and perplexity is 225.24857084021014
At time: 118.11561322212219 and batch: 150, loss is 5.410645608901977 and perplexity is 223.77601284038386
At time: 118.66676759719849 and batch: 200, loss is 5.414674148559571 and perplexity is 224.67932166858867
At time: 119.23251485824585 and batch: 250, loss is 5.405346803665161 and perplexity is 222.59340330750683
At time: 119.78481721878052 and batch: 300, loss is 5.429512195587158 and perplexity is 228.03798035530164
At time: 120.33696126937866 and batch: 350, loss is 5.384916429519653 and perplexity is 218.09187730292297
At time: 120.88950395584106 and batch: 400, loss is 5.4386513137817385 and perplexity is 230.13159875143938
At time: 121.4408130645752 and batch: 450, loss is 5.400857858657837 and perplexity is 221.59643310739807
At time: 121.99188113212585 and batch: 500, loss is 5.399155855178833 and perplexity is 221.21959598732815
At time: 122.54235672950745 and batch: 550, loss is 5.4243785095214845 and perplexity is 226.87030475756038
At time: 123.09211897850037 and batch: 600, loss is 5.460947027206421 and perplexity is 235.32017348036018
At time: 123.65373587608337 and batch: 650, loss is 5.431218891143799 and perplexity is 228.42750406776338
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.550756716260723 and perplexity of 257.43228541539844
Finished 16 epochs...
Completing Train Step...
At time: 124.71681761741638 and batch: 50, loss is 5.4336183071136475 and perplexity is 228.9762547462748
At time: 125.27947115898132 and batch: 100, loss is 5.412250881195068 and perplexity is 224.13552275210333
At time: 125.83331751823425 and batch: 150, loss is 5.407027521133423 and perplexity is 222.96783449714474
At time: 126.3930881023407 and batch: 200, loss is 5.412636470794678 and perplexity is 224.2219637428821
At time: 126.9500777721405 and batch: 250, loss is 5.404070453643799 and perplexity is 222.30947744537494
At time: 127.50072574615479 and batch: 300, loss is 5.422710723876953 and perplexity is 226.49224906567366
At time: 128.05104684829712 and batch: 350, loss is 5.381098537445069 and perplexity is 217.26081351844098
At time: 128.6017234325409 and batch: 400, loss is 5.436329460144043 and perplexity is 229.59788670213146
At time: 129.1665380001068 and batch: 450, loss is 5.397520732879639 and perplexity is 220.8581704609058
At time: 129.72624945640564 and batch: 500, loss is 5.395477390289306 and perplexity is 220.40734230982108
At time: 130.27798438072205 and batch: 550, loss is 5.4175797462463375 and perplexity is 225.33309873249632
At time: 130.83066654205322 and batch: 600, loss is 5.455437965393067 and perplexity is 234.0273445058476
At time: 131.38170790672302 and batch: 650, loss is 5.42841591835022 and perplexity is 227.7881244889538
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.549966251148897 and perplexity of 257.22887458029555
Finished 17 epochs...
Completing Train Step...
At time: 132.4402973651886 and batch: 50, loss is 5.428389272689819 and perplexity is 227.7820550048084
At time: 132.99622678756714 and batch: 100, loss is 5.408770275115967 and perplexity is 223.35675137347684
At time: 133.5744936466217 and batch: 150, loss is 5.404178304672241 and perplexity is 222.33345504413097
At time: 134.13432455062866 and batch: 200, loss is 5.407256727218628 and perplexity is 223.01894593891944
At time: 134.6940951347351 and batch: 250, loss is 5.396130857467651 and perplexity is 220.5514183432393
At time: 135.2663791179657 and batch: 300, loss is 5.42007285118103 and perplexity is 225.89557866229563
At time: 135.8296868801117 and batch: 350, loss is 5.377823858261109 and perplexity is 216.55051768515122
At time: 136.40127110481262 and batch: 400, loss is 5.431048851013184 and perplexity is 228.38866552728447
At time: 136.95885705947876 and batch: 450, loss is 5.391137466430664 and perplexity is 219.45286390623704
At time: 137.50977540016174 and batch: 500, loss is 5.391003227233886 and perplexity is 219.42340670725523
At time: 138.06596064567566 and batch: 550, loss is 5.4156945705413815 and perplexity is 224.90870640198565
At time: 138.61934781074524 and batch: 600, loss is 5.457346477508545 and perplexity is 234.47441501222767
At time: 139.18460059165955 and batch: 650, loss is 5.425531711578369 and perplexity is 227.1320829722296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.5469994638480395 and perplexity of 256.46686214510987
Finished 18 epochs...
Completing Train Step...
At time: 140.25276684761047 and batch: 50, loss is 5.428345365524292 and perplexity is 227.77205395997552
At time: 140.83330655097961 and batch: 100, loss is 5.4077308654785154 and perplexity is 223.12471282595135
At time: 141.3987500667572 and batch: 150, loss is 5.402700710296631 and perplexity is 222.00517897061434
At time: 141.96153044700623 and batch: 200, loss is 5.40295524597168 and perplexity is 222.06169440099953
At time: 142.52469849586487 and batch: 250, loss is 5.3898586177825925 and perplexity is 219.17239628395916
At time: 143.09014773368835 and batch: 300, loss is 5.4179025077819825 and perplexity is 225.4058393277765
At time: 143.64815020561218 and batch: 350, loss is 5.373432283401489 and perplexity is 215.60160501142326
At time: 144.20350646972656 and batch: 400, loss is 5.430004005432129 and perplexity is 228.15015926215193
At time: 144.75463342666626 and batch: 450, loss is 5.389584341049194 and perplexity is 219.11229063822225
At time: 145.30442357063293 and batch: 500, loss is 5.386158418655396 and perplexity is 218.3629133221823
At time: 145.86064314842224 and batch: 550, loss is 5.4102547740936275 and perplexity is 223.6885704741654
At time: 146.41921973228455 and batch: 600, loss is 5.451168899536133 and perplexity is 233.03039589357851
At time: 146.969872713089 and batch: 650, loss is 5.41788254737854 and perplexity is 225.40134018118783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.549308028875613 and perplexity of 257.05961651653985
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 148.02094554901123 and batch: 50, loss is 5.398032751083374 and perplexity is 220.97128281994767
At time: 148.58506894111633 and batch: 100, loss is 5.342331399917603 and perplexity is 208.9994039229835
At time: 149.152170419693 and batch: 150, loss is 5.323637742996215 and perplexity is 205.1287319932692
At time: 149.71970224380493 and batch: 200, loss is 5.3163465309143065 and perplexity is 203.63853418015367
At time: 150.2840540409088 and batch: 250, loss is 5.302017869949341 and perplexity is 200.7414717434896
At time: 150.85270738601685 and batch: 300, loss is 5.326155652999878 and perplexity is 205.6458784705178
At time: 151.41989135742188 and batch: 350, loss is 5.273272056579589 and perplexity is 195.05314435362592
At time: 151.98032522201538 and batch: 400, loss is 5.306574401855468 and perplexity is 201.65824372860567
At time: 152.53275275230408 and batch: 450, loss is 5.262287254333496 and perplexity is 192.9222492929977
At time: 153.08347129821777 and batch: 500, loss is 5.259082098007202 and perplexity is 192.3048932150722
At time: 153.63596296310425 and batch: 550, loss is 5.278027725219727 and perplexity is 195.98296167450079
At time: 154.19399094581604 and batch: 600, loss is 5.32873833656311 and perplexity is 206.177683146682
At time: 154.7484085559845 and batch: 650, loss is 5.307787208557129 and perplexity is 201.90296456758497
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.461019478592218 and perplexity of 235.33722337067135
Finished 20 epochs...
Completing Train Step...
At time: 155.77724719047546 and batch: 50, loss is 5.333842992782593 and perplexity is 207.23284015493778
At time: 156.3417615890503 and batch: 100, loss is 5.303477554321289 and perplexity is 201.03470489445388
At time: 156.89361548423767 and batch: 150, loss is 5.295057983398437 and perplexity is 199.34918457466117
At time: 157.44628286361694 and batch: 200, loss is 5.293376436233521 and perplexity is 199.01425120056552
At time: 157.99900197982788 and batch: 250, loss is 5.28292160987854 and perplexity is 196.9444304236699
At time: 158.55080270767212 and batch: 300, loss is 5.308200922012329 and perplexity is 201.98651182179128
At time: 159.11405730247498 and batch: 350, loss is 5.256536016464233 and perplexity is 191.81589205840825
At time: 159.6796519756317 and batch: 400, loss is 5.297315435409546 and perplexity is 199.79971412538407
At time: 160.24734568595886 and batch: 450, loss is 5.2615112209320065 and perplexity is 192.7725932602065
At time: 160.81328678131104 and batch: 500, loss is 5.263919095993042 and perplexity is 193.23732486325002
At time: 161.38156175613403 and batch: 550, loss is 5.2836985206604 and perplexity is 197.09749812736897
At time: 161.9582462310791 and batch: 600, loss is 5.33168176651001 and perplexity is 206.78544672960953
At time: 162.50986456871033 and batch: 650, loss is 5.303856229782104 and perplexity is 201.11084621948538
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.455822514552696 and perplexity of 234.11735683048025
Finished 21 epochs...
Completing Train Step...
At time: 163.5808675289154 and batch: 50, loss is 5.322025623321533 and perplexity is 204.7983063429889
At time: 164.14187574386597 and batch: 100, loss is 5.294244108200073 and perplexity is 199.18700522334976
At time: 164.70117688179016 and batch: 150, loss is 5.286365785598755 and perplexity is 197.6239111029095
At time: 165.2575192451477 and batch: 200, loss is 5.2855313110351565 and perplexity is 197.45906776428748
At time: 165.81185007095337 and batch: 250, loss is 5.276268253326416 and perplexity is 195.6384383403078
At time: 166.36489868164062 and batch: 300, loss is 5.301850080490112 and perplexity is 200.7077922661106
At time: 166.91934967041016 and batch: 350, loss is 5.252144918441773 and perplexity is 190.97545624256037
At time: 167.4844410419464 and batch: 400, loss is 5.295700941085816 and perplexity is 199.47739887912547
At time: 168.03894305229187 and batch: 450, loss is 5.262182035446167 and perplexity is 192.90195129647108
At time: 168.5893063545227 and batch: 500, loss is 5.265901002883911 and perplexity is 193.62068301356499
At time: 169.14023303985596 and batch: 550, loss is 5.284677400588989 and perplexity is 197.2905273730745
At time: 169.69010996818542 and batch: 600, loss is 5.330863771438598 and perplexity is 206.61636641620447
At time: 170.24034714698792 and batch: 650, loss is 5.29946870803833 and perplexity is 200.23040090766955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.453550450942096 and perplexity of 233.58603113487573
Finished 22 epochs...
Completing Train Step...
At time: 171.2919545173645 and batch: 50, loss is 5.315723724365235 and perplexity is 203.51174625369663
At time: 171.8703534603119 and batch: 100, loss is 5.2884206199646 and perplexity is 198.0304130109691
At time: 172.42471718788147 and batch: 150, loss is 5.281168546676636 and perplexity is 196.59947684093382
At time: 172.97675347328186 and batch: 200, loss is 5.281108131408692 and perplexity is 196.5875995896501
At time: 173.53859663009644 and batch: 250, loss is 5.272895450592041 and perplexity is 194.9797000022315
At time: 174.1050248146057 and batch: 300, loss is 5.298481521606445 and perplexity is 200.03283370651744
At time: 174.66376757621765 and batch: 350, loss is 5.249118213653564 and perplexity is 190.39830379037093
At time: 175.21752262115479 and batch: 400, loss is 5.294112253189087 and perplexity is 199.16074315001876
At time: 175.77284860610962 and batch: 450, loss is 5.2620061588287355 and perplexity is 192.86802733708416
At time: 176.3518614768982 and batch: 500, loss is 5.267183561325073 and perplexity is 193.8691721718184
At time: 176.91004133224487 and batch: 550, loss is 5.284652862548828 and perplexity is 197.28568630958586
At time: 177.4609408378601 and batch: 600, loss is 5.329128971099854 and perplexity is 206.25823900335035
At time: 178.01124715805054 and batch: 650, loss is 5.296142835617065 and perplexity is 199.56556632972044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.452840169270833 and perplexity of 233.4201781664321
Finished 23 epochs...
Completing Train Step...
At time: 179.04335832595825 and batch: 50, loss is 5.311306419372559 and perplexity is 202.6147554005393
At time: 179.59896230697632 and batch: 100, loss is 5.284099187850952 and perplexity is 197.17648445076588
At time: 180.14959359169006 and batch: 150, loss is 5.277271451950074 and perplexity is 195.83480103130765
At time: 180.71367740631104 and batch: 200, loss is 5.278115243911743 and perplexity is 196.00011459755373
At time: 181.27941298484802 and batch: 250, loss is 5.270527448654175 and perplexity is 194.51853393135286
At time: 181.83273077011108 and batch: 300, loss is 5.296551237106323 and perplexity is 199.64708584942767
At time: 182.38618898391724 and batch: 350, loss is 5.248120756149292 and perplexity is 190.2084842576511
At time: 182.93663716316223 and batch: 400, loss is 5.293459815979004 and perplexity is 199.03084564998903
At time: 183.4932577610016 and batch: 450, loss is 5.2618818187713625 and perplexity is 192.8440476063511
At time: 184.05461931228638 and batch: 500, loss is 5.26571063041687 and perplexity is 193.58382647481594
At time: 184.61484217643738 and batch: 550, loss is 5.2840149784088135 and perplexity is 197.15988102809973
At time: 185.17449283599854 and batch: 600, loss is 5.328160190582276 and perplexity is 206.05851679892479
At time: 185.73175430297852 and batch: 650, loss is 5.294541683197021 and perplexity is 199.24628711578828
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.450451420802696 and perplexity of 232.86326150513491
Finished 24 epochs...
Completing Train Step...
At time: 186.7777442932129 and batch: 50, loss is 5.308648653030396 and perplexity is 202.07696769680442
At time: 187.34623551368713 and batch: 100, loss is 5.281327524185181 and perplexity is 196.6307342204876
At time: 187.90007734298706 and batch: 150, loss is 5.2754498481750485 and perplexity is 195.47839233473542
At time: 188.45321536064148 and batch: 200, loss is 5.2757698440551755 and perplexity is 195.54095462423996
At time: 189.01225209236145 and batch: 250, loss is 5.268576517105102 and perplexity is 194.1394115278051
At time: 189.57711958885193 and batch: 300, loss is 5.294770402908325 and perplexity is 199.29186388100936
At time: 190.13847947120667 and batch: 350, loss is 5.246028804779053 and perplexity is 189.81099326924655
At time: 190.7179412841797 and batch: 400, loss is 5.291783466339111 and perplexity is 198.69747986043402
At time: 191.27162909507751 and batch: 450, loss is 5.261103134155274 and perplexity is 192.69394136347157
At time: 191.82868480682373 and batch: 500, loss is 5.265034894943238 and perplexity is 193.4530592031634
At time: 192.39266896247864 and batch: 550, loss is 5.282260179519653 and perplexity is 196.81420846949862
At time: 192.9551990032196 and batch: 600, loss is 5.326892948150634 and perplexity is 205.79755608820207
At time: 193.52004742622375 and batch: 650, loss is 5.2922453022003175 and perplexity is 198.7892666757549
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.449468276079963 and perplexity of 232.63443572133002
Finished 25 epochs...
Completing Train Step...
At time: 194.59434986114502 and batch: 50, loss is 5.305608940124512 and perplexity is 201.4636443657884
At time: 195.15331172943115 and batch: 100, loss is 5.279245023727417 and perplexity is 196.22167670552773
At time: 195.71480131149292 and batch: 150, loss is 5.2737032985687256 and perplexity is 195.13727759917447
At time: 196.2836790084839 and batch: 200, loss is 5.274456453323364 and perplexity is 195.2843015265407
At time: 196.8399064540863 and batch: 250, loss is 5.266818561553955 and perplexity is 193.7984228808719
At time: 197.39240980148315 and batch: 300, loss is 5.293425779342652 and perplexity is 199.02407142475937
At time: 197.9543559551239 and batch: 350, loss is 5.245247898101806 and perplexity is 189.6628264569438
At time: 198.51972341537476 and batch: 400, loss is 5.290432291030884 and perplexity is 198.42918602865814
At time: 199.08705759048462 and batch: 450, loss is 5.25970718383789 and perplexity is 192.4251378566886
At time: 199.64103055000305 and batch: 500, loss is 5.264077806472779 and perplexity is 193.26799608563985
At time: 200.20618987083435 and batch: 550, loss is 5.2816150188446045 and perplexity is 196.687272633311
At time: 200.76794385910034 and batch: 600, loss is 5.325867347717285 and perplexity is 205.58659822322605
At time: 201.3421516418457 and batch: 650, loss is 5.290741958618164 and perplexity is 198.4906426310094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.447408040364583 and perplexity of 232.15564732606
Finished 26 epochs...
Completing Train Step...
At time: 202.40431094169617 and batch: 50, loss is 5.303361139297485 and perplexity is 201.01130279670247
At time: 202.98927068710327 and batch: 100, loss is 5.277836389541626 and perplexity is 195.94546672880747
At time: 203.55650854110718 and batch: 150, loss is 5.272492532730102 and perplexity is 194.90115502303533
At time: 204.11576795578003 and batch: 200, loss is 5.272638626098633 and perplexity is 194.92963086931866
At time: 204.67588782310486 and batch: 250, loss is 5.265426931381225 and perplexity is 193.52891471950286
At time: 205.2419891357422 and batch: 300, loss is 5.292369184494018 and perplexity is 198.81389467152857
At time: 205.80980920791626 and batch: 350, loss is 5.243605613708496 and perplexity is 189.3516017867221
At time: 206.36774063110352 and batch: 400, loss is 5.289247035980225 and perplexity is 198.1941361582596
At time: 206.91860270500183 and batch: 450, loss is 5.2589208602905275 and perplexity is 192.27388891278355
At time: 207.468364238739 and batch: 500, loss is 5.263327388763428 and perplexity is 193.1230187623053
At time: 208.0211536884308 and batch: 550, loss is 5.280162181854248 and perplexity is 196.40172556499485
At time: 208.58101749420166 and batch: 600, loss is 5.325371599197387 and perplexity is 205.48470423043102
At time: 209.14253163337708 and batch: 650, loss is 5.28925784111023 and perplexity is 198.1962776832367
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.447157317516851 and perplexity of 232.09744789731343
Finished 27 epochs...
Completing Train Step...
At time: 210.20695424079895 and batch: 50, loss is 5.3014568424224855 and perplexity is 200.62888183803125
At time: 210.7622196674347 and batch: 100, loss is 5.2760750102996825 and perplexity is 195.6006362289524
At time: 211.31673097610474 and batch: 150, loss is 5.270717926025391 and perplexity is 194.55558883929763
At time: 211.8716995716095 and batch: 200, loss is 5.271318836212158 and perplexity is 194.67253440789636
At time: 212.4283847808838 and batch: 250, loss is 5.264649400711059 and perplexity is 193.37849853692006
At time: 212.98718404769897 and batch: 300, loss is 5.291308097839355 and perplexity is 198.60304778432294
At time: 213.5533742904663 and batch: 350, loss is 5.242982177734375 and perplexity is 189.2335899766371
At time: 214.119943857193 and batch: 400, loss is 5.288007526397705 and perplexity is 197.94862481555654
At time: 214.68605017662048 and batch: 450, loss is 5.258674945831299 and perplexity is 192.22661179666923
At time: 215.23833870887756 and batch: 500, loss is 5.262714500427246 and perplexity is 193.0046921808524
At time: 215.79216194152832 and batch: 550, loss is 5.279308319091797 and perplexity is 196.23409702112411
At time: 216.34624671936035 and batch: 600, loss is 5.324412870407104 and perplexity is 205.28779453510214
At time: 216.9066038131714 and batch: 650, loss is 5.288121995925903 and perplexity is 197.97128519818347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.446517645143995 and perplexity of 231.9490290468632
Finished 28 epochs...
Completing Train Step...
At time: 218.00180459022522 and batch: 50, loss is 5.300439767837524 and perplexity is 200.42493103547474
At time: 218.5681231021881 and batch: 100, loss is 5.274912557601929 and perplexity is 195.37339184769723
At time: 219.11999106407166 and batch: 150, loss is 5.269831447601319 and perplexity is 194.38319593009564
At time: 219.68551445007324 and batch: 200, loss is 5.270417222976684 and perplexity is 194.49709417579334
At time: 220.23865580558777 and batch: 250, loss is 5.2639071559906006 and perplexity is 193.23501762289354
At time: 220.79093194007874 and batch: 300, loss is 5.290687980651856 and perplexity is 198.479928798947
At time: 221.34314274787903 and batch: 350, loss is 5.242048463821411 and perplexity is 189.05698240418036
At time: 221.90431571006775 and batch: 400, loss is 5.287524070739746 and perplexity is 197.85294856238067
At time: 222.46040201187134 and batch: 450, loss is 5.258263120651245 and perplexity is 192.1474643362321
At time: 223.01496171951294 and batch: 500, loss is 5.262202863693237 and perplexity is 192.90596914781526
At time: 223.57789278030396 and batch: 550, loss is 5.27868655204773 and perplexity is 196.11212305030128
At time: 224.1382737159729 and batch: 600, loss is 5.323445520401001 and perplexity is 205.08930540551432
At time: 224.69829893112183 and batch: 650, loss is 5.2870711326599125 and perplexity is 197.76335371975915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.445622462852328 and perplexity of 231.7414852920977
Finished 29 epochs...
Completing Train Step...
At time: 225.78762197494507 and batch: 50, loss is 5.2991587352752685 and perplexity is 200.16834455543759
At time: 226.35888361930847 and batch: 100, loss is 5.27384617805481 and perplexity is 195.1651607050282
At time: 226.91638565063477 and batch: 150, loss is 5.268956642150879 and perplexity is 194.21322280836867
At time: 227.4781620502472 and batch: 200, loss is 5.269386568069458 and perplexity is 194.2967380579835
At time: 228.04372572898865 and batch: 250, loss is 5.262573614120483 and perplexity is 192.97750237796353
At time: 228.59993147850037 and batch: 300, loss is 5.289722127914429 and perplexity is 198.28831896472923
At time: 229.1538119316101 and batch: 350, loss is 5.241286172866821 and perplexity is 188.91292089196142
At time: 229.71406292915344 and batch: 400, loss is 5.2868674373626705 and perplexity is 197.72307435713728
At time: 230.27424311637878 and batch: 450, loss is 5.25750111579895 and perplexity is 192.00110280723155
At time: 230.82347345352173 and batch: 500, loss is 5.261498765945435 and perplexity is 192.77019229509804
At time: 231.37524509429932 and batch: 550, loss is 5.278046293258667 and perplexity is 195.9866007275497
At time: 231.92713236808777 and batch: 600, loss is 5.323254156112671 and perplexity is 205.05006239151672
At time: 232.477689743042 and batch: 650, loss is 5.286560487747193 and perplexity is 197.66239264908216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.445292753331802 and perplexity of 231.66509051283103
Finished 30 epochs...
Completing Train Step...
At time: 233.50938653945923 and batch: 50, loss is 5.298377895355225 and perplexity is 200.0121061278187
At time: 234.0914022922516 and batch: 100, loss is 5.272821769714356 and perplexity is 194.96533425605142
At time: 234.65695095062256 and batch: 150, loss is 5.268490266799927 and perplexity is 194.12266766640622
At time: 235.2167227268219 and batch: 200, loss is 5.26837911605835 and perplexity is 194.1010919870368
At time: 235.77064275741577 and batch: 250, loss is 5.261695871353149 and perplexity is 192.8081920873048
At time: 236.33578443527222 and batch: 300, loss is 5.288707408905029 and perplexity is 198.08721408786036
At time: 236.90239453315735 and batch: 350, loss is 5.23999282836914 and perplexity is 188.66874933818912
At time: 237.46729063987732 and batch: 400, loss is 5.285735874176026 and perplexity is 197.499464743108
At time: 238.03140830993652 and batch: 450, loss is 5.257272491455078 and perplexity is 191.9572116985586
At time: 238.5837061405182 and batch: 500, loss is 5.261400842666626 and perplexity is 192.7513165300155
At time: 239.13549828529358 and batch: 550, loss is 5.277431402206421 and perplexity is 195.86612736319495
At time: 239.69483280181885 and batch: 600, loss is 5.322757139205932 and perplexity is 204.94817436591345
At time: 240.2531819343567 and batch: 650, loss is 5.285244674682617 and perplexity is 197.40247692821055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.445989272173713 and perplexity of 231.8265058212624
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 241.30317902565002 and batch: 50, loss is 5.2959224891662595 and perplexity is 199.52159760983028
At time: 241.86605978012085 and batch: 100, loss is 5.26275297164917 and perplexity is 193.01211745002627
At time: 242.43252515792847 and batch: 150, loss is 5.2564694118499755 and perplexity is 191.80311666036417
At time: 242.99586725234985 and batch: 200, loss is 5.24838080406189 and perplexity is 190.25795400891477
At time: 243.55846285820007 and batch: 250, loss is 5.241331577301025 and perplexity is 188.92149857097928
At time: 244.11106157302856 and batch: 300, loss is 5.26718204498291 and perplexity is 193.86887820004145
At time: 244.66427183151245 and batch: 350, loss is 5.215101251602173 and perplexity is 184.0304534146055
At time: 245.23302745819092 and batch: 400, loss is 5.261351699829102 and perplexity is 192.74184441612977
At time: 245.79718852043152 and batch: 450, loss is 5.229836082458496 and perplexity is 186.76218741237147
At time: 246.3545744419098 and batch: 500, loss is 5.226355924606323 and perplexity is 186.11335519349075
At time: 246.91572046279907 and batch: 550, loss is 5.247833938598633 and perplexity is 190.1539369490184
At time: 247.46538853645325 and batch: 600, loss is 5.291176414489746 and perplexity is 198.576896791611
At time: 248.04618620872498 and batch: 650, loss is 5.264217662811279 and perplexity is 193.29502773015105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.429619284237132 and perplexity of 228.06240194237483
Finished 32 epochs...
Completing Train Step...
At time: 249.11072993278503 and batch: 50, loss is 5.286798076629639 and perplexity is 197.70936061536565
At time: 249.6749587059021 and batch: 100, loss is 5.255727396011353 and perplexity is 191.6608484990471
At time: 250.23673057556152 and batch: 150, loss is 5.251204614639282 and perplexity is 190.79596569591988
At time: 250.79768013954163 and batch: 200, loss is 5.243830022811889 and perplexity is 189.39409877808265
At time: 251.36822533607483 and batch: 250, loss is 5.238399448394776 and perplexity is 188.36836770579632
At time: 251.92629623413086 and batch: 300, loss is 5.265288581848145 and perplexity is 193.50214193655756
At time: 252.48997402191162 and batch: 350, loss is 5.213984069824218 and perplexity is 183.82497274646275
At time: 253.0491383075714 and batch: 400, loss is 5.260348434448242 and perplexity is 192.54857016507816
At time: 253.6102738380432 and batch: 450, loss is 5.229573593139649 and perplexity is 186.71317076646568
At time: 254.16559171676636 and batch: 500, loss is 5.227532711029053 and perplexity is 186.33249978086886
At time: 254.7191789150238 and batch: 550, loss is 5.249451036453247 and perplexity is 190.46168323337008
At time: 255.2713131904602 and batch: 600, loss is 5.2925165748596195 and perplexity is 198.84320008376548
At time: 255.83063983917236 and batch: 650, loss is 5.264075183868409 and perplexity is 193.26748922081336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.428457222732843 and perplexity of 227.79753333111645
Finished 33 epochs...
Completing Train Step...
At time: 256.90596771240234 and batch: 50, loss is 5.284362239837646 and perplexity is 197.22835893927459
At time: 257.4609851837158 and batch: 100, loss is 5.253566541671753 and perplexity is 191.247144460895
At time: 258.0114161968231 and batch: 150, loss is 5.249499893188476 and perplexity is 190.47098879671697
At time: 258.5759947299957 and batch: 200, loss is 5.242205057144165 and perplexity is 189.08658978334336
At time: 259.14512062072754 and batch: 250, loss is 5.23747109413147 and perplexity is 188.19357627530306
At time: 259.70394921302795 and batch: 300, loss is 5.264694375991821 and perplexity is 193.38719598476868
At time: 260.25438928604126 and batch: 350, loss is 5.2139060688018795 and perplexity is 183.81063476985278
At time: 260.80467414855957 and batch: 400, loss is 5.260307455062867 and perplexity is 192.54067980468997
At time: 261.362868309021 and batch: 450, loss is 5.229605007171631 and perplexity is 186.7190362721127
At time: 261.9274170398712 and batch: 500, loss is 5.228107128143311 and perplexity is 186.4395631042443
At time: 262.49669909477234 and batch: 550, loss is 5.250138902664185 and perplexity is 190.59274045949584
At time: 263.04732942581177 and batch: 600, loss is 5.292945384979248 and perplexity is 198.92848434425088
At time: 263.5985631942749 and batch: 650, loss is 5.263836030960083 and perplexity is 193.2212742651222
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.427980310776654 and perplexity of 227.68891986546936
Finished 34 epochs...
Completing Train Step...
At time: 264.6183304786682 and batch: 50, loss is 5.282988500595093 and perplexity is 196.95760461835286
At time: 265.1830585002899 and batch: 100, loss is 5.252287998199463 and perplexity is 191.00278291946478
At time: 265.73484468460083 and batch: 150, loss is 5.248494548797607 and perplexity is 190.27959608042417
At time: 266.28532576560974 and batch: 200, loss is 5.241284294128418 and perplexity is 188.9125659743355
At time: 266.83596420288086 and batch: 250, loss is 5.237083072662354 and perplexity is 188.1205672928008
At time: 267.38621163368225 and batch: 300, loss is 5.2645389842987065 and perplexity is 193.3571475556565
At time: 267.9482560157776 and batch: 350, loss is 5.214031438827515 and perplexity is 183.83368055844136
At time: 268.51373386383057 and batch: 400, loss is 5.260362272262573 and perplexity is 192.55123463487695
At time: 269.0724923610687 and batch: 450, loss is 5.229612512588501 and perplexity is 186.72043768157658
At time: 269.62426948547363 and batch: 500, loss is 5.228463182449341 and perplexity is 186.50595753281155
At time: 270.17320108413696 and batch: 550, loss is 5.250534763336182 and perplexity is 190.6682035652643
At time: 270.7239508628845 and batch: 600, loss is 5.293153228759766 and perplexity is 198.9698346895469
At time: 271.2748992443085 and batch: 650, loss is 5.263658771514892 and perplexity is 193.18702700466181
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.4277607038909315 and perplexity of 227.63892330086006
Finished 35 epochs...
Completing Train Step...
At time: 272.32237362861633 and batch: 50, loss is 5.28205472946167 and perplexity is 196.77377713240915
At time: 272.87361311912537 and batch: 100, loss is 5.251389837265014 and perplexity is 190.83130869872602
At time: 273.4287996292114 and batch: 150, loss is 5.247821531295776 and perplexity is 190.15157766616957
At time: 273.9887025356293 and batch: 200, loss is 5.240641355514526 and perplexity is 188.79114582805417
At time: 274.53934144973755 and batch: 250, loss is 5.23685772895813 and perplexity is 188.07818028332863
At time: 275.08990597724915 and batch: 300, loss is 5.264469976425171 and perplexity is 193.34380485045196
At time: 275.64528465270996 and batch: 350, loss is 5.214143991470337 and perplexity is 183.85437268948303
At time: 276.21175503730774 and batch: 400, loss is 5.260408973693847 and perplexity is 192.56022726311068
At time: 276.780148267746 and batch: 450, loss is 5.229561367034912 and perplexity is 186.71088800563885
At time: 277.33294916152954 and batch: 500, loss is 5.228653469085693 and perplexity is 186.5414505009419
At time: 277.8929579257965 and batch: 550, loss is 5.250728607177734 and perplexity is 190.7051670047571
At time: 278.4510819911957 and batch: 600, loss is 5.293241004943848 and perplexity is 198.9873002689031
At time: 279.0131514072418 and batch: 650, loss is 5.263521890640259 and perplexity is 193.16058520516725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.427594353170956 and perplexity of 227.6010585515759
Finished 36 epochs...
Completing Train Step...
At time: 280.0831229686737 and batch: 50, loss is 5.281361131668091 and perplexity is 196.63734259557236
At time: 280.6578621864319 and batch: 100, loss is 5.2507015991210935 and perplexity is 190.70001649835783
At time: 281.2088797092438 and batch: 150, loss is 5.247305212020874 and perplexity is 190.05342408294135
At time: 281.76300048828125 and batch: 200, loss is 5.240135936737061 and perplexity is 188.6957513470451
At time: 282.3273549079895 and batch: 250, loss is 5.236693859100342 and perplexity is 188.04736246379736
At time: 282.8958797454834 and batch: 300, loss is 5.264421215057373 and perplexity is 193.33437737192247
At time: 283.4633455276489 and batch: 350, loss is 5.214229001998901 and perplexity is 183.8700029112415
At time: 284.0315296649933 and batch: 400, loss is 5.260421476364136 and perplexity is 192.56263479519308
At time: 284.5916659832001 and batch: 450, loss is 5.229465560913086 and perplexity is 186.69300081642112
At time: 285.14454913139343 and batch: 500, loss is 5.228752307891845 and perplexity is 186.55988894640922
At time: 285.70801973342896 and batch: 550, loss is 5.250816974639893 and perplexity is 190.7220198809978
At time: 286.26547622680664 and batch: 600, loss is 5.293272676467896 and perplexity is 198.99360259977047
At time: 286.82241678237915 and batch: 650, loss is 5.2634090805053715 and perplexity is 193.13879596254208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.427456724877451 and perplexity of 227.56973636174675
Finished 37 epochs...
Completing Train Step...
At time: 287.8944957256317 and batch: 50, loss is 5.280810346603394 and perplexity is 196.52906750499758
At time: 288.44942831993103 and batch: 100, loss is 5.250151596069336 and perplexity is 190.59515974572378
At time: 289.0038712024689 and batch: 150, loss is 5.2468906497955325 and perplexity is 189.97465144172872
At time: 289.5683591365814 and batch: 200, loss is 5.239726152420044 and perplexity is 188.6184426284874
At time: 290.1207118034363 and batch: 250, loss is 5.236557178497314 and perplexity is 188.02166179332963
At time: 290.67276072502136 and batch: 300, loss is 5.2643751049041745 and perplexity is 193.3254628996887
At time: 291.2460367679596 and batch: 350, loss is 5.21428955078125 and perplexity is 183.881136353083
At time: 291.8057632446289 and batch: 400, loss is 5.260405101776123 and perplexity is 192.55948168719718
At time: 292.3710596561432 and batch: 450, loss is 5.229338989257813 and perplexity is 186.66937226966354
At time: 292.92852878570557 and batch: 500, loss is 5.228793563842774 and perplexity is 186.56758581080243
At time: 293.488333940506 and batch: 550, loss is 5.250830459594726 and perplexity is 190.72459177616258
At time: 294.0477204322815 and batch: 600, loss is 5.293246650695801 and perplexity is 198.98842370501356
At time: 294.6048176288605 and batch: 650, loss is 5.263310832977295 and perplexity is 193.11982148537606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.427349614162071 and perplexity of 227.5453625098599
Finished 38 epochs...
Completing Train Step...
At time: 295.6336295604706 and batch: 50, loss is 5.280362548828125 and perplexity is 196.44108192713654
At time: 296.2019944190979 and batch: 100, loss is 5.2496780586242675 and perplexity is 190.504927166674
At time: 296.7624349594116 and batch: 150, loss is 5.2465344905853275 and perplexity is 189.9070022675656
At time: 297.3236315250397 and batch: 200, loss is 5.23936240196228 and perplexity is 188.54984506059336
At time: 297.88898038864136 and batch: 250, loss is 5.236427736282349 and perplexity is 187.99732542807612
At time: 298.4552447795868 and batch: 300, loss is 5.26431718826294 and perplexity is 193.3142664624455
At time: 299.02156829833984 and batch: 350, loss is 5.214316987991333 and perplexity is 183.88618160766507
At time: 299.58364272117615 and batch: 400, loss is 5.260367994308472 and perplexity is 192.55233642503157
At time: 300.14093923568726 and batch: 450, loss is 5.229193344116211 and perplexity is 186.64218676227313
At time: 300.70651721954346 and batch: 500, loss is 5.228795509338379 and perplexity is 186.56794877757383
At time: 301.26605463027954 and batch: 550, loss is 5.250801029205323 and perplexity is 190.71897875975478
At time: 301.8332645893097 and batch: 600, loss is 5.293238725662231 and perplexity is 198.98684672132453
At time: 302.3987135887146 and batch: 650, loss is 5.263217506408691 and perplexity is 193.10179911610345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.4272317325367645 and perplexity of 227.51854067362817
Finished 39 epochs...
Completing Train Step...
At time: 303.44331455230713 and batch: 50, loss is 5.279979114532471 and perplexity is 196.36577411797117
At time: 304.0082550048828 and batch: 100, loss is 5.249285936355591 and perplexity is 190.4302405865319
At time: 304.56773352622986 and batch: 150, loss is 5.246239147186279 and perplexity is 189.8509227697752
At time: 305.12510895729065 and batch: 200, loss is 5.239061098098755 and perplexity is 188.49304282160634
At time: 305.7148745059967 and batch: 250, loss is 5.236301727294922 and perplexity is 187.973637567933
At time: 306.26854825019836 and batch: 300, loss is 5.26426134109497 and perplexity is 193.30347070959442
At time: 306.82264137268066 and batch: 350, loss is 5.214340009689331 and perplexity is 183.89041502853408
At time: 307.3840651512146 and batch: 400, loss is 5.260320138931275 and perplexity is 192.5431219808239
At time: 307.9415650367737 and batch: 450, loss is 5.229029455184937 and perplexity is 186.61160068018262
At time: 308.4992890357971 and batch: 500, loss is 5.228773784637451 and perplexity is 186.56389568871012
At time: 309.06092071533203 and batch: 550, loss is 5.250709266662597 and perplexity is 190.7014787042549
At time: 309.61645340919495 and batch: 600, loss is 5.293122386932373 and perplexity is 198.96369819087982
At time: 310.1671361923218 and batch: 650, loss is 5.263124141693115 and perplexity is 193.08377106315695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.427161123238358 and perplexity of 227.50247631624995
Finished 40 epochs...
Completing Train Step...
At time: 311.2039153575897 and batch: 50, loss is 5.279653205871582 and perplexity is 196.30178723898936
At time: 311.7724840641022 and batch: 100, loss is 5.248898983001709 and perplexity is 190.3565672212536
At time: 312.3279411792755 and batch: 150, loss is 5.24595687866211 and perplexity is 189.79734139251667
At time: 312.8864541053772 and batch: 200, loss is 5.2387599277496335 and perplexity is 188.43628285373143
At time: 313.4412486553192 and batch: 250, loss is 5.236179046630859 and perplexity is 187.95057825174484
At time: 313.9982397556305 and batch: 300, loss is 5.264189081192017 and perplexity is 193.2895031242146
At time: 314.55957770347595 and batch: 350, loss is 5.214337978363037 and perplexity is 183.89004148747824
At time: 315.1158666610718 and batch: 400, loss is 5.26026725769043 and perplexity is 192.53294033082892
At time: 315.6820545196533 and batch: 450, loss is 5.228873243331909 and perplexity is 186.58245201298692
At time: 316.2378692626953 and batch: 500, loss is 5.228730840682983 and perplexity is 186.55588406929485
At time: 316.7941007614136 and batch: 550, loss is 5.250643405914307 and perplexity is 190.68891937575523
At time: 317.3602273464203 and batch: 600, loss is 5.29310248374939 and perplexity is 198.95973821939597
At time: 317.92616033554077 and batch: 650, loss is 5.263041400909424 and perplexity is 193.06779582153226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.427054012522978 and perplexity of 227.47810966824886
Finished 41 epochs...
Completing Train Step...
At time: 318.9854984283447 and batch: 50, loss is 5.279379396438599 and perplexity is 196.24804531579048
At time: 319.548579454422 and batch: 100, loss is 5.248592767715454 and perplexity is 190.298286054279
At time: 320.1314973831177 and batch: 150, loss is 5.2457248878479 and perplexity is 189.75331525977884
At time: 320.693133354187 and batch: 200, loss is 5.238509149551391 and perplexity is 188.38903306708974
At time: 321.25502729415894 and batch: 250, loss is 5.236073045730591 and perplexity is 187.93065637713124
At time: 321.8136465549469 and batch: 300, loss is 5.264134492874145 and perplexity is 193.27895206336183
At time: 322.3645055294037 and batch: 350, loss is 5.214344654083252 and perplexity is 183.89126909004307
At time: 322.914338350296 and batch: 400, loss is 5.26020694732666 and perplexity is 192.52132894930676
At time: 323.46464252471924 and batch: 450, loss is 5.2287126159667965 and perplexity is 186.55248417223586
At time: 324.0143709182739 and batch: 500, loss is 5.228672933578491 and perplexity is 186.54508147099907
At time: 324.5671982765198 and batch: 550, loss is 5.250544033050537 and perplexity is 190.66997101323983
At time: 325.1175787448883 and batch: 600, loss is 5.292984247207642 and perplexity is 198.93621529866354
At time: 325.67302942276 and batch: 650, loss is 5.262954368591308 and perplexity is 193.0509934148952
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.4269738290824145 and perplexity of 227.45987042201526
Finished 42 epochs...
Completing Train Step...
At time: 326.6987590789795 and batch: 50, loss is 5.279121208190918 and perplexity is 196.1973829173589
At time: 327.2653670310974 and batch: 100, loss is 5.248298196792603 and perplexity is 190.24223796801246
At time: 327.81793189048767 and batch: 150, loss is 5.245502958297729 and perplexity is 189.71120806446808
At time: 328.3724205493927 and batch: 200, loss is 5.238274602890015 and perplexity is 188.3448522297804
At time: 328.9269959926605 and batch: 250, loss is 5.23595817565918 and perplexity is 187.9090700090504
At time: 329.49160647392273 and batch: 300, loss is 5.264072074890136 and perplexity is 193.2668883573227
At time: 330.04766297340393 and batch: 350, loss is 5.214337730407715 and perplexity is 183.88999589096943
At time: 330.6097376346588 and batch: 400, loss is 5.260137100219726 and perplexity is 192.5078823610647
At time: 331.1735701560974 and batch: 450, loss is 5.228549785614014 and perplexity is 186.52211023839195
At time: 331.7392704486847 and batch: 500, loss is 5.228601064682007 and perplexity is 186.53167516360313
At time: 332.30669832229614 and batch: 550, loss is 5.250454006195068 and perplexity is 190.65280636796822
At time: 332.87336897850037 and batch: 600, loss is 5.292876386642456 and perplexity is 198.91475908320618
At time: 333.4304316043854 and batch: 650, loss is 5.262875566482544 and perplexity is 193.03578118890061
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.4269062117034315 and perplexity of 227.44449070172755
Finished 43 epochs...
Completing Train Step...
At time: 334.47189688682556 and batch: 50, loss is 5.278901672363281 and perplexity is 196.15431529013654
At time: 335.023699760437 and batch: 100, loss is 5.248013210296631 and perplexity is 190.18802922397504
At time: 335.5766944885254 and batch: 150, loss is 5.245288372039795 and perplexity is 189.67050301376977
At time: 336.1295027732849 and batch: 200, loss is 5.238025426864624 and perplexity is 188.29792705465604
At time: 336.6832013130188 and batch: 250, loss is 5.235842704772949 and perplexity is 187.88737323490267
At time: 337.2441899776459 and batch: 300, loss is 5.264000368118286 and perplexity is 193.25303030951696
At time: 337.8078145980835 and batch: 350, loss is 5.214314546585083 and perplexity is 183.88573266733994
At time: 338.3650782108307 and batch: 400, loss is 5.260057373046875 and perplexity is 192.4925348636669
At time: 338.9251937866211 and batch: 450, loss is 5.228385524749756 and perplexity is 186.4914744715586
At time: 339.4846181869507 and batch: 500, loss is 5.228512868881226 and perplexity is 186.515224578588
At time: 340.0383131504059 and batch: 550, loss is 5.250367317199707 and perplexity is 190.63627958407696
At time: 340.6020152568817 and batch: 600, loss is 5.2928055381774906 and perplexity is 198.9006667770811
At time: 341.1691052913666 and batch: 650, loss is 5.262798843383789 and perplexity is 193.02097145372886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.426809273514093 and perplexity of 227.4224437132387
Finished 44 epochs...
Completing Train Step...
At time: 342.2538366317749 and batch: 50, loss is 5.278684139251709 and perplexity is 196.111649872322
At time: 342.836549282074 and batch: 100, loss is 5.247746210098267 and perplexity is 190.13725576100975
At time: 343.3944790363312 and batch: 150, loss is 5.24508038520813 and perplexity is 189.63105814893606
At time: 343.9461236000061 and batch: 200, loss is 5.237786521911621 and perplexity is 188.25294712042103
At time: 344.5109488964081 and batch: 250, loss is 5.235703611373902 and perplexity is 187.86124115896308
At time: 345.0674419403076 and batch: 300, loss is 5.2639125633239745 and perplexity is 193.23606251187837
At time: 345.61643505096436 and batch: 350, loss is 5.214267063140869 and perplexity is 183.8770013467093
At time: 346.16634941101074 and batch: 400, loss is 5.259884700775147 and perplexity is 192.45929960986726
At time: 346.72791171073914 and batch: 450, loss is 5.228196439743042 and perplexity is 186.45621506347373
At time: 347.2905707359314 and batch: 500, loss is 5.228390893936157 and perplexity is 186.49247578173538
At time: 347.8579592704773 and batch: 550, loss is 5.250216732025146 and perplexity is 190.60757474795378
At time: 348.42468667030334 and batch: 600, loss is 5.292621955871582 and perplexity is 198.8641554855436
At time: 348.9896981716156 and batch: 650, loss is 5.262682676315308 and perplexity is 192.99855007565782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.4267249013863355 and perplexity of 227.40325640721116
Finished 45 epochs...
Completing Train Step...
At time: 350.0269594192505 and batch: 50, loss is 5.278479890823364 and perplexity is 196.0715984664131
At time: 350.586674451828 and batch: 100, loss is 5.247437887191772 and perplexity is 190.07864112626217
At time: 351.15598130226135 and batch: 150, loss is 5.244840402603149 and perplexity is 189.58555545376223
At time: 351.71829652786255 and batch: 200, loss is 5.237549638748169 and perplexity is 188.20835844813922
At time: 352.2686536312103 and batch: 250, loss is 5.235562295913696 and perplexity is 187.8346953369256
At time: 352.82312202453613 and batch: 300, loss is 5.2638360786437985 and perplexity is 193.2212834786307
At time: 353.3830318450928 and batch: 350, loss is 5.214234781265259 and perplexity is 183.87106554803418
At time: 353.9476246833801 and batch: 400, loss is 5.2597527599334715 and perplexity is 192.43390804301768
At time: 354.51094365119934 and batch: 450, loss is 5.2280318069458005 and perplexity is 186.42552078193688
At time: 355.06239581108093 and batch: 500, loss is 5.228293733596802 and perplexity is 186.47435698972944
At time: 355.6140127182007 and batch: 550, loss is 5.2501105785369875 and perplexity is 190.58734216292353
At time: 356.1643490791321 and batch: 600, loss is 5.292535171508789 and perplexity is 198.84689793538098
At time: 356.7272369861603 and batch: 650, loss is 5.262597455978393 and perplexity is 192.98210337500302
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.42662078259038 and perplexity of 227.3795806865228
Finished 46 epochs...
Completing Train Step...
At time: 357.7724771499634 and batch: 50, loss is 5.2782742786407475 and perplexity is 196.03128790141687
At time: 358.33734035491943 and batch: 100, loss is 5.24720929145813 and perplexity is 190.0351949258415
At time: 358.88980531692505 and batch: 150, loss is 5.244657688140869 and perplexity is 189.55091859536526
At time: 359.45093274116516 and batch: 200, loss is 5.237352876663208 and perplexity is 188.1713298221583
At time: 360.01347732543945 and batch: 250, loss is 5.235436763763428 and perplexity is 187.8111175236427
At time: 360.5656387805939 and batch: 300, loss is 5.263756980895996 and perplexity is 193.20600071470417
At time: 361.1171329021454 and batch: 350, loss is 5.2142006778717045 and perplexity is 183.86479502764612
At time: 361.6800615787506 and batch: 400, loss is 5.259646968841553 and perplexity is 192.41355132656224
At time: 362.2461440563202 and batch: 450, loss is 5.227860193252564 and perplexity is 186.39353035487812
At time: 362.81849455833435 and batch: 500, loss is 5.2281898021697994 and perplexity is 186.45497745079717
At time: 363.3859655857086 and batch: 550, loss is 5.24997709274292 and perplexity is 190.56190315812643
At time: 363.95230984687805 and batch: 600, loss is 5.292381343841552 and perplexity is 198.81631213346586
At time: 364.5125079154968 and batch: 650, loss is 5.262513055801391 and perplexity is 192.965816338644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.42655735389859 and perplexity of 227.36515875456718
Finished 47 epochs...
Completing Train Step...
At time: 365.62257957458496 and batch: 50, loss is 5.278100643157959 and perplexity is 195.99725286903066
At time: 366.1748445034027 and batch: 100, loss is 5.2469704055786135 and perplexity is 189.989803623049
At time: 366.7264976501465 and batch: 150, loss is 5.244466495513916 and perplexity is 189.5146813215579
At time: 367.2766478061676 and batch: 200, loss is 5.23712233543396 and perplexity is 188.12795357267086
At time: 367.8285675048828 and batch: 250, loss is 5.235323305130005 and perplexity is 187.78980993969444
At time: 368.37969040870667 and batch: 300, loss is 5.2636657810211185 and perplexity is 193.18838115507637
At time: 368.9406969547272 and batch: 350, loss is 5.214146900177002 and perplexity is 183.8549074687
At time: 369.49772334098816 and batch: 400, loss is 5.259538202285767 and perplexity is 192.39262430539856
At time: 370.05075788497925 and batch: 450, loss is 5.227681674957275 and perplexity is 186.36025866947702
At time: 370.60618829727173 and batch: 500, loss is 5.228078927993774 and perplexity is 186.43430555481743
At time: 371.1583979129791 and batch: 550, loss is 5.249859714508057 and perplexity is 190.53953665099777
At time: 371.70853090286255 and batch: 600, loss is 5.292273979187012 and perplexity is 198.79496743465032
At time: 372.261025428772 and batch: 650, loss is 5.262419815063477 and perplexity is 192.9478249023167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.426465202780331 and perplexity of 227.34420776627798
Finished 48 epochs...
Completing Train Step...
At time: 373.29671812057495 and batch: 50, loss is 5.277933979034424 and perplexity is 195.96458988061576
At time: 373.869989156723 and batch: 100, loss is 5.246751728057862 and perplexity is 189.9482616661362
At time: 374.4241044521332 and batch: 150, loss is 5.244294786453247 and perplexity is 189.4821427273109
At time: 374.97851490974426 and batch: 200, loss is 5.236933488845825 and perplexity is 188.09242960490144
At time: 375.53483629226685 and batch: 250, loss is 5.235201168060303 and perplexity is 187.76687524320525
At time: 376.08933687210083 and batch: 300, loss is 5.263569221496582 and perplexity is 193.16972787743643
At time: 376.64252638816833 and batch: 350, loss is 5.214095792770386 and perplexity is 183.84551136129303
At time: 377.2195918560028 and batch: 400, loss is 5.259412441253662 and perplexity is 192.36843033175802
At time: 377.7805919647217 and batch: 450, loss is 5.22748477935791 and perplexity is 186.32356876680714
At time: 378.33764600753784 and batch: 500, loss is 5.227965288162231 and perplexity is 186.4131203955021
At time: 378.8899130821228 and batch: 550, loss is 5.249718828201294 and perplexity is 190.51269413030286
At time: 379.4539740085602 and batch: 600, loss is 5.292098579406738 and perplexity is 198.7601018988357
At time: 380.01991271972656 and batch: 650, loss is 5.262327890396119 and perplexity is 192.93008905288863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.4264059627757355 and perplexity of 227.33074029327574
Finished 49 epochs...
Completing Train Step...
At time: 381.0887258052826 and batch: 50, loss is 5.277780160903931 and perplexity is 195.9344492919014
At time: 381.6394863128662 and batch: 100, loss is 5.24650689125061 and perplexity is 189.90176103297247
At time: 382.2013695240021 and batch: 150, loss is 5.244121618270874 and perplexity is 189.44933328991996
At time: 382.75929069519043 and batch: 200, loss is 5.236708946228028 and perplexity is 188.05019957976776
At time: 383.32668709754944 and batch: 250, loss is 5.235067825317383 and perplexity is 187.7418395622312
At time: 383.8826713562012 and batch: 300, loss is 5.263474674224853 and perplexity is 193.1514650700479
At time: 384.4373371601105 and batch: 350, loss is 5.214041690826416 and perplexity is 183.8355652307933
At time: 384.9873216152191 and batch: 400, loss is 5.259292678833008 and perplexity is 192.34539320240273
At time: 385.5376980304718 and batch: 450, loss is 5.22730902671814 and perplexity is 186.2908247852498
At time: 386.0900709629059 and batch: 500, loss is 5.227867202758789 and perplexity is 186.39483688606856
At time: 386.644650220871 and batch: 550, loss is 5.249601736068725 and perplexity is 190.49038789863334
At time: 387.1984713077545 and batch: 600, loss is 5.291983423233032 and perplexity is 198.73721476383832
At time: 387.7554523944855 and batch: 650, loss is 5.262239055633545 and perplexity is 192.91295091547676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.426279105392157 and perplexity of 227.30190353947148
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa09ad998d0>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'seq_len': 20, 'anneal': 6.600065067976981, 'data': 'ptb', 'dropout': 0.13744330649552117, 'batch_size': 80, 'num_layers': 1, 'lr': 17.472087439493734}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7947936058044434 and batch: 50, loss is 6.663869390487671 and perplexity is 783.5770443176363
At time: 1.3694407939910889 and batch: 100, loss is 5.703611011505127 and perplexity is 299.94856546436756
At time: 1.9287514686584473 and batch: 150, loss is 5.525260066986084 and perplexity is 250.951593759687
At time: 2.4866034984588623 and batch: 200, loss is 5.5188420867919925 and perplexity is 249.3461487690869
At time: 3.0464370250701904 and batch: 250, loss is 5.512371797561645 and perplexity is 247.73801522298396
At time: 3.6056902408599854 and batch: 300, loss is 5.555562343597412 and perplexity is 258.67238638852626
At time: 4.165212392807007 and batch: 350, loss is 5.524704875946045 and perplexity is 250.81230635248633
At time: 4.7231316566467285 and batch: 400, loss is 5.5953742790222165 and perplexity is 269.1783790612375
At time: 5.282806873321533 and batch: 450, loss is 5.566583204269409 and perplexity is 261.538945679737
At time: 5.857271432876587 and batch: 500, loss is 5.597262601852417 and perplexity is 269.6871549552021
At time: 6.419259071350098 and batch: 550, loss is 5.624161148071289 and perplexity is 277.03979165966047
At time: 6.9788548946380615 and batch: 600, loss is 5.666478395462036 and perplexity is 289.01494381038464
At time: 7.539437770843506 and batch: 650, loss is 5.657474927902221 and perplexity is 286.42448618910703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.703428979013481 and perplexity of 299.89397004885086
Finished 1 epochs...
Completing Train Step...
At time: 8.598809003829956 and batch: 50, loss is 5.550136404037476 and perplexity is 257.2726465401884
At time: 9.157312154769897 and batch: 100, loss is 5.5200718402862545 and perplexity is 249.65297168642664
At time: 9.706341981887817 and batch: 150, loss is 5.495345573425293 and perplexity is 243.55567798940697
At time: 10.259613037109375 and batch: 200, loss is 5.470326328277588 and perplexity is 237.5376953818269
At time: 10.82666826248169 and batch: 250, loss is 5.475061521530152 and perplexity is 238.66514952419968
At time: 11.384333848953247 and batch: 300, loss is 5.525629386901856 and perplexity is 251.04429229781172
At time: 11.934865236282349 and batch: 350, loss is 5.465651073455811 and perplexity is 236.42973813551222
At time: 12.486407279968262 and batch: 400, loss is 5.497563753128052 and perplexity is 244.09652788017672
At time: 13.04059362411499 and batch: 450, loss is 5.483571882247925 and perplexity is 240.70494341711935
At time: 13.60223913192749 and batch: 500, loss is 5.506328134536743 and perplexity is 246.24528546195052
At time: 14.159149885177612 and batch: 550, loss is 5.502499589920044 and perplexity is 245.30432680037808
At time: 14.711252689361572 and batch: 600, loss is 5.538714513778687 and perplexity is 254.35082473373342
At time: 15.263115406036377 and batch: 650, loss is 5.5359142303466795 and perplexity is 253.63956666019638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.649191463694853 and perplexity of 284.0616987525854
Finished 2 epochs...
Completing Train Step...
At time: 16.306036710739136 and batch: 50, loss is 5.514506187438965 and perplexity is 248.26734943674316
At time: 16.876978158950806 and batch: 100, loss is 5.4775705242156985 and perplexity is 239.2647128637913
At time: 17.429200172424316 and batch: 150, loss is 5.461061849594116 and perplexity is 235.34719505586318
At time: 17.978795051574707 and batch: 200, loss is 5.43465145111084 and perplexity is 229.2129424345446
At time: 18.52890920639038 and batch: 250, loss is 5.435210886001587 and perplexity is 229.3412080267319
At time: 19.078933715820312 and batch: 300, loss is 5.500306272506714 and perplexity is 244.76688615325952
At time: 19.627617120742798 and batch: 350, loss is 5.451577844619751 and perplexity is 233.12571201651446
At time: 20.200460195541382 and batch: 400, loss is 5.482477684020996 and perplexity is 240.44170853665256
At time: 20.749852180480957 and batch: 450, loss is 5.48860384941101 and perplexity is 241.9192153143608
At time: 21.298702478408813 and batch: 500, loss is 5.468297176361084 and perplexity is 237.0561840070821
At time: 21.84604787826538 and batch: 550, loss is 5.480725307464599 and perplexity is 240.02073308491768
At time: 22.395861864089966 and batch: 600, loss is 5.507579116821289 and perplexity is 246.55352671369033
At time: 22.944661855697632 and batch: 650, loss is 5.532590093612671 and perplexity is 252.79783385202464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.694258147594976 and perplexity of 297.15626570627074
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 23.983021020889282 and batch: 50, loss is 5.398918771743775 and perplexity is 221.16715470233493
At time: 24.533186674118042 and batch: 100, loss is 5.231211051940918 and perplexity is 187.01915634226805
At time: 25.084603309631348 and batch: 150, loss is 5.151493158340454 and perplexity is 172.689150150371
At time: 25.634677410125732 and batch: 200, loss is 5.111686630249023 and perplexity is 165.95001528118115
At time: 26.18747854232788 and batch: 250, loss is 5.082587728500366 and perplexity is 161.19063429865113
At time: 26.738824129104614 and batch: 300, loss is 5.1245387840270995 and perplexity is 168.0965949357535
At time: 27.28985047340393 and batch: 350, loss is 5.042516469955444 and perplexity is 154.85922368828062
At time: 27.84111738204956 and batch: 400, loss is 5.058485994338989 and perplexity is 157.3521039139415
At time: 28.392558336257935 and batch: 450, loss is 5.018251953125 and perplexity is 151.14686093421068
At time: 28.94728446006775 and batch: 500, loss is 4.985895547866821 and perplexity is 146.33456597146971
At time: 29.514033555984497 and batch: 550, loss is 4.985342435836792 and perplexity is 146.25364894277564
At time: 30.079509496688843 and batch: 600, loss is 5.040402030944824 and perplexity is 154.53212923705954
At time: 30.633654832839966 and batch: 650, loss is 5.034907989501953 and perplexity is 153.68545128636185
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.2086833879059435 and perplexity of 182.8531529665184
Finished 4 epochs...
Completing Train Step...
At time: 31.664711952209473 and batch: 50, loss is 5.0937516975402835 and perplexity is 163.0002439677465
At time: 32.23112416267395 and batch: 100, loss is 5.058245010375977 and perplexity is 157.31418914896065
At time: 32.79672050476074 and batch: 150, loss is 5.021173791885376 and perplexity is 151.58913350091626
At time: 33.34884238243103 and batch: 200, loss is 5.007475671768188 and perplexity is 149.52680460382697
At time: 33.90161848068237 and batch: 250, loss is 4.991661491394043 and perplexity is 147.18076002411533
At time: 34.465882301330566 and batch: 300, loss is 5.034932651519775 and perplexity is 153.68924152643785
At time: 35.013744831085205 and batch: 350, loss is 4.9631537914276125 and perplexity is 143.04421688170103
At time: 35.56539797782898 and batch: 400, loss is 4.9930886459350585 and perplexity is 147.39095967214757
At time: 36.11700654029846 and batch: 450, loss is 4.96557056427002 and perplexity is 143.39034034274496
At time: 36.66819763183594 and batch: 500, loss is 4.951301679611206 and perplexity is 141.35884814683573
At time: 37.22837972640991 and batch: 550, loss is 4.971180191040039 and perplexity is 144.1969669563796
At time: 37.78705358505249 and batch: 600, loss is 5.039489393234253 and perplexity is 154.3911617243414
At time: 38.34525775909424 and batch: 650, loss is 5.010710697174073 and perplexity is 150.01131088810507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.191395479090073 and perplexity of 179.71917237119754
Finished 5 epochs...
Completing Train Step...
At time: 39.402745485305786 and batch: 50, loss is 5.050049591064453 and perplexity is 156.0302019983416
At time: 39.954176902770996 and batch: 100, loss is 5.0235958003997805 and perplexity is 151.95672865254363
At time: 40.50595545768738 and batch: 150, loss is 4.988466491699219 and perplexity is 146.71126795364728
At time: 41.05571985244751 and batch: 200, loss is 4.975190200805664 and perplexity is 144.77635910975087
At time: 41.6057186126709 and batch: 250, loss is 4.961775493621826 and perplexity is 142.84719515993626
At time: 42.15551519393921 and batch: 300, loss is 5.000960655212403 and perplexity is 148.55580148156017
At time: 42.70677208900452 and batch: 350, loss is 4.937284107208252 and perplexity is 139.39116355820582
At time: 43.25718569755554 and batch: 400, loss is 4.976046371459961 and perplexity is 144.9003654575795
At time: 43.81181240081787 and batch: 450, loss is 4.952887840270996 and perplexity is 141.5832439074968
At time: 44.37739634513855 and batch: 500, loss is 4.942024765014648 and perplexity is 140.05353817105245
At time: 44.93342638015747 and batch: 550, loss is 4.965091609954834 and perplexity is 143.32167936453038
At time: 45.484984397888184 and batch: 600, loss is 5.032656469345093 and perplexity is 153.3398146449196
At time: 46.036755084991455 and batch: 650, loss is 4.992911958694458 and perplexity is 147.36491987071176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.183340035232843 and perplexity of 178.27727004680378
Finished 6 epochs...
Completing Train Step...
At time: 47.08382606506348 and batch: 50, loss is 5.024706001281738 and perplexity is 152.12552482820723
At time: 47.65686750411987 and batch: 100, loss is 5.00217038154602 and perplexity is 148.735622091558
At time: 48.20904731750488 and batch: 150, loss is 4.968991527557373 and perplexity is 143.88171343826758
At time: 48.77585816383362 and batch: 200, loss is 4.959265213012696 and perplexity is 142.4890583159119
At time: 49.32601046562195 and batch: 250, loss is 4.947242126464844 and perplexity is 140.78615761040305
At time: 49.87849760055542 and batch: 300, loss is 4.983915224075317 and perplexity is 146.0450628984801
At time: 50.436973094940186 and batch: 350, loss is 4.9220726108551025 and perplexity is 137.2868607552811
At time: 50.99118399620056 and batch: 400, loss is 4.9640192127227785 and perplexity is 143.16806397536726
At time: 51.54197120666504 and batch: 450, loss is 4.9406606960296635 and perplexity is 139.86262572187368
At time: 52.09364318847656 and batch: 500, loss is 4.9319918251037596 and perplexity is 138.6554148100012
At time: 52.65523147583008 and batch: 550, loss is 4.954426126480103 and perplexity is 141.80120706091577
At time: 53.21908187866211 and batch: 600, loss is 5.02224271774292 and perplexity is 151.7512576793643
At time: 53.78318166732788 and batch: 650, loss is 4.975842924118042 and perplexity is 144.87088886195326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.1743205949371935 and perplexity of 176.67653855625082
Finished 7 epochs...
Completing Train Step...
At time: 54.842469215393066 and batch: 50, loss is 5.004245300292968 and perplexity is 149.04455681856538
At time: 55.39637351036072 and batch: 100, loss is 4.986934337615967 and perplexity is 146.48665579954724
At time: 55.94811272621155 and batch: 150, loss is 4.955014305114746 and perplexity is 141.88463603444032
At time: 56.510475397109985 and batch: 200, loss is 4.946700210571289 and perplexity is 140.70988402280278
At time: 57.06507468223572 and batch: 250, loss is 4.936100978851318 and perplexity is 139.2263434407976
At time: 57.614044189453125 and batch: 300, loss is 4.973759117126465 and perplexity is 144.5693202054282
At time: 58.163835287094116 and batch: 350, loss is 4.910724878311157 and perplexity is 135.73777212248797
At time: 58.715248823165894 and batch: 400, loss is 4.953395290374756 and perplexity is 141.65510857163179
At time: 59.26707696914673 and batch: 450, loss is 4.932575645446778 and perplexity is 138.73638829652128
At time: 59.81788730621338 and batch: 500, loss is 4.922069416046143 and perplexity is 137.28642215068896
At time: 60.36971139907837 and batch: 550, loss is 4.945793800354004 and perplexity is 140.58240093096805
At time: 60.921239376068115 and batch: 600, loss is 5.012902698516846 and perplexity is 150.34049653888937
At time: 61.48576331138611 and batch: 650, loss is 4.964496631622314 and perplexity is 143.2364314335788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.1687275007659315 and perplexity of 175.69112835314127
Finished 8 epochs...
Completing Train Step...
At time: 62.52126455307007 and batch: 50, loss is 4.988250494003296 and perplexity is 146.6795820799644
At time: 63.08980441093445 and batch: 100, loss is 4.97420301437378 and perplexity is 144.6335083741352
At time: 63.643701791763306 and batch: 150, loss is 4.943500204086304 and perplexity is 140.2603311511006
At time: 64.19757580757141 and batch: 200, loss is 4.936644372940063 and perplexity is 139.30201877172254
At time: 64.75207185745239 and batch: 250, loss is 4.926074914932251 and perplexity is 137.83742554674353
At time: 65.3044068813324 and batch: 300, loss is 4.963539142608642 and perplexity is 143.09934976169384
At time: 65.8559103012085 and batch: 350, loss is 4.902192449569702 and perplexity is 134.584526245039
At time: 66.40812015533447 and batch: 400, loss is 4.945181903839111 and perplexity is 140.4964053626606
At time: 66.9577283859253 and batch: 450, loss is 4.922648038864136 and perplexity is 137.36588219362596
At time: 67.51674222946167 and batch: 500, loss is 4.913658285140992 and perplexity is 136.136530805081
At time: 68.07562255859375 and batch: 550, loss is 4.936937198638916 and perplexity is 139.34281595566148
At time: 68.63925004005432 and batch: 600, loss is 5.003434219360352 and perplexity is 148.9237186318186
At time: 69.19008898735046 and batch: 650, loss is 4.953368215560913 and perplexity is 141.6512733378567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.164146273743873 and perplexity of 174.88808826735684
Finished 9 epochs...
Completing Train Step...
At time: 70.25379014015198 and batch: 50, loss is 4.9775048828125 and perplexity is 145.11185848058238
At time: 70.80595898628235 and batch: 100, loss is 4.963623600006104 and perplexity is 143.11143607073504
At time: 71.35711193084717 and batch: 150, loss is 4.932049951553345 and perplexity is 138.66347459122082
At time: 71.90876245498657 and batch: 200, loss is 4.926093683242798 and perplexity is 137.84001254662797
At time: 72.45998001098633 and batch: 250, loss is 4.916928625106811 and perplexity is 136.5824723353354
At time: 73.01986765861511 and batch: 300, loss is 4.951772003173828 and perplexity is 141.42534818094492
At time: 73.57109928131104 and batch: 350, loss is 4.89143798828125 and perplexity is 133.1448972600166
At time: 74.12407565116882 and batch: 400, loss is 4.934551057815551 and perplexity is 139.0107207446072
At time: 74.67796468734741 and batch: 450, loss is 4.911678352355957 and perplexity is 135.86725628519022
At time: 75.2310562133789 and batch: 500, loss is 4.901141233444214 and perplexity is 134.4431231564341
At time: 75.78475499153137 and batch: 550, loss is 4.925561399459839 and perplexity is 137.76666206667377
At time: 76.33716177940369 and batch: 600, loss is 4.99109453201294 and perplexity is 147.09733816213995
At time: 76.90398025512695 and batch: 650, loss is 4.935521421432495 and perplexity is 139.14567715819
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.150812485638787 and perplexity of 172.57164535567944
Finished 10 epochs...
Completing Train Step...
At time: 77.92649006843567 and batch: 50, loss is 4.95765233039856 and perplexity is 142.2594254262776
At time: 78.48909401893616 and batch: 100, loss is 4.941475992202759 and perplexity is 139.97670168191166
At time: 79.04711508750916 and batch: 150, loss is 4.912189245223999 and perplexity is 135.9366876319065
At time: 79.60597014427185 and batch: 200, loss is 4.902799091339111 and perplexity is 134.66619560969474
At time: 80.15618801116943 and batch: 250, loss is 4.894030838012696 and perplexity is 133.4905699160879
At time: 80.7053952217102 and batch: 300, loss is 4.929137191772461 and perplexity is 138.26016885086668
At time: 81.25592398643494 and batch: 350, loss is 4.869027061462402 and perplexity is 130.1941843156583
At time: 81.80700325965881 and batch: 400, loss is 4.911896638870239 and perplexity is 135.89691751217492
At time: 82.35893201828003 and batch: 450, loss is 4.887083435058594 and perplexity is 132.56637124391034
At time: 82.91009378433228 and batch: 500, loss is 4.879477319717407 and perplexity is 131.5618811104986
At time: 83.46232891082764 and batch: 550, loss is 4.903096599578857 and perplexity is 134.70626587282507
At time: 84.02572870254517 and batch: 600, loss is 4.970679798126221 and perplexity is 144.1248298658702
At time: 84.57789397239685 and batch: 650, loss is 4.917382497787475 and perplexity is 136.64447745833724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.140627991919424 and perplexity of 170.8230101143122
Finished 11 epochs...
Completing Train Step...
At time: 85.61356902122498 and batch: 50, loss is 4.9389036273956295 and perplexity is 139.61709326100612
At time: 86.1658444404602 and batch: 100, loss is 4.924186401367187 and perplexity is 137.57736334161842
At time: 86.71881103515625 and batch: 150, loss is 4.897783517837524 and perplexity is 133.9924584090365
At time: 87.27056336402893 and batch: 200, loss is 4.891541414260864 and perplexity is 133.15866861359186
At time: 87.83308339118958 and batch: 250, loss is 4.881249580383301 and perplexity is 131.7952497915063
At time: 88.38915252685547 and batch: 300, loss is 4.919230337142944 and perplexity is 136.8972079322377
At time: 88.94075226783752 and batch: 350, loss is 4.856647872924805 and perplexity is 128.59242068355778
At time: 89.49531936645508 and batch: 400, loss is 4.900664911270142 and perplexity is 134.37910016471895
At time: 90.04776167869568 and batch: 450, loss is 4.876727180480957 and perplexity is 131.20056468220625
At time: 90.59873366355896 and batch: 500, loss is 4.86970814704895 and perplexity is 130.28288790198613
At time: 91.16341042518616 and batch: 550, loss is 4.894194383621215 and perplexity is 133.51240349792104
At time: 91.71475720405579 and batch: 600, loss is 4.96394679069519 and perplexity is 143.15769582933387
At time: 92.26542973518372 and batch: 650, loss is 4.907244606018066 and perplexity is 135.26618881020823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.138431324678309 and perplexity of 170.44818064249978
Finished 12 epochs...
Completing Train Step...
At time: 93.2908616065979 and batch: 50, loss is 4.929566850662232 and perplexity is 138.3195863252236
At time: 93.85489106178284 and batch: 100, loss is 4.916855974197388 and perplexity is 136.5725498549519
At time: 94.40618586540222 and batch: 150, loss is 4.891295642852783 and perplexity is 133.12594604140952
At time: 94.95846962928772 and batch: 200, loss is 4.884406185150146 and perplexity is 132.21193261074154
At time: 95.51041960716248 and batch: 250, loss is 4.8732451152801515 and perplexity is 130.744510228069
At time: 96.06063508987427 and batch: 300, loss is 4.91018723487854 and perplexity is 135.66481321544433
At time: 96.61037302017212 and batch: 350, loss is 4.847104949951172 and perplexity is 127.37110982416142
At time: 97.1623682975769 and batch: 400, loss is 4.891603145599365 and perplexity is 133.16688893016138
At time: 97.71415829658508 and batch: 450, loss is 4.867308225631714 and perplexity is 129.97059409919484
At time: 98.266366481781 and batch: 500, loss is 4.860919132232666 and perplexity is 129.14284692711547
At time: 98.82615494728088 and batch: 550, loss is 4.885410223007202 and perplexity is 132.34474505944323
At time: 99.38865733146667 and batch: 600, loss is 4.953445024490357 and perplexity is 141.66215383837053
At time: 99.95562934875488 and batch: 650, loss is 4.8971222877502445 and perplexity is 133.90388785005416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.13337916953891 and perplexity of 169.58922161482315
Finished 13 epochs...
Completing Train Step...
At time: 101.01397633552551 and batch: 50, loss is 4.919979133605957 and perplexity is 136.99975446578304
At time: 101.56719017028809 and batch: 100, loss is 4.905669956207276 and perplexity is 135.05335954135208
At time: 102.12048292160034 and batch: 150, loss is 4.881916837692261 and perplexity is 131.88322048149487
At time: 102.67411375045776 and batch: 200, loss is 4.875312547683716 and perplexity is 131.01509527687202
At time: 103.22774767875671 and batch: 250, loss is 4.864752426147461 and perplexity is 129.63883945159483
At time: 103.78523278236389 and batch: 300, loss is 4.904090547561646 and perplexity is 134.8402234564649
At time: 104.33998823165894 and batch: 350, loss is 4.838923444747925 and perplexity is 126.3332737461599
At time: 104.89204549789429 and batch: 400, loss is 4.883937854766845 and perplexity is 132.1500282426447
At time: 105.45889496803284 and batch: 450, loss is 4.859134035110474 and perplexity is 128.91252004180703
At time: 106.0117917060852 and batch: 500, loss is 4.852659826278686 and perplexity is 128.0806093536058
At time: 106.57237243652344 and batch: 550, loss is 4.876199083328247 and perplexity is 131.13129632938342
At time: 107.13643527030945 and batch: 600, loss is 4.943820657730103 and perplexity is 140.3052852877722
At time: 107.6970751285553 and batch: 650, loss is 4.889052200317383 and perplexity is 132.82762039484086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.12916774375766 and perplexity of 168.87651101198836
Finished 14 epochs...
Completing Train Step...
At time: 108.74521517753601 and batch: 50, loss is 4.9104463005065915 and perplexity is 135.69996385844954
At time: 109.31310844421387 and batch: 100, loss is 4.894529628753662 and perplexity is 133.55717038483616
At time: 109.86545515060425 and batch: 150, loss is 4.872661428451538 and perplexity is 130.6682186468969
At time: 110.42042565345764 and batch: 200, loss is 4.864572525024414 and perplexity is 129.61551937650378
At time: 110.97364974021912 and batch: 250, loss is 4.8532147884368895 and perplexity is 128.1517089719707
At time: 111.526437997818 and batch: 300, loss is 4.891093358993531 and perplexity is 133.0990195347683
At time: 112.07668232917786 and batch: 350, loss is 4.826896343231201 and perplexity is 124.82295124468156
At time: 112.63714957237244 and batch: 400, loss is 4.872439241409301 and perplexity is 130.63918908699685
At time: 113.1946222782135 and batch: 450, loss is 4.844892072677612 and perplexity is 127.08956481708701
At time: 113.7487542629242 and batch: 500, loss is 4.836097297668457 and perplexity is 125.9767413772842
At time: 114.30318832397461 and batch: 550, loss is 4.85970555305481 and perplexity is 128.98621691779806
At time: 114.86497282981873 and batch: 600, loss is 4.9269938468933105 and perplexity is 137.96414697777786
At time: 115.4209885597229 and batch: 650, loss is 4.86871416091919 and perplexity is 130.15345285744283
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.113970588235294 and perplexity of 166.32947130976632
Finished 15 epochs...
Completing Train Step...
At time: 116.46976661682129 and batch: 50, loss is 4.891213159561158 and perplexity is 133.11496582802758
At time: 117.02196788787842 and batch: 100, loss is 4.874068613052368 and perplexity is 130.85222238524506
At time: 117.57394981384277 and batch: 150, loss is 4.853920764923096 and perplexity is 128.2422130082739
At time: 118.1247193813324 and batch: 200, loss is 4.844169492721558 and perplexity is 126.99776561505239
At time: 118.67714071273804 and batch: 250, loss is 4.833058280944824 and perplexity is 125.59447710141751
At time: 119.22718644142151 and batch: 300, loss is 4.869703369140625 and perplexity is 130.28226542377848
At time: 119.79057121276855 and batch: 350, loss is 4.807173519134522 and perplexity is 122.3852087382776
At time: 120.34218502044678 and batch: 400, loss is 4.851470489501953 and perplexity is 127.92836892503104
At time: 120.89369893074036 and batch: 450, loss is 4.825517787933349 and perplexity is 124.65099445715234
At time: 121.44478964805603 and batch: 500, loss is 4.817722692489624 and perplexity is 123.68310535319921
At time: 121.99976754188538 and batch: 550, loss is 4.841694440841675 and perplexity is 126.6838282224547
At time: 122.55248737335205 and batch: 600, loss is 4.912020292282104 and perplexity is 135.91372266867336
At time: 123.10471987724304 and batch: 650, loss is 4.853854732513428 and perplexity is 128.23374514550756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.105058258655024 and perplexity of 164.8536744052308
Finished 16 epochs...
Completing Train Step...
At time: 124.13337826728821 and batch: 50, loss is 4.874548320770264 and perplexity is 130.9150082644584
At time: 124.70734596252441 and batch: 100, loss is 4.857956972122192 and perplexity is 128.76087115339544
At time: 125.26025009155273 and batch: 150, loss is 4.838755121231079 and perplexity is 126.31201067481601
At time: 125.81378030776978 and batch: 200, loss is 4.829841165542603 and perplexity is 125.19107441912095
At time: 126.36745762825012 and batch: 250, loss is 4.818902759552002 and perplexity is 123.82914586384992
At time: 126.92827272415161 and batch: 300, loss is 4.857720060348511 and perplexity is 128.7303698002372
At time: 127.49154901504517 and batch: 350, loss is 4.793342714309692 and perplexity is 120.70417462728096
At time: 128.0393545627594 and batch: 400, loss is 4.837932367324829 and perplexity is 126.20812971471973
At time: 128.5888376235962 and batch: 450, loss is 4.8136292552948 and perplexity is 123.1778511480659
At time: 129.13824558258057 and batch: 500, loss is 4.805789604187011 and perplexity is 122.21595516184087
At time: 129.68761014938354 and batch: 550, loss is 4.830312490463257 and perplexity is 125.25009399994518
At time: 130.2395462989807 and batch: 600, loss is 4.901567211151123 and perplexity is 134.5004051293015
At time: 130.78963208198547 and batch: 650, loss is 4.839096250534058 and perplexity is 126.35510675322502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.0959209367340685 and perplexity of 163.3542142698808
Finished 17 epochs...
Completing Train Step...
At time: 131.8136501312256 and batch: 50, loss is 4.860292634963989 and perplexity is 129.0619646251627
At time: 132.36681962013245 and batch: 100, loss is 4.846406631469726 and perplexity is 127.28219527311937
At time: 132.9267807006836 and batch: 150, loss is 4.826000537872314 and perplexity is 124.71118424425757
At time: 133.49526143074036 and batch: 200, loss is 4.817814960479736 and perplexity is 123.69451787123795
At time: 134.06935954093933 and batch: 250, loss is 4.805628099441528 and perplexity is 122.1962182989499
At time: 134.62140655517578 and batch: 300, loss is 4.843705739974975 and perplexity is 126.93888370681722
At time: 135.1731812953949 and batch: 350, loss is 4.779782609939575 and perplexity is 119.07846075709962
At time: 135.72808122634888 and batch: 400, loss is 4.824843845367432 and perplexity is 124.56701514789789
At time: 136.2901051044464 and batch: 450, loss is 4.800563135147095 and perplexity is 121.57886357595929
At time: 136.85002660751343 and batch: 500, loss is 4.7932952117919925 and perplexity is 120.69844101127097
At time: 137.4069366455078 and batch: 550, loss is 4.817062950134277 and perplexity is 123.60153328119111
At time: 137.95888328552246 and batch: 600, loss is 4.88850881576538 and perplexity is 132.755463524079
At time: 138.51161289215088 and batch: 650, loss is 4.824229040145874 and perplexity is 124.49045423398752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.09104949352788 and perplexity of 162.56037862507566
Finished 18 epochs...
Completing Train Step...
At time: 139.53459548950195 and batch: 50, loss is 4.846454153060913 and perplexity is 127.28824406929154
At time: 140.10101199150085 and batch: 100, loss is 4.833489933013916 and perplexity is 125.64870191961039
At time: 140.65298891067505 and batch: 150, loss is 4.811638431549072 and perplexity is 122.93286969541207
At time: 141.20706009864807 and batch: 200, loss is 4.803934459686279 and perplexity is 121.98943708157076
At time: 141.75940442085266 and batch: 250, loss is 4.791392078399658 and perplexity is 120.46895421903362
At time: 142.30915307998657 and batch: 300, loss is 4.83087607383728 and perplexity is 125.32070276561268
At time: 142.86112523078918 and batch: 350, loss is 4.763434038162232 and perplexity is 117.14752502251149
At time: 143.4139211177826 and batch: 400, loss is 4.80820252418518 and perplexity is 122.51120855229976
At time: 143.96506071090698 and batch: 450, loss is 4.786391534805298 and perplexity is 119.86804764361797
At time: 144.51650762557983 and batch: 500, loss is 4.779712800979614 and perplexity is 119.07014830374564
At time: 145.06759881973267 and batch: 550, loss is 4.802964382171631 and perplexity is 121.87115525218059
At time: 145.61960458755493 and batch: 600, loss is 4.873743906021118 and perplexity is 130.80974064601477
At time: 146.1706109046936 and batch: 650, loss is 4.805074968338013 and perplexity is 122.12864645964261
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.077967026654412 and perplexity of 160.44753857124206
Finished 19 epochs...
Completing Train Step...
At time: 147.2068853378296 and batch: 50, loss is 4.824772300720215 and perplexity is 124.55810336354337
At time: 147.75838613510132 and batch: 100, loss is 4.8161203575134275 and perplexity is 123.48508227954403
At time: 148.32277011871338 and batch: 150, loss is 4.79528058052063 and perplexity is 120.93830995701121
At time: 148.87374901771545 and batch: 200, loss is 4.788860092163086 and perplexity is 120.16431431978542
At time: 149.4389250278473 and batch: 250, loss is 4.773946933746338 and perplexity is 118.38558109554525
At time: 149.99076890945435 and batch: 300, loss is 4.81404746055603 and perplexity is 123.22937554661937
At time: 150.54210448265076 and batch: 350, loss is 4.743349142074585 and perplexity is 114.81810058692626
At time: 151.09284448623657 and batch: 400, loss is 4.789723958969116 and perplexity is 120.2681651323722
At time: 151.64644408226013 and batch: 450, loss is 4.76704607963562 and perplexity is 117.57143186505861
At time: 152.19848942756653 and batch: 500, loss is 4.766042575836182 and perplexity is 117.45350766506398
At time: 152.75065636634827 and batch: 550, loss is 4.78820237159729 and perplexity is 120.08530576460659
At time: 153.30181121826172 and batch: 600, loss is 4.860035734176636 and perplexity is 129.0288127633851
At time: 153.85274291038513 and batch: 650, loss is 4.788777961730957 and perplexity is 120.15444557799141
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.067406747855392 and perplexity of 158.7620829354221
Finished 20 epochs...
Completing Train Step...
At time: 154.8729350566864 and batch: 50, loss is 4.808422555923462 and perplexity is 122.5381678723208
At time: 155.4467842578888 and batch: 100, loss is 4.8023787689208985 and perplexity is 121.79980678212667
At time: 155.9993438720703 and batch: 150, loss is 4.7817082023620605 and perplexity is 119.30797824646477
At time: 156.55545568466187 and batch: 200, loss is 4.771081132888794 and perplexity is 118.0467972710722
At time: 157.10618925094604 and batch: 250, loss is 4.755411081314087 and perplexity is 116.21141568805895
At time: 157.65764141082764 and batch: 300, loss is 4.798683729171753 and perplexity is 121.35058211724214
At time: 158.20893263816833 and batch: 350, loss is 4.728035936355591 and perplexity is 113.07326103200647
At time: 158.76131916046143 and batch: 400, loss is 4.772554883956909 and perplexity is 118.22089712304818
At time: 159.32089138031006 and batch: 450, loss is 4.750726642608643 and perplexity is 115.66830351540611
At time: 159.8715250492096 and batch: 500, loss is 4.748599071502685 and perplexity is 115.42247257910076
At time: 160.42282342910767 and batch: 550, loss is 4.771543140411377 and perplexity is 118.1013483799693
At time: 160.9741175174713 and batch: 600, loss is 4.84309567451477 and perplexity is 126.8614662955561
At time: 161.52536606788635 and batch: 650, loss is 4.772328023910522 and perplexity is 118.19408056676059
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.058156929764093 and perplexity of 157.30033342914146
Finished 21 epochs...
Completing Train Step...
At time: 162.56505012512207 and batch: 50, loss is 4.7943618106842045 and perplexity is 120.82724651443769
At time: 163.11716842651367 and batch: 100, loss is 4.786727304458618 and perplexity is 119.90830245421317
At time: 163.66802716255188 and batch: 150, loss is 4.766777105331421 and perplexity is 117.53981242357827
At time: 164.2211937904358 and batch: 200, loss is 4.754004077911377 and perplexity is 116.04802080628497
At time: 164.77646017074585 and batch: 250, loss is 4.7410234928131105 and perplexity is 114.55138422066695
At time: 165.32814240455627 and batch: 300, loss is 4.78309289932251 and perplexity is 119.47329807381605
At time: 165.8787488937378 and batch: 350, loss is 4.717784824371338 and perplexity is 111.92005529007314
At time: 166.43106651306152 and batch: 400, loss is 4.764288749694824 and perplexity is 117.24769516535291
At time: 166.9941005706787 and batch: 450, loss is 4.739762649536133 and perplexity is 114.40704389239191
At time: 167.5535147190094 and batch: 500, loss is 4.73858380317688 and perplexity is 114.2722550285166
At time: 168.11557602882385 and batch: 550, loss is 4.7633035945892335 and perplexity is 117.13224487739969
At time: 168.66962814331055 and batch: 600, loss is 4.833997650146484 and perplexity is 125.71251211565409
At time: 169.22182273864746 and batch: 650, loss is 4.761752080917359 and perplexity is 116.95065350523045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.053073658662684 and perplexity of 156.50276204318064
Finished 22 epochs...
Completing Train Step...
At time: 170.2662055492401 and batch: 50, loss is 4.783234043121338 and perplexity is 119.49016217906866
At time: 170.83927035331726 and batch: 100, loss is 4.776893730163574 and perplexity is 118.73495381408975
At time: 171.39246892929077 and batch: 150, loss is 4.758126745223999 and perplexity is 116.52743574347629
At time: 171.94716429710388 and batch: 200, loss is 4.7429595375061036 and perplexity is 114.7733756434801
At time: 172.49939489364624 and batch: 250, loss is 4.725662136077881 and perplexity is 112.80516602146376
At time: 173.05210542678833 and batch: 300, loss is 4.771916399002075 and perplexity is 118.14543895090542
At time: 173.6043999195099 and batch: 350, loss is 4.705898962020874 and perplexity is 110.59766337103308
At time: 174.15792536735535 and batch: 400, loss is 4.753678312301636 and perplexity is 116.01022250905471
At time: 174.7135534286499 and batch: 450, loss is 4.728694972991943 and perplexity is 113.1478050145281
At time: 175.27818894386292 and batch: 500, loss is 4.726087417602539 and perplexity is 112.85315017712286
At time: 175.8436222076416 and batch: 550, loss is 4.746485185623169 and perplexity is 115.17874034596005
At time: 176.4232144355774 and batch: 600, loss is 4.817646579742432 and perplexity is 123.67369185051786
At time: 176.97809386253357 and batch: 650, loss is 4.7447598266601565 and perplexity is 114.98018701109912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.044253480200674 and perplexity of 155.12844950270204
Finished 23 epochs...
Completing Train Step...
At time: 178.01516008377075 and batch: 50, loss is 4.769010925292969 and perplexity is 117.80266868032
At time: 178.56738710403442 and batch: 100, loss is 4.76175365447998 and perplexity is 116.95083753455216
At time: 179.13265705108643 and batch: 150, loss is 4.741532764434814 and perplexity is 114.60973684728465
At time: 179.6852855682373 and batch: 200, loss is 4.727814493179321 and perplexity is 113.04822450211904
At time: 180.23776388168335 and batch: 250, loss is 4.705963926315308 and perplexity is 110.604848503586
At time: 180.78893661499023 and batch: 300, loss is 4.757281970977783 and perplexity is 116.42903793460184
At time: 181.34129762649536 and batch: 350, loss is 4.690094318389892 and perplexity is 108.863447148262
At time: 181.89327549934387 and batch: 400, loss is 4.742120409011841 and perplexity is 114.67710643036379
At time: 182.44690227508545 and batch: 450, loss is 4.714442386627197 and perplexity is 111.54659395626328
At time: 182.9994945526123 and batch: 500, loss is 4.7136119365692135 and perplexity is 111.45399853410281
At time: 183.55270195007324 and batch: 550, loss is 4.733730249404907 and perplexity is 113.7189722740301
At time: 184.1044635772705 and batch: 600, loss is 4.806258754730225 and perplexity is 122.27330629570025
At time: 184.65536403656006 and batch: 650, loss is 4.7335649108886715 and perplexity is 113.70017170215864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.033215691061581 and perplexity of 153.4255895806393
Finished 24 epochs...
Completing Train Step...
At time: 185.6769061088562 and batch: 50, loss is 4.7552155590057374 and perplexity is 116.18869598498387
At time: 186.24364852905273 and batch: 100, loss is 4.7506551265716555 and perplexity is 115.66003167252185
At time: 186.79522919654846 and batch: 150, loss is 4.730682430267334 and perplexity is 113.37290505706214
At time: 187.34435176849365 and batch: 200, loss is 4.715661315917969 and perplexity is 111.6826442679322
At time: 187.89398550987244 and batch: 250, loss is 4.696637926101684 and perplexity is 109.57814263380044
At time: 188.44411444664001 and batch: 300, loss is 4.746392621994018 and perplexity is 115.16807947716457
At time: 188.99573969841003 and batch: 350, loss is 4.6773019218444825 and perplexity is 107.47969240312635
At time: 189.54714274406433 and batch: 400, loss is 4.728919143676758 and perplexity is 113.17317227865577
At time: 190.09845304489136 and batch: 450, loss is 4.704686136245727 and perplexity is 110.46360898296649
At time: 190.68103909492493 and batch: 500, loss is 4.702516384124756 and perplexity is 110.22419016675617
At time: 191.2490816116333 and batch: 550, loss is 4.7251631546020505 and perplexity is 112.74889237416242
At time: 191.80765676498413 and batch: 600, loss is 4.796683759689331 and perplexity is 121.10812718837865
At time: 192.35833144187927 and batch: 650, loss is 4.719281015396118 and perplexity is 112.08763440621401
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.02653533337163 and perplexity of 152.40406762843088
Finished 25 epochs...
Completing Train Step...
At time: 193.39928889274597 and batch: 50, loss is 4.743451910018921 and perplexity is 114.82990081342824
At time: 193.9512641429901 and batch: 100, loss is 4.7416273212432865 and perplexity is 114.62057449059915
At time: 194.5090570449829 and batch: 150, loss is 4.719354152679443 and perplexity is 112.09583249107804
At time: 195.06663489341736 and batch: 200, loss is 4.705446701049805 and perplexity is 110.54765567351625
At time: 195.61905336380005 and batch: 250, loss is 4.685546398162842 and perplexity is 108.3694690130025
At time: 196.168466091156 and batch: 300, loss is 4.7365763473510745 and perplexity is 114.04308862210989
At time: 196.72579741477966 and batch: 350, loss is 4.667076301574707 and perplexity is 106.3862459936322
At time: 197.27886414527893 and batch: 400, loss is 4.721158390045166 and perplexity is 112.29826254161705
At time: 197.83118224143982 and batch: 450, loss is 4.694435920715332 and perplexity is 109.33711644137252
At time: 198.38268995285034 and batch: 500, loss is 4.691616687774658 and perplexity is 109.02930374278893
At time: 198.9378535747528 and batch: 550, loss is 4.71665542602539 and perplexity is 111.7937243171744
At time: 199.49294590950012 and batch: 600, loss is 4.787750110626221 and perplexity is 120.03100814688254
At time: 200.05407857894897 and batch: 650, loss is 4.709265527725219 and perplexity is 110.97062511937925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.024546903722427 and perplexity of 152.10132395369914
Finished 26 epochs...
Completing Train Step...
At time: 201.07900285720825 and batch: 50, loss is 4.735129642486572 and perplexity is 113.87822121703087
At time: 201.65715527534485 and batch: 100, loss is 4.735877418518067 and perplexity is 113.96340846788891
At time: 202.22114658355713 and batch: 150, loss is 4.7134985256195066 and perplexity is 111.44135914701641
At time: 202.77426528930664 and batch: 200, loss is 4.698783769607544 and perplexity is 109.81353264429406
At time: 203.32739067077637 and batch: 250, loss is 4.677930755615234 and perplexity is 107.54730051828459
At time: 203.87687373161316 and batch: 300, loss is 4.727761459350586 and perplexity is 113.04222928091819
At time: 204.42827486991882 and batch: 350, loss is 4.658993825912476 and perplexity is 105.52984732241715
At time: 204.99377536773682 and batch: 400, loss is 4.712962217330933 and perplexity is 111.38160824629976
At time: 205.54553985595703 and batch: 450, loss is 4.6863863658905025 and perplexity is 108.46053411015428
At time: 206.0967869758606 and batch: 500, loss is 4.682149581909179 and perplexity is 108.00198233496826
At time: 206.6486473083496 and batch: 550, loss is 4.707267827987671 and perplexity is 110.74916041434612
At time: 207.1997730731964 and batch: 600, loss is 4.780215473175049 and perplexity is 119.13001660240812
At time: 207.7640025615692 and batch: 650, loss is 4.701045551300049 and perplexity is 110.06218797805614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.020709467869179 and perplexity of 151.51876336417382
Finished 27 epochs...
Completing Train Step...
At time: 208.81945943832397 and batch: 50, loss is 4.726306581497193 and perplexity is 112.87788622356474
At time: 209.37435746192932 and batch: 100, loss is 4.726939649581909 and perplexity is 112.94936823490576
At time: 209.92709302902222 and batch: 150, loss is 4.703731412887573 and perplexity is 110.3581971228339
At time: 210.48006582260132 and batch: 200, loss is 4.688627309799195 and perplexity is 108.70386062210564
At time: 211.03345036506653 and batch: 250, loss is 4.6672258377075195 and perplexity is 106.40215577095614
At time: 211.58543181419373 and batch: 300, loss is 4.7178894138336185 and perplexity is 111.9317615606399
At time: 212.138085603714 and batch: 350, loss is 4.647860260009765 and perplexity is 104.3614401509324
At time: 212.6890332698822 and batch: 400, loss is 4.704146661758423 and perplexity is 110.40403275551688
At time: 213.23971438407898 and batch: 450, loss is 4.675724029541016 and perplexity is 107.31023475182519
At time: 213.79087233543396 and batch: 500, loss is 4.674580793380738 and perplexity is 107.18762391101669
At time: 214.34286189079285 and batch: 550, loss is 4.69794584274292 and perplexity is 109.72155547564402
At time: 214.9014048576355 and batch: 600, loss is 4.77453088760376 and perplexity is 118.45473300108507
At time: 215.45371556282043 and batch: 650, loss is 4.692311582565307 and perplexity is 109.10509396805443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.016255995806525 and perplexity of 150.84547912363473
Finished 28 epochs...
Completing Train Step...
At time: 216.47727751731873 and batch: 50, loss is 4.715521640777588 and perplexity is 111.6670460682817
At time: 217.04349207878113 and batch: 100, loss is 4.710882663726807 and perplexity is 111.15022489186772
At time: 217.59515047073364 and batch: 150, loss is 4.690152053833008 and perplexity is 108.86973260906736
At time: 218.1601414680481 and batch: 200, loss is 4.676291904449463 and perplexity is 107.3711908476409
At time: 218.71422743797302 and batch: 250, loss is 4.652298622131347 and perplexity is 104.82566344733617
At time: 219.28128480911255 and batch: 300, loss is 4.7019869995117185 and perplexity is 110.16585461883047
At time: 219.84470582008362 and batch: 350, loss is 4.635034456253051 and perplexity is 103.0314680124029
At time: 220.40128135681152 and batch: 400, loss is 4.691393909454345 and perplexity is 109.00501708300729
At time: 220.95514369010925 and batch: 450, loss is 4.661058530807495 and perplexity is 105.74796040684618
At time: 221.5224530696869 and batch: 500, loss is 4.661372346878052 and perplexity is 105.78115102385252
At time: 222.09064531326294 and batch: 550, loss is 4.684956874847412 and perplexity is 108.30560151187878
At time: 222.65434336662292 and batch: 600, loss is 4.7593472385406494 and perplexity is 116.6697435251907
At time: 223.22255730628967 and batch: 650, loss is 4.679670124053955 and perplexity is 107.7345276797804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.007096533681832 and perplexity of 149.47012404282918
Finished 29 epochs...
Completing Train Step...
At time: 224.27923679351807 and batch: 50, loss is 4.70539626121521 and perplexity is 110.54207980867321
At time: 224.83185458183289 and batch: 100, loss is 4.699420900344848 and perplexity is 109.88352051465793
At time: 225.3951313495636 and batch: 150, loss is 4.677658357620239 and perplexity is 107.51800883893476
At time: 225.96220111846924 and batch: 200, loss is 4.662327041625977 and perplexity is 105.8821879552042
At time: 226.5267894268036 and batch: 250, loss is 4.641662807464599 and perplexity is 103.71666512310154
At time: 227.09259057044983 and batch: 300, loss is 4.691329889297485 and perplexity is 108.99803878809324
At time: 227.64815616607666 and batch: 350, loss is 4.624455862045288 and perplexity is 101.94728460044323
At time: 228.20026206970215 and batch: 400, loss is 4.6813140392303465 and perplexity is 107.91177975862774
At time: 228.75535345077515 and batch: 450, loss is 4.652772397994995 and perplexity is 104.8753390831978
At time: 229.3131067752838 and batch: 500, loss is 4.6496003723144534 and perplexity is 104.54319887150344
At time: 229.88025093078613 and batch: 550, loss is 4.671791734695435 and perplexity is 106.88908784841203
At time: 230.44626331329346 and batch: 600, loss is 4.747275285720825 and perplexity is 115.26977904005984
At time: 231.01633620262146 and batch: 650, loss is 4.669442310333252 and perplexity is 106.63825479339349
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.003376680261948 and perplexity of 148.91514994183737
Finished 30 epochs...
Completing Train Step...
At time: 232.07542991638184 and batch: 50, loss is 4.6955366039276125 and perplexity is 109.4575282253767
At time: 232.64144682884216 and batch: 100, loss is 4.687230720520019 and perplexity is 108.55215193777538
At time: 233.20070004463196 and batch: 150, loss is 4.6662107181549075 and perplexity is 106.29419966564008
At time: 233.77695655822754 and batch: 200, loss is 4.65509503364563 and perplexity is 105.11920938592183
At time: 234.34220814704895 and batch: 250, loss is 4.631026773452759 and perplexity is 102.61937688698585
At time: 234.90998005867004 and batch: 300, loss is 4.679019336700439 and perplexity is 107.66443822076987
At time: 235.46622681617737 and batch: 350, loss is 4.611310052871704 and perplexity is 100.61587545023218
At time: 236.02042889595032 and batch: 400, loss is 4.676669378280639 and perplexity is 107.41172831284294
At time: 236.5750322341919 and batch: 450, loss is 4.646647500991821 and perplexity is 104.234951588839
At time: 237.13779854774475 and batch: 500, loss is 4.6408612918853756 and perplexity is 103.63356790648575
At time: 237.69938969612122 and batch: 550, loss is 4.658629713058471 and perplexity is 105.491429543154
At time: 238.26944613456726 and batch: 600, loss is 4.738121290206909 and perplexity is 114.2194148490342
At time: 238.8363926410675 and batch: 650, loss is 4.6610082912445066 and perplexity is 105.74264780898079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.003391041475184 and perplexity of 148.91728855941622
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 239.92403721809387 and batch: 50, loss is 4.681987476348877 and perplexity is 107.98447603208095
At time: 240.48504304885864 and batch: 100, loss is 4.65702446937561 and perplexity is 105.32222593512907
At time: 241.05312633514404 and batch: 150, loss is 4.626738414764405 and perplexity is 102.18025042936077
At time: 241.61434483528137 and batch: 200, loss is 4.598508939743042 and perplexity is 99.33608906752873
At time: 242.17358088493347 and batch: 250, loss is 4.570986623764038 and perplexity is 96.63940949057695
At time: 242.73284029960632 and batch: 300, loss is 4.6152009105682374 and perplexity is 101.00812009282267
At time: 243.28645253181458 and batch: 350, loss is 4.544076261520385 and perplexity is 94.07348776037583
At time: 243.83800148963928 and batch: 400, loss is 4.5968059730529784 and perplexity is 99.16706697706884
At time: 244.39014196395874 and batch: 450, loss is 4.55509916305542 and perplexity is 95.11618677964087
At time: 244.957661151886 and batch: 500, loss is 4.534764499664306 and perplexity is 93.20156372017931
At time: 245.51922702789307 and batch: 550, loss is 4.550631008148193 and perplexity is 94.69214097957102
At time: 246.08066701889038 and batch: 600, loss is 4.629731788635254 and perplexity is 102.4865723604174
At time: 246.64638757705688 and batch: 650, loss is 4.581208915710449 and perplexity is 97.63235217646094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.947132484585631 and perplexity of 140.7707223977038
Finished 32 epochs...
Completing Train Step...
At time: 247.68204188346863 and batch: 50, loss is 4.637624521255493 and perplexity is 103.29867210045366
At time: 248.2492220401764 and batch: 100, loss is 4.623671312332153 and perplexity is 101.86733325455877
At time: 248.80320000648499 and batch: 150, loss is 4.599212808609009 and perplexity is 99.40603326077252
At time: 249.3673186302185 and batch: 200, loss is 4.574177465438843 and perplexity is 96.94826303514013
At time: 249.92626070976257 and batch: 250, loss is 4.550849466323853 and perplexity is 94.71282951164591
At time: 250.4766972064972 and batch: 300, loss is 4.598490619659424 and perplexity is 99.33426923874049
At time: 251.02642178535461 and batch: 350, loss is 4.528722057342529 and perplexity is 92.6400966715435
At time: 251.57927060127258 and batch: 400, loss is 4.58390170097351 and perplexity is 97.89560942414302
At time: 252.1318323612213 and batch: 450, loss is 4.547182569503784 and perplexity is 94.36616332130863
At time: 252.68911814689636 and batch: 500, loss is 4.53380352973938 and perplexity is 93.11204284081359
At time: 253.25611305236816 and batch: 550, loss is 4.55418104171753 and perplexity is 95.02889865565591
At time: 253.82285022735596 and batch: 600, loss is 4.633742122650147 and perplexity is 102.89840298485672
At time: 254.38204503059387 and batch: 650, loss is 4.578609027862549 and perplexity is 97.37884869358236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9422149658203125 and perplexity of 140.08017900032203
Finished 33 epochs...
Completing Train Step...
At time: 255.4323694705963 and batch: 50, loss is 4.627378253936768 and perplexity is 102.24565027668827
At time: 255.98645329475403 and batch: 100, loss is 4.614012184143067 and perplexity is 100.88812040883987
At time: 256.5510756969452 and batch: 150, loss is 4.590404815673828 and perplexity is 98.5343103225731
At time: 257.12946796417236 and batch: 200, loss is 4.565633144378662 and perplexity is 96.12343476638682
At time: 257.67698907852173 and batch: 250, loss is 4.5439321899414065 and perplexity is 94.05993542073107
At time: 258.2218141555786 and batch: 300, loss is 4.591931419372559 and perplexity is 98.68484804164684
At time: 258.76696848869324 and batch: 350, loss is 4.523237838745117 and perplexity is 92.13342873926551
At time: 259.3114318847656 and batch: 400, loss is 4.579477319717407 and perplexity is 97.4634386738193
At time: 259.8697090148926 and batch: 450, loss is 4.545168046951294 and perplexity is 94.17625191174113
At time: 260.42183017730713 and batch: 500, loss is 4.534415712356568 and perplexity is 93.16906186613888
At time: 260.96893334388733 and batch: 550, loss is 4.556080799102784 and perplexity is 95.20960209969299
At time: 261.532541513443 and batch: 600, loss is 4.633672466278076 and perplexity is 102.89123570503921
At time: 262.09331727027893 and batch: 650, loss is 4.575368089675903 and perplexity is 97.06376073038433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9388876522288605 and perplexity of 139.61486287247294
Finished 34 epochs...
Completing Train Step...
At time: 263.07560110092163 and batch: 50, loss is 4.620457639694214 and perplexity is 101.54049045715053
At time: 263.6346278190613 and batch: 100, loss is 4.60707763671875 and perplexity is 100.19092710720234
At time: 264.1823432445526 and batch: 150, loss is 4.584423027038574 and perplexity is 97.94665826237994
At time: 264.7277808189392 and batch: 200, loss is 4.559975328445435 and perplexity is 95.58112166611804
At time: 265.27316999435425 and batch: 250, loss is 4.5394604206085205 and perplexity is 93.64026013095261
At time: 265.8187417984009 and batch: 300, loss is 4.587387561798096 and perplexity is 98.23745536146015
At time: 266.3766062259674 and batch: 350, loss is 4.519842720031738 and perplexity is 91.82115521402996
At time: 266.94187331199646 and batch: 400, loss is 4.576676778793335 and perplexity is 97.19087017297993
At time: 267.49472069740295 and batch: 450, loss is 4.544007225036621 and perplexity is 94.06699348173906
At time: 268.041672706604 and batch: 500, loss is 4.534887609481811 and perplexity is 93.21303845399176
At time: 268.5865972042084 and batch: 550, loss is 4.556366634368897 and perplexity is 95.23682025141417
At time: 269.1334195137024 and batch: 600, loss is 4.633515491485595 and perplexity is 102.87508564227583
At time: 269.6789836883545 and batch: 650, loss is 4.572770490646362 and perplexity is 96.81195518621463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.937585269703584 and perplexity of 139.4331492707939
Finished 35 epochs...
Completing Train Step...
At time: 270.6731026172638 and batch: 50, loss is 4.615743055343628 and perplexity is 101.06289596428759
At time: 271.218389749527 and batch: 100, loss is 4.6023266410827635 and perplexity is 99.71604941391779
At time: 271.76380729675293 and batch: 150, loss is 4.580428600311279 and perplexity is 97.55619786465576
At time: 272.30922985076904 and batch: 200, loss is 4.55613935470581 and perplexity is 95.21517731858643
At time: 272.8552026748657 and batch: 250, loss is 4.5362817001342775 and perplexity is 93.34307650094068
At time: 273.40207386016846 and batch: 300, loss is 4.584426012039184 and perplexity is 97.94695063365099
At time: 273.94982957839966 and batch: 350, loss is 4.517359895706177 and perplexity is 91.59346219411394
At time: 274.49780559539795 and batch: 400, loss is 4.574776563644409 and perplexity is 97.00636196730255
At time: 275.0458564758301 and batch: 450, loss is 4.5430309867858885 and perplexity is 93.97520649484427
At time: 275.59346985816956 and batch: 500, loss is 4.534828224182129 and perplexity is 93.20750313412881
At time: 276.15439558029175 and batch: 550, loss is 4.556187229156494 and perplexity is 95.21973580201389
At time: 276.7019772529602 and batch: 600, loss is 4.633009614944458 and perplexity is 102.82305671099941
At time: 277.393226146698 and batch: 650, loss is 4.570432558059692 and perplexity is 96.58587973895915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9362834855621935 and perplexity of 139.25175550167148
Finished 36 epochs...
Completing Train Step...
At time: 278.4045259952545 and batch: 50, loss is 4.612090978622437 and perplexity is 100.69447966633732
At time: 278.9528753757477 and batch: 100, loss is 4.598453483581543 and perplexity is 99.33058042207627
At time: 279.5014433860779 and batch: 150, loss is 4.577247581481934 and perplexity is 97.24636281915106
At time: 280.0500166416168 and batch: 200, loss is 4.552590560913086 and perplexity is 94.87787714671789
At time: 280.6001696586609 and batch: 250, loss is 4.533610906600952 and perplexity is 93.09410903418464
At time: 281.1528639793396 and batch: 300, loss is 4.582241268157959 and perplexity is 97.73319521801069
At time: 281.7028155326843 and batch: 350, loss is 4.515385303497315 and perplexity is 91.41278090192525
At time: 282.2652189731598 and batch: 400, loss is 4.5731425380706785 and perplexity is 96.84798052593658
At time: 282.829806804657 and batch: 450, loss is 4.542033214569091 and perplexity is 93.88148740766181
At time: 283.3920524120331 and batch: 500, loss is 4.534575881958008 and perplexity is 93.18398591280197
At time: 283.9428849220276 and batch: 550, loss is 4.555660037994385 and perplexity is 95.1695500287511
At time: 284.49368691444397 and batch: 600, loss is 4.632076902389526 and perplexity is 102.72719706676486
At time: 285.0445399284363 and batch: 650, loss is 4.56829647064209 and perplexity is 96.37978405409599
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.935077143650429 and perplexity of 139.08387155580272
Finished 37 epochs...
Completing Train Step...
At time: 286.0741984844208 and batch: 50, loss is 4.608872938156128 and perplexity is 100.37096158237027
At time: 286.6248731613159 and batch: 100, loss is 4.595320930480957 and perplexity is 99.01990895587315
At time: 287.1752755641937 and batch: 150, loss is 4.574626541137695 and perplexity is 96.99180992130748
At time: 287.7346558570862 and batch: 200, loss is 4.549594860076905 and perplexity is 94.59407671365236
At time: 288.2878305912018 and batch: 250, loss is 4.5313629531860355 and perplexity is 92.8850728536973
At time: 288.8391501903534 and batch: 300, loss is 4.580317850112915 and perplexity is 97.54539409466146
At time: 289.3864588737488 and batch: 350, loss is 4.513762292861938 and perplexity is 91.26453731930916
At time: 289.9340708255768 and batch: 400, loss is 4.571682767868042 and perplexity is 96.70670786767462
At time: 290.4948923587799 and batch: 450, loss is 4.540980005264283 and perplexity is 93.78266260231214
At time: 291.04291105270386 and batch: 500, loss is 4.5341440773010255 and perplexity is 93.14375731980059
At time: 291.60022258758545 and batch: 550, loss is 4.5548999214172365 and perplexity is 95.09723756256834
At time: 292.16394448280334 and batch: 600, loss is 4.631113042831421 and perplexity is 102.62823017874707
At time: 292.7148151397705 and batch: 650, loss is 4.566362257003784 and perplexity is 96.19354513231566
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.933781343347886 and perplexity of 138.90376335029706
Finished 38 epochs...
Completing Train Step...
At time: 293.8031952381134 and batch: 50, loss is 4.606233263015747 and perplexity is 100.1063642294329
At time: 294.3518569469452 and batch: 100, loss is 4.5926799392700195 and perplexity is 98.75874326655682
At time: 294.90254306793213 and batch: 150, loss is 4.572317514419556 and perplexity is 96.76811160284481
At time: 295.45178723335266 and batch: 200, loss is 4.546887130737304 and perplexity is 94.33828801634519
At time: 296.00219464302063 and batch: 250, loss is 4.529463739395141 and perplexity is 92.70883165519805
At time: 296.5504426956177 and batch: 300, loss is 4.578608922958374 and perplexity is 97.3788384781351
At time: 297.10011553764343 and batch: 350, loss is 4.512145109176636 and perplexity is 91.11706507550323
At time: 297.64917826652527 and batch: 400, loss is 4.570209074020386 and perplexity is 96.56429674823192
At time: 298.1974108219147 and batch: 450, loss is 4.5400137519836425 and perplexity is 93.69208856268398
At time: 298.74584555625916 and batch: 500, loss is 4.533431358337403 and perplexity is 93.07739564903541
At time: 299.3076186180115 and batch: 550, loss is 4.553957290649414 and perplexity is 95.00763821669155
At time: 299.85791015625 and batch: 600, loss is 4.630188283920288 and perplexity is 102.53336767758744
At time: 300.4084360599518 and batch: 650, loss is 4.564822444915771 and perplexity is 96.04553912869262
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.932595645680147 and perplexity of 138.7391630844121
Finished 39 epochs...
Completing Train Step...
At time: 301.42170429229736 and batch: 50, loss is 4.603973751068115 and perplexity is 99.88042805229273
At time: 301.98362708091736 and batch: 100, loss is 4.590497694015503 and perplexity is 98.54346245092458
At time: 302.5357596874237 and batch: 150, loss is 4.570077056884766 and perplexity is 96.55154944782164
At time: 303.08397674560547 and batch: 200, loss is 4.544630994796753 and perplexity is 94.1256879317079
At time: 303.6398777961731 and batch: 250, loss is 4.527647800445557 and perplexity is 92.54063084424895
At time: 304.19589161872864 and batch: 300, loss is 4.577061319351197 and perplexity is 97.22825119121352
At time: 304.7430577278137 and batch: 350, loss is 4.510604076385498 and perplexity is 90.97675882642132
At time: 305.29081559181213 and batch: 400, loss is 4.568784732818603 and perplexity is 96.42685414756814
At time: 305.83868980407715 and batch: 450, loss is 4.538992958068848 and perplexity is 93.59649704673436
At time: 306.3870141506195 and batch: 500, loss is 4.532713994979859 and perplexity is 93.0106492795434
At time: 306.9350583553314 and batch: 550, loss is 4.5530157470703125 and perplexity is 94.91822648408969
At time: 307.4829385280609 and batch: 600, loss is 4.628849153518677 and perplexity is 102.39615402174493
At time: 308.0312252044678 and batch: 650, loss is 4.563046426773071 and perplexity is 95.87511189440207
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.931977515127144 and perplexity of 138.65343066845423
Finished 40 epochs...
Completing Train Step...
At time: 309.03367733955383 and batch: 50, loss is 4.601772146224976 and perplexity is 99.66077270402086
At time: 309.59480905532837 and batch: 100, loss is 4.588228149414062 and perplexity is 98.32006726626877
At time: 310.14383912086487 and batch: 150, loss is 4.567960758209228 and perplexity is 96.34743359284235
At time: 310.69347739219666 and batch: 200, loss is 4.542038555145264 and perplexity is 93.88198879023531
At time: 311.2423610687256 and batch: 250, loss is 4.525995082855225 and perplexity is 92.38781363246396
At time: 311.79028248786926 and batch: 300, loss is 4.57551064491272 and perplexity is 97.07759866409312
At time: 312.3360505104065 and batch: 350, loss is 4.509145879745484 and perplexity is 90.84419349902863
At time: 312.88457322120667 and batch: 400, loss is 4.5670825958251955 and perplexity is 96.26286204007616
At time: 313.43465852737427 and batch: 450, loss is 4.537839279174805 and perplexity is 93.48857900688732
At time: 313.9994065761566 and batch: 500, loss is 4.531875381469726 and perplexity is 92.93268198925225
At time: 314.55583333969116 and batch: 550, loss is 4.552006464004517 and perplexity is 94.82247545353887
At time: 315.1263167858124 and batch: 600, loss is 4.627658023834228 and perplexity is 102.27425953359977
At time: 315.6750705242157 and batch: 650, loss is 4.5613933944702145 and perplexity is 95.71675815538225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.930595846737132 and perplexity of 138.46198989042384
Finished 41 epochs...
Completing Train Step...
At time: 316.700443983078 and batch: 50, loss is 4.599886245727539 and perplexity is 99.47299951962755
At time: 317.24889039993286 and batch: 100, loss is 4.5862382125854495 and perplexity is 98.12461108065305
At time: 317.7975449562073 and batch: 150, loss is 4.5659160232543945 and perplexity is 96.15062990182841
At time: 318.34867882728577 and batch: 200, loss is 4.539964914321899 and perplexity is 93.68751297188624
At time: 318.91136837005615 and batch: 250, loss is 4.524346904754639 and perplexity is 92.23566747769605
At time: 319.47322630882263 and batch: 300, loss is 4.573737077713012 and perplexity is 96.90557760981746
At time: 320.0221381187439 and batch: 350, loss is 4.507374696731567 and perplexity is 90.68343421567256
At time: 320.5813641548157 and batch: 400, loss is 4.565650501251221 and perplexity is 96.12510318307318
At time: 321.134149312973 and batch: 450, loss is 4.53673490524292 and perplexity is 93.38538964761018
At time: 321.68332958221436 and batch: 500, loss is 4.530851669311524 and perplexity is 92.8375943522947
At time: 322.2315309047699 and batch: 550, loss is 4.5509921836853025 and perplexity is 94.7263476413822
At time: 322.78000044822693 and batch: 600, loss is 4.626358146667481 and perplexity is 102.14140192687798
At time: 323.328884601593 and batch: 650, loss is 4.559997272491455 and perplexity is 95.58321912566385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.929959166283701 and perplexity of 138.37386190557703
Finished 42 epochs...
Completing Train Step...
At time: 324.34264278411865 and batch: 50, loss is 4.59799262046814 and perplexity is 99.28481316855633
At time: 324.9135413169861 and batch: 100, loss is 4.584295463562012 and perplexity is 97.93416464301603
At time: 325.461466550827 and batch: 150, loss is 4.563844299316406 and perplexity is 95.9516385389663
At time: 326.00973200798035 and batch: 200, loss is 4.537568969726562 and perplexity is 93.46331157584551
At time: 326.558012008667 and batch: 250, loss is 4.522814474105835 and perplexity is 92.09443095916454
At time: 327.1068277359009 and batch: 300, loss is 4.572080430984497 and perplexity is 96.74517220592477
At time: 327.65500950813293 and batch: 350, loss is 4.5057533073425295 and perplexity is 90.53652019227833
At time: 328.2017352581024 and batch: 400, loss is 4.56421464920044 and perplexity is 95.98718079830162
At time: 328.74981904029846 and batch: 450, loss is 4.5356386947631835 and perplexity is 93.28307569388406
At time: 329.32534885406494 and batch: 500, loss is 4.530005645751953 and perplexity is 92.75908477541748
At time: 329.8814616203308 and batch: 550, loss is 4.549882755279541 and perplexity is 94.62131381506335
At time: 330.42957305908203 and batch: 600, loss is 4.624936532974243 and perplexity is 101.99629947550567
At time: 330.97863721847534 and batch: 650, loss is 4.5584330368042 and perplexity is 95.43382132035666
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.928873099532782 and perplexity of 138.22366023424988
Finished 43 epochs...
Completing Train Step...
At time: 331.9916181564331 and batch: 50, loss is 4.596103267669678 and perplexity is 99.09740622361765
At time: 332.53861570358276 and batch: 100, loss is 4.582209596633911 and perplexity is 97.73009990778493
At time: 333.0851159095764 and batch: 150, loss is 4.56191653251648 and perplexity is 95.76684433308881
At time: 333.63173627853394 and batch: 200, loss is 4.535626373291016 and perplexity is 93.28192631614418
At time: 334.17978024482727 and batch: 250, loss is 4.521584558486938 and perplexity is 91.9812322068408
At time: 334.7277195453644 and batch: 300, loss is 4.5706446170806885 and perplexity is 96.60636381789878
At time: 335.27595591545105 and batch: 350, loss is 4.5044354057312015 and perplexity is 90.41728055674538
At time: 335.8292090892792 and batch: 400, loss is 4.562747020721435 and perplexity is 95.84641060258424
At time: 336.3905436992645 and batch: 450, loss is 4.533911428451538 and perplexity is 93.12209005235334
At time: 336.94111919403076 and batch: 500, loss is 4.529155693054199 and perplexity is 92.68027743707066
At time: 337.50123715400696 and batch: 550, loss is 4.548905096054077 and perplexity is 94.5288516203101
At time: 338.0603325366974 and batch: 600, loss is 4.6238059806823735 and perplexity is 101.8810524840218
At time: 338.6104793548584 and batch: 650, loss is 4.556981687545776 and perplexity is 95.2954139775473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.927782245710785 and perplexity of 138.0729606367527
Finished 44 epochs...
Completing Train Step...
At time: 339.6235761642456 and batch: 50, loss is 4.594159908294678 and perplexity is 98.90501135692936
At time: 340.1866669654846 and batch: 100, loss is 4.580066738128662 and perplexity is 97.52090235240927
At time: 340.74864506721497 and batch: 150, loss is 4.559576416015625 and perplexity is 95.54300077258493
At time: 341.30212688446045 and batch: 200, loss is 4.5334671783447265 and perplexity is 93.0807297417425
At time: 341.8525731563568 and batch: 250, loss is 4.519608039855957 and perplexity is 91.79960913750129
At time: 342.4030809402466 and batch: 300, loss is 4.569009399414062 and perplexity is 96.4485204743591
At time: 342.9605610370636 and batch: 350, loss is 4.502939367294312 and perplexity is 90.282113962094
At time: 343.5357310771942 and batch: 400, loss is 4.561617517471314 and perplexity is 95.7382128866351
At time: 344.0852816104889 and batch: 450, loss is 4.533496217727661 and perplexity is 93.08343278794453
At time: 344.64788150787354 and batch: 500, loss is 4.528021688461304 and perplexity is 92.5752371461289
At time: 345.20447874069214 and batch: 550, loss is 4.547532367706299 and perplexity is 94.39917820955426
At time: 345.75269317626953 and batch: 600, loss is 4.622337522506714 and perplexity is 101.73155421240092
At time: 346.300874710083 and batch: 650, loss is 4.555391960144043 and perplexity is 95.14404059977056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.92671921673943 and perplexity of 137.92626306513387
Finished 45 epochs...
Completing Train Step...
At time: 347.3134458065033 and batch: 50, loss is 4.592200508117676 and perplexity is 98.71140659671234
At time: 347.8786060810089 and batch: 100, loss is 4.577957344055176 and perplexity is 97.31540914821555
At time: 348.43616342544556 and batch: 150, loss is 4.557541980743408 and perplexity is 95.34882231053004
At time: 348.9868347644806 and batch: 200, loss is 4.531201648712158 and perplexity is 92.87009128421913
At time: 349.53744769096375 and batch: 250, loss is 4.518112878799439 and perplexity is 91.66245649511067
At time: 350.0870385169983 and batch: 300, loss is 4.567567167282104 and perplexity is 96.30951957892393
At time: 350.6508684158325 and batch: 350, loss is 4.501681203842163 and perplexity is 90.16859573314228
At time: 351.2056221961975 and batch: 400, loss is 4.560276746749878 and perplexity is 95.60993590809923
At time: 351.75475907325745 and batch: 450, loss is 4.5323484420776365 and perplexity is 92.97665518046104
At time: 352.30612683296204 and batch: 500, loss is 4.5270568370819095 and perplexity is 92.48595887792686
At time: 352.85607504844666 and batch: 550, loss is 4.546334590911865 and perplexity is 94.28617675325229
At time: 353.4060399532318 and batch: 600, loss is 4.621023263931274 and perplexity is 101.59794046562301
At time: 353.95514464378357 and batch: 650, loss is 4.5540289783477785 and perplexity is 95.01444933973627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.925977519914215 and perplexity of 137.82400152192363
Finished 46 epochs...
Completing Train Step...
At time: 354.9587869644165 and batch: 50, loss is 4.590701580047607 and perplexity is 98.56355613481468
At time: 355.5270335674286 and batch: 100, loss is 4.57613733291626 and perplexity is 97.13845509759506
At time: 356.08426666259766 and batch: 150, loss is 4.555511436462402 and perplexity is 95.15540873855345
At time: 356.6406943798065 and batch: 200, loss is 4.529248342514038 and perplexity is 92.68886461250538
At time: 357.18914890289307 and batch: 250, loss is 4.516848802566528 and perplexity is 91.54666136476006
At time: 357.7495620250702 and batch: 300, loss is 4.566277847290039 and perplexity is 96.1854258053868
At time: 358.297700881958 and batch: 350, loss is 4.500309867858887 and perplexity is 90.04502903834394
At time: 358.8521099090576 and batch: 400, loss is 4.559091196060181 and perplexity is 95.49665264743399
At time: 359.4104564189911 and batch: 450, loss is 4.5312333679199215 and perplexity is 92.87303709665876
At time: 359.9570424556732 and batch: 500, loss is 4.526238956451416 and perplexity is 92.41034732839378
At time: 360.5053427219391 and batch: 550, loss is 4.545220108032226 and perplexity is 94.18115495684162
At time: 361.0538353919983 and batch: 600, loss is 4.620015420913696 and perplexity is 101.4955972723237
At time: 361.601939201355 and batch: 650, loss is 4.552710475921631 and perplexity is 94.88925511034735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.925274418849571 and perplexity of 137.72713137846117
Finished 47 epochs...
Completing Train Step...
At time: 362.61486983299255 and batch: 50, loss is 4.589138927459717 and perplexity is 98.40965581643155
At time: 363.17508244514465 and batch: 100, loss is 4.574283695220947 and perplexity is 96.95856237503637
At time: 363.7268304824829 and batch: 150, loss is 4.553594722747802 and perplexity is 94.97319774054908
At time: 364.2755527496338 and batch: 200, loss is 4.5273698139190675 and perplexity is 92.51490937099855
At time: 364.8236229419708 and batch: 250, loss is 4.515276145935059 and perplexity is 91.40280305019185
At time: 365.37142610549927 and batch: 300, loss is 4.564792633056641 and perplexity is 96.04267587528966
At time: 365.9199757575989 and batch: 350, loss is 4.498830995559692 and perplexity is 89.911962357785
At time: 366.46877813339233 and batch: 400, loss is 4.557877578735352 and perplexity is 95.38082655381116
At time: 367.01759576797485 and batch: 450, loss is 4.530174369812012 and perplexity is 92.77473678520629
At time: 367.56695675849915 and batch: 500, loss is 4.52535189628601 and perplexity is 92.32841013738944
At time: 368.11553478240967 and batch: 550, loss is 4.54395318031311 and perplexity is 94.06190979445934
At time: 368.6637508869171 and batch: 600, loss is 4.618637628555298 and perplexity is 101.3558537049069
At time: 369.21184968948364 and batch: 650, loss is 4.550530471801758 and perplexity is 94.68262145621784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9231839647480085 and perplexity of 137.43951985585664
Finished 48 epochs...
Completing Train Step...
At time: 370.2124330997467 and batch: 50, loss is 4.5859904479980464 and perplexity is 98.10030228842786
At time: 370.7725110054016 and batch: 100, loss is 4.5714005088806156 and perplexity is 96.6794153821904
At time: 371.3335614204407 and batch: 150, loss is 4.549392499923706 and perplexity is 94.57493657846256
At time: 371.89989829063416 and batch: 200, loss is 4.523592863082886 and perplexity is 92.16614415583098
At time: 372.44803261756897 and batch: 250, loss is 4.5119747543334965 and perplexity is 91.10154416424369
At time: 373.01186656951904 and batch: 300, loss is 4.561767282485962 and perplexity is 95.75255219522707
At time: 373.5721402168274 and batch: 350, loss is 4.495248908996582 and perplexity is 89.59046608307222
At time: 374.1278066635132 and batch: 400, loss is 4.554415836334228 and perplexity is 95.05121354909696
At time: 374.6813156604767 and batch: 450, loss is 4.527516098022461 and perplexity is 92.52844382147974
At time: 375.2324228286743 and batch: 500, loss is 4.5221804428100585 and perplexity is 92.03605871464008
At time: 375.77928948402405 and batch: 550, loss is 4.540784196853638 and perplexity is 93.76430096594204
At time: 376.3287420272827 and batch: 600, loss is 4.6153182029724125 and perplexity is 101.01996827290674
At time: 376.8778636455536 and batch: 650, loss is 4.54782958984375 and perplexity is 94.42723990514735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.921295465207567 and perplexity of 137.18021031581148
Finished 49 epochs...
Completing Train Step...
At time: 377.90755581855774 and batch: 50, loss is 4.5835568141937255 and perplexity is 97.86185234417356
At time: 378.46207642555237 and batch: 100, loss is 4.569399023056031 and perplexity is 96.4861064198798
At time: 379.0110626220703 and batch: 150, loss is 4.549488801956176 and perplexity is 94.58404477563771
At time: 379.55978894233704 and batch: 200, loss is 4.523752746582031 and perplexity is 92.18088117953337
At time: 380.10959458351135 and batch: 250, loss is 4.512145099639892 and perplexity is 91.11706420654312
At time: 380.65798354148865 and batch: 300, loss is 4.561263675689697 and perplexity is 95.70434269951558
At time: 381.2059769630432 and batch: 350, loss is 4.493444843292236 and perplexity is 89.4289847010841
At time: 381.7540454864502 and batch: 400, loss is 4.55281704902649 and perplexity is 94.89936829176916
At time: 382.3118507862091 and batch: 450, loss is 4.525497255325317 and perplexity is 92.34183188185003
At time: 382.8666498661041 and batch: 500, loss is 4.522675867080689 and perplexity is 92.08166690867071
At time: 383.4125382900238 and batch: 550, loss is 4.541234798431397 and perplexity is 93.80656082836006
At time: 383.96136593818665 and batch: 600, loss is 4.616287393569946 and perplexity is 101.11792333721597
At time: 384.51064586639404 and batch: 650, loss is 4.548521223068238 and perplexity is 94.49257151171297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.922870411592371 and perplexity of 137.39643201621368
Annealing...
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa09ad998d0>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'seq_len': 20, 'anneal': 5.576268026348681, 'data': 'ptb', 'dropout': 0.10527237077851527, 'batch_size': 80, 'num_layers': 1, 'lr': 15.60061705262029}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7564802169799805 and batch: 50, loss is 6.6016468334197995 and perplexity is 736.3067659250141
At time: 1.3261659145355225 and batch: 100, loss is 5.626740159988404 and perplexity is 277.7552027141486
At time: 1.8812315464019775 and batch: 150, loss is 5.453576307296753 and perplexity is 233.5920708962225
At time: 2.435655117034912 and batch: 200, loss is 5.424133949279785 and perplexity is 226.81482808496514
At time: 2.9900050163269043 and batch: 250, loss is 5.382435750961304 and perplexity is 217.55153194781226
At time: 3.5448639392852783 and batch: 300, loss is 5.419883708953858 and perplexity is 225.85285630986695
At time: 4.100061655044556 and batch: 350, loss is 5.379076452255249 and perplexity is 216.82193751696923
At time: 4.65498948097229 and batch: 400, loss is 5.438643751144409 and perplexity is 230.1298583562009
At time: 5.209770917892456 and batch: 450, loss is 5.409009523391724 and perplexity is 223.41019548407235
At time: 5.764328718185425 and batch: 500, loss is 5.409508104324341 and perplexity is 223.5216113202907
At time: 6.317959308624268 and batch: 550, loss is 5.425617265701294 and perplexity is 227.1515158896476
At time: 6.873336315155029 and batch: 600, loss is 5.465723733901978 and perplexity is 236.4469178499077
At time: 7.444684028625488 and batch: 650, loss is 5.457544116973877 and perplexity is 234.52076098999106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.447225533279718 and perplexity of 232.11328114181347
Finished 1 epochs...
Completing Train Step...
At time: 8.461007833480835 and batch: 50, loss is 5.356740283966064 and perplexity is 212.0326524846675
At time: 9.007891654968262 and batch: 100, loss is 5.327588586807251 and perplexity is 205.94076662927338
At time: 9.55427074432373 and batch: 150, loss is 5.300086641311646 and perplexity is 200.35416817073641
At time: 10.099548101425171 and batch: 200, loss is 5.281585454940796 and perplexity is 196.6814578756564
At time: 10.646769523620605 and batch: 250, loss is 5.239597158432007 and perplexity is 188.59411355254144
At time: 11.193528413772583 and batch: 300, loss is 5.278188457489014 and perplexity is 196.0144649924044
At time: 11.740068674087524 and batch: 350, loss is 5.232685604095459 and perplexity is 187.29512926041937
At time: 12.28861141204834 and batch: 400, loss is 5.298033275604248 and perplexity is 199.94318988124448
At time: 12.834371566772461 and batch: 450, loss is 5.273469619750976 and perplexity is 195.09168347824388
At time: 13.387319087982178 and batch: 500, loss is 5.272857828140259 and perplexity is 194.9723645258598
At time: 13.94735836982727 and batch: 550, loss is 5.292902450561524 and perplexity is 198.91994364895302
At time: 14.49445390701294 and batch: 600, loss is 5.34433609008789 and perplexity is 209.41880321598768
At time: 15.055588483810425 and batch: 650, loss is 5.331332912445069 and perplexity is 206.7133213672916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.447162702971814 and perplexity of 232.09869785103186
Finished 2 epochs...
Completing Train Step...
At time: 16.05518364906311 and batch: 50, loss is 5.304257068634033 and perplexity is 201.19147541877277
At time: 16.62367844581604 and batch: 100, loss is 5.281797819137573 and perplexity is 196.72323041081754
At time: 17.176635026931763 and batch: 150, loss is 5.257072381973266 and perplexity is 191.9188030834878
At time: 17.727630615234375 and batch: 200, loss is 5.245178823471069 and perplexity is 189.6497260197009
At time: 18.275470733642578 and batch: 250, loss is 5.215009651184082 and perplexity is 184.01359692017408
At time: 18.82306456565857 and batch: 300, loss is 5.249360427856446 and perplexity is 190.44442654932163
At time: 19.369443893432617 and batch: 350, loss is 5.207075338363648 and perplexity is 182.55935232388742
At time: 19.915153741836548 and batch: 400, loss is 5.243686771392822 and perplexity is 189.36696974785215
At time: 20.47439932823181 and batch: 450, loss is 5.229151420593261 and perplexity is 186.63436222829029
At time: 21.024224996566772 and batch: 500, loss is 5.23538233757019 and perplexity is 187.80089595763118
At time: 21.57123851776123 and batch: 550, loss is 5.240286226272583 and perplexity is 188.72411247500708
At time: 22.118483543395996 and batch: 600, loss is 5.304629888534546 and perplexity is 201.26649758863232
At time: 22.66564154624939 and batch: 650, loss is 5.270419387817383 and perplexity is 194.49751523147435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.4640341366038605 and perplexity of 236.0477550834849
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 23.683550596237183 and batch: 50, loss is 5.1808108043670655 and perplexity is 177.82693541292906
At time: 24.2297306060791 and batch: 100, loss is 5.036992015838623 and perplexity is 154.0060697870639
At time: 24.77723503112793 and batch: 150, loss is 4.957403745651245 and perplexity is 142.22406629800696
At time: 25.324928045272827 and batch: 200, loss is 4.928847274780273 and perplexity is 138.22009068852498
At time: 25.887756824493408 and batch: 250, loss is 4.895081911087036 and perplexity is 133.63095202280738
At time: 26.43596601486206 and batch: 300, loss is 4.9314683818817135 and perplexity is 138.58285556489818
At time: 26.98286485671997 and batch: 350, loss is 4.867318334579468 and perplexity is 129.97190797178112
At time: 27.530415534973145 and batch: 400, loss is 4.874839916229248 and perplexity is 130.9531880526371
At time: 28.077358722686768 and batch: 450, loss is 4.824269638061524 and perplexity is 124.4955083895411
At time: 28.624062061309814 and batch: 500, loss is 4.7896912574768065 and perplexity is 120.26423224820098
At time: 29.190091371536255 and batch: 550, loss is 4.7863973617553714 and perplexity is 119.86874611078197
At time: 29.736496448516846 and batch: 600, loss is 4.863173589706421 and perplexity is 129.43432241964973
At time: 30.28378963470459 and batch: 650, loss is 4.848770523071289 and perplexity is 127.58343249134343
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.059281891467524 and perplexity of 157.4773898523613
Finished 4 epochs...
Completing Train Step...
At time: 31.284767866134644 and batch: 50, loss is 4.902157535552979 and perplexity is 134.57982744066658
At time: 31.842238664627075 and batch: 100, loss is 4.866742296218872 and perplexity is 129.8970607265014
At time: 32.38915014266968 and batch: 150, loss is 4.832214870452881 and perplexity is 125.4885940594109
At time: 32.93676567077637 and batch: 200, loss is 4.8099805450439455 and perplexity is 122.7292298022664
At time: 33.48743772506714 and batch: 250, loss is 4.784374694824219 and perplexity is 119.62653659910677
At time: 34.0442156791687 and batch: 300, loss is 4.8370928287506105 and perplexity is 126.10221758632373
At time: 34.590673208236694 and batch: 350, loss is 4.774166555404663 and perplexity is 118.41158398848196
At time: 35.13759112358093 and batch: 400, loss is 4.80724928855896 and perplexity is 122.39448214641948
At time: 35.68456268310547 and batch: 450, loss is 4.765819511413574 and perplexity is 117.42731088808651
At time: 36.23060417175293 and batch: 500, loss is 4.749188289642334 and perplexity is 115.49050163367583
At time: 36.77773475646973 and batch: 550, loss is 4.766124410629272 and perplexity is 117.46311984186138
At time: 37.3249135017395 and batch: 600, loss is 4.8510838317871094 and perplexity is 127.87891399591994
At time: 37.872490644454956 and batch: 650, loss is 4.816030569076538 and perplexity is 123.47399524477824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.032409069584865 and perplexity of 153.30188310386367
Finished 5 epochs...
Completing Train Step...
At time: 38.88680100440979 and batch: 50, loss is 4.849699544906616 and perplexity is 127.70201536046227
At time: 39.44222927093506 and batch: 100, loss is 4.819004669189453 and perplexity is 123.84176589025095
At time: 39.99067234992981 and batch: 150, loss is 4.788602905273438 and perplexity is 120.1334136073382
At time: 40.540879011154175 and batch: 200, loss is 4.767250127792359 and perplexity is 117.59542454676375
At time: 41.09083437919617 and batch: 250, loss is 4.742879457473755 and perplexity is 114.76418495584603
At time: 41.64278841018677 and batch: 300, loss is 4.797650842666626 and perplexity is 121.22530544801218
At time: 42.20470142364502 and batch: 350, loss is 4.737463912963867 and perplexity is 114.1443542792572
At time: 42.7568838596344 and batch: 400, loss is 4.776400804519653 and perplexity is 118.67644073300887
At time: 43.3196747303009 and batch: 450, loss is 4.7383135890960695 and perplexity is 114.24138122761767
At time: 43.867433309555054 and batch: 500, loss is 4.723790454864502 and perplexity is 112.59422817722897
At time: 44.41522812843323 and batch: 550, loss is 4.747113666534424 and perplexity is 115.25115073754084
At time: 44.96349310874939 and batch: 600, loss is 4.83228217124939 and perplexity is 125.49703982594413
At time: 45.511484146118164 and batch: 650, loss is 4.7904700756073 and perplexity is 120.35793269579324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.020665785845588 and perplexity of 151.5121448625339
Finished 6 epochs...
Completing Train Step...
At time: 46.50868034362793 and batch: 50, loss is 4.814964370727539 and perplexity is 123.3424176312856
At time: 47.06835412979126 and batch: 100, loss is 4.7881070995330814 and perplexity is 120.0738655346211
At time: 47.614137411117554 and batch: 150, loss is 4.757493143081665 and perplexity is 116.45362709567645
At time: 48.16220164299011 and batch: 200, loss is 4.742348890304566 and perplexity is 114.7033109973996
At time: 48.71035289764404 and batch: 250, loss is 4.716855754852295 and perplexity is 111.81612206620467
At time: 49.25778818130493 and batch: 300, loss is 4.77056489944458 and perplexity is 117.98587329318906
At time: 49.80487298965454 and batch: 350, loss is 4.714771995544433 and perplexity is 111.58336676830908
At time: 50.352508783340454 and batch: 400, loss is 4.75645341873169 and perplexity is 116.33261034687426
At time: 50.90010595321655 and batch: 450, loss is 4.721271553039551 and perplexity is 112.3109712683357
At time: 51.44750690460205 and batch: 500, loss is 4.706543741226196 and perplexity is 110.66899743942251
At time: 51.994635820388794 and batch: 550, loss is 4.72967755317688 and perplexity is 113.25903644365444
At time: 52.542420387268066 and batch: 600, loss is 4.814556388854981 and perplexity is 123.29210642450255
At time: 53.09039640426636 and batch: 650, loss is 4.767203493118286 and perplexity is 117.58994065033843
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.007918713139553 and perplexity of 149.5930658415454
Finished 7 epochs...
Completing Train Step...
At time: 54.105493783950806 and batch: 50, loss is 4.787550106048584 and perplexity is 120.00700379636476
At time: 54.653491497039795 and batch: 100, loss is 4.7636838722229005 and perplexity is 117.17679612069156
At time: 55.201441049575806 and batch: 150, loss is 4.733805961608887 and perplexity is 113.72758251400131
At time: 55.747950315475464 and batch: 200, loss is 4.717527294158936 and perplexity is 111.89123620551396
At time: 56.29489254951477 and batch: 250, loss is 4.69491213798523 and perplexity is 109.38919706432453
At time: 56.84149932861328 and batch: 300, loss is 4.748489770889282 and perplexity is 115.40985752147674
At time: 57.40363621711731 and batch: 350, loss is 4.690573215484619 and perplexity is 108.91559402231987
At time: 57.951443672180176 and batch: 400, loss is 4.7357165813446045 and perplexity is 113.9450803893505
At time: 58.50666689872742 and batch: 450, loss is 4.7020759010314945 and perplexity is 110.17564896609322
At time: 59.05688285827637 and batch: 500, loss is 4.690457859039307 and perplexity is 108.90303063120236
At time: 59.61391520500183 and batch: 550, loss is 4.7104064559936525 and perplexity is 111.0973068962118
At time: 60.16480779647827 and batch: 600, loss is 4.797714672088623 and perplexity is 121.23304343614333
At time: 60.71390914916992 and batch: 650, loss is 4.746166429519653 and perplexity is 115.1420322702519
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.995301788928462 and perplexity of 147.7175181688711
Finished 8 epochs...
Completing Train Step...
At time: 61.741081953048706 and batch: 50, loss is 4.765416803359986 and perplexity is 117.38003148481783
At time: 62.30490064620972 and batch: 100, loss is 4.741426305770874 and perplexity is 114.59753629726399
At time: 62.85453128814697 and batch: 150, loss is 4.71260272026062 and perplexity is 111.34157408096227
At time: 63.40327072143555 and batch: 200, loss is 4.699517488479614 and perplexity is 109.89413447152886
At time: 63.96081519126892 and batch: 250, loss is 4.675941162109375 and perplexity is 107.33353782854498
At time: 64.51602149009705 and batch: 300, loss is 4.730531187057495 and perplexity is 113.35575947160194
At time: 65.06478238105774 and batch: 350, loss is 4.673185596466064 and perplexity is 107.0381803446881
At time: 65.61385035514832 and batch: 400, loss is 4.722554769515991 and perplexity is 112.45518306484013
At time: 66.17032265663147 and batch: 450, loss is 4.6892084121704105 and perplexity is 108.76704705038705
At time: 66.72778129577637 and batch: 500, loss is 4.676667175292969 and perplexity is 107.41149168639045
At time: 67.27780771255493 and batch: 550, loss is 4.694253120422363 and perplexity is 109.31713141114504
At time: 67.82768726348877 and batch: 600, loss is 4.784271116256714 and perplexity is 119.61414649549589
At time: 68.3908212184906 and batch: 650, loss is 4.72761079788208 and perplexity is 113.02519945555296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.990029428519454 and perplexity of 146.94074768071906
Finished 9 epochs...
Completing Train Step...
At time: 69.42898344993591 and batch: 50, loss is 4.7451053237915035 and perplexity is 115.01991919915646
At time: 69.97880911827087 and batch: 100, loss is 4.724530096054077 and perplexity is 112.67753831210291
At time: 70.53273606300354 and batch: 150, loss is 4.695335807800293 and perplexity is 109.43555178406912
At time: 71.09748244285583 and batch: 200, loss is 4.68644383430481 and perplexity is 108.46676734416985
At time: 71.6671154499054 and batch: 250, loss is 4.660564804077149 and perplexity is 105.69576269887587
At time: 72.21586608886719 and batch: 300, loss is 4.713727359771728 and perplexity is 111.4668636539991
At time: 72.7658257484436 and batch: 350, loss is 4.65800537109375 and perplexity is 105.42558737292548
At time: 73.31604051589966 and batch: 400, loss is 4.7078321743011475 and perplexity is 110.81167893413537
At time: 73.87279033660889 and batch: 450, loss is 4.673786277770996 and perplexity is 107.10249549305487
At time: 74.42861199378967 and batch: 500, loss is 4.659969234466553 and perplexity is 105.63283225623343
At time: 74.97861313819885 and batch: 550, loss is 4.680257816314697 and perplexity is 107.79786103635135
At time: 75.52748346328735 and batch: 600, loss is 4.7691960048675535 and perplexity is 117.82447356588149
At time: 76.08424663543701 and batch: 650, loss is 4.710924482345581 and perplexity is 111.15487313794006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9761523078469665 and perplexity of 144.91571649187412
Finished 10 epochs...
Completing Train Step...
At time: 77.11462378501892 and batch: 50, loss is 4.725855026245117 and perplexity is 112.82692712748695
At time: 77.6770715713501 and batch: 100, loss is 4.705894269943237 and perplexity is 110.59714443942755
At time: 78.2377576828003 and batch: 150, loss is 4.675873584747315 and perplexity is 107.32628475627246
At time: 78.79924392700195 and batch: 200, loss is 4.666698770523071 and perplexity is 106.34608946294821
At time: 79.34579133987427 and batch: 250, loss is 4.642700300216675 and perplexity is 103.82432625059883
At time: 79.9002377986908 and batch: 300, loss is 4.694124612808228 and perplexity is 109.30308423000743
At time: 80.46056461334229 and batch: 350, loss is 4.639306879043579 and perplexity is 103.47260369252145
At time: 81.0219337940216 and batch: 400, loss is 4.690042028427124 and perplexity is 108.85775483149057
At time: 81.57116389274597 and batch: 450, loss is 4.657835531234741 and perplexity is 105.40768342647507
At time: 82.12105965614319 and batch: 500, loss is 4.642866621017456 and perplexity is 103.84159583178682
At time: 82.67081713676453 and batch: 550, loss is 4.663866720199585 and perplexity is 106.04533805845426
At time: 83.22055554389954 and batch: 600, loss is 4.752713632583618 and perplexity is 115.89836376292406
At time: 83.78005814552307 and batch: 650, loss is 4.69200029373169 and perplexity is 109.071136056245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.967715095071232 and perplexity of 143.6981753070095
Finished 11 epochs...
Completing Train Step...
At time: 84.83144760131836 and batch: 50, loss is 4.7056522560119625 and perplexity is 110.57038162833118
At time: 85.38589096069336 and batch: 100, loss is 4.68654634475708 and perplexity is 108.47788689147171
At time: 85.9473705291748 and batch: 150, loss is 4.658394947052002 and perplexity is 105.46666664838007
At time: 86.49465537071228 and batch: 200, loss is 4.649421329498291 and perplexity is 104.5244828383025
At time: 87.03936266899109 and batch: 250, loss is 4.624148368835449 and perplexity is 101.91594132183675
At time: 87.58640146255493 and batch: 300, loss is 4.6744573783874515 and perplexity is 107.17439616739907
At time: 88.13398098945618 and batch: 350, loss is 4.621812086105347 and perplexity is 101.67811479138808
At time: 88.68173742294312 and batch: 400, loss is 4.672369546890259 and perplexity is 106.95086751366762
At time: 89.23450016975403 and batch: 450, loss is 4.642324028015136 and perplexity is 103.78526739162949
At time: 89.78153896331787 and batch: 500, loss is 4.626598138809204 and perplexity is 102.16591800240006
At time: 90.32954549789429 and batch: 550, loss is 4.645565118789673 and perplexity is 104.12219056866688
At time: 90.87702965736389 and batch: 600, loss is 4.737253589630127 and perplexity is 114.12034958259997
At time: 91.42370557785034 and batch: 650, loss is 4.673843355178833 and perplexity is 107.10860880033465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.961486517214308 and perplexity of 142.80592165447672
Finished 12 epochs...
Completing Train Step...
At time: 92.44612216949463 and batch: 50, loss is 4.687179231643677 and perplexity is 108.54656285333665
At time: 93.00747966766357 and batch: 100, loss is 4.66900372505188 and perplexity is 106.59149507922068
At time: 93.56211066246033 and batch: 150, loss is 4.643052339553833 and perplexity is 103.86088293191034
At time: 94.11904859542847 and batch: 200, loss is 4.633663654327393 and perplexity is 102.89032903653914
At time: 94.66546392440796 and batch: 250, loss is 4.603932285308838 and perplexity is 99.87628652037311
At time: 95.21092343330383 and batch: 300, loss is 4.656937875747681 and perplexity is 105.3131060963494
At time: 95.7586350440979 and batch: 350, loss is 4.604391069412231 and perplexity is 99.9221186856651
At time: 96.30659031867981 and batch: 400, loss is 4.656281328201294 and perplexity is 105.2439857278265
At time: 96.86852049827576 and batch: 450, loss is 4.625034561157227 and perplexity is 102.00629847749828
At time: 97.41865992546082 and batch: 500, loss is 4.609482307434082 and perplexity is 100.43214320196371
At time: 97.96659827232361 and batch: 550, loss is 4.629769544601441 and perplexity is 102.49044191302698
At time: 98.51442694664001 and batch: 600, loss is 4.720166788101197 and perplexity is 112.18696255793635
At time: 99.0627064704895 and batch: 650, loss is 4.653881387710571 and perplexity is 104.9917092704025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9531725715188415 and perplexity of 141.62356282096343
Finished 13 epochs...
Completing Train Step...
At time: 100.09281826019287 and batch: 50, loss is 4.666530237197876 and perplexity is 106.32816811308429
At time: 100.64677119255066 and batch: 100, loss is 4.649788265228271 and perplexity is 104.56284364325816
At time: 101.19557237625122 and batch: 150, loss is 4.624894437789917 and perplexity is 101.99200601284629
At time: 101.75290846824646 and batch: 200, loss is 4.615066146850586 and perplexity is 100.99450878022208
At time: 102.31061816215515 and batch: 250, loss is 4.584318971633911 and perplexity is 97.93646691346068
At time: 102.85734558105469 and batch: 300, loss is 4.640333375930786 and perplexity is 103.5788725311078
At time: 103.40540790557861 and batch: 350, loss is 4.585498561859131 and perplexity is 98.05205997534402
At time: 103.95360088348389 and batch: 400, loss is 4.638197317123413 and perplexity is 103.35785810212481
At time: 104.50167393684387 and batch: 450, loss is 4.609584083557129 and perplexity is 100.44236531630291
At time: 105.04978394508362 and batch: 500, loss is 4.5938988780975345 and perplexity is 98.87919753155693
At time: 105.59862422943115 and batch: 550, loss is 4.615125617980957 and perplexity is 101.00051521642355
At time: 106.14819312095642 and batch: 600, loss is 4.705067873001099 and perplexity is 110.505785052212
At time: 106.69592118263245 and batch: 650, loss is 4.635331335067749 and perplexity is 103.0620604133962
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944432277305453 and perplexity of 140.39112499472597
Finished 14 epochs...
Completing Train Step...
At time: 107.693124294281 and batch: 50, loss is 4.649623422622681 and perplexity is 104.5456086522335
At time: 108.25439548492432 and batch: 100, loss is 4.632367715835572 and perplexity is 102.75707586131345
At time: 108.80305433273315 and batch: 150, loss is 4.608634614944458 and perplexity is 100.34704370265378
At time: 109.35114431381226 and batch: 200, loss is 4.599281711578369 and perplexity is 99.41288286761294
At time: 109.89818382263184 and batch: 250, loss is 4.566855516433716 and perplexity is 96.24100520965379
At time: 110.44513010978699 and batch: 300, loss is 4.623697443008423 and perplexity is 101.86999515164496
At time: 111.00299286842346 and batch: 350, loss is 4.566870222091675 and perplexity is 96.24242050736444
At time: 111.5643663406372 and batch: 400, loss is 4.623600625991822 and perplexity is 101.86013288005876
At time: 112.11332845687866 and batch: 450, loss is 4.592845249176025 and perplexity is 98.77507041460592
At time: 112.66153812408447 and batch: 500, loss is 4.577904434204101 and perplexity is 97.31026034062278
At time: 113.20992660522461 and batch: 550, loss is 4.5979663753509525 and perplexity is 99.28220746119352
At time: 113.7825870513916 and batch: 600, loss is 4.690349912643432 and perplexity is 108.89127557601525
At time: 114.34032988548279 and batch: 650, loss is 4.621386213302612 and perplexity is 101.63482206691636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.938876881318934 and perplexity of 139.61335910145903
Finished 15 epochs...
Completing Train Step...
At time: 115.38594508171082 and batch: 50, loss is 4.633434038162232 and perplexity is 102.86670646591948
At time: 115.94652533531189 and batch: 100, loss is 4.616956758499145 and perplexity is 101.18563078677941
At time: 116.49632692337036 and batch: 150, loss is 4.592392406463623 and perplexity is 98.73035097000316
At time: 117.05312824249268 and batch: 200, loss is 4.585492515563965 and perplexity is 98.05146712544007
At time: 117.61025738716125 and batch: 250, loss is 4.5499711608886715 and perplexity is 94.62967923971765
At time: 118.17238807678223 and batch: 300, loss is 4.608831977844238 and perplexity is 100.36685044067657
At time: 118.73468446731567 and batch: 350, loss is 4.553545093536377 and perplexity is 94.96848441259897
At time: 119.28773474693298 and batch: 400, loss is 4.614920263290405 and perplexity is 100.97977641635352
At time: 119.83571219444275 and batch: 450, loss is 4.578925857543945 and perplexity is 97.40970609120028
At time: 120.38305163383484 and batch: 500, loss is 4.562912635803222 and perplexity is 95.86228552824248
At time: 120.94429755210876 and batch: 550, loss is 4.582933540344238 and perplexity is 97.80087661500772
At time: 121.4961588382721 and batch: 600, loss is 4.676770486831665 and perplexity is 107.42258910610612
At time: 122.04348063468933 and batch: 650, loss is 4.605011386871338 and perplexity is 99.98412134911588
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.929798201018689 and perplexity of 138.35159031274918
Finished 16 epochs...
Completing Train Step...
At time: 123.04368090629578 and batch: 50, loss is 4.617770462036133 and perplexity is 101.26799939971401
At time: 123.61865448951721 and batch: 100, loss is 4.602745056152344 and perplexity is 99.75778084159187
At time: 124.1701455116272 and batch: 150, loss is 4.577072687149048 and perplexity is 97.22935646860078
At time: 124.71722078323364 and batch: 200, loss is 4.571466732025146 and perplexity is 96.68581800908711
At time: 125.26296830177307 and batch: 250, loss is 4.5349749660491945 and perplexity is 93.22118158073908
At time: 125.81399273872375 and batch: 300, loss is 4.594338245391846 and perplexity is 98.92265136243707
At time: 126.36112880706787 and batch: 350, loss is 4.5409066867828365 and perplexity is 93.77578685196704
At time: 126.90890312194824 and batch: 400, loss is 4.602584314346314 and perplexity is 99.74174688443198
At time: 127.45654034614563 and batch: 450, loss is 4.567819147109986 and perplexity is 96.33379069287791
At time: 128.01754665374756 and batch: 500, loss is 4.550381374359131 and perplexity is 94.6685055718448
At time: 128.57261729240417 and batch: 550, loss is 4.568875055313111 and perplexity is 96.43556405491663
At time: 129.12354922294617 and batch: 600, loss is 4.661027593612671 and perplexity is 105.74468891219847
At time: 129.67128586769104 and batch: 650, loss is 4.590382509231567 and perplexity is 98.53211239718318
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9261803720511645 and perplexity of 137.8519622510062
Finished 17 epochs...
Completing Train Step...
At time: 130.7015688419342 and batch: 50, loss is 4.604433708190918 and perplexity is 99.92637933360339
At time: 131.25060558319092 and batch: 100, loss is 4.586631336212158 and perplexity is 98.16319376701635
At time: 131.7986605167389 and batch: 150, loss is 4.564489021301269 and perplexity is 96.01352061604027
At time: 132.3456573486328 and batch: 200, loss is 4.557234649658203 and perplexity is 95.31952315599743
At time: 132.891508102417 and batch: 250, loss is 4.51963062286377 and perplexity is 91.8016822722004
At time: 133.43929934501648 and batch: 300, loss is 4.581168575286865 and perplexity is 97.62841372545859
At time: 133.98612260818481 and batch: 350, loss is 4.5276326656341555 and perplexity is 92.5392302698529
At time: 134.53189182281494 and batch: 400, loss is 4.588439102172852 and perplexity is 98.34081034353069
At time: 135.09023475646973 and batch: 450, loss is 4.553495960235596 and perplexity is 94.96381841211857
At time: 135.66545963287354 and batch: 500, loss is 4.535524072647095 and perplexity is 93.2723840031166
At time: 136.22136926651 and batch: 550, loss is 4.553921966552735 and perplexity is 95.00428221696805
At time: 136.76707553863525 and batch: 600, loss is 4.647825355529785 and perplexity is 104.35779753270617
At time: 137.3211908340454 and batch: 650, loss is 4.576336565017701 and perplexity is 97.15781012414212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.919395596373315 and perplexity of 136.9198333289368
Finished 18 epochs...
Completing Train Step...
At time: 138.36190748214722 and batch: 50, loss is 4.588502588272095 and perplexity is 98.3470538161606
At time: 138.92363953590393 and batch: 100, loss is 4.5716651344299315 and perplexity is 96.70500261096137
At time: 139.4713888168335 and batch: 150, loss is 4.550140247344971 and perplexity is 94.64568118965943
At time: 140.0162332057953 and batch: 200, loss is 4.5448394393920895 and perplexity is 94.1453099676222
At time: 140.559894323349 and batch: 250, loss is 4.503891143798828 and perplexity is 90.36808326223137
At time: 141.10538601875305 and batch: 300, loss is 4.565394601821899 and perplexity is 96.10050797110974
At time: 141.6525559425354 and batch: 350, loss is 4.509745559692383 and perplexity is 90.89868727794237
At time: 142.21790885925293 and batch: 400, loss is 4.573040208816528 and perplexity is 96.83807065136709
At time: 142.76284623146057 and batch: 450, loss is 4.537795906066894 and perplexity is 93.48452420459697
At time: 143.30724239349365 and batch: 500, loss is 4.5184906959533695 and perplexity is 91.69709468658536
At time: 143.85641050338745 and batch: 550, loss is 4.538565149307251 and perplexity is 93.55646420905295
At time: 144.40465235710144 and batch: 600, loss is 4.633613548278809 and perplexity is 102.88517373787052
At time: 144.9524326324463 and batch: 650, loss is 4.560100154876709 and perplexity is 95.593053461119
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.914988798253677 and perplexity of 136.3177827967698
Finished 19 epochs...
Completing Train Step...
At time: 145.96806526184082 and batch: 50, loss is 4.572519016265869 and perplexity is 96.78761252066654
At time: 146.51580381393433 and batch: 100, loss is 4.557969760894776 and perplexity is 95.38961936963469
At time: 147.06403946876526 and batch: 150, loss is 4.5382408618927 and perplexity is 93.52612994393611
At time: 147.61057496070862 and batch: 200, loss is 4.531819543838501 and perplexity is 92.92749299329851
At time: 148.15730047225952 and batch: 250, loss is 4.489143791198731 and perplexity is 89.04517197017105
At time: 148.70284128189087 and batch: 300, loss is 4.549465684890747 and perplexity is 94.5818582953587
At time: 149.2491579055786 and batch: 350, loss is 4.497702531814575 and perplexity is 89.81055719479792
At time: 149.79460620880127 and batch: 400, loss is 4.558935375213623 and perplexity is 95.48177343745066
At time: 150.33988976478577 and batch: 450, loss is 4.524785604476929 and perplexity is 92.27614011642231
At time: 150.88535904884338 and batch: 500, loss is 4.505827951431274 and perplexity is 90.54327846055544
At time: 151.43139004707336 and batch: 550, loss is 4.524377565383912 and perplexity is 92.23849552465694
At time: 151.97722959518433 and batch: 600, loss is 4.621248750686646 and perplexity is 101.62085203860207
At time: 152.52211713790894 and batch: 650, loss is 4.546884803771973 and perplexity is 94.33806849467494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.909975238874847 and perplexity of 135.6360558655302
Finished 20 epochs...
Completing Train Step...
At time: 153.53317594528198 and batch: 50, loss is 4.557419948577881 and perplexity is 95.33718739719428
At time: 154.10188508033752 and batch: 100, loss is 4.545288181304931 and perplexity is 94.18756639450797
At time: 154.64751529693604 and batch: 150, loss is 4.526429977416992 and perplexity is 92.42800132825805
At time: 155.1925768852234 and batch: 200, loss is 4.5190638828277585 and perplexity is 91.74966932378588
At time: 155.73723578453064 and batch: 250, loss is 4.4738902759552 and perplexity is 87.69722666965971
At time: 156.2950291633606 and batch: 300, loss is 4.534647035598755 and perplexity is 93.19061652855237
At time: 156.84113955497742 and batch: 350, loss is 4.484117479324341 and perplexity is 88.59872609295718
At time: 157.3869423866272 and batch: 400, loss is 4.544885444641113 and perplexity is 94.14964124568166
At time: 157.93146896362305 and batch: 450, loss is 4.510168361663818 and perplexity is 90.9371275478627
At time: 158.47861671447754 and batch: 500, loss is 4.49318868637085 and perplexity is 89.40607978143247
At time: 159.02384853363037 and batch: 550, loss is 4.509378690719604 and perplexity is 90.86534548632217
At time: 159.57211923599243 and batch: 600, loss is 4.608044834136963 and perplexity is 100.28787839118661
At time: 160.11786818504333 and batch: 650, loss is 4.5313890933990475 and perplexity is 92.88750092102228
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.904283112170649 and perplexity of 134.86619141154705
Finished 21 epochs...
Completing Train Step...
At time: 161.11732530593872 and batch: 50, loss is 4.544026699066162 and perplexity is 94.06882536298595
At time: 161.6697657108307 and batch: 100, loss is 4.532581834793091 and perplexity is 92.99835778700411
At time: 162.2326900959015 and batch: 150, loss is 4.51095139503479 and perplexity is 91.00836223932356
At time: 162.7840371131897 and batch: 200, loss is 4.503781824111939 and perplexity is 90.35820479162983
At time: 163.32936573028564 and batch: 250, loss is 4.457813987731933 and perplexity is 86.29865283666705
At time: 163.87706303596497 and batch: 300, loss is 4.519471645355225 and perplexity is 91.78708902950194
At time: 164.42654871940613 and batch: 350, loss is 4.469110946655274 and perplexity is 87.27909274050185
At time: 164.97241806983948 and batch: 400, loss is 4.530657110214233 and perplexity is 92.81953371073091
At time: 165.52344298362732 and batch: 450, loss is 4.496691560745239 and perplexity is 89.71980720029187
At time: 166.07857251167297 and batch: 500, loss is 4.479757347106934 and perplexity is 88.21326487449545
At time: 166.6251482963562 and batch: 550, loss is 4.497092628479004 and perplexity is 89.75579813695882
At time: 167.1718032360077 and batch: 600, loss is 4.5932418155670165 and perplexity is 98.81424905575287
At time: 167.71797275543213 and batch: 650, loss is 4.518600158691406 and perplexity is 91.70713265102125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.898553287281709 and perplexity of 134.09564141773953
Finished 22 epochs...
Completing Train Step...
At time: 168.70747184753418 and batch: 50, loss is 4.528553562164307 and perplexity is 92.62448857692556
At time: 169.27243614196777 and batch: 100, loss is 4.519118757247925 and perplexity is 91.75470417183138
At time: 169.8335428237915 and batch: 150, loss is 4.4969411277771 and perplexity is 89.74220110054735
At time: 170.39297080039978 and batch: 200, loss is 4.490646257400512 and perplexity is 89.17905988733663
At time: 170.93916821479797 and batch: 250, loss is 4.442300958633423 and perplexity is 84.97022989667016
At time: 171.49903845787048 and batch: 300, loss is 4.502905960083008 and perplexity is 90.27909793881464
At time: 172.04972887039185 and batch: 350, loss is 4.454742889404297 and perplexity is 86.0340277410948
At time: 172.59830808639526 and batch: 400, loss is 4.518834676742554 and perplexity is 91.72864215113135
At time: 173.14597010612488 and batch: 450, loss is 4.482771377563477 and perplexity is 88.47954342574876
At time: 173.69726943969727 and batch: 500, loss is 4.468122997283936 and perplexity is 87.19290799579008
At time: 174.24669742584229 and batch: 550, loss is 4.482881650924683 and perplexity is 88.48930090038515
At time: 174.79718732833862 and batch: 600, loss is 4.579403810501098 and perplexity is 97.45627447614415
At time: 175.35981631278992 and batch: 650, loss is 4.50473690032959 and perplexity is 90.4445449882671
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.892689424402573 and perplexity of 133.31162389592578
Finished 23 epochs...
Completing Train Step...
At time: 176.42590713500977 and batch: 50, loss is 4.5155867576599125 and perplexity is 91.43119824221523
At time: 176.97605776786804 and batch: 100, loss is 4.510036907196045 and perplexity is 90.92517424183501
At time: 177.5285906791687 and batch: 150, loss is 4.483767328262329 and perplexity is 88.5677085856463
At time: 178.08644366264343 and batch: 200, loss is 4.476700649261475 and perplexity is 87.9440352642242
At time: 178.64846348762512 and batch: 250, loss is 4.426822004318237 and perplexity is 83.66510662037622
At time: 179.1989438533783 and batch: 300, loss is 4.49018253326416 and perplexity is 89.13771499186353
At time: 179.76265716552734 and batch: 350, loss is 4.438728694915771 and perplexity is 84.6672353377756
At time: 180.31566190719604 and batch: 400, loss is 4.504007759094239 and perplexity is 90.37862217744407
At time: 180.8718466758728 and batch: 450, loss is 4.469459743499756 and perplexity is 87.30954072241028
At time: 181.4256775379181 and batch: 500, loss is 4.453777122497558 and perplexity is 85.9509790335507
At time: 181.98816633224487 and batch: 550, loss is 4.469441404342652 and perplexity is 87.30793955370838
At time: 182.54910516738892 and batch: 600, loss is 4.565937767028808 and perplexity is 96.15272060216452
At time: 183.10917949676514 and batch: 650, loss is 4.48983470916748 and perplexity is 89.1067161380531
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.886940750421262 and perplexity of 132.54745740869592
Finished 24 epochs...
Completing Train Step...
At time: 184.1689293384552 and batch: 50, loss is 4.499138603210449 and perplexity is 89.93962421958285
At time: 184.74026012420654 and batch: 100, loss is 4.495402307510376 and perplexity is 89.60421018155463
At time: 185.31254720687866 and batch: 150, loss is 4.470132160186767 and perplexity is 87.36826885720347
At time: 185.86872124671936 and batch: 200, loss is 4.464302225112915 and perplexity is 86.86039938374012
At time: 186.42022228240967 and batch: 250, loss is 4.413165712356568 and perplexity is 82.53031763811174
At time: 186.97489142417908 and batch: 300, loss is 4.4750816822052 and perplexity is 87.80177195921094
At time: 187.5248486995697 and batch: 350, loss is 4.42545262336731 and perplexity is 83.55061562589759
At time: 188.08040022850037 and batch: 400, loss is 4.491884336471558 and perplexity is 89.28953899177993
At time: 188.64072585105896 and batch: 450, loss is 4.4609074211120605 and perplexity is 86.56602530625858
At time: 189.2001268863678 and batch: 500, loss is 4.439608774185181 and perplexity is 84.7417820150756
At time: 189.75537276268005 and batch: 550, loss is 4.457913179397583 and perplexity is 86.30721336834488
At time: 190.30465412139893 and batch: 600, loss is 4.554370737075805 and perplexity is 95.04692690651663
At time: 190.8558967113495 and batch: 650, loss is 4.478283262252807 and perplexity is 88.08332683019128
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.884057736864277 and perplexity of 132.1658716148554
Finished 25 epochs...
Completing Train Step...
At time: 191.9303584098816 and batch: 50, loss is 4.486919183731079 and perplexity is 88.84730158921441
At time: 192.4900906085968 and batch: 100, loss is 4.48451644897461 and perplexity is 88.63408134808867
At time: 193.03992867469788 and batch: 150, loss is 4.460447177886963 and perplexity is 86.52619304655474
At time: 193.58929753303528 and batch: 200, loss is 4.452968044281006 and perplexity is 85.88146609321815
At time: 194.13781332969666 and batch: 250, loss is 4.400672960281372 and perplexity is 81.50570031215392
At time: 194.68793177604675 and batch: 300, loss is 4.4607202911376955 and perplexity is 86.54982772373602
At time: 195.2453818321228 and batch: 350, loss is 4.414144735336304 and perplexity is 82.61115628059183
At time: 195.81017398834229 and batch: 400, loss is 4.481175060272217 and perplexity is 88.33841467375801
At time: 196.36496877670288 and batch: 450, loss is 4.450420312881469 and perplexity is 85.66294167437826
At time: 196.9146077632904 and batch: 500, loss is 4.430907850265503 and perplexity is 84.00764866766964
At time: 197.47901678085327 and batch: 550, loss is 4.447290649414063 and perplexity is 85.3952645834025
At time: 198.0317234992981 and batch: 600, loss is 4.545511627197266 and perplexity is 94.20861457080451
At time: 198.60545992851257 and batch: 650, loss is 4.466848258972168 and perplexity is 87.08183066775754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.881531060910692 and perplexity of 131.83235280956887
Finished 26 epochs...
Completing Train Step...
At time: 199.63389229774475 and batch: 50, loss is 4.4752392578125 and perplexity is 87.81560846686887
At time: 200.21140909194946 and batch: 100, loss is 4.475332899093628 and perplexity is 87.82383201797461
At time: 200.77500891685486 and batch: 150, loss is 4.451315746307373 and perplexity is 85.73968148828449
At time: 201.3359889984131 and batch: 200, loss is 4.442065572738647 and perplexity is 84.95023145684429
At time: 201.88390374183655 and batch: 250, loss is 4.390036926269532 and perplexity is 80.6433967853027
At time: 202.43392062187195 and batch: 300, loss is 4.450085830688477 and perplexity is 85.63429373716833
At time: 202.98395538330078 and batch: 350, loss is 4.405082206726075 and perplexity is 81.86587249190742
At time: 203.53454613685608 and batch: 400, loss is 4.470893974304199 and perplexity is 87.4348525968307
At time: 204.08360314369202 and batch: 450, loss is 4.440499801635742 and perplexity is 84.81732291857197
At time: 204.63748335838318 and batch: 500, loss is 4.421761665344238 and perplexity is 83.24280222344382
At time: 205.19903945922852 and batch: 550, loss is 4.436834154129028 and perplexity is 84.50698165862606
At time: 205.7484908103943 and batch: 600, loss is 4.535739440917968 and perplexity is 93.29247407848432
At time: 206.29651165008545 and batch: 650, loss is 4.455899133682251 and perplexity is 86.1335616250327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.880029117359834 and perplexity of 131.63449667903595
Finished 27 epochs...
Completing Train Step...
At time: 207.31842136383057 and batch: 50, loss is 4.464580373764038 and perplexity is 86.8845628470263
At time: 207.86395072937012 and batch: 100, loss is 4.463393926620483 and perplexity is 86.78154003326405
At time: 208.41048669815063 and batch: 150, loss is 4.44154767036438 and perplexity is 84.90624692110913
At time: 208.95749974250793 and batch: 200, loss is 4.433412685394287 and perplexity is 84.21833773767212
At time: 209.50353026390076 and batch: 250, loss is 4.38006175994873 and perplexity is 79.84296435020022
At time: 210.0590054988861 and batch: 300, loss is 4.439198989868164 and perplexity is 84.70706327589299
At time: 210.61687898635864 and batch: 350, loss is 4.396074733734131 and perplexity is 81.13177898355637
At time: 211.16659426689148 and batch: 400, loss is 4.461266641616821 and perplexity is 86.59712718344579
At time: 211.71564865112305 and batch: 450, loss is 4.433182983398438 and perplexity is 84.19899483904294
At time: 212.26496052742004 and batch: 500, loss is 4.414130334854126 and perplexity is 82.60996664867373
At time: 212.83300137519836 and batch: 550, loss is 4.4291996574401855 and perplexity is 83.86426989909965
At time: 213.38250517845154 and batch: 600, loss is 4.528105554580688 and perplexity is 92.58300139759369
At time: 213.9329869747162 and batch: 650, loss is 4.445262403488159 and perplexity is 85.22223751595834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.876662609623928 and perplexity of 131.19209322280963
Finished 28 epochs...
Completing Train Step...
At time: 214.93676924705505 and batch: 50, loss is 4.453685655593872 and perplexity is 85.94311772316
At time: 215.49966311454773 and batch: 100, loss is 4.453625583648682 and perplexity is 85.93795510796842
At time: 216.04914712905884 and batch: 150, loss is 4.43079704284668 and perplexity is 83.99834051267526
At time: 216.598406791687 and batch: 200, loss is 4.42353777885437 and perplexity is 83.39078226486825
At time: 217.1592197418213 and batch: 250, loss is 4.37063009262085 and perplexity is 79.09345220274093
At time: 217.71190524101257 and batch: 300, loss is 4.431851186752319 and perplexity is 84.08693353810557
At time: 218.2714786529541 and batch: 350, loss is 4.386887817382813 and perplexity is 80.38984139413867
At time: 218.82786345481873 and batch: 400, loss is 4.452977027893066 and perplexity is 85.88223762245826
At time: 219.37724804878235 and batch: 450, loss is 4.424656200408935 and perplexity is 83.48410048803407
At time: 219.9269893169403 and batch: 500, loss is 4.40412281036377 and perplexity is 81.78736833596415
At time: 220.4763581752777 and batch: 550, loss is 4.421096506118775 and perplexity is 83.18745091634851
At time: 221.02579545974731 and batch: 600, loss is 4.521218252182007 and perplexity is 91.94754507182974
At time: 221.5746955871582 and batch: 650, loss is 4.437580299377442 and perplexity is 84.57005967120163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.875138825061274 and perplexity of 130.99233696781482
Finished 29 epochs...
Completing Train Step...
At time: 222.59326839447021 and batch: 50, loss is 4.443263778686523 and perplexity is 85.05208033522993
At time: 223.14183926582336 and batch: 100, loss is 4.444836559295655 and perplexity is 85.18595384718309
At time: 223.69168829917908 and batch: 150, loss is 4.421765117645264 and perplexity is 83.24308960315136
At time: 224.24032258987427 and batch: 200, loss is 4.4155561637878415 and perplexity is 82.72783834180065
At time: 224.7876410484314 and batch: 250, loss is 4.361104097366333 and perplexity is 78.34358563559334
At time: 225.33681917190552 and batch: 300, loss is 4.422647094726562 and perplexity is 83.31654048659433
At time: 225.8858551979065 and batch: 350, loss is 4.378334112167359 and perplexity is 79.70514291770891
At time: 226.44366693496704 and batch: 400, loss is 4.444390029907226 and perplexity is 85.14792430659465
At time: 227.0128951072693 and batch: 450, loss is 4.415506420135498 and perplexity is 82.72372325932149
At time: 227.56154942512512 and batch: 500, loss is 4.395032777786255 and perplexity is 81.04728726984868
At time: 228.11132550239563 and batch: 550, loss is 4.41164077758789 and perplexity is 82.40456019759209
At time: 228.65995121002197 and batch: 600, loss is 4.514449529647827 and perplexity is 91.32727922340499
At time: 229.2079496383667 and batch: 650, loss is 4.429386768341065 and perplexity is 83.87996328634817
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.8757467830882355 and perplexity of 131.0719990236843
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 230.2087483406067 and batch: 50, loss is 4.431562929153443 and perplexity is 84.06269833370568
At time: 230.77047514915466 and batch: 100, loss is 4.416148357391357 and perplexity is 82.77684374741105
At time: 231.31902503967285 and batch: 150, loss is 4.3831679630279545 and perplexity is 80.09135839337101
At time: 231.86817073822021 and batch: 200, loss is 4.362104301452637 and perplexity is 78.42198441093358
At time: 232.41650772094727 and batch: 250, loss is 4.305760660171509 and perplexity is 74.12557838878107
At time: 232.9786024093628 and batch: 300, loss is 4.365399847030639 and perplexity is 78.6808539587838
At time: 233.52720427513123 and batch: 350, loss is 4.3126801586151124 and perplexity is 74.64026885943866
At time: 234.0765085220337 and batch: 400, loss is 4.3648544216156 and perplexity is 78.63795112256992
At time: 234.62560391426086 and batch: 450, loss is 4.323855628967285 and perplexity is 75.47908733660208
At time: 235.17358446121216 and batch: 500, loss is 4.28977593421936 and perplexity is 72.95012104242917
At time: 235.7225501537323 and batch: 550, loss is 4.303142318725586 and perplexity is 73.93174618486736
At time: 236.27167510986328 and batch: 600, loss is 4.404619426727295 and perplexity is 81.82799536859903
At time: 236.82081580162048 and batch: 650, loss is 4.344667882919311 and perplexity is 77.06643814809304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.825842763863358 and perplexity of 124.69150961287122
Finished 31 epochs...
Completing Train Step...
At time: 237.83494567871094 and batch: 50, loss is 4.389904775619507 and perplexity is 80.63274041213614
At time: 238.38546514511108 and batch: 100, loss is 4.381275234222412 and perplexity is 79.93991054233244
At time: 238.9342439174652 and batch: 150, loss is 4.355765810012818 and perplexity is 77.92647937024996
At time: 239.48270392417908 and batch: 200, loss is 4.338534908294678 and perplexity is 76.59523804461016
At time: 240.0309648513794 and batch: 250, loss is 4.28552812576294 and perplexity is 72.64090012195058
At time: 240.579576253891 and batch: 300, loss is 4.347472324371338 and perplexity is 77.28286980489956
At time: 241.1402668952942 and batch: 350, loss is 4.29835916519165 and perplexity is 73.5789636708934
At time: 241.688330411911 and batch: 400, loss is 4.3523762416839595 and perplexity is 77.6627893938584
At time: 242.2374906539917 and batch: 450, loss is 4.315997076034546 and perplexity is 74.88825551567851
At time: 242.78566598892212 and batch: 500, loss is 4.2891912841796875 and perplexity is 72.9074832165854
At time: 243.3343575000763 and batch: 550, loss is 4.306838569641113 and perplexity is 74.20552212998358
At time: 243.88274669647217 and batch: 600, loss is 4.410161123275757 and perplexity is 82.28272009760046
At time: 244.43154454231262 and batch: 650, loss is 4.342593746185303 and perplexity is 76.90675747479209
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.820535397997089 and perplexity of 124.03147921166726
Finished 32 epochs...
Completing Train Step...
At time: 245.43240404129028 and batch: 50, loss is 4.377898378372192 and perplexity is 79.67042025875847
At time: 245.99379682540894 and batch: 100, loss is 4.370569677352905 and perplexity is 79.08867389497625
At time: 246.54257917404175 and batch: 150, loss is 4.344965963363648 and perplexity is 77.08941357031156
At time: 247.0916554927826 and batch: 200, loss is 4.328704710006714 and perplexity is 75.84598037600581
At time: 247.6404995918274 and batch: 250, loss is 4.278083038330078 and perplexity is 72.1020905005304
At time: 248.18894219398499 and batch: 300, loss is 4.340921373367309 and perplexity is 76.7782481917387
At time: 248.7378170490265 and batch: 350, loss is 4.292177305221558 and perplexity is 73.12551185262632
At time: 249.28733611106873 and batch: 400, loss is 4.347398414611816 and perplexity is 77.27715805765669
At time: 249.83624529838562 and batch: 450, loss is 4.3131028842926025 and perplexity is 74.67182788759612
At time: 250.38536143302917 and batch: 500, loss is 4.290038976669312 and perplexity is 72.9693125449717
At time: 250.94303846359253 and batch: 550, loss is 4.3090923881530765 and perplexity is 74.37295652193978
At time: 251.49962759017944 and batch: 600, loss is 4.411245956420898 and perplexity is 82.3720315548903
At time: 252.04851770401 and batch: 650, loss is 4.339599351882935 and perplexity is 76.67681276277835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.818917367972579 and perplexity of 123.83095482530541
Finished 33 epochs...
Completing Train Step...
At time: 253.06804490089417 and batch: 50, loss is 4.370359077453613 and perplexity is 79.0720195819787
At time: 253.62915420532227 and batch: 100, loss is 4.363745841979981 and perplexity is 78.55082299452579
At time: 254.17802333831787 and batch: 150, loss is 4.337666578292847 and perplexity is 76.52875696935196
At time: 254.7276747226715 and batch: 200, loss is 4.322385187149048 and perplexity is 75.36818129060732
At time: 255.28966808319092 and batch: 250, loss is 4.273789491653442 and perplexity is 71.79318044390799
At time: 255.84934258460999 and batch: 300, loss is 4.337012977600097 and perplexity is 76.47875406352854
At time: 256.4022388458252 and batch: 350, loss is 4.2883165836334225 and perplexity is 72.84373888385461
At time: 256.95161986351013 and batch: 400, loss is 4.344074993133545 and perplexity is 77.02075978654472
At time: 257.505384683609 and batch: 450, loss is 4.311190099716186 and perplexity is 74.5291332824468
At time: 258.0672745704651 and batch: 500, loss is 4.290460357666015 and perplexity is 73.00006690581723
At time: 258.6232268810272 and batch: 550, loss is 4.3098814582824705 and perplexity is 74.4316651599361
At time: 259.1785218715668 and batch: 600, loss is 4.411122360229492 and perplexity is 82.36185131464417
At time: 259.72649908065796 and batch: 650, loss is 4.33652961730957 and perplexity is 76.44179620344579
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.81788844688266 and perplexity of 123.70360807041662
Finished 34 epochs...
Completing Train Step...
At time: 260.73049306869507 and batch: 50, loss is 4.364830455780029 and perplexity is 78.63606652094678
At time: 261.3073878288269 and batch: 100, loss is 4.358686256408691 and perplexity is 78.15439211756156
At time: 261.85931611061096 and batch: 150, loss is 4.3317797565460205 and perplexity is 76.07956925975938
At time: 262.4213967323303 and batch: 200, loss is 4.31758337020874 and perplexity is 75.00714459071179
At time: 262.97423911094666 and batch: 250, loss is 4.27025987625122 and perplexity is 71.54022480910496
At time: 263.5228488445282 and batch: 300, loss is 4.333759660720825 and perplexity is 76.23034873175229
At time: 264.0735640525818 and batch: 350, loss is 4.284949398040771 and perplexity is 72.59887298159532
At time: 264.62237191200256 and batch: 400, loss is 4.3413033866882325 and perplexity is 76.80758410830379
At time: 265.17107820510864 and batch: 450, loss is 4.309472436904907 and perplexity is 74.40122724302176
At time: 265.7198648452759 and batch: 500, loss is 4.290291185379028 and perplexity is 72.98771836209372
At time: 266.2689905166626 and batch: 550, loss is 4.309862651824951 and perplexity is 74.4302653771497
At time: 266.8188829421997 and batch: 600, loss is 4.410086975097657 and perplexity is 82.2766192100037
At time: 267.3684287071228 and batch: 650, loss is 4.3337929248809814 and perplexity is 76.23288451245632
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.816887350643382 and perplexity of 123.57983082039942
Finished 35 epochs...
Completing Train Step...
At time: 268.3896508216858 and batch: 50, loss is 4.360360889434815 and perplexity is 78.28538169286045
At time: 268.9407091140747 and batch: 100, loss is 4.354172286987304 and perplexity is 77.80240061850648
At time: 269.5025758743286 and batch: 150, loss is 4.326989612579346 and perplexity is 75.71600861915957
At time: 270.0518398284912 and batch: 200, loss is 4.313387403488159 and perplexity is 74.69307647867012
At time: 270.6015408039093 and batch: 250, loss is 4.267312784194946 and perplexity is 71.32969955189634
At time: 271.1506538391113 and batch: 300, loss is 4.330900583267212 and perplexity is 76.01271152947436
At time: 271.70063185691833 and batch: 350, loss is 4.282130155563355 and perplexity is 72.39448739670794
At time: 272.2501230239868 and batch: 400, loss is 4.338993482589721 and perplexity is 76.63037070675237
At time: 272.79950165748596 and batch: 450, loss is 4.30782793045044 and perplexity is 74.27897449486873
At time: 273.3487460613251 and batch: 500, loss is 4.289676961898803 and perplexity is 72.94290135694511
At time: 273.8987805843353 and batch: 550, loss is 4.309343490600586 and perplexity is 74.39163409824539
At time: 274.44882321357727 and batch: 600, loss is 4.4088380241394045 and perplexity is 82.17392389165711
At time: 274.99870252609253 and batch: 650, loss is 4.3312412548065184 and perplexity is 76.03861130832624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.815938912185968 and perplexity of 123.46267852094354
Finished 36 epochs...
Completing Train Step...
At time: 276.0064694881439 and batch: 50, loss is 4.3562680435180665 and perplexity is 77.9656264888102
At time: 276.56863141059875 and batch: 100, loss is 4.349976720809937 and perplexity is 77.47665931039006
At time: 277.1175479888916 and batch: 150, loss is 4.322816820144653 and perplexity is 75.40071970629283
At time: 277.666867017746 and batch: 200, loss is 4.3097776222229 and perplexity is 74.42393687036315
At time: 278.21766090393066 and batch: 250, loss is 4.264730110168457 and perplexity is 71.14571587674334
At time: 278.766432762146 and batch: 300, loss is 4.328478994369507 and perplexity is 75.8288626841537
At time: 279.31479024887085 and batch: 350, loss is 4.279852180480957 and perplexity is 72.22976224943801
At time: 279.8638572692871 and batch: 400, loss is 4.336695423126221 and perplexity is 76.45447174870208
At time: 280.41388416290283 and batch: 450, loss is 4.306187839508056 and perplexity is 74.15725006844032
At time: 280.9768018722534 and batch: 500, loss is 4.2888274669647215 and perplexity is 72.88096304363162
At time: 281.5397047996521 and batch: 550, loss is 4.308580417633056 and perplexity is 74.33488950614023
At time: 282.09359788894653 and batch: 600, loss is 4.407346324920654 and perplexity is 82.05143649347728
At time: 282.6447114944458 and batch: 650, loss is 4.32871958732605 and perplexity is 75.8471087692699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.815162808287377 and perplexity of 123.36689582828143
Finished 37 epochs...
Completing Train Step...
At time: 283.6934931278229 and batch: 50, loss is 4.352541608810425 and perplexity is 77.67563332812671
At time: 284.2516391277313 and batch: 100, loss is 4.346030797958374 and perplexity is 77.1715447650766
At time: 284.79949283599854 and batch: 150, loss is 4.31896556854248 and perplexity is 75.1108910235399
At time: 285.3631684780121 and batch: 200, loss is 4.306442670822143 and perplexity is 74.17615006597765
At time: 285.9159495830536 and batch: 250, loss is 4.2624493503570555 and perplexity is 70.98363449183319
At time: 286.4750425815582 and batch: 300, loss is 4.3261455631256105 and perplexity is 75.6521275266387
At time: 287.02926445007324 and batch: 350, loss is 4.277423486709595 and perplexity is 72.05455112895865
At time: 287.5775771141052 and batch: 400, loss is 4.334579496383667 and perplexity is 76.29287071560758
At time: 288.1258177757263 and batch: 450, loss is 4.304607105255127 and perplexity is 74.0401197634863
At time: 288.6737036705017 and batch: 500, loss is 4.28772126197815 and perplexity is 72.80038633428218
At time: 289.22259879112244 and batch: 550, loss is 4.307508945465088 and perplexity is 74.25528439587134
At time: 289.7761240005493 and batch: 600, loss is 4.405688781738281 and perplexity is 81.91554534814713
At time: 290.33580803871155 and batch: 650, loss is 4.3262234306335445 and perplexity is 75.65801859863767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.814407647824755 and perplexity of 123.27376919335738
Finished 38 epochs...
Completing Train Step...
At time: 291.33862376213074 and batch: 50, loss is 4.349086790084839 and perplexity is 77.40774112154716
At time: 291.8999457359314 and batch: 100, loss is 4.342407693862915 and perplexity is 76.8924501249557
At time: 292.44830536842346 and batch: 150, loss is 4.315404586791992 and perplexity is 74.84389817181396
At time: 292.99758434295654 and batch: 200, loss is 4.3033562183380125 and perplexity is 73.94756184814416
At time: 293.5457751750946 and batch: 250, loss is 4.2603099822998045 and perplexity is 70.83193669817958
At time: 294.0944995880127 and batch: 300, loss is 4.324173173904419 and perplexity is 75.50305914450696
At time: 294.6446633338928 and batch: 350, loss is 4.2754706859588625 and perplexity is 71.91398024555508
At time: 295.1936180591583 and batch: 400, loss is 4.332626829147339 and perplexity is 76.14404148087036
At time: 295.7425413131714 and batch: 450, loss is 4.303063774108887 and perplexity is 73.9259394722474
At time: 296.2951912879944 and batch: 500, loss is 4.286724653244018 and perplexity is 72.72786897513225
At time: 296.85730481147766 and batch: 550, loss is 4.306363716125488 and perplexity is 74.17029374174533
At time: 297.42072010040283 and batch: 600, loss is 4.40409252166748 and perplexity is 81.78489114071998
At time: 297.9686677455902 and batch: 650, loss is 4.323938674926758 and perplexity is 75.48535583011301
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.813686595243566 and perplexity of 123.18491436224006
Finished 39 epochs...
Completing Train Step...
At time: 298.9893295764923 and batch: 50, loss is 4.346105089187622 and perplexity is 77.17727814696755
At time: 299.53835797309875 and batch: 100, loss is 4.3392142391204835 and perplexity is 76.64728922890903
At time: 300.08762979507446 and batch: 150, loss is 4.312262392044067 and perplexity is 74.60909316279071
At time: 300.6366205215454 and batch: 200, loss is 4.30061071395874 and perplexity is 73.74481693912546
At time: 301.18538522720337 and batch: 250, loss is 4.258340854644775 and perplexity is 70.69259680680844
At time: 301.7342178821564 and batch: 300, loss is 4.321929779052734 and perplexity is 75.33386582500665
At time: 302.28263092041016 and batch: 350, loss is 4.27332857131958 and perplexity is 71.76009713218002
At time: 302.8313682079315 and batch: 400, loss is 4.3306464576721195 and perplexity is 75.99339720815799
At time: 303.38044333457947 and batch: 450, loss is 4.301400928497315 and perplexity is 73.80311419625177
At time: 303.92955803871155 and batch: 500, loss is 4.285490436553955 and perplexity is 72.63816239547674
At time: 304.47862243652344 and batch: 550, loss is 4.304997968673706 and perplexity is 74.06906499425645
At time: 305.02783942222595 and batch: 600, loss is 4.402339353561401 and perplexity is 81.64163409155059
At time: 305.5766878128052 and batch: 650, loss is 4.321599645614624 and perplexity is 75.30899970167094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.812899421243107 and perplexity of 123.08798455572224
Finished 40 epochs...
Completing Train Step...
At time: 306.5792541503906 and batch: 50, loss is 4.343004016876221 and perplexity is 76.93831653675394
At time: 307.1408050060272 and batch: 100, loss is 4.335918769836426 and perplexity is 76.39511618402977
At time: 307.6899251937866 and batch: 150, loss is 4.309186038970947 and perplexity is 74.37992193629879
At time: 308.2390179634094 and batch: 200, loss is 4.297801036834716 and perplexity is 73.53790862285388
At time: 308.7854652404785 and batch: 250, loss is 4.256497993469238 and perplexity is 70.56244013193253
At time: 309.3314869403839 and batch: 300, loss is 4.320323390960693 and perplexity is 75.2129475468613
At time: 309.87833285331726 and batch: 350, loss is 4.271534872055054 and perplexity is 71.63149646866898
At time: 310.42661714553833 and batch: 400, loss is 4.3286738777160645 and perplexity is 75.84364190674464
At time: 310.9758298397064 and batch: 450, loss is 4.2998487091064455 and perplexity is 73.68864443536454
At time: 311.53761315345764 and batch: 500, loss is 4.28410122871399 and perplexity is 72.53732295048754
At time: 312.0868909358978 and batch: 550, loss is 4.303498697280884 and perplexity is 73.95809856919682
At time: 312.63582968711853 and batch: 600, loss is 4.400593948364258 and perplexity is 81.49926064492512
At time: 313.1866147518158 and batch: 650, loss is 4.3192899036407475 and perplexity is 75.1352560727677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.812227136948529 and perplexity of 123.00526224642451
Finished 41 epochs...
Completing Train Step...
At time: 314.20375204086304 and batch: 50, loss is 4.340032510757446 and perplexity is 76.71003319903974
At time: 314.752721786499 and batch: 100, loss is 4.332680053710938 and perplexity is 76.14809432210325
At time: 315.30205035209656 and batch: 150, loss is 4.3061971092224125 and perplexity is 74.15793748815197
At time: 315.8613269329071 and batch: 200, loss is 4.295023889541626 and perplexity is 73.33396633893942
At time: 316.4194567203522 and batch: 250, loss is 4.254627819061279 and perplexity is 70.4305993832584
At time: 316.96885347366333 and batch: 300, loss is 4.318486871719361 and perplexity is 75.07494428310693
At time: 317.51787304878235 and batch: 350, loss is 4.269594287872314 and perplexity is 71.49262430977456
At time: 318.06667280197144 and batch: 400, loss is 4.326763401031494 and perplexity is 75.69888272076369
At time: 318.6157765388489 and batch: 450, loss is 4.2983682823181155 and perplexity is 73.57963450266841
At time: 319.1646740436554 and batch: 500, loss is 4.282833423614502 and perplexity is 72.44541803361898
At time: 319.714670419693 and batch: 550, loss is 4.30204963684082 and perplexity is 73.8510064245721
At time: 320.2634835243225 and batch: 600, loss is 4.398852987289429 and perplexity is 81.357497042778
At time: 320.8131079673767 and batch: 650, loss is 4.3171327018737795 and perplexity is 74.97334886165342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.811544979319853 and perplexity of 122.92138188148401
Finished 42 epochs...
Completing Train Step...
At time: 321.81551027297974 and batch: 50, loss is 4.337373313903808 and perplexity is 76.50631710076345
At time: 322.3768744468689 and batch: 100, loss is 4.329730033874512 and perplexity is 75.92378695161494
At time: 322.9324164390564 and batch: 150, loss is 4.303494472503662 and perplexity is 73.95778611336661
At time: 323.4895384311676 and batch: 200, loss is 4.292433099746704 and perplexity is 73.14421935074253
At time: 324.05222964286804 and batch: 250, loss is 4.252708425521851 and perplexity is 70.29554499854333
At time: 324.61320424079895 and batch: 300, loss is 4.316679201126099 and perplexity is 74.93935610034244
At time: 325.17261576652527 and batch: 350, loss is 4.2677607917785645 and perplexity is 71.3616629576223
At time: 325.7358911037445 and batch: 400, loss is 4.324830465316772 and perplexity is 75.55270297033194
At time: 326.2856545448303 and batch: 450, loss is 4.296883573532105 and perplexity is 73.47047123073824
At time: 326.8358561992645 and batch: 500, loss is 4.281483163833618 and perplexity is 72.34766391092116
At time: 327.38524770736694 and batch: 550, loss is 4.300565767288208 and perplexity is 73.74150242962368
At time: 327.9346640110016 and batch: 600, loss is 4.397157907485962 and perplexity is 81.21970640873337
At time: 328.4975206851959 and batch: 650, loss is 4.314985561370849 and perplexity is 74.81254324557695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.810996859681373 and perplexity of 122.85402471966354
Finished 43 epochs...
Completing Train Step...
At time: 329.52732706069946 and batch: 50, loss is 4.334770088195801 and perplexity is 76.30741289785654
At time: 330.076523065567 and batch: 100, loss is 4.3264954566955565 and perplexity is 75.6786023510283
At time: 330.62565755844116 and batch: 150, loss is 4.300380258560181 and perplexity is 73.72782400607787
At time: 331.1753125190735 and batch: 200, loss is 4.289124660491943 and perplexity is 72.90262601299355
At time: 331.723938703537 and batch: 250, loss is 4.249830484390259 and perplexity is 70.09352939216934
At time: 332.2726113796234 and batch: 300, loss is 4.31399094581604 and perplexity is 74.73817051863675
At time: 332.82162380218506 and batch: 350, loss is 4.265223150253296 and perplexity is 71.18080221531007
At time: 333.3698205947876 and batch: 400, loss is 4.321988048553467 and perplexity is 75.33825561965088
At time: 333.91864919662476 and batch: 450, loss is 4.294598054885864 and perplexity is 73.30274484269722
At time: 334.4675028324127 and batch: 500, loss is 4.279170980453491 and perplexity is 72.18057608811905
At time: 335.01651334762573 and batch: 550, loss is 4.298294343948364 and perplexity is 73.57419434556704
At time: 335.57988834381104 and batch: 600, loss is 4.394388818740845 and perplexity is 80.99511293697515
At time: 336.1312232017517 and batch: 650, loss is 4.312312479019165 and perplexity is 74.61283020016971
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.8105214436848955 and perplexity of 122.79563183263635
Finished 44 epochs...
Completing Train Step...
At time: 337.1472325325012 and batch: 50, loss is 4.331327934265136 and perplexity is 76.04520257964829
At time: 337.72274804115295 and batch: 100, loss is 4.323879308700562 and perplexity is 75.48087468242001
At time: 338.2710473537445 and batch: 150, loss is 4.297964286804199 and perplexity is 73.54991466415868
At time: 338.8199963569641 and batch: 200, loss is 4.286984519958496 and perplexity is 72.7467709833888
At time: 339.3686809539795 and batch: 250, loss is 4.24830530166626 and perplexity is 69.98670543581036
At time: 339.9297330379486 and batch: 300, loss is 4.313045177459717 and perplexity is 74.66751893723209
At time: 340.47868394851685 and batch: 350, loss is 4.263772125244141 and perplexity is 71.07759198954219
At time: 341.02835297584534 and batch: 400, loss is 4.321391658782959 and perplexity is 75.29333805019223
At time: 341.5767912864685 and batch: 450, loss is 4.293911037445068 and perplexity is 73.252401873761
At time: 342.1360070705414 and batch: 500, loss is 4.2791657066345214 and perplexity is 72.1801954218314
At time: 342.6844141483307 and batch: 550, loss is 4.2982985877990725 and perplexity is 73.57450658412637
At time: 343.2334325313568 and batch: 600, loss is 4.394244756698608 and perplexity is 80.98344545603499
At time: 343.7916738986969 and batch: 650, loss is 4.311176595687866 and perplexity is 74.52812684571573
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.810292262657016 and perplexity of 122.76749262812287
Finished 45 epochs...
Completing Train Step...
At time: 344.82786226272583 and batch: 50, loss is 4.330197534561157 and perplexity is 75.95928967227407
At time: 345.37687826156616 and batch: 100, loss is 4.321553030014038 and perplexity is 75.30548920924282
At time: 345.9295485019684 and batch: 150, loss is 4.295788536071777 and perplexity is 73.39006234591807
At time: 346.48879194259644 and batch: 200, loss is 4.284737520217895 and perplexity is 72.58349251989165
At time: 347.03825879096985 and batch: 250, loss is 4.246046495437622 and perplexity is 69.82879743857387
At time: 347.5925130844116 and batch: 300, loss is 4.310492315292358 and perplexity is 74.47714615413449
At time: 348.15079069137573 and batch: 350, loss is 4.261383790969848 and perplexity is 70.90803749745866
At time: 348.70404601097107 and batch: 400, loss is 4.318608236312866 and perplexity is 75.08405627612778
At time: 349.2667226791382 and batch: 450, loss is 4.291363821029663 and perplexity is 73.06604959379293
At time: 349.81595277786255 and batch: 500, loss is 4.276520586013794 and perplexity is 71.98952238627494
At time: 350.3786060810089 and batch: 550, loss is 4.295566759109497 and perplexity is 73.3737879255419
At time: 350.92797923088074 and batch: 600, loss is 4.390671319961548 and perplexity is 80.6945726786417
At time: 351.47642612457275 and batch: 650, loss is 4.308347148895264 and perplexity is 74.31755152257458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.80923402075674 and perplexity of 122.63764364136918
Finished 46 epochs...
Completing Train Step...
At time: 352.48359060287476 and batch: 50, loss is 4.325971822738648 and perplexity is 75.63898483846859
At time: 353.0597298145294 and batch: 100, loss is 4.317501735687256 and perplexity is 75.00102166827956
At time: 353.6086845397949 and batch: 150, loss is 4.292160377502442 and perplexity is 73.12427401497835
At time: 354.18336510658264 and batch: 200, loss is 4.280845880508423 and perplexity is 72.30157263925324
At time: 354.735787153244 and batch: 250, loss is 4.242795314788818 and perplexity is 69.60214005531218
At time: 355.2845718860626 and batch: 300, loss is 4.30738935470581 and perplexity is 74.24640468100657
At time: 355.8391363620758 and batch: 350, loss is 4.2579969310760495 and perplexity is 70.66828813703401
At time: 356.39759516716003 and batch: 400, loss is 4.3155162525177 and perplexity is 74.85225613665854
At time: 356.9451723098755 and batch: 450, loss is 4.288605546951294 and perplexity is 72.86479109384037
At time: 357.4937300682068 and batch: 500, loss is 4.274176797866821 and perplexity is 71.820991774175
At time: 358.0427918434143 and batch: 550, loss is 4.293640031814575 and perplexity is 73.23255275013967
At time: 358.6112222671509 and batch: 600, loss is 4.388356409072876 and perplexity is 80.50798798048856
At time: 359.1596460342407 and batch: 650, loss is 4.30522045135498 and perplexity is 74.08554591172724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.808880076688879 and perplexity of 122.59424445580099
Finished 47 epochs...
Completing Train Step...
At time: 360.1612913608551 and batch: 50, loss is 4.32279706954956 and perplexity is 75.39923051191451
At time: 360.7080292701721 and batch: 100, loss is 4.314240188598633 and perplexity is 74.75680078985656
At time: 361.2568242549896 and batch: 150, loss is 4.288792581558227 and perplexity is 72.8784206059574
At time: 361.81241607666016 and batch: 200, loss is 4.2775954246521 and perplexity is 72.06694110535088
At time: 362.36876702308655 and batch: 250, loss is 4.240224094390869 and perplexity is 69.42340749193673
At time: 362.9171495437622 and batch: 300, loss is 4.304919338226318 and perplexity is 74.06324113950751
At time: 363.4658079147339 and batch: 350, loss is 4.255891027450562 and perplexity is 70.51962412382306
At time: 364.01475715637207 and batch: 400, loss is 4.313374862670899 and perplexity is 74.69213977232094
At time: 364.56365036964417 and batch: 450, loss is 4.286553916931152 and perplexity is 72.71545274692147
At time: 365.1123197078705 and batch: 500, loss is 4.272408103942871 and perplexity is 71.69407469423199
At time: 365.6614990234375 and batch: 550, loss is 4.29188533782959 and perplexity is 73.10416470413165
At time: 366.2108266353607 and batch: 600, loss is 4.38634181022644 and perplexity is 80.34595994632028
At time: 366.7591166496277 and batch: 650, loss is 4.302977161407471 and perplexity is 73.9195368242043
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.80763454063266 and perplexity of 122.44164395848537
Finished 48 epochs...
Completing Train Step...
At time: 367.77148032188416 and batch: 50, loss is 4.320194072723389 and perplexity is 75.2032217699358
At time: 368.3427436351776 and batch: 100, loss is 4.31132866859436 and perplexity is 74.53946141639943
At time: 368.89249539375305 and batch: 150, loss is 4.2861097240448 and perplexity is 72.68316023266694
At time: 369.44553780555725 and batch: 200, loss is 4.274650058746338 and perplexity is 71.85498988426299
At time: 369.995952129364 and batch: 250, loss is 4.237176866531372 and perplexity is 69.21218054229624
At time: 370.5598478317261 and batch: 300, loss is 4.301849098205566 and perplexity is 73.83619792941892
At time: 371.1115071773529 and batch: 350, loss is 4.2533143043518065 and perplexity is 70.33814848607045
At time: 371.66054224967957 and batch: 400, loss is 4.311144733428955 and perplexity is 74.52575224907227
At time: 372.20985078811646 and batch: 450, loss is 4.284890480041504 and perplexity is 72.59459572725508
At time: 372.7588245868683 and batch: 500, loss is 4.270320749282837 and perplexity is 71.54457981202137
At time: 373.3078601360321 and batch: 550, loss is 4.290098171234131 and perplexity is 72.97363205951761
At time: 373.856814622879 and batch: 600, loss is 4.384472522735596 and perplexity is 80.19591053488895
At time: 374.4063935279846 and batch: 650, loss is 4.301087846755982 and perplexity is 73.7800114054631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.806461409026501 and perplexity of 122.29808801751739
Finished 49 epochs...
Completing Train Step...
At time: 375.42246675491333 and batch: 50, loss is 4.31757568359375 and perplexity is 75.00656804188569
At time: 375.9726343154907 and batch: 100, loss is 4.308613719940186 and perplexity is 74.3373650706818
At time: 376.5226423740387 and batch: 150, loss is 4.283609247207641 and perplexity is 72.50164470631368
At time: 377.0726613998413 and batch: 200, loss is 4.272096357345581 and perplexity is 71.67172779386628
At time: 377.6225962638855 and batch: 250, loss is 4.235267295837402 and perplexity is 69.08014110011459
At time: 378.17888283729553 and batch: 300, loss is 4.2996853256225585 and perplexity is 73.67660591138855
At time: 378.743754863739 and batch: 350, loss is 4.2511818313598635 and perplexity is 70.1883140997919
At time: 379.296128988266 and batch: 400, loss is 4.309185285568237 and perplexity is 74.37986589828515
At time: 379.8458230495453 and batch: 450, loss is 4.2828590679168705 and perplexity is 72.44727586964561
At time: 380.3949041366577 and batch: 500, loss is 4.268538236618042 and perplexity is 71.4171642860404
At time: 380.94467973709106 and batch: 550, loss is 4.288273277282715 and perplexity is 72.84058435565773
At time: 381.4945945739746 and batch: 600, loss is 4.382478590011597 and perplexity is 80.03616459879335
At time: 382.05667424201965 and batch: 650, loss is 4.298931140899658 and perplexity is 73.62106108893607
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.805749332203584 and perplexity of 122.21103338202563
Finished Training.
Improved accuracyfrom -131.226796241437 to -122.21103338202563
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa092233860>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'tune_wordvecs': True, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'seq_len': 20, 'anneal': 7.417179330718226, 'data': 'ptb', 'dropout': 0.9588655967622915, 'batch_size': 80, 'num_layers': 1, 'lr': 27.24601701842465}, 'best_accuracy': -131.226796241437}, {'params': {'tune_wordvecs': True, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'seq_len': 20, 'anneal': 2.8637643628573874, 'data': 'ptb', 'dropout': 0.4074046926609942, 'batch_size': 80, 'num_layers': 1, 'lr': 15.763860501961515}, 'best_accuracy': -142.15370197515767}, {'params': {'tune_wordvecs': True, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'seq_len': 20, 'anneal': 3.8249845852050006, 'data': 'ptb', 'dropout': 0.6607439212537519, 'batch_size': 80, 'num_layers': 1, 'lr': 23.07657387936441}, 'best_accuracy': -222.06702121730007}, {'params': {'tune_wordvecs': True, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'seq_len': 20, 'anneal': 5.209266033185621, 'data': 'ptb', 'dropout': 0.6712772295736538, 'batch_size': 80, 'num_layers': 1, 'lr': 22.05395899225164}, 'best_accuracy': -227.30190353947148}, {'params': {'tune_wordvecs': True, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'seq_len': 20, 'anneal': 6.600065067976981, 'data': 'ptb', 'dropout': 0.13744330649552117, 'batch_size': 80, 'num_layers': 1, 'lr': 17.472087439493734}, 'best_accuracy': -137.18021031581148}, {'params': {'tune_wordvecs': True, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'seq_len': 20, 'anneal': 5.576268026348681, 'data': 'ptb', 'dropout': 0.10527237077851527, 'batch_size': 80, 'num_layers': 1, 'lr': 15.60061705262029}, 'best_accuracy': -122.21103338202563}]
