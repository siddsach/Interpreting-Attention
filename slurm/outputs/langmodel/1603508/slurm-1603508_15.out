Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 30], 'name': 'lr', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [2, 8], 'name': 'anneal', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'batch_size': 50, 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.9907225759239384, 'anneal': 3.7323286251692505, 'tune_wordvecs': True, 'lr': 2.682445178203625, 'wordvec_dim': 200, 'seq_len': 20, 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.6233043670654297 and batch: 50, loss is 11.217249965667724 and perplexity is 74402.88267242664
At time: 2.082948923110962 and batch: 100, loss is 11.04492540359497 and perplexity is 62625.34852210853
At time: 2.5303056240081787 and batch: 150, loss is 10.83928611755371 and perplexity is 50984.96771403349
At time: 2.9882431030273438 and batch: 200, loss is 10.716267948150636 and perplexity is 45083.33514162665
At time: 3.440764904022217 and batch: 250, loss is 10.496353759765626 and perplexity is 36183.32874265633
At time: 3.8899009227752686 and batch: 300, loss is 10.333198432922364 and perplexity is 30736.262729808404
At time: 4.341423273086548 and batch: 350, loss is 10.208789329528809 and perplexity is 27140.689259990446
At time: 4.791900157928467 and batch: 400, loss is 10.020164012908936 and perplexity is 22475.115826764308
At time: 5.242265462875366 and batch: 450, loss is 9.811449451446533 and perplexity is 18241.40796014215
At time: 5.693232297897339 and batch: 500, loss is 9.656987991333008 and perplexity is 15630.634089412053
At time: 6.143778562545776 and batch: 550, loss is 9.55287811279297 and perplexity is 14085.175112604502
At time: 6.594656467437744 and batch: 600, loss is 9.405534954071046 and perplexity is 12155.474872560528
At time: 7.045508861541748 and batch: 650, loss is 9.221506004333497 and perplexity is 10112.282006850908
At time: 7.496082544326782 and batch: 700, loss is 9.09518030166626 and perplexity is 8912.234740708278
At time: 7.946621417999268 and batch: 750, loss is 8.93770305633545 and perplexity is 7613.688741420615
At time: 8.396940469741821 and batch: 800, loss is 8.829756927490234 and perplexity is 6834.625304171294
At time: 8.845826864242554 and batch: 850, loss is 8.684169845581055 and perplexity is 5908.6333829504965
At time: 9.296020746231079 and batch: 900, loss is 8.599160385131835 and perplexity is 5427.101003208258
At time: 9.744231939315796 and batch: 950, loss is 8.530325241088867 and perplexity is 5066.093268589592
At time: 10.192463874816895 and batch: 1000, loss is 8.352145357131958 and perplexity is 4239.265733347305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 7.505062940644055 and perplexity of 1817.2196381482154
Finished 1 epochs...
Completing Train Step...
At time: 11.596922874450684 and batch: 50, loss is 6.33062518119812 and perplexity is 561.5075280919824
At time: 12.063284635543823 and batch: 100, loss is 5.676603202819824 and perplexity is 291.95602827219244
At time: 12.507796049118042 and batch: 150, loss is 5.500924205780029 and perplexity is 244.9181824971275
At time: 12.952945232391357 and batch: 200, loss is 5.3821655368804935 and perplexity is 217.49275440219597
At time: 13.398393869400024 and batch: 250, loss is 5.299454898834228 and perplexity is 200.22763590428727
At time: 13.843965530395508 and batch: 300, loss is 5.122700138092041 and perplexity is 167.7878087759771
At time: 14.290977001190186 and batch: 350, loss is 5.131774578094483 and perplexity is 169.31731840587375
At time: 14.73749041557312 and batch: 400, loss is 5.0343659973144534 and perplexity is 153.60217754133046
At time: 15.183071851730347 and batch: 450, loss is 5.008736200332642 and perplexity is 149.71540625609364
At time: 15.62809705734253 and batch: 500, loss is 4.970435342788696 and perplexity is 144.08960208792428
At time: 16.074328899383545 and batch: 550, loss is 4.998343372344971 and perplexity is 148.16749730008812
At time: 16.519065141677856 and batch: 600, loss is 4.9931504344940185 and perplexity is 147.40006702851176
At time: 16.979047298431396 and batch: 650, loss is 4.94392183303833 and perplexity is 140.31948143639485
At time: 17.42477798461914 and batch: 700, loss is 4.904150028228759 and perplexity is 134.84824408144354
At time: 17.870014667510986 and batch: 750, loss is 4.819520921707153 and perplexity is 123.90571601947553
At time: 18.31485652923584 and batch: 800, loss is 4.853977060317993 and perplexity is 128.24943265751224
At time: 18.759116888046265 and batch: 850, loss is 4.807959632873535 and perplexity is 122.48145525771794
At time: 19.204373598098755 and batch: 900, loss is 4.936669197082519 and perplexity is 139.30547686780295
At time: 19.649312019348145 and batch: 950, loss is 4.838295021057129 and perplexity is 126.2539078643141
At time: 20.097337245941162 and batch: 1000, loss is 4.7932278919219975 and perplexity is 120.6903158814079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.872711181640625 and perplexity of 130.67471996921628
Finished 2 epochs...
Completing Train Step...
At time: 21.516958475112915 and batch: 50, loss is 4.8203450965881345 and perplexity is 124.00787809215292
At time: 21.965546369552612 and batch: 100, loss is 4.716727180480957 and perplexity is 111.8017463028017
At time: 22.413095712661743 and batch: 150, loss is 4.728753938674926 and perplexity is 113.15447704883746
At time: 22.85970950126648 and batch: 200, loss is 4.730454626083374 and perplexity is 113.34708117644824
At time: 23.307254314422607 and batch: 250, loss is 4.702170314788819 and perplexity is 110.18605155414373
At time: 23.753836154937744 and batch: 300, loss is 4.582064199447632 and perplexity is 97.71589125921734
At time: 24.199970960617065 and batch: 350, loss is 4.634149808883667 and perplexity is 102.94036179963925
At time: 24.644007444381714 and batch: 400, loss is 4.55401312828064 and perplexity is 95.01294336627001
At time: 25.08817434310913 and batch: 450, loss is 4.593022012710572 and perplexity is 98.79253178839909
At time: 25.532114267349243 and batch: 500, loss is 4.546601991653443 and perplexity is 94.31139231801674
At time: 25.975112438201904 and batch: 550, loss is 4.602368173599243 and perplexity is 99.72019095838711
At time: 26.418966054916382 and batch: 600, loss is 4.60799015045166 and perplexity is 100.28239443034796
At time: 26.86279535293579 and batch: 650, loss is 4.574579906463623 and perplexity is 96.98728684533107
At time: 27.305864334106445 and batch: 700, loss is 4.550091352462768 and perplexity is 94.64105361335999
At time: 27.749449014663696 and batch: 750, loss is 4.486861591339111 and perplexity is 88.8421848079413
At time: 28.209723234176636 and batch: 800, loss is 4.520604057312012 and perplexity is 91.8910887007195
At time: 28.652523279190063 and batch: 850, loss is 4.494454851150513 and perplexity is 89.51935430771404
At time: 29.096567392349243 and batch: 900, loss is 4.646694688796997 and perplexity is 104.23987032347836
At time: 29.54000473022461 and batch: 950, loss is 4.55282826423645 and perplexity is 94.90043261407799
At time: 29.985331058502197 and batch: 1000, loss is 4.512920150756836 and perplexity is 91.18771196320384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.70142680842702 and perplexity of 110.10415797180406
Finished 3 epochs...
Completing Train Step...
At time: 31.39210605621338 and batch: 50, loss is 4.56332685470581 and perplexity is 95.90200172398518
At time: 31.834540128707886 and batch: 100, loss is 4.45196364402771 and perplexity is 85.7952500318857
At time: 32.277793884277344 and batch: 150, loss is 4.480644340515137 and perplexity is 88.29154417043273
At time: 32.71829891204834 and batch: 200, loss is 4.500118141174316 and perplexity is 90.0277666583467
At time: 33.160611391067505 and batch: 250, loss is 4.475661678314209 and perplexity is 87.8527114162268
At time: 33.60569357872009 and batch: 300, loss is 4.370583009719849 and perplexity is 79.08972834122683
At time: 34.04962658882141 and batch: 350, loss is 4.430343656539917 and perplexity is 83.96026544730486
At time: 34.49267101287842 and batch: 400, loss is 4.344980778694153 and perplexity is 77.09055568391246
At time: 34.9377715587616 and batch: 450, loss is 4.400770854949951 and perplexity is 81.51367967623554
At time: 35.38105010986328 and batch: 500, loss is 4.3466105556488035 and perplexity is 77.21629853355367
At time: 35.82483983039856 and batch: 550, loss is 4.412107982635498 and perplexity is 82.44306901911973
At time: 36.26816964149475 and batch: 600, loss is 4.417524824142456 and perplexity is 82.89086177359917
At time: 36.71173286437988 and batch: 650, loss is 4.386758022308349 and perplexity is 80.37940786581376
At time: 37.155110597610474 and batch: 700, loss is 4.3633044338226314 and perplexity is 78.51615767183048
At time: 37.599191427230835 and batch: 750, loss is 4.311581382751465 and perplexity is 74.5583009739735
At time: 38.0435004234314 and batch: 800, loss is 4.3384730052948 and perplexity is 76.59049671635155
At time: 38.487284898757935 and batch: 850, loss is 4.317137069702149 and perplexity is 74.97367633308868
At time: 38.93125295639038 and batch: 900, loss is 4.481579742431641 and perplexity is 88.3741708886314
At time: 39.39063572883606 and batch: 950, loss is 4.391798868179321 and perplexity is 80.78561101566153
At time: 39.835047245025635 and batch: 1000, loss is 4.349851636886597 and perplexity is 77.46696883195041
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.621300674066311 and perplexity of 101.62612867367355
Finished 4 epochs...
Completing Train Step...
At time: 41.21807336807251 and batch: 50, loss is 4.4061665439605715 and perplexity is 81.95469085150955
At time: 41.67869830131531 and batch: 100, loss is 4.290198526382446 and perplexity is 72.98095570666273
At time: 42.12234377861023 and batch: 150, loss is 4.3265696620941165 and perplexity is 75.68421832024326
At time: 42.56982493400574 and batch: 200, loss is 4.353794918060303 and perplexity is 77.77304594918535
At time: 43.0139102935791 and batch: 250, loss is 4.330994358062744 and perplexity is 76.0198399401833
At time: 43.45915222167969 and batch: 300, loss is 4.2329612159729 and perplexity is 68.92102032081634
At time: 43.90342164039612 and batch: 350, loss is 4.297562026977539 and perplexity is 73.5203344381022
At time: 44.34818744659424 and batch: 400, loss is 4.21110155582428 and perplexity is 67.43077767991986
At time: 44.79271173477173 and batch: 450, loss is 4.272678084373474 and perplexity is 71.71343330449504
At time: 45.238078117370605 and batch: 500, loss is 4.2134253740310665 and perplexity is 67.58765675742121
At time: 45.68218922615051 and batch: 550, loss is 4.280685720443725 and perplexity is 72.2899937419648
At time: 46.12704682350159 and batch: 600, loss is 4.287974691390991 and perplexity is 72.8188384314988
At time: 46.57166647911072 and batch: 650, loss is 4.256797838211059 and perplexity is 70.58360108091729
At time: 47.01731634140015 and batch: 700, loss is 4.233574595451355 and perplexity is 68.96330802819537
At time: 47.46237349510193 and batch: 750, loss is 4.189601864814758 and perplexity is 65.99651019629631
At time: 47.90735483169556 and batch: 800, loss is 4.210965514183044 and perplexity is 67.42160491020796
At time: 48.351736545562744 and batch: 850, loss is 4.193461785316467 and perplexity is 66.25174375276956
At time: 48.7961003780365 and batch: 900, loss is 4.362503824234008 and perplexity is 78.45332203989763
At time: 49.2410991191864 and batch: 950, loss is 4.277171463966369 and perplexity is 72.03639403141811
At time: 49.68551683425903 and batch: 1000, loss is 4.231610946655273 and perplexity is 68.82802118280877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.576813488471799 and perplexity of 97.20415801385845
Finished 5 epochs...
Completing Train Step...
At time: 51.10202193260193 and batch: 50, loss is 4.292102108001709 and perplexity is 73.12001322417834
At time: 51.54954290390015 and batch: 100, loss is 4.174863905906677 and perplexity is 65.03098872530212
At time: 51.99733591079712 and batch: 150, loss is 4.2138598537445064 and perplexity is 67.61702860343482
At time: 52.443227767944336 and batch: 200, loss is 4.243440446853637 and perplexity is 69.6470571148017
At time: 52.89156198501587 and batch: 250, loss is 4.2224565124511715 and perplexity is 68.20081483130733
At time: 53.3381233215332 and batch: 300, loss is 4.130448212623596 and perplexity is 62.2057981112655
At time: 53.785861015319824 and batch: 350, loss is 4.197405247688294 and perplexity is 66.51352082610686
At time: 54.232810258865356 and batch: 400, loss is 4.109694857597351 and perplexity is 60.92812297893275
At time: 54.6809024810791 and batch: 450, loss is 4.1743190574646 and perplexity is 64.99556634319956
At time: 55.12800073623657 and batch: 500, loss is 4.110600266456604 and perplexity is 60.98331282217184
At time: 55.575167179107666 and batch: 550, loss is 4.179702234268189 and perplexity is 65.34639240041767
At time: 56.02152347564697 and batch: 600, loss is 4.187104063034058 and perplexity is 65.83186970086882
At time: 56.46959137916565 and batch: 650, loss is 4.155933117866516 and perplexity is 63.81148040829593
At time: 56.91680359840393 and batch: 700, loss is 4.133539414405822 and perplexity is 62.398386296365544
At time: 57.36362290382385 and batch: 750, loss is 4.094162015914917 and perplexity is 59.98904822120279
At time: 57.80974864959717 and batch: 800, loss is 4.11231038570404 and perplexity is 61.087690783143636
At time: 58.255913734436035 and batch: 850, loss is 4.097798242568969 and perplexity is 60.20757907035802
At time: 58.703242778778076 and batch: 900, loss is 4.269573955535889 and perplexity is 71.49117071246275
At time: 59.15057826042175 and batch: 950, loss is 4.187481441497803 and perplexity is 65.85671791901831
At time: 59.59750032424927 and batch: 1000, loss is 4.139539737701416 and perplexity is 62.773922329341715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5494273115948936 and perplexity of 94.5782289473634
Finished 6 epochs...
Completing Train Step...
At time: 60.98755645751953 and batch: 50, loss is 4.202425127029419 and perplexity is 66.848250122639
At time: 61.449596643447876 and batch: 100, loss is 4.081477885246277 and perplexity is 59.232944690123944
At time: 61.89379811286926 and batch: 150, loss is 4.1233375358581545 and perplexity is 61.765041683321165
At time: 62.36222171783447 and batch: 200, loss is 4.15453917503357 and perplexity is 63.72259281902305
At time: 62.80693554878235 and batch: 250, loss is 4.135892305374146 and perplexity is 62.545375753190605
At time: 63.25265121459961 and batch: 300, loss is 4.046345529556274 and perplexity is 57.18808254461008
At time: 63.69711399078369 and batch: 350, loss is 4.114161682128906 and perplexity is 61.200886954197436
At time: 64.1414623260498 and batch: 400, loss is 4.026399922370911 and perplexity is 56.05873174846321
At time: 64.58608150482178 and batch: 450, loss is 4.093570084571838 and perplexity is 59.95354933080863
At time: 65.02981519699097 and batch: 500, loss is 4.027324619293213 and perplexity is 56.11059305948114
At time: 65.4745717048645 and batch: 550, loss is 4.0978835010528565 and perplexity is 60.21271249609901
At time: 65.91859483718872 and batch: 600, loss is 4.1042810773849485 and perplexity is 60.599162774950024
At time: 66.36320304870605 and batch: 650, loss is 4.073817768096924 and perplexity is 58.780946783324595
At time: 66.80750679969788 and batch: 700, loss is 4.051488475799561 and perplexity is 57.48295538653491
At time: 67.25208497047424 and batch: 750, loss is 4.015778412818909 and perplexity is 55.46645440168119
At time: 67.69678473472595 and batch: 800, loss is 4.030828185081482 and perplexity is 56.30752499423395
At time: 68.14093828201294 and batch: 850, loss is 4.017085938453675 and perplexity is 55.539025646723445
At time: 68.58456754684448 and batch: 900, loss is 4.190767440795899 and perplexity is 66.07347899119557
At time: 69.02965521812439 and batch: 950, loss is 4.110210185050964 and perplexity is 60.95952900489961
At time: 69.47474646568298 and batch: 1000, loss is 4.062721171379089 and perplexity is 58.13228395314229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.533327428306022 and perplexity of 93.06772261505282
Finished 7 epochs...
Completing Train Step...
At time: 70.86320686340332 and batch: 50, loss is 4.126263613700867 and perplexity is 61.94603567536166
At time: 71.32478404045105 and batch: 100, loss is 4.004725813865662 and perplexity is 54.856781368043514
At time: 71.7688307762146 and batch: 150, loss is 4.04764410495758 and perplexity is 57.262393820805464
At time: 72.21429705619812 and batch: 200, loss is 4.0798233795166015 and perplexity is 59.13502447086764
At time: 72.65806484222412 and batch: 250, loss is 4.062965688705444 and perplexity is 58.14650004175856
At time: 73.10274982452393 and batch: 300, loss is 3.9766741466522215 and perplexity is 53.3393400761805
At time: 73.54691052436829 and batch: 350, loss is 4.044508838653565 and perplexity is 57.083142114724254
At time: 74.00763034820557 and batch: 400, loss is 3.955477318763733 and perplexity is 52.220613875439604
At time: 74.45259857177734 and batch: 450, loss is 4.025418400764465 and perplexity is 56.00373588625086
At time: 74.89702439308167 and batch: 500, loss is 3.9568582010269164 and perplexity is 52.29277420590649
At time: 75.34191370010376 and batch: 550, loss is 4.028885102272033 and perplexity is 56.19822103808454
At time: 75.78607773780823 and batch: 600, loss is 4.033632087707519 and perplexity is 56.465627359668076
At time: 76.23038339614868 and batch: 650, loss is 4.005052838325501 and perplexity is 54.87472381098785
At time: 76.67491698265076 and batch: 700, loss is 3.9821551609039307 and perplexity is 53.63249642246604
At time: 77.11890912055969 and batch: 750, loss is 3.9487563467025755 and perplexity is 51.87081739149811
At time: 77.56322574615479 and batch: 800, loss is 3.961756911277771 and perplexity is 52.549569825086024
At time: 78.00752687454224 and batch: 850, loss is 3.9484838962554933 and perplexity is 51.85668708910029
At time: 78.4514410495758 and batch: 900, loss is 4.122636685371399 and perplexity is 61.721768789478546
At time: 78.89486622810364 and batch: 950, loss is 4.044170250892639 and perplexity is 57.063817733133206
At time: 79.33892226219177 and batch: 1000, loss is 3.99677255153656 and perplexity is 54.422221370770885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5261595191025155 and perplexity of 92.40300678695148
Finished 8 epochs...
Completing Train Step...
At time: 80.74456644058228 and batch: 50, loss is 4.060149850845337 and perplexity is 57.98299922942941
At time: 81.19079160690308 and batch: 100, loss is 3.9385628747940062 and perplexity is 51.34475940527443
At time: 81.63694906234741 and batch: 150, loss is 3.9823312091827394 and perplexity is 53.64193916231409
At time: 82.08179092407227 and batch: 200, loss is 4.016150727272033 and perplexity is 55.48710920912158
At time: 82.52777695655823 and batch: 250, loss is 3.9988281297683717 and perplexity is 54.534205561047884
At time: 82.97505068778992 and batch: 300, loss is 3.9160387468338014 and perplexity is 50.20119076495561
At time: 83.42013096809387 and batch: 350, loss is 3.9846400117874143 and perplexity is 53.765930892307566
At time: 83.8674669265747 and batch: 400, loss is 3.893840403556824 and perplexity is 49.09908521445856
At time: 84.31268239021301 and batch: 450, loss is 3.9648747873306274 and perplexity is 52.713668557249534
At time: 84.75785875320435 and batch: 500, loss is 3.8953645753860475 and perplexity is 49.173977717005826
At time: 85.228520154953 and batch: 550, loss is 3.968376049995422 and perplexity is 52.89855643853548
At time: 85.67461156845093 and batch: 600, loss is 3.9715646743774413 and perplexity is 53.067499269843175
At time: 86.11920356750488 and batch: 650, loss is 3.9436794757843017 and perplexity is 51.6081432926164
At time: 86.56390690803528 and batch: 700, loss is 3.920749182701111 and perplexity is 50.438218067202136
At time: 87.00944685935974 and batch: 750, loss is 3.8903235816955566 and perplexity is 48.92671575221059
At time: 87.45450639724731 and batch: 800, loss is 3.9001210927963257 and perplexity is 49.40843174845787
At time: 87.89910912513733 and batch: 850, loss is 3.887398362159729 and perplexity is 48.783803494240054
At time: 88.3438458442688 and batch: 900, loss is 4.061243839263916 and perplexity is 58.046466669052954
At time: 88.78896856307983 and batch: 950, loss is 3.9863266515731812 and perplexity is 53.85669116890017
At time: 89.23428106307983 and batch: 1000, loss is 3.9384644556045534 and perplexity is 51.339706344334274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.524329580911776 and perplexity of 92.23406961532694
Finished 9 epochs...
Completing Train Step...
At time: 90.62933683395386 and batch: 50, loss is 4.002392029762268 and perplexity is 54.728906757629524
At time: 91.09064173698425 and batch: 100, loss is 3.8802887201309204 and perplexity is 48.438198135559226
At time: 91.53704977035522 and batch: 150, loss is 3.924791736602783 and perplexity is 50.6425299751506
At time: 91.98315763473511 and batch: 200, loss is 3.959252829551697 and perplexity is 52.418146024216625
At time: 92.42856788635254 and batch: 250, loss is 3.941283187866211 and perplexity is 51.48462337611727
At time: 92.87428402900696 and batch: 300, loss is 3.8630058574676513 and perplexity is 47.60824009370519
At time: 93.32116270065308 and batch: 350, loss is 3.931302795410156 and perplexity is 50.97334226641352
At time: 93.76823925971985 and batch: 400, loss is 3.838877396583557 and perplexity is 46.4732740888597
At time: 94.21582794189453 and batch: 450, loss is 3.911166162490845 and perplexity is 49.95717620238924
At time: 94.66133046150208 and batch: 500, loss is 3.8409859848022463 and perplexity is 46.57137047318163
At time: 95.10665559768677 and batch: 550, loss is 3.914659843444824 and perplexity is 50.13201587658374
At time: 95.54994297027588 and batch: 600, loss is 3.9168706130981445 and perplexity is 50.24296881644945
At time: 95.99219298362732 and batch: 650, loss is 3.8888275718688963 and perplexity is 48.85357562747264
At time: 96.45018553733826 and batch: 700, loss is 3.865948910713196 and perplexity is 47.74856006251495
At time: 96.93475437164307 and batch: 750, loss is 3.8375968503952027 and perplexity is 46.41380100200919
At time: 97.38832902908325 and batch: 800, loss is 3.846103100776672 and perplexity is 46.81029235111521
At time: 97.87727499008179 and batch: 850, loss is 3.832809681892395 and perplexity is 46.192141300242234
At time: 98.35856652259827 and batch: 900, loss is 4.007469086647034 and perplexity is 55.007475085726824
At time: 98.8323609828949 and batch: 950, loss is 3.9348490905761717 and perplexity is 51.15442968863084
At time: 99.28802108764648 and batch: 1000, loss is 3.8867979717254637 and perplexity is 48.75452295603212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.52415429092035 and perplexity of 92.2179033229909
Finished 10 epochs...
Completing Train Step...
At time: 100.6922128200531 and batch: 50, loss is 3.9526172876358032 and perplexity is 52.071474667285564
At time: 101.15132975578308 and batch: 100, loss is 3.829067063331604 and perplexity is 46.01958484276879
At time: 101.60825276374817 and batch: 150, loss is 3.872533373832703 and perplexity is 48.06399604390199
At time: 102.06497883796692 and batch: 200, loss is 3.9075246238708496 and perplexity is 49.7755860503474
At time: 102.51534628868103 and batch: 250, loss is 3.890407433509827 and perplexity is 48.93081851810248
At time: 102.96458840370178 and batch: 300, loss is 3.8160821866989134 and perplexity is 45.42588908572837
At time: 103.40861463546753 and batch: 350, loss is 3.8835085248947143 and perplexity is 48.59441102921852
At time: 103.85280990600586 and batch: 400, loss is 3.7887270784378053 and perplexity is 44.20010118956939
At time: 104.29877233505249 and batch: 450, loss is 3.8621207523345946 and perplexity is 47.566120438933524
At time: 104.74428153038025 and batch: 500, loss is 3.791245698928833 and perplexity is 44.31156477844845
At time: 105.18813920021057 and batch: 550, loss is 3.865592703819275 and perplexity is 47.731554725134885
At time: 105.6339168548584 and batch: 600, loss is 3.868272891044617 and perplexity is 47.859655819129266
At time: 106.07770109176636 and batch: 650, loss is 3.8396821641921997 and perplexity is 46.51068932778082
At time: 106.52159810066223 and batch: 700, loss is 3.8153975677490233 and perplexity is 45.39480030444336
At time: 106.96606135368347 and batch: 750, loss is 3.7885473775863647 and perplexity is 44.1921591073727
At time: 107.40963101387024 and batch: 800, loss is 3.797286958694458 and perplexity is 44.58007269830294
At time: 107.85382294654846 and batch: 850, loss is 3.7839781522750853 and perplexity is 43.99069579156958
At time: 108.31570386886597 and batch: 900, loss is 3.958207912445068 and perplexity is 52.363402013195994
At time: 108.75951218605042 and batch: 950, loss is 3.887368011474609 and perplexity is 48.78232289484997
At time: 109.20469856262207 and batch: 1000, loss is 3.8403740882873536 and perplexity is 46.54288233068174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5293467451886436 and perplexity of 92.69798589346124
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 110.60970282554626 and batch: 50, loss is 3.9165550088882446 and perplexity is 50.22711442596089
At time: 111.05549788475037 and batch: 100, loss is 3.7931341695785523 and perplexity is 44.39532493237888
At time: 111.5018367767334 and batch: 150, loss is 3.8344462966918944 and perplexity is 46.26780193908728
At time: 111.94615435600281 and batch: 200, loss is 3.8648031759262085 and perplexity is 47.69388420422031
At time: 112.3906090259552 and batch: 250, loss is 3.8361194849014284 and perplexity is 46.34528148062479
At time: 112.83667135238647 and batch: 300, loss is 3.7620935106277464 and perplexity is 43.0384331476033
At time: 113.28295302391052 and batch: 350, loss is 3.824370183944702 and perplexity is 45.80394322110407
At time: 113.72781085968018 and batch: 400, loss is 3.719445652961731 and perplexity is 41.24152565306664
At time: 114.17294526100159 and batch: 450, loss is 3.788185963630676 and perplexity is 44.176190330182564
At time: 114.61722183227539 and batch: 500, loss is 3.702617230415344 and perplexity is 40.55330292608615
At time: 115.06265878677368 and batch: 550, loss is 3.7743413639068604 and perplexity is 43.568802877432375
At time: 115.50823712348938 and batch: 600, loss is 3.763748631477356 and perplexity is 43.1097259384576
At time: 115.95300817489624 and batch: 650, loss is 3.731455030441284 and perplexity is 41.73979667628287
At time: 116.39997029304504 and batch: 700, loss is 3.693567142486572 and perplexity is 40.18794771088485
At time: 116.85297536849976 and batch: 750, loss is 3.6629109811782836 and perplexity is 38.97463232072173
At time: 117.30773520469666 and batch: 800, loss is 3.661679391860962 and perplexity is 38.926661126377844
At time: 117.76165819168091 and batch: 850, loss is 3.636492667198181 and perplexity is 37.95846999444182
At time: 118.21434497833252 and batch: 900, loss is 3.8047905492782594 and perplexity is 44.915841472310575
At time: 118.66787600517273 and batch: 950, loss is 3.7280575466156005 and perplexity is 41.59822701867581
At time: 119.12030482292175 and batch: 1000, loss is 3.6691493558883668 and perplexity is 39.218530654937524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4876158179306405 and perplexity of 88.90921722180985
Finished 12 epochs...
Completing Train Step...
At time: 120.54142236709595 and batch: 50, loss is 3.8598725175857544 and perplexity is 47.45930075694951
At time: 121.00694251060486 and batch: 100, loss is 3.7322051906585694 and perplexity is 41.77111995849818
At time: 121.46180248260498 and batch: 150, loss is 3.7748199033737184 and perplexity is 43.589657258557224
At time: 121.91764116287231 and batch: 200, loss is 3.8092424392700197 and perplexity is 45.11624761950036
At time: 122.3744764328003 and batch: 250, loss is 3.7840949058532716 and perplexity is 43.99583216254916
At time: 122.82949352264404 and batch: 300, loss is 3.713069591522217 and perplexity is 40.9794036925343
At time: 123.28538918495178 and batch: 350, loss is 3.777560806274414 and perplexity is 43.709296160985474
At time: 123.74108815193176 and batch: 400, loss is 3.676620445251465 and perplexity is 39.51263306685472
At time: 124.19733476638794 and batch: 450, loss is 3.7487817907333376 and perplexity is 42.46931396249546
At time: 124.65321326255798 and batch: 500, loss is 3.6656506872177124 and perplexity is 39.08155776153155
At time: 125.10900616645813 and batch: 550, loss is 3.740294828414917 and perplexity is 42.11040367854679
At time: 125.56548762321472 and batch: 600, loss is 3.732492642402649 and perplexity is 41.78312886569039
At time: 126.022465467453 and batch: 650, loss is 3.702728719711304 and perplexity is 40.55782443732358
At time: 126.47801303863525 and batch: 700, loss is 3.668940806388855 and perplexity is 39.21035250280225
At time: 126.93359231948853 and batch: 750, loss is 3.6423045015335083 and perplexity is 38.1797206469689
At time: 127.38947820663452 and batch: 800, loss is 3.644790759086609 and perplexity is 38.27476336718452
At time: 127.84560227394104 and batch: 850, loss is 3.6227528762817385 and perplexity is 37.44049513500667
At time: 128.30241680145264 and batch: 900, loss is 3.795637402534485 and perplexity is 44.50659598341629
At time: 128.75755834579468 and batch: 950, loss is 3.7227167797088625 and perplexity is 41.37665279924257
At time: 129.21258234977722 and batch: 1000, loss is 3.6671668100357055 and perplexity is 39.1408551427143
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.489089593654725 and perplexity of 89.04034607132182
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 130.6154923439026 and batch: 50, loss is 3.8427074718475343 and perplexity is 46.651611531287216
At time: 131.08095502853394 and batch: 100, loss is 3.718725461959839 and perplexity is 41.211834570295665
At time: 131.5288426876068 and batch: 150, loss is 3.7650197029113768 and perplexity is 43.1645563189095
At time: 131.99226140975952 and batch: 200, loss is 3.8001510190963743 and perplexity is 44.707935735560284
At time: 132.4406590461731 and batch: 250, loss is 3.772925057411194 and perplexity is 43.50713977614173
At time: 132.89561772346497 and batch: 300, loss is 3.700538411140442 and perplexity is 40.46908750295288
At time: 133.35021305084229 and batch: 350, loss is 3.7640019464492798 and perplexity is 43.12064766073283
At time: 133.80430364608765 and batch: 400, loss is 3.6566507053375243 and perplexity is 38.73140250882828
At time: 134.25741815567017 and batch: 450, loss is 3.7275570154190065 and perplexity is 41.57741101829334
At time: 134.71326065063477 and batch: 500, loss is 3.6413575553894044 and perplexity is 38.14358362032867
At time: 135.1661937236786 and batch: 550, loss is 3.7126635265350343 and perplexity is 40.96276676956369
At time: 135.62031984329224 and batch: 600, loss is 3.701913146972656 and perplexity is 40.52476006640617
At time: 136.07398653030396 and batch: 650, loss is 3.6704679679870607 and perplexity is 39.270278794315416
At time: 136.52920126914978 and batch: 700, loss is 3.633704628944397 and perplexity is 37.85278771961175
At time: 136.98302221298218 and batch: 750, loss is 3.602764353752136 and perplexity is 36.699544874804225
At time: 137.43701314926147 and batch: 800, loss is 3.6024191761016846 and perplexity is 36.6868791982117
At time: 137.89115715026855 and batch: 850, loss is 3.5758489608764648 and perplexity is 35.72493701904308
At time: 138.34659361839294 and batch: 900, loss is 3.7454222774505617 and perplexity is 42.32687713130697
At time: 138.800053358078 and batch: 950, loss is 3.6689511823654173 and perplexity is 39.21075935061154
At time: 139.25368309020996 and batch: 1000, loss is 3.611323790550232 and perplexity is 37.01502053056682
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.477015797684833 and perplexity of 87.97175505597633
Finished 14 epochs...
Completing Train Step...
At time: 140.6747486591339 and batch: 50, loss is 3.8273486709594726 and perplexity is 45.94057304528402
At time: 141.12209296226501 and batch: 100, loss is 3.697114486694336 and perplexity is 40.33076134922564
At time: 141.57098984718323 and batch: 150, loss is 3.741745777130127 and perplexity is 42.17154806261686
At time: 142.01943135261536 and batch: 200, loss is 3.777450723648071 and perplexity is 43.70448479169741
At time: 142.46632528305054 and batch: 250, loss is 3.752076449394226 and perplexity is 42.609466606347844
At time: 142.91715574264526 and batch: 300, loss is 3.6810779047012328 and perplexity is 39.68915214755539
At time: 143.3886203765869 and batch: 350, loss is 3.745923752784729 and perplexity is 42.348108339179255
At time: 143.84267616271973 and batch: 400, loss is 3.640785326957703 and perplexity is 38.1217630210732
At time: 144.29541635513306 and batch: 450, loss is 3.7124564695358275 and perplexity is 40.95428602002676
At time: 144.748841047287 and batch: 500, loss is 3.627974171638489 and perplexity is 37.63649425778845
At time: 145.20280385017395 and batch: 550, loss is 3.700543274879456 and perplexity is 40.469284334511286
At time: 145.65807271003723 and batch: 600, loss is 3.6916045808792113 and perplexity is 40.109153731938385
At time: 146.11282896995544 and batch: 650, loss is 3.661527271270752 and perplexity is 38.920740030084154
At time: 146.56552624702454 and batch: 700, loss is 3.626417646408081 and perplexity is 37.57795767354563
At time: 147.01454162597656 and batch: 750, loss is 3.5974656200408934 and perplexity is 36.50559804926025
At time: 147.46419048309326 and batch: 800, loss is 3.598995409011841 and perplexity is 36.56148664851575
At time: 147.91292238235474 and batch: 850, loss is 3.574428458213806 and perplexity is 35.6742256772095
At time: 148.36051082611084 and batch: 900, loss is 3.746150879859924 and perplexity is 42.357727833548836
At time: 148.80986762046814 and batch: 950, loss is 3.6715444040298464 and perplexity is 39.31257349750737
At time: 149.25725889205933 and batch: 1000, loss is 3.6150678825378417 and perplexity is 37.15386793889156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.477016542016006 and perplexity of 87.97182053612039
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 150.65001964569092 and batch: 50, loss is 3.8215308141708375 and perplexity is 45.674073350741594
At time: 151.11486172676086 and batch: 100, loss is 3.692327618598938 and perplexity is 40.138164649722945
At time: 151.56499934196472 and batch: 150, loss is 3.737823362350464 and perplexity is 42.00645774722546
At time: 152.01460576057434 and batch: 200, loss is 3.7750600576400757 and perplexity is 43.60012675781405
At time: 152.46426510810852 and batch: 250, loss is 3.7506113481521606 and perplexity is 42.54708513264504
At time: 152.9140498638153 and batch: 300, loss is 3.6768353843688963 and perplexity is 39.5211267901175
At time: 153.3639805316925 and batch: 350, loss is 3.7422867679595946 and perplexity is 42.19436865569241
At time: 153.813871383667 and batch: 400, loss is 3.6353095293045046 and perplexity is 37.91358644715393
At time: 154.26312685012817 and batch: 450, loss is 3.705733323097229 and perplexity is 40.67986786819285
At time: 154.71252608299255 and batch: 500, loss is 3.6195899391174318 and perplexity is 37.322260284765214
At time: 155.1812653541565 and batch: 550, loss is 3.6911548805236816 and perplexity is 40.09112068628256
At time: 155.63947892189026 and batch: 600, loss is 3.6814616394042967 and perplexity is 39.704385175103326
At time: 156.09522128105164 and batch: 650, loss is 3.6503644704818727 and perplexity is 38.48869148488033
At time: 156.55157923698425 and batch: 700, loss is 3.6149645185470582 and perplexity is 37.150027765299605
At time: 157.00755763053894 and batch: 750, loss is 3.5849867773056032 and perplexity is 36.05288099947991
At time: 157.46448874473572 and batch: 800, loss is 3.583843483924866 and perplexity is 36.011685533016035
At time: 157.9224889278412 and batch: 850, loss is 3.5591852855682373 and perplexity is 35.134560847711924
At time: 158.38632893562317 and batch: 900, loss is 3.729679374694824 and perplexity is 41.665746929344174
At time: 158.839093208313 and batch: 950, loss is 3.652817130088806 and perplexity is 38.5832070036264
At time: 159.28898406028748 and batch: 1000, loss is 3.5957684659957887 and perplexity is 36.4436949702509
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.475242800828887 and perplexity of 87.81591959955986
Finished 16 epochs...
Completing Train Step...
At time: 160.66353702545166 and batch: 50, loss is 3.8164003133773803 and perplexity is 45.44034257183748
At time: 161.1242003440857 and batch: 100, loss is 3.68511531829834 and perplexity is 39.84971758652604
At time: 161.57401514053345 and batch: 150, loss is 3.7303519248962402 and perplexity is 41.69377866115048
At time: 162.02340006828308 and batch: 200, loss is 3.767194542884827 and perplexity is 43.258534478114235
At time: 162.47276616096497 and batch: 250, loss is 3.7430266618728636 and perplexity is 42.22559956458992
At time: 162.92290425300598 and batch: 300, loss is 3.6701611709594726 and perplexity is 39.25823263746581
At time: 163.37766456604004 and batch: 350, loss is 3.736014838218689 and perplexity is 41.930556709815946
At time: 163.82100701332092 and batch: 400, loss is 3.6298504114151 and perplexity is 37.70717563223539
At time: 164.2715458869934 and batch: 450, loss is 3.700800447463989 and perplexity is 40.47969326334601
At time: 164.72553062438965 and batch: 500, loss is 3.615371918678284 and perplexity is 37.16516577489039
At time: 165.17947053909302 and batch: 550, loss is 3.687397336959839 and perplexity is 39.940759225564484
At time: 165.63480949401855 and batch: 600, loss is 3.678177614212036 and perplexity is 39.57420884211757
At time: 166.08985257148743 and batch: 650, loss is 3.6477069997787477 and perplexity is 38.38654470101507
At time: 166.55846285820007 and batch: 700, loss is 3.613075408935547 and perplexity is 37.079913538361865
At time: 167.01306700706482 and batch: 750, loss is 3.5839056730270387 and perplexity is 36.013925137045824
At time: 167.46959733963013 and batch: 800, loss is 3.5836437225341795 and perplexity is 36.004492507101354
At time: 167.9269561767578 and batch: 850, loss is 3.560067405700684 and perplexity is 35.165567424932746
At time: 168.38445854187012 and batch: 900, loss is 3.7312711381912234 and perplexity is 41.73212175685572
At time: 168.84248065948486 and batch: 950, loss is 3.6551932764053343 and perplexity is 38.67499535691974
At time: 169.30184936523438 and batch: 1000, loss is 3.5984956645965576 and perplexity is 36.54321981450302
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.475100261409108 and perplexity of 87.80340326138973
Finished 17 epochs...
Completing Train Step...
At time: 170.73063254356384 and batch: 50, loss is 3.8129359245300294 and perplexity is 45.283191928456624
At time: 171.18540740013123 and batch: 100, loss is 3.681143946647644 and perplexity is 39.691773382969416
At time: 171.63841676712036 and batch: 150, loss is 3.726078763008118 and perplexity is 41.51599451594747
At time: 172.0909526348114 and batch: 200, loss is 3.762803735733032 and perplexity is 43.069010980603885
At time: 172.54342103004456 and batch: 250, loss is 3.738720474243164 and perplexity is 42.04415914869998
At time: 172.99561262130737 and batch: 300, loss is 3.666090517044067 and perplexity is 39.09875077701882
At time: 173.4484508037567 and batch: 350, loss is 3.7321905660629273 and perplexity is 41.77050907722624
At time: 173.90107011795044 and batch: 400, loss is 3.626484866142273 and perplexity is 37.5804837387717
At time: 174.35350441932678 and batch: 450, loss is 3.697625412940979 and perplexity is 40.351372658727186
At time: 174.80765104293823 and batch: 500, loss is 3.6125679492950438 and perplexity is 37.06110175228259
At time: 175.2610080242157 and batch: 550, loss is 3.684855556488037 and perplexity is 39.83936749608303
At time: 175.71199822425842 and batch: 600, loss is 3.6760474681854247 and perplexity is 39.489999719102364
At time: 176.1623969078064 and batch: 650, loss is 3.6459484338760375 and perplexity is 38.31909875383418
At time: 176.61358976364136 and batch: 700, loss is 3.611733150482178 and perplexity is 37.030176098681714
At time: 177.06436705589294 and batch: 750, loss is 3.5829843616485597 and perplexity is 35.980760377914244
At time: 177.5151879787445 and batch: 800, loss is 3.58314959526062 and perplexity is 35.986706100119214
At time: 177.96940970420837 and batch: 850, loss is 3.5601703405380247 and perplexity is 35.16918737320183
At time: 178.44034099578857 and batch: 900, loss is 3.7318408346176146 and perplexity is 41.75590317093644
At time: 178.8936505317688 and batch: 950, loss is 3.6562154483795166 and perplexity is 38.714548064666054
At time: 179.34781503677368 and batch: 1000, loss is 3.599699378013611 and perplexity is 36.587233863332195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4752018626143295 and perplexity of 87.81232464618769
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 180.76162433624268 and batch: 50, loss is 3.8111683082580567 and perplexity is 45.20321932281919
At time: 181.2315845489502 and batch: 100, loss is 3.6796865272521972 and perplexity is 39.63396795620837
At time: 181.68689036369324 and batch: 150, loss is 3.7247510814666747 and perplexity is 41.46091107108196
At time: 182.14232182502747 and batch: 200, loss is 3.761940112113953 and perplexity is 43.03183162226749
At time: 182.59538888931274 and batch: 250, loss is 3.7386764764785765 and perplexity is 42.042309340377486
At time: 183.04686331748962 and batch: 300, loss is 3.6649276876449584 and perplexity is 39.05331202402588
At time: 183.5015046596527 and batch: 350, loss is 3.7310109758377075 and perplexity is 41.721266042027636
At time: 183.95603847503662 and batch: 400, loss is 3.6251402378082274 and perplexity is 37.529985913541154
At time: 184.4104175567627 and batch: 450, loss is 3.69601224899292 and perplexity is 40.28633175402539
At time: 184.86477875709534 and batch: 500, loss is 3.6101283597946168 and perplexity is 36.97079807431336
At time: 185.32030582427979 and batch: 550, loss is 3.6813250827789306 and perplexity is 39.69896364843269
At time: 185.77577686309814 and batch: 600, loss is 3.6724444818496704 and perplexity is 39.34797380207723
At time: 186.23082447052002 and batch: 650, loss is 3.642245736122131 and perplexity is 38.17747706590195
At time: 186.68526601791382 and batch: 700, loss is 3.6077570724487305 and perplexity is 36.88323355002792
At time: 187.1415514945984 and batch: 750, loss is 3.5782895374298094 and perplexity is 35.81223294556632
At time: 187.5962152481079 and batch: 800, loss is 3.576816620826721 and perplexity is 35.75952334102236
At time: 188.05143904685974 and batch: 850, loss is 3.5539984273910523 and perplexity is 34.95279466868516
At time: 188.50684642791748 and batch: 900, loss is 3.7255708312988283 and perplexity is 41.49491258043515
At time: 188.96297764778137 and batch: 950, loss is 3.6497434091567995 and perplexity is 38.46479506848451
At time: 189.41826581954956 and batch: 1000, loss is 3.593393864631653 and perplexity is 36.35725838928798
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.475528623999619 and perplexity of 87.84102301153801
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 190.8490116596222 and batch: 50, loss is 3.8106863784790037 and perplexity is 45.18143979384179
At time: 191.32076120376587 and batch: 100, loss is 3.6783765029907225 and perplexity is 39.58208049094702
At time: 191.77641797065735 and batch: 150, loss is 3.7237531328201294 and perplexity is 41.419555849619826
At time: 192.23201513290405 and batch: 200, loss is 3.760590558052063 and perplexity is 42.97379700834375
At time: 192.69039940834045 and batch: 250, loss is 3.7373881769180297 and perplexity is 41.988181125893824
At time: 193.14620661735535 and batch: 300, loss is 3.6637692975997926 and perplexity is 39.008099248217
At time: 193.59998607635498 and batch: 350, loss is 3.7299092531204225 and perplexity is 41.675326086628296
At time: 194.054132938385 and batch: 400, loss is 3.6238881397247313 and perplexity is 37.48302409664101
At time: 194.50906467437744 and batch: 450, loss is 3.6951357984542845 and perplexity is 40.25103824562642
At time: 194.96188521385193 and batch: 500, loss is 3.6093395137786866 and perplexity is 36.94164530758039
At time: 195.4164333343506 and batch: 550, loss is 3.680280957221985 and perplexity is 39.65753457824243
At time: 195.87128114700317 and batch: 600, loss is 3.6713850545883178 and perplexity is 39.30630955996622
At time: 196.32643580436707 and batch: 650, loss is 3.64125816822052 and perplexity is 38.13979282592282
At time: 196.7814519405365 and batch: 700, loss is 3.6067598247528077 and perplexity is 36.8464701644995
At time: 197.23904609680176 and batch: 750, loss is 3.5771136140823363 and perplexity is 35.77014525551906
At time: 197.69818091392517 and batch: 800, loss is 3.574973020553589 and perplexity is 35.693657807537186
At time: 198.15640711784363 and batch: 850, loss is 3.552250475883484 and perplexity is 34.89175224369179
At time: 198.6127450466156 and batch: 900, loss is 3.723651237487793 and perplexity is 41.415335605226545
At time: 199.07013154029846 and batch: 950, loss is 3.64788507938385 and perplexity is 38.3933811704365
At time: 199.52829003334045 and batch: 1000, loss is 3.591620054244995 and perplexity is 36.292824670221265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.475640273675686 and perplexity of 87.85083098082076
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 200.96851921081543 and batch: 50, loss is 3.810543279647827 and perplexity is 45.17497484519075
At time: 201.42378067970276 and batch: 100, loss is 3.6779982948303225 and perplexity is 39.567113055681226
At time: 201.89466524124146 and batch: 150, loss is 3.7234961414337158 and perplexity is 41.40891274818866
At time: 202.34850358963013 and batch: 200, loss is 3.7602763843536375 and perplexity is 42.960297892246906
At time: 202.80219650268555 and batch: 250, loss is 3.736990361213684 and perplexity is 41.97148089007331
At time: 203.25789523124695 and batch: 300, loss is 3.6634538173675537 and perplexity is 38.99579490499755
At time: 203.7123987674713 and batch: 350, loss is 3.7295791578292845 and perplexity is 41.661571528012864
At time: 204.1659541130066 and batch: 400, loss is 3.623415455818176 and perplexity is 37.46531066113926
At time: 204.62059783935547 and batch: 450, loss is 3.6948404836654665 and perplexity is 40.23915327375756
At time: 205.07436966896057 and batch: 500, loss is 3.609081382751465 and perplexity is 36.93211075336496
At time: 205.52616906166077 and batch: 550, loss is 3.679971070289612 and perplexity is 39.64524713046458
At time: 205.97657442092896 and batch: 600, loss is 3.671060872077942 and perplexity is 39.29356920707084
At time: 206.428453207016 and batch: 650, loss is 3.6409548473358155 and perplexity is 38.12822598454138
At time: 206.88332891464233 and batch: 700, loss is 3.606478099822998 and perplexity is 36.836091057373935
At time: 207.36190581321716 and batch: 750, loss is 3.5767910480499268 and perplexity is 35.75860888240636
At time: 207.84453749656677 and batch: 800, loss is 3.5744970512390135 and perplexity is 35.67667276419622
At time: 208.3174753189087 and batch: 850, loss is 3.55177677154541 and perplexity is 34.8752277834542
At time: 208.80944967269897 and batch: 900, loss is 3.7231192588806152 and perplexity is 41.39330939193207
At time: 209.2997281551361 and batch: 950, loss is 3.6473676490783693 and perplexity is 38.37352041021176
At time: 209.78907370567322 and batch: 1000, loss is 3.5911314725875854 and perplexity is 36.275096992853456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.475663347942073 and perplexity of 87.85285809768419
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 211.2324788570404 and batch: 50, loss is 3.8104982662200926 and perplexity is 45.17294141049142
At time: 211.7011251449585 and batch: 100, loss is 3.677898769378662 and perplexity is 39.56317531683933
At time: 212.15546798706055 and batch: 150, loss is 3.7234188747406005 and perplexity is 41.40571334204047
At time: 212.60790848731995 and batch: 200, loss is 3.7601887369155884 and perplexity is 42.956532697206086
At time: 213.06065106391907 and batch: 250, loss is 3.73688750743866 and perplexity is 41.967164186818835
At time: 213.5290195941925 and batch: 300, loss is 3.6633715343475344 and perplexity is 38.992586345231516
At time: 213.98126673698425 and batch: 350, loss is 3.72949182510376 and perplexity is 41.65793326829357
At time: 214.43481993675232 and batch: 400, loss is 3.623288435935974 and perplexity is 37.460552124013226
At time: 214.88768553733826 and batch: 450, loss is 3.694759335517883 and perplexity is 40.235888073493335
At time: 215.33971118927002 and batch: 500, loss is 3.609011340141296 and perplexity is 36.9295240225205
At time: 215.79287910461426 and batch: 550, loss is 3.6798857975006105 and perplexity is 39.6418666138062
At time: 216.24574303627014 and batch: 600, loss is 3.6709685707092286 and perplexity is 39.289942524227854
At time: 216.69955730438232 and batch: 650, loss is 3.640865936279297 and perplexity is 38.12483611438662
At time: 217.1539249420166 and batch: 700, loss is 3.606398067474365 and perplexity is 36.83314309645985
At time: 217.60711526870728 and batch: 750, loss is 3.576699385643005 and perplexity is 35.755331312465295
At time: 218.06058406829834 and batch: 800, loss is 3.574370107650757 and perplexity is 35.67214412678529
At time: 218.51743745803833 and batch: 850, loss is 3.551649136543274 and perplexity is 34.8707767677403
At time: 218.97255325317383 and batch: 900, loss is 3.7229769372940065 and perplexity is 41.38741864966426
At time: 219.4276897907257 and batch: 950, loss is 3.6472280645370483 and perplexity is 38.36816443378088
At time: 219.8800494670868 and batch: 1000, loss is 3.5909990453720093 and perplexity is 36.2702935008275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.475668558260289 and perplexity of 87.85331584022356
Annealing...
Model not improving. Stopping early with 87.80340326138973loss at 21 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -87.80340326138973
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f75e40c6898>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.9146541825277869, 'anneal': 7.330669139294035, 'tune_wordvecs': True, 'lr': 5.188309388641641, 'wordvec_dim': 200, 'seq_len': 20, 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7264964580535889 and batch: 50, loss is 8.403864154815674 and perplexity is 4464.284146011166
At time: 1.2045726776123047 and batch: 100, loss is 7.316885328292846 and perplexity is 1505.5074979672988
At time: 1.6579511165618896 and batch: 150, loss is 6.932088613510132 and perplexity is 1024.6318061320142
At time: 2.112250328063965 and batch: 200, loss is 6.752306842803955 and perplexity is 856.0312160320234
At time: 2.5661516189575195 and batch: 250, loss is 6.657588396072388 and perplexity is 778.6708253747719
At time: 3.0205118656158447 and batch: 300, loss is 6.495400762557983 and perplexity is 662.0895128343129
At time: 3.475177049636841 and batch: 350, loss is 6.493673906326294 and perplexity is 660.9471660513459
At time: 3.9286012649536133 and batch: 400, loss is 6.397659254074097 and perplexity is 600.4379190479631
At time: 4.382353067398071 and batch: 450, loss is 6.333457717895508 and perplexity is 563.1002734610981
At time: 4.836266279220581 and batch: 500, loss is 6.309477987289429 and perplexity is 549.7578932872087
At time: 5.290132284164429 and batch: 550, loss is 6.360300741195679 and perplexity is 578.4202850467898
At time: 5.75994348526001 and batch: 600, loss is 6.377832708358764 and perplexity is 588.6505467587167
At time: 6.217780351638794 and batch: 650, loss is 6.324679594039917 and perplexity is 558.1789411510684
At time: 6.671875 and batch: 700, loss is 6.3049490261077885 and perplexity is 547.2736908037409
At time: 7.1258862018585205 and batch: 750, loss is 6.208989877700805 and perplexity is 497.1987659830761
At time: 7.5796592235565186 and batch: 800, loss is 6.249274158477784 and perplexity is 517.6369658746182
At time: 8.033623456954956 and batch: 850, loss is 6.219644680023193 and perplexity is 502.52464324945464
At time: 8.487319231033325 and batch: 900, loss is 6.256289625167847 and perplexity is 521.2811988172306
At time: 8.941368103027344 and batch: 950, loss is 6.278364543914795 and perplexity is 532.9163893847891
At time: 9.395577907562256 and batch: 1000, loss is 6.191590871810913 and perplexity is 488.62282449867763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.988640389791349 and perplexity of 398.87193074564306
Finished 1 epochs...
Completing Train Step...
At time: 10.81370210647583 and batch: 50, loss is 5.718281717300415 and perplexity is 304.38145996027237
At time: 11.259610891342163 and batch: 100, loss is 5.472850322723389 and perplexity is 238.13799646525183
At time: 11.706467866897583 and batch: 150, loss is 5.337169523239136 and perplexity is 207.92335438085283
At time: 12.1530020236969 and batch: 200, loss is 5.241685581207276 and perplexity is 188.98838935854954
At time: 12.600290298461914 and batch: 250, loss is 5.173086414337158 and perplexity is 176.45862230155112
At time: 13.047915935516357 and batch: 300, loss is 4.99697093963623 and perplexity is 147.9642868586463
At time: 13.495064735412598 and batch: 350, loss is 5.024791898727417 and perplexity is 152.13859258344792
At time: 13.94236421585083 and batch: 400, loss is 4.9227426242828365 and perplexity is 137.378875617593
At time: 14.389646291732788 and batch: 450, loss is 4.916669807434082 and perplexity is 136.54712695191222
At time: 14.836829423904419 and batch: 500, loss is 4.877259321212769 and perplexity is 131.27040042628437
At time: 15.284132242202759 and batch: 550, loss is 4.908977327346801 and perplexity is 135.50077059435588
At time: 15.731667280197144 and batch: 600, loss is 4.895626916885376 and perplexity is 133.70380151639242
At time: 16.178946256637573 and batch: 650, loss is 4.860458641052246 and perplexity is 129.08339147549748
At time: 16.6271755695343 and batch: 700, loss is 4.821709623336792 and perplexity is 124.17720565857773
At time: 17.074778079986572 and batch: 750, loss is 4.747809314727784 and perplexity is 115.33135288533933
At time: 17.539049863815308 and batch: 800, loss is 4.771998567581177 and perplexity is 118.1551471926021
At time: 17.98709273338318 and batch: 850, loss is 4.734346151351929 and perplexity is 113.78903358370077
At time: 18.434812545776367 and batch: 900, loss is 4.872953262329101 and perplexity is 130.70635762467109
At time: 18.88259506225586 and batch: 950, loss is 4.766246652603149 and perplexity is 117.47747964315592
At time: 19.3308744430542 and batch: 1000, loss is 4.720999975204467 and perplexity is 112.28047423924853
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.82197719667016 and perplexity of 124.21043661308443
Finished 2 epochs...
Completing Train Step...
At time: 20.7212016582489 and batch: 50, loss is 4.750744438171386 and perplexity is 115.67036191627389
At time: 21.180973768234253 and batch: 100, loss is 4.630071144104004 and perplexity is 102.52135764117523
At time: 21.62488341331482 and batch: 150, loss is 4.644276103973389 and perplexity is 103.98806198775583
At time: 22.06889295578003 and batch: 200, loss is 4.650362787246704 and perplexity is 104.62293455936543
At time: 22.512568473815918 and batch: 250, loss is 4.62125678062439 and perplexity is 101.6216680509937
At time: 22.957001447677612 and batch: 300, loss is 4.502437906265259 and perplexity is 90.2368523497373
At time: 23.400975942611694 and batch: 350, loss is 4.556631374359131 and perplexity is 95.262036584016
At time: 23.845730781555176 and batch: 400, loss is 4.477972640991211 and perplexity is 88.05597052503062
At time: 24.289652824401855 and batch: 450, loss is 4.520036487579346 and perplexity is 91.83894889795789
At time: 24.73347544670105 and batch: 500, loss is 4.467943868637085 and perplexity is 87.17729064696486
At time: 25.177847862243652 and batch: 550, loss is 4.519708528518676 and perplexity is 91.80883442097216
At time: 25.62108016014099 and batch: 600, loss is 4.519466829299927 and perplexity is 91.78664697887002
At time: 26.065349102020264 and batch: 650, loss is 4.499179697036743 and perplexity is 89.94332025881913
At time: 26.51253914833069 and batch: 700, loss is 4.465194139480591 and perplexity is 86.93790598141847
At time: 26.962652921676636 and batch: 750, loss is 4.414617347717285 and perplexity is 82.65020856342646
At time: 27.41690421104431 and batch: 800, loss is 4.432498655319214 and perplexity is 84.14139481353742
At time: 27.876789808273315 and batch: 850, loss is 4.411542329788208 and perplexity is 82.396448049275
At time: 28.338457822799683 and batch: 900, loss is 4.568719711303711 and perplexity is 96.42058453126735
At time: 28.817030906677246 and batch: 950, loss is 4.475547885894775 and perplexity is 87.8427150124092
At time: 29.279147148132324 and batch: 1000, loss is 4.427754230499268 and perplexity is 83.74313778890455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.682774241377667 and perplexity of 108.06946787139452
Finished 3 epochs...
Completing Train Step...
At time: 30.6459698677063 and batch: 50, loss is 4.4832234191894536 and perplexity is 88.51954890381404
At time: 31.104784727096558 and batch: 100, loss is 4.358718423843384 and perplexity is 78.15690618430128
At time: 31.560893774032593 and batch: 150, loss is 4.38933274269104 and perplexity is 80.58662901938277
At time: 32.016069650650024 and batch: 200, loss is 4.40816086769104 and perplexity is 82.11829812500534
At time: 32.47114419937134 and batch: 250, loss is 4.38172080039978 and perplexity is 79.9755369990746
At time: 32.9266631603241 and batch: 300, loss is 4.284977960586548 and perplexity is 72.60094661984225
At time: 33.3827600479126 and batch: 350, loss is 4.340323343276977 and perplexity is 76.73234621578649
At time: 33.83757019042969 and batch: 400, loss is 4.2640792179107665 and perplexity is 71.09942274866505
At time: 34.2922899723053 and batch: 450, loss is 4.314981174468994 and perplexity is 74.81221505101207
At time: 34.74730920791626 and batch: 500, loss is 4.2565826892852785 and perplexity is 70.5684167284741
At time: 35.203455448150635 and batch: 550, loss is 4.315611629486084 and perplexity is 74.85939565839315
At time: 35.66312122344971 and batch: 600, loss is 4.3193958568573 and perplexity is 75.14321731657735
At time: 36.12590932846069 and batch: 650, loss is 4.296999506950378 and perplexity is 73.4789894073713
At time: 36.58838224411011 and batch: 700, loss is 4.26561562538147 and perplexity is 71.20874439272919
At time: 37.053303241729736 and batch: 750, loss is 4.224853019714356 and perplexity is 68.36445458301378
At time: 37.51554489135742 and batch: 800, loss is 4.239381279945373 and perplexity is 69.36492109130074
At time: 37.978041648864746 and batch: 850, loss is 4.2259289121627805 and perplexity is 68.43804696508552
At time: 38.437384843826294 and batch: 900, loss is 4.389028367996215 and perplexity is 80.56210422132138
At time: 38.899186849594116 and batch: 950, loss is 4.3069898128509525 and perplexity is 74.21674606008855
At time: 39.36148190498352 and batch: 1000, loss is 4.250649352073669 and perplexity is 70.15095022502835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6116489317359 and perplexity of 100.64997782178493
Finished 4 epochs...
Completing Train Step...
At time: 40.82210683822632 and batch: 50, loss is 4.309796710014343 and perplexity is 74.4253574725065
At time: 41.285091400146484 and batch: 100, loss is 4.185288691520691 and perplexity is 65.71246881122029
At time: 41.74798011779785 and batch: 150, loss is 4.220457649230957 and perplexity is 68.06462688683335
At time: 42.210591316223145 and batch: 200, loss is 4.248476724624634 and perplexity is 69.99870379227052
At time: 42.673959255218506 and batch: 250, loss is 4.225503344535827 and perplexity is 68.40892814429846
At time: 43.1327908039093 and batch: 300, loss is 4.13637861251831 and perplexity is 62.57579941327589
At time: 43.590739011764526 and batch: 350, loss is 4.19299394607544 and perplexity is 66.22075583650233
At time: 44.05483269691467 and batch: 400, loss is 4.112574768066406 and perplexity is 61.10384342628727
At time: 44.51778197288513 and batch: 450, loss is 4.172273826599121 and perplexity is 64.86277124939731
At time: 44.98073697090149 and batch: 500, loss is 4.107939901351929 and perplexity is 60.82129055951906
At time: 45.44402885437012 and batch: 550, loss is 4.175026679039002 and perplexity is 65.0415748845829
At time: 45.907578468322754 and batch: 600, loss is 4.179087157249451 and perplexity is 65.30621169457588
At time: 46.370376110076904 and batch: 650, loss is 4.154528751373291 and perplexity is 63.72192859982522
At time: 46.83256936073303 and batch: 700, loss is 4.12378182888031 and perplexity is 61.79248955735217
At time: 47.29672884941101 and batch: 750, loss is 4.088616590499878 and perplexity is 59.65730410982823
At time: 47.758482694625854 and batch: 800, loss is 4.096631627082825 and perplexity is 60.13738093131093
At time: 48.2212450504303 and batch: 850, loss is 4.089426574707031 and perplexity is 59.70564515909878
At time: 48.68404293060303 and batch: 900, loss is 4.254990849494934 and perplexity is 70.45617247591967
At time: 49.145707845687866 and batch: 950, loss is 4.178692960739136 and perplexity is 65.2804732871525
At time: 49.60778880119324 and batch: 1000, loss is 4.117124118804932 and perplexity is 61.382459522142625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.581491237733422 and perplexity of 97.65991983092991
Finished 5 epochs...
Completing Train Step...
At time: 51.03161072731018 and batch: 50, loss is 4.18284423828125 and perplexity is 65.5520339216437
At time: 51.50391697883606 and batch: 100, loss is 4.054014320373535 and perplexity is 57.62833191946694
At time: 51.9606556892395 and batch: 150, loss is 4.098513951301575 and perplexity is 60.250685584481936
At time: 52.43348526954651 and batch: 200, loss is 4.122976822853088 and perplexity is 61.7427662472888
At time: 52.89101243019104 and batch: 250, loss is 4.101427965164184 and perplexity is 60.42651297481376
At time: 53.349753618240356 and batch: 300, loss is 4.0190905618667605 and perplexity is 55.65047214468362
At time: 53.80694627761841 and batch: 350, loss is 4.077000813484192 and perplexity is 58.96834729879067
At time: 54.26379346847534 and batch: 400, loss is 4.000429921150207 and perplexity is 54.62162797907681
At time: 54.721075773239136 and batch: 450, loss is 4.064328856468201 and perplexity is 58.22581752536849
At time: 55.18118643760681 and batch: 500, loss is 3.9923319959640504 and perplexity is 54.181092242268136
At time: 55.645684242248535 and batch: 550, loss is 4.061817903518676 and perplexity is 58.0797986371131
At time: 56.10344481468201 and batch: 600, loss is 4.066180806159974 and perplexity is 58.33374872091217
At time: 56.56234669685364 and batch: 650, loss is 4.040008087158203 and perplexity is 56.82680237095166
At time: 57.02038097381592 and batch: 700, loss is 4.013822193145752 and perplexity is 55.35805589259661
At time: 57.479942083358765 and batch: 750, loss is 3.9822131538391115 and perplexity is 53.635606818544275
At time: 57.93851065635681 and batch: 800, loss is 3.9841466426849363 and perplexity is 53.7394109858285
At time: 58.397446155548096 and batch: 850, loss is 3.981656131744385 and perplexity is 53.605738919794305
At time: 58.855700731277466 and batch: 900, loss is 4.1458921194076535 and perplexity is 63.17395548161225
At time: 59.3164701461792 and batch: 950, loss is 4.0733485651016235 and perplexity is 58.753373056370776
At time: 59.775505781173706 and batch: 1000, loss is 4.012068023681641 and perplexity is 55.261033603005146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.568747915872714 and perplexity of 96.4233040706486
Finished 6 epochs...
Completing Train Step...
At time: 61.196743965148926 and batch: 50, loss is 4.077334027290345 and perplexity is 58.987999640270544
At time: 61.66749429702759 and batch: 100, loss is 3.9502816915512087 and perplexity is 51.94999864961028
At time: 62.1204948425293 and batch: 150, loss is 3.9959811115264894 and perplexity is 54.37916648726724
At time: 62.57150101661682 and batch: 200, loss is 4.020411257743835 and perplexity is 55.724018048998545
At time: 63.01828050613403 and batch: 250, loss is 4.001871223449707 and perplexity is 54.70041101853511
At time: 63.46699810028076 and batch: 300, loss is 3.923523254394531 and perplexity is 50.57833155278273
At time: 63.92398142814636 and batch: 350, loss is 3.9834038734436037 and perplexity is 53.69950982481292
At time: 64.39505767822266 and batch: 400, loss is 3.909093322753906 and perplexity is 49.85373023290914
At time: 64.85040235519409 and batch: 450, loss is 3.9729704523086546 and perplexity is 53.14215285006491
At time: 65.30633020401001 and batch: 500, loss is 3.8970605373382567 and perplexity is 49.25744567148368
At time: 65.76225566864014 and batch: 550, loss is 3.9677271127700804 and perplexity is 52.864239731994125
At time: 66.21791362762451 and batch: 600, loss is 3.9717340230941773 and perplexity is 53.076486943748954
At time: 66.67310762405396 and batch: 650, loss is 3.9488927221298216 and perplexity is 51.877891778756805
At time: 67.12810206413269 and batch: 700, loss is 3.921746072769165 and perplexity is 50.48852449666746
At time: 67.5850477218628 and batch: 750, loss is 3.893923182487488 and perplexity is 49.10314975245592
At time: 68.04244828224182 and batch: 800, loss is 3.8934421300888062 and perplexity is 49.079534245088325
At time: 68.50038743019104 and batch: 850, loss is 3.8950911378860473 and perplexity is 49.16053354562789
At time: 68.95995354652405 and batch: 900, loss is 4.05767942905426 and perplexity is 57.83993355366311
At time: 69.41742277145386 and batch: 950, loss is 3.9870115613937376 and perplexity is 53.89359078059516
At time: 69.8763997554779 and batch: 1000, loss is 3.9224274158477783 and perplexity is 50.52293622514793
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5661427567644814 and perplexity of 96.17243294328978
Finished 7 epochs...
Completing Train Step...
At time: 71.30753469467163 and batch: 50, loss is 3.9900081872940065 and perplexity is 54.05533192841
At time: 71.76430583000183 and batch: 100, loss is 3.862899923324585 and perplexity is 47.60319702270938
At time: 72.21706032752991 and batch: 150, loss is 3.9094834995269774 and perplexity is 49.87318579580453
At time: 72.66970038414001 and batch: 200, loss is 3.932635688781738 and perplexity is 51.04132959631295
At time: 73.127361536026 and batch: 250, loss is 3.914700722694397 and perplexity is 50.13406527766106
At time: 73.58419466018677 and batch: 300, loss is 3.8459278678894044 and perplexity is 46.802090367082336
At time: 74.04371333122253 and batch: 350, loss is 3.90165518283844 and perplexity is 49.4842869010401
At time: 74.49991035461426 and batch: 400, loss is 3.8271840381622315 and perplexity is 45.93301034278823
At time: 74.95582365989685 and batch: 450, loss is 3.8908771896362304 and perplexity is 48.953809469518646
At time: 75.41184878349304 and batch: 500, loss is 3.815302515029907 and perplexity is 45.39048561030564
At time: 75.88340449333191 and batch: 550, loss is 3.8901310634613036 and perplexity is 48.91729737392004
At time: 76.33982992172241 and batch: 600, loss is 3.8952222585678102 and perplexity is 49.166979930920206
At time: 76.7969114780426 and batch: 650, loss is 3.8660918283462524 and perplexity is 47.75538466136708
At time: 77.25300669670105 and batch: 700, loss is 3.8442648458480835 and perplexity is 46.724322142281224
At time: 77.71067547798157 and batch: 750, loss is 3.815299491882324 and perplexity is 45.3903483883762
At time: 78.16727781295776 and batch: 800, loss is 3.8138406753540037 and perplexity is 45.32418047307983
At time: 78.62478971481323 and batch: 850, loss is 3.81742488861084 and perplexity is 45.48692348018416
At time: 79.08121752738953 and batch: 900, loss is 3.9766808319091798 and perplexity is 53.33969666456683
At time: 79.53713941574097 and batch: 950, loss is 3.908435125350952 and perplexity is 49.82092743368515
At time: 80.00415539741516 and batch: 1000, loss is 3.8426510524749755 and perplexity is 46.64897955088381
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.570174426567264 and perplexity of 96.56095109924655
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 81.43752908706665 and batch: 50, loss is 3.93372700214386 and perplexity is 51.09706208659625
At time: 81.90813207626343 and batch: 100, loss is 3.8162080907821654 and perplexity is 45.43160875070665
At time: 82.36363577842712 and batch: 150, loss is 3.856820249557495 and perplexity is 47.31466309934564
At time: 82.81664681434631 and batch: 200, loss is 3.8687191820144653 and perplexity is 47.88101991828906
At time: 83.27072525024414 and batch: 250, loss is 3.8412348079681395 and perplexity is 46.58295995082822
At time: 83.72619342803955 and batch: 300, loss is 3.7540684175491332 and perplexity is 42.694427898926456
At time: 84.18403482437134 and batch: 350, loss is 3.8149892807006838 and perplexity is 45.37626997852055
At time: 84.64106774330139 and batch: 400, loss is 3.7225336265563964 and perplexity is 41.36907522879304
At time: 85.09916114807129 and batch: 450, loss is 3.7739631271362306 and perplexity is 43.55232667028193
At time: 85.55930256843567 and batch: 500, loss is 3.684178614616394 and perplexity is 39.81240768622675
At time: 86.02150440216064 and batch: 550, loss is 3.74441876411438 and perplexity is 42.28442285090781
At time: 86.48388266563416 and batch: 600, loss is 3.731913251876831 and perplexity is 41.75892712849222
At time: 86.94625401496887 and batch: 650, loss is 3.693447985649109 and perplexity is 40.183159327421464
At time: 87.40788865089417 and batch: 700, loss is 3.6496760177612306 and perplexity is 38.462202959608476
At time: 87.88469171524048 and batch: 750, loss is 3.6032810974121094 and perplexity is 36.7185140326161
At time: 88.34580206871033 and batch: 800, loss is 3.578725972175598 and perplexity is 35.827866059516964
At time: 88.8076536655426 and batch: 850, loss is 3.561314387321472 and perplexity is 35.20944559314515
At time: 89.26828813552856 and batch: 900, loss is 3.7004846477508546 and perplexity is 40.466911806122155
At time: 89.72961282730103 and batch: 950, loss is 3.6104296541213987 and perplexity is 36.981938844270864
At time: 90.19191861152649 and batch: 1000, loss is 3.5167041063308715 and perplexity is 33.673261877217726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4954164086318595 and perplexity of 89.60547371031643
Finished 9 epochs...
Completing Train Step...
At time: 91.60731148719788 and batch: 50, loss is 3.8389518213272096 and perplexity is 46.476732979082364
At time: 92.07565975189209 and batch: 100, loss is 3.7145543813705446 and perplexity is 41.04029468910758
At time: 92.52769136428833 and batch: 150, loss is 3.756774444580078 and perplexity is 42.81011663271707
At time: 92.98059511184692 and batch: 200, loss is 3.7757518911361694 and perplexity is 43.63030122258823
At time: 93.43647074699402 and batch: 250, loss is 3.756535906791687 and perplexity is 42.799906020031635
At time: 93.89406824111938 and batch: 300, loss is 3.6771914052963255 and perplexity is 39.535199643296835
At time: 94.35147523880005 and batch: 350, loss is 3.736712961196899 and perplexity is 41.95983961528949
At time: 94.80919933319092 and batch: 400, loss is 3.6538987731933594 and perplexity is 38.624962841819624
At time: 95.26680326461792 and batch: 450, loss is 3.708661370277405 and perplexity is 40.799154994535925
At time: 95.72412848472595 and batch: 500, loss is 3.622219738960266 and perplexity is 37.42053952972744
At time: 96.18091297149658 and batch: 550, loss is 3.6846201038360595 and perplexity is 39.829988315572756
At time: 96.63957262039185 and batch: 600, loss is 3.67723473072052 and perplexity is 39.53691255969814
At time: 97.10295033454895 and batch: 650, loss is 3.6425056934356688 and perplexity is 38.18740287036447
At time: 97.56050109863281 and batch: 700, loss is 3.6061011028289793 and perplexity is 36.82220657914163
At time: 98.01844549179077 and batch: 750, loss is 3.5659908866882324 and perplexity is 35.37448815309392
At time: 98.47693753242493 and batch: 800, loss is 3.54733314037323 and perplexity is 34.72059894534118
At time: 98.93468523025513 and batch: 850, loss is 3.5383921384811403 and perplexity is 34.41154568620979
At time: 99.41078329086304 and batch: 900, loss is 3.6860266733169555 and perplexity is 39.88605138061693
At time: 99.86907625198364 and batch: 950, loss is 3.605224838256836 and perplexity is 36.789954716694105
At time: 100.3307614326477 and batch: 1000, loss is 3.523047890663147 and perplexity is 33.887556790020106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.493547765220084 and perplexity of 89.43818937826846
Finished 10 epochs...
Completing Train Step...
At time: 101.77474594116211 and batch: 50, loss is 3.8022369146347046 and perplexity is 44.80128914812435
At time: 102.22989320755005 and batch: 100, loss is 3.675427122116089 and perplexity is 39.465509849880966
At time: 102.68522810935974 and batch: 150, loss is 3.717160520553589 and perplexity is 41.14739090238333
At time: 103.14010810852051 and batch: 200, loss is 3.7359985446929933 and perplexity is 41.929873518778564
At time: 103.59531569480896 and batch: 250, loss is 3.7177529764175414 and perplexity is 41.17177613828452
At time: 104.05000138282776 and batch: 300, loss is 3.6406746244430543 and perplexity is 38.1175430796273
At time: 104.50554633140564 and batch: 350, loss is 3.6994572401046755 and perplexity is 40.42535714196045
At time: 104.9625244140625 and batch: 400, loss is 3.6202026176452637 and perplexity is 37.34513383860398
At time: 105.42253422737122 and batch: 450, loss is 3.6756921911239626 and perplexity is 39.47597231999914
At time: 105.88146781921387 and batch: 500, loss is 3.5900552797317506 and perplexity is 36.236078991837864
At time: 106.34031796455383 and batch: 550, loss is 3.654092650413513 and perplexity is 38.63245206821568
At time: 106.79995131492615 and batch: 600, loss is 3.648765230178833 and perplexity is 38.427188010772014
At time: 107.25995016098022 and batch: 650, loss is 3.615204863548279 and perplexity is 37.158957661853215
At time: 107.71987795829773 and batch: 700, loss is 3.582854161262512 and perplexity is 35.97607597398497
At time: 108.18249797821045 and batch: 750, loss is 3.545882291793823 and perplexity is 34.67026113878267
At time: 108.65047764778137 and batch: 800, loss is 3.529673767089844 and perplexity is 34.112837069167185
At time: 109.11301970481873 and batch: 850, loss is 3.5244773387908936 and perplexity is 33.93603193272431
At time: 109.5750789642334 and batch: 900, loss is 3.676117043495178 and perplexity is 39.492747343647295
At time: 110.03578925132751 and batch: 950, loss is 3.599202136993408 and perplexity is 36.569045712161774
At time: 110.50000834465027 and batch: 1000, loss is 3.5212167739868163 and perplexity is 33.825561497291446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.494841784965701 and perplexity of 89.55399907519332
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 111.98080587387085 and batch: 50, loss is 3.784488115310669 and perplexity is 44.01313514146609
At time: 112.47390294075012 and batch: 100, loss is 3.6617017078399656 and perplexity is 38.92752982262309
At time: 112.94214153289795 and batch: 150, loss is 3.7068417692184448 and perplexity is 40.72498430989595
At time: 113.3910083770752 and batch: 200, loss is 3.725011649131775 and perplexity is 41.471715851499745
At time: 113.8537347316742 and batch: 250, loss is 3.7076995515823366 and perplexity is 40.759932470021575
At time: 114.31746411323547 and batch: 300, loss is 3.629104566574097 and perplexity is 37.67906241517348
At time: 114.77925705909729 and batch: 350, loss is 3.688264489173889 and perplexity is 39.975408964485446
At time: 115.24171733856201 and batch: 400, loss is 3.6030846786499025 and perplexity is 36.71130253579956
At time: 115.70450639724731 and batch: 450, loss is 3.655439839363098 and perplexity is 38.68453235385321
At time: 116.16711735725403 and batch: 500, loss is 3.5658605337142943 and perplexity is 35.369877283888414
At time: 116.62985062599182 and batch: 550, loss is 3.6256172180175783 and perplexity is 37.54789124398379
At time: 117.09159517288208 and batch: 600, loss is 3.618528275489807 and perplexity is 37.28265762459184
At time: 117.55322027206421 and batch: 650, loss is 3.5780836391448974 and perplexity is 35.80486002728562
At time: 118.01540064811707 and batch: 700, loss is 3.543516936302185 and perplexity is 34.58835055821248
At time: 118.47948718070984 and batch: 750, loss is 3.502671551704407 and perplexity is 33.204039881734104
At time: 118.94021582603455 and batch: 800, loss is 3.4806429529190064 and perplexity is 32.48059885959795
At time: 119.3964855670929 and batch: 850, loss is 3.4696152925491335 and perplexity is 32.12438157829924
At time: 119.85757422447205 and batch: 900, loss is 3.6146226739883422 and perplexity is 37.13733040083801
At time: 120.31998205184937 and batch: 950, loss is 3.531956658363342 and perplexity is 34.190801926012234
At time: 120.78309226036072 and batch: 1000, loss is 3.452135362625122 and perplexity is 31.56772893763438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.48616772163205 and perplexity of 88.78056128901287
Finished 12 epochs...
Completing Train Step...
At time: 122.20570015907288 and batch: 50, loss is 3.772023015022278 and perplexity is 43.46791218697717
At time: 122.67005658149719 and batch: 100, loss is 3.6450171661376953 and perplexity is 38.28343002454865
At time: 123.11130785942078 and batch: 150, loss is 3.690067014694214 and perplexity is 40.0475306403825
At time: 123.57570147514343 and batch: 200, loss is 3.707786297798157 and perplexity is 40.7634683932822
At time: 124.03322958946228 and batch: 250, loss is 3.691515073776245 and perplexity is 40.10556383844829
At time: 124.49474763870239 and batch: 300, loss is 3.614504518508911 and perplexity is 37.13294268098138
At time: 124.95413088798523 and batch: 350, loss is 3.673219556808472 and perplexity is 39.37848325327925
At time: 125.41504168510437 and batch: 400, loss is 3.5894307136535644 and perplexity is 36.213454232157254
At time: 125.8763120174408 and batch: 450, loss is 3.6438768815994265 and perplexity is 38.239800900754794
At time: 126.33761715888977 and batch: 500, loss is 3.5551925611495974 and perplexity is 34.99455791124072
At time: 126.79801154136658 and batch: 550, loss is 3.6157977056503294 and perplexity is 37.180993587690374
At time: 127.25883364677429 and batch: 600, loss is 3.6096259212493895 and perplexity is 36.952227186068725
At time: 127.72150325775146 and batch: 650, loss is 3.570985770225525 and perplexity is 35.551621613840425
At time: 128.1830861568451 and batch: 700, loss is 3.5377708292007446 and perplexity is 34.390172114009125
At time: 128.64381408691406 and batch: 750, loss is 3.4978604030609133 and perplexity is 33.04467398421485
At time: 129.1038794517517 and batch: 800, loss is 3.4779835987091063 and perplexity is 32.39433619460989
At time: 129.5584704875946 and batch: 850, loss is 3.4689588499069215 and perplexity is 32.10330068433243
At time: 130.01944160461426 and batch: 900, loss is 3.6164774179458616 and perplexity is 37.20627455711157
At time: 130.4794180393219 and batch: 950, loss is 3.5363355398178102 and perplexity is 34.340847670988886
At time: 130.94042444229126 and batch: 1000, loss is 3.4589275550842284 and perplexity is 31.782872850388657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.485361238805259 and perplexity of 88.70899015530478
Finished 13 epochs...
Completing Train Step...
At time: 132.35697221755981 and batch: 50, loss is 3.765600872039795 and perplexity is 43.18964951747381
At time: 132.81337642669678 and batch: 100, loss is 3.637096347808838 and perplexity is 37.98139170478759
At time: 133.26838493347168 and batch: 150, loss is 3.6814997816085815 and perplexity is 39.70589961675556
At time: 133.72395300865173 and batch: 200, loss is 3.6986830902099608 and perplexity is 40.39407396648105
At time: 134.18068671226501 and batch: 250, loss is 3.682791328430176 and perplexity is 39.757214776036356
At time: 134.63618779182434 and batch: 300, loss is 3.606417145729065 and perplexity is 36.83384581524854
At time: 135.1091046333313 and batch: 350, loss is 3.6646940183639525 and perplexity is 39.044187530782736
At time: 135.56689381599426 and batch: 400, loss is 3.5818322896957397 and perplexity is 35.93933182196263
At time: 136.02447414398193 and batch: 450, loss is 3.637222547531128 and perplexity is 37.9861852483385
At time: 136.48393988609314 and batch: 500, loss is 3.5488741493225096 and perplexity is 34.77414494587696
At time: 136.94299054145813 and batch: 550, loss is 3.610027103424072 and perplexity is 36.96705473500589
At time: 137.40238285064697 and batch: 600, loss is 3.60435079574585 and perplexity is 36.75781278104976
At time: 137.86160397529602 and batch: 650, loss is 3.5666085195541384 and perplexity is 35.39634334814049
At time: 138.32115864753723 and batch: 700, loss is 3.5342648077011107 and perplexity is 34.26981054960977
At time: 138.78054881095886 and batch: 750, loss is 3.495154843330383 and perplexity is 32.95539048036431
At time: 139.23929381370544 and batch: 800, loss is 3.476561393737793 and perplexity is 32.34829755458294
At time: 139.69683861732483 and batch: 850, loss is 3.468686189651489 and perplexity is 32.09454858339688
At time: 140.15494990348816 and batch: 900, loss is 3.6176073932647705 and perplexity is 37.2483404913274
At time: 140.61440110206604 and batch: 950, loss is 3.5386761379241944 and perplexity is 34.421319933894054
At time: 141.07694745063782 and batch: 1000, loss is 3.4623678398132323 and perplexity is 31.892403282406566
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.485329976895961 and perplexity of 88.70621698624808
Finished 14 epochs...
Completing Train Step...
At time: 142.50134468078613 and batch: 50, loss is 3.7602381181716917 and perplexity is 42.958653997124316
At time: 142.97366881370544 and batch: 100, loss is 3.630952486991882 and perplexity is 37.748754696996606
At time: 143.42981815338135 and batch: 150, loss is 3.675033211708069 and perplexity is 39.449967036232295
At time: 143.88608241081238 and batch: 200, loss is 3.6919643020629884 and perplexity is 40.12358443955914
At time: 144.34462118148804 and batch: 250, loss is 3.6762984800338745 and perplexity is 39.49991342110342
At time: 144.80324912071228 and batch: 300, loss is 3.6003053903579714 and perplexity is 36.60941289840513
At time: 145.26125144958496 and batch: 350, loss is 3.6584399557113647 and perplexity is 38.80076471992285
At time: 145.72067379951477 and batch: 400, loss is 3.5763347291946412 and perplexity is 35.74229527732041
At time: 146.17942118644714 and batch: 450, loss is 3.632113480567932 and perplexity is 37.79260620944018
At time: 146.64062404632568 and batch: 500, loss is 3.5440169382095337 and perplexity is 34.605649123761125
At time: 147.11830496788025 and batch: 550, loss is 3.6055245876312254 and perplexity is 36.80098413555236
At time: 147.57608342170715 and batch: 600, loss is 3.60028151512146 and perplexity is 36.60853885044773
At time: 148.0259141921997 and batch: 650, loss is 3.563140382766724 and perplexity is 35.27379661488411
At time: 148.47246479988098 and batch: 700, loss is 3.531498713493347 and perplexity is 34.17514800825844
At time: 148.92016458511353 and batch: 750, loss is 3.4929863882064818 and perplexity is 32.88400562042178
At time: 149.37986540794373 and batch: 800, loss is 3.4751938724517824 and perplexity is 32.30409080283715
At time: 149.83865118026733 and batch: 850, loss is 3.468140597343445 and perplexity is 32.07704282049867
At time: 150.29772567749023 and batch: 900, loss is 3.6180364179611204 and perplexity is 37.26432437779208
At time: 150.75717973709106 and batch: 950, loss is 3.539834542274475 and perplexity is 34.46121684456385
At time: 151.21636152267456 and batch: 1000, loss is 3.4641926431655885 and perplexity is 31.950653778517225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.485561463891006 and perplexity of 88.72675369875931
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 152.63412761688232 and batch: 50, loss is 3.757012948989868 and perplexity is 42.82032825202731
At time: 153.10653257369995 and batch: 100, loss is 3.627984004020691 and perplexity is 37.636864316004015
At time: 153.56014728546143 and batch: 150, loss is 3.6722223567962646 and perplexity is 39.33923460192861
At time: 154.01632571220398 and batch: 200, loss is 3.689437508583069 and perplexity is 40.02232840841989
At time: 154.47528862953186 and batch: 250, loss is 3.673634853363037 and perplexity is 39.39484039799616
At time: 154.93766021728516 and batch: 300, loss is 3.5977163314819336 and perplexity is 36.514751567751176
At time: 155.4002251625061 and batch: 350, loss is 3.6567110586166383 and perplexity is 38.733740146515714
At time: 155.86092877388 and batch: 400, loss is 3.5737463140487673 and perplexity is 35.64989901041172
At time: 156.32244992256165 and batch: 450, loss is 3.6282192277908325 and perplexity is 37.64571844243441
At time: 156.78388595581055 and batch: 500, loss is 3.5392749452590944 and perplexity is 34.441937845209594
At time: 157.24496626853943 and batch: 550, loss is 3.600565848350525 and perplexity is 36.6189493544664
At time: 157.70544528961182 and batch: 600, loss is 3.59495228767395 and perplexity is 36.41396255157679
At time: 158.16103744506836 and batch: 650, loss is 3.555772910118103 and perplexity is 35.01487286113774
At time: 158.64458394050598 and batch: 700, loss is 3.523518328666687 and perplexity is 33.903502535029396
At time: 159.1084542274475 and batch: 750, loss is 3.4844015312194823 and perplexity is 32.60290944662806
At time: 159.57293176651 and batch: 800, loss is 3.465586929321289 and perplexity is 31.99523320376192
At time: 160.03699445724487 and batch: 850, loss is 3.457828369140625 and perplexity is 31.74795675645949
At time: 160.50161266326904 and batch: 900, loss is 3.6064631271362306 and perplexity is 36.83553952624976
At time: 160.96698260307312 and batch: 950, loss is 3.527023506164551 and perplexity is 34.02254884655391
At time: 161.43191599845886 and batch: 1000, loss is 3.45132257938385 and perplexity is 31.54208164084728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.484971209270198 and perplexity of 88.6743977755822
Finished 16 epochs...
Completing Train Step...
At time: 162.86192870140076 and batch: 50, loss is 3.7556653118133543 and perplexity is 42.76266085185813
At time: 163.3190474510193 and batch: 100, loss is 3.6262987947463987 and perplexity is 37.5734917362308
At time: 163.7778720855713 and batch: 150, loss is 3.670598611831665 and perplexity is 39.275409549659294
At time: 164.23563981056213 and batch: 200, loss is 3.6876458168029784 and perplexity is 39.95068493226968
At time: 164.6941113471985 and batch: 250, loss is 3.6719675493240356 and perplexity is 39.32921194797806
At time: 165.15262746810913 and batch: 300, loss is 3.596268095970154 and perplexity is 36.46190788211413
At time: 165.61165285110474 and batch: 350, loss is 3.6550916767120363 and perplexity is 38.67106618885769
At time: 166.06963968276978 and batch: 400, loss is 3.5722752237319946 and perplexity is 35.59749334536773
At time: 166.52835178375244 and batch: 450, loss is 3.627043294906616 and perplexity is 37.6014756225597
At time: 166.98619627952576 and batch: 500, loss is 3.5383412933349607 and perplexity is 34.409796070619215
At time: 167.44502925872803 and batch: 550, loss is 3.599560251235962 and perplexity is 36.582143953461305
At time: 167.90245389938354 and batch: 600, loss is 3.594002389907837 and perplexity is 36.37938943295741
At time: 168.36181926727295 and batch: 650, loss is 3.555115933418274 and perplexity is 34.99187646039738
At time: 168.82026886940002 and batch: 700, loss is 3.523037524223328 and perplexity is 33.88720549852285
At time: 169.27888941764832 and batch: 750, loss is 3.484114575386047 and perplexity is 32.593555193768296
At time: 169.7380030155182 and batch: 800, loss is 3.4655655670166015 and perplexity is 31.994549719142103
At time: 170.19977355003357 and batch: 850, loss is 3.457996520996094 and perplexity is 31.753295683158075
At time: 170.67624878883362 and batch: 900, loss is 3.60699933052063 and perplexity is 36.855296163524436
At time: 171.13514399528503 and batch: 950, loss is 3.527730007171631 and perplexity is 34.04659430464743
At time: 171.59321928024292 and batch: 1000, loss is 3.452212862968445 and perplexity is 31.5701755422701
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4848148997237045 and perplexity of 88.66053820390019
Finished 17 epochs...
Completing Train Step...
At time: 173.01233577728271 and batch: 50, loss is 3.7546749401092527 and perplexity is 42.72033088721786
At time: 173.49059224128723 and batch: 100, loss is 3.625043821334839 and perplexity is 37.52636757908938
At time: 173.94262719154358 and batch: 150, loss is 3.6692982864379884 and perplexity is 39.22437192722446
At time: 174.3952510356903 and batch: 200, loss is 3.6863100051879885 and perplexity is 39.89735397129918
At time: 174.85178327560425 and batch: 250, loss is 3.67071400642395 and perplexity is 39.279941981035115
At time: 175.30775594711304 and batch: 300, loss is 3.5950700998306275 and perplexity is 36.4182528117556
At time: 175.763352394104 and batch: 350, loss is 3.65381564617157 and perplexity is 38.621752197139365
At time: 176.21865391731262 and batch: 400, loss is 3.5711557626724244 and perplexity is 35.557665634694196
At time: 176.6730237007141 and batch: 450, loss is 3.6261158561706544 and perplexity is 37.56661872385555
At time: 177.12565970420837 and batch: 500, loss is 3.53751838684082 and perplexity is 34.381491673504
At time: 177.58221888542175 and batch: 550, loss is 3.598750867843628 and perplexity is 36.55254695296548
At time: 178.04073476791382 and batch: 600, loss is 3.5932651138305665 and perplexity is 36.35257766447506
At time: 178.50059628486633 and batch: 650, loss is 3.554593629837036 and perplexity is 34.973604850087
At time: 178.95959186553955 and batch: 700, loss is 3.5226985025405884 and perplexity is 33.87571894829622
At time: 179.4194152355194 and batch: 750, loss is 3.483910331726074 and perplexity is 32.58689884654548
At time: 179.881600856781 and batch: 800, loss is 3.4655318117141722 and perplexity is 31.993469751667664
At time: 180.34435367584229 and batch: 850, loss is 3.458148202896118 and perplexity is 31.758112448678165
At time: 180.80256032943726 and batch: 900, loss is 3.6074232149124144 and perplexity is 36.87092185983317
At time: 181.25962591171265 and batch: 950, loss is 3.5283049726486206 and perplexity is 34.06617554971219
At time: 181.72199296951294 and batch: 1000, loss is 3.4529332399368284 and perplexity is 31.592926163146362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.48474604909013 and perplexity of 88.65443407981057
Finished 18 epochs...
Completing Train Step...
At time: 183.21615409851074 and batch: 50, loss is 3.7537934923172 and perplexity is 42.682691736791554
At time: 183.71532320976257 and batch: 100, loss is 3.623958830833435 and perplexity is 37.485673906829874
At time: 184.20035982131958 and batch: 150, loss is 3.6681635093688967 and perplexity is 39.17988625484614
At time: 184.69100642204285 and batch: 200, loss is 3.6851495122909546 and perplexity is 39.851080230771885
At time: 185.16586756706238 and batch: 250, loss is 3.6696071147918703 and perplexity is 39.23648739614267
At time: 185.62669682502747 and batch: 300, loss is 3.594005479812622 and perplexity is 36.37950184198058
At time: 186.0976538658142 and batch: 350, loss is 3.6527036809921265 and perplexity is 38.578830021931836
At time: 186.55621767044067 and batch: 400, loss is 3.5701907300949096 and perplexity is 35.523367880862985
At time: 187.01351475715637 and batch: 450, loss is 3.6252927017211913 and perplexity is 37.53570831826604
At time: 187.47191262245178 and batch: 500, loss is 3.536759657859802 and perplexity is 34.355415333049244
At time: 187.94524884223938 and batch: 550, loss is 3.598033971786499 and perplexity is 36.52635196683989
At time: 188.41678762435913 and batch: 600, loss is 3.5926251125335695 and perplexity is 36.32931941107067
At time: 188.88445138931274 and batch: 650, loss is 3.554126486778259 and perplexity is 34.957270988762886
At time: 189.34578847885132 and batch: 700, loss is 3.52239381313324 and perplexity is 33.86539894784361
At time: 189.80440139770508 and batch: 750, loss is 3.4837207651138304 and perplexity is 32.580722044003885
At time: 190.26297616958618 and batch: 800, loss is 3.4654775428771973 and perplexity is 31.991733550384687
At time: 190.72104930877686 and batch: 850, loss is 3.4582583332061767 and perplexity is 31.761610172048158
At time: 191.16887211799622 and batch: 900, loss is 3.607758431434631 and perplexity is 36.88328367385634
At time: 191.62373113632202 and batch: 950, loss is 3.528780312538147 and perplexity is 34.08237241103675
At time: 192.08294701576233 and batch: 1000, loss is 3.453532099723816 and perplexity is 31.611851562443427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4847185088367 and perplexity of 88.65199254784869
Finished 19 epochs...
Completing Train Step...
At time: 193.528559923172 and batch: 50, loss is 3.752971053123474 and perplexity is 42.64760224967614
At time: 193.9853332042694 and batch: 100, loss is 3.6229751110076904 and perplexity is 37.4488166378117
At time: 194.45703148841858 and batch: 150, loss is 3.667133822441101 and perplexity is 39.139564001348326
At time: 194.91591024398804 and batch: 200, loss is 3.6840961933135987 and perplexity is 39.80912643094236
At time: 195.37334084510803 and batch: 250, loss is 3.6685902786254885 and perplexity is 39.19661059423958
At time: 195.83080768585205 and batch: 300, loss is 3.5930305767059325 and perplexity is 36.344052635193606
At time: 196.29034042358398 and batch: 350, loss is 3.6516955852508546 and perplexity is 38.53995846410139
At time: 196.75309681892395 and batch: 400, loss is 3.5693177509307863 and perplexity is 35.492370252973735
At time: 197.21377825737 and batch: 450, loss is 3.6245316791534425 and perplexity is 37.50715366388573
At time: 197.67447113990784 and batch: 500, loss is 3.5360496711730955 and perplexity is 34.33103210245531
At time: 198.1349151134491 and batch: 550, loss is 3.597375273704529 and perplexity is 36.502300051202965
At time: 198.5978000164032 and batch: 600, loss is 3.5920429706573485 and perplexity is 36.30817674751887
At time: 199.0623073577881 and batch: 650, loss is 3.5536891555786134 and perplexity is 34.94198642595673
At time: 199.52695393562317 and batch: 700, loss is 3.5220999574661254 and perplexity is 33.85544887045861
At time: 199.99155569076538 and batch: 750, loss is 3.483531160354614 and perplexity is 32.57454516964653
At time: 200.45431971549988 and batch: 800, loss is 3.46540611743927 and perplexity is 31.989448608408264
At time: 200.91830325126648 and batch: 850, loss is 3.4583314990997316 and perplexity is 31.7639341236531
At time: 201.38277411460876 and batch: 900, loss is 3.608026928901672 and perplexity is 36.89318807169229
At time: 201.84605836868286 and batch: 950, loss is 3.5291780853271484 and perplexity is 34.095932148034876
At time: 202.30975222587585 and batch: 1000, loss is 3.4540380573272706 and perplexity is 31.6278498659911
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.484714042849657 and perplexity of 88.65159663008271
Finished 20 epochs...
Completing Train Step...
At time: 203.73148322105408 and batch: 50, loss is 3.7521874618530275 and perplexity is 42.6141970505683
At time: 204.20417499542236 and batch: 100, loss is 3.6220598793029786 and perplexity is 37.41455797322023
At time: 204.66069865226746 and batch: 150, loss is 3.666177525520325 and perplexity is 39.102152847749856
At time: 205.11751651763916 and batch: 200, loss is 3.683118071556091 and perplexity is 39.77020729516253
At time: 205.57341527938843 and batch: 250, loss is 3.6676387691497805 and perplexity is 39.15933238594029
At time: 206.04014134407043 and batch: 300, loss is 3.5921216773986817 and perplexity is 36.31103455825743
At time: 206.4830801486969 and batch: 350, loss is 3.6507609415054323 and perplexity is 38.50395416119333
At time: 206.93047165870667 and batch: 400, loss is 3.5685074853897096 and perplexity is 35.46362365614779
At time: 207.3865077495575 and batch: 450, loss is 3.6238138341903685 and perplexity is 37.480239003981154
At time: 207.84065103530884 and batch: 500, loss is 3.5353783798217773 and perplexity is 34.30799371112468
At time: 208.29700136184692 and batch: 550, loss is 3.5967575216293337 and perplexity is 36.479757643123506
At time: 208.75017952919006 and batch: 600, loss is 3.5914992761611937 and perplexity is 36.28844155709918
At time: 209.20866465568542 and batch: 650, loss is 3.5532698822021485 and perplexity is 34.927339252127105
At time: 209.66687893867493 and batch: 700, loss is 3.5218095684051516 and perplexity is 33.84561904575813
At time: 210.12608647346497 and batch: 750, loss is 3.4833371353149416 and perplexity is 32.568225505334055
At time: 210.58438420295715 and batch: 800, loss is 3.465319676399231 and perplexity is 31.986683526710262
At time: 211.03605389595032 and batch: 850, loss is 3.4583741807937622 and perplexity is 31.765289891103595
At time: 211.49813318252563 and batch: 900, loss is 3.6082440328598024 and perplexity is 36.90119859837776
At time: 211.9608018398285 and batch: 950, loss is 3.529513988494873 and perplexity is 34.10738700340333
At time: 212.42358303070068 and batch: 1000, loss is 3.454470543861389 and perplexity is 31.64153144349702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.484725207817264 and perplexity of 88.65258642781293
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 213.85369229316711 and batch: 50, loss is 3.7516471099853517 and perplexity is 42.59117660973174
At time: 214.33022546768188 and batch: 100, loss is 3.6215292692184446 and perplexity is 37.39471069749924
At time: 214.7891228199005 and batch: 150, loss is 3.66565221786499 and perplexity is 39.08161758165733
At time: 215.24910402297974 and batch: 200, loss is 3.6826312732696533 and perplexity is 39.75085193785957
At time: 215.70869660377502 and batch: 250, loss is 3.6670721483230593 and perplexity is 39.13715017769399
At time: 216.16880178451538 and batch: 300, loss is 3.5915236616134645 and perplexity is 36.289326477948315
At time: 216.62844610214233 and batch: 350, loss is 3.6502949094772337 and perplexity is 38.4860142659495
At time: 217.08682680130005 and batch: 400, loss is 3.567800784111023 and perplexity is 35.43857032161758
At time: 217.54581451416016 and batch: 450, loss is 3.6231489086151125 and perplexity is 37.45532571815905
At time: 218.02204132080078 and batch: 500, loss is 3.534585747718811 and perplexity is 34.28081086834098
At time: 218.4818196296692 and batch: 550, loss is 3.595962438583374 and perplexity is 36.45076473371345
At time: 218.94080209732056 and batch: 600, loss is 3.5907386207580565 and perplexity is 36.260849053481245
At time: 219.40003895759583 and batch: 650, loss is 3.552210221290588 and perplexity is 34.89034771867927
At time: 219.86034083366394 and batch: 700, loss is 3.52062912940979 and perplexity is 33.80568992879427
At time: 220.31908988952637 and batch: 750, loss is 3.4820962858200075 and perplexity is 32.527838301567606
At time: 220.77911520004272 and batch: 800, loss is 3.4637519979476927 and perplexity is 31.93657797716611
At time: 221.23847007751465 and batch: 850, loss is 3.456770133972168 and perplexity is 31.71437772248416
At time: 221.69909286499023 and batch: 900, loss is 3.606448073387146 and perplexity is 36.834985017454066
At time: 222.15899538993835 and batch: 950, loss is 3.5275877571105956 and perplexity is 34.041751518980995
At time: 222.61855912208557 and batch: 1000, loss is 3.4525097036361694 and perplexity is 31.579548245289132
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.48471367068407 and perplexity of 88.6515636370154
Finished 22 epochs...
Completing Train Step...
At time: 224.0582902431488 and batch: 50, loss is 3.7515072870254516 and perplexity is 42.585221801671686
At time: 224.51456594467163 and batch: 100, loss is 3.621368727684021 and perplexity is 37.388707775136616
At time: 224.97352862358093 and batch: 150, loss is 3.665506534576416 and perplexity is 39.075924457791814
At time: 225.4327585697174 and batch: 200, loss is 3.682482919692993 and perplexity is 39.74495519421162
At time: 225.89104223251343 and batch: 250, loss is 3.6669174575805665 and perplexity is 39.131096491110654
At time: 226.34958243370056 and batch: 300, loss is 3.591409134864807 and perplexity is 36.285170617358965
At time: 226.80903482437134 and batch: 350, loss is 3.650147948265076 and perplexity is 38.48035873022422
At time: 227.26806354522705 and batch: 400, loss is 3.5676710844039916 and perplexity is 35.43397424749044
At time: 227.7256634235382 and batch: 450, loss is 3.6230420017242433 and perplexity is 37.45132169977242
At time: 228.18398356437683 and batch: 500, loss is 3.5345058059692382 and perplexity is 34.27807050987936
At time: 228.642493724823 and batch: 550, loss is 3.5958706665039064 and perplexity is 36.44741972472721
At time: 229.0995738506317 and batch: 600, loss is 3.5906348848342895 and perplexity is 36.257087695905426
At time: 229.57337093353271 and batch: 650, loss is 3.552139010429382 and perplexity is 34.8878632354326
At time: 230.03317141532898 and batch: 700, loss is 3.520579209327698 and perplexity is 33.804002388099285
At time: 230.495192527771 and batch: 750, loss is 3.4820668125152587 and perplexity is 32.52687961280446
At time: 230.95608735084534 and batch: 800, loss is 3.4637540006637573 and perplexity is 31.93664193712792
At time: 231.41819095611572 and batch: 850, loss is 3.45679123878479 and perplexity is 31.715047055546464
At time: 231.87961959838867 and batch: 900, loss is 3.6064929437637328 and perplexity is 36.83663785418479
At time: 232.34143590927124 and batch: 950, loss is 3.5276537418365477 and perplexity is 34.04399782873615
At time: 232.80248022079468 and batch: 1000, loss is 3.45259268283844 and perplexity is 31.582168799734852
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.484708460365853 and perplexity of 88.65110173536176
Finished 23 epochs...
Completing Train Step...
At time: 234.22979187965393 and batch: 50, loss is 3.751378827095032 and perplexity is 42.57975165839677
At time: 234.69812750816345 and batch: 100, loss is 3.6212236642837525 and perplexity is 37.38328443542867
At time: 235.15215945243835 and batch: 150, loss is 3.665365209579468 and perplexity is 39.07040244309562
At time: 235.60653114318848 and batch: 200, loss is 3.68233606338501 and perplexity is 39.739118825395124
At time: 236.0604486465454 and batch: 250, loss is 3.66677134513855 and perplexity is 39.125379368725085
At time: 236.5148241519928 and batch: 300, loss is 3.591287040710449 and perplexity is 36.280740680576905
At time: 236.97267198562622 and batch: 350, loss is 3.6500072050094605 and perplexity is 38.47494326036365
At time: 237.43343591690063 and batch: 400, loss is 3.5675510931015015 and perplexity is 35.429722733845516
At time: 237.89466428756714 and batch: 450, loss is 3.6229366493225097 and perplexity is 37.44737632091447
At time: 238.35502886772156 and batch: 500, loss is 3.534419641494751 and perplexity is 34.27511708518941
At time: 238.81463885307312 and batch: 550, loss is 3.5957819175720216 and perplexity is 36.44418519868917
At time: 239.27538990974426 and batch: 600, loss is 3.590545630455017 and perplexity is 36.253851736462835
At time: 239.73610496520996 and batch: 650, loss is 3.5520773458480837 and perplexity is 34.88571195628333
At time: 240.19917607307434 and batch: 700, loss is 3.520535082817078 and perplexity is 33.80251076833913
At time: 240.66107845306396 and batch: 750, loss is 3.4820409774780274 and perplexity is 32.526039290513566
At time: 241.12371444702148 and batch: 800, loss is 3.463756031990051 and perplexity is 31.93670681093431
At time: 241.60205054283142 and batch: 850, loss is 3.4568102884292604 and perplexity is 31.715651221671784
At time: 242.06641054153442 and batch: 900, loss is 3.6065358114242554 and perplexity is 36.83821698851777
At time: 242.5287103652954 and batch: 950, loss is 3.527716383934021 and perplexity is 34.04613048296279
At time: 242.98845052719116 and batch: 1000, loss is 3.4526692533493044 and perplexity is 31.58458715512022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.484706599537919 and perplexity of 88.65093677106871
Finished 24 epochs...
Completing Train Step...
At time: 244.42811346054077 and batch: 50, loss is 3.75125629901886 and perplexity is 42.5745347629568
At time: 244.9049940109253 and batch: 100, loss is 3.621086196899414 and perplexity is 37.3781458063044
At time: 245.36572670936584 and batch: 150, loss is 3.665226411819458 and perplexity is 39.064979935078526
At time: 245.81754112243652 and batch: 200, loss is 3.682191548347473 and perplexity is 39.733376340094125
At time: 246.2643563747406 and batch: 250, loss is 3.6666297340393066 and perplexity is 39.11983917303022
At time: 246.71210956573486 and batch: 300, loss is 3.5911620092391967 and perplexity is 36.276204729765695
At time: 247.16875338554382 and batch: 350, loss is 3.64987015247345 and perplexity is 38.469670533145525
At time: 247.62807321548462 and batch: 400, loss is 3.5674354076385497 and perplexity is 35.425624267039886
At time: 248.08747148513794 and batch: 450, loss is 3.6228325080871584 and perplexity is 37.44347670794253
At time: 248.54774355888367 and batch: 500, loss is 3.534331135749817 and perplexity is 34.272083674658205
At time: 249.0083463191986 and batch: 550, loss is 3.595694670677185 and perplexity is 36.44100569539864
At time: 249.46696305274963 and batch: 600, loss is 3.590462713241577 and perplexity is 36.250845792724334
At time: 249.92578792572021 and batch: 650, loss is 3.5520193481445315 and perplexity is 34.883688723775066
At time: 250.3861711025238 and batch: 700, loss is 3.5204935884475708 and perplexity is 33.80110818356693
At time: 250.84573698043823 and batch: 750, loss is 3.4820163536071775 and perplexity is 32.525238383383574
At time: 251.3043053150177 and batch: 800, loss is 3.4637566614151 and perplexity is 31.936726912703886
At time: 251.76264429092407 and batch: 850, loss is 3.45682728767395 and perplexity is 31.716190368369926
At time: 252.22161030769348 and batch: 900, loss is 3.6065767288208006 and perplexity is 36.839724343288616
At time: 252.67598485946655 and batch: 950, loss is 3.5277762413024902 and perplexity is 34.0481684557333
At time: 253.15080857276917 and batch: 1000, loss is 3.4527417182922364 and perplexity is 31.586876013355937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.484705110875572 and perplexity of 88.65080479985534
Finished 25 epochs...
Completing Train Step...
At time: 254.59177660942078 and batch: 50, loss is 3.751137580871582 and perplexity is 42.569480693079356
At time: 255.05047011375427 and batch: 100, loss is 3.6209529972076417 and perplexity is 37.37316738037374
At time: 255.50950813293457 and batch: 150, loss is 3.6650901079177856 and perplexity is 39.05965558876744
At time: 255.96914982795715 and batch: 200, loss is 3.682049832344055 and perplexity is 39.72774588376921
At time: 256.42998123168945 and batch: 250, loss is 3.6664915800094606 and perplexity is 39.11443498291546
At time: 256.8881857395172 and batch: 300, loss is 3.591036343574524 and perplexity is 36.271646342808886
At time: 257.34656262397766 and batch: 350, loss is 3.649735565185547 and perplexity is 38.46449335292109
At time: 257.8056809902191 and batch: 400, loss is 3.567322163581848 and perplexity is 35.42161275278105
At time: 258.26489782333374 and batch: 450, loss is 3.6227296495437624 and perplexity is 37.43962552453562
At time: 258.7250394821167 and batch: 500, loss is 3.5342419290542604 and perplexity is 34.269026511685475
At time: 259.1874170303345 and batch: 550, loss is 3.595608458518982 and perplexity is 36.437864173071205
At time: 259.6495771408081 and batch: 600, loss is 3.5903832292556763 and perplexity is 36.24796454551648
At time: 260.10353207588196 and batch: 650, loss is 3.551962833404541 and perplexity is 34.88171733688367
At time: 260.56952452659607 and batch: 700, loss is 3.52045343875885 and perplexity is 33.79975110683823
At time: 261.03526496887207 and batch: 750, loss is 3.481991901397705 and perplexity is 32.524443079164996
At time: 261.5001564025879 and batch: 800, loss is 3.4637557315826415 and perplexity is 31.93669721691239
At time: 261.9652736186981 and batch: 850, loss is 3.4568425273895262 and perplexity is 31.716673717773343
At time: 262.4306962490082 and batch: 900, loss is 3.6066160440444945 and perplexity is 36.841172733763706
At time: 262.8959889411926 and batch: 950, loss is 3.527833828926086 and perplexity is 34.0501292653011
At time: 263.36262702941895 and batch: 1000, loss is 3.4528108978271486 and perplexity is 31.589061254333973
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.484705483041158 and perplexity of 88.65083779264022
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 264.7952208518982 and batch: 50, loss is 3.7510504484176637 and perplexity is 42.56577167135497
At time: 265.26861476898193 and batch: 100, loss is 3.620867519378662 and perplexity is 37.369972939692836
At time: 265.7255117893219 and batch: 150, loss is 3.665010304450989 and perplexity is 39.056538617213796
At time: 266.1826846599579 and batch: 200, loss is 3.681977424621582 and perplexity is 39.72486939231214
At time: 266.63944911956787 and batch: 250, loss is 3.666405591964722 and perplexity is 39.11107175373104
At time: 267.09717535972595 and batch: 300, loss is 3.590949144363403 and perplexity is 36.268483621757134
At time: 267.55424785614014 and batch: 350, loss is 3.649667115211487 and perplexity is 38.46186054945754
At time: 268.0104765892029 and batch: 400, loss is 3.5672104787826537 and perplexity is 35.41765691798104
At time: 268.4667778015137 and batch: 450, loss is 3.622630658149719 and perplexity is 37.43591950724743
At time: 268.9245858192444 and batch: 500, loss is 3.534133486747742 and perplexity is 34.26531050089844
At time: 269.38249945640564 and batch: 550, loss is 3.5954930114746095 and perplexity is 36.433657772162135
At time: 269.8396532535553 and batch: 600, loss is 3.590271110534668 and perplexity is 36.24390069791344
At time: 270.29865407943726 and batch: 650, loss is 3.5518100023269654 and perplexity is 34.876386733786674
At time: 270.75641679763794 and batch: 700, loss is 3.520284481048584 and perplexity is 33.79404086069229
At time: 271.2134954929352 and batch: 750, loss is 3.481804757118225 and perplexity is 32.518356885215404
At time: 271.672997713089 and batch: 800, loss is 3.4635429096221926 and perplexity is 31.929901109606515
At time: 272.13489627838135 and batch: 850, loss is 3.456614603996277 and perplexity is 31.709445569640327
At time: 272.5957250595093 and batch: 900, loss is 3.6063689184188843 and perplexity is 36.832069460776005
At time: 273.05580258369446 and batch: 950, loss is 3.5275685453414916 and perplexity is 34.04109752299314
At time: 273.5187773704529 and batch: 1000, loss is 3.4525386190414427 and perplexity is 31.580461393926957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.484705855206745 and perplexity of 88.65087078543746
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 274.94341468811035 and batch: 50, loss is 3.7510376596450805 and perplexity is 42.5652273108621
At time: 275.41517424583435 and batch: 100, loss is 3.620854902267456 and perplexity is 37.36950144156297
At time: 275.8711543083191 and batch: 150, loss is 3.664998140335083 and perplexity is 39.05606353184067
At time: 276.3298180103302 and batch: 200, loss is 3.681966609954834 and perplexity is 39.72443978341108
At time: 276.78978514671326 and batch: 250, loss is 3.666393599510193 and perplexity is 39.1106027187939
At time: 277.26773262023926 and batch: 300, loss is 3.5909377193450926 and perplexity is 36.268069256034735
At time: 277.7308261394501 and batch: 350, loss is 3.649657530784607 and perplexity is 38.461491916334005
At time: 278.19518303871155 and batch: 400, loss is 3.567195453643799 and perplexity is 35.41712476676577
At time: 278.6572701931 and batch: 450, loss is 3.6226171350479124 and perplexity is 37.43541326091972
At time: 279.12050557136536 and batch: 500, loss is 3.534119138717651 and perplexity is 34.26481886471932
At time: 279.58348512649536 and batch: 550, loss is 3.5954773139953615 and perplexity is 36.433085860064125
At time: 280.047554731369 and batch: 600, loss is 3.5902552700042722 and perplexity is 36.24332657984995
At time: 280.51082134246826 and batch: 650, loss is 3.551788640022278 and perplexity is 34.8756417017447
At time: 280.9741427898407 and batch: 700, loss is 3.5202611780166624 and perplexity is 33.7932533662549
At time: 281.43746757507324 and batch: 750, loss is 3.4817787265777587 and perplexity is 32.51751042582755
At time: 281.8977515697479 and batch: 800, loss is 3.4635134887695314 and perplexity is 31.92896171850939
At time: 282.35710096359253 and batch: 850, loss is 3.4565829038619995 and perplexity is 31.70844039189012
At time: 282.81541681289673 and batch: 900, loss is 3.6063346910476684 and perplexity is 36.8308088174363
At time: 283.2737329006195 and batch: 950, loss is 3.5275318002700806 and perplexity is 34.039846703414625
At time: 283.754567861557 and batch: 1000, loss is 3.452500858306885 and perplexity is 31.57926891502163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.484706227372333 and perplexity of 88.650903778247
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 285.2724621295929 and batch: 50, loss is 3.7510366535186765 and perplexity is 42.565184484884554
At time: 285.7712023258209 and batch: 100, loss is 3.6208539390563965 and perplexity is 37.36946544686323
At time: 286.26012444496155 and batch: 150, loss is 3.6649971055984496 and perplexity is 39.05602311912189
At time: 286.7401473522186 and batch: 200, loss is 3.6819656753540038 and perplexity is 39.72440265693403
At time: 287.207891702652 and batch: 250, loss is 3.6663924407958985 and perplexity is 39.110557400805725
At time: 287.6707899570465 and batch: 300, loss is 3.5909365463256835 and perplexity is 36.26802671291052
At time: 288.1336395740509 and batch: 350, loss is 3.6496569204330442 and perplexity is 38.46146844130948
At time: 288.59027791023254 and batch: 400, loss is 3.5671940898895262 and perplexity is 35.41707646654348
At time: 289.0622446537018 and batch: 450, loss is 3.6226160287857057 and perplexity is 37.43537184755974
At time: 289.51815128326416 and batch: 500, loss is 3.5341177797317505 and perplexity is 34.26477229934523
At time: 289.9749584197998 and batch: 550, loss is 3.5954757595062254 and perplexity is 36.43302922527199
At time: 290.43251729011536 and batch: 600, loss is 3.590253643989563 and perplexity is 36.24326764771574
At time: 290.88852643966675 and batch: 650, loss is 3.551786332130432 and perplexity is 34.87556121262847
At time: 291.3455011844635 and batch: 700, loss is 3.5202585363388064 and perplexity is 33.79316409548372
At time: 291.8010621070862 and batch: 750, loss is 3.481775736808777 and perplexity is 32.51741320612885
At time: 292.2564525604248 and batch: 800, loss is 3.4635101413726805 and perplexity is 31.928854839782367
At time: 292.7126052379608 and batch: 850, loss is 3.456579337120056 and perplexity is 31.708327296267502
At time: 293.1681869029999 and batch: 900, loss is 3.6063306951522827 and perplexity is 36.830661645671334
At time: 293.62356209754944 and batch: 950, loss is 3.5275273895263672 and perplexity is 34.039696562705885
At time: 294.0794689655304 and batch: 1000, loss is 3.4524964046478273 and perplexity is 31.57912827203778
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.484706227372333 and perplexity of 88.650903778247
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 295.49693036079407 and batch: 50, loss is 3.7510366344451906 and perplexity is 42.56518367301812
At time: 295.9685561656952 and batch: 100, loss is 3.6208538675308226 and perplexity is 37.36946277399086
At time: 296.42509508132935 and batch: 150, loss is 3.664997091293335 and perplexity is 39.05602256042101
At time: 296.8832881450653 and batch: 200, loss is 3.6819656705856323 and perplexity is 39.72440246751332
At time: 297.34137511253357 and batch: 250, loss is 3.666392388343811 and perplexity is 39.110555349375396
At time: 297.7995285987854 and batch: 300, loss is 3.5909365129470827 and perplexity is 36.26802550233455
At time: 298.25744891166687 and batch: 350, loss is 3.6496568870544435 and perplexity is 38.4614671575195
At time: 298.7178773880005 and batch: 400, loss is 3.5671939849853516 and perplexity is 35.417072751144495
At time: 299.1789824962616 and batch: 450, loss is 3.6226159715652466 and perplexity is 37.43536970549064
At time: 299.6396095752716 and batch: 500, loss is 3.5341177082061765 and perplexity is 34.26476984853781
At time: 300.09974932670593 and batch: 550, loss is 3.5954757070541383 and perplexity is 36.43302731428361
At time: 300.56369185447693 and batch: 600, loss is 3.590253701210022 and perplexity is 36.24326972157221
At time: 301.04419708251953 and batch: 650, loss is 3.551786298751831 and perplexity is 34.87556004853105
At time: 301.50889134407043 and batch: 700, loss is 3.5202584505081176 and perplexity is 33.79316119499329
At time: 301.9732987880707 and batch: 750, loss is 3.481775679588318 and perplexity is 32.51741134546759
At time: 302.43825936317444 and batch: 800, loss is 3.4635100412368773 and perplexity is 31.928851642561003
At time: 302.9031472206116 and batch: 850, loss is 3.4565791511535644 and perplexity is 31.70832139958167
At time: 303.3674728870392 and batch: 900, loss is 3.606330518722534 and perplexity is 36.83065514764753
At time: 303.8319718837738 and batch: 950, loss is 3.5275272035598757 and perplexity is 34.039690232463535
At time: 304.2971739768982 and batch: 1000, loss is 3.4524962377548216 and perplexity is 31.57912300170259
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.484706227372333 and perplexity of 88.650903778247
Annealing...
Model not improving. Stopping early with 88.65080479985534loss at 29 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f75e40c6898>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.719666449878337, 'anneal': 2.889909243886338, 'tune_wordvecs': True, 'lr': 15.676588363372192, 'wordvec_dim': 200, 'seq_len': 20, 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7242982387542725 and batch: 50, loss is 6.70925389289856 and perplexity is 819.9586342084203
At time: 1.1921043395996094 and batch: 100, loss is 6.189626817703247 and perplexity is 487.6640846497864
At time: 1.6435437202453613 and batch: 150, loss is 6.104096632003785 and perplexity is 447.688031687025
At time: 2.096112012863159 and batch: 200, loss is 6.04967246055603 and perplexity is 423.9741390461049
At time: 2.5477378368377686 and batch: 250, loss is 6.019025793075562 and perplexity is 411.1778282556776
At time: 2.999668836593628 and batch: 300, loss is 5.9031816101074215 and perplexity is 366.20072429561577
At time: 3.4510245323181152 and batch: 350, loss is 5.9277825450897215 and perplexity is 375.3213322637105
At time: 3.9036593437194824 and batch: 400, loss is 5.8655369758605955 and perplexity is 352.6714813438209
At time: 4.355850458145142 and batch: 450, loss is 5.837435474395752 and perplexity is 342.8988392603909
At time: 4.807678699493408 and batch: 500, loss is 5.832272729873657 and perplexity is 341.1331021032689
At time: 5.259649991989136 and batch: 550, loss is 5.883460521697998 and perplexity is 359.04959324744755
At time: 5.711657762527466 and batch: 600, loss is 5.895053195953369 and perplexity is 363.2361580786915
At time: 6.163421869277954 and batch: 650, loss is 5.8446054267883305 and perplexity is 345.3662426252965
At time: 6.61616063117981 and batch: 700, loss is 5.849584503173828 and perplexity is 347.09013566441433
At time: 7.067887783050537 and batch: 750, loss is 5.7404279708862305 and perplexity is 311.19756598423527
At time: 7.520421981811523 and batch: 800, loss is 5.803581991195679 and perplexity is 331.4848115383343
At time: 7.972489833831787 and batch: 850, loss is 5.7621067523956295 and perplexity is 318.01760802518424
At time: 8.439871788024902 and batch: 900, loss is 5.838668594360351 and perplexity is 343.32193547550287
At time: 8.891596794128418 and batch: 950, loss is 5.818362140655518 and perplexity is 336.42059253433666
At time: 9.34316110610962 and batch: 1000, loss is 5.7608187389373775 and perplexity is 317.60826074508117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.5292577976133765 and perplexity of 251.95683864235002
Finished 1 epochs...
Completing Train Step...
At time: 10.75984239578247 and batch: 50, loss is 5.461618013381958 and perplexity is 235.47812304865286
At time: 11.205404996871948 and batch: 100, loss is 5.360240125656128 and perplexity is 212.77603330027412
At time: 11.651697158813477 and batch: 150, loss is 5.294770317077637 and perplexity is 199.29184677565223
At time: 12.096421957015991 and batch: 200, loss is 5.246491708755493 and perplexity is 189.89887787229972
At time: 12.542641162872314 and batch: 250, loss is 5.2197064018249515 and perplexity is 184.8798957016092
At time: 12.988427877426147 and batch: 300, loss is 5.095244340896606 and perplexity is 163.24372687034608
At time: 13.43364930152893 and batch: 350, loss is 5.137544374465943 and perplexity is 170.29706861680955
At time: 13.879207611083984 and batch: 400, loss is 5.073347406387329 and perplexity is 159.7080412837104
At time: 14.324564456939697 and batch: 450, loss is 5.085065031051636 and perplexity is 161.5904472926431
At time: 14.769885063171387 and batch: 500, loss is 5.064745950698852 and perplexity is 158.34021074364435
At time: 15.215445041656494 and batch: 550, loss is 5.106421995162964 and perplexity is 165.07864474475156
At time: 15.660596132278442 and batch: 600, loss is 5.109549226760865 and perplexity is 165.59569194052494
At time: 16.105024337768555 and batch: 650, loss is 5.096814975738526 and perplexity is 163.50032461342133
At time: 16.55121374130249 and batch: 700, loss is 5.070074729919433 and perplexity is 159.18622287242573
At time: 16.99705481529236 and batch: 750, loss is 4.978165092468262 and perplexity is 145.2076943631233
At time: 17.443618535995483 and batch: 800, loss is 5.046936273574829 and perplexity is 155.5451858379595
At time: 17.888647317886353 and batch: 850, loss is 5.0197970485687256 and perplexity is 151.38057777128725
At time: 18.333871841430664 and batch: 900, loss is 5.153472843170166 and perplexity is 173.03135886211325
At time: 18.7786123752594 and batch: 950, loss is 5.078167982101441 and perplexity is 160.47978461798112
At time: 19.22437357902527 and batch: 1000, loss is 5.050242824554443 and perplexity is 156.0603551720253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.175779017006478 and perplexity of 176.93439550563392
Finished 2 epochs...
Completing Train Step...
At time: 20.617848873138428 and batch: 50, loss is 5.092562971115112 and perplexity is 162.80659639023207
At time: 21.082184314727783 and batch: 100, loss is 5.023883047103882 and perplexity is 152.00038399164112
At time: 21.530871868133545 and batch: 150, loss is 5.054253702163696 and perplexity is 156.6875511185506
At time: 21.97918939590454 and batch: 200, loss is 5.0580619430541995 and perplexity is 157.28539269760063
At time: 22.428222179412842 and batch: 250, loss is 5.037571315765381 and perplexity is 154.09531133832806
At time: 22.879263639450073 and batch: 300, loss is 4.944263925552368 and perplexity is 140.3674918921085
At time: 23.337000131607056 and batch: 350, loss is 5.00141791343689 and perplexity is 148.6237453763514
At time: 23.794971227645874 and batch: 400, loss is 4.947440128326416 and perplexity is 140.81403629161784
At time: 24.286224126815796 and batch: 450, loss is 4.964766969680786 and perplexity is 143.27515892687686
At time: 24.756695985794067 and batch: 500, loss is 4.931248512268066 and perplexity is 138.55238875547252
At time: 25.22640872001648 and batch: 550, loss is 4.984979124069214 and perplexity is 146.20052292230207
At time: 25.692285776138306 and batch: 600, loss is 5.017399692535401 and perplexity is 151.0180992984225
At time: 26.15843176841736 and batch: 650, loss is 5.008162097930908 and perplexity is 149.6294789496821
At time: 26.624117612838745 and batch: 700, loss is 4.973103857040405 and perplexity is 144.4746207300103
At time: 27.089624166488647 and batch: 750, loss is 4.902446870803833 and perplexity is 134.6187717625099
At time: 27.555612802505493 and batch: 800, loss is 4.95320240020752 and perplexity is 141.62778732913378
At time: 28.02027916908264 and batch: 850, loss is 4.938034591674804 and perplexity is 139.49581372548155
At time: 28.486907243728638 and batch: 900, loss is 5.061219882965088 and perplexity is 157.78287561374563
At time: 28.95230007171631 and batch: 950, loss is 4.984007873535156 and perplexity is 146.05859452150895
At time: 29.417853116989136 and batch: 1000, loss is 4.942876996994019 and perplexity is 140.17294714995387
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.160919561618712 and perplexity of 174.3246842125773
Finished 3 epochs...
Completing Train Step...
At time: 30.849271774291992 and batch: 50, loss is 5.026196165084839 and perplexity is 152.35238576683304
At time: 31.323442697525024 and batch: 100, loss is 4.970232830047608 and perplexity is 144.06042506210255
At time: 31.780581951141357 and batch: 150, loss is 4.994387989044189 and perplexity is 147.58259557337814
At time: 32.25439167022705 and batch: 200, loss is 4.986523637771606 and perplexity is 146.42650610538993
At time: 32.71113467216492 and batch: 250, loss is 4.970479125976563 and perplexity is 144.09591092815165
At time: 33.32843828201294 and batch: 300, loss is 4.891685438156128 and perplexity is 133.17784802484738
At time: 33.78609752655029 and batch: 350, loss is 4.9420264720916744 and perplexity is 140.05377725343402
At time: 34.24376964569092 and batch: 400, loss is 4.8783126640319825 and perplexity is 131.40874600982062
At time: 34.70207929611206 and batch: 450, loss is 4.90659948348999 and perplexity is 135.1789536862171
At time: 35.15977764129639 and batch: 500, loss is 4.882493228912353 and perplexity is 131.95925872364035
At time: 35.61773061752319 and batch: 550, loss is 4.9364673519134525 and perplexity is 139.27736156783683
At time: 36.074774980545044 and batch: 600, loss is 4.9533704662322995 and perplexity is 141.6515921486832
At time: 36.53158164024353 and batch: 650, loss is 4.947633457183838 and perplexity is 140.84126234006865
At time: 36.98856019973755 and batch: 700, loss is 4.9185823059082034 and perplexity is 136.80852300395415
At time: 37.446290016174316 and batch: 750, loss is 4.852073183059693 and perplexity is 128.0054937678227
At time: 37.90257263183594 and batch: 800, loss is 4.8975465965271 and perplexity is 133.96071650051897
At time: 38.36008286476135 and batch: 850, loss is 4.884793615341186 and perplexity is 132.26316542898013
At time: 38.819961071014404 and batch: 900, loss is 4.995809087753296 and perplexity is 147.79247410316674
At time: 39.28622388839722 and batch: 950, loss is 4.921080188751221 and perplexity is 137.1506818247623
At time: 39.750704526901245 and batch: 1000, loss is 4.892149505615234 and perplexity is 133.2396658731059
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.14793842594798 and perplexity of 172.07637620345892
Finished 4 epochs...
Completing Train Step...
At time: 41.19640231132507 and batch: 50, loss is 4.982954158782959 and perplexity is 145.90477148281158
At time: 41.65203380584717 and batch: 100, loss is 4.930262613296509 and perplexity is 138.4158574120062
At time: 42.1074321269989 and batch: 150, loss is 4.934087514877319 and perplexity is 138.94629823911978
At time: 42.56252837181091 and batch: 200, loss is 4.949047660827636 and perplexity is 141.04058147216918
At time: 43.02116918563843 and batch: 250, loss is 4.928242406845093 and perplexity is 138.13651106753434
At time: 43.482847452163696 and batch: 300, loss is 4.837435541152954 and perplexity is 126.14544178656523
At time: 43.96183204650879 and batch: 350, loss is 4.886532363891601 and perplexity is 132.4933378641481
At time: 44.42488956451416 and batch: 400, loss is 4.842300300598144 and perplexity is 126.76060411113232
At time: 44.88716268539429 and batch: 450, loss is 4.898437423706055 and perplexity is 134.08010551728506
At time: 45.35010862350464 and batch: 500, loss is 4.86931396484375 and perplexity is 130.2315428262555
At time: 45.81272339820862 and batch: 550, loss is 4.899745578765869 and perplexity is 134.25561785938808
At time: 46.27594184875488 and batch: 600, loss is 4.911710186004639 and perplexity is 135.87158150453755
At time: 46.73979926109314 and batch: 650, loss is 4.915587892532349 and perplexity is 136.39947446858784
At time: 47.20245385169983 and batch: 700, loss is 4.884033403396606 and perplexity is 132.16265560001995
At time: 47.66701412200928 and batch: 750, loss is 4.83498519897461 and perplexity is 125.83672068059143
At time: 48.13045120239258 and batch: 800, loss is 4.869029207229614 and perplexity is 130.1944636823699
At time: 48.5949342250824 and batch: 850, loss is 4.858161582946777 and perplexity is 128.7872197169251
At time: 49.057764768600464 and batch: 900, loss is 4.962325534820557 and perplexity is 142.92578861523495
At time: 49.520066022872925 and batch: 950, loss is 4.897979249954224 and perplexity is 134.01868760340676
At time: 49.98281145095825 and batch: 1000, loss is 4.874615440368652 and perplexity is 130.92379552213146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.138214483493712 and perplexity of 170.41122446405248
Finished 5 epochs...
Completing Train Step...
At time: 51.43172478675842 and batch: 50, loss is 4.939181270599366 and perplexity is 139.65586237981756
At time: 51.89297151565552 and batch: 100, loss is 4.886314907073975 and perplexity is 132.46452941694957
At time: 52.35336375236511 and batch: 150, loss is 4.920256929397583 and perplexity is 137.03781770768686
At time: 52.81457877159119 and batch: 200, loss is 4.9228143310546875 and perplexity is 137.38872696648403
At time: 53.2920401096344 and batch: 250, loss is 4.906920566558838 and perplexity is 135.22236432834842
At time: 53.75357484817505 and batch: 300, loss is 4.82138162612915 and perplexity is 124.13648256076519
At time: 54.21393370628357 and batch: 350, loss is 4.872567462921142 and perplexity is 130.6559409152767
At time: 54.67533850669861 and batch: 400, loss is 4.812770357131958 and perplexity is 123.0720993395302
At time: 55.135199308395386 and batch: 450, loss is 4.863150491714477 and perplexity is 129.43133278124066
At time: 55.59542942047119 and batch: 500, loss is 4.82750545501709 and perplexity is 124.89900553586524
At time: 56.055949687957764 and batch: 550, loss is 4.889994688034058 and perplexity is 132.9528678083013
At time: 56.516433000564575 and batch: 600, loss is 4.899962348937988 and perplexity is 134.2847236272966
At time: 56.976391315460205 and batch: 650, loss is 4.892597417831421 and perplexity is 133.2993589147545
At time: 57.43619918823242 and batch: 700, loss is 4.869920663833618 and perplexity is 130.31057814463676
At time: 57.89576196670532 and batch: 750, loss is 4.807279663085938 and perplexity is 122.39819987738116
At time: 58.3556342124939 and batch: 800, loss is 4.838419208526611 and perplexity is 126.26958799126149
At time: 58.8155677318573 and batch: 850, loss is 4.82035343170166 and perplexity is 124.00891171620258
At time: 59.27588391304016 and batch: 900, loss is 4.954638977050781 and perplexity is 141.83139274116724
At time: 59.73637294769287 and batch: 950, loss is 4.883972911834717 and perplexity is 132.15466111636107
At time: 60.196593046188354 and batch: 1000, loss is 4.829484825134277 and perplexity is 125.1464717278863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.1594465302258 and perplexity of 174.0680875140662
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 61.62208271026611 and batch: 50, loss is 4.887662801742554 and perplexity is 132.64319803611542
At time: 62.09814786911011 and batch: 100, loss is 4.7796817398071285 and perplexity is 119.06644990276992
At time: 62.556705713272095 and batch: 150, loss is 4.800699090957641 and perplexity is 121.59539405258379
At time: 63.01566123962402 and batch: 200, loss is 4.80402512550354 and perplexity is 122.00049785499043
At time: 63.47355246543884 and batch: 250, loss is 4.793678588867188 and perplexity is 120.74472289770677
At time: 63.9311249256134 and batch: 300, loss is 4.686380491256714 and perplexity is 108.45989694610746
At time: 64.38891220092773 and batch: 350, loss is 4.757758836746216 and perplexity is 116.48457219738648
At time: 64.86317372322083 and batch: 400, loss is 4.675012893676758 and perplexity is 107.2339497230161
At time: 65.32184743881226 and batch: 450, loss is 4.726023607254028 and perplexity is 112.8459492080301
At time: 65.78049755096436 and batch: 500, loss is 4.653188581466675 and perplexity is 104.91899554983084
At time: 66.24535655975342 and batch: 550, loss is 4.720507354736328 and perplexity is 112.22517620105688
At time: 66.71378684043884 and batch: 600, loss is 4.7275947570800785 and perplexity is 113.02338645524837
At time: 67.18027377128601 and batch: 650, loss is 4.699959316253662 and perplexity is 109.94269948023813
At time: 67.6350929737091 and batch: 700, loss is 4.6710719299316406 and perplexity is 106.81217625776657
At time: 68.0916440486908 and batch: 750, loss is 4.618891057968139 and perplexity is 101.38154351453855
At time: 68.55966639518738 and batch: 800, loss is 4.666122026443482 and perplexity is 106.2847726692115
At time: 69.02517533302307 and batch: 850, loss is 4.613439388275147 and perplexity is 100.8303486576372
At time: 69.4842438697815 and batch: 900, loss is 4.7346664237976075 and perplexity is 113.8254829123243
At time: 69.9420211315155 and batch: 950, loss is 4.708366222381592 and perplexity is 110.87087350352716
At time: 70.40028643608093 and batch: 1000, loss is 4.646654090881348 and perplexity is 104.23563848791805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.019899693931022 and perplexity of 151.39611708304267
Finished 7 epochs...
Completing Train Step...
At time: 71.84473466873169 and batch: 50, loss is 4.770634450912476 and perplexity is 117.99407966924694
At time: 72.29872846603394 and batch: 100, loss is 4.700121183395385 and perplexity is 109.96049703113682
At time: 72.75210356712341 and batch: 150, loss is 4.723124599456787 and perplexity is 112.51928165606192
At time: 73.20787405967712 and batch: 200, loss is 4.732302951812744 and perplexity is 113.55677723654837
At time: 73.66079831123352 and batch: 250, loss is 4.728197088241577 and perplexity is 113.09148446960769
At time: 74.11472511291504 and batch: 300, loss is 4.63215485572815 and perplexity is 102.7352053068741
At time: 74.5694580078125 and batch: 350, loss is 4.69645474433899 and perplexity is 109.5580717548456
At time: 75.02439546585083 and batch: 400, loss is 4.622035522460937 and perplexity is 101.70083591706856
At time: 75.48236966133118 and batch: 450, loss is 4.675155868530274 and perplexity is 107.24928257734982
At time: 75.93923902511597 and batch: 500, loss is 4.604250383377075 and perplexity is 99.90806202777377
At time: 76.41274333000183 and batch: 550, loss is 4.666095771789551 and perplexity is 106.28198223591804
At time: 76.87025618553162 and batch: 600, loss is 4.688759765625 and perplexity is 108.7182600353546
At time: 77.32711720466614 and batch: 650, loss is 4.67625165939331 and perplexity is 107.36686977498756
At time: 77.78482723236084 and batch: 700, loss is 4.623781347274781 and perplexity is 101.87854283744066
At time: 78.24475741386414 and batch: 750, loss is 4.580749387741089 and perplexity is 97.58749768665707
At time: 78.70695281028748 and batch: 800, loss is 4.626826581954956 and perplexity is 102.18925977212932
At time: 79.16984510421753 and batch: 850, loss is 4.591867904663086 and perplexity is 98.67858030124312
At time: 79.6328477859497 and batch: 900, loss is 4.698519496917725 and perplexity is 109.78451576099744
At time: 80.09370613098145 and batch: 950, loss is 4.664873552322388 and perplexity is 106.15216167898731
At time: 80.55590987205505 and batch: 1000, loss is 4.615753507614135 and perplexity is 101.06395230653509
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.015517816310975 and perplexity of 150.73416917236557
Finished 8 epochs...
Completing Train Step...
At time: 81.98668336868286 and batch: 50, loss is 4.719718227386474 and perplexity is 112.13665117852659
At time: 82.45610523223877 and batch: 100, loss is 4.652475061416626 and perplexity is 104.84416044422419
At time: 82.91024494171143 and batch: 150, loss is 4.688220615386963 and perplexity is 108.65966035801208
At time: 83.36500787734985 and batch: 200, loss is 4.692332429885864 and perplexity is 109.10736854063208
At time: 83.81978058815002 and batch: 250, loss is 4.686570243835449 and perplexity is 108.48047944397159
At time: 84.27537560462952 and batch: 300, loss is 4.59335693359375 and perplexity is 98.82562501189355
At time: 84.73177886009216 and batch: 350, loss is 4.661348581314087 and perplexity is 105.77863710501406
At time: 85.18635511398315 and batch: 400, loss is 4.581338548660279 and perplexity is 97.64500936664912
At time: 85.64136171340942 and batch: 450, loss is 4.639755907058716 and perplexity is 103.5190762233319
At time: 86.09682703018188 and batch: 500, loss is 4.579598331451416 and perplexity is 97.47523360718401
At time: 86.55179953575134 and batch: 550, loss is 4.637664947509766 and perplexity is 103.30284816324878
At time: 87.00695013999939 and batch: 600, loss is 4.651202945709229 and perplexity is 104.71087133845161
At time: 87.46237301826477 and batch: 650, loss is 4.652588386535644 and perplexity is 104.85604259444517
At time: 87.91762137413025 and batch: 700, loss is 4.600138387680054 and perplexity is 99.49808399824069
At time: 88.3893313407898 and batch: 750, loss is 4.554808797836304 and perplexity is 95.08857235655316
At time: 88.84555315971375 and batch: 800, loss is 4.598289470672608 and perplexity is 99.31429026056995
At time: 89.30241060256958 and batch: 850, loss is 4.573296146392822 and perplexity is 96.86285832437576
At time: 89.7582778930664 and batch: 900, loss is 4.674832906723022 and perplexity is 107.21465074790237
At time: 90.21443271636963 and batch: 950, loss is 4.632713985443115 and perplexity is 102.7926636747778
At time: 90.67242288589478 and batch: 1000, loss is 4.591294279098511 and perplexity is 98.6219919767128
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.009134432164634 and perplexity of 149.7750395697583
Finished 9 epochs...
Completing Train Step...
At time: 92.09779047966003 and batch: 50, loss is 4.686893663406372 and perplexity is 108.51556982823949
At time: 92.57099390029907 and batch: 100, loss is 4.623935642242432 and perplexity is 101.89426339668255
At time: 93.02968239784241 and batch: 150, loss is 4.665586347579956 and perplexity is 106.22785340956123
At time: 93.4957230091095 and batch: 200, loss is 4.669838352203369 and perplexity is 106.68049637141256
At time: 93.96174836158752 and batch: 250, loss is 4.658020343780517 and perplexity is 105.42716588903981
At time: 94.42263579368591 and batch: 300, loss is 4.56849796295166 and perplexity is 96.39920579598108
At time: 94.88536047935486 and batch: 350, loss is 4.64237961769104 and perplexity is 103.79103694136955
At time: 95.34811878204346 and batch: 400, loss is 4.560180740356445 and perplexity is 95.60075718359154
At time: 95.81176924705505 and batch: 450, loss is 4.617362184524536 and perplexity is 101.22666239197422
At time: 96.27491092681885 and batch: 500, loss is 4.555620288848877 and perplexity is 95.16576719564173
At time: 96.73735857009888 and batch: 550, loss is 4.616432466506958 and perplexity is 101.13259387546233
At time: 97.1995177268982 and batch: 600, loss is 4.631845636367798 and perplexity is 102.70344250349345
At time: 97.661865234375 and batch: 650, loss is 4.625758695602417 and perplexity is 102.08019150286569
At time: 98.12455224990845 and batch: 700, loss is 4.578968477249146 and perplexity is 97.41385775262823
At time: 98.58641171455383 and batch: 750, loss is 4.534274749755859 and perplexity is 93.15592943848497
At time: 99.04840922355652 and batch: 800, loss is 4.58108512878418 and perplexity is 97.62026731566989
At time: 99.50922346115112 and batch: 850, loss is 4.559755878448486 and perplexity is 95.56014869060394
At time: 99.98748660087585 and batch: 900, loss is 4.660251989364624 and perplexity is 105.662704680039
At time: 100.44898414611816 and batch: 950, loss is 4.619835138320923 and perplexity is 101.47730103218872
At time: 100.91089653968811 and batch: 1000, loss is 4.575431509017944 and perplexity is 97.06991664542586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.013518170612614 and perplexity of 150.43305540014936
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 102.35224795341492 and batch: 50, loss is 4.6452825736999515 and perplexity is 104.09277551072645
At time: 102.80828762054443 and batch: 100, loss is 4.552654218673706 and perplexity is 94.88391705215084
At time: 103.26471900939941 and batch: 150, loss is 4.5807893848419186 and perplexity is 97.59140098170148
At time: 103.7203414440155 and batch: 200, loss is 4.584261922836304 and perplexity is 97.9308799151487
At time: 104.17899513244629 and batch: 250, loss is 4.561034259796142 and perplexity is 95.68238912056431
At time: 104.63764572143555 and batch: 300, loss is 4.469349565505982 and perplexity is 87.29992166229027
At time: 105.09808444976807 and batch: 350, loss is 4.534029006958008 and perplexity is 93.133039852339
At time: 105.55749487876892 and batch: 400, loss is 4.451716499328613 and perplexity is 85.77404881062525
At time: 106.01742196083069 and batch: 450, loss is 4.4983002471923825 and perplexity is 89.86425439214435
At time: 106.47752785682678 and batch: 500, loss is 4.437996025085449 and perplexity is 84.60522492817893
At time: 106.9385871887207 and batch: 550, loss is 4.499637651443481 and perplexity is 89.98451963168549
At time: 107.39831852912903 and batch: 600, loss is 4.5109439468383785 and perplexity is 91.00768439369087
At time: 107.85967063903809 and batch: 650, loss is 4.489508094787598 and perplexity is 89.0776173555174
At time: 108.32036566734314 and batch: 700, loss is 4.447685918807983 and perplexity is 85.42902538974525
At time: 108.78100109100342 and batch: 750, loss is 4.398818769454956 and perplexity is 81.35471321303966
At time: 109.24206519126892 and batch: 800, loss is 4.444829387664795 and perplexity is 85.18534292715829
At time: 109.70273876190186 and batch: 850, loss is 4.415254163742065 and perplexity is 82.70285830301074
At time: 110.16303372383118 and batch: 900, loss is 4.517571716308594 and perplexity is 91.61286563140473
At time: 110.62388563156128 and batch: 950, loss is 4.463699188232422 and perplexity is 86.80803514982635
At time: 111.0853853225708 and batch: 1000, loss is 4.432802333831787 and perplexity is 84.16695062733957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.927687482136052 and perplexity of 138.05987696936612
Finished 11 epochs...
Completing Train Step...
At time: 112.5092179775238 and batch: 50, loss is 4.578804922103882 and perplexity is 97.39792651782602
At time: 112.98025679588318 and batch: 100, loss is 4.506449909210205 and perplexity is 90.59961007305026
At time: 113.43600845336914 and batch: 150, loss is 4.548689842224121 and perplexity is 94.50850611276026
At time: 113.89030385017395 and batch: 200, loss is 4.5573343086242675 and perplexity is 95.32902307448909
At time: 114.34563779830933 and batch: 250, loss is 4.536476917266846 and perplexity is 93.36130044743574
At time: 114.80120062828064 and batch: 300, loss is 4.447717151641846 and perplexity is 85.43169362197027
At time: 115.25738883018494 and batch: 350, loss is 4.51278862953186 and perplexity is 91.17571963226402
At time: 115.71400570869446 and batch: 400, loss is 4.433700065612793 and perplexity is 84.24254389999294
At time: 116.16986656188965 and batch: 450, loss is 4.480755491256714 and perplexity is 88.30135838646083
At time: 116.6245367527008 and batch: 500, loss is 4.422622394561768 and perplexity is 83.31448257972964
At time: 117.08050751686096 and batch: 550, loss is 4.485587549209595 and perplexity is 88.72906819458393
At time: 117.53754925727844 and batch: 600, loss is 4.499597263336182 and perplexity is 89.98088540064167
At time: 117.99551844596863 and batch: 650, loss is 4.477906188964844 and perplexity is 88.05011922177316
At time: 118.452960729599 and batch: 700, loss is 4.439576930999756 and perplexity is 84.73908360976083
At time: 118.91198635101318 and batch: 750, loss is 4.394182510375977 and perplexity is 80.97840469124749
At time: 119.36981201171875 and batch: 800, loss is 4.440004577636719 and perplexity is 84.77532974360207
At time: 119.83087754249573 and batch: 850, loss is 4.410477972030639 and perplexity is 82.308795405758
At time: 120.29323506355286 and batch: 900, loss is 4.516692123413086 and perplexity is 91.53231903495833
At time: 120.75644540786743 and batch: 950, loss is 4.464754247665406 and perplexity is 86.8996711183661
At time: 121.2186529636383 and batch: 1000, loss is 4.431614646911621 and perplexity is 84.06704598043423
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.926859041539634 and perplexity of 137.94554992566913
Finished 12 epochs...
Completing Train Step...
At time: 122.65660786628723 and batch: 50, loss is 4.560928821563721 and perplexity is 95.67230107042401
At time: 123.13257384300232 and batch: 100, loss is 4.489708366394043 and perplexity is 89.09545885955721
At time: 123.59157872200012 and batch: 150, loss is 4.533178062438965 and perplexity is 93.0538225120957
At time: 124.06675267219543 and batch: 200, loss is 4.543004703521729 and perplexity is 93.97273655212672
At time: 124.5279049873352 and batch: 250, loss is 4.523031883239746 and perplexity is 92.11445530629649
At time: 124.98808288574219 and batch: 300, loss is 4.434845628738404 and perplexity is 84.33910434937768
At time: 125.44719696044922 and batch: 350, loss is 4.500530452728271 and perplexity is 90.06489380016549
At time: 125.90737247467041 and batch: 400, loss is 4.420882339477539 and perplexity is 83.16963684705058
At time: 126.36712455749512 and batch: 450, loss is 4.467524194717408 and perplexity is 87.14071228772076
At time: 126.82659554481506 and batch: 500, loss is 4.408832664489746 and perplexity is 82.17348346939426
At time: 127.28556489944458 and batch: 550, loss is 4.473222351074218 and perplexity is 87.6386710675227
At time: 127.74522399902344 and batch: 600, loss is 4.487704648971557 and perplexity is 88.91711547092243
At time: 128.20406103134155 and batch: 650, loss is 4.465543661117554 and perplexity is 86.9682979716509
At time: 128.66370558738708 and batch: 700, loss is 4.429713163375855 and perplexity is 83.90734575839156
At time: 129.12211561203003 and batch: 750, loss is 4.387280197143554 and perplexity is 80.42139093016588
At time: 129.58158779144287 and batch: 800, loss is 4.43253888130188 and perplexity is 84.14477955190343
At time: 130.04470229148865 and batch: 850, loss is 4.402708339691162 and perplexity is 81.67176428061045
At time: 130.50863027572632 and batch: 900, loss is 4.510217227935791 and perplexity is 90.9415714148466
At time: 130.97219109535217 and batch: 950, loss is 4.458375015258789 and perplexity is 86.34708234031103
At time: 131.4363031387329 and batch: 1000, loss is 4.425083837509155 and perplexity is 83.51980902128388
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.926140389791349 and perplexity of 137.84645072821766
Finished 13 epochs...
Completing Train Step...
At time: 132.87851858139038 and batch: 50, loss is 4.5457798194885255 and perplexity is 94.23388398338201
At time: 133.34010863304138 and batch: 100, loss is 4.47556432723999 and perplexity is 87.84415927668414
At time: 133.7988338470459 and batch: 150, loss is 4.520587463378906 and perplexity is 91.889563878792
At time: 134.25623059272766 and batch: 200, loss is 4.5311205959320064 and perplexity is 92.86256421017687
At time: 134.71437048912048 and batch: 250, loss is 4.511565561294556 and perplexity is 91.06427367246788
At time: 135.1726222038269 and batch: 300, loss is 4.423688850402832 and perplexity is 83.40338119111816
At time: 135.64657998085022 and batch: 350, loss is 4.48980710029602 and perplexity is 89.10425603614148
At time: 136.1045925617218 and batch: 400, loss is 4.412520561218262 and perplexity is 82.47709028143383
At time: 136.56321787834167 and batch: 450, loss is 4.458426151275635 and perplexity is 86.35149789906426
At time: 137.0213906764984 and batch: 500, loss is 4.40023193359375 and perplexity is 81.4697620485721
At time: 137.47915720939636 and batch: 550, loss is 4.464309329986572 and perplexity is 86.8610165180959
At time: 137.93753743171692 and batch: 600, loss is 4.4783234214782714 and perplexity is 88.08686425940282
At time: 138.39609575271606 and batch: 650, loss is 4.4553530883789065 and perplexity is 86.08654163693718
At time: 138.85431742668152 and batch: 700, loss is 4.421047925949097 and perplexity is 83.18340975402887
At time: 139.3147337436676 and batch: 750, loss is 4.380417108535767 and perplexity is 79.8713414763541
At time: 139.77499103546143 and batch: 800, loss is 4.425660638809204 and perplexity is 83.56799725188822
At time: 140.23617720603943 and batch: 850, loss is 4.396080236434937 and perplexity is 81.1322254286903
At time: 140.6971571445465 and batch: 900, loss is 4.5040999698638915 and perplexity is 90.38695644400389
At time: 141.15771651268005 and batch: 950, loss is 4.452337074279785 and perplexity is 85.82729455655698
At time: 141.61891436576843 and batch: 1000, loss is 4.418381109237671 and perplexity is 82.96187038053252
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.927249071074695 and perplexity of 137.9993632580827
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 143.04626321792603 and batch: 50, loss is 4.531081743240357 and perplexity is 92.8589563196924
At time: 143.523095369339 and batch: 100, loss is 4.450886344909668 and perplexity is 85.70287265266289
At time: 143.97580933570862 and batch: 150, loss is 4.494088764190674 and perplexity is 89.48658843739365
At time: 144.42927527427673 and batch: 200, loss is 4.498680200576782 and perplexity is 89.89840510716621
At time: 144.88433694839478 and batch: 250, loss is 4.474766588211059 and perplexity is 87.77411050639934
At time: 145.3392596244812 and batch: 300, loss is 4.386760082244873 and perplexity is 80.3795734424623
At time: 145.79325413703918 and batch: 350, loss is 4.452672138214111 and perplexity is 85.85605700590429
At time: 146.24748253822327 and batch: 400, loss is 4.3682809257507325 and perplexity is 78.90786655635657
At time: 146.70292854309082 and batch: 450, loss is 4.416425170898438 and perplexity is 82.79976066754404
At time: 147.1570885181427 and batch: 500, loss is 4.353986215591431 and perplexity is 77.78792516399686
At time: 147.62830591201782 and batch: 550, loss is 4.418397121429443 and perplexity is 82.96319879254622
At time: 148.08212566375732 and batch: 600, loss is 4.430732975006103 and perplexity is 83.99295909277639
At time: 148.53643035888672 and batch: 650, loss is 4.401501445770264 and perplexity is 81.57325458212667
At time: 148.99089241027832 and batch: 700, loss is 4.368702068328857 and perplexity is 78.94110501728673
At time: 149.4470624923706 and batch: 750, loss is 4.324510583877563 and perplexity is 75.52853892799003
At time: 149.90411710739136 and batch: 800, loss is 4.365526342391968 and perplexity is 78.69080735135068
At time: 150.36340761184692 and batch: 850, loss is 4.32984959602356 and perplexity is 75.93286510543771
At time: 150.82395124435425 and batch: 900, loss is 4.439995584487915 and perplexity is 84.77456734987493
At time: 151.28194999694824 and batch: 950, loss is 4.384939947128296 and perplexity is 80.23340482185725
At time: 151.73923254013062 and batch: 1000, loss is 4.351529340744019 and perplexity is 77.59704454813773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.897706938952934 and perplexity of 133.9821978089057
Finished 15 epochs...
Completing Train Step...
At time: 153.18989849090576 and batch: 50, loss is 4.506493072509766 and perplexity is 90.60352073555792
At time: 153.66170001029968 and batch: 100, loss is 4.431606750488282 and perplexity is 84.06638215407122
At time: 154.11974549293518 and batch: 150, loss is 4.475929145812988 and perplexity is 87.876212303932
At time: 154.57704544067383 and batch: 200, loss is 4.4822053050994874 and perplexity is 88.42947176602004
At time: 155.03432488441467 and batch: 250, loss is 4.460670614242554 and perplexity is 86.54552830381147
At time: 155.49134826660156 and batch: 300, loss is 4.375242900848389 and perplexity is 79.45913790038213
At time: 155.95069813728333 and batch: 350, loss is 4.440916814804077 and perplexity is 84.85270023502271
At time: 156.4066083431244 and batch: 400, loss is 4.359197187423706 and perplexity is 78.19433382331327
At time: 156.8655354976654 and batch: 450, loss is 4.4080683135986325 and perplexity is 82.1106980921648
At time: 157.3257339000702 and batch: 500, loss is 4.347845621109009 and perplexity is 77.31172463344723
At time: 157.7880208492279 and batch: 550, loss is 4.412109785079956 and perplexity is 82.44321761830648
At time: 158.25055623054504 and batch: 600, loss is 4.425222311019898 and perplexity is 83.53137510323508
At time: 158.71263670921326 and batch: 650, loss is 4.398081607818604 and perplexity is 81.2947637384637
At time: 159.19849300384521 and batch: 700, loss is 4.3680346393585205 and perplexity is 78.88843501554553
At time: 159.66143989562988 and batch: 750, loss is 4.32319242477417 and perplexity is 75.42904588507237
At time: 160.12433409690857 and batch: 800, loss is 4.364648914337158 and perplexity is 78.62179211170645
At time: 160.58611679077148 and batch: 850, loss is 4.3318126773834225 and perplexity is 76.08207390411586
At time: 161.0470814704895 and batch: 900, loss is 4.442938375473022 and perplexity is 85.02440861745102
At time: 161.50646090507507 and batch: 950, loss is 4.38831880569458 and perplexity is 80.50496066507971
At time: 161.96522784233093 and batch: 1000, loss is 4.354957513809204 and perplexity is 77.8635171423069
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.896554714295922 and perplexity of 133.8279091216742
Finished 16 epochs...
Completing Train Step...
At time: 163.47183418273926 and batch: 50, loss is 4.498626928329468 and perplexity is 89.89361614465675
At time: 163.95413374900818 and batch: 100, loss is 4.424025325775147 and perplexity is 83.43144909667141
At time: 164.4570324420929 and batch: 150, loss is 4.46763503074646 and perplexity is 87.15037115350482
At time: 164.95083570480347 and batch: 200, loss is 4.474494123458863 and perplexity is 87.7501984128802
At time: 165.4418294429779 and batch: 250, loss is 4.453462152481079 and perplexity is 85.92391131525171
At time: 165.9202013015747 and batch: 300, loss is 4.368746013641357 and perplexity is 78.94457418504209
At time: 166.39421892166138 and batch: 350, loss is 4.434110050201416 and perplexity is 84.27708912571948
At time: 166.85743975639343 and batch: 400, loss is 4.353818359375 and perplexity is 77.77486907299853
At time: 167.32205033302307 and batch: 450, loss is 4.40322211265564 and perplexity is 81.71373580605332
At time: 167.7838728427887 and batch: 500, loss is 4.343830986022949 and perplexity is 77.00196846619443
At time: 168.25796723365784 and batch: 550, loss is 4.408205862045288 and perplexity is 82.12199306792657
At time: 168.74429655075073 and batch: 600, loss is 4.422313241958618 and perplexity is 83.28872967155449
At time: 169.21689701080322 and batch: 650, loss is 4.39562653541565 and perplexity is 81.0954240043694
At time: 169.68959879875183 and batch: 700, loss is 4.367335186004639 and perplexity is 78.83327552808453
At time: 170.1501386165619 and batch: 750, loss is 4.322061948776245 and perplexity is 75.34382333925018
At time: 170.61264848709106 and batch: 800, loss is 4.363870515823364 and perplexity is 78.56061683803482
At time: 171.07662296295166 and batch: 850, loss is 4.332017688751221 and perplexity is 76.09767319311304
At time: 171.56679558753967 and batch: 900, loss is 4.443346014022827 and perplexity is 85.0590749092563
At time: 172.02839183807373 and batch: 950, loss is 4.38879319190979 and perplexity is 80.5431601686178
At time: 172.49040842056274 and batch: 1000, loss is 4.3558094024658205 and perplexity is 77.92987645068258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.896227208579459 and perplexity of 133.7840868928215
Finished 17 epochs...
Completing Train Step...
At time: 173.92832207679749 and batch: 50, loss is 4.493285446166992 and perplexity is 89.41473111402986
At time: 174.3994746208191 and batch: 100, loss is 4.418805904388428 and perplexity is 82.99711966710088
At time: 174.8571994304657 and batch: 150, loss is 4.461928853988647 and perplexity is 86.65449186413557
At time: 175.31418800354004 and batch: 200, loss is 4.468774833679199 and perplexity is 87.24976203438334
At time: 175.77013325691223 and batch: 250, loss is 4.447863130569458 and perplexity is 85.4441657593018
At time: 176.22610878944397 and batch: 300, loss is 4.363378734588623 and perplexity is 78.52199169922186
At time: 176.68316626548767 and batch: 350, loss is 4.428618755340576 and perplexity is 83.81556711577714
At time: 177.13970184326172 and batch: 400, loss is 4.349673948287964 and perplexity is 77.4532050576874
At time: 177.5948338508606 and batch: 450, loss is 4.399158296585083 and perplexity is 81.38234003510156
At time: 178.04988741874695 and batch: 500, loss is 4.340535402297974 and perplexity is 76.74861972741526
At time: 178.50675010681152 and batch: 550, loss is 4.405074157714844 and perplexity is 81.86521355523222
At time: 178.96416068077087 and batch: 600, loss is 4.4196247959136965 and perplexity is 83.06511314085806
At time: 179.42080426216125 and batch: 650, loss is 4.393147411346436 and perplexity is 80.8946273895198
At time: 179.87846112251282 and batch: 700, loss is 4.366347627639771 and perplexity is 78.75546149667974
At time: 180.33619570732117 and batch: 750, loss is 4.3202523994445805 and perplexity is 75.20760825520809
At time: 180.79291129112244 and batch: 800, loss is 4.362289876937866 and perplexity is 78.43653895918519
At time: 181.2513427734375 and batch: 850, loss is 4.331322154998779 and perplexity is 76.04476309543736
At time: 181.7092990875244 and batch: 900, loss is 4.44324499130249 and perplexity is 85.05048244414397
At time: 182.16766595840454 and batch: 950, loss is 4.388404483795166 and perplexity is 80.51185847268854
At time: 182.62550854682922 and batch: 1000, loss is 4.355477275848389 and perplexity is 77.90399816209228
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.896215299280678 and perplexity of 133.78249362764592
Finished 18 epochs...
Completing Train Step...
At time: 184.07997679710388 and batch: 50, loss is 4.488683214187622 and perplexity is 89.0041692541904
At time: 184.55182337760925 and batch: 100, loss is 4.4141056346893315 and perplexity is 82.60792619408375
At time: 185.0063829421997 and batch: 150, loss is 4.456890735626221 and perplexity is 86.21901419264529
At time: 185.46382021903992 and batch: 200, loss is 4.463924665451049 and perplexity is 86.82761059097155
At time: 185.92025184631348 and batch: 250, loss is 4.442754764556884 and perplexity is 85.00879864101557
At time: 186.37565326690674 and batch: 300, loss is 4.358686246871948 and perplexity is 78.15439137222322
At time: 186.83187913894653 and batch: 350, loss is 4.423870067596436 and perplexity is 83.41849668734723
At time: 187.28887391090393 and batch: 400, loss is 4.345735969543457 and perplexity is 77.148795754549
At time: 187.74481868743896 and batch: 450, loss is 4.39561559677124 and perplexity is 81.09453693521463
At time: 188.20384669303894 and batch: 500, loss is 4.337792501449585 and perplexity is 76.53839431878079
At time: 188.66575384140015 and batch: 550, loss is 4.401883630752564 and perplexity is 81.60443661325773
At time: 189.12516164779663 and batch: 600, loss is 4.416896018981934 and perplexity is 82.83875595587622
At time: 189.58382415771484 and batch: 650, loss is 4.390367603302002 and perplexity is 80.67006811399483
At time: 190.0421826839447 and batch: 700, loss is 4.365129623413086 and perplexity is 78.65959540620571
At time: 190.501403093338 and batch: 750, loss is 4.31864592552185 and perplexity is 75.08688618814438
At time: 190.96047925949097 and batch: 800, loss is 4.360212707519532 and perplexity is 78.27378207451059
At time: 191.41959190368652 and batch: 850, loss is 4.3303355884552 and perplexity is 75.96977687188476
At time: 191.87899613380432 and batch: 900, loss is 4.442115383148193 and perplexity is 84.95446296805
At time: 192.33848547935486 and batch: 950, loss is 4.387178649902344 and perplexity is 80.41322477441493
At time: 192.79815912246704 and batch: 1000, loss is 4.3546645450592045 and perplexity is 77.8407089062325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.896348162395198 and perplexity of 133.80026956727474
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 194.23926043510437 and batch: 50, loss is 4.484647169113159 and perplexity is 88.64566836479439
At time: 194.6965456008911 and batch: 100, loss is 4.407070093154907 and perplexity is 82.02877441043029
At time: 195.16779375076294 and batch: 150, loss is 4.450378313064575 and perplexity is 85.65934392206628
At time: 195.6256275177002 and batch: 200, loss is 4.453765125274658 and perplexity is 85.9499478666823
At time: 196.0847611427307 and batch: 250, loss is 4.430784463882446 and perplexity is 83.99728390719976
At time: 196.5416648387909 and batch: 300, loss is 4.347128448486328 and perplexity is 77.25629865849939
At time: 196.99852991104126 and batch: 350, loss is 4.410119943618774 and perplexity is 82.2793317931763
At time: 197.45395946502686 and batch: 400, loss is 4.329511814117431 and perplexity is 75.9072206888784
At time: 197.91285848617554 and batch: 450, loss is 4.379232139587402 and perplexity is 79.77675247042595
At time: 198.36937379837036 and batch: 500, loss is 4.321000003814698 and perplexity is 75.26385481427268
At time: 198.82636737823486 and batch: 550, loss is 4.380903177261352 and perplexity is 79.91017387435888
At time: 199.2825756072998 and batch: 600, loss is 4.396982021331787 and perplexity is 81.20542224316026
At time: 199.74007201194763 and batch: 650, loss is 4.37003879070282 and perplexity is 79.04669791706148
At time: 200.19592905044556 and batch: 700, loss is 4.344600896835328 and perplexity is 77.06127594209484
At time: 200.65404438972473 and batch: 750, loss is 4.296173028945923 and perplexity is 73.41828572742
At time: 201.1106572151184 and batch: 800, loss is 4.335841007232666 and perplexity is 76.38917573185628
At time: 201.5683629512787 and batch: 850, loss is 4.303610639572144 and perplexity is 73.96637807161245
At time: 202.02774691581726 and batch: 900, loss is 4.413753499984741 and perplexity is 82.5788421974394
At time: 202.4920938014984 and batch: 950, loss is 4.358914623260498 and perplexity is 78.17224202813067
At time: 202.95575523376465 and batch: 1000, loss is 4.327795372009278 and perplexity is 75.77704209294762
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.887848272556212 and perplexity of 132.6678017595472
Finished 20 epochs...
Completing Train Step...
At time: 204.4061713218689 and batch: 50, loss is 4.476194772720337 and perplexity is 87.89955769088003
At time: 204.88617610931396 and batch: 100, loss is 4.400516977310181 and perplexity is 81.49298780234352
At time: 205.34663224220276 and batch: 150, loss is 4.444351778030396 and perplexity is 85.14466730097539
At time: 205.8071916103363 and batch: 200, loss is 4.448050928115845 and perplexity is 85.46021347079771
At time: 206.2686574459076 and batch: 250, loss is 4.426847085952759 and perplexity is 83.66720510431932
At time: 206.7462980747223 and batch: 300, loss is 4.342554235458374 and perplexity is 76.90371889292723
At time: 207.20637345314026 and batch: 350, loss is 4.405642747879028 and perplexity is 81.91177454625493
At time: 207.668035030365 and batch: 400, loss is 4.326170454025268 and perplexity is 75.65401059958945
At time: 208.12990760803223 and batch: 450, loss is 4.376032466888428 and perplexity is 79.52190091174961
At time: 208.5927619934082 and batch: 500, loss is 4.317813158035278 and perplexity is 75.02438229987413
At time: 209.05406522750854 and batch: 550, loss is 4.378554229736328 and perplexity is 79.7226893510706
At time: 209.51431393623352 and batch: 600, loss is 4.395523920059204 and perplexity is 81.08710279547829
At time: 209.977520942688 and batch: 650, loss is 4.369110565185547 and perplexity is 78.97335879788542
At time: 210.4408881664276 and batch: 700, loss is 4.343562126159668 and perplexity is 76.98126851029856
At time: 210.903817653656 and batch: 750, loss is 4.296432514190673 and perplexity is 73.43733916119672
At time: 211.36599802970886 and batch: 800, loss is 4.336485843658448 and perplexity is 76.43845014016284
At time: 211.8281319141388 and batch: 850, loss is 4.304605655670166 and perplexity is 74.04001243611994
At time: 212.2892665863037 and batch: 900, loss is 4.415393924713134 and perplexity is 82.7144177425578
At time: 212.75104999542236 and batch: 950, loss is 4.360381317138672 and perplexity is 78.28698089978803
At time: 213.2121045589447 and batch: 1000, loss is 4.328645153045654 and perplexity is 75.8414633544174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.887249085961319 and perplexity of 132.58833280190356
Finished 21 epochs...
Completing Train Step...
At time: 214.6308331489563 and batch: 50, loss is 4.473355922698975 and perplexity is 87.65037788904084
At time: 215.10192680358887 and batch: 100, loss is 4.397957715988159 and perplexity is 81.28469260525704
At time: 215.55839157104492 and batch: 150, loss is 4.442007637023925 and perplexity is 84.94530994703604
At time: 216.01378107070923 and batch: 200, loss is 4.445344762802124 and perplexity is 85.2292566500163
At time: 216.47058629989624 and batch: 250, loss is 4.424822368621826 and perplexity is 83.49797404445708
At time: 216.9270203113556 and batch: 300, loss is 4.339989500045776 and perplexity is 76.7067339168724
At time: 217.38381123542786 and batch: 350, loss is 4.403176393508911 and perplexity is 81.71000000917562
At time: 217.84079718589783 and batch: 400, loss is 4.324153337478638 and perplexity is 75.50156144853251
At time: 218.296000957489 and batch: 450, loss is 4.374062786102295 and perplexity is 79.36542230848617
At time: 218.76682448387146 and batch: 500, loss is 4.315877103805542 and perplexity is 74.87927154366244
At time: 219.2229266166687 and batch: 550, loss is 4.376998109817505 and perplexity is 79.59872776075368
At time: 219.67859172821045 and batch: 600, loss is 4.394658393859864 and perplexity is 81.0169501474371
At time: 220.13461685180664 and batch: 650, loss is 4.368461766242981 and perplexity is 78.92213758413786
At time: 220.5911955833435 and batch: 700, loss is 4.342816343307495 and perplexity is 76.92387860316938
At time: 221.0489022731781 and batch: 750, loss is 4.296523275375367 and perplexity is 73.44400472358231
At time: 221.50700855255127 and batch: 800, loss is 4.336578512191773 and perplexity is 76.44553390744308
At time: 221.96390795707703 and batch: 850, loss is 4.304844465255737 and perplexity is 74.05769601222522
At time: 222.42263388633728 and batch: 900, loss is 4.415999774932861 and perplexity is 82.764545474137
At time: 222.88079142570496 and batch: 950, loss is 4.36089822769165 and perplexity is 78.32745872718272
At time: 223.33912658691406 and batch: 1000, loss is 4.328485145568847 and perplexity is 75.8293291240384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8870324855897485 and perplexity of 132.55961722976684
Finished 22 epochs...
Completing Train Step...
At time: 224.7777910232544 and batch: 50, loss is 4.471273880004883 and perplexity is 87.46807590623341
At time: 225.24006581306458 and batch: 100, loss is 4.3962098979949955 and perplexity is 81.14274584164225
At time: 225.7029893398285 and batch: 150, loss is 4.440135898590088 and perplexity is 84.78646325174165
At time: 226.16203141212463 and batch: 200, loss is 4.443181791305542 and perplexity is 85.04510742376551
At time: 226.61839532852173 and batch: 250, loss is 4.4231378364562985 and perplexity is 83.35743742388449
At time: 227.0727379322052 and batch: 300, loss is 4.337981882095337 and perplexity is 76.55289058193422
At time: 227.52910208702087 and batch: 350, loss is 4.401152296066284 and perplexity is 81.54477827597287
At time: 227.985276222229 and batch: 400, loss is 4.322498121261597 and perplexity is 75.37669340992106
At time: 228.4418807029724 and batch: 450, loss is 4.372498350143433 and perplexity is 79.24135725915241
At time: 228.89565205574036 and batch: 500, loss is 4.314378814697266 and perplexity is 74.76716475183756
At time: 229.35167813301086 and batch: 550, loss is 4.375570640563965 and perplexity is 79.48518408358873
At time: 229.8087055683136 and batch: 600, loss is 4.393904685974121 and perplexity is 80.95591003932721
At time: 230.2799482345581 and batch: 650, loss is 4.367775797843933 and perplexity is 78.86801805603503
At time: 230.7354986667633 and batch: 700, loss is 4.342176103591919 and perplexity is 76.87464464346145
At time: 231.19304990768433 and batch: 750, loss is 4.296332540512085 and perplexity is 73.42999772723626
At time: 231.6524109840393 and batch: 800, loss is 4.336587858200073 and perplexity is 76.4462483713762
At time: 232.11002397537231 and batch: 850, loss is 4.304717617034912 and perplexity is 74.04830252103409
At time: 232.5708088874817 and batch: 900, loss is 4.4162423801422115 and perplexity is 82.78462701986383
At time: 233.0312774181366 and batch: 950, loss is 4.36076397895813 and perplexity is 78.31694407085418
At time: 233.4950487613678 and batch: 1000, loss is 4.328016452789306 and perplexity is 75.79379679252459
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.886970706102325 and perplexity of 132.5514280175266
Finished 23 epochs...
Completing Train Step...
At time: 234.941175699234 and batch: 50, loss is 4.469485540390014 and perplexity is 87.31179306610242
At time: 235.41658806800842 and batch: 100, loss is 4.394737567901611 and perplexity is 81.0233648407656
At time: 235.8759195804596 and batch: 150, loss is 4.438526010513305 and perplexity is 84.65007634876116
At time: 236.33491849899292 and batch: 200, loss is 4.4414217662811275 and perplexity is 84.89555755085993
At time: 236.79564666748047 and batch: 250, loss is 4.4216203689575195 and perplexity is 83.23104114718764
At time: 237.25601172447205 and batch: 300, loss is 4.3363414478302005 and perplexity is 76.42741354368334
At time: 237.71544337272644 and batch: 350, loss is 4.399383268356323 and perplexity is 81.40065082391503
At time: 238.17503428459167 and batch: 400, loss is 4.320991792678833 and perplexity is 75.26323681507235
At time: 238.63570618629456 and batch: 450, loss is 4.370971660614014 and perplexity is 79.1204726088725
At time: 239.09446001052856 and batch: 500, loss is 4.312837219238281 and perplexity is 74.65199282724193
At time: 239.55421447753906 and batch: 550, loss is 4.374282169342041 and perplexity is 79.38283566198514
At time: 240.01281929016113 and batch: 600, loss is 4.393226747512817 and perplexity is 80.90104551372904
At time: 240.4730863571167 and batch: 650, loss is 4.3673026609420775 and perplexity is 78.83071151256365
At time: 240.9319450855255 and batch: 700, loss is 4.341367578506469 and perplexity is 76.81251468503173
At time: 241.39212656021118 and batch: 750, loss is 4.296043214797973 and perplexity is 73.40875561379957
At time: 241.85110688209534 and batch: 800, loss is 4.336273431777954 and perplexity is 76.4222154295103
At time: 242.3286156654358 and batch: 850, loss is 4.304473114013672 and perplexity is 74.03019970053732
At time: 242.78689193725586 and batch: 900, loss is 4.41607286453247 and perplexity is 82.77059492270072
At time: 243.2466516494751 and batch: 950, loss is 4.360511302947998 and perplexity is 78.29715775776921
At time: 243.70742440223694 and batch: 1000, loss is 4.327304391860962 and perplexity is 75.73984620155127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.886731031464367 and perplexity of 132.5196626088392
Finished 24 epochs...
Completing Train Step...
At time: 245.13244700431824 and batch: 50, loss is 4.467873830795288 and perplexity is 87.17118515148458
At time: 245.60761332511902 and batch: 100, loss is 4.3933483982086186 and perplexity is 80.91088778085403
At time: 246.066663980484 and batch: 150, loss is 4.437238321304322 and perplexity is 84.54114350978003
At time: 246.5241870880127 and batch: 200, loss is 4.439814491271973 and perplexity is 84.75921664084002
At time: 246.9812343120575 and batch: 250, loss is 4.42038743019104 and perplexity is 83.128485605326
At time: 247.44000244140625 and batch: 300, loss is 4.334720869064331 and perplexity is 76.30365720569569
At time: 247.8976707458496 and batch: 350, loss is 4.3977313804626466 and perplexity is 81.26629707349979
At time: 248.35562944412231 and batch: 400, loss is 4.319460172653198 and perplexity is 75.1480503678245
At time: 248.81315922737122 and batch: 450, loss is 4.369453191757202 and perplexity is 79.00042180505034
At time: 249.2726559638977 and batch: 500, loss is 4.311363925933838 and perplexity is 74.5420895258249
At time: 249.73023223876953 and batch: 550, loss is 4.373111534118652 and perplexity is 79.28996168980444
At time: 250.1880326271057 and batch: 600, loss is 4.392584505081177 and perplexity is 80.84910411080791
At time: 250.64522290229797 and batch: 650, loss is 4.366887402534485 and perplexity is 78.79798319265687
At time: 251.1053626537323 and batch: 700, loss is 4.340700349807739 and perplexity is 76.76128026525063
At time: 251.56284594535828 and batch: 750, loss is 4.295753593444824 and perplexity is 73.38749794915111
At time: 252.02112221717834 and batch: 800, loss is 4.335966291427613 and perplexity is 76.3987466877727
At time: 252.48030734062195 and batch: 850, loss is 4.3040651512145995 and perplexity is 74.00000429277193
At time: 252.93997430801392 and batch: 900, loss is 4.41571270942688 and perplexity is 82.74079003785945
At time: 253.3969168663025 and batch: 950, loss is 4.36021728515625 and perplexity is 78.27414038426963
At time: 253.87758564949036 and batch: 1000, loss is 4.326632623672485 and perplexity is 75.68898366810166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.886735869616997 and perplexity of 132.52030376074433
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 255.32029223442078 and batch: 50, loss is 4.467135248184204 and perplexity is 87.10682580021866
At time: 255.77568459510803 and batch: 100, loss is 4.39125804901123 and perplexity is 80.74193242089359
At time: 256.2328712940216 and batch: 150, loss is 4.43429799079895 and perplexity is 84.29292970070517
At time: 256.68499422073364 and batch: 200, loss is 4.436282453536987 and perplexity is 84.46037196529333
At time: 257.13676476478577 and batch: 250, loss is 4.417716550827026 and perplexity is 82.9067556873032
At time: 257.5871078968048 and batch: 300, loss is 4.3315756893157955 and perplexity is 76.06404549678336
At time: 258.03591561317444 and batch: 350, loss is 4.392965517044067 and perplexity is 80.87991445584538
At time: 258.50505328178406 and batch: 400, loss is 4.313075561523437 and perplexity is 74.66978767435187
At time: 258.98750948905945 and batch: 450, loss is 4.363799152374267 and perplexity is 78.5550106814938
At time: 259.4528684616089 and batch: 500, loss is 4.30452335357666 and perplexity is 74.03391903884638
At time: 259.93674755096436 and batch: 550, loss is 4.365474233627319 and perplexity is 78.68670697742401
At time: 260.43047428131104 and batch: 600, loss is 4.385503597259522 and perplexity is 80.27864113854407
At time: 260.9172248840332 and batch: 650, loss is 4.35849549293518 and perplexity is 78.13948453620728
At time: 261.39942026138306 and batch: 700, loss is 4.332093443870544 and perplexity is 76.10343819978763
At time: 261.87173438072205 and batch: 750, loss is 4.287293510437012 and perplexity is 72.76925251607634
At time: 262.3223121166229 and batch: 800, loss is 4.325824050903321 and perplexity is 75.62780835266196
At time: 262.7732050418854 and batch: 850, loss is 4.293180837631225 and perplexity is 73.19893250758565
At time: 263.2240390777588 and batch: 900, loss is 4.404321947097778 and perplexity is 81.80365682714235
At time: 263.67375349998474 and batch: 950, loss is 4.34858959197998 and perplexity is 77.36926370560191
At time: 264.12397861480713 and batch: 1000, loss is 4.315111398696899 and perplexity is 74.82195804832834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.884041018602325 and perplexity of 132.16366204966278
Finished 26 epochs...
Completing Train Step...
At time: 265.5100345611572 and batch: 50, loss is 4.464164905548095 and perplexity is 86.84847257040784
At time: 265.9759876728058 and batch: 100, loss is 4.388507375717163 and perplexity is 80.5201429187443
At time: 266.4271836280823 and batch: 150, loss is 4.431430053710938 and perplexity is 84.05152920753416
At time: 266.8783712387085 and batch: 200, loss is 4.434457998275757 and perplexity is 84.30641827880714
At time: 267.3281800746918 and batch: 250, loss is 4.416289415359497 and perplexity is 82.78852090435774
At time: 267.7780649662018 and batch: 300, loss is 4.329910049438476 and perplexity is 75.93745564519324
At time: 268.2278003692627 and batch: 350, loss is 4.3915176105499265 and perplexity is 80.7628926412259
At time: 268.67783641815186 and batch: 400, loss is 4.3119642162323 and perplexity is 74.5868498522451
At time: 269.12800097465515 and batch: 450, loss is 4.3626757335662845 and perplexity is 78.46681005742917
At time: 269.578111410141 and batch: 500, loss is 4.303530921936035 and perplexity is 73.96048188181938
At time: 270.02791357040405 and batch: 550, loss is 4.36502311706543 and perplexity is 78.65121810611664
At time: 270.4777309894562 and batch: 600, loss is 4.385307788848877 and perplexity is 80.2629234442926
At time: 270.93186497688293 and batch: 650, loss is 4.358035349845887 and perplexity is 78.1035374634298
At time: 271.3891530036926 and batch: 700, loss is 4.331897282600403 and perplexity is 76.08851111679391
At time: 271.84809947013855 and batch: 750, loss is 4.287340297698974 and perplexity is 72.77265726980555
At time: 272.3066051006317 and batch: 800, loss is 4.325791425704956 and perplexity is 75.62534102066142
At time: 272.76564383506775 and batch: 850, loss is 4.293453693389893 and perplexity is 73.21890798293646
At time: 273.22473192214966 and batch: 900, loss is 4.404992008209229 and perplexity is 81.85848864463941
At time: 273.6815903186798 and batch: 950, loss is 4.349500942230224 and perplexity is 77.4398063430839
At time: 274.138959646225 and batch: 1000, loss is 4.3155936431884765 and perplexity is 74.8580492271328
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.883675924161586 and perplexity of 132.11541863861717
Finished 27 epochs...
Completing Train Step...
At time: 275.5374553203583 and batch: 50, loss is 4.462772569656372 and perplexity is 86.7276344680416
At time: 276.00414991378784 and batch: 100, loss is 4.3870081615448 and perplexity is 80.39951642438852
At time: 276.4558742046356 and batch: 150, loss is 4.429842319488525 and perplexity is 83.91818360486621
At time: 276.90981817245483 and batch: 200, loss is 4.433492908477783 and perplexity is 84.22509426342339
At time: 277.3659427165985 and batch: 250, loss is 4.415590648651123 and perplexity is 82.73069124918621
At time: 277.85948729515076 and batch: 300, loss is 4.329035005569458 and perplexity is 75.87103610444615
At time: 278.31786608695984 and batch: 350, loss is 4.390592555999756 and perplexity is 80.68821710470081
At time: 278.7790503501892 and batch: 400, loss is 4.3113264465332035 and perplexity is 74.53929578534162
At time: 279.23819971084595 and batch: 450, loss is 4.362051916122437 and perplexity is 78.4178763569871
At time: 279.7001392841339 and batch: 500, loss is 4.302867517471314 and perplexity is 73.91143243953444
At time: 280.1592471599579 and batch: 550, loss is 4.3646502113342285 and perplexity is 78.62189408400664
At time: 280.62261033058167 and batch: 600, loss is 4.385166664123535 and perplexity is 80.2515971604945
At time: 281.08245825767517 and batch: 650, loss is 4.3576537084579465 and perplexity is 78.07373560816166
At time: 281.5422856807709 and batch: 700, loss is 4.331665616035462 and perplexity is 76.07088599444658
At time: 282.0027229785919 and batch: 750, loss is 4.287338848114014 and perplexity is 72.7725517797325
At time: 282.46176290512085 and batch: 800, loss is 4.3256878089904784 and perplexity is 75.61750537725241
At time: 282.92069005966187 and batch: 850, loss is 4.293532514572144 and perplexity is 73.22467941127923
At time: 283.3809177875519 and batch: 900, loss is 4.405352258682251 and perplexity is 81.8879835163463
At time: 283.8395187854767 and batch: 950, loss is 4.350027189254761 and perplexity is 77.48056953556629
At time: 284.3135895729065 and batch: 1000, loss is 4.315747575759888 and perplexity is 74.86957320607627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.883518125952744 and perplexity of 132.09457270696421
Finished 28 epochs...
Completing Train Step...
At time: 285.7734169960022 and batch: 50, loss is 4.4617512416839595 and perplexity is 86.6391023268501
At time: 286.2308247089386 and batch: 100, loss is 4.3858708477020265 and perplexity is 80.30812891939473
At time: 286.6867558956146 and batch: 150, loss is 4.428655338287354 and perplexity is 83.81863339229446
At time: 287.1440010070801 and batch: 200, loss is 4.432761793136597 and perplexity is 84.16353850981437
At time: 287.60035943984985 and batch: 250, loss is 4.415154294967651 and perplexity is 82.6945992823275
At time: 288.05709505081177 and batch: 300, loss is 4.3283167552948 and perplexity is 75.81656127754714
At time: 288.5154402256012 and batch: 350, loss is 4.389886255264282 and perplexity is 80.63124707896955
At time: 288.9702832698822 and batch: 400, loss is 4.310753717422485 and perplexity is 74.49661718355834
At time: 289.4409246444702 and batch: 450, loss is 4.361523942947388 and perplexity is 78.37648474961699
At time: 289.8955717086792 and batch: 500, loss is 4.302271671295166 and perplexity is 73.86740571302265
At time: 290.3532044887543 and batch: 550, loss is 4.36436466217041 and perplexity is 78.59944687293685
At time: 290.80645394325256 and batch: 600, loss is 4.385126295089722 and perplexity is 80.24835754644569
At time: 291.26261734962463 and batch: 650, loss is 4.357236437797546 and perplexity is 78.0411645248958
At time: 291.72040009498596 and batch: 700, loss is 4.331493258476257 and perplexity is 76.05777573206892
At time: 292.1794559955597 and batch: 750, loss is 4.287275619506836 and perplexity is 72.76795061810675
At time: 292.6372084617615 and batch: 800, loss is 4.325523328781128 and perplexity is 75.60506881694947
At time: 293.09398460388184 and batch: 850, loss is 4.293522939682007 and perplexity is 73.22397829637512
At time: 293.5517454147339 and batch: 900, loss is 4.405477771759033 and perplexity is 81.89826217414837
At time: 294.00944471359253 and batch: 950, loss is 4.350310792922974 and perplexity is 77.50254642551779
At time: 294.46819710731506 and batch: 1000, loss is 4.315756320953369 and perplexity is 74.87022795784277
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.883397172137005 and perplexity of 132.0785963305782
Finished 29 epochs...
Completing Train Step...
At time: 295.8887360095978 and batch: 50, loss is 4.460900869369507 and perplexity is 86.56545814980481
At time: 296.35897064208984 and batch: 100, loss is 4.3848171043396 and perplexity is 80.2235493320127
At time: 296.81368470191956 and batch: 150, loss is 4.427612047195435 and perplexity is 83.73123175933975
At time: 297.2682394981384 and batch: 200, loss is 4.432121095657348 and perplexity is 84.10963241344056
At time: 297.7233130931854 and batch: 250, loss is 4.414738578796387 and perplexity is 82.66022894477713
At time: 298.17936539649963 and batch: 300, loss is 4.327726640701294 and perplexity is 75.77183401671044
At time: 298.6362154483795 and batch: 350, loss is 4.389255466461182 and perplexity is 80.5804018291252
At time: 299.09388256073 and batch: 400, loss is 4.310306673049927 and perplexity is 74.46332133298762
At time: 299.5562665462494 and batch: 450, loss is 4.361061105728149 and perplexity is 78.34021758890512
At time: 300.0146381855011 and batch: 500, loss is 4.3017590665817265 and perplexity is 73.82955063585898
At time: 300.4737460613251 and batch: 550, loss is 4.364147491455078 and perplexity is 78.58237922819816
At time: 300.93330788612366 and batch: 600, loss is 4.385085716247558 and perplexity is 80.24510122708018
At time: 301.41323709487915 and batch: 650, loss is 4.356859927177429 and perplexity is 78.0117867285191
At time: 301.87649607658386 and batch: 700, loss is 4.331352167129516 and perplexity is 76.04704539505741
At time: 302.3397138118744 and batch: 750, loss is 4.287213344573974 and perplexity is 72.76341913996772
At time: 302.80098247528076 and batch: 800, loss is 4.325380725860596 and perplexity is 75.59428808202952
At time: 303.2642378807068 and batch: 850, loss is 4.293475894927979 and perplexity is 73.2205335733559
At time: 303.7271611690521 and batch: 900, loss is 4.405542688369751 and perplexity is 81.9035789043226
At time: 304.1909544467926 and batch: 950, loss is 4.350497236251831 and perplexity is 77.51699760538946
At time: 304.65361618995667 and batch: 1000, loss is 4.3156970119476314 and perplexity is 74.86578761074084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.88337260920827 and perplexity of 132.07535213327282
Finished 30 epochs...
Completing Train Step...
At time: 306.0811777114868 and batch: 50, loss is 4.4601539134979244 and perplexity is 86.50082171585082
At time: 306.55555486679077 and batch: 100, loss is 4.383918027877808 and perplexity is 80.15145464130299
At time: 307.01440143585205 and batch: 150, loss is 4.426710233688355 and perplexity is 83.65575584129107
At time: 307.47345662117004 and batch: 200, loss is 4.431549816131592 and perplexity is 84.06159602493294
At time: 307.93280148506165 and batch: 250, loss is 4.414393434524536 and perplexity is 82.63170416311264
At time: 308.39219522476196 and batch: 300, loss is 4.327177219390869 and perplexity is 75.73021479066347
At time: 308.85229325294495 and batch: 350, loss is 4.388656044006348 and perplexity is 80.53211460051965
At time: 309.31078147888184 and batch: 400, loss is 4.30983645439148 and perplexity is 74.42831552076495
At time: 309.76904344558716 and batch: 450, loss is 4.360655155181885 and perplexity is 78.30842178897825
At time: 310.2270131111145 and batch: 500, loss is 4.301247458457947 and perplexity is 73.79178849851048
At time: 310.68882513046265 and batch: 550, loss is 4.363977546691895 and perplexity is 78.56902569908311
At time: 311.15299558639526 and batch: 600, loss is 4.385028305053711 and perplexity is 80.24049439226158
At time: 311.6144497394562 and batch: 650, loss is 4.356499876976013 and perplexity is 77.9837036249616
At time: 312.07572531700134 and batch: 700, loss is 4.331184029579163 and perplexity is 76.03426010600676
At time: 312.53739881515503 and batch: 750, loss is 4.287113175392151 and perplexity is 72.7561308528428
At time: 313.01512145996094 and batch: 800, loss is 4.3251975154876705 and perplexity is 75.58043969294201
At time: 313.47646951675415 and batch: 850, loss is 4.293390617370606 and perplexity is 73.21428977133509
At time: 313.93952465057373 and batch: 900, loss is 4.405555486679077 and perplexity is 81.90462713836813
At time: 314.4015920162201 and batch: 950, loss is 4.350553102493286 and perplexity is 77.52132830966353
At time: 314.86439418792725 and batch: 1000, loss is 4.315570545196533 and perplexity is 74.85632017648375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.883359955578316 and perplexity of 132.07368091121435
Finished 31 epochs...
Completing Train Step...
At time: 316.30508065223694 and batch: 50, loss is 4.459431238174439 and perplexity is 86.43833228903854
At time: 316.761367559433 and batch: 100, loss is 4.383081407546997 and perplexity is 80.08442634733234
At time: 317.2174279689789 and batch: 150, loss is 4.42590669631958 and perplexity is 83.58856231522948
At time: 317.67418670654297 and batch: 200, loss is 4.431023006439209 and perplexity is 84.01732322408401
At time: 318.1297526359558 and batch: 250, loss is 4.414064378738403 and perplexity is 82.60451819583497
At time: 318.58686685562134 and batch: 300, loss is 4.326649827957153 and perplexity is 75.69028585412447
At time: 319.0440003871918 and batch: 350, loss is 4.388162126541138 and perplexity is 80.49234820407545
At time: 319.5007929801941 and batch: 400, loss is 4.309416704177856 and perplexity is 74.39708077527962
At time: 319.9589183330536 and batch: 450, loss is 4.3602476978302 and perplexity is 78.27652094637928
At time: 320.4157552719116 and batch: 500, loss is 4.300758309364319 and perplexity is 73.7557021385739
At time: 320.8742311000824 and batch: 550, loss is 4.363739986419677 and perplexity is 78.55036303679152
At time: 321.33175897598267 and batch: 600, loss is 4.384919738769531 and perplexity is 80.23178345281033
At time: 321.79145908355713 and batch: 650, loss is 4.356148290634155 and perplexity is 77.95629043921281
At time: 322.2533140182495 and batch: 700, loss is 4.331034760475159 and perplexity is 76.02291138715479
At time: 322.7158889770508 and batch: 750, loss is 4.287028980255127 and perplexity is 72.75000539830666
At time: 323.17732858657837 and batch: 800, loss is 4.325000953674317 and perplexity is 75.5655849246498
At time: 323.64109778404236 and batch: 850, loss is 4.293287363052368 and perplexity is 73.20673047003146
At time: 324.10590147972107 and batch: 900, loss is 4.4055577087402344 and perplexity is 81.9048091356609
At time: 324.57033228874207 and batch: 950, loss is 4.350614042282104 and perplexity is 77.52605258698637
At time: 325.0516622066498 and batch: 1000, loss is 4.3154345226287845 and perplexity is 74.84613872007073
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8833093410584985 and perplexity of 132.06699623444683
Finished 32 epochs...
Completing Train Step...
At time: 326.4799768924713 and batch: 50, loss is 4.458794956207275 and perplexity is 86.38335063070518
At time: 326.95412015914917 and batch: 100, loss is 4.3823178768157955 and perplexity is 80.02330276456193
At time: 327.4132263660431 and batch: 150, loss is 4.425195474624633 and perplexity is 83.5291334523149
At time: 327.8722085952759 and batch: 200, loss is 4.430568742752075 and perplexity is 83.9791658724589
At time: 328.33125495910645 and batch: 250, loss is 4.41371633529663 and perplexity is 82.57577323755339
At time: 328.79061579704285 and batch: 300, loss is 4.326208953857422 and perplexity is 75.65692332236857
At time: 329.24938583374023 and batch: 350, loss is 4.387649192810058 and perplexity is 80.4510715505867
At time: 329.70885181427 and batch: 400, loss is 4.309040985107422 and perplexity is 74.36913362371536
At time: 330.1686592102051 and batch: 450, loss is 4.3598450183868405 and perplexity is 78.24500694594238
At time: 330.62627696990967 and batch: 500, loss is 4.3002910327911374 and perplexity is 73.72124587775467
At time: 331.0865263938904 and batch: 550, loss is 4.363482236862183 and perplexity is 78.53011932449377
At time: 331.54561614990234 and batch: 600, loss is 4.3848367691040036 and perplexity is 80.22512692472137
At time: 332.0044846534729 and batch: 650, loss is 4.355782918930053 and perplexity is 77.92781261934118
At time: 332.4623444080353 and batch: 700, loss is 4.330858101844788 and perplexity is 76.00948246995428
At time: 332.9212076663971 and batch: 750, loss is 4.286917538642883 and perplexity is 72.74189847214716
At time: 333.37915992736816 and batch: 800, loss is 4.324793004989624 and perplexity is 75.54987279437175
At time: 333.83961176872253 and batch: 850, loss is 4.293198661804199 and perplexity is 73.20023722964794
At time: 334.2987611293793 and batch: 900, loss is 4.405544929504394 and perplexity is 81.90376246147638
At time: 334.7577850818634 and batch: 950, loss is 4.35067084312439 and perplexity is 77.53045625713729
At time: 335.2173156738281 and batch: 1000, loss is 4.315353164672851 and perplexity is 74.84004963891593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.883291104944741 and perplexity of 132.06458786763955
Finished 33 epochs...
Completing Train Step...
At time: 336.6409456729889 and batch: 50, loss is 4.4582386302948 and perplexity is 86.3353066996239
At time: 337.11452746391296 and batch: 100, loss is 4.381623840332031 and perplexity is 79.96778294151196
At time: 337.5709526538849 and batch: 150, loss is 4.424531288146973 and perplexity is 83.47367295148136
At time: 338.026823759079 and batch: 200, loss is 4.430105562210083 and perplexity is 83.94027736378959
At time: 338.48347544670105 and batch: 250, loss is 4.413420391082764 and perplexity is 82.55133903101753
At time: 338.9408712387085 and batch: 300, loss is 4.325768384933472 and perplexity is 75.62359857453426
At time: 339.3983545303345 and batch: 350, loss is 4.387147006988525 and perplexity is 80.4106803059309
At time: 339.85452008247375 and batch: 400, loss is 4.308685293197632 and perplexity is 74.34268582845958
At time: 340.3118464946747 and batch: 450, loss is 4.35944169998169 and perplexity is 78.21345565756529
At time: 340.76788902282715 and batch: 500, loss is 4.299831438064575 and perplexity is 73.68737176669131
At time: 341.2243208885193 and batch: 550, loss is 4.363241605758667 and perplexity is 78.51122480861675
At time: 341.6808557510376 and batch: 600, loss is 4.384761705398559 and perplexity is 80.21910515543564
At time: 342.1378083229065 and batch: 650, loss is 4.355442457199096 and perplexity is 77.90128569732423
At time: 342.59554839134216 and batch: 700, loss is 4.330686936378479 and perplexity is 75.99647338482822
At time: 343.0530984401703 and batch: 750, loss is 4.2867900037765505 and perplexity is 72.73262193540225
At time: 343.5111629962921 and batch: 800, loss is 4.324556283950805 and perplexity is 75.53199066662266
At time: 343.9703016281128 and batch: 850, loss is 4.293060817718506 and perplexity is 73.19014770528108
At time: 344.4281973838806 and batch: 900, loss is 4.405495700836181 and perplexity is 81.89973054757245
At time: 344.8873164653778 and batch: 950, loss is 4.350679368972778 and perplexity is 77.53111727287066
At time: 345.34577345848083 and batch: 1000, loss is 4.315257110595703 and perplexity is 74.83286129225468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.883275101824505 and perplexity of 132.06247443907174
Finished 34 epochs...
Completing Train Step...
At time: 346.7834177017212 and batch: 50, loss is 4.457681694030762 and perplexity is 86.28723682362711
At time: 347.2414302825928 and batch: 100, loss is 4.3809268760681155 and perplexity is 79.91206767256824
At time: 347.69954228401184 and batch: 150, loss is 4.423866195678711 and perplexity is 83.4181736984166
At time: 348.158433675766 and batch: 200, loss is 4.429671373367309 and perplexity is 83.90383934296239
At time: 348.63260674476624 and batch: 250, loss is 4.413108396530151 and perplexity is 82.52558748030964
At time: 349.0910565853119 and batch: 300, loss is 4.325319137573242 and perplexity is 75.58963250265889
At time: 349.54932045936584 and batch: 350, loss is 4.386675348281861 and perplexity is 80.37276285120755
At time: 350.0087718963623 and batch: 400, loss is 4.308313035964966 and perplexity is 74.31501637636049
At time: 350.46759724617004 and batch: 450, loss is 4.35902660369873 and perplexity is 78.18099628019392
At time: 350.9264795780182 and batch: 500, loss is 4.299414696693421 and perplexity is 73.65666958822283
At time: 351.38742423057556 and batch: 550, loss is 4.363002109527588 and perplexity is 78.49242391763882
At time: 351.84584856033325 and batch: 600, loss is 4.38468035697937 and perplexity is 80.21257972346285
At time: 352.30452847480774 and batch: 650, loss is 4.355103783607483 and perplexity is 77.87490705623249
At time: 352.763596534729 and batch: 700, loss is 4.330538496971131 and perplexity is 75.98519335057995
At time: 353.22304916381836 and batch: 750, loss is 4.286651787757873 and perplexity is 72.72256981666828
At time: 353.6835587024689 and batch: 800, loss is 4.324362344741822 and perplexity is 75.51734347247834
At time: 354.14571022987366 and batch: 850, loss is 4.292920732498169 and perplexity is 73.17989556541673
At time: 354.6077153682709 and batch: 900, loss is 4.40544376373291 and perplexity is 81.89547702326792
At time: 355.073548078537 and batch: 950, loss is 4.350675611495972 and perplexity is 77.53082595204307
At time: 355.5397334098816 and batch: 1000, loss is 4.315127611160278 and perplexity is 74.82317110641644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.883260587366616 and perplexity of 132.06055763775848
Finished 35 epochs...
Completing Train Step...
At time: 356.9819941520691 and batch: 50, loss is 4.457148666381836 and perplexity is 86.24125559637213
At time: 357.4569139480591 and batch: 100, loss is 4.3802745819091795 and perplexity is 79.85995849470075
At time: 357.9152338504791 and batch: 150, loss is 4.42321967124939 and perplexity is 83.36425924165592
At time: 358.37273645401 and batch: 200, loss is 4.4292432308197025 and perplexity is 83.86792422837505
At time: 358.8293936252594 and batch: 250, loss is 4.4128022956848145 and perplexity is 82.50033019405556
At time: 359.2860071659088 and batch: 300, loss is 4.324893236160278 and perplexity is 75.55744562607525
At time: 359.7450873851776 and batch: 350, loss is 4.386198854446411 and perplexity is 80.33447484789409
At time: 360.20515632629395 and batch: 400, loss is 4.307939167022705 and perplexity is 74.2872374929482
At time: 360.6782808303833 and batch: 450, loss is 4.35859356880188 and perplexity is 78.14714850969594
At time: 361.13662695884705 and batch: 500, loss is 4.298956022262574 and perplexity is 73.6228929040642
At time: 361.5949831008911 and batch: 550, loss is 4.362650051116943 and perplexity is 78.46479486343256
At time: 362.053209066391 and batch: 600, loss is 4.384525890350342 and perplexity is 80.2001905135517
At time: 362.51184368133545 and batch: 650, loss is 4.354749732017517 and perplexity is 77.84734020190305
At time: 362.9696490764618 and batch: 700, loss is 4.330398263931275 and perplexity is 75.97453846303345
At time: 363.428106546402 and batch: 750, loss is 4.286387720108032 and perplexity is 72.70336867387832
At time: 363.88586711883545 and batch: 800, loss is 4.324122018814087 and perplexity is 75.49919687748427
At time: 364.3437645435333 and batch: 850, loss is 4.292729091644287 and perplexity is 73.16587265146809
At time: 364.80253195762634 and batch: 900, loss is 4.40533938407898 and perplexity is 81.88692924783248
At time: 365.2615144252777 and batch: 950, loss is 4.350556268692016 and perplexity is 77.52157375798333
At time: 365.71923780441284 and batch: 1000, loss is 4.315007047653198 and perplexity is 74.81415070627347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.883207739853278 and perplexity of 132.05357875008738
Finished 36 epochs...
Completing Train Step...
At time: 367.1404700279236 and batch: 50, loss is 4.456606683731079 and perplexity is 86.1945269962485
At time: 367.610378742218 and batch: 100, loss is 4.379625635147095 and perplexity is 79.8081504453665
At time: 368.06527972221375 and batch: 150, loss is 4.422530956268311 and perplexity is 83.30686479390629
At time: 368.5201139450073 and batch: 200, loss is 4.428775310516357 and perplexity is 83.82868990381357
At time: 368.97641038894653 and batch: 250, loss is 4.412397737503052 and perplexity is 82.46696076087116
At time: 369.4349744319916 and batch: 300, loss is 4.3244437408447265 and perplexity is 75.52349054010901
At time: 369.8963198661804 and batch: 350, loss is 4.385702838897705 and perplexity is 80.29463758004134
At time: 370.35547518730164 and batch: 400, loss is 4.307529678344727 and perplexity is 74.2568239377048
At time: 370.8217182159424 and batch: 450, loss is 4.358097944259644 and perplexity is 78.10842646158007
At time: 371.2799472808838 and batch: 500, loss is 4.298563413619995 and perplexity is 73.5939935934492
At time: 371.73861718177795 and batch: 550, loss is 4.362369222640991 and perplexity is 78.4427628084342
At time: 372.2314052581787 and batch: 600, loss is 4.384358434677124 and perplexity is 80.18676166105722
At time: 372.6903147697449 and batch: 650, loss is 4.3544075441360475 and perplexity is 77.82070634263316
At time: 373.1487486362457 and batch: 700, loss is 4.330210614204407 and perplexity is 75.96028319918193
At time: 373.6072895526886 and batch: 750, loss is 4.286228113174438 and perplexity is 72.69176563812943
At time: 374.06656074523926 and batch: 800, loss is 4.323911771774292 and perplexity is 75.48332506339331
At time: 374.5257496833801 and batch: 850, loss is 4.2925694561004635 and perplexity is 73.15419370980993
At time: 374.9852740764618 and batch: 900, loss is 4.405247707366943 and perplexity is 81.87942246750397
At time: 375.44415068626404 and batch: 950, loss is 4.350516605377197 and perplexity is 77.51849905637492
At time: 375.90376567840576 and batch: 1000, loss is 4.314853458404541 and perplexity is 74.8026609394525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.883182432593369 and perplexity of 132.05023687813502
Finished 37 epochs...
Completing Train Step...
At time: 377.341908454895 and batch: 50, loss is 4.4560435962677 and perplexity is 86.14600560086186
At time: 377.80123591423035 and batch: 100, loss is 4.378987970352173 and perplexity is 79.75727581968243
At time: 378.2608211040497 and batch: 150, loss is 4.421896562576294 and perplexity is 83.25403220448197
At time: 378.72040843963623 and batch: 200, loss is 4.428296194076538 and perplexity is 83.78853582036103
At time: 379.17972564697266 and batch: 250, loss is 4.412074947357178 and perplexity is 82.44034553437491
At time: 379.6384196281433 and batch: 300, loss is 4.3240024375915525 and perplexity is 75.4901691310065
At time: 380.0967936515808 and batch: 350, loss is 4.385256929397583 and perplexity is 80.2588414198526
At time: 380.55591011047363 and batch: 400, loss is 4.307145462036133 and perplexity is 74.22829873519848
At time: 381.0152373313904 and batch: 450, loss is 4.357653064727783 and perplexity is 78.07368534975924
At time: 381.4741241931915 and batch: 500, loss is 4.2981943511962895 and perplexity is 73.56683782719813
At time: 381.9327664375305 and batch: 550, loss is 4.3620703125 and perplexity is 78.41931897511772
At time: 382.3939793109894 and batch: 600, loss is 4.384186162948608 and perplexity is 80.17294893882668
At time: 382.85467195510864 and batch: 650, loss is 4.354152998924255 and perplexity is 77.80089997537029
At time: 383.3145897388458 and batch: 700, loss is 4.329947333335877 and perplexity is 75.94028694227895
At time: 383.79265666007996 and batch: 750, loss is 4.286080679893494 and perplexity is 72.68104924261984
At time: 384.2543046474457 and batch: 800, loss is 4.323654270172119 and perplexity is 75.46389048857657
At time: 384.71761083602905 and batch: 850, loss is 4.292426118850708 and perplexity is 73.14370874033676
At time: 385.18007254600525 and batch: 900, loss is 4.405150899887085 and perplexity is 81.87149631062448
At time: 385.6412971019745 and batch: 950, loss is 4.350688304901123 and perplexity is 77.53181008847457
At time: 386.1036021709442 and batch: 1000, loss is 4.314765577316284 and perplexity is 74.79608748904981
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8832270924637955 and perplexity of 132.05613435629314
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 387.5265939235687 and batch: 50, loss is 4.455803298950196 and perplexity is 86.12530743375883
At time: 387.99978518486023 and batch: 100, loss is 4.378134298324585 and perplexity is 79.68921831784441
At time: 388.45730471611023 and batch: 150, loss is 4.420773754119873 and perplexity is 83.16060633258627
At time: 388.9148976802826 and batch: 200, loss is 4.427450876235962 and perplexity is 83.71773780382536
At time: 389.3722550868988 and batch: 250, loss is 4.410717391967774 and perplexity is 82.32850413161648
At time: 389.8301582336426 and batch: 300, loss is 4.32292989730835 and perplexity is 75.40924628788966
At time: 390.2875635623932 and batch: 350, loss is 4.383512268066406 and perplexity is 80.11893899940063
At time: 390.74566411972046 and batch: 400, loss is 4.3052568531036375 and perplexity is 74.08824280423416
At time: 391.20367550849915 and batch: 450, loss is 4.355467014312744 and perplexity is 77.90319875153983
At time: 391.66189885139465 and batch: 500, loss is 4.295664887428284 and perplexity is 73.38098832526987
At time: 392.1222667694092 and batch: 550, loss is 4.359759464263916 and perplexity is 78.23831304934919
At time: 392.5830316543579 and batch: 600, loss is 4.381200952529907 and perplexity is 79.93397269101824
At time: 393.0440323352814 and batch: 650, loss is 4.350420613288879 and perplexity is 77.51105825090215
At time: 393.50763750076294 and batch: 700, loss is 4.3269091320037845 and perplexity is 75.70991519641107
At time: 393.9760901927948 and batch: 750, loss is 4.282790379524231 and perplexity is 72.44229975361768
At time: 394.4409239292145 and batch: 800, loss is 4.319414930343628 and perplexity is 75.14465057337405
At time: 394.9054865837097 and batch: 850, loss is 4.287987432479858 and perplexity is 72.81976622870106
At time: 395.37049889564514 and batch: 900, loss is 4.400734643936158 and perplexity is 81.51072803669766
At time: 395.85122895240784 and batch: 950, loss is 4.346088142395019 and perplexity is 77.1759702507235
At time: 396.31599020957947 and batch: 1000, loss is 4.310427141189575 and perplexity is 74.47229233112955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.882444428234566 and perplexity of 131.9528191795026
Finished 39 epochs...
Completing Train Step...
At time: 397.7450032234192 and batch: 50, loss is 4.454713897705078 and perplexity is 86.03153350459617
At time: 398.22087264060974 and batch: 100, loss is 4.377546825408936 and perplexity is 79.64241680906886
At time: 398.6775276660919 and batch: 150, loss is 4.420187377929688 and perplexity is 83.11185722712567
At time: 399.135267496109 and batch: 200, loss is 4.426943655014038 and perplexity is 83.6752851579114
At time: 399.59305024147034 and batch: 250, loss is 4.410010099411011 and perplexity is 82.27029438152292
At time: 400.05022716522217 and batch: 300, loss is 4.322383871078491 and perplexity is 75.3680821008283
At time: 400.5070219039917 and batch: 350, loss is 4.383246736526489 and perplexity is 80.09766771837461
At time: 400.9634037017822 and batch: 400, loss is 4.304893579483032 and perplexity is 74.06133338806282
At time: 401.41964316368103 and batch: 450, loss is 4.355212888717651 and perplexity is 77.88340407007162
At time: 401.87472438812256 and batch: 500, loss is 4.2954637241363525 and perplexity is 73.36622824873572
At time: 402.3309874534607 and batch: 550, loss is 4.359744482040405 and perplexity is 78.2371408742369
At time: 402.78812408447266 and batch: 600, loss is 4.381029052734375 and perplexity is 79.92023323839496
At time: 403.2441990375519 and batch: 650, loss is 4.3502587842941285 and perplexity is 77.49851572916258
At time: 403.70403814315796 and batch: 700, loss is 4.326685972213745 and perplexity is 75.69302167267968
At time: 404.1639287471771 and batch: 750, loss is 4.2827294111251835 and perplexity is 72.43788319721493
At time: 404.62412571907043 and batch: 800, loss is 4.319369087219238 and perplexity is 75.14120578677107
At time: 405.0867967605591 and batch: 850, loss is 4.288070116043091 and perplexity is 72.82578747537224
At time: 405.54633378982544 and batch: 900, loss is 4.400860223770142 and perplexity is 81.52096478314344
At time: 406.0063259601593 and batch: 950, loss is 4.346293058395386 and perplexity is 77.19178646231373
At time: 406.46581387519836 and batch: 1000, loss is 4.310731582641601 and perplexity is 74.49496823551002
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.882381532250381 and perplexity of 131.94452013806534
Finished 40 epochs...
Completing Train Step...
At time: 407.89944219589233 and batch: 50, loss is 4.454307537078858 and perplexity is 85.99658077895326
At time: 408.3540189266205 and batch: 100, loss is 4.377243099212646 and perplexity is 79.6182309938669
At time: 408.80871534347534 and batch: 150, loss is 4.419784393310547 and perplexity is 83.0783711746301
At time: 409.26225090026855 and batch: 200, loss is 4.426689577102661 and perplexity is 83.65402781684939
At time: 409.71647810935974 and batch: 250, loss is 4.409565114974976 and perplexity is 82.23369352498484
At time: 410.17069578170776 and batch: 300, loss is 4.322007999420166 and perplexity is 75.3397586981399
At time: 410.62688159942627 and batch: 350, loss is 4.383069791793823 and perplexity is 80.08349611180554
At time: 411.0831322669983 and batch: 400, loss is 4.304733858108521 and perplexity is 74.04950515473193
At time: 411.5391752719879 and batch: 450, loss is 4.355084352493286 and perplexity is 77.87339387472186
At time: 411.9981098175049 and batch: 500, loss is 4.295367889404297 and perplexity is 73.35919755280736
At time: 412.45769906044006 and batch: 550, loss is 4.359736347198487 and perplexity is 78.23650443005243
At time: 412.91731119155884 and batch: 600, loss is 4.380903968811035 and perplexity is 79.91023712725669
At time: 413.37699484825134 and batch: 650, loss is 4.3501173114776615 and perplexity is 77.48755257138305
At time: 413.83699226379395 and batch: 700, loss is 4.32655553817749 and perplexity is 75.68314937020267
At time: 414.2975523471832 and batch: 750, loss is 4.28268292427063 and perplexity is 72.43451586614351
At time: 414.7564251422882 and batch: 800, loss is 4.319304313659668 and perplexity is 75.1363387810302
At time: 415.21562027931213 and batch: 850, loss is 4.288099317550659 and perplexity is 72.82791412920697
At time: 415.6743366718292 and batch: 900, loss is 4.400931568145752 and perplexity is 81.5267810529516
At time: 416.1333329677582 and batch: 950, loss is 4.346421480178833 and perplexity is 77.20170020575539
At time: 416.597660779953 and batch: 1000, loss is 4.310924158096314 and perplexity is 74.50931551931478
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8823435713605186 and perplexity of 131.93951150173515
Finished 41 epochs...
Completing Train Step...
At time: 418.0256268978119 and batch: 50, loss is 4.454020347595215 and perplexity is 85.97188701138919
At time: 418.50275015830994 and batch: 100, loss is 4.377003288269043 and perplexity is 79.59913995997513
At time: 418.963520526886 and batch: 150, loss is 4.419467639923096 and perplexity is 83.05205998643564
At time: 419.440025806427 and batch: 200, loss is 4.426493692398071 and perplexity is 83.63764287715463
At time: 419.9012596607208 and batch: 250, loss is 4.40921838760376 and perplexity is 82.20518579509425
At time: 420.36264967918396 and batch: 300, loss is 4.321700267791748 and perplexity is 75.31657783843585
At time: 420.824809551239 and batch: 350, loss is 4.382914152145386 and perplexity is 80.07103291453403
At time: 421.28709626197815 and batch: 400, loss is 4.304606981277466 and perplexity is 74.040110584166
At time: 421.74812626838684 and batch: 450, loss is 4.354982938766479 and perplexity is 77.86549684407038
At time: 422.20971608161926 and batch: 500, loss is 4.295289115905762 and perplexity is 73.35341901976705
At time: 422.6702916622162 and batch: 550, loss is 4.359721088409424 and perplexity is 78.23531064484223
At time: 423.132443189621 and batch: 600, loss is 4.380793943405151 and perplexity is 79.90144545464503
At time: 423.5945327281952 and batch: 650, loss is 4.3499799251556395 and perplexity is 77.47690757278812
At time: 424.0557904243469 and batch: 700, loss is 4.326439390182495 and perplexity is 75.67435943462498
At time: 424.5190849304199 and batch: 750, loss is 4.282626776695252 and perplexity is 72.4304489578785
At time: 424.98079776763916 and batch: 800, loss is 4.3192235946655275 and perplexity is 75.13027409611134
At time: 425.4429359436035 and batch: 850, loss is 4.28809850692749 and perplexity is 72.82785509323634
At time: 425.90549397468567 and batch: 900, loss is 4.400965423583984 and perplexity is 81.52954122457498
At time: 426.3671295642853 and batch: 950, loss is 4.3465029239654545 and perplexity is 77.20798806060364
At time: 426.829715013504 and batch: 1000, loss is 4.31107087135315 and perplexity is 74.52024782559674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.882319752762958 and perplexity of 131.93636892503432
Finished 42 epochs...
Completing Train Step...
At time: 428.2686161994934 and batch: 50, loss is 4.45377103805542 and perplexity is 85.95045607138297
At time: 428.74220514297485 and batch: 100, loss is 4.37679536819458 and perplexity is 79.58259142131382
At time: 429.19966077804565 and batch: 150, loss is 4.419194355010986 and perplexity is 83.02936621259593
At time: 429.6580882072449 and batch: 200, loss is 4.42632640838623 and perplexity is 83.62365280690338
At time: 430.1168169975281 and batch: 250, loss is 4.408906307220459 and perplexity is 82.17953517193833
At time: 430.57676672935486 and batch: 300, loss is 4.321431131362915 and perplexity is 75.29631013115602
At time: 431.03853058815 and batch: 350, loss is 4.382768745422363 and perplexity is 80.05939089446342
At time: 431.5188903808594 and batch: 400, loss is 4.304482021331787 and perplexity is 74.03085911401298
At time: 431.98132061958313 and batch: 450, loss is 4.354904870986939 and perplexity is 77.85941829490145
At time: 432.4437367916107 and batch: 500, loss is 4.2952137422561645 and perplexity is 73.3478903132271
At time: 432.9047341346741 and batch: 550, loss is 4.359700326919556 and perplexity is 78.23368638009404
At time: 433.36962938308716 and batch: 600, loss is 4.3806853771209715 and perplexity is 79.8927713224791
At time: 433.83131980895996 and batch: 650, loss is 4.3498321151733395 and perplexity is 77.46545655875907
At time: 434.2925341129303 and batch: 700, loss is 4.326328883171081 and perplexity is 75.66599734936626
At time: 434.75421500205994 and batch: 750, loss is 4.282564716339111 and perplexity is 72.42595403790038
At time: 435.21631145477295 and batch: 800, loss is 4.319124450683594 and perplexity is 75.1228257508091
At time: 435.67757773399353 and batch: 850, loss is 4.288084411621094 and perplexity is 72.82682856953924
At time: 436.1386067867279 and batch: 900, loss is 4.4009859657287596 and perplexity is 81.53121603341631
At time: 436.6001408100128 and batch: 950, loss is 4.346546354293824 and perplexity is 77.21134130169348
At time: 437.06228137016296 and batch: 1000, loss is 4.311183395385743 and perplexity is 74.52863361618455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8823048661394814 and perplexity of 131.93440485260658
Finished 43 epochs...
Completing Train Step...
At time: 438.50792169570923 and batch: 50, loss is 4.453559064865113 and perplexity is 85.93223880985491
At time: 438.9678132534027 and batch: 100, loss is 4.3765996551513675 and perplexity is 79.56701759421027
At time: 439.42723274230957 and batch: 150, loss is 4.418948030471801 and perplexity is 83.00891656095374
At time: 439.88719415664673 and batch: 200, loss is 4.426182088851928 and perplexity is 83.61158515109385
At time: 440.34653759002686 and batch: 250, loss is 4.408640251159668 and perplexity is 82.15767371684815
At time: 440.8076674938202 and batch: 300, loss is 4.321184530258178 and perplexity is 75.27774426717046
At time: 441.26770186424255 and batch: 350, loss is 4.38263726234436 and perplexity is 80.04886513132061
At time: 441.7264676094055 and batch: 400, loss is 4.304378595352173 and perplexity is 74.02320279582534
At time: 442.18629908561707 and batch: 450, loss is 4.35482008934021 and perplexity is 77.85281752502108
At time: 442.6471230983734 and batch: 500, loss is 4.295141673088073 and perplexity is 73.3426043822696
At time: 443.12287640571594 and batch: 550, loss is 4.359682140350341 and perplexity is 78.23226359067968
At time: 443.58292055130005 and batch: 600, loss is 4.380580005645752 and perplexity is 79.8843533468205
At time: 444.04328894615173 and batch: 650, loss is 4.349688920974732 and perplexity is 77.45436474894771
At time: 444.502233505249 and batch: 700, loss is 4.326221947669983 and perplexity is 75.65790640063612
At time: 444.96179485321045 and batch: 750, loss is 4.282498350143433 and perplexity is 72.42114756235802
At time: 445.4214971065521 and batch: 800, loss is 4.319022169113159 and perplexity is 75.11514246315188
At time: 445.88047981262207 and batch: 850, loss is 4.288061742782593 and perplexity is 72.82517768863568
At time: 446.34034061431885 and batch: 900, loss is 4.400988960266114 and perplexity is 81.5314601820538
At time: 446.8007261753082 and batch: 950, loss is 4.346566686630249 and perplexity is 77.21291120462051
At time: 447.2620623111725 and batch: 1000, loss is 4.31127815246582 and perplexity is 74.53569606649148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.882292212509528 and perplexity of 131.93273541403167
Finished 44 epochs...
Completing Train Step...
At time: 448.6889634132385 and batch: 50, loss is 4.45336464881897 and perplexity is 85.9155338276602
At time: 449.1631326675415 and batch: 100, loss is 4.376421270370483 and perplexity is 79.55282531509202
At time: 449.6207389831543 and batch: 150, loss is 4.41872618675232 and perplexity is 82.99050359662947
At time: 450.07817339897156 and batch: 200, loss is 4.426043338775635 and perplexity is 83.59998484206562
At time: 450.5347595214844 and batch: 250, loss is 4.408374853134156 and perplexity is 82.13587212564057
At time: 450.99235582351685 and batch: 300, loss is 4.320953788757325 and perplexity is 75.26037657127854
At time: 451.44889187812805 and batch: 350, loss is 4.382510948181152 and perplexity is 80.03875446447944
At time: 451.90584540367126 and batch: 400, loss is 4.304259853363037 and perplexity is 74.01441365531353
At time: 452.3633396625519 and batch: 450, loss is 4.354741497039795 and perplexity is 77.84669913343038
At time: 452.82098722457886 and batch: 500, loss is 4.295067272186279 and perplexity is 73.33714782935238
At time: 453.2782332897186 and batch: 550, loss is 4.359660539627075 and perplexity is 78.2305737354545
At time: 453.73606753349304 and batch: 600, loss is 4.380476493835449 and perplexity is 79.87608480074417
At time: 454.19598937034607 and batch: 650, loss is 4.349547433853149 and perplexity is 77.44340672905302
At time: 454.65504789352417 and batch: 700, loss is 4.3261162328720095 and perplexity is 75.64990866309296
At time: 455.12863516807556 and batch: 750, loss is 4.282429246902466 and perplexity is 72.41614319925776
At time: 455.58828806877136 and batch: 800, loss is 4.318913831710815 and perplexity is 75.10700512453774
At time: 456.0480568408966 and batch: 850, loss is 4.288030700683594 and perplexity is 72.8229170773475
At time: 456.50658226013184 and batch: 900, loss is 4.400983924865723 and perplexity is 81.53104963954097
At time: 456.964812040329 and batch: 950, loss is 4.346579723358154 and perplexity is 77.21391781489609
At time: 457.42317748069763 and batch: 1000, loss is 4.311354665756226 and perplexity is 74.54139925603234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.882282908369855 and perplexity of 131.93150789914444
Finished 45 epochs...
Completing Train Step...
At time: 458.84016251564026 and batch: 50, loss is 4.453187561035156 and perplexity is 85.90032058325909
At time: 459.31082010269165 and batch: 100, loss is 4.3762477588653566 and perplexity is 79.53902318208361
At time: 459.7667672634125 and batch: 150, loss is 4.418516054153442 and perplexity is 82.97306641855047
At time: 460.22363448143005 and batch: 200, loss is 4.425915365219116 and perplexity is 83.58928693921939
At time: 460.68133783340454 and batch: 250, loss is 4.408130950927735 and perplexity is 82.11584144806535
At time: 461.13896226882935 and batch: 300, loss is 4.320736694335937 and perplexity is 75.24403973675538
At time: 461.59604144096375 and batch: 350, loss is 4.382392129898071 and perplexity is 80.0292449620566
At time: 462.05351066589355 and batch: 400, loss is 4.304155759811401 and perplexity is 74.00670963310036
At time: 462.51110458374023 and batch: 450, loss is 4.354662952423095 and perplexity is 77.84058493440746
At time: 462.9718613624573 and batch: 500, loss is 4.294999227523804 and perplexity is 73.33215779765587
At time: 463.4316248893738 and batch: 550, loss is 4.359640035629273 and perplexity is 78.22896971238701
At time: 463.8928759098053 and batch: 600, loss is 4.380375986099243 and perplexity is 79.86805703971667
At time: 464.3517048358917 and batch: 650, loss is 4.349410572052002 and perplexity is 77.43280841019015
At time: 464.80815505981445 and batch: 700, loss is 4.326013493537903 and perplexity is 75.6421368410944
At time: 465.26625633239746 and batch: 750, loss is 4.282370948791504 and perplexity is 72.41192159796299
At time: 465.7252323627472 and batch: 800, loss is 4.318800477981568 and perplexity is 75.09849194792353
At time: 466.20561242103577 and batch: 850, loss is 4.287993879318237 and perplexity is 72.82023568747796
At time: 466.70959734916687 and batch: 900, loss is 4.400962877273559 and perplexity is 81.52933362531854
At time: 467.18228483200073 and batch: 950, loss is 4.346588544845581 and perplexity is 77.21459895950562
At time: 467.6794512271881 and batch: 1000, loss is 4.311421031951904 and perplexity is 74.5463464492829
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8822743485613564 and perplexity of 131.93037859553522
Finished 46 epochs...
Completing Train Step...
At time: 469.2075843811035 and batch: 50, loss is 4.453015556335449 and perplexity is 85.8855465950466
At time: 469.68345046043396 and batch: 100, loss is 4.376084566116333 and perplexity is 79.52604404931486
At time: 470.14085030555725 and batch: 150, loss is 4.418313751220703 and perplexity is 82.95628242165878
At time: 470.59672951698303 and batch: 200, loss is 4.425788049697876 and perplexity is 83.57864540301337
At time: 471.0526113510132 and batch: 250, loss is 4.407889385223388 and perplexity is 82.09600747268853
At time: 471.52894139289856 and batch: 300, loss is 4.320535497665405 and perplexity is 75.22890240932398
At time: 472.00004506111145 and batch: 350, loss is 4.382274618148804 and perplexity is 80.01984113802928
At time: 472.4677481651306 and batch: 400, loss is 4.304053688049317 and perplexity is 73.99915602335373
At time: 472.9308030605316 and batch: 450, loss is 4.3545903396606445 and perplexity is 77.83493291971126
At time: 473.38662672042847 and batch: 500, loss is 4.29493136882782 and perplexity is 73.32718174189026
At time: 473.8428490161896 and batch: 550, loss is 4.359616031646729 and perplexity is 78.22709192810086
At time: 474.3015239238739 and batch: 600, loss is 4.380274467468261 and perplexity is 79.8599493554543
At time: 474.75898027420044 and batch: 650, loss is 4.349272351264954 and perplexity is 77.42210632611064
At time: 475.2150433063507 and batch: 700, loss is 4.325910549163819 and perplexity is 75.63435030945962
At time: 475.6716363430023 and batch: 750, loss is 4.282314891815186 and perplexity is 72.40786251835972
At time: 476.12961530685425 and batch: 800, loss is 4.318682584762573 and perplexity is 75.08963886683554
At time: 476.58580255508423 and batch: 850, loss is 4.2879533767700195 and perplexity is 72.81728634209922
At time: 477.04288697242737 and batch: 900, loss is 4.400940198898315 and perplexity is 81.52748469346263
At time: 477.4999556541443 and batch: 950, loss is 4.346591100692749 and perplexity is 77.21479630847195
At time: 477.95787358283997 and batch: 1000, loss is 4.31147406578064 and perplexity is 74.55030003228924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.882268766077553 and perplexity of 131.9296420983893
Finished 47 epochs...
Completing Train Step...
At time: 479.37177896499634 and batch: 50, loss is 4.4528540515899655 and perplexity is 85.8716767917527
At time: 479.8439943790436 and batch: 100, loss is 4.375921506881713 and perplexity is 79.51307765061411
At time: 480.29776668548584 and batch: 150, loss is 4.418112497329712 and perplexity is 82.93958882692111
At time: 480.7568213939667 and batch: 200, loss is 4.425667915344238 and perplexity is 83.56860533956032
At time: 481.21318554878235 and batch: 250, loss is 4.4076675701141355 and perplexity is 82.07779935731396
At time: 481.6715581417084 and batch: 300, loss is 4.320341215133667 and perplexity is 75.21428816739595
At time: 482.1273443698883 and batch: 350, loss is 4.38216082572937 and perplexity is 80.01073600476087
At time: 482.5854244232178 and batch: 400, loss is 4.303960704803467 and perplexity is 73.9922756615207
At time: 483.04599595069885 and batch: 450, loss is 4.354512948989868 and perplexity is 77.82890945512574
At time: 483.50649857521057 and batch: 500, loss is 4.294866271018982 and perplexity is 73.32240845839745
At time: 483.9695625305176 and batch: 550, loss is 4.359594554901123 and perplexity is 78.22541188278905
At time: 484.4316689968109 and batch: 600, loss is 4.380173053741455 and perplexity is 79.85185087102327
At time: 484.8941285610199 and batch: 650, loss is 4.349136953353882 and perplexity is 77.41162424428526
At time: 485.357946395874 and batch: 700, loss is 4.3258104372024535 and perplexity is 75.62677878530994
At time: 485.8209924697876 and batch: 750, loss is 4.282261271476745 and perplexity is 72.40398008835527
At time: 486.2826066017151 and batch: 800, loss is 4.3185648727417 and perplexity is 75.08080043390295
At time: 486.7467153072357 and batch: 850, loss is 4.287910060882568 and perplexity is 72.81413226503085
At time: 487.2102916240692 and batch: 900, loss is 4.400906343460083 and perplexity is 81.52472459146284
At time: 487.6731810569763 and batch: 950, loss is 4.346593141555786 and perplexity is 77.21495389345642
At time: 488.1358206272125 and batch: 1000, loss is 4.311521978378296 and perplexity is 74.55387201639063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.882262439262576 and perplexity of 131.92880740659427
Finished 48 epochs...
Completing Train Step...
At time: 489.5680797100067 and batch: 50, loss is 4.4526960563659665 and perplexity is 85.85811054867185
At time: 490.041380405426 and batch: 100, loss is 4.375767755508423 and perplexity is 79.50085334550668
At time: 490.50093722343445 and batch: 150, loss is 4.417917280197144 and perplexity is 82.92339917851368
At time: 490.97531294822693 and batch: 200, loss is 4.425544376373291 and perplexity is 83.55828199773381
At time: 491.4334189891815 and batch: 250, loss is 4.407440176010132 and perplexity is 82.05913747155233
At time: 491.89234018325806 and batch: 300, loss is 4.32015703201294 and perplexity is 75.20043624076268
At time: 492.3526360988617 and batch: 350, loss is 4.382045373916626 and perplexity is 80.00149915346603
At time: 492.81024193763733 and batch: 400, loss is 4.303859453201294 and perplexity is 73.98478420432897
At time: 493.2668538093567 and batch: 450, loss is 4.354440937042236 and perplexity is 77.82330504556847
At time: 493.72555327415466 and batch: 500, loss is 4.2948001194000245 and perplexity is 73.31755822279928
At time: 494.1843867301941 and batch: 550, loss is 4.35956883430481 and perplexity is 78.22339990442332
At time: 494.6446862220764 and batch: 600, loss is 4.380071640014648 and perplexity is 79.8437532078481
At time: 495.10258984565735 and batch: 650, loss is 4.348998084068298 and perplexity is 77.40087489372519
At time: 495.56177592277527 and batch: 700, loss is 4.325711388587951 and perplexity is 75.61928842861283
At time: 496.01997780799866 and batch: 750, loss is 4.282205848693848 and perplexity is 72.39996736948498
At time: 496.4777615070343 and batch: 800, loss is 4.318448600769043 and perplexity is 75.07207114862328
At time: 496.9360239505768 and batch: 850, loss is 4.287863121032715 and perplexity is 72.81071446081137
At time: 497.39587020874023 and batch: 900, loss is 4.400874462127685 and perplexity is 81.52212551605076
At time: 497.8547885417938 and batch: 950, loss is 4.3465908432006835 and perplexity is 77.21477642627708
At time: 498.3136684894562 and batch: 1000, loss is 4.3115587902069095 and perplexity is 74.5566165312648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8822572289443595 and perplexity of 131.92812001731647
Finished 49 epochs...
Completing Train Step...
At time: 499.76109504699707 and batch: 50, loss is 4.452543439865113 and perplexity is 85.84500818411394
At time: 500.21465396881104 and batch: 100, loss is 4.37561333656311 and perplexity is 79.48857785539015
At time: 500.66873836517334 and batch: 150, loss is 4.41772494316101 and perplexity is 82.9074514714061
At time: 501.12424516677856 and batch: 200, loss is 4.42542797088623 and perplexity is 83.5485559213152
At time: 501.57882475852966 and batch: 250, loss is 4.407234468460083 and perplexity is 82.04225902349555
At time: 502.03360056877136 and batch: 300, loss is 4.319977655410766 and perplexity is 75.18694825177883
At time: 502.5038685798645 and batch: 350, loss is 4.381935367584228 and perplexity is 79.99269896600487
At time: 502.96063923835754 and batch: 400, loss is 4.3037740516662595 and perplexity is 73.97846605998215
At time: 503.4201910495758 and batch: 450, loss is 4.3543587493896485 and perplexity is 77.8169091936438
At time: 503.87643551826477 and batch: 500, loss is 4.294733581542968 and perplexity is 73.31267999188586
At time: 504.3346059322357 and batch: 550, loss is 4.359544839859009 and perplexity is 78.22152299981168
At time: 504.7943232059479 and batch: 600, loss is 4.379975090026855 and perplexity is 79.83604466658628
At time: 505.2523081302643 and batch: 650, loss is 4.348862719535828 and perplexity is 77.39039826958007
At time: 505.71059584617615 and batch: 700, loss is 4.325615019798279 and perplexity is 75.61200144043582
At time: 506.16736578941345 and batch: 750, loss is 4.282150864601135 and perplexity is 72.39598663240636
At time: 506.62777185440063 and batch: 800, loss is 4.318335790634155 and perplexity is 75.06360273582108
At time: 507.0873849391937 and batch: 850, loss is 4.2878132247924805 and perplexity is 72.80708157054552
At time: 507.5481653213501 and batch: 900, loss is 4.400831441879273 and perplexity is 81.51861848939707
At time: 508.01043462753296 and batch: 950, loss is 4.346583843231201 and perplexity is 77.21423592709026
At time: 508.47249007225037 and batch: 1000, loss is 4.311591920852661 and perplexity is 74.5590866810341
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.882255368116425 and perplexity of 131.9278745220138
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f75e40c6898>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.68016409211674, 'anneal': 4.6070830650539705, 'tune_wordvecs': True, 'lr': 28.69692105694313, 'wordvec_dim': 200, 'seq_len': 20, 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7276113033294678 and batch: 50, loss is 6.674625968933105 and perplexity is 792.0511467294305
At time: 1.1997809410095215 and batch: 100, loss is 6.2452787399292 and perplexity is 515.5729156577565
At time: 1.6536870002746582 and batch: 150, loss is 6.18561882019043 and perplexity is 485.7134399123176
At time: 2.1080493927001953 and batch: 200, loss is 6.119071655273437 and perplexity is 454.44261920508654
At time: 2.5614609718322754 and batch: 250, loss is 6.081732568740844 and perplexity is 437.78703415488286
At time: 3.0159950256347656 and batch: 300, loss is 5.955575065612793 and perplexity is 385.8987639909914
At time: 3.469754457473755 and batch: 350, loss is 5.991665782928467 and perplexity is 400.08050242790847
At time: 3.923846483230591 and batch: 400, loss is 5.920959854125977 and perplexity is 372.76934640638046
At time: 4.376506805419922 and batch: 450, loss is 5.887330179214477 and perplexity is 360.4416839246972
At time: 4.831954002380371 and batch: 500, loss is 5.880417366027832 and perplexity is 357.9586102993054
At time: 5.285698652267456 and batch: 550, loss is 5.939343004226685 and perplexity is 379.6853958755571
At time: 5.754002571105957 and batch: 600, loss is 5.9401470565795895 and perplexity is 379.99080557771526
At time: 6.207913875579834 and batch: 650, loss is 5.900556564331055 and perplexity is 365.2406912476381
At time: 6.663021802902222 and batch: 700, loss is 5.8895489120483395 and perplexity is 361.24229556674624
At time: 7.116723537445068 and batch: 750, loss is 5.79767578125 and perplexity is 329.53276292653857
At time: 7.570555686950684 and batch: 800, loss is 5.855077590942383 and perplexity is 349.00197839585195
At time: 8.023913145065308 and batch: 850, loss is 5.817880687713623 and perplexity is 336.2586608347214
At time: 8.478761434555054 and batch: 900, loss is 5.889921789169311 and perplexity is 361.37701967009735
At time: 8.931825637817383 and batch: 950, loss is 5.879108724594116 and perplexity is 357.4904772063016
At time: 9.385416746139526 and batch: 1000, loss is 5.821032075881958 and perplexity is 337.3200138893258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.653571524271151 and perplexity of 285.3086350352227
Finished 1 epochs...
Completing Train Step...
At time: 10.810291528701782 and batch: 50, loss is 5.660664539337159 and perplexity is 287.33952754291107
At time: 11.25841474533081 and batch: 100, loss is 5.618971309661865 and perplexity is 275.6057244154122
At time: 11.706483364105225 and batch: 150, loss is 5.636053018569946 and perplexity is 280.35397987692323
At time: 12.156401872634888 and batch: 200, loss is 5.589018611907959 and perplexity is 267.4729960535896
At time: 12.608548164367676 and batch: 250, loss is 5.576206455230713 and perplexity is 264.06794969340325
At time: 13.05707311630249 and batch: 300, loss is 5.43455774307251 and perplexity is 229.19146434569976
At time: 13.50566816329956 and batch: 350, loss is 5.458125715255737 and perplexity is 234.65719753342708
At time: 13.955322504043579 and batch: 400, loss is 5.394175624847412 and perplexity is 220.12061031785262
At time: 14.404685497283936 and batch: 450, loss is 5.416334266662598 and perplexity is 225.05262565648545
At time: 14.85439658164978 and batch: 500, loss is 5.40815842628479 and perplexity is 223.22013260549227
At time: 15.303618907928467 and batch: 550, loss is 5.446827850341797 and perplexity is 232.02099200236293
At time: 15.754799127578735 and batch: 600, loss is 5.429897270202637 and perplexity is 228.12580890208818
At time: 16.203154802322388 and batch: 650, loss is 5.425081853866577 and perplexity is 227.02992883224576
At time: 16.651628971099854 and batch: 700, loss is 5.4018388366699215 and perplexity is 221.81392099380906
At time: 17.1005916595459 and batch: 750, loss is 5.324154119491578 and perplexity is 205.23468300194497
At time: 17.56644916534424 and batch: 800, loss is 5.3666489315032955 and perplexity is 214.144052589274
At time: 18.016024589538574 and batch: 850, loss is 5.372030773162842 and perplexity is 215.29964880131695
At time: 18.46538496017456 and batch: 900, loss is 5.444438314437866 and perplexity is 231.46723139043206
At time: 18.91475009918213 and batch: 950, loss is 5.368968811035156 and perplexity is 214.64141768423573
At time: 19.364418745040894 and batch: 1000, loss is 5.378681526184082 and perplexity is 216.7363257873077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.572200496022294 and perplexity of 263.0122202750095
Finished 2 epochs...
Completing Train Step...
At time: 20.766267776489258 and batch: 50, loss is 5.420384464263916 and perplexity is 225.96598164864898
At time: 21.228679418563843 and batch: 100, loss is 5.411107397079467 and perplexity is 223.87937382112207
At time: 21.675522565841675 and batch: 150, loss is 5.397768907546997 and perplexity is 220.91298866585674
At time: 22.12246870994568 and batch: 200, loss is 5.406050243377686 and perplexity is 222.75003943271582
At time: 22.57104468345642 and batch: 250, loss is 5.352085962295532 and perplexity is 211.04807735740303
At time: 23.018996477127075 and batch: 300, loss is 5.30316195487976 and perplexity is 200.9712684646387
At time: 23.46614122390747 and batch: 350, loss is 5.330831565856934 and perplexity is 206.60971232309265
At time: 23.915913105010986 and batch: 400, loss is 5.30652361869812 and perplexity is 201.6480031463105
At time: 24.371148347854614 and batch: 450, loss is 5.299287729263305 and perplexity is 200.19416673389762
At time: 24.828566551208496 and batch: 500, loss is 5.289707498550415 and perplexity is 198.28541815395002
At time: 25.29179048538208 and batch: 550, loss is 5.328150358200073 and perplexity is 206.05649076279192
At time: 25.756575107574463 and batch: 600, loss is 5.335501432418823 and perplexity is 207.57680845747444
At time: 26.22196125984192 and batch: 650, loss is 5.362640867233276 and perplexity is 213.2874672347539
At time: 26.68657088279724 and batch: 700, loss is 5.39802656173706 and perplexity is 220.9699151563854
At time: 27.150734186172485 and batch: 750, loss is 5.282382307052612 and perplexity is 196.83824637104513
At time: 27.61447548866272 and batch: 800, loss is 5.327668542861939 and perplexity is 205.95723349877653
At time: 28.079732656478882 and batch: 850, loss is 5.315372638702392 and perplexity is 203.44030873844443
At time: 28.54346227645874 and batch: 900, loss is 5.4418876838684085 and perplexity is 230.87759628454944
At time: 29.023133277893066 and batch: 950, loss is 5.378006677627564 and perplexity is 216.5901109326959
At time: 29.487337827682495 and batch: 1000, loss is 5.324617137908936 and perplexity is 205.32973244317836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.646395427424733 and perplexity of 283.2685612772124
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 30.922418117523193 and batch: 50, loss is 5.347520942687988 and perplexity is 210.08683446177537
At time: 31.410197257995605 and batch: 100, loss is 5.271895141601562 and perplexity is 194.78475757294598
At time: 31.873029232025146 and batch: 150, loss is 5.249374799728393 and perplexity is 190.44716361190143
At time: 32.33288264274597 and batch: 200, loss is 5.2450540733337405 and perplexity is 189.62606866599532
At time: 32.78787302970886 and batch: 250, loss is 5.223113145828247 and perplexity is 185.51080824619203
At time: 33.244136810302734 and batch: 300, loss is 5.109654903411865 and perplexity is 165.61319246335137
At time: 33.699451208114624 and batch: 350, loss is 5.16987232208252 and perplexity is 175.89237847775152
At time: 34.15359902381897 and batch: 400, loss is 5.099717302322388 and perplexity is 163.97554524116245
At time: 34.60848808288574 and batch: 450, loss is 5.12685712814331 and perplexity is 168.4867527728919
At time: 35.06837725639343 and batch: 500, loss is 5.0879037475585935 and perplexity is 162.04980845231978
At time: 35.52378273010254 and batch: 550, loss is 5.1379560947418215 and perplexity is 170.36719780869623
At time: 35.979936838150024 and batch: 600, loss is 5.151602325439453 and perplexity is 172.70800315296654
At time: 36.434881925582886 and batch: 650, loss is 5.107276172637939 and perplexity is 165.21971144411347
At time: 36.893303632736206 and batch: 700, loss is 5.086489133834839 and perplexity is 161.82073263445005
At time: 37.35039734840393 and batch: 750, loss is 5.023737363815307 and perplexity is 151.9782416887575
At time: 37.806695222854614 and batch: 800, loss is 5.047347173690796 and perplexity is 155.6091125056966
At time: 38.26347517967224 and batch: 850, loss is 5.030069770812989 and perplexity is 152.94368332848833
At time: 38.722084045410156 and batch: 900, loss is 5.106639060974121 and perplexity is 165.11448156401127
At time: 39.179397106170654 and batch: 950, loss is 5.066662693023682 and perplexity is 158.64399917636516
At time: 39.63621473312378 and batch: 1000, loss is 5.018814821243286 and perplexity is 151.23196063111592
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.236568171803544 and perplexity of 188.02372878439706
Finished 4 epochs...
Completing Train Step...
At time: 41.071245193481445 and batch: 50, loss is 5.142766590118408 and perplexity is 171.18872281296171
At time: 41.525938272476196 and batch: 100, loss is 5.113275918960571 and perplexity is 166.21396745969923
At time: 41.98085880279541 and batch: 150, loss is 5.121408500671387 and perplexity is 167.5712276656765
At time: 42.431787729263306 and batch: 200, loss is 5.119472589492798 and perplexity is 167.2471384580058
At time: 42.88296341896057 and batch: 250, loss is 5.090155534744262 and perplexity is 162.4151212834475
At time: 43.33397030830383 and batch: 300, loss is 4.9897449016571045 and perplexity is 146.8989450380995
At time: 43.785356521606445 and batch: 350, loss is 5.051662397384644 and perplexity is 156.28205153193352
At time: 44.25009870529175 and batch: 400, loss is 4.983390130996704 and perplexity is 145.96839577723546
At time: 44.730210065841675 and batch: 450, loss is 5.022084894180298 and perplexity is 151.72730964507682
At time: 45.19914412498474 and batch: 500, loss is 4.979892702102661 and perplexity is 145.45877339572345
At time: 45.6750214099884 and batch: 550, loss is 5.039475574493408 and perplexity is 154.3890282476298
At time: 46.16783905029297 and batch: 600, loss is 5.062323160171509 and perplexity is 157.95704992758164
At time: 46.65213060379028 and batch: 650, loss is 5.026935729980469 and perplexity is 152.46510191842856
At time: 47.1385293006897 and batch: 700, loss is 4.9997920894622805 and perplexity is 148.382305650353
At time: 47.61087393760681 and batch: 750, loss is 4.94486517906189 and perplexity is 140.4519137161471
At time: 48.06849408149719 and batch: 800, loss is 4.9792209815979005 and perplexity is 145.36109856380259
At time: 48.54039669036865 and batch: 850, loss is 4.965898199081421 and perplexity is 143.43732770678767
At time: 48.99575209617615 and batch: 900, loss is 5.049963407516479 and perplexity is 156.01675534139093
At time: 49.4509072303772 and batch: 950, loss is 5.005992431640625 and perplexity is 149.30518484538194
At time: 49.90721106529236 and batch: 1000, loss is 4.968817234039307 and perplexity is 143.856637973556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.2128489424542686 and perplexity of 183.6164263743583
Finished 5 epochs...
Completing Train Step...
At time: 51.337990045547485 and batch: 50, loss is 5.068014736175537 and perplexity is 158.85863777666233
At time: 51.81037497520447 and batch: 100, loss is 5.035632877349854 and perplexity is 153.7968963901192
At time: 52.26704025268555 and batch: 150, loss is 5.042545566558838 and perplexity is 154.8637296312476
At time: 52.739171266555786 and batch: 200, loss is 5.0492887210845945 and perplexity is 155.9115284549833
At time: 53.19615983963013 and batch: 250, loss is 5.023865509033203 and perplexity is 151.99771822153969
At time: 53.65369653701782 and batch: 300, loss is 4.93758674621582 and perplexity is 139.4333551456969
At time: 54.11242699623108 and batch: 350, loss is 4.999461126327515 and perplexity is 148.33320470308112
At time: 54.56878876686096 and batch: 400, loss is 4.915627813339233 and perplexity is 136.4049197543566
At time: 55.02654576301575 and batch: 450, loss is 4.968571290969849 and perplexity is 143.82126178089393
At time: 55.48260140419006 and batch: 500, loss is 4.92255392074585 and perplexity is 137.35295418366678
At time: 55.93906211853027 and batch: 550, loss is 4.97560941696167 and perplexity is 144.83706442193215
At time: 56.39704442024231 and batch: 600, loss is 5.000349702835083 and perplexity is 148.4650686810128
At time: 56.857518672943115 and batch: 650, loss is 4.966747570037842 and perplexity is 143.55921096172932
At time: 57.3147873878479 and batch: 700, loss is 4.938454465866089 and perplexity is 139.55439671533924
At time: 57.7720844745636 and batch: 750, loss is 4.885941009521485 and perplexity is 132.4150105116845
At time: 58.229145765304565 and batch: 800, loss is 4.91545373916626 and perplexity is 136.3811772472995
At time: 58.68636393547058 and batch: 850, loss is 4.914673252105713 and perplexity is 136.2747750313335
At time: 59.14408254623413 and batch: 900, loss is 5.005456352233887 and perplexity is 149.22516686038045
At time: 59.60142540931702 and batch: 950, loss is 4.963012065887451 and perplexity is 143.02394529933093
At time: 60.059632301330566 and batch: 1000, loss is 4.928171758651733 and perplexity is 138.12675231731353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.178832263481326 and perplexity of 177.4754453839881
Finished 6 epochs...
Completing Train Step...
At time: 61.47911024093628 and batch: 50, loss is 5.012500791549683 and perplexity is 150.28008578644798
At time: 61.948511600494385 and batch: 100, loss is 4.9863363456726075 and perplexity is 146.39908414575075
At time: 62.40206432342529 and batch: 150, loss is 5.003802604675293 and perplexity is 148.9785900491007
At time: 62.85706090927124 and batch: 200, loss is 5.01351490020752 and perplexity is 150.4325634239231
At time: 63.31056237220764 and batch: 250, loss is 4.993196754455567 and perplexity is 147.40689475207742
At time: 63.76412653923035 and batch: 300, loss is 4.906811408996582 and perplexity is 135.20760459027903
At time: 64.21834421157837 and batch: 350, loss is 4.966826601028442 and perplexity is 143.57055703672134
At time: 64.68821978569031 and batch: 400, loss is 4.887683458328247 and perplexity is 132.64593802000155
At time: 65.14183282852173 and batch: 450, loss is 4.933879623413086 and perplexity is 138.9174154920713
At time: 65.59648323059082 and batch: 500, loss is 4.894567928314209 and perplexity is 133.56228566372565
At time: 66.04965114593506 and batch: 550, loss is 4.953799257278442 and perplexity is 141.71234410708547
At time: 66.5042667388916 and batch: 600, loss is 4.977111930847168 and perplexity is 145.05484769258365
At time: 66.96055889129639 and batch: 650, loss is 4.938365335464478 and perplexity is 139.54195873022292
At time: 67.4144835472107 and batch: 700, loss is 4.908901634216309 and perplexity is 135.49051450500812
At time: 67.86877036094666 and batch: 750, loss is 4.858226175308228 and perplexity is 128.79553865623836
At time: 68.32409644126892 and batch: 800, loss is 4.894264307022095 and perplexity is 133.52173946563067
At time: 68.77890205383301 and batch: 850, loss is 4.87735746383667 and perplexity is 131.28328428004005
At time: 69.2345061302185 and batch: 900, loss is 4.962304878234863 and perplexity is 142.92283628692724
At time: 69.68967700004578 and batch: 950, loss is 4.93455207824707 and perplexity is 139.01086259560049
At time: 70.14545750617981 and batch: 1000, loss is 4.922594614028931 and perplexity is 137.35854364003927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.1721277004334985 and perplexity of 176.2895300354348
Finished 7 epochs...
Completing Train Step...
At time: 71.57533717155457 and batch: 50, loss is 4.98041036605835 and perplexity is 145.53409165284063
At time: 72.03082990646362 and batch: 100, loss is 4.954323873519898 and perplexity is 141.78670820902107
At time: 72.48623752593994 and batch: 150, loss is 4.961025867462158 and perplexity is 142.74015329131421
At time: 72.94151401519775 and batch: 200, loss is 4.9657823276519775 and perplexity is 143.42070838146185
At time: 73.39714241027832 and batch: 250, loss is 4.944542627334595 and perplexity is 140.40661801427225
At time: 73.85119342803955 and batch: 300, loss is 4.86163070678711 and perplexity is 129.2347743936214
At time: 74.30603098869324 and batch: 350, loss is 4.943887929916382 and perplexity is 140.31472424854624
At time: 74.76144695281982 and batch: 400, loss is 4.864779586791992 and perplexity is 129.64236057384844
At time: 75.21665787696838 and batch: 450, loss is 4.915964393615723 and perplexity is 136.45083868723216
At time: 75.6718692779541 and batch: 500, loss is 4.86849609375 and perplexity is 130.1250737568055
At time: 76.14196634292603 and batch: 550, loss is 4.918575811386108 and perplexity is 136.80763450086388
At time: 76.59769129753113 and batch: 600, loss is 4.9703765487670895 and perplexity is 144.08113072978085
At time: 77.05481839179993 and batch: 650, loss is 4.9251162815094 and perplexity is 137.7053532982448
At time: 77.50903296470642 and batch: 700, loss is 4.891459684371949 and perplexity is 133.1477860151208
At time: 77.96492409706116 and batch: 750, loss is 4.843671436309815 and perplexity is 126.93452931254092
At time: 78.41995072364807 and batch: 800, loss is 4.869621524810791 and perplexity is 130.2716029954112
At time: 78.87531304359436 and batch: 850, loss is 4.852422552108765 and perplexity is 128.05022273846038
At time: 79.33109545707703 and batch: 900, loss is 4.940942993164063 and perplexity is 139.9021141137922
At time: 79.78672575950623 and batch: 950, loss is 4.908457460403443 and perplexity is 135.43034653008186
At time: 80.24096751213074 and batch: 1000, loss is 4.8968245315551755 and perplexity is 133.86402307318963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.188676787585747 and perplexity of 179.2312349602945
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 81.65963864326477 and batch: 50, loss is 4.944236078262329 and perplexity is 140.3635830922747
At time: 82.1324462890625 and batch: 100, loss is 4.893868074417115 and perplexity is 133.4688442790716
At time: 82.58926272392273 and batch: 150, loss is 4.895862894058228 and perplexity is 133.73535628442986
At time: 83.04629611968994 and batch: 200, loss is 4.893775196075439 and perplexity is 133.4564484898087
At time: 83.50385856628418 and batch: 250, loss is 4.8596944236755375 and perplexity is 128.98478138925736
At time: 83.96082925796509 and batch: 300, loss is 4.76684250831604 and perplexity is 117.54750012951908
At time: 84.41780829429626 and batch: 350, loss is 4.826499929428101 and perplexity is 124.77347951015149
At time: 84.87278079986572 and batch: 400, loss is 4.748884334564209 and perplexity is 115.45540304369656
At time: 85.32786202430725 and batch: 450, loss is 4.8010897445678715 and perplexity is 121.64290501181745
At time: 85.78243160247803 and batch: 500, loss is 4.755679740905761 and perplexity is 116.24264119387365
At time: 86.23979163169861 and batch: 550, loss is 4.8047778606414795 and perplexity is 122.09236648860845
At time: 86.69690203666687 and batch: 600, loss is 4.822294235229492 and perplexity is 124.24982235403169
At time: 87.1555347442627 and batch: 650, loss is 4.781102161407471 and perplexity is 119.23569463106618
At time: 87.61470913887024 and batch: 700, loss is 4.7370759391784665 and perplexity is 114.10007785164737
At time: 88.09154963493347 and batch: 750, loss is 4.688277263641357 and perplexity is 108.66581591244345
At time: 88.55178666114807 and batch: 800, loss is 4.726368198394775 and perplexity is 112.88484162300237
At time: 89.01197838783264 and batch: 850, loss is 4.702301025390625 and perplexity is 110.20045498057279
At time: 89.47112131118774 and batch: 900, loss is 4.787069158554077 and perplexity is 119.94930060578041
At time: 89.9322247505188 and batch: 950, loss is 4.7384340190887455 and perplexity is 114.25514014479872
At time: 90.39255785942078 and batch: 1000, loss is 4.729127264022827 and perplexity is 113.19672836960198
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.102793251595846 and perplexity of 164.48070222074037
Finished 9 epochs...
Completing Train Step...
At time: 91.8170862197876 and batch: 50, loss is 4.86601752281189 and perplexity is 129.80294890023592
At time: 92.29019451141357 and batch: 100, loss is 4.835165061950684 and perplexity is 125.85935608324684
At time: 92.74773287773132 and batch: 150, loss is 4.852523202896118 and perplexity is 128.06311174283314
At time: 93.20440912246704 and batch: 200, loss is 4.858704948425293 and perplexity is 128.85721726159585
At time: 93.66154479980469 and batch: 250, loss is 4.826676473617554 and perplexity is 124.7955094875319
At time: 94.11910581588745 and batch: 300, loss is 4.74122522354126 and perplexity is 114.5744950858238
At time: 94.57531499862671 and batch: 350, loss is 4.801058139801025 and perplexity is 121.6390605769176
At time: 95.03136587142944 and batch: 400, loss is 4.724556541442871 and perplexity is 112.68051815281322
At time: 95.4873218536377 and batch: 450, loss is 4.777801361083984 and perplexity is 118.8427702508692
At time: 95.94393491744995 and batch: 500, loss is 4.735246829986572 and perplexity is 113.89156710304997
At time: 96.40035629272461 and batch: 550, loss is 4.783871097564697 and perplexity is 119.5663081698182
At time: 96.8589301109314 and batch: 600, loss is 4.804065237045288 and perplexity is 122.00539158120016
At time: 97.31578993797302 and batch: 650, loss is 4.76807020187378 and perplexity is 117.69190106006539
At time: 97.77392649650574 and batch: 700, loss is 4.725848131179809 and perplexity is 112.82614918113791
At time: 98.23172354698181 and batch: 750, loss is 4.6808089256286625 and perplexity is 107.85728581486447
At time: 98.68946695327759 and batch: 800, loss is 4.71898847579956 and perplexity is 112.05484913059519
At time: 99.14641189575195 and batch: 850, loss is 4.698886060714722 and perplexity is 109.82476616666635
At time: 99.620676279068 and batch: 900, loss is 4.787100677490234 and perplexity is 119.95308133971032
At time: 100.07944822311401 and batch: 950, loss is 4.741510486602783 and perplexity is 114.60718361925828
At time: 100.53748202323914 and batch: 1000, loss is 4.731764259338379 and perplexity is 113.4956215287766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.097851636933117 and perplexity of 163.66990693977266
Finished 10 epochs...
Completing Train Step...
At time: 101.97457242012024 and batch: 50, loss is 4.845533361434937 and perplexity is 127.17109206463711
At time: 102.4301905632019 and batch: 100, loss is 4.815619707107544 and perplexity is 123.423274896217
At time: 102.88504791259766 and batch: 150, loss is 4.832728338241577 and perplexity is 125.55304495562275
At time: 103.33994960784912 and batch: 200, loss is 4.841638011932373 and perplexity is 126.67667979389284
At time: 103.79486846923828 and batch: 250, loss is 4.810773515701294 and perplexity is 122.82658907672091
At time: 104.24886512756348 and batch: 300, loss is 4.726732788085937 and perplexity is 112.92600577610352
At time: 104.70183253288269 and batch: 350, loss is 4.787072162628174 and perplexity is 119.94966094290855
At time: 105.15428733825684 and batch: 400, loss is 4.710960063934326 and perplexity is 111.15882827528775
At time: 105.60672783851624 and batch: 450, loss is 4.7629557132720945 and perplexity is 117.09150384470523
At time: 106.06049537658691 and batch: 500, loss is 4.720763988494873 and perplexity is 112.25398066576972
At time: 106.51592230796814 and batch: 550, loss is 4.769894294738769 and perplexity is 117.90677793516322
At time: 106.97045731544495 and batch: 600, loss is 4.791376609802246 and perplexity is 120.46709074769275
At time: 107.4258873462677 and batch: 650, loss is 4.758106241226196 and perplexity is 116.52504648968456
At time: 107.88011288642883 and batch: 700, loss is 4.716378335952759 and perplexity is 111.76275167728727
At time: 108.33442044258118 and batch: 750, loss is 4.6713245677948 and perplexity is 106.8391644667139
At time: 108.78963375091553 and batch: 800, loss is 4.709707517623901 and perplexity is 111.01968385566668
At time: 109.2442512512207 and batch: 850, loss is 4.691595430374146 and perplexity is 109.02698608784539
At time: 109.70007824897766 and batch: 900, loss is 4.7830601406097415 and perplexity is 119.46938434646567
At time: 110.15505623817444 and batch: 950, loss is 4.737741422653198 and perplexity is 114.17603483918002
At time: 110.6119544506073 and batch: 1000, loss is 4.726997699737549 and perplexity is 112.95592515362445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.095331331578697 and perplexity of 163.25792817117502
Finished 11 epochs...
Completing Train Step...
At time: 112.03601574897766 and batch: 50, loss is 4.830758419036865 and perplexity is 125.30595905069232
At time: 112.5101706981659 and batch: 100, loss is 4.800748729705811 and perplexity is 121.60143004553608
At time: 112.96829319000244 and batch: 150, loss is 4.818946447372436 and perplexity is 123.83455580751239
At time: 113.42738342285156 and batch: 200, loss is 4.828571786880493 and perplexity is 125.0322603594811
At time: 113.88636350631714 and batch: 250, loss is 4.799181814193726 and perplexity is 121.41104007995536
At time: 114.34577226638794 and batch: 300, loss is 4.716084938049317 and perplexity is 111.72996553019087
At time: 114.80371046066284 and batch: 350, loss is 4.776620607376099 and perplexity is 118.70252902070985
At time: 115.26031851768494 and batch: 400, loss is 4.7009093284606935 and perplexity is 110.04719601545116
At time: 115.71763515472412 and batch: 450, loss is 4.753028326034546 and perplexity is 115.93484195839702
At time: 116.17416405677795 and batch: 500, loss is 4.710513877868652 and perplexity is 111.10924181825038
At time: 116.63206934928894 and batch: 550, loss is 4.759294681549072 and perplexity is 116.66361187559484
At time: 117.0905385017395 and batch: 600, loss is 4.781025800704956 and perplexity is 119.22659005727955
At time: 117.54881429672241 and batch: 650, loss is 4.749134864807129 and perplexity is 115.48433173747256
At time: 118.00685620307922 and batch: 700, loss is 4.7077349662780765 and perplexity is 110.80090767342796
At time: 118.4648826122284 and batch: 750, loss is 4.66295467376709 and perplexity is 105.94866387856716
At time: 118.92241930961609 and batch: 800, loss is 4.700886754989624 and perplexity is 110.04471189629335
At time: 119.38094902038574 and batch: 850, loss is 4.684329223632813 and perplexity is 108.23764469836117
At time: 119.83943843841553 and batch: 900, loss is 4.777081613540649 and perplexity is 118.75726423400208
At time: 120.29847741127014 and batch: 950, loss is 4.7336138820648195 and perplexity is 113.70573986963387
At time: 120.75782632827759 and batch: 1000, loss is 4.721479215621948 and perplexity is 112.3342964764635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.093764142292302 and perplexity of 163.00227247798372
Finished 12 epochs...
Completing Train Step...
At time: 122.19016718864441 and batch: 50, loss is 4.818915519714356 and perplexity is 123.8307259539363
At time: 122.66140866279602 and batch: 100, loss is 4.7887428188323975 and perplexity is 120.15022307669493
At time: 123.11595320701599 and batch: 150, loss is 4.807908039093018 and perplexity is 122.47513613941294
At time: 123.58641123771667 and batch: 200, loss is 4.818925065994263 and perplexity is 123.8319080823498
At time: 124.04251313209534 and batch: 250, loss is 4.789953060150147 and perplexity is 120.29572186756418
At time: 124.49868059158325 and batch: 300, loss is 4.707611303329468 and perplexity is 110.78720655365449
At time: 124.95341157913208 and batch: 350, loss is 4.767184057235718 and perplexity is 117.58765520827062
At time: 125.40776228904724 and batch: 400, loss is 4.692242250442505 and perplexity is 109.09752974250605
At time: 125.86403679847717 and batch: 450, loss is 4.742996854782104 and perplexity is 114.77765875313304
At time: 126.32134675979614 and batch: 500, loss is 4.701899757385254 and perplexity is 110.15624393464411
At time: 126.77865433692932 and batch: 550, loss is 4.750229272842407 and perplexity is 115.61078790277486
At time: 127.2356743812561 and batch: 600, loss is 4.771919479370117 and perplexity is 118.14580288290043
At time: 127.69360709190369 and batch: 650, loss is 4.74078706741333 and perplexity is 114.52430456514251
At time: 128.15218329429626 and batch: 700, loss is 4.700562868118286 and perplexity is 110.00907563022086
At time: 128.61204743385315 and batch: 750, loss is 4.656200885772705 and perplexity is 105.23551998652718
At time: 129.06885623931885 and batch: 800, loss is 4.693489847183227 and perplexity is 109.23372440537682
At time: 129.52545833587646 and batch: 850, loss is 4.677197828292846 and perplexity is 107.4685050424915
At time: 129.98227167129517 and batch: 900, loss is 4.770779399871826 and perplexity is 118.0111840279041
At time: 130.44149470329285 and batch: 950, loss is 4.72653489112854 and perplexity is 112.90366027427574
At time: 130.90097522735596 and batch: 1000, loss is 4.71399938583374 and perplexity is 111.49718967051012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.092440349299733 and perplexity of 162.78663397376593
Finished 13 epochs...
Completing Train Step...
At time: 132.33705854415894 and batch: 50, loss is 4.808477077484131 and perplexity is 122.54484902660653
At time: 132.7909393310547 and batch: 100, loss is 4.778320665359497 and perplexity is 118.90450183692526
At time: 133.24636006355286 and batch: 150, loss is 4.7973739051818844 and perplexity is 121.1917382650545
At time: 133.7008912563324 and batch: 200, loss is 4.809714965820312 and perplexity is 122.69663979650419
At time: 134.1565477848053 and batch: 250, loss is 4.781264114379883 and perplexity is 119.25500677001618
At time: 134.61078691482544 and batch: 300, loss is 4.699477319717407 and perplexity is 109.88972024883083
At time: 135.0801284313202 and batch: 350, loss is 4.759486951828003 and perplexity is 116.68604497733152
At time: 135.53566932678223 and batch: 400, loss is 4.683971767425537 and perplexity is 108.19896139460825
At time: 135.98950123786926 and batch: 450, loss is 4.734281826019287 and perplexity is 113.78171430167467
At time: 136.44291877746582 and batch: 500, loss is 4.693106966018677 and perplexity is 109.19190887546856
At time: 136.89785885810852 and batch: 550, loss is 4.741453037261963 and perplexity is 114.60059970122884
At time: 137.35199332237244 and batch: 600, loss is 4.763644371032715 and perplexity is 117.17216758919953
At time: 137.80635285377502 and batch: 650, loss is 4.733594627380371 and perplexity is 113.70355052257034
At time: 138.26011276245117 and batch: 700, loss is 4.693451747894287 and perplexity is 109.22956275742715
At time: 138.71526885032654 and batch: 750, loss is 4.649165391921997 and perplexity is 104.49773451859755
At time: 139.17045760154724 and batch: 800, loss is 4.6874637699127195 and perplexity is 108.57745289893309
At time: 139.6250147819519 and batch: 850, loss is 4.671906471252441 and perplexity is 106.90135263792973
At time: 140.0795030593872 and batch: 900, loss is 4.7678525924682615 and perplexity is 117.6662929818218
At time: 140.5359606742859 and batch: 950, loss is 4.722266578674317 and perplexity is 112.42277918045765
At time: 140.9988911151886 and batch: 1000, loss is 4.7090585327148435 and perplexity is 110.94765713089278
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.092038782631478 and perplexity of 162.72127741091154
Finished 14 epochs...
Completing Train Step...
At time: 142.4203290939331 and batch: 50, loss is 4.799540996551514 and perplexity is 121.45465661626902
At time: 142.896586894989 and batch: 100, loss is 4.769684209823608 and perplexity is 117.882010101489
At time: 143.34769558906555 and batch: 150, loss is 4.788465824127197 and perplexity is 120.1169467099755
At time: 143.79823279380798 and batch: 200, loss is 4.802030124664307 and perplexity is 121.75734938073272
At time: 144.25007462501526 and batch: 250, loss is 4.773352603912354 and perplexity is 118.31524191719325
At time: 144.70830535888672 and batch: 300, loss is 4.692144041061401 and perplexity is 109.08681586774019
At time: 145.1750886440277 and batch: 350, loss is 4.752198886871338 and perplexity is 115.83872092888028
At time: 145.62935042381287 and batch: 400, loss is 4.676276702880859 and perplexity is 107.36955864952317
At time: 146.08455276489258 and batch: 450, loss is 4.727021760940552 and perplexity is 112.9586430417677
At time: 146.5395791530609 and batch: 500, loss is 4.685736141204834 and perplexity is 108.39003331661716
At time: 147.01092767715454 and batch: 550, loss is 4.733314046859741 and perplexity is 113.67165199642906
At time: 147.46628284454346 and batch: 600, loss is 4.755718612670899 and perplexity is 116.24715983834436
At time: 147.92299270629883 and batch: 650, loss is 4.726054916381836 and perplexity is 112.84948237158629
At time: 148.3789427280426 and batch: 700, loss is 4.687179069519043 and perplexity is 108.54654525526634
At time: 148.83594465255737 and batch: 750, loss is 4.642973184585571 and perplexity is 103.85266215238029
At time: 149.29170322418213 and batch: 800, loss is 4.680957355499268 and perplexity is 107.8732962460255
At time: 149.7479431629181 and batch: 850, loss is 4.66536171913147 and perplexity is 106.20399429148247
At time: 150.204669713974 and batch: 900, loss is 4.761999483108521 and perplexity is 116.97959093260934
At time: 150.66170001029968 and batch: 950, loss is 4.7175336360931395 and perplexity is 111.89194581462208
At time: 151.11786937713623 and batch: 1000, loss is 4.7030738353729244 and perplexity is 110.28565190851369
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.092014591868331 and perplexity of 162.71734110664198
Finished 15 epochs...
Completing Train Step...
At time: 152.54161190986633 and batch: 50, loss is 4.791429023742676 and perplexity is 120.47340506808881
At time: 153.01466512680054 and batch: 100, loss is 4.761492977142334 and perplexity is 116.92035507480398
At time: 153.47173142433167 and batch: 150, loss is 4.780287990570068 and perplexity is 119.13865591412717
At time: 153.92906141281128 and batch: 200, loss is 4.794946584701538 and perplexity is 120.89792381189481
At time: 154.38572669029236 and batch: 250, loss is 4.766745824813842 and perplexity is 117.53613577491357
At time: 154.84198784828186 and batch: 300, loss is 4.6861585426330565 and perplexity is 108.43582709249247
At time: 155.29927349090576 and batch: 350, loss is 4.746186571121216 and perplexity is 115.1443514385448
At time: 155.75690388679504 and batch: 400, loss is 4.6700711441040035 and perplexity is 106.70533361776621
At time: 156.21745228767395 and batch: 450, loss is 4.720376977920532 and perplexity is 112.21054559369838
At time: 156.67990732192993 and batch: 500, loss is 4.679275102615357 and perplexity is 107.6919786361151
At time: 157.1400589942932 and batch: 550, loss is 4.726763582229614 and perplexity is 112.92948328929349
At time: 157.59905338287354 and batch: 600, loss is 4.7495029926300045 and perplexity is 115.5268525591599
At time: 158.05853605270386 and batch: 650, loss is 4.720370054244995 and perplexity is 112.2097686869784
At time: 158.5347294807434 and batch: 700, loss is 4.680894870758056 and perplexity is 107.86655602160873
At time: 158.99553775787354 and batch: 750, loss is 4.636902322769165 and perplexity is 103.22409688811716
At time: 159.45621609687805 and batch: 800, loss is 4.674688310623169 and perplexity is 107.1991490483267
At time: 159.91585326194763 and batch: 850, loss is 4.658942079544067 and perplexity is 105.52438667734498
At time: 160.37613654136658 and batch: 900, loss is 4.756955499649048 and perplexity is 116.39103339591762
At time: 160.83536338806152 and batch: 950, loss is 4.712795467376709 and perplexity is 111.3630369166505
At time: 161.29634141921997 and batch: 1000, loss is 4.698135137557983 and perplexity is 109.74232716312062
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.091920061809261 and perplexity of 162.70196015376806
Finished 16 epochs...
Completing Train Step...
At time: 162.73464035987854 and batch: 50, loss is 4.784136695861816 and perplexity is 119.59806899528479
At time: 163.19095730781555 and batch: 100, loss is 4.754520893096924 and perplexity is 116.10801168645396
At time: 163.6471564769745 and batch: 150, loss is 4.772742233276367 and perplexity is 118.24304780256249
At time: 164.10309052467346 and batch: 200, loss is 4.7878255367279055 and perplexity is 120.04006195935091
At time: 164.55877351760864 and batch: 250, loss is 4.7602823543548585 and perplexity is 116.77889427374254
At time: 165.01353526115417 and batch: 300, loss is 4.6795651245117185 and perplexity is 107.72321619755154
At time: 165.4681921005249 and batch: 350, loss is 4.740261840820312 and perplexity is 114.46416914858044
At time: 165.92274641990662 and batch: 400, loss is 4.664006910324097 and perplexity is 106.06020560971815
At time: 166.37916731834412 and batch: 450, loss is 4.714200239181519 and perplexity is 111.5195865034875
At time: 166.83566641807556 and batch: 500, loss is 4.672938375473023 and perplexity is 107.01172153017188
At time: 167.29276585578918 and batch: 550, loss is 4.720151834487915 and perplexity is 112.18528497002603
At time: 167.74984860420227 and batch: 600, loss is 4.742705249786377 and perplexity is 114.74419389393995
At time: 168.207044839859 and batch: 650, loss is 4.7145498085021975 and perplexity is 111.55857714415272
At time: 168.66349387168884 and batch: 700, loss is 4.67496883392334 and perplexity is 107.22922512571631
At time: 169.12042832374573 and batch: 750, loss is 4.63153657913208 and perplexity is 102.6717061658796
At time: 169.57737803459167 and batch: 800, loss is 4.668854150772095 and perplexity is 106.5755529254108
At time: 170.03454208374023 and batch: 850, loss is 4.652864332199097 and perplexity is 104.88498115723773
At time: 170.50687646865845 and batch: 900, loss is 4.75164984703064 and perplexity is 115.77513831228687
At time: 170.96447920799255 and batch: 950, loss is 4.707621173858643 and perplexity is 110.7883000874058
At time: 171.42372679710388 and batch: 1000, loss is 4.692934617996216 and perplexity is 109.17309148752153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.092332049113948 and perplexity of 162.76900510568538
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 172.842707157135 and batch: 50, loss is 4.778726959228516 and perplexity is 118.95282182241257
At time: 173.31423091888428 and batch: 100, loss is 4.737402248382568 and perplexity is 114.13731583245661
At time: 173.76935124397278 and batch: 150, loss is 4.755582294464111 and perplexity is 116.23131431401234
At time: 174.22503232955933 and batch: 200, loss is 4.765315456390381 and perplexity is 117.36813597713171
At time: 174.68054127693176 and batch: 250, loss is 4.734832067489624 and perplexity is 113.84433894720792
At time: 175.136873960495 and batch: 300, loss is 4.6522759914398195 and perplexity is 104.82329119692542
At time: 175.59245920181274 and batch: 350, loss is 4.713141164779663 and perplexity is 111.40154148437877
At time: 176.0487401485443 and batch: 400, loss is 4.630107707977295 and perplexity is 102.52510628763773
At time: 176.5046021938324 and batch: 450, loss is 4.680647706985473 and perplexity is 107.83989861119568
At time: 176.96006274223328 and batch: 500, loss is 4.633500709533691 and perplexity is 102.87356495894709
At time: 177.41589760780334 and batch: 550, loss is 4.679807643890381 and perplexity is 107.74934433317287
At time: 177.87040543556213 and batch: 600, loss is 4.70399730682373 and perplexity is 110.38754459973923
At time: 178.32619261741638 and batch: 650, loss is 4.669306964874267 and perplexity is 106.62382276652987
At time: 178.7816517353058 and batch: 700, loss is 4.629137678146362 and perplexity is 102.42570209642913
At time: 179.23711133003235 and batch: 750, loss is 4.5809071922302245 and perplexity is 97.60289864701392
At time: 179.6946041584015 and batch: 800, loss is 4.614781770706177 and perplexity is 100.96579243452406
At time: 180.150053024292 and batch: 850, loss is 4.593614273071289 and perplexity is 98.85106001917694
At time: 180.6083788871765 and batch: 900, loss is 4.688164892196656 and perplexity is 108.65360566377421
At time: 181.07550764083862 and batch: 950, loss is 4.646795129776001 and perplexity is 104.25034080392867
At time: 181.54000854492188 and batch: 1000, loss is 4.632652788162232 and perplexity is 102.78637323574696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.060666340153392 and perplexity of 157.69556020577863
Finished 18 epochs...
Completing Train Step...
At time: 183.0314061641693 and batch: 50, loss is 4.758737535476684 and perplexity is 116.59863130596327
At time: 183.5132384300232 and batch: 100, loss is 4.722180128097534 and perplexity is 112.41306058644909
At time: 183.96765661239624 and batch: 150, loss is 4.742457647323608 and perplexity is 114.71578646596303
At time: 184.4225766658783 and batch: 200, loss is 4.752664680480957 and perplexity is 115.89269043318473
At time: 184.8769233226776 and batch: 250, loss is 4.725062770843506 and perplexity is 112.73757478463475
At time: 185.33076167106628 and batch: 300, loss is 4.642731962203979 and perplexity is 103.82761358713995
At time: 185.78535866737366 and batch: 350, loss is 4.7028771591186525 and perplexity is 110.26396347246647
At time: 186.23955059051514 and batch: 400, loss is 4.622478742599487 and perplexity is 101.74592176639449
At time: 186.6928985118866 and batch: 450, loss is 4.673825321197509 and perplexity is 107.10667722310103
At time: 187.14773607254028 and batch: 500, loss is 4.627364501953125 and perplexity is 102.2442442058463
At time: 187.60772705078125 and batch: 550, loss is 4.674399003982544 and perplexity is 107.16814010840014
At time: 188.07336497306824 and batch: 600, loss is 4.700114049911499 and perplexity is 109.95971263250087
At time: 188.53494572639465 and batch: 650, loss is 4.6662319374084475 and perplexity is 106.29645517314263
At time: 189.0011579990387 and batch: 700, loss is 4.628134031295776 and perplexity is 102.3229544329103
At time: 189.4656584262848 and batch: 750, loss is 4.582151069641113 and perplexity is 97.724380226311
At time: 189.9259717464447 and batch: 800, loss is 4.616682558059693 and perplexity is 101.15788944586913
At time: 190.3837537765503 and batch: 850, loss is 4.5962419509887695 and perplexity is 99.11115033384351
At time: 190.84077668190002 and batch: 900, loss is 4.692284679412841 and perplexity is 109.10215873656028
At time: 191.29788494110107 and batch: 950, loss is 4.651144485473633 and perplexity is 104.70475009517016
At time: 191.7548053264618 and batch: 1000, loss is 4.635494689941407 and perplexity is 103.07889747842475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.059009086794969 and perplexity of 157.4344351439768
Finished 19 epochs...
Completing Train Step...
At time: 193.20184588432312 and batch: 50, loss is 4.75356294631958 and perplexity is 115.9968396477847
At time: 193.65977215766907 and batch: 100, loss is 4.716689987182617 and perplexity is 111.79758810442559
At time: 194.13381218910217 and batch: 150, loss is 4.73730652809143 and perplexity is 114.12639109822314
At time: 194.59248447418213 and batch: 200, loss is 4.747301187515259 and perplexity is 115.2727647728487
At time: 195.05068850517273 and batch: 250, loss is 4.720359373092651 and perplexity is 112.2085701637454
At time: 195.5086259841919 and batch: 300, loss is 4.638096466064453 and perplexity is 103.3474348782888
At time: 195.96791124343872 and batch: 350, loss is 4.698163232803345 and perplexity is 109.74541044404135
At time: 196.42654132843018 and batch: 400, loss is 4.618740530014038 and perplexity is 101.36628390673724
At time: 196.8848512172699 and batch: 450, loss is 4.670242214202881 and perplexity is 106.7235892711927
At time: 197.34301090240479 and batch: 500, loss is 4.623926963806152 and perplexity is 101.89337911764746
At time: 197.8022861480713 and batch: 550, loss is 4.671849203109741 and perplexity is 106.89523077130768
At time: 198.26086449623108 and batch: 600, loss is 4.69812195777893 and perplexity is 109.74088079302729
At time: 198.71966552734375 and batch: 650, loss is 4.664952507019043 and perplexity is 106.1605432215887
At time: 199.1785707473755 and batch: 700, loss is 4.627837371826172 and perplexity is 102.29260386163469
At time: 199.63619685173035 and batch: 750, loss is 4.583413171768188 and perplexity is 97.8477962398859
At time: 200.09435439109802 and batch: 800, loss is 4.618055562973023 and perplexity is 101.2968751172713
At time: 200.5521912574768 and batch: 850, loss is 4.597564363479615 and perplexity is 99.24230285678127
At time: 201.01049900054932 and batch: 900, loss is 4.694348230361938 and perplexity is 109.32752905135968
At time: 201.46923971176147 and batch: 950, loss is 4.6528706550598145 and perplexity is 104.88564433246155
At time: 201.92869973182678 and batch: 1000, loss is 4.636076993942261 and perplexity is 103.13893821211327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.05849921994093 and perplexity of 157.3541850039942
Finished 20 epochs...
Completing Train Step...
At time: 203.35351181030273 and batch: 50, loss is 4.750126647949219 and perplexity is 115.59892396679285
At time: 203.82365560531616 and batch: 100, loss is 4.7131231498718265 and perplexity is 111.39953461395291
At time: 204.27777934074402 and batch: 150, loss is 4.734146070480347 and perplexity is 113.7662688521545
At time: 204.73280143737793 and batch: 200, loss is 4.743670301437378 and perplexity is 114.85498141694278
At time: 205.1871359348297 and batch: 250, loss is 4.717143716812134 and perplexity is 111.84832549231477
At time: 205.65803694725037 and batch: 300, loss is 4.63486011505127 and perplexity is 103.01350694817052
At time: 206.11224508285522 and batch: 350, loss is 4.69478663444519 and perplexity is 109.37546919431703
At time: 206.5665521621704 and batch: 400, loss is 4.615921945571899 and perplexity is 101.08097674600579
At time: 207.02066779136658 and batch: 450, loss is 4.6674665927886965 and perplexity is 106.42777571454823
At time: 207.47457361221313 and batch: 500, loss is 4.621418914794922 and perplexity is 101.63814573161267
At time: 207.92845463752747 and batch: 550, loss is 4.669924392700195 and perplexity is 106.68967560920943
At time: 208.38230323791504 and batch: 600, loss is 4.696763153076172 and perplexity is 109.59186563229932
At time: 208.8364760875702 and batch: 650, loss is 4.663972702026367 and perplexity is 106.05657753268292
At time: 209.29102754592896 and batch: 700, loss is 4.627724952697754 and perplexity is 102.28110486263081
At time: 209.7453362941742 and batch: 750, loss is 4.584251127243042 and perplexity is 97.92982269890798
At time: 210.20003414154053 and batch: 800, loss is 4.618861608505249 and perplexity is 101.37855792649722
At time: 210.6567850112915 and batch: 850, loss is 4.59827675819397 and perplexity is 99.31302773780149
At time: 211.11199116706848 and batch: 900, loss is 4.695367002487183 and perplexity is 109.4389656450885
At time: 211.5665786266327 and batch: 950, loss is 4.653756761550904 and perplexity is 104.9786253721941
At time: 212.02118372917175 and batch: 1000, loss is 4.636122627258301 and perplexity is 103.14364489126658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.05814380180545 and perplexity of 157.29826841042447
Finished 21 epochs...
Completing Train Step...
At time: 213.4355912208557 and batch: 50, loss is 4.747271718978882 and perplexity is 115.26936790323742
At time: 213.90555047988892 and batch: 100, loss is 4.710303363800048 and perplexity is 111.08585422149119
At time: 214.3595368862152 and batch: 150, loss is 4.73150405883789 and perplexity is 113.46609375298911
At time: 214.81203174591064 and batch: 200, loss is 4.7407846355438235 and perplexity is 114.52402605731712
At time: 215.26506686210632 and batch: 250, loss is 4.714438371658325 and perplexity is 111.54614610105982
At time: 215.72104406356812 and batch: 300, loss is 4.632227153778076 and perplexity is 102.7426331303819
At time: 216.1765751838684 and batch: 350, loss is 4.691895904541016 and perplexity is 109.05975080288525
At time: 216.63153386116028 and batch: 400, loss is 4.613426589965821 and perplexity is 100.82905820790343
At time: 217.08623838424683 and batch: 450, loss is 4.665488748550415 and perplexity is 106.21748618008226
At time: 217.5580723285675 and batch: 500, loss is 4.619250316619873 and perplexity is 101.4179722544504
At time: 218.01586151123047 and batch: 550, loss is 4.668258838653564 and perplexity is 106.51212608847072
At time: 218.4741849899292 and batch: 600, loss is 4.69541558265686 and perplexity is 109.4442823377507
At time: 218.9314706325531 and batch: 650, loss is 4.662958106994629 and perplexity is 105.94902762506209
At time: 219.38757944107056 and batch: 700, loss is 4.627513666152954 and perplexity is 102.25949652424197
At time: 219.84423542022705 and batch: 750, loss is 4.584869089126587 and perplexity is 97.99035829901857
At time: 220.30159950256348 and batch: 800, loss is 4.619146785736084 and perplexity is 101.40747290566365
At time: 220.75878643989563 and batch: 850, loss is 4.598640670776367 and perplexity is 99.34917557511874
At time: 221.21434354782104 and batch: 900, loss is 4.6957845211029055 and perplexity is 109.48466799065685
At time: 221.6705458164215 and batch: 950, loss is 4.653907699584961 and perplexity is 104.99447183541272
At time: 222.12717652320862 and batch: 1000, loss is 4.635560379028321 and perplexity is 103.08566885948069
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.058067507860137 and perplexity of 157.28626796272317
Finished 22 epochs...
Completing Train Step...
At time: 223.56040048599243 and batch: 50, loss is 4.744978322982788 and perplexity is 115.00531250395044
At time: 224.0176296234131 and batch: 100, loss is 4.707916355133056 and perplexity is 110.8210075460929
At time: 224.47492027282715 and batch: 150, loss is 4.7292542552948 and perplexity is 113.21110427890899
At time: 224.93144941329956 and batch: 200, loss is 4.738273296356201 and perplexity is 114.23677822209615
At time: 225.38805222511292 and batch: 250, loss is 4.712095117568969 and perplexity is 111.28507114000178
At time: 225.84644865989685 and batch: 300, loss is 4.629738903045654 and perplexity is 102.48730149454738
At time: 226.30619311332703 and batch: 350, loss is 4.689686765670777 and perplexity is 108.81908859420307
At time: 226.76489162445068 and batch: 400, loss is 4.611444272994995 and perplexity is 100.6293810317803
At time: 227.2235426902771 and batch: 450, loss is 4.663551025390625 and perplexity is 106.01186537956826
At time: 227.68084383010864 and batch: 500, loss is 4.617296714782714 and perplexity is 101.22003532546049
At time: 228.13798141479492 and batch: 550, loss is 4.6668803501129155 and perplexity is 106.36540149553684
At time: 228.59589052200317 and batch: 600, loss is 4.694295883178711 and perplexity is 109.32180621295318
At time: 229.0684950351715 and batch: 650, loss is 4.6619657611846925 and perplexity is 105.84394170084343
At time: 229.52637767791748 and batch: 700, loss is 4.627086391448975 and perplexity is 102.21581296124086
At time: 229.983252286911 and batch: 750, loss is 4.585208692550659 and perplexity is 98.02364181150047
At time: 230.44132781028748 and batch: 800, loss is 4.619235401153564 and perplexity is 101.41645956938342
At time: 230.90112400054932 and batch: 850, loss is 4.598682308197022 and perplexity is 99.35331230465461
At time: 231.36100125312805 and batch: 900, loss is 4.695940675735474 and perplexity is 109.50176586368012
At time: 231.82113480567932 and batch: 950, loss is 4.653868474960327 and perplexity is 104.99035354743606
At time: 232.28162455558777 and batch: 1000, loss is 4.634929180145264 and perplexity is 103.02062183140276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.057967395317264 and perplexity of 157.27052242265452
Finished 23 epochs...
Completing Train Step...
At time: 233.7082121372223 and batch: 50, loss is 4.742879219055176 and perplexity is 114.76415759393538
At time: 234.18078136444092 and batch: 100, loss is 4.705753698348999 and perplexity is 110.58159871518468
At time: 234.6375334262848 and batch: 150, loss is 4.727167491912842 and perplexity is 112.97510581418547
At time: 235.0948987007141 and batch: 200, loss is 4.736023721694946 and perplexity is 113.98008289632692
At time: 235.55156230926514 and batch: 250, loss is 4.7099493217468265 and perplexity is 111.04653211882842
At time: 236.00878024101257 and batch: 300, loss is 4.627521200180054 and perplexity is 102.26026695296223
At time: 236.46544122695923 and batch: 350, loss is 4.6875322151184085 and perplexity is 108.58488475936464
At time: 236.92153429985046 and batch: 400, loss is 4.609488582611084 and perplexity is 100.4327734334164
At time: 237.37885236740112 and batch: 450, loss is 4.661764163970947 and perplexity is 105.82260600778508
At time: 237.8341042995453 and batch: 500, loss is 4.6153915500640865 and perplexity is 101.02737806552061
At time: 238.28808665275574 and batch: 550, loss is 4.665581874847412 and perplexity is 106.22737828184674
At time: 238.74189043045044 and batch: 600, loss is 4.693297739028931 and perplexity is 109.21274173173026
At time: 239.22006845474243 and batch: 650, loss is 4.661131763458252 and perplexity is 105.7557048938701
At time: 239.71567702293396 and batch: 700, loss is 4.626573343276977 and perplexity is 102.16338477549426
At time: 240.19098830223083 and batch: 750, loss is 4.585166292190552 and perplexity is 98.01948566190036
At time: 240.66782879829407 and batch: 800, loss is 4.619111089706421 and perplexity is 101.40385312610904
At time: 241.16767287254333 and batch: 850, loss is 4.598462390899658 and perplexity is 99.33146519509512
At time: 241.634051322937 and batch: 900, loss is 4.695797548294068 and perplexity is 109.48609427764633
At time: 242.13607120513916 and batch: 950, loss is 4.653575344085693 and perplexity is 104.95958214351718
At time: 242.62361979484558 and batch: 1000, loss is 4.6342207717895505 and perplexity is 102.94766700604156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0578248558974845 and perplexity of 157.24810677123494
Finished 24 epochs...
Completing Train Step...
At time: 244.1020634174347 and batch: 50, loss is 4.740995140075683 and perplexity is 114.54813642139044
At time: 244.5805537700653 and batch: 100, loss is 4.703696594238282 and perplexity is 110.35435466636676
At time: 245.03455328941345 and batch: 150, loss is 4.7253769016265865 and perplexity is 112.77299469023454
At time: 245.48974990844727 and batch: 200, loss is 4.733878593444825 and perplexity is 113.73584305710163
At time: 245.95201539993286 and batch: 250, loss is 4.70792366027832 and perplexity is 110.8218171126083
At time: 246.42479252815247 and batch: 300, loss is 4.625514907836914 and perplexity is 102.0553086342698
At time: 246.88413166999817 and batch: 350, loss is 4.685435371398926 and perplexity is 108.35743776945807
At time: 247.34917211532593 and batch: 400, loss is 4.60767786026001 and perplexity is 100.25108211169163
At time: 247.80148077011108 and batch: 450, loss is 4.660113897323608 and perplexity is 105.64811450890696
At time: 248.25450491905212 and batch: 500, loss is 4.613582544326782 and perplexity is 100.84478416547653
At time: 248.70654463768005 and batch: 550, loss is 4.664241199493408 and perplexity is 106.08505727831218
At time: 249.16425609588623 and batch: 600, loss is 4.6923231029510495 and perplexity is 109.1063509080636
At time: 249.62076950073242 and batch: 650, loss is 4.660226354598999 and perplexity is 105.65999607608659
At time: 250.0780279636383 and batch: 700, loss is 4.626031684875488 and perplexity is 102.10806210415355
At time: 250.53480625152588 and batch: 750, loss is 4.585217781066895 and perplexity is 98.02453270500898
At time: 250.99270844459534 and batch: 800, loss is 4.618900365829468 and perplexity is 101.38248716427854
At time: 251.44905376434326 and batch: 850, loss is 4.597878370285034 and perplexity is 99.27347050843508
At time: 251.90568923950195 and batch: 900, loss is 4.695553646087647 and perplexity is 109.45939363398493
At time: 252.36296129226685 and batch: 950, loss is 4.653130407333374 and perplexity is 104.9128921557295
At time: 252.83722805976868 and batch: 1000, loss is 4.633307542800903 and perplexity is 102.8536951276707
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.057841603348895 and perplexity of 157.25074029831487
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 254.27015805244446 and batch: 50, loss is 4.73980562210083 and perplexity is 114.41196036212325
At time: 254.72482776641846 and batch: 100, loss is 4.699833126068115 and perplexity is 109.92882666591609
At time: 255.17912459373474 and batch: 150, loss is 4.7214608478546145 and perplexity is 112.3322331651915
At time: 255.6325535774231 and batch: 200, loss is 4.728554821014404 and perplexity is 113.13194823710643
At time: 256.08983755111694 and batch: 250, loss is 4.703472509384155 and perplexity is 110.32962869736015
At time: 256.5451340675354 and batch: 300, loss is 4.620587415695191 and perplexity is 101.55366883103909
At time: 257.0005741119385 and batch: 350, loss is 4.677938928604126 and perplexity is 107.54817950476904
At time: 257.45522236824036 and batch: 400, loss is 4.598410625457763 and perplexity is 99.32632339099027
At time: 257.9117205142975 and batch: 450, loss is 4.651379451751709 and perplexity is 104.72935507115352
At time: 258.3675422668457 and batch: 500, loss is 4.6033972644805905 and perplexity is 99.82286491894571
At time: 258.8226852416992 and batch: 550, loss is 4.65368182182312 and perplexity is 104.97075859735632
At time: 259.27898621559143 and batch: 600, loss is 4.681659240722656 and perplexity is 107.94903749638154
At time: 259.7381782531738 and batch: 650, loss is 4.6476505088806155 and perplexity is 104.33955251657929
At time: 260.1956968307495 and batch: 700, loss is 4.6144182491302494 and perplexity is 100.92909586094575
At time: 260.6536066532135 and batch: 750, loss is 4.572716979980469 and perplexity is 96.80677485262899
At time: 261.1104784011841 and batch: 800, loss is 4.6039698028564455 and perplexity is 99.8800337039996
At time: 261.5691363811493 and batch: 850, loss is 4.580828666687012 and perplexity is 97.59523462729311
At time: 262.02775979042053 and batch: 900, loss is 4.679440021514893 and perplexity is 107.70974054331744
At time: 262.48389649391174 and batch: 950, loss is 4.636448659896851 and perplexity is 103.17727856850047
At time: 262.94030570983887 and batch: 1000, loss is 4.616424646377563 and perplexity is 101.13180300858455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0543975830078125 and perplexity of 156.71009707759498
Finished 26 epochs...
Completing Train Step...
At time: 264.37246894836426 and batch: 50, loss is 4.736319665908813 and perplexity is 114.01381963420617
At time: 264.8464915752411 and batch: 100, loss is 4.696544036865235 and perplexity is 109.56785490861787
At time: 265.3053979873657 and batch: 150, loss is 4.718819551467895 and perplexity is 112.0359219387726
At time: 265.7631890773773 and batch: 200, loss is 4.726258554458618 and perplexity is 112.87246516314866
At time: 266.2211842536926 and batch: 250, loss is 4.701663694381714 and perplexity is 110.13024318986987
At time: 266.6810255050659 and batch: 300, loss is 4.618433847427368 and perplexity is 101.33520139906314
At time: 267.13810181617737 and batch: 350, loss is 4.676385278701782 and perplexity is 107.38121702039287
At time: 267.5955538749695 and batch: 400, loss is 4.5972506713867185 and perplexity is 99.21117621344048
At time: 268.053674697876 and batch: 450, loss is 4.6499403381347655 and perplexity is 104.57874602793255
At time: 268.5135889053345 and batch: 500, loss is 4.60228856086731 and perplexity is 99.71225227757024
At time: 268.97138142585754 and batch: 550, loss is 4.653038520812988 and perplexity is 104.90325251800881
At time: 269.42848539352417 and batch: 600, loss is 4.681058921813965 and perplexity is 107.88425309559345
At time: 269.8865809440613 and batch: 650, loss is 4.6469979572296145 and perplexity is 104.27148777961342
At time: 270.34551334381104 and batch: 700, loss is 4.613964691162109 and perplexity is 100.8833290450374
At time: 270.8034174442291 and batch: 750, loss is 4.572350149154663 and perplexity is 96.77126965606462
At time: 271.2627286911011 and batch: 800, loss is 4.603870296478272 and perplexity is 99.8700954980595
At time: 271.720623254776 and batch: 850, loss is 4.581396312713623 and perplexity is 97.65064990108746
At time: 272.1795949935913 and batch: 900, loss is 4.680607624053955 and perplexity is 107.83557615855361
At time: 272.6369514465332 and batch: 950, loss is 4.637865619659424 and perplexity is 103.32358024796194
At time: 273.09522342681885 and batch: 1000, loss is 4.6173265838623045 and perplexity is 101.22305871990451
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.053883622332317 and perplexity of 156.62957494462148
Finished 27 epochs...
Completing Train Step...
At time: 274.51513266563416 and batch: 50, loss is 4.735015325546264 and perplexity is 113.86520375128626
At time: 274.9820656776428 and batch: 100, loss is 4.694933767318726 and perplexity is 109.39156310533653
At time: 275.4364187717438 and batch: 150, loss is 4.717425317764282 and perplexity is 111.87982652242141
At time: 275.8906579017639 and batch: 200, loss is 4.7251659107208255 and perplexity is 112.74920312392979
At time: 276.3443124294281 and batch: 250, loss is 4.700902738571167 and perplexity is 110.0464708189762
At time: 276.81300020217896 and batch: 300, loss is 4.61728720664978 and perplexity is 101.21907291648436
At time: 277.2669620513916 and batch: 350, loss is 4.675471525192261 and perplexity is 107.2831418715635
At time: 277.7209801673889 and batch: 400, loss is 4.596694679260254 and perplexity is 99.15603091220572
At time: 278.17475748062134 and batch: 450, loss is 4.649063663482666 and perplexity is 104.48710466783957
At time: 278.62857389450073 and batch: 500, loss is 4.601623449325562 and perplexity is 99.64595455786007
At time: 279.08265352249146 and batch: 550, loss is 4.652678461074829 and perplexity is 104.86548787954607
At time: 279.5361831188202 and batch: 600, loss is 4.680741319656372 and perplexity is 107.84999426466727
At time: 279.99002504348755 and batch: 650, loss is 4.6466284847259525 and perplexity is 104.23296944813329
At time: 280.4429852962494 and batch: 700, loss is 4.613750400543213 and perplexity is 100.86171301015953
At time: 280.89909076690674 and batch: 750, loss is 4.572112283706665 and perplexity is 96.74825385209533
At time: 281.3531811237335 and batch: 800, loss is 4.603797950744629 and perplexity is 99.86287058408071
At time: 281.80699253082275 and batch: 850, loss is 4.581696729660035 and perplexity is 97.67999021808943
At time: 282.2605128288269 and batch: 900, loss is 4.681363382339478 and perplexity is 107.91710459272376
At time: 282.7161011695862 and batch: 950, loss is 4.6386935329437256 and perplexity is 103.40915863348371
At time: 283.1701991558075 and batch: 1000, loss is 4.6177105140686034 and perplexity is 101.26192877093695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.053587750690739 and perplexity of 156.5832395501656
Finished 28 epochs...
Completing Train Step...
At time: 284.60270071029663 and batch: 50, loss is 4.734053535461426 and perplexity is 113.75574197537355
At time: 285.05745935440063 and batch: 100, loss is 4.693744277954101 and perplexity is 109.26152036200611
At time: 285.512060880661 and batch: 150, loss is 4.71635256767273 and perplexity is 111.75987178051041
At time: 285.9642331600189 and batch: 200, loss is 4.72436861038208 and perplexity is 112.65934397321166
At time: 286.4197952747345 and batch: 250, loss is 4.700374298095703 and perplexity is 109.98833317210925
At time: 286.87172651290894 and batch: 300, loss is 4.616472759246826 and perplexity is 101.13666886685529
At time: 287.325909614563 and batch: 350, loss is 4.674779815673828 and perplexity is 107.20895876070328
At time: 287.7806429862976 and batch: 400, loss is 4.5962694549560545 and perplexity is 99.11387632116744
At time: 288.2530562877655 and batch: 450, loss is 4.648377561569214 and perplexity is 104.41544045268475
At time: 288.7073800563812 and batch: 500, loss is 4.601132364273071 and perplexity is 99.59703193260374
At time: 289.16186022758484 and batch: 550, loss is 4.652427263259888 and perplexity is 104.83914920637508
At time: 289.6168987751007 and batch: 600, loss is 4.68058424949646 and perplexity is 107.83305557913742
At time: 290.0732374191284 and batch: 650, loss is 4.64630051612854 and perplexity is 104.1987899125528
At time: 290.527978181839 and batch: 700, loss is 4.613596477508545 and perplexity is 100.84618926397287
At time: 290.9821171760559 and batch: 750, loss is 4.57188235282898 and perplexity is 96.72601099843004
At time: 291.43885564804077 and batch: 800, loss is 4.603699607849121 and perplexity is 99.85305026312105
At time: 291.8964114189148 and batch: 850, loss is 4.581872663497925 and perplexity is 97.69717694547269
At time: 292.3534734249115 and batch: 900, loss is 4.681909837722778 and perplexity is 107.97609259116545
At time: 292.8090560436249 and batch: 950, loss is 4.639265193939209 and perplexity is 103.46829051613531
At time: 293.2669575214386 and batch: 1000, loss is 4.617928600311279 and perplexity is 101.28401501277389
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0534474442644814 and perplexity of 156.56127145658084
Finished 29 epochs...
Completing Train Step...
At time: 294.6911313533783 and batch: 50, loss is 4.733328714370727 and perplexity is 113.67331928886102
At time: 295.1675043106079 and batch: 100, loss is 4.692761287689209 and perplexity is 109.15417012192755
At time: 295.62619042396545 and batch: 150, loss is 4.715463361740112 and perplexity is 111.66053840995095
At time: 296.0844233036041 and batch: 200, loss is 4.723736391067505 and perplexity is 112.58814107028135
At time: 296.5432198047638 and batch: 250, loss is 4.699886178970337 and perplexity is 109.93465886391468
At time: 297.003470659256 and batch: 300, loss is 4.615805644989013 and perplexity is 101.06922165306689
At time: 297.46221804618835 and batch: 350, loss is 4.6741961574554445 and perplexity is 107.14640362802272
At time: 297.91969656944275 and batch: 400, loss is 4.595867757797241 and perplexity is 99.07407055411726
At time: 298.37802362442017 and batch: 450, loss is 4.647783060073852 and perplexity is 104.35338376542116
At time: 298.83794951438904 and batch: 500, loss is 4.600702934265136 and perplexity is 99.55427116042715
At time: 299.29638385772705 and batch: 550, loss is 4.652209177017212 and perplexity is 104.81628772321737
At time: 299.7553904056549 and batch: 600, loss is 4.680417261123657 and perplexity is 107.81505021603694
At time: 300.22930908203125 and batch: 650, loss is 4.64601167678833 and perplexity is 104.1686975489622
At time: 300.68934416770935 and batch: 700, loss is 4.613438873291016 and perplexity is 100.8302967316211
At time: 301.1478650569916 and batch: 750, loss is 4.571681671142578 and perplexity is 96.7066018070237
At time: 301.60611605644226 and batch: 800, loss is 4.603524036407471 and perplexity is 99.8355204580448
At time: 302.0659556388855 and batch: 850, loss is 4.58197024345398 and perplexity is 97.70671069684968
At time: 302.52669763565063 and batch: 900, loss is 4.682275228500366 and perplexity is 108.01555326844294
At time: 302.98907470703125 and batch: 950, loss is 4.639659376144409 and perplexity is 103.50908391454692
At time: 303.4495017528534 and batch: 1000, loss is 4.618056755065918 and perplexity is 101.29699587262844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.053284807903011 and perplexity of 156.53581097150038
Finished 30 epochs...
Completing Train Step...
At time: 304.8723940849304 and batch: 50, loss is 4.732774295806885 and perplexity is 113.61031415765474
At time: 305.3446931838989 and batch: 100, loss is 4.69184814453125 and perplexity is 109.05454223250354
At time: 305.800803899765 and batch: 150, loss is 4.714655389785767 and perplexity is 111.5703562637371
At time: 306.2566964626312 and batch: 200, loss is 4.723197736740112 and perplexity is 112.52751131158765
At time: 306.71369552612305 and batch: 250, loss is 4.699511756896973 and perplexity is 109.89350460602039
At time: 307.17031145095825 and batch: 300, loss is 4.6152355670928955 and perplexity is 101.01162074388718
At time: 307.6289713382721 and batch: 350, loss is 4.673684158325195 and perplexity is 107.09155880400505
At time: 308.0866868495941 and batch: 400, loss is 4.595495538711548 and perplexity is 99.03720015651824
At time: 308.5420889854431 and batch: 450, loss is 4.647238416671753 and perplexity is 104.29656385816496
At time: 308.99938321113586 and batch: 500, loss is 4.600302515029907 and perplexity is 99.51441569528521
At time: 309.45728635787964 and batch: 550, loss is 4.652010383605957 and perplexity is 104.79545300679666
At time: 309.9139347076416 and batch: 600, loss is 4.680239124298096 and perplexity is 107.79584609577499
At time: 310.37111139297485 and batch: 650, loss is 4.645733213424682 and perplexity is 104.13969442139756
At time: 310.82854986190796 and batch: 700, loss is 4.613246259689331 and perplexity is 100.81087731529054
At time: 311.2873866558075 and batch: 750, loss is 4.571490020751953 and perplexity is 96.68806972490849
At time: 311.7595579624176 and batch: 800, loss is 4.603389415740967 and perplexity is 99.82208143834515
At time: 312.21489906311035 and batch: 850, loss is 4.582032136917114 and perplexity is 97.7127582906975
At time: 312.6726076602936 and batch: 900, loss is 4.6825868606567385 and perplexity is 108.04921963371831
At time: 313.12958121299744 and batch: 950, loss is 4.639965591430664 and perplexity is 103.54078483171278
At time: 313.5866959095001 and batch: 1000, loss is 4.618106555938721 and perplexity is 101.30204067705195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.053207769626525 and perplexity of 156.52375218691475
Finished 31 epochs...
Completing Train Step...
At time: 315.02076864242554 and batch: 50, loss is 4.732235536575318 and perplexity is 113.54912203749103
At time: 315.47627115249634 and batch: 100, loss is 4.691014785766601 and perplexity is 108.96369853186381
At time: 315.93196272850037 and batch: 150, loss is 4.713971710205078 and perplexity is 111.49410395839159
At time: 316.3879716396332 and batch: 200, loss is 4.722683343887329 and perplexity is 112.4696428488653
At time: 316.84507274627686 and batch: 250, loss is 4.699154548645019 and perplexity is 109.85425674958545
At time: 317.30002880096436 and batch: 300, loss is 4.614728794097901 and perplexity is 100.96044375096777
At time: 317.75620436668396 and batch: 350, loss is 4.673209505081177 and perplexity is 107.04073950993724
At time: 318.21122550964355 and batch: 400, loss is 4.595143098831176 and perplexity is 99.0023016477171
At time: 318.66600036621094 and batch: 450, loss is 4.64671516418457 and perplexity is 104.24200469707478
At time: 319.1200017929077 and batch: 500, loss is 4.599924192428589 and perplexity is 99.47677426342193
At time: 319.5743727684021 and batch: 550, loss is 4.65181414604187 and perplexity is 104.77489022003276
At time: 320.0307059288025 and batch: 600, loss is 4.680058841705322 and perplexity is 107.7764141328256
At time: 320.48504400253296 and batch: 650, loss is 4.64545768737793 and perplexity is 104.11100517558239
At time: 320.9391624927521 and batch: 700, loss is 4.613056230545044 and perplexity is 100.79172213061882
At time: 321.39498949050903 and batch: 750, loss is 4.571276750564575 and perplexity is 96.66745124089319
At time: 321.8504717350006 and batch: 800, loss is 4.603249807357788 and perplexity is 99.80814641169586
At time: 322.30453515052795 and batch: 850, loss is 4.582056159973145 and perplexity is 97.71510567796042
At time: 322.75977301597595 and batch: 900, loss is 4.682845373153686 and perplexity is 108.07715531798523
At time: 323.21410870552063 and batch: 950, loss is 4.640200023651123 and perplexity is 103.56506097325268
At time: 323.6871864795685 and batch: 1000, loss is 4.618110456466675 and perplexity is 101.30243580926403
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.053167947908727 and perplexity of 156.5175192663301
Finished 32 epochs...
Completing Train Step...
At time: 325.1058611869812 and batch: 50, loss is 4.731744928359985 and perplexity is 113.49342756857477
At time: 325.57777166366577 and batch: 100, loss is 4.690253667831421 and perplexity is 108.88079585998496
At time: 326.03355383872986 and batch: 150, loss is 4.713320722579956 and perplexity is 111.42154629607322
At time: 326.4892029762268 and batch: 200, loss is 4.7222122478485105 and perplexity is 112.41667132394929
At time: 326.94582080841064 and batch: 250, loss is 4.698824329376221 and perplexity is 109.81798674610378
At time: 327.4019093513489 and batch: 300, loss is 4.6142676734924315 and perplexity is 100.91389954208852
At time: 327.8597159385681 and batch: 350, loss is 4.672766237258911 and perplexity is 106.9933023089101
At time: 328.31561398506165 and batch: 400, loss is 4.594832668304443 and perplexity is 98.97157308085818
At time: 328.7731156349182 and batch: 450, loss is 4.646242551803589 and perplexity is 104.19275027507821
At time: 329.23047971725464 and batch: 500, loss is 4.599591455459595 and perplexity is 99.44368016918781
At time: 329.68681931495667 and batch: 550, loss is 4.6516628074646 and perplexity is 104.75903493700149
At time: 330.1427607536316 and batch: 600, loss is 4.679910936355591 and perplexity is 107.7604746034004
At time: 330.6007158756256 and batch: 650, loss is 4.645175657272339 and perplexity is 104.081646877956
At time: 331.0572316646576 and batch: 700, loss is 4.612941732406616 and perplexity is 100.78018232672162
At time: 331.51343727111816 and batch: 750, loss is 4.571061716079712 and perplexity is 96.64666664009565
At time: 331.969361782074 and batch: 800, loss is 4.603090124130249 and perplexity is 99.79220999716512
At time: 332.4273281097412 and batch: 850, loss is 4.582069625854492 and perplexity is 97.71642150683874
At time: 332.88386034965515 and batch: 900, loss is 4.683040456771851 and perplexity is 108.0982414571989
At time: 333.34076738357544 and batch: 950, loss is 4.640393810272217 and perplexity is 103.58513244121023
At time: 333.79769349098206 and batch: 1000, loss is 4.618100929260254 and perplexity is 101.30147068464458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.053118077720084 and perplexity of 156.5097139027475
Finished 33 epochs...
Completing Train Step...
At time: 335.21455550193787 and batch: 50, loss is 4.731305904388428 and perplexity is 113.44361216913536
At time: 335.68244528770447 and batch: 100, loss is 4.689542942047119 and perplexity is 108.80343896397855
At time: 336.1421136856079 and batch: 150, loss is 4.712753801345825 and perplexity is 111.35839695758008
At time: 336.6264109611511 and batch: 200, loss is 4.7217857456207275 and perplexity is 112.36873558626579
At time: 337.10697078704834 and batch: 250, loss is 4.698512954711914 and perplexity is 109.78379753044989
At time: 337.5790984630585 and batch: 300, loss is 4.61381516456604 and perplexity is 100.86824543197392
At time: 338.07660698890686 and batch: 350, loss is 4.672338161468506 and perplexity is 106.94751086825899
At time: 338.5679032802582 and batch: 400, loss is 4.5945495510101315 and perplexity is 98.94355648305257
At time: 339.0592381954193 and batch: 450, loss is 4.645766820907593 and perplexity is 104.14319435320974
At time: 339.54591703414917 and batch: 500, loss is 4.599242286682129 and perplexity is 99.40896360227977
At time: 340.0193929672241 and batch: 550, loss is 4.651462726593017 and perplexity is 104.73807675472072
At time: 340.48650193214417 and batch: 600, loss is 4.679741926193238 and perplexity is 107.74226352706424
At time: 340.9533190727234 and batch: 650, loss is 4.644874076843262 and perplexity is 104.05026262290795
At time: 341.41248202323914 and batch: 700, loss is 4.612784509658813 and perplexity is 100.76433863505912
At time: 341.87123131752014 and batch: 750, loss is 4.57083800315857 and perplexity is 96.62504795026318
At time: 342.33086490631104 and batch: 800, loss is 4.60289400100708 and perplexity is 99.77264035636476
At time: 342.7899532318115 and batch: 850, loss is 4.582037868499756 and perplexity is 97.71331834105183
At time: 343.25021958351135 and batch: 900, loss is 4.683187274932862 and perplexity is 108.11411340733491
At time: 343.71090745925903 and batch: 950, loss is 4.640526037216187 and perplexity is 103.59883009229299
At time: 344.17335414886475 and batch: 1000, loss is 4.617997055053711 and perplexity is 101.29094862125054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.053075650843178 and perplexity of 156.50307382524105
Finished 34 epochs...
Completing Train Step...
At time: 345.609836101532 and batch: 50, loss is 4.730851545333862 and perplexity is 113.39207974476199
At time: 346.0619602203369 and batch: 100, loss is 4.688910264968872 and perplexity is 108.73462329345571
At time: 346.5159351825714 and batch: 150, loss is 4.712230625152588 and perplexity is 111.3001521328562
At time: 346.9678692817688 and batch: 200, loss is 4.721374616622925 and perplexity is 112.32254703599666
At time: 347.4353184700012 and batch: 250, loss is 4.698232049942017 and perplexity is 109.75296306904254
At time: 347.8870918750763 and batch: 300, loss is 4.613390130996704 and perplexity is 100.82538215139719
At time: 348.3394932746887 and batch: 350, loss is 4.671983957290649 and perplexity is 106.90963632115547
At time: 348.7921164035797 and batch: 400, loss is 4.594318189620972 and perplexity is 98.92066741230195
At time: 349.24477982521057 and batch: 450, loss is 4.645379886627198 and perplexity is 104.10290557630175
At time: 349.69706416130066 and batch: 500, loss is 4.598914794921875 and perplexity is 99.3764133160708
At time: 350.1509835720062 and batch: 550, loss is 4.6512991809844975 and perplexity is 104.72094870286969
At time: 350.6021738052368 and batch: 600, loss is 4.679576740264893 and perplexity is 107.72446749110938
At time: 351.05521392822266 and batch: 650, loss is 4.644594058990479 and perplexity is 104.02113077069646
At time: 351.5069742202759 and batch: 700, loss is 4.612673654556274 and perplexity is 100.75316901308366
At time: 351.9608120918274 and batch: 750, loss is 4.570625085830688 and perplexity is 96.6044769932815
At time: 352.41508746147156 and batch: 800, loss is 4.602692251205444 and perplexity is 99.75251327634969
At time: 352.8689818382263 and batch: 850, loss is 4.582015686035156 and perplexity is 97.71115084286707
At time: 353.3223526477814 and batch: 900, loss is 4.683290367126465 and perplexity is 108.1252597029837
At time: 353.7761228084564 and batch: 950, loss is 4.6406275653839115 and perplexity is 103.6093488256555
At time: 354.2322723865509 and batch: 1000, loss is 4.617931184768676 and perplexity is 101.28427677733399
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.053049227086509 and perplexity of 156.49893848073626
Finished 35 epochs...
Completing Train Step...
At time: 355.6464092731476 and batch: 50, loss is 4.73044680595398 and perplexity is 113.34619479107276
At time: 356.1161878108978 and batch: 100, loss is 4.688302059173584 and perplexity is 108.66851037258898
At time: 356.56986021995544 and batch: 150, loss is 4.711791305541992 and perplexity is 111.25126653234967
At time: 357.02253007888794 and batch: 200, loss is 4.720994882583618 and perplexity is 112.27990243882041
At time: 357.4741327762604 and batch: 250, loss is 4.697965431213379 and perplexity is 109.72370477414283
At time: 357.93043851852417 and batch: 300, loss is 4.6129599285125735 and perplexity is 100.78201615028178
At time: 358.3851673603058 and batch: 350, loss is 4.671598081588745 and perplexity is 106.8683904486015
At time: 358.84080815315247 and batch: 400, loss is 4.594007682800293 and perplexity is 98.88995663856346
At time: 359.31203413009644 and batch: 450, loss is 4.644984617233276 and perplexity is 104.0617650152471
At time: 359.76643991470337 and batch: 500, loss is 4.598577375411987 and perplexity is 99.34288743185712
At time: 360.2205743789673 and batch: 550, loss is 4.651114768981934 and perplexity is 104.70163868356342
At time: 360.6749711036682 and batch: 600, loss is 4.6794070053100585 and perplexity is 107.70618443516595
At time: 361.1292493343353 and batch: 650, loss is 4.644285974502563 and perplexity is 103.98908841002114
At time: 361.58452463150024 and batch: 700, loss is 4.612541303634644 and perplexity is 100.73983512070355
At time: 362.0423629283905 and batch: 750, loss is 4.57047028541565 and perplexity is 96.58952373756333
At time: 362.49922370910645 and batch: 800, loss is 4.602477169036865 and perplexity is 99.7310605966001
At time: 362.9565706253052 and batch: 850, loss is 4.581989059448242 and perplexity is 97.70854916305376
At time: 363.4127769470215 and batch: 900, loss is 4.683383998870849 and perplexity is 108.13538413363837
At time: 363.8694815635681 and batch: 950, loss is 4.640703449249267 and perplexity is 103.61721140184895
At time: 364.3276104927063 and batch: 1000, loss is 4.6178139972686765 and perplexity is 101.27240822158593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.053011638362233 and perplexity of 156.49305599584642
Finished 36 epochs...
Completing Train Step...
At time: 365.7500653266907 and batch: 50, loss is 4.730006093978882 and perplexity is 113.29625277152795
At time: 366.2245144844055 and batch: 100, loss is 4.687752122879028 and perplexity is 108.60876604395293
At time: 366.6837112903595 and batch: 150, loss is 4.711301460266113 and perplexity is 111.19678397010547
At time: 367.1426980495453 and batch: 200, loss is 4.720596075057983 and perplexity is 112.23513329647912
At time: 367.60040521621704 and batch: 250, loss is 4.697693901062012 and perplexity is 109.69391552449973
At time: 368.0581738948822 and batch: 300, loss is 4.612531633377075 and perplexity is 100.73886094526075
At time: 368.5163080692291 and batch: 350, loss is 4.671251983642578 and perplexity is 106.83140991796905
At time: 368.9749984741211 and batch: 400, loss is 4.593716354370117 and perplexity is 98.86115137883503
At time: 369.4344003200531 and batch: 450, loss is 4.6445918369293215 and perplexity is 104.02089962963903
At time: 369.8916873931885 and batch: 500, loss is 4.598318557739258 and perplexity is 99.3171790639633
At time: 370.350643157959 and batch: 550, loss is 4.650937585830689 and perplexity is 104.68308896067865
At time: 370.82500886917114 and batch: 600, loss is 4.679222440719604 and perplexity is 107.6863075216899
At time: 371.28330159187317 and batch: 650, loss is 4.643994855880737 and perplexity is 103.95881965603101
At time: 371.7415235042572 and batch: 700, loss is 4.61241229057312 and perplexity is 100.72683920449673
At time: 372.1994171142578 and batch: 750, loss is 4.570295763015747 and perplexity is 96.57266817295479
At time: 372.65663290023804 and batch: 800, loss is 4.602265548706055 and perplexity is 99.70995770954335
At time: 373.1161890029907 and batch: 850, loss is 4.581976099014282 and perplexity is 97.70728282606119
At time: 373.5760569572449 and batch: 900, loss is 4.683451318740845 and perplexity is 108.14266403867857
At time: 374.03536653518677 and batch: 950, loss is 4.640754776000977 and perplexity is 103.62252987322015
At time: 374.49443912506104 and batch: 1000, loss is 4.617725391387939 and perplexity is 101.26343528819426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.052975538300305 and perplexity of 156.4874066888044
Finished 37 epochs...
Completing Train Step...
At time: 375.9294579029083 and batch: 50, loss is 4.729576683044433 and perplexity is 113.2476125658213
At time: 376.38536643981934 and batch: 100, loss is 4.6871145153045655 and perplexity is 108.53953834446834
At time: 376.84106612205505 and batch: 150, loss is 4.7108540439605715 and perplexity is 111.14704384393498
At time: 377.29605889320374 and batch: 200, loss is 4.720247678756714 and perplexity is 112.1960378019244
At time: 377.7519052028656 and batch: 250, loss is 4.697374248504639 and perplexity is 109.65885718741616
At time: 378.2073743343353 and batch: 300, loss is 4.6120563316345216 and perplexity is 100.69099096635402
At time: 378.663937330246 and batch: 350, loss is 4.670897254943847 and perplexity is 106.79352047157963
At time: 379.1193926334381 and batch: 400, loss is 4.593418712615967 and perplexity is 98.83173055097195
At time: 379.5741927623749 and batch: 450, loss is 4.644180192947387 and perplexity is 103.9780888643126
At time: 380.0312864780426 and batch: 500, loss is 4.598055658340454 and perplexity is 99.2910720692034
At time: 380.48892045021057 and batch: 550, loss is 4.650741624832153 and perplexity is 104.66257716785746
At time: 380.94526195526123 and batch: 600, loss is 4.679024667739868 and perplexity is 107.66501218566499
At time: 381.4022924900055 and batch: 650, loss is 4.643766260147094 and perplexity is 103.93505782941268
At time: 381.85781836509705 and batch: 700, loss is 4.612198400497436 and perplexity is 100.70529703714593
At time: 382.33067202568054 and batch: 750, loss is 4.570059232711792 and perplexity is 96.54982851164054
At time: 382.78711199760437 and batch: 800, loss is 4.602120952606201 and perplexity is 99.69554108086123
At time: 383.2441737651825 and batch: 850, loss is 4.581892461776733 and perplexity is 97.69911120056805
At time: 383.7024030685425 and batch: 900, loss is 4.683513107299805 and perplexity is 108.14934622449081
At time: 384.160281419754 and batch: 950, loss is 4.640772304534912 and perplexity is 103.62434624017058
At time: 384.61869955062866 and batch: 1000, loss is 4.617597246170044 and perplexity is 101.25045969461237
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.052970700147675 and perplexity of 156.48664958067775
Finished 38 epochs...
Completing Train Step...
At time: 386.04098677635193 and batch: 50, loss is 4.729161853790283 and perplexity is 113.20064388583114
At time: 386.51253390312195 and batch: 100, loss is 4.6865987300872805 and perplexity is 108.48356969024236
At time: 386.96897625923157 and batch: 150, loss is 4.710435304641724 and perplexity is 111.10051194955062
At time: 387.42474842071533 and batch: 200, loss is 4.719931631088257 and perplexity is 112.16058410859071
At time: 387.8815875053406 and batch: 250, loss is 4.697092924118042 and perplexity is 109.62801181566408
At time: 388.3379416465759 and batch: 300, loss is 4.611709117889404 and perplexity is 100.65603573909989
At time: 388.79365038871765 and batch: 350, loss is 4.670591535568238 and perplexity is 106.76087661336612
At time: 389.2489366531372 and batch: 400, loss is 4.59313117980957 and perplexity is 98.8033172711964
At time: 389.7047231197357 and batch: 450, loss is 4.64379189491272 and perplexity is 103.93772221441074
At time: 390.16114497184753 and batch: 500, loss is 4.597788963317871 and perplexity is 99.26459516528183
At time: 390.61671113967896 and batch: 550, loss is 4.650599966049194 and perplexity is 104.64775184464793
At time: 391.07181191444397 and batch: 600, loss is 4.6788653469085695 and perplexity is 107.6478602727866
At time: 391.5277693271637 and batch: 650, loss is 4.643467760086059 and perplexity is 103.90403783827186
At time: 391.983589887619 and batch: 700, loss is 4.612030448913575 and perplexity is 100.6883848432599
At time: 392.43965435028076 and batch: 750, loss is 4.569873094558716 and perplexity is 96.53185857737867
At time: 392.89510917663574 and batch: 800, loss is 4.601951780319214 and perplexity is 99.67867678470017
At time: 393.35149574279785 and batch: 850, loss is 4.581767482757568 and perplexity is 97.68690162446325
At time: 393.80799627304077 and batch: 900, loss is 4.683538961410522 and perplexity is 108.1521423658078
At time: 394.28048181533813 and batch: 950, loss is 4.640788354873657 and perplexity is 103.62600945937757
At time: 394.7377495765686 and batch: 1000, loss is 4.617445955276489 and perplexity is 101.23514258079143
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0529844702743905 and perplexity of 156.488804436508
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 396.1548981666565 and batch: 50, loss is 4.729136238098144 and perplexity is 113.19774421012619
At time: 396.62619400024414 and batch: 100, loss is 4.686191730499267 and perplexity is 108.43942590593261
At time: 397.08183455467224 and batch: 150, loss is 4.709244432449341 and perplexity is 110.96828418811937
At time: 397.5376479625702 and batch: 200, loss is 4.71936297416687 and perplexity is 112.09682134741654
At time: 397.9951870441437 and batch: 250, loss is 4.695756940841675 and perplexity is 109.48164841655345
At time: 398.45160961151123 and batch: 300, loss is 4.610937604904175 and perplexity is 100.57840824964403
At time: 398.90707302093506 and batch: 350, loss is 4.668838710784912 and perplexity is 106.57390741294299
At time: 399.3635997772217 and batch: 400, loss is 4.591156663894654 and perplexity is 98.60842102498819
At time: 399.8192238807678 and batch: 450, loss is 4.641537227630615 and perplexity is 103.70364121930095
At time: 400.2753734588623 and batch: 500, loss is 4.595723447799682 and perplexity is 99.05977420681518
At time: 400.73189663887024 and batch: 550, loss is 4.648470191955567 and perplexity is 104.42511294325139
At time: 401.1890573501587 and batch: 600, loss is 4.675975227355957 and perplexity is 107.33719423425552
At time: 401.6453549861908 and batch: 650, loss is 4.639778566360474 and perplexity is 103.52142191989353
At time: 402.101909160614 and batch: 700, loss is 4.609109096527099 and perplexity is 100.39466782425397
At time: 402.5576114654541 and batch: 750, loss is 4.566761932373047 and perplexity is 96.2319990070085
At time: 403.0139491558075 and batch: 800, loss is 4.598016719818116 and perplexity is 99.28720589684767
At time: 403.4683735370636 and batch: 850, loss is 4.5772905349731445 and perplexity is 97.25053997965287
At time: 403.92569375038147 and batch: 900, loss is 4.679275875091553 and perplexity is 107.69206182563721
At time: 404.38400983810425 and batch: 950, loss is 4.636286106109619 and perplexity is 103.16050807420352
At time: 404.842161655426 and batch: 1000, loss is 4.613285760879517 and perplexity is 100.81485954357908
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.051965108731898 and perplexity of 156.32936704341424
Finished 40 epochs...
Completing Train Step...
At time: 406.2930030822754 and batch: 50, loss is 4.7282228183746335 and perplexity is 113.09439436598646
At time: 406.74922370910645 and batch: 100, loss is 4.685887546539306 and perplexity is 108.4064453882712
At time: 407.2066469192505 and batch: 150, loss is 4.7090965938568115 and perplexity is 110.95188000578504
At time: 407.6623568534851 and batch: 200, loss is 4.718844900131225 and perplexity is 112.03876193563363
At time: 408.118289232254 and batch: 250, loss is 4.695243291854858 and perplexity is 109.42542771885753
At time: 408.5778052806854 and batch: 300, loss is 4.610594539642334 and perplexity is 100.54390920973181
At time: 409.0402705669403 and batch: 350, loss is 4.668513317108154 and perplexity is 106.53923457882985
At time: 409.4965035915375 and batch: 400, loss is 4.590736694335938 and perplexity is 98.56701718470946
At time: 409.95330691337585 and batch: 450, loss is 4.641317224502563 and perplexity is 103.68082860335775
At time: 410.4099979400635 and batch: 500, loss is 4.59561936378479 and perplexity is 99.04946420436394
At time: 410.86684823036194 and batch: 550, loss is 4.648396825790405 and perplexity is 104.41745195420029
At time: 411.3238627910614 and batch: 600, loss is 4.675905809402466 and perplexity is 107.32974336451335
At time: 411.7815101146698 and batch: 650, loss is 4.639683485031128 and perplexity is 103.51157943340738
At time: 412.2387270927429 and batch: 700, loss is 4.608935995101929 and perplexity is 100.37729086820536
At time: 412.6966133117676 and batch: 750, loss is 4.566766939163208 and perplexity is 96.23248082164048
At time: 413.15480852127075 and batch: 800, loss is 4.598037805557251 and perplexity is 99.28929946304278
At time: 413.61223244667053 and batch: 850, loss is 4.57727952003479 and perplexity is 97.24946877684971
At time: 414.0708074569702 and batch: 900, loss is 4.679392614364624 and perplexity is 107.70463445249568
At time: 414.52950644493103 and batch: 950, loss is 4.636447067260742 and perplexity is 103.17711424477191
At time: 414.9862494468689 and batch: 1000, loss is 4.6134654140472415 and perplexity is 100.83297287946016
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.051848248737614 and perplexity of 156.31109946187064
Finished 41 epochs...
Completing Train Step...
At time: 416.41041135787964 and batch: 50, loss is 4.727966785430908 and perplexity is 113.06544218179273
At time: 416.8798701763153 and batch: 100, loss is 4.685765075683594 and perplexity is 108.39316957110664
At time: 417.33286237716675 and batch: 150, loss is 4.708842573165893 and perplexity is 110.9236995119331
At time: 417.80215859413147 and batch: 200, loss is 4.718611631393433 and perplexity is 112.01262984307166
At time: 418.25565934181213 and batch: 250, loss is 4.694900588989258 and perplexity is 109.38793373622336
At time: 418.708865404129 and batch: 300, loss is 4.610345497131347 and perplexity is 100.5188726198348
At time: 419.1627516746521 and batch: 350, loss is 4.668326454162598 and perplexity is 106.51932820357904
At time: 419.6163282394409 and batch: 400, loss is 4.590582494735718 and perplexity is 98.55181936184391
At time: 420.07121658325195 and batch: 450, loss is 4.641206932067871 and perplexity is 103.66939402292567
At time: 420.52478528022766 and batch: 500, loss is 4.5955651664733885 and perplexity is 99.04409613517709
At time: 420.9781811237335 and batch: 550, loss is 4.648383646011353 and perplexity is 104.41607576432331
At time: 421.43283438682556 and batch: 600, loss is 4.675888662338257 and perplexity is 107.32790299029085
At time: 421.88675451278687 and batch: 650, loss is 4.6396016597747805 and perplexity is 103.50310991840016
At time: 422.340548992157 and batch: 700, loss is 4.6088732242584225 and perplexity is 100.37099029873681
At time: 422.79483819007874 and batch: 750, loss is 4.566818504333496 and perplexity is 96.237443193843
At time: 423.2491760253906 and batch: 800, loss is 4.598050174713134 and perplexity is 99.29052759546087
At time: 423.7026822566986 and batch: 850, loss is 4.577267837524414 and perplexity is 97.248332665558
At time: 424.1567268371582 and batch: 900, loss is 4.67946491241455 and perplexity is 107.71242156902791
At time: 424.61069560050964 and batch: 950, loss is 4.6365235328674315 and perplexity is 103.18500404705449
At time: 425.06452441215515 and batch: 1000, loss is 4.613578157424927 and perplexity is 100.84434177027617
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.051781258931974 and perplexity of 156.30062856242392
Finished 42 epochs...
Completing Train Step...
At time: 426.4847435951233 and batch: 50, loss is 4.727801237106323 and perplexity is 113.04672593653537
At time: 426.95568203926086 and batch: 100, loss is 4.685678453445434 and perplexity is 108.38378071880479
At time: 427.41072154045105 and batch: 150, loss is 4.708610496520996 and perplexity is 110.89795969883104
At time: 427.8658514022827 and batch: 200, loss is 4.71842695236206 and perplexity is 111.99194536914386
At time: 428.3217821121216 and batch: 250, loss is 4.694623155593872 and perplexity is 109.35759007971912
At time: 428.7775094509125 and batch: 300, loss is 4.610145969390869 and perplexity is 100.49881831706692
At time: 429.2330734729767 and batch: 350, loss is 4.668177127838135 and perplexity is 106.50342325135772
At time: 429.703941822052 and batch: 400, loss is 4.590466766357422 and perplexity is 98.54041477954073
At time: 430.16018867492676 and batch: 450, loss is 4.641113004684448 and perplexity is 103.65965708529373
At time: 430.61614513397217 and batch: 500, loss is 4.595523700714112 and perplexity is 99.03998928167644
At time: 431.07183051109314 and batch: 550, loss is 4.6483770275115965 and perplexity is 104.41538468883826
At time: 431.5269994735718 and batch: 600, loss is 4.675871152877807 and perplexity is 107.32602375307057
At time: 431.9826862812042 and batch: 650, loss is 4.639530649185181 and perplexity is 103.4957603624907
At time: 432.44037890434265 and batch: 700, loss is 4.608846206665039 and perplexity is 100.36827855276597
At time: 432.9121367931366 and batch: 750, loss is 4.566867113113403 and perplexity is 96.24212129223544
At time: 433.3748893737793 and batch: 800, loss is 4.598051261901856 and perplexity is 99.29063554306128
At time: 433.82605171203613 and batch: 850, loss is 4.577240905761719 and perplexity is 97.24571363180786
At time: 434.2791061401367 and batch: 900, loss is 4.679511709213257 and perplexity is 107.71746228348198
At time: 434.7424178123474 and batch: 950, loss is 4.636567621231079 and perplexity is 103.18955340532207
At time: 435.2059164047241 and batch: 1000, loss is 4.613654317855835 and perplexity is 100.85202241127683
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.051732505240092 and perplexity of 156.29300851549226
Finished 43 epochs...
Completing Train Step...
At time: 436.6406035423279 and batch: 50, loss is 4.727671175003052 and perplexity is 113.03202379770839
At time: 437.10000133514404 and batch: 100, loss is 4.685613231658936 and perplexity is 108.37671196551969
At time: 437.55785751342773 and batch: 150, loss is 4.708401393890381 and perplexity is 110.87477306800541
At time: 438.0154995918274 and batch: 200, loss is 4.718273029327393 and perplexity is 111.97470855565969
At time: 438.4742395877838 and batch: 250, loss is 4.6943882465362545 and perplexity is 109.3319040083537
At time: 438.9312119483948 and batch: 300, loss is 4.609974365234375 and perplexity is 100.4815737817803
At time: 439.39016914367676 and batch: 350, loss is 4.6680506992340085 and perplexity is 106.48995902337109
At time: 439.8483738899231 and batch: 400, loss is 4.590358667373657 and perplexity is 98.52976323656411
At time: 440.30806255340576 and batch: 450, loss is 4.641034088134766 and perplexity is 103.65147694559373
At time: 440.7664682865143 and batch: 500, loss is 4.5954875659942624 and perplexity is 99.03641056406825
At time: 441.23975348472595 and batch: 550, loss is 4.648372554779053 and perplexity is 104.41491766779356
At time: 441.69810795783997 and batch: 600, loss is 4.675851697921753 and perplexity is 107.32393575030605
At time: 442.15644931793213 and batch: 650, loss is 4.639467258453369 and perplexity is 103.48919989844036
At time: 442.61521649360657 and batch: 700, loss is 4.608827838897705 and perplexity is 100.36643502850859
At time: 443.07432079315186 and batch: 750, loss is 4.566914176940918 and perplexity is 96.24665092142155
At time: 443.5340645313263 and batch: 800, loss is 4.598045063018799 and perplexity is 99.29002005393062
At time: 443.9928193092346 and batch: 850, loss is 4.577204322814941 and perplexity is 97.24215616211347
At time: 444.45144486427307 and batch: 900, loss is 4.6795369911193845 and perplexity is 107.72018562067717
At time: 444.9087505340576 and batch: 950, loss is 4.636595687866211 and perplexity is 103.19244962951034
At time: 445.36678743362427 and batch: 1000, loss is 4.61370735168457 and perplexity is 100.857371121991
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.051694916515816 and perplexity of 156.28713377110162
Finished 44 epochs...
Completing Train Step...
At time: 446.7893273830414 and batch: 50, loss is 4.727561779022217 and perplexity is 113.0196592249289
At time: 447.2607114315033 and batch: 100, loss is 4.685558137893676 and perplexity is 108.37074124886723
At time: 447.71563148498535 and batch: 150, loss is 4.708208866119385 and perplexity is 110.8534286498495
At time: 448.173095703125 and batch: 200, loss is 4.718139219284057 and perplexity is 111.9597262174714
At time: 448.6306266784668 and batch: 250, loss is 4.694184818267822 and perplexity is 109.30966507052788
At time: 449.0876843929291 and batch: 300, loss is 4.609821243286133 and perplexity is 100.46618902534235
At time: 449.54458713531494 and batch: 350, loss is 4.667938060760498 and perplexity is 106.47796483245888
At time: 450.0009834766388 and batch: 400, loss is 4.590264768600464 and perplexity is 98.52051184702702
At time: 450.4589455127716 and batch: 450, loss is 4.64096378326416 and perplexity is 103.64418999807597
At time: 450.91555190086365 and batch: 500, loss is 4.595453701019287 and perplexity is 99.03305675529148
At time: 451.37268710136414 and batch: 550, loss is 4.648364496231079 and perplexity is 104.4140762385607
At time: 451.8293831348419 and batch: 600, loss is 4.675831470489502 and perplexity is 107.32176488462235
At time: 452.2860207557678 and batch: 650, loss is 4.639406347274781 and perplexity is 103.48289644128086
At time: 452.742956161499 and batch: 700, loss is 4.608810806274414 and perplexity is 100.36472553938833
At time: 453.2167658805847 and batch: 750, loss is 4.56695743560791 and perplexity is 96.25081451329797
At time: 453.67337369918823 and batch: 800, loss is 4.598031454086303 and perplexity is 99.28866883194458
At time: 454.130126953125 and batch: 850, loss is 4.577160873413086 and perplexity is 97.23793114038112
At time: 454.58689761161804 and batch: 900, loss is 4.679549674987793 and perplexity is 107.72155193800165
At time: 455.04341340065 and batch: 950, loss is 4.636612787246704 and perplexity is 103.19421417155678
At time: 455.4998528957367 and batch: 1000, loss is 4.613744335174561 and perplexity is 100.86110124854248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0516603051162345 and perplexity of 156.28172454827623
Finished 45 epochs...
Completing Train Step...
At time: 456.91781973838806 and batch: 50, loss is 4.727464923858642 and perplexity is 113.00871321744474
At time: 457.38852190971375 and batch: 100, loss is 4.685509824752808 and perplexity is 108.36550564445453
At time: 457.8428647518158 and batch: 150, loss is 4.708031826019287 and perplexity is 110.83380488489301
At time: 458.29840326309204 and batch: 200, loss is 4.7180196475982665 and perplexity is 111.94633980460081
At time: 458.7515208721161 and batch: 250, loss is 4.694001512527466 and perplexity is 109.28962981778854
At time: 459.20739340782166 and batch: 300, loss is 4.609682264328003 and perplexity is 100.45222730927921
At time: 459.6623980998993 and batch: 350, loss is 4.667835006713867 and perplexity is 106.46699241269167
At time: 460.11888790130615 and batch: 400, loss is 4.590180406570434 and perplexity is 98.51220080722112
At time: 460.576043844223 and batch: 450, loss is 4.640897035598755 and perplexity is 103.63727222123606
At time: 461.03365087509155 and batch: 500, loss is 4.595420198440552 and perplexity is 99.02973894808801
At time: 461.4905264377594 and batch: 550, loss is 4.648353757858277 and perplexity is 104.41295500730433
At time: 461.9487524032593 and batch: 600, loss is 4.675809898376465 and perplexity is 107.31944975235014
At time: 462.4066982269287 and batch: 650, loss is 4.639346370697021 and perplexity is 103.47669007741578
At time: 462.864474773407 and batch: 700, loss is 4.608793678283692 and perplexity is 100.36300650802222
At time: 463.3230607509613 and batch: 750, loss is 4.5669969463348385 and perplexity is 96.25461752807628
At time: 463.780885219574 and batch: 800, loss is 4.59801362991333 and perplexity is 99.28689910930902
At time: 464.24289441108704 and batch: 850, loss is 4.5771133327484135 and perplexity is 97.23330849438604
At time: 464.71956729888916 and batch: 900, loss is 4.67955605506897 and perplexity is 107.72223921243987
At time: 465.18154287338257 and batch: 950, loss is 4.636622619628906 and perplexity is 103.19522882149982
At time: 465.64255475997925 and batch: 1000, loss is 4.6137717628479 and perplexity is 100.8638676718183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.051632020531631 and perplexity of 156.27730424712976
Finished 46 epochs...
Completing Train Step...
At time: 467.0827236175537 and batch: 50, loss is 4.72737790107727 and perplexity is 112.99887931279436
At time: 467.5389745235443 and batch: 100, loss is 4.68546615600586 and perplexity is 108.36077356193343
At time: 467.99429202079773 and batch: 150, loss is 4.707868080139161 and perplexity is 110.81565779176081
At time: 468.4495921134949 and batch: 200, loss is 4.717910375595093 and perplexity is 111.93410787211843
At time: 468.904559135437 and batch: 250, loss is 4.693830680847168 and perplexity is 109.27096128132177
At time: 469.3601641654968 and batch: 300, loss is 4.609556808471679 and perplexity is 100.43962577956685
At time: 469.8160469532013 and batch: 350, loss is 4.6677396106719975 and perplexity is 106.45683636745672
At time: 470.27226066589355 and batch: 400, loss is 4.5900969886779786 and perplexity is 98.50398346978984
At time: 470.7279167175293 and batch: 450, loss is 4.64083402633667 and perplexity is 103.63074231891319
At time: 471.1837296485901 and batch: 500, loss is 4.59538800239563 and perplexity is 99.02655063349009
At time: 471.6400520801544 and batch: 550, loss is 4.648341007232666 and perplexity is 104.41162368529376
At time: 472.0966143608093 and batch: 600, loss is 4.675788335800171 and perplexity is 107.31713569347562
At time: 472.55213046073914 and batch: 650, loss is 4.639287633895874 and perplexity is 103.47061236614172
At time: 473.0081694126129 and batch: 700, loss is 4.608776540756225 and perplexity is 100.36128654897955
At time: 473.4621615409851 and batch: 750, loss is 4.56703293800354 and perplexity is 96.25808195472621
At time: 473.91667103767395 and batch: 800, loss is 4.597993688583374 and perplexity is 99.28491921623446
At time: 474.3731052875519 and batch: 850, loss is 4.577062997817993 and perplexity is 97.22841438574179
At time: 474.8286955356598 and batch: 900, loss is 4.679559602737426 and perplexity is 107.72262137590792
At time: 475.28415274620056 and batch: 950, loss is 4.636624851226807 and perplexity is 103.19545911201274
At time: 475.7403841018677 and batch: 1000, loss is 4.613794813156128 and perplexity is 100.86619264185269
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.051604108112614 and perplexity of 156.2729422304083
Finished 47 epochs...
Completing Train Step...
At time: 477.1602346897125 and batch: 50, loss is 4.72729829788208 and perplexity is 112.98988459895688
At time: 477.63665080070496 and batch: 100, loss is 4.685426378250122 and perplexity is 108.35646329927803
At time: 478.09282779693604 and batch: 150, loss is 4.707711410522461 and perplexity is 110.79829770506485
At time: 478.5469777584076 and batch: 200, loss is 4.717807111740112 and perplexity is 111.9225497214156
At time: 479.0024428367615 and batch: 250, loss is 4.693668069839478 and perplexity is 109.25319406480818
At time: 479.457480430603 and batch: 300, loss is 4.609443264007568 and perplexity is 100.42822206350888
At time: 479.9124562740326 and batch: 350, loss is 4.667645692825317 and perplexity is 106.44683864011057
At time: 480.36793875694275 and batch: 400, loss is 4.590016527175903 and perplexity is 98.49605801017093
At time: 480.8222460746765 and batch: 450, loss is 4.640771780014038 and perplexity is 103.6242918870522
At time: 481.2769808769226 and batch: 500, loss is 4.59535626411438 and perplexity is 99.02340775084996
At time: 481.73323607444763 and batch: 550, loss is 4.648327322006225 and perplexity is 104.4101947983579
At time: 482.19052362442017 and batch: 600, loss is 4.67576602935791 and perplexity is 107.31474185668381
At time: 482.64843344688416 and batch: 650, loss is 4.639229183197021 and perplexity is 103.46456461328764
At time: 483.1059069633484 and batch: 700, loss is 4.608759050369263 and perplexity is 100.35953120659258
At time: 483.5635120868683 and batch: 750, loss is 4.56706503868103 and perplexity is 96.26117195396616
At time: 484.02075028419495 and batch: 800, loss is 4.597970380783081 and perplexity is 99.28260513013346
At time: 484.4790949821472 and batch: 850, loss is 4.577011022567749 and perplexity is 97.2233610458987
At time: 484.93819093704224 and batch: 900, loss is 4.679561710357666 and perplexity is 107.72284841452421
At time: 485.3985335826874 and batch: 950, loss is 4.636621122360229 and perplexity is 103.19507431063177
At time: 485.85964465141296 and batch: 1000, loss is 4.6138127899169925 and perplexity is 100.8680059055754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.051581406011814 and perplexity of 156.2693945465915
Finished 48 epochs...
Completing Train Step...
At time: 487.29392409324646 and batch: 50, loss is 4.727224245071411 and perplexity is 112.9815176902255
At time: 487.7711443901062 and batch: 100, loss is 4.6853896999359135 and perplexity is 108.3524890397556
At time: 488.226585149765 and batch: 150, loss is 4.70756157875061 and perplexity is 110.78169784342603
At time: 488.6979806423187 and batch: 200, loss is 4.717711296081543 and perplexity is 111.91182630234924
At time: 489.15442538261414 and batch: 250, loss is 4.693514623641968 and perplexity is 109.23643086377072
At time: 489.6099419593811 and batch: 300, loss is 4.609334049224853 and perplexity is 100.41725441598325
At time: 490.06568121910095 and batch: 350, loss is 4.667555160522461 and perplexity is 106.4372021988879
At time: 490.52268862724304 and batch: 400, loss is 4.589937295913696 and perplexity is 98.48825435232328
At time: 490.97869062423706 and batch: 450, loss is 4.6407107067108155 and perplexity is 103.61796340250524
At time: 491.4354112148285 and batch: 500, loss is 4.5953256702423095 and perplexity is 99.02037828772299
At time: 491.892085313797 and batch: 550, loss is 4.64831434249878 and perplexity is 104.40883961425195
At time: 492.3496825695038 and batch: 600, loss is 4.675743350982666 and perplexity is 107.31230816029498
At time: 492.8069660663605 and batch: 650, loss is 4.6391703891754155 and perplexity is 103.4584816942617
At time: 493.2650876045227 and batch: 700, loss is 4.608743495941162 and perplexity is 100.3579701836207
At time: 493.7210273742676 and batch: 750, loss is 4.567092094421387 and perplexity is 96.2637764064735
At time: 494.1781265735626 and batch: 800, loss is 4.597944478988648 and perplexity is 99.28003356580884
At time: 494.63469886779785 and batch: 850, loss is 4.576958265304565 and perplexity is 97.21823194275228
At time: 495.09194564819336 and batch: 900, loss is 4.679563236236572 and perplexity is 107.72301278667179
At time: 495.5478422641754 and batch: 950, loss is 4.6366119480133055 and perplexity is 103.19412756756208
At time: 496.00448870658875 and batch: 1000, loss is 4.613825197219849 and perplexity is 100.86925741323708
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.051548655440167 and perplexity of 156.2642767183956
Finished 49 epochs...
Completing Train Step...
At time: 497.4393937587738 and batch: 50, loss is 4.72715500831604 and perplexity is 112.97369548731886
At time: 497.8936972618103 and batch: 100, loss is 4.68534589767456 and perplexity is 108.3477430596555
At time: 498.34757447242737 and batch: 150, loss is 4.70738299369812 and perplexity is 110.76191565455592
At time: 498.80049109458923 and batch: 200, loss is 4.717597284317017 and perplexity is 111.89906776488667
At time: 499.253520488739 and batch: 250, loss is 4.693357543945313 and perplexity is 109.21927338592793
At time: 499.7071132659912 and batch: 300, loss is 4.609219150543213 and perplexity is 100.40571726865113
At time: 500.1771583557129 and batch: 350, loss is 4.667464075088501 and perplexity is 106.42750776165379
At time: 500.6301290988922 and batch: 400, loss is 4.589842128753662 and perplexity is 98.47888195083932
At time: 501.08409667015076 and batch: 450, loss is 4.6406122589111325 and perplexity is 103.6077629441152
At time: 501.5372235774994 and batch: 500, loss is 4.595295171737671 and perplexity is 99.01735836030838
At time: 501.9905982017517 and batch: 550, loss is 4.648264169692993 and perplexity is 104.40360126123211
At time: 502.44509053230286 and batch: 600, loss is 4.67568868637085 and perplexity is 107.3064421349598
At time: 502.89844512939453 and batch: 650, loss is 4.63906662940979 and perplexity is 103.44774742335146
At time: 503.3510961532593 and batch: 700, loss is 4.608728895187378 and perplexity is 100.35650489230493
At time: 503.8049099445343 and batch: 750, loss is 4.567092990875244 and perplexity is 96.26386270254581
At time: 504.2589364051819 and batch: 800, loss is 4.597905235290527 and perplexity is 99.27613752659018
At time: 504.71490120887756 and batch: 850, loss is 4.576901912689209 and perplexity is 97.21275359548304
At time: 505.17528772354126 and batch: 900, loss is 4.679564037322998 and perplexity is 107.72309908214963
At time: 505.636953830719 and batch: 950, loss is 4.636582279205323 and perplexity is 103.19106596622358
At time: 506.099906206131 and batch: 1000, loss is 4.613829441070557 and perplexity is 100.86968548821493
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.051525581173781 and perplexity of 156.26067107644684
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f75e40c6898>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.2069436965219552, 'anneal': 7.408443099136203, 'tune_wordvecs': True, 'lr': 24.790210893573033, 'wordvec_dim': 200, 'seq_len': 20, 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7374749183654785 and batch: 50, loss is 6.377518911361694 and perplexity is 588.4658589635733
At time: 1.2091829776763916 and batch: 100, loss is 5.647650566101074 and perplexity is 283.62432582447923
At time: 1.6650261878967285 and batch: 150, loss is 5.558217868804932 and perplexity is 259.36021029372915
At time: 2.1219325065612793 and batch: 200, loss is 5.504560632705688 and perplexity is 245.81043088534747
At time: 2.57918643951416 and batch: 250, loss is 5.4738197517395015 and perplexity is 238.36896628519824
At time: 3.036341428756714 and batch: 300, loss is 5.342741365432739 and perplexity is 209.08510403712143
At time: 3.4937915802001953 and batch: 350, loss is 5.384318437576294 and perplexity is 217.96149910382522
At time: 3.9498634338378906 and batch: 400, loss is 5.320251111984253 and perplexity is 204.43521167960458
At time: 4.40667200088501 and batch: 450, loss is 5.3165069007873536 and perplexity is 203.67119428480603
At time: 4.863279581069946 and batch: 500, loss is 5.3188252449035645 and perplexity is 204.14392196069932
At time: 5.320267677307129 and batch: 550, loss is 5.352501010894775 and perplexity is 211.13569074693274
At time: 5.79218316078186 and batch: 600, loss is 5.350863208770752 and perplexity is 210.7901752843837
At time: 6.248263120651245 and batch: 650, loss is 5.33539644241333 and perplexity is 207.55501611122355
At time: 6.705060005187988 and batch: 700, loss is 5.307764520645142 and perplexity is 201.8983838628584
At time: 7.162134885787964 and batch: 750, loss is 5.239513511657715 and perplexity is 188.57833892304996
At time: 7.619657516479492 and batch: 800, loss is 5.288475131988525 and perplexity is 198.0412083438161
At time: 8.075748443603516 and batch: 850, loss is 5.254261274337768 and perplexity is 191.3800562631736
At time: 8.533171892166138 and batch: 900, loss is 5.372816114425659 and perplexity is 215.46879891097737
At time: 8.990610837936401 and batch: 950, loss is 5.300224113464355 and perplexity is 200.38171318283185
At time: 9.448519706726074 and batch: 1000, loss is 5.274614400863648 and perplexity is 195.31514863767129
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.27472407643388 and perplexity of 195.33657111271265
Finished 1 epochs...
Completing Train Step...
At time: 10.874019384384155 and batch: 50, loss is 5.254959011077881 and perplexity is 191.513635755994
At time: 11.32059359550476 and batch: 100, loss is 5.245132808685303 and perplexity is 189.64099952896274
At time: 11.766551494598389 and batch: 150, loss is 5.26134651184082 and perplexity is 192.74084447629338
At time: 12.209981203079224 and batch: 200, loss is 5.2624605274200436 and perplexity is 192.95568042287024
At time: 12.653045177459717 and batch: 250, loss is 5.228520288467407 and perplexity is 186.5166084495047
At time: 13.09830117225647 and batch: 300, loss is 5.117883720397949 and perplexity is 166.98161564486338
At time: 13.553177833557129 and batch: 350, loss is 5.175577898025512 and perplexity is 176.89881421846803
At time: 14.024386405944824 and batch: 400, loss is 5.133162717819214 and perplexity is 169.55251770861793
At time: 14.485473871231079 and batch: 450, loss is 5.163819732666016 and perplexity is 174.8309894455725
At time: 14.950981140136719 and batch: 500, loss is 5.172161378860474 and perplexity is 176.2954672894767
At time: 15.427152156829834 and batch: 550, loss is 5.1965568733215335 and perplexity is 180.64917185314053
At time: 15.90355372428894 and batch: 600, loss is 5.218911600112915 and perplexity is 184.73301122373408
At time: 16.379650354385376 and batch: 650, loss is 5.210803842544555 and perplexity is 183.24129615749098
At time: 16.84493851661682 and batch: 700, loss is 5.165673112869262 and perplexity is 175.15531819973145
At time: 17.300599813461304 and batch: 750, loss is 5.1195388031005855 and perplexity is 167.25821286106932
At time: 17.77189564704895 and batch: 800, loss is 5.195406560897827 and perplexity is 180.44148833978926
At time: 18.218263149261475 and batch: 850, loss is 5.156775217056275 and perplexity is 173.60371765429866
At time: 18.83077335357666 and batch: 900, loss is 5.27663296699524 and perplexity is 195.709803366027
At time: 19.290237188339233 and batch: 950, loss is 5.215876951217651 and perplexity is 184.17326114734647
At time: 19.758201837539673 and batch: 1000, loss is 5.191308031082153 and perplexity is 179.70345697473869
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.367654195645961 and perplexity of 214.35943216525163
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 21.189918518066406 and batch: 50, loss is 5.212843980789184 and perplexity is 183.61551533340685
At time: 21.635058641433716 and batch: 100, loss is 5.096209211349487 and perplexity is 163.40131193138043
At time: 22.081605195999146 and batch: 150, loss is 5.093578701019287 and perplexity is 162.97204793159693
At time: 22.529447555541992 and batch: 200, loss is 5.078369684219361 and perplexity is 160.51215699510067
At time: 22.991629600524902 and batch: 250, loss is 5.048780555725098 and perplexity is 155.83231974434227
At time: 23.438048601150513 and batch: 300, loss is 4.926239738464355 and perplexity is 137.86014627048186
At time: 23.884461879730225 and batch: 350, loss is 4.982515354156494 and perplexity is 145.84076183895263
At time: 24.334714651107788 and batch: 400, loss is 4.910216293334961 and perplexity is 135.66875548278477
At time: 24.789027214050293 and batch: 450, loss is 4.924455919265747 and perplexity is 137.6144479007274
At time: 25.24424123764038 and batch: 500, loss is 4.88510645866394 and perplexity is 132.30454955016637
At time: 25.703257083892822 and batch: 550, loss is 4.934931373596191 and perplexity is 139.06359876992084
At time: 26.163110494613647 and batch: 600, loss is 4.950403537750244 and perplexity is 141.2319448450121
At time: 26.625193119049072 and batch: 650, loss is 4.91635027885437 and perplexity is 136.5035032122605
At time: 27.08827781677246 and batch: 700, loss is 4.888960256576538 and perplexity is 132.81540828795625
At time: 27.5531005859375 and batch: 750, loss is 4.805094223022461 and perplexity is 122.13099803083163
At time: 28.018391132354736 and batch: 800, loss is 4.833743982315063 and perplexity is 125.68062693961592
At time: 28.48146653175354 and batch: 850, loss is 4.808548612594604 and perplexity is 122.55361559947474
At time: 28.944482803344727 and batch: 900, loss is 4.908562154769897 and perplexity is 135.44452606665638
At time: 29.408720016479492 and batch: 950, loss is 4.821016035079956 and perplexity is 124.09110766869513
At time: 29.87178111076355 and batch: 1000, loss is 4.758570775985718 and perplexity is 116.57918899869901
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0505285495665015 and perplexity of 156.1049518897841
Finished 3 epochs...
Completing Train Step...
At time: 31.305012941360474 and batch: 50, loss is 4.967458362579346 and perplexity is 143.66128805166457
At time: 31.77914524078369 and batch: 100, loss is 4.896079196929931 and perplexity is 133.76428675480162
At time: 32.2344868183136 and batch: 150, loss is 4.926957397460938 and perplexity is 137.9591183545785
At time: 32.690991163253784 and batch: 200, loss is 4.935229749679565 and perplexity is 139.10509821277446
At time: 33.14893078804016 and batch: 250, loss is 4.914258871078491 and perplexity is 136.218317048438
At time: 33.60535907745361 and batch: 300, loss is 4.803753976821899 and perplexity is 121.96742206526855
At time: 34.061336278915405 and batch: 350, loss is 4.856514959335327 and perplexity is 128.57533013915307
At time: 34.51779103279114 and batch: 400, loss is 4.7935355758666995 and perplexity is 120.72745606731208
At time: 34.975117921829224 and batch: 450, loss is 4.820417585372925 and perplexity is 124.01686759835556
At time: 35.432671308517456 and batch: 500, loss is 4.781545267105103 and perplexity is 119.2885403539741
At time: 35.891786336898804 and batch: 550, loss is 4.831930122375488 and perplexity is 125.45286651043463
At time: 36.35464954376221 and batch: 600, loss is 4.849289302825928 and perplexity is 127.6496373645363
At time: 36.81893277168274 and batch: 650, loss is 4.824947061538697 and perplexity is 124.57987314183241
At time: 37.28140640258789 and batch: 700, loss is 4.79664234161377 and perplexity is 121.10311122669214
At time: 37.74465298652649 and batch: 750, loss is 4.7257060718536374 and perplexity is 112.81012231282068
At time: 38.22506499290466 and batch: 800, loss is 4.761761989593506 and perplexity is 116.95181233712246
At time: 38.68867468833923 and batch: 850, loss is 4.744020805358887 and perplexity is 114.89524559429807
At time: 39.15290975570679 and batch: 900, loss is 4.849622774124145 and perplexity is 127.6922119531324
At time: 39.61614418029785 and batch: 950, loss is 4.772419710159301 and perplexity is 118.2049178354423
At time: 40.07987594604492 and batch: 1000, loss is 4.720580406188965 and perplexity is 112.2333747126538
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.020987906107089 and perplexity of 151.56095785566254
Finished 4 epochs...
Completing Train Step...
At time: 41.54110860824585 and batch: 50, loss is 4.887208328247071 and perplexity is 132.5829289146484
At time: 42.0000958442688 and batch: 100, loss is 4.817226476669312 and perplexity is 123.62174706435349
At time: 42.45919609069824 and batch: 150, loss is 4.85181619644165 and perplexity is 127.97260229540359
At time: 42.917341470718384 and batch: 200, loss is 4.863688821792603 and perplexity is 129.50102831864757
At time: 43.37590479850769 and batch: 250, loss is 4.846262159347535 and perplexity is 127.26380787251641
At time: 43.83535361289978 and batch: 300, loss is 4.740037307739258 and perplexity is 114.43847104115679
At time: 44.29412817955017 and batch: 350, loss is 4.789672203063965 and perplexity is 120.26194070570175
At time: 44.752254486083984 and batch: 400, loss is 4.725000505447388 and perplexity is 112.73055535341948
At time: 45.21103501319885 and batch: 450, loss is 4.760260992050171 and perplexity is 116.77639963406767
At time: 45.670079469680786 and batch: 500, loss is 4.723851852416992 and perplexity is 112.60114139948887
At time: 46.12760257720947 and batch: 550, loss is 4.774804229736328 and perplexity is 118.4871160960469
At time: 46.5870041847229 and batch: 600, loss is 4.792116107940674 and perplexity is 120.55620888432941
At time: 47.04596757888794 and batch: 650, loss is 4.7715256309509275 and perplexity is 118.09928050718452
At time: 47.505223512649536 and batch: 700, loss is 4.743159704208374 and perplexity is 114.79635175104019
At time: 47.96334171295166 and batch: 750, loss is 4.680122442245484 and perplexity is 107.78326898896508
At time: 48.42212891578674 and batch: 800, loss is 4.715848226547241 and perplexity is 111.70352089222177
At time: 48.88101506233215 and batch: 850, loss is 4.6989970302581785 and perplexity is 109.83695404705733
At time: 49.340044021606445 and batch: 900, loss is 4.803906564712524 and perplexity is 121.98603423688634
At time: 49.82175135612488 and batch: 950, loss is 4.734614315032959 and perplexity is 113.81955176155807
At time: 50.27973794937134 and batch: 1000, loss is 4.684066801071167 and perplexity is 108.20924442497218
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.009789071431974 and perplexity of 149.87312029217136
Finished 5 epochs...
Completing Train Step...
At time: 51.70162296295166 and batch: 50, loss is 4.837506656646728 and perplexity is 126.1544130009377
At time: 52.17027544975281 and batch: 100, loss is 4.764791402816773 and perplexity is 117.30664489976188
At time: 52.625773429870605 and batch: 150, loss is 4.802261028289795 and perplexity is 121.78546684022133
At time: 53.080646276474 and batch: 200, loss is 4.818831415176391 and perplexity is 123.82031166589475
At time: 53.53398895263672 and batch: 250, loss is 4.799249935150146 and perplexity is 121.41931099783385
At time: 53.99007225036621 and batch: 300, loss is 4.692560005187988 and perplexity is 109.13220150856957
At time: 54.44517993927002 and batch: 350, loss is 4.742916584014893 and perplexity is 114.7684458321752
At time: 54.899346590042114 and batch: 400, loss is 4.677856788635254 and perplexity is 107.53934586345476
At time: 55.353814363479614 and batch: 450, loss is 4.715012741088867 and perplexity is 111.61023320054865
At time: 55.80863976478577 and batch: 500, loss is 4.675633907318115 and perplexity is 107.30056415070398
At time: 56.262980937957764 and batch: 550, loss is 4.727773962020874 and perplexity is 113.0436426194747
At time: 56.719759464263916 and batch: 600, loss is 4.746716747283935 and perplexity is 115.20541441458614
At time: 57.17768955230713 and batch: 650, loss is 4.726597557067871 and perplexity is 112.91073570989283
At time: 57.635451793670654 and batch: 700, loss is 4.69922194480896 and perplexity is 109.86166075458084
At time: 58.09582161903381 and batch: 750, loss is 4.641543073654175 and perplexity is 103.70424747500282
At time: 58.55234408378601 and batch: 800, loss is 4.679220390319824 and perplexity is 107.68608672193504
At time: 59.009613037109375 and batch: 850, loss is 4.662699499130249 and perplexity is 105.92163191582047
At time: 59.467896699905396 and batch: 900, loss is 4.766068992614746 and perplexity is 117.45661044935011
At time: 59.92758655548096 and batch: 950, loss is 4.702489919662476 and perplexity is 110.22127318143153
At time: 60.38546681404114 and batch: 1000, loss is 4.65233118057251 and perplexity is 104.82907646309279
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.999298467868712 and perplexity of 148.30907901479168
Finished 6 epochs...
Completing Train Step...
At time: 61.826876401901245 and batch: 50, loss is 4.795009155273437 and perplexity is 120.90548870079635
At time: 62.28117799758911 and batch: 100, loss is 4.7230266571044925 and perplexity is 112.50826179260255
At time: 62.736294746398926 and batch: 150, loss is 4.762150611877441 and perplexity is 116.9972712501445
At time: 63.191139459609985 and batch: 200, loss is 4.780212497711181 and perplexity is 119.12966213587556
At time: 63.64586353302002 and batch: 250, loss is 4.7603200340270995 and perplexity is 116.78329454710335
At time: 64.10093021392822 and batch: 300, loss is 4.65629885673523 and perplexity is 105.2458305167701
At time: 64.55646991729736 and batch: 350, loss is 4.707890100479126 and perplexity is 110.8180980170861
At time: 65.00925326347351 and batch: 400, loss is 4.640881090164185 and perplexity is 103.63561969306801
At time: 65.45930051803589 and batch: 450, loss is 4.682798166275024 and perplexity is 108.0720534532505
At time: 65.90996932983398 and batch: 500, loss is 4.642951126098633 and perplexity is 103.85037134505465
At time: 66.36041212081909 and batch: 550, loss is 4.69422251701355 and perplexity is 109.31378598547315
At time: 66.81120133399963 and batch: 600, loss is 4.714098796844483 and perplexity is 111.5082742697867
At time: 67.26396417617798 and batch: 650, loss is 4.690811834335327 and perplexity is 108.9415864372064
At time: 67.71764969825745 and batch: 700, loss is 4.664024877548218 and perplexity is 106.062111234322
At time: 68.17117190361023 and batch: 750, loss is 4.6096240234375 and perplexity is 100.44637705247143
At time: 68.62466311454773 and batch: 800, loss is 4.648830699920654 and perplexity is 104.46276581489487
At time: 69.07856631278992 and batch: 850, loss is 4.634183654785156 and perplexity is 102.94384596794607
At time: 69.53583264350891 and batch: 900, loss is 4.733563184738159 and perplexity is 113.6999754387184
At time: 69.99155783653259 and batch: 950, loss is 4.673393592834473 and perplexity is 107.06044621301047
At time: 70.44862151145935 and batch: 1000, loss is 4.625527544021606 and perplexity is 102.05659823214633
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.9966676293349845 and perplexity of 147.91941457168303
Finished 7 epochs...
Completing Train Step...
At time: 71.8636827468872 and batch: 50, loss is 4.762249298095703 and perplexity is 117.00881783812635
At time: 72.3203227519989 and batch: 100, loss is 4.6885667991638185 and perplexity is 108.69728308143915
At time: 72.77716088294983 and batch: 150, loss is 4.727150926589966 and perplexity is 112.97323436058136
At time: 73.23378562927246 and batch: 200, loss is 4.748056564331055 and perplexity is 115.35987204211479
At time: 73.70438265800476 and batch: 250, loss is 4.72718487739563 and perplexity is 112.97706995801688
At time: 74.16045141220093 and batch: 300, loss is 4.625710229873658 and perplexity is 102.07524423188013
At time: 74.61668062210083 and batch: 350, loss is 4.679091186523437 and perplexity is 107.67217416950913
At time: 75.07222890853882 and batch: 400, loss is 4.6099296569824215 and perplexity is 100.47708152668412
At time: 75.52797079086304 and batch: 450, loss is 4.651327743530273 and perplexity is 104.72393984247776
At time: 75.98397088050842 and batch: 500, loss is 4.610238952636719 and perplexity is 100.50816345786212
At time: 76.44432950019836 and batch: 550, loss is 4.6643001747131345 and perplexity is 106.09131385236445
At time: 76.90309190750122 and batch: 600, loss is 4.687888956069946 and perplexity is 108.62362834477716
At time: 77.3610475063324 and batch: 650, loss is 4.661981630325317 and perplexity is 105.84562136656595
At time: 77.81902027130127 and batch: 700, loss is 4.636630954742432 and perplexity is 103.19608896903206
At time: 78.27772498130798 and batch: 750, loss is 4.584658918380737 and perplexity is 97.96976575637954
At time: 78.73647332191467 and batch: 800, loss is 4.626237058639527 and perplexity is 102.12903457473045
At time: 79.1957573890686 and batch: 850, loss is 4.611201038360596 and perplexity is 100.6049074576063
At time: 79.65490365028381 and batch: 900, loss is 4.707168188095093 and perplexity is 110.73812592964583
At time: 80.11471891403198 and batch: 950, loss is 4.648864393234253 and perplexity is 104.46628557091864
At time: 80.5768072605133 and batch: 1000, loss is 4.601879062652588 and perplexity is 99.67142864744903
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.992540312976372 and perplexity of 147.31016250504615
Finished 8 epochs...
Completing Train Step...
At time: 82.00661540031433 and batch: 50, loss is 4.7329870796203615 and perplexity is 113.63449116569826
At time: 82.48442053794861 and batch: 100, loss is 4.663874292373658 and perplexity is 106.04614105525386
At time: 82.94588351249695 and batch: 150, loss is 4.700958118438721 and perplexity is 110.05256534671045
At time: 83.41020894050598 and batch: 200, loss is 4.723797855377197 and perplexity is 112.59506143532725
At time: 83.870290517807 and batch: 250, loss is 4.702673406600952 and perplexity is 110.24149920095097
At time: 84.330970287323 and batch: 300, loss is 4.601283140182495 and perplexity is 99.61204989781457
At time: 84.79317998886108 and batch: 350, loss is 4.652344074249267 and perplexity is 104.83042810403327
At time: 85.27218103408813 and batch: 400, loss is 4.585684900283813 and perplexity is 98.07033254412465
At time: 85.73261380195618 and batch: 450, loss is 4.624987392425537 and perplexity is 102.00148708324933
At time: 86.19351387023926 and batch: 500, loss is 4.5840012645721435 and perplexity is 97.90535674853896
At time: 86.65473794937134 and batch: 550, loss is 4.636576509475708 and perplexity is 103.19047058339187
At time: 87.11797451972961 and batch: 600, loss is 4.662274398803711 and perplexity is 105.87661416471389
At time: 87.57965540885925 and batch: 650, loss is 4.6345180225372316 and perplexity is 102.97827282560715
At time: 88.04061365127563 and batch: 700, loss is 4.61243010520935 and perplexity is 100.72863363247927
At time: 88.50197219848633 and batch: 750, loss is 4.558029975891113 and perplexity is 95.39536342815317
At time: 88.96380758285522 and batch: 800, loss is 4.602299633026123 and perplexity is 99.71335631357516
At time: 89.42465472221375 and batch: 850, loss is 4.58780499458313 and perplexity is 98.27847145618209
At time: 89.8864357471466 and batch: 900, loss is 4.681970195770264 and perplexity is 107.98261001397684
At time: 90.3490903377533 and batch: 950, loss is 4.628523645401001 and perplexity is 102.36282866652313
At time: 90.81137609481812 and batch: 1000, loss is 4.58253890991211 and perplexity is 97.76228902722558
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.990014425138148 and perplexity of 146.9385430891904
Finished 9 epochs...
Completing Train Step...
At time: 92.25215220451355 and batch: 50, loss is 4.708116359710694 and perplexity is 110.8431744715699
At time: 92.70614194869995 and batch: 100, loss is 4.641633157730102 and perplexity is 103.71358999710628
At time: 93.15997672080994 and batch: 150, loss is 4.681038160324096 and perplexity is 107.88201328101685
At time: 93.61378502845764 and batch: 200, loss is 4.701191129684449 and perplexity is 110.0782118198997
At time: 94.06991744041443 and batch: 250, loss is 4.679387683868408 and perplexity is 107.70410341651223
At time: 94.5258276462555 and batch: 300, loss is 4.579798612594605 and perplexity is 97.49475801352334
At time: 94.98167586326599 and batch: 350, loss is 4.632226533889771 and perplexity is 102.74256944144493
At time: 95.43750977516174 and batch: 400, loss is 4.567012977600098 and perplexity is 96.25616062375117
At time: 95.89552807807922 and batch: 450, loss is 4.605038328170776 and perplexity is 99.98681508755449
At time: 96.3512282371521 and batch: 500, loss is 4.562815504074097 and perplexity is 95.8529747108864
At time: 96.80794072151184 and batch: 550, loss is 4.617024126052857 and perplexity is 101.19244764481122
At time: 97.28010725975037 and batch: 600, loss is 4.639716758728027 and perplexity is 103.51502370362847
At time: 97.73794031143188 and batch: 650, loss is 4.613533449172974 and perplexity is 100.83983329682
At time: 98.19525051116943 and batch: 700, loss is 4.591759614944458 and perplexity is 98.66789500411197
At time: 98.6504864692688 and batch: 750, loss is 4.534690227508545 and perplexity is 93.19464169618146
At time: 99.106201171875 and batch: 800, loss is 4.581412649154663 and perplexity is 97.65224517820262
At time: 99.56359601020813 and batch: 850, loss is 4.565753250122071 and perplexity is 96.13498043631517
At time: 100.02246236801147 and batch: 900, loss is 4.659280786514282 and perplexity is 105.56013457632977
At time: 100.48044323921204 and batch: 950, loss is 4.607354278564453 and perplexity is 100.21864794439499
At time: 100.93980002403259 and batch: 1000, loss is 4.562427701950074 and perplexity is 95.81580993045576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.991901304663681 and perplexity of 147.21606015595538
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 102.38287425041199 and batch: 50, loss is 4.6860270690917964 and perplexity is 108.42157158743656
At time: 102.83925890922546 and batch: 100, loss is 4.601497163772583 and perplexity is 99.6333715079321
At time: 103.29345607757568 and batch: 150, loss is 4.634544134140015 and perplexity is 102.98096178846886
At time: 103.74878478050232 and batch: 200, loss is 4.648797206878662 and perplexity is 104.45926709768446
At time: 104.20449662208557 and batch: 250, loss is 4.61102560043335 and perplexity is 100.58725908931301
At time: 104.66191077232361 and batch: 300, loss is 4.509767036437989 and perplexity is 90.90063950688864
At time: 105.11789917945862 and batch: 350, loss is 4.565730838775635 and perplexity is 96.13282594610665
At time: 105.57354354858398 and batch: 400, loss is 4.48552433013916 and perplexity is 88.72345900267796
At time: 106.02927041053772 and batch: 450, loss is 4.521707572937012 and perplexity is 91.99254792352187
At time: 106.48689484596252 and batch: 500, loss is 4.475070638656616 and perplexity is 87.80080232143067
At time: 106.94286775588989 and batch: 550, loss is 4.521287794113159 and perplexity is 91.95393950401674
At time: 107.3988573551178 and batch: 600, loss is 4.538630876541138 and perplexity is 93.56261361874725
At time: 107.85455369949341 and batch: 650, loss is 4.5059430789947506 and perplexity is 90.55370308766321
At time: 108.3118691444397 and batch: 700, loss is 4.4839612865448 and perplexity is 88.58488869234414
At time: 108.78438901901245 and batch: 750, loss is 4.421491584777832 and perplexity is 83.22032299600792
At time: 109.24046778678894 and batch: 800, loss is 4.461378469467163 and perplexity is 86.60681169551346
At time: 109.69734025001526 and batch: 850, loss is 4.430677995681763 and perplexity is 83.98834134357766
At time: 110.15678143501282 and batch: 900, loss is 4.5136873817443846 and perplexity is 91.25770084689266
At time: 110.61639070510864 and batch: 950, loss is 4.449358510971069 and perplexity is 85.57203287136932
At time: 111.07614970207214 and batch: 1000, loss is 4.409040956497193 and perplexity is 82.19060133192005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.924611161394817 and perplexity of 137.6358131189584
Finished 11 epochs...
Completing Train Step...
At time: 112.50936841964722 and batch: 50, loss is 4.6293487644195555 and perplexity is 102.44732503823668
At time: 112.98641753196716 and batch: 100, loss is 4.554373397827148 and perplexity is 95.04717980309147
At time: 113.44412660598755 and batch: 150, loss is 4.596745986938476 and perplexity is 99.16111850844884
At time: 113.9031879901886 and batch: 200, loss is 4.614478549957275 and perplexity is 100.9351821523995
At time: 114.36169648170471 and batch: 250, loss is 4.582498769760132 and perplexity is 97.75836491284417
At time: 114.81893706321716 and batch: 300, loss is 4.486123886108398 and perplexity is 88.7766696319157
At time: 115.27684307098389 and batch: 350, loss is 4.541654825210571 and perplexity is 93.84597037192266
At time: 115.73458981513977 and batch: 400, loss is 4.464886465072632 and perplexity is 86.91116152716947
At time: 116.19198250770569 and batch: 450, loss is 4.50302227973938 and perplexity is 90.28959978323807
At time: 116.64938592910767 and batch: 500, loss is 4.4578311729431155 and perplexity is 86.30013590998419
At time: 117.1112916469574 and batch: 550, loss is 4.505674438476563 and perplexity is 90.52937996117683
At time: 117.57282781600952 and batch: 600, loss is 4.524860782623291 and perplexity is 92.28307752635729
At time: 118.03464961051941 and batch: 650, loss is 4.493949642181397 and perplexity is 89.4741397493702
At time: 118.49643158912659 and batch: 700, loss is 4.474925861358643 and perplexity is 87.78809167863915
At time: 118.96000218391418 and batch: 750, loss is 4.415375471115112 and perplexity is 82.71289137802565
At time: 119.42190456390381 and batch: 800, loss is 4.45566912651062 and perplexity is 86.11375256634096
At time: 119.88387989997864 and batch: 850, loss is 4.429066619873047 and perplexity is 83.85311354278612
At time: 120.34545850753784 and batch: 900, loss is 4.516524238586426 and perplexity is 91.51695343730478
At time: 120.82483386993408 and batch: 950, loss is 4.458649759292602 and perplexity is 86.37080894524354
At time: 121.28671383857727 and batch: 1000, loss is 4.419673061370849 and perplexity is 83.06912241327116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.920355820074314 and perplexity of 137.05137014031126
Finished 12 epochs...
Completing Train Step...
At time: 122.74589109420776 and batch: 50, loss is 4.616095685958863 and perplexity is 101.09854011970978
At time: 123.20673108100891 and batch: 100, loss is 4.541633892059326 and perplexity is 93.84400590059246
At time: 123.66601133346558 and batch: 150, loss is 4.584483594894409 and perplexity is 97.95259086111764
At time: 124.12496089935303 and batch: 200, loss is 4.602946300506591 and perplexity is 99.77785855197368
At time: 124.58065271377563 and batch: 250, loss is 4.571727962493896 and perplexity is 96.71107858992013
At time: 125.03509283065796 and batch: 300, loss is 4.476164178848267 and perplexity is 87.89686854419291
At time: 125.48791599273682 and batch: 350, loss is 4.531581325531006 and perplexity is 92.90535859971143
At time: 125.94106888771057 and batch: 400, loss is 4.456070051193238 and perplexity is 86.14828461717083
At time: 126.41516900062561 and batch: 450, loss is 4.494876327514649 and perplexity is 89.55709255201779
At time: 126.9017539024353 and batch: 500, loss is 4.45031328201294 and perplexity is 85.65377358597303
At time: 127.37217211723328 and batch: 550, loss is 4.4983296394348145 and perplexity is 89.86689574291283
At time: 127.8632435798645 and batch: 600, loss is 4.518131036758422 and perplexity is 91.6641209133472
At time: 128.35459303855896 and batch: 650, loss is 4.488954982757568 and perplexity is 89.02836107712751
At time: 128.841468334198 and batch: 700, loss is 4.471354894638061 and perplexity is 87.4751623873682
At time: 129.3381564617157 and batch: 750, loss is 4.413445262908936 and perplexity is 82.55339225910583
At time: 129.81710529327393 and batch: 800, loss is 4.452655982971192 and perplexity is 85.85466999165106
At time: 130.28885078430176 and batch: 850, loss is 4.428302173614502 and perplexity is 83.7890368385898
At time: 130.76339292526245 and batch: 900, loss is 4.518719234466553 and perplexity is 91.71805339911353
At time: 131.22107362747192 and batch: 950, loss is 4.462574634552002 and perplexity is 86.7104697234702
At time: 131.67978382110596 and batch: 1000, loss is 4.423563585281372 and perplexity is 83.39293431077164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.918665443978658 and perplexity of 136.81989747339762
Finished 13 epochs...
Completing Train Step...
At time: 133.1250867843628 and batch: 50, loss is 4.607728910446167 and perplexity is 100.2562000787314
At time: 133.58235692977905 and batch: 100, loss is 4.5333569049835205 and perplexity is 93.07046598273038
At time: 134.0384910106659 and batch: 150, loss is 4.576465616226196 and perplexity is 97.17034926603172
At time: 134.4951856136322 and batch: 200, loss is 4.595324506759644 and perplexity is 99.02026307929631
At time: 134.95185685157776 and batch: 250, loss is 4.5643851184844975 and perplexity is 96.00354505905331
At time: 135.40734577178955 and batch: 300, loss is 4.469404077529907 and perplexity is 87.30468068741945
At time: 135.86326217651367 and batch: 350, loss is 4.524646778106689 and perplexity is 92.2633306439971
At time: 136.3182201385498 and batch: 400, loss is 4.449927701950073 and perplexity is 85.62075356490897
At time: 136.77416062355042 and batch: 450, loss is 4.48897988319397 and perplexity is 89.03057794977089
At time: 137.2298047542572 and batch: 500, loss is 4.445295505523681 and perplexity is 85.22505859218332
At time: 137.68498396873474 and batch: 550, loss is 4.493109636306762 and perplexity is 89.39901250443397
At time: 138.14055180549622 and batch: 600, loss is 4.513614854812622 and perplexity is 91.2510824458596
At time: 138.59713125228882 and batch: 650, loss is 4.485188846588135 and perplexity is 88.69369873391192
At time: 139.05285811424255 and batch: 700, loss is 4.468815851211548 and perplexity is 87.25334087771714
At time: 139.50954961776733 and batch: 750, loss is 4.412046480178833 and perplexity is 82.43799872375943
At time: 139.9664523601532 and batch: 800, loss is 4.450564775466919 and perplexity is 85.67531765832156
At time: 140.42372965812683 and batch: 850, loss is 4.427150249481201 and perplexity is 83.69257379467018
At time: 140.88097167015076 and batch: 900, loss is 4.519655637741089 and perplexity is 91.80397870874252
At time: 141.34107613563538 and batch: 950, loss is 4.464080810546875 and perplexity is 86.84116935508729
At time: 141.80264472961426 and batch: 1000, loss is 4.4250045490264895 and perplexity is 83.51318712487758
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.917452184165397 and perplexity of 136.65400004882784
Finished 14 epochs...
Completing Train Step...
At time: 143.2412543296814 and batch: 50, loss is 4.601135416030884 and perplexity is 99.59733587908784
At time: 143.72122430801392 and batch: 100, loss is 4.526969347000122 and perplexity is 92.47786762777768
At time: 144.18384647369385 and batch: 150, loss is 4.5696882629394535 and perplexity is 96.51401808644161
At time: 144.66201734542847 and batch: 200, loss is 4.589050407409668 and perplexity is 98.40094497432115
At time: 145.12479209899902 and batch: 250, loss is 4.558427133560181 and perplexity is 95.4332579528846
At time: 145.58708691596985 and batch: 300, loss is 4.463981246948242 and perplexity is 86.83252356616686
At time: 146.04956245422363 and batch: 350, loss is 4.519225864410401 and perplexity is 91.76453228416027
At time: 146.5127193927765 and batch: 400, loss is 4.4450218200683596 and perplexity is 85.20173692476384
At time: 146.97589707374573 and batch: 450, loss is 4.484179735183716 and perplexity is 88.60424205448832
At time: 147.43947458267212 and batch: 500, loss is 4.440658378601074 and perplexity is 84.83077405874032
At time: 147.90219140052795 and batch: 550, loss is 4.488980293273926 and perplexity is 89.03061445943385
At time: 148.36420154571533 and batch: 600, loss is 4.510060367584228 and perplexity is 90.9273074067407
At time: 148.82787013053894 and batch: 650, loss is 4.481928482055664 and perplexity is 88.40499583838874
At time: 149.2900583744049 and batch: 700, loss is 4.466285266876221 and perplexity is 87.03281808353434
At time: 149.75305247306824 and batch: 750, loss is 4.410491609573365 and perplexity is 82.30991790312606
At time: 150.21408605575562 and batch: 800, loss is 4.44841118812561 and perplexity is 85.49100691462739
At time: 150.67689418792725 and batch: 850, loss is 4.425347681045532 and perplexity is 83.5418480903587
At time: 151.13916444778442 and batch: 900, loss is 4.519260196685791 and perplexity is 91.76768282343602
At time: 151.6028254032135 and batch: 950, loss is 4.4643909358978275 and perplexity is 86.86810517973582
At time: 152.06847047805786 and batch: 1000, loss is 4.424974956512451 and perplexity is 83.51071579628179
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.916957576100419 and perplexity of 136.58642659085845
Finished 15 epochs...
Completing Train Step...
At time: 153.51683354377747 and batch: 50, loss is 4.595837488174438 and perplexity is 99.07107166475988
At time: 153.9737470149994 and batch: 100, loss is 4.5213234233856205 and perplexity is 91.95721581434715
At time: 154.43013620376587 and batch: 150, loss is 4.56444616317749 and perplexity is 96.00940574486773
At time: 154.88630390167236 and batch: 200, loss is 4.583944015502929 and perplexity is 97.89975191843122
At time: 155.3443112373352 and batch: 250, loss is 4.553539161682129 and perplexity is 94.96792107506212
At time: 155.80221438407898 and batch: 300, loss is 4.459168605804443 and perplexity is 86.41563376578495
At time: 156.25796031951904 and batch: 350, loss is 4.514490308761597 and perplexity is 91.3310035448515
At time: 156.72973942756653 and batch: 400, loss is 4.440981740951538 and perplexity is 84.85820957279874
At time: 157.187180519104 and batch: 450, loss is 4.480174627304077 and perplexity is 88.25008220408999
At time: 157.647305727005 and batch: 500, loss is 4.437081508636474 and perplexity is 84.52788742691143
At time: 158.10753083229065 and batch: 550, loss is 4.485320558547974 and perplexity is 88.70538152416225
At time: 158.56830024719238 and batch: 600, loss is 4.50681848526001 and perplexity is 90.6330090741027
At time: 159.02965021133423 and batch: 650, loss is 4.479598960876465 and perplexity is 88.19929421440393
At time: 159.49041295051575 and batch: 700, loss is 4.465168895721436 and perplexity is 86.93571136955867
At time: 159.9522943496704 and batch: 750, loss is 4.409221000671387 and perplexity is 82.2054006030847
At time: 160.41766786575317 and batch: 800, loss is 4.446289873123169 and perplexity is 85.30984577694024
At time: 160.88308000564575 and batch: 850, loss is 4.423781185150147 and perplexity is 83.41108257679291
At time: 161.348206281662 and batch: 900, loss is 4.5187094783782955 and perplexity is 91.7171585940547
At time: 161.81293725967407 and batch: 950, loss is 4.4640131855010985 and perplexity is 86.8352969155986
At time: 162.2774362564087 and batch: 1000, loss is 4.4243265151977536 and perplexity is 83.45658155127686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.916504650581174 and perplexity of 136.52457712033203
Finished 16 epochs...
Completing Train Step...
At time: 163.7218508720398 and batch: 50, loss is 4.590735416412354 and perplexity is 98.56689122367409
At time: 164.17746782302856 and batch: 100, loss is 4.516280927658081 and perplexity is 91.49468907109633
At time: 164.63383054733276 and batch: 150, loss is 4.559723787307739 and perplexity is 95.55708210562784
At time: 165.090482711792 and batch: 200, loss is 4.579605579376221 and perplexity is 97.47594010290778
At time: 165.54604268074036 and batch: 250, loss is 4.548869142532348 and perplexity is 94.52545303628528
At time: 166.00119256973267 and batch: 300, loss is 4.455147924423218 and perplexity is 86.06888159318758
At time: 166.4573438167572 and batch: 350, loss is 4.510098838806153 and perplexity is 90.9308055586516
At time: 166.91388368606567 and batch: 400, loss is 4.437393083572387 and perplexity is 84.55422830138437
At time: 167.37159943580627 and batch: 450, loss is 4.476393995285034 and perplexity is 87.91707101066523
At time: 167.82870936393738 and batch: 500, loss is 4.4334064292907716 and perplexity is 84.21781086068144
At time: 168.30041480064392 and batch: 550, loss is 4.482237663269043 and perplexity is 88.43233322815672
At time: 168.75677156448364 and batch: 600, loss is 4.504074630737304 and perplexity is 90.3846661464899
At time: 169.21396565437317 and batch: 650, loss is 4.477074794769287 and perplexity is 87.97694528614169
At time: 169.67171502113342 and batch: 700, loss is 4.462552661895752 and perplexity is 86.70856448505734
At time: 170.12854647636414 and batch: 750, loss is 4.407500123977661 and perplexity is 82.06405689751423
At time: 170.58434867858887 and batch: 800, loss is 4.4439169120788575 and perplexity is 85.10764883382998
At time: 171.04173517227173 and batch: 850, loss is 4.421456966400147 and perplexity is 83.21744209330173
At time: 171.49862718582153 and batch: 900, loss is 4.517529859542846 and perplexity is 91.60903109339976
At time: 171.95471453666687 and batch: 950, loss is 4.462928247451782 and perplexity is 86.74113708597837
At time: 172.41179966926575 and batch: 1000, loss is 4.423399391174317 and perplexity is 83.37924280645066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.916390023580411 and perplexity of 136.50892861441415
Finished 17 epochs...
Completing Train Step...
At time: 173.82933235168457 and batch: 50, loss is 4.586238298416138 and perplexity is 98.12461950275632
At time: 174.3015763759613 and batch: 100, loss is 4.512016477584839 and perplexity is 91.10534529616581
At time: 174.75466227531433 and batch: 150, loss is 4.555532522201538 and perplexity is 95.15741518183309
At time: 175.2081835269928 and batch: 200, loss is 4.575296630859375 and perplexity is 97.05682491673025
At time: 175.66645073890686 and batch: 250, loss is 4.545172252655029 and perplexity is 94.17664798998848
At time: 176.12443494796753 and batch: 300, loss is 4.451328182220459 and perplexity is 85.74074774614144
At time: 176.5840563774109 and batch: 350, loss is 4.50650559425354 and perplexity is 90.6046552567327
At time: 177.04404211044312 and batch: 400, loss is 4.4335635471343995 and perplexity is 84.23104402107452
At time: 177.5035421848297 and batch: 450, loss is 4.473212423324585 and perplexity is 87.63780101705699
At time: 177.96407675743103 and batch: 500, loss is 4.430313758850097 and perplexity is 83.95775526685581
At time: 178.42583847045898 and batch: 550, loss is 4.478637075424194 and perplexity is 88.11449738535468
At time: 178.8855631351471 and batch: 600, loss is 4.501048669815064 and perplexity is 90.11157906255846
At time: 179.34593296051025 and batch: 650, loss is 4.475106945037842 and perplexity is 87.80399010869985
At time: 179.80571293830872 and batch: 700, loss is 4.460421314239502 and perplexity is 86.5239551925413
At time: 180.28035926818848 and batch: 750, loss is 4.406208171844482 and perplexity is 81.95810252287609
At time: 180.74268794059753 and batch: 800, loss is 4.441598901748657 and perplexity is 84.91059689709937
At time: 181.20651483535767 and batch: 850, loss is 4.419219017028809 and perplexity is 83.03141390955533
At time: 181.6699869632721 and batch: 900, loss is 4.51624792098999 and perplexity is 91.4916691861005
At time: 182.13271713256836 and batch: 950, loss is 4.461985559463501 and perplexity is 86.65940578756678
At time: 182.59626579284668 and batch: 1000, loss is 4.422217435836792 and perplexity is 83.28075048360647
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.915983618759528 and perplexity of 136.45346199944493
Finished 18 epochs...
Completing Train Step...
At time: 184.0278582572937 and batch: 50, loss is 4.58219430923462 and perplexity is 97.72860588014478
At time: 184.50290250778198 and batch: 100, loss is 4.508157958984375 and perplexity is 90.75449095104524
At time: 184.9596929550171 and batch: 150, loss is 4.5516391849517825 and perplexity is 94.78765553928032
At time: 185.41762828826904 and batch: 200, loss is 4.571397981643677 and perplexity is 96.67917105070937
At time: 185.87622022628784 and batch: 250, loss is 4.54123701095581 and perplexity is 93.80676837789564
At time: 186.33440566062927 and batch: 300, loss is 4.447366065979004 and perplexity is 85.4017050437753
At time: 186.79112362861633 and batch: 350, loss is 4.502679262161255 and perplexity is 90.25863417456824
At time: 187.24877500534058 and batch: 400, loss is 4.430206298828125 and perplexity is 83.94873364937037
At time: 187.70639610290527 and batch: 450, loss is 4.469833879470825 and perplexity is 87.3422124736697
At time: 188.16504168510437 and batch: 500, loss is 4.427608079910279 and perplexity is 83.7308995743258
At time: 188.62280106544495 and batch: 550, loss is 4.475684795379639 and perplexity is 87.85474233657918
At time: 189.08156991004944 and batch: 600, loss is 4.498524580001831 and perplexity is 89.88441615418895
At time: 189.54078269004822 and batch: 650, loss is 4.472397022247314 and perplexity is 87.56637018604401
At time: 190.003497838974 and batch: 700, loss is 4.458151779174805 and perplexity is 86.32780870715114
At time: 190.46562719345093 and batch: 750, loss is 4.40468846321106 and perplexity is 81.83364468067492
At time: 190.92735815048218 and batch: 800, loss is 4.438735637664795 and perplexity is 84.66782316318162
At time: 191.38925290107727 and batch: 850, loss is 4.416933698654175 and perplexity is 82.8418773518557
At time: 191.86699104309082 and batch: 900, loss is 4.51515040397644 and perplexity is 91.3913106052695
At time: 192.3281533718109 and batch: 950, loss is 4.46080771446228 and perplexity is 86.55739452817065
At time: 192.78950190544128 and batch: 1000, loss is 4.420704908370972 and perplexity is 83.15488127543944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.9156267119617 and perplexity of 136.4047695211253
Finished 19 epochs...
Completing Train Step...
At time: 194.2282521724701 and batch: 50, loss is 4.578487310409546 and perplexity is 97.36699670945431
At time: 194.683988571167 and batch: 100, loss is 4.504253339767456 and perplexity is 90.40082014590735
At time: 195.1383500099182 and batch: 150, loss is 4.547546987533569 and perplexity is 94.40055831932264
At time: 195.59419345855713 and batch: 200, loss is 4.567920122146607 and perplexity is 96.34351849204513
At time: 196.05010223388672 and batch: 250, loss is 4.5375689506530765 and perplexity is 93.46330979317437
At time: 196.50436067581177 and batch: 300, loss is 4.443950748443603 and perplexity is 85.11052861599897
At time: 196.95984745025635 and batch: 350, loss is 4.498729391098022 and perplexity is 89.90282736533781
At time: 197.41412258148193 and batch: 400, loss is 4.426811208724976 and perplexity is 83.66420341079035
At time: 197.86932754516602 and batch: 450, loss is 4.466717166900635 and perplexity is 87.07041567840605
At time: 198.32304787635803 and batch: 500, loss is 4.424710111618042 and perplexity is 83.48860133815461
At time: 198.77719974517822 and batch: 550, loss is 4.4727973365783695 and perplexity is 87.6014312762082
At time: 199.23286414146423 and batch: 600, loss is 4.4955588054656985 and perplexity is 89.61823415456928
At time: 199.68831992149353 and batch: 650, loss is 4.469662551879883 and perplexity is 87.32724962463001
At time: 200.14416003227234 and batch: 700, loss is 4.456242341995239 and perplexity is 86.16312845291002
At time: 200.6035077571869 and batch: 750, loss is 4.403139686584472 and perplexity is 81.70700074142668
At time: 201.06099200248718 and batch: 800, loss is 4.436176061630249 and perplexity is 84.45138654327272
At time: 201.51877355575562 and batch: 850, loss is 4.414474430084229 and perplexity is 82.63839723529063
At time: 201.97522377967834 and batch: 900, loss is 4.513423748016358 and perplexity is 91.23364541005881
At time: 202.43301129341125 and batch: 950, loss is 4.459266176223755 and perplexity is 86.42406578675782
At time: 202.89142155647278 and batch: 1000, loss is 4.419038953781128 and perplexity is 83.01646434948086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.915502036490092 and perplexity of 136.38776425224765
Finished 20 epochs...
Completing Train Step...
At time: 204.33237171173096 and batch: 50, loss is 4.575067300796508 and perplexity is 97.0345694209951
At time: 204.80375480651855 and batch: 100, loss is 4.500966482162475 and perplexity is 90.10417330773925
At time: 205.25742435455322 and batch: 150, loss is 4.544235925674439 and perplexity is 94.08850912337265
At time: 205.71289253234863 and batch: 200, loss is 4.564233264923096 and perplexity is 95.98896768566993
At time: 206.16693234443665 and batch: 250, loss is 4.534259262084961 and perplexity is 93.15448668128016
At time: 206.62271547317505 and batch: 300, loss is 4.440539598464966 and perplexity is 84.82069844625461
At time: 207.07818818092346 and batch: 350, loss is 4.4959392261505124 and perplexity is 89.6523332701714
At time: 207.5339014530182 and batch: 400, loss is 4.423709106445313 and perplexity is 83.40507063066121
At time: 207.9898042678833 and batch: 450, loss is 4.463787899017334 and perplexity is 86.81573630034372
At time: 208.4464373588562 and batch: 500, loss is 4.42209303855896 and perplexity is 83.27039122929534
At time: 208.90628719329834 and batch: 550, loss is 4.4700146484375 and perplexity is 87.35800266231068
At time: 209.36559414863586 and batch: 600, loss is 4.493295936584473 and perplexity is 89.41566911680816
At time: 209.8253791332245 and batch: 650, loss is 4.46736330986023 and perplexity is 87.12669379438186
At time: 210.28570199012756 and batch: 700, loss is 4.454086446762085 and perplexity is 85.97756986930895
At time: 210.74593138694763 and batch: 750, loss is 4.4016702938079835 and perplexity is 81.58702922897157
At time: 211.20569682121277 and batch: 800, loss is 4.434143991470337 and perplexity is 84.27994964560992
At time: 211.66538834571838 and batch: 850, loss is 4.412092990875244 and perplexity is 82.44183306165903
At time: 212.12361526489258 and batch: 900, loss is 4.511771898269654 and perplexity is 91.0830655378986
At time: 212.5816719532013 and batch: 950, loss is 4.457825241088867 and perplexity is 86.29962399167465
At time: 213.0429756641388 and batch: 1000, loss is 4.417283802032471 and perplexity is 82.8708856506364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.915532926233803 and perplexity of 136.3919773004004
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 214.47758197784424 and batch: 50, loss is 4.572985467910766 and perplexity is 96.83276979275561
At time: 214.95496368408203 and batch: 100, loss is 4.49662745475769 and perplexity is 89.71405580779994
At time: 215.41759061813354 and batch: 150, loss is 4.539597959518432 and perplexity is 93.65314019598964
At time: 215.8958773612976 and batch: 200, loss is 4.557008409500122 and perplexity is 95.29796049127276
At time: 216.35830354690552 and batch: 250, loss is 4.527222957611084 and perplexity is 92.50132397055019
At time: 216.82155752182007 and batch: 300, loss is 4.433285055160522 and perplexity is 84.20758961744652
At time: 217.28513932228088 and batch: 350, loss is 4.48592942237854 and perplexity is 88.75940746810143
At time: 217.74823093414307 and batch: 400, loss is 4.411261911392212 and perplexity is 82.37334580877524
At time: 218.2121217250824 and batch: 450, loss is 4.4516848278045655 and perplexity is 85.77133225879456
At time: 218.67598390579224 and batch: 500, loss is 4.407023010253906 and perplexity is 82.02491234868472
At time: 219.139888048172 and batch: 550, loss is 4.453188304901123 and perplexity is 85.9003844816079
At time: 219.60472011566162 and batch: 600, loss is 4.475978755950928 and perplexity is 87.8805719630868
At time: 220.07077646255493 and batch: 650, loss is 4.4481797027587895 and perplexity is 85.47121928789325
At time: 220.53612208366394 and batch: 700, loss is 4.434995899200439 and perplexity is 84.35177897784321
At time: 221.00235843658447 and batch: 750, loss is 4.380482187271118 and perplexity is 79.87653957138913
At time: 221.46901893615723 and batch: 800, loss is 4.408063192367553 and perplexity is 82.11027758538255
At time: 221.9355731010437 and batch: 850, loss is 4.383708076477051 and perplexity is 80.13462849752636
At time: 222.40238571166992 and batch: 900, loss is 4.482218141555786 and perplexity is 88.43060689435539
At time: 222.88294553756714 and batch: 950, loss is 4.427384986877441 and perplexity is 83.71222187750703
At time: 223.36308121681213 and batch: 1000, loss is 4.386556062698364 and perplexity is 80.36317611108495
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.9085871998856705 and perplexity of 135.44791833297225
Finished 22 epochs...
Completing Train Step...
At time: 224.81533932685852 and batch: 50, loss is 4.566651935577393 and perplexity is 96.22141437762666
At time: 225.27693700790405 and batch: 100, loss is 4.490861349105835 and perplexity is 89.19824362646446
At time: 225.7350311279297 and batch: 150, loss is 4.5350316143035885 and perplexity is 93.22646254752554
At time: 226.19217920303345 and batch: 200, loss is 4.5527192497253415 and perplexity is 94.89008765369817
At time: 226.64938592910767 and batch: 250, loss is 4.523280992507934 and perplexity is 92.13740472918572
At time: 227.10586309432983 and batch: 300, loss is 4.429501152038574 and perplexity is 83.88955833544394
At time: 227.57867288589478 and batch: 350, loss is 4.482573680877685 and perplexity is 88.46205304220551
At time: 228.03460550308228 and batch: 400, loss is 4.408101034164429 and perplexity is 82.1133848446201
At time: 228.49146676063538 and batch: 450, loss is 4.448704919815063 and perplexity is 85.5161220208922
At time: 228.94772577285767 and batch: 500, loss is 4.40477294921875 and perplexity is 81.84055877067655
At time: 229.40535879135132 and batch: 550, loss is 4.451025924682617 and perplexity is 85.71483587506138
At time: 229.86171507835388 and batch: 600, loss is 4.4738436126708985 and perplexity is 87.6931345245164
At time: 230.31862664222717 and batch: 650, loss is 4.446476783752441 and perplexity is 85.32579258416503
At time: 230.77626180648804 and batch: 700, loss is 4.433189001083374 and perplexity is 84.19950152359036
At time: 231.23386526107788 and batch: 750, loss is 4.379327487945557 and perplexity is 79.78435941544203
At time: 231.692312002182 and batch: 800, loss is 4.407774515151978 and perplexity is 82.08657764006077
At time: 232.14963126182556 and batch: 850, loss is 4.384118757247925 and perplexity is 80.16754500715756
At time: 232.60731172561646 and batch: 900, loss is 4.48340726852417 and perplexity is 88.53582466008702
At time: 233.06431221961975 and batch: 950, loss is 4.429209337234497 and perplexity is 83.86508169191139
At time: 233.5221016407013 and batch: 1000, loss is 4.388136367797852 and perplexity is 80.4902748490452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.907579375476372 and perplexity of 135.31147937948998
Finished 23 epochs...
Completing Train Step...
At time: 234.94338178634644 and batch: 50, loss is 4.56446252822876 and perplexity is 96.01097695657155
At time: 235.43102860450745 and batch: 100, loss is 4.488275346755981 and perplexity is 88.96787475444667
At time: 235.89591121673584 and batch: 150, loss is 4.5327286052703855 and perplexity is 93.01200820207856
At time: 236.35567259788513 and batch: 200, loss is 4.550603456497193 and perplexity is 94.68953209069001
At time: 236.8113615512848 and batch: 250, loss is 4.521514921188355 and perplexity is 91.9748271053291
At time: 237.266375541687 and batch: 300, loss is 4.42756459236145 and perplexity is 83.7272584019155
At time: 237.72185969352722 and batch: 350, loss is 4.4808160591125485 and perplexity is 88.3067067723741
At time: 238.17803740501404 and batch: 400, loss is 4.406592235565186 and perplexity is 81.98958570205933
At time: 238.6358916759491 and batch: 450, loss is 4.447139959335328 and perplexity is 85.38239733376659
At time: 239.09065699577332 and batch: 500, loss is 4.403416681289673 and perplexity is 81.72963628282768
At time: 239.56348133087158 and batch: 550, loss is 4.449951763153076 and perplexity is 85.6228137280267
At time: 240.0226013660431 and batch: 600, loss is 4.472873830795288 and perplexity is 87.60813253539517
At time: 240.4819700717926 and batch: 650, loss is 4.4455602169036865 and perplexity is 85.2476216212678
At time: 240.94030356407166 and batch: 700, loss is 4.432329120635987 and perplexity is 84.12713113794945
At time: 241.39813017845154 and batch: 750, loss is 4.378751258850098 and perplexity is 79.73839858943865
At time: 241.85647130012512 and batch: 800, loss is 4.4076944351196286 and perplexity is 82.08000440746376
At time: 242.3153326511383 and batch: 850, loss is 4.384501342773437 and perplexity is 80.19822181737078
At time: 242.77482509613037 and batch: 900, loss is 4.484344644546509 and perplexity is 88.61885492845303
At time: 243.2342245578766 and batch: 950, loss is 4.430324935913086 and perplexity is 83.95869367321913
At time: 243.69297981262207 and batch: 1000, loss is 4.388893270492554 and perplexity is 80.55122121730125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.907014800281059 and perplexity of 135.2351074354725
Finished 24 epochs...
Completing Train Step...
At time: 245.1156346797943 and batch: 50, loss is 4.563056688308716 and perplexity is 95.87609572532806
At time: 245.5879876613617 and batch: 100, loss is 4.486442775726318 and perplexity is 88.80498410453055
At time: 246.04476165771484 and batch: 150, loss is 4.531047029495239 and perplexity is 92.85573289349975
At time: 246.50226736068726 and batch: 200, loss is 4.549084272384643 and perplexity is 94.54579047055127
At time: 246.95851635932922 and batch: 250, loss is 4.520313177108765 and perplexity is 91.86436328929665
At time: 247.4150686264038 and batch: 300, loss is 4.426168165206909 and perplexity is 83.61042098116748
At time: 247.871563911438 and batch: 350, loss is 4.479653263092041 and perplexity is 88.2040837615324
At time: 248.32741975784302 and batch: 400, loss is 4.4055428791046145 and perplexity is 81.90359452619204
At time: 248.7861008644104 and batch: 450, loss is 4.446023607254029 and perplexity is 85.28713370056781
At time: 249.24589276313782 and batch: 500, loss is 4.402440404891967 and perplexity is 81.64988450415571
At time: 249.70494627952576 and batch: 550, loss is 4.44909987449646 and perplexity is 85.54990368429311
At time: 250.1666238307953 and batch: 600, loss is 4.472176036834717 and perplexity is 87.54702143357356
At time: 250.6258192062378 and batch: 650, loss is 4.444764757156372 and perplexity is 85.1798375330449
At time: 251.1009383201599 and batch: 700, loss is 4.431785068511963 and perplexity is 84.08137404181713
At time: 251.56028127670288 and batch: 750, loss is 4.378355283737182 and perplexity is 79.70683041857095
At time: 252.01914858818054 and batch: 800, loss is 4.407589893341065 and perplexity is 82.07142406632836
At time: 252.4790403842926 and batch: 850, loss is 4.384721727371216 and perplexity is 80.21589821796019
At time: 252.93853664398193 and batch: 900, loss is 4.485053453445435 and perplexity is 88.6816910282015
At time: 253.3988494873047 and batch: 950, loss is 4.431123723983765 and perplexity is 84.02578566873048
At time: 253.85927486419678 and batch: 1000, loss is 4.389363746643067 and perplexity is 80.58912756209506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.906644123356517 and perplexity of 135.18498819135567
Finished 25 epochs...
Completing Train Step...
At time: 255.29811644554138 and batch: 50, loss is 4.561903343200684 and perplexity is 95.76558124226578
At time: 255.75484895706177 and batch: 100, loss is 4.48495114326477 and perplexity is 88.67261845248713
At time: 256.2117164134979 and batch: 150, loss is 4.529695386886597 and perplexity is 92.73030991108274
At time: 256.6693277359009 and batch: 200, loss is 4.547857828140259 and perplexity is 94.42990640719489
At time: 257.1277885437012 and batch: 250, loss is 4.5193578815460205 and perplexity is 91.77664757455929
At time: 257.58578753471375 and batch: 300, loss is 4.425185985565186 and perplexity is 83.52834084316252
At time: 258.0439693927765 and batch: 350, loss is 4.478767375946045 and perplexity is 88.12597949839308
At time: 258.50458788871765 and batch: 400, loss is 4.404731111526489 and perplexity is 81.8371348221898
At time: 258.9664466381073 and batch: 450, loss is 4.445106687545777 and perplexity is 85.20896808808916
At time: 259.4287977218628 and batch: 500, loss is 4.401668214797974 and perplexity is 81.58685960889748
At time: 259.8910036087036 and batch: 550, loss is 4.448472871780395 and perplexity is 85.49628047502969
At time: 260.3527491092682 and batch: 600, loss is 4.471702995300293 and perplexity is 87.50561784979989
At time: 260.8140461444855 and batch: 650, loss is 4.444157457351684 and perplexity is 85.12812353888638
At time: 261.27881932258606 and batch: 700, loss is 4.431317825317382 and perplexity is 84.04209676873896
At time: 261.7438237667084 and batch: 750, loss is 4.377951574325562 and perplexity is 79.67465851544752
At time: 262.20941376686096 and batch: 800, loss is 4.407422857284546 and perplexity is 82.0577163241749
At time: 262.6746070384979 and batch: 850, loss is 4.384817342758179 and perplexity is 80.22356845879962
At time: 263.1556749343872 and batch: 900, loss is 4.485546255111695 and perplexity is 88.72540428340479
At time: 263.62223649024963 and batch: 950, loss is 4.43156886100769 and perplexity is 84.06319698285884
At time: 264.0880026817322 and batch: 1000, loss is 4.3896690940856935 and perplexity is 80.61373900342876
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.906392911585366 and perplexity of 135.15103239626532
Finished 26 epochs...
Completing Train Step...
At time: 265.52236914634705 and batch: 50, loss is 4.560834970474243 and perplexity is 95.66332254206463
At time: 265.99283480644226 and batch: 100, loss is 4.483718461990357 and perplexity is 88.56338071765487
At time: 266.4473204612732 and batch: 150, loss is 4.528603525161743 and perplexity is 92.62911648962209
At time: 266.9024136066437 and batch: 200, loss is 4.546860933303833 and perplexity is 94.33581662769323
At time: 267.35768580436707 and batch: 250, loss is 4.518609933853149 and perplexity is 91.70802910745742
At time: 267.81242060661316 and batch: 300, loss is 4.424345588684082 and perplexity is 83.45817337442482
At time: 268.2668900489807 and batch: 350, loss is 4.477998495101929 and perplexity is 88.05824716327207
At time: 268.72095108032227 and batch: 400, loss is 4.404056720733642 and perplexity is 81.7819632176548
At time: 269.1757667064667 and batch: 450, loss is 4.444355249404907 and perplexity is 85.14496287051625
At time: 269.6292929649353 and batch: 500, loss is 4.400988531112671 and perplexity is 81.53142519255451
At time: 270.0839397907257 and batch: 550, loss is 4.447964658737183 and perplexity is 85.4528411892869
At time: 270.5376048088074 and batch: 600, loss is 4.471309404373169 and perplexity is 87.47118320956736
At time: 270.9948318004608 and batch: 650, loss is 4.443640327453613 and perplexity is 85.08411262169908
At time: 271.4531362056732 and batch: 700, loss is 4.430916128158569 and perplexity is 84.0083440768803
At time: 271.91461062431335 and batch: 750, loss is 4.377633323669434 and perplexity is 79.64930603753356
At time: 272.3756654262543 and batch: 800, loss is 4.407259998321533 and perplexity is 82.04435357773818
At time: 272.83746361732483 and batch: 850, loss is 4.3848875141143795 and perplexity is 80.22919805291342
At time: 273.2984027862549 and batch: 900, loss is 4.485895700454712 and perplexity is 88.75641438059043
At time: 273.7603075504303 and batch: 950, loss is 4.431965379714966 and perplexity is 84.0965362224352
At time: 274.2220482826233 and batch: 1000, loss is 4.389830055236817 and perplexity is 80.62671572800136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.906224320574505 and perplexity of 135.12824906768088
Finished 27 epochs...
Completing Train Step...
At time: 275.66805267333984 and batch: 50, loss is 4.559949989318848 and perplexity is 95.57869975466149
At time: 276.1418652534485 and batch: 100, loss is 4.482645273208618 and perplexity is 88.46838647349176
At time: 276.600932598114 and batch: 150, loss is 4.527631645202637 and perplexity is 92.53913583995377
At time: 277.06004452705383 and batch: 200, loss is 4.546000871658325 and perplexity is 94.25471689040002
At time: 277.52003598213196 and batch: 250, loss is 4.517922573089599 and perplexity is 91.64501426599226
At time: 277.98020100593567 and batch: 300, loss is 4.423587799072266 and perplexity is 83.39495359429216
At time: 278.4398009777069 and batch: 350, loss is 4.4772914695739745 and perplexity is 87.99600973889727
At time: 278.89858508110046 and batch: 400, loss is 4.403450899124145 and perplexity is 81.73243294184105
At time: 279.35845160484314 and batch: 450, loss is 4.443666534423828 and perplexity is 85.08634244772271
At time: 279.81786012649536 and batch: 500, loss is 4.400378313064575 and perplexity is 81.48168842209587
At time: 280.2771587371826 and batch: 550, loss is 4.44750189781189 and perplexity is 85.41330610178159
At time: 280.73684191703796 and batch: 600, loss is 4.470956220626831 and perplexity is 87.44029526426593
At time: 281.1967170238495 and batch: 650, loss is 4.443194961547851 and perplexity is 85.0462274958133
At time: 281.65688252449036 and batch: 700, loss is 4.4305244827270505 and perplexity is 83.97544903473012
At time: 282.1164619922638 and batch: 750, loss is 4.377341222763062 and perplexity is 79.62604380067373
At time: 282.5754671096802 and batch: 800, loss is 4.407055950164795 and perplexity is 82.0276142864887
At time: 283.03367280960083 and batch: 850, loss is 4.384880838394165 and perplexity is 80.22866246702186
At time: 283.49338388442993 and batch: 900, loss is 4.48614501953125 and perplexity is 88.77854580663931
At time: 283.9514310359955 and batch: 950, loss is 4.432260761260986 and perplexity is 84.121380456403
At time: 284.4097821712494 and batch: 1000, loss is 4.389909563064575 and perplexity is 80.63312643787557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.906079175995617 and perplexity of 135.10863735817526
Finished 28 epochs...
Completing Train Step...
At time: 285.8588252067566 and batch: 50, loss is 4.559158344268798 and perplexity is 95.50306529188461
At time: 286.31561756134033 and batch: 100, loss is 4.481660032272339 and perplexity is 88.38126672159176
At time: 286.7992990016937 and batch: 150, loss is 4.526737546920776 and perplexity is 92.4564337350088
At time: 287.25729727745056 and batch: 200, loss is 4.5452195835113525 and perplexity is 94.1811055568729
At time: 287.7171528339386 and batch: 250, loss is 4.517296228408814 and perplexity is 91.58763087155417
At time: 288.17628145217896 and batch: 300, loss is 4.422891139984131 and perplexity is 83.33687597446588
At time: 288.63579201698303 and batch: 350, loss is 4.476629428863525 and perplexity is 87.93777207807132
At time: 289.0946753025055 and batch: 400, loss is 4.402905912399292 and perplexity is 81.68790198638973
At time: 289.55395221710205 and batch: 450, loss is 4.443028411865234 and perplexity is 85.03206425309064
At time: 290.0126299858093 and batch: 500, loss is 4.399780540466309 and perplexity is 81.43299545660689
At time: 290.4715507030487 and batch: 550, loss is 4.447058534622192 and perplexity is 85.37544537959114
At time: 290.929740190506 and batch: 600, loss is 4.470611429214477 and perplexity is 87.41015179826788
At time: 291.3887941837311 and batch: 650, loss is 4.442768459320068 and perplexity is 85.0099628243531
At time: 291.84758973121643 and batch: 700, loss is 4.4301573753356935 and perplexity is 83.94462668459938
At time: 292.30807876586914 and batch: 750, loss is 4.377057609558105 and perplexity is 79.60346400530882
At time: 292.76644492149353 and batch: 800, loss is 4.406810541152954 and perplexity is 82.00748644060138
At time: 293.2251486778259 and batch: 850, loss is 4.384894275665284 and perplexity is 80.22974052855403
At time: 293.6859805583954 and batch: 900, loss is 4.486356325149536 and perplexity is 88.79730719427506
At time: 294.1444339752197 and batch: 950, loss is 4.432466506958008 and perplexity is 84.1386898490649
At time: 294.60260105133057 and batch: 1000, loss is 4.389961271286011 and perplexity is 80.63729594123039
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.905965293326029 and perplexity of 135.09325170196496
Finished 29 epochs...
Completing Train Step...
At time: 296.0338497161865 and batch: 50, loss is 4.55845085144043 and perplexity is 95.43552145431113
At time: 296.50656723976135 and batch: 100, loss is 4.480723857879639 and perplexity is 88.29856516047457
At time: 296.9644968509674 and batch: 150, loss is 4.525941925048828 and perplexity is 92.3829026294837
At time: 297.4217143058777 and batch: 200, loss is 4.544473180770874 and perplexity is 94.11083475000775
At time: 297.8794496059418 and batch: 250, loss is 4.516698522567749 and perplexity is 91.5329047662986
At time: 298.35243225097656 and batch: 300, loss is 4.422269849777222 and perplexity is 83.28511567029857
At time: 298.810968875885 and batch: 350, loss is 4.475994548797607 and perplexity is 87.88195985844527
At time: 299.2695965766907 and batch: 400, loss is 4.402426881790161 and perplexity is 81.64878035192089
At time: 299.7297327518463 and batch: 450, loss is 4.442412624359131 and perplexity is 84.97971868883171
At time: 300.1909439563751 and batch: 500, loss is 4.399242057800293 and perplexity is 81.38915700429436
At time: 300.6518249511719 and batch: 550, loss is 4.446614160537719 and perplexity is 85.33751517243886
At time: 301.1112422943115 and batch: 600, loss is 4.4702534484863286 and perplexity is 87.37886624862557
At time: 301.5716230869293 and batch: 650, loss is 4.442354307174683 and perplexity is 84.97476305540332
At time: 302.0316307544708 and batch: 700, loss is 4.429739866256714 and perplexity is 83.90958635616364
At time: 302.4917240142822 and batch: 750, loss is 4.376782474517822 and perplexity is 79.5815653157196
At time: 302.95383739471436 and batch: 800, loss is 4.406590051651001 and perplexity is 81.98940664403564
At time: 303.4175274372101 and batch: 850, loss is 4.384860200881958 and perplexity is 80.22700676410572
At time: 303.8814730644226 and batch: 900, loss is 4.486499271392822 and perplexity is 88.8100013430212
At time: 304.3465940952301 and batch: 950, loss is 4.432644481658936 and perplexity is 84.15366573985256
At time: 304.8120574951172 and batch: 1000, loss is 4.389913759231567 and perplexity is 80.63346478864914
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.905870018935785 and perplexity of 135.08038138789829
Finished 30 epochs...
Completing Train Step...
At time: 306.2474982738495 and batch: 50, loss is 4.557779731750489 and perplexity is 95.37149428409381
At time: 306.7219936847687 and batch: 100, loss is 4.479845380783081 and perplexity is 88.22103095432055
At time: 307.1825203895569 and batch: 150, loss is 4.525146951675415 and perplexity is 92.309489866197
At time: 307.6418762207031 and batch: 200, loss is 4.543795537948609 and perplexity is 94.04708282130005
At time: 308.09945821762085 and batch: 250, loss is 4.516150197982788 and perplexity is 91.48272878190242
At time: 308.55945801734924 and batch: 300, loss is 4.421756801605224 and perplexity is 83.24239735316358
At time: 309.0179171562195 and batch: 350, loss is 4.475411348342895 and perplexity is 87.8307220019177
At time: 309.4768452644348 and batch: 400, loss is 4.401963510513306 and perplexity is 81.61095541648663
At time: 309.9377529621124 and batch: 450, loss is 4.441810207366943 and perplexity is 84.92854087904111
At time: 310.41337990760803 and batch: 500, loss is 4.398746356964112 and perplexity is 81.34882232890334
At time: 310.871972322464 and batch: 550, loss is 4.446219129562378 and perplexity is 85.30381086814745
At time: 311.3308048248291 and batch: 600, loss is 4.4699525547027585 and perplexity is 87.35257844607249
At time: 311.78988099098206 and batch: 650, loss is 4.441950626373291 and perplexity is 84.94046729769025
At time: 312.2476134300232 and batch: 700, loss is 4.429401330947876 and perplexity is 83.88118480616707
At time: 312.70764899253845 and batch: 750, loss is 4.37650089263916 and perplexity is 79.55915974370072
At time: 313.1679060459137 and batch: 800, loss is 4.406376762390137 and perplexity is 81.97192104890493
At time: 313.62613463401794 and batch: 850, loss is 4.384810743331909 and perplexity is 80.22303903102139
At time: 314.08578729629517 and batch: 900, loss is 4.486629552841187 and perplexity is 88.82157239235559
At time: 314.5466558933258 and batch: 950, loss is 4.43278136253357 and perplexity is 84.16518555562594
At time: 315.00700330734253 and batch: 1000, loss is 4.38987325668335 and perplexity is 80.63019899399053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.905795957983994 and perplexity of 135.07037757673487
Finished 31 epochs...
Completing Train Step...
At time: 316.4482228755951 and batch: 50, loss is 4.557168045043945 and perplexity is 95.31317464734873
At time: 316.90456342697144 and batch: 100, loss is 4.479012145996093 and perplexity is 88.14755273893931
At time: 317.3593227863312 and batch: 150, loss is 4.524436893463135 and perplexity is 92.2439680197616
At time: 317.81009125709534 and batch: 200, loss is 4.543144674301147 and perplexity is 93.98589090989739
At time: 318.26369547843933 and batch: 250, loss is 4.515650939941406 and perplexity is 91.43706669344142
At time: 318.71747159957886 and batch: 300, loss is 4.421222715377808 and perplexity is 83.19795060545457
At time: 319.19782638549805 and batch: 350, loss is 4.47487808227539 and perplexity is 87.78389734429979
At time: 319.67885160446167 and batch: 400, loss is 4.401511726379394 and perplexity is 81.5740932091833
At time: 320.1460237503052 and batch: 450, loss is 4.4412149238586425 and perplexity is 84.87799936402944
At time: 320.6390335559845 and batch: 500, loss is 4.398184013366699 and perplexity is 81.30308919958107
At time: 321.12271761894226 and batch: 550, loss is 4.445910749435424 and perplexity is 85.27750892382923
At time: 321.61622881889343 and batch: 600, loss is 4.469640798568726 and perplexity is 87.32534998845823
At time: 322.1089677810669 and batch: 650, loss is 4.441433553695679 and perplexity is 84.89655825588818
At time: 322.57471346855164 and batch: 700, loss is 4.429016389846802 and perplexity is 83.84890170447339
At time: 323.0481560230255 and batch: 750, loss is 4.3761497974395756 and perplexity is 79.53123180760076
At time: 323.5058922767639 and batch: 800, loss is 4.406095495223999 and perplexity is 81.94886828111397
At time: 323.96244764328003 and batch: 850, loss is 4.384747371673584 and perplexity is 80.2179553250853
At time: 324.4189782142639 and batch: 900, loss is 4.486682777404785 and perplexity is 88.8263000075958
At time: 324.88979721069336 and batch: 950, loss is 4.432853031158447 and perplexity is 84.1712177748951
At time: 325.3683075904846 and batch: 1000, loss is 4.389747724533081 and perplexity is 80.62007794700592
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.90571743104516 and perplexity of 135.0597713299001
Finished 32 epochs...
Completing Train Step...
At time: 326.8166723251343 and batch: 50, loss is 4.556571683883667 and perplexity is 95.25635051746235
At time: 327.29047656059265 and batch: 100, loss is 4.478205623626709 and perplexity is 88.07648842717958
At time: 327.7495045661926 and batch: 150, loss is 4.52371111869812 and perplexity is 92.17704396438099
At time: 328.2085783481598 and batch: 200, loss is 4.542518873214721 and perplexity is 93.92709283712509
At time: 328.6662266254425 and batch: 250, loss is 4.515297260284424 and perplexity is 91.40473298128293
At time: 329.12422919273376 and batch: 300, loss is 4.420676507949829 and perplexity is 83.1525196753265
At time: 329.5829527378082 and batch: 350, loss is 4.474441556930542 and perplexity is 87.7455858108255
At time: 330.0421631336212 and batch: 400, loss is 4.401010437011719 and perplexity is 81.5332112312863
At time: 330.5010368824005 and batch: 450, loss is 4.440690431594849 and perplexity is 84.83349318259093
At time: 330.9608106613159 and batch: 500, loss is 4.397761240005493 and perplexity is 81.26872368420777
At time: 331.41914892196655 and batch: 550, loss is 4.445619506835937 and perplexity is 85.25267609681656
At time: 331.8770020008087 and batch: 600, loss is 4.469361581802368 and perplexity is 87.30097069032618
At time: 332.3347010612488 and batch: 650, loss is 4.440998935699463 and perplexity is 84.85966870086635
At time: 332.7916166782379 and batch: 700, loss is 4.428707027435303 and perplexity is 83.82296601801069
At time: 333.2508201599121 and batch: 750, loss is 4.375951023101806 and perplexity is 79.51542461075084
At time: 333.7092537879944 and batch: 800, loss is 4.405720262527466 and perplexity is 81.91812415475246
At time: 334.1838538646698 and batch: 850, loss is 4.38464937210083 and perplexity is 80.21009438492688
At time: 334.6442701816559 and batch: 900, loss is 4.486743211746216 and perplexity is 88.83166832875233
At time: 335.107786655426 and batch: 950, loss is 4.432891359329224 and perplexity is 84.17444396553107
At time: 335.5696277618408 and batch: 1000, loss is 4.389614677429199 and perplexity is 80.6093523926381
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.90566718869093 and perplexity of 135.05298577948918
Finished 33 epochs...
Completing Train Step...
At time: 336.99618339538574 and batch: 50, loss is 4.556035928726196 and perplexity is 95.20533010483574
At time: 337.4669053554535 and batch: 100, loss is 4.477406311035156 and perplexity is 88.00611590952643
At time: 337.92308735847473 and batch: 150, loss is 4.523036336898803 and perplexity is 92.11486555358825
At time: 338.3806047439575 and batch: 200, loss is 4.54192042350769 and perplexity is 93.8708990122006
At time: 338.8394215106964 and batch: 250, loss is 4.514763126373291 and perplexity is 91.35592365027945
At time: 339.29681730270386 and batch: 300, loss is 4.420184717178345 and perplexity is 83.11163608743415
At time: 339.75412154197693 and batch: 350, loss is 4.474034337997437 and perplexity is 87.70986142130447
At time: 340.2128312587738 and batch: 400, loss is 4.400486078262329 and perplexity is 81.49046978551618
At time: 340.67047476768494 and batch: 450, loss is 4.440199499130249 and perplexity is 84.79185588808825
At time: 341.128036737442 and batch: 500, loss is 4.39735297203064 and perplexity is 81.23555103909543
At time: 341.58552646636963 and batch: 550, loss is 4.445323028564453 and perplexity is 85.22740427722577
At time: 342.04307222366333 and batch: 600, loss is 4.46908242225647 and perplexity is 87.27660319235989
At time: 342.50216126441956 and batch: 650, loss is 4.440583562850952 and perplexity is 84.82442761815548
At time: 342.95973467826843 and batch: 700, loss is 4.428388319015503 and perplexity is 83.79625518967717
At time: 343.41670989990234 and batch: 750, loss is 4.375697336196899 and perplexity is 79.49525514726152
At time: 343.8752293586731 and batch: 800, loss is 4.405490856170655 and perplexity is 81.89933377173236
At time: 344.3342430591583 and batch: 850, loss is 4.384628105163574 and perplexity is 80.20838858002101
At time: 344.7927267551422 and batch: 900, loss is 4.4867447566986085 and perplexity is 88.83180556955688
At time: 345.2502498626709 and batch: 950, loss is 4.4329475402832035 and perplexity is 84.17917309893612
At time: 345.7252469062805 and batch: 1000, loss is 4.389523077011108 and perplexity is 80.60196888042847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.905604664872333 and perplexity of 135.04454201507625
Finished 34 epochs...
Completing Train Step...
At time: 347.16665506362915 and batch: 50, loss is 4.555481767654419 and perplexity is 95.15258563288226
At time: 347.62479305267334 and batch: 100, loss is 4.4766667938232425 and perplexity is 87.94105793077014
At time: 348.0808882713318 and batch: 150, loss is 4.522427453994751 and perplexity is 92.05879545853699
At time: 348.5377895832062 and batch: 200, loss is 4.54129734992981 and perplexity is 93.81242875282265
At time: 348.9941420555115 and batch: 250, loss is 4.514197092056275 and perplexity is 91.30422769465343
At time: 349.4516429901123 and batch: 300, loss is 4.419736909866333 and perplexity is 83.07442642108305
At time: 349.91087341308594 and batch: 350, loss is 4.473600931167603 and perplexity is 87.6718556049058
At time: 350.3695738315582 and batch: 400, loss is 4.40005765914917 and perplexity is 81.45556518815198
At time: 350.8287537097931 and batch: 450, loss is 4.439691801071167 and perplexity is 84.74881815343949
At time: 351.28851890563965 and batch: 500, loss is 4.397012681961059 and perplexity is 81.20791209067647
At time: 351.7501037120819 and batch: 550, loss is 4.445028018951416 and perplexity is 85.20226508200425
At time: 352.2088077068329 and batch: 600, loss is 4.468825340270996 and perplexity is 87.25416883378405
At time: 352.6680974960327 and batch: 650, loss is 4.440168409347534 and perplexity is 84.78921976869114
At time: 353.1285910606384 and batch: 700, loss is 4.4280805587768555 and perplexity is 83.77047000221347
At time: 353.5874116420746 and batch: 750, loss is 4.375429792404175 and perplexity is 79.4739895300646
At time: 354.04567289352417 and batch: 800, loss is 4.405233106613159 and perplexity is 81.87822697494413
At time: 354.5048828125 and batch: 850, loss is 4.384479293823242 and perplexity is 80.19645355026645
At time: 354.96462225914 and batch: 900, loss is 4.486741199493408 and perplexity is 88.83148957715815
At time: 355.42499256134033 and batch: 950, loss is 4.432969961166382 and perplexity is 84.18106049150066
At time: 355.88674092292786 and batch: 1000, loss is 4.389385128021241 and perplexity is 80.59085068712947
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.905540652391387 and perplexity of 135.03589775557694
Finished 35 epochs...
Completing Train Step...
At time: 357.32085061073303 and batch: 50, loss is 4.554998035430908 and perplexity is 95.10656839196977
At time: 357.7957227230072 and batch: 100, loss is 4.475957641601562 and perplexity is 87.87871644157703
At time: 358.2569706439972 and batch: 150, loss is 4.52187744140625 and perplexity is 92.00817588413027
At time: 358.71711254119873 and batch: 200, loss is 4.540766115188599 and perplexity is 93.76260556658723
At time: 359.1758406162262 and batch: 250, loss is 4.51370153427124 and perplexity is 91.25899238309388
At time: 359.63452076911926 and batch: 300, loss is 4.419249629974365 and perplexity is 83.03395578461574
At time: 360.09389638900757 and batch: 350, loss is 4.473170318603516 and perplexity is 87.6341111295714
At time: 360.55210065841675 and batch: 400, loss is 4.399649991989135 and perplexity is 81.42236519695555
At time: 361.01014161109924 and batch: 450, loss is 4.439213466644287 and perplexity is 84.7082895699605
At time: 361.46857166290283 and batch: 500, loss is 4.396599435806275 and perplexity is 81.17436016634626
At time: 361.9286279678345 and batch: 550, loss is 4.444706153869629 and perplexity is 85.17484586086682
At time: 362.38719511032104 and batch: 600, loss is 4.468564023971558 and perplexity is 87.23137087614253
At time: 362.84614276885986 and batch: 650, loss is 4.439831171035767 and perplexity is 84.76063041634366
At time: 363.30934262275696 and batch: 700, loss is 4.427821054458618 and perplexity is 83.7487340239192
At time: 363.77265644073486 and batch: 750, loss is 4.3751366996765135 and perplexity is 79.4506996949025
At time: 364.23497128486633 and batch: 800, loss is 4.404936838150024 and perplexity is 81.85397263154998
At time: 364.69787192344666 and batch: 850, loss is 4.384281406402588 and perplexity is 80.18058525104794
At time: 365.160169839859 and batch: 900, loss is 4.486706771850586 and perplexity is 88.82843137100737
At time: 365.62329626083374 and batch: 950, loss is 4.43297492980957 and perplexity is 84.18147875819258
At time: 366.0855026245117 and batch: 1000, loss is 4.389297370910644 and perplexity is 80.5837785772512
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.905516833793826 and perplexity of 135.03268142817637
Finished 36 epochs...
Completing Train Step...
At time: 367.5226650238037 and batch: 50, loss is 4.55453706741333 and perplexity is 95.0627374087963
At time: 367.99447989463806 and batch: 100, loss is 4.475315275192261 and perplexity is 87.82228423306053
At time: 368.45388007164 and batch: 150, loss is 4.521366596221924 and perplexity is 91.96118595387289
At time: 368.9124524593353 and batch: 200, loss is 4.540215902328491 and perplexity is 93.71103036517793
At time: 369.3693072795868 and batch: 250, loss is 4.513246660232544 and perplexity is 91.2174904764468
At time: 369.8497750759125 and batch: 300, loss is 4.418756628036499 and perplexity is 82.99302997258653
At time: 370.3065941333771 and batch: 350, loss is 4.472757816314697 and perplexity is 87.5979693129554
At time: 370.76586389541626 and batch: 400, loss is 4.399153757095337 and perplexity is 81.381970601642
At time: 371.2238998413086 and batch: 450, loss is 4.438723230361939 and perplexity is 84.66677267037437
At time: 371.68145394325256 and batch: 500, loss is 4.396202516555786 and perplexity is 81.1421468936071
At time: 372.139283657074 and batch: 550, loss is 4.444402322769165 and perplexity is 85.14897102470611
At time: 372.5987455844879 and batch: 600, loss is 4.468265504837036 and perplexity is 87.20533452917266
At time: 373.0567283630371 and batch: 650, loss is 4.439509620666504 and perplexity is 84.73337998576216
At time: 373.51392889022827 and batch: 700, loss is 4.427547836303711 and perplexity is 83.72585547489322
At time: 373.9717004299164 and batch: 750, loss is 4.374816493988037 and perplexity is 79.42526320157903
At time: 374.4300892353058 and batch: 800, loss is 4.4046439266204835 and perplexity is 81.83000017030403
At time: 374.8908236026764 and batch: 850, loss is 4.384178133010864 and perplexity is 80.17230515762272
At time: 375.35169076919556 and batch: 900, loss is 4.486641798019409 and perplexity is 88.82266003499872
At time: 375.8127703666687 and batch: 950, loss is 4.432907676696777 and perplexity is 84.17581748207789
At time: 376.27463579177856 and batch: 1000, loss is 4.3891535568237305 and perplexity is 80.57219032801174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.905497481183308 and perplexity of 135.03006821857173
Finished 37 epochs...
Completing Train Step...
At time: 377.7195191383362 and batch: 50, loss is 4.554103565216065 and perplexity is 95.02153643425325
At time: 378.17529940605164 and batch: 100, loss is 4.474670896530151 and perplexity is 87.76571165608124
At time: 378.63103890419006 and batch: 150, loss is 4.520838079452514 and perplexity is 91.91259576645697
At time: 379.0871424674988 and batch: 200, loss is 4.5397153663635255 and perplexity is 93.66413636121301
At time: 379.5453245639801 and batch: 250, loss is 4.512819118499756 and perplexity is 91.17849952823066
At time: 380.0028853416443 and batch: 300, loss is 4.418271036148071 and perplexity is 82.95273901370888
At time: 380.46186351776123 and batch: 350, loss is 4.472382097244263 and perplexity is 87.56506326745472
At time: 380.9204080104828 and batch: 400, loss is 4.398769187927246 and perplexity is 81.35067962206877
At time: 381.3962435722351 and batch: 450, loss is 4.4382947731018065 and perplexity is 84.6305043471998
At time: 381.85485458374023 and batch: 500, loss is 4.395823936462403 and perplexity is 81.11143390608845
At time: 382.31403613090515 and batch: 550, loss is 4.444093942642212 and perplexity is 85.12271682255671
At time: 382.77283096313477 and batch: 600, loss is 4.468042078018189 and perplexity is 87.18585269515546
At time: 383.23424243927 and batch: 650, loss is 4.439188213348388 and perplexity is 84.70615043346915
At time: 383.69283151626587 and batch: 700, loss is 4.42728624343872 and perplexity is 83.7039562529507
At time: 384.1529920101166 and batch: 750, loss is 4.3745009708404545 and perplexity is 79.40020664570565
At time: 384.6113142967224 and batch: 800, loss is 4.4043756771087645 and perplexity is 81.80805225660457
At time: 385.0731954574585 and batch: 850, loss is 4.3840350246429445 and perplexity is 80.16083265080469
At time: 385.53106904029846 and batch: 900, loss is 4.486595363616943 and perplexity is 88.8185357036107
At time: 385.9889352321625 and batch: 950, loss is 4.432906503677368 and perplexity is 84.17571874226815
At time: 386.4501883983612 and batch: 1000, loss is 4.388999872207641 and perplexity is 80.5598085733407
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.905465474942836 and perplexity of 135.02574648289897
Finished 38 epochs...
Completing Train Step...
At time: 387.87680315971375 and batch: 50, loss is 4.553689823150635 and perplexity is 94.98223015939904
At time: 388.35027146339417 and batch: 100, loss is 4.474116163253784 and perplexity is 87.71703859683258
At time: 388.8091461658478 and batch: 150, loss is 4.520347471237183 and perplexity is 91.86751375158919
At time: 389.26535964012146 and batch: 200, loss is 4.539184474945069 and perplexity is 93.61442407208312
At time: 389.7221987247467 and batch: 250, loss is 4.512421522140503 and perplexity is 91.14225449470024
At time: 390.1796669960022 and batch: 300, loss is 4.4178596782684325 and perplexity is 82.91862276835225
At time: 390.63709473609924 and batch: 350, loss is 4.471982479095459 and perplexity is 87.53007766987733
At time: 391.09280943870544 and batch: 400, loss is 4.398383073806762 and perplexity is 81.31927503922236
At time: 391.5492081642151 and batch: 450, loss is 4.437899971008301 and perplexity is 84.59709864166426
At time: 392.006844997406 and batch: 500, loss is 4.395448112487793 and perplexity is 81.08095601213195
At time: 392.4644000530243 and batch: 550, loss is 4.443784971237182 and perplexity is 85.09642039977065
At time: 392.92315220832825 and batch: 600, loss is 4.467787590026855 and perplexity is 87.16366776564918
At time: 393.39826583862305 and batch: 650, loss is 4.438858594894409 and perplexity is 84.67823432420586
At time: 393.8604791164398 and batch: 700, loss is 4.427029075622559 and perplexity is 83.68243305697234
At time: 394.32023906707764 and batch: 750, loss is 4.37418776512146 and perplexity is 79.37534194098195
At time: 394.7798342704773 and batch: 800, loss is 4.404093923568726 and perplexity is 81.78500579514112
At time: 395.23948550224304 and batch: 850, loss is 4.383836641311645 and perplexity is 80.1449316550821
At time: 395.70121932029724 and batch: 900, loss is 4.48651008605957 and perplexity is 88.8109617987831
At time: 396.16022753715515 and batch: 950, loss is 4.432834196090698 and perplexity is 84.16963241923598
At time: 396.6195878982544 and batch: 1000, loss is 4.388836441040039 and perplexity is 80.54664366557114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.905432352205602 and perplexity of 135.02127413464703
Finished 39 epochs...
Completing Train Step...
At time: 398.0479588508606 and batch: 50, loss is 4.553253307342529 and perplexity is 94.94077796237315
At time: 398.5205509662628 and batch: 100, loss is 4.47349663734436 and perplexity is 87.66271244868966
At time: 398.97864985466003 and batch: 150, loss is 4.519843130111695 and perplexity is 91.82119286805303
At time: 399.43665599823 and batch: 200, loss is 4.53875 and perplexity is 93.57375978477347
At time: 399.8950870037079 and batch: 250, loss is 4.51202392578125 and perplexity is 91.10602386919872
At time: 400.35470485687256 and batch: 300, loss is 4.41744444847107 and perplexity is 82.88419963267296
At time: 400.8129560947418 and batch: 350, loss is 4.471578578948975 and perplexity is 87.49473139734755
At time: 401.2741894721985 and batch: 400, loss is 4.398014726638794 and perplexity is 81.28932683056797
At time: 401.73509216308594 and batch: 450, loss is 4.437489175796509 and perplexity is 84.5623536956281
At time: 402.19726943969727 and batch: 500, loss is 4.395060176849365 and perplexity is 81.04950792000928
At time: 402.65946221351624 and batch: 550, loss is 4.443473806381226 and perplexity is 85.0699455036168
At time: 403.12095642089844 and batch: 600, loss is 4.467546224594116 and perplexity is 87.14263200801429
At time: 403.58244371414185 and batch: 650, loss is 4.438510093688965 and perplexity is 84.6487289990883
At time: 404.0426332950592 and batch: 700, loss is 4.42675482749939 and perplexity is 83.65948645343953
At time: 404.5053713321686 and batch: 750, loss is 4.37390100479126 and perplexity is 79.35258350498151
At time: 404.98263692855835 and batch: 800, loss is 4.403834075927734 and perplexity is 81.76375691517198
At time: 405.44587755203247 and batch: 850, loss is 4.38367356300354 and perplexity is 80.13186282087521
At time: 405.9116430282593 and batch: 900, loss is 4.486474924087524 and perplexity is 88.80783908512765
At time: 406.377724647522 and batch: 950, loss is 4.4328194522857665 and perplexity is 84.16839144774278
At time: 406.8423476219177 and batch: 1000, loss is 4.388660745620728 and perplexity is 80.53249323235777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.905364618068788 and perplexity of 135.01212889491794
Finished 40 epochs...
Completing Train Step...
At time: 408.2896912097931 and batch: 50, loss is 4.5527967262268065 and perplexity is 94.89743969051467
At time: 408.7437992095947 and batch: 100, loss is 4.47293571472168 and perplexity is 87.61355423837668
At time: 409.1980948448181 and batch: 150, loss is 4.519359846115112 and perplexity is 91.77682787630152
At time: 409.65300989151 and batch: 200, loss is 4.538232355117798 and perplexity is 93.52533434158521
At time: 410.1081852912903 and batch: 250, loss is 4.511619510650635 and perplexity is 91.06918666391951
At time: 410.5634527206421 and batch: 300, loss is 4.416957139968872 and perplexity is 82.84381929713362
At time: 411.0172007083893 and batch: 350, loss is 4.4711333274841305 and perplexity is 87.45578291160533
At time: 411.4726014137268 and batch: 400, loss is 4.397580547332764 and perplexity is 81.25404034794157
At time: 411.92812967300415 and batch: 450, loss is 4.43699125289917 and perplexity is 84.52025864438481
At time: 412.38123297691345 and batch: 500, loss is 4.394617490768432 and perplexity is 81.01363637149005
At time: 412.8344769477844 and batch: 550, loss is 4.443135719299317 and perplexity is 85.04118931530512
At time: 413.28986716270447 and batch: 600, loss is 4.467249784469605 and perplexity is 87.11680326385878
At time: 413.74391984939575 and batch: 650, loss is 4.438199157714844 and perplexity is 84.62241275562428
At time: 414.19577145576477 and batch: 700, loss is 4.426371717453003 and perplexity is 83.6274418024095
At time: 414.64911699295044 and batch: 750, loss is 4.373540019989013 and perplexity is 79.32394359791381
At time: 415.10196709632874 and batch: 800, loss is 4.403481826782227 and perplexity is 81.73496077367102
At time: 415.5692958831787 and batch: 850, loss is 4.38348240852356 and perplexity is 80.11654672022513
At time: 416.05257534980774 and batch: 900, loss is 4.486380100250244 and perplexity is 88.79941838429295
At time: 416.52777338027954 and batch: 950, loss is 4.432751893997192 and perplexity is 84.16270536733762
At time: 417.02118134498596 and batch: 1000, loss is 4.3883436489105225 and perplexity is 80.50696069204538
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.90525780654535 and perplexity of 134.99770881387724
Finished 41 epochs...
Completing Train Step...
At time: 418.55793142318726 and batch: 50, loss is 4.552408561706543 and perplexity is 94.8606110196171
At time: 419.04991340637207 and batch: 100, loss is 4.472302646636963 and perplexity is 87.55810644636531
At time: 419.51202845573425 and batch: 150, loss is 4.5188697242736815 and perplexity is 91.73185706990962
At time: 419.98102498054504 and batch: 200, loss is 4.537794485092163 and perplexity is 93.48439136554465
At time: 420.440012216568 and batch: 250, loss is 4.511223936080933 and perplexity is 91.03316913387096
At time: 420.8949599266052 and batch: 300, loss is 4.41651611328125 and perplexity is 82.80729101748419
At time: 421.351450920105 and batch: 350, loss is 4.47073504447937 and perplexity is 87.42095769520988
At time: 421.8072233200073 and batch: 400, loss is 4.3972312927246096 and perplexity is 81.22566695497645
At time: 422.2634172439575 and batch: 450, loss is 4.436616153717041 and perplexity is 84.48856110972527
At time: 422.7186779975891 and batch: 500, loss is 4.394222946166992 and perplexity is 80.9816791832992
At time: 423.17435455322266 and batch: 550, loss is 4.442816047668457 and perplexity is 85.014008404341
At time: 423.63035702705383 and batch: 600, loss is 4.467046775817871 and perplexity is 87.09911959411455
At time: 424.08634209632874 and batch: 650, loss is 4.437919282913208 and perplexity is 84.59873238856397
At time: 424.54393434524536 and batch: 700, loss is 4.426076335906982 and perplexity is 83.60274344725887
At time: 425.0059657096863 and batch: 750, loss is 4.37321907043457 and perplexity is 79.29848869864716
At time: 425.4683372974396 and batch: 800, loss is 4.403202476501465 and perplexity is 81.71213127829228
At time: 425.93045020103455 and batch: 850, loss is 4.383317308425903 and perplexity is 80.10332056238775
At time: 426.39385056495667 and batch: 900, loss is 4.48634090423584 and perplexity is 88.79593786922251
At time: 426.8566770553589 and batch: 950, loss is 4.432699556350708 and perplexity is 84.15830060468541
At time: 427.31919527053833 and batch: 1000, loss is 4.388178491592408 and perplexity is 80.49366547625921
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.905202726038491 and perplexity of 134.99027327642938
Finished 42 epochs...
Completing Train Step...
At time: 428.7551589012146 and batch: 50, loss is 4.551972312927246 and perplexity is 94.81923721914754
At time: 429.23025822639465 and batch: 100, loss is 4.471780948638916 and perplexity is 87.5124394707401
At time: 429.68892192840576 and batch: 150, loss is 4.518403100967407 and perplexity is 91.6890628326437
At time: 430.1469280719757 and batch: 200, loss is 4.5373307228088375 and perplexity is 93.44104688229483
At time: 430.605881690979 and batch: 250, loss is 4.510823907852173 and perplexity is 90.99676057917428
At time: 431.0651786327362 and batch: 300, loss is 4.416135673522949 and perplexity is 82.77579382347629
At time: 431.5234112739563 and batch: 350, loss is 4.470355472564697 and perplexity is 87.38778145169844
At time: 431.9822790622711 and batch: 400, loss is 4.3968953514099125 and perplexity is 81.1983844805442
At time: 432.4410960674286 and batch: 450, loss is 4.436294841766357 and perplexity is 84.46141828623496
At time: 432.90020751953125 and batch: 500, loss is 4.393836221694946 and perplexity is 80.95036764103175
At time: 433.3582592010498 and batch: 550, loss is 4.442505531311035 and perplexity is 84.98761426223992
At time: 433.81724977493286 and batch: 600, loss is 4.466730585098267 and perplexity is 87.07158401429
At time: 434.27576661109924 and batch: 650, loss is 4.437601261138916 and perplexity is 84.5718324272003
At time: 434.73394536972046 and batch: 700, loss is 4.425775556564331 and perplexity is 83.57760125035726
At time: 435.1938302516937 and batch: 750, loss is 4.3728883934021 and perplexity is 79.2722708447849
At time: 435.6567828655243 and batch: 800, loss is 4.402912235260009 and perplexity is 81.6884184892492
At time: 436.1191930770874 and batch: 850, loss is 4.383090400695801 and perplexity is 80.08514656173388
At time: 436.58148312568665 and batch: 900, loss is 4.486268692016601 and perplexity is 88.78952594900186
At time: 437.0420298576355 and batch: 950, loss is 4.432560062408447 and perplexity is 84.14656185032166
At time: 437.5037796497345 and batch: 1000, loss is 4.3880250453948975 and perplexity is 80.48131497696104
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.905149134193978 and perplexity of 134.98303909254162
Finished 43 epochs...
Completing Train Step...
At time: 438.9488756656647 and batch: 50, loss is 4.551552677154541 and perplexity is 94.77945602266014
At time: 439.4069538116455 and batch: 100, loss is 4.471208934783935 and perplexity is 87.46239545717881
At time: 439.86485385894775 and batch: 150, loss is 4.517944126129151 and perplexity is 91.64698951589567
At time: 440.3257327079773 and batch: 200, loss is 4.536907882690429 and perplexity is 93.401544611128
At time: 440.8001809120178 and batch: 250, loss is 4.510396595001221 and perplexity is 90.95788480063518
At time: 441.2579643726349 and batch: 300, loss is 4.415727434158325 and perplexity is 82.74200838274217
At time: 441.7168700695038 and batch: 350, loss is 4.4699867153167725 and perplexity is 87.35556251475643
At time: 442.1749861240387 and batch: 400, loss is 4.396525239944458 and perplexity is 81.16833758816422
At time: 442.63381910324097 and batch: 450, loss is 4.43595398902893 and perplexity is 84.43263428643638
At time: 443.0948758125305 and batch: 500, loss is 4.3934211158752445 and perplexity is 80.91677164574595
At time: 443.55645513534546 and batch: 550, loss is 4.442183809280396 and perplexity is 84.9602762722529
At time: 444.01798605918884 and batch: 600, loss is 4.4663545036315915 and perplexity is 87.03884416207987
At time: 444.4792447090149 and batch: 650, loss is 4.437222948074341 and perplexity is 84.53984384932805
At time: 444.94034123420715 and batch: 700, loss is 4.425423364639283 and perplexity is 83.54817107692087
At time: 445.402236700058 and batch: 750, loss is 4.372568273544312 and perplexity is 79.24689827806343
At time: 445.8678297996521 and batch: 800, loss is 4.402690486907959 and perplexity is 81.67030622532417
At time: 446.3329689502716 and batch: 850, loss is 4.38291675567627 and perplexity is 80.07124138221252
At time: 446.7986159324646 and batch: 900, loss is 4.486221809387207 and perplexity is 88.78536336013997
At time: 447.2631347179413 and batch: 950, loss is 4.432489633560181 and perplexity is 84.14063571357292
At time: 447.7282953262329 and batch: 1000, loss is 4.387805643081665 and perplexity is 80.4636591272211
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.905127920755526 and perplexity of 134.98017566852138
Finished 44 epochs...
Completing Train Step...
At time: 449.17478799819946 and batch: 50, loss is 4.551154670715332 and perplexity is 94.74174069482787
At time: 449.6504590511322 and batch: 100, loss is 4.470635719299317 and perplexity is 87.41227502405745
At time: 450.1122019290924 and batch: 150, loss is 4.517441072463989 and perplexity is 91.60089775620501
At time: 450.5718460083008 and batch: 200, loss is 4.536436882019043 and perplexity is 93.35756277945683
At time: 451.031720161438 and batch: 250, loss is 4.50991000175476 and perplexity is 90.91363607461757
At time: 451.49049043655396 and batch: 300, loss is 4.415298795700073 and perplexity is 82.70654957588339
At time: 451.9502935409546 and batch: 350, loss is 4.469600963592529 and perplexity is 87.32187145450418
At time: 452.4098901748657 and batch: 400, loss is 4.396182289123535 and perplexity is 81.14050561292763
At time: 452.8921492099762 and batch: 450, loss is 4.435522146224976 and perplexity is 84.396180532613
At time: 453.3511128425598 and batch: 500, loss is 4.392985448837281 and perplexity is 80.88152655364136
At time: 453.81155705451965 and batch: 550, loss is 4.441878232955933 and perplexity is 84.93431838956346
At time: 454.2701778411865 and batch: 600, loss is 4.466063413619995 and perplexity is 87.013511711114
At time: 454.7310128211975 and batch: 650, loss is 4.436926164627075 and perplexity is 84.51475754582309
At time: 455.1915898323059 and batch: 700, loss is 4.425141925811768 and perplexity is 83.52466068613556
At time: 455.6508321762085 and batch: 750, loss is 4.372190008163452 and perplexity is 79.2169275886988
At time: 456.1104271411896 and batch: 800, loss is 4.402336730957031 and perplexity is 81.641419978125
At time: 456.5700204372406 and batch: 850, loss is 4.3827402210235595 and perplexity is 80.05710728103907
At time: 457.02953338623047 and batch: 900, loss is 4.486128044128418 and perplexity is 88.77703876785276
At time: 457.49002742767334 and batch: 950, loss is 4.432334280014038 and perplexity is 84.12756518274304
At time: 457.9497230052948 and batch: 1000, loss is 4.387581949234009 and perplexity is 80.4456619147223
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.905115267125572 and perplexity of 134.97846769013339
Finished 45 epochs...
Completing Train Step...
At time: 459.37421584129333 and batch: 50, loss is 4.550738601684571 and perplexity is 94.70232979000139
At time: 459.8452820777893 and batch: 100, loss is 4.47008975982666 and perplexity is 87.36456448967579
At time: 460.3018193244934 and batch: 150, loss is 4.517008304595947 and perplexity is 91.56126440761079
At time: 460.75922203063965 and batch: 200, loss is 4.535992279052734 and perplexity is 93.31606495582905
At time: 461.2166094779968 and batch: 250, loss is 4.509453544616699 and perplexity is 90.87214736611313
At time: 461.6732585430145 and batch: 300, loss is 4.414907751083374 and perplexity is 82.67421394765469
At time: 462.130131483078 and batch: 350, loss is 4.469266300201416 and perplexity is 87.2926529103461
At time: 462.5883615016937 and batch: 400, loss is 4.3958094120025635 and perplexity is 81.11025581487976
At time: 463.0450527667999 and batch: 450, loss is 4.435165386199952 and perplexity is 84.36607671937212
At time: 463.50343561172485 and batch: 500, loss is 4.392557048797608 and perplexity is 80.84688432535279
At time: 463.9605848789215 and batch: 550, loss is 4.4415681934356686 and perplexity is 84.90798947594877
At time: 464.436274766922 and batch: 600, loss is 4.465730218887329 and perplexity is 86.98452409686946
At time: 464.89663887023926 and batch: 650, loss is 4.436587896347046 and perplexity is 84.48617371892449
At time: 465.35545921325684 and batch: 700, loss is 4.424821176528931 and perplexity is 83.49787450717477
At time: 465.81630086898804 and batch: 750, loss is 4.371847743988037 and perplexity is 79.18981911169365
At time: 466.27616453170776 and batch: 800, loss is 4.4019508552551265 and perplexity is 81.6099226153108
At time: 466.7362494468689 and batch: 850, loss is 4.382513608932495 and perplexity is 80.03896742798617
At time: 467.19628977775574 and batch: 900, loss is 4.4860760116577145 and perplexity is 88.77241959935833
At time: 467.65582275390625 and batch: 950, loss is 4.432198781967163 and perplexity is 84.11616683421684
At time: 468.1159236431122 and batch: 1000, loss is 4.387435741424561 and perplexity is 80.4339009905046
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.905075445407775 and perplexity of 134.97309272270527
Finished 46 epochs...
Completing Train Step...
At time: 469.557936668396 and batch: 50, loss is 4.55034761428833 and perplexity is 94.6653096103423
At time: 470.0153856277466 and batch: 100, loss is 4.469607782363892 and perplexity is 87.32246688441064
At time: 470.47383165359497 and batch: 150, loss is 4.516529054641723 and perplexity is 91.51739418907455
At time: 470.93214106559753 and batch: 200, loss is 4.535553874969483 and perplexity is 93.2751637781962
At time: 471.3880088329315 and batch: 250, loss is 4.509004173278808 and perplexity is 90.83132120141535
At time: 471.8424770832062 and batch: 300, loss is 4.414524621963501 and perplexity is 82.642545115842
At time: 472.29967522621155 and batch: 350, loss is 4.468897161483764 and perplexity is 87.26043575905517
At time: 472.7573609352112 and batch: 400, loss is 4.395393505096435 and perplexity is 81.07652851352348
At time: 473.2155454158783 and batch: 450, loss is 4.434771270751953 and perplexity is 84.3328332965535
At time: 473.67303490638733 and batch: 500, loss is 4.392200908660889 and perplexity is 80.81809663144674
At time: 474.13149189949036 and batch: 550, loss is 4.4412056350708005 and perplexity is 84.87721095396259
At time: 474.58919310569763 and batch: 600, loss is 4.465515069961548 and perplexity is 86.96581148302195
At time: 475.04803681373596 and batch: 650, loss is 4.436209020614624 and perplexity is 84.45417002107226
At time: 475.5093810558319 and batch: 700, loss is 4.424463930130005 and perplexity is 83.4680505197624
At time: 475.9867901802063 and batch: 750, loss is 4.371527395248413 and perplexity is 79.16445481587716
At time: 476.44901418685913 and batch: 800, loss is 4.401710109710693 and perplexity is 81.59027775486082
At time: 476.9102053642273 and batch: 850, loss is 4.382377271652222 and perplexity is 80.02805587669367
At time: 477.37249088287354 and batch: 900, loss is 4.48597975730896 and perplexity is 88.76387527914301
At time: 477.8356087207794 and batch: 950, loss is 4.4320219707489015 and perplexity is 84.10129546703443
At time: 478.30150747299194 and batch: 1000, loss is 4.3872467803955075 and perplexity is 80.41870355370952
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.904993568978658 and perplexity of 134.9620420602469
Finished 47 epochs...
Completing Train Step...
At time: 479.73353385925293 and batch: 50, loss is 4.549924974441528 and perplexity is 94.62530873196914
At time: 480.20442605018616 and batch: 100, loss is 4.469085025787353 and perplexity is 87.27683041998748
At time: 480.6591708660126 and batch: 150, loss is 4.516070928573608 and perplexity is 91.47547728745636
At time: 481.1149618625641 and batch: 200, loss is 4.535121812820434 and perplexity is 93.23487181542416
At time: 481.56960344314575 and batch: 250, loss is 4.508563013076782 and perplexity is 90.79125887500558
At time: 482.0240340232849 and batch: 300, loss is 4.4141032505035405 and perplexity is 82.60772924167468
At time: 482.4788773059845 and batch: 350, loss is 4.468601341247559 and perplexity is 87.2346261740245
At time: 482.9331703186035 and batch: 400, loss is 4.395026092529297 and perplexity is 81.04674544971866
At time: 483.3916630744934 and batch: 450, loss is 4.434356603622437 and perplexity is 84.29787049211032
At time: 483.8504128456116 and batch: 500, loss is 4.39184024810791 and perplexity is 80.7889539876419
At time: 484.3108186721802 and batch: 550, loss is 4.440907459259034 and perplexity is 84.851906395477
At time: 484.7720596790314 and batch: 600, loss is 4.465399837493896 and perplexity is 86.95579077532999
At time: 485.2345402240753 and batch: 650, loss is 4.435828657150268 and perplexity is 84.42205284887045
At time: 485.6964554786682 and batch: 700, loss is 4.424090518951416 and perplexity is 83.4368884351407
At time: 486.1570339202881 and batch: 750, loss is 4.371169099807739 and perplexity is 79.13609563343947
At time: 486.6182291507721 and batch: 800, loss is 4.401461343765259 and perplexity is 81.56998339665415
At time: 487.0797665119171 and batch: 850, loss is 4.3821822357177735 and perplexity is 80.01244905202898
At time: 487.54174399375916 and batch: 900, loss is 4.485903310775757 and perplexity is 88.75708984796889
At time: 488.020058631897 and batch: 950, loss is 4.431848669052124 and perplexity is 84.0867218326831
At time: 488.48208475112915 and batch: 1000, loss is 4.3870507335662845 and perplexity is 80.40293926718728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.904929556497714 and perplexity of 134.95340308160547
Finished 48 epochs...
Completing Train Step...
At time: 489.9175889492035 and batch: 50, loss is 4.549504270553589 and perplexity is 94.585507869464
At time: 490.3927090167999 and batch: 100, loss is 4.468609924316406 and perplexity is 87.23537491804011
At time: 490.8525929450989 and batch: 150, loss is 4.515595254898071 and perplexity is 91.43197515818261
At time: 491.31434750556946 and batch: 200, loss is 4.534700498580933 and perplexity is 93.1955989100083
At time: 491.77508902549744 and batch: 250, loss is 4.508117637634277 and perplexity is 90.7508316812165
At time: 492.23447728157043 and batch: 300, loss is 4.4137160682678225 and perplexity is 82.57575118744604
At time: 492.6942493915558 and batch: 350, loss is 4.468286352157593 and perplexity is 87.20715254568618
At time: 493.1530168056488 and batch: 400, loss is 4.394651927947998 and perplexity is 81.01642630067141
At time: 493.61240887641907 and batch: 450, loss is 4.433917083740234 and perplexity is 84.26082804304447
At time: 494.07126235961914 and batch: 500, loss is 4.391491689682007 and perplexity is 80.76079922408466
At time: 494.5294785499573 and batch: 550, loss is 4.440668077468872 and perplexity is 84.83159682519305
At time: 494.98861837387085 and batch: 600, loss is 4.465158758163452 and perplexity is 86.93483005821095
At time: 495.44807958602905 and batch: 650, loss is 4.435486049652099 and perplexity is 84.39313417471378
At time: 495.9073820114136 and batch: 700, loss is 4.4237635040283205 and perplexity is 83.40960778831821
At time: 496.36686635017395 and batch: 750, loss is 4.370903635025025 and perplexity is 79.11509057518225
At time: 496.82624435424805 and batch: 800, loss is 4.401205835342407 and perplexity is 81.54914424124762
At time: 497.2845792770386 and batch: 850, loss is 4.38193187713623 and perplexity is 79.99241975613617
At time: 497.74509024620056 and batch: 900, loss is 4.485780363082886 and perplexity is 88.74617803935072
At time: 498.20441937446594 and batch: 950, loss is 4.4317029857635495 and perplexity is 84.07447269479005
At time: 498.6645920276642 and batch: 1000, loss is 4.38688286781311 and perplexity is 80.38944350000001
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.904900899747523 and perplexity of 134.94953581105796
Finished 49 epochs...
Completing Train Step...
At time: 500.1060230731964 and batch: 50, loss is 4.549112958908081 and perplexity is 94.54850269948756
At time: 500.5657579898834 and batch: 100, loss is 4.468139572143555 and perplexity is 87.19435321797702
At time: 501.02372908592224 and batch: 150, loss is 4.515172309875489 and perplexity is 91.39331263602158
At time: 501.482225894928 and batch: 200, loss is 4.534320926666259 and perplexity is 93.16023119081197
At time: 501.9414007663727 and batch: 250, loss is 4.507735681533814 and perplexity is 90.71617546643137
At time: 502.4006266593933 and batch: 300, loss is 4.413327445983887 and perplexity is 82.54366664520973
At time: 502.8598437309265 and batch: 350, loss is 4.4680020236969 and perplexity is 87.18236059493707
At time: 503.3188445568085 and batch: 400, loss is 4.39430362701416 and perplexity is 80.98821311735897
At time: 503.7772409915924 and batch: 450, loss is 4.43348895072937 and perplexity is 84.22476092234992
At time: 504.2361888885498 and batch: 500, loss is 4.391179361343384 and perplexity is 80.7355792764943
At time: 504.6956162452698 and batch: 550, loss is 4.440395946502686 and perplexity is 84.80851466161407
At time: 505.1539990901947 and batch: 600, loss is 4.464841394424439 and perplexity is 86.90724447305696
At time: 505.61287212371826 and batch: 650, loss is 4.435120868682861 and perplexity is 84.3623210347074
At time: 506.0720555782318 and batch: 700, loss is 4.423417339324951 and perplexity is 83.38073932309013
At time: 506.5326325893402 and batch: 750, loss is 4.370637121200562 and perplexity is 79.0940081193281
At time: 506.9919080734253 and batch: 800, loss is 4.400940485000611 and perplexity is 81.5275080186665
At time: 507.45050954818726 and batch: 850, loss is 4.381768598556518 and perplexity is 79.97935977368738
At time: 507.90950632095337 and batch: 900, loss is 4.485680074691772 and perplexity is 88.7372782742167
At time: 508.3689579963684 and batch: 950, loss is 4.431582002639771 and perplexity is 84.06430171772408
At time: 508.8280191421509 and batch: 1000, loss is 4.386669502258301 and perplexity is 80.37229299151579
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.904859589367378 and perplexity of 134.9439611095805
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f75e40c6898>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.0, 'anneal': 8.0, 'tune_wordvecs': True, 'lr': 0.0, 'wordvec_dim': 200, 'seq_len': 20, 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7202205657958984 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 1.1852025985717773 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 1.631948471069336 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 2.093931198120117 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 2.5404577255249023 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 2.988252878189087 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 3.4357094764709473 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 3.882744789123535 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 4.330702543258667 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 4.77816367149353 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 5.224897146224976 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 5.670966386795044 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 6.116916656494141 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 6.564049005508423 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 7.010754585266113 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 7.4577248096466064 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 7.9046406745910645 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 8.351172924041748 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 8.797854900360107 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 9.244614362716675 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 1 epochs...
Completing Train Step...
At time: 10.66138744354248 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 11.106256008148193 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 11.552342653274536 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 11.99701452255249 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 12.442797660827637 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 12.8875572681427 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 13.35355806350708 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 13.799808979034424 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 14.24559998512268 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 14.6897292137146 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 15.135568380355835 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 15.580395460128784 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 16.026272773742676 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 16.471253871917725 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 16.91578459739685 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 17.361456632614136 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 17.806978702545166 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 18.252373933792114 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 18.69680643081665 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 19.14242434501648 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 2 epochs...
Completing Train Step...
At time: 20.534716606140137 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 20.997802734375 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 21.4445321559906 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 21.891220331192017 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 22.338034868240356 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 22.78458547592163 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 23.233639001846313 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 23.686866760253906 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 24.14537215232849 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 24.607072830200195 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 25.086042642593384 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 25.549408674240112 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 26.012980699539185 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 26.475360870361328 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 26.939409732818604 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 27.40341830253601 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 27.86632752418518 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 28.329859972000122 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 28.793193817138672 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 29.25697684288025 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 3 epochs...
Completing Train Step...
At time: 30.679962158203125 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 31.15263867378235 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 31.609360694885254 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 32.06575918197632 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 32.52340841293335 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 32.981324195861816 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 33.43901443481445 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 33.8965163230896 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 34.35369825363159 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 34.81113958358765 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 35.268980503082275 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 35.72673296928406 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 36.1834352016449 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 36.65620970726013 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 37.11514067649841 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 37.57286262512207 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 38.02878522872925 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 38.48647212982178 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 38.94376873970032 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 39.401012659072876 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 4 epochs...
Completing Train Step...
At time: 40.83237028121948 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 41.28631782531738 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 41.74125933647156 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 42.19507312774658 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 42.650386333465576 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 43.10382866859436 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 43.55808091163635 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 44.01228308677673 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 44.46697473526001 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 44.92085528373718 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 45.375980854034424 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 45.83321189880371 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 46.29173541069031 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 46.75081396102905 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 47.21293592453003 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 47.67468285560608 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 48.13587236404419 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 48.613117694854736 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 49.0753173828125 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 49.53712248802185 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 5 epochs...
Completing Train Step...
At time: 50.961963415145874 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 51.432541847229004 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 51.889700174331665 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 52.345112562179565 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 52.80031633377075 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 53.25476574897766 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 53.710946798324585 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 54.16524791717529 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 54.62098431587219 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 55.07716417312622 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 55.53275680541992 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 55.988216400146484 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 56.44424819946289 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 56.89944529533386 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 57.357393980026245 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 57.816880226135254 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 58.27586627006531 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 58.733659505844116 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 59.19294500350952 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 59.65100455284119 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 6 epochs...
Completing Train Step...
At time: 61.0896737575531 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 61.564321517944336 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 62.02327227592468 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 62.48126029968262 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 62.94036269187927 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 63.400424003601074 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 63.86002588272095 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 64.3192298412323 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 64.77767324447632 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 65.23816084861755 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 65.69704914093018 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 66.15573143959045 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 66.61375093460083 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 67.07259583473206 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 67.53136730194092 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 67.989572763443 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 68.44792103767395 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 68.90684509277344 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 69.3659999370575 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 69.82521462440491 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 7 epochs...
Completing Train Step...
At time: 71.25622701644897 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 71.71217179298401 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 72.18448209762573 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 72.64069104194641 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 73.09598803520203 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 73.55445504188538 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 74.01525115966797 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 74.47480726242065 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 74.9347574710846 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 75.39306282997131 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 75.85248446464539 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 76.31291246414185 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 76.77276110649109 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 77.23269438743591 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 77.69347500801086 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 78.15368914604187 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 78.613107919693 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 79.07388997077942 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 79.53367328643799 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 79.99458193778992 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 8 epochs...
Completing Train Step...
At time: 81.41580605506897 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 81.8871660232544 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 82.34374451637268 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 82.79943013191223 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 83.25538635253906 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 83.73040199279785 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 84.18721866607666 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 84.6437623500824 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 85.10054230690002 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 85.55620431900024 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 86.01231646537781 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 86.46898365020752 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 86.92508792877197 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 87.38128519058228 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 87.83708143234253 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 88.29494643211365 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 88.75204658508301 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 89.20652365684509 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 89.66103839874268 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 90.11452388763428 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 9 epochs...
Completing Train Step...
At time: 91.59635782241821 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 92.10899639129639 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 92.59446239471436 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 93.083651304245 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 93.56275796890259 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 94.0290596485138 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 94.50162720680237 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 94.95808458328247 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 95.41463780403137 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 95.89172101020813 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 96.36932349205017 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 96.84409976005554 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 97.31301093101501 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 97.78279972076416 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 98.2420449256897 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 98.70125770568848 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 99.16116642951965 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 99.62237930297852 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 100.08203554153442 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 100.54178738594055 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 10 epochs...
Completing Train Step...
At time: 101.98504829406738 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 102.44179916381836 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 102.89885330200195 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 103.35596680641174 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 103.81230449676514 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 104.26927280426025 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 104.72591137886047 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 105.18393540382385 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 105.64145994186401 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 106.09925627708435 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 106.5574381351471 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 107.0166826248169 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 107.49004554748535 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 107.94644331932068 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 108.40508079528809 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 108.86279129981995 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 109.31953358650208 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 109.77770972251892 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 110.2363383769989 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 110.6938214302063 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 11 epochs...
Completing Train Step...
At time: 112.10641407966614 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 112.57591128349304 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 113.02977013587952 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 113.4884626865387 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 113.94679689407349 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 114.40468335151672 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 114.86102843284607 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 115.31916332244873 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 115.77741169929504 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 116.23623561859131 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 116.6935784816742 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 117.1521828174591 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 117.61033511161804 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 118.06829285621643 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 118.52604269981384 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 118.9844422340393 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 119.45793652534485 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 119.91633653640747 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 120.37450838088989 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 120.83329105377197 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 12 epochs...
Completing Train Step...
At time: 122.25046396255493 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 122.72200107574463 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 123.17602205276489 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 123.63116884231567 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 124.08648490905762 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 124.54335188865662 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 125.00393056869507 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 125.4639344215393 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 125.92632508277893 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 126.39038610458374 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 126.85252690315247 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 127.31574177742004 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 127.77890658378601 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 128.24356889724731 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 128.70596432685852 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 129.16778588294983 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 129.63017892837524 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 130.09249711036682 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 130.5553286075592 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 131.03331184387207 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 13 epochs...
Completing Train Step...
At time: 132.47369861602783 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 132.93262791633606 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 133.39286422729492 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 133.85137462615967 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 134.31029534339905 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 134.76953840255737 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 135.22975707054138 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 135.68797707557678 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 136.14779496192932 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 136.60548996925354 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 137.06467938423157 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 137.52170300483704 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 137.9794886112213 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 138.43775606155396 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 138.89680337905884 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 139.3562994003296 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 139.8142991065979 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 140.2729959487915 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 140.73140859603882 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 141.190185546875 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 14 epochs...
Completing Train Step...
At time: 142.61090755462646 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 143.08226680755615 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 143.5377025604248 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 143.99556708335876 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 144.452467918396 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 144.9089012145996 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 145.3645257949829 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 145.82367968559265 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 146.280179977417 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 146.73691082000732 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 147.19622159004211 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 147.65945672988892 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 148.12013339996338 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 148.5803928375244 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 149.04117274284363 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 149.50271797180176 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 149.96360969543457 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 150.42452025413513 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 150.88696479797363 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 151.35234260559082 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 15 epochs...
Completing Train Step...
At time: 152.77983403205872 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 153.25207805633545 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 153.70615935325623 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 154.1621870994568 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 154.61902499198914 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 155.0889573097229 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 155.54456734657288 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 156.00081968307495 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 156.4576325416565 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 156.91192936897278 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 157.365731716156 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 157.82306098937988 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 158.28047370910645 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 158.73691725730896 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 159.19167256355286 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 159.64705395698547 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 160.10321521759033 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 160.55919742584229 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 161.01426649093628 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 161.4699900150299 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 16 epochs...
Completing Train Step...
At time: 162.91642546653748 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 163.37366390228271 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 163.8305299282074 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 164.28623056411743 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 164.74213814735413 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 165.19907927513123 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 165.6557538509369 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 166.11108446121216 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 166.58709859848022 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 167.04780197143555 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 167.50817251205444 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 167.96770215034485 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 168.42722535133362 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 168.88831877708435 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 169.34759426116943 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 169.80725812911987 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 170.26693058013916 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 170.72847867012024 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 171.18817615509033 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 171.64767837524414 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 17 epochs...
Completing Train Step...
At time: 173.0723237991333 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 173.54237008094788 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 173.99908900260925 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 174.45612287521362 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 174.9120306968689 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 175.36873078346252 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 175.82622170448303 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 176.28594779968262 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 176.74396276474 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 177.20553612709045 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 177.6677122116089 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 178.1279854774475 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 178.60418558120728 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 179.06509709358215 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 179.5281524658203 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 179.98919939994812 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 180.45269680023193 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 180.91796445846558 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 181.38488483428955 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 181.84978914260864 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 18 epochs...
Completing Train Step...
At time: 183.27610969543457 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 183.7471466064453 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 184.20145773887634 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 184.65471816062927 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 185.1072793006897 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 185.55818676948547 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 186.00889706611633 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 186.45969080924988 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 186.9122977256775 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 187.39001202583313 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 187.87392854690552 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 188.34142971038818 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 188.8230984210968 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 189.31625747680664 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 189.80186414718628 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 190.3082573413849 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 190.7854778766632 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 191.25202417373657 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 191.70509362220764 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 192.15765261650085 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 19 epochs...
Completing Train Step...
At time: 193.55880498886108 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 194.00981259346008 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 194.4612238407135 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 194.9120376110077 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 195.36328601837158 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 195.81403517723083 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 196.26483249664307 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 196.71580004692078 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 197.16705965995789 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 197.62177348136902 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 198.07597041130066 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 198.5299277305603 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 198.98425269126892 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 199.43824076652527 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 199.8933494091034 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 200.34684991836548 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 200.80075597763062 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 201.25557708740234 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 201.70989227294922 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 202.18068957328796 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 20 epochs...
Completing Train Step...
At time: 203.57816219329834 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 204.0500032901764 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 204.5082447528839 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 204.96634078025818 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 205.42490315437317 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 205.88162088394165 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 206.3373954296112 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 206.7941415309906 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 207.25060367584229 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 207.706960439682 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 208.1677749156952 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 208.6452374458313 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 209.10791492462158 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 209.5701026916504 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 210.03275227546692 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 210.49354195594788 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 210.9570391178131 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 211.4207329750061 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 211.88362908363342 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 212.34682703018188 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 21 epochs...
Completing Train Step...
At time: 213.76721930503845 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 214.23821020126343 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 214.69438219070435 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 215.1516523361206 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 215.61031198501587 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 216.06966066360474 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 216.53018736839294 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 216.9941098690033 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 217.4579861164093 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 217.92272853851318 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 218.38746213912964 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 218.85261511802673 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 219.3169665336609 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 219.78105187416077 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 220.24454045295715 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 220.7095446586609 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 221.17343282699585 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 221.63875317573547 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 222.1031289100647 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 222.56801557540894 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 22 epochs...
Completing Train Step...
At time: 224.0029752254486 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 224.4563431739807 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 224.91035079956055 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 225.36422991752625 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 225.83498549461365 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 226.29297614097595 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 226.74944281578064 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 227.2045624256134 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 227.65977025032043 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 228.11434984207153 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 228.5696244239807 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 229.02503848075867 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 229.48126101493835 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 229.93667197227478 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 230.3972086906433 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 230.85545015335083 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 231.31402468681335 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 231.77308082580566 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 232.2347846031189 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 232.69803023338318 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 23 epochs...
Completing Train Step...
At time: 234.12944269180298 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 234.60562014579773 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 235.06510853767395 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 235.52502870559692 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 235.98486709594727 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 236.44422459602356 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 236.90385937690735 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 237.36377954483032 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 237.8398518562317 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 238.2992067337036 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 238.75894474983215 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 239.21935105323792 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 239.67957305908203 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 240.13996052742004 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 240.60026168823242 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 241.05984735488892 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 241.5208878517151 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 241.98077654838562 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 242.44115328788757 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 242.90099024772644 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 24 epochs...
Completing Train Step...
At time: 244.3260486125946 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 244.7979030609131 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 245.2526876926422 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 245.70826411247253 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 246.1661114692688 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 246.6228485107422 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 247.0795032978058 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 247.5379180908203 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 247.996990442276 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 248.45654797554016 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 248.91726756095886 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 249.39307618141174 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 249.8523817062378 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 250.3124463558197 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 250.77246141433716 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 251.23254942893982 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 251.6925332546234 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 252.1540973186493 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 252.61532306671143 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 253.07698011398315 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 25 epochs...
Completing Train Step...
At time: 254.50925135612488 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 254.96171736717224 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 255.4141764640808 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 255.86713218688965 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 256.3193533420563 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 256.77277421951294 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 257.2337911128998 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 257.69126749038696 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 258.14990615844727 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 258.6081383228302 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 259.07144594192505 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 259.5344445705414 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 259.99753427505493 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 260.4614703655243 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 260.9419252872467 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 261.40611934661865 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 261.868905544281 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 262.33121824264526 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 262.793345451355 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 263.2562210559845 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 26 epochs...
Completing Train Step...
At time: 264.6833555698395 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 265.15879344940186 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 265.61691784858704 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 266.0770342350006 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 266.53754568099976 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 266.9965536594391 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 267.4554193019867 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 267.9149203300476 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 268.37370347976685 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 268.83377981185913 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 269.2932274341583 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 269.75098991394043 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 270.2099413871765 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 270.6693027019501 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 271.12834882736206 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 271.5861961841583 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 272.045197725296 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 272.504013299942 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 272.97925877571106 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 273.43758511543274 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 27 epochs...
Completing Train Step...
At time: 274.86131834983826 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 275.33607149124146 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 275.7964098453522 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 276.25495624542236 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 276.71381759643555 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 277.17314553260803 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 277.6319525241852 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 278.0915403366089 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 278.5491580963135 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 279.00769090652466 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 279.4659104347229 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 279.9248266220093 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 280.382972240448 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 280.8414433002472 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 281.3018915653229 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 281.76287031173706 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 282.22469663619995 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 282.68684935569763 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 283.14880871772766 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 283.6085226535797 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 28 epochs...
Completing Train Step...
At time: 285.04410433769226 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 285.4984130859375 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 285.9545347690582 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 286.41185212135315 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 286.8693518638611 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 287.3263485431671 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 287.78319668769836 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 288.24009919166565 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 288.6972336769104 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 289.15473437309265 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 289.6120455265045 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 290.0707221031189 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 290.53147292137146 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 290.99086904525757 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 291.4512827396393 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 291.91092562675476 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 292.3718843460083 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 292.83279490470886 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 293.2932457923889 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 293.7531294822693 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 29 epochs...
Completing Train Step...
At time: 295.1727330684662 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 295.6432580947876 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 296.0994348526001 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 296.57096219062805 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 297.02678871154785 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 297.48176550865173 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 297.9370172023773 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 298.3922779560089 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 298.851149559021 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 299.3098464012146 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 299.76832914352417 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 300.22927713394165 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 300.70344042778015 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 301.17640471458435 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 301.63918137550354 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 302.09997606277466 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 302.5711336135864 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 303.0423758029938 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 303.5046455860138 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 303.9677736759186 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 30 epochs...
Completing Train Step...
At time: 305.4036087989807 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 305.87522983551025 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 306.3302376270294 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 306.78574323654175 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 307.242023229599 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 307.6984214782715 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 308.1541817188263 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 308.6259744167328 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 309.08272218704224 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 309.5378620624542 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 309.9977967739105 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 310.4582169055939 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 310.91705226898193 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 311.3757472038269 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 311.83455514907837 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 312.29568457603455 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 312.75665950775146 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 313.2206950187683 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 313.6823785305023 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 314.14616346359253 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 31 epochs...
Completing Train Step...
At time: 315.5904760360718 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 316.04843258857727 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 316.5057957172394 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 316.96401596069336 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 317.4219374656677 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 317.87979459762573 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 318.33598923683167 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 318.79250860214233 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 319.2494466304779 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 319.7085454463959 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 320.1854317188263 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 320.6464273929596 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 321.1069767475128 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 321.5669915676117 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 322.027715921402 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 322.4887206554413 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 322.9489290714264 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 323.40958642959595 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 323.8716492652893 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 324.3334231376648 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 32 epochs...
Completing Train Step...
At time: 325.7507288455963 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 326.21939158439636 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 326.67301750183105 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 327.12653279304504 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 327.579398393631 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 328.03425431251526 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 328.4877610206604 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 328.9417974948883 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 329.39582109451294 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 329.8492512702942 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 330.3079152107239 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 330.7661211490631 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 331.2275309562683 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 331.68951058387756 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 332.16616201400757 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 332.6271755695343 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 333.0882816314697 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 333.54927682876587 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 334.0106794834137 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 334.47182059288025 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 33 epochs...
Completing Train Step...
At time: 335.8948199748993 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 336.36513566970825 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 336.8210175037384 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 337.27697372436523 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 337.7323031425476 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 338.18780851364136 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 338.6440691947937 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 339.09960293769836 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 339.55489897727966 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 340.01043343544006 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 340.4653420448303 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 340.920697927475 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 341.3766403198242 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 341.8359456062317 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 342.2943079471588 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 342.7525646686554 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 343.21046352386475 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 343.6853258609772 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 344.1432902812958 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 344.6037573814392 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 34 epochs...
Completing Train Step...
At time: 346.04926657676697 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 346.51196575164795 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 346.97332096099854 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 347.43494391441345 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 347.8971254825592 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 348.35966062545776 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 348.8202509880066 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 349.28229689598083 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 349.7446234226227 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 350.2073247432709 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 350.67023825645447 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 351.13196659088135 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 351.59472703933716 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 352.0582299232483 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 352.5230941772461 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 352.98531436920166 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 353.44792890548706 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 353.91021490097046 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 354.3723542690277 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 354.83469343185425 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 35 epochs...
Completing Train Step...
At time: 356.25489616394043 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 356.7274043560028 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 357.1841139793396 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 357.6415362358093 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 358.0990045070648 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 358.5570080280304 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 359.0145614147186 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 359.4721100330353 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 359.9303228855133 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 360.3868396282196 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 360.8434736728668 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 361.3023099899292 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 361.76263523101807 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 362.2227602005005 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 362.6836323738098 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 363.14468359947205 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 363.606262922287 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 364.0655381679535 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 364.52596163749695 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 364.98677611351013 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 36 epochs...
Completing Train Step...
At time: 366.4040274620056 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 366.87499809265137 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 367.3306727409363 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 367.80129981040955 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 368.2569534778595 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 368.7132134437561 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 369.16938495635986 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 369.625857591629 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 370.0825529098511 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 370.5395476818085 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 370.99511456489563 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 371.45124101638794 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 371.9071900844574 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 372.3633975982666 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 372.8197546005249 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 373.2753412723541 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 373.7324788570404 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 374.1918714046478 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 374.6500699520111 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 375.1096510887146 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 37 epochs...
Completing Train Step...
At time: 376.5625705718994 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 377.018230676651 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 377.47436594963074 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 377.92882227897644 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 378.3844780921936 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 378.83916306495667 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 379.3124420642853 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 379.7691969871521 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 380.225417137146 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 380.682519197464 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 381.1393733024597 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 381.5954327583313 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 382.0516588687897 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 382.5060431957245 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 382.9605052471161 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 383.417165517807 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 383.875324010849 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 384.33790946006775 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 384.8033857345581 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 385.2668991088867 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 38 epochs...
Completing Train Step...
At time: 386.7004146575928 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 387.17767906188965 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 387.6398375034332 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 388.101713180542 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 388.56327772140503 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 389.0241844654083 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 389.48507618904114 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 389.9451992511749 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 390.40578293800354 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 390.8669626712799 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 391.3440947532654 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 391.8047435283661 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 392.26638007164 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 392.7277863025665 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 393.1897461414337 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 393.6511709690094 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 394.11212706565857 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 394.5721113681793 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 395.03089904785156 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 395.48880076408386 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 39 epochs...
Completing Train Step...
At time: 396.8916952610016 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 397.385213136673 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 397.85742139816284 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 398.31975984573364 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 398.8049409389496 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 399.28367257118225 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 399.76775193214417 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 400.2420566082001 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 400.7113769054413 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 401.18129444122314 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 401.636492729187 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 402.094943523407 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 402.55177092552185 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 403.0419418811798 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 403.51844120025635 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 403.9830117225647 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 404.4512438774109 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 404.9086265563965 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 405.3647620677948 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 405.8263564109802 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 40 epochs...
Completing Train Step...
At time: 407.2684094905853 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 407.728178024292 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 408.1887083053589 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 408.6484696865082 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 409.10820269584656 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 409.5687735080719 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 410.02844166755676 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 410.48769760131836 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 410.9471797943115 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 411.40773367881775 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 411.86755299568176 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 412.32719326019287 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 412.78729271888733 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 413.24892354011536 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 413.71038460731506 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 414.1701543331146 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 414.6292223930359 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 415.10719323158264 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 415.5667998790741 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 416.02676701545715 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 41 epochs...
Completing Train Step...
At time: 417.44892597198486 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 417.92418909072876 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 418.38259768486023 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 418.8422269821167 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 419.30125284194946 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 419.76002287864685 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 420.21921157836914 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 420.67851758003235 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 421.1381804943085 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 421.5958249568939 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 422.0554246902466 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 422.51496744155884 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 422.9749400615692 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 423.4334409236908 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 423.893159866333 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 424.3521001338959 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 424.81024527549744 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 425.2702205181122 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 425.73325204849243 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 426.1953468322754 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 42 epochs...
Completing Train Step...
At time: 427.62886810302734 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 428.1016719341278 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 428.5565848350525 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 429.0128061771393 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 429.4700312614441 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 429.92845392227173 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 430.38929533958435 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 430.8502016067505 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 431.3113315105438 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 431.77140259742737 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 432.2294113636017 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 432.68941831588745 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 433.1507143974304 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 433.6091845035553 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 434.06890988349915 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 434.53550386428833 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 435.00452399253845 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 435.47349762916565 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 435.9359858036041 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 436.39822459220886 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 43 epochs...
Completing Train Step...
At time: 437.8293614387512 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 438.2850260734558 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 438.75613284111023 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 439.21396923065186 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 439.6728572845459 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 440.1315882205963 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 440.590172290802 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 441.04843735694885 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 441.5077540874481 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 441.9667601585388 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 442.42579460144043 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 442.8841903209686 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 443.3435139656067 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 443.80495500564575 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 444.26877522468567 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 444.73007464408875 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 445.1921136379242 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 445.6537573337555 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 446.1155228614807 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 446.57903599739075 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 44 epochs...
Completing Train Step...
At time: 448.01498103141785 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 448.4911296367645 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 448.9512183666229 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 449.4112491607666 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 449.87169647216797 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 450.34865283966064 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 450.8107011318207 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 451.2704553604126 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 451.7300226688385 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 452.1890106201172 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 452.6499717235565 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 453.1097376346588 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 453.568562746048 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 454.02903056144714 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 454.4885165691376 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 454.9471697807312 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 455.40619349479675 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 455.8660180568695 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 456.3262722492218 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 456.7854416370392 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 45 epochs...
Completing Train Step...
At time: 458.2170157432556 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 458.6902799606323 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 459.1480474472046 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 459.6066219806671 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 460.0635485649109 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 460.52112436294556 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 460.97831439971924 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 461.4373936653137 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 461.89509654045105 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 462.36920738220215 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 462.8276333808899 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 463.28659749031067 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 463.7467474937439 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 464.20689606666565 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 464.6680271625519 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 465.12966108322144 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 465.5900709629059 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 466.0503213405609 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 466.51417231559753 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 466.98038029670715 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 46 epochs...
Completing Train Step...
At time: 468.42082715034485 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 468.8786051273346 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 469.33626222610474 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 469.79338216781616 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 470.25177478790283 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 470.7090678215027 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 471.1671772003174 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 471.62499594688416 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 472.0837182998657 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 472.541362285614 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 472.9978995323181 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 473.4549651145935 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 473.9283080101013 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 474.3856098651886 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 474.84246706962585 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 475.300799369812 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 475.7613468170166 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 476.21928215026855 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 476.67638969421387 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 477.13537430763245 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 47 epochs...
Completing Train Step...
At time: 478.5517158508301 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 479.02408623695374 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 479.4791512489319 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 479.93358540534973 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 480.3882405757904 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 480.8447005748749 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 481.29988384246826 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 481.7546660900116 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 482.2108507156372 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 482.6673586368561 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 483.12231373786926 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 483.57716369628906 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 484.03280878067017 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 484.49010133743286 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 484.94711446762085 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 485.4047267436981 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 485.8789448738098 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 486.3395297527313 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 486.79846024513245 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 487.2571301460266 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 48 epochs...
Completing Train Step...
At time: 488.678777217865 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 489.1532266139984 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 489.6128399372101 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 490.071825504303 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 490.5301959514618 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 490.98960995674133 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 491.44927525520325 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 491.90684628486633 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 492.36241722106934 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 492.81710743904114 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 493.2718286514282 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 493.747358083725 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 494.2333571910858 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 494.7046468257904 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 495.18182730674744 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 495.6744067668915 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 496.16377234458923 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 496.6606423854828 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 497.1436493396759 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 497.6341416835785 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished 49 epochs...
Completing Train Step...
At time: 499.0733594894409 and batch: 50, loss is 9.176106624603271 and perplexity is 9663.45597491196
At time: 499.528156042099 and batch: 100, loss is 9.176431255340576 and perplexity is 9666.593538997327
At time: 499.9833950996399 and batch: 150, loss is 9.175780620574951 and perplexity is 9660.30616279026
At time: 500.4593231678009 and batch: 200, loss is 9.17552219390869 and perplexity is 9657.810004624367
At time: 500.9333221912384 and batch: 250, loss is 9.177336807250976 and perplexity is 9675.351105860873
At time: 501.39138770103455 and batch: 300, loss is 9.176834239959717 and perplexity is 9670.489812529457
At time: 501.8545341491699 and batch: 350, loss is 9.176166439056397 and perplexity is 9664.034006533553
At time: 502.3224470615387 and batch: 400, loss is 9.176405143737792 and perplexity is 9666.341132041953
At time: 502.7822496891022 and batch: 450, loss is 9.176947135925293 and perplexity is 9671.581633444355
At time: 503.2424063682556 and batch: 500, loss is 9.176734371185303 and perplexity is 9669.524080787914
At time: 503.7024083137512 and batch: 550, loss is 9.176182556152344 and perplexity is 9664.189763952054
At time: 504.16219663619995 and batch: 600, loss is 9.176453113555908 and perplexity is 9666.804835789706
At time: 504.62340259552 and batch: 650, loss is 9.176850929260254 and perplexity is 9670.651207587061
At time: 505.0836298465729 and batch: 700, loss is 9.175994777679444 and perplexity is 9662.375207529005
At time: 505.5437948703766 and batch: 750, loss is 9.17742582321167 and perplexity is 9676.212404868733
At time: 506.0041208267212 and batch: 800, loss is 9.176707515716553 and perplexity is 9669.264404673013
At time: 506.46534609794617 and batch: 850, loss is 9.176660194396973 and perplexity is 9668.806853148079
At time: 506.92668294906616 and batch: 900, loss is 9.176456871032714 and perplexity is 9666.841158652907
At time: 507.3872163295746 and batch: 950, loss is 9.176020164489746 and perplexity is 9662.620507529144
At time: 507.8476502895355 and batch: 1000, loss is 9.175946846008301 and perplexity is 9661.912084837315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.288410465891769 and perplexity of 10811.984414463826
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f75e40c6898>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'batch_size': 50, 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.9907225759239384, 'anneal': 3.7323286251692505, 'tune_wordvecs': True, 'lr': 2.682445178203625, 'wordvec_dim': 200, 'seq_len': 20, 'data': 'ptb'}, 'best_accuracy': -87.80340326138973}, {'params': {'batch_size': 50, 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.9146541825277869, 'anneal': 7.330669139294035, 'tune_wordvecs': True, 'lr': 5.188309388641641, 'wordvec_dim': 200, 'seq_len': 20, 'data': 'ptb'}, 'best_accuracy': -88.65080479985534}, {'params': {'batch_size': 50, 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.719666449878337, 'anneal': 2.889909243886338, 'tune_wordvecs': True, 'lr': 15.676588363372192, 'wordvec_dim': 200, 'seq_len': 20, 'data': 'ptb'}, 'best_accuracy': -131.9278745220138}, {'params': {'batch_size': 50, 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.68016409211674, 'anneal': 4.6070830650539705, 'tune_wordvecs': True, 'lr': 28.69692105694313, 'wordvec_dim': 200, 'seq_len': 20, 'data': 'ptb'}, 'best_accuracy': -156.26067107644684}, {'params': {'batch_size': 50, 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.2069436965219552, 'anneal': 7.408443099136203, 'tune_wordvecs': True, 'lr': 24.790210893573033, 'wordvec_dim': 200, 'seq_len': 20, 'data': 'ptb'}, 'best_accuracy': -134.9439611095805}, {'params': {'batch_size': 50, 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.0, 'anneal': 8.0, 'tune_wordvecs': True, 'lr': 0.0, 'wordvec_dim': 200, 'seq_len': 20, 'data': 'ptb'}, 'best_accuracy': -10811.984414463826}]
