Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'name': 'lr', 'domain': [0, 30], 'type': 'continuous'}, {'name': 'dropout', 'domain': [0, 1], 'type': 'continuous'}, {'name': 'anneal', 'domain': [2, 8], 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'anneal': 2.7983104687033795, 'data': 'ptb', 'dropout': 0.8105890619547426, 'tune_wordvecs': True, 'lr': 10.25025121514281, 'num_layers': 1, 'batch_size': 20, 'wordvec_source': '', 'wordvec_dim': 200, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.0689845085144043 and batch: 50, loss is 6.986984453201294 and perplexity is 1082.452363822095
At time: 1.5866520404815674 and batch: 100, loss is 6.385445804595947 and perplexity is 593.1491022615317
At time: 2.0920491218566895 and batch: 150, loss is 6.172482357025147 and perplexity is 479.37460926333506
At time: 2.601722478866577 and batch: 200, loss is 6.121581430435181 and perplexity is 455.5846004618596
At time: 3.118168830871582 and batch: 250, loss is 6.183424434661865 and perplexity is 484.64876594854337
At time: 3.639098882675171 and batch: 300, loss is 6.088902835845947 and perplexity is 440.93736498443525
At time: 4.148998737335205 and batch: 350, loss is 6.062526712417602 and perplexity is 429.45918698086007
At time: 4.657881498336792 and batch: 400, loss is 6.042743225097656 and perplexity is 421.0464773511336
At time: 5.164022207260132 and batch: 450, loss is 6.0775978374481205 and perplexity is 435.98063945706593
At time: 5.669121265411377 and batch: 500, loss is 6.0739053440093995 and perplexity is 434.37375234228455
At time: 6.175508737564087 and batch: 550, loss is 6.031263151168823 and perplexity is 416.240472094171
At time: 6.683415651321411 and batch: 600, loss is 5.9375934886932376 and perplexity is 379.0217111103369
At time: 7.191250801086426 and batch: 650, loss is 5.909501819610596 and perplexity is 368.52251897911157
At time: 7.699397563934326 and batch: 700, loss is 5.9925559425354 and perplexity is 400.436796486454
At time: 8.209230184555054 and batch: 750, loss is 5.86642972946167 and perplexity is 352.98647066179
At time: 8.718682765960693 and batch: 800, loss is 5.967482309341431 and perplexity is 390.521220370643
At time: 9.228968381881714 and batch: 850, loss is 5.943903541564941 and perplexity is 381.4209197529368
At time: 9.740248203277588 and batch: 900, loss is 5.966188039779663 and perplexity is 390.01610758845334
At time: 10.249519109725952 and batch: 950, loss is 5.924806213378906 and perplexity is 374.2059122341678
At time: 10.771468877792358 and batch: 1000, loss is 5.859335994720459 and perplexity is 350.4913386371769
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.665953473346989 and perplexity of 288.8632732858884
Finished 1 epochs...
Completing Train Step...
At time: 12.263328075408936 and batch: 50, loss is 5.505029230117798 and perplexity is 245.92564400930752
At time: 12.77611494064331 and batch: 100, loss is 5.300731391906738 and perplexity is 200.48338829279257
At time: 13.276079416275024 and batch: 150, loss is 5.2276677322387695 and perplexity is 186.35766031896375
At time: 13.777147054672241 and batch: 200, loss is 5.157594270706177 and perplexity is 173.74596665971126
At time: 14.288694381713867 and batch: 250, loss is 5.175784025192261 and perplexity is 176.93528162817685
At time: 14.797179460525513 and batch: 300, loss is 5.073482093811035 and perplexity is 159.729553397009
At time: 15.299378871917725 and batch: 350, loss is 5.036051073074341 and perplexity is 153.8612270450938
At time: 15.802594900131226 and batch: 400, loss is 5.026268386840821 and perplexity is 152.36338932100438
At time: 16.30503511428833 and batch: 450, loss is 5.032159996032715 and perplexity is 153.263704414136
At time: 16.806318998336792 and batch: 500, loss is 5.0488512897491455 and perplexity is 155.84334278124177
At time: 17.333173036575317 and batch: 550, loss is 4.962976026535034 and perplexity is 143.01879090184335
At time: 17.84610676765442 and batch: 600, loss is 4.87304121017456 and perplexity is 130.71785347272208
At time: 18.37869358062744 and batch: 650, loss is 4.8458830261230466 and perplexity is 127.21556708009852
At time: 18.891555070877075 and batch: 700, loss is 4.911954984664917 and perplexity is 135.9048467571383
At time: 19.39274287223816 and batch: 750, loss is 4.857438983917237 and perplexity is 128.6941918119253
At time: 19.89233899116516 and batch: 800, loss is 4.957143220901489 and perplexity is 142.1870182349037
At time: 20.39929699897766 and batch: 850, loss is 4.846735496520996 and perplexity is 127.32406082247424
At time: 20.905795097351074 and batch: 900, loss is 4.853893346786499 and perplexity is 128.23869689396307
At time: 21.411654949188232 and batch: 950, loss is 4.85296028137207 and perplexity is 128.1190976067561
At time: 21.910892963409424 and batch: 1000, loss is 4.732743034362793 and perplexity is 113.60676259068427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.926020924637958 and perplexity of 137.82998386446477
Finished 2 epochs...
Completing Train Step...
At time: 23.434723377227783 and batch: 50, loss is 4.833220338821411 and perplexity is 125.61483232500518
At time: 23.94514036178589 and batch: 100, loss is 4.718908500671387 and perplexity is 112.04588788801657
At time: 24.462730884552002 and batch: 150, loss is 4.792505216598511 and perplexity is 120.60312747658553
At time: 24.974275588989258 and batch: 200, loss is 4.794538135528565 and perplexity is 120.84855323827098
At time: 25.4844229221344 and batch: 250, loss is 4.82312352180481 and perplexity is 124.35290379981751
At time: 25.990366458892822 and batch: 300, loss is 4.727228231430054 and perplexity is 112.98196807597276
At time: 26.494374990463257 and batch: 350, loss is 4.720276918411255 and perplexity is 112.1993184232725
At time: 26.994829654693604 and batch: 400, loss is 4.729122800827026 and perplexity is 113.19622315156674
At time: 27.49540376663208 and batch: 450, loss is 4.759805097579956 and perplexity is 116.72317405276885
At time: 27.994951486587524 and batch: 500, loss is 4.788021974563598 and perplexity is 120.06364468551266
At time: 28.49419903755188 and batch: 550, loss is 4.707668046951294 and perplexity is 110.79349319936819
At time: 28.992722034454346 and batch: 600, loss is 4.634258251190186 and perplexity is 102.95152549520425
At time: 29.491422414779663 and batch: 650, loss is 4.608215761184693 and perplexity is 100.3050217672547
At time: 29.988032817840576 and batch: 700, loss is 4.686019992828369 and perplexity is 108.42080437054928
At time: 30.48515820503235 and batch: 750, loss is 4.629435958862305 and perplexity is 102.4562582651128
At time: 31.018425226211548 and batch: 800, loss is 4.74026120185852 and perplexity is 114.46409601037317
At time: 31.516191244125366 and batch: 850, loss is 4.642718477249145 and perplexity is 103.82621348590041
At time: 32.01494002342224 and batch: 900, loss is 4.628995409011841 and perplexity is 102.41113111696406
At time: 32.51313614845276 and batch: 950, loss is 4.657227325439453 and perplexity is 105.34359335449945
At time: 33.01486158370972 and batch: 1000, loss is 4.540093822479248 and perplexity is 93.69959083500082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.859598299352134 and perplexity of 128.9723834103002
Finished 3 epochs...
Completing Train Step...
At time: 34.503127336502075 and batch: 50, loss is 4.66320255279541 and perplexity is 105.9749295856457
At time: 35.00475811958313 and batch: 100, loss is 4.556438093185425 and perplexity is 95.24362600504189
At time: 35.50386714935303 and batch: 150, loss is 4.6374838924407955 and perplexity is 103.28414635202971
At time: 36.00464868545532 and batch: 200, loss is 4.656288032531738 and perplexity is 105.24469132064938
At time: 36.50640368461609 and batch: 250, loss is 4.6795148849487305 and perplexity is 107.71780436619127
At time: 37.0058753490448 and batch: 300, loss is 4.563805179595947 and perplexity is 95.94788501110806
At time: 37.506582498550415 and batch: 350, loss is 4.5682676315307615 and perplexity is 96.37700458685273
At time: 38.0088996887207 and batch: 400, loss is 4.5877805519104005 and perplexity is 98.27606929702566
At time: 38.510265588760376 and batch: 450, loss is 4.624156818389893 and perplexity is 101.91680246976978
At time: 39.01087427139282 and batch: 500, loss is 4.661776237487793 and perplexity is 105.82388366651429
At time: 39.51093053817749 and batch: 550, loss is 4.582788715362549 and perplexity is 97.78671363044445
At time: 40.014264822006226 and batch: 600, loss is 4.515737009048462 and perplexity is 91.44493693881115
At time: 40.51185059547424 and batch: 650, loss is 4.488537521362304 and perplexity is 88.99120292987983
At time: 41.012742042541504 and batch: 700, loss is 4.568065242767334 and perplexity is 96.35750093779784
At time: 41.5266489982605 and batch: 750, loss is 4.521436643600464 and perplexity is 91.96762781949167
At time: 42.03944444656372 and batch: 800, loss is 4.636732931137085 and perplexity is 103.20661307072326
At time: 42.54074239730835 and batch: 850, loss is 4.535274267196655 and perplexity is 93.24908696320203
At time: 43.03903651237488 and batch: 900, loss is 4.523720798492431 and perplexity is 92.1779362235252
At time: 43.5730414390564 and batch: 950, loss is 4.543083639144897 and perplexity is 93.98015464141922
At time: 44.07250475883484 and batch: 1000, loss is 4.437465543746948 and perplexity is 84.56035533750733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.838604438595656 and perplexity of 126.29297908206318
Finished 4 epochs...
Completing Train Step...
At time: 45.638081550598145 and batch: 50, loss is 4.579981842041016 and perplexity is 97.51262356075932
At time: 46.169102907180786 and batch: 100, loss is 4.4836150169372555 and perplexity is 88.55421974787038
At time: 46.670114278793335 and batch: 150, loss is 4.559037485122681 and perplexity is 95.49152357043715
At time: 47.17328858375549 and batch: 200, loss is 4.582711715698242 and perplexity is 97.77918437619999
At time: 47.67287492752075 and batch: 250, loss is 4.5969842910766605 and perplexity is 99.18475182918361
At time: 48.17274785041809 and batch: 300, loss is 4.482880020141602 and perplexity is 88.48915659364806
At time: 48.67309856414795 and batch: 350, loss is 4.485028219223023 and perplexity is 88.67945324292072
At time: 49.17511248588562 and batch: 400, loss is 4.505547389984131 and perplexity is 90.5178790705329
At time: 49.675068378448486 and batch: 450, loss is 4.529138088226318 and perplexity is 92.67864583110054
At time: 50.175148725509644 and batch: 500, loss is 4.571235656738281 and perplexity is 96.66347888706379
At time: 50.675111055374146 and batch: 550, loss is 4.4980971622467045 and perplexity is 89.84600616795525
At time: 51.19117617607117 and batch: 600, loss is 4.430290956497192 and perplexity is 83.9558408543177
At time: 51.7039737701416 and batch: 650, loss is 4.405433011054993 and perplexity is 81.894596432315
At time: 52.204952239990234 and batch: 700, loss is 4.47938759803772 and perplexity is 88.18065413118332
At time: 52.70213174819946 and batch: 750, loss is 4.445362749099732 and perplexity is 85.23078962257749
At time: 53.200931787490845 and batch: 800, loss is 4.55683590888977 and perplexity is 95.28152295270617
At time: 53.701353549957275 and batch: 850, loss is 4.453036546707153 and perplexity is 85.88734938351416
At time: 54.19874930381775 and batch: 900, loss is 4.431554298400879 and perplexity is 84.06197281248747
At time: 54.697922229766846 and batch: 950, loss is 4.460392322540283 and perplexity is 86.52144675241924
At time: 55.19602632522583 and batch: 1000, loss is 4.354186334609985 and perplexity is 77.80349356495068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.82920800185785 and perplexity of 125.11183306616442
Finished 5 epochs...
Completing Train Step...
At time: 56.707618713378906 and batch: 50, loss is 4.493224544525146 and perplexity is 89.4092857759165
At time: 57.22715735435486 and batch: 100, loss is 4.394543161392212 and perplexity is 81.00761490222203
At time: 57.73916268348694 and batch: 150, loss is 4.471552467346191 and perplexity is 87.492446799503
At time: 58.24327111244202 and batch: 200, loss is 4.505292930603027 and perplexity is 90.4948488772944
At time: 58.74438405036926 and batch: 250, loss is 4.5142644596099855 and perplexity is 91.31037884430829
At time: 59.24636149406433 and batch: 300, loss is 4.391448907852173 and perplexity is 80.75734420322156
At time: 59.74980640411377 and batch: 350, loss is 4.3995878124237064 and perplexity is 81.41730254706972
At time: 60.253087520599365 and batch: 400, loss is 4.4259012413024905 and perplexity is 83.58810633943725
At time: 60.75435209274292 and batch: 450, loss is 4.443727989196777 and perplexity is 85.09157157025449
At time: 61.25521373748779 and batch: 500, loss is 4.4947975730896 and perplexity is 89.55003981240567
At time: 61.75496578216553 and batch: 550, loss is 4.411267147064209 and perplexity is 82.37377708972423
At time: 62.25493597984314 and batch: 600, loss is 4.359883666038513 and perplexity is 78.24803099015169
At time: 62.75649690628052 and batch: 650, loss is 4.325785994529724 and perplexity is 75.62493028729773
At time: 63.25814199447632 and batch: 700, loss is 4.411780347824097 and perplexity is 82.41606222417498
At time: 63.760173082351685 and batch: 750, loss is 4.37736825466156 and perplexity is 79.62819627290014
At time: 64.26234793663025 and batch: 800, loss is 4.49334303855896 and perplexity is 89.41988087056382
At time: 64.76612877845764 and batch: 850, loss is 4.379411563873291 and perplexity is 79.7910676414748
At time: 65.28549838066101 and batch: 900, loss is 4.360718793869019 and perplexity is 78.31340539270576
At time: 65.81105589866638 and batch: 950, loss is 4.385636997222901 and perplexity is 80.28935102066511
At time: 66.32244110107422 and batch: 1000, loss is 4.286397142410278 and perplexity is 72.70405371021957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8386781273818595 and perplexity of 126.30228580129399
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 67.84716701507568 and batch: 50, loss is 4.400501432418824 and perplexity is 81.49172101254784
At time: 68.38212203979492 and batch: 100, loss is 4.266124386787414 and perplexity is 71.24498187097552
At time: 68.88109469413757 and batch: 150, loss is 4.325977096557617 and perplexity is 75.63938374583351
At time: 69.39306592941284 and batch: 200, loss is 4.348680467605591 and perplexity is 77.37629500532621
At time: 69.89300656318665 and batch: 250, loss is 4.34038215637207 and perplexity is 76.73685921527161
At time: 70.39436936378479 and batch: 300, loss is 4.203199896812439 and perplexity is 66.90006219550426
At time: 70.8934063911438 and batch: 350, loss is 4.217987337112427 and perplexity is 67.89669352350316
At time: 71.3931040763855 and batch: 400, loss is 4.220804619789123 and perplexity is 68.08824740599107
At time: 71.89312171936035 and batch: 450, loss is 4.248832702636719 and perplexity is 70.02362622735124
At time: 72.39063858985901 and batch: 500, loss is 4.284819293022156 and perplexity is 72.58942811829996
At time: 72.89594912528992 and batch: 550, loss is 4.184657859802246 and perplexity is 65.67102837394222
At time: 73.41903638839722 and batch: 600, loss is 4.1202452182769775 and perplexity is 61.57433956674022
At time: 73.92558288574219 and batch: 650, loss is 4.085372023582458 and perplexity is 59.46405566781254
At time: 74.42386293411255 and batch: 700, loss is 4.152826910018921 and perplexity is 63.61357621199248
At time: 74.92401671409607 and batch: 750, loss is 4.109454455375672 and perplexity is 60.91347748328127
At time: 75.4229531288147 and batch: 800, loss is 4.207403516769409 and perplexity is 67.18187653728606
At time: 75.92223238945007 and batch: 850, loss is 4.087290706634522 and perplexity is 59.57825786750706
At time: 76.42074155807495 and batch: 900, loss is 4.05841628074646 and perplexity is 57.88256871254429
At time: 76.92173099517822 and batch: 950, loss is 4.062905945777893 and perplexity is 58.14302630338591
At time: 77.42119598388672 and batch: 1000, loss is 3.9695834875106812 and perplexity is 52.962466716151226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.718484645936547 and perplexity of 111.99840677117483
Finished 7 epochs...
Completing Train Step...
At time: 78.90385842323303 and batch: 50, loss is 4.242449707984925 and perplexity is 69.57808923844587
At time: 79.41572499275208 and batch: 100, loss is 4.134831719398498 and perplexity is 62.47907616924435
At time: 79.91480469703674 and batch: 150, loss is 4.20547089099884 and perplexity is 67.0521644941669
At time: 80.41468548774719 and batch: 200, loss is 4.238738160133362 and perplexity is 69.32032547798664
At time: 80.91437482833862 and batch: 250, loss is 4.231776700019837 and perplexity is 68.83943060444496
At time: 81.41463303565979 and batch: 300, loss is 4.09553147315979 and perplexity is 60.07125693571423
At time: 81.91413617134094 and batch: 350, loss is 4.118803949356079 and perplexity is 61.485658306920776
At time: 82.4266893863678 and batch: 400, loss is 4.122250485420227 and perplexity is 61.69793644770664
At time: 82.92592453956604 and batch: 450, loss is 4.160556349754334 and perplexity is 64.10717869350464
At time: 83.42589068412781 and batch: 500, loss is 4.1998110151290895 and perplexity is 66.6737295240474
At time: 83.92426323890686 and batch: 550, loss is 4.106499962806701 and perplexity is 60.73377466274991
At time: 84.42412304878235 and batch: 600, loss is 4.047198600769043 and perplexity is 57.23688886621652
At time: 84.92374420166016 and batch: 650, loss is 4.017256112098694 and perplexity is 55.54847772938342
At time: 85.4209418296814 and batch: 700, loss is 4.096328930854797 and perplexity is 60.11918032770556
At time: 85.92898082733154 and batch: 750, loss is 4.050903253555298 and perplexity is 57.449324923983944
At time: 86.44032263755798 and batch: 800, loss is 4.150692238807678 and perplexity is 63.47792697697771
At time: 86.95169568061829 and batch: 850, loss is 4.0364189004898074 and perplexity is 56.62320596079996
At time: 87.4560329914093 and batch: 900, loss is 4.005625314712525 and perplexity is 54.90614728835531
At time: 87.95400285720825 and batch: 950, loss is 4.022646322250366 and perplexity is 55.848704112692936
At time: 88.46401476860046 and batch: 1000, loss is 3.9333157396316527 and perplexity is 51.07605210108195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.71238671279535 and perplexity of 111.31752606713461
Finished 8 epochs...
Completing Train Step...
At time: 89.97182011604309 and batch: 50, loss is 4.176323466300964 and perplexity is 65.12597468285523
At time: 90.47893738746643 and batch: 100, loss is 4.070719075202942 and perplexity is 58.59908459428051
At time: 90.97872734069824 and batch: 150, loss is 4.141691932678222 and perplexity is 62.90916953662868
At time: 91.47749543190002 and batch: 200, loss is 4.1752386569976805 and perplexity is 65.0553637262698
At time: 91.97706341743469 and batch: 250, loss is 4.170643639564514 and perplexity is 64.75711894058753
At time: 92.47713232040405 and batch: 300, loss is 4.033744540214538 and perplexity is 56.47197741805837
At time: 92.97643685340881 and batch: 350, loss is 4.06175684928894 and perplexity is 58.07625272799157
At time: 93.47532749176025 and batch: 400, loss is 4.066942253112793 and perplexity is 58.37818369141173
At time: 93.97636008262634 and batch: 450, loss is 4.107739663124084 and perplexity is 60.8091130313262
At time: 94.47491216659546 and batch: 500, loss is 4.14813334941864 and perplexity is 63.315701630077434
At time: 94.98696374893188 and batch: 550, loss is 4.055680294036865 and perplexity is 57.724419219938426
At time: 95.4856824874878 and batch: 600, loss is 4.000672469139099 and perplexity is 54.6348779519053
At time: 95.98493957519531 and batch: 650, loss is 3.970228056907654 and perplexity is 52.996615705899174
At time: 96.48509526252747 and batch: 700, loss is 4.050152983665466 and perplexity is 57.4062385905128
At time: 96.9852032661438 and batch: 750, loss is 4.010183782577514 and perplexity is 55.157006528856215
At time: 97.48493146896362 and batch: 800, loss is 4.1080570316314695 and perplexity is 60.828414991519516
At time: 97.98388862609863 and batch: 850, loss is 3.998732023239136 and perplexity is 54.528964719670356
At time: 98.4843077659607 and batch: 900, loss is 3.9659018611907957 and perplexity is 52.76783720113556
At time: 98.98693990707397 and batch: 950, loss is 3.9850579118728637 and perplexity is 53.78840437492981
At time: 99.48710942268372 and batch: 1000, loss is 3.903186402320862 and perplexity is 49.56011624608597
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.718146347418064 and perplexity of 111.96052428424758
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 100.96440768241882 and batch: 50, loss is 4.1247361326217655 and perplexity is 61.85148650735829
At time: 101.4782977104187 and batch: 100, loss is 4.006745233535766 and perplexity is 54.967672161216356
At time: 101.97937989234924 and batch: 150, loss is 4.069818615913391 and perplexity is 58.546342253927314
At time: 102.48190641403198 and batch: 200, loss is 4.102532577514649 and perplexity is 60.493297726181105
At time: 102.98315286636353 and batch: 250, loss is 4.089717025756836 and perplexity is 59.72298924509659
At time: 103.48166131973267 and batch: 300, loss is 3.946619210243225 and perplexity is 51.760080748281005
At time: 103.98040270805359 and batch: 350, loss is 3.9678421020507812 and perplexity is 52.87031890240872
At time: 104.48111414909363 and batch: 400, loss is 3.971653265953064 and perplexity is 53.07220081147326
At time: 104.9886372089386 and batch: 450, loss is 4.00961130619049 and perplexity is 55.12543948159622
At time: 105.49275183677673 and batch: 500, loss is 4.046841835975647 and perplexity is 57.21647240153995
At time: 105.99525594711304 and batch: 550, loss is 3.9472586059570314 and perplexity is 51.793186504769245
At time: 106.49633002281189 and batch: 600, loss is 3.883870759010315 and perplexity is 48.612016771228504
At time: 106.99914407730103 and batch: 650, loss is 3.852531189918518 and perplexity is 47.112162267022065
At time: 107.50015020370483 and batch: 700, loss is 3.9291779470443724 and perplexity is 50.865146633765235
At time: 108.01409220695496 and batch: 750, loss is 3.881928896903992 and perplexity is 48.51771053243864
At time: 108.51532769203186 and batch: 800, loss is 3.9731079816818236 and perplexity is 53.14946195963233
At time: 109.01718044281006 and batch: 850, loss is 3.8607928276062013 and perplexity is 47.502998131481164
At time: 109.51788759231567 and batch: 900, loss is 3.8239827966690063 and perplexity is 45.78620279275348
At time: 110.02250051498413 and batch: 950, loss is 3.830515103340149 and perplexity is 46.0862713134841
At time: 110.54016447067261 and batch: 1000, loss is 3.750700607299805 and perplexity is 42.550883018694236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.667387055187691 and perplexity of 106.41931104122187
Finished 10 epochs...
Completing Train Step...
At time: 112.04528045654297 and batch: 50, loss is 4.061386332511902 and perplexity is 58.05473848794791
At time: 112.55853486061096 and batch: 100, loss is 3.9497011137008666 and perplexity is 51.91984638478451
At time: 113.05773186683655 and batch: 150, loss is 4.019471030235291 and perplexity is 55.6716494174142
At time: 113.55707836151123 and batch: 200, loss is 4.055038475990296 and perplexity is 57.68738253263388
At time: 114.05471205711365 and batch: 250, loss is 4.045967483520508 and perplexity is 57.16646690282233
At time: 114.55194544792175 and batch: 300, loss is 3.9033786153793333 and perplexity is 49.56964326318705
At time: 115.04961705207825 and batch: 350, loss is 3.926779203414917 and perplexity is 50.743280408666294
At time: 115.54727077484131 and batch: 400, loss is 3.933749589920044 and perplexity is 51.098216268633486
At time: 116.04366898536682 and batch: 450, loss is 3.9741183042526247 and perplexity is 53.20318719601555
At time: 116.54037714004517 and batch: 500, loss is 4.012982583045959 and perplexity is 55.31159621650022
At time: 117.03793406486511 and batch: 550, loss is 3.9172919368743897 and perplexity is 50.26414183383557
At time: 117.53367161750793 and batch: 600, loss is 3.856272120475769 and perplexity is 47.28873566294892
At time: 118.0378885269165 and batch: 650, loss is 3.828415699005127 and perplexity is 45.989619087254475
At time: 118.54428911209106 and batch: 700, loss is 3.906532340049744 and perplexity is 49.72621903871772
At time: 119.04310154914856 and batch: 750, loss is 3.862094073295593 and perplexity is 47.56485143747913
At time: 119.54311418533325 and batch: 800, loss is 3.956788783073425 and perplexity is 52.289144274531395
At time: 120.03993344306946 and batch: 850, loss is 3.8462371826171875 and perplexity is 46.81656918206383
At time: 120.56267285346985 and batch: 900, loss is 3.8129153442382813 and perplexity is 45.282259996745196
At time: 121.06123948097229 and batch: 950, loss is 3.82034640789032 and perplexity is 45.62000871497547
At time: 121.5599434375763 and batch: 1000, loss is 3.7437582874298094 and perplexity is 42.25650419631939
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.664183081650153 and perplexity of 106.07889202273896
Finished 11 epochs...
Completing Train Step...
At time: 123.05208230018616 and batch: 50, loss is 4.032680759429931 and perplexity is 56.411935554964494
At time: 123.54966568946838 and batch: 100, loss is 3.9227991676330567 and perplexity is 50.54172170843926
At time: 124.0467517375946 and batch: 150, loss is 3.9911285066604614 and perplexity is 54.11592509914325
At time: 124.54621815681458 and batch: 200, loss is 4.028603620529175 and perplexity is 56.18240449102026
At time: 125.04587244987488 and batch: 250, loss is 4.019767198562622 and perplexity is 55.68814003858123
At time: 125.54463195800781 and batch: 300, loss is 3.87741849899292 and perplexity is 48.29936912567132
At time: 126.0430428981781 and batch: 350, loss is 3.902280640602112 and perplexity is 49.51524691354294
At time: 126.54227614402771 and batch: 400, loss is 3.9104084300994875 and perplexity is 49.91933636984384
At time: 127.04290819168091 and batch: 450, loss is 3.951115412712097 and perplexity is 51.99332852279882
At time: 127.54284954071045 and batch: 500, loss is 3.991969208717346 and perplexity is 54.161439598070025
At time: 128.04365706443787 and batch: 550, loss is 3.898074202537537 and perplexity is 49.30740154495397
At time: 128.54328417778015 and batch: 600, loss is 3.8380605459213255 and perplexity is 46.43532786445336
At time: 129.04308152198792 and batch: 650, loss is 3.8118332719802854 and perplexity is 45.23328781991872
At time: 129.5421109199524 and batch: 700, loss is 3.8900945568084717 and perplexity is 48.91551159972386
At time: 130.0402979850769 and batch: 750, loss is 3.8465083980560304 and perplexity is 46.82926828043759
At time: 130.5390338897705 and batch: 800, loss is 3.943158006668091 and perplexity is 51.581238255426996
At time: 131.03891921043396 and batch: 850, loss is 3.8334330320358276 and perplexity is 46.220944154346164
At time: 131.53708267211914 and batch: 900, loss is 3.801100640296936 and perplexity is 44.7504115039212
At time: 132.03428411483765 and batch: 950, loss is 3.8082306385040283 and perplexity is 45.070622051485024
At time: 132.5329294204712 and batch: 1000, loss is 3.7331744241714477 and perplexity is 41.81162555435195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.665510968464177 and perplexity of 106.2198463496864
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 134.00537204742432 and batch: 50, loss is 4.011897358894348 and perplexity is 55.251603295193355
At time: 134.5201337337494 and batch: 100, loss is 3.8991659545898436 and perplexity is 49.361262397783065
At time: 135.01985025405884 and batch: 150, loss is 3.9652744483947755 and perplexity is 52.73474036863107
At time: 135.52131271362305 and batch: 200, loss is 4.001731543540955 and perplexity is 54.692771003706035
At time: 136.0211260318756 and batch: 250, loss is 3.991379346847534 and perplexity is 54.129501250569724
At time: 136.52134895324707 and batch: 300, loss is 3.845595006942749 and perplexity is 46.786514371441434
At time: 137.02122688293457 and batch: 350, loss is 3.866838936805725 and perplexity is 47.79107644438929
At time: 137.52184915542603 and batch: 400, loss is 3.8743982410430906 and perplexity is 48.15371264289547
At time: 138.02496910095215 and batch: 450, loss is 3.912775368690491 and perplexity is 50.03763231792884
At time: 138.52649593353271 and batch: 500, loss is 3.9497102451324464 and perplexity is 51.92032048947404
At time: 139.02761673927307 and batch: 550, loss is 3.8557313013076784 and perplexity is 47.263167922652826
At time: 139.5292820930481 and batch: 600, loss is 3.790979199409485 and perplexity is 44.2997573411415
At time: 140.03841924667358 and batch: 650, loss is 3.7628497838974 and perplexity is 43.07099427516386
At time: 140.54832482337952 and batch: 700, loss is 3.8390710496902467 and perplexity is 46.482274654230636
At time: 141.0483434200287 and batch: 750, loss is 3.7935452222824098 and perplexity is 44.41357750185797
At time: 141.5680091381073 and batch: 800, loss is 3.888139314651489 and perplexity is 48.81996336971722
At time: 142.07252049446106 and batch: 850, loss is 3.7759828567504883 and perplexity is 43.64037948573434
At time: 142.57243275642395 and batch: 900, loss is 3.7406018209457397 and perplexity is 42.12333324248426
At time: 143.08515095710754 and batch: 950, loss is 3.74322274684906 and perplexity is 42.233880182101245
At time: 143.5971393585205 and batch: 1000, loss is 3.6658977460861206 and perplexity is 39.091214399797686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.650535769578887 and perplexity of 104.64103404398581
Finished 13 epochs...
Completing Train Step...
At time: 145.06236290931702 and batch: 50, loss is 3.9865961408615114 and perplexity is 53.87120692610778
At time: 145.5729489326477 and batch: 100, loss is 3.8758075046539306 and perplexity is 48.221621757532915
At time: 146.07115769386292 and batch: 150, loss is 3.9447144365310667 and perplexity is 51.66158334454554
At time: 146.58311915397644 and batch: 200, loss is 3.9826235342025758 and perplexity is 53.65762233542404
At time: 147.0829074382782 and batch: 250, loss is 3.973333115577698 and perplexity is 53.16142905211546
At time: 147.58199739456177 and batch: 300, loss is 3.8288555049896242 and perplexity is 46.00985004547595
At time: 148.08003401756287 and batch: 350, loss is 3.850677938461304 and perplexity is 47.024932438018254
At time: 148.57846760749817 and batch: 400, loss is 3.8594476127624513 and perplexity is 47.439139354789134
At time: 149.0766041278839 and batch: 450, loss is 3.8988731002807615 and perplexity is 49.34680885588246
At time: 149.5758557319641 and batch: 500, loss is 3.9378138732910157 and perplexity is 51.30631650200257
At time: 150.08891487121582 and batch: 550, loss is 3.8449266481399538 and perplexity is 46.755254640231094
At time: 150.59431672096252 and batch: 600, loss is 3.7819068670272826 and perplexity is 43.899672812155565
At time: 151.09367609024048 and batch: 650, loss is 3.754630403518677 and perplexity is 42.7184283116993
At time: 151.59191250801086 and batch: 700, loss is 3.8317530059814455 and perplexity is 46.1433569564148
At time: 152.0920045375824 and batch: 750, loss is 3.7886636304855346 and perplexity is 44.197296872623774
At time: 152.59006595611572 and batch: 800, loss is 3.8838415908813477 and perplexity is 48.61059887033282
At time: 153.08985495567322 and batch: 850, loss is 3.7728408479690554 and perplexity is 43.50347621842738
At time: 153.5891411304474 and batch: 900, loss is 3.7391165828704835 and perplexity is 42.06081650170999
At time: 154.0887486934662 and batch: 950, loss is 3.741729373931885 and perplexity is 42.170856320027234
At time: 154.58825612068176 and batch: 1000, loss is 3.665864782333374 and perplexity is 39.08992582790985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.649736730063834 and perplexity of 104.55745511876742
Finished 14 epochs...
Completing Train Step...
At time: 156.06206917762756 and batch: 50, loss is 3.974612159729004 and perplexity is 53.22946837038992
At time: 156.55909967422485 and batch: 100, loss is 3.8644090843200685 and perplexity is 47.67509214793025
At time: 157.0563678741455 and batch: 150, loss is 3.9333207845687865 and perplexity is 51.076309777203825
At time: 157.55472803115845 and batch: 200, loss is 3.9714845514297483 and perplexity is 53.063247515708795
At time: 158.05407619476318 and batch: 250, loss is 3.962934308052063 and perplexity is 52.61147795715428
At time: 158.55398654937744 and batch: 300, loss is 3.8187623405456543 and perplexity is 45.54780075514142
At time: 159.0681345462799 and batch: 350, loss is 3.840700364112854 and perplexity is 46.558070625687364
At time: 159.56620454788208 and batch: 400, loss is 3.850262637138367 and perplexity is 47.00540697612143
At time: 160.06510376930237 and batch: 450, loss is 3.8898413515090944 and perplexity is 48.903127500890946
At time: 160.56434512138367 and batch: 500, loss is 3.9296290254592896 and perplexity is 50.88809597907135
At time: 161.06319046020508 and batch: 550, loss is 3.83744571685791 and perplexity is 46.4067868501347
At time: 161.56213879585266 and batch: 600, loss is 3.7753754568099978 and perplexity is 43.613880370426955
At time: 162.06210851669312 and batch: 650, loss is 3.747978935241699 and perplexity is 42.43523092426252
At time: 162.5613350868225 and batch: 700, loss is 3.8256154918670653 and perplexity is 45.861018765410506
At time: 163.0611114501953 and batch: 750, loss is 3.7836461591720583 and perplexity is 43.97609360801566
At time: 163.56076192855835 and batch: 800, loss is 3.8793803215026856 and perplexity is 48.39421692208011
At time: 164.0612576007843 and batch: 850, loss is 3.768597264289856 and perplexity is 43.31925672862059
At time: 164.5609016418457 and batch: 900, loss is 3.7356873655319216 and perplexity is 41.91682784578921
At time: 165.06759977340698 and batch: 950, loss is 3.7388580799102784 and perplexity is 42.04994506134595
At time: 165.57339668273926 and batch: 1000, loss is 3.6634621524810793 and perplexity is 38.99611994072971
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.649747895031441 and perplexity of 104.55862250588373
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 167.06423687934875 and batch: 50, loss is 3.9658597469329835 and perplexity is 52.765614969629624
At time: 167.57814264297485 and batch: 100, loss is 3.855491247177124 and perplexity is 47.251823565654256
At time: 168.0785038471222 and batch: 150, loss is 3.9242557382583616 and perplexity is 50.61539293628025
At time: 168.57767367362976 and batch: 200, loss is 3.9606472492218017 and perplexity is 52.49128990288302
At time: 169.07624459266663 and batch: 250, loss is 3.9518946838378906 and perplexity is 52.03386121337707
At time: 169.5760269165039 and batch: 300, loss is 3.8063396072387694 and perplexity is 44.9854726315232
At time: 170.07501482963562 and batch: 350, loss is 3.8275106048583982 and perplexity is 45.94801298376898
At time: 170.5751302242279 and batch: 400, loss is 3.836571664810181 and perplexity is 46.36624262452272
At time: 171.07634782791138 and batch: 450, loss is 3.8757236671447752 and perplexity is 48.21757914634092
At time: 171.57645654678345 and batch: 500, loss is 3.9137459325790407 and perplexity is 50.08622061213287
At time: 172.09029078483582 and batch: 550, loss is 3.8215978956222534 and perplexity is 45.677137336641195
At time: 172.5901517868042 and batch: 600, loss is 3.757628149986267 and perplexity is 42.846679465449355
At time: 173.08913278579712 and batch: 650, loss is 3.7274942541122438 and perplexity is 41.574801647530634
At time: 173.58649849891663 and batch: 700, loss is 3.804064841270447 and perplexity is 44.88325751112729
At time: 174.08522152900696 and batch: 750, loss is 3.7612623262405394 and perplexity is 43.002675136730005
At time: 174.5863001346588 and batch: 800, loss is 3.855398302078247 and perplexity is 47.24743194433383
At time: 175.08601641654968 and batch: 850, loss is 3.744664430618286 and perplexity is 42.294811993319186
At time: 175.58623218536377 and batch: 900, loss is 3.7097226095199587 and perplexity is 40.84247564159875
At time: 176.08594298362732 and batch: 950, loss is 3.711758418083191 and perplexity is 40.92570779687433
At time: 176.5852222442627 and batch: 1000, loss is 3.6353966665267943 and perplexity is 37.91689027570511
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.644323209436928 and perplexity of 103.992960508991
Finished 16 epochs...
Completing Train Step...
At time: 178.07224941253662 and batch: 50, loss is 3.9567554092407224 and perplexity is 52.287399214498045
At time: 178.58724308013916 and batch: 100, loss is 3.847406234741211 and perplexity is 46.87133219587744
At time: 179.08925032615662 and batch: 150, loss is 3.916760654449463 and perplexity is 50.23744447122298
At time: 179.5903491973877 and batch: 200, loss is 3.953347430229187 and perplexity is 52.1095081520844
At time: 180.0902853012085 and batch: 250, loss is 3.945265474319458 and perplexity is 51.690058673947256
At time: 180.59231853485107 and batch: 300, loss is 3.799515099525452 and perplexity is 44.6795141221384
At time: 181.09394550323486 and batch: 350, loss is 3.8211880207061766 and perplexity is 45.65841926010579
At time: 181.59468293190002 and batch: 400, loss is 3.8309015369415285 and perplexity is 46.10408403877805
At time: 182.0952045917511 and batch: 450, loss is 3.8707991886138915 and perplexity is 47.98071640443679
At time: 182.59786772727966 and batch: 500, loss is 3.9086605739593505 and perplexity is 49.832160758665346
At time: 183.11611914634705 and batch: 550, loss is 3.816916613578796 and perplexity is 45.463809487325044
At time: 183.62114429473877 and batch: 600, loss is 3.754323902130127 and perplexity is 42.70533706045092
At time: 184.12198042869568 and batch: 650, loss is 3.724689230918884 and perplexity is 41.458346770322834
At time: 184.6373107433319 and batch: 700, loss is 3.801840319633484 and perplexity is 44.783524703677735
At time: 185.13731718063354 and batch: 750, loss is 3.759863977432251 and perplexity is 42.94258442088147
At time: 185.6382348537445 and batch: 800, loss is 3.854456877708435 and perplexity is 47.202972991144996
At time: 186.13924193382263 and batch: 850, loss is 3.7443550491333006 and perplexity is 42.281728785533
At time: 186.641348361969 and batch: 900, loss is 3.709628839492798 and perplexity is 40.838646021103145
At time: 187.14111804962158 and batch: 950, loss is 3.712089152336121 and perplexity is 40.939245568847205
At time: 187.64318108558655 and batch: 1000, loss is 3.6362221956253054 and perplexity is 37.9482046956568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.643747469274009 and perplexity of 103.93310481728138
Finished 17 epochs...
Completing Train Step...
At time: 189.12800931930542 and batch: 50, loss is 3.9515214443206785 and perplexity is 52.014443744047966
At time: 189.62670946121216 and batch: 100, loss is 3.8425450801849363 and perplexity is 46.644036313620525
At time: 190.12504363059998 and batch: 150, loss is 3.91197687625885 and perplexity is 49.997693594731885
At time: 190.62352514266968 and batch: 200, loss is 3.9483716678619385 and perplexity is 51.85086762297398
At time: 191.12247037887573 and batch: 250, loss is 3.9408462381362916 and perplexity is 51.46213209797488
At time: 191.62037086486816 and batch: 300, loss is 3.795049953460693 and perplexity is 44.48045830285799
At time: 192.11702942848206 and batch: 350, loss is 3.816869168281555 and perplexity is 45.46165249454018
At time: 192.61531281471252 and batch: 400, loss is 3.8268653535842896 and perplexity is 45.918374533000836
At time: 193.11363458633423 and batch: 450, loss is 3.8672564125061033 and perplexity is 47.81103222273618
At time: 193.61075711250305 and batch: 500, loss is 3.905233483314514 and perplexity is 49.66167373082987
At time: 194.11069416999817 and batch: 550, loss is 3.813926420211792 and perplexity is 45.328066955109406
At time: 194.61045050621033 and batch: 600, loss is 3.751765832901001 and perplexity is 42.59623345857576
At time: 195.10934352874756 and batch: 650, loss is 3.7224292945861817 and perplexity is 41.36475933681512
At time: 195.6081576347351 and batch: 700, loss is 3.7998226070404053 and perplexity is 44.69325552117888
At time: 196.10603952407837 and batch: 750, loss is 3.758394856452942 and perplexity is 42.87954288836063
At time: 196.62419199943542 and batch: 800, loss is 3.8532484102249147 and perplexity is 47.14596418674084
At time: 197.1318290233612 and batch: 850, loss is 3.7434819984436034 and perplexity is 42.24483080230374
At time: 197.64294266700745 and batch: 900, loss is 3.7089788246154787 and perplexity is 40.81210891930971
At time: 198.13911533355713 and batch: 950, loss is 3.711559796333313 and perplexity is 40.91757986839513
At time: 198.6364643573761 and batch: 1000, loss is 3.6359055709838866 and perplexity is 37.93619126092764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.643376048018292 and perplexity of 103.89450902107284
Finished 18 epochs...
Completing Train Step...
At time: 200.10301423072815 and batch: 50, loss is 3.9471996355056764 and perplexity is 51.79013232723795
At time: 200.61505818367004 and batch: 100, loss is 3.8385423707962034 and perplexity is 46.45770695145901
At time: 201.1127278804779 and batch: 150, loss is 3.907941460609436 and perplexity is 49.796338668224145
At time: 201.6141996383667 and batch: 200, loss is 3.944236831665039 and perplexity is 51.63691541218467
At time: 202.11527752876282 and batch: 250, loss is 3.9370536613464355 and perplexity is 51.26732764913888
At time: 202.6164689064026 and batch: 300, loss is 3.7913502740859983 and perplexity is 44.31619890960267
At time: 203.11795282363892 and batch: 350, loss is 3.8133411693572996 and perplexity is 45.30154642652943
At time: 203.62004590034485 and batch: 400, loss is 3.8236468267440795 and perplexity is 45.77082258942606
At time: 204.12110686302185 and batch: 450, loss is 3.86427143573761 and perplexity is 47.66853019070993
At time: 204.62110710144043 and batch: 500, loss is 3.9023587703704834 and perplexity is 49.519115679446074
At time: 205.12262392044067 and batch: 550, loss is 3.8114186334609985 and perplexity is 45.21453624426603
At time: 205.62409591674805 and batch: 600, loss is 3.749500050544739 and perplexity is 42.499828921454956
At time: 206.12373065948486 and batch: 650, loss is 3.7203012228012087 and perplexity is 41.27682575724838
At time: 206.62371134757996 and batch: 700, loss is 3.7979587173461913 and perplexity is 44.610029808680416
At time: 207.1241593360901 and batch: 750, loss is 3.757044024467468 and perplexity is 42.821658934854426
At time: 207.62530279159546 and batch: 800, loss is 3.851956114768982 and perplexity is 47.08507702203348
At time: 208.12697911262512 and batch: 850, loss is 3.7424378204345703 and perplexity is 42.20074270090342
At time: 208.62825679779053 and batch: 900, loss is 3.7080286884307863 and perplexity is 40.77335027376347
At time: 209.1301305294037 and batch: 950, loss is 3.7107615804672243 and perplexity is 40.88493183876332
At time: 209.63011288642883 and batch: 1000, loss is 3.6351557874679568 and perplexity is 37.907757990793705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.643364510885099 and perplexity of 103.8933103831986
Finished 19 epochs...
Completing Train Step...
At time: 211.12935495376587 and batch: 50, loss is 3.943545980453491 and perplexity is 51.60125430628902
At time: 211.6463885307312 and batch: 100, loss is 3.8348881578445435 and perplexity is 46.28825040074034
At time: 212.1500322818756 and batch: 150, loss is 3.904414567947388 and perplexity is 49.62102167062473
At time: 212.65334296226501 and batch: 200, loss is 3.9406321382522584 and perplexity is 51.4511152408566
At time: 213.15518927574158 and batch: 250, loss is 3.9337566471099854 and perplexity is 51.09857687972381
At time: 213.6590919494629 and batch: 300, loss is 3.7881256675720216 and perplexity is 44.17352676032148
At time: 214.16236996650696 and batch: 350, loss is 3.8102124071121217 and perplexity is 45.160030159242694
At time: 214.6632363796234 and batch: 400, loss is 3.8208723497390746 and perplexity is 45.64400849739097
At time: 215.16532731056213 and batch: 450, loss is 3.8615310096740725 and perplexity is 47.5380769385522
At time: 215.66964960098267 and batch: 500, loss is 3.899695291519165 and perplexity is 49.38739805352176
At time: 216.17389178276062 and batch: 550, loss is 3.8091140031814574 and perplexity is 45.11045343722452
At time: 216.6769895553589 and batch: 600, loss is 3.747383074760437 and perplexity is 42.40995297895377
At time: 217.18032813072205 and batch: 650, loss is 3.7182822847366332 and perplexity is 41.193574470416635
At time: 217.68294405937195 and batch: 700, loss is 3.796106038093567 and perplexity is 44.527458244924794
At time: 218.18593621253967 and batch: 750, loss is 3.7556430864334107 and perplexity is 42.761710446034904
At time: 218.68925404548645 and batch: 800, loss is 3.850583815574646 and perplexity is 47.02050652392546
At time: 219.19282937049866 and batch: 850, loss is 3.7412399530410765 and perplexity is 42.15022207178889
At time: 219.69539165496826 and batch: 900, loss is 3.706868600845337 and perplexity is 40.72607704213998
At time: 220.19823813438416 and batch: 950, loss is 3.7097956848144533 and perplexity is 40.84546032658619
At time: 220.70132899284363 and batch: 1000, loss is 3.634236636161804 and perplexity is 37.87293103359668
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.643398377953506 and perplexity of 103.89682900463079
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 222.19425654411316 and batch: 50, loss is 3.940377745628357 and perplexity is 51.43802812135174
At time: 222.69412279129028 and batch: 100, loss is 3.8322997856140137 and perplexity is 46.16859410312704
At time: 223.21004176139832 and batch: 150, loss is 3.9015430927276613 and perplexity is 49.478740512693015
At time: 223.71081018447876 and batch: 200, loss is 3.936925287246704 and perplexity is 51.260746674528676
At time: 224.21115684509277 and batch: 250, loss is 3.9299608278274536 and perplexity is 50.904983571345184
At time: 224.71113991737366 and batch: 300, loss is 3.7837358999252317 and perplexity is 43.98004023286156
At time: 225.2109591960907 and batch: 350, loss is 3.8057047939300537 and perplexity is 44.9569243171815
At time: 225.71124505996704 and batch: 400, loss is 3.8161102294921876 and perplexity is 45.42716297240688
At time: 226.21296000480652 and batch: 450, loss is 3.856626009941101 and perplexity is 47.30547360984577
At time: 226.7139859199524 and batch: 500, loss is 3.8938117265701293 and perplexity is 49.09767722083375
At time: 227.2158317565918 and batch: 550, loss is 3.8029986238479614 and perplexity is 44.83542770300954
At time: 227.71603417396545 and batch: 600, loss is 3.740383281707764 and perplexity is 42.114128647155695
At time: 228.21658611297607 and batch: 650, loss is 3.7103440523147584 and perplexity is 40.86786479194319
At time: 228.71623516082764 and batch: 700, loss is 3.7880555772781372 and perplexity is 44.170430733350926
At time: 229.21834778785706 and batch: 750, loss is 3.7473416757583617 and perplexity is 42.408197285564604
At time: 229.71996808052063 and batch: 800, loss is 3.8410125303268434 and perplexity is 46.5726067510508
At time: 230.2221291065216 and batch: 850, loss is 3.7321898412704466 and perplexity is 41.77047880228631
At time: 230.72412991523743 and batch: 900, loss is 3.6961932373046875 and perplexity is 40.29362376906169
At time: 231.22414779663086 and batch: 950, loss is 3.699284019470215 and perplexity is 40.418355242402264
At time: 231.72432351112366 and batch: 1000, loss is 3.6227526664733887 and perplexity is 37.440487279679
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641684183260289 and perplexity of 103.71888217304037
Finished 21 epochs...
Completing Train Step...
At time: 233.20713329315186 and batch: 50, loss is 3.9377971696853638 and perplexity is 51.30545950868173
At time: 233.7207314968109 and batch: 100, loss is 3.8299990272521973 and perplexity is 46.06249342700342
At time: 234.22094249725342 and batch: 150, loss is 3.899301681518555 and perplexity is 49.367962505007824
At time: 234.72131323814392 and batch: 200, loss is 3.9348063564300535 and perplexity is 51.15224369446656
At time: 235.22111248970032 and batch: 250, loss is 3.927951636314392 and perplexity is 50.802808389507355
At time: 235.73453783988953 and batch: 300, loss is 3.781716480255127 and perplexity is 43.89131569071811
At time: 236.23410391807556 and batch: 350, loss is 3.8037499809265136 and perplexity is 44.86912777779357
At time: 236.73395442962646 and batch: 400, loss is 3.814292893409729 and perplexity is 45.34468152097334
At time: 237.24219393730164 and batch: 450, loss is 3.8551056480407713 and perplexity is 47.233606815703496
At time: 237.7464098930359 and batch: 500, loss is 3.892293677330017 and perplexity is 49.02320107276973
At time: 238.24584436416626 and batch: 550, loss is 3.801681480407715 and perplexity is 44.776411888198595
At time: 238.7440242767334 and batch: 600, loss is 3.739439525604248 and perplexity is 42.07440193031508
At time: 239.24299836158752 and batch: 650, loss is 3.709527816772461 and perplexity is 40.83452059837143
At time: 239.7421042919159 and batch: 700, loss is 3.787365975379944 and perplexity is 44.13998122070073
At time: 240.2432143688202 and batch: 750, loss is 3.7472398233413697 and perplexity is 42.40387812813284
At time: 240.74308943748474 and batch: 800, loss is 3.8407436990737915 and perplexity is 46.56008826157602
At time: 241.24412655830383 and batch: 850, loss is 3.7322170782089232 and perplexity is 41.77161651774146
At time: 241.74585962295532 and batch: 900, loss is 3.696375207901001 and perplexity is 40.30095669097443
At time: 242.24580359458923 and batch: 950, loss is 3.699586730003357 and perplexity is 40.43059215629434
At time: 242.74488711357117 and batch: 1000, loss is 3.6231903886795043 and perplexity is 37.45687939970484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641522291229992 and perplexity of 103.70209227173751
Finished 22 epochs...
Completing Train Step...
At time: 244.227623462677 and batch: 50, loss is 3.9359995222091673 and perplexity is 51.21331322696074
At time: 244.74308943748474 and batch: 100, loss is 3.8284248638153078 and perplexity is 45.99004057531513
At time: 245.24395537376404 and batch: 150, loss is 3.897697377204895 and perplexity is 49.288824767285114
At time: 245.74612641334534 and batch: 200, loss is 3.933175206184387 and perplexity is 51.06887471175092
At time: 246.24732065200806 and batch: 250, loss is 3.926448926925659 and perplexity is 50.72652386345826
At time: 246.7501049041748 and batch: 300, loss is 3.7802300548553465 and perplexity is 43.826122988293406
At time: 247.25140833854675 and batch: 350, loss is 3.802332487106323 and perplexity is 44.80557112267611
At time: 247.75207805633545 and batch: 400, loss is 3.8130421304702757 and perplexity is 45.288001527832236
At time: 248.25226426124573 and batch: 450, loss is 3.853975787162781 and perplexity is 47.18026954875582
At time: 248.76709628105164 and batch: 500, loss is 3.8911840438842775 and perplexity is 48.96883345888205
At time: 249.28669261932373 and batch: 550, loss is 3.8006632280349733 and perplexity is 44.73084140561403
At time: 249.79210996627808 and batch: 600, loss is 3.7386112594604493 and perplexity is 42.039567555733605
At time: 250.30908966064453 and batch: 650, loss is 3.708800616264343 and perplexity is 40.80483650869423
At time: 250.81539511680603 and batch: 700, loss is 3.7867656993865966 and perplexity is 44.11349300054386
At time: 251.31623148918152 and batch: 750, loss is 3.747021322250366 and perplexity is 42.394613846663724
At time: 251.82900142669678 and batch: 800, loss is 3.840375294685364 and perplexity is 46.54293847995625
At time: 252.35044193267822 and batch: 850, loss is 3.7320917510986327 and perplexity is 41.7663817297884
At time: 252.85895705223083 and batch: 900, loss is 3.6962812519073487 and perplexity is 40.29717035242078
At time: 253.360009431839 and batch: 950, loss is 3.6996199178695677 and perplexity is 40.43193398364372
At time: 253.86087775230408 and batch: 1000, loss is 3.623257908821106 and perplexity is 37.45940857889018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641459023080221 and perplexity of 103.69553143978007
Finished 23 epochs...
Completing Train Step...
At time: 255.3471109867096 and batch: 50, loss is 3.9344109106063843 and perplexity is 51.132019752326556
At time: 255.84932374954224 and batch: 100, loss is 3.827051463127136 and perplexity is 45.92692117597467
At time: 256.3502588272095 and batch: 150, loss is 3.8963083744049074 and perplexity is 49.22040997684256
At time: 256.85004234313965 and batch: 200, loss is 3.931738495826721 and perplexity is 50.995556211833936
At time: 257.35156059265137 and batch: 250, loss is 3.925127320289612 and perplexity is 50.659527633978996
At time: 257.85372734069824 and batch: 300, loss is 3.7788841199874876 and perplexity is 43.76717555983886
At time: 258.35595965385437 and batch: 350, loss is 3.8010850095748903 and perplexity is 44.74971202814423
At time: 258.8575303554535 and batch: 400, loss is 3.8119398593902587 and perplexity is 45.23810937586604
At time: 259.359956741333 and batch: 450, loss is 3.8529700803756715 and perplexity is 47.132843883607016
At time: 259.86183500289917 and batch: 500, loss is 3.890188422203064 and perplexity is 48.92010328901883
At time: 260.3651387691498 and batch: 550, loss is 3.7997404861450197 and perplexity is 44.68958542171577
At time: 260.8663430213928 and batch: 600, loss is 3.7378008127212525 and perplexity is 42.00551052785853
At time: 261.4054186344147 and batch: 650, loss is 3.708069529533386 and perplexity is 40.775015536350686
At time: 261.91897106170654 and batch: 700, loss is 3.7861878633499146 and perplexity is 44.08800999779194
At time: 262.4368362426758 and batch: 750, loss is 3.7467381620407103 and perplexity is 42.38261107835202
At time: 262.9473805427551 and batch: 800, loss is 3.8399674034118654 and perplexity is 46.52395789277839
At time: 263.47114872932434 and batch: 850, loss is 3.731885995864868 and perplexity is 41.757788962185856
At time: 263.9733464717865 and batch: 900, loss is 3.696063904762268 and perplexity is 40.288412829235654
At time: 264.4744064807892 and batch: 950, loss is 3.699522867202759 and perplexity is 40.42801022789487
At time: 264.9771349430084 and batch: 1000, loss is 3.6231666707992556 and perplexity is 37.455991012460125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641435948813834 and perplexity of 103.69313876906921
Finished 24 epochs...
Completing Train Step...
At time: 266.4758458137512 and batch: 50, loss is 3.9329521322250365 and perplexity is 51.057483846219355
At time: 266.9932310581207 and batch: 100, loss is 3.8257934284210204 and perplexity is 45.86917984310598
At time: 267.49483013153076 and batch: 150, loss is 3.89503698348999 and perplexity is 49.15787135870909
At time: 267.99809312820435 and batch: 200, loss is 3.9304266738891602 and perplexity is 50.928702981831165
At time: 268.4973781108856 and batch: 250, loss is 3.9239164543151857 and perplexity is 50.59822285910988
At time: 268.9963057041168 and batch: 300, loss is 3.777654914855957 and perplexity is 43.71340977440754
At time: 269.5054931640625 and batch: 350, loss is 3.7999471759796144 and perplexity is 44.69882325938537
At time: 270.0147271156311 and batch: 400, loss is 3.810930233001709 and perplexity is 45.19245883574485
At time: 270.51392793655396 and batch: 450, loss is 3.8520294570922853 and perplexity is 47.08853047761588
At time: 271.0141034126282 and batch: 500, loss is 3.8892722082138063 and perplexity is 48.87530253270334
At time: 271.52446150779724 and batch: 550, loss is 3.798878273963928 and perplexity is 44.65107012336299
At time: 272.0357849597931 and batch: 600, loss is 3.7370486736297606 and perplexity is 41.97392841989032
At time: 272.5460751056671 and batch: 650, loss is 3.7073657941818237 and perplexity is 40.74633081086876
At time: 273.04550433158875 and batch: 700, loss is 3.7856092405319215 and perplexity is 44.06250704821313
At time: 273.5445170402527 and batch: 750, loss is 3.7464161920547485 and perplexity is 42.36896734621208
At time: 274.04214906692505 and batch: 800, loss is 3.839524211883545 and perplexity is 46.503343437189834
At time: 274.55408787727356 and batch: 850, loss is 3.7316079425811766 and perplexity is 41.74617968591871
At time: 275.05289912223816 and batch: 900, loss is 3.6957779932022095 and perplexity is 40.276895552811055
At time: 275.55191683769226 and batch: 950, loss is 3.699370641708374 and perplexity is 40.42185652243825
At time: 276.04952454566956 and batch: 1000, loss is 3.6230053329467773 and perplexity is 37.44994843076941
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641454184927592 and perplexity of 103.69502974618568
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 277.5148067474365 and batch: 50, loss is 3.9316906118392945 and perplexity is 50.99311439972379
At time: 278.0295162200928 and batch: 100, loss is 3.8251653575897215 and perplexity is 45.840379794372936
At time: 278.5276746749878 and batch: 150, loss is 3.893922128677368 and perplexity is 49.10309800708707
At time: 279.0314290523529 and batch: 200, loss is 3.929185152053833 and perplexity is 50.865513118948215
At time: 279.53110098838806 and batch: 250, loss is 3.9225295114517214 and perplexity is 50.528094658156995
At time: 280.0279903411865 and batch: 300, loss is 3.7759452199935915 and perplexity is 43.638737034289214
At time: 280.52576327323914 and batch: 350, loss is 3.798408074378967 and perplexity is 44.63008014384947
At time: 281.02455830574036 and batch: 400, loss is 3.8087159395217896 and perplexity is 45.09250017854701
At time: 281.525417804718 and batch: 450, loss is 3.849898657798767 and perplexity is 46.988301092415625
At time: 282.0230276584625 and batch: 500, loss is 3.886999635696411 and perplexity is 48.76435597818289
At time: 282.52134585380554 and batch: 550, loss is 3.796635837554932 and perplexity is 44.55105511857243
At time: 283.0208172798157 and batch: 600, loss is 3.7345837450027464 and perplexity is 41.8705930915654
At time: 283.52027583122253 and batch: 650, loss is 3.7044195127487183 and perplexity is 40.626457330153016
At time: 284.01879143714905 and batch: 700, loss is 3.7826560831069944 and perplexity is 43.932575476984205
At time: 284.5200607776642 and batch: 750, loss is 3.7431748628616335 and perplexity is 42.23185790393138
At time: 285.02108097076416 and batch: 800, loss is 3.8357283449172974 and perplexity is 46.327157532693334
At time: 285.5396282672882 and batch: 850, loss is 3.728170785903931 and perplexity is 41.60293783901914
At time: 286.04307079315186 and batch: 900, loss is 3.691628122329712 and perplexity is 40.110097970709916
At time: 286.5429537296295 and batch: 950, loss is 3.6952422428131104 and perplexity is 40.25532296962285
At time: 287.0822608470917 and batch: 1000, loss is 3.6184975337982177 and perplexity is 37.281511510246354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641077925519245 and perplexity of 103.65602085483631
Finished 26 epochs...
Completing Train Step...
At time: 288.6306777000427 and batch: 50, loss is 3.930918698310852 and perplexity is 50.95376731308921
At time: 289.1348457336426 and batch: 100, loss is 3.8244274044036866 and perplexity is 45.8065642187451
At time: 289.63863372802734 and batch: 150, loss is 3.8931293630599977 and perplexity is 49.064186185295824
At time: 290.14157223701477 and batch: 200, loss is 3.928496141433716 and perplexity is 50.83047831127485
At time: 290.64450454711914 and batch: 250, loss is 3.9218486547470093 and perplexity is 50.493703975024786
At time: 291.1561896800995 and batch: 300, loss is 3.7752659368515014 and perplexity is 43.60910404161536
At time: 291.66046023368835 and batch: 350, loss is 3.7977639722824095 and perplexity is 44.601343071457066
At time: 292.16436219215393 and batch: 400, loss is 3.8081258726119995 and perplexity is 45.06590043489801
At time: 292.6820833683014 and batch: 450, loss is 3.849389743804932 and perplexity is 46.96439417224224
At time: 293.1893939971924 and batch: 500, loss is 3.8865314817428587 and perplexity is 48.74153209510168
At time: 293.6935329437256 and batch: 550, loss is 3.7961719274520873 and perplexity is 44.53039222724314
At time: 294.19656467437744 and batch: 600, loss is 3.7342798852920533 and perplexity is 41.857872238037366
At time: 294.71428775787354 and batch: 650, loss is 3.704180417060852 and perplexity is 40.61674488054078
At time: 295.2305645942688 and batch: 700, loss is 3.7824435901641844 and perplexity is 43.92324110651493
At time: 295.7444541454315 and batch: 750, loss is 3.74316460609436 and perplexity is 42.231424743814756
At time: 296.25376892089844 and batch: 800, loss is 3.8356006002426146 and perplexity is 46.32123986300878
At time: 296.7716121673584 and batch: 850, loss is 3.728208694458008 and perplexity is 41.604514976131284
At time: 297.28423619270325 and batch: 900, loss is 3.6917601442337036 and perplexity is 40.11539373178384
At time: 297.8030090332031 and batch: 950, loss is 3.695349626541138 and perplexity is 40.25964596838199
At time: 298.3078408241272 and batch: 1000, loss is 3.6186363887786865 and perplexity is 37.28668859322252
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641061922399009 and perplexity of 103.65436204834445
Finished 27 epochs...
Completing Train Step...
At time: 299.80121970176697 and batch: 50, loss is 3.930288624763489 and perplexity is 50.92167280417905
At time: 300.3149299621582 and batch: 100, loss is 3.8239283323287965 and perplexity is 45.7837091453357
At time: 300.8211307525635 and batch: 150, loss is 3.892533893585205 and perplexity is 49.03497865707573
At time: 301.3490469455719 and batch: 200, loss is 3.927915620803833 and perplexity is 50.80097873337356
At time: 301.8678243160248 and batch: 250, loss is 3.921310977935791 and perplexity is 50.466561978749795
At time: 302.3713400363922 and batch: 300, loss is 3.774715371131897 and perplexity is 43.58510097210732
At time: 302.8720655441284 and batch: 350, loss is 3.7972497844696047 and perplexity is 44.5784154994592
At time: 303.3751473426819 and batch: 400, loss is 3.8076553297042848 and perplexity is 45.044699983320825
At time: 303.8750078678131 and batch: 450, loss is 3.848965449333191 and perplexity is 46.94447166622975
At time: 304.37508034706116 and batch: 500, loss is 3.886141119003296 and perplexity is 48.722508930311484
At time: 304.8869936466217 and batch: 550, loss is 3.7958152437210084 and perplexity is 44.51451179311173
At time: 305.3971264362335 and batch: 600, loss is 3.7340182065963745 and perplexity is 41.846920357625606
At time: 305.90012288093567 and batch: 650, loss is 3.7039562606811525 and perplexity is 40.60764139839309
At time: 306.40213775634766 and batch: 700, loss is 3.7822562789916994 and perplexity is 43.91501456320971
At time: 306.90442752838135 and batch: 750, loss is 3.7431087732315063 and perplexity is 42.22906690829187
At time: 307.40622329711914 and batch: 800, loss is 3.835460591316223 and perplexity is 46.31475492993128
At time: 307.9198782444 and batch: 850, loss is 3.728186402320862 and perplexity is 41.60358753291493
At time: 308.44016456604004 and batch: 900, loss is 3.691786766052246 and perplexity is 40.116461690731974
At time: 308.9450798034668 and batch: 950, loss is 3.6953804111480713 and perplexity is 40.26088536483547
At time: 309.44497179985046 and batch: 1000, loss is 3.618682041168213 and perplexity is 37.28839085851027
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6410582007431405 and perplexity of 103.65397628319745
Finished 28 epochs...
Completing Train Step...
At time: 310.95170879364014 and batch: 50, loss is 3.9296909189224243 and perplexity is 50.89124571703692
At time: 311.46727681159973 and batch: 100, loss is 3.823473267555237 and perplexity is 45.76287933191739
At time: 311.98533964157104 and batch: 150, loss is 3.8919921922683716 and perplexity is 49.00842353768722
At time: 312.48687529563904 and batch: 200, loss is 3.927383532524109 and perplexity is 50.77395531804989
At time: 312.9871027469635 and batch: 250, loss is 3.92082275390625 and perplexity is 50.441929004197796
At time: 313.5041615962982 and batch: 300, loss is 3.774218797683716 and perplexity is 43.5634631410591
At time: 314.00915336608887 and batch: 350, loss is 3.7967894458770752 and perplexity is 44.55789905702841
At time: 314.51892614364624 and batch: 400, loss is 3.807234172821045 and perplexity is 45.0257330921681
At time: 315.0320358276367 and batch: 450, loss is 3.8485752725601197 and perplexity is 46.92615859666102
At time: 315.5320100784302 and batch: 500, loss is 3.885776791572571 and perplexity is 48.70476121700013
At time: 316.03185081481934 and batch: 550, loss is 3.795485939979553 and perplexity is 44.49985541116236
At time: 316.5311641693115 and batch: 600, loss is 3.7337642240524294 and perplexity is 41.836293319935066
At time: 317.0308926105499 and batch: 650, loss is 3.703732614517212 and perplexity is 40.598560670640474
At time: 317.53116369247437 and batch: 700, loss is 3.7820681715011597 and perplexity is 43.90675459692834
At time: 318.0316791534424 and batch: 750, loss is 3.743027901649475 and perplexity is 42.225651914933124
At time: 318.53999280929565 and batch: 800, loss is 3.8353062200546266 and perplexity is 46.307605814605395
At time: 319.05939197540283 and batch: 850, loss is 3.7281200551986693 and perplexity is 41.600827346175464
At time: 319.5692114830017 and batch: 900, loss is 3.691761989593506 and perplexity is 40.11546775918719
At time: 320.06833386421204 and batch: 950, loss is 3.695371398925781 and perplexity is 40.26052252642195
At time: 320.568288564682 and batch: 1000, loss is 3.618679265975952 and perplexity is 37.28828737620013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641062294564596 and perplexity of 103.65440062493806
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 322.0591287612915 and batch: 50, loss is 3.9291540718078615 and perplexity is 50.86393223085633
At time: 322.5590624809265 and batch: 100, loss is 3.823339614868164 and perplexity is 45.75676340884037
At time: 323.0587270259857 and batch: 150, loss is 3.891519112586975 and perplexity is 48.9852441315796
At time: 323.55873465538025 and batch: 200, loss is 3.9269021701812745 and perplexity is 50.74952052942838
At time: 324.05893993377686 and batch: 250, loss is 3.920346064567566 and perplexity is 50.417889604536796
At time: 324.55835485458374 and batch: 300, loss is 3.773486442565918 and perplexity is 43.531570895528795
At time: 325.05586338043213 and batch: 350, loss is 3.7962413215637207 and perplexity is 44.53348248147392
At time: 325.55363750457764 and batch: 400, loss is 3.8062867736816406 and perplexity is 44.983095951769734
At time: 326.0671281814575 and batch: 450, loss is 3.8476327991485597 and perplexity is 46.88195277455518
At time: 326.56614232063293 and batch: 500, loss is 3.884843497276306 and perplexity is 48.659326546412764
At time: 327.06619453430176 and batch: 550, loss is 3.7945602703094483 and perplexity is 44.45868230397245
At time: 327.5659203529358 and batch: 600, loss is 3.7328608417510987 and perplexity is 41.798516219147835
At time: 328.0825135707855 and batch: 650, loss is 3.702697353363037 and perplexity is 40.5565523064285
At time: 328.5876624584198 and batch: 700, loss is 3.781053214073181 and perplexity is 43.86221371758352
At time: 329.09027004241943 and batch: 750, loss is 3.7418891477584837 and perplexity is 42.17759465740307
At time: 329.5866811275482 and batch: 800, loss is 3.833834309577942 and perplexity is 46.239495303041366
At time: 330.0851480960846 and batch: 850, loss is 3.7267026233673097 and perplexity is 41.54190277992753
At time: 330.5851058959961 and batch: 900, loss is 3.6902668476104736 and perplexity is 40.05553425488555
At time: 331.0869381427765 and batch: 950, loss is 3.693708047866821 and perplexity is 40.19361080790953
At time: 331.58697056770325 and batch: 1000, loss is 3.616977820396423 and perplexity is 37.22489732712749
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6410369873046875 and perplexity of 103.65177744927365
Finished 30 epochs...
Completing Train Step...
At time: 333.0843548774719 and batch: 50, loss is 3.9289063596725464 and perplexity is 50.85133417800261
At time: 333.5987346172333 and batch: 100, loss is 3.8231474494934083 and perplexity is 45.74797138804034
At time: 334.10152649879456 and batch: 150, loss is 3.8912408781051635 and perplexity is 48.97161664346869
At time: 334.614697933197 and batch: 200, loss is 3.926668043136597 and perplexity is 50.73764008498896
At time: 335.11965322494507 and batch: 250, loss is 3.920113558769226 and perplexity is 50.40616851552708
At time: 335.6215178966522 and batch: 300, loss is 3.77324399471283 and perplexity is 43.521018038934024
At time: 336.12413263320923 and batch: 350, loss is 3.7960235929489134 and perplexity is 44.52378732351564
At time: 336.64261746406555 and batch: 400, loss is 3.806103005409241 and perplexity is 44.97483024545003
At time: 337.1592164039612 and batch: 450, loss is 3.8474812602996824 and perplexity is 46.87484887567053
At time: 337.6591064929962 and batch: 500, loss is 3.884701819419861 and perplexity is 48.65243308566851
At time: 338.1618273258209 and batch: 550, loss is 3.7944016361236574 and perplexity is 44.45163019647128
At time: 338.66453862190247 and batch: 600, loss is 3.7327649450302123 and perplexity is 41.79450807069173
At time: 339.191960811615 and batch: 650, loss is 3.702623038291931 and perplexity is 40.553538455348686
At time: 339.69498658180237 and batch: 700, loss is 3.780983557701111 and perplexity is 43.85915854131248
At time: 340.1983516216278 and batch: 750, loss is 3.741905846595764 and perplexity is 42.178298980073805
At time: 340.7002694606781 and batch: 800, loss is 3.8337930059432983 and perplexity is 46.237585483262784
At time: 341.2046320438385 and batch: 850, loss is 3.726703572273254 and perplexity is 41.541942199304735
At time: 341.7073428630829 and batch: 900, loss is 3.6903118896484375 and perplexity is 40.05733847841277
At time: 342.2090926170349 and batch: 950, loss is 3.693731665611267 and perplexity is 40.19456010154799
At time: 342.7105977535248 and batch: 1000, loss is 3.6170366525650026 and perplexity is 37.22708741298551
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6410328934832314 and perplexity of 103.65135311827174
Finished 31 epochs...
Completing Train Step...
At time: 344.21074175834656 and batch: 50, loss is 3.9286747217178344 and perplexity is 50.839556443097116
At time: 344.734002828598 and batch: 100, loss is 3.8229724168777466 and perplexity is 45.73996470168314
At time: 345.23337149620056 and batch: 150, loss is 3.8910051584243774 and perplexity is 48.960074430042795
At time: 345.7325441837311 and batch: 200, loss is 3.9264510583877565 and perplexity is 50.726631985236445
At time: 346.2369282245636 and batch: 250, loss is 3.919914698600769 and perplexity is 50.39614573296395
At time: 346.7383942604065 and batch: 300, loss is 3.77303240776062 and perplexity is 43.51181053349846
At time: 347.2378487586975 and batch: 350, loss is 3.7958357429504392 and perplexity is 44.515424315654954
At time: 347.7373287677765 and batch: 400, loss is 3.805940017700195 and perplexity is 44.96750049824919
At time: 348.23608899116516 and batch: 450, loss is 3.8473342943191526 and perplexity is 46.86796037374345
At time: 348.7356917858124 and batch: 500, loss is 3.884567255973816 and perplexity is 48.64588668707689
At time: 349.2340724468231 and batch: 550, loss is 3.7942660903930663 and perplexity is 44.445605376108894
At time: 349.73284935951233 and batch: 600, loss is 3.7326735353469847 and perplexity is 41.79068782255482
At time: 350.23113346099854 and batch: 650, loss is 3.702557625770569 and perplexity is 40.550885832906474
At time: 350.7329456806183 and batch: 700, loss is 3.7809256315231323 and perplexity is 43.85661802147083
At time: 351.23255586624146 and batch: 750, loss is 3.7419090270996094 and perplexity is 42.17843312852922
At time: 351.75792050361633 and batch: 800, loss is 3.833742961883545 and perplexity is 46.23527162466993
At time: 352.26527190208435 and batch: 850, loss is 3.726693367958069 and perplexity is 41.541518294395956
At time: 352.7740857601166 and batch: 900, loss is 3.6903431034088134 and perplexity is 40.058588838091445
At time: 353.2718095779419 and batch: 950, loss is 3.693737144470215 and perplexity is 40.19478032247654
At time: 353.7726285457611 and batch: 1000, loss is 3.617068119049072 and perplexity is 37.22825883696875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641031776986471 and perplexity of 103.65123739193633
Finished 32 epochs...
Completing Train Step...
At time: 355.36288928985596 and batch: 50, loss is 3.928453288078308 and perplexity is 50.82830010139436
At time: 355.8791356086731 and batch: 100, loss is 3.8228115940093996 and perplexity is 45.732609260839126
At time: 356.39038705825806 and batch: 150, loss is 3.8907902908325194 and perplexity is 48.949555626868246
At time: 356.9017245769501 and batch: 200, loss is 3.9262451934814453 and perplexity is 50.71619022672801
At time: 357.4231631755829 and batch: 250, loss is 3.9197318935394287 and perplexity is 50.386933904462005
At time: 357.93522787094116 and batch: 300, loss is 3.772834243774414 and perplexity is 43.50318891395156
At time: 358.43616580963135 and batch: 350, loss is 3.7956614208221438 and perplexity is 44.50766496847888
At time: 358.93612265586853 and batch: 400, loss is 3.805783748626709 and perplexity is 44.96047401763444
At time: 359.4356756210327 and batch: 450, loss is 3.847192478179932 and perplexity is 46.86131421182774
At time: 359.9351418018341 and batch: 500, loss is 3.8844371318817137 and perplexity is 48.63955709706222
At time: 360.43515133857727 and batch: 550, loss is 3.7941385602951048 and perplexity is 44.43993758511597
At time: 360.9361197948456 and batch: 600, loss is 3.73258394241333 and perplexity is 41.786943839953054
At time: 361.4361147880554 and batch: 650, loss is 3.702491054534912 and perplexity is 40.54818640018288
At time: 361.9359300136566 and batch: 700, loss is 3.7808699560165406 and perplexity is 43.85417635001637
At time: 362.4354212284088 and batch: 750, loss is 3.7419032430648804 and perplexity is 42.17818916771274
At time: 362.9336779117584 and batch: 800, loss is 3.833688721656799 and perplexity is 46.232763881064265
At time: 363.43386006355286 and batch: 850, loss is 3.726677198410034 and perplexity is 41.54084659225103
At time: 363.933176279068 and batch: 900, loss is 3.6903639698028563 and perplexity is 40.05942472511188
At time: 364.4310100078583 and batch: 950, loss is 3.6937321615219116 and perplexity is 40.194580034463144
At time: 364.9539771080017 and batch: 1000, loss is 3.61708589553833 and perplexity is 37.22892063059422
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641033265648819 and perplexity of 103.65139169374562
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 366.4460859298706 and batch: 50, loss is 3.928256478309631 and perplexity is 50.81829757973844
At time: 366.96921730041504 and batch: 100, loss is 3.822791256904602 and perplexity is 45.731679201429316
At time: 367.4722146987915 and batch: 150, loss is 3.8906336498260496 and perplexity is 48.94188871970031
At time: 367.9740927219391 and batch: 200, loss is 3.9260905313491823 and perplexity is 50.70834695915117
At time: 368.4746341705322 and batch: 250, loss is 3.9196328926086426 and perplexity is 50.381945798023665
At time: 368.9768626689911 and batch: 300, loss is 3.772503905296326 and perplexity is 43.48882051008274
At time: 369.4783387184143 and batch: 350, loss is 3.7954072999954223 and perplexity is 44.49635608083444
At time: 369.9790527820587 and batch: 400, loss is 3.8053664064407347 and perplexity is 44.94171403006488
At time: 370.48031210899353 and batch: 450, loss is 3.8468768072128294 and perplexity is 46.846523790027
At time: 370.9802656173706 and batch: 500, loss is 3.8841112899780272 and perplexity is 48.623710873005365
At time: 371.48255228996277 and batch: 550, loss is 3.7937543058395384 and perplexity is 44.42286462148504
At time: 371.9852466583252 and batch: 600, loss is 3.732225441932678 and perplexity is 41.771965885463814
At time: 372.48799991607666 and batch: 650, loss is 3.7020719766616823 and perplexity is 40.53119711262887
At time: 372.9896774291992 and batch: 700, loss is 3.7805290460586547 and perplexity is 43.8392285726716
At time: 373.4986491203308 and batch: 750, loss is 3.741500024795532 and perplexity is 42.16118557958135
At time: 374.0053367614746 and batch: 800, loss is 3.8332001113891603 and perplexity is 46.21017959583679
At time: 374.5086224079132 and batch: 850, loss is 3.7261189937591555 and perplexity is 41.517664769184464
At time: 375.01209807395935 and batch: 900, loss is 3.6897751665115357 and perplexity is 40.0358445467099
At time: 375.51401114463806 and batch: 950, loss is 3.693077220916748 and perplexity is 40.168263590685584
At time: 376.01604986190796 and batch: 1000, loss is 3.6164679670333864 and perplexity is 37.205922925528824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641030660489711 and perplexity of 103.65112166573023
Finished 34 epochs...
Completing Train Step...
At time: 377.49210357666016 and batch: 50, loss is 3.9281654357910156 and perplexity is 50.81367116453847
At time: 378.00653171539307 and batch: 100, loss is 3.822695231437683 and perplexity is 45.727288006417666
At time: 378.50724697113037 and batch: 150, loss is 3.8905375576019288 and perplexity is 48.93718601071106
At time: 379.00699830055237 and batch: 200, loss is 3.9260111951828005 and perplexity is 50.704324112880585
At time: 379.50810408592224 and batch: 250, loss is 3.9195446872711184 and perplexity is 50.37750203747401
At time: 380.0081226825714 and batch: 300, loss is 3.7724305486679075 and perplexity is 43.48563043384428
At time: 380.506795167923 and batch: 350, loss is 3.795336904525757 and perplexity is 44.49322384919848
At time: 381.0073473453522 and batch: 400, loss is 3.805309634208679 and perplexity is 44.93916266107114
At time: 381.50696635246277 and batch: 450, loss is 3.8468184232711793 and perplexity is 46.84378878515649
At time: 382.0072937011719 and batch: 500, loss is 3.8840545558929445 and perplexity is 48.620952329508114
At time: 382.50710678100586 and batch: 550, loss is 3.7937076568603514 and perplexity is 44.420792388532035
At time: 383.00702357292175 and batch: 600, loss is 3.732197117805481 and perplexity is 41.770782747744555
At time: 383.5069591999054 and batch: 650, loss is 3.702057762145996 and perplexity is 40.53062098538642
At time: 384.00619626045227 and batch: 700, loss is 3.7805050230026245 and perplexity is 43.83817543307715
At time: 384.5058398246765 and batch: 750, loss is 3.7415228271484375 and perplexity is 42.16214696477473
At time: 385.0060772895813 and batch: 800, loss is 3.833186831474304 and perplexity is 46.20956593266097
At time: 385.51725125312805 and batch: 850, loss is 3.7261178970336912 and perplexity is 41.51761923572926
At time: 386.0276288986206 and batch: 900, loss is 3.6897866535186767 and perplexity is 40.036304441383514
At time: 386.5270049571991 and batch: 950, loss is 3.693079028129578 and perplexity is 40.16833618335249
At time: 387.03066849708557 and batch: 1000, loss is 3.61649019241333 and perplexity is 37.20674985049132
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641031032655297 and perplexity of 103.65116024111788
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 388.5589244365692 and batch: 50, loss is 3.9280870389938354 and perplexity is 50.809687691614
At time: 389.08454871177673 and batch: 100, loss is 3.8226720476150513 and perplexity is 45.72622788537196
At time: 389.5945289134979 and batch: 150, loss is 3.8904748153686524 and perplexity is 48.934115678691235
At time: 390.09440541267395 and batch: 200, loss is 3.925960760116577 and perplexity is 50.70176690142324
At time: 390.62441658973694 and batch: 250, loss is 3.9195218467712403 and perplexity is 50.37635140328544
At time: 391.1307580471039 and batch: 300, loss is 3.772304320335388 and perplexity is 43.48014166165263
At time: 391.6455283164978 and batch: 350, loss is 3.7952268743515014 and perplexity is 44.48832852134699
At time: 392.16401171684265 and batch: 400, loss is 3.805141987800598 and perplexity is 44.93162940334828
At time: 392.66339588165283 and batch: 450, loss is 3.846716785430908 and perplexity is 46.83902792558009
At time: 393.1631259918213 and batch: 500, loss is 3.8839399099349974 and perplexity is 48.61537845336929
At time: 393.6769554615021 and batch: 550, loss is 3.7935621452331545 and perplexity is 44.41432911700219
At time: 394.1818869113922 and batch: 600, loss is 3.7320635986328123 and perplexity is 41.765205919705366
At time: 394.679101228714 and batch: 650, loss is 3.7018759727478026 and perplexity is 40.52325361786402
At time: 395.17673087120056 and batch: 700, loss is 3.7803752708435057 and perplexity is 43.83248770416841
At time: 395.6752643585205 and batch: 750, loss is 3.741350312232971 and perplexity is 42.154873992921324
At time: 396.1873824596405 and batch: 800, loss is 3.8330235147476195 and perplexity is 46.20201975383672
At time: 396.6963040828705 and batch: 850, loss is 3.7259276247024538 and perplexity is 41.509720333025
At time: 397.19593930244446 and batch: 900, loss is 3.689556121826172 and perplexity is 40.02707586813918
At time: 397.69531655311584 and batch: 950, loss is 3.6928425931930544 and perplexity is 40.158840107982954
At time: 398.1945917606354 and batch: 1000, loss is 3.6162640857696533 and perplexity is 37.19833810817176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6410302883241235 and perplexity of 103.65108309035685
Finished 36 epochs...
Completing Train Step...
At time: 399.6823284626007 and batch: 50, loss is 3.9280511426925657 and perplexity is 50.80786384449208
At time: 400.19503021240234 and batch: 100, loss is 3.8226313638687133 and perplexity is 45.724367608957444
At time: 400.6948411464691 and batch: 150, loss is 3.890438394546509 and perplexity is 48.93233349042193
At time: 401.1960756778717 and batch: 200, loss is 3.9259291982650755 and perplexity is 50.70016668503846
At time: 401.7080852985382 and batch: 250, loss is 3.919485230445862 and perplexity is 50.374506840181844
At time: 402.21685123443604 and batch: 300, loss is 3.7722781038284303 and perplexity is 43.47900177915817
At time: 402.7158179283142 and batch: 350, loss is 3.795203504562378 and perplexity is 44.48728885063949
At time: 403.2180540561676 and batch: 400, loss is 3.8051227378845214 and perplexity is 44.93076448157794
At time: 403.7329840660095 and batch: 450, loss is 3.8466932916641237 and perplexity is 46.83792751330806
At time: 404.2329566478729 and batch: 500, loss is 3.8839182329177855 and perplexity is 48.61432462839572
At time: 404.74051237106323 and batch: 550, loss is 3.793545470237732 and perplexity is 44.41358851444225
At time: 405.24122619628906 and batch: 600, loss is 3.732054200172424 and perplexity is 41.76481339291651
At time: 405.7508690357208 and batch: 650, loss is 3.701875329017639 and perplexity is 40.52322753183174
At time: 406.25287556648254 and batch: 700, loss is 3.78036591053009 and perplexity is 43.83207742026591
At time: 406.75276136398315 and batch: 750, loss is 3.7413651847839358 and perplexity is 42.15550094809539
At time: 407.2673580646515 and batch: 800, loss is 3.83302001953125 and perplexity is 46.20185826806318
At time: 407.7728419303894 and batch: 850, loss is 3.7259257793426515 and perplexity is 41.50964373272637
At time: 408.270614862442 and batch: 900, loss is 3.6895607233047487 and perplexity is 40.027260052295034
At time: 408.7693109512329 and batch: 950, loss is 3.692840027809143 and perplexity is 40.158737085272776
At time: 409.26892590522766 and batch: 1000, loss is 3.616273274421692 and perplexity is 37.198679912327414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641029916158536 and perplexity of 103.65104451499782
Finished 37 epochs...
Completing Train Step...
At time: 410.7479088306427 and batch: 50, loss is 3.9280176639556883 and perplexity is 50.8061628898602
At time: 411.2633020877838 and batch: 100, loss is 3.8225974321365355 and perplexity is 45.722816128284094
At time: 411.7641990184784 and batch: 150, loss is 3.8904047346115114 and perplexity is 48.93068645897701
At time: 412.26537227630615 and batch: 200, loss is 3.925899052619934 and perplexity is 50.698638318841866
At time: 412.7658882141113 and batch: 250, loss is 3.9194532823562622 and perplexity is 50.372897496631644
At time: 413.26814460754395 and batch: 300, loss is 3.772250714302063 and perplexity is 43.47781092620103
At time: 413.76815128326416 and batch: 350, loss is 3.7951792430877687 and perplexity is 44.48620953650353
At time: 414.26986503601074 and batch: 400, loss is 3.805102529525757 and perplexity is 44.92985651374403
At time: 414.77240538597107 and batch: 450, loss is 3.846671724319458 and perplexity is 46.83691735447521
At time: 415.27446818351746 and batch: 500, loss is 3.883898448944092 and perplexity is 48.61336285339004
At time: 415.77620553970337 and batch: 550, loss is 3.793528504371643 and perplexity is 44.41283500583897
At time: 416.2927451133728 and batch: 600, loss is 3.732043814659119 and perplexity is 41.76437964614367
At time: 416.79534244537354 and batch: 650, loss is 3.701871781349182 and perplexity is 40.523083769110656
At time: 417.2981731891632 and batch: 700, loss is 3.780357780456543 and perplexity is 43.83172106370136
At time: 417.8121266365051 and batch: 750, loss is 3.7413758563995363 and perplexity is 42.15595081779737
At time: 418.3183925151825 and batch: 800, loss is 3.8330161714553834 and perplexity is 46.20168048014946
At time: 418.82026767730713 and batch: 850, loss is 3.7259245109558106 and perplexity is 41.50959108247388
At time: 419.32207798957825 and batch: 900, loss is 3.6895637702941895 and perplexity is 40.027382015119564
At time: 419.8354046344757 and batch: 950, loss is 3.6928381013870237 and perplexity is 40.1586597226679
At time: 420.34737825393677 and batch: 1000, loss is 3.616281132698059 and perplexity is 37.19897223098321
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641029916158536 and perplexity of 103.65104451499782
Finished 38 epochs...
Completing Train Step...
At time: 421.8339195251465 and batch: 50, loss is 3.9279853963851927 and perplexity is 50.804523524866845
At time: 422.3307318687439 and batch: 100, loss is 3.822567596435547 and perplexity is 45.72145197636406
At time: 422.8302366733551 and batch: 150, loss is 3.8903729581832884 and perplexity is 48.92913164123425
At time: 423.33029222488403 and batch: 200, loss is 3.925869836807251 and perplexity is 50.69715713855852
At time: 423.8414556980133 and batch: 250, loss is 3.919424285888672 and perplexity is 50.3714368817184
At time: 424.3457336425781 and batch: 300, loss is 3.77222291469574 and perplexity is 43.47660227697355
At time: 424.8543140888214 and batch: 350, loss is 3.7951546478271485 and perplexity is 44.485115400041316
At time: 425.37451219558716 and batch: 400, loss is 3.805081834793091 and perplexity is 44.9289267119958
At time: 425.9002516269684 and batch: 450, loss is 3.8466513633728026 and perplexity is 46.835963720207936
At time: 426.4202914237976 and batch: 500, loss is 3.883879837989807 and perplexity is 48.61245812073535
At time: 426.9326128959656 and batch: 550, loss is 3.7935114336013793 and perplexity is 44.412076851006994
At time: 427.45603132247925 and batch: 600, loss is 3.7320329761505127 and perplexity is 41.763926985008524
At time: 427.9662663936615 and batch: 650, loss is 3.701866416931152 and perplexity is 40.52286638693253
At time: 428.4891242980957 and batch: 700, loss is 3.7803500366210936 and perplexity is 43.831381639380204
At time: 429.0218155384064 and batch: 750, loss is 3.7413832759857177 and perplexity is 42.15626359866787
At time: 429.53725028038025 and batch: 800, loss is 3.833012022972107 and perplexity is 46.20148881364821
At time: 430.04264187812805 and batch: 850, loss is 3.7259235858917235 and perplexity is 41.50955268345966
At time: 430.5452570915222 and batch: 900, loss is 3.6895659732818604 and perplexity is 40.02747019504577
At time: 431.05078959465027 and batch: 950, loss is 3.6928363943099978 and perplexity is 40.158591168801
At time: 431.57339692115784 and batch: 1000, loss is 3.6162879562377928 and perplexity is 37.199226060514285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641031032655297 and perplexity of 103.65116024111788
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 433.07298517227173 and batch: 50, loss is 3.9279564809799195 and perplexity is 50.80305451271805
At time: 433.5871512889862 and batch: 100, loss is 3.8225571489334107 and perplexity is 45.720974303892106
At time: 434.08654832839966 and batch: 150, loss is 3.8903494691848755 and perplexity is 48.92798235843659
At time: 434.58790826797485 and batch: 200, loss is 3.925851879119873 and perplexity is 50.69624674303399
At time: 435.0893220901489 and batch: 250, loss is 3.9194174289703367 and perplexity is 50.37109149007343
At time: 435.58782148361206 and batch: 300, loss is 3.7721776151657105 and perplexity is 43.47463285193049
At time: 436.08689761161804 and batch: 350, loss is 3.795113945007324 and perplexity is 44.48330476725351
At time: 436.59194564819336 and batch: 400, loss is 3.8050194549560548 and perplexity is 44.92612414028218
At time: 437.1163170337677 and batch: 450, loss is 3.8466159009933474 and perplexity is 46.83430283493999
At time: 437.6362295150757 and batch: 500, loss is 3.8838389205932615 and perplexity is 48.6104690662031
At time: 438.1537494659424 and batch: 550, loss is 3.793458523750305 and perplexity is 44.40972707679856
At time: 438.6655421257019 and batch: 600, loss is 3.7319845294952394 and perplexity is 41.76190371144584
At time: 439.1661169528961 and batch: 650, loss is 3.7017953824996948 and perplexity is 40.51998797039226
At time: 439.6670322418213 and batch: 700, loss is 3.780299730300903 and perplexity is 43.829176699322744
At time: 440.1841678619385 and batch: 750, loss is 3.7413128566741944 and perplexity is 42.15329508813032
At time: 440.70111060142517 and batch: 800, loss is 3.832951879501343 and perplexity is 46.19871017931568
At time: 441.2134687900543 and batch: 850, loss is 3.72585675239563 and perplexity is 41.50677854763618
At time: 441.7140293121338 and batch: 900, loss is 3.689481468200684 and perplexity is 40.02408781334388
At time: 442.2509708404541 and batch: 950, loss is 3.692751793861389 and perplexity is 40.15519387768083
At time: 442.7540249824524 and batch: 1000, loss is 3.6162048530578614 and perplexity is 37.19613481498562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641031404820884 and perplexity of 103.65119881651998
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 444.2851142883301 and batch: 50, loss is 3.9279453325271607 and perplexity is 50.80248814042191
At time: 444.802330493927 and batch: 100, loss is 3.8225507831573484 and perplexity is 45.72068325533471
At time: 445.3037633895874 and batch: 150, loss is 3.8903406381607057 and perplexity is 48.927550276149674
At time: 445.8057236671448 and batch: 200, loss is 3.925845379829407 and perplexity is 50.69591725447158
At time: 446.3102855682373 and batch: 250, loss is 3.9194137382507326 and perplexity is 50.370905584841644
At time: 446.8121769428253 and batch: 300, loss is 3.772161827087402 and perplexity is 43.47394647644089
At time: 447.31728959083557 and batch: 350, loss is 3.7950996017456053 and perplexity is 44.48266673614685
At time: 447.8202748298645 and batch: 400, loss is 3.8049971723556517 and perplexity is 44.92512308056345
At time: 448.32327914237976 and batch: 450, loss is 3.8466028356552124 and perplexity is 46.833690932934495
At time: 448.82708191871643 and batch: 500, loss is 3.883823881149292 and perplexity is 48.60973799727469
At time: 449.32909297943115 and batch: 550, loss is 3.793439826965332 and perplexity is 44.40889676544279
At time: 449.83531856536865 and batch: 600, loss is 3.7319673347473143 and perplexity is 41.76118563221227
At time: 450.33687233924866 and batch: 650, loss is 3.7017700242996217 and perplexity is 40.51896046945819
At time: 450.8557312488556 and batch: 700, loss is 3.7802810287475586 and perplexity is 43.82835703330119
At time: 451.3593373298645 and batch: 750, loss is 3.7412875413894655 and perplexity is 42.15222797897004
At time: 451.86016488075256 and batch: 800, loss is 3.8329300832748414 and perplexity is 46.1977032327384
At time: 452.3611214160919 and batch: 850, loss is 3.725832715034485 and perplexity is 41.50578084620137
At time: 452.86316108703613 and batch: 900, loss is 3.6894512367248535 and perplexity is 40.02287784439019
At time: 453.36628341674805 and batch: 950, loss is 3.6927211809158327 and perplexity is 40.15396462773243
At time: 453.8700909614563 and batch: 1000, loss is 3.616174716949463 and perplexity is 37.19501388512515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641032521317645 and perplexity of 103.65131454281232
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 455.41632747650146 and batch: 50, loss is 3.9279416227340698 and perplexity is 50.80229967405199
At time: 455.9151132106781 and batch: 100, loss is 3.8225488662719727 and perplexity is 45.72059561410961
At time: 456.4150581359863 and batch: 150, loss is 3.890337738990784 and perplexity is 48.92740842707318
At time: 456.9152150154114 and batch: 200, loss is 3.9258434867858885 and perplexity is 50.69582128498485
At time: 457.41523838043213 and batch: 250, loss is 3.9194124221801756 and perplexity is 50.370839293219504
At time: 457.9208810329437 and batch: 300, loss is 3.7721566009521483 and perplexity is 43.47371927631027
At time: 458.4210715293884 and batch: 350, loss is 3.795094985961914 and perplexity is 44.482461414253045
At time: 458.9349195957184 and batch: 400, loss is 3.8049896574020385 and perplexity is 44.924785471616
At time: 459.44188499450684 and batch: 450, loss is 3.84659854888916 and perplexity is 46.833490168288414
At time: 459.9481716156006 and batch: 500, loss is 3.883818826675415 and perplexity is 48.60949230124476
At time: 460.4601626396179 and batch: 550, loss is 3.7934331035614015 and perplexity is 44.40859818749546
At time: 460.96015453338623 and batch: 600, loss is 3.7319616174697874 and perplexity is 41.76094687260668
At time: 461.467000246048 and batch: 650, loss is 3.7017613792419435 and perplexity is 40.518610182222
At time: 461.97382497787476 and batch: 700, loss is 3.7802744579315184 and perplexity is 43.82806904617593
At time: 462.47615790367126 and batch: 750, loss is 3.7412787246704102 and perplexity is 42.151856336256735
At time: 462.97359681129456 and batch: 800, loss is 3.8329225063323973 and perplexity is 46.197353196726056
At time: 463.4720199108124 and batch: 850, loss is 3.725824179649353 and perplexity is 41.50542657988854
At time: 463.9722316265106 and batch: 900, loss is 3.689440550804138 and perplexity is 40.022450165375815
At time: 464.4712703227997 and batch: 950, loss is 3.692710633277893 and perplexity is 40.153541100485306
At time: 464.97095370292664 and batch: 1000, loss is 3.616164093017578 and perplexity is 37.19461872993023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6410328934832314 and perplexity of 103.65135311827174
Annealing...
Finished 42 epochs...
Completing Train Step...
At time: 466.47082114219666 and batch: 50, loss is 3.9279405879974365 and perplexity is 50.80224710707866
At time: 466.9836583137512 and batch: 100, loss is 3.822548451423645 and perplexity is 45.72057664700092
At time: 467.48360872268677 and batch: 150, loss is 3.890337405204773 and perplexity is 48.927392095791426
At time: 468.014310836792 and batch: 200, loss is 3.9258432149887086 and perplexity is 50.695807506005465
At time: 468.5158808231354 and batch: 250, loss is 3.9194123697280885 and perplexity is 50.37083665116392
At time: 469.0278322696686 and batch: 300, loss is 3.7721553802490235 and perplexity is 43.473666207837695
At time: 469.53085136413574 and batch: 350, loss is 3.7950937032699583 and perplexity is 44.48240435699421
At time: 470.04178071022034 and batch: 400, loss is 3.8049874114990234 and perplexity is 44.924684575018155
At time: 470.557968378067 and batch: 450, loss is 3.8465973138809204 and perplexity is 46.833432328577885
At time: 471.069135427475 and batch: 500, loss is 3.883817377090454 and perplexity is 48.60942183770683
At time: 471.57195830345154 and batch: 550, loss is 3.793431181907654 and perplexity is 44.408512849628316
At time: 472.07016348838806 and batch: 600, loss is 3.7319601011276244 and perplexity is 41.76088354877018
At time: 472.56901812553406 and batch: 650, loss is 3.701758852005005 and perplexity is 40.518507782223054
At time: 473.08254051208496 and batch: 700, loss is 3.780272617340088 and perplexity is 43.82798837668188
At time: 473.5888841152191 and batch: 750, loss is 3.7412761783599855 and perplexity is 42.15174900468217
At time: 474.091876745224 and batch: 800, loss is 3.8329204654693605 and perplexity is 46.19725891435173
At time: 474.5959415435791 and batch: 850, loss is 3.7258218050003054 and perplexity is 41.505328019183864
At time: 475.09563064575195 and batch: 900, loss is 3.689437117576599 and perplexity is 40.022312759433596
At time: 475.59518694877625 and batch: 950, loss is 3.6927075386047363 and perplexity is 40.15341683859179
At time: 476.1074945926666 and batch: 1000, loss is 3.616160774230957 and perplexity is 37.19449528913205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641033265648819 and perplexity of 103.65139169374562
Annealing...
Model not improving. Stopping early with 103.65104451499782loss at 42 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -103.65104451499782
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd6e1b10898>
SETTINGS FOR THIS RUN
{'anneal': 5.2786219679906905, 'data': 'ptb', 'dropout': 0.902475331033966, 'tune_wordvecs': True, 'lr': 13.317757938907771, 'num_layers': 1, 'batch_size': 20, 'wordvec_source': '', 'wordvec_dim': 200, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7495660781860352 and batch: 50, loss is 7.729173469543457 and perplexity is 2273.7221233390583
At time: 1.267472505569458 and batch: 100, loss is 6.94829252243042 and perplexity is 1041.370093193507
At time: 1.7722606658935547 and batch: 150, loss is 6.646182174682617 and perplexity is 769.8395947828685
At time: 2.2781593799591064 and batch: 200, loss is 6.564195890426635 and perplexity is 709.2413590738331
At time: 2.783104419708252 and batch: 250, loss is 6.606766080856323 and perplexity is 740.0857670168731
At time: 3.287501335144043 and batch: 300, loss is 6.504648571014404 and perplexity is 668.240788902393
At time: 3.7929866313934326 and batch: 350, loss is 6.467064218521118 and perplexity is 643.5915067261413
At time: 4.298105716705322 and batch: 400, loss is 6.436322469711303 and perplexity is 624.1074004789485
At time: 4.804209470748901 and batch: 450, loss is 6.463658075332642 and perplexity is 641.4030710777513
At time: 5.310353994369507 and batch: 500, loss is 6.42960859298706 and perplexity is 619.9312550791748
At time: 5.814555406570435 and batch: 550, loss is 6.393584156036377 and perplexity is 597.996054460292
At time: 6.334636211395264 and batch: 600, loss is 6.318771333694458 and perplexity is 554.890797815911
At time: 6.840683460235596 and batch: 650, loss is 6.264101047515869 and perplexity is 525.3690917661016
At time: 7.345024347305298 and batch: 700, loss is 6.357463264465332 and perplexity is 576.7813572569637
At time: 7.847959995269775 and batch: 750, loss is 6.21573733329773 and perplexity is 500.56493635065124
At time: 8.352351665496826 and batch: 800, loss is 6.3183864307403566 and perplexity is 554.6772598069697
At time: 8.85716986656189 and batch: 850, loss is 6.304155521392822 and perplexity is 546.8395987995142
At time: 9.362258434295654 and batch: 900, loss is 6.316281442642212 and perplexity is 553.5108987954644
At time: 9.865943670272827 and batch: 950, loss is 6.288749666213989 and perplexity is 538.4796287243774
At time: 10.371554136276245 and batch: 1000, loss is 6.23109432220459 and perplexity is 508.31143573584455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.016824861852134 and perplexity of 410.273849297759
Finished 1 epochs...
Completing Train Step...
At time: 11.874258995056152 and batch: 50, loss is 5.721442222595215 and perplexity is 305.3449809807249
At time: 12.376040935516357 and batch: 100, loss is 5.457201137542724 and perplexity is 234.44033898512828
At time: 12.877703428268433 and batch: 150, loss is 5.349609365463257 and perplexity is 210.52604305863346
At time: 13.379040956497192 and batch: 200, loss is 5.277558917999268 and perplexity is 195.89110498020395
At time: 13.891683578491211 and batch: 250, loss is 5.30463397026062 and perplexity is 201.26731910502005
At time: 14.394992351531982 and batch: 300, loss is 5.211760225296021 and perplexity is 183.4166288016863
At time: 14.895851850509644 and batch: 350, loss is 5.161035194396972 and perplexity is 174.3448430256194
At time: 15.402566909790039 and batch: 400, loss is 5.159250726699829 and perplexity is 174.03400770535418
At time: 15.907908201217651 and batch: 450, loss is 5.169079933166504 and perplexity is 175.75305851172226
At time: 16.432055473327637 and batch: 500, loss is 5.186801452636718 and perplexity is 178.8954313319711
At time: 16.94901990890503 and batch: 550, loss is 5.100447740554809 and perplexity is 164.0953630029986
At time: 17.449991464614868 and batch: 600, loss is 5.019973526000976 and perplexity is 151.4072953844035
At time: 17.957035303115845 and batch: 650, loss is 4.992293853759765 and perplexity is 147.27386103139708
At time: 18.47812294960022 and batch: 700, loss is 5.06164698600769 and perplexity is 157.85027955317994
At time: 18.98968529701233 and batch: 750, loss is 4.994209585189819 and perplexity is 147.55626861797487
At time: 19.503815412521362 and batch: 800, loss is 5.097816524505615 and perplexity is 163.66416019292262
At time: 20.00523543357849 and batch: 850, loss is 5.00027943611145 and perplexity is 148.45463689357064
At time: 20.519259214401245 and batch: 900, loss is 5.0160495471954345 and perplexity is 150.81434049835389
At time: 21.01986861228943 and batch: 950, loss is 5.009015207290649 and perplexity is 149.75718372399112
At time: 21.51879644393921 and batch: 1000, loss is 4.900560035705566 and perplexity is 134.3650078197062
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.085837759622714 and perplexity of 161.71536110409926
Finished 2 epochs...
Completing Train Step...
At time: 22.98977279663086 and batch: 50, loss is 5.022513780593872 and perplexity is 151.79239738337733
At time: 23.499547004699707 and batch: 100, loss is 4.944588413238526 and perplexity is 140.41304680536874
At time: 24.01578116416931 and batch: 150, loss is 4.983141965866089 and perplexity is 145.9321760056598
At time: 24.530627727508545 and batch: 200, loss is 4.9866572189331055 and perplexity is 146.44606723461987
At time: 25.049436807632446 and batch: 250, loss is 5.024942979812622 and perplexity is 152.16157958352372
At time: 25.563528776168823 and batch: 300, loss is 4.929332485198975 and perplexity is 138.28717278975202
At time: 26.079795122146606 and batch: 350, loss is 4.9234512901306156 and perplexity is 137.47626583943568
At time: 26.582985877990723 and batch: 400, loss is 4.936982822418213 and perplexity is 139.3491734465685
At time: 27.082196474075317 and batch: 450, loss is 4.964767217636108 and perplexity is 143.27519445271943
At time: 27.57988452911377 and batch: 500, loss is 4.998866500854493 and perplexity is 148.24502821965933
At time: 28.08059048652649 and batch: 550, loss is 4.917693958282471 and perplexity is 136.68704344339213
At time: 28.57987880706787 and batch: 600, loss is 4.861265907287597 and perplexity is 129.18763821075373
At time: 29.098698139190674 and batch: 650, loss is 4.826953001022339 and perplexity is 124.83002363773359
At time: 29.60278868675232 and batch: 700, loss is 4.912131071090698 and perplexity is 135.92877986293576
At time: 30.102922201156616 and batch: 750, loss is 4.850794105529785 and perplexity is 127.84186948342035
At time: 30.613593816757202 and batch: 800, loss is 4.965031538009644 and perplexity is 143.31307001105586
At time: 31.116922616958618 and batch: 850, loss is 4.857588863372802 and perplexity is 128.71348187288433
At time: 31.61636233329773 and batch: 900, loss is 4.859528245925904 and perplexity is 128.9633487694107
At time: 32.13919281959534 and batch: 950, loss is 4.875224208831787 and perplexity is 131.00352206495947
At time: 32.64856719970703 and batch: 1000, loss is 4.765330944061279 and perplexity is 117.36995375027222
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.042002608136433 and perplexity of 154.77966788800018
Finished 3 epochs...
Completing Train Step...
At time: 34.1427276134491 and batch: 50, loss is 4.913453884124756 and perplexity is 136.1087072035208
At time: 34.66079092025757 and batch: 100, loss is 4.825144109725952 and perplexity is 124.60442379875526
At time: 35.159950733184814 and batch: 150, loss is 4.886428499221802 and perplexity is 132.47957720199594
At time: 35.66105103492737 and batch: 200, loss is 4.901920032501221 and perplexity is 134.5478681163131
At time: 36.161083698272705 and batch: 250, loss is 4.924301080703735 and perplexity is 137.5931415270691
At time: 36.66107654571533 and batch: 300, loss is 4.832642631530762 and perplexity is 125.54228467822732
At time: 37.173823833465576 and batch: 350, loss is 4.8304219150543215 and perplexity is 125.26380019014539
At time: 37.681191205978394 and batch: 400, loss is 4.842553701400757 and perplexity is 126.79272942007134
At time: 38.18090319633484 and batch: 450, loss is 4.864175271987915 and perplexity is 129.56403944381088
At time: 38.68113732337952 and batch: 500, loss is 4.908334283828736 and perplexity is 135.41366571124905
At time: 39.181623220443726 and batch: 550, loss is 4.8481561470031735 and perplexity is 127.50507235749293
At time: 39.68286633491516 and batch: 600, loss is 4.782726373672485 and perplexity is 119.42951606968288
At time: 40.18290996551514 and batch: 650, loss is 4.744230136871338 and perplexity is 114.919299307344
At time: 40.682077169418335 and batch: 700, loss is 4.8355263996124265 and perplexity is 125.90484202607962
At time: 41.18195700645447 and batch: 750, loss is 4.77518858909607 and perplexity is 118.5326664814249
At time: 41.68344497680664 and batch: 800, loss is 4.8881292819976805 and perplexity is 132.70508790305428
At time: 42.183552265167236 and batch: 850, loss is 4.807668590545655 and perplexity is 122.44581315678913
At time: 42.69209933280945 and batch: 900, loss is 4.787646141052246 and perplexity is 120.01852922282715
At time: 43.20096731185913 and batch: 950, loss is 4.80118844985962 and perplexity is 121.6549124028325
At time: 43.705049991607666 and batch: 1000, loss is 4.690578165054322 and perplexity is 108.91613310897829
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.083491999928544 and perplexity of 161.33646030694675
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 45.18248176574707 and batch: 50, loss is 4.848380670547486 and perplexity is 127.5337034623147
At time: 45.683974504470825 and batch: 100, loss is 4.712640323638916 and perplexity is 111.34576097901272
At time: 46.18557024002075 and batch: 150, loss is 4.732426195144654 and perplexity is 113.57077321457369
At time: 46.68744087219238 and batch: 200, loss is 4.729051847457885 and perplexity is 113.1881917830898
At time: 47.18998122215271 and batch: 250, loss is 4.741601371765137 and perplexity is 114.61760018509695
At time: 47.69138169288635 and batch: 300, loss is 4.627893409729004 and perplexity is 102.2983362852453
At time: 48.19345545768738 and batch: 350, loss is 4.625072317123413 and perplexity is 102.01014989656096
At time: 48.694249629974365 and batch: 400, loss is 4.626687364578247 and perplexity is 102.17503424169955
At time: 49.19519233703613 and batch: 450, loss is 4.645719308853149 and perplexity is 104.13824641363401
At time: 49.699588775634766 and batch: 500, loss is 4.6788608264923095 and perplexity is 107.64737366074851
At time: 50.20457315444946 and batch: 550, loss is 4.599049472808838 and perplexity is 99.38979802272199
At time: 50.708869218826294 and batch: 600, loss is 4.515007076263427 and perplexity is 91.37821263640458
At time: 51.21308732032776 and batch: 650, loss is 4.4843458080291745 and perplexity is 88.61895803501457
At time: 51.7365505695343 and batch: 700, loss is 4.553257436752319 and perplexity is 94.94117001256062
At time: 52.26426553726196 and batch: 750, loss is 4.4917434978485105 and perplexity is 89.27696446156634
At time: 52.77611017227173 and batch: 800, loss is 4.5792515754699705 and perplexity is 97.44143934640753
At time: 53.29043889045715 and batch: 850, loss is 4.477993021011352 and perplexity is 88.05776512577046
At time: 53.812864780426025 and batch: 900, loss is 4.4680087184906006 and perplexity is 87.18294426480936
At time: 54.322256326675415 and batch: 950, loss is 4.4437555313110355 and perplexity is 85.0939152043153
At time: 54.8376886844635 and batch: 1000, loss is 4.358379020690918 and perplexity is 78.13038398506892
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.85422050662157 and perplexity of 128.28065830855905
Finished 5 epochs...
Completing Train Step...
At time: 56.320191383361816 and batch: 50, loss is 4.659221878051758 and perplexity is 105.55391637425222
At time: 56.83420276641846 and batch: 100, loss is 4.570470275878907 and perplexity is 96.58952281641386
At time: 57.35381197929382 and batch: 150, loss is 4.608756589889526 and perplexity is 100.35928427430348
At time: 57.86551260948181 and batch: 200, loss is 4.620140533447266 and perplexity is 101.50829643804046
At time: 58.37958097457886 and batch: 250, loss is 4.636215114593506 and perplexity is 103.15318481328013
At time: 58.895594358444214 and batch: 300, loss is 4.526871976852417 and perplexity is 92.46886348252187
At time: 59.40458798408508 and batch: 350, loss is 4.5353055191040035 and perplexity is 93.25200122056593
At time: 59.904884338378906 and batch: 400, loss is 4.537102174758911 and perplexity is 93.4196935534706
At time: 60.4158673286438 and batch: 450, loss is 4.5636349964141845 and perplexity is 95.93155768411108
At time: 60.91823434829712 and batch: 500, loss is 4.597719421386719 and perplexity is 99.25769235365921
At time: 61.4158775806427 and batch: 550, loss is 4.527508716583252 and perplexity is 92.52776083091736
At time: 61.91473150253296 and batch: 600, loss is 4.4492222023010255 and perplexity is 85.56036945630605
At time: 62.427926540374756 and batch: 650, loss is 4.420586099624634 and perplexity is 83.1450023351071
At time: 62.94301080703735 and batch: 700, loss is 4.495394630432129 and perplexity is 89.60352228566234
At time: 63.44882035255432 and batch: 750, loss is 4.4391772747039795 and perplexity is 84.705223868078
At time: 63.94873023033142 and batch: 800, loss is 4.534752445220947 and perplexity is 93.20044023397998
At time: 64.44897556304932 and batch: 850, loss is 4.4352518081665036 and perplexity is 84.37336811669638
At time: 64.94747757911682 and batch: 900, loss is 4.428746681213379 and perplexity is 83.82628998120627
At time: 65.44558644294739 and batch: 950, loss is 4.408214950561524 and perplexity is 82.12273943838555
At time: 65.94379687309265 and batch: 1000, loss is 4.328039121627808 and perplexity is 75.79551496933796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.837866806402439 and perplexity of 126.19985566455271
Finished 6 epochs...
Completing Train Step...
At time: 67.41371607780457 and batch: 50, loss is 4.598000469207764 and perplexity is 99.28559243226167
At time: 67.9235246181488 and batch: 100, loss is 4.515364179611206 and perplexity is 91.41084992914747
At time: 68.42214488983154 and batch: 150, loss is 4.548807888031006 and perplexity is 94.5196631041269
At time: 68.91991019248962 and batch: 200, loss is 4.565445604324341 and perplexity is 96.1054094624953
At time: 69.41808652877808 and batch: 250, loss is 4.582102527618408 and perplexity is 97.71963660236075
At time: 69.92512512207031 and batch: 300, loss is 4.4685335540771485 and perplexity is 87.22871298597578
At time: 70.43475365638733 and batch: 350, loss is 4.4771780586242675 and perplexity is 87.98603059374523
At time: 70.96599531173706 and batch: 400, loss is 4.481736307144165 and perplexity is 88.38800824848403
At time: 71.4736225605011 and batch: 450, loss is 4.508784484863281 and perplexity is 90.81136880411593
At time: 71.9841365814209 and batch: 500, loss is 4.547640104293823 and perplexity is 94.40934900275309
At time: 72.48445200920105 and batch: 550, loss is 4.479876289367676 and perplexity is 88.22375778365985
At time: 72.98493885993958 and batch: 600, loss is 4.406263561248779 and perplexity is 81.96264225907763
At time: 73.4843077659607 and batch: 650, loss is 4.3779272365570066 and perplexity is 79.67271943564539
At time: 73.98484706878662 and batch: 700, loss is 4.456088218688965 and perplexity is 86.14984972998057
At time: 74.49281764030457 and batch: 750, loss is 4.401480684280395 and perplexity is 81.57156101740864
At time: 75.01481366157532 and batch: 800, loss is 4.499405517578125 and perplexity is 89.96363360159238
At time: 75.5292010307312 and batch: 850, loss is 4.400298147201538 and perplexity is 81.47515663403853
At time: 76.04241275787354 and batch: 900, loss is 4.3925488090515135 and perplexity is 80.8462181702979
At time: 76.56170654296875 and batch: 950, loss is 4.372628607749939 and perplexity is 79.25167972096028
At time: 77.06677961349487 and batch: 1000, loss is 4.295649299621582 and perplexity is 73.37984448552322
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.831589117282775 and perplexity of 125.41009373734028
Finished 7 epochs...
Completing Train Step...
At time: 78.54973363876343 and batch: 50, loss is 4.553056812286377 and perplexity is 94.92212440160253
At time: 79.04853534698486 and batch: 100, loss is 4.470177869796753 and perplexity is 87.37226251797173
At time: 79.54906702041626 and batch: 150, loss is 4.500698881149292 and perplexity is 90.08006456557584
At time: 80.04981708526611 and batch: 200, loss is 4.520731582641601 and perplexity is 91.90280788932321
At time: 80.54977178573608 and batch: 250, loss is 4.539538888931275 and perplexity is 93.64760821339944
At time: 81.04893398284912 and batch: 300, loss is 4.420705080032349 and perplexity is 83.15489554992206
At time: 81.56073832511902 and batch: 350, loss is 4.435803823471069 and perplexity is 84.41995636472447
At time: 82.06728267669678 and batch: 400, loss is 4.440652456283569 and perplexity is 84.83027166544983
At time: 82.56966471672058 and batch: 450, loss is 4.468841524124145 and perplexity is 87.25558095386585
At time: 83.07924771308899 and batch: 500, loss is 4.510379629135132 and perplexity is 90.95634163443255
At time: 83.60104393959045 and batch: 550, loss is 4.440961384773255 and perplexity is 84.85648220153726
At time: 84.12988591194153 and batch: 600, loss is 4.37014594078064 and perplexity is 79.055168230684
At time: 84.64140367507935 and batch: 650, loss is 4.343377313613892 and perplexity is 76.96704272068077
At time: 85.16269993782043 and batch: 700, loss is 4.422670364379883 and perplexity is 83.3184792561645
At time: 85.6699697971344 and batch: 750, loss is 4.37206199169159 and perplexity is 79.20678716620152
At time: 86.16974520683289 and batch: 800, loss is 4.467459259033203 and perplexity is 87.13505392966293
At time: 86.6701602935791 and batch: 850, loss is 4.371511964797974 and perplexity is 79.16323328210503
At time: 87.17201805114746 and batch: 900, loss is 4.363317365646362 and perplexity is 78.51717303550676
At time: 87.69282531738281 and batch: 950, loss is 4.343891792297363 and perplexity is 77.00665081137592
At time: 88.19804048538208 and batch: 1000, loss is 4.266622533798218 and perplexity is 71.2804811869343
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.826730867711509 and perplexity of 124.80229781082083
Finished 8 epochs...
Completing Train Step...
At time: 89.69209384918213 and batch: 50, loss is 4.516349258422852 and perplexity is 91.50094118677652
At time: 90.21008801460266 and batch: 100, loss is 4.43664927482605 and perplexity is 84.49135951091063
At time: 90.71469044685364 and batch: 150, loss is 4.462313985824585 and perplexity is 86.68787169508462
At time: 91.22144174575806 and batch: 200, loss is 4.484232730865479 and perplexity is 88.60893782112967
At time: 91.72575092315674 and batch: 250, loss is 4.502827348709107 and perplexity is 90.27200125383486
At time: 92.2320408821106 and batch: 300, loss is 4.3848358917236325 and perplexity is 80.22505653680061
At time: 92.73777675628662 and batch: 350, loss is 4.400257873535156 and perplexity is 81.47187539683598
At time: 93.25350522994995 and batch: 400, loss is 4.404796895980835 and perplexity is 81.8425186105321
At time: 93.76938509941101 and batch: 450, loss is 4.434787874221802 and perplexity is 84.33423352583272
At time: 94.30124282836914 and batch: 500, loss is 4.479975538253784 and perplexity is 88.23251432787964
At time: 94.82338047027588 and batch: 550, loss is 4.409796085357666 and perplexity is 82.25268926629069
At time: 95.34459900856018 and batch: 600, loss is 4.341071214675903 and perplexity is 76.789753606891
At time: 95.87294745445251 and batch: 650, loss is 4.315516633987427 and perplexity is 74.8522846905337
At time: 96.38497805595398 and batch: 700, loss is 4.395240659713745 and perplexity is 81.06413728749062
At time: 96.92411613464355 and batch: 750, loss is 4.345571208000183 and perplexity is 77.13608564699594
At time: 97.43707537651062 and batch: 800, loss is 4.44191689491272 and perplexity is 84.93760217998926
At time: 97.9415271282196 and batch: 850, loss is 4.345402178764343 and perplexity is 77.12304849524428
At time: 98.44616913795471 and batch: 900, loss is 4.338091459274292 and perplexity is 76.56127949133312
At time: 98.95413374900818 and batch: 950, loss is 4.317355628013611 and perplexity is 74.99006424398398
At time: 99.45718002319336 and batch: 1000, loss is 4.242779240608216 and perplexity is 69.60102126693442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.824758390100991 and perplexity of 124.55637069524742
Finished 9 epochs...
Completing Train Step...
At time: 100.9334044456482 and batch: 50, loss is 4.4862133884429936 and perplexity is 88.78461570669616
At time: 101.44887185096741 and batch: 100, loss is 4.407886066436768 and perplexity is 82.09573501400943
At time: 101.94905519485474 and batch: 150, loss is 4.430783853530884 and perplexity is 83.99723263934196
At time: 102.4499192237854 and batch: 200, loss is 4.4528693580627445 and perplexity is 85.87299119429541
At time: 102.95052194595337 and batch: 250, loss is 4.473087387084961 and perplexity is 87.6268438010077
At time: 103.4509174823761 and batch: 300, loss is 4.351141052246094 and perplexity is 77.56692035708363
At time: 103.95341372489929 and batch: 350, loss is 4.369832849502563 and perplexity is 79.03042062135655
At time: 104.46265482902527 and batch: 400, loss is 4.374176006317139 and perplexity is 79.37440858735577
At time: 104.97089147567749 and batch: 450, loss is 4.40597170829773 and perplexity is 81.93872471043187
At time: 105.46997737884521 and batch: 500, loss is 4.452186479568481 and perplexity is 85.81437039309101
At time: 105.97893738746643 and batch: 550, loss is 4.383116121292114 and perplexity is 80.08720642594963
At time: 106.4834771156311 and batch: 600, loss is 4.315941748619079 and perplexity is 74.88411225667096
At time: 106.98616600036621 and batch: 650, loss is 4.291018018722534 and perplexity is 73.04078755335823
At time: 107.48802495002747 and batch: 700, loss is 4.371158514022827 and perplexity is 79.13525792018626
At time: 107.98883938789368 and batch: 750, loss is 4.323427734375 and perplexity is 75.44679715219041
At time: 108.48879885673523 and batch: 800, loss is 4.4189301776885985 and perplexity is 83.00743463399075
At time: 109.01135110855103 and batch: 850, loss is 4.3226128721237185 and perplexity is 75.38534344676384
At time: 109.54423260688782 and batch: 900, loss is 4.315790996551514 and perplexity is 74.87282417279285
At time: 110.05613875389099 and batch: 950, loss is 4.29308678150177 and perplexity is 73.19204802308276
At time: 110.5761239528656 and batch: 1000, loss is 4.2196086359024045 and perplexity is 68.0068636357583
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.82475727360423 and perplexity of 124.55623162854063
Finished 10 epochs...
Completing Train Step...
At time: 112.07866907119751 and batch: 50, loss is 4.460680475234986 and perplexity is 86.54638173281887
At time: 112.58071160316467 and batch: 100, loss is 4.3833183002471925 and perplexity is 80.10340001060581
At time: 113.08323192596436 and batch: 150, loss is 4.400650568008423 and perplexity is 81.50387523469954
At time: 113.5848696231842 and batch: 200, loss is 4.42594430923462 and perplexity is 83.59170638385054
At time: 114.104407787323 and batch: 250, loss is 4.445437850952149 and perplexity is 85.23719085313024
At time: 114.62459063529968 and batch: 300, loss is 4.32462818145752 and perplexity is 75.53742142365537
At time: 115.13957333564758 and batch: 350, loss is 4.344086933135986 and perplexity is 77.02167942009478
At time: 115.64316844940186 and batch: 400, loss is 4.3477958297729495 and perplexity is 77.3078752752178
At time: 116.14315724372864 and batch: 450, loss is 4.379422397613525 and perplexity is 79.79193208185715
At time: 116.64282751083374 and batch: 500, loss is 4.427149925231934 and perplexity is 83.69254665741883
At time: 117.14293026924133 and batch: 550, loss is 4.357457914352417 and perplexity is 78.05845072732652
At time: 117.65120244026184 and batch: 600, loss is 4.292899208068848 and perplexity is 73.1783204268789
At time: 118.15979194641113 and batch: 650, loss is 4.269077587127685 and perplexity is 71.45569355945298
At time: 118.66018652915955 and batch: 700, loss is 4.348123216629029 and perplexity is 77.33318900091875
At time: 119.15873456001282 and batch: 750, loss is 4.302197484970093 and perplexity is 73.86192596491381
At time: 119.66012501716614 and batch: 800, loss is 4.396240930557251 and perplexity is 81.14526394802564
At time: 120.15757942199707 and batch: 850, loss is 4.302341909408569 and perplexity is 73.87259420245461
At time: 120.65887355804443 and batch: 900, loss is 4.2934055995941165 and perplexity is 73.21538669240576
At time: 121.15941548347473 and batch: 950, loss is 4.269538559913635 and perplexity is 71.48864028277306
At time: 121.66124987602234 and batch: 1000, loss is 4.19997205734253 and perplexity is 66.68446767365275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.82242119021532 and perplexity of 124.26559748981266
Finished 11 epochs...
Completing Train Step...
At time: 123.14492225646973 and batch: 50, loss is 4.436289100646973 and perplexity is 84.46093338454115
At time: 123.66616249084473 and batch: 100, loss is 4.359223003387451 and perplexity is 78.19635251145742
At time: 124.16604375839233 and batch: 150, loss is 4.374619035720825 and perplexity is 79.40958157501876
At time: 124.66658520698547 and batch: 200, loss is 4.400013132095337 and perplexity is 81.45193829256391
At time: 125.16799759864807 and batch: 250, loss is 4.420855484008789 and perplexity is 83.16740331745866
At time: 125.66828799247742 and batch: 300, loss is 4.296668863296508 and perplexity is 73.4546980619419
At time: 126.16808438301086 and batch: 350, loss is 4.319212455749511 and perplexity is 75.1294372309588
At time: 126.66827654838562 and batch: 400, loss is 4.323435926437378 and perplexity is 75.44741521959057
At time: 127.17069721221924 and batch: 450, loss is 4.358011703491211 and perplexity is 78.10169062131718
At time: 127.67383003234863 and batch: 500, loss is 4.4051034641265865 and perplexity is 81.86761276604402
At time: 128.17801976203918 and batch: 550, loss is 4.334148645401001 and perplexity is 76.26000693749326
At time: 128.68204045295715 and batch: 600, loss is 4.2716720676422115 and perplexity is 71.64132466806336
At time: 129.18540573120117 and batch: 650, loss is 4.2483971357345585 and perplexity is 69.99313289482266
At time: 129.68694138526917 and batch: 700, loss is 4.329086508750915 and perplexity is 75.87494380481463
At time: 130.18777227401733 and batch: 750, loss is 4.28226529121399 and perplexity is 72.40427113391564
At time: 130.69022464752197 and batch: 800, loss is 4.375515308380127 and perplexity is 79.48078611644628
At time: 131.19288110733032 and batch: 850, loss is 4.2851969385147095 and perplexity is 72.61684636549879
At time: 131.69610023498535 and batch: 900, loss is 4.274834942817688 and perplexity is 71.86827595549325
At time: 132.1991593837738 and batch: 950, loss is 4.24857500076294 and perplexity is 70.0055833326074
At time: 132.70178604125977 and batch: 1000, loss is 4.180532503128052 and perplexity is 65.40067000451634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.824399250309642 and perplexity of 124.51164557803014
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 134.17236995697021 and batch: 50, loss is 4.4151174831390385 and perplexity is 82.69155519894099
At time: 134.68750262260437 and batch: 100, loss is 4.321742010116577 and perplexity is 75.31972179311057
At time: 135.18693614006042 and batch: 150, loss is 4.333499593734741 and perplexity is 76.21052631239962
At time: 135.72288060188293 and batch: 200, loss is 4.354150295257568 and perplexity is 77.80068962795316
At time: 136.2271010875702 and batch: 250, loss is 4.370277299880981 and perplexity is 79.06555352854687
At time: 136.72800064086914 and batch: 300, loss is 4.24118558883667 and perplexity is 69.49018981291928
At time: 137.22809219360352 and batch: 350, loss is 4.257791814804077 and perplexity is 70.65379440772533
At time: 137.72911262512207 and batch: 400, loss is 4.25560450553894 and perplexity is 70.49942160068326
At time: 138.22895622253418 and batch: 450, loss is 4.289419560432434 and perplexity is 72.92412816340205
At time: 138.72929310798645 and batch: 500, loss is 4.327904100418091 and perplexity is 75.78528165808825
At time: 139.22890424728394 and batch: 550, loss is 4.255576829910279 and perplexity is 70.49747051186911
At time: 139.72993421554565 and batch: 600, loss is 4.186281490325928 and perplexity is 65.77774046719499
At time: 140.23016810417175 and batch: 650, loss is 4.156132044792176 and perplexity is 63.82417549257053
At time: 140.73122906684875 and batch: 700, loss is 4.238731713294983 and perplexity is 69.3198785824924
At time: 141.23184323310852 and batch: 750, loss is 4.181528835296631 and perplexity is 65.46586326756808
At time: 141.7409691810608 and batch: 800, loss is 4.268183078765869 and perplexity is 71.3918044230081
At time: 142.25023293495178 and batch: 850, loss is 4.176736211776733 and perplexity is 65.15286068241628
At time: 142.75017309188843 and batch: 900, loss is 4.160087494850159 and perplexity is 64.07712877345804
At time: 143.24796962738037 and batch: 950, loss is 4.1168214797973635 and perplexity is 61.36388560624825
At time: 143.74786925315857 and batch: 1000, loss is 4.050705313682556 and perplexity is 57.437954537282984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.778718529677972 and perplexity of 118.95181910781491
Finished 13 epochs...
Completing Train Step...
At time: 145.2629702091217 and batch: 50, loss is 4.37337209701538 and perplexity is 79.31062440375521
At time: 145.7626497745514 and batch: 100, loss is 4.285272169113159 and perplexity is 72.62230957980624
At time: 146.2618067264557 and batch: 150, loss is 4.303149418830872 and perplexity is 73.93227110991276
At time: 146.7610948085785 and batch: 200, loss is 4.327951698303223 and perplexity is 75.78888896306867
At time: 147.2619490623474 and batch: 250, loss is 4.349350905418396 and perplexity is 77.42818839301343
At time: 147.76140832901 and batch: 300, loss is 4.221569781303406 and perplexity is 68.14036584945181
At time: 148.2737100124359 and batch: 350, loss is 4.2379964113235475 and perplexity is 69.26892627407315
At time: 148.77287554740906 and batch: 400, loss is 4.2371084594726565 and perplexity is 69.20744610253446
At time: 149.27371191978455 and batch: 450, loss is 4.271217856407166 and perplexity is 71.60879176247049
At time: 149.77397680282593 and batch: 500, loss is 4.31283974647522 and perplexity is 74.65218149075416
At time: 150.27468633651733 and batch: 550, loss is 4.241564288139343 and perplexity is 69.51651068287704
At time: 150.7754156589508 and batch: 600, loss is 4.174667568206787 and perplexity is 65.01822194389548
At time: 151.2866644859314 and batch: 650, loss is 4.14562804222107 and perplexity is 63.15727488376291
At time: 151.79901480674744 and batch: 700, loss is 4.228510904312134 and perplexity is 68.61498178893211
At time: 152.30014777183533 and batch: 750, loss is 4.175626616477967 and perplexity is 65.08060746782868
At time: 152.80000925064087 and batch: 800, loss is 4.263296294212341 and perplexity is 71.04377911085443
At time: 153.29988050460815 and batch: 850, loss is 4.172850170135498 and perplexity is 64.90016526322236
At time: 153.80104780197144 and batch: 900, loss is 4.158405799865722 and perplexity is 63.969461144812705
At time: 154.300936460495 and batch: 950, loss is 4.11756208896637 and perplexity is 61.40934909583465
At time: 154.80008554458618 and batch: 1000, loss is 4.054595308303833 and perplexity is 57.661823012773766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.775600898556593 and perplexity of 118.58154869780394
Finished 14 epochs...
Completing Train Step...
At time: 156.27298402786255 and batch: 50, loss is 4.360571699142456 and perplexity is 78.30188675093983
At time: 156.78818464279175 and batch: 100, loss is 4.271098771095276 and perplexity is 71.60026471490254
At time: 157.2887921333313 and batch: 150, loss is 4.290757122039795 and perplexity is 73.0217339398013
At time: 157.7885673046112 and batch: 200, loss is 4.316186084747314 and perplexity is 74.90241138620426
At time: 158.28885126113892 and batch: 250, loss is 4.339750318527222 and perplexity is 76.68838927770759
At time: 158.78988695144653 and batch: 300, loss is 4.212400999069214 and perplexity is 67.5184571033563
At time: 159.29022550582886 and batch: 350, loss is 4.22817108631134 and perplexity is 68.59166914424861
At time: 159.79015183448792 and batch: 400, loss is 4.227389602661133 and perplexity is 68.53808681585573
At time: 160.29094886779785 and batch: 450, loss is 4.261026253700257 and perplexity is 70.88268976298963
At time: 160.79084467887878 and batch: 500, loss is 4.303472490310669 and perplexity is 73.95616037690763
At time: 161.3041045665741 and batch: 550, loss is 4.23352997303009 and perplexity is 68.96023078707022
At time: 161.80380010604858 and batch: 600, loss is 4.167387609481811 and perplexity is 64.54661070979316
At time: 162.30297422409058 and batch: 650, loss is 4.139292659759522 and perplexity is 62.75841419374599
At time: 162.80298256874084 and batch: 700, loss is 4.221285796165466 and perplexity is 68.12101774567378
At time: 163.30308198928833 and batch: 750, loss is 4.1714415359497075 and perplexity is 64.80880903063142
At time: 163.80304956436157 and batch: 800, loss is 4.25883686542511 and perplexity is 70.72766979448541
At time: 164.30393767356873 and batch: 850, loss is 4.169884486198425 and perplexity is 64.70797701125619
At time: 164.80406069755554 and batch: 900, loss is 4.156124086380005 and perplexity is 63.82366755549668
At time: 165.30474758148193 and batch: 950, loss is 4.1160555267333985 and perplexity is 61.31690174606424
At time: 165.8047752380371 and batch: 1000, loss is 4.0542004203796385 and perplexity is 57.6390575503779
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.774390616068026 and perplexity of 118.43811833908143
Finished 15 epochs...
Completing Train Step...
At time: 167.28380918502808 and batch: 50, loss is 4.351649923324585 and perplexity is 77.60640196417297
At time: 167.80229592323303 and batch: 100, loss is 4.260959277153015 and perplexity is 70.87794244415139
At time: 168.30629324913025 and batch: 150, loss is 4.28128689289093 and perplexity is 72.33346556013147
At time: 168.8205783367157 and batch: 200, loss is 4.307210416793823 and perplexity is 74.233120372949
At time: 169.32862663269043 and batch: 250, loss is 4.332683291435242 and perplexity is 76.14834086903805
At time: 169.83072447776794 and batch: 300, loss is 4.205429239273071 and perplexity is 67.04937171396165
At time: 170.33164024353027 and batch: 350, loss is 4.220245351791382 and perplexity is 68.05017847455441
At time: 170.833660364151 and batch: 400, loss is 4.219509592056275 and perplexity is 68.00012830797247
At time: 171.3340106010437 and batch: 450, loss is 4.252800869941711 and perplexity is 70.30204372980059
At time: 171.85483813285828 and batch: 500, loss is 4.296275730133057 and perplexity is 73.42582625972575
At time: 172.36426901817322 and batch: 550, loss is 4.226921663284302 and perplexity is 68.50602264885018
At time: 172.86895322799683 and batch: 600, loss is 4.162038693428039 and perplexity is 64.20227803181751
At time: 173.37604069709778 and batch: 650, loss is 4.133887157440186 and perplexity is 62.42008867376221
At time: 173.897723197937 and batch: 700, loss is 4.2155740404129025 and perplexity is 67.73303621336584
At time: 174.4158275127411 and batch: 750, loss is 4.16768708229065 and perplexity is 64.56594355929029
At time: 174.93393731117249 and batch: 800, loss is 4.255052938461303 and perplexity is 70.46054716265151
At time: 175.43837451934814 and batch: 850, loss is 4.166495018005371 and perplexity is 64.48902266036495
At time: 175.94425582885742 and batch: 900, loss is 4.153340201377869 and perplexity is 63.646236892479344
At time: 176.44853281974792 and batch: 950, loss is 4.113524980545044 and perplexity is 61.1619326549879
At time: 176.95121383666992 and batch: 1000, loss is 4.052319178581238 and perplexity is 57.53072647651508
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.77338130299638 and perplexity of 118.31863750498745
Finished 16 epochs...
Completing Train Step...
At time: 178.46450448036194 and batch: 50, loss is 4.344185523986816 and perplexity is 77.02927342734483
At time: 178.9701211452484 and batch: 100, loss is 4.252574377059936 and perplexity is 70.28612262039859
At time: 179.4709370136261 and batch: 150, loss is 4.273249797821045 and perplexity is 71.75444456091299
At time: 179.9707601070404 and batch: 200, loss is 4.29948582649231 and perplexity is 73.66190895865296
At time: 180.47439455986023 and batch: 250, loss is 4.3265589284896855 and perplexity is 75.68340596014193
At time: 180.97317504882812 and batch: 300, loss is 4.199730944633484 and perplexity is 66.66839113920696
At time: 181.47572255134583 and batch: 350, loss is 4.213591065406799 and perplexity is 67.59885637706647
At time: 181.97818422317505 and batch: 400, loss is 4.212876291275024 and perplexity is 67.55055572727449
At time: 182.4805691242218 and batch: 450, loss is 4.2459598684310915 and perplexity is 69.82274864088053
At time: 182.98104858398438 and batch: 500, loss is 4.29014404296875 and perplexity is 72.97697956337893
At time: 183.4812605381012 and batch: 550, loss is 4.221597027778626 and perplexity is 68.14222245953434
At time: 183.98286747932434 and batch: 600, loss is 4.156693978309631 and perplexity is 63.86005051474846
At time: 184.4977581501007 and batch: 650, loss is 4.12892110824585 and perplexity is 62.11087586118731
At time: 185.0021584033966 and batch: 700, loss is 4.210398139953614 and perplexity is 67.38336247898592
At time: 185.52193355560303 and batch: 750, loss is 4.163823356628418 and perplexity is 64.31695977850926
At time: 186.0389745235443 and batch: 800, loss is 4.250929827690125 and perplexity is 70.17062861556302
At time: 186.53925776481628 and batch: 850, loss is 4.16317304611206 and perplexity is 64.27514738017986
At time: 187.05428504943848 and batch: 900, loss is 4.150371456146241 and perplexity is 63.45756762425778
At time: 187.55380535125732 and batch: 950, loss is 4.110028119087219 and perplexity is 60.94843135978426
At time: 188.05545568466187 and batch: 1000, loss is 4.049710760116577 and perplexity is 57.38085781235744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.772866597989711 and perplexity of 118.25775397975312
Finished 17 epochs...
Completing Train Step...
At time: 189.56522870063782 and batch: 50, loss is 4.337379474639892 and perplexity is 76.50678843744375
At time: 190.08095788955688 and batch: 100, loss is 4.245383081436157 and perplexity is 69.78248739971022
At time: 190.59123063087463 and batch: 150, loss is 4.266307926177978 and perplexity is 71.25805933160673
At time: 191.10093927383423 and batch: 200, loss is 4.292734498977661 and perplexity is 73.16626828480277
At time: 191.60444951057434 and batch: 250, loss is 4.320741081237793 and perplexity is 75.24436982569699
At time: 192.11174249649048 and batch: 300, loss is 4.19450873374939 and perplexity is 66.32114223404656
At time: 192.64389777183533 and batch: 350, loss is 4.2075854635238645 and perplexity is 67.194101173763
At time: 193.16312766075134 and batch: 400, loss is 4.20671745300293 and perplexity is 67.13580129311477
At time: 193.67670512199402 and batch: 450, loss is 4.239499673843384 and perplexity is 69.37313396086118
At time: 194.18729901313782 and batch: 500, loss is 4.284465684890747 and perplexity is 72.56376444397239
At time: 194.68970203399658 and batch: 550, loss is 4.21615020275116 and perplexity is 67.77207268247956
At time: 195.19130945205688 and batch: 600, loss is 4.151935820579529 and perplexity is 63.55691607440234
At time: 195.69915962219238 and batch: 650, loss is 4.124427790641785 and perplexity is 61.832418037499814
At time: 196.2048532962799 and batch: 700, loss is 4.20531129360199 and perplexity is 67.0414639971689
At time: 196.71095943450928 and batch: 750, loss is 4.160383868217468 and perplexity is 64.0961223423351
At time: 197.21286177635193 and batch: 800, loss is 4.2471333360672 and perplexity is 69.90473146936498
At time: 197.71537923812866 and batch: 850, loss is 4.15986243724823 and perplexity is 64.06270935117897
At time: 198.21609234809875 and batch: 900, loss is 4.147322807312012 and perplexity is 63.26440238080486
At time: 198.73318362236023 and batch: 950, loss is 4.107145843505859 and perplexity is 60.77301410622706
At time: 199.25445580482483 and batch: 1000, loss is 4.046971092224121 and perplexity is 57.22386846609699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.772329935213414 and perplexity of 118.19430647166448
Finished 18 epochs...
Completing Train Step...
At time: 200.82822918891907 and batch: 50, loss is 4.330512409210205 and perplexity is 75.98321109287836
At time: 201.3527889251709 and batch: 100, loss is 4.238353929519653 and perplexity is 69.29369560311993
At time: 201.8600378036499 and batch: 150, loss is 4.260560073852539 and perplexity is 70.84965338251583
At time: 202.37098908424377 and batch: 200, loss is 4.286592359542847 and perplexity is 72.71824817256703
At time: 202.87901329994202 and batch: 250, loss is 4.315653972625732 and perplexity is 74.86256550734768
At time: 203.3892297744751 and batch: 300, loss is 4.189316148757935 and perplexity is 65.97765662715089
At time: 203.89614295959473 and batch: 350, loss is 4.201919484138489 and perplexity is 66.81445732445744
At time: 204.4122109413147 and batch: 400, loss is 4.201157903671264 and perplexity is 66.76359211026623
At time: 204.93149662017822 and batch: 450, loss is 4.233901391029358 and perplexity is 68.9858486151849
At time: 205.43904852867126 and batch: 500, loss is 4.279367742538452 and perplexity is 72.19477988610242
At time: 205.95429348945618 and batch: 550, loss is 4.211258802413941 and perplexity is 67.4413817734551
At time: 206.45996856689453 and batch: 600, loss is 4.147371373176575 and perplexity is 63.26747494581282
At time: 206.96538281440735 and batch: 650, loss is 4.119965229034424 and perplexity is 61.5571018273476
At time: 207.47155618667603 and batch: 700, loss is 4.200708346366882 and perplexity is 66.73358479527549
At time: 207.9769275188446 and batch: 750, loss is 4.156587719917297 and perplexity is 63.853265208950624
At time: 208.49808406829834 and batch: 800, loss is 4.243181657791138 and perplexity is 69.62903555017759
At time: 209.01194381713867 and batch: 850, loss is 4.156434426307678 and perplexity is 63.84347766164416
At time: 209.5267629623413 and batch: 900, loss is 4.1441786670684815 and perplexity is 63.065802603679046
At time: 210.03865265846252 and batch: 950, loss is 4.104075794219971 and perplexity is 60.586724063793255
At time: 210.5429756641388 and batch: 1000, loss is 4.043826885223389 and perplexity is 57.044227340693865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.772160227705792 and perplexity of 118.17424971243751
Finished 19 epochs...
Completing Train Step...
At time: 212.0511658191681 and batch: 50, loss is 4.324421043395996 and perplexity is 75.52177636900828
At time: 212.55520176887512 and batch: 100, loss is 4.2318103313446045 and perplexity is 68.84174580462386
At time: 213.0863687992096 and batch: 150, loss is 4.254881687164307 and perplexity is 70.44848173570232
At time: 213.60517930984497 and batch: 200, loss is 4.281326360702515 and perplexity is 72.33632046005945
At time: 214.1129503250122 and batch: 250, loss is 4.310805869102478 and perplexity is 74.5005024086079
At time: 214.62181735038757 and batch: 300, loss is 4.184779624938965 and perplexity is 65.67902530255435
At time: 215.12209391593933 and batch: 350, loss is 4.19711522102356 and perplexity is 66.49423292863942
At time: 215.62295389175415 and batch: 400, loss is 4.196184210777282 and perplexity is 66.43235492546155
At time: 216.12545442581177 and batch: 450, loss is 4.228773627281189 and perplexity is 68.63301088889503
At time: 216.62589812278748 and batch: 500, loss is 4.274528102874756 and perplexity is 71.84622728068048
At time: 217.12655520439148 and batch: 550, loss is 4.206744179725647 and perplexity is 67.13759563703871
At time: 217.62706303596497 and batch: 600, loss is 4.142728500366211 and perplexity is 62.9744129578178
At time: 218.12795877456665 and batch: 650, loss is 4.115820679664612 and perplexity is 61.30250334219975
At time: 218.62800526618958 and batch: 700, loss is 4.196460328102112 and perplexity is 66.45070058224597
At time: 219.12810683250427 and batch: 750, loss is 4.153436222076416 and perplexity is 63.65234854202338
At time: 219.63192343711853 and batch: 800, loss is 4.239591951370239 and perplexity is 69.37953583746341
At time: 220.14322328567505 and batch: 850, loss is 4.153334856033325 and perplexity is 63.6458966823235
At time: 220.6461591720581 and batch: 900, loss is 4.141005773544311 and perplexity is 62.86601864122786
At time: 221.1496980190277 and batch: 950, loss is 4.100725111961364 and perplexity is 60.384056928566004
At time: 221.66103148460388 and batch: 1000, loss is 4.040487041473389 and perplexity is 56.85402633216188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.771999824337843 and perplexity of 118.15529568496436
Finished 20 epochs...
Completing Train Step...
At time: 223.16414666175842 and batch: 50, loss is 4.318823175430298 and perplexity is 75.10019651143878
At time: 223.6812505722046 and batch: 100, loss is 4.22594256401062 and perplexity is 68.43898127726666
At time: 224.18340516090393 and batch: 150, loss is 4.2495033645629885 and perplexity is 70.07060415879543
At time: 224.68598222732544 and batch: 200, loss is 4.2759719085693355 and perplexity is 71.95003419323535
At time: 225.18611431121826 and batch: 250, loss is 4.306353325843811 and perplexity is 74.16952309550493
At time: 225.70935201644897 and batch: 300, loss is 4.180489530563355 and perplexity is 65.39785963037828
At time: 226.2123885154724 and batch: 350, loss is 4.192294631004334 and perplexity is 66.17446285250313
At time: 226.71511721611023 and batch: 400, loss is 4.191244807243347 and perplexity is 66.10502778270032
At time: 227.21679091453552 and batch: 450, loss is 4.2237601613998415 and perplexity is 68.28978273071478
At time: 227.71794390678406 and batch: 500, loss is 4.269928016662598 and perplexity is 71.51648743848472
At time: 228.21885895729065 and batch: 550, loss is 4.202345519065857 and perplexity is 66.84292868139643
At time: 228.73754215240479 and batch: 600, loss is 4.138400630950928 and perplexity is 62.702456841799
At time: 229.24323344230652 and batch: 650, loss is 4.111829328536987 and perplexity is 61.05831117887486
At time: 229.7519507408142 and batch: 700, loss is 4.192408270835877 and perplexity is 66.18198333461922
At time: 230.25460028648376 and batch: 750, loss is 4.150115513801575 and perplexity is 63.44132822387701
At time: 230.76144456863403 and batch: 800, loss is 4.235871787071228 and perplexity is 69.12191206365976
At time: 231.2699966430664 and batch: 850, loss is 4.149985556602478 and perplexity is 63.433084102257105
At time: 231.77532649040222 and batch: 900, loss is 4.137741351127625 and perplexity is 62.66113200094538
At time: 232.28292417526245 and batch: 950, loss is 4.097297415733338 and perplexity is 60.17743304864962
At time: 232.8045356273651 and batch: 1000, loss is 4.037045159339905 and perplexity is 56.65867785078697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.77167231862138 and perplexity of 118.11660548617353
Finished 21 epochs...
Completing Train Step...
At time: 234.33494067192078 and batch: 50, loss is 4.313646907806397 and perplexity is 74.71246216978527
At time: 234.85580849647522 and batch: 100, loss is 4.220047597885132 and perplexity is 68.03672261645814
At time: 235.35873889923096 and batch: 150, loss is 4.244456400871277 and perplexity is 69.71785127801228
At time: 235.85746335983276 and batch: 200, loss is 4.27100154876709 and perplexity is 71.59330390884656
At time: 236.35670256614685 and batch: 250, loss is 4.301679172515869 and perplexity is 73.82365232850128
At time: 236.85706400871277 and batch: 300, loss is 4.176309995651245 and perplexity is 65.12509739957144
At time: 237.35570240020752 and batch: 350, loss is 4.187507305145264 and perplexity is 65.8584212359804
At time: 237.8560061454773 and batch: 400, loss is 4.18660717010498 and perplexity is 65.79916643598993
At time: 238.36118817329407 and batch: 450, loss is 4.219225730895996 and perplexity is 67.98082845202121
At time: 238.8816740512848 and batch: 500, loss is 4.26594648361206 and perplexity is 71.23230828984124
At time: 239.3809895515442 and batch: 550, loss is 4.197883501052856 and perplexity is 66.54533874913874
At time: 239.8801896572113 and batch: 600, loss is 4.134198646545411 and perplexity is 62.43953487981468
At time: 240.37884736061096 and batch: 650, loss is 4.1077432489395145 and perplexity is 60.80933108197296
At time: 240.87783432006836 and batch: 700, loss is 4.189276080131531 and perplexity is 65.97501304603921
At time: 241.37776064872742 and batch: 750, loss is 4.147072415351868 and perplexity is 63.24856346613661
At time: 241.87679243087769 and batch: 800, loss is 4.2324469995498655 and perplexity is 68.88558911071084
At time: 242.3757839202881 and batch: 850, loss is 4.146588006019592 and perplexity is 63.217932691256145
At time: 242.87615609169006 and batch: 900, loss is 4.134721517562866 and perplexity is 62.47219123973481
At time: 243.37398195266724 and batch: 950, loss is 4.093825192451477 and perplexity is 59.968845904710534
At time: 243.87376928329468 and batch: 1000, loss is 4.033712201118469 and perplexity is 56.47015119488478
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.77156215760766 and perplexity of 118.1035943578487
Finished 22 epochs...
Completing Train Step...
At time: 245.35159254074097 and batch: 50, loss is 4.308740739822388 and perplexity is 74.34680799374304
At time: 245.8538055419922 and batch: 100, loss is 4.214526171684265 and perplexity is 67.66209805625145
At time: 246.35611200332642 and batch: 150, loss is 4.239896054267883 and perplexity is 69.40063756373421
At time: 246.8569803237915 and batch: 200, loss is 4.265898933410645 and perplexity is 71.22892125976239
At time: 247.35990285873413 and batch: 250, loss is 4.296986174583435 and perplexity is 73.47800976505242
At time: 247.8626673221588 and batch: 300, loss is 4.172205538749695 and perplexity is 64.85834206147241
At time: 248.36651802062988 and batch: 350, loss is 4.182735080718994 and perplexity is 65.54487881194417
At time: 248.88317799568176 and batch: 400, loss is 4.181807885169983 and perplexity is 65.484134057534
At time: 249.39296555519104 and batch: 450, loss is 4.214798007011414 and perplexity is 67.68049350496527
At time: 249.89357829093933 and batch: 500, loss is 4.261867780685424 and perplexity is 70.9423645646628
At time: 250.40151166915894 and batch: 550, loss is 4.1936445140838625 and perplexity is 66.26385095837699
At time: 250.90384554862976 and batch: 600, loss is 4.130092215538025 and perplexity is 62.18365696975701
At time: 251.42369484901428 and batch: 650, loss is 4.1040354919433595 and perplexity is 60.58428233008512
At time: 251.93915915489197 and batch: 700, loss is 4.185566782951355 and perplexity is 65.73074542685175
At time: 252.44166159629822 and batch: 750, loss is 4.143605527877807 and perplexity is 63.029667476833396
At time: 252.94494557380676 and batch: 800, loss is 4.228940749168396 and perplexity is 68.64448192570346
At time: 253.44855165481567 and batch: 850, loss is 4.14369794845581 and perplexity is 63.035492984327185
At time: 253.95555210113525 and batch: 900, loss is 4.131540470123291 and perplexity is 62.27377998087178
At time: 254.47208070755005 and batch: 950, loss is 4.090361700057984 and perplexity is 59.7615035347031
At time: 254.9754023551941 and batch: 1000, loss is 4.030429320335388 and perplexity is 56.28507038604744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.771488840987042 and perplexity of 118.09493571884259
Finished 23 epochs...
Completing Train Step...
At time: 256.44696855545044 and batch: 50, loss is 4.304439392089844 and perplexity is 74.02770330187394
At time: 256.96100664138794 and batch: 100, loss is 4.209303040504455 and perplexity is 67.30961138561405
At time: 257.46088194847107 and batch: 150, loss is 4.235101485252381 and perplexity is 69.06868783106829
At time: 257.9648332595825 and batch: 200, loss is 4.261166934967041 and perplexity is 70.89266233103888
At time: 258.4648323059082 and batch: 250, loss is 4.29293830871582 and perplexity is 73.18118180249242
At time: 258.9615364074707 and batch: 300, loss is 4.168385596275329 and perplexity is 64.61105952903397
At time: 259.46084237098694 and batch: 350, loss is 4.178230791091919 and perplexity is 65.25030960474936
At time: 259.96106362342834 and batch: 400, loss is 4.1777611303329465 and perplexity is 65.21967129018742
At time: 260.4610276222229 and batch: 450, loss is 4.210609068870545 and perplexity is 67.39757707773504
At time: 260.95974826812744 and batch: 500, loss is 4.2578496074676515 and perplexity is 70.65787779668963
At time: 261.47226071357727 and batch: 550, loss is 4.189758224487305 and perplexity is 66.00683019581466
At time: 261.9802870750427 and batch: 600, loss is 4.126125240325928 and perplexity is 61.937464586360115
At time: 262.4809160232544 and batch: 650, loss is 4.100277857780457 and perplexity is 60.35705594524597
At time: 262.98216795921326 and batch: 700, loss is 4.1813738155364994 and perplexity is 65.4557155517161
At time: 263.48344707489014 and batch: 750, loss is 4.140503277778626 and perplexity is 62.83443666862344
At time: 263.98379158973694 and batch: 800, loss is 4.2256889915466305 and perplexity is 68.4216292362441
At time: 264.50638580322266 and batch: 850, loss is 4.140474619865418 and perplexity is 62.83263599059289
At time: 265.0058388710022 and batch: 900, loss is 4.1285718727111815 and perplexity is 62.08918832349706
At time: 265.50584721565247 and batch: 950, loss is 4.087182512283325 and perplexity is 59.57181218525117
At time: 266.0051143169403 and batch: 1000, loss is 4.027442407608032 and perplexity is 56.11720262093803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.77171288467035 and perplexity of 118.12139710736383
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 267.5459768772125 and batch: 50, loss is 4.300153484344483 and perplexity is 73.71110633225408
At time: 268.0660881996155 and batch: 100, loss is 4.2035321569442745 and perplexity is 66.92229411217438
At time: 268.5658745765686 and batch: 150, loss is 4.2299261474609375 and perplexity is 68.71215741916714
At time: 269.06541633605957 and batch: 200, loss is 4.254285755157471 and perplexity is 70.40651173747774
At time: 269.5659637451172 and batch: 250, loss is 4.28512062549591 and perplexity is 72.6113049661801
At time: 270.06728410720825 and batch: 300, loss is 4.157652840614319 and perplexity is 63.92131287635333
At time: 270.56701374053955 and batch: 350, loss is 4.166241574287414 and perplexity is 64.47268039370442
At time: 271.068106174469 and batch: 400, loss is 4.164277920722961 and perplexity is 64.34620260496067
At time: 271.5690264701843 and batch: 450, loss is 4.197395539283752 and perplexity is 66.51287508907373
At time: 272.0724539756775 and batch: 500, loss is 4.241532011032104 and perplexity is 69.51426692721806
At time: 272.5738868713379 and batch: 550, loss is 4.1721786737442015 and perplexity is 64.8565996651615
At time: 273.0771870613098 and batch: 600, loss is 4.106917181015015 and perplexity is 60.75911918612948
At time: 273.57988238334656 and batch: 650, loss is 4.078574318885803 and perplexity is 59.06120735052638
At time: 274.0812087059021 and batch: 700, loss is 4.158464078903198 and perplexity is 63.97318933207261
At time: 274.58437609672546 and batch: 750, loss is 4.117256669998169 and perplexity is 61.390596379659314
At time: 275.08704137802124 and batch: 800, loss is 4.1982279825210576 and perplexity is 66.56826633397031
At time: 275.5879318714142 and batch: 850, loss is 4.115192351341247 and perplexity is 61.26399734152894
At time: 276.0945394039154 and batch: 900, loss is 4.096158294677735 and perplexity is 60.10892269579272
At time: 276.59684705734253 and batch: 950, loss is 4.054950442314148 and perplexity is 57.682304323812154
At time: 277.1241445541382 and batch: 1000, loss is 3.991302766799927 and perplexity is 54.12535616950428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.764159039753239 and perplexity of 117.23248795994624
Finished 25 epochs...
Completing Train Step...
At time: 278.6339337825775 and batch: 50, loss is 4.291529159545899 and perplexity is 73.0781312247719
At time: 279.1393163204193 and batch: 100, loss is 4.197409934997559 and perplexity is 66.51383259627997
At time: 279.64435148239136 and batch: 150, loss is 4.223758563995362 and perplexity is 68.28967364439704
At time: 280.1474814414978 and batch: 200, loss is 4.248999843597412 and perplexity is 70.03533102165814
At time: 280.65117168426514 and batch: 250, loss is 4.2799513626098635 and perplexity is 72.23692650630555
At time: 281.1682252883911 and batch: 300, loss is 4.1521320819854735 and perplexity is 63.569391068248486
At time: 281.6772606372833 and batch: 350, loss is 4.161179318428039 and perplexity is 64.14712789985039
At time: 282.188019990921 and batch: 400, loss is 4.160604710578919 and perplexity is 64.11027904449521
At time: 282.6984589099884 and batch: 450, loss is 4.19395149230957 and perplexity is 66.28419564030015
At time: 283.2064700126648 and batch: 500, loss is 4.238325128555298 and perplexity is 69.29169990660192
At time: 283.71211981773376 and batch: 550, loss is 4.169218454360962 and perplexity is 64.66489378742085
At time: 284.21658086776733 and batch: 600, loss is 4.104151592254639 and perplexity is 60.59131659245441
At time: 284.7270531654358 and batch: 650, loss is 4.076374773979187 and perplexity is 58.931442337024095
At time: 285.25388979911804 and batch: 700, loss is 4.1565542936325075 and perplexity is 63.85113086719475
At time: 285.7647955417633 and batch: 750, loss is 4.116332931518555 and perplexity is 61.333913707510206
At time: 286.2681601047516 and batch: 800, loss is 4.197321720123291 and perplexity is 66.50796534569362
At time: 286.77424478530884 and batch: 850, loss is 4.1150078773498535 and perplexity is 61.25269676977345
At time: 287.27851963043213 and batch: 900, loss is 4.096292681694031 and perplexity is 60.11700109737059
At time: 287.78207063674927 and batch: 950, loss is 4.055967183113098 and perplexity is 57.74098210098575
At time: 288.28625440597534 and batch: 1000, loss is 3.99257896900177 and perplexity is 54.19447516374853
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.763217460818407 and perplexity of 117.1221562699541
Finished 26 epochs...
Completing Train Step...
At time: 289.76623463630676 and batch: 50, loss is 4.287927055358887 and perplexity is 72.81536971359216
At time: 290.28425669670105 and batch: 100, loss is 4.194398889541626 and perplexity is 66.31385764081236
At time: 290.8018012046814 and batch: 150, loss is 4.2210969066619874 and perplexity is 68.10815161563207
At time: 291.3057415485382 and batch: 200, loss is 4.246172094345093 and perplexity is 69.83756841004285
At time: 291.80842113494873 and batch: 250, loss is 4.2772661256790165 and perplexity is 72.04321344261352
At time: 292.3224594593048 and batch: 300, loss is 4.149329338073731 and perplexity is 63.391471792011394
At time: 292.83478713035583 and batch: 350, loss is 4.158542284965515 and perplexity is 63.97819261894524
At time: 293.34490394592285 and batch: 400, loss is 4.158438220024109 and perplexity is 63.97153507849335
At time: 293.8509805202484 and batch: 450, loss is 4.192159743309021 and perplexity is 66.16553733370526
At time: 294.35312008857727 and batch: 500, loss is 4.236627850532532 and perplexity is 69.1741923768734
At time: 294.85527539253235 and batch: 550, loss is 4.1675735330581665 and perplexity is 64.558612562177
At time: 295.3644983768463 and batch: 600, loss is 4.1026557397842405 and perplexity is 60.50074867685274
At time: 295.8711693286896 and batch: 650, loss is 4.0752029132843015 and perplexity is 58.86242334427724
At time: 296.3727717399597 and batch: 700, loss is 4.155621485710144 and perplexity is 63.79159779724641
At time: 296.8760323524475 and batch: 750, loss is 4.115863585472107 and perplexity is 61.305133632034085
At time: 297.378395318985 and batch: 800, loss is 4.196967873573303 and perplexity is 66.48443589476234
At time: 297.88300466537476 and batch: 850, loss is 4.115045351982117 and perplexity is 61.25499223507062
At time: 298.4054479598999 and batch: 900, loss is 4.096237301826477 and perplexity is 60.11367191799768
At time: 298.9253396987915 and batch: 950, loss is 4.056536469459534 and perplexity is 57.773862612050635
At time: 299.43877124786377 and batch: 1000, loss is 3.9931137275695803 and perplexity is 54.22346387396009
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.7627809105849845 and perplexity of 117.07103772401398
Finished 27 epochs...
Completing Train Step...
At time: 300.92893719673157 and batch: 50, loss is 4.285379467010498 and perplexity is 72.63010221898345
At time: 301.44447016716003 and batch: 100, loss is 4.1922421503067016 and perplexity is 66.17099006165525
At time: 301.9470467567444 and batch: 150, loss is 4.219256091117859 and perplexity is 67.98289239638618
At time: 302.4654314517975 and batch: 200, loss is 4.244151020050049 and perplexity is 69.69656403385824
At time: 302.97114777565 and batch: 250, loss is 4.27531216621399 and perplexity is 71.9025813632516
At time: 303.48608660697937 and batch: 300, loss is 4.147338318824768 and perplexity is 63.26538371500037
At time: 303.999968290329 and batch: 350, loss is 4.1566681289672855 and perplexity is 63.85839979577558
At time: 304.5159134864807 and batch: 400, loss is 4.156848297119141 and perplexity is 63.86990608214928
At time: 305.0171449184418 and batch: 450, loss is 4.190883340835572 and perplexity is 66.0811373538257
At time: 305.5169584751129 and batch: 500, loss is 4.235481076240539 and perplexity is 69.09491065919264
At time: 306.0169212818146 and batch: 550, loss is 4.1662989568710325 and perplexity is 64.47638010882679
At time: 306.53139638900757 and batch: 600, loss is 4.101619844436645 and perplexity is 60.43810868261314
At time: 307.0389213562012 and batch: 650, loss is 4.074323840141297 and perplexity is 58.81070170566555
At time: 307.5395522117615 and batch: 700, loss is 4.154960618019104 and perplexity is 63.74945391860014
At time: 308.043093919754 and batch: 750, loss is 4.115452275276184 and perplexity is 61.27992339047893
At time: 308.5472927093506 and batch: 800, loss is 4.196624417304992 and perplexity is 66.4616053193857
At time: 309.0501582622528 and batch: 850, loss is 4.114955468177795 and perplexity is 61.249486650769995
At time: 309.55329990386963 and batch: 900, loss is 4.096056246757508 and perplexity is 60.10278901821438
At time: 310.0571041107178 and batch: 950, loss is 4.056829533576965 and perplexity is 57.79079653934967
At time: 310.5601592063904 and batch: 1000, loss is 3.993322591781616 and perplexity is 54.23479039782755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.7625341648008765 and perplexity of 117.04215450256754
Finished 28 epochs...
Completing Train Step...
At time: 312.10652470588684 and batch: 50, loss is 4.28333957195282 and perplexity is 72.48209544289732
At time: 312.61658549308777 and batch: 100, loss is 4.190471224784851 and perplexity is 66.05390986729975
At time: 313.120197057724 and batch: 150, loss is 4.2177813577651975 and perplexity is 67.88270964713647
At time: 313.63916277885437 and batch: 200, loss is 4.242501697540283 and perplexity is 69.58170666640149
At time: 314.16225361824036 and batch: 250, loss is 4.27367069721222 and perplexity is 71.78465231971046
At time: 314.6716923713684 and batch: 300, loss is 4.14570867061615 and perplexity is 63.16236735877073
At time: 315.1760518550873 and batch: 350, loss is 4.155142583847046 and perplexity is 63.761055196249295
At time: 315.6841812133789 and batch: 400, loss is 4.155555038452149 and perplexity is 63.78735916131406
At time: 316.2225410938263 and batch: 450, loss is 4.189822125434875 and perplexity is 66.01104822957706
At time: 316.736615896225 and batch: 500, loss is 4.234509038925171 and perplexity is 69.02778045954507
At time: 317.2463471889496 and batch: 550, loss is 4.165233874320984 and perplexity is 64.40774399953216
At time: 317.7479615211487 and batch: 600, loss is 4.100805349349976 and perplexity is 60.38890218199036
At time: 318.24979043006897 and batch: 650, loss is 4.073560776710511 and perplexity is 58.76584252723106
At time: 318.76155853271484 and batch: 700, loss is 4.1543559217453 and perplexity is 63.71091651424522
At time: 319.2769658565521 and batch: 750, loss is 4.114996209144592 and perplexity is 61.25198206490439
At time: 319.787948846817 and batch: 800, loss is 4.196258120536804 and perplexity is 66.43726510629145
At time: 320.3111901283264 and batch: 850, loss is 4.114769620895386 and perplexity is 61.23810465781595
At time: 320.82157611846924 and batch: 900, loss is 4.095807642936706 and perplexity is 60.0878490923619
At time: 321.32417941093445 and batch: 950, loss is 4.056910786628723 and perplexity is 57.795492408706295
At time: 321.8387784957886 and batch: 1000, loss is 3.9933475065231323 and perplexity is 54.2361416604446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.762355897484756 and perplexity of 117.02129157145603
Finished 29 epochs...
Completing Train Step...
At time: 323.32962369918823 and batch: 50, loss is 4.281569080352783 and perplexity is 72.35388003740425
At time: 323.8479824066162 and batch: 100, loss is 4.188923888206482 and perplexity is 65.95178127045136
At time: 324.35327100753784 and batch: 150, loss is 4.216508269309998 and perplexity is 67.79634394043387
At time: 324.85825991630554 and batch: 200, loss is 4.241106185913086 and perplexity is 69.48467230774366
At time: 325.363162279129 and batch: 250, loss is 4.272222032546997 and perplexity is 71.68073571871716
At time: 325.86725902557373 and batch: 300, loss is 4.144261102676392 and perplexity is 63.07100168574685
At time: 326.3709349632263 and batch: 350, loss is 4.153812131881714 and perplexity is 63.676280581832955
At time: 326.87590527534485 and batch: 400, loss is 4.15441605091095 and perplexity is 63.71474751367421
At time: 327.3818590641022 and batch: 450, loss is 4.188908934593201 and perplexity is 65.95079506039278
At time: 327.8862564563751 and batch: 500, loss is 4.233598713874817 and perplexity is 68.96497133451982
At time: 328.3910620212555 and batch: 550, loss is 4.164262270927429 and perplexity is 64.34519560792629
At time: 328.8947744369507 and batch: 600, loss is 4.100074548721313 and perplexity is 60.34478605632111
At time: 329.41286396980286 and batch: 650, loss is 4.072775721549988 and perplexity is 58.719726203590014
At time: 329.9180464744568 and batch: 700, loss is 4.1537518882751465 and perplexity is 63.67244460858551
At time: 330.4414794445038 and batch: 750, loss is 4.114511466026306 and perplexity is 61.2222977833239
At time: 330.95189929008484 and batch: 800, loss is 4.195878267288208 and perplexity is 66.41203348777252
At time: 331.4571022987366 and batch: 850, loss is 4.114534239768982 and perplexity is 61.22369206005608
At time: 331.9615137577057 and batch: 900, loss is 4.095598468780517 and perplexity is 60.075281581675846
At time: 332.4652919769287 and batch: 950, loss is 4.056850595474243 and perplexity is 57.79201373598818
At time: 332.9687922000885 and batch: 1000, loss is 3.993206934928894 and perplexity is 54.22851813538416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.762198843607089 and perplexity of 117.00291436698848
Finished 30 epochs...
Completing Train Step...
At time: 334.4601230621338 and batch: 50, loss is 4.279952931404114 and perplexity is 72.23703983126939
At time: 334.9740924835205 and batch: 100, loss is 4.187493004798889 and perplexity is 65.85747944447904
At time: 335.47589659690857 and batch: 150, loss is 4.215347819328308 and perplexity is 67.7177153054723
At time: 335.978143453598 and batch: 200, loss is 4.239849586486816 and perplexity is 69.39741274502762
At time: 336.4781560897827 and batch: 250, loss is 4.270879397392273 and perplexity is 71.58455922244504
At time: 336.9799165725708 and batch: 300, loss is 4.142944707870483 and perplexity is 62.98802997047355
At time: 337.48054933547974 and batch: 350, loss is 4.152569675445557 and perplexity is 63.59721470531826
At time: 337.98100543022156 and batch: 400, loss is 4.1533894538879395 and perplexity is 63.64937170660093
At time: 338.4809970855713 and batch: 450, loss is 4.18806809425354 and perplexity is 65.89536427895774
At time: 338.98236536979675 and batch: 500, loss is 4.232762880325318 and perplexity is 68.90735218110342
At time: 339.48277473449707 and batch: 550, loss is 4.163308281898498 and perplexity is 64.28384026806573
At time: 339.9866261482239 and batch: 600, loss is 4.099346194267273 and perplexity is 60.30084966519434
At time: 340.5029454231262 and batch: 650, loss is 4.072058615684509 and perplexity is 58.67763303794193
At time: 341.0108234882355 and batch: 700, loss is 4.153165640830994 and perplexity is 63.63512774019891
At time: 341.53014492988586 and batch: 750, loss is 4.114038496017456 and perplexity is 61.193348319253346
At time: 342.06308794021606 and batch: 800, loss is 4.195449185371399 and perplexity is 66.38354339787271
At time: 342.5757305622101 and batch: 850, loss is 4.11420967578888 and perplexity is 61.20382427924191
At time: 343.085116147995 and batch: 900, loss is 4.095271344184876 and perplexity is 60.055632693472894
At time: 343.58702659606934 and batch: 950, loss is 4.056742830276489 and perplexity is 57.785786103766355
At time: 344.111127614975 and batch: 1000, loss is 3.9929728603363035 and perplexity is 54.21582610259397
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.762002340177211 and perplexity of 116.97992515181342
Finished 31 epochs...
Completing Train Step...
At time: 345.61638474464417 and batch: 50, loss is 4.278293857574463 and perplexity is 72.11729261116068
At time: 346.1188600063324 and batch: 100, loss is 4.186103558540344 and perplexity is 65.76603755757267
At time: 346.61981987953186 and batch: 150, loss is 4.2140647983551025 and perplexity is 67.63088776916376
At time: 347.1210746765137 and batch: 200, loss is 4.238426885604858 and perplexity is 69.298751184296
At time: 347.62278413772583 and batch: 250, loss is 4.2694675159454345 and perplexity is 71.48356162649218
At time: 348.1262068748474 and batch: 300, loss is 4.141502113342285 and perplexity is 62.89722929312326
At time: 348.63415265083313 and batch: 350, loss is 4.151188440322876 and perplexity is 63.50943263645689
At time: 349.13680815696716 and batch: 400, loss is 4.152223591804504 and perplexity is 63.57520855789569
At time: 349.63830065727234 and batch: 450, loss is 4.186979928016663 and perplexity is 65.82369816777556
At time: 350.14750385284424 and batch: 500, loss is 4.231723480224609 and perplexity is 68.83576708153143
At time: 350.65335607528687 and batch: 550, loss is 4.162355813980103 and perplexity is 64.22264112227728
At time: 351.1554207801819 and batch: 600, loss is 4.098521928787232 and perplexity is 60.2511662353792
At time: 351.66166472435 and batch: 650, loss is 4.071231889724731 and perplexity is 58.62914276229782
At time: 352.16958117485046 and batch: 700, loss is 4.152490544319153 and perplexity is 63.592182385191656
At time: 352.67141580581665 and batch: 750, loss is 4.113391413688659 and perplexity is 61.15376399345612
At time: 353.1718418598175 and batch: 800, loss is 4.19492702960968 and perplexity is 66.34888989625429
At time: 353.67311429977417 and batch: 850, loss is 4.113662323951721 and perplexity is 61.17033342005932
At time: 354.1731369495392 and batch: 900, loss is 4.094685597419739 and perplexity is 60.020465601405135
At time: 354.6845486164093 and batch: 950, loss is 4.056523489952087 and perplexity is 57.77311274063713
At time: 355.20717692375183 and batch: 1000, loss is 3.9925559186935424 and perplexity is 54.19322597878889
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.761866127572408 and perplexity of 116.963992096665
Finished 32 epochs...
Completing Train Step...
At time: 356.69584035873413 and batch: 50, loss is 4.276769800186157 and perplexity is 72.00746543125297
At time: 357.21493887901306 and batch: 100, loss is 4.184849224090576 and perplexity is 65.6835966660737
At time: 357.7249720096588 and batch: 150, loss is 4.213118371963501 and perplexity is 67.56691039180626
At time: 358.2377653121948 and batch: 200, loss is 4.237366228103638 and perplexity is 69.22528791060046
At time: 358.7475790977478 and batch: 250, loss is 4.268357982635498 and perplexity is 71.40429221791149
At time: 359.25349259376526 and batch: 300, loss is 4.14039608001709 and perplexity is 62.82770131867886
At time: 359.7589683532715 and batch: 350, loss is 4.150086193084717 and perplexity is 63.43946810592519
At time: 360.2640233039856 and batch: 400, loss is 4.1513052606582646 and perplexity is 63.516852263051
At time: 360.7751274108887 and batch: 450, loss is 4.186232242584229 and perplexity is 65.7745011417883
At time: 361.28218388557434 and batch: 500, loss is 4.230991554260254 and perplexity is 68.78540283002826
At time: 361.78568959236145 and batch: 550, loss is 4.161519923210144 and perplexity is 64.1689804396999
At time: 362.29022097587585 and batch: 600, loss is 4.097849469184876 and perplexity is 60.210663379884416
At time: 362.7945930957794 and batch: 650, loss is 4.070574741363526 and perplexity is 58.59062737376098
At time: 363.2993702888489 and batch: 700, loss is 4.151912679672241 and perplexity is 63.555445326717276
At time: 363.804162979126 and batch: 750, loss is 4.11293396949768 and perplexity is 61.12579595675647
At time: 364.3086471557617 and batch: 800, loss is 4.194455032348633 and perplexity is 66.31758079143685
At time: 364.81282472610474 and batch: 850, loss is 4.113265538215638 and perplexity is 61.146066718945576
At time: 365.31718826293945 and batch: 900, loss is 4.094269428253174 and perplexity is 59.99549213121355
At time: 365.8208713531494 and batch: 950, loss is 4.056308851242066 and perplexity is 57.76071372494653
At time: 366.33224081993103 and batch: 1000, loss is 3.9921768283843995 and perplexity is 54.1726857455468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.761778296493903 and perplexity of 116.95371947422721
Finished 33 epochs...
Completing Train Step...
At time: 367.80904603004456 and batch: 50, loss is 4.275421919822693 and perplexity is 71.91047336411118
At time: 368.3232274055481 and batch: 100, loss is 4.183609676361084 and perplexity is 65.60222915283782
At time: 368.8259723186493 and batch: 150, loss is 4.212229795455933 and perplexity is 67.50689868898644
At time: 369.33518171310425 and batch: 200, loss is 4.236353769302368 and perplexity is 69.15523562709447
At time: 369.85145592689514 and batch: 250, loss is 4.2672996330261235 and perplexity is 71.32876148914382
At time: 370.3580639362335 and batch: 300, loss is 4.139355306625366 and perplexity is 62.76234593485493
At time: 370.8621702194214 and batch: 350, loss is 4.148991756439209 and perplexity is 63.370075607031836
At time: 371.36292719841003 and batch: 400, loss is 4.150387840270996 and perplexity is 63.458607329479726
At time: 371.8632550239563 and batch: 450, loss is 4.185507593154907 and perplexity is 65.72685495254889
At time: 372.3636758327484 and batch: 500, loss is 4.230187296867371 and perplexity is 68.73010390154651
At time: 372.8651382923126 and batch: 550, loss is 4.160611958503723 and perplexity is 64.11074371266086
At time: 373.3672728538513 and batch: 600, loss is 4.097267217636109 and perplexity is 60.1756158321138
At time: 373.8689465522766 and batch: 650, loss is 4.069931502342224 and perplexity is 58.55295171447778
At time: 374.37093901634216 and batch: 700, loss is 4.15131784915924 and perplexity is 63.51765185004049
At time: 374.88258147239685 and batch: 750, loss is 4.112458848953247 and perplexity is 61.09676073346435
At time: 375.38755989074707 and batch: 800, loss is 4.193969516754151 and perplexity is 66.28539038687833
At time: 375.8904049396515 and batch: 850, loss is 4.112829647064209 and perplexity is 61.11941949758404
At time: 376.39323258399963 and batch: 900, loss is 4.093828158378601 and perplexity is 59.969023768200984
At time: 376.8962757587433 and batch: 950, loss is 4.055991420745849 and perplexity is 57.7423816226651
At time: 377.40002846717834 and batch: 1000, loss is 3.991781702041626 and perplexity is 54.151284918643626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.761716889172066 and perplexity of 116.94653788003889
Finished 34 epochs...
Completing Train Step...
At time: 378.99296617507935 and batch: 50, loss is 4.27416410446167 and perplexity is 71.82008012701404
At time: 379.4930202960968 and batch: 100, loss is 4.182451210021973 and perplexity is 65.52627518214723
At time: 379.9941051006317 and batch: 150, loss is 4.211386876106262 and perplexity is 67.45001979337385
At time: 380.49344754219055 and batch: 200, loss is 4.235339889526367 and perplexity is 69.08515606441644
At time: 381.01365208625793 and batch: 250, loss is 4.266273083686829 and perplexity is 71.25557656655828
At time: 381.5136282444 and batch: 300, loss is 4.138368301391601 and perplexity is 62.70042973176857
At time: 382.0140721797943 and batch: 350, loss is 4.148000435829163 and perplexity is 63.30728667214701
At time: 382.51399087905884 and batch: 400, loss is 4.149535059928894 and perplexity is 63.404514144692484
At time: 383.01351499557495 and batch: 450, loss is 4.18478853225708 and perplexity is 65.6796103291317
At time: 383.51464343070984 and batch: 500, loss is 4.229485535621643 and perplexity is 68.68188869797297
At time: 384.01711654663086 and batch: 550, loss is 4.1598397159576415 and perplexity is 64.0612537802802
At time: 384.5168285369873 and batch: 600, loss is 4.096678109169006 and perplexity is 60.140176307201024
At time: 385.0197913646698 and batch: 650, loss is 4.069310383796692 and perplexity is 58.51659468245453
At time: 385.5273914337158 and batch: 700, loss is 4.150738363265991 and perplexity is 63.48085492949897
At time: 386.02805852890015 and batch: 750, loss is 4.11197434425354 and perplexity is 61.06716623566768
At time: 386.529821395874 and batch: 800, loss is 4.193476767539978 and perplexity is 66.25273635863813
At time: 387.0317852497101 and batch: 850, loss is 4.112383069992066 and perplexity is 61.092131059822556
At time: 387.5312428474426 and batch: 900, loss is 4.093374643325806 and perplexity is 59.94183307938004
At time: 388.05977988243103 and batch: 950, loss is 4.055682721138001 and perplexity is 57.72455932311192
At time: 388.57007908821106 and batch: 1000, loss is 3.9913925886154176 and perplexity is 54.13021802560661
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.76166255299638 and perplexity of 116.9401836250453
Finished 35 epochs...
Completing Train Step...
At time: 390.0590491294861 and batch: 50, loss is 4.2729913854599 and perplexity is 71.73590472103643
At time: 390.57457542419434 and batch: 100, loss is 4.181359472274781 and perplexity is 65.45477670998999
At time: 391.0769398212433 and batch: 150, loss is 4.210555262565613 and perplexity is 67.39395076071138
At time: 391.5906229019165 and batch: 200, loss is 4.23439661026001 and perplexity is 69.02002019457528
At time: 392.09989190101624 and batch: 250, loss is 4.265268864631653 and perplexity is 71.18405627581242
At time: 392.60331535339355 and batch: 300, loss is 4.1374327659606935 and perplexity is 62.641798688206954
At time: 393.1093096733093 and batch: 350, loss is 4.147054743766785 and perplexity is 63.24744577364165
At time: 393.625226020813 and batch: 400, loss is 4.148685274124145 and perplexity is 63.35065677547027
At time: 394.1607961654663 and batch: 450, loss is 4.184072184562683 and perplexity is 65.63257773955831
At time: 394.6627345085144 and batch: 500, loss is 4.228689284324646 and perplexity is 68.62722242195183
At time: 395.1778290271759 and batch: 550, loss is 4.159061975479126 and perplexity is 64.01145011978264
At time: 395.69848251342773 and batch: 600, loss is 4.096077218055725 and perplexity is 60.10404946494312
At time: 396.22851943969727 and batch: 650, loss is 4.0686904191970825 and perplexity is 58.480327708543314
At time: 396.74185371398926 and batch: 700, loss is 4.150157928466797 and perplexity is 63.444019123641255
At time: 397.25735092163086 and batch: 750, loss is 4.111472744941711 and perplexity is 61.03654266813078
At time: 397.7698564529419 and batch: 800, loss is 4.192982411384582 and perplexity is 66.2199920049607
At time: 398.27362966537476 and batch: 850, loss is 4.1119061374664305 and perplexity is 61.0630011825049
At time: 398.7845051288605 and batch: 900, loss is 4.092877759933471 and perplexity is 59.912056376404045
At time: 399.29885482788086 and batch: 950, loss is 4.055313081741333 and perplexity is 57.70322599489298
At time: 399.801549911499 and batch: 1000, loss is 3.9909296798706055 and perplexity is 54.10516647306269
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.76161342713891 and perplexity of 116.9344389793585
Finished 36 epochs...
Completing Train Step...
At time: 401.30415558815 and batch: 50, loss is 4.271902294158935 and perplexity is 71.65782029948818
At time: 401.830050945282 and batch: 100, loss is 4.180294818878174 and perplexity is 65.38512714254476
At time: 402.34501338005066 and batch: 150, loss is 4.20971483707428 and perplexity is 67.33733496054364
At time: 402.8674235343933 and batch: 200, loss is 4.233440923690796 and perplexity is 68.95409019749282
At time: 403.393185377121 and batch: 250, loss is 4.264286189079285 and perplexity is 71.11413980222267
At time: 403.91553020477295 and batch: 300, loss is 4.136527137756348 and perplexity is 62.585094189016424
At time: 404.4285943508148 and batch: 350, loss is 4.146171426773071 and perplexity is 63.19160289709266
At time: 404.9320635795593 and batch: 400, loss is 4.147852430343628 and perplexity is 63.29791753980379
At time: 405.43917894363403 and batch: 450, loss is 4.183370146751404 and perplexity is 65.58651735828983
At time: 405.94598627090454 and batch: 500, loss is 4.227926173210144 and perplexity is 68.57487220282175
At time: 406.4546012878418 and batch: 550, loss is 4.158347764015198 and perplexity is 63.96574873045518
At time: 406.99224495887756 and batch: 600, loss is 4.095486578941345 and perplexity is 60.06856014411872
At time: 407.49856996536255 and batch: 650, loss is 4.068076739311218 and perplexity is 58.44445051739106
At time: 408.00336503982544 and batch: 700, loss is 4.149612307548523 and perplexity is 63.4094121816623
At time: 408.50882744789124 and batch: 750, loss is 4.110966153144837 and perplexity is 61.0056298870473
At time: 409.0149486064911 and batch: 800, loss is 4.192475233078003 and perplexity is 66.18641517699307
At time: 409.5184061527252 and batch: 850, loss is 4.111437673568726 and perplexity is 61.0344020703142
At time: 410.02329325675964 and batch: 900, loss is 4.09241623878479 and perplexity is 59.88441207503091
At time: 410.5286273956299 and batch: 950, loss is 4.054971203804016 and perplexity is 57.68350190682073
At time: 411.0431671142578 and batch: 1000, loss is 3.990463418960571 and perplexity is 54.07994522920238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.761573605421113 and perplexity of 116.929782541843
Finished 37 epochs...
Completing Train Step...
At time: 412.55408549308777 and batch: 50, loss is 4.270821523666382 and perplexity is 71.5804164771658
At time: 413.05494570732117 and batch: 100, loss is 4.179343509674072 and perplexity is 65.32295524631996
At time: 413.55617904663086 and batch: 150, loss is 4.208913698196411 and perplexity is 67.28341000714717
At time: 414.0561397075653 and batch: 200, loss is 4.232585878372192 and perplexity is 68.89515652454202
At time: 414.5572655200958 and batch: 250, loss is 4.263388195037842 and perplexity is 71.05030839282001
At time: 415.0661721229553 and batch: 300, loss is 4.135626316070557 and perplexity is 62.52874156460749
At time: 415.56783628463745 and batch: 350, loss is 4.145308542251587 and perplexity is 63.13709935957623
At time: 416.06874322891235 and batch: 400, loss is 4.147037534713745 and perplexity is 63.24635735435808
At time: 416.57103872299194 and batch: 450, loss is 4.1827057933807374 and perplexity is 65.54295920501762
At time: 417.07369470596313 and batch: 500, loss is 4.227226386070251 and perplexity is 68.52690117584594
At time: 417.5736572742462 and batch: 550, loss is 4.157640252113342 and perplexity is 63.92050820790855
At time: 418.073926448822 and batch: 600, loss is 4.094885773658753 and perplexity is 60.03248147507917
At time: 418.57423520088196 and batch: 650, loss is 4.067415723800659 and perplexity is 58.405830594683145
At time: 419.07598519325256 and batch: 700, loss is 4.1490246152877805 and perplexity is 63.37215790896103
At time: 419.59146308898926 and batch: 750, loss is 4.110461287498474 and perplexity is 60.97483801381613
At time: 420.10688853263855 and batch: 800, loss is 4.191948342323303 and perplexity is 66.15155135226958
At time: 420.60809230804443 and batch: 850, loss is 4.11091299533844 and perplexity is 61.00238704777653
At time: 421.1088137626648 and batch: 900, loss is 4.091902275085449 and perplexity is 59.8536415692068
At time: 421.6076090335846 and batch: 950, loss is 4.054536685943604 and perplexity is 57.65844283969147
At time: 422.1081631183624 and batch: 1000, loss is 3.990007748603821 and perplexity is 54.05530821487161
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.7615177805830795 and perplexity of 116.92325513786868
Finished 38 epochs...
Completing Train Step...
At time: 423.5890872478485 and batch: 50, loss is 4.269737005233765 and perplexity is 71.50282827660352
At time: 424.1153664588928 and batch: 100, loss is 4.178405151367188 and perplexity is 65.26168765860488
At time: 424.6322479248047 and batch: 150, loss is 4.208123302459716 and perplexity is 67.23025049801946
At time: 425.1414940357208 and batch: 200, loss is 4.231703424453736 and perplexity is 68.8343865410029
At time: 425.6429810523987 and batch: 250, loss is 4.262459416389465 and perplexity is 70.98434901899473
At time: 426.1484844684601 and batch: 300, loss is 4.1347145128250125 and perplexity is 62.471753639944666
At time: 426.6503653526306 and batch: 350, loss is 4.1444379377365115 and perplexity is 63.082155836315124
At time: 427.1651520729065 and batch: 400, loss is 4.146233229637146 and perplexity is 63.195508439822824
At time: 427.6691429615021 and batch: 450, loss is 4.182013440132141 and perplexity is 65.49759602977237
At time: 428.1706738471985 and batch: 500, loss is 4.226508703231811 and perplexity is 68.4777382387089
At time: 428.68614196777344 and batch: 550, loss is 4.156920876502991 and perplexity is 63.87454188880922
At time: 429.1973240375519 and batch: 600, loss is 4.094249863624572 and perplexity is 59.994318353174535
At time: 429.7028560638428 and batch: 650, loss is 4.066704697608948 and perplexity is 58.36431727965779
At time: 430.20497274398804 and batch: 700, loss is 4.148485345840454 and perplexity is 63.33799245341072
At time: 430.70817399024963 and batch: 750, loss is 4.109927477836609 and perplexity is 60.94229774208216
At time: 431.2197542190552 and batch: 800, loss is 4.191382355690003 and perplexity is 66.11412105195615
At time: 431.724182844162 and batch: 850, loss is 4.110454111099243 and perplexity is 60.97440043560566
At time: 432.22523260116577 and batch: 900, loss is 4.091439881324768 and perplexity is 59.82597201639847
At time: 432.74000215530396 and batch: 950, loss is 4.054162797927856 and perplexity is 57.636889068506484
At time: 433.2415916919708 and batch: 1000, loss is 3.9895031118392943 and perplexity is 54.02803680068641
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.761481308355564 and perplexity of 116.91899076407152
Finished 39 epochs...
Completing Train Step...
At time: 434.7331221103668 and batch: 50, loss is 4.268651537895202 and perplexity is 71.42525640037996
At time: 435.2703688144684 and batch: 100, loss is 4.17746814250946 and perplexity is 65.20056551966336
At time: 435.784494638443 and batch: 150, loss is 4.207345833778382 and perplexity is 67.17800139747047
At time: 436.2900857925415 and batch: 200, loss is 4.23090106010437 and perplexity is 68.77917843470193
At time: 436.79522466659546 and batch: 250, loss is 4.261606993675232 and perplexity is 70.92386612969263
At time: 437.3043534755707 and batch: 300, loss is 4.133840141296386 and perplexity is 62.417153990886426
At time: 437.82818126678467 and batch: 350, loss is 4.143601140975952 and perplexity is 63.029390972474715
At time: 438.3400065898895 and batch: 400, loss is 4.145438175201416 and perplexity is 63.145284538532884
At time: 438.84436798095703 and batch: 450, loss is 4.181351385116577 and perplexity is 65.454247368996
At time: 439.3558909893036 and batch: 500, loss is 4.225796990394592 and perplexity is 68.42901909241806
At time: 439.862101316452 and batch: 550, loss is 4.156202712059021 and perplexity is 63.82868593197959
At time: 440.3681285381317 and batch: 600, loss is 4.093637371063233 and perplexity is 59.957583530511805
At time: 440.88487935066223 and batch: 650, loss is 4.06604868888855 and perplexity is 58.326042334283855
At time: 441.39789485931396 and batch: 700, loss is 4.1479445791244505 and perplexity is 63.30375063448586
At time: 441.9222934246063 and batch: 750, loss is 4.109350199699402 and perplexity is 60.907127238522584
At time: 442.43121123313904 and batch: 800, loss is 4.190819025039673 and perplexity is 66.07688742955297
At time: 442.93726444244385 and batch: 850, loss is 4.109998741149902 and perplexity is 60.94664084688914
At time: 443.4402093887329 and batch: 900, loss is 4.090939297676086 and perplexity is 59.796031607505334
At time: 443.94329810142517 and batch: 950, loss is 4.053751997947693 and perplexity is 57.61321669825798
At time: 444.44619274139404 and batch: 1000, loss is 3.98899453163147 and perplexity is 54.00056619659686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.761431066001334 and perplexity of 116.91311662628783
Finished 40 epochs...
Completing Train Step...
At time: 445.93997836112976 and batch: 50, loss is 4.267596144676208 and perplexity is 71.34991443380373
At time: 446.44485998153687 and batch: 100, loss is 4.176553354263306 and perplexity is 65.1409480815067
At time: 446.96092534065247 and batch: 150, loss is 4.206573224067688 and perplexity is 67.12611906622305
At time: 447.4651367664337 and batch: 200, loss is 4.230083026885986 and perplexity is 68.72293778850388
At time: 447.97870898246765 and batch: 250, loss is 4.260749979019165 and perplexity is 70.86310937538767
At time: 448.48514342308044 and batch: 300, loss is 4.132977070808411 and perplexity is 62.36330682762473
At time: 448.98630452156067 and batch: 350, loss is 4.14278582572937 and perplexity is 62.97802309238551
At time: 449.48879981040955 and batch: 400, loss is 4.144671010971069 and perplexity is 63.09686031196205
At time: 449.9909784793854 and batch: 450, loss is 4.180711889266968 and perplexity is 65.41240303053067
At time: 450.5073821544647 and batch: 500, loss is 4.225085000991822 and perplexity is 68.38031569619072
At time: 451.0179419517517 and batch: 550, loss is 4.1555095911026 and perplexity is 63.784460260779646
At time: 451.52663254737854 and batch: 600, loss is 4.093036918640137 and perplexity is 59.921592660681796
At time: 452.03379011154175 and batch: 650, loss is 4.0654232215881345 and perplexity is 58.28957270850923
At time: 452.53752875328064 and batch: 700, loss is 4.147411780357361 and perplexity is 63.27003145776122
At time: 453.04162216186523 and batch: 750, loss is 4.108788485527039 and perplexity is 60.8729244489499
At time: 453.54449439048767 and batch: 800, loss is 4.190252265930176 and perplexity is 66.03944836214349
At time: 454.05011320114136 and batch: 850, loss is 4.109558982849121 and perplexity is 60.919844947963306
At time: 454.5551931858063 and batch: 900, loss is 4.090440015792847 and perplexity is 59.76618398404298
At time: 455.0742988586426 and batch: 950, loss is 4.0533562278747555 and perplexity is 57.59041962278084
At time: 455.58754992485046 and batch: 1000, loss is 3.9884915924072266 and perplexity is 53.973414022244334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.761403525747904 and perplexity of 116.90989685376357
Finished 41 epochs...
Completing Train Step...
At time: 457.1472067832947 and batch: 50, loss is 4.266582584381103 and perplexity is 71.27763363013871
At time: 457.6637809276581 and batch: 100, loss is 4.175605273246765 and perplexity is 65.07921845219983
At time: 458.168824672699 and batch: 150, loss is 4.205843281745911 and perplexity is 67.07713874959781
At time: 458.6848111152649 and batch: 200, loss is 4.229234561920166 and perplexity is 68.66465351302155
At time: 459.1879770755768 and batch: 250, loss is 4.2599360513687134 and perplexity is 70.8054553975363
At time: 459.68872356414795 and batch: 300, loss is 4.132040791511535 and perplexity is 62.30494468045583
At time: 460.1942780017853 and batch: 350, loss is 4.141925549507141 and perplexity is 62.9238678941507
At time: 460.69419836997986 and batch: 400, loss is 4.143819975852966 and perplexity is 63.04318551080444
At time: 461.1948838233948 and batch: 450, loss is 4.180069017410278 and perplexity is 65.3703647516049
At time: 461.6945414543152 and batch: 500, loss is 4.224398488998413 and perplexity is 68.33338789944295
At time: 462.19508600234985 and batch: 550, loss is 4.154772529602051 and perplexity is 63.73746451229412
At time: 462.69625210762024 and batch: 600, loss is 4.092455410957337 and perplexity is 59.88675792349925
At time: 463.19713401794434 and batch: 650, loss is 4.064814691543579 and perplexity is 58.25411254261424
At time: 463.69710659980774 and batch: 700, loss is 4.146859230995179 and perplexity is 63.235081298967174
At time: 464.1979284286499 and batch: 750, loss is 4.108134546279907 and perplexity is 60.83313026747173
At time: 464.6976773738861 and batch: 800, loss is 4.1896140766143795 and perplexity is 65.99731613737552
At time: 465.1998519897461 and batch: 850, loss is 4.109110307693482 and perplexity is 60.89251785800135
At time: 465.712769985199 and batch: 900, loss is 4.089887828826904 and perplexity is 59.73319098623285
At time: 466.2144732475281 and batch: 950, loss is 4.052947134971618 and perplexity is 57.566864609247425
At time: 466.71784138679504 and batch: 1000, loss is 3.9879752016067505 and perplexity is 53.94554984279511
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.761449674280678 and perplexity of 116.91529219846277
Annealing...
Finished 42 epochs...
Completing Train Step...
At time: 468.2076439857483 and batch: 50, loss is 4.26573263168335 and perplexity is 71.21707675203189
At time: 468.7322986125946 and batch: 100, loss is 4.175430312156677 and perplexity is 65.06783311722172
At time: 469.2359507083893 and batch: 150, loss is 4.204708375930786 and perplexity is 67.00105569648498
At time: 469.7384295463562 and batch: 200, loss is 4.227978286743164 and perplexity is 68.57844597480874
At time: 470.2405183315277 and batch: 250, loss is 4.258568248748779 and perplexity is 70.70867371434677
At time: 470.74324107170105 and batch: 300, loss is 4.129298853874206 and perplexity is 62.13434240493039
At time: 471.2469754219055 and batch: 350, loss is 4.139786539077758 and perplexity is 62.78941693173698
At time: 471.7634987831116 and batch: 400, loss is 4.140292434692383 and perplexity is 62.821189858622134
At time: 472.26545786857605 and batch: 450, loss is 4.176964068412781 and perplexity is 65.16770788553292
At time: 472.76737356185913 and batch: 500, loss is 4.220367288589477 and perplexity is 68.05847680135282
At time: 473.26958107948303 and batch: 550, loss is 4.1509067249298095 and perplexity is 63.49154357160905
At time: 473.7727646827698 and batch: 600, loss is 4.088378019332886 and perplexity is 59.643073294765706
At time: 474.27521991729736 and batch: 650, loss is 4.059930005073547 and perplexity is 57.970253313332336
At time: 474.784987449646 and batch: 700, loss is 4.142312426567077 and perplexity is 62.94821640479783
At time: 475.3068335056305 and batch: 750, loss is 4.1024480676651 and perplexity is 60.488185662704474
At time: 475.8208336830139 and batch: 800, loss is 4.183594937324524 and perplexity is 65.60126224630957
At time: 476.3264138698578 and batch: 850, loss is 4.103471250534057 and perplexity is 60.55010781148757
At time: 476.8279378414154 and batch: 900, loss is 4.083050861358642 and perplexity is 59.32619001429428
At time: 477.3292579650879 and batch: 950, loss is 4.045735993385315 and perplexity is 57.15323496126153
At time: 477.8309998512268 and batch: 1000, loss is 3.9800949764251707 and perplexity is 53.52211732547124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.760535635599276 and perplexity of 116.80847592348631
Finished 43 epochs...
Completing Train Step...
At time: 479.3419704437256 and batch: 50, loss is 4.2645880651474 and perplexity is 71.13561069974557
At time: 479.85281467437744 and batch: 100, loss is 4.174449443817139 and perplexity is 65.0040414305321
At time: 480.36120414733887 and batch: 150, loss is 4.204162917137146 and perplexity is 66.96451934691461
At time: 480.86620831489563 and batch: 200, loss is 4.22733063697815 and perplexity is 68.53404553990572
At time: 481.3708276748657 and batch: 250, loss is 4.257906608581543 and perplexity is 70.66190548921962
At time: 481.8742084503174 and batch: 300, loss is 4.128563632965088 and perplexity is 62.08867672645785
At time: 482.37893295288086 and batch: 350, loss is 4.139206228256225 and perplexity is 62.75299012407133
At time: 482.88473176956177 and batch: 400, loss is 4.139779825210571 and perplexity is 62.78899537334606
At time: 483.38920378685 and batch: 450, loss is 4.1765124177932735 and perplexity is 65.13828149561836
At time: 483.894855260849 and batch: 500, loss is 4.219937939643859 and perplexity is 68.02926223816755
At time: 484.4134171009064 and batch: 550, loss is 4.150548400878907 and perplexity is 63.468797100065686
At time: 484.91858530044556 and batch: 600, loss is 4.088130893707276 and perplexity is 59.62833578404771
At time: 485.4348919391632 and batch: 650, loss is 4.059716982841492 and perplexity is 57.957905675786044
At time: 485.94552755355835 and batch: 700, loss is 4.142083463668823 and perplexity is 62.93380524860285
At time: 486.45122838020325 and batch: 750, loss is 4.1024448108673095 and perplexity is 60.48798866523584
At time: 486.95620918273926 and batch: 800, loss is 4.183495955467224 and perplexity is 65.59476923288179
At time: 487.46079444885254 and batch: 850, loss is 4.103556118011475 and perplexity is 60.555246764456776
At time: 487.9650869369507 and batch: 900, loss is 4.083183546066284 and perplexity is 59.33406221472057
At time: 488.4692704677582 and batch: 950, loss is 4.045906295776367 and perplexity is 57.162969122683776
At time: 488.97288274765015 and batch: 1000, loss is 3.9804386949539183 and perplexity is 53.5405170308725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.760345831149962 and perplexity of 116.78630725896058
Finished 44 epochs...
Completing Train Step...
At time: 490.473260641098 and batch: 50, loss is 4.264059095382691 and perplexity is 71.09799206292917
At time: 491.00635528564453 and batch: 100, loss is 4.17405369758606 and perplexity is 64.9783214157662
At time: 491.5135381221771 and batch: 150, loss is 4.203773908615112 and perplexity is 66.93847464434883
At time: 492.0152051448822 and batch: 200, loss is 4.226911144256592 and perplexity is 68.5053020358897
At time: 492.5151221752167 and batch: 250, loss is 4.257499737739563 and perplexity is 70.63316106826724
At time: 493.0171368122101 and batch: 300, loss is 4.128131899833679 and perplexity is 62.06187677325984
At time: 493.51900362968445 and batch: 350, loss is 4.138848600387573 and perplexity is 62.73055191846468
At time: 494.0191550254822 and batch: 400, loss is 4.139437675476074 and perplexity is 62.767515810072744
At time: 494.53409457206726 and batch: 450, loss is 4.1762137031555175 and perplexity is 65.11882664332659
At time: 495.0436906814575 and batch: 500, loss is 4.219664244651795 and perplexity is 68.0106455175469
At time: 495.5444574356079 and batch: 550, loss is 4.150312142372131 and perplexity is 63.45380382804978
At time: 496.06012630462646 and batch: 600, loss is 4.087985081672668 and perplexity is 59.61964188893945
At time: 496.5785427093506 and batch: 650, loss is 4.059583916664123 and perplexity is 57.95019395192568
At time: 497.08578658103943 and batch: 700, loss is 4.141934509277344 and perplexity is 62.92443168007302
At time: 497.61668062210083 and batch: 750, loss is 4.102457242012024 and perplexity is 60.48874060485015
At time: 498.12922382354736 and batch: 800, loss is 4.183433737754822 and perplexity is 65.59068820335202
At time: 498.63504552841187 and batch: 850, loss is 4.1036216926574705 and perplexity is 60.55921778352421
At time: 499.1352117061615 and batch: 900, loss is 4.083284454345703 and perplexity is 59.34004981494376
At time: 499.63534593582153 and batch: 950, loss is 4.045998735427856 and perplexity is 57.16825349186641
At time: 500.13458728790283 and batch: 1000, loss is 3.98063934803009 and perplexity is 53.55126117820163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.760244602110328 and perplexity of 116.77448569158737
Finished 45 epochs...
Completing Train Step...
At time: 501.60283875465393 and batch: 50, loss is 4.263640451431274 and perplexity is 71.0682335481399
At time: 502.1191885471344 and batch: 100, loss is 4.173759822845459 and perplexity is 64.95922873398132
At time: 502.61890745162964 and batch: 150, loss is 4.203451170921325 and perplexity is 66.91687456118451
At time: 503.1195652484894 and batch: 200, loss is 4.226570339202881 and perplexity is 68.48195906067775
At time: 503.6211202144623 and batch: 250, loss is 4.257179555892944 and perplexity is 70.61054923246671
At time: 504.12408924102783 and batch: 300, loss is 4.127778902053833 and perplexity is 62.03997293477639
At time: 504.6251971721649 and batch: 350, loss is 4.138567657470703 and perplexity is 62.712930689627456
At time: 505.1252384185791 and batch: 400, loss is 4.139152407646179 and perplexity is 62.749612810745994
At time: 505.626074552536 and batch: 450, loss is 4.175970287322998 and perplexity is 65.10297761895649
At time: 506.1261022090912 and batch: 500, loss is 4.219438529014587 and perplexity is 67.99529618371452
At time: 506.62548446655273 and batch: 550, loss is 4.1501216697692875 and perplexity is 63.44171876784728
At time: 507.1271781921387 and batch: 600, loss is 4.087864089012146 and perplexity is 59.61242878622295
At time: 507.6279344558716 and batch: 650, loss is 4.059472780227662 and perplexity is 57.94375393174467
At time: 508.1468551158905 and batch: 700, loss is 4.141810374259949 and perplexity is 62.91662103944887
At time: 508.65287017822266 and batch: 750, loss is 4.102452697753907 and perplexity is 60.488465729024206
At time: 509.15370178222656 and batch: 800, loss is 4.183369631767273 and perplexity is 65.58648358228288
At time: 509.6540598869324 and batch: 850, loss is 4.103660254478455 and perplexity is 60.56155310226612
At time: 510.16818022727966 and batch: 900, loss is 4.083352670669556 and perplexity is 59.34409791307099
At time: 510.6690146923065 and batch: 950, loss is 4.04604552268982 and perplexity is 57.17092830049152
At time: 511.1711754798889 and batch: 1000, loss is 3.9807693338394166 and perplexity is 53.5582225346553
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.760183194788491 and perplexity of 116.7673151033277
Finished 46 epochs...
Completing Train Step...
At time: 512.6452279090881 and batch: 50, loss is 4.2632785892486575 and perplexity is 71.04252129446016
At time: 513.149806022644 and batch: 100, loss is 4.1735153484344485 and perplexity is 64.94334980587178
At time: 513.6528813838959 and batch: 150, loss is 4.203168253898621 and perplexity is 66.89794531609408
At time: 514.1565942764282 and batch: 200, loss is 4.2262727069854735 and perplexity is 68.46157965628424
At time: 514.6602857112885 and batch: 250, loss is 4.256904530525207 and perplexity is 70.59113221040718
At time: 515.1649434566498 and batch: 300, loss is 4.127472939491272 and perplexity is 62.02099392925673
At time: 515.6695220470428 and batch: 350, loss is 4.138326601982117 and perplexity is 62.697815215386605
At time: 516.1721656322479 and batch: 400, loss is 4.138895626068115 and perplexity is 62.73350193472216
At time: 516.6759748458862 and batch: 450, loss is 4.175753545761109 and perplexity is 65.08886862696143
At time: 517.1817331314087 and batch: 500, loss is 4.219234967231751 and perplexity is 67.98145634867743
At time: 517.6871993541718 and batch: 550, loss is 4.149952597618103 and perplexity is 63.43099344668248
At time: 518.1914372444153 and batch: 600, loss is 4.087754325866699 and perplexity is 59.60588589762225
At time: 518.6969511508942 and batch: 650, loss is 4.059371500015259 and perplexity is 57.937885673213316
At time: 519.2018718719482 and batch: 700, loss is 4.1416958236694335 and perplexity is 62.90941431613066
At time: 519.7072370052338 and batch: 750, loss is 4.102431840896607 and perplexity is 60.48720414288261
At time: 520.2134776115417 and batch: 800, loss is 4.183298273086548 and perplexity is 65.58180358432227
At time: 520.7182104587555 and batch: 850, loss is 4.103678774833679 and perplexity is 60.562674734129
At time: 521.2398209571838 and batch: 900, loss is 4.083396558761597 and perplexity is 59.34670246945635
At time: 521.7527301311493 and batch: 950, loss is 4.046065335273743 and perplexity is 57.17206101552739
At time: 522.2564496994019 and batch: 1000, loss is 3.9808571338653564 and perplexity is 53.56292515442519
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.760141140077172 and perplexity of 116.76240459085538
Finished 47 epochs...
Completing Train Step...
At time: 523.736156463623 and batch: 50, loss is 4.262952213287353 and perplexity is 71.01933850663733
At time: 524.2651610374451 and batch: 100, loss is 4.173299412727356 and perplexity is 64.92932773169825
At time: 524.7712433338165 and batch: 150, loss is 4.202913789749146 and perplexity is 66.88092435304186
At time: 525.2829918861389 and batch: 200, loss is 4.226002626419067 and perplexity is 68.44309201076285
At time: 525.7833478450775 and batch: 250, loss is 4.256658902168274 and perplexity is 70.57379515591155
At time: 526.2852227687836 and batch: 300, loss is 4.127199053764343 and perplexity is 62.00400959023974
At time: 526.7865097522736 and batch: 350, loss is 4.13810845375061 and perplexity is 62.6841392896218
At time: 527.2891759872437 and batch: 400, loss is 4.138657011985779 and perplexity is 62.71853462350288
At time: 527.7922031879425 and batch: 450, loss is 4.175552039146424 and perplexity is 65.07575411076598
At time: 528.2940530776978 and batch: 500, loss is 4.219043955802918 and perplexity is 67.96847235365131
At time: 528.7975952625275 and batch: 550, loss is 4.1497953462600705 and perplexity is 63.42101962104098
At time: 529.30011677742 and batch: 600, loss is 4.087651529312134 and perplexity is 59.599758932840984
At time: 529.802286863327 and batch: 650, loss is 4.0592759990692135 and perplexity is 57.93235281452047
At time: 530.3040819168091 and batch: 700, loss is 4.141584587097168 and perplexity is 62.90241687771166
At time: 530.820184469223 and batch: 750, loss is 4.10239755153656 and perplexity is 60.485130110920366
At time: 531.3287162780762 and batch: 800, loss is 4.183219709396362 and perplexity is 65.57665143821212
At time: 531.835809469223 and batch: 850, loss is 4.1036821413040165 and perplexity is 60.562878616920216
At time: 532.355128288269 and batch: 900, loss is 4.083421907424927 and perplexity is 59.34820684810392
At time: 532.863481760025 and batch: 950, loss is 4.0460692358016965 and perplexity is 57.172284017184474
At time: 533.3662841320038 and batch: 1000, loss is 3.980916085243225 and perplexity is 53.56608285574022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.76011508848609 and perplexity of 116.75936278405939
Finished 48 epochs...
Completing Train Step...
At time: 534.8575320243835 and batch: 50, loss is 4.26265034198761 and perplexity is 70.99790304215392
At time: 535.38685131073 and batch: 100, loss is 4.173101844787598 and perplexity is 64.91650104530113
At time: 535.8895494937897 and batch: 150, loss is 4.202679462432862 and perplexity is 66.86525416157627
At time: 536.4051008224487 and batch: 200, loss is 4.225752449035644 and perplexity is 68.4259712387945
At time: 536.9068853855133 and batch: 250, loss is 4.256433019638061 and perplexity is 70.55785556879992
At time: 537.5922973155975 and batch: 300, loss is 4.126947302818298 and perplexity is 61.988401986863686
At time: 538.095235824585 and batch: 350, loss is 4.137904734611511 and perplexity is 62.671370631384725
At time: 538.59721326828 and batch: 400, loss is 4.138431239128113 and perplexity is 62.70437607907996
At time: 539.0987408161163 and batch: 450, loss is 4.175360255241394 and perplexity is 65.06327482522117
At time: 539.6001586914062 and batch: 500, loss is 4.218860511779785 and perplexity is 67.95600508719428
At time: 540.1006677150726 and batch: 550, loss is 4.149644956588745 and perplexity is 63.41148247190741
At time: 540.6010911464691 and batch: 600, loss is 4.087552828788757 and perplexity is 59.59387669573591
At time: 541.1042718887329 and batch: 650, loss is 4.059183316230774 and perplexity is 57.926983728439055
At time: 541.6061570644379 and batch: 700, loss is 4.141474947929383 and perplexity is 62.8955206871267
At time: 542.1128768920898 and batch: 750, loss is 4.102352170944214 and perplexity is 60.48238532216834
At time: 542.6149866580963 and batch: 800, loss is 4.183134489059448 and perplexity is 65.57106321200148
At time: 543.121732711792 and batch: 850, loss is 4.103673024177551 and perplexity is 60.5623264600138
At time: 543.6209726333618 and batch: 900, loss is 4.0834321737289425 and perplexity is 59.348816137965784
At time: 544.1200735569 and batch: 950, loss is 4.046062126159668 and perplexity is 57.17187754415608
At time: 544.62131690979 and batch: 1000, loss is 3.9809534978866576 and perplexity is 53.56808694198703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.760096108041158 and perplexity of 116.75714666043532
Finished 49 epochs...
Completing Train Step...
At time: 546.0968501567841 and batch: 50, loss is 4.262366309165954 and perplexity is 70.97774017101538
At time: 546.5971257686615 and batch: 100, loss is 4.172916569709778 and perplexity is 64.90447474963898
At time: 547.0971720218658 and batch: 150, loss is 4.202458782196045 and perplexity is 66.85049994949446
At time: 547.59685587883 and batch: 200, loss is 4.225517282485962 and perplexity is 68.4098816311725
At time: 548.1000282764435 and batch: 250, loss is 4.256220736503601 and perplexity is 70.54287891576065
At time: 548.6021356582642 and batch: 300, loss is 4.126711082458496 and perplexity is 61.97376079358487
At time: 549.1181101799011 and batch: 350, loss is 4.1377105808258055 and perplexity is 62.6592039286652
At time: 549.6201813220978 and batch: 400, loss is 4.138214178085327 and perplexity is 62.69076687888807
At time: 550.122878074646 and batch: 450, loss is 4.175175766944886 and perplexity is 65.05127251965993
At time: 550.6338045597076 and batch: 500, loss is 4.218682508468628 and perplexity is 67.943909769811
At time: 551.1541340351105 and batch: 550, loss is 4.149498500823975 and perplexity is 63.40219617477921
At time: 551.6612432003021 and batch: 600, loss is 4.087455606460571 and perplexity is 59.58808312193483
At time: 552.1689321994781 and batch: 650, loss is 4.059090900421142 and perplexity is 57.92163060669862
At time: 552.684490442276 and batch: 700, loss is 4.1413660955429075 and perplexity is 62.888674732207335
At time: 553.1880278587341 and batch: 750, loss is 4.102298078536987 and perplexity is 60.47911377283521
At time: 553.6883549690247 and batch: 800, loss is 4.183043556213379 and perplexity is 65.56510091969206
At time: 554.1900939941406 and batch: 850, loss is 4.103653173446656 and perplexity is 60.5611242655011
At time: 554.6924033164978 and batch: 900, loss is 4.083428764343262 and perplexity is 59.348613795306825
At time: 555.1940627098083 and batch: 950, loss is 4.046045923233033 and perplexity is 57.17095119992341
At time: 555.7153654098511 and batch: 1000, loss is 3.9809736824035644 and perplexity is 53.56916819885586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.760077499761814 and perplexity of 116.75497403104936
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd6e1b10898>
SETTINGS FOR THIS RUN
{'anneal': 3.8677734013888427, 'data': 'ptb', 'dropout': 0.6408287226766163, 'tune_wordvecs': True, 'lr': 5.739873896080777, 'num_layers': 1, 'batch_size': 20, 'wordvec_source': '', 'wordvec_dim': 200, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7571396827697754 and batch: 50, loss is 6.7330536460876464 and perplexity is 839.707524499872
At time: 1.2802636623382568 and batch: 100, loss is 6.059599485397339 and perplexity is 428.20390059508577
At time: 1.7904284000396729 and batch: 150, loss is 5.8488930892944335 and perplexity is 346.85023567186437
At time: 2.299625873565674 and batch: 200, loss is 5.785280036926269 and perplexity is 325.47317185853984
At time: 2.8090553283691406 and batch: 250, loss is 5.8255993747711186 and perplexity is 338.8641788612338
At time: 3.323753833770752 and batch: 300, loss is 5.714218616485596 and perplexity is 303.1472364874517
At time: 3.864144802093506 and batch: 350, loss is 5.698216953277588 and perplexity is 298.33498123301916
At time: 4.373513698577881 and batch: 400, loss is 5.672351531982422 and perplexity is 290.7173624066685
At time: 4.883457660675049 and batch: 450, loss is 5.6844120216369625 and perplexity is 294.2447846110878
At time: 5.411023139953613 and batch: 500, loss is 5.700017576217651 and perplexity is 298.87265397171234
At time: 5.932942152023315 and batch: 550, loss is 5.629181432723999 and perplexity is 278.43410727612434
At time: 6.441652059555054 and batch: 600, loss is 5.525857048034668 and perplexity is 251.10145183203755
At time: 6.9567530155181885 and batch: 650, loss is 5.494013109207153 and perplexity is 243.23136487915247
At time: 7.485382318496704 and batch: 700, loss is 5.562761077880859 and perplexity is 260.5412187077352
At time: 8.000650644302368 and batch: 750, loss is 5.4729540920257564 and perplexity is 238.16270916119973
At time: 8.518589496612549 and batch: 800, loss is 5.561128673553466 and perplexity is 260.1162570438499
At time: 9.027948141098022 and batch: 850, loss is 5.5024886798858645 and perplexity is 245.30165053638746
At time: 9.538480520248413 and batch: 900, loss is 5.533774356842041 and perplexity is 253.09739037299633
At time: 10.047736644744873 and batch: 950, loss is 5.498193922042847 and perplexity is 244.25039840137393
At time: 10.557163715362549 and batch: 1000, loss is 5.404214525222779 and perplexity is 222.34150823012013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.273255138862424 and perplexity of 195.04984452761028
Finished 1 epochs...
Completing Train Step...
At time: 12.06986927986145 and batch: 50, loss is 5.187219581604004 and perplexity is 178.970248334414
At time: 12.583918809890747 and batch: 100, loss is 5.043170328140259 and perplexity is 154.96051276992208
At time: 13.083755254745483 and batch: 150, loss is 5.0145931053161625 and perplexity is 150.59484805465027
At time: 13.584960460662842 and batch: 200, loss is 4.9620202541351315 and perplexity is 142.88216279192102
At time: 14.093875646591187 and batch: 250, loss is 4.996198873519898 and perplexity is 147.8500927347148
At time: 14.604714155197144 and batch: 300, loss is 4.880353937149048 and perplexity is 131.67726111349174
At time: 15.105159044265747 and batch: 350, loss is 4.85194917678833 and perplexity is 127.98962126799174
At time: 15.604736804962158 and batch: 400, loss is 4.844575109481812 and perplexity is 127.04928848586425
At time: 16.10491633415222 and batch: 450, loss is 4.859681091308594 and perplexity is 128.98306172828532
At time: 16.60612416267395 and batch: 500, loss is 4.865152940750122 and perplexity is 129.69077209906493
At time: 17.119911670684814 and batch: 550, loss is 4.763061037063599 and perplexity is 117.10383701531984
At time: 17.630078554153442 and batch: 600, loss is 4.681902837753296 and perplexity is 107.97533676445788
At time: 18.15794277191162 and batch: 650, loss is 4.656634244918823 and perplexity is 105.28113464466064
At time: 18.668763875961304 and batch: 700, loss is 4.707034645080566 and perplexity is 110.7233386138795
At time: 19.16836190223694 and batch: 750, loss is 4.662291116714478 and perplexity is 105.87838421529752
At time: 19.666706323623657 and batch: 800, loss is 4.757083444595337 and perplexity is 116.40592599312996
At time: 20.180628538131714 and batch: 850, loss is 4.633740482330322 and perplexity is 102.89823419870478
At time: 20.699191331863403 and batch: 900, loss is 4.634586668014526 and perplexity is 102.98534206092896
At time: 21.217928647994995 and batch: 950, loss is 4.638359127044677 and perplexity is 103.37458378216039
At time: 21.736669778823853 and batch: 1000, loss is 4.516274633407593 and perplexity is 91.4941131824174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.730227958865282 and perplexity of 113.32139202043379
Finished 2 epochs...
Completing Train Step...
At time: 23.236931085586548 and batch: 50, loss is 4.591369371414185 and perplexity is 98.62939800853128
At time: 23.77048969268799 and batch: 100, loss is 4.464167432785034 and perplexity is 86.84869205735315
At time: 24.277156114578247 and batch: 150, loss is 4.543890018463134 and perplexity is 94.0559688578467
At time: 24.776620149612427 and batch: 200, loss is 4.549531469345093 and perplexity is 94.58808051595807
At time: 25.282962799072266 and batch: 250, loss is 4.576466856002807 and perplexity is 97.17046973563272
At time: 25.784093618392944 and batch: 300, loss is 4.447704849243164 and perplexity is 85.43064261368023
At time: 26.283775329589844 and batch: 350, loss is 4.453273315429687 and perplexity is 85.90768722909736
At time: 26.792130708694458 and batch: 400, loss is 4.4600325775146485 and perplexity is 86.49032669031875
At time: 27.299431324005127 and batch: 450, loss is 4.499536666870117 and perplexity is 89.97543304217153
At time: 27.806142807006836 and batch: 500, loss is 4.508506574630737 and perplexity is 90.78613490203614
At time: 28.306803941726685 and batch: 550, loss is 4.4211381149291995 and perplexity is 83.19091231923534
At time: 28.807169198989868 and batch: 600, loss is 4.363345432281494 and perplexity is 78.5193767792796
At time: 29.307037591934204 and batch: 650, loss is 4.333195452690124 and perplexity is 76.18735108776335
At time: 29.82211685180664 and batch: 700, loss is 4.412559990882873 and perplexity is 82.48034238955611
At time: 30.32704448699951 and batch: 750, loss is 4.3707375764846805 and perplexity is 79.10195392947843
At time: 30.83474063873291 and batch: 800, loss is 4.4811856651306154 and perplexity is 88.3393514951042
At time: 31.335911512374878 and batch: 850, loss is 4.356278896331787 and perplexity is 77.96647263982265
At time: 31.83708429336548 and batch: 900, loss is 4.336178846359253 and perplexity is 76.41498734410699
At time: 32.33687472343445 and batch: 950, loss is 4.367826957702636 and perplexity is 78.87205303590859
At time: 32.84451150894165 and batch: 1000, loss is 4.2571258020401 and perplexity is 70.60675374540601
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6094605980849845 and perplexity of 100.42996290917283
Finished 3 epochs...
Completing Train Step...
At time: 34.36353087425232 and batch: 50, loss is 4.356291007995606 and perplexity is 77.96741694924695
At time: 34.88454580307007 and batch: 100, loss is 4.235747489929199 and perplexity is 69.1133209414757
At time: 35.38915753364563 and batch: 150, loss is 4.330125923156738 and perplexity is 75.9538503156243
At time: 35.89310097694397 and batch: 200, loss is 4.356168813705445 and perplexity is 77.95789035813652
At time: 36.39608597755432 and batch: 250, loss is 4.370406336784363 and perplexity is 79.07575656100806
At time: 36.90004920959473 and batch: 300, loss is 4.231266865730285 and perplexity is 68.80434284748048
At time: 37.41680860519409 and batch: 350, loss is 4.249077801704407 and perplexity is 70.04079105631156
At time: 37.93121266365051 and batch: 400, loss is 4.255927639007568 and perplexity is 70.52220600432479
At time: 38.43583559989929 and batch: 450, loss is 4.305398921966553 and perplexity is 74.0987691843625
At time: 38.94001078605652 and batch: 500, loss is 4.316692547798157 and perplexity is 74.9403562980272
At time: 39.44421911239624 and batch: 550, loss is 4.230531749725341 and perplexity is 68.75378226006904
At time: 39.948073625564575 and batch: 600, loss is 4.177616782188416 and perplexity is 65.21025763108824
At time: 40.45818519592285 and batch: 650, loss is 4.1513179063797 and perplexity is 63.51765548454981
At time: 40.96119976043701 and batch: 700, loss is 4.235875988006592 and perplexity is 69.12220244095451
At time: 41.4656720161438 and batch: 750, loss is 4.1981910753250125 and perplexity is 66.56580953125147
At time: 41.97044539451599 and batch: 800, loss is 4.319103355407715 and perplexity is 75.12124103079019
At time: 42.474520444869995 and batch: 850, loss is 4.185055994987488 and perplexity is 65.69717952649047
At time: 42.995203495025635 and batch: 900, loss is 4.158638706207276 and perplexity is 63.98436177313714
At time: 43.4997398853302 and batch: 950, loss is 4.199314661026001 and perplexity is 66.6406439565878
At time: 44.00426435470581 and batch: 1000, loss is 4.088700861930847 and perplexity is 59.66233172805261
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.559568823837653 and perplexity of 95.54227539587264
Finished 4 epochs...
Completing Train Step...
At time: 45.51003456115723 and batch: 50, loss is 4.206900091171264 and perplexity is 67.1480639726752
At time: 46.02895641326904 and batch: 100, loss is 4.0859184837341305 and perplexity is 59.496559284848715
At time: 46.54118084907532 and batch: 150, loss is 4.190894002914429 and perplexity is 66.08184191987918
At time: 47.04294419288635 and batch: 200, loss is 4.212870850563049 and perplexity is 67.5501882051568
At time: 47.54380679130554 and batch: 250, loss is 4.2254894876480105 and perplexity is 68.40798021602319
At time: 48.05508542060852 and batch: 300, loss is 4.076426157951355 and perplexity is 58.934470546417
At time: 48.56290292739868 and batch: 350, loss is 4.1031825685501095 and perplexity is 60.53263060903441
At time: 49.06399393081665 and batch: 400, loss is 4.11410843372345 and perplexity is 61.197628191317335
At time: 49.563859701156616 and batch: 450, loss is 4.165667247772217 and perplexity is 64.43566265500044
At time: 50.06393384933472 and batch: 500, loss is 4.1792891454696655 and perplexity is 65.31940411235668
At time: 50.56493592262268 and batch: 550, loss is 4.090869722366333 and perplexity is 59.79187142480889
At time: 51.06647610664368 and batch: 600, loss is 4.040375537872315 and perplexity is 56.847687256911705
At time: 51.568532943725586 and batch: 650, loss is 4.022481989860535 and perplexity is 55.83952711573671
At time: 52.070472240448 and batch: 700, loss is 4.1052335929870605 and perplexity is 60.656911922083665
At time: 52.57607316970825 and batch: 750, loss is 4.069246039390564 and perplexity is 58.512829588053755
At time: 53.079055070877075 and batch: 800, loss is 4.190903601646423 and perplexity is 66.08247622481372
At time: 53.58064675331116 and batch: 850, loss is 4.058644695281982 and perplexity is 57.89579144266599
At time: 54.09619641304016 and batch: 900, loss is 4.026843628883362 and perplexity is 56.08361089193056
At time: 54.609180212020874 and batch: 950, loss is 4.072862753868103 and perplexity is 58.72483693987698
At time: 55.11092448234558 and batch: 1000, loss is 3.9638214540481567 and perplexity is 52.65817272864981
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.531969023913872 and perplexity of 92.94138484020648
Finished 5 epochs...
Completing Train Step...
At time: 56.616982221603394 and batch: 50, loss is 4.086105360984802 and perplexity is 59.50767887723934
At time: 57.13130712509155 and batch: 100, loss is 3.967284126281738 and perplexity is 52.8408267742712
At time: 57.63202238082886 and batch: 150, loss is 4.06995701789856 and perplexity is 58.55444574467628
At time: 58.1341872215271 and batch: 200, loss is 4.10375024318695 and perplexity is 60.567003203434375
At time: 58.63507628440857 and batch: 250, loss is 4.10829867362976 and perplexity is 60.84311546732558
At time: 59.135857343673706 and batch: 300, loss is 3.955804133415222 and perplexity is 52.23768312625256
At time: 59.636255741119385 and batch: 350, loss is 3.993563008308411 and perplexity is 54.247830905280374
At time: 60.13815665245056 and batch: 400, loss is 4.000718321800232 and perplexity is 54.63738316388494
At time: 60.65322995185852 and batch: 450, loss is 4.053249521255493 and perplexity is 57.58427467166031
At time: 61.175084352493286 and batch: 500, loss is 4.073106336593628 and perplexity is 58.73914303800325
At time: 61.683162212371826 and batch: 550, loss is 3.9851133728027346 and perplexity is 53.79138761257852
At time: 62.18515586853027 and batch: 600, loss is 3.935215573310852 and perplexity is 51.1731803395992
At time: 62.68582606315613 and batch: 650, loss is 3.9178726720809935 and perplexity is 50.29334046814506
At time: 63.18479919433594 and batch: 700, loss is 4.002224545478821 and perplexity is 54.71974129345443
At time: 63.68391823768616 and batch: 750, loss is 3.9606383085250854 and perplexity is 52.490820596277715
At time: 64.18422150611877 and batch: 800, loss is 4.088808932304382 and perplexity is 59.66877980694533
At time: 64.68343353271484 and batch: 850, loss is 3.9603952169418335 and perplexity is 52.47806207040073
At time: 65.1849889755249 and batch: 900, loss is 3.9176893424987793 and perplexity is 50.28412105617017
At time: 65.68552947044373 and batch: 950, loss is 3.969948716163635 and perplexity is 52.98181365934102
At time: 66.1869158744812 and batch: 1000, loss is 3.864681444168091 and perplexity is 47.68807869720913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.521638079387386 and perplexity of 91.98615525695465
Finished 6 epochs...
Completing Train Step...
At time: 67.72403478622437 and batch: 50, loss is 3.9859699010849 and perplexity is 53.83748119481601
At time: 68.22601318359375 and batch: 100, loss is 3.8703805208206177 and perplexity is 47.96063262828853
At time: 68.73952913284302 and batch: 150, loss is 3.9719131278991697 and perplexity is 53.08599404895139
At time: 69.24010443687439 and batch: 200, loss is 4.00841157913208 and perplexity is 55.059343656646085
At time: 69.73968505859375 and batch: 250, loss is 4.0132726335525515 and perplexity is 55.327641699891046
At time: 70.23868107795715 and batch: 300, loss is 3.8552655744552613 and perplexity is 47.241161321151345
At time: 70.73796820640564 and batch: 350, loss is 3.895602192878723 and perplexity is 49.18566370263814
At time: 71.23669028282166 and batch: 400, loss is 3.907030487060547 and perplexity is 49.75099617690668
At time: 71.73567461967468 and batch: 450, loss is 3.958949537277222 and perplexity is 52.4022504161146
At time: 72.23392677307129 and batch: 500, loss is 3.980855269432068 and perplexity is 53.562825290017585
At time: 72.73416137695312 and batch: 550, loss is 3.89008975982666 and perplexity is 48.9152769534672
At time: 73.23493528366089 and batch: 600, loss is 3.844707365036011 and perplexity is 46.745003126900855
At time: 73.73683905601501 and batch: 650, loss is 3.8328945064544677 and perplexity is 46.19605969458492
At time: 74.23724746704102 and batch: 700, loss is 3.911580843925476 and perplexity is 49.97789681181606
At time: 74.73924136161804 and batch: 750, loss is 3.874004683494568 and perplexity is 48.13476511451144
At time: 75.25550937652588 and batch: 800, loss is 4.003846492767334 and perplexity is 54.80856584437056
At time: 75.76275420188904 and batch: 850, loss is 3.8761287689208985 and perplexity is 48.23711613025973
At time: 76.26342701911926 and batch: 900, loss is 3.8279661321640015 and perplexity is 45.96894832626937
At time: 76.76263284683228 and batch: 950, loss is 3.880796136856079 and perplexity is 48.462782724218
At time: 77.26444983482361 and batch: 1000, loss is 3.7817638063430787 and perplexity is 43.89339294413855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5275488132383765 and perplexity of 92.53147095901417
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 78.7534852027893 and batch: 50, loss is 3.9076056385040285 and perplexity is 49.77961876454476
At time: 79.27266120910645 and batch: 100, loss is 3.781544485092163 and perplexity is 43.883767245889786
At time: 79.77614259719849 and batch: 150, loss is 3.8745355224609375 and perplexity is 48.1603237066194
At time: 80.27985715866089 and batch: 200, loss is 3.9032026958465575 and perplexity is 49.56092376169213
At time: 80.78341603279114 and batch: 250, loss is 3.897216830253601 and perplexity is 49.265144862918966
At time: 81.30196619033813 and batch: 300, loss is 3.7253192615509034 and perplexity is 41.484475028678304
At time: 81.80619144439697 and batch: 350, loss is 3.7549515104293825 and perplexity is 42.73214769682147
At time: 82.31064748764038 and batch: 400, loss is 3.7603707551956176 and perplexity is 42.96435228303584
At time: 82.81466746330261 and batch: 450, loss is 3.8051977491378786 and perplexity is 44.93413492094486
At time: 83.3180422782898 and batch: 500, loss is 3.8251221704483034 and perplexity is 45.83840012215659
At time: 83.82199835777283 and batch: 550, loss is 3.712685251235962 and perplexity is 40.96365668308744
At time: 84.3248381614685 and batch: 600, loss is 3.6570061540603636 and perplexity is 38.74517198340992
At time: 84.84532451629639 and batch: 650, loss is 3.6286403274536134 and perplexity is 37.66157438000634
At time: 85.35472011566162 and batch: 700, loss is 3.7026411962509154 and perplexity is 40.55427483152216
At time: 85.87538743019104 and batch: 750, loss is 3.6504057312011717 and perplexity is 38.4902795887388
At time: 86.39234066009521 and batch: 800, loss is 3.7633742046356202 and perplexity is 43.093587521443176
At time: 86.9137876033783 and batch: 850, loss is 3.6208331537246705 and perplexity is 37.368688718199806
At time: 87.42018961906433 and batch: 900, loss is 3.5613217735290528 and perplexity is 35.20970565837954
At time: 87.92470479011536 and batch: 950, loss is 3.591871075630188 and perplexity is 36.30193608887517
At time: 88.42845559120178 and batch: 1000, loss is 3.483310546875 and perplexity is 32.567359578538074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.463940131955031 and perplexity of 86.82895352094165
Finished 8 epochs...
Completing Train Step...
At time: 89.9131178855896 and batch: 50, loss is 3.7972178792953493 and perplexity is 44.576993240033495
At time: 90.42495822906494 and batch: 100, loss is 3.676426410675049 and perplexity is 39.504966993600206
At time: 90.92613673210144 and batch: 150, loss is 3.7736680555343627 and perplexity is 43.53947751129044
At time: 91.42708849906921 and batch: 200, loss is 3.809360876083374 and perplexity is 45.121591360540506
At time: 91.92886304855347 and batch: 250, loss is 3.803878116607666 and perplexity is 44.87487748240738
At time: 92.43697023391724 and batch: 300, loss is 3.633083019256592 and perplexity is 37.829265371671916
At time: 92.93669414520264 and batch: 350, loss is 3.6708380508422853 and perplexity is 39.28481474080347
At time: 93.43670749664307 and batch: 400, loss is 3.6815545177459716 and perplexity is 39.7080730238136
At time: 93.94171690940857 and batch: 450, loss is 3.730826096534729 and perplexity is 41.713553356422345
At time: 94.45287227630615 and batch: 500, loss is 3.755959639549255 and perplexity is 42.77524894142883
At time: 94.95291018486023 and batch: 550, loss is 3.648681197166443 and perplexity is 38.42395899407969
At time: 95.4550530910492 and batch: 600, loss is 3.5978001070022585 and perplexity is 36.5178107382033
At time: 95.95465135574341 and batch: 650, loss is 3.574816527366638 and perplexity is 35.68807243032109
At time: 96.45584058761597 and batch: 700, loss is 3.654343662261963 and perplexity is 38.642150488577755
At time: 96.95610284805298 and batch: 750, loss is 3.6085771894454957 and perplexity is 36.913494523831325
At time: 97.45566010475159 and batch: 800, loss is 3.7274463415145873 and perplexity is 41.572809738505796
At time: 97.95615577697754 and batch: 850, loss is 3.589821400642395 and perplexity is 36.22760512165058
At time: 98.45587062835693 and batch: 900, loss is 3.534554777145386 and perplexity is 34.2797491884114
At time: 98.95471501350403 and batch: 950, loss is 3.573265266418457 and perplexity is 35.63275383512557
At time: 99.45379853248596 and batch: 1000, loss is 3.471662817001343 and perplexity is 32.190224419503544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.464810999428353 and perplexity of 86.90460296784589
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 100.96077871322632 and batch: 50, loss is 3.758178782463074 and perplexity is 42.87027873535232
At time: 101.46608400344849 and batch: 100, loss is 3.6461492919921876 and perplexity is 38.32679622884673
At time: 101.9661557674408 and batch: 150, loss is 3.740574851036072 and perplexity is 42.122197195311415
At time: 102.46596121788025 and batch: 200, loss is 3.7733506298065187 and perplexity is 43.52565915421853
At time: 102.96695137023926 and batch: 250, loss is 3.7654434728622435 and perplexity is 43.182852037133955
At time: 103.46648740768433 and batch: 300, loss is 3.5896027135849 and perplexity is 36.21968347949831
At time: 103.96972346305847 and batch: 350, loss is 3.627831039428711 and perplexity is 37.63110764870487
At time: 104.47643852233887 and batch: 400, loss is 3.6372758102416993 and perplexity is 37.988208549411866
At time: 104.9880063533783 and batch: 450, loss is 3.6800527238845824 and perplexity is 39.648484439583434
At time: 105.48737454414368 and batch: 500, loss is 3.7039130687713624 and perplexity is 40.60588751468609
At time: 105.98660063743591 and batch: 550, loss is 3.5924773025512695 and perplexity is 36.32394997184968
At time: 106.48462724685669 and batch: 600, loss is 3.5361023473739626 and perplexity is 34.33284057842972
At time: 107.00456428527832 and batch: 650, loss is 3.505779366493225 and perplexity is 33.307392404991226
At time: 107.50263714790344 and batch: 700, loss is 3.5833043384552004 and perplexity is 35.99227522886485
At time: 108.00322699546814 and batch: 750, loss is 3.535656061172485 and perplexity is 34.31752172397298
At time: 108.51493954658508 and batch: 800, loss is 3.643840208053589 and perplexity is 38.23839853737862
At time: 109.01485133171082 and batch: 850, loss is 3.5018419218063355 and perplexity is 33.17650424128612
At time: 109.51600241661072 and batch: 900, loss is 3.445145330429077 and perplexity is 31.347838910517112
At time: 110.01620984077454 and batch: 950, loss is 3.4731928873062134 and perplexity is 32.239515425731085
At time: 110.51726341247559 and batch: 1000, loss is 3.3708580684661866 and perplexity is 29.103489132961705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.44834862685785 and perplexity of 85.48565865615139
Finished 10 epochs...
Completing Train Step...
At time: 112.01030325889587 and batch: 50, loss is 3.728599524497986 and perplexity is 41.620778448302396
At time: 112.54315519332886 and batch: 100, loss is 3.610791721343994 and perplexity is 36.99533121647761
At time: 113.0601315498352 and batch: 150, loss is 3.706351399421692 and perplexity is 40.70501890323343
At time: 113.57942128181458 and batch: 200, loss is 3.7415660333633425 and perplexity is 42.16396867091157
At time: 114.08636498451233 and batch: 250, loss is 3.7344211864471437 and perplexity is 41.863787221621386
At time: 114.59319972991943 and batch: 300, loss is 3.558740568161011 and perplexity is 35.11893937073652
At time: 115.09817266464233 and batch: 350, loss is 3.599670467376709 and perplexity is 36.58617611838885
At time: 115.60422992706299 and batch: 400, loss is 3.609938802719116 and perplexity is 36.96379066212474
At time: 116.11223793029785 and batch: 450, loss is 3.655770468711853 and perplexity is 38.69732471023995
At time: 116.63239598274231 and batch: 500, loss is 3.6805749845504763 and perplexity is 39.66919669159502
At time: 117.1430253982544 and batch: 550, loss is 3.5718034219741823 and perplexity is 35.580702346756475
At time: 117.64631581306458 and batch: 600, loss is 3.5177312231063844 and perplexity is 33.7078660175824
At time: 118.15070652961731 and batch: 650, loss is 3.4910692834854125 and perplexity is 32.821023928538295
At time: 118.65526795387268 and batch: 700, loss is 3.5709997844696044 and perplexity is 35.55211984643431
At time: 119.1593713760376 and batch: 750, loss is 3.5244981956481936 and perplexity is 33.93673973908094
At time: 119.670823097229 and batch: 800, loss is 3.637152452468872 and perplexity is 37.98352269763557
At time: 120.21698069572449 and batch: 850, loss is 3.497536425590515 and perplexity is 33.03396998834769
At time: 120.7228467464447 and batch: 900, loss is 3.443792324066162 and perplexity is 31.305453765159175
At time: 121.22945427894592 and batch: 950, loss is 3.473995304107666 and perplexity is 32.26539533642835
At time: 121.73491334915161 and batch: 1000, loss is 3.375243806838989 and perplexity is 29.231409730127606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.448949674280678 and perplexity of 85.53705503525875
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 123.22717428207397 and batch: 50, loss is 3.7160219717025758 and perplexity is 41.10056924717799
At time: 123.7423152923584 and batch: 100, loss is 3.600888395309448 and perplexity is 36.63076259027888
At time: 124.2444326877594 and batch: 150, loss is 3.6958579921722414 and perplexity is 40.28011779185755
At time: 124.74732804298401 and batch: 200, loss is 3.730153980255127 and perplexity is 41.68552641786735
At time: 125.2496612071991 and batch: 250, loss is 3.723653883934021 and perplexity is 41.415445208830285
At time: 125.75163388252258 and batch: 300, loss is 3.5447272300720214 and perplexity is 34.63023796632344
At time: 126.25200009346008 and batch: 350, loss is 3.5873430299758913 and perplexity is 36.13793085673393
At time: 126.75248599052429 and batch: 400, loss is 3.597577829360962 and perplexity is 36.50969454742439
At time: 127.27065753936768 and batch: 450, loss is 3.6420300960540772 and perplexity is 38.169245359724066
At time: 127.77893352508545 and batch: 500, loss is 3.666078414916992 and perplexity is 39.09827760183166
At time: 128.27892780303955 and batch: 550, loss is 3.5559712266921997 and perplexity is 35.0218175793696
At time: 128.7840416431427 and batch: 600, loss is 3.4991220474243163 and perplexity is 33.08639092132324
At time: 129.28780508041382 and batch: 650, loss is 3.469999222755432 and perplexity is 32.13671746665436
At time: 129.78908467292786 and batch: 700, loss is 3.5465067720413206 and perplexity is 34.691918793724454
At time: 130.2921359539032 and batch: 750, loss is 3.500682020187378 and perplexity is 33.13804506904519
At time: 130.7930145263672 and batch: 800, loss is 3.6099363422393798 and perplexity is 36.96369971357872
At time: 131.2935016155243 and batch: 850, loss is 3.4692276668548585 and perplexity is 32.11193175567825
At time: 131.79599404335022 and batch: 900, loss is 3.4125730657577513 and perplexity is 30.343218997097296
At time: 132.2997498512268 and batch: 950, loss is 3.442310638427734 and perplexity is 31.25910327082156
At time: 132.82420349121094 and batch: 1000, loss is 3.3412552642822266 and perplexity is 28.25457140881848
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.445438664133956 and perplexity of 85.23726016649132
Finished 12 epochs...
Completing Train Step...
At time: 134.32603693008423 and batch: 50, loss is 3.7075814723968508 and perplexity is 40.755119854535515
At time: 134.83909845352173 and batch: 100, loss is 3.590508985519409 and perplexity is 36.25252324074187
At time: 135.35192203521729 and batch: 150, loss is 3.686378960609436 and perplexity is 39.90010520501206
At time: 135.85205483436584 and batch: 200, loss is 3.7210062837600706 and perplexity is 41.30593869758835
At time: 136.3540735244751 and batch: 250, loss is 3.7140518760681154 and perplexity is 41.01967690412004
At time: 136.8551058769226 and batch: 300, loss is 3.5353114080429076 and perplexity is 34.305696120694115
At time: 137.35687804222107 and batch: 350, loss is 3.578385615348816 and perplexity is 35.81567387567876
At time: 137.85707688331604 and batch: 400, loss is 3.5889742183685303 and perplexity is 36.19692673369505
At time: 138.3576636314392 and batch: 450, loss is 3.6344279766082765 and perplexity is 37.88017835046018
At time: 138.85807371139526 and batch: 500, loss is 3.6589320135116576 and perplexity is 38.81986163686878
At time: 139.36034393310547 and batch: 550, loss is 3.5495389986038206 and perplexity is 34.797272198369136
At time: 139.87967538833618 and batch: 600, loss is 3.4935254096984862 and perplexity is 32.90173558418087
At time: 140.38580203056335 and batch: 650, loss is 3.4661795139312743 and perplexity is 32.014198705328546
At time: 140.88719511032104 and batch: 700, loss is 3.5438280868530274 and perplexity is 34.59911441704486
At time: 141.38949275016785 and batch: 750, loss is 3.4990761423110963 and perplexity is 33.08487212166256
At time: 141.89013862609863 and batch: 800, loss is 3.609092378616333 and perplexity is 36.93251685609682
At time: 142.39007687568665 and batch: 850, loss is 3.4695499610900877 and perplexity is 32.12228291413493
At time: 142.90656280517578 and batch: 900, loss is 3.4140255784988405 and perplexity is 30.387324933759185
At time: 143.41292262077332 and batch: 950, loss is 3.4444733619689942 and perplexity is 31.32678122731933
At time: 143.91589498519897 and batch: 1000, loss is 3.3446518278121946 and perplexity is 28.350703021766194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.445414473370808 and perplexity of 85.23519823705917
Finished 13 epochs...
Completing Train Step...
At time: 145.39119863510132 and batch: 50, loss is 3.7021487045288084 and perplexity is 40.53430710424537
At time: 145.91516280174255 and batch: 100, loss is 3.584345607757568 and perplexity is 36.02977239912119
At time: 146.41691374778748 and batch: 150, loss is 3.6802304124832155 and perplexity is 39.65553014917403
At time: 146.9176824092865 and batch: 200, loss is 3.714859890937805 and perplexity is 41.052834807243784
At time: 147.41939902305603 and batch: 250, loss is 3.707876043319702 and perplexity is 40.76712689617783
At time: 147.9206199645996 and batch: 300, loss is 3.529200162887573 and perplexity is 34.09668491134667
At time: 148.42379879951477 and batch: 350, loss is 3.5724985551834108 and perplexity is 35.60544427303639
At time: 148.93408274650574 and batch: 400, loss is 3.583360333442688 and perplexity is 35.99429067229278
At time: 149.43965005874634 and batch: 450, loss is 3.62941782951355 and perplexity is 37.69086771800689
At time: 149.94103932380676 and batch: 500, loss is 3.6541593503952026 and perplexity is 38.635028937998875
At time: 150.44181513786316 and batch: 550, loss is 3.545115842819214 and perplexity is 34.643698333501206
At time: 150.9444408416748 and batch: 600, loss is 3.4896162939071655 and perplexity is 32.77336995157252
At time: 151.4465892314911 and batch: 650, loss is 3.4632890701293944 and perplexity is 31.921797068310134
At time: 151.94976329803467 and batch: 700, loss is 3.541610140800476 and perplexity is 34.522460486366825
At time: 152.461660861969 and batch: 750, loss is 3.497474780082703 and perplexity is 33.03193365525875
At time: 152.96309804916382 and batch: 800, loss is 3.608102140426636 and perplexity is 36.89596296897875
At time: 153.47620677947998 and batch: 850, loss is 3.469225459098816 and perplexity is 32.11186086044514
At time: 153.99491000175476 and batch: 900, loss is 3.4142740154266358 and perplexity is 30.39487520525394
At time: 154.50134706497192 and batch: 950, loss is 3.445248689651489 and perplexity is 31.35107916622347
At time: 155.00086545944214 and batch: 1000, loss is 3.346138858795166 and perplexity is 28.392892756495556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.44573974609375 and perplexity of 85.26292743161106
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 156.47308492660522 and batch: 50, loss is 3.698613791465759 and perplexity is 40.391274804872296
At time: 156.98816680908203 and batch: 100, loss is 3.581972141265869 and perplexity is 35.94435834542274
At time: 157.4895281791687 and batch: 150, loss is 3.677928328514099 and perplexity is 39.564344787381025
At time: 157.99334454536438 and batch: 200, loss is 3.711814618110657 and perplexity is 40.92800788740855
At time: 158.49515461921692 and batch: 250, loss is 3.7047780656814577 and perplexity is 40.64102667736056
At time: 159.01165795326233 and batch: 300, loss is 3.5246327400207518 and perplexity is 33.94130604361421
At time: 159.51589679718018 and batch: 350, loss is 3.568292670249939 and perplexity is 35.45600635106084
At time: 160.03580832481384 and batch: 400, loss is 3.5788886404037474 and perplexity is 35.83369458905267
At time: 160.5462200641632 and batch: 450, loss is 3.6257091856002805 and perplexity is 37.55134459157265
At time: 161.05001258850098 and batch: 500, loss is 3.649536900520325 and perplexity is 38.4568525762275
At time: 161.55544018745422 and batch: 550, loss is 3.540541787147522 and perplexity is 34.48559798419785
At time: 162.07198762893677 and batch: 600, loss is 3.4843086767196656 and perplexity is 32.59988226032494
At time: 162.5956768989563 and batch: 650, loss is 3.456495456695557 and perplexity is 31.70566769985933
At time: 163.1145477294922 and batch: 700, loss is 3.5342968320846557 and perplexity is 34.270908036739904
At time: 163.62706971168518 and batch: 750, loss is 3.4890535926818846 and perplexity is 32.75493352373092
At time: 164.15676307678223 and batch: 800, loss is 3.599475173950195 and perplexity is 36.579031776335874
At time: 164.6780982017517 and batch: 850, loss is 3.4602677154541017 and perplexity is 31.825495551271455
At time: 165.18583226203918 and batch: 900, loss is 3.4040718841552735 and perplexity is 30.086359135104367
At time: 165.69248700141907 and batch: 950, loss is 3.4350733041763304 and perplexity is 31.033687381042277
At time: 166.19580149650574 and batch: 1000, loss is 3.3351305103302002 and perplexity is 28.08204798200628
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.445196012171303 and perplexity of 85.21657968720169
Finished 15 epochs...
Completing Train Step...
At time: 167.6925220489502 and batch: 50, loss is 3.696559591293335 and perplexity is 40.30838820318608
At time: 168.2144012451172 and batch: 100, loss is 3.579561376571655 and perplexity is 35.85780932194951
At time: 168.71751141548157 and batch: 150, loss is 3.67579948425293 and perplexity is 39.480208047816554
At time: 169.22707796096802 and batch: 200, loss is 3.709831085205078 and perplexity is 40.84690629743081
At time: 169.73803186416626 and batch: 250, loss is 3.702684984207153 and perplexity is 40.55605065921338
At time: 170.2634973526001 and batch: 300, loss is 3.52270423412323 and perplexity is 33.875913110335354
At time: 170.77292943000793 and batch: 350, loss is 3.5664027404785155 and perplexity is 35.38906027070405
At time: 171.27265620231628 and batch: 400, loss is 3.5770629405975343 and perplexity is 35.76833270353164
At time: 171.79473233222961 and batch: 450, loss is 3.6238933801651 and perplexity is 37.483220524708315
At time: 172.29691529273987 and batch: 500, loss is 3.6479751300811767 and perplexity is 38.39683867685671
At time: 172.79797840118408 and batch: 550, loss is 3.5390573740005493 and perplexity is 34.43444508458012
At time: 173.29789447784424 and batch: 600, loss is 3.4831712293624877 and perplexity is 32.56282269105378
At time: 173.8061752319336 and batch: 650, loss is 3.4557179021835327 and perplexity is 31.681024396879874
At time: 174.31667065620422 and batch: 700, loss is 3.5337024879455567 and perplexity is 34.250545375220355
At time: 174.81717729568481 and batch: 750, loss is 3.4888720178604125 and perplexity is 32.74898659244685
At time: 175.316832780838 and batch: 800, loss is 3.5993679332733155 and perplexity is 36.57510922654078
At time: 175.816477060318 and batch: 850, loss is 3.4605076837539674 and perplexity is 31.83313357773674
At time: 176.31783080101013 and batch: 900, loss is 3.404570689201355 and perplexity is 30.10137010632204
At time: 176.81792736053467 and batch: 950, loss is 3.435677604675293 and perplexity is 31.052446721379777
At time: 177.31790471076965 and batch: 1000, loss is 3.3360318994522093 and perplexity is 28.107372246374542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.445161772937309 and perplexity of 85.21366198673982
Finished 16 epochs...
Completing Train Step...
At time: 178.80591797828674 and batch: 50, loss is 3.6949676418304445 and perplexity is 40.244270335979344
At time: 179.32058572769165 and batch: 100, loss is 3.5778104162216184 and perplexity is 35.79507865506978
At time: 179.82224798202515 and batch: 150, loss is 3.674102573394775 and perplexity is 39.41327046371692
At time: 180.32443690299988 and batch: 200, loss is 3.7081661653518676 and perplexity is 40.778956053745134
At time: 180.8252410888672 and batch: 250, loss is 3.7009876918792726 and perplexity is 40.48727356950481
At time: 181.34632468223572 and batch: 300, loss is 3.521066846847534 and perplexity is 33.82049050777052
At time: 181.8538978099823 and batch: 350, loss is 3.5648391485214233 and perplexity is 35.33376945809535
At time: 182.35375022888184 and batch: 400, loss is 3.575571451187134 and perplexity is 35.7150243783601
At time: 182.85484433174133 and batch: 450, loss is 3.6224599742889403 and perplexity is 37.42953034525305
At time: 183.35714149475098 and batch: 500, loss is 3.6466752004623415 and perplexity is 38.34695791675387
At time: 183.85815405845642 and batch: 550, loss is 3.537857313156128 and perplexity is 34.39314644073378
At time: 184.3590919971466 and batch: 600, loss is 3.4821866941452027 and perplexity is 32.530779221890505
At time: 184.8733470439911 and batch: 650, loss is 3.455033540725708 and perplexity is 31.6593505420596
At time: 185.37410926818848 and batch: 700, loss is 3.5331904220581056 and perplexity is 34.233011328983714
At time: 185.87472820281982 and batch: 750, loss is 3.488626356124878 and perplexity is 32.740942407678205
At time: 186.3748700618744 and batch: 800, loss is 3.599234685897827 and perplexity is 36.57023601390675
At time: 186.87692952156067 and batch: 850, loss is 3.460621256828308 and perplexity is 31.83674916989613
At time: 187.37688994407654 and batch: 900, loss is 3.4048767995834353 and perplexity is 30.110585858673193
At time: 187.87815189361572 and batch: 950, loss is 3.436093955039978 and perplexity is 31.06537811070411
At time: 188.39876174926758 and batch: 1000, loss is 3.3366530036926267 and perplexity is 28.124835277083573
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.445218714272103 and perplexity of 85.21851430454345
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 189.8913471698761 and batch: 50, loss is 3.6938607406616213 and perplexity is 40.19974855125957
At time: 190.42022705078125 and batch: 100, loss is 3.5771022701263426 and perplexity is 35.76973948286693
At time: 190.94062519073486 and batch: 150, loss is 3.6735563135147093 and perplexity is 39.391746454706926
At time: 191.46146726608276 and batch: 200, loss is 3.7072032356262206 and perplexity is 40.73970768452354
At time: 191.9681215286255 and batch: 250, loss is 3.7001354455947877 and perplexity is 40.452783140293946
At time: 192.4726881980896 and batch: 300, loss is 3.5195999050140383 and perplexity is 33.770914187095045
At time: 192.97720646858215 and batch: 350, loss is 3.563584532737732 and perplexity is 35.28946695035987
At time: 193.4821813106537 and batch: 400, loss is 3.5741996288299562 and perplexity is 35.666063300059946
At time: 193.9957389831543 and batch: 450, loss is 3.6212129688262937 and perplexity is 37.382884606238505
At time: 194.51343441009521 and batch: 500, loss is 3.645189094543457 and perplexity is 38.29001259948945
At time: 195.02217364311218 and batch: 550, loss is 3.5363319635391237 and perplexity is 34.34072485876689
At time: 195.52589750289917 and batch: 600, loss is 3.4806574964523316 and perplexity is 32.48107124570495
At time: 196.03024864196777 and batch: 650, loss is 3.45314781665802 and perplexity is 31.599705997026543
At time: 196.5351631641388 and batch: 700, loss is 3.5311321878433226 and perplexity is 34.16262423520226
At time: 197.041198015213 and batch: 750, loss is 3.4862892198562623 and perplexity is 32.66451171297289
At time: 197.5649495124817 and batch: 800, loss is 3.5969020414352415 and perplexity is 36.48503007159365
At time: 198.07037544250488 and batch: 850, loss is 3.4580318784713744 and perplexity is 31.75441841937371
At time: 198.59262013435364 and batch: 900, loss is 3.4020031785964964 and perplexity is 30.0241836502773
At time: 199.11154437065125 and batch: 950, loss is 3.4332642555236816 and perplexity is 30.977596681398325
At time: 199.62369799613953 and batch: 1000, loss is 3.333581109046936 and perplexity is 28.038571310928923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.445171821408156 and perplexity of 85.21451825804013
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 201.14049458503723 and batch: 50, loss is 3.6934947109222414 and perplexity is 40.18503694038201
At time: 201.64772415161133 and batch: 100, loss is 3.576759853363037 and perplexity is 35.75749342119698
At time: 202.15838885307312 and batch: 150, loss is 3.6732609558105467 and perplexity is 39.38011351693457
At time: 202.6760413646698 and batch: 200, loss is 3.706935691833496 and perplexity is 40.72880948655274
At time: 203.18792867660522 and batch: 250, loss is 3.699875774383545 and perplexity is 40.442280080827864
At time: 203.71449279785156 and batch: 300, loss is 3.5191372060775756 and perplexity is 33.755292035472486
At time: 204.23013973236084 and batch: 350, loss is 3.5631760025024413 and perplexity is 35.275053080574644
At time: 204.74488997459412 and batch: 400, loss is 3.573751697540283 and perplexity is 35.650090931857186
At time: 205.24940538406372 and batch: 450, loss is 3.6208589458465577 and perplexity is 37.36965254840354
At time: 205.75166702270508 and batch: 500, loss is 3.6447821617126466 and perplexity is 38.274434306145054
At time: 206.25470352172852 and batch: 550, loss is 3.535872931480408 and perplexity is 34.324964982558114
At time: 206.77092266082764 and batch: 600, loss is 3.480217251777649 and perplexity is 32.466774774264664
At time: 207.29007482528687 and batch: 650, loss is 3.4526308965682984 and perplexity is 31.583375695261545
At time: 207.79111695289612 and batch: 700, loss is 3.5306017112731936 and perplexity is 34.14450656939484
At time: 208.31096363067627 and batch: 750, loss is 3.485684704780579 and perplexity is 32.64477149043477
At time: 208.82605957984924 and batch: 800, loss is 3.5963161468505858 and perplexity is 36.46365995098398
At time: 209.33023381233215 and batch: 850, loss is 3.457361168861389 and perplexity is 31.733127566568275
At time: 209.85592579841614 and batch: 900, loss is 3.4012571382522583 and perplexity is 30.001792751266905
At time: 210.37379837036133 and batch: 950, loss is 3.432510724067688 and perplexity is 30.95426288034845
At time: 210.91858005523682 and batch: 1000, loss is 3.3327925300598142 and perplexity is 28.01646939846143
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.445160284274962 and perplexity of 85.21353513246419
Finished 19 epochs...
Completing Train Step...
At time: 212.4664523601532 and batch: 50, loss is 3.6933664798736574 and perplexity is 40.179884301328975
At time: 212.99189400672913 and batch: 100, loss is 3.5765880870819093 and perplexity is 35.75135201698775
At time: 213.50997066497803 and batch: 150, loss is 3.6730950927734374 and perplexity is 39.373582353359225
At time: 214.01666140556335 and batch: 200, loss is 3.7068038463592528 and perplexity is 40.723439931334184
At time: 214.51766443252563 and batch: 250, loss is 3.6997244787216186 and perplexity is 40.43616180213941
At time: 215.02992630004883 and batch: 300, loss is 3.519024167060852 and perplexity is 33.751476586102974
At time: 215.5339958667755 and batch: 350, loss is 3.563062710762024 and perplexity is 35.271056934787254
At time: 216.0480432510376 and batch: 400, loss is 3.5736397218704226 and perplexity is 35.6460992125364
At time: 216.563560962677 and batch: 450, loss is 3.620741729736328 and perplexity is 37.36527247980351
At time: 217.08110523223877 and batch: 500, loss is 3.644693207740784 and perplexity is 38.271029794617384
At time: 217.5833501815796 and batch: 550, loss is 3.53578839302063 and perplexity is 34.3220633255391
At time: 218.0851457118988 and batch: 600, loss is 3.4801529693603515 and perplexity is 32.46468779857896
At time: 218.58690476417542 and batch: 650, loss is 3.452592620849609 and perplexity is 31.58216684199319
At time: 219.0876522064209 and batch: 700, loss is 3.5305574131011963 and perplexity is 34.14299406367093
At time: 219.58894276618958 and batch: 750, loss is 3.485696439743042 and perplexity is 32.64515457785058
At time: 220.08893585205078 and batch: 800, loss is 3.59631751537323 and perplexity is 36.463709852362456
At time: 220.5899670124054 and batch: 850, loss is 3.457383937835693 and perplexity is 31.733850105560133
At time: 221.0908362865448 and batch: 900, loss is 3.401301746368408 and perplexity is 30.003131104573143
At time: 221.5928771495819 and batch: 950, loss is 3.4325510025024415 and perplexity is 30.955509694715907
At time: 222.09275269508362 and batch: 1000, loss is 3.3328532648086546 and perplexity is 28.0181710233671
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.445155073956745 and perplexity of 85.21309114398642
Finished 20 epochs...
Completing Train Step...
At time: 223.57837867736816 and batch: 50, loss is 3.693245391845703 and perplexity is 40.175019292928575
At time: 224.09578204154968 and batch: 100, loss is 3.576437692642212 and perplexity is 35.74597561673322
At time: 224.59735321998596 and batch: 150, loss is 3.6729463481903077 and perplexity is 39.36772618181345
At time: 225.099116563797 and batch: 200, loss is 3.706675658226013 and perplexity is 40.71822000416379
At time: 225.61398077011108 and batch: 250, loss is 3.6995880365371705 and perplexity is 40.4306449802646
At time: 226.11690282821655 and batch: 300, loss is 3.518910036087036 and perplexity is 33.74762471702543
At time: 226.6165463924408 and batch: 350, loss is 3.5629492139816286 and perplexity is 35.267054010547824
At time: 227.118803024292 and batch: 400, loss is 3.5735303783416748 and perplexity is 35.64220175534715
At time: 227.61990904808044 and batch: 450, loss is 3.6206318616867064 and perplexity is 37.361167455702194
At time: 228.13437724113464 and batch: 500, loss is 3.6446049213409424 and perplexity is 38.267651132325746
At time: 228.66016387939453 and batch: 550, loss is 3.5357051038742067 and perplexity is 34.3192047892254
At time: 229.17983031272888 and batch: 600, loss is 3.4800883960723876 and perplexity is 32.46259151462779
At time: 229.69924902915955 and batch: 650, loss is 3.452550754547119 and perplexity is 31.580844641120912
At time: 230.21350169181824 and batch: 700, loss is 3.5305179071426394 and perplexity is 34.14164523860593
At time: 230.7150537967682 and batch: 750, loss is 3.485702199935913 and perplexity is 32.64534262077884
At time: 231.2165002822876 and batch: 800, loss is 3.5963183736801145 and perplexity is 36.4637411494291
At time: 231.71682500839233 and batch: 850, loss is 3.4574055528640746 and perplexity is 31.734536041044045
At time: 232.2197937965393 and batch: 900, loss is 3.4013423681259156 and perplexity is 30.004349909244166
At time: 232.7379448413849 and batch: 950, loss is 3.432591037750244 and perplexity is 30.956749031025797
At time: 233.2488250732422 and batch: 1000, loss is 3.332911729812622 and perplexity is 28.019809153733323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.445149863638529 and perplexity of 85.21264715782205
Finished 21 epochs...
Completing Train Step...
At time: 234.7380816936493 and batch: 50, loss is 3.693128581047058 and perplexity is 40.170326690918024
At time: 235.2401955127716 and batch: 100, loss is 3.5762991285324097 and perplexity is 35.741022850588614
At time: 235.7423756122589 and batch: 150, loss is 3.672807331085205 and perplexity is 39.36225377487308
At time: 236.24321699142456 and batch: 200, loss is 3.7065503931045534 and perplexity is 40.71311975083795
At time: 236.7591426372528 and batch: 250, loss is 3.699459776878357 and perplexity is 40.42545969207257
At time: 237.2611689567566 and batch: 300, loss is 3.5187960481643676 and perplexity is 33.74377811462622
At time: 237.76388835906982 and batch: 350, loss is 3.5628367280960083 and perplexity is 35.26308718785419
At time: 238.26555514335632 and batch: 400, loss is 3.5734233140945433 and perplexity is 35.638385954121695
At time: 238.76852560043335 and batch: 450, loss is 3.620526385307312 and perplexity is 37.35722694284819
At time: 239.2758183479309 and batch: 500, loss is 3.6445174503326414 and perplexity is 38.26430396868792
At time: 239.7810606956482 and batch: 550, loss is 3.535622754096985 and perplexity is 34.31637872672099
At time: 240.2839379310608 and batch: 600, loss is 3.4800235748291017 and perplexity is 32.46048731728461
At time: 240.79073023796082 and batch: 650, loss is 3.452507381439209 and perplexity is 31.57947491144333
At time: 241.29639506340027 and batch: 700, loss is 3.5304813051223753 and perplexity is 34.1403956082847
At time: 241.80233240127563 and batch: 750, loss is 3.485703778266907 and perplexity is 32.645394145975565
At time: 242.30832529067993 and batch: 800, loss is 3.596318120956421 and perplexity is 36.46373193417892
At time: 242.81370973587036 and batch: 850, loss is 3.4574259376525878 and perplexity is 31.73518294944333
At time: 243.32652521133423 and batch: 900, loss is 3.401379828453064 and perplexity is 30.005473903060093
At time: 243.85065031051636 and batch: 950, loss is 3.4326298570632936 and perplexity is 30.957950774082693
At time: 244.37445759773254 and batch: 1000, loss is 3.332968006134033 and perplexity is 28.021386049889674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.445147630645008 and perplexity of 85.21245687874543
Finished 22 epochs...
Completing Train Step...
At time: 245.88302850723267 and batch: 50, loss is 3.69301477432251 and perplexity is 40.165755297745875
At time: 246.3972029685974 and batch: 100, loss is 3.5761676597595216 and perplexity is 35.73632433103375
At time: 246.89810299873352 and batch: 150, loss is 3.672674503326416 and perplexity is 39.35702572214622
At time: 247.3989646434784 and batch: 200, loss is 3.7064275550842285 and perplexity is 40.70811893895771
At time: 247.89877343177795 and batch: 250, loss is 3.699337134361267 and perplexity is 40.420502115952445
At time: 248.4001498222351 and batch: 300, loss is 3.5186828184127807 and perplexity is 33.73995753131863
At time: 248.9008128643036 and batch: 350, loss is 3.562725887298584 and perplexity is 35.25917881575818
At time: 249.4116542339325 and batch: 400, loss is 3.573318500518799 and perplexity is 35.63465076320886
At time: 249.937255859375 and batch: 450, loss is 3.6204239320755005 and perplexity is 37.35339977027282
At time: 250.43669056892395 and batch: 500, loss is 3.644430847167969 and perplexity is 38.2609903023593
At time: 250.93688082695007 and batch: 550, loss is 3.535541310310364 and perplexity is 34.3135839847031
At time: 251.43607473373413 and batch: 600, loss is 3.4799589776992796 and perplexity is 32.45839053069521
At time: 251.93768072128296 and batch: 650, loss is 3.4524633169174193 and perplexity is 31.578083407641188
At time: 252.45149326324463 and batch: 700, loss is 3.5304462623596193 and perplexity is 34.13919925546288
At time: 252.96288299560547 and batch: 750, loss is 3.4857023572921753 and perplexity is 32.64534775772834
At time: 253.48297119140625 and batch: 800, loss is 3.5963170337677 and perplexity is 36.46369229124239
At time: 254.00327277183533 and batch: 850, loss is 3.457444887161255 and perplexity is 31.735784321265513
At time: 254.51408743858337 and batch: 900, loss is 3.401414966583252 and perplexity is 30.006528257832375
At time: 255.02366948127747 and batch: 950, loss is 3.4326670122146608 and perplexity is 30.959101042798792
At time: 255.53637886047363 and batch: 1000, loss is 3.3330221462249754 and perplexity is 28.022903171346922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.445146514148247 and perplexity of 85.21236173936646
Finished 23 epochs...
Completing Train Step...
At time: 257.0294575691223 and batch: 50, loss is 3.6929030275344847 and perplexity is 40.16126715437522
At time: 257.54394364356995 and batch: 100, loss is 3.5760410690307616 and perplexity is 35.73180073002249
At time: 258.0580241680145 and batch: 150, loss is 3.672546133995056 and perplexity is 39.3519738113321
At time: 258.56885528564453 and batch: 200, loss is 3.706306886672974 and perplexity is 40.70320705128098
At time: 259.0713920593262 and batch: 250, loss is 3.6992177820205687 and perplexity is 40.4156781222959
At time: 259.5739212036133 and batch: 300, loss is 3.518570704460144 and perplexity is 33.736175023357916
At time: 260.07408142089844 and batch: 350, loss is 3.5626168346405027 and perplexity is 35.25533391823853
At time: 260.5751247406006 and batch: 400, loss is 3.5732156229019165 and perplexity is 35.63098494382852
At time: 261.0771405696869 and batch: 450, loss is 3.620323939323425 and perplexity is 37.34966488776413
At time: 261.5831034183502 and batch: 500, loss is 3.644345245361328 and perplexity is 38.2577152326435
At time: 262.1010432243347 and batch: 550, loss is 3.535460801124573 and perplexity is 34.31082153719761
At time: 262.6293566226959 and batch: 600, loss is 3.479894642829895 and perplexity is 32.45630239155078
At time: 263.1315083503723 and batch: 650, loss is 3.4524191665649413 and perplexity is 31.576689254904558
At time: 263.63306617736816 and batch: 700, loss is 3.530412263870239 and perplexity is 34.138038593990025
At time: 264.1359758377075 and batch: 750, loss is 3.4856989860534666 and perplexity is 32.64523770265383
At time: 264.6361346244812 and batch: 800, loss is 3.5963150691986083 and perplexity is 36.4636206558699
At time: 265.13788390159607 and batch: 850, loss is 3.457462844848633 and perplexity is 31.736354227676138
At time: 265.63991832733154 and batch: 900, loss is 3.4014478874206544 and perplexity is 30.007516114130603
At time: 266.1391899585724 and batch: 950, loss is 3.4327027940750123 and perplexity is 30.960208836848263
At time: 266.641086101532 and batch: 1000, loss is 3.33307457447052 and perplexity is 28.024372401509503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.445145769817073 and perplexity of 85.21229831317284
Finished 24 epochs...
Completing Train Step...
At time: 268.17941641807556 and batch: 50, loss is 3.6927930164337157 and perplexity is 40.15684921218308
At time: 268.7064847946167 and batch: 100, loss is 3.575917887687683 and perplexity is 35.72739950989762
At time: 269.221244096756 and batch: 150, loss is 3.672421112060547 and perplexity is 39.347054258971845
At time: 269.7389280796051 and batch: 200, loss is 3.70618812084198 and perplexity is 40.69837318812599
At time: 270.24472880363464 and batch: 250, loss is 3.6991012573242186 and perplexity is 40.4109689720464
At time: 270.76295161247253 and batch: 300, loss is 3.518459749221802 and perplexity is 33.73243202567389
At time: 271.2720642089844 and batch: 350, loss is 3.562509412765503 and perplexity is 35.25154692757175
At time: 271.8011541366577 and batch: 400, loss is 3.5731144523620606 and perplexity is 35.62738032019009
At time: 272.31792855262756 and batch: 450, loss is 3.620225682258606 and perplexity is 37.345995199609625
At time: 272.8263649940491 and batch: 500, loss is 3.6442602920532225 and perplexity is 38.25446525122422
At time: 273.3460338115692 and batch: 550, loss is 3.5353812265396116 and perplexity is 34.308091376441254
At time: 273.8615155220032 and batch: 600, loss is 3.4798305940628054 and perplexity is 32.45422367196874
At time: 274.36812710762024 and batch: 650, loss is 3.452375259399414 and perplexity is 31.57530284241958
At time: 274.871666431427 and batch: 700, loss is 3.5303791093826296 and perplexity is 34.136906783574844
At time: 275.4011471271515 and batch: 750, loss is 3.4856937313079834 and perplexity is 32.64506616068917
At time: 275.914922952652 and batch: 800, loss is 3.5963124418258667 and perplexity is 36.46352485247279
At time: 276.41925978660583 and batch: 850, loss is 3.457479600906372 and perplexity is 31.736886008315263
At time: 276.92433047294617 and batch: 900, loss is 3.401479034423828 and perplexity is 30.00845077288607
At time: 277.4383988380432 and batch: 950, loss is 3.432737097740173 and perplexity is 30.961270903701802
At time: 277.9481694698334 and batch: 1000, loss is 3.3331252574920653 and perplexity is 28.0257927973744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.445146514148247 and perplexity of 85.21236173936646
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 279.4469232559204 and batch: 50, loss is 3.6927071619033813 and perplexity is 40.153401712748106
At time: 279.9610583782196 and batch: 100, loss is 3.5758543539047243 and perplexity is 35.72512968515752
At time: 280.4620351791382 and batch: 150, loss is 3.6723749685287475 and perplexity is 39.34523868881116
At time: 280.9627876281738 and batch: 200, loss is 3.7061224603652954 and perplexity is 40.69570100127166
At time: 281.46315360069275 and batch: 250, loss is 3.6990556621551516 and perplexity is 40.40912646908889
At time: 281.9734046459198 and batch: 300, loss is 3.518340916633606 and perplexity is 33.728423751631645
At time: 282.4906167984009 and batch: 350, loss is 3.5624016380310057 and perplexity is 35.24774790618395
At time: 282.9939568042755 and batch: 400, loss is 3.572998547554016 and perplexity is 35.62325117481147
At time: 283.49707317352295 and batch: 450, loss is 3.620141978263855 and perplexity is 37.342869321449534
At time: 283.999142408371 and batch: 500, loss is 3.644150428771973 and perplexity is 38.25026272100537
At time: 284.5007996559143 and batch: 550, loss is 3.53525990486145 and perplexity is 34.303929313700266
At time: 285.0021650791168 and batch: 600, loss is 3.4797116184234618 and perplexity is 32.45036263964692
At time: 285.5112988948822 and batch: 650, loss is 3.4522294330596925 and perplexity is 31.570698667293637
At time: 286.01674699783325 and batch: 700, loss is 3.5302423572540285 and perplexity is 34.1322388080934
At time: 286.51895022392273 and batch: 750, loss is 3.4855188369750976 and perplexity is 32.63935722286597
At time: 287.0199513435364 and batch: 800, loss is 3.5961529302597044 and perplexity is 36.45770896237897
At time: 287.5330617427826 and batch: 850, loss is 3.457301893234253 and perplexity is 31.731246621278434
At time: 288.04718565940857 and batch: 900, loss is 3.401278157234192 and perplexity is 30.002423365034126
At time: 288.58459424972534 and batch: 950, loss is 3.4325373935699464 and perplexity is 30.955088426140634
At time: 289.0890727043152 and batch: 1000, loss is 3.3329162311553957 and perplexity is 28.019935280782647
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4451472584794205 and perplexity of 85.21242516560727
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 290.6912271976471 and batch: 50, loss is 3.6926831293106077 and perplexity is 40.152436733991784
At time: 291.20594596862793 and batch: 100, loss is 3.57583297252655 and perplexity is 35.72436584081545
At time: 291.7182648181915 and batch: 150, loss is 3.6723600244522094 and perplexity is 39.34465071494616
At time: 292.23246121406555 and batch: 200, loss is 3.706104483604431 and perplexity is 40.6949694309622
At time: 292.7346363067627 and batch: 250, loss is 3.6990413093566894 and perplexity is 40.40854648920283
At time: 293.24992775917053 and batch: 300, loss is 3.518310561180115 and perplexity is 33.72739992557255
At time: 293.76926827430725 and batch: 350, loss is 3.5623739433288573 and perplexity is 35.24677174382161
At time: 294.29611015319824 and batch: 400, loss is 3.5729683208465577 and perplexity is 35.62217441749299
At time: 294.80968499183655 and batch: 450, loss is 3.6201194429397585 and perplexity is 37.34202779726873
At time: 295.32329297065735 and batch: 500, loss is 3.64412184715271 and perplexity is 38.249169482182914
At time: 295.835946559906 and batch: 550, loss is 3.5352283096313477 and perplexity is 34.3028454902821
At time: 296.33714413642883 and batch: 600, loss is 3.479680781364441 and perplexity is 32.449361981327705
At time: 296.8386445045471 and batch: 650, loss is 3.452191972732544 and perplexity is 31.569516040744176
At time: 297.3436691761017 and batch: 700, loss is 3.530206198692322 and perplexity is 34.131004657742956
At time: 297.8489992618561 and batch: 750, loss is 3.4854735851287844 and perplexity is 32.63788026510694
At time: 298.3616466522217 and batch: 800, loss is 3.5961111307144167 and perplexity is 36.45618507857116
At time: 298.8621292114258 and batch: 850, loss is 3.4572557067871093 and perplexity is 31.72978110157739
At time: 299.36392545700073 and batch: 900, loss is 3.4012258768081667 and perplexity is 30.000854866560054
At time: 299.8654417991638 and batch: 950, loss is 3.4324851274490356 and perplexity is 30.95347056602615
At time: 300.3663363456726 and batch: 1000, loss is 3.332861762046814 and perplexity is 28.018409101450576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.445148374976181 and perplexity of 85.21252030505707
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 301.8641390800476 and batch: 50, loss is 3.6926769018173218 and perplexity is 40.1521866857402
At time: 302.3670744895935 and batch: 100, loss is 3.575827751159668 and perplexity is 35.72417931128174
At time: 302.8668620586395 and batch: 150, loss is 3.6723564529418944 and perplexity is 39.34451019537122
At time: 303.37093901634216 and batch: 200, loss is 3.706100444793701 and perplexity is 40.69480507201492
At time: 303.87196230888367 and batch: 250, loss is 3.6990380001068117 and perplexity is 40.408412767446556
At time: 304.37490916252136 and batch: 300, loss is 3.518303084373474 and perplexity is 33.72714775326753
At time: 304.87795996665955 and batch: 350, loss is 3.5623671627044677 and perplexity is 35.246532749511736
At time: 305.37786769866943 and batch: 400, loss is 3.572960810661316 and perplexity is 35.621906889369
At time: 305.87817883491516 and batch: 450, loss is 3.6201141023635866 and perplexity is 37.34182836985739
At time: 306.37782526016235 and batch: 500, loss is 3.6441147327423096 and perplexity is 38.24889736286173
At time: 306.87736439704895 and batch: 550, loss is 3.535220198631287 and perplexity is 34.3025672610286
At time: 307.37691497802734 and batch: 600, loss is 3.4796729850769044 and perplexity is 32.44910899775749
At time: 307.87623620033264 and batch: 650, loss is 3.4521823263168336 and perplexity is 31.569211509537496
At time: 308.3739194869995 and batch: 700, loss is 3.530196690559387 and perplexity is 34.13068013715627
At time: 308.8742368221283 and batch: 750, loss is 3.485461564064026 and perplexity is 32.637487925392875
At time: 309.37490487098694 and batch: 800, loss is 3.5961001253128053 and perplexity is 36.45578386582091
At time: 309.87585067749023 and batch: 850, loss is 3.457243447303772 and perplexity is 31.72939211323908
At time: 310.3770000934601 and batch: 900, loss is 3.4012121248245237 and perplexity is 30.00044229813148
At time: 310.87844157218933 and batch: 950, loss is 3.4324711894989015 and perplexity is 30.953039141103524
At time: 311.3805260658264 and batch: 1000, loss is 3.3328470754623414 and perplexity is 28.01799760974023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4451487471417686 and perplexity of 85.21255201823062
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 312.86292791366577 and batch: 50, loss is 3.692675848007202 and perplexity is 40.15214437298184
At time: 313.3889350891113 and batch: 100, loss is 3.575826826095581 and perplexity is 35.7241462641417
At time: 313.9021100997925 and batch: 150, loss is 3.672356004714966 and perplexity is 39.34449256010622
At time: 314.42655777931213 and batch: 200, loss is 3.7060998058319092 and perplexity is 40.69477906959766
At time: 314.9355158805847 and batch: 250, loss is 3.6990376996994017 and perplexity is 40.408400628461756
At time: 315.45370388031006 and batch: 300, loss is 3.518301668167114 and perplexity is 33.72709998870021
At time: 315.9600169658661 and batch: 350, loss is 3.562365746498108 and perplexity is 35.246482833183244
At time: 316.47471046447754 and batch: 400, loss is 3.5729593658447265 and perplexity is 35.621855422284156
At time: 316.9783101081848 and batch: 450, loss is 3.6201131343841553 and perplexity is 37.341792223753096
At time: 317.48235869407654 and batch: 500, loss is 3.644113426208496 and perplexity is 38.24884738941664
At time: 317.98722410202026 and batch: 550, loss is 3.5352185583114624 and perplexity is 34.30251099389364
At time: 318.49142956733704 and batch: 600, loss is 3.4796714782714844 and perplexity is 32.449060103301015
At time: 318.99510622024536 and batch: 650, loss is 3.4521803140640257 and perplexity is 31.569147984366907
At time: 319.4980194568634 and batch: 700, loss is 3.5301947355270387 and perplexity is 34.13061341063776
At time: 320.00019001960754 and batch: 750, loss is 3.485458965301514 and perplexity is 32.63740310842297
At time: 320.5044672489166 and batch: 800, loss is 3.5960978269577026 and perplexity is 36.455700077580325
At time: 321.0085549354553 and batch: 850, loss is 3.4572407674789427 and perplexity is 31.729307084140213
At time: 321.5170624256134 and batch: 900, loss is 3.4012089204788207 and perplexity is 30.00034616649713
At time: 322.01875472068787 and batch: 950, loss is 3.4324681329727174 and perplexity is 30.9529445324735
At time: 322.5318057537079 and batch: 1000, loss is 3.33284384727478 and perplexity is 28.017907162534847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4451487471417686 and perplexity of 85.21255201823062
Annealing...
Model not improving. Stopping early with 85.21229831317284loss at 28 epochs.
Finished Training.
Improved accuracyfrom -103.65104451499782 to -85.21229831317284
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd6d92888d0>
SETTINGS FOR THIS RUN
{'anneal': 4.891990058382956, 'data': 'ptb', 'dropout': 0.788981353500536, 'tune_wordvecs': True, 'lr': 20.80459897558753, 'num_layers': 1, 'batch_size': 20, 'wordvec_source': '', 'wordvec_dim': 200, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.765129804611206 and batch: 50, loss is 6.878803281784058 and perplexity is 971.4630962979516
At time: 1.2995281219482422 and batch: 100, loss is 6.424452590942383 and perplexity is 616.7431143533637
At time: 1.8147151470184326 and batch: 150, loss is 6.237981939315796 and perplexity is 511.82457496900327
At time: 2.322885274887085 and batch: 200, loss is 6.201336765289307 and perplexity is 493.4081713608082
At time: 2.83626651763916 and batch: 250, loss is 6.259443140029907 and perplexity is 522.9276615328763
At time: 3.3552353382110596 and batch: 300, loss is 6.16553991317749 and perplexity is 476.0581036042895
At time: 3.8709566593170166 and batch: 350, loss is 6.14175256729126 and perplexity is 464.8675690539129
At time: 4.376194000244141 and batch: 400, loss is 6.122924680709839 and perplexity is 456.19697579615126
At time: 4.90865159034729 and batch: 450, loss is 6.146714944839477 and perplexity is 467.1801506473243
At time: 5.421267509460449 and batch: 500, loss is 6.142949905395508 and perplexity is 465.4245060621849
At time: 5.937483787536621 and batch: 550, loss is 6.089812059402465 and perplexity is 441.3384579375773
At time: 6.4540746212005615 and batch: 600, loss is 6.011843090057373 and perplexity is 408.23504126198463
At time: 6.959506034851074 and batch: 650, loss is 5.967543821334839 and perplexity is 390.5452428482039
At time: 7.465138673782349 and batch: 700, loss is 6.054501123428345 and perplexity is 426.0263178826368
At time: 7.982957601547241 and batch: 750, loss is 5.921285629272461 and perplexity is 372.89080517786107
At time: 8.50105333328247 and batch: 800, loss is 6.023533630371094 and perplexity is 413.03553497900197
At time: 9.00659441947937 and batch: 850, loss is 5.998055620193481 and perplexity is 402.6451368033126
At time: 9.514747381210327 and batch: 900, loss is 6.020531139373779 and perplexity is 411.79725938956085
At time: 10.020749807357788 and batch: 950, loss is 5.981730794906616 and perplexity is 396.12538697293127
At time: 10.52780270576477 and batch: 1000, loss is 5.9234929943084715 and perplexity is 373.71482042009615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.75078861887862 and perplexity of 314.4385346982558
Finished 1 epochs...
Completing Train Step...
At time: 12.033644437789917 and batch: 50, loss is 5.689033451080323 and perplexity is 295.6077631509927
At time: 12.536832571029663 and batch: 100, loss is 5.586701984405518 and perplexity is 266.85407793294036
At time: 13.038636207580566 and batch: 150, loss is 5.492196779251099 and perplexity is 242.78997743883227
At time: 13.541850328445435 and batch: 200, loss is 5.446919736862182 and perplexity is 232.0423125834964
At time: 14.045925617218018 and batch: 250, loss is 5.466274557113647 and perplexity is 236.57719417690555
At time: 14.547735452651978 and batch: 300, loss is 5.405318355560302 and perplexity is 222.5870710370997
At time: 15.050230026245117 and batch: 350, loss is 5.371244468688965 and perplexity is 215.1304242639647
At time: 15.552077531814575 and batch: 400, loss is 5.35805380821228 and perplexity is 212.31134550964822
At time: 16.05415368080139 and batch: 450, loss is 5.384342355728149 and perplexity is 217.96671240240545
At time: 16.556736946105957 and batch: 500, loss is 5.413737316131591 and perplexity is 224.46893335883604
At time: 17.06013774871826 and batch: 550, loss is 5.343266429901123 and perplexity is 209.1949160227828
At time: 17.563064336776733 and batch: 600, loss is 5.2368546485900875 and perplexity is 188.07760093420492
At time: 18.066612005233765 and batch: 650, loss is 5.214654893875122 and perplexity is 183.94832832965787
At time: 18.570234298706055 and batch: 700, loss is 5.279232816696167 and perplexity is 196.2192814360079
At time: 19.0952889919281 and batch: 750, loss is 5.23431586265564 and perplexity is 187.60071777464066
At time: 19.623237133026123 and batch: 800, loss is 5.321781635284424 and perplexity is 204.74834410156342
At time: 20.125421285629272 and batch: 850, loss is 5.266311197280884 and perplexity is 193.70012142435684
At time: 20.629504203796387 and batch: 900, loss is 5.2911020946502685 and perplexity is 198.5621391369176
At time: 21.13372778892517 and batch: 950, loss is 5.256604566574096 and perplexity is 191.82904150957538
At time: 21.638952255249023 and batch: 1000, loss is 5.159399442672729 and perplexity is 174.05989126672955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.360866267506669 and perplexity of 212.90930299802895
Finished 2 epochs...
Completing Train Step...
At time: 23.165935039520264 and batch: 50, loss is 5.3040529727935795 and perplexity is 201.15041726554668
At time: 23.699477672576904 and batch: 100, loss is 5.306321887969971 and perplexity is 201.6073286505918
At time: 24.215120553970337 and batch: 150, loss is 5.296701526641845 and perplexity is 199.67709297205013
At time: 24.725629091262817 and batch: 200, loss is 5.276499404907226 and perplexity is 195.6836657015841
At time: 25.24766707420349 and batch: 250, loss is 5.315491609573364 and perplexity is 203.4645136489769
At time: 25.765600204467773 and batch: 300, loss is 5.27932578086853 and perplexity is 196.23752364703108
At time: 26.282639026641846 and batch: 350, loss is 5.263363151550293 and perplexity is 193.12992550316545
At time: 26.798852920532227 and batch: 400, loss is 5.256682453155517 and perplexity is 191.84398299969902
At time: 27.31550693511963 and batch: 450, loss is 5.297738933563233 and perplexity is 199.88434685506078
At time: 27.826960563659668 and batch: 500, loss is 5.310548429489136 and perplexity is 202.46123365702812
At time: 28.33420467376709 and batch: 550, loss is 5.2644664478302 and perplexity is 193.34312261967816
At time: 28.840814352035522 and batch: 600, loss is 5.2096548271179195 and perplexity is 183.03086999601317
At time: 29.348174333572388 and batch: 650, loss is 5.179845714569092 and perplexity is 177.65539923897694
At time: 29.856319189071655 and batch: 700, loss is 5.233802309036255 and perplexity is 187.50439948144924
At time: 30.372630834579468 and batch: 750, loss is 5.1771700572967525 and perplexity is 177.1806896413358
At time: 30.880000114440918 and batch: 800, loss is 5.263964595794678 and perplexity is 193.24611732322597
At time: 31.388104677200317 and batch: 850, loss is 5.221314849853516 and perplexity is 185.17750468550125
At time: 31.89481282234192 and batch: 900, loss is 5.239612245559693 and perplexity is 188.59695891747754
At time: 32.41618227958679 and batch: 950, loss is 5.259552373886108 and perplexity is 192.3953508360984
At time: 32.92184281349182 and batch: 1000, loss is 5.176841630935669 and perplexity is 177.12250838683437
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.32885258372237 and perplexity of 206.20123970689377
Finished 3 epochs...
Completing Train Step...
At time: 34.4700722694397 and batch: 50, loss is 5.238941631317139 and perplexity is 188.4705255094855
At time: 34.988924741744995 and batch: 100, loss is 5.193921842575073 and perplexity is 180.17378233904577
At time: 35.491164207458496 and batch: 150, loss is 5.271559352874756 and perplexity is 194.71936202735725
At time: 35.996519565582275 and batch: 200, loss is 5.284671211242676 and perplexity is 197.28930627745527
At time: 36.513423442840576 and batch: 250, loss is 5.3180540084838865 and perplexity is 203.98653943059784
At time: 37.02311706542969 and batch: 300, loss is 5.252587919235229 and perplexity is 191.06007726341232
At time: 37.52696132659912 and batch: 350, loss is 5.228084325790405 and perplexity is 186.43531189199985
At time: 38.04322266578674 and batch: 400, loss is 5.242088642120361 and perplexity is 189.064578544737
At time: 38.55274510383606 and batch: 450, loss is 5.280210266113281 and perplexity is 196.4111696234949
At time: 39.05641174316406 and batch: 500, loss is 5.292051334381103 and perplexity is 198.75071169454827
At time: 39.56186580657959 and batch: 550, loss is 5.231502866744995 and perplexity is 187.07373926439965
At time: 40.06996440887451 and batch: 600, loss is 5.19357850074768 and perplexity is 180.11193176192185
At time: 40.582825899124146 and batch: 650, loss is 5.151697082519531 and perplexity is 172.7243692344403
At time: 41.08703327178955 and batch: 700, loss is 5.2328254508972165 and perplexity is 187.32132371679398
At time: 41.5908784866333 and batch: 750, loss is 5.173579835891724 and perplexity is 176.54571227355146
At time: 42.09599852561951 and batch: 800, loss is 5.269741373062134 and perplexity is 194.36568774182982
At time: 42.60188007354736 and batch: 850, loss is 5.173142700195313 and perplexity is 176.4685547060605
At time: 43.108455419540405 and batch: 900, loss is 5.215083379745483 and perplexity is 184.02716447810533
At time: 43.613080978393555 and batch: 950, loss is 5.199277296066284 and perplexity is 181.14128304080683
At time: 44.11672639846802 and batch: 1000, loss is 5.11280891418457 and perplexity is 166.13636286532594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.377118738686166 and perplexity of 216.39787749728393
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 45.657463788986206 and batch: 50, loss is 5.214766960144043 and perplexity is 183.9689438876209
At time: 46.16123127937317 and batch: 100, loss is 5.096145553588867 and perplexity is 163.39091050084988
At time: 46.674376249313354 and batch: 150, loss is 5.1012615394592284 and perplexity is 164.22895798198144
At time: 47.185608863830566 and batch: 200, loss is 5.097341623306274 and perplexity is 163.58645433971978
At time: 47.69040846824646 and batch: 250, loss is 5.143306264877319 and perplexity is 171.28113397941814
At time: 48.193697929382324 and batch: 300, loss is 5.059509506225586 and perplexity is 157.51323810999426
At time: 48.69534230232239 and batch: 350, loss is 5.063762788772583 and perplexity is 158.18461317837819
At time: 49.19793510437012 and batch: 400, loss is 5.0533958911895756 and perplexity is 156.55320044967047
At time: 49.70089793205261 and batch: 450, loss is 5.0840963363647464 and perplexity is 161.433991276189
At time: 50.204580307006836 and batch: 500, loss is 5.109517526626587 and perplexity is 165.59044261805718
At time: 50.7073438167572 and batch: 550, loss is 5.020790767669678 and perplexity is 151.53108231016432
At time: 51.209635734558105 and batch: 600, loss is 4.951390800476074 and perplexity is 141.37144673102927
At time: 51.71509623527527 and batch: 650, loss is 4.9369143199920655 and perplexity is 139.33962801705204
At time: 52.21461081504822 and batch: 700, loss is 5.002863550186158 and perplexity is 148.83875670117223
At time: 52.71557688713074 and batch: 750, loss is 4.9380124568939205 and perplexity is 139.49272605038317
At time: 53.21561932563782 and batch: 800, loss is 5.042284784317016 and perplexity is 154.82334918613788
At time: 53.7153844833374 and batch: 850, loss is 4.941009798049927 and perplexity is 139.91146057074874
At time: 54.216291427612305 and batch: 900, loss is 4.949157476425171 and perplexity is 141.0560707783683
At time: 54.71843957901001 and batch: 950, loss is 4.922454319000244 and perplexity is 137.33927427094886
At time: 55.22099757194519 and batch: 1000, loss is 4.831766958236694 and perplexity is 125.43239877135716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.172861610970846 and perplexity of 176.4189582677141
Finished 5 epochs...
Completing Train Step...
At time: 56.689475774765015 and batch: 50, loss is 5.059194393157959 and perplexity is 157.4636114497568
At time: 57.21900415420532 and batch: 100, loss is 4.98523983001709 and perplexity is 146.238643237091
At time: 57.73414635658264 and batch: 150, loss is 5.0141658592224125 and perplexity is 150.5305207368562
At time: 58.2509400844574 and batch: 200, loss is 5.020100917816162 and perplexity is 151.4265846632169
At time: 58.754595041275024 and batch: 250, loss is 5.057410058975219 and perplexity is 157.18289426642235
At time: 59.25896954536438 and batch: 300, loss is 4.973279018402099 and perplexity is 144.49992931778627
At time: 59.76354908943176 and batch: 350, loss is 4.967296686172485 and perplexity is 143.63806328830614
At time: 60.26869487762451 and batch: 400, loss is 4.973284015655517 and perplexity is 144.50065142235624
At time: 60.78904104232788 and batch: 450, loss is 5.00533130645752 and perplexity is 149.20650805016388
At time: 61.30773138999939 and batch: 500, loss is 5.037656373977661 and perplexity is 154.1084189674792
At time: 61.813013315200806 and batch: 550, loss is 4.953706407546997 and perplexity is 141.69918676483005
At time: 62.32911777496338 and batch: 600, loss is 4.892174711227417 and perplexity is 133.2430243027768
At time: 62.83874845504761 and batch: 650, loss is 4.879279975891113 and perplexity is 131.53592074712884
At time: 63.3411910533905 and batch: 700, loss is 4.948502435684204 and perplexity is 140.96370356069582
At time: 63.843846559524536 and batch: 750, loss is 4.890562381744385 and perplexity is 133.02836574294972
At time: 64.34580516815186 and batch: 800, loss is 4.990062484741211 and perplexity is 146.94560506694242
At time: 64.84571695327759 and batch: 850, loss is 4.889859848022461 and perplexity is 132.93494165067295
At time: 65.34805822372437 and batch: 900, loss is 4.901613883972168 and perplexity is 134.50668278913741
At time: 65.85180950164795 and batch: 950, loss is 4.881461305618286 and perplexity is 131.82315712597756
At time: 66.35355281829834 and batch: 1000, loss is 4.8051302528381346 and perplexity is 122.13539846745165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.154861078029725 and perplexity of 173.27173383613749
Finished 6 epochs...
Completing Train Step...
At time: 67.82649254798889 and batch: 50, loss is 4.998793458938598 and perplexity is 148.23420051421937
At time: 68.34507012367249 and batch: 100, loss is 4.925430517196656 and perplexity is 137.74863203408282
At time: 68.84910130500793 and batch: 150, loss is 4.953341913223267 and perplexity is 141.64754762723499
At time: 69.35481762886047 and batch: 200, loss is 4.965351152420044 and perplexity is 143.35888225416662
At time: 69.85882019996643 and batch: 250, loss is 5.005235061645508 and perplexity is 149.19214838887822
At time: 70.3635778427124 and batch: 300, loss is 4.930402336120605 and perplexity is 138.43519861767604
At time: 70.86612820625305 and batch: 350, loss is 4.926875200271606 and perplexity is 137.9477789688463
At time: 71.38586616516113 and batch: 400, loss is 4.920692892074585 and perplexity is 137.09757410637846
At time: 71.89104771614075 and batch: 450, loss is 4.955375308990479 and perplexity is 141.93586618454066
At time: 72.39595890045166 and batch: 500, loss is 4.987733097076416 and perplexity is 146.60371014460583
At time: 72.9017333984375 and batch: 550, loss is 4.911181678771973 and perplexity is 135.79979136347407
At time: 73.40712523460388 and batch: 600, loss is 4.84893120765686 and perplexity is 127.60393482947971
At time: 73.91070652008057 and batch: 650, loss is 4.8389912128448485 and perplexity is 126.34183540180007
At time: 74.41561031341553 and batch: 700, loss is 4.910453929901123 and perplexity is 135.7009991709611
At time: 74.92240715026855 and batch: 750, loss is 4.847968816757202 and perplexity is 127.48118903802951
At time: 75.42920255661011 and batch: 800, loss is 4.926687440872192 and perplexity is 137.92188040814182
At time: 75.93545150756836 and batch: 850, loss is 4.851729125976562 and perplexity is 127.96146014648632
At time: 76.44375944137573 and batch: 900, loss is 4.865609722137451 and perplexity is 129.75002596186866
At time: 76.95177626609802 and batch: 950, loss is 4.843912401199341 and perplexity is 126.965119762838
At time: 77.45860195159912 and batch: 1000, loss is 4.764049510955811 and perplexity is 117.21964832968793
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.134041390767911 and perplexity of 169.70156439136656
Finished 7 epochs...
Completing Train Step...
At time: 78.98999571800232 and batch: 50, loss is 4.951334438323975 and perplexity is 141.3634789565887
At time: 79.50595045089722 and batch: 100, loss is 4.878517932891846 and perplexity is 131.43572290195337
At time: 80.02213430404663 and batch: 150, loss is 4.908044795989991 and perplexity is 135.37447077532812
At time: 80.53245759010315 and batch: 200, loss is 4.930214309692383 and perplexity is 138.40917158870107
At time: 81.03390884399414 and batch: 250, loss is 4.9672744178771975 and perplexity is 143.63486474911141
At time: 81.53702855110168 and batch: 300, loss is 4.889356412887573 and perplexity is 132.86803437354197
At time: 82.05481958389282 and batch: 350, loss is 4.890900154113769 and perplexity is 133.0733066387112
At time: 82.57378363609314 and batch: 400, loss is 4.880531721115112 and perplexity is 131.70067330030705
At time: 83.09503507614136 and batch: 450, loss is 4.918920278549194 and perplexity is 136.85476835617555
At time: 83.61558175086975 and batch: 500, loss is 4.949118394851684 and perplexity is 141.0505581928934
At time: 84.14182686805725 and batch: 550, loss is 4.8659741497039795 and perplexity is 129.79731906501874
At time: 84.68010115623474 and batch: 600, loss is 4.8067906379699705 and perplexity is 122.33835871659983
At time: 85.19999885559082 and batch: 650, loss is 4.806445760726929 and perplexity is 122.2961742753724
At time: 85.72434282302856 and batch: 700, loss is 4.8806932926177975 and perplexity is 131.72195409513554
At time: 86.2437674999237 and batch: 750, loss is 4.836564350128174 and perplexity is 126.03559286650818
At time: 86.75252103805542 and batch: 800, loss is 4.918224639892578 and perplexity is 136.7595999941983
At time: 87.27044820785522 and batch: 850, loss is 4.808245077133178 and perplexity is 122.51642187630685
At time: 87.78967714309692 and batch: 900, loss is 4.830854616165161 and perplexity is 125.31801370391359
At time: 88.32222104072571 and batch: 950, loss is 4.821210355758667 and perplexity is 124.1152234799838
At time: 88.84279298782349 and batch: 1000, loss is 4.746766471862793 and perplexity is 115.21114309772706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.141477259193978 and perplexity of 170.96814612745297
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 90.4236011505127 and batch: 50, loss is 4.905447463989258 and perplexity is 135.02331456234796
At time: 90.95338988304138 and batch: 100, loss is 4.80774847984314 and perplexity is 122.45559565753555
At time: 91.47612476348877 and batch: 150, loss is 4.822047271728516 and perplexity is 124.21914097165417
At time: 91.98997402191162 and batch: 200, loss is 4.8365928649902346 and perplexity is 126.03918680529362
At time: 92.51589894294739 and batch: 250, loss is 4.8739978694915775 and perplexity is 130.8429657605233
At time: 93.02394223213196 and batch: 300, loss is 4.791047620773315 and perplexity is 120.4274649150791
At time: 93.53631949424744 and batch: 350, loss is 4.78302152633667 and perplexity is 119.46477121210182
At time: 94.0482702255249 and batch: 400, loss is 4.775621194839477 and perplexity is 118.58395548688544
At time: 94.56337428092957 and batch: 450, loss is 4.818258600234985 and perplexity is 123.7494058512168
At time: 95.06710386276245 and batch: 500, loss is 4.848718919754028 and perplexity is 127.5768489328675
At time: 95.59950637817383 and batch: 550, loss is 4.762230024337769 and perplexity is 117.00656266022807
At time: 96.11778926849365 and batch: 600, loss is 4.703537931442261 and perplexity is 110.33684692485268
At time: 96.65324354171753 and batch: 650, loss is 4.684999895095825 and perplexity is 108.31026094598468
At time: 97.16535067558289 and batch: 700, loss is 4.768779306411743 and perplexity is 117.77538651764883
At time: 97.69575119018555 and batch: 750, loss is 4.7104819393157955 and perplexity is 111.10569320652677
At time: 98.21664476394653 and batch: 800, loss is 4.8024568939208985 and perplexity is 121.8093227637447
At time: 98.7270884513855 and batch: 850, loss is 4.686024618148804 and perplexity is 108.42130585267108
At time: 99.23205375671387 and batch: 900, loss is 4.695898189544677 and perplexity is 109.4971136495912
At time: 99.73642611503601 and batch: 950, loss is 4.669453468322754 and perplexity is 106.63944466855929
At time: 100.256751537323 and batch: 1000, loss is 4.598979167938232 and perplexity is 99.38281068145737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.058941352658156 and perplexity of 157.42377181954726
Finished 9 epochs...
Completing Train Step...
At time: 101.80598783493042 and batch: 50, loss is 4.847117509841919 and perplexity is 127.37270960143162
At time: 102.32782244682312 and batch: 100, loss is 4.766749505996704 and perplexity is 117.5365684477186
At time: 102.85467648506165 and batch: 150, loss is 4.790208101272583 and perplexity is 120.3264061362024
At time: 103.37091064453125 and batch: 200, loss is 4.810956525802612 and perplexity is 122.84906964025473
At time: 103.87955904006958 and batch: 250, loss is 4.852162456512451 and perplexity is 128.01692177032442
At time: 104.39102840423584 and batch: 300, loss is 4.76997148513794 and perplexity is 117.91587955769134
At time: 104.91653609275818 and batch: 350, loss is 4.759585771560669 and perplexity is 116.69757643086024
At time: 105.44526767730713 and batch: 400, loss is 4.755651903152466 and perplexity is 116.23940530494583
At time: 105.96143245697021 and batch: 450, loss is 4.7991437911987305 and perplexity is 121.40642375634981
At time: 106.4911241531372 and batch: 500, loss is 4.829591178894043 and perplexity is 125.15978223347463
At time: 107.00687837600708 and batch: 550, loss is 4.746150426864624 and perplexity is 115.14018970677309
At time: 107.52981328964233 and batch: 600, loss is 4.691249475479126 and perplexity is 108.98927419200271
At time: 108.05056762695312 and batch: 650, loss is 4.6747290134429935 and perplexity is 107.20351244477646
At time: 108.55709600448608 and batch: 700, loss is 4.758298568725586 and perplexity is 116.54745961575334
At time: 109.06402015686035 and batch: 750, loss is 4.701124906539917 and perplexity is 110.07092233593751
At time: 109.5695469379425 and batch: 800, loss is 4.794311437606812 and perplexity is 120.82116022749163
At time: 110.07538223266602 and batch: 850, loss is 4.684792051315307 and perplexity is 108.2877516711682
At time: 110.60727787017822 and batch: 900, loss is 4.698803310394287 and perplexity is 109.81567850808287
At time: 111.12074851989746 and batch: 950, loss is 4.67222918510437 and perplexity is 106.93585675239426
At time: 111.64570999145508 and batch: 1000, loss is 4.602667245864868 and perplexity is 99.75001896196763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.056322051257622 and perplexity of 157.0119710641288
Finished 10 epochs...
Completing Train Step...
At time: 113.26239371299744 and batch: 50, loss is 4.832308349609375 and perplexity is 125.50032517563204
At time: 113.77513837814331 and batch: 100, loss is 4.7537034893035885 and perplexity is 116.01314333542201
At time: 114.27982449531555 and batch: 150, loss is 4.779050827026367 and perplexity is 118.9913530500395
At time: 114.78434872627258 and batch: 200, loss is 4.798607082366943 and perplexity is 121.34128133930307
At time: 115.2882285118103 and batch: 250, loss is 4.844016008377075 and perplexity is 126.97827494204103
At time: 115.79274988174438 and batch: 300, loss is 4.761540832519532 and perplexity is 116.92595047638216
At time: 116.29638934135437 and batch: 350, loss is 4.74952748298645 and perplexity is 115.52968188760364
At time: 116.80077409744263 and batch: 400, loss is 4.747334976196289 and perplexity is 115.27665975333181
At time: 117.31519842147827 and batch: 450, loss is 4.789861707687378 and perplexity is 120.28473305904606
At time: 117.82221674919128 and batch: 500, loss is 4.819207735061646 and perplexity is 123.86691647998217
At time: 118.32624697685242 and batch: 550, loss is 4.7379648303985595 and perplexity is 114.20154549923113
At time: 118.82910108566284 and batch: 600, loss is 4.684977054595947 and perplexity is 108.30778711373463
At time: 119.33132863044739 and batch: 650, loss is 4.668206272125244 and perplexity is 106.50652726293492
At time: 119.83495545387268 and batch: 700, loss is 4.7520633316040035 and perplexity is 115.82301944433058
At time: 120.33859872817993 and batch: 750, loss is 4.694277420043945 and perplexity is 109.31978780834531
At time: 120.84391593933105 and batch: 800, loss is 4.7879680824279784 and perplexity is 120.05717437364127
At time: 121.34903192520142 and batch: 850, loss is 4.682650461196899 and perplexity is 108.05609184098668
At time: 121.85679483413696 and batch: 900, loss is 4.69865571975708 and perplexity is 109.79947193811498
At time: 122.35785150527954 and batch: 950, loss is 4.669203414916992 and perplexity is 106.61278244586008
At time: 122.87049722671509 and batch: 1000, loss is 4.599675645828247 and perplexity is 99.45205272172062
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0541865651200455 and perplexity of 156.67703193270913
Finished 11 epochs...
Completing Train Step...
At time: 124.3848512172699 and batch: 50, loss is 4.821328268051148 and perplexity is 124.12985905335617
At time: 124.91523551940918 and batch: 100, loss is 4.743726205825806 and perplexity is 114.86140249391829
At time: 125.42964792251587 and batch: 150, loss is 4.769199876785279 and perplexity is 117.82492977343233
At time: 125.93687725067139 and batch: 200, loss is 4.790092115402222 and perplexity is 120.3124507825868
At time: 126.44278883934021 and batch: 250, loss is 4.837054424285888 and perplexity is 126.09737479115012
At time: 126.94720554351807 and batch: 300, loss is 4.753455476760864 and perplexity is 115.98437418845609
At time: 127.45079326629639 and batch: 350, loss is 4.740780715942383 and perplexity is 114.52357716965932
At time: 127.95544362068176 and batch: 400, loss is 4.738607244491577 and perplexity is 114.27493375180417
At time: 128.46749591827393 and batch: 450, loss is 4.781307506561279 and perplexity is 119.26018161717553
At time: 128.9750018119812 and batch: 500, loss is 4.810422420501709 and perplexity is 122.78347282031287
At time: 129.48600578308105 and batch: 550, loss is 4.730863647460938 and perplexity is 113.39345203842424
At time: 129.98956894874573 and batch: 600, loss is 4.678391313552856 and perplexity is 107.59684368908263
At time: 130.49462294578552 and batch: 650, loss is 4.663393363952637 and perplexity is 105.99515271393484
At time: 131.0096399784088 and batch: 700, loss is 4.745745134353638 and perplexity is 115.09353370547404
At time: 131.51907777786255 and batch: 750, loss is 4.688867998123169 and perplexity is 108.73002752103554
At time: 132.02169036865234 and batch: 800, loss is 4.783327379226685 and perplexity is 119.50131544592685
At time: 132.52490639686584 and batch: 850, loss is 4.6813930416107175 and perplexity is 107.92030538286662
At time: 133.0277760028839 and batch: 900, loss is 4.698276386260987 and perplexity is 109.75782921929387
At time: 133.53187584877014 and batch: 950, loss is 4.6659982299804685 and perplexity is 106.27161580468635
At time: 134.04815101623535 and batch: 1000, loss is 4.596037225723267 and perplexity is 99.09086185413018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.053218934594131 and perplexity of 156.52549977929343
Finished 12 epochs...
Completing Train Step...
At time: 135.55786037445068 and batch: 50, loss is 4.8123526191711425 and perplexity is 123.02069818857227
At time: 136.08990216255188 and batch: 100, loss is 4.735472717285156 and perplexity is 113.9172966673559
At time: 136.60408067703247 and batch: 150, loss is 4.761032762527466 and perplexity is 116.86655899844283
At time: 137.14042353630066 and batch: 200, loss is 4.782674112319946 and perplexity is 119.42327468473304
At time: 137.64961194992065 and batch: 250, loss is 4.830714492797852 and perplexity is 125.30045495207334
At time: 138.15400505065918 and batch: 300, loss is 4.745640001296997 and perplexity is 115.0814342065158
At time: 138.6566252708435 and batch: 350, loss is 4.732885885238647 and perplexity is 113.62299257542952
At time: 139.1599416732788 and batch: 400, loss is 4.730486307144165 and perplexity is 113.35067218910069
At time: 139.66574835777283 and batch: 450, loss is 4.7740394878387455 and perplexity is 118.39653867263421
At time: 140.17046427726746 and batch: 500, loss is 4.803075180053711 and perplexity is 121.88465906615244
At time: 140.67539262771606 and batch: 550, loss is 4.724430112838745 and perplexity is 112.6662730127066
At time: 141.18070816993713 and batch: 600, loss is 4.673213739395141 and perplexity is 107.04119275499488
At time: 141.69033074378967 and batch: 650, loss is 4.6575964832305905 and perplexity is 105.38248894159548
At time: 142.19286704063416 and batch: 700, loss is 4.740661220550537 and perplexity is 114.5098929475469
At time: 142.69365000724792 and batch: 750, loss is 4.68307975769043 and perplexity is 108.10248990086826
At time: 143.19587802886963 and batch: 800, loss is 4.777500343322754 and perplexity is 118.80700184996196
At time: 143.69886183738708 and batch: 850, loss is 4.677946910858155 and perplexity is 107.54903798508444
At time: 144.20036387443542 and batch: 900, loss is 4.695783557891846 and perplexity is 109.4845625338646
At time: 144.7008695602417 and batch: 950, loss is 4.661599636077881 and perplexity is 105.80519666958
At time: 145.20290184020996 and batch: 1000, loss is 4.59119104385376 and perplexity is 98.6118112367479
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.052025771722561 and perplexity of 156.33885073799922
Finished 13 epochs...
Completing Train Step...
At time: 146.69738149642944 and batch: 50, loss is 4.804430723190308 and perplexity is 122.0499910111812
At time: 147.20185542106628 and batch: 100, loss is 4.727820301055909 and perplexity is 113.04888107416204
At time: 147.7064173221588 and batch: 150, loss is 4.753401584625244 and perplexity is 115.97812371125983
At time: 148.2112467288971 and batch: 200, loss is 4.7755628681182865 and perplexity is 118.57703907528385
At time: 148.73289561271667 and batch: 250, loss is 4.825434579849243 and perplexity is 124.64062291822553
At time: 149.24382400512695 and batch: 300, loss is 4.740237798690796 and perplexity is 114.46141721928201
At time: 149.77144932746887 and batch: 350, loss is 4.725937309265137 and perplexity is 112.83621124974812
At time: 150.27622532844543 and batch: 400, loss is 4.723589057922363 and perplexity is 112.5715543272703
At time: 150.78217101097107 and batch: 450, loss is 4.7673579025268555 and perplexity is 117.60809904540542
At time: 151.2898142337799 and batch: 500, loss is 4.796170883178711 and perplexity is 121.04602960025687
At time: 151.7975685596466 and batch: 550, loss is 4.718283843994141 and perplexity is 111.97591953136505
At time: 152.3215651512146 and batch: 600, loss is 4.667724733352661 and perplexity is 106.45525258688775
At time: 152.8321409225464 and batch: 650, loss is 4.652603664398193 and perplexity is 104.85764458288884
At time: 153.3505961894989 and batch: 700, loss is 4.73518593788147 and perplexity is 113.88463221691774
At time: 153.86722683906555 and batch: 750, loss is 4.67730486869812 and perplexity is 107.48000913051554
At time: 154.3856382369995 and batch: 800, loss is 4.772102479934692 and perplexity is 118.16742560995544
At time: 154.9032244682312 and batch: 850, loss is 4.674181137084961 and perplexity is 107.14479426143093
At time: 155.40868043899536 and batch: 900, loss is 4.692763423919677 and perplexity is 109.15440330064054
At time: 155.91558980941772 and batch: 950, loss is 4.656928644180298 and perplexity is 105.31213389580168
At time: 156.42292141914368 and batch: 1000, loss is 4.586448564529419 and perplexity is 98.14525395440327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.050780505668826 and perplexity of 156.14428844034975
Finished 14 epochs...
Completing Train Step...
At time: 157.91679167747498 and batch: 50, loss is 4.797440509796143 and perplexity is 121.1998104628528
At time: 158.43082571029663 and batch: 100, loss is 4.721140794754028 and perplexity is 112.2962866383767
At time: 158.93399286270142 and batch: 150, loss is 4.746628341674804 and perplexity is 115.1952300599333
At time: 159.43693351745605 and batch: 200, loss is 4.769189434051514 and perplexity is 117.82369936548427
At time: 159.94006061553955 and batch: 250, loss is 4.820230340957641 and perplexity is 123.99364830640596
At time: 160.44271397590637 and batch: 300, loss is 4.734193305969239 and perplexity is 113.77164278440229
At time: 160.9438316822052 and batch: 350, loss is 4.719532146453857 and perplexity is 112.11578662720251
At time: 161.44534158706665 and batch: 400, loss is 4.717159128189087 and perplexity is 111.85004924230307
At time: 161.9617578983307 and batch: 450, loss is 4.761344795227051 and perplexity is 116.90303087625158
At time: 162.47847747802734 and batch: 500, loss is 4.789876775741577 and perplexity is 120.28654552957828
At time: 163.00177645683289 and batch: 550, loss is 4.712192821502685 and perplexity is 111.29594466040024
At time: 163.50460028648376 and batch: 600, loss is 4.662456750869751 and perplexity is 105.89592274447834
At time: 164.0044436454773 and batch: 650, loss is 4.647145261764527 and perplexity is 104.28684857395496
At time: 164.50644755363464 and batch: 700, loss is 4.729538469314575 and perplexity is 113.24328503483373
At time: 165.01222825050354 and batch: 750, loss is 4.672007455825805 and perplexity is 106.91214857052005
At time: 165.53922247886658 and batch: 800, loss is 4.766709957122803 and perplexity is 117.53192010071334
At time: 166.06206560134888 and batch: 850, loss is 4.670672655105591 and perplexity is 106.76953735755744
At time: 166.57643818855286 and batch: 900, loss is 4.689631071090698 and perplexity is 108.81302812952836
At time: 167.1038794517517 and batch: 950, loss is 4.651884908676148 and perplexity is 104.78230462959934
At time: 167.62023401260376 and batch: 1000, loss is 4.581706657409668 and perplexity is 97.68095996539017
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.050239004739901 and perplexity of 156.05975905154972
Finished 15 epochs...
Completing Train Step...
At time: 169.12512683868408 and batch: 50, loss is 4.790455894470215 and perplexity is 120.35622589555263
At time: 169.64135265350342 and batch: 100, loss is 4.714656229019165 and perplexity is 111.57044989734563
At time: 170.16153383255005 and batch: 150, loss is 4.740364246368408 and perplexity is 114.47589151476683
At time: 170.68748140335083 and batch: 200, loss is 4.763118715286255 and perplexity is 117.110591551298
At time: 171.2068636417389 and batch: 250, loss is 4.815226202011108 and perplexity is 123.37471676308957
At time: 171.7262454032898 and batch: 300, loss is 4.72845643043518 and perplexity is 113.1208176667713
At time: 172.24707674980164 and batch: 350, loss is 4.713051385879517 and perplexity is 111.39154042545846
At time: 172.77807354927063 and batch: 400, loss is 4.711257429122925 and perplexity is 111.19188795637773
At time: 173.30168795585632 and batch: 450, loss is 4.755290365219116 and perplexity is 116.19738794647019
At time: 173.8212251663208 and batch: 500, loss is 4.7839171504974365 and perplexity is 119.57181467576062
At time: 174.33194828033447 and batch: 550, loss is 4.706877126693725 and perplexity is 110.70589902575938
At time: 174.83601713180542 and batch: 600, loss is 4.657541580200196 and perplexity is 105.3767032824286
At time: 175.34213542938232 and batch: 650, loss is 4.642885656356811 and perplexity is 103.84357251061601
At time: 175.88982963562012 and batch: 700, loss is 4.72446912765503 and perplexity is 112.67066875239856
At time: 176.40033984184265 and batch: 750, loss is 4.667422933578491 and perplexity is 106.42312926334756
At time: 176.91191792488098 and batch: 800, loss is 4.762036056518554 and perplexity is 116.98386935339185
At time: 177.41472911834717 and batch: 850, loss is 4.6671199703216555 and perplexity is 106.39089184912588
At time: 177.91771793365479 and batch: 900, loss is 4.686528329849243 and perplexity is 108.4759326899395
At time: 178.41903686523438 and batch: 950, loss is 4.647319259643555 and perplexity is 104.30499584316478
At time: 178.92277646064758 and batch: 1000, loss is 4.577122793197632 and perplexity is 97.23422836951463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.049505094202553 and perplexity of 155.9452671684645
Finished 16 epochs...
Completing Train Step...
At time: 180.442209482193 and batch: 50, loss is 4.784464225769043 and perplexity is 119.63724735541943
At time: 180.949613571167 and batch: 100, loss is 4.708833303451538 and perplexity is 110.92267128568908
At time: 181.46662616729736 and batch: 150, loss is 4.734271030426026 and perplexity is 113.78048596719682
At time: 181.98172998428345 and batch: 200, loss is 4.757437963485717 and perplexity is 116.44720140887172
At time: 182.505131483078 and batch: 250, loss is 4.810719041824341 and perplexity is 122.81989841845567
At time: 183.01899242401123 and batch: 300, loss is 4.723650274276733 and perplexity is 112.57844575836394
At time: 183.53786969184875 and batch: 350, loss is 4.707657880783081 and perplexity is 110.79236685980469
At time: 184.05388927459717 and batch: 400, loss is 4.706111974716187 and perplexity is 110.62122458673299
At time: 184.56891536712646 and batch: 450, loss is 4.749806518554688 and perplexity is 115.56192327607529
At time: 185.09519457817078 and batch: 500, loss is 4.778059883117676 and perplexity is 118.8734976972138
At time: 185.6037883758545 and batch: 550, loss is 4.70152886390686 and perplexity is 110.11539527788287
At time: 186.10905194282532 and batch: 600, loss is 4.652798767089844 and perplexity is 104.87810458742317
At time: 186.6248004436493 and batch: 650, loss is 4.638181972503662 and perplexity is 103.35627212726209
At time: 187.14239883422852 and batch: 700, loss is 4.719737195968628 and perplexity is 112.13877827198084
At time: 187.64942622184753 and batch: 750, loss is 4.662774639129639 and perplexity is 105.92959116620327
At time: 188.15378189086914 and batch: 800, loss is 4.757874879837036 and perplexity is 116.49809021149802
At time: 188.65970063209534 and batch: 850, loss is 4.663609762191772 and perplexity is 106.018092360299
At time: 189.20022678375244 and batch: 900, loss is 4.683556461334229 and perplexity is 108.15403503660866
At time: 189.71648979187012 and batch: 950, loss is 4.642797727584838 and perplexity is 103.8344420742278
At time: 190.2219274044037 and batch: 1000, loss is 4.573496494293213 and perplexity is 96.88226653879947
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0491489317359 and perplexity of 155.88973520723403
Finished 17 epochs...
Completing Train Step...
At time: 191.81321120262146 and batch: 50, loss is 4.778541736602783 and perplexity is 118.93079110877257
At time: 192.33357334136963 and batch: 100, loss is 4.703537759780883 and perplexity is 110.33682798427917
At time: 192.83704948425293 and batch: 150, loss is 4.728826742172242 and perplexity is 113.16271539038989
At time: 193.34016609191895 and batch: 200, loss is 4.752352657318116 and perplexity is 115.85653487034543
At time: 193.85875844955444 and batch: 250, loss is 4.806245698928833 and perplexity is 122.27170993011865
At time: 194.36150789260864 and batch: 300, loss is 4.718319959640503 and perplexity is 111.97996368710416
At time: 194.86312127113342 and batch: 350, loss is 4.7020893573760985 and perplexity is 110.17713153756765
At time: 195.36604237556458 and batch: 400, loss is 4.700511169433594 and perplexity is 110.00338845271762
At time: 195.8700897693634 and batch: 450, loss is 4.744665813446045 and perplexity is 114.96937786226846
At time: 196.38385796546936 and batch: 500, loss is 4.772784461975098 and perplexity is 118.24804115803586
At time: 196.9043529033661 and batch: 550, loss is 4.696574316024781 and perplexity is 109.57117258140568
At time: 197.42403554916382 and batch: 600, loss is 4.648509788513183 and perplexity is 104.42924790011718
At time: 197.9315366744995 and batch: 650, loss is 4.633831100463867 and perplexity is 102.90755906712769
At time: 198.44120717048645 and batch: 700, loss is 4.715718555450439 and perplexity is 111.68903711323514
At time: 198.9739487171173 and batch: 750, loss is 4.65828691482544 and perplexity is 105.45527346498055
At time: 199.48717880249023 and batch: 800, loss is 4.75241455078125 and perplexity is 115.86370585443152
At time: 200.00374293327332 and batch: 850, loss is 4.659710264205932 and perplexity is 105.60548003599257
At time: 200.52169346809387 and batch: 900, loss is 4.680496187210083 and perplexity is 107.82355997182471
At time: 201.03752207756042 and batch: 950, loss is 4.6381501293182374 and perplexity is 103.35298098672443
At time: 201.55060243606567 and batch: 1000, loss is 4.569511213302612 and perplexity is 96.49693182719196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.048796490925114 and perplexity of 155.83480298331182
Finished 18 epochs...
Completing Train Step...
At time: 203.07757711410522 and batch: 50, loss is 4.77316011428833 and perplexity is 118.29246965254292
At time: 203.61178469657898 and batch: 100, loss is 4.698488645553589 and perplexity is 109.78112881117144
At time: 204.11853194236755 and batch: 150, loss is 4.723673925399781 and perplexity is 112.58110839652414
At time: 204.6324462890625 and batch: 200, loss is 4.74688138961792 and perplexity is 115.224383664431
At time: 205.1419014930725 and batch: 250, loss is 4.801524810791015 and perplexity is 121.69583924518672
At time: 205.64882946014404 and batch: 300, loss is 4.7135618305206295 and perplexity is 111.44841415454414
At time: 206.14952039718628 and batch: 350, loss is 4.696879291534424 and perplexity is 109.60459420173474
At time: 206.66339874267578 and batch: 400, loss is 4.6953784847259525 and perplexity is 109.44022225663707
At time: 207.17609572410583 and batch: 450, loss is 4.739241380691528 and perplexity is 114.34742260551712
At time: 207.69203400611877 and batch: 500, loss is 4.767128562927246 and perplexity is 117.58112994371918
At time: 208.20107793807983 and batch: 550, loss is 4.691974124908447 and perplexity is 109.0682818303107
At time: 208.71471500396729 and batch: 600, loss is 4.643477945327759 and perplexity is 103.90509613140031
At time: 209.22078561782837 and batch: 650, loss is 4.629302911758423 and perplexity is 102.44262766345213
At time: 209.72281861305237 and batch: 700, loss is 4.7109067916870115 and perplexity is 111.15290675242446
At time: 210.2248785495758 and batch: 750, loss is 4.653505268096924 and perplexity is 104.95222725472125
At time: 210.72869777679443 and batch: 800, loss is 4.747950773239136 and perplexity is 115.34766864080481
At time: 211.23219513893127 and batch: 850, loss is 4.65598388671875 and perplexity is 105.21268645576451
At time: 211.73595237731934 and batch: 900, loss is 4.677138013839722 and perplexity is 107.46207706487915
At time: 212.23787808418274 and batch: 950, loss is 4.633377561569214 and perplexity is 102.8608970688523
At time: 212.74082374572754 and batch: 1000, loss is 4.565259981155395 and perplexity is 96.0875717274377
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.047863843964367 and perplexity of 155.68953188178662
Finished 19 epochs...
Completing Train Step...
At time: 214.24866676330566 and batch: 50, loss is 4.76699501991272 and perplexity is 117.56542885358394
At time: 214.751873254776 and batch: 100, loss is 4.692708806991577 and perplexity is 109.14844178524496
At time: 215.26922130584717 and batch: 150, loss is 4.718327741622925 and perplexity is 111.98083511660391
At time: 215.77375864982605 and batch: 200, loss is 4.741548681259156 and perplexity is 114.61156108485183
At time: 216.28336644172668 and batch: 250, loss is 4.797119617462158 and perplexity is 121.16092461221422
At time: 216.7924427986145 and batch: 300, loss is 4.708491106033325 and perplexity is 110.88472032768406
At time: 217.2943069934845 and batch: 350, loss is 4.691848783493042 and perplexity is 109.05461191421152
At time: 217.79660296440125 and batch: 400, loss is 4.690504894256592 and perplexity is 108.90815302937675
At time: 218.2985908985138 and batch: 450, loss is 4.734269332885742 and perplexity is 113.78029282040237
At time: 218.80794620513916 and batch: 500, loss is 4.762406568527222 and perplexity is 117.02722131252249
At time: 219.3246283531189 and batch: 550, loss is 4.687654762268067 and perplexity is 108.59819234287443
At time: 219.8254656791687 and batch: 600, loss is 4.639473123550415 and perplexity is 103.48980687442099
At time: 220.34645628929138 and batch: 650, loss is 4.625269861221313 and perplexity is 102.03030339013523
At time: 220.85187196731567 and batch: 700, loss is 4.706923389434815 and perplexity is 110.71102070257358
At time: 221.37240600585938 and batch: 750, loss is 4.649326410293579 and perplexity is 104.51456192836844
At time: 221.8787820339203 and batch: 800, loss is 4.74401086807251 and perplexity is 114.89410385301218
At time: 222.3929431438446 and batch: 850, loss is 4.652839937210083 and perplexity is 104.88242252048379
At time: 222.90898895263672 and batch: 900, loss is 4.673972053527832 and perplexity is 107.12239438852305
At time: 223.41807961463928 and batch: 950, loss is 4.62892861366272 and perplexity is 102.40429075816182
At time: 223.92060232162476 and batch: 1000, loss is 4.5616107749938966 and perplexity is 95.73756737607292
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.048427302662919 and perplexity of 155.77728122204178
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 225.39597296714783 and batch: 50, loss is 4.760855855941773 and perplexity is 116.84588636312526
At time: 225.92589282989502 and batch: 100, loss is 4.68368293762207 and perplexity is 108.16771482254116
At time: 226.4354772567749 and batch: 150, loss is 4.705313367843628 and perplexity is 110.53291698275021
At time: 226.94074845314026 and batch: 200, loss is 4.727930946350098 and perplexity is 113.0613900928855
At time: 227.45622205734253 and batch: 250, loss is 4.784319229125977 and perplexity is 119.61990161373473
At time: 227.98388481140137 and batch: 300, loss is 4.686439828872681 and perplexity is 108.46633288876508
At time: 228.4933512210846 and batch: 350, loss is 4.6719246768951415 and perplexity is 106.9032988634761
At time: 229.00346326828003 and batch: 400, loss is 4.668633146286011 and perplexity is 106.55200185264961
At time: 229.51075148582458 and batch: 450, loss is 4.713017587661743 and perplexity is 111.38777565353864
At time: 230.0174481868744 and batch: 500, loss is 4.737786741256714 and perplexity is 114.1812092548816
At time: 230.52383136749268 and batch: 550, loss is 4.662604551315308 and perplexity is 105.91157536574636
At time: 231.03087544441223 and batch: 600, loss is 4.610724954605103 and perplexity is 100.55702249497372
At time: 231.53649759292603 and batch: 650, loss is 4.592845993041992 and perplexity is 98.77514389004654
At time: 232.03986382484436 and batch: 700, loss is 4.670254783630371 and perplexity is 106.72493073404019
At time: 232.54568934440613 and batch: 750, loss is 4.616906366348267 and perplexity is 101.1805319536777
At time: 233.04932260513306 and batch: 800, loss is 4.705696144104004 and perplexity is 110.57523445790707
At time: 233.55492782592773 and batch: 850, loss is 4.619938650131226 and perplexity is 101.48780567499112
At time: 234.0749533176422 and batch: 900, loss is 4.6290007591247555 and perplexity is 102.41167902954491
At time: 234.5830361843109 and batch: 950, loss is 4.583222389221191 and perplexity is 97.82913036871906
At time: 235.08765935897827 and batch: 1000, loss is 4.517701797485351 and perplexity is 91.62478351590158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.033062074242569 and perplexity of 153.40202263979847
Finished 21 epochs...
Completing Train Step...
At time: 236.580632686615 and batch: 50, loss is 4.746561508178711 and perplexity is 115.18753141724163
At time: 237.09611320495605 and batch: 100, loss is 4.674400386810302 and perplexity is 107.16828830358153
At time: 237.59934401512146 and batch: 150, loss is 4.696985511779785 and perplexity is 109.61623704696555
At time: 238.1018886566162 and batch: 200, loss is 4.7209740829467775 and perplexity is 112.27756708191258
At time: 238.60347414016724 and batch: 250, loss is 4.777617902755737 and perplexity is 118.82096955473548
At time: 239.10559916496277 and batch: 300, loss is 4.6794694137573245 and perplexity is 107.71290642064966
At time: 239.60841035842896 and batch: 350, loss is 4.665301036834717 and perplexity is 106.19754978472025
At time: 240.11174988746643 and batch: 400, loss is 4.664683666229248 and perplexity is 106.13200677335642
At time: 240.61466455459595 and batch: 450, loss is 4.709766206741333 and perplexity is 111.02619969413225
At time: 241.1306140422821 and batch: 500, loss is 4.734760084152222 and perplexity is 113.83614434668706
At time: 241.64546394348145 and batch: 550, loss is 4.659241237640381 and perplexity is 105.55595987443135
At time: 242.1694107055664 and batch: 600, loss is 4.608696384429932 and perplexity is 100.35324227935152
At time: 242.67779636383057 and batch: 650, loss is 4.5911550617218015 and perplexity is 98.60826303737959
At time: 243.1827597618103 and batch: 700, loss is 4.668262119293213 and perplexity is 106.51247551694782
At time: 243.69288229942322 and batch: 750, loss is 4.617097816467285 and perplexity is 101.19990483297344
At time: 244.1960072517395 and batch: 800, loss is 4.705892839431763 and perplexity is 110.59698622905653
At time: 244.6987750530243 and batch: 850, loss is 4.620606899261475 and perplexity is 101.55564747795262
At time: 245.20205426216125 and batch: 900, loss is 4.629104175567627 and perplexity is 102.42227062876196
At time: 245.70266675949097 and batch: 950, loss is 4.585345697402954 and perplexity is 98.03707244607833
At time: 246.20540404319763 and batch: 1000, loss is 4.519728059768677 and perplexity is 91.81062757978074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0319764672256095 and perplexity of 153.2355786903115
Finished 22 epochs...
Completing Train Step...
At time: 247.7316472530365 and batch: 50, loss is 4.741994180679321 and perplexity is 114.66263184401156
At time: 248.23974704742432 and batch: 100, loss is 4.67121223449707 and perplexity is 106.82716354510686
At time: 248.74521660804749 and batch: 150, loss is 4.693967351913452 and perplexity is 109.28589648069551
At time: 249.25980424880981 and batch: 200, loss is 4.718099975585938 and perplexity is 111.95533258998583
At time: 249.7800018787384 and batch: 250, loss is 4.774404487609863 and perplexity is 118.43976126979003
At time: 250.29844570159912 and batch: 300, loss is 4.675922212600708 and perplexity is 107.33150393001037
At time: 250.80172777175903 and batch: 350, loss is 4.661971340179443 and perplexity is 105.84453220528577
At time: 251.30199575424194 and batch: 400, loss is 4.663058261871338 and perplexity is 105.95963946826642
At time: 251.80412793159485 and batch: 450, loss is 4.7084192943573 and perplexity is 110.87675779597654
At time: 252.3056411743164 and batch: 500, loss is 4.733631362915039 and perplexity is 113.70772756001487
At time: 252.80678343772888 and batch: 550, loss is 4.657381687164307 and perplexity is 105.35985562837598
At time: 253.3072988986969 and batch: 600, loss is 4.607941312789917 and perplexity is 100.27749699228067
At time: 253.83698868751526 and batch: 650, loss is 4.590156450271606 and perplexity is 98.50984084776842
At time: 254.3505618572235 and batch: 700, loss is 4.666969251632691 and perplexity is 106.37485796172217
At time: 254.8547809123993 and batch: 750, loss is 4.617388687133789 and perplexity is 101.2293451982041
At time: 255.38671708106995 and batch: 800, loss is 4.706053400039673 and perplexity is 110.61474517405388
At time: 255.915673494339 and batch: 850, loss is 4.620900049209594 and perplexity is 101.58542287485686
At time: 256.43034529685974 and batch: 900, loss is 4.628828468322754 and perplexity is 102.39403595914365
At time: 256.94664311408997 and batch: 950, loss is 4.586322441101074 and perplexity is 98.13287631907009
At time: 257.45432710647583 and batch: 1000, loss is 4.52017765045166 and perplexity is 91.85191406285306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.031500839605564 and perplexity of 153.16271294656653
Finished 23 epochs...
Completing Train Step...
At time: 258.9954664707184 and batch: 50, loss is 4.738706369400024 and perplexity is 114.28626180558695
At time: 259.5181710720062 and batch: 100, loss is 4.6688769245147705 and perplexity is 106.5779800772662
At time: 260.0418870449066 and batch: 150, loss is 4.6919409275054935 and perplexity is 109.06466110670895
At time: 260.55570554733276 and batch: 200, loss is 4.716170358657837 and perplexity is 111.73950997947689
At time: 261.0621304512024 and batch: 250, loss is 4.7719666862487795 and perplexity is 118.15138030912696
At time: 261.5669147968292 and batch: 300, loss is 4.673418760299683 and perplexity is 107.06314068697226
At time: 262.0727059841156 and batch: 350, loss is 4.65940185546875 and perplexity is 105.5729154051216
At time: 262.57831478118896 and batch: 400, loss is 4.662054605484009 and perplexity is 105.85334574942262
At time: 263.0851683616638 and batch: 450, loss is 4.707614974975586 and perplexity is 110.78761332581809
At time: 263.59102034568787 and batch: 500, loss is 4.732718019485474 and perplexity is 113.60392076699956
At time: 264.1088891029358 and batch: 550, loss is 4.6561190795898435 and perplexity is 105.22691142245732
At time: 264.6162338256836 and batch: 600, loss is 4.607440338134766 and perplexity is 100.22727308930722
At time: 265.1213536262512 and batch: 650, loss is 4.589353084564209 and perplexity is 98.43073320023042
At time: 265.6263725757599 and batch: 700, loss is 4.665968408584595 and perplexity is 106.26844668401534
At time: 266.1343505382538 and batch: 750, loss is 4.617485542297363 and perplexity is 101.2391502578194
At time: 266.6476254463196 and batch: 800, loss is 4.705993061065674 and perplexity is 110.60807099517945
At time: 267.1793668270111 and batch: 850, loss is 4.621018362045288 and perplexity is 101.59744244532305
At time: 267.6861231327057 and batch: 900, loss is 4.628319873809814 and perplexity is 102.34197215510183
At time: 268.1908280849457 and batch: 950, loss is 4.586779441833496 and perplexity is 98.17773336449319
At time: 268.6964602470398 and batch: 1000, loss is 4.520157766342163 and perplexity is 91.85008768749434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.031247394840892 and perplexity of 153.1238995775576
Finished 24 epochs...
Completing Train Step...
At time: 270.25149393081665 and batch: 50, loss is 4.735988130569458 and perplexity is 113.97602628908349
At time: 270.7757787704468 and batch: 100, loss is 4.667023067474365 and perplexity is 106.38058276827765
At time: 271.2805950641632 and batch: 150, loss is 4.690375633239746 and perplexity is 108.89407636057524
At time: 271.7856798171997 and batch: 200, loss is 4.714577465057373 and perplexity is 111.56166251276197
At time: 272.2897741794586 and batch: 250, loss is 4.769994421005249 and perplexity is 117.91858409167385
At time: 272.7928464412689 and batch: 300, loss is 4.671319808959961 and perplexity is 106.83865603798561
At time: 273.29382824897766 and batch: 350, loss is 4.657244243621826 and perplexity is 105.34537559169974
At time: 273.8142206668854 and batch: 400, loss is 4.661273946762085 and perplexity is 105.77074265842528
At time: 274.3445146083832 and batch: 450, loss is 4.706975507736206 and perplexity is 110.71679092328361
At time: 274.8603837490082 and batch: 500, loss is 4.731933860778809 and perplexity is 113.51487218209346
At time: 275.37932419776917 and batch: 550, loss is 4.654971885681152 and perplexity is 105.1062649663143
At time: 275.89381670951843 and batch: 600, loss is 4.60700026512146 and perplexity is 100.18317547502005
At time: 276.4076108932495 and batch: 650, loss is 4.588737697601318 and perplexity is 98.37017884436278
At time: 276.9342291355133 and batch: 700, loss is 4.665057420730591 and perplexity is 106.17168150246697
At time: 277.4606020450592 and batch: 750, loss is 4.6174570655822755 and perplexity is 101.23626734043003
At time: 277.9741098880768 and batch: 800, loss is 4.705850639343262 and perplexity is 110.59231912492652
At time: 278.4935965538025 and batch: 850, loss is 4.6210426998138425 and perplexity is 101.59991513045269
At time: 279.0126724243164 and batch: 900, loss is 4.627702369689941 and perplexity is 102.27879507372165
At time: 279.53475856781006 and batch: 950, loss is 4.586929082870483 and perplexity is 98.19242588159727
At time: 280.0784764289856 and batch: 1000, loss is 4.519794187545776 and perplexity is 91.81669901323968
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.03094482421875 and perplexity of 153.07757579245924
Finished 25 epochs...
Completing Train Step...
At time: 281.66751885414124 and batch: 50, loss is 4.7336983966827395 and perplexity is 113.71535007288988
At time: 282.1823732852936 and batch: 100, loss is 4.665298576354981 and perplexity is 106.19728848812241
At time: 282.6899552345276 and batch: 150, loss is 4.688996505737305 and perplexity is 108.74400105529072
At time: 283.20333075523376 and batch: 200, loss is 4.713207025527954 and perplexity is 111.40887871487675
At time: 283.71710443496704 and batch: 250, loss is 4.768162937164306 and perplexity is 117.70281575878388
At time: 284.2201383113861 and batch: 300, loss is 4.6694814968109135 and perplexity is 106.64243365285971
At time: 284.7236313819885 and batch: 350, loss is 4.655381155014038 and perplexity is 105.14929054118281
At time: 285.23477029800415 and batch: 400, loss is 4.6605335521698 and perplexity is 105.69245955630782
At time: 285.7546179294586 and batch: 450, loss is 4.706385326385498 and perplexity is 110.65146721638338
At time: 286.28046226501465 and batch: 500, loss is 4.7311662006378175 and perplexity is 113.42776477802828
At time: 286.78394865989685 and batch: 550, loss is 4.654034357070923 and perplexity is 105.0077710134578
At time: 287.2844407558441 and batch: 600, loss is 4.606558094024658 and perplexity is 100.13888716266655
At time: 287.8010244369507 and batch: 650, loss is 4.58790280342102 and perplexity is 98.28808442937405
At time: 288.3119478225708 and batch: 700, loss is 4.664218215942383 and perplexity is 106.08261909500439
At time: 288.8208918571472 and batch: 750, loss is 4.617457599639892 and perplexity is 101.23632140644415
At time: 289.3349175453186 and batch: 800, loss is 4.705543889999389 and perplexity is 110.55840020616662
At time: 289.84347009658813 and batch: 850, loss is 4.6209182453155515 and perplexity is 101.58727135079266
At time: 290.3469066619873 and batch: 900, loss is 4.626992311477661 and perplexity is 102.20619695283364
At time: 290.85366773605347 and batch: 950, loss is 4.586903944015503 and perplexity is 98.1899574674695
At time: 291.35836958885193 and batch: 1000, loss is 4.519297819137574 and perplexity is 91.77113541360528
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.03086815810785 and perplexity of 153.06584037991726
Finished 26 epochs...
Completing Train Step...
At time: 292.85234689712524 and batch: 50, loss is 4.731634292602539 and perplexity is 113.48087183182011
At time: 293.36840653419495 and batch: 100, loss is 4.663728170394897 and perplexity is 106.03064651535693
At time: 293.87139225006104 and batch: 150, loss is 4.687792587280273 and perplexity is 108.61316092155828
At time: 294.3750524520874 and batch: 200, loss is 4.711889905929565 and perplexity is 111.26223649115994
At time: 294.8789310455322 and batch: 250, loss is 4.766604051589966 and perplexity is 117.51947347918441
At time: 295.39076256752014 and batch: 300, loss is 4.667837724685669 and perplexity is 106.46728178736818
At time: 295.8988206386566 and batch: 350, loss is 4.653740940093994 and perplexity is 104.9769644705353
At time: 296.4024109840393 and batch: 400, loss is 4.659951438903809 and perplexity is 105.63095247726507
At time: 296.90588092803955 and batch: 450, loss is 4.705878744125366 and perplexity is 110.5954273416356
At time: 297.4089159965515 and batch: 500, loss is 4.730442457199096 and perplexity is 113.34570187732643
At time: 297.91257977485657 and batch: 550, loss is 4.653147926330567 and perplexity is 104.91473014049241
At time: 298.41651487350464 and batch: 600, loss is 4.606081161499024 and perplexity is 100.09113905751512
At time: 298.9191246032715 and batch: 650, loss is 4.587055377960205 and perplexity is 98.20482788597359
At time: 299.42135858535767 and batch: 700, loss is 4.663323173522949 and perplexity is 105.98771312971786
At time: 299.9241101741791 and batch: 750, loss is 4.617339925765991 and perplexity is 101.22440923721405
At time: 300.4251911640167 and batch: 800, loss is 4.705207567214966 and perplexity is 110.521223149264
At time: 300.927845954895 and batch: 850, loss is 4.620697994232177 and perplexity is 101.56489910806641
At time: 301.4289405345917 and batch: 900, loss is 4.6262766551971435 and perplexity is 102.13307861299681
At time: 301.9302930831909 and batch: 950, loss is 4.586766233444214 and perplexity is 98.17643660333619
At time: 302.43454933166504 and batch: 1000, loss is 4.518677444458008 and perplexity is 91.71422058096555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.030731573337462 and perplexity of 153.0449353449419
Finished 27 epochs...
Completing Train Step...
At time: 303.9502670764923 and batch: 50, loss is 4.729773597717285 and perplexity is 113.26991487815516
At time: 304.4704656600952 and batch: 100, loss is 4.662243804931641 and perplexity is 105.87337503867384
At time: 304.975928068161 and batch: 150, loss is 4.6866374111175535 and perplexity is 108.48776602764448
At time: 305.4792242050171 and batch: 200, loss is 4.710571823120117 and perplexity is 111.11568025774429
At time: 305.9804036617279 and batch: 250, loss is 4.76511432647705 and perplexity is 117.34453210791654
At time: 306.49537324905396 and batch: 300, loss is 4.666209802627564 and perplexity is 106.29410235043835
At time: 306.99707770347595 and batch: 350, loss is 4.652058820724488 and perplexity is 104.80052911951053
At time: 307.4989502429962 and batch: 400, loss is 4.65919041633606 and perplexity is 105.55059551918457
At time: 307.9998092651367 and batch: 450, loss is 4.705296545028687 and perplexity is 110.53105752358361
At time: 308.50278186798096 and batch: 500, loss is 4.729663410186768 and perplexity is 113.2574346335491
At time: 309.0044183731079 and batch: 550, loss is 4.652189502716064 and perplexity is 104.81422555629347
At time: 309.5115661621094 and batch: 600, loss is 4.605511627197266 and perplexity is 100.03414995068587
At time: 310.01738381385803 and batch: 650, loss is 4.586191778182983 and perplexity is 98.12005482875453
At time: 310.52374958992004 and batch: 700, loss is 4.662282133102417 and perplexity is 105.8774330492406
At time: 311.0305619239807 and batch: 750, loss is 4.6169430446624755 and perplexity is 101.18424315308039
At time: 311.54471588134766 and batch: 800, loss is 4.70462965965271 and perplexity is 110.45737055084308
At time: 312.04562497138977 and batch: 850, loss is 4.620279979705811 and perplexity is 101.52245237716342
At time: 312.5491874217987 and batch: 900, loss is 4.625449256896973 and perplexity is 102.04860882725869
At time: 313.0566408634186 and batch: 950, loss is 4.5864280128479 and perplexity is 98.1432369251282
At time: 313.5609362125397 and batch: 1000, loss is 4.517939701080322 and perplexity is 91.64658397438933
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.030581962771532 and perplexity of 153.0220399182937
Finished 28 epochs...
Completing Train Step...
At time: 315.0706613063812 and batch: 50, loss is 4.727960443496704 and perplexity is 113.06472513047137
At time: 315.57708644866943 and batch: 100, loss is 4.660950574874878 and perplexity is 105.73654490335483
At time: 316.08572697639465 and batch: 150, loss is 4.685765600204467 and perplexity is 108.39322642560157
At time: 316.58656191825867 and batch: 200, loss is 4.709673004150391 and perplexity is 111.01585224687027
At time: 317.08799934387207 and batch: 250, loss is 4.763836870193481 and perplexity is 117.19472530422657
At time: 317.5974054336548 and batch: 300, loss is 4.664835214614868 and perplexity is 106.14809212646944
At time: 318.1107528209686 and batch: 350, loss is 4.650619134902954 and perplexity is 104.64975784132564
At time: 318.6134181022644 and batch: 400, loss is 4.658550233840942 and perplexity is 105.48304550006088
At time: 319.16754937171936 and batch: 450, loss is 4.704751224517822 and perplexity is 110.47079910239822
At time: 319.6793394088745 and batch: 500, loss is 4.729016332626343 and perplexity is 113.18417199490753
At time: 320.18330669403076 and batch: 550, loss is 4.651350765228272 and perplexity is 104.72635079314603
At time: 320.69561529159546 and batch: 600, loss is 4.605183458328247 and perplexity is 100.00132724282332
At time: 321.1998300552368 and batch: 650, loss is 4.58533576965332 and perplexity is 98.0360991633995
At time: 321.71235251426697 and batch: 700, loss is 4.661373310089111 and perplexity is 105.78125291347612
At time: 322.230642080307 and batch: 750, loss is 4.616725158691406 and perplexity is 101.16219892765503
At time: 322.7438278198242 and batch: 800, loss is 4.704190330505371 and perplexity is 110.40885406655488
At time: 323.24643635749817 and batch: 850, loss is 4.61995906829834 and perplexity is 101.48987789112283
At time: 323.74973249435425 and batch: 900, loss is 4.624674434661865 and perplexity is 101.96956992058297
At time: 324.2531213760376 and batch: 950, loss is 4.586083011627197 and perplexity is 98.10938322870439
At time: 324.76389336586 and batch: 1000, loss is 4.517190952301025 and perplexity is 91.57798938977216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.030401090296303 and perplexity of 152.99436494606502
Finished 29 epochs...
Completing Train Step...
At time: 326.2733919620514 and batch: 50, loss is 4.7262286186218265 and perplexity is 112.86908628202832
At time: 326.79033875465393 and batch: 100, loss is 4.659640960693359 and perplexity is 105.59816145888358
At time: 327.29360914230347 and batch: 150, loss is 4.684633178710937 and perplexity is 108.2705490805852
At time: 327.79657435417175 and batch: 200, loss is 4.70865647315979 and perplexity is 110.9030585314798
At time: 328.3002438545227 and batch: 250, loss is 4.762626161575318 and perplexity is 117.05292249855826
At time: 328.8028528690338 and batch: 300, loss is 4.663341979980469 and perplexity is 105.98970640188563
At time: 329.30599546432495 and batch: 350, loss is 4.648962020874023 and perplexity is 104.47648486567296
At time: 329.81742548942566 and batch: 400, loss is 4.6579896926879885 and perplexity is 105.42393448074642
At time: 330.3330388069153 and batch: 450, loss is 4.7042258644104 and perplexity is 110.41277739399482
At time: 330.8367648124695 and batch: 500, loss is 4.728339405059814 and perplexity is 113.10758043518322
At time: 331.3379077911377 and batch: 550, loss is 4.650467596054077 and perplexity is 104.6339005390161
At time: 331.84155011177063 and batch: 600, loss is 4.604610109329224 and perplexity is 99.94400801547873
At time: 332.3744032382965 and batch: 650, loss is 4.584622383117676 and perplexity is 97.96618647060106
At time: 332.8871839046478 and batch: 700, loss is 4.660313520431519 and perplexity is 105.66920641901638
At time: 333.39089369773865 and batch: 750, loss is 4.616422157287598 and perplexity is 101.1315512827418
At time: 333.8929228782654 and batch: 800, loss is 4.703675241470337 and perplexity is 110.3519983205972
At time: 334.39574456214905 and batch: 850, loss is 4.6195355415344235 and perplexity is 101.44690331265376
At time: 334.9002318382263 and batch: 900, loss is 4.623760032653808 and perplexity is 101.8763713580571
At time: 335.40262937545776 and batch: 950, loss is 4.585588493347168 and perplexity is 98.06087833952105
At time: 335.90692925453186 and batch: 1000, loss is 4.51642373085022 and perplexity is 91.50775573771809
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0302581787109375 and perplexity of 152.97250184110126
Finished 30 epochs...
Completing Train Step...
At time: 337.4140591621399 and batch: 50, loss is 4.724628953933716 and perplexity is 112.68867792523356
At time: 337.93637585639954 and batch: 100, loss is 4.6584989452362064 and perplexity is 105.47763556056923
At time: 338.4412245750427 and batch: 150, loss is 4.6835661888122555 and perplexity is 108.15508710772497
At time: 338.9464077949524 and batch: 200, loss is 4.707725219726562 and perplexity is 110.79982775193626
At time: 339.4520606994629 and batch: 250, loss is 4.761534461975097 and perplexity is 116.92520559679178
At time: 339.95759654045105 and batch: 300, loss is 4.662045974731445 and perplexity is 105.85243215932985
At time: 340.4625928401947 and batch: 350, loss is 4.6476663398742675 and perplexity is 104.3412043284477
At time: 340.967453956604 and batch: 400, loss is 4.6574313354492185 and perplexity is 105.36508669436213
At time: 341.4699423313141 and batch: 450, loss is 4.7038333702087405 and perplexity is 110.36944952260389
At time: 341.9727683067322 and batch: 500, loss is 4.727589855194092 and perplexity is 113.022832428852
At time: 342.4763255119324 and batch: 550, loss is 4.649701204299927 and perplexity is 104.55374070128124
At time: 342.9824695587158 and batch: 600, loss is 4.604072532653809 and perplexity is 99.89029488667822
At time: 343.48856496810913 and batch: 650, loss is 4.583967199325562 and perplexity is 97.9020216352257
At time: 343.99397706985474 and batch: 700, loss is 4.659596891403198 and perplexity is 105.59350792540549
At time: 344.5139248371124 and batch: 750, loss is 4.616099872589111 and perplexity is 101.09896338280197
At time: 345.0508756637573 and batch: 800, loss is 4.703173246383667 and perplexity is 110.29661606161004
At time: 345.55590438842773 and batch: 850, loss is 4.619150037765503 and perplexity is 101.40780268628508
At time: 346.0618176460266 and batch: 900, loss is 4.62298466682434 and perplexity is 101.79741051660092
At time: 346.5688247680664 and batch: 950, loss is 4.585147094726563 and perplexity is 98.0176039544162
At time: 347.07888650894165 and batch: 1000, loss is 4.515580883026123 and perplexity is 91.43066111898635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.030229894126334 and perplexity of 152.96817513862072
Finished 31 epochs...
Completing Train Step...
At time: 348.59261894226074 and batch: 50, loss is 4.723153476715088 and perplexity is 112.52253095133729
At time: 349.0954473018646 and batch: 100, loss is 4.657564992904663 and perplexity is 105.37917046492191
At time: 349.60071301460266 and batch: 150, loss is 4.6825331115722655 and perplexity is 108.04341224315733
At time: 350.1064496040344 and batch: 200, loss is 4.706755065917969 and perplexity is 110.6923870025043
At time: 350.6109538078308 and batch: 250, loss is 4.760465002059936 and perplexity is 116.80022561878445
At time: 351.11587262153625 and batch: 300, loss is 4.660829935073853 and perplexity is 105.72378963702843
At time: 351.62092423439026 and batch: 350, loss is 4.646386518478393 and perplexity is 104.20775163869637
At time: 352.1245126724243 and batch: 400, loss is 4.656798324584961 and perplexity is 105.29841055535778
At time: 352.64164781570435 and batch: 450, loss is 4.70330843925476 and perplexity is 110.31152838580456
At time: 353.1534249782562 and batch: 500, loss is 4.726937351226806 and perplexity is 112.94910863744727
At time: 353.65840435028076 and batch: 550, loss is 4.648899250030517 and perplexity is 104.4699269944151
At time: 354.1610298156738 and batch: 600, loss is 4.603564147949219 and perplexity is 99.8395250950071
At time: 354.664009809494 and batch: 650, loss is 4.583217401504516 and perplexity is 97.82864242595106
At time: 355.1659126281738 and batch: 700, loss is 4.658624963760376 and perplexity is 105.49092853409833
At time: 355.66767835617065 and batch: 750, loss is 4.61574089050293 and perplexity is 101.06267717945418
At time: 356.17249488830566 and batch: 800, loss is 4.702829923629761 and perplexity is 110.25875522325276
At time: 356.67672204971313 and batch: 850, loss is 4.6187359237670895 and perplexity is 101.3658169896767
At time: 357.18724036216736 and batch: 900, loss is 4.622256450653076 and perplexity is 101.7233069810411
At time: 357.696604013443 and batch: 950, loss is 4.584648323059082 and perplexity is 97.96872774069797
At time: 358.2227318286896 and batch: 1000, loss is 4.514816312789917 and perplexity is 91.3607826737119
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.030148762028392 and perplexity of 152.95576501308992
Finished 32 epochs...
Completing Train Step...
At time: 359.7254214286804 and batch: 50, loss is 4.721782093048096 and perplexity is 112.36832515204706
At time: 360.2587535381317 and batch: 100, loss is 4.656648149490357 and perplexity is 105.28259854390583
At time: 360.76761865615845 and batch: 150, loss is 4.681479835510254 and perplexity is 107.92967261351335
At time: 361.2701208591461 and batch: 200, loss is 4.705910348892212 and perplexity is 110.59892273956632
At time: 361.7749435901642 and batch: 250, loss is 4.759401798248291 and perplexity is 116.67610916594022
At time: 362.28922033309937 and batch: 300, loss is 4.6596214485168455 and perplexity is 105.5961010290195
At time: 362.80557894706726 and batch: 350, loss is 4.645113344192505 and perplexity is 104.075161432051
At time: 363.31010723114014 and batch: 400, loss is 4.65611704826355 and perplexity is 105.22669767248242
At time: 363.81360483169556 and batch: 450, loss is 4.703005542755127 and perplexity is 110.27812046981313
At time: 364.31771993637085 and batch: 500, loss is 4.726296405792237 and perplexity is 112.87673761734243
At time: 364.83062720298767 and batch: 550, loss is 4.648056745529175 and perplexity is 104.38194767735678
At time: 365.35417890548706 and batch: 600, loss is 4.603015804290772 and perplexity is 99.78479373172829
At time: 365.86783957481384 and batch: 650, loss is 4.582569103240967 and perplexity is 97.76524084073036
At time: 366.3715319633484 and batch: 700, loss is 4.657935285568238 and perplexity is 105.41819882415022
At time: 366.8754301071167 and batch: 750, loss is 4.615431175231934 and perplexity is 101.03138137164895
At time: 367.37921667099 and batch: 800, loss is 4.702285375595093 and perplexity is 110.19873037947963
At time: 367.88669300079346 and batch: 850, loss is 4.61837251663208 and perplexity is 101.32898662115129
At time: 368.39057421684265 and batch: 900, loss is 4.621549320220947 and perplexity is 101.65140076155238
At time: 368.8934986591339 and batch: 950, loss is 4.584274940490722 and perplexity is 97.932154753798
At time: 369.39599418640137 and batch: 1000, loss is 4.514107961654663 and perplexity is 91.29609007483018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.030145784703697 and perplexity of 152.9553096147914
Finished 33 epochs...
Completing Train Step...
At time: 370.9003601074219 and batch: 50, loss is 4.720487823486328 and perplexity is 112.22298432448937
At time: 371.4177384376526 and batch: 100, loss is 4.655639095306396 and perplexity is 105.17641627818612
At time: 371.92110776901245 and batch: 150, loss is 4.680505952835083 and perplexity is 107.82461294141903
At time: 372.4246151447296 and batch: 200, loss is 4.704938344955444 and perplexity is 110.49147238080685
At time: 372.92887902259827 and batch: 250, loss is 4.758355560302735 and perplexity is 116.55410202856855
At time: 373.44727659225464 and batch: 300, loss is 4.658254718780517 and perplexity is 105.45187827691485
At time: 373.95365595817566 and batch: 350, loss is 4.643910894393921 and perplexity is 103.95009148538523
At time: 374.47624468803406 and batch: 400, loss is 4.655479469299316 and perplexity is 105.1596287267172
At time: 374.99664306640625 and batch: 450, loss is 4.702426977157593 and perplexity is 110.21433579673659
At time: 375.5066375732422 and batch: 500, loss is 4.725657072067261 and perplexity is 112.80459477635137
At time: 376.02436447143555 and batch: 550, loss is 4.647277708053589 and perplexity is 104.30066189478794
At time: 376.5361931324005 and batch: 600, loss is 4.602521982192993 and perplexity is 99.73552996033185
At time: 377.03962755203247 and batch: 650, loss is 4.581853036880493 and perplexity is 97.69525949917318
At time: 377.5429093837738 and batch: 700, loss is 4.657028245925903 and perplexity is 105.32262369056335
At time: 378.04596066474915 and batch: 750, loss is 4.614976825714112 and perplexity is 100.9854882387887
At time: 378.5583322048187 and batch: 800, loss is 4.701914644241333 and perplexity is 110.1578838270001
At time: 379.07974123954773 and batch: 850, loss is 4.617854347229004 and perplexity is 101.27649464168285
At time: 379.58802676200867 and batch: 900, loss is 4.620675535202026 and perplexity is 101.56261808454992
At time: 380.0917990207672 and batch: 950, loss is 4.583658103942871 and perplexity is 97.87176524867775
At time: 380.5961060523987 and batch: 1000, loss is 4.513377084732055 and perplexity is 91.22938824785246
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.029921741020389 and perplexity of 152.92104478239676
Finished 34 epochs...
Completing Train Step...
At time: 382.09838461875916 and batch: 50, loss is 4.719139518737793 and perplexity is 112.07177550252344
At time: 382.6042511463165 and batch: 100, loss is 4.654632835388184 and perplexity is 105.07063469695262
At time: 383.10841512680054 and batch: 150, loss is 4.679573469161987 and perplexity is 107.72411511386707
At time: 383.6154475212097 and batch: 200, loss is 4.7039585781097415 and perplexity is 110.38326951488132
At time: 384.13982939720154 and batch: 250, loss is 4.757484550476074 and perplexity is 116.45262645988826
At time: 384.64535331726074 and batch: 300, loss is 4.657201271057129 and perplexity is 105.34084872799778
At time: 385.15107774734497 and batch: 350, loss is 4.6426262855529785 and perplexity is 103.81664201238452
At time: 385.6554055213928 and batch: 400, loss is 4.654829969406128 and perplexity is 105.09134973509066
At time: 386.1609604358673 and batch: 450, loss is 4.701896629333496 and perplexity is 110.15589936075052
At time: 386.66712260246277 and batch: 500, loss is 4.7250537109375 and perplexity is 112.73655339743071
At time: 387.172242641449 and batch: 550, loss is 4.646424350738525 and perplexity is 104.21169412804028
At time: 387.6790678501129 and batch: 600, loss is 4.6020527267456055 and perplexity is 99.68873949879797
At time: 388.1989018917084 and batch: 650, loss is 4.5810739707946775 and perplexity is 97.61917807582887
At time: 388.7140712738037 and batch: 700, loss is 4.656374626159668 and perplexity is 105.25380523488892
At time: 389.22317934036255 and batch: 750, loss is 4.614627304077149 and perplexity is 100.95019779337639
At time: 389.72991847991943 and batch: 800, loss is 4.701348094940186 and perplexity is 110.09549163069859
At time: 390.23585510253906 and batch: 850, loss is 4.617386703491211 and perplexity is 101.22914439556396
At time: 390.7421681880951 and batch: 900, loss is 4.619968748092651 and perplexity is 101.49086029702019
At time: 391.2619915008545 and batch: 950, loss is 4.583167991638184 and perplexity is 97.82380884521956
At time: 391.77583503723145 and batch: 1000, loss is 4.512662038803101 and perplexity is 91.16417836199496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.029766920136242 and perplexity of 152.8973712436653
Finished 35 epochs...
Completing Train Step...
At time: 393.30579566955566 and batch: 50, loss is 4.717948627471924 and perplexity is 111.93838964371837
At time: 393.837167263031 and batch: 100, loss is 4.653713121414184 and perplexity is 104.97404419059266
At time: 394.36817049980164 and batch: 150, loss is 4.678756895065308 and perplexity is 107.6361862969604
At time: 394.88488936424255 and batch: 200, loss is 4.7030666732788085 and perplexity is 110.28486203512365
At time: 395.39052081108093 and batch: 250, loss is 4.756642990112304 and perplexity is 116.35466577090075
At time: 395.89250683784485 and batch: 300, loss is 4.655985298156739 and perplexity is 105.21283495705187
At time: 396.40177392959595 and batch: 350, loss is 4.641406049728394 and perplexity is 103.69003848540163
At time: 396.9263753890991 and batch: 400, loss is 4.6542919254302975 and perplexity is 105.03482117624276
At time: 397.4510941505432 and batch: 450, loss is 4.701334104537964 and perplexity is 110.0939513612624
At time: 397.954354763031 and batch: 500, loss is 4.724343709945678 and perplexity is 112.65653874130788
At time: 398.4576108455658 and batch: 550, loss is 4.645554533004761 and perplexity is 104.12108835938686
At time: 398.97250843048096 and batch: 600, loss is 4.601559896469116 and perplexity is 99.63962197404366
At time: 399.4785921573639 and batch: 650, loss is 4.580380802154541 and perplexity is 97.5515349696592
At time: 399.98054361343384 and batch: 700, loss is 4.655470218658447 and perplexity is 105.15865593725736
At time: 400.4846270084381 and batch: 750, loss is 4.614237337112427 and perplexity is 100.91083822611913
At time: 401.00216603279114 and batch: 800, loss is 4.7009033107757565 and perplexity is 110.04653378808987
At time: 401.51177430152893 and batch: 850, loss is 4.61695408821106 and perplexity is 101.18536059235582
At time: 402.0281910896301 and batch: 900, loss is 4.619242601394653 and perplexity is 101.41718979497158
At time: 402.55742287635803 and batch: 950, loss is 4.582590589523315 and perplexity is 97.76734147486621
At time: 403.0678942203522 and batch: 1000, loss is 4.511999387741088 and perplexity is 91.103788333354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.029698813833842 and perplexity of 152.8869583236599
Finished 36 epochs...
Completing Train Step...
At time: 404.5804851055145 and batch: 50, loss is 4.716613388061523 and perplexity is 111.78902483540998
At time: 405.11340975761414 and batch: 100, loss is 4.652738990783692 and perplexity is 104.87183554910654
At time: 405.62396121025085 and batch: 150, loss is 4.677897558212281 and perplexity is 107.54373028647423
At time: 406.12856936454773 and batch: 200, loss is 4.702347936630249 and perplexity is 110.20562474178212
At time: 406.6300308704376 and batch: 250, loss is 4.755885267257691 and perplexity is 116.2665345751325
At time: 407.13361644744873 and batch: 300, loss is 4.654931173324585 and perplexity is 105.101985929683
At time: 407.6377329826355 and batch: 350, loss is 4.640294065475464 and perplexity is 103.57480087849666
At time: 408.1418602466583 and batch: 400, loss is 4.6537144947052 and perplexity is 104.97418835060346
At time: 408.6446166038513 and batch: 450, loss is 4.700948972702026 and perplexity is 110.0515588395278
At time: 409.1567704677582 and batch: 500, loss is 4.723700857162475 and perplexity is 112.5841404450484
At time: 409.6619610786438 and batch: 550, loss is 4.6447646522521975 and perplexity is 104.038877588383
At time: 410.1807494163513 and batch: 600, loss is 4.601031112670898 and perplexity is 99.58694808406044
At time: 410.705162525177 and batch: 650, loss is 4.579649305343628 and perplexity is 97.4802024258741
At time: 411.21702456474304 and batch: 700, loss is 4.654766540527344 and perplexity is 105.08468412000555
At time: 411.73343229293823 and batch: 750, loss is 4.613820018768311 and perplexity is 100.86873506802839
At time: 412.2414617538452 and batch: 800, loss is 4.700215892791748 and perplexity is 109.97091181661563
At time: 412.745192527771 and batch: 850, loss is 4.616559829711914 and perplexity is 101.14547526703406
At time: 413.24859166145325 and batch: 900, loss is 4.618424987792968 and perplexity is 101.334303610204
At time: 413.7662355899811 and batch: 950, loss is 4.582096996307373 and perplexity is 97.71909608615135
At time: 414.2856888771057 and batch: 1000, loss is 4.511346979141235 and perplexity is 91.0443708227211
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.029573394031059 and perplexity of 152.86778447391433
Finished 37 epochs...
Completing Train Step...
At time: 415.7981827259064 and batch: 50, loss is 4.715391035079956 and perplexity is 111.65246266818575
At time: 416.31617975234985 and batch: 100, loss is 4.651902275085449 and perplexity is 104.78412433778998
At time: 416.82465291023254 and batch: 150, loss is 4.677100925445557 and perplexity is 107.45809154291598
At time: 417.3414845466614 and batch: 200, loss is 4.701524076461792 and perplexity is 110.11486810773864
At time: 417.85677218437195 and batch: 250, loss is 4.754925794601441 and perplexity is 116.15503351403422
At time: 418.36271357536316 and batch: 300, loss is 4.653657913208008 and perplexity is 104.96824892189248
At time: 418.8788089752197 and batch: 350, loss is 4.63917423248291 and perplexity is 103.45887931778387
At time: 419.39206862449646 and batch: 400, loss is 4.6531718063354495 and perplexity is 104.91723553467473
At time: 419.8986990451813 and batch: 450, loss is 4.700397338867187 and perplexity is 109.9908674173552
At time: 420.40494990348816 and batch: 500, loss is 4.723030509948731 and perplexity is 112.50869527024582
At time: 420.91142773628235 and batch: 550, loss is 4.643899040222168 and perplexity is 103.94885925045064
At time: 421.43399715423584 and batch: 600, loss is 4.600555410385132 and perplexity is 99.53958561133578
At time: 421.94705963134766 and batch: 650, loss is 4.578937768936157 and perplexity is 97.41086638332513
At time: 422.4712510108948 and batch: 700, loss is 4.653930597305298 and perplexity is 104.99687599699052
At time: 423.0159294605255 and batch: 750, loss is 4.613397960662842 and perplexity is 100.82617158356813
At time: 423.53151869773865 and batch: 800, loss is 4.6998598003387455 and perplexity is 109.93175897629712
At time: 424.03701543807983 and batch: 850, loss is 4.616033945083618 and perplexity is 101.09229840004346
At time: 424.543744802475 and batch: 900, loss is 4.617563381195068 and perplexity is 101.24703090838669
At time: 425.0503852367401 and batch: 950, loss is 4.581776676177978 and perplexity is 97.68779970534662
At time: 425.55711817741394 and batch: 1000, loss is 4.51051365852356 and perplexity is 90.9685332742762
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0295637177258 and perplexity of 152.866305285724
Finished 38 epochs...
Completing Train Step...
At time: 427.0608506202698 and batch: 50, loss is 4.714188985824585 and perplexity is 111.5183315408367
At time: 427.59039282798767 and batch: 100, loss is 4.650775127410888 and perplexity is 104.66608369282815
At time: 428.09729075431824 and batch: 150, loss is 4.676310625076294 and perplexity is 107.37320092245203
At time: 428.60589361190796 and batch: 200, loss is 4.700709323883057 and perplexity is 110.02518827338994
At time: 429.1104209423065 and batch: 250, loss is 4.75398097038269 and perplexity is 116.04533925429726
At time: 429.6300058364868 and batch: 300, loss is 4.652699489593505 and perplexity is 104.86769306860232
At time: 430.13971281051636 and batch: 350, loss is 4.638071794509887 and perplexity is 103.34488516786277
At time: 430.6428232192993 and batch: 400, loss is 4.652581453323364 and perplexity is 104.85531560776322
At time: 431.14660358428955 and batch: 450, loss is 4.699992218017578 and perplexity is 109.94631684848929
At time: 431.664701461792 and batch: 500, loss is 4.72243103981018 and perplexity is 112.44126987887756
At time: 432.1726031303406 and batch: 550, loss is 4.643058567047119 and perplexity is 103.86152972687547
At time: 432.6766722202301 and batch: 600, loss is 4.599991331100464 and perplexity is 99.48345322613417
At time: 433.18032717704773 and batch: 650, loss is 4.578227481842041 and perplexity is 97.34170126855842
At time: 433.6990375518799 and batch: 700, loss is 4.653231763839722 and perplexity is 104.92352629885995
At time: 434.20590829849243 and batch: 750, loss is 4.61291072845459 and perplexity is 100.7770577912203
At time: 434.7104663848877 and batch: 800, loss is 4.699195976257324 and perplexity is 109.85880784341394
At time: 435.22480821609497 and batch: 850, loss is 4.615447225570679 and perplexity is 101.03300297255744
At time: 435.75988268852234 and batch: 900, loss is 4.616693649291992 and perplexity is 101.15901141774194
At time: 436.285924911499 and batch: 950, loss is 4.581157112121582 and perplexity is 97.62729460123012
At time: 436.7900185585022 and batch: 1000, loss is 4.509869441986084 and perplexity is 90.90994871334864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.029479236137576 and perplexity of 152.85339144296623
Finished 39 epochs...
Completing Train Step...
At time: 438.3032691478729 and batch: 50, loss is 4.7130922603607175 and perplexity is 111.39609358993697
At time: 438.8328297138214 and batch: 100, loss is 4.650015611648559 and perplexity is 104.58661833389857
At time: 439.33576369285583 and batch: 150, loss is 4.675475673675537 and perplexity is 107.28358693480651
At time: 439.8395268917084 and batch: 200, loss is 4.69988844871521 and perplexity is 109.93490838782625
At time: 440.3426070213318 and batch: 250, loss is 4.753110704421997 and perplexity is 115.94439287711619
At time: 440.8459634780884 and batch: 300, loss is 4.6518503761291505 and perplexity is 104.77868629221582
At time: 441.34952116012573 and batch: 350, loss is 4.636839990615845 and perplexity is 103.21766290840753
At time: 441.853022813797 and batch: 400, loss is 4.652049903869629 and perplexity is 104.79959463256958
At time: 442.3620743751526 and batch: 450, loss is 4.699414167404175 and perplexity is 109.88278067792396
At time: 442.8761293888092 and batch: 500, loss is 4.721840276718139 and perplexity is 112.37486334380723
At time: 443.3796148300171 and batch: 550, loss is 4.642271032333374 and perplexity is 103.7797673663673
At time: 443.8947446346283 and batch: 600, loss is 4.5995198917388915 and perplexity is 99.43656386407216
At time: 444.40172123908997 and batch: 650, loss is 4.577610921859741 and perplexity is 97.28170276918482
At time: 444.9053077697754 and batch: 700, loss is 4.652640266418457 and perplexity is 104.8614826547608
At time: 445.4077959060669 and batch: 750, loss is 4.612511253356933 and perplexity is 100.73680790616619
At time: 445.91135597229004 and batch: 800, loss is 4.698718452453614 and perplexity is 109.80636017112406
At time: 446.4154145717621 and batch: 850, loss is 4.615015096664429 and perplexity is 100.9893531233478
At time: 446.9185004234314 and batch: 900, loss is 4.6160845375061035 and perplexity is 101.09741303369388
At time: 447.42274141311646 and batch: 950, loss is 4.580722255706787 and perplexity is 97.58484997524147
At time: 447.927063703537 and batch: 1000, loss is 4.509253120422363 and perplexity is 90.85393621422837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.02943569276391 and perplexity of 152.84673583553123
Finished 40 epochs...
Completing Train Step...
At time: 449.4484808444977 and batch: 50, loss is 4.711897687911987 and perplexity is 111.26310233529755
At time: 449.95385551452637 and batch: 100, loss is 4.649032535552979 and perplexity is 104.48385225121302
At time: 450.45659041404724 and batch: 150, loss is 4.674706249237061 and perplexity is 107.20107206971916
At time: 450.9565908908844 and batch: 200, loss is 4.699114151000977 and perplexity is 109.84981898606293
At time: 451.45842838287354 and batch: 250, loss is 4.7522172927856445 and perplexity is 115.84085306607297
At time: 451.974148273468 and batch: 300, loss is 4.650726814270019 and perplexity is 104.66102706773417
At time: 452.48458099365234 and batch: 350, loss is 4.635743818283081 and perplexity is 103.10458055227313
At time: 453.00341749191284 and batch: 400, loss is 4.651271829605102 and perplexity is 104.7180844796415
At time: 453.52387142181396 and batch: 450, loss is 4.698852434158325 and perplexity is 109.82107320006426
At time: 454.029723405838 and batch: 500, loss is 4.7212269115448 and perplexity is 112.30595765060991
At time: 454.5440237522125 and batch: 550, loss is 4.641448345184326 and perplexity is 103.69442419560215
At time: 455.06488370895386 and batch: 600, loss is 4.59923300743103 and perplexity is 99.40804116582477
At time: 455.5749571323395 and batch: 650, loss is 4.576788759231567 and perplexity is 97.20175425860505
At time: 456.0891160964966 and batch: 700, loss is 4.651683435440064 and perplexity is 104.76119592609211
At time: 456.6141748428345 and batch: 750, loss is 4.611920232772827 and perplexity is 100.67728796960292
At time: 457.12481594085693 and batch: 800, loss is 4.698245077133179 and perplexity is 109.75439285118607
At time: 457.640567779541 and batch: 850, loss is 4.6145728588104244 and perplexity is 100.94470168255162
At time: 458.1555972099304 and batch: 900, loss is 4.615278310775757 and perplexity is 101.01593844484616
At time: 458.66119480133057 and batch: 950, loss is 4.580159978866577 and perplexity is 97.5299956972353
At time: 459.17361974716187 and batch: 1000, loss is 4.508354768753052 and perplexity is 90.7723540791771
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.029490028939596 and perplexity of 152.85504116826058
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 460.70284271240234 and batch: 50, loss is 4.710421161651611 and perplexity is 111.09894066721998
At time: 461.22271275520325 and batch: 100, loss is 4.647219095230103 and perplexity is 104.29454871765982
At time: 461.73015451431274 and batch: 150, loss is 4.671727018356323 and perplexity is 106.88217060178737
At time: 462.25077843666077 and batch: 200, loss is 4.696785850524902 and perplexity is 109.59435311628043
At time: 462.75741696357727 and batch: 250, loss is 4.74871072769165 and perplexity is 115.43536093202864
At time: 463.2625455856323 and batch: 300, loss is 4.645794925689697 and perplexity is 104.14612131612532
At time: 463.768616437912 and batch: 350, loss is 4.630913362503052 and perplexity is 102.60773938591726
At time: 464.27402210235596 and batch: 400, loss is 4.646024570465088 and perplexity is 104.17004067513562
At time: 464.79189825057983 and batch: 450, loss is 4.694319839477539 and perplexity is 109.32442519018164
At time: 465.2989511489868 and batch: 500, loss is 4.715128650665283 and perplexity is 111.623170645175
At time: 465.80243372917175 and batch: 550, loss is 4.635340452194214 and perplexity is 103.06300004751805
At time: 466.30979180336 and batch: 600, loss is 4.592111616134644 and perplexity is 98.70263233401863
At time: 466.81500458717346 and batch: 650, loss is 4.567951135635376 and perplexity is 96.3465064870077
At time: 467.3258743286133 and batch: 700, loss is 4.645227775573731 and perplexity is 104.08707157795921
At time: 467.8316333293915 and batch: 750, loss is 4.60306794166565 and perplexity is 99.78999638455134
At time: 468.3357334136963 and batch: 800, loss is 4.689429101943969 and perplexity is 108.79105347425995
At time: 468.8392210006714 and batch: 850, loss is 4.605327510833741 and perplexity is 100.01573372218517
At time: 469.3426179885864 and batch: 900, loss is 4.603992938995361 and perplexity is 99.88234456906645
At time: 469.8470447063446 and batch: 950, loss is 4.570914192199707 and perplexity is 96.63241000046658
At time: 470.3513298034668 and batch: 1000, loss is 4.496138801574707 and perplexity is 89.67022745817454
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0269343678544205 and perplexity of 152.4648942418832
Finished 42 epochs...
Completing Train Step...
At time: 471.827917098999 and batch: 50, loss is 4.708868598937988 and perplexity is 110.92658642442339
At time: 472.34153413772583 and batch: 100, loss is 4.646150217056275 and perplexity is 104.18313010795431
At time: 472.84324979782104 and batch: 150, loss is 4.671362724304199 and perplexity is 106.84324115407266
At time: 473.3460464477539 and batch: 200, loss is 4.695664129257202 and perplexity is 109.47148772281554
At time: 473.8497543334961 and batch: 250, loss is 4.747760448455811 and perplexity is 115.32571720978059
At time: 474.35624957084656 and batch: 300, loss is 4.644529857635498 and perplexity is 104.01445268752764
At time: 474.880206823349 and batch: 350, loss is 4.629721403121948 and perplexity is 102.48550799028357
At time: 475.4019374847412 and batch: 400, loss is 4.644853200912475 and perplexity is 104.04809049949974
At time: 475.9040596485138 and batch: 450, loss is 4.693547134399414 and perplexity is 109.23998228060813
At time: 476.4074282646179 and batch: 500, loss is 4.714061822891235 and perplexity is 111.50415144428618
At time: 476.9087133407593 and batch: 550, loss is 4.6344954299926755 and perplexity is 102.97594631067108
At time: 477.40962171554565 and batch: 600, loss is 4.5915217685699465 and perplexity is 98.64442999364906
At time: 477.91084480285645 and batch: 650, loss is 4.567390089035034 and perplexity is 96.29246676790623
At time: 478.41174602508545 and batch: 700, loss is 4.645126142501831 and perplexity is 104.0764934266838
At time: 478.9126510620117 and batch: 750, loss is 4.602904691696167 and perplexity is 99.77370700034376
At time: 479.4283924102783 and batch: 800, loss is 4.689242238998413 and perplexity is 108.77072635680928
At time: 479.9417359828949 and batch: 850, loss is 4.6054798603057865 and perplexity is 100.03097222717365
At time: 480.4514226913452 and batch: 900, loss is 4.604235563278198 and perplexity is 99.90658139138752
At time: 480.953866481781 and batch: 950, loss is 4.571460313796997 and perplexity is 96.68519745943978
At time: 481.4566352367401 and batch: 1000, loss is 4.496599063873291 and perplexity is 89.71150878256886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.026579321884528 and perplexity of 152.41077180417776
Finished 43 epochs...
Completing Train Step...
At time: 482.990576505661 and batch: 50, loss is 4.708303546905517 and perplexity is 110.86392483650528
At time: 483.4976317882538 and batch: 100, loss is 4.64557523727417 and perplexity is 104.1232441327682
At time: 484.00137877464294 and batch: 150, loss is 4.671174068450927 and perplexity is 106.8230864524574
At time: 484.5045394897461 and batch: 200, loss is 4.695112991333008 and perplexity is 109.41117045740653
At time: 485.00930666923523 and batch: 250, loss is 4.747322063446045 and perplexity is 115.27517122422607
At time: 485.5118546485901 and batch: 300, loss is 4.6438275241851805 and perplexity is 103.94142550580682
At time: 486.01612663269043 and batch: 350, loss is 4.6290744972229 and perplexity is 102.4192309504131
At time: 486.5338158607483 and batch: 400, loss is 4.644118604660034 and perplexity is 103.97168522909364
At time: 487.038743019104 and batch: 450, loss is 4.693020181655884 and perplexity is 109.18243313641358
At time: 487.5420472621918 and batch: 500, loss is 4.713404655456543 and perplexity is 111.4308986194458
At time: 488.0717339515686 and batch: 550, loss is 4.634004669189453 and perplexity is 102.92542215119965
At time: 488.57973074913025 and batch: 600, loss is 4.591114187240601 and perplexity is 98.60423255815827
At time: 489.101105928421 and batch: 650, loss is 4.56697380065918 and perplexity is 96.252389675701
At time: 489.60717606544495 and batch: 700, loss is 4.645117416381836 and perplexity is 104.0755852466759
At time: 490.1169719696045 and batch: 750, loss is 4.602805919647217 and perplexity is 99.76385263354805
At time: 490.628497838974 and batch: 800, loss is 4.6890670013427735 and perplexity is 108.7516672997013
At time: 491.1315817832947 and batch: 850, loss is 4.605591497421265 and perplexity is 100.04214001973004
At time: 491.6336715221405 and batch: 900, loss is 4.60438798904419 and perplexity is 99.92181088923812
At time: 492.13430070877075 and batch: 950, loss is 4.571819248199463 and perplexity is 96.7199073319282
At time: 492.6359360218048 and batch: 1000, loss is 4.4968612766265865 and perplexity is 89.73503536863966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.026448319597942 and perplexity of 152.3908069523203
Finished 44 epochs...
Completing Train Step...
At time: 494.1263825893402 and batch: 50, loss is 4.708044528961182 and perplexity is 110.83521280921882
At time: 494.6475763320923 and batch: 100, loss is 4.645285243988037 and perplexity is 104.09305346879583
At time: 495.16680669784546 and batch: 150, loss is 4.671001176834107 and perplexity is 106.80461923278662
At time: 495.67895674705505 and batch: 200, loss is 4.694724826812744 and perplexity is 109.36870916443134
At time: 496.18774819374084 and batch: 250, loss is 4.7469923877716065 and perplexity is 115.23717406811984
At time: 496.69743156433105 and batch: 300, loss is 4.643356990814209 and perplexity is 103.89252910107723
At time: 497.21348118782043 and batch: 350, loss is 4.6285888290405275 and perplexity is 102.36950126571767
At time: 497.7256295681 and batch: 400, loss is 4.643543996810913 and perplexity is 103.91195944377054
At time: 498.2426471710205 and batch: 450, loss is 4.692622146606445 and perplexity is 109.13898334908508
At time: 498.7568521499634 and batch: 500, loss is 4.712887344360351 and perplexity is 111.37326908661493
At time: 499.26284527778625 and batch: 550, loss is 4.633628454208374 and perplexity is 102.88670734845348
At time: 499.7693660259247 and batch: 600, loss is 4.590796823501587 and perplexity is 98.57294411540222
At time: 500.2776165008545 and batch: 650, loss is 4.566630582809449 and perplexity is 96.21935980602969
At time: 500.7840225696564 and batch: 700, loss is 4.645152931213379 and perplexity is 104.0792815391899
At time: 501.30392146110535 and batch: 750, loss is 4.60272476196289 and perplexity is 99.75575635883071
At time: 501.8215322494507 and batch: 800, loss is 4.688935079574585 and perplexity is 108.73732153373788
At time: 502.3457555770874 and batch: 850, loss is 4.605694274902344 and perplexity is 100.05242262728429
At time: 502.8586320877075 and batch: 900, loss is 4.604530658721924 and perplexity is 99.93606771878075
At time: 503.3640854358673 and batch: 950, loss is 4.572091646194458 and perplexity is 96.74625722942821
At time: 503.8700497150421 and batch: 1000, loss is 4.496981630325317 and perplexity is 89.74583596198464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.026324388457508 and perplexity of 152.3719221560543
Finished 45 epochs...
Completing Train Step...
At time: 505.3701808452606 and batch: 50, loss is 4.7076913928985595 and perplexity is 110.79607980861104
At time: 505.90438532829285 and batch: 100, loss is 4.644983997344971 and perplexity is 104.06170050859595
At time: 506.41742968559265 and batch: 150, loss is 4.670896787643432 and perplexity is 106.79347056693483
At time: 506.94486904144287 and batch: 200, loss is 4.6944436359405515 and perplexity is 109.33796000510486
At time: 507.4677999019623 and batch: 250, loss is 4.746710119247436 and perplexity is 115.20465083142503
At time: 507.98961114883423 and batch: 300, loss is 4.642960996627807 and perplexity is 103.85139640823374
At time: 508.5006248950958 and batch: 350, loss is 4.628190937042237 and perplexity is 102.32877736268988
At time: 509.00561475753784 and batch: 400, loss is 4.643044815063477 and perplexity is 103.86010143463852
At time: 509.5096275806427 and batch: 450, loss is 4.692272748947143 and perplexity is 109.10085710476245
At time: 510.0152690410614 and batch: 500, loss is 4.712466497421264 and perplexity is 111.32640784861964
At time: 510.538937330246 and batch: 550, loss is 4.6333042430877684 and perplexity is 102.85335574054187
At time: 511.0510847568512 and batch: 600, loss is 4.590506639480591 and perplexity is 98.54434397197038
At time: 511.55409121513367 and batch: 650, loss is 4.566336612701416 and perplexity is 96.19107834758807
At time: 512.058171749115 and batch: 700, loss is 4.645189275741577 and perplexity is 104.08306432031398
At time: 512.561461687088 and batch: 750, loss is 4.60265100479126 and perplexity is 99.74839892772279
At time: 513.0657901763916 and batch: 800, loss is 4.688812532424927 and perplexity is 108.72399690138698
At time: 513.570433139801 and batch: 850, loss is 4.6057737445831295 and perplexity is 100.06037407731769
At time: 514.0928983688354 and batch: 900, loss is 4.604625587463379 and perplexity is 99.94555497421474
At time: 514.597489118576 and batch: 950, loss is 4.572311201095581 and perplexity is 96.76750067633432
At time: 515.102246761322 and batch: 1000, loss is 4.497039775848389 and perplexity is 89.75105443227405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.02623432438548 and perplexity of 152.35819953824893
Finished 46 epochs...
Completing Train Step...
At time: 516.6130127906799 and batch: 50, loss is 4.707406883239746 and perplexity is 110.76456173755724
At time: 517.1299502849579 and batch: 100, loss is 4.644723110198974 and perplexity is 104.03455568956385
At time: 517.642706155777 and batch: 150, loss is 4.670826721191406 and perplexity is 106.78598818948753
At time: 518.1465635299683 and batch: 200, loss is 4.694202556610107 and perplexity is 109.31160405997912
At time: 518.6496243476868 and batch: 250, loss is 4.746460418701172 and perplexity is 115.17588775840726
At time: 519.1670732498169 and batch: 300, loss is 4.642595834732056 and perplexity is 103.81348075854146
At time: 519.6726336479187 and batch: 350, loss is 4.627845582962036 and perplexity is 102.29344380355128
At time: 520.1756558418274 and batch: 400, loss is 4.642598237991333 and perplexity is 103.81373024955195
At time: 520.6929240226746 and batch: 450, loss is 4.691961507797242 and perplexity is 109.06690571235119
At time: 521.2014558315277 and batch: 500, loss is 4.7120947265625 and perplexity is 111.28502762682757
At time: 521.7052733898163 and batch: 550, loss is 4.633014650344848 and perplexity is 102.82357446756284
At time: 522.2083864212036 and batch: 600, loss is 4.590259714126587 and perplexity is 98.52001387893206
At time: 522.7125248908997 and batch: 650, loss is 4.566040019989014 and perplexity is 96.1625530051655
At time: 523.2214143276215 and batch: 700, loss is 4.64521125793457 and perplexity is 104.0853523194687
At time: 523.7298238277435 and batch: 750, loss is 4.602567071914673 and perplexity is 99.74002710900619
At time: 524.2343306541443 and batch: 800, loss is 4.68868730545044 and perplexity is 108.71038257665904
At time: 524.7577707767487 and batch: 850, loss is 4.605819368362427 and perplexity is 100.06493931388191
At time: 525.2638413906097 and batch: 900, loss is 4.604671630859375 and perplexity is 99.95015691292413
At time: 525.765453338623 and batch: 950, loss is 4.572488584518433 and perplexity is 96.78466714930394
At time: 526.2795548439026 and batch: 1000, loss is 4.49705138206482 and perplexity is 89.75209610848161
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.026191897508575 and perplexity of 152.3517355927951
Finished 47 epochs...
Completing Train Step...
At time: 527.7923510074615 and batch: 50, loss is 4.707162218093872 and perplexity is 110.73746482487229
At time: 528.3095483779907 and batch: 100, loss is 4.644489307403564 and perplexity is 104.0102349628621
At time: 528.8166995048523 and batch: 150, loss is 4.670772857666016 and perplexity is 106.78023647460658
At time: 529.3300745487213 and batch: 200, loss is 4.693979749679565 and perplexity is 109.28725139007847
At time: 529.8496797084808 and batch: 250, loss is 4.74624174118042 and perplexity is 115.15070413446834
At time: 530.3595502376556 and batch: 300, loss is 4.64225513458252 and perplexity is 103.77811751459647
At time: 530.8746793270111 and batch: 350, loss is 4.627551898956299 and perplexity is 102.26340626620251
At time: 531.3900792598724 and batch: 400, loss is 4.642191400527954 and perplexity is 103.77150352516252
At time: 531.897159576416 and batch: 450, loss is 4.691674976348877 and perplexity is 109.03565909067245
At time: 532.3994708061218 and batch: 500, loss is 4.711773977279663 and perplexity is 111.2493387579213
At time: 532.907557964325 and batch: 550, loss is 4.632764759063721 and perplexity is 102.79788296298423
At time: 533.4104671478271 and batch: 600, loss is 4.590025272369385 and perplexity is 98.49691938102181
At time: 533.9291472434998 and batch: 650, loss is 4.565751647949218 and perplexity is 96.13482641158276
At time: 534.436680316925 and batch: 700, loss is 4.6452202796936035 and perplexity is 104.08629135667208
At time: 534.9526336193085 and batch: 750, loss is 4.602480545043945 and perplexity is 99.73139728993513
At time: 535.4741313457489 and batch: 800, loss is 4.688561944961548 and perplexity is 108.69675544412146
At time: 535.996545791626 and batch: 850, loss is 4.605841646194458 and perplexity is 100.06716856862349
At time: 536.5048072338104 and batch: 900, loss is 4.604698219299316 and perplexity is 99.95281446699828
At time: 537.0071318149567 and batch: 950, loss is 4.572627067565918 and perplexity is 96.79807111305009
At time: 537.5140414237976 and batch: 1000, loss is 4.4970332050323485 and perplexity is 89.75046469654345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.026155797446647 and perplexity of 152.34623578497764
Finished 48 epochs...
Completing Train Step...
At time: 539.025283575058 and batch: 50, loss is 4.706943635940552 and perplexity is 110.7132622365809
At time: 539.5615191459656 and batch: 100, loss is 4.64427845954895 and perplexity is 103.98830693978182
At time: 540.0753960609436 and batch: 150, loss is 4.670730667114258 and perplexity is 106.77573145254821
At time: 540.6099598407745 and batch: 200, loss is 4.693766393661499 and perplexity is 109.26393678454063
At time: 541.1374180316925 and batch: 250, loss is 4.746032829284668 and perplexity is 115.12665029522455
At time: 541.651627779007 and batch: 300, loss is 4.6419484996795655 and perplexity is 103.74630039997183
At time: 542.1575200557709 and batch: 350, loss is 4.627279739379883 and perplexity is 102.23557808789464
At time: 542.6639473438263 and batch: 400, loss is 4.6418268013000485 and perplexity is 103.73367541156817
At time: 543.1756911277771 and batch: 450, loss is 4.691411800384522 and perplexity is 109.00696730160237
At time: 543.6901421546936 and batch: 500, loss is 4.711481037139893 and perplexity is 111.21675413398135
At time: 544.2061958312988 and batch: 550, loss is 4.6325278568267825 and perplexity is 102.7735327989758
At time: 544.7230303287506 and batch: 600, loss is 4.589804916381836 and perplexity is 98.47521738625119
At time: 545.2415225505829 and batch: 650, loss is 4.565478038787842 and perplexity is 96.10852664044246
At time: 545.7556400299072 and batch: 700, loss is 4.645225419998169 and perplexity is 104.08682639328583
At time: 546.275218963623 and batch: 750, loss is 4.602400131225586 and perplexity is 99.72337782991082
At time: 546.7852232456207 and batch: 800, loss is 4.68843864440918 and perplexity is 108.68335390036043
At time: 547.2899439334869 and batch: 850, loss is 4.605850629806518 and perplexity is 100.06806753728388
At time: 547.7947947978973 and batch: 900, loss is 4.60472430229187 and perplexity is 99.95542156951409
At time: 548.3014578819275 and batch: 950, loss is 4.572747631072998 and perplexity is 96.80974213151737
At time: 548.8087365627289 and batch: 1000, loss is 4.496993494033814 and perplexity is 89.74690068673705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.026132351014672 and perplexity of 152.34266385119844
Finished 49 epochs...
Completing Train Step...
At time: 550.3110375404358 and batch: 50, loss is 4.706741361618042 and perplexity is 110.6908700512276
At time: 550.8126213550568 and batch: 100, loss is 4.644082279205322 and perplexity is 103.96790847894704
At time: 551.3235054016113 and batch: 150, loss is 4.6706951713562015 and perplexity is 106.77194143428346
At time: 551.8329031467438 and batch: 200, loss is 4.69356369972229 and perplexity is 109.24179189117393
At time: 552.3426516056061 and batch: 250, loss is 4.74583930015564 and perplexity is 115.10437209067528
At time: 552.8570964336395 and batch: 300, loss is 4.641663761138916 and perplexity is 103.71676403506841
At time: 553.3813488483429 and batch: 350, loss is 4.6270334434509275 and perplexity is 102.21040098185378
At time: 553.8818521499634 and batch: 400, loss is 4.64149169921875 and perplexity is 103.69891986469027
At time: 554.3854620456696 and batch: 450, loss is 4.6911623001098635 and perplexity is 108.97977342590175
At time: 554.8865964412689 and batch: 500, loss is 4.711201229095459 and perplexity is 111.18563914481396
At time: 555.3907284736633 and batch: 550, loss is 4.632305841445923 and perplexity is 102.75071802665808
At time: 555.8963322639465 and batch: 600, loss is 4.589583110809326 and perplexity is 98.4533774564794
At time: 556.4012038707733 and batch: 650, loss is 4.565220756530762 and perplexity is 96.08380280242255
At time: 556.9016313552856 and batch: 700, loss is 4.64522232055664 and perplexity is 104.08650378275351
At time: 557.423388004303 and batch: 750, loss is 4.6023226070404055 and perplexity is 99.71564715596205
At time: 557.9298884868622 and batch: 800, loss is 4.688313179016113 and perplexity is 108.66971875603075
At time: 558.4284739494324 and batch: 850, loss is 4.60585129737854 and perplexity is 100.06813433994833
At time: 558.9286510944366 and batch: 900, loss is 4.60473331451416 and perplexity is 99.95632239405158
At time: 559.4297442436218 and batch: 950, loss is 4.57284574508667 and perplexity is 96.81924098985836
At time: 559.9305994510651 and batch: 1000, loss is 4.496921682357788 and perplexity is 89.74045604278378
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.026087691144245 and perplexity of 152.33586039949216
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd6d92888d0>
SETTINGS FOR THIS RUN
{'anneal': 4.820525634626296, 'data': 'ptb', 'dropout': 0.8464481937026472, 'tune_wordvecs': True, 'lr': 18.548676794132753, 'num_layers': 1, 'batch_size': 20, 'wordvec_source': '', 'wordvec_dim': 200, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7661316394805908 and batch: 50, loss is 7.132900829315186 and perplexity is 1252.5050054460994
At time: 1.270801067352295 and batch: 100, loss is 6.600323476791382 and perplexity is 735.3330139384602
At time: 1.775963306427002 and batch: 150, loss is 6.3911316776275635 and perplexity is 596.5312789475078
At time: 2.2818329334259033 and batch: 200, loss is 6.3415808010101316 and perplexity is 567.6930121291576
At time: 2.801729440689087 and batch: 250, loss is 6.402160844802856 and perplexity is 603.1469376894453
At time: 3.3146588802337646 and batch: 300, loss is 6.305671291351318 and perplexity is 547.6691103510149
At time: 3.8393986225128174 and batch: 350, loss is 6.276656799316406 and perplexity is 532.0070809536967
At time: 4.358179569244385 and batch: 400, loss is 6.254025468826294 and perplexity is 520.1022718263453
At time: 4.865995168685913 and batch: 450, loss is 6.288343000411987 and perplexity is 538.2606919943582
At time: 5.392822265625 and batch: 500, loss is 6.265166940689087 and perplexity is 525.9293776438349
At time: 5.905919790267944 and batch: 550, loss is 6.226744203567505 and perplexity is 506.1050232432847
At time: 6.411994934082031 and batch: 600, loss is 6.142705392837525 and perplexity is 465.31071783755146
At time: 6.933462142944336 and batch: 650, loss is 6.1076140785217286 and perplexity is 449.2655231396664
At time: 7.457978248596191 and batch: 700, loss is 6.183737754821777 and perplexity is 484.8006399687453
At time: 7.961265802383423 and batch: 750, loss is 6.044746074676514 and perplexity is 421.89061516933117
At time: 8.483975887298584 and batch: 800, loss is 6.155538101196289 and perplexity is 471.32039230390666
At time: 9.011241436004639 and batch: 850, loss is 6.134055004119873 and perplexity is 461.3029585829158
At time: 9.522917032241821 and batch: 900, loss is 6.1418528175354 and perplexity is 464.9141744772672
At time: 10.029721975326538 and batch: 950, loss is 6.118270530700683 and perplexity is 454.0786998477681
At time: 10.536092758178711 and batch: 1000, loss is 6.047727489471436 and perplexity is 423.15032301392733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.851869908774772 and perplexity of 347.884284534974
Finished 1 epochs...
Completing Train Step...
At time: 12.046979665756226 and batch: 50, loss is 5.729671907424927 and perplexity is 307.86824252092856
At time: 12.550247430801392 and batch: 100, loss is 5.548063354492188 and perplexity is 256.73986003448056
At time: 13.055259227752686 and batch: 150, loss is 5.448603372573853 and perplexity is 232.43331636918657
At time: 13.56687331199646 and batch: 200, loss is 5.39031792640686 and perplexity is 219.27308717810163
At time: 14.083019733428955 and batch: 250, loss is 5.431563138961792 and perplexity is 228.50615327424043
At time: 14.604406595230103 and batch: 300, loss is 5.3437152862548825 and perplexity is 209.28883556662885
At time: 15.129225492477417 and batch: 350, loss is 5.306258983612061 and perplexity is 201.5946470699006
At time: 15.63633418083191 and batch: 400, loss is 5.303851423263549 and perplexity is 201.10987957879456
At time: 16.141542196273804 and batch: 450, loss is 5.318921899795532 and perplexity is 204.16365442302637
At time: 16.646494388580322 and batch: 500, loss is 5.338569955825806 and perplexity is 208.21474100789882
At time: 17.151833534240723 and batch: 550, loss is 5.282891674041748 and perplexity is 196.93853481558912
At time: 17.656960248947144 and batch: 600, loss is 5.192783365249634 and perplexity is 179.96877529329362
At time: 18.173404216766357 and batch: 650, loss is 5.173114080429077 and perplexity is 176.463504289548
At time: 18.67630362510681 and batch: 700, loss is 5.227306175231933 and perplexity is 186.29029358028993
At time: 19.197428226470947 and batch: 750, loss is 5.174312343597412 and perplexity is 176.6750807441143
At time: 19.715282201766968 and batch: 800, loss is 5.267806472778321 and perplexity is 193.9899731198369
At time: 20.233671188354492 and batch: 850, loss is 5.186784152984619 and perplexity is 178.8923365300165
At time: 20.74009394645691 and batch: 900, loss is 5.201106672286987 and perplexity is 181.47296188678607
At time: 21.246189832687378 and batch: 950, loss is 5.205336132049561 and perplexity is 182.24211989191912
At time: 21.75053834915161 and batch: 1000, loss is 5.13081127166748 and perplexity is 169.1542924794187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.2921016041825455 and perplexity of 198.76070310449268
Finished 2 epochs...
Completing Train Step...
At time: 23.238993883132935 and batch: 50, loss is 5.2574130725860595 and perplexity is 191.98419915739854
At time: 23.761255502700806 and batch: 100, loss is 5.167841119766235 and perplexity is 175.5354680725029
At time: 24.297139406204224 and batch: 150, loss is 5.1920974063873295 and perplexity is 179.84536644847762
At time: 24.814621210098267 and batch: 200, loss is 5.205081615447998 and perplexity is 182.1957421491058
At time: 25.32460880279541 and batch: 250, loss is 5.251280908584595 and perplexity is 190.81052282819599
At time: 25.837098836898804 and batch: 300, loss is 5.172357015609741 and perplexity is 176.32996053557105
At time: 26.355675220489502 and batch: 350, loss is 5.16558346748352 and perplexity is 175.13961703744553
At time: 26.86373496055603 and batch: 400, loss is 5.194806671142578 and perplexity is 180.3332758005543
At time: 27.369794607162476 and batch: 450, loss is 5.213279104232788 and perplexity is 183.69542813344324
At time: 27.875574350357056 and batch: 500, loss is 5.243036127090454 and perplexity is 189.243799282355
At time: 28.37934374809265 and batch: 550, loss is 5.192252807617187 and perplexity is 179.87331681131207
At time: 28.897350788116455 and batch: 600, loss is 5.100175657272339 and perplexity is 164.05072147137716
At time: 29.427237033843994 and batch: 650, loss is 5.097057838439941 and perplexity is 163.5400375661415
At time: 29.95291543006897 and batch: 700, loss is 5.157337045669555 and perplexity is 173.7012805945081
At time: 30.458057641983032 and batch: 750, loss is 5.065315399169922 and perplexity is 158.4304030120486
At time: 30.96316885948181 and batch: 800, loss is 5.181859722137451 and perplexity is 178.01355910487965
At time: 31.46893882751465 and batch: 850, loss is 5.116335220336914 and perplexity is 166.7232446981869
At time: 31.974222898483276 and batch: 900, loss is 5.141672506332397 and perplexity is 171.0015304277336
At time: 32.489065170288086 and batch: 950, loss is 5.09244520187378 and perplexity is 162.78742390987767
At time: 33.00848627090454 and batch: 1000, loss is 5.054833202362061 and perplexity is 156.77837790003224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.2720180604516 and perplexity of 194.8087017629178
Finished 3 epochs...
Completing Train Step...
At time: 34.59805083274841 and batch: 50, loss is 5.162525835037232 and perplexity is 174.60492232828398
At time: 35.10521173477173 and batch: 100, loss is 5.091514949798584 and perplexity is 162.63606098472277
At time: 35.60829496383667 and batch: 150, loss is 5.117396106719971 and perplexity is 166.90021297324498
At time: 36.11200737953186 and batch: 200, loss is 5.152979717254639 and perplexity is 172.94605364969357
At time: 36.61790704727173 and batch: 250, loss is 5.1853712463378905 and perplexity is 178.6397568364672
At time: 37.12557578086853 and batch: 300, loss is 5.133826122283936 and perplexity is 169.66503692461905
At time: 37.633211612701416 and batch: 350, loss is 5.0857986640930175 and perplexity is 161.70903887998304
At time: 38.15189814567566 and batch: 400, loss is 5.113851184844971 and perplexity is 166.30961219262775
At time: 38.664191246032715 and batch: 450, loss is 5.167744140625 and perplexity is 175.51844561897778
At time: 39.17953896522522 and batch: 500, loss is 5.165920944213867 and perplexity is 175.1987325572526
At time: 39.686931133270264 and batch: 550, loss is 5.1289647102355955 and perplexity is 168.8422268996935
At time: 40.19187831878662 and batch: 600, loss is 5.057426671981812 and perplexity is 157.18550556857178
At time: 40.69989633560181 and batch: 650, loss is 5.040627431869507 and perplexity is 154.56696484772476
At time: 41.20402145385742 and batch: 700, loss is 5.1194461059570315 and perplexity is 167.24270922108383
At time: 41.70723366737366 and batch: 750, loss is 5.041754884719849 and perplexity is 154.74133008863456
At time: 42.21324324607849 and batch: 800, loss is 5.148883857727051 and perplexity is 172.23913960667224
At time: 42.73376750946045 and batch: 850, loss is 5.063607444763184 and perplexity is 158.16004205487965
At time: 43.25456643104553 and batch: 900, loss is 5.095128946304321 and perplexity is 163.22489051386853
At time: 43.75873589515686 and batch: 950, loss is 5.090947952270508 and perplexity is 162.54387287785252
At time: 44.262328147888184 and batch: 1000, loss is 5.029349908828736 and perplexity is 152.83362460342735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.287126866782584 and perplexity of 197.77437619393643
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 45.76735830307007 and batch: 50, loss is 5.093124933242798 and perplexity is 162.89811324363527
At time: 46.27187919616699 and batch: 100, loss is 4.963876152038575 and perplexity is 143.14758371917347
At time: 46.776390075683594 and batch: 150, loss is 4.967851266860962 and perplexity is 143.71774427706265
At time: 47.28101444244385 and batch: 200, loss is 4.9665430068969725 and perplexity is 143.5298470421245
At time: 47.78697967529297 and batch: 250, loss is 4.996235704421997 and perplexity is 147.85553828728732
At time: 48.292351722717285 and batch: 300, loss is 4.909966878890991 and perplexity is 135.63492195503258
At time: 48.79815721511841 and batch: 350, loss is 4.893304901123047 and perplexity is 133.39369935217763
At time: 49.30357027053833 and batch: 400, loss is 4.896057071685791 and perplexity is 133.7613272200403
At time: 49.81192350387573 and batch: 450, loss is 4.922637233734131 and perplexity is 137.36439794542937
At time: 50.318276166915894 and batch: 500, loss is 4.961038818359375 and perplexity is 142.74200191633886
At time: 50.82472085952759 and batch: 550, loss is 4.873040103912354 and perplexity is 130.71770886458108
At time: 51.34365463256836 and batch: 600, loss is 4.80229489326477 and perplexity is 121.78959117184304
At time: 51.867297887802124 and batch: 650, loss is 4.783168172836303 and perplexity is 119.48229158724902
At time: 52.38677215576172 and batch: 700, loss is 4.853245763778687 and perplexity is 128.15567857643762
At time: 52.8996148109436 and batch: 750, loss is 4.7811213302612305 and perplexity is 119.23798026456589
At time: 53.404656648635864 and batch: 800, loss is 4.890213031768798 and perplexity is 132.98190040343033
At time: 53.91997838020325 and batch: 850, loss is 4.786118125915527 and perplexity is 119.83527913359782
At time: 54.437251567840576 and batch: 900, loss is 4.8060768699646 and perplexity is 122.25106866644678
At time: 54.96147632598877 and batch: 950, loss is 4.762612524032593 and perplexity is 117.05132619521152
At time: 55.478283405303955 and batch: 1000, loss is 4.68370831489563 and perplexity is 108.17045985906118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.074044948670922 and perplexity of 159.8194832587618
Finished 5 epochs...
Completing Train Step...
At time: 57.02514863014221 and batch: 50, loss is 4.920868158340454 and perplexity is 137.1216047920746
At time: 57.54679870605469 and batch: 100, loss is 4.839783201217651 and perplexity is 126.4419363005785
At time: 58.05615997314453 and batch: 150, loss is 4.870291662216187 and perplexity is 130.3589321274301
At time: 58.56424379348755 and batch: 200, loss is 4.8779683113098145 and perplexity is 131.36350284066674
At time: 59.07149934768677 and batch: 250, loss is 4.905640783309937 and perplexity is 135.0494197010275
At time: 59.591400146484375 and batch: 300, loss is 4.8230766010284425 and perplexity is 124.34706920191068
At time: 60.11005735397339 and batch: 350, loss is 4.815085821151733 and perplexity is 123.35739852992786
At time: 60.63185691833496 and batch: 400, loss is 4.822918624877929 and perplexity is 124.32742688214047
At time: 61.140491008758545 and batch: 450, loss is 4.861362514495849 and perplexity is 129.20011927069237
At time: 61.65005540847778 and batch: 500, loss is 4.8986946487426755 and perplexity is 134.1145987134035
At time: 62.156386852264404 and batch: 550, loss is 4.810900182723999 and perplexity is 122.84214814045754
At time: 62.6623969078064 and batch: 600, loss is 4.743226556777954 and perplexity is 114.80402643866667
At time: 63.17123103141785 and batch: 650, loss is 4.727890338897705 and perplexity is 113.05679905108575
At time: 63.67954397201538 and batch: 700, loss is 4.806417760848999 and perplexity is 122.2927500453606
At time: 64.18871831893921 and batch: 750, loss is 4.733111839294434 and perplexity is 113.64866905217535
At time: 64.70180821418762 and batch: 800, loss is 4.840276651382446 and perplexity is 126.50434449128267
At time: 65.2257616519928 and batch: 850, loss is 4.741198244094849 and perplexity is 114.571403971072
At time: 65.73551321029663 and batch: 900, loss is 4.762297430038452 and perplexity is 117.01444983538603
At time: 66.24301958084106 and batch: 950, loss is 4.734947719573975 and perplexity is 113.85750604368512
At time: 66.76274156570435 and batch: 1000, loss is 4.6596902561187745 and perplexity is 105.60336709348172
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.05588066287157 and perplexity of 156.9426830962071
Finished 6 epochs...
Completing Train Step...
At time: 68.33456325531006 and batch: 50, loss is 4.858900861740112 and perplexity is 128.88246457923003
At time: 68.84103441238403 and batch: 100, loss is 4.781621131896973 and perplexity is 119.2975904975498
At time: 69.37284398078918 and batch: 150, loss is 4.812184658050537 and perplexity is 123.00003722941433
At time: 69.88143754005432 and batch: 200, loss is 4.827792205810547 and perplexity is 124.93482556026937
At time: 70.38486647605896 and batch: 250, loss is 4.853551321029663 and perplexity is 128.19484345653223
At time: 70.88612580299377 and batch: 300, loss is 4.772408390045166 and perplexity is 118.20357974985474
At time: 71.39005732536316 and batch: 350, loss is 4.767714738845825 and perplexity is 117.65007337508706
At time: 71.89417266845703 and batch: 400, loss is 4.767791633605957 and perplexity is 117.65912039708856
At time: 72.4009952545166 and batch: 450, loss is 4.810398759841919 and perplexity is 122.78056771670315
At time: 72.90507674217224 and batch: 500, loss is 4.850251121520996 and perplexity is 127.7724722351402
At time: 73.40876388549805 and batch: 550, loss is 4.767436971664429 and perplexity is 117.617398584017
At time: 73.91341209411621 and batch: 600, loss is 4.701778192520141 and perplexity is 110.14285361962125
At time: 74.41473817825317 and batch: 650, loss is 4.683957948684692 and perplexity is 108.1974662315317
At time: 74.91710114479065 and batch: 700, loss is 4.770244178771972 and perplexity is 117.94803885201623
At time: 75.41851234436035 and batch: 750, loss is 4.694680042266846 and perplexity is 109.36381124613226
At time: 75.9194905757904 and batch: 800, loss is 4.801531114578247 and perplexity is 121.69660639228228
At time: 76.42020964622498 and batch: 850, loss is 4.70524808883667 and perplexity is 110.52570173919791
At time: 76.92119550704956 and batch: 900, loss is 4.72482421875 and perplexity is 112.71068420768236
At time: 77.42255473136902 and batch: 950, loss is 4.707592964172363 and perplexity is 110.78517482829841
At time: 77.92495799064636 and batch: 1000, loss is 4.633398323059082 and perplexity is 102.86303263649339
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.052338390815549 and perplexity of 156.38773288806428
Finished 7 epochs...
Completing Train Step...
At time: 79.43759989738464 and batch: 50, loss is 4.819934825897217 and perplexity is 123.95701172953123
At time: 79.95239114761353 and batch: 100, loss is 4.742333202362061 and perplexity is 114.70151155256636
At time: 80.45891976356506 and batch: 150, loss is 4.774127111434937 and perplexity is 118.40691345766066
At time: 80.97397923469543 and batch: 200, loss is 4.7908984375 and perplexity is 120.40950049169508
At time: 81.48586678504944 and batch: 250, loss is 4.812770481109619 and perplexity is 123.07211459772215
At time: 81.99455547332764 and batch: 300, loss is 4.734479026794434 and perplexity is 113.80415435645786
At time: 82.51284861564636 and batch: 350, loss is 4.731669645309449 and perplexity is 113.48488375873762
At time: 83.02218389511108 and batch: 400, loss is 4.730842237472534 and perplexity is 113.39102431191995
At time: 83.52809834480286 and batch: 450, loss is 4.774562330245971 and perplexity is 118.45845758942825
At time: 84.03639435768127 and batch: 500, loss is 4.816501312255859 and perplexity is 123.53213346887581
At time: 84.546471118927 and batch: 550, loss is 4.734845342636109 and perplexity is 113.84585025751537
At time: 85.05063319206238 and batch: 600, loss is 4.672814407348633 and perplexity is 106.99845631001527
At time: 85.56306719779968 and batch: 650, loss is 4.655414705276489 and perplexity is 105.15281838665672
At time: 86.06984663009644 and batch: 700, loss is 4.746548442840576 and perplexity is 115.18602646302612
At time: 86.57482385635376 and batch: 750, loss is 4.673729248046875 and perplexity is 107.09638764145043
At time: 87.07898902893066 and batch: 800, loss is 4.773535060882568 and perplexity is 118.3368313272729
At time: 87.58422708511353 and batch: 850, loss is 4.678818216323853 and perplexity is 107.64278688574522
At time: 88.08996415138245 and batch: 900, loss is 4.694817819595337 and perplexity is 109.37888013793133
At time: 88.59501242637634 and batch: 950, loss is 4.683457727432251 and perplexity is 108.14335709386101
At time: 89.09945869445801 and batch: 1000, loss is 4.614428024291993 and perplexity is 100.93008246400447
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.045947935523056 and perplexity of 155.39153055621438
Finished 8 epochs...
Completing Train Step...
At time: 90.60891008377075 and batch: 50, loss is 4.776261968612671 and perplexity is 118.6599653254378
At time: 91.14140844345093 and batch: 100, loss is 4.707165098190307 and perplexity is 110.73778375990925
At time: 91.64912700653076 and batch: 150, loss is 4.7415729522705075 and perplexity is 114.61434285711005
At time: 92.15717244148254 and batch: 200, loss is 4.7606179141998295 and perplexity is 116.81808715681204
At time: 92.66541504859924 and batch: 250, loss is 4.781413412094116 and perplexity is 119.27281259908953
At time: 93.17400932312012 and batch: 300, loss is 4.700814180374145 and perplexity is 110.03672573344183
At time: 93.68227481842041 and batch: 350, loss is 4.699978523254394 and perplexity is 109.9448111700271
At time: 94.18884515762329 and batch: 400, loss is 4.699820938110352 and perplexity is 109.92748686618447
At time: 94.71367263793945 and batch: 450, loss is 4.741605787277222 and perplexity is 114.61810628161304
At time: 95.25473546981812 and batch: 500, loss is 4.787544317245484 and perplexity is 120.00630910145988
At time: 95.77262830734253 and batch: 550, loss is 4.7072396564483645 and perplexity is 110.74604048396716
At time: 96.28128004074097 and batch: 600, loss is 4.647239894866943 and perplexity is 104.29671802895795
At time: 96.78878808021545 and batch: 650, loss is 4.632685852050781 and perplexity is 102.78977180912078
At time: 97.30745053291321 and batch: 700, loss is 4.723296766281128 and perplexity is 112.53865541117287
At time: 97.82973384857178 and batch: 750, loss is 4.652307071685791 and perplexity is 104.82654918122859
At time: 98.33554124832153 and batch: 800, loss is 4.749173240661621 and perplexity is 115.48876363242181
At time: 98.84229707717896 and batch: 850, loss is 4.654545936584473 and perplexity is 105.0615045811961
At time: 99.349294424057 and batch: 900, loss is 4.669484481811524 and perplexity is 106.64275198106434
At time: 99.85601496696472 and batch: 950, loss is 4.659553327560425 and perplexity is 105.58890796662503
At time: 100.36473345756531 and batch: 1000, loss is 4.594155721664428 and perplexity is 98.90459727908377
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.043766300852706 and perplexity of 155.05289253221423
Finished 9 epochs...
Completing Train Step...
At time: 101.85819339752197 and batch: 50, loss is 4.756444129943848 and perplexity is 116.33152976295631
At time: 102.36338472366333 and batch: 100, loss is 4.684214658737183 and perplexity is 108.22524517418174
At time: 102.86752271652222 and batch: 150, loss is 4.712691411972046 and perplexity is 111.35144959365194
At time: 103.38830232620239 and batch: 200, loss is 4.735024318695069 and perplexity is 113.86622776261177
At time: 103.89720368385315 and batch: 250, loss is 4.756262226104736 and perplexity is 116.31037053561339
At time: 104.40152597427368 and batch: 300, loss is 4.673502864837647 and perplexity is 107.07214556162285
At time: 104.9078950881958 and batch: 350, loss is 4.673290948867798 and perplexity is 107.04945766810077
At time: 105.41186809539795 and batch: 400, loss is 4.675712785720825 and perplexity is 107.3090281816243
At time: 105.91713809967041 and batch: 450, loss is 4.717436819076538 and perplexity is 111.8811132946412
At time: 106.42099332809448 and batch: 500, loss is 4.7638838005065915 and perplexity is 117.20022541844
At time: 106.92472958564758 and batch: 550, loss is 4.682854175567627 and perplexity is 108.07810666203095
At time: 107.42874789237976 and batch: 600, loss is 4.6230167961120605 and perplexity is 101.8006812474354
At time: 107.93360900878906 and batch: 650, loss is 4.612423467636108 and perplexity is 100.72796504101487
At time: 108.45301747322083 and batch: 700, loss is 4.702924814224243 and perplexity is 110.26921823849581
At time: 108.9585587978363 and batch: 750, loss is 4.633627891540527 and perplexity is 102.88664945742771
At time: 109.46473026275635 and batch: 800, loss is 4.728633661270141 and perplexity is 113.14086794044871
At time: 109.97014784812927 and batch: 850, loss is 4.63381293296814 and perplexity is 102.90568951147065
At time: 110.48890781402588 and batch: 900, loss is 4.647533187866211 and perplexity is 104.32731201248372
At time: 111.00054860115051 and batch: 950, loss is 4.637677316665649 and perplexity is 103.30412594018343
At time: 111.52558135986328 and batch: 1000, loss is 4.573014402389527 and perplexity is 96.83557163901128
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.04278564453125 and perplexity of 154.90091346478874
Finished 10 epochs...
Completing Train Step...
At time: 113.0354392528534 and batch: 50, loss is 4.737293100357055 and perplexity is 114.12485864964697
At time: 113.5472366809845 and batch: 100, loss is 4.662969608306884 and perplexity is 105.95024618491946
At time: 114.06051135063171 and batch: 150, loss is 4.688345355987549 and perplexity is 108.67321547472369
At time: 114.56501388549805 and batch: 200, loss is 4.712730646133423 and perplexity is 111.35581846009866
At time: 115.0702166557312 and batch: 250, loss is 4.73390962600708 and perplexity is 113.73937262649743
At time: 115.57570242881775 and batch: 300, loss is 4.650752229690552 and perplexity is 104.66368710555331
At time: 116.08073377609253 and batch: 350, loss is 4.650085391998291 and perplexity is 104.59391667934081
At time: 116.58528876304626 and batch: 400, loss is 4.650905246734619 and perplexity is 104.67970365894699
At time: 117.08723068237305 and batch: 450, loss is 4.695764322280883 and perplexity is 109.48245655166826
At time: 117.59172749519348 and batch: 500, loss is 4.744280624389648 and perplexity is 114.92510144403848
At time: 118.09745073318481 and batch: 550, loss is 4.661951599121093 and perplexity is 105.84244274282368
At time: 118.60172438621521 and batch: 600, loss is 4.598989953994751 and perplexity is 99.38388263585148
At time: 119.10585594177246 and batch: 650, loss is 4.591821823120117 and perplexity is 98.67403314477565
At time: 119.60939049720764 and batch: 700, loss is 4.684821577072143 and perplexity is 108.29094899619383
At time: 120.11435079574585 and batch: 750, loss is 4.615450897216797 and perplexity is 101.03337393067159
At time: 120.61868572235107 and batch: 800, loss is 4.710235347747803 and perplexity is 111.07829885717267
At time: 121.14860606193542 and batch: 850, loss is 4.618213558197022 and perplexity is 101.3128808041236
At time: 121.65761947631836 and batch: 900, loss is 4.629905157089233 and perplexity is 102.50434183931023
At time: 122.16301560401917 and batch: 950, loss is 4.623116617202759 and perplexity is 101.81084360967195
At time: 122.66740942001343 and batch: 1000, loss is 4.55772686958313 and perplexity is 95.36645287345281
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.042887617902058 and perplexity of 154.91671003847722
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 124.15624141693115 and batch: 50, loss is 4.7024580764770505 and perplexity is 110.21776344087274
At time: 124.68127655982971 and batch: 100, loss is 4.602428169250488 and perplexity is 99.72617391565987
At time: 125.1987578868866 and batch: 150, loss is 4.617578296661377 and perplexity is 101.24854106632743
At time: 125.70504117012024 and batch: 200, loss is 4.632591485977173 and perplexity is 102.78007239960282
At time: 126.20960116386414 and batch: 250, loss is 4.655254678726196 and perplexity is 105.13599249020773
At time: 126.71348333358765 and batch: 300, loss is 4.565667037963867 and perplexity is 96.126692789426
At time: 127.21814012527466 and batch: 350, loss is 4.558548412322998 and perplexity is 95.44483268221099
At time: 127.72321605682373 and batch: 400, loss is 4.557856979370118 and perplexity is 95.37886178956518
At time: 128.23675775527954 and batch: 450, loss is 4.6001317596435545 and perplexity is 99.49742452349386
At time: 128.763653755188 and batch: 500, loss is 4.639950132369995 and perplexity is 103.53918420081052
At time: 129.27806210517883 and batch: 550, loss is 4.554087219238281 and perplexity is 95.0199832270241
At time: 129.79607272148132 and batch: 600, loss is 4.490668067932129 and perplexity is 89.18100495125319
At time: 130.30331182479858 and batch: 650, loss is 4.476937503814697 and perplexity is 87.9648676764337
At time: 130.80772352218628 and batch: 700, loss is 4.566238164901733 and perplexity is 96.18160901370081
At time: 131.31273937225342 and batch: 750, loss is 4.483350658416748 and perplexity is 88.53081277940527
At time: 131.81700038909912 and batch: 800, loss is 4.575608711242676 and perplexity is 97.08711917472819
At time: 132.32039737701416 and batch: 850, loss is 4.481813116073608 and perplexity is 88.39479749750735
At time: 132.82526969909668 and batch: 900, loss is 4.492042274475097 and perplexity is 89.30364231700146
At time: 133.33015751838684 and batch: 950, loss is 4.469729242324829 and perplexity is 87.33307371196672
At time: 133.8362934589386 and batch: 1000, loss is 4.405276603698731 and perplexity is 81.88178851664709
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.950526539872333 and perplexity of 141.24931774236444
Finished 12 epochs...
Completing Train Step...
At time: 135.36717414855957 and batch: 50, loss is 4.645440673828125 and perplexity is 104.10923389287849
At time: 135.8864254951477 and batch: 100, loss is 4.5609314823150635 and perplexity is 95.67255563096622
At time: 136.3954734802246 and batch: 150, loss is 4.5850448894500735 and perplexity is 98.00758655002774
At time: 136.9049427509308 and batch: 200, loss is 4.60705533027649 and perplexity is 100.188692228998
At time: 137.41324043273926 and batch: 250, loss is 4.633329267501831 and perplexity is 102.85592961770841
At time: 137.92403316497803 and batch: 300, loss is 4.545178289413452 and perplexity is 94.17721651337749
At time: 138.43215250968933 and batch: 350, loss is 4.539023303985596 and perplexity is 93.59933736133743
At time: 138.9395396709442 and batch: 400, loss is 4.541986589431763 and perplexity is 93.87711027246185
At time: 139.44789624214172 and batch: 450, loss is 4.584400272369384 and perplexity is 97.9444295439299
At time: 139.95578932762146 and batch: 500, loss is 4.625185213088989 and perplexity is 102.02166708104168
At time: 140.47114491462708 and batch: 550, loss is 4.5421185970306395 and perplexity is 93.88950358236596
At time: 140.98811864852905 and batch: 600, loss is 4.4808950614929195 and perplexity is 88.31368348799677
At time: 141.51350951194763 and batch: 650, loss is 4.470097560882568 and perplexity is 87.3652460281861
At time: 142.03776955604553 and batch: 700, loss is 4.559227447509766 and perplexity is 95.50966509124976
At time: 142.55924677848816 and batch: 750, loss is 4.476981287002563 and perplexity is 87.96871914307495
At time: 143.08579683303833 and batch: 800, loss is 4.5747907352447506 and perplexity is 97.00773671243606
At time: 143.595721244812 and batch: 850, loss is 4.482869434356689 and perplexity is 88.48821987142723
At time: 144.10310244560242 and batch: 900, loss is 4.49763147354126 and perplexity is 89.80417563841205
At time: 144.6094410419464 and batch: 950, loss is 4.475645952224731 and perplexity is 87.85132984748957
At time: 145.11805868148804 and batch: 1000, loss is 4.410110054016113 and perplexity is 82.27851808730125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.948046800566883 and perplexity of 140.89949017713312
Finished 13 epochs...
Completing Train Step...
At time: 146.62846112251282 and batch: 50, loss is 4.633423662185669 and perplexity is 102.86563912892139
At time: 147.15462684631348 and batch: 100, loss is 4.54883074760437 and perplexity is 94.52182380799626
At time: 147.67644953727722 and batch: 150, loss is 4.5734423160552975 and perplexity is 96.87701777049894
At time: 148.1813325881958 and batch: 200, loss is 4.596376638412476 and perplexity is 99.12450025835585
At time: 148.68652820587158 and batch: 250, loss is 4.624508438110351 and perplexity is 101.9526447284174
At time: 149.20883774757385 and batch: 300, loss is 4.536200733184814 and perplexity is 93.33551910273675
At time: 149.72345495224 and batch: 350, loss is 4.529888391494751 and perplexity is 92.7482090154604
At time: 150.2318079471588 and batch: 400, loss is 4.5340918445587155 and perplexity is 93.13889229298466
At time: 150.7512080669403 and batch: 450, loss is 4.57675253868103 and perplexity is 97.19823362131278
At time: 151.25817894935608 and batch: 500, loss is 4.617824001312256 and perplexity is 101.27342136023888
At time: 151.7875657081604 and batch: 550, loss is 4.536232614517212 and perplexity is 93.33849481088033
At time: 152.30783557891846 and batch: 600, loss is 4.475288076400757 and perplexity is 87.81989560554626
At time: 152.81952238082886 and batch: 650, loss is 4.465834846496582 and perplexity is 86.99362555579137
At time: 153.3237817287445 and batch: 700, loss is 4.554317235946655 and perplexity is 95.04184192463224
At time: 153.82720756530762 and batch: 750, loss is 4.473853673934936 and perplexity is 87.69401683273571
At time: 154.34463596343994 and batch: 800, loss is 4.57391622543335 and perplexity is 96.92293957826634
At time: 154.86815333366394 and batch: 850, loss is 4.482701921463013 and perplexity is 88.47339819510591
At time: 155.3794023990631 and batch: 900, loss is 4.499758386611939 and perplexity is 89.9953845836995
At time: 155.88386130332947 and batch: 950, loss is 4.476793603897095 and perplexity is 87.95221044993194
At time: 156.4008560180664 and batch: 1000, loss is 4.409608554840088 and perplexity is 82.23726582312875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.9473512230849845 and perplexity of 140.8015177422149
Finished 14 epochs...
Completing Train Step...
At time: 157.88653564453125 and batch: 50, loss is 4.6244473648071285 and perplexity is 101.94641833376673
At time: 158.40011620521545 and batch: 100, loss is 4.539980707168579 and perplexity is 93.68899257609792
At time: 158.9061484336853 and batch: 150, loss is 4.565134315490723 and perplexity is 96.07549757753874
At time: 159.42962431907654 and batch: 200, loss is 4.588025302886963 and perplexity is 98.30012540471664
At time: 159.9301884174347 and batch: 250, loss is 4.61831316947937 and perplexity is 101.32297321274939
At time: 160.4437017440796 and batch: 300, loss is 4.529298295974732 and perplexity is 92.69349485770793
At time: 160.94520330429077 and batch: 350, loss is 4.523235397338867 and perplexity is 92.13320380441137
At time: 161.447101354599 and batch: 400, loss is 4.528578281402588 and perplexity is 92.62677821202827
At time: 161.94848728179932 and batch: 450, loss is 4.571028499603272 and perplexity is 96.64345643168635
At time: 162.45309329032898 and batch: 500, loss is 4.611741094589234 and perplexity is 100.65925443840214
At time: 162.9728627204895 and batch: 550, loss is 4.531688632965088 and perplexity is 92.9153285702717
At time: 163.47695899009705 and batch: 600, loss is 4.471141567230225 and perplexity is 87.45650352801985
At time: 163.97811484336853 and batch: 650, loss is 4.462424049377441 and perplexity is 86.69741339532004
At time: 164.47811579704285 and batch: 700, loss is 4.551100645065308 and perplexity is 94.73662234896466
At time: 164.97922158241272 and batch: 750, loss is 4.471223936080933 and perplexity is 87.46370751639044
At time: 165.47991108894348 and batch: 800, loss is 4.572259140014649 and perplexity is 96.7624629867849
At time: 165.9810655117035 and batch: 850, loss is 4.481161336898804 and perplexity is 88.33720238102514
At time: 166.4822494983673 and batch: 900, loss is 4.499758014678955 and perplexity is 89.9953511114538
At time: 166.984721660614 and batch: 950, loss is 4.475901708602906 and perplexity is 87.87380125891008
At time: 167.48710131645203 and batch: 1000, loss is 4.407399988174438 and perplexity is 82.05583975868286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.947143182521913 and perplexity of 140.77222836197768
Finished 15 epochs...
Completing Train Step...
At time: 169.14171504974365 and batch: 50, loss is 4.617696132659912 and perplexity is 101.2604724922261
At time: 169.65047359466553 and batch: 100, loss is 4.533075275421143 and perplexity is 93.04425827873072
At time: 170.1596384048462 and batch: 150, loss is 4.558043069839478 and perplexity is 95.396612538294
At time: 170.67828726768494 and batch: 200, loss is 4.581647453308105 and perplexity is 97.67517702310428
At time: 171.19646096229553 and batch: 250, loss is 4.613019561767578 and perplexity is 100.788026289151
At time: 171.7115228176117 and batch: 300, loss is 4.523623905181885 and perplexity is 92.16900523080886
At time: 172.23802137374878 and batch: 350, loss is 4.517675476074219 and perplexity is 91.62237185404405
At time: 172.75228476524353 and batch: 400, loss is 4.524324388504028 and perplexity is 92.23359069967259
At time: 173.25942730903625 and batch: 450, loss is 4.5663767910003665 and perplexity is 96.19494321913173
At time: 173.7946219444275 and batch: 500, loss is 4.606656541824341 and perplexity is 100.14874610105785
At time: 174.31118178367615 and batch: 550, loss is 4.527580442428589 and perplexity is 92.5343977007948
At time: 174.83576083183289 and batch: 600, loss is 4.46729471206665 and perplexity is 87.12071730041517
At time: 175.34249997138977 and batch: 650, loss is 4.458898048400879 and perplexity is 86.39225653886947
At time: 175.8501591682434 and batch: 700, loss is 4.547410039901734 and perplexity is 94.38763127160134
At time: 176.35926365852356 and batch: 750, loss is 4.4684279251098635 and perplexity is 87.2194995937146
At time: 176.86620235443115 and batch: 800, loss is 4.57044937133789 and perplexity is 96.58750367787705
At time: 177.3760542869568 and batch: 850, loss is 4.47960165977478 and perplexity is 88.19953225565175
At time: 177.88626623153687 and batch: 900, loss is 4.499507541656494 and perplexity is 89.97281252662508
At time: 178.3909637928009 and batch: 950, loss is 4.474667510986328 and perplexity is 87.76541452192124
At time: 178.89853072166443 and batch: 1000, loss is 4.404770383834839 and perplexity is 81.84034881849307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.946934397627667 and perplexity of 140.74284031516333
Finished 16 epochs...
Completing Train Step...
At time: 180.39533138275146 and batch: 50, loss is 4.612191858291626 and perplexity is 100.70463820452166
At time: 180.90959548950195 and batch: 100, loss is 4.5271165084838865 and perplexity is 92.49147780941584
At time: 181.42433285713196 and batch: 150, loss is 4.5520162010192875 and perplexity is 94.82339874587798
At time: 181.92893767356873 and batch: 200, loss is 4.575803442001343 and perplexity is 97.10602686399642
At time: 182.43261098861694 and batch: 250, loss is 4.608720664978027 and perplexity is 100.35567894065886
At time: 182.93597769737244 and batch: 300, loss is 4.5188585472106935 and perplexity is 91.730831782895
At time: 183.44023895263672 and batch: 350, loss is 4.5131219959259035 and perplexity is 91.20611962002776
At time: 183.9448676109314 and batch: 400, loss is 4.520598497390747 and perplexity is 91.89057779492168
At time: 184.4491777420044 and batch: 450, loss is 4.561817140579223 and perplexity is 95.75732635391863
At time: 184.96372437477112 and batch: 500, loss is 4.602141218185425 and perplexity is 99.69756148921955
At time: 185.48094010353088 and batch: 550, loss is 4.523585596084595 and perplexity is 92.1654743870525
At time: 185.98553228378296 and batch: 600, loss is 4.463503093719482 and perplexity is 86.79101423936253
At time: 186.52547812461853 and batch: 650, loss is 4.455974035263061 and perplexity is 86.14001340658055
At time: 187.03807759284973 and batch: 700, loss is 4.543953123092652 and perplexity is 94.06190441219388
At time: 187.5427668094635 and batch: 750, loss is 4.465755033493042 and perplexity is 86.98668261031924
At time: 188.07404589653015 and batch: 800, loss is 4.56826979637146 and perplexity is 96.37721322794052
At time: 188.5972511768341 and batch: 850, loss is 4.477783374786377 and perplexity is 88.039306082734
At time: 189.12102246284485 and batch: 900, loss is 4.498547534942627 and perplexity is 89.88647946932181
At time: 189.63478541374207 and batch: 950, loss is 4.472297773361206 and perplexity is 87.5576797526076
At time: 190.15252590179443 and batch: 1000, loss is 4.401796417236328 and perplexity is 81.59731991374098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.946805628334603 and perplexity of 140.7247181259279
Finished 17 epochs...
Completing Train Step...
At time: 191.69767260551453 and batch: 50, loss is 4.60671745300293 and perplexity is 100.15484646500538
At time: 192.23302817344666 and batch: 100, loss is 4.521929607391358 and perplexity is 92.01297570645586
At time: 192.75545001029968 and batch: 150, loss is 4.546710309982299 and perplexity is 94.32160852371595
At time: 193.286208152771 and batch: 200, loss is 4.570608720779419 and perplexity is 96.6028960689987
At time: 193.79961252212524 and batch: 250, loss is 4.6042866611480715 and perplexity is 99.9116865353128
At time: 194.32074689865112 and batch: 300, loss is 4.5142552947998045 and perplexity is 91.30954200585336
At time: 194.83393836021423 and batch: 350, loss is 4.508567142486572 and perplexity is 90.79163379009293
At time: 195.34872674942017 and batch: 400, loss is 4.516863946914673 and perplexity is 91.54804778976946
At time: 195.86052632331848 and batch: 450, loss is 4.557736701965332 and perplexity is 95.3673905574766
At time: 196.3810203075409 and batch: 500, loss is 4.59778582572937 and perplexity is 99.2642837143181
At time: 196.89638543128967 and batch: 550, loss is 4.5193345260620115 and perplexity is 91.77450411156536
At time: 197.42723321914673 and batch: 600, loss is 4.460047225952149 and perplexity is 86.4915936477431
At time: 197.93658328056335 and batch: 650, loss is 4.452324380874634 and perplexity is 85.82620512284849
At time: 198.4771876335144 and batch: 700, loss is 4.540355672836304 and perplexity is 93.72412931888213
At time: 199.01577711105347 and batch: 750, loss is 4.462565107345581 and perplexity is 86.70964361886152
At time: 199.53325033187866 and batch: 800, loss is 4.565673532485962 and perplexity is 96.12731708838353
At time: 200.06542897224426 and batch: 850, loss is 4.475709524154663 and perplexity is 87.85691490359957
At time: 200.58327960968018 and batch: 900, loss is 4.497247495651245 and perplexity is 89.76969944000847
At time: 201.09591341018677 and batch: 950, loss is 4.469571084976196 and perplexity is 87.31926243678662
At time: 201.6115701198578 and batch: 1000, loss is 4.398259658813476 and perplexity is 81.3092396407114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.946359029630336 and perplexity of 140.66188468086636
Finished 18 epochs...
Completing Train Step...
At time: 203.1456081867218 and batch: 50, loss is 4.60150899887085 and perplexity is 99.63455068565237
At time: 203.66124629974365 and batch: 100, loss is 4.517000312805176 and perplexity is 91.56053267206683
At time: 204.18419694900513 and batch: 150, loss is 4.541770858764648 and perplexity is 93.85686028518619
At time: 204.71418809890747 and batch: 200, loss is 4.565906085968018 and perplexity is 96.14967443023116
At time: 205.22158098220825 and batch: 250, loss is 4.600298881530762 and perplexity is 99.51405411039777
At time: 205.73926830291748 and batch: 300, loss is 4.510095043182373 and perplexity is 90.93046042017872
At time: 206.2529718875885 and batch: 350, loss is 4.5046032047271725 and perplexity is 90.43245375862965
At time: 206.773024559021 and batch: 400, loss is 4.5132546043396 and perplexity is 91.21821512083481
At time: 207.29125547409058 and batch: 450, loss is 4.553834981918335 and perplexity is 94.99601866361932
At time: 207.8071208000183 and batch: 500, loss is 4.5933029842376705 and perplexity is 98.82029357687506
At time: 208.3307704925537 and batch: 550, loss is 4.515875186920166 and perplexity is 91.45757347860042
At time: 208.84449243545532 and batch: 600, loss is 4.456852188110352 and perplexity is 86.21569072788355
At time: 209.36079239845276 and batch: 650, loss is 4.449615898132325 and perplexity is 85.5940608487306
At time: 209.89015889167786 and batch: 700, loss is 4.537564907073975 and perplexity is 93.46293186765219
At time: 210.4068992137909 and batch: 750, loss is 4.460022773742676 and perplexity is 86.48947876303447
At time: 210.92744064331055 and batch: 800, loss is 4.56327187538147 and perplexity is 95.8967292416676
At time: 211.4583752155304 and batch: 850, loss is 4.473284683227539 and perplexity is 87.64413394485918
At time: 211.98443174362183 and batch: 900, loss is 4.496090240478516 and perplexity is 89.66587307936099
At time: 212.50228881835938 and batch: 950, loss is 4.466910343170166 and perplexity is 87.08723724120445
At time: 213.0603859424591 and batch: 1000, loss is 4.3949198722839355 and perplexity is 81.03813710172977
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.9464502101991235 and perplexity of 140.67471089626022
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 214.60174345970154 and batch: 50, loss is 4.595509223937988 and perplexity is 99.03855551230252
At time: 215.1249532699585 and batch: 100, loss is 4.508703260421753 and perplexity is 90.8039930009522
At time: 215.6479160785675 and batch: 150, loss is 4.529809122085571 and perplexity is 92.7408572111197
At time: 216.15790915489197 and batch: 200, loss is 4.553849668502807 and perplexity is 94.9974138409171
At time: 216.66901206970215 and batch: 250, loss is 4.587945890426636 and perplexity is 98.29231945985657
At time: 217.18765568733215 and batch: 300, loss is 4.493209743499756 and perplexity is 89.40796243660103
At time: 217.71528816223145 and batch: 350, loss is 4.487365665435791 and perplexity is 88.88697914087842
At time: 218.23503613471985 and batch: 400, loss is 4.493679761886597 and perplexity is 89.44999570030575
At time: 218.74500441551208 and batch: 450, loss is 4.533848552703858 and perplexity is 93.11623511538444
At time: 219.2719714641571 and batch: 500, loss is 4.569543161392212 and perplexity is 96.50001476906284
At time: 219.80135893821716 and batch: 550, loss is 4.491007900238037 and perplexity is 89.21131668797095
At time: 220.33014726638794 and batch: 600, loss is 4.429449739456177 and perplexity is 83.88524546748258
At time: 220.85230112075806 and batch: 650, loss is 4.419668560028076 and perplexity is 83.06874849151889
At time: 221.37697291374207 and batch: 700, loss is 4.5038667678833 and perplexity is 90.36588048431483
At time: 221.90769934654236 and batch: 750, loss is 4.429667558670044 and perplexity is 83.90351927582635
At time: 222.42844033241272 and batch: 800, loss is 4.526955137252807 and perplexity is 92.47655354998287
At time: 222.9601936340332 and batch: 850, loss is 4.441303014755249 and perplexity is 84.88547667243215
At time: 223.4823522567749 and batch: 900, loss is 4.4555684852600095 and perplexity is 86.1050864066818
At time: 223.9945650100708 and batch: 950, loss is 4.4246187496185305 and perplexity is 83.48097400102964
At time: 224.51491904258728 and batch: 1000, loss is 4.351488780975342 and perplexity is 77.59389729378722
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.933642503691883 and perplexity of 138.8844793382997
Finished 20 epochs...
Completing Train Step...
At time: 226.04047465324402 and batch: 50, loss is 4.58272045135498 and perplexity is 97.78003854532172
At time: 226.57405519485474 and batch: 100, loss is 4.500462951660157 and perplexity is 90.05881452881532
At time: 227.08599138259888 and batch: 150, loss is 4.522603454589844 and perplexity is 92.0749992872212
At time: 227.59053015708923 and batch: 200, loss is 4.547255945205689 and perplexity is 94.373087758818
At time: 228.1022243499756 and batch: 250, loss is 4.5821710872650145 and perplexity is 97.72633645577984
At time: 228.61328291893005 and batch: 300, loss is 4.48684027671814 and perplexity is 88.84029119062679
At time: 229.12479829788208 and batch: 350, loss is 4.48162109375 and perplexity is 88.37782535266452
At time: 229.63577914237976 and batch: 400, loss is 4.489835176467896 and perplexity is 89.10675777766828
At time: 230.15770626068115 and batch: 450, loss is 4.5304020977020265 and perplexity is 92.79586658609222
At time: 230.66778230667114 and batch: 500, loss is 4.566589307785034 and perplexity is 96.21538843156442
At time: 231.17220187187195 and batch: 550, loss is 4.487821016311646 and perplexity is 88.9274631211907
At time: 231.67700576782227 and batch: 600, loss is 4.427570810317993 and perplexity is 83.72777901598832
At time: 232.20810985565186 and batch: 650, loss is 4.4174794101715085 and perplexity is 82.88709745588774
At time: 232.73062205314636 and batch: 700, loss is 4.501897258758545 and perplexity is 90.18807920622795
At time: 233.24495697021484 and batch: 750, loss is 4.4299435043334965 and perplexity is 83.92667528287186
At time: 233.76065516471863 and batch: 800, loss is 4.5273076438903805 and perplexity is 92.50915789521555
At time: 234.26912665367126 and batch: 850, loss is 4.442159423828125 and perplexity is 84.95820450275157
At time: 234.7789237499237 and batch: 900, loss is 4.456064281463623 and perplexity is 86.14778756629572
At time: 235.2995355129242 and batch: 950, loss is 4.426466598510742 and perplexity is 83.63537683898006
At time: 235.81858801841736 and batch: 1000, loss is 4.35346848487854 and perplexity is 77.74766238958424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.933120355373475 and perplexity of 138.81197997031904
Finished 21 epochs...
Completing Train Step...
At time: 237.3970651626587 and batch: 50, loss is 4.578676624298096 and perplexity is 97.38543137913223
At time: 237.91116094589233 and batch: 100, loss is 4.497537488937378 and perplexity is 89.79573582515022
At time: 238.43590784072876 and batch: 150, loss is 4.5197827434539795 and perplexity is 91.81564826052016
At time: 238.9715175628662 and batch: 200, loss is 4.544483003616333 and perplexity is 94.11175919073382
At time: 239.4899706840515 and batch: 250, loss is 4.579640121459961 and perplexity is 97.4793071831461
At time: 240.0261640548706 and batch: 300, loss is 4.484129028320313 and perplexity is 88.59974932519658
At time: 240.52766036987305 and batch: 350, loss is 4.478730173110962 and perplexity is 88.12270102309587
At time: 241.03490924835205 and batch: 400, loss is 4.487862968444825 and perplexity is 88.93119389622325
At time: 241.54390406608582 and batch: 450, loss is 4.528734836578369 and perplexity is 92.64128054875164
At time: 242.06446433067322 and batch: 500, loss is 4.565306329727173 and perplexity is 96.09202535236165
At time: 242.58077764511108 and batch: 550, loss is 4.486112728118896 and perplexity is 88.77567906829427
At time: 243.1185073852539 and batch: 600, loss is 4.4266109275817875 and perplexity is 83.64744872636697
At time: 243.6304018497467 and batch: 650, loss is 4.416261796951294 and perplexity is 82.78623444876716
At time: 244.14045572280884 and batch: 700, loss is 4.500719614028931 and perplexity is 90.08193220407303
At time: 244.6657361984253 and batch: 750, loss is 4.430333604812622 and perplexity is 83.95942150585451
At time: 245.19175148010254 and batch: 800, loss is 4.527402229309082 and perplexity is 92.5179083264739
At time: 245.7151277065277 and batch: 850, loss is 4.442733821868896 and perplexity is 85.00701834691152
At time: 246.23612904548645 and batch: 900, loss is 4.456306962966919 and perplexity is 86.16869657790055
At time: 246.749347448349 and batch: 950, loss is 4.42734471321106 and perplexity is 83.70885054729946
At time: 247.2670338153839 and batch: 1000, loss is 4.354078979492187 and perplexity is 77.79514141006521
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.932513725466844 and perplexity of 138.72779800809988
Finished 22 epochs...
Completing Train Step...
At time: 248.85549592971802 and batch: 50, loss is 4.575799102783203 and perplexity is 97.1056055006774
At time: 249.3647518157959 and batch: 100, loss is 4.495410928726196 and perplexity is 89.60498268211896
At time: 249.89078521728516 and batch: 150, loss is 4.517840576171875 and perplexity is 91.63749996537662
At time: 250.40872359275818 and batch: 200, loss is 4.542556371688843 and perplexity is 93.93061502582307
At time: 250.91408801078796 and batch: 250, loss is 4.577936916351319 and perplexity is 97.313421238161
At time: 251.42570757865906 and batch: 300, loss is 4.482222166061401 and perplexity is 88.43096278454551
At time: 251.93691158294678 and batch: 350, loss is 4.47652455329895 and perplexity is 87.92855003816894
At time: 252.45803809165955 and batch: 400, loss is 4.486566982269287 and perplexity is 88.81601494964197
At time: 252.99546790122986 and batch: 450, loss is 4.52769305229187 and perplexity is 92.54481857340413
At time: 253.50202560424805 and batch: 500, loss is 4.564362535476684 and perplexity is 96.0013770347255
At time: 254.01253080368042 and batch: 550, loss is 4.484755363464355 and perplexity is 88.65525984423209
At time: 254.5180468559265 and batch: 600, loss is 4.4259052658081055 and perplexity is 83.58844274091747
At time: 255.04199290275574 and batch: 650, loss is 4.4151926612854 and perplexity is 82.6977720304626
At time: 255.559002161026 and batch: 700, loss is 4.4998262500762936 and perplexity is 90.00149218951259
At time: 256.0873808860779 and batch: 750, loss is 4.430612545013428 and perplexity is 83.98284443039466
At time: 256.60252475738525 and batch: 800, loss is 4.52735710144043 and perplexity is 92.51373328466501
At time: 257.1215012073517 and batch: 850, loss is 4.443174715042114 and perplexity is 85.0445056243114
At time: 257.6399185657501 and batch: 900, loss is 4.456207075119019 and perplexity is 86.16008980210628
At time: 258.1621379852295 and batch: 950, loss is 4.42771167755127 and perplexity is 83.73957434733502
At time: 258.68855834007263 and batch: 1000, loss is 4.353969993591309 and perplexity is 77.7866632985004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.932384956173781 and perplexity of 138.70993527773243
Finished 23 epochs...
Completing Train Step...
At time: 260.3051905632019 and batch: 50, loss is 4.573388223648071 and perplexity is 96.87177760113079
At time: 260.8492593765259 and batch: 100, loss is 4.493620471954346 and perplexity is 89.44469237333932
At time: 261.35452914237976 and batch: 150, loss is 4.516335182189941 and perplexity is 91.49965320728178
At time: 261.8607928752899 and batch: 200, loss is 4.541150550842286 and perplexity is 93.79865818466146
At time: 262.388897895813 and batch: 250, loss is 4.576523408889771 and perplexity is 97.17596516161352
At time: 262.92315316200256 and batch: 300, loss is 4.480634298324585 and perplexity is 88.29065753437392
At time: 263.43906116485596 and batch: 350, loss is 4.474806823730469 and perplexity is 87.77764221437586
At time: 263.97151923179626 and batch: 400, loss is 4.485516777038574 and perplexity is 88.72278886799859
At time: 264.4939911365509 and batch: 450, loss is 4.526826848983765 and perplexity is 92.46469065395239
At time: 265.01253151893616 and batch: 500, loss is 4.563497705459595 and perplexity is 95.91838805303765
At time: 265.51865124702454 and batch: 550, loss is 4.483809137344361 and perplexity is 88.57141159764917
At time: 266.03475546836853 and batch: 600, loss is 4.425267248153687 and perplexity is 83.53512884815433
At time: 266.5642189979553 and batch: 650, loss is 4.414387998580932 and perplexity is 82.63125498304979
At time: 267.08507657051086 and batch: 700, loss is 4.498989696502686 and perplexity is 89.92623260331833
At time: 267.6030616760254 and batch: 750, loss is 4.43066725730896 and perplexity is 83.9874394502996
At time: 268.1085205078125 and batch: 800, loss is 4.527294483184814 and perplexity is 92.50794041743777
At time: 268.636696100235 and batch: 850, loss is 4.443285322189331 and perplexity is 85.05391267469885
At time: 269.15519070625305 and batch: 900, loss is 4.456026010513305 and perplexity is 86.14449067168587
At time: 269.6749060153961 and batch: 950, loss is 4.427797794342041 and perplexity is 83.74678604125788
At time: 270.18825602531433 and batch: 1000, loss is 4.353606719970703 and perplexity is 77.75841058773203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.932529728587081 and perplexity of 138.7300181034958
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 271.7516345977783 and batch: 50, loss is 4.571006116867065 and perplexity is 96.64129331090331
At time: 272.288104057312 and batch: 100, loss is 4.491831703186035 and perplexity is 89.28483951365556
At time: 272.81269001960754 and batch: 150, loss is 4.51402177810669 and perplexity is 91.28822219291733
At time: 273.3370888233185 and batch: 200, loss is 4.538679113388062 and perplexity is 93.56712689307037
At time: 273.85471963882446 and batch: 250, loss is 4.573425455093384 and perplexity is 96.87538434456259
At time: 274.36289286613464 and batch: 300, loss is 4.4763765716552735 and perplexity is 87.91553918951531
At time: 274.8802351951599 and batch: 350, loss is 4.469925918579102 and perplexity is 87.35025174297856
At time: 275.3979949951172 and batch: 400, loss is 4.480678453445434 and perplexity is 88.29455610509753
At time: 275.91980600357056 and batch: 450, loss is 4.522516603469849 and perplexity is 92.06700281766572
At time: 276.4394271373749 and batch: 500, loss is 4.558024482727051 and perplexity is 95.39483940721034
At time: 276.95495533943176 and batch: 550, loss is 4.477691736221313 and perplexity is 88.03123865670472
At time: 277.46362924575806 and batch: 600, loss is 4.418384313583374 and perplexity is 82.96213621947133
At time: 277.98044085502625 and batch: 650, loss is 4.406043663024902 and perplexity is 81.9446208011365
At time: 278.5187928676605 and batch: 700, loss is 4.492325658798218 and perplexity is 89.32895315540968
At time: 279.0179944038391 and batch: 750, loss is 4.422852535247802 and perplexity is 83.33365883844118
At time: 279.5595920085907 and batch: 800, loss is 4.518957691192627 and perplexity is 91.73992679367437
At time: 280.0693349838257 and batch: 850, loss is 4.435190439224243 and perplexity is 84.36819037121813
At time: 280.57600951194763 and batch: 900, loss is 4.445620803833008 and perplexity is 85.25278666935942
At time: 281.09383249282837 and batch: 950, loss is 4.418297357559204 and perplexity is 82.95492247559287
At time: 281.60873341560364 and batch: 1000, loss is 4.342012357711792 and perplexity is 76.8620577676675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.9300079345703125 and perplexity of 138.38061032628715
Finished 25 epochs...
Completing Train Step...
At time: 283.1127972602844 and batch: 50, loss is 4.569174041748047 and perplexity is 96.46440129117089
At time: 283.64633655548096 and batch: 100, loss is 4.4906124496459965 and perplexity is 89.17604499453567
At time: 284.1671030521393 and batch: 150, loss is 4.5131997394561765 and perplexity is 91.21321058138415
At time: 284.67687940597534 and batch: 200, loss is 4.537542018890381 and perplexity is 93.46079269538937
At time: 285.1947503089905 and batch: 250, loss is 4.572510442733765 and perplexity is 96.78678271252046
At time: 285.701833486557 and batch: 300, loss is 4.475137758255005 and perplexity is 87.80669567379819
At time: 286.2082612514496 and batch: 350, loss is 4.46873272895813 and perplexity is 87.24608848482691
At time: 286.7186105251312 and batch: 400, loss is 4.4796536350250244 and perplexity is 88.20411656754649
At time: 287.23600029945374 and batch: 450, loss is 4.52179443359375 and perplexity is 92.00053880369106
At time: 287.7572109699249 and batch: 500, loss is 4.557080421447754 and perplexity is 95.3048233301133
At time: 288.27662444114685 and batch: 550, loss is 4.476982097625733 and perplexity is 87.96879045258578
At time: 288.8000681400299 and batch: 600, loss is 4.417948198318482 and perplexity is 82.92596305386613
At time: 289.3254384994507 and batch: 650, loss is 4.405721168518067 and perplexity is 81.9181983718366
At time: 289.85000014305115 and batch: 700, loss is 4.492229661941528 and perplexity is 89.32037826828308
At time: 290.3640432357788 and batch: 750, loss is 4.42284140586853 and perplexity is 83.33273139170677
At time: 290.8748791217804 and batch: 800, loss is 4.518841867446899 and perplexity is 91.72930174704857
At time: 291.376859664917 and batch: 850, loss is 4.435370979309082 and perplexity is 84.38342358652703
At time: 291.8901171684265 and batch: 900, loss is 4.445897121429443 and perplexity is 85.27634676934647
At time: 292.402227640152 and batch: 950, loss is 4.418861627578735 and perplexity is 83.00174466025354
At time: 292.9387264251709 and batch: 1000, loss is 4.3426102447509765 and perplexity is 76.9080263364482
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929789845536395 and perplexity of 138.35043432331545
Finished 26 epochs...
Completing Train Step...
At time: 294.4282388687134 and batch: 50, loss is 4.568413496017456 and perplexity is 96.39106359448598
At time: 294.95532608032227 and batch: 100, loss is 4.490075769424439 and perplexity is 89.12819881514787
At time: 295.48850178718567 and batch: 150, loss is 4.51281213760376 and perplexity is 91.17786302283004
At time: 296.01305198669434 and batch: 200, loss is 4.536877403259277 and perplexity is 93.3986978285639
At time: 296.5276634693146 and batch: 250, loss is 4.572010555267334 and perplexity is 96.73841230381412
At time: 297.04383516311646 and batch: 300, loss is 4.474477558135987 and perplexity is 87.74874481455073
At time: 297.5601031780243 and batch: 350, loss is 4.468000087738037 and perplexity is 87.18219181363679
At time: 298.07850217819214 and batch: 400, loss is 4.47903039932251 and perplexity is 88.14916173967704
At time: 298.5977747440338 and batch: 450, loss is 4.5213381290435795 and perplexity is 91.958568115653
At time: 299.1027615070343 and batch: 500, loss is 4.556566419601441 and perplexity is 95.2558490624693
At time: 299.61294960975647 and batch: 550, loss is 4.476525001525879 and perplexity is 87.92858945012168
At time: 300.1337172985077 and batch: 600, loss is 4.417602062225342 and perplexity is 82.89726435210959
At time: 300.649555683136 and batch: 650, loss is 4.405520038604736 and perplexity is 81.90172382851516
At time: 301.14974570274353 and batch: 700, loss is 4.492228775024414 and perplexity is 89.32029904854609
At time: 301.67104959487915 and batch: 750, loss is 4.422901802062988 and perplexity is 83.33776452354611
At time: 302.17718744277954 and batch: 800, loss is 4.5187438869476315 and perplexity is 91.72031450456035
At time: 302.6874589920044 and batch: 850, loss is 4.435555410385132 and perplexity is 84.3989879473717
At time: 303.2004418373108 and batch: 900, loss is 4.44609414100647 and perplexity is 85.29314953429986
At time: 303.7119653224945 and batch: 950, loss is 4.419258651733398 and perplexity is 83.03470490033557
At time: 304.2228412628174 and batch: 1000, loss is 4.342914743423462 and perplexity is 76.93144829416767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929687127834413 and perplexity of 138.33622401447113
Finished 27 epochs...
Completing Train Step...
At time: 305.80233430862427 and batch: 50, loss is 4.567823419570923 and perplexity is 96.3342022761148
At time: 306.32992720603943 and batch: 100, loss is 4.489648380279541 and perplexity is 89.09011452945484
At time: 306.8522992134094 and batch: 150, loss is 4.512547025680542 and perplexity is 91.15369388811361
At time: 307.3600516319275 and batch: 200, loss is 4.536370191574097 and perplexity is 93.35133692966065
At time: 307.8811058998108 and batch: 250, loss is 4.57163911819458 and perplexity is 96.7024867435806
At time: 308.3961977958679 and batch: 300, loss is 4.473958244323731 and perplexity is 87.70318750965312
At time: 308.9095754623413 and batch: 350, loss is 4.467418699264527 and perplexity is 87.1315198237037
At time: 309.4178283214569 and batch: 400, loss is 4.478539752960205 and perplexity is 88.1059222826378
At time: 309.9292743206024 and batch: 450, loss is 4.520985689163208 and perplexity is 91.92616395950105
At time: 310.4486484527588 and batch: 500, loss is 4.556181993484497 and perplexity is 95.21923726401465
At time: 310.9744498729706 and batch: 550, loss is 4.476142539978027 and perplexity is 87.89496657583831
At time: 311.4911015033722 and batch: 600, loss is 4.4173215389251705 and perplexity is 82.87401299936396
At time: 312.0058403015137 and batch: 650, loss is 4.405340213775634 and perplexity is 81.88699718917191
At time: 312.5264081954956 and batch: 700, loss is 4.492238512039185 and perplexity is 89.32116876585145
At time: 313.04622983932495 and batch: 750, loss is 4.422965393066407 and perplexity is 83.34306422411967
At time: 313.56557869911194 and batch: 800, loss is 4.518651008605957 and perplexity is 91.71179606944649
At time: 314.0666334629059 and batch: 850, loss is 4.435715827941895 and perplexity is 84.41252811282256
At time: 314.56734347343445 and batch: 900, loss is 4.446258592605591 and perplexity is 85.30717728254601
At time: 315.0674524307251 and batch: 950, loss is 4.4195390319824215 and perplexity is 83.05798945568529
At time: 315.56886982917786 and batch: 1000, loss is 4.343081254959106 and perplexity is 76.94425933432584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929622371022294 and perplexity of 138.3272660916498
Finished 28 epochs...
Completing Train Step...
At time: 317.0595781803131 and batch: 50, loss is 4.567331113815308 and perplexity is 96.2867880659736
At time: 317.5701205730438 and batch: 100, loss is 4.489302043914795 and perplexity is 89.0592647255667
At time: 318.0783476829529 and batch: 150, loss is 4.512350215911865 and perplexity is 91.13575571596711
At time: 318.5831937789917 and batch: 200, loss is 4.535952367782593 and perplexity is 93.31234066747311
At time: 319.1026608943939 and batch: 250, loss is 4.571325807571411 and perplexity is 96.67219357303092
At time: 319.62172770500183 and batch: 300, loss is 4.473479948043823 and perplexity is 87.66124943154409
At time: 320.14089131355286 and batch: 350, loss is 4.46691710472107 and perplexity is 87.08782608798283
At time: 320.64934754371643 and batch: 400, loss is 4.478101816177368 and perplexity is 88.06734590610813
At time: 321.1795377731323 and batch: 450, loss is 4.52069130897522 and perplexity is 91.89910670082948
At time: 321.6886668205261 and batch: 500, loss is 4.555840978622436 and perplexity is 95.1867716249009
At time: 322.2003219127655 and batch: 550, loss is 4.475790843963623 and perplexity is 87.86405970163825
At time: 322.71358585357666 and batch: 600, loss is 4.417064342498779 and perplexity is 82.8527008402029
At time: 323.21383261680603 and batch: 650, loss is 4.405153017044068 and perplexity is 81.8716696456181
At time: 323.73361253738403 and batch: 700, loss is 4.492236461639404 and perplexity is 89.32098562193437
At time: 324.2629163265228 and batch: 750, loss is 4.423008651733398 and perplexity is 83.34666961196258
At time: 324.7960412502289 and batch: 800, loss is 4.518541259765625 and perplexity is 91.70173135848837
At time: 325.31309819221497 and batch: 850, loss is 4.435866317749023 and perplexity is 84.4252322937984
At time: 325.8208122253418 and batch: 900, loss is 4.446389389038086 and perplexity is 85.31833588673771
At time: 326.33973693847656 and batch: 950, loss is 4.419741306304932 and perplexity is 83.07479165350081
At time: 326.8524651527405 and batch: 1000, loss is 4.343162631988525 and perplexity is 76.95052108435917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929581432807736 and perplexity of 138.32160333626368
Finished 29 epochs...
Completing Train Step...
At time: 328.45888471603394 and batch: 50, loss is 4.56688961982727 and perplexity is 96.24428741049731
At time: 329.00467801094055 and batch: 100, loss is 4.48899076461792 and perplexity is 89.03154673450497
At time: 329.5469419956207 and batch: 150, loss is 4.512188243865967 and perplexity is 91.12099546656547
At time: 330.0970370769501 and batch: 200, loss is 4.535592432022095 and perplexity is 93.27876026292788
At time: 330.6135971546173 and batch: 250, loss is 4.571055250167847 and perplexity is 96.64604173328736
At time: 331.14410066604614 and batch: 300, loss is 4.473046436309814 and perplexity is 87.62325548730298
At time: 331.6627140045166 and batch: 350, loss is 4.466477775573731 and perplexity is 87.04957427078837
At time: 332.18054151535034 and batch: 400, loss is 4.477712917327881 and perplexity is 88.03310327549924
At time: 332.7419397830963 and batch: 450, loss is 4.520414018630982 and perplexity is 91.87362749862886
At time: 333.26958751678467 and batch: 500, loss is 4.555536994934082 and perplexity is 95.15784079645256
At time: 333.81479024887085 and batch: 550, loss is 4.475478458404541 and perplexity is 87.83661652487432
At time: 334.32900524139404 and batch: 600, loss is 4.416811246871948 and perplexity is 82.83173383738934
At time: 334.8436870574951 and batch: 650, loss is 4.404976768493652 and perplexity is 81.85724115406066
At time: 335.3711988925934 and batch: 700, loss is 4.492221937179566 and perplexity is 89.3196882922875
At time: 335.90966033935547 and batch: 750, loss is 4.423046760559082 and perplexity is 83.34984591618836
At time: 336.4366354942322 and batch: 800, loss is 4.5184577941894535 and perplexity is 91.6940777400559
At time: 336.95626950263977 and batch: 850, loss is 4.435973129272461 and perplexity is 84.43425036308454
At time: 337.47261667251587 and batch: 900, loss is 4.4464927005767825 and perplexity is 85.32715071062586
At time: 337.99941658973694 and batch: 950, loss is 4.419916877746582 and perplexity is 83.08937849491511
At time: 338.5192549228668 and batch: 1000, loss is 4.343194179534912 and perplexity is 76.95294872278538
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929549426567264 and perplexity of 138.3171762526123
Finished 30 epochs...
Completing Train Step...
At time: 340.07859325408936 and batch: 50, loss is 4.566471405029297 and perplexity is 96.20404504084566
At time: 340.6219382286072 and batch: 100, loss is 4.488706731796265 and perplexity is 89.0062624440238
At time: 341.1595981121063 and batch: 150, loss is 4.512046070098877 and perplexity is 91.10804137226701
At time: 341.6771984100342 and batch: 200, loss is 4.5352687168121335 and perplexity is 93.24856939634944
At time: 342.2048795223236 and batch: 250, loss is 4.570816278457642 and perplexity is 96.62294882279585
At time: 342.7347626686096 and batch: 300, loss is 4.472644853591919 and perplexity is 87.5880745667112
At time: 343.26422691345215 and batch: 350, loss is 4.466065845489502 and perplexity is 87.0137233168771
At time: 343.77142691612244 and batch: 400, loss is 4.477347269058227 and perplexity is 88.00092000785097
At time: 344.2983977794647 and batch: 450, loss is 4.520153732299804 and perplexity is 91.8497171610973
At time: 344.8237371444702 and batch: 500, loss is 4.555252046585083 and perplexity is 95.13072958965354
At time: 345.3539719581604 and batch: 550, loss is 4.475181198120117 and perplexity is 87.81051006766198
At time: 345.9145109653473 and batch: 600, loss is 4.416571493148804 and perplexity is 82.81187700127767
At time: 346.43598556518555 and batch: 650, loss is 4.404791412353515 and perplexity is 81.84206981789167
At time: 346.9598398208618 and batch: 700, loss is 4.492188091278076 and perplexity is 89.31666523807577
At time: 347.4716613292694 and batch: 750, loss is 4.423080940246582 and perplexity is 83.35269483656226
At time: 347.9986662864685 and batch: 800, loss is 4.518362150192261 and perplexity is 91.68530817132695
At time: 348.5152621269226 and batch: 850, loss is 4.436060018539429 and perplexity is 84.44158711194335
At time: 349.03381156921387 and batch: 900, loss is 4.446573123931885 and perplexity is 85.33401328231918
At time: 349.54576659202576 and batch: 950, loss is 4.420068817138672 and perplexity is 83.10200400370455
At time: 350.0736303329468 and batch: 1000, loss is 4.343194646835327 and perplexity is 76.95298468293868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929532679115853 and perplexity of 138.3148598118211
Finished 31 epochs...
Completing Train Step...
At time: 351.6596667766571 and batch: 50, loss is 4.566081714630127 and perplexity is 96.1665625518893
At time: 352.17146706581116 and batch: 100, loss is 4.488450384140014 and perplexity is 88.9834488214889
At time: 352.67781352996826 and batch: 150, loss is 4.511919517517089 and perplexity is 91.09651214395197
At time: 353.1795241832733 and batch: 200, loss is 4.534969997406006 and perplexity is 93.2207183991009
At time: 353.6874659061432 and batch: 250, loss is 4.570595293045044 and perplexity is 96.60159891967933
At time: 354.1911463737488 and batch: 300, loss is 4.472282838821411 and perplexity is 87.55637212871937
At time: 354.7112627029419 and batch: 350, loss is 4.465689182281494 and perplexity is 86.98095462048045
At time: 355.2340111732483 and batch: 400, loss is 4.477014741897583 and perplexity is 87.97166217656805
At time: 355.74750900268555 and batch: 450, loss is 4.519932689666748 and perplexity is 91.82941670148674
At time: 356.2667770385742 and batch: 500, loss is 4.554975757598877 and perplexity is 95.10444964741465
At time: 356.78488779067993 and batch: 550, loss is 4.4749015522003175 and perplexity is 87.78595764995778
At time: 357.29895877838135 and batch: 600, loss is 4.4163430881500245 and perplexity is 82.79296451454775
At time: 357.81923174858093 and batch: 650, loss is 4.4046204471588135 and perplexity is 81.82807886850723
At time: 358.3419506549835 and batch: 700, loss is 4.492156314849853 and perplexity is 89.31382711856662
At time: 358.88728523254395 and batch: 750, loss is 4.423092975616455 and perplexity is 83.35369802311139
At time: 359.40975737571716 and batch: 800, loss is 4.51825701713562 and perplexity is 91.67566952130909
At time: 359.9392647743225 and batch: 850, loss is 4.436134691238403 and perplexity is 84.4478928285882
At time: 360.45459389686584 and batch: 900, loss is 4.446635713577271 and perplexity is 85.33935447509978
At time: 360.9706070423126 and batch: 950, loss is 4.420195074081421 and perplexity is 83.11249687105108
At time: 361.485059261322 and batch: 1000, loss is 4.343159580230713 and perplexity is 76.95028625036365
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929520397651486 and perplexity of 138.3131611132301
Finished 32 epochs...
Completing Train Step...
At time: 362.9922239780426 and batch: 50, loss is 4.565706758499146 and perplexity is 96.13051106894973
At time: 363.51633381843567 and batch: 100, loss is 4.488206634521484 and perplexity is 88.96176178299423
At time: 364.0203106403351 and batch: 150, loss is 4.51178786277771 and perplexity is 91.08451964583918
At time: 364.5475957393646 and batch: 200, loss is 4.534687662124634 and perplexity is 93.19440261645376
At time: 365.06761264801025 and batch: 250, loss is 4.570383224487305 and perplexity is 96.58111493000295
At time: 365.6029586791992 and batch: 300, loss is 4.471963663101196 and perplexity is 87.52843071993259
At time: 366.1367144584656 and batch: 350, loss is 4.46533763885498 and perplexity is 86.95038241169608
At time: 366.65260791778564 and batch: 400, loss is 4.4767312335968015 and perplexity is 87.9467250152206
At time: 367.17942094802856 and batch: 450, loss is 4.519707546234131 and perplexity is 91.80874423861728
At time: 367.7112681865692 and batch: 500, loss is 4.554715385437012 and perplexity is 95.0796903197167
At time: 368.21677589416504 and batch: 550, loss is 4.474637384414673 and perplexity is 87.76277049069981
At time: 368.74099254608154 and batch: 600, loss is 4.416125755310059 and perplexity is 82.77497283960236
At time: 369.24234676361084 and batch: 650, loss is 4.404437198638916 and perplexity is 81.81308536797884
At time: 369.7545819282532 and batch: 700, loss is 4.492115907669067 and perplexity is 89.31021827151969
At time: 370.2624650001526 and batch: 750, loss is 4.423065128326416 and perplexity is 83.35137688082568
At time: 370.7720458507538 and batch: 800, loss is 4.518148393630981 and perplexity is 91.6657119296196
At time: 371.2890808582306 and batch: 850, loss is 4.436178293228149 and perplexity is 84.45157500501989
At time: 371.8104717731476 and batch: 900, loss is 4.4466617393493655 and perplexity is 85.34157552659224
At time: 372.35298252105713 and batch: 950, loss is 4.420263309478759 and perplexity is 83.11816827879207
At time: 372.87416076660156 and batch: 1000, loss is 4.3429532146453855 and perplexity is 76.93440799791924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929462339819931 and perplexity of 138.30513118412279
Finished 33 epochs...
Completing Train Step...
At time: 374.3650300502777 and batch: 50, loss is 4.565339803695679 and perplexity is 96.0952419876284
At time: 374.9168429374695 and batch: 100, loss is 4.487926244735718 and perplexity is 88.93682131035678
At time: 375.435213804245 and batch: 150, loss is 4.511642169952393 and perplexity is 91.07125025148059
At time: 375.95946049690247 and batch: 200, loss is 4.534361162185669 and perplexity is 93.16397961651167
At time: 376.4952988624573 and batch: 250, loss is 4.570175590515137 and perplexity is 96.56106349122564
At time: 377.01776099205017 and batch: 300, loss is 4.471627321243286 and perplexity is 87.49899619523316
At time: 377.54602432250977 and batch: 350, loss is 4.46498399734497 and perplexity is 86.91963858363135
At time: 378.07810711860657 and batch: 400, loss is 4.4764001750946045 and perplexity is 87.91761432310088
At time: 378.6250088214874 and batch: 450, loss is 4.519431705474854 and perplexity is 91.78342313735463
At time: 379.16268944740295 and batch: 500, loss is 4.554321508407593 and perplexity is 95.04224798805677
At time: 379.68392992019653 and batch: 550, loss is 4.474328145980835 and perplexity is 87.73563506487854
At time: 380.2221910953522 and batch: 600, loss is 4.41579815864563 and perplexity is 82.74786047580432
At time: 380.75802421569824 and batch: 650, loss is 4.404194469451904 and perplexity is 81.79322935419512
At time: 381.28326058387756 and batch: 700, loss is 4.492002038955689 and perplexity is 89.30004921085347
At time: 381.81101417541504 and batch: 750, loss is 4.422914705276489 and perplexity is 83.33883985545208
At time: 382.34851026535034 and batch: 800, loss is 4.518091068267823 and perplexity is 91.66045731000698
At time: 382.8782238960266 and batch: 850, loss is 4.436151037216186 and perplexity is 84.4492732232501
At time: 383.4049298763275 and batch: 900, loss is 4.446686210632325 and perplexity is 85.34366396998847
At time: 383.9423243999481 and batch: 950, loss is 4.420352544784546 and perplexity is 83.12558568489716
At time: 384.4624626636505 and batch: 1000, loss is 4.342853841781616 and perplexity is 76.92676318532381
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929448941858803 and perplexity of 138.30327818976457
Finished 34 epochs...
Completing Train Step...
At time: 386.0681502819061 and batch: 50, loss is 4.564997472763062 and perplexity is 96.0623512438994
At time: 386.5852108001709 and batch: 100, loss is 4.487692489624023 and perplexity is 88.91603430338684
At time: 387.1017665863037 and batch: 150, loss is 4.511525392532349 and perplexity is 91.06061580677962
At time: 387.621301651001 and batch: 200, loss is 4.534105167388916 and perplexity is 93.14013317489778
At time: 388.13044691085815 and batch: 250, loss is 4.569974670410156 and perplexity is 96.54166438111277
At time: 388.65409326553345 and batch: 300, loss is 4.471320533752442 and perplexity is 87.47215671495795
At time: 389.17454385757446 and batch: 350, loss is 4.464674224853516 and perplexity is 86.89271744056104
At time: 389.69832277297974 and batch: 400, loss is 4.476124124526978 and perplexity is 87.8933479652876
At time: 390.21175479888916 and batch: 450, loss is 4.519248094558716 and perplexity is 91.76657224599724
At time: 390.7310893535614 and batch: 500, loss is 4.554061460494995 and perplexity is 95.01753566319235
At time: 391.24689984321594 and batch: 550, loss is 4.474083833694458 and perplexity is 87.71420278946977
At time: 391.7576937675476 and batch: 600, loss is 4.415598840713501 and perplexity is 82.73136898694551
At time: 392.2804992198944 and batch: 650, loss is 4.404026393890381 and perplexity is 81.77948306648246
At time: 392.7998869419098 and batch: 700, loss is 4.491951217651367 and perplexity is 89.29551098119697
At time: 393.3377251625061 and batch: 750, loss is 4.42291708946228 and perplexity is 83.33903855096676
At time: 393.86105942726135 and batch: 800, loss is 4.517991857528687 and perplexity is 91.65136405936923
At time: 394.3819053173065 and batch: 850, loss is 4.4361850452423095 and perplexity is 84.45214522517524
At time: 394.8952269554138 and batch: 900, loss is 4.446710920333862 and perplexity is 85.34577281250759
At time: 395.41121006011963 and batch: 950, loss is 4.420422563552856 and perplexity is 83.13140623979356
At time: 395.92444682121277 and batch: 1000, loss is 4.34276967048645 and perplexity is 76.92028843253158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929426984089177 and perplexity of 138.30024139158425
Finished 35 epochs...
Completing Train Step...
At time: 397.44773960113525 and batch: 50, loss is 4.56466757774353 and perplexity is 96.03066597935242
At time: 397.9725806713104 and batch: 100, loss is 4.487480306625367 and perplexity is 88.89716983403031
At time: 398.4856972694397 and batch: 150, loss is 4.511419200897217 and perplexity is 91.0509464445028
At time: 399.0219979286194 and batch: 200, loss is 4.533858108520508 and perplexity is 93.11712492130575
At time: 399.5426342487335 and batch: 250, loss is 4.569787292480469 and perplexity is 96.52357629861915
At time: 400.0677230358124 and batch: 300, loss is 4.471031179428101 and perplexity is 87.44684992964349
At time: 400.58319544792175 and batch: 350, loss is 4.4643738746643065 and perplexity is 86.86662311535078
At time: 401.090961933136 and batch: 400, loss is 4.47586573600769 and perplexity is 87.87064026708222
At time: 401.61562943458557 and batch: 450, loss is 4.519096126556397 and perplexity is 91.75262772292085
At time: 402.13943886756897 and batch: 500, loss is 4.553795347213745 and perplexity is 94.99225359909643
At time: 402.6575071811676 and batch: 550, loss is 4.473839883804321 and perplexity is 87.69280752912766
At time: 403.19229221343994 and batch: 600, loss is 4.41540111541748 and perplexity is 82.71501251961935
At time: 403.70602083206177 and batch: 650, loss is 4.403848524093628 and perplexity is 81.7649382600301
At time: 404.22847294807434 and batch: 700, loss is 4.491888017654419 and perplexity is 89.2898676835055
At time: 404.7513918876648 and batch: 750, loss is 4.4229090213775635 and perplexity is 83.33836616725594
At time: 405.2838463783264 and batch: 800, loss is 4.517892360687256 and perplexity is 91.64224549177436
At time: 405.7986125946045 and batch: 850, loss is 4.436215620040894 and perplexity is 84.45472737197963
At time: 406.31172919273376 and batch: 900, loss is 4.446728839874267 and perplexity is 85.3473021832347
At time: 406.85121750831604 and batch: 950, loss is 4.4204771900177 and perplexity is 83.13594753867038
At time: 407.3837356567383 and batch: 1000, loss is 4.342666854858399 and perplexity is 76.9123802313168
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929405398485137 and perplexity of 138.29725612955448
Finished 36 epochs...
Completing Train Step...
At time: 408.96824169158936 and batch: 50, loss is 4.564345674514771 and perplexity is 95.99975837280978
At time: 409.5160279273987 and batch: 100, loss is 4.487262315750122 and perplexity is 88.8777931742157
At time: 410.0349600315094 and batch: 150, loss is 4.511323366165161 and perplexity is 91.04222101955293
At time: 410.56402945518494 and batch: 200, loss is 4.533620729446411 and perplexity is 93.09502348772206
At time: 411.0921514034271 and batch: 250, loss is 4.569614582061767 and perplexity is 96.50690711085466
At time: 411.618700504303 and batch: 300, loss is 4.470780611038208 and perplexity is 87.42494125818031
At time: 412.1315543651581 and batch: 350, loss is 4.464084043502807 and perplexity is 86.84145010921476
At time: 412.6758234500885 and batch: 400, loss is 4.475646715164185 and perplexity is 87.85139687276076
At time: 413.1995007991791 and batch: 450, loss is 4.518921775817871 and perplexity is 91.7366319789911
At time: 413.72845697402954 and batch: 500, loss is 4.553544912338257 and perplexity is 94.96846720448973
At time: 414.2489516735077 and batch: 550, loss is 4.473611831665039 and perplexity is 87.6728112769517
At time: 414.7742404937744 and batch: 600, loss is 4.415215587615966 and perplexity is 82.69966800865511
At time: 415.30030512809753 and batch: 650, loss is 4.403668432235718 and perplexity is 81.75021438625166
At time: 415.8187520503998 and batch: 700, loss is 4.491829442977905 and perplexity is 89.28463771156346
At time: 416.35329127311707 and batch: 750, loss is 4.422896900177002 and perplexity is 83.33735601232732
At time: 416.8644530773163 and batch: 800, loss is 4.51779275894165 and perplexity is 91.63311821870585
At time: 417.37538409233093 and batch: 850, loss is 4.436244058609009 and perplexity is 84.4571291776484
At time: 417.89705777168274 and batch: 900, loss is 4.446738338470459 and perplexity is 85.34811286664433
At time: 418.432941198349 and batch: 950, loss is 4.420532474517822 and perplexity is 83.14054379502191
At time: 418.94205141067505 and batch: 1000, loss is 4.342558670043945 and perplexity is 76.90405992980554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.9293975830078125 and perplexity of 138.29617527470882
Finished 37 epochs...
Completing Train Step...
At time: 420.56463289260864 and batch: 50, loss is 4.564045724868774 and perplexity is 95.97096759737741
At time: 421.0861005783081 and batch: 100, loss is 4.487075538635254 and perplexity is 88.86119438661581
At time: 421.6049757003784 and batch: 150, loss is 4.511232252120972 and perplexity is 91.0339261724981
At time: 422.1195487976074 and batch: 200, loss is 4.533392238616943 and perplexity is 93.07375455855622
At time: 422.63511967658997 and batch: 250, loss is 4.569442987442017 and perplexity is 96.49034846555351
At time: 423.1529483795166 and batch: 300, loss is 4.470515213012695 and perplexity is 87.40174193005394
At time: 423.67544388771057 and batch: 350, loss is 4.463799104690552 and perplexity is 86.81670913456549
At time: 424.2072379589081 and batch: 400, loss is 4.475400104522705 and perplexity is 87.82973445462423
At time: 424.73520708084106 and batch: 450, loss is 4.518764400482178 and perplexity is 91.72219603169889
At time: 425.260249376297 and batch: 500, loss is 4.553306941986084 and perplexity is 94.9458702137173
At time: 425.7980217933655 and batch: 550, loss is 4.473383226394653 and perplexity is 87.65277110095646
At time: 426.30915999412537 and batch: 600, loss is 4.415034046173096 and perplexity is 82.68465595429613
At time: 426.8455879688263 and batch: 650, loss is 4.403488664627075 and perplexity is 81.7355196665623
At time: 427.3607499599457 and batch: 700, loss is 4.491768245697021 and perplexity is 89.27917390169767
At time: 427.8734278678894 and batch: 750, loss is 4.422880687713623 and perplexity is 83.33600491944718
At time: 428.41032457351685 and batch: 800, loss is 4.517705326080322 and perplexity is 91.62510682322231
At time: 428.93143129348755 and batch: 850, loss is 4.436268882751465 and perplexity is 84.45922577947762
At time: 429.43698620796204 and batch: 900, loss is 4.446747522354126 and perplexity is 85.34889669738347
At time: 429.9626705646515 and batch: 950, loss is 4.420582580566406 and perplexity is 83.14470974351734
At time: 430.4934666156769 and batch: 1000, loss is 4.342453517913818 and perplexity is 76.89597372923659
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929386790205792 and perplexity of 138.29468267952362
Finished 38 epochs...
Completing Train Step...
At time: 432.0674192905426 and batch: 50, loss is 4.5637509155273435 and perplexity is 95.94267862975441
At time: 432.6119873523712 and batch: 100, loss is 4.48689564704895 and perplexity is 88.84521044312834
At time: 433.13159561157227 and batch: 150, loss is 4.5111379718780515 and perplexity is 91.02534387640142
At time: 433.6551718711853 and batch: 200, loss is 4.533167028427124 and perplexity is 93.05279576078094
At time: 434.18184638023376 and batch: 250, loss is 4.569274291992188 and perplexity is 96.47407235570641
At time: 434.70136046409607 and batch: 300, loss is 4.470250692367554 and perplexity is 87.37862542242364
At time: 435.2317681312561 and batch: 350, loss is 4.463519592285156 and perplexity is 86.79244617842328
At time: 435.76885175704956 and batch: 400, loss is 4.475160474777222 and perplexity is 87.80869035920735
At time: 436.2828929424286 and batch: 450, loss is 4.518615818023681 and perplexity is 91.70856873472748
At time: 436.81201124191284 and batch: 500, loss is 4.553070688247681 and perplexity is 94.92344154646597
At time: 437.32444953918457 and batch: 550, loss is 4.473157005310059 and perplexity is 87.63294443869934
At time: 437.842223405838 and batch: 600, loss is 4.414852409362793 and perplexity is 82.66963874100827
At time: 438.37174224853516 and batch: 650, loss is 4.403306674957276 and perplexity is 81.72064599979517
At time: 438.885968208313 and batch: 700, loss is 4.491702766418457 and perplexity is 89.27332815718941
At time: 439.424031496048 and batch: 750, loss is 4.422862100601196 and perplexity is 83.33445595814993
At time: 439.9429044723511 and batch: 800, loss is 4.517614269256592 and perplexity is 91.61676411185724
At time: 440.4715096950531 and batch: 850, loss is 4.436285877227784 and perplexity is 84.46066113198651
At time: 441.00329399108887 and batch: 900, loss is 4.446746730804444 and perplexity is 85.3488291395181
At time: 441.5159595012665 and batch: 950, loss is 4.420623760223389 and perplexity is 83.1481336846424
At time: 442.03178334236145 and batch: 1000, loss is 4.3423588085174565 and perplexity is 76.88869130284458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929377113900533 and perplexity of 138.29334450443258
Finished 39 epochs...
Completing Train Step...
At time: 443.5944435596466 and batch: 50, loss is 4.563463182449341 and perplexity is 95.91507671870228
At time: 444.12329602241516 and batch: 100, loss is 4.48671703338623 and perplexity is 88.8293428917989
At time: 444.64107632637024 and batch: 150, loss is 4.511044054031372 and perplexity is 91.01679537354607
At time: 445.18004989624023 and batch: 200, loss is 4.532943029403686 and perplexity is 93.0319543597162
At time: 445.7270851135254 and batch: 250, loss is 4.5691102027893065 and perplexity is 96.45824330079887
At time: 446.26052260398865 and batch: 300, loss is 4.4699877166748045 and perplexity is 87.35564998899439
At time: 446.7700424194336 and batch: 350, loss is 4.463246288299561 and perplexity is 86.76872869815176
At time: 447.30726957321167 and batch: 400, loss is 4.474930543899536 and perplexity is 87.78850275093077
At time: 447.83108949661255 and batch: 450, loss is 4.51847993850708 and perplexity is 91.69610826532008
At time: 448.37271904945374 and batch: 500, loss is 4.5528342914581295 and perplexity is 94.90100460174654
At time: 448.90870475769043 and batch: 550, loss is 4.472929487228393 and perplexity is 87.61300862725476
At time: 449.4293189048767 and batch: 600, loss is 4.414663820266724 and perplexity is 82.65404961858118
At time: 449.95640325546265 and batch: 650, loss is 4.403122806549073 and perplexity is 81.70562153600231
At time: 450.48249220848083 and batch: 700, loss is 4.491628999710083 and perplexity is 89.26674300051134
At time: 451.0098783969879 and batch: 750, loss is 4.422840557098389 and perplexity is 83.3326606614026
At time: 451.5229923725128 and batch: 800, loss is 4.517519063949585 and perplexity is 91.6080421248993
At time: 452.0477910041809 and batch: 850, loss is 4.4362945556640625 and perplexity is 84.46139412163282
At time: 452.5818898677826 and batch: 900, loss is 4.446741542816162 and perplexity is 85.34838635194126
At time: 453.10112380981445 and batch: 950, loss is 4.4206528472900395 and perplexity is 83.15055225512315
At time: 453.6211807727814 and batch: 1000, loss is 4.342227773666382 and perplexity is 76.87861686469606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929361855111471 and perplexity of 138.29123433155942
Finished 40 epochs...
Completing Train Step...
At time: 455.2656075954437 and batch: 50, loss is 4.563180208206177 and perplexity is 95.88793906226978
At time: 455.79470014572144 and batch: 100, loss is 4.486538209915161 and perplexity is 88.81345954057053
At time: 456.31653475761414 and batch: 150, loss is 4.510946159362793 and perplexity is 91.00788575063723
At time: 456.8366310596466 and batch: 200, loss is 4.53271258354187 and perplexity is 93.0105180008723
At time: 457.364462852478 and batch: 250, loss is 4.568951015472412 and perplexity is 96.44288959394558
At time: 457.8830990791321 and batch: 300, loss is 4.469745273590088 and perplexity is 87.33447378285973
At time: 458.3990309238434 and batch: 350, loss is 4.462980060577393 and perplexity is 86.74563153184444
At time: 458.9211575984955 and batch: 400, loss is 4.474731674194336 and perplexity is 87.77104601313447
At time: 459.45216512680054 and batch: 450, loss is 4.518326625823975 and perplexity is 91.68205116652491
At time: 459.97898840904236 and batch: 500, loss is 4.552614946365356 and perplexity is 94.88019081487234
At time: 460.50922083854675 and batch: 550, loss is 4.472711019515991 and perplexity is 87.59387010433404
At time: 461.0366566181183 and batch: 600, loss is 4.414479370117188 and perplexity is 82.63880547270497
At time: 461.55876898765564 and batch: 650, loss is 4.402937612533569 and perplexity is 81.69049154489602
At time: 462.09727025032043 and batch: 700, loss is 4.491556921005249 and perplexity is 89.26030900117107
At time: 462.6253066062927 and batch: 750, loss is 4.422820911407471 and perplexity is 83.33102354978905
At time: 463.1563460826874 and batch: 800, loss is 4.517417135238648 and perplexity is 91.59870511111697
At time: 463.6867516040802 and batch: 850, loss is 4.436304998397827 and perplexity is 84.46227613409035
At time: 464.20816254615784 and batch: 900, loss is 4.446732091903686 and perplexity is 85.34757973562354
At time: 464.7221829891205 and batch: 950, loss is 4.420678329467774 and perplexity is 83.15267113927118
At time: 465.2492220401764 and batch: 1000, loss is 4.342106695175171 and perplexity is 76.86930908125726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929334687023628 and perplexity of 138.2874772741935
Finished 41 epochs...
Completing Train Step...
At time: 466.8152174949646 and batch: 50, loss is 4.562912845611573 and perplexity is 95.86230564095257
At time: 467.3556156158447 and batch: 100, loss is 4.486372547149658 and perplexity is 88.7987476758869
At time: 467.8897442817688 and batch: 150, loss is 4.510847406387329 and perplexity is 90.99889889487537
At time: 468.4017367362976 and batch: 200, loss is 4.532476005554199 and perplexity is 92.9885163623457
At time: 468.92526030540466 and batch: 250, loss is 4.568789014816284 and perplexity is 96.42726704801798
At time: 469.44412636756897 and batch: 300, loss is 4.469493827819824 and perplexity is 87.31251665945742
At time: 469.9864044189453 and batch: 350, loss is 4.462719993591309 and perplexity is 86.72307479015502
At time: 470.50190925598145 and batch: 400, loss is 4.474511528015137 and perplexity is 87.75172567943721
At time: 471.0284175872803 and batch: 450, loss is 4.518176021575928 and perplexity is 91.66824449984702
At time: 471.55299639701843 and batch: 500, loss is 4.5523912143707275 and perplexity is 94.85896545501517
At time: 472.0646059513092 and batch: 550, loss is 4.4724935531616214 and perplexity is 87.57482345581501
At time: 472.58057713508606 and batch: 600, loss is 4.414307470321655 and perplexity is 82.62460109984056
At time: 473.1049892902374 and batch: 650, loss is 4.402754421234131 and perplexity is 81.67552792824219
At time: 473.62644696235657 and batch: 700, loss is 4.491478347778321 and perplexity is 89.25329580618451
At time: 474.1582932472229 and batch: 750, loss is 4.422792453765869 and perplexity is 83.32865217912848
At time: 474.68945813179016 and batch: 800, loss is 4.517315979003906 and perplexity is 91.58943979963065
At time: 475.216588973999 and batch: 850, loss is 4.436307973861695 and perplexity is 84.46252744891503
At time: 475.74419927597046 and batch: 900, loss is 4.446718788146972 and perplexity is 85.3464442997394
At time: 476.2668333053589 and batch: 950, loss is 4.420712909698486 and perplexity is 83.15554662754079
At time: 476.7961871623993 and batch: 1000, loss is 4.342016248703003 and perplexity is 76.86235683784057
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929329104539825 and perplexity of 138.28670528874622
Finished 42 epochs...
Completing Train Step...
At time: 478.34211468696594 and batch: 50, loss is 4.562664289474487 and perplexity is 95.83848143751896
At time: 478.8896424770355 and batch: 100, loss is 4.4862002086639405 and perplexity is 88.78344555278905
At time: 479.42335867881775 and batch: 150, loss is 4.510759649276733 and perplexity is 90.99091344483607
At time: 479.97412633895874 and batch: 200, loss is 4.532264566421508 and perplexity is 92.96885702954526
At time: 480.49159359931946 and batch: 250, loss is 4.568632564544678 and perplexity is 96.41218215594658
At time: 481.0020008087158 and batch: 300, loss is 4.4692581939697265 and perplexity is 87.29194529874486
At time: 481.51084995269775 and batch: 350, loss is 4.462476892471313 and perplexity is 86.7019948759233
At time: 482.0290985107422 and batch: 400, loss is 4.474305095672608 and perplexity is 87.73361275475528
At time: 482.5528554916382 and batch: 450, loss is 4.51804409980774 and perplexity is 91.65615226057818
At time: 483.0830240249634 and batch: 500, loss is 4.552180805206299 and perplexity is 94.8390083590068
At time: 483.6084249019623 and batch: 550, loss is 4.472278881072998 and perplexity is 87.55602560331224
At time: 484.1268150806427 and batch: 600, loss is 4.414137773513794 and perplexity is 82.61058115838637
At time: 484.6591217517853 and batch: 650, loss is 4.402586345672607 and perplexity is 81.66180142160037
At time: 485.17343640327454 and batch: 700, loss is 4.491398448944092 and perplexity is 89.24616485677959
At time: 485.70011258125305 and batch: 750, loss is 4.422764291763306 and perplexity is 83.32630551045585
At time: 486.24101877212524 and batch: 800, loss is 4.517215337753296 and perplexity is 91.58022258769014
At time: 486.7460448741913 and batch: 850, loss is 4.43630090713501 and perplexity is 84.46193057742741
At time: 487.254163980484 and batch: 900, loss is 4.446696453094482 and perplexity is 85.3445381037137
At time: 487.76870131492615 and batch: 950, loss is 4.420720624923706 and perplexity is 83.15618819378618
At time: 488.2906768321991 and batch: 1000, loss is 4.341887998580932 and perplexity is 76.85249986328607
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929319428234566 and perplexity of 138.2853671908465
Finished 43 epochs...
Completing Train Step...
At time: 489.87321424484253 and batch: 50, loss is 4.562404203414917 and perplexity is 95.81355842573116
At time: 490.3928406238556 and batch: 100, loss is 4.486028747558594 and perplexity is 88.76822395007177
At time: 490.9119999408722 and batch: 150, loss is 4.5106729221343995 and perplexity is 90.98302240512328
At time: 491.42416524887085 and batch: 200, loss is 4.532057809829712 and perplexity is 92.9496370925154
At time: 491.9449963569641 and batch: 250, loss is 4.5684772491455075 and perplexity is 96.39720902219938
At time: 492.4713451862335 and batch: 300, loss is 4.469055414199829 and perplexity is 87.27424605274845
At time: 493.0419273376465 and batch: 350, loss is 4.462241048812866 and perplexity is 86.68154917134676
At time: 493.5708978176117 and batch: 400, loss is 4.4741337108612065 and perplexity is 87.71857783449505
At time: 494.09734320640564 and batch: 450, loss is 4.517884550094604 and perplexity is 91.64152971432046
At time: 494.6159372329712 and batch: 500, loss is 4.551980619430542 and perplexity is 94.82002483872519
At time: 495.14726662635803 and batch: 550, loss is 4.472072868347168 and perplexity is 87.53798980568058
At time: 495.67001509666443 and batch: 600, loss is 4.413975429534912 and perplexity is 82.59717091650887
At time: 496.220956325531 and batch: 650, loss is 4.402420997619629 and perplexity is 81.64829991798703
At time: 496.74845123291016 and batch: 700, loss is 4.491320095062256 and perplexity is 89.23917234727278
At time: 497.281210899353 and batch: 750, loss is 4.422734661102295 and perplexity is 83.32383653352287
At time: 497.8176019191742 and batch: 800, loss is 4.517115678787231 and perplexity is 91.57109625216323
At time: 498.34313678741455 and batch: 850, loss is 4.4362907981872555 and perplexity is 84.46107676049957
At time: 498.86869049072266 and batch: 900, loss is 4.446674327850342 and perplexity is 85.3426498558612
At time: 499.38605093955994 and batch: 950, loss is 4.420726318359375 and perplexity is 83.15666163954188
At time: 499.91460061073303 and batch: 1000, loss is 4.341766185760498 and perplexity is 76.84313881367981
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929313101419589 and perplexity of 138.28449228768196
Finished 44 epochs...
Completing Train Step...
At time: 501.43585753440857 and batch: 50, loss is 4.562142486572266 and perplexity is 95.78848568485921
At time: 501.9809010028839 and batch: 100, loss is 4.485860481262207 and perplexity is 88.75328850639197
At time: 502.50352787971497 and batch: 150, loss is 4.510588083267212 and perplexity is 90.9753038359911
At time: 503.0116994380951 and batch: 200, loss is 4.531855525970459 and perplexity is 92.93083678277193
At time: 503.5330867767334 and batch: 250, loss is 4.568319702148438 and perplexity is 96.38202312766948
At time: 504.0607008934021 and batch: 300, loss is 4.468832025527954 and perplexity is 87.25475215227316
At time: 504.5851171016693 and batch: 350, loss is 4.4620106983184815 and perplexity is 86.66158433318455
At time: 505.11002802848816 and batch: 400, loss is 4.473941621780395 and perplexity is 87.7017296717346
At time: 505.62500977516174 and batch: 450, loss is 4.517728929519653 and perplexity is 91.62726951639581
At time: 506.1313624382019 and batch: 500, loss is 4.551776065826416 and perplexity is 94.80063104450396
At time: 506.66853380203247 and batch: 550, loss is 4.471864967346192 and perplexity is 87.51979246166508
At time: 507.1876015663147 and batch: 600, loss is 4.413813915252685 and perplexity is 82.58383137102687
At time: 507.7029724121094 and batch: 650, loss is 4.402256603240967 and perplexity is 81.6348784996863
At time: 508.2212002277374 and batch: 700, loss is 4.491238613128662 and perplexity is 89.2319012631926
At time: 508.7349863052368 and batch: 750, loss is 4.422700157165528 and perplexity is 83.32096158273475
At time: 509.2430853843689 and batch: 800, loss is 4.517011833190918 and perplexity is 91.56158749079792
At time: 509.7610728740692 and batch: 850, loss is 4.436279067993164 and perplexity is 84.46008602148676
At time: 510.29984760284424 and batch: 900, loss is 4.4466469287872314 and perplexity is 85.3403115792452
At time: 510.8118073940277 and batch: 950, loss is 4.4207212924957275 and perplexity is 83.15624370654938
At time: 511.34339594841003 and batch: 1000, loss is 4.341641492843628 and perplexity is 76.83355761592584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929306774604611 and perplexity of 138.28361739005263
Finished 45 epochs...
Completing Train Step...
At time: 512.8911466598511 and batch: 50, loss is 4.561876726150513 and perplexity is 95.7630322789083
At time: 513.4363145828247 and batch: 100, loss is 4.485687351226806 and perplexity is 88.73792397648013
At time: 513.9592583179474 and batch: 150, loss is 4.510501289367676 and perplexity is 90.96740807726651
At time: 514.4788346290588 and batch: 200, loss is 4.531657695770264 and perplexity is 92.91245407511417
At time: 514.9969666004181 and batch: 250, loss is 4.568162488937378 and perplexity is 96.36687179135144
At time: 515.5344336032867 and batch: 300, loss is 4.468602514266967 and perplexity is 87.23472850199414
At time: 516.0774073600769 and batch: 350, loss is 4.461781482696534 and perplexity is 86.64172242064991
At time: 516.5927197933197 and batch: 400, loss is 4.473754711151123 and perplexity is 87.68533881811352
At time: 517.118301153183 and batch: 450, loss is 4.51758415222168 and perplexity is 91.61400492812336
At time: 517.6384661197662 and batch: 500, loss is 4.551566781997681 and perplexity is 94.78079288144828
At time: 518.1629316806793 and batch: 550, loss is 4.471651601791382 and perplexity is 87.5011207446111
At time: 518.6921081542969 and batch: 600, loss is 4.413646678924561 and perplexity is 82.57002150909342
At time: 519.2320828437805 and batch: 650, loss is 4.402085752487182 and perplexity is 81.6209323105519
At time: 519.7682347297668 and batch: 700, loss is 4.491148691177369 and perplexity is 89.22387771726528
At time: 520.2849810123444 and batch: 750, loss is 4.4226539421081545 and perplexity is 83.31711098869323
At time: 520.8026397228241 and batch: 800, loss is 4.516899900436401 and perplexity is 91.55133932366564
At time: 521.3160552978516 and batch: 850, loss is 4.436254777908325 and perplexity is 84.4580345037477
At time: 521.8316187858582 and batch: 900, loss is 4.446618232727051 and perplexity is 85.33786268366528
At time: 522.3787150382996 and batch: 950, loss is 4.420706996917724 and perplexity is 83.15505494847804
At time: 522.8984968662262 and batch: 1000, loss is 4.341511487960815 and perplexity is 76.8235695275361
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929297098299352 and perplexity of 138.2822793220322
Finished 46 epochs...
Completing Train Step...
At time: 524.5206396579742 and batch: 50, loss is 4.5615991020202635 and perplexity is 95.73644984049574
At time: 525.040461063385 and batch: 100, loss is 4.485506410598755 and perplexity is 88.72186913331437
At time: 525.5616545677185 and batch: 150, loss is 4.51040641784668 and perplexity is 90.95877827026904
At time: 526.0792388916016 and batch: 200, loss is 4.5314648246765135 and perplexity is 92.8945356764998
At time: 526.6054356098175 and batch: 250, loss is 4.567999649047851 and perplexity is 96.35118069819754
At time: 527.1257627010345 and batch: 300, loss is 4.4683921337127686 and perplexity is 87.2163779418347
At time: 527.6640779972076 and batch: 350, loss is 4.461553220748901 and perplexity is 86.62194766934142
At time: 528.2139551639557 and batch: 400, loss is 4.473591575622558 and perplexity is 87.67103539074834
At time: 528.7372677326202 and batch: 450, loss is 4.517405605316162 and perplexity is 91.59764899123572
At time: 529.2506895065308 and batch: 500, loss is 4.5513612651824955 and perplexity is 94.76131583625323
At time: 529.7880048751831 and batch: 550, loss is 4.471444578170776 and perplexity is 87.48300782075374
At time: 530.2957110404968 and batch: 600, loss is 4.413475036621094 and perplexity is 82.5558502166357
At time: 530.8052597045898 and batch: 650, loss is 4.401903991699219 and perplexity is 81.60609817375384
At time: 531.3470702171326 and batch: 700, loss is 4.491056795120239 and perplexity is 89.21567877143245
At time: 531.8896977901459 and batch: 750, loss is 4.422593336105347 and perplexity is 83.31206162464316
At time: 532.4158177375793 and batch: 800, loss is 4.516781024932861 and perplexity is 91.54045675895173
At time: 532.9497680664062 and batch: 850, loss is 4.4362193775177 and perplexity is 84.45504470925509
At time: 533.492972612381 and batch: 900, loss is 4.446583938598633 and perplexity is 85.33493614622532
At time: 534.0292129516602 and batch: 950, loss is 4.420695323944091 and perplexity is 83.15408428737943
At time: 534.5582211017609 and batch: 1000, loss is 4.341385107040406 and perplexity is 76.81386110760249
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.9292714188738564 and perplexity of 138.27872835813656
Finished 47 epochs...
Completing Train Step...
At time: 536.1857507228851 and batch: 50, loss is 4.561311950683594 and perplexity is 95.70896293759536
At time: 536.7354247570038 and batch: 100, loss is 4.485317220687866 and perplexity is 88.7050854385021
At time: 537.2505073547363 and batch: 150, loss is 4.510296144485474 and perplexity is 90.94874849307678
At time: 537.770699262619 and batch: 200, loss is 4.531256437301636 and perplexity is 92.87517964491605
At time: 538.3124358654022 and batch: 250, loss is 4.567854118347168 and perplexity is 96.33715966362912
At time: 538.8433856964111 and batch: 300, loss is 4.468129634857178 and perplexity is 87.19348674702384
At time: 539.3850665092468 and batch: 350, loss is 4.461309595108032 and perplexity is 86.60084691227425
At time: 539.9129757881165 and batch: 400, loss is 4.473376722335815 and perplexity is 87.65220100402986
At time: 540.4302449226379 and batch: 450, loss is 4.517221698760986 and perplexity is 91.5808051320431
At time: 540.948513507843 and batch: 500, loss is 4.551115274429321 and perplexity is 94.73800829563615
At time: 541.4953470230103 and batch: 550, loss is 4.471252365112305 and perplexity is 87.46619406022016
At time: 542.0352566242218 and batch: 600, loss is 4.413304328918457 and perplexity is 82.54175849992251
At time: 542.5573947429657 and batch: 650, loss is 4.401725950241089 and perplexity is 81.59157019837207
At time: 543.086686372757 and batch: 700, loss is 4.490962352752685 and perplexity is 89.20725342936724
At time: 543.607969045639 and batch: 750, loss is 4.422527704238892 and perplexity is 83.30659387797168
At time: 544.1374802589417 and batch: 800, loss is 4.516665077209472 and perplexity is 91.529843466698
At time: 544.6574459075928 and batch: 850, loss is 4.4361927318573 and perplexity is 84.45279437879562
At time: 545.177253484726 and batch: 900, loss is 4.446554822921753 and perplexity is 85.33245159796772
At time: 545.6937460899353 and batch: 950, loss is 4.420681886672973 and perplexity is 83.15296693091142
At time: 546.2052783966064 and batch: 1000, loss is 4.341252202987671 and perplexity is 76.80365291252537
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929260998237424 and perplexity of 138.27728741328974
Finished 48 epochs...
Completing Train Step...
At time: 547.7772376537323 and batch: 50, loss is 4.561054906845093 and perplexity is 95.68436469993107
At time: 548.3068306446075 and batch: 100, loss is 4.485145359039307 and perplexity is 88.68984174622358
At time: 548.8305296897888 and batch: 150, loss is 4.51019515991211 and perplexity is 90.939564536239
At time: 549.3497233390808 and batch: 200, loss is 4.531078653335571 and perplexity is 92.85866939480218
At time: 549.8665850162506 and batch: 250, loss is 4.56769642829895 and perplexity is 96.32196944998078
At time: 550.394184589386 and batch: 300, loss is 4.467915630340576 and perplexity is 87.17482894354004
At time: 550.9083697795868 and batch: 350, loss is 4.461083898544311 and perplexity is 86.58130360422254
At time: 551.4268860816956 and batch: 400, loss is 4.473209352493286 and perplexity is 87.63753189656782
At time: 551.9460990428925 and batch: 450, loss is 4.51709939956665 and perplexity is 91.56960555822216
At time: 552.4698548316956 and batch: 500, loss is 4.55092981338501 and perplexity is 94.7204397148757
At time: 552.994824886322 and batch: 550, loss is 4.471064291000366 and perplexity is 87.44974548027217
At time: 553.5214080810547 and batch: 600, loss is 4.413151025772095 and perplexity is 82.52910555853084
At time: 554.0416288375854 and batch: 650, loss is 4.40155104637146 and perplexity is 81.57730076494119
At time: 554.5550734996796 and batch: 700, loss is 4.490870380401612 and perplexity is 89.19904920582316
At time: 555.0751414299011 and batch: 750, loss is 4.42247353553772 and perplexity is 83.30208139020117
At time: 555.5992999076843 and batch: 800, loss is 4.516550331115723 and perplexity is 91.5193413772471
At time: 556.1194605827332 and batch: 850, loss is 4.436165342330932 and perplexity is 84.45048128843452
At time: 556.6417241096497 and batch: 900, loss is 4.446520147323608 and perplexity is 85.32949269546855
At time: 557.1754429340363 and batch: 950, loss is 4.420664854049683 and perplexity is 83.1515506298119
At time: 557.6947238445282 and batch: 1000, loss is 4.3411163711547855 and perplexity is 76.7932212400706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929259881740663 and perplexity of 138.27713302723242
Finished 49 epochs...
Completing Train Step...
At time: 559.3343207836151 and batch: 50, loss is 4.560818929672241 and perplexity is 95.66178803795624
At time: 559.8477208614349 and batch: 100, loss is 4.484979429244995 and perplexity is 88.67512667989281
At time: 560.3795111179352 and batch: 150, loss is 4.510095920562744 and perplexity is 90.93054020081482
At time: 560.902242898941 and batch: 200, loss is 4.530901012420654 and perplexity is 92.84217536086389
At time: 561.4217703342438 and batch: 250, loss is 4.567543983459473 and perplexity is 96.30728678198678
At time: 561.9308180809021 and batch: 300, loss is 4.467737407684326 and perplexity is 87.15929379836612
At time: 562.4478509426117 and batch: 350, loss is 4.46087080001831 and perplexity is 86.56285522177659
At time: 562.959874868393 and batch: 400, loss is 4.473085060119629 and perplexity is 87.62663989661722
At time: 563.4970552921295 and batch: 450, loss is 4.51696174621582 and perplexity is 91.55700156269391
At time: 564.0064952373505 and batch: 500, loss is 4.550757751464844 and perplexity is 94.7041433361727
At time: 564.5267131328583 and batch: 550, loss is 4.470883741378784 and perplexity is 87.43395788708311
At time: 565.0637397766113 and batch: 600, loss is 4.41301082611084 and perplexity is 82.51753581694376
At time: 565.5818147659302 and batch: 650, loss is 4.401378507614136 and perplexity is 81.56322673303424
At time: 566.1056587696075 and batch: 700, loss is 4.490782670974731 and perplexity is 89.19122595143062
At time: 566.6350848674774 and batch: 750, loss is 4.42242052078247 and perplexity is 83.29766526780533
At time: 567.1749820709229 and batch: 800, loss is 4.516443872451783 and perplexity is 91.50959886903578
At time: 567.7042217254639 and batch: 850, loss is 4.436143293380737 and perplexity is 84.44861926450652
At time: 568.2269423007965 and batch: 900, loss is 4.446503705978394 and perplexity is 85.32808977535517
At time: 568.7465450763702 and batch: 950, loss is 4.420654258728027 and perplexity is 83.15066961705412
At time: 569.2870457172394 and batch: 1000, loss is 4.340990343093872 and perplexity is 76.78354374913692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.929262486899772 and perplexity of 138.27749326163433
Annealing...
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd6d92888d0>
SETTINGS FOR THIS RUN
{'anneal': 2.0, 'data': 'ptb', 'dropout': 0.0, 'tune_wordvecs': True, 'lr': 0.0, 'num_layers': 1, 'batch_size': 20, 'wordvec_source': '', 'wordvec_dim': 200, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7956628799438477 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 1.3363230228424072 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 1.865607738494873 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 2.3950157165527344 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 2.9210071563720703 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 3.458669424057007 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 3.9744913578033447 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 4.497738361358643 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 5.010789394378662 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 5.531037092208862 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 6.061307668685913 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 6.572989225387573 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 7.08584189414978 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 7.611864805221558 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 8.132406949996948 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 8.662401676177979 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 9.189880847930908 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 9.714490175247192 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 10.237127780914307 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 10.771658658981323 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 1 epochs...
Completing Train Step...
At time: 12.411073207855225 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 12.934236288070679 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 13.45581603050232 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 13.987802505493164 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 14.505706310272217 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 15.034874439239502 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 15.561013460159302 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 16.072638273239136 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 16.611114501953125 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 17.13768768310547 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 17.655128479003906 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 18.180012226104736 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 18.701690435409546 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 19.23065948486328 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 19.768399000167847 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 20.278528451919556 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 20.784757375717163 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 21.304062604904175 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 21.824278116226196 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 22.34013605117798 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 2 epochs...
Completing Train Step...
At time: 23.967803239822388 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 24.52416682243347 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 25.049861669540405 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 25.586939811706543 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 26.112276792526245 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 26.650110483169556 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 27.165310859680176 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 27.681031703948975 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 28.214246034622192 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 28.7324321269989 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 29.252580165863037 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 29.772193908691406 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 30.31917381286621 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 30.826322317123413 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 31.374675035476685 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 31.8907949924469 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 32.40969705581665 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 32.93902063369751 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 33.445316314697266 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 33.955570220947266 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 3 epochs...
Completing Train Step...
At time: 35.56258988380432 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 36.10774040222168 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 36.62293791770935 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 37.14843821525574 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 37.6750168800354 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 38.20706129074097 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 38.73046255111694 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 39.245386362075806 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 39.74902629852295 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 40.26263952255249 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 40.77315139770508 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 41.2947781085968 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 41.82540249824524 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 42.37386894226074 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 42.88934111595154 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 43.43282222747803 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 43.958796977996826 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 44.46373391151428 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 45.00471019744873 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 45.52495074272156 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 4 epochs...
Completing Train Step...
At time: 47.09409189224243 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 47.63959789276123 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 48.185444355010986 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 48.70606851577759 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 49.230650186538696 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 49.737549781799316 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 50.25304341316223 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 50.78865075111389 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 51.301892042160034 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 51.81995105743408 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 52.33467698097229 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 52.854395389556885 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 53.37744331359863 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 53.88974165916443 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 54.41010403633118 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 54.931493282318115 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 55.45975923538208 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 55.97067642211914 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 56.48663592338562 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 57.041680574417114 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 5 epochs...
Completing Train Step...
At time: 58.58548045158386 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 59.1089870929718 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 59.618470668792725 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 60.138832330703735 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 60.66345453262329 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 61.17993640899658 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 61.682591915130615 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 62.2093665599823 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 62.71512293815613 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 63.24087953567505 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 63.7708535194397 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 64.28983783721924 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 64.81073117256165 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 65.32911276817322 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 65.8442451953888 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 66.3550271987915 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 66.86914443969727 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 67.37927007675171 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 67.88859105110168 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 68.39515519142151 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 6 epochs...
Completing Train Step...
At time: 69.9398365020752 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 70.46973371505737 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 71.00659394264221 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 71.52540969848633 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 72.04138112068176 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 72.55261850357056 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 73.0826096534729 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 73.59424471855164 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 74.11372637748718 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 74.63047170639038 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 75.15932822227478 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 75.69395542144775 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 76.21077799797058 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 76.72546291351318 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 77.23715782165527 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 77.74753069877625 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 78.26501727104187 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 78.77430987358093 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 79.30433440208435 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 79.81603145599365 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 7 epochs...
Completing Train Step...
At time: 81.38460612297058 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 81.91797161102295 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 82.4429624080658 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 82.97091174125671 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 83.54283738136292 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 84.06996631622314 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 84.58781790733337 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 85.10048651695251 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 85.61201572418213 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 86.14523267745972 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 86.67949056625366 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 87.18917942047119 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 87.71212410926819 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 88.22440981864929 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 88.7508795261383 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 89.2730278968811 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 89.7959361076355 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 90.30603623390198 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 90.82063031196594 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 91.34760856628418 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 8 epochs...
Completing Train Step...
At time: 92.95704746246338 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 93.48984980583191 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 94.00512361526489 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 94.51553583145142 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 95.02694058418274 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 95.55485892295837 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 96.07641768455505 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 96.61210012435913 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 97.14305377006531 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 97.6588225364685 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 98.16579914093018 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 98.67321300506592 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 99.1858594417572 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 99.71458125114441 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 100.21940517425537 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 100.72880482673645 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 101.24538445472717 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 101.76880931854248 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 102.29490613937378 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 102.8203604221344 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 9 epochs...
Completing Train Step...
At time: 104.34037232398987 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 104.87348675727844 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 105.41099500656128 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 105.92209243774414 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 106.44394278526306 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 106.96071791648865 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 107.47682166099548 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 107.98455333709717 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 108.4928047657013 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 109.01572751998901 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 109.55238151550293 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 110.09040141105652 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 110.61409258842468 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 111.14595603942871 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 111.67450523376465 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 112.17751336097717 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 112.69278955459595 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 113.19904851913452 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 113.7229995727539 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 114.23782634735107 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 10 epochs...
Completing Train Step...
At time: 115.7348906993866 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 116.24895334243774 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 116.7655234336853 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 117.29600548744202 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 117.80543732643127 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 118.32042050361633 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 118.85230278968811 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 119.36790299415588 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 119.88753843307495 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 120.42071437835693 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 120.94496655464172 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 121.46282720565796 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 121.97893691062927 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 122.48467135429382 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 123.03731632232666 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 123.5527114868164 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 124.05971384048462 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 124.57647180557251 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 125.09513545036316 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 125.6139783859253 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 11 epochs...
Completing Train Step...
At time: 127.18069362640381 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 127.7064299583435 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 128.2540967464447 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 128.781090259552 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 129.31149673461914 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 129.84553027153015 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 130.36385679244995 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 130.9003279209137 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 131.42293763160706 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 131.94837093353271 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 132.46373653411865 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 132.9869909286499 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 133.49590873718262 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 134.01034545898438 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 134.53667306900024 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 135.0547320842743 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 135.58138418197632 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 136.09713006019592 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 136.63864064216614 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 137.15770721435547 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 12 epochs...
Completing Train Step...
At time: 138.68741917610168 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 139.22352409362793 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 139.74088525772095 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 140.2603681087494 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 140.7778103351593 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 141.3135108947754 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 141.81785774230957 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 142.3337836265564 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 142.8457314968109 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 143.37003183364868 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 143.89196109771729 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 144.41050171852112 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 144.93790125846863 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 145.4537787437439 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 145.9803011417389 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 146.53311967849731 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 147.0549259185791 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 147.56266450881958 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 148.07741951942444 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 148.59818983078003 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 13 epochs...
Completing Train Step...
At time: 150.15361905097961 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 150.67885565757751 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 151.19783902168274 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 151.70873403549194 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 152.2153398990631 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 152.73270964622498 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 153.25350999832153 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 153.7711479663849 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 154.28840613365173 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 154.82271003723145 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 155.34339118003845 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 155.86316871643066 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 156.37217736244202 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 156.90375065803528 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 157.416987657547 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 157.95115280151367 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 158.48258805274963 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 158.99919891357422 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 159.52216148376465 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 160.0547263622284 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 14 epochs...
Completing Train Step...
At time: 161.66060304641724 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 162.20783352851868 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 162.74830865859985 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 163.27859783172607 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 163.79481029510498 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 164.31270384788513 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 164.83372497558594 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 165.34927797317505 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 165.8700909614563 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 166.3963553905487 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 166.92679572105408 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 167.46480798721313 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 167.98091983795166 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 168.4959533214569 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 169.01674675941467 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 169.55586862564087 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 170.06652641296387 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 170.59245681762695 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 171.11868906021118 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 171.64160418510437 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 15 epochs...
Completing Train Step...
At time: 173.22571063041687 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 173.7643747329712 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 174.28009009361267 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 174.81737732887268 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 175.3345079421997 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 175.83736968040466 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 176.34901237487793 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 176.8888123035431 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 177.40745210647583 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 177.9153814315796 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 178.42958426475525 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 178.95852661132812 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 179.4806935787201 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 179.99672436714172 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 180.51503038406372 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 181.03513288497925 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 181.54728937149048 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 182.05837678909302 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 182.5723741054535 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 183.1092188358307 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 16 epochs...
Completing Train Step...
At time: 184.69941878318787 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 185.21039628982544 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 185.73455619812012 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 186.26685190200806 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 186.79453897476196 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 187.30230927467346 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 187.83214020729065 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 188.3456370830536 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 188.86082005500793 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 189.3685896396637 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 189.9192500114441 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 190.43543028831482 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 190.9436571598053 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 191.45672225952148 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 191.9806685447693 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 192.51021456718445 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 193.02647924423218 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 193.536869764328 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 194.04378080368042 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 194.55961322784424 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 17 epochs...
Completing Train Step...
At time: 196.08225631713867 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 196.61151766777039 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 197.1257519721985 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 197.63972210884094 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 198.16751098632812 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 198.69692397117615 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 199.2285099029541 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 199.74366164207458 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 200.25867676734924 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 200.77752494812012 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 201.3017418384552 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 201.82336616516113 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 202.34907341003418 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 202.85945224761963 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 203.4119918346405 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 203.93570494651794 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 204.45685601234436 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 204.96518993377686 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 205.47885370254517 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 205.98885488510132 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 18 epochs...
Completing Train Step...
At time: 207.4755802154541 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 208.0109248161316 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 208.52679347991943 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 209.06683087348938 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 209.58338856697083 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 210.10576629638672 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 210.63587427139282 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 211.15275025367737 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 211.67350506782532 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 212.1906852722168 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 212.71560835838318 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 213.23327040672302 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 213.75213527679443 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 214.27027320861816 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 214.7875416278839 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 215.3452169895172 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 215.85273122787476 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 216.37866353988647 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 216.89832997322083 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 217.427588224411 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 19 epochs...
Completing Train Step...
At time: 218.96324706077576 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 219.4927797317505 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 220.0093376636505 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 220.52959728240967 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 221.04009699821472 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 221.55363059043884 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 222.0762972831726 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 222.58237051963806 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 223.09767723083496 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 223.60365414619446 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 224.12554717063904 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 224.6591351032257 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 225.17627835273743 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 225.69486951828003 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 226.21924352645874 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 226.73200631141663 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 227.25176453590393 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 227.77347254753113 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 228.2976369857788 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 228.8129370212555 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 20 epochs...
Completing Train Step...
At time: 230.37064170837402 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 230.90952491760254 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 231.44125413894653 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 231.95084524154663 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 232.467182636261 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 232.99814629554749 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 233.53386211395264 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 234.0655016899109 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 234.59278297424316 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 235.09979486465454 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 235.62217164039612 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 236.14598536491394 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 236.66970562934875 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 237.18859887123108 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 237.70405912399292 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 238.22156262397766 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 238.7550449371338 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 239.2791907787323 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 239.8068287372589 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 240.32814764976501 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 21 epochs...
Completing Train Step...
At time: 241.91082501411438 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 242.43859148025513 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 242.95877170562744 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 243.48868012428284 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 244.0078251361847 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 244.53584003448486 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 245.04578948020935 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 245.55884504318237 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 246.06428170204163 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 246.56595611572266 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 247.09905433654785 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 247.62498688697815 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 248.15934872627258 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 248.6766481399536 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 249.20348978042603 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 249.71570229530334 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 250.22701978683472 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 250.7375967502594 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 251.25800561904907 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 251.79018902778625 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 22 epochs...
Completing Train Step...
At time: 253.33961510658264 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 253.8615791797638 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 254.36956429481506 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 254.8987693786621 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 255.45025753974915 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 255.97260403633118 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 256.49492478370667 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 257.0229187011719 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 257.53943610191345 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 258.0565857887268 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 258.57110381126404 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 259.0930423736572 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 259.63531708717346 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 260.156706571579 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 260.6661002635956 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 261.2030494213104 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 261.7309617996216 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 262.25293350219727 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 262.75646448135376 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 263.29167437553406 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 23 epochs...
Completing Train Step...
At time: 264.82207584381104 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 265.3467011451721 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 265.84568309783936 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 266.35605931282043 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 266.86821699142456 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 267.3751468658447 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 267.8894393444061 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 268.4023690223694 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 268.8967800140381 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 269.4204180240631 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 269.9574806690216 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 270.46316862106323 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 270.9628863334656 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 271.4937026500702 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 272.0148503780365 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 272.534307718277 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 273.04567408561707 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 273.57029390335083 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 274.085236787796 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 274.594256401062 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 24 epochs...
Completing Train Step...
At time: 276.1215035915375 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 276.6484320163727 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 277.17971754074097 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 277.70893359184265 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 278.23107647895813 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 278.75072383880615 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 279.26561641693115 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 279.78671407699585 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 280.3006706237793 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 280.80934166908264 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 281.32010865211487 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 281.84286069869995 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 282.3613209724426 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 282.8771605491638 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 283.38761281967163 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 283.9048204421997 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 284.4262623786926 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 284.93194484710693 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 285.43964076042175 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 285.94835782051086 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 25 epochs...
Completing Train Step...
At time: 287.5479700565338 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 288.0774772167206 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 288.5897583961487 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 289.0919780731201 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 289.610312461853 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 290.1176829338074 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 290.639785528183 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 291.1595628261566 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 291.69018626213074 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 292.2146725654602 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 292.7475094795227 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 293.28388357162476 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 293.795286655426 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 294.31620836257935 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 294.845290184021 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 295.3738634586334 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 295.90180802345276 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 296.456668138504 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 296.981153011322 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 297.4919364452362 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 26 epochs...
Completing Train Step...
At time: 299.00664043426514 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 299.52556800842285 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 300.03666520118713 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 300.54660058021545 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 301.0711200237274 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 301.58188700675964 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 302.10227847099304 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 302.6138081550598 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 303.1271712779999 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 303.6442573070526 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 304.15197014808655 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 304.6582794189453 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 305.1948232650757 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 305.7412049770355 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 306.26628613471985 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 306.7856237888336 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 307.3159432411194 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 307.82694935798645 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 308.33876633644104 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 308.8656783103943 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 27 epochs...
Completing Train Step...
At time: 310.543377161026 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 311.11862325668335 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 311.6404519081116 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 312.1721737384796 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 312.71380257606506 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 313.22970747947693 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 313.7487790584564 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 314.2683129310608 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 314.78045439720154 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 315.31410908699036 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 315.8597688674927 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 316.38916873931885 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 316.91606736183167 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 317.4393939971924 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 317.94982266426086 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 318.4712417125702 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 318.99663186073303 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 319.51750898361206 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 320.06241059303284 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 320.5966966152191 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 28 epochs...
Completing Train Step...
At time: 322.178026676178 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 322.7061653137207 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 323.23901629447937 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 323.77107286453247 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 324.2937607765198 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 324.81988739967346 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 325.34180641174316 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 325.87961077690125 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 326.3998477458954 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 326.91979694366455 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 327.4366149902344 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 327.9815900325775 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 328.49958753585815 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 329.01415729522705 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 329.52633833885193 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 330.0360426902771 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 330.5432515144348 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 331.06086826324463 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 331.5951693058014 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 332.1322932243347 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 29 epochs...
Completing Train Step...
At time: 333.7356023788452 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 334.27333879470825 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 334.79046416282654 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 335.30981373786926 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 335.82838892936707 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 336.3558177947998 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 336.87476897239685 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 337.408490896225 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 337.9274024963379 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 338.442396402359 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 338.94309067726135 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 339.47235918045044 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 339.99779438972473 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 340.5346918106079 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 341.0415949821472 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 341.56701254844666 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 342.0702679157257 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 342.57105898857117 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 343.0861201286316 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 343.60930848121643 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 30 epochs...
Completing Train Step...
At time: 345.14657855033875 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 345.694632768631 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 346.22214698791504 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 346.74539494514465 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 347.25515484809875 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 347.7591669559479 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 348.2713270187378 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 348.7993550300598 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 349.3343801498413 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 349.8731870651245 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 350.4030022621155 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 350.936158657074 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 351.4551281929016 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 352.0070996284485 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 352.5323805809021 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 353.0610134601593 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 353.58033514022827 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 354.10173749923706 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 354.6448769569397 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 355.1949369907379 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 31 epochs...
Completing Train Step...
At time: 356.7606153488159 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 357.284875869751 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 357.79863262176514 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 358.30409383773804 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 358.8478252887726 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 359.36232805252075 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 359.89461374282837 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 360.44643568992615 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 360.9830288887024 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 361.52407121658325 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 362.0381019115448 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 362.56302762031555 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 363.128342628479 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 363.6528332233429 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 364.16916370391846 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 364.6868498325348 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 365.22528290748596 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 365.75005531311035 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 366.2676131725311 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 366.7799460887909 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 32 epochs...
Completing Train Step...
At time: 368.3983132839203 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 368.9348666667938 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 369.4718773365021 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 370.0064253807068 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 370.5319788455963 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 371.06253147125244 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 371.5781784057617 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 372.0931878089905 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 372.6213984489441 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 373.1507182121277 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 373.6719055175781 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 374.19843006134033 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 374.7118709087372 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 375.225145816803 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 375.75536704063416 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 376.29474449157715 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 376.84735465049744 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 377.38665866851807 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 377.90986800193787 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 378.43251943588257 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 33 epochs...
Completing Train Step...
At time: 380.05698561668396 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 380.6261668205261 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 381.1602075099945 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 381.6879119873047 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 382.2092101573944 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 382.7465705871582 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 383.2897641658783 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 383.81019377708435 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 384.32934522628784 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 384.8459734916687 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 385.38290762901306 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 385.9004695415497 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 386.42125487327576 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 386.9436388015747 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 387.45801162719727 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 387.98456263542175 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 388.5002932548523 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 389.02059602737427 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 389.54588413238525 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 390.08996391296387 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 34 epochs...
Completing Train Step...
At time: 391.6292221546173 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 392.14900302886963 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 392.68600630760193 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 393.19993329048157 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 393.71830892562866 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 394.231627702713 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 394.7713243961334 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 395.2826075553894 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 395.8003168106079 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 396.32880544662476 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 396.8494942188263 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 397.3639633655548 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 397.87566924095154 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 398.3913164138794 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 398.9113700389862 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 399.42795634269714 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 399.9473650455475 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 400.4727621078491 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 400.9867420196533 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 401.5297272205353 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 35 epochs...
Completing Train Step...
At time: 403.16466546058655 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 403.69685077667236 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 404.221581697464 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 404.74020075798035 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 405.2567045688629 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 405.79383420944214 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 406.3103220462799 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 406.84001064300537 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 407.361656665802 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 407.87634682655334 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 408.3986117839813 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 408.90968227386475 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 409.4466915130615 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 409.96979451179504 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 410.4821240901947 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 410.99932861328125 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 411.52408146858215 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 412.04090690612793 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 412.5922019481659 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 413.1052544116974 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 36 epochs...
Completing Train Step...
At time: 414.68494176864624 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 415.2518539428711 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 415.7651832103729 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 416.27873396873474 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 416.8061943054199 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 417.3357882499695 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 417.8646171092987 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 418.3769886493683 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 418.9087555408478 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 419.452050447464 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 419.97535157203674 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 420.4982614517212 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 421.01164507865906 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 421.5413691997528 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 422.07765460014343 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 422.6212842464447 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 423.14577770233154 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 423.66057085990906 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 424.19251680374146 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 424.73081827163696 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 37 epochs...
Completing Train Step...
At time: 426.3381266593933 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 426.85942482948303 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 427.3838481903076 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 427.9061381816864 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 428.4223918914795 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 428.9461557865143 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 429.4607741832733 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 429.96614480018616 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 430.49828577041626 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 431.0362038612366 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 431.56608295440674 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 432.0751476287842 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 432.5928621292114 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 433.11363220214844 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 433.6604549884796 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 434.19261050224304 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 434.7403917312622 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 435.2720847129822 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 435.8101923465729 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 436.33974266052246 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 38 epochs...
Completing Train Step...
At time: 437.89740777015686 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 438.4368166923523 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 438.95568227767944 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 439.46755838394165 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 439.98868584632874 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 440.5149042606354 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 441.03832936286926 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 441.57142424583435 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 442.09859919548035 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 442.61209201812744 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 443.12930846214294 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 443.6618101596832 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 444.21985626220703 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 444.7377710342407 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 445.2524266242981 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 445.7734076976776 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 446.3107039928436 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 446.8514177799225 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 447.36596488952637 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 447.90396761894226 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 39 epochs...
Completing Train Step...
At time: 449.49350595474243 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 450.03260231018066 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 450.571368932724 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 451.0939931869507 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 451.62709307670593 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 452.1494653224945 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 452.67929100990295 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 453.22456073760986 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 453.75388979911804 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 454.27017641067505 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 454.7928857803345 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 455.3160376548767 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 455.8397927284241 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 456.3507785797119 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 456.860769033432 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 457.4018576145172 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 457.91723704338074 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 458.4527451992035 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 458.98551201820374 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 459.5028281211853 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 40 epochs...
Completing Train Step...
At time: 461.0379340648651 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 461.5560836791992 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 462.06946206092834 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 462.57030296325684 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 463.1015748977661 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 463.60742020606995 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 464.1253571510315 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 464.64537525177 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 465.17046666145325 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 465.73949122428894 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 466.2510929107666 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 466.7566092014313 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 467.2910125255585 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 467.79629135131836 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 468.30113101005554 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 468.81139850616455 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 469.32338404655457 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 469.83740878105164 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 470.3641426563263 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 470.9099636077881 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 41 epochs...
Completing Train Step...
At time: 472.46179580688477 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 473.00361824035645 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 473.51941752433777 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 474.0385138988495 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 474.5473110675812 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 475.0480282306671 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 475.558278799057 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 476.07438111305237 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 476.59875535964966 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 477.12157559394836 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 477.6599431037903 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 478.2032175064087 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 478.723335981369 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 479.2417109012604 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 479.76440238952637 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 480.28341794013977 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 480.8277053833008 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 481.3456883430481 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 481.8949251174927 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 482.4331839084625 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 42 epochs...
Completing Train Step...
At time: 483.98237133026123 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 484.5364713668823 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 485.0809805393219 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 485.59939074516296 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 486.16597080230713 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 486.7163302898407 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 487.2475802898407 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 487.7875690460205 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 488.3248496055603 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 488.8422830104828 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 489.39571833610535 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 489.9262239933014 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 490.44151997566223 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 490.9818739891052 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 491.4952585697174 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 492.030650138855 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 492.57400369644165 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 493.08914518356323 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 493.6195261478424 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 494.12919664382935 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 43 epochs...
Completing Train Step...
At time: 495.65939927101135 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 496.189954996109 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 496.7331118583679 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 497.25455474853516 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 497.83293199539185 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 498.35658621788025 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 498.876571893692 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 499.3916597366333 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 499.917222738266 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 500.43167901039124 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 500.9712710380554 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 501.49741673469543 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 502.0107464790344 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 502.515878200531 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 503.0473551750183 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 503.56292176246643 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 504.08020997047424 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 504.6108660697937 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 505.1494719982147 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 505.66143822669983 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 44 epochs...
Completing Train Step...
At time: 507.19642090797424 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 507.73820877075195 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 508.25798654556274 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 508.77981662750244 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 509.32800364494324 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 509.84768414497375 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 510.3766996860504 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 510.9160509109497 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 511.4661979675293 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 511.9966037273407 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 512.5285959243774 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 513.0667726993561 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 513.5823104381561 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 514.0918118953705 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 514.625296831131 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 515.1534523963928 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 515.6640241146088 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 516.1920139789581 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 516.7296297550201 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 517.2569944858551 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 45 epochs...
Completing Train Step...
At time: 518.8497579097748 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 519.3669166564941 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 519.8944923877716 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 520.414892911911 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 520.9308965206146 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 521.4498212337494 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 521.9970524311066 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 522.5264065265656 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 523.0482971668243 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 523.5718295574188 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 524.1011357307434 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 524.6359159946442 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 525.1494402885437 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 525.6544282436371 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 526.1686177253723 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 526.6891715526581 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 527.2258703708649 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 527.7466559410095 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 528.2583355903625 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 528.7754113674164 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 46 epochs...
Completing Train Step...
At time: 530.3260412216187 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 530.8609082698822 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 531.3765077590942 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 531.9016945362091 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 532.4132823944092 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 532.956609249115 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 533.4889464378357 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 534.0016491413116 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 534.5174436569214 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 535.0460636615753 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 535.5644149780273 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 536.0791015625 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 536.6139614582062 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 537.1200470924377 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 537.6531989574432 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 538.179083108902 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 538.6974024772644 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 539.2389147281647 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 539.7722737789154 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 540.3017616271973 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 47 epochs...
Completing Train Step...
At time: 541.8722002506256 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 542.4220385551453 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 542.9337823390961 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 543.4563856124878 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 543.9645001888275 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 544.4927921295166 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 545.0244240760803 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 545.5568380355835 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 546.0780501365662 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 546.6163356304169 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 547.1350281238556 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 547.6526327133179 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 548.182288646698 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 548.6964030265808 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 549.2449975013733 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 549.7811589241028 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 550.3053634166718 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 550.8156251907349 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 551.3570654392242 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 551.8836364746094 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 48 epochs...
Completing Train Step...
At time: 553.3941466808319 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 553.9449853897095 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 554.456130027771 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 554.961742401123 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 555.4736685752869 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 555.996887922287 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 556.5351462364197 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 557.0549125671387 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 557.5735363960266 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 558.0936088562012 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 558.5927259922028 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 559.1458566188812 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 559.6951200962067 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 560.2327837944031 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 560.755259513855 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 561.2766411304474 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 561.823778629303 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 562.3789086341858 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 562.9161307811737 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 563.4422376155853 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished 49 epochs...
Completing Train Step...
At time: 564.9998037815094 and batch: 50, loss is 9.156083946228028 and perplexity is 9471.891916612038
At time: 565.515527009964 and batch: 100, loss is 9.159246234893798 and perplexity is 9501.892182813837
At time: 566.0662405490875 and batch: 150, loss is 9.154249305725097 and perplexity is 9454.530331066242
At time: 566.613201379776 and batch: 200, loss is 9.153185482025146 and perplexity is 9444.477725676137
At time: 567.1471335887909 and batch: 250, loss is 9.156516838073731 and perplexity is 9475.993109008448
At time: 567.654079914093 and batch: 300, loss is 9.154121170043945 and perplexity is 9453.318945994783
At time: 568.1827714443207 and batch: 350, loss is 9.154473533630371 and perplexity is 9456.65053829366
At time: 568.7060041427612 and batch: 400, loss is 9.154541492462158 and perplexity is 9457.293223054665
At time: 569.2442049980164 and batch: 450, loss is 9.156114959716797 and perplexity is 9472.185677580868
At time: 569.7889132499695 and batch: 500, loss is 9.154795818328857 and perplexity is 9459.698763232822
At time: 570.3113627433777 and batch: 550, loss is 9.155502815246582 and perplexity is 9466.389105848157
At time: 570.8519430160522 and batch: 600, loss is 9.154802570343017 and perplexity is 9459.762635468456
At time: 571.3635151386261 and batch: 650, loss is 9.153084144592285 and perplexity is 9443.520695041037
At time: 571.910676240921 and batch: 700, loss is 9.15599681854248 and perplexity is 9471.06668854221
At time: 572.430675983429 and batch: 750, loss is 9.153008346557618 and perplexity is 9442.804921859462
At time: 572.9581165313721 and batch: 800, loss is 9.154977779388428 and perplexity is 9461.420216656998
At time: 573.5002398490906 and batch: 850, loss is 9.155659408569337 and perplexity is 9467.871595243672
At time: 574.028605222702 and batch: 900, loss is 9.155709590911865 and perplexity is 9468.346727140597
At time: 574.5695352554321 and batch: 950, loss is 9.155002784729003 and perplexity is 9461.65680564982
At time: 575.096111536026 and batch: 1000, loss is 9.1566477394104 and perplexity is 9477.233610362562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 9.267456799018674 and perplexity of 10587.79073735975
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd6d92888d0>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'anneal': 2.7983104687033795, 'data': 'ptb', 'dropout': 0.8105890619547426, 'tune_wordvecs': True, 'lr': 10.25025121514281, 'num_layers': 1, 'batch_size': 20, 'wordvec_source': '', 'wordvec_dim': 200, 'seq_len': 50}, 'best_accuracy': -103.65104451499782}, {'params': {'anneal': 5.2786219679906905, 'data': 'ptb', 'dropout': 0.902475331033966, 'tune_wordvecs': True, 'lr': 13.317757938907771, 'num_layers': 1, 'batch_size': 20, 'wordvec_source': '', 'wordvec_dim': 200, 'seq_len': 50}, 'best_accuracy': -116.75497403104936}, {'params': {'anneal': 3.8677734013888427, 'data': 'ptb', 'dropout': 0.6408287226766163, 'tune_wordvecs': True, 'lr': 5.739873896080777, 'num_layers': 1, 'batch_size': 20, 'wordvec_source': '', 'wordvec_dim': 200, 'seq_len': 50}, 'best_accuracy': -85.21229831317284}, {'params': {'anneal': 4.891990058382956, 'data': 'ptb', 'dropout': 0.788981353500536, 'tune_wordvecs': True, 'lr': 20.80459897558753, 'num_layers': 1, 'batch_size': 20, 'wordvec_source': '', 'wordvec_dim': 200, 'seq_len': 50}, 'best_accuracy': -152.33586039949216}, {'params': {'anneal': 4.820525634626296, 'data': 'ptb', 'dropout': 0.8464481937026472, 'tune_wordvecs': True, 'lr': 18.548676794132753, 'num_layers': 1, 'batch_size': 20, 'wordvec_source': '', 'wordvec_dim': 200, 'seq_len': 50}, 'best_accuracy': -138.27713302723242}, {'params': {'anneal': 2.0, 'data': 'ptb', 'dropout': 0.0, 'tune_wordvecs': True, 'lr': 0.0, 'num_layers': 1, 'batch_size': 20, 'wordvec_source': '', 'wordvec_dim': 200, 'seq_len': 50}, 'best_accuracy': -10587.79073735975}]
