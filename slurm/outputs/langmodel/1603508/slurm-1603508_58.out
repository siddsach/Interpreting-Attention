Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'name': 'lr', 'type': 'continuous', 'domain': [0, 30]}, {'name': 'dropout', 'type': 'continuous', 'domain': [0, 1]}, {'name': 'anneal', 'type': 'continuous', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'seq_len': 50, 'lr': 16.112077727517786, 'dropout': 0.9874767116379771, 'wordvec_source': 'glove', 'num_layers': 1, 'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 6.354709920012811, 'batch_size': 50, 'data': 'wikitext'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.4150772094726562 and batch: 50, loss is 9.907650680541993 and perplexity is 20083.436117228473
At time: 3.9582879543304443 and batch: 100, loss is 8.4080299949646 and perplexity is 4482.920431083613
At time: 5.521662473678589 and batch: 150, loss is 7.875568151473999 and perplexity is 2632.1812432038223
At time: 7.099056005477905 and batch: 200, loss is 7.70083233833313 and perplexity is 2210.186849743526
At time: 8.672033071517944 and batch: 250, loss is 7.5958890628814695 and perplexity is 1989.9982988749605
At time: 10.235075950622559 and batch: 300, loss is 7.464128208160401 and perplexity is 1744.33418885282
At time: 11.799709558486938 and batch: 350, loss is 7.405032606124878 and perplexity is 1644.2384474470484
At time: 13.373058319091797 and batch: 400, loss is 7.3839272689819335 and perplexity is 1609.8998788287638
At time: 14.948009729385376 and batch: 450, loss is 7.3459402942657475 and perplexity is 1549.8916345345706
At time: 16.51409411430359 and batch: 500, loss is 7.3020831489562985 and perplexity is 1483.3868269529983
At time: 18.079053163528442 and batch: 550, loss is 7.2369911575317385 and perplexity is 1389.9056658569305
At time: 19.643134593963623 and batch: 600, loss is 7.230700950622559 and perplexity is 1381.1903110633875
At time: 21.20843529701233 and batch: 650, loss is 7.2345945835113525 and perplexity is 1386.578642368794
At time: 22.775529861450195 and batch: 700, loss is 7.194218730926513 and perplexity is 1331.709495612056
At time: 24.34105396270752 and batch: 750, loss is 7.158422031402588 and perplexity is 1284.8818292180752
At time: 25.905892372131348 and batch: 800, loss is 7.163093185424804 and perplexity is 1290.8977498498434
At time: 27.48167371749878 and batch: 850, loss is 7.15625937461853 and perplexity is 1282.1060733988513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.62239138285319 and perplexity of 751.7406490282955
Finished 1 epochs...
Completing Train Step...
At time: 31.603023290634155 and batch: 50, loss is 6.689306545257568 and perplexity is 803.7646844101273
At time: 33.17601752281189 and batch: 100, loss is 6.425124406814575 and perplexity is 617.1575913771843
At time: 34.762747049331665 and batch: 150, loss is 6.2141181755065915 and perplexity is 499.7550985385174
At time: 36.33865761756897 and batch: 200, loss is 6.176894092559815 and perplexity is 481.49415526184083
At time: 37.915443420410156 and batch: 250, loss is 6.22008807182312 and perplexity is 502.7475079600743
At time: 39.507914304733276 and batch: 300, loss is 6.132838544845581 and perplexity is 460.7421434942034
At time: 41.09881782531738 and batch: 350, loss is 6.092908296585083 and perplexity is 442.70706415252215
At time: 42.680901765823364 and batch: 400, loss is 6.0672509765625 and perplexity is 431.49286565481555
At time: 44.25398588180542 and batch: 450, loss is 6.056144771575927 and perplexity is 426.7271310381932
At time: 45.87446570396423 and batch: 500, loss is 6.071931009292602 and perplexity is 433.5169992000078
At time: 47.44788575172424 and batch: 550, loss is 6.038490352630615 and perplexity is 419.2596227059485
At time: 49.01887083053589 and batch: 600, loss is 6.053133897781372 and perplexity is 425.4442417799034
At time: 50.60755467414856 and batch: 650, loss is 6.037925443649292 and perplexity is 419.02284606449825
At time: 52.19029521942139 and batch: 700, loss is 5.989623775482178 and perplexity is 399.2643686220824
At time: 53.77128458023071 and batch: 750, loss is 5.997430047988892 and perplexity is 402.39333196668036
At time: 55.345853328704834 and batch: 800, loss is 6.0008603286743165 and perplexity is 403.7760241966701
At time: 56.926093339920044 and batch: 850, loss is 5.976854696273803 and perplexity is 394.1985420757494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.667434056599935 and perplexity of 289.29127617947836
Finished 2 epochs...
Completing Train Step...
At time: 61.079700231552124 and batch: 50, loss is 6.003226356506348 and perplexity is 404.7325005865026
At time: 62.66835403442383 and batch: 100, loss is 5.929777278900146 and perplexity is 376.0707456065263
At time: 64.25146293640137 and batch: 150, loss is 5.935913715362549 and perplexity is 378.3855749800938
At time: 65.83790111541748 and batch: 200, loss is 5.924008302688598 and perplexity is 373.9074484259865
At time: 67.41742086410522 and batch: 250, loss is 5.985280179977417 and perplexity is 397.5338866828343
At time: 68.9951844215393 and batch: 300, loss is 5.945509147644043 and perplexity is 382.03382340963674
At time: 70.57325434684753 and batch: 350, loss is 5.906943626403809 and perplexity is 367.5809720175162
At time: 72.15249681472778 and batch: 400, loss is 5.872554607391358 and perplexity is 355.15510423384717
At time: 73.7310860157013 and batch: 450, loss is 5.892834758758545 and perplexity is 362.4312346412677
At time: 75.3128981590271 and batch: 500, loss is 5.927298336029053 and perplexity is 375.1396422654878
At time: 76.89149618148804 and batch: 550, loss is 5.901240396499634 and perplexity is 365.4905399991577
At time: 78.51815605163574 and batch: 600, loss is 5.914635496139526 and perplexity is 370.41925884398455
At time: 80.10108208656311 and batch: 650, loss is 5.915946407318115 and perplexity is 370.90516401083994
At time: 81.68188667297363 and batch: 700, loss is 5.886105604171753 and perplexity is 360.00056618022035
At time: 83.26519274711609 and batch: 750, loss is 5.916826000213623 and perplexity is 371.23155308172164
At time: 84.85487461090088 and batch: 800, loss is 5.8963962745666505 and perplexity is 363.72434055451447
At time: 86.43097949028015 and batch: 850, loss is 5.890321931838989 and perplexity is 361.5216509701558
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.596391042073567 and perplexity of 269.45220887771876
Finished 3 epochs...
Completing Train Step...
At time: 90.54138350486755 and batch: 50, loss is 5.890872650146484 and perplexity is 361.72080239503526
At time: 92.1463851928711 and batch: 100, loss is 5.831002655029297 and perplexity is 340.7001125544802
At time: 93.72075390815735 and batch: 150, loss is 5.85281681060791 and perplexity is 348.213852811498
At time: 95.30053424835205 and batch: 200, loss is 5.856550569534302 and perplexity is 349.51642963338065
At time: 96.88304448127747 and batch: 250, loss is 5.900374174118042 and perplexity is 365.1740809948743
At time: 98.46642327308655 and batch: 300, loss is 5.841673135757446 and perplexity is 344.3550116260455
At time: 100.04211664199829 and batch: 350, loss is 5.8095620822906495 and perplexity is 333.473059935978
At time: 101.62145781517029 and batch: 400, loss is 5.795483255386353 and perplexity is 328.81104530230004
At time: 103.19743347167969 and batch: 450, loss is 5.825961685180664 and perplexity is 338.98697512449615
At time: 104.77297759056091 and batch: 500, loss is 5.8574455451965335 and perplexity is 349.8293783513068
At time: 106.35365176200867 and batch: 550, loss is 5.826102828979492 and perplexity is 339.03482441065404
At time: 107.94652462005615 and batch: 600, loss is 5.84965178489685 and perplexity is 347.11348927241255
At time: 109.52991580963135 and batch: 650, loss is 5.858153562545777 and perplexity is 350.0771513238857
At time: 111.1039035320282 and batch: 700, loss is 5.823875856399536 and perplexity is 338.2806432351315
At time: 112.68010473251343 and batch: 750, loss is 5.856712579727173 and perplexity is 349.5730594447366
At time: 114.25476288795471 and batch: 800, loss is 5.853729276657105 and perplexity is 348.53173113458115
At time: 115.82933187484741 and batch: 850, loss is 5.819655895233154 and perplexity is 336.856119887944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.56126340230306 and perplexity of 260.1513045428101
Finished 4 epochs...
Completing Train Step...
At time: 119.95795822143555 and batch: 50, loss is 5.8377453804016115 and perplexity is 343.0051221380803
At time: 121.56712579727173 and batch: 100, loss is 5.7674002361297605 and perplexity is 319.70549251363025
At time: 123.14902138710022 and batch: 150, loss is 5.787508487701416 and perplexity is 326.1992815500219
At time: 124.72393655776978 and batch: 200, loss is 5.793296203613282 and perplexity is 328.0927043334414
At time: 126.29458498954773 and batch: 250, loss is 5.8426494121551515 and perplexity is 344.6913614548121
At time: 127.86826014518738 and batch: 300, loss is 5.824013395309448 and perplexity is 338.32717318581234
At time: 129.440491437912 and batch: 350, loss is 5.781922521591187 and perplexity is 324.3822231562127
At time: 131.01693773269653 and batch: 400, loss is 5.760868673324585 and perplexity is 317.6241207149291
At time: 132.59191966056824 and batch: 450, loss is 5.7860714626312255 and perplexity is 325.7308616507346
At time: 134.17665433883667 and batch: 500, loss is 5.857048273086548 and perplexity is 349.6904284983123
At time: 135.7574017047882 and batch: 550, loss is 5.808731060028077 and perplexity is 333.19605151522694
At time: 137.33392977714539 and batch: 600, loss is 5.818874378204345 and perplexity is 336.5929639378151
At time: 138.91716146469116 and batch: 650, loss is 5.813187932968139 and perplexity is 334.6843781586918
At time: 140.4930145740509 and batch: 700, loss is 5.787173957824707 and perplexity is 326.09017639506686
At time: 142.06616640090942 and batch: 750, loss is 5.810596933364868 and perplexity is 333.8183335128285
At time: 143.64853358268738 and batch: 800, loss is 5.824739351272583 and perplexity is 338.5728729875046
At time: 145.23060655593872 and batch: 850, loss is 5.797090263366699 and perplexity is 329.33987207674335
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.559085210164388 and perplexity of 259.5852617152085
Finished 5 epochs...
Completing Train Step...
At time: 149.38689017295837 and batch: 50, loss is 5.800646524429322 and perplexity is 330.51317569059404
At time: 150.96785473823547 and batch: 100, loss is 5.751124448776245 and perplexity is 314.5441502926174
At time: 152.55440425872803 and batch: 150, loss is 5.760025568008423 and perplexity is 317.3564429863272
At time: 154.13427066802979 and batch: 200, loss is 5.758390474319458 and perplexity is 316.83795946932617
At time: 155.7095286846161 and batch: 250, loss is 5.801126985549927 and perplexity is 330.67201257577955
At time: 157.29564094543457 and batch: 300, loss is 5.761252355575562 and perplexity is 317.74601083457196
At time: 158.91181802749634 and batch: 350, loss is 5.732108888626098 and perplexity is 308.6194265793384
At time: 160.49482560157776 and batch: 400, loss is 5.7092509269714355 and perplexity is 301.6450294777912
At time: 162.07497787475586 and batch: 450, loss is 5.750759840011597 and perplexity is 314.42948564369004
At time: 163.65361332893372 and batch: 500, loss is 5.794247465133667 and perplexity is 328.4049547905691
At time: 165.23054885864258 and batch: 550, loss is 5.767773933410645 and perplexity is 319.8249879130287
At time: 166.81676030158997 and batch: 600, loss is 5.786196737289429 and perplexity is 325.77167002916866
At time: 168.39941692352295 and batch: 650, loss is 5.788644132614135 and perplexity is 326.5699385321689
At time: 169.98409056663513 and batch: 700, loss is 5.759495220184326 and perplexity is 317.1881783106725
At time: 171.56683802604675 and batch: 750, loss is 5.788032903671264 and perplexity is 326.37039052483783
At time: 173.14236521720886 and batch: 800, loss is 5.776360540390015 and perplexity is 322.5830235245672
At time: 174.71874332427979 and batch: 850, loss is 5.749342250823974 and perplexity is 313.98406958765526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.534936269124349 and perplexity of 253.3916382514988
Finished 6 epochs...
Completing Train Step...
At time: 178.84237384796143 and batch: 50, loss is 5.774771890640259 and perplexity is 322.07095893830376
At time: 180.43233370780945 and batch: 100, loss is 5.7118226146698 and perplexity is 302.42176462139577
At time: 182.01643300056458 and batch: 150, loss is 5.726314697265625 and perplexity is 306.8363971594247
At time: 183.5942621231079 and batch: 200, loss is 5.730540866851807 and perplexity is 308.1358838003693
At time: 185.16920256614685 and batch: 250, loss is 5.7662233638763425 and perplexity is 319.32946130397534
At time: 186.7429554462433 and batch: 300, loss is 5.746550426483155 and perplexity is 313.1087037219869
At time: 188.31792068481445 and batch: 350, loss is 5.706272277832031 and perplexity is 300.74787159293726
At time: 189.89623832702637 and batch: 400, loss is 5.695802087783814 and perplexity is 297.6154115629224
At time: 191.47405815124512 and batch: 450, loss is 5.725198097229004 and perplexity is 306.49397483708617
At time: 193.058580160141 and batch: 500, loss is 5.779512968063354 and perplexity is 323.6015477433515
At time: 194.63812947273254 and batch: 550, loss is 5.749929122924804 and perplexity is 314.1683921597985
At time: 196.21401453018188 and batch: 600, loss is 5.762600736618042 and perplexity is 318.17474251378184
At time: 197.81848454475403 and batch: 650, loss is 5.772940340042115 and perplexity is 321.48160955717276
At time: 199.39190125465393 and batch: 700, loss is 5.739817247390747 and perplexity is 311.00756834286886
At time: 200.9698097705841 and batch: 750, loss is 5.757156782150268 and perplexity is 316.44731997381615
At time: 202.5703480243683 and batch: 800, loss is 5.751918697357178 and perplexity is 314.7940757759699
At time: 204.17299056053162 and batch: 850, loss is 5.726928501129151 and perplexity is 307.0247923384008
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.5437672932942705 and perplexity of 255.63925570833962
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 208.35770463943481 and batch: 50, loss is 5.69231409072876 and perplexity is 296.5791381937014
At time: 209.93997287750244 and batch: 100, loss is 5.562573051452636 and perplexity is 260.49223467826715
At time: 211.52565097808838 and batch: 150, loss is 5.531365575790406 and perplexity is 252.4884678497908
At time: 213.11717581748962 and batch: 200, loss is 5.5109480857849125 and perplexity is 247.38555865098726
At time: 214.7027280330658 and batch: 250, loss is 5.5060419082641605 and perplexity is 246.1748136776708
At time: 216.2895655632019 and batch: 300, loss is 5.4683638858795165 and perplexity is 237.0719984384394
At time: 217.8749918937683 and batch: 350, loss is 5.4147924709320066 and perplexity is 224.70590783180373
At time: 219.4643702507019 and batch: 400, loss is 5.4041172218322755 and perplexity is 222.31987470004472
At time: 221.0616750717163 and batch: 450, loss is 5.412873125076294 and perplexity is 224.27503310993293
At time: 222.66598320007324 and batch: 500, loss is 5.428343162536621 and perplexity is 227.77155218150168
At time: 224.27254939079285 and batch: 550, loss is 5.368527221679687 and perplexity is 214.5466552435261
At time: 225.87623023986816 and batch: 600, loss is 5.364241704940796 and perplexity is 213.62917929464513
At time: 227.4781231880188 and batch: 650, loss is 5.3453372287750245 and perplexity is 209.6285654647317
At time: 229.08295893669128 and batch: 700, loss is 5.294698085784912 and perplexity is 199.27745218780623
At time: 230.67987084388733 and batch: 750, loss is 5.259254465103149 and perplexity is 192.33804310794548
At time: 232.27966737747192 and batch: 800, loss is 5.239988765716553 and perplexity is 188.6679828441634
At time: 233.87612080574036 and batch: 850, loss is 5.267621097564697 and perplexity is 193.95401552005575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.15399964650472 and perplexity of 173.12253637314387
Finished 8 epochs...
Completing Train Step...
At time: 238.01455068588257 and batch: 50, loss is 5.387422685623169 and perplexity is 218.63915692651926
At time: 239.6216595172882 and batch: 100, loss is 5.326455821990967 and perplexity is 205.7076162518001
At time: 241.2034604549408 and batch: 150, loss is 5.3294124412536625 and perplexity is 206.3167153458423
At time: 242.784512758255 and batch: 200, loss is 5.3391452884674075 and perplexity is 208.33456821180891
At time: 244.36210012435913 and batch: 250, loss is 5.3493408107757565 and perplexity is 210.46951289398925
At time: 245.9408586025238 and batch: 300, loss is 5.328845195770263 and perplexity is 206.19971630763783
At time: 247.51458024978638 and batch: 350, loss is 5.278321533203125 and perplexity is 196.04055149301078
At time: 249.0954258441925 and batch: 400, loss is 5.276148910522461 and perplexity is 195.6150916936698
At time: 250.69607138633728 and batch: 450, loss is 5.300024557113647 and perplexity is 200.34172972900922
At time: 252.29892706871033 and batch: 500, loss is 5.320563268661499 and perplexity is 204.4990374572977
At time: 253.90061831474304 and batch: 550, loss is 5.271580171585083 and perplexity is 194.7234158755482
At time: 255.50411176681519 and batch: 600, loss is 5.289022912979126 and perplexity is 198.14972127104852
At time: 257.11807656288147 and batch: 650, loss is 5.276809711456298 and perplexity is 195.7443970467775
At time: 258.7208454608917 and batch: 700, loss is 5.243293552398682 and perplexity is 189.29252169663772
At time: 260.32077145576477 and batch: 750, loss is 5.227632961273193 and perplexity is 186.35118059582572
At time: 261.9226520061493 and batch: 800, loss is 5.238055753707886 and perplexity is 188.3036376229676
At time: 263.52824211120605 and batch: 850, loss is 5.267422227859497 and perplexity is 193.91544777727194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.1339982350667315 and perplexity of 169.69424095938902
Finished 9 epochs...
Completing Train Step...
At time: 267.692547082901 and batch: 50, loss is 5.330369234085083 and perplexity is 206.5142121667921
At time: 269.3024115562439 and batch: 100, loss is 5.271263647079468 and perplexity is 194.661790896029
At time: 270.8855471611023 and batch: 150, loss is 5.271882839202881 and perplexity is 194.7823612679415
At time: 272.4675073623657 and batch: 200, loss is 5.284185962677002 and perplexity is 197.1935951482834
At time: 274.05586409568787 and batch: 250, loss is 5.293041095733643 and perplexity is 198.94752485073417
At time: 275.6370289325714 and batch: 300, loss is 5.27241662979126 and perplexity is 194.88636201400877
At time: 277.21828866004944 and batch: 350, loss is 5.2267529392242436 and perplexity is 186.18725958568407
At time: 278.83889079093933 and batch: 400, loss is 5.240132570266724 and perplexity is 188.69511610946472
At time: 280.45292258262634 and batch: 450, loss is 5.267748117446899 and perplexity is 193.978653100958
At time: 282.0529544353485 and batch: 500, loss is 5.281312284469604 and perplexity is 196.62773764685798
At time: 283.65475487709045 and batch: 550, loss is 5.242484798431397 and perplexity is 189.13949250856078
At time: 285.25750374794006 and batch: 600, loss is 5.2653249168395995 and perplexity is 193.5091729629667
At time: 286.86149430274963 and batch: 650, loss is 5.258982362747193 and perplexity is 192.28571459295475
At time: 288.4677653312683 and batch: 700, loss is 5.229530591964721 and perplexity is 186.70514205337156
At time: 290.08683371543884 and batch: 750, loss is 5.219454393386841 and perplexity is 184.8333102780619
At time: 291.6958518028259 and batch: 800, loss is 5.238160829544068 and perplexity is 188.3234248247072
At time: 293.29612493515015 and batch: 850, loss is 5.2624384212493895 and perplexity is 192.95141495881686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.126595815022786 and perplexity of 168.44273072577755
Finished 10 epochs...
Completing Train Step...
At time: 297.42273354530334 and batch: 50, loss is 5.299150133132935 and perplexity is 200.16662268625294
At time: 299.0251169204712 and batch: 100, loss is 5.242384586334229 and perplexity is 189.1205393930408
At time: 300.61277079582214 and batch: 150, loss is 5.239310293197632 and perplexity is 188.54002021707512
At time: 302.2010374069214 and batch: 200, loss is 5.2551484966278075 and perplexity is 191.54992826092763
At time: 303.799631357193 and batch: 250, loss is 5.262206430435181 and perplexity is 192.90665719485364
At time: 305.41003036499023 and batch: 300, loss is 5.2451731395721435 and perplexity is 189.64864807289038
At time: 307.00930070877075 and batch: 350, loss is 5.203640279769897 and perplexity is 181.93332608579513
At time: 308.6060218811035 and batch: 400, loss is 5.2245141220092775 and perplexity is 185.77088660912577
At time: 310.20245575904846 and batch: 450, loss is 5.251384668350219 and perplexity is 190.83032231050046
At time: 311.8083679676056 and batch: 500, loss is 5.2653578090667725 and perplexity is 193.51553801532364
At time: 313.40995478630066 and batch: 550, loss is 5.231024770736695 and perplexity is 186.98432143325684
At time: 314.99973702430725 and batch: 600, loss is 5.253251466751099 and perplexity is 191.18689677379464
At time: 316.5925531387329 and batch: 650, loss is 5.250346002578735 and perplexity is 190.63221628733805
At time: 318.185001373291 and batch: 700, loss is 5.222958984375 and perplexity is 185.48221183468866
At time: 319.8037109375 and batch: 750, loss is 5.2133637809753415 and perplexity is 183.71098352250013
At time: 321.3972477912903 and batch: 800, loss is 5.234424715042114 and perplexity is 187.62113967194068
At time: 322.99028158187866 and batch: 850, loss is 5.254891986846924 and perplexity is 191.50080013199332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.119407971700032 and perplexity of 167.23633166623156
Finished 11 epochs...
Completing Train Step...
At time: 327.0720167160034 and batch: 50, loss is 5.2790211486816405 and perplexity is 196.17775248562478
At time: 328.6446120738983 and batch: 100, loss is 5.223300170898438 and perplexity is 185.54550666276117
At time: 330.21824860572815 and batch: 150, loss is 5.21955472946167 and perplexity is 184.85185665733275
At time: 331.80348348617554 and batch: 200, loss is 5.240267086029053 and perplexity is 188.72050028410345
At time: 333.40849590301514 and batch: 250, loss is 5.24753478050232 and perplexity is 190.097059367353
At time: 335.0027959346771 and batch: 300, loss is 5.228956165313721 and perplexity is 186.59792444117247
At time: 336.5975773334503 and batch: 350, loss is 5.188018436431885 and perplexity is 179.11327670318812
At time: 338.18982124328613 and batch: 400, loss is 5.214139461517334 and perplexity is 183.85353983970177
At time: 339.782644033432 and batch: 450, loss is 5.241146516799927 and perplexity is 188.88653989861996
At time: 341.3746061325073 and batch: 500, loss is 5.253120908737182 and perplexity is 191.1619374216222
At time: 342.9750466346741 and batch: 550, loss is 5.2224305152893065 and perplexity is 185.38421611592253
At time: 344.5697853565216 and batch: 600, loss is 5.24583592414856 and perplexity is 189.7743859357265
At time: 346.1682188510895 and batch: 650, loss is 5.244185247421265 and perplexity is 189.46138817355614
At time: 347.761714220047 and batch: 700, loss is 5.216312847137451 and perplexity is 184.25355901990298
At time: 349.35425543785095 and batch: 750, loss is 5.206712112426758 and perplexity is 182.49305407345256
At time: 350.9602196216583 and batch: 800, loss is 5.226841478347779 and perplexity is 186.2037451722601
At time: 352.5573196411133 and batch: 850, loss is 5.24923529624939 and perplexity is 190.42059742309223
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.1164547602335615 and perplexity of 166.7431759688941
Finished 12 epochs...
Completing Train Step...
At time: 356.72905230522156 and batch: 50, loss is 5.267023553848267 and perplexity is 193.8381541363737
At time: 358.3242440223694 and batch: 100, loss is 5.211111364364624 and perplexity is 183.2976555198197
At time: 359.94483184814453 and batch: 150, loss is 5.204484481811523 and perplexity is 182.08697941922
At time: 361.5329682826996 and batch: 200, loss is 5.229131593704223 and perplexity is 186.63066188618293
At time: 363.11610555648804 and batch: 250, loss is 5.238329658508301 and perplexity is 188.3552219575241
At time: 364.7254989147186 and batch: 300, loss is 5.2181048011779785 and perplexity is 184.58402893448863
At time: 366.34237718582153 and batch: 350, loss is 5.179060211181641 and perplexity is 177.51590511480177
At time: 367.9499099254608 and batch: 400, loss is 5.2067158412933345 and perplexity is 182.49373456697106
At time: 369.5523862838745 and batch: 450, loss is 5.232601232528687 and perplexity is 187.27932754353205
At time: 371.1614375114441 and batch: 500, loss is 5.245641212463379 and perplexity is 189.7374382424274
At time: 372.7666461467743 and batch: 550, loss is 5.215366373062134 and perplexity is 184.07925030535768
At time: 374.3683202266693 and batch: 600, loss is 5.237846822738647 and perplexity is 188.26429927109103
At time: 375.96908044815063 and batch: 650, loss is 5.236046638488769 and perplexity is 187.92569371236803
At time: 377.580260515213 and batch: 700, loss is 5.209091415405274 and perplexity is 182.92777730462365
At time: 379.1843740940094 and batch: 750, loss is 5.200570783615112 and perplexity is 181.37573863500566
At time: 380.78698682785034 and batch: 800, loss is 5.218138799667359 and perplexity is 184.5903046193174
At time: 382.4017868041992 and batch: 850, loss is 5.24197907447815 and perplexity is 189.04386431946526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.109633445739746 and perplexity of 165.6096388278954
Finished 13 epochs...
Completing Train Step...
At time: 386.63319730758667 and batch: 50, loss is 5.252014760971069 and perplexity is 190.95060097779367
At time: 388.21103405952454 and batch: 100, loss is 5.190527105331421 and perplexity is 179.56317669906048
At time: 389.8003029823303 and batch: 150, loss is 5.183674926757813 and perplexity is 178.33698359185922
At time: 391.38891553878784 and batch: 200, loss is 5.206691942214966 and perplexity is 182.48937318702352
At time: 392.97255873680115 and batch: 250, loss is 5.2145000839233395 and perplexity is 183.91985350196893
At time: 394.55017948150635 and batch: 300, loss is 5.197605171203613 and perplexity is 180.83864529234467
At time: 396.13079738616943 and batch: 350, loss is 5.155535335540772 and perplexity is 173.38860299965654
At time: 397.70938658714294 and batch: 400, loss is 5.186033420562744 and perplexity is 178.75808665213165
At time: 399.2844967842102 and batch: 450, loss is 5.213522939682007 and perplexity is 183.74022505199812
At time: 400.9041612148285 and batch: 500, loss is 5.223831605911255 and perplexity is 185.64413824729007
At time: 402.4995017051697 and batch: 550, loss is 5.194019966125488 and perplexity is 180.1914624976663
At time: 404.1018171310425 and batch: 600, loss is 5.215926847457886 and perplexity is 184.18245092989537
At time: 405.70110630989075 and batch: 650, loss is 5.209354019165039 and perplexity is 182.97582113467843
At time: 407.29810190200806 and batch: 700, loss is 5.181291360855102 and perplexity is 177.91241183696954
At time: 408.89474749565125 and batch: 750, loss is 5.175752973556518 and perplexity is 176.9297875835617
At time: 410.49477195739746 and batch: 800, loss is 5.197400588989257 and perplexity is 180.80165270599127
At time: 412.1021955013275 and batch: 850, loss is 5.217821130752563 and perplexity is 184.53167533041156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.099266370137532 and perplexity of 163.90162005915857
Finished 14 epochs...
Completing Train Step...
At time: 416.1984272003174 and batch: 50, loss is 5.229010009765625 and perplexity is 186.60797197464
At time: 417.7919330596924 and batch: 100, loss is 5.165208797454834 and perplexity is 175.0740097634132
At time: 419.36613178253174 and batch: 150, loss is 5.160817728042603 and perplexity is 174.3069330104293
At time: 420.9468557834625 and batch: 200, loss is 5.189821662902832 and perplexity is 179.43654988481975
At time: 422.5308508872986 and batch: 250, loss is 5.192986764907837 and perplexity is 180.00538460370996
At time: 424.11324644088745 and batch: 300, loss is 5.1784821891784665 and perplexity is 177.4133266648886
At time: 425.6918330192566 and batch: 350, loss is 5.1373022937774655 and perplexity is 170.25584797475514
At time: 427.26856422424316 and batch: 400, loss is 5.169441137313843 and perplexity is 175.81655271185807
At time: 428.8618993759155 and batch: 450, loss is 5.196704607009888 and perplexity is 180.67586179304968
At time: 430.4608862400055 and batch: 500, loss is 5.209092874526977 and perplexity is 182.9280442187084
At time: 432.0614206790924 and batch: 550, loss is 5.180482196807861 and perplexity is 177.76850973780722
At time: 433.6726129055023 and batch: 600, loss is 5.203378601074219 and perplexity is 181.8857242387908
At time: 435.28701758384705 and batch: 650, loss is 5.192315301895142 and perplexity is 179.8845582156275
At time: 436.8906099796295 and batch: 700, loss is 5.165847759246827 and perplexity is 175.18591111295913
At time: 438.48574662208557 and batch: 750, loss is 5.164748697280884 and perplexity is 174.99347670912368
At time: 440.1066224575043 and batch: 800, loss is 5.185960340499878 and perplexity is 178.74502347725593
At time: 441.70041847229004 and batch: 850, loss is 5.205497303009033 and perplexity is 182.27149439633337
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.092440605163574 and perplexity of 162.78667562498472
Finished 15 epochs...
Completing Train Step...
At time: 445.7994682788849 and batch: 50, loss is 5.2163012504577635 and perplexity is 184.25142230278715
At time: 447.4050941467285 and batch: 100, loss is 5.15113468170166 and perplexity is 172.62725621869606
At time: 448.98202180862427 and batch: 150, loss is 5.147958812713623 and perplexity is 172.07988431997268
At time: 450.55686354637146 and batch: 200, loss is 5.17796498298645 and perplexity is 177.3215911189324
At time: 452.1304271221161 and batch: 250, loss is 5.180310897827148 and perplexity is 177.73806078129834
At time: 453.70832204818726 and batch: 300, loss is 5.1657181739807125 and perplexity is 175.16321107087538
At time: 455.27798867225647 and batch: 350, loss is 5.122962379455567 and perplexity is 167.83181544966638
At time: 456.8587398529053 and batch: 400, loss is 5.1542688083648684 and perplexity is 173.16914062883043
At time: 458.44766759872437 and batch: 450, loss is 5.183200311660767 and perplexity is 178.2523622499566
At time: 460.0388126373291 and batch: 500, loss is 5.1980258750915525 and perplexity is 180.91474081922846
At time: 461.64672446250916 and batch: 550, loss is 5.169150276184082 and perplexity is 175.7654219470421
At time: 463.2475657463074 and batch: 600, loss is 5.190236797332764 and perplexity is 179.51105563854685
At time: 464.85564732551575 and batch: 650, loss is 5.181050281524659 and perplexity is 177.86952600149826
At time: 466.46014618873596 and batch: 700, loss is 5.155468883514405 and perplexity is 173.37708135846074
At time: 468.0583128929138 and batch: 750, loss is 5.15347074508667 and perplexity is 173.03099582825573
At time: 469.653742313385 and batch: 800, loss is 5.16929633140564 and perplexity is 175.7910952795033
At time: 471.24839425086975 and batch: 850, loss is 5.189417552947998 and perplexity is 179.36405243820673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.080795605977376 and perplexity of 160.90201962610243
Finished 16 epochs...
Completing Train Step...
At time: 475.37992811203003 and batch: 50, loss is 5.197075319290161 and perplexity is 180.74285297022413
At time: 476.99608755111694 and batch: 100, loss is 5.135286912918091 and perplexity is 169.91306313457895
At time: 478.58587098121643 and batch: 150, loss is 5.133382177352905 and perplexity is 169.5897317084245
At time: 480.19780254364014 and batch: 200, loss is 5.165824851989746 and perplexity is 175.18189813021976
At time: 481.78505206108093 and batch: 250, loss is 5.165037965774536 and perplexity is 175.04410413063619
At time: 483.3691141605377 and batch: 300, loss is 5.149910640716553 and perplexity is 172.41608265085512
At time: 484.94909834861755 and batch: 350, loss is 5.107416496276856 and perplexity is 165.242897301968
At time: 486.52248454093933 and batch: 400, loss is 5.142697658538818 and perplexity is 171.17692291058782
At time: 488.1004593372345 and batch: 450, loss is 5.170616588592529 and perplexity is 176.0233380127713
At time: 489.67367601394653 and batch: 500, loss is 5.181234588623047 and perplexity is 177.90231163894737
At time: 491.2577521800995 and batch: 550, loss is 5.153728637695313 and perplexity is 173.075624997665
At time: 492.84514832496643 and batch: 600, loss is 5.1760142993927 and perplexity is 176.97602995014688
At time: 494.42556166648865 and batch: 650, loss is 5.167048120498658 and perplexity is 175.396323752851
At time: 496.0152323246002 and batch: 700, loss is 5.142170972824097 and perplexity is 171.08679020848774
At time: 497.6076138019562 and batch: 750, loss is 5.139101047515869 and perplexity is 170.56237191568016
At time: 499.2028398513794 and batch: 800, loss is 5.152507486343384 and perplexity is 172.8644024578064
At time: 500.8027195930481 and batch: 850, loss is 5.173428735733032 and perplexity is 176.51903820368943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.071633021036784 and perplexity of 159.43447472362453
Finished 17 epochs...
Completing Train Step...
At time: 504.9201533794403 and batch: 50, loss is 5.1785189819335935 and perplexity is 177.41985431005713
At time: 506.49667596817017 and batch: 100, loss is 5.11570689201355 and perplexity is 166.61852066536312
At time: 508.073123216629 and batch: 150, loss is 5.118373193740845 and perplexity is 167.06336870081373
At time: 509.6489996910095 and batch: 200, loss is 5.149375925064087 and perplexity is 172.32391371700666
At time: 511.22304821014404 and batch: 250, loss is 5.146917858123779 and perplexity is 171.9008501739447
At time: 512.8084151744843 and batch: 300, loss is 5.132875947952271 and perplexity is 169.5039021267498
At time: 514.3970799446106 and batch: 350, loss is 5.0919904613494875 and perplexity is 162.71341470014954
At time: 515.9758968353271 and batch: 400, loss is 5.122010202407837 and perplexity is 167.6720859045878
At time: 517.5561399459839 and batch: 450, loss is 5.145956773757934 and perplexity is 171.73571831989943
At time: 519.1425166130066 and batch: 500, loss is 5.159557819366455 and perplexity is 174.08746047992145
At time: 520.7660539150238 and batch: 550, loss is 5.132160587310791 and perplexity is 169.38268906728624
At time: 522.3636226654053 and batch: 600, loss is 5.159131555557251 and perplexity is 174.0132691095541
At time: 523.9692721366882 and batch: 650, loss is 5.154124927520752 and perplexity is 173.14422669906347
At time: 525.5836503505707 and batch: 700, loss is 5.129230194091797 and perplexity is 168.8870577358487
At time: 527.1975774765015 and batch: 750, loss is 5.119976511001587 and perplexity is 167.33143912703449
At time: 528.787380695343 and batch: 800, loss is 5.131230688095092 and perplexity is 169.22525344854355
At time: 530.3776409626007 and batch: 850, loss is 5.146203651428222 and perplexity is 171.77812126789865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.068543752034505 and perplexity of 158.9426987482133
Finished 18 epochs...
Completing Train Step...
At time: 534.4727356433868 and batch: 50, loss is 5.165247354507446 and perplexity is 175.0807602313569
At time: 536.0441443920135 and batch: 100, loss is 5.101015701293945 and perplexity is 164.18858919856189
At time: 537.6155772209167 and batch: 150, loss is 5.103224487304687 and perplexity is 164.55164746891606
At time: 539.1863634586334 and batch: 200, loss is 5.135465412139893 and perplexity is 169.94339519116613
At time: 540.7570533752441 and batch: 250, loss is 5.135148153305054 and perplexity is 169.8894876993854
At time: 542.3331327438354 and batch: 300, loss is 5.122270355224609 and perplexity is 167.71571194450252
At time: 543.905143737793 and batch: 350, loss is 5.078646955490112 and perplexity is 160.5566685754478
At time: 545.4811224937439 and batch: 400, loss is 5.110174760818482 and perplexity is 165.6993100905906
At time: 547.0711505413055 and batch: 450, loss is 5.134950485229492 and perplexity is 169.85590929009277
At time: 548.6632714271545 and batch: 500, loss is 5.1474333667755126 and perplexity is 171.98948939462926
At time: 550.2667977809906 and batch: 550, loss is 5.118851747512817 and perplexity is 167.143336639017
At time: 551.8717522621155 and batch: 600, loss is 5.150397996902466 and perplexity is 172.5001311744063
At time: 553.47665143013 and batch: 650, loss is 5.145392055511475 and perplexity is 171.63876340487678
At time: 555.0752081871033 and batch: 700, loss is 5.119710254669189 and perplexity is 167.28689200249784
At time: 556.67187333107 and batch: 750, loss is 5.110262746810913 and perplexity is 165.71388995023642
At time: 558.267023563385 and batch: 800, loss is 5.119959602355957 and perplexity is 167.32860980294757
At time: 559.8615386486053 and batch: 850, loss is 5.132674026489258 and perplexity is 169.46967910614342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.062736511230469 and perplexity of 158.02235513749835
Finished 19 epochs...
Completing Train Step...
At time: 563.9806337356567 and batch: 50, loss is 5.1534709548950195 and perplexity is 173.03103213160716
At time: 565.5533833503723 and batch: 100, loss is 5.08993236541748 and perplexity is 162.37887925437647
At time: 567.1260688304901 and batch: 150, loss is 5.09311074256897 and perplexity is 162.89580162604477
At time: 568.6994204521179 and batch: 200, loss is 5.125881452560424 and perplexity is 168.32244453097582
At time: 570.2728657722473 and batch: 250, loss is 5.125489091873169 and perplexity is 168.25641437562473
At time: 571.8455846309662 and batch: 300, loss is 5.109505739212036 and perplexity is 165.58849074636814
At time: 573.4185750484467 and batch: 350, loss is 5.068244380950928 and perplexity is 158.89512302201604
At time: 574.9902229309082 and batch: 400, loss is 5.099669733047485 and perplexity is 163.96774522889555
At time: 576.5621607303619 and batch: 450, loss is 5.123045234680176 and perplexity is 167.84572176852967
At time: 578.133661031723 and batch: 500, loss is 5.136030550003052 and perplexity is 170.0394637818246
At time: 579.709722995758 and batch: 550, loss is 5.109888334274292 and perplexity is 165.65185620617711
At time: 581.2901215553284 and batch: 600, loss is 5.137498235702514 and perplexity is 170.28921150190826
At time: 582.8761332035065 and batch: 650, loss is 5.134221124649048 and perplexity is 169.73206825337618
At time: 584.4689645767212 and batch: 700, loss is 5.110043077468872 and perplexity is 165.67749168700163
At time: 586.0605947971344 and batch: 750, loss is 5.101108551025391 and perplexity is 164.2038347727382
At time: 587.6574556827545 and batch: 800, loss is 5.111240224838257 and perplexity is 165.8759508290589
At time: 589.2591280937195 and batch: 850, loss is 5.122497386932373 and perplexity is 167.75379305165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.054766337076823 and perplexity of 156.76789521954248
Finished 20 epochs...
Completing Train Step...
At time: 593.3566136360168 and batch: 50, loss is 5.139299430847168 and perplexity is 170.5962120037588
At time: 594.956449508667 and batch: 100, loss is 5.080487995147705 and perplexity is 160.85253203419938
At time: 596.5411996841431 and batch: 150, loss is 5.0864033699035645 and perplexity is 161.80685484737296
At time: 598.1352114677429 and batch: 200, loss is 5.113645715713501 and perplexity is 166.27544421140837
At time: 599.7185456752777 and batch: 250, loss is 5.114130687713623 and perplexity is 166.35610270313293
At time: 601.3423087596893 and batch: 300, loss is 5.099241800308228 and perplexity is 163.89759307380237
At time: 602.918372631073 and batch: 350, loss is 5.060209589004517 and perplexity is 157.62354902432895
At time: 604.4965934753418 and batch: 400, loss is 5.09211256980896 and perplexity is 162.73328459767072
At time: 606.0759592056274 and batch: 450, loss is 5.113227462768554 and perplexity is 166.20591355890835
At time: 607.675833940506 and batch: 500, loss is 5.1255811500549315 and perplexity is 168.27190446818602
At time: 609.2817997932434 and batch: 550, loss is 5.09922233581543 and perplexity is 163.89440292132983
At time: 610.8830127716064 and batch: 600, loss is 5.128995523452759 and perplexity is 168.84742955205215
At time: 612.4802753925323 and batch: 650, loss is 5.126737174987793 and perplexity is 168.46654346734303
At time: 614.0763459205627 and batch: 700, loss is 5.104779376983642 and perplexity is 164.80770614712947
At time: 615.6729466915131 and batch: 750, loss is 5.0939900588989255 and perplexity is 163.03910155824772
At time: 617.2680652141571 and batch: 800, loss is 5.104053211212158 and perplexity is 164.6880718744627
At time: 618.8706657886505 and batch: 850, loss is 5.113646459579468 and perplexity is 166.27556789809847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.054918607076009 and perplexity of 156.7917680843343
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 623.0069763660431 and batch: 50, loss is 5.132514734268188 and perplexity is 169.4426860545033
At time: 624.6086955070496 and batch: 100, loss is 5.075499486923218 and perplexity is 160.0521159561877
At time: 626.1847252845764 and batch: 150, loss is 5.0729446125030515 and perplexity is 159.64372481542097
At time: 627.7590141296387 and batch: 200, loss is 5.099984292984009 and perplexity is 164.0193310254137
At time: 629.3337976932526 and batch: 250, loss is 5.096760292053222 and perplexity is 163.49138405757665
At time: 630.919148683548 and batch: 300, loss is 5.062844753265381 and perplexity is 158.03946072453513
At time: 632.510656118393 and batch: 350, loss is 5.022130241394043 and perplexity is 151.7341902118243
At time: 634.0912034511566 and batch: 400, loss is 5.047572355270386 and perplexity is 155.64415675696108
At time: 635.6717987060547 and batch: 450, loss is 5.071953496932983 and perplexity is 159.48557781801426
At time: 637.2518815994263 and batch: 500, loss is 5.076138973236084 and perplexity is 160.15449982673977
At time: 638.8300061225891 and batch: 550, loss is 5.037681894302368 and perplexity is 154.11235191455611
At time: 640.4510571956635 and batch: 600, loss is 5.0497253894805905 and perplexity is 155.97962495874452
At time: 642.056343793869 and batch: 650, loss is 5.045542392730713 and perplexity is 155.32852541750958
At time: 643.6618542671204 and batch: 700, loss is 5.0158337879180905 and perplexity is 150.78180441534195
At time: 645.2696495056152 and batch: 750, loss is 4.989716453552246 and perplexity is 146.89476610094917
At time: 646.8779029846191 and batch: 800, loss is 4.992329416275024 and perplexity is 147.2790985534564
At time: 648.4802207946777 and batch: 850, loss is 5.023228664398193 and perplexity is 151.90095010655452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.00266170501709 and perplexity of 148.8087173489131
Finished 22 epochs...
Completing Train Step...
At time: 652.6744351387024 and batch: 50, loss is 5.093992528915405 and perplexity is 163.0395042680127
At time: 654.2976055145264 and batch: 100, loss is 5.039177207946778 and perplexity is 154.34297059780658
At time: 655.8817231655121 and batch: 150, loss is 5.0373478031158445 and perplexity is 154.0608729358624
At time: 657.463131904602 and batch: 200, loss is 5.070650539398193 and perplexity is 159.27791020312745
At time: 659.0482063293457 and batch: 250, loss is 5.068241052627563 and perplexity is 158.89459416854567
At time: 660.6268148422241 and batch: 300, loss is 5.038538799285889 and perplexity is 154.2444681543817
At time: 662.2136645317078 and batch: 350, loss is 4.999720973968506 and perplexity is 148.3717537446257
At time: 663.8001873493195 and batch: 400, loss is 5.025120668411255 and perplexity is 152.18861936362475
At time: 665.384758234024 and batch: 450, loss is 5.051985731124878 and perplexity is 156.33259096229807
At time: 666.9628176689148 and batch: 500, loss is 5.05647946357727 and perplexity is 157.03668862808007
At time: 668.5498731136322 and batch: 550, loss is 5.023503980636597 and perplexity is 151.94277666224858
At time: 670.1263289451599 and batch: 600, loss is 5.042312021255493 and perplexity is 154.8275661576029
At time: 671.7085340023041 and batch: 650, loss is 5.0404308414459225 and perplexity is 154.53658144927363
At time: 673.3036575317383 and batch: 700, loss is 5.014047746658325 and perplexity is 150.51274224103108
At time: 674.8929851055145 and batch: 750, loss is 4.993949089050293 and perplexity is 147.5178357856566
At time: 676.4816319942474 and batch: 800, loss is 5.00114224433899 and perplexity is 148.5827800492338
At time: 678.0815093517303 and batch: 850, loss is 5.03014760017395 and perplexity is 152.95558730085813
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.000108400980632 and perplexity of 148.42924810657814
Finished 23 epochs...
Completing Train Step...
At time: 682.213677406311 and batch: 50, loss is 5.08363634109497 and perplexity is 161.35974948056736
At time: 683.8029654026031 and batch: 100, loss is 5.027409410476684 and perplexity is 152.53733877080418
At time: 685.3844685554504 and batch: 150, loss is 5.024209814071655 and perplexity is 152.0500608121484
At time: 686.9711585044861 and batch: 200, loss is 5.059539451599121 and perplexity is 157.51795497336988
At time: 688.5642418861389 and batch: 250, loss is 5.058066244125366 and perplexity is 157.28606919472293
At time: 690.1444823741913 and batch: 300, loss is 5.027967624664306 and perplexity is 152.6225110474231
At time: 691.7351932525635 and batch: 350, loss is 4.99017071723938 and perplexity is 146.96151021758536
At time: 693.326354265213 and batch: 400, loss is 5.016452331542968 and perplexity is 150.8750983894326
At time: 694.9107217788696 and batch: 450, loss is 5.0443158626556395 and perplexity is 155.1381270980699
At time: 696.5101051330566 and batch: 500, loss is 5.049811325073242 and perplexity is 155.99302973622244
At time: 698.1229155063629 and batch: 550, loss is 5.019926834106445 and perplexity is 151.4002260559776
At time: 699.7366282939911 and batch: 600, loss is 5.041012201309204 and perplexity is 154.6264489352806
At time: 701.3287298679352 and batch: 650, loss is 5.040891380310058 and perplexity is 154.6077679417767
At time: 702.9211473464966 and batch: 700, loss is 5.015137186050415 and perplexity is 150.67680610403056
At time: 704.5151579380035 and batch: 750, loss is 4.997742662429809 and perplexity is 148.07851834330174
At time: 706.1076650619507 and batch: 800, loss is 5.00557089805603 and perplexity is 149.242260958813
At time: 707.7015469074249 and batch: 850, loss is 5.032685031890869 and perplexity is 153.34419448294412
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9992367426554365 and perplexity of 148.2999248877819
Finished 24 epochs...
Completing Train Step...
At time: 711.8182470798492 and batch: 50, loss is 5.077890977859497 and perplexity is 160.43533719324262
At time: 713.3904881477356 and batch: 100, loss is 5.021115312576294 and perplexity is 151.58026893232417
At time: 714.9679064750671 and batch: 150, loss is 5.016812429428101 and perplexity is 150.92943797647982
At time: 716.5390453338623 and batch: 200, loss is 5.053663730621338 and perplexity is 156.5951371858246
At time: 718.1119449138641 and batch: 250, loss is 5.052338199615479 and perplexity is 156.38770298672165
At time: 719.6830031871796 and batch: 300, loss is 5.021911888122559 and perplexity is 151.7010621719351
At time: 721.2977187633514 and batch: 350, loss is 4.984966135025024 and perplexity is 146.19862392958237
At time: 722.8697993755341 and batch: 400, loss is 5.01174955368042 and perplexity is 150.1672320903017
At time: 724.4492878913879 and batch: 450, loss is 5.04042760848999 and perplexity is 154.53608184012347
At time: 726.0277688503265 and batch: 500, loss is 5.046685028076172 and perplexity is 155.5061107191093
At time: 727.6068017482758 and batch: 550, loss is 5.018345813751221 and perplexity is 151.1610483390383
At time: 729.1915583610535 and batch: 600, loss is 5.040431289672852 and perplexity is 154.53665071674652
At time: 730.784348487854 and batch: 650, loss is 5.041680021286011 and perplexity is 154.72974605492314
At time: 732.3876197338104 and batch: 700, loss is 5.01603590965271 and perplexity is 150.81228377536624
At time: 733.9928438663483 and batch: 750, loss is 4.999540233612061 and perplexity is 148.34493940425725
At time: 735.5911092758179 and batch: 800, loss is 5.00731855392456 and perplexity is 149.50331312023698
At time: 737.1872704029083 and batch: 850, loss is 5.033483810424805 and perplexity is 153.46673146724171
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.998426119486491 and perplexity of 148.17975824422794
Finished 25 epochs...
Completing Train Step...
At time: 741.3418111801147 and batch: 50, loss is 5.0730384254455565 and perplexity is 159.65870216552202
At time: 742.9329535961151 and batch: 100, loss is 5.015861492156983 and perplexity is 150.78598176833702
At time: 744.5212693214417 and batch: 150, loss is 5.011461906433105 and perplexity is 150.12404311124735
At time: 746.0992550849915 and batch: 200, loss is 5.049447183609009 and perplexity is 155.93623654696918
At time: 747.678955078125 and batch: 250, loss is 5.048483991622925 and perplexity is 155.7861123244175
At time: 749.2624034881592 and batch: 300, loss is 5.017549390792847 and perplexity is 151.04070813693994
At time: 750.8410604000092 and batch: 350, loss is 4.98126856803894 and perplexity is 145.6590429075787
At time: 752.4274616241455 and batch: 400, loss is 5.008463869094848 and perplexity is 149.67463962545662
At time: 754.0186088085175 and batch: 450, loss is 5.037964963912964 and perplexity is 154.15598261297166
At time: 755.5994718074799 and batch: 500, loss is 5.04453070640564 and perplexity is 155.1714611357403
At time: 757.1762638092041 and batch: 550, loss is 5.0174962997436525 and perplexity is 151.0326894401363
At time: 758.7554712295532 and batch: 600, loss is 5.040324001312256 and perplexity is 154.520071622227
At time: 760.3395435810089 and batch: 650, loss is 5.0422318744659425 and perplexity is 154.81515772249617
At time: 761.9705452919006 and batch: 700, loss is 5.016409959793091 and perplexity is 150.86870568293693
At time: 763.5614442825317 and batch: 750, loss is 5.000156993865967 and perplexity is 148.43646088725524
At time: 765.1526482105255 and batch: 800, loss is 5.007971525192261 and perplexity is 149.60096636704301
At time: 766.7451107501984 and batch: 850, loss is 5.0334302425384525 and perplexity is 153.4585107989955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.99772294362386 and perplexity of 148.0755984405219
Finished 26 epochs...
Completing Train Step...
At time: 770.8810038566589 and batch: 50, loss is 5.068931169509888 and perplexity is 159.00428785687106
At time: 772.520295381546 and batch: 100, loss is 5.011851797103882 and perplexity is 150.18258648713208
At time: 774.1121807098389 and batch: 150, loss is 5.007790307998658 and perplexity is 149.57385855603258
At time: 775.6984121799469 and batch: 200, loss is 5.0466104221344 and perplexity is 155.49450947203417
At time: 777.283361196518 and batch: 250, loss is 5.044794683456421 and perplexity is 155.21242824736703
At time: 778.8642346858978 and batch: 300, loss is 5.014146299362182 and perplexity is 150.52757640970563
At time: 780.4439959526062 and batch: 350, loss is 4.978321847915649 and perplexity is 145.23045824435178
At time: 782.0242643356323 and batch: 400, loss is 5.005898590087891 and perplexity is 149.2911744723927
At time: 783.6188957691193 and batch: 450, loss is 5.0360997200012205 and perplexity is 153.86871210301655
At time: 785.2211964130402 and batch: 500, loss is 5.0431454372406 and perplexity is 154.9566557113507
At time: 786.8127400875092 and batch: 550, loss is 5.016607809066772 and perplexity is 150.8985578997999
At time: 788.4017810821533 and batch: 600, loss is 5.039823913574219 and perplexity is 154.44281734769586
At time: 789.9920098781586 and batch: 650, loss is 5.04197509765625 and perplexity is 154.7754098835842
At time: 791.5837678909302 and batch: 700, loss is 5.015874032974243 and perplexity is 150.78787275963705
At time: 793.1851110458374 and batch: 750, loss is 5.000166244506836 and perplexity is 148.43783402599803
At time: 794.7726657390594 and batch: 800, loss is 5.007958726882935 and perplexity is 149.599051739852
At time: 796.360312461853 and batch: 850, loss is 5.032633981704712 and perplexity is 153.33636643308301
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.99702803293864 and perplexity of 147.9727348695818
Finished 27 epochs...
Completing Train Step...
At time: 800.509877204895 and batch: 50, loss is 5.065143623352051 and perplexity is 158.40319083725925
At time: 802.1274356842041 and batch: 100, loss is 5.008604927062988 and perplexity is 149.69575391513865
At time: 803.7092649936676 and batch: 150, loss is 5.004493942260742 and perplexity is 149.08162015801864
At time: 805.2904951572418 and batch: 200, loss is 5.043897609710694 and perplexity is 155.07325368723107
At time: 806.8720123767853 and batch: 250, loss is 5.041733722686768 and perplexity is 154.73805548213707
At time: 808.4635789394379 and batch: 300, loss is 5.0112997341156005 and perplexity is 150.0996991212768
At time: 810.0443406105042 and batch: 350, loss is 4.975771074295044 and perplexity is 144.86048028816256
At time: 811.6301527023315 and batch: 400, loss is 5.003733606338501 and perplexity is 148.9683111287879
At time: 813.2200183868408 and batch: 450, loss is 5.034465856552124 and perplexity is 153.61751690356007
At time: 814.8263404369354 and batch: 500, loss is 5.041226377487183 and perplexity is 154.65956978384972
At time: 816.4365077018738 and batch: 550, loss is 5.015320291519165 and perplexity is 150.70439837731297
At time: 818.040577173233 and batch: 600, loss is 5.0391612720489505 and perplexity is 154.34051102359462
At time: 819.646164894104 and batch: 650, loss is 5.041382532119751 and perplexity is 154.68372247787096
At time: 821.2547981739044 and batch: 700, loss is 5.015529165267944 and perplexity is 150.73587985767247
At time: 822.8660600185394 and batch: 750, loss is 4.999800348281861 and perplexity is 148.3835311181047
At time: 824.4636335372925 and batch: 800, loss is 5.007544403076172 and perplexity is 149.53708212987587
At time: 826.0646715164185 and batch: 850, loss is 5.031620082855224 and perplexity is 153.180977655135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.996503194173177 and perplexity of 147.8950934184946
Finished 28 epochs...
Completing Train Step...
At time: 830.2360968589783 and batch: 50, loss is 5.062109432220459 and perplexity is 157.92329369840004
At time: 831.8172075748444 and batch: 100, loss is 5.005596065521241 and perplexity is 149.24601705548912
At time: 833.3973119258881 and batch: 150, loss is 5.0020466327667235 and perplexity is 148.71721737869012
At time: 834.9784708023071 and batch: 200, loss is 5.041571550369262 and perplexity is 154.71296328775787
At time: 836.5619993209839 and batch: 250, loss is 5.038869028091431 and perplexity is 154.29541253204926
At time: 838.1427943706512 and batch: 300, loss is 5.0088957500457765 and perplexity is 149.73929521190098
At time: 839.7318775653839 and batch: 350, loss is 4.974023866653442 and perplexity is 144.60759993161406
At time: 841.3262615203857 and batch: 400, loss is 5.001761684417724 and perplexity is 148.6748466901459
At time: 842.9477818012238 and batch: 450, loss is 5.032897710800171 and perplexity is 153.37681102727709
At time: 844.5263640880585 and batch: 500, loss is 5.039654474258423 and perplexity is 154.41665087927174
At time: 846.1068458557129 and batch: 550, loss is 5.014345397949219 and perplexity is 150.55754922115202
At time: 847.685952425003 and batch: 600, loss is 5.038100061416626 and perplexity is 154.17681010823677
At time: 849.2647006511688 and batch: 650, loss is 5.040665235519409 and perplexity is 154.57280815359874
At time: 850.8511536121368 and batch: 700, loss is 5.014771127700806 and perplexity is 150.62165969508126
At time: 852.4447672367096 and batch: 750, loss is 4.999259920120239 and perplexity is 148.30336214390448
At time: 854.0525889396667 and batch: 800, loss is 5.0068136501312255 and perplexity is 149.42784738339807
At time: 855.6533155441284 and batch: 850, loss is 5.03052583694458 and perplexity is 153.01345167077503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.995850245157878 and perplexity of 147.7985569829945
Finished 29 epochs...
Completing Train Step...
At time: 859.8767294883728 and batch: 50, loss is 5.059430379867553 and perplexity is 157.5007751542013
At time: 861.4566421508789 and batch: 100, loss is 5.002929849624634 and perplexity is 148.8486249542912
At time: 863.0344591140747 and batch: 150, loss is 4.999686775207519 and perplexity is 148.3666797012457
At time: 864.6223247051239 and batch: 200, loss is 5.039505376815796 and perplexity is 154.39362946778596
At time: 866.209299325943 and batch: 250, loss is 5.036673803329467 and perplexity is 153.95707092561045
At time: 867.7912974357605 and batch: 300, loss is 5.006672401428222 and perplexity is 149.40674238432456
At time: 869.3721694946289 and batch: 350, loss is 4.971646795272827 and perplexity is 144.26426557120487
At time: 870.9521079063416 and batch: 400, loss is 4.999590816497803 and perplexity is 148.35244330916055
At time: 872.5311741828918 and batch: 450, loss is 5.030807323455811 and perplexity is 153.0565289560101
At time: 874.1172738075256 and batch: 500, loss is 5.037853689193725 and perplexity is 154.13882990363658
At time: 875.7053685188293 and batch: 550, loss is 5.013087863922119 and perplexity is 150.36833697532333
At time: 877.2984440326691 and batch: 600, loss is 5.036909704208374 and perplexity is 153.99339381808838
At time: 878.8892688751221 and batch: 650, loss is 5.039601287841797 and perplexity is 154.40843822934667
At time: 880.4828882217407 and batch: 700, loss is 5.0138262748718265 and perplexity is 150.47941160615002
At time: 882.0862967967987 and batch: 750, loss is 4.99867733001709 and perplexity is 148.21698723588204
At time: 883.7334430217743 and batch: 800, loss is 5.005828533172608 and perplexity is 149.28071595958
At time: 885.3281052112579 and batch: 850, loss is 5.029489316940308 and perplexity is 152.85493233561886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.995159149169922 and perplexity of 147.69644928041367
Finished 30 epochs...
Completing Train Step...
At time: 889.4748957157135 and batch: 50, loss is 5.056840982437134 and perplexity is 157.09347061597094
At time: 891.0476622581482 and batch: 100, loss is 5.000063362121582 and perplexity is 148.42256317313587
At time: 892.6218979358673 and batch: 150, loss is 4.997300214767456 and perplexity is 148.01301584079917
At time: 894.1951200962067 and batch: 200, loss is 5.037614507675171 and perplexity is 154.1019671528521
At time: 895.7739186286926 and batch: 250, loss is 5.034435567855835 and perplexity is 153.61286409970984
At time: 897.3497903347015 and batch: 300, loss is 5.0045254516601565 and perplexity is 149.08631770434155
At time: 898.9239354133606 and batch: 350, loss is 4.969337396621704 and perplexity is 143.93148627881553
At time: 900.5037252902985 and batch: 400, loss is 4.997558536529541 and perplexity is 148.05125576275205
At time: 902.0880148410797 and batch: 450, loss is 5.029099311828613 and perplexity is 152.79532975407147
At time: 903.6817262172699 and batch: 500, loss is 5.036282968521118 and perplexity is 153.89691090038303
At time: 905.2793979644775 and batch: 550, loss is 5.011698637008667 and perplexity is 150.1595862692889
At time: 906.8902366161346 and batch: 600, loss is 5.035760679244995 and perplexity is 153.81655318100553
At time: 908.4925906658173 and batch: 650, loss is 5.038656072616577 and perplexity is 154.26255797760857
At time: 910.091878414154 and batch: 700, loss is 5.01280179977417 and perplexity is 150.32532813706456
At time: 911.7027134895325 and batch: 750, loss is 4.997490482330322 and perplexity is 148.04118059593034
At time: 913.3129813671112 and batch: 800, loss is 5.004959306716919 and perplexity is 149.15101359047551
At time: 914.9118633270264 and batch: 850, loss is 5.02845383644104 and perplexity is 152.6967359527383
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.994506200154622 and perplexity of 147.6000425070718
Finished 31 epochs...
Completing Train Step...
At time: 919.0843575000763 and batch: 50, loss is 5.0543879127502445 and perplexity is 156.70858165791998
At time: 920.69335770607 and batch: 100, loss is 4.997583684921264 and perplexity is 148.05497906054433
At time: 922.2818927764893 and batch: 150, loss is 4.995775690078736 and perplexity is 147.7875382606375
At time: 923.8934824466705 and batch: 200, loss is 5.036026735305786 and perplexity is 153.8574824517281
At time: 925.4879834651947 and batch: 250, loss is 5.032559061050415 and perplexity is 153.32487880251816
At time: 927.0656764507294 and batch: 300, loss is 5.002525663375854 and perplexity is 148.7884745437382
At time: 928.6442716121674 and batch: 350, loss is 4.967582931518555 and perplexity is 143.67918490059517
At time: 930.2229747772217 and batch: 400, loss is 4.995995531082153 and perplexity is 147.82003159289383
At time: 931.7994434833527 and batch: 450, loss is 5.027825746536255 and perplexity is 152.60085878728458
At time: 933.3865270614624 and batch: 500, loss is 5.0350241947174075 and perplexity is 153.70331137503845
At time: 934.9816267490387 and batch: 550, loss is 5.01056056022644 and perplexity is 149.9887903384036
At time: 936.5739185810089 and batch: 600, loss is 5.034642343521118 and perplexity is 153.6446307860558
At time: 938.1588530540466 and batch: 650, loss is 5.037665967941284 and perplexity is 154.1098974851371
At time: 939.7449426651001 and batch: 700, loss is 5.01185562133789 and perplexity is 150.18316082158503
At time: 941.3295207023621 and batch: 750, loss is 4.996694202423096 and perplexity is 147.9233452995454
At time: 942.9118576049805 and batch: 800, loss is 5.004186897277832 and perplexity is 149.03585242124154
At time: 944.5157189369202 and batch: 850, loss is 5.02709734916687 and perplexity is 152.48974519551473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.993931134541829 and perplexity of 147.5151871992025
Finished 32 epochs...
Completing Train Step...
At time: 948.6355679035187 and batch: 50, loss is 5.052279634475708 and perplexity is 156.37854438722778
At time: 950.2416224479675 and batch: 100, loss is 4.996121959686279 and perplexity is 147.8387214545917
At time: 951.8189413547516 and batch: 150, loss is 4.994320507049561 and perplexity is 147.5726367414811
At time: 953.3966329097748 and batch: 200, loss is 5.034469003677368 and perplexity is 153.6180003578862
At time: 954.9717001914978 and batch: 250, loss is 5.031044149398804 and perplexity is 153.09278100535482
At time: 956.558153629303 and batch: 300, loss is 5.000282773971557 and perplexity is 148.45513241520788
At time: 958.1463642120361 and batch: 350, loss is 4.965614328384399 and perplexity is 143.39661583132025
At time: 959.729504108429 and batch: 400, loss is 4.994333400726318 and perplexity is 147.57453950762425
At time: 961.3102750778198 and batch: 450, loss is 5.026512184143066 and perplexity is 152.40053963267073
At time: 962.9400613307953 and batch: 500, loss is 5.033900737762451 and perplexity is 153.5307292833021
At time: 964.534749507904 and batch: 550, loss is 5.009652099609375 and perplexity is 149.852593303568
At time: 966.1316998004913 and batch: 600, loss is 5.033559656143188 and perplexity is 153.47837170316322
At time: 967.7378532886505 and batch: 650, loss is 5.036642932891846 and perplexity is 153.95231827681465
At time: 969.343709230423 and batch: 700, loss is 5.011228160858154 and perplexity is 150.08895638132006
At time: 970.9388298988342 and batch: 750, loss is 4.995734205245972 and perplexity is 147.78140744649698
At time: 972.5344934463501 and batch: 800, loss is 5.003170747756958 and perplexity is 148.88448662937353
At time: 974.1307933330536 and batch: 850, loss is 5.025864839553833 and perplexity is 152.3019158931594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.993366241455078 and perplexity of 147.43188042168953
Finished 33 epochs...
Completing Train Step...
At time: 978.2423968315125 and batch: 50, loss is 5.050178813934326 and perplexity is 156.05036597162587
At time: 979.8505983352661 and batch: 100, loss is 4.993438549041748 and perplexity is 147.44254125058583
At time: 981.4315969944 and batch: 150, loss is 4.992409362792968 and perplexity is 147.29087347522747
At time: 983.0135667324066 and batch: 200, loss is 5.032912092208862 and perplexity is 153.37901681774142
At time: 984.5932166576385 and batch: 250, loss is 5.029342346191406 and perplexity is 152.83246878252322
At time: 986.1717331409454 and batch: 300, loss is 4.998609132766724 and perplexity is 148.2068795895558
At time: 987.7624278068542 and batch: 350, loss is 4.964154558181763 and perplexity is 143.18744243406152
At time: 989.3524043560028 and batch: 400, loss is 4.992958869934082 and perplexity is 147.3718331039328
At time: 990.9386637210846 and batch: 450, loss is 5.025688505172729 and perplexity is 152.275062196764
At time: 992.5247056484222 and batch: 500, loss is 5.032441682815552 and perplexity is 153.30688285507063
At time: 994.1053898334503 and batch: 550, loss is 5.008482389450073 and perplexity is 149.67741167862022
At time: 995.6878833770752 and batch: 600, loss is 5.0325954246521 and perplexity is 153.33045434871212
At time: 997.2706472873688 and batch: 650, loss is 5.035251369476319 and perplexity is 153.73823285423305
At time: 998.8495264053345 and batch: 700, loss is 5.009963350296021 and perplexity is 149.89924228551658
At time: 1000.4277119636536 and batch: 750, loss is 4.994619941711425 and perplexity is 147.61683172048834
At time: 1002.0193452835083 and batch: 800, loss is 5.001884078979492 and perplexity is 148.69304479650427
At time: 1003.6644921302795 and batch: 850, loss is 5.0242994594573975 and perplexity is 152.06369200947987
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.992629051208496 and perplexity of 147.32323512845295
Finished 34 epochs...
Completing Train Step...
At time: 1007.8043406009674 and batch: 50, loss is 5.048043556213379 and perplexity is 155.7175137119722
At time: 1009.3814961910248 and batch: 100, loss is 4.991147098541259 and perplexity is 147.10507076176836
At time: 1010.9667229652405 and batch: 150, loss is 4.990869827270508 and perplexity is 147.06428840601336
At time: 1012.5425863265991 and batch: 200, loss is 5.031143388748169 and perplexity is 153.10797458722237
At time: 1014.1313149929047 and batch: 250, loss is 5.027511949539185 and perplexity is 152.55298060845402
At time: 1015.7220902442932 and batch: 300, loss is 4.996943311691284 and perplexity is 147.9601989659453
At time: 1017.3060419559479 and batch: 350, loss is 4.963211660385132 and perplexity is 143.05249494093016
At time: 1018.8911719322205 and batch: 400, loss is 4.992036972045899 and perplexity is 147.23603392832112
At time: 1020.4724249839783 and batch: 450, loss is 5.024514169692993 and perplexity is 152.0963451459726
At time: 1022.0662922859192 and batch: 500, loss is 5.030699758529663 and perplexity is 153.04006632719316
At time: 1023.6614980697632 and batch: 550, loss is 5.00730806350708 and perplexity is 149.50174477629395
At time: 1025.2490239143372 and batch: 600, loss is 5.031217384338379 and perplexity is 153.1193043213378
At time: 1026.8412511348724 and batch: 650, loss is 5.033833999633789 and perplexity is 153.52048327164118
At time: 1028.438190460205 and batch: 700, loss is 5.0086955738067624 and perplexity is 149.7093239628189
At time: 1030.0354571342468 and batch: 750, loss is 4.993283243179321 and perplexity is 147.4196443376169
At time: 1031.641939163208 and batch: 800, loss is 5.000630197525024 and perplexity is 148.50671818537884
At time: 1033.2570190429688 and batch: 850, loss is 5.02290225982666 and perplexity is 151.85137703289857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.991964975992839 and perplexity of 147.22543389659492
Finished 35 epochs...
Completing Train Step...
At time: 1037.416384935379 and batch: 50, loss is 5.045867700576782 and perplexity is 155.3790632252732
At time: 1038.9958045482635 and batch: 100, loss is 4.989133424758911 and perplexity is 146.80914718429432
At time: 1040.5728678703308 and batch: 150, loss is 4.989013748168945 and perplexity is 146.7915786174775
At time: 1042.1501615047455 and batch: 200, loss is 5.029380168914795 and perplexity is 152.83824943203408
At time: 1043.7723922729492 and batch: 250, loss is 5.0263799285888675 and perplexity is 152.38038514764204
At time: 1045.360647201538 and batch: 300, loss is 4.995626554489136 and perplexity is 147.7654995224045
At time: 1046.9434125423431 and batch: 350, loss is 4.961256380081177 and perplexity is 142.77306049050512
At time: 1048.5400528907776 and batch: 400, loss is 4.9906400680542 and perplexity is 147.0305029117752
At time: 1050.1273002624512 and batch: 450, loss is 5.02364652633667 and perplexity is 151.96443699547595
At time: 1051.7054891586304 and batch: 500, loss is 5.030019788742066 and perplexity is 152.93603907750034
At time: 1053.2878353595734 and batch: 550, loss is 5.006402578353882 and perplexity is 149.36643443601903
At time: 1054.8672144412994 and batch: 600, loss is 5.030040321350097 and perplexity is 152.93917928548282
At time: 1056.44428896904 and batch: 650, loss is 5.032474088668823 and perplexity is 153.31185097591953
At time: 1058.026816368103 and batch: 700, loss is 5.007246809005737 and perplexity is 149.49258740193594
At time: 1059.6209466457367 and batch: 750, loss is 4.99218189239502 and perplexity is 147.2573729719547
At time: 1061.2245626449585 and batch: 800, loss is 4.999467029571533 and perplexity is 148.33408035276904
At time: 1062.8254075050354 and batch: 850, loss is 5.021791801452637 and perplexity is 151.68284599033746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.991320927937825 and perplexity of 147.13064417009133
Finished 36 epochs...
Completing Train Step...
At time: 1067.0189666748047 and batch: 50, loss is 5.0437641620635985 and perplexity is 155.0525609071312
At time: 1068.590879201889 and batch: 100, loss is 4.986812410354614 and perplexity is 146.46879617159072
At time: 1070.1623260974884 and batch: 150, loss is 4.98687702178955 and perplexity is 146.47826003641845
At time: 1071.7347943782806 and batch: 200, loss is 5.02785361289978 and perplexity is 152.60511127754023
At time: 1073.306667804718 and batch: 250, loss is 5.025009765625 and perplexity is 152.17174215758294
At time: 1074.8790709972382 and batch: 300, loss is 4.993968715667725 and perplexity is 147.52073109019634
At time: 1076.4517228603363 and batch: 350, loss is 4.959640779495239 and perplexity is 142.54258248065656
At time: 1078.024307012558 and batch: 400, loss is 4.9891869068145756 and perplexity is 146.8169990492411
At time: 1079.6097416877747 and batch: 450, loss is 5.022470970153808 and perplexity is 151.7858992231425
At time: 1081.201431274414 and batch: 500, loss is 5.02932541847229 and perplexity is 152.82988169931662
At time: 1082.7928173542023 and batch: 550, loss is 5.005398187637329 and perplexity is 149.2164874911722
At time: 1084.42560505867 and batch: 600, loss is 5.028989849090576 and perplexity is 152.77860527428763
At time: 1086.0221636295319 and batch: 650, loss is 5.031312961578369 and perplexity is 153.1339397412294
At time: 1087.6174049377441 and batch: 700, loss is 5.0060451793670655 and perplexity is 149.3130605621396
At time: 1089.210505247116 and batch: 750, loss is 4.991058702468872 and perplexity is 147.09206782599747
At time: 1090.8079161643982 and batch: 800, loss is 4.998322772979736 and perplexity is 148.16444517513133
At time: 1092.4001483917236 and batch: 850, loss is 5.020622453689575 and perplexity is 151.50557965687815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.990872701009114 and perplexity of 147.0647110309438
Finished 37 epochs...
Completing Train Step...
At time: 1096.4760501384735 and batch: 50, loss is 5.041527585983276 and perplexity is 154.7061615768405
At time: 1098.0769982337952 and batch: 100, loss is 4.984361305236816 and perplexity is 146.1102253825668
At time: 1099.6552608013153 and batch: 150, loss is 4.985381860733032 and perplexity is 146.2594150913741
At time: 1101.228844165802 and batch: 200, loss is 5.026235218048096 and perplexity is 152.35833569513693
At time: 1102.8022603988647 and batch: 250, loss is 5.023396320343018 and perplexity is 151.92641933883888
At time: 1104.3754062652588 and batch: 300, loss is 4.9922499847412105 and perplexity is 147.2674004133664
At time: 1105.9494142532349 and batch: 350, loss is 4.958404998779297 and perplexity is 142.36653990344112
At time: 1107.5234966278076 and batch: 400, loss is 4.9876070308685305 and perplexity is 146.5852295357206
At time: 1109.0979206562042 and batch: 450, loss is 5.021153621673584 and perplexity is 151.58607594682397
At time: 1110.6722087860107 and batch: 500, loss is 5.027956771850586 and perplexity is 152.6208546727293
At time: 1112.2462034225464 and batch: 550, loss is 5.004381217956543 and perplexity is 149.06481598325465
At time: 1113.823161125183 and batch: 600, loss is 5.028217782974243 and perplexity is 152.6606956127391
At time: 1115.4039070606232 and batch: 650, loss is 5.030330352783203 and perplexity is 152.9835428879376
At time: 1116.998616218567 and batch: 700, loss is 5.004581298828125 and perplexity is 149.0946439854656
At time: 1118.5939552783966 and batch: 750, loss is 4.989756011962891 and perplexity is 146.90057713936514
At time: 1120.1986935138702 and batch: 800, loss is 4.997139329910278 and perplexity is 147.98920470336225
At time: 1121.8113932609558 and batch: 850, loss is 5.01952205657959 and perplexity is 151.33895504830943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9906005859375 and perplexity of 147.02469795089766
Finished 38 epochs...
Completing Train Step...
At time: 1125.8816998004913 and batch: 50, loss is 5.039815626144409 and perplexity is 154.44153741899112
At time: 1127.4781124591827 and batch: 100, loss is 4.982671298980713 and perplexity is 145.8635067243491
At time: 1129.0514860153198 and batch: 150, loss is 4.9837684822082515 and perplexity is 146.02363354560475
At time: 1130.6333239078522 and batch: 200, loss is 5.024891748428344 and perplexity is 152.15378433485014
At time: 1132.2165648937225 and batch: 250, loss is 5.022338609695435 and perplexity is 151.76581010147876
At time: 1133.789909362793 and batch: 300, loss is 4.990803518295288 and perplexity is 147.05453704706258
At time: 1135.3652744293213 and batch: 350, loss is 4.956601638793945 and perplexity is 142.1100331386596
At time: 1136.9538595676422 and batch: 400, loss is 4.985793790817261 and perplexity is 146.31967615537243
At time: 1138.5498368740082 and batch: 450, loss is 5.019886045455933 and perplexity is 151.39405077101117
At time: 1140.1480395793915 and batch: 500, loss is 5.026831760406494 and perplexity is 152.44925101075637
At time: 1141.744292974472 and batch: 550, loss is 5.003296327590943 and perplexity is 148.90318469251264
At time: 1143.3496372699738 and batch: 600, loss is 5.026664953231812 and perplexity is 152.42382350271725
At time: 1144.9580173492432 and batch: 650, loss is 5.028957624435424 and perplexity is 152.77368211574213
At time: 1146.56737947464 and batch: 700, loss is 5.003498935699463 and perplexity is 148.93335674156356
At time: 1148.170568227768 and batch: 750, loss is 4.98863094329834 and perplexity is 146.7353968402339
At time: 1149.7698695659637 and batch: 800, loss is 4.995963258743286 and perplexity is 147.81526117171967
At time: 1151.3768844604492 and batch: 850, loss is 5.018543834686279 and perplexity is 151.19098435505592
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.990058898925781 and perplexity of 146.9450781480692
Finished 39 epochs...
Completing Train Step...
At time: 1155.5685658454895 and batch: 50, loss is 5.037903261184693 and perplexity is 154.1464710617124
At time: 1157.158509016037 and batch: 100, loss is 4.980786561965942 and perplexity is 145.588851282064
At time: 1158.7361941337585 and batch: 150, loss is 4.9820912647247315 and perplexity is 145.77892542614796
At time: 1160.3306548595428 and batch: 200, loss is 5.023480596542359 and perplexity is 151.93922365958238
At time: 1161.9115896224976 and batch: 250, loss is 5.020888242721558 and perplexity is 151.54585353017492
At time: 1163.499588727951 and batch: 300, loss is 4.989236030578613 and perplexity is 146.82421143000732
At time: 1165.121372461319 and batch: 350, loss is 4.954945020675659 and perplexity is 141.8748059775565
At time: 1166.7093136310577 and batch: 400, loss is 4.984627847671509 and perplexity is 146.14917514842588
At time: 1168.2980275154114 and batch: 450, loss is 5.01868860244751 and perplexity is 151.21287351976366
At time: 1169.8835656642914 and batch: 500, loss is 5.025731439590454 and perplexity is 152.28160017824467
At time: 1171.4698314666748 and batch: 550, loss is 5.00188871383667 and perplexity is 148.69373396912738
At time: 1173.0545544624329 and batch: 600, loss is 5.025404977798462 and perplexity is 152.231894168161
At time: 1174.6348371505737 and batch: 650, loss is 5.027884750366211 and perplexity is 152.60986308804914
At time: 1176.2163622379303 and batch: 700, loss is 5.002600231170654 and perplexity is 148.79956978584536
At time: 1177.8025653362274 and batch: 750, loss is 4.987697696685791 and perplexity is 146.59852040785952
At time: 1179.3890602588654 and batch: 800, loss is 4.994764194488526 and perplexity is 147.6381273943539
At time: 1180.974939107895 and batch: 850, loss is 5.0172992706298825 and perplexity is 151.0029345345732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.98948605855306 and perplexity of 146.86092617985685
Finished 40 epochs...
Completing Train Step...
At time: 1185.1347253322601 and batch: 50, loss is 5.036110582351685 and perplexity is 153.87038348797049
At time: 1186.7239327430725 and batch: 100, loss is 4.97894814491272 and perplexity is 145.3214441333539
At time: 1188.3258166313171 and batch: 150, loss is 4.980643472671509 and perplexity is 145.56802056641843
At time: 1189.920057296753 and batch: 200, loss is 5.022176790237427 and perplexity is 151.74125342727137
At time: 1191.5080490112305 and batch: 250, loss is 5.019628496170044 and perplexity is 151.35506436202363
At time: 1193.0985713005066 and batch: 300, loss is 4.98757122039795 and perplexity is 146.57998034365932
At time: 1194.6827557086945 and batch: 350, loss is 4.953856124877929 and perplexity is 141.72040317706038
At time: 1196.2822527885437 and batch: 400, loss is 4.983974151611328 and perplexity is 146.05366922775565
At time: 1197.887387752533 and batch: 450, loss is 5.017712535858155 and perplexity is 151.06535169331573
At time: 1199.499342918396 and batch: 500, loss is 5.0251837158203125 and perplexity is 152.19821476424315
At time: 1201.1139159202576 and batch: 550, loss is 5.000935020446778 and perplexity is 148.55199333721762
At time: 1202.7225425243378 and batch: 600, loss is 5.024007949829102 and perplexity is 152.01937043954095
At time: 1204.353420972824 and batch: 650, loss is 5.026552429199219 and perplexity is 152.40667312436645
At time: 1205.9470875263214 and batch: 700, loss is 5.0013143348693845 and perplexity is 148.60835193893692
At time: 1207.539627790451 and batch: 750, loss is 4.986387300491333 and perplexity is 146.406544074603
At time: 1209.137797832489 and batch: 800, loss is 4.9933296585083005 and perplexity is 147.42648702770848
At time: 1210.7387254238129 and batch: 850, loss is 5.01595025062561 and perplexity is 150.79936589513775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.989351272583008 and perplexity of 146.8411327214292
Finished 41 epochs...
Completing Train Step...
At time: 1214.9082162380219 and batch: 50, loss is 5.034799470901489 and perplexity is 153.66877446116584
At time: 1216.4934515953064 and batch: 100, loss is 4.977629928588867 and perplexity is 145.13000524015035
At time: 1218.0766639709473 and batch: 150, loss is 4.979219722747803 and perplexity is 145.3609155760846
At time: 1219.6636266708374 and batch: 200, loss is 5.021088047027588 and perplexity is 151.5761360694614
At time: 1221.2458190917969 and batch: 250, loss is 5.018555669784546 and perplexity is 151.1927737258015
At time: 1222.8366296291351 and batch: 300, loss is 4.986052103042603 and perplexity is 146.35747719855706
At time: 1224.4293315410614 and batch: 350, loss is 4.95153429031372 and perplexity is 141.39173355241007
At time: 1226.0187406539917 and batch: 400, loss is 4.98168384552002 and perplexity is 145.71954438959582
At time: 1227.6098301410675 and batch: 450, loss is 5.015454845428467 and perplexity is 150.72467760755785
At time: 1229.1909306049347 and batch: 500, loss is 5.02330906867981 and perplexity is 151.9131640843462
At time: 1230.7711763381958 and batch: 550, loss is 4.998909311294556 and perplexity is 148.25137479040336
At time: 1232.3533380031586 and batch: 600, loss is 5.022034521102905 and perplexity is 151.7196668660622
At time: 1233.937347650528 and batch: 650, loss is 5.025182905197144 and perplexity is 152.19809138889403
At time: 1235.5328459739685 and batch: 700, loss is 5.000110845565796 and perplexity is 148.42961095495954
At time: 1237.1263318061829 and batch: 750, loss is 4.985235137939453 and perplexity is 146.2379570756332
At time: 1238.7322463989258 and batch: 800, loss is 4.99192852973938 and perplexity is 147.2200681788963
At time: 1240.3335831165314 and batch: 850, loss is 5.0147905349731445 and perplexity is 150.62458287901646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.988795280456543 and perplexity of 146.75951289988654
Finished 42 epochs...
Completing Train Step...
At time: 1244.497991323471 and batch: 50, loss is 5.033095026016236 and perplexity is 153.4070775918129
At time: 1246.1227271556854 and batch: 100, loss is 4.975527620315551 and perplexity is 144.82521772034562
At time: 1247.7135169506073 and batch: 150, loss is 4.977684831619262 and perplexity is 145.13797353597906
At time: 1249.3043456077576 and batch: 200, loss is 5.019591245651245 and perplexity is 151.34942641236228
At time: 1250.8956167697906 and batch: 250, loss is 5.01716160774231 and perplexity is 150.98214846534117
At time: 1252.4781897068024 and batch: 300, loss is 4.983990039825439 and perplexity is 146.05598977815876
At time: 1254.0607721805573 and batch: 350, loss is 4.949864044189453 and perplexity is 141.15577166950658
At time: 1255.6471993923187 and batch: 400, loss is 4.9800899219512935 and perplexity is 145.48746358203894
At time: 1257.2350273132324 and batch: 450, loss is 5.014224700927734 and perplexity is 150.5393784699988
At time: 1258.8326716423035 and batch: 500, loss is 5.021624698638916 and perplexity is 151.6575014776079
At time: 1260.4368617534637 and batch: 550, loss is 4.996796941757202 and perplexity is 147.9385436262598
At time: 1262.043837070465 and batch: 600, loss is 5.0205152797698975 and perplexity is 151.48934308013764
At time: 1263.6505188941956 and batch: 650, loss is 5.024115142822265 and perplexity is 152.03566672428536
At time: 1265.2538843154907 and batch: 700, loss is 4.999001111984253 and perplexity is 148.26498499356111
At time: 1266.8538272380829 and batch: 750, loss is 4.984065256118774 and perplexity is 146.06697598149483
At time: 1268.4589984416962 and batch: 800, loss is 4.9904618835449215 and perplexity is 147.00430668771492
At time: 1270.0517256259918 and batch: 850, loss is 5.013389301300049 and perplexity is 150.41367044480265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.988423029581706 and perplexity of 146.70489170984254
Finished 43 epochs...
Completing Train Step...
At time: 1274.1597309112549 and batch: 50, loss is 5.031352872848511 and perplexity is 153.14005163323222
At time: 1275.7750477790833 and batch: 100, loss is 4.973742198944092 and perplexity is 144.56687437599294
At time: 1277.351457118988 and batch: 150, loss is 4.976292524337769 and perplexity is 144.9360374897389
At time: 1278.934520483017 and batch: 200, loss is 5.018273267745972 and perplexity is 151.15008260659687
At time: 1280.5146503448486 and batch: 250, loss is 5.015724391937256 and perplexity is 150.76531039416216
At time: 1282.089925289154 and batch: 300, loss is 4.982411680221557 and perplexity is 145.8256427370406
At time: 1283.6637647151947 and batch: 350, loss is 4.947901773452759 and perplexity is 140.87905741237944
At time: 1285.2701234817505 and batch: 400, loss is 4.978419017791748 and perplexity is 145.24457095563932
At time: 1286.8476853370667 and batch: 450, loss is 5.012736539840699 and perplexity is 150.3155182362515
At time: 1288.4290652275085 and batch: 500, loss is 5.0202578353881835 and perplexity is 151.45034801961708
At time: 1290.0047438144684 and batch: 550, loss is 4.995333051681518 and perplexity is 147.72213629735106
At time: 1291.5838360786438 and batch: 600, loss is 5.019248580932617 and perplexity is 151.2975731883887
At time: 1293.165671825409 and batch: 650, loss is 5.023228397369385 and perplexity is 151.90090954463028
At time: 1294.743578672409 and batch: 700, loss is 4.997918624877929 and perplexity is 148.1045768944994
At time: 1296.3338627815247 and batch: 750, loss is 4.982767105102539 and perplexity is 145.87748201069263
At time: 1297.919826745987 and batch: 800, loss is 4.9891456031799315 and perplexity is 146.81093509878528
At time: 1299.5163226127625 and batch: 850, loss is 5.012137842178345 and perplexity is 150.2255516209564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.987875938415527 and perplexity of 146.6246527105781
Finished 44 epochs...
Completing Train Step...
At time: 1303.6935820579529 and batch: 50, loss is 5.029491176605225 and perplexity is 152.85521659483825
At time: 1305.3044312000275 and batch: 100, loss is 4.971982431411743 and perplexity is 144.31269399899543
At time: 1306.8834807872772 and batch: 150, loss is 4.974878377914429 and perplexity is 144.73122156469643
At time: 1308.4609062671661 and batch: 200, loss is 5.0170863914489745 and perplexity is 150.9707925748532
At time: 1310.0447187423706 and batch: 250, loss is 5.013294439315796 and perplexity is 150.39940258231508
At time: 1311.6260402202606 and batch: 300, loss is 4.979569063186646 and perplexity is 145.41170489298722
At time: 1313.2023541927338 and batch: 350, loss is 4.9461408042907715 and perplexity is 140.63119204239567
At time: 1314.7786717414856 and batch: 400, loss is 4.977189636230468 and perplexity is 145.06611967306426
At time: 1316.353458404541 and batch: 450, loss is 5.011426401138306 and perplexity is 150.11871300746424
At time: 1317.9281771183014 and batch: 500, loss is 5.018873062133789 and perplexity is 151.2407687716701
At time: 1319.519385099411 and batch: 550, loss is 4.9938528442382815 and perplexity is 147.50363864249468
At time: 1321.1223635673523 and batch: 600, loss is 5.019095802307129 and perplexity is 151.2744599187691
At time: 1322.7255551815033 and batch: 650, loss is 5.022887353897095 and perplexity is 151.84911356383773
At time: 1324.319593667984 and batch: 700, loss is 4.996336469650268 and perplexity is 147.87043773501458
At time: 1325.966314315796 and batch: 750, loss is 4.981373682022094 and perplexity is 145.674354514479
At time: 1327.5756559371948 and batch: 800, loss is 4.987950029373169 and perplexity is 146.6355166739671
At time: 1329.1835594177246 and batch: 850, loss is 5.011012926101684 and perplexity is 150.0566554976056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.987474759419759 and perplexity of 146.56584177729312
Finished 45 epochs...
Completing Train Step...
At time: 1333.3218395709991 and batch: 50, loss is 5.027753553390503 and perplexity is 152.5898424489011
At time: 1334.8949463367462 and batch: 100, loss is 4.969987869262695 and perplexity is 144.02514022918115
At time: 1336.4697930812836 and batch: 150, loss is 4.973318796157837 and perplexity is 144.5056773149993
At time: 1338.0463607311249 and batch: 200, loss is 5.0154664421081545 and perplexity is 150.7264255235001
At time: 1339.6221845149994 and batch: 250, loss is 5.011814651489257 and perplexity is 150.17700796626062
At time: 1341.1974534988403 and batch: 300, loss is 4.978638515472412 and perplexity is 145.27645530123507
At time: 1342.7760984897614 and batch: 350, loss is 4.944965200424194 and perplexity is 140.46596261047833
At time: 1344.3480315208435 and batch: 400, loss is 4.976332378387451 and perplexity is 144.94181389288357
At time: 1345.9212667942047 and batch: 450, loss is 5.010808801651001 and perplexity is 150.0260283912077
At time: 1347.4950442314148 and batch: 500, loss is 5.018659906387329 and perplexity is 151.20853436830365
At time: 1349.0747299194336 and batch: 550, loss is 4.993660736083984 and perplexity is 147.4753047123996
At time: 1350.675406217575 and batch: 600, loss is 5.018037948608399 and perplexity is 151.1145182841607
At time: 1352.2701013088226 and batch: 650, loss is 5.0220949172973635 and perplexity is 151.72883043328488
At time: 1353.872262954712 and batch: 700, loss is 4.9957568645477295 and perplexity is 147.7847561079415
At time: 1355.4688971042633 and batch: 750, loss is 4.9805711650848385 and perplexity is 145.5574952746887
At time: 1357.0619611740112 and batch: 800, loss is 4.986928968429566 and perplexity is 146.48586928749842
At time: 1358.6523666381836 and batch: 850, loss is 5.009690570831299 and perplexity is 149.85835842683576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.986669222513835 and perplexity of 146.44782512236088
Finished 46 epochs...
Completing Train Step...
At time: 1362.7563145160675 and batch: 50, loss is 5.026124649047851 and perplexity is 152.34149051757456
At time: 1364.3285763263702 and batch: 100, loss is 4.968627500534057 and perplexity is 143.82934613854954
At time: 1365.9308071136475 and batch: 150, loss is 4.971752424240112 and perplexity is 144.27950486143436
At time: 1367.5188047885895 and batch: 200, loss is 5.014167604446411 and perplexity is 150.5307834465627
At time: 1369.0973534584045 and batch: 250, loss is 5.010148115158081 and perplexity is 149.9269409571323
At time: 1370.6720416545868 and batch: 300, loss is 4.976220827102662 and perplexity is 144.92564634909598
At time: 1372.2452540397644 and batch: 350, loss is 4.942476224899292 and perplexity is 140.11678099996513
At time: 1373.8243036270142 and batch: 400, loss is 4.973385362625122 and perplexity is 144.51529686760705
At time: 1375.410415649414 and batch: 450, loss is 5.008554668426513 and perplexity is 149.68823059971817
At time: 1377.004005908966 and batch: 500, loss is 5.015680484771728 and perplexity is 150.7586908620464
At time: 1378.6055700778961 and batch: 550, loss is 4.990483932495117 and perplexity is 147.0075480140854
At time: 1380.2010004520416 and batch: 600, loss is 5.016238012313843 and perplexity is 150.84276641945675
At time: 1381.8096299171448 and batch: 650, loss is 5.020250406265259 and perplexity is 151.44922288054406
At time: 1383.4122445583344 and batch: 700, loss is 4.993328065872192 and perplexity is 147.42625223114888
At time: 1385.0109453201294 and batch: 750, loss is 4.978265380859375 and perplexity is 145.22225773942472
At time: 1386.60014295578 and batch: 800, loss is 4.985150136947632 and perplexity is 146.22552723252187
At time: 1388.1824398040771 and batch: 850, loss is 5.007797822952271 and perplexity is 149.57498260086493
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.985934257507324 and perplexity of 146.34023063954953
Finished 47 epochs...
Completing Train Step...
At time: 1392.4286510944366 and batch: 50, loss is 5.024243431091309 and perplexity is 152.05517236794816
At time: 1394.0092463493347 and batch: 100, loss is 4.966796817779541 and perplexity is 143.5662811027622
At time: 1395.5889027118683 and batch: 150, loss is 4.970320177078247 and perplexity is 144.07300886203518
At time: 1397.171813249588 and batch: 200, loss is 5.012765836715698 and perplexity is 150.31992207570883
At time: 1398.7492861747742 and batch: 250, loss is 5.0084285640716555 and perplexity is 149.66935545211274
At time: 1400.33180975914 and batch: 300, loss is 4.974720935821534 and perplexity is 144.7084365719715
At time: 1401.90909075737 and batch: 350, loss is 4.9411945629119876 and perplexity is 139.93731368076143
At time: 1403.4994173049927 and batch: 400, loss is 4.972445554733277 and perplexity is 144.37954405189407
At time: 1405.0773568153381 and batch: 450, loss is 5.007661323547364 and perplexity is 149.5545670981346
At time: 1406.6843848228455 and batch: 500, loss is 5.0150752449035645 and perplexity is 150.667473298902
At time: 1408.2628479003906 and batch: 550, loss is 4.989879159927368 and perplexity is 146.9186687603707
At time: 1409.8484077453613 and batch: 600, loss is 5.014580545425415 and perplexity is 150.5929566116897
At time: 1411.4260325431824 and batch: 650, loss is 5.018205413818359 and perplexity is 151.1398268277844
At time: 1413.005631685257 and batch: 700, loss is 4.991748781204223 and perplexity is 147.19360796545902
At time: 1414.5998356342316 and batch: 750, loss is 4.976511669158936 and perplexity is 144.96780295224642
At time: 1416.2045261859894 and batch: 800, loss is 4.983943977355957 and perplexity is 146.04926223353175
At time: 1417.801961183548 and batch: 850, loss is 5.006153364181518 and perplexity is 149.3292148417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9854841232299805 and perplexity of 146.27437270915223
Finished 48 epochs...
Completing Train Step...
At time: 1422.0436923503876 and batch: 50, loss is 5.022040615081787 and perplexity is 151.72059144532517
At time: 1423.6517219543457 and batch: 100, loss is 4.964706954956054 and perplexity is 143.26656056566964
At time: 1425.232095003128 and batch: 150, loss is 4.96869722366333 and perplexity is 143.83937472025156
At time: 1426.810423374176 and batch: 200, loss is 5.010892305374146 and perplexity is 150.0385566462177
At time: 1428.3902759552002 and batch: 250, loss is 5.006182708740234 and perplexity is 149.33359690590754
At time: 1429.9705991744995 and batch: 300, loss is 4.972056312561035 and perplexity is 144.3233563805551
At time: 1431.5518217086792 and batch: 350, loss is 4.938257074356079 and perplexity is 139.5268525808216
At time: 1433.1339087486267 and batch: 400, loss is 4.969508399963379 and perplexity is 143.95610114849399
At time: 1434.7224202156067 and batch: 450, loss is 5.004840965270996 and perplexity is 149.13336388823262
At time: 1436.317137002945 and batch: 500, loss is 5.011497802734375 and perplexity is 150.12943210584865
At time: 1437.9238951206207 and batch: 550, loss is 4.986459865570068 and perplexity is 146.41716846247624
At time: 1439.5196723937988 and batch: 600, loss is 5.012125883102417 and perplexity is 150.2237550729208
At time: 1441.120012998581 and batch: 650, loss is 5.016151838302612 and perplexity is 150.82976825326935
At time: 1442.7209177017212 and batch: 700, loss is 4.990199213027954 and perplexity is 146.9656980613767
At time: 1444.3306095600128 and batch: 750, loss is 4.974790763854981 and perplexity is 144.71854163032458
At time: 1445.9572184085846 and batch: 800, loss is 4.982435827255249 and perplexity is 145.82916403626317
At time: 1447.5633943080902 and batch: 850, loss is 5.003560018539429 and perplexity is 148.94245429180825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.985134124755859 and perplexity of 146.22318586008308
Finished 49 epochs...
Completing Train Step...
At time: 1451.6859974861145 and batch: 50, loss is 5.019937410354614 and perplexity is 151.4018273108088
At time: 1453.2989659309387 and batch: 100, loss is 4.962695722579956 and perplexity is 142.97870778709333
At time: 1454.8770694732666 and batch: 150, loss is 4.966592035293579 and perplexity is 143.53688425290034
At time: 1456.4508814811707 and batch: 200, loss is 5.009205751419067 and perplexity is 149.78572179483268
At time: 1458.0353317260742 and batch: 250, loss is 5.004089193344116 and perplexity is 149.0212917435246
At time: 1459.617451429367 and batch: 300, loss is 4.970307035446167 and perplexity is 144.07111552000083
At time: 1461.2024686336517 and batch: 350, loss is 4.936779766082764 and perplexity is 139.32088058668003
At time: 1462.7930436134338 and batch: 400, loss is 4.967957363128662 and perplexity is 143.73299300223485
At time: 1464.3838119506836 and batch: 450, loss is 5.003838024139404 and perplexity is 148.98386688437503
At time: 1465.981407880783 and batch: 500, loss is 5.010340576171875 and perplexity is 149.95579882510208
At time: 1467.5689787864685 and batch: 550, loss is 4.985154781341553 and perplexity is 146.2262063630487
At time: 1469.1546697616577 and batch: 600, loss is 5.010387287139893 and perplexity is 149.962803569223
At time: 1470.7447581291199 and batch: 650, loss is 5.014457521438598 and perplexity is 150.5744312053389
At time: 1472.3305389881134 and batch: 700, loss is 4.988801975250244 and perplexity is 146.76049542783795
At time: 1473.923048734665 and batch: 750, loss is 4.972761745452881 and perplexity is 144.4252027418706
At time: 1475.5096774101257 and batch: 800, loss is 4.9801859855651855 and perplexity is 145.50144030488312
At time: 1477.093472957611 and batch: 850, loss is 5.001525774002075 and perplexity is 148.63977688209573
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.985585848490397 and perplexity of 146.28925326465978
Annealing...
Finished Training.
Improved accuracyfrom -10000000 to -146.22318586008308
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd131070898>
SETTINGS FOR THIS RUN
{'seq_len': 50, 'lr': 15.321074118726964, 'dropout': 0.8450289434344702, 'wordvec_source': 'glove', 'num_layers': 1, 'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 7.572970340928041, 'batch_size': 50, 'data': 'wikitext'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.0412497520446777 and batch: 50, loss is 7.392348318099976 and perplexity is 1623.5141676272287
At time: 3.599562883377075 and batch: 100, loss is 6.692315597534179 and perplexity is 806.1868968178522
At time: 5.16245436668396 and batch: 150, loss is 6.583746175765992 and perplexity is 723.2436585065261
At time: 6.721298933029175 and batch: 200, loss is 6.590333213806153 and perplexity is 728.0234169402997
At time: 8.303818225860596 and batch: 250, loss is 6.591370544433594 and perplexity is 728.7790097602183
At time: 9.86119294166565 and batch: 300, loss is 6.53718789100647 and perplexity is 690.3425274464912
At time: 11.418837308883667 and batch: 350, loss is 6.526581087112427 and perplexity is 683.0588959501279
At time: 12.976116418838501 and batch: 400, loss is 6.5560848903656 and perplexity is 703.51196931797
At time: 14.54604172706604 and batch: 450, loss is 6.562492218017578 and perplexity is 708.0340728411512
At time: 16.121702671051025 and batch: 500, loss is 6.56662727355957 and perplexity is 710.9678946356344
At time: 17.694883108139038 and batch: 550, loss is 6.523041734695434 and perplexity is 680.6455830974378
At time: 19.277289390563965 and batch: 600, loss is 6.556195755004882 and perplexity is 703.5899682422607
At time: 20.852932691574097 and batch: 650, loss is 6.590068368911743 and perplexity is 727.830629185866
At time: 22.426119089126587 and batch: 700, loss is 6.57386794090271 and perplexity is 716.1344587660495
At time: 24.00019860267639 and batch: 750, loss is 6.5580290412902835 and perplexity is 704.8810331657949
At time: 25.573546648025513 and batch: 800, loss is 6.601846160888672 and perplexity is 736.4535467172158
At time: 27.154850482940674 and batch: 850, loss is 6.596923427581787 and perplexity is 732.837091040443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.114073435465495 and perplexity of 452.1768821470471
Finished 1 epochs...
Completing Train Step...
At time: 31.287181615829468 and batch: 50, loss is 6.554633712768554 and perplexity is 702.4917889193465
At time: 32.855963468551636 and batch: 100, loss is 6.625412673950195 and perplexity is 754.0153108358982
At time: 34.42747116088867 and batch: 150, loss is 6.677480211257935 and perplexity is 794.3150820088366
At time: 35.997873067855835 and batch: 200, loss is 6.73185396194458 and perplexity is 838.7007447275302
At time: 37.573644399642944 and batch: 250, loss is 6.773221778869629 and perplexity is 874.1235951472871
At time: 39.153484582901 and batch: 300, loss is 6.7608213710784915 and perplexity is 863.3510362106039
At time: 40.736570835113525 and batch: 350, loss is 6.730800943374634 and perplexity is 837.8180421013981
At time: 42.316370725631714 and batch: 400, loss is 6.724687261581421 and perplexity is 832.7115149380403
At time: 43.89405846595764 and batch: 450, loss is 6.742301321029663 and perplexity is 847.5088833509177
At time: 45.47252535820007 and batch: 500, loss is 6.715599699020386 and perplexity is 825.1784772684591
At time: 47.04585289955139 and batch: 550, loss is 6.647191753387451 and perplexity is 770.6172009051425
At time: 48.614911794662476 and batch: 600, loss is 6.684622449874878 and perplexity is 800.008577811547
At time: 50.18328857421875 and batch: 650, loss is 6.718701066970826 and perplexity is 827.7416319396556
At time: 51.75075554847717 and batch: 700, loss is 6.6715813159942625 and perplexity is 789.643293276921
At time: 53.32930660247803 and batch: 750, loss is 6.709957933425903 and perplexity is 820.5361215810514
At time: 54.94237542152405 and batch: 800, loss is 6.7372394371032716 and perplexity is 843.2297311795451
At time: 56.521665811538696 and batch: 850, loss is 6.730138988494873 and perplexity is 837.2636278789219
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.236340204874675 and perplexity of 510.9849843174873
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 60.86328482627869 and batch: 50, loss is 6.697414608001709 and perplexity is 810.3081504719249
At time: 62.435112714767456 and batch: 100, loss is 6.588284206390381 and perplexity is 726.5332187950336
At time: 64.01045060157776 and batch: 150, loss is 6.51296706199646 and perplexity is 673.8227283472994
At time: 65.58446502685547 and batch: 200, loss is 6.512856063842773 and perplexity is 673.7479394193338
At time: 67.16273593902588 and batch: 250, loss is 6.560552301406861 and perplexity is 706.6618771851953
At time: 68.7323088645935 and batch: 300, loss is 6.552543134689331 and perplexity is 701.0247090414385
At time: 70.2997534275055 and batch: 350, loss is 6.480338096618652 and perplexity is 652.1914127109447
At time: 71.86971116065979 and batch: 400, loss is 6.435932445526123 and perplexity is 623.8640309616957
At time: 73.44053220748901 and batch: 450, loss is 6.433812694549561 and perplexity is 622.5429952009001
At time: 75.01561903953552 and batch: 500, loss is 6.424127416610718 and perplexity is 616.5425979265093
At time: 76.58944821357727 and batch: 550, loss is 6.360095987319946 and perplexity is 578.3018633756859
At time: 78.15818858146667 and batch: 600, loss is 6.327269468307495 and perplexity is 559.6264280228819
At time: 79.73217940330505 and batch: 650, loss is 6.313704919815064 and perplexity is 552.0866009854824
At time: 81.3037919998169 and batch: 700, loss is 6.256430768966675 and perplexity is 521.354779618504
At time: 82.8847119808197 and batch: 750, loss is 6.229177389144898 and perplexity is 507.33797007241327
At time: 84.46266889572144 and batch: 800, loss is 6.210074405670166 and perplexity is 497.7382844596737
At time: 86.03569388389587 and batch: 850, loss is 6.215725145339966 and perplexity is 500.5588355235273
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.81459108988444 and perplexity of 335.1543224843944
Finished 3 epochs...
Completing Train Step...
At time: 90.22150611877441 and batch: 50, loss is 6.342451801300049 and perplexity is 568.1876883075802
At time: 91.7981767654419 and batch: 100, loss is 6.296978015899658 and perplexity is 542.9287065900459
At time: 93.408118724823 and batch: 150, loss is 6.257115240097046 and perplexity is 521.7117540692498
At time: 94.98076152801514 and batch: 200, loss is 6.289846868515014 and perplexity is 539.0707740557737
At time: 96.55487132072449 and batch: 250, loss is 6.346132793426514 and perplexity is 570.2830368293547
At time: 98.126944065094 and batch: 300, loss is 6.352314090728759 and perplexity is 573.8190431207338
At time: 99.70236587524414 and batch: 350, loss is 6.279549026489258 and perplexity is 533.5479935498801
At time: 101.2759919166565 and batch: 400, loss is 6.248180532455445 and perplexity is 517.071174057332
At time: 102.8523633480072 and batch: 450, loss is 6.254468173980713 and perplexity is 520.3325747572937
At time: 104.4276123046875 and batch: 500, loss is 6.25767370223999 and perplexity is 522.0031917042951
At time: 106.00031876564026 and batch: 550, loss is 6.212036981582641 and perplexity is 498.716092825191
At time: 107.57133269309998 and batch: 600, loss is 6.195723552703857 and perplexity is 490.64632507045957
At time: 109.14386558532715 and batch: 650, loss is 6.194605588912964 and perplexity is 490.098106746113
At time: 110.7153000831604 and batch: 700, loss is 6.148695487976074 and perplexity is 468.10633796245673
At time: 112.2947895526886 and batch: 750, loss is 6.137624025344849 and perplexity is 462.9523001500085
At time: 113.8711154460907 and batch: 800, loss is 6.148762788772583 and perplexity is 468.1378429519961
At time: 115.45711588859558 and batch: 850, loss is 6.153636264801025 and perplexity is 470.42486986649936
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.755826950073242 and perplexity of 316.02677786419537
Finished 4 epochs...
Completing Train Step...
At time: 119.63729047775269 and batch: 50, loss is 6.239926872253418 and perplexity is 512.821008126873
At time: 121.24045372009277 and batch: 100, loss is 6.184041395187378 and perplexity is 484.94786736326745
At time: 122.82705211639404 and batch: 150, loss is 6.147902536392212 and perplexity is 467.73529942759205
At time: 124.42293334007263 and batch: 200, loss is 6.183002090454101 and perplexity is 484.4441205678826
At time: 126.01445412635803 and batch: 250, loss is 6.233362674713135 and perplexity is 509.46577398428565
At time: 127.60613203048706 and batch: 300, loss is 6.226240396499634 and perplexity is 505.850108174888
At time: 129.19766497612 and batch: 350, loss is 6.165956220626831 and perplexity is 476.2563313981298
At time: 130.78002500534058 and batch: 400, loss is 6.1517244243621825 and perplexity is 469.52635176215836
At time: 132.38944005966187 and batch: 450, loss is 6.163999309539795 and perplexity is 475.3252514204424
At time: 133.96401071548462 and batch: 500, loss is 6.169751653671264 and perplexity is 478.06736507041956
At time: 135.53900146484375 and batch: 550, loss is 6.124504413604736 and perplexity is 456.91821469778705
At time: 137.11592984199524 and batch: 600, loss is 6.118735141754151 and perplexity is 454.28971884794555
At time: 138.69096565246582 and batch: 650, loss is 6.122171030044556 and perplexity is 455.85329216680776
At time: 140.27182054519653 and batch: 700, loss is 6.08200156211853 and perplexity is 437.90481180790147
At time: 141.85143375396729 and batch: 750, loss is 6.074509687423706 and perplexity is 434.63634259821407
At time: 143.42897152900696 and batch: 800, loss is 6.074714117050171 and perplexity is 434.72520422604447
At time: 145.00723814964294 and batch: 850, loss is 6.069877195358276 and perplexity is 432.6275496408855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.666287740071614 and perplexity of 288.9598468058721
Finished 5 epochs...
Completing Train Step...
At time: 149.1473388671875 and batch: 50, loss is 6.119367141723632 and perplexity is 454.57692068260906
At time: 150.71824264526367 and batch: 100, loss is 6.063246421813965 and perplexity is 429.7683840457542
At time: 152.29414796829224 and batch: 150, loss is 6.024087896347046 and perplexity is 413.2645299791063
At time: 153.870215177536 and batch: 200, loss is 6.059135179519654 and perplexity is 428.00512915613643
At time: 155.44554615020752 and batch: 250, loss is 6.103242845535278 and perplexity is 447.30596482842424
At time: 157.0138657093048 and batch: 300, loss is 6.078850975036621 and perplexity is 436.5273256491955
At time: 158.58279061317444 and batch: 350, loss is 6.0319366359710695 and perplexity is 416.5208981469489
At time: 160.15207505226135 and batch: 400, loss is 6.033878183364868 and perplexity is 417.33037877942957
At time: 161.72201442718506 and batch: 450, loss is 6.047013416290283 and perplexity is 422.84827057327516
At time: 163.29169249534607 and batch: 500, loss is 6.052996425628662 and perplexity is 425.38575906409255
At time: 164.85914492607117 and batch: 550, loss is 6.0088894653320315 and perplexity is 407.03104709912026
At time: 166.42765140533447 and batch: 600, loss is 6.001086206436157 and perplexity is 403.86723852255653
At time: 167.99864602088928 and batch: 650, loss is 6.0084646129608155 and perplexity is 406.8581557228578
At time: 169.57037043571472 and batch: 700, loss is 5.9641474342346195 and perplexity is 389.2210500319709
At time: 171.14368510246277 and batch: 750, loss is 5.9657252979278566 and perplexity is 389.8356725632321
At time: 172.73843145370483 and batch: 800, loss is 5.985450429916382 and perplexity is 397.60157256437344
At time: 174.30653858184814 and batch: 850, loss is 5.9915588855743405 and perplexity is 400.0377371665486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.5999298095703125 and perplexity of 270.40742674655536
Finished 6 epochs...
Completing Train Step...
At time: 178.35429525375366 and batch: 50, loss is 6.021318979263306 and perplexity is 412.1218175300429
At time: 179.961772441864 and batch: 100, loss is 5.969657793045044 and perplexity is 391.37171770759136
At time: 181.5322618484497 and batch: 150, loss is 5.932456703186035 and perplexity is 377.0797498671138
At time: 183.10386967658997 and batch: 200, loss is 5.960426816940307 and perplexity is 387.7755981160579
At time: 184.67494559288025 and batch: 250, loss is 6.0049663925170895 and perplexity is 405.43736277719387
At time: 186.2450532913208 and batch: 300, loss is 5.9781166934967045 and perplexity is 394.6963335808024
At time: 187.82103276252747 and batch: 350, loss is 5.94054479598999 and perplexity is 380.14197295730287
At time: 189.39718580245972 and batch: 400, loss is 5.947996463775635 and perplexity is 382.9852450541026
At time: 190.96809220314026 and batch: 450, loss is 5.965418739318848 and perplexity is 389.71618339786096
At time: 192.54632091522217 and batch: 500, loss is 5.9775269508361815 and perplexity is 394.46363293843007
At time: 194.11578392982483 and batch: 550, loss is 5.936680288314819 and perplexity is 378.67574633194806
At time: 195.68379759788513 and batch: 600, loss is 5.930994348526001 and perplexity is 376.5287285301968
At time: 197.25489401817322 and batch: 650, loss is 5.9414697360992434 and perplexity is 380.4937441738767
At time: 198.84124445915222 and batch: 700, loss is 5.902538108825683 and perplexity is 365.96514946464015
At time: 200.4364972114563 and batch: 750, loss is 5.908994693756103 and perplexity is 368.33567906145515
At time: 202.0319094657898 and batch: 800, loss is 5.930915784835816 and perplexity is 376.4991482058076
At time: 203.62774300575256 and batch: 850, loss is 5.93834698677063 and perplexity is 379.30741086453884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.559956232706706 and perplexity of 259.81146482952056
Finished 7 epochs...
Completing Train Step...
At time: 207.75459551811218 and batch: 50, loss is 5.958591871261596 and perplexity is 387.06470338406024
At time: 209.36074018478394 and batch: 100, loss is 5.9125294017791745 and perplexity is 369.6399418775651
At time: 210.93746089935303 and batch: 150, loss is 5.8751694869995115 and perplexity is 356.08500733611146
At time: 212.54210090637207 and batch: 200, loss is 5.910773096084594 and perplexity is 368.99131090656635
At time: 214.12413501739502 and batch: 250, loss is 5.953938083648682 and perplexity is 385.2675714407267
At time: 215.70341420173645 and batch: 300, loss is 5.927071914672852 and perplexity is 375.0547122542682
At time: 217.2932369709015 and batch: 350, loss is 5.894054317474366 and perplexity is 362.87351044820053
At time: 218.88462686538696 and batch: 400, loss is 5.912832450866699 and perplexity is 369.75197790001357
At time: 220.48005843162537 and batch: 450, loss is 5.929214010238647 and perplexity is 375.8589763881008
At time: 222.08348202705383 and batch: 500, loss is 5.939789791107177 and perplexity is 379.8550722309111
At time: 223.67387199401855 and batch: 550, loss is 5.906328601837158 and perplexity is 367.35497019495244
At time: 225.2640619277954 and batch: 600, loss is 5.903768033981323 and perplexity is 366.4155361222185
At time: 226.8559000492096 and batch: 650, loss is 5.9163963508605955 and perplexity is 371.07208794461616
At time: 228.45197987556458 and batch: 700, loss is 5.876431827545166 and perplexity is 356.5347917093693
At time: 230.04401779174805 and batch: 750, loss is 5.884646692276001 and perplexity is 359.4757400024072
At time: 231.63715600967407 and batch: 800, loss is 5.908156070709229 and perplexity is 368.02691375895336
At time: 233.23488688468933 and batch: 850, loss is 5.9148204898834225 and perplexity is 370.487790428251
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.540273666381836 and perplexity of 254.74770580290615
Finished 8 epochs...
Completing Train Step...
At time: 237.3660523891449 and batch: 50, loss is 5.924616250991821 and perplexity is 374.13483393713386
At time: 238.96507692337036 and batch: 100, loss is 5.880025978088379 and perplexity is 357.81853702969335
At time: 240.5509352684021 and batch: 150, loss is 5.848703260421753 and perplexity is 346.78439973161665
At time: 242.13361167907715 and batch: 200, loss is 5.889125833511352 and perplexity is 361.08949403064435
At time: 243.70985651016235 and batch: 250, loss is 5.930779056549072 and perplexity is 376.4476736414078
At time: 245.30406999588013 and batch: 300, loss is 5.8965592861175535 and perplexity is 363.7836366562129
At time: 246.90194010734558 and batch: 350, loss is 5.865194206237793 and perplexity is 352.55061698868553
At time: 248.50553917884827 and batch: 400, loss is 5.889628868103028 and perplexity is 361.27118023022274
At time: 250.10417532920837 and batch: 450, loss is 5.9040555572509765 and perplexity is 366.52090426238635
At time: 251.6971731185913 and batch: 500, loss is 5.920588665008545 and perplexity is 372.63100415883883
At time: 253.3230059146881 and batch: 550, loss is 5.887469635009766 and perplexity is 360.4919531114673
At time: 254.9167516231537 and batch: 600, loss is 5.886714487075806 and perplexity is 360.2198311169783
At time: 256.5095884799957 and batch: 650, loss is 5.902089204788208 and perplexity is 365.80090309965647
At time: 258.1006338596344 and batch: 700, loss is 5.862417631149292 and perplexity is 351.5730914426675
At time: 259.6929259300232 and batch: 750, loss is 5.869513244628906 and perplexity is 354.0765896333528
At time: 261.28730964660645 and batch: 800, loss is 5.892872743606567 and perplexity is 362.44500179810416
At time: 262.8923671245575 and batch: 850, loss is 5.900257358551025 and perplexity is 365.1314254690063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.527589162190755 and perplexity of 251.53676510867794
Finished 9 epochs...
Completing Train Step...
At time: 267.0111577510834 and batch: 50, loss is 5.902816572189331 and perplexity is 366.06707154126144
At time: 268.5837528705597 and batch: 100, loss is 5.859998741149902 and perplexity is 350.7237025110684
At time: 270.1518280506134 and batch: 150, loss is 5.830367002487183 and perplexity is 340.4836144779024
At time: 271.7269148826599 and batch: 200, loss is 5.874736042022705 and perplexity is 355.93069752318803
At time: 273.31265664100647 and batch: 250, loss is 5.912042226791382 and perplexity is 369.4599064013444
At time: 274.9021201133728 and batch: 300, loss is 5.875876932144165 and perplexity is 356.33700707312056
At time: 276.50263810157776 and batch: 350, loss is 5.844356031417846 and perplexity is 345.280120622925
At time: 278.10795617103577 and batch: 400, loss is 5.874989442825317 and perplexity is 356.0209020760851
At time: 279.7135679721832 and batch: 450, loss is 5.890461959838867 and perplexity is 361.5722776683487
At time: 281.3168866634369 and batch: 500, loss is 5.903071765899658 and perplexity is 366.1605014763354
At time: 282.9210193157196 and batch: 550, loss is 5.874667701721191 and perplexity is 355.90637394315115
At time: 284.53780937194824 and batch: 600, loss is 5.874193601608276 and perplexity is 355.73767868345817
At time: 286.1440222263336 and batch: 650, loss is 5.888619546890259 and perplexity is 360.9067255213287
At time: 287.7495160102844 and batch: 700, loss is 5.8525666332244874 and perplexity is 348.1267484771515
At time: 289.359037399292 and batch: 750, loss is 5.855969848632813 and perplexity is 349.31351706076725
At time: 290.96436762809753 and batch: 800, loss is 5.881257667541504 and perplexity is 358.25952987525267
At time: 292.61394023895264 and batch: 850, loss is 5.88758017539978 and perplexity is 360.5318042350999
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.5223948160807295 and perplexity of 250.2335836093021
Finished 10 epochs...
Completing Train Step...
At time: 296.726487159729 and batch: 50, loss is 5.88813289642334 and perplexity is 360.731132824439
At time: 298.29946541786194 and batch: 100, loss is 5.8427284240722654 and perplexity is 344.71859725605555
At time: 299.88002610206604 and batch: 150, loss is 5.815875473022461 and perplexity is 335.58506560521454
At time: 301.4915382862091 and batch: 200, loss is 5.860272102355957 and perplexity is 350.8195898707237
At time: 303.095267534256 and batch: 250, loss is 5.898003816604614 and perplexity is 364.3095129407752
At time: 304.69867062568665 and batch: 300, loss is 5.855718536376953 and perplexity is 349.2257413228137
At time: 306.30205249786377 and batch: 350, loss is 5.824423761367798 and perplexity is 338.4660396654013
At time: 307.90679931640625 and batch: 400, loss is 5.856291055679321 and perplexity is 349.42573704584794
At time: 309.52796626091003 and batch: 450, loss is 5.87384150505066 and perplexity is 355.6124467195447
At time: 311.1423623561859 and batch: 500, loss is 5.889978799819946 and perplexity is 361.3976225964008
At time: 312.757164478302 and batch: 550, loss is 5.863138475418091 and perplexity is 351.82661225423215
At time: 314.37573075294495 and batch: 600, loss is 5.863211212158203 and perplexity is 351.85220390580747
At time: 315.9923963546753 and batch: 650, loss is 5.87594988822937 and perplexity is 356.36300497451134
At time: 317.6026005744934 and batch: 700, loss is 5.841349649429321 and perplexity is 344.2436355030609
At time: 319.2065966129303 and batch: 750, loss is 5.843451433181762 and perplexity is 344.96792206327706
At time: 320.8123686313629 and batch: 800, loss is 5.869318332672119 and perplexity is 354.00758259778155
At time: 322.41710329055786 and batch: 850, loss is 5.875611562728881 and perplexity is 356.2424586755909
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.513049443562825 and perplexity of 247.90595079234726
Finished 11 epochs...
Completing Train Step...
At time: 326.583021402359 and batch: 50, loss is 5.8740953540802 and perplexity is 355.70273005272054
At time: 328.1611144542694 and batch: 100, loss is 5.835621023178101 and perplexity is 342.27723015420366
At time: 329.7457330226898 and batch: 150, loss is 5.807886180877685 and perplexity is 332.9146600059572
At time: 331.3500626087189 and batch: 200, loss is 5.848851928710937 and perplexity is 346.83595940759
At time: 332.9596652984619 and batch: 250, loss is 5.892067098617554 and perplexity is 362.1531173920049
At time: 334.5923795700073 and batch: 300, loss is 5.851928691864014 and perplexity is 347.90473484897825
At time: 336.2000970840454 and batch: 350, loss is 5.823296489715577 and perplexity is 338.0847114642634
At time: 337.8071777820587 and batch: 400, loss is 5.8531738376617435 and perplexity is 348.33819677322947
At time: 339.41432929039 and batch: 450, loss is 5.873550481796265 and perplexity is 355.50897028575173
At time: 341.03168392181396 and batch: 500, loss is 5.888751525878906 and perplexity is 360.95436076931577
At time: 342.6505584716797 and batch: 550, loss is 5.860182914733887 and perplexity is 350.7883025009714
At time: 344.26965713500977 and batch: 600, loss is 5.861945734024048 and perplexity is 351.40722425070624
At time: 345.8945417404175 and batch: 650, loss is 5.870780410766602 and perplexity is 354.5255478901625
At time: 347.52598214149475 and batch: 700, loss is 5.826388998031616 and perplexity is 339.1318595685602
At time: 349.1527051925659 and batch: 750, loss is 5.8288601207733155 and perplexity is 339.970932318072
At time: 350.7710757255554 and batch: 800, loss is 5.8467935276031495 and perplexity is 346.1227661552327
At time: 352.390127658844 and batch: 850, loss is 5.848721771240235 and perplexity is 346.7908190541056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.488731384277344 and perplexity of 241.95007041665826
Finished 12 epochs...
Completing Train Step...
At time: 356.514057636261 and batch: 50, loss is 5.83185791015625 and perplexity is 340.9916227125055
At time: 358.1066265106201 and batch: 100, loss is 5.787657566070557 and perplexity is 326.2479144318956
At time: 359.7060458660126 and batch: 150, loss is 5.763394289016723 and perplexity is 318.4273310517684
At time: 361.3170278072357 and batch: 200, loss is 5.80316725730896 and perplexity is 331.3473620585055
At time: 362.9238214492798 and batch: 250, loss is 5.841152381896973 and perplexity is 344.1757341081491
At time: 364.53029584884644 and batch: 300, loss is 5.794407472610474 and perplexity is 328.4575062429565
At time: 366.1379613876343 and batch: 350, loss is 5.757478685379028 and perplexity is 316.5492017850103
At time: 367.75195360183716 and batch: 400, loss is 5.783136053085327 and perplexity is 324.776110148949
At time: 369.3738303184509 and batch: 450, loss is 5.7988491630554195 and perplexity is 329.9196576180516
At time: 370.9956605434418 and batch: 500, loss is 5.813885946273803 and perplexity is 334.9180738597104
At time: 372.62054777145386 and batch: 550, loss is 5.785369510650635 and perplexity is 325.50229445824135
At time: 374.2669105529785 and batch: 600, loss is 5.788421792984009 and perplexity is 326.49733716420496
At time: 375.88935947418213 and batch: 650, loss is 5.8018631172180175 and perplexity is 330.91552033187696
At time: 377.5100564956665 and batch: 700, loss is 5.764788341522217 and perplexity is 318.87154502730175
At time: 379.13091373443604 and batch: 750, loss is 5.766435461044312 and perplexity is 319.39719736142604
At time: 380.7523937225342 and batch: 800, loss is 5.788311157226563 and perplexity is 326.4612168821379
At time: 382.38388228416443 and batch: 850, loss is 5.79392412185669 and perplexity is 328.298784421974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.449765523274739 and perplexity of 232.70359593308683
Finished 13 epochs...
Completing Train Step...
At time: 386.56894183158875 and batch: 50, loss is 5.782424335479736 and perplexity is 324.54504351033177
At time: 388.17188239097595 and batch: 100, loss is 5.740125761032105 and perplexity is 311.10353322274415
At time: 389.7661964893341 and batch: 150, loss is 5.718380098342895 and perplexity is 304.4114067986915
At time: 391.37838411331177 and batch: 200, loss is 5.759763822555542 and perplexity is 317.2733872506378
At time: 392.9971969127655 and batch: 250, loss is 5.793577127456665 and perplexity is 328.18488634438495
At time: 394.62624311447144 and batch: 300, loss is 5.748178567886352 and perplexity is 313.6189041927683
At time: 396.2472770214081 and batch: 350, loss is 5.725829753875733 and perplexity is 306.6876349503841
At time: 397.8646981716156 and batch: 400, loss is 5.753509864807129 and perplexity is 315.2953645742273
At time: 399.4810211658478 and batch: 450, loss is 5.766598148345947 and perplexity is 319.44916345661204
At time: 401.09695529937744 and batch: 500, loss is 5.781755056381225 and perplexity is 324.3279049674345
At time: 402.7119381427765 and batch: 550, loss is 5.7529435729980465 and perplexity is 315.1168659377552
At time: 404.32910418510437 and batch: 600, loss is 5.758156824111938 and perplexity is 316.76393886214794
At time: 405.945924282074 and batch: 650, loss is 5.766742553710937 and perplexity is 319.49529696053935
At time: 407.5611946582794 and batch: 700, loss is 5.7276378345489505 and perplexity is 307.24265254294073
At time: 409.17718982696533 and batch: 750, loss is 5.7276387119293215 and perplexity is 307.2429221117315
At time: 410.79271125793457 and batch: 800, loss is 5.746901035308838 and perplexity is 313.21850164384557
At time: 412.40942645072937 and batch: 850, loss is 5.752229223251343 and perplexity is 314.89184266661783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.413167317708333 and perplexity of 224.3410228786002
Finished 14 epochs...
Completing Train Step...
At time: 416.53006315231323 and batch: 50, loss is 5.742094783782959 and perplexity is 311.7167066356223
At time: 418.1319417953491 and batch: 100, loss is 5.688569946289062 and perplexity is 295.47077928523
At time: 419.732177734375 and batch: 150, loss is 5.664656467437744 and perplexity is 288.4888587748769
At time: 421.3385272026062 and batch: 200, loss is 5.706027555465698 and perplexity is 300.6742808671469
At time: 422.94904947280884 and batch: 250, loss is 5.7374840831756595 and perplexity is 310.2827824648173
At time: 424.5666642189026 and batch: 300, loss is 5.686184988021851 and perplexity is 294.76693346253546
At time: 426.18349719047546 and batch: 350, loss is 5.662796392440796 and perplexity is 287.9527466207518
At time: 427.80257296562195 and batch: 400, loss is 5.691901111602784 and perplexity is 296.45668248799285
At time: 429.4207510948181 and batch: 450, loss is 5.703749084472657 and perplexity is 299.9899831121705
At time: 431.03979444503784 and batch: 500, loss is 5.721919183731079 and perplexity is 305.49065340696524
At time: 432.65956568717957 and batch: 550, loss is 5.698104028701782 and perplexity is 298.30129378392166
At time: 434.28759932518005 and batch: 600, loss is 5.7019277191162105 and perplexity is 299.44408903683876
At time: 435.9132571220398 and batch: 650, loss is 5.714060974121094 and perplexity is 303.0994514068751
At time: 437.53903317451477 and batch: 700, loss is 5.675829563140869 and perplexity is 291.73024685220486
At time: 439.1574385166168 and batch: 750, loss is 5.679512882232666 and perplexity is 292.80676380072737
At time: 440.77673530578613 and batch: 800, loss is 5.701110363006592 and perplexity is 299.1994365788864
At time: 442.3953700065613 and batch: 850, loss is 5.706486673355102 and perplexity is 300.8123575026781
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.376272837320964 and perplexity of 216.21490363711177
Finished 15 epochs...
Completing Train Step...
At time: 446.58689880371094 and batch: 50, loss is 5.697259178161621 and perplexity is 298.04938020438357
At time: 448.1674406528473 and batch: 100, loss is 5.647516241073609 and perplexity is 283.5862305377555
At time: 449.7549641132355 and batch: 150, loss is 5.629736490249634 and perplexity is 278.5886971219212
At time: 451.3500008583069 and batch: 200, loss is 5.6749653816223145 and perplexity is 291.4782478665877
At time: 452.9639880657196 and batch: 250, loss is 5.711319351196289 and perplexity is 302.26960508504703
At time: 454.5737679004669 and batch: 300, loss is 5.657481174468995 and perplexity is 286.42627536437357
At time: 456.21052837371826 and batch: 350, loss is 5.633693628311157 and perplexity is 279.6932951408414
At time: 457.81993794441223 and batch: 400, loss is 5.661942691802978 and perplexity is 287.7070260781159
At time: 459.4325108528137 and batch: 450, loss is 5.679889831542969 and perplexity is 292.91715791357916
At time: 461.0595052242279 and batch: 500, loss is 5.697736825942993 and perplexity is 298.19177683458736
At time: 462.6891858577728 and batch: 550, loss is 5.677117700576782 and perplexity is 292.10627764203645
At time: 464.3155152797699 and batch: 600, loss is 5.679938917160034 and perplexity is 292.9315362859071
At time: 465.9407720565796 and batch: 650, loss is 5.69568320274353 and perplexity is 297.58003164584295
At time: 467.5656530857086 and batch: 700, loss is 5.659107761383057 and perplexity is 286.8925517124223
At time: 469.18755555152893 and batch: 750, loss is 5.662723665237427 and perplexity is 287.93180538429567
At time: 470.8097710609436 and batch: 800, loss is 5.683995809555054 and perplexity is 294.12234185953787
At time: 472.43391013145447 and batch: 850, loss is 5.689311208724976 and perplexity is 295.6898818710499
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.364993413289388 and perplexity of 213.78982650458343
Finished 16 epochs...
Completing Train Step...
At time: 476.6254048347473 and batch: 50, loss is 5.6818732738494875 and perplexity is 293.4987187524188
At time: 478.2095239162445 and batch: 100, loss is 5.632780132293701 and perplexity is 279.43791309262224
At time: 479.7960307598114 and batch: 150, loss is 5.614024982452393 and perplexity is 274.2458542755518
At time: 481.3854298591614 and batch: 200, loss is 5.652965755462646 and perplexity is 285.13585630058884
At time: 482.98669362068176 and batch: 250, loss is 5.687469863891602 and perplexity is 295.1459158030355
At time: 484.5918300151825 and batch: 300, loss is 5.642018117904663 and perplexity is 282.0313169833628
At time: 486.19841957092285 and batch: 350, loss is 5.623178329467773 and perplexity is 276.7676455555364
At time: 487.80662870407104 and batch: 400, loss is 5.648766889572143 and perplexity is 283.94111910535236
At time: 489.418573141098 and batch: 450, loss is 5.6685098361968995 and perplexity is 289.60265729066515
At time: 491.02445912361145 and batch: 500, loss is 5.684317293167115 and perplexity is 294.2169125730395
At time: 492.6298325061798 and batch: 550, loss is 5.662572689056397 and perplexity is 287.88833782128756
At time: 494.23432302474976 and batch: 600, loss is 5.663907604217529 and perplexity is 288.2729009506268
At time: 495.8845388889313 and batch: 650, loss is 5.679803094863892 and perplexity is 292.89175235386983
At time: 497.48966217041016 and batch: 700, loss is 5.643612728118897 and perplexity is 282.48140576486924
At time: 499.09979796409607 and batch: 750, loss is 5.6488574123382564 and perplexity is 283.9668234042616
At time: 500.7183446884155 and batch: 800, loss is 5.668870038986206 and perplexity is 289.70699176526824
At time: 502.33417105674744 and batch: 850, loss is 5.674899835586547 and perplexity is 291.45914324905056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.356201807657878 and perplexity of 211.9185086593735
Finished 17 epochs...
Completing Train Step...
At time: 506.5058970451355 and batch: 50, loss is 5.668753490447998 and perplexity is 289.67322880641933
At time: 508.07783818244934 and batch: 100, loss is 5.617268781661988 and perplexity is 275.13689716167704
At time: 509.66570687294006 and batch: 150, loss is 5.597660808563233 and perplexity is 269.7945675749196
At time: 511.2633249759674 and batch: 200, loss is 5.6361215305328365 and perplexity is 280.37318813637904
At time: 512.8669111728668 and batch: 250, loss is 5.668947200775147 and perplexity is 289.7293469374929
At time: 514.4709520339966 and batch: 300, loss is 5.61726523399353 and perplexity is 275.1359210689169
At time: 516.0804800987244 and batch: 350, loss is 5.600047664642334 and perplexity is 270.4392975113385
At time: 517.694343328476 and batch: 400, loss is 5.625141315460205 and perplexity is 277.3114701523258
At time: 519.3148114681244 and batch: 450, loss is 5.643179512023925 and perplexity is 282.3590567770506
At time: 520.9289858341217 and batch: 500, loss is 5.658947992324829 and perplexity is 286.8467188210591
At time: 522.5489017963409 and batch: 550, loss is 5.637982349395752 and perplexity is 280.89539757141705
At time: 524.1645567417145 and batch: 600, loss is 5.639444332122803 and perplexity is 281.30636212911753
At time: 525.7804210186005 and batch: 650, loss is 5.656909084320068 and perplexity is 286.2624605767249
At time: 527.395977973938 and batch: 700, loss is 5.621506299972534 and perplexity is 276.3052685505563
At time: 529.0122230052948 and batch: 750, loss is 5.623242540359497 and perplexity is 276.7854176234318
At time: 530.6311275959015 and batch: 800, loss is 5.641575050354004 and perplexity is 281.90638573707224
At time: 532.2449359893799 and batch: 850, loss is 5.649069986343384 and perplexity is 284.02719378559544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.337446212768555 and perplexity of 207.98089255566984
Finished 18 epochs...
Completing Train Step...
At time: 536.3389112949371 and batch: 50, loss is 5.645070295333863 and perplexity is 282.8934416127737
At time: 537.940670967102 and batch: 100, loss is 5.597669792175293 and perplexity is 269.7969913155377
At time: 539.5304889678955 and batch: 150, loss is 5.570241165161133 and perplexity is 262.49739683438946
At time: 541.1337659358978 and batch: 200, loss is 5.6151448917388915 and perplexity is 274.5531567978948
At time: 542.7307951450348 and batch: 250, loss is 5.660084295272827 and perplexity is 287.17284884944416
At time: 544.3309423923492 and batch: 300, loss is 5.616188249588013 and perplexity is 274.83976347970656
At time: 545.9300217628479 and batch: 350, loss is 5.588158302307129 and perplexity is 267.24298542148586
At time: 547.5366778373718 and batch: 400, loss is 5.608971090316772 and perplexity is 272.86334178655625
At time: 549.1342394351959 and batch: 450, loss is 5.628152322769165 and perplexity is 278.14771535422585
At time: 550.7421340942383 and batch: 500, loss is 5.641937456130981 and perplexity is 282.0085687545697
At time: 552.3516974449158 and batch: 550, loss is 5.619304027557373 and perplexity is 275.6974386286635
At time: 553.9656434059143 and batch: 600, loss is 5.620244197845459 and perplexity is 275.95676305442174
At time: 555.5781135559082 and batch: 650, loss is 5.639204616546631 and perplexity is 281.23893669422523
At time: 557.1930730342865 and batch: 700, loss is 5.603970785140991 and perplexity is 271.5023473358879
At time: 558.8094303607941 and batch: 750, loss is 5.605630159378052 and perplexity is 271.95324533715893
At time: 560.4216158390045 and batch: 800, loss is 5.621959428787232 and perplexity is 276.43049879996386
At time: 562.0393681526184 and batch: 850, loss is 5.631683959960937 and perplexity is 279.1317688076675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.327134768168132 and perplexity of 205.847328074505
Finished 19 epochs...
Completing Train Step...
At time: 566.3706142902374 and batch: 50, loss is 5.625926103591919 and perplexity is 277.5291863222485
At time: 567.9992144107819 and batch: 100, loss is 5.576156711578369 and perplexity is 264.0548143158221
At time: 569.579564332962 and batch: 150, loss is 5.553913917541504 and perplexity is 258.2463353401601
At time: 571.2026033401489 and batch: 200, loss is 5.597054471969605 and perplexity is 269.63103084000056
At time: 572.8324654102325 and batch: 250, loss is 5.632645998001099 and perplexity is 279.40043339953564
At time: 574.4615952968597 and batch: 300, loss is 5.589517259597779 and perplexity is 267.6064041042046
At time: 576.0907323360443 and batch: 350, loss is 5.5676047992706295 and perplexity is 261.80626908414524
At time: 577.7590823173523 and batch: 400, loss is 5.596591863632202 and perplexity is 269.5063261240562
At time: 579.3858740329742 and batch: 450, loss is 5.622476682662964 and perplexity is 276.5735205329228
At time: 581.0198276042938 and batch: 500, loss is 5.628595476150513 and perplexity is 278.27100477084235
At time: 582.668534040451 and batch: 550, loss is 5.610899076461792 and perplexity is 273.3899259894131
At time: 584.3235397338867 and batch: 600, loss is 5.616287870407104 and perplexity is 274.8671446059058
At time: 585.9791004657745 and batch: 650, loss is 5.636095447540283 and perplexity is 280.365875259972
At time: 587.6288511753082 and batch: 700, loss is 5.596941585540772 and perplexity is 269.60059487376253
At time: 589.2548177242279 and batch: 750, loss is 5.6067125034332275 and perplexity is 272.2477516653717
At time: 590.862557888031 and batch: 800, loss is 5.620311288833618 and perplexity is 275.9752778874267
At time: 592.4697711467743 and batch: 850, loss is 5.625380477905273 and perplexity is 277.37780057312966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.321202278137207 and perplexity of 204.62975604076473
Finished 20 epochs...
Completing Train Step...
At time: 596.6952648162842 and batch: 50, loss is 5.6265187835693355 and perplexity is 277.69372106753866
At time: 598.291540145874 and batch: 100, loss is 5.573678188323974 and perplexity is 263.4011587030452
At time: 599.878701210022 and batch: 150, loss is 5.552943143844605 and perplexity is 257.99575823700036
At time: 601.4732074737549 and batch: 200, loss is 5.584046955108643 and perplexity is 266.1465122574832
At time: 603.0632755756378 and batch: 250, loss is 5.621997013092041 and perplexity is 276.44088844333174
At time: 604.6531527042389 and batch: 300, loss is 5.569757509231567 and perplexity is 262.3704691090627
At time: 606.2523293495178 and batch: 350, loss is 5.5591663646697995 and perplexity is 259.60632908357957
At time: 607.8556208610535 and batch: 400, loss is 5.584726009368897 and perplexity is 266.32730155640814
At time: 609.4584703445435 and batch: 450, loss is 5.594030055999756 and perplexity is 268.81678637201145
At time: 611.0620260238647 and batch: 500, loss is 5.604138889312744 and perplexity is 271.54799184952503
At time: 612.6732399463654 and batch: 550, loss is 5.587046327590943 and perplexity is 266.945983138737
At time: 614.2867319583893 and batch: 600, loss is 5.588557004928589 and perplexity is 267.3495571441411
At time: 615.9024198055267 and batch: 650, loss is 5.604785737991333 and perplexity is 271.7236991311112
At time: 617.5172317028046 and batch: 700, loss is 5.569930238723755 and perplexity is 262.41579214115643
At time: 619.1581690311432 and batch: 750, loss is 5.5708607006073 and perplexity is 262.66007366308247
At time: 620.7636787891388 and batch: 800, loss is 5.587691421508789 and perplexity is 267.1182439250723
At time: 622.366096496582 and batch: 850, loss is 5.598236694335937 and perplexity is 269.94998317446004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.299112637837728 and perplexity of 200.15911752034975
Finished 21 epochs...
Completing Train Step...
At time: 626.4764325618744 and batch: 50, loss is 5.593486499786377 and perplexity is 268.6707090417353
At time: 628.0517582893372 and batch: 100, loss is 5.549116258621216 and perplexity is 257.01032485497353
At time: 629.6294555664062 and batch: 150, loss is 5.5229126071929935 and perplexity is 250.3631858854427
At time: 631.2360780239105 and batch: 200, loss is 5.565223360061646 and perplexity is 261.18353516558255
At time: 632.8472185134888 and batch: 250, loss is 5.600874433517456 and perplexity is 270.6629807595629
At time: 634.4523847103119 and batch: 300, loss is 5.548716430664062 and perplexity is 256.9075854822249
At time: 636.0560328960419 and batch: 350, loss is 5.531599931716919 and perplexity is 252.54764695282347
At time: 637.6603169441223 and batch: 400, loss is 5.5581502437591555 and perplexity is 259.34267164066824
At time: 639.2694373130798 and batch: 450, loss is 5.5732275581359865 and perplexity is 263.282488929482
At time: 640.8730654716492 and batch: 500, loss is 5.5885071849823 and perplexity is 267.3362381353426
At time: 642.4777660369873 and batch: 550, loss is 5.572628669738769 and perplexity is 263.1248593076588
At time: 644.0886735916138 and batch: 600, loss is 5.5774149990081785 and perplexity is 264.38728029447157
At time: 645.6952261924744 and batch: 650, loss is 5.595040473937988 and perplexity is 269.08854094478676
At time: 647.2995913028717 and batch: 700, loss is 5.557815408706665 and perplexity is 259.2558491600125
At time: 648.9053485393524 and batch: 750, loss is 5.560220832824707 and perplexity is 259.8802200696937
At time: 650.5214598178864 and batch: 800, loss is 5.577306098937989 and perplexity is 264.3584900687474
At time: 652.1357047557831 and batch: 850, loss is 5.587405405044556 and perplexity is 267.0418546342308
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.2965084711710615 and perplexity of 199.63854793764628
Finished 22 epochs...
Completing Train Step...
At time: 656.2834930419922 and batch: 50, loss is 5.583633623123169 and perplexity is 266.03652812269115
At time: 657.8618764877319 and batch: 100, loss is 5.536574478149414 and perplexity is 253.80708692312754
At time: 659.489152431488 and batch: 150, loss is 5.509904832839966 and perplexity is 247.12760751592361
At time: 661.0806477069855 and batch: 200, loss is 5.556263008117676 and perplexity is 258.8536924619898
At time: 662.6892228126526 and batch: 250, loss is 5.590196933746338 and perplexity is 267.78835108438824
At time: 664.2980995178223 and batch: 300, loss is 5.540682735443116 and perplexity is 254.8519365251105
At time: 665.9067471027374 and batch: 350, loss is 5.523322629928589 and perplexity is 250.46586153204805
At time: 667.5228037834167 and batch: 400, loss is 5.553048267364502 and perplexity is 258.0228810848244
At time: 669.1329078674316 and batch: 450, loss is 5.567353439331055 and perplexity is 261.7404697461733
At time: 670.747793674469 and batch: 500, loss is 5.579657182693482 and perplexity is 264.9807502266157
At time: 672.3623125553131 and batch: 550, loss is 5.56405011177063 and perplexity is 260.87728172010395
At time: 673.970413684845 and batch: 600, loss is 5.571515045166016 and perplexity is 262.83200009650443
At time: 675.5785837173462 and batch: 650, loss is 5.587916259765625 and perplexity is 267.1783090776245
At time: 677.1948299407959 and batch: 700, loss is 5.55001501083374 and perplexity is 257.2414172849394
At time: 678.8107271194458 and batch: 750, loss is 5.555109453201294 and perplexity is 258.5552626730949
At time: 680.4305627346039 and batch: 800, loss is 5.566800918579101 and perplexity is 261.5958926496136
At time: 682.0497677326202 and batch: 850, loss is 5.577784280776978 and perplexity is 264.4849317263238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.291665077209473 and perplexity of 198.6739576311476
Finished 23 epochs...
Completing Train Step...
At time: 686.206449508667 and batch: 50, loss is 5.574457654953003 and perplexity is 263.60655115418325
At time: 687.7794592380524 and batch: 100, loss is 5.526482229232788 and perplexity is 251.2584848204648
At time: 689.3495969772339 and batch: 150, loss is 5.5035531711578365 and perplexity is 245.56291103247221
At time: 690.9365224838257 and batch: 200, loss is 5.551039838790894 and perplexity is 257.50518061404244
At time: 692.5274987220764 and batch: 250, loss is 5.594096908569336 and perplexity is 268.83475806564684
At time: 694.123851776123 and batch: 300, loss is 5.553309421539307 and perplexity is 258.09027363695515
At time: 695.7309987545013 and batch: 350, loss is 5.521851043701172 and perplexity is 250.097550486977
At time: 697.3380961418152 and batch: 400, loss is 5.546605081558227 and perplexity is 256.36573609922766
At time: 698.9483947753906 and batch: 450, loss is 5.5763307762146 and perplexity is 264.1007809214852
At time: 700.5782113075256 and batch: 500, loss is 5.572984399795533 and perplexity is 263.2184773791899
At time: 702.1805672645569 and batch: 550, loss is 5.561687955856323 and perplexity is 260.26177615243773
At time: 703.7831211090088 and batch: 600, loss is 5.576009941101074 and perplexity is 264.016061708632
At time: 705.3862910270691 and batch: 650, loss is 5.582734184265137 and perplexity is 265.7973521098658
At time: 706.9883031845093 and batch: 700, loss is 5.545022821426391 and perplexity is 255.96041955849137
At time: 708.5925042629242 and batch: 750, loss is 5.554038333892822 and perplexity is 258.2784674057802
At time: 710.2023661136627 and batch: 800, loss is 5.570570125579834 and perplexity is 262.58376229258977
At time: 711.8145322799683 and batch: 850, loss is 5.577060308456421 and perplexity is 264.29352125285226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.2870588302612305 and perplexity of 197.76092077110258
Finished 24 epochs...
Completing Train Step...
At time: 715.94895362854 and batch: 50, loss is 5.571288042068481 and perplexity is 262.77234318975883
At time: 717.546149969101 and batch: 100, loss is 5.5216961765289305 and perplexity is 250.05882158554363
At time: 719.1235229969025 and batch: 150, loss is 5.495565128326416 and perplexity is 243.6091577028576
At time: 720.7270975112915 and batch: 200, loss is 5.54309983253479 and perplexity is 255.46868346804263
At time: 722.3254508972168 and batch: 250, loss is 5.576071519851684 and perplexity is 264.03231998843006
At time: 723.9182341098785 and batch: 300, loss is 5.526104860305786 and perplexity is 251.16368556391978
At time: 725.5226061344147 and batch: 350, loss is 5.502978181838989 and perplexity is 245.42175556685493
At time: 727.1373867988586 and batch: 400, loss is 5.531517782211304 and perplexity is 252.52690114062287
At time: 728.7446031570435 and batch: 450, loss is 5.55258505821228 and perplexity is 257.9033902015952
At time: 730.3504271507263 and batch: 500, loss is 5.56330605506897 and perplexity is 260.6832464259115
At time: 731.9546205997467 and batch: 550, loss is 5.548684139251709 and perplexity is 256.8992897073871
At time: 733.5579199790955 and batch: 600, loss is 5.560067777633667 and perplexity is 259.84044709677096
At time: 735.166750907898 and batch: 650, loss is 5.5790247631073 and perplexity is 264.81322418916835
At time: 736.7714021205902 and batch: 700, loss is 5.537164268493652 and perplexity is 253.95682404470764
At time: 738.3758018016815 and batch: 750, loss is 5.54346604347229 and perplexity is 255.5622560267188
At time: 740.0237424373627 and batch: 800, loss is 5.55366135597229 and perplexity is 258.1811204761937
At time: 741.6338164806366 and batch: 850, loss is 5.565779542922973 and perplexity is 261.3288413761688
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.280648549397786 and perplexity of 196.49727222331782
Finished 25 epochs...
Completing Train Step...
At time: 745.7593123912811 and batch: 50, loss is 5.561928167343139 and perplexity is 260.3243015300051
At time: 747.361674785614 and batch: 100, loss is 5.51367154121399 and perplexity is 248.0602204824755
At time: 748.9435675144196 and batch: 150, loss is 5.484627828598023 and perplexity is 240.9592491665554
At time: 750.5590074062347 and batch: 200, loss is 5.533065719604492 and perplexity is 252.91809967098817
At time: 752.2020950317383 and batch: 250, loss is 5.562213039398193 and perplexity is 260.3984712126957
At time: 753.840808391571 and batch: 300, loss is 5.51525390625 and perplexity is 248.45305302246302
At time: 755.4543795585632 and batch: 350, loss is 5.488924932479859 and perplexity is 241.99690395001264
At time: 757.0658857822418 and batch: 400, loss is 5.518833904266358 and perplexity is 249.34410849617998
At time: 758.6757514476776 and batch: 450, loss is 5.540984373092652 and perplexity is 254.92882105927578
At time: 760.313393831253 and batch: 500, loss is 5.5522535705566405 and perplexity is 257.8179125795668
At time: 761.9307899475098 and batch: 550, loss is 5.538184986114502 and perplexity is 254.21617458926332
At time: 763.5458655357361 and batch: 600, loss is 5.5468660163879395 and perplexity is 256.432639577265
At time: 765.1705141067505 and batch: 650, loss is 5.556862621307373 and perplexity is 259.00895109310164
At time: 766.7849860191345 and batch: 700, loss is 5.519527406692505 and perplexity is 249.51708921470865
At time: 768.39874792099 and batch: 750, loss is 5.525520906448365 and perplexity is 251.01706037622918
At time: 770.015949010849 and batch: 800, loss is 5.538756408691406 and perplexity is 254.3614809625551
At time: 771.6241519451141 and batch: 850, loss is 5.5532940483093265 and perplexity is 258.08630598632067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.273519198099772 and perplexity of 195.10135604154746
Finished 26 epochs...
Completing Train Step...
At time: 775.7508120536804 and batch: 50, loss is 5.546729612350464 and perplexity is 256.3976635153783
At time: 777.3527915477753 and batch: 100, loss is 5.497109537124634 and perplexity is 243.9856805070659
At time: 778.9235463142395 and batch: 150, loss is 5.469724454879761 and perplexity is 237.39477077756848
At time: 780.5534901618958 and batch: 200, loss is 5.516885747909546 and perplexity is 248.85882004855182
At time: 782.1389236450195 and batch: 250, loss is 5.546569309234619 and perplexity is 256.35656546518226
At time: 783.763778924942 and batch: 300, loss is 5.501002187728882 and perplexity is 244.93728243902382
At time: 785.4063127040863 and batch: 350, loss is 5.478517084121704 and perplexity is 239.49129846941702
At time: 787.0480618476868 and batch: 400, loss is 5.507320146560669 and perplexity is 246.48968494953647
At time: 788.6904702186584 and batch: 450, loss is 5.531683692932129 and perplexity is 252.56880153658514
At time: 790.3329706192017 and batch: 500, loss is 5.541245632171631 and perplexity is 254.99543222927846
At time: 791.9674754142761 and batch: 550, loss is 5.526650638580322 and perplexity is 251.30080266121576
At time: 793.6063921451569 and batch: 600, loss is 5.5366229057312015 and perplexity is 253.81937848421063
At time: 795.2377555370331 and batch: 650, loss is 5.551789197921753 and perplexity is 257.69821679005656
At time: 796.8547971248627 and batch: 700, loss is 5.51116436958313 and perplexity is 247.43906992583882
At time: 798.4632863998413 and batch: 750, loss is 5.517135133743286 and perplexity is 248.92088965219148
At time: 800.0716214179993 and batch: 800, loss is 5.53127760887146 and perplexity is 252.46625819407657
At time: 801.6770799160004 and batch: 850, loss is 5.5469200229644775 and perplexity is 256.44648900021775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.26848824818929 and perplexity of 194.12227580871993
Finished 27 epochs...
Completing Train Step...
At time: 805.8391609191895 and batch: 50, loss is 5.542257280349731 and perplexity is 255.25352842297727
At time: 807.420036315918 and batch: 100, loss is 5.494784002304077 and perplexity is 243.41894255116995
At time: 808.9957361221313 and batch: 150, loss is 5.466822624206543 and perplexity is 236.70688988969323
At time: 810.5821840763092 and batch: 200, loss is 5.514504613876343 and perplexity is 248.2669587728293
At time: 812.1769788265228 and batch: 250, loss is 5.556542177200317 and perplexity is 258.9259664977214
At time: 813.7836546897888 and batch: 300, loss is 5.518982524871826 and perplexity is 249.381168922458
At time: 815.383734703064 and batch: 350, loss is 5.494232187271118 and perplexity is 243.2846573730607
At time: 816.9787356853485 and batch: 400, loss is 5.522380228042603 and perplexity is 250.22993321886986
At time: 818.571222782135 and batch: 450, loss is 5.5421585273742675 and perplexity is 255.2283226221423
At time: 820.1701231002808 and batch: 500, loss is 5.546005525588989 and perplexity is 256.2120765601912
At time: 821.8374698162079 and batch: 550, loss is 5.529454746246338 and perplexity is 252.00646608415389
At time: 823.4440875053406 and batch: 600, loss is 5.544341382980346 and perplexity is 255.78605770312151
At time: 825.0509643554688 and batch: 650, loss is 5.556105794906617 and perplexity is 258.8130004405488
At time: 826.6561982631683 and batch: 700, loss is 5.517017669677735 and perplexity is 248.89165210970611
At time: 828.262609243393 and batch: 750, loss is 5.529612979888916 and perplexity is 252.04634514025656
At time: 829.8701829910278 and batch: 800, loss is 5.541740493774414 and perplexity is 255.12165090538633
At time: 831.4803500175476 and batch: 850, loss is 5.547098188400269 and perplexity is 256.49218297110986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.263779322306315 and perplexity of 193.2103172574609
Finished 28 epochs...
Completing Train Step...
At time: 835.6475183963776 and batch: 50, loss is 5.5327902507781985 and perplexity is 252.84843821414378
At time: 837.2534029483795 and batch: 100, loss is 5.4839380168914795 and perplexity is 240.79308997154814
At time: 838.8353362083435 and batch: 150, loss is 5.457973146438599 and perplexity is 234.62139889331397
At time: 840.4345610141754 and batch: 200, loss is 5.506476488113403 and perplexity is 246.28181954069802
At time: 842.0329687595367 and batch: 250, loss is 5.5465560626983645 and perplexity is 256.3531696511352
At time: 843.6257457733154 and batch: 300, loss is 5.489555044174194 and perplexity is 242.14943708059243
At time: 845.2172548770905 and batch: 350, loss is 5.467072668075562 and perplexity is 236.76608439656803
At time: 846.8085103034973 and batch: 400, loss is 5.5035365772247316 and perplexity is 245.55883621176224
At time: 848.4006316661835 and batch: 450, loss is 5.5333021450042725 and perplexity is 252.97790300304936
At time: 849.9965624809265 and batch: 500, loss is 5.531237716674805 and perplexity is 252.456186961339
At time: 851.5917375087738 and batch: 550, loss is 5.518000364303589 and perplexity is 249.13635681396082
At time: 853.2082285881042 and batch: 600, loss is 5.527627964019775 and perplexity is 251.546525384587
At time: 854.8482255935669 and batch: 650, loss is 5.535555486679077 and perplexity is 253.54859139118014
At time: 856.4891104698181 and batch: 700, loss is 5.4993628406524655 and perplexity is 244.53607417080536
At time: 858.1299567222595 and batch: 750, loss is 5.509273519515991 and perplexity is 246.9716418013739
At time: 859.7741968631744 and batch: 800, loss is 5.529759149551392 and perplexity is 252.08318936214218
At time: 861.4013631343842 and batch: 850, loss is 5.535209074020385 and perplexity is 253.46077416090708
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.259361902872722 and perplexity of 192.3587085884076
Finished 29 epochs...
Completing Train Step...
At time: 865.5632915496826 and batch: 50, loss is 5.527099618911743 and perplexity is 251.41365711169942
At time: 867.1502606868744 and batch: 100, loss is 5.47551794052124 and perplexity is 238.77410569389463
At time: 868.7301716804504 and batch: 150, loss is 5.451520118713379 and perplexity is 233.11225501190225
At time: 870.3138267993927 and batch: 200, loss is 5.498577880859375 and perplexity is 244.34419850181595
At time: 871.9180192947388 and batch: 250, loss is 5.530492830276489 and perplexity is 252.26820580249688
At time: 873.5285289287567 and batch: 300, loss is 5.479777069091797 and perplexity is 239.79324408948145
At time: 875.1353466510773 and batch: 350, loss is 5.454306344985962 and perplexity is 233.76266417406177
At time: 876.7445778846741 and batch: 400, loss is 5.490160188674927 and perplexity is 242.2960168272879
At time: 878.3517429828644 and batch: 450, loss is 5.5141332149505615 and perplexity is 248.17476981153428
At time: 879.9681570529938 and batch: 500, loss is 5.520023918151855 and perplexity is 249.6410080698276
At time: 881.5761249065399 and batch: 550, loss is 5.50498815536499 and perplexity is 245.91554288172279
At time: 883.1923677921295 and batch: 600, loss is 5.517454252243042 and perplexity is 249.0003375890342
At time: 884.7998597621918 and batch: 650, loss is 5.525867729187012 and perplexity is 251.10413389922212
At time: 886.4101650714874 and batch: 700, loss is 5.488885679244995 and perplexity is 241.98740497513964
At time: 888.0228607654572 and batch: 750, loss is 5.499595947265625 and perplexity is 244.59308379125233
At time: 889.6314926147461 and batch: 800, loss is 5.511988477706909 and perplexity is 247.64307052123334
At time: 891.2395136356354 and batch: 850, loss is 5.5282543277740475 and perplexity is 251.70413436572474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.256848653157552 and perplexity of 191.87587011981023
Finished 30 epochs...
Completing Train Step...
At time: 895.3839495182037 and batch: 50, loss is 5.519103546142578 and perplexity is 249.41135117473289
At time: 896.9828939437866 and batch: 100, loss is 5.469327840805054 and perplexity is 237.30063533917016
At time: 898.5553188323975 and batch: 150, loss is 5.438634872436523 and perplexity is 230.12781510948358
At time: 900.1487767696381 and batch: 200, loss is 5.485736417770386 and perplexity is 241.22652210173058
At time: 901.7538545131683 and batch: 250, loss is 5.5150282764434815 and perplexity is 248.39700093192963
At time: 903.3792591094971 and batch: 300, loss is 5.470919790267945 and perplexity is 237.67870681363672
At time: 904.976642370224 and batch: 350, loss is 5.445786828994751 and perplexity is 231.77957887663652
At time: 906.5803446769714 and batch: 400, loss is 5.478145456314087 and perplexity is 239.40231337891447
At time: 908.2007460594177 and batch: 450, loss is 5.498927230834961 and perplexity is 244.42957505387707
At time: 909.809463262558 and batch: 500, loss is 5.51148796081543 and perplexity is 247.51915199562418
At time: 911.4139790534973 and batch: 550, loss is 5.494117879867554 and perplexity is 243.25684972488975
At time: 913.0222148895264 and batch: 600, loss is 5.506201248168946 and perplexity is 246.21404227430017
At time: 914.630224943161 and batch: 650, loss is 5.517747097015381 and perplexity is 249.07326671414293
At time: 916.2355005741119 and batch: 700, loss is 5.480831022262573 and perplexity is 240.04610816946268
At time: 917.8518512248993 and batch: 750, loss is 5.4863708972930905 and perplexity is 241.3796239551968
At time: 919.4675316810608 and batch: 800, loss is 5.502193670272828 and perplexity is 245.22929486471952
At time: 921.0738101005554 and batch: 850, loss is 5.5146746253967285 and perplexity is 248.30917060409882
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.245242118835449 and perplexity of 189.66173034811905
Finished 31 epochs...
Completing Train Step...
At time: 925.2064960002899 and batch: 50, loss is 5.505961046218872 and perplexity is 246.1549082835447
At time: 926.8119857311249 and batch: 100, loss is 5.457826862335205 and perplexity is 234.5870800225536
At time: 928.3883213996887 and batch: 150, loss is 5.433035659790039 and perplexity is 228.84288120291913
At time: 929.9714484214783 and batch: 200, loss is 5.485232992172241 and perplexity is 241.1051130583275
At time: 931.575058221817 and batch: 250, loss is 5.525206079483032 and perplexity is 250.93804587546472
At time: 933.1803278923035 and batch: 300, loss is 5.490471467971802 and perplexity is 242.37145030087183
At time: 934.7747473716736 and batch: 350, loss is 5.461854763031006 and perplexity is 235.53387901148497
At time: 936.382643699646 and batch: 400, loss is 5.489633026123047 and perplexity is 242.16832110190646
At time: 937.9931344985962 and batch: 450, loss is 5.504765148162842 and perplexity is 245.86070805904743
At time: 939.6101479530334 and batch: 500, loss is 5.5021723365783695 and perplexity is 245.22406327367545
At time: 941.2294158935547 and batch: 550, loss is 5.485055265426635 and perplexity is 241.06226603887816
At time: 942.864896774292 and batch: 600, loss is 5.495945043563843 and perplexity is 243.70172611680772
At time: 944.4759993553162 and batch: 650, loss is 5.505593748092651 and perplexity is 246.06451264906184
At time: 946.0887501239777 and batch: 700, loss is 5.46773621559143 and perplexity is 236.92324207873978
At time: 947.6973085403442 and batch: 750, loss is 5.474652290344238 and perplexity is 238.56750028394367
At time: 949.309289932251 and batch: 800, loss is 5.490898857116699 and perplexity is 242.47505936688128
At time: 950.9134645462036 and batch: 850, loss is 5.50278076171875 and perplexity is 245.37330915666706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.235733350118001 and perplexity of 187.86682799941246
Finished 32 epochs...
Completing Train Step...
At time: 955.054850101471 and batch: 50, loss is 5.496002035140991 and perplexity is 243.71561545831682
At time: 956.653430223465 and batch: 100, loss is 5.444934768676758 and perplexity is 231.5821728078366
At time: 958.22558760643 and batch: 150, loss is 5.415129041671753 and perplexity is 224.7815499941832
At time: 959.799439907074 and batch: 200, loss is 5.465740613937378 and perplexity is 236.45090911593738
At time: 961.3877265453339 and batch: 250, loss is 5.496543273925782 and perplexity is 243.84755950525874
At time: 962.9881181716919 and batch: 300, loss is 5.445046339035034 and perplexity is 231.6080119552419
At time: 964.6017036437988 and batch: 350, loss is 5.422032918930054 and perplexity is 226.33878351455274
At time: 966.2094111442566 and batch: 400, loss is 5.452391920089721 and perplexity is 233.31557120945698
At time: 967.820639371872 and batch: 450, loss is 5.475493621826172 and perplexity is 238.76829908983285
At time: 969.432003736496 and batch: 500, loss is 5.4873739528656005 and perplexity is 241.6218626011972
At time: 971.0490365028381 and batch: 550, loss is 5.4669349193573 and perplexity is 236.73347241809515
At time: 972.6878798007965 and batch: 600, loss is 5.481238241195679 and perplexity is 240.14387939532364
At time: 974.334317445755 and batch: 650, loss is 5.491159467697144 and perplexity is 242.53825916775696
At time: 975.976229429245 and batch: 700, loss is 5.450888013839721 and perplexity is 232.96495018035557
At time: 977.6154267787933 and batch: 750, loss is 5.461667175292969 and perplexity is 235.48969988775053
At time: 979.2525315284729 and batch: 800, loss is 5.475958232879639 and perplexity is 238.87925925547108
At time: 980.871743440628 and batch: 850, loss is 5.489181995391846 and perplexity is 242.059120375259
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.227004369099935 and perplexity of 186.2340785108086
Finished 33 epochs...
Completing Train Step...
At time: 985.0258100032806 and batch: 50, loss is 5.479731349945069 and perplexity is 239.7822811975795
At time: 986.6000788211823 and batch: 100, loss is 5.428310852050782 and perplexity is 227.764192890882
At time: 988.1873662471771 and batch: 150, loss is 5.404488611221313 and perplexity is 222.40245727667767
At time: 989.7725629806519 and batch: 200, loss is 5.453583660125733 and perplexity is 233.59378846508525
At time: 991.3485839366913 and batch: 250, loss is 5.4931426620483395 and perplexity is 243.01973694748705
At time: 992.9413707256317 and batch: 300, loss is 5.448929738998413 and perplexity is 232.50918717976782
At time: 994.5394008159637 and batch: 350, loss is 5.420429582595825 and perplexity is 225.97617708680815
At time: 996.1463389396667 and batch: 400, loss is 5.448370246887207 and perplexity is 232.37913650831268
At time: 997.7531371116638 and batch: 450, loss is 5.4620621871948245 and perplexity is 235.58273949663726
At time: 999.3611934185028 and batch: 500, loss is 5.4752168941497805 and perplexity is 238.7022344346106
At time: 1000.9711904525757 and batch: 550, loss is 5.454948539733887 and perplexity is 233.91283354308422
At time: 1002.5771708488464 and batch: 600, loss is 5.471658515930176 and perplexity is 237.8543510421877
At time: 1004.1838970184326 and batch: 650, loss is 5.4793665313720705 and perplexity is 239.69482012260508
At time: 1005.7912976741791 and batch: 700, loss is 5.440782117843628 and perplexity is 230.6224869043304
At time: 1007.3968715667725 and batch: 750, loss is 5.451189699172974 and perplexity is 233.0352428915938
At time: 1009.0013072490692 and batch: 800, loss is 5.463273057937622 and perplexity is 235.86817251977482
At time: 1010.6444599628448 and batch: 850, loss is 5.475782489776611 and perplexity is 238.8372815619537
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.221983909606934 and perplexity of 185.30144095688695
Finished 34 epochs...
Completing Train Step...
At time: 1015.0955767631531 and batch: 50, loss is 5.470915088653564 and perplexity is 237.6775893426378
At time: 1016.7034327983856 and batch: 100, loss is 5.418663825988769 and perplexity is 225.5775102369754
At time: 1018.3084619045258 and batch: 150, loss is 5.391971378326416 and perplexity is 219.63594458597396
At time: 1019.9054214954376 and batch: 200, loss is 5.4413980197906495 and perplexity is 230.7645714936379
At time: 1021.4804666042328 and batch: 250, loss is 5.482729568481445 and perplexity is 240.50227969482333
At time: 1023.0553259849548 and batch: 300, loss is 5.448436079025268 and perplexity is 232.39443502727133
At time: 1024.6708753108978 and batch: 350, loss is 5.418974275588989 and perplexity is 225.64755155643437
At time: 1026.2614715099335 and batch: 400, loss is 5.437750749588012 and perplexity is 229.92444376594062
At time: 1027.8741085529327 and batch: 450, loss is 5.464937047958374 and perplexity is 236.26098152956078
At time: 1029.485939502716 and batch: 500, loss is 5.461989336013794 and perplexity is 235.56557764097278
At time: 1031.096685886383 and batch: 550, loss is 5.449439888000488 and perplexity is 232.6278317702174
At time: 1032.705973625183 and batch: 600, loss is 5.4652121639251705 and perplexity is 236.32598963988508
At time: 1034.313380241394 and batch: 650, loss is 5.471689233779907 and perplexity is 237.86165752862016
At time: 1035.9219071865082 and batch: 700, loss is 5.430861339569092 and perplexity is 228.34584405368088
At time: 1037.5299916267395 and batch: 750, loss is 5.439783916473389 and perplexity is 230.39239408048613
At time: 1039.1375901699066 and batch: 800, loss is 5.456254234313965 and perplexity is 234.2184517406451
At time: 1040.7451717853546 and batch: 850, loss is 5.469752187728882 and perplexity is 237.4013545022209
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.219154675801595 and perplexity of 184.77792078564937
Finished 35 epochs...
Completing Train Step...
At time: 1044.9138526916504 and batch: 50, loss is 5.4787646484375 and perplexity is 239.5505953084495
At time: 1046.4900074005127 and batch: 100, loss is 5.410103807449341 and perplexity is 223.6548035102223
At time: 1048.0707657337189 and batch: 150, loss is 5.376991634368896 and perplexity is 216.37037414074078
At time: 1049.6441214084625 and batch: 200, loss is 5.426184968948364 and perplexity is 227.28050715371054
At time: 1051.2242405414581 and batch: 250, loss is 5.455842638015747 and perplexity is 234.1220681298638
At time: 1052.8227422237396 and batch: 300, loss is 5.416124057769776 and perplexity is 225.00532256515888
At time: 1054.4295630455017 and batch: 350, loss is 5.395194387435913 and perplexity is 220.34497522848574
At time: 1056.07293009758 and batch: 400, loss is 5.424089870452881 and perplexity is 226.8048305737596
At time: 1057.709359407425 and batch: 450, loss is 5.437836952209473 and perplexity is 229.9442647100275
At time: 1059.3302495479584 and batch: 500, loss is 5.44644793510437 and perplexity is 231.93286043442114
At time: 1060.9357135295868 and batch: 550, loss is 5.430861883163452 and perplexity is 228.3459681812275
At time: 1062.54030251503 and batch: 600, loss is 5.447357797622681 and perplexity is 232.1439834828045
At time: 1064.145766735077 and batch: 650, loss is 5.452483081817627 and perplexity is 233.3368416295852
At time: 1065.791321516037 and batch: 700, loss is 5.413893175125122 and perplexity is 224.50392158741252
At time: 1067.3951830863953 and batch: 750, loss is 5.424435710906982 and perplexity is 226.8832824244875
At time: 1069.000063419342 and batch: 800, loss is 5.437848138809204 and perplexity is 229.94683701886493
At time: 1070.6165056228638 and batch: 850, loss is 5.451945066452026 and perplexity is 233.21133658827543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.19700304667155 and perplexity of 180.72979068297317
Finished 36 epochs...
Completing Train Step...
At time: 1074.7264332771301 and batch: 50, loss is 5.442421255111694 and perplexity is 231.00081880171814
At time: 1076.3328380584717 and batch: 100, loss is 5.389247436523437 and perplexity is 219.0384831495938
At time: 1077.915211200714 and batch: 150, loss is 5.361958827972412 and perplexity is 213.1420464052443
At time: 1079.4913885593414 and batch: 200, loss is 5.408102931976319 and perplexity is 223.20774550230652
At time: 1081.0780324935913 and batch: 250, loss is 5.438246450424194 and perplexity is 230.0384457580811
At time: 1082.6774904727936 and batch: 300, loss is 5.40011040687561 and perplexity is 221.4308623443264
At time: 1084.2739119529724 and batch: 350, loss is 5.375183582305908 and perplexity is 215.97951868938588
At time: 1085.872412443161 and batch: 400, loss is 5.4088304996490475 and perplexity is 223.37020333460367
At time: 1087.4693412780762 and batch: 450, loss is 5.438677377700806 and perplexity is 230.1375969609722
At time: 1089.0698761940002 and batch: 500, loss is 5.43446494102478 and perplexity is 229.17019589537938
At time: 1090.6742885112762 and batch: 550, loss is 5.412097520828247 and perplexity is 224.10115188174848
At time: 1092.27170753479 and batch: 600, loss is 5.43510139465332 and perplexity is 229.31609852331363
At time: 1093.8725526332855 and batch: 650, loss is 5.437842473983765 and perplexity is 229.94553441386248
At time: 1095.4800910949707 and batch: 700, loss is 5.4001115131378175 and perplexity is 221.4311073050564
At time: 1097.0759897232056 and batch: 750, loss is 5.408211860656738 and perplexity is 223.23206055176263
At time: 1098.671051979065 and batch: 800, loss is 5.423483419418335 and perplexity is 226.66732624865253
At time: 1100.2720205783844 and batch: 850, loss is 5.433373947143554 and perplexity is 228.92030895124773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.190033912658691 and perplexity of 179.47463929081556
Finished 37 epochs...
Completing Train Step...
At time: 1104.346997499466 and batch: 50, loss is 5.429224815368652 and perplexity is 227.97245616630838
At time: 1105.9423677921295 and batch: 100, loss is 5.374166488647461 and perplexity is 215.75995896590112
At time: 1107.5304803848267 and batch: 150, loss is 5.347444124221802 and perplexity is 210.07069653323947
At time: 1109.1268219947815 and batch: 200, loss is 5.394490327835083 and perplexity is 220.1898938328604
At time: 1110.7194244861603 and batch: 250, loss is 5.424871654510498 and perplexity is 226.9822123025621
At time: 1112.3255624771118 and batch: 300, loss is 5.386426162719727 and perplexity is 218.42138652367132
At time: 1113.9197452068329 and batch: 350, loss is 5.364192991256714 and perplexity is 213.61877288376363
At time: 1115.5214774608612 and batch: 400, loss is 5.399655342102051 and perplexity is 221.33011988300578
At time: 1117.1216855049133 and batch: 450, loss is 5.409646854400635 and perplexity is 223.5526271125868
At time: 1118.7209825515747 and batch: 500, loss is 5.4089204120635985 and perplexity is 223.39028799184072
At time: 1120.313594341278 and batch: 550, loss is 5.395376834869385 and perplexity is 220.38518027123658
At time: 1121.911246061325 and batch: 600, loss is 5.419647092819214 and perplexity is 225.79942220194397
At time: 1123.5173337459564 and batch: 650, loss is 5.422579326629639 and perplexity is 226.4624905627438
At time: 1125.115573644638 and batch: 700, loss is 5.384895830154419 and perplexity is 218.0873847949594
At time: 1126.714138507843 and batch: 750, loss is 5.3967634963989255 and perplexity is 220.69099190200942
At time: 1128.3146271705627 and batch: 800, loss is 5.400142335891724 and perplexity is 221.43793252676966
At time: 1129.9187564849854 and batch: 850, loss is 5.41305606842041 and perplexity is 224.3160664877691
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.184699376424153 and perplexity of 178.51977446923624
Finished 38 epochs...
Completing Train Step...
At time: 1134.0888094902039 and batch: 50, loss is 5.4175786113739015 and perplexity is 225.33284300831875
At time: 1135.6689658164978 and batch: 100, loss is 5.360118274688721 and perplexity is 212.75010791432405
At time: 1137.2516951560974 and batch: 150, loss is 5.336140918731689 and perplexity is 207.70959343791506
At time: 1138.8480989933014 and batch: 200, loss is 5.380799951553345 and perplexity is 217.19595218852092
At time: 1140.4722084999084 and batch: 250, loss is 5.406630277633667 and perplexity is 222.87927956433634
At time: 1142.1021239757538 and batch: 300, loss is 5.366003484725952 and perplexity is 214.00587859741879
At time: 1143.7180638313293 and batch: 350, loss is 5.345044765472412 and perplexity is 209.56726576654705
At time: 1145.3277661800385 and batch: 400, loss is 5.375696668624878 and perplexity is 216.09036325958758
At time: 1146.9679458141327 and batch: 450, loss is 5.395685119628906 and perplexity is 220.4531321372627
At time: 1148.5765726566315 and batch: 500, loss is 5.393639669418335 and perplexity is 220.00266709068086
At time: 1150.185787677765 and batch: 550, loss is 5.3824687194824214 and perplexity is 217.55870441831968
At time: 1151.7953414916992 and batch: 600, loss is 5.406101980209351 and perplexity is 222.76156411213196
At time: 1153.4047598838806 and batch: 650, loss is 5.409205360412598 and perplexity is 223.45395175559784
At time: 1155.0125305652618 and batch: 700, loss is 5.369614028930664 and perplexity is 214.77995285592274
At time: 1156.625321149826 and batch: 750, loss is 5.379450845718384 and perplexity is 216.90312943095233
At time: 1158.2377216815948 and batch: 800, loss is 5.389014511108399 and perplexity is 218.98746946141966
At time: 1159.8511769771576 and batch: 850, loss is 5.396190195083618 and perplexity is 220.56450572688524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.1782026290893555 and perplexity of 177.36373591159733
Finished 39 epochs...
Completing Train Step...
At time: 1164.0613942146301 and batch: 50, loss is 5.399391717910767 and perplexity is 221.27177959943776
At time: 1165.63862657547 and batch: 100, loss is 5.345690050125122 and perplexity is 209.70253994733903
At time: 1167.2242922782898 and batch: 150, loss is 5.323893947601318 and perplexity is 205.1812936520268
At time: 1168.825715303421 and batch: 200, loss is 5.367399988174438 and perplexity is 214.30494732152448
At time: 1170.4274008274078 and batch: 250, loss is 5.395412540435791 and perplexity is 220.3930493894104
At time: 1172.0197021961212 and batch: 300, loss is 5.354011611938477 and perplexity is 211.4548735599873
At time: 1173.616509437561 and batch: 350, loss is 5.329603414535523 and perplexity is 206.35612008858132
At time: 1175.225295305252 and batch: 400, loss is 5.364746770858765 and perplexity is 213.73710336428087
At time: 1176.8479266166687 and batch: 450, loss is 5.3817065334320064 and perplexity is 217.39294738553914
At time: 1178.4540741443634 and batch: 500, loss is 5.379200010299683 and perplexity is 216.84872926669325
At time: 1180.0744075775146 and batch: 550, loss is 5.369913120269775 and perplexity is 214.84420128723227
At time: 1181.6900482177734 and batch: 600, loss is 5.395644750595093 and perplexity is 220.44423283694644
At time: 1183.3101997375488 and batch: 650, loss is 5.397771511077881 and perplexity is 220.91356382039416
At time: 1184.9302117824554 and batch: 700, loss is 5.359651203155518 and perplexity is 212.65076159795566
At time: 1186.5307757854462 and batch: 750, loss is 5.364587507247925 and perplexity is 213.7030655319946
At time: 1188.1848509311676 and batch: 800, loss is 5.374360656738281 and perplexity is 215.80185673268198
At time: 1189.7821538448334 and batch: 850, loss is 5.378334436416626 and perplexity is 216.6611118801347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.168278058369954 and perplexity of 175.61218305347347
Finished 40 epochs...
Completing Train Step...
At time: 1193.9674966335297 and batch: 50, loss is 5.388980741500855 and perplexity is 218.98007446538264
At time: 1195.560290336609 and batch: 100, loss is 5.339252128601074 and perplexity is 208.35682789401653
At time: 1197.1553597450256 and batch: 150, loss is 5.314116725921631 and perplexity is 203.1849658323504
At time: 1198.744604587555 and batch: 200, loss is 5.358298883438111 and perplexity is 212.36338413702407
At time: 1200.334599018097 and batch: 250, loss is 5.385489301681519 and perplexity is 218.21685186196382
At time: 1201.9318311214447 and batch: 300, loss is 5.345502099990845 and perplexity is 209.6631300304632
At time: 1203.5658822059631 and batch: 350, loss is 5.326208620071411 and perplexity is 205.65677121894922
At time: 1205.1902639865875 and batch: 400, loss is 5.357339744567871 and perplexity is 212.15979581102866
At time: 1206.815997838974 and batch: 450, loss is 5.3711341285705565 and perplexity is 215.1066880570302
At time: 1208.4311168193817 and batch: 500, loss is 5.371841325759887 and perplexity is 215.25886470533706
At time: 1210.0472254753113 and batch: 550, loss is 5.358143463134765 and perplexity is 212.33038112017795
At time: 1211.6735746860504 and batch: 600, loss is 5.38696888923645 and perplexity is 218.5399617760135
At time: 1213.3008244037628 and batch: 650, loss is 5.390495252609253 and perplexity is 219.31197348961697
At time: 1214.9269981384277 and batch: 700, loss is 5.353002271652222 and perplexity is 211.24155131285772
At time: 1216.551353931427 and batch: 750, loss is 5.356728734970093 and perplexity is 212.03020373455854
At time: 1218.181381225586 and batch: 800, loss is 5.366865997314453 and perplexity is 214.1905409870922
At time: 1219.8099434375763 and batch: 850, loss is 5.369643630981446 and perplexity is 214.78631087709877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.166697820027669 and perplexity of 175.33489309825254
Finished 41 epochs...
Completing Train Step...
At time: 1224.1299784183502 and batch: 50, loss is 5.381317720413208 and perplexity is 217.30843860752194
At time: 1225.7307786941528 and batch: 100, loss is 5.328670682907105 and perplexity is 206.16373494445918
At time: 1227.3062326908112 and batch: 150, loss is 5.308625736236572 and perplexity is 202.07233679366217
At time: 1228.922726392746 and batch: 200, loss is 5.351065607070923 and perplexity is 210.83284317536254
At time: 1230.4951872825623 and batch: 250, loss is 5.384149093627929 and perplexity is 217.92459176808072
At time: 1232.0683679580688 and batch: 300, loss is 5.351961517333985 and perplexity is 211.02181512166703
At time: 1233.6428644657135 and batch: 350, loss is 5.324723854064941 and perplexity is 205.3516456121621
At time: 1235.219453573227 and batch: 400, loss is 5.3471309852600095 and perplexity is 210.00492551169617
At time: 1236.8198976516724 and batch: 450, loss is 5.357909908294678 and perplexity is 212.28079612260322
At time: 1238.4260942935944 and batch: 500, loss is 5.356901407241821 and perplexity is 212.0668186326102
At time: 1240.0336918830872 and batch: 550, loss is 5.349811029434204 and perplexity is 210.5685028576264
At time: 1241.6416800022125 and batch: 600, loss is 5.376904697418213 and perplexity is 216.35156437783823
At time: 1243.2554321289062 and batch: 650, loss is 5.384875555038452 and perplexity is 218.08296309276705
At time: 1244.8711383342743 and batch: 700, loss is 5.347611646652222 and perplexity is 210.10589103473467
At time: 1246.4898643493652 and batch: 750, loss is 5.347494144439697 and perplexity is 210.08120457805853
At time: 1248.099036693573 and batch: 800, loss is 5.356397695541382 and perplexity is 211.96002499365082
At time: 1249.7117683887482 and batch: 850, loss is 5.358268985748291 and perplexity is 212.35703505734793
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.16645876566569 and perplexity of 175.29298353678166
Finished 42 epochs...
Completing Train Step...
At time: 1253.8674855232239 and batch: 50, loss is 5.371540994644165 and perplexity is 215.1942254774025
At time: 1255.4696424007416 and batch: 100, loss is 5.3220010471344 and perplexity is 204.79327324333505
At time: 1257.0496830940247 and batch: 150, loss is 5.29939600944519 and perplexity is 200.21584496832423
At time: 1258.6378362178802 and batch: 200, loss is 5.343417654037475 and perplexity is 209.22655373542014
At time: 1260.2201895713806 and batch: 250, loss is 5.376869840621948 and perplexity is 216.34402318686887
At time: 1261.799197435379 and batch: 300, loss is 5.339444093704223 and perplexity is 208.3968289732579
At time: 1263.385065317154 and batch: 350, loss is 5.314282217025757 and perplexity is 203.2185939191857
At time: 1264.9728150367737 and batch: 400, loss is 5.3317466068267825 and perplexity is 206.79885519817944
At time: 1266.5754616260529 and batch: 450, loss is 5.348089418411255 and perplexity is 210.2062976796972
At time: 1268.2114598751068 and batch: 500, loss is 5.349826669692993 and perplexity is 210.57179622925835
At time: 1269.8181054592133 and batch: 550, loss is 5.341967163085937 and perplexity is 208.92329250441102
At time: 1271.4223432540894 and batch: 600, loss is 5.370174732208252 and perplexity is 214.90041444789787
At time: 1273.0226118564606 and batch: 650, loss is 5.376711540222168 and perplexity is 216.30977855204947
At time: 1274.6228120326996 and batch: 700, loss is 5.3408419799804685 and perplexity is 208.68834774806885
At time: 1276.2175006866455 and batch: 750, loss is 5.340666742324829 and perplexity is 208.65178089528854
At time: 1277.815853357315 and batch: 800, loss is 5.348288860321045 and perplexity is 210.24822580613014
At time: 1279.4168446063995 and batch: 850, loss is 5.352099142074585 and perplexity is 211.05085894276246
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.159668922424316 and perplexity of 174.10680320361408
Finished 43 epochs...
Completing Train Step...
At time: 1283.5588083267212 and batch: 50, loss is 5.3686851978302 and perplexity is 214.58055117553084
At time: 1285.1826078891754 and batch: 100, loss is 5.306908302307129 and perplexity is 201.7255887499085
At time: 1286.760410785675 and batch: 150, loss is 5.292197017669678 and perplexity is 198.7796684610418
At time: 1288.3451189994812 and batch: 200, loss is 5.338602352142334 and perplexity is 208.2214865078184
At time: 1289.9290778636932 and batch: 250, loss is 5.365028419494629 and perplexity is 213.79731060612278
At time: 1291.5332000255585 and batch: 300, loss is 5.317261438369751 and perplexity is 203.82492984768191
At time: 1293.1350891590118 and batch: 350, loss is 5.296226568222046 and perplexity is 199.58227717406527
At time: 1294.7376070022583 and batch: 400, loss is 5.3260065364837645 and perplexity is 205.61521555979718
At time: 1296.335471868515 and batch: 450, loss is 5.340728225708008 and perplexity is 208.66460990706585
At time: 1297.9417510032654 and batch: 500, loss is 5.341331033706665 and perplexity is 208.79043252258393
At time: 1299.5460450649261 and batch: 550, loss is 5.332877531051635 and perplexity is 207.0328613298555
At time: 1301.158925294876 and batch: 600, loss is 5.360911436080933 and perplexity is 212.91892002488498
At time: 1302.7619392871857 and batch: 650, loss is 5.368226499557495 and perplexity is 214.48214601821368
At time: 1304.3597271442413 and batch: 700, loss is 5.333023843765258 and perplexity is 207.06315508573257
At time: 1305.9566555023193 and batch: 750, loss is 5.329704494476318 and perplexity is 206.37697960720428
At time: 1307.5530338287354 and batch: 800, loss is 5.338304948806763 and perplexity is 208.15956995074512
At time: 1309.1683888435364 and batch: 850, loss is 5.339139671325683 and perplexity is 208.33339797029993
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.15162467956543 and perplexity of 172.71186393257827
Finished 44 epochs...
Completing Train Step...
At time: 1313.3272790908813 and batch: 50, loss is 5.3537998867034915 and perplexity is 211.41010796636422
At time: 1314.9054865837097 and batch: 100, loss is 5.3044633865356445 and perplexity is 201.2329891041642
At time: 1316.4814550876617 and batch: 150, loss is 5.283691148757935 and perplexity is 197.0960451491922
At time: 1318.0666980743408 and batch: 200, loss is 5.3263879489898684 and perplexity is 205.69365473234672
At time: 1319.6634707450867 and batch: 250, loss is 5.353800802230835 and perplexity is 211.4103015181874
At time: 1321.2675197124481 and batch: 300, loss is 5.308396234512329 and perplexity is 202.02596616521924
At time: 1322.8692512512207 and batch: 350, loss is 5.2874563026428225 and perplexity is 197.83954089889696
At time: 1324.472733259201 and batch: 400, loss is 5.317069444656372 and perplexity is 203.78580049893588
At time: 1326.0656270980835 and batch: 450, loss is 5.327837314605713 and perplexity is 205.99199619361482
At time: 1327.6653044223785 and batch: 500, loss is 5.327387475967408 and perplexity is 205.89935387315518
At time: 1329.2585000991821 and batch: 550, loss is 5.322522487640381 and perplexity is 204.90008859786258
At time: 1330.850260734558 and batch: 600, loss is 5.350438470840454 and perplexity is 210.70066371243186
At time: 1332.4429349899292 and batch: 650, loss is 5.3581219577789305 and perplexity is 212.32581492887638
At time: 1334.0508642196655 and batch: 700, loss is 5.325299577713013 and perplexity is 205.46990544993753
At time: 1335.6634855270386 and batch: 750, loss is 5.319318904876709 and perplexity is 204.2447245227799
At time: 1337.2680125236511 and batch: 800, loss is 5.329111623764038 and perplexity is 206.25466100344806
At time: 1338.8732919692993 and batch: 850, loss is 5.329360847473144 and perplexity is 206.3060709611079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.142897288004558 and perplexity of 171.2110982793472
Finished 45 epochs...
Completing Train Step...
At time: 1342.9944534301758 and batch: 50, loss is 5.341452875137329 and perplexity is 208.8158733974362
At time: 1344.566567659378 and batch: 100, loss is 5.291041660308838 and perplexity is 198.55013952740367
At time: 1346.1385428905487 and batch: 150, loss is 5.272093954086304 and perplexity is 194.82348706441454
At time: 1347.7242896556854 and batch: 200, loss is 5.3187184906005855 and perplexity is 204.12212988182281
At time: 1349.3393330574036 and batch: 250, loss is 5.347549133300781 and perplexity is 210.0927570218584
At time: 1350.917572259903 and batch: 300, loss is 5.300965127944946 and perplexity is 200.53025396258323
At time: 1352.5034317970276 and batch: 350, loss is 5.276698255538941 and perplexity is 195.72258139120157
At time: 1354.0928809642792 and batch: 400, loss is 5.307273988723755 and perplexity is 201.79937054727858
At time: 1355.6816413402557 and batch: 450, loss is 5.320097360610962 and perplexity is 204.40378190130755
At time: 1357.2851881980896 and batch: 500, loss is 5.324849805831909 and perplexity is 205.37751164367873
At time: 1358.915997505188 and batch: 550, loss is 5.319252681732178 and perplexity is 204.2311992427164
At time: 1360.5436899662018 and batch: 600, loss is 5.3429694175720215 and perplexity is 209.13279177982832
At time: 1362.1693725585938 and batch: 650, loss is 5.351230096817017 and perplexity is 210.86752586860027
At time: 1363.7980585098267 and batch: 700, loss is 5.318711347579956 and perplexity is 204.12067183844562
At time: 1365.4171011447906 and batch: 750, loss is 5.315077133178711 and perplexity is 203.38019988515933
At time: 1367.008849620819 and batch: 800, loss is 5.322642517089844 and perplexity is 204.9246841187558
At time: 1368.6003272533417 and batch: 850, loss is 5.322161388397217 and perplexity is 204.82611268807204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.140297253926595 and perplexity of 170.76652179674923
Finished 46 epochs...
Completing Train Step...
At time: 1372.7231447696686 and batch: 50, loss is 5.337659139633178 and perplexity is 208.02518199006118
At time: 1374.3051979541779 and batch: 100, loss is 5.292889432907105 and perplexity is 198.91735419472383
At time: 1375.8814165592194 and batch: 150, loss is 5.271513023376465 and perplexity is 194.71034098597875
At time: 1377.4615757465363 and batch: 200, loss is 5.31652135848999 and perplexity is 203.674138923655
At time: 1379.0368263721466 and batch: 250, loss is 5.343304433822632 and perplexity is 209.2028664010231
At time: 1380.6268246173859 and batch: 300, loss is 5.295558023452759 and perplexity is 199.44889207854007
At time: 1382.2328882217407 and batch: 350, loss is 5.270874977111816 and perplexity is 194.58614640538167
At time: 1383.8560483455658 and batch: 400, loss is 5.303085222244262 and perplexity is 200.955848001184
At time: 1385.452817440033 and batch: 450, loss is 5.313557081222534 and perplexity is 203.07128625633928
At time: 1387.046787261963 and batch: 500, loss is 5.320328340530396 and perplexity is 204.4510005234499
At time: 1388.6479864120483 and batch: 550, loss is 5.315550661087036 and perplexity is 203.47652889124197
At time: 1390.267821073532 and batch: 600, loss is 5.33780876159668 and perplexity is 208.05630945486644
At time: 1391.8726801872253 and batch: 650, loss is 5.345388374328613 and perplexity is 209.63928730794538
At time: 1393.4725165367126 and batch: 700, loss is 5.313125829696656 and perplexity is 202.9837303349485
At time: 1395.0702276229858 and batch: 750, loss is 5.309409170150757 and perplexity is 202.2307091444599
At time: 1396.6694695949554 and batch: 800, loss is 5.314905815124511 and perplexity is 203.3453601694736
At time: 1398.2662885189056 and batch: 850, loss is 5.316359157562256 and perplexity is 203.64110546846703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.14066219329834 and perplexity of 170.82885259671855
Annealing...
Finished 47 epochs...
Completing Train Step...
At time: 1402.3728711605072 and batch: 50, loss is 5.339358463287353 and perplexity is 208.37898462993869
At time: 1403.9929895401 and batch: 100, loss is 5.291350355148316 and perplexity is 198.6114403919969
At time: 1405.5792217254639 and batch: 150, loss is 5.267159252166748 and perplexity is 193.86445943269945
At time: 1407.1523175239563 and batch: 200, loss is 5.309491138458252 and perplexity is 202.2472863328048
At time: 1408.7301201820374 and batch: 250, loss is 5.328692502975464 and perplexity is 206.16823350032809
At time: 1410.356903553009 and batch: 300, loss is 5.2746921730041505 and perplexity is 195.33033930555118
At time: 1411.9572253227234 and batch: 350, loss is 5.244893445968628 and perplexity is 189.59561197638317
At time: 1413.5520305633545 and batch: 400, loss is 5.26571252822876 and perplexity is 193.5841938608521
At time: 1415.146811723709 and batch: 450, loss is 5.285877094268799 and perplexity is 197.5273576053117
At time: 1416.741878271103 and batch: 500, loss is 5.28571569442749 and perplexity is 197.49547929378645
At time: 1418.3351893424988 and batch: 550, loss is 5.269692764282227 and perplexity is 194.3562400925141
At time: 1419.9291076660156 and batch: 600, loss is 5.273150320053101 and perplexity is 195.0294007066155
At time: 1421.5232284069061 and batch: 650, loss is 5.278990468978882 and perplexity is 196.1717339028153
At time: 1423.1161873340607 and batch: 700, loss is 5.23959755897522 and perplexity is 188.59418909264872
At time: 1424.7093544006348 and batch: 750, loss is 5.219922485351563 and perplexity is 184.91984951799728
At time: 1426.3028225898743 and batch: 800, loss is 5.218142023086548 and perplexity is 184.59089963220646
At time: 1427.896517753601 and batch: 850, loss is 5.229792203903198 and perplexity is 186.75399273719017
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.091344833374023 and perplexity of 162.60839627270113
Finished 48 epochs...
Completing Train Step...
At time: 1431.9903025627136 and batch: 50, loss is 5.302411279678345 and perplexity is 200.82046092802605
At time: 1433.5886702537537 and batch: 100, loss is 5.257084093093872 and perplexity is 191.92105068089813
At time: 1435.1654033660889 and batch: 150, loss is 5.236209125518799 and perplexity is 187.95623168115026
At time: 1436.7442886829376 and batch: 200, loss is 5.283062162399292 and perplexity is 196.972113405225
At time: 1438.3553669452667 and batch: 250, loss is 5.302593870162964 and perplexity is 200.85713218111738
At time: 1439.9657039642334 and batch: 300, loss is 5.253892307281494 and perplexity is 191.30945635250615
At time: 1441.5604469776154 and batch: 350, loss is 5.225250463485718 and perplexity is 185.90772779278794
At time: 1443.1518409252167 and batch: 400, loss is 5.247101984024048 and perplexity is 190.01480383076625
At time: 1444.743396282196 and batch: 450, loss is 5.269453392028809 and perplexity is 194.30972216912951
At time: 1446.3354036808014 and batch: 500, loss is 5.268985977172852 and perplexity is 194.21892014109235
At time: 1447.9337434768677 and batch: 550, loss is 5.25709900856018 and perplexity is 191.92391329421196
At time: 1449.5331420898438 and batch: 600, loss is 5.265511341094971 and perplexity is 193.54525112926203
At time: 1451.1252329349518 and batch: 650, loss is 5.273498439788819 and perplexity is 195.0973061089663
At time: 1452.7209582328796 and batch: 700, loss is 5.238233947753907 and perplexity is 188.33719519982753
At time: 1454.3153507709503 and batch: 750, loss is 5.2227620506286625 and perplexity is 185.445687724366
At time: 1455.907185792923 and batch: 800, loss is 5.225096683502198 and perplexity is 185.87914110355825
At time: 1457.4990565776825 and batch: 850, loss is 5.23722993850708 and perplexity is 188.1481978077665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.087386131286621 and perplexity of 161.96595053957725
Finished 49 epochs...
Completing Train Step...
At time: 1461.640590429306 and batch: 50, loss is 5.293919191360474 and perplexity is 199.1222965241657
At time: 1463.2122540473938 and batch: 100, loss is 5.24714433670044 and perplexity is 190.02285163668438
At time: 1464.7844998836517 and batch: 150, loss is 5.225706453323364 and perplexity is 185.99251915793255
At time: 1466.3554363250732 and batch: 200, loss is 5.273271274566651 and perplexity is 195.0529918196029
At time: 1467.9560415744781 and batch: 250, loss is 5.292470970153809 and perplexity is 198.834132104896
At time: 1469.5457079410553 and batch: 300, loss is 5.244815034866333 and perplexity is 189.5807461582881
At time: 1471.162817955017 and batch: 350, loss is 5.216417179107666 and perplexity is 184.2727835595843
At time: 1472.7538692951202 and batch: 400, loss is 5.239107990264893 and perplexity is 188.5018818759257
At time: 1474.3531823158264 and batch: 450, loss is 5.262657299041748 and perplexity is 192.99365236080146
At time: 1475.9471616744995 and batch: 500, loss is 5.262193603515625 and perplexity is 192.90418281254938
At time: 1477.548733472824 and batch: 550, loss is 5.252010850906372 and perplexity is 190.9498543500496
At time: 1479.1492400169373 and batch: 600, loss is 5.2630917263031005 and perplexity is 193.07751227885342
At time: 1480.75456905365 and batch: 650, loss is 5.272611522674561 and perplexity is 194.92434768046556
At time: 1482.3472356796265 and batch: 700, loss is 5.239306268692016 and perplexity is 188.5392614382318
At time: 1483.9453501701355 and batch: 750, loss is 5.226007766723633 and perplexity is 186.0485696402612
At time: 1485.5370607376099 and batch: 800, loss is 5.229059934616089 and perplexity is 186.61728858229935
At time: 1487.1286103725433 and batch: 850, loss is 5.240703191757202 and perplexity is 188.80282032411236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.086207389831543 and perplexity of 161.7751470354575
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd131070898>
SETTINGS FOR THIS RUN
{'seq_len': 50, 'lr': 2.6058867308125433, 'dropout': 0.641160805004905, 'wordvec_source': 'glove', 'num_layers': 1, 'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 2.1345853047413064, 'batch_size': 50, 'data': 'wikitext'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.0454230308532715 and batch: 50, loss is 7.522045822143554 and perplexity is 1848.3448134175867
At time: 3.6117043495178223 and batch: 100, loss is 6.6554890060424805 and perplexity is 777.0378063790853
At time: 5.177881717681885 and batch: 150, loss is 6.4552880859375 and perplexity is 636.0569389205544
At time: 6.749659776687622 and batch: 200, loss is 6.403549633026123 and perplexity is 603.9851629773859
At time: 8.321403503417969 and batch: 250, loss is 6.347844219207763 and perplexity is 571.259869571136
At time: 9.893179416656494 and batch: 300, loss is 6.232728443145752 and perplexity is 509.1427571524872
At time: 11.46104645729065 and batch: 350, loss is 6.165665588378906 and perplexity is 476.1179360619952
At time: 13.041192531585693 and batch: 400, loss is 6.1579827785491945 and perplexity is 472.47402815209693
At time: 14.626456499099731 and batch: 450, loss is 6.117159452438354 and perplexity is 453.5744630501263
At time: 16.24663543701172 and batch: 500, loss is 6.088763828277588 and perplexity is 440.8760756134719
At time: 17.829155206680298 and batch: 550, loss is 6.0246679306030275 and perplexity is 413.50430709602455
At time: 19.407427072525024 and batch: 600, loss is 6.025857048034668 and perplexity is 413.996304739164
At time: 20.986836194992065 and batch: 650, loss is 6.023315191268921 and perplexity is 412.945321720986
At time: 22.569321632385254 and batch: 700, loss is 5.959270677566528 and perplexity is 387.3275345407648
At time: 24.1474392414093 and batch: 750, loss is 5.930399494171143 and perplexity is 376.30481538076003
At time: 25.726028442382812 and batch: 800, loss is 5.934647560119629 and perplexity is 377.90678327676466
At time: 27.315035343170166 and batch: 850, loss is 5.92115777015686 and perplexity is 372.8431307371662
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.295223236083984 and perplexity of 199.38213028488232
Finished 1 epochs...
Completing Train Step...
At time: 31.422288179397583 and batch: 50, loss is 5.574499912261963 and perplexity is 263.6176906930212
At time: 32.9877347946167 and batch: 100, loss is 5.420620746612549 and perplexity is 226.01937972976776
At time: 34.55257248878479 and batch: 150, loss is 5.3546850299835205 and perplexity is 211.59731904485187
At time: 36.121054887771606 and batch: 200, loss is 5.346562986373901 and perplexity is 209.88567681769698
At time: 37.689373254776 and batch: 250, loss is 5.350459079742432 and perplexity is 210.7050060665024
At time: 39.29110288619995 and batch: 300, loss is 5.282002086639404 and perplexity is 196.76341867808452
At time: 40.85507130622864 and batch: 350, loss is 5.225630588531494 and perplexity is 185.9784094094006
At time: 42.42280578613281 and batch: 400, loss is 5.217168684005737 and perplexity is 184.41131750696064
At time: 43.99037003517151 and batch: 450, loss is 5.19807222366333 and perplexity is 180.92312615340163
At time: 45.560038805007935 and batch: 500, loss is 5.17734733581543 and perplexity is 177.21210275588658
At time: 47.12542653083801 and batch: 550, loss is 5.154611797332763 and perplexity is 173.2285459207435
At time: 48.7022008895874 and batch: 600, loss is 5.168002510070801 and perplexity is 175.56380008135153
At time: 50.277605295181274 and batch: 650, loss is 5.159887046813965 and perplexity is 174.14478428594606
At time: 51.845731019973755 and batch: 700, loss is 5.096314830780029 and perplexity is 163.41857119633727
At time: 53.41847467422485 and batch: 750, loss is 5.072051134109497 and perplexity is 159.50115029974123
At time: 54.986818075180054 and batch: 800, loss is 5.050002756118775 and perplexity is 156.02289450343125
At time: 56.5513973236084 and batch: 850, loss is 5.045163173675537 and perplexity is 155.26963304811892
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.770480791727702 and perplexity of 117.97595018807318
Finished 2 epochs...
Completing Train Step...
At time: 60.63796544075012 and batch: 50, loss is 5.004606790542603 and perplexity is 149.09844471200358
At time: 62.20289659500122 and batch: 100, loss is 4.928055019378662 and perplexity is 138.11062844181936
At time: 63.76770830154419 and batch: 150, loss is 4.913119554519653 and perplexity is 136.06320963920552
At time: 65.36956644058228 and batch: 200, loss is 4.936515445709229 and perplexity is 139.28406010589808
At time: 66.97346329689026 and batch: 250, loss is 4.953799390792847 and perplexity is 141.71236302772604
At time: 68.58787226676941 and batch: 300, loss is 4.913470735549927 and perplexity is 136.11100084854087
At time: 70.15395450592041 and batch: 350, loss is 4.864793519973755 and perplexity is 129.64416691700652
At time: 71.71934223175049 and batch: 400, loss is 4.875864915847778 and perplexity is 131.08748383524147
At time: 73.2861716747284 and batch: 450, loss is 4.878859243392944 and perplexity is 131.48059095093183
At time: 74.85288834571838 and batch: 500, loss is 4.8625296783447265 and perplexity is 129.35100501624058
At time: 76.4183177947998 and batch: 550, loss is 4.864231300354004 and perplexity is 129.57129890861088
At time: 77.9857850074768 and batch: 600, loss is 4.8879701519012455 and perplexity is 132.68397220973458
At time: 79.55351853370667 and batch: 650, loss is 4.881424617767334 and perplexity is 131.8183209063527
At time: 81.12417888641357 and batch: 700, loss is 4.829172105789184 and perplexity is 125.10734212381783
At time: 82.69137763977051 and batch: 750, loss is 4.8194427490234375 and perplexity is 123.89603035570869
At time: 84.25839138031006 and batch: 800, loss is 4.796754674911499 and perplexity is 121.11671590265628
At time: 85.82552552223206 and batch: 850, loss is 4.803098039627075 and perplexity is 121.88744532930457
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.629325230916341 and perplexity of 102.44491412215233
Finished 3 epochs...
Completing Train Step...
At time: 89.9095528125763 and batch: 50, loss is 4.780289754867554 and perplexity is 119.1388661103437
At time: 91.48236536979675 and batch: 100, loss is 4.715945806503296 and perplexity is 111.7144214487109
At time: 93.05302381515503 and batch: 150, loss is 4.707292613983154 and perplexity is 110.75190547655528
At time: 94.62615394592285 and batch: 200, loss is 4.742508897781372 and perplexity is 114.72166585319151
At time: 96.19716095924377 and batch: 250, loss is 4.755352869033813 and perplexity is 116.20465095345516
At time: 97.7729799747467 and batch: 300, loss is 4.7251971817016605 and perplexity is 112.75272895722767
At time: 99.34455609321594 and batch: 350, loss is 4.677879028320312 and perplexity is 107.54173753123298
At time: 100.91424584388733 and batch: 400, loss is 4.691332740783691 and perplexity is 108.99834959494048
At time: 102.48473024368286 and batch: 450, loss is 4.705639190673828 and perplexity is 110.5689369983449
At time: 104.05509543418884 and batch: 500, loss is 4.687962274551392 and perplexity is 108.63159275622228
At time: 105.62727212905884 and batch: 550, loss is 4.6995541381835935 and perplexity is 109.89816213283213
At time: 107.21177577972412 and batch: 600, loss is 4.726724615097046 and perplexity is 112.9250828368844
At time: 108.81196784973145 and batch: 650, loss is 4.716556777954102 and perplexity is 111.78269662582753
At time: 110.38381028175354 and batch: 700, loss is 4.672087116241455 and perplexity is 106.92066557594276
At time: 111.96132135391235 and batch: 750, loss is 4.667455568313598 and perplexity is 106.42660241065265
At time: 113.53852200508118 and batch: 800, loss is 4.643759880065918 and perplexity is 103.93439471742202
At time: 115.11431908607483 and batch: 850, loss is 4.653205547332764 and perplexity is 104.92077560655959
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.552671750386556 and perplexity of 94.8855805443205
Finished 4 epochs...
Completing Train Step...
At time: 119.21357011795044 and batch: 50, loss is 4.63378719329834 and perplexity is 102.90304078709087
At time: 120.78189253807068 and batch: 100, loss is 4.577228078842163 and perplexity is 97.24446627686183
At time: 122.35063910484314 and batch: 150, loss is 4.571304368972778 and perplexity is 96.67012107888968
At time: 123.91887187957764 and batch: 200, loss is 4.609745244979859 and perplexity is 100.4585540552647
At time: 125.48870325088501 and batch: 250, loss is 4.62108793258667 and perplexity is 101.60451088027158
At time: 127.07430386543274 and batch: 300, loss is 4.596468276977539 and perplexity is 99.13358430154013
At time: 128.66706919670105 and batch: 350, loss is 4.551424312591553 and perplexity is 94.76729048003646
At time: 130.25638246536255 and batch: 400, loss is 4.563113155364991 and perplexity is 95.88150971907545
At time: 131.85236716270447 and batch: 450, loss is 4.584552783966064 and perplexity is 97.95936834440673
At time: 133.45138144493103 and batch: 500, loss is 4.566556987762451 and perplexity is 96.2122787982894
At time: 135.0462191104889 and batch: 550, loss is 4.582058420181275 and perplexity is 97.71532653468628
At time: 136.63614583015442 and batch: 600, loss is 4.611812896728516 and perplexity is 100.66648224769236
At time: 138.22658586502075 and batch: 650, loss is 4.59959939956665 and perplexity is 99.44447016356699
At time: 139.8160240650177 and batch: 700, loss is 4.558124437332153 and perplexity is 95.4043750372692
At time: 141.40658116340637 and batch: 750, loss is 4.555875978469849 and perplexity is 95.1901032056855
At time: 142.99778175354004 and batch: 800, loss is 4.5317947292327885 and perplexity is 92.92518706281052
At time: 144.58754348754883 and batch: 850, loss is 4.5432470703125 and perplexity is 93.99551518298517
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.505560239156087 and perplexity of 90.51904215779848
Finished 5 epochs...
Completing Train Step...
At time: 148.72310757637024 and batch: 50, loss is 4.524894552230835 and perplexity is 92.28619394228808
At time: 150.29216814041138 and batch: 100, loss is 4.474177026748658 and perplexity is 87.72237752483278
At time: 151.86210370063782 and batch: 150, loss is 4.469993972778321 and perplexity is 87.35619649669295
At time: 153.4298174381256 and batch: 200, loss is 4.51164547920227 and perplexity is 91.07155162950302
At time: 154.99786019325256 and batch: 250, loss is 4.516465291976929 and perplexity is 91.51155898218217
At time: 156.56719326972961 and batch: 300, loss is 4.498433828353882 and perplexity is 89.8762593654246
At time: 158.15111207962036 and batch: 350, loss is 4.454364004135132 and perplexity is 86.00143688982337
At time: 159.74035000801086 and batch: 400, loss is 4.465504140853882 and perplexity is 86.9648610294988
At time: 161.32963132858276 and batch: 450, loss is 4.4917482662200925 and perplexity is 89.27739016832157
At time: 162.91934084892273 and batch: 500, loss is 4.472009477615356 and perplexity is 87.53244088432238
At time: 164.50785040855408 and batch: 550, loss is 4.4907848834991455 and perplexity is 89.19142328941389
At time: 166.10802793502808 and batch: 600, loss is 4.52306583404541 and perplexity is 92.11758271935629
At time: 167.70511865615845 and batch: 650, loss is 4.5090767288208005 and perplexity is 90.83791175624273
At time: 169.2939488887787 and batch: 700, loss is 4.471050291061402 and perplexity is 87.44852119774293
At time: 170.88189697265625 and batch: 750, loss is 4.468690595626831 and perplexity is 87.24241259391559
At time: 172.47013926506042 and batch: 800, loss is 4.4445999908447265 and perplexity is 85.16580392155292
At time: 174.0624144077301 and batch: 850, loss is 4.456958456039429 and perplexity is 86.22485317761979
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.471829096476237 and perplexity of 87.51665310687727
Finished 6 epochs...
Completing Train Step...
At time: 178.34105563163757 and batch: 50, loss is 4.439402742385864 and perplexity is 84.72432431173252
At time: 179.99921798706055 and batch: 100, loss is 4.392861270904541 and perplexity is 80.87148347645433
At time: 181.60524439811707 and batch: 150, loss is 4.389248685836792 and perplexity is 80.57985544553961
At time: 183.21129298210144 and batch: 200, loss is 4.429292249679565 and perplexity is 83.87203543916237
At time: 184.81353044509888 and batch: 250, loss is 4.433230037689209 and perplexity is 84.20295685624298
At time: 186.4163203239441 and batch: 300, loss is 4.418504867553711 and perplexity is 82.9721382372596
At time: 188.02227568626404 and batch: 350, loss is 4.37649905204773 and perplexity is 79.55901330792786
At time: 189.68001508712769 and batch: 400, loss is 4.3868966484069825 and perplexity is 80.39055132190572
At time: 191.284029006958 and batch: 450, loss is 4.415902080535889 and perplexity is 82.75646023672385
At time: 192.88941502571106 and batch: 500, loss is 4.395198078155517 and perplexity is 81.06068552369977
At time: 194.47534561157227 and batch: 550, loss is 4.4174618434906 and perplexity is 82.88564141748425
At time: 196.04475903511047 and batch: 600, loss is 4.451074676513672 and perplexity is 85.7190147321215
At time: 197.6147153377533 and batch: 650, loss is 4.43534423828125 and perplexity is 84.38116711721862
At time: 199.19779872894287 and batch: 700, loss is 4.39982084274292 and perplexity is 81.43627745785066
At time: 200.7989683151245 and batch: 750, loss is 4.396530485153198 and perplexity is 81.16876333415453
At time: 202.40262150764465 and batch: 800, loss is 4.37334527015686 and perplexity is 79.30849677739405
At time: 204.0057430267334 and batch: 850, loss is 4.385474681854248 and perplexity is 80.27631988266091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.450352668762207 and perplexity of 85.65714727611596
Finished 7 epochs...
Completing Train Step...
At time: 208.1532883644104 and batch: 50, loss is 4.369256515502929 and perplexity is 78.98488582583292
At time: 209.7750208377838 and batch: 100, loss is 4.326071481704712 and perplexity is 75.64652331712401
At time: 211.35475063323975 and batch: 150, loss is 4.322264461517334 and perplexity is 75.35908296852129
At time: 212.9312129020691 and batch: 200, loss is 4.363113870620728 and perplexity is 78.50119680696379
At time: 214.50523018836975 and batch: 250, loss is 4.364743604660034 and perplexity is 78.62923718706934
At time: 216.07841181755066 and batch: 300, loss is 4.35225552558899 and perplexity is 77.6534148110412
At time: 217.64981079101562 and batch: 350, loss is 4.310978269577026 and perplexity is 74.51334743778993
At time: 219.22490167617798 and batch: 400, loss is 4.321402587890625 and perplexity is 75.29416094368705
At time: 220.80457997322083 and batch: 450, loss is 4.352855243682861 and perplexity is 77.69999893622948
At time: 222.4024953842163 and batch: 500, loss is 4.330357608795166 and perplexity is 75.97144977061744
At time: 224.0132236480713 and batch: 550, loss is 4.355487194061279 and perplexity is 77.9047708343628
At time: 225.62871098518372 and batch: 600, loss is 4.390594387054444 and perplexity is 80.68836484937422
At time: 227.23581886291504 and batch: 650, loss is 4.373564853668213 and perplexity is 79.32591352774214
At time: 228.88128662109375 and batch: 700, loss is 4.340565519332886 and perplexity is 76.75093120308227
At time: 230.46434664726257 and batch: 750, loss is 4.337340154647827 and perplexity is 76.50378025027081
At time: 232.0530867576599 and batch: 800, loss is 4.313009347915649 and perplexity is 74.66484368199879
At time: 233.64628982543945 and batch: 850, loss is 4.325600204467773 and perplexity is 75.61088123194244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.4365590413411455 and perplexity of 84.48373590505508
Finished 8 epochs...
Completing Train Step...
At time: 237.7782473564148 and batch: 50, loss is 4.310060367584229 and perplexity is 74.44498286847401
At time: 239.374165058136 and batch: 100, loss is 4.268749141693116 and perplexity is 71.43222811689904
At time: 240.94318795204163 and batch: 150, loss is 4.266025476455688 and perplexity is 71.23793535467723
At time: 242.51268815994263 and batch: 200, loss is 4.306417360305786 and perplexity is 74.17427265307732
At time: 244.08212757110596 and batch: 250, loss is 4.306988525390625 and perplexity is 74.21665050903385
At time: 245.65283012390137 and batch: 300, loss is 4.295029239654541 and perplexity is 73.33435868498941
At time: 247.2457411289215 and batch: 350, loss is 4.255149297714233 and perplexity is 70.46733701546444
At time: 248.84532928466797 and batch: 400, loss is 4.265697164535522 and perplexity is 71.21455093023499
At time: 250.44648098945618 and batch: 450, loss is 4.299036026000977 and perplexity is 73.62878324635014
At time: 252.047434091568 and batch: 500, loss is 4.275229616165161 and perplexity is 71.89664604663287
At time: 253.65775537490845 and batch: 550, loss is 4.302892589569092 and perplexity is 73.91328557742642
At time: 255.26854634284973 and batch: 600, loss is 4.339772052764893 and perplexity is 76.6900560594998
At time: 256.8720865249634 and batch: 650, loss is 4.319640998840332 and perplexity is 75.16164033191674
At time: 258.4809637069702 and batch: 700, loss is 4.290391969680786 and perplexity is 72.99507474902364
At time: 260.0895392894745 and batch: 750, loss is 4.285688514709473 and perplexity is 72.65255185377586
At time: 261.67621326446533 and batch: 800, loss is 4.260687685012817 and perplexity is 70.85869516589322
At time: 263.2616081237793 and batch: 850, loss is 4.273688945770264 and perplexity is 71.78596229805757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.427639961242676 and perplexity of 83.73356906952036
Finished 9 epochs...
Completing Train Step...
At time: 267.4183735847473 and batch: 50, loss is 4.259090375900269 and perplexity is 70.74560227260136
At time: 268.987291097641 and batch: 100, loss is 4.218704309463501 and perplexity is 67.94539103078593
At time: 270.5911340713501 and batch: 150, loss is 4.216184206008911 and perplexity is 67.77437719291555
At time: 272.1654052734375 and batch: 200, loss is 4.256672697067261 and perplexity is 70.57476872100193
At time: 273.7356207370758 and batch: 250, loss is 4.258563632965088 and perplexity is 70.70834733915706
At time: 275.314076423645 and batch: 300, loss is 4.2453342533111575 and perplexity is 69.77908013487855
At time: 276.89321208000183 and batch: 350, loss is 4.20639778137207 and perplexity is 67.11434331196298
At time: 278.48998975753784 and batch: 400, loss is 4.217814216613769 and perplexity is 67.88494023146042
At time: 280.0819265842438 and batch: 450, loss is 4.252044696807861 and perplexity is 70.24890330728938
At time: 281.679146528244 and batch: 500, loss is 4.2272541713714595 and perplexity is 68.5288052428884
At time: 283.27780199050903 and batch: 550, loss is 4.2559410762786865 and perplexity is 70.5231536366935
At time: 284.8716154098511 and batch: 600, loss is 4.2937071418762205 and perplexity is 73.23746755618383
At time: 286.4822521209717 and batch: 650, loss is 4.272294664382935 and perplexity is 71.68594221123003
At time: 288.0857181549072 and batch: 700, loss is 4.245381469726563 and perplexity is 69.7823749306964
At time: 289.69017243385315 and batch: 750, loss is 4.240886783599853 and perplexity is 69.46942888218769
At time: 291.278995513916 and batch: 800, loss is 4.215260734558106 and perplexity is 67.7118183805662
At time: 292.8732101917267 and batch: 850, loss is 4.2290701580047605 and perplexity is 68.65336570303953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.421703020731608 and perplexity of 83.23792062469431
Finished 10 epochs...
Completing Train Step...
At time: 296.9792048931122 and batch: 50, loss is 4.21483959197998 and perplexity is 67.6833080546813
At time: 298.55030393600464 and batch: 100, loss is 4.176770567893982 and perplexity is 65.15509912018877
At time: 300.1213526725769 and batch: 150, loss is 4.174243326187134 and perplexity is 64.9906443323082
At time: 301.69385981559753 and batch: 200, loss is 4.213402299880982 and perplexity is 67.58609724767756
At time: 303.2892882823944 and batch: 250, loss is 4.215107440948486 and perplexity is 67.70143938704977
At time: 304.8916914463043 and batch: 300, loss is 4.202675666809082 and perplexity is 66.8650003667092
At time: 306.4969313144684 and batch: 350, loss is 4.163399367332459 and perplexity is 64.28969585622855
At time: 308.1029005050659 and batch: 400, loss is 4.175790452957154 and perplexity is 65.09127091892685
At time: 309.723938703537 and batch: 450, loss is 4.210992994308472 and perplexity is 67.42345768982462
At time: 311.30445766448975 and batch: 500, loss is 4.185163850784302 and perplexity is 65.70426573027392
At time: 312.89395356178284 and batch: 550, loss is 4.213807063102722 and perplexity is 67.61345915131697
At time: 314.4807798862457 and batch: 600, loss is 4.253007802963257 and perplexity is 70.31659304944586
At time: 316.06131196022034 and batch: 650, loss is 4.229736294746399 and perplexity is 68.69911346781326
At time: 317.65336084365845 and batch: 700, loss is 4.205581707954407 and perplexity is 67.05959542262913
At time: 319.2810342311859 and batch: 750, loss is 4.200752258300781 and perplexity is 66.73651526038057
At time: 320.91017270088196 and batch: 800, loss is 4.17375195980072 and perplexity is 64.9587179586677
At time: 322.53861117362976 and batch: 850, loss is 4.189177284240722 and perplexity is 65.96849530782238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.418801943461101 and perplexity of 82.99679092219255
Finished 11 epochs...
Completing Train Step...
At time: 326.9359791278839 and batch: 50, loss is 4.175107564926147 and perplexity is 65.04683604284264
At time: 328.5418469905853 and batch: 100, loss is 4.140607810020446 and perplexity is 62.84100523645953
At time: 330.1474344730377 and batch: 150, loss is 4.136469016075134 and perplexity is 62.58145674383148
At time: 331.75333547592163 and batch: 200, loss is 4.1749161434173585 and perplexity is 65.03438587099873
At time: 333.35948371887207 and batch: 250, loss is 4.175262055397034 and perplexity is 65.0568859354589
At time: 334.96739315986633 and batch: 300, loss is 4.163679308891297 and perplexity is 64.30769573324302
At time: 336.5741629600525 and batch: 350, loss is 4.12559130191803 and perplexity is 61.904402622441296
At time: 338.1822235584259 and batch: 400, loss is 4.138955450057983 and perplexity is 62.73725501536728
At time: 339.78013610839844 and batch: 450, loss is 4.173613963127136 and perplexity is 64.94975449014775
At time: 341.3547422885895 and batch: 500, loss is 4.148261184692383 and perplexity is 63.323796127497644
At time: 342.93711495399475 and batch: 550, loss is 4.17626341342926 and perplexity is 65.1220637984841
At time: 344.50801372528076 and batch: 600, loss is 4.217268381118775 and perplexity is 67.84789633236372
At time: 346.10133695602417 and batch: 650, loss is 4.193022003173828 and perplexity is 66.22261382482897
At time: 347.69682121276855 and batch: 700, loss is 4.170817651748657 and perplexity is 64.76838844878061
At time: 349.3101315498352 and batch: 750, loss is 4.165353856086731 and perplexity is 64.41547221799837
At time: 350.98808336257935 and batch: 800, loss is 4.137291121482849 and perplexity is 62.632926451707
At time: 352.58829498291016 and batch: 850, loss is 4.153941307067871 and perplexity is 63.68450650851229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.417158444722493 and perplexity of 82.86049783044395
Finished 12 epochs...
Completing Train Step...
At time: 356.8180227279663 and batch: 50, loss is 4.140552515983582 and perplexity is 62.8375305996636
At time: 358.4139895439148 and batch: 100, loss is 4.105788817405701 and perplexity is 60.690599471951856
At time: 359.98300218582153 and batch: 150, loss is 4.100603427886963 and perplexity is 60.37670959752569
At time: 361.55164527893066 and batch: 200, loss is 4.140733394622803 and perplexity is 62.848897594682796
At time: 363.1202154159546 and batch: 250, loss is 4.13992290019989 and perplexity is 62.79797955087869
At time: 364.6956396102905 and batch: 300, loss is 4.129445323944092 and perplexity is 62.14344389293624
At time: 366.2732379436493 and batch: 350, loss is 4.090969710350037 and perplexity is 59.79785019237203
At time: 367.85908579826355 and batch: 400, loss is 4.105222897529602 and perplexity is 60.65626317213199
At time: 369.4500644207001 and batch: 450, loss is 4.140446548461914 and perplexity is 62.83087221507073
At time: 371.0401875972748 and batch: 500, loss is 4.114325485229492 and perplexity is 61.21091267033841
At time: 372.62831711769104 and batch: 550, loss is 4.14248815536499 and perplexity is 62.959279191194035
At time: 374.21819734573364 and batch: 600, loss is 4.184536237716674 and perplexity is 65.66304181219074
At time: 375.81134486198425 and batch: 650, loss is 4.16026424407959 and perplexity is 64.08845535754593
At time: 377.39853048324585 and batch: 700, loss is 4.138745265007019 and perplexity is 62.724069967923015
At time: 378.9862859249115 and batch: 750, loss is 4.13320095539093 and perplexity is 62.37727057361376
At time: 380.5804624557495 and batch: 800, loss is 4.104137868881225 and perplexity is 60.59048508089677
At time: 382.174067735672 and batch: 850, loss is 4.12179497718811 and perplexity is 61.669838929561344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.4159955978393555 and perplexity of 82.76419975961363
Finished 13 epochs...
Completing Train Step...
At time: 386.2662696838379 and batch: 50, loss is 4.107557015419006 and perplexity is 60.79800740062307
At time: 387.8597912788391 and batch: 100, loss is 4.072541122436523 and perplexity is 58.70595222362469
At time: 389.4384582042694 and batch: 150, loss is 4.069014725685119 and perplexity is 58.49929633389959
At time: 391.0328447818756 and batch: 200, loss is 4.1106693029403685 and perplexity is 60.98752304098506
At time: 392.6010043621063 and batch: 250, loss is 4.107464632987976 and perplexity is 60.79239099233032
At time: 394.17422103881836 and batch: 300, loss is 4.0980203819274905 and perplexity is 60.220955028959246
At time: 395.7620918750763 and batch: 350, loss is 4.059514436721802 and perplexity is 57.94616771566534
At time: 397.35795521736145 and batch: 400, loss is 4.076163291931152 and perplexity is 58.9189807126564
At time: 398.94706439971924 and batch: 450, loss is 4.11025269985199 and perplexity is 60.962120742239065
At time: 400.5477330684662 and batch: 500, loss is 4.084109983444214 and perplexity is 59.38905697840961
At time: 402.14374828338623 and batch: 550, loss is 4.112003846168518 and perplexity is 61.068967860589446
At time: 403.73217844963074 and batch: 600, loss is 4.155584354400634 and perplexity is 63.789229175659756
At time: 405.31999039649963 and batch: 650, loss is 4.1305474233627315 and perplexity is 62.21196990062344
At time: 406.9207253456116 and batch: 700, loss is 4.109237713813782 and perplexity is 60.90027643169133
At time: 408.517285823822 and batch: 750, loss is 4.104511818885803 and perplexity is 60.61314713004275
At time: 410.10817694664 and batch: 800, loss is 4.073489665985107 and perplexity is 58.76166379411814
At time: 411.7024025917053 and batch: 850, loss is 4.092652215957641 and perplexity is 59.8985450967599
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.416077295939128 and perplexity of 82.77096171367882
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 415.86569595336914 and batch: 50, loss is 4.087028732299805 and perplexity is 59.56265193730447
At time: 417.4735481739044 and batch: 100, loss is 4.0505522441864015 and perplexity is 57.42916321138088
At time: 419.05767726898193 and batch: 150, loss is 4.044306492805481 and perplexity is 57.07159274644451
At time: 420.63866686820984 and batch: 200, loss is 4.080127248764038 and perplexity is 59.15299651668847
At time: 422.2115476131439 and batch: 250, loss is 4.069223990440369 and perplexity is 58.51153945581146
At time: 423.78835940361023 and batch: 300, loss is 4.0580890226364135 and perplexity is 57.863629271714615
At time: 425.3647892475128 and batch: 350, loss is 4.012375283241272 and perplexity is 55.27801569267585
At time: 426.9668688774109 and batch: 400, loss is 4.025531678199768 and perplexity is 56.01008020514674
At time: 428.55951476097107 and batch: 450, loss is 4.056347661018371 and perplexity is 57.76295544882556
At time: 430.1589996814728 and batch: 500, loss is 4.02738254070282 and perplexity is 56.11384315824924
At time: 431.77738881111145 and batch: 550, loss is 4.049939341545105 and perplexity is 57.39397550998028
At time: 433.37665843963623 and batch: 600, loss is 4.088240733146668 and perplexity is 59.63488568673433
At time: 434.9832775592804 and batch: 650, loss is 4.060453128814697 and perplexity is 58.00058686252914
At time: 436.58173394203186 and batch: 700, loss is 4.033653964996338 and perplexity is 56.46686268801892
At time: 438.17400598526 and batch: 750, loss is 4.026927013397216 and perplexity is 56.08828759153945
At time: 439.76553654670715 and batch: 800, loss is 3.986884369850159 and perplexity is 53.88673640751316
At time: 441.3572897911072 and batch: 850, loss is 4.003898620605469 and perplexity is 54.811422970886575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3840376536051435 and perplexity of 80.1610433908806
Finished 15 epochs...
Completing Train Step...
At time: 445.4589195251465 and batch: 50, loss is 4.046537799835205 and perplexity is 57.19907917032073
At time: 447.0267581939697 and batch: 100, loss is 4.009140214920044 and perplexity is 55.09947648422778
At time: 448.595584154129 and batch: 150, loss is 4.006600732803345 and perplexity is 54.9597298661771
At time: 450.1667742729187 and batch: 200, loss is 4.047515325546264 and perplexity is 57.25502007824198
At time: 451.73662400245667 and batch: 250, loss is 4.038197221755982 and perplexity is 56.72398979857991
At time: 453.31021785736084 and batch: 300, loss is 4.0294642782211305 and perplexity is 56.23077912361177
At time: 454.8888807296753 and batch: 350, loss is 3.9851766633987427 and perplexity is 53.79479220929896
At time: 456.4767858982086 and batch: 400, loss is 3.9998557233810423 and perplexity is 54.59027336487931
At time: 458.06979608535767 and batch: 450, loss is 4.034189434051513 and perplexity is 56.49710704236646
At time: 459.6567904949188 and batch: 500, loss is 4.007253837585449 and perplexity is 54.99563605255059
At time: 461.2443165779114 and batch: 550, loss is 4.031970767974854 and perplexity is 56.37189777767783
At time: 462.83147954940796 and batch: 600, loss is 4.07206353187561 and perplexity is 58.67792150910841
At time: 464.4192440509796 and batch: 650, loss is 4.046753630638123 and perplexity is 57.21142582585058
At time: 466.01829385757446 and batch: 700, loss is 4.0209617424011235 and perplexity is 55.7547017106446
At time: 467.6095588207245 and batch: 750, loss is 4.017600474357605 and perplexity is 55.567609822649736
At time: 469.19753408432007 and batch: 800, loss is 3.978465223312378 and perplexity is 53.43496052947797
At time: 470.8386764526367 and batch: 850, loss is 3.9963372325897217 and perplexity is 54.398535502506824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.383607864379883 and perplexity of 80.12659844070998
Finished 16 epochs...
Completing Train Step...
At time: 474.9871196746826 and batch: 50, loss is 4.024486269950867 and perplexity is 55.951557400719736
At time: 476.5554258823395 and batch: 100, loss is 3.9879362392425537 and perplexity is 53.943448037581256
At time: 478.1240301132202 and batch: 150, loss is 3.9867730808258055 and perplexity is 53.88073973888068
At time: 479.69322681427 and batch: 200, loss is 4.028475947380066 and perplexity is 56.17523196439365
At time: 481.2640850543976 and batch: 250, loss is 4.019397134780884 and perplexity is 55.66753568757775
At time: 482.851788520813 and batch: 300, loss is 4.011737675666809 and perplexity is 55.2427812452379
At time: 484.44965171813965 and batch: 350, loss is 3.9678291511535644 and perplexity is 52.86963418877663
At time: 486.0429177284241 and batch: 400, loss is 3.982703528404236 and perplexity is 53.66191480576981
At time: 487.6306846141815 and batch: 450, loss is 4.018698315620423 and perplexity is 55.628647736428874
At time: 489.22418189048767 and batch: 500, loss is 3.992097330093384 and perplexity is 54.16837928079087
At time: 490.8174407482147 and batch: 550, loss is 4.0175106859207155 and perplexity is 55.56262071780741
At time: 492.4047439098358 and batch: 600, loss is 4.058765635490418 and perplexity is 57.90279379517505
At time: 493.9988763332367 and batch: 650, loss is 4.034574055671692 and perplexity is 56.51884123066393
At time: 495.595246553421 and batch: 700, loss is 4.008822069168091 and perplexity is 55.081949608047935
At time: 497.18868613243103 and batch: 750, loss is 4.007279810905456 and perplexity is 54.997064490355314
At time: 498.77889919281006 and batch: 800, loss is 3.968009843826294 and perplexity is 52.87918820742903
At time: 500.375367641449 and batch: 850, loss is 3.9862332439422605 and perplexity is 53.851660777910965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384020805358887 and perplexity of 80.15969282925867
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 504.5175290107727 and batch: 50, loss is 4.018237719535827 and perplexity is 55.60303129895841
At time: 506.09238028526306 and batch: 100, loss is 3.9858358335494994 and perplexity is 53.83026382021867
At time: 507.67689847946167 and batch: 150, loss is 3.985411596298218 and perplexity is 53.807431860486176
At time: 509.249751329422 and batch: 200, loss is 4.026655683517456 and perplexity is 56.07307122763171
At time: 510.8270494937897 and batch: 250, loss is 4.0115902137756345 and perplexity is 55.23463564083962
At time: 512.4329092502594 and batch: 300, loss is 4.000605130195618 and perplexity is 54.63119902081587
At time: 514.027987241745 and batch: 350, loss is 3.9531336069107055 and perplexity is 52.09836711527606
At time: 515.6221470832825 and batch: 400, loss is 3.96418728351593 and perplexity is 52.67744016403599
At time: 517.2231643199921 and batch: 450, loss is 4.000333943367004 and perplexity is 54.61638576788038
At time: 518.8161933422089 and batch: 500, loss is 3.9717745685577395 and perplexity is 53.078638998144065
At time: 520.4085121154785 and batch: 550, loss is 3.992184886932373 and perplexity is 54.17312230049273
At time: 522.003157377243 and batch: 600, loss is 4.0292513465881346 and perplexity is 56.218807086646265
At time: 523.6002824306488 and batch: 650, loss is 4.005056142807007 and perplexity is 54.874905143797456
At time: 525.1978597640991 and batch: 700, loss is 3.978407287597656 and perplexity is 53.43186482652532
At time: 526.8141505718231 and batch: 750, loss is 3.973793134689331 and perplexity is 53.18588995129035
At time: 528.4247620105743 and batch: 800, loss is 3.930992031097412 and perplexity is 50.95750403184233
At time: 530.0200159549713 and batch: 850, loss is 3.9478437519073486 and perplexity is 51.82350194672059
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.366252263387044 and perplexity of 78.74795139904909
Finished 18 epochs...
Completing Train Step...
At time: 534.1347334384918 and batch: 50, loss is 4.00007173538208 and perplexity is 54.602066792781166
At time: 535.7685708999634 and batch: 100, loss is 3.9640232801437376 and perplexity is 52.66880159460724
At time: 537.3606653213501 and batch: 150, loss is 3.9636788463592527 and perplexity is 52.65066380376301
At time: 538.9666295051575 and batch: 200, loss is 4.008237118721008 and perplexity is 55.04973881877476
At time: 540.5737097263336 and batch: 250, loss is 3.994050078392029 and perplexity is 54.27425983666393
At time: 542.1817314624786 and batch: 300, loss is 3.9849033069610598 and perplexity is 53.7800890662257
At time: 543.7785227298737 and batch: 350, loss is 3.9390257215499878 and perplexity is 51.36852966117038
At time: 545.3669021129608 and batch: 400, loss is 3.9511685276031496 and perplexity is 51.99609021612164
At time: 546.9437386989594 and batch: 450, loss is 3.9889101648330687 and perplexity is 53.99601053389104
At time: 548.5304226875305 and batch: 500, loss is 3.96189968585968 and perplexity is 52.55707310357328
At time: 550.1396262645721 and batch: 550, loss is 3.9836091136932374 and perplexity is 53.71053225669922
At time: 551.7910661697388 and batch: 600, loss is 4.021856880187988 and perplexity is 55.80463219493877
At time: 553.3994877338409 and batch: 650, loss is 3.9994007682800294 and perplexity is 54.565442890350084
At time: 555.0036256313324 and batch: 700, loss is 3.973880944252014 and perplexity is 53.19056038607931
At time: 556.6069355010986 and batch: 750, loss is 3.9703533267974853 and perplexity is 53.003255001953356
At time: 558.2196478843689 and batch: 800, loss is 3.929045581817627 and perplexity is 50.85841430267071
At time: 559.8286437988281 and batch: 850, loss is 3.9461317825317384 and perplexity is 51.73485759830602
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.366153081258138 and perplexity of 78.74014139689505
Finished 19 epochs...
Completing Train Step...
At time: 563.9635508060455 and batch: 50, loss is 3.9884331035614013 and perplexity is 53.97025727187117
At time: 565.5946848392487 and batch: 100, loss is 3.9524522542953493 and perplexity is 52.06288184694929
At time: 567.1659367084503 and batch: 150, loss is 3.9526833486557007 and perplexity is 52.07491467563363
At time: 568.7386479377747 and batch: 200, loss is 3.9977831077575683 and perplexity is 54.47724588313716
At time: 570.3063414096832 and batch: 250, loss is 3.983768930435181 and perplexity is 53.71911678492992
At time: 571.8828194141388 and batch: 300, loss is 3.9753407764434816 and perplexity is 53.26826638347133
At time: 573.4518053531647 and batch: 350, loss is 3.9302924919128417 and perplexity is 50.92186972627385
At time: 575.026442527771 and batch: 400, loss is 3.9426647424697876 and perplexity is 51.55580135136424
At time: 576.5990419387817 and batch: 450, loss is 3.98107253074646 and perplexity is 53.57446368408326
At time: 578.170880317688 and batch: 500, loss is 3.9545797300338745 and perplexity is 52.17376227084042
At time: 579.7585208415985 and batch: 550, loss is 3.976684260368347 and perplexity is 53.339879537852326
At time: 581.3521540164948 and batch: 600, loss is 4.015330786705017 and perplexity is 55.44163172429979
At time: 582.9409213066101 and batch: 650, loss is 3.9940049600601197 and perplexity is 54.27181112783573
At time: 584.5346293449402 and batch: 700, loss is 3.9689671182632447 and perplexity is 52.92983233884548
At time: 586.1389939785004 and batch: 750, loss is 3.965931005477905 and perplexity is 52.769375104543535
At time: 587.7386589050293 and batch: 800, loss is 3.9251599502563477 and perplexity is 50.6611806796498
At time: 589.3280885219574 and batch: 850, loss is 3.942346625328064 and perplexity is 51.53940317560777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.36552079518636 and perplexity of 78.69037083847358
Finished 20 epochs...
Completing Train Step...
At time: 593.4509780406952 and batch: 50, loss is 3.9789689874649046 and perplexity is 53.4618859285404
At time: 595.0474843978882 and batch: 100, loss is 3.942852520942688 and perplexity is 51.56548333001628
At time: 596.6182813644409 and batch: 150, loss is 3.9435377979278563 and perplexity is 51.60083207943031
At time: 598.1931884288788 and batch: 200, loss is 3.989318928718567 and perplexity is 54.01808666461337
At time: 599.7690351009369 and batch: 250, loss is 3.9753166103363036 and perplexity is 53.26697911239095
At time: 601.3495655059814 and batch: 300, loss is 3.96724160194397 and perplexity is 52.838579800881355
At time: 602.9416389465332 and batch: 350, loss is 3.922505989074707 and perplexity is 50.52690613124318
At time: 604.5322287082672 and batch: 400, loss is 3.9351968955993653 and perplexity is 51.17222455062696
At time: 606.123566865921 and batch: 450, loss is 3.974030685424805 and perplexity is 53.19852579933319
At time: 607.7146980762482 and batch: 500, loss is 3.948094606399536 and perplexity is 51.83650373569522
At time: 609.3118991851807 and batch: 550, loss is 3.9702845764160157 and perplexity is 52.999611133212966
At time: 610.9040038585663 and batch: 600, loss is 4.009218091964722 and perplexity is 55.10376763570859
At time: 612.4986388683319 and batch: 650, loss is 3.9885603332519532 and perplexity is 53.977124327840315
At time: 614.091304063797 and batch: 700, loss is 3.963734703063965 and perplexity is 52.65360477847981
At time: 615.6846005916595 and batch: 750, loss is 3.961096291542053 and perplexity is 52.51486600643196
At time: 617.271989107132 and batch: 800, loss is 3.9208214569091795 and perplexity is 50.441863581206064
At time: 618.8657143115997 and batch: 850, loss is 3.9380974531173707 and perplexity is 51.32086800148546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.365713119506836 and perplexity of 78.70550636599143
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 623.2598748207092 and batch: 50, loss is 3.9798249292373655 and perplexity is 53.50766577958961
At time: 624.8662841320038 and batch: 100, loss is 3.950838398933411 and perplexity is 51.97892764911055
At time: 626.4739720821381 and batch: 150, loss is 3.9517876148223876 and perplexity is 52.02829029732571
At time: 628.0826156139374 and batch: 200, loss is 3.999285202026367 and perplexity is 54.559137330897784
At time: 629.6882035732269 and batch: 250, loss is 3.978944711685181 and perplexity is 53.46058811532676
At time: 631.2646908760071 and batch: 300, loss is 3.968912386894226 and perplexity is 52.92693549593446
At time: 632.8677837848663 and batch: 350, loss is 3.9219705867767334 and perplexity is 50.499861150209554
At time: 634.4403593540192 and batch: 400, loss is 3.931083059310913 and perplexity is 50.96214281352564
At time: 636.0130558013916 and batch: 450, loss is 3.9720884275436403 and perplexity is 53.095300820547585
At time: 637.5866756439209 and batch: 500, loss is 3.9435426902771 and perplexity is 51.60108452933964
At time: 639.1595911979675 and batch: 550, loss is 3.963885922431946 and perplexity is 52.66156762536949
At time: 640.7326135635376 and batch: 600, loss is 3.9976266765594484 and perplexity is 54.468724608807335
At time: 642.3113350868225 and batch: 650, loss is 3.9751451873779295 and perplexity is 53.25784871185035
At time: 643.903599023819 and batch: 700, loss is 3.9519460821151733 and perplexity is 52.036535732936066
At time: 645.4979863166809 and batch: 750, loss is 3.947909560203552 and perplexity is 51.82691247530631
At time: 647.091474533081 and batch: 800, loss is 3.904317269325256 and perplexity is 49.61619384846144
At time: 648.6845452785492 and batch: 850, loss is 3.9233674430847167 and perplexity is 50.57045149061261
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.356481870015462 and perplexity of 77.98229938812986
Finished 22 epochs...
Completing Train Step...
At time: 652.8380777835846 and batch: 50, loss is 3.9718709325790407 and perplexity is 53.08375411569581
At time: 654.4068162441254 and batch: 100, loss is 3.938274974822998 and perplexity is 51.329979378217004
At time: 655.9764695167542 and batch: 150, loss is 3.9388500928878782 and perplexity is 51.35950866722719
At time: 657.5452692508698 and batch: 200, loss is 3.987507348060608 and perplexity is 53.9203171290701
At time: 659.1135652065277 and batch: 250, loss is 3.9685889530181884 and perplexity is 52.90981990007117
At time: 660.6869568824768 and batch: 300, loss is 3.9607716035842895 and perplexity is 52.49781782965472
At time: 662.2709550857544 and batch: 350, loss is 3.9142778873443604 and perplexity is 50.112871303717085
At time: 663.8581869602203 and batch: 400, loss is 3.9241603803634644 and perplexity is 50.61056658907926
At time: 665.4454054832458 and batch: 450, loss is 3.965911030769348 and perplexity is 52.76832106218222
At time: 667.0320167541504 and batch: 500, loss is 3.938797459602356 and perplexity is 51.35680551868163
At time: 668.6190512180328 and batch: 550, loss is 3.9600745582580568 and perplexity is 52.461237221751155
At time: 670.2063708305359 and batch: 600, loss is 3.994580659866333 and perplexity is 54.30306439437143
At time: 671.7937150001526 and batch: 650, loss is 3.9729779720306397 and perplexity is 53.14255246578253
At time: 673.4131305217743 and batch: 700, loss is 3.950999870300293 and perplexity is 51.98732143526695
At time: 675.0084848403931 and batch: 750, loss is 3.9472484827041625 and perplexity is 51.792662191899254
At time: 676.6014473438263 and batch: 800, loss is 3.9044142961502075 and perplexity is 49.62100818377279
At time: 678.195065498352 and batch: 850, loss is 3.9237421131134034 and perplexity is 50.58940227304671
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.356224377950032 and perplexity of 77.96222214976916
Finished 23 epochs...
Completing Train Step...
At time: 682.3517837524414 and batch: 50, loss is 3.965786156654358 and perplexity is 52.76173207619552
At time: 683.9322962760925 and batch: 100, loss is 3.9320574426651 and perplexity is 51.01182367734951
At time: 685.5132446289062 and batch: 150, loss is 3.9327036857604982 and perplexity is 51.04480037051713
At time: 687.0903913974762 and batch: 200, loss is 3.9820413112640383 and perplexity is 53.62639072963387
At time: 688.6749987602234 and batch: 250, loss is 3.9632415342330933 and perplexity is 52.62764406380386
At time: 690.2676212787628 and batch: 300, loss is 3.9558037328720093 and perplexity is 52.23766220280732
At time: 691.8516473770142 and batch: 350, loss is 3.9098461627960206 and perplexity is 49.89127624857593
At time: 693.4368529319763 and batch: 400, loss is 3.9199694776535035 and perplexity is 50.39890646170302
At time: 695.0076565742493 and batch: 450, loss is 3.9620247220993043 and perplexity is 52.563645053217186
At time: 696.5951828956604 and batch: 500, loss is 3.9354826641082763 and perplexity is 51.18685005058858
At time: 698.1931374073029 and batch: 550, loss is 3.9569354343414305 and perplexity is 52.29681310615037
At time: 699.7930099964142 and batch: 600, loss is 3.9916867065429686 and perplexity is 54.146141034656615
At time: 701.3911876678467 and batch: 650, loss is 3.970576162338257 and perplexity is 53.01506732699841
At time: 702.9829194545746 and batch: 700, loss is 3.949144859313965 and perplexity is 51.890973773468545
At time: 704.5785865783691 and batch: 750, loss is 3.945546703338623 and perplexity is 51.704597462717544
At time: 706.1814579963684 and batch: 800, loss is 3.90305064201355 and perplexity is 49.55338840617112
At time: 707.7751021385193 and batch: 850, loss is 3.9224731969833373 and perplexity is 50.52524927548672
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.356298764546712 and perplexity of 77.96802170984657
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 711.8764531612396 and batch: 50, loss is 3.967779302597046 and perplexity is 52.86699877951487
At time: 713.482973575592 and batch: 100, loss is 3.940035037994385 and perplexity is 51.42040293675324
At time: 715.0567104816437 and batch: 150, loss is 3.9437061738967896 and perplexity is 51.609521151024346
At time: 716.6299564838409 and batch: 200, loss is 3.9965662479400637 and perplexity is 54.41099502882996
At time: 718.2092146873474 and batch: 250, loss is 3.9722385025024414 and perplexity is 53.103269693579946
At time: 719.791389465332 and batch: 300, loss is 3.962620940208435 and perplexity is 52.594993794694865
At time: 721.3723857402802 and batch: 350, loss is 3.9144386529922484 and perplexity is 50.12092837957291
At time: 722.9603910446167 and batch: 400, loss is 3.921449990272522 and perplexity is 50.473577941098604
At time: 724.5399117469788 and batch: 450, loss is 3.965415349006653 and perplexity is 52.742171249311745
At time: 726.1269435882568 and batch: 500, loss is 3.9371249151229857 and perplexity is 51.2709807699953
At time: 727.7311630249023 and batch: 550, loss is 3.9595994329452515 and perplexity is 52.43631748047475
At time: 729.3439066410065 and batch: 600, loss is 3.9899749755859375 and perplexity is 54.053536688318076
At time: 730.9739050865173 and batch: 650, loss is 3.965684342384338 and perplexity is 52.75636045241782
At time: 732.587860584259 and batch: 700, loss is 3.946445541381836 and perplexity is 51.75109241451148
At time: 734.1823809146881 and batch: 750, loss is 3.9425324773788453 and perplexity is 51.54898276955001
At time: 735.7765114307404 and batch: 800, loss is 3.8960918951034547 and perplexity is 49.20975593010546
At time: 737.3699278831482 and batch: 850, loss is 3.918308067321777 and perplexity is 50.31524271694945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.350577672322591 and perplexity of 77.52323301886665
Finished 25 epochs...
Completing Train Step...
At time: 741.5461785793304 and batch: 50, loss is 3.9637824440002443 and perplexity is 52.65611857087534
At time: 743.1946890354156 and batch: 100, loss is 3.9325403118133546 and perplexity is 51.036461661181924
At time: 744.8006241321564 and batch: 150, loss is 3.934481143951416 and perplexity is 51.13561105122498
At time: 746.4110701084137 and batch: 200, loss is 3.9875851535797118 and perplexity is 53.924512590547494
At time: 748.016758441925 and batch: 250, loss is 3.964806413650513 and perplexity is 52.710064452952274
At time: 749.6238312721252 and batch: 300, loss is 3.956833062171936 and perplexity is 52.2914596419626
At time: 751.2312569618225 and batch: 350, loss is 3.910042576789856 and perplexity is 49.9010765558287
At time: 752.8769028186798 and batch: 400, loss is 3.9174002170562745 and perplexity is 50.26958473892968
At time: 754.4852809906006 and batch: 450, loss is 3.9615832805633544 and perplexity is 52.540446397811536
At time: 756.0928151607513 and batch: 500, loss is 3.9341952848434447 and perplexity is 51.12099556014945
At time: 757.6996836662292 and batch: 550, loss is 3.9573795557022096 and perplexity is 52.32004439632573
At time: 759.3056132793427 and batch: 600, loss is 3.9885661792755127 and perplexity is 53.97743988030317
At time: 760.908608675003 and batch: 650, loss is 3.9649901676177977 and perplexity is 52.71975102635724
At time: 762.5120460987091 and batch: 700, loss is 3.946618127822876 and perplexity is 51.760024722146646
At time: 764.1297421455383 and batch: 750, loss is 3.9430637407302855 and perplexity is 51.57637613079965
At time: 765.7470381259918 and batch: 800, loss is 3.897001247406006 and perplexity is 49.25452528744251
At time: 767.3460502624512 and batch: 850, loss is 3.919374165534973 and perplexity is 50.36891231075229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3502756754557295 and perplexity of 77.49982478017125
Finished 26 epochs...
Completing Train Step...
At time: 771.5180857181549 and batch: 50, loss is 3.9605050754547118 and perplexity is 52.48382754894617
At time: 773.1354703903198 and batch: 100, loss is 3.9288490867614745 and perplexity is 50.84842185746168
At time: 774.7124950885773 and batch: 150, loss is 3.9308741188049314 and perplexity is 50.951495869947884
At time: 776.2842164039612 and batch: 200, loss is 3.984264268875122 and perplexity is 53.74573251978728
At time: 777.8552558422089 and batch: 250, loss is 3.961635546684265 and perplexity is 52.543192554900536
At time: 779.4294471740723 and batch: 300, loss is 3.953902831077576 and perplexity is 52.138457855722
At time: 781.0002388954163 and batch: 350, loss is 3.9075432920455935 and perplexity is 49.776515278359234
At time: 782.582522392273 and batch: 400, loss is 3.9151120805740356 and perplexity is 50.1546925627586
At time: 784.1592733860016 and batch: 450, loss is 3.9594017934799193 and perplexity is 52.425955018773344
At time: 785.7554693222046 and batch: 500, loss is 3.932312397956848 and perplexity is 51.02483106981908
At time: 787.3431558609009 and batch: 550, loss is 3.9559100246429444 and perplexity is 52.243214931531824
At time: 788.931156873703 and batch: 600, loss is 3.987350974082947 and perplexity is 53.911886053821455
At time: 790.5192558765411 and batch: 650, loss is 3.9640290260314943 and perplexity is 52.669104224498916
At time: 792.1072940826416 and batch: 700, loss is 3.9460433530807495 and perplexity is 51.73028291552294
At time: 793.7250056266785 and batch: 750, loss is 3.9425957727432253 and perplexity is 51.55224568446042
At time: 795.3249566555023 and batch: 800, loss is 3.8967376279830934 and perplexity is 49.24154254923657
At time: 796.915853023529 and batch: 850, loss is 3.91918429851532 and perplexity is 50.3593498233179
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3501841227213545 and perplexity of 77.49272978408715
Finished 27 epochs...
Completing Train Step...
At time: 801.0634841918945 and batch: 50, loss is 3.9575585317611695 and perplexity is 52.329409269695454
At time: 802.6378057003021 and batch: 100, loss is 3.92591676235199 and perplexity is 50.699536186092566
At time: 804.2094898223877 and batch: 150, loss is 3.928051872253418 and perplexity is 50.80790091193405
At time: 805.7819471359253 and batch: 200, loss is 3.98172372341156 and perplexity is 53.609362343513304
At time: 807.3510041236877 and batch: 250, loss is 3.959136862754822 and perplexity is 52.412067612177864
At time: 808.9202861785889 and batch: 300, loss is 3.95154363155365 and perplexity is 52.015597813431945
At time: 810.4975588321686 and batch: 350, loss is 3.9054736852645875 and perplexity is 49.67360399448285
At time: 812.0700743198395 and batch: 400, loss is 3.9132507181167604 and perplexity is 50.06142333181747
At time: 813.6565451622009 and batch: 450, loss is 3.957658438682556 and perplexity is 52.33463760104246
At time: 815.244934797287 and batch: 500, loss is 3.9306833600997924 and perplexity is 50.941777355545916
At time: 816.8343267440796 and batch: 550, loss is 3.954611883163452 and perplexity is 52.17543984754879
At time: 818.4256453514099 and batch: 600, loss is 3.9861099243164064 and perplexity is 53.845020220716144
At time: 820.0130486488342 and batch: 650, loss is 3.9629191732406617 and perplexity is 52.61068169838347
At time: 821.6086931228638 and batch: 700, loss is 3.9451957893371583 and perplexity is 51.686456778623956
At time: 823.2058453559875 and batch: 750, loss is 3.94181143283844 and perplexity is 51.51182705403328
At time: 824.799697637558 and batch: 800, loss is 3.896090431213379 and perplexity is 49.20968389248485
At time: 826.3922455310822 and batch: 850, loss is 3.9186353349685668 and perplexity is 50.33171196280975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.35014533996582 and perplexity of 77.48972446077
Finished 28 epochs...
Completing Train Step...
At time: 830.524436712265 and batch: 50, loss is 3.9548648500442507 and perplexity is 52.188640175373806
At time: 832.1004700660706 and batch: 100, loss is 3.9232913208007814 and perplexity is 50.56660209885961
At time: 833.7103505134583 and batch: 150, loss is 3.925556139945984 and perplexity is 50.68125609367331
At time: 835.285325050354 and batch: 200, loss is 3.979469690322876 and perplexity is 53.488661150273
At time: 836.8589150905609 and batch: 250, loss is 3.9568849182128907 and perplexity is 52.29417134034372
At time: 838.4322099685669 and batch: 300, loss is 3.9494246006011964 and perplexity is 51.90549185182592
At time: 840.0127971172333 and batch: 350, loss is 3.9035797929763794 and perplexity is 49.579616568074286
At time: 841.5857026576996 and batch: 400, loss is 3.911480355262756 and perplexity is 49.972874852129124
At time: 843.1786034107208 and batch: 450, loss is 3.955892300605774 and perplexity is 52.242288979054315
At time: 844.791464805603 and batch: 500, loss is 3.929158525466919 and perplexity is 50.86415876197326
At time: 846.4024686813354 and batch: 550, loss is 3.95314884185791 and perplexity is 52.09916083719466
At time: 848.0021526813507 and batch: 600, loss is 3.984824786186218 and perplexity is 53.77586637774772
At time: 849.6195251941681 and batch: 650, loss is 3.9618013095855713 and perplexity is 52.55190298885577
At time: 851.2405977249146 and batch: 700, loss is 3.944274568557739 and perplexity is 51.63886406568878
At time: 852.8545787334442 and batch: 750, loss is 3.940923390388489 and perplexity is 51.46610267053644
At time: 854.4563252925873 and batch: 800, loss is 3.8953232622146605 and perplexity is 49.17194622600058
At time: 856.0532584190369 and batch: 850, loss is 3.917950096130371 and perplexity is 50.297234532966016
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.350108464558919 and perplexity of 77.48686704833429
Finished 29 epochs...
Completing Train Step...
At time: 860.2241344451904 and batch: 50, loss is 3.9523481798171995 and perplexity is 52.057463711639784
At time: 861.8022031784058 and batch: 100, loss is 3.9208679580688477 and perplexity is 50.444209240895944
At time: 863.3843450546265 and batch: 150, loss is 3.9233147525787353 and perplexity is 50.56778697813373
At time: 864.964791059494 and batch: 200, loss is 3.9774684286117554 and perplexity is 53.38172338165567
At time: 866.548054933548 and batch: 250, loss is 3.954856286048889 and perplexity is 52.18819323401521
At time: 868.1317102909088 and batch: 300, loss is 3.9474728107452393 and perplexity is 51.80428204163125
At time: 869.7055771350861 and batch: 350, loss is 3.9018296003341675 and perplexity is 49.49291857917521
At time: 871.2969138622284 and batch: 400, loss is 3.909828805923462 and perplexity is 49.89041029956739
At time: 872.8952474594116 and batch: 450, loss is 3.9541413021087646 and perplexity is 52.15089285016549
At time: 874.5266239643097 and batch: 500, loss is 3.9276045846939085 and perplexity is 50.785180251644505
At time: 876.1384828090668 and batch: 550, loss is 3.9517968273162842 and perplexity is 52.02876960984035
At time: 877.741946220398 and batch: 600, loss is 3.9835746479034424 and perplexity is 53.70868111268543
At time: 879.3518235683441 and batch: 650, loss is 3.960680341720581 and perplexity is 52.493026999572336
At time: 880.9398868083954 and batch: 700, loss is 3.943319435119629 and perplexity is 51.589565606964634
At time: 882.5295059680939 and batch: 750, loss is 3.9399931621551514 and perplexity is 51.418249709310956
At time: 884.1291315555573 and batch: 800, loss is 3.894485840797424 and perplexity is 49.13078582181113
At time: 885.721750497818 and batch: 850, loss is 3.9171681118011477 and perplexity is 50.25791825811686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.350117365519206 and perplexity of 77.48755675893018
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 889.9057767391205 and batch: 50, loss is 3.954473466873169 and perplexity is 52.168218416514584
At time: 891.525627374649 and batch: 100, loss is 3.927678122520447 and perplexity is 50.78891502074229
At time: 893.096987247467 and batch: 150, loss is 3.9328051471710204 and perplexity is 51.049979710709685
At time: 894.6653141975403 and batch: 200, loss is 3.9915612077713014 and perplexity is 54.13934618684769
At time: 896.2327387332916 and batch: 250, loss is 3.967107262611389 and perplexity is 52.83148197810545
At time: 897.8085658550262 and batch: 300, loss is 3.9573211336135863 and perplexity is 52.31698783934132
At time: 899.4122717380524 and batch: 350, loss is 3.90836630821228 and perplexity is 49.817499017981405
At time: 901.0188345909119 and batch: 400, loss is 3.9150601530075075 and perplexity is 50.15208821924305
At time: 902.6234765052795 and batch: 450, loss is 3.958971266746521 and perplexity is 52.40338910157768
At time: 904.2289438247681 and batch: 500, loss is 3.930034346580505 and perplexity is 50.90872617983573
At time: 905.836528301239 and batch: 550, loss is 3.9551400566101074 and perplexity is 52.20300480834319
At time: 907.4532387256622 and batch: 600, loss is 3.9859893131256103 and perplexity is 53.83852630033649
At time: 909.06112408638 and batch: 650, loss is 3.9606159210205076 and perplexity is 52.48964547094543
At time: 910.6780180931091 and batch: 700, loss is 3.9428164911270143 and perplexity is 51.56362546862618
At time: 912.308308839798 and batch: 750, loss is 3.9411620330810546 and perplexity is 51.47838614547608
At time: 913.9727368354797 and batch: 800, loss is 3.891995277404785 and perplexity is 49.008574735592475
At time: 915.5634076595306 and batch: 850, loss is 3.9165977239608765 and perplexity is 50.22925992662397
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.346731185913086 and perplexity of 77.22561371788795
Finished 31 epochs...
Completing Train Step...
At time: 919.7314736843109 and batch: 50, loss is 3.953853440284729 and perplexity is 52.13588275954425
At time: 921.3364758491516 and batch: 100, loss is 3.923387870788574 and perplexity is 50.57148453937097
At time: 922.9230003356934 and batch: 150, loss is 3.9262148571014404 and perplexity is 50.71465170444561
At time: 924.5120611190796 and batch: 200, loss is 3.984298596382141 and perplexity is 53.74757750846433
At time: 926.0909039974213 and batch: 250, loss is 3.961118779182434 and perplexity is 52.51604695513169
At time: 927.6752812862396 and batch: 300, loss is 3.9531460046768188 and perplexity is 52.09901302265034
At time: 929.2658746242523 and batch: 350, loss is 3.90569589138031 and perplexity is 49.6846429995022
At time: 930.8474032878876 and batch: 400, loss is 3.913032555580139 and perplexity is 50.050502995963924
At time: 932.4248201847076 and batch: 450, loss is 3.957108974456787 and perplexity is 52.305889488665045
At time: 934.0178804397583 and batch: 500, loss is 3.928056182861328 and perplexity is 50.80811992534566
At time: 935.6149413585663 and batch: 550, loss is 3.9539170169830324 and perplexity is 52.139197492201994
At time: 937.2284269332886 and batch: 600, loss is 3.985413460731506 and perplexity is 53.807532180946815
At time: 938.830658197403 and batch: 650, loss is 3.960571265220642 and perplexity is 52.48730155617736
At time: 940.4443516731262 and batch: 700, loss is 3.9433030033111574 and perplexity is 51.588717904068105
At time: 942.0477240085602 and batch: 750, loss is 3.9420627546310425 and perplexity is 51.524774725696595
At time: 943.6606259346008 and batch: 800, loss is 3.893021206855774 and perplexity is 49.058879876115
At time: 945.2738571166992 and batch: 850, loss is 3.9176863336563112 and perplexity is 50.283969759398886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.34655221303304 and perplexity of 77.21179366413186
Finished 32 epochs...
Completing Train Step...
At time: 949.4118790626526 and batch: 50, loss is 3.9522912549972533 and perplexity is 52.05450043423396
At time: 951.009527683258 and batch: 100, loss is 3.9215035581588746 and perplexity is 50.47628177640428
At time: 952.5819654464722 and batch: 150, loss is 3.9239077138900758 and perplexity is 50.59778061106501
At time: 954.1804111003876 and batch: 200, loss is 3.9822669410705567 and perplexity is 53.638491806929004
At time: 955.7572386264801 and batch: 250, loss is 3.959262657165527 and perplexity is 52.418661172044786
At time: 957.3280675411224 and batch: 300, loss is 3.951498966217041 and perplexity is 52.013274571131284
At time: 958.8992042541504 and batch: 350, loss is 3.904363751411438 and perplexity is 49.61850016626074
At time: 960.4720120429993 and batch: 400, loss is 3.9118869304656982 and perplexity is 49.9931967147665
At time: 962.0453562736511 and batch: 450, loss is 3.9560050487518312 and perplexity is 52.248179532350726
At time: 963.6351430416107 and batch: 500, loss is 3.92703188419342 and perplexity is 50.75610388031792
At time: 965.2277357578278 and batch: 550, loss is 3.953193941116333 and perplexity is 52.10151052369703
At time: 966.8230164051056 and batch: 600, loss is 3.9849282026290895 and perplexity is 53.781427974136136
At time: 968.415374994278 and batch: 650, loss is 3.9602839040756224 and perplexity is 52.47222091200254
At time: 970.0182857513428 and batch: 700, loss is 3.9431922960281374 and perplexity is 51.583006973401105
At time: 971.6231172084808 and batch: 750, loss is 3.9420920991897583 and perplexity is 51.526286719658145
At time: 973.2177875041962 and batch: 800, loss is 3.893101468086243 and perplexity is 49.06281756019885
At time: 974.8166291713715 and batch: 850, loss is 3.917826042175293 and perplexity is 50.290995349098424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.346482912699382 and perplexity of 77.20644304647051
Finished 33 epochs...
Completing Train Step...
At time: 978.9787003993988 and batch: 50, loss is 3.9508615589141844 and perplexity is 51.98013149401599
At time: 980.5740482807159 and batch: 100, loss is 3.919963450431824 and perplexity is 50.398602697236804
At time: 982.155796289444 and batch: 150, loss is 3.9223326873779296 and perplexity is 50.51815049138325
At time: 983.7229969501495 and batch: 200, loss is 3.980896682739258 and perplexity is 53.565043549687715
At time: 985.289999961853 and batch: 250, loss is 3.9579555559158326 and perplexity is 52.350189434015405
At time: 986.8606135845184 and batch: 300, loss is 3.9502800130844116 and perplexity is 51.94991145333561
At time: 988.468414068222 and batch: 350, loss is 3.903298568725586 and perplexity is 49.56567553792007
At time: 990.0648055076599 and batch: 400, loss is 3.910962996482849 and perplexity is 49.9470276332865
At time: 991.647472858429 and batch: 450, loss is 3.9550970554351808 and perplexity is 52.20076006606536
At time: 993.2293393611908 and batch: 500, loss is 3.926169719696045 and perplexity is 50.712362628314
At time: 994.8633093833923 and batch: 550, loss is 3.9525194501876832 and perplexity is 52.06638037629455
At time: 996.4516150951385 and batch: 600, loss is 3.9843980741500853 and perplexity is 53.752924463454455
At time: 998.045695066452 and batch: 650, loss is 3.9598757076263427 and perplexity is 52.4508063087203
At time: 999.6355273723602 and batch: 700, loss is 3.94287793636322 and perplexity is 51.566793905114395
At time: 1001.2240550518036 and batch: 750, loss is 3.94187077999115 and perplexity is 51.514884225016125
At time: 1002.8177125453949 and batch: 800, loss is 3.892912549972534 and perplexity is 49.05354958072431
At time: 1004.4074409008026 and batch: 850, loss is 3.9177071809768678 and perplexity is 50.28501805636236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3464352289835615 and perplexity of 77.20276164415296
Finished 34 epochs...
Completing Train Step...
At time: 1008.5489721298218 and batch: 50, loss is 3.949556756019592 and perplexity is 51.91235189710473
At time: 1010.1180641651154 and batch: 100, loss is 3.9185991621017457 and perplexity is 50.32989135342457
At time: 1011.6883702278137 and batch: 150, loss is 3.920986123085022 and perplexity is 50.45017033388617
At time: 1013.2599186897278 and batch: 200, loss is 3.979750418663025 and perplexity is 53.503679041209665
At time: 1014.8282690048218 and batch: 250, loss is 3.95683509349823 and perplexity is 52.29156586308739
At time: 1016.405969619751 and batch: 300, loss is 3.949219546318054 and perplexity is 51.89484949957035
At time: 1018.0089275836945 and batch: 350, loss is 3.9023434686660767 and perplexity is 49.51835795837269
At time: 1019.5956585407257 and batch: 400, loss is 3.9101302433013916 and perplexity is 49.905451400893114
At time: 1021.2052023410797 and batch: 450, loss is 3.9542668533325194 and perplexity is 52.157440869630065
At time: 1022.8239748477936 and batch: 500, loss is 3.9253629636764527 and perplexity is 50.67146662326311
At time: 1024.434597492218 and batch: 550, loss is 3.9518553495407103 and perplexity is 52.031814538269195
At time: 1026.0386362075806 and batch: 600, loss is 3.9838452434539793 and perplexity is 53.72321640932434
At time: 1027.626573085785 and batch: 650, loss is 3.9594196557998655 and perplexity is 52.426891476319
At time: 1029.2149753570557 and batch: 700, loss is 3.9424811601638794 and perplexity is 51.54633748719479
At time: 1030.8041808605194 and batch: 750, loss is 3.941549425125122 and perplexity is 51.49833232595663
At time: 1032.391405582428 and batch: 800, loss is 3.8926145124435423 and perplexity is 49.03893196042691
At time: 1033.9802179336548 and batch: 850, loss is 3.917478380203247 and perplexity is 50.27351412143433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.346397399902344 and perplexity of 77.1998411898519
Finished 35 epochs...
Completing Train Step...
At time: 1038.1087591648102 and batch: 50, loss is 3.948303861618042 and perplexity is 51.847351929592016
At time: 1039.6824219226837 and batch: 100, loss is 3.917331314086914 and perplexity is 50.26612113460034
At time: 1041.2566907405853 and batch: 150, loss is 3.919759063720703 and perplexity is 50.3883029451885
At time: 1042.8356540203094 and batch: 200, loss is 3.978712658882141 and perplexity is 53.448183875277444
At time: 1044.4083194732666 and batch: 250, loss is 3.955807952880859 and perplexity is 52.237882646669256
At time: 1045.9853856563568 and batch: 300, loss is 3.948241505622864 and perplexity is 51.84411903716126
At time: 1047.5596301555634 and batch: 350, loss is 3.9014508533477783 and perplexity is 49.47417683482938
At time: 1049.1323637962341 and batch: 400, loss is 3.909345073699951 and perplexity is 49.86628253662002
At time: 1050.7122526168823 and batch: 450, loss is 3.9534768295288085 and perplexity is 52.11625152222719
At time: 1052.3059387207031 and batch: 500, loss is 3.9245823431015014 and perplexity is 50.631926868634885
At time: 1053.9026308059692 and batch: 550, loss is 3.9511964893341065 and perplexity is 51.99754413713405
At time: 1055.4952449798584 and batch: 600, loss is 3.9832805585861206 and perplexity is 53.69288828568743
At time: 1057.0930075645447 and batch: 650, loss is 3.958936405181885 and perplexity is 52.401562269284675
At time: 1058.6944048404694 and batch: 700, loss is 3.942041068077087 and perplexity is 51.52365734300561
At time: 1060.2954511642456 and batch: 750, loss is 3.941175413131714 and perplexity is 51.47907493349857
At time: 1061.88800740242 and batch: 800, loss is 3.892259202003479 and perplexity is 49.02151101103769
At time: 1063.4802594184875 and batch: 850, loss is 3.917189531326294 and perplexity is 50.25899477038995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.34636656443278 and perplexity of 77.19746073320002
Finished 36 epochs...
Completing Train Step...
At time: 1067.556331396103 and batch: 50, loss is 3.947102904319763 and perplexity is 51.78512284860964
At time: 1069.1510074138641 and batch: 100, loss is 3.9161282634735106 and perplexity is 50.20568480800511
At time: 1070.725165605545 and batch: 150, loss is 3.918607401847839 and perplexity is 50.33030606065877
At time: 1072.3004591464996 and batch: 200, loss is 3.9777409124374388 and perplexity is 53.39627101977231
At time: 1073.8764035701752 and batch: 250, loss is 3.954837875366211 and perplexity is 52.18723242259467
At time: 1075.479649066925 and batch: 300, loss is 3.94731644153595 and perplexity is 51.79618208031937
At time: 1077.0569560527802 and batch: 350, loss is 3.9006008243560792 and perplexity is 49.43214021888159
At time: 1078.6318967342377 and batch: 400, loss is 3.9085901975631714 and perplexity is 49.82865387417973
At time: 1080.205267906189 and batch: 450, loss is 3.9527127742767334 and perplexity is 52.07644703488339
At time: 1081.7908344268799 and batch: 500, loss is 3.923817367553711 and perplexity is 50.59320949345359
At time: 1083.3767728805542 and batch: 550, loss is 3.9505401611328126 and perplexity is 51.963427879474516
At time: 1084.9636898040771 and batch: 600, loss is 3.9827104377746583 and perplexity is 53.662285577097684
At time: 1086.5542736053467 and batch: 650, loss is 3.9584415197372436 and perplexity is 52.37563591465804
At time: 1088.1425986289978 and batch: 700, loss is 3.9415766191482544 and perplexity is 51.49973279183925
At time: 1089.7288920879364 and batch: 750, loss is 3.940770273208618 and perplexity is 51.45822292931387
At time: 1091.330617904663 and batch: 800, loss is 3.8918721294403076 and perplexity is 49.00253980097443
At time: 1092.9198942184448 and batch: 850, loss is 3.916865267753601 and perplexity is 50.242700251188026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.34634272257487 and perplexity of 77.19562022425085
Finished 37 epochs...
Completing Train Step...
At time: 1096.9924211502075 and batch: 50, loss is 3.945944619178772 and perplexity is 51.72517563497532
At time: 1098.587652206421 and batch: 100, loss is 3.9149724817276 and perplexity is 50.14769151421403
At time: 1100.1575214862823 and batch: 150, loss is 3.917508616447449 and perplexity is 50.275034226665205
At time: 1101.7264986038208 and batch: 200, loss is 3.976813135147095 and perplexity is 53.34675414599848
At time: 1103.295449256897 and batch: 250, loss is 3.9539081239700318 and perplexity is 52.13873381970257
At time: 1104.8664031028748 and batch: 300, loss is 3.9464286613464354 and perplexity is 51.75021886161233
At time: 1106.4372227191925 and batch: 350, loss is 3.899783034324646 and perplexity is 49.391731632499784
At time: 1108.0175895690918 and batch: 400, loss is 3.907857608795166 and perplexity is 49.79216332994011
At time: 1109.6103057861328 and batch: 450, loss is 3.9519662189483644 and perplexity is 52.03758359452624
At time: 1111.1998331546783 and batch: 500, loss is 3.9230640363693237 and perplexity is 50.55511040344202
At time: 1112.7890923023224 and batch: 550, loss is 3.9498877906799317 and perplexity is 51.929539529576495
At time: 1114.4032213687897 and batch: 600, loss is 3.9821352672576906 and perplexity is 53.631429487167885
At time: 1115.9914543628693 and batch: 650, loss is 3.9579314947128297 and perplexity is 52.348929840633915
At time: 1117.5798971652985 and batch: 700, loss is 3.941093349456787 and perplexity is 51.47485054476449
At time: 1119.1694071292877 and batch: 750, loss is 3.940343098640442 and perplexity is 51.436245979486145
At time: 1120.7579340934753 and batch: 800, loss is 3.8914618635177614 and perplexity is 48.98243985221982
At time: 1122.347641468048 and batch: 850, loss is 3.9165155124664306 and perplexity is 50.22513067383884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.346323013305664 and perplexity of 77.19409876998375
Finished 38 epochs...
Completing Train Step...
At time: 1126.4231390953064 and batch: 50, loss is 3.9448250102996827 and perplexity is 51.66729607634194
At time: 1128.0261311531067 and batch: 100, loss is 3.9138549041748045 and perplexity is 50.0916788849117
At time: 1129.5989530086517 and batch: 150, loss is 3.9164517974853514 and perplexity is 50.22193068253303
At time: 1131.1722314357758 and batch: 200, loss is 3.975921311378479 and perplexity is 53.29919945102811
At time: 1132.7513391971588 and batch: 250, loss is 3.9530091094970703 and perplexity is 52.09188140705093
At time: 1134.3284175395966 and batch: 300, loss is 3.9455707263946533 and perplexity is 51.70583958007908
At time: 1135.9042019844055 and batch: 350, loss is 3.8989901161193847 and perplexity is 49.352583551962965
At time: 1137.4773933887482 and batch: 400, loss is 3.9071416902542113 and perplexity is 49.7565289541951
At time: 1139.057329416275 and batch: 450, loss is 3.951232876777649 and perplexity is 51.999436229259665
At time: 1140.6585574150085 and batch: 500, loss is 3.9223192644119265 and perplexity is 50.517472392517725
At time: 1142.257982492447 and batch: 550, loss is 3.9492352533340456 and perplexity is 51.89566461920284
At time: 1143.850393295288 and batch: 600, loss is 3.981561312675476 and perplexity is 53.600656314509514
At time: 1145.4429287910461 and batch: 650, loss is 3.9574324178695677 and perplexity is 52.32281022037188
At time: 1147.0357360839844 and batch: 700, loss is 3.940605320930481 and perplexity is 51.44973547824423
At time: 1148.6281623840332 and batch: 750, loss is 3.939904613494873 and perplexity is 51.41369689376117
At time: 1150.2209055423737 and batch: 800, loss is 3.891040716171265 and perplexity is 48.96181537092875
At time: 1151.8187930583954 and batch: 850, loss is 3.916149787902832 and perplexity is 50.20676546834956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.346302668253581 and perplexity of 77.19252826799979
Finished 39 epochs...
Completing Train Step...
At time: 1155.944180727005 and batch: 50, loss is 3.943783092498779 and perplexity is 51.61349103591767
At time: 1157.5143506526947 and batch: 100, loss is 3.9127944135665893 and perplexity is 50.038585287511154
At time: 1159.09450674057 and batch: 150, loss is 3.9154140758514404 and perplexity is 50.169841330365244
At time: 1160.6643407344818 and batch: 200, loss is 3.9750467777252196 and perplexity is 53.25260788333282
At time: 1162.2452237606049 and batch: 250, loss is 3.952114977836609 and perplexity is 52.04532522341247
At time: 1163.821366071701 and batch: 300, loss is 3.944720106124878 and perplexity is 51.66187624556907
At time: 1165.4040174484253 and batch: 350, loss is 3.8982139730453493 and perplexity is 49.31429374715891
At time: 1166.9815723896027 and batch: 400, loss is 3.9064349794387816 and perplexity is 49.72137789932324
At time: 1168.5683360099792 and batch: 450, loss is 3.95050199508667 and perplexity is 51.96144467873405
At time: 1170.1590983867645 and batch: 500, loss is 3.921568169593811 and perplexity is 50.47954322676247
At time: 1171.7493250370026 and batch: 550, loss is 3.9485797691345215 and perplexity is 51.861658977319195
At time: 1173.343104839325 and batch: 600, loss is 3.9809739494323733 and perplexity is 53.56918250336895
At time: 1174.9347066879272 and batch: 650, loss is 3.956900215148926 and perplexity is 52.294971287056086
At time: 1176.5248720645905 and batch: 700, loss is 3.9400942134857178 and perplexity is 51.42344585439374
At time: 1178.1163864135742 and batch: 750, loss is 3.939451422691345 and perplexity is 51.390401958077554
At time: 1179.714329957962 and batch: 800, loss is 3.890604271888733 and perplexity is 48.94045092908111
At time: 1181.3148052692413 and batch: 850, loss is 3.9157723999023437 and perplexity is 50.18782161233527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.34629758199056 and perplexity of 77.19213564749623
Finished 40 epochs...
Completing Train Step...
At time: 1185.4424831867218 and batch: 50, loss is 3.942744936943054 and perplexity is 51.55993600748351
At time: 1187.0232820510864 and batch: 100, loss is 3.911699252128601 and perplexity is 49.983814955144965
At time: 1188.591258764267 and batch: 150, loss is 3.9144120693206785 and perplexity is 50.119595998983954
At time: 1190.1623327732086 and batch: 200, loss is 3.9742088317871094 and perplexity is 53.20800376739201
At time: 1191.7339062690735 and batch: 250, loss is 3.9512659883499146 and perplexity is 52.00115804085594
At time: 1193.3081529140472 and batch: 300, loss is 3.943906741142273 and perplexity is 51.61987336864551
At time: 1194.9103009700775 and batch: 350, loss is 3.8974594116210937 and perplexity is 49.2770971187682
At time: 1196.478462934494 and batch: 400, loss is 3.905747094154358 and perplexity is 49.687187056182175
At time: 1198.0512626171112 and batch: 450, loss is 3.949791502952576 and perplexity is 51.924539592952684
At time: 1199.6427128314972 and batch: 500, loss is 3.920843548774719 and perplexity is 50.442977948383046
At time: 1201.229120016098 and batch: 550, loss is 3.9479368257522585 and perplexity is 51.82832558377721
At time: 1202.8177843093872 and batch: 600, loss is 3.9803988695144654 and perplexity is 53.53838479871203
At time: 1204.4046926498413 and batch: 650, loss is 3.9563836097717284 and perplexity is 52.26796240075743
At time: 1205.9922082424164 and batch: 700, loss is 3.9395902156829834 and perplexity is 51.397535080709076
At time: 1207.5797138214111 and batch: 750, loss is 3.938994140625 and perplexity is 51.366907421104486
At time: 1209.1672565937042 and batch: 800, loss is 3.890163822174072 and perplexity is 48.91889986786181
At time: 1210.7556920051575 and batch: 850, loss is 3.915382833480835 and perplexity is 50.16827393007397
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.346280097961426 and perplexity of 77.19078602974608
Finished 41 epochs...
Completing Train Step...
At time: 1214.8847861289978 and batch: 50, loss is 3.9416742086410523 and perplexity is 51.50475886988385
At time: 1216.456226348877 and batch: 100, loss is 3.9106547260284423 and perplexity is 49.931632813387694
At time: 1218.0279195308685 and batch: 150, loss is 3.913447380065918 and perplexity is 50.07126947705683
At time: 1219.5959687232971 and batch: 200, loss is 3.9733919858932496 and perplexity is 53.164558774341906
At time: 1221.1637964248657 and batch: 250, loss is 3.950436625480652 and perplexity is 51.958048090585294
At time: 1222.734112739563 and batch: 300, loss is 3.943115587234497 and perplexity is 51.57905025492333
At time: 1224.3085415363312 and batch: 350, loss is 3.8967230463027955 and perplexity is 49.24082453004072
At time: 1225.8972387313843 and batch: 400, loss is 3.905071301460266 and perplexity is 49.65362016158819
At time: 1227.4864468574524 and batch: 450, loss is 3.9490891027450563 and perplexity is 51.888080591471315
At time: 1229.0743489265442 and batch: 500, loss is 3.9201235246658324 and perplexity is 50.40667086069397
At time: 1230.6628682613373 and batch: 550, loss is 3.947295618057251 and perplexity is 51.79510351485493
At time: 1232.2538282871246 and batch: 600, loss is 3.9798201513290405 and perplexity is 53.50741012547857
At time: 1233.8436131477356 and batch: 650, loss is 3.9558628034591674 and perplexity is 52.24074800332454
At time: 1235.4731607437134 and batch: 700, loss is 3.9390809154510498 and perplexity is 51.37136496895933
At time: 1237.067135334015 and batch: 750, loss is 3.9385307884216307 and perplexity is 51.343111964634964
At time: 1238.6540389060974 and batch: 800, loss is 3.8897187328338623 and perplexity is 48.89713143180462
At time: 1240.2409601211548 and batch: 850, loss is 3.914982795715332 and perplexity is 50.148208739556416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.346273422241211 and perplexity of 77.19027072737538
Finished 42 epochs...
Completing Train Step...
At time: 1244.298701286316 and batch: 50, loss is 3.9406363677978518 and perplexity is 51.45133285615455
At time: 1245.9017663002014 and batch: 100, loss is 3.9096325731277464 and perplexity is 49.88062112538484
At time: 1247.4730870723724 and batch: 150, loss is 3.91249641418457 and perplexity is 50.02367604160173
At time: 1249.045019865036 and batch: 200, loss is 3.972585582733154 and perplexity is 53.121703987581256
At time: 1250.615816116333 and batch: 250, loss is 3.9496188879013063 and perplexity is 51.915577409414844
At time: 1252.192626953125 and batch: 300, loss is 3.9423342323303223 and perplexity is 51.538764451858455
At time: 1253.7640416622162 and batch: 350, loss is 3.8959999084472656 and perplexity is 49.20522949739447
At time: 1255.3372209072113 and batch: 400, loss is 3.9044050550460816 and perplexity is 49.62054963298809
At time: 1256.9269652366638 and batch: 450, loss is 3.948392601013184 and perplexity is 51.85195303638865
At time: 1258.5250935554504 and batch: 500, loss is 3.91940860748291 and perplexity is 50.37064714408308
At time: 1260.1529424190521 and batch: 550, loss is 3.946656594276428 and perplexity is 51.7620157850278
At time: 1261.7816779613495 and batch: 600, loss is 3.9792424964904787 and perplexity is 53.47651023671504
At time: 1263.4102201461792 and batch: 650, loss is 3.9553423643112184 and perplexity is 52.213566946601986
At time: 1265.0370535850525 and batch: 700, loss is 3.938569550514221 and perplexity is 51.345102169666816
At time: 1266.6675884723663 and batch: 750, loss is 3.9380635404586792 and perplexity is 51.319127603916094
At time: 1268.296885728836 and batch: 800, loss is 3.889268898963928 and perplexity is 48.875140792382005
At time: 1269.919692516327 and batch: 850, loss is 3.9145760536193848 and perplexity is 50.1278154997014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3462677001953125 and perplexity of 77.18982904236704
Finished 43 epochs...
Completing Train Step...
At time: 1274.2273571491241 and batch: 50, loss is 3.9396209383010863 and perplexity is 51.39911417180757
At time: 1275.8217697143555 and batch: 100, loss is 3.9086279106140136 and perplexity is 49.83053310017213
At time: 1277.3890368938446 and batch: 150, loss is 3.9115624809265137 and perplexity is 49.97697907617498
At time: 1278.968810081482 and batch: 200, loss is 3.971791858673096 and perplexity is 53.079556741869226
At time: 1280.573989391327 and batch: 250, loss is 3.9488127851486206 and perplexity is 51.87374498244034
At time: 1282.1579682826996 and batch: 300, loss is 3.9415642261505126 and perplexity is 51.49909455972187
At time: 1283.7262613773346 and batch: 350, loss is 3.8952872705459596 and perplexity is 49.17017647745092
At time: 1285.2933719158173 and batch: 400, loss is 3.903746371269226 and perplexity is 49.58787614387611
At time: 1286.8618247509003 and batch: 450, loss is 3.9477014636993406 and perplexity is 51.81612859807862
At time: 1288.4339618682861 and batch: 500, loss is 3.918699164390564 and perplexity is 50.33492470942525
At time: 1290.0089268684387 and batch: 550, loss is 3.9460199308395385 and perplexity is 51.72907129054813
At time: 1291.5838649272919 and batch: 600, loss is 3.9786706829071044 and perplexity is 53.44594038273206
At time: 1293.157170534134 and batch: 650, loss is 3.95483452796936 and perplexity is 52.18705773150958
At time: 1294.7475488185883 and batch: 700, loss is 3.938060989379883 and perplexity is 51.31899668494482
At time: 1296.350246667862 and batch: 750, loss is 3.937594108581543 and perplexity is 51.29504242313011
At time: 1297.9611887931824 and batch: 800, loss is 3.8888179779052736 and perplexity is 48.85310693029357
At time: 1299.5693428516388 and batch: 850, loss is 3.914164938926697 and perplexity is 50.107211453840314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.346262296040853 and perplexity of 77.18941189773531
Finished 44 epochs...
Completing Train Step...
At time: 1303.7962458133698 and batch: 50, loss is 3.938623700141907 and perplexity is 51.34788256311075
At time: 1305.371951341629 and batch: 100, loss is 3.9076359701156616 and perplexity is 49.78112868350738
At time: 1306.9500849246979 and batch: 150, loss is 3.910643000602722 and perplexity is 49.93104734716849
At time: 1308.5270652770996 and batch: 200, loss is 3.9710058307647706 and perplexity is 53.03785112194647
At time: 1310.1020681858063 and batch: 250, loss is 3.948017268180847 and perplexity is 51.83249494785148
At time: 1311.6706562042236 and batch: 300, loss is 3.9408034086227417 and perplexity is 51.45992804709043
At time: 1313.2399439811707 and batch: 350, loss is 3.89458544254303 and perplexity is 49.13567957755123
At time: 1314.8081402778625 and batch: 400, loss is 3.9030952882766723 and perplexity is 49.55560082917636
At time: 1316.4077010154724 and batch: 450, loss is 3.9470158052444457 and perplexity is 51.78061260871604
At time: 1317.988995552063 and batch: 500, loss is 3.917995390892029 and perplexity is 50.299512785812304
At time: 1319.5652050971985 and batch: 550, loss is 3.9453900146484373 and perplexity is 51.6964965717401
At time: 1321.1406116485596 and batch: 600, loss is 3.9780966234207153 and perplexity is 53.41526803836383
At time: 1322.7158801555634 and batch: 650, loss is 3.954302968978882 and perplexity is 52.159324603335605
At time: 1324.3110349178314 and batch: 700, loss is 3.9375445652008056 and perplexity is 51.292501156265395
At time: 1325.9099671840668 and batch: 750, loss is 3.9371188116073608 and perplexity is 51.27066783771806
At time: 1327.5000023841858 and batch: 800, loss is 3.888361530303955 and perplexity is 48.83081313518
At time: 1329.0899381637573 and batch: 850, loss is 3.9137492513656618 and perplexity is 50.086386837887574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.346273104349772 and perplexity of 77.19024618925309
Annealing...
Finished 45 epochs...
Completing Train Step...
At time: 1333.2792942523956 and batch: 50, loss is 3.9403777551651 and perplexity is 51.438028611903
At time: 1334.8890030384064 and batch: 100, loss is 3.9124332809448243 and perplexity is 50.02051798455946
At time: 1336.4927678108215 and batch: 150, loss is 3.9162448978424074 and perplexity is 50.21154085786942
At time: 1338.0985939502716 and batch: 200, loss is 3.9798497772216797 and perplexity is 53.50899535374813
At time: 1339.6777765750885 and batch: 250, loss is 3.9577537965774536 and perplexity is 52.339628359864676
At time: 1341.2632508277893 and batch: 300, loss is 3.9510732460021973 and perplexity is 51.991136181420515
At time: 1342.846203327179 and batch: 350, loss is 3.902821464538574 and perplexity is 49.54203318696964
At time: 1344.4336020946503 and batch: 400, loss is 3.909537019729614 and perplexity is 49.87585509024442
At time: 1346.025314092636 and batch: 450, loss is 3.9522023725509645 and perplexity is 52.049873908506534
At time: 1347.6167511940002 and batch: 500, loss is 3.9221515703201293 and perplexity is 50.50900162113398
At time: 1349.214881181717 and batch: 550, loss is 3.9480888748168947 and perplexity is 51.83620663134166
At time: 1350.8166341781616 and batch: 600, loss is 3.9796509313583375 and perplexity is 53.49835636916454
At time: 1352.4093842506409 and batch: 650, loss is 3.954474024772644 and perplexity is 52.16824752114437
At time: 1354.0057964324951 and batch: 700, loss is 3.9362824535369874 and perplexity is 51.227805127683794
At time: 1355.6124248504639 and batch: 750, loss is 3.937059144973755 and perplexity is 51.267608780828176
At time: 1357.2347440719604 and batch: 800, loss is 3.8876617908477784 and perplexity is 48.796656240409305
At time: 1358.8276526927948 and batch: 850, loss is 3.914291458129883 and perplexity is 50.11355137936003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.344037691752116 and perplexity of 77.01788685938838
Finished 46 epochs...
Completing Train Step...
At time: 1362.9349281787872 and batch: 50, loss is 3.942149772644043 and perplexity is 51.529258504295456
At time: 1364.505607843399 and batch: 100, loss is 3.9112892150878906 and perplexity is 49.96332394090185
At time: 1366.077669620514 and batch: 150, loss is 3.912290892601013 and perplexity is 50.01339615289201
At time: 1367.6484735012054 and batch: 200, loss is 3.9749207353591918 and perplexity is 53.2458962216238
At time: 1369.2189965248108 and batch: 250, loss is 3.953251643180847 and perplexity is 52.10451697515694
At time: 1370.7902972698212 and batch: 300, loss is 3.9472927188873292 and perplexity is 51.79495335226639
At time: 1372.3601348400116 and batch: 350, loss is 3.9000396871566774 and perplexity is 49.40440978717476
At time: 1373.9338448047638 and batch: 400, loss is 3.9076941776275635 and perplexity is 49.78402640348144
At time: 1375.5050156116486 and batch: 450, loss is 3.951022686958313 and perplexity is 51.98850762573391
At time: 1377.0941755771637 and batch: 500, loss is 3.920803952217102 and perplexity is 50.440980619644264
At time: 1378.6835520267487 and batch: 550, loss is 3.9474775266647337 and perplexity is 51.80452634703089
At time: 1380.2817504405975 and batch: 600, loss is 3.9795205211639404 and perplexity is 53.49138009300916
At time: 1381.874739408493 and batch: 650, loss is 3.954627470970154 and perplexity is 52.17625315455853
At time: 1383.4647996425629 and batch: 700, loss is 3.936841254234314 and perplexity is 51.25643926055326
At time: 1385.0672781467438 and batch: 750, loss is 3.938010711669922 and perplexity is 51.31641654817622
At time: 1386.6650414466858 and batch: 800, loss is 3.8887564277648927 and perplexity is 48.85010010724011
At time: 1388.2548604011536 and batch: 850, loss is 3.915360698699951 and perplexity is 50.16716347861304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.343933741251628 and perplexity of 77.00988122760472
Finished 47 epochs...
Completing Train Step...
At time: 1392.322030544281 and batch: 50, loss is 3.9414047241210937 and perplexity is 51.49088100468312
At time: 1393.9206037521362 and batch: 100, loss is 3.9103608751296997 and perplexity is 49.91696251375572
At time: 1395.5011794567108 and batch: 150, loss is 3.910980911254883 and perplexity is 49.94792243091535
At time: 1397.0951101779938 and batch: 200, loss is 3.9736007261276245 and perplexity is 53.17565751513747
At time: 1398.6634862422943 and batch: 250, loss is 3.952100839614868 and perplexity is 52.04458940026551
At time: 1400.2325513362885 and batch: 300, loss is 3.9463363885879517 and perplexity is 51.745443946466516
At time: 1401.8028736114502 and batch: 350, loss is 3.899301176071167 and perplexity is 49.36793755210644
At time: 1403.3737223148346 and batch: 400, loss is 3.907081379890442 and perplexity is 49.75352821032286
At time: 1404.9485492706299 and batch: 450, loss is 3.950484495162964 and perplexity is 51.960535365372984
At time: 1406.5243256092072 and batch: 500, loss is 3.9202760314941405 and perplexity is 50.41435880840987
At time: 1408.1083114147186 and batch: 550, loss is 3.947132306098938 and perplexity is 51.7866454457396
At time: 1409.6962938308716 and batch: 600, loss is 3.979274716377258 and perplexity is 53.47823327157808
At time: 1411.2848026752472 and batch: 650, loss is 3.95451798915863 and perplexity is 52.17054111653249
At time: 1412.8867435455322 and batch: 700, loss is 3.9368456649780272 and perplexity is 51.256665340069084
At time: 1414.483639717102 and batch: 750, loss is 3.9381557416915896 and perplexity is 51.32385950889344
At time: 1416.073867559433 and batch: 800, loss is 3.8889935064315795 and perplexity is 48.861682796791094
At time: 1417.667723417282 and batch: 850, loss is 3.9155779600143434 and perplexity is 50.178064046582726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.343885103861491 and perplexity of 77.00613575905273
Finished 48 epochs...
Completing Train Step...
At time: 1421.7482945919037 and batch: 50, loss is 3.9408093643188478 and perplexity is 51.46023452769617
At time: 1423.345053434372 and batch: 100, loss is 3.9096355676651 and perplexity is 49.88077049499167
At time: 1424.9167971611023 and batch: 150, loss is 3.910113515853882 and perplexity is 49.90461661705628
At time: 1426.4864556789398 and batch: 200, loss is 3.972800974845886 and perplexity is 53.13314721598139
At time: 1428.0551400184631 and batch: 250, loss is 3.951391439437866 and perplexity is 52.0076820519205
At time: 1429.6242578029633 and batch: 300, loss is 3.945725531578064 and perplexity is 51.7138445316467
At time: 1431.192990064621 and batch: 350, loss is 3.8987886238098146 and perplexity is 49.34264038768904
At time: 1432.7755272388458 and batch: 400, loss is 3.9066347312927245 and perplexity is 49.73131082876674
At time: 1434.3594393730164 and batch: 450, loss is 3.950051188468933 and perplexity is 51.938025394786415
At time: 1435.9708135128021 and batch: 500, loss is 3.919852809906006 and perplexity is 50.39302687779537
At time: 1437.559886932373 and batch: 550, loss is 3.9468162488937377 and perplexity is 51.7702804895807
At time: 1439.1509773731232 and batch: 600, loss is 3.9789998531341553 and perplexity is 53.46353609089554
At time: 1440.7396824359894 and batch: 650, loss is 3.954324598312378 and perplexity is 52.160452786963276
At time: 1442.3276958465576 and batch: 700, loss is 3.936696319580078 and perplexity is 51.24901096457342
At time: 1443.9154345989227 and batch: 750, loss is 3.9380891799926756 and perplexity is 51.320443419301334
At time: 1445.5027742385864 and batch: 800, loss is 3.888999137878418 and perplexity is 48.86195795953499
At time: 1447.0914113521576 and batch: 850, loss is 3.9155730295181272 and perplexity is 50.17781664443772
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.34386666615804 and perplexity of 77.00471595584668
Finished 49 epochs...
Completing Train Step...
At time: 1451.1731572151184 and batch: 50, loss is 3.9402335929870604 and perplexity is 51.43061372814998
At time: 1452.7756659984589 and batch: 100, loss is 3.90900297164917 and perplexity is 49.849226096787284
At time: 1454.353708267212 and batch: 150, loss is 3.909402208328247 and perplexity is 49.8691317095297
At time: 1455.9270799160004 and batch: 200, loss is 3.972180972099304 and perplexity is 53.10021472894408
At time: 1457.5005712509155 and batch: 250, loss is 3.9508298349380495 and perplexity is 51.978482503721395
At time: 1459.0804045200348 and batch: 300, loss is 3.9452257442474363 and perplexity is 51.68800506498862
At time: 1460.653214931488 and batch: 350, loss is 3.898347845077515 and perplexity is 49.32089599379587
At time: 1462.225114107132 and batch: 400, loss is 3.906243896484375 and perplexity is 49.71187789921002
At time: 1463.797577381134 and batch: 450, loss is 3.9496601819992065 and perplexity is 51.917721260614826
At time: 1465.3723182678223 and batch: 500, loss is 3.9194708490371704 and perplexity is 50.373782389020676
At time: 1466.9519946575165 and batch: 550, loss is 3.946503782272339 and perplexity is 51.754106531989784
At time: 1468.5445992946625 and batch: 600, loss is 3.97871196269989 and perplexity is 53.448146665613436
At time: 1470.1369383335114 and batch: 650, loss is 3.9540978717803954 and perplexity is 52.14862796894688
At time: 1471.729807138443 and batch: 700, loss is 3.936487774848938 and perplexity is 51.23832436771609
At time: 1473.3226709365845 and batch: 750, loss is 3.937944793701172 and perplexity is 51.313033985720885
At time: 1474.9148671627045 and batch: 800, loss is 3.8889119815826416 and perplexity is 48.857699517852545
At time: 1476.535637140274 and batch: 850, loss is 3.915488476753235 and perplexity is 50.17357415066399
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.343840599060059 and perplexity of 77.00270869253269
Finished Training.
Improved accuracyfrom -146.22318586008308 to -77.00270869253269
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd1269705c0>
SETTINGS FOR THIS RUN
{'seq_len': 50, 'lr': 27.184867592859394, 'dropout': 0.8958463364823845, 'wordvec_source': 'glove', 'num_layers': 1, 'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 7.096489828386398, 'batch_size': 50, 'data': 'wikitext'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.029672384262085 and batch: 50, loss is 7.509174022674561 and perplexity is 1824.7057546239846
At time: 3.608229637145996 and batch: 100, loss is 6.8533087253570555 and perplexity is 947.0091217098463
At time: 5.166243076324463 and batch: 150, loss is 6.700630521774292 and perplexity is 812.9182262529176
At time: 6.724969863891602 and batch: 200, loss is 6.711893939971924 and perplexity is 822.1262236109907
At time: 8.28519892692566 and batch: 250, loss is 6.739060163497925 and perplexity is 844.766420328591
At time: 9.85410451889038 and batch: 300, loss is 6.6724997329711915 and perplexity is 790.3688482131703
At time: 11.43140983581543 and batch: 350, loss is 6.67011058807373 and perplexity is 788.4827964349747
At time: 13.003251314163208 and batch: 400, loss is 6.709193820953369 and perplexity is 819.9093791777251
At time: 14.576176404953003 and batch: 450, loss is 6.688495655059814 and perplexity is 803.1131836897113
At time: 16.151028871536255 and batch: 500, loss is 6.696996707916259 and perplexity is 809.9695933730499
At time: 17.727741956710815 and batch: 550, loss is 6.663778619766235 and perplexity is 783.5059216919982
At time: 19.30285382270813 and batch: 600, loss is 6.691801204681396 and perplexity is 805.7723066803653
At time: 20.883659601211548 and batch: 650, loss is 6.707289485931397 and perplexity is 818.3494827861904
At time: 22.463277101516724 and batch: 700, loss is 6.684960803985596 and perplexity is 800.2793098015169
At time: 24.043787479400635 and batch: 750, loss is 6.681604490280152 and perplexity is 797.5978238570691
At time: 25.619406700134277 and batch: 800, loss is 6.709616603851319 and perplexity is 820.2560961289441
At time: 27.192519426345825 and batch: 850, loss is 6.71502233505249 and perplexity is 824.7021864584082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.08541997273763 and perplexity of 439.4043117598157
Finished 1 epochs...
Completing Train Step...
At time: 31.29157829284668 and batch: 50, loss is 6.568786134719849 and perplexity is 712.5044335999376
At time: 32.854168176651 and batch: 100, loss is 6.9714421463012695 and perplexity is 1065.7586226514597
At time: 34.41484975814819 and batch: 150, loss is 7.099960680007935 and perplexity is 1211.9194208937
At time: 35.97557711601257 and batch: 200, loss is 7.173914556503296 and perplexity is 1304.9428902038494
At time: 37.54330015182495 and batch: 250, loss is 7.486236476898194 and perplexity is 1783.327850797742
At time: 39.103853940963745 and batch: 300, loss is 7.434049797058106 and perplexity is 1692.6485949424593
At time: 40.666571855545044 and batch: 350, loss is 7.289423313140869 and perplexity is 1464.7257654957025
At time: 42.22800850868225 and batch: 400, loss is 7.277842044830322 and perplexity is 1447.8602340347697
At time: 43.79582858085632 and batch: 450, loss is 7.604434423446655 and perplexity is 2007.0764172779573
At time: 45.358306646347046 and batch: 500, loss is 7.536443939208985 and perplexity is 1875.1500075086683
At time: 46.919493198394775 and batch: 550, loss is 7.213135643005371 and perplexity is 1357.1411124989065
At time: 48.50708055496216 and batch: 600, loss is 7.249024486541748 and perplexity is 1406.7318927517808
At time: 50.068544149398804 and batch: 650, loss is 7.1098157405853275 and perplexity is 1223.9220061609878
At time: 51.6321005821228 and batch: 700, loss is 7.355994119644165 and perplexity is 1565.5525686156388
At time: 53.20439076423645 and batch: 750, loss is 7.465695486068726 and perplexity is 1747.0701887678076
At time: 54.76612901687622 and batch: 800, loss is 7.132767810821533 and perplexity is 1252.3384101973527
At time: 56.33071517944336 and batch: 850, loss is 7.422492971420288 and perplexity is 1673.1995513352365
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.836222330729167 and perplexity of 930.9656033993267
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 60.41732311248779 and batch: 50, loss is 6.673430891036987 and perplexity is 791.1051492942817
At time: 61.9835307598114 and batch: 100, loss is 6.482302293777466 and perplexity is 653.4737041551359
At time: 63.55017566680908 and batch: 150, loss is 6.462238111495972 and perplexity is 640.492948235776
At time: 65.12024211883545 and batch: 200, loss is 6.531111278533936 and perplexity is 686.1603031810772
At time: 66.72497606277466 and batch: 250, loss is 6.570707750320435 and perplexity is 713.87490957732
At time: 68.33155632019043 and batch: 300, loss is 6.536052923202515 and perplexity is 689.5594553689964
At time: 69.93505692481995 and batch: 350, loss is 6.524975452423096 and perplexity is 681.9630329052277
At time: 71.54132509231567 and batch: 400, loss is 6.553121643066406 and perplexity is 701.4303750374307
At time: 73.14471769332886 and batch: 450, loss is 6.558338804244995 and perplexity is 705.0994130185965
At time: 74.74167370796204 and batch: 500, loss is 6.5585810089111325 and perplexity is 705.270212069849
At time: 76.31918716430664 and batch: 550, loss is 6.512328968048096 and perplexity is 673.3929032910386
At time: 77.88796782493591 and batch: 600, loss is 6.5356862449646 and perplexity is 689.3066552739431
At time: 79.45909214019775 and batch: 650, loss is 6.562721652984619 and perplexity is 708.1965392523427
At time: 81.0278251171112 and batch: 700, loss is 6.520082616806031 and perplexity is 678.6344496341754
At time: 82.62187242507935 and batch: 750, loss is 6.509716873168945 and perplexity is 671.6362324309939
At time: 84.19098711013794 and batch: 800, loss is 6.561840171813965 and perplexity is 707.5725523946059
At time: 85.76027417182922 and batch: 850, loss is 6.56192907333374 and perplexity is 707.6354594660907
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.102755864461263 and perplexity of 447.08818832008143
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 89.81769132614136 and batch: 50, loss is 6.540904760360718 and perplexity is 692.9132149259184
At time: 91.42412567138672 and batch: 100, loss is 6.503769073486328 and perplexity is 667.6533311520807
At time: 92.99169588088989 and batch: 150, loss is 6.4879366970062256 and perplexity is 657.1660307640217
At time: 94.55974578857422 and batch: 200, loss is 6.52801965713501 and perplexity is 684.0422311301736
At time: 96.12837743759155 and batch: 250, loss is 6.545705652236938 and perplexity is 696.2478144715083
At time: 97.70055079460144 and batch: 300, loss is 6.4944155979156495 and perplexity is 661.4375668460057
At time: 99.2689459323883 and batch: 350, loss is 6.475111579895019 and perplexity is 648.7916156726429
At time: 100.8402955532074 and batch: 400, loss is 6.49784743309021 and perplexity is 663.7114110512646
At time: 102.4367139339447 and batch: 450, loss is 6.505565166473389 and perplexity is 668.8535761711204
At time: 104.01169729232788 and batch: 500, loss is 6.4939752864837645 and perplexity is 661.146392432261
At time: 105.58081698417664 and batch: 550, loss is 6.427273263931275 and perplexity is 618.4852007701237
At time: 107.15034747123718 and batch: 600, loss is 6.430512104034424 and perplexity is 620.4916229278953
At time: 108.72084593772888 and batch: 650, loss is 6.448738851547241 and perplexity is 631.9048642382147
At time: 110.291743516922 and batch: 700, loss is 6.412981729507447 and perplexity is 609.7089605790093
At time: 111.86849546432495 and batch: 750, loss is 6.381856250762939 and perplexity is 591.0237783922321
At time: 113.43789005279541 and batch: 800, loss is 6.420972270965576 and perplexity is 614.6003818321639
At time: 115.00782132148743 and batch: 850, loss is 6.443255338668823 and perplexity is 628.449288782266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.0085188547770185 and perplexity of 406.8802250466959
Finished 4 epochs...
Completing Train Step...
At time: 119.11490321159363 and batch: 50, loss is 6.477993698120117 and perplexity is 650.6642070310652
At time: 120.68081259727478 and batch: 100, loss is 6.446335163116455 and perplexity is 630.3877858490116
At time: 122.26955986022949 and batch: 150, loss is 6.431430473327636 and perplexity is 621.0617251231942
At time: 123.83543610572815 and batch: 200, loss is 6.477928953170776 and perplexity is 650.6220811736762
At time: 125.41822910308838 and batch: 250, loss is 6.501996583938599 and perplexity is 666.4709707713689
At time: 127.02619099617004 and batch: 300, loss is 6.456780023574829 and perplexity is 637.0066044519357
At time: 128.6126401424408 and batch: 350, loss is 6.438441429138184 and perplexity is 625.4312608464389
At time: 130.1824824810028 and batch: 400, loss is 6.461121530532837 and perplexity is 639.7781851224576
At time: 131.75864434242249 and batch: 450, loss is 6.470962438583374 and perplexity is 646.1052644566877
At time: 133.3256196975708 and batch: 500, loss is 6.460328149795532 and perplexity is 639.270798736141
At time: 134.8932752609253 and batch: 550, loss is 6.401076412200927 and perplexity is 602.4932200069089
At time: 136.46069025993347 and batch: 600, loss is 6.413285751342773 and perplexity is 609.8943535965541
At time: 138.02856516838074 and batch: 650, loss is 6.434515199661255 and perplexity is 622.9804884899218
At time: 139.6022081375122 and batch: 700, loss is 6.407167863845825 and perplexity is 606.1744790456805
At time: 141.16973304748535 and batch: 750, loss is 6.383461389541626 and perplexity is 591.9732153632559
At time: 142.73640489578247 and batch: 800, loss is 6.426368570327758 and perplexity is 617.9259141947579
At time: 144.30342292785645 and batch: 850, loss is 6.4444448852539065 and perplexity is 629.1973032984869
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.003587086995442 and perplexity of 404.8785262757674
Finished 5 epochs...
Completing Train Step...
At time: 148.37933683395386 and batch: 50, loss is 6.461511707305908 and perplexity is 640.0278604157926
At time: 149.94723296165466 and batch: 100, loss is 6.425928821563721 and perplexity is 617.6542417759072
At time: 151.51410150527954 and batch: 150, loss is 6.407471466064453 and perplexity is 606.3585429020779
At time: 153.09303855895996 and batch: 200, loss is 6.457824811935425 and perplexity is 637.6724893317032
At time: 154.66200590133667 and batch: 250, loss is 6.482728815078735 and perplexity is 653.7524840583344
At time: 156.23187589645386 and batch: 300, loss is 6.43669394493103 and perplexity is 624.3392839794953
At time: 157.79940915107727 and batch: 350, loss is 6.42064359664917 and perplexity is 614.3984116648239
At time: 159.36781978607178 and batch: 400, loss is 6.445249691009521 and perplexity is 629.7038887336922
At time: 160.97459053993225 and batch: 450, loss is 6.456301555633545 and perplexity is 636.7018901171526
At time: 162.54470038414001 and batch: 500, loss is 6.445827131271362 and perplexity is 630.0676101156635
At time: 164.11649894714355 and batch: 550, loss is 6.393040323257447 and perplexity is 597.670933018067
At time: 165.69118356704712 and batch: 600, loss is 6.402541055679321 and perplexity is 603.3763043163411
At time: 167.26081919670105 and batch: 650, loss is 6.425549020767212 and perplexity is 617.4197007451681
At time: 168.8335382938385 and batch: 700, loss is 6.399053993225098 and perplexity is 601.275957608147
At time: 170.40162062644958 and batch: 750, loss is 6.379472599029541 and perplexity is 589.6166612432496
At time: 171.9713649749756 and batch: 800, loss is 6.422060527801514 and perplexity is 615.2695889676372
At time: 173.54090332984924 and batch: 850, loss is 6.438947649002075 and perplexity is 625.747946723756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.9855162302653 and perplexity of 397.6277357473424
Finished 6 epochs...
Completing Train Step...
At time: 177.6105353832245 and batch: 50, loss is 6.446383247375488 and perplexity is 630.4180983073679
At time: 179.18162655830383 and batch: 100, loss is 6.4062487983703615 and perplexity is 605.6176209436078
At time: 180.75038743019104 and batch: 150, loss is 6.384655437469482 and perplexity is 592.6804819254612
At time: 182.32130789756775 and batch: 200, loss is 6.439499454498291 and perplexity is 626.0933331643083
At time: 183.89273071289062 and batch: 250, loss is 6.465091419219971 and perplexity is 642.3230814370954
At time: 185.4645459651947 and batch: 300, loss is 6.419101152420044 and perplexity is 613.4514668728744
At time: 187.03624629974365 and batch: 350, loss is 6.402414064407349 and perplexity is 603.2996856570318
At time: 188.62102556228638 and batch: 400, loss is 6.42965425491333 and perplexity is 619.9595629807282
At time: 190.1949598789215 and batch: 450, loss is 6.442690496444702 and perplexity is 628.0944143217176
At time: 191.76991653442383 and batch: 500, loss is 6.431305131912231 and perplexity is 620.9838852458954
At time: 193.3397035598755 and batch: 550, loss is 6.382785158157349 and perplexity is 591.573039817257
At time: 194.91081309318542 and batch: 600, loss is 6.398946762084961 and perplexity is 601.2114855584432
At time: 196.48157835006714 and batch: 650, loss is 6.424024143218994 and perplexity is 616.4789287690008
At time: 198.05156755447388 and batch: 700, loss is 6.397996530532837 and perplexity is 600.6404667783635
At time: 199.62564134597778 and batch: 750, loss is 6.379967107772827 and perplexity is 589.9083039414102
At time: 201.2758412361145 and batch: 800, loss is 6.4219200325012205 and perplexity is 615.1831525540711
At time: 202.88215684890747 and batch: 850, loss is 6.437578201293945 and perplexity is 624.8916041242845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.983212153116862 and perplexity of 396.71262541506707
Finished 7 epochs...
Completing Train Step...
At time: 207.220440864563 and batch: 50, loss is 6.438450012207031 and perplexity is 625.4366289890478
At time: 208.8609585762024 and batch: 100, loss is 6.396093072891236 and perplexity is 599.4982605075119
At time: 210.44008493423462 and batch: 150, loss is 6.371673460006714 and perplexity is 585.0360445836836
At time: 212.00786876678467 and batch: 200, loss is 6.428753528594971 and perplexity is 619.4014004995882
At time: 213.57574653625488 and batch: 250, loss is 6.457544612884521 and perplexity is 637.4938391353816
At time: 215.14475870132446 and batch: 300, loss is 6.411712799072266 and perplexity is 608.9357729867099
At time: 216.7131109237671 and batch: 350, loss is 6.396294279098511 and perplexity is 599.6188954146156
At time: 218.2854871749878 and batch: 400, loss is 6.4251704788208 and perplexity is 617.1860257205848
At time: 219.85979580879211 and batch: 450, loss is 6.439013977050781 and perplexity is 625.7894527405352
At time: 221.4352593421936 and batch: 500, loss is 6.427518434524536 and perplexity is 618.6368537433852
At time: 223.01606440544128 and batch: 550, loss is 6.380509271621704 and perplexity is 590.2282176129369
At time: 224.59090447425842 and batch: 600, loss is 6.398465824127197 and perplexity is 600.9224096537544
At time: 226.15968799591064 and batch: 650, loss is 6.4238047027587895 and perplexity is 616.3436631910783
At time: 227.72950148582458 and batch: 700, loss is 6.397084646224975 and perplexity is 600.0930018123336
At time: 229.2981493473053 and batch: 750, loss is 6.369134426116943 and perplexity is 583.5525024186603
At time: 230.8678777217865 and batch: 800, loss is 6.393236875534058 and perplexity is 597.7884181462224
At time: 232.43726348876953 and batch: 850, loss is 6.409366550445557 and perplexity is 607.5087330155129
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.960060119628906 and perplexity of 387.6334279151101
Finished 8 epochs...
Completing Train Step...
At time: 236.52101731300354 and batch: 50, loss is 6.406554050445557 and perplexity is 605.8025151974174
At time: 238.11581349372864 and batch: 100, loss is 6.358715381622314 and perplexity is 577.5040074171303
At time: 239.68263864517212 and batch: 150, loss is 6.337767429351807 and perplexity is 565.5323100844199
At time: 241.28004837036133 and batch: 200, loss is 6.393359384536743 and perplexity is 597.8616570952702
At time: 242.84982085227966 and batch: 250, loss is 6.424667549133301 and perplexity is 616.8757025874393
At time: 244.43013381958008 and batch: 300, loss is 6.378137788772583 and perplexity is 588.8301599079504
At time: 245.99856638908386 and batch: 350, loss is 6.36425747871399 and perplexity is 580.7134760756713
At time: 247.56650376319885 and batch: 400, loss is 6.392554922103882 and perplexity is 597.3808932562401
At time: 249.13485503196716 and batch: 450, loss is 6.4033006572723385 and perplexity is 603.8348040348012
At time: 250.7064619064331 and batch: 500, loss is 6.397567920684814 and perplexity is 600.3830815220492
At time: 252.2746934890747 and batch: 550, loss is 6.351415510177612 and perplexity is 573.3036520835681
At time: 253.8439257144928 and batch: 600, loss is 6.37019528388977 and perplexity is 584.1718971134519
At time: 255.41507983207703 and batch: 650, loss is 6.39629358291626 and perplexity is 599.6184779707288
At time: 256.9941692352295 and batch: 700, loss is 6.370388050079345 and perplexity is 584.2845165583773
At time: 258.5724756717682 and batch: 750, loss is 6.352744064331055 and perplexity is 574.0658232124306
At time: 260.140257358551 and batch: 800, loss is 6.386448602676392 and perplexity is 593.7442093789646
At time: 261.7091419696808 and batch: 850, loss is 6.404207010269165 and perplexity is 604.3823396117992
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.955816268920898 and perplexity of 385.99185527597143
Finished 9 epochs...
Completing Train Step...
At time: 265.77000284194946 and batch: 50, loss is 6.399985904693604 and perplexity is 601.8365547416564
At time: 267.3766689300537 and batch: 100, loss is 6.3520911693572994 and perplexity is 573.691140849276
At time: 268.9595968723297 and batch: 150, loss is 6.331384706497192 and perplexity is 561.9341692670607
At time: 270.5448591709137 and batch: 200, loss is 6.3882488346099855 and perplexity is 594.814049356189
At time: 272.11610412597656 and batch: 250, loss is 6.419872226715088 and perplexity is 613.9246659426116
At time: 273.685822725296 and batch: 300, loss is 6.374517889022827 and perplexity is 586.7025070278535
At time: 275.2594690322876 and batch: 350, loss is 6.360848894119263 and perplexity is 578.7374347324211
At time: 276.83500266075134 and batch: 400, loss is 6.389871120452881 and perplexity is 595.7797909102362
At time: 278.43161940574646 and batch: 450, loss is 6.40105037689209 and perplexity is 602.4775341140479
At time: 280.0222182273865 and batch: 500, loss is 6.395704708099365 and perplexity is 599.2654816948165
At time: 281.6515648365021 and batch: 550, loss is 6.349475631713867 and perplexity is 572.1925906861652
At time: 283.23823618888855 and batch: 600, loss is 6.368254833221435 and perplexity is 583.0394394597226
At time: 284.814879655838 and batch: 650, loss is 6.3937334537506105 and perplexity is 598.0853405692836
At time: 286.39037895202637 and batch: 700, loss is 6.36784948348999 and perplexity is 582.8031524721845
At time: 287.96287512779236 and batch: 750, loss is 6.350629673004151 and perplexity is 572.853305734706
At time: 289.5371913909912 and batch: 800, loss is 6.385108156204224 and perplexity is 592.9488602287034
At time: 291.10932970046997 and batch: 850, loss is 6.4028134059906 and perplexity is 603.5406564202978
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.954589207967122 and perplexity of 385.518510212901
Finished 10 epochs...
Completing Train Step...
At time: 295.49217104911804 and batch: 50, loss is 6.397103328704834 and perplexity is 600.1042131424804
At time: 297.09918236732483 and batch: 100, loss is 6.349315872192383 and perplexity is 572.1011847733573
At time: 298.7099530696869 and batch: 150, loss is 6.328303070068359 and perplexity is 560.2051579209101
At time: 300.322425365448 and batch: 200, loss is 6.385482406616211 and perplexity is 593.1708131143201
At time: 301.93007469177246 and batch: 250, loss is 6.417252120971679 and perplexity is 612.318223842454
At time: 303.5212790966034 and batch: 300, loss is 6.372355737686157 and perplexity is 585.435337817514
At time: 305.1093053817749 and batch: 350, loss is 6.35884949684143 and perplexity is 577.5814646876088
At time: 306.691034078598 and batch: 400, loss is 6.38811469078064 and perplexity is 594.7342640733309
At time: 308.27302980422974 and batch: 450, loss is 6.399431142807007 and perplexity is 601.5027713528368
At time: 309.85940384864807 and batch: 500, loss is 6.3942271327972415 and perplexity is 598.3806756643957
At time: 311.4606878757477 and batch: 550, loss is 6.348007259368896 and perplexity is 571.3530154653741
At time: 313.04746103286743 and batch: 600, loss is 6.366660318374634 and perplexity is 582.1105152058046
At time: 314.6327118873596 and batch: 650, loss is 6.392322359085083 and perplexity is 597.2419807059186
At time: 316.21673679351807 and batch: 700, loss is 6.3663736343383786 and perplexity is 581.9436573326459
At time: 317.80954670906067 and batch: 750, loss is 6.349246444702149 and perplexity is 572.0614666027209
At time: 319.3996801376343 and batch: 800, loss is 6.38434757232666 and perplexity is 592.4980443487714
At time: 321.03337025642395 and batch: 850, loss is 6.40173906326294 and perplexity is 602.8925950875484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.95352045694987 and perplexity of 385.10670700968546
Finished 11 epochs...
Completing Train Step...
At time: 325.1888175010681 and batch: 50, loss is 6.39492769241333 and perplexity is 598.8000238728732
At time: 326.7625515460968 and batch: 100, loss is 6.347204599380493 and perplexity is 570.8945972621934
At time: 328.3290331363678 and batch: 150, loss is 6.325959873199463 and perplexity is 558.8940236716704
At time: 329.89635848999023 and batch: 200, loss is 6.382968158721924 and perplexity is 591.681307923791
At time: 331.4710884094238 and batch: 250, loss is 6.4153151512146 and perplexity is 611.1333298835065
At time: 333.0400278568268 and batch: 300, loss is 6.370331296920776 and perplexity is 584.2513575075089
At time: 334.6091830730438 and batch: 350, loss is 6.356876459121704 and perplexity is 576.4429981595924
At time: 336.19194078445435 and batch: 400, loss is 6.386596212387085 and perplexity is 593.8318582586902
At time: 337.775771856308 and batch: 450, loss is 6.397952432632446 and perplexity is 600.6139803788905
At time: 339.3453767299652 and batch: 500, loss is 6.392875308990479 and perplexity is 597.572316923922
At time: 340.91389417648315 and batch: 550, loss is 6.346692667007447 and perplexity is 570.6024126320242
At time: 342.484495639801 and batch: 600, loss is 6.3654835987091065 and perplexity is 581.4259371722619
At time: 344.06273007392883 and batch: 650, loss is 6.390957670211792 and perplexity is 596.4274871117677
At time: 345.63085865974426 and batch: 700, loss is 6.365040903091431 and perplexity is 581.1685994232223
At time: 347.19951272010803 and batch: 750, loss is 6.3479981613159175 and perplexity is 571.3478172890167
At time: 348.7664954662323 and batch: 800, loss is 6.383463315963745 and perplexity is 591.9743557546501
At time: 350.3553423881531 and batch: 850, loss is 6.400546255111695 and perplexity is 602.1738886104889
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.951967239379883 and perplexity of 384.5090167976673
Finished 12 epochs...
Completing Train Step...
At time: 354.56811022758484 and batch: 50, loss is 6.392723417282104 and perplexity is 597.4815575368008
At time: 356.13821172714233 and batch: 100, loss is 6.345255203247071 and perplexity is 569.7827815783845
At time: 357.70724844932556 and batch: 150, loss is 6.3230862808227535 and perplexity is 557.2902954000965
At time: 359.2785015106201 and batch: 200, loss is 6.380248956680298 and perplexity is 590.0745923853913
At time: 360.8485689163208 and batch: 250, loss is 6.412973651885986 and perplexity is 609.7040356007157
At time: 362.4466607570648 and batch: 300, loss is 6.368005199432373 and perplexity is 582.8939112804087
At time: 364.01718974113464 and batch: 350, loss is 6.354594383239746 and perplexity is 575.1290113752
At time: 365.59577894210815 and batch: 400, loss is 6.383951435089111 and perplexity is 592.2633802928841
At time: 367.1662971973419 and batch: 450, loss is 6.395319385528564 and perplexity is 599.0346156606211
At time: 368.74689841270447 and batch: 500, loss is 6.390438270568848 and perplexity is 596.1177833249004
At time: 370.3183572292328 and batch: 550, loss is 6.3433333683013915 and perplexity is 568.6888046755341
At time: 371.9027624130249 and batch: 600, loss is 6.362748107910156 and perplexity is 579.8376252675895
At time: 373.49410676956177 and batch: 650, loss is 6.387780094146729 and perplexity is 594.5353012784975
At time: 375.0773456096649 and batch: 700, loss is 6.3622705078125 and perplexity is 579.5607608816287
At time: 376.6570827960968 and batch: 750, loss is 6.3451411914825435 and perplexity is 569.7178233411316
At time: 378.23567843437195 and batch: 800, loss is 6.381444072723388 and perplexity is 590.7802215677618
At time: 379.8335590362549 and batch: 850, loss is 6.397986793518067 and perplexity is 600.6346183617399
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.949541727701823 and perplexity of 383.57751582702315
Finished 13 epochs...
Completing Train Step...
At time: 383.90554332733154 and batch: 50, loss is 6.389943981170655 and perplexity is 595.8232014348788
At time: 385.50577044487 and batch: 100, loss is 6.342394046783447 and perplexity is 568.1548738499034
At time: 387.07938170433044 and batch: 150, loss is 6.319857482910156 and perplexity is 555.49381944721
At time: 388.66292810440063 and batch: 200, loss is 6.3769361019134525 and perplexity is 588.1229954227623
At time: 390.2474820613861 and batch: 250, loss is 6.409802551269531 and perplexity is 607.7736650747804
At time: 391.8215272426605 and batch: 300, loss is 6.365030345916748 and perplexity is 581.1624639571846
At time: 393.3971366882324 and batch: 350, loss is 6.351779689788819 and perplexity is 573.5124756070138
At time: 394.97340726852417 and batch: 400, loss is 6.381208019256592 and perplexity is 590.640782306554
At time: 396.55312418937683 and batch: 450, loss is 6.3933393669128415 and perplexity is 597.8496894452556
At time: 398.1308243274689 and batch: 500, loss is 6.38830943107605 and perplexity is 594.8500940776258
At time: 399.7042245864868 and batch: 550, loss is 6.340811767578125 and perplexity is 567.2566050510384
At time: 401.3246874809265 and batch: 600, loss is 6.360658712387085 and perplexity is 578.6273799101476
At time: 402.90426564216614 and batch: 650, loss is 6.385800981521607 and perplexity is 593.3598125536325
At time: 404.4775757789612 and batch: 700, loss is 6.360234184265137 and perplexity is 578.3817884491757
At time: 406.0540225505829 and batch: 750, loss is 6.3432223510742185 and perplexity is 568.62567392568
At time: 407.6270842552185 and batch: 800, loss is 6.37941533088684 and perplexity is 589.5828959590016
At time: 409.20049953460693 and batch: 850, loss is 6.3960498046875 and perplexity is 599.4723218558007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.94741948445638 and perplexity of 382.7643342247452
Finished 14 epochs...
Completing Train Step...
At time: 413.31024050712585 and batch: 50, loss is 6.38726734161377 and perplexity is 594.2305299396488
At time: 414.928542137146 and batch: 100, loss is 6.3392088413238525 and perplexity is 566.3480629034553
At time: 416.5079252719879 and batch: 150, loss is 6.315180816650391 and perplexity is 552.9020254458757
At time: 418.0787401199341 and batch: 200, loss is 6.371311359405517 and perplexity is 584.8242410296295
At time: 419.6564054489136 and batch: 250, loss is 6.402554626464844 and perplexity is 603.3844926623176
At time: 421.2367961406708 and batch: 300, loss is 6.3564048099517825 and perplexity is 576.1711834036442
At time: 422.8140699863434 and batch: 350, loss is 6.341520414352417 and perplexity is 567.6587320805867
At time: 424.38529539108276 and batch: 400, loss is 6.36791446685791 and perplexity is 582.8410262144351
At time: 425.95549845695496 and batch: 450, loss is 6.3775685596466065 and perplexity is 588.4950760094825
At time: 427.5284216403961 and batch: 500, loss is 6.3709170627594 and perplexity is 584.593692248116
At time: 429.0989270210266 and batch: 550, loss is 6.318991165161133 and perplexity is 555.0127936826135
At time: 430.67342162132263 and batch: 600, loss is 6.331571378707886 and perplexity is 562.0390765520349
At time: 432.24900817871094 and batch: 650, loss is 6.350429039001465 and perplexity is 572.7383834120732
At time: 433.82064747810364 and batch: 700, loss is 6.319608745574951 and perplexity is 555.3556645776869
At time: 435.3931770324707 and batch: 750, loss is 6.298584699630737 and perplexity is 543.801722452449
At time: 436.97316670417786 and batch: 800, loss is 6.3282103347778325 and perplexity is 560.153209541597
At time: 438.55218839645386 and batch: 850, loss is 6.335575942993164 and perplexity is 564.2943107662761
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.883725484212239 and perplexity of 359.1447405350873
Finished 15 epochs...
Completing Train Step...
At time: 442.6504259109497 and batch: 50, loss is 6.321955528259277 and perplexity is 556.6604941117885
At time: 444.2540395259857 and batch: 100, loss is 6.26970591545105 and perplexity is 528.3219836970178
At time: 445.8284981250763 and batch: 150, loss is 6.241356010437012 and perplexity is 513.5544241625373
At time: 447.39949464797974 and batch: 200, loss is 6.294342861175537 and perplexity is 541.4998888487389
At time: 448.9707489013672 and batch: 250, loss is 6.320443744659424 and perplexity is 555.8195797065658
At time: 450.54944586753845 and batch: 300, loss is 6.270493841171264 and perplexity is 528.7384262178689
At time: 452.1285665035248 and batch: 350, loss is 6.251460952758789 and perplexity is 518.7701700218157
At time: 453.7048809528351 and batch: 400, loss is 6.274982233047485 and perplexity is 531.1169453446452
At time: 455.28408455848694 and batch: 450, loss is 6.286144247055054 and perplexity is 537.0784896528952
At time: 456.865186214447 and batch: 500, loss is 6.280073127746582 and perplexity is 533.8277000149947
At time: 458.4365463256836 and batch: 550, loss is 6.232250366210938 and perplexity is 508.89940591870584
At time: 460.0117938518524 and batch: 600, loss is 6.24206847190857 and perplexity is 513.92044227472
At time: 461.588383436203 and batch: 650, loss is 6.264072608947754 and perplexity is 525.3541512338448
At time: 463.1626627445221 and batch: 700, loss is 6.2319571399688725 and perplexity is 508.7502051341808
At time: 464.7469573020935 and batch: 750, loss is 6.217442855834961 and perplexity is 501.4193895684352
At time: 466.32221579551697 and batch: 800, loss is 6.251910505294799 and perplexity is 519.0034368962845
At time: 467.89383935928345 and batch: 850, loss is 6.256132869720459 and perplexity is 521.1994915538917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.8164927164713545 and perplexity of 335.79226722884965
Finished 16 epochs...
Completing Train Step...
At time: 472.0058083534241 and batch: 50, loss is 6.2477985191345216 and perplexity is 516.873683705351
At time: 473.5849041938782 and batch: 100, loss is 6.201137323379516 and perplexity is 493.30977490532024
At time: 475.1590132713318 and batch: 150, loss is 6.179556341171264 and perplexity is 482.77772023452815
At time: 476.7353346347809 and batch: 200, loss is 6.232807903289795 and perplexity is 509.18321531669363
At time: 478.3072760105133 and batch: 250, loss is 6.263884792327881 and perplexity is 525.2554902582991
At time: 479.8789722919464 and batch: 300, loss is 6.215501918792724 and perplexity is 500.44710997350046
At time: 481.4761164188385 and batch: 350, loss is 6.199435005187988 and perplexity is 492.47071907406695
At time: 483.0478515625 and batch: 400, loss is 6.22308798789978 and perplexity is 504.25797279278675
At time: 484.6220426559448 and batch: 450, loss is 6.23698694229126 and perplexity is 511.31556431386247
At time: 486.1927258968353 and batch: 500, loss is 6.234347620010376 and perplexity is 509.96781710439524
At time: 487.76529359817505 and batch: 550, loss is 6.185324935913086 and perplexity is 485.5707173420197
At time: 489.33730578422546 and batch: 600, loss is 6.195871562957763 and perplexity is 490.7189511321798
At time: 490.9127595424652 and batch: 650, loss is 6.22201810836792 and perplexity is 503.71876600350544
At time: 492.4846074581146 and batch: 700, loss is 6.1928009605407714 and perplexity is 489.21445936490204
At time: 494.0560817718506 and batch: 750, loss is 6.177947940826416 and perplexity is 482.0018445094726
At time: 495.6286725997925 and batch: 800, loss is 6.213642244338989 and perplexity is 499.51730610185933
At time: 497.20094895362854 and batch: 850, loss is 6.219733753204346 and perplexity is 502.5694067116189
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.785865783691406 and perplexity of 325.6638725618024
Finished 17 epochs...
Completing Train Step...
At time: 501.3089632987976 and batch: 50, loss is 6.215352029800415 and perplexity is 500.3721040819015
At time: 502.88335943222046 and batch: 100, loss is 6.170087032318115 and perplexity is 478.22772554565677
At time: 504.45941972732544 and batch: 150, loss is 6.151035976409912 and perplexity is 469.20321854971274
At time: 506.0347054004669 and batch: 200, loss is 6.203451776504517 and perplexity is 494.4528395299357
At time: 507.60752630233765 and batch: 250, loss is 6.23663709640503 and perplexity is 511.1367139539772
At time: 509.17833733558655 and batch: 300, loss is 6.187920064926147 and perplexity is 486.8324724990666
At time: 510.7494742870331 and batch: 350, loss is 6.174784832000732 and perplexity is 480.47962895701255
At time: 512.3341937065125 and batch: 400, loss is 6.198835239410401 and perplexity is 492.1754405481313
At time: 513.9081325531006 and batch: 450, loss is 6.214118700027466 and perplexity is 499.7553606705676
At time: 515.480372428894 and batch: 500, loss is 6.213072624206543 and perplexity is 499.23285201087816
At time: 517.0525815486908 and batch: 550, loss is 6.165979471206665 and perplexity is 476.267404762715
At time: 518.6251888275146 and batch: 600, loss is 6.180272369384766 and perplexity is 483.12352649180986
At time: 520.2248213291168 and batch: 650, loss is 6.206887216567993 and perplexity is 496.1544237961931
At time: 521.7979383468628 and batch: 700, loss is 6.178873233795166 and perplexity is 482.4480438278323
At time: 523.3695542812347 and batch: 750, loss is 6.166066961288452 and perplexity is 476.3090752597613
At time: 524.9415545463562 and batch: 800, loss is 6.200330753326416 and perplexity is 492.91204643347913
At time: 526.516275882721 and batch: 850, loss is 6.206970424652099 and perplexity is 496.19570957284947
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.777910232543945 and perplexity of 323.08331545423056
Finished 18 epochs...
Completing Train Step...
At time: 530.6242368221283 and batch: 50, loss is 6.202304019927978 and perplexity is 493.88565358953855
At time: 532.1987473964691 and batch: 100, loss is 6.159433145523071 and perplexity is 473.1597860586416
At time: 533.7684948444366 and batch: 150, loss is 6.1423570346832275 and perplexity is 465.14865128495245
At time: 535.3392977714539 and batch: 200, loss is 6.1936701393127445 and perplexity is 489.6398590353244
At time: 536.9102649688721 and batch: 250, loss is 6.227293348312378 and perplexity is 506.3830244816256
At time: 538.4791586399078 and batch: 300, loss is 6.178645524978638 and perplexity is 482.3381986615671
At time: 540.0536115169525 and batch: 350, loss is 6.165255537033081 and perplexity is 475.9227432838072
At time: 541.6214685440063 and batch: 400, loss is 6.189015483856201 and perplexity is 487.3660501974074
At time: 543.1910514831543 and batch: 450, loss is 6.203980474472046 and perplexity is 494.7143248585242
At time: 544.7613425254822 and batch: 500, loss is 6.203425807952881 and perplexity is 494.43999947256026
At time: 546.3388297557831 and batch: 550, loss is 6.156486864089966 and perplexity is 471.7677758000288
At time: 547.9117364883423 and batch: 600, loss is 6.172070436477661 and perplexity is 479.1771856760441
At time: 549.4849095344543 and batch: 650, loss is 6.199602546691895 and perplexity is 492.5532352712216
At time: 551.0670058727264 and batch: 700, loss is 6.171919355392456 and perplexity is 479.1047965352793
At time: 552.6576204299927 and batch: 750, loss is 6.1585565948486325 and perplexity is 472.7452192500067
At time: 554.2622303962708 and batch: 800, loss is 6.192035245895386 and perplexity is 488.84000406987934
At time: 555.8690721988678 and batch: 850, loss is 6.1995165920257564 and perplexity is 492.51089984181834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.772268295288086 and perplexity of 321.2656321093373
Finished 19 epochs...
Completing Train Step...
At time: 560.1660010814667 and batch: 50, loss is 6.195326147079467 and perplexity is 490.45137820034745
At time: 561.772787809372 and batch: 100, loss is 6.152001161575317 and perplexity is 469.656305156879
At time: 563.3423268795013 and batch: 150, loss is 6.136296424865723 and perplexity is 462.338092256077
At time: 564.9178023338318 and batch: 200, loss is 6.186560878753662 and perplexity is 486.17122601446647
At time: 566.4857339859009 and batch: 250, loss is 6.2194897651672365 and perplexity is 502.4468007463668
At time: 568.0576028823853 and batch: 300, loss is 6.1707952690124515 and perplexity is 478.5665439367834
At time: 569.6273362636566 and batch: 350, loss is 6.1578263092041015 and perplexity is 472.4001062337464
At time: 571.2061913013458 and batch: 400, loss is 6.182244663238525 and perplexity is 484.0773283332848
At time: 572.7865419387817 and batch: 450, loss is 6.198955469131469 and perplexity is 492.23461822145157
At time: 574.360454082489 and batch: 500, loss is 6.198018503189087 and perplexity is 491.77362714870947
At time: 575.9401471614838 and batch: 550, loss is 6.152088756561279 and perplexity is 469.69744649619764
At time: 577.513678073883 and batch: 600, loss is 6.167857246398926 and perplexity is 477.16256807495955
At time: 579.0930891036987 and batch: 650, loss is 6.195248708724976 and perplexity is 490.4133999231682
At time: 580.680438041687 and batch: 700, loss is 6.1674549007415775 and perplexity is 476.97062240468824
At time: 582.2538154125214 and batch: 750, loss is 6.154051685333252 and perplexity is 470.6203346135358
At time: 583.8377299308777 and batch: 800, loss is 6.187252111434937 and perplexity is 486.50739962829203
At time: 585.4200875759125 and batch: 850, loss is 6.194436302185059 and perplexity is 490.01514666348805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.768389383951823 and perplexity of 320.0218849589038
Finished 20 epochs...
Completing Train Step...
At time: 589.5192863941193 and batch: 50, loss is 6.190386152267456 and perplexity is 488.0345254714448
At time: 591.1324121952057 and batch: 100, loss is 6.147708215713501 and perplexity is 467.64441761714386
At time: 592.7248442173004 and batch: 150, loss is 6.132643127441407 and perplexity is 460.65211525735685
At time: 594.3129303455353 and batch: 200, loss is 6.182492265701294 and perplexity is 484.1972019118347
At time: 595.9046611785889 and batch: 250, loss is 6.215102081298828 and perplexity is 500.2470524531351
At time: 597.4958963394165 and batch: 300, loss is 6.166293344497681 and perplexity is 476.41691584299133
At time: 599.085191488266 and batch: 350, loss is 6.153273897171021 and perplexity is 470.2544340033955
At time: 600.7207889556885 and batch: 400, loss is 6.178122940063477 and perplexity is 482.086201845543
At time: 602.2991244792938 and batch: 450, loss is 6.195398759841919 and perplexity is 490.4869925227784
At time: 603.8759746551514 and batch: 500, loss is 6.194544401168823 and perplexity is 490.0681196659838
At time: 605.4656744003296 and batch: 550, loss is 6.148957719802857 and perplexity is 468.2291064387867
At time: 607.067878484726 and batch: 600, loss is 6.1646616077423095 and perplexity is 475.640162751161
At time: 608.658528804779 and batch: 650, loss is 6.191617231369019 and perplexity is 488.6357045501674
At time: 610.236093044281 and batch: 700, loss is 6.164162836074829 and perplexity is 475.40298606748684
At time: 611.8077213764191 and batch: 750, loss is 6.150638761520386 and perplexity is 469.0168810555548
At time: 613.3791522979736 and batch: 800, loss is 6.183936386108399 and perplexity is 484.89694610800666
At time: 614.9542627334595 and batch: 850, loss is 6.190724458694458 and perplexity is 488.1996586192384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.766578674316406 and perplexity of 319.44294255473926
Finished 21 epochs...
Completing Train Step...
At time: 619.0408070087433 and batch: 50, loss is 6.186747341156006 and perplexity is 486.261887121401
At time: 620.6502060890198 and batch: 100, loss is 6.144697570800782 and perplexity is 466.2386235654791
At time: 622.2269592285156 and batch: 150, loss is 6.129494113922119 and perplexity is 459.2037971022762
At time: 623.8074517250061 and batch: 200, loss is 6.178781728744507 and perplexity is 482.40389941489093
At time: 625.3765761852264 and batch: 250, loss is 6.211138553619385 and perplexity is 498.26823355644126
At time: 626.9596819877625 and batch: 300, loss is 6.162670030593872 and perplexity is 474.69383133095465
At time: 628.534353017807 and batch: 350, loss is 6.149503421783447 and perplexity is 468.48468971934926
At time: 630.1135873794556 and batch: 400, loss is 6.174866600036621 and perplexity is 480.518918438847
At time: 631.6857368946075 and batch: 450, loss is 6.192368803024292 and perplexity is 489.0030873354122
At time: 633.2536845207214 and batch: 500, loss is 6.191599769592285 and perplexity is 488.62717217708575
At time: 634.8237240314484 and batch: 550, loss is 6.146239519119263 and perplexity is 466.95809397764157
At time: 636.3945763111115 and batch: 600, loss is 6.162040548324585 and perplexity is 474.3951140092846
At time: 637.968906879425 and batch: 650, loss is 6.1888955879211425 and perplexity is 487.3076204919154
At time: 639.5386672019958 and batch: 700, loss is 6.161460866928101 and perplexity is 474.1201956773301
At time: 641.1316068172455 and batch: 750, loss is 6.147913179397583 and perplexity is 467.7402775633873
At time: 642.7011077404022 and batch: 800, loss is 6.181251811981201 and perplexity is 483.59695006075407
At time: 644.2698485851288 and batch: 850, loss is 6.187856998443603 and perplexity is 486.80177065557683
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.764992396036784 and perplexity of 318.9366188447089
Finished 22 epochs...
Completing Train Step...
At time: 648.3663005828857 and batch: 50, loss is 6.183918409347534 and perplexity is 484.88822930991256
At time: 649.9365878105164 and batch: 100, loss is 6.142340612411499 and perplexity is 465.14101255012986
At time: 651.512035369873 and batch: 150, loss is 6.127021818161011 and perplexity is 458.06991172876565
At time: 653.0874936580658 and batch: 200, loss is 6.176037874221802 and perplexity is 481.0820675801499
At time: 654.6569819450378 and batch: 250, loss is 6.208379039764404 and perplexity is 496.89515085414166
At time: 656.2274346351624 and batch: 300, loss is 6.159738531112671 and perplexity is 473.30430430465395
At time: 657.8102588653564 and batch: 350, loss is 6.146287174224853 and perplexity is 466.9803474451579
At time: 659.3807818889618 and batch: 400, loss is 6.172284870147705 and perplexity is 479.2799484160721
At time: 660.9494910240173 and batch: 450, loss is 6.189721021652222 and perplexity is 487.71002669626074
At time: 662.5188446044922 and batch: 500, loss is 6.188825445175171 and perplexity is 487.2734405960311
At time: 664.0978393554688 and batch: 550, loss is 6.143721046447754 and perplexity is 465.78355242543375
At time: 665.6690773963928 and batch: 600, loss is 6.159252529144287 and perplexity is 473.0743333688075
At time: 667.2407429218292 and batch: 650, loss is 6.186367778778076 and perplexity is 486.07735542608856
At time: 668.8137786388397 and batch: 700, loss is 6.1588975620269775 and perplexity is 472.90643733696663
At time: 670.3854749202728 and batch: 750, loss is 6.145632390975952 and perplexity is 466.67467662107356
At time: 671.9567573070526 and batch: 800, loss is 6.178496007919311 and perplexity is 482.2660862636536
At time: 673.5377521514893 and batch: 850, loss is 6.18523416519165 and perplexity is 485.526643738025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.763571421305339 and perplexity of 318.483739809434
Finished 23 epochs...
Completing Train Step...
At time: 677.6852111816406 and batch: 50, loss is 6.181067695617676 and perplexity is 483.5079201450808
At time: 679.2605955600739 and batch: 100, loss is 6.139928197860717 and perplexity is 464.02025201634325
At time: 680.866925239563 and batch: 150, loss is 6.12460150718689 and perplexity is 456.9625806777945
At time: 682.4464552402496 and batch: 200, loss is 6.172976608276367 and perplexity is 479.6115993252597
At time: 684.0359966754913 and batch: 250, loss is 6.205572900772094 and perplexity is 495.5027485471397
At time: 685.6143474578857 and batch: 300, loss is 6.156853876113892 and perplexity is 471.9409520231823
At time: 687.192257642746 and batch: 350, loss is 6.143726835250854 and perplexity is 465.78624876251024
At time: 688.7744474411011 and batch: 400, loss is 6.169809436798095 and perplexity is 478.09499009573165
At time: 690.3526065349579 and batch: 450, loss is 6.187507448196411 and perplexity is 486.63163871287395
At time: 691.9303419589996 and batch: 500, loss is 6.186401920318604 and perplexity is 486.0939511391182
At time: 693.5106339454651 and batch: 550, loss is 6.14189811706543 and perplexity is 464.9352353478952
At time: 695.087325334549 and batch: 600, loss is 6.157734289169311 and perplexity is 472.3566379595425
At time: 696.6645500659943 and batch: 650, loss is 6.184199619293213 and perplexity is 485.0246038765806
At time: 698.2494690418243 and batch: 700, loss is 6.157081365585327 and perplexity is 472.04832583364947
At time: 699.8266489505768 and batch: 750, loss is 6.144075660705567 and perplexity is 465.9487552040775
At time: 701.4000594615936 and batch: 800, loss is 6.1769015502929685 and perplexity is 481.4977461301557
At time: 702.9859528541565 and batch: 850, loss is 6.182834053039551 and perplexity is 484.3627226695061
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.761644999186198 and perplexity of 317.8707962720306
Finished 24 epochs...
Completing Train Step...
At time: 707.1322345733643 and batch: 50, loss is 6.178986673355102 and perplexity is 482.50277562593334
At time: 708.714076757431 and batch: 100, loss is 6.137982549667359 and perplexity is 463.11830956720297
At time: 710.2908215522766 and batch: 150, loss is 6.121839418411255 and perplexity is 455.7021509735205
At time: 711.8655362129211 and batch: 200, loss is 6.1703700256347656 and perplexity is 478.3630799471154
At time: 713.4510171413422 and batch: 250, loss is 6.202943897247314 and perplexity is 494.20178094819295
At time: 715.0326313972473 and batch: 300, loss is 6.154293642044068 and perplexity is 470.7342181386277
At time: 716.613255739212 and batch: 350, loss is 6.140990543365478 and perplexity is 464.5134637794549
At time: 718.1900110244751 and batch: 400, loss is 6.16752254486084 and perplexity is 477.0028877536231
At time: 719.7746379375458 and batch: 450, loss is 6.184778118133545 and perplexity is 485.3052712224957
At time: 721.3797121047974 and batch: 500, loss is 6.184034328460694 and perplexity is 484.9444403813417
At time: 722.9557747840881 and batch: 550, loss is 6.139906921386719 and perplexity is 464.01037940654396
At time: 724.5398640632629 and batch: 600, loss is 6.155785341262817 and perplexity is 471.4369359955974
At time: 726.1173181533813 and batch: 650, loss is 6.1820810031890865 and perplexity is 483.9981106963556
At time: 727.6916425228119 and batch: 700, loss is 6.155108995437622 and perplexity is 471.11818939571106
At time: 729.2646613121033 and batch: 750, loss is 6.142562456130982 and perplexity is 465.2442126091571
At time: 730.8439998626709 and batch: 800, loss is 6.175562877655029 and perplexity is 480.85360951240773
At time: 732.4133734703064 and batch: 850, loss is 6.180825929641724 and perplexity is 483.39103851045695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.760578155517578 and perplexity of 317.5318586544612
Finished 25 epochs...
Completing Train Step...
At time: 736.4990916252136 and batch: 50, loss is 6.177263259887695 and perplexity is 481.6719399866657
At time: 738.101095199585 and batch: 100, loss is 6.136164102554321 and perplexity is 462.276918658466
At time: 739.6737098693848 and batch: 150, loss is 6.119862947463989 and perplexity is 454.8023584113592
At time: 741.2422103881836 and batch: 200, loss is 6.16791561126709 and perplexity is 477.19041841807103
At time: 742.8127334117889 and batch: 250, loss is 6.200777339935303 and perplexity is 493.13222351318376
At time: 744.3822481632233 and batch: 300, loss is 6.152480812072754 and perplexity is 469.88163007155555
At time: 745.9514317512512 and batch: 350, loss is 6.13887188911438 and perplexity is 463.53036214868797
At time: 747.5263645648956 and batch: 400, loss is 6.165434942245484 and perplexity is 476.0081339641908
At time: 749.0941362380981 and batch: 450, loss is 6.182708406448365 and perplexity is 484.3018679676778
At time: 750.662638425827 and batch: 500, loss is 6.1819492435455325 and perplexity is 483.9343434788746
At time: 752.2441780567169 and batch: 550, loss is 6.138344898223877 and perplexity is 463.2861502247452
At time: 753.8281452655792 and batch: 600, loss is 6.154363327026367 and perplexity is 470.76702238725477
At time: 755.4008941650391 and batch: 650, loss is 6.180671472549438 and perplexity is 483.31638110204307
At time: 756.98583984375 and batch: 700, loss is 6.153920345306396 and perplexity is 470.55852738511896
At time: 758.5677402019501 and batch: 750, loss is 6.141341199874878 and perplexity is 464.67637701085624
At time: 760.170681476593 and batch: 800, loss is 6.1740719604492185 and perplexity is 480.1372307559158
At time: 761.7395405769348 and batch: 850, loss is 6.179129428863526 and perplexity is 482.57166047171205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.759435653686523 and perplexity of 317.16928508445335
Finished 26 epochs...
Completing Train Step...
At time: 765.8604989051819 and batch: 50, loss is 6.175691909790039 and perplexity is 480.91565908337884
At time: 767.4574029445648 and batch: 100, loss is 6.134561214447022 and perplexity is 461.5365340186556
At time: 769.0308439731598 and batch: 150, loss is 6.118180179595948 and perplexity is 454.0376751889395
At time: 770.5988402366638 and batch: 200, loss is 6.1662362194061275 and perplexity is 476.3897012603815
At time: 772.1790456771851 and batch: 250, loss is 6.19914291381836 and perplexity is 492.3268936333384
At time: 773.7502188682556 and batch: 300, loss is 6.150906448364258 and perplexity is 469.14244750966117
At time: 775.324925661087 and batch: 350, loss is 6.137097434997559 and perplexity is 462.70857811396155
At time: 776.8954081535339 and batch: 400, loss is 6.1634753227233885 and perplexity is 475.0762524969666
At time: 778.4656252861023 and batch: 450, loss is 6.181104869842529 and perplexity is 483.52589451131195
At time: 780.0345146656036 and batch: 500, loss is 6.180468997955322 and perplexity is 483.2185317202911
At time: 781.6040890216827 and batch: 550, loss is 6.137165126800537 and perplexity is 462.73990075199845
At time: 783.1782903671265 and batch: 600, loss is 6.153108444213867 and perplexity is 470.1766354528527
At time: 784.7544906139374 and batch: 650, loss is 6.1794391536712645 and perplexity is 482.72114803528024
At time: 786.3297710418701 and batch: 700, loss is 6.152871952056885 and perplexity is 470.06545551328196
At time: 787.8965065479279 and batch: 750, loss is 6.140178308486939 and perplexity is 464.1363229268335
At time: 789.4640777111053 and batch: 800, loss is 6.172805252075196 and perplexity is 479.52942194456443
At time: 791.0348637104034 and batch: 850, loss is 6.177476892471313 and perplexity is 481.7748517999282
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.7581837972005205 and perplexity of 316.772483079162
Finished 27 epochs...
Completing Train Step...
At time: 795.1178126335144 and batch: 50, loss is 6.174155120849609 and perplexity is 480.1771608205451
At time: 796.7158341407776 and batch: 100, loss is 6.133072862625122 and perplexity is 460.8501162196786
At time: 798.2861361503601 and batch: 150, loss is 6.116732931137085 and perplexity is 453.3810451312869
At time: 799.8864834308624 and batch: 200, loss is 6.164579105377197 and perplexity is 475.6009229315029
At time: 801.4584050178528 and batch: 250, loss is 6.1971344566345214 and perplexity is 491.33906848135075
At time: 803.0305564403534 and batch: 300, loss is 6.149202632904053 and perplexity is 468.3437959252204
At time: 804.6021258831024 and batch: 350, loss is 6.13528525352478 and perplexity is 461.8708255105463
At time: 806.1732859611511 and batch: 400, loss is 6.161886348724365 and perplexity is 474.321968112037
At time: 807.7446954250336 and batch: 450, loss is 6.179540920257568 and perplexity is 482.77027541837305
At time: 809.3164050579071 and batch: 500, loss is 6.1788068294525145 and perplexity is 482.4160082462812
At time: 810.8935875892639 and batch: 550, loss is 6.13569525718689 and perplexity is 462.0602330666695
At time: 812.4697797298431 and batch: 600, loss is 6.151573324203492 and perplexity is 469.4554116155662
At time: 814.0563454627991 and batch: 650, loss is 6.178190040588379 and perplexity is 482.1185511680509
At time: 815.6357049942017 and batch: 700, loss is 6.1516681480407716 and perplexity is 469.49992928976246
At time: 817.2060241699219 and batch: 750, loss is 6.1390092658996585 and perplexity is 463.5940448338799
At time: 818.7768256664276 and batch: 800, loss is 6.171581163406372 and perplexity is 478.94279452803096
At time: 820.3469080924988 and batch: 850, loss is 6.1758403396606445 and perplexity is 480.98704663031964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.7578786214192705 and perplexity of 316.6758265385276
Finished 28 epochs...
Completing Train Step...
At time: 824.4567034244537 and batch: 50, loss is 6.17285120010376 and perplexity is 479.5514558823454
At time: 826.0287730693817 and batch: 100, loss is 6.131818275451661 and perplexity is 460.2723021098561
At time: 827.599954366684 and batch: 150, loss is 6.11574761390686 and perplexity is 452.93454098589984
At time: 829.1686050891876 and batch: 200, loss is 6.163296031951904 and perplexity is 474.9910833443922
At time: 830.7471783161163 and batch: 250, loss is 6.1955658721923825 and perplexity is 490.56896580615387
At time: 832.3179273605347 and batch: 300, loss is 6.1476000308990475 and perplexity is 467.59382832913946
At time: 833.8897113800049 and batch: 350, loss is 6.133728294372559 and perplexity is 461.15227102676874
At time: 835.4588868618011 and batch: 400, loss is 6.161012296676636 and perplexity is 473.9075671549107
At time: 837.0275757312775 and batch: 450, loss is 6.178584966659546 and perplexity is 482.3089899554464
At time: 838.5985145568848 and batch: 500, loss is 6.177645092010498 and perplexity is 481.85589292334043
At time: 840.2090630531311 and batch: 550, loss is 6.134618463516236 and perplexity is 461.56295731198367
At time: 841.7866406440735 and batch: 600, loss is 6.150777101516724 and perplexity is 469.0817693373811
At time: 843.3579094409943 and batch: 650, loss is 6.177003545761108 and perplexity is 481.54685922279435
At time: 844.9266533851624 and batch: 700, loss is 6.150526170730591 and perplexity is 468.9640770471677
At time: 846.4930381774902 and batch: 750, loss is 6.138014078140259 and perplexity is 463.1329112104582
At time: 848.0596735477448 and batch: 800, loss is 6.170136232376098 and perplexity is 478.25125495630226
At time: 849.6278977394104 and batch: 850, loss is 6.174232664108277 and perplexity is 480.21439676601244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.757069269816081 and perplexity of 316.419628141904
Finished 29 epochs...
Completing Train Step...
At time: 853.7250092029572 and batch: 50, loss is 6.171167602539063 and perplexity is 478.7447634823025
At time: 855.2912802696228 and batch: 100, loss is 6.130123863220215 and perplexity is 459.49307144678176
At time: 856.857284784317 and batch: 150, loss is 6.114259033203125 and perplexity is 452.2608129418241
At time: 858.4228630065918 and batch: 200, loss is 6.161650381088257 and perplexity is 474.2100566827244
At time: 859.9900698661804 and batch: 250, loss is 6.193567581176758 and perplexity is 489.58964505904703
At time: 861.557765007019 and batch: 300, loss is 6.14616473197937 and perplexity is 466.92317282318623
At time: 863.1249849796295 and batch: 350, loss is 6.132566223144531 and perplexity is 460.6166904925045
At time: 864.6923010349274 and batch: 400, loss is 6.159754695892334 and perplexity is 473.3119552262843
At time: 866.2591433525085 and batch: 450, loss is 6.1772549438476565 and perplexity is 481.6679344001825
At time: 867.8260610103607 and batch: 500, loss is 6.176216487884521 and perplexity is 481.1680030847083
At time: 869.3952541351318 and batch: 550, loss is 6.133187427520752 and perplexity is 460.9029164896153
At time: 870.9639141559601 and batch: 600, loss is 6.149354400634766 and perplexity is 468.4148807943799
At time: 872.5321533679962 and batch: 650, loss is 6.176376113891601 and perplexity is 481.24481614229313
At time: 874.0994243621826 and batch: 700, loss is 6.150118856430054 and perplexity is 468.77310016859553
At time: 875.6634299755096 and batch: 750, loss is 6.137342414855957 and perplexity is 462.8219462817995
At time: 877.2292442321777 and batch: 800, loss is 6.169496994018555 and perplexity is 477.9456361016454
At time: 878.7953898906708 and batch: 850, loss is 6.173741970062256 and perplexity is 479.9788162244174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.756584803263347 and perplexity of 316.26637054252865
Finished 30 epochs...
Completing Train Step...
At time: 882.8938672542572 and batch: 50, loss is 6.170461168289185 and perplexity is 478.40668121493536
At time: 884.5034432411194 and batch: 100, loss is 6.129724187850952 and perplexity is 459.30946007869255
At time: 886.1119096279144 and batch: 150, loss is 6.113506269454956 and perplexity is 451.9204955025098
At time: 887.7203290462494 and batch: 200, loss is 6.160577421188354 and perplexity is 473.70152117566414
At time: 889.3277931213379 and batch: 250, loss is 6.192460222244263 and perplexity is 489.04779365969733
At time: 890.9080402851105 and batch: 300, loss is 6.145017414093018 and perplexity is 466.3877707123937
At time: 892.4800429344177 and batch: 350, loss is 6.131004438400269 and perplexity is 459.8978678415768
At time: 894.0549895763397 and batch: 400, loss is 6.158293752670288 and perplexity is 472.62097819538724
At time: 895.6304543018341 and batch: 450, loss is 6.176342306137085 and perplexity is 481.2285466107062
At time: 897.2025825977325 and batch: 500, loss is 6.175468950271607 and perplexity is 480.8084463121209
At time: 898.7763319015503 and batch: 550, loss is 6.132257041931152 and perplexity is 460.47429847883745
At time: 900.3507387638092 and batch: 600, loss is 6.148429307937622 and perplexity is 467.98175398103854
At time: 901.9240913391113 and batch: 650, loss is 6.175432405471802 and perplexity is 480.7908755847672
At time: 903.4968543052673 and batch: 700, loss is 6.149015960693359 and perplexity is 468.2563773130371
At time: 905.070641040802 and batch: 750, loss is 6.135817461013794 and perplexity is 462.1167020457028
At time: 906.6422877311707 and batch: 800, loss is 6.167939195632934 and perplexity is 477.2016727841894
At time: 908.2151701450348 and batch: 850, loss is 6.172404899597168 and perplexity is 479.3374795770666
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.75547726949056 and perplexity of 315.9162887554423
Finished 31 epochs...
Completing Train Step...
At time: 912.3081736564636 and batch: 50, loss is 6.16888385772705 and perplexity is 477.652680106939
At time: 913.9032161235809 and batch: 100, loss is 6.128341016769409 and perplexity is 458.67459568028727
At time: 915.4767973423004 and batch: 150, loss is 6.1121496486663816 and perplexity is 451.30782643728736
At time: 917.0468685626984 and batch: 200, loss is 6.159049167633056 and perplexity is 472.97813803899453
At time: 918.6190958023071 and batch: 250, loss is 6.190764169692994 and perplexity is 488.21904590010854
At time: 920.2175776958466 and batch: 300, loss is 6.143713283538818 and perplexity is 465.779936604167
At time: 921.7904813289642 and batch: 350, loss is 6.130283584594727 and perplexity is 459.56646817308905
At time: 923.3619604110718 and batch: 400, loss is 6.1574430084228515 and perplexity is 472.2190696019221
At time: 924.9428613185883 and batch: 450, loss is 6.175707769393921 and perplexity is 480.92328627571436
At time: 926.5138144493103 and batch: 500, loss is 6.174691257476806 and perplexity is 480.43467040799703
At time: 928.0860047340393 and batch: 550, loss is 6.131566247940063 and perplexity is 460.1563154434277
At time: 929.6619699001312 and batch: 600, loss is 6.147997446060181 and perplexity is 467.7796941362684
At time: 931.2345857620239 and batch: 650, loss is 6.174322156906128 and perplexity is 480.2573744190141
At time: 932.8189933300018 and batch: 700, loss is 6.1480402088165285 and perplexity is 467.7996981130626
At time: 934.4140725135803 and batch: 750, loss is 6.135110969543457 and perplexity is 461.79033583845325
At time: 936.0054817199707 and batch: 800, loss is 6.166788940429687 and perplexity is 476.6530846457374
At time: 937.593953371048 and batch: 850, loss is 6.170724143981934 and perplexity is 478.53250708719145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.755250930786133 and perplexity of 315.8447927633984
Finished 32 epochs...
Completing Train Step...
At time: 941.6606111526489 and batch: 50, loss is 6.16766978263855 and perplexity is 477.0731257694948
At time: 943.2623608112335 and batch: 100, loss is 6.127292957305908 and perplexity is 458.1941292522904
At time: 944.8289155960083 and batch: 150, loss is 6.110922899246216 and perplexity is 450.75452427383254
At time: 946.4025418758392 and batch: 200, loss is 6.15726526260376 and perplexity is 472.13514209570764
At time: 947.9746420383453 and batch: 250, loss is 6.189228067398071 and perplexity is 487.46966721179143
At time: 949.5443985462189 and batch: 300, loss is 6.142179393768311 and perplexity is 465.06602919171536
At time: 951.1130657196045 and batch: 350, loss is 6.129050369262695 and perplexity is 459.0000730737928
At time: 952.6861052513123 and batch: 400, loss is 6.155940961837769 and perplexity is 471.5103069915007
At time: 954.2584846019745 and batch: 450, loss is 6.174569225311279 and perplexity is 480.37604550190764
At time: 955.8276407718658 and batch: 500, loss is 6.173572807312012 and perplexity is 479.89762855496434
At time: 957.397275686264 and batch: 550, loss is 6.1304605674743655 and perplexity is 459.64781076792275
At time: 959.0095815658569 and batch: 600, loss is 6.147022428512574 and perplexity is 467.32382300336303
At time: 960.5859572887421 and batch: 650, loss is 6.1735398387908935 and perplexity is 479.88180730066574
At time: 962.1554207801819 and batch: 700, loss is 6.147211446762085 and perplexity is 467.41216408311703
At time: 963.7218337059021 and batch: 750, loss is 6.134198312759399 and perplexity is 461.36907201951976
At time: 965.2947874069214 and batch: 800, loss is 6.1661879920959475 and perplexity is 476.3667268204944
At time: 966.872421503067 and batch: 850, loss is 6.170272846221923 and perplexity is 478.31659516259936
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.754229227701823 and perplexity of 315.5222579599279
Finished 33 epochs...
Completing Train Step...
At time: 970.9483439922333 and batch: 50, loss is 6.166767683029175 and perplexity is 476.642952347905
At time: 972.5435883998871 and batch: 100, loss is 6.126459817886353 and perplexity is 457.8125486383991
At time: 974.1117405891418 and batch: 150, loss is 6.110025987625122 and perplexity is 450.3504185533996
At time: 975.6810603141785 and batch: 200, loss is 6.156312160491943 and perplexity is 471.6853634712401
At time: 977.2514505386353 and batch: 250, loss is 6.188241424560547 and perplexity is 486.98894594522113
At time: 978.8234932422638 and batch: 300, loss is 6.141255350112915 and perplexity is 464.6364863668255
At time: 980.3941404819489 and batch: 350, loss is 6.127951440811157 and perplexity is 458.4959418870202
At time: 981.9647588729858 and batch: 400, loss is 6.1547943878173825 and perplexity is 470.9699953360061
At time: 983.5317349433899 and batch: 450, loss is 6.173677883148193 and perplexity is 479.9480568489174
At time: 985.0999157428741 and batch: 500, loss is 6.172822847366333 and perplexity is 479.5378594785825
At time: 986.6704323291779 and batch: 550, loss is 6.129662818908692 and perplexity is 459.2812736078531
At time: 988.2408137321472 and batch: 600, loss is 6.145952358245849 and perplexity is 466.8240211346862
At time: 989.8104124069214 and batch: 650, loss is 6.1723610782623295 and perplexity is 479.31647482910495
At time: 991.3839588165283 and batch: 700, loss is 6.145995483398438 and perplexity is 466.84415342593053
At time: 992.9513564109802 and batch: 750, loss is 6.133073968887329 and perplexity is 460.8506260410274
At time: 994.5237419605255 and batch: 800, loss is 6.165697183609009 and perplexity is 476.1329793553878
At time: 996.1008777618408 and batch: 850, loss is 6.169524946212769 and perplexity is 477.9589959176064
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.752933502197266 and perplexity of 315.1136924745553
Finished 34 epochs...
Completing Train Step...
At time: 1000.2005038261414 and batch: 50, loss is 6.165939493179321 and perplexity is 476.2483649119748
At time: 1001.7715227603912 and batch: 100, loss is 6.125936298370362 and perplexity is 457.5729375605315
At time: 1003.3507168292999 and batch: 150, loss is 6.109087457656861 and perplexity is 449.92794947032615
At time: 1004.9265375137329 and batch: 200, loss is 6.155341348648071 and perplexity is 471.2276679378708
At time: 1006.5192148685455 and batch: 250, loss is 6.187155666351319 and perplexity is 486.4604806440431
At time: 1008.1265635490417 and batch: 300, loss is 6.140246772766114 and perplexity is 464.1681007734328
At time: 1009.7214019298553 and batch: 350, loss is 6.126953468322754 and perplexity is 458.038603794347
At time: 1011.3017239570618 and batch: 400, loss is 6.153932762145996 and perplexity is 470.564370271151
At time: 1012.8731162548065 and batch: 450, loss is 6.173042335510254 and perplexity is 479.64312390502363
At time: 1014.4436492919922 and batch: 500, loss is 6.172387771606445 and perplexity is 479.3292695594741
At time: 1016.0240683555603 and batch: 550, loss is 6.129197835922241 and perplexity is 459.06776527234877
At time: 1017.6100287437439 and batch: 600, loss is 6.145305643081665 and perplexity is 466.52221656252846
At time: 1019.1833460330963 and batch: 650, loss is 6.17148455619812 and perplexity is 478.89652743664254
At time: 1020.7550919055939 and batch: 700, loss is 6.145127458572388 and perplexity is 466.43909693583817
At time: 1022.3251740932465 and batch: 750, loss is 6.132131652832031 and perplexity is 460.41656364111833
At time: 1023.8958926200867 and batch: 800, loss is 6.164663581848145 and perplexity is 475.64110171610855
At time: 1025.4711532592773 and batch: 850, loss is 6.168797855377197 and perplexity is 477.6116026204416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.753255844116211 and perplexity of 315.2152831994685
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 1029.883888244629 and batch: 50, loss is 6.166888217926026 and perplexity is 476.7004079196322
At time: 1031.4924924373627 and batch: 100, loss is 6.125321731567383 and perplexity is 457.2918148163667
At time: 1033.0964014530182 and batch: 150, loss is 6.108177995681762 and perplexity is 449.5189431247999
At time: 1034.7097985744476 and batch: 200, loss is 6.154890432357788 and perplexity is 471.0152316050664
At time: 1036.3273622989655 and batch: 250, loss is 6.1833422565460205 and perplexity is 484.6089400625418
At time: 1037.930284023285 and batch: 300, loss is 6.133661251068116 and perplexity is 461.1213548910392
At time: 1039.6771321296692 and batch: 350, loss is 6.117894287109375 and perplexity is 453.90788778248833
At time: 1041.2770981788635 and batch: 400, loss is 6.142167358398438 and perplexity is 465.0604319837209
At time: 1042.8764774799347 and batch: 450, loss is 6.1597779178619385 and perplexity is 473.3229465897419
At time: 1044.4713487625122 and batch: 500, loss is 6.157541532516479 and perplexity is 472.26559684973813
At time: 1046.0665369033813 and batch: 550, loss is 6.112736597061157 and perplexity is 451.57279859644893
At time: 1047.6461577415466 and batch: 600, loss is 6.121861333847046 and perplexity is 455.71213799418473
At time: 1049.2172656059265 and batch: 650, loss is 6.146165790557862 and perplexity is 466.9236670982758
At time: 1050.789260149002 and batch: 700, loss is 6.1208464050292966 and perplexity is 455.24985724344634
At time: 1052.3605642318726 and batch: 750, loss is 6.1025907421112064 and perplexity is 447.01437016242517
At time: 1053.9413311481476 and batch: 800, loss is 6.135657777786255 and perplexity is 462.0429156506023
At time: 1055.5251257419586 and batch: 850, loss is 6.147989377975464 and perplexity is 467.77592006529215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.745567957560222 and perplexity of 312.8012352149386
Finished 36 epochs...
Completing Train Step...
At time: 1059.790682554245 and batch: 50, loss is 6.160436391830444 and perplexity is 473.6347200648613
At time: 1061.3619997501373 and batch: 100, loss is 6.117932443618774 and perplexity is 453.92520765350565
At time: 1062.9366133213043 and batch: 150, loss is 6.1007606887817385 and perplexity is 446.19705811640955
At time: 1064.507811307907 and batch: 200, loss is 6.148261766433716 and perplexity is 467.9033541819695
At time: 1066.0777938365936 and batch: 250, loss is 6.1780244255065915 and perplexity is 482.0387116762625
At time: 1067.6495370864868 and batch: 300, loss is 6.129205827713013 and perplexity is 459.0714340605388
At time: 1069.2190227508545 and batch: 350, loss is 6.1141143417358395 and perplexity is 452.1953793951558
At time: 1070.7901990413666 and batch: 400, loss is 6.138622636795044 and perplexity is 463.41484052845357
At time: 1072.3646891117096 and batch: 450, loss is 6.156932506561279 and perplexity is 471.9780624103646
At time: 1073.938097000122 and batch: 500, loss is 6.155067844390869 and perplexity is 471.0988027879652
At time: 1075.5135080814362 and batch: 550, loss is 6.111381902694702 and perplexity is 450.96146964558125
At time: 1077.0837066173553 and batch: 600, loss is 6.121756715774536 and perplexity is 455.6644647624727
At time: 1078.6585125923157 and batch: 650, loss is 6.146626024246216 and perplexity is 467.13861055808104
At time: 1080.2848658561707 and batch: 700, loss is 6.121792268753052 and perplexity is 455.68066527938544
At time: 1081.864610671997 and batch: 750, loss is 6.104851636886597 and perplexity is 448.0261659673793
At time: 1083.4409716129303 and batch: 800, loss is 6.138222017288208 and perplexity is 463.22922468672664
At time: 1085.0116174221039 and batch: 850, loss is 6.150129308700562 and perplexity is 468.7779999374521
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.745344161987305 and perplexity of 312.73123951595045
Finished 37 epochs...
Completing Train Step...
At time: 1089.1478197574615 and batch: 50, loss is 6.158112783432006 and perplexity is 472.535456075637
At time: 1090.760619878769 and batch: 100, loss is 6.115167894363403 and perplexity is 452.67204207580943
At time: 1092.33345413208 and batch: 150, loss is 6.098043947219849 and perplexity is 444.9865011536003
At time: 1093.909063577652 and batch: 200, loss is 6.145756397247315 and perplexity is 466.7325507959668
At time: 1095.4871561527252 and batch: 250, loss is 6.175613775253296 and perplexity is 480.8780844291017
At time: 1097.0658991336823 and batch: 300, loss is 6.127249698638916 and perplexity is 458.1743088137415
At time: 1098.6383934020996 and batch: 350, loss is 6.112611093521118 and perplexity is 451.51612816788395
At time: 1100.2116975784302 and batch: 400, loss is 6.13707540512085 and perplexity is 462.69838481331266
At time: 1101.7852764129639 and batch: 450, loss is 6.155611734390259 and perplexity is 471.3550984075204
At time: 1103.3677299022675 and batch: 500, loss is 6.154164533615113 and perplexity is 470.6734463064308
At time: 1104.9438107013702 and batch: 550, loss is 6.111334142684936 and perplexity is 450.9399322357046
At time: 1106.5205006599426 and batch: 600, loss is 6.122361898422241 and perplexity is 455.94030844921025
At time: 1108.0961410999298 and batch: 650, loss is 6.147626371383667 and perplexity is 467.60614513939726
At time: 1109.6753890514374 and batch: 700, loss is 6.122951393127441 and perplexity is 456.20916208303953
At time: 1111.25563454628 and batch: 750, loss is 6.10654013633728 and perplexity is 448.7832969302308
At time: 1112.8286457061768 and batch: 800, loss is 6.1398640060424805 and perplexity is 463.99046666866565
At time: 1114.4027400016785 and batch: 850, loss is 6.1512935352325435 and perplexity is 469.3240815422541
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.745320638020833 and perplexity of 312.7238829232859
Finished 38 epochs...
Completing Train Step...
At time: 1118.4821259975433 and batch: 50, loss is 6.156627626419067 and perplexity is 471.83418760497585
At time: 1120.082051038742 and batch: 100, loss is 6.113367872238159 and perplexity is 451.85795529151414
At time: 1121.6539134979248 and batch: 150, loss is 6.096269483566284 and perplexity is 444.1975889360097
At time: 1123.2313091754913 and batch: 200, loss is 6.1441566753387455 and perplexity is 465.986505400699
At time: 1124.8004839420319 and batch: 250, loss is 6.1740459537506105 and perplexity is 480.12474413403373
At time: 1126.3714497089386 and batch: 300, loss is 6.126040906906128 and perplexity is 457.6208060992211
At time: 1127.9414286613464 and batch: 350, loss is 6.1117103385925295 and perplexity is 451.10960590605123
At time: 1129.512701034546 and batch: 400, loss is 6.136185283660889 and perplexity is 462.28671029884225
At time: 1131.1121983528137 and batch: 450, loss is 6.154854803085327 and perplexity is 470.998449974007
At time: 1132.7219016551971 and batch: 500, loss is 6.153759288787842 and perplexity is 470.48274696955093
At time: 1134.3334612846375 and batch: 550, loss is 6.11150239944458 and perplexity is 451.0158123109855
At time: 1135.9397203922272 and batch: 600, loss is 6.122958583831787 and perplexity is 456.2124425600383
At time: 1137.5189609527588 and batch: 650, loss is 6.148485298156738 and perplexity is 468.0079571155391
At time: 1139.0905184745789 and batch: 700, loss is 6.12385365486145 and perplexity is 456.62096790307106
At time: 1140.6644172668457 and batch: 750, loss is 6.107728824615479 and perplexity is 449.31707756128105
At time: 1142.245087146759 and batch: 800, loss is 6.140997400283814 and perplexity is 464.51664892126183
At time: 1143.8187947273254 and batch: 850, loss is 6.152049074172973 and perplexity is 469.67880814954896
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.745314280192058 and perplexity of 312.72189468470475
Finished 39 epochs...
Completing Train Step...
At time: 1147.9346194267273 and batch: 50, loss is 6.155547704696655 and perplexity is 471.32491865120124
At time: 1149.5085470676422 and batch: 100, loss is 6.112028493881225 and perplexity is 451.25315164665915
At time: 1151.0815215110779 and batch: 150, loss is 6.094947214126587 and perplexity is 443.6106281846243
At time: 1152.6579563617706 and batch: 200, loss is 6.142988662719727 and perplexity is 465.4425450202346
At time: 1154.2413728237152 and batch: 250, loss is 6.172897052764893 and perplexity is 479.57344509687596
At time: 1155.81418466568 and batch: 300, loss is 6.125182132720948 and perplexity is 457.227981862141
At time: 1157.3834774494171 and batch: 350, loss is 6.111076364517212 and perplexity is 450.8237047473172
At time: 1158.9568939208984 and batch: 400, loss is 6.135602588653565 and perplexity is 462.0174166064633
At time: 1160.5717225074768 and batch: 450, loss is 6.1543731689453125 and perplexity is 470.7716556609315
At time: 1162.1422867774963 and batch: 500, loss is 6.153553676605225 and perplexity is 470.3860199295326
At time: 1163.720377922058 and batch: 550, loss is 6.11170337677002 and perplexity is 451.1064653719745
At time: 1165.293285369873 and batch: 600, loss is 6.123468999862671 and perplexity is 456.4453601415866
At time: 1166.8647179603577 and batch: 650, loss is 6.149176998138428 and perplexity is 468.3317901956627
At time: 1168.4359147548676 and batch: 700, loss is 6.124536151885986 and perplexity is 456.932716726727
At time: 1170.0064678192139 and batch: 750, loss is 6.108595447540283 and perplexity is 449.70663481644476
At time: 1171.5768830776215 and batch: 800, loss is 6.141825370788574 and perplexity is 464.9014142707382
At time: 1173.1470601558685 and batch: 850, loss is 6.152582712173462 and perplexity is 469.92951349660245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.745300928751628 and perplexity of 312.71771942482974
Finished 40 epochs...
Completing Train Step...
At time: 1177.2572951316833 and batch: 50, loss is 6.154704751968384 and perplexity is 470.9277814325892
At time: 1178.8308637142181 and batch: 100, loss is 6.1109633636474605 and perplexity is 450.7727641547961
At time: 1180.408653974533 and batch: 150, loss is 6.093899793624878 and perplexity is 443.1462245732773
At time: 1181.9923739433289 and batch: 200, loss is 6.142085342407227 and perplexity is 465.02229115551904
At time: 1183.5672414302826 and batch: 250, loss is 6.171999406814575 and perplexity is 479.1431510907341
At time: 1185.1381330490112 and batch: 300, loss is 6.124522571563721 and perplexity is 456.92651147531484
At time: 1186.706754207611 and batch: 350, loss is 6.110591688156128 and perplexity is 450.6052540978193
At time: 1188.2760469913483 and batch: 400, loss is 6.135186386108399 and perplexity is 461.8251637925912
At time: 1189.847038269043 and batch: 450, loss is 6.154038867950439 and perplexity is 470.61430253120534
At time: 1191.425387620926 and batch: 500, loss is 6.153435306549072 and perplexity is 470.3303436052101
At time: 1193.007131099701 and batch: 550, loss is 6.111890697479248 and perplexity is 451.190974869948
At time: 1194.5789365768433 and batch: 600, loss is 6.1238923835754395 and perplexity is 456.63865258838894
At time: 1196.1525256633759 and batch: 650, loss is 6.149732608795166 and perplexity is 468.59207263033596
At time: 1197.7258622646332 and batch: 700, loss is 6.125056638717651 and perplexity is 457.1706060925107
At time: 1199.2980499267578 and batch: 750, loss is 6.109247779846191 and perplexity is 450.0000886868303
At time: 1200.9117093086243 and batch: 800, loss is 6.142454528808594 and perplexity is 465.19400275659007
At time: 1202.4854533672333 and batch: 850, loss is 6.152979583740234 and perplexity is 470.11605217239605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.745280583699544 and perplexity of 312.71135723126025
Finished 41 epochs...
Completing Train Step...
At time: 1206.612666130066 and batch: 50, loss is 6.154018964767456 and perplexity is 470.6049359018406
At time: 1208.1855506896973 and batch: 100, loss is 6.1100843334198 and perplexity is 450.37669537301696
At time: 1209.7602453231812 and batch: 150, loss is 6.093040571212769 and perplexity is 442.76562693770666
At time: 1211.3401882648468 and batch: 200, loss is 6.141360330581665 and perplexity is 464.68526668340814
At time: 1212.9116566181183 and batch: 250, loss is 6.17126766204834 and perplexity is 478.7926688450592
At time: 1214.4992849826813 and batch: 300, loss is 6.12398998260498 and perplexity is 456.683222252675
At time: 1216.1110620498657 and batch: 350, loss is 6.110201978683472 and perplexity is 450.42968317491676
At time: 1217.7213134765625 and batch: 400, loss is 6.134868383407593 and perplexity is 461.67832549193054
At time: 1219.3276796340942 and batch: 450, loss is 6.153789234161377 and perplexity is 470.49683596209996
At time: 1220.9125871658325 and batch: 500, loss is 6.153356580734253 and perplexity is 470.2933179231332
At time: 1222.4902675151825 and batch: 550, loss is 6.1120531940460205 and perplexity is 451.26429781152467
At time: 1224.0666766166687 and batch: 600, loss is 6.12424111366272 and perplexity is 456.7979239953133
At time: 1225.645408153534 and batch: 650, loss is 6.15018141746521 and perplexity is 468.802428016376
At time: 1227.2224297523499 and batch: 700, loss is 6.12545729637146 and perplexity is 457.35381169385045
At time: 1228.7967238426208 and batch: 750, loss is 6.1097509384155275 and perplexity is 450.22656706014425
At time: 1230.3763461112976 and batch: 800, loss is 6.142946224212647 and perplexity is 465.4227927526237
At time: 1231.9610273838043 and batch: 850, loss is 6.153285779953003 and perplexity is 470.26002196751125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.74525515238444 and perplexity of 312.70340467132047
Finished 42 epochs...
Completing Train Step...
At time: 1236.2636210918427 and batch: 50, loss is 6.153444499969482 and perplexity is 470.33466756966635
At time: 1237.9084720611572 and batch: 100, loss is 6.1093402862548825 and perplexity is 450.0417185044283
At time: 1239.5137634277344 and batch: 150, loss is 6.092318754196167 and perplexity is 442.44614649089965
At time: 1241.1461567878723 and batch: 200, loss is 6.1407620716094975 and perplexity is 464.4073476954341
At time: 1242.7173874378204 and batch: 250, loss is 6.170653467178345 and perplexity is 478.4986871343342
At time: 1244.2885687351227 and batch: 300, loss is 6.123545551300049 and perplexity is 456.4803030274372
At time: 1245.8577728271484 and batch: 350, loss is 6.109876937866211 and perplexity is 450.28329893429446
At time: 1247.4383957386017 and batch: 400, loss is 6.134612159729004 and perplexity is 461.5600477264774
At time: 1249.0118401050568 and batch: 450, loss is 6.153590545654297 and perplexity is 470.40336293449246
At time: 1250.584451675415 and batch: 500, loss is 6.153296403884887 and perplexity is 470.2650180044913
At time: 1252.1562857627869 and batch: 550, loss is 6.112190008163452 and perplexity is 451.3260413617565
At time: 1253.7333080768585 and batch: 600, loss is 6.12452877998352 and perplexity is 456.9293482757216
At time: 1255.3063645362854 and batch: 650, loss is 6.150546579360962 and perplexity is 468.97364805933887
At time: 1256.8791890144348 and batch: 700, loss is 6.125768404006958 and perplexity is 457.4961200922597
At time: 1258.449452638626 and batch: 750, loss is 6.110146379470825 and perplexity is 450.4046403353669
At time: 1260.019071340561 and batch: 800, loss is 6.1433381175994874 and perplexity is 465.60522461177305
At time: 1261.5893514156342 and batch: 850, loss is 6.153528299331665 and perplexity is 470.3740829662906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.745222727457683 and perplexity of 312.69326545070976
Finished 43 epochs...
Completing Train Step...
At time: 1265.669954776764 and batch: 50, loss is 6.152952375411988 and perplexity is 470.1032612745449
At time: 1267.265388250351 and batch: 100, loss is 6.108698816299438 and perplexity is 449.7531228359325
At time: 1268.834888935089 and batch: 150, loss is 6.091700572967529 and perplexity is 442.1727191109998
At time: 1270.407187461853 and batch: 200, loss is 6.140257501602173 and perplexity is 464.17308078360486
At time: 1271.9765343666077 and batch: 250, loss is 6.170126876831055 and perplexity is 478.2467806760741
At time: 1273.547461271286 and batch: 300, loss is 6.123165836334229 and perplexity is 456.30700352906416
At time: 1275.1166229248047 and batch: 350, loss is 6.10959789276123 and perplexity is 450.15766711315973
At time: 1276.6882600784302 and batch: 400, loss is 6.134396028518677 and perplexity is 461.46030097430514
At time: 1278.2586278915405 and batch: 450, loss is 6.153423023223877 and perplexity is 470.324566420132
At time: 1279.8562560081482 and batch: 500, loss is 6.153244457244873 and perplexity is 470.24058995137284
At time: 1281.4274010658264 and batch: 550, loss is 6.112302570343018 and perplexity is 451.3768464639798
At time: 1282.9974162578583 and batch: 600, loss is 6.12476692199707 and perplexity is 457.0381753084
At time: 1284.5704100131989 and batch: 650, loss is 6.15084587097168 and perplexity is 469.1140289442135
At time: 1286.1543111801147 and batch: 700, loss is 6.126011476516724 and perplexity is 457.60733833888
At time: 1287.7280831336975 and batch: 750, loss is 6.1104610157012935 and perplexity is 450.5463762500453
At time: 1289.2980012893677 and batch: 800, loss is 6.143655366897583 and perplexity is 465.75296097586073
At time: 1290.8685548305511 and batch: 850, loss is 6.153723831176758 and perplexity is 470.46606507103917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.745185852050781 and perplexity of 312.6817349719078
Finished 44 epochs...
Completing Train Step...
At time: 1294.945202589035 and batch: 50, loss is 6.152523164749145 and perplexity is 469.9015312376073
At time: 1296.5456573963165 and batch: 100, loss is 6.10813717842102 and perplexity is 449.500595367345
At time: 1298.119929075241 and batch: 150, loss is 6.091163330078125 and perplexity is 441.93522876248164
At time: 1299.6950054168701 and batch: 200, loss is 6.139823741912842 and perplexity is 463.9717848724703
At time: 1301.2686035633087 and batch: 250, loss is 6.169667406082153 and perplexity is 478.02709074401093
At time: 1302.8489317893982 and batch: 300, loss is 6.122835311889649 and perplexity is 456.1562078323695
At time: 1304.4238710403442 and batch: 350, loss is 6.109352989196777 and perplexity is 450.04743539453926
At time: 1305.9965353012085 and batch: 400, loss is 6.134207496643066 and perplexity is 461.37330919886153
At time: 1307.5708589553833 and batch: 450, loss is 6.1532752990722654 and perplexity is 470.25509325413407
At time: 1309.144169807434 and batch: 500, loss is 6.153196229934692 and perplexity is 470.2179120594328
At time: 1310.719447851181 and batch: 550, loss is 6.112394104003906 and perplexity is 451.4181645301449
At time: 1312.2927827835083 and batch: 600, loss is 6.124964618682862 and perplexity is 457.12853917296275
At time: 1313.8670806884766 and batch: 650, loss is 6.1510919666290285 and perplexity is 469.2294900761967
At time: 1315.4407668113708 and batch: 700, loss is 6.126201486587524 and perplexity is 457.69429660284777
At time: 1317.0141532421112 and batch: 750, loss is 6.1107134342193605 and perplexity is 450.6601168531721
At time: 1318.587234735489 and batch: 800, loss is 6.143915405273438 and perplexity is 465.8740903678449
At time: 1320.1882905960083 and batch: 850, loss is 6.15388334274292 and perplexity is 470.541115835478
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.745151519775391 and perplexity of 312.6710000807508
Finished 45 epochs...
Completing Train Step...
At time: 1324.270545244217 and batch: 50, loss is 6.152143459320069 and perplexity is 469.7231409450895
At time: 1325.846750497818 and batch: 100, loss is 6.107638731002807 and perplexity is 449.2765987859954
At time: 1327.419357061386 and batch: 150, loss is 6.090690050125122 and perplexity is 441.72611916577614
At time: 1328.992914199829 and batch: 200, loss is 6.139444999694824 and perplexity is 463.79609244274036
At time: 1330.569569349289 and batch: 250, loss is 6.169260940551758 and perplexity is 477.8328286921168
At time: 1332.1449899673462 and batch: 300, loss is 6.122542934417725 and perplexity is 456.02285752878333
At time: 1333.7217078208923 and batch: 350, loss is 6.109134359359741 and perplexity is 449.94905235220654
At time: 1335.2932398319244 and batch: 400, loss is 6.1340380382537845 and perplexity is 461.2951322450845
At time: 1336.8646891117096 and batch: 450, loss is 6.15314061164856 and perplexity is 470.1917600723265
At time: 1338.446115732193 and batch: 500, loss is 6.153148727416992 and perplexity is 470.1955760552549
At time: 1340.0198016166687 and batch: 550, loss is 6.112467193603516 and perplexity is 451.4511597088346
At time: 1341.5965130329132 and batch: 600, loss is 6.125129842758179 and perplexity is 457.2040740530673
At time: 1343.1695923805237 and batch: 650, loss is 6.1512956714630125 and perplexity is 469.32508412772773
At time: 1344.7421340942383 and batch: 700, loss is 6.1263496208190915 and perplexity is 457.76210181778055
At time: 1346.312693119049 and batch: 750, loss is 6.110916938781738 and perplexity is 450.7518375755095
At time: 1347.8836534023285 and batch: 800, loss is 6.144130239486694 and perplexity is 465.9741868132112
At time: 1349.4566371440887 and batch: 850, loss is 6.154014692306519 and perplexity is 470.6029252649304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.745111465454102 and perplexity of 312.6584765068691
Finished 46 epochs...
Completing Train Step...
At time: 1353.5880236625671 and batch: 50, loss is 6.151802997589112 and perplexity is 469.56324541215224
At time: 1355.158819437027 and batch: 100, loss is 6.107191400527954 and perplexity is 449.0756686161648
At time: 1356.7292873859406 and batch: 150, loss is 6.090268211364746 and perplexity is 441.53982126384216
At time: 1358.298998594284 and batch: 200, loss is 6.139109992980957 and perplexity is 463.64074366080234
At time: 1359.899184703827 and batch: 250, loss is 6.168897485733032 and perplexity is 477.6591896048762
At time: 1361.471229314804 and batch: 300, loss is 6.12228030204773 and perplexity is 455.903106890824
At time: 1363.0417568683624 and batch: 350, loss is 6.1089365100860595 and perplexity is 449.8600390649024
At time: 1364.628891468048 and batch: 400, loss is 6.1338820648193355 and perplexity is 461.2231880698496
At time: 1366.230702161789 and batch: 450, loss is 6.153014707565307 and perplexity is 470.1325647363672
At time: 1367.827757358551 and batch: 500, loss is 6.153100519180298 and perplexity is 470.17290930199835
At time: 1369.4341638088226 and batch: 550, loss is 6.112524490356446 and perplexity is 451.477027135445
At time: 1371.0458536148071 and batch: 600, loss is 6.1252678108215335 and perplexity is 457.2671579654047
At time: 1372.6196887493134 and batch: 650, loss is 6.151464357376098 and perplexity is 469.4042593357613
At time: 1374.1920371055603 and batch: 700, loss is 6.1264639091491695 and perplexity is 457.81442167368783
At time: 1375.7640120983124 and batch: 750, loss is 6.111080951690674 and perplexity is 450.825772758595
At time: 1377.335167169571 and batch: 800, loss is 6.1443091487884525 and perplexity is 466.0575613876324
At time: 1378.9094364643097 and batch: 850, loss is 6.154122972488404 and perplexity is 470.65388499418816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.7450714111328125 and perplexity of 312.6459534346006
Finished 47 epochs...
Completing Train Step...
At time: 1382.9838724136353 and batch: 50, loss is 6.151494569778443 and perplexity is 469.41844138034224
At time: 1384.5545687675476 and batch: 100, loss is 6.106785831451416 and perplexity is 448.89357434034787
At time: 1386.1233160495758 and batch: 150, loss is 6.0898887729644775 and perplexity is 441.3723158814037
At time: 1387.6971340179443 and batch: 200, loss is 6.138809728622436 and perplexity is 463.50154976885096
At time: 1389.273493528366 and batch: 250, loss is 6.168568696975708 and perplexity is 477.50216644864037
At time: 1390.8590848445892 and batch: 300, loss is 6.122041912078857 and perplexity is 455.79443711677
At time: 1392.4293129444122 and batch: 350, loss is 6.108755140304566 and perplexity is 449.77845544653957
At time: 1393.9989266395569 and batch: 400, loss is 6.133736190795898 and perplexity is 461.15591249470185
At time: 1395.5747239589691 and batch: 450, loss is 6.152895154953003 and perplexity is 470.07636251975157
At time: 1397.146633863449 and batch: 500, loss is 6.153050966262818 and perplexity is 470.14961143986545
At time: 1398.7169115543365 and batch: 550, loss is 6.112568473815918 and perplexity is 451.49688509367815
At time: 1400.3151443004608 and batch: 600, loss is 6.125383195877075 and perplexity is 457.31992280590225
At time: 1401.8857915401459 and batch: 650, loss is 6.151604709625244 and perplexity is 469.4701459028733
At time: 1403.5066177845001 and batch: 700, loss is 6.126551008224487 and perplexity is 457.8542986230801
At time: 1405.1206314563751 and batch: 750, loss is 6.111213045120239 and perplexity is 450.88532781438425
At time: 1406.7332594394684 and batch: 800, loss is 6.1444589805603025 and perplexity is 466.12739684949406
At time: 1408.3419423103333 and batch: 850, loss is 6.154212188720703 and perplexity is 470.69587683367354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.7450300852457685 and perplexity of 312.63303333021344
Finished 48 epochs...
Completing Train Step...
At time: 1412.7155239582062 and batch: 50, loss is 6.15121241569519 and perplexity is 469.28601173401415
At time: 1414.3689212799072 and batch: 100, loss is 6.10641508102417 and perplexity is 448.7271777035914
At time: 1415.9783458709717 and batch: 150, loss is 6.089544048309326 and perplexity is 441.220190184264
At time: 1417.5878365039825 and batch: 200, loss is 6.138538475036621 and perplexity is 463.3758403617798
At time: 1419.1969163417816 and batch: 250, loss is 6.168268671035767 and perplexity is 477.3589249014921
At time: 1420.806939125061 and batch: 300, loss is 6.12182279586792 and perplexity is 455.6945761077252
At time: 1422.411761522293 and batch: 350, loss is 6.108587684631348 and perplexity is 449.70314379834133
At time: 1424.0225775241852 and batch: 400, loss is 6.133597774505615 and perplexity is 461.0920854215076
At time: 1425.6328446865082 and batch: 450, loss is 6.152779769897461 and perplexity is 470.02212586166326
At time: 1427.2424626350403 and batch: 500, loss is 6.152999706268311 and perplexity is 470.1255121910346
At time: 1428.8597474098206 and batch: 550, loss is 6.11260142326355 and perplexity is 451.51176191173994
At time: 1430.4711577892303 and batch: 600, loss is 6.125479621887207 and perplexity is 457.3640224675554
At time: 1432.080531835556 and batch: 650, loss is 6.15172122001648 and perplexity is 469.5248472398214
At time: 1433.6899003982544 and batch: 700, loss is 6.1266154956817624 and perplexity is 457.88382543464525
At time: 1435.2974288463593 and batch: 750, loss is 6.111318531036377 and perplexity is 450.9328923749135
At time: 1436.9031603336334 and batch: 800, loss is 6.144584827423095 and perplexity is 466.1860612113349
At time: 1438.5128509998322 and batch: 850, loss is 6.154285945892334 and perplexity is 470.7305953105996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.744988759358724 and perplexity of 312.6201137597497
Finished 49 epochs...
Completing Train Step...
At time: 1442.7853186130524 and batch: 50, loss is 6.150952425003052 and perplexity is 469.16401759836947
At time: 1444.3829543590546 and batch: 100, loss is 6.106073350906372 and perplexity is 448.57386031037305
At time: 1445.9514808654785 and batch: 150, loss is 6.08922848701477 and perplexity is 441.0809801356719
At time: 1447.5214850902557 and batch: 200, loss is 6.138290672302246 and perplexity is 463.26102878739346
At time: 1449.0940907001495 and batch: 250, loss is 6.167992238998413 and perplexity is 477.22698583826474
At time: 1450.678805589676 and batch: 300, loss is 6.1216196441650395 and perplexity is 455.6020103813563
At time: 1452.2497901916504 and batch: 350, loss is 6.108431367874146 and perplexity is 449.63285315514497
At time: 1453.82155585289 and batch: 400, loss is 6.133465051651001 and perplexity is 461.0308920246614
At time: 1455.392614364624 and batch: 450, loss is 6.152667760848999 and perplexity is 469.9694820789344
At time: 1456.963225364685 and batch: 500, loss is 6.15294659614563 and perplexity is 470.10054443043293
At time: 1458.536898612976 and batch: 550, loss is 6.112624559402466 and perplexity is 451.52220827142935
At time: 1460.1085903644562 and batch: 600, loss is 6.125559816360473 and perplexity is 457.4007020051567
At time: 1461.6826696395874 and batch: 650, loss is 6.151817445755005 and perplexity is 469.57002978883
At time: 1463.2544786930084 and batch: 700, loss is 6.126661415100098 and perplexity is 457.90485167632704
At time: 1464.8311154842377 and batch: 750, loss is 6.111402492523194 and perplexity is 450.97075496048893
At time: 1466.4013676643372 and batch: 800, loss is 6.144690408706665 and perplexity is 466.23528433253415
At time: 1467.9748451709747 and batch: 850, loss is 6.154346160888672 and perplexity is 470.75894120508764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.74494743347168 and perplexity of 312.60719472318794
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd1269705c0>
SETTINGS FOR THIS RUN
{'seq_len': 50, 'lr': 22.871942858806772, 'dropout': 0.3408868476037624, 'wordvec_source': 'glove', 'num_layers': 1, 'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 3.393881574369817, 'batch_size': 50, 'data': 'wikitext'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.0560991764068604 and batch: 50, loss is 7.454905433654785 and perplexity is 1728.3205465817337
At time: 3.6256844997406006 and batch: 100, loss is 6.351124305725097 and perplexity is 573.1367278131631
At time: 5.198523998260498 and batch: 150, loss is 6.213117294311523 and perplexity is 499.25515329288373
At time: 6.777231216430664 and batch: 200, loss is 6.296996917724609 and perplexity is 542.9389690304081
At time: 8.372915983200073 and batch: 250, loss is 6.426943035125732 and perplexity is 618.2809928605506
At time: 10.034230947494507 and batch: 300, loss is 6.41756422996521 and perplexity is 612.5093636936997
At time: 11.629119396209717 and batch: 350, loss is 6.443329668045044 and perplexity is 628.4960027619762
At time: 13.225010395050049 and batch: 400, loss is 6.499590606689453 and perplexity is 664.8693842414489
At time: 14.82402229309082 and batch: 450, loss is 6.533683290481568 and perplexity is 687.9273871852943
At time: 16.407333374023438 and batch: 500, loss is 6.604654836654663 and perplexity is 738.524913483786
At time: 17.981123685836792 and batch: 550, loss is 6.625547437667847 and perplexity is 754.1169315895943
At time: 19.555912733078003 and batch: 600, loss is 6.672097511291504 and perplexity is 790.0510086527355
At time: 21.14042592048645 and batch: 650, loss is 6.714490299224853 and perplexity is 824.2635320483603
At time: 22.719807386398315 and batch: 700, loss is 6.675201969146729 and perplexity is 792.5074997762563
At time: 24.296921491622925 and batch: 750, loss is 6.708935194015503 and perplexity is 819.697355944302
At time: 25.86988615989685 and batch: 800, loss is 6.748810939788818 and perplexity is 853.043838755937
At time: 27.446123123168945 and batch: 850, loss is 6.748770980834961 and perplexity is 853.0097526975725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.295907338460286 and perplexity of 542.3477161548899
Finished 1 epochs...
Completing Train Step...
At time: 31.557207107543945 and batch: 50, loss is 6.6946366596221925 and perplexity is 808.0602799381334
At time: 33.124401807785034 and batch: 100, loss is 6.623499002456665 and perplexity is 752.573753003863
At time: 34.69203543663025 and batch: 150, loss is 6.663969268798828 and perplexity is 783.6553105779705
At time: 36.26735734939575 and batch: 200, loss is 6.698670835494995 and perplexity is 811.3267214930908
At time: 37.83549237251282 and batch: 250, loss is 6.793039875030518 and perplexity is 891.6198592905329
At time: 39.408634185791016 and batch: 300, loss is 6.73570216178894 and perplexity is 841.9344507858654
At time: 40.98162245750427 and batch: 350, loss is 6.67363224029541 and perplexity is 791.264453766807
At time: 42.55814981460571 and batch: 400, loss is 6.628629760742188 and perplexity is 756.4449496172732
At time: 44.134915828704834 and batch: 450, loss is 6.642377662658691 and perplexity is 766.9162951783318
At time: 45.712411642074585 and batch: 500, loss is 6.736132564544678 and perplexity is 842.296899687542
At time: 47.28085899353027 and batch: 550, loss is 6.7246182250976565 and perplexity is 832.6540294473798
At time: 48.85472583770752 and batch: 600, loss is 6.692909679412842 and perplexity is 806.6659801373357
At time: 50.431342363357544 and batch: 650, loss is 6.6895068168640135 and perplexity is 803.925671774739
At time: 52.00925302505493 and batch: 700, loss is 6.650046787261963 and perplexity is 772.8204828436734
At time: 53.578320026397705 and batch: 750, loss is 6.743983144760132 and perplexity is 848.9354431775497
At time: 55.14934158325195 and batch: 800, loss is 6.687322635650634 and perplexity is 802.1716686531395
At time: 56.72324514389038 and batch: 850, loss is 6.7251318359375 and perplexity is 833.0817994269888
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.21479860941569 and perplexity of 500.0952645709352
Finished 2 epochs...
Completing Train Step...
At time: 60.87509536743164 and batch: 50, loss is 6.7182114315032955 and perplexity is 827.3364394851003
At time: 62.44813561439514 and batch: 100, loss is 6.645394487380981 and perplexity is 769.2334406710506
At time: 64.01793146133423 and batch: 150, loss is 6.61877820968628 and perplexity is 749.029380976874
At time: 65.59459900856018 and batch: 200, loss is 6.644937763214111 and perplexity is 768.8821933863989
At time: 67.16439151763916 and batch: 250, loss is 6.742975645065307 and perplexity is 848.0805716913152
At time: 68.73925137519836 and batch: 300, loss is 6.667808971405029 and perplexity is 786.6700981562243
At time: 70.31449389457703 and batch: 350, loss is 6.592564296722412 and perplexity is 729.6495108491209
At time: 71.88222622871399 and batch: 400, loss is 6.575508632659912 and perplexity is 717.3103790671825
At time: 73.4534387588501 and batch: 450, loss is 6.604207630157471 and perplexity is 738.1947141832773
At time: 75.02196288108826 and batch: 500, loss is 6.659757194519043 and perplexity is 780.3614380877369
At time: 76.59693050384521 and batch: 550, loss is 6.648114614486694 and perplexity is 771.3287018000169
At time: 78.16359257698059 and batch: 600, loss is 6.63116075515747 and perplexity is 758.3619324733363
At time: 79.7320237159729 and batch: 650, loss is 6.66888843536377 and perplexity is 787.5197386702562
At time: 81.29750537872314 and batch: 700, loss is 6.571792869567871 and perplexity is 714.6499694220904
At time: 82.86533093452454 and batch: 750, loss is 6.684612274169922 and perplexity is 800.0004372017152
At time: 84.44091582298279 and batch: 800, loss is 6.627902231216431 and perplexity is 755.8948137261344
At time: 86.01150131225586 and batch: 850, loss is 6.559893245697022 and perplexity is 706.1963010772589
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.027790069580078 and perplexity of 414.7973424781211
Finished 3 epochs...
Completing Train Step...
At time: 90.09316849708557 and batch: 50, loss is 6.630600662231445 and perplexity is 757.9372982480048
At time: 91.65824270248413 and batch: 100, loss is 6.6511630439758305 and perplexity is 773.6836305538498
At time: 93.22282552719116 and batch: 150, loss is 6.63904824256897 and perplexity is 764.3671545927455
At time: 94.78954792022705 and batch: 200, loss is 6.497415580749512 and perplexity is 663.4248476058549
At time: 96.40312671661377 and batch: 250, loss is 6.560349731445313 and perplexity is 706.5187432137175
At time: 97.96913123130798 and batch: 300, loss is 6.581427402496338 and perplexity is 721.5685632772972
At time: 99.53952741622925 and batch: 350, loss is 6.562623739242554 and perplexity is 708.1272004737318
At time: 101.10710501670837 and batch: 400, loss is 6.516408376693725 and perplexity is 676.1455589088162
At time: 102.67375254631042 and batch: 450, loss is 6.56615912437439 and perplexity is 710.6351334920778
At time: 104.24525690078735 and batch: 500, loss is 6.611827955245972 and perplexity is 743.8414856758659
At time: 105.81407380104065 and batch: 550, loss is 6.596999473571778 and perplexity is 732.8928224815824
At time: 107.38009524345398 and batch: 600, loss is 6.569568796157837 and perplexity is 713.0623016269305
At time: 108.95143151283264 and batch: 650, loss is 6.574579076766968 and perplexity is 716.6439087859292
At time: 110.51637434959412 and batch: 700, loss is 6.549184770584106 and perplexity is 698.6743616918588
At time: 112.08311367034912 and batch: 750, loss is 6.5118156433105465 and perplexity is 673.0473227607949
At time: 113.65497279167175 and batch: 800, loss is 6.538256502151489 and perplexity is 691.0806294669854
At time: 115.22314190864563 and batch: 850, loss is 6.549119300842285 and perplexity is 698.6286211591089
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.229345957438151 and perplexity of 507.4234983766088
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 119.31580305099487 and batch: 50, loss is 6.47787187576294 and perplexity is 650.5849464115896
At time: 120.88693952560425 and batch: 100, loss is 6.278736772537232 and perplexity is 533.114793041753
At time: 122.46058058738708 and batch: 150, loss is 6.27810302734375 and perplexity is 532.7770411397155
At time: 124.03198051452637 and batch: 200, loss is 6.335790338516236 and perplexity is 564.4153059101444
At time: 125.60440611839294 and batch: 250, loss is 6.338340320587158 and perplexity is 565.8563914109873
At time: 127.18272948265076 and batch: 300, loss is 6.307113227844238 and perplexity is 548.4593840525927
At time: 128.75542783737183 and batch: 350, loss is 6.2334265041351316 and perplexity is 509.4982939280198
At time: 130.32209992408752 and batch: 400, loss is 6.167558460235596 and perplexity is 477.0200197987465
At time: 131.89001154899597 and batch: 450, loss is 6.175029487609863 and perplexity is 480.59719537436536
At time: 133.45866203308105 and batch: 500, loss is 6.156574602127075 and perplexity is 471.80916959452753
At time: 135.05141711235046 and batch: 550, loss is 6.111721429824829 and perplexity is 451.1146092952298
At time: 136.6213035583496 and batch: 600, loss is 6.122870035171509 and perplexity is 456.17204734794484
At time: 138.19034910202026 and batch: 650, loss is 6.13112093925476 and perplexity is 459.95144945725053
At time: 139.75881266593933 and batch: 700, loss is 6.10670163154602 and perplexity is 448.85577913505443
At time: 141.33076000213623 and batch: 750, loss is 6.0623533725738525 and perplexity is 429.3847510440351
At time: 142.8986897468567 and batch: 800, loss is 5.987730798721313 and perplexity is 398.509285353857
At time: 144.47861409187317 and batch: 850, loss is 5.971477041244507 and perplexity is 392.08436804772293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.670892715454102 and perplexity of 290.2935683073963
Finished 5 epochs...
Completing Train Step...
At time: 148.56133317947388 and batch: 50, loss is 6.0030702781677245 and perplexity is 404.6693355397003
At time: 150.13168907165527 and batch: 100, loss is 5.9461272430419925 and perplexity is 382.2700297492529
At time: 151.6993989944458 and batch: 150, loss is 5.969057388305664 and perplexity is 391.1368068012883
At time: 153.26867771148682 and batch: 200, loss is 6.017684545516968 and perplexity is 410.6267066752385
At time: 154.83816194534302 and batch: 250, loss is 6.048694324493408 and perplexity is 423.55963740363507
At time: 156.40522289276123 and batch: 300, loss is 6.01368670463562 and perplexity is 408.98836354000025
At time: 157.9705491065979 and batch: 350, loss is 5.963626613616944 and perplexity is 389.01838846402336
At time: 159.53762292861938 and batch: 400, loss is 5.947372617721558 and perplexity is 382.7463957305564
At time: 161.10477328300476 and batch: 450, loss is 5.957687597274781 and perplexity is 386.7148490475033
At time: 162.6724555492401 and batch: 500, loss is 5.975672702789307 and perplexity is 393.7328772280412
At time: 164.23959517478943 and batch: 550, loss is 5.944416608810425 and perplexity is 381.6166645444308
At time: 165.8080916404724 and batch: 600, loss is 5.956597061157226 and perplexity is 386.2933524078497
At time: 167.37568616867065 and batch: 650, loss is 5.970783023834229 and perplexity is 391.81234907387034
At time: 168.95094323158264 and batch: 700, loss is 5.944013919830322 and perplexity is 381.4630226560191
At time: 170.51761841773987 and batch: 750, loss is 5.938257265090942 and perplexity is 379.27338029318094
At time: 172.08425450325012 and batch: 800, loss is 5.928092765808105 and perplexity is 375.43778277877743
At time: 173.6515772342682 and batch: 850, loss is 5.939877099990845 and perplexity is 379.8882384010527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.631072362263997 and perplexity of 278.9611046549431
Finished 6 epochs...
Completing Train Step...
At time: 177.72420048713684 and batch: 50, loss is 5.974595193862915 and perplexity is 393.30885502309445
At time: 179.31420993804932 and batch: 100, loss is 5.905957107543945 and perplexity is 367.2185252657671
At time: 180.92195534706116 and batch: 150, loss is 5.914280023574829 and perplexity is 370.2876083604837
At time: 182.52678227424622 and batch: 200, loss is 5.915354290008545 and perplexity is 370.6856096504035
At time: 184.13118743896484 and batch: 250, loss is 5.945642375946045 and perplexity is 382.0847245178958
At time: 185.73770141601562 and batch: 300, loss is 5.91876145362854 and perplexity is 371.9507502206296
At time: 187.33992886543274 and batch: 350, loss is 5.900838479995728 and perplexity is 365.3436728352562
At time: 188.941565990448 and batch: 400, loss is 5.900118713378906 and perplexity is 365.08080526888034
At time: 190.54632925987244 and batch: 450, loss is 5.904223375320434 and perplexity is 366.5824182543909
At time: 192.1508662700653 and batch: 500, loss is 5.927133769989013 and perplexity is 375.0779120995821
At time: 193.756445646286 and batch: 550, loss is 5.889904222488403 and perplexity is 361.3706715310633
At time: 195.3655686378479 and batch: 600, loss is 5.8964698696136475 and perplexity is 363.7511098494832
At time: 196.9663016796112 and batch: 650, loss is 5.914278430938721 and perplexity is 370.2870186275378
At time: 198.56783318519592 and batch: 700, loss is 5.8780247020721434 and perplexity is 357.1031594461351
At time: 200.1752200126648 and batch: 750, loss is 5.89787691116333 and perplexity is 364.26328301474797
At time: 201.76795053482056 and batch: 800, loss is 5.905560235977173 and perplexity is 367.07281559022334
At time: 203.35184264183044 and batch: 850, loss is 5.910473442077636 and perplexity is 368.8807577463965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.607001622517903 and perplexity of 272.32647506580867
Finished 7 epochs...
Completing Train Step...
At time: 207.44260549545288 and batch: 50, loss is 5.92735107421875 and perplexity is 375.15942697280474
At time: 209.03702545166016 and batch: 100, loss is 5.863689794540405 and perplexity is 352.02063447247195
At time: 210.60840964317322 and batch: 150, loss is 5.882365293502808 and perplexity is 358.6565672753015
At time: 212.17585635185242 and batch: 200, loss is 5.874528875350952 and perplexity is 355.8569681826024
At time: 213.7446928024292 and batch: 250, loss is 5.9070264339447025 and perplexity is 367.6114117541907
At time: 215.35012578964233 and batch: 300, loss is 5.876243600845337 and perplexity is 356.46768865764267
At time: 216.91718745231628 and batch: 350, loss is 5.855761833190918 and perplexity is 349.24086201210406
At time: 218.48619270324707 and batch: 400, loss is 5.8638866424560545 and perplexity is 352.08993582132274
At time: 220.05263090133667 and batch: 450, loss is 5.871015157699585 and perplexity is 354.6087814442223
At time: 221.61954426765442 and batch: 500, loss is 5.893708543777466 and perplexity is 362.7480600229644
At time: 223.187251329422 and batch: 550, loss is 5.862257089614868 and perplexity is 351.5166538895121
At time: 224.75598216056824 and batch: 600, loss is 5.867803936004639 and perplexity is 353.4718804296033
At time: 226.32374215126038 and batch: 650, loss is 5.886014804840088 and perplexity is 359.9678798533829
At time: 227.89219117164612 and batch: 700, loss is 5.851599712371826 and perplexity is 347.7903001503443
At time: 229.47371888160706 and batch: 750, loss is 5.862110586166382 and perplexity is 351.46515925967935
At time: 231.0442340373993 and batch: 800, loss is 5.872395029067993 and perplexity is 355.09843369959395
At time: 232.6133246421814 and batch: 850, loss is 5.876664476394653 and perplexity is 356.6177487680207
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.583580652872722 and perplexity of 266.0224364743907
Finished 8 epochs...
Completing Train Step...
At time: 236.68874597549438 and batch: 50, loss is 5.896515312194825 and perplexity is 363.76764001440466
At time: 238.28411436080933 and batch: 100, loss is 5.8310663032531735 and perplexity is 340.7217982016379
At time: 239.85243916511536 and batch: 150, loss is 5.8476676082611085 and perplexity is 346.42543763077896
At time: 241.4370231628418 and batch: 200, loss is 5.847371406555176 and perplexity is 346.3228410205699
At time: 243.00799798965454 and batch: 250, loss is 5.87461441040039 and perplexity is 355.88740772777413
At time: 244.57730627059937 and batch: 300, loss is 5.843287467956543 and perplexity is 344.91136395715046
At time: 246.14986968040466 and batch: 350, loss is 5.826255445480347 and perplexity is 339.08657066778943
At time: 247.71983671188354 and batch: 400, loss is 5.832768249511719 and perplexity is 341.3021821423341
At time: 249.29139852523804 and batch: 450, loss is 5.836642780303955 and perplexity is 342.6271330805944
At time: 250.86010241508484 and batch: 500, loss is 5.861176671981812 and perplexity is 351.13707418756223
At time: 252.43224954605103 and batch: 550, loss is 5.833880786895752 and perplexity is 341.6821048791563
At time: 254.00230264663696 and batch: 600, loss is 5.838005170822144 and perplexity is 343.0942431588978
At time: 255.60501861572266 and batch: 650, loss is 5.861776485443115 and perplexity is 351.34775410943257
At time: 257.1736361980438 and batch: 700, loss is 5.824890928268433 and perplexity is 338.62419673611987
At time: 258.7438955307007 and batch: 750, loss is 5.831494264602661 and perplexity is 340.86764516840236
At time: 260.3150680065155 and batch: 800, loss is 5.845580549240112 and perplexity is 345.70318125408016
At time: 261.8854591846466 and batch: 850, loss is 5.844814176559448 and perplexity is 345.4383452748403
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.5877634684244795 and perplexity of 267.13748966396145
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 266.03552865982056 and batch: 50, loss is 5.8459841251373295 and perplexity is 345.84272688235774
At time: 267.60744762420654 and batch: 100, loss is 5.760740976333619 and perplexity is 317.58356366001755
At time: 269.17839527130127 and batch: 150, loss is 5.744019346237183 and perplexity is 312.31720256617825
At time: 270.74893736839294 and batch: 200, loss is 5.749566965103149 and perplexity is 314.05463421966823
At time: 272.3230173587799 and batch: 250, loss is 5.775798664093018 and perplexity is 322.4018226808318
At time: 273.9074716567993 and batch: 300, loss is 5.732033958435059 and perplexity is 308.5963025331017
At time: 275.4792022705078 and batch: 350, loss is 5.705075607299805 and perplexity is 300.3881907300395
At time: 277.0500876903534 and batch: 400, loss is 5.7192839241027835 and perplexity is 304.68666604422
At time: 278.6214053630829 and batch: 450, loss is 5.730050764083862 and perplexity is 307.98490255200176
At time: 280.2058744430542 and batch: 500, loss is 5.736386766433716 and perplexity is 309.9424907109467
At time: 281.78259539604187 and batch: 550, loss is 5.6997192859649655 and perplexity is 298.7835164673235
At time: 283.3549826145172 and batch: 600, loss is 5.690460052490234 and perplexity is 296.02977855499637
At time: 284.9311647415161 and batch: 650, loss is 5.705818042755127 and perplexity is 300.611292382238
At time: 286.5019657611847 and batch: 700, loss is 5.659690809249878 and perplexity is 287.05987257600145
At time: 288.0778076648712 and batch: 750, loss is 5.651463975906372 and perplexity is 284.70796647934867
At time: 289.65233874320984 and batch: 800, loss is 5.661555662155151 and perplexity is 287.5956964744538
At time: 291.22508358955383 and batch: 850, loss is 5.690901165008545 and perplexity is 296.16038980111966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.40182367960612 and perplexity of 221.81055897153578
Finished 10 epochs...
Completing Train Step...
At time: 295.4682421684265 and batch: 50, loss is 5.729190006256103 and perplexity is 307.71991619719887
At time: 297.04422092437744 and batch: 100, loss is 5.666666831970215 and perplexity is 289.0694099087472
At time: 298.6199588775635 and batch: 150, loss is 5.659101123809815 and perplexity is 286.8906474484175
At time: 300.1939034461975 and batch: 200, loss is 5.680477886199951 and perplexity is 293.08945986891865
At time: 301.7655756473541 and batch: 250, loss is 5.706285095214843 and perplexity is 300.75172641824173
At time: 303.337584733963 and batch: 300, loss is 5.664418439865113 and perplexity is 288.4201986439178
At time: 304.9105522632599 and batch: 350, loss is 5.6454187107086184 and perplexity is 282.99202320992396
At time: 306.4833941459656 and batch: 400, loss is 5.663689889907837 and perplexity is 288.2101466464962
At time: 308.0562083721161 and batch: 450, loss is 5.675354022979736 and perplexity is 291.5915503840934
At time: 309.63058376312256 and batch: 500, loss is 5.687845973968506 and perplexity is 295.2569440342356
At time: 311.20482778549194 and batch: 550, loss is 5.665099973678589 and perplexity is 288.61683376091247
At time: 312.7812385559082 and batch: 600, loss is 5.662107982635498 and perplexity is 287.75458534242506
At time: 314.35486912727356 and batch: 650, loss is 5.682154674530029 and perplexity is 293.581321113249
At time: 315.9358742237091 and batch: 700, loss is 5.644237718582153 and perplexity is 282.6580091314809
At time: 317.5153410434723 and batch: 750, loss is 5.648728551864624 and perplexity is 283.9302336624383
At time: 319.08780217170715 and batch: 800, loss is 5.665670595169067 and perplexity is 288.78157172581183
At time: 320.6649775505066 and batch: 850, loss is 5.688707361221313 and perplexity is 295.5113841721425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.395896275838216 and perplexity of 220.499687099987
Finished 11 epochs...
Completing Train Step...
At time: 324.80720496177673 and batch: 50, loss is 5.709396600723267 and perplexity is 301.68897444169255
At time: 326.38583636283875 and batch: 100, loss is 5.643684272766113 and perplexity is 282.50161652036695
At time: 327.9645233154297 and batch: 150, loss is 5.637478065490723 and perplexity is 280.7537822535888
At time: 329.53649520874023 and batch: 200, loss is 5.66349835395813 and perplexity is 288.15494932864715
At time: 331.1078553199768 and batch: 250, loss is 5.687334175109863 and perplexity is 295.10587053019367
At time: 332.6827247142792 and batch: 300, loss is 5.648143672943116 and perplexity is 283.7642174080266
At time: 334.26125979423523 and batch: 350, loss is 5.629458494186402 and perplexity is 278.5112613247823
At time: 335.87032771110535 and batch: 400, loss is 5.647902202606201 and perplexity is 283.6957050390302
At time: 337.4451177120209 and batch: 450, loss is 5.6604231834411625 and perplexity is 287.2701848222597
At time: 339.01568961143494 and batch: 500, loss is 5.672881183624267 and perplexity is 290.8713821197765
At time: 340.5909171104431 and batch: 550, loss is 5.653143100738525 and perplexity is 285.1864282819059
At time: 342.1599609851837 and batch: 600, loss is 5.654077596664429 and perplexity is 285.4530584002344
At time: 343.7305016517639 and batch: 650, loss is 5.67605788230896 and perplexity is 291.7968620639486
At time: 345.3004324436188 and batch: 700, loss is 5.637378950119018 and perplexity is 280.7259566171001
At time: 346.8705379962921 and batch: 750, loss is 5.643580102920533 and perplexity is 282.4721899033075
At time: 348.445232629776 and batch: 800, loss is 5.65710410118103 and perplexity is 286.318292027058
At time: 350.0161461830139 and batch: 850, loss is 5.678586235046387 and perplexity is 292.53556091103144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.3863875071207685 and perplexity of 218.41294347733654
Finished 12 epochs...
Completing Train Step...
At time: 354.10585379600525 and batch: 50, loss is 5.690673780441284 and perplexity is 296.0930551547649
At time: 355.70568203926086 and batch: 100, loss is 5.62835470199585 and perplexity is 278.20401237024447
At time: 357.278831243515 and batch: 150, loss is 5.620525979995728 and perplexity is 276.034533701172
At time: 358.85758805274963 and batch: 200, loss is 5.6493701171875 and perplexity is 284.1124519006733
At time: 360.43300318717957 and batch: 250, loss is 5.674247789382934 and perplexity is 291.26916036672645
At time: 362.0097641944885 and batch: 300, loss is 5.635172595977783 and perplexity is 280.10725852470017
At time: 363.5834929943085 and batch: 350, loss is 5.61770320892334 and perplexity is 275.25645009705477
At time: 365.1601707935333 and batch: 400, loss is 5.636189517974853 and perplexity is 280.39225064024856
At time: 366.74004340171814 and batch: 450, loss is 5.650139904022216 and perplexity is 284.33124212583886
At time: 368.3158423900604 and batch: 500, loss is 5.6620942497253415 and perplexity is 287.7506336616915
At time: 369.88760924339294 and batch: 550, loss is 5.645069303512574 and perplexity is 282.89316103317486
At time: 371.45758962631226 and batch: 600, loss is 5.64399224281311 and perplexity is 282.58863195486936
At time: 373.02676463127136 and batch: 650, loss is 5.66382080078125 and perplexity is 288.24787895825114
At time: 374.6222059726715 and batch: 700, loss is 5.625295286178589 and perplexity is 277.35417128587903
At time: 376.1938681602478 and batch: 750, loss is 5.631014013290406 and perplexity is 278.94482803568116
At time: 377.76134037971497 and batch: 800, loss is 5.646642818450927 and perplexity is 283.3386480463368
At time: 379.33133459091187 and batch: 850, loss is 5.6650388813018795 and perplexity is 288.5992020111679
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.378396987915039 and perplexity of 216.67466478120335
Finished 13 epochs...
Completing Train Step...
At time: 383.37200832366943 and batch: 50, loss is 5.6744380378723145 and perplexity is 291.3245791559925
At time: 385.028849363327 and batch: 100, loss is 5.61208441734314 and perplexity is 273.7141783820799
At time: 386.63369250297546 and batch: 150, loss is 5.604556198120117 and perplexity is 271.66133486602297
At time: 388.2366578578949 and batch: 200, loss is 5.629764633178711 and perplexity is 278.5965375341915
At time: 389.8425211906433 and batch: 250, loss is 5.6543405246734615 and perplexity is 285.528121872264
At time: 391.44924569129944 and batch: 300, loss is 5.613237009048462 and perplexity is 274.02984095379753
At time: 393.0552463531494 and batch: 350, loss is 5.595968036651612 and perplexity is 269.33825323603185
At time: 394.6642735004425 and batch: 400, loss is 5.612937259674072 and perplexity is 273.94771298992526
At time: 396.2712457180023 and batch: 450, loss is 5.6231624794006345 and perplexity is 276.7632588045377
At time: 397.8780586719513 and batch: 500, loss is 5.636505346298218 and perplexity is 280.48082044034237
At time: 399.4843292236328 and batch: 550, loss is 5.617616453170776 and perplexity is 275.23257105241595
At time: 401.08992052078247 and batch: 600, loss is 5.619810180664063 and perplexity is 275.8370190652877
At time: 402.6936573982239 and batch: 650, loss is 5.6380284309387205 and perplexity is 280.9083419629963
At time: 404.2970905303955 and batch: 700, loss is 5.600959939956665 and perplexity is 270.6861251767576
At time: 405.90065336227417 and batch: 750, loss is 5.603575868606567 and perplexity is 271.39514773863016
At time: 407.50451469421387 and batch: 800, loss is 5.617972755432129 and perplexity is 275.33065451252156
At time: 409.0836901664734 and batch: 850, loss is 5.637018127441406 and perplexity is 280.62468259783856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.35383669535319 and perplexity of 211.41788983019023
Finished 14 epochs...
Completing Train Step...
At time: 413.25717067718506 and batch: 50, loss is 5.649147539138794 and perplexity is 284.0492217425999
At time: 414.8826723098755 and batch: 100, loss is 5.583379449844361 and perplexity is 265.9689173388468
At time: 416.46156334877014 and batch: 150, loss is 5.578452262878418 and perplexity is 264.6616619465581
At time: 418.0433597564697 and batch: 200, loss is 5.605661363601684 and perplexity is 271.9617315594463
At time: 419.61465525627136 and batch: 250, loss is 5.632817182540894 and perplexity is 279.4482665281749
At time: 421.19004225730896 and batch: 300, loss is 5.593750076293945 and perplexity is 268.7415336623534
At time: 422.76033115386963 and batch: 350, loss is 5.577751922607422 and perplexity is 264.4763736165211
At time: 424.33488845825195 and batch: 400, loss is 5.595761947631836 and perplexity is 269.28275129880063
At time: 425.9094150066376 and batch: 450, loss is 5.608411235809326 and perplexity is 272.71062076950415
At time: 427.4921214580536 and batch: 500, loss is 5.622048568725586 and perplexity is 276.45514089586726
At time: 429.06548857688904 and batch: 550, loss is 5.603854207992554 and perplexity is 271.4706982112598
At time: 430.6341404914856 and batch: 600, loss is 5.60785439491272 and perplexity is 272.5588066150086
At time: 432.2042682170868 and batch: 650, loss is 5.627689809799194 and perplexity is 278.01909817415645
At time: 433.77688336372375 and batch: 700, loss is 5.588045330047607 and perplexity is 267.21279608289257
At time: 435.34853506088257 and batch: 750, loss is 5.5918528270721435 and perplexity is 268.23214736592087
At time: 436.93614959716797 and batch: 800, loss is 5.605999822616577 and perplexity is 272.0537950381768
At time: 438.51547145843506 and batch: 850, loss is 5.626496982574463 and perplexity is 277.68766713414067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.345758438110352 and perplexity of 209.71688157193535
Finished 15 epochs...
Completing Train Step...
At time: 442.70540595054626 and batch: 50, loss is 5.637141771316529 and perplexity is 280.6593822662065
At time: 444.28819155693054 and batch: 100, loss is 5.575901374816895 and perplexity is 263.98740002172315
At time: 445.8731732368469 and batch: 150, loss is 5.561228427886963 and perplexity is 260.14220606194493
At time: 447.4548909664154 and batch: 200, loss is 5.59208568572998 and perplexity is 268.2946148165063
At time: 449.03719544410706 and batch: 250, loss is 5.62036527633667 and perplexity is 275.99017750577667
At time: 450.6281485557556 and batch: 300, loss is 5.583167705535889 and perplexity is 265.9126058963939
At time: 452.23554706573486 and batch: 350, loss is 5.567774229049682 and perplexity is 261.85063062044736
At time: 453.83769965171814 and batch: 400, loss is 5.586104249954223 and perplexity is 266.69461771927087
At time: 455.5006079673767 and batch: 450, loss is 5.5988804149627684 and perplexity is 270.12381148927557
At time: 457.09108424186707 and batch: 500, loss is 5.614283075332642 and perplexity is 274.31664431278745
At time: 458.6681180000305 and batch: 550, loss is 5.596292638778687 and perplexity is 269.42569519709053
At time: 460.25334787368774 and batch: 600, loss is 5.602237453460694 and perplexity is 271.03215133650934
At time: 461.8349223136902 and batch: 650, loss is 5.619176445007324 and perplexity is 275.66226669011184
At time: 463.40908193588257 and batch: 700, loss is 5.579109735488892 and perplexity is 264.8357269555481
At time: 464.9933524131775 and batch: 750, loss is 5.583661794662476 and perplexity is 266.0440228867692
At time: 466.5675311088562 and batch: 800, loss is 5.598630208969116 and perplexity is 270.0562333471933
At time: 468.1467635631561 and batch: 850, loss is 5.6216016864776615 and perplexity is 276.33162560150515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.342374165852864 and perplexity of 209.00834216908623
Finished 16 epochs...
Completing Train Step...
At time: 472.2541961669922 and batch: 50, loss is 5.6337909984588626 and perplexity is 279.7205302442231
At time: 473.8514258861542 and batch: 100, loss is 5.565785865783692 and perplexity is 261.33049372725833
At time: 475.4474370479584 and batch: 150, loss is 5.560801477432251 and perplexity is 260.03116193563386
At time: 477.0291483402252 and batch: 200, loss is 5.588928928375244 and perplexity is 267.4490092060264
At time: 478.6200158596039 and batch: 250, loss is 5.6153851413726805 and perplexity is 274.61912601749424
At time: 480.2118239402771 and batch: 300, loss is 5.576775283813476 and perplexity is 264.21820182078164
At time: 481.78908371925354 and batch: 350, loss is 5.56118447303772 and perplexity is 260.1307718017932
At time: 483.3602855205536 and batch: 400, loss is 5.581816139221192 and perplexity is 265.5534501416944
At time: 484.93562150001526 and batch: 450, loss is 5.594508581161499 and perplexity is 268.94545275078394
At time: 486.5064465999603 and batch: 500, loss is 5.611137609481812 and perplexity is 273.45514629239096
At time: 488.0779490470886 and batch: 550, loss is 5.593142108917236 and perplexity is 268.57819723376105
At time: 489.657044172287 and batch: 600, loss is 5.5945641231536865 and perplexity is 268.96039093186386
At time: 491.2288031578064 and batch: 650, loss is 5.613703060150146 and perplexity is 274.15758262783015
At time: 492.79906153678894 and batch: 700, loss is 5.575843124389649 and perplexity is 263.972023090745
At time: 494.3706669807434 and batch: 750, loss is 5.580580587387085 and perplexity is 265.2255477012636
At time: 496.0012879371643 and batch: 800, loss is 5.592785978317261 and perplexity is 268.4825653489709
At time: 497.59220337867737 and batch: 850, loss is 5.615206747055054 and perplexity is 274.57013989545396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.341033935546875 and perplexity of 208.72841048307276
Finished 17 epochs...
Completing Train Step...
At time: 501.8795876502991 and batch: 50, loss is 5.627818632125854 and perplexity is 278.05491554822845
At time: 503.4605360031128 and batch: 100, loss is 5.559115810394287 and perplexity is 259.59320520543116
At time: 505.0324285030365 and batch: 150, loss is 5.5562106895446775 and perplexity is 258.8401499604501
At time: 506.6063117980957 and batch: 200, loss is 5.583652582168579 and perplexity is 266.04157196912166
At time: 508.1786410808563 and batch: 250, loss is 5.611176910400391 and perplexity is 273.46589354201745
At time: 509.751788854599 and batch: 300, loss is 5.574930925369262 and perplexity is 263.7313378629764
At time: 511.32219886779785 and batch: 350, loss is 5.559083652496338 and perplexity is 259.5848573678551
At time: 512.892995595932 and batch: 400, loss is 5.576851902008056 and perplexity is 264.2384465179265
At time: 514.4630937576294 and batch: 450, loss is 5.589380931854248 and perplexity is 267.56992441363224
At time: 516.0333473682404 and batch: 500, loss is 5.602078638076782 and perplexity is 270.9891106791922
At time: 517.6034827232361 and batch: 550, loss is 5.585932779312134 and perplexity is 266.6488913424063
At time: 519.1738426685333 and batch: 600, loss is 5.5894871425628665 and perplexity is 267.5983447141526
At time: 520.74551820755 and batch: 650, loss is 5.609072046279907 and perplexity is 272.8908903586035
At time: 522.3213477134705 and batch: 700, loss is 5.571814861297607 and perplexity is 262.91081318415905
At time: 523.8928692340851 and batch: 750, loss is 5.576145486831665 and perplexity is 264.0518503840502
At time: 525.4642686843872 and batch: 800, loss is 5.58684042930603 and perplexity is 266.89102507672396
At time: 527.0358781814575 and batch: 850, loss is 5.608792238235473 and perplexity is 272.81454397389285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.341044743855794 and perplexity of 208.73066649640523
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 531.1182396411896 and batch: 50, loss is 5.619866256713867 and perplexity is 275.8524873494028
At time: 532.7153108119965 and batch: 100, loss is 5.553648386001587 and perplexity is 258.17777189634063
At time: 534.2970082759857 and batch: 150, loss is 5.542975482940673 and perplexity is 255.4369180159953
At time: 535.9058413505554 and batch: 200, loss is 5.566846570968628 and perplexity is 261.6078353998088
At time: 537.5102586746216 and batch: 250, loss is 5.587123203277588 and perplexity is 266.9665055833163
At time: 539.1133437156677 and batch: 300, loss is 5.5388047313690185 and perplexity is 254.37377268737876
At time: 540.7170834541321 and batch: 350, loss is 5.528314456939698 and perplexity is 251.7192695803442
At time: 542.3018293380737 and batch: 400, loss is 5.539175300598145 and perplexity is 254.46805324789187
At time: 543.8720440864563 and batch: 450, loss is 5.555641288757324 and perplexity is 258.69280812752277
At time: 545.4433073997498 and batch: 500, loss is 5.564946212768555 and perplexity is 261.1111588857197
At time: 547.0137794017792 and batch: 550, loss is 5.541051626205444 and perplexity is 254.94596639256463
At time: 548.5845589637756 and batch: 600, loss is 5.5316048431396485 and perplexity is 252.5488873241231
At time: 550.1544144153595 and batch: 650, loss is 5.550253238677978 and perplexity is 257.3027066533552
At time: 551.7248668670654 and batch: 700, loss is 5.513446941375732 and perplexity is 248.00451245331837
At time: 553.2954890727997 and batch: 750, loss is 5.513158826828003 and perplexity is 247.93306903781604
At time: 554.8664994239807 and batch: 800, loss is 5.522463293075561 and perplexity is 250.25071943981203
At time: 556.4453411102295 and batch: 850, loss is 5.557922134399414 and perplexity is 259.28351989667635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.304229100545247 and perplexity of 201.18584855641188
Finished 19 epochs...
Completing Train Step...
At time: 560.5134739875793 and batch: 50, loss is 5.5932685661315915 and perplexity is 268.6121630319846
At time: 562.1129496097565 and batch: 100, loss is 5.5305373477935795 and perplexity is 252.27943640663753
At time: 563.6884822845459 and batch: 150, loss is 5.518611860275269 and perplexity is 249.2887492814929
At time: 565.2636170387268 and batch: 200, loss is 5.547699604034424 and perplexity is 256.6464877759973
At time: 566.838210105896 and batch: 250, loss is 5.56856369972229 and perplexity is 262.0574356364324
At time: 568.4124567508698 and batch: 300, loss is 5.524267024993897 and perplexity is 250.70251198387783
At time: 569.987174987793 and batch: 350, loss is 5.5147582626342775 and perplexity is 248.32993936569505
At time: 571.561986207962 and batch: 400, loss is 5.526419467926026 and perplexity is 251.24271600446326
At time: 573.1366748809814 and batch: 450, loss is 5.543978700637817 and perplexity is 255.69330543734867
At time: 574.7383043766022 and batch: 500, loss is 5.555658254623413 and perplexity is 258.69719711229493
At time: 576.312753200531 and batch: 550, loss is 5.534962558746338 and perplexity is 253.39829990944958
At time: 577.8855068683624 and batch: 600, loss is 5.530663042068482 and perplexity is 252.3111484804407
At time: 579.4569225311279 and batch: 650, loss is 5.550777320861816 and perplexity is 257.43758975959463
At time: 581.028290271759 and batch: 700, loss is 5.516224908828735 and perplexity is 248.69441874204634
At time: 582.6002321243286 and batch: 750, loss is 5.51696704864502 and perplexity is 248.87905327612788
At time: 584.1721470355988 and batch: 800, loss is 5.526435785293579 and perplexity is 251.24681565765303
At time: 585.742921590805 and batch: 850, loss is 5.5589931869506835 and perplexity is 259.5613749442808
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.303559939066569 and perplexity of 201.0512677696638
Finished 20 epochs...
Completing Train Step...
At time: 589.8763771057129 and batch: 50, loss is 5.586347751617431 and perplexity is 266.7595662094659
At time: 591.5190207958221 and batch: 100, loss is 5.522409362792969 and perplexity is 250.23722371171112
At time: 593.1204006671906 and batch: 150, loss is 5.5102293586730955 and perplexity is 247.20781982342112
At time: 594.7248554229736 and batch: 200, loss is 5.541269960403443 and perplexity is 255.00163589272657
At time: 596.3034982681274 and batch: 250, loss is 5.561857089996338 and perplexity is 260.30579902690664
At time: 597.8776545524597 and batch: 300, loss is 5.518081827163696 and perplexity is 249.1566530008251
At time: 599.451423406601 and batch: 350, loss is 5.5098260974884035 and perplexity is 247.10815060284864
At time: 601.029141664505 and batch: 400, loss is 5.523293170928955 and perplexity is 250.45848316700514
At time: 602.6135897636414 and batch: 450, loss is 5.541258487701416 and perplexity is 254.99871035172362
At time: 604.196774482727 and batch: 500, loss is 5.5537839698791505 and perplexity is 258.2127790129017
At time: 605.7737872600555 and batch: 550, loss is 5.534137477874756 and perplexity is 253.18931204710253
At time: 607.3547611236572 and batch: 600, loss is 5.53052414894104 and perplexity is 252.27610662953225
At time: 608.936872959137 and batch: 650, loss is 5.551089515686035 and perplexity is 257.51797298963834
At time: 610.5153615474701 and batch: 700, loss is 5.516761436462402 and perplexity is 248.82788597126682
At time: 612.0889768600464 and batch: 750, loss is 5.517294454574585 and perplexity is 248.9605510946215
At time: 613.6614034175873 and batch: 800, loss is 5.52648515701294 and perplexity is 251.25922045114646
At time: 615.2901566028595 and batch: 850, loss is 5.557931146621704 and perplexity is 259.2858566279234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.30363655090332 and perplexity of 201.0666712666063
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 619.3774735927582 and batch: 50, loss is 5.582785577774048 and perplexity is 265.8110127194803
At time: 620.9786524772644 and batch: 100, loss is 5.5193161773681645 and perplexity is 249.46438945460582
At time: 622.5862529277802 and batch: 150, loss is 5.506257123947144 and perplexity is 246.22780005987545
At time: 624.1892280578613 and batch: 200, loss is 5.5368897819519045 and perplexity is 253.8871258803641
At time: 625.7926337718964 and batch: 250, loss is 5.554726858139038 and perplexity is 258.45635962739067
At time: 627.3956995010376 and batch: 300, loss is 5.508188247680664 and perplexity is 246.70375582524815
At time: 628.9996228218079 and batch: 350, loss is 5.495893344879151 and perplexity is 243.689127383782
At time: 630.6026039123535 and batch: 400, loss is 5.511007499694824 and perplexity is 247.40025723092822
At time: 632.2062540054321 and batch: 450, loss is 5.528151960372925 and perplexity is 251.6783693864073
At time: 633.8079404830933 and batch: 500, loss is 5.538994398117065 and perplexity is 254.42202350927636
At time: 635.4097588062286 and batch: 550, loss is 5.515412998199463 and perplexity is 248.49258304738945
At time: 637.0196154117584 and batch: 600, loss is 5.50969274520874 and perplexity is 247.07520036468594
At time: 638.6162734031677 and batch: 650, loss is 5.52640513420105 and perplexity is 251.23911478627937
At time: 640.2143428325653 and batch: 700, loss is 5.49268440246582 and perplexity is 242.90839633768874
At time: 641.8199951648712 and batch: 750, loss is 5.494282665252686 and perplexity is 243.29693820146446
At time: 643.4229817390442 and batch: 800, loss is 5.502107772827149 and perplexity is 245.20823119935545
At time: 645.0249471664429 and batch: 850, loss is 5.540786857604981 and perplexity is 254.8784736412239
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.296234130859375 and perplexity of 199.5837865481522
Finished 22 epochs...
Completing Train Step...
At time: 649.3955693244934 and batch: 50, loss is 5.572789449691772 and perplexity is 263.1671679112687
At time: 650.9703228473663 and batch: 100, loss is 5.50970763206482 and perplexity is 247.0788785650129
At time: 652.5474715232849 and batch: 150, loss is 5.497259092330933 and perplexity is 244.02217256456873
At time: 654.1230611801147 and batch: 200, loss is 5.529231605529785 and perplexity is 251.9502394541944
At time: 655.7449789047241 and batch: 250, loss is 5.548244018554687 and perplexity is 256.78624789078305
At time: 657.3195426464081 and batch: 300, loss is 5.502576284408569 and perplexity is 245.323141011719
At time: 658.8945815563202 and batch: 350, loss is 5.49177227973938 and perplexity is 242.68693508422123
At time: 660.4689781665802 and batch: 400, loss is 5.5070634365081785 and perplexity is 246.42641669072108
At time: 662.0426580905914 and batch: 450, loss is 5.5250813674926755 and perplexity is 250.90675284365602
At time: 663.6254043579102 and batch: 500, loss is 5.536836347579956 and perplexity is 253.87355994369375
At time: 665.2106537818909 and batch: 550, loss is 5.515004529953003 and perplexity is 248.39110244494648
At time: 666.789482831955 and batch: 600, loss is 5.510830993652344 and perplexity is 247.35659344419017
At time: 668.369740486145 and batch: 650, loss is 5.528851346969605 and perplexity is 251.85445143216438
At time: 669.9473609924316 and batch: 700, loss is 5.495564393997192 and perplexity is 243.60897881359952
At time: 671.5285565853119 and batch: 750, loss is 5.497147703170777 and perplexity is 243.99499265350912
At time: 673.1097722053528 and batch: 800, loss is 5.504703397750855 and perplexity is 245.845526527771
At time: 674.68989610672 and batch: 850, loss is 5.541673984527588 and perplexity is 255.10468352078573
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.2955169677734375 and perplexity of 199.44070373687603
Finished 23 epochs...
Completing Train Step...
At time: 678.8001606464386 and batch: 50, loss is 5.5689685344696045 and perplexity is 262.16354706951853
At time: 680.371495962143 and batch: 100, loss is 5.505932331085205 and perplexity is 246.14784001393411
At time: 681.9440925121307 and batch: 150, loss is 5.493505172729492 and perplexity is 243.1078501678882
At time: 683.5163123607635 and batch: 200, loss is 5.525748586654663 and perplexity is 251.07421849895968
At time: 685.0877985954285 and batch: 250, loss is 5.544894075393676 and perplexity is 255.92746779119136
At time: 686.6585788726807 and batch: 300, loss is 5.499817819595337 and perplexity is 244.64735824936196
At time: 688.2340519428253 and batch: 350, loss is 5.489691352844238 and perplexity is 242.18244639798968
At time: 689.8045151233673 and batch: 400, loss is 5.5053309631347656 and perplexity is 245.99985909183144
At time: 691.3801550865173 and batch: 450, loss is 5.52385100364685 and perplexity is 250.59823607913953
At time: 692.9533042907715 and batch: 500, loss is 5.536193189620971 and perplexity is 253.71033163945833
At time: 694.5310981273651 and batch: 550, loss is 5.515257768630981 and perplexity is 248.45401264466298
At time: 696.1482441425323 and batch: 600, loss is 5.51176570892334 and perplexity is 247.58790951995667
At time: 697.719610452652 and batch: 650, loss is 5.5304053688049315 and perplexity is 252.24614301882605
At time: 699.2913355827332 and batch: 700, loss is 5.497119722366333 and perplexity is 243.9881655728484
At time: 700.8625569343567 and batch: 750, loss is 5.498504104614258 and perplexity is 244.3261723692928
At time: 702.4336516857147 and batch: 800, loss is 5.505823440551758 and perplexity is 246.12103830358092
At time: 704.010790348053 and batch: 850, loss is 5.5416944885253905 and perplexity is 255.10991424028126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.295018831888835 and perplexity of 199.34137990593035
Finished 24 epochs...
Completing Train Step...
At time: 708.0713474750519 and batch: 50, loss is 5.566325960159301 and perplexity is 261.47167497926387
At time: 709.6689631938934 and batch: 100, loss is 5.50321533203125 and perplexity is 245.47996428520412
At time: 711.2403140068054 and batch: 150, loss is 5.490823278427124 and perplexity is 242.45673411214776
At time: 712.8225750923157 and batch: 200, loss is 5.523242263793946 and perplexity is 250.44573336771887
At time: 714.3941264152527 and batch: 250, loss is 5.542127094268799 and perplexity is 255.2203001294453
At time: 715.968065738678 and batch: 300, loss is 5.497009792327881 and perplexity is 243.9613454186225
At time: 717.5404443740845 and batch: 350, loss is 5.4873281097412105 and perplexity is 241.61078615398606
At time: 719.1197962760925 and batch: 400, loss is 5.503096237182617 and perplexity is 245.45073062683883
At time: 720.6980936527252 and batch: 450, loss is 5.521778697967529 and perplexity is 250.07945765068237
At time: 722.2762112617493 and batch: 500, loss is 5.5347216796875 and perplexity is 253.33726891629544
At time: 723.8468449115753 and batch: 550, loss is 5.514666242599487 and perplexity is 248.307089087393
At time: 725.4263808727264 and batch: 600, loss is 5.511756162643433 and perplexity is 247.58554598775228
At time: 726.9969751834869 and batch: 650, loss is 5.530402345657349 and perplexity is 252.24538044266117
At time: 728.567592382431 and batch: 700, loss is 5.497101469039917 and perplexity is 243.98371201786685
At time: 730.13720703125 and batch: 750, loss is 5.498423233032226 and perplexity is 244.30641412415255
At time: 731.7083570957184 and batch: 800, loss is 5.505243768692017 and perplexity is 245.97841020632706
At time: 733.2786326408386 and batch: 850, loss is 5.540411672592163 and perplexity is 254.78286499441114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.293566068013509 and perplexity of 199.05199420578722
Finished 25 epochs...
Completing Train Step...
At time: 737.3920094966888 and batch: 50, loss is 5.563441123962402 and perplexity is 260.7184590015509
At time: 738.9889631271362 and batch: 100, loss is 5.5000393676757815 and perplexity is 244.70156540649208
At time: 740.5596826076508 and batch: 150, loss is 5.487938728332519 and perplexity is 241.75836324394442
At time: 742.1428287029266 and batch: 200, loss is 5.52096471786499 and perplexity is 249.87598077240068
At time: 743.7174372673035 and batch: 250, loss is 5.5399488353729245 and perplexity is 254.6649692870328
At time: 745.2895948886871 and batch: 300, loss is 5.494882888793946 and perplexity is 243.44301458614626
At time: 746.8704314231873 and batch: 350, loss is 5.485594263076782 and perplexity is 241.19223305662436
At time: 748.4449002742767 and batch: 400, loss is 5.501840591430664 and perplexity is 245.14272487313966
At time: 750.017190694809 and batch: 450, loss is 5.52078254699707 and perplexity is 249.83046479407963
At time: 751.5892553329468 and batch: 500, loss is 5.534144229888916 and perplexity is 253.1910215906941
At time: 753.1618297100067 and batch: 550, loss is 5.514541549682617 and perplexity is 248.2761288824744
At time: 754.7338376045227 and batch: 600, loss is 5.511660060882568 and perplexity is 247.56175372407574
At time: 756.3129684925079 and batch: 650, loss is 5.530578689575195 and perplexity is 252.2898663035974
At time: 757.8935885429382 and batch: 700, loss is 5.497095003128051 and perplexity is 243.98213444578855
At time: 759.4714045524597 and batch: 750, loss is 5.498308172225952 and perplexity is 244.2783056482885
At time: 761.041921377182 and batch: 800, loss is 5.504977922439576 and perplexity is 245.9130264591894
At time: 762.612799167633 and batch: 850, loss is 5.5396256923675535 and perplexity is 254.58268937829914
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.292929013570149 and perplexity of 198.9252276313109
Finished 26 epochs...
Completing Train Step...
At time: 766.7212119102478 and batch: 50, loss is 5.561476984024048 and perplexity is 260.2068740402549
At time: 768.2979958057404 and batch: 100, loss is 5.497968673706055 and perplexity is 244.19538760115134
At time: 769.8692429065704 and batch: 150, loss is 5.485935335159302 and perplexity is 241.27451102440173
At time: 771.4415633678436 and batch: 200, loss is 5.519200887680054 and perplexity is 249.43563044079153
At time: 773.0158035755157 and batch: 250, loss is 5.538228979110718 and perplexity is 254.22735856647654
At time: 774.5872316360474 and batch: 300, loss is 5.493215856552124 and perplexity is 243.03752530753982
At time: 776.1981499195099 and batch: 350, loss is 5.484257850646973 and perplexity is 240.87011604692344
At time: 777.7796316146851 and batch: 400, loss is 5.500771198272705 and perplexity is 244.88071104330533
At time: 779.3553998470306 and batch: 450, loss is 5.519929056167602 and perplexity is 249.61732775164998
At time: 780.9266159534454 and batch: 500, loss is 5.533672285079956 and perplexity is 253.0715575948028
At time: 782.4981679916382 and batch: 550, loss is 5.514385318756103 and perplexity is 248.2373435026446
At time: 784.0692040920258 and batch: 600, loss is 5.511515398025512 and perplexity is 247.52594332376407
At time: 785.6418311595917 and batch: 650, loss is 5.530533428192139 and perplexity is 252.27844757373305
At time: 787.2129347324371 and batch: 700, loss is 5.49707181930542 and perplexity is 243.97647807282675
At time: 788.783605337143 and batch: 750, loss is 5.498081293106079 and perplexity is 244.22289028783067
At time: 790.3544824123383 and batch: 800, loss is 5.504644460678101 and perplexity is 245.83103753906136
At time: 791.9283530712128 and batch: 850, loss is 5.538971681594848 and perplexity is 254.4162439913723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.292436281840007 and perplexity of 198.82723500375215
Finished 27 epochs...
Completing Train Step...
At time: 796.3040747642517 and batch: 50, loss is 5.559725999832153 and perplexity is 259.7516545745388
At time: 797.9116921424866 and batch: 100, loss is 5.496357469558716 and perplexity is 243.8022557727504
At time: 799.518586397171 and batch: 150, loss is 5.484367446899414 and perplexity is 240.8965159556064
At time: 801.1247835159302 and batch: 200, loss is 5.517819118499756 and perplexity is 249.09120598652868
At time: 802.7324163913727 and batch: 250, loss is 5.536766986846924 and perplexity is 253.85595169814616
At time: 804.3379893302917 and batch: 300, loss is 5.491850080490113 and perplexity is 242.70581704446957
At time: 805.9439301490784 and batch: 350, loss is 5.483116111755371 and perplexity is 240.59526220312708
At time: 807.5497941970825 and batch: 400, loss is 5.499888486862183 and perplexity is 244.6646474203926
At time: 809.1554708480835 and batch: 450, loss is 5.519276714324951 and perplexity is 249.45454502487135
At time: 810.7627968788147 and batch: 500, loss is 5.533192873001099 and perplexity is 252.95026111109945
At time: 812.3697769641876 and batch: 550, loss is 5.514172668457031 and perplexity is 248.18456136957508
At time: 813.9753925800323 and batch: 600, loss is 5.511389408111572 and perplexity is 247.49475951593072
At time: 815.648918390274 and batch: 650, loss is 5.530498905181885 and perplexity is 252.26973831263638
At time: 817.2600769996643 and batch: 700, loss is 5.496947355270386 and perplexity is 243.94611366558996
At time: 818.8570947647095 and batch: 750, loss is 5.497807016372681 and perplexity is 244.15591481658808
At time: 820.4318261146545 and batch: 800, loss is 5.504301271438599 and perplexity is 245.74668544743525
At time: 822.0024826526642 and batch: 850, loss is 5.538382844924927 and perplexity is 254.26647847552766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.292009671529134 and perplexity of 198.74243134556082
Finished 28 epochs...
Completing Train Step...
At time: 826.0922682285309 and batch: 50, loss is 5.558247051239014 and perplexity is 259.367779166408
At time: 827.665540933609 and batch: 100, loss is 5.494946918487549 and perplexity is 243.45860266682473
At time: 829.2376599311829 and batch: 150, loss is 5.48297604560852 and perplexity is 240.5615653117517
At time: 830.8072402477264 and batch: 200, loss is 5.516603727340698 and perplexity is 248.78864663819647
At time: 832.3778624534607 and batch: 250, loss is 5.53546064376831 and perplexity is 253.52454524507303
At time: 833.9476406574249 and batch: 300, loss is 5.490614032745361 and perplexity is 242.40600639498345
At time: 835.519049167633 and batch: 350, loss is 5.482005481719971 and perplexity is 240.3281982106469
At time: 837.0940821170807 and batch: 400, loss is 5.499060010910034 and perplexity is 244.46203258601182
At time: 838.6632652282715 and batch: 450, loss is 5.518641138076783 and perplexity is 249.2960480148589
At time: 840.2328119277954 and batch: 500, loss is 5.532767705917358 and perplexity is 252.8427378455478
At time: 841.8025703430176 and batch: 550, loss is 5.513871049880981 and perplexity is 248.10971558358423
At time: 843.3731381893158 and batch: 600, loss is 5.511183547973633 and perplexity is 247.44381545445307
At time: 844.9450685977936 and batch: 650, loss is 5.5304122734069825 and perplexity is 252.24788468407525
At time: 846.5229647159576 and batch: 700, loss is 5.496790904998779 and perplexity is 243.90795121519017
At time: 848.0925807952881 and batch: 750, loss is 5.497472267150879 and perplexity is 244.0741974922734
At time: 849.6675963401794 and batch: 800, loss is 5.503938436508179 and perplexity is 245.6575361401394
At time: 851.2513947486877 and batch: 850, loss is 5.53781343460083 and perplexity is 254.1217377299601
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.291623433430989 and perplexity of 198.6656842691338
Finished 29 epochs...
Completing Train Step...
At time: 855.3439025878906 and batch: 50, loss is 5.556973991394043 and perplexity is 259.037798548775
At time: 856.9495396614075 and batch: 100, loss is 5.493629341125488 and perplexity is 243.13803835386818
At time: 858.5248756408691 and batch: 150, loss is 5.481723442077636 and perplexity is 240.2604256892988
At time: 860.0972757339478 and batch: 200, loss is 5.515479106903076 and perplexity is 248.50901111292515
At time: 861.6685161590576 and batch: 250, loss is 5.534228658676147 and perplexity is 253.2123991040108
At time: 863.2403216362 and batch: 300, loss is 5.489542007446289 and perplexity is 242.14628026484613
At time: 864.8115565776825 and batch: 350, loss is 5.481031379699707 and perplexity is 240.094208010909
At time: 866.3873898983002 and batch: 400, loss is 5.498347635269165 and perplexity is 244.28794580383394
At time: 867.9589278697968 and batch: 450, loss is 5.518055725097656 and perplexity is 249.15014958229082
At time: 869.5303792953491 and batch: 500, loss is 5.532391195297241 and perplexity is 252.74755778880498
At time: 871.1021862030029 and batch: 550, loss is 5.513577470779419 and perplexity is 248.0368864472736
At time: 872.6744048595428 and batch: 600, loss is 5.510962190628052 and perplexity is 247.38904801009545
At time: 874.2454662322998 and batch: 650, loss is 5.530325222015381 and perplexity is 252.22592711041742
At time: 875.818957567215 and batch: 700, loss is 5.496639413833618 and perplexity is 243.87100411411896
At time: 877.3902878761292 and batch: 750, loss is 5.497153205871582 and perplexity is 243.99633528864572
At time: 878.9619421958923 and batch: 800, loss is 5.503609228134155 and perplexity is 245.57667693259447
At time: 880.5334951877594 and batch: 850, loss is 5.537336463928223 and perplexity is 254.00055801567325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.291263898213704 and perplexity of 198.594269797951
Finished 30 epochs...
Completing Train Step...
At time: 884.6082317829132 and batch: 50, loss is 5.555784711837768 and perplexity is 258.72991330775915
At time: 886.2045760154724 and batch: 100, loss is 5.492468233108521 and perplexity is 242.85589266081723
At time: 887.7773933410645 and batch: 150, loss is 5.480617837905884 and perplexity is 239.99493954868532
At time: 889.3689594268799 and batch: 200, loss is 5.514488401412964 and perplexity is 248.26293378647955
At time: 890.9780139923096 and batch: 250, loss is 5.533146476745605 and perplexity is 252.93852543840552
At time: 892.5851092338562 and batch: 300, loss is 5.488580827713013 and perplexity is 241.91364598735393
At time: 894.1859374046326 and batch: 350, loss is 5.480194959640503 and perplexity is 239.89347236059513
At time: 895.7856931686401 and batch: 400, loss is 5.4976828670501705 and perplexity is 244.125604906693
At time: 897.3565816879272 and batch: 450, loss is 5.517455015182495 and perplexity is 249.000527561288
At time: 898.9268503189087 and batch: 500, loss is 5.531984596252442 and perplexity is 252.64481176286617
At time: 900.4981412887573 and batch: 550, loss is 5.513273658752442 and perplexity is 247.9615413039969
At time: 902.0691976547241 and batch: 600, loss is 5.51072054862976 and perplexity is 247.32927564822612
At time: 903.6409003734589 and batch: 650, loss is 5.530220079421997 and perplexity is 252.1994088164479
At time: 905.2151582241058 and batch: 700, loss is 5.496450080871582 and perplexity is 243.82483566529774
At time: 906.7884240150452 and batch: 750, loss is 5.496812734603882 and perplexity is 243.91327568756205
At time: 908.3601319789886 and batch: 800, loss is 5.503247947692871 and perplexity is 245.487970907224
At time: 909.9314749240875 and batch: 850, loss is 5.536905565261841 and perplexity is 253.89113309118414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.290899594624837 and perplexity of 198.52193436956324
Finished 31 epochs...
Completing Train Step...
At time: 914.0076978206635 and batch: 50, loss is 5.554669570922852 and perplexity is 258.44155380613824
At time: 915.6390202045441 and batch: 100, loss is 5.491399183273315 and perplexity is 242.5964063354094
At time: 917.247857093811 and batch: 150, loss is 5.479621114730835 and perplexity is 239.75585020328225
At time: 918.8361687660217 and batch: 200, loss is 5.513595046997071 and perplexity is 248.04124603588795
At time: 920.4127509593964 and batch: 250, loss is 5.532159452438354 and perplexity is 252.68899213353464
At time: 921.9892120361328 and batch: 300, loss is 5.487661018371582 and perplexity is 241.69123386001232
At time: 923.5585224628448 and batch: 350, loss is 5.479413290023803 and perplexity is 239.7060281912564
At time: 925.1311712265015 and batch: 400, loss is 5.497043933868408 and perplexity is 243.96967477697194
At time: 926.7022800445557 and batch: 450, loss is 5.516890048980713 and perplexity is 248.85989041034918
At time: 928.2733500003815 and batch: 500, loss is 5.531560926437378 and perplexity is 252.53779645336877
At time: 929.8444068431854 and batch: 550, loss is 5.512922945022583 and perplexity is 247.87459303485184
At time: 931.4147171974182 and batch: 600, loss is 5.51048810005188 and perplexity is 247.27179099120576
At time: 932.9848349094391 and batch: 650, loss is 5.530063486099243 and perplexity is 252.1599191650133
At time: 934.5548603534698 and batch: 700, loss is 5.496255407333374 and perplexity is 243.7773740417468
At time: 936.185468673706 and batch: 750, loss is 5.496442003250122 and perplexity is 243.82286614852728
At time: 937.7596805095673 and batch: 800, loss is 5.502841033935547 and perplexity is 245.38809879565045
At time: 939.3333041667938 and batch: 850, loss is 5.536524667739868 and perplexity is 253.79444500303438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.290458679199219 and perplexity of 198.43442228050887
Finished 32 epochs...
Completing Train Step...
At time: 943.543208360672 and batch: 50, loss is 5.553524322509766 and perplexity is 258.14574344727026
At time: 945.1134493350983 and batch: 100, loss is 5.49037052154541 and perplexity is 242.34698500396738
At time: 946.6842324733734 and batch: 150, loss is 5.478633584976197 and perplexity is 239.51920103563702
At time: 948.2600643634796 and batch: 200, loss is 5.512667150497436 and perplexity is 247.81119617966036
At time: 949.8351519107819 and batch: 250, loss is 5.531097440719605 and perplexity is 252.42077591228426
At time: 951.4097793102264 and batch: 300, loss is 5.486497383117676 and perplexity is 241.41015698692811
At time: 952.9796078205109 and batch: 350, loss is 5.478282327651978 and perplexity is 239.4350829363963
At time: 954.549329996109 and batch: 400, loss is 5.49549750328064 and perplexity is 243.59268417943403
At time: 956.1219623088837 and batch: 450, loss is 5.515105276107788 and perplexity is 248.4161281540016
At time: 957.7062120437622 and batch: 500, loss is 5.529067153930664 and perplexity is 251.9088092411431
At time: 959.2778677940369 and batch: 550, loss is 5.510150918960571 and perplexity is 247.1884296735892
At time: 960.8477380275726 and batch: 600, loss is 5.508423862457275 and perplexity is 246.76188972389983
At time: 962.4217736721039 and batch: 650, loss is 5.528057756423951 and perplexity is 251.65466140684995
At time: 964.0000903606415 and batch: 700, loss is 5.494371213912964 and perplexity is 243.31848277324931
At time: 965.5759797096252 and batch: 750, loss is 5.494173059463501 and perplexity is 243.2702729099084
At time: 967.1481428146362 and batch: 800, loss is 5.5005183506011965 and perplexity is 244.818801352911
At time: 968.7208671569824 and batch: 850, loss is 5.5345862674713135 and perplexity is 253.30296627781937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.28801695505778 and perplexity of 197.95049121465104
Finished 33 epochs...
Completing Train Step...
At time: 972.8342990875244 and batch: 50, loss is 5.54996961593628 and perplexity is 257.22974010222345
At time: 974.4114079475403 and batch: 100, loss is 5.487635688781738 and perplexity is 241.68511199772234
At time: 976.0164678096771 and batch: 150, loss is 5.475427122116089 and perplexity is 238.7524215950965
At time: 977.5966930389404 and batch: 200, loss is 5.510318984985352 and perplexity is 247.22997714159732
At time: 979.1716039180756 and batch: 250, loss is 5.5283252716064455 and perplexity is 251.72199185507895
At time: 980.7493789196014 and batch: 300, loss is 5.484170036315918 and perplexity is 240.84896512750223
At time: 982.3230962753296 and batch: 350, loss is 5.476572570800781 and perplexity is 239.02605693007828
At time: 983.8997313976288 and batch: 400, loss is 5.493857936859131 and perplexity is 243.19362502531806
At time: 985.471408367157 and batch: 450, loss is 5.514248819351196 and perplexity is 248.20346156546552
At time: 987.044016122818 and batch: 500, loss is 5.528358221054077 and perplexity is 251.73028609231181
At time: 988.6233458518982 and batch: 550, loss is 5.509163103103638 and perplexity is 246.94437358419015
At time: 990.2027776241302 and batch: 600, loss is 5.507886343002319 and perplexity is 246.62928604915174
At time: 991.7790384292603 and batch: 650, loss is 5.5276796245574955 and perplexity is 251.55952074902095
At time: 993.3572728633881 and batch: 700, loss is 5.493842010498047 and perplexity is 243.18975186667527
At time: 994.944892168045 and batch: 750, loss is 5.493554439544678 and perplexity is 243.11982761245545
At time: 996.5333616733551 and batch: 800, loss is 5.499920406341553 and perplexity is 244.6724571131985
At time: 998.1173496246338 and batch: 850, loss is 5.533984375 and perplexity is 253.15055100286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.287501335144043 and perplexity of 197.84845030886896
Finished 34 epochs...
Completing Train Step...
At time: 1002.2496542930603 and batch: 50, loss is 5.548825235366821 and perplexity is 256.9355397564504
At time: 1003.8257093429565 and batch: 100, loss is 5.486664094924927 and perplexity is 241.4504062654098
At time: 1005.4005610942841 and batch: 150, loss is 5.4744604969024655 and perplexity is 238.52174898950844
At time: 1006.9748523235321 and batch: 200, loss is 5.509508075714112 and perplexity is 247.02957732502082
At time: 1008.5592708587646 and batch: 250, loss is 5.527356328964234 and perplexity is 251.47820580960882
At time: 1010.136705160141 and batch: 300, loss is 5.4832974052429195 and perplexity is 240.63888451140025
At time: 1011.7099823951721 and batch: 350, loss is 5.475674695968628 and perplexity is 238.8115377694215
At time: 1013.2839658260345 and batch: 400, loss is 5.493071756362915 and perplexity is 243.00250607735765
At time: 1014.8661608695984 and batch: 450, loss is 5.5135840320587155 and perplexity is 248.03851389190052
At time: 1016.4659841060638 and batch: 500, loss is 5.52771637916565 and perplexity is 251.5687668905517
At time: 1018.0401465892792 and batch: 550, loss is 5.508578777313232 and perplexity is 246.80011976762657
At time: 1019.6151015758514 and batch: 600, loss is 5.507495765686035 and perplexity is 246.53297705376843
At time: 1021.1869344711304 and batch: 650, loss is 5.527398843765258 and perplexity is 251.48889758276852
At time: 1022.7642741203308 and batch: 700, loss is 5.493509273529053 and perplexity is 243.10884710649762
At time: 1024.3378307819366 and batch: 750, loss is 5.493067941665649 and perplexity is 243.00157909813026
At time: 1025.9215667247772 and batch: 800, loss is 5.499423637390136 and perplexity is 244.55094161829936
At time: 1027.4915511608124 and batch: 850, loss is 5.5334258460998536 and perplexity is 253.00919858241747
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.2870527903238935 and perplexity of 197.75972631114067
Finished 35 epochs...
Completing Train Step...
At time: 1031.5460658073425 and batch: 50, loss is 5.547811489105225 and perplexity is 256.6752042928978
At time: 1033.1424491405487 and batch: 100, loss is 5.485766201019287 and perplexity is 241.23370671827013
At time: 1034.7116088867188 and batch: 150, loss is 5.473701486587524 and perplexity is 238.3407772100993
At time: 1036.283254623413 and batch: 200, loss is 5.508750925064087 and perplexity is 246.84260951030703
At time: 1037.8541243076324 and batch: 250, loss is 5.526470642089844 and perplexity is 251.25557346935247
At time: 1039.4264252185822 and batch: 300, loss is 5.482489805221558 and perplexity is 240.44462299648865
At time: 1041.0009942054749 and batch: 350, loss is 5.474817142486573 and perplexity is 238.60683188931745
At time: 1042.5760440826416 and batch: 400, loss is 5.492355995178222 and perplexity is 242.82863654767772
At time: 1044.1457426548004 and batch: 450, loss is 5.512934455871582 and perplexity is 247.87744629828453
At time: 1045.7149868011475 and batch: 500, loss is 5.527086706161499 and perplexity is 251.41041069089732
At time: 1047.293076992035 and batch: 550, loss is 5.508113145828247 and perplexity is 246.68522861191016
At time: 1048.8675005435944 and batch: 600, loss is 5.507103395462036 and perplexity is 246.43626382927434
At time: 1050.4388546943665 and batch: 650, loss is 5.527062540054321 and perplexity is 251.40433515337836
At time: 1052.0094754695892 and batch: 700, loss is 5.493185043334961 and perplexity is 243.03003665486918
At time: 1053.5835082530975 and batch: 750, loss is 5.492497653961181 and perplexity is 242.86303779336043
At time: 1055.198250055313 and batch: 800, loss is 5.498889179229736 and perplexity is 244.42027429313737
At time: 1056.7682802677155 and batch: 850, loss is 5.532852773666382 and perplexity is 252.86424752299024
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.286570866902669 and perplexity of 197.66444422843412
Finished 36 epochs...
Completing Train Step...
At time: 1060.8462393283844 and batch: 50, loss is 5.546746158599854 and perplexity is 256.40190597016016
At time: 1062.4446232318878 and batch: 100, loss is 5.484791593551636 and perplexity is 240.99871307813243
At time: 1064.0164992809296 and batch: 150, loss is 5.472946186065673 and perplexity is 238.1608262637709
At time: 1065.5888803005219 and batch: 200, loss is 5.507981958389283 and perplexity is 246.65286873118956
At time: 1067.1640255451202 and batch: 250, loss is 5.525531911849976 and perplexity is 251.01982293499142
At time: 1068.7417335510254 and batch: 300, loss is 5.481609153747558 and perplexity is 240.2329682955227
At time: 1070.313553571701 and batch: 350, loss is 5.473927869796753 and perplexity is 238.39473966800088
At time: 1071.8849325180054 and batch: 400, loss is 5.491617708206177 and perplexity is 242.64942549160918
At time: 1073.4565331935883 and batch: 450, loss is 5.512263641357422 and perplexity is 247.71122226855397
At time: 1075.028277873993 and batch: 500, loss is 5.526433048248291 and perplexity is 251.24612798468118
At time: 1076.6045894622803 and batch: 550, loss is 5.507673015594483 and perplexity is 246.57667887433797
At time: 1078.1783480644226 and batch: 600, loss is 5.506699752807617 and perplexity is 246.3368117145053
At time: 1079.750596523285 and batch: 650, loss is 5.5266788291931155 and perplexity is 251.3078870846949
At time: 1081.3285760879517 and batch: 700, loss is 5.492797107696533 and perplexity is 242.93577492736617
At time: 1082.9048998355865 and batch: 750, loss is 5.491956338882447 and perplexity is 242.73160794462433
At time: 1084.476170539856 and batch: 800, loss is 5.498354444503784 and perplexity is 244.28960922343475
At time: 1086.0477106571198 and batch: 850, loss is 5.5323223304748534 and perplexity is 252.7301529724256
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.286170959472656 and perplexity of 197.58541255226837
Finished 37 epochs...
Completing Train Step...
At time: 1090.1110413074493 and batch: 50, loss is 5.545797052383423 and perplexity is 256.1586687745255
At time: 1091.7076697349548 and batch: 100, loss is 5.483832845687866 and perplexity is 240.76776680411135
At time: 1093.2780039310455 and batch: 150, loss is 5.4722559452056885 and perplexity is 237.99649465095715
At time: 1094.8752148151398 and batch: 200, loss is 5.507183294296265 and perplexity is 246.45595458608977
At time: 1096.4516758918762 and batch: 250, loss is 5.524615726470947 and perplexity is 250.78994756367882
At time: 1098.0327787399292 and batch: 300, loss is 5.480777187347412 and perplexity is 240.03318565543893
At time: 1099.6102228164673 and batch: 350, loss is 5.473096084594727 and perplexity is 238.1965288971252
At time: 1101.181424856186 and batch: 400, loss is 5.490945615768433 and perplexity is 242.4863974388107
At time: 1102.7529203891754 and batch: 450, loss is 5.511634969711304 and perplexity is 247.55554218764226
At time: 1104.3244805335999 and batch: 500, loss is 5.525855913162231 and perplexity is 251.10116686408273
At time: 1105.8968245983124 and batch: 550, loss is 5.507237634658813 and perplexity is 246.4693474558976
At time: 1107.4680886268616 and batch: 600, loss is 5.506264381408691 and perplexity is 246.22958705515077
At time: 1109.0401566028595 and batch: 650, loss is 5.526319732666016 and perplexity is 251.21765949638643
At time: 1110.6124184131622 and batch: 700, loss is 5.492352876663208 and perplexity is 242.8278792841095
At time: 1112.182776927948 and batch: 750, loss is 5.491461744308472 and perplexity is 242.61158389247194
At time: 1113.7535936832428 and batch: 800, loss is 5.49782826423645 and perplexity is 244.1611026633196
At time: 1115.3248319625854 and batch: 850, loss is 5.531835384368897 and perplexity is 252.6071169669609
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.285781542460124 and perplexity of 197.508484410728
Finished 38 epochs...
Completing Train Step...
At time: 1119.44562458992 and batch: 50, loss is 5.544939308166504 and perplexity is 255.93904436202058
At time: 1121.0165417194366 and batch: 100, loss is 5.482920866012574 and perplexity is 240.54829158800024
At time: 1122.5874848365784 and batch: 150, loss is 5.471557903289795 and perplexity is 237.8304210917512
At time: 1124.1573071479797 and batch: 200, loss is 5.506322364807129 and perplexity is 246.24386469733338
At time: 1125.7282693386078 and batch: 250, loss is 5.523712854385376 and perplexity is 250.56361850914942
At time: 1127.3002746105194 and batch: 300, loss is 5.4799463748931885 and perplexity is 239.8338459138056
At time: 1128.882758140564 and batch: 350, loss is 5.472233543395996 and perplexity is 237.99116315849423
At time: 1130.4571206569672 and batch: 400, loss is 5.490242414474487 and perplexity is 242.31594063011588
At time: 1132.0279121398926 and batch: 450, loss is 5.510976819992066 and perplexity is 247.39266718100475
At time: 1133.5974555015564 and batch: 500, loss is 5.525319490432739 and perplexity is 250.9665066114141
At time: 1135.1942188739777 and batch: 550, loss is 5.506777744293213 and perplexity is 246.35602463762035
At time: 1136.764440536499 and batch: 600, loss is 5.505834188461304 and perplexity is 246.12368360445356
At time: 1138.3450572490692 and batch: 650, loss is 5.5259273242950435 and perplexity is 251.1190989231257
At time: 1139.9243154525757 and batch: 700, loss is 5.49186336517334 and perplexity is 242.7090413357832
At time: 1141.494285583496 and batch: 750, loss is 5.490859718322754 and perplexity is 242.46556937121053
At time: 1143.0644211769104 and batch: 800, loss is 5.497255744934082 and perplexity is 244.021355726884
At time: 1144.635421514511 and batch: 850, loss is 5.531217880249024 and perplexity is 252.45117918259174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.285322189331055 and perplexity of 197.41777910487414
Finished 39 epochs...
Completing Train Step...
At time: 1148.722981929779 and batch: 50, loss is 5.5440611743927 and perplexity is 255.71439429397324
At time: 1150.293446779251 and batch: 100, loss is 5.4820044803619385 and perplexity is 240.32795755619577
At time: 1151.8684248924255 and batch: 150, loss is 5.470625114440918 and perplexity is 237.608678962404
At time: 1153.4389479160309 and batch: 200, loss is 5.505457983016968 and perplexity is 246.03110794952622
At time: 1155.0146701335907 and batch: 250, loss is 5.522675743103028 and perplexity is 250.30389085996532
At time: 1156.597728252411 and batch: 300, loss is 5.479106864929199 and perplexity is 239.63258750150052
At time: 1158.1757950782776 and batch: 350, loss is 5.4714282131195064 and perplexity is 237.79957882395277
At time: 1159.7462661266327 and batch: 400, loss is 5.489482717514038 and perplexity is 242.13192385389394
At time: 1161.338190317154 and batch: 450, loss is 5.510137014389038 and perplexity is 247.18499264828185
At time: 1162.9495191574097 and batch: 500, loss is 5.524615116119385 and perplexity is 250.78979449368924
At time: 1164.556495666504 and batch: 550, loss is 5.506004266738891 and perplexity is 246.16554745659957
At time: 1166.1681282520294 and batch: 600, loss is 5.5052866363525395 and perplexity is 245.98895495132433
At time: 1167.7735424041748 and batch: 650, loss is 5.525337715148925 and perplexity is 250.9710804464476
At time: 1169.3822469711304 and batch: 700, loss is 5.491177825927735 and perplexity is 242.5427117819168
At time: 1170.9902112483978 and batch: 750, loss is 5.490292282104492 and perplexity is 242.3280246530858
At time: 1172.595687866211 and batch: 800, loss is 5.49662525177002 and perplexity is 243.86755042190467
At time: 1174.199448108673 and batch: 850, loss is 5.5305823802948 and perplexity is 252.29079743647128
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.284867922465007 and perplexity of 197.3281191153811
Finished 40 epochs...
Completing Train Step...
At time: 1178.3470134735107 and batch: 50, loss is 5.543139362335205 and perplexity is 255.47878229371344
At time: 1179.918538570404 and batch: 100, loss is 5.48106499671936 and perplexity is 240.1022793982856
At time: 1181.489753961563 and batch: 150, loss is 5.469799947738648 and perplexity is 237.41269306399306
At time: 1183.0603692531586 and batch: 200, loss is 5.50459397315979 and perplexity is 245.8186264533573
At time: 1184.6366057395935 and batch: 250, loss is 5.521835994720459 and perplexity is 250.0937868020832
At time: 1186.2080523967743 and batch: 300, loss is 5.4783628559112545 and perplexity is 239.4543650032002
At time: 1187.781728029251 and batch: 350, loss is 5.470561323165893 and perplexity is 237.5935220852594
At time: 1189.354008436203 and batch: 400, loss is 5.48881646156311 and perplexity is 241.97065574759998
At time: 1190.9243214130402 and batch: 450, loss is 5.509558305740357 and perplexity is 247.04198593881304
At time: 1192.4971356391907 and batch: 500, loss is 5.524133911132813 and perplexity is 250.66914222556213
At time: 1194.0694799423218 and batch: 550, loss is 5.505593805313111 and perplexity is 246.06452672898672
At time: 1195.6408910751343 and batch: 600, loss is 5.504914617538452 and perplexity is 245.89745945210348
At time: 1197.2135863304138 and batch: 650, loss is 5.525032119750977 and perplexity is 250.89439655696356
At time: 1198.7899222373962 and batch: 700, loss is 5.490763635635376 and perplexity is 242.44227374687975
At time: 1200.3605563640594 and batch: 750, loss is 5.489840497970581 and perplexity is 242.21856942327545
At time: 1201.931256055832 and batch: 800, loss is 5.4961254596710205 and perplexity is 243.74569780002682
At time: 1203.5029141902924 and batch: 850, loss is 5.53008773803711 and perplexity is 252.16603460586074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.284481684366862 and perplexity of 197.25191819474017
Finished 41 epochs...
Completing Train Step...
At time: 1207.558973789215 and batch: 50, loss is 5.542335262298584 and perplexity is 255.27343436671734
At time: 1209.1629631519318 and batch: 100, loss is 5.480242652893066 and perplexity is 239.90491393340145
At time: 1210.7399468421936 and batch: 150, loss is 5.469009456634521 and perplexity is 237.2250945993468
At time: 1212.312983751297 and batch: 200, loss is 5.503782815933228 and perplexity is 245.619309747608
At time: 1213.8834793567657 and batch: 250, loss is 5.521094255447387 and perplexity is 249.9083511993973
At time: 1215.4865181446075 and batch: 300, loss is 5.477677822113037 and perplexity is 239.29038684174623
At time: 1217.0575158596039 and batch: 350, loss is 5.469891052246094 and perplexity is 237.43432341575254
At time: 1218.627841949463 and batch: 400, loss is 5.488228855133056 and perplexity is 241.82851400019055
At time: 1220.196744441986 and batch: 450, loss is 5.508963365554809 and perplexity is 246.8950544459193
At time: 1221.7670052051544 and batch: 500, loss is 5.523695821762085 and perplexity is 250.55935078977018
At time: 1223.3373618125916 and batch: 550, loss is 5.505189714431762 and perplexity is 245.9651143846759
At time: 1224.9078891277313 and batch: 600, loss is 5.5045372581481935 and perplexity is 245.80468524244907
At time: 1226.484314918518 and batch: 650, loss is 5.524686193466186 and perplexity is 250.80762060039535
At time: 1228.056666135788 and batch: 700, loss is 5.490381526947021 and perplexity is 242.34965214454311
At time: 1229.6259095668793 and batch: 750, loss is 5.48940505027771 and perplexity is 242.11311886681645
At time: 1231.1963028907776 and batch: 800, loss is 5.495619163513184 and perplexity is 243.62232152484398
At time: 1232.7666306495667 and batch: 850, loss is 5.529583864212036 and perplexity is 252.0390067471443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.284146308898926 and perplexity of 197.1857758322571
Finished 42 epochs...
Completing Train Step...
At time: 1236.8632714748383 and batch: 50, loss is 5.541460676193237 and perplexity is 255.05027336893465
At time: 1238.4596366882324 and batch: 100, loss is 5.479428243637085 and perplexity is 239.7096126893038
At time: 1240.0307121276855 and batch: 150, loss is 5.468205509185791 and perplexity is 237.03445473225423
At time: 1241.609130859375 and batch: 200, loss is 5.502994794845581 and perplexity is 245.42583279396533
At time: 1243.1868946552277 and batch: 250, loss is 5.520367422103882 and perplexity is 249.72677547256507
At time: 1244.7570881843567 and batch: 300, loss is 5.476998882293701 and perplexity is 239.12797820881767
At time: 1246.3262791633606 and batch: 350, loss is 5.4691622447967525 and perplexity is 237.26134255464453
At time: 1247.8979225158691 and batch: 400, loss is 5.487547245025635 and perplexity is 241.66373740386155
At time: 1249.466697692871 and batch: 450, loss is 5.508313694000244 and perplexity is 246.73470584468544
At time: 1251.0369925498962 and batch: 500, loss is 5.523185920715332 and perplexity is 250.4316228815932
At time: 1252.6040799617767 and batch: 550, loss is 5.5046905994415285 and perplexity is 245.84238014081032
At time: 1254.1971595287323 and batch: 600, loss is 5.5039973545074465 and perplexity is 245.67201021706148
At time: 1255.7689456939697 and batch: 650, loss is 5.524213237762451 and perplexity is 250.68902775248472
At time: 1257.3554739952087 and batch: 700, loss is 5.489976577758789 and perplexity is 242.2515327176707
At time: 1258.9641053676605 and batch: 750, loss is 5.488940305709839 and perplexity is 242.0006242526681
At time: 1260.5699954032898 and batch: 800, loss is 5.494976825714112 and perplexity is 243.4658839472943
At time: 1262.177078485489 and batch: 850, loss is 5.528968620300293 and perplexity is 251.8839889744816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.2836354573567705 and perplexity of 197.08506889991807
Finished 43 epochs...
Completing Train Step...
At time: 1266.5347287654877 and batch: 50, loss is 5.5404212760925295 and perplexity is 254.7853118134975
At time: 1268.178899526596 and batch: 100, loss is 5.478436546325684 and perplexity is 239.4720111447619
At time: 1269.7874290943146 and batch: 150, loss is 5.467352132797242 and perplexity is 236.83226141108688
At time: 1271.3956143856049 and batch: 200, loss is 5.502141723632812 and perplexity is 245.21655635768215
At time: 1273.0030238628387 and batch: 250, loss is 5.519593954086304 and perplexity is 249.53369447921665
At time: 1274.605239391327 and batch: 300, loss is 5.476255111694336 and perplexity is 238.95018797491068
At time: 1276.1783487796783 and batch: 350, loss is 5.468341922760009 and perplexity is 237.06679165498474
At time: 1277.7493493556976 and batch: 400, loss is 5.4869539833068846 and perplexity is 241.52041007916105
At time: 1279.320629119873 and batch: 450, loss is 5.5076744556427 and perplexity is 246.57703395690052
At time: 1280.8973705768585 and batch: 500, loss is 5.522721519470215 and perplexity is 250.31534912503867
At time: 1282.5053215026855 and batch: 550, loss is 5.50420129776001 and perplexity is 245.72211847533578
At time: 1284.1136944293976 and batch: 600, loss is 5.503521680831909 and perplexity is 245.55517829812194
At time: 1285.721890449524 and batch: 650, loss is 5.523826246261597 and perplexity is 250.5920319988637
At time: 1287.3228495121002 and batch: 700, loss is 5.489594974517822 and perplexity is 242.1591063838724
At time: 1288.9096024036407 and batch: 750, loss is 5.488512630462647 and perplexity is 241.89714870441264
At time: 1290.4795761108398 and batch: 800, loss is 5.4944408416748045 and perplexity is 243.33542508444003
At time: 1292.0496644973755 and batch: 850, loss is 5.528371686935425 and perplexity is 251.73367588529922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.283297856648763 and perplexity of 197.01854407116616
Finished 44 epochs...
Completing Train Step...
At time: 1296.1411912441254 and batch: 50, loss is 5.539583206176758 and perplexity is 254.57187335935228
At time: 1297.710488319397 and batch: 100, loss is 5.4776614475250245 and perplexity is 239.28646859232626
At time: 1299.2798900604248 and batch: 150, loss is 5.466640176773072 and perplexity is 236.66370726454724
At time: 1300.8496453762054 and batch: 200, loss is 5.50143458366394 and perplexity is 245.04321522509719
At time: 1302.4242589473724 and batch: 250, loss is 5.518928346633911 and perplexity is 249.3676582561518
At time: 1303.9938659667969 and batch: 300, loss is 5.475581817626953 and perplexity is 238.78935837982894
At time: 1305.561285495758 and batch: 350, loss is 5.467675094604492 and perplexity is 236.90876153889224
At time: 1307.1303927898407 and batch: 400, loss is 5.486360445022583 and perplexity is 241.37710100325745
At time: 1308.7007160186768 and batch: 450, loss is 5.507135791778564 and perplexity is 246.4442475858028
At time: 1310.2695333957672 and batch: 500, loss is 5.522297496795654 and perplexity is 250.20923224079155
At time: 1311.8388314247131 and batch: 550, loss is 5.503761472702027 and perplexity is 245.61406749383613
At time: 1313.4138824939728 and batch: 600, loss is 5.503062572479248 and perplexity is 245.4424677398852
At time: 1314.9855167865753 and batch: 650, loss is 5.523397397994995 and perplexity is 250.48458908031824
At time: 1316.5552008152008 and batch: 700, loss is 5.48918664932251 and perplexity is 242.0602469042432
At time: 1318.1238422393799 and batch: 750, loss is 5.4880695629119876 and perplexity is 241.7899956669946
At time: 1319.6930937767029 and batch: 800, loss is 5.493968067169189 and perplexity is 243.22040948950985
At time: 1321.262389421463 and batch: 850, loss is 5.527770328521728 and perplexity is 251.5823392296411
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.282944997151692 and perplexity of 196.94903647072135
Finished 45 epochs...
Completing Train Step...
At time: 1325.33305478096 and batch: 50, loss is 5.538700952529907 and perplexity is 254.34737544231024
At time: 1326.9014689922333 and batch: 100, loss is 5.476867847442627 and perplexity is 239.09664616264516
At time: 1328.4707624912262 and batch: 150, loss is 5.465890007019043 and perplexity is 236.4862358846345
At time: 1330.0405189990997 and batch: 200, loss is 5.500668115615845 and perplexity is 244.85546939000787
At time: 1331.6138689517975 and batch: 250, loss is 5.518236970901489 and perplexity is 249.19531109397056
At time: 1333.1952621936798 and batch: 300, loss is 5.4748746776580814 and perplexity is 238.62056056925059
At time: 1334.8248603343964 and batch: 350, loss is 5.467014760971069 and perplexity is 236.75237435513677
At time: 1336.3979089260101 and batch: 400, loss is 5.485766124725342 and perplexity is 241.2336883135996
At time: 1337.9659225940704 and batch: 450, loss is 5.506583633422852 and perplexity is 246.3082088961869
At time: 1339.5443506240845 and batch: 500, loss is 5.521840200424195 and perplexity is 250.0948386246684
At time: 1341.1178345680237 and batch: 550, loss is 5.503285341262817 and perplexity is 245.49715075046757
At time: 1342.6934151649475 and batch: 600, loss is 5.502534103393555 and perplexity is 245.31279325086572
At time: 1344.2642793655396 and batch: 650, loss is 5.522935848236084 and perplexity is 250.36900465465112
At time: 1345.838178396225 and batch: 700, loss is 5.488760890960694 and perplexity is 241.95720966609983
At time: 1347.408861875534 and batch: 750, loss is 5.487613115310669 and perplexity is 241.67965638741506
At time: 1348.985568523407 and batch: 800, loss is 5.493480615615844 and perplexity is 243.10188021408575
At time: 1350.5563204288483 and batch: 850, loss is 5.527171640396118 and perplexity is 251.4317649485457
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.282584508260091 and perplexity of 196.87805132630876
Finished 46 epochs...
Completing Train Step...
At time: 1354.65966963768 and batch: 50, loss is 5.537828559875488 and perplexity is 254.12558142010832
At time: 1356.2278079986572 and batch: 100, loss is 5.476039066314697 and perplexity is 238.89856946700576
At time: 1357.796685218811 and batch: 150, loss is 5.465152492523194 and perplexity is 236.3118881574915
At time: 1359.3655066490173 and batch: 200, loss is 5.49998664855957 and perplexity is 244.68866529627255
At time: 1360.933658361435 and batch: 250, loss is 5.517520198822021 and perplexity is 249.01675885091987
At time: 1362.5010733604431 and batch: 300, loss is 5.47425085067749 and perplexity is 238.471749046591
At time: 1364.0685880184174 and batch: 350, loss is 5.466346368789673 and perplexity is 236.594183791748
At time: 1365.649164199829 and batch: 400, loss is 5.48522518157959 and perplexity is 241.1032298918577
At time: 1367.2232069969177 and batch: 450, loss is 5.506121044158935 and perplexity is 246.19429571267622
At time: 1368.8059747219086 and batch: 500, loss is 5.521438083648682 and perplexity is 249.99429151178347
At time: 1370.3838753700256 and batch: 550, loss is 5.502903690338135 and perplexity is 245.4034744128443
At time: 1371.9624242782593 and batch: 600, loss is 5.50211106300354 and perplexity is 245.20903797901602
At time: 1373.5474045276642 and batch: 650, loss is 5.522601451873779 and perplexity is 250.28529616694755
At time: 1375.1513953208923 and batch: 700, loss is 5.488428220748902 and perplexity is 241.8767310970691
At time: 1376.722698688507 and batch: 750, loss is 5.487220964431763 and perplexity is 241.5849000783482
At time: 1378.2940692901611 and batch: 800, loss is 5.493017930984497 and perplexity is 242.98942672751508
At time: 1379.8707230091095 and batch: 850, loss is 5.526642436981201 and perplexity is 251.2987416012256
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.282315254211426 and perplexity of 196.82504824986256
Finished 47 epochs...
Completing Train Step...
At time: 1383.9338252544403 and batch: 50, loss is 5.537048959732056 and perplexity is 253.9275422860823
At time: 1385.5359230041504 and batch: 100, loss is 5.475291299819946 and perplexity is 238.71999589510224
At time: 1387.106966495514 and batch: 150, loss is 5.464455690383911 and perplexity is 236.14728288359905
At time: 1388.6770367622375 and batch: 200, loss is 5.499391326904297 and perplexity is 244.54304018621335
At time: 1390.246750831604 and batch: 250, loss is 5.516883478164673 and perplexity is 248.85825520316197
At time: 1391.8235712051392 and batch: 300, loss is 5.473686065673828 and perplexity is 238.33710180588278
At time: 1393.4093101024628 and batch: 350, loss is 5.465748157501221 and perplexity is 236.4526928051937
At time: 1394.993241071701 and batch: 400, loss is 5.484746789932251 and perplexity is 240.98791570540183
At time: 1396.5661036968231 and batch: 450, loss is 5.505729494094848 and perplexity is 246.0979171901517
At time: 1398.1349461078644 and batch: 500, loss is 5.521093406677246 and perplexity is 249.90813908474075
At time: 1399.7037177085876 and batch: 550, loss is 5.502541980743408 and perplexity is 245.31472567317283
At time: 1401.2721483707428 and batch: 600, loss is 5.5017626953125 and perplexity is 245.12362995019578
At time: 1402.8405210971832 and batch: 650, loss is 5.52232063293457 and perplexity is 250.21502118331333
At time: 1404.4092559814453 and batch: 700, loss is 5.488131313323975 and perplexity is 241.80492675983726
At time: 1405.97820353508 and batch: 750, loss is 5.486856746673584 and perplexity is 241.49692658935777
At time: 1407.5468385219574 and batch: 800, loss is 5.492587823867797 and perplexity is 242.88493771814097
At time: 1409.116085767746 and batch: 850, loss is 5.526147174835205 and perplexity is 251.17431366194145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.282075564066569 and perplexity of 196.77787687901835
Finished 48 epochs...
Completing Train Step...
At time: 1413.156084537506 and batch: 50, loss is 5.53630651473999 and perplexity is 253.73908502219712
At time: 1414.7496066093445 and batch: 100, loss is 5.47460018157959 and perplexity is 238.55506915010702
At time: 1416.3176083564758 and batch: 150, loss is 5.4637548542022705 and perplexity is 235.98184030440257
At time: 1417.8854882717133 and batch: 200, loss is 5.498807182312012 and perplexity is 244.40023340567276
At time: 1419.4528315067291 and batch: 250, loss is 5.516302280426025 and perplexity is 248.71366137086835
At time: 1421.0211689472198 and batch: 300, loss is 5.473151206970215 and perplexity is 238.20965921751494
At time: 1422.5884230136871 and batch: 350, loss is 5.465183744430542 and perplexity is 236.31927347012743
At time: 1424.1567952632904 and batch: 400, loss is 5.484304637908935 and perplexity is 240.88138596378408
At time: 1425.725201368332 and batch: 450, loss is 5.505362558364868 and perplexity is 246.00763163677163
At time: 1427.2932713031769 and batch: 500, loss is 5.520772771835327 and perplexity is 249.828022672814
At time: 1428.8639302253723 and batch: 550, loss is 5.502183980941773 and perplexity is 245.22691876840864
At time: 1430.4317548274994 and batch: 600, loss is 5.501443214416504 and perplexity is 245.04533014158199
At time: 1431.9997737407684 and batch: 650, loss is 5.522054662704468 and perplexity is 250.14848028589535
At time: 1433.5668814182281 and batch: 700, loss is 5.4878357410430905 and perplexity is 241.7334664874639
At time: 1435.1342856884003 and batch: 750, loss is 5.486509141921997 and perplexity is 241.412995698415
At time: 1436.703001499176 and batch: 800, loss is 5.492172880172729 and perplexity is 242.78417505147024
At time: 1438.2717702388763 and batch: 850, loss is 5.525669536590576 and perplexity is 251.0543718503466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.28184445699056 and perplexity of 196.73240537386513
Finished 49 epochs...
Completing Train Step...
At time: 1442.3417055606842 and batch: 50, loss is 5.5356041622161865 and perplexity is 253.56093330542166
At time: 1443.9096505641937 and batch: 100, loss is 5.473949422836304 and perplexity is 238.39987785462534
At time: 1445.4774749279022 and batch: 150, loss is 5.463070058822632 and perplexity is 235.82029634907363
At time: 1447.0460782051086 and batch: 200, loss is 5.498238973617553 and perplexity is 244.261402514319
At time: 1448.6139509677887 and batch: 250, loss is 5.515733880996704 and perplexity is 248.5723328370148
At time: 1450.1865842342377 and batch: 300, loss is 5.472623081207275 and perplexity is 238.08388777399418
At time: 1451.7534596920013 and batch: 350, loss is 5.464621658325195 and perplexity is 236.18647901453974
At time: 1453.3222053050995 and batch: 400, loss is 5.483874959945679 and perplexity is 240.77790677343265
At time: 1454.9152383804321 and batch: 450, loss is 5.505012950897217 and perplexity is 245.92164056408868
At time: 1456.4867134094238 and batch: 500, loss is 5.520464515686035 and perplexity is 249.7510235169009
At time: 1458.0549857616425 and batch: 550, loss is 5.501822719573974 and perplexity is 245.13834375664166
At time: 1459.6237344741821 and batch: 600, loss is 5.501132659912109 and perplexity is 244.96924202589057
At time: 1461.1923308372498 and batch: 650, loss is 5.521774864196777 and perplexity is 250.07849890520967
At time: 1462.7605829238892 and batch: 700, loss is 5.487550888061524 and perplexity is 241.6646177951336
At time: 1464.3281292915344 and batch: 750, loss is 5.486180992126465 and perplexity is 241.33378906976802
At time: 1465.895786523819 and batch: 800, loss is 5.4917611980438235 and perplexity is 242.68424571639244
At time: 1467.4640791416168 and batch: 850, loss is 5.5251984119415285 and perplexity is 250.9361218049595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.2815853754679365 and perplexity of 196.68144224481915
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd1269705c0>
SETTINGS FOR THIS RUN
{'seq_len': 50, 'lr': 9.914190006120485, 'dropout': 1.0, 'wordvec_source': 'glove', 'num_layers': 1, 'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 2.0, 'batch_size': 50, 'data': 'wikitext'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.0066978931427 and batch: 50, loss is 7.639612894058228 and perplexity is 2078.9388911883616
At time: 3.556009531021118 and batch: 100, loss is 6.907218523025513 and perplexity is 999.4633880710841
At time: 5.105907201766968 and batch: 150, loss is 6.888187961578369 and perplexity is 980.6228799803492
At time: 6.652472019195557 and batch: 200, loss is 6.923094329833984 and perplexity is 1015.4572979179975
At time: 8.203541994094849 and batch: 250, loss is 6.955300807952881 and perplexity is 1048.6939459903758
At time: 9.750069618225098 and batch: 300, loss is 6.933967504501343 and perplexity is 1026.5587873287786
At time: 11.2959725856781 and batch: 350, loss is 6.935042972564697 and perplexity is 1027.6634124077334
At time: 12.841874837875366 and batch: 400, loss is 6.94669771194458 and perplexity is 1039.7106288665227
At time: 14.422483682632446 and batch: 450, loss is 6.921726818084717 and perplexity is 1014.069597196996
At time: 15.975107908248901 and batch: 500, loss is 6.953776845932007 and perplexity is 1047.0969934017844
At time: 17.53231978416443 and batch: 550, loss is 6.93049165725708 and perplexity is 1022.9968198104219
At time: 19.089536666870117 and batch: 600, loss is 6.9730692386627195 and perplexity is 1067.4941218914196
At time: 20.64609122276306 and batch: 650, loss is 7.005335245132446 and perplexity is 1102.4996006871909
At time: 22.205836057662964 and batch: 700, loss is 6.959331445693969 and perplexity is 1052.929381406769
At time: 23.76272749900818 and batch: 750, loss is 6.947241287231446 and perplexity is 1040.2759435014789
At time: 25.31963586807251 and batch: 800, loss is 6.974331998825074 and perplexity is 1068.8429623938828
At time: 26.876160621643066 and batch: 850, loss is 6.983520212173462 and perplexity is 1078.7089756757578
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.6882063547770185 and perplexity of 802.8808764234522
Finished 1 epochs...
Completing Train Step...
At time: 30.94891595840454 and batch: 50, loss is 6.477785663604736 and perplexity is 650.5288604969434
At time: 32.512776613235474 and batch: 100, loss is 5.957789115905761 and perplexity is 386.7541098023739
At time: 34.08372211456299 and batch: 150, loss is 5.748475255966187 and perplexity is 313.7119649875847
At time: 35.680723905563354 and batch: 200, loss is 5.6422210216522215 and perplexity is 282.0885480005051
At time: 37.25174832344055 and batch: 250, loss is 5.641062889099121 and perplexity is 281.7620411757947
At time: 38.82452702522278 and batch: 300, loss is 5.556495943069458 and perplexity is 258.91399555743874
At time: 40.389320373535156 and batch: 350, loss is 5.450311203002929 and perplexity is 232.8306122200191
At time: 41.95359468460083 and batch: 400, loss is 5.405137281417847 and perplexity is 222.54676992294497
At time: 43.51845836639404 and batch: 450, loss is 5.33863299369812 and perplexity is 208.2278668358642
At time: 45.08294987678528 and batch: 500, loss is 5.340997695922852 and perplexity is 208.72084638101478
At time: 46.647358655929565 and batch: 550, loss is 5.302172527313233 and perplexity is 200.77252029121453
At time: 48.21242165565491 and batch: 600, loss is 5.314267044067383 and perplexity is 203.21551051531154
At time: 49.77778220176697 and batch: 650, loss is 5.287594518661499 and perplexity is 197.8668873823943
At time: 51.342886209487915 and batch: 700, loss is 5.223642206192016 and perplexity is 185.6089806291543
At time: 52.90803909301758 and batch: 750, loss is 5.204899959564209 and perplexity is 182.1626482264869
At time: 54.4749972820282 and batch: 800, loss is 5.157098369598389 and perplexity is 173.65982720246237
At time: 56.03969359397888 and batch: 850, loss is 5.146424169540405 and perplexity is 171.8160056318557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.926392555236816 and perplexity of 137.8812152228896
Finished 2 epochs...
Completing Train Step...
At time: 60.08413100242615 and batch: 50, loss is 5.1710966682434085 and perplexity is 176.1078635232964
At time: 61.64737272262573 and batch: 100, loss is 5.047965612411499 and perplexity is 155.7053769699322
At time: 63.21189546585083 and batch: 150, loss is 5.029557952880859 and perplexity is 152.86542403771745
At time: 64.77584028244019 and batch: 200, loss is 5.036619300842285 and perplexity is 153.94868011098797
At time: 66.37487936019897 and batch: 250, loss is 5.039731645584107 and perplexity is 154.42856787674705
At time: 67.93880677223206 and batch: 300, loss is 4.99230486869812 and perplexity is 147.275483252832
At time: 69.5044424533844 and batch: 350, loss is 4.926580181121826 and perplexity is 137.90708773502263
At time: 71.06805515289307 and batch: 400, loss is 4.925208024978637 and perplexity is 137.71798744463032
At time: 72.63044834136963 and batch: 450, loss is 4.926829776763916 and perplexity is 137.9415130391588
At time: 74.19490909576416 and batch: 500, loss is 4.923823766708374 and perplexity is 137.52748206628885
At time: 75.7589738368988 and batch: 550, loss is 4.92348316192627 and perplexity is 137.48064752471356
At time: 77.32258081436157 and batch: 600, loss is 4.941245031356812 and perplexity is 139.9443762775734
At time: 78.88688468933105 and batch: 650, loss is 4.940778913497925 and perplexity is 139.87916090474542
At time: 80.45056390762329 and batch: 700, loss is 4.905109539031982 and perplexity is 134.97769452305224
At time: 82.01788711547852 and batch: 750, loss is 4.882307758331299 and perplexity is 131.93478643276455
At time: 83.58177399635315 and batch: 800, loss is 4.8518758964538575 and perplexity is 127.98024248938042
At time: 85.15147757530212 and batch: 850, loss is 4.858867082595825 and perplexity is 128.87811111339153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.772960027058919 and perplexity of 118.26880320778562
Finished 3 epochs...
Completing Train Step...
At time: 89.20583653450012 and batch: 50, loss is 4.854545602798462 and perplexity is 128.32236863971153
At time: 90.77758264541626 and batch: 100, loss is 4.761413784027099 and perplexity is 116.91109615427752
At time: 92.34067845344543 and batch: 150, loss is 4.768525648117065 and perplexity is 117.74551560261254
At time: 93.90394878387451 and batch: 200, loss is 4.7841341686248775 and perplexity is 119.59776674300895
At time: 95.46790862083435 and batch: 250, loss is 4.780821046829224 and perplexity is 119.20218044994108
At time: 97.03162884712219 and batch: 300, loss is 4.747745208740234 and perplexity is 115.32395969204372
At time: 98.59598302841187 and batch: 350, loss is 4.699058294296265 and perplexity is 109.84368330852209
At time: 100.1603455543518 and batch: 400, loss is 4.714813203811645 and perplexity is 111.5879650202456
At time: 101.72518181800842 and batch: 450, loss is 4.734326009750366 and perplexity is 113.78674171340518
At time: 103.28894829750061 and batch: 500, loss is 4.736451330184937 and perplexity is 114.028832169522
At time: 104.87775158882141 and batch: 550, loss is 4.7413350772857665 and perplexity is 114.58708221449284
At time: 106.4420862197876 and batch: 600, loss is 4.759948492050171 and perplexity is 116.73991271056079
At time: 108.00585532188416 and batch: 650, loss is 4.757347412109375 and perplexity is 116.43665743190599
At time: 109.56956815719604 and batch: 700, loss is 4.736820449829102 and perplexity is 114.07093022060778
At time: 111.13210225105286 and batch: 750, loss is 4.7098807430267335 and perplexity is 111.03891695090716
At time: 112.69519019126892 and batch: 800, loss is 4.6712164783477785 and perplexity is 106.82761690460251
At time: 114.26477313041687 and batch: 850, loss is 4.692151556015014 and perplexity is 109.08763565318156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.729464213053386 and perplexity of 113.23487632408579
Finished 4 epochs...
Completing Train Step...
At time: 118.3217523097992 and batch: 50, loss is 4.693220710754394 and perplexity is 109.20432958666792
At time: 119.8900625705719 and batch: 100, loss is 4.609170560836792 and perplexity is 100.40083870285105
At time: 121.45915627479553 and batch: 150, loss is 4.6235313224792485 and perplexity is 101.85307385966928
At time: 123.02996397018433 and batch: 200, loss is 4.642789945602417 and perplexity is 103.83363403956892
At time: 124.59757566452026 and batch: 250, loss is 4.641459379196167 and perplexity is 103.69556836741897
At time: 126.16548991203308 and batch: 300, loss is 4.6213399600982665 and perplexity is 101.63012123943767
At time: 127.73471665382385 and batch: 350, loss is 4.570176725387573 and perplexity is 96.56117307577716
At time: 129.30625748634338 and batch: 400, loss is 4.59525203704834 and perplexity is 99.01308736943167
At time: 130.8762218952179 and batch: 450, loss is 4.614150428771973 and perplexity is 100.90206861371682
At time: 132.44456148147583 and batch: 500, loss is 4.6175462436676025 and perplexity is 101.24529579948147
At time: 134.01318955421448 and batch: 550, loss is 4.617031803131104 and perplexity is 101.19322451013181
At time: 135.58039927482605 and batch: 600, loss is 4.651361179351807 and perplexity is 104.72744143197957
At time: 137.1509084701538 and batch: 650, loss is 4.639219484329224 and perplexity is 103.46356112902004
At time: 138.7219648361206 and batch: 700, loss is 4.62507040977478 and perplexity is 102.00995532782657
At time: 140.29014682769775 and batch: 750, loss is 4.598243236541748 and perplexity is 99.309698656823
At time: 141.85895109176636 and batch: 800, loss is 4.558795747756958 and perplexity is 95.46844249097244
At time: 143.43114852905273 and batch: 850, loss is 4.578772201538086 and perplexity is 97.39473965470124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.706113815307617 and perplexity of 110.62142819539835
Finished 5 epochs...
Completing Train Step...
At time: 147.51419401168823 and batch: 50, loss is 4.583168964385987 and perplexity is 97.823904003161
At time: 149.07735657691956 and batch: 100, loss is 4.507930717468262 and perplexity is 90.7338701059722
At time: 150.63944339752197 and batch: 150, loss is 4.519976329803467 and perplexity is 91.83342423723045
At time: 152.20153164863586 and batch: 200, loss is 4.541938419342041 and perplexity is 93.87258831254971
At time: 153.76353454589844 and batch: 250, loss is 4.5452942943573 and perplexity is 94.18814216979365
At time: 155.32532024383545 and batch: 300, loss is 4.520489673614502 and perplexity is 91.88057845933734
At time: 156.8871247768402 and batch: 350, loss is 4.470563097000122 and perplexity is 87.40592717416789
At time: 158.4495804309845 and batch: 400, loss is 4.503626403808593 and perplexity is 90.34416238329732
At time: 160.01253247261047 and batch: 450, loss is 4.522688760757446 and perplexity is 92.08285418757329
At time: 161.5753390789032 and batch: 500, loss is 4.525733318328857 and perplexity is 92.36363294514747
At time: 163.13772106170654 and batch: 550, loss is 4.530677194595337 and perplexity is 92.82139795234086
At time: 164.70096111297607 and batch: 600, loss is 4.5591894721984865 and perplexity is 95.50603815085525
At time: 166.26326155662537 and batch: 650, loss is 4.547887077331543 and perplexity is 94.43266844598384
At time: 167.82505416870117 and batch: 700, loss is 4.540732564926148 and perplexity is 93.75945985933232
At time: 169.38710069656372 and batch: 750, loss is 4.522692565917969 and perplexity is 92.08320457828151
At time: 170.9498519897461 and batch: 800, loss is 4.476871232986451 and perplexity is 87.95903836495498
At time: 172.5122311115265 and batch: 850, loss is 4.4984784507751465 and perplexity is 89.88026995121206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.691476186116536 and perplexity of 109.01398602093703
Finished 6 epochs...
Completing Train Step...
At time: 176.54314923286438 and batch: 50, loss is 4.5051712226867675 and perplexity is 90.48383560801992
At time: 178.12640357017517 and batch: 100, loss is 4.456584520339966 and perplexity is 86.19261665440482
At time: 179.69009137153625 and batch: 150, loss is 4.4582061767578125 and perplexity is 86.33250485901966
At time: 181.256085395813 and batch: 200, loss is 4.480675373077393 and perplexity is 88.29428412578756
At time: 182.82948994636536 and batch: 250, loss is 4.472503833770752 and perplexity is 87.57572378297256
At time: 184.42923188209534 and batch: 300, loss is 4.446374616622925 and perplexity is 85.31707553816834
At time: 185.99298930168152 and batch: 350, loss is 4.3991703414917 and perplexity is 81.38332028369105
At time: 187.55676865577698 and batch: 400, loss is 4.432110404968261 and perplexity is 84.10873322831767
At time: 189.12036895751953 and batch: 450, loss is 4.454841909408569 and perplexity is 86.0425472526833
At time: 190.68472862243652 and batch: 500, loss is 4.450714044570923 and perplexity is 85.68810729074838
At time: 192.25732803344727 and batch: 550, loss is 4.460348043441773 and perplexity is 86.51761574557261
At time: 193.83117270469666 and batch: 600, loss is 4.498668203353882 and perplexity is 89.8973265824314
At time: 195.3975031375885 and batch: 650, loss is 4.478853244781494 and perplexity is 88.1335470985312
At time: 196.96205949783325 and batch: 700, loss is 4.478147821426392 and perplexity is 88.07139755948533
At time: 198.52546167373657 and batch: 750, loss is 4.460973825454712 and perplexity is 86.57177385712724
At time: 200.08952236175537 and batch: 800, loss is 4.415630216598511 and perplexity is 82.7339647975883
At time: 201.65859484672546 and batch: 850, loss is 4.427565097808838 and perplexity is 83.72730072165025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.681186040242513 and perplexity of 107.8979680440049
Finished 7 epochs...
Completing Train Step...
At time: 205.6861732006073 and batch: 50, loss is 4.439172830581665 and perplexity is 84.70484742853891
At time: 207.2810413837433 and batch: 100, loss is 4.388429956436157 and perplexity is 80.51390934847545
At time: 208.85000276565552 and batch: 150, loss is 4.387996101379395 and perplexity is 80.4789855582442
At time: 210.41912245750427 and batch: 200, loss is 4.418228130340577 and perplexity is 82.94917993581072
At time: 211.98919415473938 and batch: 250, loss is 4.41644253730774 and perplexity is 82.80119861456382
At time: 213.55883264541626 and batch: 300, loss is 4.406896533966065 and perplexity is 82.01453879827828
At time: 215.13212966918945 and batch: 350, loss is 4.34378529548645 and perplexity is 76.99845028531811
At time: 216.70349645614624 and batch: 400, loss is 4.380530242919922 and perplexity is 79.88037818255513
At time: 218.27519392967224 and batch: 450, loss is 4.404522666931152 and perplexity is 81.82007809149165
At time: 219.84820318222046 and batch: 500, loss is 4.396953201293945 and perplexity is 81.20308193354226
At time: 221.42630076408386 and batch: 550, loss is 4.407619886398315 and perplexity is 82.07388567616442
At time: 223.02208590507507 and batch: 600, loss is 4.442854022979736 and perplexity is 85.01723689907436
At time: 224.59329271316528 and batch: 650, loss is 4.425679292678833 and perplexity is 83.56955613295361
At time: 226.16498160362244 and batch: 700, loss is 4.427355222702026 and perplexity is 83.7097302893309
At time: 227.7354588508606 and batch: 750, loss is 4.413718929290772 and perplexity is 82.5759874389032
At time: 229.30395436286926 and batch: 800, loss is 4.374659051895142 and perplexity is 79.41275930625736
At time: 230.87222599983215 and batch: 850, loss is 4.3755080509185795 and perplexity is 79.48020928979045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.690377871195476 and perplexity of 108.89432006097
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 234.9027500152588 and batch: 50, loss is 4.364963645935059 and perplexity is 78.64654076835544
At time: 236.49352025985718 and batch: 100, loss is 4.28667121887207 and perplexity is 72.72398291095604
At time: 238.05966234207153 and batch: 150, loss is 4.279756488800049 and perplexity is 72.2228507927665
At time: 239.64025402069092 and batch: 200, loss is 4.2916609573364255 and perplexity is 73.08776339573862
At time: 241.2103931903839 and batch: 250, loss is 4.266965217590332 and perplexity is 71.30491203832048
At time: 242.77678680419922 and batch: 300, loss is 4.264732718467712 and perplexity is 71.14590144630311
At time: 244.34504175186157 and batch: 350, loss is 4.190246839523315 and perplexity is 66.0390900062001
At time: 245.910218000412 and batch: 400, loss is 4.218085055351257 and perplexity is 67.90332859299505
At time: 247.47573804855347 and batch: 450, loss is 4.234481763839722 and perplexity is 69.02589774661038
At time: 249.04135084152222 and batch: 500, loss is 4.205127687454223 and perplexity is 67.0291559021793
At time: 250.61113572120667 and batch: 550, loss is 4.210119686126709 and perplexity is 67.36460193592765
At time: 252.18905973434448 and batch: 600, loss is 4.244509325027466 and perplexity is 69.72154113410288
At time: 253.77640891075134 and batch: 650, loss is 4.213340368270874 and perplexity is 67.5819116614655
At time: 255.36463856697083 and batch: 700, loss is 4.214495887756348 and perplexity is 67.66004901317788
At time: 256.95228362083435 and batch: 750, loss is 4.190650968551636 and perplexity is 66.0657837129631
At time: 258.5398499965668 and batch: 800, loss is 4.132847714424133 and perplexity is 62.355240257484326
At time: 260.12690258026123 and batch: 850, loss is 4.1405174446105955 and perplexity is 62.835326839835076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.565504391988118 and perplexity of 96.11105944106683
Finished 9 epochs...
Completing Train Step...
At time: 264.20793652534485 and batch: 50, loss is 4.236247224807739 and perplexity is 69.14786790996547
At time: 265.77406549453735 and batch: 100, loss is 4.181652011871338 and perplexity is 65.4739276250254
At time: 267.33938217163086 and batch: 150, loss is 4.187837495803833 and perplexity is 65.88017066199167
At time: 268.9042775630951 and batch: 200, loss is 4.207465887069702 and perplexity is 67.18606682177325
At time: 270.47044563293457 and batch: 250, loss is 4.186358966827393 and perplexity is 65.78283689382476
At time: 272.03632164001465 and batch: 300, loss is 4.189145045280457 and perplexity is 65.96636858640518
At time: 273.6020841598511 and batch: 350, loss is 4.119768099784851 and perplexity is 61.54496831803258
At time: 275.1675808429718 and batch: 400, loss is 4.156387629508973 and perplexity is 63.840490061176986
At time: 276.7334678173065 and batch: 450, loss is 4.173757963180542 and perplexity is 64.95910793169492
At time: 278.29823207855225 and batch: 500, loss is 4.151589250564575 and perplexity is 63.534892969542334
At time: 279.8633153438568 and batch: 550, loss is 4.171130638122559 and perplexity is 64.78866324452655
At time: 281.4280514717102 and batch: 600, loss is 4.197103977203369 and perplexity is 66.49348528364386
At time: 282.99421644210815 and batch: 650, loss is 4.171125440597534 and perplexity is 64.78832650470314
At time: 284.5651445388794 and batch: 700, loss is 4.174494824409485 and perplexity is 65.00699141937272
At time: 286.1309771537781 and batch: 750, loss is 4.157815327644348 and perplexity is 63.93170010451025
At time: 287.70255994796753 and batch: 800, loss is 4.101455783843994 and perplexity is 60.428193984011834
At time: 289.2752935886383 and batch: 850, loss is 4.114576406478882 and perplexity is 61.22627371614776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.566146532694499 and perplexity of 96.17279608435179
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 293.3487386703491 and batch: 50, loss is 4.1832746887207035 and perplexity is 65.5802568973127
At time: 294.9195237159729 and batch: 100, loss is 4.130097002983093 and perplexity is 62.18395467131149
At time: 296.4870526790619 and batch: 150, loss is 4.127457270622253 and perplexity is 62.02002213803987
At time: 298.0567183494568 and batch: 200, loss is 4.145645651817322 and perplexity is 63.15838706766653
At time: 299.62322425842285 and batch: 250, loss is 4.116832427978515 and perplexity is 61.36455743286169
At time: 301.1905629634857 and batch: 300, loss is 4.117785787582397 and perplexity is 61.4230878188479
At time: 302.7935073375702 and batch: 350, loss is 4.047530708312988 and perplexity is 57.25590082563374
At time: 304.36043190956116 and batch: 400, loss is 4.071174168586731 and perplexity is 58.62575871912397
At time: 305.92672657966614 and batch: 450, loss is 4.088948712348938 and perplexity is 59.67712089459056
At time: 307.49318766593933 and batch: 500, loss is 4.059575805664062 and perplexity is 57.949723919805216
At time: 309.06740283966064 and batch: 550, loss is 4.068836555480957 and perplexity is 58.48887443079233
At time: 310.65276050567627 and batch: 600, loss is 4.093780536651611 and perplexity is 59.96616800772188
At time: 312.2386100292206 and batch: 650, loss is 4.070513730049133 and perplexity is 58.587052791619904
At time: 313.82421231269836 and batch: 700, loss is 4.060137243270874 and perplexity is 57.98226820905721
At time: 315.4109961986542 and batch: 750, loss is 4.043216834068298 and perplexity is 57.00943805661382
At time: 316.998108625412 and batch: 800, loss is 3.985710234642029 and perplexity is 53.82350322246474
At time: 318.5824284553528 and batch: 850, loss is 3.9983660697937013 and perplexity is 54.50901330802179
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.512164115905762 and perplexity of 91.11879692933624
Finished 11 epochs...
Completing Train Step...
At time: 322.6538393497467 and batch: 50, loss is 4.116702198982239 and perplexity is 61.356566508476575
At time: 324.22371435165405 and batch: 100, loss is 4.0704003429412845 and perplexity is 58.58041015174846
At time: 325.7932584285736 and batch: 150, loss is 4.07245945930481 and perplexity is 58.70115830746164
At time: 327.36277747154236 and batch: 200, loss is 4.09917049407959 and perplexity is 60.29025572529657
At time: 328.9315769672394 and batch: 250, loss is 4.068561058044434 and perplexity is 58.47276311523648
At time: 330.5013475418091 and batch: 300, loss is 4.076237902641297 and perplexity is 58.92337686364628
At time: 332.0718445777893 and batch: 350, loss is 4.007653975486756 and perplexity is 55.01764629421394
At time: 333.64247703552246 and batch: 400, loss is 4.032840075492859 and perplexity is 56.42092359839008
At time: 335.2133958339691 and batch: 450, loss is 4.054986734390258 and perplexity is 57.684397772378475
At time: 336.7847228050232 and batch: 500, loss is 4.029914345741272 and perplexity is 56.25609246685707
At time: 338.3564932346344 and batch: 550, loss is 4.042545766830444 and perplexity is 56.97119372418004
At time: 339.9358711242676 and batch: 600, loss is 4.071394438743591 and perplexity is 58.638673646527394
At time: 341.51132917404175 and batch: 650, loss is 4.050261993408203 and perplexity is 57.41249677090814
At time: 343.1169047355652 and batch: 700, loss is 4.042119655609131 and perplexity is 56.946922830659844
At time: 344.6881320476532 and batch: 750, loss is 4.029103937149048 and perplexity is 56.210520514601996
At time: 346.2603187561035 and batch: 800, loss is 3.977139024734497 and perplexity is 53.36414213082408
At time: 347.8335270881653 and batch: 850, loss is 3.990050163269043 and perplexity is 54.05760100129655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.511834144592285 and perplexity of 91.08873530024144
Finished 12 epochs...
Completing Train Step...
At time: 351.91953110694885 and batch: 50, loss is 4.083242816925049 and perplexity is 59.33757909976542
At time: 353.51751732826233 and batch: 100, loss is 4.040393500328064 and perplexity is 56.848708390149525
At time: 355.09421396255493 and batch: 150, loss is 4.042113213539124 and perplexity is 56.946555975777926
At time: 356.65895223617554 and batch: 200, loss is 4.07538405418396 and perplexity is 58.87308670235389
At time: 358.22301983833313 and batch: 250, loss is 4.04095232963562 and perplexity is 56.880485992795485
At time: 359.78697776794434 and batch: 300, loss is 4.0511068344116214 and perplexity is 57.461021697326366
At time: 361.3509442806244 and batch: 350, loss is 3.9832304382324217 and perplexity is 53.690197246573916
At time: 362.9155840873718 and batch: 400, loss is 4.010946044921875 and perplexity is 55.199066666353744
At time: 364.4848940372467 and batch: 450, loss is 4.034343113899231 and perplexity is 56.50579017636748
At time: 366.0684103965759 and batch: 500, loss is 4.01068751335144 and perplexity is 55.18479780951643
At time: 367.6528091430664 and batch: 550, loss is 4.023631544113159 and perplexity is 55.90375459100787
At time: 369.2370865345001 and batch: 600, loss is 4.053950247764587 and perplexity is 57.624639640179765
At time: 370.82149863243103 and batch: 650, loss is 4.034146585464478 and perplexity is 56.49468627302172
At time: 372.40471291542053 and batch: 700, loss is 4.028102011680603 and perplexity is 56.1542299666796
At time: 373.98745560646057 and batch: 750, loss is 4.015686693191529 and perplexity is 55.46136727244973
At time: 375.563800573349 and batch: 800, loss is 3.963150157928467 and perplexity is 52.62283536387207
At time: 377.1338484287262 and batch: 850, loss is 3.978537440299988 and perplexity is 53.43881958070332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.513807614644368 and perplexity of 91.26867368454614
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 381.174259185791 and batch: 50, loss is 4.066447925567627 and perplexity is 58.34933287863954
At time: 382.76759791374207 and batch: 100, loss is 4.03461413860321 and perplexity is 56.52110671690981
At time: 384.33451294898987 and batch: 150, loss is 4.021103401184082 and perplexity is 55.76260041328022
At time: 385.90353631973267 and batch: 200, loss is 4.064848384857178 and perplexity is 58.2560753497631
At time: 387.4693603515625 and batch: 250, loss is 4.026915106773377 and perplexity is 56.087619773373014
At time: 389.0353744029999 and batch: 300, loss is 4.024566473960877 and perplexity is 55.95604511995373
At time: 390.60141038894653 and batch: 350, loss is 3.956074571609497 and perplexity is 52.251812101371485
At time: 392.16687297821045 and batch: 400, loss is 3.973760681152344 and perplexity is 53.18416390905186
At time: 393.73255491256714 and batch: 450, loss is 3.9972553491592406 and perplexity is 54.44850263362481
At time: 395.29921436309814 and batch: 500, loss is 3.9684906244277953 and perplexity is 52.90461760783367
At time: 396.86608695983887 and batch: 550, loss is 3.9789684534072878 and perplexity is 53.46185737682064
At time: 398.43469619750977 and batch: 600, loss is 4.004412522315979 and perplexity is 54.83959789385731
At time: 400.00086760520935 and batch: 650, loss is 3.988570365905762 and perplexity is 53.9776658643588
At time: 401.5715961456299 and batch: 700, loss is 3.9761011743545533 and perplexity is 53.30878686584838
At time: 403.1392066478729 and batch: 750, loss is 3.9592979621887205 and perplexity is 52.420511846762096
At time: 404.71275329589844 and batch: 800, loss is 3.899936623573303 and perplexity is 49.39931825404793
At time: 406.301456451416 and batch: 850, loss is 3.9160511350631713 and perplexity is 50.20181267267361
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.488131841023763 and perplexity of 88.95510827048986
Finished 14 epochs...
Completing Train Step...
At time: 410.3507966995239 and batch: 50, loss is 4.034939379692077 and perplexity is 56.53949269297777
At time: 411.95394468307495 and batch: 100, loss is 4.003335690498352 and perplexity is 54.78057665365724
At time: 413.5248236656189 and batch: 150, loss is 3.9899637031555177 and perplexity is 54.052927377021035
At time: 415.09885001182556 and batch: 200, loss is 4.036859884262085 and perplexity is 56.64818138223602
At time: 416.67086935043335 and batch: 250, loss is 4.001858196258545 and perplexity is 54.69969843046564
At time: 418.24567675590515 and batch: 300, loss is 4.001787171363831 and perplexity is 54.69581352810775
At time: 419.8169434070587 and batch: 350, loss is 3.935813698768616 and perplexity is 51.20379747704695
At time: 421.3879806995392 and batch: 400, loss is 3.9546598482131956 and perplexity is 52.17794250513604
At time: 422.994428396225 and batch: 450, loss is 3.9810373783111572 and perplexity is 53.572580444315165
At time: 424.564492225647 and batch: 500, loss is 3.955049376487732 and perplexity is 52.19827124810195
At time: 426.13550901412964 and batch: 550, loss is 3.9673266935348512 and perplexity is 52.84307611099291
At time: 427.7064952850342 and batch: 600, loss is 3.996126093864441 and perplexity is 54.387051077509774
At time: 429.2847683429718 and batch: 650, loss is 3.980844831466675 and perplexity is 53.56226620601872
At time: 430.87240409851074 and batch: 700, loss is 3.9699436807632447 and perplexity is 52.98154687536751
At time: 432.46316933631897 and batch: 750, loss is 3.9560297775268554 and perplexity is 52.249471581803135
At time: 434.0531258583069 and batch: 800, loss is 3.8984514808654787 and perplexity is 49.32600766858692
At time: 435.6439278125763 and batch: 850, loss is 3.9136491441726684 and perplexity is 50.08137308125479
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.487740516662598 and perplexity of 88.92030477974455
Finished 15 epochs...
Completing Train Step...
At time: 439.7240741252899 and batch: 50, loss is 4.016113834381104 and perplexity is 55.4850621670144
At time: 441.29046726226807 and batch: 100, loss is 3.9870244312286376 and perplexity is 53.89428438667397
At time: 442.8655273914337 and batch: 150, loss is 3.971938314437866 and perplexity is 53.087331118232754
At time: 444.436297416687 and batch: 200, loss is 4.02183078289032 and perplexity is 55.80317586384437
At time: 446.00431060791016 and batch: 250, loss is 3.987899103164673 and perplexity is 53.941444826689704
At time: 447.57725954055786 and batch: 300, loss is 3.988623328208923 and perplexity is 53.98052472156741
At time: 449.1454408168793 and batch: 350, loss is 3.92295690536499 and perplexity is 50.54969467379177
At time: 450.7134735584259 and batch: 400, loss is 3.943145442008972 and perplexity is 51.58059015882294
At time: 452.2809567451477 and batch: 450, loss is 3.970071039199829 and perplexity is 52.98829495204851
At time: 453.848064661026 and batch: 500, loss is 3.945994930267334 and perplexity is 51.7277780503322
At time: 455.4159417152405 and batch: 550, loss is 3.9583432817459108 and perplexity is 52.37049089011359
At time: 456.98407649993896 and batch: 600, loss is 3.98885124206543 and perplexity is 53.99282903324126
At time: 458.5618624687195 and batch: 650, loss is 3.973625993728638 and perplexity is 53.17700115341037
At time: 460.12969756126404 and batch: 700, loss is 3.9648232507705687 and perplexity is 52.71095194610702
At time: 461.7229311466217 and batch: 750, loss is 3.9515276193618774 and perplexity is 52.01476493637271
At time: 463.29057264328003 and batch: 800, loss is 3.895154151916504 and perplexity is 49.16363144659059
At time: 464.85851788520813 and batch: 850, loss is 3.9082316637039183 and perplexity is 49.810791816872346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.488527933756511 and perplexity of 88.99034972139715
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 468.9111614227295 and batch: 50, loss is 4.0082909297943115 and perplexity is 55.05270118400897
At time: 470.47655606269836 and batch: 100, loss is 3.99182475566864 and perplexity is 54.15361637805536
At time: 472.04154419898987 and batch: 150, loss is 3.9798963117599486 and perplexity is 53.51148542807694
At time: 473.6074571609497 and batch: 200, loss is 4.018022303581238 and perplexity is 55.591054808902896
At time: 475.1774699687958 and batch: 250, loss is 3.9841099548339844 and perplexity is 53.73743943849415
At time: 476.7474970817566 and batch: 300, loss is 3.983221855163574 and perplexity is 53.68973642189215
At time: 478.3181254863739 and batch: 350, loss is 3.916579427719116 and perplexity is 50.22834092834804
At time: 479.8843195438385 and batch: 400, loss is 3.928332986831665 and perplexity is 50.82218576131616
At time: 481.4497106075287 and batch: 450, loss is 3.9548292636871336 and perplexity is 52.18678300483223
At time: 483.0147705078125 and batch: 500, loss is 3.928568162918091 and perplexity is 50.834139329608725
At time: 484.58777499198914 and batch: 550, loss is 3.9381012678146363 and perplexity is 51.3210637754337
At time: 486.17059326171875 and batch: 600, loss is 3.9640960931777953 and perplexity is 52.672636709473004
At time: 487.7532789707184 and batch: 650, loss is 3.9484349584579466 and perplexity is 51.854149399141065
At time: 489.33726835250854 and batch: 700, loss is 3.9385938119888304 and perplexity is 51.3463478926709
At time: 490.92071080207825 and batch: 750, loss is 3.925390639305115 and perplexity is 50.672869007362976
At time: 492.50575256347656 and batch: 800, loss is 3.863476176261902 and perplexity is 47.63063641007387
At time: 494.090051651001 and batch: 850, loss is 3.874502806663513 and perplexity is 48.1587481289984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.477892875671387 and perplexity of 88.04894699250018
Finished 17 epochs...
Completing Train Step...
At time: 498.1639041900635 and batch: 50, loss is 3.9925367641448974 and perplexity is 54.19218794194724
At time: 499.7290415763855 and batch: 100, loss is 3.9734267950057984 and perplexity is 53.16640941766133
At time: 501.32147693634033 and batch: 150, loss is 3.9614628601074218 and perplexity is 52.53411983423288
At time: 502.8872900009155 and batch: 200, loss is 4.000823850631714 and perplexity is 54.64314928732626
At time: 504.45190691947937 and batch: 250, loss is 3.968965587615967 and perplexity is 52.929751322003696
At time: 506.0222096443176 and batch: 300, loss is 3.968987531661987 and perplexity is 52.93091282764657
At time: 507.5915412902832 and batch: 350, loss is 3.9043964767456054 and perplexity is 49.620123974829255
At time: 509.1624765396118 and batch: 400, loss is 3.9170498275756835 and perplexity is 50.251973890751636
At time: 510.7299916744232 and batch: 450, loss is 3.946258544921875 and perplexity is 51.741416048182174
At time: 512.296820640564 and batch: 500, loss is 3.921600646972656 and perplexity is 50.48118269663446
At time: 513.8632023334503 and batch: 550, loss is 3.9326684284210205 and perplexity is 51.04300069838791
At time: 515.4315783977509 and batch: 600, loss is 3.961013479232788 and perplexity is 52.510517309172535
At time: 517.0068538188934 and batch: 650, loss is 3.945577540397644 and perplexity is 51.70619190502499
At time: 518.5937392711639 and batch: 700, loss is 3.9364525651931763 and perplexity is 51.236520315713506
At time: 520.1798477172852 and batch: 750, loss is 3.925325698852539 and perplexity is 50.6695783951644
At time: 521.7664108276367 and batch: 800, loss is 3.8632158708572386 and perplexity is 47.618239511548616
At time: 523.3589763641357 and batch: 850, loss is 3.8736555433273314 and perplexity is 48.11796226801448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.478060722351074 and perplexity of 88.06372695625207
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 527.4140892028809 and batch: 50, loss is 3.9882512140274047 and perplexity is 53.96044153964285
At time: 529.013524055481 and batch: 100, loss is 3.9757040309906007 and perplexity is 53.28761983835471
At time: 530.587509393692 and batch: 150, loss is 3.973963665962219 and perplexity is 53.19496058219457
At time: 532.1651356220245 and batch: 200, loss is 4.007296118736267 and perplexity is 54.99796138049127
At time: 533.7417407035828 and batch: 250, loss is 3.968888201713562 and perplexity is 52.92565546391646
At time: 535.3240523338318 and batch: 300, loss is 3.967898726463318 and perplexity is 52.87331273791848
At time: 536.8989527225494 and batch: 350, loss is 3.904014892578125 and perplexity is 49.60119333317831
At time: 538.4760820865631 and batch: 400, loss is 3.911693797111511 and perplexity is 49.98354229332384
At time: 540.0531363487244 and batch: 450, loss is 3.9409108543395996 and perplexity is 51.46545749300123
At time: 541.6951205730438 and batch: 500, loss is 3.9149798774719238 and perplexity is 50.148062395090356
At time: 543.288149356842 and batch: 550, loss is 3.92639328956604 and perplexity is 50.72370165211877
At time: 544.8804771900177 and batch: 600, loss is 3.9504487133026123 and perplexity is 51.95867615401597
At time: 546.4784429073334 and batch: 650, loss is 3.9330757188797 and perplexity is 51.06379425977686
At time: 548.0707061290741 and batch: 700, loss is 3.92249596118927 and perplexity is 50.52639945575745
At time: 549.6622371673584 and batch: 750, loss is 3.912539315223694 and perplexity is 50.02582215531964
At time: 551.254741191864 and batch: 800, loss is 3.8472057485580446 and perplexity is 46.86193608331242
At time: 552.8457868099213 and batch: 850, loss is 3.8580836725234984 and perplexity is 47.37447930996444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.471966743469238 and perplexity of 87.52870034012864
Finished 19 epochs...
Completing Train Step...
At time: 556.9140536785126 and batch: 50, loss is 3.979326591491699 and perplexity is 53.48100753300608
At time: 558.5067315101624 and batch: 100, loss is 3.96318603515625 and perplexity is 52.62472335919082
At time: 560.0745372772217 and batch: 150, loss is 3.96118106842041 and perplexity is 52.51931824155995
At time: 561.6479609012604 and batch: 200, loss is 3.995253577232361 and perplexity is 54.33961816688574
At time: 563.2162079811096 and batch: 250, loss is 3.959851679801941 and perplexity is 52.44954604509735
At time: 564.7832646369934 and batch: 300, loss is 3.9592331790924074 and perplexity is 52.41711599369246
At time: 566.3496398925781 and batch: 350, loss is 3.8957941341400146 and perplexity is 49.195105367060194
At time: 567.9551367759705 and batch: 400, loss is 3.9039575815200807 and perplexity is 49.598350717765555
At time: 569.574339389801 and batch: 450, loss is 3.9352920341491697 and perplexity is 51.177093233457015
At time: 571.1933801174164 and batch: 500, loss is 3.910149974822998 and perplexity is 49.90643612110069
At time: 572.8037950992584 and batch: 550, loss is 3.922385239601135 and perplexity is 50.52080540226393
At time: 574.4157643318176 and batch: 600, loss is 3.9483395528793337 and perplexity is 51.849202460000704
At time: 576.0075190067291 and batch: 650, loss is 3.932110528945923 and perplexity is 51.014531777227376
At time: 577.5909836292267 and batch: 700, loss is 3.9227275753021242 and perplexity is 50.538103438294556
At time: 579.175675868988 and batch: 750, loss is 3.913710942268372 and perplexity is 50.08446811037389
At time: 580.7621402740479 and batch: 800, loss is 3.848429198265076 and perplexity is 46.91930439174865
At time: 582.3909306526184 and batch: 850, loss is 3.8590499353408814 and perplexity is 47.42027763085847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.471837043762207 and perplexity of 87.51734862951038
Finished 20 epochs...
Completing Train Step...
At time: 586.4993731975555 and batch: 50, loss is 3.9732347679138185 and perplexity is 53.15620100684693
At time: 588.0934460163116 and batch: 100, loss is 3.956930594444275 and perplexity is 52.296559995565886
At time: 589.661847114563 and batch: 150, loss is 3.955651068687439 and perplexity is 52.229687991407
At time: 591.2316973209381 and batch: 200, loss is 3.9899510526657105 and perplexity is 54.052243585339355
At time: 592.8009660243988 and batch: 250, loss is 3.954641623497009 and perplexity is 52.17699158560784
At time: 594.3814675807953 and batch: 300, loss is 3.954145951271057 and perplexity is 52.15113530869366
At time: 595.9616992473602 and batch: 350, loss is 3.8908199882507324 and perplexity is 48.95100932387845
At time: 597.5526192188263 and batch: 400, loss is 3.899753522872925 and perplexity is 49.39027403230435
At time: 599.1545972824097 and batch: 450, loss is 3.9321302795410156 and perplexity is 51.01553935453844
At time: 600.7557904720306 and batch: 500, loss is 3.9073870182037354 and perplexity is 49.76873711885963
At time: 602.359991312027 and batch: 550, loss is 3.9199292707443236 and perplexity is 50.39688011818493
At time: 603.9607536792755 and batch: 600, loss is 3.9464404678344724 and perplexity is 51.75082985355906
At time: 605.5626168251038 and batch: 650, loss is 3.9304233598709106 and perplexity is 50.928534203459726
At time: 607.1648128032684 and batch: 700, loss is 3.9216634607315064 and perplexity is 50.484353709061416
At time: 608.7656557559967 and batch: 750, loss is 3.913214297294617 and perplexity is 50.0596000868243
At time: 610.364227771759 and batch: 800, loss is 3.848055934906006 and perplexity is 46.901794402708134
At time: 611.9650518894196 and batch: 850, loss is 3.858673439025879 and perplexity is 47.40242743155204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.471762339274089 and perplexity of 87.51081093498016
Finished 21 epochs...
Completing Train Step...
At time: 616.0978901386261 and batch: 50, loss is 3.968128452301025 and perplexity is 52.88546049925346
At time: 617.6775026321411 and batch: 100, loss is 3.9518657398223875 and perplexity is 52.03235516628706
At time: 619.2551689147949 and batch: 150, loss is 3.951266927719116 and perplexity is 52.00120688916519
At time: 620.841903924942 and batch: 200, loss is 3.985988163948059 and perplexity is 53.83846443034622
At time: 622.4873466491699 and batch: 250, loss is 3.950562930107117 and perplexity is 51.96461104689836
At time: 624.085031747818 and batch: 300, loss is 3.950033130645752 and perplexity is 51.937087515575506
At time: 625.6838212013245 and batch: 350, loss is 3.8871073389053343 and perplexity is 48.76960833864579
At time: 627.2864675521851 and batch: 400, loss is 3.8963340187072752 and perplexity is 49.22167221610323
At time: 628.8843259811401 and batch: 450, loss is 3.929419255256653 and perplexity is 50.87742229241587
At time: 630.4801924228668 and batch: 500, loss is 3.9049073791503908 and perplexity is 49.645481492550566
At time: 632.0750567913055 and batch: 550, loss is 3.9173830938339234 and perplexity is 50.2687239690223
At time: 633.6700973510742 and batch: 600, loss is 3.944558334350586 and perplexity is 51.65351948814754
At time: 635.2774815559387 and batch: 650, loss is 3.9286166715621946 and perplexity is 50.83660528459137
At time: 636.8758819103241 and batch: 700, loss is 3.9202798557281495 and perplexity is 50.414551605084014
At time: 638.473531961441 and batch: 750, loss is 3.9124201345443725 and perplexity is 50.01986039912168
At time: 640.0698890686035 and batch: 800, loss is 3.8472945261001588 and perplexity is 46.86609655549212
At time: 641.6657745838165 and batch: 850, loss is 3.857789855003357 and perplexity is 47.360561902624774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.471805890401204 and perplexity of 87.51462221242326
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 645.7847828865051 and batch: 50, loss is 3.967588744163513 and perplexity is 52.85692548684779
At time: 647.3581614494324 and batch: 100, loss is 3.9548191928863528 and perplexity is 52.18625744478361
At time: 648.953536272049 and batch: 150, loss is 3.9581410837173463 and perplexity is 52.359902750587196
At time: 650.5575931072235 and batch: 200, loss is 3.997789182662964 and perplexity is 54.47757682825734
At time: 652.162344455719 and batch: 250, loss is 3.9593089914321897 and perplexity is 52.42109000853837
At time: 653.7661366462708 and batch: 300, loss is 3.955642590522766 and perplexity is 52.2292451813885
At time: 655.3749778270721 and batch: 350, loss is 3.8888742685317994 and perplexity is 48.855856979690685
At time: 656.9804103374481 and batch: 400, loss is 3.894920997619629 and perplexity is 49.15217007085526
At time: 658.5836172103882 and batch: 450, loss is 3.9272357654571532 and perplexity is 50.766453153894844
At time: 660.1862254142761 and batch: 500, loss is 3.9017669439315794 and perplexity is 49.489817628091686
At time: 661.8275320529938 and batch: 550, loss is 3.915273003578186 and perplexity is 50.16276425600119
At time: 663.4315721988678 and batch: 600, loss is 3.939181079864502 and perplexity is 51.37651080931043
At time: 665.0370097160339 and batch: 650, loss is 3.920941529273987 and perplexity is 50.44792061868568
At time: 666.6433053016663 and batch: 700, loss is 3.9130212450027466 and perplexity is 50.04993689907771
At time: 668.249743938446 and batch: 750, loss is 3.9052226400375365 and perplexity is 49.661135238465945
At time: 669.8538963794708 and batch: 800, loss is 3.8385177183151247 and perplexity is 46.45656166783453
At time: 671.4573469161987 and batch: 850, loss is 3.8519015073776246 and perplexity is 47.08250589900726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.468257586161296 and perplexity of 87.20464398113984
Finished 23 epochs...
Completing Train Step...
At time: 675.5879580974579 and batch: 50, loss is 3.962260751724243 and perplexity is 52.576053094919615
At time: 677.1569111347198 and batch: 100, loss is 3.947146668434143 and perplexity is 51.78738922824186
At time: 678.7590019702911 and batch: 150, loss is 3.9498397397994993 and perplexity is 51.92704432943041
At time: 680.3598818778992 and batch: 200, loss is 3.988827338218689 and perplexity is 53.99153841235641
At time: 681.965505361557 and batch: 250, loss is 3.951599154472351 and perplexity is 52.01848595141873
At time: 683.5775127410889 and batch: 300, loss is 3.9494516324996947 and perplexity is 51.906894974777614
At time: 685.1939511299133 and batch: 350, loss is 3.88333411693573 and perplexity is 48.585936516205614
At time: 686.8102433681488 and batch: 400, loss is 3.8901091623306274 and perplexity is 48.91622604152967
At time: 688.4224107265472 and batch: 450, loss is 3.923589572906494 and perplexity is 50.581685943696755
At time: 690.0346908569336 and batch: 500, loss is 3.8990581035614014 and perplexity is 49.35593902193919
At time: 691.6481041908264 and batch: 550, loss is 3.9134715890884397 and perplexity is 50.07248166822021
At time: 693.2604169845581 and batch: 600, loss is 3.9379685068130494 and perplexity is 51.31425079186364
At time: 694.8743586540222 and batch: 650, loss is 3.9207443809509277 and perplexity is 50.43797587606076
At time: 696.4870734214783 and batch: 700, loss is 3.913844485282898 and perplexity is 50.09115698784278
At time: 698.0999047756195 and batch: 750, loss is 3.9067699480056763 and perplexity is 49.73803578780097
At time: 699.7125670909882 and batch: 800, loss is 3.839952211380005 and perplexity is 46.52325110469659
At time: 701.3254396915436 and batch: 850, loss is 3.853356609344482 and perplexity is 47.15106561453703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.468084335327148 and perplexity of 87.18953701251382
Finished 24 epochs...
Completing Train Step...
At time: 705.4705970287323 and batch: 50, loss is 3.9592398023605346 and perplexity is 52.41746316745586
At time: 707.0673813819885 and batch: 100, loss is 3.9437582969665526 and perplexity is 51.612211267803694
At time: 708.6388747692108 and batch: 150, loss is 3.9466500234603883 and perplexity is 51.76167566746165
At time: 710.2266070842743 and batch: 200, loss is 3.9861745119094847 and perplexity is 53.848498053282654
At time: 711.8289194107056 and batch: 250, loss is 3.949002466201782 and perplexity is 51.88358538225924
At time: 713.4416546821594 and batch: 300, loss is 3.947079381942749 and perplexity is 51.783904753752566
At time: 715.0636689662933 and batch: 350, loss is 3.880689916610718 and perplexity is 48.45763526893297
At time: 716.6822230815887 and batch: 400, loss is 3.8876642560958863 and perplexity is 48.79677653642206
At time: 718.2940084934235 and batch: 450, loss is 3.9216394233703613 and perplexity is 50.483140213003814
At time: 719.9113101959229 and batch: 500, loss is 3.8974357891082763 and perplexity is 49.27593308365868
At time: 721.5237021446228 and batch: 550, loss is 3.912192883491516 and perplexity is 50.00849462467327
At time: 723.1383466720581 and batch: 600, loss is 3.936995325088501 and perplexity is 51.264336992322235
At time: 724.7509279251099 and batch: 650, loss is 3.9200544929504395 and perplexity is 50.403191321837866
At time: 726.3659801483154 and batch: 700, loss is 3.914075469970703 and perplexity is 50.102728614484256
At time: 727.980236530304 and batch: 750, loss is 3.907162127494812 and perplexity is 49.75754585074004
At time: 729.5925414562225 and batch: 800, loss is 3.84014226436615 and perplexity is 46.53209382776057
At time: 731.2057299613953 and batch: 850, loss is 3.8536269283294677 and perplexity is 47.163813165609845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.4679921468098955 and perplexity of 87.18149950886514
Finished 25 epochs...
Completing Train Step...
At time: 735.3220624923706 and batch: 50, loss is 3.956553544998169 and perplexity is 52.27684532352295
At time: 736.9192860126495 and batch: 100, loss is 3.940936608314514 and perplexity is 51.46678295017029
At time: 738.5028836727142 and batch: 150, loss is 3.944098644256592 and perplexity is 51.629780333664016
At time: 740.1055128574371 and batch: 200, loss is 3.9841866540908812 and perplexity is 53.74156121823332
At time: 741.7107746601105 and batch: 250, loss is 3.947018570899963 and perplexity is 51.78075581625103
At time: 743.3551869392395 and batch: 300, loss is 3.945203490257263 and perplexity is 51.68685481343072
At time: 744.9662230014801 and batch: 350, loss is 3.878523030281067 and perplexity is 48.35274676327712
At time: 746.5837421417236 and batch: 400, loss is 3.8858402442932127 and perplexity is 48.70785176465835
At time: 748.200526714325 and batch: 450, loss is 3.9200650787353517 and perplexity is 50.403724882004155
At time: 749.8188891410828 and batch: 500, loss is 3.8960933589935305 and perplexity is 49.20982796783153
At time: 751.437623500824 and batch: 550, loss is 3.9110851097106933 and perplexity is 49.95312719846324
At time: 753.0606501102448 and batch: 600, loss is 3.936100330352783 and perplexity is 51.21847620622637
At time: 754.6806893348694 and batch: 650, loss is 3.9192313671112062 and perplexity is 50.36172022298908
At time: 756.3127789497375 and batch: 700, loss is 3.9132955265045166 and perplexity is 50.063666553742955
At time: 757.9327299594879 and batch: 750, loss is 3.9068916845321655 and perplexity is 49.744091092080545
At time: 759.5551393032074 and batch: 800, loss is 3.839969067573547 and perplexity is 46.52403531623082
At time: 761.1923079490662 and batch: 850, loss is 3.8535672569274904 and perplexity is 47.16099891872152
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.467990239461263 and perplexity of 87.18133322350982
Finished 26 epochs...
Completing Train Step...
At time: 765.376166343689 and batch: 50, loss is 3.9540705728530883 and perplexity is 52.147204386774014
At time: 766.9495370388031 and batch: 100, loss is 3.9384175968170165 and perplexity is 51.33730068430607
At time: 768.5306377410889 and batch: 150, loss is 3.9418214893341066 and perplexity is 51.512345085103604
At time: 770.120730638504 and batch: 200, loss is 3.982443528175354 and perplexity is 53.64796450925681
At time: 771.7209475040436 and batch: 250, loss is 3.945306448936462 and perplexity is 51.69217669769665
At time: 773.3293278217316 and batch: 300, loss is 3.9435788106918337 and perplexity is 51.602948415575526
At time: 774.9540860652924 and batch: 350, loss is 3.8766047143936158 and perplexity is 48.26007983160022
At time: 776.5713186264038 and batch: 400, loss is 3.884089946746826 and perplexity is 48.62267309697713
At time: 778.1825654506683 and batch: 450, loss is 3.9186252689361574 and perplexity is 50.33120532471583
At time: 779.79323554039 and batch: 500, loss is 3.8948561191558837 and perplexity is 49.14898125701511
At time: 781.4067077636719 and batch: 550, loss is 3.909904570579529 and perplexity is 49.89419037254095
At time: 783.0252642631531 and batch: 600, loss is 3.9351205348968508 and perplexity is 51.168317152798764
At time: 784.6655840873718 and batch: 650, loss is 3.9183644342422483 and perplexity is 50.31807891216721
At time: 786.276109457016 and batch: 700, loss is 3.9126562547683714 and perplexity is 50.03167249424591
At time: 787.8863837718964 and batch: 750, loss is 3.9065745496749877 and perplexity is 49.728318008086184
At time: 789.4886629581451 and batch: 800, loss is 3.839633059501648 and perplexity is 46.508405490848055
At time: 791.0887320041656 and batch: 850, loss is 3.8533485651016237 and perplexity is 47.15068632143976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.4679915110270185 and perplexity of 87.18144408037818
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 795.201009273529 and batch: 50, loss is 3.9540037059783937 and perplexity is 52.14371758276975
At time: 796.77397108078 and batch: 100, loss is 3.9405079746246336 and perplexity is 51.44472728032734
At time: 798.3674499988556 and batch: 150, loss is 3.945535249710083 and perplexity is 51.70400526085582
At time: 799.9721255302429 and batch: 200, loss is 3.9890731716156007 and perplexity is 54.004812967246416
At time: 801.5831677913666 and batch: 250, loss is 3.9520915079116823 and perplexity is 52.044103737870834
At time: 803.199125289917 and batch: 300, loss is 3.9510899925231935 and perplexity is 51.99200685936458
At time: 804.8155841827393 and batch: 350, loss is 3.8804933500289915 and perplexity is 48.448111053311536
At time: 806.4306070804596 and batch: 400, loss is 3.8868146896362306 and perplexity is 48.75533803660959
At time: 808.0440294742584 and batch: 450, loss is 3.9194214582443236 and perplexity is 50.371294449410954
At time: 809.661895275116 and batch: 500, loss is 3.8943294048309327 and perplexity is 49.123100600984756
At time: 811.2775337696075 and batch: 550, loss is 3.907746434211731 and perplexity is 49.78662801462245
At time: 812.8938562870026 and batch: 600, loss is 3.93143048286438 and perplexity is 50.97985133827536
At time: 814.5100417137146 and batch: 650, loss is 3.9122806882858274 and perplexity is 50.01288580303804
At time: 816.1247129440308 and batch: 700, loss is 3.9067729187011717 and perplexity is 49.7381835445793
At time: 817.7393825054169 and batch: 750, loss is 3.901905307769775 and perplexity is 49.49666570296235
At time: 819.3541529178619 and batch: 800, loss is 3.8366073751449585 and perplexity is 46.36789840813334
At time: 820.9692583084106 and batch: 850, loss is 3.852612166404724 and perplexity is 47.11597739884591
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.466094017028809 and perplexity of 87.01617466193267
Finished 28 epochs...
Completing Train Step...
At time: 825.0963127613068 and batch: 50, loss is 3.951852207183838 and perplexity is 52.031651035996084
At time: 826.7037327289581 and batch: 100, loss is 3.9353260135650636 and perplexity is 51.17883223073712
At time: 828.2943818569183 and batch: 150, loss is 3.9388801670074463 and perplexity is 51.36105328245816
At time: 829.904664516449 and batch: 200, loss is 3.9822210311889648 and perplexity is 53.6360293266479
At time: 831.5223731994629 and batch: 250, loss is 3.9458403825759887 and perplexity is 51.71978425938297
At time: 833.1320652961731 and batch: 300, loss is 3.9454886293411255 and perplexity is 51.70159485724138
At time: 834.7498586177826 and batch: 350, loss is 3.8757597970962525 and perplexity is 48.21932127660718
At time: 836.3564279079437 and batch: 400, loss is 3.8825723791122435 and perplexity is 48.54894086295477
At time: 837.9626970291138 and batch: 450, loss is 3.9164324426651 and perplexity is 50.22095865549873
At time: 839.5765175819397 and batch: 500, loss is 3.8924732208251953 and perplexity is 49.03200365983514
At time: 841.1938006877899 and batch: 550, loss is 3.907138729095459 and perplexity is 49.75638161743204
At time: 842.8162362575531 and batch: 600, loss is 3.931178455352783 and perplexity is 50.967004632130696
At time: 844.4380977153778 and batch: 650, loss is 3.91270836353302 and perplexity is 50.03427965082015
At time: 846.0557584762573 and batch: 700, loss is 3.9075912284851073 and perplexity is 49.77890144446478
At time: 847.6811635494232 and batch: 750, loss is 3.903352813720703 and perplexity is 49.568364300672904
At time: 849.3020234107971 and batch: 800, loss is 3.838496837615967 and perplexity is 46.45559163247398
At time: 850.9203395843506 and batch: 850, loss is 3.8546155405044558 and perplexity is 47.210462940992734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.466057459513347 and perplexity of 87.01299362492777
Finished 29 epochs...
Completing Train Step...
At time: 855.0582375526428 and batch: 50, loss is 3.9504771852493286 and perplexity is 51.96015553973527
At time: 856.6520261764526 and batch: 100, loss is 3.933331708908081 and perplexity is 51.07686775518951
At time: 858.2314765453339 and batch: 150, loss is 3.9368338489532473 and perplexity is 51.25605969361946
At time: 859.8269817829132 and batch: 200, loss is 3.980453724861145 and perplexity is 53.541321745923725
At time: 861.422708272934 and batch: 250, loss is 3.9441220474243166 and perplexity is 51.63098864821189
At time: 863.0337119102478 and batch: 300, loss is 3.9437919950485227 and perplexity is 51.61395052963439
At time: 864.6505329608917 and batch: 350, loss is 3.874159154891968 and perplexity is 48.142201133253515
At time: 866.2679586410522 and batch: 400, loss is 3.8811968564987183 and perplexity is 48.48220660469786
At time: 867.9145839214325 and batch: 450, loss is 3.915301022529602 and perplexity is 50.16416978374638
At time: 869.5198531150818 and batch: 500, loss is 3.891659984588623 and perplexity is 48.99214526704859
At time: 871.1237864494324 and batch: 550, loss is 3.90665256023407 and perplexity is 49.732197493294656
At time: 872.7325110435486 and batch: 600, loss is 3.9307375240325926 and perplexity is 50.94453663727743
At time: 874.3360707759857 and batch: 650, loss is 3.9125034952163698 and perplexity is 50.02403026209664
At time: 875.9461815357208 and batch: 700, loss is 3.9076277303695677 and perplexity is 49.780718501336665
At time: 877.5514595508575 and batch: 750, loss is 3.903706512451172 and perplexity is 49.58589966913343
At time: 879.1548764705658 and batch: 800, loss is 3.839034523963928 and perplexity is 46.48057688639424
At time: 880.7634935379028 and batch: 850, loss is 3.855257511138916 and perplexity is 47.24078040225883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.466039975484212 and perplexity of 87.01147230051163
Finished 30 epochs...
Completing Train Step...
At time: 884.8922967910767 and batch: 50, loss is 3.9491060829162596 and perplexity is 51.88896166744356
At time: 886.4735100269318 and batch: 100, loss is 3.931738061904907 and perplexity is 50.99553408375448
At time: 888.0670714378357 and batch: 150, loss is 3.9352526903152465 and perplexity is 51.17507977000911
At time: 889.6557483673096 and batch: 200, loss is 3.97929416179657 and perplexity is 53.47927318835886
At time: 891.2451298236847 and batch: 250, loss is 3.9429804992675783 and perplexity is 51.572083016494425
At time: 892.8449442386627 and batch: 300, loss is 3.942674732208252 and perplexity is 51.55631638290857
At time: 894.4492235183716 and batch: 350, loss is 3.8730204916000366 and perplexity is 48.08741457367193
At time: 896.0510587692261 and batch: 400, loss is 3.880190396308899 and perplexity is 48.43343574091895
At time: 897.6529269218445 and batch: 450, loss is 3.9144194316864014 and perplexity is 50.119964999137935
At time: 899.2840313911438 and batch: 500, loss is 3.891082820892334 and perplexity is 48.96387693790853
At time: 900.9350173473358 and batch: 550, loss is 3.9060983657836914 and perplexity is 49.70464382119004
At time: 902.564065694809 and batch: 600, loss is 3.930246877670288 and perplexity is 50.91954701673158
At time: 904.1765818595886 and batch: 650, loss is 3.9121106624603272 and perplexity is 50.00438304370856
At time: 905.7909138202667 and batch: 700, loss is 3.907435140609741 and perplexity is 49.77113216786096
At time: 907.4331269264221 and batch: 750, loss is 3.9037655639648436 and perplexity is 49.58882787802241
At time: 909.0454382896423 and batch: 800, loss is 3.8392720794677735 and perplexity is 46.49161991486967
At time: 910.647310256958 and batch: 850, loss is 3.855573129653931 and perplexity is 47.255692820411376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.466020584106445 and perplexity of 87.00978504454137
Finished 31 epochs...
Completing Train Step...
At time: 914.7611725330353 and batch: 50, loss is 3.9478166770935057 and perplexity is 51.82209885404702
At time: 916.3269655704498 and batch: 100, loss is 3.9303094959259033 and perplexity is 50.92273560977352
At time: 917.9075067043304 and batch: 150, loss is 3.933882431983948 and perplexity is 51.105004712029846
At time: 919.5127313137054 and batch: 200, loss is 3.9782994318008424 and perplexity is 53.42610220094109
At time: 921.113002538681 and batch: 250, loss is 3.9420081520080568 and perplexity is 51.52196141465561
At time: 922.7203929424286 and batch: 300, loss is 3.9417145156860354 and perplexity is 51.50683491635583
At time: 924.3213379383087 and batch: 350, loss is 3.8720169639587403 and perplexity is 48.03918172949276
At time: 925.923098564148 and batch: 400, loss is 3.879281916618347 and perplexity is 48.38945492906677
At time: 927.5245788097382 and batch: 450, loss is 3.9136305475234985 and perplexity is 50.08044174418955
At time: 929.1256017684937 and batch: 500, loss is 3.890516829490662 and perplexity is 48.936171645787304
At time: 930.7368793487549 and batch: 550, loss is 3.905550060272217 and perplexity is 49.67739796124719
At time: 932.349710226059 and batch: 600, loss is 3.929750151634216 and perplexity is 50.894260232805294
At time: 933.9613213539124 and batch: 650, loss is 3.9116619491577147 and perplexity is 49.981950445127
At time: 935.5749366283417 and batch: 700, loss is 3.9071478033065796 and perplexity is 49.75683311939195
At time: 937.191083908081 and batch: 750, loss is 3.903725752830505 and perplexity is 49.586853729830864
At time: 938.8034870624542 and batch: 800, loss is 3.8393789196014403 and perplexity is 46.49658735111182
At time: 940.4151978492737 and batch: 850, loss is 3.8557676315307616 and perplexity is 47.26488503527843
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.466014544169108 and perplexity of 87.00925951247909
Finished 32 epochs...
Completing Train Step...
At time: 944.5202612876892 and batch: 50, loss is 3.9465933990478517 and perplexity is 51.75874477596586
At time: 946.1299209594727 and batch: 100, loss is 3.9289841747283933 and perplexity is 50.85529133137262
At time: 947.7014393806458 and batch: 150, loss is 3.9326210165023805 and perplexity is 51.040580709160274
At time: 949.2996468544006 and batch: 200, loss is 3.977394242286682 and perplexity is 53.37776333466439
At time: 950.88343501091 and batch: 250, loss is 3.941119365692139 and perplexity is 51.47618974401135
At time: 952.4752621650696 and batch: 300, loss is 3.940824990272522 and perplexity is 51.461038649219525
At time: 954.0797257423401 and batch: 350, loss is 3.871077284812927 and perplexity is 47.9940615148221
At time: 955.6891231536865 and batch: 400, loss is 3.8784343338012697 and perplexity is 48.34845823504218
At time: 957.29927110672 and batch: 450, loss is 3.9128863430023193 and perplexity is 50.04318551786641
At time: 958.922390460968 and batch: 500, loss is 3.8899079179763794 and perplexity is 48.90638291767746
At time: 960.5405361652374 and batch: 550, loss is 3.904983606338501 and perplexity is 49.6492659722454
At time: 962.1587536334991 and batch: 600, loss is 3.929237232208252 and perplexity is 50.868162271709586
At time: 963.7759969234467 and batch: 650, loss is 3.9111737823486328 and perplexity is 49.9575568704177
At time: 965.3936111927032 and batch: 700, loss is 3.9068193531036375 and perplexity is 49.74049316103434
At time: 967.0107407569885 and batch: 750, loss is 3.9036187744140625 and perplexity is 49.581549290477795
At time: 968.6276066303253 and batch: 800, loss is 3.8394025421142577 and perplexity is 46.49768573031567
At time: 970.245157957077 and batch: 850, loss is 3.8558884906768798 and perplexity is 47.27059777413658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.466013590494792 and perplexity of 87.00917653402257
Finished 33 epochs...
Completing Train Step...
At time: 974.3740200996399 and batch: 50, loss is 3.9454191446304323 and perplexity is 51.69800251168834
At time: 975.9413104057312 and batch: 100, loss is 3.927739825248718 and perplexity is 50.79204893204945
At time: 977.506561756134 and batch: 150, loss is 3.9314610147476197 and perplexity is 50.98140787290585
At time: 979.0917656421661 and batch: 200, loss is 3.976539731025696 and perplexity is 53.33217091719998
At time: 980.6919133663177 and batch: 250, loss is 3.940279040336609 and perplexity is 51.43295116634437
At time: 982.2926394939423 and batch: 300, loss is 3.939976758956909 and perplexity is 51.41740629248508
At time: 983.8943860530853 and batch: 350, loss is 3.870173759460449 and perplexity is 47.950717247745864
At time: 985.491453409195 and batch: 400, loss is 3.8776268053054808 and perplexity is 48.30943123711719
At time: 987.0986604690552 and batch: 450, loss is 3.9121732902526856 and perplexity is 50.00751480589347
At time: 988.7007346153259 and batch: 500, loss is 3.889202313423157 and perplexity is 48.87188652304691
At time: 990.3379900455475 and batch: 550, loss is 3.9044205808639525 and perplexity is 49.62132003858492
At time: 991.9408276081085 and batch: 600, loss is 3.928716096878052 and perplexity is 50.841659981407155
At time: 993.5467250347137 and batch: 650, loss is 3.9106744050979616 and perplexity is 49.93261543112951
At time: 995.1499254703522 and batch: 700, loss is 3.9064485311508177 and perplexity is 49.72205171368423
At time: 996.7563464641571 and batch: 750, loss is 3.9034811210632325 and perplexity is 49.574724693803695
At time: 998.3589842319489 and batch: 800, loss is 3.8393853521347046 and perplexity is 46.49688644291859
At time: 999.9611048698425 and batch: 850, loss is 3.85596314907074 and perplexity is 47.2741270527867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.466012954711914 and perplexity of 87.0091212150955
Finished 34 epochs...
Completing Train Step...
At time: 1004.1045639514923 and batch: 50, loss is 3.944291286468506 and perplexity is 51.639727366826605
At time: 1005.6737220287323 and batch: 100, loss is 3.926548490524292 and perplexity is 50.731574630152316
At time: 1007.2710461616516 and batch: 150, loss is 3.9303342866897584 and perplexity is 50.9239980389351
At time: 1008.876567363739 and batch: 200, loss is 3.9757332134246828 and perplexity is 53.28917492349851
At time: 1010.4857761859894 and batch: 250, loss is 3.939489526748657 and perplexity is 51.392360178206886
At time: 1012.0888357162476 and batch: 300, loss is 3.9391766595840454 and perplexity is 51.37628371122568
At time: 1013.6956377029419 and batch: 350, loss is 3.8693011379241944 and perplexity is 47.90889267036412
At time: 1015.3032121658325 and batch: 400, loss is 3.8768636417388915 and perplexity is 48.27257730385265
At time: 1016.905980348587 and batch: 450, loss is 3.9114477396011353 and perplexity is 49.97124498033254
At time: 1018.5095725059509 and batch: 500, loss is 3.888574357032776 and perplexity is 48.84120674338481
At time: 1020.1133441925049 and batch: 550, loss is 3.9039043378829956 and perplexity is 49.595709991481485
At time: 1021.7229926586151 and batch: 600, loss is 3.928219060897827 and perplexity is 50.81639612614468
At time: 1023.3248090744019 and batch: 650, loss is 3.9101960945129393 and perplexity is 49.90873784353763
At time: 1024.926041841507 and batch: 700, loss is 3.906071882247925 and perplexity is 49.703327483908346
At time: 1026.530513048172 and batch: 750, loss is 3.9033287000656127 and perplexity is 49.56716904064386
At time: 1028.1360619068146 and batch: 800, loss is 3.8394032669067384 and perplexity is 46.49771943150087
At time: 1029.7356531620026 and batch: 850, loss is 3.8560058879852295 and perplexity is 47.27614754083681
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.466036478678386 and perplexity of 87.0111680388203
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 1033.833981513977 and batch: 50, loss is 3.944374957084656 and perplexity is 51.64404827539721
At time: 1035.4348838329315 and batch: 100, loss is 3.927630748748779 and perplexity is 50.78650901527007
At time: 1037.030701637268 and batch: 150, loss is 3.9316234588623047 and perplexity is 50.989690175260584
At time: 1038.6287467479706 and batch: 200, loss is 3.9784054708480836 and perplexity is 53.43176775429591
At time: 1040.222846031189 and batch: 250, loss is 3.942441325187683 and perplexity is 51.54428418096496
At time: 1041.8196432590485 and batch: 300, loss is 3.9432973527908324 and perplexity is 51.588426401792624
At time: 1043.4275126457214 and batch: 350, loss is 3.871601972579956 and perplexity is 48.019250019261186
At time: 1045.043741941452 and batch: 400, loss is 3.8795925521850587 and perplexity is 48.404488749720336
At time: 1046.6541035175323 and batch: 450, loss is 3.913071813583374 and perplexity is 50.05246791734164
At time: 1048.2675476074219 and batch: 500, loss is 3.889627366065979 and perplexity is 48.89266406303455
At time: 1049.8852384090424 and batch: 550, loss is 3.9033029460906983 and perplexity is 49.56589250545381
At time: 1051.5071861743927 and batch: 600, loss is 3.925457696914673 and perplexity is 50.67626712276154
At time: 1053.124202489853 and batch: 650, loss is 3.9050007820129395 and perplexity is 49.65011873919727
At time: 1054.7431619167328 and batch: 700, loss is 3.9011104774475096 and perplexity is 49.45733988295805
At time: 1056.3623824119568 and batch: 750, loss is 3.8987577629089354 and perplexity is 49.34111765285152
At time: 1057.9804060459137 and batch: 800, loss is 3.8364633321762085 and perplexity is 46.36121991939815
At time: 1059.5983755588531 and batch: 850, loss is 3.8559227991104126 and perplexity is 47.27221958211906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.466068585713704 and perplexity of 87.01396175431434
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 1063.714349269867 and batch: 50, loss is 3.945461382865906 and perplexity is 51.70018619020898
At time: 1065.3075170516968 and batch: 100, loss is 3.9265944480896 and perplexity is 50.7339061833824
At time: 1066.878826379776 and batch: 150, loss is 3.928983211517334 and perplexity is 50.855242347017175
At time: 1068.4532883167267 and batch: 200, loss is 3.9757967472076414 and perplexity is 53.292560693926404
At time: 1070.0486035346985 and batch: 250, loss is 3.940111870765686 and perplexity is 51.42435386059057
At time: 1071.6776847839355 and batch: 300, loss is 3.94198516368866 and perplexity is 51.52077702496426
At time: 1073.281780719757 and batch: 350, loss is 3.8702503442764282 and perplexity is 47.954389685227035
At time: 1074.892193555832 and batch: 400, loss is 3.879022789001465 and perplexity is 48.37691750939477
At time: 1076.4962029457092 and batch: 450, loss is 3.9129707479476927 and perplexity is 50.047409588470074
At time: 1078.101768732071 and batch: 500, loss is 3.8901026105880736 and perplexity is 48.91590555605981
At time: 1079.717157125473 and batch: 550, loss is 3.90366735458374 and perplexity is 49.5839580290632
At time: 1081.321713924408 and batch: 600, loss is 3.924417243003845 and perplexity is 50.62356822258997
At time: 1082.9348504543304 and batch: 650, loss is 3.902374978065491 and perplexity is 49.51991827667415
At time: 1084.5383260250092 and batch: 700, loss is 3.898312735557556 and perplexity is 49.31916439121163
At time: 1086.1408281326294 and batch: 750, loss is 3.8961992168426516 and perplexity is 49.21503749010537
At time: 1087.7451333999634 and batch: 800, loss is 3.8345234966278077 and perplexity is 46.27137394830949
At time: 1089.34890127182 and batch: 850, loss is 3.8557019519805906 and perplexity is 47.261780800833904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.466259956359863 and perplexity of 87.03061526584594
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 1093.6528186798096 and batch: 50, loss is 3.9463743019104003 and perplexity is 51.74740582535854
At time: 1095.2202062606812 and batch: 100, loss is 3.926201500892639 and perplexity is 50.713974353491594
At time: 1096.7861082553864 and batch: 150, loss is 3.927548861503601 and perplexity is 50.78235041822495
At time: 1098.3549590110779 and batch: 200, loss is 3.974115877151489 and perplexity is 53.20305806665619
At time: 1099.9424321651459 and batch: 250, loss is 3.938428020477295 and perplexity is 51.337835809677
At time: 1101.5445506572723 and batch: 300, loss is 3.940770835876465 and perplexity is 51.45825188320952
At time: 1103.1558182239532 and batch: 350, loss is 3.8691358757019043 and perplexity is 47.900975794492226
At time: 1104.7622549533844 and batch: 400, loss is 3.87823842048645 and perplexity is 48.33898705611826
At time: 1106.367775440216 and batch: 450, loss is 3.912585072517395 and perplexity is 50.02811125392772
At time: 1107.9732973575592 and batch: 500, loss is 3.889979281425476 and perplexity is 48.90987317038207
At time: 1109.5858600139618 and batch: 550, loss is 3.9037586784362794 and perplexity is 49.5884864339071
At time: 1111.1906225681305 and batch: 600, loss is 3.9240112113952637 and perplexity is 50.60301762613062
At time: 1112.8436641693115 and batch: 650, loss is 3.9010827255249025 and perplexity is 49.45596736573435
At time: 1114.436250448227 and batch: 700, loss is 3.8972296476364137 and perplexity is 49.26577631718677
At time: 1116.0305404663086 and batch: 750, loss is 3.894590826034546 and perplexity is 49.135944099777404
At time: 1117.6289420127869 and batch: 800, loss is 3.833430166244507 and perplexity is 46.22081169495537
At time: 1119.2472898960114 and batch: 850, loss is 3.8553416872024537 and perplexity is 47.2447571125611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.466336568196614 and perplexity of 87.03728309654915
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 1123.441993713379 and batch: 50, loss is 3.946864585876465 and perplexity is 51.77278296921517
At time: 1125.014771938324 and batch: 100, loss is 3.9261012506484985 and perplexity is 50.70889052001334
At time: 1126.589328289032 and batch: 150, loss is 3.926953868865967 and perplexity is 50.75214428071018
At time: 1128.1873683929443 and batch: 200, loss is 3.973187417984009 and perplexity is 53.1536841240479
At time: 1129.7790117263794 and batch: 250, loss is 3.937554063796997 and perplexity is 51.29298836533542
At time: 1131.3705170154572 and batch: 300, loss is 3.940061092376709 and perplexity is 51.42174268104364
At time: 1132.971497297287 and batch: 350, loss is 3.868527660369873 and perplexity is 47.87185054470433
At time: 1134.5913479328156 and batch: 400, loss is 3.877741665840149 and perplexity is 48.31498040290258
At time: 1136.2192018032074 and batch: 450, loss is 3.91229284286499 and perplexity is 50.013493692312004
At time: 1137.8394129276276 and batch: 500, loss is 3.8898992586135863 and perplexity is 48.90595942139849
At time: 1139.4514389038086 and batch: 550, loss is 3.903772192001343 and perplexity is 49.58915655567277
At time: 1141.056176662445 and batch: 600, loss is 3.9238183975219725 and perplexity is 50.593261602880446
At time: 1142.664743423462 and batch: 650, loss is 3.900444803237915 and perplexity is 49.42442836271325
At time: 1144.2656116485596 and batch: 700, loss is 3.89659227848053 and perplexity is 49.234385835646556
At time: 1145.8770525455475 and batch: 750, loss is 3.8937758731842043 and perplexity is 49.09591693442063
At time: 1147.480176448822 and batch: 800, loss is 3.8328960752487182 and perplexity is 46.19613216675461
At time: 1149.0818030834198 and batch: 850, loss is 3.8550985288619994 and perplexity is 47.23327055240949
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.466339111328125 and perplexity of 87.03750444408786
Annealing...
Model not improving. Stopping early with 87.0091212150955loss at 38 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd1269705c0>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'seq_len': 50, 'lr': 16.112077727517786, 'dropout': 0.9874767116379771, 'wordvec_source': 'glove', 'num_layers': 1, 'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 6.354709920012811, 'batch_size': 50, 'data': 'wikitext'}, 'best_accuracy': -146.22318586008308}, {'params': {'seq_len': 50, 'lr': 15.321074118726964, 'dropout': 0.8450289434344702, 'wordvec_source': 'glove', 'num_layers': 1, 'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 7.572970340928041, 'batch_size': 50, 'data': 'wikitext'}, 'best_accuracy': -161.7751470354575}, {'params': {'seq_len': 50, 'lr': 2.6058867308125433, 'dropout': 0.641160805004905, 'wordvec_source': 'glove', 'num_layers': 1, 'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 2.1345853047413064, 'batch_size': 50, 'data': 'wikitext'}, 'best_accuracy': -77.00270869253269}, {'params': {'seq_len': 50, 'lr': 27.184867592859394, 'dropout': 0.8958463364823845, 'wordvec_source': 'glove', 'num_layers': 1, 'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 7.096489828386398, 'batch_size': 50, 'data': 'wikitext'}, 'best_accuracy': -312.60719472318794}, {'params': {'seq_len': 50, 'lr': 22.871942858806772, 'dropout': 0.3408868476037624, 'wordvec_source': 'glove', 'num_layers': 1, 'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 3.393881574369817, 'batch_size': 50, 'data': 'wikitext'}, 'best_accuracy': -196.68144224481915}, {'params': {'seq_len': 50, 'lr': 9.914190006120485, 'dropout': 1.0, 'wordvec_source': 'glove', 'num_layers': 1, 'tune_wordvecs': True, 'wordvec_dim': 200, 'anneal': 2.0, 'batch_size': 50, 'data': 'wikitext'}, 'best_accuracy': -87.0091212150955}]
