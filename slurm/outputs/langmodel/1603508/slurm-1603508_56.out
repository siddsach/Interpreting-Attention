Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'name': 'lr', 'type': 'continuous', 'domain': [0, 30]}, {'name': 'dropout', 'type': 'continuous', 'domain': [0, 1]}, {'name': 'anneal', 'type': 'continuous', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'anneal': 3.8431223942323567, 'data': 'wikitext', 'lr': 9.119988588126452, 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.37835670774145425, 'num_layers': 1, 'wordvec_source': 'glove', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.9569127559661865 and batch: 50, loss is 7.3923336696624755 and perplexity is 1623.4903858555967
At time: 3.131169557571411 and batch: 100, loss is 6.495056924819946 and perplexity is 661.861900606921
At time: 4.305749177932739 and batch: 150, loss is 6.0620708084106445 and perplexity is 429.263439441125
At time: 5.476842164993286 and batch: 200, loss is 5.916698350906372 and perplexity is 371.1841686554998
At time: 6.6439125537872314 and batch: 250, loss is 5.8654478073120115 and perplexity is 352.64003554171165
At time: 7.810986042022705 and batch: 300, loss is 5.797377080917358 and perplexity is 329.43434607994027
At time: 8.98691725730896 and batch: 350, loss is 5.777137413024902 and perplexity is 322.8337268176776
At time: 10.162436485290527 and batch: 400, loss is 5.711728839874268 and perplexity is 302.3934064119173
At time: 11.330495595932007 and batch: 450, loss is 5.65390287399292 and perplexity is 285.4031876361838
At time: 12.498616933822632 and batch: 500, loss is 5.6322932624816895 and perplexity is 279.30189632232253
At time: 13.677453517913818 and batch: 550, loss is 5.612518520355224 and perplexity is 273.8330243251904
At time: 14.849050283432007 and batch: 600, loss is 5.615905609130859 and perplexity is 274.7620936201521
At time: 16.032422065734863 and batch: 650, loss is 5.5778850746154784 and perplexity is 264.511591521367
At time: 17.21552848815918 and batch: 700, loss is 5.577858238220215 and perplexity is 264.5044930789939
At time: 18.39151406288147 and batch: 750, loss is 5.5495346164703365 and perplexity is 257.1178696362148
At time: 19.56636953353882 and batch: 800, loss is 5.559055948257447 and perplexity is 259.577665866572
At time: 20.741673469543457 and batch: 850, loss is 5.602589387893676 and perplexity is 271.12755366970833
At time: 21.920868158340454 and batch: 900, loss is 5.570105514526367 and perplexity is 262.461791310895
At time: 23.103187322616577 and batch: 950, loss is 5.536242408752441 and perplexity is 253.7228193489412
At time: 24.285531759262085 and batch: 1000, loss is 5.520525617599487 and perplexity is 249.76628424854968
At time: 25.46003293991089 and batch: 1050, loss is 5.509678506851197 and perplexity is 247.07168244468772
At time: 26.632964849472046 and batch: 1100, loss is 5.479505624771118 and perplexity is 239.7281624086622
At time: 27.811163663864136 and batch: 1150, loss is 5.508507595062256 and perplexity is 246.78255260480282
At time: 28.99409055709839 and batch: 1200, loss is 5.49222583770752 and perplexity is 242.79703264331724
At time: 30.176273345947266 and batch: 1250, loss is 5.485353050231933 and perplexity is 241.1340614080897
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.962521295477874 and perplexity of 142.9537706003584
Finished 1 epochs...
Completing Train Step...
At time: 33.09065818786621 and batch: 50, loss is 5.1768397521972656 and perplexity is 177.1221756202884
At time: 34.28297805786133 and batch: 100, loss is 5.161805658340454 and perplexity is 174.47922120102476
At time: 35.465994119644165 and batch: 150, loss is 5.019078121185303 and perplexity is 151.27178524025982
At time: 36.643468141555786 and batch: 200, loss is 5.061417942047119 and perplexity is 157.8141290401602
At time: 37.81327724456787 and batch: 250, loss is 5.0586955356597905 and perplexity is 157.3850791363417
At time: 38.98645758628845 and batch: 300, loss is 5.0381139087677 and perplexity is 154.17894506343563
At time: 40.162209033966064 and batch: 350, loss is 5.021908874511719 and perplexity is 151.70060500465857
At time: 41.331300020217896 and batch: 400, loss is 5.00965835571289 and perplexity is 149.85353079983628
At time: 42.50729250907898 and batch: 450, loss is 4.934885311126709 and perplexity is 139.05719330467332
At time: 43.68133497238159 and batch: 500, loss is 4.938839626312256 and perplexity is 139.60815790171935
At time: 44.84234666824341 and batch: 550, loss is 4.930962104797363 and perplexity is 138.5127119983802
At time: 46.000704288482666 and batch: 600, loss is 4.9420866107940675 and perplexity is 140.06220015913206
At time: 47.160247802734375 and batch: 650, loss is 4.919249944686889 and perplexity is 136.89989217657381
At time: 48.32028269767761 and batch: 700, loss is 4.923320293426514 and perplexity is 137.45825808122274
At time: 49.47370433807373 and batch: 750, loss is 4.924477272033691 and perplexity is 137.61738638147142
At time: 50.62674593925476 and batch: 800, loss is 4.943417110443115 and perplexity is 140.2486768933945
At time: 51.77938652038574 and batch: 850, loss is 4.971279535293579 and perplexity is 144.21129280800838
At time: 52.93583941459656 and batch: 900, loss is 4.9398963832855225 and perplexity is 139.7557677762522
At time: 54.102707624435425 and batch: 950, loss is 4.920233888626099 and perplexity is 137.03466028701916
At time: 55.273154497146606 and batch: 1000, loss is 4.895646286010742 and perplexity is 133.7063912671665
At time: 56.44019842147827 and batch: 1050, loss is 4.875630922317505 and perplexity is 131.05681380055907
At time: 57.60588526725769 and batch: 1100, loss is 4.836765451431274 and perplexity is 126.06094133719051
At time: 58.77430176734924 and batch: 1150, loss is 4.849698448181153 and perplexity is 127.70187530648711
At time: 59.961331367492676 and batch: 1200, loss is 4.852308931350708 and perplexity is 128.03567440159574
At time: 61.14527678489685 and batch: 1250, loss is 4.852264461517334 and perplexity is 128.0299808030867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.736262189210766 and perplexity of 114.0072666846446
Finished 2 epochs...
Completing Train Step...
At time: 64.18630051612854 and batch: 50, loss is 4.777885665893555 and perplexity is 118.85278969032159
At time: 65.35631036758423 and batch: 100, loss is 4.77445611000061 and perplexity is 118.44587557124234
At time: 66.52861547470093 and batch: 150, loss is 4.673829479217529 and perplexity is 107.10712257573508
At time: 67.70996761322021 and batch: 200, loss is 4.738342552185059 and perplexity is 114.24469005882517
At time: 68.89526844024658 and batch: 250, loss is 4.7412275791168215 and perplexity is 114.57476497502223
At time: 70.08114194869995 and batch: 300, loss is 4.734878654479981 and perplexity is 113.84964273587146
At time: 71.26048398017883 and batch: 350, loss is 4.719994792938232 and perplexity is 112.16766860230982
At time: 72.43982362747192 and batch: 400, loss is 4.729400930404663 and perplexity is 113.22771074791488
At time: 73.62900066375732 and batch: 450, loss is 4.642237510681152 and perplexity is 103.77628855540603
At time: 74.80763602256775 and batch: 500, loss is 4.670223531723022 and perplexity is 106.72159542851065
At time: 75.99806571006775 and batch: 550, loss is 4.661106958389282 and perplexity is 105.75308164885122
At time: 77.16984677314758 and batch: 600, loss is 4.684093551635742 and perplexity is 108.21213912207
At time: 78.34584712982178 and batch: 650, loss is 4.684686203002929 and perplexity is 108.27629020200186
At time: 79.52027869224548 and batch: 700, loss is 4.668390569686889 and perplexity is 106.52615796509852
At time: 80.6995849609375 and batch: 750, loss is 4.67056643486023 and perplexity is 106.75819687340739
At time: 81.87153220176697 and batch: 800, loss is 4.700720138549805 and perplexity is 110.02637816556913
At time: 83.04357242584229 and batch: 850, loss is 4.732121267318726 and perplexity is 113.53614760503262
At time: 84.21977877616882 and batch: 900, loss is 4.704851398468017 and perplexity is 110.48186595302137
At time: 85.402272939682 and batch: 950, loss is 4.687084150314331 and perplexity is 108.53624259248447
At time: 86.58455348014832 and batch: 1000, loss is 4.666763029098511 and perplexity is 106.3529233307256
At time: 87.79720187187195 and batch: 1050, loss is 4.6546243476867675 and perplexity is 105.06974289256237
At time: 88.97559833526611 and batch: 1100, loss is 4.613786659240723 and perplexity is 100.86537019080376
At time: 90.15325617790222 and batch: 1150, loss is 4.621957626342773 and perplexity is 101.69291412527903
At time: 91.33102941513062 and batch: 1200, loss is 4.635241889953614 and perplexity is 103.05284242789747
At time: 92.51887154579163 and batch: 1250, loss is 4.659216957092285 and perplexity is 105.55339694898558
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.662092445540602 and perplexity of 105.8573513218014
Finished 3 epochs...
Completing Train Step...
At time: 95.51901507377625 and batch: 50, loss is 4.5863926887512205 and perplexity is 98.13977016516904
At time: 96.69082021713257 and batch: 100, loss is 4.595920343399047 and perplexity is 99.07928056072521
At time: 97.86556077003479 and batch: 150, loss is 4.50924898147583 and perplexity is 90.85356017542226
At time: 99.03838801383972 and batch: 200, loss is 4.57763482093811 and perplexity is 97.28402774000536
At time: 100.21213150024414 and batch: 250, loss is 4.573418369293213 and perplexity is 96.87469790737966
At time: 101.38326930999756 and batch: 300, loss is 4.570549116134644 and perplexity is 96.59713825929204
At time: 102.55913472175598 and batch: 350, loss is 4.5544272232055665 and perplexity is 95.05229589119843
At time: 103.7361946105957 and batch: 400, loss is 4.576794853210449 and perplexity is 97.20234660584771
At time: 104.91393876075745 and batch: 450, loss is 4.482994318008423 and perplexity is 88.4992712935168
At time: 106.10061120986938 and batch: 500, loss is 4.5177782440185545 and perplexity is 91.63178818069463
At time: 107.28355669975281 and batch: 550, loss is 4.512737264633179 and perplexity is 91.17103652093773
At time: 108.46426725387573 and batch: 600, loss is 4.54258734703064 and perplexity is 93.93352460379108
At time: 109.64877486228943 and batch: 650, loss is 4.545834245681763 and perplexity is 94.23901291453406
At time: 110.82281732559204 and batch: 700, loss is 4.520705785751343 and perplexity is 91.90043711325309
At time: 111.99411463737488 and batch: 750, loss is 4.525428895950317 and perplexity is 92.33551966768822
At time: 113.17008352279663 and batch: 800, loss is 4.560494050979615 and perplexity is 95.63071460914495
At time: 114.34227895736694 and batch: 850, loss is 4.600506591796875 and perplexity is 99.53472634790268
At time: 115.52778434753418 and batch: 900, loss is 4.560087022781372 and perplexity is 95.59179813226993
At time: 116.70464849472046 and batch: 950, loss is 4.55058180809021 and perplexity is 94.68748223535044
At time: 117.90332841873169 and batch: 1000, loss is 4.531014776229858 and perplexity is 92.85273804120172
At time: 119.07398176193237 and batch: 1050, loss is 4.520564556121826 and perplexity is 91.88745896503828
At time: 120.2455701828003 and batch: 1100, loss is 4.486560592651367 and perplexity is 88.81544745105431
At time: 121.40829801559448 and batch: 1150, loss is 4.488709383010864 and perplexity is 89.006498419039
At time: 122.58314323425293 and batch: 1200, loss is 4.503011875152588 and perplexity is 90.28866036214785
At time: 123.74988055229187 and batch: 1250, loss is 4.534587306976318 and perplexity is 93.18505054762895
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.663133161781478 and perplexity of 105.96757613296506
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 126.62918043136597 and batch: 50, loss is 4.467719650268554 and perplexity is 87.15774608828949
At time: 127.80861830711365 and batch: 100, loss is 4.450843715667725 and perplexity is 85.69921928204016
At time: 128.97780561447144 and batch: 150, loss is 4.357963981628418 and perplexity is 78.09796355208549
At time: 130.15094017982483 and batch: 200, loss is 4.42033694267273 and perplexity is 83.12428876033195
At time: 131.33128023147583 and batch: 250, loss is 4.404380350112915 and perplexity is 81.80843454686554
At time: 132.51622414588928 and batch: 300, loss is 4.394947519302368 and perplexity is 81.04037759557133
At time: 133.69384574890137 and batch: 350, loss is 4.3640055751800535 and perplexity is 78.57122790095163
At time: 134.8723726272583 and batch: 400, loss is 4.3766020393371585 and perplexity is 79.5672072969892
At time: 136.05053997039795 and batch: 450, loss is 4.27829176902771 and perplexity is 72.1171419909807
At time: 137.22680926322937 and batch: 500, loss is 4.300698127746582 and perplexity is 73.75126353466437
At time: 138.40445756912231 and batch: 550, loss is 4.286546025276184 and perplexity is 72.71487890392186
At time: 139.5797758102417 and batch: 600, loss is 4.300477180480957 and perplexity is 73.73497019470076
At time: 140.74961280822754 and batch: 650, loss is 4.303310279846191 and perplexity is 73.94416488670525
At time: 141.9242022037506 and batch: 700, loss is 4.269678716659546 and perplexity is 71.4986606001558
At time: 143.1060254573822 and batch: 750, loss is 4.257048301696777 and perplexity is 70.6012819097871
At time: 144.27806997299194 and batch: 800, loss is 4.290012259483337 and perplexity is 72.9673630363208
At time: 145.44822907447815 and batch: 850, loss is 4.308846979141236 and perplexity is 74.35470696756619
At time: 146.61865520477295 and batch: 900, loss is 4.2625263929367065 and perplexity is 70.98910346481664
At time: 147.8543815612793 and batch: 950, loss is 4.2428620910644534 and perplexity is 69.6067879821852
At time: 149.03274774551392 and batch: 1000, loss is 4.204091172218323 and perplexity is 66.9597151552503
At time: 150.20982313156128 and batch: 1050, loss is 4.198077945709229 and perplexity is 66.55827939274374
At time: 151.38114762306213 and batch: 1100, loss is 4.138742880821228 and perplexity is 62.72392042226492
At time: 152.55358624458313 and batch: 1150, loss is 4.134955387115479 and perplexity is 62.48680329174122
At time: 153.73599481582642 and batch: 1200, loss is 4.148105530738831 and perplexity is 63.31394029534599
At time: 154.91462182998657 and batch: 1250, loss is 4.19610267162323 and perplexity is 66.4269383082754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.450535683736314 and perplexity of 85.67282525131515
Finished 5 epochs...
Completing Train Step...
At time: 157.8792588710785 and batch: 50, loss is 4.29280795097351 and perplexity is 73.17164269061513
At time: 159.048406124115 and batch: 100, loss is 4.300781526565552 and perplexity is 73.75741455943124
At time: 160.2345540523529 and batch: 150, loss is 4.2157054090499875 and perplexity is 67.74193479450325
At time: 161.40848636627197 and batch: 200, loss is 4.283056859970093 and perplexity is 72.46160678231382
At time: 162.57966876029968 and batch: 250, loss is 4.276393737792969 and perplexity is 71.98039122259038
At time: 163.749409198761 and batch: 300, loss is 4.274644660949707 and perplexity is 71.85460202668746
At time: 164.9223334789276 and batch: 350, loss is 4.2502798748016355 and perplexity is 70.12503583099326
At time: 166.0992558002472 and batch: 400, loss is 4.269606504440308 and perplexity is 71.49349770961545
At time: 167.27353143692017 and batch: 450, loss is 4.177110614776612 and perplexity is 65.17725867596242
At time: 168.44727873802185 and batch: 500, loss is 4.2070488929748535 and perplexity is 67.15805646913417
At time: 169.6262664794922 and batch: 550, loss is 4.193646297454834 and perplexity is 66.26396913151063
At time: 170.79649114608765 and batch: 600, loss is 4.214837169647216 and perplexity is 67.68314410338522
At time: 171.97679662704468 and batch: 650, loss is 4.22497218132019 and perplexity is 68.3726014865933
At time: 173.14873361587524 and batch: 700, loss is 4.194600577354431 and perplexity is 66.32723368656599
At time: 174.31885886192322 and batch: 750, loss is 4.184760060310364 and perplexity is 65.67774032938743
At time: 175.48848056793213 and batch: 800, loss is 4.22591423034668 and perplexity is 68.43704217764176
At time: 176.66116976737976 and batch: 850, loss is 4.250182723999023 and perplexity is 70.11822345839815
At time: 177.86805152893066 and batch: 900, loss is 4.207020144462586 and perplexity is 67.15612580267592
At time: 179.03884601593018 and batch: 950, loss is 4.19404598236084 and perplexity is 66.2904591332588
At time: 180.20872449874878 and batch: 1000, loss is 4.161819367408753 and perplexity is 64.18819834582483
At time: 181.3813750743866 and batch: 1050, loss is 4.162786159515381 and perplexity is 64.25028499693042
At time: 182.55975437164307 and batch: 1100, loss is 4.108907861709595 and perplexity is 60.880191660048965
At time: 183.7345871925354 and batch: 1150, loss is 4.112026028633117 and perplexity is 61.0703225358321
At time: 184.9124617576599 and batch: 1200, loss is 4.132266788482666 and perplexity is 62.31902700046099
At time: 186.08162689208984 and batch: 1250, loss is 4.176143898963928 and perplexity is 65.1142812349216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.440412758040602 and perplexity of 84.80994043515862
Finished 6 epochs...
Completing Train Step...
At time: 189.042560338974 and batch: 50, loss is 4.22791524887085 and perplexity is 68.57412307174262
At time: 190.24222493171692 and batch: 100, loss is 4.235257387161255 and perplexity is 69.07945661075942
At time: 191.4224648475647 and batch: 150, loss is 4.153914141654968 and perplexity is 63.68277651609554
At time: 192.59416460990906 and batch: 200, loss is 4.222497501373291 and perplexity is 68.20361036648754
At time: 193.77079486846924 and batch: 250, loss is 4.215576539039612 and perplexity is 67.73320545315063
At time: 194.94287729263306 and batch: 300, loss is 4.217001905441284 and perplexity is 67.82981892692368
At time: 196.10942602157593 and batch: 350, loss is 4.193233876228333 and perplexity is 66.23664609877518
At time: 197.2695541381836 and batch: 400, loss is 4.215526714324951 and perplexity is 67.7298307495884
At time: 198.4280378818512 and batch: 450, loss is 4.123251852989196 and perplexity is 61.75974970406764
At time: 199.58646082878113 and batch: 500, loss is 4.156476230621338 and perplexity is 63.84614665019668
At time: 200.74518418312073 and batch: 550, loss is 4.144390950202942 and perplexity is 63.079191831036304
At time: 201.9184741973877 and batch: 600, loss is 4.166498427391052 and perplexity is 64.48924252869023
At time: 203.10306930541992 and batch: 650, loss is 4.181411962509156 and perplexity is 65.45821253673334
At time: 204.27648496627808 and batch: 700, loss is 4.151024479866027 and perplexity is 63.49902045448399
At time: 205.44996786117554 and batch: 750, loss is 4.143263425827026 and perplexity is 63.008108586210135
At time: 206.66358351707458 and batch: 800, loss is 4.185786318778992 and perplexity is 65.74517726454499
At time: 207.83874654769897 and batch: 850, loss is 4.2125860214233395 and perplexity is 67.53095068299662
At time: 209.01675534248352 and batch: 900, loss is 4.170854630470276 and perplexity is 64.7707835452703
At time: 210.19195318222046 and batch: 950, loss is 4.159512019157409 and perplexity is 64.0402645516369
At time: 211.36613202095032 and batch: 1000, loss is 4.129864501953125 and perplexity is 62.16949851840326
At time: 212.540855884552 and batch: 1050, loss is 4.133290338516235 and perplexity is 62.38284629818974
At time: 213.71515107154846 and batch: 1100, loss is 4.08221930027008 and perplexity is 59.27687716930891
At time: 214.89339399337769 and batch: 1150, loss is 4.087736930847168 and perplexity is 59.604849061090796
At time: 216.07080006599426 and batch: 1200, loss is 4.110047373771668 and perplexity is 60.9496049138959
At time: 217.24804210662842 and batch: 1250, loss is 4.151524934768677 and perplexity is 63.53080680373762
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.436785398608577 and perplexity of 84.50286157719272
Finished 7 epochs...
Completing Train Step...
At time: 220.22078037261963 and batch: 50, loss is 4.180297269821167 and perplexity is 65.38528739796033
At time: 221.39073824882507 and batch: 100, loss is 4.187859125137329 and perplexity is 65.88159562158414
At time: 222.56196808815002 and batch: 150, loss is 4.109028105735779 and perplexity is 60.88751257954863
At time: 223.73032879829407 and batch: 200, loss is 4.1771875 and perplexity is 65.18227003670286
At time: 224.90089178085327 and batch: 250, loss is 4.1725644826889035 and perplexity is 64.88162674896124
At time: 226.06954789161682 and batch: 300, loss is 4.1748039388656615 and perplexity is 65.02708912625883
At time: 227.24853038787842 and batch: 350, loss is 4.149749817848206 and perplexity is 63.41813222846851
At time: 228.43651509284973 and batch: 400, loss is 4.175313568115234 and perplexity is 65.06023727880874
At time: 229.6114227771759 and batch: 450, loss is 4.084147191047668 and perplexity is 59.39126674400097
At time: 230.78479385375977 and batch: 500, loss is 4.117663192749023 and perplexity is 61.41555812719152
At time: 231.9673216342926 and batch: 550, loss is 4.106411714553833 and perplexity is 60.72841524972869
At time: 233.14564418792725 and batch: 600, loss is 4.129681487083435 and perplexity is 62.15812161683604
At time: 234.32844305038452 and batch: 650, loss is 4.146258969306945 and perplexity is 63.19713509227751
At time: 235.50778460502625 and batch: 700, loss is 4.116852226257325 and perplexity is 61.36577235750542
At time: 236.71281957626343 and batch: 750, loss is 4.109282221794128 and perplexity is 60.90298704031927
At time: 237.89123225212097 and batch: 800, loss is 4.153200120925903 and perplexity is 63.63732192327055
At time: 239.08096027374268 and batch: 850, loss is 4.181552896499634 and perplexity is 65.4674384739445
At time: 240.26522135734558 and batch: 900, loss is 4.139242386817932 and perplexity is 62.75525922296043
At time: 241.44400811195374 and batch: 950, loss is 4.129668116569519 and perplexity is 62.157290536362
At time: 242.62640404701233 and batch: 1000, loss is 4.102031679153442 and perplexity is 60.463004320078205
At time: 243.79754519462585 and batch: 1050, loss is 4.107212181091309 and perplexity is 60.777045774967476
At time: 244.9686200618744 and batch: 1100, loss is 4.057634739875794 and perplexity is 57.83734879230591
At time: 246.13794445991516 and batch: 1150, loss is 4.06295684337616 and perplexity is 58.14598571909365
At time: 247.31160807609558 and batch: 1200, loss is 4.087686343193054 and perplexity is 59.60183386786941
At time: 248.48889470100403 and batch: 1250, loss is 4.127133479118347 and perplexity is 61.999943832567304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.434955680457345 and perplexity of 84.34838652353554
Finished 8 epochs...
Completing Train Step...
At time: 251.45435523986816 and batch: 50, loss is 4.140420680046081 and perplexity is 62.82924690096338
At time: 252.65801882743835 and batch: 100, loss is 4.1487682056427 and perplexity is 63.35591075949552
At time: 253.83667635917664 and batch: 150, loss is 4.070662837028504 and perplexity is 58.59578918140405
At time: 255.0126757621765 and batch: 200, loss is 4.139668016433716 and perplexity is 62.78197540502778
At time: 256.18863797187805 and batch: 250, loss is 4.135685567855835 and perplexity is 62.5324466139407
At time: 257.3642315864563 and batch: 300, loss is 4.140459208488465 and perplexity is 62.83166766061634
At time: 258.54367184638977 and batch: 350, loss is 4.114038281440735 and perplexity is 61.19333518858668
At time: 259.72193479537964 and batch: 400, loss is 4.141638054847717 and perplexity is 62.905780218360526
At time: 260.896281003952 and batch: 450, loss is 4.051066727638244 and perplexity is 57.45871716736507
At time: 262.0709526538849 and batch: 500, loss is 4.084808759689331 and perplexity is 59.430571143512985
At time: 263.2453601360321 and batch: 550, loss is 4.073932995796204 and perplexity is 58.787720366828665
At time: 264.4271025657654 and batch: 600, loss is 4.098681468963623 and perplexity is 60.26077948389743
At time: 265.606552362442 and batch: 650, loss is 4.116571888923645 and perplexity is 61.348571651616204
At time: 266.82858777046204 and batch: 700, loss is 4.0874442911148074 and perplexity is 59.58740886598565
At time: 268.0070264339447 and batch: 750, loss is 4.079907479286194 and perplexity is 59.13999792193062
At time: 269.1933090686798 and batch: 800, loss is 4.124430994987488 and perplexity is 61.83261617026033
At time: 270.37605261802673 and batch: 850, loss is 4.153645606040954 and perplexity is 63.665677718522495
At time: 271.5578269958496 and batch: 900, loss is 4.1117463970184325 and perplexity is 61.05324773037105
At time: 272.7383940219879 and batch: 950, loss is 4.103126845359802 and perplexity is 60.52925763171658
At time: 273.9168527126312 and batch: 1000, loss is 4.077032837867737 and perplexity is 58.970235753999724
At time: 275.0923545360565 and batch: 1050, loss is 4.083440699577332 and perplexity is 59.34932213913129
At time: 276.2646322250366 and batch: 1100, loss is 4.033451375961303 and perplexity is 56.455424279482706
At time: 277.4320869445801 and batch: 1150, loss is 4.038508434295654 and perplexity is 56.74164576274167
At time: 278.602397441864 and batch: 1200, loss is 4.065336112976074 and perplexity is 58.28449540587461
At time: 279.77814841270447 and batch: 1250, loss is 4.1037215232849125 and perplexity is 60.56526375001423
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.4344317582401915 and perplexity of 84.30420610441698
Finished 9 epochs...
Completing Train Step...
At time: 282.66088581085205 and batch: 50, loss is 4.10640862941742 and perplexity is 60.72822789457251
At time: 283.8452615737915 and batch: 100, loss is 4.115665154457092 and perplexity is 61.29296999900279
At time: 285.00844955444336 and batch: 150, loss is 4.0377270078659055 and perplexity is 56.697323660561636
At time: 286.184278011322 and batch: 200, loss is 4.1072589302062985 and perplexity is 60.779887114483685
At time: 287.36257100105286 and batch: 250, loss is 4.104342470169067 and perplexity is 60.60288324047174
At time: 288.5410077571869 and batch: 300, loss is 4.109677872657776 and perplexity is 60.927088127234015
At time: 289.71885991096497 and batch: 350, loss is 4.0837082862854 and perplexity is 59.36520535384241
At time: 290.8915641307831 and batch: 400, loss is 4.112205390930176 and perplexity is 61.08127723156454
At time: 292.06595182418823 and batch: 450, loss is 4.021737031936645 and perplexity is 55.79794450811517
At time: 293.2462544441223 and batch: 500, loss is 4.055876488685608 and perplexity is 57.73574555313784
At time: 294.4308066368103 and batch: 550, loss is 4.045194811820984 and perplexity is 57.122313052096914
At time: 295.6077389717102 and batch: 600, loss is 4.0706708002090455 and perplexity is 58.59625579211013
At time: 296.8097276687622 and batch: 650, loss is 4.090721611976623 and perplexity is 59.78301628321614
At time: 297.98598647117615 and batch: 700, loss is 4.061064829826355 and perplexity is 58.03607673367811
At time: 299.1613185405731 and batch: 750, loss is 4.052731719017029 and perplexity is 57.55446512372634
At time: 300.33917355537415 and batch: 800, loss is 4.098829860687256 and perplexity is 60.26972234833966
At time: 301.5134313106537 and batch: 850, loss is 4.12876184463501 and perplexity is 62.10098464650142
At time: 302.6902663707733 and batch: 900, loss is 4.0869450283050535 and perplexity is 59.557666514052165
At time: 303.8636283874512 and batch: 950, loss is 4.078742337226868 and perplexity is 59.07113155030716
At time: 305.03621220588684 and batch: 1000, loss is 4.054688186645508 and perplexity is 57.667178795987404
At time: 306.2086589336395 and batch: 1050, loss is 4.061663455963135 and perplexity is 58.07082904687074
At time: 307.38334941864014 and batch: 1100, loss is 4.011273021697998 and perplexity is 55.217118430310286
At time: 308.56129217147827 and batch: 1150, loss is 4.016225619316101 and perplexity is 55.491264907762094
At time: 309.7468650341034 and batch: 1200, loss is 4.044941730499268 and perplexity is 57.1078582908024
At time: 310.92208528518677 and batch: 1250, loss is 4.081706509590149 and perplexity is 59.24648833138341
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.435373570797217 and perplexity of 84.38364226545141
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 313.90648221969604 and batch: 50, loss is 4.093521590232849 and perplexity is 59.95064199355906
At time: 315.09175300598145 and batch: 100, loss is 4.109212989807129 and perplexity is 60.89877075146499
At time: 316.26978850364685 and batch: 150, loss is 4.029315447807312 and perplexity is 56.22241089622426
At time: 317.4445233345032 and batch: 200, loss is 4.098809285163879 and perplexity is 60.26848228001615
At time: 318.6182150840759 and batch: 250, loss is 4.091012678146362 and perplexity is 59.80041962942092
At time: 319.7921950817108 and batch: 300, loss is 4.090441465377808 and perplexity is 59.76627062026723
At time: 320.9699749946594 and batch: 350, loss is 4.062760109901428 and perplexity is 58.13454758245052
At time: 322.1469147205353 and batch: 400, loss is 4.085160126686096 and perplexity is 59.45145675385396
At time: 323.3079254627228 and batch: 450, loss is 3.991771535873413 and perplexity is 54.150734410370475
At time: 324.4680383205414 and batch: 500, loss is 4.017975549697876 and perplexity is 55.58845577196844
At time: 325.66958594322205 and batch: 550, loss is 4.003258996009826 and perplexity is 54.77637544645644
At time: 326.84292244911194 and batch: 600, loss is 4.024596877098084 and perplexity is 55.957746385132836
At time: 328.00806498527527 and batch: 650, loss is 4.043977756500244 and perplexity is 57.05283432536701
At time: 329.16930961608887 and batch: 700, loss is 4.013842148780823 and perplexity is 55.35916060878084
At time: 330.3385121822357 and batch: 750, loss is 3.9995842504501344 and perplexity is 54.57545559477263
At time: 331.50839400291443 and batch: 800, loss is 4.034212293624878 and perplexity is 56.49839855689147
At time: 332.67499136924744 and batch: 850, loss is 4.0585414409637455 and perplexity is 57.88981376080783
At time: 333.84452724456787 and batch: 900, loss is 4.009781160354614 and perplexity is 55.13480356228286
At time: 335.01636147499084 and batch: 950, loss is 3.9994802236557008 and perplexity is 54.56977858035827
At time: 336.1919867992401 and batch: 1000, loss is 3.9733810949325563 and perplexity is 53.16397976437501
At time: 337.3658938407898 and batch: 1050, loss is 3.972239079475403 and perplexity is 53.103300332739565
At time: 338.5398712158203 and batch: 1100, loss is 3.915054483413696 and perplexity is 50.1518038780801
At time: 339.7182788848877 and batch: 1150, loss is 3.9105774402618407 and perplexity is 49.92777395798726
At time: 340.9014570713043 and batch: 1200, loss is 3.943318848609924 and perplexity is 51.58953534919261
At time: 342.0782160758972 and batch: 1250, loss is 3.982356071472168 and perplexity is 53.643272840310125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.395135809905337 and perplexity of 81.05563817380028
Finished 11 epochs...
Completing Train Step...
At time: 345.0627899169922 and batch: 50, loss is 4.058846154212952 and perplexity is 57.90745624186719
At time: 346.26175022125244 and batch: 100, loss is 4.071101422309876 and perplexity is 58.621494068569916
At time: 347.44409108161926 and batch: 150, loss is 3.990886483192444 and perplexity is 54.1028293600778
At time: 348.6221458911896 and batch: 200, loss is 4.061840515136719 and perplexity is 58.081111930183674
At time: 349.7958424091339 and batch: 250, loss is 4.054767861366272 and perplexity is 57.671773595397475
At time: 350.96906542778015 and batch: 300, loss is 4.05717565536499 and perplexity is 57.810802655255806
At time: 352.1422393321991 and batch: 350, loss is 4.031098413467407 and perplexity is 56.32274294189668
At time: 353.3186695575714 and batch: 400, loss is 4.057615656852722 and perplexity is 57.836245091375496
At time: 354.4989631175995 and batch: 450, loss is 3.9658188915252683 and perplexity is 52.76345925295334
At time: 355.6985242366791 and batch: 500, loss is 3.9935950803756715 and perplexity is 54.24957077326234
At time: 356.86413049697876 and batch: 550, loss is 3.979701771736145 and perplexity is 53.501076314955384
At time: 358.02831768989563 and batch: 600, loss is 4.0031576919555665 and perplexity is 54.77082665860815
At time: 359.19320273399353 and batch: 650, loss is 4.024784188270569 and perplexity is 55.96822887793124
At time: 360.3615679740906 and batch: 700, loss is 3.997179265022278 and perplexity is 54.44436012388466
At time: 361.5354974269867 and batch: 750, loss is 3.98573769569397 and perplexity is 53.82498129277696
At time: 362.70029640197754 and batch: 800, loss is 4.02250470161438 and perplexity is 55.84079534373315
At time: 363.8606915473938 and batch: 850, loss is 4.049559555053711 and perplexity is 57.37218219206
At time: 365.01910638809204 and batch: 900, loss is 4.002191882133484 and perplexity is 54.717953992837586
At time: 366.1862108707428 and batch: 950, loss is 3.9942581605911256 and perplexity is 54.285554519075504
At time: 367.3630139827728 and batch: 1000, loss is 3.970600419044495 and perplexity is 53.01635331350958
At time: 368.5388958454132 and batch: 1050, loss is 3.9714992666244506 and perplexity is 53.06402835747263
At time: 369.71677327156067 and batch: 1100, loss is 3.9168825340270996 and perplexity is 50.24356776288119
At time: 370.88944911956787 and batch: 1150, loss is 3.9141197347640992 and perplexity is 50.10494645050062
At time: 372.06045627593994 and batch: 1200, loss is 3.948555612564087 and perplexity is 51.8604061926328
At time: 373.2374150753021 and batch: 1250, loss is 3.986943254470825 and perplexity is 53.88990960097069
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.393313219947537 and perplexity of 80.90804152658274
Finished 12 epochs...
Completing Train Step...
At time: 376.2526774406433 and batch: 50, loss is 4.042984099388122 and perplexity is 56.99617152713134
At time: 377.4306917190552 and batch: 100, loss is 4.054401669502258 and perplexity is 57.65065852744263
At time: 378.60815143585205 and batch: 150, loss is 3.9740603303909303 and perplexity is 53.20010289120474
At time: 379.7842962741852 and batch: 200, loss is 4.044732689857483 and perplexity is 57.09592167511669
At time: 380.9668061733246 and batch: 250, loss is 4.038100385665894 and perplexity is 56.71849713514194
At time: 382.1398174762726 and batch: 300, loss is 4.041397500038147 and perplexity is 56.90581313867041
At time: 383.3187596797943 and batch: 350, loss is 4.015611581802368 and perplexity is 55.457201648554
At time: 384.49163150787354 and batch: 400, loss is 4.04334059715271 and perplexity is 57.01649415714167
At time: 385.69468450546265 and batch: 450, loss is 3.9525604963302614 and perplexity is 52.06851754422796
At time: 386.86682868003845 and batch: 500, loss is 3.980933651924133 and perplexity is 53.567023842290226
At time: 388.0386452674866 and batch: 550, loss is 3.967773699760437 and perplexity is 52.866702575188505
At time: 389.21050095558167 and batch: 600, loss is 3.992264494895935 and perplexity is 54.177435084102555
At time: 390.3883283138275 and batch: 650, loss is 4.014587512016297 and perplexity is 55.4004386735102
At time: 391.57046270370483 and batch: 700, loss is 3.9882438564300537 and perplexity is 53.96004452190167
At time: 392.7577586174011 and batch: 750, loss is 3.977687315940857 and perplexity is 53.393409243406396
At time: 393.9332318305969 and batch: 800, loss is 4.015106143951416 and perplexity is 55.4291785622938
At time: 395.107652425766 and batch: 850, loss is 4.0435659265518185 and perplexity is 57.02934309707696
At time: 396.28117752075195 and batch: 900, loss is 3.996428146362305 and perplexity is 54.403481303409656
At time: 397.45774722099304 and batch: 950, loss is 3.9899766349792483 and perplexity is 54.0536263844697
At time: 398.64459919929504 and batch: 1000, loss is 3.9673181915283204 and perplexity is 52.842626840724556
At time: 399.8296604156494 and batch: 1050, loss is 3.9690572166442872 and perplexity is 52.93460144588925
At time: 401.0103461742401 and batch: 1100, loss is 3.9154069519042967 and perplexity is 50.16948392434047
At time: 402.19664907455444 and batch: 1150, loss is 3.91289776802063 and perplexity is 50.043757265443375
At time: 403.3715274333954 and batch: 1200, loss is 3.9479834508895872 and perplexity is 51.830742142910836
At time: 404.55095744132996 and batch: 1250, loss is 3.9859695291519164 and perplexity is 53.83746117088472
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.392702422872947 and perplexity of 80.8586382207408
Finished 13 epochs...
Completing Train Step...
At time: 407.53408098220825 and batch: 50, loss is 4.030285224914551 and perplexity is 56.276960549452504
At time: 408.73601365089417 and batch: 100, loss is 4.041590619087219 and perplexity is 56.91680379640896
At time: 409.91388034820557 and batch: 150, loss is 3.9616193532943726 and perplexity is 52.54234170938635
At time: 411.08478689193726 and batch: 200, loss is 4.032159199714661 and perplexity is 56.382521033300144
At time: 412.2642366886139 and batch: 250, loss is 4.025823068618775 and perplexity is 56.026403383979755
At time: 413.4428036212921 and batch: 300, loss is 4.029505195617676 and perplexity is 56.2330799877716
At time: 414.6208167076111 and batch: 350, loss is 4.00405207157135 and perplexity is 54.819834482043554
At time: 415.84522008895874 and batch: 400, loss is 4.032337746620178 and perplexity is 56.39258885671852
At time: 417.03495693206787 and batch: 450, loss is 3.9424855279922486 and perplexity is 51.54656263324169
At time: 418.2197115421295 and batch: 500, loss is 3.9711455059051515 and perplexity is 53.045259708633075
At time: 419.3987326622009 and batch: 550, loss is 3.9586013031005858 and perplexity is 52.38400533855122
At time: 420.5800850391388 and batch: 600, loss is 3.9836701583862304 and perplexity is 53.71381109972828
At time: 421.76214814186096 and batch: 650, loss is 4.006369514465332 and perplexity is 54.94702363779292
At time: 422.9498221874237 and batch: 700, loss is 3.9810764312744142 and perplexity is 53.57467265318405
At time: 424.1396722793579 and batch: 750, loss is 3.9709067726135254 and perplexity is 53.03259755067713
At time: 425.3215584754944 and batch: 800, loss is 4.008621578216553 and perplexity is 55.070907282538585
At time: 426.5047550201416 and batch: 850, loss is 4.038017406463623 and perplexity is 56.71379087475921
At time: 427.6915512084961 and batch: 900, loss is 3.990859560966492 and perplexity is 54.10137281108798
At time: 428.8731780052185 and batch: 950, loss is 3.9854102754592895 and perplexity is 53.807360789582475
At time: 430.05952310562134 and batch: 1000, loss is 3.9632197570800782 and perplexity is 52.62649799602534
At time: 431.24310517311096 and batch: 1050, loss is 3.9655723476409914 and perplexity is 52.75045234821352
At time: 432.4205210208893 and batch: 1100, loss is 3.912573690414429 and perplexity is 50.02754183205483
At time: 433.5980427265167 and batch: 1150, loss is 3.9099423456192017 and perplexity is 49.8960751631605
At time: 434.7675051689148 and batch: 1200, loss is 3.9453829097747803 and perplexity is 51.69612927596825
At time: 435.9393632411957 and batch: 1250, loss is 3.983183822631836 and perplexity is 53.687694504117566
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.392496596287637 and perplexity of 80.8419970759967
Finished 14 epochs...
Completing Train Step...
At time: 438.89605021476746 and batch: 50, loss is 4.01923810005188 and perplexity is 55.658683320060966
At time: 440.08437991142273 and batch: 100, loss is 4.030712132453918 and perplexity is 56.30099073717294
At time: 441.24710845947266 and batch: 150, loss is 3.9511835622787475 and perplexity is 51.99687196634706
At time: 442.41274976730347 and batch: 200, loss is 4.02154462814331 and perplexity is 55.78720980466347
At time: 443.57955026626587 and batch: 250, loss is 4.015493793487549 and perplexity is 55.45066982292115
At time: 444.8041009902954 and batch: 300, loss is 4.019344758987427 and perplexity is 55.66462013257924
At time: 445.97998905181885 and batch: 350, loss is 3.9942037773132326 and perplexity is 54.282602372952944
At time: 447.153217792511 and batch: 400, loss is 4.0228317975997925 and perplexity is 55.85906363129123
At time: 448.3268437385559 and batch: 450, loss is 3.9338274002075195 and perplexity is 51.10219239022041
At time: 449.5021848678589 and batch: 500, loss is 3.962579402923584 and perplexity is 52.592809186827544
At time: 450.6777341365814 and batch: 550, loss is 3.9505345392227174 and perplexity is 51.96313574657591
At time: 451.8531906604767 and batch: 600, loss is 3.9759818601608274 and perplexity is 53.30242675035855
At time: 453.0272128582001 and batch: 650, loss is 3.9989919519424437 and perplexity is 54.54314020499061
At time: 454.20376658439636 and batch: 700, loss is 3.9746308279037477 and perplexity is 53.230462076682294
At time: 455.3768501281738 and batch: 750, loss is 3.964670009613037 and perplexity is 52.702875077686336
At time: 456.5546338558197 and batch: 800, loss is 4.002507743835449 and perplexity is 54.73524002876843
At time: 457.72773575782776 and batch: 850, loss is 4.032623672485352 and perplexity is 56.40871526184507
At time: 458.90361523628235 and batch: 900, loss is 3.985338068008423 and perplexity is 53.803475637492156
At time: 460.07817792892456 and batch: 950, loss is 3.980639114379883 and perplexity is 53.55124866593982
At time: 461.2517886161804 and batch: 1000, loss is 3.958629398345947 and perplexity is 52.385477100708876
At time: 462.42797446250916 and batch: 1050, loss is 3.961556620597839 and perplexity is 52.539045689993905
At time: 463.599609375 and batch: 1100, loss is 3.909034614562988 and perplexity is 49.850803496509194
At time: 464.78079748153687 and batch: 1150, loss is 3.906200680732727 and perplexity is 49.70972960946109
At time: 465.9520936012268 and batch: 1200, loss is 3.9418962001800537 and perplexity is 51.51619375974871
At time: 467.1246294975281 and batch: 1250, loss is 3.979631986618042 and perplexity is 53.49734286629719
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.392442689324818 and perplexity of 80.83763924692583
Finished 15 epochs...
Completing Train Step...
At time: 470.14834356307983 and batch: 50, loss is 4.009294643402099 and perplexity is 55.10798606978756
At time: 471.32649207115173 and batch: 100, loss is 4.021038012504578 and perplexity is 55.75895428968239
At time: 472.50357580184937 and batch: 150, loss is 3.9418916749954223 and perplexity is 51.515960639987895
At time: 473.68405652046204 and batch: 200, loss is 4.012245841026306 and perplexity is 55.27086084696523
At time: 474.9139106273651 and batch: 250, loss is 4.006263656616211 and perplexity is 54.941207371909144
At time: 476.09432315826416 and batch: 300, loss is 4.010254878997802 and perplexity is 55.1609281339786
At time: 477.2728898525238 and batch: 350, loss is 3.9853295564651487 and perplexity is 53.80301768882989
At time: 478.45394349098206 and batch: 400, loss is 4.014234414100647 and perplexity is 55.38088034729384
At time: 479.62983536720276 and batch: 450, loss is 3.925967049598694 and perplexity is 50.70208579028229
At time: 480.8102753162384 and batch: 500, loss is 3.954752941131592 and perplexity is 52.18280012818139
At time: 481.9895529747009 and batch: 550, loss is 3.9430705881118775 and perplexity is 51.57672929513728
At time: 483.16890811920166 and batch: 600, loss is 3.9688136529922486 and perplexity is 52.92171007104078
At time: 484.3524558544159 and batch: 650, loss is 3.9920676612854002 and perplexity is 54.1667721933875
At time: 485.5399377346039 and batch: 700, loss is 3.9685778760910035 and perplexity is 52.90923382509474
At time: 486.7139503955841 and batch: 750, loss is 3.9586046981811522 and perplexity is 52.384183186771644
At time: 487.8925852775574 and batch: 800, loss is 3.996626262664795 and perplexity is 54.41426058770879
At time: 489.0739760398865 and batch: 850, loss is 4.027308702468872 and perplexity is 56.10969996413537
At time: 490.25310754776 and batch: 900, loss is 3.9798409271240236 and perplexity is 53.5085217960093
At time: 491.4377315044403 and batch: 950, loss is 3.9757668161392212 and perplexity is 53.29096561451732
At time: 492.6292269229889 and batch: 1000, loss is 3.9538663959503175 and perplexity is 52.13655821898194
At time: 493.8107452392578 and batch: 1050, loss is 3.9572295236587522 and perplexity is 52.31219530197372
At time: 494.99183225631714 and batch: 1100, loss is 3.9051027154922484 and perplexity is 49.65518000650036
At time: 496.17242908477783 and batch: 1150, loss is 3.9020388078689576 and perplexity is 49.50327395383416
At time: 497.3489785194397 and batch: 1200, loss is 3.9379428577423097 and perplexity is 51.31293464589415
At time: 498.5260739326477 and batch: 1250, loss is 3.9757215785980224 and perplexity is 53.28855491679224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.392535801351506 and perplexity of 80.84516655378475
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 501.51154804229736 and batch: 50, loss is 4.009210638999939 and perplexity is 55.1033569507994
At time: 502.7102327346802 and batch: 100, loss is 4.0297099351882935 and perplexity is 56.244594303100556
At time: 503.8862431049347 and batch: 150, loss is 3.951035795211792 and perplexity is 51.98918910873638
At time: 505.09441089630127 and batch: 200, loss is 4.02578537940979 and perplexity is 56.02429183294554
At time: 506.26995873451233 and batch: 250, loss is 4.017679286003113 and perplexity is 55.57198936999423
At time: 507.44195652008057 and batch: 300, loss is 4.018131303787231 and perplexity is 55.59711457558038
At time: 508.6182518005371 and batch: 350, loss is 3.9897827339172363 and perplexity is 54.04314634498638
At time: 509.8010129928589 and batch: 400, loss is 4.0120263767242434 and perplexity is 55.258732197016556
At time: 510.97755694389343 and batch: 450, loss is 3.9273872900009157 and perplexity is 50.77414610036781
At time: 512.1542823314667 and batch: 500, loss is 3.9508598232269287 and perplexity is 51.98004127284251
At time: 513.331226348877 and batch: 550, loss is 3.9375066804885863 and perplexity is 51.290557991428436
At time: 514.5079717636108 and batch: 600, loss is 3.9604607582092286 and perplexity is 52.48150166181562
At time: 515.6802051067352 and batch: 650, loss is 3.983881731033325 and perplexity is 53.725176675208246
At time: 516.8567674160004 and batch: 700, loss is 3.9632383823394775 and perplexity is 52.62747818732993
At time: 518.0294859409332 and batch: 750, loss is 3.9506079864501955 and perplexity is 51.96695243498844
At time: 519.2014765739441 and batch: 800, loss is 3.9854005575180054 and perplexity is 53.8068378953504
At time: 520.3769700527191 and batch: 850, loss is 4.010655126571655 and perplexity is 55.183010580563746
At time: 521.5633256435394 and batch: 900, loss is 3.9614267921447754 and perplexity is 52.532225069731396
At time: 522.7369859218597 and batch: 950, loss is 3.952584114074707 and perplexity is 52.06974729969094
At time: 523.9098556041718 and batch: 1000, loss is 3.9328978872299194 and perplexity is 51.05471430837489
At time: 525.0774912834167 and batch: 1050, loss is 3.9374262952804564 and perplexity is 51.286435154958944
At time: 526.2434048652649 and batch: 1100, loss is 3.878902153968811 and perplexity is 48.371081910367245
At time: 527.4099066257477 and batch: 1150, loss is 3.8726746034622193 and perplexity is 48.070784583616536
At time: 528.5780603885651 and batch: 1200, loss is 3.905582938194275 and perplexity is 49.67903127771529
At time: 529.7428822517395 and batch: 1250, loss is 3.951535577774048 and perplexity is 52.01517889295825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.380627263201414 and perplexity of 79.88812857529868
Finished 17 epochs...
Completing Train Step...
At time: 532.7049143314362 and batch: 50, loss is 4.0034755611419675 and perplexity is 54.78823938405445
At time: 533.8710706233978 and batch: 100, loss is 4.0169164657592775 and perplexity is 55.529614095925865
At time: 535.0757348537445 and batch: 150, loss is 3.937772445678711 and perplexity is 51.304191047840305
At time: 536.2569692134857 and batch: 200, loss is 4.011156358718872 and perplexity is 55.21067701252025
At time: 537.4320788383484 and batch: 250, loss is 4.0041882610321045 and perplexity is 54.82730087415071
At time: 538.6131227016449 and batch: 300, loss is 4.006588959693909 and perplexity is 54.95908282307169
At time: 539.786150932312 and batch: 350, loss is 3.9806309747695923 and perplexity is 53.55081278141907
At time: 540.9583315849304 and batch: 400, loss is 4.0040496063232425 and perplexity is 54.819699337716905
At time: 542.1341025829315 and batch: 450, loss is 3.920134596824646 and perplexity is 50.40722897444877
At time: 543.3115792274475 and batch: 500, loss is 3.943961262702942 and perplexity is 51.62268784142712
At time: 544.4842534065247 and batch: 550, loss is 3.930912070274353 and perplexity is 50.95342959077893
At time: 545.6619522571564 and batch: 600, loss is 3.953835577964783 and perplexity is 52.13495150004297
At time: 546.8414769172668 and batch: 650, loss is 3.9784360933303833 and perplexity is 53.43340399271092
At time: 548.0153849124908 and batch: 700, loss is 3.958011155128479 and perplexity is 52.353100144245666
At time: 549.1900384426117 and batch: 750, loss is 3.946946520805359 and perplexity is 51.77702514229511
At time: 550.3702840805054 and batch: 800, loss is 3.9821428537368773 and perplexity is 53.63183636243482
At time: 551.5427253246307 and batch: 850, loss is 4.008487544059753 and perplexity is 55.063526394573394
At time: 552.7118196487427 and batch: 900, loss is 3.9599311637878416 and perplexity is 52.45371510976043
At time: 553.8855965137482 and batch: 950, loss is 3.952046298980713 and perplexity is 52.041750932761886
At time: 555.0589511394501 and batch: 1000, loss is 3.933744931221008 and perplexity is 51.09797821797712
At time: 556.2370002269745 and batch: 1050, loss is 3.939161558151245 and perplexity is 51.37550786158793
At time: 557.4178993701935 and batch: 1100, loss is 3.8806602716445924 and perplexity is 48.456198765269555
At time: 558.5981085300446 and batch: 1150, loss is 3.8753662300109863 and perplexity is 48.200347472856066
At time: 559.7736146450043 and batch: 1200, loss is 3.9081861066818235 and perplexity is 49.80852263721791
At time: 560.9470987319946 and batch: 1250, loss is 3.9538747596740724 and perplexity is 52.13699427657595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.380040969291743 and perplexity of 79.84130437977159
Finished 18 epochs...
Completing Train Step...
At time: 563.9320075511932 and batch: 50, loss is 3.99898229598999 and perplexity is 54.54261354156484
At time: 565.1358363628387 and batch: 100, loss is 4.011892743110657 and perplexity is 55.25134826633253
At time: 566.3095304965973 and batch: 150, loss is 3.9325432109832765 and perplexity is 51.036609624770975
At time: 567.4957695007324 and batch: 200, loss is 4.005804958343506 and perplexity is 54.91601171403149
At time: 568.6702992916107 and batch: 250, loss is 3.9990668439865114 and perplexity is 54.547225205215526
At time: 569.8472480773926 and batch: 300, loss is 4.001909503936767 and perplexity is 54.70250501699068
At time: 571.0219349861145 and batch: 350, loss is 3.976178140640259 and perplexity is 53.31289000306851
At time: 572.2000753879547 and batch: 400, loss is 3.999683165550232 and perplexity is 54.580854198422955
At time: 573.3787989616394 and batch: 450, loss is 3.9161395740509035 and perplexity is 50.20625266650009
At time: 574.5589492321014 and batch: 500, loss is 3.940209765434265 and perplexity is 51.429388277085884
At time: 575.7349691390991 and batch: 550, loss is 3.927367310523987 and perplexity is 50.773131669621144
At time: 576.9105033874512 and batch: 600, loss is 3.9503317070007324 and perplexity is 51.95259701712421
At time: 578.0957159996033 and batch: 650, loss is 3.975427436828613 and perplexity is 53.27288283198012
At time: 579.2704389095306 and batch: 700, loss is 3.955390005111694 and perplexity is 52.216054501981105
At time: 580.4464590549469 and batch: 750, loss is 3.9450920915603636 and perplexity is 51.68109728585416
At time: 581.6183004379272 and batch: 800, loss is 3.9803620338439942 and perplexity is 53.53641271273381
At time: 582.7887065410614 and batch: 850, loss is 4.0071106004714965 and perplexity is 54.987759200504655
At time: 583.9604024887085 and batch: 900, loss is 3.958948450088501 and perplexity is 52.402193445009964
At time: 585.132682800293 and batch: 950, loss is 3.9513332271575927 and perplexity is 52.00465465427323
At time: 586.3101081848145 and batch: 1000, loss is 3.933780479431152 and perplexity is 51.099794691930754
At time: 587.483250617981 and batch: 1050, loss is 3.939775104522705 and perplexity is 51.407038789872516
At time: 588.6552982330322 and batch: 1100, loss is 3.8811433124542236 and perplexity is 48.47961074076736
At time: 589.8306379318237 and batch: 1150, loss is 3.8762179231643676 and perplexity is 48.24141686556701
At time: 591.0056767463684 and batch: 1200, loss is 3.9088841819763185 and perplexity is 49.84330487522429
At time: 592.1816408634186 and batch: 1250, loss is 3.954398446083069 and perplexity is 52.16430486235166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.37977633511063 and perplexity of 79.82017843701458
Finished 19 epochs...
Completing Train Step...
At time: 595.1828372478485 and batch: 50, loss is 3.995254888534546 and perplexity is 54.3396894225925
At time: 596.3884220123291 and batch: 100, loss is 4.007930316925049 and perplexity is 55.03285205061592
At time: 597.5634398460388 and batch: 150, loss is 3.928431525230408 and perplexity is 50.827193944866835
At time: 598.7360191345215 and batch: 200, loss is 4.001798982620239 and perplexity is 54.696459558201006
At time: 599.9115591049194 and batch: 250, loss is 3.995348162651062 and perplexity is 54.344758145501785
At time: 601.0917391777039 and batch: 300, loss is 3.9984588861465453 and perplexity is 54.51407287063561
At time: 602.2700836658478 and batch: 350, loss is 3.9727871561050416 and perplexity is 53.13241298786282
At time: 603.4444971084595 and batch: 400, loss is 3.996267514228821 and perplexity is 54.39474305797923
At time: 604.6144597530365 and batch: 450, loss is 3.913000473976135 and perplexity is 50.04889732130305
At time: 605.7810366153717 and batch: 500, loss is 3.937183437347412 and perplexity is 51.27398134963793
At time: 606.9460823535919 and batch: 550, loss is 3.9246174097061157 and perplexity is 50.63370238952581
At time: 608.1145656108856 and batch: 600, loss is 3.9476023054122926 and perplexity is 51.81099085425452
At time: 609.282395362854 and batch: 650, loss is 3.9730352926254273 and perplexity is 53.145598715804
At time: 610.4494860172272 and batch: 700, loss is 3.953343005180359 and perplexity is 52.10927756547661
At time: 611.6121001243591 and batch: 750, loss is 3.9435164070129396 and perplexity is 51.59972830222711
At time: 612.7744073867798 and batch: 800, loss is 3.9787841129302977 and perplexity is 53.45200310082989
At time: 613.9402458667755 and batch: 850, loss is 4.00580696105957 and perplexity is 54.916121695320456
At time: 615.1093902587891 and batch: 900, loss is 3.957891221046448 and perplexity is 52.34682159975165
At time: 616.282252073288 and batch: 950, loss is 3.950358123779297 and perplexity is 51.95396945550309
At time: 617.4505219459534 and batch: 1000, loss is 3.9333829402923586 and perplexity is 51.07948456085988
At time: 618.6300456523895 and batch: 1050, loss is 3.9397665643692017 and perplexity is 51.40659976774475
At time: 619.8093061447144 and batch: 1100, loss is 3.881020040512085 and perplexity is 48.47363493332949
At time: 620.9853754043579 and batch: 1150, loss is 3.876318054199219 and perplexity is 48.246247570408144
At time: 622.1574473381042 and batch: 1200, loss is 3.908822565078735 and perplexity is 49.84023378002923
At time: 623.3800604343414 and batch: 1250, loss is 3.9542703151702883 and perplexity is 52.157621430541326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.37960793154083 and perplexity of 79.80673756580089
Finished 20 epochs...
Completing Train Step...
At time: 626.4008040428162 and batch: 50, loss is 3.991928734779358 and perplexity is 54.15924751568378
At time: 627.5878474712372 and batch: 100, loss is 4.004499611854553 and perplexity is 54.84437405710735
At time: 628.7652204036713 and batch: 150, loss is 3.924897165298462 and perplexity is 50.64786943249283
At time: 629.9457247257233 and batch: 200, loss is 3.9984328746795654 and perplexity is 54.51265489807105
At time: 631.1250905990601 and batch: 250, loss is 3.9922698640823366 and perplexity is 54.177725973631205
At time: 632.3089678287506 and batch: 300, loss is 3.9955582189559937 and perplexity is 54.35617480361878
At time: 633.4841589927673 and batch: 350, loss is 3.969896521568298 and perplexity is 52.979048367184134
At time: 634.6615715026855 and batch: 400, loss is 3.9933096981048584 and perplexity is 54.2340911164793
At time: 635.8418121337891 and batch: 450, loss is 3.910285611152649 and perplexity is 49.91320570601257
At time: 637.0218601226807 and batch: 500, loss is 3.934529757499695 and perplexity is 51.13809699514638
At time: 638.1987226009369 and batch: 550, loss is 3.922220392227173 and perplexity is 50.512477866567956
At time: 639.3862187862396 and batch: 600, loss is 3.9452049112319947 and perplexity is 51.68692825919755
At time: 640.569274187088 and batch: 650, loss is 3.970899295806885 and perplexity is 53.03220103768193
At time: 641.7596471309662 and batch: 700, loss is 3.9515073776245115 and perplexity is 52.0137120778176
At time: 642.9430973529816 and batch: 750, loss is 3.9420484733581542 and perplexity is 51.52403889158257
At time: 644.1266422271729 and batch: 800, loss is 3.9772371673583984 and perplexity is 53.36937968476449
At time: 645.3029446601868 and batch: 850, loss is 4.0044451379776005 and perplexity is 54.841386552794646
At time: 646.4836375713348 and batch: 900, loss is 3.956745858192444 and perplexity is 52.286899817408674
At time: 647.6705088615417 and batch: 950, loss is 3.9492530632019043 and perplexity is 51.89658888236262
At time: 648.8535354137421 and batch: 1000, loss is 3.9327347230911256 and perplexity is 51.04638468944943
At time: 650.0335412025452 and batch: 1050, loss is 3.9395031929016113 and perplexity is 51.393062518860894
At time: 651.217187166214 and batch: 1100, loss is 3.8805976629257204 and perplexity is 48.45316507971203
At time: 652.3946783542633 and batch: 1150, loss is 3.8760443115234375 and perplexity is 48.23304232100443
At time: 653.6226553916931 and batch: 1200, loss is 3.9083816957473756 and perplexity is 49.81826559239374
At time: 654.7989308834076 and batch: 1250, loss is 3.953819541931152 and perplexity is 52.13411546891071
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.379487197764599 and perplexity of 79.79710277863965
Finished 21 epochs...
Completing Train Step...
At time: 657.7347440719604 and batch: 50, loss is 3.9888609218597413 and perplexity is 53.993351675250125
At time: 658.9336974620819 and batch: 100, loss is 4.001393280029297 and perplexity is 54.67427356360491
At time: 660.1034021377563 and batch: 150, loss is 3.92172182559967 and perplexity is 50.487300307698064
At time: 661.2727959156036 and batch: 200, loss is 3.99543336391449 and perplexity is 54.34938858481331
At time: 662.444495677948 and batch: 250, loss is 3.9895395946502688 and perplexity is 54.0300079312963
At time: 663.6264083385468 and batch: 300, loss is 3.9929603052139284 and perplexity is 54.21514542053563
At time: 664.803724527359 and batch: 350, loss is 3.9672947406768797 and perplexity is 52.84138765066287
At time: 665.9768300056458 and batch: 400, loss is 3.9906380701065065 and perplexity is 54.0893911784577
At time: 667.1497144699097 and batch: 450, loss is 3.9078163146972655 and perplexity is 49.79010724992515
At time: 668.3248481750488 and batch: 500, loss is 3.932087483406067 and perplexity is 51.01335613334879
At time: 669.5004715919495 and batch: 550, loss is 3.920018558502197 and perplexity is 50.4013801435101
At time: 670.6800720691681 and batch: 600, loss is 3.942987961769104 and perplexity is 51.572467874678615
At time: 671.8535432815552 and batch: 650, loss is 3.9689024829864503 and perplexity is 52.92641131504218
At time: 673.0252244472504 and batch: 700, loss is 3.9497988176345826 and perplexity is 51.92491940583726
At time: 674.1972219944 and batch: 750, loss is 3.9406125259399416 and perplexity is 51.45010617541053
At time: 675.3683001995087 and batch: 800, loss is 3.9757026290893553 and perplexity is 53.28754513442646
At time: 676.5394861698151 and batch: 850, loss is 4.003089089393615 and perplexity is 54.767069368460525
At time: 677.7130658626556 and batch: 900, loss is 3.955543694496155 and perplexity is 52.224080171970826
At time: 678.8883304595947 and batch: 950, loss is 3.9480440044403076 and perplexity is 51.83388077341071
At time: 680.0614566802979 and batch: 1000, loss is 3.9319302654266357 and perplexity is 51.005336547001605
At time: 681.2315180301666 and batch: 1050, loss is 3.939040155410767 and perplexity is 51.36927111272692
At time: 682.4025375843048 and batch: 1100, loss is 3.8799659633636474 and perplexity is 48.42256690199536
At time: 683.600026845932 and batch: 1150, loss is 3.875532169342041 and perplexity is 48.208346469928074
At time: 684.7737278938293 and batch: 1200, loss is 3.907706561088562 and perplexity is 49.78464290584796
At time: 685.9538476467133 and batch: 1250, loss is 3.953174901008606 and perplexity is 52.100518514767934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.379396758810447 and perplexity of 79.78988633844861
Finished 22 epochs...
Completing Train Step...
At time: 688.9164035320282 and batch: 50, loss is 3.9859861373901366 and perplexity is 53.83835532369015
At time: 690.0861396789551 and batch: 100, loss is 3.9985155868530273 and perplexity is 54.5171639447128
At time: 691.2629489898682 and batch: 150, loss is 3.91879026889801 and perplexity is 50.339510656845306
At time: 692.434278011322 and batch: 200, loss is 3.992675471305847 and perplexity is 54.1997053078265
At time: 693.6052362918854 and batch: 250, loss is 3.987047863006592 and perplexity is 53.89554724037412
At time: 694.7723288536072 and batch: 300, loss is 3.9905650424957275 and perplexity is 54.08544130367816
At time: 695.9376578330994 and batch: 350, loss is 3.964889507293701 and perplexity is 52.71444450621514
At time: 697.1042029857635 and batch: 400, loss is 3.988146576881409 and perplexity is 53.954795568437774
At time: 698.2805800437927 and batch: 450, loss is 3.9055163526535033 and perplexity is 49.67572348267952
At time: 699.4598007202148 and batch: 500, loss is 3.9297876739501953 and perplexity is 50.896169939147356
At time: 700.6369791030884 and batch: 550, loss is 3.91794020652771 and perplexity is 50.296737115761154
At time: 701.8125343322754 and batch: 600, loss is 3.9408846950531005 and perplexity is 51.46411121096283
At time: 702.9954044818878 and batch: 650, loss is 3.9669878816604616 and perplexity is 52.825175282005176
At time: 704.1721892356873 and batch: 700, loss is 3.948143439292908 and perplexity is 51.83903512396191
At time: 705.3533108234406 and batch: 750, loss is 3.939197006225586 and perplexity is 51.37732905668863
At time: 706.5304191112518 and batch: 800, loss is 3.974191527366638 and perplexity is 53.20708304168872
At time: 707.7126252651215 and batch: 850, loss is 4.001707558631897 and perplexity is 54.691459218298974
At time: 708.8954246044159 and batch: 900, loss is 3.954301633834839 and perplexity is 52.15925496317057
At time: 710.0726718902588 and batch: 950, loss is 3.94679274559021 and perplexity is 51.76906373126361
At time: 711.2567913532257 and batch: 1000, loss is 3.931034517288208 and perplexity is 50.95966906807288
At time: 712.4383072853088 and batch: 1050, loss is 3.9384518909454345 and perplexity is 51.33906128247729
At time: 713.6587460041046 and batch: 1100, loss is 3.8792187690734865 and perplexity is 48.38639935026802
At time: 714.8347771167755 and batch: 1150, loss is 3.8748774337768555 and perplexity is 48.176793081645364
At time: 716.0180931091309 and batch: 1200, loss is 3.9068882513046264 and perplexity is 49.743920309590266
At time: 717.1985425949097 and batch: 1250, loss is 3.9524066162109377 and perplexity is 52.06050585097121
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.379329040972856 and perplexity of 79.78448332282642
Finished 23 epochs...
Completing Train Step...
At time: 720.1657543182373 and batch: 50, loss is 3.983251690864563 and perplexity is 53.69133831671092
At time: 721.365344285965 and batch: 100, loss is 3.9958012533187866 and perplexity is 54.36938682734888
At time: 722.5382950305939 and batch: 150, loss is 3.916036310195923 and perplexity is 50.20106844298167
At time: 723.7129652500153 and batch: 200, loss is 3.9900844144821166 and perplexity is 54.059452571415775
At time: 724.883572101593 and batch: 250, loss is 3.984710249900818 and perplexity is 53.76970744248616
At time: 726.0577948093414 and batch: 300, loss is 3.988308324813843 and perplexity is 53.963523350897226
At time: 727.2275326251984 and batch: 350, loss is 3.9626181745529174 and perplexity is 52.59484833526123
At time: 728.4117345809937 and batch: 400, loss is 3.985793113708496 and perplexity is 53.82796424902611
At time: 729.5861918926239 and batch: 450, loss is 3.903330717086792 and perplexity is 49.56726901877445
At time: 730.7603545188904 and batch: 500, loss is 3.9275855827331543 and perplexity is 50.78421524281102
At time: 731.9335415363312 and batch: 550, loss is 3.915945611000061 and perplexity is 50.19651545292188
At time: 733.1064932346344 and batch: 600, loss is 3.9388605070114138 and perplexity is 51.36004353428026
At time: 734.2803373336792 and batch: 650, loss is 3.9651330661773683 and perplexity is 52.72728514113408
At time: 735.4540205001831 and batch: 700, loss is 3.946545624732971 and perplexity is 51.75627209646087
At time: 736.626615524292 and batch: 750, loss is 3.9378016901016237 and perplexity is 51.30569143123931
At time: 737.7992508411407 and batch: 800, loss is 3.972685914039612 and perplexity is 53.127034024923915
At time: 738.974378824234 and batch: 850, loss is 4.000331130027771 and perplexity is 54.61623211367564
At time: 740.1517884731293 and batch: 900, loss is 3.953031873703003 and perplexity is 52.093067250864046
At time: 741.3262915611267 and batch: 950, loss is 3.9454988193511964 and perplexity is 51.70212169969791
At time: 742.527163028717 and batch: 1000, loss is 3.9300691795349123 and perplexity is 50.9104995120587
At time: 743.7015399932861 and batch: 1050, loss is 3.9377676963806154 and perplexity is 51.303947389522065
At time: 744.8764255046844 and batch: 1100, loss is 3.8783829736709596 and perplexity is 48.34597511569415
At time: 746.0535714626312 and batch: 1150, loss is 3.874122338294983 and perplexity is 48.14042873386349
At time: 747.2254512310028 and batch: 1200, loss is 3.905967602729797 and perplexity is 49.698144715101996
At time: 748.403801202774 and batch: 1250, loss is 3.951551022529602 and perplexity is 52.015982260885245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.379277807082573 and perplexity of 79.78039575807345
Finished 24 epochs...
Completing Train Step...
At time: 751.4062867164612 and batch: 50, loss is 3.9806287574768064 and perplexity is 53.55069404371985
At time: 752.5829708576202 and batch: 100, loss is 3.9932130718231202 and perplexity is 54.228850931085155
At time: 753.7560403347015 and batch: 150, loss is 3.913410167694092 and perplexity is 50.069406241027174
At time: 754.9281439781189 and batch: 200, loss is 3.987622170448303 and perplexity is 53.926508744089354
At time: 756.1001150608063 and batch: 250, loss is 3.982494945526123 and perplexity is 53.65072301638299
At time: 757.2748262882233 and batch: 300, loss is 3.9861586761474608 and perplexity is 53.847645328033934
At time: 758.4495029449463 and batch: 350, loss is 3.960450701713562 and perplexity is 52.48097388447539
At time: 759.6228277683258 and batch: 400, loss is 3.983539161682129 and perplexity is 53.70677522835753
At time: 760.795473575592 and batch: 450, loss is 3.901234540939331 and perplexity is 49.463476113873384
At time: 761.9713950157166 and batch: 500, loss is 3.925464186668396 and perplexity is 50.67659600032194
At time: 763.1462042331696 and batch: 550, loss is 3.914015917778015 and perplexity is 50.099744975977586
At time: 764.3234362602234 and batch: 600, loss is 3.9368996143341066 and perplexity is 51.25943067875241
At time: 765.4976046085358 and batch: 650, loss is 3.963321180343628 and perplexity is 52.63183581788638
At time: 766.6760032176971 and batch: 700, loss is 3.944977951049805 and perplexity is 51.67519871566303
At time: 767.8478357791901 and batch: 750, loss is 3.9364207935333253 and perplexity is 51.23489247227787
At time: 769.0165853500366 and batch: 800, loss is 3.971193051338196 and perplexity is 53.04778182843403
At time: 770.1844279766083 and batch: 850, loss is 3.9989419317245485 and perplexity is 54.5404120134658
At time: 771.3519675731659 and batch: 900, loss is 3.951738591194153 and perplexity is 52.025739744283676
At time: 772.5646774768829 and batch: 950, loss is 3.944188566207886 and perplexity is 51.634423193000856
At time: 773.7277207374573 and batch: 1000, loss is 3.9290569829940796 and perplexity is 50.85899415173176
At time: 774.8914289474487 and batch: 1050, loss is 3.9370164823532106 and perplexity is 51.26542161694395
At time: 776.0555241107941 and batch: 1100, loss is 3.877494425773621 and perplexity is 48.30303648050238
At time: 777.2223463058472 and batch: 1150, loss is 3.873302311897278 and perplexity is 48.10096849293572
At time: 778.3964262008667 and batch: 1200, loss is 3.904979724884033 and perplexity is 49.64907326125418
At time: 779.5745384693146 and batch: 1250, loss is 3.950635118484497 and perplexity is 51.96836242325229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.379239047530794 and perplexity of 79.77730356561945
Finished 25 epochs...
Completing Train Step...
At time: 782.5698089599609 and batch: 50, loss is 3.978096423149109 and perplexity is 53.41525734080336
At time: 783.7475802898407 and batch: 100, loss is 3.990726270675659 and perplexity is 54.094162103940846
At time: 784.9184944629669 and batch: 150, loss is 3.9108851099014283 and perplexity is 49.9431375815454
At time: 786.0920739173889 and batch: 200, loss is 3.985256576538086 and perplexity is 53.79909129179902
At time: 787.2643389701843 and batch: 250, loss is 3.9803662729263305 and perplexity is 53.536639658476304
At time: 788.4402096271515 and batch: 300, loss is 3.9840877437591553 and perplexity is 53.736245885460754
At time: 789.6082458496094 and batch: 350, loss is 3.9583607387542723 and perplexity is 52.37140513019088
At time: 790.7774548530579 and batch: 400, loss is 3.981376008987427 and perplexity is 53.59072483541086
At time: 791.9489767551422 and batch: 450, loss is 3.8992047691345215 and perplexity is 49.363178369891315
At time: 793.1202485561371 and batch: 500, loss is 3.9234044313430787 and perplexity is 50.572322038131766
At time: 794.2880773544312 and batch: 550, loss is 3.912136673927307 and perplexity is 50.00568374798348
At time: 795.4565858840942 and batch: 600, loss is 3.934987726211548 and perplexity is 51.16152200710571
At time: 796.6261234283447 and batch: 650, loss is 3.9615462112426756 and perplexity is 52.53849879525378
At time: 797.8009428977966 and batch: 700, loss is 3.943451385498047 and perplexity is 51.59637331879908
At time: 798.9764363765717 and batch: 750, loss is 3.935050730705261 and perplexity is 51.16474551444403
At time: 800.1506886482239 and batch: 800, loss is 3.969701523780823 and perplexity is 52.968718577145815
At time: 801.3246200084686 and batch: 850, loss is 3.9975627660751343 and perplexity is 54.465243597475364
At time: 802.5233037471771 and batch: 900, loss is 3.9504397249221803 and perplexity is 51.95820913176684
At time: 803.7014784812927 and batch: 950, loss is 3.9428604507446288 and perplexity is 51.56589223570734
At time: 804.8779346942902 and batch: 1000, loss is 3.9280104303359984 and perplexity is 50.805795378729165
At time: 806.0525252819061 and batch: 1050, loss is 3.9362148952484133 and perplexity is 51.22434438174432
At time: 807.2265648841858 and batch: 1100, loss is 3.8765627479553224 and perplexity is 48.25805457043473
At time: 808.4077479839325 and batch: 1150, loss is 3.872431616783142 and perplexity is 48.059105442305274
At time: 809.5869266986847 and batch: 1200, loss is 3.9039382219314573 and perplexity is 49.59739052339378
At time: 810.7630162239075 and batch: 1250, loss is 3.949675450325012 and perplexity is 51.91851396334972
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.379216326414234 and perplexity of 79.77549095679863
Finished 26 epochs...
Completing Train Step...
At time: 813.745144367218 and batch: 50, loss is 3.9756340837478636 and perplexity is 53.28389264662991
At time: 814.943196773529 and batch: 100, loss is 3.988329539299011 and perplexity is 53.964668171406316
At time: 816.1143901348114 and batch: 150, loss is 3.9084612703323365 and perplexity is 49.82223001793339
At time: 817.2917087078094 and batch: 200, loss is 3.982972722053528 and perplexity is 53.67636219693011
At time: 818.4704058170319 and batch: 250, loss is 3.9783121442794798 and perplexity is 53.426781383441025
At time: 819.6522836685181 and batch: 300, loss is 3.9820869398117065 and perplexity is 53.62883767978452
At time: 820.8268618583679 and batch: 350, loss is 3.956336269378662 and perplexity is 52.26548807344087
At time: 821.9989399909973 and batch: 400, loss is 3.9792794609069824 and perplexity is 53.478487001247345
At time: 823.1738452911377 and batch: 450, loss is 3.8972338151931765 and perplexity is 49.265981635533876
At time: 824.3500561714172 and batch: 500, loss is 3.9213978338241575 and perplexity is 50.47094548718726
At time: 825.522304058075 and batch: 550, loss is 3.910299997329712 and perplexity is 49.913923771392724
At time: 826.696498632431 and batch: 600, loss is 3.9331161308288576 and perplexity is 51.065857888931646
At time: 827.8696119785309 and batch: 650, loss is 3.9597965812683107 and perplexity is 52.44665623163366
At time: 829.0452415943146 and batch: 700, loss is 3.9419330072402956 and perplexity is 51.51808995429232
At time: 830.2198634147644 and batch: 750, loss is 3.9336895418167113 and perplexity is 51.09514800978535
At time: 831.3983414173126 and batch: 800, loss is 3.968224596977234 and perplexity is 52.89054539916843
At time: 832.6187443733215 and batch: 850, loss is 3.996177854537964 and perplexity is 54.389866260761735
At time: 833.7934198379517 and batch: 900, loss is 3.949122290611267 and perplexity is 51.88980267472389
At time: 834.9674024581909 and batch: 950, loss is 3.9415225315093996 and perplexity is 51.496947368220056
At time: 836.1391258239746 and batch: 1000, loss is 3.9269355726242066 and perplexity is 50.75121571570322
At time: 837.3132071495056 and batch: 1050, loss is 3.9353730869293213 and perplexity is 51.18124144725352
At time: 838.4910404682159 and batch: 1100, loss is 3.875604581832886 and perplexity is 48.21183748277041
At time: 839.6691761016846 and batch: 1150, loss is 3.871524438858032 and perplexity is 48.01552705241279
At time: 840.8452496528625 and batch: 1200, loss is 3.902859468460083 and perplexity is 49.543916014287454
At time: 842.02454829216 and batch: 1250, loss is 3.948669099807739 and perplexity is 51.866292021163204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.37918113095917 and perplexity of 79.77268327150061
Finished 27 epochs...
Completing Train Step...
At time: 845.0692365169525 and batch: 50, loss is 3.973260397911072 and perplexity is 53.15756341759195
At time: 846.2505557537079 and batch: 100, loss is 3.9860037326812745 and perplexity is 53.83930263356053
At time: 847.4294543266296 and batch: 150, loss is 3.906129536628723 and perplexity is 49.70619318108725
At time: 848.6082847118378 and batch: 200, loss is 3.9807561111450194 and perplexity is 53.5575143553283
At time: 849.7903156280518 and batch: 250, loss is 3.9763180589675904 and perplexity is 53.32034997534419
At time: 850.965823173523 and batch: 300, loss is 3.980136866569519 and perplexity is 53.52435942165238
At time: 852.1532037258148 and batch: 350, loss is 3.9543653774261474 and perplexity is 52.162579887372075
At time: 853.3341381549835 and batch: 400, loss is 3.9772501945495606 and perplexity is 53.37007494240447
At time: 854.5162773132324 and batch: 450, loss is 3.8953092670440674 and perplexity is 49.171258061040255
At time: 855.6917245388031 and batch: 500, loss is 3.9194381523132322 and perplexity is 50.37213535829058
At time: 856.860554933548 and batch: 550, loss is 3.908497757911682 and perplexity is 49.824047943669996
At time: 858.032496213913 and batch: 600, loss is 3.9312836265563966 and perplexity is 50.97236517523488
At time: 859.2038516998291 and batch: 650, loss is 3.9580725955963136 and perplexity is 52.35631684202782
At time: 860.3746955394745 and batch: 700, loss is 3.9404390716552733 and perplexity is 51.44118270797697
At time: 861.5506567955017 and batch: 750, loss is 3.932342863082886 and perplexity is 51.0263855714076
At time: 862.7646114826202 and batch: 800, loss is 3.966753911972046 and perplexity is 52.81281723796419
At time: 863.9308345317841 and batch: 850, loss is 3.9948059558868407 and perplexity is 54.315300036949886
At time: 865.1003420352936 and batch: 900, loss is 3.947802438735962 and perplexity is 51.821360997727815
At time: 866.2711856365204 and batch: 950, loss is 3.940173420906067 and perplexity is 51.4275191342002
At time: 867.4488184452057 and batch: 1000, loss is 3.9258412742614746 and perplexity is 50.69570911936666
At time: 868.6353986263275 and batch: 1050, loss is 3.9344975662231447 and perplexity is 51.13645082102012
At time: 869.8194732666016 and batch: 1100, loss is 3.874616904258728 and perplexity is 48.16424323983198
At time: 871.0024490356445 and batch: 1150, loss is 3.8705809211730955 and perplexity is 47.97024491909336
At time: 872.1934912204742 and batch: 1200, loss is 3.9017467308044433 and perplexity is 49.488817294226
At time: 873.3742434978485 and batch: 1250, loss is 3.947637939453125 and perplexity is 51.81283712211298
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.379164647011861 and perplexity of 79.77136831363079
Finished 28 epochs...
Completing Train Step...
At time: 876.3467659950256 and batch: 50, loss is 3.9709323263168335 and perplexity is 53.03395274725567
At time: 877.5790112018585 and batch: 100, loss is 3.9837436628341676 and perplexity is 53.717759448868605
At time: 878.7513196468353 and batch: 150, loss is 3.903871846199036 and perplexity is 49.59409856952573
At time: 879.9331936836243 and batch: 200, loss is 3.9785932207107546 and perplexity is 53.44180050314819
At time: 881.1132547855377 and batch: 250, loss is 3.9743754148483275 and perplexity is 53.2168680578407
At time: 882.2939476966858 and batch: 300, loss is 3.978238410949707 and perplexity is 53.422842194177136
At time: 883.4748406410217 and batch: 350, loss is 3.9524425268173218 and perplexity is 52.06237540887326
At time: 884.6474313735962 and batch: 400, loss is 3.97527250289917 and perplexity is 53.26462969427223
At time: 885.8198499679565 and batch: 450, loss is 3.89342725276947 and perplexity is 49.07880407861597
At time: 886.9925880432129 and batch: 500, loss is 3.9175182723999025 and perplexity is 50.27551968234907
At time: 888.1645789146423 and batch: 550, loss is 3.90672634601593 and perplexity is 49.735867157753184
At time: 889.3370478153229 and batch: 600, loss is 3.9294818210601807 and perplexity is 50.88060557880442
At time: 890.5129795074463 and batch: 650, loss is 3.956371374130249 and perplexity is 52.26732287262116
At time: 891.6874663829803 and batch: 700, loss is 3.9389528465270995 and perplexity is 51.36478631479547
At time: 892.9123690128326 and batch: 750, loss is 3.9310051822662353 and perplexity is 50.95817418698732
At time: 894.0849351882935 and batch: 800, loss is 3.965295844078064 and perplexity is 52.73586867650473
At time: 895.2587366104126 and batch: 850, loss is 3.9934337902069093 and perplexity is 54.240821556437346
At time: 896.433210849762 and batch: 900, loss is 3.946472668647766 and perplexity is 51.752496299199265
At time: 897.6161262989044 and batch: 950, loss is 3.9388188886642457 and perplexity is 51.35790605863729
At time: 898.7887496948242 and batch: 1000, loss is 3.92473192691803 and perplexity is 50.63950115197507
At time: 899.9624929428101 and batch: 1050, loss is 3.9335978269577025 and perplexity is 51.0904620403794
At time: 901.1390464305878 and batch: 1100, loss is 3.873615570068359 and perplexity is 48.11603887468993
At time: 902.3232491016388 and batch: 1150, loss is 3.869615702629089 and perplexity is 47.92396548761267
At time: 903.5029833316803 and batch: 1200, loss is 3.9006275653839113 and perplexity is 49.43346210279317
At time: 904.6774842739105 and batch: 1250, loss is 3.946605267524719 and perplexity is 51.75935907707633
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.379176230326186 and perplexity of 79.77229233581568
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 907.7026336193085 and batch: 50, loss is 3.97280960559845 and perplexity is 53.13360579700689
At time: 908.8870606422424 and batch: 100, loss is 3.989874768257141 and perplexity is 54.048120399175
At time: 910.0709798336029 and batch: 150, loss is 3.910179352760315 and perplexity is 49.90790229078919
At time: 911.2469651699066 and batch: 200, loss is 3.9846775722503662 and perplexity is 53.767950403489564
At time: 912.4235887527466 and batch: 250, loss is 3.983346767425537 and perplexity is 53.696443347192684
At time: 913.6020934581757 and batch: 300, loss is 3.9885382080078124 and perplexity is 53.97593008399806
At time: 914.780410528183 and batch: 350, loss is 3.960730404853821 and perplexity is 52.49565503076037
At time: 915.9563477039337 and batch: 400, loss is 3.981796269416809 and perplexity is 53.61325162966965
At time: 917.1335394382477 and batch: 450, loss is 3.899648985862732 and perplexity is 49.38511119058311
At time: 918.3206870555878 and batch: 500, loss is 3.9219759464263917 and perplexity is 50.50013181249844
At time: 919.4989109039307 and batch: 550, loss is 3.909920377731323 and perplexity is 49.89497906381528
At time: 920.6766283512115 and batch: 600, loss is 3.9311416816711424 and perplexity is 50.96513042218918
At time: 921.8821141719818 and batch: 650, loss is 3.9553079128265383 and perplexity is 52.21176814268614
At time: 923.0647730827332 and batch: 700, loss is 3.936861500740051 and perplexity is 51.25747703485043
At time: 924.241781949997 and batch: 750, loss is 3.929198508262634 and perplexity is 50.866192493899135
At time: 925.4208610057831 and batch: 800, loss is 3.9634719610214235 and perplexity is 52.63977228008216
At time: 926.6053531169891 and batch: 850, loss is 3.991523017883301 and perplexity is 54.13727865076013
At time: 927.7895152568817 and batch: 900, loss is 3.9425848531723022 and perplexity is 51.55168275913089
At time: 928.9702081680298 and batch: 950, loss is 3.9318507289886475 and perplexity is 51.00127992554102
At time: 930.1464927196503 and batch: 1000, loss is 3.916468982696533 and perplexity is 50.22279376443386
At time: 931.3245596885681 and batch: 1050, loss is 3.9278560876846313 and perplexity is 50.79795448267351
At time: 932.5030932426453 and batch: 1100, loss is 3.8663249683380125 and perplexity is 47.76651964930859
At time: 933.6786439418793 and batch: 1150, loss is 3.8618035554885863 and perplexity is 47.55103500820545
At time: 934.8564007282257 and batch: 1200, loss is 3.890485153198242 and perplexity is 48.93462155385507
At time: 936.0322885513306 and batch: 1250, loss is 3.937816653251648 and perplexity is 51.306459131740894
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.377896274093294 and perplexity of 79.67025261015065
Finished 30 epochs...
Completing Train Step...
At time: 938.9986579418182 and batch: 50, loss is 3.971188368797302 and perplexity is 53.04753343060787
At time: 940.199224948883 and batch: 100, loss is 3.9858807134628296 and perplexity is 53.832679772006905
At time: 941.3728656768799 and batch: 150, loss is 3.905541920661926 and perplexity is 49.67699360823317
At time: 942.5453417301178 and batch: 200, loss is 3.97904260635376 and perplexity is 53.4658218780569
At time: 943.7181935310364 and batch: 250, loss is 3.9781214809417724 and perplexity is 53.41659582601654
At time: 944.8856992721558 and batch: 300, loss is 3.983277974128723 and perplexity is 53.692749518884426
At time: 946.054684638977 and batch: 350, loss is 3.9560984325408937 and perplexity is 52.253058893150126
At time: 947.2143535614014 and batch: 400, loss is 3.9771267127990724 and perplexity is 53.36348511899673
At time: 948.3736200332642 and batch: 450, loss is 3.8951890897750854 and perplexity is 49.16534914859966
At time: 949.5331420898438 and batch: 500, loss is 3.9184150314331054 and perplexity is 50.32062493001962
At time: 950.6977627277374 and batch: 550, loss is 3.9065278291702272 and perplexity is 49.725994730240735
At time: 951.9080893993378 and batch: 600, loss is 3.9286125755310057 and perplexity is 50.83639705669704
At time: 953.0837202072144 and batch: 650, loss is 3.9540068674087525 and perplexity is 52.14388243176212
At time: 954.2570955753326 and batch: 700, loss is 3.935494246482849 and perplexity is 51.18744291929247
At time: 955.4343585968018 and batch: 750, loss is 3.92808084487915 and perplexity is 50.80937297155603
At time: 956.6087155342102 and batch: 800, loss is 3.9631413793563843 and perplexity is 52.62237341254628
At time: 957.78276014328 and batch: 850, loss is 3.991679573059082 and perplexity is 54.145754785409665
At time: 958.9602606296539 and batch: 900, loss is 3.9427638483047485 and perplexity is 51.56091108530228
At time: 960.1344628334045 and batch: 950, loss is 3.9320812940597536 and perplexity is 51.01304039499819
At time: 961.3069896697998 and batch: 1000, loss is 3.9168973684310915 and perplexity is 50.24431310179169
At time: 962.4825141429901 and batch: 1050, loss is 3.9285400247573854 and perplexity is 50.832708970550875
At time: 963.6556921005249 and batch: 1100, loss is 3.8675830793380737 and perplexity is 47.82665305243018
At time: 964.8299543857574 and batch: 1150, loss is 3.863699278831482 and perplexity is 47.64126411293751
At time: 966.0043263435364 and batch: 1200, loss is 3.8920612859725954 and perplexity is 49.011809828191936
At time: 967.1790707111359 and batch: 1250, loss is 3.9389438581466676 and perplexity is 51.364324630630165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.377682873802463 and perplexity of 79.65325276902315
Finished 31 epochs...
Completing Train Step...
At time: 970.1819479465485 and batch: 50, loss is 3.9700762271881103 and perplexity is 52.98856985541486
At time: 971.3562972545624 and batch: 100, loss is 3.9844945430755616 and perplexity is 53.75811020044587
At time: 972.53062748909 and batch: 150, loss is 3.9040437936782837 and perplexity is 49.60262688295031
At time: 973.70285987854 and batch: 200, loss is 3.9771457290649415 and perplexity is 53.36449990286612
At time: 974.8700370788574 and batch: 250, loss is 3.9764197301864623 and perplexity is 53.325771395913414
At time: 976.0380940437317 and batch: 300, loss is 3.9817247581481934 and perplexity is 53.609417815113105
At time: 977.2106084823608 and batch: 350, loss is 3.9546700286865235 and perplexity is 52.17847370399193
At time: 978.3851344585419 and batch: 400, loss is 3.975630593299866 and perplexity is 53.28370666229809
At time: 979.5601572990417 and batch: 450, loss is 3.8938137006759646 and perplexity is 49.097774144940516
At time: 980.7385859489441 and batch: 500, loss is 3.9171892738342287 and perplexity is 50.25898182909924
At time: 981.9465637207031 and batch: 550, loss is 3.9053415632247925 and perplexity is 49.667041450137056
At time: 983.1305832862854 and batch: 600, loss is 3.927629475593567 and perplexity is 50.78644435620256
At time: 984.3059706687927 and batch: 650, loss is 3.9532372999191283 and perplexity is 52.10376963179292
At time: 985.4806430339813 and batch: 700, loss is 3.934729495048523 and perplexity is 51.14831221343929
At time: 986.6562948226929 and batch: 750, loss is 3.927544102668762 and perplexity is 50.782108753981596
At time: 987.8322925567627 and batch: 800, loss is 3.9628089570999148 and perplexity is 52.60488347161957
At time: 989.0114648342133 and batch: 850, loss is 3.991578640937805 and perplexity is 54.1402900153111
At time: 990.1859345436096 and batch: 900, loss is 3.942703924179077 and perplexity is 51.557821435359884
At time: 991.3603000640869 and batch: 950, loss is 3.9321083498001097 and perplexity is 51.014420609245164
At time: 992.5398700237274 and batch: 1000, loss is 3.9170349550247194 and perplexity is 50.25122652126655
At time: 993.7168426513672 and batch: 1050, loss is 3.9288311195373535 and perplexity is 50.847508260677394
At time: 994.8944036960602 and batch: 1100, loss is 3.868027935028076 and perplexity is 47.84793374424179
At time: 996.0711624622345 and batch: 1150, loss is 3.864354567527771 and perplexity is 47.672493125679686
At time: 997.2477643489838 and batch: 1200, loss is 3.892483658790588 and perplexity is 49.03251545686354
At time: 998.4227633476257 and batch: 1250, loss is 3.939152226448059 and perplexity is 51.37502844283442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.377633867472627 and perplexity of 79.64934935109233
Finished 32 epochs...
Completing Train Step...
At time: 1001.4279720783234 and batch: 50, loss is 3.9691068983078 and perplexity is 52.93723139027594
At time: 1002.6049525737762 and batch: 100, loss is 3.9834705877304075 and perplexity is 53.703092468818035
At time: 1003.7795679569244 and batch: 150, loss is 3.902967834472656 and perplexity is 49.54928518182557
At time: 1004.9542207717896 and batch: 200, loss is 3.97590322971344 and perplexity is 53.29823572146973
At time: 1006.1343371868134 and batch: 250, loss is 3.9753439664840697 and perplexity is 53.26843631167419
At time: 1007.3120613098145 and batch: 300, loss is 3.980775671005249 and perplexity is 53.558561943068646
At time: 1008.489025592804 and batch: 350, loss is 3.9537476825714113 and perplexity is 52.13036927935346
At time: 1009.6630899906158 and batch: 400, loss is 3.97470130443573 and perplexity is 53.23421370724468
At time: 1010.8411152362823 and batch: 450, loss is 3.892963447570801 and perplexity is 49.05604635212372
At time: 1012.0476367473602 and batch: 500, loss is 3.9163572931289674 and perplexity is 50.217184715558325
At time: 1013.227725982666 and batch: 550, loss is 3.904576554298401 and perplexity is 49.629060249911184
At time: 1014.4148681163788 and batch: 600, loss is 3.9269434070587157 and perplexity is 50.751613324336525
At time: 1015.5908308029175 and batch: 650, loss is 3.9526140451431275 and perplexity is 52.07130582618408
At time: 1016.7639744281769 and batch: 700, loss is 3.934108729362488 and perplexity is 51.11657094928168
At time: 1017.9353446960449 and batch: 750, loss is 3.9270924854278566 and perplexity is 50.75917985607128
At time: 1019.1090819835663 and batch: 800, loss is 3.962465314865112 and perplexity is 52.58680931760105
At time: 1020.2834751605988 and batch: 850, loss is 3.9913952445983885 and perplexity is 54.13036179473482
At time: 1021.4570310115814 and batch: 900, loss is 3.9425309228897096 and perplexity is 51.54890263727862
At time: 1022.6354336738586 and batch: 950, loss is 3.931999559402466 and perplexity is 51.00887103201735
At time: 1023.8096964359283 and batch: 1000, loss is 3.9169924736022947 and perplexity is 50.249091823028174
At time: 1024.9769616127014 and batch: 1050, loss is 3.92891610622406 and perplexity is 50.85182980556603
At time: 1026.1431856155396 and batch: 1100, loss is 3.8681868267059327 and perplexity is 47.85553698674638
At time: 1027.3111188411713 and batch: 1150, loss is 3.8646350431442262 and perplexity is 47.68586597286818
At time: 1028.4804706573486 and batch: 1200, loss is 3.892578001022339 and perplexity is 49.03714151201285
At time: 1029.650380373001 and batch: 1250, loss is 3.9391212558746336 and perplexity is 51.37343735338241
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.377605354698905 and perplexity of 79.64707835959342
Finished 33 epochs...
Completing Train Step...
At time: 1032.6042189598083 and batch: 50, loss is 3.9682256507873537 and perplexity is 52.89060113578977
At time: 1033.796588420868 and batch: 100, loss is 3.98259081363678 and perplexity is 53.65586665638551
At time: 1034.9669799804688 and batch: 150, loss is 3.902054200172424 and perplexity is 49.504035929113705
At time: 1036.1501009464264 and batch: 200, loss is 3.9748802042007445 and perplexity is 53.24373814750229
At time: 1037.3279583454132 and batch: 250, loss is 3.974472622871399 and perplexity is 53.222041415820534
At time: 1038.507252216339 and batch: 300, loss is 3.9800102519989013 and perplexity is 53.51758288687969
At time: 1039.686743736267 and batch: 350, loss is 3.952988691329956 and perplexity is 52.09081779716957
At time: 1040.8617420196533 and batch: 400, loss is 3.9739395713806154 and perplexity is 53.19367888731693
At time: 1042.0633828639984 and batch: 450, loss is 3.892274980545044 and perplexity is 49.02228450508904
At time: 1043.2361006736755 and batch: 500, loss is 3.9156579780578613 and perplexity is 50.18207935774178
At time: 1044.4089441299438 and batch: 550, loss is 3.9039375829696654 and perplexity is 49.59735883256638
At time: 1045.580714225769 and batch: 600, loss is 3.926356382369995 and perplexity is 50.721829617063754
At time: 1046.752436876297 and batch: 650, loss is 3.9520600509643553 and perplexity is 52.04246661499045
At time: 1047.9273524284363 and batch: 700, loss is 3.933556523323059 and perplexity is 51.08835186218082
At time: 1049.1028316020966 and batch: 750, loss is 3.926671042442322 and perplexity is 50.73779226291156
At time: 1050.2757775783539 and batch: 800, loss is 3.9621220874786376 and perplexity is 52.56876318161714
At time: 1051.4464213848114 and batch: 850, loss is 3.9911813735961914 and perplexity is 54.11878611790342
At time: 1052.6155726909637 and batch: 900, loss is 3.942314829826355 and perplexity is 51.537764480477705
At time: 1053.7861177921295 and batch: 950, loss is 3.9318254137039186 and perplexity is 50.99998882996046
At time: 1054.955649614334 and batch: 1000, loss is 3.9168573570251466 and perplexity is 50.242302796401596
At time: 1056.1264870166779 and batch: 1050, loss is 3.92889723777771 and perplexity is 50.850870319595586
At time: 1057.2963404655457 and batch: 1100, loss is 3.8682148551940916 and perplexity is 47.856878323895884
At time: 1058.465532541275 and batch: 1150, loss is 3.8647564601898194 and perplexity is 47.69165620134026
At time: 1059.63774061203 and batch: 1200, loss is 3.892535619735718 and perplexity is 49.03506329890234
At time: 1060.812597990036 and batch: 1250, loss is 3.938990845680237 and perplexity is 51.36673817026071
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.377595553432938 and perplexity of 79.64629772122056
Finished 34 epochs...
Completing Train Step...
At time: 1063.7909739017487 and batch: 50, loss is 3.967401614189148 and perplexity is 52.84703529714076
At time: 1064.9672236442566 and batch: 100, loss is 3.981791796684265 and perplexity is 53.613011832470576
At time: 1066.1430296897888 and batch: 150, loss is 3.901226906776428 and perplexity is 49.463098503080346
At time: 1067.317217350006 and batch: 200, loss is 3.9739665174484253 and perplexity is 53.19511226710718
At time: 1068.4910645484924 and batch: 250, loss is 3.9737039899826048 and perplexity is 53.181148922050646
At time: 1069.6678159236908 and batch: 300, loss is 3.979334444999695 and perplexity is 53.48142754817567
At time: 1070.8673357963562 and batch: 350, loss is 3.952308416366577 and perplexity is 52.055393768406276
At time: 1072.043672800064 and batch: 400, loss is 3.973254165649414 and perplexity is 53.15723212677999
At time: 1073.2185809612274 and batch: 450, loss is 3.8916591739654542 and perplexity is 48.99210555289665
At time: 1074.3921687602997 and batch: 500, loss is 3.915019431114197 and perplexity is 50.15004597283962
At time: 1075.573237657547 and batch: 550, loss is 3.9033547401428224 and perplexity is 49.56845979035828
At time: 1076.746966123581 and batch: 600, loss is 3.925815181732178 and perplexity is 50.69438635734842
At time: 1077.9244327545166 and batch: 650, loss is 3.9515423250198363 and perplexity is 52.01552985333897
At time: 1079.1024391651154 and batch: 700, loss is 3.9330391454696656 and perplexity is 51.061926716842905
At time: 1080.2850320339203 and batch: 750, loss is 3.926261558532715 and perplexity is 50.717020206572585
At time: 1081.4580233097076 and batch: 800, loss is 3.961776738166809 and perplexity is 52.55061172990477
At time: 1082.6317963600159 and batch: 850, loss is 3.990948324203491 and perplexity is 54.10617523720108
At time: 1083.8085594177246 and batch: 900, loss is 3.9420752191543578 and perplexity is 51.52541696145505
At time: 1084.9847753047943 and batch: 950, loss is 3.9316127157211302 and perplexity is 50.98914238876307
At time: 1086.1667478084564 and batch: 1000, loss is 3.91666974067688 and perplexity is 50.232877403229004
At time: 1087.3395771980286 and batch: 1050, loss is 3.9288188791275025 and perplexity is 50.84688587014554
At time: 1088.5118653774261 and batch: 1100, loss is 3.8681686878204347 and perplexity is 47.85466894851317
At time: 1089.6836156845093 and batch: 1150, loss is 3.864791975021362 and perplexity is 47.69334999255342
At time: 1090.8589417934418 and batch: 1200, loss is 3.8924201250076296 and perplexity is 49.02940033462739
At time: 1092.033471107483 and batch: 1250, loss is 3.9388080167770387 and perplexity is 51.35734770431062
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.377588425239507 and perplexity of 79.6457299890278
Finished 35 epochs...
Completing Train Step...
At time: 1094.9752955436707 and batch: 50, loss is 3.966618618965149 and perplexity is 52.80567251644368
At time: 1096.169432401657 and batch: 100, loss is 3.9810451316833495 and perplexity is 53.572995814080905
At time: 1097.3373591899872 and batch: 150, loss is 3.9004537010192872 and perplexity is 49.42486813242775
At time: 1098.5051114559174 and batch: 200, loss is 3.9731209659576416 and perplexity is 53.150152071386294
At time: 1099.6740128993988 and batch: 250, loss is 3.9729984521865847 and perplexity is 53.143640844689386
At time: 1100.881403684616 and batch: 300, loss is 3.9787125635147094 and perplexity is 53.44817877806166
At time: 1102.0517411231995 and batch: 350, loss is 3.9516754579544067 and perplexity is 52.0224552944635
At time: 1103.2322828769684 and batch: 400, loss is 3.972613787651062 and perplexity is 53.12320230201117
At time: 1104.4082453250885 and batch: 450, loss is 3.891084761619568 and perplexity is 48.9639719635302
At time: 1105.5831453800201 and batch: 500, loss is 3.9144156265258787 and perplexity is 50.11977428498857
At time: 1106.7567534446716 and batch: 550, loss is 3.9028038072586058 and perplexity is 49.541158417142505
At time: 1107.9292042255402 and batch: 600, loss is 3.9252997970581056 and perplexity is 50.66826597915782
At time: 1109.1102485656738 and batch: 650, loss is 3.9510451984405517 and perplexity is 51.98967797727307
At time: 1110.2890751361847 and batch: 700, loss is 3.932541208267212 and perplexity is 51.03650741303535
At time: 1111.4583961963654 and batch: 750, loss is 3.925856657028198 and perplexity is 50.69648896563201
At time: 1112.625103712082 and batch: 800, loss is 3.9614283275604247 and perplexity is 52.532305728593784
At time: 1113.7911324501038 and batch: 850, loss is 3.990701036453247 and perplexity is 54.09279709704563
At time: 1114.950485944748 and batch: 900, loss is 3.9418189764022826 and perplexity is 51.51221563825496
At time: 1116.1096444129944 and batch: 950, loss is 3.9313735485076906 and perplexity is 50.976948915859865
At time: 1117.2686281204224 and batch: 1000, loss is 3.9164490842819215 and perplexity is 50.2217944204033
At time: 1118.4346561431885 and batch: 1050, loss is 3.928701934814453 and perplexity is 50.84093996368351
At time: 1119.6124708652496 and batch: 1100, loss is 3.8680747365951538 and perplexity is 47.85017315492601
At time: 1120.7898800373077 and batch: 1150, loss is 3.864773449897766 and perplexity is 47.692466475533756
At time: 1121.9633877277374 and batch: 1200, loss is 3.8922592878341673 and perplexity is 49.0215152185879
At time: 1123.142692565918 and batch: 1250, loss is 3.938592915534973 and perplexity is 51.3463018630599
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.377587534215328 and perplexity of 79.64565902278825
Finished 36 epochs...
Completing Train Step...
At time: 1126.1615505218506 and batch: 50, loss is 3.9658668279647826 and perplexity is 52.76598860594999
At time: 1127.339277267456 and batch: 100, loss is 3.9803356552124023 and perplexity is 53.535000514052115
At time: 1128.5189504623413 and batch: 150, loss is 3.899717960357666 and perplexity is 49.38851762116182
At time: 1129.7000699043274 and batch: 200, loss is 3.9723230838775634 and perplexity is 53.1077614311101
At time: 1130.933435201645 and batch: 250, loss is 3.972336378097534 and perplexity is 53.10846746206577
At time: 1132.1096403598785 and batch: 300, loss is 3.9781267738342283 and perplexity is 53.416878555061835
At time: 1133.2873187065125 and batch: 350, loss is 3.95107515335083 and perplexity is 51.9912353467376
At time: 1134.4636597633362 and batch: 400, loss is 3.972003674507141 and perplexity is 53.090801023267645
At time: 1135.6387276649475 and batch: 450, loss is 3.8905371379852296 and perplexity is 48.937165475854904
At time: 1136.8133354187012 and batch: 500, loss is 3.913835105895996 and perplexity is 50.09068716570436
At time: 1137.988787651062 and batch: 550, loss is 3.9022740936279297 and perplexity is 49.514922739560944
At time: 1139.1639680862427 and batch: 600, loss is 3.924800534248352 and perplexity is 50.64297551213988
At time: 1140.3418924808502 and batch: 650, loss is 3.950561079978943 and perplexity is 51.964514905796364
At time: 1141.5201115608215 and batch: 700, loss is 3.932055993080139 and perplexity is 51.011749731430676
At time: 1142.7006685733795 and batch: 750, loss is 3.9254542446136473 and perplexity is 50.67609217333457
At time: 1143.88126206398 and batch: 800, loss is 3.961076145172119 and perplexity is 52.51380803317155
At time: 1145.0554630756378 and batch: 850, loss is 3.9904417610168457 and perplexity is 54.07877398147538
At time: 1146.237519979477 and batch: 900, loss is 3.941550874710083 and perplexity is 51.49840697721879
At time: 1147.414856672287 and batch: 950, loss is 3.931115674972534 and perplexity is 50.9638050046377
At time: 1148.5977790355682 and batch: 1000, loss is 3.9162062931060793 and perplexity is 50.20960249198927
At time: 1149.775815486908 and batch: 1050, loss is 3.9285584688186646 and perplexity is 50.83364654079639
At time: 1150.956215620041 and batch: 1100, loss is 3.8679477405548095 and perplexity is 47.844096758252874
At time: 1152.1311519145966 and batch: 1150, loss is 3.8647177743911745 and perplexity is 47.6898112472184
At time: 1153.3124299049377 and batch: 1200, loss is 3.8920677042007448 and perplexity is 49.01212439817892
At time: 1154.4882655143738 and batch: 1250, loss is 3.9383562183380127 and perplexity is 51.3341497755739
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.377590652799954 and perplexity of 79.64590740490331
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 1157.4853065013885 and batch: 50, loss is 3.966473197937012 and perplexity is 52.79799401957585
At time: 1158.6898756027222 and batch: 100, loss is 3.9825850772857665 and perplexity is 53.655558868383245
At time: 1159.8744592666626 and batch: 150, loss is 3.902653274536133 and perplexity is 49.53370141296717
At time: 1161.0754363536835 and batch: 200, loss is 3.97414457321167 and perplexity is 53.20458480671789
At time: 1162.2462141513824 and batch: 250, loss is 3.975825982093811 and perplexity is 53.29411871864621
At time: 1163.4167459011078 and batch: 300, loss is 3.981712331771851 and perplexity is 53.608751648450855
At time: 1164.5895190238953 and batch: 350, loss is 3.9534443759918214 and perplexity is 52.11456019297574
At time: 1165.7674012184143 and batch: 400, loss is 3.9742382669448855 and perplexity is 53.20956997642854
At time: 1166.941021680832 and batch: 450, loss is 3.892929725646973 and perplexity is 49.0543921157575
At time: 1168.1169996261597 and batch: 500, loss is 3.9163808155059816 and perplexity is 50.21836595700255
At time: 1169.2947397232056 and batch: 550, loss is 3.903908944129944 and perplexity is 49.59593844209543
At time: 1170.47350025177 and batch: 600, loss is 3.9264274072647094 and perplexity is 50.7254322576091
At time: 1171.6459119319916 and batch: 650, loss is 3.950313968658447 and perplexity is 51.95167547234908
At time: 1172.8202085494995 and batch: 700, loss is 3.930367794036865 and perplexity is 50.92570439560104
At time: 1173.9954748153687 and batch: 750, loss is 3.9238321876525877 and perplexity is 50.59395929537683
At time: 1175.1687860488892 and batch: 800, loss is 3.958902587890625 and perplexity is 52.39979022035406
At time: 1176.3394975662231 and batch: 850, loss is 3.9883832502365113 and perplexity is 53.96756674216748
At time: 1177.5121610164642 and batch: 900, loss is 3.9393456602096557 and perplexity is 51.384967069040194
At time: 1178.6859924793243 and batch: 950, loss is 3.928507022857666 and perplexity is 50.83103142226827
At time: 1179.868714094162 and batch: 1000, loss is 3.9116581439971925 and perplexity is 49.981760256144184
At time: 1181.0470216274261 and batch: 1050, loss is 3.9240806341171264 and perplexity is 50.6065307472925
At time: 1182.218005657196 and batch: 1100, loss is 3.8637900066375734 and perplexity is 47.6455866963962
At time: 1183.3893246650696 and batch: 1150, loss is 3.8606189107894897 and perplexity is 47.49473727963323
At time: 1184.5652706623077 and batch: 1200, loss is 3.8880250740051268 and perplexity is 48.814386464107194
At time: 1185.7419924736023 and batch: 1250, loss is 3.93478777885437 and perplexity is 51.151293418614884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.377600454065922 and perplexity of 79.64668803945062
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 1188.7904312610626 and batch: 50, loss is 3.966139106750488 and perplexity is 52.780357621354426
At time: 1189.974881887436 and batch: 100, loss is 3.982438325881958 and perplexity is 53.64768541753129
At time: 1191.1791498661041 and batch: 150, loss is 3.9023412036895753 and perplexity is 49.51824580058254
At time: 1192.368115901947 and batch: 200, loss is 3.973459520339966 and perplexity is 53.16814933464553
At time: 1193.6945881843567 and batch: 250, loss is 3.9753770446777343 and perplexity is 53.27019836446932
At time: 1194.8634719848633 and batch: 300, loss is 3.9813300943374634 and perplexity is 53.58826429252667
At time: 1196.0324110984802 and batch: 350, loss is 3.9527695894241335 and perplexity is 52.07940584994971
At time: 1197.1970162391663 and batch: 400, loss is 3.9736819887161254 and perplexity is 53.17997888229276
At time: 1198.3630933761597 and batch: 450, loss is 3.8923954916000367 and perplexity is 49.02819258830043
At time: 1199.534372329712 and batch: 500, loss is 3.916219916343689 and perplexity is 50.21028651399359
At time: 1200.7110660076141 and batch: 550, loss is 3.9033072566986085 and perplexity is 49.56610616504262
At time: 1201.8871750831604 and batch: 600, loss is 3.9261762857437135 and perplexity is 50.71269560919759
At time: 1203.068324804306 and batch: 650, loss is 3.9501125049591064 and perplexity is 51.94121014984815
At time: 1204.2438971996307 and batch: 700, loss is 3.930011262893677 and perplexity is 50.90755103230719
At time: 1205.422579050064 and batch: 750, loss is 3.9233195304870607 and perplexity is 50.568028586961326
At time: 1206.6082074642181 and batch: 800, loss is 3.958388648033142 and perplexity is 52.37286679873842
At time: 1207.7948787212372 and batch: 850, loss is 3.987739987373352 and perplexity is 53.93286257381519
At time: 1208.9710721969604 and batch: 900, loss is 3.9383862352371217 and perplexity is 51.33569069069519
At time: 1210.1447427272797 and batch: 950, loss is 3.9275444030761717 and perplexity is 50.78212400930563
At time: 1211.3161811828613 and batch: 1000, loss is 3.9102705097198487 and perplexity is 49.912451950782156
At time: 1212.4938473701477 and batch: 1050, loss is 3.9226437044143676 and perplexity is 50.53386494043943
At time: 1213.6732342243195 and batch: 1100, loss is 3.8626089334487914 and perplexity is 47.58934698952782
At time: 1214.858009338379 and batch: 1150, loss is 3.859492073059082 and perplexity is 47.44124855988434
At time: 1216.034435749054 and batch: 1200, loss is 3.8869525718688966 and perplexity is 48.76206099495006
At time: 1217.2086129188538 and batch: 1250, loss is 3.9338135051727297 and perplexity is 51.10148232841248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.3774338325444795 and perplexity of 79.63341829265505
Finished 39 epochs...
Completing Train Step...
At time: 1220.2548863887787 and batch: 50, loss is 3.9659174251556397 and perplexity is 52.76865848428985
At time: 1221.4290194511414 and batch: 100, loss is 3.9821235275268556 and perplexity is 53.630799872317176
At time: 1222.603538274765 and batch: 150, loss is 3.901824049949646 and perplexity is 49.49264387520837
At time: 1223.783542394638 and batch: 200, loss is 3.972914071083069 and perplexity is 53.13915671482065
At time: 1224.9650433063507 and batch: 250, loss is 3.974760718345642 and perplexity is 53.2373766539827
At time: 1226.142736673355 and batch: 300, loss is 3.980789542198181 and perplexity is 53.559304869367146
At time: 1227.32794880867 and batch: 350, loss is 3.9522984218597412 and perplexity is 52.05487350301731
At time: 1228.5054709911346 and batch: 400, loss is 3.973244061470032 and perplexity is 53.15669501928464
At time: 1229.677960395813 and batch: 450, loss is 3.892040500640869 and perplexity is 49.01079111205336
At time: 1230.8498747348785 and batch: 500, loss is 3.9159220314025878 and perplexity is 50.1953318532474
At time: 1232.0283813476562 and batch: 550, loss is 3.90300491809845 and perplexity is 49.55112268304599
At time: 1233.2025108337402 and batch: 600, loss is 3.925880150794983 and perplexity is 50.69768003111183
At time: 1234.3769888877869 and batch: 650, loss is 3.9498764753341673 and perplexity is 51.92895193220577
At time: 1235.5631530284882 and batch: 700, loss is 3.92978554725647 and perplexity is 50.89606169869719
At time: 1236.7415714263916 and batch: 750, loss is 3.923083267211914 and perplexity is 50.55608263016067
At time: 1237.9178433418274 and batch: 800, loss is 3.958293571472168 and perplexity is 52.367887603380986
At time: 1239.093209028244 and batch: 850, loss is 3.987726058959961 and perplexity is 53.93211137984138
At time: 1240.2717351913452 and batch: 900, loss is 3.938421993255615 and perplexity is 51.33752638609249
At time: 1241.4478552341461 and batch: 950, loss is 3.927645764350891 and perplexity is 50.7872716110075
At time: 1242.6210944652557 and batch: 1000, loss is 3.9105066776275637 and perplexity is 49.92424106217839
At time: 1243.8007595539093 and batch: 1050, loss is 3.9229130029678343 and perplexity is 50.547475469734636
At time: 1244.9815683364868 and batch: 1100, loss is 3.8629440307617187 and perplexity is 47.605296724035306
At time: 1246.1592271327972 and batch: 1150, loss is 3.859970016479492 and perplexity is 47.46392821185308
At time: 1247.334722995758 and batch: 1200, loss is 3.8873575019836424 and perplexity is 48.78181022016215
At time: 1248.5085003376007 and batch: 1250, loss is 3.9340746116638186 and perplexity is 51.114826999266974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.37741066591583 and perplexity of 79.63157347619456
Finished 40 epochs...
Completing Train Step...
At time: 1251.4736394882202 and batch: 50, loss is 3.96576229095459 and perplexity is 52.76047289556423
At time: 1252.6778280735016 and batch: 100, loss is 3.981931414604187 and perplexity is 53.62049769223129
At time: 1253.852791070938 and batch: 150, loss is 3.9015133428573607 and perplexity is 49.477268548475614
At time: 1255.03293466568 and batch: 200, loss is 3.972519106864929 and perplexity is 53.118172793557974
At time: 1256.2124290466309 and batch: 250, loss is 3.97431517124176 and perplexity is 53.213662178346524
At time: 1257.3895230293274 and batch: 300, loss is 3.980371322631836 and perplexity is 53.5369100034229
At time: 1258.5646958351135 and batch: 350, loss is 3.9519453763961794 and perplexity is 52.03649900977738
At time: 1259.7383151054382 and batch: 400, loss is 3.9729026222229002 and perplexity is 53.13854833552857
At time: 1260.9150035381317 and batch: 450, loss is 3.891745238304138 and perplexity is 48.99632220751098
At time: 1262.0948638916016 and batch: 500, loss is 3.9156844854354858 and perplexity is 50.183409570699446
At time: 1263.2664761543274 and batch: 550, loss is 3.9027634954452513 and perplexity is 49.53916136346373
At time: 1264.440661430359 and batch: 600, loss is 3.9256556367874147 and perplexity is 50.68629896944532
At time: 1265.6144454479218 and batch: 650, loss is 3.9497244119644166 and perplexity is 51.92105604114045
At time: 1266.7839090824127 and batch: 700, loss is 3.929647126197815 and perplexity is 50.88901709952718
At time: 1267.9532215595245 and batch: 750, loss is 3.9229274082183836 and perplexity is 50.54820362402803
At time: 1269.121952533722 and batch: 800, loss is 3.958238286972046 and perplexity is 52.36499255091888
At time: 1270.2912902832031 and batch: 850, loss is 3.9877330827713013 and perplexity is 53.932490190147256
At time: 1271.463237285614 and batch: 900, loss is 3.938457522392273 and perplexity is 51.33935039648571
At time: 1272.668258190155 and batch: 950, loss is 3.927735028266907 and perplexity is 50.79180528409895
At time: 1273.8404228687286 and batch: 1000, loss is 3.910688729286194 and perplexity is 49.93333068043456
At time: 1275.015681028366 and batch: 1050, loss is 3.9231262683868406 and perplexity is 50.55825664785578
At time: 1276.1914882659912 and batch: 1100, loss is 3.863219504356384 and perplexity is 47.61841253269554
At time: 1277.3645777702332 and batch: 1150, loss is 3.860375509262085 and perplexity is 47.48317839481785
At time: 1278.5381743907928 and batch: 1200, loss is 3.887697548866272 and perplexity is 48.79840114334254
At time: 1279.710432767868 and batch: 1250, loss is 3.934291625022888 and perplexity is 51.125920803280636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.37741111142792 and perplexity of 79.63160895303115
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 1282.7259876728058 and batch: 50, loss is 3.965729765892029 and perplexity is 52.75875688578936
At time: 1283.8960480690002 and batch: 100, loss is 3.9819831228256226 and perplexity is 53.62327038448433
At time: 1285.065173149109 and batch: 150, loss is 3.901572198867798 and perplexity is 49.48018066880676
At time: 1286.234988451004 and batch: 200, loss is 3.972428150177002 and perplexity is 53.11334156021175
At time: 1287.4060037136078 and batch: 250, loss is 3.974307885169983 and perplexity is 53.21327446119684
At time: 1288.5710215568542 and batch: 300, loss is 3.980352210998535 and perplexity is 53.53588683540809
At time: 1289.7359397411346 and batch: 350, loss is 3.951865553855896 and perplexity is 52.03234549001342
At time: 1290.9008417129517 and batch: 400, loss is 3.9728111267089843 and perplexity is 53.13368661915587
At time: 1292.0661194324493 and batch: 450, loss is 3.891631507873535 and perplexity is 48.99075015155053
At time: 1293.2374312877655 and batch: 500, loss is 3.915676350593567 and perplexity is 50.1830013382561
At time: 1294.4141318798065 and batch: 550, loss is 3.902618489265442 and perplexity is 49.5319783997231
At time: 1295.5954160690308 and batch: 600, loss is 3.9255874490737916 and perplexity is 50.6828429044385
At time: 1296.7741940021515 and batch: 650, loss is 3.949673042297363 and perplexity is 51.91838894228314
At time: 1297.9455652236938 and batch: 700, loss is 3.929591155052185 and perplexity is 50.88616886265042
At time: 1299.1193554401398 and batch: 750, loss is 3.9228220176696778 and perplexity is 50.54287660182582
At time: 1300.3142993450165 and batch: 800, loss is 3.9580650615692137 and perplexity is 52.35592238960379
At time: 1301.4983475208282 and batch: 850, loss is 3.9875429344177244 and perplexity is 53.922235990873745
At time: 1302.7095398902893 and batch: 900, loss is 3.938155183792114 and perplexity is 51.32383087534713
At time: 1303.885968208313 and batch: 950, loss is 3.9274252319335936 and perplexity is 50.776072606148375
At time: 1305.0624778270721 and batch: 1000, loss is 3.9102396392822265 and perplexity is 49.910911155330275
At time: 1306.2378480434418 and batch: 1050, loss is 3.922702054977417 and perplexity is 50.53681370594199
At time: 1307.4184346199036 and batch: 1100, loss is 3.8627975273132322 and perplexity is 47.59832289475651
At time: 1308.5981605052948 and batch: 1150, loss is 3.859988627433777 and perplexity is 47.464811569071244
At time: 1309.7830266952515 and batch: 1200, loss is 3.8873713493347166 and perplexity is 48.78248572369126
At time: 1310.9592924118042 and batch: 1250, loss is 3.9340231037139892 and perplexity is 51.11219424712677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.377384826214644 and perplexity of 79.62951584671536
Finished 42 epochs...
Completing Train Step...
At time: 1313.8964014053345 and batch: 50, loss is 3.9656931161880493 and perplexity is 52.756823328399534
At time: 1315.0935056209564 and batch: 100, loss is 3.981913537979126 and perplexity is 53.61953914726628
At time: 1316.2699520587921 and batch: 150, loss is 3.9014486503601074 and perplexity is 49.474067843947836
At time: 1317.4463629722595 and batch: 200, loss is 3.972312722206116 and perplexity is 53.107211148785765
At time: 1318.6215243339539 and batch: 250, loss is 3.974180097579956 and perplexity is 53.206474899555054
At time: 1319.8003344535828 and batch: 300, loss is 3.9802537536621094 and perplexity is 53.530616094062935
At time: 1320.9782671928406 and batch: 350, loss is 3.95176709651947 and perplexity is 52.027222776057
At time: 1322.1584577560425 and batch: 400, loss is 3.9727124786376953 and perplexity is 53.128445341975606
At time: 1323.335883140564 and batch: 450, loss is 3.8915521192550657 and perplexity is 48.98686099795753
At time: 1324.5065574645996 and batch: 500, loss is 3.9156146335601805 and perplexity is 50.179904287858434
At time: 1325.681911945343 and batch: 550, loss is 3.9025616979599 and perplexity is 49.529165493878764
At time: 1326.862741947174 and batch: 600, loss is 3.9255291700363157 and perplexity is 50.679889243206595
At time: 1328.0434803962708 and batch: 650, loss is 3.9496150493621824 and perplexity is 51.91537812982229
At time: 1329.2209844589233 and batch: 700, loss is 3.929539008140564 and perplexity is 50.88351537528619
At time: 1330.4009547233582 and batch: 750, loss is 3.9227693700790405 and perplexity is 50.540215711194215
At time: 1331.5809803009033 and batch: 800, loss is 3.958052282333374 and perplexity is 52.355253325199044
At time: 1332.7929196357727 and batch: 850, loss is 3.9875470924377443 and perplexity is 53.922460201076646
At time: 1333.9664325714111 and batch: 900, loss is 3.9381683921813964 and perplexity is 51.32450878496183
At time: 1335.1454463005066 and batch: 950, loss is 3.927443518638611 and perplexity is 50.77700114169997
At time: 1336.3208873271942 and batch: 1000, loss is 3.9102868700027464 and perplexity is 49.913268539295984
At time: 1337.493242740631 and batch: 1050, loss is 3.922758083343506 and perplexity is 50.53964528036477
At time: 1338.6648926734924 and batch: 1100, loss is 3.862886695861816 and perplexity is 47.602567357357536
At time: 1339.8450708389282 and batch: 1150, loss is 3.86009916305542 and perplexity is 47.47005841150035
At time: 1341.0255281925201 and batch: 1200, loss is 3.8874622869491575 and perplexity is 48.786922088282616
At time: 1342.1969933509827 and batch: 1250, loss is 3.93408890247345 and perplexity is 51.11555747674849
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.377375470460766 and perplexity of 79.62877085604863
Finished 43 epochs...
Completing Train Step...
At time: 1345.2448816299438 and batch: 50, loss is 3.96565975189209 and perplexity is 52.75506316349562
At time: 1346.423535823822 and batch: 100, loss is 3.981862759590149 and perplexity is 53.616816502577024
At time: 1347.6104452610016 and batch: 150, loss is 3.9013640022277833 and perplexity is 49.4698801337498
At time: 1348.7868599891663 and batch: 200, loss is 3.9722132015228273 and perplexity is 53.1019261458326
At time: 1349.9629564285278 and batch: 250, loss is 3.974070634841919 and perplexity is 53.200651091882015
At time: 1351.1388230323792 and batch: 300, loss is 3.980159730911255 and perplexity is 53.52558323488818
At time: 1352.3152332305908 and batch: 350, loss is 3.951681318283081 and perplexity is 52.022760164043284
At time: 1353.5005791187286 and batch: 400, loss is 3.972630467414856 and perplexity is 53.124088391867396
At time: 1354.6788446903229 and batch: 450, loss is 3.8914795541763305 and perplexity is 48.98330639150393
At time: 1355.8559277057648 and batch: 500, loss is 3.915556960105896 and perplexity is 50.177010322895775
At time: 1357.0330801010132 and batch: 550, loss is 3.9025091218948362 and perplexity is 49.52656151370532
At time: 1358.2103230953217 and batch: 600, loss is 3.9254789257049563 and perplexity is 50.677342930027635
At time: 1359.387320280075 and batch: 650, loss is 3.9495738840103147 and perplexity is 51.913241059001166
At time: 1360.5622642040253 and batch: 700, loss is 3.9295001745224 and perplexity is 50.88153942264621
At time: 1361.7720079421997 and batch: 750, loss is 3.9227289628982542 and perplexity is 50.538173564819964
At time: 1362.9481618404388 and batch: 800, loss is 3.9580427503585813 and perplexity is 52.35475427862254
At time: 1364.1302881240845 and batch: 850, loss is 3.987551894187927 and perplexity is 53.922719123881414
At time: 1365.3078441619873 and batch: 900, loss is 3.9381813478469847 and perplexity is 51.32517373244154
At time: 1366.489026069641 and batch: 950, loss is 3.927463426589966 and perplexity is 50.77801201783085
At time: 1367.6686162948608 and batch: 1000, loss is 3.910331783294678 and perplexity is 49.91551035884051
At time: 1368.8518998622894 and batch: 1050, loss is 3.9228118896484374 and perplexity is 50.5423647050903
At time: 1370.0305435657501 and batch: 1100, loss is 3.862963342666626 and perplexity is 47.60621608187597
At time: 1371.213206768036 and batch: 1150, loss is 3.860202293395996 and perplexity is 47.47495426724279
At time: 1372.3889563083649 and batch: 1200, loss is 3.8875478982925413 and perplexity is 48.79109898101431
At time: 1373.5671734809875 and batch: 1250, loss is 3.934146776199341 and perplexity is 51.118515810114744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.377371460851962 and perplexity of 79.62845157646804
Finished 44 epochs...
Completing Train Step...
At time: 1376.5261335372925 and batch: 50, loss is 3.9656272649765016 and perplexity is 52.75334934205031
At time: 1377.716248512268 and batch: 100, loss is 3.98181764125824 and perplexity is 53.61439745582623
At time: 1378.881339788437 and batch: 150, loss is 3.901292448043823 and perplexity is 49.46634048348612
At time: 1380.0434675216675 and batch: 200, loss is 3.9721222734451294 and perplexity is 53.09709790928056
At time: 1381.2055859565735 and batch: 250, loss is 3.9739707803726194 and perplexity is 53.19533903432161
At time: 1382.3677353858948 and batch: 300, loss is 3.980069513320923 and perplexity is 53.52075450356914
At time: 1383.5350711345673 and batch: 350, loss is 3.951602759361267 and perplexity is 52.01867347262015
At time: 1384.7083795070648 and batch: 400, loss is 3.972554612159729 and perplexity is 53.1200588034237
At time: 1385.8817479610443 and batch: 450, loss is 3.8914117860794066 and perplexity is 48.97998699852448
At time: 1387.0547449588776 and batch: 500, loss is 3.9155034732818605 and perplexity is 50.17432658574693
At time: 1388.2265059947968 and batch: 550, loss is 3.9024585628509523 and perplexity is 49.52405756140758
At time: 1389.3985152244568 and batch: 600, loss is 3.925432300567627 and perplexity is 50.67498014703701
At time: 1390.5745267868042 and batch: 650, loss is 3.9495396423339844 and perplexity is 51.911463493037154
At time: 1391.7767827510834 and batch: 700, loss is 3.9294684171676635 and perplexity is 50.879923585206726
At time: 1392.9535632133484 and batch: 750, loss is 3.9226944828033448 and perplexity is 50.53643103384039
At time: 1394.1282005310059 and batch: 800, loss is 3.9580331659317016 and perplexity is 52.35425249071303
At time: 1395.3043417930603 and batch: 850, loss is 3.9875565242767332 and perplexity is 53.92296879143762
At time: 1396.481863975525 and batch: 900, loss is 3.9381933975219727 and perplexity is 51.325792187829805
At time: 1397.6547944545746 and batch: 950, loss is 3.9274834489822386 and perplexity is 50.77902872528472
At time: 1398.8288054466248 and batch: 1000, loss is 3.91037308216095 and perplexity is 49.91757185539616
At time: 1400.0010106563568 and batch: 1050, loss is 3.9228624963760375 and perplexity is 50.54492255349482
At time: 1401.1781477928162 and batch: 1100, loss is 3.8630322933197023 and perplexity is 47.60949867473247
At time: 1402.3596761226654 and batch: 1150, loss is 3.860299835205078 and perplexity is 47.4795852860234
At time: 1403.5319316387177 and batch: 1200, loss is 3.8876292371749877 and perplexity is 48.79506775588442
At time: 1404.7113852500916 and batch: 1250, loss is 3.9341997003555296 and perplexity is 51.12122128602149
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.377368787779425 and perplexity of 79.62823872412548
Finished 45 epochs...
Completing Train Step...
At time: 1407.691236257553 and batch: 50, loss is 3.9655952215194703 and perplexity is 52.75165896945025
At time: 1408.8894171714783 and batch: 100, loss is 3.981775789260864 and perplexity is 53.61215363315917
At time: 1410.0611658096313 and batch: 150, loss is 3.9012278938293456 and perplexity is 49.463147325800136
At time: 1411.2466616630554 and batch: 200, loss is 3.972037272453308 and perplexity is 53.092584795107754
At time: 1412.4197981357574 and batch: 250, loss is 3.9738775539398192 and perplexity is 53.19038005377947
At time: 1413.5938425064087 and batch: 300, loss is 3.979983673095703 and perplexity is 53.51616046712799
At time: 1414.7680795192719 and batch: 350, loss is 3.9515293312072752 and perplexity is 52.0148539776849
At time: 1415.9402647018433 and batch: 400, loss is 3.9724825477600096 and perplexity is 53.11623087620328
At time: 1417.1120476722717 and batch: 450, loss is 3.8913470840454103 and perplexity is 48.97681799626211
At time: 1418.2913055419922 and batch: 500, loss is 3.915452742576599 and perplexity is 50.171781271336556
At time: 1419.4726114273071 and batch: 550, loss is 3.902409973144531 and perplexity is 49.521651260451094
At time: 1420.6477749347687 and batch: 600, loss is 3.9253882551193238 and perplexity is 50.67274819397272
At time: 1421.8699443340302 and batch: 650, loss is 3.9495090532302854 and perplexity is 51.90987559218356
At time: 1423.0413858890533 and batch: 700, loss is 3.9294409561157226 and perplexity is 50.87852638816674
At time: 1424.224211215973 and batch: 750, loss is 3.9226636028289796 and perplexity is 50.53487049424039
At time: 1425.4046740531921 and batch: 800, loss is 3.9580241298675536 and perplexity is 52.35377941646647
At time: 1426.583743095398 and batch: 850, loss is 3.9875613164901735 and perplexity is 53.92322720243258
At time: 1427.7551934719086 and batch: 900, loss is 3.9382045698165893 and perplexity is 51.32636561790482
At time: 1428.9264643192291 and batch: 950, loss is 3.927502827644348 and perplexity is 50.78001276445929
At time: 1430.0983793735504 and batch: 1000, loss is 3.9104115676879885 and perplexity is 49.91949299642533
At time: 1431.272269487381 and batch: 1050, loss is 3.9229096221923827 and perplexity is 50.5473045803593
At time: 1432.445394039154 and batch: 1100, loss is 3.863096284866333 and perplexity is 47.61254537766755
At time: 1433.6174981594086 and batch: 1150, loss is 3.8603927230834962 and perplexity is 47.483995768805826
At time: 1434.79243683815 and batch: 1200, loss is 3.887706699371338 and perplexity is 48.798847675402385
At time: 1435.9686975479126 and batch: 1250, loss is 3.9342491102218626 and perplexity is 51.12374724113505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.3773678967552465 and perplexity of 79.62816777347106
Finished 46 epochs...
Completing Train Step...
At time: 1438.964890241623 and batch: 50, loss is 3.9655642223358156 and perplexity is 52.75002373643134
At time: 1440.1472899913788 and batch: 100, loss is 3.981736159324646 and perplexity is 53.61002902902942
At time: 1441.31680393219 and batch: 150, loss is 3.901168041229248 and perplexity is 49.460186916418664
At time: 1442.4854409694672 and batch: 200, loss is 3.9719568252563477 and perplexity is 53.08831381727801
At time: 1443.6609797477722 and batch: 250, loss is 3.9737896060943605 and perplexity is 53.18570228015768
At time: 1444.8301653862 and batch: 300, loss is 3.9799016666412355 and perplexity is 53.5117719764961
At time: 1445.9989397525787 and batch: 350, loss is 3.951459879875183 and perplexity is 52.01124160223117
At time: 1447.170825958252 and batch: 400, loss is 3.9724138021469115 and perplexity is 53.11257949385594
At time: 1448.3430905342102 and batch: 450, loss is 3.8912850618362427 and perplexity is 48.973780440010934
At time: 1449.5128684043884 and batch: 500, loss is 3.915404133796692 and perplexity is 50.16934254153552
At time: 1450.7101821899414 and batch: 550, loss is 3.902363266944885 and perplexity is 49.51933834633465
At time: 1451.8791966438293 and batch: 600, loss is 3.925346164703369 and perplexity is 50.67061540180903
At time: 1453.0482578277588 and batch: 650, loss is 3.9494809913635254 and perplexity is 51.90841892460966
At time: 1454.2203068733215 and batch: 700, loss is 3.929416546821594 and perplexity is 50.87728449440824
At time: 1455.3915679454803 and batch: 750, loss is 3.9226356458663942 and perplexity is 50.53345771250536
At time: 1456.567993402481 and batch: 800, loss is 3.958015832901001 and perplexity is 52.35334504071176
At time: 1457.7370479106903 and batch: 850, loss is 3.987566204071045 and perplexity is 53.92349075721046
At time: 1458.906952381134 and batch: 900, loss is 3.9382150030136107 and perplexity is 51.32690111878319
At time: 1460.068926334381 and batch: 950, loss is 3.9275214672088623 and perplexity is 50.780959290604635
At time: 1461.23237657547 and batch: 1000, loss is 3.9104472827911376 and perplexity is 49.9212759081051
At time: 1462.4006776809692 and batch: 1050, loss is 3.922954077720642 and perplexity is 50.54955173743542
At time: 1463.566953420639 and batch: 1100, loss is 3.863156352043152 and perplexity is 47.61540541474588
At time: 1464.7327630519867 and batch: 1150, loss is 3.860481343269348 and perplexity is 47.48820399580002
At time: 1465.8919594287872 and batch: 1200, loss is 3.8877804899215698 and perplexity is 48.80244870208228
At time: 1467.0514130592346 and batch: 1250, loss is 3.934295902252197 and perplexity is 51.12613948103521
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.377367451243157 and perplexity of 79.62813229816756
Finished 47 epochs...
Completing Train Step...
At time: 1469.9525821208954 and batch: 50, loss is 3.9655340671539308 and perplexity is 52.74843307385461
At time: 1471.1485843658447 and batch: 100, loss is 3.981698236465454 and perplexity is 53.60799602199623
At time: 1472.322252035141 and batch: 150, loss is 3.9011116552352907 and perplexity is 49.457398133242954
At time: 1473.4937291145325 and batch: 200, loss is 3.97188006401062 and perplexity is 53.08423884857764
At time: 1474.6665523052216 and batch: 250, loss is 3.9737063312530516 and perplexity is 53.1812734336487
At time: 1475.8453006744385 and batch: 300, loss is 3.979823660850525 and perplexity is 53.50759791121351
At time: 1477.0173869132996 and batch: 350, loss is 3.9513935947418215 and perplexity is 52.00779414440412
At time: 1478.189926624298 and batch: 400, loss is 3.9723478984832763 and perplexity is 53.109079295621385
At time: 1479.377314567566 and batch: 450, loss is 3.8912252235412597 and perplexity is 48.97085002016707
At time: 1480.6189291477203 and batch: 500, loss is 3.9153573751449584 and perplexity is 50.16699674556347
At time: 1481.8015525341034 and batch: 550, loss is 3.9023181772232056 and perplexity is 49.517105583488565
At time: 1482.9820020198822 and batch: 600, loss is 3.925306029319763 and perplexity is 50.66858175803314
At time: 1484.1591925621033 and batch: 650, loss is 3.949455165863037 and perplexity is 51.907078381021556
At time: 1485.3377392292023 and batch: 700, loss is 3.9293943738937376 and perplexity is 50.87615640855614
At time: 1486.5150306224823 and batch: 750, loss is 3.922609887123108 and perplexity is 50.53215605090543
At time: 1487.6958606243134 and batch: 800, loss is 3.9580084562301634 and perplexity is 52.352958848742546
At time: 1488.8709778785706 and batch: 850, loss is 3.9875713872909544 and perplexity is 53.92377025524568
At time: 1490.0477650165558 and batch: 900, loss is 3.9382249402999876 and perplexity is 51.327411171432715
At time: 1491.2189598083496 and batch: 950, loss is 3.927539391517639 and perplexity is 50.781869512356465
At time: 1492.3900408744812 and batch: 1000, loss is 3.9104808282852175 and perplexity is 49.92295057005906
At time: 1493.5659854412079 and batch: 1050, loss is 3.922995924949646 and perplexity is 50.551667140364586
At time: 1494.7453501224518 and batch: 1100, loss is 3.863213453292847 and perplexity is 47.61812439152753
At time: 1495.9165334701538 and batch: 1150, loss is 3.860566358566284 and perplexity is 47.49224139118144
At time: 1497.0955080986023 and batch: 1200, loss is 3.887851104736328 and perplexity is 48.80589499963552
At time: 1498.267019033432 and batch: 1250, loss is 3.9343404150009156 and perplexity is 51.12841529668591
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.377369233291515 and perplexity of 79.6282741994764
Annealing...
Finished 48 epochs...
Completing Train Step...
At time: 1501.2753584384918 and batch: 50, loss is 3.965529856681824 and perplexity is 52.748210978516035
At time: 1502.4648578166962 and batch: 100, loss is 3.981714687347412 and perplexity is 53.60887792806484
At time: 1503.64506149292 and batch: 150, loss is 3.9011312675476075 and perplexity is 49.4583681166933
At time: 1504.8213560581207 and batch: 200, loss is 3.971863374710083 and perplexity is 53.08335291715452
At time: 1506.00039935112 and batch: 250, loss is 3.9737131643295287 and perplexity is 53.18163682659877
At time: 1507.176481962204 and batch: 300, loss is 3.9798284339904786 and perplexity is 53.507853311076445
At time: 1508.3594393730164 and batch: 350, loss is 3.9513785552978518 and perplexity is 52.00701198197976
At time: 1509.5422356128693 and batch: 400, loss is 3.972330732345581 and perplexity is 53.10816762567827
At time: 1510.7521283626556 and batch: 450, loss is 3.891198740005493 and perplexity is 48.969553116082416
At time: 1511.9316666126251 and batch: 500, loss is 3.9153576564788817 and perplexity is 50.16701085924347
At time: 1513.1114931106567 and batch: 550, loss is 3.9022851276397703 and perplexity is 49.51546909081896
At time: 1514.2901108264923 and batch: 600, loss is 3.9252892112731934 and perplexity is 50.6677296186312
At time: 1515.470017194748 and batch: 650, loss is 3.9494364356994627 and perplexity is 51.906106162057746
At time: 1516.6478192806244 and batch: 700, loss is 3.929374489784241 and perplexity is 50.87514479154893
At time: 1517.8312120437622 and batch: 750, loss is 3.9225775003433228 and perplexity is 50.53051950359673
At time: 1519.0107707977295 and batch: 800, loss is 3.9579529333114625 and perplexity is 52.35005214035985
At time: 1520.1910400390625 and batch: 850, loss is 3.9875142860412596 and perplexity is 53.92069122848483
At time: 1521.374921321869 and batch: 900, loss is 3.9381378364562987 and perplexity is 51.322940551340004
At time: 1522.556151151657 and batch: 950, loss is 3.9274542856216432 and perplexity is 50.777547859752936
At time: 1523.74112033844 and batch: 1000, loss is 3.9103543853759763 and perplexity is 49.91663856601357
At time: 1524.9223401546478 and batch: 1050, loss is 3.92287561416626 and perplexity is 50.545585595534504
At time: 1526.1009130477905 and batch: 1100, loss is 3.863083691596985 and perplexity is 47.611945783834706
At time: 1527.2836859226227 and batch: 1150, loss is 3.860449781417847 and perplexity is 47.4867052038099
At time: 1528.463238954544 and batch: 1200, loss is 3.887756314277649 and perplexity is 48.80126888572145
At time: 1529.6393239498138 and batch: 1250, loss is 3.934262022972107 and perplexity is 51.12440739357691
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.377369233291515 and perplexity of 79.6282741994764
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 1532.6288480758667 and batch: 50, loss is 3.96552960395813 and perplexity is 52.74819764779499
At time: 1533.8414466381073 and batch: 100, loss is 3.9817171812057497 and perplexity is 53.60901162117874
At time: 1535.014965057373 and batch: 150, loss is 3.9011320209503175 and perplexity is 49.45840537877591
At time: 1536.1884789466858 and batch: 200, loss is 3.9718576002120973 and perplexity is 53.083046388325066
At time: 1537.3789038658142 and batch: 250, loss is 3.973713240623474 and perplexity is 53.181640884035815
At time: 1538.5547988414764 and batch: 300, loss is 3.9798296689987183 and perplexity is 53.50791939375698
At time: 1539.7289233207703 and batch: 350, loss is 3.9513732051849364 and perplexity is 52.00673373933758
At time: 1540.9524471759796 and batch: 400, loss is 3.9723249435424806 and perplexity is 53.1078601938427
At time: 1542.1287529468536 and batch: 450, loss is 3.8911914300918578 and perplexity is 48.969195154186714
At time: 1543.2891767024994 and batch: 500, loss is 3.915357584953308 and perplexity is 50.16700727101938
At time: 1544.4557235240936 and batch: 550, loss is 3.90227659702301 and perplexity is 49.5150466951301
At time: 1545.6218931674957 and batch: 600, loss is 3.9252846384048463 and perplexity is 50.667497922303966
At time: 1546.7886130809784 and batch: 650, loss is 3.9494296598434446 and perplexity is 51.9057544549475
At time: 1547.9488246440887 and batch: 700, loss is 3.9293679571151734 and perplexity is 50.87481244214981
At time: 1549.1161935329437 and batch: 750, loss is 3.922568135261536 and perplexity is 50.53004628336472
At time: 1550.2873270511627 and batch: 800, loss is 3.957938313484192 and perplexity is 52.349286797234555
At time: 1551.4532353878021 and batch: 850, loss is 3.9874993705749513 and perplexity is 53.91988698222936
At time: 1552.6340637207031 and batch: 900, loss is 3.9381148481369017 and perplexity is 51.321760736751244
At time: 1553.8100826740265 and batch: 950, loss is 3.9274315881729125 and perplexity is 50.77639535204326
At time: 1554.9859874248505 and batch: 1000, loss is 3.9103202867507934 and perplexity is 49.91493650628382
At time: 1556.1702055931091 and batch: 1050, loss is 3.9228432989120483 and perplexity is 50.54395222847818
At time: 1557.3464312553406 and batch: 1100, loss is 3.8630491399765017 and perplexity is 47.6103007423731
At time: 1558.5243394374847 and batch: 1150, loss is 3.860418381690979 and perplexity is 47.485214157645984
At time: 1559.701067686081 and batch: 1200, loss is 3.887730565071106 and perplexity is 48.80001230794736
At time: 1560.8848328590393 and batch: 1250, loss is 3.9342410659790037 and perplexity is 51.12333599095049
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.377368787779425 and perplexity of 79.62823872412548
Annealing...
Finished Training.
Improved accuracyfrom -10000000 to -79.62813229816756
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f58a487a898>
SETTINGS FOR THIS RUN
{'anneal': 7.614927514455996, 'data': 'wikitext', 'lr': 23.582210378130824, 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.6714728479092035, 'num_layers': 1, 'wordvec_source': 'glove', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.6590723991394043 and batch: 50, loss is 7.4216961002349855 and perplexity is 1671.866757928374
At time: 2.832923412322998 and batch: 100, loss is 6.578530874252319 and perplexity is 719.481543568687
At time: 4.004632949829102 and batch: 150, loss is 6.451911430358887 and perplexity is 633.9128157294389
At time: 5.176148176193237 and batch: 200, loss is 6.48088264465332 and perplexity is 652.5466589785137
At time: 6.373240232467651 and batch: 250, loss is 6.552571363449097 and perplexity is 701.0444983788532
At time: 7.550417900085449 and batch: 300, loss is 6.611975755691528 and perplexity is 743.9514339038705
At time: 8.725232362747192 and batch: 350, loss is 6.668077993392944 and perplexity is 786.881758179187
At time: 9.896639108657837 and batch: 400, loss is 6.926764497756958 and perplexity is 1019.1910442668361
At time: 11.067092418670654 and batch: 450, loss is 6.90100567817688 and perplexity is 993.2731265878994
At time: 12.23835301399231 and batch: 500, loss is 6.822107305526734 and perplexity is 917.9173056087532
At time: 13.4023118019104 and batch: 550, loss is 6.998428134918213 and perplexity is 1094.9107531082225
At time: 14.574864149093628 and batch: 600, loss is 7.024960842132568 and perplexity is 1124.3505309741051
At time: 15.741685628890991 and batch: 650, loss is 7.0731774520874025 and perplexity is 1179.8911318831324
At time: 16.906076908111572 and batch: 700, loss is 7.01236496925354 and perplexity is 1110.2771737915841
At time: 18.07034683227539 and batch: 750, loss is 7.120667467117309 and perplexity is 1237.2759989604156
At time: 19.23446750640869 and batch: 800, loss is 7.240252542495727 and perplexity is 1394.446083296942
At time: 20.400670289993286 and batch: 850, loss is 6.953039855957031 and perplexity is 1046.3255777125514
At time: 21.566705226898193 and batch: 900, loss is 7.0691969776153565 and perplexity is 1175.203940165091
At time: 22.729877471923828 and batch: 950, loss is 6.988205223083496 and perplexity is 1083.7745959731537
At time: 23.896711111068726 and batch: 1000, loss is 7.099465217590332 and perplexity is 1211.319109095732
At time: 25.053338766098022 and batch: 1050, loss is 6.86146800994873 and perplexity is 954.7676476123014
At time: 26.213016033172607 and batch: 1100, loss is 6.926062602996826 and perplexity is 1018.475930409985
At time: 27.372102737426758 and batch: 1150, loss is 6.965775194168091 and perplexity is 1059.736100341636
At time: 28.533570289611816 and batch: 1200, loss is 6.948895893096924 and perplexity is 1041.9986149574438
At time: 29.68580412864685 and batch: 1250, loss is 6.989502010345459 and perplexity is 1085.1809327267626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.474876236741561 and perplexity of 648.6389449735796
Finished 1 epochs...
Completing Train Step...
At time: 32.65165066719055 and batch: 50, loss is 7.20444088935852 and perplexity is 1345.392255586431
At time: 33.80186438560486 and batch: 100, loss is 7.055020952224732 and perplexity is 1158.6617475665334
At time: 34.967750549316406 and batch: 150, loss is 6.876789455413818 and perplexity is 969.5087068579097
At time: 36.13377356529236 and batch: 200, loss is 6.9081205368042 and perplexity is 1000.3653245368232
At time: 37.307833433151245 and batch: 250, loss is 6.77354603767395 and perplexity is 874.4070833783719
At time: 38.47564506530762 and batch: 300, loss is 6.849646520614624 and perplexity is 943.5473231897356
At time: 39.638898849487305 and batch: 350, loss is 6.790869913101196 and perplexity is 889.6871758235923
At time: 40.833176612854004 and batch: 400, loss is 6.817966718673706 and perplexity is 914.1244470300402
At time: 41.999690771102905 and batch: 450, loss is 6.9362445831298825 and perplexity is 1028.8990058239187
At time: 43.163280725479126 and batch: 500, loss is 6.95483172416687 and perplexity is 1048.2021360228828
At time: 44.334959983825684 and batch: 550, loss is 6.982381067276001 and perplexity is 1077.4808694785295
At time: 45.50606942176819 and batch: 600, loss is 6.8245965766906735 and perplexity is 920.2050969736707
At time: 46.66898775100708 and batch: 650, loss is 6.933247737884521 and perplexity is 1025.8201704312346
At time: 47.83525586128235 and batch: 700, loss is 7.053284101486206 and perplexity is 1156.651071681977
At time: 49.00515413284302 and batch: 750, loss is 6.8382651805877686 and perplexity is 932.8693702434359
At time: 50.1686646938324 and batch: 800, loss is 6.915848770141602 and perplexity is 1008.126331998263
At time: 51.330817222595215 and batch: 850, loss is 6.856088409423828 and perplexity is 949.6451698697692
At time: 52.495575189590454 and batch: 900, loss is 6.836750831604004 and perplexity is 931.4577495735529
At time: 53.66001844406128 and batch: 950, loss is 6.769334535598755 and perplexity is 870.7322598272821
At time: 54.824474811553955 and batch: 1000, loss is 6.806110773086548 and perplexity is 903.3506304180238
At time: 55.98600101470947 and batch: 1050, loss is 6.879850788116455 and perplexity is 972.4812432075829
At time: 57.14937448501587 and batch: 1100, loss is 7.012082910537719 and perplexity is 1109.9640545988138
At time: 58.314319372177124 and batch: 1150, loss is 7.006610212326049 and perplexity is 1103.9061479693146
At time: 59.477551221847534 and batch: 1200, loss is 7.200293865203857 and perplexity is 1339.8244343291642
At time: 60.64378356933594 and batch: 1250, loss is 6.967287006378174 and perplexity is 1061.3394339819072
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.230049523123859 and perplexity of 507.7806297561485
Finished 2 epochs...
Completing Train Step...
At time: 63.6770281791687 and batch: 50, loss is 6.742876691818237 and perplexity is 847.9966555169267
At time: 64.84193158149719 and batch: 100, loss is 6.665133771896362 and perplexity is 784.5684111655829
At time: 66.01141667366028 and batch: 150, loss is 6.702470541000366 and perplexity is 814.4153883996086
At time: 67.17695832252502 and batch: 200, loss is 6.893663587570191 and perplexity is 986.0071317318013
At time: 68.376145362854 and batch: 250, loss is 7.129262800216675 and perplexity is 1247.9566343600072
At time: 69.54337048530579 and batch: 300, loss is 7.005660848617554 and perplexity is 1102.8586368480446
At time: 70.7085075378418 and batch: 350, loss is 7.0045259571075436 and perplexity is 1101.6077219049814
At time: 71.87876892089844 and batch: 400, loss is 6.879166469573975 and perplexity is 971.8159839112392
At time: 73.04248237609863 and batch: 450, loss is 6.719730072021484 and perplexity is 828.5938206376038
At time: 74.21454668045044 and batch: 500, loss is 6.7671121311187745 and perplexity is 868.7992892689839
At time: 75.38127064704895 and batch: 550, loss is 6.798161993026733 and perplexity is 896.1985577238182
At time: 76.55275893211365 and batch: 600, loss is 6.891458978652954 and perplexity is 983.835766002309
At time: 77.71694278717041 and batch: 650, loss is 6.9523830986022945 and perplexity is 1045.6386213004698
At time: 78.88275790214539 and batch: 700, loss is 6.996431531906128 and perplexity is 1092.7268319378989
At time: 80.06566143035889 and batch: 750, loss is 6.962006502151489 and perplexity is 1055.7497976522675
At time: 81.23688340187073 and batch: 800, loss is 7.041917886734009 and perplexity is 1143.5787592977601
At time: 82.40364408493042 and batch: 850, loss is 6.857835397720337 and perplexity is 951.305638855002
At time: 83.57153844833374 and batch: 900, loss is 6.836685638427735 and perplexity is 931.3970268636724
At time: 84.7385504245758 and batch: 950, loss is 6.771193418502808 and perplexity is 872.3523544556733
At time: 85.90904498100281 and batch: 1000, loss is 6.831613693237305 and perplexity is 926.6849918857368
At time: 87.07619023323059 and batch: 1050, loss is 6.947670164108277 and perplexity is 1040.722189484622
At time: 88.23336052894592 and batch: 1100, loss is 6.910610294342041 and perplexity is 1002.8590947976182
At time: 89.39169454574585 and batch: 1150, loss is 6.940111274719238 and perplexity is 1032.8851425702564
At time: 90.55850195884705 and batch: 1200, loss is 6.876614217758179 and perplexity is 969.3388273100793
At time: 91.73371195793152 and batch: 1250, loss is 6.8126061916351315 and perplexity is 909.2373685758236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 7.286977948933623 and perplexity of 1461.1483533558142
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 94.74875473976135 and batch: 50, loss is 6.44487587928772 and perplexity is 629.4685420292385
At time: 95.97415351867676 and batch: 100, loss is 6.359902095794678 and perplexity is 578.1897464149461
At time: 97.13894271850586 and batch: 150, loss is 6.242514953613282 and perplexity is 514.1499495814674
At time: 98.333651304245 and batch: 200, loss is 6.277435054779053 and perplexity is 532.4212795258604
At time: 99.49897050857544 and batch: 250, loss is 6.300872259140014 and perplexity is 545.0471251771373
At time: 100.66625738143921 and batch: 300, loss is 6.308852224349976 and perplexity is 549.413982786363
At time: 101.83266282081604 and batch: 350, loss is 6.333218450546265 and perplexity is 562.9655580684494
At time: 102.99389958381653 and batch: 400, loss is 6.32491319656372 and perplexity is 558.3093483915923
At time: 104.15337872505188 and batch: 450, loss is 6.406926927566528 and perplexity is 606.0284472149835
At time: 105.31359386444092 and batch: 500, loss is 6.3425172996520995 and perplexity is 568.2249048836186
At time: 106.47405171394348 and batch: 550, loss is 6.278405494689942 and perplexity is 532.9382131708703
At time: 107.64173007011414 and batch: 600, loss is 6.262147026062012 and perplexity is 524.3435116187252
At time: 108.81240892410278 and batch: 650, loss is 6.217694873809815 and perplexity is 501.5457721922239
At time: 109.98647665977478 and batch: 700, loss is 6.221782617568969 and perplexity is 503.6001588348493
At time: 111.16953158378601 and batch: 750, loss is 6.158498506546021 and perplexity is 472.7177590802178
At time: 112.34169292449951 and batch: 800, loss is 6.165652265548706 and perplexity is 476.1115928658326
At time: 113.51378273963928 and batch: 850, loss is 6.19318808555603 and perplexity is 489.4038831829319
At time: 114.69010591506958 and batch: 900, loss is 6.187558164596558 and perplexity is 486.6563195436433
At time: 115.86639046669006 and batch: 950, loss is 6.148454465866089 and perplexity is 467.993527580628
At time: 117.04137015342712 and batch: 1000, loss is 6.14959156036377 and perplexity is 468.52598311454767
At time: 118.2176444530487 and batch: 1050, loss is 6.1559206199646 and perplexity is 471.5007156861909
At time: 119.38793849945068 and batch: 1100, loss is 6.137500257492065 and perplexity is 462.8950050835946
At time: 120.5613603591919 and batch: 1150, loss is 6.184531803131104 and perplexity is 485.1857479742185
At time: 121.7327527999878 and batch: 1200, loss is 6.172780017852784 and perplexity is 479.5173215451507
At time: 122.91170740127563 and batch: 1250, loss is 6.1636505031585695 and perplexity is 475.15948385166286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.771083414119525 and perplexity of 320.88519594207963
Finished 4 epochs...
Completing Train Step...
At time: 125.9585177898407 and batch: 50, loss is 6.162753210067749 and perplexity is 474.733317756305
At time: 127.13107776641846 and batch: 100, loss is 6.182238368988037 and perplexity is 484.0742814389137
At time: 128.3513731956482 and batch: 150, loss is 6.093833112716675 and perplexity is 443.1166761657243
At time: 129.5292513370514 and batch: 200, loss is 6.124409828186035 and perplexity is 456.87499894096027
At time: 130.70238041877747 and batch: 250, loss is 6.1579231262207035 and perplexity is 472.44584481677526
At time: 131.87440085411072 and batch: 300, loss is 6.166083517074585 and perplexity is 476.31696099622167
At time: 133.04607462882996 and batch: 350, loss is 6.190115756988526 and perplexity is 487.90258107917674
At time: 134.22108387947083 and batch: 400, loss is 6.17861008644104 and perplexity is 482.32110560405727
At time: 135.40193796157837 and batch: 450, loss is 6.239152908325195 and perplexity is 512.424256720392
At time: 136.57285070419312 and batch: 500, loss is 6.204237222671509 and perplexity is 494.84135817777013
At time: 137.75188660621643 and batch: 550, loss is 6.165433130264282 and perplexity is 476.00727144718167
At time: 138.9317593574524 and batch: 600, loss is 6.171790714263916 and perplexity is 479.04316791763665
At time: 140.11037492752075 and batch: 650, loss is 6.143511476516724 and perplexity is 465.68594842626584
At time: 141.2876100540161 and batch: 700, loss is 6.154563999176025 and perplexity is 470.8615016969914
At time: 142.46166276931763 and batch: 750, loss is 6.098161039352417 and perplexity is 445.038608622612
At time: 143.6321177482605 and batch: 800, loss is 6.108786678314209 and perplexity is 449.7926407874922
At time: 144.80389547348022 and batch: 850, loss is 6.1310436820983885 and perplexity is 459.9159162888098
At time: 145.975604057312 and batch: 900, loss is 6.114328298568726 and perplexity is 452.2921400373081
At time: 147.14728593826294 and batch: 950, loss is 6.066499195098877 and perplexity is 431.16859922075025
At time: 148.31820344924927 and batch: 1000, loss is 6.05091233253479 and perplexity is 424.50013871957094
At time: 149.48719000816345 and batch: 1050, loss is 6.051243028640747 and perplexity is 424.640542476632
At time: 150.65307593345642 and batch: 1100, loss is 6.033385858535767 and perplexity is 417.1249672407677
At time: 151.82391333580017 and batch: 1150, loss is 6.0762717819213865 and perplexity is 435.4028880704355
At time: 153.00218415260315 and batch: 1200, loss is 6.063803482055664 and perplexity is 430.00785762005637
At time: 154.1734790802002 and batch: 1250, loss is 6.048123741149903 and perplexity is 423.3180302646108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.661908003535584 and perplexity of 287.6970461929575
Finished 5 epochs...
Completing Train Step...
At time: 157.1945674419403 and batch: 50, loss is 6.040987758636475 and perplexity is 420.3079927636547
At time: 158.4064543247223 and batch: 100, loss is 6.053327589035034 and perplexity is 425.5266545895198
At time: 159.5786190032959 and batch: 150, loss is 5.975331144332886 and perplexity is 393.59841739850606
At time: 160.75598335266113 and batch: 200, loss is 6.011429405212402 and perplexity is 408.06619553909474
At time: 161.9376516342163 and batch: 250, loss is 6.032516050338745 and perplexity is 416.7623062706862
At time: 163.11666750907898 and batch: 300, loss is 6.035395450592041 and perplexity is 417.96406109719544
At time: 164.2993128299713 and batch: 350, loss is 6.064239702224731 and perplexity is 430.1954766390327
At time: 165.47473573684692 and batch: 400, loss is 6.041004638671875 and perplexity is 420.3150876373324
At time: 166.65870332717896 and batch: 450, loss is 6.062246170043945 and perplexity is 429.33872237965824
At time: 167.8329734802246 and batch: 500, loss is 6.043743858337402 and perplexity is 421.4680013121211
At time: 169.009747505188 and batch: 550, loss is 6.023314943313599 and perplexity is 412.94521932900847
At time: 170.18394112586975 and batch: 600, loss is 6.037205018997192 and perplexity is 418.721080389168
At time: 171.35814881324768 and batch: 650, loss is 6.020351963043213 and perplexity is 411.7234816774934
At time: 172.53399276733398 and batch: 700, loss is 6.0381828308105465 and perplexity is 419.13071104628284
At time: 173.70066952705383 and batch: 750, loss is 5.986307773590088 and perplexity is 397.94259992520335
At time: 174.8677215576172 and batch: 800, loss is 6.001391658782959 and perplexity is 403.9906195609142
At time: 176.03632068634033 and batch: 850, loss is 6.024487771987915 and perplexity is 413.4298174428929
At time: 177.20521116256714 and batch: 900, loss is 6.0280965805053714 and perplexity is 414.92450188225104
At time: 178.36767077445984 and batch: 950, loss is 5.997937622070313 and perplexity is 402.59762823588295
At time: 179.53034853935242 and batch: 1000, loss is 5.988568859100342 and perplexity is 398.84340018123123
At time: 180.6925528049469 and batch: 1050, loss is 5.9957981872558594 and perplexity is 401.73721757826445
At time: 181.8550419807434 and batch: 1100, loss is 5.980322484970093 and perplexity is 395.56791229505063
At time: 183.02461171150208 and batch: 1150, loss is 6.029481267929077 and perplexity is 415.49944058512654
At time: 184.2008204460144 and batch: 1200, loss is 6.020604400634766 and perplexity is 411.8274292811832
At time: 185.3794767856598 and batch: 1250, loss is 6.004811735153198 and perplexity is 405.37466375200154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.629898850935219 and perplexity of 278.6339326459181
Finished 6 epochs...
Completing Train Step...
At time: 188.38472962379456 and batch: 50, loss is 6.001270227432251 and perplexity is 403.9415654127235
At time: 189.58261942863464 and batch: 100, loss is 6.013596305847168 and perplexity is 408.95139315850923
At time: 190.7612373828888 and batch: 150, loss is 5.9366263580322265 and perplexity is 378.6553247926121
At time: 191.9395203590393 and batch: 200, loss is 5.970457010269165 and perplexity is 391.6846337527074
At time: 193.11257243156433 and batch: 250, loss is 5.993917474746704 and perplexity is 400.98237541081977
At time: 194.2854721546173 and batch: 300, loss is 5.997475652694702 and perplexity is 402.411683414658
At time: 195.45725202560425 and batch: 350, loss is 6.024334897994995 and perplexity is 413.3666196066842
At time: 196.62958788871765 and batch: 400, loss is 6.002515735626221 and perplexity is 404.44499138783885
At time: 197.80131435394287 and batch: 450, loss is 6.002376461029053 and perplexity is 404.3886663969979
At time: 198.97710061073303 and batch: 500, loss is 5.990603981018066 and perplexity is 399.65592163635426
At time: 200.15393137931824 and batch: 550, loss is 5.980678014755249 and perplexity is 395.70857347306105
At time: 201.32499527931213 and batch: 600, loss is 6.008082151412964 and perplexity is 406.7025778760325
At time: 202.4990532398224 and batch: 650, loss is 5.996958255767822 and perplexity is 402.2035306997496
At time: 203.6722960472107 and batch: 700, loss is 6.014476299285889 and perplexity is 409.31142609132
At time: 204.8486795425415 and batch: 750, loss is 5.96838849067688 and perplexity is 390.87526380118646
At time: 206.0279893875122 and batch: 800, loss is 5.9821129322052 and perplexity is 396.2767901847744
At time: 207.20033955574036 and batch: 850, loss is 6.002466945648194 and perplexity is 404.4252590069708
At time: 208.37137866020203 and batch: 900, loss is 6.001514482498169 and perplexity is 404.040242237077
At time: 209.5463752746582 and batch: 950, loss is 5.978293380737305 and perplexity is 394.7660775481109
At time: 210.72057342529297 and batch: 1000, loss is 5.9667621612548825 and perplexity is 390.24008850147516
At time: 211.8934941291809 and batch: 1050, loss is 5.97544337272644 and perplexity is 393.64259279541676
At time: 213.06335997581482 and batch: 1100, loss is 5.960362157821655 and perplexity is 387.7505256982378
At time: 214.24228930473328 and batch: 1150, loss is 6.009453773498535 and perplexity is 407.2608028634489
At time: 215.41400170326233 and batch: 1200, loss is 6.003923387527466 and perplexity is 405.01471003761094
At time: 216.63410139083862 and batch: 1250, loss is 5.985024461746216 and perplexity is 397.4322430171121
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.613731217210311 and perplexity of 274.16530220805856
Finished 7 epochs...
Completing Train Step...
At time: 219.61977529525757 and batch: 50, loss is 5.979950475692749 and perplexity is 395.42078472996366
At time: 220.79314732551575 and batch: 100, loss is 5.996233882904053 and perplexity is 401.91229087224735
At time: 221.96981000900269 and batch: 150, loss is 5.9222578430175785 and perplexity is 373.2535110293209
At time: 223.1444845199585 and batch: 200, loss is 5.9539624691009525 and perplexity is 385.2769664792522
At time: 224.3180227279663 and batch: 250, loss is 5.97396481513977 and perplexity is 393.0609996188578
At time: 225.49175262451172 and batch: 300, loss is 5.9796420860290525 and perplexity is 395.29885984829514
At time: 226.665620803833 and batch: 350, loss is 6.007859888076783 and perplexity is 406.61219284925113
At time: 227.84269309043884 and batch: 400, loss is 5.984225635528564 and perplexity is 397.11489049328407
At time: 229.01776003837585 and batch: 450, loss is 5.96833309173584 and perplexity is 390.85361032528857
At time: 230.20122075080872 and batch: 500, loss is 5.967331743240356 and perplexity is 390.4624255394701
At time: 231.38227939605713 and batch: 550, loss is 5.959206590652466 and perplexity is 387.3027127097304
At time: 232.55509901046753 and batch: 600, loss is 5.982928228378296 and perplexity is 396.60000487524536
At time: 233.73526787757874 and batch: 650, loss is 5.973189640045166 and perplexity is 392.75642658525936
At time: 234.92593955993652 and batch: 700, loss is 5.996245489120484 and perplexity is 401.9169555803511
At time: 236.11043643951416 and batch: 750, loss is 5.947459754943847 and perplexity is 382.7797486414405
At time: 237.29970717430115 and batch: 800, loss is 5.960421495437622 and perplexity is 387.7735345726617
At time: 238.48278093338013 and batch: 850, loss is 5.9806931209564205 and perplexity is 395.7145511715274
At time: 239.66630053520203 and batch: 900, loss is 5.979222154617309 and perplexity is 395.1328962891133
At time: 240.84982919692993 and batch: 950, loss is 5.957053461074829 and perplexity is 386.4696969008041
At time: 242.03508138656616 and batch: 1000, loss is 5.949631404876709 and perplexity is 383.6119155174028
At time: 243.21773266792297 and batch: 1050, loss is 5.957775678634643 and perplexity is 386.74891291746053
At time: 244.40103697776794 and batch: 1100, loss is 5.934731073379517 and perplexity is 377.93834482205534
At time: 245.59116220474243 and batch: 1150, loss is 5.986879653930664 and perplexity is 398.17024056017624
At time: 246.8273561000824 and batch: 1200, loss is 5.980174169540406 and perplexity is 395.50924782069904
At time: 248.0086522102356 and batch: 1250, loss is 5.96501163482666 and perplexity is 389.5575604791637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.605168920363823 and perplexity of 271.8278388137773
Finished 8 epochs...
Completing Train Step...
At time: 251.0053973197937 and batch: 50, loss is 5.95804274559021 and perplexity is 386.8522145658164
At time: 252.19785594940186 and batch: 100, loss is 5.971144781112671 and perplexity is 391.9541156839369
At time: 253.36032009124756 and batch: 150, loss is 5.895210847854615 and perplexity is 363.2934274638089
At time: 254.52941989898682 and batch: 200, loss is 5.9254246520996094 and perplexity is 374.4374072351675
At time: 255.69337391853333 and batch: 250, loss is 5.948774452209473 and perplexity is 383.2833190791228
At time: 256.8603951931 and batch: 300, loss is 5.954047689437866 and perplexity is 385.30980131121845
At time: 258.0336000919342 and batch: 350, loss is 5.986015748977661 and perplexity is 397.8264078579895
At time: 259.21398878097534 and batch: 400, loss is 5.955514421463013 and perplexity is 385.87536219814604
At time: 260.3915936946869 and batch: 450, loss is 5.9310878467559816 and perplexity is 376.5639349456943
At time: 261.5682032108307 and batch: 500, loss is 5.935560703277588 and perplexity is 378.25202387331507
At time: 262.7477674484253 and batch: 550, loss is 5.933806343078613 and perplexity is 377.58901532540364
At time: 263.914847612381 and batch: 600, loss is 5.964156122207641 and perplexity is 389.2244315886425
At time: 265.07941341400146 and batch: 650, loss is 5.956301136016846 and perplexity is 386.1790554058225
At time: 266.247740983963 and batch: 700, loss is 5.9751601409912105 and perplexity is 393.53111650835535
At time: 267.4279999732971 and batch: 750, loss is 5.930852365493775 and perplexity is 376.4752716346756
At time: 268.61045932769775 and batch: 800, loss is 5.949789781570434 and perplexity is 383.67267551561275
At time: 269.7874004840851 and batch: 850, loss is 5.969784688949585 and perplexity is 391.42138432690047
At time: 270.96305441856384 and batch: 900, loss is 5.9689414501190186 and perplexity is 391.09146173784035
At time: 272.1381812095642 and batch: 950, loss is 5.945274143218994 and perplexity is 381.94405431909706
At time: 273.31493616104126 and batch: 1000, loss is 5.9371155834198 and perplexity is 378.8406179119939
At time: 274.4949994087219 and batch: 1050, loss is 5.94174596786499 and perplexity is 380.5988631506172
At time: 275.6741533279419 and batch: 1100, loss is 5.9227752590179445 and perplexity is 373.4466883403263
At time: 276.89795184135437 and batch: 1150, loss is 5.97064326286316 and perplexity is 391.7575928259692
At time: 278.075581073761 and batch: 1200, loss is 5.9637132835388185 and perplexity is 389.05210611849157
At time: 279.2504971027374 and batch: 1250, loss is 5.94792254447937 and perplexity is 382.9569361006104
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.597094904767336 and perplexity of 269.64193299733347
Finished 9 epochs...
Completing Train Step...
At time: 282.2648482322693 and batch: 50, loss is 5.939894990921021 and perplexity is 379.8950350157992
At time: 283.4440085887909 and batch: 100, loss is 5.957515907287598 and perplexity is 386.6484596793849
At time: 284.6246647834778 and batch: 150, loss is 5.87571536064148 and perplexity is 356.2794378183261
At time: 285.80842566490173 and batch: 200, loss is 5.910611915588379 and perplexity is 368.93184149675847
At time: 286.99208188056946 and batch: 250, loss is 5.9369839096069335 and perplexity is 378.79073780738855
At time: 288.17434763908386 and batch: 300, loss is 5.9394261837005615 and perplexity is 379.71697922054955
At time: 289.357213973999 and batch: 350, loss is 5.964806499481202 and perplexity is 389.4776566502488
At time: 290.5359625816345 and batch: 400, loss is 5.940595092773438 and perplexity is 380.1610933566393
At time: 291.72249603271484 and batch: 450, loss is 5.916919145584107 and perplexity is 371.2661331927321
At time: 292.8958101272583 and batch: 500, loss is 5.9193208789825436 and perplexity is 372.1588871138431
At time: 294.0757441520691 and batch: 550, loss is 5.921601476669312 and perplexity is 373.00860036965923
At time: 295.25337433815 and batch: 600, loss is 5.950994787216186 and perplexity is 384.1352819214183
At time: 296.43020129203796 and batch: 650, loss is 5.9421186351776125 and perplexity is 380.7407263383793
At time: 297.6080367565155 and batch: 700, loss is 5.967653379440308 and perplexity is 390.5880325890488
At time: 298.78358268737793 and batch: 750, loss is 5.919320039749145 and perplexity is 372.1585747858066
At time: 299.95875906944275 and batch: 800, loss is 5.932666273117065 and perplexity is 377.1587827254524
At time: 301.13420057296753 and batch: 850, loss is 5.95446138381958 and perplexity is 385.46923478733305
At time: 302.3163230419159 and batch: 900, loss is 5.944861812591553 and perplexity is 381.7865995514744
At time: 303.49968218803406 and batch: 950, loss is 5.9261869525909425 and perplexity is 374.7229498754964
At time: 304.682532787323 and batch: 1000, loss is 5.918514442443848 and perplexity is 371.85888557143545
At time: 305.8614673614502 and batch: 1050, loss is 5.9238505554199214 and perplexity is 373.84847019920875
At time: 307.0849130153656 and batch: 1100, loss is 5.90646614074707 and perplexity is 367.4054992718812
At time: 308.2617208957672 and batch: 1150, loss is 5.954014291763306 and perplexity is 385.2969330747552
At time: 309.43631625175476 and batch: 1200, loss is 5.944716558456421 and perplexity is 381.73114749656946
At time: 310.61567091941833 and batch: 1250, loss is 5.929808692932129 and perplexity is 376.08255969051964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.573284511148494 and perplexity of 263.29748408737277
Finished 10 epochs...
Completing Train Step...
At time: 313.6233477592468 and batch: 50, loss is 5.917096710205078 and perplexity is 371.3320627761595
At time: 314.8256983757019 and batch: 100, loss is 5.936855268478394 and perplexity is 378.74201287347904
At time: 316.00021481513977 and batch: 150, loss is 5.854929313659668 and perplexity is 348.9502331672492
At time: 317.17437052726746 and batch: 200, loss is 5.8910958862304685 and perplexity is 361.80156054418876
At time: 318.35199332237244 and batch: 250, loss is 5.9171507358551025 and perplexity is 371.35212477415234
At time: 319.52993083000183 and batch: 300, loss is 5.92271146774292 and perplexity is 373.4228664597454
At time: 320.70552277565 and batch: 350, loss is 5.948566598892212 and perplexity is 383.20366064872275
At time: 321.8807215690613 and batch: 400, loss is 5.924906826019287 and perplexity is 374.24356397313244
At time: 323.05363845825195 and batch: 450, loss is 5.8952289581298825 and perplexity is 363.3000068673605
At time: 324.2289276123047 and batch: 500, loss is 5.89402886390686 and perplexity is 362.86427414035546
At time: 325.40715765953064 and batch: 550, loss is 5.8997285938262936 and perplexity is 364.938407886269
At time: 326.58186745643616 and batch: 600, loss is 5.934181089401245 and perplexity is 377.73054193699545
At time: 327.75661420822144 and batch: 650, loss is 5.927751321792602 and perplexity is 375.3096136771854
At time: 328.93070006370544 and batch: 700, loss is 5.950544548034668 and perplexity is 383.96236809570456
At time: 330.1078119277954 and batch: 750, loss is 5.900515184402466 and perplexity is 365.2255779266129
At time: 331.2944622039795 and batch: 800, loss is 5.91812352180481 and perplexity is 371.71354666809515
At time: 332.46609807014465 and batch: 850, loss is 5.939473648071289 and perplexity is 379.73500267575525
At time: 333.64138293266296 and batch: 900, loss is 5.932335529327393 and perplexity is 377.03406042704535
At time: 334.8220863342285 and batch: 950, loss is 5.912214918136597 and perplexity is 369.5237144389732
At time: 336.05414843559265 and batch: 1000, loss is 5.899838666915894 and perplexity is 364.9785799952324
At time: 337.2318799495697 and batch: 1050, loss is 5.90999475479126 and perplexity is 368.70422147368004
At time: 338.41585302352905 and batch: 1100, loss is 5.893058567047119 and perplexity is 362.5123588334135
At time: 339.593786239624 and batch: 1150, loss is 5.939212112426758 and perplexity is 379.6357014230542
At time: 340.76685333251953 and batch: 1200, loss is 5.930603322982788 and perplexity is 376.38152496164116
At time: 341.94729566574097 and batch: 1250, loss is 5.91330319404602 and perplexity is 369.92607709644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.559517296561359 and perplexity of 259.69744921124993
Finished 11 epochs...
Completing Train Step...
At time: 344.9307403564453 and batch: 50, loss is 5.90017539024353 and perplexity is 365.10149749063686
At time: 346.10069704055786 and batch: 100, loss is 5.917877740859986 and perplexity is 371.6221977877581
At time: 347.2646293640137 and batch: 150, loss is 5.824175405502319 and perplexity is 338.3819900767213
At time: 348.4278120994568 and batch: 200, loss is 5.858970994949341 and perplexity is 350.3634327230303
At time: 349.5917081832886 and batch: 250, loss is 5.887969455718994 and perplexity is 360.67217949182907
At time: 350.7557656764984 and batch: 300, loss is 5.885892648696899 and perplexity is 359.9239102511412
At time: 351.93107509613037 and batch: 350, loss is 5.909644231796265 and perplexity is 368.5750048137302
At time: 353.1090784072876 and batch: 400, loss is 5.883606672286987 and perplexity is 359.10207239181256
At time: 354.28400802612305 and batch: 450, loss is 5.850312280654907 and perplexity is 347.3428319912501
At time: 355.46173572540283 and batch: 500, loss is 5.853017053604126 and perplexity is 348.283587178385
At time: 356.6433973312378 and batch: 550, loss is 5.857956991195679 and perplexity is 350.0083429487107
At time: 357.8254990577698 and batch: 600, loss is 5.892472677230835 and perplexity is 362.30002874118884
At time: 359.0113787651062 and batch: 650, loss is 5.881219329833985 and perplexity is 358.24579528945844
At time: 360.187602519989 and batch: 700, loss is 5.901902227401734 and perplexity is 365.7325129967256
At time: 361.36272072792053 and batch: 750, loss is 5.857530832290649 and perplexity is 349.85921555476887
At time: 362.53772497177124 and batch: 800, loss is 5.873254423141479 and perplexity is 355.4037343570172
At time: 363.71382570266724 and batch: 850, loss is 5.89417763710022 and perplexity is 362.918262633096
At time: 364.8873589038849 and batch: 900, loss is 5.887866411209107 and perplexity is 360.635016118637
At time: 366.10796093940735 and batch: 950, loss is 5.865230770111084 and perplexity is 352.563507840442
At time: 367.2934455871582 and batch: 1000, loss is 5.852476692199707 and perplexity is 348.09543900866385
At time: 368.4715452194214 and batch: 1050, loss is 5.861089115142822 and perplexity is 351.1063310811985
At time: 369.649129152298 and batch: 1100, loss is 5.84409873008728 and perplexity is 345.1912910169486
At time: 370.8292224407196 and batch: 1150, loss is 5.889204721450806 and perplexity is 361.1179807604018
At time: 372.01163029670715 and batch: 1200, loss is 5.877401866912842 and perplexity is 356.8808122929345
At time: 373.18974900245667 and batch: 1250, loss is 5.866493616104126 and perplexity is 353.0090225026061
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.523647336194115 and perplexity of 250.54720257184584
Finished 12 epochs...
Completing Train Step...
At time: 376.1715440750122 and batch: 50, loss is 5.858621883392334 and perplexity is 350.2411381479809
At time: 377.3533926010132 and batch: 100, loss is 5.880141439437867 and perplexity is 357.8598536260395
At time: 378.53359961509705 and batch: 150, loss is 5.790182981491089 and perplexity is 327.0728671816728
At time: 379.7128553390503 and batch: 200, loss is 5.830758180618286 and perplexity is 340.6168302757398
At time: 380.89611077308655 and batch: 250, loss is 5.859608612060547 and perplexity is 350.5869016791194
At time: 382.07741379737854 and batch: 300, loss is 5.861381244659424 and perplexity is 351.20891458707854
At time: 383.2529227733612 and batch: 350, loss is 5.881996088027954 and perplexity is 358.5241737487824
At time: 384.4283435344696 and batch: 400, loss is 5.857915124893188 and perplexity is 349.9936897002914
At time: 385.59973073005676 and batch: 450, loss is 5.8259959888458255 and perplexity is 338.9986038196373
At time: 386.77730226516724 and batch: 500, loss is 5.827777328491211 and perplexity is 339.6030136421165
At time: 387.96224451065063 and batch: 550, loss is 5.831135530471801 and perplexity is 340.745386240511
At time: 389.1358742713928 and batch: 600, loss is 5.866771678924561 and perplexity is 353.10719483544744
At time: 390.3136658668518 and batch: 650, loss is 5.859925994873047 and perplexity is 350.69818959550173
At time: 391.4919888973236 and batch: 700, loss is 5.880501365661621 and perplexity is 357.98867995438457
At time: 392.6727206707001 and batch: 750, loss is 5.838168067932129 and perplexity is 343.1501367718913
At time: 393.8532974720001 and batch: 800, loss is 5.852923488616943 and perplexity is 348.2510015534752
At time: 395.03161120414734 and batch: 850, loss is 5.874654178619385 and perplexity is 355.90156101756565
At time: 396.24790024757385 and batch: 900, loss is 5.868195037841797 and perplexity is 353.61015096859165
At time: 397.42997574806213 and batch: 950, loss is 5.847880554199219 and perplexity is 346.4992153756373
At time: 398.61254382133484 and batch: 1000, loss is 5.837785987854004 and perplexity is 343.0190509850535
At time: 399.795480966568 and batch: 1050, loss is 5.846900901794434 and perplexity is 346.15993280266724
At time: 400.9749548435211 and batch: 1100, loss is 5.828480548858643 and perplexity is 339.8419133878892
At time: 402.15403389930725 and batch: 1150, loss is 5.870655317306518 and perplexity is 354.48120183644727
At time: 403.3376319408417 and batch: 1200, loss is 5.8597682762145995 and perplexity is 350.64288230912905
At time: 404.51964688301086 and batch: 1250, loss is 5.851574096679688 and perplexity is 347.78139137518974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.512895792940237 and perplexity of 247.86786281486042
Finished 13 epochs...
Completing Train Step...
At time: 407.55129170417786 and batch: 50, loss is 5.84202654838562 and perplexity is 344.47673254331687
At time: 408.7605710029602 and batch: 100, loss is 5.861922769546509 and perplexity is 351.39915446005745
At time: 409.94051909446716 and batch: 150, loss is 5.772926902770996 and perplexity is 321.47728975064865
At time: 411.12125611305237 and batch: 200, loss is 5.817398138046265 and perplexity is 336.0964384731106
At time: 412.30047035217285 and batch: 250, loss is 5.8451352787017825 and perplexity is 345.54928407794284
At time: 413.49077892303467 and batch: 300, loss is 5.84508171081543 and perplexity is 345.53077422893534
At time: 414.6793932914734 and batch: 350, loss is 5.864926614761353 and perplexity is 352.4562900696682
At time: 415.868111371994 and batch: 400, loss is 5.842085695266723 and perplexity is 344.49710787022195
At time: 417.04464173316956 and batch: 450, loss is 5.806100988388062 and perplexity is 332.3208734245973
At time: 418.21851539611816 and batch: 500, loss is 5.809481267929077 and perplexity is 333.4461116124581
At time: 419.4003448486328 and batch: 550, loss is 5.810642576217651 and perplexity is 333.8335702816033
At time: 420.5748133659363 and batch: 600, loss is 5.844396657943726 and perplexity is 345.2941484396299
At time: 421.7516586780548 and batch: 650, loss is 5.835738220214844 and perplexity is 342.3173463820279
At time: 422.92875123023987 and batch: 700, loss is 5.858242797851562 and perplexity is 350.1083919593957
At time: 424.10540890693665 and batch: 750, loss is 5.817542448043823 and perplexity is 336.1449440491615
At time: 425.2741713523865 and batch: 800, loss is 5.830417232513428 and perplexity is 340.50071740833033
At time: 426.48501443862915 and batch: 850, loss is 5.852570476531983 and perplexity is 348.1280864378644
At time: 427.65706753730774 and batch: 900, loss is 5.84780403137207 and perplexity is 346.4727012905513
At time: 428.83239245414734 and batch: 950, loss is 5.831637620925903 and perplexity is 340.91651420348495
At time: 429.99667954444885 and batch: 1000, loss is 5.820893573760986 and perplexity is 337.27329758718395
At time: 431.1649401187897 and batch: 1050, loss is 5.826015501022339 and perplexity is 339.005218484766
At time: 432.3297801017761 and batch: 1100, loss is 5.81150032043457 and perplexity is 334.12003693592044
At time: 433.4970557689667 and batch: 1150, loss is 5.852212438583374 and perplexity is 348.00346568275404
At time: 434.6689524650574 and batch: 1200, loss is 5.840746383666993 and perplexity is 344.0360277314889
At time: 435.8465120792389 and batch: 1250, loss is 5.833440093994141 and perplexity is 341.5315611751359
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.499898423243613 and perplexity of 244.66707851373008
Finished 14 epochs...
Completing Train Step...
At time: 438.8798131942749 and batch: 50, loss is 5.821178293228149 and perplexity is 337.3693395326182
At time: 440.057252407074 and batch: 100, loss is 5.84663592338562 and perplexity is 346.0682200459651
At time: 441.2388298511505 and batch: 150, loss is 5.753930616378784 and perplexity is 315.42805350698836
At time: 442.4130735397339 and batch: 200, loss is 5.79986457824707 and perplexity is 330.25483319280727
At time: 443.5899980068207 and batch: 250, loss is 5.8282215023040775 and perplexity is 339.7538899127104
At time: 444.76730489730835 and batch: 300, loss is 5.828039894104004 and perplexity is 339.6921934227591
At time: 445.9445970058441 and batch: 350, loss is 5.847742443084717 and perplexity is 346.4513632873563
At time: 447.120557308197 and batch: 400, loss is 5.823269109725953 and perplexity is 338.0754548350953
At time: 448.29518699645996 and batch: 450, loss is 5.788852138519287 and perplexity is 326.6378740730805
At time: 449.4739456176758 and batch: 500, loss is 5.792921237945556 and perplexity is 327.9697038953971
At time: 450.65134835243225 and batch: 550, loss is 5.791616010665893 and perplexity is 327.5419081370502
At time: 451.8282232284546 and batch: 600, loss is 5.825554513931275 and perplexity is 338.8489774705532
At time: 453.0133502483368 and batch: 650, loss is 5.815215072631836 and perplexity is 335.36351825982945
At time: 454.1986246109009 and batch: 700, loss is 5.841228790283203 and perplexity is 344.20203302528813
At time: 455.3779761791229 and batch: 750, loss is 5.803874053955078 and perplexity is 331.58164004640355
At time: 456.58561849594116 and batch: 800, loss is 5.817183313369751 and perplexity is 336.0242444192478
At time: 457.7684805393219 and batch: 850, loss is 5.837929525375366 and perplexity is 343.0682906231952
At time: 458.95208048820496 and batch: 900, loss is 5.8350715160369875 and perplexity is 342.08919803921987
At time: 460.13257241249084 and batch: 950, loss is 5.816612186431885 and perplexity is 335.83238671425096
At time: 461.309828042984 and batch: 1000, loss is 5.804253196716308 and perplexity is 331.70738066029946
At time: 462.4852933883667 and batch: 1050, loss is 5.803665771484375 and perplexity is 331.5125845949444
At time: 463.66046237945557 and batch: 1100, loss is 5.790400762557983 and perplexity is 327.14410521650575
At time: 464.83695697784424 and batch: 1150, loss is 5.830390691757202 and perplexity is 340.49168038172024
At time: 466.01701068878174 and batch: 1200, loss is 5.822435998916626 and perplexity is 337.79391781135706
At time: 467.19900369644165 and batch: 1250, loss is 5.81198670387268 and perplexity is 334.28258691583136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.485020101505475 and perplexity of 241.05378949339553
Finished 15 epochs...
Completing Train Step...
At time: 470.15628600120544 and batch: 50, loss is 5.7998965454101565 and perplexity is 330.26539067166544
At time: 471.3574993610382 and batch: 100, loss is 5.81532190322876 and perplexity is 335.3993472584544
At time: 472.54591846466064 and batch: 150, loss is 5.730474767684936 and perplexity is 308.1155169482905
At time: 473.72430086135864 and batch: 200, loss is 5.778288879394531 and perplexity is 323.20567309781046
At time: 474.9002089500427 and batch: 250, loss is 5.802454118728638 and perplexity is 331.11114970721474
At time: 476.08291149139404 and batch: 300, loss is 5.8003995323181154 and perplexity is 330.4315516242139
At time: 477.26129817962646 and batch: 350, loss is 5.8184591579437255 and perplexity is 336.4532327312276
At time: 478.45308470726013 and batch: 400, loss is 5.796348533630371 and perplexity is 329.09568147317697
At time: 479.63612508773804 and batch: 450, loss is 5.761264781951905 and perplexity is 317.7499592906166
At time: 480.8097779750824 and batch: 500, loss is 5.763894033432007 and perplexity is 318.586503101494
At time: 481.98516035079956 and batch: 550, loss is 5.761198110580445 and perplexity is 317.72877517124425
At time: 483.16339349746704 and batch: 600, loss is 5.794061603546143 and perplexity is 328.3439225962669
At time: 484.3405113220215 and batch: 650, loss is 5.785760803222656 and perplexity is 325.6296860102974
At time: 485.51843905448914 and batch: 700, loss is 5.810418357849121 and perplexity is 333.75872705404106
At time: 486.72199034690857 and batch: 750, loss is 5.776894359588623 and perplexity is 322.7552705059522
At time: 487.8957214355469 and batch: 800, loss is 5.7911745548248295 and perplexity is 327.3973447600312
At time: 489.0730547904968 and batch: 850, loss is 5.806219005584717 and perplexity is 332.36009531685227
At time: 490.2502784729004 and batch: 900, loss is 5.799334659576416 and perplexity is 330.07987135249175
At time: 491.43618988990784 and batch: 950, loss is 5.782635078430176 and perplexity is 324.61344629780126
At time: 492.61661434173584 and batch: 1000, loss is 5.767954454421997 and perplexity is 319.88272825481823
At time: 493.8000330924988 and batch: 1050, loss is 5.772290115356445 and perplexity is 321.27264222387174
At time: 494.9877977371216 and batch: 1100, loss is 5.752813892364502 and perplexity is 315.076004032496
At time: 496.1727020740509 and batch: 1150, loss is 5.7957807064056395 and perplexity is 328.9088650304456
At time: 497.3593747615814 and batch: 1200, loss is 5.782900619506836 and perplexity is 324.6996559474244
At time: 498.54368352890015 and batch: 1250, loss is 5.772616577148438 and perplexity is 321.3775425884191
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.451899841754106 and perplexity of 233.2007899145134
Finished 16 epochs...
Completing Train Step...
At time: 501.51800060272217 and batch: 50, loss is 5.763119707107544 and perplexity is 318.3399086701247
At time: 502.68879890441895 and batch: 100, loss is 5.778788385391235 and perplexity is 323.3671565973221
At time: 503.85728645324707 and batch: 150, loss is 5.692284727096558 and perplexity is 296.57042968082607
At time: 505.0348017215729 and batch: 200, loss is 5.740177888870239 and perplexity is 311.11975080005686
At time: 506.20622062683105 and batch: 250, loss is 5.764504432678223 and perplexity is 318.7810274255522
At time: 507.3682904243469 and batch: 300, loss is 5.762361078262329 and perplexity is 318.0984984147943
At time: 508.5310859680176 and batch: 350, loss is 5.78076699256897 and perplexity is 324.00760656492
At time: 509.6931223869324 and batch: 400, loss is 5.756028385162353 and perplexity is 316.09044315835274
At time: 510.85720777511597 and batch: 450, loss is 5.721765184402466 and perplexity is 305.4436116737337
At time: 512.0340805053711 and batch: 500, loss is 5.720185861587525 and perplexity is 304.9615983366583
At time: 513.2145190238953 and batch: 550, loss is 5.721661577224731 and perplexity is 305.41196716249857
At time: 514.3946771621704 and batch: 600, loss is 5.753814582824707 and perplexity is 315.39145539223006
At time: 515.6044702529907 and batch: 650, loss is 5.743704557418823 and perplexity is 312.2189040754737
At time: 516.780181646347 and batch: 700, loss is 5.766368322372436 and perplexity is 319.375754177636
At time: 517.9550507068634 and batch: 750, loss is 5.739173183441162 and perplexity is 310.807324072031
At time: 519.1317918300629 and batch: 800, loss is 5.7499797153472905 and perplexity is 314.18428710190483
At time: 520.3079364299774 and batch: 850, loss is 5.7656705188751225 and perplexity is 319.1529703981039
At time: 521.4836475849152 and batch: 900, loss is 5.762108478546143 and perplexity is 318.01815697191535
At time: 522.659086227417 and batch: 950, loss is 5.749755973815918 and perplexity is 314.11399889186464
At time: 523.8335087299347 and batch: 1000, loss is 5.738314361572265 and perplexity is 310.540510534155
At time: 525.0087847709656 and batch: 1050, loss is 5.738923292160035 and perplexity is 310.7296657351097
At time: 526.1857454776764 and batch: 1100, loss is 5.7234839344024655 and perplexity is 305.96904429560874
At time: 527.3621218204498 and batch: 1150, loss is 5.764433298110962 and perplexity is 318.75835188163245
At time: 528.5387620925903 and batch: 1200, loss is 5.760087471008301 and perplexity is 317.3760889102432
At time: 529.7152650356293 and batch: 1250, loss is 5.749728345870972 and perplexity is 314.1053206874775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.432901424213047 and perplexity of 228.81216440840984
Finished 17 epochs...
Completing Train Step...
At time: 532.7894115447998 and batch: 50, loss is 5.742800664901734 and perplexity is 311.9368192507728
At time: 534.0038459300995 and batch: 100, loss is 5.757373447418213 and perplexity is 316.51589054535106
At time: 535.1703839302063 and batch: 150, loss is 5.67029896736145 and perplexity is 290.12125821436155
At time: 536.3454301357269 and batch: 200, loss is 5.719576435089111 and perplexity is 304.7758032776
At time: 537.5248575210571 and batch: 250, loss is 5.748609399795532 and perplexity is 313.75405033456116
At time: 538.7099471092224 and batch: 300, loss is 5.742905178070068 and perplexity is 311.96942245977567
At time: 539.888733625412 and batch: 350, loss is 5.765323896408081 and perplexity is 319.0423639785831
At time: 541.0654528141022 and batch: 400, loss is 5.741459121704102 and perplexity is 311.5186231096306
At time: 542.2411122322083 and batch: 450, loss is 5.704874048233032 and perplexity is 300.3276508680304
At time: 543.4183340072632 and batch: 500, loss is 5.707617893218994 and perplexity is 301.1528349579115
At time: 544.5974833965302 and batch: 550, loss is 5.705777502059936 and perplexity is 300.5991056384937
At time: 545.8130638599396 and batch: 600, loss is 5.735598678588867 and perplexity is 309.6983250260567
At time: 546.9876747131348 and batch: 650, loss is 5.723918628692627 and perplexity is 306.10207620414093
At time: 548.1631417274475 and batch: 700, loss is 5.751085214614868 and perplexity is 314.5318096587533
At time: 549.3368635177612 and batch: 750, loss is 5.724061937332153 and perplexity is 306.1459464196532
At time: 550.5156855583191 and batch: 800, loss is 5.735837507247925 and perplexity is 309.7722986948994
At time: 551.7026860713959 and batch: 850, loss is 5.750791788101196 and perplexity is 314.43953122553785
At time: 552.8791933059692 and batch: 900, loss is 5.749923419952393 and perplexity is 314.1666004712344
At time: 554.0560176372528 and batch: 950, loss is 5.737817430496216 and perplexity is 310.38623164026154
At time: 555.229779958725 and batch: 1000, loss is 5.7265598106384275 and perplexity is 306.9116160818362
At time: 556.4096758365631 and batch: 1050, loss is 5.728542394638062 and perplexity is 307.5206977194299
At time: 557.5876379013062 and batch: 1100, loss is 5.709088201522827 and perplexity is 301.5959481485479
At time: 558.765406370163 and batch: 1150, loss is 5.751864919662475 and perplexity is 314.777147331459
At time: 559.9410281181335 and batch: 1200, loss is 5.74708514213562 and perplexity is 313.27617261691967
At time: 561.116382598877 and batch: 1250, loss is 5.739698791503907 and perplexity is 310.970729847458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.429149766908075 and perplexity of 227.9553478264043
Finished 18 epochs...
Completing Train Step...
At time: 564.1268815994263 and batch: 50, loss is 5.732294044494629 and perplexity is 308.67657456779153
At time: 565.3021397590637 and batch: 100, loss is 5.743944969177246 and perplexity is 312.29397419472343
At time: 566.4763276576996 and batch: 150, loss is 5.657847375869751 and perplexity is 286.53118427534804
At time: 567.6512279510498 and batch: 200, loss is 5.708613719940185 and perplexity is 301.4528803699491
At time: 568.8294072151184 and batch: 250, loss is 5.733190975189209 and perplexity is 308.9535602621356
At time: 570.0059587955475 and batch: 300, loss is 5.729907674789429 and perplexity is 307.9408363623745
At time: 571.1827526092529 and batch: 350, loss is 5.753099002838135 and perplexity is 315.16584830839946
At time: 572.3572602272034 and batch: 400, loss is 5.728316993713379 and perplexity is 307.451390081101
At time: 573.5257034301758 and batch: 450, loss is 5.694225559234619 and perplexity is 297.14658202859
At time: 574.7006888389587 and batch: 500, loss is 5.695903606414795 and perplexity is 297.6456266057323
At time: 575.8969647884369 and batch: 550, loss is 5.690994863510132 and perplexity is 296.18814088597065
At time: 577.0627489089966 and batch: 600, loss is 5.720429420471191 and perplexity is 305.0358834891476
At time: 578.2321264743805 and batch: 650, loss is 5.708557662963867 and perplexity is 301.4359823066046
At time: 579.3934755325317 and batch: 700, loss is 5.737172145843505 and perplexity is 310.1860087758897
At time: 580.5540132522583 and batch: 750, loss is 5.711443710327148 and perplexity is 302.30719740784633
At time: 581.7145547866821 and batch: 800, loss is 5.722380332946777 and perplexity is 305.63156266978785
At time: 582.88352227211 and batch: 850, loss is 5.738766222000122 and perplexity is 310.68086320963374
At time: 584.0703535079956 and batch: 900, loss is 5.737213754653931 and perplexity is 310.19891551524074
At time: 585.2501060962677 and batch: 950, loss is 5.725566015243531 and perplexity is 306.60676023839073
At time: 586.4277245998383 and batch: 1000, loss is 5.7136310195922855 and perplexity is 302.96916043666687
At time: 587.6023616790771 and batch: 1050, loss is 5.718077201843261 and perplexity is 304.31921561202216
At time: 588.7776446342468 and batch: 1100, loss is 5.698399410247803 and perplexity is 298.38941949597336
At time: 589.9527747631073 and batch: 1150, loss is 5.7373595046997075 and perplexity is 310.2441303163265
At time: 591.1326503753662 and batch: 1200, loss is 5.733712272644043 and perplexity is 309.1146589532828
At time: 592.3087317943573 and batch: 1250, loss is 5.729518957138062 and perplexity is 307.8211575858372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.419314196510037 and perplexity of 225.72426691783096
Finished 19 epochs...
Completing Train Step...
At time: 595.2731401920319 and batch: 50, loss is 5.717218837738037 and perplexity is 304.0581109982565
At time: 596.454350233078 and batch: 100, loss is 5.729647827148438 and perplexity is 307.86082905778585
At time: 597.6223378181458 and batch: 150, loss is 5.644662771224976 and perplexity is 282.7781792027711
At time: 598.8054201602936 and batch: 200, loss is 5.695654439926147 and perplexity is 297.5714725288286
At time: 599.9890978336334 and batch: 250, loss is 5.723770122528077 and perplexity is 306.0566215340758
At time: 601.1854083538055 and batch: 300, loss is 5.718665218353271 and perplexity is 304.49821295665464
At time: 602.3926246166229 and batch: 350, loss is 5.740599031448364 and perplexity is 311.2508041681542
At time: 603.575204372406 and batch: 400, loss is 5.71683409690857 and perplexity is 303.94114992971674
At time: 604.7599368095398 and batch: 450, loss is 5.682060976028442 and perplexity is 293.55381427206174
At time: 605.9827830791473 and batch: 500, loss is 5.683373336791992 and perplexity is 293.93931568303714
At time: 607.1578531265259 and batch: 550, loss is 5.67843243598938 and perplexity is 292.4905726772849
At time: 608.3319926261902 and batch: 600, loss is 5.712308473587036 and perplexity is 302.5687346329576
At time: 609.5085070133209 and batch: 650, loss is 5.699343624114991 and perplexity is 298.6712959785834
At time: 610.6880924701691 and batch: 700, loss is 5.724359817504883 and perplexity is 306.2371548109645
At time: 611.8655545711517 and batch: 750, loss is 5.697128763198853 and perplexity is 298.0105126400721
At time: 613.0534799098969 and batch: 800, loss is 5.7072641563415525 and perplexity is 301.0463249338132
At time: 614.2386100292206 and batch: 850, loss is 5.7239541339874265 and perplexity is 306.11294464153764
At time: 615.422310590744 and batch: 900, loss is 5.721923818588257 and perplexity is 305.49206931579425
At time: 616.5984282493591 and batch: 950, loss is 5.713577308654785 and perplexity is 302.95288811603086
At time: 617.7720453739166 and batch: 1000, loss is 5.703610506057739 and perplexity is 299.94841385618696
At time: 618.9462032318115 and batch: 1050, loss is 5.705143079757691 and perplexity is 300.4084593433668
At time: 620.1239495277405 and batch: 1100, loss is 5.6855049991607665 and perplexity is 294.5665633636033
At time: 621.308007478714 and batch: 1150, loss is 5.724975366592407 and perplexity is 306.42571684081315
At time: 622.4847538471222 and batch: 1200, loss is 5.719391584396362 and perplexity is 304.719470465971
At time: 623.6589117050171 and batch: 1250, loss is 5.716852483749389 and perplexity is 303.94673849863693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.417320975421989 and perplexity of 225.27479664454924
Finished 20 epochs...
Completing Train Step...
At time: 626.7007613182068 and batch: 50, loss is 5.707846059799194 and perplexity is 301.22155580998424
At time: 627.9095947742462 and batch: 100, loss is 5.718420095443726 and perplexity is 304.4235826159202
At time: 629.0838997364044 and batch: 150, loss is 5.633185443878173 and perplexity is 279.55119547171904
At time: 630.2558438777924 and batch: 200, loss is 5.680567226409912 and perplexity is 293.11564571250756
At time: 631.426629781723 and batch: 250, loss is 5.713118667602539 and perplexity is 302.81397334308826
At time: 632.5985014438629 and batch: 300, loss is 5.707135610580444 and perplexity is 301.00762919198576
At time: 633.7803912162781 and batch: 350, loss is 5.727661418914795 and perplexity is 307.24989875150953
At time: 634.9715559482574 and batch: 400, loss is 5.7015502071380615 and perplexity is 299.33106664143776
At time: 636.1936423778534 and batch: 450, loss is 5.669156522750854 and perplexity is 289.7900000046027
At time: 637.3713545799255 and batch: 500, loss is 5.665774412155152 and perplexity is 288.81155371451797
At time: 638.5563197135925 and batch: 550, loss is 5.665480480194092 and perplexity is 288.7266752430169
At time: 639.7497582435608 and batch: 600, loss is 5.69789761543274 and perplexity is 298.2397267930526
At time: 640.9313967227936 and batch: 650, loss is 5.685754480361939 and perplexity is 294.64006135145837
At time: 642.100474357605 and batch: 700, loss is 5.707357540130615 and perplexity is 301.0744390930018
At time: 643.2699570655823 and batch: 750, loss is 5.682485303878784 and perplexity is 293.67840376255754
At time: 644.4538006782532 and batch: 800, loss is 5.691824369430542 and perplexity is 296.433932631151
At time: 645.6369738578796 and batch: 850, loss is 5.70944637298584 and perplexity is 301.703990558234
At time: 646.8229701519012 and batch: 900, loss is 5.707470989227295 and perplexity is 301.1085976537425
At time: 647.9983627796173 and batch: 950, loss is 5.696854572296143 and perplexity is 297.9288120698827
At time: 649.1707355976105 and batch: 1000, loss is 5.690537309646606 and perplexity is 296.0526498573634
At time: 650.3427560329437 and batch: 1050, loss is 5.691348085403442 and perplexity is 296.2927795011074
At time: 651.5148611068726 and batch: 1100, loss is 5.670375375747681 and perplexity is 290.1434267584334
At time: 652.6903703212738 and batch: 1150, loss is 5.705280399322509 and perplexity is 300.4497141347519
At time: 653.8646385669708 and batch: 1200, loss is 5.702423830032348 and perplexity is 299.5926833747717
At time: 655.0401859283447 and batch: 1250, loss is 5.702652063369751 and perplexity is 299.66106821631314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.408550178917655 and perplexity of 223.3075968111968
Finished 21 epochs...
Completing Train Step...
At time: 658.0668866634369 and batch: 50, loss is 5.691076164245605 and perplexity is 296.21222217856337
At time: 659.2397639751434 and batch: 100, loss is 5.70153468132019 and perplexity is 299.32641931789067
At time: 660.4093673229218 and batch: 150, loss is 5.614462852478027 and perplexity is 274.3659646092319
At time: 661.5797474384308 and batch: 200, loss is 5.666435098648071 and perplexity is 289.00243065502116
At time: 662.7427878379822 and batch: 250, loss is 5.69661774635315 and perplexity is 297.85826315225614
At time: 663.9125311374664 and batch: 300, loss is 5.690801124572754 and perplexity is 296.1307632686104
At time: 665.1129033565521 and batch: 350, loss is 5.7128093051910405 and perplexity is 302.7203085709862
At time: 666.2820658683777 and batch: 400, loss is 5.687901344299316 and perplexity is 295.27329296151936
At time: 667.4534435272217 and batch: 450, loss is 5.6557386684417725 and perplexity is 285.92761044268804
At time: 668.6313519477844 and batch: 500, loss is 5.652749824523926 and perplexity is 285.074293294393
At time: 669.8089537620544 and batch: 550, loss is 5.652598476409912 and perplexity is 285.0311511025761
At time: 670.9838292598724 and batch: 600, loss is 5.682422246932983 and perplexity is 293.65988588321517
At time: 672.1641199588776 and batch: 650, loss is 5.66869083404541 and perplexity is 289.65507949256755
At time: 673.3400208950043 and batch: 700, loss is 5.692890977859497 and perplexity is 296.75028024184655
At time: 674.5169150829315 and batch: 750, loss is 5.670695056915283 and perplexity is 290.23619497520764
At time: 675.6940388679504 and batch: 800, loss is 5.679490737915039 and perplexity is 292.80027986653766
At time: 676.8687598705292 and batch: 850, loss is 5.699018039703369 and perplexity is 298.57406908905267
At time: 678.0431125164032 and batch: 900, loss is 5.702191972732544 and perplexity is 299.52322867626395
At time: 679.2186172008514 and batch: 950, loss is 5.687283258438111 and perplexity is 295.0908451039772
At time: 680.3953132629395 and batch: 1000, loss is 5.682721138000488 and perplexity is 293.7476713184779
At time: 681.5717544555664 and batch: 1050, loss is 5.682921648025513 and perplexity is 293.8065765767549
At time: 682.7493755817413 and batch: 1100, loss is 5.664149036407471 and perplexity is 288.34250771072226
At time: 683.9244801998138 and batch: 1150, loss is 5.698126745223999 and perplexity is 298.3080702288576
At time: 685.0995917320251 and batch: 1200, loss is 5.69568883895874 and perplexity is 297.58170887567013
At time: 686.2740631103516 and batch: 1250, loss is 5.696286582946778 and perplexity is 297.7596397263507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.405600443373632 and perplexity of 222.64986899409672
Finished 22 epochs...
Completing Train Step...
At time: 689.2649700641632 and batch: 50, loss is 5.6894999027252195 and perplexity is 295.7456820421049
At time: 690.4706914424896 and batch: 100, loss is 5.695206203460693 and perplexity is 297.43812003272086
At time: 691.6456685066223 and batch: 150, loss is 5.609193801879883 and perplexity is 272.92411837549446
At time: 692.8191049098969 and batch: 200, loss is 5.662504301071167 and perplexity is 287.86865039109244
At time: 693.9957156181335 and batch: 250, loss is 5.692806129455566 and perplexity is 296.72510252236185
At time: 695.1969256401062 and batch: 300, loss is 5.68437385559082 and perplexity is 294.23355466536395
At time: 696.3718285560608 and batch: 350, loss is 5.705100984573364 and perplexity is 300.3958138600563
At time: 697.5474908351898 and batch: 400, loss is 5.680416278839111 and perplexity is 293.07140395700026
At time: 698.7245576381683 and batch: 450, loss is 5.648607921600342 and perplexity is 283.89598514905896
At time: 699.895928144455 and batch: 500, loss is 5.646965961456299 and perplexity is 283.4302217435075
At time: 701.069659948349 and batch: 550, loss is 5.644479990005493 and perplexity is 282.72649738571005
At time: 702.2432065010071 and batch: 600, loss is 5.675463104248047 and perplexity is 291.6233592950848
At time: 703.4191763401031 and batch: 650, loss is 5.6614085388183595 and perplexity is 287.5533875483331
At time: 704.6000046730042 and batch: 700, loss is 5.688823556900024 and perplexity is 295.5457233129844
At time: 705.7798709869385 and batch: 750, loss is 5.664044818878174 and perplexity is 288.3124589328092
At time: 706.957231760025 and batch: 800, loss is 5.6776008129119875 and perplexity is 292.24743188188796
At time: 708.148731470108 and batch: 850, loss is 5.695891256332398 and perplexity is 297.64195068041744
At time: 709.337087392807 and batch: 900, loss is 5.694633073806763 and perplexity is 297.2676982674695
At time: 710.5229279994965 and batch: 950, loss is 5.6839198303222656 and perplexity is 294.0999955185972
At time: 711.7050127983093 and batch: 1000, loss is 5.6787136459350585 and perplexity is 292.57283550135907
At time: 712.8846244812012 and batch: 1050, loss is 5.6798429203033445 and perplexity is 292.90341712889534
At time: 714.0637362003326 and batch: 1100, loss is 5.658783559799194 and perplexity is 286.7995557682895
At time: 715.2482204437256 and batch: 1150, loss is 5.691615600585937 and perplexity is 296.3720529210365
At time: 716.4315326213837 and batch: 1200, loss is 5.690113964080811 and perplexity is 295.92734380655634
At time: 717.6158108711243 and batch: 1250, loss is 5.691201190948487 and perplexity is 296.2492589312988
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.403285117044936 and perplexity of 222.13495821360254
Finished 23 epochs...
Completing Train Step...
At time: 720.6781170368195 and batch: 50, loss is 5.6795725440979 and perplexity is 292.82423371954764
At time: 721.8554599285126 and batch: 100, loss is 5.69049654006958 and perplexity is 296.0405801620911
At time: 723.0312914848328 and batch: 150, loss is 5.603386907577515 and perplexity is 271.34386947718275
At time: 724.204861164093 and batch: 200, loss is 5.657615995407104 and perplexity is 286.46489422677456
At time: 725.4113729000092 and batch: 250, loss is 5.689744930267334 and perplexity is 295.81815675845496
At time: 726.5849936008453 and batch: 300, loss is 5.679969539642334 and perplexity is 292.9405067140398
At time: 727.7630748748779 and batch: 350, loss is 5.69996711730957 and perplexity is 298.8575735644112
At time: 728.9409852027893 and batch: 400, loss is 5.67457124710083 and perplexity is 291.3633888632777
At time: 730.1151134967804 and batch: 450, loss is 5.640734748840332 and perplexity is 281.6695988745361
At time: 731.2896180152893 and batch: 500, loss is 5.642871208190918 and perplexity is 282.27201781547416
At time: 732.4634103775024 and batch: 550, loss is 5.640047025680542 and perplexity is 281.47595476238394
At time: 733.6352245807648 and batch: 600, loss is 5.667643213272095 and perplexity is 289.35178970827263
At time: 734.8093309402466 and batch: 650, loss is 5.653735952377319 and perplexity is 285.35555165085435
At time: 735.984484910965 and batch: 700, loss is 5.6803544235229495 and perplexity is 293.0532764932964
At time: 737.1544396877289 and batch: 750, loss is 5.657033452987671 and perplexity is 286.29806487151495
At time: 738.3239722251892 and batch: 800, loss is 5.671094837188721 and perplexity is 290.35224887708097
At time: 739.4945828914642 and batch: 850, loss is 5.687755727767945 and perplexity is 295.23029941915286
At time: 740.6665015220642 and batch: 900, loss is 5.689184141159058 and perplexity is 295.65231166452185
At time: 741.8408005237579 and batch: 950, loss is 5.675714302062988 and perplexity is 291.69662364726275
At time: 743.0264575481415 and batch: 1000, loss is 5.672968759536743 and perplexity is 290.8968565619362
At time: 744.2010953426361 and batch: 1050, loss is 5.673574285507202 and perplexity is 291.0730555044435
At time: 745.3744502067566 and batch: 1100, loss is 5.650180988311767 and perplexity is 284.342923912886
At time: 746.5478467941284 and batch: 1150, loss is 5.683961877822876 and perplexity is 294.11236194832514
At time: 747.7268822193146 and batch: 1200, loss is 5.679636211395263 and perplexity is 292.8428776406087
At time: 748.9041383266449 and batch: 1250, loss is 5.682114839553833 and perplexity is 293.5696265412389
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.396283894559763 and perplexity of 220.5851734737913
Finished 24 epochs...
Completing Train Step...
At time: 751.9006552696228 and batch: 50, loss is 5.671608028411865 and perplexity is 290.5012933437097
At time: 753.1018121242523 and batch: 100, loss is 5.678104476928711 and perplexity is 292.3946634718213
At time: 754.2777571678162 and batch: 150, loss is 5.594896183013916 and perplexity is 269.04971671161627
At time: 755.4809749126434 and batch: 200, loss is 5.650852432250977 and perplexity is 284.5339083563027
At time: 756.6558575630188 and batch: 250, loss is 5.680054740905762 and perplexity is 292.96546667861617
At time: 757.8283622264862 and batch: 300, loss is 5.6714873695373536 and perplexity is 290.46624389916656
At time: 759.0010190010071 and batch: 350, loss is 5.693866739273071 and perplexity is 297.0399790302381
At time: 760.1738958358765 and batch: 400, loss is 5.6667929363250735 and perplexity is 289.10586511872526
At time: 761.3490860462189 and batch: 450, loss is 5.634985942840576 and perplexity is 280.05498050522186
At time: 762.5256922245026 and batch: 500, loss is 5.635796556472778 and perplexity is 280.28208892634063
At time: 763.7018971443176 and batch: 550, loss is 5.63165099143982 and perplexity is 279.12256639774876
At time: 764.8737173080444 and batch: 600, loss is 5.660236701965332 and perplexity is 287.2166192488809
At time: 766.0457248687744 and batch: 650, loss is 5.647366094589233 and perplexity is 283.5436542586423
At time: 767.2193069458008 and batch: 700, loss is 5.672424926757812 and perplexity is 290.73870032522075
At time: 768.3944225311279 and batch: 750, loss is 5.650582036972046 and perplexity is 284.4569821315001
At time: 769.5695555210114 and batch: 800, loss is 5.663488483428955 and perplexity is 288.15210510084984
At time: 770.7434592247009 and batch: 850, loss is 5.683838233947754 and perplexity is 294.07599900424674
At time: 771.918219089508 and batch: 900, loss is 5.681182880401611 and perplexity is 293.2961590910092
At time: 773.0929729938507 and batch: 950, loss is 5.670034856796264 and perplexity is 290.0446442426071
At time: 774.2646288871765 and batch: 1000, loss is 5.66705491065979 and perplexity is 289.1816133570162
At time: 775.4387354850769 and batch: 1050, loss is 5.6651369571685795 and perplexity is 288.62750801607933
At time: 776.6137573719025 and batch: 1100, loss is 5.642377042770386 and perplexity is 282.132563204747
At time: 777.7867262363434 and batch: 1150, loss is 5.67502290725708 and perplexity is 291.49501582010566
At time: 778.9578759670258 and batch: 1200, loss is 5.677136344909668 and perplexity is 292.1117238194848
At time: 780.1314990520477 and batch: 1250, loss is 5.676812686920166 and perplexity is 292.0171948246009
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.396528035184763 and perplexity of 220.63903385039438
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 783.1981887817383 and batch: 50, loss is 5.672742128372192 and perplexity is 290.8309377384815
At time: 784.3814141750336 and batch: 100, loss is 5.691363887786865 and perplexity is 296.29746167020926
At time: 785.6054191589355 and batch: 150, loss is 5.596388416290283 and perplexity is 269.4515013555417
At time: 786.7920265197754 and batch: 200, loss is 5.646115636825561 and perplexity is 283.1893164832173
At time: 787.9691865444183 and batch: 250, loss is 5.6684598541259765 and perplexity is 289.5881827118467
At time: 789.1529574394226 and batch: 300, loss is 5.654640588760376 and perplexity is 285.6138114629835
At time: 790.3311564922333 and batch: 350, loss is 5.677003335952759 and perplexity is 292.0728729276143
At time: 791.5100703239441 and batch: 400, loss is 5.638171005249023 and perplexity is 280.94839513131853
At time: 792.6896243095398 and batch: 450, loss is 5.604987335205078 and perplexity is 271.77848339375646
At time: 793.8679850101471 and batch: 500, loss is 5.594660959243774 and perplexity is 268.98643726560243
At time: 795.0474634170532 and batch: 550, loss is 5.5947194671630855 and perplexity is 269.0021755627728
At time: 796.2259984016418 and batch: 600, loss is 5.626568441390991 and perplexity is 277.7075110752023
At time: 797.4108550548553 and batch: 650, loss is 5.612966508865356 and perplexity is 273.95572585616867
At time: 798.5958371162415 and batch: 700, loss is 5.631145076751709 and perplexity is 278.98138990626654
At time: 799.7767531871796 and batch: 750, loss is 5.596598958969116 and perplexity is 269.5082383690244
At time: 800.9561240673065 and batch: 800, loss is 5.591696872711181 and perplexity is 268.19031865455867
At time: 802.1358151435852 and batch: 850, loss is 5.611449394226074 and perplexity is 273.54041872787644
At time: 803.3150005340576 and batch: 900, loss is 5.606197862625122 and perplexity is 272.10767790937695
At time: 804.5049557685852 and batch: 950, loss is 5.588506021499634 and perplexity is 267.33592709444457
At time: 805.6874585151672 and batch: 1000, loss is 5.577804594039917 and perplexity is 264.4903043328528
At time: 806.866632938385 and batch: 1050, loss is 5.56257417678833 and perplexity is 260.49252781964174
At time: 808.0477821826935 and batch: 1100, loss is 5.525770244598388 and perplexity is 251.07965630914055
At time: 809.2386076450348 and batch: 1150, loss is 5.550444936752319 and perplexity is 257.3520358147448
At time: 810.4228737354279 and batch: 1200, loss is 5.561937828063964 and perplexity is 260.3268164625542
At time: 811.6126413345337 and batch: 1250, loss is 5.589751014709472 and perplexity is 267.668965780857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.315555767421305 and perplexity of 203.47756791306722
Finished 26 epochs...
Completing Train Step...
At time: 814.681768655777 and batch: 50, loss is 5.613653020858765 and perplexity is 274.1438643198986
At time: 815.8616802692413 and batch: 100, loss is 5.637381353378296 and perplexity is 280.7266312751706
At time: 817.0474736690521 and batch: 150, loss is 5.549793100357055 and perplexity is 257.1843390527697
At time: 818.2309188842773 and batch: 200, loss is 5.599701023101806 and perplexity is 270.345568262794
At time: 819.4109282493591 and batch: 250, loss is 5.629304990768433 and perplexity is 278.4685121753804
At time: 820.5893692970276 and batch: 300, loss is 5.6184804725646975 and perplexity is 275.4704800958572
At time: 821.7794570922852 and batch: 350, loss is 5.641809177398682 and perplexity is 281.9723953730596
At time: 822.9674339294434 and batch: 400, loss is 5.608070125579834 and perplexity is 272.61761225104897
At time: 824.1456532478333 and batch: 450, loss is 5.575524625778198 and perplexity is 263.8879617553486
At time: 825.3200511932373 and batch: 500, loss is 5.567132234573364 and perplexity is 261.6825779121959
At time: 826.49649477005 and batch: 550, loss is 5.56739577293396 and perplexity is 261.75155039782396
At time: 827.6601526737213 and batch: 600, loss is 5.601022930145263 and perplexity is 270.70317628385413
At time: 828.8215544223785 and batch: 650, loss is 5.590757665634155 and perplexity is 267.93855065893723
At time: 829.9831717014313 and batch: 700, loss is 5.608251104354858 and perplexity is 272.66695471739905
At time: 831.1489140987396 and batch: 750, loss is 5.5765800952911375 and perplexity is 264.16663449323215
At time: 832.3302671909332 and batch: 800, loss is 5.575881061553955 and perplexity is 263.9820376307176
At time: 833.5170195102692 and batch: 850, loss is 5.60061318397522 and perplexity is 270.5922794154883
At time: 834.7024993896484 and batch: 900, loss is 5.595161809921264 and perplexity is 269.1211930483881
At time: 835.8865549564362 and batch: 950, loss is 5.579982261657715 and perplexity is 265.06690389705545
At time: 837.0680551528931 and batch: 1000, loss is 5.572326812744141 and perplexity is 263.0454452148689
At time: 838.2522251605988 and batch: 1050, loss is 5.561200847625733 and perplexity is 260.1350313708851
At time: 839.4470863342285 and batch: 1100, loss is 5.530946941375732 and perplexity is 252.38278960965818
At time: 840.6224434375763 and batch: 1150, loss is 5.561984767913819 and perplexity is 260.33903645103186
At time: 841.7948307991028 and batch: 1200, loss is 5.576014051437378 and perplexity is 264.01714690566536
At time: 842.967291355133 and batch: 1250, loss is 5.596665086746216 and perplexity is 269.52606093901494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.311987661097172 and perplexity of 202.7528320523886
Finished 27 epochs...
Completing Train Step...
At time: 846.0257647037506 and batch: 50, loss is 5.604095592498779 and perplexity is 271.5362349411595
At time: 847.2455430030823 and batch: 100, loss is 5.62584587097168 and perplexity is 277.50692032167876
At time: 848.4244232177734 and batch: 150, loss is 5.537812356948852 and perplexity is 254.12146387531448
At time: 849.6091268062592 and batch: 200, loss is 5.585872182846069 and perplexity is 266.6327338514593
At time: 850.795077085495 and batch: 250, loss is 5.617134370803833 and perplexity is 275.09991826045814
At time: 851.9824771881104 and batch: 300, loss is 5.6071991443634035 and perplexity is 272.3802708064843
At time: 853.1606216430664 and batch: 350, loss is 5.630731678009033 and perplexity is 278.86608318591794
At time: 854.3376262187958 and batch: 400, loss is 5.596857566833496 and perplexity is 269.57794433184836
At time: 855.5178782939911 and batch: 450, loss is 5.564459419250488 and perplexity is 260.9840825985451
At time: 856.7029633522034 and batch: 500, loss is 5.557741403579712 and perplexity is 259.2366636079055
At time: 857.8843622207642 and batch: 550, loss is 5.558434209823608 and perplexity is 259.4163266157405
At time: 859.0706655979156 and batch: 600, loss is 5.593014192581177 and perplexity is 268.5438438920491
At time: 860.2498571872711 and batch: 650, loss is 5.583686761856079 and perplexity is 266.05066534231696
At time: 861.4255836009979 and batch: 700, loss is 5.600937242507935 and perplexity is 270.6799813620342
At time: 862.6005096435547 and batch: 750, loss is 5.569887971878051 and perplexity is 262.40470088775766
At time: 863.7748589515686 and batch: 800, loss is 5.571205406188965 and perplexity is 262.7506296632374
At time: 864.953626871109 and batch: 850, loss is 5.597842254638672 and perplexity is 269.8435251818294
At time: 866.1317553520203 and batch: 900, loss is 5.592650136947632 and perplexity is 268.44609678659816
At time: 867.3104918003082 and batch: 950, loss is 5.578862819671631 and perplexity is 264.7703428980982
At time: 868.4842221736908 and batch: 1000, loss is 5.572089977264405 and perplexity is 262.9831540973235
At time: 869.6582117080688 and batch: 1050, loss is 5.563063087463379 and perplexity is 260.6199165355773
At time: 870.8333868980408 and batch: 1100, loss is 5.534790296554565 and perplexity is 253.35465272240356
At time: 872.0120537281036 and batch: 1150, loss is 5.567929792404175 and perplexity is 261.89136815147396
At time: 873.1924769878387 and batch: 1200, loss is 5.582537965774536 and perplexity is 265.745202871119
At time: 874.4112060070038 and batch: 1250, loss is 5.599179363250732 and perplexity is 270.20457661195456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.3110275825444795 and perplexity of 202.55826682073553
Finished 28 epochs...
Completing Train Step...
At time: 877.4255840778351 and batch: 50, loss is 5.598422079086304 and perplexity is 270.0000324237428
At time: 878.602769613266 and batch: 100, loss is 5.619023790359497 and perplexity is 275.6201887756475
At time: 879.7843379974365 and batch: 150, loss is 5.5311292552947995 and perplexity is 252.4288066997876
At time: 880.9656684398651 and batch: 200, loss is 5.578394842147827 and perplexity is 264.64646531687475
At time: 882.1431305408478 and batch: 250, loss is 5.610673837661743 and perplexity is 273.3283549048965
At time: 883.3190593719482 and batch: 300, loss is 5.601097450256348 and perplexity is 270.7233498662812
At time: 884.4946513175964 and batch: 350, loss is 5.624954776763916 and perplexity is 277.25974565667764
At time: 885.6667585372925 and batch: 400, loss is 5.590529060363769 and perplexity is 267.87730549486747
At time: 886.8437819480896 and batch: 450, loss is 5.558080644607544 and perplexity is 259.32462223886415
At time: 888.019394159317 and batch: 500, loss is 5.552643795013427 and perplexity is 257.91853906663425
At time: 889.1915175914764 and batch: 550, loss is 5.5534507942199705 and perplexity is 258.12676313004033
At time: 890.3693730831146 and batch: 600, loss is 5.588799638748169 and perplexity is 267.4144330585812
At time: 891.5458428859711 and batch: 650, loss is 5.5798123264312744 and perplexity is 265.0218635198015
At time: 892.7252819538116 and batch: 700, loss is 5.596839294433594 and perplexity is 269.57301854084807
At time: 893.9040133953094 and batch: 750, loss is 5.566083908081055 and perplexity is 261.40839287602375
At time: 895.092321395874 and batch: 800, loss is 5.568834848403931 and perplexity is 262.1285017989322
At time: 896.2757506370544 and batch: 850, loss is 5.596445713043213 and perplexity is 269.46694049394347
At time: 897.459498167038 and batch: 900, loss is 5.591805467605591 and perplexity is 268.21944433531604
At time: 898.6377487182617 and batch: 950, loss is 5.57851809501648 and perplexity is 264.6790857631443
At time: 899.8165435791016 and batch: 1000, loss is 5.572065544128418 and perplexity is 262.97672867265425
At time: 900.9941415786743 and batch: 1050, loss is 5.5642830276489255 and perplexity is 260.93805125812355
At time: 902.1722173690796 and batch: 1100, loss is 5.5369041156768795 and perplexity is 253.89076505468253
At time: 903.358015537262 and batch: 1150, loss is 5.57084924697876 and perplexity is 262.65706526939505
At time: 904.5821311473846 and batch: 1200, loss is 5.585667867660522 and perplexity is 266.5782622998416
At time: 905.7519085407257 and batch: 1250, loss is 5.600143384933472 and perplexity is 270.46518527860394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.308744778598312 and perplexity of 202.09639339372518
Finished 29 epochs...
Completing Train Step...
At time: 908.7363708019257 and batch: 50, loss is 5.593643474578857 and perplexity is 268.71288688087475
At time: 909.9419538974762 and batch: 100, loss is 5.613856287002563 and perplexity is 274.1995941498469
At time: 911.1171391010284 and batch: 150, loss is 5.526042432785034 and perplexity is 251.1480065271339
At time: 912.2945971488953 and batch: 200, loss is 5.572349548339844 and perplexity is 263.05142577774836
At time: 913.4735126495361 and batch: 250, loss is 5.605519571304321 and perplexity is 271.92317221450736
At time: 914.6540293693542 and batch: 300, loss is 5.596409950256348 and perplexity is 269.4573037775023
At time: 915.8358838558197 and batch: 350, loss is 5.620506267547608 and perplexity is 276.0290924383776
At time: 917.0121290683746 and batch: 400, loss is 5.585723857879639 and perplexity is 266.5931884930161
At time: 918.1862907409668 and batch: 450, loss is 5.553638868331909 and perplexity is 258.1753146572831
At time: 919.3605012893677 and batch: 500, loss is 5.548477563858032 and perplexity is 256.84622611648547
At time: 920.5355844497681 and batch: 550, loss is 5.549870901107788 and perplexity is 257.2043489658077
At time: 921.7085795402527 and batch: 600, loss is 5.58555115699768 and perplexity is 266.54715158966053
At time: 922.8817372322083 and batch: 650, loss is 5.577060251235962 and perplexity is 264.29350612985615
At time: 924.0581135749817 and batch: 700, loss is 5.593883094787597 and perplexity is 268.7772836339939
At time: 925.2372443675995 and batch: 750, loss is 5.563137636184693 and perplexity is 260.63934614132097
At time: 926.418060541153 and batch: 800, loss is 5.566507368087769 and perplexity is 261.51911231679924
At time: 927.593905210495 and batch: 850, loss is 5.594565410614013 and perplexity is 268.9607372079196
At time: 928.7688908576965 and batch: 900, loss is 5.590756998062134 and perplexity is 267.9383717907172
At time: 929.9454300403595 and batch: 950, loss is 5.577469654083252 and perplexity is 264.4017307960176
At time: 931.1248066425323 and batch: 1000, loss is 5.5719145488739015 and perplexity is 262.93702343230325
At time: 932.3038036823273 and batch: 1050, loss is 5.564798383712769 and perplexity is 261.0725619225931
At time: 933.485121011734 and batch: 1100, loss is 5.538523502349854 and perplexity is 254.3022454590222
At time: 934.7039341926575 and batch: 1150, loss is 5.573239688873291 and perplexity is 263.28568275956377
At time: 935.8810317516327 and batch: 1200, loss is 5.589536962509155 and perplexity is 267.6116767814118
At time: 937.2820136547089 and batch: 1250, loss is 5.599284734725952 and perplexity is 270.2330499669167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.308855711108577 and perplexity of 202.1188136975071
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 940.4234869480133 and batch: 50, loss is 5.5931081962585445 and perplexity is 268.5690891874661
At time: 941.5995869636536 and batch: 100, loss is 5.61761643409729 and perplexity is 275.2325658027712
At time: 942.776168346405 and batch: 150, loss is 5.52874979019165 and perplexity is 251.82887520430512
At time: 943.952431678772 and batch: 200, loss is 5.575615491867065 and perplexity is 263.9119413117803
At time: 945.1296765804291 and batch: 250, loss is 5.605197582244873 and perplexity is 271.8356300226142
At time: 946.3099074363708 and batch: 300, loss is 5.598075752258301 and perplexity is 269.90654035929225
At time: 947.489207983017 and batch: 350, loss is 5.619188976287842 and perplexity is 275.6657211129481
At time: 948.6977779865265 and batch: 400, loss is 5.581719770431518 and perplexity is 265.5278603101609
At time: 949.8774130344391 and batch: 450, loss is 5.550455389022827 and perplexity is 257.35472574189674
At time: 951.0548613071442 and batch: 500, loss is 5.542747898101807 and perplexity is 255.37879106082573
At time: 952.2316217422485 and batch: 550, loss is 5.543798503875732 and perplexity is 255.6472344826651
At time: 953.4111974239349 and batch: 600, loss is 5.579067983627319 and perplexity is 264.8246698017668
At time: 954.5976686477661 and batch: 650, loss is 5.571441707611084 and perplexity is 262.8127253470496
At time: 955.7766783237457 and batch: 700, loss is 5.5863891696929935 and perplexity is 266.7706151061465
At time: 956.9571752548218 and batch: 750, loss is 5.5536906528472905 and perplexity is 258.18868448700823
At time: 958.1366455554962 and batch: 800, loss is 5.5560793113708495 and perplexity is 258.8061462479567
At time: 959.3149642944336 and batch: 850, loss is 5.577286996841431 and perplexity is 264.3534403155748
At time: 960.4943995475769 and batch: 900, loss is 5.57296573638916 and perplexity is 263.2135648716241
At time: 961.6827049255371 and batch: 950, loss is 5.560149717330932 and perplexity is 259.8617392166664
At time: 962.8648340702057 and batch: 1000, loss is 5.55351879119873 and perplexity is 258.1443155668197
At time: 964.0453264713287 and batch: 1050, loss is 5.540996255874634 and perplexity is 254.93185034087557
At time: 965.2309384346008 and batch: 1100, loss is 5.51223198890686 and perplexity is 247.70338172542435
At time: 966.4211802482605 and batch: 1150, loss is 5.5428002834320065 and perplexity is 255.39216951353578
At time: 967.6064851284027 and batch: 1200, loss is 5.565434074401855 and perplexity is 261.2385760806199
At time: 968.7980210781097 and batch: 1250, loss is 5.58600115776062 and perplexity is 266.6671250032746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.299543617415602 and perplexity of 200.24540060411437
Finished 31 epochs...
Completing Train Step...
At time: 971.8774337768555 and batch: 50, loss is 5.587773303985596 and perplexity is 267.14011712398883
At time: 973.0548481941223 and batch: 100, loss is 5.6098588180541995 and perplexity is 273.1056776918251
At time: 974.2400448322296 and batch: 150, loss is 5.522785530090332 and perplexity is 250.33137247858795
At time: 975.4099535942078 and batch: 200, loss is 5.569022378921509 and perplexity is 262.17766350206216
At time: 976.5809473991394 and batch: 250, loss is 5.599217672348022 and perplexity is 270.2149281036451
At time: 977.7513453960419 and batch: 300, loss is 5.592367658615112 and perplexity is 268.37027728999306
At time: 978.9280707836151 and batch: 350, loss is 5.614020528793335 and perplexity is 274.2446328807387
At time: 980.1073944568634 and batch: 400, loss is 5.576808309555053 and perplexity is 264.22692796692786
At time: 981.2905185222626 and batch: 450, loss is 5.54576774597168 and perplexity is 256.1511617931086
At time: 982.4763972759247 and batch: 500, loss is 5.539302864074707 and perplexity is 254.50051614793438
At time: 983.6553735733032 and batch: 550, loss is 5.5401871871948245 and perplexity is 254.7256763809723
At time: 984.8315267562866 and batch: 600, loss is 5.575703468322754 and perplexity is 263.93516037034107
At time: 985.9992804527283 and batch: 650, loss is 5.5689122676849365 and perplexity is 262.14879638465845
At time: 987.2288918495178 and batch: 700, loss is 5.5837195777893065 and perplexity is 266.05939618644067
At time: 988.4023985862732 and batch: 750, loss is 5.551685562133789 and perplexity is 257.6715114161428
At time: 989.5673294067383 and batch: 800, loss is 5.55472806930542 and perplexity is 258.4566726612343
At time: 990.7383413314819 and batch: 850, loss is 5.576971454620361 and perplexity is 264.2700388029116
At time: 991.9179975986481 and batch: 900, loss is 5.57236605644226 and perplexity is 263.05576829346916
At time: 993.0821266174316 and batch: 950, loss is 5.559842472076416 and perplexity is 259.7819101946338
At time: 994.2568809986115 and batch: 1000, loss is 5.5536313819885255 and perplexity is 258.1733818754592
At time: 995.4368431568146 and batch: 1050, loss is 5.542172889709473 and perplexity is 255.23198832318968
At time: 996.6198728084564 and batch: 1100, loss is 5.5140671539306645 and perplexity is 248.1583756746405
At time: 997.805025100708 and batch: 1150, loss is 5.545625381469726 and perplexity is 256.11469755620317
At time: 998.9892809391022 and batch: 1200, loss is 5.568519592285156 and perplexity is 262.0458772094728
At time: 1000.1773660182953 and batch: 1250, loss is 5.587394027709961 and perplexity is 267.03881642698326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.299058009238139 and perplexity of 200.14818340672505
Finished 32 epochs...
Completing Train Step...
At time: 1003.1848344802856 and batch: 50, loss is 5.585724153518677 and perplexity is 266.5932673083815
At time: 1004.4018366336823 and batch: 100, loss is 5.607111206054688 and perplexity is 272.3563191992894
At time: 1005.5796549320221 and batch: 150, loss is 5.520295467376709 and perplexity is 249.70880709703067
At time: 1006.757262468338 and batch: 200, loss is 5.56601240158081 and perplexity is 261.38970114501274
At time: 1007.940417766571 and batch: 250, loss is 5.596550674438476 and perplexity is 269.4952256043914
At time: 1009.1216259002686 and batch: 300, loss is 5.589790630340576 and perplexity is 267.6795698659056
At time: 1010.2986204624176 and batch: 350, loss is 5.611419124603271 and perplexity is 273.53213888789463
At time: 1011.477441072464 and batch: 400, loss is 5.574414806365967 and perplexity is 263.59525622792046
At time: 1012.6552782058716 and batch: 450, loss is 5.543464527130127 and perplexity is 255.56186850718856
At time: 1013.8420467376709 and batch: 500, loss is 5.537493209838868 and perplexity is 254.04037468491222
At time: 1015.0196290016174 and batch: 550, loss is 5.538512258529663 and perplexity is 254.29938614637507
At time: 1016.1938507556915 and batch: 600, loss is 5.574250860214233 and perplexity is 263.55204434235463
At time: 1017.4008922576904 and batch: 650, loss is 5.567643556594849 and perplexity is 261.8164161912349
At time: 1018.5741724967957 and batch: 700, loss is 5.582521133422851 and perplexity is 265.740729792052
At time: 1019.7576439380646 and batch: 750, loss is 5.551162910461426 and perplexity is 257.5368741570399
At time: 1020.9361245632172 and batch: 800, loss is 5.554328184127808 and perplexity is 258.35334033069284
At time: 1022.112949848175 and batch: 850, loss is 5.577211694717407 and perplexity is 264.33353468950355
At time: 1023.2878503799438 and batch: 900, loss is 5.572572517395019 and perplexity is 263.1100846449297
At time: 1024.4626016616821 and batch: 950, loss is 5.560267896652221 and perplexity is 259.8924513153678
At time: 1025.6381726264954 and batch: 1000, loss is 5.554262800216675 and perplexity is 258.33644873107306
At time: 1026.8157589435577 and batch: 1050, loss is 5.543372278213501 and perplexity is 255.53829428905252
At time: 1027.9921684265137 and batch: 1100, loss is 5.5155086421966555 and perplexity is 248.51635100791802
At time: 1029.1698679924011 and batch: 1150, loss is 5.547469186782837 and perplexity is 256.58735881008295
At time: 1030.3483664989471 and batch: 1200, loss is 5.570357503890992 and perplexity is 262.52793722460905
At time: 1031.526639699936 and batch: 1250, loss is 5.588077802658081 and perplexity is 267.2214733208188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.2988356987055205 and perplexity of 200.1036933029619
Finished 33 epochs...
Completing Train Step...
At time: 1034.6114168167114 and batch: 50, loss is 5.584208288192749 and perplexity is 266.1894539589953
At time: 1035.787305355072 and batch: 100, loss is 5.605283374786377 and perplexity is 271.85895249261785
At time: 1036.966937303543 and batch: 150, loss is 5.518553133010864 and perplexity is 249.27410966507733
At time: 1038.147260427475 and batch: 200, loss is 5.5638994693756105 and perplexity is 260.83798550149197
At time: 1039.3269550800323 and batch: 250, loss is 5.594652452468872 and perplexity is 268.98414906826144
At time: 1040.5029635429382 and batch: 300, loss is 5.588000249862671 and perplexity is 267.20075035214177
At time: 1041.68212556839 and batch: 350, loss is 5.609558162689209 and perplexity is 273.02357934688996
At time: 1042.8589107990265 and batch: 400, loss is 5.5726933097839355 and perplexity is 263.14186826017266
At time: 1044.04079580307 and batch: 450, loss is 5.541884937286377 and perplexity is 255.1585042341689
At time: 1045.2206993103027 and batch: 500, loss is 5.536233797073364 and perplexity is 253.72063437885447
At time: 1046.438225030899 and batch: 550, loss is 5.537436065673828 and perplexity is 254.02585817458532
At time: 1047.6147315502167 and batch: 600, loss is 5.573364534378052 and perplexity is 263.3185548454476
At time: 1048.7917850017548 and batch: 650, loss is 5.566822528839111 and perplexity is 261.6015458659548
At time: 1049.9669697284698 and batch: 700, loss is 5.581801710128784 and perplexity is 265.54961847406685
At time: 1051.147183895111 and batch: 750, loss is 5.550971946716309 and perplexity is 257.4876986465643
At time: 1052.3358030319214 and batch: 800, loss is 5.554195699691772 and perplexity is 258.3191148013261
At time: 1053.517139673233 and batch: 850, loss is 5.577513828277588 and perplexity is 264.4134107874316
At time: 1054.6993851661682 and batch: 900, loss is 5.57290057182312 and perplexity is 263.19641323273896
At time: 1055.877958536148 and batch: 950, loss is 5.5607393360137936 and perplexity is 260.0150037324401
At time: 1057.0674700737 and batch: 1000, loss is 5.554859781265259 and perplexity is 258.49071673808083
At time: 1058.2557501792908 and batch: 1050, loss is 5.54431303024292 and perplexity is 255.77880557099954
At time: 1059.4471907615662 and batch: 1100, loss is 5.516569414138794 and perplexity is 248.78011004959268
At time: 1060.6447219848633 and batch: 1150, loss is 5.548794937133789 and perplexity is 256.92775518152274
At time: 1061.826617717743 and batch: 1200, loss is 5.571638736724854 and perplexity is 262.8645122070043
At time: 1062.9973077774048 and batch: 1250, loss is 5.58848539352417 and perplexity is 267.330412552377
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.298677987425867 and perplexity of 200.0721371818712
Finished 34 epochs...
Completing Train Step...
At time: 1065.9317014217377 and batch: 50, loss is 5.582978105545044 and perplexity is 265.8621936479823
At time: 1067.1456351280212 and batch: 100, loss is 5.603885021209717 and perplexity is 271.47906322571293
At time: 1068.318440914154 and batch: 150, loss is 5.517163896560669 and perplexity is 248.92804942125042
At time: 1069.489149570465 and batch: 200, loss is 5.562230052947998 and perplexity is 260.4029015527428
At time: 1070.657588481903 and batch: 250, loss is 5.593149299621582 and perplexity is 268.5801285071147
At time: 1071.8226716518402 and batch: 300, loss is 5.5866079521179195 and perplexity is 266.8289862132718
At time: 1073.006534576416 and batch: 350, loss is 5.6080835342407225 and perplexity is 272.6212677126712
At time: 1074.1820833683014 and batch: 400, loss is 5.571319799423218 and perplexity is 262.7806882767849
At time: 1075.3561308383942 and batch: 450, loss is 5.540665912628174 and perplexity is 254.84764923420695
At time: 1076.5708799362183 and batch: 500, loss is 5.535254621505738 and perplexity is 253.47231892472104
At time: 1077.7487659454346 and batch: 550, loss is 5.536665945053101 and perplexity is 253.83030293323424
At time: 1078.9265434741974 and batch: 600, loss is 5.572746448516845 and perplexity is 263.15585165715424
At time: 1080.1008729934692 and batch: 650, loss is 5.566239175796508 and perplexity is 261.44898431117423
At time: 1081.2761538028717 and batch: 700, loss is 5.5813051700592045 and perplexity is 265.4177951785156
At time: 1082.453877210617 and batch: 750, loss is 5.550881328582764 and perplexity is 257.4643666490693
At time: 1083.631583929062 and batch: 800, loss is 5.5541477394104 and perplexity is 258.30672604098305
At time: 1084.8097968101501 and batch: 850, loss is 5.577791805267334 and perplexity is 264.4869218481292
At time: 1085.9919600486755 and batch: 900, loss is 5.573232479095459 and perplexity is 263.2837845351277
At time: 1087.1655757427216 and batch: 950, loss is 5.561161966323852 and perplexity is 260.1249171788284
At time: 1088.3439226150513 and batch: 1000, loss is 5.555357398986817 and perplexity is 258.6193783090718
At time: 1089.5240948200226 and batch: 1050, loss is 5.545042390823364 and perplexity is 255.9654285985629
At time: 1090.7083575725555 and batch: 1100, loss is 5.517369890213013 and perplexity is 248.97933230111403
At time: 1091.890046596527 and batch: 1150, loss is 5.549805593490601 and perplexity is 257.1875521111338
At time: 1093.073578119278 and batch: 1200, loss is 5.572604188919067 and perplexity is 263.1184178742651
At time: 1094.2557430267334 and batch: 1250, loss is 5.588757247924804 and perplexity is 267.4030973808504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.298549234432025 and perplexity of 200.04637895348483
Finished 35 epochs...
Completing Train Step...
At time: 1097.2745566368103 and batch: 50, loss is 5.581930627822876 and perplexity is 265.5838547253295
At time: 1098.4604535102844 and batch: 100, loss is 5.602740640640259 and perplexity is 271.16856555837734
At time: 1099.647028684616 and batch: 150, loss is 5.515998468399048 and perplexity is 248.63811064646092
At time: 1100.8295631408691 and batch: 200, loss is 5.560827951431275 and perplexity is 260.0380460914863
At time: 1102.0101118087769 and batch: 250, loss is 5.591898212432861 and perplexity is 268.2443214549458
At time: 1103.188354730606 and batch: 300, loss is 5.585464754104614 and perplexity is 266.52412213954455
At time: 1104.3673889636993 and batch: 350, loss is 5.6068504047393795 and perplexity is 272.2852975746788
At time: 1105.5427849292755 and batch: 400, loss is 5.570165166854858 and perplexity is 262.477448234868
At time: 1106.7558681964874 and batch: 450, loss is 5.539659614562988 and perplexity is 254.59132552852031
At time: 1107.9294300079346 and batch: 500, loss is 5.534443645477295 and perplexity is 253.26684227977762
At time: 1109.1073496341705 and batch: 550, loss is 5.53607515335083 and perplexity is 253.68038638556314
At time: 1110.2915470600128 and batch: 600, loss is 5.572277021408081 and perplexity is 263.03234815676996
At time: 1111.4744861125946 and batch: 650, loss is 5.5657899379730225 and perplexity is 261.33155791667343
At time: 1112.6541829109192 and batch: 700, loss is 5.580927190780639 and perplexity is 265.3174917092863
At time: 1113.8352220058441 and batch: 750, loss is 5.550820598602295 and perplexity is 257.4487313178826
At time: 1115.0229942798615 and batch: 800, loss is 5.554117040634155 and perplexity is 258.2987964623126
At time: 1116.218887090683 and batch: 850, loss is 5.578029718399048 and perplexity is 264.5498542459285
At time: 1117.4129600524902 and batch: 900, loss is 5.5735328483581545 and perplexity is 263.36287876951195
At time: 1118.5898888111115 and batch: 950, loss is 5.561521453857422 and perplexity is 260.2184456538782
At time: 1119.767064332962 and batch: 1000, loss is 5.555763263702392 and perplexity is 258.724364093063
At time: 1120.9490361213684 and batch: 1050, loss is 5.545618896484375 and perplexity is 256.1130366615267
At time: 1122.1345267295837 and batch: 1100, loss is 5.517990303039551 and perplexity is 249.13385019990335
At time: 1123.310168504715 and batch: 1150, loss is 5.550602502822876 and perplexity is 257.3925889585939
At time: 1124.4862003326416 and batch: 1200, loss is 5.573365020751953 and perplexity is 263.3186829167515
At time: 1125.661374092102 and batch: 1250, loss is 5.588944721221924 and perplexity is 267.45323302057625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.298429837192062 and perplexity of 200.02249539381705
Finished 36 epochs...
Completing Train Step...
At time: 1128.6651906967163 and batch: 50, loss is 5.581010799407959 and perplexity is 265.339675467936
At time: 1129.873305797577 and batch: 100, loss is 5.601764574050903 and perplexity is 270.9040161113936
At time: 1131.0448863506317 and batch: 150, loss is 5.514988288879395 and perplexity is 248.38706833952736
At time: 1132.2123827934265 and batch: 200, loss is 5.5596036624908445 and perplexity is 259.71987919141515
At time: 1133.38046336174 and batch: 250, loss is 5.590823802947998 and perplexity is 267.95627198096673
At time: 1134.557222366333 and batch: 300, loss is 5.58449345588684 and perplexity is 266.265373416146
At time: 1135.7344455718994 and batch: 350, loss is 5.605782861709595 and perplexity is 271.9947764026572
At time: 1136.9489650726318 and batch: 400, loss is 5.569162225723266 and perplexity is 262.2143307736357
At time: 1138.1259169578552 and batch: 450, loss is 5.538790216445923 and perplexity is 254.3700804984267
At time: 1139.2953417301178 and batch: 500, loss is 5.533746147155762 and perplexity is 253.09025067572057
At time: 1140.468308210373 and batch: 550, loss is 5.535602006912232 and perplexity is 253.56038680512822
At time: 1141.6401896476746 and batch: 600, loss is 5.571901540756226 and perplexity is 262.93360313880703
At time: 1142.8122918605804 and batch: 650, loss is 5.5654310131072995 and perplexity is 261.2377763536133
At time: 1143.9733216762543 and batch: 700, loss is 5.58062047958374 and perplexity is 265.2361283420116
At time: 1145.1347680091858 and batch: 750, loss is 5.550769176483154 and perplexity is 257.4354930989198
At time: 1146.2992391586304 and batch: 800, loss is 5.55408579826355 and perplexity is 258.29072672164625
At time: 1147.4680306911469 and batch: 850, loss is 5.5782272911071775 and perplexity is 264.60212724075575
At time: 1148.6485996246338 and batch: 900, loss is 5.573795061111451 and perplexity is 263.43194492968837
At time: 1149.832020521164 and batch: 950, loss is 5.561823234558106 and perplexity is 260.2969864091845
At time: 1151.0118353366852 and batch: 1000, loss is 5.556098566055298 and perplexity is 258.81112952661164
At time: 1152.2038061618805 and batch: 1050, loss is 5.546084308624268 and perplexity is 256.2322625204028
At time: 1153.3806312084198 and batch: 1100, loss is 5.518486461639404 and perplexity is 249.25749077232481
At time: 1154.5593755245209 and batch: 1150, loss is 5.551248483657837 and perplexity is 257.5589133535239
At time: 1155.7406632900238 and batch: 1200, loss is 5.573979749679565 and perplexity is 263.480602291484
At time: 1156.9184038639069 and batch: 1250, loss is 5.5890767288208005 and perplexity is 267.48854121010254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.298310885464188 and perplexity of 199.9987037874307
Finished 37 epochs...
Completing Train Step...
At time: 1159.9075636863708 and batch: 50, loss is 5.5801879501342775 and perplexity is 265.12143071228166
At time: 1161.1192800998688 and batch: 100, loss is 5.600910902023315 and perplexity is 270.6728516140493
At time: 1162.2962756156921 and batch: 150, loss is 5.514092864990235 and perplexity is 248.16475617144462
At time: 1163.4709858894348 and batch: 200, loss is 5.558510227203369 and perplexity is 259.43604751471304
At time: 1164.6456501483917 and batch: 250, loss is 5.589887104034424 and perplexity is 267.7053951484883
At time: 1165.847748041153 and batch: 300, loss is 5.583645181655884 and perplexity is 266.0396031323761
At time: 1167.0268907546997 and batch: 350, loss is 5.6048378658294675 and perplexity is 271.73786386930226
At time: 1168.2080118656158 and batch: 400, loss is 5.568270969390869 and perplexity is 261.9807347033397
At time: 1169.3852789402008 and batch: 450, loss is 5.538018665313721 and perplexity is 254.17389666748036
At time: 1170.5623307228088 and batch: 500, loss is 5.533130674362183 and perplexity is 252.93452843842545
At time: 1171.7393763065338 and batch: 550, loss is 5.53520824432373 and perplexity is 253.46056386543773
At time: 1172.919429063797 and batch: 600, loss is 5.571587219238281 and perplexity is 262.8509704368482
At time: 1174.1036143302917 and batch: 650, loss is 5.565126934051514 and perplexity is 261.15835149357434
At time: 1175.2883214950562 and batch: 700, loss is 5.580356922149658 and perplexity is 265.1662325997757
At time: 1176.467414855957 and batch: 750, loss is 5.550720081329346 and perplexity is 257.4228545740381
At time: 1177.6441977024078 and batch: 800, loss is 5.554049978256225 and perplexity is 258.2814749116241
At time: 1178.8232426643372 and batch: 850, loss is 5.578387908935547 and perplexity is 264.64463047311204
At time: 1180.0035371780396 and batch: 900, loss is 5.574020891189575 and perplexity is 263.4914425043105
At time: 1181.183210849762 and batch: 950, loss is 5.562079429626465 and perplexity is 260.36368175655605
At time: 1182.3599984645844 and batch: 1000, loss is 5.55637936592102 and perplexity is 258.8838138614282
At time: 1183.5353422164917 and batch: 1050, loss is 5.5464621257781985 and perplexity is 256.3290897549167
At time: 1184.7116010189056 and batch: 1100, loss is 5.5188908195495605 and perplexity is 249.358300390594
At time: 1185.8910126686096 and batch: 1150, loss is 5.551785078048706 and perplexity is 257.6971551083059
At time: 1187.0712282657623 and batch: 1200, loss is 5.574482183456421 and perplexity is 263.6130171076742
At time: 1188.2506790161133 and batch: 1250, loss is 5.589168748855591 and perplexity is 267.51315664751
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.298194606808851 and perplexity of 199.97544955909652
Finished 38 epochs...
Completing Train Step...
At time: 1191.2326719760895 and batch: 50, loss is 5.5794415855407715 and perplexity is 264.9236272893364
At time: 1192.4071819782257 and batch: 100, loss is 5.600148859024048 and perplexity is 270.46666583357825
At time: 1193.5832960605621 and batch: 150, loss is 5.513285551071167 and perplexity is 247.96449015921203
At time: 1194.7640130519867 and batch: 200, loss is 5.557525997161865 and perplexity is 259.1808283806722
At time: 1195.9729821681976 and batch: 250, loss is 5.589056148529052 and perplexity is 267.48303627453197
At time: 1197.1604447364807 and batch: 300, loss is 5.582887477874756 and perplexity is 265.8381002685344
At time: 1198.338604927063 and batch: 350, loss is 5.603986368179322 and perplexity is 271.50657820033814
At time: 1199.5142967700958 and batch: 400, loss is 5.567464332580567 and perplexity is 261.76949660680395
At time: 1200.6897311210632 and batch: 450, loss is 5.537319049835205 and perplexity is 253.9961348648422
At time: 1201.8655760288239 and batch: 500, loss is 5.532576198577881 and perplexity is 252.79432124173235
At time: 1203.049869775772 and batch: 550, loss is 5.534870901107788 and perplexity is 253.37507508404985
At time: 1204.2285060882568 and batch: 600, loss is 5.571314153671264 and perplexity is 262.77920468638854
At time: 1205.4052023887634 and batch: 650, loss is 5.564857521057129 and perplexity is 261.0880015171143
At time: 1206.5843567848206 and batch: 700, loss is 5.5801237678527835 and perplexity is 265.10441516003993
At time: 1207.7669184207916 and batch: 750, loss is 5.550667095184326 and perplexity is 257.4092150906892
At time: 1208.9427206516266 and batch: 800, loss is 5.554003982543946 and perplexity is 258.2695953444236
At time: 1210.1222610473633 and batch: 850, loss is 5.5785156345367435 and perplexity is 264.6784345264183
At time: 1211.3114693164825 and batch: 900, loss is 5.5742124080657955 and perplexity is 263.54191039486176
At time: 1212.4923145771027 and batch: 950, loss is 5.56229681968689 and perplexity is 260.4202883857019
At time: 1213.6685264110565 and batch: 1000, loss is 5.556613464355468 and perplexity is 258.94442525119604
At time: 1214.845955133438 and batch: 1050, loss is 5.546770420074463 and perplexity is 256.4081267339537
At time: 1216.01486992836 and batch: 1100, loss is 5.519221525192261 and perplexity is 249.4407782247783
At time: 1217.1837689876556 and batch: 1150, loss is 5.552237958908081 and perplexity is 257.81388764834117
At time: 1218.3635034561157 and batch: 1200, loss is 5.574898748397827 and perplexity is 263.7228519237781
At time: 1219.5356726646423 and batch: 1250, loss is 5.589228744506836 and perplexity is 267.52920675502327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.298077437129334 and perplexity of 199.95201987241165
Finished 39 epochs...
Completing Train Step...
At time: 1222.423761844635 and batch: 50, loss is 5.5787536239624025 and perplexity is 264.7414326911868
At time: 1223.6071367263794 and batch: 100, loss is 5.5994555854797365 and perplexity is 270.27922343147435
At time: 1224.7826852798462 and batch: 150, loss is 5.512546272277832 and perplexity is 247.78124301384727
At time: 1225.9869093894958 and batch: 200, loss is 5.556635465621948 and perplexity is 258.9501224191717
At time: 1227.1670875549316 and batch: 250, loss is 5.588304481506348 and perplexity is 267.2820536425028
At time: 1228.3439729213715 and batch: 300, loss is 5.582199420928955 and perplexity is 265.65525142960655
At time: 1229.5199899673462 and batch: 350, loss is 5.603206424713135 and perplexity is 271.294900977508
At time: 1230.6965734958649 and batch: 400, loss is 5.566724119186401 and perplexity is 261.57580301537087
At time: 1231.8741192817688 and batch: 450, loss is 5.536675453186035 and perplexity is 253.83271639697105
At time: 1233.0533764362335 and batch: 500, loss is 5.532069063186645 and perplexity is 252.66615279685138
At time: 1234.235852241516 and batch: 550, loss is 5.534574537277222 and perplexity is 253.2999950022878
At time: 1235.4120287895203 and batch: 600, loss is 5.571070318222046 and perplexity is 262.7151376121979
At time: 1236.5916666984558 and batch: 650, loss is 5.564612455368042 and perplexity is 261.02402564557593
At time: 1237.7716913223267 and batch: 700, loss is 5.579910459518433 and perplexity is 265.0478722095688
At time: 1238.955825328827 and batch: 750, loss is 5.550606937408447 and perplexity is 257.3937303905858
At time: 1240.1381080150604 and batch: 800, loss is 5.5539445877075195 and perplexity is 258.2542559196001
At time: 1241.3125128746033 and batch: 850, loss is 5.57861367225647 and perplexity is 264.704384268606
At time: 1242.4881520271301 and batch: 900, loss is 5.574373197555542 and perplexity is 263.58428857105235
At time: 1243.662626028061 and batch: 950, loss is 5.562481536865234 and perplexity is 260.46839692965654
At time: 1244.8415205478668 and batch: 1000, loss is 5.556807832717896 and perplexity is 258.99476074674726
At time: 1246.020921945572 and batch: 1050, loss is 5.547022523880005 and perplexity is 256.4727763473397
At time: 1247.1972122192383 and batch: 1100, loss is 5.51949104309082 and perplexity is 249.50801603963066
At time: 1248.3735675811768 and batch: 1150, loss is 5.55262321472168 and perplexity is 257.9132310824734
At time: 1249.5485935211182 and batch: 1200, loss is 5.575246562957764 and perplexity is 263.81459452524655
At time: 1250.7224593162537 and batch: 1250, loss is 5.589261512756348 and perplexity is 267.5379733624544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.297954921304744 and perplexity of 199.92752408641002
Finished 40 epochs...
Completing Train Step...
At time: 1253.7075083255768 and batch: 50, loss is 5.578108892440796 and perplexity is 264.57080055632423
At time: 1254.8903563022614 and batch: 100, loss is 5.5988147163391115 and perplexity is 270.10606530959996
At time: 1256.0929882526398 and batch: 150, loss is 5.5118599700927735 and perplexity is 247.61124854581027
At time: 1257.2722685337067 and batch: 200, loss is 5.555818939208985 and perplexity is 258.73876910410115
At time: 1258.4523437023163 and batch: 250, loss is 5.587612400054931 and perplexity is 267.0971366870587
At time: 1259.629961490631 and batch: 300, loss is 5.581565561294556 and perplexity is 265.48691664500785
At time: 1260.8074867725372 and batch: 350, loss is 5.602483291625976 and perplexity is 271.0987895741019
At time: 1261.9829108715057 and batch: 400, loss is 5.566036567687989 and perplexity is 261.3960179928725
At time: 1263.1642756462097 and batch: 450, loss is 5.536075496673584 and perplexity is 253.68047347982701
At time: 1264.3444993495941 and batch: 500, loss is 5.531596584320068 and perplexity is 252.5468015770404
At time: 1265.5285649299622 and batch: 550, loss is 5.534305610656738 and perplexity is 253.23188504936354
At time: 1266.7060418128967 and batch: 600, loss is 5.570846023559571 and perplexity is 262.6562186169352
At time: 1267.8835442066193 and batch: 650, loss is 5.564382152557373 and perplexity is 260.96391800056307
At time: 1269.060567855835 and batch: 700, loss is 5.579708395004272 and perplexity is 264.9943208506383
At time: 1270.2414801120758 and batch: 750, loss is 5.550535879135132 and perplexity is 257.37544108635296
At time: 1271.421983718872 and batch: 800, loss is 5.5538702392578125 and perplexity is 258.23505582979953
At time: 1272.5993342399597 and batch: 850, loss is 5.578683376312256 and perplexity is 264.7228358808425
At time: 1273.777973651886 and batch: 900, loss is 5.574507379531861 and perplexity is 263.619659204817
At time: 1274.9549775123596 and batch: 950, loss is 5.5626383399963375 and perplexity is 260.50924239211315
At time: 1276.1423184871674 and batch: 1000, loss is 5.556967573165894 and perplexity is 259.03613599042006
At time: 1277.3221039772034 and batch: 1050, loss is 5.5472265720367435 and perplexity is 256.5251144841755
At time: 1278.502099275589 and batch: 1100, loss is 5.519707670211792 and perplexity is 249.56207209759748
At time: 1279.680599451065 and batch: 1150, loss is 5.5529529571533205 and perplexity is 257.998290041446
At time: 1280.8572075366974 and batch: 1200, loss is 5.575535879135132 and perplexity is 263.8909313974821
At time: 1282.0338952541351 and batch: 1250, loss is 5.589270401000976 and perplexity is 267.5403513159769
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.297828395871351 and perplexity of 199.9022297699986
Finished 41 epochs...
Completing Train Step...
At time: 1284.990876197815 and batch: 50, loss is 5.577492094039917 and perplexity is 264.40766402596915
At time: 1286.1948146820068 and batch: 100, loss is 5.598213300704956 and perplexity is 269.9436681380365
At time: 1287.3684322834015 and batch: 150, loss is 5.511212797164917 and perplexity is 247.45105309179036
At time: 1288.546434879303 and batch: 200, loss is 5.555058479309082 and perplexity is 258.54208344090574
At time: 1289.7237584590912 and batch: 250, loss is 5.5869649887084964 and perplexity is 266.9242709338302
At time: 1290.9007122516632 and batch: 300, loss is 5.580973596572876 and perplexity is 265.3298042633679
At time: 1292.0754680633545 and batch: 350, loss is 5.601805400848389 and perplexity is 270.9150764805757
At time: 1293.2531065940857 and batch: 400, loss is 5.565390338897705 and perplexity is 261.2271509296358
At time: 1294.4330506324768 and batch: 450, loss is 5.535508756637573 and perplexity is 253.53674333181348
At time: 1295.596833229065 and batch: 500, loss is 5.531148300170899 and perplexity is 252.43361422091428
At time: 1296.7676358222961 and batch: 550, loss is 5.534050760269165 and perplexity is 253.16735702815754
At time: 1297.9389514923096 and batch: 600, loss is 5.570634679794312 and perplexity is 262.60071372823666
At time: 1299.113492488861 and batch: 650, loss is 5.564155988693237 and perplexity is 260.90490406612923
At time: 1300.2787899971008 and batch: 700, loss is 5.57950909614563 and perplexity is 264.94151304738836
At time: 1301.4426288604736 and batch: 750, loss is 5.550449457168579 and perplexity is 257.3531991557014
At time: 1302.6066966056824 and batch: 800, loss is 5.553779144287109 and perplexity is 258.2115329863767
At time: 1303.7709250450134 and batch: 850, loss is 5.57872504234314 and perplexity is 264.73386606048837
At time: 1304.9416799545288 and batch: 900, loss is 5.574615516662598 and perplexity is 263.64816781976145
At time: 1306.1188371181488 and batch: 950, loss is 5.562772283554077 and perplexity is 260.54413826384956
At time: 1307.2966232299805 and batch: 1000, loss is 5.557096796035767 and perplexity is 259.0696115461707
At time: 1308.4719409942627 and batch: 1050, loss is 5.547387638092041 and perplexity is 256.5664353000513
At time: 1309.6469593048096 and batch: 1100, loss is 5.5198759174346925 and perplexity is 249.60406375556
At time: 1310.82186293602 and batch: 1150, loss is 5.553234968185425 and perplexity is 258.07105866579695
At time: 1312.0003633499146 and batch: 1200, loss is 5.575771741867065 and perplexity is 263.95318077435934
At time: 1313.1775362491608 and batch: 1250, loss is 5.5892558193206785 and perplexity is 267.53645015655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.297692069171989 and perplexity of 199.87497961632286
Finished 42 epochs...
Completing Train Step...
At time: 1316.12602353096 and batch: 50, loss is 5.576886310577392 and perplexity is 264.247538741259
At time: 1317.330759048462 and batch: 100, loss is 5.597638931274414 and perplexity is 269.78866526580657
At time: 1318.5116801261902 and batch: 150, loss is 5.510590076446533 and perplexity is 247.2970081627067
At time: 1319.6917634010315 and batch: 200, loss is 5.554338226318359 and perplexity is 258.3559347771929
At time: 1320.868480205536 and batch: 250, loss is 5.586351585388184 and perplexity is 266.76058890644913
At time: 1322.044425725937 and batch: 300, loss is 5.580412139892578 and perplexity is 265.1808748848918
At time: 1323.2212154865265 and batch: 350, loss is 5.601163473129272 and perplexity is 270.74122438966424
At time: 1324.3997390270233 and batch: 400, loss is 5.564774684906006 and perplexity is 261.06637488770986
At time: 1325.580917596817 and batch: 450, loss is 5.534963636398316 and perplexity is 253.3985729847757
At time: 1326.76136136055 and batch: 500, loss is 5.530715007781982 and perplexity is 252.32426034997644
At time: 1327.941816329956 and batch: 550, loss is 5.533796920776367 and perplexity is 253.103101310321
At time: 1329.1192064285278 and batch: 600, loss is 5.5704297351837155 and perplexity is 262.5469006417587
At time: 1330.296330690384 and batch: 650, loss is 5.563924751281738 and perplexity is 260.84458006631723
At time: 1331.4800305366516 and batch: 700, loss is 5.579302673339844 and perplexity is 264.88682872113606
At time: 1332.6567838191986 and batch: 750, loss is 5.5503450202941895 and perplexity is 257.3263233954021
At time: 1333.8338344097137 and batch: 800, loss is 5.553671607971191 and perplexity is 258.18376736232506
At time: 1335.0171535015106 and batch: 850, loss is 5.5787394332885745 and perplexity is 264.7376758585227
At time: 1336.2027280330658 and batch: 900, loss is 5.574697122573853 and perplexity is 263.6696839466568
At time: 1337.3831808567047 and batch: 950, loss is 5.562886590957642 and perplexity is 260.573922090032
At time: 1338.5617935657501 and batch: 1000, loss is 5.557197875976563 and perplexity is 259.09579961068954
At time: 1339.7420918941498 and batch: 1050, loss is 5.547510461807251 and perplexity is 256.5979496781499
At time: 1340.9221403598785 and batch: 1100, loss is 5.519999313354492 and perplexity is 249.63486577897572
At time: 1342.1009068489075 and batch: 1150, loss is 5.553474779129028 and perplexity is 258.13295435122706
At time: 1343.2776861190796 and batch: 1200, loss is 5.575958042144776 and perplexity is 264.0023599061409
At time: 1344.4789848327637 and batch: 1250, loss is 5.5892196369171145 and perplexity is 267.52677021986517
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.297544159158303 and perplexity of 199.84541829161424
Finished 43 epochs...
Completing Train Step...
At time: 1347.4770674705505 and batch: 50, loss is 5.576281785964966 and perplexity is 264.087842875221
At time: 1348.6589257717133 and batch: 100, loss is 5.597082624435425 and perplexity is 269.63862172523085
At time: 1349.8379373550415 and batch: 150, loss is 5.509983415603638 and perplexity is 247.14702824935938
At time: 1351.020168542862 and batch: 200, loss is 5.553652963638306 and perplexity is 258.1789537430942
At time: 1352.213891506195 and batch: 250, loss is 5.585771656036377 and perplexity is 266.6059314605676
At time: 1353.402230501175 and batch: 300, loss is 5.579872369766235 and perplexity is 265.037776794063
At time: 1354.5838491916656 and batch: 350, loss is 5.6005463123321535 and perplexity is 270.5741850701687
At time: 1355.759893655777 and batch: 400, loss is 5.564178972244263 and perplexity is 260.910900656216
At time: 1356.9357631206512 and batch: 450, loss is 5.534433221817016 and perplexity is 253.2642023260128
At time: 1358.1115882396698 and batch: 500, loss is 5.530294046401978 and perplexity is 252.21806393499395
At time: 1359.2920422554016 and batch: 550, loss is 5.533539848327637 and perplexity is 253.038043838886
At time: 1360.4748384952545 and batch: 600, loss is 5.57022644996643 and perplexity is 262.49353416250614
At time: 1361.6530935764313 and batch: 650, loss is 5.563690233230591 and perplexity is 260.78341447624547
At time: 1362.8288083076477 and batch: 700, loss is 5.579081134796143 and perplexity is 264.8281525786092
At time: 1364.005210161209 and batch: 750, loss is 5.55022632598877 and perplexity is 257.29578203875883
At time: 1365.1817727088928 and batch: 800, loss is 5.553556423187256 and perplexity is 258.1540302335312
At time: 1366.3608486652374 and batch: 850, loss is 5.5787287616729735 and perplexity is 264.73485069488544
At time: 1367.5416615009308 and batch: 900, loss is 5.574751625061035 and perplexity is 263.68405499185167
At time: 1368.7207987308502 and batch: 950, loss is 5.5629834461212155 and perplexity is 260.5991612421305
At time: 1369.9065988063812 and batch: 1000, loss is 5.55727481842041 and perplexity is 259.11573584166354
At time: 1371.0894660949707 and batch: 1050, loss is 5.547602005004883 and perplexity is 256.6214405501676
At time: 1372.2666923999786 and batch: 1100, loss is 5.520082864761353 and perplexity is 249.65572399456752
At time: 1373.4505655765533 and batch: 1150, loss is 5.553676824569703 and perplexity is 258.1851142068945
At time: 1374.6557161808014 and batch: 1200, loss is 5.576106777191162 and perplexity is 264.04162922967765
At time: 1375.8316476345062 and batch: 1250, loss is 5.589165153503418 and perplexity is 267.51219484523017
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.297367736370894 and perplexity of 199.81016411577986
Finished 44 epochs...
Completing Train Step...
At time: 1378.7831227779388 and batch: 50, loss is 5.5756885433197025 and perplexity is 263.9312211666636
At time: 1379.98020529747 and batch: 100, loss is 5.596544227600098 and perplexity is 269.4934882178284
At time: 1381.1503596305847 and batch: 150, loss is 5.509399652481079 and perplexity is 247.0027950315302
At time: 1382.3218100070953 and batch: 200, loss is 5.553009443283081 and perplexity is 258.0128637779383
At time: 1383.4949488639832 and batch: 250, loss is 5.58523323059082 and perplexity is 266.46242268098916
At time: 1384.659559726715 and batch: 300, loss is 5.579357833862304 and perplexity is 264.90144041999207
At time: 1385.8241980075836 and batch: 350, loss is 5.599940004348755 and perplexity is 270.41018350441254
At time: 1386.9881353378296 and batch: 400, loss is 5.563597955703735 and perplexity is 260.7593511379825
At time: 1388.151873588562 and batch: 450, loss is 5.5339172744750975 and perplexity is 253.13356503789828
At time: 1389.3296341896057 and batch: 500, loss is 5.529890394210815 and perplexity is 252.116276105658
At time: 1390.5084619522095 and batch: 550, loss is 5.533290748596191 and perplexity is 252.9750199800593
At time: 1391.6865229606628 and batch: 600, loss is 5.570025300979614 and perplexity is 262.4407391640718
At time: 1392.8631663322449 and batch: 650, loss is 5.563464002609253 and perplexity is 260.72442395533665
At time: 1394.0416684150696 and batch: 700, loss is 5.578846321105957 and perplexity is 264.76597460324285
At time: 1395.2189419269562 and batch: 750, loss is 5.550099515914917 and perplexity is 257.26315641030953
At time: 1396.397912979126 and batch: 800, loss is 5.55344895362854 and perplexity is 258.12628802456936
At time: 1397.576755285263 and batch: 850, loss is 5.578697395324707 and perplexity is 264.72654705958837
At time: 1398.7554833889008 and batch: 900, loss is 5.574783029556275 and perplexity is 263.692335986531
At time: 1399.9330670833588 and batch: 950, loss is 5.563066148757935 and perplexity is 260.6207143711301
At time: 1401.112633228302 and batch: 1000, loss is 5.557332096099853 and perplexity is 259.1305778147725
At time: 1402.2903311252594 and batch: 1050, loss is 5.54766505241394 and perplexity is 256.63762037714554
At time: 1403.469769001007 and batch: 1100, loss is 5.520135335922241 and perplexity is 249.66882406391284
At time: 1404.674189567566 and batch: 1150, loss is 5.553845386505127 and perplexity is 258.2286380575723
At time: 1405.8531155586243 and batch: 1200, loss is 5.576234092712403 and perplexity is 264.0752479673803
At time: 1407.0339353084564 and batch: 1250, loss is 5.5890963172912596 and perplexity is 267.4937809528093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.297175720660356 and perplexity of 199.7718011084122
Finished 45 epochs...
Completing Train Step...
At time: 1410.0132040977478 and batch: 50, loss is 5.5751183891296385 and perplexity is 263.7807825657014
At time: 1411.1949627399445 and batch: 100, loss is 5.596028575897217 and perplexity is 269.3545592642677
At time: 1412.372720003128 and batch: 150, loss is 5.508854455947876 and perplexity is 246.86816666678124
At time: 1413.5481827259064 and batch: 200, loss is 5.552408647537232 and perplexity is 257.8578973032669
At time: 1414.7350661754608 and batch: 250, loss is 5.584733409881592 and perplexity is 266.32927252227745
At time: 1415.9189414978027 and batch: 300, loss is 5.578878889083862 and perplexity is 264.77459763607027
At time: 1417.1021180152893 and batch: 350, loss is 5.599344215393066 and perplexity is 270.24912408705006
At time: 1418.2820510864258 and batch: 400, loss is 5.563033027648926 and perplexity is 260.61208246698925
At time: 1419.4635076522827 and batch: 450, loss is 5.533423080444336 and perplexity is 253.00849884709893
At time: 1420.6431682109833 and batch: 500, loss is 5.529503946304321 and perplexity is 252.01886512191157
At time: 1421.8188705444336 and batch: 550, loss is 5.53305799484253 and perplexity is 252.91614594641825
At time: 1423.0006794929504 and batch: 600, loss is 5.569830827713012 and perplexity is 262.3897064186504
At time: 1424.1901383399963 and batch: 650, loss is 5.5632506942749025 and perplexity is 260.66881519385527
At time: 1425.367425918579 and batch: 700, loss is 5.578604955673217 and perplexity is 264.7020769608592
At time: 1426.5415713787079 and batch: 750, loss is 5.549970035552978 and perplexity is 257.22984804014044
At time: 1427.7217864990234 and batch: 800, loss is 5.55336893081665 and perplexity is 258.1056328596322
At time: 1428.896357536316 and batch: 850, loss is 5.578656339645386 and perplexity is 264.71567875446885
At time: 1430.0729112625122 and batch: 900, loss is 5.5747971534729 and perplexity is 263.6960603814008
At time: 1431.2485494613647 and batch: 950, loss is 5.563135509490967 and perplexity is 260.6387918418482
At time: 1432.429089307785 and batch: 1000, loss is 5.557372436523438 and perplexity is 259.1410314628962
At time: 1433.6027557849884 and batch: 1050, loss is 5.547705802917481 and perplexity is 256.6480787024929
At time: 1434.808867931366 and batch: 1100, loss is 5.52017201423645 and perplexity is 249.67798166343167
At time: 1435.9897646903992 and batch: 1150, loss is 5.553982286453247 and perplexity is 258.2639919646443
At time: 1437.1663038730621 and batch: 1200, loss is 5.576347122192383 and perplexity is 264.10509794226544
At time: 1438.3467783927917 and batch: 1250, loss is 5.5890183258056645 and perplexity is 267.4729195289626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.2969894966069795 and perplexity of 199.73460225762744
Finished 46 epochs...
Completing Train Step...
At time: 1441.343088388443 and batch: 50, loss is 5.574570808410645 and perplexity is 263.6363808345373
At time: 1442.5441660881042 and batch: 100, loss is 5.595539779663086 and perplexity is 269.22293194214376
At time: 1443.7202730178833 and batch: 150, loss is 5.508354911804199 and perplexity is 246.74487591701237
At time: 1444.8945260047913 and batch: 200, loss is 5.551837434768677 and perplexity is 257.71064763930246
At time: 1446.0685694217682 and batch: 250, loss is 5.58425308227539 and perplexity is 266.2013779384541
At time: 1447.2454149723053 and batch: 300, loss is 5.578432645797729 and perplexity is 264.65647010830503
At time: 1448.421192407608 and batch: 350, loss is 5.598778295516968 and perplexity is 270.09622800377775
At time: 1449.6033794879913 and batch: 400, loss is 5.56248984336853 and perplexity is 260.47056052024004
At time: 1450.7765238285065 and batch: 450, loss is 5.532951717376709 and perplexity is 252.88926808764256
At time: 1451.9537017345428 and batch: 500, loss is 5.529125804901123 and perplexity is 251.92358437055586
At time: 1453.1281611919403 and batch: 550, loss is 5.532841796875 and perplexity is 252.86147190013105
At time: 1454.3079750537872 and batch: 600, loss is 5.569646282196045 and perplexity is 262.34128804246694
At time: 1455.4859762191772 and batch: 650, loss is 5.563052396774292 and perplexity is 260.617130343973
At time: 1456.6666264533997 and batch: 700, loss is 5.578363904953003 and perplexity is 264.6382780242642
At time: 1457.8430552482605 and batch: 750, loss is 5.549844388961792 and perplexity is 257.19753001695
At time: 1459.0291910171509 and batch: 800, loss is 5.5533195495605465 and perplexity is 258.0928875939653
At time: 1460.2069420814514 and batch: 850, loss is 5.578611669540405 and perplexity is 264.703854141414
At time: 1461.374665260315 and batch: 900, loss is 5.574791212081909 and perplexity is 263.69449366465733
At time: 1462.5494346618652 and batch: 950, loss is 5.563185205459595 and perplexity is 260.6517448609246
At time: 1463.7521467208862 and batch: 1000, loss is 5.557391901016235 and perplexity is 259.1460755607268
At time: 1464.9253387451172 and batch: 1050, loss is 5.547727584838867 and perplexity is 256.65366905165126
At time: 1466.0923755168915 and batch: 1100, loss is 5.520192289352417 and perplexity is 249.68304396478362
At time: 1467.2598123550415 and batch: 1150, loss is 5.5540814876556395 and perplexity is 258.2896133339961
At time: 1468.425455570221 and batch: 1200, loss is 5.576440296173096 and perplexity is 264.1297068120027
At time: 1469.592269897461 and batch: 1250, loss is 5.588926248550415 and perplexity is 267.4482924904912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.2968104007470345 and perplexity of 199.69883382036048
Finished 47 epochs...
Completing Train Step...
At time: 1472.6245155334473 and batch: 50, loss is 5.574034118652344 and perplexity is 263.49492785060716
At time: 1473.8083918094635 and batch: 100, loss is 5.595062246322632 and perplexity is 269.0943997077828
At time: 1474.9898686408997 and batch: 150, loss is 5.507883605957031 and perplexity is 246.62861101455024
At time: 1476.173493385315 and batch: 200, loss is 5.551278047561645 and perplexity is 257.5665279130209
At time: 1477.35897397995 and batch: 250, loss is 5.583775262832642 and perplexity is 266.07421212795646
At time: 1478.5376453399658 and batch: 300, loss is 5.57800672531128 and perplexity is 264.54377149784165
At time: 1479.7186546325684 and batch: 350, loss is 5.598239955902099 and perplexity is 269.9508636356265
At time: 1480.8985760211945 and batch: 400, loss is 5.5619602870941165 and perplexity is 260.3326632160304
At time: 1482.0873908996582 and batch: 450, loss is 5.532488441467285 and perplexity is 252.77213771592034
At time: 1483.2812571525574 and batch: 500, loss is 5.528744974136353 and perplexity is 251.82766238543707
At time: 1484.4645042419434 and batch: 550, loss is 5.532633295059204 and perplexity is 252.8087553200374
At time: 1485.6508939266205 and batch: 600, loss is 5.569466428756714 and perplexity is 262.2941093022905
At time: 1486.844875574112 and batch: 650, loss is 5.5628687763214115 and perplexity is 260.5692801017467
At time: 1488.0363643169403 and batch: 700, loss is 5.578126277923584 and perplexity is 264.57540028740755
At time: 1489.2171521186829 and batch: 750, loss is 5.549710845947265 and perplexity is 257.16318537675716
At time: 1490.4041357040405 and batch: 800, loss is 5.553277463912964 and perplexity is 258.08202581621845
At time: 1491.5923972129822 and batch: 850, loss is 5.5785567092895505 and perplexity is 264.68930635096706
At time: 1492.7847952842712 and batch: 900, loss is 5.57475697517395 and perplexity is 263.68546573509366
At time: 1493.994524717331 and batch: 950, loss is 5.563211650848388 and perplexity is 260.65863798880247
At time: 1495.174067735672 and batch: 1000, loss is 5.557388362884521 and perplexity is 259.14515866940036
At time: 1496.3579478263855 and batch: 1050, loss is 5.547723369598389 and perplexity is 256.65258719699665
At time: 1497.5482800006866 and batch: 1100, loss is 5.520190963745117 and perplexity is 249.68271298333713
At time: 1498.7355234622955 and batch: 1150, loss is 5.554140310287476 and perplexity is 258.3048070556914
At time: 1499.923351764679 and batch: 1200, loss is 5.576507472991944 and perplexity is 264.1474508014553
At time: 1501.1106090545654 and batch: 1250, loss is 5.588811578750611 and perplexity is 267.4176260066268
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.296618830548586 and perplexity of 199.6605811392894
Finished 48 epochs...
Completing Train Step...
At time: 1504.1543900966644 and batch: 50, loss is 5.573495740890503 and perplexity is 263.3531062213258
At time: 1505.3306965827942 and batch: 100, loss is 5.59457573890686 and perplexity is 268.9635151275233
At time: 1506.5021450519562 and batch: 150, loss is 5.50742841720581 and perplexity is 246.51637399154038
At time: 1507.6759078502655 and batch: 200, loss is 5.550718851089478 and perplexity is 257.42253788237423
At time: 1508.8487811088562 and batch: 250, loss is 5.583297443389893 and perplexity is 265.9471070652409
At time: 1510.022376537323 and batch: 300, loss is 5.577590618133545 and perplexity is 264.43371583476005
At time: 1511.1973934173584 and batch: 350, loss is 5.597719564437866 and perplexity is 269.8104200564174
At time: 1512.3723092079163 and batch: 400, loss is 5.5614387321472165 and perplexity is 260.19692082932414
At time: 1513.5461430549622 and batch: 450, loss is 5.532023200988769 and perplexity is 252.65456523747278
At time: 1514.7244591712952 and batch: 500, loss is 5.528357648849488 and perplexity is 251.730142051128
At time: 1515.9007949829102 and batch: 550, loss is 5.532426252365112 and perplexity is 252.7564185324075
At time: 1517.0842261314392 and batch: 600, loss is 5.5692893409729 and perplexity is 262.2476643323065
At time: 1518.2671053409576 and batch: 650, loss is 5.562699298858643 and perplexity is 260.5251232231832
At time: 1519.4450829029083 and batch: 700, loss is 5.577897844314575 and perplexity is 264.51496927636464
At time: 1520.618034362793 and batch: 750, loss is 5.549564189910889 and perplexity is 257.12547360868473
At time: 1521.7929737567902 and batch: 800, loss is 5.553217468261718 and perplexity is 258.06654248147595
At time: 1522.9782412052155 and batch: 850, loss is 5.578487319946289 and perplexity is 264.67094037104005
At time: 1524.1852827072144 and batch: 900, loss is 5.57470290184021 and perplexity is 263.67120776839386
At time: 1525.3640666007996 and batch: 950, loss is 5.563219079971313 and perplexity is 260.6605744610587
At time: 1526.5435438156128 and batch: 1000, loss is 5.55736834526062 and perplexity is 259.1399712509985
At time: 1527.7201714515686 and batch: 1050, loss is 5.547701616287231 and perplexity is 256.6470042141323
At time: 1528.8960807323456 and batch: 1100, loss is 5.5201764488220215 and perplexity is 249.67908888426174
At time: 1530.0723600387573 and batch: 1150, loss is 5.554167032241821 and perplexity is 258.31170955717647
At time: 1531.2533683776855 and batch: 1200, loss is 5.576555404663086 and perplexity is 264.1601121336372
At time: 1532.4273474216461 and batch: 1250, loss is 5.588685064315796 and perplexity is 267.383795956853
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.296428151374315 and perplexity of 199.62251365399706
Finished 49 epochs...
Completing Train Step...
At time: 1535.4201350212097 and batch: 50, loss is 5.572972192764282 and perplexity is 263.215264282622
At time: 1536.6330370903015 and batch: 100, loss is 5.59409161567688 and perplexity is 268.83333515594967
At time: 1537.8209838867188 and batch: 150, loss is 5.507000217437744 and perplexity is 246.41083833415718
At time: 1539.0076406002045 and batch: 200, loss is 5.5501772403717045 and perplexity is 257.2831528264881
At time: 1540.1927599906921 and batch: 250, loss is 5.582842235565185 and perplexity is 265.8260734109698
At time: 1541.379920244217 and batch: 300, loss is 5.57718581199646 and perplexity is 264.326693106928
At time: 1542.5557749271393 and batch: 350, loss is 5.597226543426514 and perplexity is 269.67743063623084
At time: 1543.7333211898804 and batch: 400, loss is 5.560937662124633 and perplexity is 260.0665766108455
At time: 1544.9114379882812 and batch: 450, loss is 5.531577062606812 and perplexity is 252.5418714789183
At time: 1546.0881609916687 and batch: 500, loss is 5.527989873886108 and perplexity is 251.63757902957792
At time: 1547.2644093036652 and batch: 550, loss is 5.532226934432983 and perplexity is 252.70604466610752
At time: 1548.4450211524963 and batch: 600, loss is 5.569116926193237 and perplexity is 262.2024528567188
At time: 1549.6194186210632 and batch: 650, loss is 5.562535409927368 and perplexity is 260.4824295377747
At time: 1550.797503232956 and batch: 700, loss is 5.577684936523437 and perplexity is 264.45865797330345
At time: 1551.9742550849915 and batch: 750, loss is 5.549428081512451 and perplexity is 257.0904790538546
At time: 1553.148636341095 and batch: 800, loss is 5.553135242462158 and perplexity is 258.0453236260613
At time: 1554.3464546203613 and batch: 850, loss is 5.578412294387817 and perplexity is 264.6510840308033
At time: 1555.5120921134949 and batch: 900, loss is 5.574653673171997 and perplexity is 263.6582279054826
At time: 1556.6773841381073 and batch: 950, loss is 5.56321195602417 and perplexity is 260.6587175355182
At time: 1557.8432652950287 and batch: 1000, loss is 5.557340545654297 and perplexity is 259.13276736194814
At time: 1559.0105888843536 and batch: 1050, loss is 5.547675218582153 and perplexity is 256.64022941162597
At time: 1560.18132686615 and batch: 1100, loss is 5.52015417098999 and perplexity is 249.67352663741542
At time: 1561.358092546463 and batch: 1150, loss is 5.55417917251587 and perplexity is 258.3148455511563
At time: 1562.5406353473663 and batch: 1200, loss is 5.576597356796265 and perplexity is 264.1711944463037
At time: 1563.717212677002 and batch: 1250, loss is 5.588564739227295 and perplexity is 267.3516249134715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.296282914433166 and perplexity of 199.5935231960231
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f58a487a898>
SETTINGS FOR THIS RUN
{'anneal': 4.329807450795583, 'data': 'wikitext', 'lr': 13.795690389813887, 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.35428703004268747, 'num_layers': 1, 'wordvec_source': 'glove', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.6879198551177979 and batch: 50, loss is 7.401470499038696 and perplexity is 1638.3919131946004
At time: 2.861760139465332 and batch: 100, loss is 6.515293378829956 and perplexity is 675.392078197789
At time: 4.032784938812256 and batch: 150, loss is 6.118760766983033 and perplexity is 454.3013602751262
At time: 5.2091217041015625 and batch: 200, loss is 6.023901720046997 and perplexity is 413.18759707973663
At time: 6.389626979827881 and batch: 250, loss is 6.016316146850586 and perplexity is 410.06518991445733
At time: 7.561792612075806 and batch: 300, loss is 6.011597967147827 and perplexity is 408.1349857643401
At time: 8.740822553634644 and batch: 350, loss is 6.029203453063965 and perplexity is 415.38402469695336
At time: 9.912562608718872 and batch: 400, loss is 5.985618648529052 and perplexity is 397.6684621751869
At time: 11.083673000335693 and batch: 450, loss is 5.97733925819397 and perplexity is 394.3896019646594
At time: 12.282092809677124 and batch: 500, loss is 5.9771991634368895 and perplexity is 394.3343539192482
At time: 13.457341432571411 and batch: 550, loss is 5.978545417785645 and perplexity is 394.8655857644327
At time: 14.627420425415039 and batch: 600, loss is 6.003628616333008 and perplexity is 404.89534096190704
At time: 15.796770095825195 and batch: 650, loss is 5.987285232543945 and perplexity is 398.3317626469301
At time: 16.96525263786316 and batch: 700, loss is 6.031930322647095 and perplexity is 416.51826852387757
At time: 18.142770528793335 and batch: 750, loss is 6.000967025756836 and perplexity is 403.81910821887203
At time: 19.31588649749756 and batch: 800, loss is 6.006456308364868 and perplexity is 406.0418805577596
At time: 20.486581802368164 and batch: 850, loss is 6.045732460021973 and perplexity is 422.3069671975327
At time: 21.667884826660156 and batch: 900, loss is 6.025222816467285 and perplexity is 413.73381846125903
At time: 22.843586921691895 and batch: 950, loss is 6.003050851821899 and perplexity is 404.66147436960034
At time: 24.02006483078003 and batch: 1000, loss is 6.008873367309571 and perplexity is 407.0244947569218
At time: 25.193361520767212 and batch: 1050, loss is 6.012623767852784 and perplexity is 408.55386572740434
At time: 26.364527463912964 and batch: 1100, loss is 5.996004991531372 and perplexity is 401.82030714383467
At time: 27.53534722328186 and batch: 1150, loss is 6.034198722839355 and perplexity is 417.46417108144277
At time: 28.705054998397827 and batch: 1200, loss is 6.02837984085083 and perplexity is 415.0420501875942
At time: 29.877320289611816 and batch: 1250, loss is 6.0124994659423825 and perplexity is 408.5030848575376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.500470906278513 and perplexity of 244.8071863662353
Finished 1 epochs...
Completing Train Step...
At time: 32.9001739025116 and batch: 50, loss is 5.772229766845703 and perplexity is 321.25325448338776
At time: 34.06695890426636 and batch: 100, loss is 5.754497919082642 and perplexity is 315.60704746169216
At time: 35.22600531578064 and batch: 150, loss is 5.649550266265869 and perplexity is 284.1636391075622
At time: 36.390135288238525 and batch: 200, loss is 5.684115047454834 and perplexity is 294.15741448081104
At time: 37.56199765205383 and batch: 250, loss is 5.685064601898193 and perplexity is 294.4368656168149
At time: 38.730098247528076 and batch: 300, loss is 5.6665834617614745 and perplexity is 289.04531113627655
At time: 39.89551877975464 and batch: 350, loss is 5.699581069946289 and perplexity is 298.7422226530303
At time: 41.06321430206299 and batch: 400, loss is 5.664500865936279 and perplexity is 288.4439729675357
At time: 42.229071617126465 and batch: 450, loss is 5.632323408126831 and perplexity is 279.310316185087
At time: 43.39845561981201 and batch: 500, loss is 5.624937086105347 and perplexity is 277.25484079256756
At time: 44.560709953308105 and batch: 550, loss is 5.610709362030029 and perplexity is 273.3380648945088
At time: 45.7298150062561 and batch: 600, loss is 5.62794900894165 and perplexity is 278.0911698260413
At time: 46.90687966346741 and batch: 650, loss is 5.602495660781861 and perplexity is 271.1021428580288
At time: 48.07949733734131 and batch: 700, loss is 5.630026960372925 and perplexity is 278.6696305689268
At time: 49.251179933547974 and batch: 750, loss is 5.622683591842652 and perplexity is 276.6307520538389
At time: 50.420777797698975 and batch: 800, loss is 5.635688610076905 and perplexity is 280.25183511793733
At time: 51.61559224128723 and batch: 850, loss is 5.650921869277954 and perplexity is 284.55366623092925
At time: 52.79099106788635 and batch: 900, loss is 5.6215372657775875 and perplexity is 276.3138246981108
At time: 53.96703863143921 and batch: 950, loss is 5.612065839767456 and perplexity is 273.70909348344793
At time: 55.13897919654846 and batch: 1000, loss is 5.621223030090332 and perplexity is 276.2270106742551
At time: 56.31155753135681 and batch: 1050, loss is 5.612332153320312 and perplexity is 273.7819956315668
At time: 57.48377323150635 and batch: 1100, loss is 5.60103175163269 and perplexity is 270.705564299053
At time: 58.661025285720825 and batch: 1150, loss is 5.630330505371094 and perplexity is 278.7542321809761
At time: 59.84082055091858 and batch: 1200, loss is 5.620538396835327 and perplexity is 276.0379611989801
At time: 61.02291011810303 and batch: 1250, loss is 5.610602731704712 and perplexity is 273.30892032160284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.37123141323563 and perplexity of 215.12761565708357
Finished 2 epochs...
Completing Train Step...
At time: 64.01634621620178 and batch: 50, loss is 5.583021230697632 and perplexity is 265.87365924287684
At time: 65.1910753250122 and batch: 100, loss is 5.585090599060059 and perplexity is 266.42441944804585
At time: 66.36846590042114 and batch: 150, loss is 5.4901690101623535 and perplexity is 242.2981542479815
At time: 67.54219627380371 and batch: 200, loss is 5.541213569641113 and perplexity is 254.98725656151774
At time: 68.71474432945251 and batch: 250, loss is 5.542625055313111 and perplexity is 255.34742154475688
At time: 69.890620470047 and batch: 300, loss is 5.54405499458313 and perplexity is 255.71281403259488
At time: 71.05914163589478 and batch: 350, loss is 5.586374187469483 and perplexity is 266.76661831910536
At time: 72.23147296905518 and batch: 400, loss is 5.546173696517944 and perplexity is 256.25516760634486
At time: 73.40747094154358 and batch: 450, loss is 5.5402234840393065 and perplexity is 254.73492228703105
At time: 74.57695603370667 and batch: 500, loss is 5.5295862865448 and perplexity is 252.03961727022752
At time: 75.74659490585327 and batch: 550, loss is 5.500541677474976 and perplexity is 244.8245122767977
At time: 76.92133688926697 and batch: 600, loss is 5.506115570068359 and perplexity is 246.1929480264908
At time: 78.09214901924133 and batch: 650, loss is 5.497143125534057 and perplexity is 243.99387573562782
At time: 79.27462005615234 and batch: 700, loss is 5.538892049789428 and perplexity is 254.39598517316895
At time: 80.44527959823608 and batch: 750, loss is 5.5125273323059085 and perplexity is 247.77655008850354
At time: 81.64106440544128 and batch: 800, loss is 5.538372430801392 and perplexity is 254.26383052679805
At time: 82.81208992004395 and batch: 850, loss is 5.564016675949096 and perplexity is 260.86855921969294
At time: 83.98692226409912 and batch: 900, loss is 5.516729793548584 and perplexity is 248.8200124564866
At time: 85.16735768318176 and batch: 950, loss is 5.513193817138672 and perplexity is 247.94174444470337
At time: 86.34900212287903 and batch: 1000, loss is 5.525240058898926 and perplexity is 250.9465727485573
At time: 87.52634811401367 and batch: 1050, loss is 5.525078001022339 and perplexity is 250.90590817493717
At time: 88.69858598709106 and batch: 1100, loss is 5.509328117370606 and perplexity is 246.98512629127558
At time: 89.87109112739563 and batch: 1150, loss is 5.534087114334106 and perplexity is 253.17656085799334
At time: 91.04478478431702 and batch: 1200, loss is 5.53152813911438 and perplexity is 252.52951655080594
At time: 92.21791100502014 and batch: 1250, loss is 5.554138174057007 and perplexity is 258.3042552576817
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.352933340699133 and perplexity of 211.2269907332369
Finished 3 epochs...
Completing Train Step...
At time: 95.1880042552948 and batch: 50, loss is 5.502482767105103 and perplexity is 245.3002001257953
At time: 96.38798713684082 and batch: 100, loss is 5.47026837348938 and perplexity is 237.52392933390755
At time: 97.57133269309998 and batch: 150, loss is 5.41420126914978 and perplexity is 224.57310056043272
At time: 98.74930381774902 and batch: 200, loss is 5.471102752685547 and perplexity is 237.72219706286714
At time: 99.93286108970642 and batch: 250, loss is 5.469520568847656 and perplexity is 237.34637423356958
At time: 101.10681438446045 and batch: 300, loss is 5.473815803527832 and perplexity is 238.36802515592177
At time: 102.28245282173157 and batch: 350, loss is 5.4972812366485595 and perplexity is 244.02757632889706
At time: 103.46138787269592 and batch: 400, loss is 5.45776517868042 and perplexity is 234.57261028037038
At time: 104.64097046852112 and batch: 450, loss is 5.432137279510498 and perplexity is 228.63738559175655
At time: 105.81982493400574 and batch: 500, loss is 5.437408456802368 and perplexity is 229.84575575553367
At time: 106.99187874794006 and batch: 550, loss is 5.425073614120484 and perplexity is 227.02805817098346
At time: 108.15475797653198 and batch: 600, loss is 5.44352385520935 and perplexity is 231.25566079567315
At time: 109.3285722732544 and batch: 650, loss is 5.4348759365081785 and perplexity is 229.26440316887775
At time: 110.49415946006775 and batch: 700, loss is 5.479319248199463 and perplexity is 239.68348685899053
At time: 111.69560432434082 and batch: 750, loss is 5.4551889514923095 and perplexity is 233.9690756990809
At time: 112.85620951652527 and batch: 800, loss is 5.478143882751465 and perplexity is 239.40193666467903
At time: 114.01532077789307 and batch: 850, loss is 5.509166202545166 and perplexity is 246.94513897502296
At time: 115.17503476142883 and batch: 900, loss is 5.461654777526856 and perplexity is 235.4867803596271
At time: 116.33417558670044 and batch: 950, loss is 5.468462152481079 and perplexity is 237.0952958427112
At time: 117.49992251396179 and batch: 1000, loss is 5.449164381027222 and perplexity is 232.56375000828035
At time: 118.67782258987427 and batch: 1050, loss is 5.443974161148072 and perplexity is 231.35982004309614
At time: 119.85412001609802 and batch: 1100, loss is 5.4378205585479735 and perplexity is 229.94049511248696
At time: 121.02805495262146 and batch: 1150, loss is 5.466917839050293 and perplexity is 236.72942897223916
At time: 122.20312118530273 and batch: 1200, loss is 5.478439979553222 and perplexity is 239.47283330807664
At time: 123.37972497940063 and batch: 1250, loss is 5.479860076904297 and perplexity is 239.8131496282718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.340747248517336 and perplexity of 208.66857933190823
Finished 4 epochs...
Completing Train Step...
At time: 126.36521339416504 and batch: 50, loss is 5.449315567016601 and perplexity is 232.59891304693153
At time: 127.54973196983337 and batch: 100, loss is 5.421081314086914 and perplexity is 226.1235008802145
At time: 128.73248529434204 and batch: 150, loss is 5.354094161987304 and perplexity is 211.4723298906251
At time: 129.9125587940216 and batch: 200, loss is 5.421223869323731 and perplexity is 226.1557382671819
At time: 131.0932137966156 and batch: 250, loss is 5.417902717590332 and perplexity is 225.40588661980854
At time: 132.27136254310608 and batch: 300, loss is 5.421571092605591 and perplexity is 226.23427843953365
At time: 133.4466826915741 and batch: 350, loss is 5.455076112747192 and perplexity is 233.94267641164194
At time: 134.62472105026245 and batch: 400, loss is 5.413794574737548 and perplexity is 224.48178650501393
At time: 135.80169701576233 and batch: 450, loss is 5.393187704086304 and perplexity is 219.90325597908173
At time: 136.9802565574646 and batch: 500, loss is 5.381260728836059 and perplexity is 217.29605420978442
At time: 138.1691813468933 and batch: 550, loss is 5.368632555007935 and perplexity is 214.56925534703836
At time: 139.35349869728088 and batch: 600, loss is 5.400570049285888 and perplexity is 221.5326647541645
At time: 140.55871725082397 and batch: 650, loss is 5.39256875038147 and perplexity is 219.76718815827323
At time: 141.74735713005066 and batch: 700, loss is 5.422588176727295 and perplexity is 226.4644947867696
At time: 142.93440866470337 and batch: 750, loss is 5.405699768066406 and perplexity is 222.6719847222316
At time: 144.10270261764526 and batch: 800, loss is 5.418818140029908 and perplexity is 225.6123227001268
At time: 145.27798700332642 and batch: 850, loss is 5.461727323532105 and perplexity is 235.50386460452057
At time: 146.45354342460632 and batch: 900, loss is 5.4105605792999265 and perplexity is 223.7569860639946
At time: 147.62632155418396 and batch: 950, loss is 5.4187713050842286 and perplexity is 225.60175640668646
At time: 148.79984879493713 and batch: 1000, loss is 5.420259599685669 and perplexity is 225.93776826311517
At time: 149.97570157051086 and batch: 1050, loss is 5.419802217483521 and perplexity is 225.83445197843463
At time: 151.14331436157227 and batch: 1100, loss is 5.393675727844238 and perplexity is 220.01060018357714
At time: 152.31113982200623 and batch: 1150, loss is 5.4357164859771725 and perplexity is 229.45719225425142
At time: 153.47844123840332 and batch: 1200, loss is 5.44454174041748 and perplexity is 231.4911723536247
At time: 154.65479969978333 and batch: 1250, loss is 5.4547926044464115 and perplexity is 233.87636112188414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.344248082515967 and perplexity of 209.4003735865869
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 157.66891360282898 and batch: 50, loss is 5.365158424377442 and perplexity is 213.82510710723508
At time: 158.87057876586914 and batch: 100, loss is 5.278559703826904 and perplexity is 196.08724815411148
At time: 160.04846239089966 and batch: 150, loss is 5.163008031845092 and perplexity is 174.68913656675414
At time: 161.22559881210327 and batch: 200, loss is 5.209388608932495 and perplexity is 182.98215033524377
At time: 162.40225958824158 and batch: 250, loss is 5.208513011932373 and perplexity is 182.82200183633873
At time: 163.58359956741333 and batch: 300, loss is 5.185091218948364 and perplexity is 178.58973981508947
At time: 164.76375031471252 and batch: 350, loss is 5.193211402893066 and perplexity is 180.04582519271392
At time: 165.94008612632751 and batch: 400, loss is 5.173636083602905 and perplexity is 176.55564284506906
At time: 167.1159369945526 and batch: 450, loss is 5.128644523620605 and perplexity is 168.78817453247905
At time: 168.29104280471802 and batch: 500, loss is 5.115605897903443 and perplexity is 166.60169402585123
At time: 169.46877884864807 and batch: 550, loss is 5.088406038284302 and perplexity is 162.13122501388835
At time: 170.6775095462799 and batch: 600, loss is 5.111174249649048 and perplexity is 165.8650074928161
At time: 171.85819339752197 and batch: 650, loss is 5.103054046630859 and perplexity is 164.52360356522078
At time: 173.05069303512573 and batch: 700, loss is 5.121443309783936 and perplexity is 167.57706077292232
At time: 174.23191714286804 and batch: 750, loss is 5.1043440628051755 and perplexity is 164.73597862905854
At time: 175.41274785995483 and batch: 800, loss is 5.100561809539795 and perplexity is 164.11408226212035
At time: 176.5955069065094 and batch: 850, loss is 5.138550052642822 and perplexity is 170.46841880943683
At time: 177.77296781539917 and batch: 900, loss is 5.0898833656311036 and perplexity is 162.37092291891187
At time: 178.94636416435242 and batch: 950, loss is 5.087168245315552 and perplexity is 161.93066427549437
At time: 180.126727104187 and batch: 1000, loss is 5.073887062072754 and perplexity is 159.79425189612854
At time: 181.3037781715393 and batch: 1050, loss is 5.0604783630371095 and perplexity is 157.6659198350636
At time: 182.49145698547363 and batch: 1100, loss is 5.001964101791382 and perplexity is 148.7049441081592
At time: 183.66810584068298 and batch: 1150, loss is 5.034816026687622 and perplexity is 153.67131858959112
At time: 184.84324264526367 and batch: 1200, loss is 5.0558944511413575 and perplexity is 156.94484707918153
At time: 186.00891137123108 and batch: 1250, loss is 5.107035856246949 and perplexity is 165.18001120983828
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.013750730639827 and perplexity of 150.46804418394547
Finished 6 epochs...
Completing Train Step...
At time: 188.96619486808777 and batch: 50, loss is 5.119319381713868 and perplexity is 167.22151685815453
At time: 190.13787364959717 and batch: 100, loss is 5.095850744247437 and perplexity is 163.34274843379185
At time: 191.30060696601868 and batch: 150, loss is 5.0212469482421875 and perplexity is 151.60022361530662
At time: 192.4629728794098 and batch: 200, loss is 5.080469026565551 and perplexity is 160.84948091866858
At time: 193.62574005126953 and batch: 250, loss is 5.094924211502075 and perplexity is 163.19147611884907
At time: 194.78855872154236 and batch: 300, loss is 5.081170883178711 and perplexity is 160.96241381728956
At time: 195.9671721458435 and batch: 350, loss is 5.093220977783203 and perplexity is 162.9137594694099
At time: 197.14766645431519 and batch: 400, loss is 5.077560377120972 and perplexity is 160.38230591885318
At time: 198.32749342918396 and batch: 450, loss is 5.027676801681519 and perplexity is 152.57813136714807
At time: 199.50385904312134 and batch: 500, loss is 5.034791669845581 and perplexity is 153.6675756871408
At time: 200.70941925048828 and batch: 550, loss is 5.00591100692749 and perplexity is 149.29302820846843
At time: 201.88601183891296 and batch: 600, loss is 5.038698539733887 and perplexity is 154.26910920285957
At time: 203.06254839897156 and batch: 650, loss is 5.042129163742065 and perplexity is 154.79925736216205
At time: 204.23997569084167 and batch: 700, loss is 5.054700584411621 and perplexity is 156.75758765149237
At time: 205.41943788528442 and batch: 750, loss is 5.033379411697387 and perplexity is 153.45071057207068
At time: 206.5971496105194 and batch: 800, loss is 5.046140546798706 and perplexity is 155.42146359976832
At time: 207.77497243881226 and batch: 850, loss is 5.086116333007812 and perplexity is 161.7604169750563
At time: 208.95109796524048 and batch: 900, loss is 5.044838094711304 and perplexity is 155.21916635990465
At time: 210.1315722465515 and batch: 950, loss is 5.045429582595825 and perplexity is 155.31100377393338
At time: 211.3102834224701 and batch: 1000, loss is 5.0315572452545165 and perplexity is 153.1713524324412
At time: 212.4872167110443 and batch: 1050, loss is 5.030963048934937 and perplexity is 153.0803656132544
At time: 213.6672284603119 and batch: 1100, loss is 4.9919294834136965 and perplexity is 147.22020857896112
At time: 214.85939812660217 and batch: 1150, loss is 5.031051607131958 and perplexity is 153.09392273472068
At time: 216.04756546020508 and batch: 1200, loss is 5.0544788646698 and perplexity is 156.7228352524187
At time: 217.22660279273987 and batch: 1250, loss is 5.094426364898681 and perplexity is 163.1102520170097
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.004010499828923 and perplexity of 149.0095651956464
Finished 7 epochs...
Completing Train Step...
At time: 220.24464225769043 and batch: 50, loss is 5.078683204650879 and perplexity is 160.5624887254264
At time: 221.4203131198883 and batch: 100, loss is 5.056564531326294 and perplexity is 157.05004795391162
At time: 222.59886479377747 and batch: 150, loss is 4.988360376358032 and perplexity is 146.6957004633818
At time: 223.7748782634735 and batch: 200, loss is 5.045209703445434 and perplexity is 155.2768578765002
At time: 224.9493124485016 and batch: 250, loss is 5.060739660263062 and perplexity is 157.70712288543066
At time: 226.12504696846008 and batch: 300, loss is 5.04950982093811 and perplexity is 155.9460042822458
At time: 227.2991180419922 and batch: 350, loss is 5.05759370803833 and perplexity is 157.2117634085057
At time: 228.47763109207153 and batch: 400, loss is 5.0421409416198735 and perplexity is 154.80108057963687
At time: 229.66380047798157 and batch: 450, loss is 4.992219362258911 and perplexity is 147.26289078905228
At time: 230.87023162841797 and batch: 500, loss is 5.003968877792358 and perplexity is 149.00336324314515
At time: 232.0538203716278 and batch: 550, loss is 4.974471139907837 and perplexity is 144.67229351022087
At time: 233.23218631744385 and batch: 600, loss is 5.0047796440124515 and perplexity is 149.1242191230551
At time: 234.41182279586792 and batch: 650, loss is 5.0108235073089595 and perplexity is 150.02823463888828
At time: 235.58993577957153 and batch: 700, loss is 5.026486787796021 and perplexity is 152.39666926482306
At time: 236.76502895355225 and batch: 750, loss is 5.007178611755371 and perplexity is 149.48239276615206
At time: 237.94089579582214 and batch: 800, loss is 5.023068313598633 and perplexity is 151.87659462051397
At time: 239.11802983283997 and batch: 850, loss is 5.063693046569824 and perplexity is 158.17358141970666
At time: 240.29624438285828 and batch: 900, loss is 5.023791217803955 and perplexity is 151.9864265436517
At time: 241.4754593372345 and batch: 950, loss is 5.020954265594482 and perplexity is 151.5558593531077
At time: 242.65894174575806 and batch: 1000, loss is 5.007424221038819 and perplexity is 149.51911153857844
At time: 243.83591604232788 and batch: 1050, loss is 5.010808801651001 and perplexity is 150.0260283912077
At time: 245.01540064811707 and batch: 1100, loss is 4.976444406509399 and perplexity is 144.95805236165134
At time: 246.20171546936035 and batch: 1150, loss is 5.016116619110107 and perplexity is 150.8244562441687
At time: 247.3805797100067 and batch: 1200, loss is 5.036983261108398 and perplexity is 154.00472151137174
At time: 248.5563726425171 and batch: 1250, loss is 5.0682784366607665 and perplexity is 158.9005344003641
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.988931252138458 and perplexity of 146.77946939442083
Finished 8 epochs...
Completing Train Step...
At time: 251.5118761062622 and batch: 50, loss is 5.042022571563721 and perplexity is 154.78275785148875
At time: 252.71441316604614 and batch: 100, loss is 5.02443645477295 and perplexity is 152.08452544996
At time: 253.89047574996948 and batch: 150, loss is 4.952902717590332 and perplexity is 141.5853503022977
At time: 255.06617903709412 and batch: 200, loss is 5.01067232131958 and perplexity is 150.0055541863261
At time: 256.24046897888184 and batch: 250, loss is 5.027609033584595 and perplexity is 152.56779178790381
At time: 257.407488822937 and batch: 300, loss is 5.018172998428344 and perplexity is 151.1349276507558
At time: 258.5751442909241 and batch: 350, loss is 5.024456157684326 and perplexity is 152.0875219874068
At time: 259.7724096775055 and batch: 400, loss is 5.013247356414795 and perplexity is 150.39232150883274
At time: 260.942449092865 and batch: 450, loss is 4.96160044670105 and perplexity is 142.82219238667508
At time: 262.10854387283325 and batch: 500, loss is 4.974415988922119 and perplexity is 144.66431491064355
At time: 263.2736141681671 and batch: 550, loss is 4.944561643600464 and perplexity is 140.4092880492371
At time: 264.4386057853699 and batch: 600, loss is 4.977169666290283 and perplexity is 145.0632227402574
At time: 265.60660696029663 and batch: 650, loss is 4.984934587478637 and perplexity is 146.19401179446342
At time: 266.7775459289551 and batch: 700, loss is 4.995463314056397 and perplexity is 147.74138018699747
At time: 267.95367908477783 and batch: 750, loss is 4.977984619140625 and perplexity is 145.1814906119333
At time: 269.1348876953125 and batch: 800, loss is 4.996400203704834 and perplexity is 147.87986241789523
At time: 270.311322927475 and batch: 850, loss is 5.038926858901977 and perplexity is 154.30433581883756
At time: 271.49085664749146 and batch: 900, loss is 4.9959484958648686 and perplexity is 147.8130790090983
At time: 272.67031478881836 and batch: 950, loss is 4.992892589569092 and perplexity is 147.36206556874689
At time: 273.8496265411377 and batch: 1000, loss is 4.979370784759522 and perplexity is 145.3828757470489
At time: 275.0276098251343 and batch: 1050, loss is 4.987304077148438 and perplexity is 146.54082772130914
At time: 276.2039895057678 and batch: 1100, loss is 4.949209794998169 and perplexity is 141.06345082375924
At time: 277.3794400691986 and batch: 1150, loss is 4.989630861282349 and perplexity is 146.8821935825454
At time: 278.5551815032959 and batch: 1200, loss is 5.010800104141236 and perplexity is 150.02472354403525
At time: 279.73152565956116 and batch: 1250, loss is 5.040894231796265 and perplexity is 154.60820880432294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.970912070169936 and perplexity of 144.15830992274334
Finished 9 epochs...
Completing Train Step...
At time: 282.7936542034149 and batch: 50, loss is 5.0093386840820315 and perplexity is 149.80563453320227
At time: 283.96940994262695 and batch: 100, loss is 4.98558388710022 and perplexity is 146.28896633463674
At time: 285.145295381546 and batch: 150, loss is 4.910532522201538 and perplexity is 135.7116646437611
At time: 286.32071328163147 and batch: 200, loss is 4.972040319442749 and perplexity is 144.32104821850254
At time: 287.4991719722748 and batch: 250, loss is 4.993284902572632 and perplexity is 147.41988896499154
At time: 288.67728972435 and batch: 300, loss is 4.984557847976685 and perplexity is 146.13894510882074
At time: 289.88156390190125 and batch: 350, loss is 4.985425395965576 and perplexity is 146.26578266762786
At time: 291.05742025375366 and batch: 400, loss is 4.979765710830688 and perplexity is 145.4403025739117
At time: 292.23307633399963 and batch: 450, loss is 4.925409240722656 and perplexity is 137.74570126007325
At time: 293.4117772579193 and batch: 500, loss is 4.938474445343018 and perplexity is 139.55718496704253
At time: 294.5860662460327 and batch: 550, loss is 4.913317756652832 and perplexity is 136.09018033033914
At time: 295.7597224712372 and batch: 600, loss is 4.943643779754638 and perplexity is 140.28047056761696
At time: 296.9353437423706 and batch: 650, loss is 4.951173315048218 and perplexity is 141.34070384463638
At time: 298.1099398136139 and batch: 700, loss is 4.962082080841064 and perplexity is 142.89099699847512
At time: 299.29761004447937 and batch: 750, loss is 4.943530797958374 and perplexity is 140.26462232336982
At time: 300.4834520816803 and batch: 800, loss is 4.9598448276519775 and perplexity is 142.57167099949686
At time: 301.6590461730957 and batch: 850, loss is 5.007288055419922 and perplexity is 149.49875356227858
At time: 302.8441505432129 and batch: 900, loss is 4.95985933303833 and perplexity is 142.5737390716667
At time: 304.02465319633484 and batch: 950, loss is 4.955005168914795 and perplexity is 141.88333975395705
At time: 305.20720648765564 and batch: 1000, loss is 4.944608488082886 and perplexity is 140.41586560372286
At time: 306.3890097141266 and batch: 1050, loss is 4.952884769439697 and perplexity is 141.5828091299075
At time: 307.563405752182 and batch: 1100, loss is 4.918266363143921 and perplexity is 136.76530616840162
At time: 308.7378921508789 and batch: 1150, loss is 4.964326028823852 and perplexity is 143.21199698188673
At time: 309.9239993095398 and batch: 1200, loss is 4.983001623153687 and perplexity is 145.91169692533083
At time: 311.1095640659332 and batch: 1250, loss is 5.011873388290406 and perplexity is 150.18582914237584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.953280038207117 and perplexity of 141.63878345408244
Finished 10 epochs...
Completing Train Step...
At time: 314.0685076713562 and batch: 50, loss is 4.979966564178467 and perplexity is 145.46951767946328
At time: 315.26674604415894 and batch: 100, loss is 4.951204242706299 and perplexity is 141.3450752491962
At time: 316.4433250427246 and batch: 150, loss is 4.8775428295135494 and perplexity is 131.30762195050823
At time: 317.61756706237793 and batch: 200, loss is 4.942934627532959 and perplexity is 140.18102562522415
At time: 318.79497241973877 and batch: 250, loss is 4.9661204051971435 and perplexity is 143.4692038996384
At time: 320.0172200202942 and batch: 300, loss is 4.959003620147705 and perplexity is 142.45178906981414
At time: 321.1926453113556 and batch: 350, loss is 4.956090517044068 and perplexity is 142.03741616948375
At time: 322.37192606925964 and batch: 400, loss is 4.948170967102051 and perplexity is 140.91698626482722
At time: 323.54819226264954 and batch: 450, loss is 4.8913828563690185 and perplexity is 133.13755692957182
At time: 324.7244403362274 and batch: 500, loss is 4.90968599319458 and perplexity is 135.59682939559966
At time: 325.90345287323 and batch: 550, loss is 4.88332272529602 and perplexity is 132.06876386232238
At time: 327.0841054916382 and batch: 600, loss is 4.912891807556153 and perplexity is 136.0322251848077
At time: 328.2621178627014 and batch: 650, loss is 4.921254405975342 and perplexity is 137.1745779173311
At time: 329.43696665763855 and batch: 700, loss is 4.936478261947632 and perplexity is 139.27888109690096
At time: 330.6112325191498 and batch: 750, loss is 4.913072662353516 and perplexity is 136.05682949016457
At time: 331.7888352870941 and batch: 800, loss is 4.937360792160034 and perplexity is 139.4018531727232
At time: 332.96529817581177 and batch: 850, loss is 4.983822412490845 and perplexity is 146.0315088537841
At time: 334.1394863128662 and batch: 900, loss is 4.933771524429321 and perplexity is 138.90239947225234
At time: 335.3125557899475 and batch: 950, loss is 4.930351476669312 and perplexity is 138.42815805847567
At time: 336.4786128997803 and batch: 1000, loss is 4.920285730361939 and perplexity is 137.0417645858267
At time: 337.65253233909607 and batch: 1050, loss is 4.931336088180542 and perplexity is 138.56452313867555
At time: 338.8211829662323 and batch: 1100, loss is 4.893751754760742 and perplexity is 133.45332013186783
At time: 339.9916274547577 and batch: 1150, loss is 4.929580783843994 and perplexity is 138.32151357058746
At time: 341.1565680503845 and batch: 1200, loss is 4.955064334869385 and perplexity is 141.89173466553802
At time: 342.3183686733246 and batch: 1250, loss is 4.98278805732727 and perplexity is 145.88053850049894
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.939289176551095 and perplexity of 139.6709328916466
Finished 11 epochs...
Completing Train Step...
At time: 345.26902294158936 and batch: 50, loss is 4.943211832046509 and perplexity is 140.2198898246584
At time: 346.44270944595337 and batch: 100, loss is 4.919190826416016 and perplexity is 136.89179913089137
At time: 347.6237335205078 and batch: 150, loss is 4.850024681091309 and perplexity is 127.74354265716174
At time: 348.80391812324524 and batch: 200, loss is 4.915079116821289 and perplexity is 136.33009537967115
At time: 350.03817558288574 and batch: 250, loss is 4.9387325668334965 and perplexity is 139.5932123251509
At time: 351.21933150291443 and batch: 300, loss is 4.937665176391602 and perplexity is 139.44429135710968
At time: 352.39560627937317 and batch: 350, loss is 4.931009349822998 and perplexity is 138.51925618959834
At time: 353.57349920272827 and batch: 400, loss is 4.9219104671478275 and perplexity is 137.2646023592963
At time: 354.755158662796 and batch: 450, loss is 4.866051177978516 and perplexity is 129.80731751362248
At time: 355.9349000453949 and batch: 500, loss is 4.881920576095581 and perplexity is 131.8837135150858
At time: 357.1175539493561 and batch: 550, loss is 4.855444164276123 and perplexity is 128.43772599697036
At time: 358.294970035553 and batch: 600, loss is 4.881183233261108 and perplexity is 131.786505846035
At time: 359.4768579006195 and batch: 650, loss is 4.894521484375 and perplexity is 133.55608264909696
At time: 360.6550624370575 and batch: 700, loss is 4.910598907470703 and perplexity is 135.7206741981949
At time: 361.83236813545227 and batch: 750, loss is 4.89183403968811 and perplexity is 133.197639927607
At time: 363.01209139823914 and batch: 800, loss is 4.908140468597412 and perplexity is 135.3874230235033
At time: 364.1893994808197 and batch: 850, loss is 4.957475528717041 and perplexity is 142.2342759439513
At time: 365.37186336517334 and batch: 900, loss is 4.913766288757325 and perplexity is 136.151234836753
At time: 366.5518009662628 and batch: 950, loss is 4.913460302352905 and perplexity is 136.1095807830601
At time: 367.72821068763733 and batch: 1000, loss is 4.904514284133911 and perplexity is 134.89737229772777
At time: 368.90885043144226 and batch: 1050, loss is 4.912086877822876 and perplexity is 135.9227728586981
At time: 370.0856168270111 and batch: 1100, loss is 4.871480531692505 and perplexity is 130.51400404467137
At time: 371.26181411743164 and batch: 1150, loss is 4.906727638244629 and perplexity is 135.1962786219718
At time: 372.4448661804199 and batch: 1200, loss is 4.9311392116546635 and perplexity is 138.53724572197964
At time: 373.62254309654236 and batch: 1250, loss is 4.9664152812957765 and perplexity is 143.51151577683234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.93086276089188 and perplexity of 138.4989522880942
Finished 12 epochs...
Completing Train Step...
At time: 376.6352243423462 and batch: 50, loss is 4.91339295387268 and perplexity is 136.10041431832752
At time: 377.84911155700684 and batch: 100, loss is 4.900192899703979 and perplexity is 134.31568664232606
At time: 379.0225133895874 and batch: 150, loss is 4.832371730804443 and perplexity is 125.50827978830691
At time: 380.2233684062958 and batch: 200, loss is 4.895702095031738 and perplexity is 133.71385349819195
At time: 381.39702010154724 and batch: 250, loss is 4.917362422943115 and perplexity is 136.64173436925321
At time: 382.57090997695923 and batch: 300, loss is 4.919069366455078 and perplexity is 136.875173268025
At time: 383.7444865703583 and batch: 350, loss is 4.912023963928223 and perplexity is 135.91422169668178
At time: 384.91851258277893 and batch: 400, loss is 4.8992405605316165 and perplexity is 134.18783344194227
At time: 386.09777998924255 and batch: 450, loss is 4.8458569145202635 and perplexity is 127.21224532111152
At time: 387.2745623588562 and batch: 500, loss is 4.866325845718384 and perplexity is 129.84297629307628
At time: 388.44818019866943 and batch: 550, loss is 4.843687171936035 and perplexity is 126.93652672256388
At time: 389.62197756767273 and batch: 600, loss is 4.867775087356567 and perplexity is 130.03128656132765
At time: 390.79448556900024 and batch: 650, loss is 4.878143072128296 and perplexity is 131.38646204006758
At time: 391.9659433364868 and batch: 700, loss is 4.887615613937378 and perplexity is 132.63693904240378
At time: 393.139790058136 and batch: 750, loss is 4.868723745346069 and perplexity is 130.15470030967293
At time: 394.3139672279358 and batch: 800, loss is 4.893462133407593 and perplexity is 133.4146747972362
At time: 395.4899184703827 and batch: 850, loss is 4.939342966079712 and perplexity is 139.67844592734784
At time: 396.6656394004822 and batch: 900, loss is 4.897887048721313 and perplexity is 134.0063314848101
At time: 397.8377888202667 and batch: 950, loss is 4.899630947113037 and perplexity is 134.24022879806446
At time: 399.0153229236603 and batch: 1000, loss is 4.889634265899658 and perplexity is 132.90495728643617
At time: 400.20403933525085 and batch: 1050, loss is 4.898896026611328 and perplexity is 134.14160914500786
At time: 401.38092494010925 and batch: 1100, loss is 4.858643064498901 and perplexity is 128.84924331777975
At time: 402.5573830604553 and batch: 1150, loss is 4.886923923492431 and perplexity is 132.5452270608284
At time: 403.73146319389343 and batch: 1200, loss is 4.917975826263428 and perplexity is 136.7255765747233
At time: 404.9055049419403 and batch: 1250, loss is 4.948262042999268 and perplexity is 140.92982099024258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.930673863765967 and perplexity of 138.4727927048779
Finished 13 epochs...
Completing Train Step...
At time: 407.8881821632385 and batch: 50, loss is 4.896787910461426 and perplexity is 133.85912091601273
At time: 409.09623551368713 and batch: 100, loss is 4.88362156867981 and perplexity is 132.10823763655222
At time: 410.27416825294495 and batch: 150, loss is 4.810701379776001 and perplexity is 122.81772918662924
At time: 411.4508707523346 and batch: 200, loss is 4.88022747039795 and perplexity is 131.66060937105328
At time: 412.6299133300781 and batch: 250, loss is 4.903978338241577 and perplexity is 134.82509397552326
At time: 413.809237241745 and batch: 300, loss is 4.897895202636719 and perplexity is 134.00742416555568
At time: 414.9872977733612 and batch: 350, loss is 4.89232385635376 and perplexity is 133.26289833249118
At time: 416.16382217407227 and batch: 400, loss is 4.882250537872315 and perplexity is 131.92723727971392
At time: 417.34019589424133 and batch: 450, loss is 4.828755798339844 and perplexity is 125.05526984511536
At time: 418.51281547546387 and batch: 500, loss is 4.847302846908569 and perplexity is 127.39631867355095
At time: 419.6867425441742 and batch: 550, loss is 4.824064302444458 and perplexity is 124.46994765185873
At time: 420.85672068595886 and batch: 600, loss is 4.850595636367798 and perplexity is 127.81649933239663
At time: 422.02809166908264 and batch: 650, loss is 4.858718605041504 and perplexity is 128.85897702717412
At time: 423.2047770023346 and batch: 700, loss is 4.87145094871521 and perplexity is 130.51014310896227
At time: 424.37751054763794 and batch: 750, loss is 4.852296352386475 and perplexity is 128.03406385555633
At time: 425.5479745864868 and batch: 800, loss is 4.879044990539551 and perplexity is 131.50501536384743
At time: 426.71608448028564 and batch: 850, loss is 4.926373996734619 and perplexity is 137.87865637779817
At time: 427.88581824302673 and batch: 900, loss is 4.886395330429077 and perplexity is 132.47518308723357
At time: 429.0539963245392 and batch: 950, loss is 4.887330064773559 and perplexity is 132.59907008235106
At time: 430.2298586368561 and batch: 1000, loss is 4.877430353164673 and perplexity is 131.29285377916216
At time: 431.4137740135193 and batch: 1050, loss is 4.886165542602539 and perplexity is 132.44474540008062
At time: 432.5896735191345 and batch: 1100, loss is 4.84643217086792 and perplexity is 127.28544602529836
At time: 433.7722806930542 and batch: 1150, loss is 4.870650072097778 and perplexity is 130.40566243065925
At time: 434.9473898410797 and batch: 1200, loss is 4.903097734451294 and perplexity is 134.70641874734002
At time: 436.125608921051 and batch: 1250, loss is 4.929024410247803 and perplexity is 138.24457653750318
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.9183434256671985 and perplexity of 136.77584605410163
Finished 14 epochs...
Completing Train Step...
At time: 439.1742322444916 and batch: 50, loss is 4.878786535263061 and perplexity is 131.4710315905737
At time: 440.3507857322693 and batch: 100, loss is 4.87138521194458 and perplexity is 130.50156407560192
At time: 441.531197309494 and batch: 150, loss is 4.797872829437256 and perplexity is 121.25221884918614
At time: 442.71085262298584 and batch: 200, loss is 4.861692657470703 and perplexity is 129.24278082423854
At time: 443.89110374450684 and batch: 250, loss is 4.8904070377349855 and perplexity is 133.00770218827265
At time: 445.06490755081177 and batch: 300, loss is 4.881984634399414 and perplexity is 131.89216203267273
At time: 446.2395327091217 and batch: 350, loss is 4.880156030654907 and perplexity is 131.65120390691703
At time: 447.4189577102661 and batch: 400, loss is 4.869217700958252 and perplexity is 130.21900683531996
At time: 448.59580969810486 and batch: 450, loss is 4.815650997161865 and perplexity is 123.42713687761345
At time: 449.77034616470337 and batch: 500, loss is 4.833539400100708 and perplexity is 125.65491754858682
At time: 450.9454872608185 and batch: 550, loss is 4.810356569290161 and perplexity is 122.7753876460818
At time: 452.125812292099 and batch: 600, loss is 4.835027599334717 and perplexity is 125.84205631597865
At time: 453.30729699134827 and batch: 650, loss is 4.84478943824768 and perplexity is 127.07652172139852
At time: 454.48387932777405 and batch: 700, loss is 4.856947383880615 and perplexity is 128.63094129077064
At time: 455.6598241329193 and batch: 750, loss is 4.839204711914062 and perplexity is 126.36881214571353
At time: 456.83648657798767 and batch: 800, loss is 4.863610811233521 and perplexity is 129.4909262650653
At time: 458.01798701286316 and batch: 850, loss is 4.914450416564941 and perplexity is 136.24441155130276
At time: 459.19293308258057 and batch: 900, loss is 4.8759125804901124 and perplexity is 131.09373222218537
At time: 460.37077832221985 and batch: 950, loss is 4.875663013458252 and perplexity is 131.0610196307011
At time: 461.54531836509705 and batch: 1000, loss is 4.866483221054077 and perplexity is 129.86341198305237
At time: 462.7191364765167 and batch: 1050, loss is 4.873222684860229 and perplexity is 130.74157760669212
At time: 463.8954818248749 and batch: 1100, loss is 4.831601104736328 and perplexity is 125.41159709402476
At time: 465.0714316368103 and batch: 1150, loss is 4.859158143997193 and perplexity is 128.91562801661414
At time: 466.2594175338745 and batch: 1200, loss is 4.890615825653076 and perplexity is 133.03547548876645
At time: 467.4394807815552 and batch: 1250, loss is 4.909717464447022 and perplexity is 135.60109686479876
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.918308675724225 and perplexity of 136.77109318383253
Finished 15 epochs...
Completing Train Step...
At time: 470.42235231399536 and batch: 50, loss is 4.8610806655883785 and perplexity is 129.1637094895004
At time: 471.6232144832611 and batch: 100, loss is 4.857459535598755 and perplexity is 128.69683672114724
At time: 472.80306339263916 and batch: 150, loss is 4.78461033821106 and perplexity is 119.65472912290744
At time: 473.98355293273926 and batch: 200, loss is 4.853639059066772 and perplexity is 128.20609151389868
At time: 475.16120648384094 and batch: 250, loss is 4.878218622207641 and perplexity is 131.39638867267385
At time: 476.3428199291229 and batch: 300, loss is 4.869551763534546 and perplexity is 130.26251539911266
At time: 477.522052526474 and batch: 350, loss is 4.8660369300842286 and perplexity is 129.80546804586035
At time: 478.69580578804016 and batch: 400, loss is 4.854417629241944 and perplexity is 128.30594782055363
At time: 479.8732259273529 and batch: 450, loss is 4.800512952804565 and perplexity is 121.57276261686447
At time: 481.05281615257263 and batch: 500, loss is 4.8194929790496825 and perplexity is 123.90225381286601
At time: 482.2331802845001 and batch: 550, loss is 4.793805322647095 and perplexity is 120.76002630255209
At time: 483.4174225330353 and batch: 600, loss is 4.822470102310181 and perplexity is 124.27167572915349
At time: 484.59367871284485 and batch: 650, loss is 4.831507787704468 and perplexity is 125.39989460205253
At time: 485.77103781700134 and batch: 700, loss is 4.845281362533569 and perplexity is 127.13904912671327
At time: 486.95086574554443 and batch: 750, loss is 4.828071460723877 and perplexity is 124.96971909602205
At time: 488.12987756729126 and batch: 800, loss is 4.854610404968262 and perplexity is 128.3306844770725
At time: 489.31018829345703 and batch: 850, loss is 4.9027195739746094 and perplexity is 134.65548773446903
At time: 490.4924306869507 and batch: 900, loss is 4.868444347381592 and perplexity is 130.11834043101385
At time: 491.6682286262512 and batch: 950, loss is 4.865276556015015 and perplexity is 129.706804849138
At time: 492.8442952632904 and batch: 1000, loss is 4.853788795471192 and perplexity is 128.2252900703922
At time: 494.02027916908264 and batch: 1050, loss is 4.861377277374268 and perplexity is 129.202026650424
At time: 495.2020597457886 and batch: 1100, loss is 4.819881629943848 and perplexity is 123.95041789349972
At time: 496.38239765167236 and batch: 1150, loss is 4.848744764328003 and perplexity is 127.58014614478449
At time: 497.56246185302734 and batch: 1200, loss is 4.8764159297943115 and perplexity is 131.15973477085922
At time: 498.78940868377686 and batch: 1250, loss is 4.898483438491821 and perplexity is 134.08627532656612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.911275821880703 and perplexity of 135.81257657980848
Finished 16 epochs...
Completing Train Step...
At time: 501.77996802330017 and batch: 50, loss is 4.849487466812134 and perplexity is 127.67493543201438
At time: 502.95471119880676 and batch: 100, loss is 4.842033882141113 and perplexity is 126.72683724483059
At time: 504.1303746700287 and batch: 150, loss is 4.7716336345672605 and perplexity is 118.11203634539154
At time: 505.29143595695496 and batch: 200, loss is 4.83777871131897 and perplexity is 126.18873856742216
At time: 506.4589548110962 and batch: 250, loss is 4.863944578170776 and perplexity is 129.53415326839814
At time: 507.62712121009827 and batch: 300, loss is 4.855513381958008 and perplexity is 128.4466164663157
At time: 508.79765796661377 and batch: 350, loss is 4.851687841415405 and perplexity is 127.95617742280734
At time: 509.95918679237366 and batch: 400, loss is 4.839912576675415 and perplexity is 126.45829584220671
At time: 511.12144804000854 and batch: 450, loss is 4.787514972686767 and perplexity is 120.00278762094453
At time: 512.2832834720612 and batch: 500, loss is 4.805261449813843 and perplexity is 122.15142331354019
At time: 513.4444456100464 and batch: 550, loss is 4.783777055740356 and perplexity is 119.55506446476572
At time: 514.6237552165985 and batch: 600, loss is 4.811289539337158 and perplexity is 122.8899868557665
At time: 515.802906036377 and batch: 650, loss is 4.818964691162109 and perplexity is 123.83681503971118
At time: 516.9816162586212 and batch: 700, loss is 4.833845777511597 and perplexity is 125.69342127492557
At time: 518.159015417099 and batch: 750, loss is 4.815555772781372 and perplexity is 123.41538416454956
At time: 519.3364610671997 and batch: 800, loss is 4.842442646026611 and perplexity is 126.77864918792383
At time: 520.5259971618652 and batch: 850, loss is 4.889608325958252 and perplexity is 132.90150978434585
At time: 521.7075245380402 and batch: 900, loss is 4.8557768821716305 and perplexity is 128.48046663676143
At time: 522.8827540874481 and batch: 950, loss is 4.852081880569458 and perplexity is 128.00660710169643
At time: 524.0622942447662 and batch: 1000, loss is 4.84147988319397 and perplexity is 126.65665015400455
At time: 525.2365093231201 and batch: 1050, loss is 4.85087025642395 and perplexity is 127.85160512677747
At time: 526.4121625423431 and batch: 1100, loss is 4.80590350151062 and perplexity is 122.22987602479559
At time: 527.5969457626343 and batch: 1150, loss is 4.832548580169678 and perplexity is 125.5304778107144
At time: 528.8025553226471 and batch: 1200, loss is 4.863687515258789 and perplexity is 129.5008591212857
At time: 529.9846997261047 and batch: 1250, loss is 4.884443883895874 and perplexity is 132.2169169287219
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.907819093578923 and perplexity of 135.34391987871237
Finished 17 epochs...
Completing Train Step...
At time: 532.9377446174622 and batch: 50, loss is 4.836802406311035 and perplexity is 126.06559999019962
At time: 534.1490423679352 and batch: 100, loss is 4.831002407073974 and perplexity is 125.33653593572458
At time: 535.3341300487518 and batch: 150, loss is 4.761229991912842 and perplexity is 116.88961079121579
At time: 536.5169439315796 and batch: 200, loss is 4.827616376876831 and perplexity is 124.91286033422485
At time: 537.6930980682373 and batch: 250, loss is 4.853139457702636 and perplexity is 128.1420555732414
At time: 538.8686053752899 and batch: 300, loss is 4.8450864315032955 and perplexity is 127.11426819624359
At time: 540.0432593822479 and batch: 350, loss is 4.840461139678955 and perplexity is 126.52768521527784
At time: 541.2166984081268 and batch: 400, loss is 4.827358627319336 and perplexity is 124.88066824867444
At time: 542.3921101093292 and batch: 450, loss is 4.774267959594726 and perplexity is 118.42359202807293
At time: 543.5695607662201 and batch: 500, loss is 4.794714651107788 and perplexity is 120.86988677343942
At time: 544.7460913658142 and batch: 550, loss is 4.774411306381226 and perplexity is 118.44056888619585
At time: 545.9221498966217 and batch: 600, loss is 4.79689172744751 and perplexity is 121.13331639326772
At time: 547.0956876277924 and batch: 650, loss is 4.805980205535889 and perplexity is 122.23925190787412
At time: 548.2702896595001 and batch: 700, loss is 4.820982789993286 and perplexity is 124.0869823176399
At time: 549.4448044300079 and batch: 750, loss is 4.799579877853393 and perplexity is 121.45937902324366
At time: 550.6260740756989 and batch: 800, loss is 4.827453708648681 and perplexity is 124.89254263312819
At time: 551.8050203323364 and batch: 850, loss is 4.87854118347168 and perplexity is 131.43877889424837
At time: 552.9827983379364 and batch: 900, loss is 4.8447120571136475 and perplexity is 127.06668877648589
At time: 554.1593899726868 and batch: 950, loss is 4.8432077789306645 and perplexity is 126.87568882332384
At time: 555.3361630439758 and batch: 1000, loss is 4.832474899291992 and perplexity is 125.52122895566912
At time: 556.5044193267822 and batch: 1050, loss is 4.841216716766358 and perplexity is 126.62332276136793
At time: 557.7482104301453 and batch: 1100, loss is 4.794727849960327 and perplexity is 120.87148212777973
At time: 558.9256703853607 and batch: 1150, loss is 4.820156059265137 and perplexity is 123.98443819042569
At time: 560.1012945175171 and batch: 1200, loss is 4.852970342636109 and perplexity is 128.12038665331025
At time: 561.2770352363586 and batch: 1250, loss is 4.8731818103790285 and perplexity is 130.73623372175112
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.905572376111998 and perplexity of 135.04018166462012
Finished 18 epochs...
Completing Train Step...
At time: 564.2589113712311 and batch: 50, loss is 4.826741466522217 and perplexity is 124.80362057376101
At time: 565.4643483161926 and batch: 100, loss is 4.822118272781372 and perplexity is 124.22796097455914
At time: 566.6415810585022 and batch: 150, loss is 4.747620248794556 and perplexity is 115.30954971665884
At time: 567.819632768631 and batch: 200, loss is 4.815100555419922 and perplexity is 123.35921612431129
At time: 568.9961614608765 and batch: 250, loss is 4.839878978729248 and perplexity is 126.45404717456434
At time: 570.179532289505 and batch: 300, loss is 4.83162148475647 and perplexity is 125.41415301094428
At time: 571.3603839874268 and batch: 350, loss is 4.828944244384766 and perplexity is 125.07883823672961
At time: 572.5405921936035 and batch: 400, loss is 4.815469264984131 and perplexity is 123.40470823330213
At time: 573.7191393375397 and batch: 450, loss is 4.764236717224121 and perplexity is 117.24159463680343
At time: 574.8962149620056 and batch: 500, loss is 4.78138222694397 and perplexity is 119.26909311651693
At time: 576.0780038833618 and batch: 550, loss is 4.763540916442871 and perplexity is 117.160046217677
At time: 577.2578549385071 and batch: 600, loss is 4.784411668777466 and perplexity is 119.63095974684816
At time: 578.4379279613495 and batch: 650, loss is 4.7949005889892575 and perplexity is 120.89236315366003
At time: 579.6199400424957 and batch: 700, loss is 4.806483020782471 and perplexity is 122.30073112251216
At time: 580.7962245941162 and batch: 750, loss is 4.7909896945953365 and perplexity is 120.4204892143527
At time: 581.9710204601288 and batch: 800, loss is 4.818472166061401 and perplexity is 123.77583731767437
At time: 583.1490650177002 and batch: 850, loss is 4.8673387718200685 and perplexity is 129.9745642660792
At time: 584.3263165950775 and batch: 900, loss is 4.83306116104126 and perplexity is 125.5948388261443
At time: 585.5029354095459 and batch: 950, loss is 4.82878734588623 and perplexity is 125.05921509427273
At time: 586.6841795444489 and batch: 1000, loss is 4.820683746337891 and perplexity is 124.04988044067764
At time: 587.9088068008423 and batch: 1050, loss is 4.828404064178467 and perplexity is 125.01129136947011
At time: 589.0860939025879 and batch: 1100, loss is 4.783526020050049 and perplexity is 119.52505564342827
At time: 590.2637641429901 and batch: 1150, loss is 4.810920286178589 and perplexity is 122.84461771682784
At time: 591.431253194809 and batch: 1200, loss is 4.840111417770386 and perplexity is 126.48344344832601
At time: 592.5995202064514 and batch: 1250, loss is 4.86590874671936 and perplexity is 129.78883021055756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.903797455947765 and perplexity of 134.80070870875946
Finished 19 epochs...
Completing Train Step...
At time: 595.5633316040039 and batch: 50, loss is 4.811419029235839 and perplexity is 122.90590089804498
At time: 596.729811668396 and batch: 100, loss is 4.807834749221802 and perplexity is 122.46616028138183
At time: 597.8903863430023 and batch: 150, loss is 4.736523065567017 and perplexity is 114.03701236476701
At time: 599.0507264137268 and batch: 200, loss is 4.801464099884033 and perplexity is 121.68845120467972
At time: 600.2117464542389 and batch: 250, loss is 4.8266619682312015 and perplexity is 124.79369929358054
At time: 601.3762378692627 and batch: 300, loss is 4.82063325881958 and perplexity is 124.04361762816569
At time: 602.5526146888733 and batch: 350, loss is 4.815865774154663 and perplexity is 123.45364903390035
At time: 603.7341363430023 and batch: 400, loss is 4.805207834243775 and perplexity is 122.1448742709115
At time: 604.9115326404572 and batch: 450, loss is 4.7532478713989255 and perplexity is 115.96029770976294
At time: 606.0869972705841 and batch: 500, loss is 4.770663118362426 and perplexity is 117.99746230710592
At time: 607.2630567550659 and batch: 550, loss is 4.752367649078369 and perplexity is 115.85827177675962
At time: 608.438905954361 and batch: 600, loss is 4.774666452407837 and perplexity is 118.47079238227525
At time: 609.6170651912689 and batch: 650, loss is 4.783912706375122 and perplexity is 119.57128328517162
At time: 610.8024570941925 and batch: 700, loss is 4.797614078521729 and perplexity is 121.22084878524068
At time: 611.985143661499 and batch: 750, loss is 4.779363822937012 and perplexity is 119.02860268613404
At time: 613.1615822315216 and batch: 800, loss is 4.808056125640869 and perplexity is 122.49327440250464
At time: 614.3373260498047 and batch: 850, loss is 4.85543194770813 and perplexity is 128.43615693834215
At time: 615.5142955780029 and batch: 900, loss is 4.820501165390015 and perplexity is 124.02723336344819
At time: 616.6940743923187 and batch: 950, loss is 4.814588270187378 and perplexity is 123.29603720378832
At time: 617.9286060333252 and batch: 1000, loss is 4.806841068267822 and perplexity is 122.34452843203755
At time: 619.110639333725 and batch: 1050, loss is 4.8182997894287105 and perplexity is 123.75450309444273
At time: 620.2875745296478 and batch: 1100, loss is 4.7719017505645756 and perplexity is 118.14370831750266
At time: 621.4640016555786 and batch: 1150, loss is 4.798126201629639 and perplexity is 121.28294468207875
At time: 622.6400079727173 and batch: 1200, loss is 4.825610160827637 and perplexity is 124.66250936211051
At time: 623.8218233585358 and batch: 1250, loss is 4.851387453079224 and perplexity is 127.91774665193384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.903439709739963 and perplexity of 134.75249289143707
Finished 20 epochs...
Completing Train Step...
At time: 626.7845466136932 and batch: 50, loss is 4.800347013473511 and perplexity is 121.55259058767416
At time: 627.9913654327393 and batch: 100, loss is 4.793600902557373 and perplexity is 120.73534305010287
At time: 629.1762251853943 and batch: 150, loss is 4.7269374370574955 and perplexity is 112.94911833194752
At time: 630.3572542667389 and batch: 200, loss is 4.789554834365845 and perplexity is 120.24782654658412
At time: 631.535197019577 and batch: 250, loss is 4.81307825088501 and perplexity is 123.1099983042196
At time: 632.7135255336761 and batch: 300, loss is 4.807210950851441 and perplexity is 122.38978991250623
At time: 633.8905861377716 and batch: 350, loss is 4.805994501113892 and perplexity is 122.24099940112549
At time: 635.0688047409058 and batch: 400, loss is 4.792752914428711 and perplexity is 120.63300430963737
At time: 636.250137090683 and batch: 450, loss is 4.7377277278900145 and perplexity is 114.17447123612823
At time: 637.4321618080139 and batch: 500, loss is 4.75889946937561 and perplexity is 116.61751410578147
At time: 638.6131615638733 and batch: 550, loss is 4.741141843795776 and perplexity is 114.56494229184487
At time: 639.7942378520966 and batch: 600, loss is 4.761725883483887 and perplexity is 116.9475897383974
At time: 640.9720191955566 and batch: 650, loss is 4.773086261749268 and perplexity is 118.28373377589448
At time: 642.1499865055084 and batch: 700, loss is 4.787236347198486 and perplexity is 119.9693564432539
At time: 643.3263597488403 and batch: 750, loss is 4.770478887557983 and perplexity is 117.97572554205517
At time: 644.5035443305969 and batch: 800, loss is 4.79599684715271 and perplexity is 121.02496506334944
At time: 645.6816799640656 and batch: 850, loss is 4.845573301315308 and perplexity is 127.17617136427344
At time: 646.8612513542175 and batch: 900, loss is 4.807710018157959 and perplexity is 122.45088589953995
At time: 648.105416059494 and batch: 950, loss is 4.804351444244385 and perplexity is 122.04031540008515
At time: 649.2853994369507 and batch: 1000, loss is 4.79891523361206 and perplexity is 121.37867856793726
At time: 650.4635236263275 and batch: 1050, loss is 4.806263380050659 and perplexity is 122.27387185023035
At time: 651.6399960517883 and batch: 1100, loss is 4.763296499252319 and perplexity is 117.13141378760716
At time: 652.8219544887543 and batch: 1150, loss is 4.789855613708496 and perplexity is 120.2840000486564
At time: 654.0021371841431 and batch: 1200, loss is 4.815240106582642 and perplexity is 123.37643224759071
At time: 655.1820175647736 and batch: 1250, loss is 4.842468605041504 and perplexity is 126.78194027948281
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.898376910355839 and perplexity of 134.07199212638972
Finished 21 epochs...
Completing Train Step...
At time: 658.2001538276672 and batch: 50, loss is 4.786849012374878 and perplexity is 119.92289713197302
At time: 659.37846326828 and batch: 100, loss is 4.781625003814697 and perplexity is 119.29805240889915
At time: 660.5517110824585 and batch: 150, loss is 4.717058582305908 and perplexity is 111.83880374567212
At time: 661.7275595664978 and batch: 200, loss is 4.781738834381104 and perplexity is 119.31163294670155
At time: 662.9076225757599 and batch: 250, loss is 4.805441675186157 and perplexity is 122.17344008321622
At time: 664.0835275650024 and batch: 300, loss is 4.800040874481201 and perplexity is 121.51538429551803
At time: 665.2575590610504 and batch: 350, loss is 4.798458738327026 and perplexity is 121.32328241846983
At time: 666.4312641620636 and batch: 400, loss is 4.78366491317749 and perplexity is 119.54165800516445
At time: 667.606338262558 and batch: 450, loss is 4.730407304763794 and perplexity is 113.34171756990388
At time: 668.7813711166382 and batch: 500, loss is 4.746846618652344 and perplexity is 115.22037727100656
At time: 669.9542737007141 and batch: 550, loss is 4.731720199584961 and perplexity is 113.49062104983875
At time: 671.1305904388428 and batch: 600, loss is 4.7504328346252445 and perplexity is 115.63432423634329
At time: 672.3075315952301 and batch: 650, loss is 4.763578310012817 and perplexity is 117.16442733197239
At time: 673.4821166992188 and batch: 700, loss is 4.7785525894165035 and perplexity is 118.93208184949818
At time: 674.6562774181366 and batch: 750, loss is 4.763033065795899 and perplexity is 117.10056151835606
At time: 675.8307423591614 and batch: 800, loss is 4.787796621322632 and perplexity is 120.03659100249257
At time: 677.0437459945679 and batch: 850, loss is 4.838049583435058 and perplexity is 126.22292420782378
At time: 678.2108790874481 and batch: 900, loss is 4.79958475112915 and perplexity is 121.45997092973319
At time: 679.3789386749268 and batch: 950, loss is 4.7972039699554445 and perplexity is 121.17114526935703
At time: 680.5478754043579 and batch: 1000, loss is 4.787974443435669 and perplexity is 120.05793806067965
At time: 681.7100014686584 and batch: 1050, loss is 4.802142210006714 and perplexity is 121.7709973597836
At time: 682.8722121715546 and batch: 1100, loss is 4.756385927200317 and perplexity is 116.32475914580115
At time: 684.0353894233704 and batch: 1150, loss is 4.782170209884644 and perplexity is 119.36311216508071
At time: 685.1979672908783 and batch: 1200, loss is 4.808598756790161 and perplexity is 122.559761106021
At time: 686.3630559444427 and batch: 1250, loss is 4.831326112747193 and perplexity is 125.37711465089434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.895980055314781 and perplexity of 133.75102580473018
Finished 22 epochs...
Completing Train Step...
At time: 689.3437621593475 and batch: 50, loss is 4.777544689178467 and perplexity is 118.81227056494565
At time: 690.5459749698639 and batch: 100, loss is 4.770574169158936 and perplexity is 117.98696699360157
At time: 691.726110458374 and batch: 150, loss is 4.707161102294922 and perplexity is 110.73734126419423
At time: 692.901453256607 and batch: 200, loss is 4.774137353897094 and perplexity is 118.40812624220187
At time: 694.0787570476532 and batch: 250, loss is 4.794954433441162 and perplexity is 120.89887271194368
At time: 695.2625257968903 and batch: 300, loss is 4.789191474914551 and perplexity is 120.20414129951524
At time: 696.4364085197449 and batch: 350, loss is 4.786566457748413 and perplexity is 119.88901714926762
At time: 697.6107547283173 and batch: 400, loss is 4.774009819030762 and perplexity is 118.39302604057046
At time: 698.7852199077606 and batch: 450, loss is 4.716923694610596 and perplexity is 111.82371908457887
At time: 699.9719429016113 and batch: 500, loss is 4.736419677734375 and perplexity is 114.02522293466998
At time: 701.1603004932404 and batch: 550, loss is 4.720236730575562 and perplexity is 112.19480946610206
At time: 702.3404672145844 and batch: 600, loss is 4.742500047683716 and perplexity is 114.72065055973822
At time: 703.5259408950806 and batch: 650, loss is 4.753877296447754 and perplexity is 116.03330900096836
At time: 704.7081158161163 and batch: 700, loss is 4.77022765159607 and perplexity is 117.94608952013928
At time: 705.8888545036316 and batch: 750, loss is 4.750868225097657 and perplexity is 115.68468128109286
At time: 707.0941302776337 and batch: 800, loss is 4.777861385345459 and perplexity is 118.84990391447946
At time: 708.2708642482758 and batch: 850, loss is 4.827722978591919 and perplexity is 124.9261769691485
At time: 709.4478125572205 and batch: 900, loss is 4.787195167541504 and perplexity is 119.96441624802567
At time: 710.6266334056854 and batch: 950, loss is 4.786035814285278 and perplexity is 119.82541570235416
At time: 711.8159596920013 and batch: 1000, loss is 4.77949821472168 and perplexity is 119.04460022742053
At time: 712.9962577819824 and batch: 1050, loss is 4.789398927688598 and perplexity is 120.22908056885065
At time: 714.1854727268219 and batch: 1100, loss is 4.746098794937134 and perplexity is 115.1342449503179
At time: 715.368524312973 and batch: 1150, loss is 4.7710741424560545 and perplexity is 118.04597207576002
At time: 716.5479669570923 and batch: 1200, loss is 4.796912012100219 and perplexity is 121.13577356544367
At time: 717.7286598682404 and batch: 1250, loss is 4.819009399414062 and perplexity is 123.8423516910051
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.900403544850593 and perplexity of 134.3439825699266
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 720.7361633777618 and batch: 50, loss is 4.777488508224487 and perplexity is 118.80559576574088
At time: 721.9656872749329 and batch: 100, loss is 4.767833614349366 and perplexity is 117.6640599181133
At time: 723.1457674503326 and batch: 150, loss is 4.692182703018188 and perplexity is 109.09103345903095
At time: 724.3236463069916 and batch: 200, loss is 4.756686191558838 and perplexity is 116.35969256935473
At time: 725.5007658004761 and batch: 250, loss is 4.775073032379151 and perplexity is 118.51897002701256
At time: 726.675163269043 and batch: 300, loss is 4.765721492767334 and perplexity is 117.41580138612372
At time: 727.8526654243469 and batch: 350, loss is 4.758049764633179 and perplexity is 116.5184657378886
At time: 729.0279479026794 and batch: 400, loss is 4.736122207641602 and perplexity is 113.99130888548287
At time: 730.2019486427307 and batch: 450, loss is 4.681994953155518 and perplexity is 107.98528341414674
At time: 731.3827424049377 and batch: 500, loss is 4.689971313476563 and perplexity is 108.85005723291034
At time: 732.5592257976532 and batch: 550, loss is 4.677436075210571 and perplexity is 107.49411213285444
At time: 733.7353484630585 and batch: 600, loss is 4.693406705856323 and perplexity is 109.2246429461146
At time: 734.9140477180481 and batch: 650, loss is 4.705899019241333 and perplexity is 110.59766969948228
At time: 736.0895426273346 and batch: 700, loss is 4.711540155410766 and perplexity is 111.22332927053134
At time: 737.2914905548096 and batch: 750, loss is 4.6908254718780515 and perplexity is 108.94307214287656
At time: 738.4753122329712 and batch: 800, loss is 4.70176661491394 and perplexity is 110.141578436418
At time: 739.6557211875916 and batch: 850, loss is 4.751256484985351 and perplexity is 115.7296057230701
At time: 740.8332762718201 and batch: 900, loss is 4.706000547409058 and perplexity is 110.60889904827953
At time: 742.0075397491455 and batch: 950, loss is 4.693209466934204 and perplexity is 109.20310171972501
At time: 743.1818680763245 and batch: 1000, loss is 4.688209133148193 and perplexity is 108.65841270901014
At time: 744.3561134338379 and batch: 1050, loss is 4.6839830112457275 and perplexity is 108.20017797111444
At time: 745.5300369262695 and batch: 1100, loss is 4.629222850799561 and perplexity is 102.43442633676051
At time: 746.7060751914978 and batch: 1150, loss is 4.655626544952392 and perplexity is 105.17509628521216
At time: 747.8834805488586 and batch: 1200, loss is 4.691044387817382 and perplexity is 108.96692412854298
At time: 749.0613181591034 and batch: 1250, loss is 4.735220785140991 and perplexity is 113.88860085339975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.841002526944571 and perplexity of 126.5962042387911
Finished 24 epochs...
Completing Train Step...
At time: 752.0384964942932 and batch: 50, loss is 4.725302133560181 and perplexity is 112.7645631866863
At time: 753.2132298946381 and batch: 100, loss is 4.7190484523773195 and perplexity is 112.0615699985127
At time: 754.3918967247009 and batch: 150, loss is 4.6486351108551025 and perplexity is 104.44233603812978
At time: 755.5711193084717 and batch: 200, loss is 4.71308048248291 and perplexity is 111.39478158808477
At time: 756.7479045391083 and batch: 250, loss is 4.736462106704712 and perplexity is 114.03006101010813
At time: 757.9236505031586 and batch: 300, loss is 4.731469268798828 and perplexity is 113.4621463318211
At time: 759.1003012657166 and batch: 350, loss is 4.723230791091919 and perplexity is 112.53123089700838
At time: 760.2741470336914 and batch: 400, loss is 4.707793445587158 and perplexity is 110.8073874234181
At time: 761.4414842128754 and batch: 450, loss is 4.652210330963134 and perplexity is 104.81640867561502
At time: 762.6097779273987 and batch: 500, loss is 4.662073993682862 and perplexity is 105.8553980750346
At time: 763.7791471481323 and batch: 550, loss is 4.651078701019287 and perplexity is 104.69786237687258
At time: 764.9496097564697 and batch: 600, loss is 4.670222244262695 and perplexity is 106.72145802877897
At time: 766.1109008789062 and batch: 650, loss is 4.686495428085327 and perplexity is 108.47236369912493
At time: 767.298180103302 and batch: 700, loss is 4.693169450759887 and perplexity is 109.19873191680269
At time: 768.4639003276825 and batch: 750, loss is 4.676837177276611 and perplexity is 107.42975340526336
At time: 769.823525428772 and batch: 800, loss is 4.693552513122558 and perplexity is 109.24056985380929
At time: 770.994567155838 and batch: 850, loss is 4.742510776519776 and perplexity is 114.72188138539337
At time: 772.1704487800598 and batch: 900, loss is 4.697281541824341 and perplexity is 109.64869155001959
At time: 773.3505902290344 and batch: 950, loss is 4.6871430206298825 and perplexity is 108.54263234341612
At time: 774.5309615135193 and batch: 1000, loss is 4.684854774475098 and perplexity is 108.29454403413662
At time: 775.7058348655701 and batch: 1050, loss is 4.684901695251465 and perplexity is 108.29962541742935
At time: 776.8813035488129 and batch: 1100, loss is 4.634411582946777 and perplexity is 102.9673124437415
At time: 778.0578327178955 and batch: 1150, loss is 4.664961996078492 and perplexity is 106.16155059007389
At time: 779.2408256530762 and batch: 1200, loss is 4.701061124801636 and perplexity is 110.06390204506226
At time: 780.4179153442383 and batch: 1250, loss is 4.7386987590789795 and perplexity is 114.28539205375318
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.836729174982892 and perplexity of 126.05636837690498
Finished 25 epochs...
Completing Train Step...
At time: 783.4017403125763 and batch: 50, loss is 4.712157173156738 and perplexity is 111.29197721477932
At time: 784.5645086765289 and batch: 100, loss is 4.705788726806641 and perplexity is 110.58547228587346
At time: 785.7257943153381 and batch: 150, loss is 4.636215925216675 and perplexity is 103.15326843167556
At time: 786.8939723968506 and batch: 200, loss is 4.698308210372925 and perplexity is 109.76132222031761
At time: 788.0758764743805 and batch: 250, loss is 4.722977981567383 and perplexity is 112.50278552581175
At time: 789.2529602050781 and batch: 300, loss is 4.717309103012085 and perplexity is 111.86682519159415
At time: 790.4330368041992 and batch: 350, loss is 4.710021505355835 and perplexity is 111.05454814759408
At time: 791.6148700714111 and batch: 400, loss is 4.6954160499572755 and perplexity is 109.44433348112123
At time: 792.802316904068 and batch: 450, loss is 4.64041506767273 and perplexity is 103.58733441526175
At time: 793.981969833374 and batch: 500, loss is 4.651552648544311 and perplexity is 104.7474954304238
At time: 795.1592216491699 and batch: 550, loss is 4.642187471389771 and perplexity is 103.77109579338665
At time: 796.3631591796875 and batch: 600, loss is 4.663030052185059 and perplexity is 105.95665042223897
At time: 797.5401542186737 and batch: 650, loss is 4.679194297790527 and perplexity is 107.68327695621946
At time: 798.7164912223816 and batch: 700, loss is 4.687092294692993 and perplexity is 108.53712655634237
At time: 799.8935706615448 and batch: 750, loss is 4.672103033065796 and perplexity is 106.92236742693918
At time: 801.0747420787811 and batch: 800, loss is 4.689772977828979 and perplexity is 108.82847052709694
At time: 802.2562348842621 and batch: 850, loss is 4.738013219833374 and perplexity is 114.20707178117856
At time: 803.4364926815033 and batch: 900, loss is 4.69416184425354 and perplexity is 109.30715381756828
At time: 804.6232676506042 and batch: 950, loss is 4.685354318618774 and perplexity is 108.34865545379711
At time: 805.8100974559784 and batch: 1000, loss is 4.6841154956817626 and perplexity is 108.21451376028534
At time: 806.9890685081482 and batch: 1050, loss is 4.685014686584473 and perplexity is 108.3118630278284
At time: 808.1702201366425 and batch: 1100, loss is 4.635633907318115 and perplexity is 103.09324885108275
At time: 809.356522321701 and batch: 1150, loss is 4.667247514724732 and perplexity is 106.40446227731722
At time: 810.536642074585 and batch: 1200, loss is 4.702382574081421 and perplexity is 110.20944204983843
At time: 811.7164192199707 and batch: 1250, loss is 4.736654758453369 and perplexity is 114.05203121699262
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.834404938412408 and perplexity of 125.76372377496466
Finished 26 epochs...
Completing Train Step...
At time: 814.6790328025818 and batch: 50, loss is 4.70347430229187 and perplexity is 110.32982650837994
At time: 815.8849451541901 and batch: 100, loss is 4.697250585556031 and perplexity is 109.64529728824118
At time: 817.0618879795074 and batch: 150, loss is 4.627557783126831 and perplexity is 102.26400800329453
At time: 818.2378869056702 and batch: 200, loss is 4.689801473617553 and perplexity is 108.83157172436924
At time: 819.4403104782104 and batch: 250, loss is 4.714435720443726 and perplexity is 111.54585036868085
At time: 820.6131825447083 and batch: 300, loss is 4.709484539031982 and perplexity is 110.99493160259912
At time: 821.7919275760651 and batch: 350, loss is 4.701938848495484 and perplexity is 110.16055014868505
At time: 822.9709875583649 and batch: 400, loss is 4.687626371383667 and perplexity is 108.59510918791653
At time: 824.1470324993134 and batch: 450, loss is 4.633303937911987 and perplexity is 102.85332435219344
At time: 825.3234827518463 and batch: 500, loss is 4.645265312194824 and perplexity is 104.09097872825588
At time: 826.4993603229523 and batch: 550, loss is 4.635818195343018 and perplexity is 103.11224945303223
At time: 827.6786377429962 and batch: 600, loss is 4.658393964767456 and perplexity is 105.46656305015426
At time: 828.8575518131256 and batch: 650, loss is 4.674399681091309 and perplexity is 107.16821267291166
At time: 830.0346431732178 and batch: 700, loss is 4.683802394866944 and perplexity is 108.18063701154733
At time: 831.2175884246826 and batch: 750, loss is 4.669348344802857 and perplexity is 106.62823494398904
At time: 832.3948514461517 and batch: 800, loss is 4.6871618556976316 and perplexity is 108.54467677050337
At time: 833.5798492431641 and batch: 850, loss is 4.734478321075439 and perplexity is 113.80407404273284
At time: 834.7588775157928 and batch: 900, loss is 4.691025648117066 and perplexity is 108.96488214017354
At time: 835.9378490447998 and batch: 950, loss is 4.6834228992462155 and perplexity is 108.13959072248996
At time: 837.1164827346802 and batch: 1000, loss is 4.682615814208984 and perplexity is 108.05234808773378
At time: 838.2995178699493 and batch: 1050, loss is 4.68426516532898 and perplexity is 108.23071140050094
At time: 839.4869239330292 and batch: 1100, loss is 4.635302639007568 and perplexity is 103.05910298074168
At time: 840.652251958847 and batch: 1150, loss is 4.6671645450592045 and perplexity is 106.39563430090364
At time: 841.822957277298 and batch: 1200, loss is 4.70231276512146 and perplexity is 110.20174871184612
At time: 842.9952874183655 and batch: 1250, loss is 4.734181098937988 and perplexity is 113.77025397887999
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.832250441947993 and perplexity of 125.49305795571438
Finished 27 epochs...
Completing Train Step...
At time: 845.92995262146 and batch: 50, loss is 4.696363077163697 and perplexity is 109.54802933616439
At time: 847.1276385784149 and batch: 100, loss is 4.689136352539062 and perplexity is 108.75920961945836
At time: 848.2945287227631 and batch: 150, loss is 4.619974308013916 and perplexity is 101.49142457978121
At time: 849.4855287075043 and batch: 200, loss is 4.682363777160645 and perplexity is 108.02511832445434
At time: 850.6579442024231 and batch: 250, loss is 4.707060279846192 and perplexity is 110.72617701709493
At time: 851.8402981758118 and batch: 300, loss is 4.701944313049316 and perplexity is 110.16115212858632
At time: 853.0231688022614 and batch: 350, loss is 4.6954238224029545 and perplexity is 109.44518413456393
At time: 854.2140712738037 and batch: 400, loss is 4.681193323135376 and perplexity is 107.89875385620779
At time: 855.401624917984 and batch: 450, loss is 4.626919870376587 and perplexity is 102.19879329155769
At time: 856.5835387706757 and batch: 500, loss is 4.63873701095581 and perplexity is 103.41365475587298
At time: 857.7619318962097 and batch: 550, loss is 4.6315862083435055 and perplexity is 102.67680180813765
At time: 858.9523928165436 and batch: 600, loss is 4.655029802322388 and perplexity is 105.11235254443832
At time: 860.1373615264893 and batch: 650, loss is 4.670202932357788 and perplexity is 106.71939705403071
At time: 861.3288335800171 and batch: 700, loss is 4.679910383224487 and perplexity is 107.76041499774664
At time: 862.5131487846375 and batch: 750, loss is 4.66612154006958 and perplexity is 106.28472097508447
At time: 863.7016515731812 and batch: 800, loss is 4.684869956970215 and perplexity is 108.29618822800408
At time: 864.887229681015 and batch: 850, loss is 4.73077615737915 and perplexity is 113.3835316700045
At time: 866.069766998291 and batch: 900, loss is 4.687434940338135 and perplexity is 108.57432270227788
At time: 867.2481815814972 and batch: 950, loss is 4.680327377319336 and perplexity is 107.80535982467158
At time: 868.4269485473633 and batch: 1000, loss is 4.680113821029663 and perplexity is 107.7823397701468
At time: 869.603978395462 and batch: 1050, loss is 4.681529817581176 and perplexity is 107.93506729688212
At time: 870.7805948257446 and batch: 1100, loss is 4.633726434707642 and perplexity is 102.896788733289
At time: 871.9584879875183 and batch: 1150, loss is 4.665392246246338 and perplexity is 106.20723644250215
At time: 873.1392793655396 and batch: 1200, loss is 4.699500589370728 and perplexity is 109.89227737427612
At time: 874.3192007541656 and batch: 1250, loss is 4.730571117401123 and perplexity is 113.36028589640011
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.830260784956661 and perplexity of 125.24361804780631
Finished 28 epochs...
Completing Train Step...
At time: 877.3039939403534 and batch: 50, loss is 4.689392757415772 and perplexity is 108.78709958660083
At time: 878.481938123703 and batch: 100, loss is 4.681291294097901 and perplexity is 107.90932531881805
At time: 879.6870379447937 and batch: 150, loss is 4.612942209243775 and perplexity is 100.78023038246882
At time: 880.8657100200653 and batch: 200, loss is 4.674767389297485 and perplexity is 107.20762655011167
At time: 882.0434861183167 and batch: 250, loss is 4.69973729133606 and perplexity is 109.91829217106074
At time: 883.2149829864502 and batch: 300, loss is 4.6959946346282955 and perplexity is 109.50767461714157
At time: 884.3889882564545 and batch: 350, loss is 4.689133796691895 and perplexity is 108.75893164789572
At time: 885.5636312961578 and batch: 400, loss is 4.675126800537109 and perplexity is 107.24616510124658
At time: 886.738710641861 and batch: 450, loss is 4.620811052322388 and perplexity is 101.57638249072443
At time: 887.9125709533691 and batch: 500, loss is 4.633877363204956 and perplexity is 102.91231996301408
At time: 889.0867793560028 and batch: 550, loss is 4.627025756835938 and perplexity is 102.20961533287297
At time: 890.2543749809265 and batch: 600, loss is 4.648811092376709 and perplexity is 104.46071757670406
At time: 891.4468269348145 and batch: 650, loss is 4.665553102493286 and perplexity is 106.22432191407061
At time: 892.6230812072754 and batch: 700, loss is 4.675608243942261 and perplexity is 107.29781049133102
At time: 893.7966208457947 and batch: 750, loss is 4.6612623500823975 and perplexity is 105.7695160761145
At time: 894.9722909927368 and batch: 800, loss is 4.6810165309906 and perplexity is 107.87967989020834
At time: 896.1487109661102 and batch: 850, loss is 4.725831756591797 and perplexity is 112.8243017145538
At time: 897.3263568878174 and batch: 900, loss is 4.6827389430999755 and perplexity is 108.06565327263236
At time: 898.5071597099304 and batch: 950, loss is 4.676315202713012 and perplexity is 107.37369243908415
At time: 899.6891915798187 and batch: 1000, loss is 4.67553900718689 and perplexity is 107.29038179624655
At time: 900.8693826198578 and batch: 1050, loss is 4.678179302215576 and perplexity is 107.57403435636901
At time: 902.0451874732971 and batch: 1100, loss is 4.629519109725952 and perplexity is 102.46477794567807
At time: 903.2267792224884 and batch: 1150, loss is 4.661117343902588 and perplexity is 105.75417995459108
At time: 904.4057703018188 and batch: 1200, loss is 4.695088214874268 and perplexity is 109.40845966963802
At time: 905.5829815864563 and batch: 1250, loss is 4.7250523853302 and perplexity is 112.73640395313166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.8252808508211675 and perplexity of 124.62146350912201
Finished 29 epochs...
Completing Train Step...
At time: 908.610543012619 and batch: 50, loss is 4.681424808502197 and perplexity is 107.92373372994987
At time: 909.7959470748901 and batch: 100, loss is 4.67409652709961 and perplexity is 107.13572912546393
At time: 910.9809775352478 and batch: 150, loss is 4.604743156433106 and perplexity is 99.95730616091576
At time: 912.1588995456696 and batch: 200, loss is 4.666152896881104 and perplexity is 106.28805377730073
At time: 913.3364791870117 and batch: 250, loss is 4.691258239746094 and perplexity is 108.99022940728418
At time: 914.5137984752655 and batch: 300, loss is 4.687896728515625 and perplexity is 108.62447261930897
At time: 915.6941466331482 and batch: 350, loss is 4.681239948272705 and perplexity is 107.9037847677066
At time: 916.8810377120972 and batch: 400, loss is 4.667577743530273 and perplexity is 106.43960589819795
At time: 918.0598001480103 and batch: 450, loss is 4.612397909164429 and perplexity is 100.7253906210723
At time: 919.2380139827728 and batch: 500, loss is 4.625407381057739 and perplexity is 102.04433554559569
At time: 920.4155509471893 and batch: 550, loss is 4.618775253295898 and perplexity is 101.36980373789417
At time: 921.5880806446075 and batch: 600, loss is 4.640077705383301 and perplexity is 103.55239384911368
At time: 922.7603588104248 and batch: 650, loss is 4.658431453704834 and perplexity is 105.47051695364523
At time: 923.9315729141235 and batch: 700, loss is 4.668738021850586 and perplexity is 106.56317713999753
At time: 925.1008245944977 and batch: 750, loss is 4.654868450164795 and perplexity is 105.09539380776675
At time: 926.2733161449432 and batch: 800, loss is 4.673813238143921 and perplexity is 107.1053830551992
At time: 927.4353730678558 and batch: 850, loss is 4.718437547683716 and perplexity is 111.99313196611406
At time: 928.5980014801025 and batch: 900, loss is 4.6752766990661625 and perplexity is 107.26224234858962
At time: 929.7598192691803 and batch: 950, loss is 4.667469310760498 and perplexity is 106.42806498263462
At time: 930.9231719970703 and batch: 1000, loss is 4.6666024303436275 and perplexity is 106.33584455511226
At time: 932.102456331253 and batch: 1050, loss is 4.669216375350953 and perplexity is 106.6141642027404
At time: 933.2818686962128 and batch: 1100, loss is 4.6219509506225585 and perplexity is 101.69223525410248
At time: 934.4596042633057 and batch: 1150, loss is 4.654567422866822 and perplexity is 105.06376198659908
At time: 935.6378746032715 and batch: 1200, loss is 4.686748666763306 and perplexity is 108.49983657555593
At time: 936.8149173259735 and batch: 1250, loss is 4.71528115272522 and perplexity is 111.64019470670428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.816739047530794 and perplexity of 123.56150490576425
Finished 30 epochs...
Completing Train Step...
At time: 939.8208174705505 and batch: 50, loss is 4.671624794006347 and perplexity is 106.87124519980414
At time: 941.0189833641052 and batch: 100, loss is 4.664102039337158 and perplexity is 106.07029549231558
At time: 942.1954381465912 and batch: 150, loss is 4.592546482086181 and perplexity is 98.74556408224903
At time: 943.3719472885132 and batch: 200, loss is 4.655376253128051 and perplexity is 105.14877511261226
At time: 944.5476834774017 and batch: 250, loss is 4.681381301879883 and perplexity is 107.91903843496667
At time: 945.7279527187347 and batch: 300, loss is 4.67729284286499 and perplexity is 107.4787166016328
At time: 946.9068248271942 and batch: 350, loss is 4.670899066925049 and perplexity is 106.79371397960648
At time: 948.086517572403 and batch: 400, loss is 4.65878321647644 and perplexity is 105.50762408108194
At time: 949.2616314888 and batch: 450, loss is 4.602224922180175 and perplexity is 99.70590692265128
At time: 950.4371573925018 and batch: 500, loss is 4.61567530632019 and perplexity is 101.05604928371082
At time: 951.6122453212738 and batch: 550, loss is 4.609666509628296 and perplexity is 100.4506447270696
At time: 952.787344455719 and batch: 600, loss is 4.632314720153809 and perplexity is 102.75163032431846
At time: 953.9633967876434 and batch: 650, loss is 4.651244888305664 and perplexity is 104.71526327637453
At time: 955.1394612789154 and batch: 700, loss is 4.660764274597168 and perplexity is 105.71684799050232
At time: 956.3186972141266 and batch: 750, loss is 4.6470175457000735 and perplexity is 104.27353031857656
At time: 957.4998950958252 and batch: 800, loss is 4.667777090072632 and perplexity is 106.4608263806484
At time: 958.6870901584625 and batch: 850, loss is 4.710515928268433 and perplexity is 111.109469636849
At time: 959.8646891117096 and batch: 900, loss is 4.667611207962036 and perplexity is 106.4431678987262
At time: 961.039128780365 and batch: 950, loss is 4.659786520004272 and perplexity is 105.6135333732347
At time: 962.219484090805 and batch: 1000, loss is 4.659089088439941 and perplexity is 105.53990084125034
At time: 963.3929078578949 and batch: 1050, loss is 4.663371305465699 and perplexity is 105.99281464703037
At time: 964.564122915268 and batch: 1100, loss is 4.616321430206299 and perplexity is 101.1213651097742
At time: 965.7380051612854 and batch: 1150, loss is 4.648787364959717 and perplexity is 104.45823902310379
At time: 966.9122169017792 and batch: 1200, loss is 4.680029344558716 and perplexity is 107.77323508302392
At time: 968.1224806308746 and batch: 1250, loss is 4.709101543426514 and perplexity is 110.95242917120783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.811682485315922 and perplexity of 122.93828547068355
Finished 31 epochs...
Completing Train Step...
At time: 971.0877649784088 and batch: 50, loss is 4.6644538879394535 and perplexity is 106.10762274391517
At time: 972.2556993961334 and batch: 100, loss is 4.655946540832519 and perplexity is 105.20875726811607
At time: 973.4287326335907 and batch: 150, loss is 4.584633636474609 and perplexity is 97.96728892526797
At time: 974.6034708023071 and batch: 200, loss is 4.647802104949951 and perplexity is 104.3553711816105
At time: 975.7827935218811 and batch: 250, loss is 4.67526876449585 and perplexity is 107.26139127216227
At time: 976.9605433940887 and batch: 300, loss is 4.67151891708374 and perplexity is 106.85993060023527
At time: 978.1398384571075 and batch: 350, loss is 4.66491883277893 and perplexity is 106.15696840615568
At time: 979.3115289211273 and batch: 400, loss is 4.654033346176147 and perplexity is 105.00766486170431
At time: 980.4922943115234 and batch: 450, loss is 4.595481605529785 and perplexity is 99.03582026282032
At time: 981.6696281433105 and batch: 500, loss is 4.610007467269898 and perplexity is 100.48489998145666
At time: 982.847464799881 and batch: 550, loss is 4.6042757415771485 and perplexity is 99.91059554852221
At time: 984.0271203517914 and batch: 600, loss is 4.6277867603302 and perplexity is 102.28742681093664
At time: 985.2175316810608 and batch: 650, loss is 4.645431756973267 and perplexity is 104.10830557008934
At time: 986.3873519897461 and batch: 700, loss is 4.655747632980347 and perplexity is 105.18783250129736
At time: 987.5464866161346 and batch: 750, loss is 4.642603750228882 and perplexity is 103.81430249707113
At time: 988.7071056365967 and batch: 800, loss is 4.663264617919922 and perplexity is 105.98150713696171
At time: 989.8687760829926 and batch: 850, loss is 4.705473299026489 and perplexity is 110.5505960565883
At time: 991.0284481048584 and batch: 900, loss is 4.664831304550171 and perplexity is 106.14767708137319
At time: 992.1897463798523 and batch: 950, loss is 4.655548839569092 and perplexity is 105.16692393156372
At time: 993.3619205951691 and batch: 1000, loss is 4.654775075912475 and perplexity is 105.08558106208376
At time: 994.5378332138062 and batch: 1050, loss is 4.659878797531128 and perplexity is 105.62327957856787
At time: 995.7128772735596 and batch: 1100, loss is 4.612318773269653 and perplexity is 100.7174199425464
At time: 996.8786396980286 and batch: 1150, loss is 4.644460420608521 and perplexity is 104.00723048391744
At time: 998.1084403991699 and batch: 1200, loss is 4.675597448348999 and perplexity is 107.29665215406352
At time: 999.2897844314575 and batch: 1250, loss is 4.704122257232666 and perplexity is 110.40133843033284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.809767228843522 and perplexity of 122.70305246140877
Finished 32 epochs...
Completing Train Step...
At time: 1002.2970108985901 and batch: 50, loss is 4.657696533203125 and perplexity is 105.39303298417647
At time: 1003.4989061355591 and batch: 100, loss is 4.649602851867676 and perplexity is 104.54345809225045
At time: 1004.66650223732 and batch: 150, loss is 4.579474611282349 and perplexity is 97.46317470078256
At time: 1005.8487164974213 and batch: 200, loss is 4.642888307571411 and perplexity is 103.8438478225765
At time: 1007.0360686779022 and batch: 250, loss is 4.670773372650147 and perplexity is 106.78029146474802
At time: 1008.2153315544128 and batch: 300, loss is 4.666922702789306 and perplexity is 106.36990645036333
At time: 1009.387065410614 and batch: 350, loss is 4.661066017150879 and perplexity is 105.74875207535324
At time: 1010.5641303062439 and batch: 400, loss is 4.650004320144653 and perplexity is 104.5854374003564
At time: 1011.7469480037689 and batch: 450, loss is 4.590330266952515 and perplexity is 98.52696498952906
At time: 1012.9269099235535 and batch: 500, loss is 4.605248460769653 and perplexity is 100.00782778451124
At time: 1014.1069056987762 and batch: 550, loss is 4.6000604152679445 and perplexity is 99.49032619508236
At time: 1015.2861409187317 and batch: 600, loss is 4.623831644058227 and perplexity is 101.88366712931423
At time: 1016.4637372493744 and batch: 650, loss is 4.641338109970093 and perplexity is 103.68299404855009
At time: 1017.6406991481781 and batch: 700, loss is 4.652015638351441 and perplexity is 104.79600368167684
At time: 1018.8171374797821 and batch: 750, loss is 4.6393857765197755 and perplexity is 103.48076774186549
At time: 1019.9970362186432 and batch: 800, loss is 4.660608692169189 and perplexity is 105.7004015860327
At time: 1021.1783878803253 and batch: 850, loss is 4.702680168151855 and perplexity is 110.24224460697951
At time: 1022.361270904541 and batch: 900, loss is 4.662449884414673 and perplexity is 105.89519561737823
At time: 1023.5293188095093 and batch: 950, loss is 4.651594524383545 and perplexity is 104.7518819115457
At time: 1024.6979491710663 and batch: 1000, loss is 4.651085901260376 and perplexity is 104.69861622943712
At time: 1025.870062828064 and batch: 1050, loss is 4.657137727737426 and perplexity is 105.33415523343496
At time: 1027.0393579006195 and batch: 1100, loss is 4.609010972976685 and perplexity is 100.38481722630007
At time: 1028.2629499435425 and batch: 1150, loss is 4.641021013259888 and perplexity is 103.65012172436144
At time: 1029.4406039714813 and batch: 1200, loss is 4.671242513656616 and perplexity is 106.8303982308072
At time: 1030.6040143966675 and batch: 1250, loss is 4.6999634265899655 and perplexity is 109.94315138263583
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.808940803917655 and perplexity of 122.6016894905866
Finished 33 epochs...
Completing Train Step...
At time: 1033.5159964561462 and batch: 50, loss is 4.652439174652099 and perplexity is 104.84039799403776
At time: 1034.6808607578278 and batch: 100, loss is 4.64446439743042 and perplexity is 104.00764410297175
At time: 1035.8576066493988 and batch: 150, loss is 4.574877710342407 and perplexity is 97.01617433673633
At time: 1037.0371911525726 and batch: 200, loss is 4.6387356090545655 and perplexity is 103.41350978024326
At time: 1038.2233901023865 and batch: 250, loss is 4.666580991744995 and perplexity is 106.33356488805714
At time: 1039.4056072235107 and batch: 300, loss is 4.6632687282562255 and perplexity is 105.98194275749324
At time: 1040.5839145183563 and batch: 350, loss is 4.657198810577393 and perplexity is 105.34058953929292
At time: 1041.7582199573517 and batch: 400, loss is 4.646643238067627 and perplexity is 104.2345072440891
At time: 1042.937302827835 and batch: 450, loss is 4.586288690567017 and perplexity is 98.12956433797662
At time: 1044.1125600337982 and batch: 500, loss is 4.601006021499634 and perplexity is 99.58444936223843
At time: 1045.2936239242554 and batch: 550, loss is 4.596762371063233 and perplexity is 99.16274318989493
At time: 1046.472799539566 and batch: 600, loss is 4.621058359146118 and perplexity is 101.60150612973982
At time: 1047.653476715088 and batch: 650, loss is 4.639104251861572 and perplexity is 103.45163945445431
At time: 1048.829513311386 and batch: 700, loss is 4.648675165176392 and perplexity is 104.44651948879572
At time: 1050.0058770179749 and batch: 750, loss is 4.636647310256958 and perplexity is 103.1977768079669
At time: 1051.1818933486938 and batch: 800, loss is 4.658573265075684 and perplexity is 105.48547493281936
At time: 1052.3633289337158 and batch: 850, loss is 4.699892292022705 and perplexity is 109.9353309022955
At time: 1053.5408470630646 and batch: 900, loss is 4.6603078937530515 and perplexity is 105.66861185404069
At time: 1054.723156929016 and batch: 950, loss is 4.648769626617431 and perplexity is 104.45638612353916
At time: 1055.9067075252533 and batch: 1000, loss is 4.647265300750733 and perplexity is 104.29936781291575
At time: 1057.0822789669037 and batch: 1050, loss is 4.654285850524903 and perplexity is 105.03418310157909
At time: 1058.3041861057281 and batch: 1100, loss is 4.605997743606568 and perplexity is 100.08279001387609
At time: 1059.4845972061157 and batch: 1150, loss is 4.6376472759246825 and perplexity is 103.30102265430799
At time: 1060.6685869693756 and batch: 1200, loss is 4.667951831817627 and perplexity is 106.4794311566919
At time: 1061.8455600738525 and batch: 1250, loss is 4.6969288635253905 and perplexity is 109.61002765436079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.808644983890283 and perplexity of 122.56542681932231
Finished 34 epochs...
Completing Train Step...
At time: 1064.8053607940674 and batch: 50, loss is 4.648198156356812 and perplexity is 104.39670945868146
At time: 1066.0135593414307 and batch: 100, loss is 4.64039514541626 and perplexity is 103.58527074237512
At time: 1067.1947600841522 and batch: 150, loss is 4.571251745223999 and perplexity is 96.66503406857353
At time: 1068.374165058136 and batch: 200, loss is 4.634753885269165 and perplexity is 103.00256442699474
At time: 1069.550892829895 and batch: 250, loss is 4.662697925567627 and perplexity is 105.92146524163071
At time: 1070.7270846366882 and batch: 300, loss is 4.659543905258179 and perplexity is 105.58791308070741
At time: 1071.903707742691 and batch: 350, loss is 4.653757820129394 and perplexity is 104.97873650036772
At time: 1073.0798332691193 and batch: 400, loss is 4.644413166046142 and perplexity is 104.00231578387861
At time: 1074.2568626403809 and batch: 450, loss is 4.5832670783996585 and perplexity is 97.83350236987523
At time: 1075.436993598938 and batch: 500, loss is 4.597786655426026 and perplexity is 99.26436607359646
At time: 1076.6182656288147 and batch: 550, loss is 4.594730072021484 and perplexity is 98.96141948621091
At time: 1077.7973494529724 and batch: 600, loss is 4.618291616439819 and perplexity is 101.32078941823416
At time: 1078.9753470420837 and batch: 650, loss is 4.635823936462402 and perplexity is 103.11284143446565
At time: 1080.1612842082977 and batch: 700, loss is 4.645142011642456 and perplexity is 104.07814504429855
At time: 1081.33784532547 and batch: 750, loss is 4.6336188220977785 and perplexity is 102.88571633708229
At time: 1082.5241272449493 and batch: 800, loss is 4.656118354797363 and perplexity is 105.22683515481083
At time: 1083.7049317359924 and batch: 850, loss is 4.698177986145019 and perplexity is 109.74702956752256
At time: 1084.8862779140472 and batch: 900, loss is 4.6580562400817875 and perplexity is 105.43095040227323
At time: 1086.0632832050323 and batch: 950, loss is 4.645186967849732 and perplexity is 104.08282410813575
At time: 1087.269702911377 and batch: 1000, loss is 4.644144029617309 and perplexity is 103.97432873835386
At time: 1088.4497365951538 and batch: 1050, loss is 4.651852474212647 and perplexity is 104.77890612687891
At time: 1089.6312878131866 and batch: 1100, loss is 4.603438863754272 and perplexity is 99.82701756399608
At time: 1090.8090603351593 and batch: 1150, loss is 4.634943838119507 and perplexity is 103.02213191609147
At time: 1091.9878220558167 and batch: 1200, loss is 4.665926189422607 and perplexity is 106.26396021395877
At time: 1093.1658148765564 and batch: 1250, loss is 4.6943238162994385 and perplexity is 109.32485995481436
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.808625381358349 and perplexity of 122.5630242501773
Finished 35 epochs...
Completing Train Step...
At time: 1096.1551008224487 and batch: 50, loss is 4.64469129562378 and perplexity is 104.03124592701863
At time: 1097.3589179515839 and batch: 100, loss is 4.63609582901001 and perplexity is 103.14088085929696
At time: 1098.5351316928864 and batch: 150, loss is 4.566794033050537 and perplexity is 96.2350881689547
At time: 1099.714828491211 and batch: 200, loss is 4.631064825057983 and perplexity is 102.623281793297
At time: 1100.8923664093018 and batch: 250, loss is 4.659546451568604 and perplexity is 105.58818194065351
At time: 1102.0682842731476 and batch: 300, loss is 4.656021900177002 and perplexity is 105.21668602984705
At time: 1103.2438535690308 and batch: 350, loss is 4.651359033584595 and perplexity is 104.72721671151066
At time: 1104.4133369922638 and batch: 400, loss is 4.641503372192383 and perplexity is 103.70013034651258
At time: 1105.5763499736786 and batch: 450, loss is 4.579955539703369 and perplexity is 97.51005878453958
At time: 1106.7451803684235 and batch: 500, loss is 4.594107255935669 and perplexity is 98.89980391185698
At time: 1107.9167454242706 and batch: 550, loss is 4.591454238891601 and perplexity is 98.63776879193797
At time: 1109.0908632278442 and batch: 600, loss is 4.615735740661621 and perplexity is 101.06215672404458
At time: 1110.259991645813 and batch: 650, loss is 4.632906913757324 and perplexity is 102.8124972032567
At time: 1111.42214179039 and batch: 700, loss is 4.642474966049194 and perplexity is 103.8009337181463
At time: 1112.5849750041962 and batch: 750, loss is 4.631649608612061 and perplexity is 102.68331175131057
At time: 1113.747211933136 and batch: 800, loss is 4.654112491607666 and perplexity is 105.01597606754522
At time: 1114.920084476471 and batch: 850, loss is 4.696178588867188 and perplexity is 109.52782087101622
At time: 1116.097538948059 and batch: 900, loss is 4.6553413200378415 and perplexity is 105.14510200512267
At time: 1117.303942680359 and batch: 950, loss is 4.6419776248931885 and perplexity is 103.74932207713687
At time: 1118.47989320755 and batch: 1000, loss is 4.641240653991699 and perplexity is 103.67289001327958
At time: 1119.6558544635773 and batch: 1050, loss is 4.6488472366333005 and perplexity is 104.4644932999188
At time: 1120.8307132720947 and batch: 1100, loss is 4.600835523605347 and perplexity is 99.5674718706728
At time: 1122.0087745189667 and batch: 1150, loss is 4.632078666687011 and perplexity is 102.72737830826017
At time: 1123.1867430210114 and batch: 1200, loss is 4.663384294509887 and perplexity is 105.99419140132484
At time: 1124.3624103069305 and batch: 1250, loss is 4.69199800491333 and perplexity is 109.07088641251197
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.806989460966013 and perplexity of 122.36268481382436
Finished 36 epochs...
Completing Train Step...
At time: 1127.3256213665009 and batch: 50, loss is 4.640769195556641 and perplexity is 103.62402407483054
At time: 1128.5059411525726 and batch: 100, loss is 4.632688999176025 and perplexity is 102.79009530191549
At time: 1129.6921753883362 and batch: 150, loss is 4.562750625610351 and perplexity is 95.84675611887026
At time: 1130.8692841529846 and batch: 200, loss is 4.6281373405456545 and perplexity is 102.3232930456951
At time: 1132.0443489551544 and batch: 250, loss is 4.655978870391846 and perplexity is 105.21215867585856
At time: 1133.2196352481842 and batch: 300, loss is 4.652822589874267 and perplexity is 104.88060310566019
At time: 1134.3986999988556 and batch: 350, loss is 4.648696060180664 and perplexity is 104.4487019220676
At time: 1135.579955816269 and batch: 400, loss is 4.637602243423462 and perplexity is 103.29637085562109
At time: 1136.7568266391754 and batch: 450, loss is 4.57693564414978 and perplexity is 97.2160327789535
At time: 1137.942134141922 and batch: 500, loss is 4.590572948455811 and perplexity is 98.5508785630815
At time: 1139.125823020935 and batch: 550, loss is 4.588837375640869 and perplexity is 98.3799846796454
At time: 1140.3098015785217 and batch: 600, loss is 4.612984991073608 and perplexity is 100.7845420373652
At time: 1141.4932572841644 and batch: 650, loss is 4.630518989562988 and perplexity is 102.56728164830517
At time: 1142.6771447658539 and batch: 700, loss is 4.6398743152618405 and perplexity is 103.53133445685997
At time: 1143.850946187973 and batch: 750, loss is 4.628869695663452 and perplexity is 102.39825747997
At time: 1145.0265929698944 and batch: 800, loss is 4.651653137207031 and perplexity is 104.75802189504917
At time: 1146.2017931938171 and batch: 850, loss is 4.694188385009766 and perplexity is 109.31005495059043
At time: 1147.420487165451 and batch: 900, loss is 4.652675476074219 and perplexity is 104.86517485646793
At time: 1148.5959272384644 and batch: 950, loss is 4.63930461883545 and perplexity is 103.47236982316574
At time: 1149.7719259262085 and batch: 1000, loss is 4.638542499542236 and perplexity is 103.39354157588849
At time: 1150.951284646988 and batch: 1050, loss is 4.646417970657349 and perplexity is 104.21102925109322
At time: 1152.1348476409912 and batch: 1100, loss is 4.597987289428711 and perplexity is 99.28428387871351
At time: 1153.3126990795135 and batch: 1150, loss is 4.629324712753296 and perplexity is 102.44486103899744
At time: 1154.489609003067 and batch: 1200, loss is 4.660873975753784 and perplexity is 105.72844588714042
At time: 1155.671638727188 and batch: 1250, loss is 4.689334163665771 and perplexity is 108.78072552922617
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.8050292077725825 and perplexity of 122.12305791159392
Finished 37 epochs...
Completing Train Step...
At time: 1158.6101286411285 and batch: 50, loss is 4.637183074951172 and perplexity is 103.25308134708882
At time: 1159.8135499954224 and batch: 100, loss is 4.628756437301636 and perplexity is 102.38666067780478
At time: 1160.9868347644806 and batch: 150, loss is 4.559529371261597 and perplexity is 95.5385060813411
At time: 1162.1639201641083 and batch: 200, loss is 4.624639568328857 and perplexity is 101.9660146775809
At time: 1163.3387200832367 and batch: 250, loss is 4.652552738189697 and perplexity is 104.85230471658909
At time: 1164.5147223472595 and batch: 300, loss is 4.649464817047119 and perplexity is 104.52902845069168
At time: 1165.6882548332214 and batch: 350, loss is 4.6460311985015865 and perplexity is 104.1707311202554
At time: 1166.8642184734344 and batch: 400, loss is 4.634612522125244 and perplexity is 102.98800468978474
At time: 1168.0416350364685 and batch: 450, loss is 4.573198747634888 and perplexity is 96.85342446171569
At time: 1169.2098724842072 and batch: 500, loss is 4.588390474319458 and perplexity is 98.33602835729248
At time: 1170.376255273819 and batch: 550, loss is 4.586725482940674 and perplexity is 98.17243594562376
At time: 1171.5505487918854 and batch: 600, loss is 4.610427474975586 and perplexity is 100.5271132780885
At time: 1172.7250728607178 and batch: 650, loss is 4.628044595718384 and perplexity is 102.31380352961439
At time: 1173.9017515182495 and batch: 700, loss is 4.637403287887573 and perplexity is 103.27582151507245
At time: 1175.077149629593 and batch: 750, loss is 4.6267380523681645 and perplexity is 102.18021339962891
At time: 1176.282265663147 and batch: 800, loss is 4.649985647201538 and perplexity is 104.58348450066637
At time: 1177.465892314911 and batch: 850, loss is 4.692632074356079 and perplexity is 109.14006685896547
At time: 1178.6413078308105 and batch: 900, loss is 4.6495565700531 and perplexity is 104.53861974327259
At time: 1179.8238761425018 and batch: 950, loss is 4.6364443969726565 and perplexity is 103.17683873252088
At time: 1181.0024354457855 and batch: 1000, loss is 4.635457010269165 and perplexity is 103.07501357252532
At time: 1182.1711122989655 and batch: 1050, loss is 4.643616819381714 and perplexity is 103.91952685533005
At time: 1183.3413388729095 and batch: 1100, loss is 4.59552264213562 and perplexity is 99.03988444012947
At time: 1184.51327085495 and batch: 1150, loss is 4.626576347351074 and perplexity is 102.16369168233312
At time: 1185.6748578548431 and batch: 1200, loss is 4.658285531997681 and perplexity is 105.4551276386019
At time: 1186.835081577301 and batch: 1250, loss is 4.686818437576294 and perplexity is 108.50740696145584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.803973789632756 and perplexity of 121.99423501395246
Finished 38 epochs...
Completing Train Step...
At time: 1189.7568581104279 and batch: 50, loss is 4.633962326049804 and perplexity is 102.92106405794442
At time: 1190.9313390254974 and batch: 100, loss is 4.6258952426910405 and perplexity is 102.09413120751293
At time: 1192.1105246543884 and batch: 150, loss is 4.556856031417847 and perplexity is 95.28344027711762
At time: 1193.2888140678406 and batch: 200, loss is 4.621624116897583 and perplexity is 101.65900423285834
At time: 1194.4650185108185 and batch: 250, loss is 4.6495325469970705 and perplexity is 104.53610843631797
At time: 1195.6406555175781 and batch: 300, loss is 4.646990880966187 and perplexity is 104.27074992970851
At time: 1196.816722869873 and batch: 350, loss is 4.642594308853149 and perplexity is 103.81332235186179
At time: 1197.9988086223602 and batch: 400, loss is 4.630741491317749 and perplexity is 102.59010558754201
At time: 1199.1791501045227 and batch: 450, loss is 4.570464868545532 and perplexity is 96.58900052607558
At time: 1200.3562648296356 and batch: 500, loss is 4.585557088851929 and perplexity is 98.0577988354895
At time: 1201.5318324565887 and batch: 550, loss is 4.583600816726684 and perplexity is 97.8661586082993
At time: 1202.7122721672058 and batch: 600, loss is 4.608304557800293 and perplexity is 100.3139289091715
At time: 1203.8901448249817 and batch: 650, loss is 4.626221933364868 and perplexity is 102.12748985671408
At time: 1205.069030046463 and batch: 700, loss is 4.634968929290771 and perplexity is 103.02471689447731
At time: 1206.2931725978851 and batch: 750, loss is 4.623981313705444 and perplexity is 101.89891716303579
At time: 1207.4710714817047 and batch: 800, loss is 4.647618045806885 and perplexity is 104.33616538897152
At time: 1208.6460809707642 and batch: 850, loss is 4.68971019744873 and perplexity is 108.8216384487978
At time: 1209.8220479488373 and batch: 900, loss is 4.647536897659302 and perplexity is 104.32769904594298
At time: 1211.0016481876373 and batch: 950, loss is 4.63414626121521 and perplexity is 102.93999660201256
At time: 1212.1805896759033 and batch: 1000, loss is 4.632885236740112 and perplexity is 102.81026855914045
At time: 1213.3570103645325 and batch: 1050, loss is 4.6404680824279785 and perplexity is 103.59282621801465
At time: 1214.5319125652313 and batch: 1100, loss is 4.592886142730713 and perplexity is 98.77910976094073
At time: 1215.7069096565247 and batch: 1150, loss is 4.6242194080352785 and perplexity is 101.92318160592706
At time: 1216.8820586204529 and batch: 1200, loss is 4.656167497634888 and perplexity is 105.23200642713854
At time: 1218.060923576355 and batch: 1250, loss is 4.68436336517334 and perplexity is 108.24134016137823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.802565525918111 and perplexity of 121.82255587251217
Finished 39 epochs...
Completing Train Step...
At time: 1221.0279185771942 and batch: 50, loss is 4.630856857299805 and perplexity is 102.60194167855039
At time: 1222.2312443256378 and batch: 100, loss is 4.623428516387939 and perplexity is 101.84260328148676
At time: 1223.4117667675018 and batch: 150, loss is 4.553990659713745 and perplexity is 95.0108085855789
At time: 1224.5935699939728 and batch: 200, loss is 4.618927392959595 and perplexity is 101.38522727898027
At time: 1225.772720336914 and batch: 250, loss is 4.6456809329986575 and perplexity is 104.13425009612418
At time: 1226.9510927200317 and batch: 300, loss is 4.643633689880371 and perplexity is 103.92128004435682
At time: 1228.1282558441162 and batch: 350, loss is 4.638860597610473 and perplexity is 103.42643609329582
At time: 1229.312168121338 and batch: 400, loss is 4.627561960220337 and perplexity is 102.2644351705104
At time: 1230.492502450943 and batch: 450, loss is 4.56632628440857 and perplexity is 96.19008486309217
At time: 1231.6744453907013 and batch: 500, loss is 4.581542282104492 and perplexity is 97.66490494734644
At time: 1232.856116771698 and batch: 550, loss is 4.581223697662353 and perplexity is 97.63379538386181
At time: 1234.0423374176025 and batch: 600, loss is 4.605800466537476 and perplexity is 100.06304792179066
At time: 1235.2246181964874 and batch: 650, loss is 4.623727788925171 and perplexity is 101.8730865369422
At time: 1236.4336004257202 and batch: 700, loss is 4.632223653793335 and perplexity is 102.74227353336298
At time: 1237.6104035377502 and batch: 750, loss is 4.621072235107422 and perplexity is 101.60291595808869
At time: 1238.7886455059052 and batch: 800, loss is 4.645265779495239 and perplexity is 104.09102737002482
At time: 1239.9668695926666 and batch: 850, loss is 4.687650442123413 and perplexity is 108.59772318398781
At time: 1241.141457080841 and batch: 900, loss is 4.644727592468262 and perplexity is 104.0350220015027
At time: 1242.319765329361 and batch: 950, loss is 4.631116495132447 and perplexity is 102.62858448290295
At time: 1243.4993968009949 and batch: 1000, loss is 4.629640045166016 and perplexity is 102.47717031801338
At time: 1244.6751987934113 and batch: 1050, loss is 4.638181924819946 and perplexity is 103.35626719885111
At time: 1245.8525249958038 and batch: 1100, loss is 4.590185632705689 and perplexity is 98.51271564665204
At time: 1247.029037952423 and batch: 1150, loss is 4.621221742630005 and perplexity is 101.61810749393689
At time: 1248.2078256607056 and batch: 1200, loss is 4.653685150146484 and perplexity is 104.97110797456607
At time: 1249.386634349823 and batch: 1250, loss is 4.681748714447021 and perplexity is 107.95869653091646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.801511444314553 and perplexity of 121.69421261148706
Finished 40 epochs...
Completing Train Step...
At time: 1252.3626534938812 and batch: 50, loss is 4.6276145267486575 and perplexity is 102.26981099813102
At time: 1253.5785744190216 and batch: 100, loss is 4.621142263412476 and perplexity is 101.61003128721603
At time: 1254.7581572532654 and batch: 150, loss is 4.550662469863892 and perplexity is 94.6951202036549
At time: 1255.9365599155426 and batch: 200, loss is 4.61524374961853 and perplexity is 101.01244727744489
At time: 1257.1126413345337 and batch: 250, loss is 4.642880115509033 and perplexity is 103.84299713078205
At time: 1258.288194656372 and batch: 300, loss is 4.640536069869995 and perplexity is 103.59986946870409
At time: 1259.4645433425903 and batch: 350, loss is 4.635875406265259 and perplexity is 103.11814876866886
At time: 1260.6274871826172 and batch: 400, loss is 4.624785985946655 and perplexity is 101.98094539157948
At time: 1261.7986507415771 and batch: 450, loss is 4.563793268203735 and perplexity is 95.94674214502434
At time: 1262.9729342460632 and batch: 500, loss is 4.579130926132202 and perplexity is 97.42968381044227
At time: 1264.140480041504 and batch: 550, loss is 4.57922966003418 and perplexity is 97.43930389819992
At time: 1265.3125085830688 and batch: 600, loss is 4.603690786361694 and perplexity is 99.85216941457882
At time: 1266.499941110611 and batch: 650, loss is 4.621132755279541 and perplexity is 101.60906517012408
At time: 1267.6623849868774 and batch: 700, loss is 4.630061178207398 and perplexity is 102.52033592901616
At time: 1268.8239223957062 and batch: 750, loss is 4.618832960128784 and perplexity is 101.37565363700607
At time: 1269.9847121238708 and batch: 800, loss is 4.643763694763184 and perplexity is 103.93479119642959
At time: 1271.160703420639 and batch: 850, loss is 4.685529584884644 and perplexity is 108.36764698228899
At time: 1272.3391036987305 and batch: 900, loss is 4.642111272811889 and perplexity is 103.76318888471344
At time: 1273.5166156291962 and batch: 950, loss is 4.628182344436645 and perplexity is 102.32789809564294
At time: 1274.6928555965424 and batch: 1000, loss is 4.627362308502197 and perplexity is 102.24401993835987
At time: 1275.871294260025 and batch: 1050, loss is 4.635003118515015 and perplexity is 103.0282392898395
At time: 1277.051159620285 and batch: 1100, loss is 4.587573289871216 and perplexity is 98.255702509204
At time: 1278.2390229701996 and batch: 1150, loss is 4.618773918151856 and perplexity is 101.36966839469498
At time: 1279.416820049286 and batch: 1200, loss is 4.650959024429321 and perplexity is 104.68533324346352
At time: 1280.6010677814484 and batch: 1250, loss is 4.6788058090209965 and perplexity is 107.64145133737334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.800836493499087 and perplexity of 121.61210271663491
Finished 41 epochs...
Completing Train Step...
At time: 1283.5929231643677 and batch: 50, loss is 4.625203342437744 and perplexity is 102.02351668419072
At time: 1284.7704012393951 and batch: 100, loss is 4.618579788208008 and perplexity is 101.34999141666873
At time: 1285.949001789093 and batch: 150, loss is 4.547896528244019 and perplexity is 94.43356092508553
At time: 1287.1254706382751 and batch: 200, loss is 4.612793569564819 and perplexity is 100.76525155463138
At time: 1288.3070785999298 and batch: 250, loss is 4.640167245864868 and perplexity is 103.5616663954543
At time: 1289.4914891719818 and batch: 300, loss is 4.637475156784058 and perplexity is 103.28324410112162
At time: 1290.6718950271606 and batch: 350, loss is 4.632944107055664 and perplexity is 102.81632121025152
At time: 1291.856380224228 and batch: 400, loss is 4.621953125 and perplexity is 101.6924563716452
At time: 1293.0412375926971 and batch: 450, loss is 4.561250219345093 and perplexity is 95.70305487756475
At time: 1294.2220432758331 and batch: 500, loss is 4.575969533920288 and perplexity is 97.12215672982254
At time: 1295.4472241401672 and batch: 550, loss is 4.577300806045532 and perplexity is 97.25153885211844
At time: 1296.6247844696045 and batch: 600, loss is 4.601095552444458 and perplexity is 99.59336565121555
At time: 1297.8025743961334 and batch: 650, loss is 4.619078340530396 and perplexity is 101.40053228785104
At time: 1298.9862129688263 and batch: 700, loss is 4.62771164894104 and perplexity is 102.2797441487466
At time: 1300.1747624874115 and batch: 750, loss is 4.617004470825195 and perplexity is 101.19045870376175
At time: 1301.3621225357056 and batch: 800, loss is 4.641555061340332 and perplexity is 103.70549065642616
At time: 1302.539487361908 and batch: 850, loss is 4.682971668243408 and perplexity is 108.09080579398895
At time: 1303.716364622116 and batch: 900, loss is 4.639448127746582 and perplexity is 103.48722009583909
At time: 1304.906259059906 and batch: 950, loss is 4.626101741790771 and perplexity is 102.11521573058768
At time: 1306.085847377777 and batch: 1000, loss is 4.625021266937256 and perplexity is 102.00494239234195
At time: 1307.2634043693542 and batch: 1050, loss is 4.63354977607727 and perplexity is 102.8786127330427
At time: 1308.4409263134003 and batch: 1100, loss is 4.5853044795989994 and perplexity is 98.03303165652274
At time: 1309.6220507621765 and batch: 1150, loss is 4.617259654998779 and perplexity is 101.21628420232977
At time: 1310.8010423183441 and batch: 1200, loss is 4.648550024032593 and perplexity is 104.43344974967921
At time: 1311.9799528121948 and batch: 1250, loss is 4.676845264434815 and perplexity is 107.43062221018799
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.800228815009124 and perplexity of 121.53822410718915
Finished 42 epochs...
Completing Train Step...
At time: 1314.9546575546265 and batch: 50, loss is 4.622231979370117 and perplexity is 101.72081771167036
At time: 1316.1581373214722 and batch: 100, loss is 4.6161592578887936 and perplexity is 101.10496735331205
At time: 1317.3347897529602 and batch: 150, loss is 4.54527590751648 and perplexity is 94.18641036333776
At time: 1318.5104422569275 and batch: 200, loss is 4.609974393844604 and perplexity is 100.48157665658117
At time: 1319.6895031929016 and batch: 250, loss is 4.637110204696655 and perplexity is 103.24555754290552
At time: 1320.8768904209137 and batch: 300, loss is 4.6346586894989015 and perplexity is 102.99275948523685
At time: 1322.0616075992584 and batch: 350, loss is 4.630276012420654 and perplexity is 102.5423631707461
At time: 1323.2403440475464 and batch: 400, loss is 4.619404220581055 and perplexity is 101.43358208329188
At time: 1324.4152481555939 and batch: 450, loss is 4.559169158935547 and perplexity is 95.50409813129413
At time: 1325.6175427436829 and batch: 500, loss is 4.5732193946838375 and perplexity is 96.85542421975597
At time: 1326.7918841838837 and batch: 550, loss is 4.575659122467041 and perplexity is 97.09201357864124
At time: 1327.968751192093 and batch: 600, loss is 4.599149007797241 and perplexity is 99.39969127746984
At time: 1329.1440916061401 and batch: 650, loss is 4.617032442092896 and perplexity is 101.19328916875652
At time: 1330.3246474266052 and batch: 700, loss is 4.625067348480225 and perplexity is 102.0096430457837
At time: 1331.5063972473145 and batch: 750, loss is 4.614119358062744 and perplexity is 100.89893356358672
At time: 1332.685923576355 and batch: 800, loss is 4.6396740055084225 and perplexity is 103.51059819769068
At time: 1333.8616454601288 and batch: 850, loss is 4.68044900894165 and perplexity is 107.81847316296339
At time: 1335.0370573997498 and batch: 900, loss is 4.63606782913208 and perplexity is 103.13799296765379
At time: 1336.212639093399 and batch: 950, loss is 4.623573865890503 and perplexity is 101.85740712905339
At time: 1337.3918755054474 and batch: 1000, loss is 4.623116569519043 and perplexity is 101.81083875495275
At time: 1338.570477962494 and batch: 1050, loss is 4.631663255691528 and perplexity is 102.68471308818806
At time: 1339.7537322044373 and batch: 1100, loss is 4.582258281707763 and perplexity is 97.73485802074015
At time: 1340.9287428855896 and batch: 1150, loss is 4.61311107635498 and perplexity is 100.79725028585064
At time: 1342.1036465168 and batch: 1200, loss is 4.645974082946777 and perplexity is 104.16478152106153
At time: 1343.2829022407532 and batch: 1250, loss is 4.674867935180664 and perplexity is 107.21840637753127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.799567229556342 and perplexity of 121.45784277865354
Finished 43 epochs...
Completing Train Step...
At time: 1346.245407819748 and batch: 50, loss is 4.619723854064941 and perplexity is 101.46600883457788
At time: 1347.4169890880585 and batch: 100, loss is 4.614542379379272 and perplexity is 100.94162499235462
At time: 1348.5870199203491 and batch: 150, loss is 4.542881097793579 and perplexity is 93.9611217014448
At time: 1349.7503929138184 and batch: 200, loss is 4.606591024398804 and perplexity is 100.14218482798374
At time: 1350.912062883377 and batch: 250, loss is 4.634678211212158 and perplexity is 102.99477009998029
At time: 1352.0734202861786 and batch: 300, loss is 4.6316046714782715 and perplexity is 102.6786975612675
At time: 1353.236970424652 and batch: 350, loss is 4.627585439682007 and perplexity is 102.26683631258484
At time: 1354.4152791500092 and batch: 400, loss is 4.617137804031372 and perplexity is 101.20395165156418
At time: 1355.6224014759064 and batch: 450, loss is 4.55584789276123 and perplexity is 95.18742976172648
At time: 1356.802598953247 and batch: 500, loss is 4.571301164627076 and perplexity is 96.66981131489892
At time: 1357.9814891815186 and batch: 550, loss is 4.573245744705201 and perplexity is 96.8579763958781
At time: 1359.157069683075 and batch: 600, loss is 4.596551761627198 and perplexity is 99.14186077956954
At time: 1360.3411519527435 and batch: 650, loss is 4.614728927612305 and perplexity is 100.96045723064215
At time: 1361.5166375637054 and batch: 700, loss is 4.623188915252686 and perplexity is 101.81820460121584
At time: 1362.6931221485138 and batch: 750, loss is 4.611930589675904 and perplexity is 100.67833067991603
At time: 1363.8765978813171 and batch: 800, loss is 4.637987661361694 and perplexity is 103.33619080307122
At time: 1365.0573689937592 and batch: 850, loss is 4.678024520874024 and perplexity is 107.55738519153839
At time: 1366.2381920814514 and batch: 900, loss is 4.632209644317627 and perplexity is 102.74083417806011
At time: 1367.4183213710785 and batch: 950, loss is 4.620271883010864 and perplexity is 101.52163038416403
At time: 1368.5979664325714 and batch: 1000, loss is 4.620791997909546 and perplexity is 101.57444703083702
At time: 1369.783102273941 and batch: 1050, loss is 4.628441867828369 and perplexity is 102.35445802513708
At time: 1370.9692087173462 and batch: 1100, loss is 4.579857215881348 and perplexity is 97.50047169420178
At time: 1372.1471135616302 and batch: 1150, loss is 4.611042890548706 and perplexity is 100.5889982696613
At time: 1373.3235116004944 and batch: 1200, loss is 4.643578929901123 and perplexity is 103.91558947302737
At time: 1374.498613834381 and batch: 1250, loss is 4.672085151672364 and perplexity is 106.92045552311427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.799158694970346 and perplexity of 121.4082331834458
Finished 44 epochs...
Completing Train Step...
At time: 1377.4695155620575 and batch: 50, loss is 4.617600145339966 and perplexity is 101.25075323732521
At time: 1378.6745233535767 and batch: 100, loss is 4.61138653755188 and perplexity is 100.62357131759084
At time: 1379.8515343666077 and batch: 150, loss is 4.540299196243286 and perplexity is 93.7188362488433
At time: 1381.0282032489777 and batch: 200, loss is 4.603788404464722 and perplexity is 99.8619172697161
At time: 1382.2061400413513 and batch: 250, loss is 4.632192611694336 and perplexity is 102.73908424703792
At time: 1383.3864290714264 and batch: 300, loss is 4.6290673160552975 and perplexity is 102.41849546339073
At time: 1384.565505027771 and batch: 350, loss is 4.625211868286133 and perplexity is 102.02438652493416
At time: 1385.770589351654 and batch: 400, loss is 4.61550971031189 and perplexity is 101.03931619083976
At time: 1386.946542263031 and batch: 450, loss is 4.554132232666015 and perplexity is 95.02426049843903
At time: 1388.122332572937 and batch: 500, loss is 4.5689029788970945 and perplexity is 96.43825691908557
At time: 1389.298075914383 and batch: 550, loss is 4.571177043914795 and perplexity is 96.65781333367688
At time: 1390.4728274345398 and batch: 600, loss is 4.593852558135986 and perplexity is 98.87461755700228
At time: 1391.64843416214 and batch: 650, loss is 4.6121899700164795 and perplexity is 100.70444804663569
At time: 1392.8271551132202 and batch: 700, loss is 4.621267166137695 and perplexity is 101.62272344965977
At time: 1394.006291627884 and batch: 750, loss is 4.610264635086059 and perplexity is 100.51074478682573
At time: 1395.1820631027222 and batch: 800, loss is 4.636254625320435 and perplexity is 103.15726055111425
At time: 1396.3571321964264 and batch: 850, loss is 4.675378370285034 and perplexity is 107.27314838591376
At time: 1397.5477321147919 and batch: 900, loss is 4.629492225646973 and perplexity is 102.46202331152328
At time: 1398.7295842170715 and batch: 950, loss is 4.618162698745728 and perplexity is 101.30772821762685
At time: 1399.9082324504852 and batch: 1000, loss is 4.618415374755859 and perplexity is 101.3333294844651
At time: 1401.0868000984192 and batch: 1050, loss is 4.626308612823486 and perplexity is 102.13634259591463
At time: 1402.2592980861664 and batch: 1100, loss is 4.57758171081543 and perplexity is 97.27886111055864
At time: 1403.4344155788422 and batch: 1150, loss is 4.608197889328003 and perplexity is 100.3032291462992
At time: 1404.6112260818481 and batch: 1200, loss is 4.640501623153686 and perplexity is 103.59630085485475
At time: 1405.7863829135895 and batch: 1250, loss is 4.670049505233765 and perplexity is 106.70302465987999
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.798656602845575 and perplexity of 121.34729036641647
Finished 45 epochs...
Completing Train Step...
At time: 1408.7094099521637 and batch: 50, loss is 4.6149523639678955 and perplexity is 100.98301798761734
At time: 1409.8962166309357 and batch: 100, loss is 4.608381662368775 and perplexity is 100.32166386956933
At time: 1411.0667712688446 and batch: 150, loss is 4.538382577896118 and perplexity is 93.53938503249378
At time: 1412.242344379425 and batch: 200, loss is 4.60098593711853 and perplexity is 99.58244929029058
At time: 1413.4235248565674 and batch: 250, loss is 4.628700151443481 and perplexity is 102.38089791892737
At time: 1414.6497795581818 and batch: 300, loss is 4.626201143264771 and perplexity is 102.12536663804829
At time: 1415.8267633914948 and batch: 350, loss is 4.623038101196289 and perplexity is 101.80285014262806
At time: 1417.0029571056366 and batch: 400, loss is 4.613263549804688 and perplexity is 100.81262036205727
At time: 1418.1797926425934 and batch: 450, loss is 4.55268196105957 and perplexity is 94.88654939490355
At time: 1419.3503577709198 and batch: 500, loss is 4.566744260787964 and perplexity is 96.23029845007612
At time: 1420.527827501297 and batch: 550, loss is 4.569243383407593 and perplexity is 96.47109052476212
At time: 1421.7053265571594 and batch: 600, loss is 4.591999521255493 and perplexity is 98.69156889446437
At time: 1422.8818492889404 and batch: 650, loss is 4.609821119308472 and perplexity is 100.46617656977999
At time: 1424.0572974681854 and batch: 700, loss is 4.619344663619995 and perplexity is 101.42754118728409
At time: 1425.2365906238556 and batch: 750, loss is 4.608460893630982 and perplexity is 100.32961279652204
At time: 1426.4143390655518 and batch: 800, loss is 4.634497013092041 and perplexity is 102.97610933195536
At time: 1427.5907905101776 and batch: 850, loss is 4.672642011642456 and perplexity is 106.98001182550048
At time: 1428.7661385536194 and batch: 900, loss is 4.627025146484375 and perplexity is 102.20955294909353
At time: 1429.9450898170471 and batch: 950, loss is 4.616279802322388 and perplexity is 101.11715572894086
At time: 1431.1167395114899 and batch: 1000, loss is 4.616185417175293 and perplexity is 101.10761222171337
At time: 1432.283843278885 and batch: 1050, loss is 4.623781099319458 and perplexity is 101.8785175761168
At time: 1433.4519171714783 and batch: 1100, loss is 4.574579801559448 and perplexity is 96.98727667096028
At time: 1434.61802816391 and batch: 1150, loss is 4.6054712009429934 and perplexity is 100.03010602644498
At time: 1435.7838561534882 and batch: 1200, loss is 4.638689460754395 and perplexity is 103.40873753266864
At time: 1436.9565875530243 and batch: 1250, loss is 4.667988119125366 and perplexity is 106.48329507868341
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.798301084198221 and perplexity of 121.30415680973158
Finished 46 epochs...
Completing Train Step...
At time: 1439.9062571525574 and batch: 50, loss is 4.612307548522949 and perplexity is 100.71628942136381
At time: 1441.067927122116 and batch: 100, loss is 4.606124391555786 and perplexity is 100.09546609666643
At time: 1442.2369766235352 and batch: 150, loss is 4.535681467056275 and perplexity is 93.28706571026856
At time: 1443.4197990894318 and batch: 200, loss is 4.597658596038818 and perplexity is 99.25165515359923
At time: 1444.6479966640472 and batch: 250, loss is 4.6260424327850345 and perplexity is 102.10915955826665
At time: 1445.8248829841614 and batch: 300, loss is 4.6240816402435305 and perplexity is 101.90914084147404
At time: 1447.0025091171265 and batch: 350, loss is 4.619892644882202 and perplexity is 101.48313681061514
At time: 1448.1784403324127 and batch: 400, loss is 4.610527582168579 and perplexity is 100.53717726894908
At time: 1449.3599121570587 and batch: 450, loss is 4.5500492763519285 and perplexity is 94.6370715696732
At time: 1450.5391714572906 and batch: 500, loss is 4.5642978477478025 and perplexity is 95.99516712453024
At time: 1451.716313123703 and batch: 550, loss is 4.566941289901734 and perplexity is 96.24926048847304
At time: 1452.8926451206207 and batch: 600, loss is 4.589755697250366 and perplexity is 98.47037064085738
At time: 1454.0684292316437 and batch: 650, loss is 4.608022203445435 and perplexity is 100.28560883282769
At time: 1455.2472648620605 and batch: 700, loss is 4.617914438247681 and perplexity is 101.2825806322687
At time: 1456.4250540733337 and batch: 750, loss is 4.606899518966674 and perplexity is 100.17308291371863
At time: 1457.6016414165497 and batch: 800, loss is 4.632958583831787 and perplexity is 102.81780966988947
At time: 1458.7774584293365 and batch: 850, loss is 4.669875869750976 and perplexity is 106.68449883709482
At time: 1459.9544563293457 and batch: 900, loss is 4.625241355895996 and perplexity is 102.02739502459706
At time: 1461.1301879882812 and batch: 950, loss is 4.614479341506958 and perplexity is 100.93526204764252
At time: 1462.3071205615997 and batch: 1000, loss is 4.6146055030822755 and perplexity is 100.94799700262163
At time: 1463.4837696552277 and batch: 1050, loss is 4.621204090118408 and perplexity is 101.61631369494852
At time: 1464.6650784015656 and batch: 1100, loss is 4.571907215118408 and perplexity is 96.72841585840574
At time: 1465.8435280323029 and batch: 1150, loss is 4.603110370635986 and perplexity is 99.79423046117091
At time: 1467.0215344429016 and batch: 1200, loss is 4.636687564849853 and perplexity is 103.20193107607365
At time: 1468.1973104476929 and batch: 1250, loss is 4.666298952102661 and perplexity is 106.30357883627441
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.798673977817062 and perplexity of 121.3493987904435
Annealing...
Finished 47 epochs...
Completing Train Step...
At time: 1471.1476895809174 and batch: 50, loss is 4.613163089752197 and perplexity is 100.80249322961859
At time: 1472.3484365940094 and batch: 100, loss is 4.611243629455567 and perplexity is 100.60919242202438
At time: 1473.5258553028107 and batch: 150, loss is 4.53806881904602 and perplexity is 93.51004082635006
At time: 1474.7250471115112 and batch: 200, loss is 4.601330795288086 and perplexity is 99.61679703368227
At time: 1475.9015142917633 and batch: 250, loss is 4.626844282150269 and perplexity is 102.19106855799406
At time: 1477.0874586105347 and batch: 300, loss is 4.624264717102051 and perplexity is 101.92779975478929
At time: 1478.2617723941803 and batch: 350, loss is 4.618985376358032 and perplexity is 101.39110610944488
At time: 1479.444209098816 and batch: 400, loss is 4.603353223800659 and perplexity is 99.81846874890785
At time: 1480.6239321231842 and batch: 450, loss is 4.543663139343262 and perplexity is 94.03463194295523
At time: 1481.8071315288544 and batch: 500, loss is 4.555869693756104 and perplexity is 95.18950496501539
At time: 1482.989409685135 and batch: 550, loss is 4.554575138092041 and perplexity is 95.06635658062129
At time: 1484.1671154499054 and batch: 600, loss is 4.57595401763916 and perplexity is 97.12064976682618
At time: 1485.3435113430023 and batch: 650, loss is 4.598235759735108 and perplexity is 99.30895614018446
At time: 1486.5192680358887 and batch: 700, loss is 4.60625729560852 and perplexity is 100.10877007382761
At time: 1487.6913478374481 and batch: 750, loss is 4.591219129562378 and perplexity is 98.61458085823783
At time: 1488.8652663230896 and batch: 800, loss is 4.6106112098693846 and perplexity is 100.54558531349734
At time: 1490.037927865982 and batch: 850, loss is 4.642338743209839 and perplexity is 103.78679462328317
At time: 1491.2103281021118 and batch: 900, loss is 4.598877725601196 and perplexity is 99.3727295682151
At time: 1492.3837308883667 and batch: 950, loss is 4.586009817123413 and perplexity is 98.10220242388331
At time: 1493.5570106506348 and batch: 1000, loss is 4.584823131561279 and perplexity is 97.98585500420852
At time: 1494.7309653759003 and batch: 1050, loss is 4.58800669670105 and perplexity is 98.29829643132327
At time: 1495.9073085784912 and batch: 1100, loss is 4.535354223251343 and perplexity is 93.2565430903761
At time: 1497.084142446518 and batch: 1150, loss is 4.561565084457397 and perplexity is 95.73319317518693
At time: 1498.2583367824554 and batch: 1200, loss is 4.602776365280151 and perplexity is 99.76090421959691
At time: 1499.4405434131622 and batch: 1250, loss is 4.641596193313599 and perplexity is 103.70975635562314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.783832188070256 and perplexity of 119.56165599572253
Finished 48 epochs...
Completing Train Step...
At time: 1502.4231431484222 and batch: 50, loss is 4.603947744369507 and perplexity is 99.87783052588023
At time: 1503.6128768920898 and batch: 100, loss is 4.599830551147461 and perplexity is 99.46745956696425
At time: 1504.8190879821777 and batch: 150, loss is 4.527001132965088 and perplexity is 92.48080717275612
At time: 1505.997230052948 and batch: 200, loss is 4.59021523475647 and perplexity is 98.51563186822608
At time: 1507.1755583286285 and batch: 250, loss is 4.616619749069214 and perplexity is 101.15153602048227
At time: 1508.3532774448395 and batch: 300, loss is 4.615026578903199 and perplexity is 100.99051271387087
At time: 1509.5356786251068 and batch: 350, loss is 4.610102844238281 and perplexity is 100.49448438364357
At time: 1510.7162251472473 and batch: 400, loss is 4.596226902008056 and perplexity is 99.10965882327656
At time: 1511.8980045318604 and batch: 450, loss is 4.537143287658691 and perplexity is 93.42353438692247
At time: 1513.0753679275513 and batch: 500, loss is 4.549601850509643 and perplexity is 94.5947379694943
At time: 1514.2525346279144 and batch: 550, loss is 4.548560657501221 and perplexity is 94.4962978461641
At time: 1515.4301574230194 and batch: 600, loss is 4.570538558959961 and perplexity is 96.59611847181262
At time: 1516.6087429523468 and batch: 650, loss is 4.5931300449371335 and perplexity is 98.80320514209858
At time: 1517.787738084793 and batch: 700, loss is 4.601520290374756 and perplexity is 99.63567571592225
At time: 1518.9592010974884 and batch: 750, loss is 4.587114543914795 and perplexity is 98.2106384402526
At time: 1520.132910490036 and batch: 800, loss is 4.607395658493042 and perplexity is 100.22279507069346
At time: 1521.3046481609344 and batch: 850, loss is 4.640959768295288 and perplexity is 103.64377387071474
At time: 1522.4771163463593 and batch: 900, loss is 4.598740978240967 and perplexity is 99.35914153885263
At time: 1523.644546031952 and batch: 950, loss is 4.586923980712891 and perplexity is 98.19192488964411
At time: 1524.807805776596 and batch: 1000, loss is 4.586695623397827 and perplexity is 98.16950460533077
At time: 1525.9709165096283 and batch: 1050, loss is 4.59099232673645 and perplexity is 98.59221732877317
At time: 1527.1340248584747 and batch: 1100, loss is 4.539181470870972 and perplexity is 93.61414284783908
At time: 1528.2970461845398 and batch: 1150, loss is 4.566250982284546 and perplexity is 96.18284181810375
At time: 1529.476440668106 and batch: 1200, loss is 4.607198581695557 and perplexity is 100.20304542936792
At time: 1530.6588144302368 and batch: 1250, loss is 4.643443870544433 and perplexity is 103.90155564808414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.782522382527373 and perplexity of 119.40515599064426
Finished 49 epochs...
Completing Train Step...
At time: 1533.6405081748962 and batch: 50, loss is 4.600316553115845 and perplexity is 99.51581269701042
At time: 1534.842747926712 and batch: 100, loss is 4.595721435546875 and perplexity is 99.05957487370702
At time: 1536.0212092399597 and batch: 150, loss is 4.522841243743897 and perplexity is 92.09689632674717
At time: 1537.2012491226196 and batch: 200, loss is 4.586013832092285 and perplexity is 98.10259630196303
At time: 1538.380464553833 and batch: 250, loss is 4.612806510925293 and perplexity is 100.76655560251305
At time: 1539.5569639205933 and batch: 300, loss is 4.61153902053833 and perplexity is 100.63891587011446
At time: 1540.7357785701752 and batch: 350, loss is 4.606772518157959 and perplexity is 100.16036165899895
At time: 1541.9152193069458 and batch: 400, loss is 4.593303461074829 and perplexity is 98.82034069807425
At time: 1543.0945010185242 and batch: 450, loss is 4.53446397781372 and perplexity is 93.17355882202529
At time: 1544.278327703476 and batch: 500, loss is 4.547028217315674 and perplexity is 94.35159882157656
At time: 1545.4547564983368 and batch: 550, loss is 4.545736026763916 and perplexity is 94.22975731521154
At time: 1546.6311993598938 and batch: 600, loss is 4.568377084732056 and perplexity is 96.38755393585541
At time: 1547.8082990646362 and batch: 650, loss is 4.591152391433716 and perplexity is 98.6079997252612
At time: 1548.987916469574 and batch: 700, loss is 4.599729595184326 and perplexity is 99.45741824065749
At time: 1550.1652173995972 and batch: 750, loss is 4.58543137550354 and perplexity is 98.04547243607495
At time: 1551.3406112194061 and batch: 800, loss is 4.605822257995605 and perplexity is 100.06522846526828
At time: 1552.5172805786133 and batch: 850, loss is 4.640584783554077 and perplexity is 103.60491632294075
At time: 1553.6931314468384 and batch: 900, loss is 4.59875114440918 and perplexity is 99.36015164573347
At time: 1554.8715958595276 and batch: 950, loss is 4.58793685913086 and perplexity is 98.29143175685557
At time: 1556.0498442649841 and batch: 1000, loss is 4.587938766479493 and perplexity is 98.29161923306233
At time: 1557.2251334190369 and batch: 1050, loss is 4.592997598648071 and perplexity is 98.79011989079565
At time: 1558.4096374511719 and batch: 1100, loss is 4.541158199310303 and perplexity is 93.79937560344221
At time: 1559.5963308811188 and batch: 1150, loss is 4.568284311294556 and perplexity is 96.37861214593124
At time: 1560.7721230983734 and batch: 1200, loss is 4.609008016586304 and perplexity is 100.38452045003076
At time: 1561.9515755176544 and batch: 1250, loss is 4.64407678604126 and perplexity is 103.96733736773716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.781919604670392 and perplexity of 119.33320289464895
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f58a487a898>
SETTINGS FOR THIS RUN
{'anneal': 4.652774405702031, 'data': 'wikitext', 'lr': 14.781429953862904, 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.6609921934764259, 'num_layers': 1, 'wordvec_source': 'glove', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.6662976741790771 and batch: 50, loss is 7.373626585006714 and perplexity is 1593.401924924681
At time: 2.8344972133636475 and batch: 100, loss is 6.527553606033325 and perplexity is 683.7235067714446
At time: 4.008805513381958 and batch: 150, loss is 6.336170597076416 and perplexity is 564.6299704731456
At time: 5.189336538314819 and batch: 200, loss is 6.301569738388062 and perplexity is 545.427416843512
At time: 6.364442825317383 and batch: 250, loss is 6.330669078826904 and perplexity is 561.5321774820311
At time: 7.522749185562134 and batch: 300, loss is 6.338103895187378 and perplexity is 565.7226244010121
At time: 8.687336206436157 and batch: 350, loss is 6.37721284866333 and perplexity is 588.285779074158
At time: 9.85110068321228 and batch: 400, loss is 6.366382160186768 and perplexity is 581.9486189171902
At time: 11.023137331008911 and batch: 450, loss is 6.361182956695557 and perplexity is 578.9308015473755
At time: 12.187716722488403 and batch: 500, loss is 6.389229927062988 and perplexity is 595.3979032914652
At time: 13.343899250030518 and batch: 550, loss is 6.411100635528564 and perplexity is 608.563118780329
At time: 14.499659538269043 and batch: 600, loss is 6.443431406021118 and perplexity is 628.559947926039
At time: 15.656299114227295 and batch: 650, loss is 6.441474905014038 and perplexity is 627.331372001833
At time: 16.822436571121216 and batch: 700, loss is 6.479188470840454 and perplexity is 651.4420674664083
At time: 18.001354217529297 and batch: 750, loss is 6.434342727661133 and perplexity is 622.8730510642753
At time: 19.172062158584595 and batch: 800, loss is 6.466716089248657 and perplexity is 643.3674926783289
At time: 20.342907905578613 and batch: 850, loss is 6.5212994480133055 and perplexity is 679.4607358342839
At time: 21.513817071914673 and batch: 900, loss is 6.527625236511231 and perplexity is 683.772483967099
At time: 22.683438301086426 and batch: 950, loss is 6.516769361495972 and perplexity is 676.3896812392634
At time: 23.85516095161438 and batch: 1000, loss is 6.54736255645752 and perplexity is 697.4023866575005
At time: 25.02532935142517 and batch: 1050, loss is 6.562517433166504 and perplexity is 708.05192625083
At time: 26.20217514038086 and batch: 1100, loss is 6.555836095809936 and perplexity is 703.3369611416028
At time: 27.38122820854187 and batch: 1150, loss is 6.6191236019134525 and perplexity is 749.2881345861578
At time: 28.552995681762695 and batch: 1200, loss is 6.634570732116699 and perplexity is 760.9523433003487
At time: 29.730169534683228 and batch: 1250, loss is 6.647744731903076 and perplexity is 771.0434535044169
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.210282151716469 and perplexity of 497.8416983619063
Finished 1 epochs...
Completing Train Step...
At time: 32.74958801269531 and batch: 50, loss is 6.606996212005615 and perplexity is 740.2561034041142
At time: 33.91908311843872 and batch: 100, loss is 6.641142816543579 and perplexity is 765.9698560442229
At time: 35.11064410209656 and batch: 150, loss is 6.628356046676636 and perplexity is 756.2379283283593
At time: 36.280805349349976 and batch: 200, loss is 6.667019348144532 and perplexity is 786.0491703301722
At time: 37.445871353149414 and batch: 250, loss is 6.6893470573425295 and perplexity is 803.7972472529009
At time: 38.61446166038513 and batch: 300, loss is 6.701949691772461 and perplexity is 813.9913112230882
At time: 39.78425908088684 and batch: 350, loss is 6.7071043968200685 and perplexity is 818.1980292243006
At time: 40.953322649002075 and batch: 400, loss is 6.660953302383422 and perplexity is 781.2953929849424
At time: 42.12966179847717 and batch: 450, loss is 6.63010910987854 and perplexity is 757.5648239385081
At time: 43.31355333328247 and batch: 500, loss is 6.584216938018799 and perplexity is 723.5842144746913
At time: 44.48659944534302 and batch: 550, loss is 6.541037187576294 and perplexity is 693.0049815696736
At time: 45.66112732887268 and batch: 600, loss is 6.566321935653686 and perplexity is 710.7508423263708
At time: 46.83848333358765 and batch: 650, loss is 6.568235549926758 and perplexity is 712.1122474695546
At time: 48.013427734375 and batch: 700, loss is 6.598140850067138 and perplexity is 733.7298066889676
At time: 49.18562936782837 and batch: 750, loss is 6.497003087997436 and perplexity is 663.1512460978696
At time: 50.35741424560547 and batch: 800, loss is 6.504396514892578 and perplexity is 668.0723759463461
At time: 51.54126977920532 and batch: 850, loss is 6.535579690933227 and perplexity is 689.2332107839445
At time: 52.719482421875 and batch: 900, loss is 6.503414106369019 and perplexity is 667.4163782314952
At time: 53.8976514339447 and batch: 950, loss is 6.4914288902282715 and perplexity is 659.4649933968773
At time: 55.07510042190552 and batch: 1000, loss is 6.51237286567688 and perplexity is 673.4224642915568
At time: 56.254668951034546 and batch: 1050, loss is 6.545814323425293 and perplexity is 696.3234806601891
At time: 57.426079511642456 and batch: 1100, loss is 6.532774639129639 and perplexity is 687.3025849415901
At time: 58.59446620941162 and batch: 1150, loss is 6.573508539199829 and perplexity is 715.8771250679317
At time: 59.767635345458984 and batch: 1200, loss is 6.560960674285889 and perplexity is 706.950517662941
At time: 60.933361768722534 and batch: 1250, loss is 6.572531032562256 and perplexity is 715.1776923322551
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.163938202127053 and perplexity of 475.29620641155867
Finished 2 epochs...
Completing Train Step...
At time: 63.93309497833252 and batch: 50, loss is 6.528419446945191 and perplexity is 684.3157589170804
At time: 65.13796281814575 and batch: 100, loss is 6.5551920413970945 and perplexity is 702.8841197109792
At time: 66.30661487579346 and batch: 150, loss is 6.4960678195953365 and perplexity is 662.5313116393258
At time: 67.47310853004456 and batch: 200, loss is 6.484329509735107 and perplexity is 654.7997801431627
At time: 68.64647936820984 and batch: 250, loss is 6.507130556106567 and perplexity is 669.9014125475013
At time: 69.827321767807 and batch: 300, loss is 6.516595411300659 and perplexity is 676.2720333548372
At time: 71.00114464759827 and batch: 350, loss is 6.551435136795044 and perplexity is 700.248405290858
At time: 72.17323350906372 and batch: 400, loss is 6.552133321762085 and perplexity is 700.7374789126845
At time: 73.34600353240967 and batch: 450, loss is 6.54474105834961 and perplexity is 695.5765418903659
At time: 74.51864838600159 and batch: 500, loss is 6.505327053070069 and perplexity is 668.6943321295983
At time: 75.67344784736633 and batch: 550, loss is 6.474365272521973 and perplexity is 648.3075983416126
At time: 76.82937383651733 and batch: 600, loss is 6.522572832107544 and perplexity is 680.3265014370418
At time: 77.98343324661255 and batch: 650, loss is 6.514544696807861 and perplexity is 674.8866135307422
At time: 79.13956022262573 and batch: 700, loss is 6.539795026779175 and perplexity is 692.1446923687356
At time: 80.31132960319519 and batch: 750, loss is 6.468840503692627 and perplexity is 644.7357247039097
At time: 81.46837520599365 and batch: 800, loss is 6.500969524383545 and perplexity is 665.7868167862559
At time: 82.62857842445374 and batch: 850, loss is 6.5404606056213375 and perplexity is 692.6055225740963
At time: 83.80107998847961 and batch: 900, loss is 6.503162879943847 and perplexity is 667.2487266608254
At time: 84.95821022987366 and batch: 950, loss is 6.469061355590821 and perplexity is 644.8781315373745
At time: 86.11667037010193 and batch: 1000, loss is 6.461830310821533 and perplexity is 640.2318080297073
At time: 87.27691626548767 and batch: 1050, loss is 6.491980657577515 and perplexity is 659.8289650528064
At time: 88.4343466758728 and batch: 1100, loss is 6.474592390060425 and perplexity is 648.454857089397
At time: 89.64667057991028 and batch: 1150, loss is 6.50026044845581 and perplexity is 665.3148907169506
At time: 90.80299305915833 and batch: 1200, loss is 6.497786321640015 and perplexity is 663.6708519237502
At time: 91.95917010307312 and batch: 1250, loss is 6.499472789764404 and perplexity is 664.791055989326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.115261996749544 and perplexity of 452.7146415994347
Finished 3 epochs...
Completing Train Step...
At time: 94.93089413642883 and batch: 50, loss is 6.4697435760498045 and perplexity is 645.3182306975167
At time: 96.10553169250488 and batch: 100, loss is 6.465873565673828 and perplexity is 642.8256686802655
At time: 97.28063344955444 and batch: 150, loss is 6.411022081375122 and perplexity is 608.5153154973146
At time: 98.44515872001648 and batch: 200, loss is 6.4528584289550786 and perplexity is 634.5134146143077
At time: 99.60832858085632 and batch: 250, loss is 6.448083448410034 and perplexity is 631.4908474965323
At time: 100.77161860466003 and batch: 300, loss is 6.462537612915039 and perplexity is 640.6848055120043
At time: 101.93349027633667 and batch: 350, loss is 6.509120798110962 and perplexity is 671.2360061191149
At time: 103.09533977508545 and batch: 400, loss is 6.496949014663696 and perplexity is 663.1153882687039
At time: 104.26478886604309 and batch: 450, loss is 6.4785178279876705 and perplexity is 651.0053289640058
At time: 105.44497609138489 and batch: 500, loss is 6.440755538940429 and perplexity is 626.880253375076
At time: 106.60956645011902 and batch: 550, loss is 6.428872909545898 and perplexity is 619.4753496417572
At time: 107.76837158203125 and batch: 600, loss is 6.475554637908935 and perplexity is 649.0791316857618
At time: 108.92656016349792 and batch: 650, loss is 6.467761974334717 and perplexity is 644.0407331484904
At time: 110.08454918861389 and batch: 700, loss is 6.492552375793457 and perplexity is 660.2063091485014
At time: 111.24949312210083 and batch: 750, loss is 6.415788068771362 and perplexity is 611.4224139158003
At time: 112.41241216659546 and batch: 800, loss is 6.445835218429566 and perplexity is 630.0727055927093
At time: 113.57158970832825 and batch: 850, loss is 6.485430126190185 and perplexity is 655.5208602995223
At time: 114.73264384269714 and batch: 900, loss is 6.448013534545899 and perplexity is 631.4466990745291
At time: 115.889901638031 and batch: 950, loss is 6.424349422454834 and perplexity is 616.6794891811627
At time: 117.04833173751831 and batch: 1000, loss is 6.4250625705719 and perplexity is 617.1194298504896
At time: 118.20764422416687 and batch: 1050, loss is 6.457311668395996 and perplexity is 637.3453557539503
At time: 119.41366338729858 and batch: 1100, loss is 6.429644927978516 and perplexity is 619.9537806852626
At time: 120.5783486366272 and batch: 1150, loss is 6.465974550247193 and perplexity is 642.8905874340062
At time: 121.73642063140869 and batch: 1200, loss is 6.484710760116577 and perplexity is 655.049470403358
At time: 122.89451169967651 and batch: 1250, loss is 6.499276990890503 and perplexity is 664.6609033914657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.168669540516651 and perplexity of 477.5503218859548
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 125.79483103752136 and batch: 50, loss is 6.393859872817993 and perplexity is 598.1609547396891
At time: 126.9942398071289 and batch: 100, loss is 6.320946054458618 and perplexity is 556.0988434606247
At time: 128.153626203537 and batch: 150, loss is 6.192595891952514 and perplexity is 489.1141471321606
At time: 129.3148295879364 and batch: 200, loss is 6.223299388885498 and perplexity is 504.3645846938246
At time: 130.4730360507965 and batch: 250, loss is 6.213482275009155 and perplexity is 499.43740504419134
At time: 131.63385939598083 and batch: 300, loss is 6.213209953308105 and perplexity is 499.3014159177174
At time: 132.80775141716003 and batch: 350, loss is 6.2260651206970214 and perplexity is 505.76145266098627
At time: 133.98187708854675 and batch: 400, loss is 6.216901025772095 and perplexity is 501.1477790590774
At time: 135.1517641544342 and batch: 450, loss is 6.2111165237426755 and perplexity is 498.2572568895955
At time: 136.31634330749512 and batch: 500, loss is 6.14834225654602 and perplexity is 467.9410172912287
At time: 137.4760992527008 and batch: 550, loss is 6.129824247360229 and perplexity is 459.35542065723257
At time: 138.63430190086365 and batch: 600, loss is 6.170559549331665 and perplexity is 478.4537496782473
At time: 139.79227924346924 and batch: 650, loss is 6.147195310592651 and perplexity is 467.4046219020932
At time: 140.9523570537567 and batch: 700, loss is 6.174785509109497 and perplexity is 480.4799542940907
At time: 142.1084337234497 and batch: 750, loss is 6.133830728530884 and perplexity is 461.1995111909731
At time: 143.26368832588196 and batch: 800, loss is 6.143602361679077 and perplexity is 465.7282742926617
At time: 144.42007970809937 and batch: 850, loss is 6.172194843292236 and perplexity is 479.2368022916109
At time: 145.57683324813843 and batch: 900, loss is 6.144068670272827 and perplexity is 465.94549803202875
At time: 146.73444867134094 and batch: 950, loss is 6.109070816040039 and perplexity is 449.9204620040957
At time: 147.89129638671875 and batch: 1000, loss is 6.099213914871216 and perplexity is 445.5074256382217
At time: 149.09228825569153 and batch: 1050, loss is 6.089883031845093 and perplexity is 441.3697819175192
At time: 150.25086045265198 and batch: 1100, loss is 6.030614233016967 and perplexity is 415.97045371568095
At time: 151.42459416389465 and batch: 1150, loss is 6.06287392616272 and perplexity is 429.6083270037813
At time: 152.59196281433105 and batch: 1200, loss is 6.055779304504394 and perplexity is 426.5712048179541
At time: 153.75975608825684 and batch: 1250, loss is 6.084079341888428 and perplexity is 438.81562747652316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.7314987739507295 and perplexity of 308.4311907666334
Finished 5 epochs...
Completing Train Step...
At time: 156.6385896205902 and batch: 50, loss is 6.105387840270996 and perplexity is 448.26646353229694
At time: 157.7943079471588 and batch: 100, loss is 6.123995361328125 and perplexity is 456.6856786318982
At time: 158.95175170898438 and batch: 150, loss is 6.036572151184082 and perplexity is 418.4561691304651
At time: 160.10887479782104 and batch: 200, loss is 6.0645123386383055 and perplexity is 430.31277958071945
At time: 161.26713919639587 and batch: 250, loss is 6.0818564891815186 and perplexity is 437.8412882786048
At time: 162.4252154827118 and batch: 300, loss is 6.092634134292602 and perplexity is 442.58570720542616
At time: 163.58316922187805 and batch: 350, loss is 6.106651010513306 and perplexity is 448.8330581670593
At time: 164.74498534202576 and batch: 400, loss is 6.099021606445312 and perplexity is 445.4217590439368
At time: 165.90310215950012 and batch: 450, loss is 6.08789493560791 and perplexity is 440.4931680000346
At time: 167.0593945980072 and batch: 500, loss is 6.057300386428833 and perplexity is 427.2205482942025
At time: 168.21734309196472 and batch: 550, loss is 6.048709220886231 and perplexity is 423.5659469613723
At time: 169.37489223480225 and batch: 600, loss is 6.086864461898804 and perplexity is 440.03948516563264
At time: 170.53357195854187 and batch: 650, loss is 6.074147748947143 and perplexity is 434.47905944764415
At time: 171.69313049316406 and batch: 700, loss is 6.092486982345581 and perplexity is 442.52058464846016
At time: 172.8505220413208 and batch: 750, loss is 6.052720260620117 and perplexity is 425.2682986222842
At time: 174.00744819641113 and batch: 800, loss is 6.070398769378662 and perplexity is 432.85325578739577
At time: 175.1655867099762 and batch: 850, loss is 6.0951503467559816 and perplexity is 443.70074913089735
At time: 176.32358860969543 and batch: 900, loss is 6.072835369110107 and perplexity is 433.9092318872975
At time: 177.483868598938 and batch: 950, loss is 6.046095314025879 and perplexity is 422.4602307759279
At time: 178.69480299949646 and batch: 1000, loss is 6.042508010864258 and perplexity is 420.94745287317835
At time: 179.8661081790924 and batch: 1050, loss is 6.042487735748291 and perplexity is 420.9389182012766
At time: 181.03476190567017 and batch: 1100, loss is 6.016832866668701 and perplexity is 410.27713347780946
At time: 182.20137810707092 and batch: 1150, loss is 6.062303552627563 and perplexity is 429.3633596516641
At time: 183.3630223274231 and batch: 1200, loss is 6.056744337081909 and perplexity is 426.98305862143906
At time: 184.52822136878967 and batch: 1250, loss is 6.05786979675293 and perplexity is 427.46388135667195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.719175464045392 and perplexity of 304.6536215029733
Finished 6 epochs...
Completing Train Step...
At time: 187.410391330719 and batch: 50, loss is 6.052877569198609 and perplexity is 425.33520223593615
At time: 188.5946707725525 and batch: 100, loss is 6.079359502792358 and perplexity is 436.74936836308217
At time: 189.75419878959656 and batch: 150, loss is 5.999078702926636 and perplexity is 403.0572868862317
At time: 190.91189789772034 and batch: 200, loss is 6.025282249450684 and perplexity is 413.7584086271494
At time: 192.07223320007324 and batch: 250, loss is 6.050775346755981 and perplexity is 424.44199222017653
At time: 193.22795391082764 and batch: 300, loss is 6.0631882762908935 and perplexity is 429.74339566475277
At time: 194.3843812942505 and batch: 350, loss is 6.070360097885132 and perplexity is 432.83651702917354
At time: 195.54433703422546 and batch: 400, loss is 6.05360330581665 and perplexity is 425.6439956049151
At time: 196.70073914527893 and batch: 450, loss is 6.02910400390625 and perplexity is 415.3427171596029
At time: 197.85591197013855 and batch: 500, loss is 6.019632816314697 and perplexity is 411.42749852300676
At time: 199.01258826255798 and batch: 550, loss is 6.014837074279785 and perplexity is 409.4591220594738
At time: 200.17120671272278 and batch: 600, loss is 6.040708703994751 and perplexity is 420.19072023080366
At time: 201.32853436470032 and batch: 650, loss is 6.026563205718994 and perplexity is 414.2887546567638
At time: 202.48494338989258 and batch: 700, loss is 6.0375510597229 and perplexity is 418.8660000082939
At time: 203.6417694091797 and batch: 750, loss is 5.995895957946777 and perplexity is 401.77649762378167
At time: 204.79803371429443 and batch: 800, loss is 6.012379207611084 and perplexity is 408.45396191200325
At time: 205.9553291797638 and batch: 850, loss is 6.034032096862793 and perplexity is 417.39461650121746
At time: 207.11591124534607 and batch: 900, loss is 6.018731384277344 and perplexity is 411.0567917033942
At time: 208.30449771881104 and batch: 950, loss is 5.991206340789795 and perplexity is 399.89673080567803
At time: 209.4616301059723 and batch: 1000, loss is 5.981521377563476 and perplexity is 396.04244013239787
At time: 210.6194179058075 and batch: 1050, loss is 5.976022062301635 and perplexity is 393.8704555847717
At time: 211.77551865577698 and batch: 1100, loss is 5.958693675994873 and perplexity is 387.1041104088257
At time: 212.93302965164185 and batch: 1150, loss is 6.000971450805664 and perplexity is 403.82089514209713
At time: 214.0948829650879 and batch: 1200, loss is 5.993821029663086 and perplexity is 400.9437044969336
At time: 215.254243850708 and batch: 1250, loss is 5.98283579826355 and perplexity is 396.56334878537587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.653730183622263 and perplexity of 285.3539055093209
Finished 7 epochs...
Completing Train Step...
At time: 218.20558714866638 and batch: 50, loss is 5.969081430435181 and perplexity is 391.1462106761002
At time: 219.37209129333496 and batch: 100, loss is 5.9920911693573 and perplexity is 400.25072744722155
At time: 220.5304594039917 and batch: 150, loss is 5.911866111755371 and perplexity is 369.3948446859654
At time: 221.6988697052002 and batch: 200, loss is 5.942449760437012 and perplexity is 380.86682008541226
At time: 222.8571858406067 and batch: 250, loss is 5.964428672790527 and perplexity is 389.3305293921977
At time: 224.01459503173828 and batch: 300, loss is 5.97647723197937 and perplexity is 394.04977428022994
At time: 225.17219042778015 and batch: 350, loss is 5.983699378967285 and perplexity is 396.9059611566191
At time: 226.32982301712036 and batch: 400, loss is 5.966042175292968 and perplexity is 389.9592222379944
At time: 227.488183259964 and batch: 450, loss is 5.9374883842468265 and perplexity is 378.98187633665543
At time: 228.64945483207703 and batch: 500, loss is 5.93683762550354 and perplexity is 378.735330796616
At time: 229.81107473373413 and batch: 550, loss is 5.9407281494140625 and perplexity is 380.211679679966
At time: 230.96867108345032 and batch: 600, loss is 5.968913564682007 and perplexity is 391.0805561335726
At time: 232.12566781044006 and batch: 650, loss is 5.950351896286011 and perplexity is 383.8884041989382
At time: 233.28619480133057 and batch: 700, loss is 5.959290065765381 and perplexity is 387.3350441968247
At time: 234.44525527954102 and batch: 750, loss is 5.920189723968506 and perplexity is 372.48237600739503
At time: 235.6065275669098 and batch: 800, loss is 5.941071977615357 and perplexity is 380.34242965437755
At time: 236.7643096446991 and batch: 850, loss is 5.95789475440979 and perplexity is 386.79496808602636
At time: 237.9483187198639 and batch: 900, loss is 5.941295909881592 and perplexity is 380.4276101335683
At time: 239.11406803131104 and batch: 950, loss is 5.9207666301727295 and perplexity is 372.69732539793443
At time: 240.27211141586304 and batch: 1000, loss is 5.915247421264649 and perplexity is 370.64599706163125
At time: 241.43008399009705 and batch: 1050, loss is 5.918979482650757 and perplexity is 372.03185512030274
At time: 242.59460353851318 and batch: 1100, loss is 5.899752025604248 and perplexity is 364.94695914219477
At time: 243.7634618282318 and batch: 1150, loss is 5.947502593994141 and perplexity is 382.79614691358466
At time: 244.93339347839355 and batch: 1200, loss is 5.9372608089447025 and perplexity is 378.8956392347381
At time: 246.1011791229248 and batch: 1250, loss is 5.930683536529541 and perplexity is 376.4117170695826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.606752715841697 and perplexity of 272.25869962328704
Finished 8 epochs...
Completing Train Step...
At time: 249.02644968032837 and batch: 50, loss is 5.9162421226501465 and perplexity is 371.01486257354156
At time: 250.2301459312439 and batch: 100, loss is 5.943086576461792 and perplexity is 381.1094394234918
At time: 251.40059185028076 and batch: 150, loss is 5.86302170753479 and perplexity is 351.7855326038688
At time: 252.5710952281952 and batch: 200, loss is 5.89442458152771 and perplexity is 363.0078943422579
At time: 253.7396388053894 and batch: 250, loss is 5.918539018630981 and perplexity is 371.8680245572947
At time: 254.90722823143005 and batch: 300, loss is 5.931026220321655 and perplexity is 376.5407293681336
At time: 256.0750398635864 and batch: 350, loss is 5.9395361328125 and perplexity is 379.7587310604496
At time: 257.240031003952 and batch: 400, loss is 5.9197581768035885 and perplexity is 372.32166697330535
At time: 258.39641761779785 and batch: 450, loss is 5.894655284881591 and perplexity is 363.0916511420838
At time: 259.56087851524353 and batch: 500, loss is 5.89736047744751 and perplexity is 364.07521374080085
At time: 260.72796058654785 and batch: 550, loss is 5.899685497283936 and perplexity is 364.9226806416128
At time: 261.8944163322449 and batch: 600, loss is 5.927994413375854 and perplexity is 375.40085937546525
At time: 263.0603725910187 and batch: 650, loss is 5.912391309738159 and perplexity is 369.5889010677963
At time: 264.2305107116699 and batch: 700, loss is 5.924883279800415 and perplexity is 374.23475205600795
At time: 265.39978885650635 and batch: 750, loss is 5.8918162536621095 and perplexity is 362.06228450237097
At time: 266.5669765472412 and batch: 800, loss is 5.910608110427856 and perplexity is 368.93043765455053
At time: 267.7608971595764 and batch: 850, loss is 5.926002807617188 and perplexity is 374.6539528806461
At time: 268.9277665615082 and batch: 900, loss is 5.913201971054077 and perplexity is 369.88863396720325
At time: 270.0918941497803 and batch: 950, loss is 5.895183963775635 and perplexity is 363.28366078589676
At time: 271.25505685806274 and batch: 1000, loss is 5.885915861129761 and perplexity is 359.9322650577107
At time: 272.4187362194061 and batch: 1050, loss is 5.89151219367981 and perplexity is 361.9522125856343
At time: 273.58303141593933 and batch: 1100, loss is 5.871214027404785 and perplexity is 354.6793094007541
At time: 274.74002623558044 and batch: 1150, loss is 5.91980809211731 and perplexity is 372.34025198995226
At time: 275.9046461582184 and batch: 1200, loss is 5.909755258560181 and perplexity is 368.6159287755604
At time: 277.07438039779663 and batch: 1250, loss is 5.903728132247925 and perplexity is 366.4009157988732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.583469363024635 and perplexity of 265.99283252518796
Finished 9 epochs...
Completing Train Step...
At time: 280.04884481430054 and batch: 50, loss is 5.887725887298584 and perplexity is 360.584341836458
At time: 281.2167971134186 and batch: 100, loss is 5.905257225036621 and perplexity is 366.9616053609292
At time: 282.37972044944763 and batch: 150, loss is 5.8169225025177 and perplexity is 335.9366170773288
At time: 283.54781317710876 and batch: 200, loss is 5.849512243270874 and perplexity is 347.06505587103766
At time: 284.7188353538513 and batch: 250, loss is 5.874730501174927 and perplexity is 355.92872537083724
At time: 285.8897485733032 and batch: 300, loss is 5.88614974975586 and perplexity is 360.0164589662886
At time: 287.0608162879944 and batch: 350, loss is 5.887838773727417 and perplexity is 360.625049212713
At time: 288.2302157878876 and batch: 400, loss is 5.866708974838257 and perplexity is 353.0850542655874
At time: 289.39668440818787 and batch: 450, loss is 5.844120969772339 and perplexity is 345.19896804791296
At time: 290.55224561691284 and batch: 500, loss is 5.845171937942505 and perplexity is 345.5619518845239
At time: 291.70939564704895 and batch: 550, loss is 5.850991086959839 and perplexity is 347.57869053765626
At time: 292.8657829761505 and batch: 600, loss is 5.8730480766296385 and perplexity is 355.33040560196395
At time: 294.0368368625641 and batch: 650, loss is 5.859066438674927 and perplexity is 350.396874310228
At time: 295.20429849624634 and batch: 700, loss is 5.874873008728027 and perplexity is 355.9794515169116
At time: 296.37225437164307 and batch: 750, loss is 5.841672344207764 and perplexity is 344.3547390520532
At time: 297.55819749832153 and batch: 800, loss is 5.861763534545898 and perplexity is 351.34320387024667
At time: 298.7152554988861 and batch: 850, loss is 5.872205696105957 and perplexity is 355.031208225527
At time: 299.8730516433716 and batch: 900, loss is 5.855339431762696 and perplexity is 349.09337332510546
At time: 301.0310184955597 and batch: 950, loss is 5.8369816875457765 and perplexity is 342.74327157621684
At time: 302.1897683143616 and batch: 1000, loss is 5.8312538051605225 and perplexity is 340.78569017842267
At time: 303.3489956855774 and batch: 1050, loss is 5.839019918441773 and perplexity is 343.4425739294718
At time: 304.51120138168335 and batch: 1100, loss is 5.816538505554199 and perplexity is 335.80764320081545
At time: 305.6719629764557 and batch: 1150, loss is 5.863383102416992 and perplexity is 351.9126890704488
At time: 306.83969044685364 and batch: 1200, loss is 5.863391313552857 and perplexity is 351.9155786852147
At time: 308.00084495544434 and batch: 1250, loss is 5.861945362091064 and perplexity is 351.40709355079304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.535886889826642 and perplexity of 253.6326321173393
Finished 10 epochs...
Completing Train Step...
At time: 310.9062292575836 and batch: 50, loss is 5.837217721939087 and perplexity is 342.82418032463266
At time: 312.1006257534027 and batch: 100, loss is 5.860185947418213 and perplexity is 350.78936633277124
At time: 313.2701621055603 and batch: 150, loss is 5.7767323398590085 and perplexity is 322.7029820202906
At time: 314.4479775428772 and batch: 200, loss is 5.810316123962402 and perplexity is 333.7246073462728
At time: 315.6189115047455 and batch: 250, loss is 5.831445484161377 and perplexity is 340.8510178997978
At time: 316.78992891311646 and batch: 300, loss is 5.848974571228028 and perplexity is 346.8784988511881
At time: 317.96344900131226 and batch: 350, loss is 5.853192319869995 and perplexity is 348.34463489181945
At time: 319.1418514251709 and batch: 400, loss is 5.831202278137207 and perplexity is 340.76813095861024
At time: 320.3192346096039 and batch: 450, loss is 5.806437969207764 and perplexity is 332.43287805557424
At time: 321.4921078681946 and batch: 500, loss is 5.805744724273682 and perplexity is 332.20250051019946
At time: 322.66288924217224 and batch: 550, loss is 5.81583420753479 and perplexity is 335.5712178095471
At time: 323.83216190338135 and batch: 600, loss is 5.841110210418702 and perplexity is 344.1612200146992
At time: 325.0047445297241 and batch: 650, loss is 5.83205280303955 and perplexity is 341.05808602944046
At time: 326.1778314113617 and batch: 700, loss is 5.842881641387939 and perplexity is 344.7714181606253
At time: 327.3960921764374 and batch: 750, loss is 5.809570531845093 and perplexity is 333.4758776466576
At time: 328.574227809906 and batch: 800, loss is 5.831440515518189 and perplexity is 340.84932433691677
At time: 329.7472801208496 and batch: 850, loss is 5.844750423431396 and perplexity is 345.4163232017753
At time: 330.9164171218872 and batch: 900, loss is 5.829020986557007 and perplexity is 340.0256264076174
At time: 332.0876364707947 and batch: 950, loss is 5.806860990524292 and perplexity is 332.57353399744005
At time: 333.2596914768219 and batch: 1000, loss is 5.808129901885986 and perplexity is 332.9958081909292
At time: 334.4356746673584 and batch: 1050, loss is 5.811437158584595 and perplexity is 334.09893396273225
At time: 335.609486579895 and batch: 1100, loss is 5.7909417152404785 and perplexity is 327.3211225724753
At time: 336.7827818393707 and batch: 1150, loss is 5.842158222198487 and perplexity is 344.5220940945333
At time: 337.95636916160583 and batch: 1200, loss is 5.8392652320861815 and perplexity is 343.52683541375086
At time: 339.1382944583893 and batch: 1250, loss is 5.835931224822998 and perplexity is 342.3834215835348
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.520852638857208 and perplexity of 249.84797648978252
Finished 11 epochs...
Completing Train Step...
At time: 342.12859201431274 and batch: 50, loss is 5.813392028808594 and perplexity is 334.7526928192711
At time: 343.29952597618103 and batch: 100, loss is 5.834370250701904 and perplexity is 341.8493868385973
At time: 344.47007942199707 and batch: 150, loss is 5.750208492279053 and perplexity is 314.25617344177056
At time: 345.63677167892456 and batch: 200, loss is 5.784089508056641 and perplexity is 325.08591721580865
At time: 346.8048059940338 and batch: 250, loss is 5.804410228729248 and perplexity is 331.7594734280005
At time: 347.97270607948303 and batch: 300, loss is 5.823263196945191 and perplexity is 338.07345587495956
At time: 349.14175939559937 and batch: 350, loss is 5.828482685089111 and perplexity is 339.8426393693143
At time: 350.31109285354614 and batch: 400, loss is 5.806247911453247 and perplexity is 332.36970261292504
At time: 351.4782540798187 and batch: 450, loss is 5.782406435012818 and perplexity is 324.53923405451303
At time: 352.6485574245453 and batch: 500, loss is 5.780503005981445 and perplexity is 323.92208419140724
At time: 353.82234144210815 and batch: 550, loss is 5.791782464981079 and perplexity is 327.5964334389237
At time: 354.98405957221985 and batch: 600, loss is 5.81375789642334 and perplexity is 334.87519039610424
At time: 356.1435422897339 and batch: 650, loss is 5.80519362449646 and perplexity is 332.01947422368085
At time: 357.3446695804596 and batch: 700, loss is 5.818407211303711 and perplexity is 336.4357555702091
At time: 358.50293016433716 and batch: 750, loss is 5.787180299758911 and perplexity is 326.092244444068
At time: 359.65975880622864 and batch: 800, loss is 5.8019255256652835 and perplexity is 330.93617290011775
At time: 360.82375597953796 and batch: 850, loss is 5.8193784427642825 and perplexity is 336.76267129019806
At time: 361.98815536499023 and batch: 900, loss is 5.803852300643921 and perplexity is 331.57442712626647
At time: 363.1480619907379 and batch: 950, loss is 5.785311594009399 and perplexity is 325.4834430045435
At time: 364.307799577713 and batch: 1000, loss is 5.783950328826904 and perplexity is 325.04067515670283
At time: 365.46899938583374 and batch: 1050, loss is 5.787368488311768 and perplexity is 326.15361704627713
At time: 366.6268651485443 and batch: 1100, loss is 5.768909149169922 and perplexity is 320.18826443912343
At time: 367.7848331928253 and batch: 1150, loss is 5.8157719707489015 and perplexity is 335.5503335854042
At time: 368.9434232711792 and batch: 1200, loss is 5.800202102661133 and perplexity is 330.36632107575474
At time: 370.11216259002686 and batch: 1250, loss is 5.79305214881897 and perplexity is 328.012641506226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.478145404453695 and perplexity of 239.40230096341708
Finished 12 epochs...
Completing Train Step...
At time: 373.087988615036 and batch: 50, loss is 5.764788093566895 and perplexity is 318.8714659614149
At time: 374.293826341629 and batch: 100, loss is 5.782718849182129 and perplexity is 324.6406405493172
At time: 375.46527576446533 and batch: 150, loss is 5.696754789352417 and perplexity is 297.89908533912853
At time: 376.6352496147156 and batch: 200, loss is 5.731196794509888 and perplexity is 308.3380649498563
At time: 377.8067967891693 and batch: 250, loss is 5.7473234272003175 and perplexity is 313.3508305445612
At time: 378.97927141189575 and batch: 300, loss is 5.75994104385376 and perplexity is 317.329619834875
At time: 380.14914536476135 and batch: 350, loss is 5.772717781066895 and perplexity is 321.41006890090057
At time: 381.3156142234802 and batch: 400, loss is 5.748082056045532 and perplexity is 313.5886377155688
At time: 382.4789309501648 and batch: 450, loss is 5.7203703880310055 and perplexity is 305.01787700808944
At time: 383.649112701416 and batch: 500, loss is 5.721203966140747 and perplexity is 305.27223923410634
At time: 384.8221275806427 and batch: 550, loss is 5.734493083953858 and perplexity is 309.3561134279481
At time: 385.9939420223236 and batch: 600, loss is 5.75627028465271 and perplexity is 316.1669145242787
At time: 387.2098913192749 and batch: 650, loss is 5.746264724731446 and perplexity is 313.01926079446713
At time: 388.3879029750824 and batch: 700, loss is 5.7610822105407715 and perplexity is 317.69195252751075
At time: 389.5587251186371 and batch: 750, loss is 5.733257150650024 and perplexity is 308.97400608285477
At time: 390.7283744812012 and batch: 800, loss is 5.746939706802368 and perplexity is 313.2306145053151
At time: 391.90327620506287 and batch: 850, loss is 5.7632881546020505 and perplexity is 318.3935367467725
At time: 393.081494808197 and batch: 900, loss is 5.747634382247925 and perplexity is 313.4482837179837
At time: 394.25524973869324 and batch: 950, loss is 5.725015907287598 and perplexity is 306.43813980421436
At time: 395.42520356178284 and batch: 1000, loss is 5.726557884216309 and perplexity is 306.91102484108
At time: 396.5942392349243 and batch: 1050, loss is 5.7330467319488525 and perplexity is 308.9089990133905
At time: 397.76416659355164 and batch: 1100, loss is 5.710134220123291 and perplexity is 301.91158817402476
At time: 398.9359233379364 and batch: 1150, loss is 5.750406618118286 and perplexity is 314.31844187815744
At time: 400.10792088508606 and batch: 1200, loss is 5.754375114440918 and perplexity is 315.5682918310374
At time: 401.2825655937195 and batch: 1250, loss is 5.75468879699707 and perplexity is 315.6672956265296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.4510582694171985 and perplexity of 233.00461713914515
Finished 13 epochs...
Completing Train Step...
At time: 404.30128741264343 and batch: 50, loss is 5.729247169494629 and perplexity is 307.7375069669354
At time: 405.4607071876526 and batch: 100, loss is 5.75089204788208 and perplexity is 314.4710584444693
At time: 406.6251652240753 and batch: 150, loss is 5.666134128570556 and perplexity is 288.91546265910375
At time: 407.79442501068115 and batch: 200, loss is 5.7044880580902095 and perplexity is 300.21174972496664
At time: 408.96685338020325 and batch: 250, loss is 5.717381687164306 and perplexity is 304.10763071920474
At time: 410.1380729675293 and batch: 300, loss is 5.733989057540893 and perplexity is 309.2002290639869
At time: 411.3088779449463 and batch: 350, loss is 5.747513618469238 and perplexity is 313.41043280437486
At time: 412.47864413261414 and batch: 400, loss is 5.724028882980346 and perplexity is 306.1358271310802
At time: 413.6484568119049 and batch: 450, loss is 5.693014841079712 and perplexity is 296.787038963637
At time: 414.8172969818115 and batch: 500, loss is 5.6977471923828125 and perplexity is 298.19486803771895
At time: 415.9885036945343 and batch: 550, loss is 5.709700174331665 and perplexity is 301.78057315505043
At time: 417.2086889743805 and batch: 600, loss is 5.7278244590759275 and perplexity is 307.29999690840486
At time: 418.3816611766815 and batch: 650, loss is 5.724637107849121 and perplexity is 306.3220831914063
At time: 419.5512022972107 and batch: 700, loss is 5.736771459579468 and perplexity is 310.06174639965275
At time: 420.7214844226837 and batch: 750, loss is 5.712025537490844 and perplexity is 302.4831391259511
At time: 421.8918488025665 and batch: 800, loss is 5.7226864624023435 and perplexity is 305.7251398163015
At time: 423.06324529647827 and batch: 850, loss is 5.74242431640625 and perplexity is 311.8194443864742
At time: 424.2443220615387 and batch: 900, loss is 5.727824621200561 and perplexity is 307.30004672930835
At time: 425.4172387123108 and batch: 950, loss is 5.707563610076904 and perplexity is 301.13648787947017
At time: 426.59038615226746 and batch: 1000, loss is 5.707965459823608 and perplexity is 301.25752381834985
At time: 427.75967359542847 and batch: 1050, loss is 5.712306089401245 and perplexity is 302.56801325373965
At time: 428.93071937561035 and batch: 1100, loss is 5.684936656951904 and perplexity is 294.3991963177076
At time: 430.10241508483887 and batch: 1150, loss is 5.727341012954712 and perplexity is 307.15146982216464
At time: 431.27602434158325 and batch: 1200, loss is 5.728535165786743 and perplexity is 307.51847470606367
At time: 432.4492259025574 and batch: 1250, loss is 5.7296209812164305 and perplexity is 307.85256435783856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.431690967866104 and perplexity of 228.53536483244307
Finished 14 epochs...
Completing Train Step...
At time: 435.42318177223206 and batch: 50, loss is 5.700618896484375 and perplexity is 299.05242620064047
At time: 436.6191940307617 and batch: 100, loss is 5.724278049468994 and perplexity is 306.21211542402415
At time: 437.7917904853821 and batch: 150, loss is 5.641765241622925 and perplexity is 281.96000696927587
At time: 438.96633887290955 and batch: 200, loss is 5.673768749237061 and perplexity is 291.129664160465
At time: 440.1456387042999 and batch: 250, loss is 5.6909003162384035 and perplexity is 296.1601384291304
At time: 441.31822395324707 and batch: 300, loss is 5.7007845020294186 and perplexity is 299.10195504169013
At time: 442.48909187316895 and batch: 350, loss is 5.716261196136474 and perplexity is 303.7670716797937
At time: 443.6605236530304 and batch: 400, loss is 5.696408052444458 and perplexity is 297.7958106370043
At time: 444.82952094078064 and batch: 450, loss is 5.664542922973633 and perplexity is 288.4561043215837
At time: 445.9959478378296 and batch: 500, loss is 5.6627716636657714 and perplexity is 287.9456259901056
At time: 447.1917064189911 and batch: 550, loss is 5.675040588378907 and perplexity is 291.5001698245563
At time: 448.3590774536133 and batch: 600, loss is 5.6945272636413575 and perplexity is 297.23624598716094
At time: 449.5224230289459 and batch: 650, loss is 5.686344556808471 and perplexity is 294.81397281735053
At time: 450.6905539035797 and batch: 700, loss is 5.703189449310303 and perplexity is 299.8221451376635
At time: 451.8650641441345 and batch: 750, loss is 5.680273971557617 and perplexity is 293.02970072962637
At time: 453.04207158088684 and batch: 800, loss is 5.687071952819824 and perplexity is 295.02849733795045
At time: 454.21489119529724 and batch: 850, loss is 5.707228479385376 and perplexity is 301.03558470886173
At time: 455.3868000507355 and batch: 900, loss is 5.683561840057373 and perplexity is 293.994729426539
At time: 456.56078028678894 and batch: 950, loss is 5.666636571884156 and perplexity is 289.0606627758715
At time: 457.73535895347595 and batch: 1000, loss is 5.667261991500855 and perplexity is 289.2415035295703
At time: 458.9076898097992 and batch: 1050, loss is 5.667671489715576 and perplexity is 289.3599716634782
At time: 460.0785837173462 and batch: 1100, loss is 5.64382963180542 and perplexity is 282.5426836686152
At time: 461.2496659755707 and batch: 1150, loss is 5.686156797409057 and perplexity is 294.75862391917605
At time: 462.4218394756317 and batch: 1200, loss is 5.689350233078003 and perplexity is 295.70142120254263
At time: 463.59464025497437 and batch: 1250, loss is 5.68935396194458 and perplexity is 295.70252383574456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.395548354100137 and perplexity of 220.42298380975
Finished 15 epochs...
Completing Train Step...
At time: 466.5450623035431 and batch: 50, loss is 5.658063764572144 and perplexity is 286.5931930952706
At time: 467.71694445610046 and batch: 100, loss is 5.679290399551392 and perplexity is 292.7416266130385
At time: 468.88694882392883 and batch: 150, loss is 5.593498430252075 and perplexity is 268.6739144275344
At time: 470.0550627708435 and batch: 200, loss is 5.627663793563843 and perplexity is 278.01186525795333
At time: 471.22420620918274 and batch: 250, loss is 5.643442115783691 and perplexity is 282.43321506366055
At time: 472.3954327106476 and batch: 300, loss is 5.645685663223267 and perplexity is 283.06757872656414
At time: 473.5682849884033 and batch: 350, loss is 5.667143049240113 and perplexity is 289.20710253714657
At time: 474.74006605148315 and batch: 400, loss is 5.646601877212524 and perplexity is 283.3270480486591
At time: 475.913765668869 and batch: 450, loss is 5.611234216690064 and perplexity is 273.4815653067694
At time: 477.1294963359833 and batch: 500, loss is 5.610836277008056 and perplexity is 273.37275779048645
At time: 478.29640769958496 and batch: 550, loss is 5.6241453266143795 and perplexity is 277.0354085212083
At time: 479.6652591228485 and batch: 600, loss is 5.641796102523804 and perplexity is 281.9687086433729
At time: 480.8370668888092 and batch: 650, loss is 5.632658777236938 and perplexity is 279.40400394638203
At time: 482.0076913833618 and batch: 700, loss is 5.6461099433898925 and perplexity is 283.18770416765165
At time: 483.1803665161133 and batch: 750, loss is 5.621842670440674 and perplexity is 276.39822511613335
At time: 484.35681891441345 and batch: 800, loss is 5.635963344573975 and perplexity is 280.32884054245045
At time: 485.5294680595398 and batch: 850, loss is 5.657356929779053 and perplexity is 286.39069063125265
At time: 486.7025361061096 and batch: 900, loss is 5.641555843353271 and perplexity is 281.9009712129133
At time: 487.87384057044983 and batch: 950, loss is 5.6231352233886716 and perplexity is 276.7557154446462
At time: 489.04563212394714 and batch: 1000, loss is 5.619585657119751 and perplexity is 275.77509411218006
At time: 490.2190990447998 and batch: 1050, loss is 5.619256134033203 and perplexity is 275.684234822914
At time: 491.3976809978485 and batch: 1100, loss is 5.59195692062378 and perplexity is 268.26007005606397
At time: 492.57077503204346 and batch: 1150, loss is 5.631901569366455 and perplexity is 279.19251711540085
At time: 493.7412443161011 and batch: 1200, loss is 5.631717596054077 and perplexity is 279.14115786774624
At time: 494.91043853759766 and batch: 1250, loss is 5.632779598236084 and perplexity is 279.4377638567162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.353312471487226 and perplexity of 211.30708857151893
Finished 16 epochs...
Completing Train Step...
At time: 497.8886640071869 and batch: 50, loss is 5.6059716796875 and perplexity is 272.0461387552536
At time: 499.06170439720154 and batch: 100, loss is 5.629293928146362 and perplexity is 278.46543160051124
At time: 500.2334589958191 and batch: 150, loss is 5.549183626174926 and perplexity is 257.02763959505666
At time: 501.40378856658936 and batch: 200, loss is 5.593773384094238 and perplexity is 268.7477975093483
At time: 502.57291293144226 and batch: 250, loss is 5.607011680603027 and perplexity is 272.32921416245136
At time: 503.74044489860535 and batch: 300, loss is 5.602877655029297 and perplexity is 271.2057220991459
At time: 504.9089517593384 and batch: 350, loss is 5.627209157943725 and perplexity is 277.88549988846734
At time: 506.1247808933258 and batch: 400, loss is 5.610430259704589 and perplexity is 273.26178625023425
At time: 507.2962791919708 and batch: 450, loss is 5.576349229812622 and perplexity is 264.10565457610176
At time: 508.4668970108032 and batch: 500, loss is 5.57659029006958 and perplexity is 264.1693276272707
At time: 509.63658809661865 and batch: 550, loss is 5.5826942729949955 and perplexity is 265.7867440116357
At time: 510.80676770210266 and batch: 600, loss is 5.5947486019134525 and perplexity is 269.01001298817613
At time: 511.9758117198944 and batch: 650, loss is 5.578600311279297 and perplexity is 264.70084758299714
At time: 513.1526775360107 and batch: 700, loss is 5.592445001602173 and perplexity is 268.3910346515849
At time: 514.3232066631317 and batch: 750, loss is 5.582940464019775 and perplexity is 265.85218637784783
At time: 515.493081331253 and batch: 800, loss is 5.598020334243774 and perplexity is 269.89158308917274
At time: 516.6623775959015 and batch: 850, loss is 5.6197551822662355 and perplexity is 275.8218488883492
At time: 517.8325796127319 and batch: 900, loss is 5.612562847137451 and perplexity is 273.8451627310525
At time: 519.0022060871124 and batch: 950, loss is 5.599807462692261 and perplexity is 270.37434526584013
At time: 520.1670963764191 and batch: 1000, loss is 5.5890114212036135 and perplexity is 267.47107274126955
At time: 521.3326654434204 and batch: 1050, loss is 5.584302272796631 and perplexity is 266.21447284505973
At time: 522.4973061084747 and batch: 1100, loss is 5.549876565933228 and perplexity is 257.20580598767384
At time: 523.6572012901306 and batch: 1150, loss is 5.585325841903686 and perplexity is 266.48710125852335
At time: 524.8172090053558 and batch: 1200, loss is 5.579539632797241 and perplexity is 264.9496035975981
At time: 525.9863021373749 and batch: 1250, loss is 5.591549978256226 and perplexity is 268.1509258772367
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.3327471879276915 and perplexity of 207.00587777854912
Finished 17 epochs...
Completing Train Step...
At time: 528.9822881221771 and batch: 50, loss is 5.572457227706909 and perplexity is 263.07975251385693
At time: 530.1573598384857 and batch: 100, loss is 5.603441944122315 and perplexity is 271.35880371717207
At time: 531.3280699253082 and batch: 150, loss is 5.523029623031616 and perplexity is 250.39248405774092
At time: 532.5006418228149 and batch: 200, loss is 5.5671462726593015 and perplexity is 261.6862514604976
At time: 533.6714677810669 and batch: 250, loss is 5.5790384483337405 and perplexity is 264.81684824290386
At time: 534.8422939777374 and batch: 300, loss is 5.578693141937256 and perplexity is 264.72542107740975
At time: 536.0131080150604 and batch: 350, loss is 5.602545862197876 and perplexity is 271.1157529111042
At time: 537.1833641529083 and batch: 400, loss is 5.587126054763794 and perplexity is 266.96726683570984
At time: 538.3554217815399 and batch: 450, loss is 5.549059953689575 and perplexity is 256.99585431358685
At time: 539.5340015888214 and batch: 500, loss is 5.5552076244354245 and perplexity is 258.58064660828813
At time: 540.7072520256042 and batch: 550, loss is 5.558210887908936 and perplexity is 259.3583997333952
At time: 541.873382806778 and batch: 600, loss is 5.570279321670532 and perplexity is 262.50741300986897
At time: 543.049173116684 and batch: 650, loss is 5.54956127166748 and perplexity is 257.124723255061
At time: 544.2211124897003 and batch: 700, loss is 5.57020302772522 and perplexity is 262.48738604763435
At time: 545.4002065658569 and batch: 750, loss is 5.560669927597046 and perplexity is 259.9969571290008
At time: 546.5748331546783 and batch: 800, loss is 5.576121988296509 and perplexity is 264.04564562526264
At time: 547.7480607032776 and batch: 850, loss is 5.603505983352661 and perplexity is 271.376181882546
At time: 548.9198274612427 and batch: 900, loss is 5.595010766983032 and perplexity is 269.0805472623562
At time: 550.091007232666 and batch: 950, loss is 5.579256496429443 and perplexity is 264.87459734818566
At time: 551.2643768787384 and batch: 1000, loss is 5.57129732131958 and perplexity is 262.77478153162616
At time: 552.4362020492554 and batch: 1050, loss is 5.565007085800171 and perplexity is 261.1270539973369
At time: 553.6109306812286 and batch: 1100, loss is 5.5316845703125 and perplexity is 252.56902313559115
At time: 554.7846939563751 and batch: 1150, loss is 5.563855495452881 and perplexity is 260.8265156842608
At time: 555.9565522670746 and batch: 1200, loss is 5.5618485069274906 and perplexity is 260.3035648139004
At time: 557.1286914348602 and batch: 1250, loss is 5.574504289627075 and perplexity is 263.618844646429
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.327312385948905 and perplexity of 205.8838934673315
Finished 18 epochs...
Completing Train Step...
At time: 560.0993192195892 and batch: 50, loss is 5.5515824317932125 and perplexity is 257.64493903564636
At time: 561.2951521873474 and batch: 100, loss is 5.579418268203735 and perplexity is 264.9174500478487
At time: 562.4734084606171 and batch: 150, loss is 5.505223693847657 and perplexity is 245.97347227759045
At time: 563.6425294876099 and batch: 200, loss is 5.55428150177002 and perplexity is 258.34128006912664
At time: 564.8116700649261 and batch: 250, loss is 5.5634918308258055 and perplexity is 260.7316795520217
At time: 565.9819839000702 and batch: 300, loss is 5.566143884658813 and perplexity is 261.424071727003
At time: 567.1544392108917 and batch: 350, loss is 5.587470788955688 and perplexity is 267.05931544594466
At time: 568.3243594169617 and batch: 400, loss is 5.570425930023194 and perplexity is 262.545901610558
At time: 569.4979295730591 and batch: 450, loss is 5.5299022769927975 and perplexity is 252.1192719662007
At time: 570.6697869300842 and batch: 500, loss is 5.536709222793579 and perplexity is 253.84128837292093
At time: 571.8384702205658 and batch: 550, loss is 5.546080884933471 and perplexity is 256.2313852618655
At time: 573.0064995288849 and batch: 600, loss is 5.553685293197632 and perplexity is 258.18730068982194
At time: 574.1822674274445 and batch: 650, loss is 5.5364661693573 and perplexity is 253.77959887273843
At time: 575.3595108985901 and batch: 700, loss is 5.5574519824981685 and perplexity is 259.16164590872387
At time: 576.5329163074493 and batch: 750, loss is 5.54562762260437 and perplexity is 256.11527154436794
At time: 577.7026443481445 and batch: 800, loss is 5.564512062072754 and perplexity is 260.99782189885417
At time: 578.8740847110748 and batch: 850, loss is 5.587846784591675 and perplexity is 267.15974746291886
At time: 580.0464589595795 and batch: 900, loss is 5.578376007080078 and perplexity is 264.64148072971363
At time: 581.2183663845062 and batch: 950, loss is 5.564425010681152 and perplexity is 260.97510266413764
At time: 582.4005787372589 and batch: 1000, loss is 5.554461812973022 and perplexity is 258.3878660959861
At time: 583.5812346935272 and batch: 1050, loss is 5.551800231933594 and perplexity is 257.7010602509194
At time: 584.7666985988617 and batch: 1100, loss is 5.513840198516846 and perplexity is 248.10206117847827
At time: 585.9383838176727 and batch: 1150, loss is 5.548143463134766 and perplexity is 256.76042793998613
At time: 587.1087825298309 and batch: 1200, loss is 5.541390581130981 and perplexity is 255.03239623070135
At time: 588.3267800807953 and batch: 1250, loss is 5.558923645019531 and perplexity is 259.54332517262986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.313293011519161 and perplexity of 203.01766836175625
Finished 19 epochs...
Completing Train Step...
At time: 591.333384513855 and batch: 50, loss is 5.532890310287476 and perplexity is 252.87373937058257
At time: 592.505631685257 and batch: 100, loss is 5.5653953361511235 and perplexity is 261.22845635117045
At time: 593.681978225708 and batch: 150, loss is 5.487340593338013 and perplexity is 241.61380234444997
At time: 594.8548038005829 and batch: 200, loss is 5.535638551712037 and perplexity is 253.56965328802235
At time: 596.0286998748779 and batch: 250, loss is 5.548006420135498 and perplexity is 256.72524313181884
At time: 597.2037489414215 and batch: 300, loss is 5.548397979736328 and perplexity is 256.8257860485301
At time: 598.3760778903961 and batch: 350, loss is 5.566976165771484 and perplexity is 261.6417406125853
At time: 599.547070980072 and batch: 400, loss is 5.552750616073609 and perplexity is 257.9460916699907
At time: 600.725430727005 and batch: 450, loss is 5.5102645301818844 and perplexity is 247.2165146483329
At time: 601.8963897228241 and batch: 500, loss is 5.515642194747925 and perplexity is 248.5495432170301
At time: 603.0701837539673 and batch: 550, loss is 5.526021203994751 and perplexity is 251.14267501536432
At time: 604.236419916153 and batch: 600, loss is 5.533001194000244 and perplexity is 252.90178050428932
At time: 605.4006524085999 and batch: 650, loss is 5.518377008438111 and perplexity is 249.2302102350117
At time: 606.5669128894806 and batch: 700, loss is 5.538656244277954 and perplexity is 254.3360042699599
At time: 607.7329235076904 and batch: 750, loss is 5.529147758483886 and perplexity is 251.92911505652432
At time: 608.8926351070404 and batch: 800, loss is 5.539973907470703 and perplexity is 254.6713543520866
At time: 610.0591161251068 and batch: 850, loss is 5.565078620910644 and perplexity is 261.1457344181372
At time: 611.2445561885834 and batch: 900, loss is 5.561390113830567 and perplexity is 260.18427080055
At time: 612.4277667999268 and batch: 950, loss is 5.543230209350586 and perplexity is 255.50199283286602
At time: 613.6130928993225 and batch: 1000, loss is 5.530691337585449 and perplexity is 252.31828785582934
At time: 614.7868683338165 and batch: 1050, loss is 5.526669273376465 and perplexity is 251.3054856440769
At time: 615.9605734348297 and batch: 1100, loss is 5.4929094314575195 and perplexity is 242.96306391985664
At time: 617.1349294185638 and batch: 1150, loss is 5.530972843170166 and perplexity is 252.38932686145617
At time: 618.3590657711029 and batch: 1200, loss is 5.524409370422363 and perplexity is 250.7382008803792
At time: 619.5394477844238 and batch: 1250, loss is 5.5387764739990235 and perplexity is 254.366584855122
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.306460192603787 and perplexity of 201.63521380663408
Finished 20 epochs...
Completing Train Step...
At time: 622.4959146976471 and batch: 50, loss is 5.513397607803345 and perplexity is 247.9922778065428
At time: 623.6967813968658 and batch: 100, loss is 5.548199663162231 and perplexity is 256.7748582885776
At time: 624.8701837062836 and batch: 150, loss is 5.464843063354492 and perplexity is 236.23877767822546
At time: 626.0479202270508 and batch: 200, loss is 5.511152362823486 and perplexity is 247.43609900223524
At time: 627.2247891426086 and batch: 250, loss is 5.531736631393432 and perplexity is 252.5821724942276
At time: 628.4042019844055 and batch: 300, loss is 5.531667728424072 and perplexity is 252.56476943210345
At time: 629.5773856639862 and batch: 350, loss is 5.547474040985107 and perplexity is 256.58860434004566
At time: 630.7509574890137 and batch: 400, loss is 5.534500761032104 and perplexity is 253.2813081690989
At time: 631.9275567531586 and batch: 450, loss is 5.491468105316162 and perplexity is 242.6131271515312
At time: 633.103776216507 and batch: 500, loss is 5.492380542755127 and perplexity is 242.83459747547084
At time: 634.2825648784637 and batch: 550, loss is 5.506274070739746 and perplexity is 246.2319728666937
At time: 635.4558613300323 and batch: 600, loss is 5.515705156326294 and perplexity is 248.5651927812293
At time: 636.6286647319794 and batch: 650, loss is 5.499808015823365 and perplexity is 244.6449597942051
At time: 637.8055906295776 and batch: 700, loss is 5.518876218795777 and perplexity is 249.35465959802528
At time: 638.9834756851196 and batch: 750, loss is 5.512086009979248 and perplexity is 247.66722489052626
At time: 640.1573693752289 and batch: 800, loss is 5.515332984924316 and perplexity is 248.4727011373891
At time: 641.3304762840271 and batch: 850, loss is 5.546930837631225 and perplexity is 256.4492623985316
At time: 642.5046846866608 and batch: 900, loss is 5.538390817642212 and perplexity is 254.2685056783569
At time: 643.6796205043793 and batch: 950, loss is 5.521205844879151 and perplexity is 249.936239886313
At time: 644.8574016094208 and batch: 1000, loss is 5.510509071350097 and perplexity is 247.27697665605018
At time: 646.0330471992493 and batch: 1050, loss is 5.507285671234131 and perplexity is 246.48118728364022
At time: 647.2525660991669 and batch: 1100, loss is 5.468351135253906 and perplexity is 237.0689756414159
At time: 648.4250938892365 and batch: 1150, loss is 5.513963623046875 and perplexity is 248.13268494860185
At time: 649.600988149643 and batch: 1200, loss is 5.509937210083008 and perplexity is 247.1356089560662
At time: 650.7811131477356 and batch: 1250, loss is 5.5190354347229 and perplexity is 249.39436399203777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.29930125883896 and perplexity of 200.19687529434484
Finished 21 epochs...
Completing Train Step...
At time: 653.792985200882 and batch: 50, loss is 5.4954273128509525 and perplexity is 243.57558690430238
At time: 654.9948034286499 and batch: 100, loss is 5.533121280670166 and perplexity is 252.9321524605244
At time: 656.1720383167267 and batch: 150, loss is 5.4496669006347656 and perplexity is 232.68064722177436
At time: 657.3492925167084 and batch: 200, loss is 5.490810060501099 and perplexity is 242.4535293581522
At time: 658.5254809856415 and batch: 250, loss is 5.515098581314087 and perplexity is 248.41446506483848
At time: 659.6991965770721 and batch: 300, loss is 5.508987035751343 and perplexity is 246.90089856954668
At time: 660.8728909492493 and batch: 350, loss is 5.523300638198853 and perplexity is 250.4603534150799
At time: 662.0469160079956 and batch: 400, loss is 5.510208787918091 and perplexity is 247.20273462422776
At time: 663.2234601974487 and batch: 450, loss is 5.469988050460816 and perplexity is 237.4573552382423
At time: 664.3999471664429 and batch: 500, loss is 5.474063873291016 and perplexity is 238.42716439049744
At time: 665.5737066268921 and batch: 550, loss is 5.485991878509521 and perplexity is 241.2881538792804
At time: 666.7472867965698 and batch: 600, loss is 5.500495567321777 and perplexity is 244.81322364129215
At time: 667.9285395145416 and batch: 650, loss is 5.479948225021363 and perplexity is 239.83428963757152
At time: 669.1059951782227 and batch: 700, loss is 5.498108749389648 and perplexity is 244.22959583281477
At time: 670.2936046123505 and batch: 750, loss is 5.5057816982269285 and perplexity is 246.110764853673
At time: 671.4705364704132 and batch: 800, loss is 5.528911495208741 and perplexity is 251.8696004895266
At time: 672.6446702480316 and batch: 850, loss is 5.529473524093628 and perplexity is 252.0111982675202
At time: 673.8185513019562 and batch: 900, loss is 5.517701663970947 and perplexity is 249.061950814409
At time: 674.9799902439117 and batch: 950, loss is 5.499571905136109 and perplexity is 244.58720332334272
At time: 676.1500854492188 and batch: 1000, loss is 5.489156923294067 and perplexity is 242.0530515214046
At time: 677.3613545894623 and batch: 1050, loss is 5.487567577362061 and perplexity is 241.66865104222512
At time: 678.5215380191803 and batch: 1100, loss is 5.448056144714355 and perplexity is 232.30615717869298
At time: 679.6852493286133 and batch: 1150, loss is 5.492889356613159 and perplexity is 242.95818652311985
At time: 680.8617060184479 and batch: 1200, loss is 5.4876626777648925 and perplexity is 241.6916349211618
At time: 682.0398941040039 and batch: 1250, loss is 5.505316238403321 and perplexity is 245.99623683663927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.2740514156592155 and perplexity of 195.2052200457757
Finished 22 epochs...
Completing Train Step...
At time: 685.0098035335541 and batch: 50, loss is 5.470037288665772 and perplexity is 237.46904750001843
At time: 686.1831350326538 and batch: 100, loss is 5.506422004699707 and perplexity is 246.2684016319682
At time: 687.3598773479462 and batch: 150, loss is 5.421508703231812 and perplexity is 226.22016426486624
At time: 688.5358288288116 and batch: 200, loss is 5.4637298583984375 and perplexity is 235.97594182233314
At time: 689.7080850601196 and batch: 250, loss is 5.486425466537476 and perplexity is 241.39279621828302
At time: 690.8808562755585 and batch: 300, loss is 5.479240303039551 and perplexity is 239.66456575466674
At time: 692.053697347641 and batch: 350, loss is 5.500365524291992 and perplexity is 244.7813894579102
At time: 693.2282812595367 and batch: 400, loss is 5.485631971359253 and perplexity is 241.20132817295774
At time: 694.4185755252838 and batch: 450, loss is 5.444767999649048 and perplexity is 231.54355529423455
At time: 695.5943896770477 and batch: 500, loss is 5.449433145523071 and perplexity is 232.6262632876028
At time: 696.767986536026 and batch: 550, loss is 5.462772808074951 and perplexity is 235.750209006939
At time: 697.9607057571411 and batch: 600, loss is 5.471206874847412 and perplexity is 237.74695050061698
At time: 699.1393682956696 and batch: 650, loss is 5.458138666152954 and perplexity is 234.66023657435264
At time: 700.3123803138733 and batch: 700, loss is 5.479894409179687 and perplexity is 239.82138310070326
At time: 701.4846849441528 and batch: 750, loss is 5.473799467086792 and perplexity is 238.36413110254057
At time: 702.658121585846 and batch: 800, loss is 5.4819371700286865 and perplexity is 240.3117815456953
At time: 703.832501411438 and batch: 850, loss is 5.507654628753662 and perplexity is 246.572145149874
At time: 705.0103356838226 and batch: 900, loss is 5.492517786026001 and perplexity is 242.86792717699612
At time: 706.1859900951385 and batch: 950, loss is 5.477784833908081 and perplexity is 239.31599510574762
At time: 707.3903861045837 and batch: 1000, loss is 5.466567783355713 and perplexity is 236.64657499015303
At time: 708.5655860900879 and batch: 1050, loss is 5.457009449005127 and perplexity is 234.3954037663151
At time: 709.7422626018524 and batch: 1100, loss is 5.4166200065612795 and perplexity is 225.11694135928218
At time: 710.9176089763641 and batch: 1150, loss is 5.466553812026977 and perplexity is 236.64326874615602
At time: 712.0954742431641 and batch: 1200, loss is 5.460351171493531 and perplexity is 235.17999837684903
At time: 713.2742624282837 and batch: 1250, loss is 5.47404372215271 and perplexity is 238.42235986014055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.265565746892108 and perplexity of 193.55578139938302
Finished 23 epochs...
Completing Train Step...
At time: 716.2518198490143 and batch: 50, loss is 5.449388017654419 and perplexity is 232.6157655970192
At time: 717.4513993263245 and batch: 100, loss is 5.488820266723633 and perplexity is 241.97157648653865
At time: 718.6272559165955 and batch: 150, loss is 5.399926128387452 and perplexity is 221.3900611592877
At time: 719.8015439510345 and batch: 200, loss is 5.4539232921600345 and perplexity is 233.6731378726995
At time: 720.9845736026764 and batch: 250, loss is 5.468314161300659 and perplexity is 237.06021042623763
At time: 722.1614270210266 and batch: 300, loss is 5.460917282104492 and perplexity is 235.31317396191525
At time: 723.3438351154327 and batch: 350, loss is 5.477110824584961 and perplexity is 239.15474824092408
At time: 724.5218861103058 and batch: 400, loss is 5.458453359603882 and perplexity is 234.73409423464733
At time: 725.697826385498 and batch: 450, loss is 5.421270732879639 and perplexity is 226.16633697761077
At time: 726.872398853302 and batch: 500, loss is 5.423561038970948 and perplexity is 226.68492074793767
At time: 728.0459337234497 and batch: 550, loss is 5.435899610519409 and perplexity is 229.49921534515812
At time: 729.227504491806 and batch: 600, loss is 5.447141227722168 and perplexity is 232.09371352707296
At time: 730.4103045463562 and batch: 650, loss is 5.433839607238769 and perplexity is 229.02693282742612
At time: 731.594886302948 and batch: 700, loss is 5.458244066238404 and perplexity is 234.68497108682618
At time: 732.7682449817657 and batch: 750, loss is 5.455802841186523 and perplexity is 234.1127509992982
At time: 733.9466559886932 and batch: 800, loss is 5.479263973236084 and perplexity is 239.6702387291802
At time: 735.1244623661041 and batch: 850, loss is 5.495670433044434 and perplexity is 243.63481224726456
At time: 736.3006501197815 and batch: 900, loss is 5.475705204010009 and perplexity is 238.818823552835
At time: 737.5034608840942 and batch: 950, loss is 5.463671741485595 and perplexity is 235.96222802759496
At time: 738.6811382770538 and batch: 1000, loss is 5.4546176528930665 and perplexity is 233.8354476682557
At time: 739.8641610145569 and batch: 1050, loss is 5.440196743011475 and perplexity is 230.4875258100359
At time: 741.0374045372009 and batch: 1100, loss is 5.40520544052124 and perplexity is 222.56193902819643
At time: 742.2120921611786 and batch: 1150, loss is 5.449635629653931 and perplexity is 232.67337118347945
At time: 743.3876478672028 and batch: 1200, loss is 5.445197038650512 and perplexity is 231.64291782366962
At time: 744.5615787506104 and batch: 1250, loss is 5.46988962173462 and perplexity is 237.43398376347136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.263568070683166 and perplexity of 193.16950557524993
Finished 24 epochs...
Completing Train Step...
At time: 747.5426917076111 and batch: 50, loss is 5.454296340942383 and perplexity is 233.7603256138797
At time: 748.7218058109283 and batch: 100, loss is 5.473792839050293 and perplexity is 238.3625512216153
At time: 749.9034543037415 and batch: 150, loss is 5.385264501571656 and perplexity is 218.1678022030822
At time: 751.0803279876709 and batch: 200, loss is 5.434478921890259 and perplexity is 229.17339991545734
At time: 752.2556986808777 and batch: 250, loss is 5.453715887069702 and perplexity is 233.62467790002697
At time: 753.43639087677 and batch: 300, loss is 5.445886421203613 and perplexity is 231.80266346636952
At time: 754.6188659667969 and batch: 350, loss is 5.458713283538819 and perplexity is 234.79511517414232
At time: 755.7991759777069 and batch: 400, loss is 5.438234844207764 and perplexity is 230.0357758975858
At time: 756.9738547801971 and batch: 450, loss is 5.402617044448853 and perplexity is 221.9866054960983
At time: 758.1503386497498 and batch: 500, loss is 5.411944980621338 and perplexity is 224.0669700527897
At time: 759.3247790336609 and batch: 550, loss is 5.417165374755859 and perplexity is 225.23974646311564
At time: 760.4947724342346 and batch: 600, loss is 5.428499994277954 and perplexity is 227.80727679195945
At time: 761.6615173816681 and batch: 650, loss is 5.413258085250854 and perplexity is 224.361386686106
At time: 762.8207926750183 and batch: 700, loss is 5.443842725753784 and perplexity is 231.3294131722398
At time: 763.9824855327606 and batch: 750, loss is 5.443724012374878 and perplexity is 231.30195290594438
At time: 765.1573464870453 and batch: 800, loss is 5.440554752349853 and perplexity is 230.57005726929066
At time: 766.3946309089661 and batch: 850, loss is 5.454301061630249 and perplexity is 233.76142912601713
At time: 767.5743296146393 and batch: 900, loss is 5.4399183654785155 and perplexity is 230.42337219110038
At time: 768.7498693466187 and batch: 950, loss is 5.430413761138916 and perplexity is 228.24366424770645
At time: 769.923374414444 and batch: 1000, loss is 5.424955530166626 and perplexity is 227.00125138299492
At time: 771.0969760417938 and batch: 1050, loss is 5.414349336624145 and perplexity is 224.6063549941323
At time: 772.2730376720428 and batch: 1100, loss is 5.3751993656158445 and perplexity is 215.98292758797106
At time: 773.4476633071899 and batch: 1150, loss is 5.410450143814087 and perplexity is 223.73227671694806
At time: 774.6245367527008 and batch: 1200, loss is 5.407814931869507 and perplexity is 223.14347090375054
At time: 775.8013527393341 and batch: 1250, loss is 5.430449724197388 and perplexity is 228.25187273554985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.228942425581661 and perplexity of 186.59536065330053
Finished 25 epochs...
Completing Train Step...
At time: 778.7529907226562 and batch: 50, loss is 5.409578199386597 and perplexity is 223.53727963068093
At time: 779.9590029716492 and batch: 100, loss is 5.442783575057984 and perplexity is 231.08453017021282
At time: 781.1304352283478 and batch: 150, loss is 5.343569002151489 and perplexity is 209.25822217614856
At time: 782.3028383255005 and batch: 200, loss is 5.398384170532227 and perplexity is 221.0489500724608
At time: 783.4749295711517 and batch: 250, loss is 5.412089157104492 and perplexity is 224.09927756945902
At time: 784.6473090648651 and batch: 300, loss is 5.407612276077271 and perplexity is 223.09825416874358
At time: 785.8241808414459 and batch: 350, loss is 5.424721670150757 and perplexity is 226.94817107366714
At time: 787.0023233890533 and batch: 400, loss is 5.409456119537354 and perplexity is 223.5099918989584
At time: 788.1770718097687 and batch: 450, loss is 5.3785855674743654 and perplexity is 216.7155290469665
At time: 789.3519814014435 and batch: 500, loss is 5.37806227684021 and perplexity is 216.60215350710646
At time: 790.5271146297455 and batch: 550, loss is 5.387854223251343 and perplexity is 218.73352831066236
At time: 791.7038855552673 and batch: 600, loss is 5.399523992538452 and perplexity is 221.30105017753618
At time: 792.8917417526245 and batch: 650, loss is 5.385374794006347 and perplexity is 218.19186578814984
At time: 794.0716006755829 and batch: 700, loss is 5.409073944091797 and perplexity is 223.42458818885797
At time: 795.2606451511383 and batch: 750, loss is 5.403539228439331 and perplexity is 222.19141241010158
At time: 796.494122505188 and batch: 800, loss is 5.406812105178833 and perplexity is 222.91980884116572
At time: 797.6696453094482 and batch: 850, loss is 5.436084060668946 and perplexity is 229.54155041398187
At time: 798.8445556163788 and batch: 900, loss is 5.416258201599121 and perplexity is 225.03550766527815
At time: 800.025741815567 and batch: 950, loss is 5.407695960998535 and perplexity is 223.1169249097968
At time: 801.2107903957367 and batch: 1000, loss is 5.397727174758911 and perplexity is 220.90376954328653
At time: 802.4045057296753 and batch: 1050, loss is 5.387329225540161 and perplexity is 218.61872384761764
At time: 803.5918741226196 and batch: 1100, loss is 5.359553527832031 and perplexity is 212.62999188038793
At time: 804.7736961841583 and batch: 1150, loss is 5.400042896270752 and perplexity is 221.41591391746957
At time: 805.9600629806519 and batch: 1200, loss is 5.396817417144775 and perplexity is 220.7028920457245
At time: 807.1471254825592 and batch: 1250, loss is 5.411421499252319 and perplexity is 223.9497058640415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.214649506729015 and perplexity of 183.94733737580623
Finished 26 epochs...
Completing Train Step...
At time: 810.150269985199 and batch: 50, loss is 5.391563453674316 and perplexity is 219.5463679411931
At time: 811.358870267868 and batch: 100, loss is 5.418910923004151 and perplexity is 225.63325665359508
At time: 812.5327019691467 and batch: 150, loss is 5.335159835815429 and perplexity is 207.50591303426927
At time: 813.7166063785553 and batch: 200, loss is 5.387593555450439 and perplexity is 218.67651895342848
At time: 814.9020230770111 and batch: 250, loss is 5.398129978179932 and perplexity is 220.99276826066549
At time: 816.0806879997253 and batch: 300, loss is 5.401797800064087 and perplexity is 221.80481869012976
At time: 817.2567226886749 and batch: 350, loss is 5.408265151977539 and perplexity is 223.24395720010574
At time: 818.4324178695679 and batch: 400, loss is 5.398879976272583 and perplexity is 221.15857458479198
At time: 819.6079926490784 and batch: 450, loss is 5.357832117080688 and perplexity is 212.26428318405527
At time: 820.7893726825714 and batch: 500, loss is 5.366114206314087 and perplexity is 214.02957497999373
At time: 821.9795105457306 and batch: 550, loss is 5.374748878479004 and perplexity is 215.8856519696679
At time: 823.1662557125092 and batch: 600, loss is 5.384219779968261 and perplexity is 217.9399966043905
At time: 824.3464891910553 and batch: 650, loss is 5.372326412200928 and perplexity is 215.36330919217352
At time: 825.5213735103607 and batch: 700, loss is 5.396197519302368 and perplexity is 220.56612119548964
At time: 826.744068145752 and batch: 750, loss is 5.383806505203247 and perplexity is 217.84994611258622
At time: 827.9285516738892 and batch: 800, loss is 5.392549991607666 and perplexity is 219.76306563396798
At time: 829.10799908638 and batch: 850, loss is 5.417810869216919 and perplexity is 225.38518440651174
At time: 830.285543680191 and batch: 900, loss is 5.401254615783691 and perplexity is 221.68437051504043
At time: 831.459890127182 and batch: 950, loss is 5.391294889450073 and perplexity is 219.48741355807493
At time: 832.635463476181 and batch: 1000, loss is 5.383410301208496 and perplexity is 217.76365019020355
At time: 833.8107583522797 and batch: 1050, loss is 5.370686044692993 and perplexity is 215.01032380940103
At time: 834.9898800849915 and batch: 1100, loss is 5.342264194488525 and perplexity is 208.98535850033545
At time: 836.1683268547058 and batch: 1150, loss is 5.378411016464233 and perplexity is 216.6777044337183
At time: 837.3471705913544 and batch: 1200, loss is 5.373533096313476 and perplexity is 215.62334153269617
At time: 838.5246846675873 and batch: 1250, loss is 5.392649965286255 and perplexity is 219.78503725433094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.20159912109375 and perplexity of 181.56235003806472
Finished 27 epochs...
Completing Train Step...
At time: 841.5134923458099 and batch: 50, loss is 5.369865226745605 and perplexity is 214.83391188768496
At time: 842.6864066123962 and batch: 100, loss is 5.398374500274659 and perplexity is 221.04681248251399
At time: 843.8587522506714 and batch: 150, loss is 5.312797746658325 and perplexity is 202.91714573920402
At time: 845.0234093666077 and batch: 200, loss is 5.352699460983277 and perplexity is 211.17759480124147
At time: 846.1929728984833 and batch: 250, loss is 5.37532304763794 and perplexity is 216.00964244523288
At time: 847.3598070144653 and batch: 300, loss is 5.373982887268067 and perplexity is 215.720348776171
At time: 848.5185823440552 and batch: 350, loss is 5.387394762039184 and perplexity is 218.63305182289707
At time: 849.6777868270874 and batch: 400, loss is 5.3717138671875 and perplexity is 215.23142986618788
At time: 850.8605964183807 and batch: 450, loss is 5.3345919036865235 and perplexity is 207.38809721817955
At time: 852.0374944210052 and batch: 500, loss is 5.341298675537109 and perplexity is 208.78367655567257
At time: 853.2130424976349 and batch: 550, loss is 5.354610824584961 and perplexity is 211.5816179640178
At time: 854.3863365650177 and batch: 600, loss is 5.365077857971191 and perplexity is 213.8078806807343
At time: 855.5591878890991 and batch: 650, loss is 5.350250883102417 and perplexity is 210.66114255848092
At time: 856.7596163749695 and batch: 700, loss is 5.371164855957031 and perplexity is 215.11329782491725
At time: 857.9331045150757 and batch: 750, loss is 5.364827117919922 and perplexity is 213.7542772023209
At time: 859.1100692749023 and batch: 800, loss is 5.371716594696045 and perplexity is 215.23201691255244
At time: 860.286600112915 and batch: 850, loss is 5.398896474838256 and perplexity is 221.1622234141592
At time: 861.4640681743622 and batch: 900, loss is 5.3834106063842775 and perplexity is 217.76371664640584
At time: 862.6363279819489 and batch: 950, loss is 5.375407609939575 and perplexity is 216.02790949011435
At time: 863.8097834587097 and batch: 1000, loss is 5.366731119155884 and perplexity is 214.1616533095431
At time: 864.9828655719757 and batch: 1050, loss is 5.355123262405396 and perplexity is 211.69006817179218
At time: 866.1573915481567 and batch: 1100, loss is 5.317269430160523 and perplexity is 203.8265587803843
At time: 867.3312482833862 and batch: 1150, loss is 5.3605908203125 and perplexity is 212.8506658040017
At time: 868.5071425437927 and batch: 1200, loss is 5.36071328163147 and perplexity is 212.87673337338154
At time: 869.6854541301727 and batch: 1250, loss is 5.375164213180542 and perplexity is 215.97533539552546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.19398487397354 and perplexity of 180.18513930243253
Finished 28 epochs...
Completing Train Step...
At time: 872.6558470726013 and batch: 50, loss is 5.3539493370056155 and perplexity is 211.44170563195337
At time: 873.8663432598114 and batch: 100, loss is 5.380777320861816 and perplexity is 217.19103694954347
At time: 875.0435712337494 and batch: 150, loss is 5.295120763778686 and perplexity is 199.3617001851344
At time: 876.222177028656 and batch: 200, loss is 5.337783670425415 and perplexity is 208.05108914386525
At time: 877.3986768722534 and batch: 250, loss is 5.36139214515686 and perplexity is 213.02129668690145
At time: 878.5740740299225 and batch: 300, loss is 5.359164905548096 and perplexity is 212.54737518169503
At time: 879.7480092048645 and batch: 350, loss is 5.372398014068604 and perplexity is 215.37873015941895
At time: 880.93448138237 and batch: 400, loss is 5.358093357086181 and perplexity is 212.31974235032123
At time: 882.1141228675842 and batch: 450, loss is 5.320251321792602 and perplexity is 204.43525457182338
At time: 883.2918701171875 and batch: 500, loss is 5.330526323318481 and perplexity is 206.54665587427905
At time: 884.4689328670502 and batch: 550, loss is 5.346684198379517 and perplexity is 209.91111902345332
At time: 885.6735942363739 and batch: 600, loss is 5.351281929016113 and perplexity is 210.87845587944486
At time: 886.8473584651947 and batch: 650, loss is 5.339243869781495 and perplexity is 208.35510711967262
At time: 888.0220115184784 and batch: 700, loss is 5.360073099136352 and perplexity is 212.74049702777282
At time: 889.1958677768707 and batch: 750, loss is 5.3516825008392335 and perplexity is 210.96294476777754
At time: 890.3704698085785 and batch: 800, loss is 5.3607589054107665 and perplexity is 212.88644583604034
At time: 891.5451457500458 and batch: 850, loss is 5.38976222038269 and perplexity is 219.1512696531195
At time: 892.7195377349854 and batch: 900, loss is 5.372531433105468 and perplexity is 215.40746769918277
At time: 893.8970310688019 and batch: 950, loss is 5.368556156158447 and perplexity is 214.55286312897573
At time: 895.0838968753815 and batch: 1000, loss is 5.358725442886352 and perplexity is 212.4539890678091
At time: 896.2612819671631 and batch: 1050, loss is 5.344096593856811 and perplexity is 209.36865420738937
At time: 897.4350357055664 and batch: 1100, loss is 5.3106526756286625 and perplexity is 202.4823405591781
At time: 898.6082901954651 and batch: 1150, loss is 5.35027494430542 and perplexity is 210.66621137997765
At time: 899.7822663784027 and batch: 1200, loss is 5.347692823410034 and perplexity is 210.12294744204857
At time: 900.9571318626404 and batch: 1250, loss is 5.369615535736084 and perplexity is 214.78027648776364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.18150296176437 and perplexity of 177.9500622483745
Finished 29 epochs...
Completing Train Step...
At time: 903.9314343929291 and batch: 50, loss is 5.339005908966064 and perplexity is 208.3055326671059
At time: 905.1105463504791 and batch: 100, loss is 5.369223232269287 and perplexity is 214.69603396609492
At time: 906.2752101421356 and batch: 150, loss is 5.277316446304321 and perplexity is 195.84361268995525
At time: 907.4482238292694 and batch: 200, loss is 5.328284502029419 and perplexity is 206.08413382355596
At time: 908.6240541934967 and batch: 250, loss is 5.351761779785156 and perplexity is 210.97967035065204
At time: 909.8003015518188 and batch: 300, loss is 5.354116172790527 and perplexity is 211.47698461769232
At time: 910.974226474762 and batch: 350, loss is 5.365043783187867 and perplexity is 213.80059534765076
At time: 912.1532068252563 and batch: 400, loss is 5.347115001678467 and perplexity is 210.00156890767022
At time: 913.3286492824554 and batch: 450, loss is 5.308666620254517 and perplexity is 202.08059849159028
At time: 914.5047693252563 and batch: 500, loss is 5.31453498840332 and perplexity is 203.26996825582452
At time: 915.7291853427887 and batch: 550, loss is 5.3297394847869874 and perplexity is 206.38420092817313
At time: 916.9082088470459 and batch: 600, loss is 5.341196317672729 and perplexity is 208.76230699811273
At time: 918.0810232162476 and batch: 650, loss is 5.327889051437378 and perplexity is 206.00265384254035
At time: 919.2554707527161 and batch: 700, loss is 5.349423303604126 and perplexity is 210.48687583554258
At time: 920.433513879776 and batch: 750, loss is 5.340510158538819 and perplexity is 208.61911196725714
At time: 921.6100211143494 and batch: 800, loss is 5.346790628433228 and perplexity is 209.93346106403663
At time: 922.7873795032501 and batch: 850, loss is 5.379924640655518 and perplexity is 217.00592138475605
At time: 923.9721410274506 and batch: 900, loss is 5.367850141525269 and perplexity is 214.40143912808645
At time: 925.1562030315399 and batch: 950, loss is 5.386230621337891 and perplexity is 218.37868027948304
At time: 926.3316638469696 and batch: 1000, loss is 5.375067043304443 and perplexity is 215.95435011852948
At time: 927.5055634975433 and batch: 1050, loss is 5.355544996261597 and perplexity is 211.7793638687475
At time: 928.6771402359009 and batch: 1100, loss is 5.324897890090942 and perplexity is 205.38738730657823
At time: 929.8372304439545 and batch: 1150, loss is 5.371068744659424 and perplexity is 215.09262400024082
At time: 930.9976327419281 and batch: 1200, loss is 5.360384712219238 and perplexity is 212.806800079819
At time: 932.1676325798035 and batch: 1250, loss is 5.387284107208252 and perplexity is 218.6088603579874
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.203609271641195 and perplexity of 181.92768476126858
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 935.1536891460419 and batch: 50, loss is 5.371377077102661 and perplexity is 215.15895425988106
At time: 936.3758461475372 and batch: 100, loss is 5.388517723083496 and perplexity is 218.8787061273854
At time: 937.5520279407501 and batch: 150, loss is 5.2969711399078365 and perplexity is 199.7309358232773
At time: 938.7270348072052 and batch: 200, loss is 5.342851066589356 and perplexity is 209.1080421730126
At time: 939.9007647037506 and batch: 250, loss is 5.356199941635132 and perplexity is 211.91811321498497
At time: 941.0816009044647 and batch: 300, loss is 5.351455764770508 and perplexity is 210.91511728134716
At time: 942.2724018096924 and batch: 350, loss is 5.364590826034546 and perplexity is 213.70377476804637
At time: 943.4562640190125 and batch: 400, loss is 5.33092957496643 and perplexity is 206.6299629493675
At time: 944.6310966014862 and batch: 450, loss is 5.297255449295044 and perplexity is 199.78772927632087
At time: 945.8369147777557 and batch: 500, loss is 5.2962917232513425 and perplexity is 199.59528138682188
At time: 947.0101366043091 and batch: 550, loss is 5.3093082714080815 and perplexity is 202.21030534955278
At time: 948.1851418018341 and batch: 600, loss is 5.313109359741211 and perplexity is 202.98038722948434
At time: 949.3608705997467 and batch: 650, loss is 5.30074333190918 and perplexity is 200.4857820792292
At time: 950.5439352989197 and batch: 700, loss is 5.315403432846069 and perplexity is 203.44657360500148
At time: 951.7161936759949 and batch: 750, loss is 5.305554313659668 and perplexity is 201.4526394196855
At time: 952.8895781040192 and batch: 800, loss is 5.305299596786499 and perplexity is 201.40133256791876
At time: 954.0642426013947 and batch: 850, loss is 5.332437629699707 and perplexity is 206.9418073231216
At time: 955.242151260376 and batch: 900, loss is 5.316839637756348 and perplexity is 203.7389744965296
At time: 956.4185209274292 and batch: 950, loss is 5.296524257659912 and perplexity is 199.6416995542338
At time: 957.5925853252411 and batch: 1000, loss is 5.282299070358277 and perplexity is 196.82186288796137
At time: 958.7661778926849 and batch: 1050, loss is 5.254423780441284 and perplexity is 191.4111592175491
At time: 959.9405405521393 and batch: 1100, loss is 5.21475193977356 and perplexity is 183.96618062667892
At time: 961.1162147521973 and batch: 1150, loss is 5.257598857879639 and perplexity is 192.019870311687
At time: 962.2945673465729 and batch: 1200, loss is 5.259894676208496 and perplexity is 192.4612194843588
At time: 963.471652507782 and batch: 1250, loss is 5.303345575332641 and perplexity is 201.00817428819855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1240929373859485 and perplexity of 168.02166633808545
Finished 31 epochs...
Completing Train Step...
At time: 966.4279675483704 and batch: 50, loss is 5.315878696441651 and perplexity is 203.54328733551895
At time: 967.6494607925415 and batch: 100, loss is 5.3372954082489015 and perplexity is 207.9495304619031
At time: 968.8290688991547 and batch: 150, loss is 5.256053791046143 and perplexity is 191.72341585864314
At time: 970.0075762271881 and batch: 200, loss is 5.29538143157959 and perplexity is 199.4136741347793
At time: 971.1900973320007 and batch: 250, loss is 5.318381633758545 and perplexity is 204.05338152558735
At time: 972.3724908828735 and batch: 300, loss is 5.326963844299317 and perplexity is 205.8121468595456
At time: 973.5479011535645 and batch: 350, loss is 5.334643697738647 and perplexity is 207.39883896627362
At time: 974.7621757984161 and batch: 400, loss is 5.303820791244507 and perplexity is 201.10371927148552
At time: 975.9394598007202 and batch: 450, loss is 5.267436838150024 and perplexity is 193.91828095899834
At time: 977.1156070232391 and batch: 500, loss is 5.2876199436187745 and perplexity is 197.87191820350617
At time: 978.2964818477631 and batch: 550, loss is 5.299119482040405 and perplexity is 200.16048745460586
At time: 979.4810702800751 and batch: 600, loss is 5.295733795166016 and perplexity is 199.4839526332446
At time: 980.6625714302063 and batch: 650, loss is 5.286034469604492 and perplexity is 197.55844598573134
At time: 981.8535652160645 and batch: 700, loss is 5.300443286895752 and perplexity is 200.4256363437184
At time: 983.0323588848114 and batch: 750, loss is 5.292536611557007 and perplexity is 198.8471842847081
At time: 984.2076725959778 and batch: 800, loss is 5.315751714706421 and perplexity is 203.51744269663197
At time: 985.3833906650543 and batch: 850, loss is 5.335219697952271 and perplexity is 207.5183351534343
At time: 986.5580739974976 and batch: 900, loss is 5.318795261383056 and perplexity is 204.13780109899153
At time: 987.7326264381409 and batch: 950, loss is 5.30434928894043 and perplexity is 201.21003021383157
At time: 988.9084866046906 and batch: 1000, loss is 5.291463222503662 and perplexity is 198.6338584051226
At time: 990.0880703926086 and batch: 1050, loss is 5.27865478515625 and perplexity is 196.10589327672122
At time: 991.2771136760712 and batch: 1100, loss is 5.2332361125946045 and perplexity is 187.39826520692867
At time: 992.4549279212952 and batch: 1150, loss is 5.278636474609375 and perplexity is 196.1023025034445
At time: 993.6324501037598 and batch: 1200, loss is 5.276846332550049 and perplexity is 195.7515655519513
At time: 994.812826871872 and batch: 1250, loss is 5.317445449829101 and perplexity is 203.86243942146496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.132491285783531 and perplexity of 169.4387129268612
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 997.7979621887207 and batch: 50, loss is 5.311244487762451 and perplexity is 202.60220753106464
At time: 998.9817836284637 and batch: 100, loss is 5.350710592269897 and perplexity is 210.7580076801333
At time: 1000.1571640968323 and batch: 150, loss is 5.258401002883911 and perplexity is 192.1739598842066
At time: 1001.3312973976135 and batch: 200, loss is 5.3018089866638185 and perplexity is 200.69954458442498
At time: 1002.4937946796417 and batch: 250, loss is 5.323208799362183 and perplexity is 205.04076219791418
At time: 1003.6648895740509 and batch: 300, loss is 5.321830205917358 and perplexity is 204.75828909974416
At time: 1004.8886194229126 and batch: 350, loss is 5.3310794925689695 and perplexity is 206.6609427401756
At time: 1006.0505313873291 and batch: 400, loss is 5.294430818557739 and perplexity is 199.2241989724585
At time: 1007.2131626605988 and batch: 450, loss is 5.255558271408081 and perplexity is 191.62843667497756
At time: 1008.3905341625214 and batch: 500, loss is 5.265134391784668 and perplexity is 193.47230812910686
At time: 1009.5795884132385 and batch: 550, loss is 5.283473978042602 and perplexity is 197.05324630757366
At time: 1010.7688663005829 and batch: 600, loss is 5.287990560531616 and perplexity is 197.9452664741841
At time: 1011.9559237957001 and batch: 650, loss is 5.276158132553101 and perplexity is 195.6168956703571
At time: 1013.1371719837189 and batch: 700, loss is 5.287014751434326 and perplexity is 197.7522038938238
At time: 1014.3241562843323 and batch: 750, loss is 5.271607904434204 and perplexity is 194.72881618554368
At time: 1015.5074164867401 and batch: 800, loss is 5.285793962478638 and perplexity is 197.51093748499463
At time: 1016.6854491233826 and batch: 850, loss is 5.308957967758179 and perplexity is 202.13948274697321
At time: 1017.8635566234589 and batch: 900, loss is 5.294486713409424 and perplexity is 199.2353348907295
At time: 1019.0449466705322 and batch: 950, loss is 5.277764415740966 and perplexity is 195.93136429635385
At time: 1020.2239003181458 and batch: 1000, loss is 5.263620653152466 and perplexity is 193.17966317189988
At time: 1021.4049003124237 and batch: 1050, loss is 5.237822637557984 and perplexity is 188.2597461200603
At time: 1022.583377122879 and batch: 1100, loss is 5.201813669204712 and perplexity is 181.60130807632447
At time: 1023.7757403850555 and batch: 1150, loss is 5.24179012298584 and perplexity is 189.00814757366263
At time: 1024.9622964859009 and batch: 1200, loss is 5.244898071289063 and perplexity is 189.5964889188697
At time: 1026.1350903511047 and batch: 1250, loss is 5.2891592693328855 and perplexity is 198.17674208672753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.116733968692975 and perplexity of 166.78973857420817
Finished 33 epochs...
Completing Train Step...
At time: 1029.1350638866425 and batch: 50, loss is 5.301344003677368 and perplexity is 200.60624440398416
At time: 1030.3449416160583 and batch: 100, loss is 5.342804050445556 and perplexity is 209.09821095034704
At time: 1031.5354533195496 and batch: 150, loss is 5.250149526596069 and perplexity is 190.5947653145441
At time: 1032.7232892513275 and batch: 200, loss is 5.290238904953003 and perplexity is 198.39081629684048
At time: 1033.9146876335144 and batch: 250, loss is 5.312835264205932 and perplexity is 202.92475883569108
At time: 1035.1465260982513 and batch: 300, loss is 5.313367729187012 and perplexity is 203.03283793517932
At time: 1036.3342411518097 and batch: 350, loss is 5.32287784576416 and perplexity is 204.97291444777127
At time: 1037.5287945270538 and batch: 400, loss is 5.287322797775269 and perplexity is 197.8131301202153
At time: 1038.7153911590576 and batch: 450, loss is 5.250226440429688 and perplexity is 190.60942525238084
At time: 1039.9055042266846 and batch: 500, loss is 5.255289878845215 and perplexity is 191.57701192905876
At time: 1041.0917701721191 and batch: 550, loss is 5.27272783279419 and perplexity is 194.94702067318653
At time: 1042.2793653011322 and batch: 600, loss is 5.2802894592285154 and perplexity is 196.4267246518017
At time: 1043.476993560791 and batch: 650, loss is 5.270835304260254 and perplexity is 194.57842677121022
At time: 1044.67528963089 and batch: 700, loss is 5.2820485401153565 and perplexity is 196.772559235126
At time: 1045.863129377365 and batch: 750, loss is 5.267792587280273 and perplexity is 193.98727949114516
At time: 1047.0482823848724 and batch: 800, loss is 5.2813599491119385 and perplexity is 196.63711006101056
At time: 1048.235995054245 and batch: 850, loss is 5.306327543258667 and perplexity is 201.6084688014625
At time: 1049.4199409484863 and batch: 900, loss is 5.291372594833374 and perplexity is 198.61585749699748
At time: 1050.6131262779236 and batch: 950, loss is 5.277527647018433 and perplexity is 195.8849793689918
At time: 1051.7982532978058 and batch: 1000, loss is 5.2641541957855225 and perplexity is 193.28276025894155
At time: 1052.9923794269562 and batch: 1050, loss is 5.238074913024902 and perplexity is 188.3072454266176
At time: 1054.1848945617676 and batch: 1100, loss is 5.203389873504639 and perplexity is 181.88777454451767
At time: 1055.3689658641815 and batch: 1150, loss is 5.244779205322265 and perplexity is 189.5739536882755
At time: 1056.5520420074463 and batch: 1200, loss is 5.248356332778931 and perplexity is 190.2532982096539
At time: 1057.7343742847443 and batch: 1250, loss is 5.29016809463501 and perplexity is 198.37676867741564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.115048150946624 and perplexity of 166.50879834655686
Finished 34 epochs...
Completing Train Step...
At time: 1060.8049240112305 and batch: 50, loss is 5.296935262680054 and perplexity is 199.7237701595404
At time: 1061.991057395935 and batch: 100, loss is 5.336092252731323 and perplexity is 207.69948528872828
At time: 1063.1746809482574 and batch: 150, loss is 5.245332937240601 and perplexity is 189.6789559061746
At time: 1064.3619101047516 and batch: 200, loss is 5.284480972290039 and perplexity is 197.2517777362709
At time: 1065.5685963630676 and batch: 250, loss is 5.308357791900635 and perplexity is 202.0181999087284
At time: 1066.761708021164 and batch: 300, loss is 5.309996480941773 and perplexity is 202.34951630716446
At time: 1067.9479818344116 and batch: 350, loss is 5.317928848266601 and perplexity is 203.9610100286655
At time: 1069.1332902908325 and batch: 400, loss is 5.284498119354248 and perplexity is 197.25516005416748
At time: 1070.3140013217926 and batch: 450, loss is 5.248808908462524 and perplexity is 190.3394217133788
At time: 1071.488837480545 and batch: 500, loss is 5.250545225143433 and perplexity is 190.67019830969315
At time: 1072.660089969635 and batch: 550, loss is 5.268084468841553 and perplexity is 194.04390906531367
At time: 1073.8317399024963 and batch: 600, loss is 5.2773611354827885 and perplexity is 195.85236497567917
At time: 1075.0008473396301 and batch: 650, loss is 5.270740823745728 and perplexity is 194.5600437697645
At time: 1076.1637094020844 and batch: 700, loss is 5.281230764389038 and perplexity is 196.61170919117288
At time: 1077.3296082019806 and batch: 750, loss is 5.26726523399353 and perplexity is 193.8850066310538
At time: 1078.4925990104675 and batch: 800, loss is 5.2783316802978515 and perplexity is 196.0425407451495
At time: 1079.660724401474 and batch: 850, loss is 5.303935213088989 and perplexity is 201.12673124648796
At time: 1080.8438827991486 and batch: 900, loss is 5.288159036636353 and perplexity is 197.97861833104727
At time: 1082.028641462326 and batch: 950, loss is 5.2750136661529545 and perplexity is 195.39314676693476
At time: 1083.2109634876251 and batch: 1000, loss is 5.2632357788085935 and perplexity is 193.10532758163546
At time: 1084.3931267261505 and batch: 1050, loss is 5.237522449493408 and perplexity is 188.20324127269973
At time: 1085.5719001293182 and batch: 1100, loss is 5.2033265113830565 and perplexity is 181.87625011434264
At time: 1086.7544975280762 and batch: 1150, loss is 5.244458694458007 and perplexity is 189.51320291269994
At time: 1087.9374072551727 and batch: 1200, loss is 5.248535976409912 and perplexity is 190.2874790730458
At time: 1089.1288225650787 and batch: 1250, loss is 5.289234733581543 and perplexity is 198.1916979099783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.112651741417655 and perplexity of 166.1102528054381
Finished 35 epochs...
Completing Train Step...
At time: 1092.1660730838776 and batch: 50, loss is 5.292508974075317 and perplexity is 198.84168872523543
At time: 1093.4184136390686 and batch: 100, loss is 5.328483657836914 and perplexity is 206.12518076287205
At time: 1094.6012506484985 and batch: 150, loss is 5.2388769626617435 and perplexity is 188.45833776810585
At time: 1095.8179428577423 and batch: 200, loss is 5.27936616897583 and perplexity is 196.2454494692459
At time: 1096.9994609355927 and batch: 250, loss is 5.303200473785401 and perplexity is 200.97900980705816
At time: 1098.1826043128967 and batch: 300, loss is 5.304449491500854 and perplexity is 201.23019298420576
At time: 1099.3708794116974 and batch: 350, loss is 5.311573162078857 and perplexity is 202.66880861756098
At time: 1100.5555408000946 and batch: 400, loss is 5.27991584777832 and perplexity is 196.35335108580313
At time: 1101.7387473583221 and batch: 450, loss is 5.245546436309814 and perplexity is 189.71945650997665
At time: 1102.9272842407227 and batch: 500, loss is 5.248969631195068 and perplexity is 190.3700160438838
At time: 1104.1105613708496 and batch: 550, loss is 5.264213447570801 and perplexity is 193.29421294684312
At time: 1105.2875192165375 and batch: 600, loss is 5.273271322250366 and perplexity is 195.05300112045452
At time: 1106.476063966751 and batch: 650, loss is 5.26837477684021 and perplexity is 194.10024974188474
At time: 1107.6512019634247 and batch: 700, loss is 5.277142658233642 and perplexity is 195.80958036364282
At time: 1108.8261921405792 and batch: 750, loss is 5.263859052658081 and perplexity is 193.22572259814896
At time: 1110.0136816501617 and batch: 800, loss is 5.273453350067139 and perplexity is 195.08850942405488
At time: 1111.2013087272644 and batch: 850, loss is 5.300916423797608 and perplexity is 200.5204875453829
At time: 1112.3770008087158 and batch: 900, loss is 5.28594292640686 and perplexity is 197.54036168162673
At time: 1113.555447101593 and batch: 950, loss is 5.27261589050293 and perplexity is 194.92519907842058
At time: 1114.7296688556671 and batch: 1000, loss is 5.2617662811279295 and perplexity is 192.82176814662353
At time: 1115.9025111198425 and batch: 1050, loss is 5.2372291374206545 and perplexity is 188.14804708485963
At time: 1117.0804929733276 and batch: 1100, loss is 5.201912832260132 and perplexity is 181.61931710980227
At time: 1118.258630990982 and batch: 1150, loss is 5.24411675453186 and perplexity is 189.4484118600472
At time: 1119.4426770210266 and batch: 1200, loss is 5.249246578216553 and perplexity is 190.42274575413822
At time: 1120.6273636817932 and batch: 1250, loss is 5.287921686172485 and perplexity is 197.93163359029603
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.110760542598084 and perplexity of 165.7964021619411
Finished 36 epochs...
Completing Train Step...
At time: 1123.6703078746796 and batch: 50, loss is 5.288616895675659 and perplexity is 198.0692853858209
At time: 1124.8798818588257 and batch: 100, loss is 5.323377485275269 and perplexity is 205.0753526034804
At time: 1126.069135427475 and batch: 150, loss is 5.2343398952484135 and perplexity is 187.60522636047125
At time: 1127.255110502243 and batch: 200, loss is 5.275034713745117 and perplexity is 195.39725936547924
At time: 1128.4340448379517 and batch: 250, loss is 5.298906078338623 and perplexity is 200.11777702307677
At time: 1129.6161077022552 and batch: 300, loss is 5.299888944625854 and perplexity is 200.3145627307656
At time: 1130.797857284546 and batch: 350, loss is 5.307169923782348 and perplexity is 201.77837140026318
At time: 1131.9758789539337 and batch: 400, loss is 5.27622802734375 and perplexity is 195.63056875016042
At time: 1133.1585364341736 and batch: 450, loss is 5.2424669933319095 and perplexity is 189.13612489106018
At time: 1134.3408575057983 and batch: 500, loss is 5.246648941040039 and perplexity is 189.92873845416253
At time: 1135.5231561660767 and batch: 550, loss is 5.260664978027344 and perplexity is 192.60952982631474
At time: 1136.7081906795502 and batch: 600, loss is 5.270114421844482 and perplexity is 194.43820915114293
At time: 1137.9032192230225 and batch: 650, loss is 5.2654914283752445 and perplexity is 193.54139715529357
At time: 1139.0801129341125 and batch: 700, loss is 5.274790592193604 and perplexity is 195.34956450527002
At time: 1140.2545800209045 and batch: 750, loss is 5.261422319412231 and perplexity is 192.75545624545927
At time: 1141.429167509079 and batch: 800, loss is 5.2709353637695315 and perplexity is 194.5978971671918
At time: 1142.6047639846802 and batch: 850, loss is 5.298840103149414 and perplexity is 200.10457465039278
At time: 1143.7811105251312 and batch: 900, loss is 5.284179515838623 and perplexity is 197.19232387714402
At time: 1144.971078634262 and batch: 950, loss is 5.270823287963867 and perplexity is 194.57608867321125
At time: 1146.147983789444 and batch: 1000, loss is 5.260764245986938 and perplexity is 192.62865073036994
At time: 1147.3237931728363 and batch: 1050, loss is 5.236681489944458 and perplexity is 188.04503649104277
At time: 1148.4973640441895 and batch: 1100, loss is 5.201488590240478 and perplexity is 181.54228290563202
At time: 1149.6608176231384 and batch: 1150, loss is 5.244097080230713 and perplexity is 189.4446846316058
At time: 1150.8298606872559 and batch: 1200, loss is 5.24961181640625 and perplexity is 190.49230811571678
At time: 1151.995770931244 and batch: 1250, loss is 5.287003240585327 and perplexity is 197.7499276111665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.109504198505931 and perplexity of 165.58823562336607
Finished 37 epochs...
Completing Train Step...
At time: 1154.939688205719 and batch: 50, loss is 5.28535532951355 and perplexity is 197.42432167451184
At time: 1156.1159329414368 and batch: 100, loss is 5.319482383728027 and perplexity is 204.27811694513576
At time: 1157.2928159236908 and batch: 150, loss is 5.230705518722534 and perplexity is 186.92463583990266
At time: 1158.47328042984 and batch: 200, loss is 5.272291564941407 and perplexity is 194.86199010447143
At time: 1159.6643707752228 and batch: 250, loss is 5.295977973937989 and perplexity is 199.53266832725396
At time: 1160.8474643230438 and batch: 300, loss is 5.2961569499969485 and perplexity is 199.56838309381368
At time: 1162.0281565189362 and batch: 350, loss is 5.304064712524414 and perplexity is 201.1527787311641
At time: 1163.2084844112396 and batch: 400, loss is 5.27322546005249 and perplexity is 195.04405576624924
At time: 1164.3866062164307 and batch: 450, loss is 5.239999189376831 and perplexity is 188.6699494653716
At time: 1165.561363697052 and batch: 500, loss is 5.244525699615479 and perplexity is 189.5259017001414
At time: 1166.7361793518066 and batch: 550, loss is 5.2577151012420655 and perplexity is 192.04219264445106
At time: 1167.9140689373016 and batch: 600, loss is 5.267759675979614 and perplexity is 193.98089522252366
At time: 1169.0907123088837 and batch: 650, loss is 5.263530111312866 and perplexity is 193.16217312162559
At time: 1170.2640435695648 and batch: 700, loss is 5.272837028503418 and perplexity is 194.96830921365842
At time: 1171.4379341602325 and batch: 750, loss is 5.25968487739563 and perplexity is 192.4208455843344
At time: 1172.6130356788635 and batch: 800, loss is 5.268573799133301 and perplexity is 194.1388838630761
At time: 1173.7926886081696 and batch: 850, loss is 5.296920127868653 and perplexity is 199.72074740082115
At time: 1174.968712091446 and batch: 900, loss is 5.282790412902832 and perplexity is 196.9185936049092
At time: 1176.1519858837128 and batch: 950, loss is 5.269570140838623 and perplexity is 194.33240892222807
At time: 1177.3377208709717 and batch: 1000, loss is 5.26000485420227 and perplexity is 192.4824256436054
At time: 1178.5242915153503 and batch: 1050, loss is 5.235914306640625 and perplexity is 187.90082680338355
At time: 1179.7010507583618 and batch: 1100, loss is 5.201224269866944 and perplexity is 181.4943039227928
At time: 1180.8753430843353 and batch: 1150, loss is 5.243729677200317 and perplexity is 189.37509486490822
At time: 1182.0496809482574 and batch: 1200, loss is 5.249638519287109 and perplexity is 190.4973948770403
At time: 1183.2248566150665 and batch: 1250, loss is 5.286359901428223 and perplexity is 197.62274825353654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.108419822080292 and perplexity of 165.40877296441673
Finished 38 epochs...
Completing Train Step...
At time: 1186.2149455547333 and batch: 50, loss is 5.282851991653442 and perplexity is 196.93071997923488
At time: 1187.4179379940033 and batch: 100, loss is 5.315933809280396 and perplexity is 203.55450549302094
At time: 1188.595906496048 and batch: 150, loss is 5.227309379577637 and perplexity is 186.29089051974816
At time: 1189.7765111923218 and batch: 200, loss is 5.269436912536621 and perplexity is 194.3065200699656
At time: 1190.9548377990723 and batch: 250, loss is 5.292921371459961 and perplexity is 198.9237074286109
At time: 1192.1302330493927 and batch: 300, loss is 5.29305645942688 and perplexity is 198.9505814429564
At time: 1193.3062977790833 and batch: 350, loss is 5.3014517021179195 and perplexity is 200.62785054712444
At time: 1194.4838206768036 and batch: 400, loss is 5.271123027801513 and perplexity is 194.63441962005757
At time: 1195.6633172035217 and batch: 450, loss is 5.237770013809204 and perplexity is 188.24983944714052
At time: 1196.8406071662903 and batch: 500, loss is 5.242544813156128 and perplexity is 189.1508440037645
At time: 1198.0236451625824 and batch: 550, loss is 5.255161151885987 and perplexity is 191.5523523900629
At time: 1199.209133863449 and batch: 600, loss is 5.265639886856079 and perplexity is 193.57013215001797
At time: 1200.4014971256256 and batch: 650, loss is 5.261717748641968 and perplexity is 192.8124102539506
At time: 1201.5892510414124 and batch: 700, loss is 5.271121635437011 and perplexity is 194.6341486181895
At time: 1202.7649307250977 and batch: 750, loss is 5.258267660140991 and perplexity is 192.14833658965597
At time: 1203.9400947093964 and batch: 800, loss is 5.266921310424805 and perplexity is 193.81833647302741
At time: 1205.1168835163116 and batch: 850, loss is 5.295425510406494 and perplexity is 199.42246424933134
At time: 1206.296323299408 and batch: 900, loss is 5.281540813446045 and perplexity is 196.67267791736379
At time: 1207.4767980575562 and batch: 950, loss is 5.268229751586914 and perplexity is 194.07210234509225
At time: 1208.6560344696045 and batch: 1000, loss is 5.259065246582031 and perplexity is 192.3016526308585
At time: 1209.8355581760406 and batch: 1050, loss is 5.235514860153199 and perplexity is 187.8257854666294
At time: 1211.0172469615936 and batch: 1100, loss is 5.20088713645935 and perplexity is 181.4331264427181
At time: 1212.2074987888336 and batch: 1150, loss is 5.243288612365722 and perplexity is 189.29158658765127
At time: 1213.3881030082703 and batch: 1200, loss is 5.2490950870513915 and perplexity is 190.3939005754607
At time: 1214.6193053722382 and batch: 1250, loss is 5.285589427947998 and perplexity is 197.47054380919138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.107187535640967 and perplexity of 165.205067514057
Finished 39 epochs...
Completing Train Step...
At time: 1217.5700323581696 and batch: 50, loss is 5.28030234336853 and perplexity is 196.42925545752826
At time: 1218.7488191127777 and batch: 100, loss is 5.3130474948883055 and perplexity is 202.96783026610706
At time: 1219.916805267334 and batch: 150, loss is 5.224746856689453 and perplexity is 185.81412696857717
At time: 1221.083633184433 and batch: 200, loss is 5.267081775665283 and perplexity is 193.84944007445534
At time: 1222.247902393341 and batch: 250, loss is 5.2903780841827395 and perplexity is 198.41843009942883
At time: 1223.406858921051 and batch: 300, loss is 5.290022230148315 and perplexity is 198.34783466215433
At time: 1224.5707685947418 and batch: 350, loss is 5.299032440185547 and perplexity is 200.14306587272293
At time: 1225.7453022003174 and batch: 400, loss is 5.26914273262024 and perplexity is 194.24936740113483
At time: 1226.9318783283234 and batch: 450, loss is 5.235944108963013 and perplexity is 187.90642676784643
At time: 1228.109142780304 and batch: 500, loss is 5.240774164199829 and perplexity is 188.81622059696505
At time: 1229.2886667251587 and batch: 550, loss is 5.252978010177612 and perplexity is 191.13462260779028
At time: 1230.461415529251 and batch: 600, loss is 5.263654203414917 and perplexity is 193.18614450902413
At time: 1231.6370558738708 and batch: 650, loss is 5.259859399795532 and perplexity is 192.4544302626511
At time: 1232.8067603111267 and batch: 700, loss is 5.269525089263916 and perplexity is 194.32365413839935
At time: 1233.9860699176788 and batch: 750, loss is 5.256658382415772 and perplexity is 191.83936522868933
At time: 1235.1583135128021 and batch: 800, loss is 5.265462474822998 and perplexity is 193.53579352546208
At time: 1236.3314611911774 and batch: 850, loss is 5.294026403427124 and perplexity is 199.14364598153148
At time: 1237.5107538700104 and batch: 900, loss is 5.2803700733184815 and perplexity is 196.44256005172412
At time: 1238.6878538131714 and batch: 950, loss is 5.267109804153442 and perplexity is 193.8548734573355
At time: 1239.86497092247 and batch: 1000, loss is 5.258311233520508 and perplexity is 192.15670932446258
At time: 1241.0383987426758 and batch: 1050, loss is 5.235047855377197 and perplexity is 187.73809040636007
At time: 1242.2119731903076 and batch: 1100, loss is 5.200517206192017 and perplexity is 181.36602125063612
At time: 1243.3849754333496 and batch: 1150, loss is 5.242530498504639 and perplexity is 189.14813639473303
At time: 1244.6053848266602 and batch: 1200, loss is 5.24834644317627 and perplexity is 190.2514166894334
At time: 1245.7808785438538 and batch: 1250, loss is 5.284523439407349 and perplexity is 197.26015462852556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1060826656592155 and perplexity of 165.022638193026
Finished 40 epochs...
Completing Train Step...
At time: 1248.7444033622742 and batch: 50, loss is 5.277965421676636 and perplexity is 195.97075162197174
At time: 1249.9452273845673 and batch: 100, loss is 5.310566215515137 and perplexity is 202.46483466981795
At time: 1251.1202983856201 and batch: 150, loss is 5.222532815933228 and perplexity is 185.40318201069914
At time: 1252.2952427864075 and batch: 200, loss is 5.26529935836792 and perplexity is 193.50422722745276
At time: 1253.4713203907013 and batch: 250, loss is 5.288064756393433 and perplexity is 197.959953738683
At time: 1254.647878408432 and batch: 300, loss is 5.287428712844848 and perplexity is 197.834082621229
At time: 1255.8230245113373 and batch: 350, loss is 5.296699714660645 and perplexity is 199.67673116123927
At time: 1256.9969308376312 and batch: 400, loss is 5.266801099777222 and perplexity is 193.79503884562607
At time: 1258.1708250045776 and batch: 450, loss is 5.233911170959472 and perplexity is 187.52481268207848
At time: 1259.3464562892914 and batch: 500, loss is 5.23908374786377 and perplexity is 188.4973121930831
At time: 1260.5263314247131 and batch: 550, loss is 5.250996313095093 and perplexity is 190.7562267406269
At time: 1261.70401263237 and batch: 600, loss is 5.261974220275879 and perplexity is 192.86186750976722
At time: 1262.8784086704254 and batch: 650, loss is 5.258356990814209 and perplexity is 192.16550209661307
At time: 1264.0530734062195 and batch: 700, loss is 5.267948951721191 and perplexity is 194.0176145752504
At time: 1265.2289667129517 and batch: 750, loss is 5.255601072311402 and perplexity is 191.6366387206955
At time: 1266.407176733017 and batch: 800, loss is 5.26431360244751 and perplexity is 193.3135732744089
At time: 1267.5853958129883 and batch: 850, loss is 5.293017492294312 and perplexity is 198.94282906031992
At time: 1268.7603900432587 and batch: 900, loss is 5.279421262741089 and perplexity is 196.25626166781163
At time: 1269.934737920761 and batch: 950, loss is 5.2661417293548585 and perplexity is 193.66729824782453
At time: 1271.1098520755768 and batch: 1000, loss is 5.257573795318604 and perplexity is 192.0150578622737
At time: 1272.2869157791138 and batch: 1050, loss is 5.234528322219848 and perplexity is 187.64057957574474
At time: 1273.5114078521729 and batch: 1100, loss is 5.200479001998901 and perplexity is 181.35909244049125
At time: 1274.685926914215 and batch: 1150, loss is 5.242290477752686 and perplexity is 189.1027423647776
At time: 1275.865446805954 and batch: 1200, loss is 5.247813377380371 and perplexity is 190.15002719261236
At time: 1277.0429496765137 and batch: 1250, loss is 5.283645048141479 and perplexity is 197.08695910944746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.105220599766195 and perplexity of 164.88043910636122
Finished 41 epochs...
Completing Train Step...
At time: 1280.0106480121613 and batch: 50, loss is 5.275902538299561 and perplexity is 195.5669035050557
At time: 1281.2085242271423 and batch: 100, loss is 5.308623294830323 and perplexity is 202.07184345359846
At time: 1282.3814153671265 and batch: 150, loss is 5.220329780578613 and perplexity is 184.99518183029696
At time: 1283.5545961856842 and batch: 200, loss is 5.263244390487671 and perplexity is 193.1069905499052
At time: 1284.7289385795593 and batch: 250, loss is 5.286035070419311 and perplexity is 197.55856468180903
At time: 1285.9049785137177 and batch: 300, loss is 5.284986839294434 and perplexity is 197.35158614494785
At time: 1287.0866277217865 and batch: 350, loss is 5.294689807891846 and perplexity is 199.27580259719412
At time: 1288.2660262584686 and batch: 400, loss is 5.265218143463135 and perplexity is 193.48851243820914
At time: 1289.436560869217 and batch: 450, loss is 5.2321839427948 and perplexity is 187.2011941060533
At time: 1290.5988457202911 and batch: 500, loss is 5.237331733703614 and perplexity is 188.1673513653933
At time: 1291.776401758194 and batch: 550, loss is 5.249266443252563 and perplexity is 190.42652854641227
At time: 1292.9421970844269 and batch: 600, loss is 5.260514173507691 and perplexity is 192.58048562874183
At time: 1294.1072239875793 and batch: 650, loss is 5.256929216384887 and perplexity is 191.8913288818504
At time: 1295.2660086154938 and batch: 700, loss is 5.2664155006408695 and perplexity is 193.72032605153794
At time: 1296.4397723674774 and batch: 750, loss is 5.254462337493896 and perplexity is 191.41853960996775
At time: 1297.6146776676178 and batch: 800, loss is 5.263399066925049 and perplexity is 193.13686196137851
At time: 1298.790560722351 and batch: 850, loss is 5.292087869644165 and perplexity is 198.75797323673416
At time: 1299.965329170227 and batch: 900, loss is 5.278635320663452 and perplexity is 196.10207621212263
At time: 1301.1459302902222 and batch: 950, loss is 5.265305786132813 and perplexity is 193.5054710311286
At time: 1302.317456483841 and batch: 1000, loss is 5.256904029846192 and perplexity is 191.88649586433394
At time: 1303.516639471054 and batch: 1050, loss is 5.23414080619812 and perplexity is 187.56787993188087
At time: 1304.6888291835785 and batch: 1100, loss is 5.2000728034973145 and perplexity is 181.2854396087405
At time: 1305.8676602840424 and batch: 1150, loss is 5.241578311920166 and perplexity is 188.96811779602848
At time: 1307.0403220653534 and batch: 1200, loss is 5.247224473953247 and perplexity is 190.03808015616298
At time: 1308.22248005867 and batch: 1250, loss is 5.282947425842285 and perplexity is 196.94951479957433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.104387492158987 and perplexity of 164.7431331615172
Finished 42 epochs...
Completing Train Step...
At time: 1311.2161304950714 and batch: 50, loss is 5.27423053741455 and perplexity is 195.24018867916882
At time: 1312.392348766327 and batch: 100, loss is 5.306467361450196 and perplexity is 201.63665930369376
At time: 1313.572197675705 and batch: 150, loss is 5.218389310836792 and perplexity is 184.63655234493768
At time: 1314.7518496513367 and batch: 200, loss is 5.261390476226807 and perplexity is 192.74931839544934
At time: 1315.928419828415 and batch: 250, loss is 5.284189529418946 and perplexity is 197.19429848820454
At time: 1317.1093246936798 and batch: 300, loss is 5.2830283737182615 and perplexity is 196.96545808975097
At time: 1318.2825541496277 and batch: 350, loss is 5.292950658798218 and perplexity is 198.9295334598317
At time: 1319.4556939601898 and batch: 400, loss is 5.263282556533813 and perplexity is 193.11436082086306
At time: 1320.6304631233215 and batch: 450, loss is 5.230383949279785 and perplexity is 186.86453625253296
At time: 1321.8068034648895 and batch: 500, loss is 5.2358497047424315 and perplexity is 187.88868844538462
At time: 1322.9816975593567 and batch: 550, loss is 5.247746429443359 and perplexity is 190.13729746668824
At time: 1324.158162355423 and batch: 600, loss is 5.259463291168213 and perplexity is 192.37821249871178
At time: 1325.332927942276 and batch: 650, loss is 5.255541086196899 and perplexity is 191.62514352812178
At time: 1326.5063724517822 and batch: 700, loss is 5.264867448806763 and perplexity is 193.4206689476979
At time: 1327.680457353592 and batch: 750, loss is 5.253470249176026 and perplexity is 191.22872968267083
At time: 1328.86257314682 and batch: 800, loss is 5.262638931274414 and perplexity is 192.9901075308534
At time: 1330.0399179458618 and batch: 850, loss is 5.291216020584106 and perplexity is 198.5847618026733
At time: 1331.2203686237335 and batch: 900, loss is 5.277708749771119 and perplexity is 195.92045789049732
At time: 1332.393274307251 and batch: 950, loss is 5.264608345031738 and perplexity is 193.3705594142707
At time: 1333.5950996875763 and batch: 1000, loss is 5.256330289840698 and perplexity is 191.77643448147094
At time: 1334.7753880023956 and batch: 1050, loss is 5.233532085418701 and perplexity is 187.4537382095584
At time: 1335.9523830413818 and batch: 1100, loss is 5.199754066467285 and perplexity is 181.22766643384628
At time: 1337.1266989707947 and batch: 1150, loss is 5.241074161529541 and perplexity is 188.87287345637887
At time: 1338.2995612621307 and batch: 1200, loss is 5.2467388153076175 and perplexity is 189.94580892750983
At time: 1339.4725360870361 and batch: 1250, loss is 5.281793069839478 and perplexity is 196.72229611577268
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.103680018960994 and perplexity of 164.62662302900966
Finished 43 epochs...
Completing Train Step...
At time: 1342.4151525497437 and batch: 50, loss is 5.272449102401733 and perplexity is 194.89269058568112
At time: 1343.6166112422943 and batch: 100, loss is 5.304770259857178 and perplexity is 201.29475161608158
At time: 1344.7877976894379 and batch: 150, loss is 5.216650762557983 and perplexity is 184.31583165963383
At time: 1345.9593441486359 and batch: 200, loss is 5.259852561950684 and perplexity is 192.45311429361573
At time: 1347.1363816261292 and batch: 250, loss is 5.282465600967408 and perplexity is 196.85464248200554
At time: 1348.311220407486 and batch: 300, loss is 5.281267595291138 and perplexity is 196.6189507111408
At time: 1349.4874262809753 and batch: 350, loss is 5.291167850494385 and perplexity is 198.57519618727008
At time: 1350.6638102531433 and batch: 400, loss is 5.261555061340332 and perplexity is 192.78104467466443
At time: 1351.8365361690521 and batch: 450, loss is 5.228810634613037 and perplexity is 186.57077069038246
At time: 1353.0082795619965 and batch: 500, loss is 5.234458017349243 and perplexity is 187.62738799279907
At time: 1354.1838853359222 and batch: 550, loss is 5.246462888717652 and perplexity is 189.8934050583171
At time: 1355.3586921691895 and batch: 600, loss is 5.2581996154785156 and perplexity is 192.13526236576817
At time: 1356.533131122589 and batch: 650, loss is 5.254254503250122 and perplexity is 191.37876041642608
At time: 1357.7062792778015 and batch: 700, loss is 5.263505916595459 and perplexity is 193.1574996739699
At time: 1358.8790729045868 and batch: 750, loss is 5.252333431243897 and perplexity is 191.01146095452148
At time: 1360.059861421585 and batch: 800, loss is 5.262013454437255 and perplexity is 192.86943443184043
At time: 1361.2440991401672 and batch: 850, loss is 5.2904093551635745 and perplexity is 198.4246349353689
At time: 1362.4252038002014 and batch: 900, loss is 5.276912202835083 and perplexity is 195.76446018805217
At time: 1363.6609389781952 and batch: 950, loss is 5.263889570236206 and perplexity is 193.23161946921277
At time: 1364.8351590633392 and batch: 1000, loss is 5.25562575340271 and perplexity is 191.64136858044262
At time: 1366.0133979320526 and batch: 1050, loss is 5.233026628494263 and perplexity is 187.3590123615032
At time: 1367.1775333881378 and batch: 1100, loss is 5.199593763351441 and perplexity is 181.198617402627
At time: 1368.3476631641388 and batch: 1150, loss is 5.240484762191772 and perplexity is 188.7615847098197
At time: 1369.5115630626678 and batch: 1200, loss is 5.2461779689788814 and perplexity is 189.83930838592454
At time: 1370.6750807762146 and batch: 1250, loss is 5.280870189666748 and perplexity is 196.5408287583465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.102974327811359 and perplexity of 164.51048846053192
Finished 44 epochs...
Completing Train Step...
At time: 1373.6267342567444 and batch: 50, loss is 5.270749177932739 and perplexity is 194.56166916754455
At time: 1374.7997632026672 and batch: 100, loss is 5.3030924797058105 and perplexity is 200.957306435816
At time: 1375.9727511405945 and batch: 150, loss is 5.21497950553894 and perplexity is 184.00804979519128
At time: 1377.1448571681976 and batch: 200, loss is 5.258384771347046 and perplexity is 192.1708406308075
At time: 1378.3200376033783 and batch: 250, loss is 5.280819263458252 and perplexity is 196.53081993398098
At time: 1379.4961893558502 and batch: 300, loss is 5.2796042537689205 and perplexity is 196.29217808894384
At time: 1380.6697890758514 and batch: 350, loss is 5.289551439285279 and perplexity is 198.2544762917506
At time: 1381.85085272789 and batch: 400, loss is 5.25994104385376 and perplexity is 192.4701436648059
At time: 1383.024441242218 and batch: 450, loss is 5.227433462142944 and perplexity is 186.31400740550876
At time: 1384.2080249786377 and batch: 500, loss is 5.232971935272217 and perplexity is 187.34876537366083
At time: 1385.3906688690186 and batch: 550, loss is 5.244884328842163 and perplexity is 189.59388341709143
At time: 1386.5684492588043 and batch: 600, loss is 5.257056703567505 and perplexity is 191.91579412620774
At time: 1387.7415344715118 and batch: 650, loss is 5.252990684509277 and perplexity is 191.13704512674175
At time: 1388.9148890972137 and batch: 700, loss is 5.262261514663696 and perplexity is 192.91728360191172
At time: 1390.0895547866821 and batch: 750, loss is 5.251773843765259 and perplexity is 190.90460323360472
At time: 1391.266197681427 and batch: 800, loss is 5.261626863479615 and perplexity is 192.7948872630429
At time: 1392.469955444336 and batch: 850, loss is 5.28941930770874 and perplexity is 198.2282823458041
At time: 1393.642861366272 and batch: 900, loss is 5.275954542160034 and perplexity is 195.57707400346902
At time: 1394.8153047561646 and batch: 950, loss is 5.263070163726806 and perplexity is 193.07334907514905
At time: 1395.990329027176 and batch: 1000, loss is 5.254896507263184 and perplexity is 191.50166579728062
At time: 1397.1769840717316 and batch: 1050, loss is 5.232388591766357 and perplexity is 187.23950855827434
At time: 1398.3542687892914 and batch: 1100, loss is 5.199306411743164 and perplexity is 181.14655716865303
At time: 1399.5284368991852 and batch: 1150, loss is 5.239642152786255 and perplexity is 188.60259941380215
At time: 1400.7018883228302 and batch: 1200, loss is 5.245682325363159 and perplexity is 189.7452390590663
At time: 1401.8828465938568 and batch: 1250, loss is 5.280187482833862 and perplexity is 196.4066947839124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1024205562842155 and perplexity of 164.41941245608248
Finished 45 epochs...
Completing Train Step...
At time: 1404.8243570327759 and batch: 50, loss is 5.268992376327515 and perplexity is 194.2201629819774
At time: 1406.0298874378204 and batch: 100, loss is 5.301562070846558 and perplexity is 200.6499948099133
At time: 1407.2054805755615 and batch: 150, loss is 5.213533382415772 and perplexity is 183.74214381226875
At time: 1408.3809041976929 and batch: 200, loss is 5.256851692199707 and perplexity is 191.87645323955408
At time: 1409.555124759674 and batch: 250, loss is 5.279341039657592 and perplexity is 196.24051801685573
At time: 1410.732661485672 and batch: 300, loss is 5.278062057495117 and perplexity is 195.98969033101724
At time: 1411.9237031936646 and batch: 350, loss is 5.287971105575561 and perplexity is 197.94141549518397
At time: 1413.1084880828857 and batch: 400, loss is 5.258614950180053 and perplexity is 192.21507938185863
At time: 1414.2839484214783 and batch: 450, loss is 5.226020927429199 and perplexity is 186.05101818681953
At time: 1415.4638574123383 and batch: 500, loss is 5.23163143157959 and perplexity is 187.09779191487468
At time: 1416.639441728592 and batch: 550, loss is 5.243820581436157 and perplexity is 189.3923106456759
At time: 1417.8232131004333 and batch: 600, loss is 5.256219158172607 and perplexity is 191.75512323060582
At time: 1419.0018887519836 and batch: 650, loss is 5.25194902420044 and perplexity is 190.93804891450617
At time: 1420.1766576766968 and batch: 700, loss is 5.260989933013916 and perplexity is 192.67212942396804
At time: 1421.350965976715 and batch: 750, loss is 5.250519886016845 and perplexity is 190.66536695461318
At time: 1422.572506427765 and batch: 800, loss is 5.2609560012817385 and perplexity is 192.6655918357908
At time: 1423.7516994476318 and batch: 850, loss is 5.28858118057251 and perplexity is 198.06221144718654
At time: 1424.9272272586823 and batch: 900, loss is 5.275245771408081 and perplexity is 195.43850380671518
At time: 1426.1011130809784 and batch: 950, loss is 5.262053203582764 and perplexity is 192.87710097942215
At time: 1427.281349182129 and batch: 1000, loss is 5.254153537750244 and perplexity is 191.35943873964237
At time: 1428.4661157131195 and batch: 1050, loss is 5.2317838287353515 and perplexity is 187.12630725898524
At time: 1429.6536049842834 and batch: 1100, loss is 5.198862934112549 and perplexity is 181.06624053331774
At time: 1430.8381395339966 and batch: 1150, loss is 5.239117355346679 and perplexity is 188.50364721973267
At time: 1432.0203578472137 and batch: 1200, loss is 5.245170631408691 and perplexity is 189.64817240367907
At time: 1433.195545911789 and batch: 1250, loss is 5.279293336868286 and perplexity is 196.23115702004606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.101839608519617 and perplexity of 164.3239211063883
Finished 46 epochs...
Completing Train Step...
At time: 1436.0979971885681 and batch: 50, loss is 5.267514114379883 and perplexity is 193.93326681166917
At time: 1437.2896299362183 and batch: 100, loss is 5.30015076637268 and perplexity is 200.36701630593822
At time: 1438.4490699768066 and batch: 150, loss is 5.212101898193359 and perplexity is 183.4793079999509
At time: 1439.6093199253082 and batch: 200, loss is 5.255524406433105 and perplexity is 191.621947292647
At time: 1440.7912125587463 and batch: 250, loss is 5.278014640808106 and perplexity is 195.9803973695358
At time: 1441.9766426086426 and batch: 300, loss is 5.276613292694091 and perplexity is 195.70595295029514
At time: 1443.1568620204926 and batch: 350, loss is 5.286613187789917 and perplexity is 197.67280974010794
At time: 1444.3295257091522 and batch: 400, loss is 5.257364320755005 and perplexity is 191.9748398043007
At time: 1445.507274389267 and batch: 450, loss is 5.224784154891967 and perplexity is 185.82105763076464
At time: 1446.6817338466644 and batch: 500, loss is 5.230790596008301 and perplexity is 186.94053955707597
At time: 1447.8580689430237 and batch: 550, loss is 5.242898950576782 and perplexity is 189.21784125819178
At time: 1449.0400593280792 and batch: 600, loss is 5.255597314834595 and perplexity is 191.63591865182295
At time: 1450.2171516418457 and batch: 650, loss is 5.2510082721710205 and perplexity is 190.75850802246714
At time: 1451.3953194618225 and batch: 700, loss is 5.25977991104126 and perplexity is 192.43913290772727
At time: 1452.596069574356 and batch: 750, loss is 5.249539184570312 and perplexity is 190.47847281209417
At time: 1453.7780938148499 and batch: 800, loss is 5.260320949554443 and perplexity is 192.5432780608028
At time: 1454.954280614853 and batch: 850, loss is 5.287925682067871 and perplexity is 197.93242450597768
At time: 1456.125969171524 and batch: 900, loss is 5.274414920806885 and perplexity is 195.2761910464949
At time: 1457.3028037548065 and batch: 950, loss is 5.261553707122803 and perplexity is 192.7807836073712
At time: 1458.4788806438446 and batch: 1000, loss is 5.253537759780884 and perplexity is 191.24164008566768
At time: 1459.6557762622833 and batch: 1050, loss is 5.231474580764771 and perplexity is 187.06844777514826
At time: 1460.8363420963287 and batch: 1100, loss is 5.198835763931275 and perplexity is 181.06132099757238
At time: 1462.015052318573 and batch: 1150, loss is 5.23859543800354 and perplexity is 188.40528956651892
At time: 1463.1882147789001 and batch: 1200, loss is 5.244354782104492 and perplexity is 189.49351117288825
At time: 1464.3665957450867 and batch: 1250, loss is 5.278638439178467 and perplexity is 196.10268776034536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.10137939453125 and perplexity of 164.2483143382524
Finished 47 epochs...
Completing Train Step...
At time: 1467.3536825180054 and batch: 50, loss is 5.266032133102417 and perplexity is 193.6460742007754
At time: 1468.520071029663 and batch: 100, loss is 5.298607006072998 and perplexity is 200.05793629490765
At time: 1469.6865627765656 and batch: 150, loss is 5.210605297088623 and perplexity is 183.20491804227404
At time: 1470.8650975227356 and batch: 200, loss is 5.254148254394531 and perplexity is 191.35842772232928
At time: 1472.0404093265533 and batch: 250, loss is 5.276722574234009 and perplexity is 195.72734116685194
At time: 1473.2245562076569 and batch: 300, loss is 5.275178680419922 and perplexity is 195.42539208421445
At time: 1474.4010825157166 and batch: 350, loss is 5.285451784133911 and perplexity is 197.4433650809065
At time: 1475.5941965579987 and batch: 400, loss is 5.256297187805176 and perplexity is 191.77008639619223
At time: 1476.7734842300415 and batch: 450, loss is 5.223602800369263 and perplexity is 185.60166669866888
At time: 1477.9517171382904 and batch: 500, loss is 5.229494829177856 and perplexity is 186.69846507656402
At time: 1479.1345224380493 and batch: 550, loss is 5.241744499206543 and perplexity is 188.99952450436243
At time: 1480.3087859153748 and batch: 600, loss is 5.254248971939087 and perplexity is 191.37770184390436
At time: 1481.4982511997223 and batch: 650, loss is 5.249985303878784 and perplexity is 190.5634678942278
At time: 1482.7382180690765 and batch: 700, loss is 5.258617753982544 and perplexity is 192.21561831573248
At time: 1483.9168031215668 and batch: 750, loss is 5.248775987625122 and perplexity is 190.33315568336747
At time: 1485.0914075374603 and batch: 800, loss is 5.259723148345947 and perplexity is 192.42820985387365
At time: 1486.2659375667572 and batch: 850, loss is 5.287062892913818 and perplexity is 197.76172420665114
At time: 1487.447361946106 and batch: 900, loss is 5.273762178421021 and perplexity is 195.14876759151798
At time: 1488.6328506469727 and batch: 950, loss is 5.260963020324707 and perplexity is 192.6669441686044
At time: 1489.816785812378 and batch: 1000, loss is 5.25296272277832 and perplexity is 191.1317006788303
At time: 1490.9958193302155 and batch: 1050, loss is 5.231039342880249 and perplexity is 186.98704621548404
At time: 1492.1706557273865 and batch: 1100, loss is 5.198412666320801 and perplexity is 180.984730589061
At time: 1493.3441407680511 and batch: 1150, loss is 5.237716493606567 and perplexity is 188.2397645471947
At time: 1494.5209290981293 and batch: 1200, loss is 5.24371000289917 and perplexity is 189.3713690789133
At time: 1495.7021305561066 and batch: 1250, loss is 5.277943029403686 and perplexity is 195.96636344054212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.100954821510037 and perplexity of 164.1785937369951
Finished 48 epochs...
Completing Train Step...
At time: 1498.6691324710846 and batch: 50, loss is 5.264480047225952 and perplexity is 193.34575198719742
At time: 1499.8864650726318 and batch: 100, loss is 5.297286672592163 and perplexity is 199.79396740533983
At time: 1501.0631606578827 and batch: 150, loss is 5.209437208175659 and perplexity is 182.99104334535764
At time: 1502.2369074821472 and batch: 200, loss is 5.252973442077637 and perplexity is 191.13374948771965
At time: 1503.4099299907684 and batch: 250, loss is 5.275410795211792 and perplexity is 195.47075847332562
At time: 1504.583545923233 and batch: 300, loss is 5.2739736938476565 and perplexity is 195.1900489320188
At time: 1505.749704837799 and batch: 350, loss is 5.284188137054444 and perplexity is 197.1940239220545
At time: 1506.9186522960663 and batch: 400, loss is 5.254980106353759 and perplexity is 191.51767583158778
At time: 1508.085372686386 and batch: 450, loss is 5.222169771194458 and perplexity is 185.33588457764756
At time: 1509.2445359230042 and batch: 500, loss is 5.228283367156982 and perplexity is 186.47242392454936
At time: 1510.40451669693 and batch: 550, loss is 5.240796489715576 and perplexity is 188.82043606352735
At time: 1511.611073732376 and batch: 600, loss is 5.253325119018554 and perplexity is 191.2009786408242
At time: 1512.7873549461365 and batch: 650, loss is 5.248999195098877 and perplexity is 190.375644207921
At time: 1513.963657617569 and batch: 700, loss is 5.257621746063233 and perplexity is 192.02426534802933
At time: 1515.137945652008 and batch: 750, loss is 5.248009271621704 and perplexity is 190.1872801366282
At time: 1516.3101961612701 and batch: 800, loss is 5.25906192779541 and perplexity is 192.30101442376568
At time: 1517.4827525615692 and batch: 850, loss is 5.286290826797486 and perplexity is 197.60909800662398
At time: 1518.6572959423065 and batch: 900, loss is 5.272973833084106 and perplexity is 194.99498359599502
At time: 1519.830023765564 and batch: 950, loss is 5.260269117355347 and perplexity is 192.5332983779164
At time: 1521.005872964859 and batch: 1000, loss is 5.252557020187378 and perplexity is 191.05417378014903
At time: 1522.1839742660522 and batch: 1050, loss is 5.230556802749634 and perplexity is 186.89683922777613
At time: 1523.3588705062866 and batch: 1100, loss is 5.197868967056275 and perplexity is 180.88635606964962
At time: 1524.5319151878357 and batch: 1150, loss is 5.2369999504089355 and perplexity is 188.10493093720672
At time: 1525.7047352790833 and batch: 1200, loss is 5.2431779384613035 and perplexity is 189.2706381079364
At time: 1526.878805398941 and batch: 1250, loss is 5.276999654769898 and perplexity is 195.7815809174731
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.10037253720917 and perplexity of 164.08302294670938
Finished 49 epochs...
Completing Train Step...
At time: 1529.9019401073456 and batch: 50, loss is 5.263111753463745 and perplexity is 193.08137911192938
At time: 1531.0762224197388 and batch: 100, loss is 5.295991020202637 and perplexity is 199.53527150023172
At time: 1532.249412059784 and batch: 150, loss is 5.2082447338104245 and perplexity is 182.77296127158462
At time: 1533.4323189258575 and batch: 200, loss is 5.2518348884582515 and perplexity is 190.91625730220593
At time: 1534.612116575241 and batch: 250, loss is 5.27435079574585 and perplexity is 195.26366935030683
At time: 1535.7965242862701 and batch: 300, loss is 5.272886142730713 and perplexity is 194.97788516666827
At time: 1536.970336675644 and batch: 350, loss is 5.283025426864624 and perplexity is 196.96487766222955
At time: 1538.1445708274841 and batch: 400, loss is 5.2537667846679685 and perplexity is 191.28544419661844
At time: 1539.3206746578217 and batch: 450, loss is 5.220880784988403 and perplexity is 185.0971430792447
At time: 1540.4971332550049 and batch: 500, loss is 5.22689076423645 and perplexity is 186.21292261547228
At time: 1541.6995255947113 and batch: 550, loss is 5.23987624168396 and perplexity is 188.64675435629252
At time: 1542.872499704361 and batch: 600, loss is 5.25247899055481 and perplexity is 191.0392664747816
At time: 1544.0592329502106 and batch: 650, loss is 5.2482394599914555 and perplexity is 190.2310640756737
At time: 1545.2363131046295 and batch: 700, loss is 5.256464223861695 and perplexity is 191.80212159062393
At time: 1546.411259174347 and batch: 750, loss is 5.247324695587158 and perplexity is 190.05712703750038
At time: 1547.5871636867523 and batch: 800, loss is 5.258673181533814 and perplexity is 192.22627265204065
At time: 1548.7622003555298 and batch: 850, loss is 5.285484247207641 and perplexity is 197.44977480346378
At time: 1549.9353857040405 and batch: 900, loss is 5.2721816921234135 and perplexity is 194.84058124464633
At time: 1551.1088480949402 and batch: 950, loss is 5.2594919109344485 and perplexity is 192.3837183969706
At time: 1552.2831807136536 and batch: 1000, loss is 5.251843538284302 and perplexity is 190.91790870176388
At time: 1553.4603719711304 and batch: 1050, loss is 5.229917678833008 and perplexity is 186.77742715150558
At time: 1554.63538980484 and batch: 1100, loss is 5.197464570999146 and perplexity is 180.8132211292041
At time: 1555.813996553421 and batch: 1150, loss is 5.236416463851929 and perplexity is 187.99520625325022
At time: 1556.992670059204 and batch: 1200, loss is 5.242445487976074 and perplexity is 189.13205749512858
At time: 1558.1806907653809 and batch: 1250, loss is 5.276121282577515 and perplexity is 195.609687325342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.099957319941834 and perplexity of 164.01490698474564
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f58a487a898>
SETTINGS FOR THIS RUN
{'anneal': 4.266939190823227, 'data': 'wikitext', 'lr': 27.542070663602637, 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.7807224165803062, 'num_layers': 1, 'wordvec_source': 'glove', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.6765484809875488 and batch: 50, loss is 7.452231855392456 and perplexity is 1723.7059178723468
At time: 2.846445322036743 and batch: 100, loss is 6.687119369506836 and perplexity is 802.0086308819789
At time: 4.0137128829956055 and batch: 150, loss is 6.548490543365478 and perplexity is 698.1894912576149
At time: 5.181149482727051 and batch: 200, loss is 6.562193031311035 and perplexity is 707.8222701446334
At time: 6.348551034927368 and batch: 250, loss is 6.582832450866699 and perplexity is 722.5831145911616
At time: 7.521393537521362 and batch: 300, loss is 6.606522798538208 and perplexity is 739.9057391355575
At time: 8.692298412322998 and batch: 350, loss is 6.642919311523437 and perplexity is 767.3318070395757
At time: 9.890535354614258 and batch: 400, loss is 6.623396291732788 and perplexity is 752.4964595784207
At time: 11.061423778533936 and batch: 450, loss is 6.698904190063477 and perplexity is 811.5160703819374
At time: 12.229669094085693 and batch: 500, loss is 7.068568954467773 and perplexity is 1174.4661165969264
At time: 13.397877931594849 and batch: 550, loss is 7.327162961959839 and perplexity is 1521.0603388113345
At time: 14.56383466720581 and batch: 600, loss is 7.289276123046875 and perplexity is 1464.5101882384113
At time: 15.726882457733154 and batch: 650, loss is 6.888103427886963 and perplexity is 980.5399878120672
At time: 16.89424228668213 and batch: 700, loss is 7.111108064651489 and perplexity is 1225.504732502049
At time: 18.05834436416626 and batch: 750, loss is 7.47737340927124 and perplexity is 1767.591932717914
At time: 19.22249174118042 and batch: 800, loss is 7.222931070327759 and perplexity is 1370.5002118586467
At time: 20.387224912643433 and batch: 850, loss is 7.155833683013916 and perplexity is 1281.5604077580615
At time: 21.563425540924072 and batch: 900, loss is 7.0023386192321775 and perplexity is 1099.2007669825582
At time: 22.739022254943848 and batch: 950, loss is 6.909585027694702 and perplexity is 1001.831423724202
At time: 23.906917572021484 and batch: 1000, loss is 7.138666639328003 and perplexity is 1259.747570911192
At time: 25.070821523666382 and batch: 1050, loss is 6.925370979309082 and perplexity is 1017.7717718655502
At time: 26.236198902130127 and batch: 1100, loss is 7.097965812683105 and perplexity is 1209.5042122519765
At time: 27.39976453781128 and batch: 1150, loss is 7.182536163330078 and perplexity is 1316.2422339924517
At time: 28.56807279586792 and batch: 1200, loss is 7.106194515228271 and perplexity is 1219.4979238911824
At time: 29.736135959625244 and batch: 1250, loss is 7.253509492874145 and perplexity is 1413.0552637786113
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 7.014337470061587 and perplexity of 1112.4693577455828
Finished 1 epochs...
Completing Train Step...
At time: 32.69361424446106 and batch: 50, loss is 7.142486963272095 and perplexity is 1264.5694193668896
At time: 33.88390564918518 and batch: 100, loss is 7.136061677932739 and perplexity is 1256.4702476247896
At time: 35.04397749900818 and batch: 150, loss is 7.116623010635376 and perplexity is 1232.2819958481105
At time: 36.206212520599365 and batch: 200, loss is 7.586171293258667 and perplexity is 1970.7536130160408
At time: 37.36482787132263 and batch: 250, loss is 7.307842988967895 and perplexity is 1491.9555513027565
At time: 38.525195598602295 and batch: 300, loss is 7.113847827911377 and perplexity is 1228.8669290511264
At time: 39.695271253585815 and batch: 350, loss is 7.415272369384765 and perplexity is 1661.1615563367511
At time: 40.86776399612427 and batch: 400, loss is 7.457532711029053 and perplexity is 1732.8672942174856
At time: 42.035622119903564 and batch: 450, loss is 7.1912606716156 and perplexity is 1327.776040504064
At time: 43.20289587974548 and batch: 500, loss is 7.257729721069336 and perplexity is 1419.03128065206
At time: 44.36831831932068 and batch: 550, loss is 7.129176940917969 and perplexity is 1247.8494902782888
At time: 45.532676696777344 and batch: 600, loss is 7.324503812789917 and perplexity is 1517.0209854757313
At time: 46.72359347343445 and batch: 650, loss is 7.364833030700684 and perplexity is 1579.4516845582473
At time: 47.89339017868042 and batch: 700, loss is 7.31253752708435 and perplexity is 1498.9760596348235
At time: 49.06258201599121 and batch: 750, loss is 7.428087148666382 and perplexity is 1682.5859563212803
At time: 50.23139023780823 and batch: 800, loss is 7.094765720367431 and perplexity is 1205.6398735346497
At time: 51.39814805984497 and batch: 850, loss is 7.014600505828858 and perplexity is 1112.7620154646986
At time: 52.56149196624756 and batch: 900, loss is 7.165880393981934 and perplexity is 1294.500769956326
At time: 53.7280170917511 and batch: 950, loss is 7.3212721633911135 and perplexity is 1512.1264185419143
At time: 54.903690576553345 and batch: 1000, loss is 7.327942380905151 and perplexity is 1522.2463441938578
At time: 56.07674193382263 and batch: 1050, loss is 7.439229326248169 and perplexity is 1701.43846178357
At time: 57.25006890296936 and batch: 1100, loss is 7.358458652496338 and perplexity is 1569.41568278333
At time: 58.41850304603577 and batch: 1150, loss is 7.28748197555542 and perplexity is 1461.8849966526222
At time: 59.58653163909912 and batch: 1200, loss is 7.446306886672974 and perplexity is 1713.5232101286595
At time: 60.76053786277771 and batch: 1250, loss is 7.404585227966309 and perplexity is 1643.5030155985207
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 7.042439481637774 and perplexity of 1144.1753997394442
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 63.701592683792114 and batch: 50, loss is 6.563586368560791 and perplexity is 708.80919267813
At time: 64.90377712249756 and batch: 100, loss is 6.511969060897827 and perplexity is 673.1505879783261
At time: 66.07912683486938 and batch: 150, loss is 6.461721267700195 and perplexity is 640.1619989611486
At time: 67.2562747001648 and batch: 200, loss is 6.488234510421753 and perplexity is 657.3617727700632
At time: 68.43345212936401 and batch: 250, loss is 6.528297863006592 and perplexity is 684.2325621696626
At time: 69.60568165779114 and batch: 300, loss is 6.545738964080811 and perplexity is 696.2710081563107
At time: 70.77677273750305 and batch: 350, loss is 6.572679843902588 and perplexity is 715.2841268023583
At time: 71.94767260551453 and batch: 400, loss is 6.544097127914429 and perplexity is 695.1287831632528
At time: 73.11799645423889 and batch: 450, loss is 6.5330095386505125 and perplexity is 687.46405095289
At time: 74.29604911804199 and batch: 500, loss is 6.538124170303345 and perplexity is 690.989183540809
At time: 75.51275062561035 and batch: 550, loss is 6.534414958953858 and perplexity is 688.4309061477348
At time: 76.67877411842346 and batch: 600, loss is 6.559514293670654 and perplexity is 705.9287372580802
At time: 77.84206676483154 and batch: 650, loss is 6.545994968414306 and perplexity is 696.4492793698141
At time: 79.01543760299683 and batch: 700, loss is 6.572622871398925 and perplexity is 715.2433764356607
At time: 80.17335438728333 and batch: 750, loss is 6.498616876602173 and perplexity is 664.2222960136683
At time: 81.33438372612 and batch: 800, loss is 6.5221585845947265 and perplexity is 680.044736240213
At time: 82.49717712402344 and batch: 850, loss is 6.546911153793335 and perplexity is 697.0876484043421
At time: 83.66997623443604 and batch: 900, loss is 6.5524091720581055 and perplexity is 700.9308042168706
At time: 84.84327244758606 and batch: 950, loss is 6.5358369922637936 and perplexity is 689.4105742230955
At time: 86.01674389839172 and batch: 1000, loss is 6.533377304077148 and perplexity is 687.7169229588258
At time: 87.18886756896973 and batch: 1050, loss is 6.52672080039978 and perplexity is 683.1543350208351
At time: 88.35787558555603 and batch: 1100, loss is 6.534638433456421 and perplexity is 688.5847700937296
At time: 89.53351831436157 and batch: 1150, loss is 6.560460748672486 and perplexity is 706.5971833195557
At time: 90.70424008369446 and batch: 1200, loss is 6.556203126907349 and perplexity is 703.5951550580011
At time: 91.87522053718567 and batch: 1250, loss is 6.543415079116821 and perplexity is 694.6548330594359
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.121020686017336 and perplexity of 455.32920555262154
Finished 3 epochs...
Completing Train Step...
At time: 94.84264469146729 and batch: 50, loss is 6.5479460620880126 and perplexity is 697.8094436253011
At time: 96.01239275932312 and batch: 100, loss is 6.56582859992981 and perplexity is 710.400290022173
At time: 97.18204736709595 and batch: 150, loss is 6.4835426616668705 and perplexity is 654.2847548509757
At time: 98.34908866882324 and batch: 200, loss is 6.51232533454895 and perplexity is 673.3904565229446
At time: 99.5178849697113 and batch: 250, loss is 6.547423849105835 and perplexity is 697.4451336067435
At time: 100.6994321346283 and batch: 300, loss is 6.539069423675537 and perplexity is 691.6426521947673
At time: 101.87180638313293 and batch: 350, loss is 6.575382890701294 and perplexity is 717.2201887256556
At time: 103.03980875015259 and batch: 400, loss is 6.554485359191895 and perplexity is 702.3875794799989
At time: 104.23424243927002 and batch: 450, loss is 6.55465012550354 and perplexity is 702.5033188255259
At time: 105.40516328811646 and batch: 500, loss is 6.530465726852417 and perplexity is 685.7174941869905
At time: 106.57565760612488 and batch: 550, loss is 6.5231040096282955 and perplexity is 680.6879715752835
At time: 107.74678707122803 and batch: 600, loss is 6.552024421691894 and perplexity is 700.6611727069966
At time: 108.91433811187744 and batch: 650, loss is 6.53076735496521 and perplexity is 685.9243570569316
At time: 110.08801460266113 and batch: 700, loss is 6.562106304168701 and perplexity is 707.7608854037632
At time: 111.2552101612091 and batch: 750, loss is 6.488397903442383 and perplexity is 657.4691898711261
At time: 112.42403197288513 and batch: 800, loss is 6.506637992858887 and perplexity is 669.571524984026
At time: 113.59387707710266 and batch: 850, loss is 6.553033542633057 and perplexity is 701.3685814394868
At time: 114.76625204086304 and batch: 900, loss is 6.541687507629394 and perplexity is 693.4558031793648
At time: 115.93403744697571 and batch: 950, loss is 6.513552598953247 and perplexity is 674.217391990868
At time: 117.10146474838257 and batch: 1000, loss is 6.519037437438965 and perplexity is 677.9255254506382
At time: 118.26903796195984 and batch: 1050, loss is 6.516878604888916 and perplexity is 676.4635763791988
At time: 119.43959164619446 and batch: 1100, loss is 6.508751468658447 and perplexity is 670.9881446665519
At time: 120.61590480804443 and batch: 1150, loss is 6.537491283416748 and perplexity is 690.5520039049877
At time: 121.78948879241943 and batch: 1200, loss is 6.532216987609863 and perplexity is 686.9194164577251
At time: 122.95720171928406 and batch: 1250, loss is 6.516138515472412 and perplexity is 675.9631180604658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.090859963075958 and perplexity of 441.80118053091314
Finished 4 epochs...
Completing Train Step...
At time: 125.93423533439636 and batch: 50, loss is 6.513807601928711 and perplexity is 674.3893413548035
At time: 127.12998127937317 and batch: 100, loss is 6.521436719894409 and perplexity is 679.5540130896538
At time: 128.30482578277588 and batch: 150, loss is 6.448402738571167 and perplexity is 631.6925085034577
At time: 129.473060131073 and batch: 200, loss is 6.468915843963623 and perplexity is 644.784301097986
At time: 130.64280033111572 and batch: 250, loss is 6.51014554977417 and perplexity is 671.9242088910339
At time: 131.81261610984802 and batch: 300, loss is 6.496381187438965 and perplexity is 662.7389601812795
At time: 132.99221467971802 and batch: 350, loss is 6.518606939315796 and perplexity is 677.6337425947755
At time: 134.19438409805298 and batch: 400, loss is 6.485118436813354 and perplexity is 655.3165732497812
At time: 135.36556100845337 and batch: 450, loss is 6.463169078826905 and perplexity is 641.089503890103
At time: 136.53947043418884 and batch: 500, loss is 6.436419878005982 and perplexity is 624.1681966775043
At time: 137.72375583648682 and batch: 550, loss is 6.439428291320801 and perplexity is 626.0487799586075
At time: 138.90019965171814 and batch: 600, loss is 6.455014715194702 and perplexity is 635.8830833272988
At time: 140.074360370636 and batch: 650, loss is 6.431556911468506 and perplexity is 621.1402559776267
At time: 141.24618411064148 and batch: 700, loss is 6.461709995269775 and perplexity is 640.1547828202293
At time: 142.41679048538208 and batch: 750, loss is 6.4006697463989255 and perplexity is 602.2482564309246
At time: 143.58856344223022 and batch: 800, loss is 6.4119578456878665 and perplexity is 609.085008921132
At time: 144.76440930366516 and batch: 850, loss is 6.4460959720611575 and perplexity is 630.2370207608168
At time: 145.93932366371155 and batch: 900, loss is 6.423483533859253 and perplexity is 616.1457445593908
At time: 147.11182951927185 and batch: 950, loss is 6.411146602630615 and perplexity is 608.5910933062635
At time: 148.28203010559082 and batch: 1000, loss is 6.403117628097534 and perplexity is 603.7242947624194
At time: 149.4526846408844 and batch: 1050, loss is 6.411182289123535 and perplexity is 608.6128121755386
At time: 150.62197589874268 and batch: 1100, loss is 6.4120309543609615 and perplexity is 609.1295399457185
At time: 151.78413653373718 and batch: 1150, loss is 6.429752779006958 and perplexity is 620.0206469438286
At time: 152.9504804611206 and batch: 1200, loss is 6.408424396514892 and perplexity is 606.9366358189191
At time: 154.11463928222656 and batch: 1250, loss is 6.397404680252075 and perplexity is 600.2850827269127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.004934046390283 and perplexity of 405.424248660942
Finished 5 epochs...
Completing Train Step...
At time: 157.03732466697693 and batch: 50, loss is 6.3910734081268314 and perplexity is 596.4965203804044
At time: 158.20941352844238 and batch: 100, loss is 6.41773136138916 and perplexity is 612.6117418108986
At time: 159.3818736076355 and batch: 150, loss is 6.33645115852356 and perplexity is 564.7884060991873
At time: 160.55205535888672 and batch: 200, loss is 6.360319480895996 and perplexity is 578.4311245711536
At time: 161.71999621391296 and batch: 250, loss is 6.393198442459107 and perplexity is 597.7654437406343
At time: 162.88842368125916 and batch: 300, loss is 6.403961267471313 and perplexity is 604.2338352523769
At time: 164.10208749771118 and batch: 350, loss is 6.430420160293579 and perplexity is 620.43457522955
At time: 165.28124451637268 and batch: 400, loss is 6.407262573242187 and perplexity is 606.2318921834199
At time: 166.45999598503113 and batch: 450, loss is 6.391283931732178 and perplexity is 596.6221101977984
At time: 167.63225317001343 and batch: 500, loss is 6.368992471694947 and perplexity is 583.4696704397966
At time: 168.80527472496033 and batch: 550, loss is 6.371092376708984 and perplexity is 584.696188661469
At time: 169.98065614700317 and batch: 600, loss is 6.390445718765259 and perplexity is 596.1222233437696
At time: 171.1488938331604 and batch: 650, loss is 6.373648815155029 and perplexity is 586.1928407117413
At time: 172.31793975830078 and batch: 700, loss is 6.392868432998657 and perplexity is 597.5682080356845
At time: 173.48735165596008 and batch: 750, loss is 6.345742835998535 and perplexity is 570.0606940780141
At time: 174.65790700912476 and batch: 800, loss is 6.347365341186523 and perplexity is 570.9863712665722
At time: 175.82588648796082 and batch: 850, loss is 6.37995717048645 and perplexity is 589.9024418827843
At time: 177.00011086463928 and batch: 900, loss is 6.360840139389038 and perplexity is 578.7323680644874
At time: 178.1744031906128 and batch: 950, loss is 6.343006725311279 and perplexity is 568.5030767989375
At time: 179.3474462032318 and batch: 1000, loss is 6.347847146987915 and perplexity is 571.2615420968921
At time: 180.51541996002197 and batch: 1050, loss is 6.344788990020752 and perplexity is 569.5172032223655
At time: 181.6843798160553 and batch: 1100, loss is 6.3399719142913815 and perplexity is 566.7803927290927
At time: 182.8524045944214 and batch: 1150, loss is 6.370303821563721 and perplexity is 584.2353052133736
At time: 184.0277876853943 and batch: 1200, loss is 6.347296142578125 and perplexity is 570.9468611713039
At time: 185.20055174827576 and batch: 1250, loss is 6.337754573822021 and perplexity is 565.525039913694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.958661378735173 and perplexity of 387.0916082087356
Finished 6 epochs...
Completing Train Step...
At time: 188.1970579624176 and batch: 50, loss is 6.321003093719482 and perplexity is 556.1305638322676
At time: 189.39320302009583 and batch: 100, loss is 6.355097818374634 and perplexity is 575.418624421194
At time: 190.57250785827637 and batch: 150, loss is 6.281272115707398 and perplexity is 534.4681368617743
At time: 191.74867177009583 and batch: 200, loss is 6.303600749969482 and perplexity is 546.5363119521663
At time: 192.93315196037292 and batch: 250, loss is 6.321946983337402 and perplexity is 556.6557375116778
At time: 194.13094329833984 and batch: 300, loss is 6.337088890075684 and perplexity is 565.1487043606043
At time: 195.30597972869873 and batch: 350, loss is 6.368940916061401 and perplexity is 583.4395900666956
At time: 196.47817063331604 and batch: 400, loss is 6.3332375812530515 and perplexity is 562.9763281004906
At time: 197.65582919120789 and batch: 450, loss is 6.315780544281006 and perplexity is 553.2337155194681
At time: 198.83102226257324 and batch: 500, loss is 6.295900363922119 and perplexity is 542.3439335432346
At time: 200.0099265575409 and batch: 550, loss is 6.303131189346313 and perplexity is 546.2797402636368
At time: 201.18976497650146 and batch: 600, loss is 6.333707609176636 and perplexity is 563.2410048928836
At time: 202.36053276062012 and batch: 650, loss is 6.318407049179077 and perplexity is 554.6886965039638
At time: 203.53096270561218 and batch: 700, loss is 6.342228794097901 and perplexity is 568.0609924884808
At time: 204.7053108215332 and batch: 750, loss is 6.289907445907593 and perplexity is 539.1034305467938
At time: 205.87933921813965 and batch: 800, loss is 6.3035260105133055 and perplexity is 546.4954656518643
At time: 207.05030846595764 and batch: 850, loss is 6.332071933746338 and perplexity is 562.3204784663075
At time: 208.2256305217743 and batch: 900, loss is 6.31629846572876 and perplexity is 553.5203213396003
At time: 209.40064001083374 and batch: 950, loss is 6.293960857391357 and perplexity is 541.2930733467327
At time: 210.5884885787964 and batch: 1000, loss is 6.29427243232727 and perplexity is 541.461752978179
At time: 211.76312351226807 and batch: 1050, loss is 6.305091524124146 and perplexity is 547.3516817757325
At time: 212.93259477615356 and batch: 1100, loss is 6.289836359024048 and perplexity is 539.0651087261134
At time: 214.1007649898529 and batch: 1150, loss is 6.320558996200561 and perplexity is 555.883642461316
At time: 215.27130126953125 and batch: 1200, loss is 6.312669296264648 and perplexity is 551.5151430583514
At time: 216.45822858810425 and batch: 1250, loss is 6.304768495559692 and perplexity is 547.1749001020195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.945838649777601 and perplexity of 382.15972511084186
Finished 7 epochs...
Completing Train Step...
At time: 219.44459223747253 and batch: 50, loss is 6.285274343490601 and perplexity is 536.6114863137393
At time: 220.6400501728058 and batch: 100, loss is 6.314621953964234 and perplexity is 552.5931154619251
At time: 221.8152310848236 and batch: 150, loss is 6.2369695568084715 and perplexity is 511.306674923193
At time: 223.0114951133728 and batch: 200, loss is 6.266327571868897 and perplexity is 526.5401420454484
At time: 224.18411779403687 and batch: 250, loss is 6.286766157150269 and perplexity is 537.4126080725732
At time: 225.3697600364685 and batch: 300, loss is 6.297444467544556 and perplexity is 543.1820156519184
At time: 226.5445966720581 and batch: 350, loss is 6.334480180740356 and perplexity is 563.6763170100146
At time: 227.71685314178467 and batch: 400, loss is 6.297180442810059 and perplexity is 543.0386210951405
At time: 228.88679575920105 and batch: 450, loss is 6.284822845458985 and perplexity is 536.3692619699482
At time: 230.05741786956787 and batch: 500, loss is 6.2671130180358885 and perplexity is 526.9538734423852
At time: 231.23256134986877 and batch: 550, loss is 6.273438796997071 and perplexity is 530.2978325908982
At time: 232.41107034683228 and batch: 600, loss is 6.306317195892334 and perplexity is 548.022966582736
At time: 233.58036613464355 and batch: 650, loss is 6.287098398208618 and perplexity is 537.5911882704439
At time: 234.75748109817505 and batch: 700, loss is 6.314944915771484 and perplexity is 552.7716107551986
At time: 235.9195191860199 and batch: 750, loss is 6.2595118713378906 and perplexity is 522.9636042702158
At time: 237.08638644218445 and batch: 800, loss is 6.278712882995605 and perplexity is 533.1020573258387
At time: 238.2489514350891 and batch: 850, loss is 6.30745608329773 and perplexity is 548.6474585827808
At time: 239.4063277244568 and batch: 900, loss is 6.2904394912719725 and perplexity is 539.3903343440871
At time: 240.56811809539795 and batch: 950, loss is 6.270538787841797 and perplexity is 528.7621917837978
At time: 241.73760509490967 and batch: 1000, loss is 6.267961416244507 and perplexity is 527.4011298635844
At time: 242.91073656082153 and batch: 1050, loss is 6.279429960250854 and perplexity is 533.4844697791234
At time: 244.08302068710327 and batch: 1100, loss is 6.265636415481567 and perplexity is 526.1763461974823
At time: 245.25922775268555 and batch: 1150, loss is 6.299505949020386 and perplexity is 544.3029302906077
At time: 246.4309525489807 and batch: 1200, loss is 6.290572004318237 and perplexity is 539.4618153363944
At time: 247.6036024093628 and batch: 1250, loss is 6.27878134727478 and perplexity is 533.1385570233687
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.908330541457573 and perplexity of 368.09112929171096
Finished 8 epochs...
Completing Train Step...
At time: 250.5791094303131 and batch: 50, loss is 6.25887056350708 and perplexity is 522.6283311337032
At time: 251.7480731010437 and batch: 100, loss is 6.295873966217041 and perplexity is 542.3296170969876
At time: 252.94465279579163 and batch: 150, loss is 6.218660726547241 and perplexity is 502.03042556345645
At time: 254.1140124797821 and batch: 200, loss is 6.246516494750977 and perplexity is 516.2114636214769
At time: 255.2878201007843 and batch: 250, loss is 6.264264240264892 and perplexity is 525.4548351886011
At time: 256.45975947380066 and batch: 300, loss is 6.277785615921021 and perplexity is 532.6079584568968
At time: 257.62860321998596 and batch: 350, loss is 6.315626153945923 and perplexity is 553.1483081739535
At time: 258.7984344959259 and batch: 400, loss is 6.283119344711304 and perplexity is 535.4563343383708
At time: 259.96918845176697 and batch: 450, loss is 6.261929178237915 and perplexity is 524.2292969667983
At time: 261.1415503025055 and batch: 500, loss is 6.252422742843628 and perplexity is 519.2693580462181
At time: 262.320378780365 and batch: 550, loss is 6.260137128829956 and perplexity is 523.2906934286805
At time: 263.4995975494385 and batch: 600, loss is 6.285584030151367 and perplexity is 536.7776934678055
At time: 264.67095851898193 and batch: 650, loss is 6.27825626373291 and perplexity is 532.8586882252196
At time: 265.8397927284241 and batch: 700, loss is 6.302045555114746 and perplexity is 545.6870020842406
At time: 267.00776529312134 and batch: 750, loss is 6.244764556884766 and perplexity is 515.3078849493435
At time: 268.18908643722534 and batch: 800, loss is 6.265888175964355 and perplexity is 526.3088332852396
At time: 269.3598747253418 and batch: 850, loss is 6.295970144271851 and perplexity is 542.38177981304
At time: 270.5320625305176 and batch: 900, loss is 6.2779482555389405 and perplexity is 532.6945886563216
At time: 271.699688911438 and batch: 950, loss is 6.252900066375733 and perplexity is 519.51727669431
At time: 272.86891984939575 and batch: 1000, loss is 6.256366271972656 and perplexity is 521.3211548867597
At time: 274.03596782684326 and batch: 1050, loss is 6.270720739364624 and perplexity is 528.8584096230315
At time: 275.20429372787476 and batch: 1100, loss is 6.254318027496338 and perplexity is 520.254454515375
At time: 276.3815360069275 and batch: 1150, loss is 6.289878273010254 and perplexity is 539.0877035671614
At time: 277.56475472450256 and batch: 1200, loss is 6.280967292785644 and perplexity is 534.3052435507807
At time: 278.7418291568756 and batch: 1250, loss is 6.271158256530762 and perplexity is 529.0898448804554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.8936767578125 and perplexity of 362.73652990908596
Finished 9 epochs...
Completing Train Step...
At time: 281.74999260902405 and batch: 50, loss is 6.2472593879699705 and perplexity is 516.5950960987075
At time: 282.9476652145386 and batch: 100, loss is 6.284171667098999 and perplexity is 536.0201036081144
At time: 284.12369322776794 and batch: 150, loss is 6.205919513702392 and perplexity is 495.6745259752034
At time: 285.3062448501587 and batch: 200, loss is 6.2377597618103025 and perplexity is 511.71087169330906
At time: 286.4779567718506 and batch: 250, loss is 6.256491756439209 and perplexity is 521.3865766984081
At time: 287.64945006370544 and batch: 300, loss is 6.271443300247192 and perplexity is 529.2406801124619
At time: 288.8247535228729 and batch: 350, loss is 6.304766111373901 and perplexity is 547.1735955369526
At time: 289.9993999004364 and batch: 400, loss is 6.273402280807495 and perplexity is 530.2784684882657
At time: 291.175918340683 and batch: 450, loss is 6.258236436843872 and perplexity is 522.2970236305463
At time: 292.3527536392212 and batch: 500, loss is 6.244097213745118 and perplexity is 514.964112487371
At time: 293.5330846309662 and batch: 550, loss is 6.251806983947754 and perplexity is 518.9497117422737
At time: 294.7140233516693 and batch: 600, loss is 6.283348836898804 and perplexity is 535.5792314852769
At time: 295.8887906074524 and batch: 650, loss is 6.268922777175903 and perplexity is 527.9083964991033
At time: 297.0650818347931 and batch: 700, loss is 6.295284900665283 and perplexity is 542.0102434770919
At time: 298.24694538116455 and batch: 750, loss is 6.239374694824218 and perplexity is 512.5379181061189
At time: 299.42121291160583 and batch: 800, loss is 6.255357618331909 and perplexity is 520.7955875081685
At time: 300.5903515815735 and batch: 850, loss is 6.28908187866211 and perplexity is 538.6585480780717
At time: 301.76144313812256 and batch: 900, loss is 6.270767068862915 and perplexity is 528.8829119354019
At time: 302.9311077594757 and batch: 950, loss is 6.246497087478637 and perplexity is 516.2014454622306
At time: 304.1055715084076 and batch: 1000, loss is 6.250800685882568 and perplexity is 518.4277563168323
At time: 305.2881133556366 and batch: 1050, loss is 6.264867753982544 and perplexity is 525.7720501017913
At time: 306.46763730049133 and batch: 1100, loss is 6.242448835372925 and perplexity is 514.1159560153313
At time: 307.64662647247314 and batch: 1150, loss is 6.283263816833496 and perplexity is 535.5336984396771
At time: 308.8150861263275 and batch: 1200, loss is 6.270180492401123 and perplexity is 528.5727726373156
At time: 309.98157477378845 and batch: 1250, loss is 6.259193601608277 and perplexity is 522.7971872694386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.897417277315237 and perplexity of 364.0958937520895
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 312.91458678245544 and batch: 50, loss is 6.220096673965454 and perplexity is 502.7518326842965
At time: 314.07388734817505 and batch: 100, loss is 6.233190279006958 and perplexity is 509.37795184270453
At time: 315.2467088699341 and batch: 150, loss is 6.139916181564331 and perplexity is 464.0146762449659
At time: 316.4250192642212 and batch: 200, loss is 6.155843009948731 and perplexity is 471.4641239281261
At time: 317.6008038520813 and batch: 250, loss is 6.171373310089112 and perplexity is 478.8432550245769
At time: 318.78466534614563 and batch: 300, loss is 6.182179641723633 and perplexity is 484.0458539153398
At time: 319.96491265296936 and batch: 350, loss is 6.201170845031738 and perplexity is 493.32631174120183
At time: 321.13691210746765 and batch: 400, loss is 6.182939672470093 and perplexity is 484.41388348619085
At time: 322.3086109161377 and batch: 450, loss is 6.158242540359497 and perplexity is 472.59677480272654
At time: 323.4789650440216 and batch: 500, loss is 6.136847686767578 and perplexity is 462.59303189491334
At time: 324.6501100063324 and batch: 550, loss is 6.148641929626465 and perplexity is 468.0812676309229
At time: 325.82275891304016 and batch: 600, loss is 6.176555833816528 and perplexity is 481.3313131969109
At time: 326.998183965683 and batch: 650, loss is 6.151393966674805 and perplexity is 469.3712188036433
At time: 328.1764326095581 and batch: 700, loss is 6.164361019134521 and perplexity is 475.49721222255783
At time: 329.3490860462189 and batch: 750, loss is 6.123880386352539 and perplexity is 456.6331742255515
At time: 330.52166628837585 and batch: 800, loss is 6.125421190261841 and perplexity is 457.33729872510224
At time: 331.6961889266968 and batch: 850, loss is 6.159675979614258 and perplexity is 473.2746993371416
At time: 332.8702960014343 and batch: 900, loss is 6.1412453365325925 and perplexity is 464.63183371534353
At time: 334.0454902648926 and batch: 950, loss is 6.108309412002564 and perplexity is 449.5780211322581
At time: 335.22244572639465 and batch: 1000, loss is 6.101268587112426 and perplexity is 446.42373841778516
At time: 336.3976285457611 and batch: 1050, loss is 6.099686641693115 and perplexity is 445.7180787344375
At time: 337.57524132728577 and batch: 1100, loss is 6.081360673904419 and perplexity is 437.6242536879102
At time: 338.7463355064392 and batch: 1150, loss is 6.1286807632446285 and perplexity is 458.83045523231
At time: 339.91751980781555 and batch: 1200, loss is 6.120593318939209 and perplexity is 455.13465441579507
At time: 341.087735414505 and batch: 1250, loss is 6.1114734935760495 and perplexity is 451.0027754956307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.739709116246578 and perplexity of 310.9739405567969
Finished 11 epochs...
Completing Train Step...
At time: 344.0593590736389 and batch: 50, loss is 6.115302248001099 and perplexity is 452.7328642970971
At time: 345.2619709968567 and batch: 100, loss is 6.147734775543213 and perplexity is 467.65683833818673
At time: 346.4401066303253 and batch: 150, loss is 6.073648567199707 and perplexity is 434.26222955478954
At time: 347.6152505874634 and batch: 200, loss is 6.0905375671386714 and perplexity is 441.6587685829719
At time: 348.79308891296387 and batch: 250, loss is 6.1200480651855464 and perplexity is 454.8865581809014
At time: 349.9850935935974 and batch: 300, loss is 6.13516921043396 and perplexity is 461.81723170205015
At time: 351.1630563735962 and batch: 350, loss is 6.152820014953614 and perplexity is 470.04104230915567
At time: 352.3443088531494 and batch: 400, loss is 6.131566734313965 and perplexity is 460.1565392515044
At time: 353.51802229881287 and batch: 450, loss is 6.105925903320313 and perplexity is 448.5077240534367
At time: 354.69416308403015 and batch: 500, loss is 6.09741358757019 and perplexity is 444.7060880084815
At time: 355.87024331092834 and batch: 550, loss is 6.110959110260009 and perplexity is 450.7708468476553
At time: 357.04551553726196 and batch: 600, loss is 6.143844566345215 and perplexity is 465.84108951548274
At time: 358.2190809249878 and batch: 650, loss is 6.122533559799194 and perplexity is 456.01858250849125
At time: 359.3930718898773 and batch: 700, loss is 6.137333917617798 and perplexity is 462.8180135902051
At time: 360.56790232658386 and batch: 750, loss is 6.097271728515625 and perplexity is 444.6430068976963
At time: 361.7463049888611 and batch: 800, loss is 6.10515775680542 and perplexity is 448.1633366952109
At time: 362.9239640235901 and batch: 850, loss is 6.14015947341919 and perplexity is 464.1275809700743
At time: 364.10053396224976 and batch: 900, loss is 6.125075263977051 and perplexity is 457.1791210929359
At time: 365.27770590782166 and batch: 950, loss is 6.094715261459351 and perplexity is 443.5077434488584
At time: 366.45674228668213 and batch: 1000, loss is 6.089230766296387 and perplexity is 441.08198548458716
At time: 367.6352114677429 and batch: 1050, loss is 6.088227481842041 and perplexity is 440.63967670332204
At time: 368.81299233436584 and batch: 1100, loss is 6.079810009002686 and perplexity is 436.9461709929614
At time: 369.98874592781067 and batch: 1150, loss is 6.13578010559082 and perplexity is 462.09943980325596
At time: 371.171391248703 and batch: 1200, loss is 6.129328374862671 and perplexity is 459.1276954035356
At time: 372.4056785106659 and batch: 1250, loss is 6.111835021972656 and perplexity is 451.1658552831314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.736958524606524 and perplexity of 310.119753534016
Finished 12 epochs...
Completing Train Step...
At time: 375.36400151252747 and batch: 50, loss is 6.1037859535217285 and perplexity is 447.5489662523434
At time: 376.56282114982605 and batch: 100, loss is 6.1357104778289795 and perplexity is 462.06726597362353
At time: 377.74076628685 and batch: 150, loss is 6.062363519668579 and perplexity is 429.3891080738835
At time: 378.91301584243774 and batch: 200, loss is 6.079822177886963 and perplexity is 436.9514881727036
At time: 380.0842809677124 and batch: 250, loss is 6.110059242248536 and perplexity is 450.3653950359899
At time: 381.2594065666199 and batch: 300, loss is 6.124596138000488 and perplexity is 456.96012716710675
At time: 382.44093799591064 and batch: 350, loss is 6.140246772766114 and perplexity is 464.1681007734328
At time: 383.61591362953186 and batch: 400, loss is 6.118185758590698 and perplexity is 454.04020826981196
At time: 384.78713488578796 and batch: 450, loss is 6.090703573226929 and perplexity is 441.7320927134465
At time: 385.9565360546112 and batch: 500, loss is 6.087863149642945 and perplexity is 440.47916672215246
At time: 387.12848925590515 and batch: 550, loss is 6.104652729034424 and perplexity is 447.93705890733014
At time: 388.308354139328 and batch: 600, loss is 6.138891267776489 and perplexity is 463.5393448339891
At time: 389.48169803619385 and batch: 650, loss is 6.119207572937012 and perplexity is 454.504390181906
At time: 390.65237379074097 and batch: 700, loss is 6.133205661773681 and perplexity is 460.9113207865934
At time: 391.82394790649414 and batch: 750, loss is 6.093187389373779 and perplexity is 442.83063774507895
At time: 392.9866609573364 and batch: 800, loss is 6.102214813232422 and perplexity is 446.84635613409074
At time: 394.15834736824036 and batch: 850, loss is 6.137823553085327 and perplexity is 463.0446811923905
At time: 395.3215548992157 and batch: 900, loss is 6.123620471954346 and perplexity is 456.51450411158123
At time: 396.4800605773926 and batch: 950, loss is 6.094148235321045 and perplexity is 443.25633425031134
At time: 397.6461522579193 and batch: 1000, loss is 6.0876031017303465 and perplexity is 440.3646359266959
At time: 398.8159399032593 and batch: 1050, loss is 6.08859055519104 and perplexity is 440.7996902730181
At time: 399.991441488266 and batch: 1100, loss is 6.081420946121216 and perplexity is 437.65063106670766
At time: 401.19374108314514 and batch: 1150, loss is 6.13801568031311 and perplexity is 463.1336532300295
At time: 402.3647389411926 and batch: 1200, loss is 6.13068042755127 and perplexity is 459.74888008109343
At time: 403.5365982055664 and batch: 1250, loss is 6.11020936012268 and perplexity is 450.4330080065118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.736219865562272 and perplexity of 309.89076535577294
Finished 13 epochs...
Completing Train Step...
At time: 406.5077238082886 and batch: 50, loss is 6.098669271469117 and perplexity is 445.2648490231345
At time: 407.6827735900879 and batch: 100, loss is 6.129918909072876 and perplexity is 459.39890608623546
At time: 408.8616452217102 and batch: 150, loss is 6.056218900680542 and perplexity is 426.75876511082015
At time: 410.0347375869751 and batch: 200, loss is 6.075087604522705 and perplexity is 434.8875989681309
At time: 411.2093918323517 and batch: 250, loss is 6.104968843460083 and perplexity is 448.0786806565976
At time: 412.3857681751251 and batch: 300, loss is 6.119472913742065 and perplexity is 454.6250047439719
At time: 413.56203055381775 and batch: 350, loss is 6.135654287338257 and perplexity is 462.0413029166469
At time: 414.74510645866394 and batch: 400, loss is 6.113571329116821 and perplexity is 451.94989825359323
At time: 415.9208810329437 and batch: 450, loss is 6.086953325271606 and perplexity is 440.07859029593186
At time: 417.09129762649536 and batch: 500, loss is 6.084551029205322 and perplexity is 439.02266006594357
At time: 418.2624204158783 and batch: 550, loss is 6.102079706192017 and perplexity is 446.785988123561
At time: 419.43438386917114 and batch: 600, loss is 6.135928382873535 and perplexity is 462.1679637326797
At time: 420.60617446899414 and batch: 650, loss is 6.116934032440185 and perplexity is 453.4722298186364
At time: 421.77834820747375 and batch: 700, loss is 6.131305341720581 and perplexity is 460.03627345932546
At time: 422.9510235786438 and batch: 750, loss is 6.091508474349975 and perplexity is 442.0877865009184
At time: 424.127464056015 and batch: 800, loss is 6.101074314117431 and perplexity is 446.3370187650019
At time: 425.3078727722168 and batch: 850, loss is 6.137236108779907 and perplexity is 462.7727481118592
At time: 426.48381090164185 and batch: 900, loss is 6.1227786827087405 and perplexity is 456.13037681135546
At time: 427.66605710983276 and batch: 950, loss is 6.09309157371521 and perplexity is 442.78820966855835
At time: 428.8421251773834 and batch: 1000, loss is 6.086762990951538 and perplexity is 439.9948362075628
At time: 430.01366448402405 and batch: 1050, loss is 6.088444299697876 and perplexity is 440.73522561120126
At time: 431.2464954853058 and batch: 1100, loss is 6.081393871307373 and perplexity is 437.638781917751
At time: 432.423544883728 and batch: 1150, loss is 6.1386964321136475 and perplexity is 463.44903963610744
At time: 433.5986533164978 and batch: 1200, loss is 6.129784622192383 and perplexity is 459.3372189822124
At time: 434.7707567214966 and batch: 1250, loss is 6.107647285461426 and perplexity is 449.28044212050696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.735728020215556 and perplexity of 309.7383845018215
Finished 14 epochs...
Completing Train Step...
At time: 437.7187466621399 and batch: 50, loss is 6.094869861602783 and perplexity is 443.57631511006656
At time: 438.9169292449951 and batch: 100, loss is 6.126173439025879 and perplexity is 457.6814595738739
At time: 440.09007573127747 and batch: 150, loss is 6.053185024261475 and perplexity is 425.465993802513
At time: 441.26091504096985 and batch: 200, loss is 6.072129964828491 and perplexity is 433.6032583874874
At time: 442.4312756061554 and batch: 250, loss is 6.101778974533081 and perplexity is 446.651645633696
At time: 443.60431480407715 and batch: 300, loss is 6.1161900806427 and perplexity is 453.13499379737925
At time: 444.77898478507996 and batch: 350, loss is 6.131962795257568 and perplexity is 460.3388253804709
At time: 445.9541380405426 and batch: 400, loss is 6.110526485443115 and perplexity is 450.5758743705837
At time: 447.12876534461975 and batch: 450, loss is 6.084787073135376 and perplexity is 439.1263009314258
At time: 448.30060386657715 and batch: 500, loss is 6.081616039276123 and perplexity is 437.73602203839596
At time: 449.4699492454529 and batch: 550, loss is 6.099949645996094 and perplexity is 445.8353199238529
At time: 450.6404366493225 and batch: 600, loss is 6.134468603134155 and perplexity is 461.49379249350915
At time: 451.8106770515442 and batch: 650, loss is 6.115695571899414 and perplexity is 452.9109699764917
At time: 452.98178005218506 and batch: 700, loss is 6.130092811584473 and perplexity is 459.4788036568216
At time: 454.15270805358887 and batch: 750, loss is 6.090838861465454 and perplexity is 441.7918579128431
At time: 455.323965549469 and batch: 800, loss is 6.100470275878906 and perplexity is 446.0674955477581
At time: 456.4946255683899 and batch: 850, loss is 6.136826868057251 and perplexity is 462.58340140483045
At time: 457.6773281097412 and batch: 900, loss is 6.121539659500122 and perplexity is 455.5655706645491
At time: 458.8519604206085 and batch: 950, loss is 6.092040300369263 and perplexity is 442.3229628194883
At time: 460.0289294719696 and batch: 1000, loss is 6.085135803222657 and perplexity is 439.27946418944265
At time: 461.2318603992462 and batch: 1050, loss is 6.087534351348877 and perplexity is 440.33436173068344
At time: 462.4021635055542 and batch: 1100, loss is 6.081023502349853 and perplexity is 437.47672411076485
At time: 463.57405138015747 and batch: 1150, loss is 6.1374531269073485 and perplexity is 462.87318908544694
At time: 464.7460904121399 and batch: 1200, loss is 6.128747291564942 and perplexity is 458.86098146722344
At time: 465.92797565460205 and batch: 1250, loss is 6.105910396575927 and perplexity is 448.50076921272847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.734354951955976 and perplexity of 309.3133844011343
Finished 15 epochs...
Completing Train Step...
At time: 468.9111108779907 and batch: 50, loss is 6.092125244140625 and perplexity is 442.36053699593396
At time: 470.0821747779846 and batch: 100, loss is 6.123409204483032 and perplexity is 456.41806763398364
At time: 471.25527834892273 and batch: 150, loss is 6.05097728729248 and perplexity is 424.5277129187488
At time: 472.4393627643585 and batch: 200, loss is 6.069671430587769 and perplexity is 432.5385392903296
At time: 473.62384057044983 and batch: 250, loss is 6.09868369102478 and perplexity is 445.2712695907007
At time: 474.79507398605347 and batch: 300, loss is 6.113857421875 and perplexity is 452.07921634414583
At time: 475.96720719337463 and batch: 350, loss is 6.129922084808349 and perplexity is 459.40036501795464
At time: 477.1398344039917 and batch: 400, loss is 6.1087123394012455 and perplexity is 449.7592049343251
At time: 478.3031339645386 and batch: 450, loss is 6.082403621673584 and perplexity is 438.0809110205061
At time: 479.46737027168274 and batch: 500, loss is 6.079064474105835 and perplexity is 436.6205337764783
At time: 480.6333529949188 and batch: 550, loss is 6.0982700538635255 and perplexity is 445.0871269335075
At time: 481.79232025146484 and batch: 600, loss is 6.132937479019165 and perplexity is 460.7877288923451
At time: 482.9512162208557 and batch: 650, loss is 6.114307432174683 and perplexity is 452.2827024297564
At time: 484.1206042766571 and batch: 700, loss is 6.128468151092529 and perplexity is 458.73291267150876
At time: 485.29651713371277 and batch: 750, loss is 6.089136028289795 and perplexity is 441.04020023589567
At time: 486.47068309783936 and batch: 800, loss is 6.099148740768433 and perplexity is 445.4783910376349
At time: 487.65360164642334 and batch: 850, loss is 6.13536449432373 and perplexity is 461.90742597387725
At time: 488.8315441608429 and batch: 900, loss is 6.1198147106170655 and perplexity is 454.78042070872294
At time: 490.0553226470947 and batch: 950, loss is 6.090009317398072 and perplexity is 441.4255240641365
At time: 491.2309379577637 and batch: 1000, loss is 6.083174514770508 and perplexity is 438.41875477468375
At time: 492.4032573699951 and batch: 1050, loss is 6.085994968414306 and perplexity is 439.65703999124776
At time: 493.5823414325714 and batch: 1100, loss is 6.079849519729614 and perplexity is 436.96343539486827
At time: 494.7570700645447 and batch: 1150, loss is 6.136586399078369 and perplexity is 462.47217782009363
At time: 495.94053888320923 and batch: 1200, loss is 6.12752911567688 and perplexity is 458.3023484093703
At time: 497.1152911186218 and batch: 1250, loss is 6.104191570281983 and perplexity is 447.7305364355551
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.733863106609261 and perplexity of 309.16128745949266
Finished 16 epochs...
Completing Train Step...
At time: 500.0750620365143 and batch: 50, loss is 6.089463691711426 and perplexity is 441.1847366553375
At time: 501.275337934494 and batch: 100, loss is 6.121585330963135 and perplexity is 455.5863774857948
At time: 502.4518988132477 and batch: 150, loss is 6.049588470458985 and perplexity is 423.93853091240794
At time: 503.6303789615631 and batch: 200, loss is 6.067134943008423 and perplexity is 431.4428009087061
At time: 504.81738352775574 and batch: 250, loss is 6.096731472015381 and perplexity is 444.4028505018
At time: 505.9944496154785 and batch: 300, loss is 6.111801795959472 and perplexity is 451.1508650895094
At time: 507.17335867881775 and batch: 350, loss is 6.128460340499878 and perplexity is 458.7293297095845
At time: 508.3492784500122 and batch: 400, loss is 6.107336711883545 and perplexity is 449.140929151768
At time: 509.52237820625305 and batch: 450, loss is 6.0808924865722656 and perplexity is 437.41941151208823
At time: 510.6970500946045 and batch: 500, loss is 6.077422742843628 and perplexity is 435.9043082822172
At time: 511.8726370334625 and batch: 550, loss is 6.09693395614624 and perplexity is 444.49284413756953
At time: 513.0460538864136 and batch: 600, loss is 6.131499691009521 and perplexity is 460.1256898706856
At time: 514.2197666168213 and batch: 650, loss is 6.113199319839477 and perplexity is 451.78179996754784
At time: 515.3935978412628 and batch: 700, loss is 6.12709755897522 and perplexity is 458.10460763077594
At time: 516.5698063373566 and batch: 750, loss is 6.087574224472046 and perplexity is 440.3519195869654
At time: 517.7479758262634 and batch: 800, loss is 6.098081436157226 and perplexity is 445.00318353737634
At time: 518.9252061843872 and batch: 850, loss is 6.134660291671753 and perplexity is 461.5822640429227
At time: 520.1284866333008 and batch: 900, loss is 6.118419380187988 and perplexity is 454.14629426000846
At time: 521.3000979423523 and batch: 950, loss is 6.088376770019531 and perplexity is 440.70546390809074
At time: 522.4742453098297 and batch: 1000, loss is 6.082069511413574 and perplexity is 437.9345681421097
At time: 523.6460611820221 and batch: 1050, loss is 6.084621391296387 and perplexity is 439.053551705118
At time: 524.8263831138611 and batch: 1100, loss is 6.078851175308228 and perplexity is 436.52741307323305
At time: 526.0122058391571 and batch: 1150, loss is 6.135923681259155 and perplexity is 462.16579080224335
At time: 527.1936192512512 and batch: 1200, loss is 6.126460886001587 and perplexity is 457.8130376352176
At time: 528.3826060295105 and batch: 1250, loss is 6.102610130310058 and perplexity is 447.02303704994085
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.733893846943431 and perplexity of 309.1707913268568
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 531.3506932258606 and batch: 50, loss is 6.088759117126465 and perplexity is 440.87399858454563
At time: 532.5527472496033 and batch: 100, loss is 6.129710006713867 and perplexity is 459.3029465944591
At time: 533.7276449203491 and batch: 150, loss is 6.047648611068726 and perplexity is 423.11694690868603
At time: 534.9014871120453 and batch: 200, loss is 6.06895884513855 and perplexity is 432.23042841170695
At time: 536.0731253623962 and batch: 250, loss is 6.0970055198669435 and perplexity is 444.5246548375544
At time: 537.2444221973419 and batch: 300, loss is 6.109446430206299 and perplexity is 450.08949024602856
At time: 538.4159464836121 and batch: 350, loss is 6.12058648109436 and perplexity is 455.13154228628315
At time: 539.5912857055664 and batch: 400, loss is 6.091753377914428 and perplexity is 442.1960686344326
At time: 540.7759664058685 and batch: 450, loss is 6.067226753234864 and perplexity is 431.48241358835054
At time: 541.9484899044037 and batch: 500, loss is 6.056554441452026 and perplexity is 426.90198410266487
At time: 543.1213717460632 and batch: 550, loss is 6.07980583190918 and perplexity is 436.94434583176013
At time: 544.2932515144348 and batch: 600, loss is 6.110662775039673 and perplexity is 450.637287359601
At time: 545.4782748222351 and batch: 650, loss is 6.099084959030152 and perplexity is 445.4499785575961
At time: 546.6543459892273 and batch: 700, loss is 6.107897739410401 and perplexity is 449.3929802736573
At time: 547.8372626304626 and batch: 750, loss is 6.061899671554565 and perplexity is 429.18998293140663
At time: 549.011510848999 and batch: 800, loss is 6.0657705879211425 and perplexity is 430.85456110365016
At time: 550.219484090805 and batch: 850, loss is 6.096232576370239 and perplexity is 444.18119515103825
At time: 551.3934967517853 and batch: 900, loss is 6.077747030258179 and perplexity is 436.04568948617515
At time: 552.5651767253876 and batch: 950, loss is 6.0477007961273195 and perplexity is 423.13902786749543
At time: 553.7382965087891 and batch: 1000, loss is 6.041310997009277 and perplexity is 420.44387439520705
At time: 554.9117453098297 and batch: 1050, loss is 6.034398679733276 and perplexity is 417.54765426665654
At time: 556.0839107036591 and batch: 1100, loss is 6.028080749511719 and perplexity is 414.91793326708927
At time: 557.2548162937164 and batch: 1150, loss is 6.078861341476441 and perplexity is 436.5318509069018
At time: 558.4258036613464 and batch: 1200, loss is 6.082713851928711 and perplexity is 438.21683805654754
At time: 559.5987215042114 and batch: 1250, loss is 6.072237529754639 and perplexity is 433.64990139848425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.699317208171761 and perplexity of 298.66340639879036
Finished 18 epochs...
Completing Train Step...
At time: 562.5761897563934 and batch: 50, loss is 6.066596546173096 and perplexity is 431.21057599026244
At time: 563.7500293254852 and batch: 100, loss is 6.109059362411499 and perplexity is 449.91530881176294
At time: 564.9230246543884 and batch: 150, loss is 6.028472394943237 and perplexity is 415.08046580559403
At time: 566.0979828834534 and batch: 200, loss is 6.047800807952881 and perplexity is 423.18134889040493
At time: 567.2660109996796 and batch: 250, loss is 6.080068502426148 and perplexity is 437.05913330394816
At time: 568.4306318759918 and batch: 300, loss is 6.094418468475342 and perplexity is 443.37613299374516
At time: 569.6091530323029 and batch: 350, loss is 6.1060515308380126 and perplexity is 448.56407250486257
At time: 570.7714486122131 and batch: 400, loss is 6.079149122238159 and perplexity is 436.6574944535008
At time: 571.9317739009857 and batch: 450, loss is 6.055047235488892 and perplexity is 426.259039533209
At time: 573.0993354320526 and batch: 500, loss is 6.046220397949218 and perplexity is 422.51307706408977
At time: 574.2777059078217 and batch: 550, loss is 6.068882360458374 and perplexity is 432.1973706698491
At time: 575.4554941654205 and batch: 600, loss is 6.100983810424805 and perplexity is 446.2966254445477
At time: 576.6294589042664 and batch: 650, loss is 6.0904492664337155 and perplexity is 441.6197715241153
At time: 577.8036546707153 and batch: 700, loss is 6.099378280639648 and perplexity is 445.5806578268466
At time: 578.9788336753845 and batch: 750, loss is 6.055533256530762 and perplexity is 426.4662607485658
At time: 580.2021338939667 and batch: 800, loss is 6.061659021377563 and perplexity is 429.086710712784
At time: 581.3887362480164 and batch: 850, loss is 6.095388460159302 and perplexity is 443.80641280579795
At time: 582.5743398666382 and batch: 900, loss is 6.077091512680053 and perplexity is 435.7599475365186
At time: 583.7478544712067 and batch: 950, loss is 6.048668155670166 and perplexity is 423.5485534913783
At time: 584.9219965934753 and batch: 1000, loss is 6.044217166900634 and perplexity is 421.66753294256023
At time: 586.0968432426453 and batch: 1050, loss is 6.03982590675354 and perplexity is 419.8199407078516
At time: 587.2738218307495 and batch: 1100, loss is 6.034706621170044 and perplexity is 417.67625429085234
At time: 588.4486048221588 and batch: 1150, loss is 6.086172170639038 and perplexity is 439.7349551000147
At time: 589.6226630210876 and batch: 1200, loss is 6.088964109420776 and perplexity is 440.9643836208193
At time: 590.8000376224518 and batch: 1250, loss is 6.074011669158936 and perplexity is 434.4199396518498
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.698689036125685 and perplexity of 298.4758533096758
Finished 19 epochs...
Completing Train Step...
At time: 593.7620377540588 and batch: 50, loss is 6.062290620803833 and perplexity is 429.35780723628244
At time: 594.963968038559 and batch: 100, loss is 6.1043854999542235 and perplexity is 447.81737309156824
At time: 596.1366844177246 and batch: 150, loss is 6.023391904830933 and perplexity is 412.9770014426481
At time: 597.3078365325928 and batch: 200, loss is 6.042144641876221 and perplexity is 420.7945214101694
At time: 598.48868060112 and batch: 250, loss is 6.075140228271485 and perplexity is 434.91048498605517
At time: 599.668074131012 and batch: 300, loss is 6.089857006072998 and perplexity is 441.3582950776431
At time: 600.8404786586761 and batch: 350, loss is 6.101740951538086 and perplexity is 446.63466292327826
At time: 602.0125114917755 and batch: 400, loss is 6.074724521636963 and perplexity is 434.7297273856933
At time: 603.1855852603912 and batch: 450, loss is 6.050688457489014 and perplexity is 424.4051143687704
At time: 604.3594267368317 and batch: 500, loss is 6.042889232635498 and perplexity is 421.107957798804
At time: 605.5320870876312 and batch: 550, loss is 6.065749950408936 and perplexity is 430.8456694291374
At time: 606.703750371933 and batch: 600, loss is 6.098671283721924 and perplexity is 445.26574500947834
At time: 607.8809678554535 and batch: 650, loss is 6.088529396057129 and perplexity is 440.77273217010827
At time: 609.0820748806 and batch: 700, loss is 6.097499647140503 and perplexity is 444.74436087018034
At time: 610.2569532394409 and batch: 750, loss is 6.054412841796875 and perplexity is 425.9887092443454
At time: 611.4319429397583 and batch: 800, loss is 6.061191625595093 and perplexity is 428.88620425546924
At time: 612.6110138893127 and batch: 850, loss is 6.096024122238159 and perplexity is 444.08861339537674
At time: 613.7822353839874 and batch: 900, loss is 6.077776670455933 and perplexity is 436.05861415818526
At time: 614.9539539813995 and batch: 950, loss is 6.0502666664123534 and perplexity is 424.22614182580475
At time: 616.1247129440308 and batch: 1000, loss is 6.046192779541015 and perplexity is 422.5014080865963
At time: 617.2959434986115 and batch: 1050, loss is 6.042630186080933 and perplexity is 420.9988853612657
At time: 618.4678449630737 and batch: 1100, loss is 6.037631673812866 and perplexity is 418.89976787076677
At time: 619.6401233673096 and batch: 1150, loss is 6.089058408737182 and perplexity is 441.0059682214231
At time: 620.8215053081512 and batch: 1200, loss is 6.091405429840088 and perplexity is 442.0422341286324
At time: 621.9959013462067 and batch: 1250, loss is 6.074682655334473 and perplexity is 434.7115272404144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.698403462876368 and perplexity of 298.3906287599087
Finished 20 epochs...
Completing Train Step...
At time: 624.9702174663544 and batch: 50, loss is 6.059503622055054 and perplexity is 428.16285350548236
At time: 626.1465828418732 and batch: 100, loss is 6.102033519744873 and perplexity is 446.7653531426681
At time: 627.3234260082245 and batch: 150, loss is 6.02083194732666 and perplexity is 411.92114991285285
At time: 628.4991700649261 and batch: 200, loss is 6.03943431854248 and perplexity is 419.65557635197484
At time: 629.6757872104645 and batch: 250, loss is 6.072840280532837 and perplexity is 433.9113630041951
At time: 630.8504490852356 and batch: 300, loss is 6.087716732025147 and perplexity is 440.4146775331632
At time: 632.0249967575073 and batch: 350, loss is 6.099650583267212 and perplexity is 445.70200713188194
At time: 633.198410987854 and batch: 400, loss is 6.072493324279785 and perplexity is 433.7608408573404
At time: 634.3717494010925 and batch: 450, loss is 6.048447885513306 and perplexity is 423.45526865937234
At time: 635.5442326068878 and batch: 500, loss is 6.04132227897644 and perplexity is 420.44861785594964
At time: 636.7171759605408 and batch: 550, loss is 6.064521017074585 and perplexity is 430.316514038962
At time: 637.8907611370087 and batch: 600, loss is 6.097819776535034 and perplexity is 444.886759404909
At time: 639.110791683197 and batch: 650, loss is 6.08795129776001 and perplexity is 440.5179958426372
At time: 640.2837257385254 and batch: 700, loss is 6.096803731918335 and perplexity is 444.4349641689013
At time: 641.4659526348114 and batch: 750, loss is 6.053953676223755 and perplexity is 425.793154793894
At time: 642.6436321735382 and batch: 800, loss is 6.061060762405395 and perplexity is 428.8300825109783
At time: 643.819785118103 and batch: 850, loss is 6.096421022415161 and perplexity is 444.26490722785013
At time: 644.9863293170929 and batch: 900, loss is 6.078305072784424 and perplexity is 436.28908943173826
At time: 646.154130935669 and batch: 950, loss is 6.051276063919067 and perplexity is 424.6545708268526
At time: 647.3146076202393 and batch: 1000, loss is 6.047351083755493 and perplexity is 422.9910767861172
At time: 648.4800913333893 and batch: 1050, loss is 6.043991565704346 and perplexity is 421.5724149724596
At time: 649.648220539093 and batch: 1100, loss is 6.038956136703491 and perplexity is 419.45495264786143
At time: 650.8166213035583 and batch: 1150, loss is 6.090304527282715 and perplexity is 441.55585647893616
At time: 651.9897999763489 and batch: 1200, loss is 6.092464876174927 and perplexity is 442.5108023210233
At time: 653.1566805839539 and batch: 1250, loss is 6.0745518684387205 and perplexity is 434.6546763869731
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.698292084854015 and perplexity of 298.3573964524973
Finished 21 epochs...
Completing Train Step...
At time: 656.0994324684143 and batch: 50, loss is 6.057949647903443 and perplexity is 427.4980162022363
At time: 657.2834665775299 and batch: 100, loss is 6.1004850292205814 and perplexity is 446.0740765824762
At time: 658.4432518482208 and batch: 150, loss is 6.0192368125915525 and perplexity is 411.26460395734347
At time: 659.623480796814 and batch: 200, loss is 6.0378712558746335 and perplexity is 419.00014076411935
At time: 660.8013308048248 and batch: 250, loss is 6.07150505065918 and perplexity is 433.332378214706
At time: 661.9727087020874 and batch: 300, loss is 6.086383981704712 and perplexity is 439.82810569428335
At time: 663.1454997062683 and batch: 350, loss is 6.098408193588257 and perplexity is 445.14861539360516
At time: 664.3238430023193 and batch: 400, loss is 6.0711119556427 and perplexity is 433.1620708920234
At time: 665.498868227005 and batch: 450, loss is 6.047111148834229 and perplexity is 422.8895986299765
At time: 666.6714673042297 and batch: 500, loss is 6.040372295379639 and perplexity is 420.0493882265126
At time: 667.8789746761322 and batch: 550, loss is 6.063767347335816 and perplexity is 429.9923196873198
At time: 669.1160922050476 and batch: 600, loss is 6.0973615550994875 and perplexity is 444.6829494539692
At time: 670.2887964248657 and batch: 650, loss is 6.087688055038452 and perplexity is 440.4020479484056
At time: 671.4538178443909 and batch: 700, loss is 6.096455278396607 and perplexity is 444.28012621893856
At time: 672.6289644241333 and batch: 750, loss is 6.0537362384796145 and perplexity is 425.7005813556897
At time: 673.7935452461243 and batch: 800, loss is 6.060935964584351 and perplexity is 428.7765687903499
At time: 674.9549930095673 and batch: 850, loss is 6.096650285720825 and perplexity is 444.36677254561545
At time: 676.1173460483551 and batch: 900, loss is 6.078507070541382 and perplexity is 436.3772277507602
At time: 677.2806513309479 and batch: 950, loss is 6.0518254470825195 and perplexity is 424.8879329951016
At time: 678.4435613155365 and batch: 1000, loss is 6.0479493331909175 and perplexity is 423.2442066688492
At time: 679.6074254512787 and batch: 1050, loss is 6.0447069358825685 and perplexity is 421.8741032026079
At time: 680.7706515789032 and batch: 1100, loss is 6.039680109024048 and perplexity is 419.7587363755356
At time: 681.9339559078217 and batch: 1150, loss is 6.0910715675354 and perplexity is 441.89467752274186
At time: 683.096752166748 and batch: 1200, loss is 6.093077154159546 and perplexity is 442.7818249053544
At time: 684.2606680393219 and batch: 1250, loss is 6.074359674453735 and perplexity is 434.5711463998634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.698230604185675 and perplexity of 298.3390538042243
Finished 22 epochs...
Completing Train Step...
At time: 687.3334882259369 and batch: 50, loss is 6.056772117614746 and perplexity is 426.9949206030852
At time: 688.5255262851715 and batch: 100, loss is 6.099324750900268 and perplexity is 445.55680664873984
At time: 689.6887092590332 and batch: 150, loss is 6.0179791831970215 and perplexity is 410.747710600744
At time: 690.8501832485199 and batch: 200, loss is 6.036764621734619 and perplexity is 418.53671737104685
At time: 692.0260500907898 and batch: 250, loss is 6.070546245574951 and perplexity is 432.91709604645405
At time: 693.2029089927673 and batch: 300, loss is 6.085422811508178 and perplexity is 439.4055591296053
At time: 694.3644871711731 and batch: 350, loss is 6.097518396377564 and perplexity is 444.75269956580553
At time: 695.5280332565308 and batch: 400, loss is 6.070126218795776 and perplexity is 432.7352974557631
At time: 696.6912140846252 and batch: 450, loss is 6.046165409088135 and perplexity is 422.48984418996974
At time: 697.8546662330627 and batch: 500, loss is 6.0396523189544675 and perplexity is 419.74707141313047
At time: 699.0898251533508 and batch: 550, loss is 6.0631529808044435 and perplexity is 429.728227930232
At time: 700.2667925357819 and batch: 600, loss is 6.096975517272949 and perplexity is 444.5113181448836
At time: 701.4444444179535 and batch: 650, loss is 6.0874909496307374 and perplexity is 440.3152508775534
At time: 702.6208295822144 and batch: 700, loss is 6.096204719543457 and perplexity is 444.16882184476634
At time: 703.8046615123749 and batch: 750, loss is 6.053504247665405 and perplexity is 425.60183418587246
At time: 704.9873912334442 and batch: 800, loss is 6.060743684768677 and perplexity is 428.694131636492
At time: 706.1691064834595 and batch: 850, loss is 6.0967375183105466 and perplexity is 444.4055375007299
At time: 707.3489396572113 and batch: 900, loss is 6.078574256896973 and perplexity is 436.4065473312825
At time: 708.5304019451141 and batch: 950, loss is 6.052161712646484 and perplexity is 425.0308322002063
At time: 709.7072465419769 and batch: 1000, loss is 6.048302345275879 and perplexity is 423.39364336362075
At time: 710.8862307071686 and batch: 1050, loss is 6.045123596191406 and perplexity is 422.04991802172367
At time: 712.0668787956238 and batch: 1100, loss is 6.0401412391662594 and perplexity is 419.95234441715604
At time: 713.2301912307739 and batch: 1150, loss is 6.0914108848571775 and perplexity is 442.0446454831509
At time: 714.3951280117035 and batch: 1200, loss is 6.0933935165405275 and perplexity is 442.92192657801786
At time: 715.5597116947174 and batch: 1250, loss is 6.0741252422332765 and perplexity is 434.46928086181435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.698160213275547 and perplexity of 298.31805418580007
Finished 23 epochs...
Completing Train Step...
At time: 718.5530164241791 and batch: 50, loss is 6.0558067893981935 and perplexity is 426.58292924333784
At time: 719.7187721729279 and batch: 100, loss is 6.098487672805786 and perplexity is 445.18399686326796
At time: 720.8868265151978 and batch: 150, loss is 6.017054891586303 and perplexity is 410.3682353376311
At time: 722.0718882083893 and batch: 200, loss is 6.035960607528686 and perplexity is 418.2003431475236
At time: 723.2647035121918 and batch: 250, loss is 6.069757614135742 and perplexity is 432.57581860268965
At time: 724.4533998966217 and batch: 300, loss is 6.084776172637939 and perplexity is 439.12151426239654
At time: 725.6371653079987 and batch: 350, loss is 6.096813097000122 and perplexity is 444.43912635817907
At time: 726.8150956630707 and batch: 400, loss is 6.069245157241821 and perplexity is 432.3541989324169
At time: 728.0645968914032 and batch: 450, loss is 6.045355176925659 and perplexity is 422.14766796969604
At time: 729.2514419555664 and batch: 500, loss is 6.039111051559448 and perplexity is 419.5199374848599
At time: 730.4440286159515 and batch: 550, loss is 6.062643032073975 and perplexity is 429.5091444313762
At time: 731.6224291324615 and batch: 600, loss is 6.096553621292114 and perplexity is 444.32382016142634
At time: 732.800035238266 and batch: 650, loss is 6.087293300628662 and perplexity is 440.2282316075382
At time: 733.99280834198 and batch: 700, loss is 6.096031894683838 and perplexity is 444.09206506341496
At time: 735.1857867240906 and batch: 750, loss is 6.053288078308105 and perplexity is 425.5098420542091
At time: 736.3705480098724 and batch: 800, loss is 6.06052788734436 and perplexity is 428.6016305281746
At time: 737.55992603302 and batch: 850, loss is 6.096814022064209 and perplexity is 444.43953749304416
At time: 738.7514526844025 and batch: 900, loss is 6.078594560623169 and perplexity is 436.41540810028283
At time: 739.9318971633911 and batch: 950, loss is 6.05229549407959 and perplexity is 425.0876972377104
At time: 741.1180138587952 and batch: 1000, loss is 6.048494815826416 and perplexity is 423.47514201404647
At time: 742.3093194961548 and batch: 1050, loss is 6.045336408615112 and perplexity is 422.13974504551726
At time: 743.4792654514313 and batch: 1100, loss is 6.040370950698852 and perplexity is 420.0488233945507
At time: 744.6487491130829 and batch: 1150, loss is 6.091569480895996 and perplexity is 442.11475757250815
At time: 745.8211944103241 and batch: 1200, loss is 6.093561124801636 and perplexity is 442.99617017368456
At time: 746.9909031391144 and batch: 1250, loss is 6.073827066421509 and perplexity is 434.3397519434601
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.698058636519161 and perplexity of 298.2877535444308
Finished 24 epochs...
Completing Train Step...
At time: 750.1171147823334 and batch: 50, loss is 6.055055885314942 and perplexity is 426.2627266156995
At time: 751.3206820487976 and batch: 100, loss is 6.097775239944458 and perplexity is 444.8669461066642
At time: 752.5015993118286 and batch: 150, loss is 6.016384887695312 and perplexity is 410.0933791108685
At time: 753.6753611564636 and batch: 200, loss is 6.035417165756225 and perplexity is 417.9731373539511
At time: 754.8354637622833 and batch: 250, loss is 6.069219627380371 and perplexity is 432.3431611305183
At time: 755.9954855442047 and batch: 300, loss is 6.084158344268799 and perplexity is 438.85029632508054
At time: 757.1562972068787 and batch: 350, loss is 6.096206378936768 and perplexity is 444.1695588961497
At time: 758.3777060508728 and batch: 400, loss is 6.068498411178589 and perplexity is 432.03146065320936
At time: 759.5393362045288 and batch: 450, loss is 6.044709434509278 and perplexity is 421.875157309827
At time: 760.7013335227966 and batch: 500, loss is 6.038667249679565 and perplexity is 419.33379505619075
At time: 761.8626372814178 and batch: 550, loss is 6.062306928634643 and perplexity is 429.36480918785304
At time: 763.0251822471619 and batch: 600, loss is 6.096327705383301 and perplexity is 444.22345167963255
At time: 764.1883075237274 and batch: 650, loss is 6.087099723815918 and perplexity is 440.1430218771635
At time: 765.3507957458496 and batch: 700, loss is 6.095772361755371 and perplexity is 443.97682350439646
At time: 766.5126271247864 and batch: 750, loss is 6.053092956542969 and perplexity is 425.42682392233024
At time: 767.6824510097504 and batch: 800, loss is 6.060202989578247 and perplexity is 428.4624014346991
At time: 768.845166683197 and batch: 850, loss is 6.096784095764161 and perplexity is 444.426237261106
At time: 770.0144414901733 and batch: 900, loss is 6.0785948848724365 and perplexity is 436.4155496076822
At time: 771.1755499839783 and batch: 950, loss is 6.052376003265381 and perplexity is 425.12192207979336
At time: 772.3442618846893 and batch: 1000, loss is 6.04857141494751 and perplexity is 423.50758108011604
At time: 773.5105223655701 and batch: 1050, loss is 6.045468053817749 and perplexity is 422.1953213758932
At time: 774.6772663593292 and batch: 1100, loss is 6.040472564697265 and perplexity is 420.09150840368494
At time: 775.8430595397949 and batch: 1150, loss is 6.091668748855591 and perplexity is 442.1586475807992
At time: 777.0149734020233 and batch: 1200, loss is 6.093620557785034 and perplexity is 443.0224995401204
At time: 778.189608335495 and batch: 1250, loss is 6.073536539077759 and perplexity is 434.21358269773845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.6978594926151915 and perplexity of 298.228357271083
Finished 25 epochs...
Completing Train Step...
At time: 781.2647817134857 and batch: 50, loss is 6.054476509094238 and perplexity is 426.01583165756637
At time: 782.4289910793304 and batch: 100, loss is 6.097198543548584 and perplexity is 444.610466904625
At time: 783.5924634933472 and batch: 150, loss is 6.015810289382935 and perplexity is 409.85780783322485
At time: 784.7562987804413 and batch: 200, loss is 6.035083770751953 and perplexity is 417.83381042477885
At time: 785.9199669361115 and batch: 250, loss is 6.068793458938599 and perplexity is 432.1589493746348
At time: 787.081417798996 and batch: 300, loss is 6.083600482940674 and perplexity is 438.6055469903664
At time: 788.2762207984924 and batch: 350, loss is 6.095630159378052 and perplexity is 443.9136934333492
At time: 789.4396171569824 and batch: 400, loss is 6.067932653427124 and perplexity is 431.7871046351416
At time: 790.6030261516571 and batch: 450, loss is 6.044158411026001 and perplexity is 421.642758225695
At time: 791.7653110027313 and batch: 500, loss is 6.038254699707031 and perplexity is 419.1608345904286
At time: 792.9294037818909 and batch: 550, loss is 6.061898899078369 and perplexity is 429.18965139248934
At time: 794.0916359424591 and batch: 600, loss is 6.096006631851196 and perplexity is 444.0808461816087
At time: 795.2546284198761 and batch: 650, loss is 6.086929407119751 and perplexity is 440.0680645552594
At time: 796.4173851013184 and batch: 700, loss is 6.0956503105163575 and perplexity is 443.92263888971155
At time: 797.5801544189453 and batch: 750, loss is 6.0528781127929685 and perplexity is 425.3354334458159
At time: 798.7431464195251 and batch: 800, loss is 6.060024785995483 and perplexity is 428.3860547025164
At time: 799.907101392746 and batch: 850, loss is 6.096749877929687 and perplexity is 444.41103021786125
At time: 801.0709450244904 and batch: 900, loss is 6.0785273170471195 and perplexity is 436.38606295424637
At time: 802.2339768409729 and batch: 950, loss is 6.052446374893188 and perplexity is 425.1518396541287
At time: 803.3960490226746 and batch: 1000, loss is 6.0486298942565915 and perplexity is 423.5323482350243
At time: 804.558895111084 and batch: 1050, loss is 6.045573196411133 and perplexity is 422.2397144206562
At time: 805.7229204177856 and batch: 1100, loss is 6.0404678440094 and perplexity is 420.08952528747966
At time: 806.8861730098724 and batch: 1150, loss is 6.091565313339234 and perplexity is 442.11291503800015
At time: 808.0489840507507 and batch: 1200, loss is 6.09363166809082 and perplexity is 443.0274216829038
At time: 809.2106382846832 and batch: 1250, loss is 6.073245277404785 and perplexity is 434.087131339325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.697712473625685 and perplexity of 298.18451526223754
Finished 26 epochs...
Completing Train Step...
At time: 812.1189498901367 and batch: 50, loss is 6.053968648910523 and perplexity is 425.7995301091563
At time: 813.3074955940247 and batch: 100, loss is 6.096683197021484 and perplexity is 444.381397474731
At time: 814.4667813777924 and batch: 150, loss is 6.015229959487915 and perplexity is 409.6200240978104
At time: 815.6269772052765 and batch: 200, loss is 6.03456862449646 and perplexity is 417.61862033386376
At time: 816.7878384590149 and batch: 250, loss is 6.068299036026001 and perplexity is 431.9453329009712
At time: 817.974612236023 and batch: 300, loss is 6.083087606430054 and perplexity is 438.3806541839359
At time: 819.1359448432922 and batch: 350, loss is 6.095217018127442 and perplexity is 443.7303322545225
At time: 820.2964823246002 and batch: 400, loss is 6.067397289276123 and perplexity is 431.5560031656961
At time: 821.4579467773438 and batch: 450, loss is 6.043688459396362 and perplexity is 421.4446530779059
At time: 822.6190404891968 and batch: 500, loss is 6.0378564357757565 and perplexity is 418.99393118661715
At time: 823.77951836586 and batch: 550, loss is 6.061465511322021 and perplexity is 429.00368615286425
At time: 824.9416699409485 and batch: 600, loss is 6.095670728683472 and perplexity is 443.9317030688749
At time: 826.1025238037109 and batch: 650, loss is 6.086744594573974 and perplexity is 439.98674197088246
At time: 827.2627427577972 and batch: 700, loss is 6.095297403335572 and perplexity is 443.766003043317
At time: 828.4287724494934 and batch: 750, loss is 6.052792596817016 and perplexity is 425.2990620263087
At time: 829.5913414955139 and batch: 800, loss is 6.05985279083252 and perplexity is 428.3123807091934
At time: 830.7510459423065 and batch: 850, loss is 6.0967068862915035 and perplexity is 444.39192467033763
At time: 831.9109194278717 and batch: 900, loss is 6.078431615829468 and perplexity is 436.34430227496114
At time: 833.0707025527954 and batch: 950, loss is 6.052429800033569 and perplexity is 425.1447928804695
At time: 834.2304301261902 and batch: 1000, loss is 6.04860595703125 and perplexity is 423.52221016710433
At time: 835.3904809951782 and batch: 1050, loss is 6.045583362579346 and perplexity is 422.2440070024387
At time: 836.5502650737762 and batch: 1100, loss is 6.0403625011444095 and perplexity is 420.0452741841434
At time: 837.7098298072815 and batch: 1150, loss is 6.09146598815918 and perplexity is 442.06900427386813
At time: 838.8696136474609 and batch: 1200, loss is 6.093594522476196 and perplexity is 443.01096546267013
At time: 840.036652803421 and batch: 1250, loss is 6.072993392944336 and perplexity is 433.9778053058021
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.697580156535127 and perplexity of 298.145062964893
Finished 27 epochs...
Completing Train Step...
At time: 843.1623232364655 and batch: 50, loss is 6.053402881622315 and perplexity is 425.55869479848064
At time: 844.3897888660431 and batch: 100, loss is 6.096110544204712 and perplexity is 444.1269940651127
At time: 845.5760011672974 and batch: 150, loss is 6.014646339416504 and perplexity is 409.38103137735305
At time: 846.8033974170685 and batch: 200, loss is 6.03403681755066 and perplexity is 417.39658689556984
At time: 847.9788846969604 and batch: 250, loss is 6.067700538635254 and perplexity is 431.68689209207383
At time: 849.1402924060822 and batch: 300, loss is 6.082384176254273 and perplexity is 438.07239243632307
At time: 850.3024487495422 and batch: 350, loss is 6.094796400070191 and perplexity is 443.5437305110089
At time: 851.4644951820374 and batch: 400, loss is 6.067016496658325 and perplexity is 431.39170111001454
At time: 852.626522064209 and batch: 450, loss is 6.043364953994751 and perplexity is 421.3083355070792
At time: 853.7881488800049 and batch: 500, loss is 6.037395095825195 and perplexity is 418.80067712845437
At time: 854.9501836299896 and batch: 550, loss is 6.061114015579224 and perplexity is 428.8529196819758
At time: 856.111832857132 and batch: 600, loss is 6.095385570526123 and perplexity is 443.80513036991533
At time: 857.274377822876 and batch: 650, loss is 6.086602611541748 and perplexity is 439.9242757537944
At time: 858.4362151622772 and batch: 700, loss is 6.095046834945679 and perplexity is 443.6548232400992
At time: 859.5985064506531 and batch: 750, loss is 6.052502927780151 and perplexity is 425.175883897938
At time: 860.7602727413177 and batch: 800, loss is 6.0596099376678465 and perplexity is 428.20837632147794
At time: 861.9215087890625 and batch: 850, loss is 6.096348638534546 and perplexity is 444.2327507736626
At time: 863.082845211029 and batch: 900, loss is 6.077968263626099 and perplexity is 436.1421680143687
At time: 864.2446143627167 and batch: 950, loss is 6.052339401245117 and perplexity is 425.10636204335304
At time: 865.4058327674866 and batch: 1000, loss is 6.048555307388305 and perplexity is 423.5007594616201
At time: 866.5679376125336 and batch: 1050, loss is 6.045444536209106 and perplexity is 422.1853924683068
At time: 867.7296919822693 and batch: 1100, loss is 6.04007664680481 and perplexity is 419.92521957957183
At time: 868.8920331001282 and batch: 1150, loss is 6.091430540084839 and perplexity is 442.05333405668205
At time: 870.0531423091888 and batch: 1200, loss is 6.093488121032715 and perplexity is 442.96383096410085
At time: 871.2146723270416 and batch: 1250, loss is 6.072683219909668 and perplexity is 433.84321796671253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.697399724138914 and perplexity of 298.0912727896541
Finished 28 epochs...
Completing Train Step...
At time: 874.1686112880707 and batch: 50, loss is 6.052813844680786 and perplexity is 425.30809881884596
At time: 875.3294575214386 and batch: 100, loss is 6.095530738830567 and perplexity is 443.8695614847594
At time: 876.5163359642029 and batch: 150, loss is 6.0141685199737545 and perplexity is 409.18546788680806
At time: 877.6764855384827 and batch: 200, loss is 6.033598375320435 and perplexity is 417.2136227176652
At time: 878.836927652359 and batch: 250, loss is 6.067307748794556 and perplexity is 431.51736316329755
At time: 879.9969174861908 and batch: 300, loss is 6.082043304443359 and perplexity is 437.92309135431276
At time: 881.1580839157104 and batch: 350, loss is 6.094430265426635 and perplexity is 443.3813635112426
At time: 882.3186500072479 and batch: 400, loss is 6.066566972732544 and perplexity is 431.1978237984922
At time: 883.479161977768 and batch: 450, loss is 6.042878561019897 and perplexity is 421.1034639205304
At time: 884.6414222717285 and batch: 500, loss is 6.036959829330445 and perplexity is 418.61842689230974
At time: 885.8022019863129 and batch: 550, loss is 6.060825414657593 and perplexity is 428.7291701920788
At time: 886.9622275829315 and batch: 600, loss is 6.095192861557007 and perplexity is 443.71961338096355
At time: 888.1235363483429 and batch: 650, loss is 6.086231460571289 and perplexity is 439.7610277286255
At time: 889.2834634780884 and batch: 700, loss is 6.094783506393433 and perplexity is 443.5380116383883
At time: 890.4438009262085 and batch: 750, loss is 6.052212209701538 and perplexity is 425.05229554745284
At time: 891.6049129962921 and batch: 800, loss is 6.059432373046875 and perplexity is 428.13234841357286
At time: 892.764899969101 and batch: 850, loss is 6.096073570251465 and perplexity is 444.11057323797183
At time: 893.9256346225739 and batch: 900, loss is 6.077856893539429 and perplexity is 436.09359752801623
At time: 895.0869572162628 and batch: 950, loss is 6.052229652404785 and perplexity is 425.0597096731696
At time: 896.2476391792297 and batch: 1000, loss is 6.048508949279785 and perplexity is 423.48112722251494
At time: 897.4090092182159 and batch: 1050, loss is 6.045266036987305 and perplexity is 422.1100394297246
At time: 898.5701925754547 and batch: 1100, loss is 6.039995326995849 and perplexity is 419.89107272936445
At time: 899.7313506603241 and batch: 1150, loss is 6.0914229393005375 and perplexity is 442.04997411740925
At time: 900.8919157981873 and batch: 1200, loss is 6.093471431732178 and perplexity is 442.95643826928847
At time: 902.0533680915833 and batch: 1250, loss is 6.072423238754272 and perplexity is 433.7304415661496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.697449621492929 and perplexity of 298.1061471265132
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 904.9396135807037 and batch: 50, loss is 6.053585948944092 and perplexity is 425.636607820443
At time: 906.1278007030487 and batch: 100, loss is 6.099083642959595 and perplexity is 445.4493923143807
At time: 907.291375875473 and batch: 150, loss is 6.015190114974976 and perplexity is 409.6037033126093
At time: 908.4545636177063 and batch: 200, loss is 6.03646918296814 and perplexity is 418.4130836635347
At time: 909.617650270462 and batch: 250, loss is 6.06910062789917 and perplexity is 432.29171557970045
At time: 910.777939081192 and batch: 300, loss is 6.082865571975708 and perplexity is 438.2833293797181
At time: 911.9372382164001 and batch: 350, loss is 6.093554763793946 and perplexity is 442.9933522806016
At time: 913.0966897010803 and batch: 400, loss is 6.06360429763794 and perplexity is 429.922215284912
At time: 914.2559583187103 and batch: 450, loss is 6.03933427810669 and perplexity is 419.6135959251398
At time: 915.4157059192657 and batch: 500, loss is 6.032526588439941 and perplexity is 416.76669817718556
At time: 916.5748279094696 and batch: 550, loss is 6.0550867938995365 and perplexity is 426.2759019968599
At time: 917.7367260456085 and batch: 600, loss is 6.089180564880371 and perplexity is 441.05984310013076
At time: 918.8959383964539 and batch: 650, loss is 6.0807977962493895 and perplexity is 437.37799408772554
At time: 920.0562524795532 and batch: 700, loss is 6.089117679595947 and perplexity is 441.03210779853003
At time: 921.215854883194 and batch: 750, loss is 6.043327522277832 and perplexity is 421.29256550787983
At time: 922.3833858966827 and batch: 800, loss is 6.050321979522705 and perplexity is 424.24960774218204
At time: 923.5456943511963 and batch: 850, loss is 6.081775846481324 and perplexity is 437.80598099851755
At time: 924.7046058177948 and batch: 900, loss is 6.0627134418487545 and perplexity is 429.5393871381799
At time: 925.8641376495361 and batch: 950, loss is 6.037037258148193 and perplexity is 418.6508412770793
At time: 927.0228266716003 and batch: 1000, loss is 6.033607912063599 and perplexity is 417.21760159580236
At time: 928.1804294586182 and batch: 1050, loss is 6.026911611557007 and perplexity is 414.4331204249871
At time: 929.3393359184265 and batch: 1100, loss is 6.021050968170166 and perplexity is 412.01137911124164
At time: 930.4984164237976 and batch: 1150, loss is 6.0713909721374515 and perplexity is 433.282947117149
At time: 931.6573297977448 and batch: 1200, loss is 6.07991117477417 and perplexity is 436.9903772254882
At time: 932.8181040287018 and batch: 1250, loss is 6.063709259033203 and perplexity is 429.96734288876974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.692903170620438 and perplexity of 296.75389846913083
Finished 30 epochs...
Completing Train Step...
At time: 935.756115436554 and batch: 50, loss is 6.0490740871429445 and perplexity is 423.7205200804543
At time: 936.9186315536499 and batch: 100, loss is 6.092429904937744 and perplexity is 442.4953274413889
At time: 938.0809669494629 and batch: 150, loss is 6.0102734375 and perplexity is 407.59475672904756
At time: 939.2431738376617 and batch: 200, loss is 6.030400047302246 and perplexity is 415.88136832749814
At time: 940.4066133499146 and batch: 250, loss is 6.0640873336792 and perplexity is 430.1299333734557
At time: 941.5682857036591 and batch: 300, loss is 6.078028945922852 and perplexity is 436.1686349258629
At time: 942.7307062149048 and batch: 350, loss is 6.089279870986939 and perplexity is 441.10364521078515
At time: 943.8920450210571 and batch: 400, loss is 6.059878368377685 and perplexity is 428.3233360285604
At time: 945.0544240474701 and batch: 450, loss is 6.03598949432373 and perplexity is 418.21242378960807
At time: 946.2161738872528 and batch: 500, loss is 6.0303899192810055 and perplexity is 415.87715629349606
At time: 947.3781642913818 and batch: 550, loss is 6.052694320678711 and perplexity is 425.2572673306121
At time: 948.54421210289 and batch: 600, loss is 6.08696029663086 and perplexity is 440.08165825257845
At time: 949.7217457294464 and batch: 650, loss is 6.078914899826049 and perplexity is 436.55523145849423
At time: 950.8906271457672 and batch: 700, loss is 6.08748083114624 and perplexity is 440.3107955770539
At time: 952.0559437274933 and batch: 750, loss is 6.0427570629119876 and perplexity is 421.0523037544245
At time: 953.2385876178741 and batch: 800, loss is 6.050118618011474 and perplexity is 424.163340472831
At time: 954.405953168869 and batch: 850, loss is 6.082823467254639 and perplexity is 438.264875970876
At time: 955.5691010951996 and batch: 900, loss is 6.063939228057861 and perplexity is 430.0662334296934
At time: 956.7361788749695 and batch: 950, loss is 6.03854344367981 and perplexity is 419.2818822300886
At time: 957.8975639343262 and batch: 1000, loss is 6.035470333099365 and perplexity is 417.9953604659346
At time: 959.05921626091 and batch: 1050, loss is 6.0292723274230955 and perplexity is 415.41263499069396
At time: 960.2190420627594 and batch: 1100, loss is 6.023713331222535 and perplexity is 413.1097644856661
At time: 961.3869388103485 and batch: 1150, loss is 6.073962383270263 and perplexity is 434.3985294066828
At time: 962.5499248504639 and batch: 1200, loss is 6.0822509765625 and perplexity is 438.0140452146788
At time: 963.7106971740723 and batch: 1250, loss is 6.064205255508423 and perplexity is 430.1806580727186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.692914753934763 and perplexity of 296.75733588272215
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 966.5799539089203 and batch: 50, loss is 6.0483590030670165 and perplexity is 423.4176325918175
At time: 967.7738034725189 and batch: 100, loss is 6.091340599060058 and perplexity is 442.0135771147261
At time: 968.9316005706787 and batch: 150, loss is 6.0095916366577145 and perplexity is 407.3169529947698
At time: 970.0903429985046 and batch: 200, loss is 6.028879766464233 and perplexity is 415.249592212587
At time: 971.2481977939606 and batch: 250, loss is 6.063249721527099 and perplexity is 429.76980216047565
At time: 972.4069499969482 and batch: 300, loss is 6.077392797470093 and perplexity is 435.8912551603216
At time: 973.565571308136 and batch: 350, loss is 6.087087469100952 and perplexity is 440.13762808293615
At time: 974.7256321907043 and batch: 400, loss is 6.057964706420899 and perplexity is 427.5044537370454
At time: 975.8844821453094 and batch: 450, loss is 6.034225044250488 and perplexity is 417.47515947213725
At time: 977.0414574146271 and batch: 500, loss is 6.029228286743164 and perplexity is 415.39434033865433
At time: 978.1998715400696 and batch: 550, loss is 6.049905738830566 and perplexity is 424.0730545386788
At time: 979.3652691841125 and batch: 600, loss is 6.083644323348999 and perplexity is 438.62477605814206
At time: 980.522652387619 and batch: 650, loss is 6.0761119079589845 and perplexity is 435.33328404956194
At time: 981.6818597316742 and batch: 700, loss is 6.08516224861145 and perplexity is 439.2910812592704
At time: 982.8412418365479 and batch: 750, loss is 6.0406763458251955 and perplexity is 420.17712384821107
At time: 984.0008687973022 and batch: 800, loss is 6.047900438308716 and perplexity is 423.22351269914026
At time: 985.1595106124878 and batch: 850, loss is 6.078775691986084 and perplexity is 436.4944637774642
At time: 986.3174777030945 and batch: 900, loss is 6.058861780166626 and perplexity is 427.8881288253687
At time: 987.4757091999054 and batch: 950, loss is 6.034752531051636 and perplexity is 417.69543019840887
At time: 988.6330318450928 and batch: 1000, loss is 6.031300582885742 and perplexity is 416.2560529813005
At time: 989.7899444103241 and batch: 1050, loss is 6.024675149917602 and perplexity is 413.50729232447105
At time: 990.9473791122437 and batch: 1100, loss is 6.018870830535889 and perplexity is 411.11411603178533
At time: 992.1070749759674 and batch: 1150, loss is 6.069080190658569 and perplexity is 432.28288082017895
At time: 993.264139175415 and batch: 1200, loss is 6.078667306900025 and perplexity is 436.44715685117234
At time: 994.4693937301636 and batch: 1250, loss is 6.062442712783813 and perplexity is 429.42311408150266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.693828053718065 and perplexity of 297.0284880960585
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 997.3823111057281 and batch: 50, loss is 6.047770538330078 and perplexity is 423.1685395444648
At time: 998.5695040225983 and batch: 100, loss is 6.089894952774048 and perplexity is 441.3750434866937
At time: 999.7290089130402 and batch: 150, loss is 6.0087342453002925 and perplexity is 406.96787263016915
At time: 1000.889285326004 and batch: 200, loss is 6.028076858520508 and perplexity is 414.91631882819854
At time: 1002.0494885444641 and batch: 250, loss is 6.062514410018921 and perplexity is 429.4539036352235
At time: 1003.2104501724243 and batch: 300, loss is 6.076919975280762 and perplexity is 435.6852048191719
At time: 1004.3720791339874 and batch: 350, loss is 6.086424865722656 and perplexity is 439.84608800204103
At time: 1005.5345253944397 and batch: 400, loss is 6.057171449661255 and perplexity is 427.16546740871473
At time: 1006.6969141960144 and batch: 450, loss is 6.0336247825622555 and perplexity is 417.2246403241629
At time: 1007.8578839302063 and batch: 500, loss is 6.028754367828369 and perplexity is 415.1975237449059
At time: 1009.0207676887512 and batch: 550, loss is 6.048865842819214 and perplexity is 423.6322918741297
At time: 1010.1828517913818 and batch: 600, loss is 6.082833499908447 and perplexity is 438.26927295270957
At time: 1011.3439836502075 and batch: 650, loss is 6.075755767822265 and perplexity is 435.178271998907
At time: 1012.5066840648651 and batch: 700, loss is 6.084862337112427 and perplexity is 439.15935256704466
At time: 1013.6681940555573 and batch: 750, loss is 6.040442094802857 and perplexity is 420.07870845478953
At time: 1014.8305840492249 and batch: 800, loss is 6.047865619659424 and perplexity is 423.2087768846216
At time: 1015.9933972358704 and batch: 850, loss is 6.0775468635559085 and perplexity is 435.95841639334657
At time: 1017.1546576023102 and batch: 900, loss is 6.057937383651733 and perplexity is 427.4927732911106
At time: 1018.3165214061737 and batch: 950, loss is 6.03374137878418 and perplexity is 417.27328997704626
At time: 1019.4786779880524 and batch: 1000, loss is 6.029986019134522 and perplexity is 415.7092173667114
At time: 1020.6401948928833 and batch: 1050, loss is 6.023866319656372 and perplexity is 413.17297033629666
At time: 1021.8036949634552 and batch: 1100, loss is 6.01821011543274 and perplexity is 410.8425764412379
At time: 1023.0132493972778 and batch: 1150, loss is 6.068054962158203 and perplexity is 431.83991919778106
At time: 1024.1747045516968 and batch: 1200, loss is 6.077822160720825 and perplexity is 436.07845103124095
At time: 1025.3398613929749 and batch: 1250, loss is 6.062313995361328 and perplexity is 429.36784340232856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.693096077355155 and perplexity of 296.8111498166067
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 1028.2655770778656 and batch: 50, loss is 6.047920999526977 and perplexity is 423.23221477962056
At time: 1029.423478603363 and batch: 100, loss is 6.089653911590577 and perplexity is 441.2686667449597
At time: 1030.5801048278809 and batch: 150, loss is 6.008636674880981 and perplexity is 406.92816654129183
At time: 1031.7381842136383 and batch: 200, loss is 6.027991046905518 and perplexity is 414.88071571639637
At time: 1032.8964920043945 and batch: 250, loss is 6.0621774291992185 and perplexity is 429.30921028756273
At time: 1034.0533802509308 and batch: 300, loss is 6.07641414642334 and perplexity is 435.4648783982511
At time: 1035.2121212482452 and batch: 350, loss is 6.0860857582092285 and perplexity is 439.69695817579907
At time: 1036.3697700500488 and batch: 400, loss is 6.056878967285156 and perplexity is 427.04054730717417
At time: 1037.5276436805725 and batch: 450, loss is 6.033485231399536 and perplexity is 417.1664202029276
At time: 1038.6857657432556 and batch: 500, loss is 6.0287306690216065 and perplexity is 415.18768417561597
At time: 1039.8431520462036 and batch: 550, loss is 6.048733100891114 and perplexity is 423.5760618390251
At time: 1041.002398967743 and batch: 600, loss is 6.082648153305054 and perplexity is 438.1880487591429
At time: 1042.161631822586 and batch: 650, loss is 6.075714807510376 and perplexity is 435.1604473262132
At time: 1043.318877696991 and batch: 700, loss is 6.084921407699585 and perplexity is 439.1852947340586
At time: 1044.477480173111 and batch: 750, loss is 6.04042275428772 and perplexity is 420.07058399473584
At time: 1045.635380744934 and batch: 800, loss is 6.047707614898681 and perplexity is 423.1419131656178
At time: 1046.79154920578 and batch: 850, loss is 6.077279977798462 and perplexity is 435.8420808260156
At time: 1047.9492659568787 and batch: 900, loss is 6.057567853927612 and perplexity is 427.33483118848045
At time: 1049.107429265976 and batch: 950, loss is 6.03346097946167 and perplexity is 417.15630323150367
At time: 1050.2655880451202 and batch: 1000, loss is 6.029690551757812 and perplexity is 415.5864069989043
At time: 1051.4218254089355 and batch: 1050, loss is 6.023675899505616 and perplexity is 413.09430136731277
At time: 1052.6246464252472 and batch: 1100, loss is 6.018090686798096 and perplexity is 410.79351300312794
At time: 1053.7829155921936 and batch: 1150, loss is 6.067847461700439 and perplexity is 431.75032151296864
At time: 1054.9400317668915 and batch: 1200, loss is 6.077528514862061 and perplexity is 435.95041719922136
At time: 1056.0988509654999 and batch: 1250, loss is 6.062053813934326 and perplexity is 429.25614439575685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.69287109375 and perplexity of 296.74437968544487
Finished 34 epochs...
Completing Train Step...
At time: 1058.9706332683563 and batch: 50, loss is 6.047782917022705 and perplexity is 423.17377785016663
At time: 1060.1572155952454 and batch: 100, loss is 6.0895421504974365 and perplexity is 441.2193528321356
At time: 1061.316482782364 and batch: 150, loss is 6.008427839279175 and perplexity is 406.84319432566105
At time: 1062.4748635292053 and batch: 200, loss is 6.027770910263062 and perplexity is 414.78939532047025
At time: 1063.6336629390717 and batch: 250, loss is 6.062031230926514 and perplexity is 429.2464506103524
At time: 1064.795355796814 and batch: 300, loss is 6.07635615348816 and perplexity is 435.4396252440442
At time: 1065.9570636749268 and batch: 350, loss is 6.085986175537109 and perplexity is 439.65317415788223
At time: 1067.1171369552612 and batch: 400, loss is 6.0568138790130615 and perplexity is 427.012752880391
At time: 1068.275614976883 and batch: 450, loss is 6.033441772460938 and perplexity is 417.14829098702796
At time: 1069.4367580413818 and batch: 500, loss is 6.028676137924195 and perplexity is 415.16504415286425
At time: 1070.5964212417603 and batch: 550, loss is 6.048699750900268 and perplexity is 423.5619358167931
At time: 1071.7566845417023 and batch: 600, loss is 6.082625255584717 and perplexity is 438.17801536661887
At time: 1072.9178898334503 and batch: 650, loss is 6.0756831741333 and perplexity is 435.1466819494182
At time: 1074.077425956726 and batch: 700, loss is 6.084889879226685 and perplexity is 439.17144811067806
At time: 1075.2370190620422 and batch: 750, loss is 6.040391931533813 and perplexity is 420.0576364620423
At time: 1076.3982017040253 and batch: 800, loss is 6.047702283859253 and perplexity is 423.13965738540765
At time: 1077.564343214035 and batch: 850, loss is 6.077277240753173 and perplexity is 435.84088790813445
At time: 1078.724589586258 and batch: 900, loss is 6.057575225830078 and perplexity is 427.3379814707882
At time: 1079.8847703933716 and batch: 950, loss is 6.033471326828003 and perplexity is 417.1606197229237
At time: 1081.0445353984833 and batch: 1000, loss is 6.029713735580445 and perplexity is 415.5960419921401
At time: 1082.2485394477844 and batch: 1050, loss is 6.023748073577881 and perplexity is 413.12411714122203
At time: 1083.4076437950134 and batch: 1100, loss is 6.018208312988281 and perplexity is 410.84183592098003
At time: 1084.5675511360168 and batch: 1150, loss is 6.0679336643219 and perplexity is 431.78754112669054
At time: 1085.7288875579834 and batch: 1200, loss is 6.0775914478302 and perplexity is 435.9778537162592
At time: 1086.8966958522797 and batch: 1250, loss is 6.062066946029663 and perplexity is 429.2617814653824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.6927815458200275 and perplexity of 296.71780803024916
Finished 35 epochs...
Completing Train Step...
At time: 1089.801756620407 and batch: 50, loss is 6.047711992263794 and perplexity is 423.1437654163203
At time: 1090.960553407669 and batch: 100, loss is 6.089457864761353 and perplexity is 441.1821659013936
At time: 1092.1197230815887 and batch: 150, loss is 6.008338861465454 and perplexity is 406.806995918154
At time: 1093.293995141983 and batch: 200, loss is 6.027667617797851 and perplexity is 414.7465529139714
At time: 1094.4619727134705 and batch: 250, loss is 6.061932668685913 and perplexity is 429.20414520330263
At time: 1095.620666027069 and batch: 300, loss is 6.076269607543946 and perplexity is 435.40194134124727
At time: 1096.7800571918488 and batch: 350, loss is 6.085922718048096 and perplexity is 439.6252757566039
At time: 1097.939369916916 and batch: 400, loss is 6.0567646408081055 and perplexity is 426.9917280565624
At time: 1099.0982146263123 and batch: 450, loss is 6.0333960819244385 and perplexity is 417.129231693231
At time: 1100.2585577964783 and batch: 500, loss is 6.0286286926269534 and perplexity is 415.14534699121276
At time: 1101.4165208339691 and batch: 550, loss is 6.0486773586273195 and perplexity is 423.55245140850474
At time: 1102.5763382911682 and batch: 600, loss is 6.0826201820373536 and perplexity is 438.17579225534377
At time: 1103.734744310379 and batch: 650, loss is 6.075687942504882 and perplexity is 435.1487568954375
At time: 1104.8936967849731 and batch: 700, loss is 6.084902868270874 and perplexity is 439.177152565072
At time: 1106.0536878108978 and batch: 750, loss is 6.040409545898438 and perplexity is 420.0650355755795
At time: 1107.2121641635895 and batch: 800, loss is 6.047712287902832 and perplexity is 423.14389051415446
At time: 1108.3718252182007 and batch: 850, loss is 6.077277154922485 and perplexity is 435.84085049961266
At time: 1109.5310866832733 and batch: 900, loss is 6.057573833465576 and perplexity is 427.3373864609667
At time: 1110.7158932685852 and batch: 950, loss is 6.033486347198487 and perplexity is 417.16688567704136
At time: 1111.8762557506561 and batch: 1000, loss is 6.029728746414184 and perplexity is 415.60228048205164
At time: 1113.0345287322998 and batch: 1050, loss is 6.023813915252686 and perplexity is 413.15131882048905
At time: 1114.194536447525 and batch: 1100, loss is 6.018294019699097 and perplexity is 410.8770493323935
At time: 1115.3529069423676 and batch: 1150, loss is 6.067997970581055 and perplexity is 431.8153086610137
At time: 1116.51256108284 and batch: 1200, loss is 6.077644720077514 and perplexity is 436.0010798549545
At time: 1117.6723170280457 and batch: 1250, loss is 6.0620794773101805 and perplexity is 429.26716069888556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.692750805485858 and perplexity of 296.7086869658692
Finished 36 epochs...
Completing Train Step...
At time: 1120.5482094287872 and batch: 50, loss is 6.047655429840088 and perplexity is 423.1198320562429
At time: 1121.737902879715 and batch: 100, loss is 6.089383068084717 and perplexity is 441.1491681756689
At time: 1122.8978717327118 and batch: 150, loss is 6.008269033432007 and perplexity is 406.77859037739955
At time: 1124.0586986541748 and batch: 200, loss is 6.027583780288697 and perplexity is 414.7117830535743
At time: 1125.2191278934479 and batch: 250, loss is 6.061847248077393 and perplexity is 429.1674838898778
At time: 1126.3789656162262 and batch: 300, loss is 6.076189079284668 and perplexity is 435.3668805925344
At time: 1127.5386295318604 and batch: 350, loss is 6.085862474441528 and perplexity is 439.5987919422024
At time: 1128.6989045143127 and batch: 400, loss is 6.056718730926514 and perplexity is 426.9721253668688
At time: 1129.862133026123 and batch: 450, loss is 6.033351173400879 and perplexity is 417.1104994559239
At time: 1131.0244715213776 and batch: 500, loss is 6.028586797714233 and perplexity is 415.1279548774574
At time: 1132.1836042404175 and batch: 550, loss is 6.048658695220947 and perplexity is 423.5445465507501
At time: 1133.3426430225372 and batch: 600, loss is 6.082615976333618 and perplexity is 438.1739494216527
At time: 1134.5026333332062 and batch: 650, loss is 6.0756954574584965 and perplexity is 435.1520270304482
At time: 1135.6617681980133 and batch: 700, loss is 6.084919242858887 and perplexity is 439.1843439688878
At time: 1136.8248777389526 and batch: 750, loss is 6.040431032180786 and perplexity is 420.07406130850273
At time: 1137.985539674759 and batch: 800, loss is 6.0477252769470216 and perplexity is 423.14938678454246
At time: 1139.144788980484 and batch: 850, loss is 6.077280893325805 and perplexity is 435.8424798515408
At time: 1140.3497636318207 and batch: 900, loss is 6.057570505142212 and perplexity is 427.3359641463258
At time: 1141.5110478401184 and batch: 950, loss is 6.033499965667724 and perplexity is 417.17256689012555
At time: 1142.6702635288239 and batch: 1000, loss is 6.029742126464844 and perplexity is 415.6078412988206
At time: 1143.8289830684662 and batch: 1050, loss is 6.023871231079101 and perplexity is 413.17499960839746
At time: 1144.990463256836 and batch: 1100, loss is 6.018368406295776 and perplexity is 410.9076142145418
At time: 1146.1495325565338 and batch: 1150, loss is 6.0680567646026615 and perplexity is 431.84069756595187
At time: 1147.309318304062 and batch: 1200, loss is 6.077695322036743 and perplexity is 436.0231429220337
At time: 1148.471343278885 and batch: 1250, loss is 6.062092199325561 and perplexity is 429.2726218770449
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.692736994611086 and perplexity of 296.70458918764683
Finished 37 epochs...
Completing Train Step...
At time: 1151.3509738445282 and batch: 50, loss is 6.047605819702149 and perplexity is 423.09884154368524
At time: 1152.5398790836334 and batch: 100, loss is 6.089313545227051 and perplexity is 441.1184992909468
At time: 1153.7089326381683 and batch: 150, loss is 6.008207197189331 and perplexity is 406.7534374954576
At time: 1154.8718483448029 and batch: 200, loss is 6.0275070762634275 and perplexity is 414.6799742104363
At time: 1156.0347142219543 and batch: 250, loss is 6.061768569946289 and perplexity is 429.1337191226068
At time: 1157.1970164775848 and batch: 300, loss is 6.076114444732666 and perplexity is 435.33438839298043
At time: 1158.3598282337189 and batch: 350, loss is 6.085804386138916 and perplexity is 439.57325713619196
At time: 1159.5215528011322 and batch: 400, loss is 6.056674737930297 and perplexity is 426.95334199694435
At time: 1160.6837511062622 and batch: 450, loss is 6.033308153152466 and perplexity is 417.09255564459784
At time: 1161.8468861579895 and batch: 500, loss is 6.028548231124878 and perplexity is 415.1119451168148
At time: 1163.0082058906555 and batch: 550, loss is 6.048642253875732 and perplexity is 423.5375829658918
At time: 1164.1713621616364 and batch: 600, loss is 6.082612190246582 and perplexity is 438.1722904600838
At time: 1165.3327195644379 and batch: 650, loss is 6.075703659057617 and perplexity is 435.1555959875661
At time: 1166.4929435253143 and batch: 700, loss is 6.0849362564086915 and perplexity is 439.191816117161
At time: 1167.6555993556976 and batch: 750, loss is 6.0404527759552 and perplexity is 420.0831954034335
At time: 1168.8172414302826 and batch: 800, loss is 6.047738447189331 and perplexity is 423.1549598011983
At time: 1170.0261392593384 and batch: 850, loss is 6.07728687286377 and perplexity is 435.84508599598735
At time: 1171.1887702941895 and batch: 900, loss is 6.057566823959351 and perplexity is 427.3343910473942
At time: 1172.3503720760345 and batch: 950, loss is 6.033511943817139 and perplexity is 417.17756387539066
At time: 1173.513489484787 and batch: 1000, loss is 6.029754495620727 and perplexity is 415.6129820487893
At time: 1174.675043106079 and batch: 1050, loss is 6.023923435211182 and perplexity is 413.1965696136661
At time: 1175.8381674289703 and batch: 1100, loss is 6.018437023162842 and perplexity is 410.93581037503765
At time: 1177.0003826618195 and batch: 1150, loss is 6.068112115859986 and perplexity is 431.86460115306664
At time: 1178.161626815796 and batch: 1200, loss is 6.077744340896606 and perplexity is 436.04451680323086
At time: 1179.3259253501892 and batch: 1250, loss is 6.062104225158691 and perplexity is 429.277784269004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.692729866417655 and perplexity of 296.70247422748116
Finished 38 epochs...
Completing Train Step...
At time: 1182.3364503383636 and batch: 50, loss is 6.0475601482391355 and perplexity is 423.0795184418532
At time: 1183.4992308616638 and batch: 100, loss is 6.089247512817383 and perplexity is 441.0893721351681
At time: 1184.6650738716125 and batch: 150, loss is 6.008150072097778 and perplexity is 406.7302023317629
At time: 1185.824594259262 and batch: 200, loss is 6.02743390083313 and perplexity is 414.6496309350924
At time: 1186.9833016395569 and batch: 250, loss is 6.061693935394287 and perplexity is 429.10169211490677
At time: 1188.1425151824951 and batch: 300, loss is 6.076044387817383 and perplexity is 435.3038912768924
At time: 1189.301141500473 and batch: 350, loss is 6.085748147964478 and perplexity is 439.548537033792
At time: 1190.4606437683105 and batch: 400, loss is 6.056632204055786 and perplexity is 426.9351824032753
At time: 1191.6207637786865 and batch: 450, loss is 6.033267040252685 and perplexity is 417.0754081126532
At time: 1192.7806887626648 and batch: 500, loss is 6.028512134552002 and perplexity is 415.0969612686704
At time: 1193.9396557807922 and batch: 550, loss is 6.0486267566680905 and perplexity is 423.5310193668834
At time: 1195.0986287593842 and batch: 600, loss is 6.082608766555786 and perplexity is 438.170790296214
At time: 1196.2577397823334 and batch: 650, loss is 6.075711784362793 and perplexity is 435.15913177394685
At time: 1197.416377544403 and batch: 700, loss is 6.084953708648682 and perplexity is 439.1994810650226
At time: 1198.5746974945068 and batch: 750, loss is 6.040473966598511 and perplexity is 420.0920973309067
At time: 1199.7599742412567 and batch: 800, loss is 6.047751750946045 and perplexity is 423.1605893892831
At time: 1200.9187524318695 and batch: 850, loss is 6.077294025421143 and perplexity is 435.84820341411944
At time: 1202.0773491859436 and batch: 900, loss is 6.057563028335571 and perplexity is 427.33276904989594
At time: 1203.2353472709656 and batch: 950, loss is 6.033522777557373 and perplexity is 417.1820834932312
At time: 1204.394639492035 and batch: 1000, loss is 6.029767227172852 and perplexity is 415.6182734808179
At time: 1205.553146123886 and batch: 1050, loss is 6.023972492218018 and perplexity is 413.21684029781187
At time: 1206.7124066352844 and batch: 1100, loss is 6.018502216339112 and perplexity is 410.96260145904745
At time: 1207.8811767101288 and batch: 1150, loss is 6.068165721893311 and perplexity is 431.88775232178364
At time: 1209.0393064022064 and batch: 1200, loss is 6.0777913856506345 and perplexity is 436.0650308928056
At time: 1210.1997785568237 and batch: 1250, loss is 6.0621153926849365 and perplexity is 429.2825782666947
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.692724074760493 and perplexity of 296.70075583344754
Finished 39 epochs...
Completing Train Step...
At time: 1213.0723731517792 and batch: 50, loss is 6.047516965866089 and perplexity is 423.06124925871586
At time: 1214.2608444690704 and batch: 100, loss is 6.089183788299561 and perplexity is 441.06126482318444
At time: 1215.4214000701904 and batch: 150, loss is 6.0080960083007815 and perplexity is 406.7082135470756
At time: 1216.5837762355804 and batch: 200, loss is 6.027363567352295 and perplexity is 414.62046820879186
At time: 1217.7457649707794 and batch: 250, loss is 6.061622438430786 and perplexity is 429.0710137436054
At time: 1218.906678199768 and batch: 300, loss is 6.075977602005005 and perplexity is 435.2748201236632
At time: 1220.0672023296356 and batch: 350, loss is 6.0856935501098635 and perplexity is 439.52453928178994
At time: 1221.2353348731995 and batch: 400, loss is 6.0565907001495365 and perplexity is 426.9174632931989
At time: 1222.4022147655487 and batch: 450, loss is 6.033227853775024 and perplexity is 417.0590647167124
At time: 1223.56693816185 and batch: 500, loss is 6.028477725982666 and perplexity is 415.0826786218216
At time: 1224.7298753261566 and batch: 550, loss is 6.0486119365692135 and perplexity is 423.5247426418099
At time: 1225.891319513321 and batch: 600, loss is 6.082606029510498 and perplexity is 438.1695910045583
At time: 1227.0528848171234 and batch: 650, loss is 6.07571964263916 and perplexity is 435.16255138810425
At time: 1228.262448310852 and batch: 700, loss is 6.084971237182617 and perplexity is 439.2071796555032
At time: 1229.4261877536774 and batch: 750, loss is 6.040495147705078 and perplexity is 420.10099544062393
At time: 1230.5876693725586 and batch: 800, loss is 6.047765083312989 and perplexity is 423.1662311591459
At time: 1231.7506403923035 and batch: 850, loss is 6.077302055358887 and perplexity is 435.85170326211045
At time: 1232.911209821701 and batch: 900, loss is 6.057560091018677 and perplexity is 427.3315138399773
At time: 1234.0720648765564 and batch: 950, loss is 6.033533010482788 and perplexity is 417.1863525082185
At time: 1235.233894586563 and batch: 1000, loss is 6.029779872894287 and perplexity is 415.62352930695954
At time: 1236.3944737911224 and batch: 1050, loss is 6.024019546508789 and perplexity is 413.236284380627
At time: 1237.5560438632965 and batch: 1100, loss is 6.0185647964477536 and perplexity is 410.98832034803155
At time: 1238.7173006534576 and batch: 1150, loss is 6.068217601776123 and perplexity is 431.9101591889898
At time: 1239.8819406032562 and batch: 1200, loss is 6.077837114334106 and perplexity is 436.08497202851396
At time: 1241.0444538593292 and batch: 1250, loss is 6.062126407623291 and perplexity is 429.28730681387316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.692722292712135 and perplexity of 296.700227098824
Finished 40 epochs...
Completing Train Step...
At time: 1243.9429731369019 and batch: 50, loss is 6.047475566864014 and perplexity is 423.0437353077123
At time: 1245.1025822162628 and batch: 100, loss is 6.0891219425201415 and perplexity is 441.03398788898005
At time: 1246.2631690502167 and batch: 150, loss is 6.008044328689575 and perplexity is 406.6871955678302
At time: 1247.4247715473175 and batch: 200, loss is 6.027295141220093 and perplexity is 414.592098304453
At time: 1248.5858421325684 and batch: 250, loss is 6.061553039550781 and perplexity is 429.0412377290322
At time: 1249.7445154190063 and batch: 300, loss is 6.075913639068603 and perplexity is 435.246979558418
At time: 1250.9037218093872 and batch: 350, loss is 6.085640087127685 and perplexity is 439.50104161731264
At time: 1252.0623197555542 and batch: 400, loss is 6.056550340652466 and perplexity is 426.9002334667856
At time: 1253.2219898700714 and batch: 450, loss is 6.033190202713013 and perplexity is 417.04336229561267
At time: 1254.3803346157074 and batch: 500, loss is 6.028444862365722 and perplexity is 415.0690377278173
At time: 1255.5383260250092 and batch: 550, loss is 6.048597288131714 and perplexity is 423.5185387115269
At time: 1256.6977188587189 and batch: 600, loss is 6.082603397369385 and perplexity is 438.1684376818812
At time: 1257.885812997818 and batch: 650, loss is 6.075727424621582 and perplexity is 435.1659378286065
At time: 1259.04487657547 and batch: 700, loss is 6.084989099502564 and perplexity is 439.2150249847368
At time: 1260.2049882411957 and batch: 750, loss is 6.040516347885132 and perplexity is 420.1099017517752
At time: 1261.3636946678162 and batch: 800, loss is 6.047778339385986 and perplexity is 423.17184071877654
At time: 1262.5221309661865 and batch: 850, loss is 6.077310180664062 and perplexity is 435.8552447045983
At time: 1263.681487083435 and batch: 900, loss is 6.057557315826416 and perplexity is 427.3303279145128
At time: 1264.840006351471 and batch: 950, loss is 6.033542413711547 and perplexity is 417.1902754253703
At time: 1265.9988160133362 and batch: 1000, loss is 6.029792747497559 and perplexity is 415.62888032945574
At time: 1267.1589169502258 and batch: 1050, loss is 6.024064855575562 and perplexity is 413.25500815520394
At time: 1268.324119567871 and batch: 1100, loss is 6.018625221252441 and perplexity is 411.01315498732407
At time: 1269.4832968711853 and batch: 1150, loss is 6.0682680034637455 and perplexity is 431.93192873852064
At time: 1270.6418080329895 and batch: 1200, loss is 6.077881450653076 and perplexity is 436.1043068595463
At time: 1271.8002631664276 and batch: 1250, loss is 6.062136583328247 and perplexity is 429.29167513707404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.692720065151688 and perplexity of 296.6995661818695
Finished 41 epochs...
Completing Train Step...
At time: 1274.6922035217285 and batch: 50, loss is 6.047436027526856 and perplexity is 423.0270087695097
At time: 1275.8797216415405 and batch: 100, loss is 6.089061689376831 and perplexity is 441.007415005461
At time: 1277.0397508144379 and batch: 150, loss is 6.007994318008423 and perplexity is 406.66685737273184
At time: 1278.2001132965088 and batch: 200, loss is 6.02722861289978 and perplexity is 414.5645171060132
At time: 1279.36101937294 and batch: 250, loss is 6.061485986709595 and perplexity is 429.01247025953757
At time: 1280.5222973823547 and batch: 300, loss is 6.075851917266846 and perplexity is 435.2201161596673
At time: 1281.683179616928 and batch: 350, loss is 6.085587730407715 and perplexity is 439.47803138672543
At time: 1282.8451330661774 and batch: 400, loss is 6.05651065826416 and perplexity is 426.883293382067
At time: 1284.0049242973328 and batch: 450, loss is 6.033153963088989 and perplexity is 417.028249074812
At time: 1285.164967060089 and batch: 500, loss is 6.028413219451904 and perplexity is 415.0559039418246
At time: 1286.3257641792297 and batch: 550, loss is 6.048582706451416 and perplexity is 423.51236314462034
At time: 1287.5122244358063 and batch: 600, loss is 6.082601318359375 and perplexity is 438.16752672625995
At time: 1288.6739342212677 and batch: 650, loss is 6.075735626220703 and perplexity is 435.1695068998156
At time: 1289.8341703414917 and batch: 700, loss is 6.085006628036499 and perplexity is 439.22272384768183
At time: 1290.9954392910004 and batch: 750, loss is 6.0405372047424315 and perplexity is 420.11866401542267
At time: 1292.158369064331 and batch: 800, loss is 6.047791213989258 and perplexity is 423.17728892341313
At time: 1293.3186211585999 and batch: 850, loss is 6.07731858253479 and perplexity is 435.8589067194043
At time: 1294.4793956279755 and batch: 900, loss is 6.0575549507141115 and perplexity is 427.3293172314915
At time: 1295.641627073288 and batch: 950, loss is 6.033551759719849 and perplexity is 417.19417450736813
At time: 1296.8027913570404 and batch: 1000, loss is 6.029805498123169 and perplexity is 415.63417989148803
At time: 1297.962639093399 and batch: 1050, loss is 6.024108953475952 and perplexity is 413.27323223520847
At time: 1299.1240282058716 and batch: 1100, loss is 6.018684005737304 and perplexity is 411.0373168940776
At time: 1300.2833909988403 and batch: 1150, loss is 6.068316955566406 and perplexity is 431.95307323216804
At time: 1301.443398475647 and batch: 1200, loss is 6.077924642562866 and perplexity is 436.12314344421833
At time: 1302.6060230731964 and batch: 1250, loss is 6.06214656829834 and perplexity is 429.2959616230118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.692719174127509 and perplexity of 296.69930181549995
Finished 42 epochs...
Completing Train Step...
At time: 1305.4786098003387 and batch: 50, loss is 6.047397403717041 and perplexity is 423.01067017030806
At time: 1306.6633920669556 and batch: 100, loss is 6.089002676010132 and perplexity is 440.9813904410684
At time: 1307.8227488994598 and batch: 150, loss is 6.007945775985718 and perplexity is 406.6471174200202
At time: 1308.9827136993408 and batch: 200, loss is 6.027163887023926 and perplexity is 414.5376849229233
At time: 1310.1421904563904 and batch: 250, loss is 6.061420621871949 and perplexity is 428.9844288455419
At time: 1311.3013603687286 and batch: 300, loss is 6.075792369842529 and perplexity is 435.1942006943464
At time: 1312.4609060287476 and batch: 350, loss is 6.085537004470825 and perplexity is 439.45573901724623
At time: 1313.6194891929626 and batch: 400, loss is 6.05647159576416 and perplexity is 426.86661857910116
At time: 1314.7793500423431 and batch: 450, loss is 6.033118696212768 and perplexity is 417.0135420505081
At time: 1315.9383056163788 and batch: 500, loss is 6.02838285446167 and perplexity is 415.0433009647004
At time: 1317.1233615875244 and batch: 550, loss is 6.048568744659423 and perplexity is 423.5064501943775
At time: 1318.2829041481018 and batch: 600, loss is 6.0825992202758785 and perplexity is 438.1666074151679
At time: 1319.441891670227 and batch: 650, loss is 6.075743646621704 and perplexity is 435.17299714776095
At time: 1320.6016657352448 and batch: 700, loss is 6.085023899078369 and perplexity is 439.2303097472435
At time: 1321.7606313228607 and batch: 750, loss is 6.0405583095550535 and perplexity is 420.12753063466954
At time: 1322.9195175170898 and batch: 800, loss is 6.0478041458129885 and perplexity is 423.182761412905
At time: 1324.0792713165283 and batch: 850, loss is 6.077327136993408 and perplexity is 435.8626352723329
At time: 1325.2383968830109 and batch: 900, loss is 6.057553052902222 and perplexity is 427.3285062416019
At time: 1326.3980910778046 and batch: 950, loss is 6.033560781478882 and perplexity is 417.19793834965884
At time: 1327.55859041214 and batch: 1000, loss is 6.02981858253479 and perplexity is 415.63961825576024
At time: 1328.7178552150726 and batch: 1050, loss is 6.024152135848999 and perplexity is 413.2910787394173
At time: 1329.8779368400574 and batch: 1100, loss is 6.018741416931152 and perplexity is 411.06091571456824
At time: 1331.0364127159119 and batch: 1150, loss is 6.068364791870117 and perplexity is 431.97373676479765
At time: 1332.1950109004974 and batch: 1200, loss is 6.077967147827149 and perplexity is 436.141681367667
At time: 1333.354747056961 and batch: 1250, loss is 6.062155866622925 and perplexity is 429.29995337476424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.69271828310333 and perplexity of 296.69903744936596
Finished 43 epochs...
Completing Train Step...
At time: 1336.2653567790985 and batch: 50, loss is 6.0473600769042966 and perplexity is 422.9948808249184
At time: 1337.4239852428436 and batch: 100, loss is 6.088944940567017 and perplexity is 440.9559309200511
At time: 1338.5855197906494 and batch: 150, loss is 6.007898817062378 and perplexity is 406.6280221575568
At time: 1339.744121313095 and batch: 200, loss is 6.027100400924683 and perplexity is 414.5113683776944
At time: 1340.9046454429626 and batch: 250, loss is 6.061356458663941 and perplexity is 428.9569047114295
At time: 1342.0641293525696 and batch: 300, loss is 6.075734052658081 and perplexity is 435.16882213388425
At time: 1343.2231976985931 and batch: 350, loss is 6.08548674583435 and perplexity is 439.4336531260202
At time: 1344.3822717666626 and batch: 400, loss is 6.05643325805664 and perplexity is 426.8502538052241
At time: 1345.5694653987885 and batch: 450, loss is 6.03308445930481 and perplexity is 416.9992650406532
At time: 1346.7292206287384 and batch: 500, loss is 6.028353672027588 and perplexity is 415.03118916765567
At time: 1347.8887362480164 and batch: 550, loss is 6.048554754257202 and perplexity is 423.5005252102424
At time: 1349.0483918190002 and batch: 600, loss is 6.082597417831421 and perplexity is 438.1658176449066
At time: 1350.2080166339874 and batch: 650, loss is 6.07575177192688 and perplexity is 435.17653307553246
At time: 1351.3674097061157 and batch: 700, loss is 6.085041418075561 and perplexity is 439.2380046892104
At time: 1352.5283033847809 and batch: 750, loss is 6.040579538345337 and perplexity is 420.13644952857817
At time: 1353.687803030014 and batch: 800, loss is 6.047816896438599 and perplexity is 423.18815729226094
At time: 1354.847689151764 and batch: 850, loss is 6.077335567474365 and perplexity is 435.8663098194685
At time: 1356.0066940784454 and batch: 900, loss is 6.057551670074463 and perplexity is 427.32791532028983
At time: 1357.1651678085327 and batch: 950, loss is 6.033570079803467 and perplexity is 417.201817609541
At time: 1358.3259582519531 and batch: 1000, loss is 6.029831972122192 and perplexity is 415.6451835360151
At time: 1359.484708070755 and batch: 1050, loss is 6.024194583892823 and perplexity is 413.30862250958637
At time: 1360.6435587406158 and batch: 1100, loss is 6.01879768371582 and perplexity is 411.08404544130974
At time: 1361.8037512302399 and batch: 1150, loss is 6.068411417007447 and perplexity is 431.99387806913904
At time: 1362.9621393680573 and batch: 1200, loss is 6.078008527755737 and perplexity is 436.159729252704
At time: 1364.1225814819336 and batch: 1250, loss is 6.062165222167969 and perplexity is 429.3039697286028
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.69271872861542 and perplexity of 296.69916963240354
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 1366.9894094467163 and batch: 50, loss is 6.047348585128784 and perplexity is 422.9900198906355
At time: 1368.1792538166046 and batch: 100, loss is 6.088917284011841 and perplexity is 440.9437357666563
At time: 1369.3412580490112 and batch: 150, loss is 6.007879409790039 and perplexity is 406.6201306933665
At time: 1370.5027060508728 and batch: 200, loss is 6.027082777023315 and perplexity is 414.504063134596
At time: 1371.665641784668 and batch: 250, loss is 6.061303653717041 and perplexity is 428.9342542628872
At time: 1372.8278412818909 and batch: 300, loss is 6.075677547454834 and perplexity is 435.14423352584146
At time: 1373.988830089569 and batch: 350, loss is 6.085431680679322 and perplexity is 439.4094563099929
At time: 1375.197785615921 and batch: 400, loss is 6.056382055282593 and perplexity is 426.8283984476587
At time: 1376.359245300293 and batch: 450, loss is 6.03308521270752 and perplexity is 416.9995792091479
At time: 1377.5215315818787 and batch: 500, loss is 6.028388919830323 and perplexity is 415.04581836296205
At time: 1378.6826438903809 and batch: 550, loss is 6.048527631759644 and perplexity is 423.48903897405
At time: 1379.8474793434143 and batch: 600, loss is 6.082534732818604 and perplexity is 438.1383520758604
At time: 1381.0117263793945 and batch: 650, loss is 6.07571964263916 and perplexity is 435.16255138810425
At time: 1382.1749153137207 and batch: 700, loss is 6.085041017532348 and perplexity is 439.23782875544407
At time: 1383.3378493785858 and batch: 750, loss is 6.0405599308013915 and perplexity is 420.12821176544224
At time: 1384.50061917305 and batch: 800, loss is 6.047742700576782 and perplexity is 423.1567596470219
At time: 1385.6637425422668 and batch: 850, loss is 6.077279596328736 and perplexity is 435.8419145654882
At time: 1386.8276236057281 and batch: 900, loss is 6.057413244247437 and perplexity is 427.2687661941782
At time: 1387.9928274154663 and batch: 950, loss is 6.0334752082824705 and perplexity is 417.16223891601703
At time: 1389.153992652893 and batch: 1000, loss is 6.029739770889282 and perplexity is 415.60686230429957
At time: 1390.317181110382 and batch: 1050, loss is 6.024163236618042 and perplexity is 413.29566661369455
At time: 1391.4792058467865 and batch: 1100, loss is 6.018705158233643 and perplexity is 411.04601145137366
At time: 1392.642457485199 and batch: 1150, loss is 6.068306732177734 and perplexity is 431.94865723058575
At time: 1393.803964138031 and batch: 1200, loss is 6.077920999526977 and perplexity is 436.1215546348488
At time: 1394.9652655124664 and batch: 1250, loss is 6.06211142539978 and perplexity is 429.2808751836724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.692706699789006 and perplexity of 296.69560071105985
Finished 45 epochs...
Completing Train Step...
At time: 1397.865415096283 and batch: 50, loss is 6.047337265014648 and perplexity is 422.9852316224339
At time: 1399.024061203003 and batch: 100, loss is 6.088901844024658 and perplexity is 440.93692765358634
At time: 1400.1849591732025 and batch: 150, loss is 6.007853784561157 and perplexity is 406.60971109295235
At time: 1401.3432521820068 and batch: 200, loss is 6.027054309844971 and perplexity is 414.4922635414576
At time: 1402.502385377884 and batch: 250, loss is 6.061286859512329 and perplexity is 428.9270507137021
At time: 1403.6626989841461 and batch: 300, loss is 6.075670948028565 and perplexity is 435.1413618330315
At time: 1404.8485207557678 and batch: 350, loss is 6.085414190292358 and perplexity is 439.4017709357768
At time: 1406.0078535079956 and batch: 400, loss is 6.056374807357788 and perplexity is 426.8253048387332
At time: 1407.167554616928 and batch: 450, loss is 6.033075914382935 and perplexity is 416.9957018297352
At time: 1408.3283507823944 and batch: 500, loss is 6.028380012512207 and perplexity is 415.04212143429004
At time: 1409.487637758255 and batch: 550, loss is 6.048522405624389 and perplexity is 423.4868257688368
At time: 1410.64741897583 and batch: 600, loss is 6.082531251907349 and perplexity is 438.1368269577937
At time: 1411.8072361946106 and batch: 650, loss is 6.0757139205932615 and perplexity is 435.1600613751359
At time: 1412.9675087928772 and batch: 700, loss is 6.085030565261841 and perplexity is 439.23323774683394
At time: 1414.1275146007538 and batch: 750, loss is 6.040551347732544 and perplexity is 420.124605791551
At time: 1415.2862620353699 and batch: 800, loss is 6.0477457618713375 and perplexity is 423.1580550564892
At time: 1416.446427822113 and batch: 850, loss is 6.0772816181182865 and perplexity is 435.84279574700764
At time: 1417.6055274009705 and batch: 900, loss is 6.057419967651367 and perplexity is 427.2716389043374
At time: 1418.7671520709991 and batch: 950, loss is 6.033477439880371 and perplexity is 417.1631698554322
At time: 1419.9265894889832 and batch: 1000, loss is 6.029745626449585 and perplexity is 415.60929592246896
At time: 1421.0858266353607 and batch: 1050, loss is 6.0241733741760255 and perplexity is 413.29985644371635
At time: 1422.2447979450226 and batch: 1100, loss is 6.018732099533081 and perplexity is 411.057085714228
At time: 1423.40305685997 and batch: 1150, loss is 6.068327684402465 and perplexity is 431.95770761073686
At time: 1424.564570426941 and batch: 1200, loss is 6.077933940887451 and perplexity is 436.1271986776185
At time: 1425.7252609729767 and batch: 1250, loss is 6.062113819122314 and perplexity is 429.28190276420656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.692695116474681 and perplexity of 296.6921640125623
Finished 46 epochs...
Completing Train Step...
At time: 1428.6159327030182 and batch: 50, loss is 6.047325248718262 and perplexity is 422.98014893706124
At time: 1429.8053529262543 and batch: 100, loss is 6.088886766433716 and perplexity is 440.93027943707966
At time: 1430.9679999351501 and batch: 150, loss is 6.0078339004516605 and perplexity is 406.60162610131624
At time: 1432.1312518119812 and batch: 200, loss is 6.02703164100647 and perplexity is 414.4828675897737
At time: 1433.2932295799255 and batch: 250, loss is 6.061270837783813 and perplexity is 428.9201786159942
At time: 1434.480908870697 and batch: 300, loss is 6.075661096572876 and perplexity is 435.1370750783026
At time: 1435.6438376903534 and batch: 350, loss is 6.085400190353393 and perplexity is 439.3956193808636
At time: 1436.8068170547485 and batch: 400, loss is 6.056367378234864 and perplexity is 426.82213391285507
At time: 1437.9694514274597 and batch: 450, loss is 6.033067722320556 and perplexity is 416.9922857889264
At time: 1439.131433725357 and batch: 500, loss is 6.028372440338135 and perplexity is 415.0389786749982
At time: 1440.2924704551697 and batch: 550, loss is 6.0485177421569825 and perplexity is 423.48485085643256
At time: 1441.4559514522552 and batch: 600, loss is 6.082529745101929 and perplexity is 438.13616677134553
At time: 1442.6185739040375 and batch: 650, loss is 6.075712480545044 and perplexity is 435.1594347241164
At time: 1443.7816939353943 and batch: 700, loss is 6.085027475357055 and perplexity is 439.23188056004756
At time: 1444.9444541931152 and batch: 750, loss is 6.040549659729004 and perplexity is 420.12389662032786
At time: 1446.1080543994904 and batch: 800, loss is 6.047748632431031 and perplexity is 423.1592697586893
At time: 1447.2695384025574 and batch: 850, loss is 6.077283029556274 and perplexity is 435.8434109125204
At time: 1448.4427556991577 and batch: 900, loss is 6.057424898147583 and perplexity is 427.27374557072955
At time: 1449.6063029766083 and batch: 950, loss is 6.033480405807495 and perplexity is 417.16440713282776
At time: 1450.7674584388733 and batch: 1000, loss is 6.029750652313233 and perplexity is 415.6113847233701
At time: 1451.9304640293121 and batch: 1050, loss is 6.024185676574707 and perplexity is 413.30494105460156
At time: 1453.0937552452087 and batch: 1100, loss is 6.018754444122314 and perplexity is 411.06627071857685
At time: 1454.2558512687683 and batch: 1150, loss is 6.068344163894653 and perplexity is 431.96482611305936
At time: 1455.4207065105438 and batch: 1200, loss is 6.077945404052734 and perplexity is 436.1321981044358
At time: 1456.6021237373352 and batch: 1250, loss is 6.062114925384521 and perplexity is 429.2823776628145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.692683533160356 and perplexity of 296.6887273538726
Finished 47 epochs...
Completing Train Step...
At time: 1459.496604681015 and batch: 50, loss is 6.0473134803771975 and perplexity is 422.9751711916951
At time: 1460.6823778152466 and batch: 100, loss is 6.088871240615845 and perplexity is 440.92343368701023
At time: 1461.8411271572113 and batch: 150, loss is 6.007817306518555 and perplexity is 406.5948790371123
At time: 1463.0276181697845 and batch: 200, loss is 6.027011461257935 and perplexity is 414.4745035141263
At time: 1464.1862919330597 and batch: 250, loss is 6.0612546253204345 and perplexity is 428.9132248196752
At time: 1465.3460040092468 and batch: 300, loss is 6.075649404525757 and perplexity is 435.13198746485995
At time: 1466.5056366920471 and batch: 350, loss is 6.085387706756592 and perplexity is 439.3901341773524
At time: 1467.6643307209015 and batch: 400, loss is 6.056359558105469 and perplexity is 426.81879612159037
At time: 1468.8251876831055 and batch: 450, loss is 6.0330603981018065 and perplexity is 416.9892316573929
At time: 1469.9839627742767 and batch: 500, loss is 6.028365573883057 and perplexity is 415.0361288382795
At time: 1471.1432576179504 and batch: 550, loss is 6.04851357460022 and perplexity is 423.4830859629561
At time: 1472.3028028011322 and batch: 600, loss is 6.082528791427612 and perplexity is 438.1357489323354
At time: 1473.4631607532501 and batch: 650, loss is 6.075712871551514 and perplexity is 435.1596048743041
At time: 1474.6214363574982 and batch: 700, loss is 6.085027952194213 and perplexity is 439.2320900021792
At time: 1475.7800660133362 and batch: 750, loss is 6.040551404953003 and perplexity is 420.12462983127466
At time: 1476.9407029151917 and batch: 800, loss is 6.047751569747925 and perplexity is 423.16051271338694
At time: 1478.0994441509247 and batch: 850, loss is 6.077284202575684 and perplexity is 435.8439221656008
At time: 1479.2586393356323 and batch: 900, loss is 6.057428579330445 and perplexity is 427.275318446414
At time: 1480.4180552959442 and batch: 950, loss is 6.033483715057373 and perplexity is 417.1657876363753
At time: 1481.5769102573395 and batch: 1000, loss is 6.029755353927612 and perplexity is 415.61333877242646
At time: 1482.7365090847015 and batch: 1050, loss is 6.024198665618896 and perplexity is 413.31030952561025
At time: 1483.8959245681763 and batch: 1100, loss is 6.01877384185791 and perplexity is 411.0742445507454
At time: 1485.055335521698 and batch: 1150, loss is 6.068358335494995 and perplexity is 431.97094778931375
At time: 1486.2132811546326 and batch: 1200, loss is 6.077955904006958 and perplexity is 436.1367774965933
At time: 1487.3732516765594 and batch: 1250, loss is 6.062115678787231 and perplexity is 429.282701085443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.692674622918568 and perplexity of 296.6860837973535
Finished 48 epochs...
Completing Train Step...
At time: 1490.3007199764252 and batch: 50, loss is 6.04730185508728 and perplexity is 422.970254011284
At time: 1491.4608054161072 and batch: 100, loss is 6.088856153488159 and perplexity is 440.9167814690481
At time: 1492.647207736969 and batch: 150, loss is 6.0078022670745845 and perplexity is 406.588764122193
At time: 1493.808342218399 and batch: 200, loss is 6.026993398666382 and perplexity is 414.4670170980724
At time: 1494.9687685966492 and batch: 250, loss is 6.061238746643067 and perplexity is 428.9064142990307
At time: 1496.1288418769836 and batch: 300, loss is 6.075636987686157 and perplexity is 435.12658453431044
At time: 1497.2905251979828 and batch: 350, loss is 6.085375957489013 and perplexity is 439.38497169542245
At time: 1498.4511332511902 and batch: 400, loss is 6.056351642608643 and perplexity is 426.8154176521355
At time: 1499.6123988628387 and batch: 450, loss is 6.033053207397461 and perplexity is 416.98623322189314
At time: 1500.7737264633179 and batch: 500, loss is 6.0283589553833 and perplexity is 415.0333819308522
At time: 1501.9341881275177 and batch: 550, loss is 6.048509464263916 and perplexity is 423.4813453086314
At time: 1503.0940449237823 and batch: 600, loss is 6.082528409957885 and perplexity is 438.13558179684276
At time: 1504.2535886764526 and batch: 650, loss is 6.075714139938355 and perplexity is 435.1601568253706
At time: 1505.415512561798 and batch: 700, loss is 6.0850300598144536 and perplexity is 439.2330157375977
At time: 1506.5751340389252 and batch: 750, loss is 6.040555105209351 and perplexity is 420.1261844029791
At time: 1507.74112534523 and batch: 800, loss is 6.047754383087158 and perplexity is 423.1617032091337
At time: 1508.9009232521057 and batch: 850, loss is 6.077285423278808 and perplexity is 435.84445420196306
At time: 1510.0618469715118 and batch: 900, loss is 6.057431163787842 and perplexity is 427.2764227226985
At time: 1511.2223725318909 and batch: 950, loss is 6.033486795425415 and perplexity is 417.16707266251507
At time: 1512.3849472999573 and batch: 1000, loss is 6.029759683609009 and perplexity is 415.61513824966306
At time: 1513.5521757602692 and batch: 1050, loss is 6.024211721420288 and perplexity is 413.31570565815
At time: 1514.7147207260132 and batch: 1100, loss is 6.018791284561157 and perplexity is 411.0814148593402
At time: 1515.8754544258118 and batch: 1150, loss is 6.068371229171753 and perplexity is 431.9765175189902
At time: 1517.0373437404633 and batch: 1200, loss is 6.077965755462646 and perplexity is 436.1410740998944
At time: 1518.1979460716248 and batch: 1250, loss is 6.062116231918335 and perplexity is 429.28293853512287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.692668385749315 and perplexity of 296.684233321805
Finished 49 epochs...
Completing Train Step...
At time: 1521.084234714508 and batch: 50, loss is 6.047290506362915 and perplexity is 422.9654538656946
At time: 1522.269676208496 and batch: 100, loss is 6.0888411235809325 and perplexity is 440.9101545805289
At time: 1523.4277210235596 and batch: 150, loss is 6.007788257598877 and perplexity is 406.5830680666786
At time: 1524.5879237651825 and batch: 200, loss is 6.026976051330567 and perplexity is 414.459827261905
At time: 1525.7463812828064 and batch: 250, loss is 6.061223077774048 and perplexity is 428.8996938732545
At time: 1526.9058771133423 and batch: 300, loss is 6.075624027252197 and perplexity is 435.12094514149203
At time: 1528.0652890205383 and batch: 350, loss is 6.085364599227905 and perplexity is 439.3799810745292
At time: 1529.2234025001526 and batch: 400, loss is 6.056343593597412 and perplexity is 426.81198222387127
At time: 1530.3813235759735 and batch: 450, loss is 6.0330459499359135 and perplexity is 416.98320697132124
At time: 1531.541101694107 and batch: 500, loss is 6.0283527755737305 and perplexity is 415.0308171115118
At time: 1532.6997923851013 and batch: 550, loss is 6.048505744934082 and perplexity is 423.4797702447586
At time: 1533.8593871593475 and batch: 600, loss is 6.082527961730957 and perplexity is 438.1353854127207
At time: 1535.0193030834198 and batch: 650, loss is 6.075715942382812 and perplexity is 435.16094117809035
At time: 1536.1776139736176 and batch: 700, loss is 6.08503342628479 and perplexity is 439.23449440500485
At time: 1537.3356046676636 and batch: 750, loss is 6.04055962562561 and perplexity is 420.12808355250644
At time: 1538.49484872818 and batch: 800, loss is 6.047757244110107 and perplexity is 423.1629138862097
At time: 1539.6673736572266 and batch: 850, loss is 6.077286491394043 and perplexity is 435.8449197343131
At time: 1540.8328847885132 and batch: 900, loss is 6.057433271408081 and perplexity is 427.27732326008373
At time: 1541.9924147129059 and batch: 950, loss is 6.033490161895752 and perplexity is 417.1684770454547
At time: 1543.1505935192108 and batch: 1000, loss is 6.0297638702392575 and perplexity is 415.6168782802151
At time: 1544.3103444576263 and batch: 1050, loss is 6.024224433898926 and perplexity is 413.3209599586264
At time: 1545.4713714122772 and batch: 1100, loss is 6.018807621002197 and perplexity is 411.0881305214915
At time: 1546.630467414856 and batch: 1150, loss is 6.068383083343506 and perplexity is 431.9816382731735
At time: 1547.7888414859772 and batch: 1200, loss is 6.077975158691406 and perplexity is 436.14517525346787
At time: 1548.9473476409912 and batch: 1250, loss is 6.062117013931275 and perplexity is 429.2832742400669
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.692661257555885 and perplexity of 296.6821185067396
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f58a487a898>
SETTINGS FOR THIS RUN
{'anneal': 2.8139391373442404, 'data': 'wikitext', 'lr': 11.175666046230907, 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.0, 'num_layers': 1, 'wordvec_source': 'glove', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.74537992477417 and batch: 50, loss is 7.331454448699951 and perplexity is 1527.6019757205834
At time: 2.957041025161743 and batch: 100, loss is 6.37822847366333 and perplexity is 588.8835603279391
At time: 4.121127128601074 and batch: 150, loss is 6.033309669494629 and perplexity is 417.09318810010535
At time: 5.2676098346710205 and batch: 200, loss is 5.78666485786438 and perplexity is 325.9242061505082
At time: 6.41409969329834 and batch: 250, loss is 5.6984012413024905 and perplexity is 298.3899658638188
At time: 7.562290191650391 and batch: 300, loss is 5.626861619949341 and perplexity is 277.7889408990981
At time: 8.739157438278198 and batch: 350, loss is 5.60527361869812 and perplexity is 271.8563002256217
At time: 9.911588430404663 and batch: 400, loss is 5.541287097930908 and perplexity is 255.00600602771198
At time: 11.086387395858765 and batch: 450, loss is 5.479568767547607 and perplexity is 239.74329998834872
At time: 12.261274337768555 and batch: 500, loss is 5.468604192733765 and perplexity is 237.12897531030677
At time: 13.437368392944336 and batch: 550, loss is 5.432473278045654 and perplexity is 228.7142203258534
At time: 14.61122751235962 and batch: 600, loss is 5.435882635116577 and perplexity is 229.4953195365946
At time: 15.783555746078491 and batch: 650, loss is 5.407593030929565 and perplexity is 223.09396065120404
At time: 16.955755949020386 and batch: 700, loss is 5.414892101287842 and perplexity is 224.72829647663517
At time: 18.128302097320557 and batch: 750, loss is 5.3901468276977536 and perplexity is 219.235573045345
At time: 19.30014204978943 and batch: 800, loss is 5.394805431365967 and perplexity is 220.25928737837353
At time: 20.476794242858887 and batch: 850, loss is 5.420360298156738 and perplexity is 225.96052099649958
At time: 21.651280164718628 and batch: 900, loss is 5.395031661987304 and perplexity is 220.30912241070516
At time: 22.812750339508057 and batch: 950, loss is 5.371342906951904 and perplexity is 215.15160237158557
At time: 23.96074342727661 and batch: 1000, loss is 5.361929016113281 and perplexity is 213.13569233929576
At time: 25.110701084136963 and batch: 1050, loss is 5.360359315872192 and perplexity is 212.80139563309734
At time: 26.259155988693237 and batch: 1100, loss is 5.320803394317627 and perplexity is 204.54814881905767
At time: 27.415717840194702 and batch: 1150, loss is 5.3451591491699215 and perplexity is 209.59123821628492
At time: 28.56817889213562 and batch: 1200, loss is 5.32136269569397 and perplexity is 204.66258487936037
At time: 29.717336893081665 and batch: 1250, loss is 5.314914178848267 and perplexity is 203.3470609010052
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.075557513828695 and perplexity of 160.06140355465436
Finished 1 epochs...
Completing Train Step...
At time: 32.60418486595154 and batch: 50, loss is 5.298456554412842 and perplexity is 200.02783951037705
At time: 33.75673246383667 and batch: 100, loss is 5.305986804962158 and perplexity is 201.53978477754453
At time: 34.907857179641724 and batch: 150, loss is 5.190003423690796 and perplexity is 179.4691673777171
At time: 36.08432173728943 and batch: 200, loss is 5.218768701553345 and perplexity is 184.7066150285592
At time: 37.23903775215149 and batch: 250, loss is 5.231904344558716 and perplexity is 187.1488602989495
At time: 38.398330211639404 and batch: 300, loss is 5.21999888420105 and perplexity is 184.93397772143007
At time: 39.556936264038086 and batch: 350, loss is 5.230338792800904 and perplexity is 186.85609829856355
At time: 40.71776223182678 and batch: 400, loss is 5.19225378036499 and perplexity is 179.87349178277094
At time: 41.8758430480957 and batch: 450, loss is 5.145722417831421 and perplexity is 171.69547575224348
At time: 43.03508424758911 and batch: 500, loss is 5.1411895847320555 and perplexity is 170.91897003170237
At time: 44.193012952804565 and batch: 550, loss is 5.1361137390136715 and perplexity is 170.05360978497075
At time: 45.35081934928894 and batch: 600, loss is 5.150499382019043 and perplexity is 172.5176210069043
At time: 46.51058006286621 and batch: 650, loss is 5.1370761013031006 and perplexity is 170.21734173830123
At time: 47.6686851978302 and batch: 700, loss is 5.139716901779175 and perplexity is 170.6674458313725
At time: 48.82854771614075 and batch: 750, loss is 5.112458238601684 and perplexity is 166.07811311341942
At time: 49.98809552192688 and batch: 800, loss is 5.133192481994629 and perplexity is 169.5575643746016
At time: 51.14582538604736 and batch: 850, loss is 5.18534107208252 and perplexity is 178.6343665961487
At time: 52.30827307701111 and batch: 900, loss is 5.163471794128418 and perplexity is 174.77016958817762
At time: 53.46633005142212 and batch: 950, loss is 5.148462152481079 and perplexity is 172.16652077088656
At time: 54.624232053756714 and batch: 1000, loss is 5.138229713439942 and perplexity is 170.4138198376016
At time: 55.78350019454956 and batch: 1050, loss is 5.138539009094238 and perplexity is 170.4665362435668
At time: 56.94542384147644 and batch: 1100, loss is 5.088593864440918 and perplexity is 162.16168035881586
At time: 58.1180362701416 and batch: 1150, loss is 5.1022161102294925 and perplexity is 164.38580099186868
At time: 59.290194272994995 and batch: 1200, loss is 5.097090711593628 and perplexity is 163.5454137312957
At time: 60.44902467727661 and batch: 1250, loss is 5.11406247138977 and perplexity is 166.34475488841355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.998058903826414 and perplexity of 148.12535430609458
Finished 2 epochs...
Completing Train Step...
At time: 63.35895323753357 and batch: 50, loss is 5.090246095657348 and perplexity is 162.42983041115585
At time: 64.51533555984497 and batch: 100, loss is 5.069401578903198 and perplexity is 159.07910256284615
At time: 65.67083168029785 and batch: 150, loss is 4.983845920562744 and perplexity is 146.0349418133447
At time: 66.8275454044342 and batch: 200, loss is 5.047166395187378 and perplexity is 155.58098426578673
At time: 67.98281455039978 and batch: 250, loss is 5.049784421920776 and perplexity is 155.9888330884116
At time: 69.13830876350403 and batch: 300, loss is 5.034165840148926 and perplexity is 153.57143604152333
At time: 70.29495859146118 and batch: 350, loss is 5.014748411178589 and perplexity is 150.61823813366556
At time: 71.44952082633972 and batch: 400, loss is 5.011851787567139 and perplexity is 150.18258505487938
At time: 72.6062741279602 and batch: 450, loss is 4.9701961898803715 and perplexity is 144.05514676073568
At time: 73.76071810722351 and batch: 500, loss is 4.973483362197876 and perplexity is 144.52945999893552
At time: 74.91565299034119 and batch: 550, loss is 4.951076889038086 and perplexity is 141.32707558156557
At time: 76.07190823554993 and batch: 600, loss is 4.967274465560913 and perplexity is 143.63487159815563
At time: 77.22703814506531 and batch: 650, loss is 4.982689342498779 and perplexity is 145.86613863891233
At time: 78.38339877128601 and batch: 700, loss is 4.989806470870971 and perplexity is 146.907989769099
At time: 79.5386528968811 and batch: 750, loss is 4.968733139038086 and perplexity is 143.84454085807064
At time: 80.69477391242981 and batch: 800, loss is 4.981406421661377 and perplexity is 145.67912391837237
At time: 81.85189080238342 and batch: 850, loss is 5.03917293548584 and perplexity is 154.34231117490236
At time: 83.00739359855652 and batch: 900, loss is 5.000186729431152 and perplexity is 148.44087479493854
At time: 84.16477274894714 and batch: 950, loss is 4.985127601623535 and perplexity is 146.22223203000385
At time: 85.32019114494324 and batch: 1000, loss is 4.987002592086792 and perplexity is 146.4966545099463
At time: 86.47610402107239 and batch: 1050, loss is 4.980179262161255 and perplexity is 145.50046204321612
At time: 87.632817029953 and batch: 1100, loss is 4.921779661178589 and perplexity is 137.24664850420388
At time: 88.78839373588562 and batch: 1150, loss is 4.941628322601319 and perplexity is 139.99802601279868
At time: 89.97332072257996 and batch: 1200, loss is 4.95020830154419 and perplexity is 141.20437394743317
At time: 91.1303071975708 and batch: 1250, loss is 4.971512498855591 and perplexity is 144.24489269808635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.925917131187272 and perplexity of 137.81567875723258
Finished 3 epochs...
Completing Train Step...
At time: 94.00101733207703 and batch: 50, loss is 4.9420586109161375 and perplexity is 140.0582784895284
At time: 95.18124628067017 and batch: 100, loss is 4.9422855567932125 and perplexity is 140.09006774546563
At time: 96.33986806869507 and batch: 150, loss is 4.859206037521362 and perplexity is 128.9218023882154
At time: 97.50030970573425 and batch: 200, loss is 4.911990184783935 and perplexity is 135.9096307081166
At time: 98.65894627571106 and batch: 250, loss is 4.914838876724243 and perplexity is 136.29734735817826
At time: 99.8188784122467 and batch: 300, loss is 4.916747312545777 and perplexity is 136.557710462371
At time: 100.97729921340942 and batch: 350, loss is 4.888414068222046 and perplexity is 132.74288586590637
At time: 102.13569116592407 and batch: 400, loss is 4.895046234130859 and perplexity is 133.62618456223294
At time: 103.29513502120972 and batch: 450, loss is 4.844599809646606 and perplexity is 127.05242666298345
At time: 104.45447134971619 and batch: 500, loss is 4.84872368812561 and perplexity is 127.57745726813887
At time: 105.62631058692932 and batch: 550, loss is 4.831713581085205 and perplexity is 125.42570372588912
At time: 106.8006010055542 and batch: 600, loss is 4.857606029510498 and perplexity is 128.7156914052019
At time: 107.97908806800842 and batch: 650, loss is 4.87470235824585 and perplexity is 130.9351756350734
At time: 109.14383363723755 and batch: 700, loss is 4.872359075546265 and perplexity is 130.6287167034267
At time: 110.31582021713257 and batch: 750, loss is 4.856453218460083 and perplexity is 128.56739203079084
At time: 111.48650312423706 and batch: 800, loss is 4.88835807800293 and perplexity is 132.73545377070488
At time: 112.65014147758484 and batch: 850, loss is 4.946675138473511 and perplexity is 140.70635617505764
At time: 113.80954694747925 and batch: 900, loss is 4.8977405548095705 and perplexity is 133.98670181096156
At time: 114.97279000282288 and batch: 950, loss is 4.890044164657593 and perplexity is 132.95944603002252
At time: 116.13327407836914 and batch: 1000, loss is 4.894281091690064 and perplexity is 133.5239806025026
At time: 117.29302525520325 and batch: 1050, loss is 4.888781604766845 and perplexity is 132.7916826942838
At time: 118.45262408256531 and batch: 1100, loss is 4.829268350601196 and perplexity is 125.11938363589904
At time: 119.64913201332092 and batch: 1150, loss is 4.845253610610962 and perplexity is 127.1355208226204
At time: 120.80743932723999 and batch: 1200, loss is 4.848670854568481 and perplexity is 127.57071707531763
At time: 121.96747469902039 and batch: 1250, loss is 4.883480987548828 and perplexity is 132.08966701646327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.919680407447537 and perplexity of 136.9588351675636
Finished 4 epochs...
Completing Train Step...
At time: 124.8728973865509 and batch: 50, loss is 4.841647024154663 and perplexity is 126.6778214374345
At time: 126.02886867523193 and batch: 100, loss is 4.83065245628357 and perplexity is 125.2926819897164
At time: 127.18510293960571 and batch: 150, loss is 4.7524267673492435 and perplexity is 115.86512131991813
At time: 128.3430392742157 and batch: 200, loss is 4.825301790237427 and perplexity is 124.62407303714221
At time: 129.49956107139587 and batch: 250, loss is 4.835854215621948 and perplexity is 125.94612241478855
At time: 130.655517578125 and batch: 300, loss is 4.837468166351318 and perplexity is 126.14955737376182
At time: 131.81284999847412 and batch: 350, loss is 4.8051777267456055 and perplexity is 122.14119684969226
At time: 132.97726106643677 and batch: 400, loss is 4.810220327377319 and perplexity is 122.75866163183693
At time: 134.1353635787964 and batch: 450, loss is 4.760296897888184 and perplexity is 116.78059266383336
At time: 135.30009245872498 and batch: 500, loss is 4.7688365650177005 and perplexity is 117.7821303651667
At time: 136.45561122894287 and batch: 550, loss is 4.759113550186157 and perplexity is 116.64248235024088
At time: 137.61058831214905 and batch: 600, loss is 4.786072778701782 and perplexity is 119.82984506079174
At time: 138.7681529521942 and batch: 650, loss is 4.800480375289917 and perplexity is 121.568802142921
At time: 139.92316102981567 and batch: 700, loss is 4.803519773483276 and perplexity is 121.93886023258645
At time: 141.07879066467285 and batch: 750, loss is 4.787820863723755 and perplexity is 120.03950101295378
At time: 142.23545002937317 and batch: 800, loss is 4.802122344970703 and perplexity is 121.76857839856243
At time: 143.39153146743774 and batch: 850, loss is 4.868448362350464 and perplexity is 130.1188628531491
At time: 144.54744005203247 and batch: 900, loss is 4.820805807113647 and perplexity is 124.06502298945352
At time: 145.71322965621948 and batch: 950, loss is 4.809136552810669 and perplexity is 122.62569098464232
At time: 146.86864304542542 and batch: 1000, loss is 4.814563179016114 and perplexity is 123.29294360061385
At time: 148.0554621219635 and batch: 1050, loss is 4.825644388198852 and perplexity is 124.66677630511782
At time: 149.21729493141174 and batch: 1100, loss is 4.7747827243804934 and perplexity is 118.4845680158522
At time: 150.38303351402283 and batch: 1150, loss is 4.784615354537964 and perplexity is 119.65532935164981
At time: 151.56814289093018 and batch: 1200, loss is 4.799695730209351 and perplexity is 121.47345119358793
At time: 152.75201177597046 and batch: 1250, loss is 4.823370571136475 and perplexity is 124.38362889674289
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.916355996236314 and perplexity of 136.5042836571561
Finished 5 epochs...
Completing Train Step...
At time: 155.84976482391357 and batch: 50, loss is 4.776178064346314 and perplexity is 118.65000966576822
At time: 157.03403520584106 and batch: 100, loss is 4.771837663650513 and perplexity is 118.13613709443142
At time: 158.21891927719116 and batch: 150, loss is 4.691913795471192 and perplexity is 109.06170200072614
At time: 159.40328097343445 and batch: 200, loss is 4.765685119628906 and perplexity is 117.41153068259625
At time: 160.58683061599731 and batch: 250, loss is 4.765333843231201 and perplexity is 117.37029402620513
At time: 161.7687623500824 and batch: 300, loss is 4.768624095916748 and perplexity is 117.75710796015785
At time: 162.95210242271423 and batch: 350, loss is 4.743221950531006 and perplexity is 114.80349762418815
At time: 164.13630843162537 and batch: 400, loss is 4.750379867553711 and perplexity is 115.62819958702356
At time: 165.3201563358307 and batch: 450, loss is 4.693540058135986 and perplexity is 109.23920927245169
At time: 166.50614094734192 and batch: 500, loss is 4.716101274490357 and perplexity is 111.73179081509444
At time: 167.69095540046692 and batch: 550, loss is 4.693752975463867 and perplexity is 109.26247066927912
At time: 168.87738370895386 and batch: 600, loss is 4.728800582885742 and perplexity is 113.15975517321567
At time: 170.0622522830963 and batch: 650, loss is 4.740315132141113 and perplexity is 114.47026925787871
At time: 171.25394940376282 and batch: 700, loss is 4.747206783294677 and perplexity is 115.2618830509844
At time: 172.4372375011444 and batch: 750, loss is 4.7400201797485355 and perplexity is 114.43651095687268
At time: 173.62484049797058 and batch: 800, loss is 4.758839960098267 and perplexity is 116.61057448827933
At time: 174.8053903579712 and batch: 850, loss is 4.824742050170898 and perplexity is 124.55433546948545
At time: 175.9869065284729 and batch: 900, loss is 4.779115877151489 and perplexity is 118.99909370420639
At time: 177.16709685325623 and batch: 950, loss is 4.771587657928467 and perplexity is 118.10660607579301
At time: 178.41485023498535 and batch: 1000, loss is 4.767320213317871 and perplexity is 117.6036665727109
At time: 179.5946226119995 and batch: 1050, loss is 4.7754512214660645 and perplexity is 118.5638010848431
At time: 180.77707171440125 and batch: 1100, loss is 4.711214456558228 and perplexity is 111.18710985844305
At time: 181.96118116378784 and batch: 1150, loss is 4.729691905975342 and perplexity is 113.26066203944438
At time: 183.1432020664215 and batch: 1200, loss is 4.743847846984863 and perplexity is 114.87537521786057
At time: 184.31984448432922 and batch: 1250, loss is 4.766542959213257 and perplexity is 117.51229415454317
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.906419740106068 and perplexity of 135.1546583471751
Finished 6 epochs...
Completing Train Step...
At time: 187.43311166763306 and batch: 50, loss is 4.721067848205567 and perplexity is 112.28809531062959
At time: 188.62447309494019 and batch: 100, loss is 4.709531497955322 and perplexity is 111.00014392746502
At time: 189.7836787700653 and batch: 150, loss is 4.643444566726685 and perplexity is 103.90162798252825
At time: 190.93925642967224 and batch: 200, loss is 4.709037647247315 and perplexity is 110.94533996140004
At time: 192.0960009098053 and batch: 250, loss is 4.71035327911377 and perplexity is 111.09139924514432
At time: 193.25364017486572 and batch: 300, loss is 4.724667301177979 and perplexity is 112.69299930834764
At time: 194.4093656539917 and batch: 350, loss is 4.684948425292969 and perplexity is 108.30468638166857
At time: 195.58502197265625 and batch: 400, loss is 4.703074512481689 and perplexity is 110.28572658392049
At time: 196.75444436073303 and batch: 450, loss is 4.641357650756836 and perplexity is 103.68502011562111
At time: 197.92508721351624 and batch: 500, loss is 4.66692214012146 and perplexity is 106.36984659945398
At time: 199.09257411956787 and batch: 550, loss is 4.647215118408203 and perplexity is 104.29413395763922
At time: 200.26189041137695 and batch: 600, loss is 4.685188150405883 and perplexity is 108.33065284712315
At time: 201.42423844337463 and batch: 650, loss is 4.699768838882446 and perplexity is 109.92175987818021
At time: 202.588773727417 and batch: 700, loss is 4.697832307815552 and perplexity is 109.70909895395091
At time: 203.76068925857544 and batch: 750, loss is 4.698746194839478 and perplexity is 109.80940650379434
At time: 204.93925309181213 and batch: 800, loss is 4.716814231872559 and perplexity is 111.8114792240269
At time: 206.1181936264038 and batch: 850, loss is 4.77233060836792 and perplexity is 118.19438603472118
At time: 207.29548382759094 and batch: 900, loss is 4.718237524032593 and perplexity is 111.97073293120053
At time: 208.50592589378357 and batch: 950, loss is 4.719992761611938 and perplexity is 112.16744075340668
At time: 209.68189811706543 and batch: 1000, loss is 4.7188232898712155 and perplexity is 112.03634077501806
At time: 210.86055541038513 and batch: 1050, loss is 4.731304006576538 and perplexity is 113.44339687470365
At time: 212.05006074905396 and batch: 1100, loss is 4.663057699203491 and perplexity is 105.95957984820093
At time: 213.22876286506653 and batch: 1150, loss is 4.680465230941772 and perplexity is 107.82022220843464
At time: 214.40669322013855 and batch: 1200, loss is 4.6951626205444335 and perplexity is 109.4166005822628
At time: 215.5829632282257 and batch: 1250, loss is 4.723108263015747 and perplexity is 112.51744350646574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.902734018590328 and perplexity of 134.65743279529156
Finished 7 epochs...
Completing Train Step...
At time: 218.5934660434723 and batch: 50, loss is 4.677245197296142 and perplexity is 107.4735958390331
At time: 219.7708282470703 and batch: 100, loss is 4.6613775157928465 and perplexity is 105.78169779902217
At time: 220.94745993614197 and batch: 150, loss is 4.591101236343384 and perplexity is 98.60295555314646
At time: 222.1219129562378 and batch: 200, loss is 4.66802321434021 and perplexity is 106.48703219837867
At time: 223.29680824279785 and batch: 250, loss is 4.674506502151489 and perplexity is 107.17966110646304
At time: 224.47348642349243 and batch: 300, loss is 4.685115060806274 and perplexity is 108.32273529243014
At time: 225.65083861351013 and batch: 350, loss is 4.668138217926026 and perplexity is 106.49927929314072
At time: 226.8273754119873 and batch: 400, loss is 4.671292037963867 and perplexity is 106.83568906328422
At time: 228.00356125831604 and batch: 450, loss is 4.607605657577515 and perplexity is 100.24384397594969
At time: 229.17939281463623 and batch: 500, loss is 4.624736661911011 and perplexity is 101.97591540384461
At time: 230.35517001152039 and batch: 550, loss is 4.609595203399659 and perplexity is 100.44348222579843
At time: 231.53219747543335 and batch: 600, loss is 4.644161748886108 and perplexity is 103.9761711037556
At time: 232.71266198158264 and batch: 650, loss is 4.6472726249694825 and perplexity is 104.3001317270986
At time: 233.89075016975403 and batch: 700, loss is 4.662791404724121 and perplexity is 105.93136715366015
At time: 235.06591391563416 and batch: 750, loss is 4.651674880981445 and perplexity is 104.76029975460985
At time: 236.24112272262573 and batch: 800, loss is 4.673236665725708 and perplexity is 107.04364684489575
At time: 237.41844081878662 and batch: 850, loss is 4.732102842330932 and perplexity is 113.53405572217042
At time: 238.62597823143005 and batch: 900, loss is 4.685631198883057 and perplexity is 108.37865921168631
At time: 239.81377291679382 and batch: 950, loss is 4.678437671661377 and perplexity is 107.60183179085745
At time: 240.9978928565979 and batch: 1000, loss is 4.671412868499756 and perplexity is 106.84859885677872
At time: 242.18213033676147 and batch: 1050, loss is 4.693112125396729 and perplexity is 109.19247223925996
At time: 243.367751121521 and batch: 1100, loss is 4.631420087814331 and perplexity is 102.65974650014498
At time: 244.55338716506958 and batch: 1150, loss is 4.642554264068604 and perplexity is 103.8091652529709
At time: 245.73971486091614 and batch: 1200, loss is 4.671504154205322 and perplexity is 106.85835305171663
At time: 246.92537593841553 and batch: 1250, loss is 4.695969161987304 and perplexity is 109.50488520298731
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.9154088375342155 and perplexity of 136.3750536473418
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 249.93798303604126 and batch: 50, loss is 4.616613063812256 and perplexity is 101.15085979873265
At time: 251.1418116092682 and batch: 100, loss is 4.574395093917847 and perplexity is 96.96936403417244
At time: 252.32401323318481 and batch: 150, loss is 4.484052419662476 and perplexity is 88.59296207730038
At time: 253.50628113746643 and batch: 200, loss is 4.535911312103272 and perplexity is 93.30850974457907
At time: 254.68638062477112 and batch: 250, loss is 4.5370487880706785 and perplexity is 93.41470631854321
At time: 255.87008023262024 and batch: 300, loss is 4.530626392364502 and perplexity is 92.81668253803342
At time: 257.05162024497986 and batch: 350, loss is 4.501289253234863 and perplexity is 90.1332610224683
At time: 258.2329807281494 and batch: 400, loss is 4.505234203338623 and perplexity is 90.48953451842756
At time: 259.41548252105713 and batch: 450, loss is 4.422913875579834 and perplexity is 83.33877070952407
At time: 260.5967357158661 and batch: 500, loss is 4.4293218517303465 and perplexity is 83.8745182601627
At time: 261.77894616127014 and batch: 550, loss is 4.413172035217285 and perplexity is 82.53083946746487
At time: 262.9618949890137 and batch: 600, loss is 4.434638586044311 and perplexity is 84.32164436153968
At time: 264.1448509693146 and batch: 650, loss is 4.441933746337891 and perplexity is 84.93903351169654
At time: 265.32693457603455 and batch: 700, loss is 4.443820199966431 and perplexity is 85.09941829133057
At time: 266.5086896419525 and batch: 750, loss is 4.428842821121216 and perplexity is 83.83434942041
At time: 267.7184991836548 and batch: 800, loss is 4.434414529800415 and perplexity is 84.30275368699071
At time: 268.90121936798096 and batch: 850, loss is 4.487569255828857 and perplexity is 88.90507751816551
At time: 270.0833592414856 and batch: 900, loss is 4.4317760181427 and perplexity is 84.08061307777743
At time: 271.2653262615204 and batch: 950, loss is 4.412409553527832 and perplexity is 82.46793519828002
At time: 272.4474604129791 and batch: 1000, loss is 4.40412633895874 and perplexity is 81.78765693096983
At time: 273.62161231040955 and batch: 1050, loss is 4.422682676315308 and perplexity is 83.3195050742157
At time: 274.7937412261963 and batch: 1100, loss is 4.35161283493042 and perplexity is 77.60352372072227
At time: 275.96549129486084 and batch: 1150, loss is 4.38545000076294 and perplexity is 80.27433859993019
At time: 277.13824582099915 and batch: 1200, loss is 4.366647605895996 and perplexity is 78.77908996652415
At time: 278.306866645813 and batch: 1250, loss is 4.416769514083862 and perplexity is 82.82827711032425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.700259466240876 and perplexity of 109.97570373294663
Finished 9 epochs...
Completing Train Step...
At time: 281.2822790145874 and batch: 50, loss is 4.447102460861206 and perplexity is 85.37919568417861
At time: 282.46100425720215 and batch: 100, loss is 4.4420257186889645 and perplexity is 84.94684591356352
At time: 283.6406261920929 and batch: 150, loss is 4.364797096252442 and perplexity is 78.63344330267121
At time: 284.8267138004303 and batch: 200, loss is 4.432776355743409 and perplexity is 84.16476415925791
At time: 286.00400590896606 and batch: 250, loss is 4.431511192321778 and perplexity is 84.05834930853605
At time: 287.18007826805115 and batch: 300, loss is 4.440348482131958 and perplexity is 84.8044893743629
At time: 288.3576228618622 and batch: 350, loss is 4.415150671005249 and perplexity is 82.69429960075186
At time: 289.5364136695862 and batch: 400, loss is 4.431816711425781 and perplexity is 84.08403466358433
At time: 290.71675872802734 and batch: 450, loss is 4.350535116195679 and perplexity is 77.51993400052088
At time: 291.8997480869293 and batch: 500, loss is 4.362287101745605 and perplexity is 78.43632128301135
At time: 293.0800440311432 and batch: 550, loss is 4.344226732254028 and perplexity is 77.03244773563078
At time: 294.25794529914856 and batch: 600, loss is 4.377421092987061 and perplexity is 79.63240380461234
At time: 295.43671584129333 and batch: 650, loss is 4.390605707168579 and perplexity is 80.68927825604361
At time: 296.6164462566376 and batch: 700, loss is 4.384900226593017 and perplexity is 80.23021797136262
At time: 297.8223567008972 and batch: 750, loss is 4.380994863510132 and perplexity is 79.91750087432821
At time: 298.9988090991974 and batch: 800, loss is 4.391569995880127 and perplexity is 80.76712354284251
At time: 300.17602801322937 and batch: 850, loss is 4.446740703582764 and perplexity is 85.348314724755
At time: 301.35469222068787 and batch: 900, loss is 4.397487535476684 and perplexity is 81.24648311029667
At time: 302.53443908691406 and batch: 950, loss is 4.380956830978394 and perplexity is 79.91446146723831
At time: 303.7135877609253 and batch: 1000, loss is 4.376909036636352 and perplexity is 79.59163796461635
At time: 304.8866353034973 and batch: 1050, loss is 4.405962886810303 and perplexity is 81.93800189219024
At time: 306.0531802177429 and batch: 1100, loss is 4.33087266921997 and perplexity is 76.01058973666781
At time: 307.2237322330475 and batch: 1150, loss is 4.362120027542114 and perplexity is 78.42321769177477
At time: 308.4067885875702 and batch: 1200, loss is 4.353716745376587 and perplexity is 77.76696645909081
At time: 309.5865545272827 and batch: 1250, loss is 4.403623065948486 and perplexity is 81.74650576666834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.698732250798358 and perplexity of 109.8078753275901
Finished 10 epochs...
Completing Train Step...
At time: 312.57048511505127 and batch: 50, loss is 4.401246528625489 and perplexity is 81.5524628111806
At time: 313.7768392562866 and batch: 100, loss is 4.402060499191284 and perplexity is 81.61887113902259
At time: 314.95866656303406 and batch: 150, loss is 4.323891801834106 and perplexity is 75.48181768095797
At time: 316.14194655418396 and batch: 200, loss is 4.385622892379761 and perplexity is 80.28821855994978
At time: 317.32257628440857 and batch: 250, loss is 4.397465467453003 and perplexity is 81.24469018076664
At time: 318.50314474105835 and batch: 300, loss is 4.405588579177857 and perplexity is 81.90733761198966
At time: 319.68289279937744 and batch: 350, loss is 4.378841171264648 and perplexity is 79.74556838371011
At time: 320.8638548851013 and batch: 400, loss is 4.400902938842774 and perplexity is 81.52444703144688
At time: 322.0495195388794 and batch: 450, loss is 4.313328046798706 and perplexity is 74.6886430765026
At time: 323.23236560821533 and batch: 500, loss is 4.3282584381103515 and perplexity is 75.8121399980784
At time: 324.4117786884308 and batch: 550, loss is 4.316033010482788 and perplexity is 74.89094663217186
At time: 325.5914900302887 and batch: 600, loss is 4.344925203323364 and perplexity is 77.08627146674559
At time: 326.77130341529846 and batch: 650, loss is 4.360419301986695 and perplexity is 78.2899546753385
At time: 327.9799597263336 and batch: 700, loss is 4.350539321899414 and perplexity is 77.52026002708246
At time: 329.16350269317627 and batch: 750, loss is 4.354389514923096 and perplexity is 77.81930330919428
At time: 330.343514919281 and batch: 800, loss is 4.368759384155274 and perplexity is 78.94562972162637
At time: 331.5270013809204 and batch: 850, loss is 4.419208908081055 and perplexity is 83.03057455357272
At time: 332.7100749015808 and batch: 900, loss is 4.373055686950684 and perplexity is 79.28553369364242
At time: 333.89210391044617 and batch: 950, loss is 4.3607131862640385 and perplexity is 78.31296624329495
At time: 335.07450461387634 and batch: 1000, loss is 4.356479711532593 and perplexity is 77.98213106485422
At time: 336.2554531097412 and batch: 1050, loss is 4.380752735137939 and perplexity is 79.89815292237036
At time: 337.43887543678284 and batch: 1100, loss is 4.311865816116333 and perplexity is 74.57951085865483
At time: 338.6261558532715 and batch: 1150, loss is 4.342059841156006 and perplexity is 76.86570752955062
At time: 339.8149139881134 and batch: 1200, loss is 4.3312308216094975 and perplexity is 76.0378179866517
At time: 341.00473618507385 and batch: 1250, loss is 4.38386833190918 and perplexity is 80.1474715361006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.696948420392336 and perplexity of 109.61217130404899
Finished 11 epochs...
Completing Train Step...
At time: 343.9959075450897 and batch: 50, loss is 4.366316289901733 and perplexity is 78.75299351732899
At time: 345.1987113952637 and batch: 100, loss is 4.368196468353272 and perplexity is 78.9012024847271
At time: 346.3765251636505 and batch: 150, loss is 4.294558153152466 and perplexity is 73.29981999446879
At time: 347.5551335811615 and batch: 200, loss is 4.360051050186157 and perplexity is 78.26112956634019
At time: 348.72430181503296 and batch: 250, loss is 4.386123418807983 and perplexity is 80.32841499406355
At time: 349.8946361541748 and batch: 300, loss is 4.377333068847657 and perplexity is 79.62539453929554
At time: 351.06741213798523 and batch: 350, loss is 4.350329370498657 and perplexity is 77.50398624831381
At time: 352.2338180541992 and batch: 400, loss is 4.374727363586426 and perplexity is 79.41818431144863
At time: 353.3965549468994 and batch: 450, loss is 4.2865144157409665 and perplexity is 72.7125804567229
At time: 354.56133675575256 and batch: 500, loss is 4.306686458587646 and perplexity is 74.19423550827105
At time: 355.72055411338806 and batch: 550, loss is 4.29184862613678 and perplexity is 73.1014809757563
At time: 356.88016533851624 and batch: 600, loss is 4.3236281776428225 and perplexity is 75.46192147049162
At time: 358.0786316394806 and batch: 650, loss is 4.338409099578858 and perplexity is 76.58560230221671
At time: 359.23634243011475 and batch: 700, loss is 4.327345342636108 and perplexity is 75.7429478704701
At time: 360.3947219848633 and batch: 750, loss is 4.331452684402466 and perplexity is 76.05468982086838
At time: 361.56549763679504 and batch: 800, loss is 4.349680833816528 and perplexity is 77.45373836577926
At time: 362.72448778152466 and batch: 850, loss is 4.399074010848999 and perplexity is 81.37548095373289
At time: 363.88491582870483 and batch: 900, loss is 4.356719875335694 and perplexity is 78.00086179915681
At time: 365.0446283817291 and batch: 950, loss is 4.339255666732788 and perplexity is 76.65046460886514
At time: 366.2022669315338 and batch: 1000, loss is 4.341004381179809 and perplexity is 76.78462165068846
At time: 367.36316442489624 and batch: 1050, loss is 4.361684055328369 and perplexity is 78.3890347998856
At time: 368.5465281009674 and batch: 1100, loss is 4.293970146179199 and perplexity is 73.25673185847653
At time: 369.72556734085083 and batch: 1150, loss is 4.324503660202026 and perplexity is 75.52801599470303
At time: 370.9051265716553 and batch: 1200, loss is 4.318220100402832 and perplexity is 75.05491911257151
At time: 372.08339381217957 and batch: 1250, loss is 4.366945514678955 and perplexity is 78.8025624454939
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.698512167826186 and perplexity of 109.78371114318041
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 375.094792842865 and batch: 50, loss is 4.345706405639649 and perplexity is 77.146514968687
At time: 376.2737452983856 and batch: 100, loss is 4.341285629272461 and perplexity is 76.80622021620799
At time: 377.45431303977966 and batch: 150, loss is 4.265919723510742 and perplexity is 71.23040213155889
At time: 378.63626194000244 and batch: 200, loss is 4.328242359161377 and perplexity is 75.81092102834758
At time: 379.81662487983704 and batch: 250, loss is 4.3466339015960695 and perplexity is 77.2181012422302
At time: 380.9964385032654 and batch: 300, loss is 4.336711645126343 and perplexity is 76.45571200321179
At time: 382.1736617088318 and batch: 350, loss is 4.301840591430664 and perplexity is 73.83556982417507
At time: 383.35038137435913 and batch: 400, loss is 4.3188978099823 and perplexity is 75.10580179013179
At time: 384.52950525283813 and batch: 450, loss is 4.230053653717041 and perplexity is 68.72091920768803
At time: 385.70840096473694 and batch: 500, loss is 4.242604880332947 and perplexity is 69.58888667163377
At time: 386.9134702682495 and batch: 550, loss is 4.230394563674927 and perplexity is 68.74435084717365
At time: 388.0907428264618 and batch: 600, loss is 4.264738006591797 and perplexity is 71.14627767565284
At time: 389.2686927318573 and batch: 650, loss is 4.271551513671875 and perplexity is 71.63268854250455
At time: 390.4472053050995 and batch: 700, loss is 4.2590439510345455 and perplexity is 70.74231799375204
At time: 391.628529548645 and batch: 750, loss is 4.254349842071533 and perplexity is 70.41102401813238
At time: 392.81032061576843 and batch: 800, loss is 4.264288048744202 and perplexity is 71.11427205081654
At time: 393.99087405204773 and batch: 850, loss is 4.312868633270264 and perplexity is 74.65433798416741
At time: 395.17190170288086 and batch: 900, loss is 4.261732683181763 and perplexity is 70.93278107567356
At time: 396.35106134414673 and batch: 950, loss is 4.243957796096802 and perplexity is 69.68309828922196
At time: 397.53012347221375 and batch: 1000, loss is 4.237276563644409 and perplexity is 69.21908114086247
At time: 398.7098431587219 and batch: 1050, loss is 4.251179523468018 and perplexity is 70.18815211294104
At time: 399.8873028755188 and batch: 1100, loss is 4.180793013572693 and perplexity is 65.41770978156264
At time: 401.0655176639557 and batch: 1150, loss is 4.205790624618531 and perplexity is 67.07360675315158
At time: 402.24440026283264 and batch: 1200, loss is 4.203550834655761 and perplexity is 66.92354407914908
At time: 403.4248275756836 and batch: 1250, loss is 4.267105479240417 and perplexity is 71.31491408437748
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.635261925467609 and perplexity of 103.05490716524818
Finished 13 epochs...
Completing Train Step...
At time: 406.40769934654236 and batch: 50, loss is 4.295439023971557 and perplexity is 73.36441611318769
At time: 407.609126329422 and batch: 100, loss is 4.29232985496521 and perplexity is 73.13666798162456
At time: 408.78472113609314 and batch: 150, loss is 4.219724092483521 and perplexity is 68.01471592901686
At time: 409.963504076004 and batch: 200, loss is 4.282108411788941 and perplexity is 72.39291328441881
At time: 411.1418421268463 and batch: 250, loss is 4.305228624343872 and perplexity is 74.08615141454541
At time: 412.32141637802124 and batch: 300, loss is 4.299477424621582 and perplexity is 73.66129006341625
At time: 413.49932861328125 and batch: 350, loss is 4.267092781066895 and perplexity is 71.3140085209732
At time: 414.6735827922821 and batch: 400, loss is 4.287791414260864 and perplexity is 72.80549362670804
At time: 415.84581446647644 and batch: 450, loss is 4.198895664215088 and perplexity is 66.61272758813584
At time: 417.05395889282227 and batch: 500, loss is 4.216218657493592 and perplexity is 67.77671216105446
At time: 418.23490858078003 and batch: 550, loss is 4.205368061065673 and perplexity is 67.04526987906577
At time: 419.41541719436646 and batch: 600, loss is 4.240024356842041 and perplexity is 69.40954241542732
At time: 420.6060276031494 and batch: 650, loss is 4.251880302429199 and perplexity is 70.23735573168275
At time: 421.78329277038574 and batch: 700, loss is 4.23874571800232 and perplexity is 69.32084939390252
At time: 422.9629011154175 and batch: 750, loss is 4.236503391265869 and perplexity is 69.16558354335388
At time: 424.125128030777 and batch: 800, loss is 4.249941339492798 and perplexity is 70.10130004825794
At time: 425.2833843231201 and batch: 850, loss is 4.301514596939087 and perplexity is 73.81150375804555
At time: 426.46366238594055 and batch: 900, loss is 4.2523488473892215 and perplexity is 70.27027280167758
At time: 427.6470584869385 and batch: 950, loss is 4.238599939346313 and perplexity is 69.310744630192
At time: 428.82843947410583 and batch: 1000, loss is 4.232571420669555 and perplexity is 68.89416046605623
At time: 429.9869794845581 and batch: 1050, loss is 4.249510154724121 and perplexity is 70.07107995110371
At time: 431.14818835258484 and batch: 1100, loss is 4.1829303359985355 and perplexity is 65.55767804509735
At time: 432.30691051483154 and batch: 1150, loss is 4.20983546257019 and perplexity is 67.34545804988251
At time: 433.46520805358887 and batch: 1200, loss is 4.209653301239014 and perplexity is 67.33319142888138
At time: 434.62480568885803 and batch: 1250, loss is 4.267678980827331 and perplexity is 71.35582503090059
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.634037658245894 and perplexity of 102.92881761976659
Finished 14 epochs...
Completing Train Step...
At time: 437.60968947410583 and batch: 50, loss is 4.275968351364136 and perplexity is 71.94977825265481
At time: 438.7800147533417 and batch: 100, loss is 4.272336511611939 and perplexity is 71.68894213203885
At time: 439.95408368110657 and batch: 150, loss is 4.1991120719909665 and perplexity is 66.62714466028541
At time: 441.1283164024353 and batch: 200, loss is 4.263517732620239 and perplexity is 71.05951267413512
At time: 442.30588030815125 and batch: 250, loss is 4.285547957420349 and perplexity is 72.64234072568046
At time: 443.48946809768677 and batch: 300, loss is 4.282528324127197 and perplexity is 72.42331834519258
At time: 444.6789290904999 and batch: 350, loss is 4.250767965316772 and perplexity is 70.15927155024123
At time: 445.88112902641296 and batch: 400, loss is 4.272985429763794 and perplexity is 71.73547748506027
At time: 447.10232043266296 and batch: 450, loss is 4.1849696779251095 and perplexity is 65.69150898368301
At time: 448.28145956993103 and batch: 500, loss is 4.202326169013977 and perplexity is 66.84163527977238
At time: 449.4595329761505 and batch: 550, loss is 4.19193742275238 and perplexity is 66.15082900965676
At time: 450.6360363960266 and batch: 600, loss is 4.228378028869629 and perplexity is 68.60586514856666
At time: 451.8152177333832 and batch: 650, loss is 4.238856906890869 and perplexity is 69.32855753062158
At time: 452.99419474601746 and batch: 700, loss is 4.227877922058106 and perplexity is 68.57156346606291
At time: 454.1734268665314 and batch: 750, loss is 4.226299157142639 and perplexity is 68.46339049977442
At time: 455.35056042671204 and batch: 800, loss is 4.242980461120606 and perplexity is 69.61502782925321
At time: 456.5327191352844 and batch: 850, loss is 4.295146093368531 and perplexity is 73.34292857786694
At time: 457.7016818523407 and batch: 900, loss is 4.2498064041137695 and perplexity is 70.09184154092353
At time: 458.8596408367157 and batch: 950, loss is 4.2308686828613284 and perplexity is 68.77695159057531
At time: 460.01767206192017 and batch: 1000, loss is 4.226758842468262 and perplexity is 68.49486935035803
At time: 461.17621088027954 and batch: 1050, loss is 4.2443599510192875 and perplexity is 69.7111273258428
At time: 462.3358144760132 and batch: 1100, loss is 4.1800148391723635 and perplexity is 65.3668231963693
At time: 463.4947214126587 and batch: 1150, loss is 4.208255186080932 and perplexity is 67.2391176516137
At time: 464.65521240234375 and batch: 1200, loss is 4.208262205123901 and perplexity is 67.23958960752603
At time: 465.8221113681793 and batch: 1250, loss is 4.263478856086731 and perplexity is 71.05675018030803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.633728918367929 and perplexity of 102.89704429427422
Finished 15 epochs...
Completing Train Step...
At time: 468.7402822971344 and batch: 50, loss is 4.261322631835937 and perplexity is 70.9037009559089
At time: 469.94345235824585 and batch: 100, loss is 4.257428627014161 and perplexity is 70.62813847152859
At time: 471.12545943260193 and batch: 150, loss is 4.185423226356506 and perplexity is 65.72131002213489
At time: 472.29369258880615 and batch: 200, loss is 4.249110050201416 and perplexity is 70.04304980297289
At time: 473.4576256275177 and batch: 250, loss is 4.27265275478363 and perplexity is 71.71161685564817
At time: 474.61896419525146 and batch: 300, loss is 4.268624620437622 and perplexity is 71.423333839946
At time: 475.77955961227417 and batch: 350, loss is 4.237838573455811 and perplexity is 69.25799387724544
At time: 476.97185707092285 and batch: 400, loss is 4.2617941570281985 and perplexity is 70.93714172059609
At time: 478.1612207889557 and batch: 450, loss is 4.174966011047363 and perplexity is 65.03762906255541
At time: 479.35261249542236 and batch: 500, loss is 4.191367411613465 and perplexity is 66.11313304485336
At time: 480.5415577888489 and batch: 550, loss is 4.182531642913818 and perplexity is 65.53154586191717
At time: 481.72890615463257 and batch: 600, loss is 4.21975793838501 and perplexity is 68.01701798734942
At time: 482.91520166397095 and batch: 650, loss is 4.230019044876099 and perplexity is 68.71854089748126
At time: 484.1038279533386 and batch: 700, loss is 4.219782934188843 and perplexity is 68.0187181486367
At time: 485.2922639846802 and batch: 750, loss is 4.21912335395813 and perplexity is 67.97386913921336
At time: 486.49247884750366 and batch: 800, loss is 4.236521248817444 and perplexity is 69.16681868235742
At time: 487.6909475326538 and batch: 850, loss is 4.28618447303772 and perplexity is 72.688593428757
At time: 488.8898205757141 and batch: 900, loss is 4.242128143310547 and perplexity is 69.5557189797713
At time: 490.05963373184204 and batch: 950, loss is 4.224767847061157 and perplexity is 68.35863204899631
At time: 491.22117614746094 and batch: 1000, loss is 4.222657747268677 and perplexity is 68.21454059083767
At time: 492.3830580711365 and batch: 1050, loss is 4.239151735305786 and perplexity is 69.34900057279356
At time: 493.54400277137756 and batch: 1100, loss is 4.176806087493897 and perplexity is 65.1574134443436
At time: 494.70512199401855 and batch: 1150, loss is 4.204653558731079 and perplexity is 66.99738298689901
At time: 495.8666651248932 and batch: 1200, loss is 4.2025501155853275 and perplexity is 66.85660591106493
At time: 497.03728199005127 and batch: 1250, loss is 4.259279799461365 and perplexity is 70.75900442581745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.634766516024179 and perplexity of 103.00386543536811
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 499.91072487831116 and batch: 50, loss is 4.255979404449463 and perplexity is 70.52585671197143
At time: 501.0948293209076 and batch: 100, loss is 4.259324703216553 and perplexity is 70.76218184216796
At time: 502.2669849395752 and batch: 150, loss is 4.187126092910766 and perplexity is 65.83331998481657
At time: 503.44041657447815 and batch: 200, loss is 4.247089252471924 and perplexity is 69.9016498853992
At time: 504.63005328178406 and batch: 250, loss is 4.272048664093018 and perplexity is 71.66830961756358
At time: 505.85347151756287 and batch: 300, loss is 4.261470499038697 and perplexity is 70.91418606301717
At time: 507.0379207134247 and batch: 350, loss is 4.229801359176636 and perplexity is 68.70358348190675
At time: 508.2195761203766 and batch: 400, loss is 4.249916706085205 and perplexity is 70.09957323562979
At time: 509.40131521224976 and batch: 450, loss is 4.161520237922669 and perplexity is 64.1690006344849
At time: 510.57169342041016 and batch: 500, loss is 4.173300657272339 and perplexity is 64.92940853921759
At time: 511.74674129486084 and batch: 550, loss is 4.167138776779175 and perplexity is 64.53055140032775
At time: 512.9226334095001 and batch: 600, loss is 4.208281025886536 and perplexity is 67.2408551197906
At time: 514.0884773731232 and batch: 650, loss is 4.2198885774612425 and perplexity is 68.02590424818118
At time: 515.2611429691315 and batch: 700, loss is 4.199761333465577 and perplexity is 66.67041714453498
At time: 516.4325368404388 and batch: 750, loss is 4.196047396659851 and perplexity is 66.423266663169
At time: 517.6009798049927 and batch: 800, loss is 4.204054975509644 and perplexity is 66.95729147781242
At time: 518.7597315311432 and batch: 850, loss is 4.2517362976074216 and perplexity is 70.22724194202323
At time: 519.9188411235809 and batch: 900, loss is 4.204954071044922 and perplexity is 67.01751955097302
At time: 521.076461315155 and batch: 950, loss is 4.188065242767334 and perplexity is 65.89517637950334
At time: 522.2357180118561 and batch: 1000, loss is 4.181884069442749 and perplexity is 65.48912310870608
At time: 523.4028244018555 and batch: 1050, loss is 4.195962500572205 and perplexity is 66.41762782706157
At time: 524.5619070529938 and batch: 1100, loss is 4.131125268936157 and perplexity is 62.247929200505126
At time: 525.7209620475769 and batch: 1150, loss is 4.150120010375977 and perplexity is 63.44161349317088
At time: 526.8818936347961 and batch: 1200, loss is 4.155178890228272 and perplexity is 63.763370171450546
At time: 528.0419492721558 and batch: 1250, loss is 4.218088526725769 and perplexity is 67.9035643112883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.613944617501141 and perplexity of 100.88130396761825
Finished 17 epochs...
Completing Train Step...
At time: 530.9909851551056 and batch: 50, loss is 4.23810492515564 and perplexity is 69.27644331855235
At time: 532.1517436504364 and batch: 100, loss is 4.238642778396606 and perplexity is 69.31371390026696
At time: 533.317342042923 and batch: 150, loss is 4.167062654495239 and perplexity is 64.5256393743312
At time: 534.4810981750488 and batch: 200, loss is 4.225590438842773 and perplexity is 68.41488643194681
At time: 535.6706049442291 and batch: 250, loss is 4.252290620803833 and perplexity is 70.26618132275557
At time: 536.8322257995605 and batch: 300, loss is 4.24425217628479 and perplexity is 69.70361463245025
At time: 537.99547123909 and batch: 350, loss is 4.21330605506897 and perplexity is 67.57959274947048
At time: 539.1559426784515 and batch: 400, loss is 4.236136722564697 and perplexity is 69.1402273376178
At time: 540.3174607753754 and batch: 450, loss is 4.1489502668380736 and perplexity is 63.36744646241246
At time: 541.480740070343 and batch: 500, loss is 4.160232930183411 and perplexity is 64.08644852972954
At time: 542.6474130153656 and batch: 550, loss is 4.152616052627564 and perplexity is 63.600164233314636
At time: 543.813396692276 and batch: 600, loss is 4.192481083869934 and perplexity is 66.18680242106976
At time: 544.9749855995178 and batch: 650, loss is 4.20825535774231 and perplexity is 67.23912919397425
At time: 546.1364541053772 and batch: 700, loss is 4.191805362701416 and perplexity is 66.14209370461155
At time: 547.2997097969055 and batch: 750, loss is 4.189907650947571 and perplexity is 66.01669409975001
At time: 548.4623622894287 and batch: 800, loss is 4.199614849090576 and perplexity is 66.66065168540959
At time: 549.625825881958 and batch: 850, loss is 4.248719530105591 and perplexity is 70.0157019247475
At time: 550.7875406742096 and batch: 900, loss is 4.20351881980896 and perplexity is 66.92140156643428
At time: 551.9491255283356 and batch: 950, loss is 4.187404217720032 and perplexity is 65.85163241083264
At time: 553.1121289730072 and batch: 1000, loss is 4.183551025390625 and perplexity is 65.59838163126528
At time: 554.2736513614655 and batch: 1050, loss is 4.1992654037475585 and perplexity is 66.63736150067591
At time: 555.4367105960846 and batch: 1100, loss is 4.134505243301391 and perplexity is 62.458681573660904
At time: 556.5988357067108 and batch: 1150, loss is 4.1542652893066405 and perplexity is 63.70514250017272
At time: 557.7614533901215 and batch: 1200, loss is 4.159980983734131 and perplexity is 64.07030421041286
At time: 558.9247665405273 and batch: 1250, loss is 4.219307823181152 and perplexity is 67.9864093826482
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.613158734175411 and perplexity of 100.80205417757524
Finished 18 epochs...
Completing Train Step...
At time: 561.7988817691803 and batch: 50, loss is 4.229027118682861 and perplexity is 68.65041097231882
At time: 562.9812104701996 and batch: 100, loss is 4.2304287528991695 and perplexity is 68.7467012033784
At time: 564.1407470703125 and batch: 150, loss is 4.158501434326172 and perplexity is 63.97557912225461
At time: 565.3236925601959 and batch: 200, loss is 4.216436347961426 and perplexity is 67.79146811128963
At time: 566.4842569828033 and batch: 250, loss is 4.243695001602173 and perplexity is 69.66478836060249
At time: 567.6427273750305 and batch: 300, loss is 4.235926580429077 and perplexity is 69.12569958908736
At time: 568.8006508350372 and batch: 350, loss is 4.205250425338745 and perplexity is 67.03738342387997
At time: 569.9613234996796 and batch: 400, loss is 4.229385690689087 and perplexity is 68.67503150176192
At time: 571.1201100349426 and batch: 450, loss is 4.142721366882324 and perplexity is 62.97396373245999
At time: 572.2804880142212 and batch: 500, loss is 4.154395580291748 and perplexity is 63.71344324668996
At time: 573.4389140605927 and batch: 550, loss is 4.1480614185333256 and perplexity is 63.31114743940031
At time: 574.5972073078156 and batch: 600, loss is 4.187262663841247 and perplexity is 65.8423115165605
At time: 575.7565834522247 and batch: 650, loss is 4.20306972026825 and perplexity is 66.89135394341896
At time: 576.9188408851624 and batch: 700, loss is 4.1877173328399655 and perplexity is 65.87225478103223
At time: 578.0847382545471 and batch: 750, loss is 4.186268863677978 and perplexity is 65.77690992006671
At time: 579.2424228191376 and batch: 800, loss is 4.196493535041809 and perplexity is 66.4529072432911
At time: 580.4020299911499 and batch: 850, loss is 4.247579736709595 and perplexity is 69.9359439525173
At time: 581.5634517669678 and batch: 900, loss is 4.199969596862793 and perplexity is 66.68430359807316
At time: 582.7232959270477 and batch: 950, loss is 4.185266375541687 and perplexity is 65.71100238950837
At time: 583.8836154937744 and batch: 1000, loss is 4.182504000663758 and perplexity is 65.5297344475755
At time: 585.0434339046478 and batch: 1050, loss is 4.198782939910888 and perplexity is 66.605219137968
At time: 586.2068696022034 and batch: 1100, loss is 4.135147976875305 and perplexity is 62.49883876909711
At time: 587.3729372024536 and batch: 1150, loss is 4.155189108848572 and perplexity is 63.764021748448485
At time: 588.5339500904083 and batch: 1200, loss is 4.16257794380188 and perplexity is 64.23690847064694
At time: 589.7037134170532 and batch: 1250, loss is 4.2185771274566655 and perplexity is 67.9367501490934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.613257637859261 and perplexity of 100.81202436510914
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 592.6163856983185 and batch: 50, loss is 4.227112312316894 and perplexity is 68.51908450087008
At time: 593.7770965099335 and batch: 100, loss is 4.232725429534912 and perplexity is 68.90477159462228
At time: 594.9624018669128 and batch: 150, loss is 4.163101696968079 and perplexity is 64.27056156703387
At time: 596.122641324997 and batch: 200, loss is 4.219768924713135 and perplexity is 68.0177652487319
At time: 597.2830874919891 and batch: 250, loss is 4.245470571517944 and perplexity is 69.78859294232346
At time: 598.443959236145 and batch: 300, loss is 4.2366317653656 and perplexity is 69.17446318281928
At time: 599.6037917137146 and batch: 350, loss is 4.203723020553589 and perplexity is 66.93506836180318
At time: 600.7656602859497 and batch: 400, loss is 4.225675201416015 and perplexity is 68.4206856995458
At time: 601.9255149364471 and batch: 450, loss is 4.139474573135376 and perplexity is 62.769831827213885
At time: 603.0847854614258 and batch: 500, loss is 4.148695521354675 and perplexity is 63.35130594758057
At time: 604.2464673519135 and batch: 550, loss is 4.143336796760559 and perplexity is 63.012731719556996
At time: 605.4056599140167 and batch: 600, loss is 4.184238095283508 and perplexity is 65.64346779120386
At time: 606.5663020610809 and batch: 650, loss is 4.201499447822571 and perplexity is 66.78639871918197
At time: 607.7260553836823 and batch: 700, loss is 4.181989226341248 and perplexity is 65.4960101038788
At time: 608.8856451511383 and batch: 750, loss is 4.176929292678833 and perplexity is 65.16544167006633
At time: 610.0467286109924 and batch: 800, loss is 4.18520555973053 and perplexity is 65.70700624311179
At time: 611.2066605091095 and batch: 850, loss is 4.232328491210938 and perplexity is 68.87742607767255
At time: 612.368087053299 and batch: 900, loss is 4.183766889572143 and perplexity is 65.61254350069022
At time: 613.5272789001465 and batch: 950, loss is 4.169836730957031 and perplexity is 64.70488693997801
At time: 614.686539888382 and batch: 1000, loss is 4.166242618560791 and perplexity is 64.4727477208432
At time: 615.8478558063507 and batch: 1050, loss is 4.179675235748291 and perplexity is 65.34462816835811
At time: 617.0078203678131 and batch: 1100, loss is 4.114191603660584 and perplexity is 61.2027182058719
At time: 618.1685798168182 and batch: 1150, loss is 4.1307250595092775 and perplexity is 62.22302197682067
At time: 619.3282701969147 and batch: 1200, loss is 4.139466762542725 and perplexity is 62.76934155954131
At time: 620.4881932735443 and batch: 1250, loss is 4.203256158828736 and perplexity is 66.90382623377894
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.608502687328923 and perplexity of 100.3338060296786
Finished 20 epochs...
Completing Train Step...
At time: 623.3557186126709 and batch: 50, loss is 4.220076265335083 and perplexity is 68.03867308375595
At time: 624.5401997566223 and batch: 100, loss is 4.22455662727356 and perplexity is 68.34419487801627
At time: 625.6993117332458 and batch: 150, loss is 4.15547200679779 and perplexity is 63.78206301123284
At time: 626.857919216156 and batch: 200, loss is 4.211215553283691 and perplexity is 67.43846505542379
At time: 628.0176732540131 and batch: 250, loss is 4.236593976020813 and perplexity is 69.17184917457071
At time: 629.177551984787 and batch: 300, loss is 4.229147624969483 and perplexity is 68.65868427690273
At time: 630.3364381790161 and batch: 350, loss is 4.195869660377502 and perplexity is 66.41146188779035
At time: 631.4962043762207 and batch: 400, loss is 4.219098491668701 and perplexity is 67.9721791742135
At time: 632.6549556255341 and batch: 450, loss is 4.132949004173279 and perplexity is 62.36155652400903
At time: 633.813530921936 and batch: 500, loss is 4.1434812831878665 and perplexity is 63.02183686180691
At time: 634.9729459285736 and batch: 550, loss is 4.13904752254486 and perplexity is 62.743031656387174
At time: 636.1316940784454 and batch: 600, loss is 4.1803055763244625 and perplexity is 65.38583052332133
At time: 637.2904832363129 and batch: 650, loss is 4.198065028190613 and perplexity is 66.55741963048366
At time: 638.450700044632 and batch: 700, loss is 4.178802976608276 and perplexity is 65.28765557023438
At time: 639.6088318824768 and batch: 750, loss is 4.174436597824097 and perplexity is 65.00320639443163
At time: 640.7683129310608 and batch: 800, loss is 4.182705435752869 and perplexity is 65.54293576503302
At time: 641.9258852005005 and batch: 850, loss is 4.230860967636108 and perplexity is 68.77642096295077
At time: 643.0852229595184 and batch: 900, loss is 4.1831843423843384 and perplexity is 65.57433222900421
At time: 644.2442564964294 and batch: 950, loss is 4.17018569946289 and perplexity is 64.72747084800238
At time: 645.4022302627563 and batch: 1000, loss is 4.167503929138183 and perplexity is 64.55411918605411
At time: 646.5622410774231 and batch: 1050, loss is 4.181637043952942 and perplexity is 65.47294762395393
At time: 647.7202122211456 and batch: 1100, loss is 4.1167595052719115 and perplexity is 61.360082726399966
At time: 648.8791925907135 and batch: 1150, loss is 4.133312969207764 and perplexity is 62.38425808111575
At time: 650.0394670963287 and batch: 1200, loss is 4.142715034484863 and perplexity is 62.97356495755451
At time: 651.1978940963745 and batch: 1250, loss is 4.204661316871643 and perplexity is 66.99790276402993
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.607997031107436 and perplexity of 100.28308444135827
Finished 21 epochs...
Completing Train Step...
At time: 654.0720572471619 and batch: 50, loss is 4.2159561443328855 and perplexity is 67.75892221727213
At time: 655.2581036090851 and batch: 100, loss is 4.220659332275391 and perplexity is 68.07835575239532
At time: 656.4162650108337 and batch: 150, loss is 4.152056183815002 and perplexity is 63.56456645086057
At time: 657.576418876648 and batch: 200, loss is 4.207395963668823 and perplexity is 67.18136910773136
At time: 658.7343671321869 and batch: 250, loss is 4.2322110414505 and perplexity is 68.86933691552457
At time: 659.8927941322327 and batch: 300, loss is 4.2251912879943845 and perplexity is 68.38758402123777
At time: 661.0536484718323 and batch: 350, loss is 4.19191888332367 and perplexity is 66.1496026224465
At time: 662.2122843265533 and batch: 400, loss is 4.215568428039551 and perplexity is 67.7326560713451
At time: 663.3720061779022 and batch: 450, loss is 4.129661793708801 and perplexity is 62.15689752571382
At time: 664.5296297073364 and batch: 500, loss is 4.140735292434693 and perplexity is 62.849016870181096
At time: 665.6889064311981 and batch: 550, loss is 4.13680242061615 and perplexity is 62.60232516431305
At time: 666.8488900661469 and batch: 600, loss is 4.178266878128052 and perplexity is 65.25266433751715
At time: 668.007630109787 and batch: 650, loss is 4.1961432504653935 and perplexity is 66.42963389121188
At time: 669.1673691272736 and batch: 700, loss is 4.177152428627014 and perplexity is 65.17998404508505
At time: 670.3257415294647 and batch: 750, loss is 4.172690281867981 and perplexity is 64.88978931775492
At time: 671.4843001365662 and batch: 800, loss is 4.181570806503296 and perplexity is 65.46861100650733
At time: 672.6448335647583 and batch: 850, loss is 4.229787826538086 and perplexity is 68.70265374743528
At time: 673.801923751831 and batch: 900, loss is 4.182431511878967 and perplexity is 65.52498444892063
At time: 674.9604935646057 and batch: 950, loss is 4.170089821815491 and perplexity is 64.72126522787026
At time: 676.1197865009308 and batch: 1000, loss is 4.167745132446289 and perplexity is 64.56969173115282
At time: 677.2856531143188 and batch: 1050, loss is 4.182476258277893 and perplexity is 65.52791652161375
At time: 678.4446730613708 and batch: 1100, loss is 4.117810831069947 and perplexity is 61.424626086444654
At time: 679.6032602787018 and batch: 1150, loss is 4.134687461853027 and perplexity is 62.470063741143896
At time: 680.7625524997711 and batch: 1200, loss is 4.143882837295532 and perplexity is 63.04714862095171
At time: 681.9458248615265 and batch: 1250, loss is 4.204714946746826 and perplexity is 67.00149594954284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.608045591925182 and perplexity of 100.28795438818824
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 684.8412606716156 and batch: 50, loss is 4.214776659011841 and perplexity is 67.679048677241
At time: 686.0037939548492 and batch: 100, loss is 4.221605167388916 and perplexity is 68.14277711292682
At time: 687.1733818054199 and batch: 150, loss is 4.15468765258789 and perplexity is 63.73205489619546
At time: 688.3472392559052 and batch: 200, loss is 4.214006519317627 and perplexity is 67.62694642097105
At time: 689.5176916122437 and batch: 250, loss is 4.2411372137069705 and perplexity is 69.48682829728179
At time: 690.68048787117 and batch: 300, loss is 4.2343441772460935 and perplexity is 69.0164013617699
At time: 691.8432252407074 and batch: 350, loss is 4.193996806144714 and perplexity is 66.28719929946719
At time: 693.0042264461517 and batch: 400, loss is 4.216794595718384 and perplexity is 67.81575860342879
At time: 694.1661243438721 and batch: 450, loss is 4.1291143369674685 and perplexity is 62.122878625930696
At time: 695.3290376663208 and batch: 500, loss is 4.139773368835449 and perplexity is 62.788589985347095
At time: 696.4902489185333 and batch: 550, loss is 4.135736713409424 and perplexity is 62.535644952329726
At time: 697.6526176929474 and batch: 600, loss is 4.178686423301697 and perplexity is 65.28004652153734
At time: 698.8139667510986 and batch: 650, loss is 4.198384275436402 and perplexity is 66.57867129547475
At time: 699.9764087200165 and batch: 700, loss is 4.178845801353455 and perplexity is 65.29045155731576
At time: 701.139098405838 and batch: 750, loss is 4.1732904958724975 and perplexity is 64.92874876888804
At time: 702.3007254600525 and batch: 800, loss is 4.179786968231201 and perplexity is 65.35192969380941
At time: 703.4636807441711 and batch: 850, loss is 4.22478684425354 and perplexity is 68.35993068341554
At time: 704.625052690506 and batch: 900, loss is 4.174518456459046 and perplexity is 65.00852768596823
At time: 705.7874698638916 and batch: 950, loss is 4.161668539047241 and perplexity is 64.17851767511816
At time: 706.9495432376862 and batch: 1000, loss is 4.159467258453369 and perplexity is 64.03739812846068
At time: 708.1111152172089 and batch: 1050, loss is 4.1738596534729 and perplexity is 64.96571397825154
At time: 709.2744307518005 and batch: 1100, loss is 4.106708621978759 and perplexity is 60.74644864411224
At time: 710.4353482723236 and batch: 1150, loss is 4.123301491737366 and perplexity is 61.76281545681965
At time: 711.622704744339 and batch: 1200, loss is 4.133500347137451 and perplexity is 62.395948609479355
At time: 712.787406206131 and batch: 1250, loss is 4.1979718542099 and perplexity is 66.55121849964722
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.606850728501368 and perplexity of 100.16819554162166
Finished 23 epochs...
Completing Train Step...
At time: 715.6671602725983 and batch: 50, loss is 4.21330867767334 and perplexity is 67.57976998423817
At time: 716.860089302063 and batch: 100, loss is 4.219612655639648 and perplexity is 68.0071370060303
At time: 718.0236537456512 and batch: 150, loss is 4.152192392349243 and perplexity is 63.5732250769625
At time: 719.1828274726868 and batch: 200, loss is 4.210536460876465 and perplexity is 67.39268365251066
At time: 720.3406798839569 and batch: 250, loss is 4.237562804222107 and perplexity is 69.23889728659317
At time: 721.4991598129272 and batch: 300, loss is 4.230643911361694 and perplexity is 68.76149422927824
At time: 722.6572268009186 and batch: 350, loss is 4.19060447216034 and perplexity is 66.06271196384546
At time: 723.8167684078217 and batch: 400, loss is 4.213722848892212 and perplexity is 67.60776537698642
At time: 724.9750862121582 and batch: 450, loss is 4.12597240447998 and perplexity is 61.92799904491963
At time: 726.134932756424 and batch: 500, loss is 4.136954255104065 and perplexity is 62.61183107794116
At time: 727.2931327819824 and batch: 550, loss is 4.133508067131043 and perplexity is 62.39643030766213
At time: 728.4514055252075 and batch: 600, loss is 4.177556738853455 and perplexity is 65.20634230729213
At time: 729.610321521759 and batch: 650, loss is 4.197427225112915 and perplexity is 66.51498263806049
At time: 730.7680284976959 and batch: 700, loss is 4.178291578292846 and perplexity is 65.25427610898498
At time: 731.9269731044769 and batch: 750, loss is 4.173368468284607 and perplexity is 64.93381161742347
At time: 733.0848128795624 and batch: 800, loss is 4.180571756362915 and perplexity is 65.40323724276213
At time: 734.2435491085052 and batch: 850, loss is 4.2253166198730465 and perplexity is 68.39615570276156
At time: 735.4023294448853 and batch: 900, loss is 4.17466245174408 and perplexity is 65.01788928143867
At time: 736.5604665279388 and batch: 950, loss is 4.161960949897766 and perplexity is 64.19728691408781
At time: 737.7204692363739 and batch: 1000, loss is 4.159923753738403 and perplexity is 64.0666375720985
At time: 738.8788707256317 and batch: 1050, loss is 4.1744421863555905 and perplexity is 65.00356966791284
At time: 740.0376362800598 and batch: 1100, loss is 4.107385358810425 and perplexity is 60.78757191654969
At time: 741.2224736213684 and batch: 1150, loss is 4.12440824508667 and perplexity is 61.83120950037598
At time: 742.3807933330536 and batch: 1200, loss is 4.1344508934021 and perplexity is 62.45528704285454
At time: 743.5391314029694 and batch: 1250, loss is 4.1981777095794675 and perplexity is 66.56491983552502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.606614607093978 and perplexity of 100.14454647844957
Finished 24 epochs...
Completing Train Step...
At time: 746.4340238571167 and batch: 50, loss is 4.211396036148071 and perplexity is 67.4506376412049
At time: 747.5977709293365 and batch: 100, loss is 4.217931785583496 and perplexity is 67.89292186313031
At time: 748.7588596343994 and batch: 150, loss is 4.150774941444397 and perplexity is 63.483176986004
At time: 749.9198145866394 and batch: 200, loss is 4.208909482955932 and perplexity is 67.28312639199153
At time: 751.0830523967743 and batch: 250, loss is 4.236094818115235 and perplexity is 69.1373301151592
At time: 752.2443928718567 and batch: 300, loss is 4.2293753242492675 and perplexity is 68.67431958987076
At time: 753.4063518047333 and batch: 350, loss is 4.189016618728638 and perplexity is 65.95789729713249
At time: 754.5688331127167 and batch: 400, loss is 4.212106113433838 and perplexity is 67.49854981556463
At time: 755.7299346923828 and batch: 450, loss is 4.1242298936843875 and perplexity is 61.820182800799664
At time: 756.8918318748474 and batch: 500, loss is 4.135463557243347 and perplexity is 62.51856528812542
At time: 758.054648399353 and batch: 550, loss is 4.132436170578003 and perplexity is 62.32958362185973
At time: 759.2163078784943 and batch: 600, loss is 4.176953296661377 and perplexity is 65.16700591896473
At time: 760.3785371780396 and batch: 650, loss is 4.1969939661026 and perplexity is 66.48617066448585
At time: 761.5418426990509 and batch: 700, loss is 4.178166055679322 and perplexity is 65.24608573575341
At time: 762.7034201622009 and batch: 750, loss is 4.1735432338714595 and perplexity is 64.9451608048122
At time: 763.8642942905426 and batch: 800, loss is 4.1810568857193 and perplexity is 65.43497397073796
At time: 765.0268714427948 and batch: 850, loss is 4.225749187469482 and perplexity is 68.42574806332605
At time: 766.1882832050323 and batch: 900, loss is 4.174643740653992 and perplexity is 65.01667273723646
At time: 767.3491907119751 and batch: 950, loss is 4.161880722045899 and perplexity is 64.19213671026063
At time: 768.5133969783783 and batch: 1000, loss is 4.16005877494812 and perplexity is 64.07528851102295
At time: 769.6749844551086 and batch: 1050, loss is 4.1746167469024655 and perplexity is 65.01491771701491
At time: 770.8619163036346 and batch: 1100, loss is 4.107669863700867 and perplexity is 60.80486873843658
At time: 772.030023097992 and batch: 1150, loss is 4.12488851070404 and perplexity is 61.8609120363619
At time: 773.1930887699127 and batch: 1200, loss is 4.134801635742187 and perplexity is 62.47719659846257
At time: 774.355167388916 and batch: 1250, loss is 4.197963781356812 and perplexity is 66.55068124360602
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.606545552720118 and perplexity of 100.13763129826148
Finished 25 epochs...
Completing Train Step...
At time: 777.2235200405121 and batch: 50, loss is 4.209760894775391 and perplexity is 67.3404364348137
At time: 778.4085216522217 and batch: 100, loss is 4.216520204544067 and perplexity is 67.79715311049667
At time: 779.5674314498901 and batch: 150, loss is 4.149178791046142 and perplexity is 63.381929112687686
At time: 780.7253756523132 and batch: 200, loss is 4.20746319770813 and perplexity is 67.18588613438989
At time: 781.8867948055267 and batch: 250, loss is 4.2351244974136355 and perplexity is 69.07027726913844
At time: 783.0481863021851 and batch: 300, loss is 4.228552665710449 and perplexity is 68.61784730635061
At time: 784.2064099311829 and batch: 350, loss is 4.187803997993469 and perplexity is 65.87796385748986
At time: 785.3663504123688 and batch: 400, loss is 4.210838327407837 and perplexity is 67.41303031899896
At time: 786.5251262187958 and batch: 450, loss is 4.122949833869934 and perplexity is 61.74109989529734
At time: 787.6855401992798 and batch: 500, loss is 4.134356937408447 and perplexity is 62.4494192699621
At time: 788.8441867828369 and batch: 550, loss is 4.131666851043701 and perplexity is 62.281650695846835
At time: 790.0032618045807 and batch: 600, loss is 4.1765552473068235 and perplexity is 65.14107139627292
At time: 791.1626114845276 and batch: 650, loss is 4.196632905006409 and perplexity is 66.46216942804129
At time: 792.3212659358978 and batch: 700, loss is 4.17795627117157 and perplexity is 65.2323995533993
At time: 793.4820823669434 and batch: 750, loss is 4.173695330619812 and perplexity is 64.95503950383178
At time: 794.640837430954 and batch: 800, loss is 4.1813237810134884 and perplexity is 65.45244058814143
At time: 795.7997405529022 and batch: 850, loss is 4.225964488983155 and perplexity is 68.44048181650102
At time: 796.9595468044281 and batch: 900, loss is 4.174551029205322 and perplexity is 65.0106452267332
At time: 798.1167838573456 and batch: 950, loss is 4.161701817512512 and perplexity is 64.18065347322758
At time: 799.3069276809692 and batch: 1000, loss is 4.160152292251587 and perplexity is 64.08128093941684
At time: 800.4656300544739 and batch: 1050, loss is 4.174707579612732 and perplexity is 65.02082346641244
At time: 801.6242356300354 and batch: 1100, loss is 4.107715721130371 and perplexity is 60.80765715735265
At time: 802.7836201190948 and batch: 1150, loss is 4.125090579986573 and perplexity is 61.87341348951119
At time: 803.9420857429504 and batch: 1200, loss is 4.134956655502319 and perplexity is 62.48688254923045
At time: 805.0998995304108 and batch: 1250, loss is 4.197675948143005 and perplexity is 66.53152850367223
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.606535305942062 and perplexity of 100.13660521543551
Finished 26 epochs...
Completing Train Step...
At time: 807.9948978424072 and batch: 50, loss is 4.208336400985718 and perplexity is 67.24457869190755
At time: 809.1564018726349 and batch: 100, loss is 4.215228309631348 and perplexity is 67.70962286540947
At time: 810.3168661594391 and batch: 150, loss is 4.148219699859619 and perplexity is 63.32116920489443
At time: 811.4785025119781 and batch: 200, loss is 4.206015739440918 and perplexity is 67.0887077158842
At time: 812.6421213150024 and batch: 250, loss is 4.234369416236877 and perplexity is 69.01814328806991
At time: 813.8062951564789 and batch: 300, loss is 4.227983131408691 and perplexity is 68.57877821524667
At time: 814.9686019420624 and batch: 350, loss is 4.186765265464783 and perplexity is 65.80956980122257
At time: 816.1333918571472 and batch: 400, loss is 4.209828071594238 and perplexity is 67.344960303061
At time: 817.2956278324127 and batch: 450, loss is 4.121704216003418 and perplexity is 61.664241955918136
At time: 818.4671630859375 and batch: 500, loss is 4.1333883142471315 and perplexity is 62.38895860257506
At time: 819.6302661895752 and batch: 550, loss is 4.131221284866333 and perplexity is 62.253906280271714
At time: 820.7894766330719 and batch: 600, loss is 4.176171779632568 and perplexity is 65.11609668992841
At time: 821.9485890865326 and batch: 650, loss is 4.196263542175293 and perplexity is 66.43762530610151
At time: 823.1090204715729 and batch: 700, loss is 4.177755951881409 and perplexity is 65.2193335541548
At time: 824.2683923244476 and batch: 750, loss is 4.173669652938843 and perplexity is 64.95337163046374
At time: 825.4304294586182 and batch: 800, loss is 4.18142725944519 and perplexity is 65.45921385448197
At time: 826.5905883312225 and batch: 850, loss is 4.22599313735962 and perplexity is 68.44244255327537
At time: 827.750417470932 and batch: 900, loss is 4.174370999336243 and perplexity is 64.99894242224309
At time: 828.9374117851257 and batch: 950, loss is 4.161503500938416 and perplexity is 64.16792664791943
At time: 830.0982117652893 and batch: 1000, loss is 4.160006985664368 and perplexity is 64.0719701836524
At time: 831.2582769393921 and batch: 1050, loss is 4.174724340438843 and perplexity is 65.0219132782612
At time: 832.4211804866791 and batch: 1100, loss is 4.107769412994385 and perplexity is 60.81092212146194
At time: 833.5814423561096 and batch: 1150, loss is 4.125108404159546 and perplexity is 61.87451634176435
At time: 834.7412846088409 and batch: 1200, loss is 4.1349797487258915 and perplexity is 62.48832558944167
At time: 835.9017674922943 and batch: 1250, loss is 4.197382926940918 and perplexity is 66.51203621118034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.606483626539689 and perplexity of 100.1314303492405
Finished 27 epochs...
Completing Train Step...
At time: 838.8066339492798 and batch: 50, loss is 4.206989507675171 and perplexity is 67.1540683862426
At time: 839.9653992652893 and batch: 100, loss is 4.214049654006958 and perplexity is 67.62986355120965
At time: 841.1240317821503 and batch: 150, loss is 4.1469385004043575 and perplexity is 63.240094105179594
At time: 842.2832183837891 and batch: 200, loss is 4.205005044937134 and perplexity is 67.02093578185949
At time: 843.441445350647 and batch: 250, loss is 4.234071545600891 and perplexity is 68.99758787141361
At time: 844.6004786491394 and batch: 300, loss is 4.2279222965240475 and perplexity is 68.57460636008341
At time: 845.760228395462 and batch: 350, loss is 4.185867509841919 and perplexity is 65.75051540207102
At time: 846.9195685386658 and batch: 400, loss is 4.208903665542603 and perplexity is 67.2827349793737
At time: 848.0784721374512 and batch: 450, loss is 4.120596542358398 and perplexity is 61.595975915490214
At time: 849.2477893829346 and batch: 500, loss is 4.1323650932312015 and perplexity is 62.325153557869136
At time: 850.4308695793152 and batch: 550, loss is 4.130406680107117 and perplexity is 62.203214601580534
At time: 851.6038153171539 and batch: 600, loss is 4.175729517936706 and perplexity is 65.08730470184446
At time: 852.7813897132874 and batch: 650, loss is 4.195771002769471 and perplexity is 66.40491021500611
At time: 853.9395697116852 and batch: 700, loss is 4.177446665763855 and perplexity is 65.19916523874119
At time: 855.1081762313843 and batch: 750, loss is 4.1734020090103146 and perplexity is 64.93598958111315
At time: 856.2725808620453 and batch: 800, loss is 4.181297240257263 and perplexity is 65.45070345392394
At time: 857.4555594921112 and batch: 850, loss is 4.2258298969268795 and perplexity is 68.43127089119348
At time: 858.6770932674408 and batch: 900, loss is 4.174091143608093 and perplexity is 64.98075464097865
At time: 859.867537021637 and batch: 950, loss is 4.161249151229859 and perplexity is 64.15160762993453
At time: 861.0725457668304 and batch: 1000, loss is 4.1598359394073485 and perplexity is 64.0610118501903
At time: 862.2868988513947 and batch: 1050, loss is 4.17462073802948 and perplexity is 65.01517720032717
At time: 863.4680306911469 and batch: 1100, loss is 4.107809324264526 and perplexity is 60.81334921103606
At time: 864.6592240333557 and batch: 1150, loss is 4.12499167919159 and perplexity is 61.86729446232208
At time: 865.8393306732178 and batch: 1200, loss is 4.134998798370361 and perplexity is 62.4895159811659
At time: 867.0198483467102 and batch: 1250, loss is 4.197111592292786 and perplexity is 66.49399163940791
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.606482735515511 and perplexity of 100.13134112975483
Finished 28 epochs...
Completing Train Step...
At time: 870.070175409317 and batch: 50, loss is 4.205775489807129 and perplexity is 67.07259161444533
At time: 871.276805639267 and batch: 100, loss is 4.213040733337403 and perplexity is 67.56166479334694
At time: 872.4571137428284 and batch: 150, loss is 4.145980820655823 and perplexity is 63.179559338846175
At time: 873.6435191631317 and batch: 200, loss is 4.2046452903747555 and perplexity is 66.9968290309539
At time: 874.8213946819305 and batch: 250, loss is 4.233186650276184 and perplexity is 68.93655923445023
At time: 875.998437166214 and batch: 300, loss is 4.227075214385986 and perplexity is 68.51654263175674
At time: 877.1764626502991 and batch: 350, loss is 4.18491485118866 and perplexity is 65.68790743136456
At time: 878.3540403842926 and batch: 400, loss is 4.20788727760315 and perplexity is 67.21438436026382
At time: 879.5347309112549 and batch: 450, loss is 4.119605317115783 and perplexity is 61.534950679193194
At time: 880.7154161930084 and batch: 500, loss is 4.131518411636352 and perplexity is 62.27240633065985
At time: 881.894054889679 and batch: 550, loss is 4.12998815536499 and perplexity is 62.17718646431959
At time: 883.073397397995 and batch: 600, loss is 4.175436654090881 and perplexity is 65.0682457744467
At time: 884.249710559845 and batch: 650, loss is 4.195486359596252 and perplexity is 66.38601120051057
At time: 885.4271450042725 and batch: 700, loss is 4.177181830406189 and perplexity is 65.18190048075573
At time: 886.6060130596161 and batch: 750, loss is 4.173291964530945 and perplexity is 64.92884412711342
At time: 887.7854816913605 and batch: 800, loss is 4.181330027580262 and perplexity is 65.452849442459
At time: 888.9905848503113 and batch: 850, loss is 4.225784769058228 and perplexity is 68.42818280346896
At time: 890.1696219444275 and batch: 900, loss is 4.173895506858826 and perplexity is 64.96804326082284
At time: 891.3504934310913 and batch: 950, loss is 4.1610316467285156 and perplexity is 64.13765588384541
At time: 892.5281136035919 and batch: 1000, loss is 4.159685049057007 and perplexity is 64.0513463908996
At time: 893.7033793926239 and batch: 1050, loss is 4.1744347047805785 and perplexity is 65.00308334064957
At time: 894.878844499588 and batch: 1100, loss is 4.107661910057068 and perplexity is 60.804385120092675
At time: 896.0554933547974 and batch: 1150, loss is 4.124891695976257 and perplexity is 61.86110908052013
At time: 897.2325909137726 and batch: 1200, loss is 4.134975504875183 and perplexity is 62.488060398879576
At time: 898.403050661087 and batch: 1250, loss is 4.196781725883484 and perplexity is 66.47206112241494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.606513030337592 and perplexity of 100.13437463686857
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 901.3942835330963 and batch: 50, loss is 4.205475902557373 and perplexity is 67.05250053085604
At time: 902.5604197978973 and batch: 100, loss is 4.214102048873901 and perplexity is 67.6334071017429
At time: 903.7251877784729 and batch: 150, loss is 4.14791259765625 and perplexity is 63.30172611997159
At time: 904.8909597396851 and batch: 200, loss is 4.206663732528686 and perplexity is 67.13219482290211
At time: 906.0552237033844 and batch: 250, loss is 4.237899508476257 and perplexity is 69.26221424310116
At time: 907.2197675704956 and batch: 300, loss is 4.235638675689697 and perplexity is 69.10580083717261
At time: 908.3875460624695 and batch: 350, loss is 4.193495545387268 and perplexity is 66.25398045408473
At time: 909.5524055957794 and batch: 400, loss is 4.2204385471344 and perplexity is 68.06332672217643
At time: 910.7187411785126 and batch: 450, loss is 4.129272317886352 and perplexity is 62.13269363065103
At time: 911.8907403945923 and batch: 500, loss is 4.137323136329651 and perplexity is 62.63493166735028
At time: 913.0691401958466 and batch: 550, loss is 4.132937231063843 and perplexity is 62.36082233890132
At time: 914.2488451004028 and batch: 600, loss is 4.174365487098694 and perplexity is 64.9985841336195
At time: 915.4284393787384 and batch: 650, loss is 4.192992115020752 and perplexity is 66.22063458278797
At time: 916.6195712089539 and batch: 700, loss is 4.176066422462464 and perplexity is 65.10923660363889
At time: 917.8072757720947 and batch: 750, loss is 4.170789322853088 and perplexity is 64.766553657857
At time: 919.0200371742249 and batch: 800, loss is 4.180226035118103 and perplexity is 65.38062986231888
At time: 920.2022888660431 and batch: 850, loss is 4.224676885604858 and perplexity is 68.35241433106518
At time: 921.3845062255859 and batch: 900, loss is 4.170833005905151 and perplexity is 64.76938292038733
At time: 922.5675015449524 and batch: 950, loss is 4.157016444206238 and perplexity is 63.88064652377181
At time: 923.7577350139618 and batch: 1000, loss is 4.155396537780762 and perplexity is 63.77724962326644
At time: 924.9398350715637 and batch: 1050, loss is 4.170029706954956 and perplexity is 64.71737463497985
At time: 926.1193664073944 and batch: 1100, loss is 4.104174327850342 and perplexity is 60.59269418779173
At time: 927.2999935150146 and batch: 1150, loss is 4.120491795539856 and perplexity is 61.58952427087857
At time: 928.4830198287964 and batch: 1200, loss is 4.1301499938964845 and perplexity is 62.18724994317678
At time: 929.6663908958435 and batch: 1250, loss is 4.1936038923263546 and perplexity is 66.26115925896296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.6064208093350825 and perplexity of 100.12514057024791
Finished 30 epochs...
Completing Train Step...
At time: 932.6646914482117 and batch: 50, loss is 4.204255418777466 and perplexity is 66.97071396129903
At time: 933.868616104126 and batch: 100, loss is 4.212555541992187 and perplexity is 67.52889240940229
At time: 935.0481553077698 and batch: 150, loss is 4.145961155891419 and perplexity is 63.178316939912385
At time: 936.2287809848785 and batch: 200, loss is 4.203925666809082 and perplexity is 66.94863387722185
At time: 937.4074974060059 and batch: 250, loss is 4.235224604606628 and perplexity is 69.07719204681881
At time: 938.5869562625885 and batch: 300, loss is 4.232290458679199 and perplexity is 68.87480654459326
At time: 939.764092206955 and batch: 350, loss is 4.190452318191529 and perplexity is 66.05266102469427
At time: 940.9415895938873 and batch: 400, loss is 4.215990839004516 and perplexity is 67.76127313161038
At time: 942.1199517250061 and batch: 450, loss is 4.12436152935028 and perplexity is 61.82832107736024
At time: 943.2973601818085 and batch: 500, loss is 4.133544611930847 and perplexity is 62.39871061438264
At time: 944.47407579422 and batch: 550, loss is 4.129436507225036 and perplexity is 62.14289599406565
At time: 945.6519451141357 and batch: 600, loss is 4.173078227043152 and perplexity is 64.91496788207408
At time: 946.8301947116852 and batch: 650, loss is 4.193544940948486 and perplexity is 66.25725318746079
At time: 948.0089185237885 and batch: 700, loss is 4.176463975906372 and perplexity is 65.13512615077921
At time: 949.2149178981781 and batch: 750, loss is 4.171790242195129 and perplexity is 64.83141220780425
At time: 950.3959131240845 and batch: 800, loss is 4.181193423271179 and perplexity is 65.44390891185485
At time: 951.576858997345 and batch: 850, loss is 4.225907316207886 and perplexity is 68.43656899606931
At time: 952.7549524307251 and batch: 900, loss is 4.1717649269104005 and perplexity is 64.82977100291869
At time: 953.9317548274994 and batch: 950, loss is 4.157555890083313 and perplexity is 63.91511597152883
At time: 955.1091351509094 and batch: 1000, loss is 4.155726389884949 and perplexity is 63.7982901531947
At time: 956.2868287563324 and batch: 1050, loss is 4.170283274650574 and perplexity is 64.73378695126112
At time: 957.4643590450287 and batch: 1100, loss is 4.104879379272461 and perplexity is 60.63543021677753
At time: 958.6422121524811 and batch: 1150, loss is 4.121170969009399 and perplexity is 61.631368449884796
At time: 959.81791639328 and batch: 1200, loss is 4.130452084541321 and perplexity is 62.20603896746404
At time: 960.9942407608032 and batch: 1250, loss is 4.193637294769287 and perplexity is 66.26337258051872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.60627779995438 and perplexity of 100.11082275971718
Finished 31 epochs...
Completing Train Step...
At time: 963.9992580413818 and batch: 50, loss is 4.2032912158966065 and perplexity is 66.90617172686885
At time: 965.1796817779541 and batch: 100, loss is 4.211603126525879 and perplexity is 67.4646074656956
At time: 966.3687651157379 and batch: 150, loss is 4.144863457679748 and perplexity is 63.10900426355135
At time: 967.5491900444031 and batch: 200, loss is 4.2029585266113285 and perplexity is 66.8839164626656
At time: 968.7319052219391 and batch: 250, loss is 4.234134869575501 and perplexity is 69.00195721125664
At time: 969.9104595184326 and batch: 300, loss is 4.2312045001983645 and perplexity is 68.8000519618434
At time: 971.0740547180176 and batch: 350, loss is 4.189316220283509 and perplexity is 65.97766134624081
At time: 972.2400619983673 and batch: 400, loss is 4.21508264541626 and perplexity is 67.69976071463954
At time: 973.4060640335083 and batch: 450, loss is 4.123477735519409 and perplexity is 61.77370172829605
At time: 974.571319103241 and batch: 500, loss is 4.132756571769715 and perplexity is 62.34955729435447
At time: 975.7382917404175 and batch: 550, loss is 4.128928880691529 and perplexity is 62.11135861647211
At time: 976.9035491943359 and batch: 600, loss is 4.172796578407287 and perplexity is 64.8966872444021
At time: 978.1043395996094 and batch: 650, loss is 4.193528623580932 and perplexity is 66.25617205232807
At time: 979.2668876647949 and batch: 700, loss is 4.176433820724487 and perplexity is 65.13316201881757
At time: 980.4293639659882 and batch: 750, loss is 4.1719457626342775 and perplexity is 64.84149560156798
At time: 981.5915348529816 and batch: 800, loss is 4.181432461738586 and perplexity is 65.4595543934037
At time: 982.7537784576416 and batch: 850, loss is 4.22629487991333 and perplexity is 68.4630976667802
At time: 983.9347453117371 and batch: 900, loss is 4.172103133201599 and perplexity is 64.85170054747478
At time: 985.1152651309967 and batch: 950, loss is 4.157729253768921 and perplexity is 63.92619749213859
At time: 986.2969491481781 and batch: 1000, loss is 4.155781679153442 and perplexity is 63.80181761150279
At time: 987.4772667884827 and batch: 1050, loss is 4.170368428230286 and perplexity is 64.73929949965162
At time: 988.6536960601807 and batch: 1100, loss is 4.105169215202332 and perplexity is 60.65300709015919
At time: 989.8271889686584 and batch: 1150, loss is 4.1214447641372685 and perplexity is 61.64824512856106
At time: 990.9994525909424 and batch: 1200, loss is 4.130543632507324 and perplexity is 62.21173406448795
At time: 992.1721277236938 and batch: 1250, loss is 4.1935603809356685 and perplexity is 66.25827620649841
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.6062377038663325 and perplexity of 100.10680878782617
Finished 32 epochs...
Completing Train Step...
At time: 995.0917980670929 and batch: 50, loss is 4.202517900466919 and perplexity is 66.8544521522811
At time: 996.2956562042236 and batch: 100, loss is 4.210894975662232 and perplexity is 67.41684925765708
At time: 997.4750535488129 and batch: 150, loss is 4.144002170562744 and perplexity is 63.054672692112106
At time: 998.6545186042786 and batch: 200, loss is 4.202019309997558 and perplexity is 66.82112746797898
At time: 999.8328311443329 and batch: 250, loss is 4.23347092628479 and perplexity is 68.95615903010027
At time: 1001.0094861984253 and batch: 300, loss is 4.2306383609771725 and perplexity is 68.76111257760414
At time: 1002.1869950294495 and batch: 350, loss is 4.188891968727112 and perplexity is 65.94967615752694
At time: 1003.366598367691 and batch: 400, loss is 4.214772682189942 and perplexity is 67.67877953025327
At time: 1004.5461950302124 and batch: 450, loss is 4.123100943565369 and perplexity is 61.75043027903629
At time: 1005.7258787155151 and batch: 500, loss is 4.13232834815979 and perplexity is 62.322863457726136
At time: 1006.903814792633 and batch: 550, loss is 4.128535137176514 and perplexity is 62.086907485861076
At time: 1008.1078171730042 and batch: 600, loss is 4.172538728713989 and perplexity is 64.87995581069028
At time: 1009.2854068279266 and batch: 650, loss is 4.193431596755982 and perplexity is 66.24974373818496
At time: 1010.4649021625519 and batch: 700, loss is 4.176323194503784 and perplexity is 65.12595698180135
At time: 1011.6456208229065 and batch: 750, loss is 4.1719479608535766 and perplexity is 64.84163813755166
At time: 1012.8266742229462 and batch: 800, loss is 4.1814818811416625 and perplexity is 65.46278944544399
At time: 1014.0055499076843 and batch: 850, loss is 4.226466665267944 and perplexity is 68.47485963452885
At time: 1015.1832518577576 and batch: 900, loss is 4.172285437583923 and perplexity is 64.86352437442076
At time: 1016.3598394393921 and batch: 950, loss is 4.157829713821411 and perplexity is 63.93261984388364
At time: 1017.5373063087463 and batch: 1000, loss is 4.155791172981262 and perplexity is 63.80242333784912
At time: 1018.7156934738159 and batch: 1050, loss is 4.170403537750244 and perplexity is 64.74157250528131
At time: 1019.8928718566895 and batch: 1100, loss is 4.105309844017029 and perplexity is 60.661537250433206
At time: 1021.0695869922638 and batch: 1150, loss is 4.12157986164093 and perplexity is 61.65657421518977
At time: 1022.2473230361938 and batch: 1200, loss is 4.130567083358764 and perplexity is 62.21319299972791
At time: 1023.4318313598633 and batch: 1250, loss is 4.1934507369995115 and perplexity is 66.25101178654923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.606223447479471 and perplexity of 100.10538163660567
Finished 33 epochs...
Completing Train Step...
At time: 1026.4398596286774 and batch: 50, loss is 4.201851162910462 and perplexity is 66.80989263461746
At time: 1027.6164453029633 and batch: 100, loss is 4.210282716751099 and perplexity is 67.37558532433304
At time: 1028.7940618991852 and batch: 150, loss is 4.143321504592896 and perplexity is 63.01176812566633
At time: 1029.9723055362701 and batch: 200, loss is 4.201440267562866 and perplexity is 66.7824463997118
At time: 1031.1517760753632 and batch: 250, loss is 4.232960209846497 and perplexity is 68.92095097759292
At time: 1032.331419467926 and batch: 300, loss is 4.23021653175354 and perplexity is 68.73211324768187
At time: 1033.511162519455 and batch: 350, loss is 4.188617253303528 and perplexity is 65.93156125264193
At time: 1034.6906707286835 and batch: 400, loss is 4.214605712890625 and perplexity is 67.66748019520378
At time: 1035.869307756424 and batch: 450, loss is 4.12285276889801 and perplexity is 61.73510728801027
At time: 1037.0490112304688 and batch: 500, loss is 4.131996855735779 and perplexity is 62.30220732451222
At time: 1038.2550084590912 and batch: 550, loss is 4.128202457427978 and perplexity is 62.06625586446988
At time: 1039.4325301647186 and batch: 600, loss is 4.172294564247132 and perplexity is 64.86411636466367
At time: 1040.6118803024292 and batch: 650, loss is 4.193299584388733 and perplexity is 66.24099852993494
At time: 1041.7910373210907 and batch: 700, loss is 4.176190361976624 and perplexity is 65.11730671088314
At time: 1042.9715313911438 and batch: 750, loss is 4.171895818710327 and perplexity is 64.83825724371165
At time: 1044.1537981033325 and batch: 800, loss is 4.181494708061218 and perplexity is 65.46362913676343
At time: 1045.331106185913 and batch: 850, loss is 4.226562652587891 and perplexity is 68.48143266824779
At time: 1046.5089151859283 and batch: 900, loss is 4.172401418685913 and perplexity is 64.87104775373314
At time: 1047.6867606639862 and batch: 950, loss is 4.157882494926453 and perplexity is 63.935994367261934
At time: 1048.8651435375214 and batch: 1000, loss is 4.155782027244568 and perplexity is 63.80183982035317
At time: 1050.045222043991 and batch: 1050, loss is 4.170411319732666 and perplexity is 64.74207632502088
At time: 1051.2290732860565 and batch: 1100, loss is 4.1053836107254025 and perplexity is 60.666012217410824
At time: 1052.4087953567505 and batch: 1150, loss is 4.121651577949524 and perplexity is 61.66099615565376
At time: 1053.5854542255402 and batch: 1200, loss is 4.130562524795533 and perplexity is 62.21290939760017
At time: 1054.7621929645538 and batch: 1250, loss is 4.193330645561218 and perplexity is 66.24305608497077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.606209191092609 and perplexity of 100.10395450573097
Finished 34 epochs...
Completing Train Step...
At time: 1057.7628047466278 and batch: 50, loss is 4.201246099472046 and perplexity is 66.76948063840338
At time: 1058.9401602745056 and batch: 100, loss is 4.209729709625244 and perplexity is 67.33833644593693
At time: 1060.1219601631165 and batch: 150, loss is 4.142736668586731 and perplexity is 62.97492734881081
At time: 1061.2969553470612 and batch: 200, loss is 4.201074895858764 and perplexity is 66.75805044053465
At time: 1062.474949836731 and batch: 250, loss is 4.232508826255798 and perplexity is 68.8898482114335
At time: 1063.6541435718536 and batch: 300, loss is 4.229856605529785 and perplexity is 68.70737920919147
At time: 1064.8354740142822 and batch: 350, loss is 4.188371677398681 and perplexity is 65.91537203775124
At time: 1066.0138819217682 and batch: 400, loss is 4.214508113861084 and perplexity is 67.66087623708039
At time: 1067.1917922496796 and batch: 450, loss is 4.122636885643005 and perplexity is 61.72178115059757
At time: 1068.3960900306702 and batch: 500, loss is 4.131695981025696 and perplexity is 62.283464985635185
At time: 1069.5734236240387 and batch: 550, loss is 4.127977299690246 and perplexity is 62.052282739847556
At time: 1070.7531893253326 and batch: 600, loss is 4.17206750869751 and perplexity is 64.84939027895479
At time: 1071.9330055713654 and batch: 650, loss is 4.193163628578186 and perplexity is 66.23199329345938
At time: 1073.1129696369171 and batch: 700, loss is 4.176061720848083 and perplexity is 65.1089304858354
At time: 1074.2892882823944 and batch: 750, loss is 4.171854138374329 and perplexity is 64.83555481968354
At time: 1075.4680953025818 and batch: 800, loss is 4.181499862670899 and perplexity is 65.46396657708958
At time: 1076.6461572647095 and batch: 850, loss is 4.226618213653564 and perplexity is 68.48523767532984
At time: 1077.8213160037994 and batch: 900, loss is 4.172473406791687 and perplexity is 64.8757178656747
At time: 1078.9972968101501 and batch: 950, loss is 4.157900786399841 and perplexity is 63.93716386149732
At time: 1080.170793056488 and batch: 1000, loss is 4.155767436027527 and perplexity is 63.800908880652536
At time: 1081.3417572975159 and batch: 1050, loss is 4.170397710800171 and perplexity is 64.7411952604698
At time: 1082.5095388889313 and batch: 1100, loss is 4.105433111190796 and perplexity is 60.66901528757521
At time: 1083.6783003807068 and batch: 1150, loss is 4.1216980791091915 and perplexity is 61.663863530148866
At time: 1084.8412775993347 and batch: 1200, loss is 4.130519642829895 and perplexity is 62.21024164295684
At time: 1086.0026235580444 and batch: 1250, loss is 4.193192644119263 and perplexity is 66.23391507846205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.606215873773951 and perplexity of 100.1046234707952
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 1088.9699625968933 and batch: 50, loss is 4.200966415405273 and perplexity is 66.75080888973912
At time: 1090.175344467163 and batch: 100, loss is 4.209979476928711 and perplexity is 67.35515746123338
At time: 1091.3510522842407 and batch: 150, loss is 4.143523859977722 and perplexity is 63.02452018643455
At time: 1092.5281956195831 and batch: 200, loss is 4.201376066207886 and perplexity is 66.77815901379353
At time: 1093.7041556835175 and batch: 250, loss is 4.233980884552002 and perplexity is 68.99133276127812
At time: 1094.886223077774 and batch: 300, loss is 4.23241005897522 and perplexity is 68.88304448446456
At time: 1096.0696156024933 and batch: 350, loss is 4.191864714622498 and perplexity is 66.14601948143734
At time: 1097.246593952179 and batch: 400, loss is 4.220532903671264 and perplexity is 68.06974924497216
At time: 1098.450029373169 and batch: 450, loss is 4.129449014663696 and perplexity is 62.143673247386154
At time: 1099.6269011497498 and batch: 500, loss is 4.137616419792176 and perplexity is 62.6533041510258
At time: 1100.806144475937 and batch: 550, loss is 4.132787027359009 and perplexity is 62.35145621578033
At time: 1101.9871292114258 and batch: 600, loss is 4.172981209754944 and perplexity is 64.90867031341755
At time: 1103.166915178299 and batch: 650, loss is 4.190812511444092 and perplexity is 66.07645703283272
At time: 1104.3505198955536 and batch: 700, loss is 4.174707813262939 and perplexity is 65.02083865854311
At time: 1105.528077840805 and batch: 750, loss is 4.169146189689636 and perplexity is 64.66022096896685
At time: 1106.7047717571259 and batch: 800, loss is 4.178962635993957 and perplexity is 65.29808018938728
At time: 1107.8844857215881 and batch: 850, loss is 4.223751964569092 and perplexity is 68.28922297321795
At time: 1109.0649466514587 and batch: 900, loss is 4.169881563186646 and perplexity is 64.7077878693536
At time: 1110.245287656784 and batch: 950, loss is 4.155927395820617 and perplexity is 63.811115277120834
At time: 1111.4238522052765 and batch: 1000, loss is 4.153743782043457 and perplexity is 63.67192846708926
At time: 1112.6029329299927 and batch: 1050, loss is 4.168429236412049 and perplexity is 64.61387922603093
At time: 1113.7797529697418 and batch: 1100, loss is 4.103837237358094 and perplexity is 60.57227240886844
At time: 1114.9589068889618 and batch: 1150, loss is 4.120296688079834 and perplexity is 61.57750886741966
At time: 1116.1367816925049 and batch: 1200, loss is 4.129207873344422 and perplexity is 62.12868964668994
At time: 1117.3141219615936 and batch: 1250, loss is 4.192469682693481 and perplexity is 66.18604781795821
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.605958813298358 and perplexity of 100.07889383585453
Finished 36 epochs...
Completing Train Step...
At time: 1120.3160145282745 and batch: 50, loss is 4.200760326385498 and perplexity is 66.73705369841147
At time: 1121.498940706253 and batch: 100, loss is 4.209284677505493 and perplexity is 67.30837539063833
At time: 1122.6822204589844 and batch: 150, loss is 4.142498593330384 and perplexity is 62.959936361401176
At time: 1123.8636462688446 and batch: 200, loss is 4.200237302780152 and perplexity is 66.70215777046846
At time: 1125.0432193279266 and batch: 250, loss is 4.232658596038818 and perplexity is 68.90016660172466
At time: 1126.2251098155975 and batch: 300, loss is 4.230813484191895 and perplexity is 68.7731552991359
At time: 1127.4334959983826 and batch: 350, loss is 4.1905655717849735 and perplexity is 66.06014214953605
At time: 1128.6151926517487 and batch: 400, loss is 4.218473854064942 and perplexity is 67.92973445275643
At time: 1129.7993993759155 and batch: 450, loss is 4.1272727441787715 and perplexity is 62.00857885975628
At time: 1130.98157954216 and batch: 500, loss is 4.1358346843719485 and perplexity is 62.54177192978586
At time: 1132.1659026145935 and batch: 550, loss is 4.1307474756240845 and perplexity is 62.22441679085804
At time: 1133.3526804447174 and batch: 600, loss is 4.171649346351623 and perplexity is 64.8222783747702
At time: 1134.5361952781677 and batch: 650, loss is 4.190751910209656 and perplexity is 66.07245283930013
At time: 1135.7193250656128 and batch: 700, loss is 4.174363579750061 and perplexity is 64.99846015877715
At time: 1136.9011232852936 and batch: 750, loss is 4.169414501190186 and perplexity is 64.67757237756807
At time: 1138.0821154117584 and batch: 800, loss is 4.179411940574646 and perplexity is 65.32742550792588
At time: 1139.2631690502167 and batch: 850, loss is 4.224387855529785 and perplexity is 68.3326612823697
At time: 1140.4463312625885 and batch: 900, loss is 4.170567307472229 and perplexity is 64.75217608286017
At time: 1141.6290802955627 and batch: 950, loss is 4.1565090894699095 and perplexity is 63.848244595529195
At time: 1142.8106400966644 and batch: 1000, loss is 4.154341783523559 and perplexity is 63.71001576154776
At time: 1143.9907517433167 and batch: 1050, loss is 4.16879376411438 and perplexity is 64.63743706844612
At time: 1145.1704711914062 and batch: 1100, loss is 4.104471931457519 and perplexity is 60.61072947570087
At time: 1146.3509242534637 and batch: 1150, loss is 4.121100702285767 and perplexity is 61.627037967697014
At time: 1147.5336170196533 and batch: 1200, loss is 4.129943451881409 and perplexity is 62.1744069896119
At time: 1148.715308189392 and batch: 1250, loss is 4.192921862602234 and perplexity is 66.21598258646164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.605801102018704 and perplexity of 100.06311150999949
Finished 37 epochs...
Completing Train Step...
At time: 1151.6884849071503 and batch: 50, loss is 4.200573806762695 and perplexity is 66.7246070891332
At time: 1152.8923468589783 and batch: 100, loss is 4.208859272003174 and perplexity is 67.2797481269245
At time: 1154.0705869197845 and batch: 150, loss is 4.141800966262817 and perplexity is 62.916029122842986
At time: 1155.2469265460968 and batch: 200, loss is 4.1994227600097656 and perplexity is 66.6478481318519
At time: 1156.4244203567505 and batch: 250, loss is 4.231752772331237 and perplexity is 68.8377834556924
At time: 1157.6350073814392 and batch: 300, loss is 4.229756650924682 and perplexity is 68.7005119334486
At time: 1158.8155870437622 and batch: 350, loss is 4.189606232643127 and perplexity is 65.99679845835536
At time: 1159.9931638240814 and batch: 400, loss is 4.217330007553101 and perplexity is 67.85207768513082
At time: 1161.1695446968079 and batch: 450, loss is 4.126105275154114 and perplexity is 61.9362280065822
At time: 1162.3409976959229 and batch: 500, loss is 4.13490927696228 and perplexity is 62.48392208209555
At time: 1163.5097796916962 and batch: 550, loss is 4.129815702438354 and perplexity is 62.166464751065824
At time: 1164.6776814460754 and batch: 600, loss is 4.171160430908203 and perplexity is 64.79059350803657
At time: 1165.84840965271 and batch: 650, loss is 4.190779252052307 and perplexity is 66.07425940660657
At time: 1167.0106875896454 and batch: 700, loss is 4.174379258155823 and perplexity is 64.99947923899819
At time: 1168.1717646121979 and batch: 750, loss is 4.169644742012024 and perplexity is 64.69246550942445
At time: 1169.3393468856812 and batch: 800, loss is 4.1797030067443846 and perplexity is 65.34644287896978
At time: 1170.5164637565613 and batch: 850, loss is 4.2247725677490235 and perplexity is 68.35895474952288
At time: 1171.6948411464691 and batch: 900, loss is 4.1709962701797485 and perplexity is 64.7799583099741
At time: 1172.8730869293213 and batch: 950, loss is 4.156844229698181 and perplexity is 63.86964629688289
At time: 1174.0488686561584 and batch: 1000, loss is 4.154646635055542 and perplexity is 63.729440818184
At time: 1175.22487616539 and batch: 1050, loss is 4.169013576507568 and perplexity is 64.65164673985143
At time: 1176.4035279750824 and batch: 1100, loss is 4.104903182983398 and perplexity is 60.63687358220964
At time: 1177.5872128009796 and batch: 1150, loss is 4.1216067123413085 and perplexity is 61.65822975961656
At time: 1178.7673556804657 and batch: 1200, loss is 4.130333456993103 and perplexity is 62.198660055256795
At time: 1179.944769859314 and batch: 1250, loss is 4.1931280469894405 and perplexity is 66.22963669583821
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.6057293745723085 and perplexity of 100.05593449592998
Finished 38 epochs...
Completing Train Step...
At time: 1182.9496262073517 and batch: 50, loss is 4.200347962379456 and perplexity is 66.70953941293733
At time: 1184.136010169983 and batch: 100, loss is 4.208496780395508 and perplexity is 67.25536420261353
At time: 1185.3205502033234 and batch: 150, loss is 4.141283774375916 and perplexity is 62.88349787619816
At time: 1186.5015354156494 and batch: 200, loss is 4.199137535095215 and perplexity is 66.62884121581534
At time: 1187.7071154117584 and batch: 250, loss is 4.23119695186615 and perplexity is 68.79953263815484
At time: 1188.8874690532684 and batch: 300, loss is 4.229163904190063 and perplexity is 68.6598019958666
At time: 1190.0680270195007 and batch: 350, loss is 4.189122891426086 and perplexity is 65.96490719326997
At time: 1191.2495889663696 and batch: 400, loss is 4.21672589302063 and perplexity is 67.8110996379061
At time: 1192.4333992004395 and batch: 450, loss is 4.125508990287781 and perplexity is 61.89930737984356
At time: 1193.6124572753906 and batch: 500, loss is 4.1344207715988155 and perplexity is 62.45340580531744
At time: 1194.7920973300934 and batch: 550, loss is 4.129397897720337 and perplexity is 62.14049673394816
At time: 1195.9706118106842 and batch: 600, loss is 4.170932693481445 and perplexity is 64.77583994502595
At time: 1197.151062965393 and batch: 650, loss is 4.190813264846802 and perplexity is 66.07650681503327
At time: 1198.3329524993896 and batch: 700, loss is 4.174427704811096 and perplexity is 65.0026283226425
At time: 1199.5129392147064 and batch: 750, loss is 4.169779262542725 and perplexity is 64.70116855957343
At time: 1200.6919605731964 and batch: 800, loss is 4.179867000579834 and perplexity is 65.3571601715314
At time: 1201.8780257701874 and batch: 850, loss is 4.224977741241455 and perplexity is 68.37298163393102
At time: 1203.0639259815216 and batch: 900, loss is 4.1712154245376585 and perplexity is 64.79415667590303
At time: 1204.2443797588348 and batch: 950, loss is 4.15700514793396 and perplexity is 63.879924914671165
At time: 1205.4251554012299 and batch: 1000, loss is 4.154791040420532 and perplexity is 63.738644355850084
At time: 1206.6042652130127 and batch: 1050, loss is 4.169131274223328 and perplexity is 64.65925653881199
At time: 1207.7882556915283 and batch: 1100, loss is 4.1051825475692745 and perplexity is 60.653815743696526
At time: 1208.978180885315 and batch: 1150, loss is 4.121919999122619 and perplexity is 61.67754949410998
At time: 1210.158444404602 and batch: 1200, loss is 4.130535688400268 and perplexity is 62.21123984977547
At time: 1211.3398575782776 and batch: 1250, loss is 4.193205008506775 and perplexity is 66.2347340253174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.605683932339188 and perplexity of 100.05138783413555
Finished 39 epochs...
Completing Train Step...
At time: 1214.330376625061 and batch: 50, loss is 4.2000930213928225 and perplexity is 66.69253458484746
At time: 1215.5404589176178 and batch: 100, loss is 4.2081739044189455 and perplexity is 67.23365256648906
At time: 1216.7224600315094 and batch: 150, loss is 4.14089804649353 and perplexity is 62.85924663521595
At time: 1217.9312176704407 and batch: 200, loss is 4.198735542297364 and perplexity is 66.60206228434683
At time: 1219.109206199646 and batch: 250, loss is 4.230824227333069 and perplexity is 68.77389414282099
At time: 1220.2844393253326 and batch: 300, loss is 4.228801984786987 and perplexity is 68.63495717749504
At time: 1221.4600183963776 and batch: 350, loss is 4.188638772964477 and perplexity is 65.93298009275236
At time: 1222.6350162029266 and batch: 400, loss is 4.216387681961059 and perplexity is 67.78816905195436
At time: 1223.8108215332031 and batch: 450, loss is 4.125187306404114 and perplexity is 61.87939857259224
At time: 1224.987811088562 and batch: 500, loss is 4.134130830764771 and perplexity is 62.435300637589854
At time: 1226.1624133586884 and batch: 550, loss is 4.129215097427368 and perplexity is 62.12913847111845
At time: 1227.3403437137604 and batch: 600, loss is 4.170841774940491 and perplexity is 64.76995088788534
At time: 1228.5195891857147 and batch: 650, loss is 4.190821790695191 and perplexity is 66.07707017571401
At time: 1229.6999094486237 and batch: 700, loss is 4.174462337493896 and perplexity is 65.0048795770336
At time: 1230.8783190250397 and batch: 750, loss is 4.169838714599609 and perplexity is 64.70501529147407
At time: 1232.046953678131 and batch: 800, loss is 4.17995276927948 and perplexity is 65.36276601057122
At time: 1233.2166912555695 and batch: 850, loss is 4.225072736740112 and perplexity is 68.37947706792964
At time: 1234.375812292099 and batch: 900, loss is 4.171330795288086 and perplexity is 64.80163245761705
At time: 1235.5356276035309 and batch: 950, loss is 4.157090439796447 and perplexity is 63.885373584803
At time: 1236.6929287910461 and batch: 1000, loss is 4.154869847297668 and perplexity is 63.74366759729504
At time: 1237.8534650802612 and batch: 1050, loss is 4.169192032814026 and perplexity is 64.66318526346555
At time: 1239.010675907135 and batch: 1100, loss is 4.105387587547302 and perplexity is 60.666253475816475
At time: 1240.1696746349335 and batch: 1150, loss is 4.1221440362930295 and perplexity is 61.69136910577201
At time: 1241.3412284851074 and batch: 1200, loss is 4.130666923522949 and perplexity is 62.21940468521416
At time: 1242.5069077014923 and batch: 1250, loss is 4.193227143287658 and perplexity is 66.23620013286786
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.605652746492929 and perplexity of 100.04826769558888
Finished 40 epochs...
Completing Train Step...
At time: 1245.5116662979126 and batch: 50, loss is 4.199837102890014 and perplexity is 66.6754689150515
At time: 1246.6748492717743 and batch: 100, loss is 4.207885417938233 and perplexity is 67.21425936414752
At time: 1247.8636031150818 and batch: 150, loss is 4.14052261352539 and perplexity is 62.835651631125025
At time: 1249.0244629383087 and batch: 200, loss is 4.198408193588257 and perplexity is 66.58026375328939
At time: 1250.1841020584106 and batch: 250, loss is 4.23052948474884 and perplexity is 68.7536265345442
At time: 1251.346284866333 and batch: 300, loss is 4.228534040451049 and perplexity is 68.61656929304678
At time: 1252.5148231983185 and batch: 350, loss is 4.188301167488098 and perplexity is 65.91072451461295
At time: 1253.6782994270325 and batch: 400, loss is 4.216168909072876 and perplexity is 67.77334046053225
At time: 1254.8543934822083 and batch: 450, loss is 4.124966921806336 and perplexity is 61.865762808838404
At time: 1256.0175290107727 and batch: 500, loss is 4.133920540809632 and perplexity is 62.42217250142564
At time: 1257.1809849739075 and batch: 550, loss is 4.129100623130799 and perplexity is 62.12202668876144
At time: 1258.3399891853333 and batch: 600, loss is 4.170782136917114 and perplexity is 64.76608825122135
At time: 1259.509599685669 and batch: 650, loss is 4.1908161830902095 and perplexity is 66.07669964264505
At time: 1260.6685433387756 and batch: 700, loss is 4.174486021995545 and perplexity is 65.0064192034437
At time: 1261.828521490097 and batch: 750, loss is 4.169881329536438 and perplexity is 64.7077727503673
At time: 1262.9897525310516 and batch: 800, loss is 4.1800051164627074 and perplexity is 65.36618765681581
At time: 1264.1502754688263 and batch: 850, loss is 4.225118408203125 and perplexity is 68.38260013000422
At time: 1265.3131127357483 and batch: 900, loss is 4.171396255493164 and perplexity is 64.80587452460887
At time: 1266.477549791336 and batch: 950, loss is 4.157135739326477 and perplexity is 63.888267627751034
At time: 1267.636348247528 and batch: 1000, loss is 4.154898524284363 and perplexity is 63.74549559981327
At time: 1268.7970871925354 and batch: 1050, loss is 4.169221096038818 and perplexity is 64.66506461146447
At time: 1269.9564306735992 and batch: 1100, loss is 4.105544943809509 and perplexity is 60.67580044182328
At time: 1271.115731716156 and batch: 1150, loss is 4.122315526008606 and perplexity is 61.70194944829752
At time: 1272.2768585681915 and batch: 1200, loss is 4.130754895210266 and perplexity is 62.224878471993776
At time: 1273.4365899562836 and batch: 1250, loss is 4.193224954605102 and perplexity is 66.2360551630107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.605634034985173 and perplexity of 100.04639565916631
Finished 41 epochs...
Completing Train Step...
At time: 1276.3403289318085 and batch: 50, loss is 4.199583897590637 and perplexity is 66.65858847018298
At time: 1277.4988782405853 and batch: 100, loss is 4.207611465454102 and perplexity is 67.19584837280927
At time: 1278.658409833908 and batch: 150, loss is 4.140280394554138 and perplexity is 62.82043348736496
At time: 1279.8162770271301 and batch: 200, loss is 4.198085603713989 and perplexity is 66.55878909831581
At time: 1280.9744350910187 and batch: 250, loss is 4.230284104347229 and perplexity is 68.73675781176455
At time: 1282.133459329605 and batch: 300, loss is 4.228321495056153 and perplexity is 68.60198670701558
At time: 1283.291622877121 and batch: 350, loss is 4.188076276779174 and perplexity is 65.89590347167108
At time: 1284.4505076408386 and batch: 400, loss is 4.216012468338013 and perplexity is 67.76273877863554
At time: 1285.6100125312805 and batch: 450, loss is 4.124806604385376 and perplexity is 61.855845444283574
At time: 1286.7680237293243 and batch: 500, loss is 4.133760023117065 and perplexity is 62.41215344247036
At time: 1287.9267134666443 and batch: 550, loss is 4.129025359153747 and perplexity is 62.117351313916195
At time: 1289.0852699279785 and batch: 600, loss is 4.170727081298828 and perplexity is 64.76252261234386
At time: 1290.2435371875763 and batch: 650, loss is 4.190797128677368 and perplexity is 66.07544060192605
At time: 1291.4031314849854 and batch: 700, loss is 4.174491744041443 and perplexity is 65.00679117422229
At time: 1292.5617489814758 and batch: 750, loss is 4.16989266872406 and perplexity is 64.70850648810308
At time: 1293.7195901870728 and batch: 800, loss is 4.180028448104858 and perplexity is 65.36771277510667
At time: 1294.878157377243 and batch: 850, loss is 4.225139207839966 and perplexity is 68.38402247804531
At time: 1296.050805091858 and batch: 900, loss is 4.171441822052002 and perplexity is 64.80882757258306
At time: 1297.2124857902527 and batch: 950, loss is 4.1571630144119265 and perplexity is 63.89001020947425
At time: 1298.3714263439178 and batch: 1000, loss is 4.1549012565612795 and perplexity is 63.74566977039738
At time: 1299.5285568237305 and batch: 1050, loss is 4.169238452911377 and perplexity is 64.66618700449052
At time: 1300.6860208511353 and batch: 1100, loss is 4.105672087669372 and perplexity is 60.68351548774173
At time: 1301.8458626270294 and batch: 1150, loss is 4.122453069686889 and perplexity is 61.71043674505552
At time: 1303.0035400390625 and batch: 1200, loss is 4.130826554298401 and perplexity is 62.229337609811324
At time: 1304.1623859405518 and batch: 1250, loss is 4.19321273803711 and perplexity is 66.23524599068189
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.605617105525775 and perplexity of 100.04470194211
Finished 42 epochs...
Completing Train Step...
At time: 1307.0292756557465 and batch: 50, loss is 4.199342679977417 and perplexity is 66.64251118371189
At time: 1308.212239742279 and batch: 100, loss is 4.207355928421021 and perplexity is 67.17867953881057
At time: 1309.3709604740143 and batch: 150, loss is 4.140043959617615 and perplexity is 62.80558229790021
At time: 1310.529539346695 and batch: 200, loss is 4.197971324920655 and perplexity is 66.55118327481232
At time: 1311.689834356308 and batch: 250, loss is 4.23006950378418 and perplexity is 68.72200844750354
At time: 1312.8475232124329 and batch: 300, loss is 4.228139581680298 and perplexity is 68.58950822305937
At time: 1314.0082051753998 and batch: 350, loss is 4.187892565727234 and perplexity is 65.88379877784278
At time: 1315.1662199497223 and batch: 400, loss is 4.215868787765503 and perplexity is 67.7530032889501
At time: 1316.3260633945465 and batch: 450, loss is 4.124680151939392 and perplexity is 61.84802411585223
At time: 1317.485090970993 and batch: 500, loss is 4.133623051643371 and perplexity is 62.40360534327318
At time: 1318.6435861587524 and batch: 550, loss is 4.128954310417175 and perplexity is 62.112938111364215
At time: 1319.8029923439026 and batch: 600, loss is 4.1706609582901 and perplexity is 64.75824046107184
At time: 1320.9620683193207 and batch: 650, loss is 4.190770092010498 and perplexity is 66.07365416639996
At time: 1322.1194789409637 and batch: 700, loss is 4.174477825164795 and perplexity is 65.00588635901164
At time: 1323.278849363327 and batch: 750, loss is 4.169873490333557 and perplexity is 64.70726549499693
At time: 1324.4360473155975 and batch: 800, loss is 4.180030612945557 and perplexity is 65.36785428594483
At time: 1325.5944604873657 and batch: 850, loss is 4.225159559249878 and perplexity is 68.38541420347993
At time: 1326.7519319057465 and batch: 900, loss is 4.171471419334412 and perplexity is 64.81074576614189
At time: 1327.909892320633 and batch: 950, loss is 4.157184691429138 and perplexity is 63.89139516933604
At time: 1329.0689644813538 and batch: 1000, loss is 4.154904727935791 and perplexity is 63.745891055874715
At time: 1330.2276303768158 and batch: 1050, loss is 4.169246687889099 and perplexity is 64.66671953129254
At time: 1331.38542842865 and batch: 1100, loss is 4.105772023200989 and perplexity is 60.68958023015892
At time: 1332.5458295345306 and batch: 1150, loss is 4.122570052146911 and perplexity is 61.71765620602191
At time: 1333.704009771347 and batch: 1200, loss is 4.130886301994324 and perplexity is 62.23305578042729
At time: 1334.8882231712341 and batch: 1250, loss is 4.193199253082275 and perplexity is 66.23435281740346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.605604631187272 and perplexity of 100.04345395841639
Finished 43 epochs...
Completing Train Step...
At time: 1337.7812700271606 and batch: 50, loss is 4.199112977981567 and perplexity is 66.62720502387957
At time: 1338.9423892498016 and batch: 100, loss is 4.2071162033081055 and perplexity is 67.16257705243494
At time: 1340.103763103485 and batch: 150, loss is 4.139825911521911 and perplexity is 62.79188915321691
At time: 1341.2651262283325 and batch: 200, loss is 4.197790803909302 and perplexity is 66.5391704722151
At time: 1342.4320356845856 and batch: 250, loss is 4.229880962371826 and perplexity is 68.70905272435462
At time: 1343.5974388122559 and batch: 300, loss is 4.227980718612671 and perplexity is 68.57861274884317
At time: 1344.7639632225037 and batch: 350, loss is 4.187742590904236 and perplexity is 65.87391860768956
At time: 1345.92520570755 and batch: 400, loss is 4.215760831832886 and perplexity is 67.74568934509132
At time: 1347.087725162506 and batch: 450, loss is 4.124578981399536 and perplexity is 61.841767234374785
At time: 1348.2497470378876 and batch: 500, loss is 4.1335145950317385 and perplexity is 62.3968376266924
At time: 1349.4121778011322 and batch: 550, loss is 4.128891139030457 and perplexity is 62.10901447486267
At time: 1350.593662261963 and batch: 600, loss is 4.17058723449707 and perplexity is 64.75346641393766
At time: 1351.7558736801147 and batch: 650, loss is 4.190729413032532 and perplexity is 66.07096641234587
At time: 1352.9177119731903 and batch: 700, loss is 4.174448480606079 and perplexity is 65.00397881795064
At time: 1354.080632686615 and batch: 750, loss is 4.169854989051819 and perplexity is 64.70606833872203
At time: 1355.2424581050873 and batch: 800, loss is 4.180015950202942 and perplexity is 65.36689582094904
At time: 1356.4060952663422 and batch: 850, loss is 4.22515941619873 and perplexity is 68.38540442086862
At time: 1357.5680985450745 and batch: 900, loss is 4.171490430831909 and perplexity is 64.81197792718538
At time: 1358.7299411296844 and batch: 950, loss is 4.157200136184692 and perplexity is 63.89238196393685
At time: 1359.8923954963684 and batch: 1000, loss is 4.154906668663025 and perplexity is 63.746014769381595
At time: 1361.0537815093994 and batch: 1050, loss is 4.1692492866516115 and perplexity is 64.66688758495745
At time: 1362.2170641422272 and batch: 1100, loss is 4.105852575302124 and perplexity is 60.6944691002652
At time: 1363.3791041374207 and batch: 1150, loss is 4.122670564651489 and perplexity is 61.723859913993756
At time: 1364.5659565925598 and batch: 1200, loss is 4.130939321517944 and perplexity is 62.23635543487049
At time: 1365.7288618087769 and batch: 1250, loss is 4.1931843566894536 and perplexity is 66.23336617181437
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.605595720945484 and perplexity of 100.04256255102365
Finished 44 epochs...
Completing Train Step...
At time: 1368.595389842987 and batch: 50, loss is 4.1988945579528805 and perplexity is 66.61265389703355
At time: 1369.7791316509247 and batch: 100, loss is 4.206891393661499 and perplexity is 67.14747995427281
At time: 1370.9383797645569 and batch: 150, loss is 4.139602398872375 and perplexity is 62.777855940061634
At time: 1372.097983598709 and batch: 200, loss is 4.197509708404541 and perplexity is 66.52046923904429
At time: 1373.2557933330536 and batch: 250, loss is 4.229711446762085 and perplexity is 68.69740645452747
At time: 1374.415198802948 and batch: 300, loss is 4.2278388977050785 and perplexity is 68.56888755737572
At time: 1375.573689699173 and batch: 350, loss is 4.18761067867279 and perplexity is 65.8652296051974
At time: 1376.732629776001 and batch: 400, loss is 4.2156799697875975 and perplexity is 67.74021151156883
At time: 1377.8923377990723 and batch: 450, loss is 4.12449285030365 and perplexity is 61.83644096457334
At time: 1379.0505440235138 and batch: 500, loss is 4.13342089176178 and perplexity is 62.39099111289473
At time: 1380.2094640731812 and batch: 550, loss is 4.128824977874756 and perplexity is 62.10490540661743
At time: 1381.3684022426605 and batch: 600, loss is 4.17050910949707 and perplexity is 64.74840774698083
At time: 1382.5269556045532 and batch: 650, loss is 4.190681562423706 and perplexity is 66.06780495201684
At time: 1383.6845879554749 and batch: 700, loss is 4.174410676956176 and perplexity is 65.00152147674163
At time: 1384.8445625305176 and batch: 750, loss is 4.169828863143921 and perplexity is 64.70437785602294
At time: 1386.00332736969 and batch: 800, loss is 4.179990935325622 and perplexity is 65.36526069652061
At time: 1387.1627242565155 and batch: 850, loss is 4.225150947570801 and perplexity is 68.38482529277499
At time: 1388.3222579956055 and batch: 900, loss is 4.1715008687973025 and perplexity is 64.81265443589875
At time: 1389.4802632331848 and batch: 950, loss is 4.157211618423462 and perplexity is 63.893115595733974
At time: 1390.6393156051636 and batch: 1000, loss is 4.154903273582459 and perplexity is 63.745798346893075
At time: 1391.7973256111145 and batch: 1050, loss is 4.169246730804443 and perplexity is 64.66672230648712
At time: 1392.956003665924 and batch: 1100, loss is 4.10591796875 and perplexity is 60.69843825064349
At time: 1394.1412642002106 and batch: 1150, loss is 4.122759866714477 and perplexity is 61.7293722281465
At time: 1395.298794746399 and batch: 1200, loss is 4.130985832214355 and perplexity is 62.239250158421115
At time: 1396.458472251892 and batch: 1250, loss is 4.193168463706971 and perplexity is 66.23231353445081
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.605586365191606 and perplexity of 100.04162658180942
Finished 45 epochs...
Completing Train Step...
At time: 1399.3482398986816 and batch: 50, loss is 4.1986839485168455 and perplexity is 66.59862612080637
At time: 1400.5095565319061 and batch: 100, loss is 4.206673679351806 and perplexity is 67.13286257829068
At time: 1401.6712009906769 and batch: 150, loss is 4.139382405281067 and perplexity is 62.76404673310295
At time: 1402.8333761692047 and batch: 200, loss is 4.197450037002564 and perplexity is 66.51649998781113
At time: 1403.995502948761 and batch: 250, loss is 4.229555501937866 and perplexity is 68.68669428482902
At time: 1405.1581888198853 and batch: 300, loss is 4.227709970474243 and perplexity is 68.56004773044093
At time: 1406.3202049732208 and batch: 350, loss is 4.187496294975281 and perplexity is 65.85769612755959
At time: 1407.4804513454437 and batch: 400, loss is 4.2156154155731205 and perplexity is 67.73583873656824
At time: 1408.6428048610687 and batch: 450, loss is 4.124420032501221 and perplexity is 61.831938334770086
At time: 1409.8048558235168 and batch: 500, loss is 4.133340225219727 and perplexity is 62.385958450372854
At time: 1410.965662240982 and batch: 550, loss is 4.128738842010498 and perplexity is 62.09955617729912
At time: 1412.1297869682312 and batch: 600, loss is 4.170430779457092 and perplexity is 64.74333620024333
At time: 1413.2931761741638 and batch: 650, loss is 4.190631990432739 and perplexity is 66.06452992056212
At time: 1414.4624309539795 and batch: 700, loss is 4.17436749458313 and perplexity is 64.99871461739652
At time: 1415.629289150238 and batch: 750, loss is 4.169792437553406 and perplexity is 64.70202100377578
At time: 1416.792227268219 and batch: 800, loss is 4.179958295822144 and perplexity is 65.36312724168437
At time: 1417.9562809467316 and batch: 850, loss is 4.22513445854187 and perplexity is 68.38369770270877
At time: 1419.1215159893036 and batch: 900, loss is 4.171505255699158 and perplexity is 64.81293876327638
At time: 1420.284770488739 and batch: 950, loss is 4.1572185754776 and perplexity is 63.89356010514446
At time: 1421.448142528534 and batch: 1000, loss is 4.154895157814026 and perplexity is 63.74528100285442
At time: 1422.6134266853333 and batch: 1050, loss is 4.1692383050918576 and perplexity is 64.66617744556655
At time: 1423.8017468452454 and batch: 1100, loss is 4.105973024368286 and perplexity is 60.7017801326842
At time: 1424.9660091400146 and batch: 1150, loss is 4.122839784622192 and perplexity is 61.73430570755357
At time: 1426.131010055542 and batch: 1200, loss is 4.13102686882019 and perplexity is 62.2418042984036
At time: 1427.2942695617676 and batch: 1250, loss is 4.193150148391724 and perplexity is 66.23110047985769
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.605578791486086 and perplexity of 100.04086889885923
Finished 46 epochs...
Completing Train Step...
At time: 1430.1655805110931 and batch: 50, loss is 4.1984791707992555 and perplexity is 66.58498960242994
At time: 1431.3494079113007 and batch: 100, loss is 4.206467485427856 and perplexity is 67.11902161693976
At time: 1432.5097937583923 and batch: 150, loss is 4.139244394302368 and perplexity is 62.75538520329309
At time: 1433.6681048870087 and batch: 200, loss is 4.197235479354858 and perplexity is 66.50222989497367
At time: 1434.8383128643036 and batch: 250, loss is 4.229409317970276 and perplexity is 68.6766541252109
At time: 1436.008564710617 and batch: 300, loss is 4.227589178085327 and perplexity is 68.55176669864424
At time: 1437.1955306529999 and batch: 350, loss is 4.187388091087342 and perplexity is 65.85057045430759
At time: 1438.3825600147247 and batch: 400, loss is 4.215560102462769 and perplexity is 67.73209216026379
At time: 1439.5652205944061 and batch: 450, loss is 4.1243537902832035 and perplexity is 61.82784258568771
At time: 1440.7377223968506 and batch: 500, loss is 4.133265404701233 and perplexity is 62.38129087523224
At time: 1441.9204587936401 and batch: 550, loss is 4.128684048652649 and perplexity is 62.09615362731464
At time: 1443.116226196289 and batch: 600, loss is 4.1703506326675415 and perplexity is 64.73814743763619
At time: 1444.30379986763 and batch: 650, loss is 4.190580387115478 and perplexity is 66.06112085962512
At time: 1445.5182285308838 and batch: 700, loss is 4.174321575164795 and perplexity is 64.9957299827556
At time: 1446.6983697414398 and batch: 750, loss is 4.169743175506592 and perplexity is 64.69883372829463
At time: 1447.8782544136047 and batch: 800, loss is 4.179920859336853 and perplexity is 65.36068032173517
At time: 1449.058269739151 and batch: 850, loss is 4.225113344192505 and perplexity is 68.38225384066777
At time: 1450.2386775016785 and batch: 900, loss is 4.171504774093628 and perplexity is 64.8129075490142
At time: 1451.4191586971283 and batch: 950, loss is 4.157223057746887 and perplexity is 63.893846493928415
At time: 1452.5783586502075 and batch: 1000, loss is 4.154883189201355 and perplexity is 63.744518064842175
At time: 1453.7668924331665 and batch: 1050, loss is 4.169226927757263 and perplexity is 64.6654417210141
At time: 1454.9251852035522 and batch: 1100, loss is 4.106020784378051 and perplexity is 60.704679319528175
At time: 1456.0850248336792 and batch: 1150, loss is 4.122913179397583 and perplexity is 61.7388368493339
At time: 1457.2462708950043 and batch: 1200, loss is 4.131063098907471 and perplexity is 62.24405936525622
At time: 1458.4194803237915 and batch: 1250, loss is 4.193130664825439 and perplexity is 66.22981007439226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.605569881244297 and perplexity of 100.03997751449981
Finished 47 epochs...
Completing Train Step...
At time: 1461.346523284912 and batch: 50, loss is 4.19827935218811 and perplexity is 66.57168601148125
At time: 1462.5277438163757 and batch: 100, loss is 4.206264553070068 and perplexity is 67.10540237756952
At time: 1463.688341140747 and batch: 150, loss is 4.1390307331085205 and perplexity is 62.74197824509453
At time: 1464.8488557338715 and batch: 200, loss is 4.197139129638672 and perplexity is 66.49582273266665
At time: 1466.0080800056458 and batch: 250, loss is 4.22927161693573 and perplexity is 68.66719792996756
At time: 1467.1784174442291 and batch: 300, loss is 4.227476663589478 and perplexity is 68.54405406507425
At time: 1468.3423302173615 and batch: 350, loss is 4.187290258407593 and perplexity is 65.84412843166238
At time: 1469.5027358531952 and batch: 400, loss is 4.2155136871337895 and perplexity is 67.728948425883
At time: 1470.663514137268 and batch: 450, loss is 4.124296264648438 and perplexity is 61.824286002095114
At time: 1471.8226194381714 and batch: 500, loss is 4.1331982946395875 and perplexity is 62.37710460342812
At time: 1472.9825956821442 and batch: 550, loss is 4.128636403083801 and perplexity is 62.09319509123294
At time: 1474.1427237987518 and batch: 600, loss is 4.170275526046753 and perplexity is 64.73328535673555
At time: 1475.302081823349 and batch: 650, loss is 4.190530982017517 and perplexity is 66.05785718409939
At time: 1476.4622185230255 and batch: 700, loss is 4.174277367591858 and perplexity is 64.99285674279177
At time: 1477.6209597587585 and batch: 750, loss is 4.1697139549255375 and perplexity is 64.69694321840058
At time: 1478.7802953720093 and batch: 800, loss is 4.179880118370056 and perplexity is 65.35801751867128
At time: 1479.9411280155182 and batch: 850, loss is 4.225086727142334 and perplexity is 68.38043373100956
At time: 1481.1001121997833 and batch: 900, loss is 4.171498703956604 and perplexity is 64.81251412697851
At time: 1482.287029504776 and batch: 950, loss is 4.157222423553467 and perplexity is 63.89380597288421
At time: 1483.4453039169312 and batch: 1000, loss is 4.154865064620972 and perplexity is 63.743362732670505
At time: 1484.6054153442383 and batch: 1050, loss is 4.1692126178741455 and perplexity is 64.66451637272216
At time: 1485.7658696174622 and batch: 1100, loss is 4.106064190864563 and perplexity is 60.70731435356063
At time: 1486.9252452850342 and batch: 1150, loss is 4.122980914115906 and perplexity is 61.74301885368933
At time: 1488.0861840248108 and batch: 1200, loss is 4.131093516349792 and perplexity is 62.24595269913685
At time: 1489.245858669281 and batch: 1250, loss is 4.193108024597168 and perplexity is 66.22831063334775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.605564535099225 and perplexity of 100.0394426876966
Finished 48 epochs...
Completing Train Step...
At time: 1492.1401319503784 and batch: 50, loss is 4.198081645965576 and perplexity is 66.5585256758952
At time: 1493.298555612564 and batch: 100, loss is 4.206067953109741 and perplexity is 67.09221075490312
At time: 1494.4581246376038 and batch: 150, loss is 4.138861594200134 and perplexity is 62.73136703279386
At time: 1495.6166858673096 and batch: 200, loss is 4.196916656494141 and perplexity is 66.48103084334518
At time: 1496.7746980190277 and batch: 250, loss is 4.229138994216919 and perplexity is 68.6580917033446
At time: 1497.9347186088562 and batch: 300, loss is 4.227369689941407 and perplexity is 68.53672204973053
At time: 1499.092046737671 and batch: 350, loss is 4.187195844650269 and perplexity is 65.83791213355606
At time: 1500.2499759197235 and batch: 400, loss is 4.215472173690796 and perplexity is 67.7261368224036
At time: 1501.4098069667816 and batch: 450, loss is 4.12424204826355 and perplexity is 61.820934203671854
At time: 1502.5674448013306 and batch: 500, loss is 4.133131065368652 and perplexity is 62.372911177124664
At time: 1503.726608991623 and batch: 550, loss is 4.128593344688415 and perplexity is 62.09052151544827
At time: 1504.8841803073883 and batch: 600, loss is 4.170220060348511 and perplexity is 64.72969497943603
At time: 1506.0416748523712 and batch: 650, loss is 4.190481767654419 and perplexity is 66.054606268727
At time: 1507.2016003131866 and batch: 700, loss is 4.174232711791992 and perplexity is 64.98995449958984
At time: 1508.3592584133148 and batch: 750, loss is 4.169675240516662 and perplexity is 64.69443856297129
At time: 1509.5169281959534 and batch: 800, loss is 4.179837942123413 and perplexity is 65.35526102093411
At time: 1510.675637960434 and batch: 850, loss is 4.225058555603027 and perplexity is 68.37850737606722
At time: 1511.8589253425598 and batch: 900, loss is 4.171490688323974 and perplexity is 64.81199461575757
At time: 1513.017834186554 and batch: 950, loss is 4.157220773696899 and perplexity is 63.89370055735574
At time: 1514.1765110492706 and batch: 1000, loss is 4.1548458862304685 and perplexity is 63.7421402492907
At time: 1515.3342878818512 and batch: 1050, loss is 4.169196105003357 and perplexity is 64.66344858473482
At time: 1516.4933302402496 and batch: 1100, loss is 4.106102733612061 and perplexity is 60.709654225341275
At time: 1517.6512353420258 and batch: 1150, loss is 4.123044104576111 and perplexity is 61.74692054673872
At time: 1518.8098409175873 and batch: 1200, loss is 4.131121082305908 and perplexity is 62.247668591987356
At time: 1519.968103170395 and batch: 1250, loss is 4.193084750175476 and perplexity is 66.22676922565587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.605560971002509 and perplexity of 100.03908613808288
Finished 49 epochs...
Completing Train Step...
At time: 1522.8384146690369 and batch: 50, loss is 4.19788767337799 and perplexity is 66.54561639850739
At time: 1524.021735906601 and batch: 100, loss is 4.2058742713928225 and perplexity is 67.07921747865288
At time: 1525.1802949905396 and batch: 150, loss is 4.138696312904358 and perplexity is 62.720999567962494
At time: 1526.3385272026062 and batch: 200, loss is 4.196818761825561 and perplexity is 66.47452302340959
At time: 1527.4957447052002 and batch: 250, loss is 4.229010663032532 and perplexity is 68.6492812944557
At time: 1528.6543943881989 and batch: 300, loss is 4.227266616821289 and perplexity is 68.52965812000315
At time: 1529.8139162063599 and batch: 350, loss is 4.187106704711914 and perplexity is 65.83204360769093
At time: 1530.9806456565857 and batch: 400, loss is 4.215433902740479 and perplexity is 67.72354492838353
At time: 1532.1390707492828 and batch: 450, loss is 4.1241910362243654 and perplexity is 61.81778067218853
At time: 1533.2969415187836 and batch: 500, loss is 4.133065643310547 and perplexity is 62.36883074638199
At time: 1534.4555339813232 and batch: 550, loss is 4.128549904823303 and perplexity is 62.08782437015114
At time: 1535.6146256923676 and batch: 600, loss is 4.17016487121582 and perplexity is 64.72612270228714
At time: 1536.773564338684 and batch: 650, loss is 4.19043197631836 and perplexity is 66.05131740350717
At time: 1537.932177066803 and batch: 700, loss is 4.174187927246094 and perplexity is 64.98704401916237
At time: 1539.0900449752808 and batch: 750, loss is 4.169633746147156 and perplexity is 64.69175416372664
At time: 1540.2486126422882 and batch: 800, loss is 4.179794864654541 and perplexity is 65.35244574234983
At time: 1541.4376723766327 and batch: 850, loss is 4.22502890586853 and perplexity is 68.37648000153393
At time: 1542.598347902298 and batch: 900, loss is 4.1714808464050295 and perplexity is 64.81135674449887
At time: 1543.7569029331207 and batch: 950, loss is 4.157217869758606 and perplexity is 63.89351501426139
At time: 1544.9151146411896 and batch: 1000, loss is 4.154824724197388 and perplexity is 63.74079135028289
At time: 1546.0741317272186 and batch: 1050, loss is 4.169176936149597 and perplexity is 64.66220907242537
At time: 1547.2334430217743 and batch: 1100, loss is 4.106137285232544 and perplexity is 60.71175187851217
At time: 1548.39168715477 and batch: 1150, loss is 4.12310296535492 and perplexity is 61.7505551255372
At time: 1549.5511887073517 and batch: 1200, loss is 4.131145749092102 and perplexity is 62.24920406085705
At time: 1550.709356546402 and batch: 1250, loss is 4.193060989379883 and perplexity is 66.22519564362435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.605561862026688 and perplexity of 100.03917527536717
Annealing...
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f58a487a898>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'anneal': 3.8431223942323567, 'data': 'wikitext', 'lr': 9.119988588126452, 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.37835670774145425, 'num_layers': 1, 'wordvec_source': 'glove', 'seq_len': 35}, 'best_accuracy': -79.62813229816756}, {'params': {'anneal': 7.614927514455996, 'data': 'wikitext', 'lr': 23.582210378130824, 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.6714728479092035, 'num_layers': 1, 'wordvec_source': 'glove', 'seq_len': 35}, 'best_accuracy': -199.5935231960231}, {'params': {'anneal': 4.329807450795583, 'data': 'wikitext', 'lr': 13.795690389813887, 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.35428703004268747, 'num_layers': 1, 'wordvec_source': 'glove', 'seq_len': 35}, 'best_accuracy': -119.33320289464895}, {'params': {'anneal': 4.652774405702031, 'data': 'wikitext', 'lr': 14.781429953862904, 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.6609921934764259, 'num_layers': 1, 'wordvec_source': 'glove', 'seq_len': 35}, 'best_accuracy': -164.01490698474564}, {'params': {'anneal': 4.266939190823227, 'data': 'wikitext', 'lr': 27.542070663602637, 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.7807224165803062, 'num_layers': 1, 'wordvec_source': 'glove', 'seq_len': 35}, 'best_accuracy': -296.6821185067396}, {'params': {'anneal': 2.8139391373442404, 'data': 'wikitext', 'lr': 11.175666046230907, 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.0, 'num_layers': 1, 'wordvec_source': 'glove', 'seq_len': 35}, 'best_accuracy': -100.03908613808288}]
