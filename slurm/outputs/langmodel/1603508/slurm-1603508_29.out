Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'name': 'lr', 'type': 'continuous', 'domain': [0, 30]}, {'name': 'dropout', 'type': 'continuous', 'domain': [0, 1]}, {'name': 'anneal', 'type': 'continuous', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'data': 'ptb', 'wordvec_dim': 200, 'seq_len': 35, 'lr': 20.72877720884464, 'anneal': 5.948948781071943, 'tune_wordvecs': True, 'wordvec_source': '', 'batch_size': 80, 'num_layers': 1, 'dropout': 0.6665245902185484}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.4099235534667969 and batch: 50, loss is 6.44591911315918 and perplexity is 630.1255675893726
At time: 2.3607211112976074 and batch: 100, loss is 5.925776109695435 and perplexity is 374.5690292345223
At time: 3.3089053630828857 and batch: 150, loss is 5.8710064125061034 and perplexity is 354.6056803353783
At time: 4.254723310470581 and batch: 200, loss is 5.793128490447998 and perplexity is 328.0376834814808
At time: 5.2005774974823 and batch: 250, loss is 5.7795537662506105 and perplexity is 323.61475036921246
At time: 6.145334959030151 and batch: 300, loss is 5.761385154724121 and perplexity is 317.788210036216
At time: 7.090092420578003 and batch: 350, loss is 5.774390029907226 and perplexity is 321.9479961646568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.6029031687769395 and perplexity of 271.21264166177036
Finished 1 epochs...
Completing Train Step...
At time: 8.90541410446167 and batch: 50, loss is 5.404810810089112 and perplexity is 222.47412664188192
At time: 9.845780849456787 and batch: 100, loss is 5.276746129989624 and perplexity is 195.73195172656995
At time: 10.791635036468506 and batch: 150, loss is 5.213308715820313 and perplexity is 183.70086772722848
At time: 11.742902755737305 and batch: 200, loss is 5.175625982284546 and perplexity is 176.9073204713805
At time: 12.686846733093262 and batch: 250, loss is 5.17228590965271 and perplexity is 176.31742287073055
At time: 13.629347801208496 and batch: 300, loss is 5.134871397018433 and perplexity is 169.84247622129485
At time: 14.571368217468262 and batch: 350, loss is 5.166893224716187 and perplexity is 175.36915770604818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.320982308223329 and perplexity of 204.5847486012744
Finished 2 epochs...
Completing Train Step...
At time: 16.38081383705139 and batch: 50, loss is 5.090412902832031 and perplexity is 162.45692713215183
At time: 17.319477796554565 and batch: 100, loss is 5.050325975418091 and perplexity is 156.0733322648598
At time: 18.262871742248535 and batch: 150, loss is 5.033069038391114 and perplexity is 153.4030909579912
At time: 19.207854986190796 and batch: 200, loss is 5.002928304672241 and perplexity is 148.84839499042957
At time: 20.14902091026306 and batch: 250, loss is 5.015763692855835 and perplexity is 150.77123572578523
At time: 21.086794137954712 and batch: 300, loss is 4.990274095535279 and perplexity is 146.97670363339554
At time: 22.025919675827026 and batch: 350, loss is 5.086742153167725 and perplexity is 161.86168158848844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.255220084354796 and perplexity of 191.56364137573607
Finished 3 epochs...
Completing Train Step...
At time: 23.814019203186035 and batch: 50, loss is 5.013882865905762 and perplexity is 150.48792763260192
At time: 24.78050971031189 and batch: 100, loss is 4.970339460372925 and perplexity is 144.07578709110675
At time: 25.725021600723267 and batch: 150, loss is 4.92420521736145 and perplexity is 137.579952020852
At time: 26.660868406295776 and batch: 200, loss is 4.981906576156616 and perplexity is 145.75200421124646
At time: 27.596668481826782 and batch: 250, loss is 4.975616455078125 and perplexity is 144.83808380564582
At time: 28.53225564956665 and batch: 300, loss is 4.942005405426025 and perplexity is 140.05082681841373
At time: 29.468011617660522 and batch: 350, loss is 4.982511510848999 and perplexity is 145.84020132913676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.175206151501886 and perplexity of 176.83306492104225
Finished 4 epochs...
Completing Train Step...
At time: 31.26538920402527 and batch: 50, loss is 4.939744138717652 and perplexity is 139.73449233935563
At time: 32.221593379974365 and batch: 100, loss is 4.955495157241821 and perplexity is 141.95287796930165
At time: 33.16886854171753 and batch: 150, loss is 4.924900741577148 and perplexity is 137.67567549421506
At time: 34.12573599815369 and batch: 200, loss is 4.891128711700439 and perplexity is 133.1037250285712
At time: 35.07258629798889 and batch: 250, loss is 4.974482879638672 and perplexity is 144.67399193397551
At time: 36.01759886741638 and batch: 300, loss is 4.928148450851441 and perplexity is 138.1235329240741
At time: 36.96543574333191 and batch: 350, loss is 4.999935665130615 and perplexity is 148.4036112685034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.268533640894397 and perplexity of 194.13108774393763
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 38.78991913795471 and batch: 50, loss is 4.852152652740479 and perplexity is 128.0156667277669
At time: 39.731231927871704 and batch: 100, loss is 4.745830278396607 and perplexity is 115.10333365145091
At time: 40.67971897125244 and batch: 150, loss is 4.726292610168457 and perplexity is 112.87630918052605
At time: 41.636693239212036 and batch: 200, loss is 4.685387535095215 and perplexity is 108.35225447413141
At time: 42.58983659744263 and batch: 250, loss is 4.7131458759307865 and perplexity is 111.40206631511235
At time: 43.53169226646423 and batch: 300, loss is 4.681477193832397 and perplexity is 107.9293874984637
At time: 44.47370481491089 and batch: 350, loss is 4.72920428276062 and perplexity is 113.20544697448705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.082698427397629 and perplexity of 161.20847891178838
Finished 6 epochs...
Completing Train Step...
At time: 46.28069734573364 and batch: 50, loss is 4.716943864822388 and perplexity is 111.8259746154233
At time: 47.23604655265808 and batch: 100, loss is 4.6661991786956785 and perplexity is 106.29297309513375
At time: 48.17969489097595 and batch: 150, loss is 4.662966413497925 and perplexity is 105.94990769466439
At time: 49.128368854522705 and batch: 200, loss is 4.635917835235595 and perplexity is 103.12252405836296
At time: 50.070496559143066 and batch: 250, loss is 4.666862258911133 and perplexity is 106.36347723500181
At time: 51.01385283470154 and batch: 300, loss is 4.640014238357544 and perplexity is 103.54582189521949
At time: 51.95441961288452 and batch: 350, loss is 4.710664501190186 and perplexity is 111.12597872175849
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.076643845130658 and perplexity of 160.23537774726833
Finished 7 epochs...
Completing Train Step...
At time: 53.77257823944092 and batch: 50, loss is 4.690932855606079 and perplexity is 108.95477148425428
At time: 54.71186709403992 and batch: 100, loss is 4.636073303222656 and perplexity is 103.13855755591446
At time: 55.655216455459595 and batch: 150, loss is 4.6265762996673585 and perplexity is 102.16368681078882
At time: 56.608684062957764 and batch: 200, loss is 4.611501569747925 and perplexity is 100.6351469337449
At time: 57.56819152832031 and batch: 250, loss is 4.641185836791992 and perplexity is 103.66720711152537
At time: 58.518678426742554 and batch: 300, loss is 4.6132261753082275 and perplexity is 100.80885261154373
At time: 59.47356104850769 and batch: 350, loss is 4.685021915435791 and perplexity is 108.31264600101224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.061230889682112 and perplexity of 157.7846122947663
Finished 8 epochs...
Completing Train Step...
At time: 61.283323764801025 and batch: 50, loss is 4.647310571670532 and perplexity is 104.30408964811126
At time: 62.2238872051239 and batch: 100, loss is 4.6012538719177245 and perplexity is 99.60913446862878
At time: 63.17392539978027 and batch: 150, loss is 4.592846326828003 and perplexity is 98.77517685981324
At time: 64.12113213539124 and batch: 200, loss is 4.588048896789551 and perplexity is 98.30244471566053
At time: 65.07129216194153 and batch: 250, loss is 4.617668886184692 and perplexity is 101.25771353885763
At time: 66.01364850997925 and batch: 300, loss is 4.588319778442383 and perplexity is 98.32907665125106
At time: 66.95514106750488 and batch: 350, loss is 4.659973621368408 and perplexity is 105.63329565811766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.057238348599138 and perplexity of 157.15590664962752
Finished 9 epochs...
Completing Train Step...
At time: 68.75883412361145 and batch: 50, loss is 4.616890087127685 and perplexity is 101.1788848268865
At time: 69.71269536018372 and batch: 100, loss is 4.57634238243103 and perplexity is 97.15837533292584
At time: 70.66997814178467 and batch: 150, loss is 4.5658419227600096 and perplexity is 96.14350535658667
At time: 71.6146309375763 and batch: 200, loss is 4.565401964187622 and perplexity is 96.10121550080008
At time: 72.55693411827087 and batch: 250, loss is 4.593082437515259 and perplexity is 98.79850148819409
At time: 73.51645088195801 and batch: 300, loss is 4.565482530593872 and perplexity is 96.10895834227154
At time: 74.46522974967957 and batch: 350, loss is 4.636191129684448 and perplexity is 103.15071072319388
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.056174442685884 and perplexity of 156.98879646176132
Finished 10 epochs...
Completing Train Step...
At time: 76.33529901504517 and batch: 50, loss is 4.589814138412476 and perplexity is 98.47612553191063
At time: 77.29437971115112 and batch: 100, loss is 4.55240493774414 and perplexity is 94.86026724895214
At time: 78.23455715179443 and batch: 150, loss is 4.541911888122558 and perplexity is 93.87009779134421
At time: 79.17958378791809 and batch: 200, loss is 4.544285764694214 and perplexity is 94.09319851929594
At time: 80.12741088867188 and batch: 250, loss is 4.572533159255982 and perplexity is 96.7889813965934
At time: 81.08099436759949 and batch: 300, loss is 4.545057783126831 and perplexity is 94.16586825051337
At time: 82.03020405769348 and batch: 350, loss is 4.616376762390137 and perplexity is 101.12696053054036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.055203141837285 and perplexity of 156.83638714017138
Finished 11 epochs...
Completing Train Step...
At time: 83.86719036102295 and batch: 50, loss is 4.56915168762207 and perplexity is 96.46224493789393
At time: 84.81857943534851 and batch: 100, loss is 4.533069124221802 and perplexity is 93.04368594671078
At time: 85.76007652282715 and batch: 150, loss is 4.518818283081055 and perplexity is 91.72713839514827
At time: 86.71386957168579 and batch: 200, loss is 4.525012807846069 and perplexity is 92.29710794823889
At time: 87.65710544586182 and batch: 250, loss is 4.5533590984344485 and perplexity is 94.95082238223718
At time: 88.59736347198486 and batch: 300, loss is 4.525809736251831 and perplexity is 92.3706914518302
At time: 89.53732299804688 and batch: 350, loss is 4.597580528259277 and perplexity is 99.2439070997062
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.0523486959523165 and perplexity of 156.3893444933444
Finished 12 epochs...
Completing Train Step...
At time: 91.35834240913391 and batch: 50, loss is 4.548073263168335 and perplexity is 94.45025210824255
At time: 92.29972124099731 and batch: 100, loss is 4.5126048183441165 and perplexity is 91.15896205510731
At time: 93.24131393432617 and batch: 150, loss is 4.4991534805297855 and perplexity is 89.94096229004678
At time: 94.18670630455017 and batch: 200, loss is 4.507437572479248 and perplexity is 90.68913618365704
At time: 95.14816880226135 and batch: 250, loss is 4.535220394134521 and perplexity is 93.24406348466243
At time: 96.1003987789154 and batch: 300, loss is 4.5087125205993654 and perplexity is 90.80483386594857
At time: 97.04029488563538 and batch: 350, loss is 4.580187711715698 and perplexity is 97.5327005193968
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.05243235620959 and perplexity of 156.4024286134424
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 98.84524059295654 and batch: 50, loss is 4.513747148513794 and perplexity is 91.26315518784853
At time: 99.80867457389832 and batch: 100, loss is 4.451655931472779 and perplexity is 85.76885381772891
At time: 100.74481892585754 and batch: 150, loss is 4.411332588195801 and perplexity is 82.37916789929889
At time: 101.68818998336792 and batch: 200, loss is 4.400205507278442 and perplexity is 81.46760913139913
At time: 102.63586807250977 and batch: 250, loss is 4.407781925201416 and perplexity is 82.08718590791293
At time: 103.57380437850952 and batch: 300, loss is 4.367409162521362 and perplexity is 78.83910755492381
At time: 104.51001048088074 and batch: 350, loss is 4.459071245193481 and perplexity is 86.4072206964423
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.992163296403556 and perplexity of 147.25463460056534
Finished 14 epochs...
Completing Train Step...
At time: 106.30818843841553 and batch: 50, loss is 4.4686238479614255 and perplexity is 87.2365895608897
At time: 107.24352860450745 and batch: 100, loss is 4.420343084335327 and perplexity is 83.12479928323488
At time: 108.17988300323486 and batch: 150, loss is 4.38387095451355 and perplexity is 80.14768173148532
At time: 109.11698412895203 and batch: 200, loss is 4.381451444625855 and perplexity is 79.95399802736455
At time: 110.06391859054565 and batch: 250, loss is 4.398996562957763 and perplexity is 81.36917883838069
At time: 111.00624322891235 and batch: 300, loss is 4.371766147613525 and perplexity is 79.18335777317034
At time: 111.94362378120422 and batch: 350, loss is 4.463642654418945 and perplexity is 86.80312769927866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.988347020642511 and perplexity of 146.69374125042162
Finished 15 epochs...
Completing Train Step...
At time: 113.75112628936768 and batch: 50, loss is 4.457026872634888 and perplexity is 86.23075259032474
At time: 114.69239664077759 and batch: 100, loss is 4.409446573257446 and perplexity is 82.22394597947236
At time: 115.6327133178711 and batch: 150, loss is 4.373560304641724 and perplexity is 79.32555267288103
At time: 116.56956243515015 and batch: 200, loss is 4.37478702545166 and perplexity is 79.422922689807
At time: 117.51373434066772 and batch: 250, loss is 4.397258214950561 and perplexity is 81.227853760172
At time: 118.453449010849 and batch: 300, loss is 4.373410549163818 and perplexity is 79.31367412629129
At time: 119.39059138298035 and batch: 350, loss is 4.464126749038696 and perplexity is 86.84515879906928
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.9873025828394395 and perplexity of 146.54060874419528
Finished 16 epochs...
Completing Train Step...
At time: 121.17107915878296 and batch: 50, loss is 4.44984299659729 and perplexity is 85.61350133592839
At time: 122.11934518814087 and batch: 100, loss is 4.402639207839965 and perplexity is 81.66611835551413
At time: 123.05603289604187 and batch: 150, loss is 4.367022857666016 and perplexity is 78.8086575067636
At time: 124.00018429756165 and batch: 200, loss is 4.370151872634888 and perplexity is 79.05563717581035
At time: 124.93728041648865 and batch: 250, loss is 4.3953320598602295 and perplexity is 81.071546900128
At time: 125.875648021698 and batch: 300, loss is 4.3729753494262695 and perplexity is 79.27916434599584
At time: 126.81759452819824 and batch: 350, loss is 4.463164663314819 and perplexity is 86.76164649104214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.986860604121767 and perplexity of 146.47585522474841
Finished 17 epochs...
Completing Train Step...
At time: 128.59833192825317 and batch: 50, loss is 4.444648370742798 and perplexity is 85.16992433413762
At time: 129.55717277526855 and batch: 100, loss is 4.39731538772583 and perplexity is 81.23249791475892
At time: 130.49424958229065 and batch: 150, loss is 4.361847467422486 and perplexity is 78.40184556290633
At time: 131.43979787826538 and batch: 200, loss is 4.365819520950318 and perplexity is 78.71388119101756
At time: 132.3773376941681 and batch: 250, loss is 4.392842245101929 and perplexity is 80.8699448462096
At time: 133.31408619880676 and batch: 300, loss is 4.3722841262817385 and perplexity is 79.22438368773138
At time: 134.26515197753906 and batch: 350, loss is 4.461914234161377 and perplexity is 86.65322499969298
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.988052894329202 and perplexity of 146.65060110575905
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 136.08796572685242 and batch: 50, loss is 4.4396695804595945 and perplexity is 84.74693500379256
At time: 137.02549600601196 and batch: 100, loss is 4.387448263168335 and perplexity is 80.43490816950937
At time: 137.9674186706543 and batch: 150, loss is 4.346577672958374 and perplexity is 77.21375949565832
At time: 138.904723405838 and batch: 200, loss is 4.34299370765686 and perplexity is 76.93752336686
At time: 139.8430826663971 and batch: 250, loss is 4.367069492340088 and perplexity is 78.81233280851804
At time: 140.78416967391968 and batch: 300, loss is 4.340649509429932 and perplexity is 76.75737779196344
At time: 141.72028040885925 and batch: 350, loss is 4.4375680065155025 and perplexity is 84.56902006952377
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.977242305360991 and perplexity of 145.07376038066872
Finished 19 epochs...
Completing Train Step...
At time: 143.59557676315308 and batch: 50, loss is 4.43139310836792 and perplexity is 84.04842395231913
At time: 144.53112483024597 and batch: 100, loss is 4.381398944854737 and perplexity is 79.94980057095187
At time: 145.4687361717224 and batch: 150, loss is 4.342436332702636 and perplexity is 76.89465226704384
At time: 146.40547704696655 and batch: 200, loss is 4.340487546920777 and perplexity is 76.7449469811499
At time: 147.34125804901123 and batch: 250, loss is 4.366683406829834 and perplexity is 78.7819103819983
At time: 148.27753067016602 and batch: 300, loss is 4.342132997512818 and perplexity is 76.8713309503692
At time: 149.21375799179077 and batch: 350, loss is 4.438864860534668 and perplexity is 84.67876488922207
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.975904793574892 and perplexity of 144.87985222243682
Finished 20 epochs...
Completing Train Step...
At time: 151.01532983779907 and batch: 50, loss is 4.428366546630859 and perplexity is 83.79443076523859
At time: 151.97104120254517 and batch: 100, loss is 4.3789974975585935 and perplexity is 79.75803568733238
At time: 152.91669988632202 and batch: 150, loss is 4.340008773803711 and perplexity is 76.70821235814137
At time: 153.86660957336426 and batch: 200, loss is 4.339056997299195 and perplexity is 76.6352380170559
At time: 154.8026671409607 and batch: 250, loss is 4.367084074020386 and perplexity is 78.81348203313738
At time: 155.73934316635132 and batch: 300, loss is 4.343078880310059 and perplexity is 76.94407661893067
At time: 156.67611360549927 and batch: 350, loss is 4.439406137466431 and perplexity is 84.72461195812782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.9751607960668105 and perplexity of 144.7721020613081
Finished 21 epochs...
Completing Train Step...
At time: 158.55143022537231 and batch: 50, loss is 4.426101140975952 and perplexity is 83.60481724479637
At time: 159.48761534690857 and batch: 100, loss is 4.377257499694824 and perplexity is 79.6193775430388
At time: 160.4243950843811 and batch: 150, loss is 4.3382449722290035 and perplexity is 76.57303354174077
At time: 161.36178851127625 and batch: 200, loss is 4.337983694076538 and perplexity is 76.55302929445848
At time: 162.3021264076233 and batch: 250, loss is 4.367456693649292 and perplexity is 78.84285495568925
At time: 163.2560591697693 and batch: 300, loss is 4.343657789230346 and perplexity is 76.98863312708428
At time: 164.19308590888977 and batch: 350, loss is 4.439681148529052 and perplexity is 84.74791536789347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.9747467041015625 and perplexity of 144.71216550753155
Finished 22 epochs...
Completing Train Step...
At time: 165.99736881256104 and batch: 50, loss is 4.424278573989868 and perplexity is 83.45258063785639
At time: 166.94264817237854 and batch: 100, loss is 4.375864562988281 and perplexity is 79.50854999530625
At time: 167.88183116912842 and batch: 150, loss is 4.336790590286255 and perplexity is 76.46174804987729
At time: 168.81885814666748 and batch: 200, loss is 4.337097091674805 and perplexity is 76.48518727371933
At time: 169.75731468200684 and batch: 250, loss is 4.367753801345825 and perplexity is 78.8662832549049
At time: 170.69486093521118 and batch: 300, loss is 4.3440376567840575 and perplexity is 77.01788416622244
At time: 171.63247799873352 and batch: 350, loss is 4.4397725486755375 and perplexity is 84.75566169377467
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.974473098228717 and perplexity of 144.6725768252715
Finished 23 epochs...
Completing Train Step...
At time: 173.41349267959595 and batch: 50, loss is 4.422719106674195 and perplexity is 83.32254048897812
At time: 174.3660819530487 and batch: 100, loss is 4.3746577167510985 and perplexity is 79.41265327885559
At time: 175.3013951778412 and batch: 150, loss is 4.335549192428589 and perplexity is 76.36688749168384
At time: 176.23659706115723 and batch: 200, loss is 4.33626350402832 and perplexity is 76.42145673265513
At time: 177.17275381088257 and batch: 250, loss is 4.367950353622437 and perplexity is 78.88178612593896
At time: 178.1214780807495 and batch: 300, loss is 4.344273548126221 and perplexity is 77.03605416127702
At time: 179.05689764022827 and batch: 350, loss is 4.439759588241577 and perplexity is 84.75456323073679
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.974196335365033 and perplexity of 144.63254236889364
Finished 24 epochs...
Completing Train Step...
At time: 180.8365671634674 and batch: 50, loss is 4.421301412582397 and perplexity is 83.2044983092344
At time: 181.78594255447388 and batch: 100, loss is 4.373578729629517 and perplexity is 79.32701425868552
At time: 182.7224862575531 and batch: 150, loss is 4.334349508285523 and perplexity is 76.27532628095261
At time: 183.65903639793396 and batch: 200, loss is 4.335473194122314 and perplexity is 76.36108395811115
At time: 184.59559774398804 and batch: 250, loss is 4.368072643280029 and perplexity is 78.8914331424077
At time: 185.53216075897217 and batch: 300, loss is 4.344495439529419 and perplexity is 77.05314969603766
At time: 186.46818280220032 and batch: 350, loss is 4.439705171585083 and perplexity is 84.74995129626735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.973989552464978 and perplexity of 144.6026379243108
Finished 25 epochs...
Completing Train Step...
At time: 188.27271723747253 and batch: 50, loss is 4.419995241165161 and perplexity is 83.09588991778692
At time: 189.20928525924683 and batch: 100, loss is 4.372588396072388 and perplexity is 79.24849294204324
At time: 190.14578533172607 and batch: 150, loss is 4.333246784210205 and perplexity is 76.19126200068129
At time: 191.08240222930908 and batch: 200, loss is 4.334700126647949 and perplexity is 76.3020744998811
At time: 192.01903867721558 and batch: 250, loss is 4.368144226074219 and perplexity is 78.89708061375818
At time: 192.96561884880066 and batch: 300, loss is 4.3446431159973145 and perplexity is 77.06452947326694
At time: 193.90255856513977 and batch: 350, loss is 4.439654874801636 and perplexity is 84.74568875351682
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.973862220501077 and perplexity of 144.5842265586412
Finished 26 epochs...
Completing Train Step...
At time: 195.6982662677765 and batch: 50, loss is 4.418781099319458 and perplexity is 82.99506094335668
At time: 196.63824462890625 and batch: 100, loss is 4.371690301895142 and perplexity is 79.17735228226431
At time: 197.57893443107605 and batch: 150, loss is 4.3322504615783695 and perplexity is 76.1153887254089
At time: 198.51560497283936 and batch: 200, loss is 4.334019145965576 and perplexity is 76.25013194906587
At time: 199.46535325050354 and batch: 250, loss is 4.368181209564209 and perplexity is 78.89999855710687
At time: 200.40181136131287 and batch: 300, loss is 4.344748153686523 and perplexity is 77.0726245785009
At time: 201.3380630016327 and batch: 350, loss is 4.439530286788941 and perplexity is 84.73513111426185
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.973773824757543 and perplexity of 144.57144649329118
Finished 27 epochs...
Completing Train Step...
At time: 203.15066766738892 and batch: 50, loss is 4.4176381492614745 and perplexity is 82.9002559226617
At time: 204.10074257850647 and batch: 100, loss is 4.3708008193969725 and perplexity is 79.10695672560693
At time: 205.0375599861145 and batch: 150, loss is 4.331303014755249 and perplexity is 76.04330759408185
At time: 205.97316670417786 and batch: 200, loss is 4.333323125839233 and perplexity is 76.19707878776882
At time: 206.9093062877655 and batch: 250, loss is 4.368144817352295 and perplexity is 78.89712726388602
At time: 207.8455958366394 and batch: 300, loss is 4.344761075973511 and perplexity is 77.07362053950966
At time: 208.78175902366638 and batch: 350, loss is 4.439317026138306 and perplexity is 84.71706237181316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.973680167362608 and perplexity of 144.55790694228014
Finished 28 epochs...
Completing Train Step...
At time: 210.64290404319763 and batch: 50, loss is 4.416551122665405 and perplexity is 82.81019010049353
At time: 211.5784192085266 and batch: 100, loss is 4.369994564056396 and perplexity is 79.04320202400815
At time: 212.51620745658875 and batch: 150, loss is 4.330422773361206 and perplexity is 75.97640057848004
At time: 213.4527382850647 and batch: 200, loss is 4.332694702148437 and perplexity is 76.14920978087348
At time: 214.3892240524292 and batch: 250, loss is 4.368140230178833 and perplexity is 78.89676534990765
At time: 215.32652688026428 and batch: 300, loss is 4.344775199890137 and perplexity is 77.07470912858774
At time: 216.28075337409973 and batch: 350, loss is 4.439178867340088 and perplexity is 84.70535877278353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.973597033270474 and perplexity of 144.54588975145174
Finished 29 epochs...
Completing Train Step...
At time: 218.14538478851318 and batch: 50, loss is 4.4155425930023195 and perplexity is 82.72671566766762
At time: 219.08176565170288 and batch: 100, loss is 4.36916669845581 and perplexity is 78.97779195520106
At time: 220.01858353614807 and batch: 150, loss is 4.32956166267395 and perplexity is 75.91100464857078
At time: 220.95590662956238 and batch: 200, loss is 4.332029790878296 and perplexity is 76.09859414239682
At time: 221.8960883617401 and batch: 250, loss is 4.368051929473877 and perplexity is 78.88979901747908
At time: 222.8400957584381 and batch: 300, loss is 4.344720773696899 and perplexity is 77.07051435972866
At time: 223.7785828113556 and batch: 350, loss is 4.438900194168091 and perplexity is 84.68175695051924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.973445497710129 and perplexity of 144.52398756857437
Finished 30 epochs...
Completing Train Step...
At time: 225.55803155899048 and batch: 50, loss is 4.414580039978027 and perplexity is 82.64712512851412
At time: 226.51964592933655 and batch: 100, loss is 4.368365268707276 and perplexity is 78.91452215978916
At time: 227.45495557785034 and batch: 150, loss is 4.3286925792694095 and perplexity is 75.8450603139228
At time: 228.39058995246887 and batch: 200, loss is 4.331402940750122 and perplexity is 76.05090667691329
At time: 229.3261435031891 and batch: 250, loss is 4.367948999404907 and perplexity is 78.88167930291378
At time: 230.26754665374756 and batch: 300, loss is 4.344663343429565 and perplexity is 77.06608830658136
At time: 231.2295787334442 and batch: 350, loss is 4.438696050643921 and perplexity is 84.66447148264069
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.973404456829202 and perplexity of 144.51805629852262
Finished 31 epochs...
Completing Train Step...
At time: 233.04726243019104 and batch: 50, loss is 4.413659439086914 and perplexity is 82.57107512269553
At time: 233.9968478679657 and batch: 100, loss is 4.367637453079223 and perplexity is 78.85710783333742
At time: 234.93315267562866 and batch: 150, loss is 4.327935390472412 and perplexity is 75.7876530207679
At time: 235.8698000907898 and batch: 200, loss is 4.3308487892150875 and perplexity is 76.00877462508628
At time: 236.8064911365509 and batch: 250, loss is 4.367927045822143 and perplexity is 78.87994758644739
At time: 237.74497842788696 and batch: 300, loss is 4.34461672782898 and perplexity is 77.06249590832171
At time: 238.681499004364 and batch: 350, loss is 4.438487100601196 and perplexity is 84.64678268580887
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.973379727067618 and perplexity of 144.5144824456362
Finished 32 epochs...
Completing Train Step...
At time: 240.50084280967712 and batch: 50, loss is 4.412794218063355 and perplexity is 82.49966379030944
At time: 241.4388632774353 and batch: 100, loss is 4.366898317337036 and perplexity is 78.79884326177854
At time: 242.3765857219696 and batch: 150, loss is 4.327128868103028 and perplexity is 75.72655322577126
At time: 243.31310415267944 and batch: 200, loss is 4.330227537155151 and perplexity is 75.96156868219012
At time: 244.24982452392578 and batch: 250, loss is 4.367769565582275 and perplexity is 78.86752653144173
At time: 245.18674397468567 and batch: 300, loss is 4.344534616470337 and perplexity is 77.05616846186348
At time: 246.1233503818512 and batch: 350, loss is 4.438272523880005 and perplexity is 84.62862140528445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.973211880387931 and perplexity of 144.49022820514475
Finished 33 epochs...
Completing Train Step...
At time: 247.93110013008118 and batch: 50, loss is 4.411907377243042 and perplexity is 82.42653215365155
At time: 248.8675582408905 and batch: 100, loss is 4.36614504814148 and perplexity is 78.73950887069411
At time: 249.8039722442627 and batch: 150, loss is 4.326365442276001 and perplexity is 75.66876368106918
At time: 250.7534441947937 and batch: 200, loss is 4.329606447219849 and perplexity is 75.91440436456946
At time: 251.69205236434937 and batch: 250, loss is 4.3676511096954345 and perplexity is 78.85818476194822
At time: 252.62827110290527 and batch: 300, loss is 4.344441585540771 and perplexity is 77.04900018832342
At time: 253.56705355644226 and batch: 350, loss is 4.43806622505188 and perplexity is 84.6111644206024
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.973192412277748 and perplexity of 144.48741528084298
Finished 34 epochs...
Completing Train Step...
At time: 255.36643624305725 and batch: 50, loss is 4.411082992553711 and perplexity is 82.35860898380677
At time: 256.32711362838745 and batch: 100, loss is 4.3654521942138675 and perplexity is 78.68497278766613
At time: 257.27812147140503 and batch: 150, loss is 4.32561469078064 and perplexity is 75.61197656275772
At time: 258.2233030796051 and batch: 200, loss is 4.329035873413086 and perplexity is 75.87110194866996
At time: 259.1659460067749 and batch: 250, loss is 4.367508172988892 and perplexity is 78.84691383826794
At time: 260.1114327907562 and batch: 300, loss is 4.344359388351441 and perplexity is 77.04266723734615
At time: 261.0476279258728 and batch: 350, loss is 4.437908105850219 and perplexity is 84.5977868284876
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.973149792901401 and perplexity of 144.4812574485361
Finished 35 epochs...
Completing Train Step...
At time: 262.8516983985901 and batch: 50, loss is 4.410263242721558 and perplexity is 82.2911231924282
At time: 263.7930383682251 and batch: 100, loss is 4.364789581298828 and perplexity is 78.63285237821266
At time: 264.72953844070435 and batch: 150, loss is 4.324866018295288 and perplexity is 75.55538914170788
At time: 265.66541600227356 and batch: 200, loss is 4.328418121337891 and perplexity is 75.82424689188817
At time: 266.602801322937 and batch: 250, loss is 4.367371206283569 and perplexity is 78.83611517580017
At time: 267.5386049747467 and batch: 300, loss is 4.344223575592041 and perplexity is 77.03220457061505
At time: 268.49442625045776 and batch: 350, loss is 4.437646932601929 and perplexity is 84.57569503472176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.973100333378233 and perplexity of 144.47411165115136
Finished 36 epochs...
Completing Train Step...
At time: 270.37311577796936 and batch: 50, loss is 4.409471073150635 and perplexity is 82.22596048204387
At time: 271.3101427555084 and batch: 100, loss is 4.364164962768554 and perplexity is 78.58375217757292
At time: 272.2475895881653 and batch: 150, loss is 4.324151916503906 and perplexity is 75.50145416279773
At time: 273.19445276260376 and batch: 200, loss is 4.327887525558472 and perplexity is 75.78402553809362
At time: 274.1467173099518 and batch: 250, loss is 4.36721435546875 and perplexity is 78.82375063661705
At time: 275.0942692756653 and batch: 300, loss is 4.344078073501587 and perplexity is 77.02099703919711
At time: 276.0321252346039 and batch: 350, loss is 4.437434720993042 and perplexity is 84.55774899465206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.973003518992457 and perplexity of 144.46012515582925
Finished 37 epochs...
Completing Train Step...
At time: 277.81804299354553 and batch: 50, loss is 4.408662977218628 and perplexity is 82.15954085820137
At time: 278.77383971214294 and batch: 100, loss is 4.36351128578186 and perplexity is 78.53240057275605
At time: 279.7096436023712 and batch: 150, loss is 4.323453979492188 and perplexity is 75.44877728820747
At time: 280.6461479663849 and batch: 200, loss is 4.327375450134277 and perplexity is 75.74522833546382
At time: 281.58234786987305 and batch: 250, loss is 4.36703405380249 and perplexity is 78.80953986418785
At time: 282.52054023742676 and batch: 300, loss is 4.3439570331573485 and perplexity is 77.01167495538742
At time: 283.4644179344177 and batch: 350, loss is 4.437225704193115 and perplexity is 84.54007685149993
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.972931434368265 and perplexity of 144.44971217730892
Finished 38 epochs...
Completing Train Step...
At time: 285.25959300994873 and batch: 50, loss is 4.407938165664673 and perplexity is 82.10001224983735
At time: 286.21043372154236 and batch: 100, loss is 4.362909212112426 and perplexity is 78.4851325130287
At time: 287.14990639686584 and batch: 150, loss is 4.3227083778381346 and perplexity is 75.3925435216649
At time: 288.08641505241394 and batch: 200, loss is 4.326754779815674 and perplexity is 75.69823010717155
At time: 289.02287793159485 and batch: 250, loss is 4.36684365272522 and perplexity is 78.79453587133209
At time: 289.9681625366211 and batch: 300, loss is 4.343779268264771 and perplexity is 76.99798619998786
At time: 290.9087266921997 and batch: 350, loss is 4.4369791316986085 and perplexity is 84.51923416358724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.972846195615571 and perplexity of 144.43739998876126
Finished 39 epochs...
Completing Train Step...
At time: 292.80224800109863 and batch: 50, loss is 4.407195167541504 and perplexity is 82.03903475071247
At time: 293.7570037841797 and batch: 100, loss is 4.362260141372681 and perplexity is 78.43420663904466
At time: 294.6993999481201 and batch: 150, loss is 4.322025356292724 and perplexity is 75.3410663720787
At time: 295.6391234397888 and batch: 200, loss is 4.326202878952026 and perplexity is 75.6564637151129
At time: 296.5747148990631 and batch: 250, loss is 4.366643877029419 and perplexity is 78.77879621035628
At time: 297.5102574825287 and batch: 300, loss is 4.3435742473602295 and perplexity is 76.98220162134886
At time: 298.4593415260315 and batch: 350, loss is 4.436779499053955 and perplexity is 84.50236304941569
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.97283672464305 and perplexity of 144.43603203259278
Finished 40 epochs...
Completing Train Step...
At time: 300.27337551116943 and batch: 50, loss is 4.406546182632447 and perplexity is 81.98580992812707
At time: 301.22648644447327 and batch: 100, loss is 4.361649265289307 and perplexity is 78.38630768974127
At time: 302.1714849472046 and batch: 150, loss is 4.321341505050659 and perplexity is 75.28956190296644
At time: 303.1145935058594 and batch: 200, loss is 4.325674228668213 and perplexity is 75.6164784741333
At time: 304.0564224720001 and batch: 250, loss is 4.366476306915283 and perplexity is 78.76559634446623
At time: 305.0040202140808 and batch: 300, loss is 4.343444890975952 and perplexity is 76.97224412614011
At time: 305.9436993598938 and batch: 350, loss is 4.436584310531616 and perplexity is 84.48587076764235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.972718863651671 and perplexity of 144.41900966182303
Finished 41 epochs...
Completing Train Step...
At time: 307.73879051208496 and batch: 50, loss is 4.40581018447876 and perplexity is 81.92549072352595
At time: 308.6907207965851 and batch: 100, loss is 4.3609994029998775 and perplexity is 78.3353839328729
At time: 309.6299283504486 and batch: 150, loss is 4.320646667480469 and perplexity is 75.23726605737626
At time: 310.5777883529663 and batch: 200, loss is 4.325098991394043 and perplexity is 75.5729935654428
At time: 311.5226001739502 and batch: 250, loss is 4.366302328109741 and perplexity is 78.75189399209046
At time: 312.4666130542755 and batch: 300, loss is 4.343282833099365 and perplexity is 76.95977117839786
At time: 313.4266777038574 and batch: 350, loss is 4.436322956085205 and perplexity is 84.46379289485911
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.972649409853179 and perplexity of 144.4089795613459
Finished 42 epochs...
Completing Train Step...
At time: 315.29123973846436 and batch: 50, loss is 4.405112447738648 and perplexity is 81.8683482362211
At time: 316.2304241657257 and batch: 100, loss is 4.3603560256958005 and perplexity is 78.28500093412123
At time: 317.1696922779083 and batch: 150, loss is 4.319915409088135 and perplexity is 75.1822682863971
At time: 318.1187846660614 and batch: 200, loss is 4.3244991207122805 and perplexity is 75.52767313682709
At time: 319.06726121902466 and batch: 250, loss is 4.366052265167236 and perplexity is 78.73220352378179
At time: 320.01313042640686 and batch: 300, loss is 4.343086128234863 and perplexity is 76.94463430583318
At time: 320.95580196380615 and batch: 350, loss is 4.436071147918701 and perplexity is 84.4425268996223
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.972644674366918 and perplexity of 144.4082957162264
Finished 43 epochs...
Completing Train Step...
At time: 322.7700181007385 and batch: 50, loss is 4.404419965744019 and perplexity is 81.81167550382557
At time: 323.70937275886536 and batch: 100, loss is 4.359778060913086 and perplexity is 78.23976803333748
At time: 324.6601414680481 and batch: 150, loss is 4.319308776855468 and perplexity is 75.13667412997027
At time: 325.6067690849304 and batch: 200, loss is 4.3239945125579835 and perplexity is 75.48957087125262
At time: 326.56780910491943 and batch: 250, loss is 4.365856685638428 and perplexity is 78.716806622223
At time: 327.5167818069458 and batch: 300, loss is 4.342927522659302 and perplexity is 76.93243142557075
At time: 328.45949482917786 and batch: 350, loss is 4.4358526229858395 and perplexity is 84.42407611815219
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.972583639210668 and perplexity of 144.3994820023095
Finished 44 epochs...
Completing Train Step...
At time: 330.2556574344635 and batch: 50, loss is 4.40376277923584 and perplexity is 81.75792763759365
At time: 331.20793318748474 and batch: 100, loss is 4.359183588027954 and perplexity is 78.19327043485279
At time: 332.1506290435791 and batch: 150, loss is 4.318589239120484 and perplexity is 75.08262990341441
At time: 333.09232544898987 and batch: 200, loss is 4.323400573730469 and perplexity is 75.4447479963803
At time: 334.04168677330017 and batch: 250, loss is 4.365582847595215 and perplexity is 78.69525391703961
At time: 334.99037194252014 and batch: 300, loss is 4.342724857330322 and perplexity is 76.91684146887187
At time: 335.93262100219727 and batch: 350, loss is 4.435597686767578 and perplexity is 84.40255610668815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.972547333816002 and perplexity of 144.39423961728988
Finished 45 epochs...
Completing Train Step...
At time: 337.7755563259125 and batch: 50, loss is 4.4030835151672365 and perplexity is 81.70241127229664
At time: 338.72825360298157 and batch: 100, loss is 4.358596467971802 and perplexity is 78.1473750718868
At time: 339.66722297668457 and batch: 150, loss is 4.317955083847046 and perplexity is 75.03503095187585
At time: 340.6071729660034 and batch: 200, loss is 4.322869682312012 and perplexity is 75.40470565710902
At time: 341.55177998542786 and batch: 250, loss is 4.365370683670044 and perplexity is 78.67855939412658
At time: 342.4948089122772 and batch: 300, loss is 4.342499589920044 and perplexity is 76.89951656262903
At time: 343.4440953731537 and batch: 350, loss is 4.435306072235107 and perplexity is 84.3779466831569
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.972558383283944 and perplexity of 144.39583510562625
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 345.2525804042816 and batch: 50, loss is 4.402374515533447 and perplexity is 81.6445048228763
At time: 346.2067220211029 and batch: 100, loss is 4.357012567520141 and perplexity is 78.02369538322837
At time: 347.16765427589417 and batch: 150, loss is 4.314484176635742 and perplexity is 74.77504278026505
At time: 348.1157896518707 and batch: 200, loss is 4.318772764205932 and perplexity is 75.0964107140067
At time: 349.0655891895294 and batch: 250, loss is 4.360314607620239 and perplexity is 78.28175858718357
At time: 350.01119899749756 and batch: 300, loss is 4.336490879058838 and perplexity is 76.43883503933355
At time: 350.95496582984924 and batch: 350, loss is 4.430270767211914 and perplexity is 83.9541458630063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.971338732489224 and perplexity of 144.21982996480833
Finished 47 epochs...
Completing Train Step...
At time: 352.76150369644165 and batch: 50, loss is 4.401006498336792 and perplexity is 81.53289009910395
At time: 353.7006027698517 and batch: 100, loss is 4.356385707855225 and perplexity is 77.9748008023063
At time: 354.6413679122925 and batch: 150, loss is 4.314135980606079 and perplexity is 74.74901093960591
At time: 355.589802980423 and batch: 200, loss is 4.318941478729248 and perplexity is 75.10908163799742
At time: 356.52959060668945 and batch: 250, loss is 4.360343570709229 and perplexity is 78.28402590155784
At time: 357.47266149520874 and batch: 300, loss is 4.336455507278442 and perplexity is 76.43613130946501
At time: 358.4193375110626 and batch: 350, loss is 4.430073928833008 and perplexity is 83.93762209134178
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.9708972799366915 and perplexity of 144.1561778034828
Finished 48 epochs...
Completing Train Step...
At time: 360.2977294921875 and batch: 50, loss is 4.4003650665283205 and perplexity is 81.4806090791049
At time: 361.2612850666046 and batch: 100, loss is 4.356036853790283 and perplexity is 77.94760372026558
At time: 362.20074820518494 and batch: 150, loss is 4.314091892242431 and perplexity is 74.74571545067617
At time: 363.1441345214844 and batch: 200, loss is 4.319229736328125 and perplexity is 75.13073552232264
At time: 364.088595867157 and batch: 250, loss is 4.360495386123657 and perplexity is 78.29591152558083
At time: 365.0322570800781 and batch: 300, loss is 4.33640417098999 and perplexity is 76.43220746289866
At time: 365.97215962409973 and batch: 350, loss is 4.429806299209595 and perplexity is 83.91516090292507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.970713122137662 and perplexity of 144.12963276337504
Finished 49 epochs...
Completing Train Step...
At time: 367.79719138145447 and batch: 50, loss is 4.399845714569092 and perplexity is 81.43830295197645
At time: 368.749276638031 and batch: 100, loss is 4.355759992599487 and perplexity is 77.92602604102866
At time: 369.69363045692444 and batch: 150, loss is 4.314122724533081 and perplexity is 74.74802006782788
At time: 370.6377112865448 and batch: 200, loss is 4.319544439315796 and perplexity is 75.15438311004552
At time: 371.5949387550354 and batch: 250, loss is 4.360631265640259 and perplexity is 78.3065510590218
At time: 372.5357642173767 and batch: 300, loss is 4.336339378356934 and perplexity is 76.42725537935785
At time: 373.4753224849701 and batch: 350, loss is 4.429523591995239 and perplexity is 83.89144083461917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.970607889109645 and perplexity of 144.11446636370945
Finished Training.
Improved accuracyfrom -10000000 to -144.11446636370945
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efbebe31898>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'wordvec_dim': 200, 'seq_len': 35, 'lr': 3.0211221666783503, 'anneal': 4.808244963475975, 'tune_wordvecs': True, 'wordvec_source': '', 'batch_size': 80, 'num_layers': 1, 'dropout': 0.9129203945657647}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.293463945388794 and batch: 50, loss is 8.270383100509644 and perplexity is 3906.445266696502
At time: 2.2709240913391113 and batch: 100, loss is 7.057803392410278 and perplexity is 1161.8901439014205
At time: 3.2157938480377197 and batch: 150, loss is 6.673100385665894 and perplexity is 790.8437279962916
At time: 4.160120964050293 and batch: 200, loss is 6.467306461334228 and perplexity is 643.7474310282328
At time: 5.11107325553894 and batch: 250, loss is 6.395431604385376 and perplexity is 599.101842412365
At time: 6.063479661941528 and batch: 300, loss is 6.329906187057495 and perplexity is 561.1039525709707
At time: 7.01775050163269 and batch: 350, loss is 6.299590034484863 and perplexity is 544.3487001795818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 6.094903880152209 and perplexity of 443.5914051895363
Finished 1 epochs...
Completing Train Step...
At time: 8.851255655288696 and batch: 50, loss is 5.710668125152588 and perplexity is 302.0728233276535
At time: 9.803909063339233 and batch: 100, loss is 5.3941612052917485 and perplexity is 220.11743629934352
At time: 10.741291284561157 and batch: 150, loss is 5.23040060043335 and perplexity is 186.8676477885271
At time: 11.678598642349243 and batch: 200, loss is 5.098284645080566 and perplexity is 163.7407926889157
At time: 12.61536955833435 and batch: 250, loss is 5.022700786590576 and perplexity is 151.82078612629195
At time: 13.561381340026855 and batch: 300, loss is 4.961347770690918 and perplexity is 142.78610920381382
At time: 14.498266220092773 and batch: 350, loss is 4.959056339263916 and perplexity is 142.4592992001986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.019604913119612 and perplexity of 151.35149498999618
Finished 2 epochs...
Completing Train Step...
At time: 16.32017755508423 and batch: 50, loss is 4.8328993606567385 and perplexity is 125.57451917683616
At time: 17.255995750427246 and batch: 100, loss is 4.7300280094146725 and perplexity is 113.29873573548839
At time: 18.216840982437134 and batch: 150, loss is 4.65782769203186 and perplexity is 105.40685711749832
At time: 19.167895317077637 and batch: 200, loss is 4.658126010894775 and perplexity is 105.43830666201997
At time: 20.111247301101685 and batch: 250, loss is 4.644384155273437 and perplexity is 103.99929864009967
At time: 21.048960208892822 and batch: 300, loss is 4.602547931671142 and perplexity is 99.73811807886729
At time: 21.986000776290894 and batch: 350, loss is 4.64482385635376 and perplexity is 104.04503729899648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.818263612944504 and perplexity of 123.75002617259618
Finished 3 epochs...
Completing Train Step...
At time: 23.799058198928833 and batch: 50, loss is 4.556336164474487 and perplexity is 95.2339184397664
At time: 24.740578413009644 and batch: 100, loss is 4.478674592971802 and perplexity is 88.11780328721957
At time: 25.68125629425049 and batch: 150, loss is 4.4101237869262695 and perplexity is 82.27964801855656
At time: 26.639753818511963 and batch: 200, loss is 4.450887098312378 and perplexity is 85.70293722146373
At time: 27.590323209762573 and batch: 250, loss is 4.451296720504761 and perplexity is 85.73805023754642
At time: 28.53346300125122 and batch: 300, loss is 4.410310497283936 and perplexity is 82.29501191532145
At time: 29.473809480667114 and batch: 350, loss is 4.46893427848816 and perplexity is 87.26367466514165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.726939234240302 and perplexity of 112.94932132234335
Finished 4 epochs...
Completing Train Step...
At time: 31.306397199630737 and batch: 50, loss is 4.390222597122192 and perplexity is 80.65837130366805
At time: 32.26304030418396 and batch: 100, loss is 4.321736526489258 and perplexity is 75.31930876895889
At time: 33.20604181289673 and batch: 150, loss is 4.25284574508667 and perplexity is 70.30519861499133
At time: 34.16128921508789 and batch: 200, loss is 4.31338716506958 and perplexity is 74.69305867045507
At time: 35.10287833213806 and batch: 250, loss is 4.319878005981446 and perplexity is 75.1794562885843
At time: 36.049824714660645 and batch: 300, loss is 4.278110704421997 and perplexity is 72.10408531118789
At time: 36.995442152023315 and batch: 350, loss is 4.343411254882812 and perplexity is 76.96965512410968
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.675017784381735 and perplexity of 107.23447417391017
Finished 5 epochs...
Completing Train Step...
At time: 38.84072923660278 and batch: 50, loss is 4.270129013061523 and perplexity is 71.5308634396375
At time: 39.787100315093994 and batch: 100, loss is 4.207458066940307 and perplexity is 67.18554142009154
At time: 40.732563734054565 and batch: 150, loss is 4.135937070846557 and perplexity is 62.54817568915312
At time: 41.678244829177856 and batch: 200, loss is 4.208008308410644 and perplexity is 67.22251986379153
At time: 42.624486446380615 and batch: 250, loss is 4.218234190940857 and perplexity is 67.91345615111145
At time: 43.58280038833618 and batch: 300, loss is 4.17644356250763 and perplexity is 65.13379653504802
At time: 44.53523564338684 and batch: 350, loss is 4.245447835922241 and perplexity is 69.78700627512663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.646002933896821 and perplexity of 104.16778681732217
Finished 6 epochs...
Completing Train Step...
At time: 46.39506530761719 and batch: 50, loss is 4.174754815101624 and perplexity is 65.0238948293351
At time: 47.34957695007324 and batch: 100, loss is 4.116839051246643 and perplexity is 61.364963868125066
At time: 48.312986850738525 and batch: 150, loss is 4.043462605476379 and perplexity is 57.023451068407184
At time: 49.25813126564026 and batch: 200, loss is 4.121935749053955 and perplexity is 61.67852091892941
At time: 50.20409870147705 and batch: 250, loss is 4.135086388587951 and perplexity is 62.494989691186014
At time: 51.149556159973145 and batch: 300, loss is 4.0931038618087765 and perplexity is 59.925604136233375
At time: 52.088969230651855 and batch: 350, loss is 4.164768953323364 and perplexity is 64.37780644677512
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.627953364931304 and perplexity of 102.30446978655803
Finished 7 epochs...
Completing Train Step...
At time: 53.91767120361328 and batch: 50, loss is 4.095405149459839 and perplexity is 60.06366899155608
At time: 54.888551235198975 and batch: 100, loss is 4.041316661834717 and perplexity is 56.90121316090104
At time: 55.83931565284729 and batch: 150, loss is 3.966725583076477 and perplexity is 52.81132113037159
At time: 56.790852785110474 and batch: 200, loss is 4.050113420486451 and perplexity is 57.40396746214566
At time: 57.740678548812866 and batch: 250, loss is 4.064187221527099 and perplexity is 58.21757129912336
At time: 58.68952655792236 and batch: 300, loss is 4.022828001976013 and perplexity is 55.85885161170337
At time: 59.65663409233093 and batch: 350, loss is 4.0957402324676515 and perplexity is 60.083798678791766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.617873619342673 and perplexity of 101.27844647260741
Finished 8 epochs...
Completing Train Step...
At time: 61.47103548049927 and batch: 50, loss is 4.026984219551086 and perplexity is 56.09149627852715
At time: 62.434388160705566 and batch: 100, loss is 3.9768582248687743 and perplexity is 53.34915959052544
At time: 63.38150429725647 and batch: 150, loss is 3.900894727706909 and perplexity is 49.44667062569364
At time: 64.31738233566284 and batch: 200, loss is 3.987138366699219 and perplexity is 53.90042520714919
At time: 65.25612592697144 and batch: 250, loss is 4.002411570549011 and perplexity is 54.72997621397415
At time: 66.19723510742188 and batch: 300, loss is 3.9614287185668946 and perplexity is 52.532326269069216
At time: 67.1469349861145 and batch: 350, loss is 4.034830818176269 and perplexity is 56.53335501311194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.611193952889278 and perplexity of 100.60419462694543
Finished 9 epochs...
Completing Train Step...
At time: 68.97005796432495 and batch: 50, loss is 3.9669137382507325 and perplexity is 52.821258788583116
At time: 69.90671062469482 and batch: 100, loss is 3.9197710084915163 and perplexity is 50.38890482551351
At time: 70.8532874584198 and batch: 150, loss is 3.8421596622467042 and perplexity is 46.626062329284196
At time: 71.80602622032166 and batch: 200, loss is 3.9304751539230347 and perplexity is 50.93117206692709
At time: 72.76926064491272 and batch: 250, loss is 3.9474566650390623 and perplexity is 51.80344563166693
At time: 73.71036028862 and batch: 300, loss is 3.906413869857788 and perplexity is 49.72032831294757
At time: 74.64999556541443 and batch: 350, loss is 3.9801779842376708 and perplexity is 53.52656026374755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.605382458917026 and perplexity of 100.02122954604269
Finished 10 epochs...
Completing Train Step...
At time: 76.46284818649292 and batch: 50, loss is 3.912203140258789 and perplexity is 50.009007552794785
At time: 77.40027785301208 and batch: 100, loss is 3.868034348487854 and perplexity is 47.84824061602437
At time: 78.33738446235657 and batch: 150, loss is 3.789568943977356 and perplexity is 44.2373273991384
At time: 79.28199672698975 and batch: 200, loss is 3.879418716430664 and perplexity is 48.39607505022452
At time: 80.22283625602722 and batch: 250, loss is 3.8976391983032226 and perplexity is 49.28595728100985
At time: 81.16339802742004 and batch: 300, loss is 3.855949115753174 and perplexity is 47.27346364460404
At time: 82.0989818572998 and batch: 350, loss is 3.9301248502731325 and perplexity is 50.91333381604286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.607234034044989 and perplexity of 100.20659792572044
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 83.8755853176117 and batch: 50, loss is 3.8713638591766357 and perplexity is 48.00781735340371
At time: 84.82079195976257 and batch: 100, loss is 3.816584906578064 and perplexity is 45.4487313243425
At time: 85.75302648544312 and batch: 150, loss is 3.7180168056488037 and perplexity is 41.18263988935731
At time: 86.701828956604 and batch: 200, loss is 3.7887127733230592 and perplexity is 44.199468906572534
At time: 87.64253664016724 and batch: 250, loss is 3.784515438079834 and perplexity is 44.01433771862658
At time: 88.58392310142517 and batch: 300, loss is 3.7163282442092895 and perplexity is 41.113159149420724
At time: 89.5225510597229 and batch: 350, loss is 3.75890784740448 and perplexity is 42.90154534891753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.56415163237473 and perplexity of 95.98113218144321
Finished 12 epochs...
Completing Train Step...
At time: 91.35756587982178 and batch: 50, loss is 3.820347809791565 and perplexity is 45.62007266976731
At time: 92.29772162437439 and batch: 100, loss is 3.768032183647156 and perplexity is 43.29478477015329
At time: 93.23909878730774 and batch: 150, loss is 3.6760577964782715 and perplexity is 39.49040758549026
At time: 94.18100595474243 and batch: 200, loss is 3.7551281785964967 and perplexity is 42.73969777394143
At time: 95.13080716133118 and batch: 250, loss is 3.759901456832886 and perplexity is 42.9441939133685
At time: 96.08604073524475 and batch: 300, loss is 3.701085238456726 and perplexity is 40.49122315710317
At time: 97.04975056648254 and batch: 350, loss is 3.757908000946045 and perplexity is 42.85867182777986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.563856453731142 and perplexity of 95.95280478206318
Finished 13 epochs...
Completing Train Step...
At time: 98.85995101928711 and batch: 50, loss is 3.7963979387283326 and perplexity is 44.54045773543845
At time: 99.79636883735657 and batch: 100, loss is 3.745137529373169 and perplexity is 42.314826350221594
At time: 100.73342418670654 and batch: 150, loss is 3.654611015319824 and perplexity is 38.652482966821495
At time: 101.66905474662781 and batch: 200, loss is 3.73657244682312 and perplexity is 41.95394406891627
At time: 102.60535144805908 and batch: 250, loss is 3.745194983482361 and perplexity is 42.31725758071658
At time: 103.54012846946716 and batch: 300, loss is 3.690528826713562 and perplexity is 40.06602934251017
At time: 104.47856140136719 and batch: 350, loss is 3.7532317543029787 and perplexity is 42.65872197928601
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.56518870386584 and perplexity of 96.08072310984339
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 106.28149127960205 and batch: 50, loss is 3.7854010343551634 and perplexity is 44.05333391706666
At time: 107.24946546554565 and batch: 100, loss is 3.7368349123001097 and perplexity is 41.96495697604866
At time: 108.19844841957092 and batch: 150, loss is 3.639540920257568 and perplexity is 38.07435354819966
At time: 109.14819145202637 and batch: 200, loss is 3.71531662940979 and perplexity is 41.07158949895288
At time: 110.09146070480347 and batch: 250, loss is 3.7165705347061158 and perplexity is 41.123121684042125
At time: 111.03249454498291 and batch: 300, loss is 3.653405313491821 and perplexity is 38.60590768104741
At time: 111.97254347801208 and batch: 350, loss is 3.7064352321624754 and perplexity is 40.708431459571706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.5595113820043105 and perplexity of 95.53678743003312
Finished 15 epochs...
Completing Train Step...
At time: 113.77318120002747 and batch: 50, loss is 3.774300479888916 and perplexity is 43.56702164612591
At time: 114.73141145706177 and batch: 100, loss is 3.723913159370422 and perplexity is 41.42618460863167
At time: 115.66830778121948 and batch: 150, loss is 3.6282214307785035 and perplexity is 37.64580137557935
At time: 116.60524845123291 and batch: 200, loss is 3.706604104042053 and perplexity is 40.71530654939528
At time: 117.5445909500122 and batch: 250, loss is 3.7110718297958374 and perplexity is 40.89761832930252
At time: 118.50078105926514 and batch: 300, loss is 3.6521027183532713 and perplexity is 38.555652551532354
At time: 119.4513201713562 and batch: 350, loss is 3.7092983961105346 and perplexity is 40.82515339018295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.558945228313577 and perplexity of 95.48271423354305
Finished 16 epochs...
Completing Train Step...
At time: 121.2730085849762 and batch: 50, loss is 3.7679255962371827 and perplexity is 43.290170337103895
At time: 122.21219182014465 and batch: 100, loss is 3.716992712020874 and perplexity is 41.14048659840772
At time: 123.15668892860413 and batch: 150, loss is 3.621908411979675 and perplexity is 37.408891319438716
At time: 124.1101644039154 and batch: 200, loss is 3.7016695833206175 and perplexity is 40.514890909779105
At time: 125.06414723396301 and batch: 250, loss is 3.707843770980835 and perplexity is 40.765811266873335
At time: 126.01751780509949 and batch: 300, loss is 3.6508266305923462 and perplexity is 38.506483533859935
At time: 126.96609950065613 and batch: 350, loss is 3.7101121759414672 and perplexity is 40.85838959825031
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.559055196827855 and perplexity of 95.49321490312745
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 128.75598812103271 and batch: 50, loss is 3.765245246887207 and perplexity is 43.17429292253148
At time: 129.69045114517212 and batch: 100, loss is 3.7147827911376954 and perplexity is 41.049669763899885
At time: 130.62459421157837 and batch: 150, loss is 3.6178057527542116 and perplexity is 37.25572978597391
At time: 131.55986976623535 and batch: 200, loss is 3.6965150308609007 and perplexity is 40.30659208399524
At time: 132.49430584907532 and batch: 250, loss is 3.7012173748016357 and perplexity is 40.4965738728363
At time: 133.42952394485474 and batch: 300, loss is 3.639973692893982 and perplexity is 38.09083465259274
At time: 134.3664951324463 and batch: 350, loss is 3.696883282661438 and perplexity is 40.32143779241535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.557993395575162 and perplexity of 95.39187389944308
Finished 18 epochs...
Completing Train Step...
At time: 136.247661113739 and batch: 50, loss is 3.7627954483032227 and perplexity is 43.06865405067743
At time: 137.2045934200287 and batch: 100, loss is 3.7118716764450075 and perplexity is 40.93034323799186
At time: 138.1380546092987 and batch: 150, loss is 3.6154953718185423 and perplexity is 37.16975421453634
At time: 139.07307267189026 and batch: 200, loss is 3.6946510219573976 and perplexity is 40.23153021720852
At time: 140.0077314376831 and batch: 250, loss is 3.7001076221466063 and perplexity is 40.45165762003645
At time: 140.94548654556274 and batch: 300, loss is 3.640056486129761 and perplexity is 38.09398844660174
At time: 141.9127640724182 and batch: 350, loss is 3.698042950630188 and perplexity is 40.368224395496306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.557765039904364 and perplexity of 95.37009311106806
Finished 19 epochs...
Completing Train Step...
At time: 143.7965486049652 and batch: 50, loss is 3.7610875463485716 and perplexity is 42.995159790600106
At time: 144.75798273086548 and batch: 100, loss is 3.709952530860901 and perplexity is 40.851867277993115
At time: 145.74675107002258 and batch: 150, loss is 3.6138677072525023 and perplexity is 37.10930353273526
At time: 146.7033131122589 and batch: 200, loss is 3.6933819341659544 and perplexity is 40.18050525780364
At time: 147.6464364528656 and batch: 250, loss is 3.69934561252594 and perplexity is 40.42084480907825
At time: 148.5959506034851 and batch: 300, loss is 3.640059266090393 and perplexity is 38.094094346537155
At time: 149.54660606384277 and batch: 350, loss is 3.6987589597702026 and perplexity is 40.39713876337019
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.557696112271013 and perplexity of 95.36351970280477
Finished 20 epochs...
Completing Train Step...
At time: 151.39929699897766 and batch: 50, loss is 3.7596314048767088 and perplexity is 42.93259831557315
At time: 152.33959579467773 and batch: 100, loss is 3.708386926651001 and perplexity is 40.787959462826436
At time: 153.27969002723694 and batch: 150, loss is 3.6125072717666624 and perplexity is 37.058853044452874
At time: 154.21784472465515 and batch: 200, loss is 3.69233521938324 and perplexity is 40.13846973241416
At time: 155.15495920181274 and batch: 250, loss is 3.698707699775696 and perplexity is 40.39506805933168
At time: 156.09486961364746 and batch: 300, loss is 3.639977021217346 and perplexity is 38.09096143141865
At time: 157.03255486488342 and batch: 350, loss is 3.6992167711257933 and perplexity is 40.41563726631869
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.557696112271013 and perplexity of 95.36351970280477
Finished 21 epochs...
Completing Train Step...
At time: 158.84625458717346 and batch: 50, loss is 3.758314871788025 and perplexity is 42.87611331964919
At time: 159.80250358581543 and batch: 100, loss is 3.707012815475464 and perplexity is 40.731950761805095
At time: 160.75202655792236 and batch: 150, loss is 3.611294889450073 and perplexity is 37.01395077120977
At time: 161.69616556167603 and batch: 200, loss is 3.691401047706604 and perplexity is 40.1009910193483
At time: 162.63989853858948 and batch: 250, loss is 3.698127727508545 and perplexity is 40.371646832615085
At time: 163.5808229446411 and batch: 300, loss is 3.6398326921463013 and perplexity is 38.0854641950551
At time: 164.52100443840027 and batch: 350, loss is 3.6995123338699343 and perplexity is 40.42758438845046
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.557733469995959 and perplexity of 95.36708233348922
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 166.33518481254578 and batch: 50, loss is 3.7576753902435303 and perplexity is 42.84870360142016
At time: 167.29848337173462 and batch: 100, loss is 3.7062783288955687 and perplexity is 40.70204467475186
At time: 168.2380952835083 and batch: 150, loss is 3.6102223205566406 and perplexity is 36.97427204187882
At time: 169.19089150428772 and batch: 200, loss is 3.690154204368591 and perplexity is 40.05102252376452
At time: 170.12832188606262 and batch: 250, loss is 3.696403307914734 and perplexity is 40.302089164322076
At time: 171.06484603881836 and batch: 300, loss is 3.6369801712036134 and perplexity is 37.976979411945194
At time: 172.0023114681244 and batch: 350, loss is 3.6961611223220827 and perplexity is 40.2923297608139
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.557765039904364 and perplexity of 95.37009311106806
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 173.81843161582947 and batch: 50, loss is 3.757427663803101 and perplexity is 42.83809015926936
At time: 174.76088190078735 and batch: 100, loss is 3.7059918737411497 and perplexity is 40.690387034034735
At time: 175.70189499855042 and batch: 150, loss is 3.609894051551819 and perplexity is 36.962136526357405
At time: 176.64236974716187 and batch: 200, loss is 3.689861173629761 and perplexity is 40.039288062406385
At time: 177.57746028900146 and batch: 250, loss is 3.69602059841156 and perplexity is 40.286668122878915
At time: 178.51123762130737 and batch: 300, loss is 3.6363928079605103 and perplexity is 37.95467967981714
At time: 179.444837808609 and batch: 350, loss is 3.695452561378479 and perplexity is 40.26379030178565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.557766092234645 and perplexity of 95.37019347195772
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 181.24787831306458 and batch: 50, loss is 3.757375535964966 and perplexity is 42.83585716044074
At time: 182.18968439102173 and batch: 100, loss is 3.70592885017395 and perplexity is 40.68782266150191
At time: 183.13081312179565 and batch: 150, loss is 3.6098233032226563 and perplexity is 36.959521609457475
At time: 184.07654404640198 and batch: 200, loss is 3.6897964477539062 and perplexity is 40.03669656828722
At time: 185.0315341949463 and batch: 250, loss is 3.6959392404556275 and perplexity is 40.28339061523657
At time: 185.99012279510498 and batch: 300, loss is 3.636271495819092 and perplexity is 37.95007559561963
At time: 186.92855048179626 and batch: 350, loss is 3.6953051328659057 and perplexity is 40.25785470861947
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.557766092234645 and perplexity of 95.37019347195772
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 188.80295968055725 and batch: 50, loss is 3.757364773750305 and perplexity is 42.835396154231525
At time: 189.75306057929993 and batch: 100, loss is 3.7059157705307006 and perplexity is 40.68729048277727
At time: 190.68986010551453 and batch: 150, loss is 3.6098086166381838 and perplexity is 36.95897880430728
At time: 191.62669563293457 and batch: 200, loss is 3.6897831106185914 and perplexity is 40.036162597008364
At time: 192.56618452072144 and batch: 250, loss is 3.6959224796295165 and perplexity is 40.28271543798959
At time: 193.53014826774597 and batch: 300, loss is 3.636246433258057 and perplexity is 37.94912448145246
At time: 194.47105717658997 and batch: 350, loss is 3.69527428150177 and perplexity is 40.25661271804317
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.557766092234645 and perplexity of 95.37019347195772
Annealing...
Model not improving. Stopping early with 95.36351970280477loss at 25 epochs.
Finished Training.
Improved accuracyfrom -144.11446636370945 to -95.36351970280477
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efbe69b8e48>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'wordvec_dim': 200, 'seq_len': 35, 'lr': 14.095412846842905, 'anneal': 5.930177463664579, 'tune_wordvecs': True, 'wordvec_source': '', 'batch_size': 80, 'num_layers': 1, 'dropout': 0.11815698249148554}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.1664321422576904 and batch: 50, loss is 6.082380590438842 and perplexity is 438.070821592395
At time: 2.1306498050689697 and batch: 100, loss is 5.234548273086548 and perplexity is 187.64432320527956
At time: 3.0789592266082764 and batch: 150, loss is 5.11835638999939 and perplexity is 167.0605614347458
At time: 4.03027081489563 and batch: 200, loss is 5.068501892089844 and perplexity is 158.9360455548916
At time: 4.995873689651489 and batch: 250, loss is 5.048866062164307 and perplexity is 155.84564498080596
At time: 5.9546990394592285 and batch: 300, loss is 5.008645839691162 and perplexity is 149.70187848714315
At time: 6.909799575805664 and batch: 350, loss is 5.046296319961548 and perplexity is 155.44567597849718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.069205448545259 and perplexity of 159.04790538097706
Finished 1 epochs...
Completing Train Step...
At time: 8.709610939025879 and batch: 50, loss is 4.858771057128906 and perplexity is 128.8657361267626
At time: 9.65869402885437 and batch: 100, loss is 4.774724025726318 and perplexity is 118.47761333528629
At time: 10.594598770141602 and batch: 150, loss is 4.712911739349365 and perplexity is 111.37598606943097
At time: 11.532066583633423 and batch: 200, loss is 4.7500334644317626 and perplexity is 115.58815255431034
At time: 12.470807552337646 and batch: 250, loss is 4.768757619857788 and perplexity is 117.77283240306959
At time: 13.419613361358643 and batch: 300, loss is 4.7185164737701415 and perplexity is 112.0019714945568
At time: 14.35759711265564 and batch: 350, loss is 4.78197377204895 and perplexity is 119.33966703649587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.9747203958445585 and perplexity of 144.70835843276876
Finished 2 epochs...
Completing Train Step...
At time: 16.16283416748047 and batch: 50, loss is 4.67812297821045 and perplexity is 107.56797552653683
At time: 17.10133147239685 and batch: 100, loss is 4.617517213821412 and perplexity is 101.24235670677777
At time: 18.03954577445984 and batch: 150, loss is 4.563985385894775 and perplexity is 95.96517698236057
At time: 18.97844958305359 and batch: 200, loss is 4.6066782188415525 and perplexity is 100.15091705068056
At time: 19.93083381652832 and batch: 250, loss is 4.631343288421631 and perplexity is 102.65186259670236
At time: 20.874693632125854 and batch: 300, loss is 4.586517581939697 and perplexity is 98.15202791942039
At time: 21.825671672821045 and batch: 350, loss is 4.6548406028747555 and perplexity is 105.09246722660247
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.947224584119073 and perplexity of 140.78368791260874
Finished 3 epochs...
Completing Train Step...
At time: 23.648313522338867 and batch: 50, loss is 4.563766498565673 and perplexity is 95.94417371984197
At time: 24.583613872528076 and batch: 100, loss is 4.526957416534424 and perplexity is 92.47676433033156
At time: 25.52971625328064 and batch: 150, loss is 4.470084943771362 and perplexity is 87.36414373811533
At time: 26.496124982833862 and batch: 200, loss is 4.518288316726685 and perplexity is 91.67853897718186
At time: 27.44088649749756 and batch: 250, loss is 4.543320608139038 and perplexity is 94.00242766303735
At time: 28.387807369232178 and batch: 300, loss is 4.504115056991577 and perplexity is 90.38832013384395
At time: 29.327596426010132 and batch: 350, loss is 4.572856607437134 and perplexity is 96.8202926800976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.948693637190194 and perplexity of 140.99065860994753
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 31.136653184890747 and batch: 50, loss is 4.438996391296387 and perplexity is 84.68990348418723
At time: 32.08712601661682 and batch: 100, loss is 4.317401285171509 and perplexity is 74.99348815535039
At time: 33.024439334869385 and batch: 150, loss is 4.210139045715332 and perplexity is 67.36590609953286
At time: 33.96204733848572 and batch: 200, loss is 4.218863859176635 and perplexity is 67.9562325632924
At time: 34.90345811843872 and batch: 250, loss is 4.190369615554809 and perplexity is 66.04719852135004
At time: 35.84647226333618 and batch: 300, loss is 4.080714130401612 and perplexity is 59.1877225131797
At time: 36.80221962928772 and batch: 350, loss is 4.092922134399414 and perplexity is 59.91471500089636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.758979008115571 and perplexity of 116.62679008480583
Finished 5 epochs...
Completing Train Step...
At time: 38.62264084815979 and batch: 50, loss is 4.255492429733277 and perplexity is 70.49152076396314
At time: 39.56413745880127 and batch: 100, loss is 4.1821721649169925 and perplexity is 65.50799294671286
At time: 40.501938581466675 and batch: 150, loss is 4.092032279968262 and perplexity is 59.861423340688106
At time: 41.43848443031311 and batch: 200, loss is 4.120995240211487 and perplexity is 61.620538995144805
At time: 42.37840008735657 and batch: 250, loss is 4.108396143913269 and perplexity is 60.84904615206778
At time: 43.321879386901855 and batch: 300, loss is 4.0195794105529785 and perplexity is 55.67768345544382
At time: 44.26467275619507 and batch: 350, loss is 4.064583158493042 and perplexity is 58.24062635154063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.744406338395743 and perplexity of 114.93955004711871
Finished 6 epochs...
Completing Train Step...
At time: 46.08255338668823 and batch: 50, loss is 4.174808464050293 and perplexity is 65.02738338650897
At time: 47.02642750740051 and batch: 100, loss is 4.1107837581634525 and perplexity is 60.99450378102352
At time: 47.96305584907532 and batch: 150, loss is 4.027266497612 and perplexity is 56.107331912251226
At time: 48.90033459663391 and batch: 200, loss is 4.061620450019836 and perplexity is 58.06833170978998
At time: 49.85294485092163 and batch: 250, loss is 4.057946896553039 and perplexity is 57.85540592510627
At time: 50.79314589500427 and batch: 300, loss is 3.9791526079177855 and perplexity is 53.47170352557469
At time: 51.75321841239929 and batch: 350, loss is 4.03687325000763 and perplexity is 56.64893853247389
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.737897149447737 and perplexity of 114.19381649160259
Finished 7 epochs...
Completing Train Step...
At time: 53.6312210559845 and batch: 50, loss is 4.11724657535553 and perplexity is 61.38997666665535
At time: 54.58391833305359 and batch: 100, loss is 4.059195423126221 and perplexity is 57.92768504862001
At time: 55.53064227104187 and batch: 150, loss is 3.9787845516204836 and perplexity is 53.452026549704215
At time: 56.472418785095215 and batch: 200, loss is 4.016795597076416 and perplexity is 55.52290271021663
At time: 57.416677951812744 and batch: 250, loss is 4.016984686851502 and perplexity is 55.53340251607389
At time: 58.36015868186951 and batch: 300, loss is 3.944665389060974 and perplexity is 51.65904953672046
At time: 59.302987575531006 and batch: 350, loss is 4.008470792770385 and perplexity is 55.06260401723466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.736913220635776 and perplexity of 114.0815131637089
Finished 8 epochs...
Completing Train Step...
At time: 61.089356899261475 and batch: 50, loss is 4.0698139238357545 and perplexity is 58.54606755058858
At time: 62.03577971458435 and batch: 100, loss is 4.016823306083679 and perplexity is 55.52444121604622
At time: 62.96882128715515 and batch: 150, loss is 3.937394461631775 and perplexity is 51.28480254658617
At time: 63.90220880508423 and batch: 200, loss is 3.978843803405762 and perplexity is 53.455193771534866
At time: 64.84916090965271 and batch: 250, loss is 3.9834201622009275 and perplexity is 53.700384530220774
At time: 65.78887796401978 and batch: 300, loss is 3.914568657875061 and perplexity is 50.12744476856538
At time: 66.7467954158783 and batch: 350, loss is 3.98179922580719 and perplexity is 53.613410131605356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.737233129040948 and perplexity of 114.11801463689751
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 68.5756573677063 and batch: 50, loss is 4.026190328598022 and perplexity is 56.0469834186041
At time: 69.53297185897827 and batch: 100, loss is 3.959806509017944 and perplexity is 52.44717691149043
At time: 70.48241662979126 and batch: 150, loss is 3.864453125 and perplexity is 47.677191837639775
At time: 71.43634080886841 and batch: 200, loss is 3.8895334005355835 and perplexity is 48.888070053766064
At time: 72.3869743347168 and batch: 250, loss is 3.878147406578064 and perplexity is 48.33458773618154
At time: 73.34141707420349 and batch: 300, loss is 3.7860242128372192 and perplexity is 44.080795562691186
At time: 74.29997944831848 and batch: 350, loss is 3.8451410961151122 and perplexity is 46.76528228508235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.707742099104257 and perplexity of 110.80169799986166
Finished 10 epochs...
Completing Train Step...
At time: 76.1059582233429 and batch: 50, loss is 3.9921786642074584 and perplexity is 54.17278519710374
At time: 77.04454493522644 and batch: 100, loss is 3.930619230270386 and perplexity is 50.938510572804674
At time: 77.98111176490784 and batch: 150, loss is 3.840814371109009 and perplexity is 46.56337887404913
At time: 78.91885304450989 and batch: 200, loss is 3.870876135826111 and perplexity is 47.984408528851915
At time: 79.85578203201294 and batch: 250, loss is 3.8649408626556396 and perplexity is 47.70045147125264
At time: 80.79318237304688 and batch: 300, loss is 3.7804195928573607 and perplexity is 43.834430491349536
At time: 81.73025131225586 and batch: 350, loss is 3.846872215270996 and perplexity is 46.846308674008554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.705355940193965 and perplexity of 110.53762272898244
Finished 11 epochs...
Completing Train Step...
At time: 83.52175784111023 and batch: 50, loss is 3.9757008075714113 and perplexity is 53.287448070295206
At time: 84.47512340545654 and batch: 100, loss is 3.9155033302307127 and perplexity is 50.17431940825244
At time: 85.41499829292297 and batch: 150, loss is 3.827958731651306 and perplexity is 45.96860813374248
At time: 86.35744118690491 and batch: 200, loss is 3.860025691986084 and perplexity is 47.466570863665915
At time: 87.29887270927429 and batch: 250, loss is 3.8571831226348876 and perplexity is 47.3318354322505
At time: 88.2390239238739 and batch: 300, loss is 3.7766483306884764 and perplexity is 43.669430686262295
At time: 89.17704463005066 and batch: 350, loss is 3.845938467979431 and perplexity is 46.80258647608306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.704751376448007 and perplexity of 110.47081588624843
Finished 12 epochs...
Completing Train Step...
At time: 90.98329377174377 and batch: 50, loss is 3.962814769744873 and perplexity is 52.60518924601894
At time: 91.92103004455566 and batch: 100, loss is 3.9040090560913088 and perplexity is 49.600903837312174
At time: 92.86279511451721 and batch: 150, loss is 3.8179918384552 and perplexity is 45.512719596232685
At time: 93.80680203437805 and batch: 200, loss is 3.851417407989502 and perplexity is 47.059718802764955
At time: 94.74949359893799 and batch: 250, loss is 3.850696315765381 and perplexity is 47.02579663744168
At time: 95.68874740600586 and batch: 300, loss is 3.772934288978577 and perplexity is 43.5075414170881
At time: 96.62932467460632 and batch: 350, loss is 3.8437117242813112 and perplexity is 46.69848505819445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.704552486025054 and perplexity of 110.44884648377712
Finished 13 epochs...
Completing Train Step...
At time: 98.43696308135986 and batch: 50, loss is 3.9520147895812987 and perplexity is 52.040111154279884
At time: 99.37953853607178 and batch: 100, loss is 3.8942744827270506 and perplexity is 49.120402731037565
At time: 100.32456469535828 and batch: 150, loss is 3.809443769454956 and perplexity is 45.12533179640605
At time: 101.28129863739014 and batch: 200, loss is 3.843839898109436 and perplexity is 46.70447096540212
At time: 102.23789930343628 and batch: 250, loss is 3.8444895505905152 and perplexity is 46.73482249874927
At time: 103.18951797485352 and batch: 300, loss is 3.768466749191284 and perplexity is 43.31360328049679
At time: 104.12730646133423 and batch: 350, loss is 3.840484342575073 and perplexity is 46.548014165919746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.704362014244342 and perplexity of 110.42781109869706
Finished 14 epochs...
Completing Train Step...
At time: 105.90263867378235 and batch: 50, loss is 3.941955575942993 and perplexity is 51.51925266386841
At time: 106.85424280166626 and batch: 100, loss is 3.8852740478515626 and perplexity is 48.68028135817834
At time: 107.79153513908386 and batch: 150, loss is 3.801409478187561 and perplexity is 44.76423426100039
At time: 108.7283866405487 and batch: 200, loss is 3.836913299560547 and perplexity is 46.38208565035716
At time: 109.66505694389343 and batch: 250, loss is 3.8390019226074217 and perplexity is 46.479061581237175
At time: 110.60317182540894 and batch: 300, loss is 3.7644670581817627 and perplexity is 43.14070824471695
At time: 111.54087162017822 and batch: 350, loss is 3.837118220329285 and perplexity is 46.39159127692118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.704698233768858 and perplexity of 110.46494532711448
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 113.39861607551575 and batch: 50, loss is 3.933510398864746 and perplexity is 51.08599549396861
At time: 114.35005402565002 and batch: 100, loss is 3.8746269035339354 and perplexity is 48.16472484976316
At time: 115.30997657775879 and batch: 150, loss is 3.787343602180481 and perplexity is 44.13899367914387
At time: 116.26299405097961 and batch: 200, loss is 3.8188757514953613 and perplexity is 45.55296666741209
At time: 117.19445633888245 and batch: 250, loss is 3.8166877460479736 and perplexity is 45.453405488120104
At time: 118.14510679244995 and batch: 300, loss is 3.7355836915969847 and perplexity is 41.91248238856558
At time: 119.07640838623047 and batch: 350, loss is 3.807052788734436 and perplexity is 45.017566881330026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.702859286604257 and perplexity of 110.26199279580253
Finished 16 epochs...
Completing Train Step...
At time: 120.87048745155334 and batch: 50, loss is 3.9286018991470337 and perplexity is 50.835854310699595
At time: 121.80403518676758 and batch: 100, loss is 3.8704722499847413 and perplexity is 47.96503221881266
At time: 122.75980043411255 and batch: 150, loss is 3.783873424530029 and perplexity is 43.98608898643025
At time: 123.70583081245422 and batch: 200, loss is 3.81640398979187 and perplexity is 45.440509629678424
At time: 124.64691400527954 and batch: 250, loss is 3.8150352478027343 and perplexity is 45.378355842093505
At time: 125.59237861633301 and batch: 300, loss is 3.735326762199402 and perplexity is 41.901715222974204
At time: 126.54924631118774 and batch: 350, loss is 3.8079959201812743 and perplexity is 45.06004439210542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.702508334455819 and perplexity of 110.22330290208761
Finished 17 epochs...
Completing Train Step...
At time: 128.37340998649597 and batch: 50, loss is 3.9256284618377686 and perplexity is 50.684921590538266
At time: 129.32354640960693 and batch: 100, loss is 3.8678117990493774 and perplexity is 47.837593201775164
At time: 130.27583622932434 and batch: 150, loss is 3.7816128063201906 and perplexity is 43.88676554118094
At time: 131.23317003250122 and batch: 200, loss is 3.814760894775391 and perplexity is 45.36590786044116
At time: 132.18909573554993 and batch: 250, loss is 3.814014096260071 and perplexity is 45.33204131512169
At time: 133.13835978507996 and batch: 300, loss is 3.7352202224731443 and perplexity is 41.89725126350336
At time: 134.07943606376648 and batch: 350, loss is 3.8084322881698607 and perplexity is 45.07971144376622
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.702362586712015 and perplexity of 110.20723927502209
Finished 18 epochs...
Completing Train Step...
At time: 135.92463541030884 and batch: 50, loss is 3.923170790672302 and perplexity is 50.56050766710057
At time: 136.94467902183533 and batch: 100, loss is 3.86560839176178 and perplexity is 47.73230354089476
At time: 137.8899862766266 and batch: 150, loss is 3.7797435331344604 and perplexity is 43.80480581357213
At time: 138.83594036102295 and batch: 200, loss is 3.8133832311630247 and perplexity is 45.30345193144848
At time: 139.78769612312317 and batch: 250, loss is 3.8131400966644287 and perplexity is 45.29243843831277
At time: 140.7459192276001 and batch: 300, loss is 3.7350165367126467 and perplexity is 41.888718259072235
At time: 141.69868421554565 and batch: 350, loss is 3.808584671020508 and perplexity is 45.086581342116425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.702297342234645 and perplexity of 110.20004909585548
Finished 19 epochs...
Completing Train Step...
At time: 143.58604288101196 and batch: 50, loss is 3.9209714460372926 and perplexity is 50.44942987976209
At time: 144.53658628463745 and batch: 100, loss is 3.863651466369629 and perplexity is 47.63898632126834
At time: 145.4876594543457 and batch: 150, loss is 3.778068561553955 and perplexity is 43.73149542229535
At time: 146.43745112419128 and batch: 200, loss is 3.8121351480484007 and perplexity is 45.246944728236905
At time: 147.4136848449707 and batch: 250, loss is 3.8123049306869508 and perplexity is 45.25462752608359
At time: 148.3662872314453 and batch: 300, loss is 3.734708852767944 and perplexity is 41.87583175558653
At time: 149.31896233558655 and batch: 350, loss is 3.808549280166626 and perplexity is 45.08498571773954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.7022731386382 and perplexity of 110.19738189061712
Finished 20 epochs...
Completing Train Step...
At time: 151.18054580688477 and batch: 50, loss is 3.918928027153015 and perplexity is 50.34644581766806
At time: 152.1296305656433 and batch: 100, loss is 3.8618510007858275 and perplexity is 47.55329113471642
At time: 153.08006477355957 and batch: 150, loss is 3.776516623497009 and perplexity is 43.663679486939095
At time: 154.03064846992493 and batch: 200, loss is 3.810950589179993 and perplexity is 45.19337879085735
At time: 154.98282599449158 and batch: 250, loss is 3.8114753198623657 and perplexity is 45.21709936626159
At time: 155.93850946426392 and batch: 300, loss is 3.7343310928344726 and perplexity is 41.86001573168662
At time: 156.89512944221497 and batch: 350, loss is 3.8084010553359984 and perplexity is 45.078303498615306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.702289449757543 and perplexity of 110.19917934792358
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 158.7395806312561 and batch: 50, loss is 3.9171817445755006 and perplexity is 50.25860341764622
At time: 159.7096300125122 and batch: 100, loss is 3.8595644283294677 and perplexity is 47.44468130843856
At time: 160.66014432907104 and batch: 150, loss is 3.773637070655823 and perplexity is 43.53812846676817
At time: 161.61503052711487 and batch: 200, loss is 3.807392463684082 and perplexity is 45.03286081843023
At time: 162.56959462165833 and batch: 250, loss is 3.807141318321228 and perplexity is 45.021552444341935
At time: 163.5255537033081 and batch: 300, loss is 3.728773341178894 and perplexity is 41.62801346263396
At time: 164.48309302330017 and batch: 350, loss is 3.802696342468262 and perplexity is 44.82187683625459
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.702132126380658 and perplexity of 110.18184380457747
Finished 22 epochs...
Completing Train Step...
At time: 166.31531357765198 and batch: 50, loss is 3.9165167760849 and perplexity is 50.22519413928168
At time: 167.2803213596344 and batch: 100, loss is 3.8590861463546755 and perplexity is 47.42199479827589
At time: 168.23277282714844 and batch: 150, loss is 3.7732114028930663 and perplexity is 43.51959963287252
At time: 169.18542098999023 and batch: 200, loss is 3.807083053588867 and perplexity is 45.01892935205591
At time: 170.1368591785431 and batch: 250, loss is 3.8069587659835817 and perplexity is 45.01333440483273
At time: 171.1105399131775 and batch: 300, loss is 3.728792986869812 and perplexity is 41.62883128175326
At time: 172.0587453842163 and batch: 350, loss is 3.802803020477295 and perplexity is 44.826658599886585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.7020895070043105 and perplexity of 110.17714802317609
Finished 23 epochs...
Completing Train Step...
At time: 173.9078779220581 and batch: 50, loss is 3.9160305738449095 and perplexity is 50.200780472857794
At time: 174.86125588417053 and batch: 100, loss is 3.8586971950531006 and perplexity is 47.40355353828434
At time: 175.81694102287292 and batch: 150, loss is 3.7728799200057983 and perplexity is 43.50517602105579
At time: 176.7725818157196 and batch: 200, loss is 3.8068406772613526 and perplexity is 45.00801915153151
At time: 177.72510051727295 and batch: 250, loss is 3.806825623512268 and perplexity is 45.00734161720413
At time: 178.67812848091125 and batch: 300, loss is 3.7287959337234495 and perplexity is 41.6289539560069
At time: 179.62994074821472 and batch: 350, loss is 3.8028594493865966 and perplexity is 44.82918819070937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.702066355738147 and perplexity of 110.17459731222326
Finished 24 epochs...
Completing Train Step...
At time: 181.4726529121399 and batch: 50, loss is 3.91559289932251 and perplexity is 50.178813677744195
At time: 182.4261736869812 and batch: 100, loss is 3.858339858055115 and perplexity is 47.38661752088214
At time: 183.37952709197998 and batch: 150, loss is 3.772581796646118 and perplexity is 43.492208044941265
At time: 184.330881357193 and batch: 200, loss is 3.806620144844055 and perplexity is 44.99809451866211
At time: 185.2803840637207 and batch: 250, loss is 3.8067039394378663 and perplexity is 45.001865273696815
At time: 186.23119235038757 and batch: 300, loss is 3.728784499168396 and perplexity is 41.628477950162534
At time: 187.17987895011902 and batch: 350, loss is 3.8028862285614013 and perplexity is 44.83038869545047
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.702050044618804 and perplexity of 110.17280025587394
Finished 25 epochs...
Completing Train Step...
At time: 189.0080041885376 and batch: 50, loss is 3.9151820945739746 and perplexity is 50.15820421633165
At time: 189.9738323688507 and batch: 100, loss is 3.8579998350143434 and perplexity is 47.37050771810809
At time: 190.92557907104492 and batch: 150, loss is 3.7723013591766357 and perplexity is 43.480012910241214
At time: 191.87437272071838 and batch: 200, loss is 3.8064092350006105 and perplexity is 44.988604978345776
At time: 192.82372570037842 and batch: 250, loss is 3.80658456325531 and perplexity is 44.9964934434532
At time: 193.7723889350891 and batch: 300, loss is 3.7287627601623536 and perplexity is 41.62757299826525
At time: 194.72206592559814 and batch: 350, loss is 3.802894630432129 and perplexity is 44.83076535616329
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.702038468985722 and perplexity of 110.17152494334391
Finished 26 epochs...
Completing Train Step...
At time: 196.57623267173767 and batch: 50, loss is 3.914789385795593 and perplexity is 50.138510516425555
At time: 197.5265781879425 and batch: 100, loss is 3.8576715993881225 and perplexity is 47.35496158138037
At time: 198.48271703720093 and batch: 150, loss is 3.772032198905945 and perplexity is 43.46831139305907
At time: 199.42973017692566 and batch: 200, loss is 3.8062035417556763 and perplexity is 44.97935207786491
At time: 200.37730741500854 and batch: 250, loss is 3.8064652633666993 and perplexity is 44.991125686990266
At time: 201.3242974281311 and batch: 300, loss is 3.72873309135437 and perplexity is 41.62633797611605
At time: 202.27494049072266 and batch: 350, loss is 3.8028902864456176 and perplexity is 44.830570612346264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.702032155004041 and perplexity of 110.17082932454967
Finished 27 epochs...
Completing Train Step...
At time: 204.1120960712433 and batch: 50, loss is 3.9144096899032594 and perplexity is 50.11947674368608
At time: 205.05992794036865 and batch: 100, loss is 3.8573523330688477 and perplexity is 47.339845150308946
At time: 206.00777649879456 and batch: 150, loss is 3.771770644187927 and perplexity is 43.45694353785284
At time: 206.95497798919678 and batch: 200, loss is 3.8060011053085328 and perplexity is 44.97024753921177
At time: 207.90157556533813 and batch: 250, loss is 3.8063452005386353 and perplexity is 44.98572424946492
At time: 208.84131526947021 and batch: 300, loss is 3.7286971855163573 and perplexity is 41.62484337440024
At time: 209.78583574295044 and batch: 350, loss is 3.802876706123352 and perplexity is 44.829961802883936
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.7020263671875 and perplexity of 110.17019167784669
Finished 28 epochs...
Completing Train Step...
At time: 211.58766961097717 and batch: 50, loss is 3.9140403842926026 and perplexity is 50.10097075711411
At time: 212.54418396949768 and batch: 100, loss is 3.8570405435562134 and perplexity is 47.3250873838393
At time: 213.4855477809906 and batch: 150, loss is 3.771514964103699 and perplexity is 43.44583388318802
At time: 214.43561601638794 and batch: 200, loss is 3.8058011388778685 and perplexity is 44.96125589836839
At time: 215.37735891342163 and batch: 250, loss is 3.8062240505218505 and perplexity is 44.98027455833878
At time: 216.3260383605957 and batch: 300, loss is 3.7286560344696045 and perplexity is 41.62313050376792
At time: 217.26933193206787 and batch: 350, loss is 3.8028556632995607 and perplexity is 44.82901846382243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.7020237363618 and perplexity of 110.16990183965626
Finished 29 epochs...
Completing Train Step...
At time: 219.10455322265625 and batch: 50, loss is 3.9136791229248047 and perplexity is 50.08287448082997
At time: 220.07396578788757 and batch: 100, loss is 3.8567349910736084 and perplexity is 47.310629294864654
At time: 221.02500677108765 and batch: 150, loss is 3.771263613700867 and perplexity is 43.434915127614396
At time: 221.9758758544922 and batch: 200, loss is 3.805603036880493 and perplexity is 44.95234986595094
At time: 222.90780067443848 and batch: 250, loss is 3.806101884841919 and perplexity is 44.974779848153176
At time: 223.8408100605011 and batch: 300, loss is 3.7286102867126463 and perplexity is 41.62122638246476
At time: 224.77889728546143 and batch: 350, loss is 3.802828507423401 and perplexity is 44.8278011090779
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.7020242625269395 and perplexity of 110.16995980723333
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 226.60655570030212 and batch: 50, loss is 3.9133387184143067 and perplexity is 50.06582894581109
At time: 227.55178332328796 and batch: 100, loss is 3.8563063764572143 and perplexity is 47.29035561274669
At time: 228.4948332309723 and batch: 150, loss is 3.770755953788757 and perplexity is 43.41287055846368
At time: 229.43894338607788 and batch: 200, loss is 3.804954833984375 and perplexity is 44.923221064287226
At time: 230.38340640068054 and batch: 250, loss is 3.805288734436035 and perplexity is 44.93822345260336
At time: 231.3264400959015 and batch: 300, loss is 3.7275725412368774 and perplexity is 41.57805654661552
At time: 232.27093768119812 and batch: 350, loss is 3.8017934942245484 and perplexity is 44.78142774591574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.702009003737877 and perplexity of 110.16827875988102
Finished 31 epochs...
Completing Train Step...
At time: 234.10824251174927 and batch: 50, loss is 3.9132686710357665 and perplexity is 50.062322088563505
At time: 235.04886746406555 and batch: 100, loss is 3.8562450551986696 and perplexity is 47.287455797534555
At time: 235.98604798316956 and batch: 150, loss is 3.770704221725464 and perplexity is 43.410624779186115
At time: 236.92737364768982 and batch: 200, loss is 3.804915051460266 and perplexity is 44.92143394071058
At time: 237.87546920776367 and batch: 250, loss is 3.805268816947937 and perplexity is 44.93732840498618
At time: 238.82274317741394 and batch: 300, loss is 3.7275719261169433 and perplexity is 41.57803097113198
At time: 239.76971673965454 and batch: 350, loss is 3.8017980909347533 and perplexity is 44.781633593634766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.702001637425916 and perplexity of 110.16746722896043
Finished 32 epochs...
Completing Train Step...
At time: 241.60273385047913 and batch: 50, loss is 3.913199715614319 and perplexity is 50.05887013906193
At time: 242.5736424922943 and batch: 100, loss is 3.856187310218811 and perplexity is 47.28472526319004
At time: 243.51225996017456 and batch: 150, loss is 3.770655951499939 and perplexity is 43.40852938911072
At time: 244.46542525291443 and batch: 200, loss is 3.8048777532577516 and perplexity is 44.919758483216185
At time: 245.40364241600037 and batch: 250, loss is 3.8052492141723633 and perplexity is 44.93644751725652
At time: 246.3486168384552 and batch: 300, loss is 3.7275696086883543 and perplexity is 41.57793461712598
At time: 247.28807926177979 and batch: 350, loss is 3.8018010377883913 and perplexity is 44.78176555874908
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.701995323444235 and perplexity of 110.16677163578653
Finished 33 epochs...
Completing Train Step...
At time: 249.10238218307495 and batch: 50, loss is 3.9131317567825317 and perplexity is 50.05546831232008
At time: 250.04442358016968 and batch: 100, loss is 3.8561313056945803 and perplexity is 47.282077178801345
At time: 250.98371481895447 and batch: 150, loss is 3.7706099271774294 and perplexity is 43.40653158692855
At time: 251.9237048625946 and batch: 200, loss is 3.8048417139053345 and perplexity is 44.918139633381045
At time: 252.86342573165894 and batch: 250, loss is 3.8052295589447023 and perplexity is 44.93556428983034
At time: 253.8014976978302 and batch: 300, loss is 3.7275661039352417 and perplexity is 41.577788896985574
At time: 254.74050164222717 and batch: 350, loss is 3.801802649497986 and perplexity is 44.781837734008455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.701991640288254 and perplexity of 110.1663658751299
Finished 34 epochs...
Completing Train Step...
At time: 256.57908868789673 and batch: 50, loss is 3.9130647373199463 and perplexity is 50.0521137341466
At time: 257.51728224754333 and batch: 100, loss is 3.856076374053955 and perplexity is 47.279479968064926
At time: 258.4716737270355 and batch: 150, loss is 3.770564932823181 and perplexity is 43.40457858200707
At time: 259.42364835739136 and batch: 200, loss is 3.804806389808655 and perplexity is 44.91655296869787
At time: 260.3713822364807 and batch: 250, loss is 3.805209770202637 and perplexity is 44.934675080337236
At time: 261.31963181495667 and batch: 300, loss is 3.7275616359710693 and perplexity is 41.57760312932941
At time: 262.26931595802307 and batch: 350, loss is 3.801803321838379 and perplexity is 44.781867842656965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.701987430967134 and perplexity of 110.16590215049528
Finished 35 epochs...
Completing Train Step...
At time: 264.09924125671387 and batch: 50, loss is 3.9129986238479613 and perplexity is 50.04880472451371
At time: 265.055082321167 and batch: 100, loss is 3.856021957397461 and perplexity is 47.27690724684436
At time: 265.99632835388184 and batch: 150, loss is 3.770520920753479 and perplexity is 43.4026682987072
At time: 266.9383189678192 and batch: 200, loss is 3.804771661758423 and perplexity is 44.91499313147534
At time: 267.88142561912537 and batch: 250, loss is 3.8051897382736204 and perplexity is 44.93377496113124
At time: 268.84784603118896 and batch: 300, loss is 3.7275562715530395 and perplexity is 41.57738009028379
At time: 269.79159283638 and batch: 350, loss is 3.801803259849548 and perplexity is 44.78186506668142
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.701985852471713 and perplexity of 110.16572825426047
Finished 36 epochs...
Completing Train Step...
At time: 271.60639333724976 and batch: 50, loss is 3.912933421134949 and perplexity is 50.045541513048924
At time: 272.5585973262787 and batch: 100, loss is 3.855967917442322 and perplexity is 47.27435247392815
At time: 273.4974958896637 and batch: 150, loss is 3.7704776525497437 and perplexity is 43.40079038383988
At time: 274.4372191429138 and batch: 200, loss is 3.8047371816635134 and perplexity is 44.9134444849482
At time: 275.3769245147705 and batch: 250, loss is 3.80516966342926 and perplexity is 44.932872931646465
At time: 276.31670117378235 and batch: 300, loss is 3.727550368309021 and perplexity is 41.57713464958791
At time: 277.2582676410675 and batch: 350, loss is 3.801802382469177 and perplexity is 44.78182577596927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.701985326306573 and perplexity of 110.16567028890991
Finished 37 epochs...
Completing Train Step...
At time: 279.0740659236908 and batch: 50, loss is 3.9128688907623292 and perplexity is 50.042312159803934
At time: 280.01626086235046 and batch: 100, loss is 3.855914444923401 and perplexity is 47.27182466280584
At time: 280.9591474533081 and batch: 150, loss is 3.7704348182678222 and perplexity is 43.398931381963756
At time: 281.90212988853455 and batch: 200, loss is 3.8047031450271604 and perplexity is 44.91191580838656
At time: 282.8541247844696 and batch: 250, loss is 3.8051492881774904 and perplexity is 44.93195742237464
At time: 283.7952661514282 and batch: 300, loss is 3.7275439929962157 and perplexity is 41.57686958319392
At time: 284.7389154434204 and batch: 350, loss is 3.8018009519577025 and perplexity is 44.78176171509946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.701983221646013 and perplexity of 110.16543842781252
Finished 38 epochs...
Completing Train Step...
At time: 286.5682713985443 and batch: 50, loss is 3.9128049659729003 and perplexity is 50.03911331778033
At time: 287.50662207603455 and batch: 100, loss is 3.855861201286316 and perplexity is 47.269307805933046
At time: 288.4444501399994 and batch: 150, loss is 3.7703921461105345 and perplexity is 43.39707949544999
At time: 289.38297414779663 and batch: 200, loss is 3.8046691608428955 and perplexity is 44.91038953949869
At time: 290.3219528198242 and batch: 250, loss is 3.8051290464401246 and perplexity is 44.93104793069804
At time: 291.2602014541626 and batch: 300, loss is 3.727536931037903 and perplexity is 41.57657597011089
At time: 292.2057156562805 and batch: 350, loss is 3.8017988872528075 and perplexity is 44.78166925407229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.701982695480873 and perplexity of 110.16538046261445
Finished 39 epochs...
Completing Train Step...
At time: 294.01507329940796 and batch: 50, loss is 3.912741570472717 and perplexity is 50.03594116371403
At time: 294.96732544898987 and batch: 100, loss is 3.855808277130127 and perplexity is 47.266806183902474
At time: 295.90670466423035 and batch: 150, loss is 3.7703499794006348 and perplexity is 43.395249621968546
At time: 296.8473377227783 and batch: 200, loss is 3.804635510444641 and perplexity is 44.90887831243175
At time: 297.7893464565277 and batch: 250, loss is 3.8051086711883544 and perplexity is 44.93013245861067
At time: 298.7324752807617 and batch: 300, loss is 3.7275294923782347 and perplexity is 41.57626669726237
At time: 299.6854317188263 and batch: 350, loss is 3.801796455383301 and perplexity is 44.781560351028794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.701981643150592 and perplexity of 110.1652645323097
Finished 40 epochs...
Completing Train Step...
At time: 301.5227472782135 and batch: 50, loss is 3.9126787614822387 and perplexity is 50.03279855545496
At time: 302.4760892391205 and batch: 100, loss is 3.8557556104660033 and perplexity is 47.26431686444965
At time: 303.42491602897644 and batch: 150, loss is 3.7703080987930297 and perplexity is 43.39343224060399
At time: 304.3685278892517 and batch: 200, loss is 3.8046019506454467 and perplexity is 44.907371204782805
At time: 305.3117251396179 and batch: 250, loss is 3.8050880432128906 and perplexity is 44.92920565049985
At time: 306.26225209236145 and batch: 300, loss is 3.727521834373474 and perplexity is 41.57594830723319
At time: 307.21512842178345 and batch: 350, loss is 3.801793828010559 and perplexity is 44.781442693332366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.7019805908203125 and perplexity of 110.16514860212705
Finished 41 epochs...
Completing Train Step...
At time: 309.05945110321045 and batch: 50, loss is 3.9126163578033446 and perplexity is 50.02967642217705
At time: 309.99883008003235 and batch: 100, loss is 3.8557032108306886 and perplexity is 47.261840296368746
At time: 310.93832540512085 and batch: 150, loss is 3.7702663564682006 and perplexity is 43.39162093566426
At time: 311.8790030479431 and batch: 200, loss is 3.804568524360657 and perplexity is 44.905870143291345
At time: 312.82770895957947 and batch: 250, loss is 3.8050676584243774 and perplexity is 44.92828978747947
At time: 313.77597427368164 and batch: 300, loss is 3.727513842582703 and perplexity is 41.5756160422809
At time: 314.72573804855347 and batch: 350, loss is 3.801790642738342 and perplexity is 44.781300052474286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.701980064655173 and perplexity of 110.16509063708148
Finished 42 epochs...
Completing Train Step...
At time: 316.564199924469 and batch: 50, loss is 3.912554392814636 and perplexity is 50.026576429888934
At time: 317.53337836265564 and batch: 100, loss is 3.855650992393494 and perplexity is 47.25937242136437
At time: 318.48611760139465 and batch: 150, loss is 3.7702249002456667 and perplexity is 43.38982212025694
At time: 319.43006920814514 and batch: 200, loss is 3.8045350742340087 and perplexity is 44.90436806137036
At time: 320.37078762054443 and batch: 250, loss is 3.805047001838684 and perplexity is 44.927361731996676
At time: 321.3154184818268 and batch: 300, loss is 3.7275054264068603 and perplexity is 41.57526613605796
At time: 322.2610831260681 and batch: 350, loss is 3.8017871046066283 and perplexity is 44.78114161061668
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.701980064655173 and perplexity of 110.16509063708148
Finished 43 epochs...
Completing Train Step...
At time: 324.0564320087433 and batch: 50, loss is 3.9124927043914797 and perplexity is 50.02349046445821
At time: 325.0091509819031 and batch: 100, loss is 3.8555988264083862 and perplexity is 47.25690715394855
At time: 325.9476981163025 and batch: 150, loss is 3.770183482170105 and perplexity is 43.388025034541926
At time: 326.8870189189911 and batch: 200, loss is 3.8045017242431642 and perplexity is 44.902870526078175
At time: 327.82527446746826 and batch: 250, loss is 3.805026345252991 and perplexity is 44.92643369568416
At time: 328.76339077949524 and batch: 300, loss is 3.727496848106384 and perplexity is 41.57490949246237
At time: 329.70122814178467 and batch: 350, loss is 3.801783351898193 and perplexity is 44.780973560364146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.701979012324892 and perplexity of 110.16497470708173
Finished 44 epochs...
Completing Train Step...
At time: 331.5291028022766 and batch: 50, loss is 3.9124313735961915 and perplexity is 50.02042257808386
At time: 332.4708414077759 and batch: 100, loss is 3.8555471229553224 and perplexity is 47.254463871831184
At time: 333.4132263660431 and batch: 150, loss is 3.77014217376709 and perplexity is 43.38623278153558
At time: 334.3551034927368 and batch: 200, loss is 3.8044686126708984 and perplexity is 44.901383746050755
At time: 335.2974588871002 and batch: 250, loss is 3.80500554561615 and perplexity is 44.92549925189681
At time: 336.2402443885803 and batch: 300, loss is 3.7274879598617554 and perplexity is 41.57453996613861
At time: 337.1863489151001 and batch: 350, loss is 3.80177924156189 and perplexity is 44.78078949588111
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.701979012324892 and perplexity of 110.16497470708173
Finished 45 epochs...
Completing Train Step...
At time: 339.0062334537506 and batch: 50, loss is 3.912370400428772 and perplexity is 50.01737276746287
At time: 339.94379472732544 and batch: 100, loss is 3.8554955244064333 and perplexity is 47.252025672971165
At time: 340.8852050304413 and batch: 150, loss is 3.770101075172424 and perplexity is 43.38444970498164
At time: 341.84615778923035 and batch: 200, loss is 3.804435429573059 and perplexity is 44.89989380376137
At time: 342.7834951877594 and batch: 250, loss is 3.804984841346741 and perplexity is 44.92456911188593
At time: 343.72879791259766 and batch: 300, loss is 3.7274789237976074 and perplexity is 41.574164297625835
At time: 344.6663222312927 and batch: 350, loss is 3.8017749881744383 and perplexity is 44.780599026238065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.701979012324892 and perplexity of 110.16497470708173
Finished 46 epochs...
Completing Train Step...
At time: 346.4578549861908 and batch: 50, loss is 3.9123097038269044 and perplexity is 50.0143369750336
At time: 347.40984559059143 and batch: 100, loss is 3.8554440784454345 and perplexity is 47.249594809630864
At time: 348.3482081890106 and batch: 150, loss is 3.7700600528717043 and perplexity is 43.38267001154309
At time: 349.28672075271606 and batch: 200, loss is 3.8044022035598757 and perplexity is 44.898401984081666
At time: 350.23230171203613 and batch: 250, loss is 3.804963870048523 and perplexity is 44.92362699522851
At time: 351.1703758239746 and batch: 300, loss is 3.7274696683883666 and perplexity is 41.57377951350208
At time: 352.10921144485474 and batch: 350, loss is 3.8017705059051514 and perplexity is 44.78039830798424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.701978486159752 and perplexity of 110.16491674212764
Finished 47 epochs...
Completing Train Step...
At time: 353.91709303855896 and batch: 50, loss is 3.912249279022217 and perplexity is 50.01131495979358
At time: 354.8564302921295 and batch: 100, loss is 3.855392894744873 and perplexity is 47.24717646240899
At time: 355.80260729789734 and batch: 150, loss is 3.7700191926956177 and perplexity is 43.38089742422167
At time: 356.74117136001587 and batch: 200, loss is 3.8043692111968994 and perplexity is 44.89692070414195
At time: 357.6808693408966 and batch: 250, loss is 3.8049430418014527 and perplexity is 44.92269132457039
At time: 358.61972761154175 and batch: 300, loss is 3.727460112571716 and perplexity is 41.57338224398571
At time: 359.5585527420044 and batch: 350, loss is 3.8017656993865967 and perplexity is 44.78018307068616
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.701979012324892 and perplexity of 110.16497470708173
Annealing...
Finished 48 epochs...
Completing Train Step...
At time: 361.36295318603516 and batch: 50, loss is 3.912191333770752 and perplexity is 50.00841712553081
At time: 362.30516290664673 and batch: 100, loss is 3.8553167152404786 and perplexity is 47.24357733301373
At time: 363.24693512916565 and batch: 150, loss is 3.7699293804168703 and perplexity is 43.37700146192514
At time: 364.18500089645386 and batch: 200, loss is 3.80425555229187 and perplexity is 44.891818059281526
At time: 365.13691782951355 and batch: 250, loss is 3.804801287651062 and perplexity is 44.91632379795073
At time: 366.077556848526 and batch: 300, loss is 3.7272788524627685 and perplexity is 41.56584733110103
At time: 367.01804995536804 and batch: 350, loss is 3.801587977409363 and perplexity is 44.77222535516135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.701979012324892 and perplexity of 110.16497470708173
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 368.8374607563019 and batch: 50, loss is 3.912181749343872 and perplexity is 50.007937825810416
At time: 369.8022768497467 and batch: 100, loss is 3.8553040075302123 and perplexity is 47.24297697913561
At time: 370.7450304031372 and batch: 150, loss is 3.769914298057556 and perplexity is 43.37634723933673
At time: 371.6873552799225 and batch: 200, loss is 3.804236569404602 and perplexity is 44.89096589104839
At time: 372.62973380088806 and batch: 250, loss is 3.8047774267196655 and perplexity is 44.915252065416325
At time: 373.57142996788025 and batch: 300, loss is 3.7272482347488403 and perplexity is 41.564574699360904
At time: 374.51705384254456 and batch: 350, loss is 3.8015578603744506 and perplexity is 44.77087696879204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.701979012324892 and perplexity of 110.16497470708173
Annealing...
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efbe69b8e48>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'wordvec_dim': 200, 'seq_len': 35, 'lr': 25.162112630757676, 'anneal': 7.9373867945790515, 'tune_wordvecs': True, 'wordvec_source': '', 'batch_size': 80, 'num_layers': 1, 'dropout': 0.4782148580572463}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.2117438316345215 and batch: 50, loss is 6.314502153396607 and perplexity is 552.5269184583244
At time: 2.164222478866577 and batch: 100, loss is 5.7101953125 and perplexity is 301.9300332339227
At time: 3.11637020111084 and batch: 150, loss is 5.662051982879639 and perplexity is 287.7384716070854
At time: 4.069249868392944 and batch: 200, loss is 5.599276237487793 and perplexity is 270.23075374208844
At time: 5.0276710987091064 and batch: 250, loss is 5.587231197357178 and perplexity is 266.9953379422021
At time: 5.984202146530151 and batch: 300, loss is 5.570137166976929 and perplexity is 262.4700990012476
At time: 6.9489428997039795 and batch: 350, loss is 5.599446420669556 and perplexity is 270.2767463850466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.470088564116379 and perplexity of 237.48122414461307
Finished 1 epochs...
Completing Train Step...
At time: 8.783700227737427 and batch: 50, loss is 5.313236675262451 and perplexity is 203.00623142843492
At time: 9.73439645767212 and batch: 100, loss is 5.251314992904663 and perplexity is 190.81702658596598
At time: 10.691754579544067 and batch: 150, loss is 5.236815729141235 and perplexity is 188.07028120007607
At time: 11.640488862991333 and batch: 200, loss is 5.2231245040893555 and perplexity is 185.51291533835698
At time: 12.583154916763306 and batch: 250, loss is 5.236485710144043 and perplexity is 188.0082246749521
At time: 13.52549934387207 and batch: 300, loss is 5.21961633682251 and perplexity is 184.86324524317445
At time: 14.477673530578613 and batch: 350, loss is 5.253542289733887 and perplexity is 191.24250640327165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.4295064991918105 and perplexity of 228.0366813645111
Finished 2 epochs...
Completing Train Step...
At time: 16.36654829978943 and batch: 50, loss is 5.1846654415130615 and perplexity is 178.51371651935008
At time: 17.311657190322876 and batch: 100, loss is 5.101596956253052 and perplexity is 164.28405237179595
At time: 18.27254056930542 and batch: 150, loss is 5.085079851150513 and perplexity is 161.59284209679512
At time: 19.24699878692627 and batch: 200, loss is 5.0759540557861325 and perplexity is 160.12488720306334
At time: 20.19593620300293 and batch: 250, loss is 5.114060735702514 and perplexity is 166.34446616619292
At time: 21.142669916152954 and batch: 300, loss is 5.041521587371826 and perplexity is 154.70523355747164
At time: 22.090480089187622 and batch: 350, loss is 5.133930959701538 and perplexity is 169.68282510136459
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.372229214372306 and perplexity of 215.3423773634358
Finished 3 epochs...
Completing Train Step...
At time: 23.987220525741577 and batch: 50, loss is 5.093789758682251 and perplexity is 163.00644806123967
At time: 24.93041205406189 and batch: 100, loss is 4.994786071777344 and perplexity is 147.64135735163865
At time: 25.874408721923828 and batch: 150, loss is 5.018889360427856 and perplexity is 151.24323375828197
At time: 26.819265365600586 and batch: 200, loss is 5.083684730529785 and perplexity is 161.36755777656455
At time: 27.75650978088379 and batch: 250, loss is 5.091892538070678 and perplexity is 162.6974820491773
At time: 28.69202494621277 and batch: 300, loss is 5.07772349357605 and perplexity is 160.40846904581218
At time: 29.638800621032715 and batch: 350, loss is 5.061552238464356 and perplexity is 157.83532433547387
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.3859058248585665 and perplexity of 218.3077631704059
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 31.453362226486206 and batch: 50, loss is 5.037677478790283 and perplexity is 154.11167143110615
At time: 32.39723086357117 and batch: 100, loss is 4.893712139129638 and perplexity is 133.44803339908725
At time: 33.34143662452698 and batch: 150, loss is 4.817513809204102 and perplexity is 123.65727271788629
At time: 34.286219358444214 and batch: 200, loss is 4.802180347442627 and perplexity is 121.7756414819483
At time: 35.2285852432251 and batch: 250, loss is 4.795314455032349 and perplexity is 120.94240675259724
At time: 36.16903758049011 and batch: 300, loss is 4.773961687088013 and perplexity is 118.38732769135663
At time: 37.13267421722412 and batch: 350, loss is 4.796771144866943 and perplexity is 121.11871070599783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.129754954370959 and perplexity of 168.97570621297865
Finished 5 epochs...
Completing Train Step...
At time: 38.94670915603638 and batch: 50, loss is 4.874267988204956 and perplexity is 130.87831366793984
At time: 39.89048409461975 and batch: 100, loss is 4.7980130004882815 and perplexity is 121.26921609137426
At time: 40.834139823913574 and batch: 150, loss is 4.746719999313354 and perplexity is 115.20578906659225
At time: 41.778348207473755 and batch: 200, loss is 4.746346082687378 and perplexity is 115.16271975931832
At time: 42.718711614608765 and batch: 250, loss is 4.753948993682862 and perplexity is 116.04162856664553
At time: 43.664793968200684 and batch: 300, loss is 4.732532844543457 and perplexity is 113.582886115164
At time: 44.61515998840332 and batch: 350, loss is 4.773513641357422 and perplexity is 118.33429663568458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.110434170426993 and perplexity of 165.74229965946432
Finished 6 epochs...
Completing Train Step...
At time: 46.570070028305054 and batch: 50, loss is 4.816621704101562 and perplexity is 123.54700662571324
At time: 47.531272172927856 and batch: 100, loss is 4.748815135955811 and perplexity is 115.44741396689359
At time: 48.49305701255798 and batch: 150, loss is 4.705226078033447 and perplexity is 110.52326900649918
At time: 49.44488596916199 and batch: 200, loss is 4.710205678939819 and perplexity is 111.07500334533991
At time: 50.38875913619995 and batch: 250, loss is 4.723413867950439 and perplexity is 112.55183464722381
At time: 51.337050437927246 and batch: 300, loss is 4.701068572998047 and perplexity is 110.06472182567538
At time: 52.27908778190613 and batch: 350, loss is 4.749895610809326 and perplexity is 115.57221940701628
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.1015125143116915 and perplexity of 164.27018049317158
Finished 7 epochs...
Completing Train Step...
At time: 54.128212451934814 and batch: 50, loss is 4.776265363693238 and perplexity is 118.66036818626401
At time: 55.077966928482056 and batch: 100, loss is 4.714342279434204 and perplexity is 111.53542789876505
At time: 56.0220832824707 and batch: 150, loss is 4.676877298355103 and perplexity is 107.43406368929809
At time: 56.963666677474976 and batch: 200, loss is 4.679962882995605 and perplexity is 107.76607254337821
At time: 57.90314984321594 and batch: 250, loss is 4.69700366973877 and perplexity is 109.61822747217286
At time: 58.84220361709595 and batch: 300, loss is 4.6743237590789795 and perplexity is 107.1600765554069
At time: 59.78037428855896 and batch: 350, loss is 4.728190364837647 and perplexity is 113.0907241124326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.0906787740773165 and perplexity of 162.50012550002117
Finished 8 epochs...
Completing Train Step...
At time: 61.5877149105072 and batch: 50, loss is 4.739508428573608 and perplexity is 114.37796292022698
At time: 62.54736852645874 and batch: 100, loss is 4.6837180519104 and perplexity is 108.17151312155437
At time: 63.49167013168335 and batch: 150, loss is 4.650372343063355 and perplexity is 104.62393432172232
At time: 64.43822741508484 and batch: 200, loss is 4.655001993179321 and perplexity is 105.10942950063222
At time: 65.39555644989014 and batch: 250, loss is 4.675174818038941 and perplexity is 107.25131491781542
At time: 66.34270644187927 and batch: 300, loss is 4.654072713851929 and perplexity is 105.0117988507814
At time: 67.2850513458252 and batch: 350, loss is 4.7127885818481445 and perplexity is 111.3622701259187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.091946832064925 and perplexity of 162.7063157851388
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 69.11521911621094 and batch: 50, loss is 4.702495784759521 and perplexity is 110.22191964179098
At time: 70.05855226516724 and batch: 100, loss is 4.62673059463501 and perplexity is 102.17945136970522
At time: 71.00122451782227 and batch: 150, loss is 4.56532925605774 and perplexity is 96.09422841515365
At time: 71.95157766342163 and batch: 200, loss is 4.553671617507934 and perplexity is 94.98050096259443
At time: 72.89132046699524 and batch: 250, loss is 4.5564431476593015 and perplexity is 95.24410741267809
At time: 73.83086013793945 and batch: 300, loss is 4.519630470275879 and perplexity is 91.8016682643764
At time: 74.77078652381897 and batch: 350, loss is 4.5937628650665285 and perplexity is 98.86574958676582
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.038312188510237 and perplexity of 154.20951865592926
Finished 10 epochs...
Completing Train Step...
At time: 76.60539031028748 and batch: 50, loss is 4.660526247024536 and perplexity is 105.69168746035761
At time: 77.56124687194824 and batch: 100, loss is 4.5973630619049075 and perplexity is 99.22232723556809
At time: 78.50001358985901 and batch: 150, loss is 4.540429353713989 and perplexity is 93.73103524940504
At time: 79.4415636062622 and batch: 200, loss is 4.5358675098419186 and perplexity is 93.30442271036009
At time: 80.3858048915863 and batch: 250, loss is 4.54710620880127 and perplexity is 94.35895772989937
At time: 81.32926511764526 and batch: 300, loss is 4.522361583709717 and perplexity is 92.0527317191526
At time: 82.27356910705566 and batch: 350, loss is 4.596810712814331 and perplexity is 99.16753700641424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.03290058004445 and perplexity of 153.37725110344607
Finished 11 epochs...
Completing Train Step...
At time: 84.06875157356262 and batch: 50, loss is 4.647866468429566 and perplexity is 104.36208807257512
At time: 85.02237367630005 and batch: 100, loss is 4.585909090042114 and perplexity is 98.09232137301723
At time: 85.96187376976013 and batch: 150, loss is 4.5302021694183345 and perplexity is 92.77731592221494
At time: 86.9011116027832 and batch: 200, loss is 4.529521846771241 and perplexity is 92.71421887866396
At time: 87.83995962142944 and batch: 250, loss is 4.545541858673095 and perplexity is 94.21146267930988
At time: 88.78109765052795 and batch: 300, loss is 4.524076280593872 and perplexity is 92.21070965483936
At time: 89.73336553573608 and batch: 350, loss is 4.596606550216674 and perplexity is 99.14729277108397
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.031026905980603 and perplexity of 153.09014118526827
Finished 12 epochs...
Completing Train Step...
At time: 91.58049392700195 and batch: 50, loss is 4.639133615493774 and perplexity is 103.45467721494559
At time: 92.5484414100647 and batch: 100, loss is 4.5782277870178225 and perplexity is 97.34173097489273
At time: 93.49887228012085 and batch: 150, loss is 4.523692970275879 and perplexity is 92.17537111164602
At time: 94.45363736152649 and batch: 200, loss is 4.525567932128906 and perplexity is 92.34835853800388
At time: 95.40931963920593 and batch: 250, loss is 4.544358444213867 and perplexity is 94.10003741628783
At time: 96.37012505531311 and batch: 300, loss is 4.52426703453064 and perplexity is 92.2283008884634
At time: 97.33040428161621 and batch: 350, loss is 4.5952122783660885 and perplexity is 99.0091508178088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.029919854525862 and perplexity of 152.92075629795485
Finished 13 epochs...
Completing Train Step...
At time: 99.17330694198608 and batch: 50, loss is 4.632473669052124 and perplexity is 102.76796388082462
At time: 100.13051152229309 and batch: 100, loss is 4.5722800064086915 and perplexity is 96.76448209153179
At time: 101.08825755119324 and batch: 150, loss is 4.5189213466644285 and perplexity is 91.73659260990812
At time: 102.04055047035217 and batch: 200, loss is 4.522655391693116 and perplexity is 92.07978152015448
At time: 102.98387622833252 and batch: 250, loss is 4.5430881214141845 and perplexity is 93.98057588672404
At time: 103.93884944915771 and batch: 300, loss is 4.523429374694825 and perplexity is 92.15107729315122
At time: 104.88011646270752 and batch: 350, loss is 4.593302555084229 and perplexity is 98.82025116781506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.028932242557921 and perplexity of 152.76980448206788
Finished 14 epochs...
Completing Train Step...
At time: 106.72889757156372 and batch: 50, loss is 4.627033805847168 and perplexity is 102.21043802252551
At time: 107.68723320960999 and batch: 100, loss is 4.567195882797241 and perplexity is 96.27376798597628
At time: 108.64212989807129 and batch: 150, loss is 4.514987106323242 and perplexity is 91.37638783718465
At time: 109.61223340034485 and batch: 200, loss is 4.5197942924499515 and perplexity is 91.81670864519526
At time: 110.55927395820618 and batch: 250, loss is 4.541492376327515 and perplexity is 93.83072643707001
At time: 111.5061993598938 and batch: 300, loss is 4.522290191650391 and perplexity is 92.04616011965135
At time: 112.4670615196228 and batch: 350, loss is 4.591337394714356 and perplexity is 98.62624421630102
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.028481845198007 and perplexity of 152.7010128584004
Finished 15 epochs...
Completing Train Step...
At time: 114.29922890663147 and batch: 50, loss is 4.622397480010986 and perplexity is 101.73765396535833
At time: 115.27244687080383 and batch: 100, loss is 4.562533664703369 and perplexity is 95.82596337541919
At time: 116.2216362953186 and batch: 150, loss is 4.511192026138306 and perplexity is 91.03026431701326
At time: 117.16574192047119 and batch: 200, loss is 4.516983966827393 and perplexity is 91.559036037866
At time: 118.1099784374237 and batch: 250, loss is 4.539432830810547 and perplexity is 93.63767665073244
At time: 119.0793867111206 and batch: 300, loss is 4.520621767044068 and perplexity is 91.8927160816889
At time: 120.02969193458557 and batch: 350, loss is 4.588984537124634 and perplexity is 98.394463489499
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.027981988314925 and perplexity of 152.62470327959247
Finished 16 epochs...
Completing Train Step...
At time: 121.85967469215393 and batch: 50, loss is 4.6179121971130375 and perplexity is 101.28235364462286
At time: 122.80584049224854 and batch: 100, loss is 4.558427677154541 and perplexity is 95.43330982987953
At time: 123.74717569351196 and batch: 150, loss is 4.507666940689087 and perplexity is 90.70993977422526
At time: 124.68630480766296 and batch: 200, loss is 4.5139635562896725 and perplexity is 91.28290738146947
At time: 125.62402176856995 and batch: 250, loss is 4.537228527069092 and perplexity is 93.43149809331719
At time: 126.56191515922546 and batch: 300, loss is 4.518756256103516 and perplexity is 91.72144901444467
At time: 127.50977325439453 and batch: 350, loss is 4.586559228897094 and perplexity is 98.15611573786764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.02740478515625 and perplexity of 152.53663323836582
Finished 17 epochs...
Completing Train Step...
At time: 129.3710858821869 and batch: 50, loss is 4.613908786773681 and perplexity is 100.877689381867
At time: 130.31680917739868 and batch: 100, loss is 4.554669256210327 and perplexity is 95.07530446828758
At time: 131.27368998527527 and batch: 150, loss is 4.5044468402862545 and perplexity is 90.41831444402867
At time: 132.2356517314911 and batch: 200, loss is 4.511303701400757 and perplexity is 91.04043071332875
At time: 133.2062430381775 and batch: 250, loss is 4.53513466835022 and perplexity is 93.23607040680008
At time: 134.15686225891113 and batch: 300, loss is 4.516976852416992 and perplexity is 91.55838465162479
At time: 135.1122443675995 and batch: 350, loss is 4.584448547363281 and perplexity is 97.9491579247987
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.02679811674973 and perplexity of 152.44412214675884
Finished 18 epochs...
Completing Train Step...
At time: 136.9387879371643 and batch: 50, loss is 4.609838142395019 and perplexity is 100.46788682875574
At time: 137.90180444717407 and batch: 100, loss is 4.550868911743164 and perplexity is 94.71467126023664
At time: 138.84165811538696 and batch: 150, loss is 4.501204118728638 and perplexity is 90.12558789842514
At time: 139.79018378257751 and batch: 200, loss is 4.5088646125793455 and perplexity is 90.8186456032236
At time: 140.73319625854492 and batch: 250, loss is 4.533106393814087 and perplexity is 93.04715371157141
At time: 141.67946481704712 and batch: 300, loss is 4.515052604675293 and perplexity is 91.3823730360125
At time: 142.62682509422302 and batch: 350, loss is 4.582325487136841 and perplexity is 97.74142655452738
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.026847050107759 and perplexity of 152.45158193208192
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 144.4364995956421 and batch: 50, loss is 4.605696935653686 and perplexity is 100.05268884225622
At time: 145.39060497283936 and batch: 100, loss is 4.542965030670166 and perplexity is 93.96900845965104
At time: 146.34182953834534 and batch: 150, loss is 4.488473176956177 and perplexity is 88.98547702799358
At time: 147.2939145565033 and batch: 200, loss is 4.491495380401611 and perplexity is 89.25481603689613
At time: 148.23658204078674 and batch: 250, loss is 4.513447370529175 and perplexity is 91.23580060347228
At time: 149.1805064678192 and batch: 300, loss is 4.491635332107544 and perplexity is 89.2673082747978
At time: 150.12418031692505 and batch: 350, loss is 4.564690599441528 and perplexity is 96.03287679379137
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.021898993130388 and perplexity of 151.6991060005385
Finished 20 epochs...
Completing Train Step...
At time: 151.9560739994049 and batch: 50, loss is 4.599916973114014 and perplexity is 99.47605611188786
At time: 152.90366291999817 and batch: 100, loss is 4.539188957214355 and perplexity is 93.6148436780813
At time: 153.85088562965393 and batch: 150, loss is 4.485773363113403 and perplexity is 88.74555682098703
At time: 154.8007152080536 and batch: 200, loss is 4.49030327796936 and perplexity is 89.14847854879065
At time: 155.75150537490845 and batch: 250, loss is 4.512756929397583 and perplexity is 91.1728293955196
At time: 156.6916525363922 and batch: 300, loss is 4.492270402908325 and perplexity is 89.32401734097753
At time: 157.648619890213 and batch: 350, loss is 4.564751615524292 and perplexity is 96.03873652251693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.020634618298761 and perplexity of 151.50742267425392
Finished 21 epochs...
Completing Train Step...
At time: 159.47222328186035 and batch: 50, loss is 4.597476358413696 and perplexity is 99.23356941567565
At time: 160.41326427459717 and batch: 100, loss is 4.537376327514648 and perplexity is 93.44530833091886
At time: 161.35440301895142 and batch: 150, loss is 4.484705572128296 and perplexity is 88.65084569028997
At time: 162.29504704475403 and batch: 200, loss is 4.490360813140869 and perplexity is 89.15360786935045
At time: 163.23908019065857 and batch: 250, loss is 4.513023433685302 and perplexity is 91.19713058351854
At time: 164.1918821334839 and batch: 300, loss is 4.492868595123291 and perplexity is 89.37746625752945
At time: 165.1351501941681 and batch: 350, loss is 4.564474573135376 and perplexity is 96.01213340678778
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.019950077451509 and perplexity of 151.4037451445721
Finished 22 epochs...
Completing Train Step...
At time: 166.9572925567627 and batch: 50, loss is 4.595578966140747 and perplexity is 99.0454629201881
At time: 167.92431950569153 and batch: 100, loss is 4.536040830612182 and perplexity is 93.32059570629085
At time: 168.8743646144867 and batch: 150, loss is 4.48405255317688 and perplexity is 88.59297390573771
At time: 169.82793498039246 and batch: 200, loss is 4.490629072189331 and perplexity is 89.17752733952815
At time: 170.78461384773254 and batch: 250, loss is 4.513321208953857 and perplexity is 91.22429087720067
At time: 171.73535561561584 and batch: 300, loss is 4.493300161361694 and perplexity is 89.41604687888827
At time: 172.6811237335205 and batch: 350, loss is 4.564083442687989 and perplexity is 95.97458748124973
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.019504941742996 and perplexity of 151.33636492898825
Finished 23 epochs...
Completing Train Step...
At time: 174.51244020462036 and batch: 50, loss is 4.593959846496582 and perplexity is 98.88522622170746
At time: 175.45514822006226 and batch: 100, loss is 4.53494891166687 and perplexity is 93.21875279207387
At time: 176.40011405944824 and batch: 150, loss is 4.483604402542114 and perplexity is 88.553279803379
At time: 177.3496298789978 and batch: 200, loss is 4.490942182540894 and perplexity is 89.20545411831853
At time: 178.3028953075409 and batch: 250, loss is 4.5136093902587895 and perplexity is 91.25058380076973
At time: 179.25973773002625 and batch: 300, loss is 4.493585453033448 and perplexity is 89.44156017157583
At time: 180.22412514686584 and batch: 350, loss is 4.563636846542359 and perplexity is 95.93173516995294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.019175562365302 and perplexity of 151.28652605968387
Finished 24 epochs...
Completing Train Step...
At time: 182.06971526145935 and batch: 50, loss is 4.592578363418579 and perplexity is 98.74871227258436
At time: 183.01307940483093 and batch: 100, loss is 4.534036264419556 and perplexity is 93.13371576424728
At time: 183.9640417098999 and batch: 150, loss is 4.483248605728149 and perplexity is 88.52177843293475
At time: 184.9106957912445 and batch: 200, loss is 4.49126651763916 and perplexity is 89.23439127045928
At time: 185.8539571762085 and batch: 250, loss is 4.5138466262817385 and perplexity is 91.27223429439935
At time: 186.80780506134033 and batch: 300, loss is 4.49382266998291 and perplexity is 89.46277974235629
At time: 187.76225471496582 and batch: 350, loss is 4.563183174133301 and perplexity is 95.8882234593309
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.018951416015625 and perplexity of 151.25261953726593
Finished 25 epochs...
Completing Train Step...
At time: 189.65272092819214 and batch: 50, loss is 4.591297702789307 and perplexity is 98.622329628497
At time: 190.62658262252808 and batch: 100, loss is 4.533241310119629 and perplexity is 93.05970813667064
At time: 191.58132600784302 and batch: 150, loss is 4.482944612503052 and perplexity is 88.49487250183516
At time: 192.5256209373474 and batch: 200, loss is 4.491565847396851 and perplexity is 89.26110577719987
At time: 193.46837949752808 and batch: 250, loss is 4.514046478271484 and perplexity is 91.2904770548969
At time: 194.41142892837524 and batch: 300, loss is 4.493986959457398 and perplexity is 89.47747874283884
At time: 195.35880851745605 and batch: 350, loss is 4.5627219772338865 and perplexity is 95.84401030424979
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.018771993702855 and perplexity of 151.2254838769003
Finished 26 epochs...
Completing Train Step...
At time: 197.18226957321167 and batch: 50, loss is 4.5901067924499515 and perplexity is 98.50494918511602
At time: 198.14659023284912 and batch: 100, loss is 4.5324993991851805 and perplexity is 92.99069172682776
At time: 199.092218875885 and batch: 150, loss is 4.482656764984131 and perplexity is 88.4694031381707
At time: 200.0503008365631 and batch: 200, loss is 4.491841459274292 and perplexity is 89.28571058867902
At time: 201.00571703910828 and batch: 250, loss is 4.514198417663574 and perplexity is 91.30434872828437
At time: 201.95831108093262 and batch: 300, loss is 4.494083223342895 and perplexity is 89.48609260720254
At time: 202.91310334205627 and batch: 350, loss is 4.562242193222046 and perplexity is 95.79803691000677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.018634664601293 and perplexity of 151.2047176430029
Finished 27 epochs...
Completing Train Step...
At time: 204.75064826011658 and batch: 50, loss is 4.588956727981567 and perplexity is 98.39172726183311
At time: 205.6948001384735 and batch: 100, loss is 4.531799154281616 and perplexity is 92.9255982622104
At time: 206.6666750907898 and batch: 150, loss is 4.482349548339844 and perplexity is 88.44222803955158
At time: 207.6091113090515 and batch: 200, loss is 4.492099685668945 and perplexity is 89.30876949289907
At time: 208.54988551139832 and batch: 250, loss is 4.514310960769653 and perplexity is 91.31462498153864
At time: 209.49175453186035 and batch: 300, loss is 4.494124956130982 and perplexity is 89.48982718926872
At time: 210.43861413002014 and batch: 350, loss is 4.5617809009552 and perplexity is 95.75385620729286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.018524696087015 and perplexity of 151.18809079908334
Finished 28 epochs...
Completing Train Step...
At time: 212.26819443702698 and batch: 50, loss is 4.587906322479248 and perplexity is 98.28843031147487
At time: 213.21476244926453 and batch: 100, loss is 4.531146287918091 and perplexity is 92.86495006453286
At time: 214.17003870010376 and batch: 150, loss is 4.48210711479187 and perplexity is 88.4207892752601
At time: 215.12699007987976 and batch: 200, loss is 4.49231104850769 and perplexity is 89.3276480429856
At time: 216.08423733711243 and batch: 250, loss is 4.51441032409668 and perplexity is 91.3236987572758
At time: 217.02812671661377 and batch: 300, loss is 4.49416558265686 and perplexity is 89.49346292390203
At time: 217.97418975830078 and batch: 350, loss is 4.561301670074463 and perplexity is 95.70797899621482
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.018444192820582 and perplexity of 151.17592015382323
Finished 29 epochs...
Completing Train Step...
At time: 219.8698832988739 and batch: 50, loss is 4.5869094753265385 and perplexity is 98.19050058816694
At time: 220.84814262390137 and batch: 100, loss is 4.53053129196167 and perplexity is 92.8078560538423
At time: 221.8029854297638 and batch: 150, loss is 4.481819467544556 and perplexity is 88.39535893627854
At time: 222.7607250213623 and batch: 200, loss is 4.492484474182129 and perplexity is 89.34314109400108
At time: 223.71245670318604 and batch: 250, loss is 4.514434070587158 and perplexity is 91.3258674003675
At time: 224.6646556854248 and batch: 300, loss is 4.494154548645019 and perplexity is 89.4924754574203
At time: 225.61183381080627 and batch: 350, loss is 4.560792245864868 and perplexity is 95.65923545128783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.018380000673491 and perplexity of 151.16621615838355
Finished 30 epochs...
Completing Train Step...
At time: 227.52812695503235 and batch: 50, loss is 4.586021499633789 and perplexity is 98.10334851057561
At time: 228.48279237747192 and batch: 100, loss is 4.529979257583618 and perplexity is 92.75663706536926
At time: 229.4297959804535 and batch: 150, loss is 4.4815654277801515 and perplexity is 88.3729058522288
At time: 230.37348818778992 and batch: 200, loss is 4.492637147903443 and perplexity is 89.35678248514022
At time: 231.33663606643677 and batch: 250, loss is 4.514457406997681 and perplexity is 91.32799864316821
At time: 232.28208541870117 and batch: 300, loss is 4.4941365432739255 and perplexity is 89.49086412669594
At time: 233.23470330238342 and batch: 350, loss is 4.560285520553589 and perplexity is 95.61077477458973
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.018290026434537 and perplexity of 151.15261570498348
Finished 31 epochs...
Completing Train Step...
At time: 235.07944464683533 and batch: 50, loss is 4.585101175308227 and perplexity is 98.01310314639402
At time: 236.0360734462738 and batch: 100, loss is 4.529425210952759 and perplexity is 92.7052597971288
At time: 236.99412369728088 and batch: 150, loss is 4.4813422679901125 and perplexity is 88.35318677344824
At time: 237.95307803153992 and batch: 200, loss is 4.492819604873657 and perplexity is 89.37308774039904
At time: 238.90884947776794 and batch: 250, loss is 4.5144873046875 and perplexity is 91.3307291801616
At time: 239.86793851852417 and batch: 300, loss is 4.494112710952759 and perplexity is 89.48873137709491
At time: 240.83135557174683 and batch: 350, loss is 4.559783201217652 and perplexity is 95.56275969415779
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.018221624966325 and perplexity of 151.14227699774068
Finished 32 epochs...
Completing Train Step...
At time: 242.68678855895996 and batch: 50, loss is 4.584196405410767 and perplexity is 97.92446394619722
At time: 243.65741777420044 and batch: 100, loss is 4.52886474609375 and perplexity is 92.65331631437527
At time: 244.60726022720337 and batch: 150, loss is 4.481059093475341 and perplexity is 88.32817094474493
At time: 245.55922174453735 and batch: 200, loss is 4.492985706329346 and perplexity is 89.38793397332849
At time: 246.5154745578766 and batch: 250, loss is 4.514471874237061 and perplexity is 91.32931991674423
At time: 247.47610926628113 and batch: 300, loss is 4.494079322814941 and perplexity is 89.48574356487755
At time: 248.43308997154236 and batch: 350, loss is 4.5592805194854735 and perplexity is 95.51473411238561
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.018165851461476 and perplexity of 151.13384749829518
Finished 33 epochs...
Completing Train Step...
At time: 250.2823190689087 and batch: 50, loss is 4.583347396850586 and perplexity is 97.84136052080743
At time: 251.24737095832825 and batch: 100, loss is 4.528348178863525 and perplexity is 92.60546700715169
At time: 252.19343733787537 and batch: 150, loss is 4.480822534561157 and perplexity is 88.307278599767
At time: 253.15176010131836 and batch: 200, loss is 4.493145818710327 and perplexity is 89.40224723410249
At time: 254.11305737495422 and batch: 250, loss is 4.514477548599243 and perplexity is 91.3298381538536
At time: 255.0751838684082 and batch: 300, loss is 4.494025764465332 and perplexity is 89.48095098448113
At time: 256.0193703174591 and batch: 350, loss is 4.5588181114196775 and perplexity is 95.47057753889435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.018113761112608 and perplexity of 151.12597508849328
Finished 34 epochs...
Completing Train Step...
At time: 257.8443670272827 and batch: 50, loss is 4.582523698806763 and perplexity is 97.76080196605818
At time: 258.7990002632141 and batch: 100, loss is 4.5278516864776615 and perplexity is 92.55950050984549
At time: 259.7515215873718 and batch: 150, loss is 4.480604867935181 and perplexity is 88.28805914417798
At time: 260.7102270126343 and batch: 200, loss is 4.493293237686157 and perplexity is 89.41542779333506
At time: 261.66226840019226 and batch: 250, loss is 4.514472942352295 and perplexity is 91.32941746703429
At time: 262.61950945854187 and batch: 300, loss is 4.493974695205688 and perplexity is 89.47638137524643
At time: 263.5646641254425 and batch: 350, loss is 4.558362941741944 and perplexity is 95.42713211515276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.018074298727101 and perplexity of 151.1200114146754
Finished 35 epochs...
Completing Train Step...
At time: 265.3972113132477 and batch: 50, loss is 4.5817430877685545 and perplexity is 97.68451858263859
At time: 266.3460669517517 and batch: 100, loss is 4.527372398376465 and perplexity is 92.51514847214942
At time: 267.2954213619232 and batch: 150, loss is 4.480398263931274 and perplexity is 88.26982036182925
At time: 268.24971866607666 and batch: 200, loss is 4.493434228897095 and perplexity is 89.42803547154115
At time: 269.1938488483429 and batch: 250, loss is 4.514457712173462 and perplexity is 91.32802651426574
At time: 270.1497948169708 and batch: 300, loss is 4.493906755447387 and perplexity is 89.47030257802051
At time: 271.1015040874481 and batch: 350, loss is 4.5579120063781735 and perplexity is 95.38411034736613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.018047990470097 and perplexity of 151.1160357628731
Finished 36 epochs...
Completing Train Step...
At time: 272.915150642395 and batch: 50, loss is 4.580993604660034 and perplexity is 97.61133311505868
At time: 273.8926954269409 and batch: 100, loss is 4.526917018890381 and perplexity is 92.4730285623825
At time: 274.8486955165863 and batch: 150, loss is 4.480189781188965 and perplexity is 88.25141954581002
At time: 275.7983717918396 and batch: 200, loss is 4.4935465335845945 and perplexity is 89.43807922308814
At time: 276.7467453479767 and batch: 250, loss is 4.51443925857544 and perplexity is 91.3263411991264
At time: 277.6921534538269 and batch: 300, loss is 4.493836889266968 and perplexity is 89.46405184807824
At time: 278.63536858558655 and batch: 350, loss is 4.557469091415405 and perplexity is 95.34187265222702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.018014842066272 and perplexity of 151.11102659051866
Finished 37 epochs...
Completing Train Step...
At time: 280.48113226890564 and batch: 50, loss is 4.5802530097961425 and perplexity is 97.53906942545764
At time: 281.4287278652191 and batch: 100, loss is 4.526461896896362 and perplexity is 92.43095162902542
At time: 282.3677716255188 and batch: 150, loss is 4.4799814128875735 and perplexity is 88.23303266311213
At time: 283.3218581676483 and batch: 200, loss is 4.493668966293335 and perplexity is 89.44903003974733
At time: 284.2716691493988 and batch: 250, loss is 4.514408645629882 and perplexity is 91.32354547360826
At time: 285.229070186615 and batch: 300, loss is 4.493751039505005 and perplexity is 89.4563717101966
At time: 286.17959809303284 and batch: 350, loss is 4.5570155620574955 and perplexity is 95.2986421178404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.017996952451509 and perplexity of 151.10832329664703
Finished 38 epochs...
Completing Train Step...
At time: 288.00033473968506 and batch: 50, loss is 4.57952395439148 and perplexity is 97.4679839554989
At time: 288.94044280052185 and batch: 100, loss is 4.526016712188721 and perplexity is 92.38981194090692
At time: 289.8796737194061 and batch: 150, loss is 4.479761915206909 and perplexity is 88.21366784242898
At time: 290.821706533432 and batch: 200, loss is 4.493770627975464 and perplexity is 89.4581240408539
At time: 291.77981662750244 and batch: 250, loss is 4.514375286102295 and perplexity is 91.32049901408816
At time: 292.72962832450867 and batch: 300, loss is 4.493660621643066 and perplexity is 89.44828362198908
At time: 293.6964111328125 and batch: 350, loss is 4.556566076278687 and perplexity is 95.25581635897446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.017966434873384 and perplexity of 151.10371190695008
Finished 39 epochs...
Completing Train Step...
At time: 295.5631775856018 and batch: 50, loss is 4.578805313110352 and perplexity is 97.39796460105289
At time: 296.525367975235 and batch: 100, loss is 4.525577964782715 and perplexity is 92.34928504176257
At time: 297.4694199562073 and batch: 150, loss is 4.4795490074157716 and perplexity is 88.19488846446937
At time: 298.4177100658417 and batch: 200, loss is 4.4938829898834225 and perplexity is 89.46817629108794
At time: 299.36971950531006 and batch: 250, loss is 4.514327049255371 and perplexity is 91.31609410739647
At time: 300.3264329433441 and batch: 300, loss is 4.493567190170288 and perplexity is 89.43992672751736
At time: 301.28369784355164 and batch: 350, loss is 4.556109924316406 and perplexity is 95.21237514007552
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.017950649919181 and perplexity of 151.10132676060243
Finished 40 epochs...
Completing Train Step...
At time: 303.10312151908875 and batch: 50, loss is 4.578106260299682 and perplexity is 97.32990207256765
At time: 304.0676198005676 and batch: 100, loss is 4.525149698257446 and perplexity is 92.30974340213135
At time: 305.0265271663666 and batch: 150, loss is 4.479338760375977 and perplexity is 88.17634769938346
At time: 305.9788637161255 and batch: 200, loss is 4.493938016891479 and perplexity is 89.47309959260157
At time: 306.934677362442 and batch: 250, loss is 4.514277477264404 and perplexity is 91.31156749900163
At time: 307.8908679485321 and batch: 300, loss is 4.493463792800903 and perplexity is 89.43067935246114
At time: 308.8434257507324 and batch: 350, loss is 4.555662012100219 and perplexity is 95.16973790370052
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.0179117136988145 and perplexity of 151.0954435605816
Finished 41 epochs...
Completing Train Step...
At time: 310.6848204135895 and batch: 50, loss is 4.577420997619629 and perplexity is 97.26322837013315
At time: 311.6325623989105 and batch: 100, loss is 4.524722328186035 and perplexity is 92.27030140926561
At time: 312.5782060623169 and batch: 150, loss is 4.479131021499634 and perplexity is 88.1580319465052
At time: 313.5208818912506 and batch: 200, loss is 4.494016647338867 and perplexity is 89.48013517905375
At time: 314.46304631233215 and batch: 250, loss is 4.514224328994751 and perplexity is 91.30671457615314
At time: 315.41757583618164 and batch: 300, loss is 4.493358917236328 and perplexity is 89.42130075127538
At time: 316.36358428001404 and batch: 350, loss is 4.555222091674804 and perplexity is 95.12787999986378
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.017879091460129 and perplexity of 151.0905145693553
Finished 42 epochs...
Completing Train Step...
At time: 318.2104046344757 and batch: 50, loss is 4.576746339797974 and perplexity is 97.19763110269672
At time: 319.1579270362854 and batch: 100, loss is 4.524291276931763 and perplexity is 92.23053675102965
At time: 320.10333609580994 and batch: 150, loss is 4.478911094665527 and perplexity is 88.13864576148741
At time: 321.0629940032959 and batch: 200, loss is 4.494095258712768 and perplexity is 89.48716961190662
At time: 322.00579047203064 and batch: 250, loss is 4.51412615776062 and perplexity is 91.29775132327272
At time: 322.95003294944763 and batch: 300, loss is 4.493147840499878 and perplexity is 89.4024279868145
At time: 323.90373492240906 and batch: 350, loss is 4.554765272140503 and perplexity is 95.08443365034927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.017876460634429 and perplexity of 151.09011707706932
Finished 43 epochs...
Completing Train Step...
At time: 325.72805762290955 and batch: 50, loss is 4.5760420799255375 and perplexity is 97.12920280989387
At time: 326.69970178604126 and batch: 100, loss is 4.523870325088501 and perplexity is 92.19172030707925
At time: 327.6417410373688 and batch: 150, loss is 4.478684864044189 and perplexity is 88.11870835620375
At time: 328.6092150211334 and batch: 200, loss is 4.49414966583252 and perplexity is 89.49203848350938
At time: 329.5800187587738 and batch: 250, loss is 4.514038496017456 and perplexity is 91.28974835402701
At time: 330.5365800857544 and batch: 300, loss is 4.493023328781128 and perplexity is 89.39129702982707
At time: 331.4798061847687 and batch: 350, loss is 4.5543062782287596 and perplexity is 95.04080048864603
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.017831736597522 and perplexity of 151.08335986820288
Finished 44 epochs...
Completing Train Step...
At time: 333.3415880203247 and batch: 50, loss is 4.575353002548217 and perplexity is 97.06229632807937
At time: 334.2921407222748 and batch: 100, loss is 4.523392944335938 and perplexity is 92.147720257483
At time: 335.2410502433777 and batch: 150, loss is 4.478423728942871 and perplexity is 88.09570047258285
At time: 336.18521904945374 and batch: 200, loss is 4.494160108566284 and perplexity is 89.49297302992085
At time: 337.12548565864563 and batch: 250, loss is 4.513943099975586 and perplexity is 91.2810400887444
At time: 338.0688998699188 and batch: 300, loss is 4.49291467666626 and perplexity is 89.38158500397962
At time: 339.0083420276642 and batch: 350, loss is 4.5538737583160405 and perplexity is 94.9997023384388
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.0177764892578125 and perplexity of 151.07501314506507
Finished 45 epochs...
Completing Train Step...
At time: 340.86864590644836 and batch: 50, loss is 4.574716701507568 and perplexity is 97.00055513299448
At time: 341.8257312774658 and batch: 100, loss is 4.522977313995361 and perplexity is 92.10942882722037
At time: 342.78702092170715 and batch: 150, loss is 4.478202133178711 and perplexity is 88.0761810013134
At time: 343.7488179206848 and batch: 200, loss is 4.49420470237732 and perplexity is 89.49696395163373
At time: 344.7085294723511 and batch: 250, loss is 4.513867292404175 and perplexity is 91.27412055705913
At time: 345.6546378135681 and batch: 300, loss is 4.492814569473267 and perplexity is 89.37263771225122
At time: 346.6008427143097 and batch: 350, loss is 4.553452558517456 and perplexity is 94.95969690867922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.017740710028287 and perplexity of 151.06960789419256
Finished 46 epochs...
Completing Train Step...
At time: 348.4526357650757 and batch: 50, loss is 4.574088926315308 and perplexity is 96.93967970088882
At time: 349.41932249069214 and batch: 100, loss is 4.52256760597229 and perplexity is 92.07169858494859
At time: 350.38176679611206 and batch: 150, loss is 4.477979793548584 and perplexity is 88.05660035266428
At time: 351.3487319946289 and batch: 200, loss is 4.494213352203369 and perplexity is 89.4977380881519
At time: 352.3031244277954 and batch: 250, loss is 4.5137953948974605 and perplexity is 91.26755841126732
At time: 353.3394320011139 and batch: 300, loss is 4.49269250869751 and perplexity is 89.36172948450752
At time: 354.29452323913574 and batch: 350, loss is 4.553047361373902 and perplexity is 94.92122730515209
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.017704404633621 and perplexity of 151.06412335201583
Finished 47 epochs...
Completing Train Step...
At time: 356.1291208267212 and batch: 50, loss is 4.573459625244141 and perplexity is 96.87869464760674
At time: 357.0922050476074 and batch: 100, loss is 4.522165727615357 and perplexity is 92.03470439608105
At time: 358.0419108867645 and batch: 150, loss is 4.477762174606323 and perplexity is 88.03743965337767
At time: 358.99645233154297 and batch: 200, loss is 4.4942386436462405 and perplexity is 89.50000164370607
At time: 359.9523479938507 and batch: 250, loss is 4.51372594833374 and perplexity is 91.26122041303518
At time: 360.90790247917175 and batch: 300, loss is 4.49256383895874 and perplexity is 89.35023207381904
At time: 361.8629288673401 and batch: 350, loss is 4.552634582519532 and perplexity is 94.88205391521937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.017673887055496 and perplexity of 151.05951331117353
Finished 48 epochs...
Completing Train Step...
At time: 363.69791054725647 and batch: 50, loss is 4.57284969329834 and perplexity is 96.8196232534702
At time: 364.6492977142334 and batch: 100, loss is 4.5217732906341555 and perplexity is 91.99859366057956
At time: 365.5968163013458 and batch: 150, loss is 4.4775467872619625 and perplexity is 88.01847954500339
At time: 366.5449311733246 and batch: 200, loss is 4.494241456985474 and perplexity is 89.50025343792629
At time: 367.5008955001831 and batch: 250, loss is 4.513631296157837 and perplexity is 91.25258274874079
At time: 368.45235538482666 and batch: 300, loss is 4.492423181533813 and perplexity is 89.3376651840928
At time: 369.4013421535492 and batch: 350, loss is 4.552213096618653 and perplexity is 94.84207089397947
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.017632846174569 and perplexity of 151.053313822892
Finished 49 epochs...
Completing Train Step...
At time: 371.3093264102936 and batch: 50, loss is 4.572242212295532 and perplexity is 96.76082503285392
At time: 372.27175760269165 and batch: 100, loss is 4.521371002197266 and perplexity is 91.96159113348322
At time: 373.2240905761719 and batch: 150, loss is 4.477283391952515 and perplexity is 87.99529894331141
At time: 374.1780869960785 and batch: 200, loss is 4.494243459701538 and perplexity is 89.50043268170108
At time: 375.1299877166748 and batch: 250, loss is 4.513525066375732 and perplexity is 91.24288952162269
At time: 376.08310437202454 and batch: 300, loss is 4.492276544570923 and perplexity is 89.32456594063859
At time: 377.0362241268158 and batch: 350, loss is 4.551781215667725 and perplexity is 94.80111925396693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.017603907091864 and perplexity of 151.0489425418011
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efbe69b8e48>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'wordvec_dim': 200, 'seq_len': 35, 'lr': 24.308474250631857, 'anneal': 7.2762522963817196, 'tune_wordvecs': True, 'wordvec_source': '', 'batch_size': 80, 'num_layers': 1, 'dropout': 0.13888425829454654}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.2130835056304932 and batch: 50, loss is 6.218865661621094 and perplexity is 502.1333197487494
At time: 2.182340383529663 and batch: 100, loss is 5.375000829696655 and perplexity is 215.94005147527827
At time: 3.140958547592163 and batch: 150, loss is 5.305537672042846 and perplexity is 201.44928694994795
At time: 4.117058992385864 and batch: 200, loss is 5.267729959487915 and perplexity is 193.97513087650944
At time: 5.082409143447876 and batch: 250, loss is 5.259104375839233 and perplexity is 192.3091773989031
At time: 6.043911457061768 and batch: 300, loss is 5.234451313018798 and perplexity is 187.62613008100624
At time: 7.0050554275512695 and batch: 350, loss is 5.261153621673584 and perplexity is 192.70367024795596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.265923861799569 and perplexity of 193.62510902301435
Finished 1 epochs...
Completing Train Step...
At time: 8.841419696807861 and batch: 50, loss is 5.1123304271698 and perplexity is 166.05688778842622
At time: 9.827505350112915 and batch: 100, loss is 5.079169435501099 and perplexity is 160.6405781440364
At time: 10.783316850662231 and batch: 150, loss is 5.048289375305176 and perplexity is 155.75579675492835
At time: 11.744828462600708 and batch: 200, loss is 5.058640098571777 and perplexity is 157.3763544076966
At time: 12.694952964782715 and batch: 250, loss is 5.078278551101684 and perplexity is 160.49752968833494
At time: 13.644654512405396 and batch: 300, loss is 5.064682722091675 and perplexity is 158.3301994291629
At time: 14.59397292137146 and batch: 350, loss is 5.100487184524536 and perplexity is 164.10183570318284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.259185264850485 and perplexity of 192.32473372727705
Finished 2 epochs...
Completing Train Step...
At time: 16.430760145187378 and batch: 50, loss is 5.006569719314575 and perplexity is 149.39140177184012
At time: 17.38788151741028 and batch: 100, loss is 5.034081945419311 and perplexity is 153.55855274784784
At time: 18.339321851730347 and batch: 150, loss is 5.011536855697631 and perplexity is 150.13529521952958
At time: 19.28930354118347 and batch: 200, loss is 5.01458969116211 and perplexity is 150.59433390151716
At time: 20.242791175842285 and batch: 250, loss is 5.003905553817749 and perplexity is 148.99392805669427
At time: 21.194340467453003 and batch: 300, loss is 4.992852563858032 and perplexity is 147.356167415329
At time: 22.15005087852478 and batch: 350, loss is 5.005950126647949 and perplexity is 149.29886862423547
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.31811155121902 and perplexity of 203.99827771173022
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 23.986785650253296 and batch: 50, loss is 4.931234998703003 and perplexity is 138.5505164314033
At time: 24.938758850097656 and batch: 100, loss is 4.796197805404663 and perplexity is 121.04928847268428
At time: 25.890460968017578 and batch: 150, loss is 4.733207063674927 and perplexity is 113.65949169156106
At time: 26.84317421913147 and batch: 200, loss is 4.731950578689575 and perplexity is 113.51676992946034
At time: 27.80031132698059 and batch: 250, loss is 4.699985990524292 and perplexity is 109.94563216067121
At time: 28.770813703536987 and batch: 300, loss is 4.700657863616943 and perplexity is 110.01952649360175
At time: 29.728189706802368 and batch: 350, loss is 4.707259092330933 and perplexity is 110.74819295192239
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.070593472184806 and perplexity of 159.26882091599055
Finished 4 epochs...
Completing Train Step...
At time: 31.548216104507446 and batch: 50, loss is 4.74262845993042 and perplexity is 114.73538304211523
At time: 32.52222561836243 and batch: 100, loss is 4.675346374511719 and perplexity is 107.26971615348394
At time: 33.472363233566284 and batch: 150, loss is 4.638028812408447 and perplexity is 103.34044328298666
At time: 34.42159295082092 and batch: 200, loss is 4.657888517379761 and perplexity is 105.41326872124564
At time: 35.372140407562256 and batch: 250, loss is 4.6496890258789065 and perplexity is 104.55246740956112
At time: 36.323068618774414 and batch: 300, loss is 4.65107871055603 and perplexity is 104.69786337534919
At time: 37.26918864250183 and batch: 350, loss is 4.6882878684997555 and perplexity is 108.66696830414443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.058042855098329 and perplexity of 157.2823904696191
Finished 5 epochs...
Completing Train Step...
At time: 39.0996880531311 and batch: 50, loss is 4.691611194610596 and perplexity is 109.02870482858084
At time: 40.04442501068115 and batch: 100, loss is 4.628833885192871 and perplexity is 102.39459061583943
At time: 40.992753744125366 and batch: 150, loss is 4.592861213684082 and perplexity is 98.77664732260062
At time: 41.95167922973633 and batch: 200, loss is 4.613230791091919 and perplexity is 100.80931792447544
At time: 42.894426107406616 and batch: 250, loss is 4.612569894790649 and perplexity is 100.74271543022094
At time: 43.83352255821228 and batch: 300, loss is 4.609614543914795 and perplexity is 100.44542487327259
At time: 44.773958921432495 and batch: 350, loss is 4.654331350326538 and perplexity is 105.03896224479938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.045239152579472 and perplexity of 155.28143071283353
Finished 6 epochs...
Completing Train Step...
At time: 46.58003568649292 and batch: 50, loss is 4.648611087799072 and perplexity is 104.43982704417614
At time: 47.51840424537659 and batch: 100, loss is 4.5920131969451905 and perplexity is 98.6929185789652
At time: 48.465954065322876 and batch: 150, loss is 4.558177671432495 and perplexity is 95.40945393852718
At time: 49.42102766036987 and batch: 200, loss is 4.576813592910766 and perplexity is 97.20416816576089
At time: 50.374515771865845 and batch: 250, loss is 4.58170563697815 and perplexity is 97.68086028871078
At time: 51.32366394996643 and batch: 300, loss is 4.5765386486053465 and perplexity is 97.17744610696793
At time: 52.29037666320801 and batch: 350, loss is 4.625304851531983 and perplexity is 102.03387352460824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.042718821558459 and perplexity of 154.8905628710956
Finished 7 epochs...
Completing Train Step...
At time: 54.10736036300659 and batch: 50, loss is 4.613959379196167 and perplexity is 100.88279315765273
At time: 55.078866720199585 and batch: 100, loss is 4.560484809875488 and perplexity is 95.62983087983689
At time: 56.036184549331665 and batch: 150, loss is 4.526536598205566 and perplexity is 92.43785660002507
At time: 56.99628186225891 and batch: 200, loss is 4.54469560623169 and perplexity is 94.1317697239441
At time: 57.953582525253296 and batch: 250, loss is 4.553425903320313 and perplexity is 94.95716577297152
At time: 58.907124042510986 and batch: 300, loss is 4.546063842773438 and perplexity is 94.26065240190816
At time: 59.857038259506226 and batch: 350, loss is 4.599543151855468 and perplexity is 99.43887679703907
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.041551261112608 and perplexity of 154.70982430858186
Finished 8 epochs...
Completing Train Step...
At time: 61.6830518245697 and batch: 50, loss is 4.586815195083618 and perplexity is 98.18124360030141
At time: 62.64695906639099 and batch: 100, loss is 4.534816741943359 and perplexity is 93.20643290946701
At time: 63.595142126083374 and batch: 150, loss is 4.500327224731445 and perplexity is 90.04659195200112
At time: 64.54577565193176 and batch: 200, loss is 4.516975002288818 and perplexity is 91.55821525703455
At time: 65.49896764755249 and batch: 250, loss is 4.5303365516662595 and perplexity is 92.78978438423621
At time: 66.45579433441162 and batch: 300, loss is 4.520239276885986 and perplexity is 91.85757474322477
At time: 67.41126751899719 and batch: 350, loss is 4.575993299484253 and perplexity is 97.12446492007831
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.040034327013739 and perplexity of 154.47531761122934
Finished 9 epochs...
Completing Train Step...
At time: 69.2604968547821 and batch: 50, loss is 4.560489454269409 and perplexity is 95.63027502347349
At time: 70.21117687225342 and batch: 100, loss is 4.512845411300659 and perplexity is 91.18089689788201
At time: 71.16158509254456 and batch: 150, loss is 4.477260398864746 and perplexity is 87.99327568294015
At time: 72.11179566383362 and batch: 200, loss is 4.494066047668457 and perplexity is 89.48455563640844
At time: 73.06459021568298 and batch: 250, loss is 4.510463800430298 and perplexity is 90.96399786972455
At time: 74.02129817008972 and batch: 300, loss is 4.499683303833008 and perplexity is 89.9886277337987
At time: 74.97773718833923 and batch: 350, loss is 4.557157783508301 and perplexity is 95.3121965928279
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.039946457435345 and perplexity of 154.46174452653793
Finished 10 epochs...
Completing Train Step...
At time: 76.81170129776001 and batch: 50, loss is 4.539130992889405 and perplexity is 93.60941751412564
At time: 77.77083492279053 and batch: 100, loss is 4.493998575210571 and perplexity is 89.47851809718294
At time: 78.72387671470642 and batch: 150, loss is 4.457572193145752 and perplexity is 86.2777888121221
At time: 79.67512130737305 and batch: 200, loss is 4.4706712818145755 and perplexity is 87.41538367969729
At time: 80.64019584655762 and batch: 250, loss is 4.489757823944092 and perplexity is 89.09986541164058
At time: 81.59951186180115 and batch: 300, loss is 4.479051351547241 and perplexity is 88.15100868007237
At time: 82.55558800697327 and batch: 350, loss is 4.538309354782104 and perplexity is 93.53253603819417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.040783586173222 and perplexity of 154.59110302908934
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 84.37585353851318 and batch: 50, loss is 4.508754730224609 and perplexity is 90.80866678484885
At time: 85.3416748046875 and batch: 100, loss is 4.442719268798828 and perplexity is 85.00578124281908
At time: 86.2932996749878 and batch: 150, loss is 4.371286325454712 and perplexity is 79.1453729572085
At time: 87.25073790550232 and batch: 200, loss is 4.36657455444336 and perplexity is 78.7733352497625
At time: 88.20815634727478 and batch: 250, loss is 4.367901878356934 and perplexity is 78.87796240309189
At time: 89.16537690162659 and batch: 300, loss is 4.333972043991089 and perplexity is 76.24654050187897
At time: 90.11853361129761 and batch: 350, loss is 4.416430072784424 and perplexity is 82.80016654352526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.968376948915679 and perplexity of 143.79331397725082
Finished 12 epochs...
Completing Train Step...
At time: 91.94882774353027 and batch: 50, loss is 4.460446319580078 and perplexity is 86.52611878055937
At time: 92.90014410018921 and batch: 100, loss is 4.407255229949951 and perplexity is 82.04396236070684
At time: 93.85099244117737 and batch: 150, loss is 4.343617525100708 and perplexity is 76.98553330918553
At time: 94.80492663383484 and batch: 200, loss is 4.346156263351441 and perplexity is 77.18122773069997
At time: 95.7628002166748 and batch: 250, loss is 4.357324943542481 and perplexity is 78.0480719219653
At time: 96.72022438049316 and batch: 300, loss is 4.338200931549072 and perplexity is 76.56966128753791
At time: 97.67725348472595 and batch: 350, loss is 4.420569133758545 and perplexity is 83.1435917200977
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.963228949185075 and perplexity of 143.05496816827613
Finished 13 epochs...
Completing Train Step...
At time: 99.50653433799744 and batch: 50, loss is 4.445909557342529 and perplexity is 85.27740726517729
At time: 100.45736360549927 and batch: 100, loss is 4.394499998092652 and perplexity is 81.0041184217337
At time: 101.42225456237793 and batch: 150, loss is 4.332594385147095 and perplexity is 76.14157110364468
At time: 102.37270593643188 and batch: 200, loss is 4.339675722122192 and perplexity is 76.68266881292564
At time: 103.32253861427307 and batch: 250, loss is 4.356717777252197 and perplexity is 78.00069814700764
At time: 104.27250218391418 and batch: 300, loss is 4.340788269042969 and perplexity is 76.76802935498988
At time: 105.22278165817261 and batch: 350, loss is 4.420341510772705 and perplexity is 83.12466848126064
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.961172169652478 and perplexity of 142.76103801609057
Finished 14 epochs...
Completing Train Step...
At time: 107.04116916656494 and batch: 50, loss is 4.436210384368897 and perplexity is 84.45428519588604
At time: 108.01287651062012 and batch: 100, loss is 4.386085414886475 and perplexity is 80.32536225729358
At time: 108.96542239189148 and batch: 150, loss is 4.326360864639282 and perplexity is 75.66841729775092
At time: 109.91775679588318 and batch: 200, loss is 4.336257438659668 and perplexity is 76.42099320975287
At time: 110.86936974525452 and batch: 250, loss is 4.35607138633728 and perplexity is 77.95029549603102
At time: 111.82145237922668 and batch: 300, loss is 4.341008453369141 and perplexity is 76.78493433284223
At time: 112.7764003276825 and batch: 350, loss is 4.4186937046051025 and perplexity is 82.98780793065475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.960243488180226 and perplexity of 142.62852002814978
Finished 15 epochs...
Completing Train Step...
At time: 114.60809779167175 and batch: 50, loss is 4.428833074569702 and perplexity is 83.8335323285867
At time: 115.57242846488953 and batch: 100, loss is 4.379448146820068 and perplexity is 79.7939866872489
At time: 116.52293157577515 and batch: 150, loss is 4.321551027297974 and perplexity is 75.30533839388089
At time: 117.47534847259521 and batch: 200, loss is 4.333390483856201 and perplexity is 76.20221144475555
At time: 118.42542171478271 and batch: 250, loss is 4.355149440765381 and perplexity is 77.87846268432956
At time: 119.38180685043335 and batch: 300, loss is 4.340279684066773 and perplexity is 76.7289962152833
At time: 120.33831453323364 and batch: 350, loss is 4.416632022857666 and perplexity is 82.8168897317909
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.959852547481142 and perplexity of 142.57277163268898
Finished 16 epochs...
Completing Train Step...
At time: 122.17486453056335 and batch: 50, loss is 4.422936630249024 and perplexity is 83.34066707725778
At time: 123.12513780593872 and batch: 100, loss is 4.37407546043396 and perplexity is 79.36642821854586
At time: 124.07543015480042 and batch: 150, loss is 4.317207832336425 and perplexity is 74.97898185564158
At time: 125.02414631843567 and batch: 200, loss is 4.330581836700439 and perplexity is 75.98848659965496
At time: 125.97665524482727 and batch: 250, loss is 4.353919038772583 and perplexity is 77.78269979415339
At time: 126.92413806915283 and batch: 300, loss is 4.33919129371643 and perplexity is 76.64553054606502
At time: 127.8759651184082 and batch: 350, loss is 4.41426495552063 and perplexity is 82.62108840603625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.959730477168642 and perplexity of 142.5553687921085
Finished 17 epochs...
Completing Train Step...
At time: 129.71262741088867 and batch: 50, loss is 4.4173939037323 and perplexity is 82.88001037832768
At time: 130.65049386024475 and batch: 100, loss is 4.369289817810059 and perplexity is 78.98751624855863
At time: 131.58965420722961 and batch: 150, loss is 4.313372707366943 and perplexity is 74.6919787882301
At time: 132.53112840652466 and batch: 200, loss is 4.32801736831665 and perplexity is 75.7938661838498
At time: 133.47188591957092 and batch: 250, loss is 4.3522984981536865 and perplexity is 77.65675184913313
At time: 134.41286730766296 and batch: 300, loss is 4.33767294883728 and perplexity is 76.52924450075157
At time: 135.36399245262146 and batch: 350, loss is 4.412036762237549 and perplexity is 82.43719760002094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.9593484812769395 and perplexity of 142.50092362646666
Finished 18 epochs...
Completing Train Step...
At time: 137.19054627418518 and batch: 50, loss is 4.4127445316314695 and perplexity is 82.49556477821744
At time: 138.16009783744812 and batch: 100, loss is 4.364910125732422 and perplexity is 78.64233170219288
At time: 139.1105875968933 and batch: 150, loss is 4.309844942092895 and perplexity is 74.4289472487649
At time: 140.06162238121033 and batch: 200, loss is 4.325173130035401 and perplexity is 75.57859665220913
At time: 141.01511907577515 and batch: 250, loss is 4.350389652252197 and perplexity is 77.5086584653345
At time: 141.96106719970703 and batch: 300, loss is 4.335950355529786 and perplexity is 76.39752921485214
At time: 142.90787959098816 and batch: 350, loss is 4.409446811676025 and perplexity is 82.22396558319106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.95880126953125 and perplexity of 142.42296677868333
Finished 19 epochs...
Completing Train Step...
At time: 144.73105192184448 and batch: 50, loss is 4.40826418876648 and perplexity is 82.12678311421242
At time: 145.68326711654663 and batch: 100, loss is 4.360903682708741 and perplexity is 78.32788600597412
At time: 146.63815593719482 and batch: 150, loss is 4.306239633560181 and perplexity is 74.16109107238549
At time: 147.58757066726685 and batch: 200, loss is 4.322646751403808 and perplexity is 75.38789749119348
At time: 148.5370054244995 and batch: 250, loss is 4.348515529632568 and perplexity is 77.36353376850323
At time: 149.48500680923462 and batch: 300, loss is 4.334097843170166 and perplexity is 76.25613285742416
At time: 150.446222782135 and batch: 350, loss is 4.406714954376221 and perplexity is 81.99964798393692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.958866514008621 and perplexity of 142.43225939385903
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 152.2619926929474 and batch: 50, loss is 4.403334894180298 and perplexity is 81.72295212546854
At time: 153.20769953727722 and batch: 100, loss is 4.351630687713623 and perplexity is 77.60490917197407
At time: 154.15274286270142 and batch: 150, loss is 4.291110401153564 and perplexity is 73.04753555057017
At time: 155.10538053512573 and batch: 200, loss is 4.302275009155274 and perplexity is 73.8676522725009
At time: 156.05724024772644 and batch: 250, loss is 4.324894981384277 and perplexity is 75.55757749085772
At time: 157.00327587127686 and batch: 300, loss is 4.306792421340942 and perplexity is 74.20209775028927
At time: 157.94804906845093 and batch: 350, loss is 4.3859454536437985 and perplexity is 80.31412060648967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.9543478077855605 and perplexity of 141.79010181037432
Finished 21 epochs...
Completing Train Step...
At time: 159.75080800056458 and batch: 50, loss is 4.397176494598389 and perplexity is 81.2212160625777
At time: 160.71770238876343 and batch: 100, loss is 4.347393426895142 and perplexity is 77.27677262204811
At time: 161.67178630828857 and batch: 150, loss is 4.287928743362427 and perplexity is 72.81549262629771
At time: 162.61999320983887 and batch: 200, loss is 4.300809516906738 and perplexity is 73.759479083523
At time: 163.5677535533905 and batch: 250, loss is 4.324197740554809 and perplexity is 75.50491402454837
At time: 164.51557731628418 and batch: 300, loss is 4.307714748382568 and perplexity is 74.27056792268161
At time: 165.46355772018433 and batch: 350, loss is 4.386070852279663 and perplexity is 80.32419251914322
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.953048179889548 and perplexity of 141.60594713088264
Finished 22 epochs...
Completing Train Step...
At time: 167.26843571662903 and batch: 50, loss is 4.39438796043396 and perplexity is 80.99504341834188
At time: 168.2285087108612 and batch: 100, loss is 4.34525936126709 and perplexity is 77.11203476097333
At time: 169.17163228988647 and batch: 150, loss is 4.286552181243897 and perplexity is 72.71532653574641
At time: 170.11684346199036 and batch: 200, loss is 4.300730009078979 and perplexity is 73.75361486069335
At time: 171.06196856498718 and batch: 250, loss is 4.324462299346924 and perplexity is 75.52489215598006
At time: 172.00501418113708 and batch: 300, loss is 4.308455839157104 and perplexity is 74.32562955570272
At time: 172.95541191101074 and batch: 350, loss is 4.385806331634521 and perplexity is 80.30294792185875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.952344697097252 and perplexity of 141.50636481511896
Finished 23 epochs...
Completing Train Step...
At time: 174.7784059047699 and batch: 50, loss is 4.392276086807251 and perplexity is 80.82417261453173
At time: 175.74189615249634 and batch: 100, loss is 4.343721771240235 and perplexity is 76.99355917215777
At time: 176.7166817188263 and batch: 150, loss is 4.285689926147461 and perplexity is 72.65265439841983
At time: 177.68227314949036 and batch: 200, loss is 4.300866746902466 and perplexity is 73.76370045898936
At time: 178.62984538078308 and batch: 250, loss is 4.324705781936646 and perplexity is 75.5432833911926
At time: 179.57698917388916 and batch: 300, loss is 4.308924922943115 and perplexity is 74.36050268198196
At time: 180.5233449935913 and batch: 350, loss is 4.385348443984985 and perplexity is 80.26618661070145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.951943233095366 and perplexity of 141.44956650560914
Finished 24 epochs...
Completing Train Step...
At time: 182.35072255134583 and batch: 50, loss is 4.390518341064453 and perplexity is 80.68222905609274
At time: 183.29957556724548 and batch: 100, loss is 4.342505321502686 and perplexity is 76.89995731982648
At time: 184.24781918525696 and batch: 150, loss is 4.28502345085144 and perplexity is 72.60424933125549
At time: 185.20313262939453 and batch: 200, loss is 4.301090517044067 and perplexity is 73.78020841961157
At time: 186.16454601287842 and batch: 250, loss is 4.324949340820313 and perplexity is 75.5616848697947
At time: 187.11450338363647 and batch: 300, loss is 4.309214220046997 and perplexity is 74.38201807207105
At time: 188.06240248680115 and batch: 350, loss is 4.384852266311645 and perplexity is 80.226370199805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.95166331324084 and perplexity of 141.40997750466235
Finished 25 epochs...
Completing Train Step...
At time: 189.87891578674316 and batch: 50, loss is 4.388959484100342 and perplexity is 80.55655498085167
At time: 190.85070276260376 and batch: 100, loss is 4.341445903778077 and perplexity is 76.81853128172997
At time: 191.80786156654358 and batch: 150, loss is 4.284498853683472 and perplexity is 72.5661713363512
At time: 192.7622377872467 and batch: 200, loss is 4.301290731430054 and perplexity is 73.79498175760631
At time: 193.71375632286072 and batch: 250, loss is 4.325099496841431 and perplexity is 75.57303176362464
At time: 194.6679310798645 and batch: 300, loss is 4.309455947875977 and perplexity is 74.40000044914763
At time: 195.6194670200348 and batch: 350, loss is 4.384375324249268 and perplexity is 80.18811599259026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.951470210634429 and perplexity of 141.3826735057551
Finished 26 epochs...
Completing Train Step...
At time: 197.46328949928284 and batch: 50, loss is 4.3875783348083495 and perplexity is 80.44537115038317
At time: 198.41964292526245 and batch: 100, loss is 4.340528688430786 and perplexity is 76.74810444910534
At time: 199.38725638389587 and batch: 150, loss is 4.284016819000244 and perplexity is 72.53120035422833
At time: 200.33738374710083 and batch: 200, loss is 4.301524591445923 and perplexity is 73.812241471311
At time: 201.29231357574463 and batch: 250, loss is 4.3252302837371825 and perplexity is 75.5829163722261
At time: 202.2418520450592 and batch: 300, loss is 4.309586038589478 and perplexity is 74.40967982787554
At time: 203.20018315315247 and batch: 350, loss is 4.383853693008422 and perplexity is 80.14629827380648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.951307625606142 and perplexity of 141.35968866832977
Finished 27 epochs...
Completing Train Step...
At time: 205.0494349002838 and batch: 50, loss is 4.386328372955322 and perplexity is 80.34488032312686
At time: 206.0206334590912 and batch: 100, loss is 4.339729490280152 and perplexity is 76.6867920096225
At time: 206.97704577445984 and batch: 150, loss is 4.2836212635040285 and perplexity is 72.50251591279935
At time: 207.93276715278625 and batch: 200, loss is 4.301699619293213 and perplexity is 73.82516179971503
At time: 208.88306069374084 and batch: 250, loss is 4.32529725074768 and perplexity is 75.58797810366288
At time: 209.83905673027039 and batch: 300, loss is 4.30966329574585 and perplexity is 74.41542873021464
At time: 210.7820737361908 and batch: 350, loss is 4.383354587554932 and perplexity is 80.1063068000724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.951166613348599 and perplexity of 141.33975662486935
Finished 28 epochs...
Completing Train Step...
At time: 212.6215271949768 and batch: 50, loss is 4.38516282081604 and perplexity is 80.25128872952239
At time: 213.58436512947083 and batch: 100, loss is 4.338951234817505 and perplexity is 76.6271333126927
At time: 214.53336358070374 and batch: 150, loss is 4.283205032348633 and perplexity is 72.47234438643211
At time: 215.47815012931824 and batch: 200, loss is 4.301891412734985 and perplexity is 73.83932233949189
At time: 216.42219519615173 and batch: 250, loss is 4.325355930328369 and perplexity is 75.59241370466144
At time: 217.37089824676514 and batch: 300, loss is 4.309708442687988 and perplexity is 74.41878843510936
At time: 218.32035446166992 and batch: 350, loss is 4.382812395095825 and perplexity is 80.06288553700294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.951061906485722 and perplexity of 141.32495815711746
Finished 29 epochs...
Completing Train Step...
At time: 220.12284183502197 and batch: 50, loss is 4.384055061340332 and perplexity is 80.16243882524202
At time: 221.08506107330322 and batch: 100, loss is 4.338261737823486 and perplexity is 76.57431734493126
At time: 222.04452896118164 and batch: 150, loss is 4.282864017486572 and perplexity is 72.44763445337463
At time: 223.02291989326477 and batch: 200, loss is 4.302030096054077 and perplexity is 73.8495633319044
At time: 223.98331904411316 and batch: 250, loss is 4.325372295379639 and perplexity is 75.59365078850975
At time: 224.9416196346283 and batch: 300, loss is 4.309766836166382 and perplexity is 74.42313413390296
At time: 225.90318703651428 and batch: 350, loss is 4.38229453086853 and perplexity is 80.02143456656312
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.950992978852371 and perplexity of 141.31521729792928
Finished 30 epochs...
Completing Train Step...
At time: 227.7620506286621 and batch: 50, loss is 4.383018865585327 and perplexity is 80.0794178668311
At time: 228.72188019752502 and batch: 100, loss is 4.337594032287598 and perplexity is 76.52320531512474
At time: 229.68316841125488 and batch: 150, loss is 4.2825168800354 and perplexity is 72.42248953083173
At time: 230.64530181884766 and batch: 200, loss is 4.30216965675354 and perplexity is 73.85987054784243
At time: 231.60083985328674 and batch: 250, loss is 4.325387449264526 and perplexity is 75.59479633467173
At time: 232.55391812324524 and batch: 300, loss is 4.3098007106781 and perplexity is 74.4256552239324
At time: 233.50375247001648 and batch: 350, loss is 4.381755447387695 and perplexity is 79.97830795854094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.950907213934537 and perplexity of 141.30309792964474
Finished 31 epochs...
Completing Train Step...
At time: 235.33210039138794 and batch: 50, loss is 4.382025547027588 and perplexity is 79.99991298834384
At time: 236.28180313110352 and batch: 100, loss is 4.336954708099365 and perplexity is 76.47429781454562
At time: 237.2431161403656 and batch: 150, loss is 4.2821743869781494 and perplexity is 72.39768957812683
At time: 238.19957613945007 and batch: 200, loss is 4.3022901153564455 and perplexity is 73.86876814054446
At time: 239.15707325935364 and batch: 250, loss is 4.325398626327515 and perplexity is 75.59564126719393
At time: 240.11508870124817 and batch: 300, loss is 4.30978196144104 and perplexity is 74.4242598127607
At time: 241.07088923454285 and batch: 350, loss is 4.381250419616699 and perplexity is 79.93792688958352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.9508514404296875 and perplexity of 141.29521718039769
Finished 32 epochs...
Completing Train Step...
At time: 242.9900586605072 and batch: 50, loss is 4.381070995330811 and perplexity is 79.92358537078283
At time: 243.96256852149963 and batch: 100, loss is 4.336353158950805 and perplexity is 76.42830859958191
At time: 244.91665029525757 and batch: 150, loss is 4.281852016448974 and perplexity is 72.37435445810605
At time: 245.87088918685913 and batch: 200, loss is 4.302396516799927 and perplexity is 73.8766283022617
At time: 246.82543540000916 and batch: 250, loss is 4.325390396118164 and perplexity is 75.59501910180052
At time: 247.79248905181885 and batch: 300, loss is 4.309752149581909 and perplexity is 74.4220411202831
At time: 248.75411295890808 and batch: 350, loss is 4.380731248855591 and perplexity is 79.89643622654036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.950783565126616 and perplexity of 141.28562705017922
Finished 33 epochs...
Completing Train Step...
At time: 250.60820508003235 and batch: 50, loss is 4.380178365707398 and perplexity is 79.85227504246215
At time: 251.56165957450867 and batch: 100, loss is 4.335751218795776 and perplexity is 76.38231717508616
At time: 252.51473259925842 and batch: 150, loss is 4.281537456512451 and perplexity is 72.35159196603352
At time: 253.4667785167694 and batch: 200, loss is 4.302473258972168 and perplexity is 73.88229797274408
At time: 254.42557048797607 and batch: 250, loss is 4.325374641418457 and perplexity is 75.59382813435695
At time: 255.3894100189209 and batch: 300, loss is 4.309663209915161 and perplexity is 74.41542234308744
At time: 256.34954261779785 and batch: 350, loss is 4.380259656906128 and perplexity is 79.85876659347113
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.950718846814386 and perplexity of 141.27648357873235
Finished 34 epochs...
Completing Train Step...
At time: 258.22588658332825 and batch: 50, loss is 4.379321098327637 and perplexity is 79.78384962549804
At time: 259.189635515213 and batch: 100, loss is 4.3351459980010985 and perplexity is 76.33610299468826
At time: 260.1582946777344 and batch: 150, loss is 4.281190423965454 and perplexity is 72.32648796499893
At time: 261.1246192455292 and batch: 200, loss is 4.302523097991943 and perplexity is 73.88598028581445
At time: 262.094101190567 and batch: 250, loss is 4.325300989151001 and perplexity is 75.58826068253939
At time: 263.0644268989563 and batch: 300, loss is 4.309563570022583 and perplexity is 74.4080079677889
At time: 264.03133177757263 and batch: 350, loss is 4.379750804901123 and perplexity is 79.81814063714909
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.950689381566541 and perplexity of 141.27232089345657
Finished 35 epochs...
Completing Train Step...
At time: 265.89743852615356 and batch: 50, loss is 4.378457241058349 and perplexity is 79.71495752778125
At time: 266.8769361972809 and batch: 100, loss is 4.334591178894043 and perplexity is 76.2937620130676
At time: 267.84157061576843 and batch: 150, loss is 4.2809122276306155 and perplexity is 72.30636979966499
At time: 268.806791305542 and batch: 200, loss is 4.30256872177124 and perplexity is 73.88935132037122
At time: 269.77095580101013 and batch: 250, loss is 4.3252627658844 and perplexity is 75.58537150751663
At time: 270.73469376564026 and batch: 300, loss is 4.309484567642212 and perplexity is 74.40212979023886
At time: 271.69788551330566 and batch: 350, loss is 4.379252376556397 and perplexity is 79.77836692642799
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.950637291217673 and perplexity of 141.26496216063697
Finished 36 epochs...
Completing Train Step...
At time: 273.5900065898895 and batch: 50, loss is 4.377634229660035 and perplexity is 79.64937819908889
At time: 274.5669512748718 and batch: 100, loss is 4.3340358066558835 and perplexity is 76.25140233948294
At time: 275.52987265586853 and batch: 150, loss is 4.280555410385132 and perplexity is 72.28057424238568
At time: 276.49234414100647 and batch: 200, loss is 4.302591848373413 and perplexity is 73.89106014976365
At time: 277.4550108909607 and batch: 250, loss is 4.32518816947937 and perplexity is 75.57973332082616
At time: 278.4169638156891 and batch: 300, loss is 4.309380798339844 and perplexity is 74.39440953370558
At time: 279.3773846626282 and batch: 350, loss is 4.378771209716797 and perplexity is 79.73998945546923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.950608878300108 and perplexity of 141.260948467933
Finished 37 epochs...
Completing Train Step...
At time: 281.2434115409851 and batch: 50, loss is 4.376841554641723 and perplexity is 79.58626714334964
At time: 282.206595659256 and batch: 100, loss is 4.333507032394409 and perplexity is 76.21109321867647
At time: 283.1692340373993 and batch: 150, loss is 4.2802096939086915 and perplexity is 72.25558997592864
At time: 284.1211156845093 and batch: 200, loss is 4.302649612426758 and perplexity is 73.89532852018186
At time: 285.0849657058716 and batch: 250, loss is 4.325104808807373 and perplexity is 75.57343320606172
At time: 286.0479464530945 and batch: 300, loss is 4.3092075634002684 and perplexity is 74.38152293890172
At time: 287.01107239723206 and batch: 350, loss is 4.378240976333618 and perplexity is 79.69771985845165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.950606773639548 and perplexity of 141.26065116189886
Finished 38 epochs...
Completing Train Step...
At time: 288.86992621421814 and batch: 50, loss is 4.376053438186646 and perplexity is 79.5235686067353
At time: 289.8333270549774 and batch: 100, loss is 4.3330171680450436 and perplexity is 76.17376926366633
At time: 290.7969675064087 and batch: 150, loss is 4.279976940155029 and perplexity is 72.23877417318516
At time: 291.7620759010315 and batch: 200, loss is 4.302636384963989 and perplexity is 73.89435107893964
At time: 292.72447180747986 and batch: 250, loss is 4.325029048919678 and perplexity is 75.56770798812299
At time: 293.6875693798065 and batch: 300, loss is 4.309089193344116 and perplexity is 74.37271891493143
At time: 294.6489746570587 and batch: 350, loss is 4.377711219787598 and perplexity is 79.65551065094284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.950554157125539 and perplexity of 141.25321871440454
Finished 39 epochs...
Completing Train Step...
At time: 296.4993281364441 and batch: 50, loss is 4.375271873474121 and perplexity is 79.46144007359537
At time: 297.4765696525574 and batch: 100, loss is 4.332431020736695 and perplexity is 76.12913329674954
At time: 298.42748951911926 and batch: 150, loss is 4.279665307998657 and perplexity is 72.21626575557144
At time: 299.3800973892212 and batch: 200, loss is 4.302543754577637 and perplexity is 73.88750653366125
At time: 300.3383934497833 and batch: 250, loss is 4.324843139648437 and perplexity is 75.55366055641595
At time: 301.3011200428009 and batch: 300, loss is 4.308861360549927 and perplexity is 74.35577630068447
At time: 302.2563931941986 and batch: 350, loss is 4.377132749557495 and perplexity is 79.60944563427257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.950547843143858 and perplexity of 141.2523268469849
Finished 40 epochs...
Completing Train Step...
At time: 304.09812211990356 and batch: 50, loss is 4.374549818038941 and perplexity is 79.40408521808743
At time: 305.04612827301025 and batch: 100, loss is 4.332062692642212 and perplexity is 76.10109796156544
At time: 305.9918541908264 and batch: 150, loss is 4.279568309783936 and perplexity is 72.20926124643722
At time: 306.9420835971832 and batch: 200, loss is 4.302582216262818 and perplexity is 73.89034842632799
At time: 307.9025444984436 and batch: 250, loss is 4.324766016006469 and perplexity is 75.54783380764276
At time: 308.8444411754608 and batch: 300, loss is 4.308699979782104 and perplexity is 74.34377767661262
At time: 309.7855567932129 and batch: 350, loss is 4.376621551513672 and perplexity is 79.56875984152938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.950517325565733 and perplexity of 141.24801623384008
Finished 41 epochs...
Completing Train Step...
At time: 311.5971987247467 and batch: 50, loss is 4.37377700805664 and perplexity is 79.34274465374858
At time: 312.53811836242676 and batch: 100, loss is 4.331528263092041 and perplexity is 76.06043815188401
At time: 313.4819037914276 and batch: 150, loss is 4.279225931167603 and perplexity is 72.18454257129962
At time: 314.44236636161804 and batch: 200, loss is 4.302613468170166 and perplexity is 73.89265767673498
At time: 315.40127897262573 and batch: 250, loss is 4.3246738147735595 and perplexity is 75.54086852533077
At time: 316.35845279693604 and batch: 300, loss is 4.308539333343506 and perplexity is 74.33183557275086
At time: 317.30247592926025 and batch: 350, loss is 4.376139326095581 and perplexity is 79.53039901307444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.950513116244612 and perplexity of 141.24742167683337
Finished 42 epochs...
Completing Train Step...
At time: 319.14806723594666 and batch: 50, loss is 4.373092737197876 and perplexity is 79.28847129668364
At time: 320.11225032806396 and batch: 100, loss is 4.3311146068573 and perplexity is 76.02898178393524
At time: 321.0512671470642 and batch: 150, loss is 4.2789124011993405 and perplexity is 72.16191410149528
At time: 322.0113492012024 and batch: 200, loss is 4.302597255706787 and perplexity is 73.8914597044395
At time: 322.971399307251 and batch: 250, loss is 4.324593257904053 and perplexity is 75.53478343454378
At time: 323.93674659729004 and batch: 300, loss is 4.3083843326568605 and perplexity is 74.3203149800704
At time: 324.89926075935364 and batch: 350, loss is 4.375690517425537 and perplexity is 79.49471308914035
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.950517851730873 and perplexity of 141.24809055364184
Annealing...
Finished 43 epochs...
Completing Train Step...
At time: 326.74979996681213 and batch: 50, loss is 4.372121305465698 and perplexity is 79.21148535901574
At time: 327.7172956466675 and batch: 100, loss is 4.329283647537231 and perplexity is 75.88990317363556
At time: 328.6703910827637 and batch: 150, loss is 4.276464786529541 and perplexity is 71.98550552012476
At time: 329.63047909736633 and batch: 200, loss is 4.299824876785278 and perplexity is 73.68688828485064
At time: 330.5867328643799 and batch: 250, loss is 4.320900993347168 and perplexity is 75.25640327371578
At time: 331.544180393219 and batch: 300, loss is 4.304076948165894 and perplexity is 74.00087727236757
At time: 332.5069508552551 and batch: 350, loss is 4.372051477432251 and perplexity is 79.205954369878
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.950019573343211 and perplexity of 141.17772721454187
Finished 44 epochs...
Completing Train Step...
At time: 334.37892603874207 and batch: 50, loss is 4.3716325759887695 and perplexity is 79.17278182975768
At time: 335.33748173713684 and batch: 100, loss is 4.329085903167725 and perplexity is 75.87489785623798
At time: 336.29251074790955 and batch: 150, loss is 4.276181201934815 and perplexity is 71.96509443398728
At time: 337.24399733543396 and batch: 200, loss is 4.29968843460083 and perplexity is 73.6768349707115
At time: 338.2021872997284 and batch: 250, loss is 4.320940513610839 and perplexity is 75.25937748538655
At time: 339.1738443374634 and batch: 300, loss is 4.303886222839355 and perplexity is 73.98676477673362
At time: 340.145934343338 and batch: 350, loss is 4.37202564239502 and perplexity is 79.20390810753065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.949853305158944 and perplexity of 141.15425580150767
Finished 45 epochs...
Completing Train Step...
At time: 342.0359399318695 and batch: 50, loss is 4.371424407958984 and perplexity is 79.15630230306637
At time: 342.99838185310364 and batch: 100, loss is 4.3289957618713375 and perplexity is 75.86805870283169
At time: 343.9599230289459 and batch: 150, loss is 4.276093063354492 and perplexity is 71.95875181224999
At time: 344.9226224422455 and batch: 200, loss is 4.2996644687652585 and perplexity is 73.67506926495759
At time: 345.88854479789734 and batch: 250, loss is 4.321038837432861 and perplexity is 75.26677763882363
At time: 346.8667676448822 and batch: 300, loss is 4.303733863830566 and perplexity is 73.97549308528242
At time: 347.84082984924316 and batch: 350, loss is 4.37194429397583 and perplexity is 79.1974652568739
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.949772801892511 and perplexity of 141.14289288022695
Finished 46 epochs...
Completing Train Step...
At time: 349.69312286376953 and batch: 50, loss is 4.371259775161743 and perplexity is 79.14327165226463
At time: 350.6565365791321 and batch: 100, loss is 4.3289293193817135 and perplexity is 75.86301800758856
At time: 351.6153566837311 and batch: 150, loss is 4.276034803390503 and perplexity is 71.95455962008037
At time: 352.574125289917 and batch: 200, loss is 4.2996636009216305 and perplexity is 73.67500532654591
At time: 353.53056359291077 and batch: 250, loss is 4.32113790512085 and perplexity is 75.2742345138282
At time: 354.489951133728 and batch: 300, loss is 4.303591375350952 and perplexity is 73.9649531806693
At time: 355.44267892837524 and batch: 350, loss is 4.371841201782226 and perplexity is 79.18930103729357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.949725973195043 and perplexity of 141.13628349715208
Finished 47 epochs...
Completing Train Step...
At time: 357.2908093929291 and batch: 50, loss is 4.371114463806152 and perplexity is 79.13177207170499
At time: 358.25037002563477 and batch: 100, loss is 4.328868494033814 and perplexity is 75.85840375345875
At time: 359.2092480659485 and batch: 150, loss is 4.275989789962768 and perplexity is 71.95132077160713
At time: 360.1725072860718 and batch: 200, loss is 4.299669380187988 and perplexity is 73.67543111525599
At time: 361.13698267936707 and batch: 250, loss is 4.321225852966308 and perplexity is 75.28085501169726
At time: 362.10326409339905 and batch: 300, loss is 4.303456401824951 and perplexity is 73.95497054384899
At time: 363.0689389705658 and batch: 350, loss is 4.371729135513306 and perplexity is 79.18042708503253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.949697034112338 and perplexity of 141.13219920166952
Finished 48 epochs...
Completing Train Step...
At time: 364.967928647995 and batch: 50, loss is 4.370978832244873 and perplexity is 79.12104003373011
At time: 365.9337360858917 and batch: 100, loss is 4.32880784034729 and perplexity is 75.85380280115105
At time: 366.9001820087433 and batch: 150, loss is 4.275948371887207 and perplexity is 71.94834074808053
At time: 367.8634045124054 and batch: 200, loss is 4.299678230285645 and perplexity is 73.67608315290154
At time: 368.82835245132446 and batch: 250, loss is 4.32130615234375 and perplexity is 75.28690026019956
At time: 369.7919502258301 and batch: 300, loss is 4.303326606750488 and perplexity is 73.94537217586429
At time: 370.7709538936615 and batch: 350, loss is 4.371612386703491 and perplexity is 79.17118340401464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.9496786183324355 and perplexity of 141.12960016608355
Finished 49 epochs...
Completing Train Step...
At time: 372.63415908813477 and batch: 50, loss is 4.370852718353271 and perplexity is 79.11106240063617
At time: 373.611364364624 and batch: 100, loss is 4.328755187988281 and perplexity is 75.84980902463559
At time: 374.57115602493286 and batch: 150, loss is 4.275914077758789 and perplexity is 71.94587338475174
At time: 375.5314881801605 and batch: 200, loss is 4.2996899700164795 and perplexity is 73.67694809536378
At time: 376.4981586933136 and batch: 250, loss is 4.321382055282593 and perplexity is 75.2926149740647
At time: 377.4586660861969 and batch: 300, loss is 4.303196258544922 and perplexity is 73.93573415745409
At time: 378.41266322135925 and batch: 350, loss is 4.371496028900147 and perplexity is 79.16197175495958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.949667568864493 and perplexity of 141.12804076770607
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efbe69b8e48>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'wordvec_dim': 200, 'seq_len': 35, 'lr': 4.594217473226321, 'anneal': 4.968222421660596, 'tune_wordvecs': True, 'wordvec_source': '', 'batch_size': 80, 'num_layers': 1, 'dropout': 0.7989086460771527}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.173886775970459 and batch: 50, loss is 6.9643493747711185 and perplexity is 1058.2261847433706
At time: 2.1328909397125244 and batch: 100, loss is 6.140250453948974 and perplexity is 464.16980946423485
At time: 3.0781893730163574 and batch: 150, loss is 6.010783739089966 and perplexity is 407.80280606090565
At time: 4.022421836853027 and batch: 200, loss is 5.910392761230469 and perplexity is 368.85099733492143
At time: 4.969132900238037 and batch: 250, loss is 5.890086374282837 and perplexity is 361.43650184271104
At time: 5.915787696838379 and batch: 300, loss is 5.857406663894653 and perplexity is 349.81577679406536
At time: 6.8675453662872314 and batch: 350, loss is 5.863389711380005 and perplexity is 351.9150148560803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.70452828242861 and perplexity of 300.2238257868536
Finished 1 epochs...
Completing Train Step...
At time: 8.697072982788086 and batch: 50, loss is 5.3911049175262455 and perplexity is 219.44572107219167
At time: 9.661386489868164 and batch: 100, loss is 5.125499725341797 and perplexity is 168.25820353444095
At time: 10.602232217788696 and batch: 150, loss is 4.991450643539428 and perplexity is 147.14973054798784
At time: 11.540894031524658 and batch: 200, loss is 4.907006196975708 and perplexity is 135.2339439715538
At time: 12.482710123062134 and batch: 250, loss is 4.858232707977295 and perplexity is 128.79638003761795
At time: 13.422421932220459 and batch: 300, loss is 4.802716054916382 and perplexity is 121.84089508007106
At time: 14.361977338790894 and batch: 350, loss is 4.819221639633179 and perplexity is 123.86863880835706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.904062205347522 and perplexity of 134.83640184013598
Finished 2 epochs...
Completing Train Step...
At time: 16.179800987243652 and batch: 50, loss is 4.678216896057129 and perplexity is 107.57807855358976
At time: 17.11767840385437 and batch: 100, loss is 4.577200002670288 and perplexity is 97.24173606283996
At time: 18.056204795837402 and batch: 150, loss is 4.499036912918091 and perplexity is 89.93047869791464
At time: 18.994675874710083 and batch: 200, loss is 4.521991071701049 and perplexity is 92.01863139430003
At time: 19.93488311767578 and batch: 250, loss is 4.519837265014648 and perplexity is 91.82065432942525
At time: 20.876177310943604 and batch: 300, loss is 4.472066144943238 and perplexity is 87.5374012543944
At time: 21.817823886871338 and batch: 350, loss is 4.5306657028198245 and perplexity is 92.82033127580185
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.740504823882004 and perplexity of 114.4919853821541
Finished 3 epochs...
Completing Train Step...
At time: 23.627509355545044 and batch: 50, loss is 4.425739011764526 and perplexity is 83.57454697946058
At time: 24.56443977355957 and batch: 100, loss is 4.350203275680542 and perplexity is 77.49421401339168
At time: 25.501861333847046 and batch: 150, loss is 4.272425718307495 and perplexity is 71.69533755092725
At time: 26.439903497695923 and batch: 200, loss is 4.329309568405152 and perplexity is 75.89187033128732
At time: 27.378248691558838 and batch: 250, loss is 4.332620334625244 and perplexity is 76.14354696331644
At time: 28.319536209106445 and batch: 300, loss is 4.289563293457031 and perplexity is 72.9346105222216
At time: 29.26280975341797 and batch: 350, loss is 4.359604187011719 and perplexity is 78.22616536223653
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.673711316338901 and perplexity of 107.09446723752025
Finished 4 epochs...
Completing Train Step...
At time: 31.054306268692017 and batch: 50, loss is 4.262859201431274 and perplexity is 71.01273317333721
At time: 32.00617003440857 and batch: 100, loss is 4.200426893234253 and perplexity is 66.71480506171466
At time: 32.94272208213806 and batch: 150, loss is 4.121880135536194 and perplexity is 61.67509085479062
At time: 33.881266593933105 and batch: 200, loss is 4.192184362411499 and perplexity is 66.16716628990105
At time: 34.83632731437683 and batch: 250, loss is 4.1994435548782345 and perplexity is 66.64923407949779
At time: 35.79274868965149 and batch: 300, loss is 4.16103162765503 and perplexity is 64.13765466051676
At time: 36.74291133880615 and batch: 350, loss is 4.234924192428589 and perplexity is 69.05644353381221
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.638517708613954 and perplexity of 103.39097838578654
Finished 5 epochs...
Completing Train Step...
At time: 38.54641389846802 and batch: 50, loss is 4.144797463417053 and perplexity is 63.10483956877002
At time: 39.48573899269104 and batch: 100, loss is 4.087392868995667 and perplexity is 59.58434483392785
At time: 40.4402539730072 and batch: 150, loss is 4.010093731880188 and perplexity is 55.152039825586755
At time: 41.390570878982544 and batch: 200, loss is 4.086158628463745 and perplexity is 59.510848785696666
At time: 42.33600115776062 and batch: 250, loss is 4.09554096698761 and perplexity is 60.0718272445917
At time: 43.27942228317261 and batch: 300, loss is 4.0580206394195555 and perplexity is 57.85967250589567
At time: 44.22102165222168 and batch: 350, loss is 4.133549456596374 and perplexity is 62.39901291599718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.622489666116649 and perplexity of 101.7470331957874
Finished 6 epochs...
Completing Train Step...
At time: 46.04697251319885 and batch: 50, loss is 4.047579517364502 and perplexity is 57.25869550004879
At time: 46.99471139907837 and batch: 100, loss is 3.9945671558380127 and perplexity is 54.302331089203264
At time: 47.94251561164856 and batch: 150, loss is 3.915941376686096 and perplexity is 50.19630290556551
At time: 48.887853145599365 and batch: 200, loss is 3.99749240398407 and perplexity is 54.46141144386627
At time: 49.83947730064392 and batch: 250, loss is 4.007948908805847 and perplexity is 55.03387522435258
At time: 50.78998136520386 and batch: 300, loss is 3.971779284477234 and perplexity is 53.07888931332269
At time: 51.73623251914978 and batch: 350, loss is 4.048433871269226 and perplexity is 57.30763559323162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.615190703293373 and perplexity of 101.00708908043926
Finished 7 epochs...
Completing Train Step...
At time: 53.55628848075867 and batch: 50, loss is 3.96477689743042 and perplexity is 52.70850867404924
At time: 54.509340047836304 and batch: 100, loss is 3.915740523338318 and perplexity is 50.186221822524374
At time: 55.45360565185547 and batch: 150, loss is 3.8380743789672853 and perplexity is 46.435970210920665
At time: 56.39446997642517 and batch: 200, loss is 3.9218595361709596 and perplexity is 50.49425342141394
At time: 57.334423303604126 and batch: 250, loss is 3.9317844581604002 and perplexity is 50.99790014047049
At time: 58.27897620201111 and batch: 300, loss is 3.8993192434310915 and perplexity is 49.36882950846055
At time: 59.2238187789917 and batch: 350, loss is 3.9751214504241945 and perplexity is 53.25658454776321
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.615260683257004 and perplexity of 101.01415780019107
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 61.048237562179565 and batch: 50, loss is 3.896318063735962 and perplexity is 49.220886891999946
At time: 62.01302909851074 and batch: 100, loss is 3.83137403011322 and perplexity is 46.125873050847815
At time: 62.95740365982056 and batch: 150, loss is 3.7291034984588625 and perplexity is 41.641759523385424
At time: 63.913053035736084 and batch: 200, loss is 3.785800700187683 and perplexity is 44.07094404829136
At time: 64.85451674461365 and batch: 250, loss is 3.767323784828186 and perplexity is 43.264125656476665
At time: 65.7970130443573 and batch: 300, loss is 3.6921826124191286 and perplexity is 40.132344789770556
At time: 66.7391893863678 and batch: 350, loss is 3.7281264591217043 and perplexity is 41.601093755525014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.554359699117726 and perplexity of 95.04587778831196
Finished 9 epochs...
Completing Train Step...
At time: 68.57011342048645 and batch: 50, loss is 3.8191487264633177 and perplexity is 45.56540318438013
At time: 69.51460909843445 and batch: 100, loss is 3.7613807344436645 and perplexity is 43.007767307693975
At time: 70.46333003044128 and batch: 150, loss is 3.668470311164856 and perplexity is 39.19190855845246
At time: 71.40982460975647 and batch: 200, loss is 3.7349753522872926 and perplexity is 41.88699313180635
At time: 72.35416293144226 and batch: 250, loss is 3.728929166793823 and perplexity is 41.63450067885415
At time: 73.2977888584137 and batch: 300, loss is 3.667102560997009 and perplexity is 39.13834046118143
At time: 74.24283695220947 and batch: 350, loss is 3.723602924346924 and perplexity is 41.413334748617544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.553517308728448 and perplexity of 94.96584576815762
Finished 10 epochs...
Completing Train Step...
At time: 76.07370686531067 and batch: 50, loss is 3.7837534093856813 and perplexity is 43.98081030637835
At time: 77.0219190120697 and batch: 100, loss is 3.7277224349975584 and perplexity is 41.58428930498722
At time: 77.9691436290741 and batch: 150, loss is 3.6373545026779173 and perplexity is 37.99119805171424
At time: 78.91621446609497 and batch: 200, loss is 3.7071632957458496 and perplexity is 40.738080577965704
At time: 79.86219620704651 and batch: 250, loss is 3.7063404655456544 and perplexity is 40.704573842035764
At time: 80.80290722846985 and batch: 300, loss is 3.6502496910095217 and perplexity is 38.48427402670182
At time: 81.74526190757751 and batch: 350, loss is 3.7151471376419067 and perplexity is 41.064628792546806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.555433602168642 and perplexity of 95.14800267274349
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 83.56462097167969 and batch: 50, loss is 3.7650560092926026 and perplexity is 43.16612349619575
At time: 84.5221004486084 and batch: 100, loss is 3.7097757482528686 and perplexity is 40.844646016668236
At time: 85.46436190605164 and batch: 150, loss is 3.612157678604126 and perplexity is 37.04589978713412
At time: 86.40843057632446 and batch: 200, loss is 3.674340605735779 and perplexity is 39.42265321340664
At time: 87.35391926765442 and batch: 250, loss is 3.663549189567566 and perplexity is 38.99951419710458
At time: 88.31428384780884 and batch: 300, loss is 3.5942225790023805 and perplexity is 36.38740065973673
At time: 89.26277780532837 and batch: 350, loss is 3.646577172279358 and perplexity is 38.343199018388354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.546706627155173 and perplexity of 94.3212611541771
Finished 12 epochs...
Completing Train Step...
At time: 91.07943296432495 and batch: 50, loss is 3.7487266635894776 and perplexity is 42.46697281504599
At time: 92.02426314353943 and batch: 100, loss is 3.6925975275039673 and perplexity is 40.14899975997403
At time: 92.96728515625 and batch: 150, loss is 3.596927351951599 and perplexity is 36.48595353823074
At time: 93.90866041183472 and batch: 200, loss is 3.6623351430892943 and perplexity is 38.952195703476484
At time: 94.8534824848175 and batch: 250, loss is 3.6560542154312134 and perplexity is 38.70830650712524
At time: 95.79881286621094 and batch: 300, loss is 3.591821355819702 and perplexity is 36.30013120836209
At time: 96.745436668396 and batch: 350, loss is 3.649983687400818 and perplexity is 38.47403843234524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.5465808736866915 and perplexity of 94.30940067419931
Finished 13 epochs...
Completing Train Step...
At time: 98.57587575912476 and batch: 50, loss is 3.7396629285812377 and perplexity is 42.083802526982915
At time: 99.52061009407043 and batch: 100, loss is 3.6829977512359617 and perplexity is 39.765422418953854
At time: 100.4672954082489 and batch: 150, loss is 3.58831533908844 and perplexity is 36.173085183881376
At time: 101.41259145736694 and batch: 200, loss is 3.6553938245773314 and perplexity is 38.68275233433839
At time: 102.36296534538269 and batch: 250, loss is 3.651424136161804 and perplexity is 38.529498247256655
At time: 103.31355285644531 and batch: 300, loss is 3.5898874759674073 and perplexity is 36.229998951519086
At time: 104.26021456718445 and batch: 350, loss is 3.6508547830581666 and perplexity is 38.507567601581
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.547077047413793 and perplexity of 94.35620613189147
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 106.07094979286194 and batch: 50, loss is 3.7346427154541018 and perplexity is 41.87306229214275
At time: 107.02520680427551 and batch: 100, loss is 3.6782256746292115 and perplexity is 39.576110840808965
At time: 107.96876549720764 and batch: 150, loss is 3.5816078090667727 and perplexity is 35.931265043602195
At time: 108.91279363632202 and batch: 200, loss is 3.6471317911148073 and perplexity is 38.36447077708908
At time: 109.8582034111023 and batch: 250, loss is 3.6402652502059936 and perplexity is 38.10194193308206
At time: 110.80348706245422 and batch: 300, loss is 3.574973840713501 and perplexity is 35.693687082056435
At time: 111.74326515197754 and batch: 350, loss is 3.6328669023513793 and perplexity is 37.82109071128629
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.546333049905711 and perplexity of 94.28603145778565
Finished 15 epochs...
Completing Train Step...
At time: 113.53730273246765 and batch: 50, loss is 3.731381278038025 and perplexity is 41.73671837948373
At time: 114.49055171012878 and batch: 100, loss is 3.675133996009827 and perplexity is 39.45394317397718
At time: 115.43447685241699 and batch: 150, loss is 3.57886613368988 and perplexity is 35.832888099417495
At time: 116.37973237037659 and batch: 200, loss is 3.644937834739685 and perplexity is 38.280393066988815
At time: 117.32313537597656 and batch: 250, loss is 3.6389814376831056 and perplexity is 38.05305756877572
At time: 118.26821494102478 and batch: 300, loss is 3.5750181436538697 and perplexity is 35.69526845237619
At time: 119.20474290847778 and batch: 350, loss is 3.634018211364746 and perplexity is 37.86465954970315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.546163624730603 and perplexity of 94.27005838355433
Finished 16 epochs...
Completing Train Step...
At time: 120.98572754859924 and batch: 50, loss is 3.7290584230422974 and perplexity is 41.63988254603149
At time: 121.92222237586975 and batch: 100, loss is 3.672758045196533 and perplexity is 39.360313819022274
At time: 122.86433053016663 and batch: 150, loss is 3.5767717361450195 and perplexity is 35.75791832222004
At time: 123.80740332603455 and batch: 200, loss is 3.6433080577850343 and perplexity is 38.21805537661716
At time: 124.75345349311829 and batch: 250, loss is 3.6380516338348388 and perplexity is 38.01769213341679
At time: 125.69602656364441 and batch: 300, loss is 3.5750057697296143 and perplexity is 35.694826764560794
At time: 126.63746547698975 and batch: 350, loss is 3.6347825860977174 and perplexity is 37.893613403122764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.546138368803879 and perplexity of 94.26767753593289
Finished 17 epochs...
Completing Train Step...
At time: 128.44931817054749 and batch: 50, loss is 3.7270562982559206 and perplexity is 41.55659770622594
At time: 129.39299154281616 and batch: 100, loss is 3.6707318305969237 and perplexity is 39.28064211975505
At time: 130.3407700061798 and batch: 150, loss is 3.5749855518341063 and perplexity is 35.694105097578415
At time: 131.28885531425476 and batch: 200, loss is 3.641935362815857 and perplexity is 38.16562963477809
At time: 132.23840069770813 and batch: 250, loss is 3.6372502040863037 and perplexity is 37.9872358298944
At time: 133.17928647994995 and batch: 300, loss is 3.57488956451416 and perplexity is 35.690679080521946
At time: 134.12094831466675 and batch: 350, loss is 3.6352614641189573 and perplexity is 37.91176416738105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.546197825464709 and perplexity of 94.27328254388928
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 135.92344117164612 and batch: 50, loss is 3.7257327127456663 and perplexity is 41.501630380649026
At time: 136.87875747680664 and batch: 100, loss is 3.669428701400757 and perplexity is 39.22948770580876
At time: 137.82013821601868 and batch: 150, loss is 3.573334393501282 and perplexity is 35.63521710858967
At time: 138.76693749427795 and batch: 200, loss is 3.6399826192855835 and perplexity is 38.09117466781682
At time: 139.7160542011261 and batch: 250, loss is 3.6347449970245362 and perplexity is 37.89218904408579
At time: 140.6638822555542 and batch: 300, loss is 3.5713550233840943 and perplexity is 35.56475158640633
At time: 141.60891795158386 and batch: 350, loss is 3.631207504272461 and perplexity is 37.75838250934176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.546187302161908 and perplexity of 94.27229048281086
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 143.41894364356995 and batch: 50, loss is 3.725334415435791 and perplexity is 41.48510368440078
At time: 144.3618621826172 and batch: 100, loss is 3.6690584325790407 and perplexity is 39.2149649384493
At time: 145.30848956108093 and batch: 150, loss is 3.5729177141189576 and perplexity is 35.62037174142988
At time: 146.25273489952087 and batch: 200, loss is 3.6395442390441897 and perplexity is 38.07447990906452
At time: 147.19168210029602 and batch: 250, loss is 3.6342190217971804 and perplexity is 37.872263931855386
At time: 148.12992191314697 and batch: 300, loss is 3.570642204284668 and perplexity is 35.53940938548467
At time: 149.0719072818756 and batch: 350, loss is 3.6303864669799806 and perplexity is 37.727394192220615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.546182040510507 and perplexity of 94.27179445618654
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 150.89242720603943 and batch: 50, loss is 3.725251989364624 and perplexity is 41.48168437121434
At time: 151.83981585502625 and batch: 100, loss is 3.668981184959412 and perplexity is 39.21193579275263
At time: 152.78206157684326 and batch: 150, loss is 3.5728310441970823 and perplexity is 35.617284660374345
At time: 153.72349643707275 and batch: 200, loss is 3.6394531679153443 and perplexity is 38.071012581088134
At time: 154.67160940170288 and batch: 250, loss is 3.634112229347229 and perplexity is 37.86821967595675
At time: 155.62291169166565 and batch: 300, loss is 3.570498824119568 and perplexity is 35.53431410438926
At time: 156.57658672332764 and batch: 350, loss is 3.630220856666565 and perplexity is 37.72114666398591
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.546180988180226 and perplexity of 94.27169525117482
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 158.37941527366638 and batch: 50, loss is 3.7252354860305785 and perplexity is 41.48099979076934
At time: 159.33483266830444 and batch: 100, loss is 3.668965754508972 and perplexity is 39.21133073958887
At time: 160.27760004997253 and batch: 150, loss is 3.5728137731552123 and perplexity is 35.616669518071774
At time: 161.23322653770447 and batch: 200, loss is 3.6394349193572997 and perplexity is 38.0703178463442
At time: 162.17585921287537 and batch: 250, loss is 3.634091029167175 and perplexity is 37.86741687139112
At time: 163.11652445793152 and batch: 300, loss is 3.5704700326919556 and perplexity is 35.53329103548485
At time: 164.0615622997284 and batch: 350, loss is 3.6301874494552613 and perplexity is 37.71988652671765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.546180988180226 and perplexity of 94.27169525117482
Annealing...
Model not improving. Stopping early with 94.26767753593289loss at 21 epochs.
Finished Training.
Improved accuracyfrom -95.36351970280477 to -94.26767753593289
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efbe158d358>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -144.11446636370945, 'params': {'data': 'ptb', 'wordvec_dim': 200, 'seq_len': 35, 'lr': 20.72877720884464, 'anneal': 5.948948781071943, 'tune_wordvecs': True, 'wordvec_source': '', 'batch_size': 80, 'num_layers': 1, 'dropout': 0.6665245902185484}}, {'best_accuracy': -95.36351970280477, 'params': {'data': 'ptb', 'wordvec_dim': 200, 'seq_len': 35, 'lr': 3.0211221666783503, 'anneal': 4.808244963475975, 'tune_wordvecs': True, 'wordvec_source': '', 'batch_size': 80, 'num_layers': 1, 'dropout': 0.9129203945657647}}, {'best_accuracy': -110.16491674212764, 'params': {'data': 'ptb', 'wordvec_dim': 200, 'seq_len': 35, 'lr': 14.095412846842905, 'anneal': 5.930177463664579, 'tune_wordvecs': True, 'wordvec_source': '', 'batch_size': 80, 'num_layers': 1, 'dropout': 0.11815698249148554}}, {'best_accuracy': -151.0489425418011, 'params': {'data': 'ptb', 'wordvec_dim': 200, 'seq_len': 35, 'lr': 25.162112630757676, 'anneal': 7.9373867945790515, 'tune_wordvecs': True, 'wordvec_source': '', 'batch_size': 80, 'num_layers': 1, 'dropout': 0.4782148580572463}}, {'best_accuracy': -141.12804076770607, 'params': {'data': 'ptb', 'wordvec_dim': 200, 'seq_len': 35, 'lr': 24.308474250631857, 'anneal': 7.2762522963817196, 'tune_wordvecs': True, 'wordvec_source': '', 'batch_size': 80, 'num_layers': 1, 'dropout': 0.13888425829454654}}, {'best_accuracy': -94.26767753593289, 'params': {'data': 'ptb', 'wordvec_dim': 200, 'seq_len': 35, 'lr': 4.594217473226321, 'anneal': 4.968222421660596, 'tune_wordvecs': True, 'wordvec_source': '', 'batch_size': 80, 'num_layers': 1, 'dropout': 0.7989086460771527}}]
