Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 30], 'type': 'continuous', 'name': 'lr'}, {'domain': [0, 1], 'type': 'continuous', 'name': 'dropout'}, {'domain': [2, 8], 'type': 'continuous', 'name': 'anneal'}]
SETTINGS FOR THIS RUN
{'batch_size': 80, 'tune_wordvecs': True, 'data': 'ptb', 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.4096518539347076, 'anneal': 4.729367290024887, 'wordvec_dim': 200, 'lr': 28.12829039403859, 'seq_len': 20}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.1085772514343262 and batch: 50, loss is 6.358228406906128 and perplexity is 577.2228460318211
At time: 1.7083463668823242 and batch: 100, loss is 5.745719547271729 and perplexity is 312.84865625812415
At time: 2.2950618267059326 and batch: 150, loss is 5.658395366668701 and perplexity is 286.68824375758857
At time: 2.8776137828826904 and batch: 200, loss is 5.61960657119751 and perplexity is 275.7808617542544
At time: 3.467679500579834 and batch: 250, loss is 5.592561979293823 and perplexity is 268.4224322516515
At time: 4.0503785610198975 and batch: 300, loss is 5.605272979736328 and perplexity is 271.85612651988845
At time: 4.633620977401733 and batch: 350, loss is 5.530212659835815 and perplexity is 252.19753760814098
At time: 5.22548770904541 and batch: 400, loss is 5.5622689914703365 and perplexity is 260.41304145435686
At time: 5.811979532241821 and batch: 450, loss is 5.523982019424438 and perplexity is 250.63107055277163
At time: 6.399166822433472 and batch: 500, loss is 5.534592580795288 and perplexity is 253.30456546655734
At time: 6.993045806884766 and batch: 550, loss is 5.541199531555176 and perplexity is 254.983677053622
At time: 7.589982986450195 and batch: 600, loss is 5.564064731597901 and perplexity is 260.88109572878153
At time: 8.170166254043579 and batch: 650, loss is 5.5281564903259275 and perplexity is 251.67950948017477
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.40095729453891 and perplexity of 221.61846883951984
Finished 1 epochs...
Completing Train Step...
At time: 9.27854061126709 and batch: 50, loss is 5.371200370788574 and perplexity is 215.12093767311538
At time: 9.862003087997437 and batch: 100, loss is 5.348848686218262 and perplexity is 210.36596116035565
At time: 10.452767372131348 and batch: 150, loss is 5.3124782085418705 and perplexity is 202.85231633494263
At time: 11.02820086479187 and batch: 200, loss is 5.273728342056274 and perplexity is 195.14216457834954
At time: 11.612616777420044 and batch: 250, loss is 5.261816158294677 and perplexity is 192.8313857899544
At time: 12.19479775428772 and batch: 300, loss is 5.305641279220581 and perplexity is 201.47015962328626
At time: 12.771854877471924 and batch: 350, loss is 5.221188335418701 and perplexity is 185.1540785400596
At time: 13.357044458389282 and batch: 400, loss is 5.277693309783936 and perplexity is 195.91743290449153
At time: 13.941954851150513 and batch: 450, loss is 5.213420696258545 and perplexity is 183.72143978271262
At time: 14.520410537719727 and batch: 500, loss is 5.239132709503174 and perplexity is 188.50654155645188
At time: 15.097126245498657 and batch: 550, loss is 5.251627235412598 and perplexity is 190.87661707576166
At time: 15.674543857574463 and batch: 600, loss is 5.305054922103881 and perplexity is 201.3520607888182
At time: 16.250115156173706 and batch: 650, loss is 5.25894323348999 and perplexity is 192.27819074297437
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.440062878178615 and perplexity of 230.45667370096942
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 17.368024587631226 and batch: 50, loss is 5.193733415603638 and perplexity is 180.1398359372168
At time: 17.95028805732727 and batch: 100, loss is 5.099871063232422 and perplexity is 164.00076020871063
At time: 18.525243282318115 and batch: 150, loss is 5.040365352630615 and perplexity is 154.5264613630127
At time: 19.111563444137573 and batch: 200, loss is 5.007504386901855 and perplexity is 149.53109834765544
At time: 19.69378161430359 and batch: 250, loss is 4.994139394760132 and perplexity is 147.5459119435515
At time: 20.273253679275513 and batch: 300, loss is 5.04504955291748 and perplexity is 155.2519921969113
At time: 20.846487045288086 and batch: 350, loss is 4.982967901229858 and perplexity is 145.90677658516356
At time: 21.45639443397522 and batch: 400, loss is 5.01848949432373 and perplexity is 151.1827688053719
At time: 22.036410808563232 and batch: 450, loss is 4.99975468635559 and perplexity is 148.3767557949354
At time: 22.619816780090332 and batch: 500, loss is 4.948647565841675 and perplexity is 140.98416312980567
At time: 23.201045513153076 and batch: 550, loss is 4.9673933506011965 and perplexity is 143.6519486507346
At time: 23.787028312683105 and batch: 600, loss is 5.010884275436402 and perplexity is 150.03735185078585
At time: 24.3640718460083 and batch: 650, loss is 4.97791600227356 and perplexity is 145.17152905466094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.179022695503983 and perplexity of 177.50924561024505
Finished 3 epochs...
Completing Train Step...
At time: 25.467076063156128 and batch: 50, loss is 5.006456136703491 and perplexity is 149.3744344699669
At time: 26.068259954452515 and batch: 100, loss is 4.963751468658447 and perplexity is 143.12973670721433
At time: 26.653392553329468 and batch: 150, loss is 4.930940761566162 and perplexity is 138.5097557210921
At time: 27.249617338180542 and batch: 200, loss is 4.908053588867188 and perplexity is 135.3756611116585
At time: 27.832510232925415 and batch: 250, loss is 4.896017551422119 and perplexity is 133.75604104157568
At time: 28.415946006774902 and batch: 300, loss is 4.954043817520142 and perplexity is 141.74700555044484
At time: 28.987183332443237 and batch: 350, loss is 4.903545122146607 and perplexity is 134.76669822471
At time: 29.573609828948975 and batch: 400, loss is 4.9505500316619875 and perplexity is 141.25263598060127
At time: 30.154558420181274 and batch: 450, loss is 4.927069473266601 and perplexity is 137.9745811004004
At time: 30.729857921600342 and batch: 500, loss is 4.8713255214691165 and perplexity is 130.49377460767434
At time: 31.306424140930176 and batch: 550, loss is 4.895485324859619 and perplexity is 133.68487146448817
At time: 31.892574310302734 and batch: 600, loss is 4.944629182815552 and perplexity is 140.41877150259194
At time: 32.475350856781006 and batch: 650, loss is 4.906419887542724 and perplexity is 135.15467827392752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.144968070235907 and perplexity of 171.56600652149007
Finished 4 epochs...
Completing Train Step...
At time: 33.6143844127655 and batch: 50, loss is 4.92959246635437 and perplexity is 138.32312952254412
At time: 34.196692943573 and batch: 100, loss is 4.892110681533813 and perplexity is 133.23449306588495
At time: 34.771666288375854 and batch: 150, loss is 4.865120468139648 and perplexity is 129.68656076951726
At time: 35.346062660217285 and batch: 200, loss is 4.843398151397705 and perplexity is 126.89984476045228
At time: 35.94136333465576 and batch: 250, loss is 4.82935227394104 and perplexity is 125.12988451308114
At time: 36.52630376815796 and batch: 300, loss is 4.88710729598999 and perplexity is 132.56953443873826
At time: 37.10235524177551 and batch: 350, loss is 4.838019647598267 and perplexity is 126.21914567552237
At time: 37.69095206260681 and batch: 400, loss is 4.878786602020264 and perplexity is 131.47104036721225
At time: 38.27905464172363 and batch: 450, loss is 4.862399778366089 and perplexity is 129.33420341473968
At time: 38.85878562927246 and batch: 500, loss is 4.808722047805786 and perplexity is 122.57487255497635
At time: 39.43082857131958 and batch: 550, loss is 4.831951456069946 and perplexity is 125.45554291210632
At time: 40.01210570335388 and batch: 600, loss is 4.898746271133422 and perplexity is 134.1215222083253
At time: 40.59597706794739 and batch: 650, loss is 4.851131782531739 and perplexity is 127.88504603208509
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.125575046913297 and perplexity of 168.27087748405384
Finished 5 epochs...
Completing Train Step...
At time: 41.71325087547302 and batch: 50, loss is 4.8688156795501705 and perplexity is 130.1666665284999
At time: 42.31910967826843 and batch: 100, loss is 4.8398322486877445 and perplexity is 126.44813810975803
At time: 42.90226697921753 and batch: 150, loss is 4.811213207244873 and perplexity is 122.88060676395446
At time: 43.48430013656616 and batch: 200, loss is 4.792258090972901 and perplexity is 120.57332703563529
At time: 44.07273006439209 and batch: 250, loss is 4.781015701293946 and perplexity is 119.22538594502359
At time: 44.66300916671753 and batch: 300, loss is 4.843076477050781 and perplexity is 126.85903090050205
At time: 45.250783920288086 and batch: 350, loss is 4.8018933773040775 and perplexity is 121.74070052298602
At time: 45.832876682281494 and batch: 400, loss is 4.8460641288757325 and perplexity is 127.23860825582842
At time: 46.413609743118286 and batch: 450, loss is 4.826646881103516 and perplexity is 124.79181652930771
At time: 46.995349407196045 and batch: 500, loss is 4.782666339874267 and perplexity is 119.42234647742497
At time: 47.58723282814026 and batch: 550, loss is 4.80911527633667 and perplexity is 122.62308197007194
At time: 48.16743183135986 and batch: 600, loss is 4.874347505569458 and perplexity is 130.88872118029664
At time: 48.74670433998108 and batch: 650, loss is 4.827392568588257 and perplexity is 124.88490692895115
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.1228799258961395 and perplexity of 167.81797768959154
Finished 6 epochs...
Completing Train Step...
At time: 49.8449432849884 and batch: 50, loss is 4.828819980621338 and perplexity is 125.06329643522653
At time: 50.422487020492554 and batch: 100, loss is 4.804467058181762 and perplexity is 122.05442577712974
At time: 51.01705026626587 and batch: 150, loss is 4.779111261367798 and perplexity is 118.99854443139805
At time: 51.590614795684814 and batch: 200, loss is 4.763323726654053 and perplexity is 117.13460301508303
At time: 52.16502547264099 and batch: 250, loss is 4.747642087936401 and perplexity is 115.31206800576973
At time: 52.7474844455719 and batch: 300, loss is 4.812426376342773 and perplexity is 123.02977218195461
At time: 53.33258271217346 and batch: 350, loss is 4.766323690414429 and perplexity is 117.48653019967531
At time: 53.90419054031372 and batch: 400, loss is 4.816151237487793 and perplexity is 123.48889555459591
At time: 54.481512784957886 and batch: 450, loss is 4.796789093017578 and perplexity is 121.12088458237078
At time: 55.060182332992554 and batch: 500, loss is 4.74786153793335 and perplexity is 115.3373760155615
At time: 55.64244508743286 and batch: 550, loss is 4.770811519622803 and perplexity is 118.01497457862294
At time: 56.241337060928345 and batch: 600, loss is 4.840313472747803 and perplexity is 126.50900263972956
At time: 56.814976930618286 and batch: 650, loss is 4.789788293838501 and perplexity is 120.27590281796597
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.1099853515625 and perplexity of 165.66792807966513
Finished 7 epochs...
Completing Train Step...
At time: 57.91223502159119 and batch: 50, loss is 4.79147367477417 and perplexity is 120.47878444998922
At time: 58.509533166885376 and batch: 100, loss is 4.7755812072753905 and perplexity is 118.5792136981727
At time: 59.087589740753174 and batch: 150, loss is 4.752609605789185 and perplexity is 115.88630785474132
At time: 59.675838470458984 and batch: 200, loss is 4.740425310134888 and perplexity is 114.4828820573061
At time: 60.25888752937317 and batch: 250, loss is 4.718210344314575 and perplexity is 111.96768963961134
At time: 60.844478130340576 and batch: 300, loss is 4.780326356887818 and perplexity is 119.14322691334186
At time: 61.42205238342285 and batch: 350, loss is 4.738771257400512 and perplexity is 114.29367785320245
At time: 62.008339643478394 and batch: 400, loss is 4.786363563537598 and perplexity is 119.86469482926007
At time: 62.59126830101013 and batch: 450, loss is 4.761252565383911 and perplexity is 116.89224942524473
At time: 63.17126965522766 and batch: 500, loss is 4.714304399490357 and perplexity is 111.53120302303878
At time: 63.756123781204224 and batch: 550, loss is 4.733846492767334 and perplexity is 113.73219211808369
At time: 64.3426764011383 and batch: 600, loss is 4.80897349357605 and perplexity is 122.60569736344237
At time: 64.92335987091064 and batch: 650, loss is 4.763723545074463 and perplexity is 117.18144495054615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.108734130859375 and perplexity of 165.46077056511987
Finished 8 epochs...
Completing Train Step...
At time: 66.06260085105896 and batch: 50, loss is 4.765156183242798 and perplexity is 117.34944387330334
At time: 66.63493752479553 and batch: 100, loss is 4.742418823242187 and perplexity is 114.71133281738398
At time: 67.20874905586243 and batch: 150, loss is 4.725711984634399 and perplexity is 112.81078933631362
At time: 67.78443503379822 and batch: 200, loss is 4.71281831741333 and perplexity is 111.36558159519517
At time: 68.36900329589844 and batch: 250, loss is 4.690056562423706 and perplexity is 108.8593369812247
At time: 68.94768142700195 and batch: 300, loss is 4.747735090255738 and perplexity is 115.32279279424912
At time: 69.52780055999756 and batch: 350, loss is 4.708641300201416 and perplexity is 110.90137581675503
At time: 70.11094331741333 and batch: 400, loss is 4.764408988952637 and perplexity is 117.26179378878685
At time: 70.68999886512756 and batch: 450, loss is 4.742521333694458 and perplexity is 114.72309253072814
At time: 71.26913499832153 and batch: 500, loss is 4.697184801101685 and perplexity is 109.63808456943245
At time: 71.8469717502594 and batch: 550, loss is 4.7187477684021 and perplexity is 112.02787994545953
At time: 72.43581914901733 and batch: 600, loss is 4.785764989852905 and perplexity is 119.79296844611585
At time: 73.01258635520935 and batch: 650, loss is 4.733210973739624 and perplexity is 113.65993610839584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.088733149509804 and perplexity of 162.18426863270338
Finished 9 epochs...
Completing Train Step...
At time: 74.11448168754578 and batch: 50, loss is 4.732119493484497 and perplexity is 113.5359462109064
At time: 74.71450281143188 and batch: 100, loss is 4.7125214576721195 and perplexity is 111.33252654406279
At time: 75.29394459724426 and batch: 150, loss is 4.696571226119995 and perplexity is 109.57083401743824
At time: 75.8789587020874 and batch: 200, loss is 4.688716220855713 and perplexity is 108.71352602687546
At time: 76.46112418174744 and batch: 250, loss is 4.672472276687622 and perplexity is 106.96185511898267
At time: 77.04504990577698 and batch: 300, loss is 4.732444562911987 and perplexity is 113.57285927527009
At time: 77.62142324447632 and batch: 350, loss is 4.692176837921142 and perplexity is 109.09039363140917
At time: 78.2079689502716 and batch: 400, loss is 4.75025975227356 and perplexity is 115.61431170752671
At time: 78.78774333000183 and batch: 450, loss is 4.728445720672608 and perplexity is 113.11960617615945
At time: 79.37590312957764 and batch: 500, loss is 4.68568639755249 and perplexity is 108.38464173458192
At time: 79.94995093345642 and batch: 550, loss is 4.704733829498291 and perplexity is 110.46887747740368
At time: 80.56643009185791 and batch: 600, loss is 4.771409101486206 and perplexity is 118.08551926305384
At time: 81.14762043952942 and batch: 650, loss is 4.718525247573853 and perplexity is 112.0029541821809
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.091748405905331 and perplexity of 162.6740337986571
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 82.249516248703 and batch: 50, loss is 4.697481098175049 and perplexity is 109.67057482616596
At time: 82.8392608165741 and batch: 100, loss is 4.646391220092774 and perplexity is 104.2082415845118
At time: 83.42340993881226 and batch: 150, loss is 4.608622150421143 and perplexity is 100.34579293238302
At time: 83.99773788452148 and batch: 200, loss is 4.592478342056275 and perplexity is 98.73883578579509
At time: 84.57802534103394 and batch: 250, loss is 4.569377250671387 and perplexity is 96.48400571012688
At time: 85.1682276725769 and batch: 300, loss is 4.619383506774902 and perplexity is 101.4314810294957
At time: 85.74066233634949 and batch: 350, loss is 4.5593430614471435 and perplexity is 95.52070797803208
At time: 86.3180718421936 and batch: 400, loss is 4.608731508255005 and perplexity is 100.35676713098165
At time: 86.90631675720215 and batch: 450, loss is 4.576870365142822 and perplexity is 97.20968682000448
At time: 87.47793817520142 and batch: 500, loss is 4.529090881347656 and perplexity is 92.67427086477728
At time: 88.05846333503723 and batch: 550, loss is 4.538552398681641 and perplexity is 93.55527131320947
At time: 88.64108729362488 and batch: 600, loss is 4.622311868667603 and perplexity is 101.72894444095205
At time: 89.22289872169495 and batch: 650, loss is 4.588906764984131 and perplexity is 98.38681143902183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.005652334175858 and perplexity of 149.25441516434879
Finished 11 epochs...
Completing Train Step...
At time: 90.31500697135925 and batch: 50, loss is 4.63856541633606 and perplexity is 103.39591105151375
At time: 90.90062141418457 and batch: 100, loss is 4.604819126129151 and perplexity is 99.96490017553613
At time: 91.48138761520386 and batch: 150, loss is 4.577025117874146 and perplexity is 97.22473144861965
At time: 92.05436754226685 and batch: 200, loss is 4.567387485504151 and perplexity is 96.29221606782153
At time: 92.6378014087677 and batch: 250, loss is 4.548139638900757 and perplexity is 94.45652152096984
At time: 93.21045780181885 and batch: 300, loss is 4.599432582855225 and perplexity is 99.42788254766906
At time: 93.78573250770569 and batch: 350, loss is 4.5449289703369145 and perplexity is 94.15373926351022
At time: 94.36298274993896 and batch: 400, loss is 4.59732424736023 and perplexity is 99.21847604085623
At time: 94.93540573120117 and batch: 450, loss is 4.567660989761353 and perplexity is 96.3185560007292
At time: 95.54781031608582 and batch: 500, loss is 4.525674333572388 and perplexity is 92.35818505942426
At time: 96.13013052940369 and batch: 550, loss is 4.541683292388916 and perplexity is 93.84864193992397
At time: 96.71464085578918 and batch: 600, loss is 4.624985637664795 and perplexity is 102.00130809520121
At time: 97.28823494911194 and batch: 650, loss is 4.58134612083435 and perplexity is 97.64574875445666
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.002063826018689 and perplexity of 148.7197743332612
Finished 12 epochs...
Completing Train Step...
At time: 98.36242008209229 and batch: 50, loss is 4.619288139343261 and perplexity is 101.42180823090472
At time: 98.93563747406006 and batch: 100, loss is 4.587492971420288 and perplexity is 98.24781108030159
At time: 99.5108323097229 and batch: 150, loss is 4.562764310836792 and perplexity is 95.84806781240678
At time: 100.08908224105835 and batch: 200, loss is 4.554393339157104 and perplexity is 95.04907518916353
At time: 100.67958998680115 and batch: 250, loss is 4.537623472213745 and perplexity is 93.46840569760664
At time: 101.25689601898193 and batch: 300, loss is 4.5879886436462405 and perplexity is 98.29652186280836
At time: 101.82644867897034 and batch: 350, loss is 4.535921936035156 and perplexity is 93.30950105309662
At time: 102.40788722038269 and batch: 400, loss is 4.588968267440796 and perplexity is 98.39286265570924
At time: 102.97957158088684 and batch: 450, loss is 4.561582136154175 and perplexity is 95.73482560248625
At time: 103.55585956573486 and batch: 500, loss is 4.522268991470337 and perplexity is 92.04420874516842
At time: 104.12884426116943 and batch: 550, loss is 4.539064588546753 and perplexity is 93.60320164867215
At time: 104.70970320701599 and batch: 600, loss is 4.622417001724243 and perplexity is 101.73964007805263
At time: 105.28433632850647 and batch: 650, loss is 4.573474626541138 and perplexity is 96.88014796457867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.000515806908701 and perplexity of 148.48973138194145
Finished 13 epochs...
Completing Train Step...
At time: 106.35790705680847 and batch: 50, loss is 4.605932512283325 and perplexity is 100.07626169397747
At time: 106.9598069190979 and batch: 100, loss is 4.576190795898437 and perplexity is 97.1436485479161
At time: 107.53105020523071 and batch: 150, loss is 4.553379936218262 and perplexity is 94.95280096756146
At time: 108.10620999336243 and batch: 200, loss is 4.546312999725342 and perplexity is 94.28414102480043
At time: 108.68195343017578 and batch: 250, loss is 4.530174503326416 and perplexity is 92.7747491719708
At time: 109.26453495025635 and batch: 300, loss is 4.57974458694458 and perplexity is 97.48949093812752
At time: 109.83680725097656 and batch: 350, loss is 4.528313503265381 and perplexity is 92.60225591286544
At time: 110.43959856033325 and batch: 400, loss is 4.582147636413574 and perplexity is 97.72404471685354
At time: 111.01607489585876 and batch: 450, loss is 4.554335241317749 and perplexity is 95.04355320367158
At time: 111.59362435340881 and batch: 500, loss is 4.517227649688721 and perplexity is 91.58135012441791
At time: 112.18479037284851 and batch: 550, loss is 4.5355544090271 and perplexity is 93.27521359252118
At time: 112.7562484741211 and batch: 600, loss is 4.617701673507691 and perplexity is 101.2610335626446
At time: 113.32891988754272 and batch: 650, loss is 4.565787448883056 and perplexity is 96.13826818975171
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.000188790115655 and perplexity of 148.441180685064
Finished 14 epochs...
Completing Train Step...
At time: 114.43638682365417 and batch: 50, loss is 4.595275478363037 and perplexity is 99.01540839357568
At time: 115.01996374130249 and batch: 100, loss is 4.566506481170654 and perplexity is 96.20741956671094
At time: 115.59775161743164 and batch: 150, loss is 4.545517463684082 and perplexity is 94.20916441974597
At time: 116.17033791542053 and batch: 200, loss is 4.53900520324707 and perplexity is 93.59764315953886
At time: 116.74456858634949 and batch: 250, loss is 4.523301792144776 and perplexity is 92.13932117367419
At time: 117.32236528396606 and batch: 300, loss is 4.57172061920166 and perplexity is 96.71036841481505
At time: 117.902104139328 and batch: 350, loss is 4.521230516433715 and perplexity is 91.94867274658151
At time: 118.47938251495361 and batch: 400, loss is 4.575623359680176 and perplexity is 97.0885413597418
At time: 119.05894422531128 and batch: 450, loss is 4.547679634094238 and perplexity is 94.41308105923974
At time: 119.64021563529968 and batch: 500, loss is 4.512938404083252 and perplexity is 91.18937645746672
At time: 120.21294498443604 and batch: 550, loss is 4.5309974956512455 and perplexity is 92.8511335060263
At time: 120.78575301170349 and batch: 600, loss is 4.612126960754394 and perplexity is 100.69810293357811
At time: 121.36086392402649 and batch: 650, loss is 4.557964220046997 and perplexity is 95.38909083173836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.000018849092371 and perplexity of 148.41595658228655
Finished 15 epochs...
Completing Train Step...
At time: 122.4495255947113 and batch: 50, loss is 4.586254873275757 and perplexity is 98.12624591802852
At time: 123.04711318016052 and batch: 100, loss is 4.558560390472412 and perplexity is 95.44597594152467
At time: 123.62143087387085 and batch: 150, loss is 4.538384075164795 and perplexity is 93.53952508618991
At time: 124.20354962348938 and batch: 200, loss is 4.532470903396606 and perplexity is 92.98804192149127
At time: 124.78876996040344 and batch: 250, loss is 4.517362289428711 and perplexity is 91.59368144371078
At time: 125.38099718093872 and batch: 300, loss is 4.563665285110473 and perplexity is 95.9344633699308
At time: 125.95601773262024 and batch: 350, loss is 4.513348731994629 and perplexity is 91.22680168163019
At time: 126.54084491729736 and batch: 400, loss is 4.568959264755249 and perplexity is 96.44368518190099
At time: 127.11666822433472 and batch: 450, loss is 4.541425933837891 and perplexity is 93.8244922971105
At time: 127.693279504776 and batch: 500, loss is 4.507629108428955 and perplexity is 90.70650807710203
At time: 128.26820635795593 and batch: 550, loss is 4.526750078201294 and perplexity is 92.45759233977502
At time: 128.84474754333496 and batch: 600, loss is 4.6068829154968265 and perplexity is 100.17141970676444
At time: 129.42079997062683 and batch: 650, loss is 4.551288766860962 and perplexity is 94.7544460489358
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9996296003753065 and perplexity of 148.35819710372567
Finished 16 epochs...
Completing Train Step...
At time: 130.51599144935608 and batch: 50, loss is 4.577275934219361 and perplexity is 97.24912005882928
At time: 131.09431886672974 and batch: 100, loss is 4.551055936813355 and perplexity is 94.7323869348632
At time: 131.6787703037262 and batch: 150, loss is 4.532347764968872 and perplexity is 92.97659222517423
At time: 132.259126663208 and batch: 200, loss is 4.526914119720459 and perplexity is 92.47276046774813
At time: 132.83901739120483 and batch: 250, loss is 4.513304824829102 and perplexity is 91.22279625928223
At time: 133.4211049079895 and batch: 300, loss is 4.558622770309448 and perplexity is 95.45193003165532
At time: 134.00408697128296 and batch: 350, loss is 4.508840532302856 and perplexity is 90.81645869145792
At time: 134.58112478256226 and batch: 400, loss is 4.5632743072509765 and perplexity is 95.8969624502828
At time: 135.15615057945251 and batch: 450, loss is 4.5360828876495365 and perplexity is 93.324520576604
At time: 135.73462843894958 and batch: 500, loss is 4.502941465377807 and perplexity is 90.28230338170594
At time: 136.3171136379242 and batch: 550, loss is 4.52265622138977 and perplexity is 92.07985791847285
At time: 136.90757966041565 and batch: 600, loss is 4.602674026489257 and perplexity is 99.7506953316721
At time: 137.4875180721283 and batch: 650, loss is 4.545540227890014 and perplexity is 94.21130904097578
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.999726239372702 and perplexity of 148.37253498393721
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 138.6299934387207 and batch: 50, loss is 4.568038339614868 and perplexity is 96.35490865212937
At time: 139.22652506828308 and batch: 100, loss is 4.535809717178345 and perplexity is 93.29903055506344
At time: 139.79791736602783 and batch: 150, loss is 4.5117846202850345 and perplexity is 91.08422430543023
At time: 140.38383436203003 and batch: 200, loss is 4.498678274154663 and perplexity is 89.89823192505699
At time: 140.95947241783142 and batch: 250, loss is 4.479418983459473 and perplexity is 88.18342176163509
At time: 141.54188990592957 and batch: 300, loss is 4.5214570617675784 and perplexity is 91.96950564905646
At time: 142.11655950546265 and batch: 350, loss is 4.468407192230225 and perplexity is 87.21769130107297
At time: 142.69189858436584 and batch: 400, loss is 4.520154781341553 and perplexity is 91.84981351533574
At time: 143.2734613418579 and batch: 450, loss is 4.484954833984375 and perplexity is 88.67294571886237
At time: 143.8516492843628 and batch: 500, loss is 4.447732639312744 and perplexity is 85.43301677017162
At time: 144.43442368507385 and batch: 550, loss is 4.466595382690429 and perplexity is 87.05981252226202
At time: 145.0088152885437 and batch: 600, loss is 4.546739978790283 and perplexity is 94.3244069749211
At time: 145.5866355895996 and batch: 650, loss is 4.509441614151001 and perplexity is 90.87106322554324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.982109817804075 and perplexity of 145.78163009920786
Finished 18 epochs...
Completing Train Step...
At time: 146.69597482681274 and batch: 50, loss is 4.553689489364624 and perplexity is 94.98219845566459
At time: 147.2709014415741 and batch: 100, loss is 4.523166007995606 and perplexity is 92.12681096370687
At time: 147.85627818107605 and batch: 150, loss is 4.502026691436767 and perplexity is 90.19975324634818
At time: 148.4377179145813 and batch: 200, loss is 4.488912830352783 and perplexity is 89.02460839670682
At time: 149.0182590484619 and batch: 250, loss is 4.470585994720459 and perplexity is 87.40792859355797
At time: 149.60006546974182 and batch: 300, loss is 4.515225133895874 and perplexity is 91.39814052574451
At time: 150.18020033836365 and batch: 350, loss is 4.462715139389038 and perplexity is 86.72265381983023
At time: 150.76092314720154 and batch: 400, loss is 4.516174335479736 and perplexity is 91.48493697263916
At time: 151.34945607185364 and batch: 450, loss is 4.483699941635132 and perplexity is 88.56174050757278
At time: 151.93834948539734 and batch: 500, loss is 4.449494686126709 and perplexity is 85.58368644970989
At time: 152.52016472816467 and batch: 550, loss is 4.469402704238892 and perplexity is 87.30456079276821
At time: 153.11436128616333 and batch: 600, loss is 4.550277299880982 and perplexity is 94.65865350921327
At time: 153.70156860351562 and batch: 650, loss is 4.5095172882080075 and perplexity is 90.87794006775799
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.980562097886029 and perplexity of 145.55617548192424
Finished 19 epochs...
Completing Train Step...
At time: 154.84322261810303 and batch: 50, loss is 4.549612092971802 and perplexity is 94.59570685748024
At time: 155.4378523826599 and batch: 100, loss is 4.51875168800354 and perplexity is 91.72103002265267
At time: 156.0264232158661 and batch: 150, loss is 4.497874059677124 and perplexity is 89.8259635289779
At time: 156.60979747772217 and batch: 200, loss is 4.484297676086426 and perplexity is 88.61469273504939
At time: 157.19629764556885 and batch: 250, loss is 4.466451969146728 and perplexity is 87.04732786129038
At time: 157.77992272377014 and batch: 300, loss is 4.511967735290527 and perplexity is 91.1009047208348
At time: 158.3610851764679 and batch: 350, loss is 4.46002688407898 and perplexity is 86.48983426460957
At time: 158.94456148147583 and batch: 400, loss is 4.5144530868530275 and perplexity is 91.32760409385541
At time: 159.53057050704956 and batch: 450, loss is 4.483219079971313 and perplexity is 88.51916479901502
At time: 160.11523342132568 and batch: 500, loss is 4.450501327514648 and perplexity is 85.66988190730146
At time: 160.699147939682 and batch: 550, loss is 4.470578870773315 and perplexity is 87.40730590631276
At time: 161.27948236465454 and batch: 600, loss is 4.551458902359009 and perplexity is 94.77056851526956
At time: 161.87236428260803 and batch: 650, loss is 4.508905897140503 and perplexity is 90.82239508854963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.979879042681525 and perplexity of 145.45678652664955
Finished 20 epochs...
Completing Train Step...
At time: 163.00390338897705 and batch: 50, loss is 4.54712218284607 and perplexity is 94.36046503615627
At time: 163.5878551006317 and batch: 100, loss is 4.51574291229248 and perplexity is 91.44547676218147
At time: 164.1781771183014 and batch: 150, loss is 4.4949384117126465 and perplexity is 89.56265280488402
At time: 164.75738787651062 and batch: 200, loss is 4.480604772567749 and perplexity is 88.2880507243729
At time: 165.33597779273987 and batch: 250, loss is 4.463494663238525 and perplexity is 86.79028255245399
At time: 165.92424941062927 and batch: 300, loss is 4.50921401977539 and perplexity is 90.85038383599297
At time: 166.50473952293396 and batch: 350, loss is 4.457780981063843 and perplexity is 86.29580445268431
At time: 167.08871960639954 and batch: 400, loss is 4.5129888725280765 and perplexity is 91.19397875961555
At time: 167.68346571922302 and batch: 450, loss is 4.482324771881103 and perplexity is 88.44003678148354
At time: 168.259015083313 and batch: 500, loss is 4.450684719085693 and perplexity is 85.68559448226853
At time: 168.83887600898743 and batch: 550, loss is 4.470908718109131 and perplexity is 87.43614172874493
At time: 169.411630153656 and batch: 600, loss is 4.551773633956909 and perplexity is 94.80040050202044
At time: 170.0114779472351 and batch: 650, loss is 4.508088064193726 and perplexity is 90.74814790657518
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.979669608321845 and perplexity of 145.42632606754685
Finished 21 epochs...
Completing Train Step...
At time: 171.15075087547302 and batch: 50, loss is 4.544966087341309 and perplexity is 94.15723403312151
At time: 171.74739360809326 and batch: 100, loss is 4.513330879211426 and perplexity is 91.22517304385538
At time: 172.3248610496521 and batch: 150, loss is 4.49246768951416 and perplexity is 89.34164151162753
At time: 172.89873027801514 and batch: 200, loss is 4.477799491882324 and perplexity is 88.04072503211194
At time: 173.4812092781067 and batch: 250, loss is 4.460897150039673 and perplexity is 86.5651361849125
At time: 174.0679259300232 and batch: 300, loss is 4.507417182922364 and perplexity is 90.68728709120715
At time: 174.64542937278748 and batch: 350, loss is 4.4560935497283936 and perplexity is 86.15030899945044
At time: 175.2299771308899 and batch: 400, loss is 4.511791095733643 and perplexity is 91.0848141185534
At time: 175.82340574264526 and batch: 450, loss is 4.4816233253479005 and perplexity is 88.37802257665406
At time: 176.4016444683075 and batch: 500, loss is 4.450662622451782 and perplexity is 85.68370113997412
At time: 176.9848394393921 and batch: 550, loss is 4.470812788009644 and perplexity is 87.42775437327647
At time: 177.56880831718445 and batch: 600, loss is 4.551728076934815 and perplexity is 94.79608177645511
At time: 178.14580392837524 and batch: 650, loss is 4.507367944717407 and perplexity is 90.68282192190775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.979252534754136 and perplexity of 145.36568523761775
Finished 22 epochs...
Completing Train Step...
At time: 179.26098775863647 and batch: 50, loss is 4.543337593078613 and perplexity is 94.00402430215046
At time: 179.84213089942932 and batch: 100, loss is 4.511380949020386 and perplexity is 91.04746364152633
At time: 180.42615294456482 and batch: 150, loss is 4.490346021652222 and perplexity is 89.15228916452462
At time: 181.0100326538086 and batch: 200, loss is 4.475434741973877 and perplexity is 87.83277670545047
At time: 181.58263039588928 and batch: 250, loss is 4.458785610198975 and perplexity is 86.38254329496584
At time: 182.16554641723633 and batch: 300, loss is 4.505628290176392 and perplexity is 90.5252022805733
At time: 182.74795031547546 and batch: 350, loss is 4.4544970989227295 and perplexity is 86.01288399455737
At time: 183.3236973285675 and batch: 400, loss is 4.510438117980957 and perplexity is 90.96166172145658
At time: 183.90828013420105 and batch: 450, loss is 4.480858526229858 and perplexity is 88.31045698327961
At time: 184.48627281188965 and batch: 500, loss is 4.450468797683715 and perplexity is 85.66709512585398
At time: 185.07941603660583 and batch: 550, loss is 4.4705014896392825 and perplexity is 87.4005424915427
At time: 185.66384196281433 and batch: 600, loss is 4.551471681594848 and perplexity is 94.77177961845372
At time: 186.24581289291382 and batch: 650, loss is 4.506442193984985 and perplexity is 90.59891107935019
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.978880938361673 and perplexity of 145.31167790848136
Finished 23 epochs...
Completing Train Step...
At time: 187.38185214996338 and batch: 50, loss is 4.54165506362915 and perplexity is 93.84599274654825
At time: 187.97883820533752 and batch: 100, loss is 4.509455461502075 and perplexity is 90.87232155777043
At time: 188.5603849887848 and batch: 150, loss is 4.4882785415649415 and perplexity is 88.96815899026417
At time: 189.1333885192871 and batch: 200, loss is 4.473242826461792 and perplexity is 87.64046552165027
At time: 189.7236921787262 and batch: 250, loss is 4.4566524219512935 and perplexity is 86.19846947066576
At time: 190.30352902412415 and batch: 300, loss is 4.503959302902222 and perplexity is 90.37424287967652
At time: 190.89502215385437 and batch: 350, loss is 4.452737302780151 and perplexity is 85.86165196089541
At time: 191.47151064872742 and batch: 400, loss is 4.509110174179077 and perplexity is 90.84094991355238
At time: 192.05457854270935 and batch: 450, loss is 4.480162963867188 and perplexity is 88.2490529108283
At time: 192.6400773525238 and batch: 500, loss is 4.450357971191406 and perplexity is 85.65760146827894
At time: 193.22102332115173 and batch: 550, loss is 4.470611486434937 and perplexity is 87.41015679991706
At time: 193.80751872062683 and batch: 600, loss is 4.5512792682647705 and perplexity is 94.75354601898997
At time: 194.3956015110016 and batch: 650, loss is 4.5058431053161625 and perplexity is 90.54465055337089
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.979049084233303 and perplexity of 145.3361135215374
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 195.59877920150757 and batch: 50, loss is 4.541012535095215 and perplexity is 93.78571338609143
At time: 196.18263411521912 and batch: 100, loss is 4.50631965637207 and perplexity is 90.58780998521848
At time: 196.7575273513794 and batch: 150, loss is 4.48439115524292 and perplexity is 88.62297674896458
At time: 197.33811473846436 and batch: 200, loss is 4.466465148925781 and perplexity is 87.04847513339914
At time: 197.91413044929504 and batch: 250, loss is 4.449404468536377 and perplexity is 85.57596564402817
At time: 198.49966955184937 and batch: 300, loss is 4.494333915710449 and perplexity is 89.50852889980752
At time: 199.078773021698 and batch: 350, loss is 4.441937274932862 and perplexity is 84.93933322767181
At time: 199.66027092933655 and batch: 400, loss is 4.49747932434082 and perplexity is 89.79051304429528
At time: 200.25876140594482 and batch: 450, loss is 4.4679731845855715 and perplexity is 87.17984636938819
At time: 200.84396171569824 and batch: 500, loss is 4.434144229888916 and perplexity is 84.27996973951817
At time: 201.4176104068756 and batch: 550, loss is 4.454846754074096 and perplexity is 86.04296410105562
At time: 201.9952371120453 and batch: 600, loss is 4.537786912918091 and perplexity is 93.48368348814033
At time: 202.57240295410156 and batch: 650, loss is 4.497831125259399 and perplexity is 89.82210698632699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9745525285309435 and perplexity of 144.68406866816682
Finished 25 epochs...
Completing Train Step...
At time: 203.67854762077332 and batch: 50, loss is 4.536308059692383 and perplexity is 93.34553701561832
At time: 204.2785587310791 and batch: 100, loss is 4.503362827301025 and perplexity is 90.32035292244225
At time: 204.8620617389679 and batch: 150, loss is 4.48177885055542 and perplexity is 88.39176865585868
At time: 205.43846797943115 and batch: 200, loss is 4.464639949798584 and perplexity is 86.88973923893688
At time: 206.02507662773132 and batch: 250, loss is 4.448089056015014 and perplexity is 85.46347195131904
At time: 206.60011625289917 and batch: 300, loss is 4.493184289932251 and perplexity is 89.40568671395638
At time: 207.1865038871765 and batch: 350, loss is 4.44109338760376 and perplexity is 84.8676842367085
At time: 207.77057790756226 and batch: 400, loss is 4.497050943374634 and perplexity is 89.75205673512656
At time: 208.36156177520752 and batch: 450, loss is 4.468061990737915 and perplexity is 87.18758881988971
At time: 208.9524540901184 and batch: 500, loss is 4.434506063461304 and perplexity is 84.31047057983098
At time: 209.5278811454773 and batch: 550, loss is 4.455990858078003 and perplexity is 86.14146253627437
At time: 210.10633635520935 and batch: 600, loss is 4.538506736755371 and perplexity is 93.55099949683908
At time: 210.67889714241028 and batch: 650, loss is 4.497633666992187 and perplexity is 89.80437261968044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.973836861404718 and perplexity of 144.58056007979732
Finished 26 epochs...
Completing Train Step...
At time: 211.8058397769928 and batch: 50, loss is 4.534260902404785 and perplexity is 93.15463948455668
At time: 212.38572788238525 and batch: 100, loss is 4.501776103973389 and perplexity is 90.17715315075338
At time: 212.96381902694702 and batch: 150, loss is 4.4805013465881345 and perplexity is 88.27891991842789
At time: 213.54750728607178 and batch: 200, loss is 4.463710641860962 and perplexity is 86.80902942250924
At time: 214.13392066955566 and batch: 250, loss is 4.447403602600097 and perplexity is 85.4049107953844
At time: 214.7083842754364 and batch: 300, loss is 4.492532444000244 and perplexity is 89.34742697102475
At time: 215.30765104293823 and batch: 350, loss is 4.440631713867187 and perplexity is 84.82851209888206
At time: 215.8903636932373 and batch: 400, loss is 4.49687123298645 and perplexity is 89.73592880739186
At time: 216.46857571601868 and batch: 450, loss is 4.468176460266113 and perplexity is 87.1975697132898
At time: 217.04860854148865 and batch: 500, loss is 4.43474666595459 and perplexity is 84.33075832980636
At time: 217.6275749206543 and batch: 550, loss is 4.456686916351319 and perplexity is 86.20144288643604
At time: 218.21285843849182 and batch: 600, loss is 4.538943319320679 and perplexity is 93.5918511490972
At time: 218.788654088974 and batch: 650, loss is 4.497453079223633 and perplexity is 89.78815651268201
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.973492491479013 and perplexity of 144.5307794550313
Finished 27 epochs...
Completing Train Step...
At time: 219.87482070922852 and batch: 50, loss is 4.532685413360595 and perplexity is 93.00799092256851
At time: 220.48101997375488 and batch: 100, loss is 4.5005525207519534 and perplexity is 90.06688137630567
At time: 221.0587179660797 and batch: 150, loss is 4.4795370769500735 and perplexity is 88.19383626465442
At time: 221.63742184638977 and batch: 200, loss is 4.4630746746063235 and perplexity is 86.75383927381344
At time: 222.21376276016235 and batch: 250, loss is 4.4468856525421145 and perplexity is 85.36068677079264
At time: 222.79154562950134 and batch: 300, loss is 4.49200177192688 and perplexity is 89.3000253651709
At time: 223.3645408153534 and batch: 350, loss is 4.440311889648438 and perplexity is 84.8013862242594
At time: 223.93844509124756 and batch: 400, loss is 4.496798086166382 and perplexity is 89.72936514961198
At time: 224.51337003707886 and batch: 450, loss is 4.468299179077149 and perplexity is 87.2082711519907
At time: 225.095556974411 and batch: 500, loss is 4.434982128143311 and perplexity is 84.35061737267431
At time: 225.66812109947205 and batch: 550, loss is 4.457180681228638 and perplexity is 86.24401664115503
At time: 226.25508379936218 and batch: 600, loss is 4.539220056533813 and perplexity is 93.61775508128198
At time: 226.83228087425232 and batch: 650, loss is 4.497283086776734 and perplexity is 89.7728945015039
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.973286049038756 and perplexity of 144.50094524785754
Finished 28 epochs...
Completing Train Step...
At time: 227.94719648361206 and batch: 50, loss is 4.53137243270874 and perplexity is 92.88595336402773
At time: 228.52801942825317 and batch: 100, loss is 4.499513454437256 and perplexity is 89.97334451771285
At time: 229.11481094360352 and batch: 150, loss is 4.478712110519409 and perplexity is 88.12110931311607
At time: 229.69295573234558 and batch: 200, loss is 4.462564277648926 and perplexity is 86.70957167619007
At time: 230.29016184806824 and batch: 250, loss is 4.4464399528503415 and perplexity is 85.32265001612394
At time: 230.87026691436768 and batch: 300, loss is 4.491557188034058 and perplexity is 89.26033283624824
At time: 231.45737147331238 and batch: 350, loss is 4.44003212928772 and perplexity is 84.77766547607713
At time: 232.03505516052246 and batch: 400, loss is 4.496729373931885 and perplexity is 89.72319985625053
At time: 232.6136667728424 and batch: 450, loss is 4.468380565643311 and perplexity is 87.21536902255242
At time: 233.19366884231567 and batch: 500, loss is 4.435129508972168 and perplexity is 84.36304995271679
At time: 233.78466391563416 and batch: 550, loss is 4.457566013336182 and perplexity is 86.2772556334646
At time: 234.37684321403503 and batch: 600, loss is 4.539406585693359 and perplexity is 93.63521915118436
At time: 234.95606184005737 and batch: 650, loss is 4.497122859954834 and perplexity is 89.75851162821695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.973150814280791 and perplexity of 144.48140501879007
Finished 29 epochs...
Completing Train Step...
At time: 236.0581967830658 and batch: 50, loss is 4.530216522216797 and perplexity is 92.77864754588852
At time: 236.65279150009155 and batch: 100, loss is 4.498631105422974 and perplexity is 89.8939916394812
At time: 237.2336175441742 and batch: 150, loss is 4.477987823486328 and perplexity is 88.05730744452201
At time: 237.81729173660278 and batch: 200, loss is 4.4621100330352785 and perplexity is 86.67019326469728
At time: 238.39466381072998 and batch: 250, loss is 4.44605715751648 and perplexity is 85.28999515428815
At time: 238.9885766506195 and batch: 300, loss is 4.491172246932983 and perplexity is 89.22597947787796
At time: 239.57017993927002 and batch: 350, loss is 4.439758338928223 and perplexity is 84.75445734579527
At time: 240.14974117279053 and batch: 400, loss is 4.496637020111084 and perplexity is 89.71491395855239
At time: 240.7305474281311 and batch: 450, loss is 4.4684428691864015 and perplexity is 87.22080301833135
At time: 241.30461072921753 and batch: 500, loss is 4.435232906341553 and perplexity is 84.37177332113409
At time: 241.88373041152954 and batch: 550, loss is 4.457879304885864 and perplexity is 86.3042898031517
At time: 242.46909189224243 and batch: 600, loss is 4.539499158859253 and perplexity is 93.64388766108986
At time: 243.05638933181763 and batch: 650, loss is 4.496984186172486 and perplexity is 89.7460653389185
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.973066741344976 and perplexity of 144.46925855350113
Finished 30 epochs...
Completing Train Step...
At time: 244.13724303245544 and batch: 50, loss is 4.529199914932251 and perplexity is 92.68437602362022
At time: 244.71485304832458 and batch: 100, loss is 4.497824010848999 and perplexity is 89.82146795726803
At time: 245.32400274276733 and batch: 150, loss is 4.477332429885864 and perplexity is 87.99961415671977
At time: 245.9102566242218 and batch: 200, loss is 4.461675968170166 and perplexity is 86.63258094263284
At time: 246.4991421699524 and batch: 250, loss is 4.44569281578064 and perplexity is 85.25892610962238
At time: 247.0910930633545 and batch: 300, loss is 4.490819511413574 and perplexity is 89.19451185586229
At time: 247.67135047912598 and batch: 350, loss is 4.439488916397095 and perplexity is 84.73162566119605
At time: 248.26588320732117 and batch: 400, loss is 4.496524238586426 and perplexity is 89.70479634432229
At time: 248.848375082016 and batch: 450, loss is 4.468471155166626 and perplexity is 87.22327017913358
At time: 249.43068623542786 and batch: 500, loss is 4.435271053314209 and perplexity is 84.37499191025324
At time: 250.0072376728058 and batch: 550, loss is 4.458111829757691 and perplexity is 86.32436003039933
At time: 250.59742879867554 and batch: 600, loss is 4.539579582214356 and perplexity is 93.65141911956894
At time: 251.17969870567322 and batch: 650, loss is 4.4968469619750975 and perplexity is 89.73375085207573
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9729886522480085 and perplexity of 144.45797752002971
Finished 31 epochs...
Completing Train Step...
At time: 252.2632031440735 and batch: 50, loss is 4.528280296325684 and perplexity is 92.59918092639322
At time: 252.85951781272888 and batch: 100, loss is 4.497083292007447 and perplexity is 89.75496013841436
At time: 253.4377043247223 and batch: 150, loss is 4.47671555519104 and perplexity is 87.94534616158958
At time: 254.01598453521729 and batch: 200, loss is 4.461257238388061 and perplexity is 86.5963128946774
At time: 254.59970211982727 and batch: 250, loss is 4.445357875823975 and perplexity is 85.23037427044872
At time: 255.17384958267212 and batch: 300, loss is 4.490467529296875 and perplexity is 89.16312250735012
At time: 255.75130605697632 and batch: 350, loss is 4.439234800338745 and perplexity is 84.71009673000489
At time: 256.3270969390869 and batch: 400, loss is 4.496400308609009 and perplexity is 89.69367991978046
At time: 256.9026348590851 and batch: 450, loss is 4.468491554260254 and perplexity is 87.2250494729364
At time: 257.48147344589233 and batch: 500, loss is 4.435300617218018 and perplexity is 84.3774864012712
At time: 258.06194949150085 and batch: 550, loss is 4.458308305740356 and perplexity is 86.34132236015492
At time: 258.652131319046 and batch: 600, loss is 4.539613218307495 and perplexity is 93.65456924040365
At time: 259.2343900203705 and batch: 650, loss is 4.496691646575928 and perplexity is 89.7198149010051
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.972916247797947 and perplexity of 144.44751849825485
Finished 32 epochs...
Completing Train Step...
At time: 260.34194779396057 and batch: 50, loss is 4.527433195114136 and perplexity is 92.52077326234439
At time: 260.9216012954712 and batch: 100, loss is 4.496384983062744 and perplexity is 89.69230532567242
At time: 261.5034501552582 and batch: 150, loss is 4.476106033325196 and perplexity is 87.89175788337757
At time: 262.0878577232361 and batch: 200, loss is 4.460848054885864 and perplexity is 86.56088636056076
At time: 262.6718680858612 and batch: 250, loss is 4.444990921020508 and perplexity is 85.19910431289034
At time: 263.24565291404724 and batch: 300, loss is 4.4901260566711425 and perplexity is 89.13268093956583
At time: 263.8271985054016 and batch: 350, loss is 4.439002132415771 and perplexity is 84.69038970042949
At time: 264.41097593307495 and batch: 400, loss is 4.496283159255982 and perplexity is 89.68317297865994
At time: 264.9925813674927 and batch: 450, loss is 4.468475618362427 and perplexity is 87.2236594745355
At time: 265.573059797287 and batch: 500, loss is 4.435320014953613 and perplexity is 84.37912314931718
At time: 266.1710455417633 and batch: 550, loss is 4.458451042175293 and perplexity is 86.35364729228381
At time: 266.76148748397827 and batch: 600, loss is 4.539632158279419 and perplexity is 93.65634307211373
At time: 267.35641646385193 and batch: 650, loss is 4.496547136306763 and perplexity is 89.70685040317838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.972847732843137 and perplexity of 144.4376220220847
Finished 33 epochs...
Completing Train Step...
At time: 268.4873378276825 and batch: 50, loss is 4.526636991500855 and perplexity is 92.44713720690612
At time: 269.08053159713745 and batch: 100, loss is 4.495695714950561 and perplexity is 89.63050458079141
At time: 269.65882754325867 and batch: 150, loss is 4.475530977249146 and perplexity is 87.84122972362707
At time: 270.2316234111786 and batch: 200, loss is 4.4604831790924075 and perplexity is 86.52930814988032
At time: 270.80439591407776 and batch: 250, loss is 4.444675312042237 and perplexity is 85.17221895348213
At time: 271.3805937767029 and batch: 300, loss is 4.489786539077759 and perplexity is 89.10242396292
At time: 271.9547517299652 and batch: 350, loss is 4.438760662078858 and perplexity is 84.66994195235687
At time: 272.5334367752075 and batch: 400, loss is 4.4961764526367185 and perplexity is 89.67360370102828
At time: 273.11927604675293 and batch: 450, loss is 4.468454742431641 and perplexity is 87.22183861846352
At time: 273.70056200027466 and batch: 500, loss is 4.435327482223511 and perplexity is 84.37975323335596
At time: 274.28325033187866 and batch: 550, loss is 4.458590326309204 and perplexity is 86.36567582292926
At time: 274.8777458667755 and batch: 600, loss is 4.539643468856812 and perplexity is 93.6574023854211
At time: 275.45728945732117 and batch: 650, loss is 4.496412029266358 and perplexity is 89.69473119482996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9728046492034315 and perplexity of 144.43139925766818
Finished 34 epochs...
Completing Train Step...
At time: 276.55100202560425 and batch: 50, loss is 4.52588903427124 and perplexity is 92.37801655514308
At time: 277.1273362636566 and batch: 100, loss is 4.495053615570068 and perplexity is 89.57297136232546
At time: 277.7070016860962 and batch: 150, loss is 4.474991989135742 and perplexity is 87.79389710194542
At time: 278.2931156158447 and batch: 200, loss is 4.460109052658081 and perplexity is 86.4969413033814
At time: 278.87734508514404 and batch: 250, loss is 4.4443450450897215 and perplexity is 85.14409402891162
At time: 279.45695400238037 and batch: 300, loss is 4.4894664859771725 and perplexity is 89.07391101893236
At time: 280.0477783679962 and batch: 350, loss is 4.438507928848266 and perplexity is 84.64854574827304
At time: 280.6228723526001 and batch: 400, loss is 4.496031475067139 and perplexity is 89.66060398226485
At time: 281.19761204719543 and batch: 450, loss is 4.468428640365601 and perplexity is 87.2195619779844
At time: 281.776784658432 and batch: 500, loss is 4.435302505493164 and perplexity is 84.37764572933207
At time: 282.3567750453949 and batch: 550, loss is 4.458668346405029 and perplexity is 86.37241434409964
At time: 282.93358635902405 and batch: 600, loss is 4.539654245376587 and perplexity is 93.6584116917084
At time: 283.50691390037537 and batch: 650, loss is 4.496254215240478 and perplexity is 89.68057722507679
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.972795075061274 and perplexity of 144.4300164575393
Finished 35 epochs...
Completing Train Step...
At time: 284.60007667541504 and batch: 50, loss is 4.525165100097656 and perplexity is 92.31116515299776
At time: 285.1949288845062 and batch: 100, loss is 4.49442687034607 and perplexity is 89.51684951921044
At time: 285.7883656024933 and batch: 150, loss is 4.474470081329346 and perplexity is 87.7480887366055
At time: 286.36295223236084 and batch: 200, loss is 4.459742794036865 and perplexity is 86.46526685379445
At time: 286.9442460536957 and batch: 250, loss is 4.444044771194458 and perplexity is 85.11853131823834
At time: 287.5296130180359 and batch: 300, loss is 4.489132966995239 and perplexity is 89.04420813232609
At time: 288.10728216171265 and batch: 350, loss is 4.438286876678466 and perplexity is 84.62983607154848
At time: 288.6852824687958 and batch: 400, loss is 4.495903482437134 and perplexity is 89.6491288201371
At time: 289.26184010505676 and batch: 450, loss is 4.468390874862671 and perplexity is 87.21626814955788
At time: 289.8584659099579 and batch: 500, loss is 4.435271606445313 and perplexity is 84.37503858069854
At time: 290.4323570728302 and batch: 550, loss is 4.458777885437012 and perplexity is 86.38187601295846
At time: 291.01380014419556 and batch: 600, loss is 4.539653387069702 and perplexity is 93.65833130408329
At time: 291.59480357170105 and batch: 650, loss is 4.496108980178833 and perplexity is 89.66755340669552
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9727528889973955 and perplexity of 144.42392365215565
Finished 36 epochs...
Completing Train Step...
At time: 292.7032651901245 and batch: 50, loss is 4.5244707679748535 and perplexity is 92.247092792062
At time: 293.28004240989685 and batch: 100, loss is 4.493839464187622 and perplexity is 89.46428221120976
At time: 293.85769414901733 and batch: 150, loss is 4.47398473739624 and perplexity is 87.70551106733805
At time: 294.4396221637726 and batch: 200, loss is 4.459380130767823 and perplexity is 86.43391476292815
At time: 295.01642751693726 and batch: 250, loss is 4.443754367828369 and perplexity is 85.09381619907751
At time: 295.5977714061737 and batch: 300, loss is 4.488802442550659 and perplexity is 89.01478170823421
At time: 296.1912534236908 and batch: 350, loss is 4.438064641952515 and perplexity is 84.61103047282774
At time: 296.7766683101654 and batch: 400, loss is 4.495772228240967 and perplexity is 89.6373627679855
At time: 297.36487913131714 and batch: 450, loss is 4.468342399597168 and perplexity is 87.21204042027414
At time: 297.9436511993408 and batch: 500, loss is 4.435192956924438 and perplexity is 84.36840278529488
At time: 298.5240435600281 and batch: 550, loss is 4.458851633071899 and perplexity is 86.38824670692043
At time: 299.1031529903412 and batch: 600, loss is 4.539651746749878 and perplexity is 93.65817767459177
At time: 299.6821217536926 and batch: 650, loss is 4.495983533859253 and perplexity is 89.65630564764463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.972715789196538 and perplexity of 144.4185656527401
Finished 37 epochs...
Completing Train Step...
At time: 300.80036640167236 and batch: 50, loss is 4.5237721824646 and perplexity is 92.18267281372604
At time: 301.3964774608612 and batch: 100, loss is 4.493275556564331 and perplexity is 89.41384684223964
At time: 301.97891759872437 and batch: 150, loss is 4.47348690032959 and perplexity is 87.66185887971933
At time: 302.56826090812683 and batch: 200, loss is 4.45904709815979 and perplexity is 86.40513424356386
At time: 303.1465699672699 and batch: 250, loss is 4.443483972549439 and perplexity is 85.0708103433887
At time: 303.72155690193176 and batch: 300, loss is 4.48847188949585 and perplexity is 88.98536246279599
At time: 304.30171155929565 and batch: 350, loss is 4.437834234237671 and perplexity is 84.59153768437653
At time: 304.8895072937012 and batch: 400, loss is 4.495658102035523 and perplexity is 89.6271333796385
At time: 305.4719409942627 and batch: 450, loss is 4.468289451599121 and perplexity is 87.20742283957523
At time: 306.0445168018341 and batch: 500, loss is 4.435154323577881 and perplexity is 84.36514341451218
At time: 306.6315426826477 and batch: 550, loss is 4.458902492523193 and perplexity is 86.39264047747763
At time: 307.21858382225037 and batch: 600, loss is 4.539620237350464 and perplexity is 93.65522660815644
At time: 307.8050410747528 and batch: 650, loss is 4.495853109359741 and perplexity is 89.64461303137088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.972696940104167 and perplexity of 144.41584351951096
Finished 38 epochs...
Completing Train Step...
At time: 308.9669156074524 and batch: 50, loss is 4.5230934524536135 and perplexity is 92.12012689549141
At time: 309.55902767181396 and batch: 100, loss is 4.492739706039429 and perplexity is 89.36594722014065
At time: 310.14438557624817 and batch: 150, loss is 4.47299934387207 and perplexity is 87.61912919175843
At time: 310.7261254787445 and batch: 200, loss is 4.458684797286987 and perplexity is 86.37383525818005
At time: 311.30434346199036 and batch: 250, loss is 4.443187036514282 and perplexity is 85.04555350427619
At time: 311.88876938819885 and batch: 300, loss is 4.488108444213867 and perplexity is 88.95302702907969
At time: 312.47707319259644 and batch: 350, loss is 4.437550392150879 and perplexity is 84.56753045308776
At time: 313.06405448913574 and batch: 400, loss is 4.495483722686767 and perplexity is 89.61150562110772
At time: 313.6558802127838 and batch: 450, loss is 4.4681559085845945 and perplexity is 87.19577767502264
At time: 314.23875164985657 and batch: 500, loss is 4.435060386657715 and perplexity is 84.35721878498393
At time: 314.82925939559937 and batch: 550, loss is 4.458904495239258 and perplexity is 86.39281349757987
At time: 315.41443037986755 and batch: 600, loss is 4.539577016830444 and perplexity is 93.6511788680332
At time: 315.99178194999695 and batch: 650, loss is 4.495681991577149 and perplexity is 89.62927455634794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.972708608589921 and perplexity of 144.41752864355504
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 317.0719940662384 and batch: 50, loss is 4.52246675491333 and perplexity is 92.06241352485799
At time: 317.66643357276917 and batch: 100, loss is 4.491940832138061 and perplexity is 89.29458360629513
At time: 318.2473545074463 and batch: 150, loss is 4.472249345779419 and perplexity is 87.5534396485801
At time: 318.8247628211975 and batch: 200, loss is 4.457474555969238 and perplexity is 86.26936530365714
At time: 319.3995168209076 and batch: 250, loss is 4.441681547164917 and perplexity is 84.91761465871373
At time: 319.9885742664337 and batch: 300, loss is 4.485727987289429 and perplexity is 88.74153000958282
At time: 320.5685555934906 and batch: 350, loss is 4.435873136520386 and perplexity is 84.42580797211735
At time: 321.14002084732056 and batch: 400, loss is 4.493349437713623 and perplexity is 89.42045308404231
At time: 321.709379196167 and batch: 450, loss is 4.464557933807373 and perplexity is 86.8826131830764
At time: 322.28616428375244 and batch: 500, loss is 4.431197786331177 and perplexity is 84.03200904611639
At time: 322.8658015727997 and batch: 550, loss is 4.455222969055176 and perplexity is 86.07534084309272
At time: 323.45555901527405 and batch: 600, loss is 4.53617190361023 and perplexity is 93.33282831821467
At time: 324.04097414016724 and batch: 650, loss is 4.493066377639771 and perplexity is 89.39514530596819
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.972140143899357 and perplexity of 144.3354557078159
Finished 40 epochs...
Completing Train Step...
At time: 325.1449828147888 and batch: 50, loss is 4.521781759262085 and perplexity is 91.99937276573829
At time: 325.7226550579071 and batch: 100, loss is 4.491475000381469 and perplexity is 89.25299704048321
At time: 326.3062472343445 and batch: 150, loss is 4.471822395324707 and perplexity is 87.51606664648834
At time: 326.88470244407654 and batch: 200, loss is 4.457203140258789 and perplexity is 86.24595361987639
At time: 327.4664044380188 and batch: 250, loss is 4.44145076751709 and perplexity is 84.89801966265856
At time: 328.0495798587799 and batch: 300, loss is 4.48560471534729 and perplexity is 88.73059134305939
At time: 328.6310484409332 and batch: 350, loss is 4.4360106086730955 and perplexity is 84.43741496748456
At time: 329.21103739738464 and batch: 400, loss is 4.493415794372559 and perplexity is 89.42638692342217
At time: 329.7973577976227 and batch: 450, loss is 4.464576454162597 and perplexity is 86.884222294836
At time: 330.37056040763855 and batch: 500, loss is 4.431215000152588 and perplexity is 84.03345557056298
At time: 330.9536101818085 and batch: 550, loss is 4.455370321273803 and perplexity is 86.08802517004399
At time: 331.5321853160858 and batch: 600, loss is 4.536220855712891 and perplexity is 93.33739726823711
At time: 332.11794352531433 and batch: 650, loss is 4.492783327102661 and perplexity is 89.36984554279898
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.971970202876072 and perplexity of 144.31092927685907
Finished 41 epochs...
Completing Train Step...
At time: 333.20880699157715 and batch: 50, loss is 4.5213819599151615 and perplexity is 91.96259882817711
At time: 333.8168022632599 and batch: 100, loss is 4.491149168014527 and perplexity is 89.22392026253573
At time: 334.3964841365814 and batch: 150, loss is 4.471592779159546 and perplexity is 87.49597384977876
At time: 334.9978497028351 and batch: 200, loss is 4.457051935195923 and perplexity is 86.23291378090703
At time: 335.57754039764404 and batch: 250, loss is 4.441364698410034 and perplexity is 84.89071288036402
At time: 336.15432929992676 and batch: 300, loss is 4.485586957931519 and perplexity is 88.72901573104669
At time: 336.7320077419281 and batch: 350, loss is 4.436228704452515 and perplexity is 84.45583241962532
At time: 337.3124017715454 and batch: 400, loss is 4.493544597625732 and perplexity is 89.43790607481361
At time: 337.8900601863861 and batch: 450, loss is 4.464580307006836 and perplexity is 86.88455704685614
At time: 338.4731140136719 and batch: 500, loss is 4.431234731674194 and perplexity is 84.0351136948658
At time: 339.0535855293274 and batch: 550, loss is 4.455439796447754 and perplexity is 86.0940063583374
At time: 339.62896156311035 and batch: 600, loss is 4.536201028823853 and perplexity is 93.33554669636396
At time: 340.2133734226227 and batch: 650, loss is 4.492531614303589 and perplexity is 89.34735283979417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.971883437212775 and perplexity of 144.29840858655024
Finished 42 epochs...
Completing Train Step...
At time: 341.3095438480377 and batch: 50, loss is 4.5210612106323245 and perplexity is 91.9331066206102
At time: 341.89811611175537 and batch: 100, loss is 4.490868339538574 and perplexity is 89.1988671629664
At time: 342.483106136322 and batch: 150, loss is 4.471410770416259 and perplexity is 87.48005026669536
At time: 343.0639009475708 and batch: 200, loss is 4.456935596466065 and perplexity is 86.22288213679144
At time: 343.6441240310669 and batch: 250, loss is 4.44131628036499 and perplexity is 84.88660273750735
At time: 344.22116136550903 and batch: 300, loss is 4.485588684082031 and perplexity is 88.72916889081483
At time: 344.8027980327606 and batch: 350, loss is 4.4364456367492675 and perplexity is 84.47415560469973
At time: 345.3874273300171 and batch: 400, loss is 4.493676404953003 and perplexity is 89.44969542311426
At time: 345.9677486419678 and batch: 450, loss is 4.46457013130188 and perplexity is 86.88367293973661
At time: 346.547101020813 and batch: 500, loss is 4.431246871948242 and perplexity is 84.03613391036856
At time: 347.1220483779907 and batch: 550, loss is 4.455476121902466 and perplexity is 86.0971338190692
At time: 347.7011139392853 and batch: 600, loss is 4.5361543560028075 and perplexity is 93.3311905647531
At time: 348.2834532260895 and batch: 650, loss is 4.49229736328125 and perplexity is 89.32642558225962
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.971832275390625 and perplexity of 144.29102620588318
Finished 43 epochs...
Completing Train Step...
At time: 349.3801257610321 and batch: 50, loss is 4.520781984329224 and perplexity is 91.90744006267178
At time: 349.97287154197693 and batch: 100, loss is 4.4906086921691895 and perplexity is 89.1757099182444
At time: 350.5469617843628 and batch: 150, loss is 4.471252384185791 and perplexity is 87.46619572850547
At time: 351.1306166648865 and batch: 200, loss is 4.456834907531738 and perplexity is 86.21420088373493
At time: 351.71167397499084 and batch: 250, loss is 4.441283721923828 and perplexity is 84.88383900703825
At time: 352.28639817237854 and batch: 300, loss is 4.485587882995605 and perplexity is 88.72909781111055
At time: 352.868531703949 and batch: 350, loss is 4.436651039123535 and perplexity is 84.49150857893522
At time: 353.44136691093445 and batch: 400, loss is 4.493800325393677 and perplexity is 89.46078075562464
At time: 354.03127789497375 and batch: 450, loss is 4.464553098678589 and perplexity is 86.8821930954682
At time: 354.62026476860046 and batch: 500, loss is 4.431253309249878 and perplexity is 84.03667487805204
At time: 355.20443749427795 and batch: 550, loss is 4.455490455627442 and perplexity is 86.09836792055118
At time: 355.78383207321167 and batch: 600, loss is 4.5360923099517825 and perplexity is 93.3253999125865
At time: 356.3586051464081 and batch: 650, loss is 4.492078914642334 and perplexity is 89.30691447733665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.971792782054228 and perplexity of 144.2853277843718
Finished 44 epochs...
Completing Train Step...
At time: 357.47282099723816 and batch: 50, loss is 4.520526571273804 and perplexity is 91.88396870016334
At time: 358.05901432037354 and batch: 100, loss is 4.4903637790679936 and perplexity is 89.1538722928464
At time: 358.6361653804779 and batch: 150, loss is 4.47110689163208 and perplexity is 87.45347097402647
At time: 359.2187566757202 and batch: 200, loss is 4.456744890213013 and perplexity is 86.20644046182677
At time: 359.7996406555176 and batch: 250, loss is 4.441262187957764 and perplexity is 84.8820111410104
At time: 360.38395166397095 and batch: 300, loss is 4.485598497390747 and perplexity is 88.73003962181363
At time: 360.97472190856934 and batch: 350, loss is 4.436845903396606 and perplexity is 84.50797455959872
At time: 361.5559096336365 and batch: 400, loss is 4.493919162750244 and perplexity is 89.47141267004791
At time: 362.139523267746 and batch: 450, loss is 4.464535303115845 and perplexity is 86.88064699170654
At time: 362.7170457839966 and batch: 500, loss is 4.431258792877197 and perplexity is 84.03713570512173
At time: 363.29333329200745 and batch: 550, loss is 4.455496530532837 and perplexity is 86.09889096157973
At time: 363.8696048259735 and batch: 600, loss is 4.53602632522583 and perplexity is 93.31924206481305
At time: 364.4681656360626 and batch: 650, loss is 4.491869010925293 and perplexity is 89.28817059130492
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.971757178213082 and perplexity of 144.28019076393096
Finished 45 epochs...
Completing Train Step...
At time: 365.57171630859375 and batch: 50, loss is 4.520287895202637 and perplexity is 91.86204081244593
At time: 366.16446924209595 and batch: 100, loss is 4.49013542175293 and perplexity is 89.13351567832146
At time: 366.7410876750946 and batch: 150, loss is 4.470973405838013 and perplexity is 87.44179795711787
At time: 367.31739497184753 and batch: 200, loss is 4.456662321090699 and perplexity is 86.19932276555498
At time: 367.89907145500183 and batch: 250, loss is 4.441239395141602 and perplexity is 84.88007646298341
At time: 368.48441767692566 and batch: 300, loss is 4.485610656738281 and perplexity is 88.7311185277615
At time: 369.06812143325806 and batch: 350, loss is 4.437032032012939 and perplexity is 84.52370537590473
At time: 369.6539397239685 and batch: 400, loss is 4.494029455184936 and perplexity is 89.48128123419059
At time: 370.23698234558105 and batch: 450, loss is 4.464515171051025 and perplexity is 86.87889792249597
At time: 370.8146131038666 and batch: 500, loss is 4.4312605285644535 and perplexity is 84.0372815674338
At time: 371.4009099006653 and batch: 550, loss is 4.455495910644531 and perplexity is 86.0988375899006
At time: 371.9770243167877 and batch: 600, loss is 4.535957736968994 and perplexity is 93.3128416801687
At time: 372.5581109523773 and batch: 650, loss is 4.491667060852051 and perplexity is 89.27014065934867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.971728455786612 and perplexity of 144.27604674627406
Finished 46 epochs...
Completing Train Step...
At time: 373.6863601207733 and batch: 50, loss is 4.520061588287353 and perplexity is 91.84125414952948
At time: 374.2684061527252 and batch: 100, loss is 4.48991771697998 and perplexity is 89.11411299863315
At time: 374.8450710773468 and batch: 150, loss is 4.470847883224487 and perplexity is 87.43082272294137
At time: 375.4272651672363 and batch: 200, loss is 4.456584300994873 and perplexity is 86.19259774847936
At time: 376.0136902332306 and batch: 250, loss is 4.441219272613526 and perplexity is 84.87836847844625
At time: 376.599080324173 and batch: 300, loss is 4.485623207092285 and perplexity is 88.7322321416983
At time: 377.18813157081604 and batch: 350, loss is 4.437210359573364 and perplexity is 84.53877962611968
At time: 377.76483130455017 and batch: 400, loss is 4.494132556915283 and perplexity is 89.49050738472738
At time: 378.347971200943 and batch: 450, loss is 4.464493618011475 and perplexity is 86.87702543835188
At time: 378.9263834953308 and batch: 500, loss is 4.4312599182128904 and perplexity is 84.03723027516328
At time: 379.52420449256897 and batch: 550, loss is 4.455491390228271 and perplexity is 86.09844838819487
At time: 380.109482049942 and batch: 600, loss is 4.535887327194214 and perplexity is 93.30627177529736
At time: 380.6887993812561 and batch: 650, loss is 4.491471014022827 and perplexity is 89.25264124673626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.971704819623162 and perplexity of 144.2726366543522
Finished 47 epochs...
Completing Train Step...
At time: 381.76157808303833 and batch: 50, loss is 4.519844942092895 and perplexity is 91.82135924647906
At time: 382.35860776901245 and batch: 100, loss is 4.489708662033081 and perplexity is 89.09548519965684
At time: 382.9397921562195 and batch: 150, loss is 4.470728330612182 and perplexity is 87.4203707644808
At time: 383.5246181488037 and batch: 200, loss is 4.456509599685669 and perplexity is 86.1861592890675
At time: 384.1043336391449 and batch: 250, loss is 4.4412008571624755 and perplexity is 84.87680541939858
At time: 384.6908366680145 and batch: 300, loss is 4.485635566711426 and perplexity is 88.73332884507047
At time: 385.2727060317993 and batch: 350, loss is 4.4373816299438475 and perplexity is 84.55325985420806
At time: 385.8581051826477 and batch: 400, loss is 4.494229574203491 and perplexity is 89.49918993224595
At time: 386.4432249069214 and batch: 450, loss is 4.46447114944458 and perplexity is 86.87507345802337
At time: 387.0208332538605 and batch: 500, loss is 4.431257581710815 and perplexity is 84.03703392222972
At time: 387.6020483970642 and batch: 550, loss is 4.455484037399292 and perplexity is 86.09781532335589
At time: 388.18953132629395 and batch: 600, loss is 4.535815315246582 and perplexity is 93.2995528508649
At time: 388.76305198669434 and batch: 650, loss is 4.49128080368042 and perplexity is 89.23566608576073
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.971685072954963 and perplexity of 144.26978777859395
Finished 48 epochs...
Completing Train Step...
At time: 389.8777709007263 and batch: 50, loss is 4.519636287689209 and perplexity is 91.80220231417844
At time: 390.45975852012634 and batch: 100, loss is 4.489506826400757 and perplexity is 89.07750437071138
At time: 391.03709864616394 and batch: 150, loss is 4.470612478256226 and perplexity is 87.41024349521446
At time: 391.62447452545166 and batch: 200, loss is 4.456437406539917 and perplexity is 86.17993746369741
At time: 392.20597982406616 and batch: 250, loss is 4.441183500289917 and perplexity is 84.87533223628867
At time: 392.7802276611328 and batch: 300, loss is 4.485647983551026 and perplexity is 88.73443063942231
At time: 393.3616349697113 and batch: 350, loss is 4.437546272277832 and perplexity is 84.56718204631608
At time: 393.9436447620392 and batch: 400, loss is 4.4943215274810795 and perplexity is 89.50742005448932
At time: 394.5495674610138 and batch: 450, loss is 4.464447727203369 and perplexity is 86.87303867292735
At time: 395.13072752952576 and batch: 500, loss is 4.431253843307495 and perplexity is 84.03671975849034
At time: 395.71118330955505 and batch: 550, loss is 4.455473699569702 and perplexity is 86.09692526341368
At time: 396.2947008609772 and batch: 600, loss is 4.535741939544677 and perplexity is 93.29270718184303
At time: 396.86932611465454 and batch: 650, loss is 4.491095685958863 and perplexity is 89.21914851146833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.971667420630362 and perplexity of 144.26724110394738
Finished 49 epochs...
Completing Train Step...
At time: 397.9486472606659 and batch: 50, loss is 4.5194344902038575 and perplexity is 91.783678729671
At time: 398.54283118247986 and batch: 100, loss is 4.489311113357544 and perplexity is 89.06007244713217
At time: 399.1249825954437 and batch: 150, loss is 4.470499696731568 and perplexity is 87.40038579057627
At time: 399.7095699310303 and batch: 200, loss is 4.456367092132568 and perplexity is 86.17387798550615
At time: 400.29099559783936 and batch: 250, loss is 4.441166772842407 and perplexity is 84.87391250049812
At time: 400.8728952407837 and batch: 300, loss is 4.485659732818603 and perplexity is 88.73547321011594
At time: 401.45601415634155 and batch: 350, loss is 4.437704572677612 and perplexity is 84.58057012468343
At time: 402.03583908081055 and batch: 400, loss is 4.494408912658692 and perplexity is 89.51524201804526
At time: 402.61188101768494 and batch: 450, loss is 4.4644229698181155 and perplexity is 86.87088795026399
At time: 403.1848576068878 and batch: 500, loss is 4.4312490940094 and perplexity is 84.036320644005
At time: 403.7706620693207 and batch: 550, loss is 4.4554609680175785 and perplexity is 86.0958291228998
At time: 404.35768365859985 and batch: 600, loss is 4.535667657852173 and perplexity is 93.2857774990327
At time: 404.9334373474121 and batch: 650, loss is 4.490915575027466 and perplexity is 89.20308061457688
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.971650665881587 and perplexity of 144.26482396281557
Finished Training.
Improved accuracyfrom -10000000 to -144.26482396281557
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb2bd5cf898>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'tune_wordvecs': True, 'data': 'ptb', 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.5157987567052392, 'anneal': 4.910993765445567, 'wordvec_dim': 200, 'lr': 22.683736482515734, 'seq_len': 20}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.8670732975006104 and batch: 50, loss is 6.337654895782471 and perplexity is 565.4686722957522
At time: 1.4685533046722412 and batch: 100, loss is 5.836555261611938 and perplexity is 342.59714811419923
At time: 2.0674898624420166 and batch: 150, loss is 5.73254240989685 and perplexity is 308.75324867059345
At time: 2.6644530296325684 and batch: 200, loss is 5.695461463928223 and perplexity is 297.51405391734863
At time: 3.269544839859009 and batch: 250, loss is 5.666451082229615 and perplexity is 289.00704998585445
At time: 3.862064838409424 and batch: 300, loss is 5.666796236038208 and perplexity is 289.1068190867196
At time: 4.452228546142578 and batch: 350, loss is 5.581413278579712 and perplexity is 265.4464906547534
At time: 5.043720722198486 and batch: 400, loss is 5.6239290523529055 and perplexity is 276.97549937146533
At time: 5.630811929702759 and batch: 450, loss is 5.57421576499939 and perplexity is 263.5427950890392
At time: 6.239551305770874 and batch: 500, loss is 5.585536088943481 and perplexity is 266.54313527299286
At time: 6.828206777572632 and batch: 550, loss is 5.584770698547363 and perplexity is 266.33920377066596
At time: 7.410366535186768 and batch: 600, loss is 5.606583700180054 and perplexity is 272.21268752752405
At time: 7.991655111312866 and batch: 650, loss is 5.579651479721069 and perplexity is 264.9792390530164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.402088240081189 and perplexity of 221.8692490415866
Finished 1 epochs...
Completing Train Step...
At time: 9.104927062988281 and batch: 50, loss is 5.3357741641998295 and perplexity is 207.6334289708964
At time: 9.694355010986328 and batch: 100, loss is 5.29526614189148 and perplexity is 199.3906851197071
At time: 10.275954008102417 and batch: 150, loss is 5.251431589126587 and perplexity is 190.8392764274431
At time: 10.85858941078186 and batch: 200, loss is 5.23473970413208 and perplexity is 187.68024759267067
At time: 11.448568105697632 and batch: 250, loss is 5.2056259918212895 and perplexity is 182.294952207802
At time: 12.023440599441528 and batch: 300, loss is 5.237170257568359 and perplexity is 188.13696928177055
At time: 12.603899478912354 and batch: 350, loss is 5.196213979721069 and perplexity is 180.58723902697454
At time: 13.189162731170654 and batch: 400, loss is 5.228268728256226 and perplexity is 186.4696941932219
At time: 13.76586389541626 and batch: 450, loss is 5.181328630447387 and perplexity is 177.91904268358488
At time: 14.34755277633667 and batch: 500, loss is 5.1612334728240965 and perplexity is 174.37941527422402
At time: 14.927713632583618 and batch: 550, loss is 5.172843532562256 and perplexity is 176.4157689225328
At time: 15.507448434829712 and batch: 600, loss is 5.236688394546508 and perplexity is 188.04633487166961
At time: 16.09100365638733 and batch: 650, loss is 5.20919508934021 and perplexity is 182.9467431302199
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.325814041436887 and perplexity of 205.57563945846323
Finished 2 epochs...
Completing Train Step...
At time: 17.190728664398193 and batch: 50, loss is 5.182971563339233 and perplexity is 178.2115919845252
At time: 17.809232473373413 and batch: 100, loss is 5.131096878051758 and perplexity is 169.20261092497861
At time: 18.388909816741943 and batch: 150, loss is 5.101811676025391 and perplexity is 164.31933119351493
At time: 18.97707986831665 and batch: 200, loss is 5.102457685470581 and perplexity is 164.42551732842344
At time: 19.560061931610107 and batch: 250, loss is 5.070838098526001 and perplexity is 159.30778703079244
At time: 20.143252849578857 and batch: 300, loss is 5.14231580734253 and perplexity is 171.11157127589146
At time: 20.72070813179016 and batch: 350, loss is 5.0857266998291015 and perplexity is 161.69740202675527
At time: 21.318689346313477 and batch: 400, loss is 5.142279386520386 and perplexity is 171.10533936527358
At time: 21.905673027038574 and batch: 450, loss is 5.136756916046142 and perplexity is 170.16301954222297
At time: 22.491962432861328 and batch: 500, loss is 5.085606565475464 and perplexity is 161.67797778066094
At time: 23.078243732452393 and batch: 550, loss is 5.127317905426025 and perplexity is 168.56440552994732
At time: 23.659257650375366 and batch: 600, loss is 5.179642248153686 and perplexity is 177.61925600880775
At time: 24.238929986953735 and batch: 650, loss is 5.149876747131348 and perplexity is 172.41023895069958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.347296621285233 and perplexity of 210.03971277376766
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 25.362108945846558 and batch: 50, loss is 5.104434299468994 and perplexity is 164.75084452489523
At time: 25.946896076202393 and batch: 100, loss is 5.007442789077759 and perplexity is 149.52188784103896
At time: 26.523277044296265 and batch: 150, loss is 4.946439456939697 and perplexity is 140.67319819273254
At time: 27.10697340965271 and batch: 200, loss is 4.912054967880249 and perplexity is 135.91843564001505
At time: 27.685798406600952 and batch: 250, loss is 4.8957962226867675 and perplexity is 133.7264402620387
At time: 28.269610166549683 and batch: 300, loss is 4.950808000564575 and perplexity is 141.28907946853403
At time: 28.84460163116455 and batch: 350, loss is 4.8955405330657955 and perplexity is 133.69225217017055
At time: 29.42359757423401 and batch: 400, loss is 4.919506702423096 and perplexity is 136.9350467958931
At time: 30.004148960113525 and batch: 450, loss is 4.893123016357422 and perplexity is 133.36943927677112
At time: 30.58884310722351 and batch: 500, loss is 4.854687004089356 and perplexity is 128.3405148712078
At time: 31.163556814193726 and batch: 550, loss is 4.8665907764434815 and perplexity is 129.87738024406502
At time: 31.742558002471924 and batch: 600, loss is 4.9124320697784425 and perplexity is 135.96970040550704
At time: 32.33085083961487 and batch: 650, loss is 4.857498950958252 and perplexity is 128.70190945320388
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.105555515663297 and perplexity of 164.9356694347803
Finished 4 epochs...
Completing Train Step...
At time: 33.42045998573303 and batch: 50, loss is 4.92110728263855 and perplexity is 137.15439782022295
At time: 34.009278297424316 and batch: 100, loss is 4.881721601486206 and perplexity is 131.85747461523025
At time: 34.601540327072144 and batch: 150, loss is 4.8390975761413575 and perplexity is 126.35527425058787
At time: 35.1816520690918 and batch: 200, loss is 4.813146696090699 and perplexity is 123.11842488175202
At time: 35.7609429359436 and batch: 250, loss is 4.795797071456909 and perplexity is 121.00078963165684
At time: 36.35661697387695 and batch: 300, loss is 4.865508337020874 and perplexity is 129.7368719071845
At time: 36.93595218658447 and batch: 350, loss is 4.817986164093018 and perplexity is 123.71569663252914
At time: 37.51823139190674 and batch: 400, loss is 4.848897466659546 and perplexity is 127.59962941809218
At time: 38.11255693435669 and batch: 450, loss is 4.827605705261231 and perplexity is 124.91152731930858
At time: 38.69433927536011 and batch: 500, loss is 4.797263355255127 and perplexity is 121.17834126779782
At time: 39.26733136177063 and batch: 550, loss is 4.819437503814697 and perplexity is 123.89538049687171
At time: 39.837637424468994 and batch: 600, loss is 4.87677339553833 and perplexity is 131.2066282639437
At time: 40.41427254676819 and batch: 650, loss is 4.8155055809021 and perplexity is 123.40918986994018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.08204620959712 and perplexity of 161.10337015287695
Finished 5 epochs...
Completing Train Step...
At time: 41.528544902801514 and batch: 50, loss is 4.855376071929932 and perplexity is 128.42898066861613
At time: 42.11277151107788 and batch: 100, loss is 4.823320951461792 and perplexity is 124.37745717466186
At time: 42.68900537490845 and batch: 150, loss is 4.789072427749634 and perplexity is 120.18983218902866
At time: 43.277116775512695 and batch: 200, loss is 4.769782390594482 and perplexity is 117.89358441629125
At time: 43.86559319496155 and batch: 250, loss is 4.750281019210815 and perplexity is 115.61677049598501
At time: 44.456024408340454 and batch: 300, loss is 4.815864887237549 and perplexity is 123.4535395407948
At time: 45.04299759864807 and batch: 350, loss is 4.7709510612487795 and perplexity is 118.03144372910455
At time: 45.6269474029541 and batch: 400, loss is 4.8103217506408695 and perplexity is 122.77111284733948
At time: 46.212005615234375 and batch: 450, loss is 4.7844505119323735 and perplexity is 119.63560668099957
At time: 46.794496297836304 and batch: 500, loss is 4.7518221950531006 and perplexity is 115.79509364800703
At time: 47.37445878982544 and batch: 550, loss is 4.774685554504394 and perplexity is 118.47305544440506
At time: 47.95725464820862 and batch: 600, loss is 4.834070053100586 and perplexity is 125.72161440241103
At time: 48.543285846710205 and batch: 650, loss is 4.771503667831421 and perplexity is 118.09668670705722
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.0510822370940565 and perplexity of 156.19140918758998
Finished 6 epochs...
Completing Train Step...
At time: 49.65016293525696 and batch: 50, loss is 4.802916116714478 and perplexity is 121.86527322710906
At time: 50.238847732543945 and batch: 100, loss is 4.7768323802948 and perplexity is 118.72766966369744
At time: 50.830310344696045 and batch: 150, loss is 4.745980825424194 and perplexity is 115.12066342063741
At time: 51.421570777893066 and batch: 200, loss is 4.725033588409424 and perplexity is 112.73428487579407
At time: 52.00695323944092 and batch: 250, loss is 4.712960863113404 and perplexity is 111.38145741147558
At time: 52.58263301849365 and batch: 300, loss is 4.775634927749634 and perplexity is 118.58558400087425
At time: 53.17331290245056 and batch: 350, loss is 4.731896648406982 and perplexity is 113.51064810305644
At time: 53.74980545043945 and batch: 400, loss is 4.77372257232666 and perplexity is 118.35902291792793
At time: 54.323071002960205 and batch: 450, loss is 4.750719270706177 and perplexity is 115.66745082309714
At time: 54.90449047088623 and batch: 500, loss is 4.718529090881348 and perplexity is 112.00338464480141
At time: 55.48756217956543 and batch: 550, loss is 4.743784208297729 and perplexity is 114.8680649324086
At time: 56.07145404815674 and batch: 600, loss is 4.809927959442138 and perplexity is 122.72277618154317
At time: 56.65727114677429 and batch: 650, loss is 4.747055234909058 and perplexity is 115.24441662222254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.063878676470588 and perplexity of 158.20294589130668
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 57.77390456199646 and batch: 50, loss is 4.76294828414917 and perplexity is 117.09063396076095
At time: 58.35573935508728 and batch: 100, loss is 4.69713436126709 and perplexity is 109.63255458204851
At time: 58.95556378364563 and batch: 150, loss is 4.652487277984619 and perplexity is 104.84544128786264
At time: 59.54094171524048 and batch: 200, loss is 4.623618268966675 and perplexity is 101.86193001167506
At time: 60.122496366500854 and batch: 250, loss is 4.598964385986328 and perplexity is 99.38134162038759
At time: 60.69766163825989 and batch: 300, loss is 4.6541803932189945 and perplexity is 105.02310706363573
At time: 61.28299856185913 and batch: 350, loss is 4.603452596664429 and perplexity is 99.82838848887286
At time: 61.861562967300415 and batch: 400, loss is 4.639562826156617 and perplexity is 103.49909059619372
At time: 62.44122624397278 and batch: 450, loss is 4.606096401214599 and perplexity is 100.0926644296291
At time: 63.026999950408936 and batch: 500, loss is 4.563234825134277 and perplexity is 95.89317630996311
At time: 63.60973501205444 and batch: 550, loss is 4.5798079204559325 and perplexity is 97.49566548543443
At time: 64.18724918365479 and batch: 600, loss is 4.657377843856811 and perplexity is 105.35945069883127
At time: 64.77326226234436 and batch: 650, loss is 4.619507055282593 and perplexity is 101.44401351177859
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.981622134937959 and perplexity of 145.71055222915254
Finished 8 epochs...
Completing Train Step...
At time: 65.88580584526062 and batch: 50, loss is 4.694478216171265 and perplexity is 109.34174100236109
At time: 66.50038385391235 and batch: 100, loss is 4.65349235534668 and perplexity is 104.95087204157292
At time: 67.0944435596466 and batch: 150, loss is 4.619395341873169 and perplexity is 101.43268148814478
At time: 67.68378829956055 and batch: 200, loss is 4.595851030349731 and perplexity is 99.07241331166324
At time: 68.26709127426147 and batch: 250, loss is 4.576745023727417 and perplexity is 97.19750318384044
At time: 68.85292482376099 and batch: 300, loss is 4.634051618576049 and perplexity is 102.930254550073
At time: 69.4337637424469 and batch: 350, loss is 4.586968364715577 and perplexity is 98.19628313701963
At time: 70.0286295413971 and batch: 400, loss is 4.6263148021698 and perplexity is 102.13697475506649
At time: 70.61088156700134 and batch: 450, loss is 4.596609840393066 and perplexity is 99.14761898370261
At time: 71.19003224372864 and batch: 500, loss is 4.557750377655029 and perplexity is 95.36869478123515
At time: 71.7707257270813 and batch: 550, loss is 4.578760957717895 and perplexity is 97.39364457191753
At time: 72.365975856781 and batch: 600, loss is 4.6595291328430175 and perplexity is 105.58635330374022
At time: 72.95500826835632 and batch: 650, loss is 4.61236946105957 and perplexity is 100.72252521535587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.977206959443934 and perplexity of 145.06863270618882
Finished 9 epochs...
Completing Train Step...
At time: 74.07588744163513 and batch: 50, loss is 4.674609432220459 and perplexity is 107.19069368415511
At time: 74.66827154159546 and batch: 100, loss is 4.636282663345337 and perplexity is 103.1601529175024
At time: 75.25017094612122 and batch: 150, loss is 4.604996147155762 and perplexity is 99.98259763115497
At time: 75.83171916007996 and batch: 200, loss is 4.582967166900635 and perplexity is 97.80416537699539
At time: 76.42059588432312 and batch: 250, loss is 4.566050996780396 and perplexity is 96.16360856724191
At time: 77.00087904930115 and batch: 300, loss is 4.623762264251709 and perplexity is 101.87659870540718
At time: 77.58621120452881 and batch: 350, loss is 4.577957000732422 and perplexity is 97.315375737627
At time: 78.17973947525024 and batch: 400, loss is 4.618810052871704 and perplexity is 101.37333142544443
At time: 78.75839304924011 and batch: 450, loss is 4.591790685653686 and perplexity is 98.67096073321478
At time: 79.34331512451172 and batch: 500, loss is 4.554258708953857 and perplexity is 95.03627957420981
At time: 79.93956232070923 and batch: 550, loss is 4.576417427062989 and perplexity is 97.16566682103448
At time: 80.52199864387512 and batch: 600, loss is 4.656483373641968 and perplexity is 105.26525194359981
At time: 81.13144636154175 and batch: 650, loss is 4.6045472621917725 and perplexity is 99.93772701804295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.975300508386948 and perplexity of 144.79232992058598
Finished 10 epochs...
Completing Train Step...
At time: 82.20875763893127 and batch: 50, loss is 4.660755519866943 and perplexity is 105.71592247206934
At time: 82.81198167800903 and batch: 100, loss is 4.624071178436279 and perplexity is 101.90807469326231
At time: 83.39378929138184 and batch: 150, loss is 4.594592065811157 and perplexity is 98.94776313809112
At time: 83.98403859138489 and batch: 200, loss is 4.574016780853271 and perplexity is 96.932686195185
At time: 84.56341814994812 and batch: 250, loss is 4.558349103927612 and perplexity is 95.42581162135274
At time: 85.15080070495605 and batch: 300, loss is 4.615624160766601 and perplexity is 101.05088084829909
At time: 85.73152685165405 and batch: 350, loss is 4.570358419418335 and perplexity is 96.57871925849848
At time: 86.31934690475464 and batch: 400, loss is 4.6119356536865235 and perplexity is 100.67884051734268
At time: 86.89460253715515 and batch: 450, loss is 4.5862892055511475 and perplexity is 98.12961487315805
At time: 87.46892476081848 and batch: 500, loss is 4.549828853607178 and perplexity is 94.61621370546091
At time: 88.04381251335144 and batch: 550, loss is 4.57244875907898 and perplexity is 96.78081273415482
At time: 88.62691569328308 and batch: 600, loss is 4.651947793960571 and perplexity is 104.78889410181675
At time: 89.2080442905426 and batch: 650, loss is 4.596766309738159 and perplexity is 99.16313376047435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9731792375153185 and perplexity of 144.4855117060122
Finished 11 epochs...
Completing Train Step...
At time: 90.33969759941101 and batch: 50, loss is 4.6488807582855225 and perplexity is 104.46799518102685
At time: 90.91648316383362 and batch: 100, loss is 4.613450803756714 and perplexity is 100.8314996911935
At time: 91.49193906784058 and batch: 150, loss is 4.586653232574463 and perplexity is 98.16534320740443
At time: 92.07411408424377 and batch: 200, loss is 4.567016401290894 and perplexity is 96.25649017564648
At time: 92.65061450004578 and batch: 250, loss is 4.552175970077514 and perplexity is 94.83854980129618
At time: 93.2328155040741 and batch: 300, loss is 4.608955297470093 and perplexity is 100.37922840632852
At time: 93.8157684803009 and batch: 350, loss is 4.56399489402771 and perplexity is 95.96608943635825
At time: 94.39817571640015 and batch: 400, loss is 4.606196346282959 and perplexity is 100.10266869774816
At time: 94.97852349281311 and batch: 450, loss is 4.581539068222046 and perplexity is 97.6645910643272
At time: 95.56256675720215 and batch: 500, loss is 4.545560913085938 and perplexity is 94.21325784051706
At time: 96.17459607124329 and batch: 550, loss is 4.568117418289185 and perplexity is 96.36252857185205
At time: 96.7637631893158 and batch: 600, loss is 4.647349195480347 and perplexity is 104.30811834723404
At time: 97.35177445411682 and batch: 650, loss is 4.590512704849243 and perplexity is 98.54494168155786
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.972936892041973 and perplexity of 144.4505005388566
Finished 12 epochs...
Completing Train Step...
At time: 98.4800329208374 and batch: 50, loss is 4.639274082183838 and perplexity is 103.46921017169983
At time: 99.08271503448486 and batch: 100, loss is 4.604902944564819 and perplexity is 99.97327942825363
At time: 99.6542739868164 and batch: 150, loss is 4.57872296333313 and perplexity is 97.38994423060849
At time: 100.23318338394165 and batch: 200, loss is 4.55959677696228 and perplexity is 95.5449461383316
At time: 100.8065173625946 and batch: 250, loss is 4.545681524276733 and perplexity is 94.22462169902441
At time: 101.38219380378723 and batch: 300, loss is 4.601908836364746 and perplexity is 99.67439628005445
At time: 101.96628093719482 and batch: 350, loss is 4.557586574554444 and perplexity is 95.35307437270177
At time: 102.54823350906372 and batch: 400, loss is 4.600157794952392 and perplexity is 99.50001500339172
At time: 103.13195180892944 and batch: 450, loss is 4.576593246459961 and perplexity is 97.18275193188428
At time: 103.71338510513306 and batch: 500, loss is 4.541192150115966 and perplexity is 93.80256022187459
At time: 104.30373191833496 and batch: 550, loss is 4.563273143768311 and perplexity is 95.8968508758942
At time: 104.8830578327179 and batch: 600, loss is 4.642479839324952 and perplexity is 103.80143956995273
At time: 105.46061277389526 and batch: 650, loss is 4.584233140945434 and perplexity is 97.92806131981264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.972462373621323 and perplexity of 144.38197237570253
Finished 13 epochs...
Completing Train Step...
At time: 106.5468738079071 and batch: 50, loss is 4.630788097381592 and perplexity is 102.59488701997368
At time: 107.1213927268982 and batch: 100, loss is 4.597301998138428 and perplexity is 99.21626853153373
At time: 107.69561100006104 and batch: 150, loss is 4.572201910018921 and perplexity is 96.75692542990029
At time: 108.27346324920654 and batch: 200, loss is 4.553860273361206 and perplexity is 94.998421280381
At time: 108.84680652618408 and batch: 250, loss is 4.540049591064453 and perplexity is 93.69544646118892
At time: 109.4278724193573 and batch: 300, loss is 4.595575399398804 and perplexity is 99.04510965121129
At time: 110.00532054901123 and batch: 350, loss is 4.551880416870117 and perplexity is 94.81052410546363
At time: 110.58196711540222 and batch: 400, loss is 4.594623079299927 and perplexity is 98.9508319010183
At time: 111.19167375564575 and batch: 450, loss is 4.571501226425171 and perplexity is 96.68915318589232
At time: 111.77436947822571 and batch: 500, loss is 4.536123018264771 and perplexity is 93.3282658221802
At time: 112.34894132614136 and batch: 550, loss is 4.5580103397369385 and perplexity is 95.39349024848043
At time: 112.93702054023743 and batch: 600, loss is 4.63715350151062 and perplexity is 103.25002784337737
At time: 113.53021764755249 and batch: 650, loss is 4.577696018218994 and perplexity is 97.28998144014973
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.971170462814032 and perplexity of 144.1955641825188
Finished 14 epochs...
Completing Train Step...
At time: 114.60898566246033 and batch: 50, loss is 4.622158575057983 and perplexity is 101.71335123905557
At time: 115.2035129070282 and batch: 100, loss is 4.5898276138305665 and perplexity is 98.47745254781515
At time: 115.77250003814697 and batch: 150, loss is 4.565359811782837 and perplexity is 96.09716468884034
At time: 116.3450882434845 and batch: 200, loss is 4.547741537094116 and perplexity is 94.41892569308335
At time: 116.91480422019958 and batch: 250, loss is 4.534406337738037 and perplexity is 93.16818844581906
At time: 117.48869848251343 and batch: 300, loss is 4.589544887542725 and perplexity is 98.44961431870522
At time: 118.06365132331848 and batch: 350, loss is 4.546181287765503 and perplexity is 94.27172349359094
At time: 118.63794541358948 and batch: 400, loss is 4.589063930511474 and perplexity is 98.40227566931542
At time: 119.21489405632019 and batch: 450, loss is 4.566302566528321 and perplexity is 96.1878034652333
At time: 119.7966582775116 and batch: 500, loss is 4.531565618515015 and perplexity is 92.90389934521856
At time: 120.38559699058533 and batch: 550, loss is 4.553124284744262 and perplexity is 94.92852924671654
At time: 120.9763081073761 and batch: 600, loss is 4.63205020904541 and perplexity is 102.72445497094145
At time: 121.55935549736023 and batch: 650, loss is 4.571854019165039 and perplexity is 96.7232704349654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.970996931487439 and perplexity of 144.170543905941
Finished 15 epochs...
Completing Train Step...
At time: 122.66985535621643 and batch: 50, loss is 4.614591903686524 and perplexity is 100.94662418019485
At time: 123.25303649902344 and batch: 100, loss is 4.5827732753753665 and perplexity is 97.78520381649521
At time: 123.84098553657532 and batch: 150, loss is 4.559272966384888 and perplexity is 95.51401268271594
At time: 124.41616678237915 and batch: 200, loss is 4.542394275665283 and perplexity is 93.91539048058937
At time: 124.99244928359985 and batch: 250, loss is 4.529193811416626 and perplexity is 92.6838103248093
At time: 125.567875623703 and batch: 300, loss is 4.583835287094116 and perplexity is 97.88910801283986
At time: 126.16721439361572 and batch: 350, loss is 4.540493040084839 and perplexity is 93.73700482896447
At time: 126.74214005470276 and batch: 400, loss is 4.583766975402832 and perplexity is 97.88242127070714
At time: 127.31627154350281 and batch: 450, loss is 4.5614072132110595 and perplexity is 95.71808084959667
At time: 127.89030933380127 and batch: 500, loss is 4.526361465454102 and perplexity is 92.42166912137944
At time: 128.4658808708191 and batch: 550, loss is 4.548249263763427 and perplexity is 94.46687687176147
At time: 129.04128122329712 and batch: 600, loss is 4.627224388122559 and perplexity is 102.22991937671404
At time: 129.61688899993896 and batch: 650, loss is 4.56608362197876 and perplexity is 96.16674597522584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.970399145986519 and perplexity of 144.08438659949434
Finished 16 epochs...
Completing Train Step...
At time: 130.6997368335724 and batch: 50, loss is 4.607585763931274 and perplexity is 100.24184978021582
At time: 131.2896945476532 and batch: 100, loss is 4.57598165512085 and perplexity is 97.12333397409802
At time: 131.86499643325806 and batch: 150, loss is 4.553561515808106 and perplexity is 94.97004402366184
At time: 132.4456124305725 and batch: 200, loss is 4.5368320274353025 and perplexity is 93.39445988184265
At time: 133.03166127204895 and batch: 250, loss is 4.524205131530762 and perplexity is 92.22259185666965
At time: 133.62288618087769 and batch: 300, loss is 4.577841320037842 and perplexity is 97.30411887848155
At time: 134.19273114204407 and batch: 350, loss is 4.535188426971436 and perplexity is 93.24108278412078
At time: 134.78814840316772 and batch: 400, loss is 4.578523063659668 and perplexity is 97.37047795827394
At time: 135.35752987861633 and batch: 450, loss is 4.556775941848755 and perplexity is 95.27580937302618
At time: 135.92663407325745 and batch: 500, loss is 4.521187219619751 and perplexity is 91.94469174818622
At time: 136.49560165405273 and batch: 550, loss is 4.542915840148925 and perplexity is 93.96438618883136
At time: 137.07065200805664 and batch: 600, loss is 4.622267866134644 and perplexity is 101.72446820820495
At time: 137.66148352622986 and batch: 650, loss is 4.559926605224609 and perplexity is 95.57646475947098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.969602996227788 and perplexity of 143.9697195020141
Finished 17 epochs...
Completing Train Step...
At time: 138.75845885276794 and batch: 50, loss is 4.600542106628418 and perplexity is 99.5382613697141
At time: 139.33981776237488 and batch: 100, loss is 4.569580059051514 and perplexity is 96.50357545942062
At time: 139.91842675209045 and batch: 150, loss is 4.5476234340667725 and perplexity is 94.40777519058749
At time: 140.48515367507935 and batch: 200, loss is 4.531927118301391 and perplexity is 92.93749015615509
At time: 141.07423543930054 and batch: 250, loss is 4.519602031707763 and perplexity is 91.7990575935024
At time: 141.64026856422424 and batch: 300, loss is 4.572341604232788 and perplexity is 96.77044275665857
At time: 142.2071397304535 and batch: 350, loss is 4.530190153121948 and perplexity is 92.77620108918698
At time: 142.77312779426575 and batch: 400, loss is 4.573199615478516 and perplexity is 96.85350851537945
At time: 143.33969831466675 and batch: 450, loss is 4.552002716064453 and perplexity is 94.82212006525012
At time: 143.90732884407043 and batch: 500, loss is 4.516514253616333 and perplexity is 91.51603964782385
At time: 144.47689485549927 and batch: 550, loss is 4.5382115840911865 and perplexity is 93.52339174455167
At time: 145.06388783454895 and batch: 600, loss is 4.617575607299805 and perplexity is 101.24826877275795
At time: 145.6315574645996 and batch: 650, loss is 4.554470539093018 and perplexity is 95.0564132549222
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.969340305702359 and perplexity of 143.931904987727
Finished 18 epochs...
Completing Train Step...
At time: 146.67393684387207 and batch: 50, loss is 4.593912878036499 and perplexity is 98.88058184397735
At time: 147.2671799659729 and batch: 100, loss is 4.563333740234375 and perplexity is 95.90266206223089
At time: 147.836341381073 and batch: 150, loss is 4.542445087432862 and perplexity is 93.9201626088216
At time: 148.4054036140442 and batch: 200, loss is 4.527049789428711 and perplexity is 92.4853070712598
At time: 148.98163866996765 and batch: 250, loss is 4.515006771087647 and perplexity is 91.37818474999146
At time: 149.55192947387695 and batch: 300, loss is 4.56736985206604 and perplexity is 96.29051811995934
At time: 150.1378951072693 and batch: 350, loss is 4.525380382537842 and perplexity is 92.33104026519285
At time: 150.72393035888672 and batch: 400, loss is 4.56824499130249 and perplexity is 96.37482261416902
At time: 151.30210638046265 and batch: 450, loss is 4.547568874359131 and perplexity is 94.40262447048616
At time: 151.87967491149902 and batch: 500, loss is 4.5119040584564205 and perplexity is 91.09510388832919
At time: 152.45023608207703 and batch: 550, loss is 4.533500108718872 and perplexity is 93.08379497546802
At time: 153.02839374542236 and batch: 600, loss is 4.612823505401611 and perplexity is 100.76826809190725
At time: 153.61896181106567 and batch: 650, loss is 4.54938645362854 and perplexity is 94.57436475220942
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.96918472589231 and perplexity of 143.9095138311401
Finished 19 epochs...
Completing Train Step...
At time: 154.81533885002136 and batch: 50, loss is 4.587487363815308 and perplexity is 98.24726014693158
At time: 155.38312101364136 and batch: 100, loss is 4.557391557693482 and perplexity is 95.3344807285505
At time: 155.9792263507843 and batch: 150, loss is 4.537022371292114 and perplexity is 93.41223863552597
At time: 156.55211639404297 and batch: 200, loss is 4.52224271774292 and perplexity is 92.04179043248665
At time: 157.1273455619812 and batch: 250, loss is 4.510624561309815 and perplexity is 90.97862249752899
At time: 157.6963667869568 and batch: 300, loss is 4.562606382369995 and perplexity is 95.83293186924128
At time: 158.26687359809875 and batch: 350, loss is 4.520736541748047 and perplexity is 91.90326364626024
At time: 158.8349847793579 and batch: 400, loss is 4.563289089202881 and perplexity is 95.89838000504659
At time: 159.4049096107483 and batch: 450, loss is 4.543324785232544 and perplexity is 94.00282032078755
At time: 159.9738199710846 and batch: 500, loss is 4.507423715591431 and perplexity is 90.68787952317739
At time: 160.54822874069214 and batch: 550, loss is 4.529051189422607 and perplexity is 92.67059251756498
At time: 161.12633323669434 and batch: 600, loss is 4.608453845977783 and perplexity is 100.32890571069713
At time: 161.69655680656433 and batch: 650, loss is 4.544594182968139 and perplexity is 94.12222305679018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.969100353764553 and perplexity of 143.8973723914604
Finished 20 epochs...
Completing Train Step...
At time: 162.79073810577393 and batch: 50, loss is 4.581599311828613 and perplexity is 97.67047490875706
At time: 163.38404941558838 and batch: 100, loss is 4.551764993667603 and perplexity is 94.79958140267239
At time: 163.9546675682068 and batch: 150, loss is 4.531931791305542 and perplexity is 92.93792445444709
At time: 164.52554178237915 and batch: 200, loss is 4.5176883316040035 and perplexity is 91.62354971574537
At time: 165.09689021110535 and batch: 250, loss is 4.506003208160401 and perplexity is 90.5591481699789
At time: 165.6683225631714 and batch: 300, loss is 4.557649145126343 and perplexity is 95.35904085575876
At time: 166.2403438091278 and batch: 350, loss is 4.515732908248902 and perplexity is 91.44456194222285
At time: 166.81092166900635 and batch: 400, loss is 4.558735628128051 and perplexity is 95.46270313616328
At time: 167.38041281700134 and batch: 450, loss is 4.538812961578369 and perplexity is 93.579651521858
At time: 167.9487326145172 and batch: 500, loss is 4.502761259078979 and perplexity is 90.26603540780356
At time: 168.51832604408264 and batch: 550, loss is 4.524649438858032 and perplexity is 92.26357613410458
At time: 169.08740091323853 and batch: 600, loss is 4.603946008682251 and perplexity is 99.87765716935306
At time: 169.6562955379486 and batch: 650, loss is 4.53974928855896 and perplexity is 93.66731370824289
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.968959135167739 and perplexity of 143.87705284122907
Finished 21 epochs...
Completing Train Step...
At time: 170.70632362365723 and batch: 50, loss is 4.575768594741821 and perplexity is 97.10264304403596
At time: 171.2749321460724 and batch: 100, loss is 4.546102857589721 and perplexity is 94.264330035685
At time: 171.84510684013367 and batch: 150, loss is 4.526802825927734 and perplexity is 92.46246939618875
At time: 172.4128382205963 and batch: 200, loss is 4.513359041213989 and perplexity is 91.22774216358805
At time: 172.98184514045715 and batch: 250, loss is 4.501834058761597 and perplexity is 90.18237950000999
At time: 173.55032205581665 and batch: 300, loss is 4.553199510574341 and perplexity is 94.93567059273076
At time: 174.1188006401062 and batch: 350, loss is 4.51162956237793 and perplexity is 91.07010207114953
At time: 174.68680334091187 and batch: 400, loss is 4.554211568832398 and perplexity is 95.03179965804034
At time: 175.25570940971375 and batch: 450, loss is 4.534521741867065 and perplexity is 93.17894105989586
At time: 175.82482075691223 and batch: 500, loss is 4.49850474357605 and perplexity is 89.88263318632295
At time: 176.39270305633545 and batch: 550, loss is 4.520290536880493 and perplexity is 91.86228348268547
At time: 176.96063709259033 and batch: 600, loss is 4.599379367828369 and perplexity is 99.4225916310085
At time: 177.52791285514832 and batch: 650, loss is 4.5349038887023925 and perplexity is 93.2145559019572
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.968764062021293 and perplexity of 143.848989029167
Finished 22 epochs...
Completing Train Step...
At time: 178.55953335762024 and batch: 50, loss is 4.570204133987427 and perplexity is 96.56381971860162
At time: 179.15139484405518 and batch: 100, loss is 4.540576696395874 and perplexity is 93.7448468490086
At time: 179.72209215164185 and batch: 150, loss is 4.522167387008667 and perplexity is 92.0348571179806
At time: 180.29125833511353 and batch: 200, loss is 4.5094115257263185 and perplexity is 90.86832909953453
At time: 180.860169172287 and batch: 250, loss is 4.497863187789917 and perplexity is 89.82498695654272
At time: 181.42941999435425 and batch: 300, loss is 4.5485196304321285 and perplexity is 94.49242101955136
At time: 181.99708008766174 and batch: 350, loss is 4.507186975479126 and perplexity is 90.66641260553489
At time: 182.56626057624817 and batch: 400, loss is 4.549789896011353 and perplexity is 94.6125277570472
At time: 183.1349925994873 and batch: 450, loss is 4.530627317428589 and perplexity is 92.81676839945287
At time: 183.70341658592224 and batch: 500, loss is 4.494193210601806 and perplexity is 89.49593547852437
At time: 184.2788119316101 and batch: 550, loss is 4.5164167594909665 and perplexity is 91.50711780650198
At time: 184.86000180244446 and batch: 600, loss is 4.595150718688965 and perplexity is 99.00305603405053
At time: 185.4284873008728 and batch: 650, loss is 4.530663146972656 and perplexity is 92.82009404152414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.969085992551317 and perplexity of 143.8953058654504
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 186.46645951271057 and batch: 50, loss is 4.564158964157104 and perplexity is 95.98183589679614
At time: 187.0365662574768 and batch: 100, loss is 4.529650287628174 and perplexity is 92.72612793717511
At time: 187.60551977157593 and batch: 150, loss is 4.506942262649536 and perplexity is 90.64422808568648
At time: 188.17411708831787 and batch: 200, loss is 4.489657773971557 and perplexity is 89.09095141848314
At time: 188.7442328929901 and batch: 250, loss is 4.473071689605713 and perplexity is 87.6254682912416
At time: 189.3132185935974 and batch: 300, loss is 4.52017053604126 and perplexity is 91.85126059296489
At time: 189.8823504447937 and batch: 350, loss is 4.47642749786377 and perplexity is 87.92001650859963
At time: 190.45160293579102 and batch: 400, loss is 4.516701650619507 and perplexity is 91.53319108641008
At time: 191.02156233787537 and batch: 450, loss is 4.493780174255371 and perplexity is 89.4589780372222
At time: 191.58943676948547 and batch: 500, loss is 4.448386001586914 and perplexity is 85.48885371918942
At time: 192.15804815292358 and batch: 550, loss is 4.471971597671509 and perplexity is 87.5291252231758
At time: 192.72920083999634 and batch: 600, loss is 4.550762424468994 and perplexity is 94.70458589006155
At time: 193.29905533790588 and batch: 650, loss is 4.50022744178772 and perplexity is 90.03760728624935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.952031154258578 and perplexity of 141.46200346276
Finished 24 epochs...
Completing Train Step...
At time: 194.3325057029724 and batch: 50, loss is 4.55399073600769 and perplexity is 95.01081583432862
At time: 194.91553926467896 and batch: 100, loss is 4.520668363571167 and perplexity is 91.89699806288597
At time: 195.4846601486206 and batch: 150, loss is 4.4992067337036135 and perplexity is 89.94575205927995
At time: 196.05413150787354 and batch: 200, loss is 4.481674547195435 and perplexity is 88.38254957819156
At time: 196.6245014667511 and batch: 250, loss is 4.4664998054504395 and perplexity is 87.05149198330051
At time: 197.19449090957642 and batch: 300, loss is 4.514918365478516 and perplexity is 91.3701067629823
At time: 197.7638828754425 and batch: 350, loss is 4.4716221046447755 and perplexity is 87.49853974929049
At time: 198.33370399475098 and batch: 400, loss is 4.512733421325684 and perplexity is 91.17068612328306
At time: 198.90260362625122 and batch: 450, loss is 4.491929063796997 and perplexity is 89.29353276336342
At time: 199.4852316379547 and batch: 500, loss is 4.4490014171600345 and perplexity is 85.54148108328516
At time: 200.05389642715454 and batch: 550, loss is 4.4741831874847415 and perplexity is 87.7229179609141
At time: 200.62411260604858 and batch: 600, loss is 4.55367148399353 and perplexity is 94.98048828133031
At time: 201.20138335227966 and batch: 650, loss is 4.500526609420777 and perplexity is 90.06454765374934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.950613283643536 and perplexity of 141.26157077229465
Finished 25 epochs...
Completing Train Step...
At time: 202.26331448554993 and batch: 50, loss is 4.55121636390686 and perplexity is 94.74758579548192
At time: 202.83155369758606 and batch: 100, loss is 4.516979742050171 and perplexity is 91.55864922215315
At time: 203.39990830421448 and batch: 150, loss is 4.495982961654663 and perplexity is 89.65625434590974
At time: 203.97005772590637 and batch: 200, loss is 4.477804050445557 and perplexity is 88.04112637223881
At time: 204.53954124450684 and batch: 250, loss is 4.463385257720947 and perplexity is 86.78078773607272
At time: 205.10703706741333 and batch: 300, loss is 4.512285995483398 and perplexity is 91.1299031266163
At time: 205.6752667427063 and batch: 350, loss is 4.469062452316284 and perplexity is 87.27486030121727
At time: 206.24428462982178 and batch: 400, loss is 4.510844087600708 and perplexity is 90.9985968894483
At time: 206.81298446655273 and batch: 450, loss is 4.491027507781983 and perplexity is 89.21306591993239
At time: 207.3806438446045 and batch: 500, loss is 4.449417400360107 and perplexity is 85.57707230448698
At time: 207.95054411888123 and batch: 550, loss is 4.475316200256348 and perplexity is 87.82236547433925
At time: 208.51981353759766 and batch: 600, loss is 4.554857769012451 and perplexity is 95.09322906980104
At time: 209.08825659751892 and batch: 650, loss is 4.500371208190918 and perplexity is 90.05055259972976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.949902104396446 and perplexity of 141.16114418963357
Finished 26 epochs...
Completing Train Step...
At time: 210.1096248626709 and batch: 50, loss is 4.549274206161499 and perplexity is 94.56374961509123
At time: 210.69019889831543 and batch: 100, loss is 4.5146294116973875 and perplexity is 91.34370883922466
At time: 211.25843214988708 and batch: 150, loss is 4.493533334732056 and perplexity is 89.43689875085956
At time: 211.8263077735901 and batch: 200, loss is 4.475055265426636 and perplexity is 87.7994525498793
At time: 212.3944125175476 and batch: 250, loss is 4.460991592407226 and perplexity is 86.57331198738636
At time: 212.96302461624146 and batch: 300, loss is 4.510381441116333 and perplexity is 90.95650644576305
At time: 213.53192114830017 and batch: 350, loss is 4.4671852493286135 and perplexity is 87.11118135008489
At time: 214.1129069328308 and batch: 400, loss is 4.509390840530395 and perplexity is 90.86644948978402
At time: 214.68116974830627 and batch: 450, loss is 4.490259923934937 and perplexity is 89.1446136863621
At time: 215.24928259849548 and batch: 500, loss is 4.449509773254395 and perplexity is 85.58497767145558
At time: 215.8172881603241 and batch: 550, loss is 4.47585657119751 and perplexity is 87.86983495303406
At time: 216.38637948036194 and batch: 600, loss is 4.555348949432373 and perplexity is 95.13994847487649
At time: 216.95849227905273 and batch: 650, loss is 4.500094194412231 and perplexity is 90.02561081065035
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9495891496246935 and perplexity of 141.1169740479622
Finished 27 epochs...
Completing Train Step...
At time: 218.01753520965576 and batch: 50, loss is 4.547861967086792 and perplexity is 94.43029724833747
At time: 218.58568954467773 and batch: 100, loss is 4.512756938934326 and perplexity is 91.17283026501144
At time: 219.15418457984924 and batch: 150, loss is 4.491417875289917 and perplexity is 89.24789860048178
At time: 219.72181105613708 and batch: 200, loss is 4.472982473373413 and perplexity is 87.61765102582584
At time: 220.28947925567627 and batch: 250, loss is 4.459040765762329 and perplexity is 86.40458709364358
At time: 220.85722851753235 and batch: 300, loss is 4.508677244186401 and perplexity is 90.80163065362923
At time: 221.42641830444336 and batch: 350, loss is 4.4656014823913575 and perplexity is 86.97332673480349
At time: 221.99517607688904 and batch: 400, loss is 4.508216791152954 and perplexity is 90.75983039162003
At time: 222.56332755088806 and batch: 450, loss is 4.4894888210296635 and perplexity is 89.07590051162818
At time: 223.13114857673645 and batch: 500, loss is 4.449361047744751 and perplexity is 85.57224994852554
At time: 223.6996488571167 and batch: 550, loss is 4.476020193099975 and perplexity is 87.88421355889372
At time: 224.26794600486755 and batch: 600, loss is 4.5555530834198 and perplexity is 95.15937175433052
At time: 224.83771800994873 and batch: 650, loss is 4.499751615524292 and perplexity is 89.9947752191257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.949424893248315 and perplexity of 141.0937965887344
Finished 28 epochs...
Completing Train Step...
At time: 225.86366868019104 and batch: 50, loss is 4.546723127365112 and perplexity is 94.32281748762779
At time: 226.4446439743042 and batch: 100, loss is 4.511190929412842 and perplexity is 91.03016448185912
At time: 227.01268935203552 and batch: 150, loss is 4.489622097015381 and perplexity is 89.08777298121254
At time: 227.58176803588867 and batch: 200, loss is 4.471129026412964 and perplexity is 87.45540675886801
At time: 228.15092015266418 and batch: 250, loss is 4.457441949844361 and perplexity is 86.26655243981749
At time: 228.7319552898407 and batch: 300, loss is 4.507236633300781 and perplexity is 90.67091501387118
At time: 229.30067801475525 and batch: 350, loss is 4.464218225479126 and perplexity is 86.85310344843334
At time: 229.86960077285767 and batch: 400, loss is 4.507032566070556 and perplexity is 90.65241393917806
At time: 230.43772268295288 and batch: 450, loss is 4.48877779006958 and perplexity is 89.01258730006133
At time: 231.00664567947388 and batch: 500, loss is 4.449160261154175 and perplexity is 85.55506991302853
At time: 231.57434248924255 and batch: 550, loss is 4.475961694717407 and perplexity is 87.87907262491687
At time: 232.1436002254486 and batch: 600, loss is 4.555572776794434 and perplexity is 95.16124578194133
At time: 232.71187043190002 and batch: 650, loss is 4.499457159042358 and perplexity is 89.96827957532076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.949201995251226 and perplexity of 141.0623505688307
Finished 29 epochs...
Completing Train Step...
At time: 233.74362421035767 and batch: 50, loss is 4.545752086639404 and perplexity is 94.23127064553321
At time: 234.31216835975647 and batch: 100, loss is 4.509662408828735 and perplexity is 90.89112928782181
At time: 234.88146471977234 and batch: 150, loss is 4.4880288791656495 and perplexity is 88.94594975875044
At time: 235.44971466064453 and batch: 200, loss is 4.469450445175171 and perplexity is 87.3087288937356
At time: 236.01770782470703 and batch: 250, loss is 4.455830020904541 and perplexity is 86.12760890104177
At time: 236.58576726913452 and batch: 300, loss is 4.505725107192993 and perplexity is 90.53396708486957
At time: 237.15317702293396 and batch: 350, loss is 4.462741632461547 and perplexity is 86.7249513998209
At time: 237.72164106369019 and batch: 400, loss is 4.505739192962647 and perplexity is 90.5352423344572
At time: 238.29044246673584 and batch: 450, loss is 4.487920579910278 and perplexity is 88.93631750021588
At time: 238.85862731933594 and batch: 500, loss is 4.448816328048706 and perplexity is 85.5256497517185
At time: 239.4265913963318 and batch: 550, loss is 4.4757306766510006 and perplexity is 87.85877331632527
At time: 239.99441051483154 and batch: 600, loss is 4.555452098846436 and perplexity is 95.14976261096811
At time: 240.56416511535645 and batch: 650, loss is 4.499100522994995 and perplexity is 89.93619936452475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9490990732230395 and perplexity of 141.0478328927166
Finished 30 epochs...
Completing Train Step...
At time: 241.58514046669006 and batch: 50, loss is 4.544734992980957 and perplexity is 94.13547734137133
At time: 242.1665496826172 and batch: 100, loss is 4.5083932781219485 and perplexity is 90.7758497325532
At time: 242.73506236076355 and batch: 150, loss is 4.486596097946167 and perplexity is 88.81860092568107
At time: 243.326997756958 and batch: 200, loss is 4.468028888702393 and perplexity is 87.18470278099463
At time: 243.89652132987976 and batch: 250, loss is 4.4544547653198245 and perplexity is 86.00924283635385
At time: 244.465270280838 and batch: 300, loss is 4.505033922195435 and perplexity is 90.47141298576885
At time: 245.03492093086243 and batch: 350, loss is 4.462019338607788 and perplexity is 86.66233311757978
At time: 245.60361170768738 and batch: 400, loss is 4.50485315322876 and perplexity is 90.45506004002691
At time: 246.17287492752075 and batch: 450, loss is 4.487334365844727 and perplexity is 88.88419705831959
At time: 246.7427098751068 and batch: 500, loss is 4.44885287284851 and perplexity is 85.5287753265782
At time: 247.3116900920868 and batch: 550, loss is 4.475679140090943 and perplexity is 87.85424549405285
At time: 247.88065910339355 and batch: 600, loss is 4.555417490005493 and perplexity is 95.1464696449514
At time: 248.45105528831482 and batch: 650, loss is 4.499072322845459 and perplexity is 89.93366318601441
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.949067658069087 and perplexity of 141.0434019229317
Finished 31 epochs...
Completing Train Step...
At time: 249.48326063156128 and batch: 50, loss is 4.544290170669556 and perplexity is 94.09361309252176
At time: 250.05200457572937 and batch: 100, loss is 4.50743688583374 and perplexity is 90.6890739123904
At time: 250.62064576148987 and batch: 150, loss is 4.48546576499939 and perplexity is 88.71826305305278
At time: 251.18997931480408 and batch: 200, loss is 4.467196722030639 and perplexity is 87.11218075644457
At time: 251.75862836837769 and batch: 250, loss is 4.453515214920044 and perplexity is 85.92847076851825
At time: 252.32785272598267 and batch: 300, loss is 4.50378493309021 and perplexity is 90.35848571376185
At time: 252.89779925346375 and batch: 350, loss is 4.460675230026245 and perplexity is 86.54592778017151
At time: 253.4656639099121 and batch: 400, loss is 4.503756551742554 and perplexity is 90.3559212545567
At time: 254.03571796417236 and batch: 450, loss is 4.486547889709473 and perplexity is 88.81431924075186
At time: 254.60470986366272 and batch: 500, loss is 4.4484188270568845 and perplexity is 85.49165997704812
At time: 255.1741428375244 and batch: 550, loss is 4.475309925079346 and perplexity is 87.8218143751803
At time: 255.74379682540894 and batch: 600, loss is 4.55510100364685 and perplexity is 95.11636184984027
At time: 256.31355142593384 and batch: 650, loss is 4.498611650466919 and perplexity is 89.89224277283635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.949137070599725 and perplexity of 141.0531924421774
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 257.3444902896881 and batch: 50, loss is 4.543647661209106 and perplexity is 94.03317647357073
At time: 257.9262647628784 and batch: 100, loss is 4.505166292190552 and perplexity is 90.4833894789107
At time: 258.49547362327576 and batch: 150, loss is 4.4826537704467775 and perplexity is 88.46913821363505
At time: 259.0642144680023 and batch: 200, loss is 4.463049068450927 and perplexity is 86.75161786996472
At time: 259.63430285453796 and batch: 250, loss is 4.448664054870606 and perplexity is 85.51262748071817
At time: 260.2024042606354 and batch: 300, loss is 4.496547260284424 and perplexity is 89.70686152482456
At time: 260.77090287208557 and batch: 350, loss is 4.452293996810913 and perplexity is 85.8235974135797
At time: 261.341078042984 and batch: 400, loss is 4.495309219360352 and perplexity is 89.59586947961076
At time: 261.91003489494324 and batch: 450, loss is 4.477447357177734 and perplexity is 88.00972829524454
At time: 262.47905945777893 and batch: 500, loss is 4.436397552490234 and perplexity is 84.47009382517464
At time: 263.0470163822174 and batch: 550, loss is 4.463376979827881 and perplexity is 86.78006937696492
At time: 263.6160509586334 and batch: 600, loss is 4.544560832977295 and perplexity is 94.11908413385478
At time: 264.18373131752014 and batch: 650, loss is 4.491494798660279 and perplexity is 89.25476411369563
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.946397968367035 and perplexity of 140.66736198359646
Finished 33 epochs...
Completing Train Step...
At time: 265.21545696258545 and batch: 50, loss is 4.540287580490112 and perplexity is 93.71774764029624
At time: 265.7842495441437 and batch: 100, loss is 4.503408908843994 and perplexity is 90.32451511956593
At time: 266.35290598869324 and batch: 150, loss is 4.481023445129394 and perplexity is 88.32502224767347
At time: 266.9202170372009 and batch: 200, loss is 4.461842393875122 and perplexity is 86.64700003080854
At time: 267.48906230926514 and batch: 250, loss is 4.447504339218139 and perplexity is 85.41351463061545
At time: 268.0579106807709 and batch: 300, loss is 4.4956160736083985 and perplexity is 89.62336657135195
At time: 268.62589931488037 and batch: 350, loss is 4.451837196350097 and perplexity is 85.7844021076308
At time: 269.19290566444397 and batch: 400, loss is 4.495074043273926 and perplexity is 89.5748011511472
At time: 269.7607183456421 and batch: 450, loss is 4.477418403625489 and perplexity is 88.00718013786752
At time: 270.3290104866028 and batch: 500, loss is 4.4365012741088865 and perplexity is 84.47885565442158
At time: 270.8969416618347 and batch: 550, loss is 4.46405514717102 and perplexity is 86.83894074611541
At time: 271.46518635749817 and batch: 600, loss is 4.54508111000061 and perplexity is 94.16806487145641
At time: 272.0458707809448 and batch: 650, loss is 4.491361150741577 and perplexity is 89.24283619732597
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.945938708735447 and perplexity of 140.6027739752232
Finished 34 epochs...
Completing Train Step...
At time: 273.0659348964691 and batch: 50, loss is 4.538759212493897 and perplexity is 93.57462183643518
At time: 273.64766573905945 and batch: 100, loss is 4.502444696426392 and perplexity is 90.23746507458513
At time: 274.21525049209595 and batch: 150, loss is 4.480194911956787 and perplexity is 88.25187234451529
At time: 274.78462171554565 and batch: 200, loss is 4.461205949783325 and perplexity is 86.59187160450853
At time: 275.3543128967285 and batch: 250, loss is 4.4468533897399904 and perplexity is 85.35793284027118
At time: 275.9232840538025 and batch: 300, loss is 4.495034675598145 and perplexity is 89.5712748688286
At time: 276.4914779663086 and batch: 350, loss is 4.451643295288086 and perplexity is 85.76777003349868
At time: 277.0605490207672 and batch: 400, loss is 4.495016412734985 and perplexity is 89.56963905583001
At time: 277.62822461128235 and batch: 450, loss is 4.477497091293335 and perplexity is 88.01410549009265
At time: 278.1965675354004 and batch: 500, loss is 4.436609802246093 and perplexity is 84.48802448478817
At time: 278.76423478126526 and batch: 550, loss is 4.464452209472657 and perplexity is 86.87342806215317
At time: 279.3312520980835 and batch: 600, loss is 4.545386724472046 and perplexity is 94.19684839293453
At time: 279.89852476119995 and batch: 650, loss is 4.4911701869964595 and perplexity is 89.22579567821329
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.945673026290595 and perplexity of 140.56542324841632
Finished 35 epochs...
Completing Train Step...
At time: 280.93135619163513 and batch: 50, loss is 4.537555541992187 and perplexity is 93.46205658374973
At time: 281.49815821647644 and batch: 100, loss is 4.501704893112183 and perplexity is 90.17073178665456
At time: 282.066055059433 and batch: 150, loss is 4.479596366882324 and perplexity is 88.1990654262483
At time: 282.6333553791046 and batch: 200, loss is 4.460739870071411 and perplexity is 86.55152229366496
At time: 283.2017436027527 and batch: 250, loss is 4.446377191543579 and perplexity is 85.31729522315116
At time: 283.7695109844208 and batch: 300, loss is 4.49460000038147 and perplexity is 89.53234891620349
At time: 284.33852648735046 and batch: 350, loss is 4.451504650115967 and perplexity is 85.7558795705568
At time: 284.91746711730957 and batch: 400, loss is 4.495003442764283 and perplexity is 89.56847734776926
At time: 285.49407720565796 and batch: 450, loss is 4.477556371688843 and perplexity is 88.01932315572749
At time: 286.06221532821655 and batch: 500, loss is 4.436711959838867 and perplexity is 84.49665601886859
At time: 286.64351892471313 and batch: 550, loss is 4.46474292755127 and perplexity is 86.89868740973853
At time: 287.211980342865 and batch: 600, loss is 4.545590286254883 and perplexity is 94.21602522309968
At time: 287.780996799469 and batch: 650, loss is 4.491010913848877 and perplexity is 89.21158553656707
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.945501589307598 and perplexity of 140.5413272018768
Finished 36 epochs...
Completing Train Step...
At time: 288.79912853240967 and batch: 50, loss is 4.536544961929321 and perplexity is 93.36765340175279
At time: 289.380508184433 and batch: 100, loss is 4.50108850479126 and perplexity is 90.115168726662
At time: 289.9497928619385 and batch: 150, loss is 4.479069290161132 and perplexity is 88.15259000116451
At time: 290.5185363292694 and batch: 200, loss is 4.460363531112671 and perplexity is 86.51895571230868
At time: 291.0863790512085 and batch: 250, loss is 4.4459780502319335 and perplexity is 85.2832483612363
At time: 291.65503096580505 and batch: 300, loss is 4.494229173660278 and perplexity is 89.49915408396005
At time: 292.2246034145355 and batch: 350, loss is 4.451388120651245 and perplexity is 85.74588706603558
At time: 292.7929298877716 and batch: 400, loss is 4.494981632232666 and perplexity is 89.56652383296593
At time: 293.36246275901794 and batch: 450, loss is 4.477574501037598 and perplexity is 88.02091890319902
At time: 293.93164253234863 and batch: 500, loss is 4.436772699356079 and perplexity is 84.50178846083068
At time: 294.50100088119507 and batch: 550, loss is 4.464938526153564 and perplexity is 86.91568633396581
At time: 295.0686502456665 and batch: 600, loss is 4.545728130340576 and perplexity is 94.22901324009432
At time: 295.63727259635925 and batch: 650, loss is 4.490843935012817 and perplexity is 89.1966903334776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.945377125459559 and perplexity of 140.52383597601988
Finished 37 epochs...
Completing Train Step...
At time: 296.66930198669434 and batch: 50, loss is 4.5356518840789795 and perplexity is 93.2843060419415
At time: 297.2391119003296 and batch: 100, loss is 4.500518569946289 and perplexity is 90.06382358502678
At time: 297.8084328174591 and batch: 150, loss is 4.478598442077637 and perplexity is 88.11109329319662
At time: 298.3772768974304 and batch: 200, loss is 4.460034027099609 and perplexity is 86.49045206548642
At time: 298.9466292858124 and batch: 250, loss is 4.445633754730225 and perplexity is 85.2538907765866
At time: 299.5157594680786 and batch: 300, loss is 4.493902931213379 and perplexity is 89.46996042330086
At time: 300.08532094955444 and batch: 350, loss is 4.451285638809204 and perplexity is 85.73710011984052
At time: 300.6552722454071 and batch: 400, loss is 4.494945106506347 and perplexity is 89.56325241037496
At time: 301.2454254627228 and batch: 450, loss is 4.477571907043457 and perplexity is 88.02069057774729
At time: 301.81232619285583 and batch: 500, loss is 4.43682017326355 and perplexity is 84.50580018614258
At time: 302.38094425201416 and batch: 550, loss is 4.465085000991821 and perplexity is 86.92841822749182
At time: 302.9490704536438 and batch: 600, loss is 4.545827198028564 and perplexity is 94.23834875299363
At time: 303.5220558643341 and batch: 650, loss is 4.49067946434021 and perplexity is 89.18202130017006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.94528138403799 and perplexity of 140.51038266822897
Finished 38 epochs...
Completing Train Step...
At time: 304.5469355583191 and batch: 50, loss is 4.5348549079895015 and perplexity is 93.20999029837148
At time: 305.12939453125 and batch: 100, loss is 4.499993762969971 and perplexity is 90.0165698627216
At time: 305.69895935058594 and batch: 150, loss is 4.478182945251465 and perplexity is 88.07449101817392
At time: 306.2669861316681 and batch: 200, loss is 4.459740819931031 and perplexity is 86.46509616237516
At time: 306.83595967292786 and batch: 250, loss is 4.445330190658569 and perplexity is 85.22801468610237
At time: 307.4049427509308 and batch: 300, loss is 4.493587045669556 and perplexity is 89.44170261954757
At time: 307.9746890068054 and batch: 350, loss is 4.4511736869812015 and perplexity is 85.72750223201503
At time: 308.54424715042114 and batch: 400, loss is 4.494883432388305 and perplexity is 89.5577288461058
At time: 309.1136841773987 and batch: 450, loss is 4.4775371074676515 and perplexity is 88.01762754834941
At time: 309.6831338405609 and batch: 500, loss is 4.436852569580078 and perplexity is 84.50853790713967
At time: 310.25243401527405 and batch: 550, loss is 4.465187463760376 and perplexity is 86.93732561021928
At time: 310.8196575641632 and batch: 600, loss is 4.545895185470581 and perplexity is 94.24475599506874
At time: 311.3880479335785 and batch: 650, loss is 4.4905266761779785 and perplexity is 89.16839638392099
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.945204491708793 and perplexity of 140.4995789129975
Finished 39 epochs...
Completing Train Step...
At time: 312.42409086227417 and batch: 50, loss is 4.53412636756897 and perplexity is 93.14210778342223
At time: 312.99233174324036 and batch: 100, loss is 4.499536199569702 and perplexity is 89.97539099662413
At time: 313.563246011734 and batch: 150, loss is 4.4778062915802 and perplexity is 88.04132368447826
At time: 314.13069248199463 and batch: 200, loss is 4.459466609954834 and perplexity is 86.44138982082087
At time: 314.6986303329468 and batch: 250, loss is 4.445048131942749 and perplexity is 85.20397877165705
At time: 315.2658929824829 and batch: 300, loss is 4.4932825469970705 and perplexity is 89.41447188590661
At time: 315.8454313278198 and batch: 350, loss is 4.451066246032715 and perplexity is 85.7182920826463
At time: 316.4140799045563 and batch: 400, loss is 4.494821004867553 and perplexity is 89.55213815363813
At time: 316.9812617301941 and batch: 450, loss is 4.477494478225708 and perplexity is 88.01387550358334
At time: 317.54912781715393 and batch: 500, loss is 4.436872434616089 and perplexity is 84.51021668896288
At time: 318.11898612976074 and batch: 550, loss is 4.465273218154907 and perplexity is 86.94478118760883
At time: 318.68725323677063 and batch: 600, loss is 4.545932855606079 and perplexity is 94.24830627466639
At time: 319.256395816803 and batch: 650, loss is 4.490382242202759 and perplexity is 89.15551836800118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.945129693723192 and perplexity of 140.48907022053643
Finished 40 epochs...
Completing Train Step...
At time: 320.2775459289551 and batch: 50, loss is 4.533444900512695 and perplexity is 93.07865612797784
At time: 320.8583719730377 and batch: 100, loss is 4.4991191959381105 and perplexity is 89.93787875373901
At time: 321.426167011261 and batch: 150, loss is 4.477458515167236 and perplexity is 88.01071031234761
At time: 321.9935133457184 and batch: 200, loss is 4.459214115142823 and perplexity is 86.41956657359235
At time: 322.56114959716797 and batch: 250, loss is 4.444788446426392 and perplexity is 85.18185540511729
At time: 323.1284863948822 and batch: 300, loss is 4.492990674972535 and perplexity is 89.38837811118113
At time: 323.69635462760925 and batch: 350, loss is 4.450958023071289 and perplexity is 85.7090158971859
At time: 324.26425528526306 and batch: 400, loss is 4.49475998878479 and perplexity is 89.54667419966115
At time: 324.83244800567627 and batch: 450, loss is 4.477445268630982 and perplexity is 88.00954448300428
At time: 325.39954710006714 and batch: 500, loss is 4.436884422302246 and perplexity is 84.51122977698986
At time: 325.9667646884918 and batch: 550, loss is 4.465340700149536 and perplexity is 86.9506485928358
At time: 326.53486680984497 and batch: 600, loss is 4.545950527191162 and perplexity is 94.24997180634593
At time: 327.1023380756378 and batch: 650, loss is 4.490246410369873 and perplexity is 89.1434090329646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.94506087957644 and perplexity of 140.47940291766858
Finished 41 epochs...
Completing Train Step...
At time: 328.1356346607208 and batch: 50, loss is 4.532804832458496 and perplexity is 93.01909851615828
At time: 328.70432329177856 and batch: 100, loss is 4.498728923797607 and perplexity is 89.90278535371907
At time: 329.2722315788269 and batch: 150, loss is 4.4771359920501705 and perplexity is 87.98232940071856
At time: 329.8402490615845 and batch: 200, loss is 4.45898021697998 and perplexity is 86.39935555948881
At time: 330.42192125320435 and batch: 250, loss is 4.444538440704346 and perplexity is 85.1605621156845
At time: 330.99075722694397 and batch: 300, loss is 4.49270562171936 and perplexity is 89.36290129450182
At time: 331.56020951271057 and batch: 350, loss is 4.450849647521973 and perplexity is 85.69972763882585
At time: 332.1289141178131 and batch: 400, loss is 4.494694547653198 and perplexity is 89.54081435571074
At time: 332.69719433784485 and batch: 450, loss is 4.47738733291626 and perplexity is 88.00444573484357
At time: 333.2645823955536 and batch: 500, loss is 4.436880073547363 and perplexity is 84.51086225916588
At time: 333.8337233066559 and batch: 550, loss is 4.465392713546753 and perplexity is 86.95517130907922
At time: 334.4023344516754 and batch: 600, loss is 4.545950021743774 and perplexity is 94.24992416795591
At time: 334.9712607860565 and batch: 650, loss is 4.490111923217773 and perplexity is 89.13142119587837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.945013607249541 and perplexity of 140.47276228637156
Finished 42 epochs...
Completing Train Step...
At time: 335.9908061027527 and batch: 50, loss is 4.532204418182373 and perplexity is 92.96326528466902
At time: 336.5709562301636 and batch: 100, loss is 4.498368301391602 and perplexity is 89.87037024011762
At time: 337.1390149593353 and batch: 150, loss is 4.476825695037842 and perplexity is 87.95503298198543
At time: 337.7073276042938 and batch: 200, loss is 4.458753576278687 and perplexity is 86.37977616778086
At time: 338.2766320705414 and batch: 250, loss is 4.444289093017578 and perplexity is 85.13933017369494
At time: 338.84506011009216 and batch: 300, loss is 4.492414817810059 and perplexity is 89.33691799166499
At time: 339.41280126571655 and batch: 350, loss is 4.45073899269104 and perplexity is 85.69024507460843
At time: 339.980917930603 and batch: 400, loss is 4.494603567123413 and perplexity is 89.5326682555571
At time: 340.5488350391388 and batch: 450, loss is 4.477322435379028 and perplexity is 87.99873464836962
At time: 341.1167001724243 and batch: 500, loss is 4.436854076385498 and perplexity is 84.50866524515855
At time: 341.68471455574036 and batch: 550, loss is 4.465420827865601 and perplexity is 86.95761602885655
At time: 342.25370955467224 and batch: 600, loss is 4.545940160751343 and perplexity is 94.24899477474939
At time: 342.8229124546051 and batch: 650, loss is 4.489981641769409 and perplexity is 89.11980978162262
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944968429266238 and perplexity of 140.4664161536163
Finished 43 epochs...
Completing Train Step...
At time: 343.8561887741089 and batch: 50, loss is 4.531631393432617 and perplexity is 92.91001029251426
At time: 344.4249975681305 and batch: 100, loss is 4.498017911911011 and perplexity is 89.83888612394234
At time: 345.00684785842896 and batch: 150, loss is 4.476543035507202 and perplexity is 87.93017516695997
At time: 345.57502794265747 and batch: 200, loss is 4.458522357940674 and perplexity is 86.3598058883338
At time: 346.14443802833557 and batch: 250, loss is 4.444041032791137 and perplexity is 85.11821311143295
At time: 346.713889837265 and batch: 300, loss is 4.492143402099609 and perplexity is 89.3126738388697
At time: 347.28335189819336 and batch: 350, loss is 4.450628671646118 and perplexity is 85.68079215866958
At time: 347.8527476787567 and batch: 400, loss is 4.494532480239868 and perplexity is 89.52630388340971
At time: 348.42085433006287 and batch: 450, loss is 4.47725359916687 and perplexity is 87.99267735728462
At time: 348.9893479347229 and batch: 500, loss is 4.436826143264771 and perplexity is 84.50630468737877
At time: 349.5591154098511 and batch: 550, loss is 4.465450201034546 and perplexity is 86.96017028711637
At time: 350.1282048225403 and batch: 600, loss is 4.545923185348511 and perplexity is 94.24739487367611
At time: 350.6976742744446 and batch: 650, loss is 4.489849290847778 and perplexity is 89.10801547317344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944934022192862 and perplexity of 140.4615831984732
Finished 44 epochs...
Completing Train Step...
At time: 351.71827483177185 and batch: 50, loss is 4.53108696937561 and perplexity is 92.85944161442572
At time: 352.2996118068695 and batch: 100, loss is 4.497675466537475 and perplexity is 89.80812648007512
At time: 352.86782360076904 and batch: 150, loss is 4.4762576389312745 and perplexity is 87.90508377671534
At time: 353.4375374317169 and batch: 200, loss is 4.458298501968383 and perplexity is 86.34047589366794
At time: 354.00685572624207 and batch: 250, loss is 4.443800888061523 and perplexity is 85.09777487532565
At time: 354.5770466327667 and batch: 300, loss is 4.491874942779541 and perplexity is 89.2887002372898
At time: 355.1460337638855 and batch: 350, loss is 4.450509281158447 and perplexity is 85.6705632977362
At time: 355.71464681625366 and batch: 400, loss is 4.494462461471557 and perplexity is 89.52003558133237
At time: 356.28326201438904 and batch: 450, loss is 4.47718487739563 and perplexity is 87.98663055241644
At time: 356.8516523838043 and batch: 500, loss is 4.436791353225708 and perplexity is 84.503364760878
At time: 357.4207272529602 and batch: 550, loss is 4.465468149185181 and perplexity is 86.96173107535853
At time: 357.98945450782776 and batch: 600, loss is 4.545901870727539 and perplexity is 94.24538604758553
At time: 358.55835485458374 and batch: 650, loss is 4.489713115692139 and perplexity is 89.09588200145508
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944900811887255 and perplexity of 140.45691850382755
Finished 45 epochs...
Completing Train Step...
At time: 359.6013026237488 and batch: 50, loss is 4.5305568218231205 and perplexity is 92.81022545579401
At time: 360.16992020606995 and batch: 100, loss is 4.49733678817749 and perplexity is 89.77771556113625
At time: 360.73868012428284 and batch: 150, loss is 4.475982389450073 and perplexity is 87.88089127765001
At time: 361.30730533599854 and batch: 200, loss is 4.458074245452881 and perplexity is 86.32111565030871
At time: 361.8760540485382 and batch: 250, loss is 4.4435697937011716 and perplexity is 85.0781115316049
At time: 362.44442415237427 and batch: 300, loss is 4.491602516174316 and perplexity is 89.26437893283486
At time: 363.0124068260193 and batch: 350, loss is 4.450393085479736 and perplexity is 85.66060932680368
At time: 363.5833840370178 and batch: 400, loss is 4.494390773773193 and perplexity is 89.51361832604594
At time: 364.1518557071686 and batch: 450, loss is 4.477099771499634 and perplexity is 87.97914269002271
At time: 364.72103691101074 and batch: 500, loss is 4.436759662628174 and perplexity is 84.50068684118779
At time: 365.29026460647583 and batch: 550, loss is 4.4654708576202395 and perplexity is 86.96196660587867
At time: 365.859459400177 and batch: 600, loss is 4.545870199203491 and perplexity is 94.24240119984252
At time: 366.428670167923 and batch: 650, loss is 4.489565992355347 and perplexity is 89.08277488220585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944864011278339 and perplexity of 140.45174969880836
Finished 46 epochs...
Completing Train Step...
At time: 367.4531877040863 and batch: 50, loss is 4.530002059936524 and perplexity is 92.75875215905641
At time: 368.03362345695496 and batch: 100, loss is 4.496994104385376 and perplexity is 89.7469554639148
At time: 368.60259914398193 and batch: 150, loss is 4.475698699951172 and perplexity is 87.85596392762136
At time: 369.17123675346375 and batch: 200, loss is 4.45782057762146 and perplexity is 86.29922153712936
At time: 369.7400722503662 and batch: 250, loss is 4.443313980102539 and perplexity is 85.05635017727305
At time: 370.30834674835205 and batch: 300, loss is 4.491266775131225 and perplexity is 89.23441424760992
At time: 370.8759410381317 and batch: 350, loss is 4.450265798568726 and perplexity is 85.64970654635253
At time: 371.4446988105774 and batch: 400, loss is 4.494287910461426 and perplexity is 89.50441113236602
At time: 372.01332235336304 and batch: 450, loss is 4.477008457183838 and perplexity is 87.97110930159093
At time: 372.581444978714 and batch: 500, loss is 4.436730327606202 and perplexity is 84.49820804804051
At time: 373.15013813972473 and batch: 550, loss is 4.465455522537232 and perplexity is 86.96063304712739
At time: 373.7314074039459 and batch: 600, loss is 4.545838642120361 and perplexity is 94.23942723147863
At time: 374.3003261089325 and batch: 650, loss is 4.489420852661133 and perplexity is 89.06984637374227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944833194508272 and perplexity of 140.44742149622328
Finished 47 epochs...
Completing Train Step...
At time: 375.33223009109497 and batch: 50, loss is 4.5294586944580075 and perplexity is 92.70836394615039
At time: 375.8993537425995 and batch: 100, loss is 4.496666641235351 and perplexity is 89.7175714545262
At time: 376.46690678596497 and batch: 150, loss is 4.47543267250061 and perplexity is 87.83259493805524
At time: 377.0352244377136 and batch: 200, loss is 4.457607650756836 and perplexity is 86.28084807063972
At time: 377.6037712097168 and batch: 250, loss is 4.4430880355834965 and perplexity is 85.03713433207989
At time: 378.17148995399475 and batch: 300, loss is 4.491004838943481 and perplexity is 89.21104358627089
At time: 378.7394289970398 and batch: 350, loss is 4.450152692794799 and perplexity is 85.64001961784115
At time: 379.30796813964844 and batch: 400, loss is 4.494210300445556 and perplexity is 89.49746496314724
At time: 379.8922872543335 and batch: 450, loss is 4.4769305324554445 and perplexity is 87.96425444387707
At time: 380.4612600803375 and batch: 500, loss is 4.436691589355469 and perplexity is 84.49493479867108
At time: 381.02930760383606 and batch: 550, loss is 4.465462598800659 and perplexity is 86.96124840565189
At time: 381.5979678630829 and batch: 600, loss is 4.545809631347656 and perplexity is 94.23669331253211
At time: 382.166686296463 and batch: 650, loss is 4.489289932250976 and perplexity is 89.05818607622456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9448218252144605 and perplexity of 140.44582471730038
Finished 48 epochs...
Completing Train Step...
At time: 383.18609166145325 and batch: 50, loss is 4.528986368179321 and perplexity is 92.6645856892291
At time: 383.7668056488037 and batch: 100, loss is 4.49636700630188 and perplexity is 89.69069296304082
At time: 384.3395347595215 and batch: 150, loss is 4.475184669494629 and perplexity is 87.81081489135762
At time: 384.90809869766235 and batch: 200, loss is 4.457407236099243 and perplexity is 86.26355785668082
At time: 385.4757549762726 and batch: 250, loss is 4.442866859436035 and perplexity is 85.01832822612482
At time: 386.04472613334656 and batch: 300, loss is 4.4907542419433595 and perplexity is 89.18869036731223
At time: 386.61404848098755 and batch: 350, loss is 4.4500364112854 and perplexity is 85.6300618460586
At time: 387.18321204185486 and batch: 400, loss is 4.494132795333862 and perplexity is 89.49052872092953
At time: 387.7522714138031 and batch: 450, loss is 4.476851558685302 and perplexity is 87.95730784936887
At time: 388.3340196609497 and batch: 500, loss is 4.436650085449219 and perplexity is 84.49142800159196
At time: 388.9028391838074 and batch: 550, loss is 4.4654544067382815 and perplexity is 86.96053601659847
At time: 389.4714369773865 and batch: 600, loss is 4.545784978866577 and perplexity is 94.23437017286899
At time: 390.0396988391876 and batch: 650, loss is 4.489152984619141 and perplexity is 89.04599060363546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944801480162377 and perplexity of 140.4429673687482
Finished 49 epochs...
Completing Train Step...
At time: 391.07452178001404 and batch: 50, loss is 4.5285233306884765 and perplexity is 92.62168844426415
At time: 391.6424632072449 and batch: 100, loss is 4.496066389083862 and perplexity is 89.66373444873994
At time: 392.21164631843567 and batch: 150, loss is 4.4749369716644285 and perplexity is 87.78906703660029
At time: 392.780065536499 and batch: 200, loss is 4.457218227386474 and perplexity is 86.24725483340673
At time: 393.34944200515747 and batch: 250, loss is 4.442648639678955 and perplexity is 84.99977757132532
At time: 393.919269323349 and batch: 300, loss is 4.490502109527588 and perplexity is 89.1662058420084
At time: 394.48822832107544 and batch: 350, loss is 4.449909963607788 and perplexity is 85.6192348081457
At time: 395.05722546577454 and batch: 400, loss is 4.494049425125122 and perplexity is 89.48306818786722
At time: 395.62714862823486 and batch: 450, loss is 4.476762151718139 and perplexity is 87.94944420477178
At time: 396.1961364746094 and batch: 500, loss is 4.43660982131958 and perplexity is 84.48802609626941
At time: 396.76509499549866 and batch: 550, loss is 4.465444183349609 and perplexity is 86.95964698978403
At time: 397.33369064331055 and batch: 600, loss is 4.545798320770263 and perplexity is 94.23562744714694
At time: 397.9017379283905 and batch: 650, loss is 4.489007720947265 and perplexity is 89.03305639553263
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944777245615043 and perplexity of 140.43956383824946
Finished Training.
Improved accuracyfrom -144.26482396281557 to -140.43956383824946
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb2b5144978>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'tune_wordvecs': True, 'data': 'ptb', 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.11419939622077135, 'anneal': 2.356437569125191, 'wordvec_dim': 200, 'lr': 24.024764080393457, 'seq_len': 20}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.8255434036254883 and batch: 50, loss is 6.237610969543457 and perplexity is 511.6347387368785
At time: 1.4026157855987549 and batch: 100, loss is 5.433832063674926 and perplexity is 229.025205154653
At time: 1.9799189567565918 and batch: 150, loss is 5.32155026435852 and perplexity is 204.70097676753468
At time: 2.5699069499969482 and batch: 200, loss is 5.271689214706421 and perplexity is 194.74465028232478
At time: 3.14506196975708 and batch: 250, loss is 5.248187894821167 and perplexity is 190.2212550313652
At time: 3.719743490219116 and batch: 300, loss is 5.265380401611328 and perplexity is 193.51991007312614
At time: 4.296005010604858 and batch: 350, loss is 5.2075106811523435 and perplexity is 182.6388455235707
At time: 4.87109899520874 and batch: 400, loss is 5.251704587936401 and perplexity is 190.891382434889
At time: 5.446779012680054 and batch: 450, loss is 5.19643795967102 and perplexity is 180.62769147783354
At time: 6.022587776184082 and batch: 500, loss is 5.196523656845093 and perplexity is 180.64317142383678
At time: 6.5984108448028564 and batch: 550, loss is 5.1957022571563725 and perplexity is 180.4948521021213
At time: 7.173294305801392 and batch: 600, loss is 5.25004430770874 and perplexity is 190.5747122004086
At time: 7.7486231327056885 and batch: 650, loss is 5.202729749679565 and perplexity is 181.7677457123558
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.22366273169424 and perplexity of 185.61279038579747
Finished 1 epochs...
Completing Train Step...
At time: 8.792805194854736 and batch: 50, loss is 5.127615766525269 and perplexity is 168.6146217874353
At time: 9.362210750579834 and batch: 100, loss is 5.11405011177063 and perplexity is 166.34269894330248
At time: 9.942323446273804 and batch: 150, loss is 5.085005588531494 and perplexity is 161.58084223470246
At time: 10.511599063873291 and batch: 200, loss is 5.067217950820923 and perplexity is 158.7321119542829
At time: 11.07975435256958 and batch: 250, loss is 5.049116563796997 and perplexity is 155.88468945947
At time: 11.649378061294556 and batch: 300, loss is 5.085068559646606 and perplexity is 161.5910174808886
At time: 12.220446348190308 and batch: 350, loss is 5.020372915267944 and perplexity is 151.46777791035254
At time: 12.789453506469727 and batch: 400, loss is 5.096930809020996 and perplexity is 163.51926448961916
At time: 13.358439922332764 and batch: 450, loss is 5.106873245239258 and perplexity is 165.15315330551965
At time: 13.92729663848877 and batch: 500, loss is 5.099298152923584 and perplexity is 163.9068293920656
At time: 14.496129035949707 and batch: 550, loss is 5.110220403671264 and perplexity is 165.70687325240806
At time: 15.065473318099976 and batch: 600, loss is 5.156750001907349 and perplexity is 173.59934026589235
At time: 15.634739637374878 and batch: 650, loss is 5.1236804103851314 and perplexity is 167.95236715887276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.273019828048407 and perplexity of 195.00395258956715
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 16.6794216632843 and batch: 50, loss is 5.0413831520080565 and perplexity is 154.68381836453125
At time: 17.24812078475952 and batch: 100, loss is 4.963713426589965 and perplexity is 143.124291859536
At time: 17.818227767944336 and batch: 150, loss is 4.907273244857788 and perplexity is 135.27006273238143
At time: 18.387661695480347 and batch: 200, loss is 4.88676775932312 and perplexity is 132.52452986167077
At time: 18.956329822540283 and batch: 250, loss is 4.876790637969971 and perplexity is 131.20889060476642
At time: 19.53958535194397 and batch: 300, loss is 4.935915822982788 and perplexity is 139.20056725263547
At time: 20.122413873672485 and batch: 350, loss is 4.866234130859375 and perplexity is 129.83106830889017
At time: 20.69525980949402 and batch: 400, loss is 4.882868604660034 and perplexity is 132.00880232719902
At time: 21.26177144050598 and batch: 450, loss is 4.856524381637573 and perplexity is 128.5765416204825
At time: 21.82941746711731 and batch: 500, loss is 4.8563475418090825 and perplexity is 128.55380617723955
At time: 22.397101163864136 and batch: 550, loss is 4.86062105178833 and perplexity is 129.10435770664765
At time: 22.96434187889099 and batch: 600, loss is 4.922386474609375 and perplexity is 137.32995688761326
At time: 23.531857013702393 and batch: 650, loss is 4.878753862380981 and perplexity is 131.46673612323465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.101024552887561 and perplexity of 164.19004253566206
Finished 3 epochs...
Completing Train Step...
At time: 24.573349475860596 and batch: 50, loss is 4.886584854125976 and perplexity is 132.50029265303334
At time: 25.140242338180542 and batch: 100, loss is 4.84823676109314 and perplexity is 127.51535147718248
At time: 25.706438302993774 and batch: 150, loss is 4.800626525878906 and perplexity is 121.58657079337465
At time: 26.272466897964478 and batch: 200, loss is 4.77881386756897 and perplexity is 118.96316026399137
At time: 26.846988677978516 and batch: 250, loss is 4.7735968017578125 and perplexity is 118.34413777236354
At time: 27.42478322982788 and batch: 300, loss is 4.865031681060791 and perplexity is 129.67504678977338
At time: 27.99188804626465 and batch: 350, loss is 4.81515287399292 and perplexity is 123.36567027129952
At time: 28.558133840560913 and batch: 400, loss is 4.849453296661377 and perplexity is 127.67057283475852
At time: 29.124351501464844 and batch: 450, loss is 4.776115236282348 and perplexity is 118.642555349544
At time: 29.690181255340576 and batch: 500, loss is 4.779293117523193 and perplexity is 119.020187017045
At time: 30.256719827651978 and batch: 550, loss is 4.793526391983033 and perplexity is 120.72634732549145
At time: 30.82347559928894 and batch: 600, loss is 4.853624858856201 and perplexity is 128.20427097332876
At time: 31.390036582946777 and batch: 650, loss is 4.790235738754273 and perplexity is 120.32973170100371
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.081165986902573 and perplexity of 160.96162570279316
Finished 4 epochs...
Completing Train Step...
At time: 32.424124240875244 and batch: 50, loss is 4.80417911529541 and perplexity is 122.0192861328275
At time: 33.002631425857544 and batch: 100, loss is 4.782245149612427 and perplexity is 119.37205753939149
At time: 33.56902527809143 and batch: 150, loss is 4.754996280670166 and perplexity is 116.16322111426182
At time: 34.13469958305359 and batch: 200, loss is 4.734495573043823 and perplexity is 113.80603740395603
At time: 34.69934844970703 and batch: 250, loss is 4.722728281021118 and perplexity is 112.47469702581249
At time: 35.26507091522217 and batch: 300, loss is 4.797588119506836 and perplexity is 121.21770205226491
At time: 35.830992698669434 and batch: 350, loss is 4.764238367080688 and perplexity is 117.24178806877782
At time: 36.39718413352966 and batch: 400, loss is 4.810269479751587 and perplexity is 122.76469565981023
At time: 36.96285009384155 and batch: 450, loss is 4.742714576721191 and perplexity is 114.74526411054758
At time: 37.52783823013306 and batch: 500, loss is 4.741037406921387 and perplexity is 114.55297811211895
At time: 38.09335517883301 and batch: 550, loss is 4.745208721160889 and perplexity is 115.03181257108969
At time: 38.65894913673401 and batch: 600, loss is 4.819478092193603 and perplexity is 123.90040931157503
At time: 39.22492551803589 and batch: 650, loss is 4.749047832489014 and perplexity is 115.47428130573802
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.0599018171721815 and perplexity of 157.57504440038792
Finished 5 epochs...
Completing Train Step...
At time: 40.26416730880737 and batch: 50, loss is 4.739017400741577 and perplexity is 114.32181394354542
At time: 40.830668449401855 and batch: 100, loss is 4.738489933013916 and perplexity is 114.26152877676023
At time: 41.39706778526306 and batch: 150, loss is 4.721251087188721 and perplexity is 112.30867275227176
At time: 41.96297597885132 and batch: 200, loss is 4.713392238616944 and perplexity is 111.4295150084413
At time: 42.52962684631348 and batch: 250, loss is 4.697716588973999 and perplexity is 109.69640427863291
At time: 43.09605288505554 and batch: 300, loss is 4.762804546356201 and perplexity is 117.07380482097882
At time: 43.66270661354065 and batch: 350, loss is 4.721585378646851 and perplexity is 112.34622285823693
At time: 44.22910213470459 and batch: 400, loss is 4.758686332702637 and perplexity is 116.59266128543238
At time: 44.79551815986633 and batch: 450, loss is 4.710876455307007 and perplexity is 111.14953482675288
At time: 45.36457347869873 and batch: 500, loss is 4.688582105636597 and perplexity is 108.698946866177
At time: 45.93080425262451 and batch: 550, loss is 4.710988311767578 and perplexity is 111.16196831568283
At time: 46.49634623527527 and batch: 600, loss is 4.796316051483155 and perplexity is 121.06360292264746
At time: 47.07436203956604 and batch: 650, loss is 4.739594049453736 and perplexity is 114.38775648134003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.074056587967218 and perplexity of 159.82134345590694
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 48.0987663269043 and batch: 50, loss is 4.6899424743652345 and perplexity is 108.84691813925626
At time: 48.677682399749756 and batch: 100, loss is 4.6287975025177 and perplexity is 102.39086529447876
At time: 49.243393659591675 and batch: 150, loss is 4.591399450302124 and perplexity is 98.63236471575884
At time: 49.80914282798767 and batch: 200, loss is 4.571089582443237 and perplexity is 96.64935986876681
At time: 50.374018907547 and batch: 250, loss is 4.554468250274658 and perplexity is 95.05619568830733
At time: 50.93942832946777 and batch: 300, loss is 4.611201057434082 and perplexity is 100.60490937649261
At time: 51.5050790309906 and batch: 350, loss is 4.564460783004761 and perplexity is 96.01080939605662
At time: 52.07059717178345 and batch: 400, loss is 4.607855243682861 and perplexity is 100.2688665690685
At time: 52.63664937019348 and batch: 450, loss is 4.5684690761566165 and perplexity is 96.39642117210046
At time: 53.20364952087402 and batch: 500, loss is 4.540286350250244 and perplexity is 93.71763234505765
At time: 53.77192831039429 and batch: 550, loss is 4.542267847061157 and perplexity is 93.90351763941412
At time: 54.34040880203247 and batch: 600, loss is 4.62020733833313 and perplexity is 101.51507791471363
At time: 54.90934920310974 and batch: 650, loss is 4.591617002487182 and perplexity is 98.65382473647281
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.979908064299939 and perplexity of 145.46100797926016
Finished 7 epochs...
Completing Train Step...
At time: 55.95293402671814 and batch: 50, loss is 4.614548645019531 and perplexity is 100.94225745824538
At time: 56.5217182636261 and batch: 100, loss is 4.583020095825195 and perplexity is 97.80934218328652
At time: 57.09009265899658 and batch: 150, loss is 4.553379983901977 and perplexity is 94.95280549526393
At time: 57.65841627120972 and batch: 200, loss is 4.536762285232544 and perplexity is 93.3879465736139
At time: 58.22722840309143 and batch: 250, loss is 4.52253023147583 and perplexity is 92.06825751588032
At time: 58.796268701553345 and batch: 300, loss is 4.578878192901612 and perplexity is 97.40506320305131
At time: 59.36502480506897 and batch: 350, loss is 4.53306432723999 and perplexity is 93.0432396189121
At time: 59.93225431442261 and batch: 400, loss is 4.586046400070191 and perplexity is 98.10579135717984
At time: 60.50068926811218 and batch: 450, loss is 4.543548736572266 and perplexity is 94.02387473583025
At time: 61.0687096118927 and batch: 500, loss is 4.515785388946533 and perplexity is 91.44936114255974
At time: 61.67397999763489 and batch: 550, loss is 4.525303678512573 and perplexity is 92.32395837435554
At time: 62.249356508255005 and batch: 600, loss is 4.606210956573486 and perplexity is 100.10413123750438
At time: 62.82498288154602 and batch: 650, loss is 4.565626878738403 and perplexity is 96.12283249341093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.978592218137255 and perplexity of 145.26972954418702
Finished 8 epochs...
Completing Train Step...
At time: 63.94879913330078 and batch: 50, loss is 4.578067140579224 and perplexity is 97.32609462847986
At time: 64.53961443901062 and batch: 100, loss is 4.550958127975464 and perplexity is 94.72312172330362
At time: 65.11586737632751 and batch: 150, loss is 4.526187696456909 and perplexity is 92.4056104959034
At time: 65.6896436214447 and batch: 200, loss is 4.513943452835083 and perplexity is 91.28107229813192
At time: 66.26283049583435 and batch: 250, loss is 4.501061744689942 and perplexity is 90.11275726788219
At time: 66.83572793006897 and batch: 300, loss is 4.56025128364563 and perplexity is 95.60750141332906
At time: 67.40786027908325 and batch: 350, loss is 4.511745309829712 and perplexity is 91.08064381347589
At time: 67.9852933883667 and batch: 400, loss is 4.564996824264527 and perplexity is 96.06228894762556
At time: 68.57235074043274 and batch: 450, loss is 4.5223703289031985 and perplexity is 92.05353674162201
At time: 69.15103697776794 and batch: 500, loss is 4.492481031417847 and perplexity is 89.34283350715552
At time: 69.72368168830872 and batch: 550, loss is 4.50358510017395 and perplexity is 90.34043091808417
At time: 70.29689526557922 and batch: 600, loss is 4.583132810592652 and perplexity is 97.82036736188432
At time: 70.8725950717926 and batch: 650, loss is 4.541943006515503 and perplexity is 93.87301892338323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.981839946672028 and perplexity of 145.7422931538531
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 72.03958535194397 and batch: 50, loss is 4.530587358474731 and perplexity is 92.81305961258728
At time: 72.6153199672699 and batch: 100, loss is 4.4862386512756345 and perplexity is 88.78685868591573
At time: 73.19522070884705 and batch: 150, loss is 4.448373908996582 and perplexity is 85.48781994375399
At time: 73.77007722854614 and batch: 200, loss is 4.426855554580689 and perplexity is 83.66791365374948
At time: 74.35916090011597 and batch: 250, loss is 4.410448770523072 and perplexity is 82.30639189993926
At time: 74.94771218299866 and batch: 300, loss is 4.466521234512329 and perplexity is 87.05335743509715
At time: 75.53575372695923 and batch: 350, loss is 4.412906742095947 and perplexity is 82.50894750747746
At time: 76.11000227928162 and batch: 400, loss is 4.457709617614746 and perplexity is 86.28964630617189
At time: 76.71311545372009 and batch: 450, loss is 4.409566040039063 and perplexity is 82.23376959645661
At time: 77.29668402671814 and batch: 500, loss is 4.376803207397461 and perplexity is 79.58321528783904
At time: 77.874596118927 and batch: 550, loss is 4.382006368637085 and perplexity is 79.99837873348464
At time: 78.47343182563782 and batch: 600, loss is 4.477059020996093 and perplexity is 87.97555756870528
At time: 79.04269552230835 and batch: 650, loss is 4.444580135345459 and perplexity is 85.16411292878333
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.92472629921109 and perplexity of 137.65166111825638
Finished 10 epochs...
Completing Train Step...
At time: 80.08058571815491 and batch: 50, loss is 4.47943452835083 and perplexity is 88.18479257400041
At time: 80.662193775177 and batch: 100, loss is 4.455496578216553 and perplexity is 86.09889506709486
At time: 81.23107290267944 and batch: 150, loss is 4.42504246711731 and perplexity is 83.51635384552927
At time: 81.80223250389099 and batch: 200, loss is 4.405868797302246 and perplexity is 81.93029274858151
At time: 82.37117290496826 and batch: 250, loss is 4.391681966781616 and perplexity is 80.77616761680335
At time: 82.94019961357117 and batch: 300, loss is 4.451103639602661 and perplexity is 85.72149745552676
At time: 83.50894093513489 and batch: 350, loss is 4.3996608924865725 and perplexity is 81.42325274607606
At time: 84.07827162742615 and batch: 400, loss is 4.447332420349121 and perplexity is 85.39883169795404
At time: 84.64695143699646 and batch: 450, loss is 4.402270488739013 and perplexity is 81.63601204850215
At time: 85.21606349945068 and batch: 500, loss is 4.373007736206055 and perplexity is 79.2817319844116
At time: 85.78474760055542 and batch: 550, loss is 4.380507221221924 and perplexity is 79.87853922178076
At time: 86.3538007736206 and batch: 600, loss is 4.473395376205445 and perplexity is 87.65383607201025
At time: 86.9224443435669 and batch: 650, loss is 4.431928062438965 and perplexity is 84.09339802733709
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.924872903262868 and perplexity of 137.67184288883874
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 87.9697470664978 and batch: 50, loss is 4.455130968093872 and perplexity is 86.06742219325791
At time: 88.53715491294861 and batch: 100, loss is 4.425603313446045 and perplexity is 83.56320682340507
At time: 89.10552668571472 and batch: 150, loss is 4.3899359703063965 and perplexity is 80.63525576445896
At time: 89.67365336418152 and batch: 200, loss is 4.367707023620605 and perplexity is 78.86259415586221
At time: 90.24179720878601 and batch: 250, loss is 4.352653465270996 and perplexity is 77.68432233549518
At time: 90.81031346321106 and batch: 300, loss is 4.409352226257324 and perplexity is 82.21618876277184
At time: 91.39129781723022 and batch: 350, loss is 4.353504190444946 and perplexity is 77.75043846346693
At time: 91.95981240272522 and batch: 400, loss is 4.398086071014404 and perplexity is 81.29512657372152
At time: 92.5277111530304 and batch: 450, loss is 4.350242748260498 and perplexity is 77.4972729703225
At time: 93.0953860282898 and batch: 500, loss is 4.319050159454346 and perplexity is 75.11724499104278
At time: 93.66427445411682 and batch: 550, loss is 4.323941192626953 and perplexity is 75.48554587984734
At time: 94.23176264762878 and batch: 600, loss is 4.419709148406983 and perplexity is 83.0721201857833
At time: 94.79957056045532 and batch: 650, loss is 4.3870291328430175 and perplexity is 80.40120252430384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.904208912568934 and perplexity of 134.85618476510865
Finished 12 epochs...
Completing Train Step...
At time: 95.82965326309204 and batch: 50, loss is 4.4317812252044675 and perplexity is 84.08105089186304
At time: 96.41085124015808 and batch: 100, loss is 4.407476892471314 and perplexity is 82.0621504480005
At time: 96.98712301254272 and batch: 150, loss is 4.377061357498169 and perplexity is 79.6037623548799
At time: 97.5628354549408 and batch: 200, loss is 4.357704858779908 and perplexity is 78.07772920701078
At time: 98.13834857940674 and batch: 250, loss is 4.343648452758789 and perplexity is 76.98791432825637
At time: 98.72028160095215 and batch: 300, loss is 4.400861940383911 and perplexity is 81.52110472327418
At time: 99.30312609672546 and batch: 350, loss is 4.346873044967651 and perplexity is 77.23656964751875
At time: 99.89189124107361 and batch: 400, loss is 4.39431529045105 and perplexity is 80.9891577237801
At time: 100.46972751617432 and batch: 450, loss is 4.3496982383728025 and perplexity is 77.45508642545848
At time: 101.04226875305176 and batch: 500, loss is 4.320008840560913 and perplexity is 75.18929300460968
At time: 101.61501216888428 and batch: 550, loss is 4.32615740776062 and perplexity is 75.6530236037838
At time: 102.1968047618866 and batch: 600, loss is 4.419466762542725 and perplexity is 83.0519871182204
At time: 102.76986193656921 and batch: 650, loss is 4.381836643218994 and perplexity is 79.9848021273877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.903321509267769 and perplexity of 134.7365660244481
Finished 13 epochs...
Completing Train Step...
At time: 103.90476155281067 and batch: 50, loss is 4.421332759857178 and perplexity is 83.2071065843868
At time: 104.47837471961975 and batch: 100, loss is 4.3982132720947265 and perplexity is 81.30546805935676
At time: 105.05745601654053 and batch: 150, loss is 4.370140895843506 and perplexity is 79.05476940333621
At time: 105.63833546638489 and batch: 200, loss is 4.35138991355896 and perplexity is 77.58622616485339
At time: 106.23521828651428 and batch: 250, loss is 4.338280687332153 and perplexity is 76.57576840436984
At time: 106.80618977546692 and batch: 300, loss is 4.395090398788452 and perplexity is 81.0519574303149
At time: 107.381108045578 and batch: 350, loss is 4.341890449523926 and perplexity is 76.85268822461298
At time: 107.9557478427887 and batch: 400, loss is 4.391260843276978 and perplexity is 80.74215803562497
At time: 108.53561520576477 and batch: 450, loss is 4.348543376922607 and perplexity is 77.36568816326343
At time: 109.11788129806519 and batch: 500, loss is 4.3193161582946775 and perplexity is 75.13722874880969
At time: 109.68534517288208 and batch: 550, loss is 4.325517749786377 and perplexity is 75.60464701784396
At time: 110.25324869155884 and batch: 600, loss is 4.417371711730957 and perplexity is 82.87817112543446
At time: 110.82249736785889 and batch: 650, loss is 4.376649198532104 and perplexity is 79.5709597109091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.902772492053462 and perplexity of 134.6626136327296
Finished 14 epochs...
Completing Train Step...
At time: 111.87164044380188 and batch: 50, loss is 4.412927083969116 and perplexity is 82.51062591109385
At time: 112.46235632896423 and batch: 100, loss is 4.390879278182983 and perplexity is 80.71135552346021
At time: 113.04404401779175 and batch: 150, loss is 4.363959693908692 and perplexity is 78.56762303582161
At time: 113.62390756607056 and batch: 200, loss is 4.34611138343811 and perplexity is 77.17776392161699
At time: 114.19526958465576 and batch: 250, loss is 4.333736267089844 and perplexity is 76.22856544796326
At time: 114.7662923336029 and batch: 300, loss is 4.390005779266358 and perplexity is 80.64088502428417
At time: 115.3340380191803 and batch: 350, loss is 4.337779054641723 and perplexity is 76.537365128618
At time: 115.9004967212677 and batch: 400, loss is 4.388045082092285 and perplexity is 80.48292757287007
At time: 116.46792244911194 and batch: 450, loss is 4.34655517578125 and perplexity is 77.21202242357423
At time: 117.03482389450073 and batch: 500, loss is 4.318014612197876 and perplexity is 75.03949779647351
At time: 117.60299348831177 and batch: 550, loss is 4.324534320831299 and perplexity is 75.53033176670249
At time: 118.16989803314209 and batch: 600, loss is 4.414640655517578 and perplexity is 82.65213498043205
At time: 118.73738837242126 and batch: 650, loss is 4.371698055267334 and perplexity is 79.17796617612562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.902869131050858 and perplexity of 134.67562792153163
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 119.7817394733429 and batch: 50, loss is 4.4050119113922115 and perplexity is 81.86011790533126
At time: 120.3505630493164 and batch: 100, loss is 4.38057695388794 and perplexity is 79.88410955949313
At time: 120.94322085380554 and batch: 150, loss is 4.351498289108276 and perplexity is 77.59463507038501
At time: 121.51371431350708 and batch: 200, loss is 4.328799896240234 and perplexity is 75.85320021281453
At time: 122.08467936515808 and batch: 250, loss is 4.314724283218384 and perplexity is 74.79299891586085
At time: 122.65335988998413 and batch: 300, loss is 4.3694335460662845 and perplexity is 78.99886980242633
At time: 123.22228932380676 and batch: 350, loss is 4.315593681335449 and perplexity is 74.85805208274081
At time: 123.79019021987915 and batch: 400, loss is 4.36550986289978 and perplexity is 78.68951057749081
At time: 124.35974597930908 and batch: 450, loss is 4.321715288162231 and perplexity is 75.31770912983471
At time: 124.92839026451111 and batch: 500, loss is 4.29090503692627 and perplexity is 73.032535740141
At time: 125.49818730354309 and batch: 550, loss is 4.298000726699829 and perplexity is 73.55259486420543
At time: 126.06706023216248 and batch: 600, loss is 4.387024774551391 and perplexity is 80.40085211317974
At time: 126.63680124282837 and batch: 650, loss is 4.3529514503479 and perplexity is 77.70747455359701
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.895220588235294 and perplexity of 133.6494848671642
Finished 16 epochs...
Completing Train Step...
At time: 127.67481780052185 and batch: 50, loss is 4.3974880123138425 and perplexity is 81.24652185164801
At time: 128.2606930732727 and batch: 100, loss is 4.374808187484741 and perplexity is 79.42460345810855
At time: 128.83071994781494 and batch: 150, loss is 4.345762157440186 and perplexity is 77.15081614569974
At time: 129.4008605480194 and batch: 200, loss is 4.323635444641114 and perplexity is 75.46246985413853
At time: 129.97032761573792 and batch: 250, loss is 4.310973386764527 and perplexity is 74.51298360397398
At time: 130.54037165641785 and batch: 300, loss is 4.365663280487061 and perplexity is 78.70158385845102
At time: 131.11028122901917 and batch: 350, loss is 4.312528095245361 and perplexity is 74.6289196715565
At time: 131.68020367622375 and batch: 400, loss is 4.3627338886260985 and perplexity is 78.47137343215185
At time: 132.24908471107483 and batch: 450, loss is 4.3194060707092286 and perplexity is 75.14398482219205
At time: 132.81820058822632 and batch: 500, loss is 4.290625829696655 and perplexity is 73.01214737458724
At time: 133.3872845172882 and batch: 550, loss is 4.297944669723511 and perplexity is 73.54847184370007
At time: 133.95676589012146 and batch: 600, loss is 4.387454986572266 and perplexity is 80.43544896770511
At time: 134.52517890930176 and batch: 650, loss is 4.352188787460327 and perplexity is 77.6482325403819
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.8947287166819855 and perplexity of 133.5837626522127
Finished 17 epochs...
Completing Train Step...
At time: 135.56701755523682 and batch: 50, loss is 4.394478483200073 and perplexity is 81.00237564557531
At time: 136.1365783214569 and batch: 100, loss is 4.372043075561524 and perplexity is 79.20528889448418
At time: 136.7069447040558 and batch: 150, loss is 4.342871723175048 and perplexity is 76.92813875534034
At time: 137.2759211063385 and batch: 200, loss is 4.320800485610962 and perplexity is 75.24883980308778
At time: 137.84479188919067 and batch: 250, loss is 4.3087312030792235 and perplexity is 74.34609897071103
At time: 138.4134612083435 and batch: 300, loss is 4.3630282592773435 and perplexity is 78.49447650171916
At time: 138.9828586578369 and batch: 350, loss is 4.310349626541138 and perplexity is 74.46651986129952
At time: 139.5644407272339 and batch: 400, loss is 4.360680465698242 and perplexity is 78.31040384065213
At time: 140.13478827476501 and batch: 450, loss is 4.317464017868042 and perplexity is 74.99819284665226
At time: 140.70422458648682 and batch: 500, loss is 4.290096101760864 and perplexity is 72.97348104269311
At time: 141.27285242080688 and batch: 550, loss is 4.297256164550781 and perplexity is 73.49785076882497
At time: 141.84142017364502 and batch: 600, loss is 4.387021160125732 and perplexity is 80.400561510802
At time: 142.4093677997589 and batch: 650, loss is 4.351052131652832 and perplexity is 77.56002336715474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.894511204139859 and perplexity of 133.5547096682202
Finished 18 epochs...
Completing Train Step...
At time: 143.44643235206604 and batch: 50, loss is 4.39223837852478 and perplexity is 80.82112493126213
At time: 144.02746057510376 and batch: 100, loss is 4.369971361160278 and perplexity is 79.04136801407999
At time: 144.59493041038513 and batch: 150, loss is 4.340318698883056 and perplexity is 76.73198984137176
At time: 145.1632857322693 and batch: 200, loss is 4.318508825302124 and perplexity is 75.07659246520139
At time: 145.7316391468048 and batch: 250, loss is 4.306830530166626 and perplexity is 74.20492555897968
At time: 146.29937624931335 and batch: 300, loss is 4.3609093570709225 and perplexity is 78.32833046802929
At time: 146.8678503036499 and batch: 350, loss is 4.308305788040161 and perplexity is 74.31447774866186
At time: 147.43672800064087 and batch: 400, loss is 4.358499917984009 and perplexity is 78.13983030800684
At time: 148.00536584854126 and batch: 450, loss is 4.315715007781982 and perplexity is 74.86713489517592
At time: 148.57473397254944 and batch: 500, loss is 4.289176902770996 and perplexity is 72.90643471181211
At time: 149.14352941513062 and batch: 550, loss is 4.296382303237915 and perplexity is 73.43365189500075
At time: 149.72391510009766 and batch: 600, loss is 4.386323881149292 and perplexity is 80.34451943031945
At time: 150.29105305671692 and batch: 650, loss is 4.349858312606812 and perplexity is 77.46748598148639
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.894516589594822 and perplexity of 133.55542892303095
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 151.32906770706177 and batch: 50, loss is 4.3906946849823 and perplexity is 80.69645813103364
At time: 151.89740538597107 and batch: 100, loss is 4.365978899002076 and perplexity is 78.72642745582115
At time: 152.4673466682434 and batch: 150, loss is 4.3351758766174315 and perplexity is 76.33838384589617
At time: 153.03689217567444 and batch: 200, loss is 4.3126060485839846 and perplexity is 74.63473747175829
At time: 153.60635042190552 and batch: 250, loss is 4.298880138397217 and perplexity is 73.61730632634014
At time: 154.1754240989685 and batch: 300, loss is 4.351857795715332 and perplexity is 77.62253586932515
At time: 154.74410009384155 and batch: 350, loss is 4.2979171848297115 and perplexity is 73.54645039954208
At time: 155.31335258483887 and batch: 400, loss is 4.346953449249267 and perplexity is 77.24278004808379
At time: 155.8821361064911 and batch: 450, loss is 4.3042017841339115 and perplexity is 74.01011582015558
At time: 156.45029044151306 and batch: 500, loss is 4.275801076889038 and perplexity is 71.93774389781244
At time: 157.01921892166138 and batch: 550, loss is 4.282749242782593 and perplexity is 72.43931977474278
At time: 157.5904495716095 and batch: 600, loss is 4.3741019821166995 and perplexity is 79.36853317768866
At time: 158.15850448608398 and batch: 650, loss is 4.341159992218017 and perplexity is 76.7965711150921
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.890632180606618 and perplexity of 133.03765129558002
Finished 20 epochs...
Completing Train Step...
At time: 159.19656443595886 and batch: 50, loss is 4.387216701507568 and perplexity is 80.41628468491572
At time: 159.77853322029114 and batch: 100, loss is 4.362037725448609 and perplexity is 78.4167635623771
At time: 160.34756231307983 and batch: 150, loss is 4.3321774959564205 and perplexity is 76.10983512134422
At time: 160.91677474975586 and batch: 200, loss is 4.309711198806763 and perplexity is 74.41899354241204
At time: 161.48433828353882 and batch: 250, loss is 4.296920824050903 and perplexity is 73.47320809488244
At time: 162.05326747894287 and batch: 300, loss is 4.349710321426391 and perplexity is 77.45602232507272
At time: 162.62243366241455 and batch: 350, loss is 4.2961648750305175 and perplexity is 73.41768708336964
At time: 163.19103741645813 and batch: 400, loss is 4.346172552108765 and perplexity is 77.18248492722753
At time: 163.76040863990784 and batch: 450, loss is 4.303993263244629 and perplexity is 73.9946847738925
At time: 164.34203004837036 and batch: 500, loss is 4.27619255065918 and perplexity is 71.96591115064221
At time: 164.90963435173035 and batch: 550, loss is 4.2838294982910154 and perplexity is 72.51761503078272
At time: 165.4786958694458 and batch: 600, loss is 4.375162839889526 and perplexity is 79.45277658026153
At time: 166.04758048057556 and batch: 650, loss is 4.340977935791016 and perplexity is 76.78259107836531
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.8900780771292895 and perplexity of 132.96395508992907
Finished 21 epochs...
Completing Train Step...
At time: 167.08810901641846 and batch: 50, loss is 4.385278663635254 and perplexity is 80.26058580354356
At time: 167.6570017337799 and batch: 100, loss is 4.359888725280761 and perplexity is 78.24842686689733
At time: 168.22618079185486 and batch: 150, loss is 4.330359277725219 and perplexity is 75.97157656175898
At time: 168.79494190216064 and batch: 200, loss is 4.307923288345337 and perplexity is 74.28605791923651
At time: 169.36484098434448 and batch: 250, loss is 4.295622434616089 and perplexity is 73.37787316207796
At time: 169.9341378211975 and batch: 300, loss is 4.34817216873169 and perplexity is 77.33697471578444
At time: 170.50371432304382 and batch: 350, loss is 4.29484377861023 and perplexity is 73.32075927936276
At time: 171.07236123085022 and batch: 400, loss is 4.345485277175904 and perplexity is 77.129457564357
At time: 171.6416413784027 and batch: 450, loss is 4.303757648468018 and perplexity is 73.97725258649022
At time: 172.2097430229187 and batch: 500, loss is 4.276331357955932 and perplexity is 71.97590123755982
At time: 172.778657913208 and batch: 550, loss is 4.2843795585632325 and perplexity is 72.55751506255223
At time: 173.35598492622375 and batch: 600, loss is 4.375690240859985 and perplexity is 79.4946911036442
At time: 173.9271218776703 and batch: 650, loss is 4.340686540603638 and perplexity is 76.76022026038332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.8898153866038605 and perplexity of 132.9290313059778
Finished 22 epochs...
Completing Train Step...
At time: 174.95405554771423 and batch: 50, loss is 4.383758707046509 and perplexity is 80.13868586211296
At time: 175.53654217720032 and batch: 100, loss is 4.358138675689697 and perplexity is 78.11160799428283
At time: 176.10581016540527 and batch: 150, loss is 4.328858861923218 and perplexity is 75.8576730804431
At time: 176.67471551895142 and batch: 200, loss is 4.306438732147217 and perplexity is 74.17585791081059
At time: 177.24355816841125 and batch: 250, loss is 4.2945412921905515 and perplexity is 73.29858409941473
At time: 177.81315302848816 and batch: 300, loss is 4.3468648719787595 and perplexity is 77.23593839647258
At time: 178.38220477104187 and batch: 350, loss is 4.2936906814575195 and perplexity is 73.23626204672489
At time: 178.9655623435974 and batch: 400, loss is 4.344835433959961 and perplexity is 77.07935179182094
At time: 179.53684377670288 and batch: 450, loss is 4.303470439910889 and perplexity is 73.95600873736814
At time: 180.10575604438782 and batch: 500, loss is 4.276280174255371 and perplexity is 71.97221733886184
At time: 180.6749656200409 and batch: 550, loss is 4.284695644378662 and perplexity is 72.58045308886769
At time: 181.24485039710999 and batch: 600, loss is 4.376008367538452 and perplexity is 79.5199845087209
At time: 181.8130180835724 and batch: 650, loss is 4.34041356086731 and perplexity is 76.73926913544253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.88969211952359 and perplexity of 132.9126465422773
Finished 23 epochs...
Completing Train Step...
At time: 182.8626627922058 and batch: 50, loss is 4.382516078948974 and perplexity is 80.03916512579887
At time: 183.43115735054016 and batch: 100, loss is 4.356606235504151 and perplexity is 77.99199829799466
At time: 184.00020790100098 and batch: 150, loss is 4.327612714767456 and perplexity is 75.76320213146862
At time: 184.56926012039185 and batch: 200, loss is 4.305139904022217 and perplexity is 74.07957875893007
At time: 185.13920736312866 and batch: 250, loss is 4.2935411167144775 and perplexity is 73.22530930310306
At time: 185.70873641967773 and batch: 300, loss is 4.3456339836120605 and perplexity is 77.14092806396152
At time: 186.27805829048157 and batch: 350, loss is 4.292585983276367 and perplexity is 73.15540275202844
At time: 186.84647250175476 and batch: 400, loss is 4.344219741821289 and perplexity is 77.03190924736828
At time: 187.4166178703308 and batch: 450, loss is 4.302979555130005 and perplexity is 73.91971376727707
At time: 187.9856629371643 and batch: 500, loss is 4.276084489822388 and perplexity is 71.95813487422585
At time: 188.5645046234131 and batch: 550, loss is 4.284803609848023 and perplexity is 72.58828969458567
At time: 189.15199160575867 and batch: 600, loss is 4.3761190700531 and perplexity is 79.52878805824938
At time: 189.73151683807373 and batch: 650, loss is 4.340042066574097 and perplexity is 76.71076622955461
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.889651130227482 and perplexity of 132.90719865810485
Finished 24 epochs...
Completing Train Step...
At time: 190.76295685768127 and batch: 50, loss is 4.381383581161499 and perplexity is 79.94857225617723
At time: 191.34495663642883 and batch: 100, loss is 4.355219612121582 and perplexity is 77.883927713417
At time: 191.91455578804016 and batch: 150, loss is 4.326275358200073 and perplexity is 75.6619474374385
At time: 192.48444294929504 and batch: 200, loss is 4.303782300949097 and perplexity is 73.9790763317897
At time: 193.05338716506958 and batch: 250, loss is 4.292642517089844 and perplexity is 73.15953862282961
At time: 193.6353464126587 and batch: 300, loss is 4.344399585723877 and perplexity is 77.04576421237927
At time: 194.2041676044464 and batch: 350, loss is 4.291500015258789 and perplexity is 73.07600144576962
At time: 194.77351832389832 and batch: 400, loss is 4.343482265472412 and perplexity is 76.9751209787658
At time: 195.34330487251282 and batch: 450, loss is 4.3024105453491215 and perplexity is 73.87766469144564
At time: 195.91273760795593 and batch: 500, loss is 4.275907068252564 and perplexity is 71.94536908147182
At time: 196.48145651817322 and batch: 550, loss is 4.284846649169922 and perplexity is 72.59141391258359
At time: 197.05077743530273 and batch: 600, loss is 4.376104211807251 and perplexity is 79.52760640874291
At time: 197.62009716033936 and batch: 650, loss is 4.339408168792724 and perplexity is 76.66215485398365
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.889515596277573 and perplexity of 132.88918644116046
Finished 25 epochs...
Completing Train Step...
At time: 198.65744256973267 and batch: 50, loss is 4.380300216674804 and perplexity is 79.86200571225943
At time: 199.226548910141 and batch: 100, loss is 4.353948774337769 and perplexity is 77.78501274108169
At time: 199.80343389511108 and batch: 150, loss is 4.325086784362793 and perplexity is 75.5720710491793
At time: 200.37176752090454 and batch: 200, loss is 4.302683534622193 and perplexity is 73.89783525447437
At time: 200.9408495426178 and batch: 250, loss is 4.291822681427002 and perplexity is 73.09958440365116
At time: 201.50872349739075 and batch: 300, loss is 4.3432138156890865 and perplexity is 76.9544597975865
At time: 202.07713341712952 and batch: 350, loss is 4.2904950141906735 and perplexity is 73.00259687827577
At time: 202.64589524269104 and batch: 400, loss is 4.342811985015869 and perplexity is 76.92354334720399
At time: 203.2149317264557 and batch: 450, loss is 4.301990661621094 and perplexity is 73.8466511736684
At time: 203.78341913223267 and batch: 500, loss is 4.275704545974731 and perplexity is 71.93080001677583
At time: 204.35169386863708 and batch: 550, loss is 4.284781799316407 and perplexity is 72.58670652266328
At time: 204.91977405548096 and batch: 600, loss is 4.3761146450042725 and perplexity is 79.52843614025763
At time: 205.4890968799591 and batch: 650, loss is 4.339023714065552 and perplexity is 76.63268739097047
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.889567356483609 and perplexity of 132.89606499084664
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 206.51348400115967 and batch: 50, loss is 4.379228687286377 and perplexity is 79.776477057537
At time: 207.09615564346313 and batch: 100, loss is 4.352217416763306 and perplexity is 77.65045558697906
At time: 207.6657428741455 and batch: 150, loss is 4.3228348541259765 and perplexity is 75.40207949372494
At time: 208.24744319915771 and batch: 200, loss is 4.300011129379272 and perplexity is 73.70061393711653
At time: 208.81680393218994 and batch: 250, loss is 4.288393421173096 and perplexity is 72.84933623257155
At time: 209.38666892051697 and batch: 300, loss is 4.338280563354492 and perplexity is 76.57575891068578
At time: 209.95603823661804 and batch: 350, loss is 4.28562294960022 and perplexity is 72.64778853743145
At time: 210.52581429481506 and batch: 400, loss is 4.337736692428589 and perplexity is 76.53412290511802
At time: 211.09510016441345 and batch: 450, loss is 4.296919527053833 and perplexity is 73.47311280040863
At time: 211.66401553153992 and batch: 500, loss is 4.269136009216308 and perplexity is 71.45986827226125
At time: 212.2334921360016 and batch: 550, loss is 4.278549222946167 and perplexity is 72.13571122203227
At time: 212.80277490615845 and batch: 600, loss is 4.370430326461792 and perplexity is 79.0776535856548
At time: 213.3721148967743 and batch: 650, loss is 4.33518816947937 and perplexity is 76.33932226887731
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.88795680625766 and perplexity of 132.6822014685478
Finished 27 epochs...
Completing Train Step...
At time: 214.4148666858673 and batch: 50, loss is 4.377010126113891 and perplexity is 79.59968424840514
At time: 214.98256921768188 and batch: 100, loss is 4.350629472732544 and perplexity is 77.5272488581281
At time: 215.5506374835968 and batch: 150, loss is 4.321543531417847 and perplexity is 75.30477391620697
At time: 216.11840176582336 and batch: 200, loss is 4.299003820419312 and perplexity is 73.62641202674189
At time: 216.68577337265015 and batch: 250, loss is 4.287637205123901 and perplexity is 72.79426721999918
At time: 217.2536976337433 and batch: 300, loss is 4.337512836456299 and perplexity is 76.51699220209841
At time: 217.83333587646484 and batch: 350, loss is 4.285323143005371 and perplexity is 72.62601151593721
At time: 218.42388486862183 and batch: 400, loss is 4.337732095718383 and perplexity is 76.53377110074278
At time: 219.010085105896 and batch: 450, loss is 4.296982107162475 and perplexity is 73.47771089966321
At time: 219.58023476600647 and batch: 500, loss is 4.269224996566773 and perplexity is 71.46622757954809
At time: 220.14864921569824 and batch: 550, loss is 4.279035158157349 and perplexity is 72.1707730222904
At time: 220.71783685684204 and batch: 600, loss is 4.370856714248657 and perplexity is 79.111378520797
At time: 221.28679871559143 and batch: 650, loss is 4.3352596473693845 and perplexity is 76.34477903757508
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.887706382601869 and perplexity of 132.648978866634
Finished 28 epochs...
Completing Train Step...
At time: 222.32664561271667 and batch: 50, loss is 4.375688877105713 and perplexity is 79.49458269249351
At time: 222.90899062156677 and batch: 100, loss is 4.349563503265381 and perplexity is 77.4446512090794
At time: 223.47816848754883 and batch: 150, loss is 4.320757436752319 and perplexity is 75.24560049614476
At time: 224.04643607139587 and batch: 200, loss is 4.298314218521118 and perplexity is 73.57565661577631
At time: 224.61591839790344 and batch: 250, loss is 4.287111377716064 and perplexity is 72.75600006100373
At time: 225.18519115447998 and batch: 300, loss is 4.336918249130249 and perplexity is 76.47150969130973
At time: 225.75612211227417 and batch: 350, loss is 4.285102996826172 and perplexity is 72.61002493674823
At time: 226.32459497451782 and batch: 400, loss is 4.337689771652221 and perplexity is 76.53053194889861
At time: 226.89344000816345 and batch: 450, loss is 4.297036867141724 and perplexity is 73.4817346477565
At time: 227.46172618865967 and batch: 500, loss is 4.269274005889892 and perplexity is 71.46973017681695
At time: 228.0298047065735 and batch: 550, loss is 4.279359197616577 and perplexity is 72.19416298998418
At time: 228.5995602607727 and batch: 600, loss is 4.371129093170166 and perplexity is 79.13292972767152
At time: 229.16917634010315 and batch: 650, loss is 4.33526346206665 and perplexity is 76.3450702703504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.887594484815411 and perplexity of 132.63413656994845
Finished 29 epochs...
Completing Train Step...
At time: 230.20970225334167 and batch: 50, loss is 4.3746186637878415 and perplexity is 79.40955203998162
At time: 230.7793846130371 and batch: 100, loss is 4.34867259979248 and perplexity is 77.37568622549283
At time: 231.3486773967743 and batch: 150, loss is 4.320093564987182 and perplexity is 75.19566364419167
At time: 231.91772890090942 and batch: 200, loss is 4.297715177536011 and perplexity is 73.53159498063758
At time: 232.4870138168335 and batch: 250, loss is 4.286658725738525 and perplexity is 72.72307436620093
At time: 233.05645108222961 and batch: 300, loss is 4.336389589309692 and perplexity is 76.43109296101065
At time: 233.62592220306396 and batch: 350, loss is 4.284885845184326 and perplexity is 72.59425926245173
At time: 234.19456815719604 and batch: 400, loss is 4.337605390548706 and perplexity is 76.52407449060772
At time: 234.7642457485199 and batch: 450, loss is 4.297042388916015 and perplexity is 73.48214039842998
At time: 235.33197903633118 and batch: 500, loss is 4.269267921447754 and perplexity is 71.46929532470195
At time: 235.90049934387207 and batch: 550, loss is 4.27959415435791 and perplexity is 72.21112748814714
At time: 236.46940541267395 and batch: 600, loss is 4.37132435798645 and perplexity is 79.14838311335899
At time: 237.050151348114 and batch: 650, loss is 4.335214004516602 and perplexity is 76.34129452358694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.8875163957184435 and perplexity of 132.62377969438145
Finished 30 epochs...
Completing Train Step...
At time: 238.07564687728882 and batch: 50, loss is 4.373667240142822 and perplexity is 79.33403584417402
At time: 238.6600046157837 and batch: 100, loss is 4.347879362106323 and perplexity is 77.3143332521489
At time: 239.2299976348877 and batch: 150, loss is 4.319464893341064 and perplexity is 75.14840511915139
At time: 239.80025053024292 and batch: 200, loss is 4.297145013809204 and perplexity is 73.48968188220502
At time: 240.36945462226868 and batch: 250, loss is 4.286221895217896 and perplexity is 72.69131364529493
At time: 240.93876361846924 and batch: 300, loss is 4.33587381362915 and perplexity is 76.39168182655015
At time: 241.50892639160156 and batch: 350, loss is 4.284656114578247 and perplexity is 72.57758405474965
At time: 242.0781388282776 and batch: 400, loss is 4.337480545043945 and perplexity is 76.51452140024419
At time: 242.64658880233765 and batch: 450, loss is 4.296986923217774 and perplexity is 73.47806477323424
At time: 243.21665716171265 and batch: 500, loss is 4.2692138290405275 and perplexity is 71.46542948303235
At time: 243.78770327568054 and batch: 550, loss is 4.279762973785401 and perplexity is 72.2233191584146
At time: 244.35930132865906 and batch: 600, loss is 4.371472806930542 and perplexity is 79.16013347940189
At time: 244.93097233772278 and batch: 650, loss is 4.3351419734954835 and perplexity is 76.33579578023132
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.887451471066942 and perplexity of 132.61516942121654
Finished 31 epochs...
Completing Train Step...
At time: 245.98106980323792 and batch: 50, loss is 4.372810802459717 and perplexity is 79.2661202732026
At time: 246.5498983860016 and batch: 100, loss is 4.347142210006714 and perplexity is 77.25736182994376
At time: 247.11893391609192 and batch: 150, loss is 4.318911304473877 and perplexity is 75.10681531157994
At time: 247.6883978843689 and batch: 200, loss is 4.296670465469361 and perplexity is 73.45481574915928
At time: 248.25900769233704 and batch: 250, loss is 4.285822381973267 and perplexity is 72.66227830311225
At time: 248.82924675941467 and batch: 300, loss is 4.3353863620758055 and perplexity is 76.35445365678294
At time: 249.39974093437195 and batch: 350, loss is 4.284431123733521 and perplexity is 72.5612565996378
At time: 249.9685413837433 and batch: 400, loss is 4.337361488342285 and perplexity is 76.50541237595313
At time: 250.53813219070435 and batch: 450, loss is 4.296908502578735 and perplexity is 73.47230280237109
At time: 251.10814762115479 and batch: 500, loss is 4.269151821136474 and perplexity is 71.46099819892657
At time: 251.70555925369263 and batch: 550, loss is 4.2799020195007325 and perplexity is 72.23336219969536
At time: 252.2777292728424 and batch: 600, loss is 4.371597967147827 and perplexity is 79.17004179895929
At time: 252.84878182411194 and batch: 650, loss is 4.335074234008789 and perplexity is 76.33062500774346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.887411678538603 and perplexity of 132.6098924333222
Finished 32 epochs...
Completing Train Step...
At time: 253.88779592514038 and batch: 50, loss is 4.372021474838257 and perplexity is 79.2035780214356
At time: 254.47123837471008 and batch: 100, loss is 4.3464638614654545 and perplexity is 77.20497218247428
At time: 255.04094767570496 and batch: 150, loss is 4.318398780822754 and perplexity is 75.0683311552348
At time: 255.6119909286499 and batch: 200, loss is 4.29622365951538 and perplexity is 73.42200303113881
At time: 256.1820333003998 and batch: 250, loss is 4.285426530838013 and perplexity is 72.63352055002593
At time: 256.761892080307 and batch: 300, loss is 4.334911918640136 and perplexity is 76.31823637966542
At time: 257.337703704834 and batch: 350, loss is 4.284203300476074 and perplexity is 72.54472734074112
At time: 257.90635323524475 and batch: 400, loss is 4.33724045753479 and perplexity is 76.496153424436
At time: 258.4736912250519 and batch: 450, loss is 4.296808633804321 and perplexity is 73.46496557992266
At time: 259.04225397109985 and batch: 500, loss is 4.2690833568572994 and perplexity is 71.45610584067357
At time: 259.611980676651 and batch: 550, loss is 4.280006780624389 and perplexity is 72.24092984427547
At time: 260.19031405448914 and batch: 600, loss is 4.371690683364868 and perplexity is 79.17738248603295
At time: 260.76120138168335 and batch: 650, loss is 4.335033378601074 and perplexity is 76.32750655264097
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.887402702780331 and perplexity of 132.60870216432494
Finished 33 epochs...
Completing Train Step...
At time: 261.8080224990845 and batch: 50, loss is 4.371277122497559 and perplexity is 79.14464458908391
At time: 262.3767395019531 and batch: 100, loss is 4.3458248805999755 and perplexity is 77.15565544043508
At time: 262.94677543640137 and batch: 150, loss is 4.317909021377563 and perplexity is 75.03157473265406
At time: 263.51789832115173 and batch: 200, loss is 4.2957995414733885 and perplexity is 73.39087003747295
At time: 264.08772706985474 and batch: 250, loss is 4.2850398921966555 and perplexity is 72.60544305259599
At time: 264.6585576534271 and batch: 300, loss is 4.334448413848877 and perplexity is 76.28287070815523
At time: 265.23022747039795 and batch: 350, loss is 4.2839695167541505 and perplexity is 72.52776954668282
At time: 265.7991433143616 and batch: 400, loss is 4.337102575302124 and perplexity is 76.48560669113175
At time: 266.37662982940674 and batch: 450, loss is 4.29670654296875 and perplexity is 73.4574658630341
At time: 266.94104647636414 and batch: 500, loss is 4.268989868164063 and perplexity is 71.44942581497305
At time: 267.50664925575256 and batch: 550, loss is 4.280078458786011 and perplexity is 72.2461081269032
At time: 268.07204365730286 and batch: 600, loss is 4.371751070022583 and perplexity is 79.18216388789291
At time: 268.63664650917053 and batch: 650, loss is 4.334968299865722 and perplexity is 76.32253941667119
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.887413174498315 and perplexity of 132.61009081252703
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 269.6631405353546 and batch: 50, loss is 4.3704674243927 and perplexity is 79.08058725740017
At time: 270.24110436439514 and batch: 100, loss is 4.345197038650513 and perplexity is 77.10722908695011
At time: 270.80585050582886 and batch: 150, loss is 4.317169675827026 and perplexity is 74.97612097399663
At time: 271.3710126876831 and batch: 200, loss is 4.294832420349121 and perplexity is 73.31992648776374
At time: 271.93585181236267 and batch: 250, loss is 4.283483943939209 and perplexity is 72.49256058241235
At time: 272.5001313686371 and batch: 300, loss is 4.332217578887939 and perplexity is 76.11288588779472
At time: 273.06536412239075 and batch: 350, loss is 4.2821534061431885 and perplexity is 72.3961706300847
At time: 273.6312892436981 and batch: 400, loss is 4.335046091079712 and perplexity is 76.32847687060506
At time: 274.1964156627655 and batch: 450, loss is 4.294053926467895 and perplexity is 73.26286958572724
At time: 274.7623209953308 and batch: 500, loss is 4.265991201400757 and perplexity is 71.23549371237372
At time: 275.327677488327 and batch: 550, loss is 4.277301483154297 and perplexity is 72.0457607537849
At time: 275.89577627182007 and batch: 600, loss is 4.369321269989014 and perplexity is 78.99000061712405
At time: 276.46108746528625 and batch: 650, loss is 4.333035593032837 and perplexity is 76.17517277736499
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.8869162166819855 and perplexity of 132.54420556382422
Finished 35 epochs...
Completing Train Step...
At time: 277.50031566619873 and batch: 50, loss is 4.369379215240478 and perplexity is 78.99457784518604
At time: 278.06606125831604 and batch: 100, loss is 4.344787225723267 and perplexity is 77.0756360217516
At time: 278.6320583820343 and batch: 150, loss is 4.316615705490112 and perplexity is 74.93459792932958
At time: 279.19736099243164 and batch: 200, loss is 4.294536657333374 and perplexity is 73.29824437173343
At time: 279.76333498954773 and batch: 250, loss is 4.283154783248901 and perplexity is 72.46870280786503
At time: 280.3287031650543 and batch: 300, loss is 4.332135200500488 and perplexity is 76.10661608924252
At time: 280.90668535232544 and batch: 350, loss is 4.282172613143921 and perplexity is 72.39756115674086
At time: 281.4713382720947 and batch: 400, loss is 4.335028591156006 and perplexity is 76.32714113977087
At time: 282.03763484954834 and batch: 450, loss is 4.294034423828125 and perplexity is 73.26144078030599
At time: 282.6034059524536 and batch: 500, loss is 4.266141910552978 and perplexity is 71.24623036227455
At time: 283.16990661621094 and batch: 550, loss is 4.2774937629699705 and perplexity is 72.059615031289
At time: 283.7351105213165 and batch: 600, loss is 4.36948296546936 and perplexity is 79.00277397588577
At time: 284.30100774765015 and batch: 650, loss is 4.332761192321778 and perplexity is 76.15427312336094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.886786666570925 and perplexity of 132.52703555948506
Finished 36 epochs...
Completing Train Step...
At time: 285.3378303050995 and batch: 50, loss is 4.3687682056427 and perplexity is 78.94632614257809
At time: 285.91980171203613 and batch: 100, loss is 4.34444107055664 and perplexity is 77.04896050932123
At time: 286.4897313117981 and batch: 150, loss is 4.316253547668457 and perplexity is 74.90746469213049
At time: 287.059130191803 and batch: 200, loss is 4.294361267089844 and perplexity is 73.28538970212749
At time: 287.62798595428467 and batch: 250, loss is 4.282948799133301 and perplexity is 72.45377694350674
At time: 288.19686484336853 and batch: 300, loss is 4.332082948684692 and perplexity is 76.1026394842511
At time: 288.7653591632843 and batch: 350, loss is 4.28220838546753 and perplexity is 72.40015103204972
At time: 289.3343417644501 and batch: 400, loss is 4.335019817352295 and perplexity is 76.3264714633545
At time: 289.9039535522461 and batch: 450, loss is 4.293983659744263 and perplexity is 73.2577218247778
At time: 290.4725468158722 and batch: 500, loss is 4.266248073577881 and perplexity is 71.25379447911037
At time: 291.0414400100708 and batch: 550, loss is 4.277601041793823 and perplexity is 72.06734591670957
At time: 291.6111650466919 and batch: 600, loss is 4.369569673538208 and perplexity is 79.00962445084227
At time: 292.1797251701355 and batch: 650, loss is 4.332507266998291 and perplexity is 76.13493807985564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.886695712220435 and perplexity of 132.5149821972045
Finished 37 epochs...
Completing Train Step...
At time: 293.2221529483795 and batch: 50, loss is 4.368250885009766 and perplexity is 78.9054961411818
At time: 293.7918174266815 and batch: 100, loss is 4.344129457473755 and perplexity is 77.02495478564681
At time: 294.3614025115967 and batch: 150, loss is 4.315946073532104 and perplexity is 74.88443612464378
At time: 294.9309537410736 and batch: 200, loss is 4.294214973449707 and perplexity is 73.27466929988155
At time: 295.51288056373596 and batch: 250, loss is 4.282770338058472 and perplexity is 72.44084791829616
At time: 296.0811514854431 and batch: 300, loss is 4.332026500701904 and perplexity is 76.09834376501082
At time: 296.6503236293793 and batch: 350, loss is 4.282237148284912 and perplexity is 72.4022334943209
At time: 297.2186198234558 and batch: 400, loss is 4.335004253387451 and perplexity is 76.32528353008047
At time: 297.78818583488464 and batch: 450, loss is 4.293921670913696 and perplexity is 73.25318080501964
At time: 298.3570878505707 and batch: 500, loss is 4.266328916549683 and perplexity is 71.25955508045718
At time: 298.926552772522 and batch: 550, loss is 4.277676448822022 and perplexity is 72.07278050599585
At time: 299.4964509010315 and batch: 600, loss is 4.369623441696167 and perplexity is 79.01387276702106
At time: 300.0654368400574 and batch: 650, loss is 4.332270641326904 and perplexity is 76.11692473031103
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.886631086760876 and perplexity of 132.50641863229671
Finished 38 epochs...
Completing Train Step...
At time: 301.09498047828674 and batch: 50, loss is 4.367779397964478 and perplexity is 78.86830199091825
At time: 301.67675590515137 and batch: 100, loss is 4.34384331703186 and perplexity is 77.00291798400798
At time: 302.24641275405884 and batch: 150, loss is 4.3156653881073 and perplexity is 74.86342010446216
At time: 302.81577229499817 and batch: 200, loss is 4.294082221984863 and perplexity is 73.26494262582544
At time: 303.383757352829 and batch: 250, loss is 4.282602596282959 and perplexity is 72.42869758093474
At time: 303.9537744522095 and batch: 300, loss is 4.3319652557373045 and perplexity is 76.09368326735833
At time: 304.52363204956055 and batch: 350, loss is 4.282257709503174 and perplexity is 72.40372218775106
At time: 305.09272718429565 and batch: 400, loss is 4.334985551834106 and perplexity is 76.32385614206618
At time: 305.6616635322571 and batch: 450, loss is 4.293859891891479 and perplexity is 73.24865543492312
At time: 306.23088121414185 and batch: 500, loss is 4.266394634246826 and perplexity is 71.2642382481983
At time: 306.7997934818268 and batch: 550, loss is 4.27773045539856 and perplexity is 72.07667301524215
At time: 307.37037229537964 and batch: 600, loss is 4.369656400680542 and perplexity is 79.01647702693568
At time: 307.9394407272339 and batch: 650, loss is 4.332045927047729 and perplexity is 76.09982209211273
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.886594884535846 and perplexity of 132.50162169194192
Finished 39 epochs...
Completing Train Step...
At time: 308.9793424606323 and batch: 50, loss is 4.367341337203979 and perplexity is 78.83376044876834
At time: 309.54723834991455 and batch: 100, loss is 4.343574199676514 and perplexity is 76.98219795055154
At time: 310.1290328502655 and batch: 150, loss is 4.315398683547974 and perplexity is 74.84345635132388
At time: 310.69709515571594 and batch: 200, loss is 4.293956356048584 and perplexity is 73.25572164554126
At time: 311.26570892333984 and batch: 250, loss is 4.2824415111541745 and perplexity is 72.41703133451186
At time: 311.83533358573914 and batch: 300, loss is 4.331897344589233 and perplexity is 76.08851583343184
At time: 312.40374302864075 and batch: 350, loss is 4.28226957321167 and perplexity is 72.40458116950043
At time: 312.97202825546265 and batch: 400, loss is 4.334961528778076 and perplexity is 76.32202263181702
At time: 313.54074358940125 and batch: 450, loss is 4.293795318603515 and perplexity is 73.24392568111227
At time: 314.10872864723206 and batch: 500, loss is 4.266446752548218 and perplexity is 71.2679525160356
At time: 314.67724895477295 and batch: 550, loss is 4.277771053314209 and perplexity is 72.07959923733233
At time: 315.246164560318 and batch: 600, loss is 4.369676542282105 and perplexity is 79.0180685613608
At time: 315.8144164085388 and batch: 650, loss is 4.331829671859741 and perplexity is 76.08336689010588
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.8865685556449145 and perplexity of 132.4981331171215
Finished 40 epochs...
Completing Train Step...
At time: 316.8396096229553 and batch: 50, loss is 4.366927995681762 and perplexity is 78.80118191571644
At time: 317.42107105255127 and batch: 100, loss is 4.343312730789185 and perplexity is 76.9620721321616
At time: 317.98931097984314 and batch: 150, loss is 4.315138149261474 and perplexity is 74.82395960472
At time: 318.5581274032593 and batch: 200, loss is 4.2938323497772215 and perplexity is 73.2466380398677
At time: 319.12612533569336 and batch: 250, loss is 4.2822835159301755 and perplexity is 72.40559069323196
At time: 319.69410586357117 and batch: 300, loss is 4.33181866645813 and perplexity is 76.08252956670485
At time: 320.2666997909546 and batch: 350, loss is 4.282269811630249 and perplexity is 72.40459843209986
At time: 320.83628845214844 and batch: 400, loss is 4.334929342269898 and perplexity is 76.31956613194465
At time: 321.40500020980835 and batch: 450, loss is 4.293729314804077 and perplexity is 73.23909146327166
At time: 321.97343587875366 and batch: 500, loss is 4.266483869552612 and perplexity is 71.27059781803487
At time: 322.5420169830322 and batch: 550, loss is 4.277804546356201 and perplexity is 72.08201344280569
At time: 323.1113121509552 and batch: 600, loss is 4.369689178466797 and perplexity is 79.01906705457773
At time: 323.68042182922363 and batch: 650, loss is 4.331621866226197 and perplexity is 76.06755798049421
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.88654551786535 and perplexity of 132.4950806894987
Finished 41 epochs...
Completing Train Step...
At time: 324.71836280822754 and batch: 50, loss is 4.366536560058594 and perplexity is 78.77034236221057
At time: 325.28730058670044 and batch: 100, loss is 4.343056507110596 and perplexity is 76.9423551530144
At time: 325.85680198669434 and batch: 150, loss is 4.314879016876221 and perplexity is 74.80457280557575
At time: 326.4240288734436 and batch: 200, loss is 4.293711128234864 and perplexity is 73.23775950757754
At time: 326.9927170276642 and batch: 250, loss is 4.282128973007202 and perplexity is 72.39440178621207
At time: 327.5613901615143 and batch: 300, loss is 4.331726369857788 and perplexity is 76.07550773193115
At time: 328.13003849983215 and batch: 350, loss is 4.282262649536133 and perplexity is 72.40407986540848
At time: 328.70869970321655 and batch: 400, loss is 4.3348907947540285 and perplexity is 76.31662425895938
At time: 329.28505206108093 and batch: 450, loss is 4.29365647315979 and perplexity is 73.23375680171851
At time: 329.8530840873718 and batch: 500, loss is 4.2665085601806645 and perplexity is 71.27235755558108
At time: 330.4226233959198 and batch: 550, loss is 4.277843065261841 and perplexity is 72.08479001655479
At time: 330.99077916145325 and batch: 600, loss is 4.369686908721924 and perplexity is 79.01888770165897
At time: 331.5594148635864 and batch: 650, loss is 4.331426315307617 and perplexity is 76.05268435398007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.886524275237439 and perplexity of 132.49226617569352
Finished 42 epochs...
Completing Train Step...
At time: 332.5861163139343 and batch: 50, loss is 4.366160926818847 and perplexity is 78.740759159878
At time: 333.1667318344116 and batch: 100, loss is 4.342809343338013 and perplexity is 76.92334014025133
At time: 333.73447465896606 and batch: 150, loss is 4.314617042541504 and perplexity is 74.78497849409663
At time: 334.30246758461 and batch: 200, loss is 4.293592510223388 and perplexity is 73.22907270539562
At time: 334.86998534202576 and batch: 250, loss is 4.2819813537597655 and perplexity is 72.38371576785218
At time: 335.4387969970703 and batch: 300, loss is 4.331636295318604 and perplexity is 76.06865557423612
At time: 336.0091116428375 and batch: 350, loss is 4.282254619598389 and perplexity is 72.40349846748903
At time: 336.5783576965332 and batch: 400, loss is 4.334855842590332 and perplexity is 76.3139568744312
At time: 337.1475019454956 and batch: 450, loss is 4.293583850860596 and perplexity is 73.22843859103365
At time: 337.715571641922 and batch: 500, loss is 4.266534872055054 and perplexity is 71.2742328895722
At time: 338.2842707633972 and batch: 550, loss is 4.277878131866455 and perplexity is 72.08731782970563
At time: 338.8674564361572 and batch: 600, loss is 4.369683647155762 and perplexity is 79.01862997674897
At time: 339.4364778995514 and batch: 650, loss is 4.331237888336181 and perplexity is 76.03835532702767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.886509016448376 and perplexity of 132.49024451957558
Finished 43 epochs...
Completing Train Step...
At time: 340.4768612384796 and batch: 50, loss is 4.365791749954224 and perplexity is 78.71169525848337
At time: 341.0460023880005 and batch: 100, loss is 4.342575664520264 and perplexity is 76.90536688513632
At time: 341.6151728630066 and batch: 150, loss is 4.314382266998291 and perplexity is 74.76742287104265
At time: 342.18359088897705 and batch: 200, loss is 4.293477582931518 and perplexity is 73.22065716998006
At time: 342.7525177001953 and batch: 250, loss is 4.281835498809815 and perplexity is 72.37315901450769
At time: 343.3215284347534 and batch: 300, loss is 4.331561031341553 and perplexity is 76.06293056013507
At time: 343.8909239768982 and batch: 350, loss is 4.282249784469604 and perplexity is 72.40314838809584
At time: 344.458957195282 and batch: 400, loss is 4.334819164276123 and perplexity is 76.31115785847435
At time: 345.0283019542694 and batch: 450, loss is 4.293511190414429 and perplexity is 73.22311797331506
At time: 345.59747862815857 and batch: 500, loss is 4.266558542251587 and perplexity is 71.27591998463926
At time: 346.1662395000458 and batch: 550, loss is 4.277903823852539 and perplexity is 72.089169919864
At time: 346.73483896255493 and batch: 600, loss is 4.36968053817749 and perplexity is 79.01838430992717
At time: 347.31150460243225 and batch: 650, loss is 4.331054077148438 and perplexity is 76.0243799110792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.8865018358417585 and perplexity of 132.48929316266467
Finished 44 epochs...
Completing Train Step...
At time: 348.371417760849 and batch: 50, loss is 4.365435590744019 and perplexity is 78.68366635493858
At time: 348.9507656097412 and batch: 100, loss is 4.342349252700806 and perplexity is 76.88795657211845
At time: 349.5170702934265 and batch: 150, loss is 4.3141573047637936 and perplexity is 74.75060491629922
At time: 350.08402132987976 and batch: 200, loss is 4.29336389541626 and perplexity is 73.21233336856591
At time: 350.6504406929016 and batch: 250, loss is 4.281691799163818 and perplexity is 72.36275976437982
At time: 351.2172827720642 and batch: 300, loss is 4.331485853195191 and perplexity is 76.05721250494776
At time: 351.78326201438904 and batch: 350, loss is 4.282242956161499 and perplexity is 72.40265399877882
At time: 352.3504240512848 and batch: 400, loss is 4.334778575897217 and perplexity is 76.30806057514171
At time: 352.91775941848755 and batch: 450, loss is 4.29343578338623 and perplexity is 73.2175966437694
At time: 353.4968750476837 and batch: 500, loss is 4.266578168869018 and perplexity is 71.27731890358088
At time: 354.06311225891113 and batch: 550, loss is 4.2779241466522215 and perplexity is 72.09063498851064
At time: 354.6296832561493 and batch: 600, loss is 4.369674968719482 and perplexity is 79.01794422157944
At time: 355.19634103775024 and batch: 650, loss is 4.330875177383422 and perplexity is 76.01078038389005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.886497347962623 and perplexity of 132.4886985680644
Finished 45 epochs...
Completing Train Step...
At time: 356.2323377132416 and batch: 50, loss is 4.365088167190552 and perplexity is 78.6563345441061
At time: 356.7986478805542 and batch: 100, loss is 4.342127132415771 and perplexity is 76.87088009387526
At time: 357.3650574684143 and batch: 150, loss is 4.313937969207764 and perplexity is 74.73421124872917
At time: 357.94475269317627 and batch: 200, loss is 4.293251047134399 and perplexity is 73.20407194868656
At time: 358.5209209918976 and batch: 250, loss is 4.281550674438477 and perplexity is 72.3525483103444
At time: 359.10847210884094 and batch: 300, loss is 4.331408338546753 and perplexity is 76.05131718534905
At time: 359.686115026474 and batch: 350, loss is 4.28223292350769 and perplexity is 72.40192761166018
At time: 360.2625765800476 and batch: 400, loss is 4.3347345733642575 and perplexity is 76.3047029010648
At time: 360.83734607696533 and batch: 450, loss is 4.293357982635498 and perplexity is 73.21190048136941
At time: 361.4120213985443 and batch: 500, loss is 4.2665925788879395 and perplexity is 71.27834601849528
At time: 361.9875690937042 and batch: 550, loss is 4.277940788269043 and perplexity is 72.0918347032171
At time: 362.55962109565735 and batch: 600, loss is 4.3696671485900875 and perplexity is 79.01732629344727
At time: 363.1289269924164 and batch: 650, loss is 4.33070029258728 and perplexity is 75.99748841637337
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.886494356043198 and perplexity of 132.48830217314665
Finished 46 epochs...
Completing Train Step...
At time: 364.15418696403503 and batch: 50, loss is 4.3647472953796385 and perplexity is 78.62952738607204
At time: 364.7358202934265 and batch: 100, loss is 4.341909065246582 and perplexity is 76.85411890625886
At time: 365.3041627407074 and batch: 150, loss is 4.313724193572998 and perplexity is 74.71823660283663
At time: 365.8731315135956 and batch: 200, loss is 4.293139314651489 and perplexity is 73.1958931328968
At time: 366.441055059433 and batch: 250, loss is 4.281411371231079 and perplexity is 72.34247007028326
At time: 367.0095019340515 and batch: 300, loss is 4.331328458786011 and perplexity is 76.04524246695493
At time: 367.57855319976807 and batch: 350, loss is 4.282219858169555 and perplexity is 72.4009816621739
At time: 368.168484210968 and batch: 400, loss is 4.334688081741333 and perplexity is 76.30115545405405
At time: 368.7371942996979 and batch: 450, loss is 4.293278608322144 and perplexity is 73.20608956766105
At time: 369.30506134033203 and batch: 500, loss is 4.266601715087891 and perplexity is 71.27899723469152
At time: 369.87255811691284 and batch: 550, loss is 4.277954530715943 and perplexity is 72.09282542823489
At time: 370.44075107574463 and batch: 600, loss is 4.3696574783325195 and perplexity is 79.01656217924427
At time: 371.008083820343 and batch: 650, loss is 4.330529365539551 and perplexity is 75.98449950015372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.8864916633157165 and perplexity of 132.48794541873468
Finished 47 epochs...
Completing Train Step...
At time: 372.04883313179016 and batch: 50, loss is 4.364412727355957 and perplexity is 78.60322486072883
At time: 372.6165430545807 and batch: 100, loss is 4.34169599533081 and perplexity is 76.8377453500349
At time: 373.18523955345154 and batch: 150, loss is 4.313515100479126 and perplexity is 74.70261516880058
At time: 373.75263690948486 and batch: 200, loss is 4.293028783798218 and perplexity is 73.18780317547608
At time: 374.3212876319885 and batch: 250, loss is 4.281272497177124 and perplexity is 72.33242427575772
At time: 374.88929510116577 and batch: 300, loss is 4.331246185302734 and perplexity is 76.03898621733579
At time: 375.4605658054352 and batch: 350, loss is 4.282204475402832 and perplexity is 72.3998679433285
At time: 376.0297544002533 and batch: 400, loss is 4.334640426635742 and perplexity is 76.29751940107315
At time: 376.59873962402344 and batch: 450, loss is 4.293198041915893 and perplexity is 73.20019185369094
At time: 377.16689562797546 and batch: 500, loss is 4.266606760025025 and perplexity is 71.27935683365861
At time: 377.7358386516571 and batch: 550, loss is 4.277965621948242 and perplexity is 72.09362503094313
At time: 378.30516505241394 and batch: 600, loss is 4.369645004272461 and perplexity is 79.01557652804956
At time: 378.8750960826874 and batch: 650, loss is 4.330362939834595 and perplexity is 75.97185477849123
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.88648956897212 and perplexity of 132.48766794374518
Finished 48 epochs...
Completing Train Step...
At time: 379.9114556312561 and batch: 50, loss is 4.364084424972535 and perplexity is 78.57742347022307
At time: 380.49711561203003 and batch: 100, loss is 4.341488151550293 and perplexity is 76.82177676209825
At time: 381.0654921531677 and batch: 150, loss is 4.313310194015503 and perplexity is 74.68730968825241
At time: 381.6344392299652 and batch: 200, loss is 4.292918901443482 and perplexity is 73.1797615691486
At time: 382.20327639579773 and batch: 250, loss is 4.281135244369507 and perplexity is 72.3224971287241
At time: 382.78649973869324 and batch: 300, loss is 4.331162195205689 and perplexity is 76.03259996369896
At time: 383.35535287857056 and batch: 350, loss is 4.282187385559082 and perplexity is 72.39863065147043
At time: 383.9234187602997 and batch: 400, loss is 4.334592552185058 and perplexity is 76.29386678667744
At time: 384.49118304252625 and batch: 450, loss is 4.293117685317993 and perplexity is 73.19430997163512
At time: 385.0622375011444 and batch: 500, loss is 4.266609420776367 and perplexity is 71.27954649055533
At time: 385.6322546005249 and batch: 550, loss is 4.277973546981811 and perplexity is 72.09419637760558
At time: 386.20070695877075 and batch: 600, loss is 4.369629068374634 and perplexity is 79.01431735392833
At time: 386.76880288124084 and batch: 650, loss is 4.3302011394500735 and perplexity is 75.95956349756905
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.8864907657398895 and perplexity of 132.48782650081088
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 387.8281078338623 and batch: 50, loss is 4.363819437026978 and perplexity is 78.55660415876564
At time: 388.396356344223 and batch: 100, loss is 4.341107559204102 and perplexity is 76.79254454497368
At time: 388.9659399986267 and batch: 150, loss is 4.3129993915557865 and perplexity is 74.6641002956467
At time: 389.53458166122437 and batch: 200, loss is 4.292402143478394 and perplexity is 73.14195511370929
At time: 390.10403323173523 and batch: 250, loss is 4.280628719329834 and perplexity is 72.28587324923579
At time: 390.67319202423096 and batch: 300, loss is 4.330366802215576 and perplexity is 75.97214821130491
At time: 391.2429780960083 and batch: 350, loss is 4.281655616760254 and perplexity is 72.3601415531699
At time: 391.81180810928345 and batch: 400, loss is 4.333850116729736 and perplexity is 76.23724453673523
At time: 392.3819634914398 and batch: 450, loss is 4.291795568466187 and perplexity is 73.0976024843516
At time: 392.95116353034973 and batch: 500, loss is 4.265390567779541 and perplexity is 71.19272012673886
At time: 393.5221281051636 and batch: 550, loss is 4.276783380508423 and perplexity is 72.0084433224791
At time: 394.09143114089966 and batch: 600, loss is 4.368310403823853 and perplexity is 78.91019264250079
At time: 394.6607208251953 and batch: 650, loss is 4.329218873977661 and perplexity is 75.8849876736705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.886410582299326 and perplexity of 132.47720359694543
Finished Training.
Improved accuracyfrom -140.43956383824946 to -132.47720359694543
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb2b5aa6dd8>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'tune_wordvecs': True, 'data': 'ptb', 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.5147093544966376, 'anneal': 7.451100558510475, 'wordvec_dim': 200, 'lr': 2.280673112310202, 'seq_len': 20}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.8140430450439453 and batch: 50, loss is 6.982435932159424 and perplexity is 1077.5399869625462
At time: 1.4051709175109863 and batch: 100, loss is 5.95026291847229 and perplexity is 383.8542481676066
At time: 1.9813072681427002 and batch: 150, loss is 5.796195859909058 and perplexity is 329.0454410461124
At time: 2.5575506687164307 and batch: 200, loss is 5.704486093521118 and perplexity is 300.2111599388216
At time: 3.132917642593384 and batch: 250, loss is 5.6319761562347415 and perplexity is 279.2133419875227
At time: 3.707516670227051 and batch: 300, loss is 5.611366090774536 and perplexity is 273.51763281594685
At time: 4.282166957855225 and batch: 350, loss is 5.501005449295044 and perplexity is 244.93808131947884
At time: 4.874796390533447 and batch: 400, loss is 5.521115083694458 and perplexity is 249.91355640648845
At time: 5.449268817901611 and batch: 450, loss is 5.459450817108154 and perplexity is 234.96834832827986
At time: 6.025712251663208 and batch: 500, loss is 5.454288330078125 and perplexity is 233.75845299914292
At time: 6.600329160690308 and batch: 550, loss is 5.4511253547668455 and perplexity is 233.02024885967913
At time: 7.175158739089966 and batch: 600, loss is 5.464215545654297 and perplexity is 236.0905801669037
At time: 7.749283075332642 and batch: 650, loss is 5.40594030380249 and perplexity is 222.72555173411405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.287071078431373 and perplexity of 197.76334299534153
Finished 1 epochs...
Completing Train Step...
At time: 8.790959358215332 and batch: 50, loss is 5.164641485214234 and perplexity is 174.97471630252434
At time: 9.359620809555054 and batch: 100, loss is 5.063944473266601 and perplexity is 158.21335548072125
At time: 9.928300619125366 and batch: 150, loss is 4.979736576080322 and perplexity is 145.43606526872978
At time: 10.497690439224243 and batch: 200, loss is 4.927431879043579 and perplexity is 138.02459294741098
At time: 11.06494665145874 and batch: 250, loss is 4.894667177200318 and perplexity is 133.5755422296427
At time: 11.633073329925537 and batch: 300, loss is 4.921293592453003 and perplexity is 137.17995341118697
At time: 12.200811386108398 and batch: 350, loss is 4.827571315765381 and perplexity is 124.90723174871988
At time: 12.769567489624023 and batch: 400, loss is 4.857408847808838 and perplexity is 128.690313528249
At time: 13.337701797485352 and batch: 450, loss is 4.79697247505188 and perplexity is 121.14309801329175
At time: 13.90479564666748 and batch: 500, loss is 4.78522515296936 and perplexity is 119.72831723548346
At time: 14.47306203842163 and batch: 550, loss is 4.790167961120606 and perplexity is 120.32157631290876
At time: 15.040881633758545 and batch: 600, loss is 4.841034440994263 and perplexity is 126.60024450081418
At time: 15.608798503875732 and batch: 650, loss is 4.7412284564971925 and perplexity is 114.57486550071614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.851721670113358 and perplexity of 127.96050608690074
Finished 2 epochs...
Completing Train Step...
At time: 16.63022232055664 and batch: 50, loss is 4.707750568389892 and perplexity is 110.80263641506475
At time: 17.2108314037323 and batch: 100, loss is 4.6602224922180175 and perplexity is 105.65958797771538
At time: 17.778109788894653 and batch: 150, loss is 4.608663644790649 and perplexity is 100.34995680418142
At time: 18.34583330154419 and batch: 200, loss is 4.563622522354126 and perplexity is 95.93036103556257
At time: 18.913721799850464 and batch: 250, loss is 4.549972896575928 and perplexity is 94.62984348738848
At time: 19.498491764068604 and batch: 300, loss is 4.622511949539184 and perplexity is 101.74930049318132
At time: 20.066348552703857 and batch: 350, loss is 4.555024080276489 and perplexity is 95.10904546011454
At time: 20.635459423065186 and batch: 400, loss is 4.601591033935547 and perplexity is 99.64272454773098
At time: 21.202819347381592 and batch: 450, loss is 4.548246564865113 and perplexity is 94.46662191561074
At time: 21.769809007644653 and batch: 500, loss is 4.537456464767456 and perplexity is 93.45279708127622
At time: 22.336318731307983 and batch: 550, loss is 4.552256879806518 and perplexity is 94.84622347309288
At time: 22.903292894363403 and batch: 600, loss is 4.625110893249512 and perplexity is 102.01408512886924
At time: 23.469488859176636 and batch: 650, loss is 4.520004949569702 and perplexity is 91.83605252597502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.719450707529106 and perplexity of 112.10665640987452
Finished 3 epochs...
Completing Train Step...
At time: 24.510610580444336 and batch: 50, loss is 4.51145260810852 and perplexity is 91.05398825351891
At time: 25.077059268951416 and batch: 100, loss is 4.475733242034912 and perplexity is 87.85899870809793
At time: 25.643723249435425 and batch: 150, loss is 4.42681360244751 and perplexity is 83.66440367991903
At time: 26.209423780441284 and batch: 200, loss is 4.379275197982788 and perplexity is 79.78018760333157
At time: 26.777666330337524 and batch: 250, loss is 4.368978099822998 and perplexity is 78.96289825612556
At time: 27.344583749771118 and batch: 300, loss is 4.456363143920899 and perplexity is 86.17353775346713
At time: 27.911755800247192 and batch: 350, loss is 4.400760774612427 and perplexity is 81.51285799497293
At time: 28.478357553482056 and batch: 400, loss is 4.452337656021118 and perplexity is 85.82734448585627
At time: 29.045811414718628 and batch: 450, loss is 4.4022270202636715 and perplexity is 81.63246353265026
At time: 29.61226224899292 and batch: 500, loss is 4.388247880935669 and perplexity is 80.49925107263151
At time: 30.179853439331055 and batch: 550, loss is 4.407574682235718 and perplexity is 82.07017567874563
At time: 30.745014429092407 and batch: 600, loss is 4.487744951248169 and perplexity is 88.9206991053195
At time: 31.310831308364868 and batch: 650, loss is 4.378980588912964 and perplexity is 79.75668709837228
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.649539124731924 and perplexity of 104.53679604938337
Finished 4 epochs...
Completing Train Step...
At time: 32.33215093612671 and batch: 50, loss is 4.381446733474731 and perplexity is 79.9536213528842
At time: 32.91119980812073 and batch: 100, loss is 4.351457471847534 and perplexity is 77.59146793457052
At time: 33.47783541679382 and batch: 150, loss is 4.303686399459838 and perplexity is 73.97198196838097
At time: 34.05721473693848 and batch: 200, loss is 4.251870718002319 and perplexity is 70.23668255010855
At time: 34.62388896942139 and batch: 250, loss is 4.245234861373901 and perplexity is 69.77214500157785
At time: 35.19041633605957 and batch: 300, loss is 4.3386047840118405 and perplexity is 76.60059037879668
At time: 35.75617051124573 and batch: 350, loss is 4.291418809890747 and perplexity is 73.07006752311361
At time: 36.324055194854736 and batch: 400, loss is 4.3450798988342285 and perplexity is 77.09819728930258
At time: 36.8906409740448 and batch: 450, loss is 4.296213283538818 and perplexity is 73.42124121010852
At time: 37.458465814590454 and batch: 500, loss is 4.280495510101319 and perplexity is 72.27624474514474
At time: 38.02592492103577 and batch: 550, loss is 4.301429748535156 and perplexity is 73.80524123544625
At time: 38.592737913131714 and batch: 600, loss is 4.385376796722412 and perplexity is 80.26846240907712
At time: 39.172237396240234 and batch: 650, loss is 4.274431610107422 and perplexity is 71.83929497385179
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.606517118566177 and perplexity of 100.1347840099181
Finished 5 epochs...
Completing Train Step...
At time: 40.221160888671875 and batch: 50, loss is 4.281110258102417 and perplexity is 72.32069008206999
At time: 40.79006600379944 and batch: 100, loss is 4.254856691360474 and perplexity is 70.4467208412801
At time: 41.35951495170593 and batch: 150, loss is 4.209131608009338 and perplexity is 67.2980733200168
At time: 41.928062200546265 and batch: 200, loss is 4.1547860479354854 and perplexity is 63.73832614241558
At time: 42.4944634437561 and batch: 250, loss is 4.149653973579407 and perplexity is 63.412054255230586
At time: 43.06070256233215 and batch: 300, loss is 4.24623963356018 and perplexity is 69.84228534388409
At time: 43.627514123916626 and batch: 350, loss is 4.2052983474731445 and perplexity is 67.04059607535613
At time: 44.194533586502075 and batch: 400, loss is 4.259895677566528 and perplexity is 70.80259676979368
At time: 44.761544704437256 and batch: 450, loss is 4.212510681152343 and perplexity is 67.52586307452484
At time: 45.327961683273315 and batch: 500, loss is 4.194778213500976 and perplexity is 66.33901684729571
At time: 45.89858555793762 and batch: 550, loss is 4.216954379081726 and perplexity is 67.82659529916485
At time: 46.48226237297058 and batch: 600, loss is 4.302820453643799 and perplexity is 73.90795396648247
At time: 47.066062211990356 and batch: 650, loss is 4.191248512268066 and perplexity is 66.10527270391603
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.579849542356005 and perplexity of 97.49972352473182
Finished 6 epochs...
Completing Train Step...
At time: 48.18886423110962 and batch: 50, loss is 4.199120335578918 and perplexity is 66.62769524183017
At time: 48.78118896484375 and batch: 100, loss is 4.176764159202576 and perplexity is 65.15468156260297
At time: 49.350016355514526 and batch: 150, loss is 4.131895418167114 and perplexity is 62.29588786059971
At time: 49.9182403087616 and batch: 200, loss is 4.076357588768006 and perplexity is 58.93042959644443
At time: 50.48648548126221 and batch: 250, loss is 4.07147400856018 and perplexity is 58.643339700670545
At time: 51.05574131011963 and batch: 300, loss is 4.169964661598206 and perplexity is 64.71316520716212
At time: 51.625226974487305 and batch: 350, loss is 4.133677263259887 and perplexity is 62.406988435295865
At time: 52.194188356399536 and batch: 400, loss is 4.188387393951416 and perplexity is 65.91640800831283
At time: 52.76281213760376 and batch: 450, loss is 4.143056206703186 and perplexity is 62.995053453837315
At time: 53.33041310310364 and batch: 500, loss is 4.123219289779663 and perplexity is 61.75773864114081
At time: 53.89875078201294 and batch: 550, loss is 4.146408343315125 and perplexity is 63.206575806732836
At time: 54.46851372718811 and batch: 600, loss is 4.233315119743347 and perplexity is 68.9454160463859
At time: 55.037696838378906 and batch: 650, loss is 4.121537489891052 and perplexity is 61.653961773596066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.56301820044424 and perplexity of 95.87240573015977
Finished 7 epochs...
Completing Train Step...
At time: 56.088281869888306 and batch: 50, loss is 4.129464406967163 and perplexity is 62.14462978902497
At time: 56.65558910369873 and batch: 100, loss is 4.110026597976685 and perplexity is 60.94833865055377
At time: 57.2244086265564 and batch: 150, loss is 4.066988701820374 and perplexity is 58.38089534557103
At time: 57.79203462600708 and batch: 200, loss is 4.010468859672546 and perplexity is 55.1727327695371
At time: 58.36093831062317 and batch: 250, loss is 4.005007791519165 and perplexity is 54.872251935607075
At time: 58.92909097671509 and batch: 300, loss is 4.1047519063949585 and perplexity is 60.627701336630956
At time: 59.49752068519592 and batch: 350, loss is 4.071988716125488 and perplexity is 58.67353164061279
At time: 60.0656304359436 and batch: 400, loss is 4.126773691177368 and perplexity is 61.97764101281832
At time: 60.63473629951477 and batch: 450, loss is 4.082858333587646 and perplexity is 59.314769174618824
At time: 61.202373027801514 and batch: 500, loss is 4.062024807929992 and perplexity is 58.0918168469269
At time: 61.7701518535614 and batch: 550, loss is 4.084885363578796 and perplexity is 59.43512393079411
At time: 62.33891201019287 and batch: 600, loss is 4.172691178321839 and perplexity is 64.88984748848296
At time: 62.91987442970276 and batch: 650, loss is 4.06137143611908 and perplexity is 58.0538736881994
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.552367565678615 and perplexity of 94.85672219107228
Finished 8 epochs...
Completing Train Step...
At time: 63.947595834732056 and batch: 50, loss is 4.06873330116272 and perplexity is 58.48283551371598
At time: 64.52908873558044 and batch: 100, loss is 4.051195702552795 and perplexity is 57.46612837842183
At time: 65.09754872322083 and batch: 150, loss is 4.009899215698242 and perplexity is 55.141312904687034
At time: 65.66514420509338 and batch: 200, loss is 3.951952109336853 and perplexity is 52.03684936961754
At time: 66.23390126228333 and batch: 250, loss is 3.946647047996521 and perplexity is 51.76152165269513
At time: 66.8020613193512 and batch: 300, loss is 4.0471555519104 and perplexity is 57.234424936513605
At time: 67.37043881416321 and batch: 350, loss is 4.017032299041748 and perplexity is 55.536046645945426
At time: 67.93747138977051 and batch: 400, loss is 4.072053751945496 and perplexity is 58.67734764594295
At time: 68.50573897361755 and batch: 450, loss is 4.029597334861755 and perplexity is 56.23826149996055
At time: 69.07482600212097 and batch: 500, loss is 4.008018751144409 and perplexity is 55.03771905312777
At time: 69.64307069778442 and batch: 550, loss is 4.03134298324585 and perplexity is 56.336519467251456
At time: 70.21067762374878 and batch: 600, loss is 4.119365696907043 and perplexity is 61.52020742792766
At time: 70.77837443351746 and batch: 650, loss is 4.008121161460877 and perplexity is 55.043355771977836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.546324486825981 and perplexity of 94.28522408243761
Finished 9 epochs...
Completing Train Step...
At time: 71.81534957885742 and batch: 50, loss is 4.015162439346313 and perplexity is 55.4322990576237
At time: 72.38231468200684 and batch: 100, loss is 3.999001865386963 and perplexity is 54.543680918065085
At time: 72.94987964630127 and batch: 150, loss is 3.9589701461791993 and perplexity is 52.403330380085215
At time: 73.51679277420044 and batch: 200, loss is 3.9006400394439695 and perplexity is 49.43407874261432
At time: 74.08521699905396 and batch: 250, loss is 3.894905476570129 and perplexity is 49.15140718351099
At time: 74.65215516090393 and batch: 300, loss is 3.996211357116699 and perplexity is 54.39168849206309
At time: 75.2193374633789 and batch: 350, loss is 3.967981595993042 and perplexity is 52.877694506035056
At time: 75.78615355491638 and batch: 400, loss is 4.023211288452148 and perplexity is 55.88026565769355
At time: 76.35516428947449 and batch: 450, loss is 3.981300196647644 and perplexity is 53.58666215117317
At time: 76.92327165603638 and batch: 500, loss is 3.959048070907593 and perplexity is 52.40741405447954
At time: 77.50383305549622 and batch: 550, loss is 3.983520259857178 and perplexity is 53.70576008188757
At time: 78.07176494598389 and batch: 600, loss is 4.071332383155823 and perplexity is 58.63503490207166
At time: 78.63974475860596 and batch: 650, loss is 3.959928550720215 and perplexity is 52.45357804483466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.543759514303768 and perplexity of 94.04369496361207
Finished 10 epochs...
Completing Train Step...
At time: 79.66164207458496 and batch: 50, loss is 3.9669667959213255 and perplexity is 52.82406143588254
At time: 80.24538135528564 and batch: 100, loss is 3.9521401071548463 and perplexity is 52.04663310338577
At time: 80.81225872039795 and batch: 150, loss is 3.9129765605926514 and perplexity is 50.04770049713859
At time: 81.37731909751892 and batch: 200, loss is 3.854901337623596 and perplexity is 47.2239574835534
At time: 81.94405174255371 and batch: 250, loss is 3.8480824708938597 and perplexity is 46.903039004668024
At time: 82.51056289672852 and batch: 300, loss is 3.950358591079712 and perplexity is 51.953993733620244
At time: 83.0761501789093 and batch: 350, loss is 3.9235993909835813 and perplexity is 50.58218256102647
At time: 83.64257264137268 and batch: 400, loss is 3.979246792793274 and perplexity is 53.476739988488994
At time: 84.20963835716248 and batch: 450, loss is 3.936983890533447 and perplexity is 51.263750810789965
At time: 84.77590203285217 and batch: 500, loss is 3.9149816131591795 and perplexity is 50.14814943651869
At time: 85.34165811538696 and batch: 550, loss is 3.939722819328308 and perplexity is 51.40435103312156
At time: 85.90774154663086 and batch: 600, loss is 4.027724623680115 and perplexity is 56.13304203240107
At time: 86.47382497787476 and batch: 650, loss is 3.915916085243225 and perplexity is 50.19503338469231
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5430534213197 and perplexity of 93.97731480844122
Finished 11 epochs...
Completing Train Step...
At time: 87.51635098457336 and batch: 50, loss is 3.923171591758728 and perplexity is 50.56054817045319
At time: 88.08408784866333 and batch: 100, loss is 3.9093914270401 and perplexity is 49.86859405894938
At time: 88.65188002586365 and batch: 150, loss is 3.8708118915557863 and perplexity is 47.98132590456056
At time: 89.21846199035645 and batch: 200, loss is 3.8130318450927736 and perplexity is 45.28753572603568
At time: 89.7857677936554 and batch: 250, loss is 3.8060076141357424 and perplexity is 44.97054024373516
At time: 90.35168814659119 and batch: 300, loss is 3.9081951427459716 and perplexity is 49.80897271225703
At time: 90.91792225837708 and batch: 350, loss is 3.882694296836853 and perplexity is 48.55486020018572
At time: 91.48697924613953 and batch: 400, loss is 3.938504419326782 and perplexity is 51.34175811109577
At time: 92.06823372840881 and batch: 450, loss is 3.896665306091309 and perplexity is 49.23798143649977
At time: 92.63744592666626 and batch: 500, loss is 3.874808735847473 and perplexity is 48.17348354939668
At time: 93.20655035972595 and batch: 550, loss is 3.899458131790161 and perplexity is 49.37568674036399
At time: 93.77504277229309 and batch: 600, loss is 3.987428641319275 and perplexity is 53.91607340362433
At time: 94.34400486946106 and batch: 650, loss is 3.8755766916275025 and perplexity is 48.21049286347205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.543985703412225 and perplexity of 94.06496902902208
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 95.35780549049377 and batch: 50, loss is 3.8971075344085695 and perplexity is 49.25976068152025
At time: 95.93836259841919 and batch: 100, loss is 3.8838433170318605 and perplexity is 48.61068277961541
At time: 96.50657796859741 and batch: 150, loss is 3.839390935897827 and perplexity is 46.49714607124327
At time: 97.0736951828003 and batch: 200, loss is 3.7753138732910156 and perplexity is 43.61119455689905
At time: 97.64069557189941 and batch: 250, loss is 3.7573322582244875 and perplexity is 42.83400336144578
At time: 98.20782279968262 and batch: 300, loss is 3.8512126445770263 and perplexity is 47.050083680646665
At time: 98.77535629272461 and batch: 350, loss is 3.812543263435364 and perplexity is 45.26541447123037
At time: 99.35138702392578 and batch: 400, loss is 3.8518300342559812 and perplexity is 47.0791408855913
At time: 99.91731810569763 and batch: 450, loss is 3.7979778337478636 and perplexity is 44.610882600079975
At time: 100.4851553440094 and batch: 500, loss is 3.757550849914551 and perplexity is 42.84336754206149
At time: 101.05271124839783 and batch: 550, loss is 3.76353898525238 and perplexity is 43.10068909445808
At time: 101.6202449798584 and batch: 600, loss is 3.827643256187439 and perplexity is 45.95410845303517
At time: 102.18725419044495 and batch: 650, loss is 3.6978137350082396 and perplexity is 40.35897242822276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5097799862132355 and perplexity of 90.90181665736195
Finished 13 epochs...
Completing Train Step...
At time: 103.22175025939941 and batch: 50, loss is 3.860005340576172 and perplexity is 47.465604861854935
At time: 103.7891206741333 and batch: 100, loss is 3.8424832916259763 and perplexity is 46.6411543348702
At time: 104.35674500465393 and batch: 150, loss is 3.799561700820923 and perplexity is 44.68159629389308
At time: 104.92617511749268 and batch: 200, loss is 3.7385044145584105 and perplexity is 42.03507608220616
At time: 105.52057790756226 and batch: 250, loss is 3.7229584646224976 and perplexity is 41.38665412053641
At time: 106.09864044189453 and batch: 300, loss is 3.8198871183395386 and perplexity is 45.59906073262907
At time: 106.69446659088135 and batch: 350, loss is 3.785396637916565 and perplexity is 44.053140239714786
At time: 107.26126790046692 and batch: 400, loss is 3.8293137407302855 and perplexity is 46.0309382345012
At time: 107.82776308059692 and batch: 450, loss is 3.7799203109741213 and perplexity is 43.81255021700991
At time: 108.39412665367126 and batch: 500, loss is 3.7438384819030763 and perplexity is 42.25989307029818
At time: 108.961758852005 and batch: 550, loss is 3.7564729118347167 and perplexity is 42.7972099267146
At time: 109.52877020835876 and batch: 600, loss is 3.829524817466736 and perplexity is 46.04065532020878
At time: 110.09571599960327 and batch: 650, loss is 3.708100228309631 and perplexity is 40.776267298642715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.508749269971661 and perplexity of 90.80817094791199
Finished 14 epochs...
Completing Train Step...
At time: 111.13752818107605 and batch: 50, loss is 3.8438211250305176 and perplexity is 46.70359418691278
At time: 111.72051095962524 and batch: 100, loss is 3.824411358833313 and perplexity is 45.805829232192
At time: 112.28781962394714 and batch: 150, loss is 3.7817632818222044 and perplexity is 43.893369921143744
At time: 112.85635280609131 and batch: 200, loss is 3.7209037017822264 and perplexity is 41.30170167002509
At time: 113.42449617385864 and batch: 250, loss is 3.7060798692703245 and perplexity is 40.69396776371591
At time: 113.99229431152344 and batch: 300, loss is 3.804233341217041 and perplexity is 44.89082097482459
At time: 114.5600848197937 and batch: 350, loss is 3.7713773107528685 and perplexity is 43.43985383017099
At time: 115.1282651424408 and batch: 400, loss is 3.817521433830261 and perplexity is 45.49131523719051
At time: 115.69771099090576 and batch: 450, loss is 3.7705167627334593 and perplexity is 43.40248782991869
At time: 116.26639580726624 and batch: 500, loss is 3.7364400100708006 and perplexity is 41.948388192726014
At time: 116.83354306221008 and batch: 550, loss is 3.7520866441726684 and perplexity is 42.60990100263372
At time: 117.40076279640198 and batch: 600, loss is 3.8291088771820068 and perplexity is 46.021509139035835
At time: 117.96905469894409 and batch: 650, loss is 3.710880088806152 and perplexity is 40.88977733123267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5092213948567705 and perplexity of 90.85105386743795
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 119.00597810745239 and batch: 50, loss is 3.838732442855835 and perplexity is 46.466538102756004
At time: 119.57544994354248 and batch: 100, loss is 3.820988631248474 and perplexity is 45.649316360191264
At time: 120.14323449134827 and batch: 150, loss is 3.779751811027527 and perplexity is 43.80516842657129
At time: 120.71208357810974 and batch: 200, loss is 3.7169579076766968 and perplexity is 41.13905475566985
At time: 121.2936463356018 and batch: 250, loss is 3.697799286842346 and perplexity is 40.35838931930627
At time: 121.86134505271912 and batch: 300, loss is 3.795086631774902 and perplexity is 44.4820898010039
At time: 122.42988348007202 and batch: 350, loss is 3.7609629392623902 and perplexity is 42.98980262279616
At time: 122.998051404953 and batch: 400, loss is 3.802706813812256 and perplexity is 44.822346184002754
At time: 123.56614446640015 and batch: 450, loss is 3.7529783630371094 and perplexity is 42.64791400110477
At time: 124.13364934921265 and batch: 500, loss is 3.713919382095337 and perplexity is 41.01424240419212
At time: 124.70282053947449 and batch: 550, loss is 3.725109920501709 and perplexity is 41.47579153408816
At time: 125.27060794830322 and batch: 600, loss is 3.7966555643081663 and perplexity is 44.551933974911556
At time: 125.83966088294983 and batch: 650, loss is 3.676673812866211 and perplexity is 39.51474181810285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5048828125 and perplexity of 90.45774291097345
Finished 16 epochs...
Completing Train Step...
At time: 126.86167120933533 and batch: 50, loss is 3.8324652433395388 and perplexity is 46.17623368569458
At time: 127.44152545928955 and batch: 100, loss is 3.81347309589386 and perplexity is 45.30752329689555
At time: 128.00875544548035 and batch: 150, loss is 3.771816987991333 and perplexity is 43.45895754456878
At time: 128.57615327835083 and batch: 200, loss is 3.7101406049728394 and perplexity is 40.85955117920125
At time: 129.14378261566162 and batch: 250, loss is 3.6919807720184328 and perplexity is 40.124245278649106
At time: 129.71238470077515 and batch: 300, loss is 3.7897519493103027 and perplexity is 44.24542380678805
At time: 130.2793471813202 and batch: 350, loss is 3.755874443054199 and perplexity is 42.771604795380306
At time: 130.8472981452942 and batch: 400, loss is 3.799139904975891 and perplexity is 44.6627537563583
At time: 131.41461324691772 and batch: 450, loss is 3.7507232427597046 and perplexity is 42.551846188401356
At time: 131.9833219051361 and batch: 500, loss is 3.7128991746902464 and perplexity is 40.972420707406975
At time: 132.55109691619873 and batch: 550, loss is 3.725700855255127 and perplexity is 41.50030826391158
At time: 133.12000560760498 and batch: 600, loss is 3.7989043235778808 and perplexity is 44.652233281652116
At time: 133.6876561641693 and batch: 650, loss is 3.6798780822753905 and perplexity is 39.641560769056966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.504127352845435 and perplexity of 90.3894315422516
Finished 17 epochs...
Completing Train Step...
At time: 134.72810554504395 and batch: 50, loss is 3.829458680152893 and perplexity is 46.03761041563035
At time: 135.2951157093048 and batch: 100, loss is 3.8096812677383425 and perplexity is 45.136050258002676
At time: 135.8735809326172 and batch: 150, loss is 3.7677752780914306 and perplexity is 43.28366352802752
At time: 136.44079327583313 and batch: 200, loss is 3.706269063949585 and perplexity is 40.70166757425343
At time: 137.00712847709656 and batch: 250, loss is 3.688583459854126 and perplexity is 39.988161981686744
At time: 137.57414078712463 and batch: 300, loss is 3.7866011667251587 and perplexity is 44.10623548720053
At time: 138.14123368263245 and batch: 350, loss is 3.7530379486083985 and perplexity is 42.650455277135755
At time: 138.70810437202454 and batch: 400, loss is 3.797131733894348 and perplexity is 44.57315330247427
At time: 139.27644634246826 and batch: 450, loss is 3.7495063829421995 and perplexity is 42.5000980481158
At time: 139.84182691574097 and batch: 500, loss is 3.712378168106079 and perplexity is 40.951079366421475
At time: 140.40746521949768 and batch: 550, loss is 3.7261282539367677 and perplexity is 41.51804923191437
At time: 140.9750566482544 and batch: 600, loss is 3.8002544164657595 and perplexity is 44.71255865750085
At time: 141.5414273738861 and batch: 650, loss is 3.6816594982147217 and perplexity is 39.71224181474978
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503912233838848 and perplexity of 90.36998914882146
Finished 18 epochs...
Completing Train Step...
At time: 142.56968140602112 and batch: 50, loss is 3.826934423446655 and perplexity is 45.921546218342556
At time: 143.14962005615234 and batch: 100, loss is 3.8067338180541994 and perplexity is 45.003209887251096
At time: 143.71605920791626 and batch: 150, loss is 3.764762969017029 and perplexity is 43.153475936683655
At time: 144.2836835384369 and batch: 200, loss is 3.703348445892334 and perplexity is 40.582966972912104
At time: 144.85107326507568 and batch: 250, loss is 3.685962052345276 and perplexity is 39.88347398849794
At time: 145.41831970214844 and batch: 300, loss is 3.784182658195496 and perplexity is 43.99969306926886
At time: 145.98579263687134 and batch: 350, loss is 3.7509234142303467 and perplexity is 42.56036470658514
At time: 146.55412077903748 and batch: 400, loss is 3.7955708169937132 and perplexity is 44.50363258631562
At time: 147.12141823768616 and batch: 450, loss is 3.748527455329895 and perplexity is 42.45851388587401
At time: 147.68767476081848 and batch: 500, loss is 3.7118528175354 and perplexity is 40.92957134362711
At time: 148.25506448745728 and batch: 550, loss is 3.7262881898880007 and perplexity is 41.524689991645566
At time: 148.82264113426208 and batch: 600, loss is 3.801065182685852 and perplexity is 44.74882478936498
At time: 149.39044857025146 and batch: 650, loss is 3.682771339416504 and perplexity is 39.7564200764693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503887101715686 and perplexity of 90.36771798766368
Finished 19 epochs...
Completing Train Step...
At time: 150.42509055137634 and batch: 50, loss is 3.8246689939498903 and perplexity is 45.81763194267723
At time: 150.99186301231384 and batch: 100, loss is 3.804198741912842 and perplexity is 44.8892678105233
At time: 151.55879616737366 and batch: 150, loss is 3.7622377490997314 and perplexity is 43.0446413931603
At time: 152.12501335144043 and batch: 200, loss is 3.7008935546875 and perplexity is 40.483462390658104
At time: 152.69302582740784 and batch: 250, loss is 3.6837203454971315 and perplexity is 39.79416706909566
At time: 153.26058888435364 and batch: 300, loss is 3.782120862007141 and perplexity is 43.90906812699405
At time: 153.82633233070374 and batch: 350, loss is 3.749132776260376 and perplexity is 42.484222693260854
At time: 154.39331555366516 and batch: 400, loss is 3.794201192855835 and perplexity is 44.44272105937277
At time: 154.959055185318 and batch: 450, loss is 3.7476215076446535 and perplexity is 42.420066111965845
At time: 155.52514481544495 and batch: 500, loss is 3.711278920173645 and perplexity is 40.90608870957061
At time: 156.0924472808838 and batch: 550, loss is 3.7262567043304444 and perplexity is 41.52338258421116
At time: 156.66043066978455 and batch: 600, loss is 3.8015536069869995 and perplexity is 44.770686541310745
At time: 157.22740745544434 and batch: 650, loss is 3.6835080194473266 and perplexity is 39.78571862774044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503947837679994 and perplexity of 90.37320672483813
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 158.2526297569275 and batch: 50, loss is 3.823660092353821 and perplexity is 45.77142977132338
At time: 158.8318121433258 and batch: 100, loss is 3.803366117477417 and perplexity is 44.851907464982624
At time: 159.40165829658508 and batch: 150, loss is 3.7617641401290896 and perplexity is 43.02425989167038
At time: 159.9691560268402 and batch: 200, loss is 3.7002275371551514 and perplexity is 40.45650867175676
At time: 160.53566551208496 and batch: 250, loss is 3.6822772359848024 and perplexity is 39.73678114510859
At time: 161.10295748710632 and batch: 300, loss is 3.780303931236267 and perplexity is 43.82936082324786
At time: 161.6704843044281 and batch: 350, loss is 3.747080078125 and perplexity is 42.397104852459954
At time: 162.23724174499512 and batch: 400, loss is 3.7918135213851927 and perplexity is 44.33673302487761
At time: 162.80525970458984 and batch: 450, loss is 3.744257035255432 and perplexity is 42.277584792419475
At time: 163.37307715415955 and batch: 500, loss is 3.7063792181015014 and perplexity is 40.70615127887146
At time: 163.93950390815735 and batch: 550, loss is 3.7208670663833616 and perplexity is 41.30018859342687
At time: 164.5194661617279 and batch: 600, loss is 3.7955976486206056 and perplexity is 44.50482670720056
At time: 165.08653450012207 and batch: 650, loss is 3.6776653814315794 and perplexity is 39.553942825990006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.50383025524663 and perplexity of 90.36258104798921
Finished 21 epochs...
Completing Train Step...
At time: 166.12100887298584 and batch: 50, loss is 3.823088150024414 and perplexity is 45.74525863806257
At time: 166.68788194656372 and batch: 100, loss is 3.80267587184906 and perplexity is 44.820959314073114
At time: 167.25561952590942 and batch: 150, loss is 3.7610016584396364 and perplexity is 42.99146718480872
At time: 167.82698893547058 and batch: 200, loss is 3.6996283435821535 and perplexity is 40.43227465293394
At time: 168.39709210395813 and batch: 250, loss is 3.6817712068557737 and perplexity is 39.71667826310623
At time: 168.96719622612 and batch: 300, loss is 3.7798424482345583 and perplexity is 43.80913898462844
At time: 169.53726816177368 and batch: 350, loss is 3.7466219758987425 and perplexity is 42.37768709233931
At time: 170.1099088191986 and batch: 400, loss is 3.791447067260742 and perplexity is 44.320488622791835
At time: 170.68919348716736 and batch: 450, loss is 3.7440634965896606 and perplexity is 42.269403236815926
At time: 171.2711443901062 and batch: 500, loss is 3.706323571205139 and perplexity is 40.703886170913634
At time: 171.85361981391907 and batch: 550, loss is 3.7210369300842285 and perplexity is 41.30720459217273
At time: 172.432959318161 and batch: 600, loss is 3.7958641719818114 and perplexity is 44.51668986403844
At time: 173.0247757434845 and batch: 650, loss is 3.6779816246032713 and perplexity is 39.566453468420576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503749772614124 and perplexity of 90.35530872223832
Finished 22 epochs...
Completing Train Step...
At time: 174.16293382644653 and batch: 50, loss is 3.822602744102478 and perplexity is 45.723059006971255
At time: 174.75735592842102 and batch: 100, loss is 3.8021380949020385 and perplexity is 44.796862115450324
At time: 175.32778549194336 and batch: 150, loss is 3.7604343557357787 and perplexity is 42.96708492594657
At time: 175.89964771270752 and batch: 200, loss is 3.6991169834136963 and perplexity is 40.411604483557376
At time: 176.46679830551147 and batch: 250, loss is 3.681340842247009 and perplexity is 39.69958928791292
At time: 177.04283452033997 and batch: 300, loss is 3.7794581508636473 and perplexity is 43.79230648224514
At time: 177.62565064430237 and batch: 350, loss is 3.746258602142334 and perplexity is 42.362290950439025
At time: 178.19627571105957 and batch: 400, loss is 3.7911781597137453 and perplexity is 44.30857211120634
At time: 178.77280616760254 and batch: 450, loss is 3.7439299821853638 and perplexity is 42.26376003935523
At time: 179.3695068359375 and batch: 500, loss is 3.7063107252120973 and perplexity is 40.703363292433565
At time: 179.94613933563232 and batch: 550, loss is 3.721178722381592 and perplexity is 41.31306205087097
At time: 180.52658009529114 and batch: 600, loss is 3.796098914146423 and perplexity is 44.527141034795704
At time: 181.12116718292236 and batch: 650, loss is 3.6782410287857057 and perplexity is 39.576718503273305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503685446346507 and perplexity of 90.34949668940406
Finished 23 epochs...
Completing Train Step...
At time: 182.18582224845886 and batch: 50, loss is 3.82216872215271 and perplexity is 45.70321850167054
At time: 182.76686811447144 and batch: 100, loss is 3.8016667413711547 and perplexity is 44.77575193189032
At time: 183.34103298187256 and batch: 150, loss is 3.759943971633911 and perplexity is 42.94601971603945
At time: 183.91482043266296 and batch: 200, loss is 3.6986578130722045 and perplexity is 40.39305293281332
At time: 184.48609447479248 and batch: 250, loss is 3.68094810962677 and perplexity is 39.68400102539942
At time: 185.06089758872986 and batch: 300, loss is 3.7791106605529787 and perplexity is 43.77709172370441
At time: 185.63622188568115 and batch: 350, loss is 3.7459411144256594 and perplexity is 42.34884357821278
At time: 186.20712614059448 and batch: 400, loss is 3.7909510421752928 and perplexity is 44.298510000060304
At time: 186.78261303901672 and batch: 450, loss is 3.743815875053406 and perplexity is 42.25893771804724
At time: 187.36498022079468 and batch: 500, loss is 3.706303095817566 and perplexity is 40.70305275160088
At time: 187.93868899345398 and batch: 550, loss is 3.7212950801849365 and perplexity is 41.31786942770312
At time: 188.50993990898132 and batch: 600, loss is 3.796301817893982 and perplexity is 44.5361766752314
At time: 189.08853697776794 and batch: 650, loss is 3.6784601974487305 and perplexity is 39.58539343035594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503638174019608 and perplexity of 90.34522575941058
Finished 24 epochs...
Completing Train Step...
At time: 190.2099006175995 and batch: 50, loss is 3.821769804954529 and perplexity is 45.684990337803754
At time: 190.80145406723022 and batch: 100, loss is 3.8012345695495604 and perplexity is 44.75640529445155
At time: 191.38013434410095 and batch: 150, loss is 3.7594996213912966 and perplexity is 42.92694088091574
At time: 191.96585083007812 and batch: 200, loss is 3.6982353830337527 and perplexity is 40.37599329741463
At time: 192.53976273536682 and batch: 250, loss is 3.6805821561813357 and perplexity is 39.669481185450316
At time: 193.11402988433838 and batch: 300, loss is 3.7787870168685913 and perplexity is 43.762925836920736
At time: 193.68080472946167 and batch: 350, loss is 3.745651364326477 and perplexity is 42.33657477411517
At time: 194.26372504234314 and batch: 400, loss is 3.7907466220855714 and perplexity is 44.28945542017212
At time: 194.83053159713745 and batch: 450, loss is 3.7437091159820555 and perplexity is 42.25442643391475
At time: 195.39746642112732 and batch: 500, loss is 3.7062906789779664 and perplexity is 40.702547351461384
At time: 195.9644947052002 and batch: 550, loss is 3.721390519142151 and perplexity is 41.32181295025548
At time: 196.532124042511 and batch: 600, loss is 3.796477408409119 and perplexity is 44.543997492046564
At time: 197.098464012146 and batch: 650, loss is 3.67864933013916 and perplexity is 39.59288103036978
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5036061604817705 and perplexity of 90.3423335354027
Finished 25 epochs...
Completing Train Step...
At time: 198.14379358291626 and batch: 50, loss is 3.8213959550857544 and perplexity is 45.66791420231539
At time: 198.71049284934998 and batch: 100, loss is 3.800829210281372 and perplexity is 44.73826654735765
At time: 199.2781045436859 and batch: 150, loss is 3.7590866756439207 and perplexity is 42.90921804276828
At time: 199.8455729484558 and batch: 200, loss is 3.6978400993347167 and perplexity is 40.36003647937458
At time: 200.41374349594116 and batch: 250, loss is 3.6802363920211794 and perplexity is 39.65576727163097
At time: 200.98114109039307 and batch: 300, loss is 3.778480539321899 and perplexity is 43.74951553785691
At time: 201.54906463623047 and batch: 350, loss is 3.74538028717041 and perplexity is 42.32509985119279
At time: 202.11588406562805 and batch: 400, loss is 3.790555877685547 and perplexity is 44.281008260220425
At time: 202.68195700645447 and batch: 450, loss is 3.743605737686157 and perplexity is 42.25005846909612
At time: 203.24807476997375 and batch: 500, loss is 3.706271252632141 and perplexity is 40.70175665738074
At time: 203.81403994560242 and batch: 550, loss is 3.7214682149887084 and perplexity is 41.32502360821971
At time: 204.38009095191956 and batch: 600, loss is 3.7966301822662354 and perplexity is 44.55080317020643
At time: 204.94626379013062 and batch: 650, loss is 3.6788155841827392 and perplexity is 39.599464054150026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5035858154296875 and perplexity of 90.3404955346188
Finished 26 epochs...
Completing Train Step...
At time: 205.97171878814697 and batch: 50, loss is 3.821040458679199 and perplexity is 45.65168230828253
At time: 206.55111122131348 and batch: 100, loss is 3.800443754196167 and perplexity is 44.7210252333736
At time: 207.11751747131348 and batch: 150, loss is 3.7586974191665647 and perplexity is 42.89251860210044
At time: 207.68415188789368 and batch: 200, loss is 3.697465753555298 and perplexity is 40.34493069763056
At time: 208.25108909606934 and batch: 250, loss is 3.679906792640686 and perplexity is 39.64269890908565
At time: 208.83105325698853 and batch: 300, loss is 3.77818724155426 and perplexity is 43.73668578417539
At time: 209.39763975143433 and batch: 350, loss is 3.745122790336609 and perplexity is 42.31420267504518
At time: 209.96436166763306 and batch: 400, loss is 3.790374422073364 and perplexity is 44.27297395171573
At time: 210.53185081481934 and batch: 450, loss is 3.7435034227371218 and perplexity is 42.24573587765374
At time: 211.098082780838 and batch: 500, loss is 3.7062444734573363 and perplexity is 40.70066671251833
At time: 211.66520428657532 and batch: 550, loss is 3.7215313720703125 and perplexity is 41.32763365852874
At time: 212.23213291168213 and batch: 600, loss is 3.7967639780044555 and perplexity is 44.556764276581305
At time: 212.79928612709045 and batch: 650, loss is 3.6789633512496946 and perplexity is 39.60531598315686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503572052600337 and perplexity of 90.33925220235119
Finished 27 epochs...
Completing Train Step...
At time: 213.83940243721008 and batch: 50, loss is 3.820699110031128 and perplexity is 45.63610182758373
At time: 214.40547156333923 and batch: 100, loss is 3.800073938369751 and perplexity is 44.70448974819927
At time: 214.97363924980164 and batch: 150, loss is 3.758326473236084 and perplexity is 42.87661074753628
At time: 215.54570531845093 and batch: 200, loss is 3.6971082735061644 and perplexity is 40.33051076739473
At time: 216.11075377464294 and batch: 250, loss is 3.679590148925781 and perplexity is 39.63014828477725
At time: 216.67626643180847 and batch: 300, loss is 3.777904362678528 and perplexity is 43.72431534942235
At time: 217.24096131324768 and batch: 350, loss is 3.7448755741119384 and perplexity is 42.303743210537846
At time: 217.805757522583 and batch: 400, loss is 3.7901994085311888 and perplexity is 44.26522625971708
At time: 218.36959648132324 and batch: 450, loss is 3.7434015893936157 and perplexity is 42.24143407215779
At time: 218.9332685470581 and batch: 500, loss is 3.7062110805511477 and perplexity is 40.699307621665106
At time: 219.50313138961792 and batch: 550, loss is 3.7215822982788085 and perplexity is 41.32973837180916
At time: 220.07845449447632 and batch: 600, loss is 3.796881790161133 and perplexity is 44.56201391430479
At time: 220.65787982940674 and batch: 650, loss is 3.6790961837768554 and perplexity is 39.610577206790985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503565769569547 and perplexity of 90.3386845998312
Finished 28 epochs...
Completing Train Step...
At time: 221.68335819244385 and batch: 50, loss is 3.8203693056106567 and perplexity is 45.62105332113629
At time: 222.2615647315979 and batch: 100, loss is 3.7997171592712404 and perplexity is 44.68854296555606
At time: 222.82770586013794 and batch: 150, loss is 3.7579705047607423 and perplexity is 42.86135074198226
At time: 223.4053132534027 and batch: 200, loss is 3.6967643976211546 and perplexity is 40.316644461592425
At time: 223.97097635269165 and batch: 250, loss is 3.679283981323242 and perplexity is 39.618016674536406
At time: 224.53701400756836 and batch: 300, loss is 3.7776301956176757 and perplexity is 43.71232922557047
At time: 225.10350799560547 and batch: 350, loss is 3.7446366357803345 and perplexity is 42.293636432210974
At time: 225.66853761672974 and batch: 400, loss is 3.7900290632247926 and perplexity is 44.25768652838433
At time: 226.2340452671051 and batch: 450, loss is 3.7432996940612795 and perplexity is 42.2371300864764
At time: 226.80014419555664 and batch: 500, loss is 3.7061714839935305 and perplexity is 40.69769610109143
At time: 227.36666321754456 and batch: 550, loss is 3.721622633934021 and perplexity is 41.331405467507615
At time: 227.93246841430664 and batch: 600, loss is 3.7969861793518067 and perplexity is 44.566665949678985
At time: 228.49915623664856 and batch: 650, loss is 3.6792162656784058 and perplexity is 39.6153340058205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503563076842065 and perplexity of 90.33844134270001
Finished 29 epochs...
Completing Train Step...
At time: 229.5294370651245 and batch: 50, loss is 3.820049138069153 and perplexity is 45.6064492786486
At time: 230.09788012504578 and batch: 100, loss is 3.799370970726013 and perplexity is 44.673074981452416
At time: 230.66717624664307 and batch: 150, loss is 3.757626929283142 and perplexity is 42.84662716240577
At time: 231.23515439033508 and batch: 200, loss is 3.6964321088790895 and perplexity is 40.30324992007092
At time: 231.8035442829132 and batch: 250, loss is 3.67898681640625 and perplexity is 39.606245339000516
At time: 232.37190079689026 and batch: 300, loss is 3.777363171577454 and perplexity is 43.70065854105916
At time: 232.94074392318726 and batch: 350, loss is 3.744404411315918 and perplexity is 42.28381595546389
At time: 233.50797963142395 and batch: 400, loss is 3.789862284660339 and perplexity is 44.25030591044064
At time: 234.07627511024475 and batch: 450, loss is 3.7431973028182983 and perplexity is 42.232805595625656
At time: 234.64366102218628 and batch: 500, loss is 3.7061265230178835 and perplexity is 40.6958663341025
At time: 235.21148943901062 and batch: 550, loss is 3.7216541051864622 and perplexity is 41.332706239071186
At time: 235.7797544002533 and batch: 600, loss is 3.7970789670944214 and perplexity is 44.57080138186406
At time: 236.3477373123169 and batch: 650, loss is 3.679325819015503 and perplexity is 39.619674235600044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503564572801777 and perplexity of 90.33857648546976
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 237.3719449043274 and batch: 50, loss is 3.8198881769180297 and perplexity is 45.599109002839526
At time: 237.95301055908203 and batch: 100, loss is 3.7992107105255126 and perplexity is 44.665916239144984
At time: 238.52086329460144 and batch: 150, loss is 3.75747829914093 and perplexity is 42.84025933535456
At time: 239.08853697776794 and batch: 200, loss is 3.6963112258911135 and perplexity is 40.29837823725316
At time: 239.65584921836853 and batch: 250, loss is 3.678731770515442 and perplexity is 39.59614521692855
At time: 240.22357416152954 and batch: 300, loss is 3.7770560598373413 and perplexity is 43.687239616430666
At time: 240.79039359092712 and batch: 350, loss is 3.7440757608413695 and perplexity is 42.269921642595726
At time: 241.35808968544006 and batch: 400, loss is 3.789443111419678 and perplexity is 44.23176125329545
At time: 241.92710041999817 and batch: 450, loss is 3.7426764249801634 and perplexity is 42.21081319132388
At time: 242.49577713012695 and batch: 500, loss is 3.7053894281387327 and perplexity is 40.66588067192168
At time: 243.06553483009338 and batch: 550, loss is 3.720812511444092 and perplexity is 41.29793552560488
At time: 243.63468527793884 and batch: 600, loss is 3.7962051820755005 and perplexity is 44.53187309328978
At time: 244.20304536819458 and batch: 650, loss is 3.678490552902222 and perplexity is 39.58659508116338
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503557990579044 and perplexity of 90.337981858795
Finished 31 epochs...
Completing Train Step...
At time: 245.2538878917694 and batch: 50, loss is 3.819837741851807 and perplexity is 45.59680926675142
At time: 245.81988263130188 and batch: 100, loss is 3.7991581392288207 and perplexity is 44.663568155731795
At time: 246.38634729385376 and batch: 150, loss is 3.757422924041748 and perplexity is 42.837887117426384
At time: 246.952378988266 and batch: 200, loss is 3.696258044242859 and perplexity is 40.29623516006321
At time: 247.51888155937195 and batch: 250, loss is 3.6786884689331054 and perplexity is 39.59443067830761
At time: 248.08556294441223 and batch: 300, loss is 3.7770163774490357 and perplexity is 43.685506036820726
At time: 248.65204429626465 and batch: 350, loss is 3.744039158821106 and perplexity is 42.268374506381555
At time: 249.2187876701355 and batch: 400, loss is 3.789418420791626 and perplexity is 44.230669156812596
At time: 249.78482604026794 and batch: 450, loss is 3.742664213180542 and perplexity is 42.21029772447873
At time: 250.3510844707489 and batch: 500, loss is 3.7053862190246583 and perplexity is 40.66575017068106
At time: 250.9172945022583 and batch: 550, loss is 3.720828204154968 and perplexity is 41.29858360725192
At time: 251.48242616653442 and batch: 600, loss is 3.79622031211853 and perplexity is 44.532546867542976
At time: 252.06030464172363 and batch: 650, loss is 3.6785102701187133 and perplexity is 39.587375626323826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503553203507965 and perplexity of 90.33754940548981
Finished 32 epochs...
Completing Train Step...
At time: 253.08604049682617 and batch: 50, loss is 3.819787836074829 and perplexity is 45.594533779337716
At time: 253.6656198501587 and batch: 100, loss is 3.7991061305999754 and perplexity is 44.66124532519681
At time: 254.23304796218872 and batch: 150, loss is 3.7573691701889036 and perplexity is 42.83558447783454
At time: 254.80002284049988 and batch: 200, loss is 3.6962060070037843 and perplexity is 40.294138309798
At time: 255.36607789993286 and batch: 250, loss is 3.6786458587646482 and perplexity is 39.59274358889028
At time: 255.93442034721375 and batch: 300, loss is 3.7769775009155273 and perplexity is 43.68380772879384
At time: 256.50779604911804 and batch: 350, loss is 3.7440036725997925 and perplexity is 42.26687458810264
At time: 257.08831334114075 and batch: 400, loss is 3.789394612312317 and perplexity is 44.229616104376994
At time: 257.65564727783203 and batch: 450, loss is 3.7426521492004396 and perplexity is 42.20978850335848
At time: 258.2233328819275 and batch: 500, loss is 3.7053825569152834 and perplexity is 40.6656012485288
At time: 258.7897458076477 and batch: 550, loss is 3.7208417892456054 and perplexity is 41.299144656064364
At time: 259.35773825645447 and batch: 600, loss is 3.7962355184555054 and perplexity is 44.53322404960573
At time: 259.9238998889923 and batch: 650, loss is 3.678529667854309 and perplexity is 39.58814353921701
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503549314012714 and perplexity of 90.3371980387037
Finished 33 epochs...
Completing Train Step...
At time: 260.96019196510315 and batch: 50, loss is 3.819738955497742 and perplexity is 45.59230514668343
At time: 261.52652406692505 and batch: 100, loss is 3.799054727554321 and perplexity is 44.65894966016698
At time: 262.09229946136475 and batch: 150, loss is 3.75731662273407 and perplexity is 42.83333363603244
At time: 262.6581106185913 and batch: 200, loss is 3.6961549615859983 and perplexity is 40.29208153116866
At time: 263.224374294281 and batch: 250, loss is 3.6786034440994264 and perplexity is 39.591064311538986
At time: 263.7900321483612 and batch: 300, loss is 3.7769392681121827 and perplexity is 43.68213760629053
At time: 264.3570535182953 and batch: 350, loss is 3.7439689874649047 and perplexity is 42.26540858128073
At time: 264.92333459854126 and batch: 400, loss is 3.789371075630188 and perplexity is 44.22857509821302
At time: 265.4896025657654 and batch: 450, loss is 3.742640118598938 and perplexity is 42.20928069726814
At time: 266.0550711154938 and batch: 500, loss is 3.705378541946411 and perplexity is 40.665437977733376
At time: 266.63484168052673 and batch: 550, loss is 3.7208539247512817 and perplexity is 41.29964584510985
At time: 267.20327162742615 and batch: 600, loss is 3.796250786781311 and perplexity is 44.533904002570544
At time: 267.7708077430725 and batch: 650, loss is 3.6785486555099487 and perplexity is 39.58889523239037
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503544826133578 and perplexity of 90.33679261718717
Finished 34 epochs...
Completing Train Step...
At time: 268.7932860851288 and batch: 50, loss is 3.8196905136108397 and perplexity is 45.59009662288686
At time: 269.37261390686035 and batch: 100, loss is 3.799003772735596 and perplexity is 44.656674129457706
At time: 269.9400637149811 and batch: 150, loss is 3.7572650718688965 and perplexity is 42.83112559753887
At time: 270.508926153183 and batch: 200, loss is 3.6961049032211304 and perplexity is 40.290064625932
At time: 271.0771074295044 and batch: 250, loss is 3.678561415672302 and perplexity is 39.58940039634391
At time: 271.64506101608276 and batch: 300, loss is 3.7769013833999634 and perplexity is 43.68048275242522
At time: 272.2130057811737 and batch: 350, loss is 3.743934931755066 and perplexity is 42.263969227299114
At time: 272.78058671951294 and batch: 400, loss is 3.7893476772308348 and perplexity is 44.2275402324572
At time: 273.346791267395 and batch: 450, loss is 3.7426278829574584 and perplexity is 42.208764242802005
At time: 273.91372299194336 and batch: 500, loss is 3.705374307632446 and perplexity is 40.66526578786601
At time: 274.4808621406555 and batch: 550, loss is 3.720864863395691 and perplexity is 41.30009760972082
At time: 275.04856419563293 and batch: 600, loss is 3.7962659549713136 and perplexity is 44.53457950641109
At time: 275.6167182922363 and batch: 650, loss is 3.6785673809051516 and perplexity is 39.58963655704002
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503542731789982 and perplexity of 90.33660342110217
Finished 35 epochs...
Completing Train Step...
At time: 276.66057777404785 and batch: 50, loss is 3.819642786979675 and perplexity is 45.58792081308304
At time: 277.22815132141113 and batch: 100, loss is 3.7989533281326295 and perplexity is 44.65442149807847
At time: 277.7972638607025 and batch: 150, loss is 3.757214345932007 and perplexity is 42.8289530036688
At time: 278.36584210395813 and batch: 200, loss is 3.696055507659912 and perplexity is 40.288074524729765
At time: 278.9345223903656 and batch: 250, loss is 3.678519687652588 and perplexity is 39.587748443530295
At time: 279.5031089782715 and batch: 300, loss is 3.7768639183044432 and perplexity is 43.678846289621845
At time: 280.07428336143494 and batch: 350, loss is 3.74390145778656 and perplexity is 42.26255450820253
At time: 280.64252495765686 and batch: 400, loss is 3.789324736595154 and perplexity is 44.22652563620745
At time: 281.223512172699 and batch: 450, loss is 3.7426156854629515 and perplexity is 42.208249404771884
At time: 281.79115176200867 and batch: 500, loss is 3.7053697729110717 and perplexity is 40.66508138263416
At time: 282.3598039150238 and batch: 550, loss is 3.7208748626708985 and perplexity is 41.30051058282763
At time: 282.92743253707886 and batch: 600, loss is 3.7962809705734255 and perplexity is 44.535248224957776
At time: 283.494101524353 and batch: 650, loss is 3.6785857629776 and perplexity is 39.59036430329615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503539739870558 and perplexity of 90.336333141668
Finished 36 epochs...
Completing Train Step...
At time: 284.51473331451416 and batch: 50, loss is 3.8195954608917235 and perplexity is 45.58576336618527
At time: 285.09429574012756 and batch: 100, loss is 3.798903374671936 and perplexity is 44.652190910902604
At time: 285.6614074707031 and batch: 150, loss is 3.7571641063690184 and perplexity is 42.82680134983617
At time: 286.22815227508545 and batch: 200, loss is 3.6960068845748903 and perplexity is 40.286115641880635
At time: 286.7950248718262 and batch: 250, loss is 3.6784782886505125 and perplexity is 39.58610958417413
At time: 287.36216163635254 and batch: 300, loss is 3.7768268966674805 and perplexity is 43.67722925716434
At time: 287.9281415939331 and batch: 350, loss is 3.7438682746887206 and perplexity is 42.26115212898912
At time: 288.49535608291626 and batch: 400, loss is 3.789301748275757 and perplexity is 44.225508954396254
At time: 289.0617711544037 and batch: 450, loss is 3.7426034307479856 and perplexity is 42.20773215787558
At time: 289.6273281574249 and batch: 500, loss is 3.7053652381896973 and perplexity is 40.66489697823854
At time: 290.1934552192688 and batch: 550, loss is 3.7208840894699096 and perplexity is 41.30089165609587
At time: 290.7611138820648 and batch: 600, loss is 3.796295690536499 and perplexity is 44.53590378699202
At time: 291.3304100036621 and batch: 650, loss is 3.678603811264038 and perplexity is 39.591078847979404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503537047143076 and perplexity of 90.33608989086864
Finished 37 epochs...
Completing Train Step...
At time: 292.3711133003235 and batch: 50, loss is 3.8195487213134767 and perplexity is 45.58363275662376
At time: 292.9388074874878 and batch: 100, loss is 3.7988538551330566 and perplexity is 44.64997980974556
At time: 293.50780153274536 and batch: 150, loss is 3.7571144104003906 and perplexity is 42.82467308334343
At time: 294.0759494304657 and batch: 200, loss is 3.695958623886108 and perplexity is 40.28417145310572
At time: 294.64475083351135 and batch: 250, loss is 3.6784371662139894 and perplexity is 39.584481740366236
At time: 295.2132649421692 and batch: 300, loss is 3.776789975166321 and perplexity is 43.675616658063646
At time: 295.7931251525879 and batch: 350, loss is 3.743835530281067 and perplexity is 42.25976833525176
At time: 296.36137294769287 and batch: 400, loss is 3.7892789697647093 and perplexity is 44.2245015746253
At time: 296.9297716617584 and batch: 450, loss is 3.7425908994674684 and perplexity is 42.207203244257904
At time: 297.49694299697876 and batch: 500, loss is 3.7053604555130004 and perplexity is 40.66470249164846
At time: 298.0657708644867 and batch: 550, loss is 3.7208927154541014 and perplexity is 41.301247918470956
At time: 298.6336817741394 and batch: 600, loss is 3.7963103580474855 and perplexity is 44.53655702264078
At time: 299.2017421722412 and batch: 650, loss is 3.678621196746826 and perplexity is 39.59176716398262
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503534354415595 and perplexity of 90.33584664072437
Finished 38 epochs...
Completing Train Step...
At time: 300.22743701934814 and batch: 50, loss is 3.8195024967193603 and perplexity is 45.58152572040005
At time: 300.8070402145386 and batch: 100, loss is 3.798804717063904 and perplexity is 44.64778584985392
At time: 301.3743386268616 and batch: 150, loss is 3.757065329551697 and perplexity is 42.82257126362344
At time: 301.941771030426 and batch: 200, loss is 3.6959112024307252 and perplexity is 40.28226116436121
At time: 302.509658575058 and batch: 250, loss is 3.6783961772918703 and perplexity is 39.582859248379386
At time: 303.0764276981354 and batch: 300, loss is 3.7767534017562867 and perplexity is 43.67401932103732
At time: 303.6428258419037 and batch: 350, loss is 3.74380295753479 and perplexity is 42.25839184095829
At time: 304.2098445892334 and batch: 400, loss is 3.7892563486099244 and perplexity is 44.223501176645016
At time: 304.77657103538513 and batch: 450, loss is 3.742578539848328 and perplexity is 42.20668158252459
At time: 305.34212374687195 and batch: 500, loss is 3.705355644226074 and perplexity is 40.66450684256767
At time: 305.9090735912323 and batch: 550, loss is 3.720900912284851 and perplexity is 41.301586459197374
At time: 306.47611474990845 and batch: 600, loss is 3.79632483959198 and perplexity is 44.537201985442955
At time: 307.04275131225586 and batch: 650, loss is 3.6786385345458985 and perplexity is 39.59245360403729
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503533157647825 and perplexity of 90.33573852975931
Finished 39 epochs...
Completing Train Step...
At time: 308.0917172431946 and batch: 50, loss is 3.8194566202163696 and perplexity is 45.57943464736495
At time: 308.65874314308167 and batch: 100, loss is 3.7987560558319093 and perplexity is 44.64561328644887
At time: 309.2266356945038 and batch: 150, loss is 3.757016806602478 and perplexity is 42.82049343658414
At time: 309.7939655780792 and batch: 200, loss is 3.6958640575408936 and perplexity is 40.28036210636224
At time: 310.3730900287628 and batch: 250, loss is 3.6783554315567017 and perplexity is 39.58124644853682
At time: 310.9404265880585 and batch: 300, loss is 3.776716923713684 and perplexity is 43.672426207356914
At time: 311.5078458786011 and batch: 350, loss is 3.743770775794983 and perplexity is 42.257031914269916
At time: 312.0753107070923 and batch: 400, loss is 3.7892337608337403 and perplexity is 44.22250227737986
At time: 312.6427915096283 and batch: 450, loss is 3.742565979957581 and perplexity is 42.20615147454418
At time: 313.21001172065735 and batch: 500, loss is 3.7053506326675416 and perplexity is 40.664303050522086
At time: 313.7769105434418 and batch: 550, loss is 3.7209084701538084 and perplexity is 41.30189861235517
At time: 314.34478664398193 and batch: 600, loss is 3.7963391971588134 and perplexity is 44.53784143588749
At time: 314.9121193885803 and batch: 650, loss is 3.678655481338501 and perplexity is 39.593124574822525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5035301657284 and perplexity of 90.33546825291282
Finished 40 epochs...
Completing Train Step...
At time: 315.93547916412354 and batch: 50, loss is 3.819411163330078 and perplexity is 45.577362795277274
At time: 316.5146999359131 and batch: 100, loss is 3.7987075901031493 and perplexity is 44.6434495566988
At time: 317.0823678970337 and batch: 150, loss is 3.756968493461609 and perplexity is 42.81842469402679
At time: 317.65000319480896 and batch: 200, loss is 3.6958174180984495 and perplexity is 40.278483496541156
At time: 318.2176425457001 and batch: 250, loss is 3.6783150005340577 and perplexity is 39.57964617061603
At time: 318.7850856781006 and batch: 300, loss is 3.7766807508468627 and perplexity is 43.67084647907177
At time: 319.35152888298035 and batch: 350, loss is 3.7437388944625853 and perplexity is 42.25568472526452
At time: 319.918155670166 and batch: 400, loss is 3.7892112493515016 and perplexity is 44.22150677451045
At time: 320.4844870567322 and batch: 450, loss is 3.742553262710571 and perplexity is 42.2056147319035
At time: 321.0517258644104 and batch: 500, loss is 3.7053457164764403 and perplexity is 40.66410313752869
At time: 321.6183955669403 and batch: 550, loss is 3.720915632247925 and perplexity is 41.30219442149952
At time: 322.1865167617798 and batch: 600, loss is 3.7963531494140623 and perplexity is 44.53846284355445
At time: 322.752690076828 and batch: 650, loss is 3.6786720085144045 and perplexity is 39.59377894276435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5035301657284 and perplexity of 90.33546825291282
Finished 41 epochs...
Completing Train Step...
At time: 323.7946915626526 and batch: 50, loss is 3.8193660831451415 and perplexity is 45.57530820564453
At time: 324.36178278923035 and batch: 100, loss is 3.79865957736969 and perplexity is 44.64130615411025
At time: 324.9424376487732 and batch: 150, loss is 3.7569206619262694 and perplexity is 42.81637667201326
At time: 325.50867676734924 and batch: 200, loss is 3.6957710456848143 and perplexity is 40.276615729350624
At time: 326.07651805877686 and batch: 250, loss is 3.6782747173309325 and perplexity is 39.57805180780295
At time: 326.6436929702759 and batch: 300, loss is 3.776644678115845 and perplexity is 43.66927118078624
At time: 327.2112612724304 and batch: 350, loss is 3.7437071084976195 and perplexity is 42.254341608896475
At time: 327.777872800827 and batch: 400, loss is 3.789188895225525 and perplexity is 44.22051825242596
At time: 328.3448734283447 and batch: 450, loss is 3.7425405025482177 and perplexity is 42.20507618484328
At time: 328.91183161735535 and batch: 500, loss is 3.7053406047821045 and perplexity is 40.66389527559428
At time: 329.4797716140747 and batch: 550, loss is 3.7209224843978883 and perplexity is 41.30247743129913
At time: 330.0471303462982 and batch: 600, loss is 3.796367115974426 and perplexity is 44.53908489702823
At time: 330.61490774154663 and batch: 650, loss is 3.6786884021759034 and perplexity is 39.59442803509429
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503528968960631 and perplexity of 90.33536014240069
Finished 42 epochs...
Completing Train Step...
At time: 331.6439745426178 and batch: 50, loss is 3.819321322441101 and perplexity is 45.57326826841724
At time: 332.22578406333923 and batch: 100, loss is 3.7986118984222412 and perplexity is 44.63917775436041
At time: 332.7938425540924 and batch: 150, loss is 3.756873092651367 and perplexity is 42.81433997646348
At time: 333.3621287345886 and batch: 200, loss is 3.695725193023682 and perplexity is 40.2747689816775
At time: 333.92928528785706 and batch: 250, loss is 3.6782346820831298 and perplexity is 39.57646732240912
At time: 334.4980709552765 and batch: 300, loss is 3.776608772277832 and perplexity is 43.66770322715858
At time: 335.0668206214905 and batch: 350, loss is 3.7436756324768066 and perplexity is 42.25301163129187
At time: 335.634104013443 and batch: 400, loss is 3.789166522026062 and perplexity is 44.21952890901818
At time: 336.2021565437317 and batch: 450, loss is 3.7425278711318968 and perplexity is 42.20454307832208
At time: 336.7702479362488 and batch: 500, loss is 3.7053353357315064 and perplexity is 40.66368101603703
At time: 337.3379898071289 and batch: 550, loss is 3.720929102897644 and perplexity is 41.302750792640545
At time: 337.90673828125 and batch: 600, loss is 3.7963806629180907 and perplexity is 44.53968826958912
At time: 338.47494888305664 and batch: 650, loss is 3.678704442977905 and perplexity is 39.59506316656877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503527173808977 and perplexity of 90.33519797687507
Finished 43 epochs...
Completing Train Step...
At time: 339.5414810180664 and batch: 50, loss is 3.819276876449585 and perplexity is 45.57124276433552
At time: 340.11107897758484 and batch: 100, loss is 3.798564519882202 and perplexity is 44.63706286539043
At time: 340.6800928115845 and batch: 150, loss is 3.756826138496399 and perplexity is 42.812329712504855
At time: 341.24857568740845 and batch: 200, loss is 3.695679769515991 and perplexity is 40.27293960194765
At time: 341.8171525001526 and batch: 250, loss is 3.6781947946548463 and perplexity is 39.57488875038988
At time: 342.38609862327576 and batch: 300, loss is 3.7765728664398193 and perplexity is 43.666135329828634
At time: 342.95481181144714 and batch: 350, loss is 3.7436443042755125 and perplexity is 42.251687941172726
At time: 343.5223002433777 and batch: 400, loss is 3.789144268035889 and perplexity is 44.21854485900593
At time: 344.0912187099457 and batch: 450, loss is 3.742514953613281 and perplexity is 42.20399790387236
At time: 344.6593282222748 and batch: 500, loss is 3.7053300380706786 and perplexity is 40.66346559421761
At time: 345.22780752182007 and batch: 550, loss is 3.720935220718384 and perplexity is 41.30300347623889
At time: 345.79569244384766 and batch: 600, loss is 3.7963941955566405 and perplexity is 44.54029101316994
At time: 346.3642544746399 and batch: 650, loss is 3.6787201833724974 and perplexity is 39.59568641339198
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503525378657322 and perplexity of 90.33503581164048
Finished 44 epochs...
Completing Train Step...
At time: 347.38829612731934 and batch: 50, loss is 3.8192327404022217 and perplexity is 45.569231474192
At time: 347.9689598083496 and batch: 100, loss is 3.798517355918884 and perplexity is 44.63495765424027
At time: 348.5370044708252 and batch: 150, loss is 3.756779279708862 and perplexity is 42.81032362564467
At time: 349.10517168045044 and batch: 200, loss is 3.6956344938278196 and perplexity is 40.271116258169364
At time: 349.6727797985077 and batch: 250, loss is 3.6781550550460818 and perplexity is 39.573316091042685
At time: 350.2409014701843 and batch: 300, loss is 3.776537523269653 and perplexity is 43.664592057449404
At time: 350.8083999156952 and batch: 350, loss is 3.7436131715774534 and perplexity is 42.25037255260547
At time: 351.37604308128357 and batch: 400, loss is 3.7891221714019774 and perplexity is 44.2175677888031
At time: 351.9440801143646 and batch: 450, loss is 3.742501983642578 and perplexity is 42.20345052280575
At time: 352.51183891296387 and batch: 500, loss is 3.7053246545791625 and perplexity is 40.66324668338482
At time: 353.0795452594757 and batch: 550, loss is 3.720941047668457 and perplexity is 41.303244147479205
At time: 353.66978549957275 and batch: 600, loss is 3.7964073419570923 and perplexity is 44.540876561520754
At time: 354.23618030548096 and batch: 650, loss is 3.67873562335968 and perplexity is 39.596297775002384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503525378657322 and perplexity of 90.33503581164048
Finished 45 epochs...
Completing Train Step...
At time: 355.2697949409485 and batch: 50, loss is 3.81918879032135 and perplexity is 45.567228746793795
At time: 355.83612871170044 and batch: 100, loss is 3.798470492362976 and perplexity is 44.632865950419514
At time: 356.4036684036255 and batch: 150, loss is 3.7567328023910522 and perplexity is 42.808333962865426
At time: 356.96974325180054 and batch: 200, loss is 3.6955896520614626 and perplexity is 40.26931047067083
At time: 357.53608679771423 and batch: 250, loss is 3.678115477561951 and perplexity is 39.571749909746046
At time: 358.10184359550476 and batch: 300, loss is 3.7765018224716185 and perplexity is 43.66303322449305
At time: 358.6678194999695 and batch: 350, loss is 3.7435821723937988 and perplexity is 42.24906284584726
At time: 359.2340681552887 and batch: 400, loss is 3.7891000413894655 and perplexity is 44.216589264302115
At time: 359.80068135261536 and batch: 450, loss is 3.742489080429077 and perplexity is 42.202905966186464
At time: 360.36701941490173 and batch: 500, loss is 3.7053192043304444 and perplexity is 40.66302505918067
At time: 360.934029340744 and batch: 550, loss is 3.7209467554092406 and perplexity is 41.303479896363115
At time: 361.5007016658783 and batch: 600, loss is 3.7964204454422 and perplexity is 44.54146020605734
At time: 362.067524433136 and batch: 650, loss is 3.6787509393692015 and perplexity is 39.596904236920395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.50352507946538 and perplexity of 90.3350087841297
Finished 46 epochs...
Completing Train Step...
At time: 363.09194684028625 and batch: 50, loss is 3.8191452550888063 and perplexity is 45.56524501007546
At time: 363.6718261241913 and batch: 100, loss is 3.79842396736145 and perplexity is 44.630789454567925
At time: 364.2394893169403 and batch: 150, loss is 3.756686596870422 and perplexity is 42.80635602720349
At time: 364.8067150115967 and batch: 200, loss is 3.695545129776001 and perplexity is 40.267517628845724
At time: 365.37235474586487 and batch: 250, loss is 3.67807608127594 and perplexity is 39.57019096047726
At time: 365.9388108253479 and batch: 300, loss is 3.7764664363861082 and perplexity is 43.66148818800229
At time: 366.50486731529236 and batch: 350, loss is 3.743551468849182 and perplexity is 42.24776566977522
At time: 367.07229447364807 and batch: 400, loss is 3.7890779638290404 and perplexity is 44.21561308065675
At time: 367.6395251750946 and batch: 450, loss is 3.7424761962890627 and perplexity is 42.20236222153983
At time: 368.2187919616699 and batch: 500, loss is 3.70531370639801 and perplexity is 40.66280149723089
At time: 368.78529381752014 and batch: 550, loss is 3.7209520530700684 and perplexity is 41.30369870877021
At time: 369.35312151908875 and batch: 600, loss is 3.7964332246780397 and perplexity is 44.54202941551898
At time: 369.9201650619507 and batch: 650, loss is 3.6787658977508544 and perplexity is 39.597496546956236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503524181889553 and perplexity of 90.33492770164587
Finished 47 epochs...
Completing Train Step...
At time: 370.9555287361145 and batch: 50, loss is 3.819101858139038 and perplexity is 45.56326766033236
At time: 371.52254605293274 and batch: 100, loss is 3.7983776426315305 and perplexity is 44.628721993188
At time: 372.0894422531128 and batch: 150, loss is 3.7566406869888307 and perplexity is 42.804390837578076
At time: 372.6564242839813 and batch: 200, loss is 3.6955008935928344 and perplexity is 40.26573638695819
At time: 373.22198462486267 and batch: 250, loss is 3.6780368280410767 and perplexity is 39.5686377329627
At time: 373.7887029647827 and batch: 300, loss is 3.776431317329407 and perplexity is 43.65995486464754
At time: 374.3551194667816 and batch: 350, loss is 3.7435207414627074 and perplexity is 42.246467526296165
At time: 374.9211902618408 and batch: 400, loss is 3.789056086540222 and perplexity is 44.21464577350017
At time: 375.48845863342285 and batch: 450, loss is 3.7424632358551024 and perplexity is 42.2018152641557
At time: 376.05608773231506 and batch: 500, loss is 3.7053079557418824 and perplexity is 40.66256766011465
At time: 376.6230881214142 and batch: 550, loss is 3.720957245826721 and perplexity is 41.30391318938333
At time: 377.1901104450226 and batch: 600, loss is 3.7964458227157594 and perplexity is 44.54259056122033
At time: 377.7565610408783 and batch: 650, loss is 3.6787807369232177 and perplexity is 39.59808414539238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5035229851217835 and perplexity of 90.33481959178064
Finished 48 epochs...
Completing Train Step...
At time: 378.7896420955658 and batch: 50, loss is 3.8190587663650515 and perplexity is 45.561304300602885
At time: 379.3687973022461 and batch: 100, loss is 3.7983314418792724 and perplexity is 44.626660160289084
At time: 379.93579030036926 and batch: 150, loss is 3.756594877243042 and perplexity is 42.80243002422769
At time: 380.50345492362976 and batch: 200, loss is 3.6954567670822143 and perplexity is 40.263959639715
At time: 381.081449508667 and batch: 250, loss is 3.677997879981995 and perplexity is 39.56709664133395
At time: 381.6502707004547 and batch: 300, loss is 3.776396117210388 and perplexity is 43.65841805608803
At time: 382.21573281288147 and batch: 350, loss is 3.7434902143478395 and perplexity is 42.24517788321387
At time: 382.7950007915497 and batch: 400, loss is 3.789034118652344 and perplexity is 44.213674481787905
At time: 383.36205649375916 and batch: 450, loss is 3.7424501276016233 and perplexity is 42.20126207568961
At time: 383.928995847702 and batch: 500, loss is 3.7053022527694703 and perplexity is 40.66233576327432
At time: 384.4963722229004 and batch: 550, loss is 3.720962142944336 and perplexity is 41.30411545999945
At time: 385.0648250579834 and batch: 600, loss is 3.7964581632614136 and perplexity is 44.5431402444844
At time: 385.63191652297974 and batch: 650, loss is 3.6787954330444337 and perplexity is 39.59866608791304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503523284313726 and perplexity of 90.33484661923482
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 386.67361187934875 and batch: 50, loss is 3.8190363788604738 and perplexity is 45.560284308111875
At time: 387.24190878868103 and batch: 100, loss is 3.798309464454651 and perplexity is 44.62567939200672
At time: 387.81067395210266 and batch: 150, loss is 3.7565717840194703 and perplexity is 42.80144158955485
At time: 388.37926387786865 and batch: 200, loss is 3.69543954372406 and perplexity is 40.263266165089405
At time: 388.94804191589355 and batch: 250, loss is 3.6779612731933593 and perplexity is 39.56564824350104
At time: 389.51511001586914 and batch: 300, loss is 3.7763536024093627 and perplexity is 43.65656196658721
At time: 390.0827362537384 and batch: 350, loss is 3.743444447517395 and perplexity is 42.243244499563346
At time: 390.6498074531555 and batch: 400, loss is 3.7889744997024537 and perplexity is 44.211038587519916
At time: 391.2174916267395 and batch: 450, loss is 3.7423774337768556 and perplexity is 42.198194416040614
At time: 391.78604078292847 and batch: 500, loss is 3.7052003860473635 and perplexity is 40.658193835382804
At time: 392.3549029827118 and batch: 550, loss is 3.720844554901123 and perplexity is 41.299258875429594
At time: 392.92291045188904 and batch: 600, loss is 3.7963402128219603 and perplexity is 44.537886671354656
At time: 393.48963928222656 and batch: 650, loss is 3.678682203292847 and perplexity is 39.594182594625934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5035238826976105 and perplexity of 90.33490067416743
Annealing...
Finished Training.
Improved accuracyfrom -132.47720359694543 to -90.33481959178064
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb2b349b438>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'tune_wordvecs': True, 'data': 'ptb', 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.11703656853499123, 'anneal': 4.4823179985909665, 'wordvec_dim': 200, 'lr': 6.182311085203539, 'seq_len': 20}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7996420860290527 and batch: 50, loss is 6.4740706825256344 and perplexity is 648.1166415369541
At time: 1.3875095844268799 and batch: 100, loss is 5.523361759185791 and perplexity is 250.47566226691086
At time: 1.9617908000946045 and batch: 150, loss is 5.25998028755188 and perplexity is 192.4776970532314
At time: 2.5364456176757812 and batch: 200, loss is 5.143702411651612 and perplexity is 171.34899988968445
At time: 3.1117022037506104 and batch: 250, loss is 5.060509204864502 and perplexity is 157.67078261513694
At time: 3.686781406402588 and batch: 300, loss is 5.061490535736084 and perplexity is 157.82558576579598
At time: 4.260695934295654 and batch: 350, loss is 4.95260760307312 and perplexity is 141.54357257490906
At time: 4.8355512619018555 and batch: 400, loss is 4.97035493850708 and perplexity is 144.07801713272633
At time: 5.410304069519043 and batch: 450, loss is 4.9035317325592045 and perplexity is 134.7648937663057
At time: 5.998204469680786 and batch: 500, loss is 4.890097637176513 and perplexity is 132.96655589660554
At time: 6.572572708129883 and batch: 550, loss is 4.888163051605225 and perplexity is 132.70956937746027
At time: 7.147511005401611 and batch: 600, loss is 4.929477024078369 and perplexity is 138.30716210732362
At time: 7.726341485977173 and batch: 650, loss is 4.843812551498413 and perplexity is 126.9524429664991
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.851689357383578 and perplexity of 127.95637140044691
Finished 1 epochs...
Completing Train Step...
At time: 8.779470205307007 and batch: 50, loss is 4.734313001632691 and perplexity is 113.78526157170602
At time: 9.346834421157837 and batch: 100, loss is 4.658762493133545 and perplexity is 105.5054376330654
At time: 9.914472579956055 and batch: 150, loss is 4.58028016090393 and perplexity is 97.54171775519752
At time: 10.48137617111206 and batch: 200, loss is 4.516414546966553 and perplexity is 91.5069153449938
At time: 11.048339128494263 and batch: 250, loss is 4.484856996536255 and perplexity is 88.6642706085182
At time: 11.615301609039307 and batch: 300, loss is 4.559844007492066 and perplexity is 95.5685706862181
At time: 12.182437658309937 and batch: 350, loss is 4.502985696792603 and perplexity is 90.28629678403179
At time: 12.748765230178833 and batch: 400, loss is 4.546570482254029 and perplexity is 94.30842066950457
At time: 13.315443277359009 and batch: 450, loss is 4.488804559707642 and perplexity is 89.0149701667004
At time: 13.881383895874023 and batch: 500, loss is 4.4657606029510495 and perplexity is 86.98716708034438
At time: 14.447799444198608 and batch: 550, loss is 4.477835330963135 and perplexity is 88.04388038731317
At time: 15.01509165763855 and batch: 600, loss is 4.548193874359131 and perplexity is 94.4616445526347
At time: 15.582140922546387 and batch: 650, loss is 4.44706690788269 and perplexity is 85.37616025342838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.666014129040288 and perplexity of 106.27330543689492
Finished 2 epochs...
Completing Train Step...
At time: 16.60508680343628 and batch: 50, loss is 4.412375984191894 and perplexity is 82.46516685092531
At time: 17.18424677848816 and batch: 100, loss is 4.369948406219482 and perplexity is 79.03955364498123
At time: 17.74941372871399 and batch: 150, loss is 4.299663906097412 and perplexity is 73.67502781037668
At time: 18.31459331512451 and batch: 200, loss is 4.244003562927246 and perplexity is 69.68628753674653
At time: 18.880785942077637 and batch: 250, loss is 4.221510381698608 and perplexity is 68.13631845885749
At time: 19.447014331817627 and batch: 300, loss is 4.316544494628906 and perplexity is 74.92926196206858
At time: 20.012633800506592 and batch: 350, loss is 4.2753400182723995 and perplexity is 71.90458402603656
At time: 20.59212875366211 and batch: 400, loss is 4.323097772598267 and perplexity is 75.42190669962808
At time: 21.15773367881775 and batch: 450, loss is 4.274187364578247 and perplexity is 71.82175068987901
At time: 21.723121881484985 and batch: 500, loss is 4.252210702896118 and perplexity is 70.26056602094573
At time: 22.289089679718018 and batch: 550, loss is 4.267575149536133 and perplexity is 71.3484164480811
At time: 22.855015754699707 and batch: 600, loss is 4.352598114013672 and perplexity is 77.68002252958046
At time: 23.420741081237793 and batch: 650, loss is 4.2475282144546505 and perplexity is 69.9323407878056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.601350672104779 and perplexity of 99.61877711817529
Finished 3 epochs...
Completing Train Step...
At time: 24.453577518463135 and batch: 50, loss is 4.2276467609405515 and perplexity is 68.55571421875509
At time: 25.020553827285767 and batch: 100, loss is 4.1926232528686525 and perplexity is 66.19621280141466
At time: 25.587488412857056 and batch: 150, loss is 4.132143774032593 and perplexity is 62.31136133112913
At time: 26.15327215194702 and batch: 200, loss is 4.0802055311203 and perplexity is 59.15762733388904
At time: 26.719299793243408 and batch: 250, loss is 4.060302166938782 and perplexity is 57.99183164600076
At time: 27.286253929138184 and batch: 300, loss is 4.158979773521423 and perplexity is 64.00618846952933
At time: 27.85312271118164 and batch: 350, loss is 4.124026780128479 and perplexity is 61.80762755875717
At time: 28.419700384140015 and batch: 400, loss is 4.171760635375977 and perplexity is 64.82949278432174
At time: 28.985517263412476 and batch: 450, loss is 4.126537303924561 and perplexity is 61.96299202000965
At time: 29.551335096359253 and batch: 500, loss is 4.107984075546264 and perplexity is 60.82397735037094
At time: 30.11748480796814 and batch: 550, loss is 4.122541546821594 and perplexity is 61.71589694922816
At time: 30.68321180343628 and batch: 600, loss is 4.213686943054199 and perplexity is 67.60533790709479
At time: 31.250660181045532 and batch: 650, loss is 4.106833801269532 and perplexity is 60.75405331743432
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.574017094630821 and perplexity of 96.93271661049054
Finished 4 epochs...
Completing Train Step...
At time: 32.27680468559265 and batch: 50, loss is 4.095500087738037 and perplexity is 60.06937160356619
At time: 32.85681986808777 and batch: 100, loss is 4.061570038795471 and perplexity is 58.06540448787472
At time: 33.42368197441101 and batch: 150, loss is 4.0061130666732785 and perplexity is 54.93293440155507
At time: 33.99092745780945 and batch: 200, loss is 3.9539365911483766 and perplexity is 52.140218083463196
At time: 34.55704641342163 and batch: 250, loss is 3.934816989898682 and perplexity is 51.15278762313707
At time: 35.13549542427063 and batch: 300, loss is 4.036122612953186 and perplexity is 56.60643169571576
At time: 35.7022590637207 and batch: 350, loss is 4.005018105506897 and perplexity is 54.87281789025901
At time: 36.26913142204285 and batch: 400, loss is 4.056308360099792 and perplexity is 57.76068535622524
At time: 36.83420538902283 and batch: 450, loss is 4.010910129547119 and perplexity is 55.197084206788844
At time: 37.399739265441895 and batch: 500, loss is 3.9942959356307983 and perplexity is 54.28760519678309
At time: 37.96573996543884 and batch: 550, loss is 4.010886812210083 and perplexity is 55.19579717277818
At time: 38.53288197517395 and batch: 600, loss is 4.101872396469116 and perplexity is 60.453374377409624
At time: 39.101080894470215 and batch: 650, loss is 3.9920894432067873 and perplexity is 54.16795206261107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.561485140931373 and perplexity of 95.72554023207259
Finished 5 epochs...
Completing Train Step...
At time: 40.13244962692261 and batch: 50, loss is 3.984938311576843 and perplexity is 53.781971650529634
At time: 40.69945240020752 and batch: 100, loss is 3.95623685836792 and perplexity is 52.26029256669413
At time: 41.2667932510376 and batch: 150, loss is 3.904120545387268 and perplexity is 49.60643411543762
At time: 41.833080768585205 and batch: 200, loss is 3.853521604537964 and perplexity is 47.15884595557276
At time: 42.398749351501465 and batch: 250, loss is 3.8363431787490843 and perplexity is 46.35564979458067
At time: 42.963752031326294 and batch: 300, loss is 3.937088632583618 and perplexity is 51.269120562363824
At time: 43.52947974205017 and batch: 350, loss is 3.9098550128936767 and perplexity is 49.89171779319677
At time: 44.09585213661194 and batch: 400, loss is 3.9569401025772093 and perplexity is 52.29705724057426
At time: 44.66171193122864 and batch: 450, loss is 3.9155221796035766 and perplexity is 50.17526517162065
At time: 45.228431940078735 and batch: 500, loss is 3.898638138771057 and perplexity is 49.33521561721034
At time: 45.79522347450256 and batch: 550, loss is 3.916902017593384 and perplexity is 50.24454669630059
At time: 46.36122131347656 and batch: 600, loss is 4.010757575035095 and perplexity is 55.188664284807416
At time: 46.92869687080383 and batch: 650, loss is 3.8969891977310183 and perplexity is 49.25393178999686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.569280586990655 and perplexity of 96.47467966175553
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 47.94799327850342 and batch: 50, loss is 3.8938685703277587 and perplexity is 49.10046819662188
At time: 48.52862906455994 and batch: 100, loss is 3.852785515785217 and perplexity is 47.124145632297974
At time: 49.099438190460205 and batch: 150, loss is 3.7818560886383055 and perplexity is 43.89744371408902
At time: 49.6804461479187 and batch: 200, loss is 3.7176197576522827 and perplexity is 41.16629165043033
At time: 50.24815130233765 and batch: 250, loss is 3.6831751823425294 and perplexity is 39.77247866783729
At time: 50.81523323059082 and batch: 300, loss is 3.7659852266311646 and perplexity is 43.20625284814428
At time: 51.382436752319336 and batch: 350, loss is 3.721626753807068 and perplexity is 41.33157574800177
At time: 51.94996476173401 and batch: 400, loss is 3.74463764667511 and perplexity is 42.29367918664869
At time: 52.51858854293823 and batch: 450, loss is 3.680099687576294 and perplexity is 39.650346522508286
At time: 53.08676290512085 and batch: 500, loss is 3.637872266769409 and perplexity is 38.01087362307078
At time: 53.65434765815735 and batch: 550, loss is 3.628426332473755 and perplexity is 37.653515854428214
At time: 54.22275114059448 and batch: 600, loss is 3.691422667503357 and perplexity is 40.10185800399571
At time: 54.79396176338196 and batch: 650, loss is 3.559996862411499 and perplexity is 35.163086817636675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.49569642310049 and perplexity of 89.63056805264932
Finished 7 epochs...
Completing Train Step...
At time: 55.830286741256714 and batch: 50, loss is 3.7802803230285646 and perplexity is 43.82832610280807
At time: 56.39845871925354 and batch: 100, loss is 3.7468833541870117 and perplexity is 42.388765147370776
At time: 56.9673068523407 and batch: 150, loss is 3.685000138282776 and perplexity is 39.84512795975592
At time: 57.53573417663574 and batch: 200, loss is 3.6293788814544676 and perplexity is 37.68939976045138
At time: 58.1037483215332 and batch: 250, loss is 3.6018182039260864 and perplexity is 36.664838028332326
At time: 58.670804500579834 and batch: 300, loss is 3.690880923271179 and perplexity is 40.080138937343165
At time: 59.238823652267456 and batch: 350, loss is 3.6540174674987793 and perplexity is 38.62954767704754
At time: 59.80648422241211 and batch: 400, loss is 3.6861436796188354 and perplexity is 39.8907185730257
At time: 60.39649510383606 and batch: 450, loss is 3.6309823894500735 and perplexity is 37.74988349443253
At time: 60.97730994224548 and batch: 500, loss is 3.597890467643738 and perplexity is 36.52111066009632
At time: 61.56185698509216 and batch: 550, loss is 3.600007200241089 and perplexity is 36.5984979607381
At time: 62.13084173202515 and batch: 600, loss is 3.6751214265823364 and perplexity is 39.45344726361591
At time: 62.698782205581665 and batch: 650, loss is 3.5559581232070925 and perplexity is 35.021358674511156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.49609375 and perplexity of 89.66618776422116
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 63.71625828742981 and batch: 50, loss is 3.739591760635376 and perplexity is 42.080807615775115
At time: 64.29616403579712 and batch: 100, loss is 3.7089875078201295 and perplexity is 40.812463300742266
At time: 64.86342144012451 and batch: 150, loss is 3.649609618186951 and perplexity is 38.45964917049229
At time: 65.43025350570679 and batch: 200, loss is 3.5921395921707155 and perplexity is 36.3116850679907
At time: 66.01306676864624 and batch: 250, loss is 3.556890397071838 and perplexity is 35.05402339577842
At time: 66.58118677139282 and batch: 300, loss is 3.6410833597183228 and perplexity is 38.133126248568985
At time: 67.14823126792908 and batch: 350, loss is 3.597947192192078 and perplexity is 36.52318236236101
At time: 67.71345782279968 and batch: 400, loss is 3.625503363609314 and perplexity is 37.5436164943988
At time: 68.27834510803223 and batch: 450, loss is 3.563952922821045 and perplexity is 35.30246963491276
At time: 68.84320092201233 and batch: 500, loss is 3.5185230493545534 and perplexity is 33.73456736068193
At time: 69.40720391273499 and batch: 550, loss is 3.5128237533569338 and perplexity is 33.54285091925917
At time: 69.97213172912598 and batch: 600, loss is 3.5771410274505615 and perplexity is 35.771125849123045
At time: 70.53775787353516 and batch: 650, loss is 3.4562292861938477 and perplexity is 31.69722970940253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.481354956533394 and perplexity of 88.35430785379513
Finished 9 epochs...
Completing Train Step...
At time: 71.55944728851318 and batch: 50, loss is 3.712686061859131 and perplexity is 40.9636898891901
At time: 72.12374925613403 and batch: 100, loss is 3.679430537223816 and perplexity is 39.623823354140285
At time: 72.69001269340515 and batch: 150, loss is 3.6195811891555785 and perplexity is 37.32193371784017
At time: 73.25649762153625 and batch: 200, loss is 3.5640578699111938 and perplexity is 35.30617472079161
At time: 73.82272481918335 and batch: 250, loss is 3.5320780992507936 and perplexity is 34.19495433947209
At time: 74.38793849945068 and batch: 300, loss is 3.619507603645325 and perplexity is 37.319187465347326
At time: 74.95276308059692 and batch: 350, loss is 3.5790514850616457 and perplexity is 35.839530389940855
At time: 75.51799774169922 and batch: 400, loss is 3.609838705062866 and perplexity is 36.96009085848726
At time: 76.08543705940247 and batch: 450, loss is 3.552415995597839 and perplexity is 34.897527994543644
At time: 76.66309833526611 and batch: 500, loss is 3.5113270044326783 and perplexity is 33.49268324679991
At time: 77.22929000854492 and batch: 550, loss is 3.5110967493057252 and perplexity is 33.48497227254667
At time: 77.79511976242065 and batch: 600, loss is 3.58055326461792 and perplexity is 35.89339389942224
At time: 78.38224720954895 and batch: 650, loss is 3.4624961948394777 and perplexity is 31.89649709539202
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.481438431085325 and perplexity of 88.36168349788935
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 79.4026153087616 and batch: 50, loss is 3.7011512899398804 and perplexity is 40.4938977507769
At time: 79.98225474357605 and batch: 100, loss is 3.6677698945999144 and perplexity is 39.164467507691555
At time: 80.54842591285706 and batch: 150, loss is 3.60854248046875 and perplexity is 36.91221331644312
At time: 81.11557912826538 and batch: 200, loss is 3.5525003910064696 and perplexity is 34.900473309962734
At time: 81.68198013305664 and batch: 250, loss is 3.519099192619324 and perplexity is 33.754008904476194
At time: 82.24855947494507 and batch: 300, loss is 3.605934085845947 and perplexity is 36.8160571588333
At time: 82.81437945365906 and batch: 350, loss is 3.5642494773864746 and perplexity is 35.31294029593837
At time: 83.38135123252869 and batch: 400, loss is 3.59345091342926 and perplexity is 36.359332586328044
At time: 83.94792318344116 and batch: 450, loss is 3.533743004798889 and perplexity is 34.25193312765737
At time: 84.51532697677612 and batch: 500, loss is 3.488404884338379 and perplexity is 32.7336920155798
At time: 85.08213973045349 and batch: 550, loss is 3.4850461053848267 and perplexity is 32.62393121408869
At time: 85.64844417572021 and batch: 600, loss is 3.5521101999282836 and perplexity is 34.88685811308964
At time: 86.21689581871033 and batch: 650, loss is 3.4340549373626708 and perplexity is 31.002099790314876
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.480367024739583 and perplexity of 88.26706292706065
Finished 11 epochs...
Completing Train Step...
At time: 87.24723672866821 and batch: 50, loss is 3.6948414707183836 and perplexity is 40.23919299195078
At time: 87.81423449516296 and batch: 100, loss is 3.6610822057724 and perplexity is 38.90342160572889
At time: 88.38147759437561 and batch: 150, loss is 3.60134964466095 and perplexity is 36.64766240298286
At time: 88.94814038276672 and batch: 200, loss is 3.5456476831436157 and perplexity is 34.662128149686666
At time: 89.5141065120697 and batch: 250, loss is 3.5130841541290283 and perplexity is 33.551586640881766
At time: 90.08040165901184 and batch: 300, loss is 3.6006372499465944 and perplexity is 36.62156409924406
At time: 90.6467342376709 and batch: 350, loss is 3.5596752691268922 and perplexity is 35.15178042317778
At time: 91.21343398094177 and batch: 400, loss is 3.58964852809906 and perplexity is 36.22134290471254
At time: 91.77997541427612 and batch: 450, loss is 3.5313302946090697 and perplexity is 34.16939275262134
At time: 92.34574866294861 and batch: 500, loss is 3.4873125314712525 and perplexity is 32.69795479563585
At time: 92.92469930648804 and batch: 550, loss is 3.4858917665481566 and perplexity is 32.65153167438542
At time: 93.48951172828674 and batch: 600, loss is 3.5542737436294556 and perplexity is 34.96241906545059
At time: 94.05517029762268 and batch: 650, loss is 3.4367101335525514 and perplexity is 31.084525827780983
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.480176439472273 and perplexity of 88.25024212822716
Finished 12 epochs...
Completing Train Step...
At time: 95.0687460899353 and batch: 50, loss is 3.6903313970565796 and perplexity is 40.058119900885295
At time: 95.64894366264343 and batch: 100, loss is 3.6562847089767456 and perplexity is 38.71722955024604
At time: 96.21598196029663 and batch: 150, loss is 3.5963785219192506 and perplexity is 36.465934445212326
At time: 96.7833731174469 and batch: 200, loss is 3.5408740997314454 and perplexity is 34.497059886733865
At time: 97.349041223526 and batch: 250, loss is 3.5087661027908323 and perplexity is 33.40702151173802
At time: 97.91522097587585 and batch: 300, loss is 3.5967921352386476 and perplexity is 36.48102036105592
At time: 98.48215413093567 and batch: 350, loss is 3.5563297176361086 and perplexity is 35.03437483450767
At time: 99.04910945892334 and batch: 400, loss is 3.586919627189636 and perplexity is 36.12263319488456
At time: 99.61616945266724 and batch: 450, loss is 3.5295270109176635 and perplexity is 34.1078311671092
At time: 100.18243288993835 and batch: 500, loss is 3.4864098405838013 and perplexity is 32.66845196777384
At time: 100.74875235557556 and batch: 550, loss is 3.486206641197205 and perplexity is 32.66181443276729
At time: 101.31562519073486 and batch: 600, loss is 3.5554898405075073 and perplexity is 35.00496261742248
At time: 101.88375401496887 and batch: 650, loss is 3.438250479698181 and perplexity is 31.132443652872713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.480275472005208 and perplexity of 88.25898220600612
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 102.91765117645264 and batch: 50, loss is 3.6873673963546754 and perplexity is 39.939563392964644
At time: 103.48588347434998 and batch: 100, loss is 3.6533184099197387 and perplexity is 38.60255283554258
At time: 104.05357813835144 and batch: 150, loss is 3.593330435752869 and perplexity is 36.35495236228781
At time: 104.62015318870544 and batch: 200, loss is 3.5376066875457766 and perplexity is 34.38452771749667
At time: 105.18724322319031 and batch: 250, loss is 3.5051642179489138 and perplexity is 33.286909711633896
At time: 105.754145860672 and batch: 300, loss is 3.5932477855682374 and perplexity is 36.35194774293069
At time: 106.32116723060608 and batch: 350, loss is 3.5524098920822142 and perplexity is 34.89731499758627
At time: 106.88827443122864 and batch: 400, loss is 3.582926731109619 and perplexity is 35.978686847052096
At time: 107.46822810173035 and batch: 450, loss is 3.5245149517059327 and perplexity is 33.93730838981565
At time: 108.03494143486023 and batch: 500, loss is 3.480542097091675 and perplexity is 32.47732316711686
At time: 108.60135054588318 and batch: 550, loss is 3.479400315284729 and perplexity is 32.440262312117575
At time: 109.16874623298645 and batch: 600, loss is 3.547932047843933 and perplexity is 34.741399599648716
At time: 109.73641777038574 and batch: 650, loss is 3.431048254966736 and perplexity is 30.909026313946203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.480328728170956 and perplexity of 88.26368266615434
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 110.76390933990479 and batch: 50, loss is 3.686360950469971 and perplexity is 39.89938660502371
At time: 111.37157988548279 and batch: 100, loss is 3.652407474517822 and perplexity is 38.56740441496117
At time: 111.94419431686401 and batch: 150, loss is 3.5923010969161986 and perplexity is 36.31755005104422
At time: 112.51177215576172 and batch: 200, loss is 3.5365627288818358 and perplexity is 34.34865042234543
At time: 113.07882022857666 and batch: 250, loss is 3.504111399650574 and perplexity is 33.2518830855661
At time: 113.64638471603394 and batch: 300, loss is 3.59230309009552 and perplexity is 36.317622438506135
At time: 114.21353554725647 and batch: 350, loss is 3.5513892936706544 and perplexity is 34.861717022040644
At time: 114.7945761680603 and batch: 400, loss is 3.5818769407272337 and perplexity is 35.94093658602661
At time: 115.36491966247559 and batch: 450, loss is 3.523330807685852 and perplexity is 33.89714551303642
At time: 115.93244957923889 and batch: 500, loss is 3.47918390750885 and perplexity is 32.43324274667336
At time: 116.49958372116089 and batch: 550, loss is 3.477912015914917 and perplexity is 32.392017400503036
At time: 117.06777739524841 and batch: 600, loss is 3.5462387895584104 and perplexity is 34.682623212771254
At time: 117.6363046169281 and batch: 650, loss is 3.429426145553589 and perplexity is 30.858929133949577
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.4803194532207415 and perplexity of 88.26286402868831
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 118.66949558258057 and batch: 50, loss is 3.686127791404724 and perplexity is 39.89008478578284
At time: 119.23642110824585 and batch: 100, loss is 3.652192029953003 and perplexity is 38.55909617231579
At time: 119.8032968044281 and batch: 150, loss is 3.5920637130737303 and perplexity is 36.30892987464985
At time: 120.37562465667725 and batch: 200, loss is 3.536323556900024 and perplexity is 34.340436169900045
At time: 120.9517469406128 and batch: 250, loss is 3.503864359855652 and perplexity is 33.24366956176326
At time: 121.51683974266052 and batch: 300, loss is 3.592080302238464 and perplexity is 36.309532214464994
At time: 122.09577345848083 and batch: 350, loss is 3.5511528921127318 and perplexity is 34.853476631883694
At time: 122.66692113876343 and batch: 400, loss is 3.5816375112533567 and perplexity is 35.932332296590516
At time: 123.238853931427 and batch: 450, loss is 3.5230673265457155 and perplexity is 33.88821543099503
At time: 123.81170082092285 and batch: 500, loss is 3.478873586654663 and perplexity is 32.42317959656358
At time: 124.41805863380432 and batch: 550, loss is 3.477569727897644 and perplexity is 32.38093189841704
At time: 125.00306439399719 and batch: 600, loss is 3.5458615446090698 and perplexity is 34.66954183593116
At time: 125.58639335632324 and batch: 650, loss is 3.429063959121704 and perplexity is 30.84775447229233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.480316162109375 and perplexity of 88.26257354625126
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 126.61112427711487 and batch: 50, loss is 3.686075553894043 and perplexity is 39.88800108147701
At time: 127.18798041343689 and batch: 100, loss is 3.6521438932418824 and perplexity is 38.55724010891502
At time: 127.75257587432861 and batch: 150, loss is 3.5920108127593995 and perplexity is 36.3070091716498
At time: 128.31655931472778 and batch: 200, loss is 3.536269736289978 and perplexity is 34.33858799641149
At time: 128.88072562217712 and batch: 250, loss is 3.503808674812317 and perplexity is 33.24181843812352
At time: 129.4453318119049 and batch: 300, loss is 3.5920297956466674 and perplexity is 36.3076983900536
At time: 130.0095555782318 and batch: 350, loss is 3.5510989952087404 and perplexity is 34.851598188021505
At time: 130.57432627677917 and batch: 400, loss is 3.581583342552185 and perplexity is 35.93038594153618
At time: 131.14408779144287 and batch: 450, loss is 3.523008255958557 and perplexity is 33.886213693334255
At time: 131.71685528755188 and batch: 500, loss is 3.4788041639328005 and perplexity is 32.42092876931472
At time: 132.2902708053589 and batch: 550, loss is 3.4774926137924194 and perplexity is 32.37843496810287
At time: 132.86108469963074 and batch: 600, loss is 3.5457768774032594 and perplexity is 34.66660658695861
At time: 133.4301974773407 and batch: 650, loss is 3.428982625007629 and perplexity is 30.845245599540984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.48031676049326 and perplexity of 88.2626263611687
Annealing...
Model not improving. Stopping early with 88.25024212822716loss at 16 epochs.
Finished Training.
Improved accuracyfrom -90.33481959178064 to -88.25024212822716
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb2b5177748>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'tune_wordvecs': True, 'data': 'ptb', 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.0, 'anneal': 2.0, 'wordvec_dim': 200, 'lr': 1.5290315552841394, 'seq_len': 20}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.814727783203125 and batch: 50, loss is 7.080039558410644 and perplexity is 1188.0155135850998
At time: 1.3816983699798584 and batch: 100, loss is 5.8656241321563725 and perplexity is 352.702220223285
At time: 1.9485819339752197 and batch: 150, loss is 5.69527024269104 and perplexity is 297.45716835091696
At time: 2.5158047676086426 and batch: 200, loss is 5.551675910949707 and perplexity is 257.6690245929538
At time: 3.0827102661132812 and batch: 250, loss is 5.456221990585327 and perplexity is 234.21089978619736
At time: 3.6492254734039307 and batch: 300, loss is 5.420807847976684 and perplexity is 226.06167222040418
At time: 4.216425895690918 and batch: 350, loss is 5.291142330169678 and perplexity is 198.57012854844885
At time: 4.783074140548706 and batch: 400, loss is 5.293006763458252 and perplexity is 198.94069464677162
At time: 5.3493077754974365 and batch: 450, loss is 5.205935974121093 and perplexity is 182.35146917550725
At time: 5.91579270362854 and batch: 500, loss is 5.185735578536987 and perplexity is 178.70485290949924
At time: 6.483197212219238 and batch: 550, loss is 5.172000350952149 and perplexity is 176.26708108467886
At time: 7.048998117446899 and batch: 600, loss is 5.190757703781128 and perplexity is 179.60458846379257
At time: 7.614765882492065 and batch: 650, loss is 5.111448898315429 and perplexity is 165.91056835225152
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.198493508731618 and perplexity of 180.9993624224313
Finished 1 epochs...
Completing Train Step...
At time: 8.686513900756836 and batch: 50, loss is 5.095964336395264 and perplexity is 163.36130394127852
At time: 9.253080129623413 and batch: 100, loss is 5.0399132919311525 and perplexity is 154.45662180985144
At time: 9.819499015808105 and batch: 150, loss is 4.973385286331177 and perplexity is 144.51528584196532
At time: 10.386648178100586 and batch: 200, loss is 4.937694692611695 and perplexity is 139.44840728624712
At time: 10.952558755874634 and batch: 250, loss is 4.919055976867676 and perplexity is 136.87334057819882
At time: 11.518406629562378 and batch: 300, loss is 4.960948429107666 and perplexity is 142.72910015669714
At time: 12.084871530532837 and batch: 350, loss is 4.877434387207031 and perplexity is 131.2933834211639
At time: 12.651287078857422 and batch: 400, loss is 4.910769023895264 and perplexity is 135.74376447799065
At time: 13.219137907028198 and batch: 450, loss is 4.854954633712769 and perplexity is 128.37486719151565
At time: 13.786075592041016 and batch: 500, loss is 4.847205629348755 and perplexity is 127.3839341163282
At time: 14.352624416351318 and batch: 550, loss is 4.860766706466674 and perplexity is 129.12316372990162
At time: 14.927692651748657 and batch: 600, loss is 4.908562393188476 and perplexity is 135.4445583591517
At time: 15.5200035572052 and batch: 650, loss is 4.827751579284668 and perplexity is 124.9297499954476
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.956285663679535 and perplexity of 142.06513699808488
Finished 2 epochs...
Completing Train Step...
At time: 16.595722913742065 and batch: 50, loss is 4.8360346412658695 and perplexity is 125.96884837513598
At time: 17.17540454864502 and batch: 100, loss is 4.790576162338257 and perplexity is 120.37070175271948
At time: 17.741188287734985 and batch: 150, loss is 4.736620845794678 and perplexity is 114.04816347496802
At time: 18.30817413330078 and batch: 200, loss is 4.699804611206055 and perplexity is 109.92569210527829
At time: 18.87504744529724 and batch: 250, loss is 4.688207626342773 and perplexity is 108.65824898204829
At time: 19.43994426727295 and batch: 300, loss is 4.755882606506348 and perplexity is 116.26622521920606
At time: 20.0071280002594 and batch: 350, loss is 4.68612135887146 and perplexity is 108.4317951155118
At time: 20.582672357559204 and batch: 400, loss is 4.729356288909912 and perplexity is 113.22265620648189
At time: 21.159377574920654 and batch: 450, loss is 4.677397108078003 and perplexity is 107.48992347714709
At time: 21.7266263961792 and batch: 500, loss is 4.666590890884399 and perplexity is 106.33461750404929
At time: 22.29550075531006 and batch: 550, loss is 4.688628005981445 and perplexity is 108.70393629983032
At time: 22.863983392715454 and batch: 600, loss is 4.747948074340821 and perplexity is 115.3473573295963
At time: 23.43191957473755 and batch: 650, loss is 4.663864183425903 and perplexity is 106.04506904577282
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.839001225490196 and perplexity of 126.3431004241236
Finished 3 epochs...
Completing Train Step...
At time: 24.471998691558838 and batch: 50, loss is 4.679664869308471 and perplexity is 107.733961563745
At time: 25.042031288146973 and batch: 100, loss is 4.638399457931518 and perplexity is 103.37875305487579
At time: 25.612919330596924 and batch: 150, loss is 4.58927059173584 and perplexity is 98.42261370555494
At time: 26.182954788208008 and batch: 200, loss is 4.547452592849732 and perplexity is 94.39164782902408
At time: 26.752739906311035 and batch: 250, loss is 4.539124698638916 and perplexity is 93.60882831485797
At time: 27.322938203811646 and batch: 300, loss is 4.619355707168579 and perplexity is 101.42866131344803
At time: 27.892935752868652 and batch: 350, loss is 4.557118473052978 and perplexity is 95.30844990062458
At time: 28.461738109588623 and batch: 400, loss is 4.605974588394165 and perplexity is 100.08047260244565
At time: 29.031380653381348 and batch: 450, loss is 4.556701555252075 and perplexity is 95.26872239341243
At time: 29.614163398742676 and batch: 500, loss is 4.542555274963379 and perplexity is 93.93051200978228
At time: 30.18402099609375 and batch: 550, loss is 4.567967081069947 and perplexity is 96.34804278617148
At time: 30.754199266433716 and batch: 600, loss is 4.634492244720459 and perplexity is 102.9756183047727
At time: 31.323524475097656 and batch: 650, loss is 4.546836423873901 and perplexity is 94.33350453893964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.765850889916513 and perplexity of 117.43099563911703
Finished 4 epochs...
Completing Train Step...
At time: 32.359416246414185 and batch: 50, loss is 4.566417856216431 and perplexity is 96.19889356636966
At time: 32.94233727455139 and batch: 100, loss is 4.527780952453614 and perplexity is 92.55295363545669
At time: 33.512457847595215 and batch: 150, loss is 4.4807041645050045 and perplexity is 88.29682628087346
At time: 34.083012104034424 and batch: 200, loss is 4.434947166442871 and perplexity is 84.34766838320897
At time: 34.65301561355591 and batch: 250, loss is 4.428911447525024 and perplexity is 83.8401028677433
At time: 35.22303342819214 and batch: 300, loss is 4.516936292648316 and perplexity is 91.55467114003305
At time: 35.792399168014526 and batch: 350, loss is 4.459284801483154 and perplexity is 86.42567547239175
At time: 36.362388372421265 and batch: 400, loss is 4.512500076293946 and perplexity is 91.14941437856054
At time: 36.93307399749756 and batch: 450, loss is 4.46476674079895 and perplexity is 86.90075677434396
At time: 37.50470185279846 and batch: 500, loss is 4.448237171173096 and perplexity is 85.4761313244767
At time: 38.07522535324097 and batch: 550, loss is 4.475049600601197 and perplexity is 87.79895518271573
At time: 38.64639711380005 and batch: 600, loss is 4.547011203765869 and perplexity is 94.34999357960633
At time: 39.216766595840454 and batch: 650, loss is 4.456181793212891 and perplexity is 86.1579115383394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.7142181396484375 and perplexity of 111.52158277402351
Finished 5 epochs...
Completing Train Step...
At time: 40.26407027244568 and batch: 50, loss is 4.4770204448699955 and perplexity is 87.97216387796112
At time: 40.83428716659546 and batch: 100, loss is 4.440670309066772 and perplexity is 84.83178613541762
At time: 41.4137237071991 and batch: 150, loss is 4.3948760604858395 and perplexity is 81.03458675300314
At time: 41.98276329040527 and batch: 200, loss is 4.346433372497558 and perplexity is 77.2026183184396
At time: 42.55214762687683 and batch: 250, loss is 4.341041650772095 and perplexity is 76.78748343555961
At time: 43.12188100814819 and batch: 300, loss is 4.434341382980347 and perplexity is 84.29658743415914
At time: 43.68995499610901 and batch: 350, loss is 4.381033067703247 and perplexity is 79.92055411628782
At time: 44.27292823791504 and batch: 400, loss is 4.437259283065796 and perplexity is 84.54291565963867
At time: 44.84245705604553 and batch: 450, loss is 4.38955454826355 and perplexity is 80.6045055652545
At time: 45.41232633590698 and batch: 500, loss is 4.371275520324707 and perplexity is 79.14451778578453
At time: 45.98710536956787 and batch: 550, loss is 4.399329891204834 and perplexity is 81.39630600500276
At time: 46.56009078025818 and batch: 600, loss is 4.475134286880493 and perplexity is 87.80639086440195
At time: 47.133788108825684 and batch: 650, loss is 4.382180671691895 and perplexity is 80.01232391058592
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.6751547420726105 and perplexity of 107.24916176564165
Finished 6 epochs...
Completing Train Step...
At time: 48.18471169471741 and batch: 50, loss is 4.4028866481781 and perplexity is 81.6863283477347
At time: 48.75790286064148 and batch: 100, loss is 4.368484773635864 and perplexity is 78.92395339765294
At time: 49.33061242103577 and batch: 150, loss is 4.323569345474243 and perplexity is 75.45748201259909
At time: 49.90343689918518 and batch: 200, loss is 4.273270969390869 and perplexity is 71.75596373122778
At time: 50.476269483566284 and batch: 250, loss is 4.267786111831665 and perplexity is 71.36346986159307
At time: 51.048073530197144 and batch: 300, loss is 4.364452705383301 and perplexity is 78.60636732541596
At time: 51.6196506023407 and batch: 350, loss is 4.315025520324707 and perplexity is 74.81553273626854
At time: 52.19151520729065 and batch: 400, loss is 4.373678693771362 and perplexity is 79.3349445119549
At time: 52.76454162597656 and batch: 450, loss is 4.325490407943725 and perplexity is 75.60257987574136
At time: 53.33743119239807 and batch: 500, loss is 4.306226615905762 and perplexity is 74.16012567521418
At time: 53.90992331504822 and batch: 550, loss is 4.33469934463501 and perplexity is 76.30201483069492
At time: 54.48310112953186 and batch: 600, loss is 4.413952770233155 and perplexity is 82.5952993434931
At time: 55.056113719940186 and batch: 650, loss is 4.3189105129241945 and perplexity is 75.10675586082763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.64754441205193 and perplexity of 104.3284830081811
Finished 7 epochs...
Completing Train Step...
At time: 56.1019287109375 and batch: 50, loss is 4.338961400985718 and perplexity is 76.6279123209794
At time: 56.685072898864746 and batch: 100, loss is 4.30698805809021 and perplexity is 74.21661582757037
At time: 57.253456115722656 and batch: 150, loss is 4.262591152191162 and perplexity is 70.99370081509038
At time: 57.82329034805298 and batch: 200, loss is 4.2102654504776 and perplexity is 67.37442200909277
At time: 58.393415689468384 and batch: 250, loss is 4.204665551185608 and perplexity is 66.99818645478581
At time: 58.974716663360596 and batch: 300, loss is 4.303851747512818 and perplexity is 73.9842141028264
At time: 59.54718351364136 and batch: 350, loss is 4.258464875221253 and perplexity is 70.70136468710469
At time: 60.11860942840576 and batch: 400, loss is 4.318616104125977 and perplexity is 75.08464702577409
At time: 60.6893846988678 and batch: 450, loss is 4.2695839214324955 and perplexity is 71.49188318962858
At time: 61.25977635383606 and batch: 500, loss is 4.2498654985427855 and perplexity is 70.09598370066614
At time: 61.8294312953949 and batch: 550, loss is 4.278398876190185 and perplexity is 72.12486666710186
At time: 62.399075508117676 and batch: 600, loss is 4.360513639450073 and perplexity is 78.29734069945454
At time: 62.96878981590271 and batch: 650, loss is 4.263264288902283 and perplexity is 71.04150536906238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.626072902305453 and perplexity of 102.11227082278788
Finished 8 epochs...
Completing Train Step...
At time: 64.00923800468445 and batch: 50, loss is 4.282642846107483 and perplexity is 72.43161288197265
At time: 64.5773491859436 and batch: 100, loss is 4.252983741760254 and perplexity is 70.31490116798042
At time: 65.14555168151855 and batch: 150, loss is 4.209306755065918 and perplexity is 67.30986141176695
At time: 65.71262240409851 and batch: 200, loss is 4.155019669532776 and perplexity is 63.75321853150173
At time: 66.27967262268066 and batch: 250, loss is 4.149304151535034 and perplexity is 63.38987520036047
At time: 66.8481113910675 and batch: 300, loss is 4.250537710189819 and perplexity is 70.14311887795269
At time: 67.41597199440002 and batch: 350, loss is 4.208757972717285 and perplexity is 67.27293308166986
At time: 67.98378443717957 and batch: 400, loss is 4.269560041427613 and perplexity is 71.49017598349305
At time: 68.55219173431396 and batch: 450, loss is 4.219901428222657 and perplexity is 68.02677843846365
At time: 69.12014055252075 and batch: 500, loss is 4.200044245719909 and perplexity is 66.6892816909265
At time: 69.68826532363892 and batch: 550, loss is 4.228614311218262 and perplexity is 68.62207741877522
At time: 70.25699639320374 and batch: 600, loss is 4.312751007080078 and perplexity is 74.64555719524502
At time: 70.82444739341736 and batch: 650, loss is 4.213565611839295 and perplexity is 67.59713576691038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.609136843213848 and perplexity of 100.39745348229958
Finished 9 epochs...
Completing Train Step...
At time: 71.8480806350708 and batch: 50, loss is 4.2325018835067745 and perplexity is 68.8893699281674
At time: 72.42914175987244 and batch: 100, loss is 4.204490871429443 and perplexity is 66.98648425001132
At time: 72.99648928642273 and batch: 150, loss is 4.161848363876342 and perplexity is 64.19005960382263
At time: 73.57609605789185 and batch: 200, loss is 4.105819973945618 and perplexity is 60.69249041049429
At time: 74.14315795898438 and batch: 250, loss is 4.100040831565857 and perplexity is 60.342751436089685
At time: 74.71107482910156 and batch: 300, loss is 4.202790660858154 and perplexity is 66.8726898859586
At time: 75.27796983718872 and batch: 350, loss is 4.1643604564666745 and perplexity is 64.35151368582179
At time: 75.84547877311707 and batch: 400, loss is 4.2252715921401975 and perplexity is 68.39307604827013
At time: 76.41433906555176 and batch: 450, loss is 4.17474362373352 and perplexity is 65.02316712706457
At time: 76.98215293884277 and batch: 500, loss is 4.155466289520263 and perplexity is 63.7816983525198
At time: 77.55013942718506 and batch: 550, loss is 4.183714289665222 and perplexity is 65.60909237777436
At time: 78.11665177345276 and batch: 600, loss is 4.269717645645142 and perplexity is 71.50144402466204
At time: 78.68571972846985 and batch: 650, loss is 4.168651485443116 and perplexity is 64.62824119399001
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.595336315678615 and perplexity of 99.02143240846381
Finished 10 epochs...
Completing Train Step...
At time: 79.72359299659729 and batch: 50, loss is 4.187430787086487 and perplexity is 65.85338207022934
At time: 80.29208374023438 and batch: 100, loss is 4.160846810340882 and perplexity is 64.12580200676821
At time: 80.86085629463196 and batch: 150, loss is 4.118939666748047 and perplexity is 61.494003546393905
At time: 81.4301929473877 and batch: 200, loss is 4.061453366279602 and perplexity is 58.05863024623976
At time: 82.0024082660675 and batch: 250, loss is 4.055730929374695 and perplexity is 57.72734218940883
At time: 82.5733551979065 and batch: 300, loss is 4.159620246887207 and perplexity is 64.04719585915804
At time: 83.14328598976135 and batch: 350, loss is 4.123944120407105 and perplexity is 61.802518768632844
At time: 83.71354603767395 and batch: 400, loss is 4.184744215011596 and perplexity is 65.67669965421447
At time: 84.28431558609009 and batch: 450, loss is 4.13338794708252 and perplexity is 62.38893569556151
At time: 84.85498380661011 and batch: 500, loss is 4.115048441886902 and perplexity is 61.25518150745667
At time: 85.42517805099487 and batch: 550, loss is 4.142797837257385 and perplexity is 62.97877955921736
At time: 85.99566793441772 and batch: 600, loss is 4.230373206138611 and perplexity is 68.74288265288251
At time: 86.56591296195984 and batch: 650, loss is 4.1274894332885745 and perplexity is 62.02201689939539
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.585307700961244 and perplexity of 98.03334745693829
Finished 11 epochs...
Completing Train Step...
At time: 87.61433339118958 and batch: 50, loss is 4.146262197494507 and perplexity is 63.19733910481225
At time: 88.1852798461914 and batch: 100, loss is 4.121146330833435 and perplexity is 61.62984998409019
At time: 88.76901006698608 and batch: 150, loss is 4.079768943786621 and perplexity is 59.13180550025746
At time: 89.33896398544312 and batch: 200, loss is 4.020865774154663 and perplexity is 55.74935128642901
At time: 89.909432888031 and batch: 250, loss is 4.015243616104126 and perplexity is 55.43679905458441
At time: 90.4801995754242 and batch: 300, loss is 4.120039319992065 and perplexity is 61.561662820933705
At time: 91.05066585540771 and batch: 350, loss is 4.086740856170654 and perplexity is 59.54550773944387
At time: 91.6208119392395 and batch: 400, loss is 4.147321605682373 and perplexity is 63.264326360469575
At time: 92.19164204597473 and batch: 450, loss is 4.09532922744751 and perplexity is 60.05910901004208
At time: 92.76172089576721 and batch: 500, loss is 4.077784056663513 and perplexity is 59.01455194698397
At time: 93.3316023349762 and batch: 550, loss is 4.1051181173324585 and perplexity is 60.649907929876406
At time: 93.90140652656555 and batch: 600, loss is 4.194035363197327 and perplexity is 66.28975518777158
At time: 94.47152781486511 and batch: 650, loss is 4.089791569709778 and perplexity is 59.72744139873494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.577093386182598 and perplexity of 97.2313690431415
Finished 12 epochs...
Completing Train Step...
At time: 95.50512933731079 and batch: 50, loss is 4.108306722640991 and perplexity is 60.84360519621621
At time: 96.08778667449951 and batch: 100, loss is 4.084511389732361 and perplexity is 59.41290090455867
At time: 96.65792417526245 and batch: 150, loss is 4.043579525947571 and perplexity is 57.03011866695688
At time: 97.22673034667969 and batch: 200, loss is 3.9836519956588745 and perplexity is 53.712835519281654
At time: 97.79715943336487 and batch: 250, loss is 3.9779700422286988 and perplexity is 53.40850709797662
At time: 98.36698722839355 and batch: 300, loss is 4.083568453788757 and perplexity is 59.356904749324706
At time: 98.93612051010132 and batch: 350, loss is 4.05232264995575 and perplexity is 57.53092618755928
At time: 99.5062096118927 and batch: 400, loss is 4.112535576820374 and perplexity is 61.10144873745175
At time: 100.07768774032593 and batch: 450, loss is 4.059920101165772 and perplexity is 57.96967918413286
At time: 100.66989517211914 and batch: 500, loss is 4.043243079185486 and perplexity is 57.01093429563083
At time: 101.24846005439758 and batch: 550, loss is 4.070352482795715 and perplexity is 58.57760655188197
At time: 101.81838846206665 and batch: 600, loss is 4.160289492607117 and perplexity is 64.09007351710315
At time: 102.38730692863464 and batch: 650, loss is 4.054880075454712 and perplexity is 57.67824554401531
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.570886350145527 and perplexity of 96.62971959312517
Finished 13 epochs...
Completing Train Step...
At time: 103.4314935207367 and batch: 50, loss is 4.073086185455322 and perplexity is 58.737959389333916
At time: 104.00007271766663 and batch: 100, loss is 4.050478167533875 and perplexity is 57.42490920877586
At time: 104.56807470321655 and batch: 150, loss is 4.010008416175842 and perplexity is 55.14733469117672
At time: 105.13559007644653 and batch: 200, loss is 3.9493716621398924 and perplexity is 51.90274412768485
At time: 105.7036943435669 and batch: 250, loss is 3.943411707878113 and perplexity is 51.59432613812246
At time: 106.27202486991882 and batch: 300, loss is 4.049529237747192 and perplexity is 57.37044284839308
At time: 106.84010910987854 and batch: 350, loss is 4.020244479179382 and perplexity is 55.714725252203976
At time: 107.40854167938232 and batch: 400, loss is 4.080160131454468 and perplexity is 59.154941658341514
At time: 107.97750639915466 and batch: 450, loss is 4.026901931762695 and perplexity is 56.08688082325125
At time: 108.54604458808899 and batch: 500, loss is 4.0110106277465825 and perplexity is 55.20263169311879
At time: 109.11513686180115 and batch: 550, loss is 4.037926859855652 and perplexity is 56.70865586585268
At time: 109.6837146282196 and batch: 600, loss is 4.128718252182007 and perplexity is 62.098277571251245
At time: 110.2519018650055 and batch: 650, loss is 4.0222893333435055 and perplexity is 55.82877030314832
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.566670137293198 and perplexity of 96.22316578840487
Finished 14 epochs...
Completing Train Step...
At time: 111.28415727615356 and batch: 50, loss is 4.040108733177185 and perplexity is 56.83252205020842
At time: 111.86645579338074 and batch: 100, loss is 4.018544054031372 and perplexity is 55.62006703468565
At time: 112.43480062484741 and batch: 150, loss is 3.9786348438262937 and perplexity is 53.44402496367932
At time: 113.00336718559265 and batch: 200, loss is 3.917828722000122 and perplexity is 50.29113012033702
At time: 113.57137632369995 and batch: 250, loss is 3.9113435888290407 and perplexity is 49.96604070760452
At time: 114.13896918296814 and batch: 300, loss is 4.017766122817993 and perplexity is 55.57681527407771
At time: 114.70700740814209 and batch: 350, loss is 3.9901782274246216 and perplexity is 54.06452428562421
At time: 115.27477240562439 and batch: 400, loss is 4.049913816452026 and perplexity is 57.39251054211
At time: 115.84310579299927 and batch: 450, loss is 3.995858759880066 and perplexity is 54.372513513725835
At time: 116.41361284255981 and batch: 500, loss is 3.980657458305359 and perplexity is 53.552231015064535
At time: 116.98873996734619 and batch: 550, loss is 4.007626609802246 and perplexity is 55.01614071926363
At time: 117.55815577507019 and batch: 600, loss is 4.099017663002014 and perplexity is 60.281042204620846
At time: 118.13999438285828 and batch: 650, loss is 3.991764044761658 and perplexity is 54.15032876268675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.564097385780484 and perplexity of 95.97592567312945
Finished 15 epochs...
Completing Train Step...
At time: 119.18537878990173 and batch: 50, loss is 4.009162459373474 and perplexity is 55.10070215559861
At time: 119.75431990623474 and batch: 100, loss is 3.988410725593567 and perplexity is 53.96904954070326
At time: 120.32401013374329 and batch: 150, loss is 3.949244666099548 and perplexity is 51.89615310322349
At time: 120.89322662353516 and batch: 200, loss is 3.8881435585021973 and perplexity is 48.82017055479297
At time: 121.46293997764587 and batch: 250, loss is 3.8812721586227417 and perplexity is 48.485857555292974
At time: 122.0323543548584 and batch: 300, loss is 3.9880331993103026 and perplexity is 53.94867865153384
At time: 122.60140442848206 and batch: 350, loss is 3.961950869560242 and perplexity is 52.55976323791035
At time: 123.17001008987427 and batch: 400, loss is 4.021638841629028 and perplexity is 55.7924659597541
At time: 123.73732209205627 and batch: 450, loss is 3.966630320549011 and perplexity is 52.806290430064315
At time: 124.30471014976501 and batch: 500, loss is 3.9520730209350585 and perplexity is 52.04314160863508
At time: 124.87183427810669 and batch: 550, loss is 3.9791141033172606 and perplexity is 53.46964465862923
At time: 125.43968772888184 and batch: 600, loss is 4.070974297523499 and perplexity is 58.61404229732181
At time: 126.00876212120056 and batch: 650, loss is 3.962964563369751 and perplexity is 52.61306975821399
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5619955623851105 and perplexity of 95.77441307328995
Finished 16 epochs...
Completing Train Step...
At time: 127.05615305900574 and batch: 50, loss is 3.980172634124756 and perplexity is 53.52627389137225
At time: 127.6253674030304 and batch: 100, loss is 3.960067138671875 and perplexity is 52.46084798252438
At time: 128.19472980499268 and batch: 150, loss is 3.9214021587371826 and perplexity is 50.47116377010881
At time: 128.76395988464355 and batch: 200, loss is 3.860168571472168 and perplexity is 47.47335334744446
At time: 129.3339204788208 and batch: 250, loss is 3.852967562675476 and perplexity is 47.132725217386145
At time: 129.9030385017395 and batch: 300, loss is 3.9600467920303344 and perplexity is 52.45978059131452
At time: 130.4723162651062 and batch: 350, loss is 3.9353126525878905 and perplexity is 51.17814843609604
At time: 131.0415563583374 and batch: 400, loss is 3.994849348068237 and perplexity is 54.31765694743707
At time: 131.61105251312256 and batch: 450, loss is 3.93904869556427 and perplexity is 51.36970981606086
At time: 132.17956852912903 and batch: 500, loss is 3.9251254034042358 and perplexity is 50.65943052556438
At time: 132.7704999446869 and batch: 550, loss is 3.9520971727371217 and perplexity is 52.04439855946872
At time: 133.33879327774048 and batch: 600, loss is 4.0443430519104 and perplexity is 57.07367927093214
At time: 133.90754866600037 and batch: 650, loss is 3.935664663314819 and perplexity is 51.196166864483864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5612649356617645 and perplexity of 95.70446328439006
Finished 17 epochs...
Completing Train Step...
At time: 134.94350242614746 and batch: 50, loss is 3.9526967096328733 and perplexity is 52.075610452027995
At time: 135.52667570114136 and batch: 100, loss is 3.9332801246643068 and perplexity is 51.07423306154692
At time: 136.097336769104 and batch: 150, loss is 3.894871168136597 and perplexity is 49.14972090465157
At time: 136.6673183441162 and batch: 200, loss is 3.8337092447280883 and perplexity is 46.23371272910989
At time: 137.23800683021545 and batch: 250, loss is 3.8262182378768923 and perplexity is 45.88866964386915
At time: 137.8088240623474 and batch: 300, loss is 3.9336800670623777 and perplexity is 51.09466389810374
At time: 138.37895941734314 and batch: 350, loss is 3.9100601434707642 and perplexity is 49.90195315981702
At time: 138.94897294044495 and batch: 400, loss is 3.9692799425125123 and perplexity is 52.946392664011334
At time: 139.51907444000244 and batch: 450, loss is 3.912777624130249 and perplexity is 50.03774517492144
At time: 140.0891273021698 and batch: 500, loss is 3.8995808124542237 and perplexity is 49.38174455398254
At time: 140.6591670513153 and batch: 550, loss is 3.9263589000701904 and perplexity is 50.72195731958485
At time: 141.22980952262878 and batch: 600, loss is 4.019123291969299 and perplexity is 55.6522936201516
At time: 141.80011248588562 and batch: 650, loss is 3.9096962356567384 and perplexity is 49.88379675295654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5615488688151045 and perplexity of 95.73164081255715
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 142.8511562347412 and batch: 50, loss is 3.927093195915222 and perplexity is 50.75921591984005
At time: 143.42159008979797 and batch: 100, loss is 3.9051319694519044 and perplexity is 49.6566326383805
At time: 143.99171495437622 and batch: 150, loss is 3.8614862966537475 and perplexity is 47.53595141507148
At time: 144.56150841712952 and batch: 200, loss is 3.797095899581909 and perplexity is 44.57155608279024
At time: 145.13164687156677 and batch: 250, loss is 3.7846020078659057 and perplexity is 44.018148195360666
At time: 145.70270681381226 and batch: 300, loss is 3.88778892993927 and perplexity is 48.80286059735134
At time: 146.27331972122192 and batch: 350, loss is 3.861980595588684 and perplexity is 47.55945419344777
At time: 146.84370875358582 and batch: 400, loss is 3.9158880138397216 and perplexity is 50.19362435943305
At time: 147.4271159172058 and batch: 450, loss is 3.855864849090576 and perplexity is 47.269480235429924
At time: 147.99716782569885 and batch: 500, loss is 3.8380731105804444 and perplexity is 46.43591131218446
At time: 148.56836891174316 and batch: 550, loss is 3.8618827056884766 and perplexity is 47.55479883108304
At time: 149.13793969154358 and batch: 600, loss is 3.951270537376404 and perplexity is 52.00139459603939
At time: 149.70840668678284 and batch: 650, loss is 3.8388645124435423 and perplexity is 46.47267532454668
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.549019308651195 and perplexity of 94.5396486225213
Finished 19 epochs...
Completing Train Step...
At time: 150.7415051460266 and batch: 50, loss is 3.896814560890198 and perplexity is 49.2453309899812
At time: 151.32419109344482 and batch: 100, loss is 3.8779114580154417 and perplexity is 48.32318460500936
At time: 151.89389300346375 and batch: 150, loss is 3.8371590852737425 and perplexity is 46.39348710545821
At time: 152.46333384513855 and batch: 200, loss is 3.7743522834777834 and perplexity is 43.56927863266294
At time: 153.03332543373108 and batch: 250, loss is 3.7631167459487913 and perplexity is 43.08249413109529
At time: 153.6032373905182 and batch: 300, loss is 3.8686114311218263 and perplexity is 47.87586097359784
At time: 154.17340350151062 and batch: 350, loss is 3.845350956916809 and perplexity is 46.77509751459422
At time: 154.74288034439087 and batch: 400, loss is 3.9008338165283205 and perplexity is 49.4436588624345
At time: 155.31180834770203 and batch: 450, loss is 3.842067551612854 and perplexity is 46.6217677709194
At time: 155.88081979751587 and batch: 500, loss is 3.8261094379425047 and perplexity is 45.88367723121477
At time: 156.44974493980408 and batch: 550, loss is 3.8516914224624634 and perplexity is 47.07261561368614
At time: 157.0185568332672 and batch: 600, loss is 3.942502999305725 and perplexity is 51.54746322726335
At time: 157.58754563331604 and batch: 650, loss is 3.830920000076294 and perplexity is 46.10493527255309
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.549828622855392 and perplexity of 94.61619187260315
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 158.6207754611969 and batch: 50, loss is 3.8835141801834108 and perplexity is 48.59468584541901
At time: 159.18810772895813 and batch: 100, loss is 3.864659252166748 and perplexity is 47.68702041504539
At time: 159.75591158866882 and batch: 150, loss is 3.822208533287048 and perplexity is 45.70503803486059
At time: 160.3258228302002 and batch: 200, loss is 3.758231244087219 and perplexity is 42.87252783879764
At time: 160.89342379570007 and batch: 250, loss is 3.742900023460388 and perplexity is 42.22025252027298
At time: 161.46088123321533 and batch: 300, loss is 3.847752170562744 and perplexity is 46.8875494735939
At time: 162.028906583786 and batch: 350, loss is 3.8219279193878175 and perplexity is 45.69221436525726
At time: 162.610680103302 and batch: 400, loss is 3.8742673444747924 and perplexity is 48.147409899672425
At time: 163.17822742462158 and batch: 450, loss is 3.8143543577194214 and perplexity is 45.34746868617597
At time: 163.74710273742676 and batch: 500, loss is 3.795166082382202 and perplexity is 44.485624070450626
At time: 164.31533193588257 and batch: 550, loss is 3.818644528388977 and perplexity is 45.54243498658513
At time: 164.8843822479248 and batch: 600, loss is 3.907613215446472 and perplexity is 49.779995943279935
At time: 165.45350861549377 and batch: 650, loss is 3.794228739738464 and perplexity is 44.44394533465592
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.541576310700061 and perplexity of 93.83860239074608
Finished 21 epochs...
Completing Train Step...
At time: 166.49801468849182 and batch: 50, loss is 3.868186831474304 and perplexity is 47.85553721493935
At time: 167.06760478019714 and batch: 100, loss is 3.8486231708526613 and perplexity is 46.92840633336428
At time: 167.6378152370453 and batch: 150, loss is 3.8077625703811644 and perplexity is 45.04953086646583
At time: 168.20697808265686 and batch: 200, loss is 3.7447691535949708 and perplexity is 42.29924146385907
At time: 168.77660179138184 and batch: 250, loss is 3.7306129598617552 and perplexity is 41.70466361584056
At time: 169.34600234031677 and batch: 300, loss is 3.8368683767318728 and perplexity is 46.38000208267016
At time: 169.91516590118408 and batch: 350, loss is 3.8127218866348267 and perplexity is 45.27350064655599
At time: 170.48418283462524 and batch: 400, loss is 3.8664054918289184 and perplexity is 47.770366131083165
At time: 171.0539321899414 and batch: 450, loss is 3.8075914764404297 and perplexity is 45.04182382403457
At time: 171.62294006347656 and batch: 500, loss is 3.789843273162842 and perplexity is 44.24946465385737
At time: 172.19166660308838 and batch: 550, loss is 3.814443073272705 and perplexity is 45.35149189040823
At time: 172.76074838638306 and batch: 600, loss is 3.9044959688186647 and perplexity is 49.6250610294238
At time: 173.33007264137268 and batch: 650, loss is 3.791558346748352 and perplexity is 44.325420858479646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.542367673387714 and perplexity of 93.91289215053456
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 174.3516867160797 and batch: 50, loss is 3.8623094749450684 and perplexity is 47.575098088468955
At time: 174.93114185333252 and batch: 100, loss is 3.843319854736328 and perplexity is 46.6801889291853
At time: 175.49833059310913 and batch: 150, loss is 3.8014698028564453 and perplexity is 44.76693473006167
At time: 176.06704211235046 and batch: 200, loss is 3.7384972858428953 and perplexity is 42.03477642717519
At time: 176.64286518096924 and batch: 250, loss is 3.7216317367553713 and perplexity is 41.33178170162014
At time: 177.23618936538696 and batch: 300, loss is 3.8276132678985597 and perplexity is 45.9527303886187
At time: 177.80365204811096 and batch: 350, loss is 3.8014616918563844 and perplexity is 44.76657162692391
At time: 178.37162232398987 and batch: 400, loss is 3.853412933349609 and perplexity is 47.15372142619072
At time: 178.93926548957825 and batch: 450, loss is 3.794075222015381 and perplexity is 44.4371229250571
At time: 179.50645232200623 and batch: 500, loss is 3.7745054626464842 and perplexity is 43.575953049722564
At time: 180.07380175590515 and batch: 550, loss is 3.7969554662704468 and perplexity is 44.565297191061234
At time: 180.65645551681519 and batch: 600, loss is 3.886103515625 and perplexity is 48.72067683382337
At time: 181.24325251579285 and batch: 650, loss is 3.772213773727417 and perplexity is 43.47620486054575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.538582296932445 and perplexity of 93.55806849399043
Finished 23 epochs...
Completing Train Step...
At time: 182.35906791687012 and batch: 50, loss is 3.8543019485473633 and perplexity is 47.195660440617665
At time: 182.92726397514343 and batch: 100, loss is 3.833853406906128 and perplexity is 46.24037836229033
At time: 183.4960720539093 and batch: 150, loss is 3.7928177976608275 and perplexity is 44.381281719848175
At time: 184.06419134140015 and batch: 200, loss is 3.7306427526474 and perplexity is 41.70590613245298
At time: 184.6326732635498 and batch: 250, loss is 3.7145915842056274 and perplexity is 41.04182153282393
At time: 185.20081782341003 and batch: 300, loss is 3.82149405002594 and perplexity is 45.67239421335707
At time: 185.7704963684082 and batch: 350, loss is 3.7963414907455446 and perplexity is 44.537943587406794
At time: 186.33855319023132 and batch: 400, loss is 3.849288945198059 and perplexity is 46.95966046531659
At time: 186.90601325035095 and batch: 450, loss is 3.790845875740051 and perplexity is 44.2938515286388
At time: 187.4737091064453 and batch: 500, loss is 3.772203288078308 and perplexity is 43.47574898670706
At time: 188.04243755340576 and batch: 550, loss is 3.795368485450745 and perplexity is 44.494629008551335
At time: 188.61091589927673 and batch: 600, loss is 3.8852887868881227 and perplexity is 48.6809988639127
At time: 189.17803072929382 and batch: 650, loss is 3.771618814468384 and perplexity is 43.4503459831684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.538979623831954 and perplexity of 93.59524901719114
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 190.2010018825531 and batch: 50, loss is 3.8518779468536377 and perplexity is 47.08139662356527
At time: 190.78308701515198 and batch: 100, loss is 3.832062392234802 and perplexity is 46.1576352853863
At time: 191.352196931839 and batch: 150, loss is 3.7906699657440184 and perplexity is 44.28606048267364
At time: 191.93419551849365 and batch: 200, loss is 3.7278922986984253 and perplexity is 41.59135356623028
At time: 192.50139212608337 and batch: 250, loss is 3.7110005140304567 and perplexity is 40.89470178834802
At time: 193.07080030441284 and batch: 300, loss is 3.8169532442092895 and perplexity is 45.4654748858333
At time: 193.6397087574005 and batch: 350, loss is 3.7908677530288695 and perplexity is 44.29482056862151
At time: 194.2093985080719 and batch: 400, loss is 3.843187575340271 and perplexity is 46.67401451036911
At time: 194.77822375297546 and batch: 450, loss is 3.7840219831466673 and perplexity is 43.9926239843645
At time: 195.34741497039795 and batch: 500, loss is 3.763724756240845 and perplexity is 43.10869669584165
At time: 195.91683316230774 and batch: 550, loss is 3.7862531995773314 and perplexity is 44.090890636143506
At time: 196.4854598045349 and batch: 600, loss is 3.875090379714966 and perplexity is 48.187053226432596
At time: 197.0530936717987 and batch: 650, loss is 3.761495604515076 and perplexity is 43.01270789675333
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.537261962890625 and perplexity of 93.43462210443964
Finished 25 epochs...
Completing Train Step...
At time: 198.0859136581421 and batch: 50, loss is 3.8474632263183595 and perplexity is 46.874003543143765
At time: 198.6539123058319 and batch: 100, loss is 3.8264036750793458 and perplexity is 45.897179899426824
At time: 199.22185897827148 and batch: 150, loss is 3.785339126586914 and perplexity is 44.050606757896944
At time: 199.7908911705017 and batch: 200, loss is 3.723353877067566 and perplexity is 41.40302215448432
At time: 200.36139345169067 and batch: 250, loss is 3.7069626903533934 and perplexity is 40.72990911897023
At time: 200.93053221702576 and batch: 300, loss is 3.813480191230774 and perplexity is 45.30784477017856
At time: 201.50015115737915 and batch: 350, loss is 3.7879430103302 and perplexity is 44.165458882612484
At time: 202.06910610198975 and batch: 400, loss is 3.8409236431121827 and perplexity is 46.56846722573537
At time: 202.63665509223938 and batch: 450, loss is 3.782442121505737 and perplexity is 43.92317659832322
At time: 203.20560836791992 and batch: 500, loss is 3.7628349685668945 and perplexity is 43.070356168875364
At time: 203.77532958984375 and batch: 550, loss is 3.785908432006836 and perplexity is 44.07569214702154
At time: 204.34639501571655 and batch: 600, loss is 3.8751210260391233 and perplexity is 48.18853000511477
At time: 204.9170823097229 and batch: 650, loss is 3.7616219234466555 and perplexity is 43.01814155923919
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.537405874214921 and perplexity of 93.44806937222559
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 205.9749722480774 and batch: 50, loss is 3.8463763666152953 and perplexity is 46.82308575283094
At time: 206.54504466056824 and batch: 100, loss is 3.825818700790405 and perplexity is 45.87033908061063
At time: 207.1278293132782 and batch: 150, loss is 3.784961767196655 and perplexity is 44.03398698380139
At time: 207.6970019340515 and batch: 200, loss is 3.721987328529358 and perplexity is 41.34648155661631
At time: 208.27178025245667 and batch: 250, loss is 3.7053575801849363 and perplexity is 40.66458556745627
At time: 208.84172821044922 and batch: 300, loss is 3.810907325744629 and perplexity is 45.19142361232933
At time: 209.4123101234436 and batch: 350, loss is 3.785173807144165 and perplexity is 44.04332493806477
At time: 209.98226690292358 and batch: 400, loss is 3.8379190874099733 and perplexity is 46.428759656674906
At time: 210.55116367340088 and batch: 450, loss is 3.778776478767395 and perplexity is 43.76246466120959
At time: 211.12064480781555 and batch: 500, loss is 3.7577902364730833 and perplexity is 42.85362489606068
At time: 211.69032263755798 and batch: 550, loss is 3.781107611656189 and perplexity is 43.86459978089258
At time: 212.25970792770386 and batch: 600, loss is 3.869025664329529 and perplexity is 47.895696853116824
At time: 212.82961082458496 and batch: 650, loss is 3.7558639574050905 and perplexity is 42.771156309691946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5368176628561585 and perplexity of 93.39311831936759
Finished 27 epochs...
Completing Train Step...
At time: 213.86163687705994 and batch: 50, loss is 3.8437165784835816 and perplexity is 46.69871174263683
At time: 214.44345545768738 and batch: 100, loss is 3.822501997947693 and perplexity is 45.71845281662316
At time: 215.01155757904053 and batch: 150, loss is 3.781650924682617 and perplexity is 43.88843846469992
At time: 215.58029890060425 and batch: 200, loss is 3.719347906112671 and perplexity is 41.23749462089719
At time: 216.14911031723022 and batch: 250, loss is 3.7031381368637084 and perplexity is 40.57443290597641
At time: 216.71818017959595 and batch: 300, loss is 3.8090339279174805 and perplexity is 45.10684135037875
At time: 217.28853607177734 and batch: 350, loss is 3.783526406288147 and perplexity is 43.9708276592955
At time: 217.85673308372498 and batch: 400, loss is 3.83670729637146 and perplexity is 46.37253177689466
At time: 218.42499828338623 and batch: 450, loss is 3.778052396774292 and perplexity is 43.730788518021
At time: 218.99393010139465 and batch: 500, loss is 3.7575263833999633 and perplexity is 42.84231932700768
At time: 219.56228637695312 and batch: 550, loss is 3.7812678146362306 and perplexity is 43.871627583418245
At time: 220.13071298599243 and batch: 600, loss is 3.8693429327011106 and perplexity is 47.91089505368987
At time: 220.70238971710205 and batch: 650, loss is 3.7561783361434937 and perplexity is 42.78460476569619
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.536873312557445 and perplexity of 93.398315763121
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 221.74771642684937 and batch: 50, loss is 3.843102159500122 and perplexity is 46.67002798046541
At time: 222.3164393901825 and batch: 100, loss is 3.8221990776062014 and perplexity is 45.70460586465108
At time: 222.8861060142517 and batch: 150, loss is 3.781665253639221 and perplexity is 43.88906734473568
At time: 223.45544600486755 and batch: 200, loss is 3.7185738372802732 and perplexity is 41.205586312791624
At time: 224.02489233016968 and batch: 250, loss is 3.7021765899658203 and perplexity is 40.53543743687278
At time: 224.59288597106934 and batch: 300, loss is 3.8075572872161865 and perplexity is 45.04028390534399
At time: 225.1604127883911 and batch: 350, loss is 3.7819483947753905 and perplexity is 43.90149590456433
At time: 225.72714042663574 and batch: 400, loss is 3.835058259963989 and perplexity is 46.29612479994594
At time: 226.29345798492432 and batch: 450, loss is 3.7759240198135378 and perplexity is 43.63781189501337
At time: 226.86276412010193 and batch: 500, loss is 3.7544566822052 and perplexity is 42.71100785478775
At time: 227.4321448802948 and batch: 550, loss is 3.7785735082626344 and perplexity is 43.75358307304843
At time: 228.00168871879578 and batch: 600, loss is 3.8658035945892335 and perplexity is 47.74162193096527
At time: 228.5706181526184 and batch: 650, loss is 3.75277801990509 and perplexity is 42.639370640270116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5368203555836395 and perplexity of 93.3933698019224
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 229.60044288635254 and batch: 50, loss is 3.8421581268310545 and perplexity is 46.62599073895337
At time: 230.1797444820404 and batch: 100, loss is 3.8212207746505737 and perplexity is 45.65991477792341
At time: 230.74644207954407 and batch: 150, loss is 3.780949144363403 and perplexity is 43.85764922724843
At time: 231.3132984638214 and batch: 200, loss is 3.717706799507141 and perplexity is 41.169874996761514
At time: 231.8796968460083 and batch: 250, loss is 3.7012671041488647 and perplexity is 40.49858779109502
At time: 232.444815158844 and batch: 300, loss is 3.8065929508209226 and perplexity is 44.99687085607708
At time: 233.01021122932434 and batch: 350, loss is 3.7808964681625366 and perplexity is 43.85533903375484
At time: 233.57415962219238 and batch: 400, loss is 3.834059362411499 and perplexity is 46.249902803556374
At time: 234.14001321792603 and batch: 450, loss is 3.7747776460647584 and perplexity is 43.587815315861036
At time: 234.70564532279968 and batch: 500, loss is 3.7528043937683107 and perplexity is 42.64049522002888
At time: 235.27313470840454 and batch: 550, loss is 3.777084107398987 and perplexity is 43.68846495416073
At time: 235.8408646583557 and batch: 600, loss is 3.864074692726135 and perplexity is 47.65915266303939
At time: 236.42151498794556 and batch: 650, loss is 3.7510202026367185 and perplexity is 42.56448425581878
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.536973541858149 and perplexity of 93.40767748014825
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 237.46598410606384 and batch: 50, loss is 3.841581015586853 and perplexity is 46.59909011849777
At time: 238.0348083972931 and batch: 100, loss is 3.8205956077575682 and perplexity is 45.63137863172314
At time: 238.60407209396362 and batch: 150, loss is 3.7804229784011842 and perplexity is 43.83457889498616
At time: 239.17340326309204 and batch: 200, loss is 3.7172039461135866 and perplexity is 41.14917778967372
At time: 239.74278664588928 and batch: 250, loss is 3.7006492137908937 and perplexity is 40.47357183354284
At time: 240.3126335144043 and batch: 300, loss is 3.806039252281189 and perplexity is 44.97196305073557
At time: 240.88110399246216 and batch: 350, loss is 3.78031702041626 and perplexity is 43.82993451739523
At time: 241.45025396347046 and batch: 400, loss is 3.833471837043762 and perplexity is 46.222737793251675
At time: 242.01948165893555 and batch: 450, loss is 3.774206266403198 and perplexity is 43.56291723850259
At time: 242.58810758590698 and batch: 500, loss is 3.751969475746155 and perplexity is 42.604908760051664
At time: 243.15747928619385 and batch: 550, loss is 3.776194829940796 and perplexity is 43.64963105670803
At time: 243.7269892692566 and batch: 600, loss is 3.8631581926345824 and perplexity is 47.615493055333694
At time: 244.2968945503235 and batch: 650, loss is 3.7501155138015747 and perplexity is 42.52599405559132
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.537103690353095 and perplexity of 93.41983513992206
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 245.343177318573 and batch: 50, loss is 3.8412773323059084 and perplexity is 46.58494090247047
At time: 245.9134645462036 and batch: 100, loss is 3.820266509056091 and perplexity is 45.61636387507281
At time: 246.4836995601654 and batch: 150, loss is 3.7800969886779785 and perplexity is 43.82029160162727
At time: 247.05371832847595 and batch: 200, loss is 3.71693217754364 and perplexity is 41.13799625593489
At time: 247.62444710731506 and batch: 250, loss is 3.7002948236465456 and perplexity is 40.45923093986425
At time: 248.19421076774597 and batch: 300, loss is 3.805751519203186 and perplexity is 44.95902499082631
At time: 248.76424384117126 and batch: 350, loss is 3.780023798942566 and perplexity is 43.81708452344327
At time: 249.33475184440613 and batch: 400, loss is 3.833139853477478 and perplexity is 46.20739515080915
At time: 249.90480661392212 and batch: 450, loss is 3.773910307884216 and perplexity is 43.55002632971539
At time: 250.47528457641602 and batch: 500, loss is 3.751552143096924 and perplexity is 42.58713205026729
At time: 251.0581681728363 and batch: 550, loss is 3.7756989192962647 and perplexity is 43.62799010646945
At time: 251.62851929664612 and batch: 600, loss is 3.8626536083221437 and perplexity is 47.591473085068664
At time: 252.19901132583618 and batch: 650, loss is 3.7496479463577272 and perplexity is 42.5061149330311
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5371560489430145 and perplexity of 93.42472659881435
Annealing...
Model not improving. Stopping early with 93.39311831936759loss at 31 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb2b5177748>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'batch_size': 80, 'tune_wordvecs': True, 'data': 'ptb', 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.4096518539347076, 'anneal': 4.729367290024887, 'wordvec_dim': 200, 'lr': 28.12829039403859, 'seq_len': 20}, 'best_accuracy': -144.26482396281557}, {'params': {'batch_size': 80, 'tune_wordvecs': True, 'data': 'ptb', 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.5157987567052392, 'anneal': 4.910993765445567, 'wordvec_dim': 200, 'lr': 22.683736482515734, 'seq_len': 20}, 'best_accuracy': -140.43956383824946}, {'params': {'batch_size': 80, 'tune_wordvecs': True, 'data': 'ptb', 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.11419939622077135, 'anneal': 2.356437569125191, 'wordvec_dim': 200, 'lr': 24.024764080393457, 'seq_len': 20}, 'best_accuracy': -132.47720359694543}, {'params': {'batch_size': 80, 'tune_wordvecs': True, 'data': 'ptb', 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.5147093544966376, 'anneal': 7.451100558510475, 'wordvec_dim': 200, 'lr': 2.280673112310202, 'seq_len': 20}, 'best_accuracy': -90.33481959178064}, {'params': {'batch_size': 80, 'tune_wordvecs': True, 'data': 'ptb', 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.11703656853499123, 'anneal': 4.4823179985909665, 'wordvec_dim': 200, 'lr': 6.182311085203539, 'seq_len': 20}, 'best_accuracy': -88.25024212822716}, {'params': {'batch_size': 80, 'tune_wordvecs': True, 'data': 'ptb', 'num_layers': 1, 'wordvec_source': '', 'dropout': 0.0, 'anneal': 2.0, 'wordvec_dim': 200, 'lr': 1.5290315552841394, 'seq_len': 20}, 'best_accuracy': -93.39311831936759}]
