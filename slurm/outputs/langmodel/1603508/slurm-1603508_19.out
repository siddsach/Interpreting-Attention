Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'type': 'continuous', 'name': 'lr', 'domain': [0, 30]}, {'type': 'continuous', 'name': 'dropout', 'domain': [0, 1]}, {'type': 'continuous', 'name': 'anneal', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'dropout': 0.4198681236269237, 'wordvec_dim': 200, 'tune_wordvecs': True, 'anneal': 2.4277448039092633, 'num_layers': 1, 'seq_len': 35, 'data': 'ptb', 'lr': 15.323145958939651, 'wordvec_source': '', 'batch_size': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.2760207653045654 and batch: 50, loss is 6.361729106903076 and perplexity is 579.2470710822545
At time: 1.970055341720581 and batch: 100, loss is 5.679952239990234 and perplexity is 292.9354389890227
At time: 2.655876636505127 and batch: 150, loss is 5.5734189510345455 and perplexity is 263.33288415066795
At time: 3.3273630142211914 and batch: 200, loss is 5.466721181869507 and perplexity is 236.68287900747083
At time: 3.996906042098999 and batch: 250, loss is 5.40113468170166 and perplexity is 221.65778459787273
At time: 4.6670920848846436 and batch: 300, loss is 5.394329061508179 and perplexity is 220.15438748052708
At time: 5.338871955871582 and batch: 350, loss is 5.4172407913208005 and perplexity is 225.25673391164165
At time: 6.01066780090332 and batch: 400, loss is 5.415903234481812 and perplexity is 224.9556416355757
At time: 6.6812684535980225 and batch: 450, loss is 5.325487699508667 and perplexity is 205.50856245350212
At time: 7.355191707611084 and batch: 500, loss is 5.339550466537475 and perplexity is 208.4189979134621
At time: 8.03116774559021 and batch: 550, loss is 5.371444253921509 and perplexity is 215.17340843946238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.209682383435838 and perplexity of 183.03591372234857
Finished 1 epochs...
Completing Train Step...
At time: 9.78331184387207 and batch: 50, loss is 5.122739696502686 and perplexity is 167.79444632630248
At time: 10.458098649978638 and batch: 100, loss is 5.085293111801147 and perplexity is 161.62730716632944
At time: 11.119595527648926 and batch: 150, loss is 5.0418952941894535 and perplexity is 154.76305876213831
At time: 11.795745372772217 and batch: 200, loss is 4.965030279159546 and perplexity is 143.3128896014972
At time: 12.45660924911499 and batch: 250, loss is 4.944275102615356 and perplexity is 140.3690607971747
At time: 13.11967420578003 and batch: 300, loss is 4.928045206069946 and perplexity is 138.1092731262356
At time: 13.786320924758911 and batch: 350, loss is 4.955549917221069 and perplexity is 141.9606515187912
At time: 14.45316481590271 and batch: 400, loss is 4.957579946517944 and perplexity is 142.249128509681
At time: 15.120872497558594 and batch: 450, loss is 4.890437049865723 and perplexity is 133.01169409272225
At time: 15.784799814224243 and batch: 500, loss is 4.930550985336303 and perplexity is 138.45577843092425
At time: 16.447146892547607 and batch: 550, loss is 4.965987596511841 and perplexity is 143.45015120849652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.082999046812666 and perplexity of 161.25694859551677
Finished 2 epochs...
Completing Train Step...
At time: 18.198586463928223 and batch: 50, loss is 4.927648553848266 and perplexity is 138.05450263935126
At time: 18.87785506248474 and batch: 100, loss is 4.911403036117553 and perplexity is 135.82985497209822
At time: 19.544804334640503 and batch: 150, loss is 4.891252136230468 and perplexity is 133.12015430714516
At time: 20.215985536575317 and batch: 200, loss is 4.832320337295532 and perplexity is 125.50182964316043
At time: 20.88844919204712 and batch: 250, loss is 4.832584829330444 and perplexity is 125.53502826766027
At time: 21.560848236083984 and batch: 300, loss is 4.8246046161651615 and perplexity is 124.53721864447385
At time: 22.234824895858765 and batch: 350, loss is 4.8620362472534175 and perplexity is 129.28719495290278
At time: 22.908629655838013 and batch: 400, loss is 4.838102264404297 and perplexity is 126.2295739289665
At time: 23.581450939178467 and batch: 450, loss is 4.7913546371459965 and perplexity is 120.46444379479883
At time: 24.276243686676025 and batch: 500, loss is 4.842532329559326 and perplexity is 126.79001965492003
At time: 24.941579580307007 and batch: 550, loss is 4.872398853302002 and perplexity is 130.63391292395832
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.085804716069648 and perplexity of 161.71001754226862
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 26.710052251815796 and batch: 50, loss is 4.7919015789031985 and perplexity is 120.53034885083895
At time: 27.37555694580078 and batch: 100, loss is 4.732735757827759 and perplexity is 113.6059359301038
At time: 28.045102834701538 and batch: 150, loss is 4.711830453872681 and perplexity is 111.25562191897387
At time: 28.7287495136261 and batch: 200, loss is 4.62936770439148 and perplexity is 102.44926540607182
At time: 29.431180000305176 and batch: 250, loss is 4.609804763793945 and perplexity is 100.46453340720707
At time: 30.158454179763794 and batch: 300, loss is 4.583070964813232 and perplexity is 97.81431777209445
At time: 30.89548897743225 and batch: 350, loss is 4.61898365020752 and perplexity is 101.39093109328618
At time: 31.63339400291443 and batch: 400, loss is 4.592345199584961 and perplexity is 98.7256903283127
At time: 32.37552309036255 and batch: 450, loss is 4.521498231887818 and perplexity is 91.97329212260662
At time: 33.11928701400757 and batch: 500, loss is 4.549588289260864 and perplexity is 94.59345515541781
At time: 33.86162877082825 and batch: 550, loss is 4.569888486862182 and perplexity is 96.53334443647545
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.900977601396277 and perplexity of 134.4211257526501
Finished 4 epochs...
Completing Train Step...
At time: 35.804441928863525 and batch: 50, loss is 4.621822109222412 and perplexity is 101.67913392814307
At time: 36.55116319656372 and batch: 100, loss is 4.599865961074829 and perplexity is 99.47098176484302
At time: 37.28764724731445 and batch: 150, loss is 4.588759260177612 and perplexity is 98.37229998171772
At time: 38.021721601486206 and batch: 200, loss is 4.52215989112854 and perplexity is 92.03416723830973
At time: 38.7532913684845 and batch: 250, loss is 4.504698781967163 and perplexity is 90.4410974560293
At time: 39.49251866340637 and batch: 300, loss is 4.480593357086182 and perplexity is 88.28704287950978
At time: 40.22440028190613 and batch: 350, loss is 4.519121694564819 and perplexity is 91.75497368486991
At time: 40.960957050323486 and batch: 400, loss is 4.499489727020264 and perplexity is 89.97120970797616
At time: 41.71236538887024 and batch: 450, loss is 4.4427068901062015 and perplexity is 85.00472898889439
At time: 42.446890354156494 and batch: 500, loss is 4.477382497787476 and perplexity is 88.00402022304354
At time: 43.17799210548401 and batch: 550, loss is 4.5094666671752925 and perplexity is 90.87333984901565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.886087620511968 and perplexity of 132.43442543071592
Finished 5 epochs...
Completing Train Step...
At time: 45.06733584403992 and batch: 50, loss is 4.537303495407104 and perplexity is 93.43850276000806
At time: 45.79656267166138 and batch: 100, loss is 4.524208631515503 and perplexity is 92.2229146348988
At time: 46.526511430740356 and batch: 150, loss is 4.509648027420044 and perplexity is 90.88982215474441
At time: 47.25590634346008 and batch: 200, loss is 4.450804271697998 and perplexity is 85.69583903129488
At time: 47.989104986190796 and batch: 250, loss is 4.435065145492554 and perplexity is 84.35762022801082
At time: 48.726073265075684 and batch: 300, loss is 4.413432960510254 and perplexity is 82.55237666060887
At time: 49.466792345047 and batch: 350, loss is 4.454344673156738 and perplexity is 85.99977441397368
At time: 50.20853924751282 and batch: 400, loss is 4.4358330249786375 and perplexity is 84.42242159071317
At time: 50.947410106658936 and batch: 450, loss is 4.387794313430786 and perplexity is 80.46274750721886
At time: 51.68384552001953 and batch: 500, loss is 4.423836002349853 and perplexity is 83.4156550640886
At time: 52.421701431274414 and batch: 550, loss is 4.455103483200073 and perplexity is 86.06505667180757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.891996343085107 and perplexity of 133.2192601115064
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 54.32674789428711 and batch: 50, loss is 4.452247953414917 and perplexity is 85.81964589466953
At time: 55.07833194732666 and batch: 100, loss is 4.4078467655181885 and perplexity is 82.0925086396121
At time: 55.81064772605896 and batch: 150, loss is 4.38053861618042 and perplexity is 79.88104704457064
At time: 56.555766105651855 and batch: 200, loss is 4.320297937393189 and perplexity is 75.21103313338814
At time: 57.297693729400635 and batch: 250, loss is 4.293006038665771 and perplexity is 73.18613852813074
At time: 58.030385971069336 and batch: 300, loss is 4.264218330383301 and perplexity is 71.10931425315924
At time: 58.76446175575256 and batch: 350, loss is 4.294614944458008 and perplexity is 73.3039829051497
At time: 59.494247913360596 and batch: 400, loss is 4.272347431182862 and perplexity is 71.68972494880086
At time: 60.222243309020996 and batch: 450, loss is 4.22013825416565 and perplexity is 68.04289085225962
At time: 60.96484327316284 and batch: 500, loss is 4.242240400314331 and perplexity is 69.56352753465431
At time: 61.692405223846436 and batch: 550, loss is 4.279270830154419 and perplexity is 72.18778365688507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.816772785592587 and perplexity of 123.56567370177518
Finished 7 epochs...
Completing Train Step...
At time: 63.56166362762451 and batch: 50, loss is 4.374829816818237 and perplexity is 79.42632137792319
At time: 64.28907632827759 and batch: 100, loss is 4.3497823715209964 and perplexity is 77.46160323985929
At time: 65.01098847389221 and batch: 150, loss is 4.32993182182312 and perplexity is 75.93910900268511
At time: 65.73058485984802 and batch: 200, loss is 4.277006950378418 and perplexity is 72.02454404054188
At time: 66.46575236320496 and batch: 250, loss is 4.254259548187256 and perplexity is 70.40466662029925
At time: 67.20361256599426 and batch: 300, loss is 4.230185794830322 and perplexity is 68.73000066646163
At time: 67.935373544693 and batch: 350, loss is 4.263611888885498 and perplexity is 71.06620368745506
At time: 68.66007971763611 and batch: 400, loss is 4.247201833724976 and perplexity is 69.90951994373586
At time: 69.38516283035278 and batch: 450, loss is 4.193293170928955 and perplexity is 66.24057369731757
At time: 70.12322568893433 and batch: 500, loss is 4.222090578079223 and perplexity is 68.17586237472489
At time: 70.85246801376343 and batch: 550, loss is 4.262462882995606 and perplexity is 70.98459509420148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.8113416306515955 and perplexity of 122.89638852344716
Finished 8 epochs...
Completing Train Step...
At time: 72.73802065849304 and batch: 50, loss is 4.337853937149048 and perplexity is 76.54309665301565
At time: 73.46672773361206 and batch: 100, loss is 4.315566215515137 and perplexity is 74.8559960731685
At time: 74.1898090839386 and batch: 150, loss is 4.295587310791015 and perplexity is 73.37529589575875
At time: 74.91698408126831 and batch: 200, loss is 4.2466417503356935 and perplexity is 69.87037574589345
At time: 75.63922762870789 and batch: 250, loss is 4.225453691482544 and perplexity is 68.40553151647144
At time: 76.36321258544922 and batch: 300, loss is 4.2046170949935915 and perplexity is 66.9949400564529
At time: 77.10272169113159 and batch: 350, loss is 4.240012564659119 and perplexity is 69.40872393023245
At time: 77.83571791648865 and batch: 400, loss is 4.225485472679138 and perplexity is 68.40770556066339
At time: 78.56055617332458 and batch: 450, loss is 4.168831796646118 and perplexity is 64.63989544057165
At time: 79.30268406867981 and batch: 500, loss is 4.202431359291077 and perplexity is 66.84866673972336
At time: 80.02973747253418 and batch: 550, loss is 4.2434438037872315 and perplexity is 69.64729091573989
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.812493506898272 and perplexity of 123.03803151612779
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 81.91829776763916 and batch: 50, loss is 4.2984796190261845 and perplexity is 73.5878270730133
At time: 82.66195774078369 and batch: 100, loss is 4.2672648429870605 and perplexity is 71.32628000191106
At time: 83.3939995765686 and batch: 150, loss is 4.235932321548462 and perplexity is 69.12609644912044
At time: 84.11943006515503 and batch: 200, loss is 4.187447581291199 and perplexity is 65.8544880346957
At time: 84.84188747406006 and batch: 250, loss is 4.158381400108337 and perplexity is 63.9679003245226
At time: 85.56520390510559 and batch: 300, loss is 4.133988795280456 and perplexity is 62.42643323918299
At time: 86.28791236877441 and batch: 350, loss is 4.163331685066223 and perplexity is 64.28534473116605
At time: 87.01066589355469 and batch: 400, loss is 4.145948567390442 and perplexity is 63.177521624613796
At time: 87.7371072769165 and batch: 450, loss is 4.0839817237854 and perplexity is 59.38144024669444
At time: 88.47081875801086 and batch: 500, loss is 4.11586519241333 and perplexity is 61.30523214585964
At time: 89.19696545600891 and batch: 550, loss is 4.155892858505249 and perplexity is 63.80891145056584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.780881191821808 and perplexity of 119.20935007980681
Finished 10 epochs...
Completing Train Step...
At time: 91.07984948158264 and batch: 50, loss is 4.2623503875732425 and perplexity is 70.97661010134107
At time: 91.8075909614563 and batch: 100, loss is 4.238152675628662 and perplexity is 69.27975138047022
At time: 92.52677845954895 and batch: 150, loss is 4.211825914382935 and perplexity is 67.479639435428
At time: 93.24952960014343 and batch: 200, loss is 4.165742926597595 and perplexity is 64.44053925478796
At time: 93.97361707687378 and batch: 250, loss is 4.141391973495484 and perplexity is 62.8903021834071
At time: 94.69930338859558 and batch: 300, loss is 4.117427048683166 and perplexity is 61.401056919842894
At time: 95.42665195465088 and batch: 350, loss is 4.149894890785217 and perplexity is 63.4273331505569
At time: 96.15602684020996 and batch: 400, loss is 4.13809136390686 and perplexity is 62.683068036629535
At time: 96.88380336761475 and batch: 450, loss is 4.080095572471619 and perplexity is 59.151122798749704
At time: 97.63316535949707 and batch: 500, loss is 4.1111229085922245 and perplexity is 61.015193601415994
At time: 98.3629105091095 and batch: 550, loss is 4.151274147033692 and perplexity is 63.514876054299314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.779275123109209 and perplexity of 119.0180453377966
Finished 11 epochs...
Completing Train Step...
At time: 100.24487900733948 and batch: 50, loss is 4.245013990402222 and perplexity is 69.75673606187159
At time: 100.97234964370728 and batch: 100, loss is 4.221357097625733 and perplexity is 68.12587504687818
At time: 101.68677639961243 and batch: 150, loss is 4.195878200531006 and perplexity is 66.4120290542911
At time: 102.4053864479065 and batch: 200, loss is 4.15147665977478 and perplexity is 63.52773992845421
At time: 103.1281807422638 and batch: 250, loss is 4.128903884887695 and perplexity is 62.1098061125394
At time: 103.85078883171082 and batch: 300, loss is 4.105062961578369 and perplexity is 60.64656283072066
At time: 104.5819137096405 and batch: 350, loss is 4.139924588203431 and perplexity is 62.79808555417998
At time: 105.30348944664001 and batch: 400, loss is 4.130639233589172 and perplexity is 62.217681857871376
At time: 106.01970148086548 and batch: 450, loss is 4.075065336227417 and perplexity is 58.85432578234458
At time: 106.73575711250305 and batch: 500, loss is 4.10475260257721 and perplexity is 60.62774354457527
At time: 107.46992087364197 and batch: 550, loss is 4.144412288665771 and perplexity is 63.08053785838754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.779552378553025 and perplexity of 119.05104831369478
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 109.32272124290466 and batch: 50, loss is 4.228855118751526 and perplexity is 68.63860412176332
At time: 110.0547821521759 and batch: 100, loss is 4.200746717453003 and perplexity is 66.73614548453273
At time: 110.7725727558136 and batch: 150, loss is 4.170726943016052 and perplexity is 64.76251365680304
At time: 111.49220776557922 and batch: 200, loss is 4.125608401298523 and perplexity is 61.90546115842605
At time: 112.20988488197327 and batch: 250, loss is 4.098760437965393 and perplexity is 60.26553840540036
At time: 112.92650246620178 and batch: 300, loss is 4.070934710502624 and perplexity is 58.61172198793321
At time: 113.65263295173645 and batch: 350, loss is 4.10413800239563 and perplexity is 60.59049317059986
At time: 114.37838816642761 and batch: 400, loss is 4.0944207620620725 and perplexity is 60.0045721645952
At time: 115.09868931770325 and batch: 450, loss is 4.034946827888489 and perplexity is 56.539913811792644
At time: 115.8333854675293 and batch: 500, loss is 4.060667719841003 and perplexity is 58.013034603529306
At time: 116.56280994415283 and batch: 550, loss is 4.101701068878174 and perplexity is 60.4430179336108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.765885373379322 and perplexity of 117.43504513630768
Finished 13 epochs...
Completing Train Step...
At time: 118.42924857139587 and batch: 50, loss is 4.212190999984741 and perplexity is 67.50427977784436
At time: 119.16265678405762 and batch: 100, loss is 4.187252998352051 and perplexity is 65.8416751214854
At time: 119.87989974021912 and batch: 150, loss is 4.1598166704177855 and perplexity is 64.0597774711142
At time: 120.59525084495544 and batch: 200, loss is 4.116070551872253 and perplexity is 61.31782304794845
At time: 121.32272148132324 and batch: 250, loss is 4.090980229377746 and perplexity is 59.79847921092349
At time: 122.04516649246216 and batch: 300, loss is 4.064172649383545 and perplexity is 58.21672295049816
At time: 122.76041793823242 and batch: 350, loss is 4.099664063453674 and perplexity is 60.320020493963156
At time: 123.47484254837036 and batch: 400, loss is 4.090303087234497 and perplexity is 59.75800084689736
At time: 124.19067692756653 and batch: 450, loss is 4.031657743453979 and perplexity is 56.354254752878944
At time: 124.9113450050354 and batch: 500, loss is 4.058698630332946 and perplexity is 57.8989141393387
At time: 125.630535364151 and batch: 550, loss is 4.100299978256226 and perplexity is 60.35839108680644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.764917576566655 and perplexity of 117.32144685282437
Finished 14 epochs...
Completing Train Step...
At time: 127.49883818626404 and batch: 50, loss is 4.203788003921509 and perplexity is 66.93941816930828
At time: 128.22614550590515 and batch: 100, loss is 4.179428386688232 and perplexity is 65.32849989902085
At time: 128.9496488571167 and batch: 150, loss is 4.152803401947022 and perplexity is 63.612080797046374
At time: 129.66539907455444 and batch: 200, loss is 4.109860925674439 and perplexity is 60.93824203535931
At time: 130.37964510917664 and batch: 250, loss is 4.085225863456726 and perplexity is 59.455365029087474
At time: 131.0951864719391 and batch: 300, loss is 4.059325656890869 and perplexity is 57.935229680393476
At time: 131.81210851669312 and batch: 350, loss is 4.096243195533752 and perplexity is 60.114026211427245
At time: 132.5289409160614 and batch: 400, loss is 4.0869670104980464 and perplexity is 59.55897573656142
At time: 133.24972414970398 and batch: 450, loss is 4.0283741283416745 and perplexity is 56.169512547471314
At time: 133.97975540161133 and batch: 500, loss is 4.056219506263733 and perplexity is 57.75555332576158
At time: 134.7004520893097 and batch: 550, loss is 4.098015236854553 and perplexity is 60.220645188550336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.764769858502327 and perplexity of 117.30411763574182
Finished 15 epochs...
Completing Train Step...
At time: 136.53382873535156 and batch: 50, loss is 4.1971595287323 and perplexity is 66.49717920101574
At time: 137.25463128089905 and batch: 100, loss is 4.172957935333252 and perplexity is 64.90715961923352
At time: 137.96195006370544 and batch: 150, loss is 4.147299041748047 and perplexity is 63.26289888446918
At time: 138.66941690444946 and batch: 200, loss is 4.104751715660095 and perplexity is 60.62768977281572
At time: 139.378262758255 and batch: 250, loss is 4.080418887138367 and perplexity is 59.17025031624203
At time: 140.09200382232666 and batch: 300, loss is 4.0550682067871096 and perplexity is 57.68909764997846
At time: 140.81533932685852 and batch: 350, loss is 4.0926901054382325 and perplexity is 59.90081466451788
At time: 141.54273557662964 and batch: 400, loss is 4.083467607498169 and perplexity is 59.350919127478875
At time: 142.26904249191284 and batch: 450, loss is 4.024917020797729 and perplexity is 55.97566377300078
At time: 142.99431037902832 and batch: 500, loss is 4.053373427391052 and perplexity is 57.59141015866002
At time: 143.7218623161316 and batch: 550, loss is 4.09477445602417 and perplexity is 60.02579917317914
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.7642341776097075 and perplexity of 117.24129688873786
Finished 16 epochs...
Completing Train Step...
At time: 145.59366464614868 and batch: 50, loss is 4.19075984954834 and perplexity is 66.07297741296331
At time: 146.33129286766052 and batch: 100, loss is 4.166936798095703 and perplexity is 64.51751892069805
At time: 147.05674171447754 and batch: 150, loss is 4.141804389953613 and perplexity is 62.91624452824154
At time: 147.78479480743408 and batch: 200, loss is 4.09965946674347 and perplexity is 60.31974322094669
At time: 148.50427556037903 and batch: 250, loss is 4.074857635498047 and perplexity is 58.84210296534094
At time: 149.22336769104004 and batch: 300, loss is 4.050301833152771 and perplexity is 57.41478411567783
At time: 149.94492077827454 and batch: 350, loss is 4.088686380386353 and perplexity is 59.66146773159708
At time: 150.6695213317871 and batch: 400, loss is 4.079428334236145 and perplexity is 59.111668072262
At time: 151.3924481868744 and batch: 450, loss is 4.021001815795898 and perplexity is 55.75693603558501
At time: 152.1302855014801 and batch: 500, loss is 4.049993200302124 and perplexity is 57.39706676140634
At time: 152.8548882007599 and batch: 550, loss is 4.091836047172547 and perplexity is 59.849677718706296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.764190024517952 and perplexity of 117.23612043727783
Finished 17 epochs...
Completing Train Step...
At time: 154.72967839241028 and batch: 50, loss is 4.185030980110168 and perplexity is 65.69553614015899
At time: 155.4513201713562 and batch: 100, loss is 4.1616592741012575 and perplexity is 64.1779230673731
At time: 156.1605453491211 and batch: 150, loss is 4.1371561050415036 and perplexity is 62.62447054772232
At time: 156.87106561660767 and batch: 200, loss is 4.095121245384217 and perplexity is 60.04661909151514
At time: 157.58204817771912 and batch: 250, loss is 4.0705302667617795 and perplexity is 58.58802163688745
At time: 158.29727602005005 and batch: 300, loss is 4.046627941131592 and perplexity is 57.2042354018605
At time: 159.0099515914917 and batch: 350, loss is 4.085186004638672 and perplexity is 59.452995255738934
At time: 159.7266228199005 and batch: 400, loss is 4.076115174293518 and perplexity is 58.916145738699356
At time: 160.44307279586792 and batch: 450, loss is 4.017677040100097 and perplexity is 55.57186456083588
At time: 161.15345191955566 and batch: 500, loss is 4.047034454345703 and perplexity is 57.227494406680584
At time: 161.8650233745575 and batch: 550, loss is 4.08866801738739 and perplexity is 59.66037217818589
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.764268266393783 and perplexity of 117.24529357011285
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 163.7100546360016 and batch: 50, loss is 4.179079208374024 and perplexity is 65.30569258569767
At time: 164.43739557266235 and batch: 100, loss is 4.15358971118927 and perplexity is 63.66211923436045
At time: 165.1428999900818 and batch: 150, loss is 4.12805585861206 and perplexity is 62.05715769175867
At time: 165.8502938747406 and batch: 200, loss is 4.085447773933411 and perplexity is 59.46856026150857
At time: 166.55979704856873 and batch: 250, loss is 4.0579962730407715 and perplexity is 57.85826269237516
At time: 167.26947832107544 and batch: 300, loss is 4.0326113033294675 and perplexity is 56.408017537967886
At time: 167.98010325431824 and batch: 350, loss is 4.069915356636048 and perplexity is 58.552006343355494
At time: 168.69291186332703 and batch: 400, loss is 4.058788461685181 and perplexity is 57.90411551070834
At time: 169.40729331970215 and batch: 450, loss is 3.9979608058929443 and perplexity is 54.486927248305804
At time: 170.13620018959045 and batch: 500, loss is 4.025963244438171 and perplexity is 56.03425748144635
At time: 170.85077476501465 and batch: 550, loss is 4.068434672355652 and perplexity is 58.46537346177823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.760709072681183 and perplexity of 116.82873660161813
Finished 19 epochs...
Completing Train Step...
At time: 172.6934368610382 and batch: 50, loss is 4.172243123054504 and perplexity is 64.86077976297928
At time: 173.4284348487854 and batch: 100, loss is 4.1477139377593994 and perplexity is 63.28915185463264
At time: 174.1371772289276 and batch: 150, loss is 4.123960118293763 and perplexity is 61.803507486231965
At time: 174.8528332710266 and batch: 200, loss is 4.081290621757507 and perplexity is 59.22185356076536
At time: 175.56774353981018 and batch: 250, loss is 4.054921226501465 and perplexity is 57.68061911303143
At time: 176.28004145622253 and batch: 300, loss is 4.0295601034164426 and perplexity is 56.23616770718075
At time: 176.9914948940277 and batch: 350, loss is 4.068052282333374 and perplexity is 58.443021160238615
At time: 177.71369743347168 and batch: 400, loss is 4.057172250747681 and perplexity is 57.8106058319315
At time: 178.4372637271881 and batch: 450, loss is 3.996962718963623 and perplexity is 54.432571688699504
At time: 179.15812802314758 and batch: 500, loss is 4.0260074424743655 and perplexity is 56.036734140317975
At time: 179.8818175792694 and batch: 550, loss is 4.06862699508667 and perplexity is 58.476618763401284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.7600935266373 and perplexity of 116.75684526347553
Finished 20 epochs...
Completing Train Step...
At time: 181.74196195602417 and batch: 50, loss is 4.168335204124451 and perplexity is 64.60780372080802
At time: 182.46490931510925 and batch: 100, loss is 4.144001812934875 and perplexity is 63.05465014200794
At time: 183.17349767684937 and batch: 150, loss is 4.12139407157898 and perplexity is 61.64512010050998
At time: 183.88156366348267 and batch: 200, loss is 4.078545708656311 and perplexity is 59.0595176200019
At time: 184.5887541770935 and batch: 250, loss is 4.052910280227661 and perplexity is 57.56474303628709
At time: 185.30404472351074 and batch: 300, loss is 4.027420258522033 and perplexity is 56.11595968995609
At time: 186.0295238494873 and batch: 350, loss is 4.066668405532837 and perplexity is 58.36219915584867
At time: 186.75708627700806 and batch: 400, loss is 4.055687127113342 and perplexity is 57.72481365665716
At time: 187.47694754600525 and batch: 450, loss is 3.9960096502304077 and perplexity is 54.38071842034392
At time: 188.2044792175293 and batch: 500, loss is 4.025791826248169 and perplexity is 56.024653013664555
At time: 188.9188973903656 and batch: 550, loss is 4.068334789276123 and perplexity is 58.45953405186502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.759927952543218 and perplexity of 116.73751495493642
Finished 21 epochs...
Completing Train Step...
At time: 190.77291893959045 and batch: 50, loss is 4.1651467514038085 and perplexity is 64.40213285341947
At time: 191.4929440021515 and batch: 100, loss is 4.141051659584045 and perplexity is 62.86890338004127
At time: 192.20157289505005 and batch: 150, loss is 4.119394478797912 and perplexity is 61.52197812130591
At time: 192.9110074043274 and batch: 200, loss is 4.076343002319336 and perplexity is 58.929570017027174
At time: 193.61898636817932 and batch: 250, loss is 4.051205277442932 and perplexity is 57.46667861292185
At time: 194.32687830924988 and batch: 300, loss is 4.025552620887757 and perplexity is 56.01125321906366
At time: 195.03875827789307 and batch: 350, loss is 4.065363626480103 and perplexity is 58.286099038634454
At time: 195.74364495277405 and batch: 400, loss is 4.054309296607971 and perplexity is 57.645333415208675
At time: 196.45250153541565 and batch: 450, loss is 3.994863739013672 and perplexity is 54.31843863549895
At time: 197.1634817123413 and batch: 500, loss is 4.024943561553955 and perplexity is 55.97714942916268
At time: 197.88671207427979 and batch: 550, loss is 4.06766366481781 and perplexity is 58.42031359111859
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.759741275868517 and perplexity of 116.71572481775029
Finished 22 epochs...
Completing Train Step...
At time: 199.7523910999298 and batch: 50, loss is 4.162290105819702 and perplexity is 64.21842130931265
At time: 200.47023105621338 and batch: 100, loss is 4.13834050655365 and perplexity is 62.69868700770419
At time: 201.18250584602356 and batch: 150, loss is 4.117499704360962 and perplexity is 61.40551821731812
At time: 201.8872148990631 and batch: 200, loss is 4.074436664581299 and perplexity is 58.817337364476835
At time: 202.59144616127014 and batch: 250, loss is 4.049647297859192 and perplexity is 57.37721640913703
At time: 203.30630946159363 and batch: 300, loss is 4.023873686790466 and perplexity is 55.917292914852915
At time: 204.0185420513153 and batch: 350, loss is 4.064162793159485 and perplexity is 58.21614915626045
At time: 204.72902631759644 and batch: 400, loss is 4.052946081161499 and perplexity is 57.566803944734936
At time: 205.43767404556274 and batch: 450, loss is 3.9936611318588255 and perplexity is 54.25315415621497
At time: 206.1623067855835 and batch: 500, loss is 4.0240118932724 and perplexity is 55.92502158124952
At time: 206.88717985153198 and batch: 550, loss is 4.0668674659729005 and perplexity is 58.37381791727525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.759678617436835 and perplexity of 116.70841182259333
Finished 23 epochs...
Completing Train Step...
At time: 208.7268054485321 and batch: 50, loss is 4.159709558486939 and perplexity is 64.05291627212436
At time: 209.44673991203308 and batch: 100, loss is 4.135952463150025 and perplexity is 62.549138457064245
At time: 210.15280079841614 and batch: 150, loss is 4.115773677825928 and perplexity is 61.299622079539574
At time: 210.86171746253967 and batch: 200, loss is 4.07265344619751 and perplexity is 58.71254666732017
At time: 211.57084369659424 and batch: 250, loss is 4.048110513687134 and perplexity is 57.28910773047469
At time: 212.28234887123108 and batch: 300, loss is 4.0222778367996215 and perplexity is 55.82812846892998
At time: 213.00267910957336 and batch: 350, loss is 4.062928876876831 and perplexity is 58.14435960216157
At time: 213.72022318840027 and batch: 400, loss is 4.0515645980834964 and perplexity is 57.48733128693597
At time: 214.42876052856445 and batch: 450, loss is 3.9924454355239867 and perplexity is 54.187238870157756
At time: 215.13709950447083 and batch: 500, loss is 4.023061671257019 and perplexity is 55.87190563449998
At time: 215.8446774482727 and batch: 550, loss is 4.066008687019348 and perplexity is 58.323709230231906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.759618231590758 and perplexity of 116.70136449918253
Finished 24 epochs...
Completing Train Step...
At time: 217.67680621147156 and batch: 50, loss is 4.157292580604553 and perplexity is 63.89828873114063
At time: 218.3916370868683 and batch: 100, loss is 4.133635864257813 and perplexity is 62.4044049017304
At time: 219.0916576385498 and batch: 150, loss is 4.114011697769165 and perplexity is 61.191708466683984
At time: 219.79104042053223 and batch: 200, loss is 4.070908102989197 and perplexity is 58.61016249650061
At time: 220.49721503257751 and batch: 250, loss is 4.046502380371094 and perplexity is 57.19705324546767
At time: 221.20265460014343 and batch: 300, loss is 4.020781149864197 and perplexity is 55.74463373674494
At time: 221.9027419090271 and batch: 350, loss is 4.061741809844971 and perplexity is 58.075379300010674
At time: 222.60249018669128 and batch: 400, loss is 4.050303492546082 and perplexity is 57.41487938946558
At time: 223.3071219921112 and batch: 450, loss is 3.9913393783569338 and perplexity is 54.12733781934262
At time: 224.02838826179504 and batch: 500, loss is 4.022108020782471 and perplexity is 55.818648763434126
At time: 224.73895907402039 and batch: 550, loss is 4.065240936279297 and perplexity is 58.278948344108244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.759634139689993 and perplexity of 116.70322101083663
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 226.56235003471375 and batch: 50, loss is 4.154884476661682 and perplexity is 63.74460013343306
At time: 227.27532505989075 and batch: 100, loss is 4.130259413719177 and perplexity is 62.194054833327435
At time: 227.97404599189758 and batch: 150, loss is 4.110587921142578 and perplexity is 60.98255996867181
At time: 228.67442274093628 and batch: 200, loss is 4.066658992767334 and perplexity is 58.361649808739216
At time: 229.38046789169312 and batch: 250, loss is 4.0416101503372195 and perplexity is 56.917915463589225
At time: 230.08771014213562 and batch: 300, loss is 4.014721875190735 and perplexity is 55.407882952422916
At time: 230.79738664627075 and batch: 350, loss is 4.054899349212646 and perplexity is 57.67935723127118
At time: 231.5206537246704 and batch: 400, loss is 4.042345161437988 and perplexity is 56.95976614175998
At time: 232.24230813980103 and batch: 450, loss is 3.9826262950897218 and perplexity is 53.657770478268326
At time: 232.95264983177185 and batch: 500, loss is 4.012851676940918 and perplexity is 55.30435606477627
At time: 233.66290879249573 and batch: 550, loss is 4.055966100692749 and perplexity is 57.74091960100558
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.758153712495845 and perplexity of 116.5305782128852
Finished 26 epochs...
Completing Train Step...
At time: 235.51345419883728 and batch: 50, loss is 4.1524223279953 and perplexity is 63.58784450824225
At time: 236.24754238128662 and batch: 100, loss is 4.128389291763305 and perplexity is 62.077853055473575
At time: 236.96476101875305 and batch: 150, loss is 4.108856768608093 and perplexity is 60.87708118169971
At time: 237.67717146873474 and batch: 200, loss is 4.065163726806641 and perplexity is 58.274448830944394
At time: 238.39008617401123 and batch: 250, loss is 4.040442452430725 and perplexity is 56.85149132207342
At time: 239.10099363327026 and batch: 300, loss is 4.013993182182312 and perplexity is 55.367522322544325
At time: 239.8083531856537 and batch: 350, loss is 4.054244198799133 and perplexity is 57.6415809524535
At time: 240.51598739624023 and batch: 400, loss is 4.041671528816223 and perplexity is 56.92140910588455
At time: 241.2293667793274 and batch: 450, loss is 3.982359881401062 and perplexity is 53.64347721775463
At time: 241.95343232154846 and batch: 500, loss is 4.012848196029663 and perplexity is 55.30416355555584
At time: 242.66482424736023 and batch: 550, loss is 4.056124453544617 and perplexity is 57.750063764276916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.757971256337267 and perplexity of 116.50931843077576
Finished 27 epochs...
Completing Train Step...
At time: 244.50349521636963 and batch: 50, loss is 4.150952343940735 and perplexity is 63.494440059095844
At time: 245.21743535995483 and batch: 100, loss is 4.1270313692092895 and perplexity is 61.99361334714911
At time: 245.92212557792664 and batch: 150, loss is 4.107664165496826 and perplexity is 60.80452226087497
At time: 246.63411402702332 and batch: 200, loss is 4.064146375656128 and perplexity is 58.21519340028179
At time: 247.34374809265137 and batch: 250, loss is 4.039621300697327 and perplexity is 56.804826783383554
At time: 248.05802249908447 and batch: 300, loss is 4.0134255027771 and perplexity is 55.336100240082565
At time: 248.77230787277222 and batch: 350, loss is 4.053691515922546 and perplexity is 57.60973223961149
At time: 249.48654079437256 and batch: 400, loss is 4.041047925949097 and perplexity is 56.88592381747902
At time: 250.20050358772278 and batch: 450, loss is 3.982026286125183 and perplexity is 53.625584991720025
At time: 250.91479229927063 and batch: 500, loss is 4.012665877342224 and perplexity is 55.2940814921487
At time: 251.6285469532013 and batch: 550, loss is 4.056028060913086 and perplexity is 57.744497351944474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.75787905429272 and perplexity of 116.49857652862786
Finished 28 epochs...
Completing Train Step...
At time: 253.4709665775299 and batch: 50, loss is 4.149705872535706 and perplexity is 63.415345360064904
At time: 254.1907444000244 and batch: 100, loss is 4.1258761596679685 and perplexity is 61.922039083105595
At time: 254.89858198165894 and batch: 150, loss is 4.106635785102844 and perplexity is 60.74202422370248
At time: 255.6099123954773 and batch: 200, loss is 4.0632726764678955 and perplexity is 58.16435304588745
At time: 256.31313729286194 and batch: 250, loss is 4.038874225616455 and perplexity is 56.762405160862016
At time: 257.014643907547 and batch: 300, loss is 4.012902755737304 and perplexity is 55.30718101686595
At time: 257.71865367889404 and batch: 350, loss is 4.0531613063812255 and perplexity is 57.57919510616027
At time: 258.4242515563965 and batch: 400, loss is 4.040458126068115 and perplexity is 56.85238239871666
At time: 259.12994599342346 and batch: 450, loss is 3.98169228553772 and perplexity is 53.60767700563515
At time: 259.858496427536 and batch: 500, loss is 4.012358469963074 and perplexity is 55.277086295832895
At time: 260.56647205352783 and batch: 550, loss is 4.055804495811462 and perplexity is 57.73158914049237
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.7578254862034575 and perplexity of 116.49233608962714
Finished 29 epochs...
Completing Train Step...
At time: 262.431036233902 and batch: 50, loss is 4.148594369888306 and perplexity is 63.344898194169865
At time: 263.15396904945374 and batch: 100, loss is 4.124838752746582 and perplexity is 61.8578340403105
At time: 263.8734345436096 and batch: 150, loss is 4.105713510513306 and perplexity is 60.6860292235958
At time: 264.5866310596466 and batch: 200, loss is 4.062467875480652 and perplexity is 58.11756114874188
At time: 265.2969093322754 and batch: 250, loss is 4.03818033695221 and perplexity is 56.72303203322885
At time: 265.99982953071594 and batch: 300, loss is 4.012406101226807 and perplexity is 55.27971927601424
At time: 266.7054386138916 and batch: 350, loss is 4.0526614046096805 and perplexity is 57.55041835789556
At time: 267.4117603302002 and batch: 400, loss is 4.039892582893372 and perplexity is 56.82023901197638
At time: 268.11796259880066 and batch: 450, loss is 3.981336069107056 and perplexity is 53.58858447101517
At time: 268.82374596595764 and batch: 500, loss is 4.012022943496704 and perplexity is 55.25854248154104
At time: 269.53037881851196 and batch: 550, loss is 4.05559624671936 and perplexity is 57.71956784122401
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.757785878282912 and perplexity of 116.48772216180984
Finished 30 epochs...
Completing Train Step...
At time: 271.3982105255127 and batch: 50, loss is 4.147563314437866 and perplexity is 63.27961975025988
At time: 272.1243085861206 and batch: 100, loss is 4.123861937522888 and perplexity is 61.79743986809082
At time: 272.836079120636 and batch: 150, loss is 4.104865970611573 and perplexity is 60.634617182306805
At time: 273.55500197410583 and batch: 200, loss is 4.0617254447937015 and perplexity is 58.07442890122761
At time: 274.2667636871338 and batch: 250, loss is 4.037518825531006 and perplexity is 56.68552150788189
At time: 274.9800672531128 and batch: 300, loss is 4.011917085647583 and perplexity is 55.252693240687876
At time: 275.6892590522766 and batch: 350, loss is 4.052187204360962 and perplexity is 57.523134404736346
At time: 276.3977406024933 and batch: 400, loss is 4.039329295158386 and perplexity is 56.788241880883774
At time: 277.10660314559937 and batch: 450, loss is 3.9809540414810183 and perplexity is 53.56811606130492
At time: 277.8274555206299 and batch: 500, loss is 4.011655907630921 and perplexity is 55.23826433619039
At time: 278.533704996109 and batch: 550, loss is 4.055359358787537 and perplexity is 57.70589639153702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.757719323990193 and perplexity of 116.47996966183479
Finished 31 epochs...
Completing Train Step...
At time: 280.35724782943726 and batch: 50, loss is 4.146575646400452 and perplexity is 63.217151346513795
At time: 281.0686402320862 and batch: 100, loss is 4.122908391952515 and perplexity is 61.73854127875144
At time: 281.7655746936798 and batch: 150, loss is 4.104068417549133 and perplexity is 60.586277137121115
At time: 282.462406873703 and batch: 200, loss is 4.060996370315552 and perplexity is 58.03210374825184
At time: 283.16057801246643 and batch: 250, loss is 4.036850590705871 and perplexity is 56.6476549216243
At time: 283.85784912109375 and batch: 300, loss is 4.011401748657226 and perplexity is 55.22422681957401
At time: 284.55617570877075 and batch: 350, loss is 4.051692819595337 and perplexity is 57.494702872053026
At time: 285.25652742385864 and batch: 400, loss is 4.038763794898987 and perplexity is 56.756137193829304
At time: 285.9723222255707 and batch: 450, loss is 3.9805458211898803 and perplexity is 53.54625293216075
At time: 286.68280839920044 and batch: 500, loss is 4.011271686553955 and perplexity is 55.21704470755274
At time: 287.39861536026 and batch: 550, loss is 4.055073499679565 and perplexity is 57.68940299297626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.757609590570977 and perplexity of 116.46718861776036
Finished 32 epochs...
Completing Train Step...
At time: 289.2371711730957 and batch: 50, loss is 4.145635566711426 and perplexity is 63.15775011185661
At time: 289.9446280002594 and batch: 100, loss is 4.12200361251831 and perplexity is 61.68270677906712
At time: 290.6410095691681 and batch: 150, loss is 4.103292064666748 and perplexity is 60.53925905990421
At time: 291.33932280540466 and batch: 200, loss is 4.060295648574829 and perplexity is 57.99145363536782
At time: 292.0459852218628 and batch: 250, loss is 4.036222648620606 and perplexity is 56.61209464113424
At time: 292.7509434223175 and batch: 300, loss is 4.0109031915664675 and perplexity is 55.19670125181507
At time: 293.46381282806396 and batch: 350, loss is 4.051218194961548 and perplexity is 57.46742094460714
At time: 294.1702632904053 and batch: 400, loss is 4.038195672035218 and perplexity is 56.7239018923032
At time: 294.8756489753723 and batch: 450, loss is 3.9801259803771973 and perplexity is 53.52377674835336
At time: 295.59724974632263 and batch: 500, loss is 4.010868511199951 and perplexity is 55.19478704317809
At time: 296.30209279060364 and batch: 550, loss is 4.0547803068161015 and perplexity is 57.67249135102874
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.757464145092254 and perplexity of 116.45025022359259
Finished 33 epochs...
Completing Train Step...
At time: 298.1617250442505 and batch: 50, loss is 4.144734616279602 and perplexity is 63.1008737348658
At time: 298.88092589378357 and batch: 100, loss is 4.1211610651016235 and perplexity is 61.6307580615182
At time: 299.5933322906494 and batch: 150, loss is 4.102521319389343 and perplexity is 60.49261668888875
At time: 300.3064730167389 and batch: 200, loss is 4.059597568511963 and perplexity is 57.95098508455599
At time: 301.0128197669983 and batch: 250, loss is 4.035619177818298 and perplexity is 56.577941201299225
At time: 301.7278287410736 and batch: 300, loss is 4.010401754379273 and perplexity is 55.16903051134586
At time: 302.4337875843048 and batch: 350, loss is 4.050726799964905 and perplexity is 57.43918867865022
At time: 303.15199971199036 and batch: 400, loss is 4.037628598213196 and perplexity is 56.69174437116312
At time: 303.86269783973694 and batch: 450, loss is 3.979688301086426 and perplexity is 53.50035562555083
At time: 304.57624435424805 and batch: 500, loss is 4.0104615592956545 and perplexity is 55.17232998926397
At time: 305.28497838974 and batch: 550, loss is 4.054453802108765 and perplexity is 57.65366408488156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.757328439266123 and perplexity of 116.43444831841238
Finished 34 epochs...
Completing Train Step...
At time: 307.1066756248474 and batch: 50, loss is 4.143868308067322 and perplexity is 63.04623260119596
At time: 307.8163242340088 and batch: 100, loss is 4.120339393615723 and perplexity is 61.58013862408673
At time: 308.5070106983185 and batch: 150, loss is 4.1018055725097655 and perplexity is 60.4493347785501
At time: 309.20746898651123 and batch: 200, loss is 4.0589204788208 and perplexity is 57.911760350792505
At time: 309.9084892272949 and batch: 250, loss is 4.035022668838501 and perplexity is 56.54420201517836
At time: 310.604617357254 and batch: 300, loss is 4.009912090301514 and perplexity is 55.14202283178456
At time: 311.29974269866943 and batch: 350, loss is 4.050256485939026 and perplexity is 57.41218057422254
At time: 312.00232434272766 and batch: 400, loss is 4.037059788703918 and perplexity is 56.6595067372728
At time: 312.70429611206055 and batch: 450, loss is 3.979257650375366 and perplexity is 53.47732061973556
At time: 313.42012882232666 and batch: 500, loss is 4.010041170120239 and perplexity is 55.149141013492745
At time: 314.1253385543823 and batch: 550, loss is 4.054123940467835 and perplexity is 57.63464948890623
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.757255391871675 and perplexity of 116.42594339597373
Finished 35 epochs...
Completing Train Step...
At time: 315.9510290622711 and batch: 50, loss is 4.1430309391021725 and perplexity is 62.99346174007031
At time: 316.6613597869873 and batch: 100, loss is 4.119519729614257 and perplexity is 61.52968428188015
At time: 317.3602433204651 and batch: 150, loss is 4.101127672195434 and perplexity is 60.408370042075255
At time: 318.0590944290161 and batch: 200, loss is 4.0582560491561885 and perplexity is 57.873294839514024
At time: 318.75865364074707 and batch: 250, loss is 4.034425611495972 and perplexity is 56.51045196054932
At time: 319.45820808410645 and batch: 300, loss is 4.0094211339950565 and perplexity is 55.114957152501105
At time: 320.1606249809265 and batch: 350, loss is 4.049791045188904 and perplexity is 57.38546482361174
At time: 320.8647232055664 and batch: 400, loss is 4.036491031646729 and perplexity is 56.627290405460585
At time: 321.5741186141968 and batch: 450, loss is 3.9788248252868654 and perplexity is 53.45417930213823
At time: 322.28377985954285 and batch: 500, loss is 4.009621543884277 and perplexity is 55.12600384185436
At time: 322.99257230758667 and batch: 550, loss is 4.053763604164123 and perplexity is 57.61388537360063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.75722487429355 and perplexity of 116.42239041236488
Finished 36 epochs...
Completing Train Step...
At time: 324.8315486907959 and batch: 50, loss is 4.142212100028992 and perplexity is 62.94190134495664
At time: 325.5557634830475 and batch: 100, loss is 4.118730216026306 and perplexity is 61.481124931733035
At time: 326.2578616142273 and batch: 150, loss is 4.100460042953491 and perplexity is 60.36805310765677
At time: 326.9709777832031 and batch: 200, loss is 4.057593483924865 and perplexity is 57.83496270670276
At time: 327.6782772541046 and batch: 250, loss is 4.033825497627259 and perplexity is 56.476549428307905
At time: 328.3870701789856 and batch: 300, loss is 4.008940553665161 and perplexity is 55.088476351796196
At time: 329.10258173942566 and batch: 350, loss is 4.049332146644592 and perplexity is 57.35913675875687
At time: 329.81929111480713 and batch: 400, loss is 4.035926690101624 and perplexity is 56.5953422885708
At time: 330.5260317325592 and batch: 450, loss is 3.9783874416351317 and perplexity is 53.430804430260686
At time: 331.2507047653198 and batch: 500, loss is 4.009187026023865 and perplexity is 55.10205581191207
At time: 331.9568257331848 and batch: 550, loss is 4.053411960601807 and perplexity is 57.59362938336197
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.75721578395113 and perplexity of 116.42133209778088
Finished 37 epochs...
Completing Train Step...
At time: 333.79063296318054 and batch: 50, loss is 4.141406078338623 and perplexity is 62.89118924751029
At time: 334.5008964538574 and batch: 100, loss is 4.117953429222107 and perplexity is 61.43338574916272
At time: 335.21192049980164 and batch: 150, loss is 4.099808325767517 and perplexity is 60.3287230273994
At time: 335.9211161136627 and batch: 200, loss is 4.056945033073426 and perplexity is 57.79747173273332
At time: 336.63351941108704 and batch: 250, loss is 4.03322546005249 and perplexity is 56.4426715415766
At time: 337.33655405044556 and batch: 300, loss is 4.008454337120056 and perplexity is 55.061697933731836
At time: 338.035404920578 and batch: 350, loss is 4.048867535591126 and perplexity is 57.33249325971281
At time: 338.7365183830261 and batch: 400, loss is 4.035367012023926 and perplexity is 56.56367597848844
At time: 339.43953561782837 and batch: 450, loss is 3.9779410314559938 and perplexity is 53.40695769839143
At time: 340.13934230804443 and batch: 500, loss is 4.00874577999115 and perplexity is 55.077747611733486
At time: 340.8441915512085 and batch: 550, loss is 4.053045372962952 and perplexity is 57.572520140183464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.757221627742686 and perplexity of 116.42201244176617
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 342.7248601913452 and batch: 50, loss is 4.140686497688294 and perplexity is 62.84595024315974
At time: 343.4373371601105 and batch: 100, loss is 4.116780753135681 and perplexity is 61.36138651092991
At time: 344.12573432922363 and batch: 150, loss is 4.098214092254639 and perplexity is 60.23262157978831
At time: 344.82712483406067 and batch: 200, loss is 4.055180711746216 and perplexity is 57.695588324660136
At time: 345.5304675102234 and batch: 250, loss is 4.030992259979248 and perplexity is 56.31676440359768
At time: 346.23546957969666 and batch: 300, loss is 4.00592342376709 and perplexity is 54.92251774798315
At time: 346.9438338279724 and batch: 350, loss is 4.04593864440918 and perplexity is 57.164818296491354
At time: 347.6547019481659 and batch: 400, loss is 4.031897869110107 and perplexity is 56.36778848010962
At time: 348.3635182380676 and batch: 450, loss is 3.9743060302734374 and perplexity is 53.213175756169406
At time: 349.08928775787354 and batch: 500, loss is 4.004527697563171 and perplexity is 54.845914421847496
At time: 349.7940618991852 and batch: 550, loss is 4.0488597726821896 and perplexity is 57.33204819451606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.756632054105718 and perplexity of 116.35339332246728
Finished 39 epochs...
Completing Train Step...
At time: 351.6277060508728 and batch: 50, loss is 4.1398035907745365 and perplexity is 62.790487606963765
At time: 352.34200716018677 and batch: 100, loss is 4.1161361408233645 and perplexity is 61.321844951541365
At time: 353.0416491031647 and batch: 150, loss is 4.097594528198242 and perplexity is 60.19531517048344
At time: 353.74848103523254 and batch: 200, loss is 4.0546056222915645 and perplexity is 57.662417739175886
At time: 354.4520070552826 and batch: 250, loss is 4.0304821634292605 and perplexity is 56.288044741891845
At time: 355.1593747138977 and batch: 300, loss is 4.00557849407196 and perplexity is 54.90357660754926
At time: 355.86598801612854 and batch: 350, loss is 4.045700340270996 and perplexity is 57.15119730676636
At time: 356.5737404823303 and batch: 400, loss is 4.031726551055908 and perplexity is 56.35813248741427
At time: 357.283349275589 and batch: 450, loss is 3.9743031311035155 and perplexity is 53.213021482354435
At time: 358.00369787216187 and batch: 500, loss is 4.0045426559448245 and perplexity is 54.84673483410354
At time: 358.70993423461914 and batch: 550, loss is 4.04893084526062 and perplexity is 57.336123075812395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.7565599806765295 and perplexity of 116.34500763660903
Finished 40 epochs...
Completing Train Step...
At time: 360.55148220062256 and batch: 50, loss is 4.139335107803345 and perplexity is 62.76107822220295
At time: 361.27117371559143 and batch: 100, loss is 4.115640635490418 and perplexity is 61.29146717713788
At time: 361.9643828868866 and batch: 150, loss is 4.097151117324829 and perplexity is 60.168629829930616
At time: 362.6577618122101 and batch: 200, loss is 4.054168701171875 and perplexity is 57.637229314131424
At time: 363.3554754257202 and batch: 250, loss is 4.030118474960327 and perplexity is 56.26757715121936
At time: 364.0570466518402 and batch: 300, loss is 4.005299606323242 and perplexity is 54.88826680762968
At time: 364.75898122787476 and batch: 350, loss is 4.045490870475769 and perplexity is 57.13922711091051
At time: 365.4721038341522 and batch: 400, loss is 4.031562404632568 and perplexity is 56.34888226075692
At time: 366.185587644577 and batch: 450, loss is 3.9742637157440184 and perplexity is 53.210924113317326
At time: 366.91065287590027 and batch: 500, loss is 4.00450258731842 and perplexity is 54.84453724480346
At time: 367.6323940753937 and batch: 550, loss is 4.048913989067078 and perplexity is 57.335156615170305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.756523294651762 and perplexity of 116.34073947906866
Finished 41 epochs...
Completing Train Step...
At time: 369.46355628967285 and batch: 50, loss is 4.138941268920899 and perplexity is 62.73636533606197
At time: 370.19064593315125 and batch: 100, loss is 4.115210590362548 and perplexity is 61.265114747071614
At time: 370.8930928707123 and batch: 150, loss is 4.0967544746398925 and perplexity is 60.144769115447666
At time: 371.5903265476227 and batch: 200, loss is 4.0537879943847654 and perplexity is 57.615290606113845
At time: 372.2932593822479 and batch: 250, loss is 4.029792966842652 and perplexity is 56.249264578701926
At time: 373.0020160675049 and batch: 300, loss is 4.0050450134277344 and perplexity is 54.874294423564045
At time: 373.71365118026733 and batch: 350, loss is 4.045290994644165 and perplexity is 57.12780750166448
At time: 374.4157416820526 and batch: 400, loss is 4.031393189430236 and perplexity is 56.33934797993936
At time: 375.11297845840454 and batch: 450, loss is 3.974202480316162 and perplexity is 53.207665819375144
At time: 375.8110177516937 and batch: 500, loss is 4.004430932998657 and perplexity is 54.8406075375864
At time: 376.50924706459045 and batch: 550, loss is 4.048856320381165 and perplexity is 57.331850267368985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.756504789311835 and perplexity of 116.3385865740574
Finished 42 epochs...
Completing Train Step...
At time: 378.33338046073914 and batch: 50, loss is 4.1385782432556155 and perplexity is 62.71359455873676
At time: 379.0593259334564 and batch: 100, loss is 4.114814739227295 and perplexity is 61.24086768127846
At time: 379.75982332229614 and batch: 150, loss is 4.096384825706482 and perplexity is 60.122540774288346
At time: 380.45888686180115 and batch: 200, loss is 4.053437838554382 and perplexity is 57.595119807856285
At time: 381.1623125076294 and batch: 250, loss is 4.029486865997314 and perplexity is 56.23204926621004
At time: 381.86822271347046 and batch: 300, loss is 4.004799818992614 and perplexity is 54.86084120133526
At time: 382.5687119960785 and batch: 350, loss is 4.045093641281128 and perplexity is 57.11653424917485
At time: 383.2745358943939 and batch: 400, loss is 4.031217908859253 and perplexity is 56.32947365227076
At time: 383.9760558605194 and batch: 450, loss is 3.974126377105713 and perplexity is 53.203616699263264
At time: 384.6980743408203 and batch: 500, loss is 4.004338293075562 and perplexity is 54.835527343239654
At time: 385.403249502182 and batch: 550, loss is 4.048774447441101 and perplexity is 57.32715653237585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.756494075693983 and perplexity of 116.3373401735761
Finished 43 epochs...
Completing Train Step...
At time: 387.22731041908264 and batch: 50, loss is 4.138234586715698 and perplexity is 62.69204632463277
At time: 387.9585020542145 and batch: 100, loss is 4.114439511299134 and perplexity is 61.21789270807495
At time: 388.6742699146271 and batch: 150, loss is 4.09603262424469 and perplexity is 60.10136925607965
At time: 389.38169264793396 and batch: 200, loss is 4.053106832504272 and perplexity is 57.576058629599785
At time: 390.09456300735474 and batch: 250, loss is 4.029192895889282 and perplexity is 56.21552115411686
At time: 390.80634331703186 and batch: 300, loss is 4.004560322761535 and perplexity is 54.84770380987438
At time: 391.5189778804779 and batch: 350, loss is 4.0448976278305055 and perplexity is 57.105339737382344
At time: 392.2244062423706 and batch: 400, loss is 4.031037664413452 and perplexity is 56.31932149247275
At time: 392.9386398792267 and batch: 450, loss is 3.974039626121521 and perplexity is 53.199001433344364
At time: 393.65149879455566 and batch: 500, loss is 4.004230642318726 and perplexity is 54.829624574944106
At time: 394.35847091674805 and batch: 550, loss is 4.048675808906555 and perplexity is 57.32150214454068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.75648790724734 and perplexity of 116.33662255511402
Finished 44 epochs...
Completing Train Step...
At time: 396.1764965057373 and batch: 50, loss is 4.1379034090042115 and perplexity is 62.67128755381341
At time: 396.8861162662506 and batch: 100, loss is 4.114079151153565 and perplexity is 61.19583619373029
At time: 397.5886194705963 and batch: 150, loss is 4.0956926965713505 and perplexity is 60.080942609451846
At time: 398.2854745388031 and batch: 200, loss is 4.052789211273193 and perplexity is 57.557774154899576
At time: 398.9828996658325 and batch: 250, loss is 4.028907608985901 and perplexity is 56.19948588959909
At time: 399.6738634109497 and batch: 300, loss is 4.004324841499328 and perplexity is 54.83478972392439
At time: 400.369247674942 and batch: 350, loss is 4.044702196121216 and perplexity is 57.094180633684786
At time: 401.06543612480164 and batch: 400, loss is 4.030853028297424 and perplexity is 56.308923871612784
At time: 401.7762393951416 and batch: 450, loss is 3.9739442014694215 and perplexity is 53.193925179344276
At time: 402.4922425746918 and batch: 500, loss is 4.004111366271973 and perplexity is 54.82308510408871
At time: 403.20519828796387 and batch: 550, loss is 4.04856493473053 and perplexity is 57.31514702253785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.7564849853515625 and perplexity of 116.3362826321244
Finished 45 epochs...
Completing Train Step...
At time: 405.01033878326416 and batch: 50, loss is 4.137581515312195 and perplexity is 62.65111730819084
At time: 405.72344732284546 and batch: 100, loss is 4.113729672431946 and perplexity is 61.17445328778186
At time: 406.4221842288971 and batch: 150, loss is 4.0953620386123655 and perplexity is 60.061079651698215
At time: 407.1197371482849 and batch: 200, loss is 4.052481079101563 and perplexity is 57.54004148509834
At time: 407.8143472671509 and batch: 250, loss is 4.028629040718078 and perplexity is 56.18383267650775
At time: 408.5094518661499 and batch: 300, loss is 4.004092502593994 and perplexity is 54.82205094881952
At time: 409.2093331813812 and batch: 350, loss is 4.044507045745849 and perplexity is 57.083039770010075
At time: 409.9096920490265 and batch: 400, loss is 4.030664377212524 and perplexity is 56.2983021339675
At time: 410.6078324317932 and batch: 450, loss is 3.973841781616211 and perplexity is 53.188477344323715
At time: 411.31275844573975 and batch: 500, loss is 4.0039829874038695 and perplexity is 54.81604743023109
At time: 412.0199558734894 and batch: 550, loss is 4.048444347381592 and perplexity is 57.308235957606264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.756486283971908 and perplexity of 116.33643370888603
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 413.8345630168915 and batch: 50, loss is 4.1373195600509645 and perplexity is 62.63470766777967
At time: 414.53691601753235 and batch: 100, loss is 4.113314752578735 and perplexity is 61.149076057725814
At time: 415.22793340682983 and batch: 150, loss is 4.094567499160767 and perplexity is 60.01337770745712
At time: 415.92247438430786 and batch: 200, loss is 4.051767225265503 and perplexity is 57.49898096310639
At time: 416.6208448410034 and batch: 250, loss is 4.027564301490783 and perplexity is 56.124043381569585
At time: 417.32761096954346 and batch: 300, loss is 4.003003964424133 and perplexity is 54.7624075217745
At time: 418.03309178352356 and batch: 350, loss is 4.0431884050369264 and perplexity is 57.0078173565484
At time: 418.739554643631 and batch: 400, loss is 4.02929322719574 and perplexity is 56.221161613750056
At time: 419.4491069316864 and batch: 450, loss is 3.9723919296264647 and perplexity is 53.1114178005796
At time: 420.1758270263672 and batch: 500, loss is 4.002214956283569 and perplexity is 54.719216577686836
At time: 420.8875455856323 and batch: 550, loss is 4.0465323352813725 and perplexity is 57.19876660372745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.7564379103640295 and perplexity of 116.3308062319716
Finished 47 epochs...
Completing Train Step...
At time: 422.77562046051025 and batch: 50, loss is 4.137025980949402 and perplexity is 62.616322125513484
At time: 423.49255084991455 and batch: 100, loss is 4.113111915588379 and perplexity is 61.13667402101364
At time: 424.19861125946045 and batch: 150, loss is 4.094305572509765 and perplexity is 59.99766066286521
At time: 424.9049537181854 and batch: 200, loss is 4.051562967300415 and perplexity is 57.48723753764518
At time: 425.6106128692627 and batch: 250, loss is 4.027391138076783 and perplexity is 56.114325592017195
At time: 426.3180639743805 and batch: 300, loss is 4.002853803634643 and perplexity is 54.75418497279434
At time: 427.02321219444275 and batch: 350, loss is 4.043087706565857 and perplexity is 57.002077045526725
At time: 427.7250442504883 and batch: 400, loss is 4.0292431926727295 and perplexity is 56.218348685117995
At time: 428.4271330833435 and batch: 450, loss is 3.9723863554000856 and perplexity is 53.1111217463386
At time: 429.12861013412476 and batch: 500, loss is 4.002230906486512 and perplexity is 54.72008936725667
At time: 429.82950091362 and batch: 550, loss is 4.0465502309799195 and perplexity is 57.199790224771036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.75642687209109 and perplexity of 116.32952214786822
Finished 48 epochs...
Completing Train Step...
At time: 431.6384108066559 and batch: 50, loss is 4.136863703727722 and perplexity is 62.60616174714843
At time: 432.3407769203186 and batch: 100, loss is 4.11293315410614 and perplexity is 61.12574611531989
At time: 433.0363235473633 and batch: 150, loss is 4.09409309387207 and perplexity is 59.98491379592909
At time: 433.73603534698486 and batch: 200, loss is 4.05138475894928 and perplexity is 57.4769937446241
At time: 434.4363315105438 and batch: 250, loss is 4.027250175476074 and perplexity is 56.10641612822713
At time: 435.13575530052185 and batch: 300, loss is 4.00272931098938 and perplexity is 54.747368903751855
At time: 435.8344702720642 and batch: 350, loss is 4.042998280525207 and perplexity is 56.996979803384214
At time: 436.5344650745392 and batch: 400, loss is 4.029197897911072 and perplexity is 56.21580234608185
At time: 437.2327585220337 and batch: 450, loss is 3.97237247467041 and perplexity is 53.11038453033144
At time: 437.9465022087097 and batch: 500, loss is 4.00222939491272 and perplexity is 54.72000665386623
At time: 438.64708137512207 and batch: 550, loss is 4.0465367937088015 and perplexity is 57.19902162084587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.756415509163065 and perplexity of 116.32820031139083
Finished 49 epochs...
Completing Train Step...
At time: 440.4667341709137 and batch: 50, loss is 4.136728935241699 and perplexity is 62.597724978032105
At time: 441.17866683006287 and batch: 100, loss is 4.112771019935608 and perplexity is 61.11583634655311
At time: 441.8746123313904 and batch: 150, loss is 4.093894410133362 and perplexity is 59.97299695287067
At time: 442.56929183006287 and batch: 200, loss is 4.051219754219055 and perplexity is 57.467510551184525
At time: 443.2639000415802 and batch: 250, loss is 4.0271200895309445 and perplexity is 56.099117946762966
At time: 443.9589297771454 and batch: 300, loss is 4.0026116466522215 and perplexity is 54.7409274698496
At time: 444.6533467769623 and batch: 350, loss is 4.042911376953125 and perplexity is 56.99202677746233
At time: 445.3482840061188 and batch: 400, loss is 4.029150009155273 and perplexity is 56.21311030571105
At time: 446.0474097728729 and batch: 450, loss is 3.972351851463318 and perplexity is 53.109289235166806
At time: 446.7493619918823 and batch: 500, loss is 4.002217597961426 and perplexity is 54.719361128420545
At time: 447.45329689979553 and batch: 550, loss is 4.046508803367614 and perplexity is 57.19742062312148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.756406094165558 and perplexity of 116.32710508683068
Finished Training.
Improved accuracyfrom -10000000 to -116.32710508683068
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efc55b01898>
SETTINGS FOR THIS RUN
{'dropout': 0.8557166803151515, 'wordvec_dim': 200, 'tune_wordvecs': True, 'anneal': 3.319342381826927, 'num_layers': 1, 'seq_len': 35, 'data': 'ptb', 'lr': 20.46347783894298, 'wordvec_source': '', 'batch_size': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.9170246124267578 and batch: 50, loss is 7.045118141174316 and perplexity is 1147.2443646063969
At time: 1.6051483154296875 and batch: 100, loss is 6.500318088531494 and perplexity is 665.3532406228403
At time: 2.2729222774505615 and batch: 150, loss is 6.407459306716919 and perplexity is 606.3511700226493
At time: 2.944758653640747 and batch: 200, loss is 6.310678691864013 and perplexity is 550.4183865538837
At time: 3.6164798736572266 and batch: 250, loss is 6.237044830322265 and perplexity is 511.3451642218348
At time: 4.287954330444336 and batch: 300, loss is 6.199884710311889 and perplexity is 492.69223548460513
At time: 4.9561448097229 and batch: 350, loss is 6.251744823455811 and perplexity is 518.9174545754692
At time: 5.621889114379883 and batch: 400, loss is 6.222704019546509 and perplexity is 504.06439085639647
At time: 6.288567066192627 and batch: 450, loss is 6.132092590332031 and perplexity is 460.3985789703665
At time: 6.955098390579224 and batch: 500, loss is 6.119460344314575 and perplexity is 454.61929040385274
At time: 7.621798992156982 and batch: 550, loss is 6.172943801879883 and perplexity is 479.59586525506404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.934306043259641 and perplexity of 377.7777437746087
Finished 1 epochs...
Completing Train Step...
At time: 9.37738037109375 and batch: 50, loss is 5.698829727172852 and perplexity is 298.51784914418175
At time: 10.059880256652832 and batch: 100, loss is 5.499364643096924 and perplexity is 244.53651493389438
At time: 10.729142427444458 and batch: 150, loss is 5.38371129989624 and perplexity is 217.82920662885525
At time: 11.39857268333435 and batch: 200, loss is 5.269576053619385 and perplexity is 194.33355797055398
At time: 12.068028450012207 and batch: 250, loss is 5.233838529586792 and perplexity is 187.51119111702414
At time: 12.737478494644165 and batch: 300, loss is 5.216083650588989 and perplexity is 184.21133357928124
At time: 13.40541410446167 and batch: 350, loss is 5.237786102294922 and perplexity is 188.25286812635724
At time: 14.080385684967041 and batch: 400, loss is 5.235368604660034 and perplexity is 187.7983169225087
At time: 14.74798035621643 and batch: 450, loss is 5.17228759765625 and perplexity is 176.31772049541564
At time: 15.412118434906006 and batch: 500, loss is 5.208070898056031 and perplexity is 182.74119155741528
At time: 16.075429916381836 and batch: 550, loss is 5.242298393249512 and perplexity is 189.10423921285872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.348130246426197 and perplexity of 210.21488016076145
Finished 2 epochs...
Completing Train Step...
At time: 17.906445026397705 and batch: 50, loss is 5.214591808319092 and perplexity is 183.93672421311436
At time: 18.603761911392212 and batch: 100, loss is 5.18108642578125 and perplexity is 177.87595507947225
At time: 19.298069953918457 and batch: 150, loss is 5.1749498081207275 and perplexity is 176.78774074480427
At time: 19.989084720611572 and batch: 200, loss is 5.095637273788452 and perplexity is 163.307883303782
At time: 20.685773849487305 and batch: 250, loss is 5.099519462585449 and perplexity is 163.9431075712634
At time: 21.37916588783264 and batch: 300, loss is 5.07469141960144 and perplexity is 159.9228353122687
At time: 22.071709156036377 and batch: 350, loss is 5.1409789562225345 and perplexity is 170.8829734148857
At time: 22.77087950706482 and batch: 400, loss is 5.111768321990967 and perplexity is 165.9635725807572
At time: 23.473307132720947 and batch: 450, loss is 5.065874547958374 and perplexity is 158.51901395098764
At time: 24.17141366004944 and batch: 500, loss is 5.103289041519165 and perplexity is 164.5622703141305
At time: 24.863386392593384 and batch: 550, loss is 5.14008243560791 and perplexity is 170.72984195954078
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.326789693629488 and perplexity of 205.77630765711223
Finished 3 epochs...
Completing Train Step...
At time: 26.655082941055298 and batch: 50, loss is 5.185752096176148 and perplexity is 178.7078047161542
At time: 27.360007286071777 and batch: 100, loss is 5.142336292266846 and perplexity is 171.11507651938084
At time: 28.051368236541748 and batch: 150, loss is 5.12624002456665 and perplexity is 168.3828110698328
At time: 28.745286226272583 and batch: 200, loss is 5.063290491104126 and perplexity is 158.10992059434332
At time: 29.44369149208069 and batch: 250, loss is 5.04269401550293 and perplexity is 154.88672069484682
At time: 30.148005723953247 and batch: 300, loss is 4.983060712814331 and perplexity is 145.920319052723
At time: 30.85519027709961 and batch: 350, loss is 5.092641000747681 and perplexity is 162.81930062477508
At time: 31.562264680862427 and batch: 400, loss is 5.081662101745605 and perplexity is 161.04150096647652
At time: 32.26550245285034 and batch: 450, loss is 5.051605777740479 and perplexity is 156.2732031482849
At time: 32.96510195732117 and batch: 500, loss is 5.071358766555786 and perplexity is 159.39075509993137
At time: 33.66415023803711 and batch: 550, loss is 5.071735200881958 and perplexity is 159.45076654590088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.3255134744847075 and perplexity of 205.5138595000408
Finished 4 epochs...
Completing Train Step...
At time: 35.47189950942993 and batch: 50, loss is 5.108107252120972 and perplexity is 165.357079230355
At time: 36.18484830856323 and batch: 100, loss is 5.054852924346924 and perplexity is 156.78146991131823
At time: 36.88788414001465 and batch: 150, loss is 5.051627073287964 and perplexity is 156.27653110713848
At time: 37.590770959854126 and batch: 200, loss is 4.976019735336304 and perplexity is 144.896505924929
At time: 38.29460859298706 and batch: 250, loss is 5.0519074535369874 and perplexity is 156.320354103111
At time: 38.995654582977295 and batch: 300, loss is 5.025375509262085 and perplexity is 152.2274081831481
At time: 39.69560480117798 and batch: 350, loss is 5.049039134979248 and perplexity is 155.8726199595295
At time: 40.39694809913635 and batch: 400, loss is 4.98982102394104 and perplexity is 146.91012774692484
At time: 41.099446535110474 and batch: 450, loss is 4.996597261428833 and perplexity is 147.90900615841386
At time: 41.81343626976013 and batch: 500, loss is 5.01427433013916 and perplexity is 150.54684980603753
At time: 42.51687812805176 and batch: 550, loss is 5.017288284301758 and perplexity is 151.00127557589957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.272726505360705 and perplexity of 194.94676189415526
Finished 5 epochs...
Completing Train Step...
At time: 44.34380745887756 and batch: 50, loss is 5.069199800491333 and perplexity is 159.0470070723675
At time: 45.05344843864441 and batch: 100, loss is 5.060220518112183 and perplexity is 157.6252717184807
At time: 45.7503719329834 and batch: 150, loss is 5.085879077911377 and perplexity is 161.72204304411287
At time: 46.446919679641724 and batch: 200, loss is 4.986631879806518 and perplexity is 146.44235646619802
At time: 47.14935660362244 and batch: 250, loss is 5.018166780471802 and perplexity is 151.13398790326525
At time: 47.8509840965271 and batch: 300, loss is 5.002553071975708 and perplexity is 148.7925526833876
At time: 48.55473804473877 and batch: 350, loss is 5.036279592514038 and perplexity is 153.89639134419923
At time: 49.257887840270996 and batch: 400, loss is 5.0055615425109865 and perplexity is 149.2408647226495
At time: 49.95973348617554 and batch: 450, loss is 4.986991481781006 and perplexity is 146.49502689635963
At time: 50.67296552658081 and batch: 500, loss is 5.019266166687012 and perplexity is 151.30023389374654
At time: 51.38478326797485 and batch: 550, loss is 5.040991773605347 and perplexity is 154.62329030423513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.289056006898272 and perplexity of 198.1562789304117
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 53.26238989830017 and batch: 50, loss is 5.013705110549926 and perplexity is 150.46117997481676
At time: 53.96858763694763 and batch: 100, loss is 4.95429295539856 and perplexity is 141.78232449814098
At time: 54.661922454833984 and batch: 150, loss is 4.93253490447998 and perplexity is 138.73073615706969
At time: 55.35632848739624 and batch: 200, loss is 4.863528022766113 and perplexity is 129.48020635348527
At time: 56.0628879070282 and batch: 250, loss is 4.857149209976196 and perplexity is 128.65690499141027
At time: 56.7656614780426 and batch: 300, loss is 4.81852650642395 and perplexity is 123.78256352429965
At time: 57.46913719177246 and batch: 350, loss is 4.890652265548706 and perplexity is 133.04032337593605
At time: 58.17150807380676 and batch: 400, loss is 4.833442506790161 and perplexity is 125.64274301746381
At time: 58.86997747421265 and batch: 450, loss is 4.7667242050170895 and perplexity is 117.53359469501592
At time: 59.56958317756653 and batch: 500, loss is 4.806423635482788 and perplexity is 122.29346847259244
At time: 60.26762056350708 and batch: 550, loss is 4.838907775878906 and perplexity is 126.331294262149
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.143361842378657 and perplexity of 171.29065362140756
Finished 7 epochs...
Completing Train Step...
At time: 62.089683055877686 and batch: 50, loss is 4.901824808120727 and perplexity is 134.5350564889243
At time: 62.79707217216492 and batch: 100, loss is 4.880018653869629 and perplexity is 131.63311932997527
At time: 63.49050760269165 and batch: 150, loss is 4.869940195083618 and perplexity is 130.31312329797112
At time: 64.18568778038025 and batch: 200, loss is 4.805842342376709 and perplexity is 122.22240078003193
At time: 64.88352131843567 and batch: 250, loss is 4.797822961807251 and perplexity is 121.24617243916062
At time: 65.5816924571991 and batch: 300, loss is 4.764346170425415 and perplexity is 117.25442780696415
At time: 66.27807450294495 and batch: 350, loss is 4.815582933425904 and perplexity is 123.41873625145082
At time: 66.97446846961975 and batch: 400, loss is 4.779550666809082 and perplexity is 119.05084452895834
At time: 67.67018437385559 and batch: 450, loss is 4.731988296508789 and perplexity is 113.52105161521378
At time: 68.37109327316284 and batch: 500, loss is 4.766869087219238 and perplexity is 117.5506244546665
At time: 69.07114934921265 and batch: 550, loss is 4.802933568954468 and perplexity is 121.86740006766289
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.136523307637965 and perplexity of 170.12327267287935
Finished 8 epochs...
Completing Train Step...
At time: 70.88412189483643 and batch: 50, loss is 4.848526029586792 and perplexity is 127.55224298634026
At time: 71.59897565841675 and batch: 100, loss is 4.8310581398010255 and perplexity is 125.34352147733127
At time: 72.30062532424927 and batch: 150, loss is 4.8239888572692875 and perplexity is 124.46055734908566
At time: 72.99865174293518 and batch: 200, loss is 4.7730775070190425 and perplexity is 118.28269823824817
At time: 73.69454050064087 and batch: 250, loss is 4.765276803970337 and perplexity is 117.36359950231356
At time: 74.39157104492188 and batch: 300, loss is 4.744952945709229 and perplexity is 115.00239401970603
At time: 75.08872246742249 and batch: 350, loss is 4.788124265670777 and perplexity is 120.07592675682204
At time: 75.78620028495789 and batch: 400, loss is 4.750980472564697 and perplexity is 115.69766732235351
At time: 76.4829523563385 and batch: 450, loss is 4.688812475204468 and perplexity is 108.7239906801502
At time: 77.18240571022034 and batch: 500, loss is 4.726415328979492 and perplexity is 112.89016207697078
At time: 77.90227007865906 and batch: 550, loss is 4.764802780151367 and perplexity is 117.3079795443034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.131187925947473 and perplexity of 169.2180171679952
Finished 9 epochs...
Completing Train Step...
At time: 79.72237229347229 and batch: 50, loss is 4.815002145767212 and perplexity is 123.34707698400807
At time: 80.4232439994812 and batch: 100, loss is 4.799237632751465 and perplexity is 121.41781725825068
At time: 81.1242470741272 and batch: 150, loss is 4.786802062988281 and perplexity is 119.91726695768138
At time: 81.81156635284424 and batch: 200, loss is 4.741498308181763 and perplexity is 114.60578789322312
At time: 82.5197217464447 and batch: 250, loss is 4.726104116439819 and perplexity is 112.8550347092489
At time: 83.23187375068665 and batch: 300, loss is 4.710625953674317 and perplexity is 111.12169517389084
At time: 83.95112800598145 and batch: 350, loss is 4.765106344223023 and perplexity is 117.34359543779088
At time: 84.6576759815216 and batch: 400, loss is 4.7368817806243895 and perplexity is 114.07792649601879
At time: 85.35914397239685 and batch: 450, loss is 4.689413299560547 and perplexity is 108.78933432990337
At time: 86.05823421478271 and batch: 500, loss is 4.712448654174804 and perplexity is 111.32442144180892
At time: 86.74661827087402 and batch: 550, loss is 4.734318447113037 and perplexity is 113.7858811887987
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.14670578976895 and perplexity of 171.8643993086286
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 88.5438928604126 and batch: 50, loss is 4.760191574096679 and perplexity is 116.7682935367466
At time: 89.25683522224426 and batch: 100, loss is 4.710881690979004 and perplexity is 111.15011677078331
At time: 89.94686317443848 and batch: 150, loss is 4.6735248947143555 and perplexity is 107.07450437377057
At time: 90.6377694606781 and batch: 200, loss is 4.622854061126709 and perplexity is 101.78411606296795
At time: 91.32895350456238 and batch: 250, loss is 4.5923035621643065 and perplexity is 98.72157973079302
At time: 92.02044868469238 and batch: 300, loss is 4.570562686920166 and perplexity is 96.5984491672324
At time: 92.71679615974426 and batch: 350, loss is 4.618993148803711 and perplexity is 101.39189416937204
At time: 93.41864943504333 and batch: 400, loss is 4.576541862487793 and perplexity is 97.17775842435803
At time: 94.1195821762085 and batch: 450, loss is 4.531247024536133 and perplexity is 92.87430543674336
At time: 94.82130074501038 and batch: 500, loss is 4.559002532958984 and perplexity is 95.4881859934018
At time: 95.52519607543945 and batch: 550, loss is 4.5928956890106205 and perplexity is 98.78005273847252
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.031999628594581 and perplexity of 153.23912787719092
Finished 11 epochs...
Completing Train Step...
At time: 97.35575675964355 and batch: 50, loss is 4.687912473678589 and perplexity is 108.62618294279683
At time: 98.05984401702881 and batch: 100, loss is 4.663278827667236 and perplexity is 105.98301311809786
At time: 98.74871516227722 and batch: 150, loss is 4.635329647064209 and perplexity is 103.06188644442015
At time: 99.43937921524048 and batch: 200, loss is 4.593865432739258 and perplexity is 98.87589053667149
At time: 100.13736200332642 and batch: 250, loss is 4.568862857818604 and perplexity is 96.43438778982757
At time: 100.83424878120422 and batch: 300, loss is 4.550651655197144 and perplexity is 94.69409611302486
At time: 101.53106427192688 and batch: 350, loss is 4.601779193878174 and perplexity is 99.66147508105936
At time: 102.22739386558533 and batch: 400, loss is 4.569462690353394 and perplexity is 96.4922496250672
At time: 102.93452334403992 and batch: 450, loss is 4.52837607383728 and perplexity is 92.60805027025327
At time: 103.64510416984558 and batch: 500, loss is 4.558940849304199 and perplexity is 95.48229611475743
At time: 104.34719777107239 and batch: 550, loss is 4.592315692901611 and perplexity is 98.72277730360672
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.0285797119140625 and perplexity of 152.7159579363681
Finished 12 epochs...
Completing Train Step...
At time: 106.16040515899658 and batch: 50, loss is 4.667529935836792 and perplexity is 106.43451738778084
At time: 106.88007092475891 and batch: 100, loss is 4.643508806228637 and perplexity is 103.90830278575264
At time: 107.58178472518921 and batch: 150, loss is 4.618400440216065 and perplexity is 101.33181612912409
At time: 108.283855676651 and batch: 200, loss is 4.579042119979858 and perplexity is 97.42103183927881
At time: 108.98123741149902 and batch: 250, loss is 4.55618314743042 and perplexity is 95.21934714192867
At time: 109.67912912368774 and batch: 300, loss is 4.539598398208618 and perplexity is 93.65318128071209
At time: 110.37215971946716 and batch: 350, loss is 4.591744346618652 and perplexity is 98.66638852204527
At time: 111.06567478179932 and batch: 400, loss is 4.562790651321411 and perplexity is 95.85059253021369
At time: 111.76320266723633 and batch: 450, loss is 4.521119527816772 and perplexity is 91.93846805687626
At time: 112.45293092727661 and batch: 500, loss is 4.555067901611328 and perplexity is 95.1132133567626
At time: 113.16488575935364 and batch: 550, loss is 4.588645181655884 and perplexity is 98.3610784552365
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.026623340363198 and perplexity of 152.41748084214524
Finished 13 epochs...
Completing Train Step...
At time: 114.96658682823181 and batch: 50, loss is 4.653047924041748 and perplexity is 104.90423895192774
At time: 115.66419768333435 and batch: 100, loss is 4.629662036895752 and perplexity is 102.47942399302813
At time: 116.34797501564026 and batch: 150, loss is 4.604437952041626 and perplexity is 99.92680340713895
At time: 117.03148889541626 and batch: 200, loss is 4.567483501434326 and perplexity is 96.30146209839195
At time: 117.71560716629028 and batch: 250, loss is 4.545721607208252 and perplexity is 94.22839857377696
At time: 118.41541838645935 and batch: 300, loss is 4.530848293304444 and perplexity is 92.83728093244792
At time: 119.12292146682739 and batch: 350, loss is 4.583611173629761 and perplexity is 97.86717220386733
At time: 119.83550930023193 and batch: 400, loss is 4.557369260787964 and perplexity is 95.33235508833883
At time: 120.55236458778381 and batch: 450, loss is 4.517850189208985 and perplexity is 91.63838088429856
At time: 121.25254821777344 and batch: 500, loss is 4.550315227508545 and perplexity is 94.66224375545352
At time: 121.9531626701355 and batch: 550, loss is 4.58410792350769 and perplexity is 97.91579978658585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.027741127825798 and perplexity of 152.5879464457199
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 123.7721939086914 and batch: 50, loss is 4.632590942382812 and perplexity is 102.78001652895026
At time: 124.48683953285217 and batch: 100, loss is 4.600445165634155 and perplexity is 99.5286124993828
At time: 125.18781065940857 and batch: 150, loss is 4.567218322753906 and perplexity is 96.2759283893974
At time: 125.88892817497253 and batch: 200, loss is 4.528148365020752 and perplexity is 92.58696500146719
At time: 126.58735084533691 and batch: 250, loss is 4.498174076080322 and perplexity is 89.85291683448438
At time: 127.28398847579956 and batch: 300, loss is 4.476339979171753 and perplexity is 87.91232220045548
At time: 127.98003506660461 and batch: 350, loss is 4.52586576461792 and perplexity is 92.37586697573346
At time: 128.67773842811584 and batch: 400, loss is 4.498410358428955 and perplexity is 89.87415000111798
At time: 129.37503337860107 and batch: 450, loss is 4.448870325088501 and perplexity is 85.53026800831661
At time: 130.0721251964569 and batch: 500, loss is 4.479755325317383 and perplexity is 88.21308652601857
At time: 130.77390694618225 and batch: 550, loss is 4.519329280853271 and perplexity is 91.77402273639673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.999788000228557 and perplexity of 148.38169888166541
Finished 15 epochs...
Completing Train Step...
At time: 132.57322573661804 and batch: 50, loss is 4.607752494812011 and perplexity is 100.25856458551479
At time: 133.26274371147156 and batch: 100, loss is 4.584050455093384 and perplexity is 97.91017288252274
At time: 133.9411416053772 and batch: 150, loss is 4.554782524108886 and perplexity is 95.08607405814247
At time: 134.62059473991394 and batch: 200, loss is 4.517719049453735 and perplexity is 91.6263642374052
At time: 135.3007788658142 and batch: 250, loss is 4.491110219955444 and perplexity is 89.22044523169103
At time: 136.00017976760864 and batch: 300, loss is 4.471622219085694 and perplexity is 87.49854976270427
At time: 136.69300937652588 and batch: 350, loss is 4.522993383407592 and perplexity is 92.11090898349515
At time: 137.38180661201477 and batch: 400, loss is 4.49624228477478 and perplexity is 89.67950730040879
At time: 138.07499408721924 and batch: 450, loss is 4.448308391571045 and perplexity is 85.48221918535212
At time: 138.77162837982178 and batch: 500, loss is 4.481051845550537 and perplexity is 88.32753075112386
At time: 139.4771113395691 and batch: 550, loss is 4.521026182174682 and perplexity is 91.92988640207902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.998503989361702 and perplexity of 148.191297432783
Finished 16 epochs...
Completing Train Step...
At time: 141.28293895721436 and batch: 50, loss is 4.601208934783935 and perplexity is 99.60465842019768
At time: 141.99300503730774 and batch: 100, loss is 4.577460527420044 and perplexity is 97.26707324213136
At time: 142.68867659568787 and batch: 150, loss is 4.549225578308105 and perplexity is 94.55915129474272
At time: 143.38886618614197 and batch: 200, loss is 4.512876062393189 and perplexity is 91.18369173482196
At time: 144.09002089500427 and batch: 250, loss is 4.487404556274414 and perplexity is 88.89043609726136
At time: 144.79206109046936 and batch: 300, loss is 4.468947248458862 and perplexity is 87.26480647978525
At time: 145.49292659759521 and batch: 350, loss is 4.521277360916137 and perplexity is 91.95298013545361
At time: 146.19346070289612 and batch: 400, loss is 4.494554042816162 and perplexity is 89.52823432198008
At time: 146.896253824234 and batch: 450, loss is 4.446991586685181 and perplexity is 85.36972986097481
At time: 147.60089445114136 and batch: 500, loss is 4.48046425819397 and perplexity is 88.27564585576476
At time: 148.2974374294281 and batch: 550, loss is 4.520733509063721 and perplexity is 91.90298493309571
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.998238746156084 and perplexity of 148.15199591046851
Finished 17 epochs...
Completing Train Step...
At time: 150.12182021141052 and batch: 50, loss is 4.596506586074829 and perplexity is 99.13738209241035
At time: 150.82521963119507 and batch: 100, loss is 4.572870855331421 and perplexity is 96.82167217521993
At time: 151.51429438591003 and batch: 150, loss is 4.545096769332885 and perplexity is 94.16953949201971
At time: 152.21063780784607 and batch: 200, loss is 4.509381217956543 and perplexity is 90.86557512486992
At time: 152.90773344039917 and batch: 250, loss is 4.484563388824463 and perplexity is 88.6382419162065
At time: 153.6052930355072 and batch: 300, loss is 4.466680164337158 and perplexity is 87.06719390943022
At time: 154.30327105522156 and batch: 350, loss is 4.519659118652344 and perplexity is 91.80429827080147
At time: 155.0010223388672 and batch: 400, loss is 4.49259747505188 and perplexity is 89.35323751709248
At time: 155.70003247261047 and batch: 450, loss is 4.445215120315551 and perplexity is 85.21820803345598
At time: 156.39979577064514 and batch: 500, loss is 4.479172611236573 and perplexity is 88.1616984921077
At time: 157.10251355171204 and batch: 550, loss is 4.519485149383545 and perplexity is 91.78832853332077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9980903787815825 and perplexity of 148.13001661835332
Finished 18 epochs...
Completing Train Step...
At time: 158.92107105255127 and batch: 50, loss is 4.59268630027771 and perplexity is 98.75937147368023
At time: 159.61980962753296 and batch: 100, loss is 4.568915586471558 and perplexity is 96.43947277925533
At time: 160.30674481391907 and batch: 150, loss is 4.541441316604614 and perplexity is 93.82593558848933
At time: 160.9931824207306 and batch: 200, loss is 4.506124715805054 and perplexity is 90.57015246731444
At time: 161.69142317771912 and batch: 250, loss is 4.481812114715576 and perplexity is 88.39470898271121
At time: 162.3860228061676 and batch: 300, loss is 4.46462233543396 and perplexity is 86.88820874486724
At time: 163.081116437912 and batch: 350, loss is 4.5180213832855225 and perplexity is 91.65407017520798
At time: 163.77924013137817 and batch: 400, loss is 4.490609378814697 and perplexity is 89.175771150366
At time: 164.48160004615784 and batch: 450, loss is 4.4434526252746585 and perplexity is 85.06814364711812
At time: 165.1794764995575 and batch: 500, loss is 4.4777317237854 and perplexity is 88.03475888188427
At time: 165.87521862983704 and batch: 550, loss is 4.518139944076538 and perplexity is 91.66493739846834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.998002721908245 and perplexity of 148.1170325733278
Finished 19 epochs...
Completing Train Step...
At time: 167.6823387145996 and batch: 50, loss is 4.589438524246216 and perplexity is 98.43914345005416
At time: 168.38561391830444 and batch: 100, loss is 4.565590705871582 and perplexity is 96.11935551787914
At time: 169.07577443122864 and batch: 150, loss is 4.538243894577026 and perplexity is 93.52641357959455
At time: 169.77043318748474 and batch: 200, loss is 4.503249940872192 and perplexity is 90.31015755581998
At time: 170.47745537757874 and batch: 250, loss is 4.479273939132691 and perplexity is 88.17063218414258
At time: 171.1876995563507 and batch: 300, loss is 4.462453413009643 and perplexity is 86.69995918365647
At time: 171.8853940963745 and batch: 350, loss is 4.516323928833008 and perplexity is 91.4986235348186
At time: 172.58235454559326 and batch: 400, loss is 4.488502168655396 and perplexity is 88.98805690557577
At time: 173.27915835380554 and batch: 450, loss is 4.4413495540618895 and perplexity is 84.88942727558879
At time: 173.9775574207306 and batch: 500, loss is 4.476014823913574 and perplexity is 87.88374169343616
At time: 174.6739740371704 and batch: 550, loss is 4.5167323589324955 and perplexity is 91.53600195944917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9980010986328125 and perplexity of 148.11679213878287
Finished 20 epochs...
Completing Train Step...
At time: 176.49285054206848 and batch: 50, loss is 4.586375579833985 and perplexity is 98.13809111432715
At time: 177.19660353660583 and batch: 100, loss is 4.562348623275756 and perplexity is 95.80823324280897
At time: 177.88107442855835 and batch: 150, loss is 4.5351040649414065 and perplexity is 93.23321710888182
At time: 178.56499934196472 and batch: 200, loss is 4.5005677223205565 and perplexity is 90.0682505445885
At time: 179.2644226551056 and batch: 250, loss is 4.476831617355347 and perplexity is 87.95555388115936
At time: 179.96343636512756 and batch: 300, loss is 4.460542001724243 and perplexity is 86.53439818121966
At time: 180.65453553199768 and batch: 350, loss is 4.514624156951904 and perplexity is 91.34322885254433
At time: 181.35391902923584 and batch: 400, loss is 4.486541051864624 and perplexity is 88.81371194429279
At time: 182.0552899837494 and batch: 450, loss is 4.439299163818359 and perplexity is 84.71554914205507
At time: 182.75583028793335 and batch: 500, loss is 4.474455099105835 and perplexity is 87.74677408497564
At time: 183.4571042060852 and batch: 550, loss is 4.515042104721069 and perplexity is 91.38141353031618
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9982780294215425 and perplexity of 148.15781591896575
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 185.29233598709106 and batch: 50, loss is 4.5815314674377445 and perplexity is 97.66384873965775
At time: 185.99153661727905 and batch: 100, loss is 4.554308099746704 and perplexity is 95.04097360732725
At time: 186.67811632156372 and batch: 150, loss is 4.525902290344238 and perplexity is 92.37924113299042
At time: 187.37511324882507 and batch: 200, loss is 4.490266399383545 and perplexity is 89.14519093959574
At time: 188.0706958770752 and batch: 250, loss is 4.461977272033692 and perplexity is 86.65868760679996
At time: 188.78498435020447 and batch: 300, loss is 4.4433294200897215 and perplexity is 85.05766345636802
At time: 189.48891258239746 and batch: 350, loss is 4.496448774337768 and perplexity is 89.69802709468652
At time: 190.1858332157135 and batch: 400, loss is 4.463352336883545 and perplexity is 86.77793088689518
At time: 190.87841129302979 and batch: 450, loss is 4.4150966358184816 and perplexity is 82.6898313195516
At time: 191.57280492782593 and batch: 500, loss is 4.448759441375732 and perplexity is 85.52078462043207
At time: 192.2769477367401 and batch: 550, loss is 4.492392511367798 and perplexity is 89.33492522508786
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.990733694522939 and perplexity of 147.04426950300845
Finished 22 epochs...
Completing Train Step...
At time: 194.16015076637268 and batch: 50, loss is 4.5740949821472165 and perplexity is 96.94026675307192
At time: 194.86773371696472 and batch: 100, loss is 4.5480867195129395 and perplexity is 94.45152307193418
At time: 195.56231832504272 and batch: 150, loss is 4.521828746795654 and perplexity is 92.00369569091544
At time: 196.2558307647705 and batch: 200, loss is 4.486340389251709 and perplexity is 88.79589214073539
At time: 196.9565553665161 and batch: 250, loss is 4.4594761085510255 and perplexity is 86.44221089657653
At time: 197.650235414505 and batch: 300, loss is 4.441019325256348 and perplexity is 84.86139896954818
At time: 198.35352492332458 and batch: 350, loss is 4.495658769607544 and perplexity is 89.62719321222501
At time: 199.04669284820557 and batch: 400, loss is 4.463367214202881 and perplexity is 86.77922191948787
At time: 199.7385311126709 and batch: 450, loss is 4.415991582870483 and perplexity is 82.76386746459495
At time: 200.43116569519043 and batch: 500, loss is 4.450765676498413 and perplexity is 85.69253164710888
At time: 201.12870168685913 and batch: 550, loss is 4.49420841217041 and perplexity is 89.49729596746808
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9898129726978055 and perplexity of 146.90894494251424
Finished 23 epochs...
Completing Train Step...
At time: 202.93656373023987 and batch: 50, loss is 4.570700387954712 and perplexity is 96.61175178948972
At time: 203.63744568824768 and batch: 100, loss is 4.544945526123047 and perplexity is 94.15529806558457
At time: 204.32941007614136 and batch: 150, loss is 4.52010181427002 and perplexity is 91.8449486285335
At time: 205.0247302055359 and batch: 200, loss is 4.484388265609741 and perplexity is 88.62272066144054
At time: 205.7318696975708 and batch: 250, loss is 4.458070077896118 and perplexity is 86.32075590290904
At time: 206.43116784095764 and batch: 300, loss is 4.439758424758911 and perplexity is 84.75446462032899
At time: 207.13962626457214 and batch: 350, loss is 4.495314836502075 and perplexity is 89.59637275372093
At time: 207.83196091651917 and batch: 400, loss is 4.463402986526489 and perplexity is 86.7823262694214
At time: 208.52358889579773 and batch: 450, loss is 4.416379232406616 and perplexity is 82.79595705878243
At time: 209.21543264389038 and batch: 500, loss is 4.451732320785522 and perplexity is 85.7754058917779
At time: 209.91241121292114 and batch: 550, loss is 4.494939737319946 and perplexity is 89.56277152986904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9893607281624 and perplexity of 146.8425211959821
Finished 24 epochs...
Completing Train Step...
At time: 211.74768471717834 and batch: 50, loss is 4.568048963546753 and perplexity is 96.35593232555333
At time: 212.45555090904236 and batch: 100, loss is 4.542613668441772 and perplexity is 93.93599709925094
At time: 213.15255856513977 and batch: 150, loss is 4.518771781921386 and perplexity is 91.72287307601172
At time: 213.8486397266388 and batch: 200, loss is 4.482942962646485 and perplexity is 88.49472649810906
At time: 214.55487966537476 and batch: 250, loss is 4.456932172775269 and perplexity is 86.22258693680882
At time: 215.25170469284058 and batch: 300, loss is 4.438677330017089 and perplexity is 84.66288652549989
At time: 215.94155955314636 and batch: 350, loss is 4.494993715286255 and perplexity is 89.56760607661155
At time: 216.63154578208923 and batch: 400, loss is 4.463350048065186 and perplexity is 86.77773226820112
At time: 217.32650876045227 and batch: 450, loss is 4.416549530029297 and perplexity is 82.81005821409967
At time: 218.02176308631897 and batch: 500, loss is 4.452263116836548 and perplexity is 85.82094722401075
At time: 218.7171540260315 and batch: 550, loss is 4.495351495742798 and perplexity is 89.59965734892263
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.989149702356217 and perplexity of 146.81153690392236
Finished 25 epochs...
Completing Train Step...
At time: 220.5274555683136 and batch: 50, loss is 4.565923748016357 and perplexity is 96.1513726454257
At time: 221.23290514945984 and batch: 100, loss is 4.540727663040161 and perplexity is 93.75900026227634
At time: 221.93582797050476 and batch: 150, loss is 4.517748012542724 and perplexity is 91.62901805837761
At time: 222.63625836372375 and batch: 200, loss is 4.481780166625977 and perplexity is 88.3918849857394
At time: 223.33206939697266 and batch: 250, loss is 4.455968780517578 and perplexity is 86.13956076392344
At time: 224.02880215644836 and batch: 300, loss is 4.437787809371948 and perplexity is 84.5876106247556
At time: 224.72476410865784 and batch: 350, loss is 4.4946925067901615 and perplexity is 89.5406316153589
At time: 225.42213416099548 and batch: 400, loss is 4.463200607299805 and perplexity is 86.76476510640835
At time: 226.1176426410675 and batch: 450, loss is 4.4165224361419675 and perplexity is 82.80781459810696
At time: 226.81381797790527 and batch: 500, loss is 4.452565937042237 and perplexity is 85.84693947619249
At time: 227.51057696342468 and batch: 550, loss is 4.495522336959839 and perplexity is 89.61496597106515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.988971466713763 and perplexity of 146.78537218713444
Finished 26 epochs...
Completing Train Step...
At time: 229.3539435863495 and batch: 50, loss is 4.563987941741943 and perplexity is 95.9654222549998
At time: 230.05656480789185 and batch: 100, loss is 4.539084529876709 and perplexity is 93.6050682396122
At time: 230.74496126174927 and batch: 150, loss is 4.516817655563354 and perplexity is 91.54381000501385
At time: 231.4353666305542 and batch: 200, loss is 4.48067048072815 and perplexity is 88.29385216037012
At time: 232.12425327301025 and batch: 250, loss is 4.455052433013916 and perplexity is 86.06066314678891
At time: 232.80876874923706 and batch: 300, loss is 4.436891441345215 and perplexity is 84.51182296702486
At time: 233.4973816871643 and batch: 350, loss is 4.494371395111084 and perplexity is 89.51188368868979
At time: 234.19765758514404 and batch: 400, loss is 4.46300859451294 and perplexity is 86.74810676141738
At time: 234.89442944526672 and batch: 450, loss is 4.416416521072388 and perplexity is 82.79904446711478
At time: 235.58876609802246 and batch: 500, loss is 4.45274619102478 and perplexity is 85.8624151236539
At time: 236.28849124908447 and batch: 550, loss is 4.495575838088989 and perplexity is 89.61976060119126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.988861733294548 and perplexity of 146.76926581007362
Finished 27 epochs...
Completing Train Step...
At time: 238.12118411064148 and batch: 50, loss is 4.562306442260742 and perplexity is 95.8041920395157
At time: 238.83145308494568 and batch: 100, loss is 4.537620687484742 and perplexity is 93.46814541378876
At time: 239.539057970047 and batch: 150, loss is 4.515928249359131 and perplexity is 91.46242656926833
At time: 240.24410557746887 and batch: 200, loss is 4.479662275314331 and perplexity is 88.20487867992406
At time: 240.9397475719452 and batch: 250, loss is 4.454183120727539 and perplexity is 85.98588206370817
At time: 241.6332848072052 and batch: 300, loss is 4.436038808822632 and perplexity is 84.43979614878778
At time: 242.32669496536255 and batch: 350, loss is 4.493997421264648 and perplexity is 89.47841484387138
At time: 243.0166733264923 and batch: 400, loss is 4.462743844985962 and perplexity is 86.72514328110547
At time: 243.70555138587952 and batch: 450, loss is 4.416275634765625 and perplexity is 82.78738003723481
At time: 244.40239119529724 and batch: 500, loss is 4.452794055938721 and perplexity is 85.86652501912366
At time: 245.09503865242004 and batch: 550, loss is 4.495458374023437 and perplexity is 89.60923411801113
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.988716937125997 and perplexity of 146.74801572122624
Finished 28 epochs...
Completing Train Step...
At time: 246.87691283226013 and batch: 50, loss is 4.560786094665527 and perplexity is 95.6586470340715
At time: 247.5751645565033 and batch: 100, loss is 4.536316404342651 and perplexity is 93.34631595472882
At time: 248.25967526435852 and batch: 150, loss is 4.515063171386719 and perplexity is 91.38333865227946
At time: 248.94838285446167 and batch: 200, loss is 4.478744049072265 and perplexity is 88.12392381876894
At time: 249.6414611339569 and batch: 250, loss is 4.453404207229614 and perplexity is 85.91893257685217
At time: 250.33630776405334 and batch: 300, loss is 4.435312376022339 and perplexity is 84.37847858545635
At time: 251.03375816345215 and batch: 350, loss is 4.493634595870971 and perplexity is 89.44595569163849
At time: 251.73553490638733 and batch: 400, loss is 4.462518110275268 and perplexity is 86.70556861540094
At time: 252.4352843761444 and batch: 450, loss is 4.4161053562164305 and perplexity is 82.77328432240347
At time: 253.13718724250793 and batch: 500, loss is 4.452819004058838 and perplexity is 85.86866725422607
At time: 253.86420154571533 and batch: 550, loss is 4.49533899307251 and perplexity is 89.59853712095179
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.98864291576629 and perplexity of 146.7371536355863
Finished 29 epochs...
Completing Train Step...
At time: 255.70666122436523 and batch: 50, loss is 4.559271173477173 and perplexity is 95.51384143505925
At time: 256.4070563316345 and batch: 100, loss is 4.5349955558776855 and perplexity is 93.22310100863984
At time: 257.09305572509766 and batch: 150, loss is 4.514184379577637 and perplexity is 91.303066998987
At time: 257.7796347141266 and batch: 200, loss is 4.477793016433716 and perplexity is 88.04015493076737
At time: 258.46849966049194 and batch: 250, loss is 4.452561359405518 and perplexity is 85.84654650098962
At time: 259.16519260406494 and batch: 300, loss is 4.434461002349853 and perplexity is 84.30667154191484
At time: 259.86765122413635 and batch: 350, loss is 4.49310420036316 and perplexity is 89.3985265377647
At time: 260.5639443397522 and batch: 400, loss is 4.462150812149048 and perplexity is 86.6737276704333
At time: 261.26267290115356 and batch: 450, loss is 4.4158902359008785 and perplexity is 82.75548002246273
At time: 261.96097469329834 and batch: 500, loss is 4.452764120101929 and perplexity is 85.86395457131918
At time: 262.66825890541077 and batch: 550, loss is 4.495185289382935 and perplexity is 89.58476655353638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.988636098009475 and perplexity of 146.7361532207674
Finished 30 epochs...
Completing Train Step...
At time: 264.47255086898804 and batch: 50, loss is 4.557996473312378 and perplexity is 95.3921674910153
At time: 265.17038011550903 and batch: 100, loss is 4.533792276382446 and perplexity is 93.11099502365636
At time: 265.8547327518463 and batch: 150, loss is 4.5134897136688235 and perplexity is 91.2396638955096
At time: 266.53895926475525 and batch: 200, loss is 4.476933364868164 and perplexity is 87.96450359530306
At time: 267.2349944114685 and batch: 250, loss is 4.451832408905029 and perplexity is 85.7839914205011
At time: 267.9301269054413 and batch: 300, loss is 4.433777732849121 and perplexity is 84.24908703964935
At time: 268.63002848625183 and batch: 350, loss is 4.492676858901977 and perplexity is 89.36033100265567
At time: 269.3309943675995 and batch: 400, loss is 4.46184591293335 and perplexity is 86.64730494718344
At time: 270.0302391052246 and batch: 450, loss is 4.415675773620605 and perplexity is 82.73773399650662
At time: 270.7309012413025 and batch: 500, loss is 4.45268609046936 and perplexity is 85.85725489988295
At time: 271.42954540252686 and batch: 550, loss is 4.494940338134765 and perplexity is 89.56282534052558
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.988609800947473 and perplexity of 146.73229454178443
Finished 31 epochs...
Completing Train Step...
At time: 273.23705315589905 and batch: 50, loss is 4.556624689102173 and perplexity is 95.26139973495187
At time: 273.95615553855896 and batch: 100, loss is 4.53267427444458 and perplexity is 93.00695492013885
At time: 274.6486577987671 and batch: 150, loss is 4.51279800415039 and perplexity is 91.17657437386124
At time: 275.34746408462524 and batch: 200, loss is 4.476111984252929 and perplexity is 87.8922809224334
At time: 276.04153299331665 and batch: 250, loss is 4.451098823547364 and perplexity is 85.72108461704894
At time: 276.7331051826477 and batch: 300, loss is 4.43305742263794 and perplexity is 84.1884234129098
At time: 277.4277513027191 and batch: 350, loss is 4.49227668762207 and perplexity is 89.32457871862111
At time: 278.1230866909027 and batch: 400, loss is 4.4615500831604 and perplexity is 86.62167588574086
At time: 278.8178632259369 and batch: 450, loss is 4.4154376029968265 and perplexity is 82.71803064526351
At time: 279.5145196914673 and batch: 500, loss is 4.452550096511841 and perplexity is 85.84557962590873
At time: 280.2146956920624 and batch: 550, loss is 4.494756326675415 and perplexity is 89.54634627054654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.98862376111619 and perplexity of 146.73434296367057
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 282.0918321609497 and batch: 50, loss is 4.555054426193237 and perplexity is 95.11193167508229
At time: 282.7918622493744 and batch: 100, loss is 4.529867296218872 and perplexity is 92.74625248704076
At time: 283.4744575023651 and batch: 150, loss is 4.510478506088257 and perplexity is 90.96533556499962
At time: 284.1650047302246 and batch: 200, loss is 4.47196668624878 and perplexity is 87.52869533169641
At time: 284.85984110832214 and batch: 250, loss is 4.44531286239624 and perplexity is 85.22653784550168
At time: 285.55494689941406 and batch: 300, loss is 4.427482442855835 and perplexity is 83.72038053154199
At time: 286.2501149177551 and batch: 350, loss is 4.485313673019409 and perplexity is 88.70477074282667
At time: 286.94705033302307 and batch: 400, loss is 4.454032249450684 and perplexity is 85.97291024245196
At time: 287.6376905441284 and batch: 450, loss is 4.4070045948028564 and perplexity is 82.02340183683495
At time: 288.33307814598083 and batch: 500, loss is 4.443990926742554 and perplexity is 85.11394828095928
At time: 289.03626775741577 and batch: 550, loss is 4.486399078369141 and perplexity is 88.80110364620452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.98614047436004 and perplexity of 146.3704115731393
Finished 33 epochs...
Completing Train Step...
At time: 290.8218502998352 and batch: 50, loss is 4.553321619033813 and perplexity is 94.94726374901259
At time: 291.51868891716003 and batch: 100, loss is 4.5290561962127684 and perplexity is 92.67105650093735
At time: 292.20382833480835 and batch: 150, loss is 4.50918872833252 and perplexity is 90.84808612775672
At time: 292.889271736145 and batch: 200, loss is 4.471081342697143 and perplexity is 87.45123665952887
At time: 293.57561898231506 and batch: 250, loss is 4.444899253845215 and perplexity is 85.19129470960809
At time: 294.26372718811035 and batch: 300, loss is 4.4273270416259765 and perplexity is 83.70737129229524
At time: 294.9501419067383 and batch: 350, loss is 4.484913196563721 and perplexity is 88.66925368298476
At time: 295.6395411491394 and batch: 400, loss is 4.453812379837036 and perplexity is 85.95400948981948
At time: 296.3299403190613 and batch: 450, loss is 4.406780529022217 and perplexity is 82.00502525812976
At time: 297.0198242664337 and batch: 500, loss is 4.444100465774536 and perplexity is 85.12327209111338
At time: 297.70852065086365 and batch: 550, loss is 4.486650619506836 and perplexity is 88.82344358643347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.985896658390127 and perplexity of 146.33472847950583
Finished 34 epochs...
Completing Train Step...
At time: 299.5290367603302 and batch: 50, loss is 4.552698411941528 and perplexity is 94.88811037516676
At time: 300.2370731830597 and batch: 100, loss is 4.52863920211792 and perplexity is 92.63242127350377
At time: 300.92605662345886 and batch: 150, loss is 4.508562660217285 and perplexity is 90.79122683845328
At time: 301.61681056022644 and batch: 200, loss is 4.470569581985473 and perplexity is 87.40649400216317
At time: 302.30159068107605 and batch: 250, loss is 4.444648704528809 and perplexity is 85.16995276267161
At time: 302.9863154888153 and batch: 300, loss is 4.427282667160034 and perplexity is 83.70365690481127
At time: 303.6859905719757 and batch: 350, loss is 4.484591045379639 and perplexity is 88.64069337853408
At time: 304.391818523407 and batch: 400, loss is 4.453662490844726 and perplexity is 85.94112689545575
At time: 305.09832668304443 and batch: 450, loss is 4.406559410095215 and perplexity is 81.98689439954784
At time: 305.80374550819397 and batch: 500, loss is 4.444080200195312 and perplexity is 85.12154703617871
At time: 306.5010356903076 and batch: 550, loss is 4.486754741668701 and perplexity is 88.83269255690702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.985743745844415 and perplexity of 146.31235377437815
Finished 35 epochs...
Completing Train Step...
At time: 308.30314087867737 and batch: 50, loss is 4.552252597808838 and perplexity is 94.84581734265353
At time: 309.00455832481384 and batch: 100, loss is 4.528341665267944 and perplexity is 92.60486381455549
At time: 309.6899137496948 and batch: 150, loss is 4.50810206413269 and perplexity is 90.74941838400031
At time: 310.37712025642395 and batch: 200, loss is 4.4701837635040285 and perplexity is 87.37277746602845
At time: 311.08016633987427 and batch: 250, loss is 4.444435796737671 and perplexity is 85.15182134638599
At time: 311.7713418006897 and batch: 300, loss is 4.4272481727600095 and perplexity is 83.70076964718388
At time: 312.46576738357544 and batch: 350, loss is 4.484300909042358 and perplexity is 88.61497922290906
At time: 313.1711280345917 and batch: 400, loss is 4.453529481887817 and perplexity is 85.9296967159868
At time: 313.87406277656555 and batch: 450, loss is 4.4063348293304445 and perplexity is 81.96848378751446
At time: 314.5705494880676 and batch: 500, loss is 4.444015035629272 and perplexity is 85.11600030823246
At time: 315.2659146785736 and batch: 550, loss is 4.486784448623657 and perplexity is 88.83533154490142
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.985651543799867 and perplexity of 146.29886409811496
Finished 36 epochs...
Completing Train Step...
At time: 317.0654082298279 and batch: 50, loss is 4.551879396438599 and perplexity is 94.81042735786595
At time: 317.77351665496826 and batch: 100, loss is 4.528079681396484 and perplexity is 92.58060601153264
At time: 318.4706892967224 and batch: 150, loss is 4.507703495025635 and perplexity is 90.71325567649694
At time: 319.18994545936584 and batch: 200, loss is 4.4698390197753906 and perplexity is 87.34266144039712
At time: 319.89262199401855 and batch: 250, loss is 4.444225311279297 and perplexity is 85.13390001239479
At time: 320.592178106308 and batch: 300, loss is 4.427207698822022 and perplexity is 83.6973820159795
At time: 321.2922863960266 and batch: 350, loss is 4.484019184112549 and perplexity is 88.59001769041556
At time: 321.99276185035706 and batch: 400, loss is 4.453402576446533 and perplexity is 85.91879246182484
At time: 322.69243335723877 and batch: 450, loss is 4.40610975265503 and perplexity is 81.9500366697806
At time: 323.408842086792 and batch: 500, loss is 4.443907089233399 and perplexity is 85.10681283865405
At time: 324.1057095527649 and batch: 550, loss is 4.486779527664185 and perplexity is 88.83489439091075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.985587262092753 and perplexity of 146.28946005963888
Finished 37 epochs...
Completing Train Step...
At time: 325.96478056907654 and batch: 50, loss is 4.551555242538452 and perplexity is 94.77969916866357
At time: 326.66113781929016 and batch: 100, loss is 4.527873086929321 and perplexity is 92.56148134615711
At time: 327.35360646247864 and batch: 150, loss is 4.507331104278564 and perplexity is 90.67948118849007
At time: 328.04805421829224 and batch: 200, loss is 4.469529542922974 and perplexity is 87.31563509068337
At time: 328.74270009994507 and batch: 250, loss is 4.444020977020264 and perplexity is 85.11650601717224
At time: 329.4402561187744 and batch: 300, loss is 4.427164039611816 and perplexity is 83.69372793415218
At time: 330.13889145851135 and batch: 350, loss is 4.483743181228638 and perplexity is 88.56556996402395
At time: 330.8538234233856 and batch: 400, loss is 4.453258600234985 and perplexity is 85.90642309005445
At time: 331.56390738487244 and batch: 450, loss is 4.4058760643005375 and perplexity is 81.93088813804225
At time: 332.2860836982727 and batch: 500, loss is 4.443763427734375 and perplexity is 85.0945871445463
At time: 333.00275588035583 and batch: 550, loss is 4.4867662525177 and perplexity is 88.83371510250242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9855333693484045 and perplexity of 146.28157633160686
Finished 38 epochs...
Completing Train Step...
At time: 334.85108733177185 and batch: 50, loss is 4.551268110275268 and perplexity is 94.75248876581662
At time: 335.5623140335083 and batch: 100, loss is 4.527685737609863 and perplexity is 92.54414163996067
At time: 336.2586500644684 and batch: 150, loss is 4.506981229782104 and perplexity is 90.6477603001585
At time: 336.9540343284607 and batch: 200, loss is 4.469250726699829 and perplexity is 87.29129346866314
At time: 337.64913392066956 and batch: 250, loss is 4.44380464553833 and perplexity is 85.09809462884182
At time: 338.342764377594 and batch: 300, loss is 4.427103538513183 and perplexity is 83.68866452483591
At time: 339.0365011692047 and batch: 350, loss is 4.483491897583008 and perplexity is 88.54331768066062
At time: 339.73764276504517 and batch: 400, loss is 4.453104295730591 and perplexity is 85.89316836467346
At time: 340.439510345459 and batch: 450, loss is 4.405632514953613 and perplexity is 81.91093635346395
At time: 341.1434941291809 and batch: 500, loss is 4.443633346557617 and perplexity is 85.08351866043118
At time: 341.8463990688324 and batch: 550, loss is 4.486717481613159 and perplexity is 88.82938270751134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.98550187780502 and perplexity of 146.27696977153366
Finished 39 epochs...
Completing Train Step...
At time: 343.68683648109436 and batch: 50, loss is 4.550987491607666 and perplexity is 94.72590317904756
At time: 344.3947877883911 and batch: 100, loss is 4.527516098022461 and perplexity is 92.52844382147974
At time: 345.0886745452881 and batch: 150, loss is 4.506640195846558 and perplexity is 90.6168516084708
At time: 345.7891616821289 and batch: 200, loss is 4.468979043960571 and perplexity is 87.26758115219963
At time: 346.4910340309143 and batch: 250, loss is 4.443626499176025 and perplexity is 85.08293606310633
At time: 347.1954770088196 and batch: 300, loss is 4.427040929794312 and perplexity is 83.68342504878609
At time: 347.899516582489 and batch: 350, loss is 4.483242073059082 and perplexity is 88.5212001513399
At time: 348.60205483436584 and batch: 400, loss is 4.452951173782349 and perplexity is 85.88001724228123
At time: 349.3047652244568 and batch: 450, loss is 4.405386085510254 and perplexity is 81.89075357393106
At time: 350.0214502811432 and batch: 500, loss is 4.443506183624268 and perplexity is 85.07269987850724
At time: 350.73131942749023 and batch: 550, loss is 4.48667353630066 and perplexity is 88.82547915830114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.985465841090425 and perplexity of 146.271698525102
Finished 40 epochs...
Completing Train Step...
At time: 352.53469157218933 and batch: 50, loss is 4.550718803405761 and perplexity is 94.70045486543252
At time: 353.23921155929565 and batch: 100, loss is 4.5273405456542966 and perplexity is 92.51220165976103
At time: 353.9302725791931 and batch: 150, loss is 4.506323709487915 and perplexity is 90.58817714885055
At time: 354.62347531318665 and batch: 200, loss is 4.468711414337158 and perplexity is 87.24422888733801
At time: 355.317134141922 and batch: 250, loss is 4.443421592712403 and perplexity is 85.06550380561514
At time: 356.01667857170105 and batch: 300, loss is 4.426985082626342 and perplexity is 83.6787516969891
At time: 356.71408581733704 and batch: 350, loss is 4.482997026443481 and perplexity is 88.49951098837042
At time: 357.41091871261597 and batch: 400, loss is 4.452788467407227 and perplexity is 85.86604515268515
At time: 358.10940170288086 and batch: 450, loss is 4.405144834518433 and perplexity is 81.87099973132325
At time: 358.8076608181 and batch: 500, loss is 4.443364028930664 and perplexity is 85.06060725445394
At time: 359.5040214061737 and batch: 550, loss is 4.486605253219604 and perplexity is 88.8194140879812
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.98543986868351 and perplexity of 146.26789954636232
Finished 41 epochs...
Completing Train Step...
At time: 361.42269372940063 and batch: 50, loss is 4.550454244613648 and perplexity is 94.67540434129504
At time: 362.1470537185669 and batch: 100, loss is 4.5271748924255375 and perplexity is 92.4968779840997
At time: 362.85278940200806 and batch: 150, loss is 4.506016473770142 and perplexity is 90.56034950026519
At time: 363.55122566223145 and batch: 200, loss is 4.468474655151367 and perplexity is 87.22357545978252
At time: 364.2461130619049 and batch: 250, loss is 4.44325101852417 and perplexity is 85.05099506380049
At time: 364.94950246810913 and batch: 300, loss is 4.426922206878662 and perplexity is 83.67349049831383
At time: 365.6496846675873 and batch: 350, loss is 4.482753391265869 and perplexity is 88.47795202066028
At time: 366.3471086025238 and batch: 400, loss is 4.452631301879883 and perplexity is 85.85255103085098
At time: 367.05659008026123 and batch: 450, loss is 4.404894437789917 and perplexity is 81.8505020672113
At time: 367.7602355480194 and batch: 500, loss is 4.443239908218384 and perplexity is 85.05005012648724
At time: 368.4585063457489 and batch: 550, loss is 4.486527442932129 and perplexity is 88.81250329270658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.985417467482547 and perplexity of 146.2646230064494
Finished 42 epochs...
Completing Train Step...
At time: 370.2867794036865 and batch: 50, loss is 4.550194835662841 and perplexity is 94.65084787920843
At time: 370.9991502761841 and batch: 100, loss is 4.5270107650756835 and perplexity is 92.48169796240883
At time: 371.6954836845398 and batch: 150, loss is 4.505719356536865 and perplexity is 90.53344645665393
At time: 372.3858516216278 and batch: 200, loss is 4.4682223415374756 and perplexity is 87.20157054042879
At time: 373.0782701969147 and batch: 250, loss is 4.443053550720215 and perplexity is 85.03420188869136
At time: 373.77042841911316 and batch: 300, loss is 4.426842708587646 and perplexity is 83.66683886321619
At time: 374.46018838882446 and batch: 350, loss is 4.482496280670166 and perplexity is 88.45520632591447
At time: 375.14909505844116 and batch: 400, loss is 4.452463750839233 and perplexity is 85.83816755160042
At time: 375.8412058353424 and batch: 450, loss is 4.404634456634522 and perplexity is 81.82922524502045
At time: 376.53129506111145 and batch: 500, loss is 4.443107223510742 and perplexity is 85.03876603408105
At time: 377.2347185611725 and batch: 550, loss is 4.486426811218262 and perplexity is 88.803566387963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9853807814577795 and perplexity of 146.25925723729216
Finished 43 epochs...
Completing Train Step...
At time: 379.0340316295624 and batch: 50, loss is 4.5499427318573 and perplexity is 94.6269890478378
At time: 379.725949048996 and batch: 100, loss is 4.526845550537109 and perplexity is 92.46641990346687
At time: 380.406023979187 and batch: 150, loss is 4.505419902801513 and perplexity is 90.50633993671512
At time: 381.0892024040222 and batch: 200, loss is 4.467979326248169 and perplexity is 87.18038180023409
At time: 381.772438287735 and batch: 250, loss is 4.442892856597901 and perplexity is 85.02053849009546
At time: 382.4587049484253 and batch: 300, loss is 4.426767082214355 and perplexity is 83.66051168288212
At time: 383.16193175315857 and batch: 350, loss is 4.4822515201568605 and perplexity is 88.43355863356786
At time: 383.87137508392334 and batch: 400, loss is 4.45230390548706 and perplexity is 85.82444781602544
At time: 384.5772240161896 and batch: 450, loss is 4.404390449523926 and perplexity is 81.80926076804234
At time: 385.28293538093567 and batch: 500, loss is 4.442986078262329 and perplexity is 85.02846461564171
At time: 385.98845958709717 and batch: 550, loss is 4.486338005065918 and perplexity is 88.79568043508343
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.985364224048371 and perplexity of 146.25683558293846
Finished 44 epochs...
Completing Train Step...
At time: 387.84315252304077 and batch: 50, loss is 4.549716987609863 and perplexity is 94.60562996034429
At time: 388.5574679374695 and batch: 100, loss is 4.526698207855224 and perplexity is 92.45279665684151
At time: 389.25846004486084 and batch: 150, loss is 4.505151538848877 and perplexity is 90.48205455639715
At time: 389.94578862190247 and batch: 200, loss is 4.467748279571533 and perplexity is 87.16024138952841
At time: 390.6489109992981 and batch: 250, loss is 4.442708797454834 and perplexity is 85.00489112270262
At time: 391.34970712661743 and batch: 300, loss is 4.4266821384429935 and perplexity is 83.65340554532105
At time: 392.04897451400757 and batch: 350, loss is 4.482015132904053 and perplexity is 88.41265653817824
At time: 392.7470622062683 and batch: 400, loss is 4.4521325492858885 and perplexity is 85.80974252463744
At time: 393.4442331790924 and batch: 450, loss is 4.404156045913696 and perplexity is 81.79008662929957
At time: 394.143522977829 and batch: 500, loss is 4.442854700088501 and perplexity is 85.0172944650101
At time: 394.84187388420105 and batch: 550, loss is 4.4862518310546875 and perplexity is 88.78802888480749
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9853502638796545 and perplexity of 146.25479382708951
Finished 45 epochs...
Completing Train Step...
At time: 396.6582477092743 and batch: 50, loss is 4.549486694335937 and perplexity is 94.58384542860071
At time: 397.36761236190796 and batch: 100, loss is 4.52653549194336 and perplexity is 92.43775433957445
At time: 398.064977645874 and batch: 150, loss is 4.50488112449646 and perplexity is 90.4575902181121
At time: 398.76297903060913 and batch: 200, loss is 4.467532033920288 and perplexity is 87.14139540412108
At time: 399.46000838279724 and batch: 250, loss is 4.442555446624755 and perplexity is 84.9918565515449
At time: 400.1571819782257 and batch: 300, loss is 4.4265993785858155 and perplexity is 83.64648268789696
At time: 400.8579866886139 and batch: 350, loss is 4.481771059036255 and perplexity is 88.3910799523822
At time: 401.5589933395386 and batch: 400, loss is 4.4519748115539555 and perplexity is 85.79620815794208
At time: 402.26406955718994 and batch: 450, loss is 4.403926677703858 and perplexity is 81.77132873486143
At time: 402.96619057655334 and batch: 500, loss is 4.442730283737182 and perplexity is 85.00671758141608
At time: 403.6694276332855 and batch: 550, loss is 4.486171865463257 and perplexity is 88.78092918143555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.985338900951629 and perplexity of 146.2531319538358
Finished 46 epochs...
Completing Train Step...
At time: 405.5100724697113 and batch: 50, loss is 4.549262180328369 and perplexity is 94.5626124140561
At time: 406.22645115852356 and batch: 100, loss is 4.52636836051941 and perplexity is 92.42230637702086
At time: 406.9282982349396 and batch: 150, loss is 4.50462124824524 and perplexity is 90.43408549296397
At time: 407.64063453674316 and batch: 200, loss is 4.467305488586426 and perplexity is 87.12165616360689
At time: 408.35080742836 and batch: 250, loss is 4.4423699378967285 and perplexity is 84.97609128268607
At time: 409.0542325973511 and batch: 300, loss is 4.426501932144165 and perplexity is 83.6383320329351
At time: 409.7545893192291 and batch: 350, loss is 4.481535720825195 and perplexity is 88.37028060128958
At time: 410.4549629688263 and batch: 400, loss is 4.4518035125732425 and perplexity is 85.78151261363739
At time: 411.1495461463928 and batch: 450, loss is 4.403695907592773 and perplexity is 81.75246053343797
At time: 411.85320568084717 and batch: 500, loss is 4.442593059539795 and perplexity is 84.99505340314549
At time: 412.5576674938202 and batch: 550, loss is 4.48606840133667 and perplexity is 88.77174401531599
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.985335005090592 and perplexity of 146.25256217306733
Finished 47 epochs...
Completing Train Step...
At time: 414.39176630973816 and batch: 50, loss is 4.549031963348389 and perplexity is 94.54084500071734
At time: 415.09622526168823 and batch: 100, loss is 4.526214542388916 and perplexity is 92.40809124393871
At time: 415.79027915000916 and batch: 150, loss is 4.504366273880005 and perplexity is 90.41103005881708
At time: 416.49214482307434 and batch: 200, loss is 4.467077026367187 and perplexity is 87.10175443017965
At time: 417.1894884109497 and batch: 250, loss is 4.44221863746643 and perplexity is 84.96323533608958
At time: 417.8923578262329 and batch: 300, loss is 4.426402378082275 and perplexity is 83.63000591170797
At time: 418.5947377681732 and batch: 350, loss is 4.4812975597381595 and perplexity is 88.3492367452132
At time: 419.30377769470215 and batch: 400, loss is 4.4516564464569095 and perplexity is 85.76889798733892
At time: 420.0083405971527 and batch: 450, loss is 4.403474617004394 and perplexity is 81.734371484887
At time: 420.7098617553711 and batch: 500, loss is 4.4424534034729 and perplexity is 84.98318415710708
At time: 421.4107904434204 and batch: 550, loss is 4.485973091125488 and perplexity is 88.76328356483697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.985334355780419 and perplexity of 146.25246720982176
Finished 48 epochs...
Completing Train Step...
At time: 423.22777938842773 and batch: 50, loss is 4.548809833526612 and perplexity is 94.51984699189498
At time: 423.94231963157654 and batch: 100, loss is 4.526068964004517 and perplexity is 92.39463960246782
At time: 424.63937282562256 and batch: 150, loss is 4.504116621017456 and perplexity is 90.3884615036263
At time: 425.3319547176361 and batch: 200, loss is 4.4668569755554195 and perplexity is 87.08258972709245
At time: 426.0228600502014 and batch: 250, loss is 4.442031211853028 and perplexity is 84.9473125418064
At time: 426.71771240234375 and batch: 300, loss is 4.426300392150879 and perplexity is 83.62147726257092
At time: 427.4129478931427 and batch: 350, loss is 4.481075954437256 and perplexity is 88.3296602552268
At time: 428.10947370529175 and batch: 400, loss is 4.451508150100708 and perplexity is 85.756179715352
At time: 428.80683946609497 and batch: 450, loss is 4.403248176574707 and perplexity is 81.71586561400552
At time: 429.5116217136383 and batch: 500, loss is 4.4423121738433835 and perplexity is 84.9711828609827
At time: 430.2021493911743 and batch: 550, loss is 4.4858751964569095 and perplexity is 88.75459453792209
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.985339550261802 and perplexity of 146.25322691751302
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 431.99278259277344 and batch: 50, loss is 4.5486556339263915 and perplexity is 94.50527319294183
At time: 432.68977642059326 and batch: 100, loss is 4.525714769363403 and perplexity is 92.36191971120014
At time: 433.38276290893555 and batch: 150, loss is 4.503096885681153 and perplexity is 90.29633617514651
At time: 434.0797030925751 and batch: 200, loss is 4.465884189605713 and perplexity is 86.99791819765615
At time: 434.7775893211365 and batch: 250, loss is 4.440250310897827 and perplexity is 84.79616442162335
At time: 435.4782807826996 and batch: 300, loss is 4.424692449569702 and perplexity is 83.48712677146389
At time: 436.17773485183716 and batch: 350, loss is 4.478720407485962 and perplexity is 88.12184045404575
At time: 436.872309923172 and batch: 400, loss is 4.4489452838897705 and perplexity is 85.53667949497442
At time: 437.56685733795166 and batch: 450, loss is 4.4006584358215335 and perplexity is 81.5045164944803
At time: 438.2754466533661 and batch: 500, loss is 4.439082736968994 and perplexity is 84.69721640658169
At time: 438.9736018180847 and batch: 550, loss is 4.483023986816407 and perplexity is 88.5018970003541
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.985054178440825 and perplexity of 146.2114963224749
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efc55b01898>
SETTINGS FOR THIS RUN
{'dropout': 0.8761539628884351, 'wordvec_dim': 200, 'tune_wordvecs': True, 'anneal': 4.520964925982288, 'num_layers': 1, 'seq_len': 35, 'data': 'ptb', 'lr': 18.63581596300943, 'wordvec_source': '', 'batch_size': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.9312574863433838 and batch: 50, loss is 7.185418167114258 and perplexity is 1320.0411206651531
At time: 1.606705665588379 and batch: 100, loss is 6.5737885475158695 and perplexity is 716.0776046828836
At time: 2.286144495010376 and batch: 150, loss is 6.465220413208008 and perplexity is 642.405942597159
At time: 2.9612112045288086 and batch: 200, loss is 6.370327291488647 and perplexity is 584.2490173330368
At time: 3.631092071533203 and batch: 250, loss is 6.2999979019165036 and perplexity is 544.5707675697904
At time: 4.324948072433472 and batch: 300, loss is 6.2600829219818115 and perplexity is 523.2623282583266
At time: 4.996998310089111 and batch: 350, loss is 6.309535903930664 and perplexity is 549.7897343399349
At time: 5.667981386184692 and batch: 400, loss is 6.282552747726441 and perplexity is 535.1530323269254
At time: 6.333888292312622 and batch: 450, loss is 6.186752786636353 and perplexity is 486.2645350581603
At time: 7.000368118286133 and batch: 500, loss is 6.17655608177185 and perplexity is 481.3314325455865
At time: 7.671616792678833 and batch: 550, loss is 6.228349361419678 and perplexity is 506.91805404218655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.990212947764295 and perplexity of 399.4996734320408
Finished 1 epochs...
Completing Train Step...
At time: 9.430668592453003 and batch: 50, loss is 5.690289630889892 and perplexity is 295.9793329850156
At time: 10.097781658172607 and batch: 100, loss is 5.453407716751099 and perplexity is 233.5526928010101
At time: 10.7540123462677 and batch: 150, loss is 5.34393343925476 and perplexity is 209.3344975344152
At time: 11.420730590820312 and batch: 200, loss is 5.2251364040374755 and perplexity is 185.8865244691745
At time: 12.082117795944214 and batch: 250, loss is 5.194612903594971 and perplexity is 180.29833644911602
At time: 12.750249147415161 and batch: 300, loss is 5.158342018127441 and perplexity is 173.87593334330668
At time: 13.407503366470337 and batch: 350, loss is 5.203857889175415 and perplexity is 181.9729207966631
At time: 14.064612627029419 and batch: 400, loss is 5.187562437057495 and perplexity is 179.03161978023437
At time: 14.72169542312622 and batch: 450, loss is 5.1162001991271975 and perplexity is 166.70073504367443
At time: 15.379835605621338 and batch: 500, loss is 5.14253791809082 and perplexity is 171.14958121608058
At time: 16.04321026802063 and batch: 550, loss is 5.178601446151734 and perplexity is 177.43448570290025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.278137531686337 and perplexity of 196.00448305260966
Finished 2 epochs...
Completing Train Step...
At time: 17.806957006454468 and batch: 50, loss is 5.162549171447754 and perplexity is 174.60899702797485
At time: 18.47399663925171 and batch: 100, loss is 5.128398599624634 and perplexity is 168.74667057374427
At time: 19.137946605682373 and batch: 150, loss is 5.114955577850342 and perplexity is 166.49338482498544
At time: 19.814465045928955 and batch: 200, loss is 5.053257989883423 and perplexity is 156.53161304734576
At time: 20.47918152809143 and batch: 250, loss is 5.0565778923034665 and perplexity is 157.0521463100353
At time: 21.160627603530884 and batch: 300, loss is 5.049859542846679 and perplexity is 156.00055155412923
At time: 21.855865001678467 and batch: 350, loss is 5.0913803768157955 and perplexity is 162.61417603748146
At time: 22.565853118896484 and batch: 400, loss is 5.086741142272949 and perplexity is 161.8615179634428
At time: 23.273574829101562 and batch: 450, loss is 5.027667140960693 and perplexity is 152.5766573595368
At time: 23.973201274871826 and batch: 500, loss is 5.063971796035767 and perplexity is 158.21767836676827
At time: 24.669334173202515 and batch: 550, loss is 5.0932136821746825 and perplexity is 162.91257091873376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.254121820977393 and perplexity of 191.35336953203938
Finished 3 epochs...
Completing Train Step...
At time: 26.514880180358887 and batch: 50, loss is 5.087681074142456 and perplexity is 162.01372828508335
At time: 27.201570749282837 and batch: 100, loss is 5.068627605438232 and perplexity is 158.95602719331086
At time: 27.896096229553223 and batch: 150, loss is 5.042729759216309 and perplexity is 154.89225702034142
At time: 28.592383861541748 and batch: 200, loss is 4.995212154388428 and perplexity is 147.70427817046453
At time: 29.293623447418213 and batch: 250, loss is 4.991964235305786 and perplexity is 147.22532484866255
At time: 29.997625827789307 and batch: 300, loss is 4.984229221343994 and perplexity is 146.0909278496918
At time: 30.697383403778076 and batch: 350, loss is 5.028742446899414 and perplexity is 152.74081218781907
At time: 31.39508843421936 and batch: 400, loss is 5.010551700592041 and perplexity is 149.9874614984437
At time: 32.093255043029785 and batch: 450, loss is 4.9316411972045895 and perplexity is 138.6068068753448
At time: 32.7930645942688 and batch: 500, loss is 5.009087972640991 and perplexity is 149.76808125440746
At time: 33.49087381362915 and batch: 550, loss is 5.039952297210693 and perplexity is 154.4626465510596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.211049830659907 and perplexity of 183.28637688302143
Finished 4 epochs...
Completing Train Step...
At time: 35.31216239929199 and batch: 50, loss is 5.029932584762573 and perplexity is 152.9227030277717
At time: 36.016348361968994 and batch: 100, loss is 5.036996355056763 and perplexity is 154.00673805444538
At time: 36.70799279212952 and batch: 150, loss is 5.019481248855591 and perplexity is 151.33277937600994
At time: 37.413270235061646 and batch: 200, loss is 4.954197616577148 and perplexity is 141.7688077827704
At time: 38.114391803741455 and batch: 250, loss is 4.9597360801696775 and perplexity is 142.55616753222515
At time: 38.816245317459106 and batch: 300, loss is 4.938808145523072 and perplexity is 139.60376299590993
At time: 39.51791596412659 and batch: 350, loss is 4.978688850402832 and perplexity is 145.28376796554392
At time: 40.22043180465698 and batch: 400, loss is 4.9686611461639405 and perplexity is 143.8341854489064
At time: 40.923758029937744 and batch: 450, loss is 4.913601703643799 and perplexity is 136.12882821425958
At time: 41.65220808982849 and batch: 500, loss is 4.9427744483947755 and perplexity is 140.15857334759107
At time: 42.359041690826416 and batch: 550, loss is 5.0182758235931395 and perplexity is 151.15046892360107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.280817884079953 and perplexity of 196.5305488438199
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 44.19243574142456 and batch: 50, loss is 4.9932949924468994 and perplexity is 147.42137642063986
At time: 44.90382933616638 and batch: 100, loss is 4.9133357810974125 and perplexity is 136.0926333023591
At time: 45.603320837020874 and batch: 150, loss is 4.877799415588379 and perplexity is 131.34131798060093
At time: 46.303398847579956 and batch: 200, loss is 4.805295419692993 and perplexity is 122.1555728531075
At time: 47.00322937965393 and batch: 250, loss is 4.788320989608764 and perplexity is 120.09955088963119
At time: 47.704814434051514 and batch: 300, loss is 4.757335729598999 and perplexity is 116.43529716739306
At time: 48.40680932998657 and batch: 350, loss is 4.82820984840393 and perplexity is 124.98701456223675
At time: 49.109938859939575 and batch: 400, loss is 4.785795860290527 and perplexity is 119.79666656455682
At time: 49.81959795951843 and batch: 450, loss is 4.705986423492432 and perplexity is 110.60733682844358
At time: 50.53838610649109 and batch: 500, loss is 4.719490985870362 and perplexity is 112.11117197097751
At time: 51.24419045448303 and batch: 550, loss is 4.7781086444854735 and perplexity is 118.87929427287975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.098774362117686 and perplexity of 163.82099898235816
Finished 6 epochs...
Completing Train Step...
At time: 53.06924629211426 and batch: 50, loss is 4.838699865341186 and perplexity is 126.3050313850872
At time: 53.77375078201294 and batch: 100, loss is 4.814436502456665 and perplexity is 123.2773262639104
At time: 54.46383309364319 and batch: 150, loss is 4.810517606735229 and perplexity is 122.79516067288166
At time: 55.156365156173706 and batch: 200, loss is 4.748343095779419 and perplexity is 115.39293100932484
At time: 55.86817932128906 and batch: 250, loss is 4.7390796661376955 and perplexity is 114.32893245819184
At time: 56.57247185707092 and batch: 300, loss is 4.71132643699646 and perplexity is 111.19956133687879
At time: 57.27390384674072 and batch: 350, loss is 4.779055881500244 and perplexity is 118.99195449024504
At time: 57.97612500190735 and batch: 400, loss is 4.743253030776978 and perplexity is 114.80706580058232
At time: 58.67927002906799 and batch: 450, loss is 4.66471646308899 and perplexity is 106.13548762697691
At time: 59.39034843444824 and batch: 500, loss is 4.689387168884277 and perplexity is 108.7864916281674
At time: 60.08881735801697 and batch: 550, loss is 4.748203573226928 and perplexity is 115.37683221614948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.086907569398272 and perplexity of 161.8884583523214
Finished 7 epochs...
Completing Train Step...
At time: 61.90423560142517 and batch: 50, loss is 4.794159851074219 and perplexity is 120.80284675482187
At time: 62.610050439834595 and batch: 100, loss is 4.777289762496948 and perplexity is 118.78198600742168
At time: 63.312811613082886 and batch: 150, loss is 4.767183198928833 and perplexity is 117.58755428201985
At time: 64.01321625709534 and batch: 200, loss is 4.703251810073852 and perplexity is 110.30528171118105
At time: 64.7098274230957 and batch: 250, loss is 4.690910863876343 and perplexity is 108.9523754067133
At time: 65.41183543205261 and batch: 300, loss is 4.667472763061523 and perplexity is 106.42843240498672
At time: 66.12730002403259 and batch: 350, loss is 4.732009620666504 and perplexity is 113.52347238183262
At time: 66.83840084075928 and batch: 400, loss is 4.700680713653565 and perplexity is 110.02204047253339
At time: 67.55044221878052 and batch: 450, loss is 4.629707326889038 and perplexity is 102.48406539055641
At time: 68.24826669692993 and batch: 500, loss is 4.658862819671631 and perplexity is 105.5160231593681
At time: 68.94546151161194 and batch: 550, loss is 4.715921926498413 and perplexity is 111.71175373963379
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.083148712807513 and perplexity of 161.28108508331502
Finished 8 epochs...
Completing Train Step...
At time: 70.7821261882782 and batch: 50, loss is 4.754722394943237 and perplexity is 116.13141002250536
At time: 71.48959827423096 and batch: 100, loss is 4.7407966995239255 and perplexity is 114.5254076812226
At time: 72.1854817867279 and batch: 150, loss is 4.7354112720489505 and perplexity is 113.91029720719818
At time: 72.88013768196106 and batch: 200, loss is 4.676168336868286 and perplexity is 107.35792406898852
At time: 73.57291674613953 and batch: 250, loss is 4.660843534469604 and perplexity is 105.72522742646059
At time: 74.26741147041321 and batch: 300, loss is 4.637622623443604 and perplexity is 103.29847605919166
At time: 74.96333336830139 and batch: 350, loss is 4.6978786754608155 and perplexity is 109.71418602447025
At time: 75.66258764266968 and batch: 400, loss is 4.672162818908691 and perplexity is 106.92876006189276
At time: 76.36455917358398 and batch: 450, loss is 4.600497903823853 and perplexity is 99.5338615966419
At time: 77.08934283256531 and batch: 500, loss is 4.631062593460083 and perplexity is 102.62305277965237
At time: 77.78946995735168 and batch: 550, loss is 4.687681102752686 and perplexity is 108.6010529095637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.080609585376496 and perplexity of 160.8720913194558
Finished 9 epochs...
Completing Train Step...
At time: 79.59289455413818 and batch: 50, loss is 4.724287395477295 and perplexity is 112.65019472685032
At time: 80.3158929347992 and batch: 100, loss is 4.714211215972901 and perplexity is 111.52081063744204
At time: 81.0114004611969 and batch: 150, loss is 4.703977975845337 and perplexity is 110.38541072112474
At time: 81.70697259902954 and batch: 200, loss is 4.650728063583374 and perplexity is 104.6611578222347
At time: 82.40444779396057 and batch: 250, loss is 4.6390135383605955 and perplexity is 103.44225541969338
At time: 83.10961318016052 and batch: 300, loss is 4.616878271102905 and perplexity is 101.17768930173936
At time: 83.81322932243347 and batch: 350, loss is 4.674759283065796 and perplexity is 107.20675750377433
At time: 84.51806569099426 and batch: 400, loss is 4.651282291412354 and perplexity is 104.71918002578778
At time: 85.2199010848999 and batch: 450, loss is 4.581539554595947 and perplexity is 97.66463856584691
At time: 85.91791129112244 and batch: 500, loss is 4.613362016677857 and perplexity is 100.82254755430206
At time: 86.61918926239014 and batch: 550, loss is 4.663758478164673 and perplexity is 106.03386011647899
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.079599583402593 and perplexity of 160.70969221519056
Finished 10 epochs...
Completing Train Step...
At time: 88.45105147361755 and batch: 50, loss is 4.700666875839233 and perplexity is 110.02051801849868
At time: 89.15613794326782 and batch: 100, loss is 4.691312036514282 and perplexity is 108.99609288710714
At time: 89.8463499546051 and batch: 150, loss is 4.6855373954772945 and perplexity is 108.36849340114162
At time: 90.53374290466309 and batch: 200, loss is 4.631362628936768 and perplexity is 102.65384795580363
At time: 91.22662329673767 and batch: 250, loss is 4.61560435295105 and perplexity is 101.04887927091345
At time: 91.9286162853241 and batch: 300, loss is 4.594669799804688 and perplexity is 98.95545504182813
At time: 92.63011288642883 and batch: 350, loss is 4.653015775680542 and perplexity is 104.90086650677145
At time: 93.33055400848389 and batch: 400, loss is 4.628123846054077 and perplexity is 102.32191225419547
At time: 94.03454613685608 and batch: 450, loss is 4.561647434234619 and perplexity is 95.74107710693315
At time: 94.75337529182434 and batch: 500, loss is 4.596519231796265 and perplexity is 99.1386357640549
At time: 95.46349763870239 and batch: 550, loss is 4.646951551437378 and perplexity is 104.2666490908878
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.084482395902593 and perplexity of 161.49632644002185
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 97.2578227519989 and batch: 50, loss is 4.66597734451294 and perplexity is 106.26939629548305
At time: 97.96332740783691 and batch: 100, loss is 4.631338729858398 and perplexity is 102.65139465276233
At time: 98.65474700927734 and batch: 150, loss is 4.600226917266846 and perplexity is 99.50689291242267
At time: 99.34720134735107 and batch: 200, loss is 4.541690378189087 and perplexity is 93.84930693500313
At time: 100.03885245323181 and batch: 250, loss is 4.50904411315918 and perplexity is 90.83494906596592
At time: 100.7357268333435 and batch: 300, loss is 4.4812061309814455 and perplexity is 88.34115945359495
At time: 101.43773818016052 and batch: 350, loss is 4.534560298919677 and perplexity is 93.1825338344916
At time: 102.1418662071228 and batch: 400, loss is 4.4959659767150875 and perplexity is 89.65473155277941
At time: 102.84430837631226 and batch: 450, loss is 4.421422758102417 and perplexity is 83.21459541495659
At time: 103.54566240310669 and batch: 500, loss is 4.449244709014892 and perplexity is 85.5622951607303
At time: 104.24841094017029 and batch: 550, loss is 4.508034143447876 and perplexity is 90.74325483067592
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9937685702709445 and perplexity of 147.49120844944784
Finished 12 epochs...
Completing Train Step...
At time: 106.07707023620605 and batch: 50, loss is 4.604808206558228 and perplexity is 99.96380860767859
At time: 106.78226327896118 and batch: 100, loss is 4.591238241195679 and perplexity is 98.61646556195512
At time: 107.47904014587402 and batch: 150, loss is 4.567726163864136 and perplexity is 96.32483368076677
At time: 108.17725586891174 and batch: 200, loss is 4.513704195022583 and perplexity is 91.25923520090345
At time: 108.87277579307556 and batch: 250, loss is 4.489079399108887 and perplexity is 89.03943835005923
At time: 109.569171667099 and batch: 300, loss is 4.461354570388794 and perplexity is 86.60474189726672
At time: 110.26601815223694 and batch: 350, loss is 4.519696636199951 and perplexity is 91.80774260754353
At time: 110.96311593055725 and batch: 400, loss is 4.491101398468017 and perplexity is 89.2196581781267
At time: 111.6595778465271 and batch: 450, loss is 4.426721572875977 and perplexity is 83.65670443498033
At time: 112.36653304100037 and batch: 500, loss is 4.45695818901062 and perplexity is 86.22483015310303
At time: 113.07111406326294 and batch: 550, loss is 4.513524398803711 and perplexity is 91.24282861044288
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.99121548267121 and perplexity of 147.11513075801187
Finished 13 epochs...
Completing Train Step...
At time: 114.89964199066162 and batch: 50, loss is 4.590296573638916 and perplexity is 98.52364534552495
At time: 115.61635971069336 and batch: 100, loss is 4.578291511535644 and perplexity is 97.34793422741056
At time: 116.32652640342712 and batch: 150, loss is 4.554677848815918 and perplexity is 95.07612141639014
At time: 117.03815484046936 and batch: 200, loss is 4.502107353210449 and perplexity is 90.2070292118729
At time: 117.73914337158203 and batch: 250, loss is 4.479512996673584 and perplexity is 88.19171255826235
At time: 118.4380874633789 and batch: 300, loss is 4.452198781967163 and perplexity is 85.81542612218222
At time: 119.1375584602356 and batch: 350, loss is 4.51248553276062 and perplexity is 91.14808875365462
At time: 119.83945870399475 and batch: 400, loss is 4.488630542755127 and perplexity is 88.99948140055677
At time: 120.54209923744202 and batch: 450, loss is 4.4286927890777585 and perplexity is 83.82177252514678
At time: 121.24356651306152 and batch: 500, loss is 4.458632841110229 and perplexity is 86.36934772050675
At time: 121.9456717967987 and batch: 550, loss is 4.513219156265259 and perplexity is 91.21498166807416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.989890565263464 and perplexity of 146.92034442672244
Finished 14 epochs...
Completing Train Step...
At time: 123.77687096595764 and batch: 50, loss is 4.579426298141479 and perplexity is 97.45846606243914
At time: 124.48176169395447 and batch: 100, loss is 4.569061050415039 and perplexity is 96.45350226564047
At time: 125.17288851737976 and batch: 150, loss is 4.545248413085938 and perplexity is 94.1838207972195
At time: 125.87132692337036 and batch: 200, loss is 4.493181724548339 and perplexity is 89.40545735434029
At time: 126.56875848770142 and batch: 250, loss is 4.471764860153198 and perplexity is 87.51103153943332
At time: 127.26958012580872 and batch: 300, loss is 4.445164842605591 and perplexity is 85.21392356481664
At time: 127.97514152526855 and batch: 350, loss is 4.506993989944458 and perplexity is 90.64891698767671
At time: 128.6847710609436 and batch: 400, loss is 4.486534633636475 and perplexity is 88.81314191945604
At time: 129.4105830192566 and batch: 450, loss is 4.428845071792603 and perplexity is 83.83453810419383
At time: 130.1481328010559 and batch: 500, loss is 4.458381690979004 and perplexity is 86.34765877119814
At time: 130.86480259895325 and batch: 550, loss is 4.510962505340576 and perplexity is 91.00937337567409
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9893415735123 and perplexity of 146.83970850580695
Finished 15 epochs...
Completing Train Step...
At time: 132.6866316795349 and batch: 50, loss is 4.571319427490234 and perplexity is 96.67157679855589
At time: 133.4074342250824 and batch: 100, loss is 4.561929931640625 and perplexity is 95.76812753352189
At time: 134.10853600502014 and batch: 150, loss is 4.537905025482178 and perplexity is 93.49472573779875
At time: 134.8233835697174 and batch: 200, loss is 4.486170997619629 and perplexity is 88.7808521335053
At time: 135.52801370620728 and batch: 250, loss is 4.465347261428833 and perplexity is 86.95121910219788
At time: 136.22716450691223 and batch: 300, loss is 4.4393768405914305 and perplexity is 84.72212982812131
At time: 136.92466568946838 and batch: 350, loss is 4.502067317962647 and perplexity is 90.20341782339678
At time: 137.62166476249695 and batch: 400, loss is 4.483845071792603 and perplexity is 88.57459441964107
At time: 138.3191363811493 and batch: 450, loss is 4.428131322860718 and perplexity is 83.77472264131764
At time: 139.01380014419556 and batch: 500, loss is 4.457030601501465 and perplexity is 86.2310741338955
At time: 139.70972180366516 and batch: 550, loss is 4.508174409866333 and perplexity is 90.75598395474361
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.989178921313996 and perplexity of 146.81582664669116
Finished 16 epochs...
Completing Train Step...
At time: 141.54101395606995 and batch: 50, loss is 4.564399604797363 and perplexity is 96.00493580651663
At time: 142.23679375648499 and batch: 100, loss is 4.555625562667847 and perplexity is 95.1662690839935
At time: 142.9184775352478 and batch: 150, loss is 4.531152782440185 and perplexity is 92.86555317996131
At time: 143.6042377948761 and batch: 200, loss is 4.479519691467285 and perplexity is 88.19230298556043
At time: 144.29437804222107 and batch: 250, loss is 4.459256763458252 and perplexity is 86.42325230112132
At time: 144.9931993484497 and batch: 300, loss is 4.433593683242798 and perplexity is 84.2335824551967
At time: 145.7017810344696 and batch: 350, loss is 4.497177515029907 and perplexity is 89.76341752047324
At time: 146.41152667999268 and batch: 400, loss is 4.481099739074707 and perplexity is 88.33176116915675
At time: 147.11956071853638 and batch: 450, loss is 4.426687335968017 and perplexity is 83.65384033711962
At time: 147.8450403213501 and batch: 500, loss is 4.455063571929932 and perplexity is 86.06162177462703
At time: 148.558988571167 and batch: 550, loss is 4.505156364440918 and perplexity is 90.48249118693293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.989733432201629 and perplexity of 146.89726019685216
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 150.406334400177 and batch: 50, loss is 4.5555323791503906 and perplexity is 95.1574015694566
At time: 151.11902928352356 and batch: 100, loss is 4.541811809539795 and perplexity is 93.86070387506616
At time: 151.82437181472778 and batch: 150, loss is 4.51034462928772 and perplexity is 90.95315823206379
At time: 152.52255249023438 and batch: 200, loss is 4.458219470977784 and perplexity is 86.33365258995899
At time: 153.23737621307373 and batch: 250, loss is 4.432550115585327 and perplexity is 84.14572486351747
At time: 153.9407606124878 and batch: 300, loss is 4.401406335830688 and perplexity is 81.56549652375246
At time: 154.6447696685791 and batch: 350, loss is 4.463663864135742 and perplexity is 86.80496878855868
At time: 155.34868240356445 and batch: 400, loss is 4.443019580841065 and perplexity is 85.0313133361917
At time: 156.05169129371643 and batch: 450, loss is 4.38490159034729 and perplexity is 80.23032738573981
At time: 156.74810695648193 and batch: 500, loss is 4.411562614440918 and perplexity is 82.39811944956
At time: 157.445250749588 and batch: 550, loss is 4.465421981811524 and perplexity is 86.95771637330091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.975862218978557 and perplexity of 144.8736841525139
Finished 18 epochs...
Completing Train Step...
At time: 159.26359963417053 and batch: 50, loss is 4.543794727325439 and perplexity is 94.04700658458658
At time: 159.96311855316162 and batch: 100, loss is 4.532084007263183 and perplexity is 92.95207216634553
At time: 160.66814303398132 and batch: 150, loss is 4.503391332626343 and perplexity is 90.32292757018055
At time: 161.36877751350403 and batch: 200, loss is 4.45201379776001 and perplexity is 85.79955309179473
At time: 162.07511472702026 and batch: 250, loss is 4.428520383834839 and perplexity is 83.80732245776214
At time: 162.7768590450287 and batch: 300, loss is 4.398478593826294 and perplexity is 81.32704302895574
At time: 163.47487831115723 and batch: 350, loss is 4.4621345329284665 and perplexity is 86.67231670118672
At time: 164.18860578536987 and batch: 400, loss is 4.442013683319092 and perplexity is 84.94582355300572
At time: 164.8961465358734 and batch: 450, loss is 4.384986429214478 and perplexity is 80.23713432457171
At time: 165.62703895568848 and batch: 500, loss is 4.413766031265259 and perplexity is 82.5798770225596
At time: 166.34745502471924 and batch: 550, loss is 4.4676021957397465 and perplexity is 87.14750961746243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9745165236452795 and perplexity of 144.6788594285966
Finished 19 epochs...
Completing Train Step...
At time: 168.20900917053223 and batch: 50, loss is 4.539280700683594 and perplexity is 93.6234326225963
At time: 168.9247591495514 and batch: 100, loss is 4.527756118774414 and perplexity is 92.55065523363616
At time: 169.62645649909973 and batch: 150, loss is 4.50037712097168 and perplexity is 90.0510850504789
At time: 170.3299195766449 and batch: 200, loss is 4.44915361404419 and perplexity is 85.55450122095912
At time: 171.03172898292542 and batch: 250, loss is 4.426604518890381 and perplexity is 83.64691265739887
At time: 171.73429679870605 and batch: 300, loss is 4.397146501541138 and perplexity is 81.21878002652659
At time: 172.43374824523926 and batch: 350, loss is 4.461670961380005 and perplexity is 86.6321471925648
At time: 173.13243532180786 and batch: 400, loss is 4.441559963226318 and perplexity is 84.90729066829553
At time: 173.83019375801086 and batch: 450, loss is 4.384901027679444 and perplexity is 80.23028224272697
At time: 174.52732729911804 and batch: 500, loss is 4.414807767868042 and perplexity is 82.66594832713771
At time: 175.22402620315552 and batch: 550, loss is 4.468385829925537 and perplexity is 87.21582815007791
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.97376949229139 and perplexity of 144.5708201437494
Finished 20 epochs...
Completing Train Step...
At time: 177.0368824005127 and batch: 50, loss is 4.535995950698853 and perplexity is 93.31640758002575
At time: 177.7427942752838 and batch: 100, loss is 4.524482746124267 and perplexity is 92.24819774814009
At time: 178.43066382408142 and batch: 150, loss is 4.498048915863037 and perplexity is 89.84167152763686
At time: 179.12210893630981 and batch: 200, loss is 4.446918468475342 and perplexity is 85.36348800735232
At time: 179.82272362709045 and batch: 250, loss is 4.425139036178589 and perplexity is 83.52441933085349
At time: 180.51788878440857 and batch: 300, loss is 4.396047687530517 and perplexity is 81.12958470661601
At time: 181.20975160598755 and batch: 350, loss is 4.46140760421753 and perplexity is 86.60933500011004
At time: 181.90843653678894 and batch: 400, loss is 4.440944881439209 and perplexity is 84.8550817982212
At time: 182.61322498321533 and batch: 450, loss is 4.384539957046509 and perplexity is 80.20131867319891
At time: 183.3469521999359 and batch: 500, loss is 4.41512089729309 and perplexity is 82.69183752113118
At time: 184.05979108810425 and batch: 550, loss is 4.468587226867676 and perplexity is 87.23339492006097
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9735587911402925 and perplexity of 144.54036211441363
Finished 21 epochs...
Completing Train Step...
At time: 185.86107063293457 and batch: 50, loss is 4.533330011367798 and perplexity is 93.06796301504018
At time: 186.56709003448486 and batch: 100, loss is 4.521739292144775 and perplexity is 91.99546590053986
At time: 187.258775472641 and batch: 150, loss is 4.496300172805786 and perplexity is 89.68469882077
At time: 187.94968128204346 and batch: 200, loss is 4.445169048309326 and perplexity is 85.21428195008693
At time: 188.6408712863922 and batch: 250, loss is 4.4237744331359865 and perplexity is 83.41051938588357
At time: 189.33162879943848 and batch: 300, loss is 4.3951170539855955 and perplexity is 81.05411791501304
At time: 190.03279399871826 and batch: 350, loss is 4.4609497547149655 and perplexity is 86.56969003556901
At time: 190.737863779068 and batch: 400, loss is 4.440362167358399 and perplexity is 84.80564995094456
At time: 191.44552755355835 and batch: 450, loss is 4.3840687084198 and perplexity is 80.1635328158799
At time: 192.15511202812195 and batch: 500, loss is 4.415306549072266 and perplexity is 82.707190833031
At time: 192.8621847629547 and batch: 550, loss is 4.468625020980835 and perplexity is 87.2366918911625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.973483146505153 and perplexity of 144.52942882498607
Finished 22 epochs...
Completing Train Step...
At time: 194.71006679534912 and batch: 50, loss is 4.531053371429444 and perplexity is 92.85632178031561
At time: 195.4258222579956 and batch: 100, loss is 4.519429101943969 and perplexity is 91.7831841766889
At time: 196.12570428848267 and batch: 150, loss is 4.494647817611694 and perplexity is 89.53663020750307
At time: 196.8232753276825 and batch: 200, loss is 4.443500566482544 and perplexity is 85.07222201443727
At time: 197.52362775802612 and batch: 250, loss is 4.422529106140137 and perplexity is 83.3067106656712
At time: 198.22530555725098 and batch: 300, loss is 4.394131088256836 and perplexity is 80.97424071713475
At time: 198.92596340179443 and batch: 350, loss is 4.460495271682739 and perplexity is 86.5303545196821
At time: 199.62833046913147 and batch: 400, loss is 4.439753170013428 and perplexity is 84.75401925835898
At time: 200.3304102420807 and batch: 450, loss is 4.383551578521729 and perplexity is 80.12208857327786
At time: 201.06067562103271 and batch: 500, loss is 4.415117321014404 and perplexity is 82.6915417926039
At time: 201.76829957962036 and batch: 550, loss is 4.468334045410156 and perplexity is 87.21131183762216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.97336594601895 and perplexity of 144.51249089824313
Finished 23 epochs...
Completing Train Step...
At time: 203.61442589759827 and batch: 50, loss is 4.528865194320678 and perplexity is 92.65335784409596
At time: 204.33408737182617 and batch: 100, loss is 4.517260303497315 and perplexity is 91.58434065312146
At time: 205.0441792011261 and batch: 150, loss is 4.493199148178101 and perplexity is 89.40701513549892
At time: 205.75482988357544 and batch: 200, loss is 4.441970462799072 and perplexity is 84.94215222967708
At time: 206.47667837142944 and batch: 250, loss is 4.421369400024414 and perplexity is 83.21015536254083
At time: 207.18873238563538 and batch: 300, loss is 4.393211946487427 and perplexity is 80.89984810416176
At time: 207.89943099021912 and batch: 350, loss is 4.45992506980896 and perplexity is 86.48102881353765
At time: 208.6082673072815 and batch: 400, loss is 4.43900538444519 and perplexity is 84.69066511651614
At time: 209.31379055976868 and batch: 450, loss is 4.382972002029419 and perplexity is 80.07566514848837
At time: 210.0172369480133 and batch: 500, loss is 4.4150514316558835 and perplexity is 82.68609347945502
At time: 210.7218074798584 and batch: 550, loss is 4.468086204528809 and perplexity is 87.18969998749347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.973268874148105 and perplexity of 144.49846348123606
Finished 24 epochs...
Completing Train Step...
At time: 212.5578110218048 and batch: 50, loss is 4.527069702148437 and perplexity is 92.48714872359442
At time: 213.27096676826477 and batch: 100, loss is 4.515344457626343 and perplexity is 91.40904714352618
At time: 213.975510597229 and batch: 150, loss is 4.491896524429321 and perplexity is 89.2906272555417
At time: 214.67825937271118 and batch: 200, loss is 4.440601415634156 and perplexity is 84.8259419837899
At time: 215.37770557403564 and batch: 250, loss is 4.42027663230896 and perplexity is 83.11927565541133
At time: 216.07903146743774 and batch: 300, loss is 4.392309007644653 and perplexity is 80.82683345777488
At time: 216.7827661037445 and batch: 350, loss is 4.459462108612061 and perplexity is 86.44100071937123
At time: 217.48590874671936 and batch: 400, loss is 4.438325328826904 and perplexity is 84.63309033313365
At time: 218.1851577758789 and batch: 450, loss is 4.382378225326538 and perplexity is 80.02813219743153
At time: 218.89593648910522 and batch: 500, loss is 4.414825239181519 and perplexity is 82.66739262245163
At time: 219.59731554985046 and batch: 550, loss is 4.467602243423462 and perplexity is 87.14751377297961
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9732961451753654 and perplexity of 144.50240415650563
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 221.4160556793213 and batch: 50, loss is 4.524877738952637 and perplexity is 92.28464232187943
At time: 222.13066816329956 and batch: 100, loss is 4.511143894195556 and perplexity is 91.02588295898515
At time: 222.83406853675842 and batch: 150, loss is 4.488854665756225 and perplexity is 89.01943046686327
At time: 223.53601837158203 and batch: 200, loss is 4.434895334243774 and perplexity is 84.34329657136924
At time: 224.2363212108612 and batch: 250, loss is 4.412977123260498 and perplexity is 82.51475478764813
At time: 224.9331545829773 and batch: 300, loss is 4.3850788879394536 and perplexity is 80.24455329067585
At time: 225.63452458381653 and batch: 350, loss is 4.45014048576355 and perplexity is 85.63897421382609
At time: 226.33548378944397 and batch: 400, loss is 4.427543277740479 and perplexity is 83.72547380615676
At time: 227.03642511367798 and batch: 450, loss is 4.370990915298462 and perplexity is 79.1219960632727
At time: 227.74478244781494 and batch: 500, loss is 4.403045587539673 and perplexity is 81.69931255243475
At time: 228.45899891853333 and batch: 550, loss is 4.456775064468384 and perplexity is 86.20904171622114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9702583475315825 and perplexity of 144.06410116859277
Finished 26 epochs...
Completing Train Step...
At time: 230.30368304252625 and batch: 50, loss is 4.5223699569702145 and perplexity is 92.05350250388177
At time: 231.01475477218628 and batch: 100, loss is 4.509973506927491 and perplexity is 90.91940974410703
At time: 231.71276473999023 and batch: 150, loss is 4.4871791553497316 and perplexity is 88.87040236866491
At time: 232.4149432182312 and batch: 200, loss is 4.433955078125 and perplexity is 84.2640295421849
At time: 233.12877821922302 and batch: 250, loss is 4.412275972366333 and perplexity is 82.45691977145299
At time: 233.84105706214905 and batch: 300, loss is 4.384616966247559 and perplexity is 80.20749515049282
At time: 234.54761910438538 and batch: 350, loss is 4.449612255096436 and perplexity is 85.59374902706304
At time: 235.27408361434937 and batch: 400, loss is 4.427333641052246 and perplexity is 83.70792371474312
At time: 235.97941493988037 and batch: 450, loss is 4.3708806896209715 and perplexity is 79.11327526828885
At time: 236.71313190460205 and batch: 500, loss is 4.403283033370972 and perplexity is 81.71871401692762
At time: 237.42117619514465 and batch: 550, loss is 4.457206354141236 and perplexity is 86.24623080467822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9699414841672205 and perplexity of 144.01845976423905
Finished 27 epochs...
Completing Train Step...
At time: 239.261563539505 and batch: 50, loss is 4.521556634902954 and perplexity is 91.97866379703804
At time: 239.98350358009338 and batch: 100, loss is 4.509265289306641 and perplexity is 90.8550418119913
At time: 240.6946702003479 and batch: 150, loss is 4.486320209503174 and perplexity is 88.79410028004074
At time: 241.40025234222412 and batch: 200, loss is 4.433304891586304 and perplexity is 84.20926001161551
At time: 242.10438323020935 and batch: 250, loss is 4.411804304122925 and perplexity is 82.41803663163952
At time: 242.81007313728333 and batch: 300, loss is 4.38430235862732 and perplexity is 80.18226523028886
At time: 243.51394987106323 and batch: 350, loss is 4.449192218780517 and perplexity is 85.55780409367325
At time: 244.21917009353638 and batch: 400, loss is 4.427228488922119 and perplexity is 83.69912211101783
At time: 244.92972326278687 and batch: 450, loss is 4.370760383605957 and perplexity is 79.10375803790811
At time: 245.6394431591034 and batch: 500, loss is 4.403355617523193 and perplexity is 81.7246457157762
At time: 246.3486430644989 and batch: 550, loss is 4.457463903427124 and perplexity is 86.26844632050484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.969746041805186 and perplexity of 143.9903152066951
Finished 28 epochs...
Completing Train Step...
At time: 248.24762892723083 and batch: 50, loss is 4.521000785827637 and perplexity is 91.92755174842604
At time: 248.9512848854065 and batch: 100, loss is 4.508722219467163 and perplexity is 90.80571457429856
At time: 249.6496353149414 and batch: 150, loss is 4.4856416034698485 and perplexity is 88.73386450835761
At time: 250.35101556777954 and batch: 200, loss is 4.432818994522095 and perplexity is 84.1683529185197
At time: 251.0611035823822 and batch: 250, loss is 4.411441240310669 and perplexity is 82.38811905638448
At time: 251.7734658718109 and batch: 300, loss is 4.3840726947784425 and perplexity is 80.16385237710868
At time: 252.48748636245728 and batch: 350, loss is 4.448804349899292 and perplexity is 85.52462531884242
At time: 253.20160007476807 and batch: 400, loss is 4.427105827331543 and perplexity is 83.68885607320698
At time: 253.91444659233093 and batch: 450, loss is 4.3705924701690675 and perplexity is 79.0904765691248
At time: 254.63519763946533 and batch: 500, loss is 4.4033855438232425 and perplexity is 81.72709146864132
At time: 255.34015417099 and batch: 550, loss is 4.457589473724365 and perplexity is 86.2792797551164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.969611634599402 and perplexity of 143.9709631713239
Finished 29 epochs...
Completing Train Step...
At time: 257.1395754814148 and batch: 50, loss is 4.520534143447876 and perplexity is 91.88466446420304
At time: 257.8492159843445 and batch: 100, loss is 4.50825777053833 and perplexity is 90.76354974989435
At time: 258.54915022850037 and batch: 150, loss is 4.485058135986328 and perplexity is 88.68210628481849
At time: 259.24985337257385 and batch: 200, loss is 4.432413539886475 and perplexity is 84.13423338708486
At time: 259.9428536891937 and batch: 250, loss is 4.411135902404785 and perplexity is 82.36296668082436
At time: 260.6411089897156 and batch: 300, loss is 4.383911294937134 and perplexity is 80.15091498813067
At time: 261.3382771015167 and batch: 350, loss is 4.44845645904541 and perplexity is 85.49487725875136
At time: 262.03666591644287 and batch: 400, loss is 4.427007446289062 and perplexity is 83.6806230812943
At time: 262.73560428619385 and batch: 450, loss is 4.370430784225464 and perplexity is 79.07768978454018
At time: 263.4371542930603 and batch: 500, loss is 4.403369874954223 and perplexity is 81.72581090758229
At time: 264.1371865272522 and batch: 550, loss is 4.457650547027588 and perplexity is 86.28454927664262
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.969518458589595 and perplexity of 143.95754915638938
Finished 30 epochs...
Completing Train Step...
At time: 265.9611885547638 and batch: 50, loss is 4.520117254257202 and perplexity is 91.84636672431073
At time: 266.6746950149536 and batch: 100, loss is 4.507868509292603 and perplexity is 90.72822589300236
At time: 267.38202834129333 and batch: 150, loss is 4.4845623016357425 and perplexity is 88.63814554976207
At time: 268.0827753543854 and batch: 200, loss is 4.4320462131500244 and perplexity is 84.10333430908723
At time: 268.7800223827362 and batch: 250, loss is 4.41084077835083 and perplexity is 82.3386629746823
At time: 269.4762282371521 and batch: 300, loss is 4.383746738433838 and perplexity is 80.13772671896176
At time: 270.17909121513367 and batch: 350, loss is 4.448159761428833 and perplexity is 85.46951489510167
At time: 270.88543486595154 and batch: 400, loss is 4.426901969909668 and perplexity is 83.67179721761448
At time: 271.59315276145935 and batch: 450, loss is 4.3702851676940915 and perplexity is 79.06617560399268
At time: 272.3218548297882 and batch: 500, loss is 4.403325719833374 and perplexity is 81.72220237419334
At time: 273.0373830795288 and batch: 550, loss is 4.457674045562744 and perplexity is 86.28657686097975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.96944963171127 and perplexity of 143.9476413486353
Finished 31 epochs...
Completing Train Step...
At time: 274.94927859306335 and batch: 50, loss is 4.519751625061035 and perplexity is 91.81279114955373
At time: 275.68057799339294 and batch: 100, loss is 4.507492990493774 and perplexity is 90.69416213478624
At time: 276.39607858657837 and batch: 150, loss is 4.484104213714599 and perplexity is 88.59755078462885
At time: 277.09491634368896 and batch: 200, loss is 4.431709823608398 and perplexity is 84.07504758495556
At time: 277.80501651763916 and batch: 250, loss is 4.410565547943115 and perplexity is 82.31600398926497
At time: 278.51014137268066 and batch: 300, loss is 4.383580551147461 and perplexity is 80.12440995419108
At time: 279.21628618240356 and batch: 350, loss is 4.447887802124024 and perplexity is 85.44627382570417
At time: 279.92198872566223 and batch: 400, loss is 4.426791734695435 and perplexity is 83.66257414748581
At time: 280.62709736824036 and batch: 450, loss is 4.37013240814209 and perplexity is 79.05409841290556
At time: 281.3375563621521 and batch: 500, loss is 4.403262948989868 and perplexity is 81.71707276361381
At time: 282.0541253089905 and batch: 550, loss is 4.45767276763916 and perplexity is 86.28646659339867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.969393791036403 and perplexity of 143.93960343962007
Finished 32 epochs...
Completing Train Step...
At time: 283.92708683013916 and batch: 50, loss is 4.519417877197266 and perplexity is 91.78215393947696
At time: 284.63731622695923 and batch: 100, loss is 4.507146444320679 and perplexity is 90.66273786527097
At time: 285.33652806282043 and batch: 150, loss is 4.4836732864379885 and perplexity is 88.55937990838142
At time: 286.0382888317108 and batch: 200, loss is 4.431396703720093 and perplexity is 84.04872613654608
At time: 286.741574048996 and batch: 250, loss is 4.410304765701294 and perplexity is 82.29454023601143
At time: 287.44487833976746 and batch: 300, loss is 4.383427019119263 and perplexity is 80.11210923532393
At time: 288.14735770225525 and batch: 350, loss is 4.447625513076782 and perplexity is 85.4238651428566
At time: 288.8518044948578 and batch: 400, loss is 4.426672449111939 and perplexity is 83.65259500370767
At time: 289.5661098957062 and batch: 450, loss is 4.3699754619598385 and perplexity is 79.0416921475518
At time: 290.3027997016907 and batch: 500, loss is 4.403180961608887 and perplexity is 81.71037326947722
At time: 291.02265906333923 and batch: 550, loss is 4.457646646499634 and perplexity is 86.28421272200255
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.969348014669215 and perplexity of 143.93301455828887
Finished 33 epochs...
Completing Train Step...
At time: 292.87511372566223 and batch: 50, loss is 4.519099311828613 and perplexity is 91.7529199804822
At time: 293.6090347766876 and batch: 100, loss is 4.506817255020142 and perplexity is 90.63289757383015
At time: 294.319540977478 and batch: 150, loss is 4.483257265090942 and perplexity is 88.52254497844818
At time: 295.02643394470215 and batch: 200, loss is 4.431094408035278 and perplexity is 84.02332240923285
At time: 295.7308051586151 and batch: 250, loss is 4.410052490234375 and perplexity is 82.27378196096035
At time: 296.4388394355774 and batch: 300, loss is 4.383271160125733 and perplexity is 80.09962401560111
At time: 297.15992760658264 and batch: 350, loss is 4.447383136749267 and perplexity is 85.40316292910575
At time: 297.8927729129791 and batch: 400, loss is 4.426551685333252 and perplexity is 83.64249341020367
At time: 298.6070065498352 and batch: 450, loss is 4.36981279373169 and perplexity is 79.0288356212428
At time: 299.329594373703 and batch: 500, loss is 4.403089828491211 and perplexity is 81.70292708771677
At time: 300.04129219055176 and batch: 550, loss is 4.457608957290649 and perplexity is 86.28096079955876
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.969307108128325 and perplexity of 143.9271268769665
Finished 34 epochs...
Completing Train Step...
At time: 301.89502215385437 and batch: 50, loss is 4.518792314529419 and perplexity is 91.72475640514696
At time: 302.6032943725586 and batch: 100, loss is 4.506504211425781 and perplexity is 90.60452996618694
At time: 303.3017826080322 and batch: 150, loss is 4.482861452102661 and perplexity is 88.48751353879676
At time: 304.00467228889465 and batch: 200, loss is 4.430801630020142 and perplexity is 83.99872582851742
At time: 304.7107377052307 and batch: 250, loss is 4.409797964096069 and perplexity is 82.25284379772192
At time: 305.41559076309204 and batch: 300, loss is 4.383116102218628 and perplexity is 80.08720489840738
At time: 306.12325143814087 and batch: 350, loss is 4.447132835388183 and perplexity is 85.38178907624754
At time: 306.84284567832947 and batch: 400, loss is 4.426429691314698 and perplexity is 83.63229014869172
At time: 307.55217695236206 and batch: 450, loss is 4.36964596748352 and perplexity is 79.01565263676339
At time: 308.3072974681854 and batch: 500, loss is 4.402975540161133 and perplexity is 81.69358993019168
At time: 309.0198407173157 and batch: 550, loss is 4.457543125152588 and perplexity is 86.27528092639652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.96926555227726 and perplexity of 143.92114598698925
Finished 35 epochs...
Completing Train Step...
At time: 310.8643054962158 and batch: 50, loss is 4.518493537902832 and perplexity is 91.69735528546461
At time: 311.5932171344757 and batch: 100, loss is 4.506188249588012 and perplexity is 90.57590691452275
At time: 312.3033561706543 and batch: 150, loss is 4.482478618621826 and perplexity is 88.4536440395811
At time: 313.0119230747223 and batch: 200, loss is 4.430493507385254 and perplexity is 83.97284790677939
At time: 313.716411113739 and batch: 250, loss is 4.409533596038818 and perplexity is 82.23110164729542
At time: 314.42181491851807 and batch: 300, loss is 4.3829622650146485 and perplexity is 80.07488545435001
At time: 315.12579584121704 and batch: 350, loss is 4.44690957069397 and perplexity is 85.36272846507799
At time: 315.83416223526 and batch: 400, loss is 4.426303482055664 and perplexity is 83.62173564537278
At time: 316.55279326438904 and batch: 450, loss is 4.369468154907227 and perplexity is 79.0016039090578
At time: 317.2624981403351 and batch: 500, loss is 4.402863044738769 and perplexity is 81.68440029219383
At time: 317.97758960723877 and batch: 550, loss is 4.457476854324341 and perplexity is 86.26956358152094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.969234385388963 and perplexity of 143.91666048260856
Finished 36 epochs...
Completing Train Step...
At time: 319.8014409542084 and batch: 50, loss is 4.518220748901367 and perplexity is 91.67234466694525
At time: 320.5180490016937 and batch: 100, loss is 4.505891666412354 and perplexity is 90.54904760761805
At time: 321.2216422557831 and batch: 150, loss is 4.48211627960205 and perplexity is 88.4215996387232
At time: 321.9244420528412 and batch: 200, loss is 4.430206756591797 and perplexity is 83.94877207805976
At time: 322.62600469589233 and batch: 250, loss is 4.4092782497406 and perplexity is 82.21010692046838
At time: 323.32323455810547 and batch: 300, loss is 4.382815475463867 and perplexity is 80.06313216053678
At time: 324.0298225879669 and batch: 350, loss is 4.446677789688111 and perplexity is 85.3429452987811
At time: 324.73611092567444 and batch: 400, loss is 4.426172285079956 and perplexity is 83.61076544619692
At time: 325.4391803741455 and batch: 450, loss is 4.369280614852905 and perplexity is 78.9867893331758
At time: 326.1684799194336 and batch: 500, loss is 4.402747049331665 and perplexity is 81.67492582643574
At time: 326.87899255752563 and batch: 550, loss is 4.457387094497681 and perplexity is 86.26182038796686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.96920451712101 and perplexity of 143.91236200542482
Finished 37 epochs...
Completing Train Step...
At time: 328.72534799575806 and batch: 50, loss is 4.517937107086182 and perplexity is 91.64634624399585
At time: 329.44568395614624 and batch: 100, loss is 4.50559700012207 and perplexity is 90.52236978639112
At time: 330.1497576236725 and batch: 150, loss is 4.481763896942138 and perplexity is 88.39044688941551
At time: 330.854336977005 and batch: 200, loss is 4.429933366775512 and perplexity is 83.92582447564732
At time: 331.5587565898895 and batch: 250, loss is 4.409031038284302 and perplexity is 82.18978615208097
At time: 332.2629156112671 and batch: 300, loss is 4.382654609680176 and perplexity is 80.05025377791034
At time: 332.9805428981781 and batch: 350, loss is 4.446460599899292 and perplexity is 85.32441169524209
At time: 333.69759917259216 and batch: 400, loss is 4.426053228378296 and perplexity is 83.60081161678643
At time: 334.407591342926 and batch: 450, loss is 4.3690962219238285 and perplexity is 78.9722260704549
At time: 335.1126344203949 and batch: 500, loss is 4.402619457244873 and perplexity is 81.66450541700598
At time: 335.8165020942688 and batch: 550, loss is 4.457318515777588 and perplexity is 86.25590486557357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.96918633643617 and perplexity of 143.90974560391064
Finished 38 epochs...
Completing Train Step...
At time: 337.6409089565277 and batch: 50, loss is 4.517662143707275 and perplexity is 91.62115031910523
At time: 338.3562421798706 and batch: 100, loss is 4.5053079891204835 and perplexity is 90.49621160581623
At time: 339.0666301250458 and batch: 150, loss is 4.481393804550171 and perplexity is 88.35774031010057
At time: 339.7697880268097 and batch: 200, loss is 4.4296630764007565 and perplexity is 83.90314319850165
At time: 340.4816093444824 and batch: 250, loss is 4.4087756156921385 and perplexity is 82.16879570468403
At time: 341.18527936935425 and batch: 300, loss is 4.382493762969971 and perplexity is 80.0373789934002
At time: 341.888165473938 and batch: 350, loss is 4.44624249458313 and perplexity is 85.3058040167435
At time: 342.5961010456085 and batch: 400, loss is 4.425933313369751 and perplexity is 83.5907872257965
At time: 343.3144781589508 and batch: 450, loss is 4.368912582397461 and perplexity is 78.95772497979058
At time: 344.0361559391022 and batch: 500, loss is 4.402473468780517 and perplexity is 81.65258421146822
At time: 344.73807740211487 and batch: 550, loss is 4.457215986251831 and perplexity is 86.24706154191263
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.969174324197972 and perplexity of 143.90801693615006
Finished 39 epochs...
Completing Train Step...
At time: 346.55702686309814 and batch: 50, loss is 4.517374706268311 and perplexity is 91.59481875482253
At time: 347.2684135437012 and batch: 100, loss is 4.505015153884887 and perplexity is 90.46971500612842
At time: 347.97629833221436 and batch: 150, loss is 4.481027059555053 and perplexity is 88.32534149247718
At time: 348.67212533950806 and batch: 200, loss is 4.429384593963623 and perplexity is 83.87978089984648
At time: 349.367151260376 and batch: 250, loss is 4.408532657623291 and perplexity is 82.14883455771916
At time: 350.06942224502563 and batch: 300, loss is 4.382316837310791 and perplexity is 80.02321957998144
At time: 350.78231024742126 and batch: 350, loss is 4.446027679443359 and perplexity is 85.28748100663086
At time: 351.4896471500397 and batch: 400, loss is 4.425763931274414 and perplexity is 83.57662964215974
At time: 352.19970440864563 and batch: 450, loss is 4.368746671676636 and perplexity is 78.94462613337406
At time: 352.8998591899872 and batch: 500, loss is 4.4023353862762455 and perplexity is 81.64131019655008
At time: 353.60911440849304 and batch: 550, loss is 4.457138576507568 and perplexity is 86.24038543733637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.969149650411403 and perplexity of 143.90446622425947
Finished 40 epochs...
Completing Train Step...
At time: 355.4539694786072 and batch: 50, loss is 4.517095861434936 and perplexity is 91.56928157346988
At time: 356.18335819244385 and batch: 100, loss is 4.504725456237793 and perplexity is 90.44350993851434
At time: 356.88263058662415 and batch: 150, loss is 4.48066740989685 and perplexity is 88.29358102526166
At time: 357.5831754207611 and batch: 200, loss is 4.4291181564331055 and perplexity is 83.85743515516788
At time: 358.28755259513855 and batch: 250, loss is 4.408275737762451 and perplexity is 82.12773160157676
At time: 358.989239692688 and batch: 300, loss is 4.382181196212769 and perplexity is 80.01236587873098
At time: 359.69725608825684 and batch: 350, loss is 4.445857181549072 and perplexity is 85.27294091027328
At time: 360.41769671440125 and batch: 400, loss is 4.425633563995361 and perplexity is 83.56573469454865
At time: 361.1336932182312 and batch: 450, loss is 4.3685979080200195 and perplexity is 78.9328829156239
At time: 361.8714075088501 and batch: 500, loss is 4.4022291469573975 and perplexity is 81.63263714008289
At time: 362.5867688655853 and batch: 550, loss is 4.457042369842529 and perplexity is 86.23208893655752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9691597147190825 and perplexity of 143.90591453037212
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 364.43181562423706 and batch: 50, loss is 4.516900177001953 and perplexity is 91.55136464361583
At time: 365.14555835723877 and batch: 100, loss is 4.504269790649414 and perplexity is 90.4023073313612
At time: 365.84699034690857 and batch: 150, loss is 4.479503746032715 and perplexity is 88.19089673217529
At time: 366.5521295070648 and batch: 200, loss is 4.42821268081665 and perplexity is 83.78153865877539
At time: 367.26672530174255 and batch: 250, loss is 4.406535701751709 and perplexity is 81.98495064913405
At time: 367.9901342391968 and batch: 300, loss is 4.380502691268921 and perplexity is 79.87817737657171
At time: 368.69940638542175 and batch: 350, loss is 4.4436092281341555 and perplexity is 85.08146660484478
At time: 369.40818214416504 and batch: 400, loss is 4.423119869232178 and perplexity is 83.35593973557884
At time: 370.1194226741791 and batch: 450, loss is 4.365962495803833 and perplexity is 78.72513610121588
At time: 370.8258047103882 and batch: 500, loss is 4.399119672775268 and perplexity is 81.37919679978003
At time: 371.5270915031433 and batch: 550, loss is 4.4542444896698 and perplexity is 85.99115908826089
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9689749859749 and perplexity of 143.87933342672306
Finished 42 epochs...
Completing Train Step...
At time: 373.37403440475464 and batch: 50, loss is 4.51656699180603 and perplexity is 91.5208661653529
At time: 374.08893752098083 and batch: 100, loss is 4.50403660774231 and perplexity is 90.38122951611736
At time: 374.793594121933 and batch: 150, loss is 4.47911883354187 and perplexity is 88.15695748668303
At time: 375.50733971595764 and batch: 200, loss is 4.427954959869385 and perplexity is 83.75994918341837
At time: 376.20613288879395 and batch: 250, loss is 4.406293954849243 and perplexity is 81.96513343673716
At time: 376.91789627075195 and batch: 300, loss is 4.3803016948699955 and perplexity is 79.86212376397948
At time: 377.6154248714447 and batch: 350, loss is 4.443588991165161 and perplexity is 85.07974483126486
At time: 378.3136422634125 and batch: 400, loss is 4.42307017326355 and perplexity is 83.35179738434277
At time: 379.02582263946533 and batch: 450, loss is 4.366107110977173 and perplexity is 78.73652177366999
At time: 379.75300574302673 and batch: 500, loss is 4.399248275756836 and perplexity is 81.3896630801091
At time: 380.4668209552765 and batch: 550, loss is 4.454279050827027 and perplexity is 85.99413109358792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.968869473071808 and perplexity of 143.86415310143244
Finished 43 epochs...
Completing Train Step...
At time: 382.2998905181885 and batch: 50, loss is 4.516470794677734 and perplexity is 91.51206254429694
At time: 383.0172760486603 and batch: 100, loss is 4.503811931610107 and perplexity is 90.36092529206861
At time: 383.7143626213074 and batch: 150, loss is 4.478843355178833 and perplexity is 88.13267549707925
At time: 384.4120280742645 and batch: 200, loss is 4.427765045166016 and perplexity is 83.74404344792934
At time: 385.1210548877716 and batch: 250, loss is 4.406162300109863 and perplexity is 81.95434304877476
At time: 385.830806016922 and batch: 300, loss is 4.38015645980835 and perplexity is 79.8505258257455
At time: 386.5337076187134 and batch: 350, loss is 4.443582973480225 and perplexity is 85.07923284970649
At time: 387.2431709766388 and batch: 400, loss is 4.423045225143433 and perplexity is 83.34971793962886
At time: 387.95061683654785 and batch: 450, loss is 4.366226167678833 and perplexity is 78.74589644230004
At time: 388.65158200263977 and batch: 500, loss is 4.399321727752685 and perplexity is 81.39564153286578
At time: 389.3525171279907 and batch: 550, loss is 4.454235162734985 and perplexity is 85.99035705806567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.968775972406915 and perplexity of 143.85070233630069
Finished 44 epochs...
Completing Train Step...
At time: 391.19261384010315 and batch: 50, loss is 4.51640380859375 and perplexity is 91.50593271489868
At time: 391.9106822013855 and batch: 100, loss is 4.503512659072876 and perplexity is 90.33388679483156
At time: 392.6212589740753 and batch: 150, loss is 4.478573551177979 and perplexity is 88.1089001561094
At time: 393.3295030593872 and batch: 200, loss is 4.427581853866577 and perplexity is 83.72870367288947
At time: 394.0304481983185 and batch: 250, loss is 4.406058082580566 and perplexity is 81.94580241467668
At time: 394.73923802375793 and batch: 300, loss is 4.3800156211853025 and perplexity is 79.83928057953968
At time: 395.46458077430725 and batch: 350, loss is 4.443558607101441 and perplexity is 85.07715980214866
At time: 396.1718442440033 and batch: 400, loss is 4.423054370880127 and perplexity is 83.35048023768854
At time: 396.87754821777344 and batch: 450, loss is 4.366334400177002 and perplexity is 78.75441976863469
At time: 397.6128559112549 and batch: 500, loss is 4.399386720657349 and perplexity is 81.40093184395042
At time: 398.3282449245453 and batch: 550, loss is 4.454202928543091 and perplexity is 85.98758527306856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.968735065866024 and perplexity of 143.84481802201773
Finished 45 epochs...
Completing Train Step...
At time: 400.16359519958496 and batch: 50, loss is 4.516373815536499 and perplexity is 91.50318821337822
At time: 400.8810293674469 and batch: 100, loss is 4.5033156776428225 and perplexity is 90.3160944490668
At time: 401.58504700660706 and batch: 150, loss is 4.478361568450928 and perplexity is 88.09022457069754
At time: 402.29423546791077 and batch: 200, loss is 4.427452983856202 and perplexity is 83.71791424920988
At time: 403.0048334598541 and batch: 250, loss is 4.40596116065979 and perplexity is 81.9378604549883
At time: 403.7153973579407 and batch: 300, loss is 4.379916048049926 and perplexity is 79.83133112782868
At time: 404.4274682998657 and batch: 350, loss is 4.443570413589478 and perplexity is 85.07816427054769
At time: 405.1352035999298 and batch: 400, loss is 4.423055973052978 and perplexity is 83.3506137796721
At time: 405.84062790870667 and batch: 450, loss is 4.366439914703369 and perplexity is 78.76272994235019
At time: 406.54582691192627 and batch: 500, loss is 4.399455080032348 and perplexity is 81.40649655097346
At time: 407.250821352005 and batch: 550, loss is 4.45418755531311 and perplexity is 85.98626337630563
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.968701301737035 and perplexity of 143.83996130901934
Finished 46 epochs...
Completing Train Step...
At time: 409.08418011665344 and batch: 50, loss is 4.5163405323028565 and perplexity is 91.5001427420677
At time: 409.7987906932831 and batch: 100, loss is 4.503146171569824 and perplexity is 90.30078661998985
At time: 410.50334763526917 and batch: 150, loss is 4.478169736862182 and perplexity is 88.07332770369344
At time: 411.2123761177063 and batch: 200, loss is 4.427335262298584 and perplexity is 83.70805942601791
At time: 411.923832654953 and batch: 250, loss is 4.405863847732544 and perplexity is 81.92988722989037
At time: 412.6320707798004 and batch: 300, loss is 4.379824638366699 and perplexity is 79.82403410465305
At time: 413.34153270721436 and batch: 350, loss is 4.4435813522338865 and perplexity is 85.0790949154236
At time: 414.0502715110779 and batch: 400, loss is 4.4230578517913814 and perplexity is 83.35077037381824
At time: 414.76749181747437 and batch: 450, loss is 4.3665360164642335 and perplexity is 78.77029954310832
At time: 415.4922122955322 and batch: 500, loss is 4.399513673782349 and perplexity is 81.41126660262701
At time: 416.2097227573395 and batch: 550, loss is 4.454167242050171 and perplexity is 85.98451673246858
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.968673056744515 and perplexity of 143.83589860776385
Finished 47 epochs...
Completing Train Step...
At time: 418.0503339767456 and batch: 50, loss is 4.516312417984008 and perplexity is 91.4975703140412
At time: 418.76088786125183 and batch: 100, loss is 4.50298526763916 and perplexity is 90.28625803736503
At time: 419.4579014778137 and batch: 150, loss is 4.477990398406982 and perplexity is 88.05753418539365
At time: 420.1549608707428 and batch: 200, loss is 4.427224044799805 and perplexity is 83.69875014270808
At time: 420.8538496494293 and batch: 250, loss is 4.405769958496093 and perplexity is 81.9221952564383
At time: 421.55113458633423 and batch: 300, loss is 4.379737720489502 and perplexity is 79.81709627057465
At time: 422.24892234802246 and batch: 350, loss is 4.443590307235718 and perplexity is 85.07985680228575
At time: 422.9451656341553 and batch: 400, loss is 4.423058490753174 and perplexity is 83.35082363179292
At time: 423.6483919620514 and batch: 450, loss is 4.366625156402588 and perplexity is 78.77732143571463
At time: 424.35356736183167 and batch: 500, loss is 4.399564819335938 and perplexity is 81.41543053340813
At time: 425.0501923561096 and batch: 550, loss is 4.454143781661987 and perplexity is 85.98249952599052
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.968649032268118 and perplexity of 143.8324430671217
Finished 48 epochs...
Completing Train Step...
At time: 426.86275696754456 and batch: 50, loss is 4.516287517547608 and perplexity is 91.49529201297622
At time: 427.5744984149933 and batch: 100, loss is 4.50283143043518 and perplexity is 90.27236972016807
At time: 428.2732515335083 and batch: 150, loss is 4.477819232940674 and perplexity is 88.04246306635721
At time: 428.9735906124115 and batch: 200, loss is 4.427116632461548 and perplexity is 83.6897603470622
At time: 429.67608737945557 and batch: 250, loss is 4.40567907333374 and perplexity is 81.9147500827551
At time: 430.37892413139343 and batch: 300, loss is 4.3796537971496585 and perplexity is 79.8103980343521
At time: 431.08160519599915 and batch: 350, loss is 4.443597345352173 and perplexity is 85.08045560633312
At time: 431.7923004627228 and batch: 400, loss is 4.423057870864868 and perplexity is 83.35077196360807
At time: 432.51132678985596 and batch: 450, loss is 4.366708307266236 and perplexity is 78.78387211037122
At time: 433.246808052063 and batch: 500, loss is 4.399609594345093 and perplexity is 81.41907599166775
At time: 433.9613575935364 and batch: 550, loss is 4.45411847114563 and perplexity is 85.9803232920708
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.968628254342587 and perplexity of 143.82945455837827
Finished 49 epochs...
Completing Train Step...
At time: 435.80696511268616 and batch: 50, loss is 4.516264781951905 and perplexity is 91.4932118366554
At time: 436.53237652778625 and batch: 100, loss is 4.502683162689209 and perplexity is 90.25898623158052
At time: 437.239271402359 and batch: 150, loss is 4.477655096054077 and perplexity is 88.02801323648866
At time: 437.9437806606293 and batch: 200, loss is 4.427011165618897 and perplexity is 83.68093431771108
At time: 438.6483542919159 and batch: 250, loss is 4.405590934753418 and perplexity is 81.90753055113967
At time: 439.35605669021606 and batch: 300, loss is 4.3795710372924805 and perplexity is 79.80379321052037
At time: 440.0668058395386 and batch: 350, loss is 4.443602676391602 and perplexity is 85.08090917480556
At time: 440.7764070034027 and batch: 400, loss is 4.423055095672607 and perplexity is 83.35054064951174
At time: 441.48538184165955 and batch: 450, loss is 4.366784839630127 and perplexity is 78.7899018570728
At time: 442.1905884742737 and batch: 500, loss is 4.3996480941772464 and perplexity is 81.42221067276948
At time: 442.89587020874023 and batch: 550, loss is 4.454091196060181 and perplexity is 85.97797820338741
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.968609424347573 and perplexity of 143.82674627596467
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efc55b01898>
SETTINGS FOR THIS RUN
{'dropout': 0.17772046552728626, 'wordvec_dim': 200, 'tune_wordvecs': True, 'anneal': 5.851289093719012, 'num_layers': 1, 'seq_len': 35, 'data': 'ptb', 'lr': 29.943287054749074, 'wordvec_source': '', 'batch_size': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.9037325382232666 and batch: 50, loss is 6.425645885467529 and perplexity is 617.4795098160976
At time: 1.5852138996124268 and batch: 100, loss is 5.58122670173645 and perplexity is 265.39696910640293
At time: 2.2539262771606445 and batch: 150, loss is 5.511158905029297 and perplexity is 247.43771778541515
At time: 2.926029920578003 and batch: 200, loss is 5.406412506103516 and perplexity is 222.83074808716546
At time: 3.5997142791748047 and batch: 250, loss is 5.364618520736695 and perplexity is 213.70969331239235
At time: 4.274734973907471 and batch: 300, loss is 5.355447683334351 and perplexity is 211.7587560016413
At time: 4.961029767990112 and batch: 350, loss is 5.382046279907226 and perplexity is 217.46681842115242
At time: 5.635265350341797 and batch: 400, loss is 5.385003843307495 and perplexity is 218.11094237327845
At time: 6.30535364151001 and batch: 450, loss is 5.3061473846435545 and perplexity is 201.5721505705476
At time: 6.976823329925537 and batch: 500, loss is 5.3308114910125735 and perplexity is 206.60556470690597
At time: 7.64829158782959 and batch: 550, loss is 5.350660886764526 and perplexity is 210.7475321071995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.325676775993185 and perplexity of 205.54742296371774
Finished 1 epochs...
Completing Train Step...
At time: 9.397536039352417 and batch: 50, loss is 5.276075553894043 and perplexity is 195.6007425563841
At time: 10.075628519058228 and batch: 100, loss is 5.321600608825683 and perplexity is 204.71128258855617
At time: 10.741983652114868 and batch: 150, loss is 5.2894389438629155 and perplexity is 198.23217482513473
At time: 11.412037134170532 and batch: 200, loss is 5.199634246826172 and perplexity is 181.20595310076214
At time: 12.082051515579224 and batch: 250, loss is 5.177076139450073 and perplexity is 177.16404999388368
At time: 12.752496004104614 and batch: 300, loss is 5.181224384307861 and perplexity is 177.9004962769494
At time: 13.419096231460571 and batch: 350, loss is 5.243902730941772 and perplexity is 189.40786976944113
At time: 14.086236715316772 and batch: 400, loss is 5.230662279129028 and perplexity is 186.9165534693732
At time: 14.768490552902222 and batch: 450, loss is 5.179873218536377 and perplexity is 177.66028553446156
At time: 15.466632604598999 and batch: 500, loss is 5.225650329589843 and perplexity is 185.98208085627138
At time: 16.174288272857666 and batch: 550, loss is 5.244772825241089 and perplexity is 189.5727441949204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.432379864631815 and perplexity of 228.69285634759586
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 18.00035262107849 and batch: 50, loss is 5.2079331493377685 and perplexity is 182.71602092615547
At time: 18.715455055236816 and batch: 100, loss is 5.110127286911011 and perplexity is 165.6914438835966
At time: 19.417965173721313 and batch: 150, loss is 5.054287405014038 and perplexity is 156.69283202462665
At time: 20.117502689361572 and batch: 200, loss is 4.990698156356811 and perplexity is 147.03904391218518
At time: 20.81367063522339 and batch: 250, loss is 4.969219913482666 and perplexity is 143.91457774925416
At time: 21.513779401779175 and batch: 300, loss is 4.9474467754364015 and perplexity is 140.8149723011154
At time: 22.215391397476196 and batch: 350, loss is 4.997398586273193 and perplexity is 148.02757682021706
At time: 22.91810369491577 and batch: 400, loss is 4.957795629501343 and perplexity is 142.27981253499593
At time: 23.620455265045166 and batch: 450, loss is 4.888499050140381 and perplexity is 132.75416709033587
At time: 24.32234025001526 and batch: 500, loss is 4.897613821029663 and perplexity is 133.96972224574864
At time: 25.036218404769897 and batch: 550, loss is 4.918923120498658 and perplexity is 136.8551572910637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.154449787545712 and perplexity of 173.20048347416449
Finished 3 epochs...
Completing Train Step...
At time: 26.854238748550415 and batch: 50, loss is 4.98800217628479 and perplexity is 146.64316346267205
At time: 27.57341170310974 and batch: 100, loss is 4.979524555206299 and perplexity is 145.40523305570895
At time: 28.28033709526062 and batch: 150, loss is 4.945389490127564 and perplexity is 140.52557351734055
At time: 28.994736909866333 and batch: 200, loss is 4.894744710922241 and perplexity is 133.58589924009334
At time: 29.693614959716797 and batch: 250, loss is 4.882168111801147 and perplexity is 131.91636348401323
At time: 30.408112049102783 and batch: 300, loss is 4.861538648605347 and perplexity is 129.2228778228669
At time: 31.118642330169678 and batch: 350, loss is 4.897775192260742 and perplexity is 133.99134284917955
At time: 31.837916135787964 and batch: 400, loss is 4.87846529006958 and perplexity is 131.42880393667178
At time: 32.545045137405396 and batch: 450, loss is 4.811043186187744 and perplexity is 122.85971624925581
At time: 33.25990009307861 and batch: 500, loss is 4.833856172561646 and perplexity is 125.69472787112163
At time: 33.973326444625854 and batch: 550, loss is 4.867090425491333 and perplexity is 129.94228956803198
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.1301587693234705 and perplexity of 169.0439549087531
Finished 4 epochs...
Completing Train Step...
At time: 35.813538789749146 and batch: 50, loss is 4.8956894683837895 and perplexity is 133.7121651510971
At time: 36.52854132652283 and batch: 100, loss is 4.898169345855713 and perplexity is 134.04416642827653
At time: 37.23436427116394 and batch: 150, loss is 4.880331811904907 and perplexity is 131.67434775417132
At time: 37.94072222709656 and batch: 200, loss is 4.839898996353149 and perplexity is 126.45657850945697
At time: 38.64435434341431 and batch: 250, loss is 4.82526104927063 and perplexity is 124.61899583534618
At time: 39.35318326950073 and batch: 300, loss is 4.8093355274200436 and perplexity is 122.65009281119674
At time: 40.05840754508972 and batch: 350, loss is 4.833292608261108 and perplexity is 125.623910766598
At time: 40.76575756072998 and batch: 400, loss is 4.814598350524903 and perplexity is 123.29728007572307
At time: 41.47312808036804 and batch: 450, loss is 4.7529880332946775 and perplexity is 115.93017072007746
At time: 42.18036365509033 and batch: 500, loss is 4.785210847854614 and perplexity is 119.72660452041738
At time: 42.888967752456665 and batch: 550, loss is 4.829844160079956 and perplexity is 125.1914493090309
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.1177238302027925 and perplexity of 166.9549190560686
Finished 5 epochs...
Completing Train Step...
At time: 44.71186351776123 and batch: 50, loss is 4.866685762405395 and perplexity is 129.88971735787538
At time: 45.43038868904114 and batch: 100, loss is 4.86766095161438 and perplexity is 130.01644619085286
At time: 46.13789105415344 and batch: 150, loss is 4.838739500045777 and perplexity is 126.31003754690263
At time: 46.84712862968445 and batch: 200, loss is 4.796266136169433 and perplexity is 121.05756014574214
At time: 47.55378842353821 and batch: 250, loss is 4.786954803466797 and perplexity is 119.93558457730417
At time: 48.25972080230713 and batch: 300, loss is 4.779112176895142 and perplexity is 118.9986533778692
At time: 48.965585708618164 and batch: 350, loss is 4.808166780471802 and perplexity is 122.50682962503565
At time: 49.66948103904724 and batch: 400, loss is 4.793870811462402 and perplexity is 120.7679349925737
At time: 50.37293338775635 and batch: 450, loss is 4.731376295089722 and perplexity is 113.4515978256079
At time: 51.0810751914978 and batch: 500, loss is 4.764620914459228 and perplexity is 117.28664718728089
At time: 51.786593198776245 and batch: 550, loss is 4.805741033554077 and perplexity is 122.21001919970188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.100923578789893 and perplexity of 164.1734644312646
Finished 6 epochs...
Completing Train Step...
At time: 53.5999972820282 and batch: 50, loss is 4.8301622104644775 and perplexity is 125.23127283022986
At time: 54.30823040008545 and batch: 100, loss is 4.835365552902221 and perplexity is 125.88459227503739
At time: 55.01067233085632 and batch: 150, loss is 4.81127857208252 and perplexity is 122.88863909737873
At time: 55.71152067184448 and batch: 200, loss is 4.771547021865845 and perplexity is 118.10180678586528
At time: 56.41188168525696 and batch: 250, loss is 4.76266863822937 and perplexity is 117.05789462065191
At time: 57.12390851974487 and batch: 300, loss is 4.748474712371826 and perplexity is 115.40811963320768
At time: 57.83531665802002 and batch: 350, loss is 4.778109064102173 and perplexity is 118.87934415662734
At time: 58.54799556732178 and batch: 400, loss is 4.763773565292358 and perplexity is 117.18730653855364
At time: 59.26120162010193 and batch: 450, loss is 4.700087423324585 and perplexity is 109.95678481963444
At time: 59.971617698669434 and batch: 500, loss is 4.725983123779297 and perplexity is 112.84138090436791
At time: 60.68418502807617 and batch: 550, loss is 4.780310573577881 and perplexity is 119.14134645370464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.101233299742353 and perplexity of 164.2243202681895
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 62.50881052017212 and batch: 50, loss is 4.787104997634888 and perplexity is 119.95359955549274
At time: 63.22692084312439 and batch: 100, loss is 4.760388355255127 and perplexity is 116.79127359776612
At time: 63.94430351257324 and batch: 150, loss is 4.719411554336548 and perplexity is 112.10226716229622
At time: 64.65386486053467 and batch: 200, loss is 4.670936470031738 and perplexity is 106.79770847098692
At time: 65.35653185844421 and batch: 250, loss is 4.636676807403564 and perplexity is 103.20082089281452
At time: 66.06010031700134 and batch: 300, loss is 4.611207818984985 and perplexity is 100.60558962400823
At time: 66.76295137405396 and batch: 350, loss is 4.643037328720093 and perplexity is 103.85932390516568
At time: 67.4666600227356 and batch: 400, loss is 4.6195993232727055 and perplexity is 101.45337397884335
At time: 68.16860508918762 and batch: 450, loss is 4.542151260375976 and perplexity is 93.8925703777306
At time: 68.86978673934937 and batch: 500, loss is 4.560726881027222 and perplexity is 95.65298290524372
At time: 69.5744276046753 and batch: 550, loss is 4.61546422958374 and perplexity is 101.0347209536658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.0090322291597404 and perplexity of 149.75973289286367
Finished 8 epochs...
Completing Train Step...
At time: 71.40201878547668 and batch: 50, loss is 4.70973692893982 and perplexity is 111.02294913867966
At time: 72.11719512939453 and batch: 100, loss is 4.710032997131347 and perplexity is 111.05582436886404
At time: 72.81717324256897 and batch: 150, loss is 4.679286365509033 and perplexity is 107.6931915662508
At time: 73.51934766769409 and batch: 200, loss is 4.63695104598999 and perplexity is 103.22912642111083
At time: 74.22064518928528 and batch: 250, loss is 4.610422992706299 and perplexity is 100.52666268950604
At time: 74.9213273525238 and batch: 300, loss is 4.587816143035889 and perplexity is 98.27956711518581
At time: 75.62316179275513 and batch: 350, loss is 4.623492498397827 and perplexity is 101.84911958439774
At time: 76.33331322669983 and batch: 400, loss is 4.608715324401856 and perplexity is 100.35514298494239
At time: 77.05102348327637 and batch: 450, loss is 4.54337857246399 and perplexity is 94.00787660822132
At time: 77.76342296600342 and batch: 500, loss is 4.568991203308105 and perplexity is 96.44676550282801
At time: 78.47640323638916 and batch: 550, loss is 4.620253810882568 and perplexity is 101.51979568881336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.004095848570478 and perplexity of 149.02228351725415
Finished 9 epochs...
Completing Train Step...
At time: 80.31038475036621 and batch: 50, loss is 4.6893739414215085 and perplexity is 108.78505266841657
At time: 81.02774691581726 and batch: 100, loss is 4.691344032287597 and perplexity is 108.99958035717925
At time: 81.73183345794678 and batch: 150, loss is 4.661890773773194 and perplexity is 105.83600503521117
At time: 82.44783568382263 and batch: 200, loss is 4.6213320541381835 and perplexity is 101.62931775893206
At time: 83.15649008750916 and batch: 250, loss is 4.596506900787354 and perplexity is 99.13741329219104
At time: 83.86946749687195 and batch: 300, loss is 4.576513338088989 and perplexity is 97.17498652675542
At time: 84.57869935035706 and batch: 350, loss is 4.614980163574219 and perplexity is 100.98582531478388
At time: 85.28614258766174 and batch: 400, loss is 4.604408302307129 and perplexity is 99.92384064787153
At time: 85.989830493927 and batch: 450, loss is 4.543863162994385 and perplexity is 94.0534429746314
At time: 86.69230532646179 and batch: 500, loss is 4.571150932312012 and perplexity is 96.65528947620035
At time: 87.40193033218384 and batch: 550, loss is 4.619406156539917 and perplexity is 101.43377845472408
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.002209277863198 and perplexity of 148.7414074720048
Finished 10 epochs...
Completing Train Step...
At time: 89.24855518341064 and batch: 50, loss is 4.6751341629028325 and perplexity is 107.24695468964309
At time: 89.96089124679565 and batch: 100, loss is 4.678072347640991 and perplexity is 107.56252943655088
At time: 90.6677496433258 and batch: 150, loss is 4.649512453079224 and perplexity is 104.53400791744691
At time: 91.383793592453 and batch: 200, loss is 4.609925861358643 and perplexity is 100.47670015420803
At time: 92.09362006187439 and batch: 250, loss is 4.586398153305054 and perplexity is 98.1403064566916
At time: 92.80029344558716 and batch: 300, loss is 4.568220586776733 and perplexity is 96.37247066102749
At time: 93.50213599205017 and batch: 350, loss is 4.608374376296997 and perplexity is 100.32093292138838
At time: 94.20325422286987 and batch: 400, loss is 4.600600461959839 and perplexity is 99.54407012742978
At time: 94.90417814254761 and batch: 450, loss is 4.542996873855591 and perplexity is 93.9720007798539
At time: 95.61457681655884 and batch: 500, loss is 4.570125551223755 and perplexity is 96.55623176492249
At time: 96.3301146030426 and batch: 550, loss is 4.6156587982177735 and perplexity is 101.05438105386912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.001136617457613 and perplexity of 148.581943993908
Finished 11 epochs...
Completing Train Step...
At time: 98.1623272895813 and batch: 50, loss is 4.663833847045899 and perplexity is 106.04185207105658
At time: 98.88253569602966 and batch: 100, loss is 4.66783763885498 and perplexity is 106.4672726492084
At time: 99.58456015586853 and batch: 150, loss is 4.639587755203247 and perplexity is 103.50167076200982
At time: 100.28979706764221 and batch: 200, loss is 4.600606203079224 and perplexity is 99.54464162346089
At time: 100.99119925498962 and batch: 250, loss is 4.577868814468384 and perplexity is 97.30679423659807
At time: 101.69261503219604 and batch: 300, loss is 4.561144142150879 and perplexity is 95.69290350444979
At time: 102.39269423484802 and batch: 350, loss is 4.602698574066162 and perplexity is 99.75314399959136
At time: 103.09531760215759 and batch: 400, loss is 4.597211570739746 and perplexity is 99.20729706810266
At time: 103.80145716667175 and batch: 450, loss is 4.541073789596558 and perplexity is 93.79145835914895
At time: 104.50725221633911 and batch: 500, loss is 4.568463773727417 and perplexity is 96.39591003825721
At time: 105.21329879760742 and batch: 550, loss is 4.612015991210938 and perplexity is 100.68692913105599
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.00087267287234 and perplexity of 148.54273176947635
Finished 12 epochs...
Completing Train Step...
At time: 107.02905797958374 and batch: 50, loss is 4.6543339824676515 and perplexity is 105.03923872253425
At time: 107.74425029754639 and batch: 100, loss is 4.659413251876831 and perplexity is 105.57411856400373
At time: 108.44490551948547 and batch: 150, loss is 4.630969591140747 and perplexity is 102.61350904152833
At time: 109.14395761489868 and batch: 200, loss is 4.592785272598267 and perplexity is 98.7691464015674
At time: 109.84474301338196 and batch: 250, loss is 4.570455617904663 and perplexity is 96.58810702005263
At time: 110.54631114006042 and batch: 300, loss is 4.55523307800293 and perplexity is 95.1289251117083
At time: 111.25183939933777 and batch: 350, loss is 4.598171672821045 and perplexity is 99.30259193957981
At time: 111.9547188282013 and batch: 400, loss is 4.593701372146606 and perplexity is 98.85967023006403
At time: 112.66339540481567 and batch: 450, loss is 4.538696079254151 and perplexity is 93.56871435388184
At time: 113.37209010124207 and batch: 500, loss is 4.566178913116455 and perplexity is 96.17591025048843
At time: 114.09017753601074 and batch: 550, loss is 4.607290477752685 and perplexity is 100.2122541172697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.00025875010389 and perplexity of 148.451565991592
Finished 13 epochs...
Completing Train Step...
At time: 115.9096326828003 and batch: 50, loss is 4.64581259727478 and perplexity is 104.14796175943097
At time: 116.63570594787598 and batch: 100, loss is 4.6516548442840575 and perplexity is 104.75820072521431
At time: 117.34016275405884 and batch: 150, loss is 4.62300597190857 and perplexity is 101.79957934210972
At time: 118.043137550354 and batch: 200, loss is 4.585343074798584 and perplexity is 98.03681533396085
At time: 118.74194431304932 and batch: 250, loss is 4.563209342956543 and perplexity is 95.89073277413434
At time: 119.43891787528992 and batch: 300, loss is 4.548982486724854 and perplexity is 94.53616755463364
At time: 120.13666033744812 and batch: 350, loss is 4.592530651092529 and perplexity is 98.74400085422464
At time: 120.83780717849731 and batch: 400, loss is 4.589240093231201 and perplexity is 98.4196120087881
At time: 121.54569101333618 and batch: 450, loss is 4.53551700592041 and perplexity is 93.27172487500057
At time: 122.25394344329834 and batch: 500, loss is 4.562764482498169 and perplexity is 95.84808426581947
At time: 122.96792769432068 and batch: 550, loss is 4.6024886512756344 and perplexity is 99.732205739025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.999936367603058 and perplexity of 148.40371551798734
Finished 14 epochs...
Completing Train Step...
At time: 124.8074905872345 and batch: 50, loss is 4.637576055526734 and perplexity is 103.29366577634902
At time: 125.52517485618591 and batch: 100, loss is 4.644337320327759 and perplexity is 103.99442795265738
At time: 126.22960066795349 and batch: 150, loss is 4.616149692535401 and perplexity is 101.10400025319487
At time: 126.93608021736145 and batch: 200, loss is 4.578626136779786 and perplexity is 97.38051475449747
At time: 127.6464262008667 and batch: 250, loss is 4.5570374584198 and perplexity is 95.30072883428102
At time: 128.3555884361267 and batch: 300, loss is 4.544028530120849 and perplexity is 94.06899760830727
At time: 129.06793808937073 and batch: 350, loss is 4.587997980117798 and perplexity is 98.2974396097732
At time: 129.77967762947083 and batch: 400, loss is 4.585698404312134 and perplexity is 98.07165689761476
At time: 130.48348927497864 and batch: 450, loss is 4.533063716888428 and perplexity is 93.04318282984278
At time: 131.1856517791748 and batch: 500, loss is 4.559526357650757 and perplexity is 95.53821816589738
At time: 131.88894414901733 and batch: 550, loss is 4.59839674949646 and perplexity is 99.32494515233269
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.999640282164228 and perplexity of 148.35978184313586
Finished 15 epochs...
Completing Train Step...
At time: 133.70845317840576 and batch: 50, loss is 4.630531234741211 and perplexity is 102.56853761063847
At time: 134.41642212867737 and batch: 100, loss is 4.638403100967407 and perplexity is 103.37912966806933
At time: 135.11661744117737 and batch: 150, loss is 4.610013885498047 and perplexity is 100.48554491853994
At time: 135.81957459449768 and batch: 200, loss is 4.572592868804931 and perplexity is 96.79476079555683
At time: 136.517254114151 and batch: 250, loss is 4.550792722702027 and perplexity is 94.70745531514315
At time: 137.22634768486023 and batch: 300, loss is 4.538621339797974 and perplexity is 93.56172134038613
At time: 137.9277527332306 and batch: 350, loss is 4.583701648712158 and perplexity is 97.8760271449062
At time: 138.62980890274048 and batch: 400, loss is 4.582696304321289 and perplexity is 97.77767747594321
At time: 139.3285210132599 and batch: 450, loss is 4.53046070098877 and perplexity is 92.80130488821999
At time: 140.02606391906738 and batch: 500, loss is 4.556623802185059 and perplexity is 95.2613152460236
At time: 140.73483872413635 and batch: 550, loss is 4.594288015365601 and perplexity is 98.91768259985447
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.999777611265791 and perplexity of 148.38015735772794
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 142.54994344711304 and batch: 50, loss is 4.622406492233276 and perplexity is 101.73857085184274
At time: 143.27138328552246 and batch: 100, loss is 4.62578462600708 and perplexity is 102.08283851785842
At time: 143.9804449081421 and batch: 150, loss is 4.590039272308349 and perplexity is 98.498298341534
At time: 144.68800115585327 and batch: 200, loss is 4.550683221817017 and perplexity is 94.69708533274057
At time: 145.39874172210693 and batch: 250, loss is 4.522039966583252 and perplexity is 92.0231307444392
At time: 146.10342383384705 and batch: 300, loss is 4.507080345153809 and perplexity is 90.65674533188486
At time: 146.80583477020264 and batch: 350, loss is 4.547473392486572 and perplexity is 94.39361116143793
At time: 147.5193591117859 and batch: 400, loss is 4.544900255203247 and perplexity is 94.15103566511927
At time: 148.22319793701172 and batch: 450, loss is 4.488229169845581 and perplexity is 88.96376658771759
At time: 148.9265444278717 and batch: 500, loss is 4.510705690383912 and perplexity is 90.98600380835023
At time: 149.62847185134888 and batch: 550, loss is 4.554314775466919 and perplexity is 95.04160807639376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.988240018804023 and perplexity of 146.6780455901603
Finished 17 epochs...
Completing Train Step...
At time: 151.45331048965454 and batch: 50, loss is 4.611747312545776 and perplexity is 100.65988033521775
At time: 152.1749403476715 and batch: 100, loss is 4.617208528518677 and perplexity is 101.21110950227265
At time: 152.8830533027649 and batch: 150, loss is 4.584351053237915 and perplexity is 97.93960892280987
At time: 153.58480525016785 and batch: 200, loss is 4.54534948348999 and perplexity is 94.1933404751134
At time: 154.28622841835022 and batch: 250, loss is 4.518837642669678 and perplexity is 91.72891421200264
At time: 154.98897910118103 and batch: 300, loss is 4.504083032608032 and perplexity is 90.38542554996087
At time: 155.6967384815216 and batch: 350, loss is 4.545136594772339 and perplexity is 94.17328990999367
At time: 156.40966320037842 and batch: 400, loss is 4.542923288345337 and perplexity is 93.96508605664177
At time: 157.11331176757812 and batch: 450, loss is 4.487295036315918 and perplexity is 88.88070135347338
At time: 157.82101559638977 and batch: 500, loss is 4.512228727340698 and perplexity is 91.1246844357536
At time: 158.5307059288025 and batch: 550, loss is 4.556040658950805 and perplexity is 95.20578044849626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.986550189079122 and perplexity of 146.4303939721958
Finished 18 epochs...
Completing Train Step...
At time: 160.36571335792542 and batch: 50, loss is 4.6077502250671385 and perplexity is 100.25833702441011
At time: 161.0720603466034 and batch: 100, loss is 4.6137181091308594 and perplexity is 100.85845609557948
At time: 161.76439833641052 and batch: 150, loss is 4.5821458435058595 and perplexity is 97.72386950681692
At time: 162.46261310577393 and batch: 200, loss is 4.543250827789307 and perplexity is 93.99586836961699
At time: 163.16384935379028 and batch: 250, loss is 4.517543001174927 and perplexity is 91.61023499349221
At time: 163.86763834953308 and batch: 300, loss is 4.5028462219238286 and perplexity is 90.27370499277534
At time: 164.5743124485016 and batch: 350, loss is 4.544034738540649 and perplexity is 94.06958162994748
At time: 165.281840801239 and batch: 400, loss is 4.542020988464356 and perplexity is 93.88033960978058
At time: 165.98744320869446 and batch: 450, loss is 4.48658706665039 and perplexity is 88.8177987822478
At time: 166.69364380836487 and batch: 500, loss is 4.512559680938721 and perplexity is 91.15484746894316
At time: 167.39502692222595 and batch: 550, loss is 4.5564146327972415 and perplexity is 95.24139157881419
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.985738876018118 and perplexity of 146.3116412603651
Finished 19 epochs...
Completing Train Step...
At time: 169.20368885993958 and batch: 50, loss is 4.604933624267578 and perplexity is 99.97634662580042
At time: 169.91452193260193 and batch: 100, loss is 4.611274824142456 and perplexity is 100.61233094323254
At time: 170.6297960281372 and batch: 150, loss is 4.580671701431275 and perplexity is 97.57991676854837
At time: 171.33062195777893 and batch: 200, loss is 4.541832189559937 and perplexity is 93.86261677759408
At time: 172.03801321983337 and batch: 250, loss is 4.516546058654785 and perplexity is 91.5189503652713
At time: 172.7419867515564 and batch: 300, loss is 4.5019127559661865 and perplexity is 90.1894768804482
At time: 173.44916367530823 and batch: 350, loss is 4.543114194869995 and perplexity is 93.98302631706191
At time: 174.15664887428284 and batch: 400, loss is 4.541090593338013 and perplexity is 93.79303441980774
At time: 174.86290979385376 and batch: 450, loss is 4.485759000778199 and perplexity is 88.74428223670508
At time: 175.5646150112152 and batch: 500, loss is 4.512429094314575 and perplexity is 91.14294464232952
At time: 176.26595878601074 and batch: 550, loss is 4.556408672332764 and perplexity is 95.24082389757471
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.985387274559508 and perplexity of 146.2602069166113
Finished 20 epochs...
Completing Train Step...
At time: 178.10912013053894 and batch: 50, loss is 4.60268479347229 and perplexity is 99.75176935149817
At time: 178.82216119766235 and batch: 100, loss is 4.609411039352417 and perplexity is 100.42498585082862
At time: 179.5204553604126 and batch: 150, loss is 4.579618787765503 and perplexity is 97.47722761157323
At time: 180.2225217819214 and batch: 200, loss is 4.540720233917236 and perplexity is 93.75830371772545
At time: 180.92563652992249 and batch: 250, loss is 4.51580020904541 and perplexity is 91.45071644117692
At time: 181.6288139820099 and batch: 300, loss is 4.501084299087524 and perplexity is 90.11478972975725
At time: 182.32930088043213 and batch: 350, loss is 4.542262573242187 and perplexity is 93.90302241056732
At time: 183.02701878547668 and batch: 400, loss is 4.540189752578735 and perplexity is 93.7085798772245
At time: 183.7282679080963 and batch: 450, loss is 4.484899196624756 and perplexity is 88.66801232753463
At time: 184.43360328674316 and batch: 500, loss is 4.512097015380859 and perplexity is 91.1126830153595
At time: 185.14059567451477 and batch: 550, loss is 4.556160154342652 and perplexity is 95.21715778029281
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.985053204475565 and perplexity of 146.21135391762627
Finished 21 epochs...
Completing Train Step...
At time: 186.96686267852783 and batch: 50, loss is 4.600830192565918 and perplexity is 99.56694107396932
At time: 187.69563341140747 and batch: 100, loss is 4.6078305912017825 and perplexity is 100.26639472320132
At time: 188.39765548706055 and batch: 150, loss is 4.5786326885223385 and perplexity is 97.38115276864987
At time: 189.09816074371338 and batch: 200, loss is 4.539793148040771 and perplexity is 93.67142199817782
At time: 189.79424619674683 and batch: 250, loss is 4.515186367034912 and perplexity is 91.39459737541739
At time: 190.49778175354004 and batch: 300, loss is 4.500329809188843 and perplexity is 90.04682467388254
At time: 191.207537651062 and batch: 350, loss is 4.54140115737915 and perplexity is 93.82216768724615
At time: 191.91342568397522 and batch: 400, loss is 4.539303092956543 and perplexity is 93.62552908752629
At time: 192.61378169059753 and batch: 450, loss is 4.484053220748901 and perplexity is 88.59303304794814
At time: 193.3179304599762 and batch: 500, loss is 4.511731386184692 and perplexity is 91.07937564775207
At time: 194.0269901752472 and batch: 550, loss is 4.555743141174316 and perplexity is 95.17745924962728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.984769131274933 and perplexity of 146.16982508924303
Finished 22 epochs...
Completing Train Step...
At time: 195.84664964675903 and batch: 50, loss is 4.599063215255737 and perplexity is 99.39116389112876
At time: 196.55700659751892 and batch: 100, loss is 4.606415319442749 and perplexity is 100.12459089551513
At time: 197.25552248954773 and batch: 150, loss is 4.577738313674927 and perplexity is 97.29409645129499
At time: 197.95500707626343 and batch: 200, loss is 4.538888568878174 and perplexity is 93.58672709410534
At time: 198.65633416175842 and batch: 250, loss is 4.5144744300842286 and perplexity is 91.32955334082617
At time: 199.3586618900299 and batch: 300, loss is 4.4995487022399905 and perplexity is 89.9765159363042
At time: 200.0676465034485 and batch: 350, loss is 4.54048677444458 and perplexity is 93.73641750845323
At time: 200.78149604797363 and batch: 400, loss is 4.538421726226806 and perplexity is 93.5430470149512
At time: 201.49525332450867 and batch: 450, loss is 4.483184976577759 and perplexity is 88.5161460465759
At time: 202.20864701271057 and batch: 500, loss is 4.511269636154175 and perplexity is 91.03732945143051
At time: 202.91711974143982 and batch: 550, loss is 4.555198020935059 and perplexity is 95.12559022898012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.984532133061835 and perplexity of 146.1351872066083
Finished 23 epochs...
Completing Train Step...
At time: 204.74468398094177 and batch: 50, loss is 4.597577676773072 and perplexity is 99.24362410747761
At time: 205.45930004119873 and batch: 100, loss is 4.605120105743408 and perplexity is 99.99499210093113
At time: 206.15762567520142 and batch: 150, loss is 4.576968259811402 and perplexity is 97.21920359589164
At time: 206.8600194454193 and batch: 200, loss is 4.5380939102172855 and perplexity is 93.51238713223512
At time: 207.56175827980042 and batch: 250, loss is 4.513755035400391 and perplexity is 91.26387497284242
At time: 208.26376819610596 and batch: 300, loss is 4.498875780105591 and perplexity is 89.91598911435759
At time: 208.9664945602417 and batch: 350, loss is 4.53969575881958 and perplexity is 93.6622998555479
At time: 209.66608500480652 and batch: 400, loss is 4.5376409149169925 and perplexity is 93.47003605348907
At time: 210.36343574523926 and batch: 450, loss is 4.482288455963134 and perplexity is 88.43682505868131
At time: 211.06229901313782 and batch: 500, loss is 4.510825042724609 and perplexity is 90.99686384894822
At time: 211.76096272468567 and batch: 550, loss is 4.5546894359588626 and perplexity is 95.0772230833822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.98439772585605 and perplexity of 146.11554690435767
Finished 24 epochs...
Completing Train Step...
At time: 213.55277156829834 and batch: 50, loss is 4.596200399398803 and perplexity is 99.107032193522
At time: 214.2676763534546 and batch: 100, loss is 4.603886680603027 and perplexity is 99.8717317955681
At time: 214.97070956230164 and batch: 150, loss is 4.576079978942871 and perplexity is 97.13288398099067
At time: 215.6724100112915 and batch: 200, loss is 4.537289323806763 and perplexity is 93.43717859627324
At time: 216.37418293952942 and batch: 250, loss is 4.513064079284668 and perplexity is 91.20083742088431
At time: 217.0768563747406 and batch: 300, loss is 4.497998809814453 and perplexity is 89.83717002925552
At time: 217.78471207618713 and batch: 350, loss is 4.538875637054443 and perplexity is 93.58551685487232
At time: 218.49343705177307 and batch: 400, loss is 4.536848993301391 and perplexity is 93.39604441318389
At time: 219.20217490196228 and batch: 450, loss is 4.481523666381836 and perplexity is 88.36921535316795
At time: 219.91034293174744 and batch: 500, loss is 4.510157356262207 and perplexity is 90.93612675375972
At time: 220.61658906936646 and batch: 550, loss is 4.554178161621094 and perplexity is 95.0286249636577
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.984339612595578 and perplexity of 146.10705590024372
Finished 25 epochs...
Completing Train Step...
At time: 222.4160611629486 and batch: 50, loss is 4.594795989990234 and perplexity is 98.96794303697531
At time: 223.12700843811035 and batch: 100, loss is 4.602741603851318 and perplexity is 99.75743644829724
At time: 223.82025790214539 and batch: 150, loss is 4.57528678894043 and perplexity is 97.05586969602692
At time: 224.51714658737183 and batch: 200, loss is 4.536536712646484 and perplexity is 93.36688318874889
At time: 225.20745491981506 and batch: 250, loss is 4.512387676239014 and perplexity is 91.13916975513628
At time: 225.89887046813965 and batch: 300, loss is 4.497265567779541 and perplexity is 89.77132178419342
At time: 226.58944821357727 and batch: 350, loss is 4.53796953201294 and perplexity is 93.50075695272494
At time: 227.28095483779907 and batch: 400, loss is 4.535950555801391 and perplexity is 93.31217158741913
At time: 227.97333002090454 and batch: 450, loss is 4.480565481185913 and perplexity is 88.28458183300953
At time: 228.6789345741272 and batch: 500, loss is 4.509493770599366 and perplexity is 90.87580286106041
At time: 229.3755202293396 and batch: 550, loss is 4.553522005081176 and perplexity is 94.96629176231373
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.984177934362533 and perplexity of 146.08343547911576
Finished 26 epochs...
Completing Train Step...
At time: 231.1849958896637 and batch: 50, loss is 4.593416109085083 and perplexity is 98.83147323984409
At time: 231.8999195098877 and batch: 100, loss is 4.601558628082276 and perplexity is 99.63949559253857
At time: 232.6015167236328 and batch: 150, loss is 4.5745582771301265 and perplexity is 96.98518909764549
At time: 233.30314326286316 and batch: 200, loss is 4.535771141052246 and perplexity is 93.29543150931492
At time: 234.00547814369202 and batch: 250, loss is 4.511692495346069 and perplexity is 91.07583356332961
At time: 234.70736479759216 and batch: 300, loss is 4.496669569015503 and perplexity is 89.71783412823568
At time: 235.40846371650696 and batch: 350, loss is 4.53710054397583 and perplexity is 93.41954120633913
At time: 236.10916996002197 and batch: 400, loss is 4.535060224533081 and perplexity is 93.22912981616918
At time: 236.81215119361877 and batch: 450, loss is 4.479573411941528 and perplexity is 88.19704084516032
At time: 237.5179009437561 and batch: 500, loss is 4.508803329467773 and perplexity is 90.81308012456894
At time: 238.2321219444275 and batch: 550, loss is 4.552725667953491 and perplexity is 94.89069668188434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.984124690928358 and perplexity of 146.07565770239435
Finished 27 epochs...
Completing Train Step...
At time: 240.06486868858337 and batch: 50, loss is 4.592091417312622 and perplexity is 98.70063867724984
At time: 240.77882862091064 and batch: 100, loss is 4.600374422073364 and perplexity is 99.52157173998117
At time: 241.47880840301514 and batch: 150, loss is 4.573821020126343 and perplexity is 96.91371243929105
At time: 242.19249653816223 and batch: 200, loss is 4.53498218536377 and perplexity is 93.2218545762033
At time: 242.90210270881653 and batch: 250, loss is 4.511043519973755 and perplexity is 91.01674676534616
At time: 243.61167073249817 and batch: 300, loss is 4.495969333648682 and perplexity is 89.6550325182648
At time: 244.32684898376465 and batch: 350, loss is 4.536364345550537 and perplexity is 93.35079119714084
At time: 245.0491259098053 and batch: 400, loss is 4.534264469146729 and perplexity is 93.15497174370911
At time: 245.75723266601562 and batch: 450, loss is 4.478780097961426 and perplexity is 88.1271006455913
At time: 246.45898866653442 and batch: 500, loss is 4.508279180526733 and perplexity is 90.76549301724447
At time: 247.1582407951355 and batch: 550, loss is 4.552229061126709 and perplexity is 94.84358501307035
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9839513251122005 and perplexity of 146.05033537185275
Finished 28 epochs...
Completing Train Step...
At time: 249.00393652915955 and batch: 50, loss is 4.59091251373291 and perplexity is 98.584348701796
At time: 249.70300149917603 and batch: 100, loss is 4.599352149963379 and perplexity is 99.41988559715891
At time: 250.39991998672485 and batch: 150, loss is 4.573150997161865 and perplexity is 96.84879977530014
At time: 251.0960555076599 and batch: 200, loss is 4.534282207489014 and perplexity is 93.15662417313906
At time: 251.79610562324524 and batch: 250, loss is 4.510415802001953 and perplexity is 90.95963184557283
At time: 252.50004172325134 and batch: 300, loss is 4.495200443267822 and perplexity is 89.58612412106258
At time: 253.20594787597656 and batch: 350, loss is 4.535507106781006 and perplexity is 93.27080156976353
At time: 253.91289806365967 and batch: 400, loss is 4.533452339172364 and perplexity is 93.07934851099839
At time: 254.61534547805786 and batch: 450, loss is 4.4779348564147945 and perplexity is 88.05264343034025
At time: 255.31659626960754 and batch: 500, loss is 4.507637186050415 and perplexity is 90.70724077289742
At time: 256.01739478111267 and batch: 550, loss is 4.551604976654053 and perplexity is 94.78441307039854
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.983869836685505 and perplexity of 146.03843444470547
Finished 29 epochs...
Completing Train Step...
At time: 257.8337161540985 and batch: 50, loss is 4.589680824279785 and perplexity is 98.46299814769698
At time: 258.5627815723419 and batch: 100, loss is 4.598443517684936 and perplexity is 99.3295905087146
At time: 259.27157068252563 and batch: 150, loss is 4.57243055343628 and perplexity is 96.77905079329663
At time: 259.98360204696655 and batch: 200, loss is 4.533636722564697 and perplexity is 93.09651237935057
At time: 260.685795545578 and batch: 250, loss is 4.5098063755035405 and perplexity is 90.90421552344301
At time: 261.3883707523346 and batch: 300, loss is 4.494509868621826 and perplexity is 89.52427957170816
At time: 262.0957190990448 and batch: 350, loss is 4.534700269699097 and perplexity is 93.19557757923094
At time: 262.8034098148346 and batch: 400, loss is 4.532634410858154 and perplexity is 93.00324740325117
At time: 263.51336789131165 and batch: 450, loss is 4.477130260467529 and perplexity is 87.98182512417176
At time: 264.2230951786041 and batch: 500, loss is 4.5071381568908695 and perplexity is 90.66198650730803
At time: 264.92366552352905 and batch: 550, loss is 4.55097469329834 and perplexity is 94.72469085539535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9838503573803195 and perplexity of 146.03558974517856
Finished 30 epochs...
Completing Train Step...
At time: 266.75576186180115 and batch: 50, loss is 4.588484306335449 and perplexity is 98.3452558579885
At time: 267.4662551879883 and batch: 100, loss is 4.5974948024749756 and perplexity is 99.2353997025898
At time: 268.1623294353485 and batch: 150, loss is 4.571820936203003 and perplexity is 96.72007059561193
At time: 268.85863614082336 and batch: 200, loss is 4.532989377975464 and perplexity is 93.03626635785719
At time: 269.55460357666016 and batch: 250, loss is 4.509157590866089 and perplexity is 90.84525739256463
At time: 270.2510025501251 and batch: 300, loss is 4.493812713623047 and perplexity is 89.46188902316102
At time: 270.94800424575806 and batch: 350, loss is 4.533925142288208 and perplexity is 93.12336712224342
At time: 271.6461431980133 and batch: 400, loss is 4.531915254592896 and perplexity is 92.9363875794039
At time: 272.34763765335083 and batch: 450, loss is 4.476325016021729 and perplexity is 87.911006765031
At time: 273.04975986480713 and batch: 500, loss is 4.506490659713745 and perplexity is 90.60330212800734
At time: 273.7519357204437 and batch: 550, loss is 4.55038369178772 and perplexity is 94.66872495960025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.983847435484541 and perplexity of 146.03516304502872
Finished 31 epochs...
Completing Train Step...
At time: 275.55609679222107 and batch: 50, loss is 4.58740930557251 and perplexity is 98.23959143775163
At time: 276.2651152610779 and batch: 100, loss is 4.5966643047332765 and perplexity is 99.15301914041062
At time: 276.96067786216736 and batch: 150, loss is 4.571185846328735 and perplexity is 96.65866415950502
At time: 277.656054019928 and batch: 200, loss is 4.53236795425415 and perplexity is 92.97846937506795
At time: 278.35401153564453 and batch: 250, loss is 4.508532543182373 and perplexity is 90.7884925170799
At time: 279.05329275131226 and batch: 300, loss is 4.493052530288696 and perplexity is 89.39390742857744
At time: 279.7563180923462 and batch: 350, loss is 4.533107652664184 and perplexity is 93.04727084406362
At time: 280.4605782032013 and batch: 400, loss is 4.531120376586914 and perplexity is 92.86254384123139
At time: 281.168151140213 and batch: 450, loss is 4.475511331558227 and perplexity is 87.8395040389292
At time: 281.8772540092468 and batch: 500, loss is 4.505905885696411 and perplexity is 90.55033515940107
At time: 282.5859811306 and batch: 550, loss is 4.549811630249024 and perplexity is 94.61458411055868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.983850032725233 and perplexity of 146.03554233398918
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 284.392893075943 and batch: 50, loss is 4.586612310409546 and perplexity is 98.16132615123443
At time: 285.10716438293457 and batch: 100, loss is 4.594164609909058 and perplexity is 98.90547637124614
At time: 285.8079481124878 and batch: 150, loss is 4.568332672119141 and perplexity is 96.38327320779246
At time: 286.5065941810608 and batch: 200, loss is 4.5285125350952145 and perplexity is 92.62068854358573
At time: 287.2026617527008 and batch: 250, loss is 4.50323489189148 and perplexity is 90.30879849022712
At time: 287.89945220947266 and batch: 300, loss is 4.486835098266601 and perplexity is 88.83983113667536
At time: 288.59555172920227 and batch: 350, loss is 4.524705543518066 and perplexity is 92.2687526958904
At time: 289.29244565963745 and batch: 400, loss is 4.522845916748047 and perplexity is 92.09732669693146
At time: 289.9888334274292 and batch: 450, loss is 4.466247358322144 and perplexity is 87.02951885777837
At time: 290.68854331970215 and batch: 500, loss is 4.496049127578735 and perplexity is 89.6621867310861
At time: 291.39351534843445 and batch: 550, loss is 4.541020622253418 and perplexity is 93.78647184905972
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.980575886178524 and perplexity of 145.55818246688273
Finished 33 epochs...
Completing Train Step...
At time: 293.2058434486389 and batch: 50, loss is 4.584852209091187 and perplexity is 97.98870423226195
At time: 293.9165143966675 and batch: 100, loss is 4.5934379768371585 and perplexity is 98.83363448562888
At time: 294.61991453170776 and batch: 150, loss is 4.567081489562988 and perplexity is 96.26275554816883
At time: 295.31425380706787 and batch: 200, loss is 4.527442951202392 and perplexity is 92.52167590757702
At time: 296.0211524963379 and batch: 250, loss is 4.502272710800171 and perplexity is 90.22194686213872
At time: 296.72861766815186 and batch: 300, loss is 4.48640214920044 and perplexity is 88.80137633983165
At time: 297.42585349082947 and batch: 350, loss is 4.524531002044678 and perplexity is 92.25264937723652
At time: 298.12361788749695 and batch: 400, loss is 4.522784481048584 and perplexity is 92.09166880704717
At time: 298.8220160007477 and batch: 450, loss is 4.466489429473877 and perplexity is 87.05058874374596
At time: 299.522353887558 and batch: 500, loss is 4.4962249183654786 and perplexity is 89.67794990290227
At time: 300.228312253952 and batch: 550, loss is 4.541415452957153 and perplexity is 93.8235089389497
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.980174287836602 and perplexity of 145.49973827848092
Finished 34 epochs...
Completing Train Step...
At time: 302.04667043685913 and batch: 50, loss is 4.5842500591278075 and perplexity is 97.92971809862831
At time: 302.76862621307373 and batch: 100, loss is 4.59272931098938 and perplexity is 98.7636192758813
At time: 303.4659614562988 and batch: 150, loss is 4.56632287979126 and perplexity is 96.1897573732217
At time: 304.1628625392914 and batch: 200, loss is 4.526734504699707 and perplexity is 92.45615246252598
At time: 304.863703250885 and batch: 250, loss is 4.501685047149659 and perplexity is 90.16894227944806
At time: 305.5620307922363 and batch: 300, loss is 4.486133432388305 and perplexity is 88.7775171228984
At time: 306.25940918922424 and batch: 350, loss is 4.524469480514527 and perplexity is 92.24697402766623
At time: 306.9689199924469 and batch: 400, loss is 4.522810773849487 and perplexity is 92.09409018679229
At time: 307.6683533191681 and batch: 450, loss is 4.466713180541992 and perplexity is 87.07006858519385
At time: 308.36432003974915 and batch: 500, loss is 4.49636944770813 and perplexity is 89.69091193472644
At time: 309.0669798851013 and batch: 550, loss is 4.541679754257202 and perplexity is 93.84830989165503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.979908395320811 and perplexity of 145.46105612989783
Finished 35 epochs...
Completing Train Step...
At time: 310.87281346321106 and batch: 50, loss is 4.583832683563233 and perplexity is 97.88885315585577
At time: 311.58720803260803 and batch: 100, loss is 4.5921656608581545 and perplexity is 98.7079668346424
At time: 312.28101682662964 and batch: 150, loss is 4.565751333236694 and perplexity is 96.13479615675361
At time: 312.97242975234985 and batch: 200, loss is 4.526162338256836 and perplexity is 92.4032672856545
At time: 313.66575932502747 and batch: 250, loss is 4.501205339431762 and perplexity is 90.12569791507903
At time: 314.3618280887604 and batch: 300, loss is 4.485923776626587 and perplexity is 88.75890635591794
At time: 315.0569508075714 and batch: 350, loss is 4.524446926116943 and perplexity is 92.24489347620101
At time: 315.75256729125977 and batch: 400, loss is 4.522851285934448 and perplexity is 92.09782118597305
At time: 316.45495772361755 and batch: 450, loss is 4.466898269653321 and perplexity is 87.08618579832589
At time: 317.1623065471649 and batch: 500, loss is 4.496473178863526 and perplexity is 89.7002161592106
At time: 317.87270283699036 and batch: 550, loss is 4.541864442825317 and perplexity is 93.8656442023042
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.979724965196975 and perplexity of 145.43437663734474
Finished 36 epochs...
Completing Train Step...
At time: 319.72501134872437 and batch: 50, loss is 4.583507041931153 and perplexity is 97.85698165957633
At time: 320.44672775268555 and batch: 100, loss is 4.591666469573974 and perplexity is 98.658704974488
At time: 321.1509609222412 and batch: 150, loss is 4.565265140533447 and perplexity is 96.08806748082522
At time: 321.8547992706299 and batch: 200, loss is 4.525654067993164 and perplexity is 92.35631338627331
At time: 322.55919551849365 and batch: 250, loss is 4.50079309463501 and perplexity is 90.08855172224827
At time: 323.2633764743805 and batch: 300, loss is 4.485728340148926 and perplexity is 88.74156132288
At time: 323.9683349132538 and batch: 350, loss is 4.524421424865722 and perplexity is 92.24254114599245
At time: 324.68459963798523 and batch: 400, loss is 4.522886333465576 and perplexity is 92.10104904379178
At time: 325.3936369419098 and batch: 450, loss is 4.467038021087647 and perplexity is 87.0983570681576
At time: 326.1016561985016 and batch: 500, loss is 4.4965380096435545 and perplexity is 89.70603168270337
At time: 326.81669187545776 and batch: 550, loss is 4.5419970703125 and perplexity is 93.87809419241476
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.979584389544548 and perplexity of 145.41393354189802
Finished 37 epochs...
Completing Train Step...
At time: 328.6400761604309 and batch: 50, loss is 4.583240232467651 and perplexity is 97.83087597357677
At time: 329.3452994823456 and batch: 100, loss is 4.591219959259033 and perplexity is 98.6146626784596
At time: 330.03848910331726 and batch: 150, loss is 4.564826574325561 and perplexity is 96.04593574090109
At time: 330.72966289520264 and batch: 200, loss is 4.525212993621826 and perplexity is 92.31558636589011
At time: 331.4249973297119 and batch: 250, loss is 4.500389547348022 and perplexity is 90.0522040661044
At time: 332.1272442340851 and batch: 300, loss is 4.485544261932373 and perplexity is 88.72522743793986
At time: 332.8290903568268 and batch: 350, loss is 4.524382848739624 and perplexity is 92.23898285472652
At time: 333.5370819568634 and batch: 400, loss is 4.522910022735596 and perplexity is 92.10323087625463
At time: 334.23930263519287 and batch: 450, loss is 4.467135667800903 and perplexity is 87.10686235170508
At time: 334.95713686943054 and batch: 500, loss is 4.496582803726196 and perplexity is 89.71005007209936
At time: 335.6639893054962 and batch: 550, loss is 4.5421094226837155 and perplexity is 93.88864221143884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.979470110954122 and perplexity of 145.39731679203211
Finished 38 epochs...
Completing Train Step...
At time: 337.4802043437958 and batch: 50, loss is 4.582997522354126 and perplexity is 97.80713431185019
At time: 338.17933177948 and batch: 100, loss is 4.5908145999908445 and perplexity is 98.57469641185916
At time: 338.8657832145691 and batch: 150, loss is 4.564413404464721 and perplexity is 96.00626065183668
At time: 339.56222200393677 and batch: 200, loss is 4.52481240272522 and perplexity is 92.27861298847071
At time: 340.262802362442 and batch: 250, loss is 4.500029754638672 and perplexity is 90.01980976758617
At time: 340.9665334224701 and batch: 300, loss is 4.485369186401368 and perplexity is 88.70969518133134
At time: 341.67266058921814 and batch: 350, loss is 4.524348316192627 and perplexity is 92.23579766271284
At time: 342.3791449069977 and batch: 400, loss is 4.522941522598266 and perplexity is 92.10613216107352
At time: 343.08600425720215 and batch: 450, loss is 4.467227392196655 and perplexity is 87.11485254246222
At time: 343.80307483673096 and batch: 500, loss is 4.496620187759399 and perplexity is 89.71340385827855
At time: 344.50465989112854 and batch: 550, loss is 4.542199649810791 and perplexity is 93.89711389607274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.979394141663897 and perplexity of 145.38627148063225
Finished 39 epochs...
Completing Train Step...
At time: 346.3074164390564 and batch: 50, loss is 4.582800464630127 and perplexity is 97.78786255945803
At time: 347.01165866851807 and batch: 100, loss is 4.590456981658935 and perplexity is 98.53945059601045
At time: 347.70202589035034 and batch: 150, loss is 4.564034614562988 and perplexity is 95.96990133650411
At time: 348.3951199054718 and batch: 200, loss is 4.524447603225708 and perplexity is 92.24495593604803
At time: 349.09468603134155 and batch: 250, loss is 4.499693593978882 and perplexity is 89.98955373466941
At time: 349.7971487045288 and batch: 300, loss is 4.485215406417847 and perplexity is 88.69605445473049
At time: 350.5016739368439 and batch: 350, loss is 4.524321098327636 and perplexity is 92.23328723538917
At time: 351.203120470047 and batch: 400, loss is 4.522953929901123 and perplexity is 92.10727495683973
At time: 351.9008741378784 and batch: 450, loss is 4.467297706604004 and perplexity is 87.12097818704805
At time: 352.60283970832825 and batch: 500, loss is 4.496647043228149 and perplexity is 89.71581318614402
At time: 353.3043475151062 and batch: 550, loss is 4.542274122238159 and perplexity is 93.90410690245727
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.979342196850067 and perplexity of 145.37871961396863
Finished 40 epochs...
Completing Train Step...
At time: 355.11869502067566 and batch: 50, loss is 4.582629261016845 and perplexity is 97.77112235708522
At time: 355.8276128768921 and batch: 100, loss is 4.590119581222535 and perplexity is 98.50620895056488
At time: 356.5240902900696 and batch: 150, loss is 4.56367971420288 and perplexity is 95.93584762715454
At time: 357.22251987457275 and batch: 200, loss is 4.52409740447998 and perplexity is 92.21265752394133
At time: 357.9217290878296 and batch: 250, loss is 4.499367914199829 and perplexity is 89.96025072864933
At time: 358.63041472435 and batch: 300, loss is 4.485067844390869 and perplexity is 88.68296725076111
At time: 359.32818579673767 and batch: 350, loss is 4.52429165840149 and perplexity is 92.23057193419403
At time: 360.0259165763855 and batch: 400, loss is 4.522973537445068 and perplexity is 92.1090809719868
At time: 360.72375559806824 and batch: 450, loss is 4.467365169525147 and perplexity is 87.12685582098831
At time: 361.4267942905426 and batch: 500, loss is 4.4966673660278325 and perplexity is 89.71763648117097
At time: 362.1328718662262 and batch: 550, loss is 4.5423232936859135 and perplexity is 93.90872441686776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.979295771172706 and perplexity of 145.37197046510477
Finished 41 epochs...
Completing Train Step...
At time: 363.9678375720978 and batch: 50, loss is 4.582469387054443 and perplexity is 97.75549254977841
At time: 364.6816713809967 and batch: 100, loss is 4.589807815551758 and perplexity is 98.47550288305334
At time: 365.38083052635193 and batch: 150, loss is 4.563344440460205 and perplexity is 95.90368824786283
At time: 366.0798387527466 and batch: 200, loss is 4.523773574829102 and perplexity is 92.18280116569672
At time: 366.7790586948395 and batch: 250, loss is 4.499018707275391 and perplexity is 89.92884147065504
At time: 367.4782259464264 and batch: 300, loss is 4.484917707443238 and perplexity is 88.6696536602071
At time: 368.19585275650024 and batch: 350, loss is 4.5242581558227535 and perplexity is 92.22748202395618
At time: 368.91141629219055 and batch: 400, loss is 4.522950992584229 and perplexity is 92.10700440898222
At time: 369.63428139686584 and batch: 450, loss is 4.467396469116211 and perplexity is 87.12958289862418
At time: 370.3494243621826 and batch: 500, loss is 4.496656856536865 and perplexity is 89.71669359943539
At time: 371.06203150749207 and batch: 550, loss is 4.542350587844848 and perplexity is 93.91128761149731
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.97926622755984 and perplexity of 145.36767571532937
Finished 42 epochs...
Completing Train Step...
At time: 372.9677402973175 and batch: 50, loss is 4.5823071098327635 and perplexity is 97.73963034711538
At time: 373.6766140460968 and batch: 100, loss is 4.5894992065429685 and perplexity is 98.445117144616
At time: 374.37532210350037 and batch: 150, loss is 4.563023052215576 and perplexity is 95.87287088227819
At time: 375.0715367794037 and batch: 200, loss is 4.523481330871582 and perplexity is 92.15586523519195
At time: 375.76903200149536 and batch: 250, loss is 4.498759145736694 and perplexity is 89.90550243127927
At time: 376.4785828590393 and batch: 300, loss is 4.484790468215943 and perplexity is 88.65837211973334
At time: 377.1902084350586 and batch: 350, loss is 4.524223289489746 and perplexity is 92.22426644591354
At time: 377.9047076702118 and batch: 400, loss is 4.522945661544799 and perplexity is 92.10651338421883
At time: 378.6192021369934 and batch: 450, loss is 4.4674405670166015 and perplexity is 87.13342521501036
At time: 379.33041167259216 and batch: 500, loss is 4.496666383743286 and perplexity is 89.71754835296642
At time: 380.0409731864929 and batch: 550, loss is 4.542374677658081 and perplexity is 93.91354994412583
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9792162306765295 and perplexity of 145.3604079662934
Finished 43 epochs...
Completing Train Step...
At time: 381.8773148059845 and batch: 50, loss is 4.582158784866333 and perplexity is 97.72513419482249
At time: 382.58079504966736 and batch: 100, loss is 4.589231100082397 and perplexity is 98.41872691055205
At time: 383.2769558429718 and batch: 150, loss is 4.562691106796264 and perplexity is 95.84105160337666
At time: 383.9717171192169 and batch: 200, loss is 4.52317096710205 and perplexity is 92.12726783150167
At time: 384.6672260761261 and batch: 250, loss is 4.4984373855590825 and perplexity is 89.87657907429046
At time: 385.3695638179779 and batch: 300, loss is 4.484662876129151 and perplexity is 88.64706073465997
At time: 386.0707960128784 and batch: 350, loss is 4.524187536239624 and perplexity is 92.22096918759216
At time: 386.7714283466339 and batch: 400, loss is 4.522895488739014 and perplexity is 92.10189225793962
At time: 387.46882462501526 and batch: 450, loss is 4.467471904754639 and perplexity is 87.13615582224935
At time: 388.16404962539673 and batch: 500, loss is 4.49665979385376 and perplexity is 89.71695712618225
At time: 388.8588843345642 and batch: 550, loss is 4.5424026775360105 and perplexity is 93.9161795488743
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.979178570686503 and perplexity of 145.35493379785828
Finished 44 epochs...
Completing Train Step...
At time: 390.6446805000305 and batch: 50, loss is 4.582008638381958 and perplexity is 97.71046221098898
At time: 391.35610580444336 and batch: 100, loss is 4.588959131240845 and perplexity is 98.39196372294866
At time: 392.0508711338043 and batch: 150, loss is 4.562368898391724 and perplexity is 95.81017578554119
At time: 392.7544322013855 and batch: 200, loss is 4.5228539562225345 and perplexity is 92.09806711401612
At time: 393.46501636505127 and batch: 250, loss is 4.49814356803894 and perplexity is 89.85017563979375
At time: 394.1681299209595 and batch: 300, loss is 4.48453085899353 and perplexity is 88.6353585760803
At time: 394.8711359500885 and batch: 350, loss is 4.524154891967774 and perplexity is 92.21795875034076
At time: 395.5731463432312 and batch: 400, loss is 4.522856359481811 and perplexity is 92.09828844981627
At time: 396.27407717704773 and batch: 450, loss is 4.467489957809448 and perplexity is 87.13772891024576
At time: 396.97280287742615 and batch: 500, loss is 4.496635046005249 and perplexity is 89.71473685199204
At time: 397.67106199264526 and batch: 550, loss is 4.542396211624146 and perplexity is 93.9155722970979
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9791412353515625 and perplexity of 145.3495070240256
Finished 45 epochs...
Completing Train Step...
At time: 399.49483132362366 and batch: 50, loss is 4.581835994720459 and perplexity is 97.6935945751132
At time: 400.2094669342041 and batch: 100, loss is 4.5886718273162845 and perplexity is 98.36369938604773
At time: 400.91208839416504 and batch: 150, loss is 4.562051277160645 and perplexity is 95.77974927186744
At time: 401.61454916000366 and batch: 200, loss is 4.52249231338501 and perplexity is 92.06476652951638
At time: 402.31528902053833 and batch: 250, loss is 4.497858390808106 and perplexity is 89.82455606874761
At time: 403.01862502098083 and batch: 300, loss is 4.484414138793945 and perplexity is 88.62501364308011
At time: 403.72083258628845 and batch: 350, loss is 4.524102144241333 and perplexity is 92.21309459096744
At time: 404.4191961288452 and batch: 400, loss is 4.522785243988037 and perplexity is 92.0917390674414
At time: 405.1171832084656 and batch: 450, loss is 4.467454748153687 and perplexity is 87.13466087481953
At time: 405.8159775733948 and batch: 500, loss is 4.49656946182251 and perplexity is 89.7088531772361
At time: 406.51398754119873 and batch: 550, loss is 4.54234619140625 and perplexity is 93.91087473719526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.97907046054272 and perplexity of 145.33922030447516
Finished 46 epochs...
Completing Train Step...
At time: 408.3158404827118 and batch: 50, loss is 4.581670045852661 and perplexity is 97.67738377882122
At time: 409.02020955085754 and batch: 100, loss is 4.588401508331299 and perplexity is 98.33711340417999
At time: 409.71647810935974 and batch: 150, loss is 4.5617877388000485 and perplexity is 95.75451095954381
At time: 410.4140236377716 and batch: 200, loss is 4.522228145599366 and perplexity is 92.04044919607587
At time: 411.11247992515564 and batch: 250, loss is 4.497588624954224 and perplexity is 89.80032773881514
At time: 411.8131847381592 and batch: 300, loss is 4.484275388717651 and perplexity is 88.61271776872205
At time: 412.5151309967041 and batch: 350, loss is 4.52404218673706 and perplexity is 92.20756588969975
At time: 413.2159218788147 and batch: 400, loss is 4.522771091461181 and perplexity is 92.09043574585375
At time: 413.9254548549652 and batch: 450, loss is 4.4674489784240725 and perplexity is 87.13415813283662
At time: 414.62780261039734 and batch: 500, loss is 4.49653920173645 and perplexity is 89.70613862069017
At time: 415.3321125507355 and batch: 550, loss is 4.542332067489624 and perplexity is 93.90954835719701
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.979065266061337 and perplexity of 145.3384653445619
Finished 47 epochs...
Completing Train Step...
At time: 417.14608502388 and batch: 50, loss is 4.581566467285156 and perplexity is 97.66726701928053
At time: 417.86257123947144 and batch: 100, loss is 4.588200979232788 and perplexity is 98.31739592850873
At time: 418.5564239025116 and batch: 150, loss is 4.5615126895904545 and perplexity is 95.72817737867064
At time: 419.24820852279663 and batch: 200, loss is 4.521946821212769 and perplexity is 92.01455961501968
At time: 419.9455773830414 and batch: 250, loss is 4.4973071098327635 and perplexity is 89.77505114668296
At time: 420.64212107658386 and batch: 300, loss is 4.484148321151733 and perplexity is 88.60145868171332
At time: 421.34083795547485 and batch: 350, loss is 4.523999500274658 and perplexity is 92.20362995891128
At time: 422.0423436164856 and batch: 400, loss is 4.522715644836426 and perplexity is 92.08532978357488
At time: 422.74355578422546 and batch: 450, loss is 4.4674246215820315 and perplexity is 87.1320358457568
At time: 423.453889131546 and batch: 500, loss is 4.496506900787353 and perplexity is 89.70324107406992
At time: 424.15784215927124 and batch: 550, loss is 4.542335157394409 and perplexity is 93.90983852920817
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.979056500374003 and perplexity of 145.33719135860076
Finished 48 epochs...
Completing Train Step...
At time: 425.9688596725464 and batch: 50, loss is 4.5814640331268315 and perplexity is 97.65726306736951
At time: 426.67862915992737 and batch: 100, loss is 4.5879931640625 and perplexity is 98.29696620500832
At time: 427.37577533721924 and batch: 150, loss is 4.561250867843628 and perplexity is 95.70311694087583
At time: 428.076354265213 and batch: 200, loss is 4.5216612625122075 and perplexity is 91.98828780819333
At time: 428.7774155139923 and batch: 250, loss is 4.497042121887207 and perplexity is 89.75126499197873
At time: 429.47481298446655 and batch: 300, loss is 4.484029150009155 and perplexity is 88.59090057377152
At time: 430.18139028549194 and batch: 350, loss is 4.523958177566528 and perplexity is 92.19981993394278
At time: 430.8801257610321 and batch: 400, loss is 4.5226860523223875 and perplexity is 92.08260478748048
At time: 431.58377385139465 and batch: 450, loss is 4.467419757843017 and perplexity is 87.13161205930527
At time: 432.28974509239197 and batch: 500, loss is 4.496472129821777 and perplexity is 89.70012205998835
At time: 432.98665046691895 and batch: 550, loss is 4.542316932678222 and perplexity is 93.90812706464935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.979048708651928 and perplexity of 145.33605893601035
Finished 49 epochs...
Completing Train Step...
At time: 434.79561376571655 and batch: 50, loss is 4.581359357833862 and perplexity is 97.64704129973993
At time: 435.50393176078796 and batch: 100, loss is 4.587791156768799 and perplexity is 98.27711150635075
At time: 436.2124559879303 and batch: 150, loss is 4.560994119644165 and perplexity is 95.67854849200567
At time: 436.9183979034424 and batch: 200, loss is 4.521389856338501 and perplexity is 91.96332500665594
At time: 437.6164565086365 and batch: 250, loss is 4.496791019439697 and perplexity is 89.72873105895334
At time: 438.3120276927948 and batch: 300, loss is 4.483918104171753 and perplexity is 88.581063469226
At time: 439.00983214378357 and batch: 350, loss is 4.523910484313965 and perplexity is 92.1954227295037
At time: 439.7073221206665 and batch: 400, loss is 4.5226124668121335 and perplexity is 92.07582909132125
At time: 440.4033603668213 and batch: 450, loss is 4.467392873764038 and perplexity is 87.12926963765216
At time: 441.0989463329315 and batch: 500, loss is 4.496419134140015 and perplexity is 89.6953684668267
At time: 441.795969247818 and batch: 550, loss is 4.542292537689209 and perplexity is 93.90583620486426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.979053903133311 and perplexity of 145.3368138834235
Annealing...
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efc55b01898>
SETTINGS FOR THIS RUN
{'dropout': 0.2456845229564283, 'wordvec_dim': 200, 'tune_wordvecs': True, 'anneal': 3.3023919730972473, 'num_layers': 1, 'seq_len': 35, 'data': 'ptb', 'lr': 13.7562397440194, 'wordvec_source': '', 'batch_size': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.8997056484222412 and batch: 50, loss is 6.3159035968780515 and perplexity is 553.301796553624
At time: 1.5872094631195068 and batch: 100, loss is 5.506343536376953 and perplexity is 246.2490781216964
At time: 2.258556604385376 and batch: 150, loss is 5.376880121231079 and perplexity is 216.34624734664195
At time: 2.929302930831909 and batch: 200, loss is 5.255359439849854 and perplexity is 191.59033868198009
At time: 3.597853899002075 and batch: 250, loss is 5.198887205123901 and perplexity is 181.07063524742904
At time: 4.267027378082275 and batch: 300, loss is 5.164867515563965 and perplexity is 175.01427036888563
At time: 4.935620307922363 and batch: 350, loss is 5.189527959823608 and perplexity is 179.38385655606842
At time: 5.603606224060059 and batch: 400, loss is 5.181714582443237 and perplexity is 177.98772414622954
At time: 6.272445917129517 and batch: 450, loss is 5.101521091461182 and perplexity is 164.27158946910896
At time: 6.945660352706909 and batch: 500, loss is 5.1155421161651615 and perplexity is 166.5910682190755
At time: 7.619863748550415 and batch: 550, loss is 5.141886339187622 and perplexity is 171.0381000829904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.06237241055103 and perplexity of 157.9648295638121
Finished 1 epochs...
Completing Train Step...
At time: 9.37273359298706 and batch: 50, loss is 4.957769145965576 and perplexity is 142.27604451238733
At time: 10.051053762435913 and batch: 100, loss is 4.91850064277649 and perplexity is 136.797351247687
At time: 10.715705156326294 and batch: 150, loss is 4.882513360977173 and perplexity is 131.96191536273224
At time: 11.379585981369019 and batch: 200, loss is 4.808012142181396 and perplexity is 122.4878868430169
At time: 12.044095516204834 and batch: 250, loss is 4.782724056243897 and perplexity is 119.42923930062872
At time: 12.723478317260742 and batch: 300, loss is 4.765873317718506 and perplexity is 117.43362938776949
At time: 13.39239501953125 and batch: 350, loss is 4.808320808410644 and perplexity is 122.52570055278468
At time: 14.061879634857178 and batch: 400, loss is 4.801637544631958 and perplexity is 121.70955925791186
At time: 14.73959469795227 and batch: 450, loss is 4.737730665206909 and perplexity is 114.17480660322406
At time: 15.429687976837158 and batch: 500, loss is 4.771756572723389 and perplexity is 118.12655771395323
At time: 16.128005504608154 and batch: 550, loss is 4.809334659576416 and perplexity is 122.6499863701415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.976972539374169 and perplexity of 145.03462969283927
Finished 2 epochs...
Completing Train Step...
At time: 17.931079864501953 and batch: 50, loss is 4.759319133758545 and perplexity is 116.66646459354781
At time: 18.638301372528076 and batch: 100, loss is 4.741169528961182 and perplexity is 114.5681140851275
At time: 19.333057403564453 and batch: 150, loss is 4.717424526214599 and perplexity is 111.87973796401528
At time: 20.031126499176025 and batch: 200, loss is 4.672945165634156 and perplexity is 107.01244815947115
At time: 20.730645418167114 and batch: 250, loss is 4.645032377243042 and perplexity is 104.06673512484576
At time: 21.429955005645752 and batch: 300, loss is 4.630235815048218 and perplexity is 102.5382413200276
At time: 22.127179622650146 and batch: 350, loss is 4.670782861709594 and perplexity is 106.78130471408893
At time: 22.829610586166382 and batch: 400, loss is 4.67510217666626 and perplexity is 107.24352431804135
At time: 23.535590410232544 and batch: 450, loss is 4.614334945678711 and perplexity is 100.9206884690871
At time: 24.24313473701477 and batch: 500, loss is 4.654054298400879 and perplexity is 105.00986502894611
At time: 24.946996212005615 and batch: 550, loss is 4.690426254272461 and perplexity is 108.89958883069943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.956734839906084 and perplexity of 142.128963613872
Finished 3 epochs...
Completing Train Step...
At time: 26.743547201156616 and batch: 50, loss is 4.656880798339844 and perplexity is 105.30709526878506
At time: 27.456968307495117 and batch: 100, loss is 4.646286602020264 and perplexity is 104.19734008939422
At time: 28.150678634643555 and batch: 150, loss is 4.626478538513184 and perplexity is 102.15369965903716
At time: 28.853477239608765 and batch: 200, loss is 4.5914734363555905 and perplexity is 98.63966240512855
At time: 29.550445795059204 and batch: 250, loss is 4.568569049835205 and perplexity is 96.4060587586723
At time: 30.266643285751343 and batch: 300, loss is 4.557312488555908 and perplexity is 95.32694301138261
At time: 30.961052894592285 and batch: 350, loss is 4.594058952331543 and perplexity is 98.89502681025718
At time: 31.666295528411865 and batch: 400, loss is 4.58988398551941 and perplexity is 98.48300404460039
At time: 32.362433433532715 and batch: 450, loss is 4.533812665939331 and perplexity is 93.1128935349409
At time: 33.061333656311035 and batch: 500, loss is 4.570821113586426 and perplexity is 96.6234160083264
At time: 33.76681423187256 and batch: 550, loss is 4.612868461608887 and perplexity is 100.77279835288533
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.953602892287234 and perplexity of 141.68451949586003
Finished 4 epochs...
Completing Train Step...
At time: 35.562461853027344 and batch: 50, loss is 4.585468616485596 and perplexity is 98.04912381374459
At time: 36.27265739440918 and batch: 100, loss is 4.581507034301758 and perplexity is 97.66146253471189
At time: 36.977057218551636 and batch: 150, loss is 4.565738410949707 and perplexity is 96.13355388335475
At time: 37.675636291503906 and batch: 200, loss is 4.52891414642334 and perplexity is 92.65789353179588
At time: 38.37303614616394 and batch: 250, loss is 4.503907899856568 and perplexity is 90.36959748773852
At time: 39.07367777824402 and batch: 300, loss is 4.499265441894531 and perplexity is 89.95103276667295
At time: 39.79098987579346 and batch: 350, loss is 4.5360798740386965 and perplexity is 93.32423933324093
At time: 40.496718883514404 and batch: 400, loss is 4.532100572586059 and perplexity is 92.95361196018652
At time: 41.200722217559814 and batch: 450, loss is 4.472354822158813 and perplexity is 87.56267495544253
At time: 41.9060914516449 and batch: 500, loss is 4.509216327667236 and perplexity is 90.85059350909495
At time: 42.610536098480225 and batch: 550, loss is 4.556673917770386 and perplexity is 95.26608944222599
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.956776071102061 and perplexity of 142.1348238818368
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 44.43340516090393 and batch: 50, loss is 4.484745941162109 and perplexity is 88.65442451151353
At time: 45.14900302886963 and batch: 100, loss is 4.41897798538208 and perplexity is 83.0114031228438
At time: 45.85455083847046 and batch: 150, loss is 4.3746989059448245 and perplexity is 79.41592428938047
At time: 46.561206102371216 and batch: 200, loss is 4.321472415924072 and perplexity is 75.2994187704455
At time: 47.26952314376831 and batch: 250, loss is 4.271521263122558 and perplexity is 71.63052164710214
At time: 47.990453481674194 and batch: 300, loss is 4.238481931686401 and perplexity is 69.30256591399649
At time: 48.69564890861511 and batch: 350, loss is 4.26204662322998 and perplexity is 70.95505321225954
At time: 49.39944887161255 and batch: 400, loss is 4.230835366249084 and perplexity is 68.77466021371849
At time: 50.10280251502991 and batch: 450, loss is 4.15405029296875 and perplexity is 63.69144760005985
At time: 50.8097243309021 and batch: 500, loss is 4.159034905433654 and perplexity is 64.00971735037037
At time: 51.517329931259155 and batch: 550, loss is 4.176028289794922 and perplexity is 65.10675386210266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.779994883435838 and perplexity of 119.10374064132385
Finished 6 epochs...
Completing Train Step...
At time: 53.33754253387451 and batch: 50, loss is 4.309004454612732 and perplexity is 74.36641693209579
At time: 54.05015325546265 and batch: 100, loss is 4.283287935256958 and perplexity is 72.47835280360907
At time: 54.74846935272217 and batch: 150, loss is 4.257844305038452 and perplexity is 70.65750313928851
At time: 55.44722127914429 and batch: 200, loss is 4.217485704421997 and perplexity is 67.86264286363577
At time: 56.15807843208313 and batch: 250, loss is 4.176504335403442 and perplexity is 65.13775502476196
At time: 56.86203575134277 and batch: 300, loss is 4.149290552139282 and perplexity is 63.389013142222716
At time: 57.56544828414917 and batch: 350, loss is 4.178043999671936 and perplexity is 65.23812254501848
At time: 58.27886080741882 and batch: 400, loss is 4.161458692550659 and perplexity is 64.16505145099761
At time: 58.993408203125 and batch: 450, loss is 4.089481272697449 and perplexity is 59.70891102722298
At time: 59.71590566635132 and batch: 500, loss is 4.1080214786529545 and perplexity is 60.82625239863176
At time: 60.42622399330139 and batch: 550, loss is 4.138484425544739 and perplexity is 62.70771118884088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.776392835251828 and perplexity of 118.67549497243664
Finished 7 epochs...
Completing Train Step...
At time: 62.25268363952637 and batch: 50, loss is 4.233295855522155 and perplexity is 68.94408787943414
At time: 62.96731162071228 and batch: 100, loss is 4.216165513992309 and perplexity is 67.77311036497169
At time: 63.66259741783142 and batch: 150, loss is 4.19339159488678 and perplexity is 66.24709367760532
At time: 64.35626792907715 and batch: 200, loss is 4.159627828598023 and perplexity is 64.04768144831638
At time: 65.04966950416565 and batch: 250, loss is 4.121373238563538 and perplexity is 61.64383586014837
At time: 65.7561993598938 and batch: 300, loss is 4.093890442848205 and perplexity is 59.972759023362045
At time: 66.4500675201416 and batch: 350, loss is 4.127717204093933 and perplexity is 62.03614531309354
At time: 67.14804887771606 and batch: 400, loss is 4.115533180236817 and perplexity is 61.284881440831114
At time: 67.84616088867188 and batch: 450, loss is 4.0461635446548465 and perplexity is 57.17767612397981
At time: 68.54967403411865 and batch: 500, loss is 4.068646063804627 and perplexity is 58.47773384818313
At time: 69.25384640693665 and batch: 550, loss is 4.1021185922622685 and perplexity is 60.46825957612466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.77910825039478 and perplexity of 118.99818613053105
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 71.08411145210266 and batch: 50, loss is 4.1669739151000975 and perplexity is 64.51991366217389
At time: 71.81701970100403 and batch: 100, loss is 4.12868405342102 and perplexity is 62.09615392341217
At time: 72.53542757034302 and batch: 150, loss is 4.094873027801514 and perplexity is 60.03171631451691
At time: 73.24141788482666 and batch: 200, loss is 4.05386209487915 and perplexity is 57.619560085815216
At time: 73.94467639923096 and batch: 250, loss is 4.004332237243652 and perplexity is 54.83519526950888
At time: 74.64819073677063 and batch: 300, loss is 3.969217882156372 and perplexity is 52.943106893985345
At time: 75.3517210483551 and batch: 350, loss is 3.9939547777175903 and perplexity is 54.26908770955435
At time: 76.05625462532043 and batch: 400, loss is 3.96963180065155 and perplexity is 52.96502556107887
At time: 76.76498985290527 and batch: 450, loss is 3.890154037475586 and perplexity is 48.91842121351807
At time: 77.47392416000366 and batch: 500, loss is 3.9025265073776243 and perplexity is 49.52742256437294
At time: 78.18153595924377 and batch: 550, loss is 3.925662202835083 and perplexity is 50.6866317791931
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.722548139856217 and perplexity of 112.45443752770791
Finished 9 epochs...
Completing Train Step...
At time: 79.99697232246399 and batch: 50, loss is 4.1002257490158085 and perplexity is 60.3539108955659
At time: 80.7081413269043 and batch: 100, loss is 4.074603433609009 and perplexity is 58.827147092598274
At time: 81.40635967254639 and batch: 150, loss is 4.0477151203155515 and perplexity is 57.266460474596656
At time: 82.10639476776123 and batch: 200, loss is 4.01316828250885 and perplexity is 55.321868503958896
At time: 82.80555367469788 and batch: 250, loss is 3.968762378692627 and perplexity is 52.918996616987926
At time: 83.52673983573914 and batch: 300, loss is 3.9385672187805176 and perplexity is 51.34498244670117
At time: 84.24438095092773 and batch: 350, loss is 3.967576713562012 and perplexity is 52.8562895900658
At time: 84.95366930961609 and batch: 400, loss is 3.947863039970398 and perplexity is 51.824501531333574
At time: 85.65900611877441 and batch: 450, loss is 3.8751043462753296 and perplexity is 48.18772623852006
At time: 86.36488604545593 and batch: 500, loss is 3.8912201595306395 and perplexity is 48.970602031890394
At time: 87.06955313682556 and batch: 550, loss is 3.9202057695388794 and perplexity is 50.41081672142519
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.717027055456283 and perplexity of 111.83527787610424
Finished 10 epochs...
Completing Train Step...
At time: 88.89095044136047 and batch: 50, loss is 4.070757303237915 and perplexity is 58.601324764954164
At time: 89.61446475982666 and batch: 100, loss is 4.04707790851593 and perplexity is 57.22998123399529
At time: 90.32208228111267 and batch: 150, loss is 4.021449761390686 and perplexity is 55.781917704257495
At time: 91.02176308631897 and batch: 200, loss is 3.9890960693359374 and perplexity is 54.006049568508196
At time: 91.72295832633972 and batch: 250, loss is 3.9460542488098143 and perplexity is 51.730846557740676
At time: 92.43221950531006 and batch: 300, loss is 3.9189754152297973 and perplexity is 50.34883169543881
At time: 93.13294696807861 and batch: 350, loss is 3.948871831893921 and perplexity is 51.876808048679244
At time: 93.83648562431335 and batch: 400, loss is 3.9316812801361083 and perplexity is 50.99263854933592
At time: 94.54204559326172 and batch: 450, loss is 3.8618137979507448 and perplexity is 47.55152205037637
At time: 95.2590901851654 and batch: 500, loss is 3.8806264114379885 and perplexity is 48.45455805614566
At time: 95.9734034538269 and batch: 550, loss is 3.9123272275924683 and perplexity is 50.015213422228896
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.7163121649559505 and perplexity of 111.7553564692796
Finished 11 epochs...
Completing Train Step...
At time: 97.796804189682 and batch: 50, loss is 4.0480982685089115 and perplexity is 57.288406219435394
At time: 98.5007574558258 and batch: 100, loss is 4.025236644744873 and perplexity is 55.99355779512656
At time: 99.19964623451233 and batch: 150, loss is 4.000549116134644 and perplexity is 54.62813899120595
At time: 99.89586758613586 and batch: 200, loss is 3.969193320274353 and perplexity is 52.941806527609884
At time: 100.59573221206665 and batch: 250, loss is 3.928473687171936 and perplexity is 50.82933696322257
At time: 101.32036852836609 and batch: 300, loss is 3.9025245666503907 and perplexity is 49.52732644524843
At time: 102.03189444541931 and batch: 350, loss is 3.933341631889343 and perplexity is 51.07737459250583
At time: 102.73570394515991 and batch: 400, loss is 3.917743110656738 and perplexity is 50.28682481342131
At time: 103.44201016426086 and batch: 450, loss is 3.849586524963379 and perplexity is 46.97363678948986
At time: 104.15394020080566 and batch: 500, loss is 3.8693773412704466 and perplexity is 47.91254362740664
At time: 104.86617016792297 and batch: 550, loss is 3.9036710977554323 and perplexity is 49.584143630678646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.7159806921127 and perplexity of 111.71831874236167
Finished 12 epochs...
Completing Train Step...
At time: 106.7163462638855 and batch: 50, loss is 4.02837414264679 and perplexity is 56.169513350982676
At time: 107.44965314865112 and batch: 100, loss is 4.00669264793396 and perplexity is 54.964781729094085
At time: 108.16845273971558 and batch: 150, loss is 3.9825130701065063 and perplexity is 53.65169542203711
At time: 108.88094019889832 and batch: 200, loss is 3.952674722671509 and perplexity is 52.07446548018023
At time: 109.59348630905151 and batch: 250, loss is 3.912849259376526 and perplexity is 50.04132976951032
At time: 110.31978154182434 and batch: 300, loss is 3.887665419578552 and perplexity is 48.79683331065873
At time: 111.03250908851624 and batch: 350, loss is 3.919189887046814 and perplexity is 50.35963125891681
At time: 111.74743509292603 and batch: 400, loss is 3.9051717281341554 and perplexity is 49.65860695990717
At time: 112.46314024925232 and batch: 450, loss is 3.838148159980774 and perplexity is 46.43939643025856
At time: 113.18329763412476 and batch: 500, loss is 3.85883047580719 and perplexity is 47.409871940697805
At time: 113.89632439613342 and batch: 550, loss is 3.8942618560791016 and perplexity is 49.119782508920835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.71692803565492 and perplexity of 111.82420451735354
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 115.77790379524231 and batch: 50, loss is 4.010486717224121 and perplexity is 55.1737180282552
At time: 116.49222087860107 and batch: 100, loss is 3.9841899776458742 and perplexity is 53.74173983156425
At time: 117.19361782073975 and batch: 150, loss is 3.9546703624725343 and perplexity is 52.17849112043943
At time: 117.89839601516724 and batch: 200, loss is 3.9214259672164915 and perplexity is 50.472365426071875
At time: 118.61365461349487 and batch: 250, loss is 3.874794912338257 and perplexity is 48.172817627405564
At time: 119.32696986198425 and batch: 300, loss is 3.845264205932617 and perplexity is 46.771039904852515
At time: 120.02679371833801 and batch: 350, loss is 3.873332152366638 and perplexity is 48.10240386982827
At time: 120.73200631141663 and batch: 400, loss is 3.852844548225403 and perplexity is 47.126927567717736
At time: 121.42552852630615 and batch: 450, loss is 3.7812627077102663 and perplexity is 43.87140353483634
At time: 122.12690043449402 and batch: 500, loss is 3.7958338260650635 and perplexity is 44.51533898477087
At time: 122.82534265518188 and batch: 550, loss is 3.8278301525115968 and perplexity is 45.96269790962892
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.701879298433345 and perplexity of 110.15399027640073
Finished 14 epochs...
Completing Train Step...
At time: 124.67687392234802 and batch: 50, loss is 3.9897433710098267 and perplexity is 54.041019091488394
At time: 125.38534092903137 and batch: 100, loss is 3.966746211051941 and perplexity is 52.812410532244115
At time: 126.08170247077942 and batch: 150, loss is 3.9396989822387694 and perplexity is 51.40312571760734
At time: 126.77218246459961 and batch: 200, loss is 3.908235259056091 and perplexity is 49.81097090453288
At time: 127.46237206459045 and batch: 250, loss is 3.8635414171218874 and perplexity is 47.63374397512393
At time: 128.15431356430054 and batch: 300, loss is 3.8355633211135864 and perplexity is 46.31951307971787
At time: 128.84913611412048 and batch: 350, loss is 3.8661334180831908 and perplexity is 47.7573708365545
At time: 129.54622888565063 and batch: 400, loss is 3.8463486099243163 and perplexity is 46.8217861169459
At time: 130.24096274375916 and batch: 450, loss is 3.776991596221924 and perplexity is 43.684423469787866
At time: 130.94425415992737 and batch: 500, loss is 3.7936807346343993 and perplexity is 44.41959649802047
At time: 131.6455318927765 and batch: 550, loss is 3.8272994709014894 and perplexity is 45.93831282202842
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.700542044132314 and perplexity of 110.00678482662505
Finished 15 epochs...
Completing Train Step...
At time: 133.4962944984436 and batch: 50, loss is 3.9794948863983155 and perplexity is 53.49000887159307
At time: 134.22019171714783 and batch: 100, loss is 3.956554570198059 and perplexity is 52.276898917766516
At time: 134.92856526374817 and batch: 150, loss is 3.9303829145431517 and perplexity is 50.92647442385609
At time: 135.637268781662 and batch: 200, loss is 3.899557900428772 and perplexity is 49.380613131156124
At time: 136.34772753715515 and batch: 250, loss is 3.855407419204712 and perplexity is 47.24786270710967
At time: 137.07402086257935 and batch: 300, loss is 3.8285241651535036 and perplexity is 45.994607674643724
At time: 137.78019499778748 and batch: 350, loss is 3.860632562637329 and perplexity is 47.49538567498562
At time: 138.48725056648254 and batch: 400, loss is 3.842138743400574 and perplexity is 46.62508697606233
At time: 139.19580960273743 and batch: 450, loss is 3.7738217544555663 and perplexity is 43.54616999631439
At time: 139.90355730056763 and batch: 500, loss is 3.7917490673065184 and perplexity is 44.33387543369176
At time: 140.61140775680542 and batch: 550, loss is 3.8264234399795534 and perplexity is 45.8980870615723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.7007105401221745 and perplexity of 110.0253220904092
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 142.43283653259277 and batch: 50, loss is 3.972153754234314 and perplexity is 53.09876947413712
At time: 143.14956092834473 and batch: 100, loss is 3.9477917385101318 and perplexity is 51.82080650042894
At time: 143.8571810722351 and batch: 150, loss is 3.921002817153931 and perplexity is 50.45101255953627
At time: 144.55306148529053 and batch: 200, loss is 3.8888158321380617 and perplexity is 48.853002103010986
At time: 145.260103225708 and batch: 250, loss is 3.842377128601074 and perplexity is 46.636203031668394
At time: 145.95700001716614 and batch: 300, loss is 3.8142533111572265 and perplexity is 45.34288671186127
At time: 146.65429258346558 and batch: 350, loss is 3.844536838531494 and perplexity is 46.7370325445311
At time: 147.35893559455872 and batch: 400, loss is 3.824078998565674 and perplexity is 45.79060772418132
At time: 148.0708155632019 and batch: 450, loss is 3.7536023807525636 and perplexity is 42.674535360204615
At time: 148.78336763381958 and batch: 500, loss is 3.7693703889846804 and perplexity is 43.35276086552789
At time: 149.4961380958557 and batch: 550, loss is 3.8028775310516356 and perplexity is 44.829998784402626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.6974068499626 and perplexity of 109.66243228431516
Finished 17 epochs...
Completing Train Step...
At time: 151.3284614086151 and batch: 50, loss is 3.9655709743499754 and perplexity is 52.750379906540964
At time: 152.0404360294342 and batch: 100, loss is 3.942362756729126 and perplexity is 51.54023458509677
At time: 152.73460721969604 and batch: 150, loss is 3.916331648826599 and perplexity is 50.215896947401426
At time: 153.4332354068756 and batch: 200, loss is 3.884603724479675 and perplexity is 48.64766076222929
At time: 154.13394689559937 and batch: 250, loss is 3.839015955924988 and perplexity is 46.4797138412452
At time: 154.8557641506195 and batch: 300, loss is 3.8114350700378417 and perplexity is 45.215279422573076
At time: 155.5603563785553 and batch: 350, loss is 3.8423118114471437 and perplexity is 46.63315698709681
At time: 156.26554417610168 and batch: 400, loss is 3.822544584274292 and perplexity is 45.720399839044376
At time: 156.97239875793457 and batch: 450, loss is 3.75310067653656 and perplexity is 42.65313073574255
At time: 157.67857575416565 and batch: 500, loss is 3.7698707008361816 and perplexity is 43.37445619234622
At time: 158.38812470436096 and batch: 550, loss is 3.804212293624878 and perplexity is 44.88987614107613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.696874090965758 and perplexity of 109.60402419699862
Finished 18 epochs...
Completing Train Step...
At time: 160.21416974067688 and batch: 50, loss is 3.9618265867233275 and perplexity is 52.55323136733571
At time: 160.92724752426147 and batch: 100, loss is 3.9389043426513672 and perplexity is 51.36229498400313
At time: 161.63807606697083 and batch: 150, loss is 3.9134160089492798 and perplexity is 50.06969871006033
At time: 162.33735513687134 and batch: 200, loss is 3.8818678092956542 and perplexity is 48.51474679206503
At time: 163.03424286842346 and batch: 250, loss is 3.8367296981811525 and perplexity is 46.3735706171624
At time: 163.7295699119568 and batch: 300, loss is 3.8094923067092896 and perplexity is 45.127522109267794
At time: 164.43025159835815 and batch: 350, loss is 3.8407265758514404 and perplexity is 46.559291009657805
At time: 165.12981534004211 and batch: 400, loss is 3.8213593673706057 and perplexity is 45.66624334824567
At time: 165.83105731010437 and batch: 450, loss is 3.7525423192977905 and perplexity is 42.62932169903263
At time: 166.53237581253052 and batch: 500, loss is 3.769888482093811 and perplexity is 43.37522745158328
At time: 167.24169492721558 and batch: 550, loss is 3.8047412395477296 and perplexity is 44.91362673887545
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.696628976375498 and perplexity of 109.57716194381636
Finished 19 epochs...
Completing Train Step...
At time: 169.07214426994324 and batch: 50, loss is 3.958757677078247 and perplexity is 52.39219747433358
At time: 169.78329730033875 and batch: 100, loss is 3.936062383651733 and perplexity is 51.21653267089715
At time: 170.478595495224 and batch: 150, loss is 3.9110455989837645 and perplexity is 49.9511535530856
At time: 171.1753966808319 and batch: 200, loss is 3.8795717096328737 and perplexity is 48.40347988715125
At time: 171.87530255317688 and batch: 250, loss is 3.8347524213790893 and perplexity is 46.28196782363594
At time: 172.6009178161621 and batch: 300, loss is 3.8077902364730836 and perplexity is 45.0507772281686
At time: 173.2990279197693 and batch: 350, loss is 3.8393511152267457 and perplexity is 46.495294560547784
At time: 174.001558303833 and batch: 400, loss is 3.820214319229126 and perplexity is 45.61398322705872
At time: 174.70675539970398 and batch: 450, loss is 3.7518799352645873 and perplexity is 42.60109406679143
At time: 175.41409611701965 and batch: 500, loss is 3.76964626789093 and perplexity is 43.36472262770145
At time: 176.12113499641418 and batch: 550, loss is 3.804890584945679 and perplexity is 44.9203348832369
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.6965341770902596 and perplexity of 109.56677459954979
Finished 20 epochs...
Completing Train Step...
At time: 177.9607367515564 and batch: 50, loss is 3.9560664319992065 and perplexity is 52.25138679371492
At time: 178.6837043762207 and batch: 100, loss is 3.933547325134277 and perplexity is 51.08788194403705
At time: 179.38931488990784 and batch: 150, loss is 3.908941111564636 and perplexity is 49.84614251482329
At time: 180.09136986732483 and batch: 200, loss is 3.8775244140625 and perplexity is 48.30448502763372
At time: 180.78719902038574 and batch: 250, loss is 3.8329962348937987 and perplexity is 46.200759386683
At time: 181.48543453216553 and batch: 300, loss is 3.806258645057678 and perplexity is 44.981830656974616
At time: 182.1842110157013 and batch: 350, loss is 3.8380379819869996 and perplexity is 46.434280112585775
At time: 182.88724637031555 and batch: 400, loss is 3.8191173124313353 and perplexity is 45.56397181382989
At time: 183.59083938598633 and batch: 450, loss is 3.7511733388900756 and perplexity is 42.57100292057301
At time: 184.29384422302246 and batch: 500, loss is 3.7692600679397583 and perplexity is 43.34797840745669
At time: 184.9968810081482 and batch: 550, loss is 3.8048517751693725 and perplexity is 44.918591568917506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.696471518658577 and perplexity of 109.55990953236831
Finished 21 epochs...
Completing Train Step...
At time: 186.8466818332672 and batch: 50, loss is 3.953595461845398 and perplexity is 52.12243456062405
At time: 187.55664086341858 and batch: 100, loss is 3.9312171649932863 and perplexity is 50.96897758474343
At time: 188.25370168685913 and batch: 150, loss is 3.9070021438598634 and perplexity is 49.74958609442106
At time: 188.95421600341797 and batch: 200, loss is 3.8756412267684937 and perplexity is 48.21360423482156
At time: 189.65135025978088 and batch: 250, loss is 3.831326389312744 and perplexity is 46.12367562967688
At time: 190.36571216583252 and batch: 300, loss is 3.8047097969055175 and perplexity is 44.91221455798092
At time: 191.06300258636475 and batch: 350, loss is 3.8367401361465454 and perplexity is 46.37405466541388
At time: 191.75599908828735 and batch: 400, loss is 3.818022680282593 and perplexity is 45.51412331332114
At time: 192.45308685302734 and batch: 450, loss is 3.750404806137085 and perplexity is 42.53829827940458
At time: 193.15085291862488 and batch: 500, loss is 3.768749027252197 and perplexity is 43.32583148623787
At time: 193.85282278060913 and batch: 550, loss is 3.804607000350952 and perplexity is 44.907597974354964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.696441650390625 and perplexity of 109.55663721650302
Finished 22 epochs...
Completing Train Step...
At time: 195.67664432525635 and batch: 50, loss is 3.9512600040435792 and perplexity is 52.00084685092756
At time: 196.3972682952881 and batch: 100, loss is 3.928995704650879 and perplexity is 50.85587769232
At time: 197.10583066940308 and batch: 150, loss is 3.905176796913147 and perplexity is 49.65885866904881
At time: 197.8022608757019 and batch: 200, loss is 3.873850255012512 and perplexity is 48.127332309733845
At time: 198.49774026870728 and batch: 250, loss is 3.829704303741455 and perplexity is 46.0489197275695
At time: 199.19371962547302 and batch: 300, loss is 3.803267116546631 and perplexity is 44.847467304189706
At time: 199.88973832130432 and batch: 350, loss is 3.8354519844055175 and perplexity is 46.314356304686726
At time: 200.58590626716614 and batch: 400, loss is 3.8169245862960817 and perplexity is 45.46417195886974
At time: 201.28200554847717 and batch: 450, loss is 3.749594931602478 and perplexity is 42.50386154148335
At time: 201.9812777042389 and batch: 500, loss is 3.768160967826843 and perplexity is 43.300360812539964
At time: 202.68789267539978 and batch: 550, loss is 3.804283561706543 and perplexity is 44.89307547043857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.696431910738032 and perplexity of 109.55557017811358
Finished 23 epochs...
Completing Train Step...
At time: 204.49189925193787 and batch: 50, loss is 3.949072170257568 and perplexity is 51.88720200463427
At time: 205.19767880439758 and batch: 100, loss is 3.9269507122039795 and perplexity is 50.751984073598415
At time: 205.88596153259277 and batch: 150, loss is 3.903471245765686 and perplexity is 49.57423513106386
At time: 206.57487320899963 and batch: 200, loss is 3.872162256240845 and perplexity is 48.04616195891113
At time: 207.274108171463 and batch: 250, loss is 3.8280973339080813 and perplexity is 45.974979928132974
At time: 207.97674250602722 and batch: 300, loss is 3.8018953084945677 and perplexity is 44.785987366405394
At time: 208.66573643684387 and batch: 350, loss is 3.834245023727417 and perplexity is 46.25849041854125
At time: 209.35544657707214 and batch: 400, loss is 3.8157970571517943 and perplexity is 45.412938668906854
At time: 210.0594437122345 and batch: 450, loss is 3.748746223449707 and perplexity is 42.46780347122235
At time: 210.77022767066956 and batch: 500, loss is 3.767514953613281 and perplexity is 43.272397197421625
At time: 211.47568821907043 and batch: 550, loss is 3.803811058998108 and perplexity is 44.87186838128689
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.696369901616523 and perplexity of 109.54877694407378
Finished 24 epochs...
Completing Train Step...
At time: 213.3069498538971 and batch: 50, loss is 3.9469386863708498 and perplexity is 51.77661950017154
At time: 214.02330470085144 and batch: 100, loss is 3.9249655771255494 and perplexity is 50.65133446430202
At time: 214.71041011810303 and batch: 150, loss is 3.9017185735702515 and perplexity is 49.48742384562549
At time: 215.4110791683197 and batch: 200, loss is 3.8705078887939455 and perplexity is 47.96674166590553
At time: 216.11243319511414 and batch: 250, loss is 3.826521830558777 and perplexity is 45.90260322311377
At time: 216.8102617263794 and batch: 300, loss is 3.8004901361465455 and perplexity is 44.723099529851446
At time: 217.51100730895996 and batch: 350, loss is 3.8330357265472412 and perplexity is 46.2025839670891
At time: 218.20842790603638 and batch: 400, loss is 3.814590539932251 and perplexity is 45.35818021656316
At time: 218.91436862945557 and batch: 450, loss is 3.7478333568572997 and perplexity is 42.429053721547454
At time: 219.61574482917786 and batch: 500, loss is 3.766773419380188 and perplexity is 43.24032112777341
At time: 220.31649780273438 and batch: 550, loss is 3.803282437324524 and perplexity is 44.84815440753881
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.696502360891788 and perplexity of 109.56328865675827
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 222.13155436515808 and batch: 50, loss is 3.945179853439331 and perplexity is 51.685633115092614
At time: 222.85215401649475 and batch: 100, loss is 3.9225225687026977 and perplexity is 50.52774385549491
At time: 223.5468373298645 and batch: 150, loss is 3.8989547443389894 and perplexity is 49.35083789408932
At time: 224.23925518989563 and batch: 200, loss is 3.8672690629959106 and perplexity is 47.81163705953772
At time: 224.93074679374695 and batch: 250, loss is 3.822232623100281 and perplexity is 45.7061390739525
At time: 225.64094352722168 and batch: 300, loss is 3.7958001375198362 and perplexity is 44.51383935302052
At time: 226.33380317687988 and batch: 350, loss is 3.827479028701782 and perplexity is 45.946562145020884
At time: 227.0311324596405 and batch: 400, loss is 3.8083617639541627 and perplexity is 45.07653234457581
At time: 227.72146844863892 and batch: 450, loss is 3.7410117769241333 and perplexity is 42.14060549496656
At time: 228.4133403301239 and batch: 500, loss is 3.7589694833755494 and perplexity is 42.90418970881849
At time: 229.10715341567993 and batch: 550, loss is 3.7948996353149416 and perplexity is 44.47377258534864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695592677339595 and perplexity of 109.46366605453433
Finished 26 epochs...
Completing Train Step...
At time: 230.94731426239014 and batch: 50, loss is 3.943741412162781 and perplexity is 51.61133981310141
At time: 231.66465854644775 and batch: 100, loss is 3.921551489830017 and perplexity is 50.478701246927045
At time: 232.3614146709442 and batch: 150, loss is 3.8978034114837645 and perplexity is 49.294051349369184
At time: 233.0572669506073 and batch: 200, loss is 3.8663283729553224 and perplexity is 47.76668227630506
At time: 233.7565631866455 and batch: 250, loss is 3.8214014053344725 and perplexity is 45.66816310448451
At time: 234.45514917373657 and batch: 300, loss is 3.7952079343795777 and perplexity is 44.4874859216333
At time: 235.1531662940979 and batch: 350, loss is 3.827028822898865 and perplexity is 45.92588139176597
At time: 235.84538388252258 and batch: 400, loss is 3.8079716062545774 and perplexity is 45.058948818808005
At time: 236.53572058677673 and batch: 450, loss is 3.7408889532089233 and perplexity is 42.13542994708499
At time: 237.2274875640869 and batch: 500, loss is 3.759077453613281 and perplexity is 42.90882233446939
At time: 237.92188429832458 and batch: 550, loss is 3.79511643409729 and perplexity is 44.48341549033884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695442362034575 and perplexity of 109.44721322676963
Finished 27 epochs...
Completing Train Step...
At time: 239.72935152053833 and batch: 50, loss is 3.9428253316879274 and perplexity is 51.56408132201303
At time: 240.4357795715332 and batch: 100, loss is 3.920725393295288 and perplexity is 50.43701818623583
At time: 241.12645769119263 and batch: 150, loss is 3.8969505405426026 and perplexity is 49.25202780827697
At time: 241.81848788261414 and batch: 200, loss is 3.865628032684326 and perplexity is 47.733241056578365
At time: 242.5125494003296 and batch: 250, loss is 3.8207677030563354 and perplexity is 45.63923225322821
At time: 243.21608424186707 and batch: 300, loss is 3.794723320007324 and perplexity is 44.465931869693755
At time: 243.9054980278015 and batch: 350, loss is 3.826657495498657 and perplexity is 45.908831019457644
At time: 244.59941935539246 and batch: 400, loss is 3.807643327713013 and perplexity is 45.04415936046906
At time: 245.2925181388855 and batch: 450, loss is 3.740748062133789 and perplexity is 42.12949385923922
At time: 245.98487520217896 and batch: 500, loss is 3.7590890741348266 and perplexity is 42.909320960260956
At time: 246.677312374115 and batch: 550, loss is 3.79519259929657 and perplexity is 44.4868037075748
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695368665329953 and perplexity of 109.43914762503259
Finished 28 epochs...
Completing Train Step...
At time: 248.46230626106262 and batch: 50, loss is 3.942036895751953 and perplexity is 51.523442370003536
At time: 249.169837474823 and batch: 100, loss is 3.9199903345108034 and perplexity is 50.39995763546522
At time: 249.8630349636078 and batch: 150, loss is 3.896207404136658 and perplexity is 49.21544042973632
At time: 250.5561182498932 and batch: 200, loss is 3.8650161695480345 and perplexity is 47.70404377927986
At time: 251.25383067131042 and batch: 250, loss is 3.820183825492859 and perplexity is 45.6125923074914
At time: 251.95615029335022 and batch: 300, loss is 3.7942754936218264 and perplexity is 44.446023310268586
At time: 252.65537810325623 and batch: 350, loss is 3.8263089942932127 and perplexity is 45.89283452406768
At time: 253.36705875396729 and batch: 400, loss is 3.807323389053345 and perplexity is 45.02975029762844
At time: 254.0685305595398 and batch: 450, loss is 3.740580286979675 and perplexity is 42.122426169822155
At time: 254.76739168167114 and batch: 500, loss is 3.7590379667282106 and perplexity is 42.907128032184936
At time: 255.46682405471802 and batch: 550, loss is 3.7951972579956053 and perplexity is 44.48701095868708
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695325810858544 and perplexity of 109.43445776870107
Finished 29 epochs...
Completing Train Step...
At time: 257.2849016189575 and batch: 50, loss is 3.941311378479004 and perplexity is 51.48607477965754
At time: 257.98457050323486 and batch: 100, loss is 3.9193082571029665 and perplexity is 50.3655926841169
At time: 258.6697108745575 and batch: 150, loss is 3.8955239629745484 and perplexity is 49.1818160633821
At time: 259.35916233062744 and batch: 200, loss is 3.864454917907715 and perplexity is 47.67727731852149
At time: 260.0501244068146 and batch: 250, loss is 3.8196397495269774 and perplexity is 45.58778234214028
At time: 260.7553286552429 and batch: 300, loss is 3.793848056793213 and perplexity is 44.427029502635996
At time: 261.45775079727173 and batch: 350, loss is 3.8259682178497316 and perplexity is 45.87719799156959
At time: 262.151673078537 and batch: 400, loss is 3.8070013523101807 and perplexity is 45.01525139821157
At time: 262.8443148136139 and batch: 450, loss is 3.740391240119934 and perplexity is 42.114463810083336
At time: 263.54160165786743 and batch: 500, loss is 3.758943614959717 and perplexity is 42.90307985975323
At time: 264.2393231391907 and batch: 550, loss is 3.7951577281951905 and perplexity is 44.48525243078019
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.6952949686253325 and perplexity of 109.43108261768202
Finished 30 epochs...
Completing Train Step...
At time: 266.0341718196869 and batch: 50, loss is 3.9406288194656374 and perplexity is 51.450944485867055
At time: 266.744176864624 and batch: 100, loss is 3.918660354614258 and perplexity is 50.33297126016361
At time: 267.4435679912567 and batch: 150, loss is 3.8948795938491823 and perplexity is 49.15013502781821
At time: 268.1429235935211 and batch: 200, loss is 3.863927764892578 and perplexity is 47.652150721392104
At time: 268.84912037849426 and batch: 250, loss is 3.819121904373169 and perplexity is 45.564181041418536
At time: 269.5475478172302 and batch: 300, loss is 3.7934316635131835 and perplexity is 44.40853423701882
At time: 270.24646043777466 and batch: 350, loss is 3.8256277894973754 and perplexity is 45.86158275073276
At time: 270.94834423065186 and batch: 400, loss is 3.806673331260681 and perplexity is 45.000487869710874
At time: 271.6482033729553 and batch: 450, loss is 3.7401879405975342 and perplexity is 42.1059028299557
At time: 272.35049962997437 and batch: 500, loss is 3.758826370239258 and perplexity is 42.89804999501655
At time: 273.0470926761627 and batch: 550, loss is 3.7950880193710326 and perplexity is 44.48215152422241
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695270944148936 and perplexity of 109.42845362480081
Finished 31 epochs...
Completing Train Step...
At time: 274.8513433933258 and batch: 50, loss is 3.939981932640076 and perplexity is 51.41767231054264
At time: 275.5559039115906 and batch: 100, loss is 3.918038558959961 and perplexity is 50.301684165463996
At time: 276.2391402721405 and batch: 150, loss is 3.8942679929733277 and perplexity is 49.12008395275547
At time: 276.92348766326904 and batch: 200, loss is 3.8634227180480956 and perplexity is 47.62809022938667
At time: 277.61334800720215 and batch: 250, loss is 3.8186185359954834 and perplexity is 45.541251245078506
At time: 278.324654340744 and batch: 300, loss is 3.7930231857299805 and perplexity is 44.39039804176698
At time: 279.02000641822815 and batch: 350, loss is 3.8252881002426147 and perplexity is 45.846006709522705
At time: 279.72480630874634 and batch: 400, loss is 3.806341471672058 and perplexity is 44.98555650401406
At time: 280.4195604324341 and batch: 450, loss is 3.739976305961609 and perplexity is 42.096992705418714
At time: 281.11839985847473 and batch: 500, loss is 3.758696346282959 and perplexity is 42.8924725834451
At time: 281.81700825691223 and batch: 550, loss is 3.794997000694275 and perplexity is 44.4781030018997
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.69526607432264 and perplexity of 109.42792072853739
Finished 32 epochs...
Completing Train Step...
At time: 283.61337423324585 and batch: 50, loss is 3.9393577480316164 and perplexity is 51.38558820512767
At time: 284.32403230667114 and batch: 100, loss is 3.917438044548035 and perplexity is 50.2714863471985
At time: 285.0242118835449 and batch: 150, loss is 3.893682289123535 and perplexity is 49.09132255413536
At time: 285.7202434539795 and batch: 200, loss is 3.862930965423584 and perplexity is 47.60467474879978
At time: 286.4164435863495 and batch: 250, loss is 3.8181265449523925 and perplexity is 45.51885086821901
At time: 287.1119155883789 and batch: 300, loss is 3.7926207160949708 and perplexity is 44.37253584920535
At time: 287.8079855442047 and batch: 350, loss is 3.824949460029602 and perplexity is 45.83048403649381
At time: 288.50370836257935 and batch: 400, loss is 3.8060070133209227 and perplexity is 44.97051322477625
At time: 289.20069789886475 and batch: 450, loss is 3.7397593307495116 and perplexity is 42.08785969235235
At time: 289.89823150634766 and batch: 500, loss is 3.7585545015335082 and perplexity is 42.88638894289462
At time: 290.60168075561523 and batch: 550, loss is 3.7948907566070558 and perplexity is 44.47337771746624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695274515354887 and perplexity of 109.42884441704345
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 292.41980266571045 and batch: 50, loss is 3.938856210708618 and perplexity is 51.35982287645564
At time: 293.13935804367065 and batch: 100, loss is 3.9168324995040895 and perplexity is 50.241053912824015
At time: 293.8409516811371 and batch: 150, loss is 3.8927125549316406 and perplexity is 49.043740095026884
At time: 294.5289657115936 and batch: 200, loss is 3.861891646385193 and perplexity is 47.555224006017475
At time: 295.2315020561218 and batch: 250, loss is 3.8167064905166628 and perplexity is 45.454257496041286
At time: 295.93884015083313 and batch: 300, loss is 3.791174530982971 and perplexity is 44.308411327618884
At time: 296.63735270500183 and batch: 350, loss is 3.8232067918777464 and perplexity is 45.75068626229389
At time: 297.3308491706848 and batch: 400, loss is 3.8039858198165892 and perplexity is 44.879710910995485
At time: 298.02374505996704 and batch: 450, loss is 3.7376153898239135 and perplexity is 41.99772246645703
At time: 298.7204236984253 and batch: 500, loss is 3.7560444688796997 and perplexity is 42.77887769106615
At time: 299.41917872428894 and batch: 550, loss is 3.792112584114075 and perplexity is 44.34999447214921
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695176144863697 and perplexity of 109.4180803773084
Finished 34 epochs...
Completing Train Step...
At time: 301.2431812286377 and batch: 50, loss is 3.938517074584961 and perplexity is 51.34240785841137
At time: 301.9492919445038 and batch: 100, loss is 3.9166119050979615 and perplexity is 50.229972239695364
At time: 302.6392319202423 and batch: 150, loss is 3.8923771715164186 and perplexity is 49.02729439594007
At time: 303.32869386672974 and batch: 200, loss is 3.861675291061401 and perplexity is 47.54493629307053
At time: 304.0178368091583 and batch: 250, loss is 3.816484498977661 and perplexity is 45.44416815538112
At time: 304.7087781429291 and batch: 300, loss is 3.7910071802139282 and perplexity is 44.30099690133041
At time: 305.40301299095154 and batch: 350, loss is 3.8230806732177736 and perplexity is 45.74491661088766
At time: 306.0978696346283 and batch: 400, loss is 3.803885612487793 and perplexity is 44.87521386037044
At time: 306.79239201545715 and batch: 450, loss is 3.7375938940048217 and perplexity is 41.9968197007155
At time: 307.4878647327423 and batch: 500, loss is 3.7560820293426516 and perplexity is 42.78048451569314
At time: 308.18567728996277 and batch: 550, loss is 3.792158703804016 and perplexity is 44.352039927310685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695159587454288 and perplexity of 109.41626871235314
Finished 35 epochs...
Completing Train Step...
At time: 309.99743366241455 and batch: 50, loss is 3.938268542289734 and perplexity is 51.32964919747915
At time: 310.7017867565155 and batch: 100, loss is 3.9164037895202637 and perplexity is 50.21951968771213
At time: 311.39038038253784 and batch: 150, loss is 3.892117986679077 and perplexity is 49.014588911222376
At time: 312.07671880722046 and batch: 200, loss is 3.861470413208008 and perplexity is 47.53519638636275
At time: 312.7645938396454 and batch: 250, loss is 3.816302080154419 and perplexity is 45.43587903977135
At time: 313.4701418876648 and batch: 300, loss is 3.790862398147583 and perplexity is 44.29458337575083
At time: 314.16509890556335 and batch: 350, loss is 3.8229767751693724 and perplexity is 45.740164050222674
At time: 314.86300230026245 and batch: 400, loss is 3.803793344497681 and perplexity is 44.871073505595724
At time: 315.5555248260498 and batch: 450, loss is 3.73756947517395 and perplexity is 41.9957941999989
At time: 316.2495255470276 and batch: 500, loss is 3.756101608276367 and perplexity is 42.78132212016347
At time: 316.94200921058655 and batch: 550, loss is 3.7921762037277222 and perplexity is 44.35281609141701
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695149847801695 and perplexity of 109.41520304109751
Finished 36 epochs...
Completing Train Step...
At time: 318.72768354415894 and batch: 50, loss is 3.9380484199523926 and perplexity is 51.31835163859105
At time: 319.42543601989746 and batch: 100, loss is 3.9162093114852907 and perplexity is 50.209754043838366
At time: 320.110835313797 and batch: 150, loss is 3.8918774795532225 and perplexity is 49.0028019707968
At time: 320.8041834831238 and batch: 200, loss is 3.8612762594223025 and perplexity is 47.52596814390817
At time: 321.4987473487854 and batch: 250, loss is 3.816132445335388 and perplexity is 45.42817218634676
At time: 322.1939458847046 and batch: 300, loss is 3.7907254695892334 and perplexity is 44.28851859753664
At time: 322.88940501213074 and batch: 350, loss is 3.8228774213790895 and perplexity is 45.735619817303245
At time: 323.5919277667999 and batch: 400, loss is 3.8037005949020384 and perplexity is 44.86691192466749
At time: 324.29212045669556 and batch: 450, loss is 3.737539892196655 and perplexity is 41.994551857748775
At time: 324.9950432777405 and batch: 500, loss is 3.756108937263489 and perplexity is 42.78163566507132
At time: 325.6898169517517 and batch: 550, loss is 3.7921783828735354 and perplexity is 44.3529127427758
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.6951423807347075 and perplexity of 109.41438603349725
Finished 37 epochs...
Completing Train Step...
At time: 327.4779143333435 and batch: 50, loss is 3.937843475341797 and perplexity is 51.30783529666868
At time: 328.18171644210815 and batch: 100, loss is 3.9160224914550783 and perplexity is 50.200374732219856
At time: 328.87180948257446 and batch: 150, loss is 3.8916476917266847 and perplexity is 48.991543017072466
At time: 329.5672962665558 and batch: 200, loss is 3.8610899829864502 and perplexity is 47.51711600045036
At time: 330.25857734680176 and batch: 250, loss is 3.815968976020813 and perplexity is 45.42074668111471
At time: 330.9646532535553 and batch: 300, loss is 3.790592622756958 and perplexity is 44.28263539892559
At time: 331.65501594543457 and batch: 350, loss is 3.82277934551239 and perplexity is 45.731134476706046
At time: 332.3460965156555 and batch: 400, loss is 3.803606781959534 and perplexity is 44.862703025066466
At time: 333.0387120246887 and batch: 450, loss is 3.737506432533264 and perplexity is 41.993146757686574
At time: 333.7327733039856 and batch: 500, loss is 3.7561075830459596 and perplexity is 42.7815777294696
At time: 334.4266290664673 and batch: 550, loss is 3.7921708250045776 and perplexity is 44.352577530540145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695137510908411 and perplexity of 109.41385320574028
Finished 38 epochs...
Completing Train Step...
At time: 336.28322196006775 and batch: 50, loss is 3.9376479625701903 and perplexity is 51.29780494014804
At time: 336.9808521270752 and batch: 100, loss is 3.9158410596847535 and perplexity is 50.19126761554636
At time: 337.6662244796753 and batch: 150, loss is 3.891425428390503 and perplexity is 48.980655203302526
At time: 338.35444927215576 and batch: 200, loss is 3.860909357070923 and perplexity is 47.50853395296299
At time: 339.0432255268097 and batch: 250, loss is 3.8158091926574706 and perplexity is 45.413489781225856
At time: 339.7304964065552 and batch: 300, loss is 3.7904621076583864 and perplexity is 44.27685622354451
At time: 340.41933155059814 and batch: 350, loss is 3.822681636810303 and perplexity is 45.7266663652016
At time: 341.10808801651 and batch: 400, loss is 3.8035123682022096 and perplexity is 44.85846756865597
At time: 341.80031657218933 and batch: 450, loss is 3.7374695110321046 and perplexity is 41.99159633629199
At time: 342.4981667995453 and batch: 500, loss is 3.7560999298095705 and perplexity is 42.78125031319504
At time: 343.2034955024719 and batch: 550, loss is 3.7921568059921267 and perplexity is 44.35195575556186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.69513393970246 and perplexity of 109.41346246703435
Finished 39 epochs...
Completing Train Step...
At time: 344.9935140609741 and batch: 50, loss is 3.9374591398239134 and perplexity is 51.28811966217043
At time: 345.6955819129944 and batch: 100, loss is 3.915663561820984 and perplexity is 50.18235956336821
At time: 346.384485244751 and batch: 150, loss is 3.8912091398239137 and perplexity is 48.97006239319116
At time: 347.072785615921 and batch: 200, loss is 3.860733623504639 and perplexity is 47.50018584240561
At time: 347.76147174835205 and batch: 250, loss is 3.8156520748138427 and perplexity is 45.406355072149545
At time: 348.46840167045593 and batch: 300, loss is 3.79033323764801 and perplexity is 44.27115063227125
At time: 349.16155982017517 and batch: 350, loss is 3.822584047317505 and perplexity is 45.72220414076034
At time: 349.861985206604 and batch: 400, loss is 3.803417224884033 and perplexity is 44.854199788231796
At time: 350.55521750450134 and batch: 450, loss is 3.7374299335479737 and perplexity is 41.989934447441264
At time: 351.249685049057 and batch: 500, loss is 3.75608717918396 and perplexity is 42.78070482896678
At time: 351.9486060142517 and batch: 550, loss is 3.792137656211853 and perplexity is 44.351106433486635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695134264357547 and perplexity of 109.41349798867726
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 353.7396283149719 and batch: 50, loss is 3.9373113775253294 and perplexity is 51.280541771596155
At time: 354.44464564323425 and batch: 100, loss is 3.9155566358566283 and perplexity is 50.17699405303955
At time: 355.1382920742035 and batch: 150, loss is 3.8908951091766357 and perplexity is 48.95468670714526
At time: 355.8332426548004 and batch: 200, loss is 3.86036771774292 and perplexity is 47.48280843016467
At time: 356.5236225128174 and batch: 250, loss is 3.8151310157775877 and perplexity is 45.382701843435235
At time: 357.2142696380615 and batch: 300, loss is 3.7898864364624023 and perplexity is 44.251374647976135
At time: 357.9054615497589 and batch: 350, loss is 3.822055106163025 and perplexity is 45.69802618023962
At time: 358.5966987609863 and batch: 400, loss is 3.802817711830139 and perplexity is 44.82731716898252
At time: 359.3003237247467 and batch: 450, loss is 3.73675106048584 and perplexity is 41.9614382857968
At time: 359.99434447288513 and batch: 500, loss is 3.755307312011719 and perplexity is 42.747354567742576
At time: 360.68894243240356 and batch: 550, loss is 3.791241340637207 and perplexity is 44.31137165614759
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695089786610705 and perplexity of 109.40863163103575
Finished 41 epochs...
Completing Train Step...
At time: 362.4836618900299 and batch: 50, loss is 3.937189025878906 and perplexity is 51.27426789669821
At time: 363.1825740337372 and batch: 100, loss is 3.915469083786011 and perplexity is 50.17260114561973
At time: 363.8678731918335 and batch: 150, loss is 3.890797343254089 and perplexity is 48.949900840987475
At time: 364.55199551582336 and batch: 200, loss is 3.8603145933151244 and perplexity is 47.48028600013861
At time: 365.2423071861267 and batch: 250, loss is 3.815076675415039 and perplexity is 45.380235797967124
At time: 365.9531376361847 and batch: 300, loss is 3.78983895778656 and perplexity is 44.24927370117913
At time: 366.64968037605286 and batch: 350, loss is 3.822004542350769 and perplexity is 45.69571557224045
At time: 367.3448598384857 and batch: 400, loss is 3.802787594795227 and perplexity is 44.82596712343613
At time: 368.04005336761475 and batch: 450, loss is 3.7367579460144045 and perplexity is 41.961727213473445
At time: 368.7384943962097 and batch: 500, loss is 3.755323510169983 and perplexity is 42.74804700176529
At time: 369.435950756073 and batch: 550, loss is 3.7912513399124146 and perplexity is 44.311814739962855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695080046958112 and perplexity of 109.40756603416227
Finished 42 epochs...
Completing Train Step...
At time: 371.2449793815613 and batch: 50, loss is 3.937104468345642 and perplexity is 51.26993245438467
At time: 371.95173358917236 and batch: 100, loss is 3.915402708053589 and perplexity is 50.1692710129924
At time: 372.6435251235962 and batch: 150, loss is 3.890719704627991 and perplexity is 48.94610058546375
At time: 373.3462932109833 and batch: 200, loss is 3.8602567291259766 and perplexity is 47.47753867137545
At time: 374.03887248039246 and batch: 250, loss is 3.8150240421295165 and perplexity is 45.37784734991578
At time: 374.731689453125 and batch: 300, loss is 3.789795322418213 and perplexity is 44.24734290994778
At time: 375.4258842468262 and batch: 350, loss is 3.8219701719284056 and perplexity is 45.6941450181865
At time: 376.1192967891693 and batch: 400, loss is 3.8027642488479616 and perplexity is 44.82492063098726
At time: 376.8112897872925 and batch: 450, loss is 3.7367565298080443 and perplexity is 41.96166778705056
At time: 377.5071105957031 and batch: 500, loss is 3.7553321647644045 and perplexity is 42.748416970375374
At time: 378.19962096214294 and batch: 550, loss is 3.791254787445068 and perplexity is 44.31196750665444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695078099027593 and perplexity of 109.40735291603295
Finished 43 epochs...
Completing Train Step...
At time: 380.01631259918213 and batch: 50, loss is 3.937031364440918 and perplexity is 51.26618455912188
At time: 380.71723985671997 and batch: 100, loss is 3.9153436851501464 and perplexity is 50.16630996433932
At time: 381.3999285697937 and batch: 150, loss is 3.8906477165222166 and perplexity is 48.942577175220904
At time: 382.0869028568268 and batch: 200, loss is 3.8601979637145996 and perplexity is 47.47474871626149
At time: 382.77904415130615 and batch: 250, loss is 3.8149711799621584 and perplexity is 45.37544864195583
At time: 383.5025761127472 and batch: 300, loss is 3.789753360748291 and perplexity is 44.24548625650409
At time: 384.1990177631378 and batch: 350, loss is 3.821939535140991 and perplexity is 45.69274511782382
At time: 384.8954997062683 and batch: 400, loss is 3.8027414417266847 and perplexity is 44.823898315244286
At time: 385.5919497013092 and batch: 450, loss is 3.736751685142517 and perplexity is 41.96146449729759
At time: 386.287814617157 and batch: 500, loss is 3.7553374576568603 and perplexity is 42.74864323374785
At time: 386.9859013557434 and batch: 550, loss is 3.791255040168762 and perplexity is 44.311978705339975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695078099027593 and perplexity of 109.40735291603295
Finished 44 epochs...
Completing Train Step...
At time: 388.7964732646942 and batch: 50, loss is 3.9369629764556886 and perplexity is 51.26267868793059
At time: 389.5033824443817 and batch: 100, loss is 3.915287928581238 and perplexity is 50.16351294099784
At time: 390.1961305141449 and batch: 150, loss is 3.8905777454376222 and perplexity is 48.939152729820584
At time: 390.8905189037323 and batch: 200, loss is 3.8601394748687743 and perplexity is 47.4719720542059
At time: 391.58450961112976 and batch: 250, loss is 3.814917998313904 and perplexity is 45.37303556497304
At time: 392.27863788604736 and batch: 300, loss is 3.789712290763855 and perplexity is 44.243669132387055
At time: 392.9731743335724 and batch: 350, loss is 3.8219098615646363 and perplexity is 45.69138927077922
At time: 393.6687185764313 and batch: 400, loss is 3.802718372344971 and perplexity is 44.822864267551616
At time: 394.36656761169434 and batch: 450, loss is 3.736745057106018 and perplexity is 41.96118637610106
At time: 395.08078169822693 and batch: 500, loss is 3.755340805053711 and perplexity is 42.74878633066108
At time: 395.7848105430603 and batch: 550, loss is 3.7912535953521727 and perplexity is 44.31191468270428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695078099027593 and perplexity of 109.40735291603295
Finished 45 epochs...
Completing Train Step...
At time: 397.6372084617615 and batch: 50, loss is 3.936897497177124 and perplexity is 51.2593221546057
At time: 398.3359854221344 and batch: 100, loss is 3.915234079360962 and perplexity is 50.1608117476689
At time: 399.02334117889404 and batch: 150, loss is 3.8905093097686767 and perplexity is 48.93580366076509
At time: 399.7118921279907 and batch: 200, loss is 3.860081548690796 and perplexity is 47.4692222639469
At time: 400.40542817115784 and batch: 250, loss is 3.814865131378174 and perplexity is 45.37063689502367
At time: 401.1183931827545 and batch: 300, loss is 3.7896718788146972 and perplexity is 44.24188119560677
At time: 401.8219177722931 and batch: 350, loss is 3.8218806648254393 and perplexity is 45.690055250677744
At time: 402.51953196525574 and batch: 400, loss is 3.80269513130188 and perplexity is 44.82182254953707
At time: 403.2163166999817 and batch: 450, loss is 3.736737723350525 and perplexity is 41.960878644148394
At time: 403.90990352630615 and batch: 500, loss is 3.7553427743911745 and perplexity is 42.74887051753041
At time: 404.6031301021576 and batch: 550, loss is 3.7912507486343383 and perplexity is 44.31178853936602
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695078748337766 and perplexity of 109.40742395536323
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 406.3909890651703 and batch: 50, loss is 3.9368419647216797 and perplexity is 51.2564756776187
At time: 407.0862021446228 and batch: 100, loss is 3.9152041339874266 and perplexity is 50.15930968591429
At time: 407.7696723937988 and batch: 150, loss is 3.89041552066803 and perplexity is 48.931214230972934
At time: 408.4576859474182 and batch: 200, loss is 3.859973430633545 and perplexity is 47.464090261292576
At time: 409.1544394493103 and batch: 250, loss is 3.8146912050247193 and perplexity is 45.36274643179425
At time: 409.852463722229 and batch: 300, loss is 3.7895320653915405 and perplexity is 44.23569601914535
At time: 410.5619857311249 and batch: 350, loss is 3.821715545654297 and perplexity is 45.682511569445644
At time: 411.26060938835144 and batch: 400, loss is 3.802511053085327 and perplexity is 44.81357258772193
At time: 411.9591455459595 and batch: 450, loss is 3.7365051555633544 and perplexity is 41.95112103015172
At time: 412.6668255329132 and batch: 500, loss is 3.755094027519226 and perplexity is 42.73823819214352
At time: 413.3761148452759 and batch: 550, loss is 3.7909726667404176 and perplexity is 44.29946794643228
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695074527821642 and perplexity of 109.40696220054085
Finished 47 epochs...
Completing Train Step...
At time: 415.1963050365448 and batch: 50, loss is 3.9368124532699587 and perplexity is 51.25496304693143
At time: 415.90325236320496 and batch: 100, loss is 3.915176887512207 and perplexity is 50.157943040144126
At time: 416.59795594215393 and batch: 150, loss is 3.890390295982361 and perplexity is 48.92997997204151
At time: 417.2921838760376 and batch: 200, loss is 3.8599556350708006 and perplexity is 47.4632456186117
At time: 417.99128890037537 and batch: 250, loss is 3.8146762323379515 and perplexity is 45.36206723468571
At time: 418.70141983032227 and batch: 300, loss is 3.7895190048217775 and perplexity is 44.235118279524286
At time: 419.3909606933594 and batch: 350, loss is 3.821701993942261 and perplexity is 45.68189249739852
At time: 420.1072630882263 and batch: 400, loss is 3.8025031328201293 and perplexity is 44.813217653748154
At time: 420.8108537197113 and batch: 450, loss is 3.7365096521377565 and perplexity is 41.9513096669128
At time: 421.5135910511017 and batch: 500, loss is 3.7550996494293214 and perplexity is 42.738478463351655
At time: 422.21823239326477 and batch: 550, loss is 3.7909736251831054 and perplexity is 44.29951040495376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695070631960605 and perplexity of 109.40653596704986
Finished 48 epochs...
Completing Train Step...
At time: 424.07630372047424 and batch: 50, loss is 3.9367860174179077 and perplexity is 51.25360809622116
At time: 424.78933215141296 and batch: 100, loss is 3.915153088569641 and perplexity is 50.15674934834284
At time: 425.4847164154053 and batch: 150, loss is 3.8903666973114013 and perplexity is 48.92882530316847
At time: 426.17856788635254 and batch: 200, loss is 3.8599379158020017 and perplexity is 47.46240461205555
At time: 426.87218403816223 and batch: 250, loss is 3.8146609926223753 and perplexity is 45.36137593495073
At time: 427.5653393268585 and batch: 300, loss is 3.7895062351226807 and perplexity is 44.23455341398093
At time: 428.2585186958313 and batch: 350, loss is 3.8216902112960813 and perplexity is 45.68135424699343
At time: 428.95216250419617 and batch: 400, loss is 3.802495908737183 and perplexity is 44.81289392051607
At time: 429.6455593109131 and batch: 450, loss is 3.736512231826782 and perplexity is 41.951417888385535
At time: 430.33936858177185 and batch: 500, loss is 3.7551040554046633 and perplexity is 42.738666768448745
At time: 431.0333344936371 and batch: 550, loss is 3.790974049568176 and perplexity is 44.299529205008604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.6950693333402596 and perplexity of 109.40639388958859
Finished 49 epochs...
Completing Train Step...
At time: 432.8138861656189 and batch: 50, loss is 3.9367611694335936 and perplexity is 51.25233456319357
At time: 433.51176381111145 and batch: 100, loss is 3.9151316356658934 and perplexity is 50.15567335196844
At time: 434.2007825374603 and batch: 150, loss is 3.890344066619873 and perplexity is 48.927718022545506
At time: 434.888876914978 and batch: 200, loss is 3.859920258522034 and perplexity is 47.46156656248822
At time: 435.5798428058624 and batch: 250, loss is 3.8146454429626466 and perplexity is 45.36067058647409
At time: 436.30204129219055 and batch: 300, loss is 3.7894934701919554 and perplexity is 44.23398876657479
At time: 436.9972970485687 and batch: 350, loss is 3.8216793155670166 and perplexity is 45.68085651804581
At time: 437.69029450416565 and batch: 400, loss is 3.8024889039993286 and perplexity is 44.812580019041064
At time: 438.3840808868408 and batch: 450, loss is 3.736513295173645 and perplexity is 41.95146249731786
At time: 439.077974319458 and batch: 500, loss is 3.755107216835022 and perplexity is 42.73880188398095
At time: 439.7794370651245 and batch: 550, loss is 3.7909743928909303 and perplexity is 44.29954441404759
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.695069008685173 and perplexity of 109.40635837025204
Finished Training.
Improved accuracyfrom -116.32710508683068 to -109.40635837025204
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efc50689978>
SETTINGS FOR THIS RUN
{'dropout': 0.0, 'wordvec_dim': 200, 'tune_wordvecs': True, 'anneal': 2.0, 'num_layers': 1, 'seq_len': 35, 'data': 'ptb', 'lr': 11.499079118767963, 'wordvec_source': '', 'batch_size': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.9122161865234375 and batch: 50, loss is 6.248020792007447 and perplexity is 516.9885834730452
At time: 1.5747504234313965 and batch: 100, loss is 5.321833362579346 and perplexity is 204.75893545347222
At time: 2.237619638442993 and batch: 150, loss is 5.136621179580689 and perplexity is 170.13992378290328
At time: 2.914047956466675 and batch: 200, loss is 4.997550468444825 and perplexity is 148.05006127749675
At time: 3.579578161239624 and batch: 250, loss is 4.923953380584717 and perplexity is 137.54530869160718
At time: 4.237500429153442 and batch: 300, loss is 4.881018619537354 and perplexity is 131.7648137640233
At time: 4.912031650543213 and batch: 350, loss is 4.89012523651123 and perplexity is 132.97022573573025
At time: 5.571783065795898 and batch: 400, loss is 4.87829029083252 and perplexity is 131.40580600862373
At time: 6.233135938644409 and batch: 450, loss is 4.779652509689331 and perplexity is 119.06296962727824
At time: 6.8948681354522705 and batch: 500, loss is 4.806045761108399 and perplexity is 122.24726563468549
At time: 7.55616307258606 and batch: 550, loss is 4.829592142105103 and perplexity is 125.15990278881912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.942525985393118 and perplexity of 140.12375345365592
Finished 1 epochs...
Completing Train Step...
At time: 9.287236213684082 and batch: 50, loss is 4.7733686161041256 and perplexity is 118.31713641870385
At time: 9.94302773475647 and batch: 100, loss is 4.697637195587158 and perplexity is 109.68769545528903
At time: 10.598518371582031 and batch: 150, loss is 4.6651514053344725 and perplexity is 106.18166047482397
At time: 11.254059076309204 and batch: 200, loss is 4.60756028175354 and perplexity is 100.23929543212863
At time: 11.90972375869751 and batch: 250, loss is 4.575141572952271 and perplexity is 97.04177665529458
At time: 12.564483642578125 and batch: 300, loss is 4.553190889358521 and perplexity is 94.9348521353536
At time: 13.231665849685669 and batch: 350, loss is 4.590920457839966 and perplexity is 98.58513186952689
At time: 13.886905193328857 and batch: 400, loss is 4.57957239151001 and perplexity is 97.47270513812997
At time: 14.54818058013916 and batch: 450, loss is 4.503815908432006 and perplexity is 90.36128464208969
At time: 15.21209716796875 and batch: 500, loss is 4.5337350559234615 and perplexity is 93.10566732221282
At time: 15.8766028881073 and batch: 550, loss is 4.58471848487854 and perplexity is 97.97560164602622
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.825419973819814 and perplexity of 124.6388024269143
Finished 2 epochs...
Completing Train Step...
At time: 17.594300746917725 and batch: 50, loss is 4.538802146911621 and perplexity is 93.57863949458479
At time: 18.270145893096924 and batch: 100, loss is 4.4907627773284915 and perplexity is 89.18945163038278
At time: 18.958154916763306 and batch: 150, loss is 4.476324300765992 and perplexity is 87.91094388620152
At time: 19.650853633880615 and batch: 200, loss is 4.434030961990357 and perplexity is 84.27042406507458
At time: 20.339170455932617 and batch: 250, loss is 4.397007160186767 and perplexity is 81.2074636801532
At time: 21.025955200195312 and batch: 300, loss is 4.377701358795166 and perplexity is 79.65472517242799
At time: 21.715436220169067 and batch: 350, loss is 4.4123279476165775 and perplexity is 82.46120560187006
At time: 22.407397270202637 and batch: 400, loss is 4.407945680618286 and perplexity is 82.10062922993933
At time: 23.100245475769043 and batch: 450, loss is 4.344673442840576 and perplexity is 77.06686663261246
At time: 23.798393726348877 and batch: 500, loss is 4.378913860321045 and perplexity is 79.75136522450866
At time: 24.49648141860962 and batch: 550, loss is 4.432872772216797 and perplexity is 84.17287942021784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.784377077792553 and perplexity of 119.626821665695
Finished 3 epochs...
Completing Train Step...
At time: 26.286771059036255 and batch: 50, loss is 4.385618648529053 and perplexity is 80.28787782945959
At time: 26.97892689704895 and batch: 100, loss is 4.351087465286255 and perplexity is 77.56276389300473
At time: 27.6718966960907 and batch: 150, loss is 4.3502440261840825 and perplexity is 77.49737200597866
At time: 28.36459255218506 and batch: 200, loss is 4.31391791343689 and perplexity is 74.73271241154211
At time: 29.052144050598145 and batch: 250, loss is 4.274645662307739 and perplexity is 71.85467397890636
At time: 29.740328550338745 and batch: 300, loss is 4.255946445465088 and perplexity is 70.5235322896675
At time: 30.42927074432373 and batch: 350, loss is 4.300285558700562 and perplexity is 73.72084232208195
At time: 31.119215726852417 and batch: 400, loss is 4.2898960781097415 and perplexity is 72.95888608029733
At time: 31.813718557357788 and batch: 450, loss is 4.222097778320313 and perplexity is 68.17635325913771
At time: 32.506524324417114 and batch: 500, loss is 4.261464600563049 and perplexity is 70.91376777865125
At time: 33.21086311340332 and batch: 550, loss is 4.314629554748535 and perplexity is 74.78591422508441
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.787361307347075 and perplexity of 119.98434876856322
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 35.01041555404663 and batch: 50, loss is 4.2407553052902225 and perplexity is 69.46029575952417
At time: 35.72144532203674 and batch: 100, loss is 4.161752815246582 and perplexity is 64.1839266245868
At time: 36.419230222702026 and batch: 150, loss is 4.143448610305786 and perplexity is 63.019777790400695
At time: 37.11727499961853 and batch: 200, loss is 4.089636564254761 and perplexity is 59.71818403699323
At time: 37.81567692756653 and batch: 250, loss is 4.03990701675415 and perplexity is 56.82105915331458
At time: 38.51663064956665 and batch: 300, loss is 4.000072708129883 and perplexity is 54.60211990684752
At time: 39.22045922279358 and batch: 350, loss is 4.0236478042602535 and perplexity is 55.90466360167096
At time: 39.92446494102478 and batch: 400, loss is 3.996337070465088 and perplexity is 54.398526683164896
At time: 40.62902879714966 and batch: 450, loss is 3.9203041791915894 and perplexity is 50.41577787650034
At time: 41.33026599884033 and batch: 500, loss is 3.942995619773865 and perplexity is 51.57286281839537
At time: 42.028279542922974 and batch: 550, loss is 3.9793577671051024 and perplexity is 53.48267486221122
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.65140095162899 and perplexity of 104.73160676364067
Finished 5 epochs...
Completing Train Step...
At time: 43.8102343082428 and batch: 50, loss is 4.048685579299927 and perplexity is 57.32206220090127
At time: 44.51105499267578 and batch: 100, loss is 4.000247492790222 and perplexity is 54.61166435391639
At time: 45.199397563934326 and batch: 150, loss is 3.9953829526901243 and perplexity is 54.346648834648896
At time: 45.896881103515625 and batch: 200, loss is 3.9570147514343263 and perplexity is 52.300961301842904
At time: 46.600510597229004 and batch: 250, loss is 3.9134385490417483 and perplexity is 50.070827298418315
At time: 47.297343254089355 and batch: 300, loss is 3.8790205192565916 and perplexity is 48.37680770625888
At time: 47.994460105895996 and batch: 350, loss is 3.918104362487793 and perplexity is 50.30499430264613
At time: 48.68783950805664 and batch: 400, loss is 3.896681342124939 and perplexity is 49.23877102475689
At time: 49.379639625549316 and batch: 450, loss is 3.827806134223938 and perplexity is 45.96159397758629
At time: 50.07218790054321 and batch: 500, loss is 3.858759016990662 and perplexity is 47.406484208400286
At time: 50.764440298080444 and batch: 550, loss is 3.9047928619384766 and perplexity is 49.63979655594366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.65354984364611 and perplexity of 104.95690566215994
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 52.55020594596863 and batch: 50, loss is 3.937367706298828 and perplexity is 51.28343042297482
At time: 53.25729942321777 and batch: 100, loss is 3.877641034126282 and perplexity is 48.310118628247636
At time: 53.95072650909424 and batch: 150, loss is 3.861895170211792 and perplexity is 47.555391582676
At time: 54.64536809921265 and batch: 200, loss is 3.823060221672058 and perplexity is 45.74398106620103
At time: 55.339500188827515 and batch: 250, loss is 3.764884605407715 and perplexity is 43.1587252889918
At time: 56.03921580314636 and batch: 300, loss is 3.726812629699707 and perplexity is 41.54647290365982
At time: 56.74356961250305 and batch: 350, loss is 3.7575486087799073 and perplexity is 42.84327152441384
At time: 57.44641828536987 and batch: 400, loss is 3.7235198974609376 and perplexity is 41.409896471131766
At time: 58.14724946022034 and batch: 450, loss is 3.649106755256653 and perplexity is 38.44031410046461
At time: 58.844648599624634 and batch: 500, loss is 3.6761892557144167 and perplexity is 39.495599305548815
At time: 59.542224407196045 and batch: 550, loss is 3.7160556888580323 and perplexity is 41.10195506482343
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.605678477185838 and perplexity of 100.0508420399607
Finished 7 epochs...
Completing Train Step...
At time: 61.322537899017334 and batch: 50, loss is 3.8362808847427368 and perplexity is 46.35276220537881
At time: 62.02507948875427 and batch: 100, loss is 3.790184817314148 and perplexity is 44.26458038090546
At time: 62.725526332855225 and batch: 150, loss is 3.781856880187988 and perplexity is 43.89747846111042
At time: 63.41921305656433 and batch: 200, loss is 3.748977026939392 and perplexity is 42.47760631968507
At time: 64.11158657073975 and batch: 250, loss is 3.6955172204971314 and perplexity is 40.26639380714943
At time: 64.80829119682312 and batch: 300, loss is 3.663980450630188 and perplexity is 39.016336796244225
At time: 65.5070493221283 and batch: 350, loss is 3.7014381980895994 and perplexity is 40.50551744686854
At time: 66.20795583724976 and batch: 400, loss is 3.6741905879974364 and perplexity is 39.41673955971967
At time: 66.9119348526001 and batch: 450, loss is 3.6041632795333864 and perplexity is 36.75092074142339
At time: 67.61484336853027 and batch: 500, loss is 3.637855792045593 and perplexity is 38.0102474095842
At time: 68.32574105262756 and batch: 550, loss is 3.6824264335632324 and perplexity is 39.74271021892077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.611474219788897 and perplexity of 100.63239460423769
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 70.13082361221313 and batch: 50, loss is 3.779517583847046 and perplexity is 43.794909267014155
At time: 70.83617186546326 and batch: 100, loss is 3.72950834274292 and perplexity is 41.65862136468629
At time: 71.5275046825409 and batch: 150, loss is 3.7181138372421265 and perplexity is 41.18663610039925
At time: 72.2198441028595 and batch: 200, loss is 3.679513916969299 and perplexity is 39.62712731618647
At time: 72.91334915161133 and batch: 250, loss is 3.6199262046813967 and perplexity is 37.334812586003366
At time: 73.60871195793152 and batch: 300, loss is 3.581628885269165 and perplexity is 35.932022346196966
At time: 74.30434656143188 and batch: 350, loss is 3.6187323427200315 and perplexity is 37.290266569610466
At time: 75.00003337860107 and batch: 400, loss is 3.5828665781021116 and perplexity is 35.97652268592313
At time: 75.6963038444519 and batch: 450, loss is 3.506888847351074 and perplexity is 33.3443668267032
At time: 76.39560341835022 and batch: 500, loss is 3.5373748111724854 and perplexity is 34.37655568221123
At time: 77.09686803817749 and batch: 550, loss is 3.5771745252609253 and perplexity is 35.77232412358291
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.590108343895445 and perplexity of 98.50510201029411
Finished 9 epochs...
Completing Train Step...
At time: 78.89335036277771 and batch: 50, loss is 3.7273952293395998 and perplexity is 41.57068491608233
At time: 79.59638595581055 and batch: 100, loss is 3.6812036418914795 and perplexity is 39.694142863782446
At time: 80.28717923164368 and batch: 150, loss is 3.6729266119003294 and perplexity is 39.36694921662097
At time: 80.97890853881836 and batch: 200, loss is 3.6408026599884034 and perplexity is 38.12242379248857
At time: 81.67030835151672 and batch: 250, loss is 3.584430775642395 and perplexity is 36.03284110930314
At time: 82.36601829528809 and batch: 300, loss is 3.550198850631714 and perplexity is 34.820240826108794
At time: 83.06150102615356 and batch: 350, loss is 3.590601806640625 and perplexity is 36.25588839677234
At time: 83.7594838142395 and batch: 400, loss is 3.558384590148926 and perplexity is 35.10644002539084
At time: 84.46253967285156 and batch: 450, loss is 3.4862344217300416 and perplexity is 32.662721807979274
At time: 85.16678786277771 and batch: 500, loss is 3.520590500831604 and perplexity is 33.80438408827927
At time: 85.86692905426025 and batch: 550, loss is 3.5643446826934815 and perplexity is 35.31630243530478
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.594061344227892 and perplexity of 98.89526335719368
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 87.65939617156982 and batch: 50, loss is 3.6975863933563233 and perplexity is 40.349798195643395
At time: 88.3635036945343 and batch: 100, loss is 3.6531040716171264 and perplexity is 38.594279716546126
At time: 89.05627775192261 and batch: 150, loss is 3.6402425622940062 and perplexity is 38.10107748938319
At time: 89.75148391723633 and batch: 200, loss is 3.607060031890869 and perplexity is 36.857533398403056
At time: 90.44770812988281 and batch: 250, loss is 3.5463629817962645 and perplexity is 34.68693079284117
At time: 91.1465208530426 and batch: 300, loss is 3.5083732271194457 and perplexity is 33.393899283603446
At time: 91.86186599731445 and batch: 350, loss is 3.5469384002685547 and perplexity is 34.706896037200735
At time: 92.56433081626892 and batch: 400, loss is 3.5106950187683106 and perplexity is 33.47152303830523
At time: 93.2665913105011 and batch: 450, loss is 3.433754415512085 and perplexity is 30.992784381725876
At time: 93.967702627182 and batch: 500, loss is 3.467865786552429 and perplexity is 32.068228914121484
At time: 94.66797852516174 and batch: 550, loss is 3.5073373126983642 and perplexity is 33.35932397338261
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.5851404717627995 and perplexity of 98.01695478952675
Finished 11 epochs...
Completing Train Step...
At time: 96.47805190086365 and batch: 50, loss is 3.6700124549865722 and perplexity is 39.252394745309445
At time: 97.18200969696045 and batch: 100, loss is 3.62668523311615 and perplexity is 37.58801438099607
At time: 97.86925101280212 and batch: 150, loss is 3.616388773918152 and perplexity is 37.202976589253495
At time: 98.55871796607971 and batch: 200, loss is 3.586099820137024 and perplexity is 36.09303174083075
At time: 99.24992489814758 and batch: 250, loss is 3.5276010036468506 and perplexity is 34.04220245726335
At time: 99.94535779953003 and batch: 300, loss is 3.4916885232925416 and perplexity is 32.84135430710578
At time: 100.63555598258972 and batch: 350, loss is 3.5328796243667604 and perplexity is 34.22237344129597
At time: 101.33742833137512 and batch: 400, loss is 3.4992162084579466 and perplexity is 33.089506516773035
At time: 102.03768229484558 and batch: 450, loss is 3.4247992992401124 and perplexity is 30.716479412060345
At time: 102.72815608978271 and batch: 500, loss is 3.461680359840393 and perplexity is 31.870485428770156
At time: 103.41947197914124 and batch: 550, loss is 3.5033790016174318 and perplexity is 33.22753838788853
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.587090350211935 and perplexity of 98.20826238999135
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 105.19480180740356 and batch: 50, loss is 3.654687376022339 and perplexity is 38.65543461026813
At time: 105.89556312561035 and batch: 100, loss is 3.6129964780807495 and perplexity is 37.076986904592836
At time: 106.58489418029785 and batch: 150, loss is 3.6010569286346437 and perplexity is 36.636936614752486
At time: 107.29445767402649 and batch: 200, loss is 3.5695918798446655 and perplexity is 35.502101071570614
At time: 107.99235010147095 and batch: 250, loss is 3.509119997024536 and perplexity is 33.41884615622958
At time: 108.69183111190796 and batch: 300, loss is 3.4706767845153808 and perplexity is 32.158499455988235
At time: 109.3920316696167 and batch: 350, loss is 3.510100841522217 and perplexity is 33.451640928257405
At time: 110.09092473983765 and batch: 400, loss is 3.4748509883880616 and perplexity is 32.29301614367844
At time: 110.79065728187561 and batch: 450, loss is 3.3971972274780273 and perplexity is 29.8802350732614
At time: 111.49027109146118 and batch: 500, loss is 3.4326986837387086 and perplexity is 30.960081580239443
At time: 112.18822479248047 and batch: 550, loss is 3.4726841831207276 and perplexity is 32.223119220058784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.582357528361868 and perplexity of 97.74455835974324
Finished 13 epochs...
Completing Train Step...
At time: 113.96495604515076 and batch: 50, loss is 3.6410146236419676 and perplexity is 38.13050521717224
At time: 114.66457772254944 and batch: 100, loss is 3.5986079454421995 and perplexity is 36.547323148484864
At time: 115.35150122642517 and batch: 150, loss is 3.587824959754944 and perplexity is 36.155350999072645
At time: 116.03842806816101 and batch: 200, loss is 3.557663502693176 and perplexity is 35.08113433677632
At time: 116.7260901927948 and batch: 250, loss is 3.498691534996033 and perplexity is 33.07214988451619
At time: 117.41317868232727 and batch: 300, loss is 3.4617251777648925 and perplexity is 31.871913829788614
At time: 118.10172605514526 and batch: 350, loss is 3.5030075693130494 and perplexity is 33.215198898520505
At time: 118.79670739173889 and batch: 400, loss is 3.469434723854065 and perplexity is 32.11858144431483
At time: 119.49860978126526 and batch: 450, loss is 3.3933894872665404 and perplexity is 29.766675241021627
At time: 120.20347547531128 and batch: 500, loss is 3.430815587043762 and perplexity is 30.901835611547927
At time: 120.90828680992126 and batch: 550, loss is 3.4720451259613037 and perplexity is 32.20253338348596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.583441227040392 and perplexity of 97.85054142495225
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 122.70072889328003 and batch: 50, loss is 3.6333516216278077 and perplexity is 37.83942776681271
At time: 123.40444374084473 and batch: 100, loss is 3.5923567485809325 and perplexity is 36.31957123940434
At time: 124.09568762779236 and batch: 150, loss is 3.580860676765442 and perplexity is 35.90442966089937
At time: 124.78776478767395 and batch: 200, loss is 3.5491776514053344 and perplexity is 34.784700573042876
At time: 125.48241114616394 and batch: 250, loss is 3.4891760683059694 and perplexity is 32.75894545033268
At time: 126.18106293678284 and batch: 300, loss is 3.451090888977051 and perplexity is 31.534774489651724
At time: 126.88213515281677 and batch: 350, loss is 3.4912317991256714 and perplexity is 32.826358291702974
At time: 127.58386564254761 and batch: 400, loss is 3.456535882949829 and perplexity is 31.706949467151983
At time: 128.2865071296692 and batch: 450, loss is 3.3793394565582275 and perplexity is 29.351376849132922
At time: 128.98568987846375 and batch: 500, loss is 3.4149862241744997 and perplexity is 30.41653041186378
At time: 129.68215417861938 and batch: 550, loss is 3.454925870895386 and perplexity is 31.655941968662177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.581498491003158 and perplexity of 97.66062818723978
Finished 15 epochs...
Completing Train Step...
At time: 131.4597294330597 and batch: 50, loss is 3.6264188957214354 and perplexity is 37.57800462021931
At time: 132.16296482086182 and batch: 100, loss is 3.58474413394928 and perplexity is 36.044134068664306
At time: 132.85302829742432 and batch: 150, loss is 3.5736137008666993 and perplexity is 35.64517167732382
At time: 133.5485589504242 and batch: 200, loss is 3.542645506858826 and perplexity is 34.55822238033605
At time: 134.247474193573 and batch: 250, loss is 3.48339195728302 and perplexity is 32.57001100849504
At time: 134.94560623168945 and batch: 300, loss is 3.446215896606445 and perplexity is 31.381416817066818
At time: 135.64038038253784 and batch: 350, loss is 3.487470121383667 and perplexity is 32.703108069509554
At time: 136.3340039253235 and batch: 400, loss is 3.454096736907959 and perplexity is 31.629705829412355
At time: 137.02732825279236 and batch: 450, loss is 3.3778858089447024 and perplexity is 29.30874128627234
At time: 137.72091031074524 and batch: 500, loss is 3.4145459842681887 and perplexity is 30.40314278846421
At time: 138.41336584091187 and batch: 550, loss is 3.4553509616851805 and perplexity is 31.669401478589503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.5819383986452795 and perplexity of 97.70359929488008
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 140.20105004310608 and batch: 50, loss is 3.6223673391342164 and perplexity is 37.42606321550981
At time: 140.9082911014557 and batch: 100, loss is 3.5822254371643067 and perplexity is 35.95346405713464
At time: 141.6012053489685 and batch: 150, loss is 3.5710413789749147 and perplexity is 35.55359865002689
At time: 142.28913283348083 and batch: 200, loss is 3.538612222671509 and perplexity is 34.4191199568395
At time: 142.97748231887817 and batch: 250, loss is 3.478421940803528 and perplexity is 32.40853910842491
At time: 143.66624069213867 and batch: 300, loss is 3.4406514072418215 and perplexity is 31.20728019695039
At time: 144.3550524711609 and batch: 350, loss is 3.4811703062057493 and perplexity is 32.49773212740741
At time: 145.04383492469788 and batch: 400, loss is 3.447303681373596 and perplexity is 31.415571617419932
At time: 145.73681640625 and batch: 450, loss is 3.370636215209961 and perplexity is 29.09703314529746
At time: 146.4313383102417 and batch: 500, loss is 3.40611279964447 and perplexity is 30.14782555402426
At time: 147.1363594532013 and batch: 550, loss is 3.4458508491516113 and perplexity is 31.36996320141248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.58041057180851 and perplexity of 97.55443908832653
Finished 17 epochs...
Completing Train Step...
At time: 148.91031694412231 and batch: 50, loss is 3.618279538154602 and perplexity is 37.27338518893322
At time: 149.6038339138031 and batch: 100, loss is 3.577781558036804 and perplexity is 35.794045688979345
At time: 150.2877242565155 and batch: 150, loss is 3.5669429969787596 and perplexity is 35.40818460611093
At time: 150.97672581672668 and batch: 200, loss is 3.535074076652527 and perplexity is 34.29755526821394
At time: 151.6860704421997 and batch: 250, loss is 3.475266842842102 and perplexity is 32.306448130958486
At time: 152.39772629737854 and batch: 300, loss is 3.43807354927063 and perplexity is 31.126935863568622
At time: 153.10363960266113 and batch: 350, loss is 3.479338388442993 and perplexity is 32.43825345132935
At time: 153.8059377670288 and batch: 400, loss is 3.446220655441284 and perplexity is 31.381566156401806
At time: 154.5057499408722 and batch: 450, loss is 3.3702225351333617 and perplexity is 29.084998771759
At time: 155.20447897911072 and batch: 500, loss is 3.406252398490906 and perplexity is 30.15203444946579
At time: 155.90512132644653 and batch: 550, loss is 3.44643497467041 and perplexity is 31.388292550241236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.5805089422997005 and perplexity of 97.56403603843803
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 157.70225977897644 and batch: 50, loss is 3.616031470298767 and perplexity is 37.18968620555875
At time: 158.41127371788025 and batch: 100, loss is 3.5762342643737792 and perplexity is 35.73870461439885
At time: 159.1080813407898 and batch: 150, loss is 3.5660141801834104 and perplexity is 35.375312158160085
At time: 159.80510830879211 and batch: 200, loss is 3.5334835052490234 and perplexity is 34.24304591959183
At time: 160.4962329864502 and batch: 250, loss is 3.4725344038009642 and perplexity is 32.218293224607535
At time: 161.18797993659973 and batch: 300, loss is 3.4352845430374144 and perplexity is 31.040243594259014
At time: 161.88029170036316 and batch: 350, loss is 3.4758095026016234 and perplexity is 32.32398429798798
At time: 162.57160210609436 and batch: 400, loss is 3.442458400726318 and perplexity is 31.26372252904023
At time: 163.26329636573792 and batch: 450, loss is 3.3663250255584716 and perplexity is 28.971860332854707
At time: 163.95655632019043 and batch: 500, loss is 3.4015254402160644 and perplexity is 30.00984337113004
At time: 164.6526334285736 and batch: 550, loss is 3.4413950347900393 and perplexity is 31.23049542088002
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.579843399372507 and perplexity of 97.49912458737879
Finished 19 epochs...
Completing Train Step...
At time: 166.43536472320557 and batch: 50, loss is 3.6139778470993043 and perplexity is 37.11339097083206
At time: 167.12866067886353 and batch: 100, loss is 3.57369797706604 and perplexity is 35.64817584350572
At time: 167.8094551563263 and batch: 150, loss is 3.5636095094680784 and perplexity is 35.2903483768675
At time: 168.4956831932068 and batch: 200, loss is 3.531514563560486 and perplexity is 34.17568969094169
At time: 169.18221807479858 and batch: 250, loss is 3.47084942817688 and perplexity is 32.16405189636566
At time: 169.87082052230835 and batch: 300, loss is 3.4339150476455687 and perplexity is 30.9977632186736
At time: 170.56184005737305 and batch: 350, loss is 3.4749606609344483 and perplexity is 32.29655799520782
At time: 171.25625777244568 and batch: 400, loss is 3.4420378828048706 and perplexity is 31.250578337303395
At time: 171.95432043075562 and batch: 450, loss is 3.366339683532715 and perplexity is 28.972285004749654
At time: 172.65131378173828 and batch: 500, loss is 3.401864309310913 and perplexity is 30.020014502833533
At time: 173.35003447532654 and batch: 550, loss is 3.4418816566467285 and perplexity is 31.24569656085056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.579831387134308 and perplexity of 97.4979534117043
Finished 20 epochs...
Completing Train Step...
At time: 175.13206791877747 and batch: 50, loss is 3.6123481702804567 and perplexity is 37.052957394872166
At time: 175.82868671417236 and batch: 100, loss is 3.571961545944214 and perplexity is 35.58632895350703
At time: 176.51612496376038 and batch: 150, loss is 3.5620277309417725 and perplexity is 35.23457098698759
At time: 177.2054886817932 and batch: 200, loss is 3.5300768280029295 and perplexity is 34.126589391749896
At time: 177.89047932624817 and batch: 250, loss is 3.4695383739471435 and perplexity is 32.1219107108075
At time: 178.57978534698486 and batch: 300, loss is 3.4328100442886353 and perplexity is 30.963529503927774
At time: 179.26945090293884 and batch: 350, loss is 3.4741725444793703 and perplexity is 32.27111457391588
At time: 179.95755004882812 and batch: 400, loss is 3.4415354490280152 and perplexity is 31.23488093498326
At time: 180.64289593696594 and batch: 450, loss is 3.3661546087265015 and perplexity is 28.966923460875492
At time: 181.3299012184143 and batch: 500, loss is 3.4019004774093626 and perplexity is 30.021100289308823
At time: 182.0167727470398 and batch: 550, loss is 3.4420567083358766 and perplexity is 31.25116665157249
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.5799268357297205 and perplexity of 97.50725989855151
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 183.77481150627136 and batch: 50, loss is 3.6112707901000975 and perplexity is 37.01305876980453
At time: 184.47443771362305 and batch: 100, loss is 3.571104211807251 and perplexity is 35.55583265351347
At time: 185.16632771492004 and batch: 150, loss is 3.5615120458602907 and perplexity is 35.21640572855613
At time: 185.86117362976074 and batch: 200, loss is 3.529589881896973 and perplexity is 34.10997562726824
At time: 186.5587980747223 and batch: 250, loss is 3.4681397342681883 and perplexity is 32.077015135608654
At time: 187.25720834732056 and batch: 300, loss is 3.4316255140304563 and perplexity is 30.926873980404707
At time: 187.95595145225525 and batch: 350, loss is 3.472186470031738 and perplexity is 32.2070853423213
At time: 188.64943385124207 and batch: 400, loss is 3.439323229789734 and perplexity is 31.165858904554238
At time: 189.34217596054077 and batch: 450, loss is 3.364002757072449 and perplexity is 28.90465795580495
At time: 190.03388237953186 and batch: 500, loss is 3.399165921211243 and perplexity is 29.93911804704988
At time: 190.72672629356384 and batch: 550, loss is 3.439332294464111 and perplexity is 31.166141414197323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.5798281405834445 and perplexity of 97.49763688015327
Finished 22 epochs...
Completing Train Step...
At time: 192.5282690525055 and batch: 50, loss is 3.61040144443512 and perplexity is 36.98089561009281
At time: 193.22904324531555 and batch: 100, loss is 3.569941658973694 and perplexity is 35.514521137575414
At time: 193.92091178894043 and batch: 150, loss is 3.5602333831787107 and perplexity is 35.17140460153385
At time: 194.61280345916748 and batch: 200, loss is 3.528561315536499 and perplexity is 34.07490929089997
At time: 195.30419397354126 and batch: 250, loss is 3.467347493171692 and perplexity is 32.05161246981185
At time: 195.99549317359924 and batch: 300, loss is 3.430926785469055 and perplexity is 30.905272038065664
At time: 196.68719601631165 and batch: 350, loss is 3.471795473098755 and perplexity is 32.194494932298376
At time: 197.3820903301239 and batch: 400, loss is 3.4391587924957276 and perplexity is 31.16073449638405
At time: 198.0836398601532 and batch: 450, loss is 3.364059567451477 and perplexity is 28.906300087023705
At time: 198.78572607040405 and batch: 500, loss is 3.399392409324646 and perplexity is 29.945899669362742
At time: 199.49182534217834 and batch: 550, loss is 3.439606943130493 and perplexity is 31.174702328941102
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.579827491273272 and perplexity of 97.49757357396638
Finished 23 epochs...
Completing Train Step...
At time: 201.30421686172485 and batch: 50, loss is 3.6095541524887085 and perplexity is 36.94957526568283
At time: 202.0028760433197 and batch: 100, loss is 3.5690265846252442 and perplexity is 35.482037574989086
At time: 202.6880168914795 and batch: 150, loss is 3.5593779420852663 and perplexity is 35.141330401910615
At time: 203.38440680503845 and batch: 200, loss is 3.527810034751892 and perplexity is 34.04931908023291
At time: 204.081383228302 and batch: 250, loss is 3.466673741340637 and perplexity is 32.03002491036752
At time: 204.77922439575195 and batch: 300, loss is 3.4303566074371337 and perplexity is 30.887655553622483
At time: 205.47596073150635 and batch: 350, loss is 3.47141685962677 and perplexity is 32.18230797001288
At time: 206.17547869682312 and batch: 400, loss is 3.4389442443847655 and perplexity is 31.15404973678857
At time: 206.87699341773987 and batch: 450, loss is 3.3640255689620973 and perplexity is 28.905317333193356
At time: 207.58354949951172 and batch: 500, loss is 3.399483289718628 and perplexity is 29.94862128819186
At time: 208.28449082374573 and batch: 550, loss is 3.439747705459595 and perplexity is 31.179090861512762
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.5798658005734705 and perplexity of 97.50130870932583
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 210.08135890960693 and batch: 50, loss is 3.608989200592041 and perplexity is 36.928706428555046
At time: 210.78537678718567 and batch: 100, loss is 3.5685352325439452 and perplexity is 35.46460768443505
At time: 211.4772868156433 and batch: 150, loss is 3.5589599227905273 and perplexity is 35.12664371763238
At time: 212.17428064346313 and batch: 200, loss is 3.527550554275513 and perplexity is 34.04048509287077
At time: 212.8682656288147 and batch: 250, loss is 3.465980262756348 and perplexity is 32.00782047407832
At time: 213.5625400543213 and batch: 300, loss is 3.4299000883102417 and perplexity is 30.873557966231388
At time: 214.25657558441162 and batch: 350, loss is 3.4703766202926634 and perplexity is 32.148848073567585
At time: 214.9489505290985 and batch: 400, loss is 3.4376936674118044 and perplexity is 31.115113550996437
At time: 215.63812851905823 and batch: 450, loss is 3.362808918952942 and perplexity is 28.870171063285493
At time: 216.33034896850586 and batch: 500, loss is 3.3979516983032227 and perplexity is 29.902787345320032
At time: 217.0214102268219 and batch: 550, loss is 3.438244023323059 and perplexity is 31.132242650786896
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.579888851084608 and perplexity of 97.50355619023081
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 218.7958221435547 and batch: 50, loss is 3.6087299680709837 and perplexity is 36.919134547613204
At time: 219.50117087364197 and batch: 100, loss is 3.568193974494934 and perplexity is 35.45250716642473
At time: 220.1960940361023 and batch: 150, loss is 3.558492603302002 and perplexity is 35.11023218746926
At time: 220.89483308792114 and batch: 200, loss is 3.5272511625289917 and perplexity is 34.03029517805071
At time: 221.59187412261963 and batch: 250, loss is 3.4655825471878052 and perplexity is 31.99509299668638
At time: 222.2864363193512 and batch: 300, loss is 3.429645256996155 and perplexity is 30.86569141924814
At time: 222.98013615608215 and batch: 350, loss is 3.4698434400558473 and perplexity is 32.1317115119823
At time: 223.67222595214844 and batch: 400, loss is 3.4370650625228882 and perplexity is 31.09556058468806
At time: 224.36437439918518 and batch: 450, loss is 3.362203106880188 and perplexity is 28.85268646183895
At time: 225.0564911365509 and batch: 500, loss is 3.3971903038024904 and perplexity is 29.88002819292497
At time: 225.74924850463867 and batch: 550, loss is 3.4374768590927123 and perplexity is 31.108368266772374
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.579897616771942 and perplexity of 97.50441087966432
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 227.5222225189209 and batch: 50, loss is 3.6085849380493165 and perplexity is 36.91378055298419
At time: 228.2212724685669 and batch: 100, loss is 3.5680062532424928 and perplexity is 35.445852601998396
At time: 228.9073841571808 and batch: 150, loss is 3.5582395076751707 and perplexity is 35.10134706568575
At time: 229.5995168685913 and batch: 200, loss is 3.5270796251296996 and perplexity is 34.02445821036216
At time: 230.29620003700256 and batch: 250, loss is 3.46537335395813 and perplexity is 31.98840053988138
At time: 230.99490880966187 and batch: 300, loss is 3.4295107650756838 and perplexity is 30.86154051227048
At time: 231.69243359565735 and batch: 350, loss is 3.4695653820037844 and perplexity is 32.12277827290692
At time: 232.3984296321869 and batch: 400, loss is 3.436743388175964 and perplexity is 31.085559549169265
At time: 233.10690307617188 and batch: 450, loss is 3.361895112991333 and perplexity is 28.843801379077487
At time: 233.81479835510254 and batch: 500, loss is 3.396810417175293 and perplexity is 29.868679325572145
At time: 234.51191329956055 and batch: 550, loss is 3.4370888614654542 and perplexity is 31.096300634954645
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.57990053866772 and perplexity of 97.504695777807
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 236.30726861953735 and batch: 50, loss is 3.6085064220428467 and perplexity is 36.910882344130854
At time: 237.0043487548828 and batch: 100, loss is 3.567906279563904 and perplexity is 35.442309126853125
At time: 237.68946051597595 and batch: 150, loss is 3.5581101846694945 and perplexity is 35.09680794749246
At time: 238.37974858283997 and batch: 200, loss is 3.526989641189575 and perplexity is 34.02139669329754
At time: 239.070734500885 and batch: 250, loss is 3.4652672529220583 and perplexity is 31.98500671748904
At time: 239.7622570991516 and batch: 300, loss is 3.429441933631897 and perplexity is 30.85941634098526
At time: 240.45765709877014 and batch: 350, loss is 3.469423384666443 and perplexity is 32.11821724775854
At time: 241.15538334846497 and batch: 400, loss is 3.436579747200012 and perplexity is 31.080473094054213
At time: 241.8513126373291 and batch: 450, loss is 3.3617390966415406 and perplexity is 28.839301625498884
At time: 242.54246425628662 and batch: 500, loss is 3.3966203784942626 and perplexity is 29.863003660464035
At time: 243.23838305473328 and batch: 550, loss is 3.4368942070007322 and perplexity is 31.090248190286513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.579901837288065 and perplexity of 97.50482239947094
Annealing...
Model not improving. Stopping early with 97.49757357396638loss at 27 epochs.
Finished Training.
Improved accuracyfrom -109.40635837025204 to -97.49757357396638
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efc44ef8470>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'dropout': 0.4198681236269237, 'wordvec_dim': 200, 'tune_wordvecs': True, 'anneal': 2.4277448039092633, 'num_layers': 1, 'seq_len': 35, 'data': 'ptb', 'lr': 15.323145958939651, 'wordvec_source': '', 'batch_size': 50}, 'best_accuracy': -116.32710508683068}, {'params': {'dropout': 0.8557166803151515, 'wordvec_dim': 200, 'tune_wordvecs': True, 'anneal': 3.319342381826927, 'num_layers': 1, 'seq_len': 35, 'data': 'ptb', 'lr': 20.46347783894298, 'wordvec_source': '', 'batch_size': 50}, 'best_accuracy': -146.2114963224749}, {'params': {'dropout': 0.8761539628884351, 'wordvec_dim': 200, 'tune_wordvecs': True, 'anneal': 4.520964925982288, 'num_layers': 1, 'seq_len': 35, 'data': 'ptb', 'lr': 18.63581596300943, 'wordvec_source': '', 'batch_size': 50}, 'best_accuracy': -143.82674627596467}, {'params': {'dropout': 0.17772046552728626, 'wordvec_dim': 200, 'tune_wordvecs': True, 'anneal': 5.851289093719012, 'num_layers': 1, 'seq_len': 35, 'data': 'ptb', 'lr': 29.943287054749074, 'wordvec_source': '', 'batch_size': 50}, 'best_accuracy': -145.33605893601035}, {'params': {'dropout': 0.2456845229564283, 'wordvec_dim': 200, 'tune_wordvecs': True, 'anneal': 3.3023919730972473, 'num_layers': 1, 'seq_len': 35, 'data': 'ptb', 'lr': 13.7562397440194, 'wordvec_source': '', 'batch_size': 50}, 'best_accuracy': -109.40635837025204}, {'params': {'dropout': 0.0, 'wordvec_dim': 200, 'tune_wordvecs': True, 'anneal': 2.0, 'num_layers': 1, 'seq_len': 35, 'data': 'ptb', 'lr': 11.499079118767963, 'wordvec_source': '', 'batch_size': 50}, 'best_accuracy': -97.49757357396638}]
