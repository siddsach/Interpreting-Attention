Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'name': 'lr', 'type': 'continuous', 'domain': [0, 30]}, {'name': 'dropout', 'type': 'continuous', 'domain': [0, 1]}, {'name': 'anneal', 'type': 'continuous', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'num_layers': 1, 'wordvec_source': 'glove', 'data': 'ptb', 'dropout': 0.3681258570530319, 'seq_len': 20, 'batch_size': 50, 'lr': 12.725560315816402, 'anneal': 6.578833205369682, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.896712064743042 and batch: 50, loss is 6.683318881988526 and perplexity is 798.9663917484285
At time: 1.2513058185577393 and batch: 100, loss is 5.849346227645874 and perplexity is 347.0074424313695
At time: 1.5838685035705566 and batch: 150, loss is 5.678229188919067 and perplexity is 292.43113086618575
At time: 1.9187850952148438 and batch: 200, loss is 5.607116661071777 and perplexity is 272.3578049117173
At time: 2.2552504539489746 and batch: 250, loss is 5.571274633407593 and perplexity is 262.7688197881403
At time: 2.590374231338501 and batch: 300, loss is 5.445360145568848 and perplexity is 231.6807034676162
At time: 2.9221620559692383 and batch: 350, loss is 5.491733999252319 and perplexity is 242.67764508795676
At time: 3.2559096813201904 and batch: 400, loss is 5.437035341262817 and perplexity is 229.7600127293839
At time: 3.59367036819458 and batch: 450, loss is 5.438237800598144 and perplexity is 230.03645597414612
At time: 3.9262256622314453 and batch: 500, loss is 5.436611642837525 and perplexity is 229.66268439418252
At time: 4.2580437660217285 and batch: 550, loss is 5.486643981933594 and perplexity is 241.4455500243182
At time: 4.595170259475708 and batch: 600, loss is 5.509056835174561 and perplexity is 246.91813271131846
At time: 4.927301645278931 and batch: 650, loss is 5.485549516677857 and perplexity is 241.18144081420502
At time: 5.256714820861816 and batch: 700, loss is 5.480754289627075 and perplexity is 240.02768950560716
At time: 5.587859630584717 and batch: 750, loss is 5.381195106506348 and perplexity is 217.28179520432974
At time: 5.916951656341553 and batch: 800, loss is 5.435092353820801 and perplexity is 229.31402532424468
At time: 6.248141288757324 and batch: 850, loss is 5.40419111251831 and perplexity is 222.33630267503514
At time: 6.57979416847229 and batch: 900, loss is 5.502785940170288 and perplexity is 245.3745798137472
At time: 6.909183502197266 and batch: 950, loss is 5.448592023849487 and perplexity is 232.4306785625136
At time: 7.238216876983643 and batch: 1000, loss is 5.420803604125976 and perplexity is 226.06071285045212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.304304913776677 and perplexity of 201.20110168389598
Finished 1 epochs...
Completing Train Step...
At time: 8.370418787002563 and batch: 50, loss is 5.252808847427368 and perplexity is 191.10229248396652
At time: 8.709556818008423 and batch: 100, loss is 5.198132066726685 and perplexity is 180.93395347146904
At time: 9.035176992416382 and batch: 150, loss is 5.180259761810302 and perplexity is 177.72897219720707
At time: 9.361229181289673 and batch: 200, loss is 5.171684646606446 and perplexity is 176.21144158445415
At time: 9.68488597869873 and batch: 250, loss is 5.151026945114136 and perplexity is 172.60865894901858
At time: 10.008793830871582 and batch: 300, loss is 5.040073003768921 and perplexity is 154.48129233081085
At time: 10.333282947540283 and batch: 350, loss is 5.091484460830689 and perplexity is 162.6311024546714
At time: 10.65938949584961 and batch: 400, loss is 5.023634052276611 and perplexity is 151.96254139378502
At time: 10.983160018920898 and batch: 450, loss is 5.01772400856018 and perplexity is 151.06708483102398
At time: 11.308032274246216 and batch: 500, loss is 5.0265874099731445 and perplexity is 152.4120045209914
At time: 11.63142204284668 and batch: 550, loss is 5.049330329895019 and perplexity is 155.91801588317998
At time: 11.957285165786743 and batch: 600, loss is 5.05581000328064 and perplexity is 156.93159398220075
At time: 12.298836946487427 and batch: 650, loss is 5.050643825531006 and perplexity is 156.12294807592107
At time: 12.622165203094482 and batch: 700, loss is 5.012462520599366 and perplexity is 150.2743345348048
At time: 12.947542190551758 and batch: 750, loss is 4.955243244171142 and perplexity is 141.91712268772113
At time: 13.273603200912476 and batch: 800, loss is 4.988998041152954 and perplexity is 146.78927297791225
At time: 13.598217487335205 and batch: 850, loss is 4.97124303817749 and perplexity is 144.20602960775977
At time: 13.921889066696167 and batch: 900, loss is 5.081164121627808 and perplexity is 160.9613254654145
At time: 14.246104717254639 and batch: 950, loss is 5.019206552505493 and perplexity is 151.2912145229833
At time: 14.570125818252563 and batch: 1000, loss is 4.991576385498047 and perplexity is 147.16823460666106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.173874645698361 and perplexity of 176.59776735364647
Finished 2 epochs...
Completing Train Step...
At time: 15.689589262008667 and batch: 50, loss is 5.0555885124206545 and perplexity is 156.89683891759742
At time: 16.01603651046753 and batch: 100, loss is 4.979328022003174 and perplexity is 145.37665890748175
At time: 16.34245800971985 and batch: 150, loss is 4.986736698150635 and perplexity is 146.4577071160121
At time: 16.666635751724243 and batch: 200, loss is 4.989170198440552 and perplexity is 146.81454599640122
At time: 16.995699167251587 and batch: 250, loss is 4.973918657302857 and perplexity is 144.5923866602388
At time: 17.319750547409058 and batch: 300, loss is 4.87930272102356 and perplexity is 131.53891258309247
At time: 17.64620852470398 and batch: 350, loss is 4.9427235984802245 and perplexity is 140.15144647731466
At time: 17.971182584762573 and batch: 400, loss is 4.8736112022399904 and perplexity is 130.7923828505708
At time: 18.295191764831543 and batch: 450, loss is 4.918794469833374 and perplexity is 136.8375519165266
At time: 18.61883854866028 and batch: 500, loss is 4.895413665771485 and perplexity is 133.67529207173104
At time: 18.94230842590332 and batch: 550, loss is 4.918483200073243 and perplexity is 136.79496515289424
At time: 19.267601251602173 and batch: 600, loss is 4.942272653579712 and perplexity is 140.0882601450742
At time: 19.59402585029602 and batch: 650, loss is 4.932516260147095 and perplexity is 138.72814963915533
At time: 19.919184684753418 and batch: 700, loss is 4.897913331985474 and perplexity is 134.00985365490658
At time: 20.24413514137268 and batch: 750, loss is 4.83393853187561 and perplexity is 125.70508042898696
At time: 20.596667051315308 and batch: 800, loss is 4.881154527664185 and perplexity is 131.7827228900145
At time: 20.91931653022766 and batch: 850, loss is 4.858955039978027 and perplexity is 128.8894473932156
At time: 21.243177890777588 and batch: 900, loss is 4.967215404510498 and perplexity is 143.62638862227206
At time: 21.5667667388916 and batch: 950, loss is 4.91115026473999 and perplexity is 135.79552541149064
At time: 21.89274787902832 and batch: 1000, loss is 4.888616466522217 and perplexity is 132.76975551945984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.1346547196551064 and perplexity of 169.80567918805244
Finished 3 epochs...
Completing Train Step...
At time: 23.01597571372986 and batch: 50, loss is 4.967220249176026 and perplexity is 143.62708444577135
At time: 23.341745376586914 and batch: 100, loss is 4.879426307678223 and perplexity is 131.55517004183838
At time: 23.666476011276245 and batch: 150, loss is 4.897187032699585 and perplexity is 133.91255773115196
At time: 23.99207043647766 and batch: 200, loss is 4.901783666610718 and perplexity is 134.52952162740823
At time: 24.316858530044556 and batch: 250, loss is 4.889374103546142 and perplexity is 132.8703849173641
At time: 24.6413516998291 and batch: 300, loss is 4.802029695510864 and perplexity is 121.75729712815829
At time: 24.966662645339966 and batch: 350, loss is 4.861409797668457 and perplexity is 129.2062284066613
At time: 25.29155945777893 and batch: 400, loss is 4.799211215972901 and perplexity is 121.41460983302348
At time: 25.618499040603638 and batch: 450, loss is 4.84575758934021 and perplexity is 127.19961056942492
At time: 25.942035913467407 and batch: 500, loss is 4.813587055206299 and perplexity is 123.17265314152503
At time: 26.268166542053223 and batch: 550, loss is 4.843213014602661 and perplexity is 126.87635310455381
At time: 26.591933488845825 and batch: 600, loss is 4.872031726837158 and perplexity is 130.58596255978026
At time: 26.915460109710693 and batch: 650, loss is 4.862370958328247 and perplexity is 129.33047605181468
At time: 27.23988962173462 and batch: 700, loss is 4.833222351074219 and perplexity is 125.61508509405857
At time: 27.565712690353394 and batch: 750, loss is 4.765545797348023 and perplexity is 117.39517377980647
At time: 27.891406774520874 and batch: 800, loss is 4.805869140625 and perplexity is 122.22567617016188
At time: 28.215538263320923 and batch: 850, loss is 4.785660018920899 and perplexity is 119.78039432654035
At time: 28.538527727127075 and batch: 900, loss is 4.892260227203369 and perplexity is 133.25441919725466
At time: 28.892394542694092 and batch: 950, loss is 4.835304937362671 and perplexity is 125.87696194381623
At time: 29.21714472770691 and batch: 1000, loss is 4.822314119338989 and perplexity is 124.2522929756673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.10576908762862 and perplexity of 164.9708988317253
Finished 4 epochs...
Completing Train Step...
At time: 30.324946641921997 and batch: 50, loss is 4.896858606338501 and perplexity is 133.86858453848598
At time: 30.665891885757446 and batch: 100, loss is 4.80054874420166 and perplexity is 121.57711395375696
At time: 30.991469383239746 and batch: 150, loss is 4.828108129501342 and perplexity is 124.97430166685957
At time: 31.316386461257935 and batch: 200, loss is 4.830276870727539 and perplexity is 125.24563270415388
At time: 31.642008781433105 and batch: 250, loss is 4.827533082962036 and perplexity is 124.90245628638235
At time: 31.96829128265381 and batch: 300, loss is 4.74020544052124 and perplexity is 114.45771351725891
At time: 32.2930212020874 and batch: 350, loss is 4.807266750335693 and perplexity is 122.39661939020004
At time: 32.61771893501282 and batch: 400, loss is 4.739941921234131 and perplexity is 114.42755567594959
At time: 32.94321084022522 and batch: 450, loss is 4.798778142929077 and perplexity is 121.3620398225273
At time: 33.27015256881714 and batch: 500, loss is 4.76043251991272 and perplexity is 116.7964317582777
At time: 33.597635984420776 and batch: 550, loss is 4.774255323410034 and perplexity is 118.42209561514663
At time: 33.925137519836426 and batch: 600, loss is 4.813873882293701 and perplexity is 123.20798746205108
At time: 34.252243995666504 and batch: 650, loss is 4.803910903930664 and perplexity is 121.9865635620473
At time: 34.5794198513031 and batch: 700, loss is 4.770469875335693 and perplexity is 117.97466232338273
At time: 34.905253171920776 and batch: 750, loss is 4.709167709350586 and perplexity is 110.95977068408831
At time: 35.231825828552246 and batch: 800, loss is 4.750905284881592 and perplexity is 115.68896860982898
At time: 35.55791449546814 and batch: 850, loss is 4.739232082366943 and perplexity is 114.34635937100944
At time: 35.88481163978577 and batch: 900, loss is 4.84160044670105 and perplexity is 126.67192124449173
At time: 36.21271085739136 and batch: 950, loss is 4.780839538574218 and perplexity is 119.20438472664517
At time: 36.53824067115784 and batch: 1000, loss is 4.766131963729858 and perplexity is 117.46400705597128
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.103994602110328 and perplexity of 164.6784199373364
Finished 5 epochs...
Completing Train Step...
At time: 37.66475248336792 and batch: 50, loss is 4.861338806152344 and perplexity is 129.19705618619426
At time: 37.9896457195282 and batch: 100, loss is 4.761317892074585 and perplexity is 116.89988585849385
At time: 38.31372141838074 and batch: 150, loss is 4.7838981819152835 and perplexity is 119.56954658948202
At time: 38.63766932487488 and batch: 200, loss is 4.787946872711181 and perplexity is 120.05462802197714
At time: 38.96523642539978 and batch: 250, loss is 4.782990388870239 and perplexity is 119.46105143971096
At time: 39.29091477394104 and batch: 300, loss is 4.697654857635498 and perplexity is 109.68963278177694
At time: 39.61553192138672 and batch: 350, loss is 4.7601370525360105 and perplexity is 116.76192732069593
At time: 39.94191288948059 and batch: 400, loss is 4.705267744064331 and perplexity is 110.52787416837776
At time: 40.26706838607788 and batch: 450, loss is 4.757488069534301 and perplexity is 116.45303626418253
At time: 40.59200072288513 and batch: 500, loss is 4.72086350440979 and perplexity is 112.26515227922616
At time: 40.91741228103638 and batch: 550, loss is 4.7415756988525395 and perplexity is 114.61465765523707
At time: 41.241286754608154 and batch: 600, loss is 4.785395002365112 and perplexity is 119.74865474492822
At time: 41.56691765785217 and batch: 650, loss is 4.766334600448609 and perplexity is 117.48781198872766
At time: 41.89324498176575 and batch: 700, loss is 4.7362080097198485 and perplexity is 114.00108999630113
At time: 42.21823239326477 and batch: 750, loss is 4.681424865722656 and perplexity is 107.92373990539559
At time: 42.542322397232056 and batch: 800, loss is 4.709505434036255 and perplexity is 110.99725086639963
At time: 42.86691951751709 and batch: 850, loss is 4.6935057449340825 and perplexity is 109.23546098971627
At time: 43.19340252876282 and batch: 900, loss is 4.804374380111694 and perplexity is 122.0431145326657
At time: 43.51790237426758 and batch: 950, loss is 4.742903699874878 and perplexity is 114.76696714897567
At time: 43.844035148620605 and batch: 1000, loss is 4.730574922561646 and perplexity is 113.36071725130553
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.090240478515625 and perplexity of 162.42891802234084
Finished 6 epochs...
Completing Train Step...
At time: 44.968050956726074 and batch: 50, loss is 4.819516763687134 and perplexity is 123.90520081809896
At time: 45.3090443611145 and batch: 100, loss is 4.717171630859375 and perplexity is 111.85144767533254
At time: 45.63331151008606 and batch: 150, loss is 4.7437906742095945 and perplexity is 114.86880766159382
At time: 45.97667050361633 and batch: 200, loss is 4.744679098129272 and perplexity is 114.97090520417929
At time: 46.302510023117065 and batch: 250, loss is 4.747771120071411 and perplexity is 115.32694792807031
At time: 46.62807655334473 and batch: 300, loss is 4.663633222579956 and perplexity is 106.02057961507617
At time: 46.95321774482727 and batch: 350, loss is 4.724159536361694 and perplexity is 112.6357922933411
At time: 47.279431104660034 and batch: 400, loss is 4.6600549697875975 and perplexity is 105.64188910926029
At time: 47.605449199676514 and batch: 450, loss is 4.715883960723877 and perplexity is 111.70751259688797
At time: 47.929529905319214 and batch: 500, loss is 4.68278639793396 and perplexity is 108.07078163194957
At time: 48.255859375 and batch: 550, loss is 4.706675682067871 and perplexity is 110.6836001634187
At time: 48.58031749725342 and batch: 600, loss is 4.74888168334961 and perplexity is 115.45509694705224
At time: 48.90772986412048 and batch: 650, loss is 4.729159212112426 and perplexity is 113.20034484659179
At time: 49.233659505844116 and batch: 700, loss is 4.701363039016724 and perplexity is 110.09713691844563
At time: 49.557926416397095 and batch: 750, loss is 4.636750059127808 and perplexity is 103.20838080777331
At time: 49.88312029838562 and batch: 800, loss is 4.669164066314697 and perplexity is 106.60858746441795
At time: 50.20771670341492 and batch: 850, loss is 4.667440576553345 and perplexity is 106.42500690050448
At time: 50.53345322608948 and batch: 900, loss is 4.773634529113769 and perplexity is 118.34860266799848
At time: 50.85836172103882 and batch: 950, loss is 4.71663046836853 and perplexity is 111.79093424258076
At time: 51.18215346336365 and batch: 1000, loss is 4.689234352111816 and perplexity is 108.76986849780833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.082865273080221 and perplexity of 161.23537809443582
Finished 7 epochs...
Completing Train Step...
At time: 52.28912138938904 and batch: 50, loss is 4.77930814743042 and perplexity is 119.02197589285733
At time: 52.629692792892456 and batch: 100, loss is 4.6814976978302 and perplexity is 107.93160050507548
At time: 52.953935623168945 and batch: 150, loss is 4.715970878601074 and perplexity is 111.7172223987214
At time: 53.278873682022095 and batch: 200, loss is 4.7168264484405515 and perplexity is 111.81284518490887
At time: 53.60315418243408 and batch: 250, loss is 4.715168151855469 and perplexity is 111.62757998035418
At time: 53.92756199836731 and batch: 300, loss is 4.625724287033081 and perplexity is 102.07667912994674
At time: 54.25351119041443 and batch: 350, loss is 4.687643899917602 and perplexity is 108.59701271765616
At time: 54.596872329711914 and batch: 400, loss is 4.627440032958984 and perplexity is 102.25196710811004
At time: 54.92287611961365 and batch: 450, loss is 4.697757730484009 and perplexity is 109.70091744718619
At time: 55.2491180896759 and batch: 500, loss is 4.649366359710694 and perplexity is 104.51873730759891
At time: 55.57404065132141 and batch: 550, loss is 4.669578428268433 and perplexity is 106.65277116039381
At time: 55.90148687362671 and batch: 600, loss is 4.7228434371948245 and perplexity is 112.48764992734971
At time: 56.226237058639526 and batch: 650, loss is 4.709810762405396 and perplexity is 111.03114665039467
At time: 56.5537850856781 and batch: 700, loss is 4.678108444213867 and perplexity is 107.56641214530927
At time: 56.87831735610962 and batch: 750, loss is 4.619682331085205 and perplexity is 101.4617957510196
At time: 57.20281267166138 and batch: 800, loss is 4.645894165039063 and perplexity is 104.15645722229951
At time: 57.52830934524536 and batch: 850, loss is 4.622008056640625 and perplexity is 101.6980426585434
At time: 57.85297346115112 and batch: 900, loss is 4.749615106582642 and perplexity is 115.53980545732279
At time: 58.17743444442749 and batch: 950, loss is 4.700460577011109 and perplexity is 109.99782325559381
At time: 58.503061056137085 and batch: 1000, loss is 4.670276823043824 and perplexity is 106.72728291483457
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0966432152724845 and perplexity of 163.47224413309507
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 59.61962151527405 and batch: 50, loss is 4.710846223831177 and perplexity is 111.14617466306883
At time: 59.94472074508667 and batch: 100, loss is 4.527735977172852 and perplexity is 92.54879113398714
At time: 60.270955085754395 and batch: 150, loss is 4.506560869216919 and perplexity is 90.6096635641497
At time: 60.59580159187317 and batch: 200, loss is 4.51010573387146 and perplexity is 90.93143253465588
At time: 60.920854568481445 and batch: 250, loss is 4.4747037601470945 and perplexity is 87.76859600220463
At time: 61.24486231803894 and batch: 300, loss is 4.3637189769744875 and perplexity is 78.54871275458049
At time: 61.57190942764282 and batch: 350, loss is 4.424949502944946 and perplexity is 83.50859017769226
At time: 61.8988242149353 and batch: 400, loss is 4.347981443405152 and perplexity is 77.32222600254957
At time: 62.22292709350586 and batch: 450, loss is 4.392599744796753 and perplexity is 80.85033623754772
At time: 62.54905152320862 and batch: 500, loss is 4.327431583404541 and perplexity is 75.7494802821739
At time: 62.88935136795044 and batch: 550, loss is 4.359007558822632 and perplexity is 78.17950734698485
At time: 63.215651512145996 and batch: 600, loss is 4.396890888214111 and perplexity is 81.19802207706425
At time: 63.541587829589844 and batch: 650, loss is 4.340839672088623 and perplexity is 76.77197556693035
At time: 63.868080139160156 and batch: 700, loss is 4.307823724746704 and perplexity is 74.27866210016515
At time: 64.1926896572113 and batch: 750, loss is 4.256493577957153 and perplexity is 70.56212856331327
At time: 64.51637625694275 and batch: 800, loss is 4.263219490051269 and perplexity is 71.03832286253441
At time: 64.84493589401245 and batch: 850, loss is 4.220424575805664 and perplexity is 68.06237579370682
At time: 65.16962456703186 and batch: 900, loss is 4.339001760482788 and perplexity is 76.63100504739226
At time: 65.49518132209778 and batch: 950, loss is 4.25191517829895 and perplexity is 70.2398053632692
At time: 65.82218599319458 and batch: 1000, loss is 4.2237762928009035 and perplexity is 68.29088434947374
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.772947357922066 and perplexity of 118.26730485362374
Finished 9 epochs...
Completing Train Step...
At time: 66.9333918094635 and batch: 50, loss is 4.474046783447266 and perplexity is 87.71095301677697
At time: 67.2885594367981 and batch: 100, loss is 4.360140085220337 and perplexity is 78.26809785889257
At time: 67.61499571800232 and batch: 150, loss is 4.39024130821228 and perplexity is 80.65988052383939
At time: 67.94068431854248 and batch: 200, loss is 4.40840934753418 and perplexity is 82.13870540213584
At time: 68.26543164253235 and batch: 250, loss is 4.382805337905884 and perplexity is 80.06232052000621
At time: 68.59096908569336 and batch: 300, loss is 4.288201084136963 and perplexity is 72.83532595454722
At time: 68.91633892059326 and batch: 350, loss is 4.353167848587036 and perplexity is 77.72429213383985
At time: 69.24120664596558 and batch: 400, loss is 4.280074462890625 and perplexity is 72.24581943958991
At time: 69.56646275520325 and batch: 450, loss is 4.332783641815186 and perplexity is 76.15598276740369
At time: 69.89116430282593 and batch: 500, loss is 4.272473878860474 and perplexity is 71.6987905211775
At time: 70.2149829864502 and batch: 550, loss is 4.304049119949341 and perplexity is 73.998817988583
At time: 70.55279898643494 and batch: 600, loss is 4.347481632232666 and perplexity is 77.28358914648256
At time: 70.88564157485962 and batch: 650, loss is 4.300519418716431 and perplexity is 73.73808469550954
At time: 71.21080541610718 and batch: 700, loss is 4.2716548633575435 and perplexity is 71.64009214092218
At time: 71.55072617530823 and batch: 750, loss is 4.2269004583358765 and perplexity is 68.50456999757482
At time: 71.87451696395874 and batch: 800, loss is 4.238656301498413 and perplexity is 69.31465124301448
At time: 72.19939756393433 and batch: 850, loss is 4.203054571151734 and perplexity is 66.89034060617978
At time: 72.52494955062866 and batch: 900, loss is 4.324642639160157 and perplexity is 75.53851352912693
At time: 72.85247254371643 and batch: 950, loss is 4.255369577407837 and perplexity is 70.48286124864518
At time: 73.17731094360352 and batch: 1000, loss is 4.229123802185058 and perplexity is 68.65704865535095
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.759291858207889 and perplexity of 116.66328249487985
Finished 10 epochs...
Completing Train Step...
At time: 74.28994727134705 and batch: 50, loss is 4.433786430358887 and perplexity is 84.24981980009323
At time: 74.62959837913513 and batch: 100, loss is 4.323769903182983 and perplexity is 75.47261710997832
At time: 74.9552674293518 and batch: 150, loss is 4.35640778541565 and perplexity is 77.97652231468598
At time: 75.28327918052673 and batch: 200, loss is 4.3750848293304445 and perplexity is 79.4465786664944
At time: 75.61023116111755 and batch: 250, loss is 4.349671392440796 and perplexity is 77.4530070993856
At time: 75.93507981300354 and batch: 300, loss is 4.260058670043946 and perplexity is 70.81413800099014
At time: 76.26058435440063 and batch: 350, loss is 4.325895853042603 and perplexity is 75.63323878604695
At time: 76.58619737625122 and batch: 400, loss is 4.254964656829834 and perplexity is 70.454327065158
At time: 76.9119176864624 and batch: 450, loss is 4.308625059127808 and perplexity is 74.3382080007907
At time: 77.23928189277649 and batch: 500, loss is 4.24808563709259 and perplexity is 69.97133352439221
At time: 77.56650447845459 and batch: 550, loss is 4.280135049819946 and perplexity is 72.25019672454779
At time: 77.89382195472717 and batch: 600, loss is 4.324894571304322 and perplexity is 75.55754650621604
At time: 78.21912002563477 and batch: 650, loss is 4.282015099525451 and perplexity is 72.38615845297886
At time: 78.5447587966919 and batch: 700, loss is 4.25504497051239 and perplexity is 70.45998573884803
At time: 78.87155270576477 and batch: 750, loss is 4.214949407577515 and perplexity is 67.69074114572564
At time: 79.19680547714233 and batch: 800, loss is 4.22486569404602 and perplexity is 68.36532106227622
At time: 79.52209854125977 and batch: 850, loss is 4.191425423622132 and perplexity is 66.11696851175105
At time: 79.86489009857178 and batch: 900, loss is 4.317222661972046 and perplexity is 74.98009377486638
At time: 80.18999719619751 and batch: 950, loss is 4.254134473800659 and perplexity is 70.3958613504765
At time: 80.5143232345581 and batch: 1000, loss is 4.226802263259888 and perplexity is 68.49784351637742
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.752444011409108 and perplexity of 115.86711932223315
Finished 11 epochs...
Completing Train Step...
At time: 81.63607263565063 and batch: 50, loss is 4.409408502578735 and perplexity is 82.22081571762861
At time: 81.96201395988464 and batch: 100, loss is 4.301279458999634 and perplexity is 73.79414991349755
At time: 82.28955125808716 and batch: 150, loss is 4.33324815750122 and perplexity is 76.19136663352867
At time: 82.61678218841553 and batch: 200, loss is 4.352096824645996 and perplexity is 77.64109211872135
At time: 82.94187116622925 and batch: 250, loss is 4.3298888015747075 and perplexity is 75.93584215362242
At time: 83.26706838607788 and batch: 300, loss is 4.2390045642852785 and perplexity is 69.33879516059699
At time: 83.59334897994995 and batch: 350, loss is 4.306287508010865 and perplexity is 74.1646415788713
At time: 83.9198477268219 and batch: 400, loss is 4.234461340904236 and perplexity is 69.02448804954898
At time: 84.2476441860199 and batch: 450, loss is 4.290311479568482 and perplexity is 72.98919960370817
At time: 84.57382488250732 and batch: 500, loss is 4.230045809745788 and perplexity is 68.7203801648874
At time: 84.90018916130066 and batch: 550, loss is 4.2634904050827025 and perplexity is 71.05757081916835
At time: 85.22585821151733 and batch: 600, loss is 4.308567934036255 and perplexity is 74.3339615451435
At time: 85.5527138710022 and batch: 650, loss is 4.267122163772583 and perplexity is 71.31610395028156
At time: 85.87888312339783 and batch: 700, loss is 4.244125938415527 and perplexity is 69.69481595203415
At time: 86.20442819595337 and batch: 750, loss is 4.204882044792175 and perplexity is 67.01269270400331
At time: 86.5299620628357 and batch: 800, loss is 4.215618901252746 and perplexity is 67.73607484241275
At time: 86.85736608505249 and batch: 850, loss is 4.183772172927856 and perplexity is 65.61289015601251
At time: 87.18426895141602 and batch: 900, loss is 4.310176935195923 and perplexity is 74.45366124812875
At time: 87.51124143600464 and batch: 950, loss is 4.2477020835876464 and perplexity is 69.94450092037198
At time: 87.83727693557739 and batch: 1000, loss is 4.2203568172454835 and perplexity is 68.0577641413618
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.748094140029535 and perplexity of 115.36420685132454
Finished 12 epochs...
Completing Train Step...
At time: 88.94932699203491 and batch: 50, loss is 4.3897637939453125 and perplexity is 80.62137347468172
At time: 89.2895758152008 and batch: 100, loss is 4.283188395500183 and perplexity is 72.47113868505132
At time: 89.61491179466248 and batch: 150, loss is 4.314732847213745 and perplexity is 74.79363944549934
At time: 89.93977785110474 and batch: 200, loss is 4.3351599311828615 and perplexity is 76.3371666068961
At time: 90.26483726501465 and batch: 250, loss is 4.3135302734375 and perplexity is 74.70374863706998
At time: 90.58877730369568 and batch: 300, loss is 4.223205466270446 and perplexity is 68.25191322483283
At time: 90.91472125053406 and batch: 350, loss is 4.291188735961914 and perplexity is 73.0532579393961
At time: 91.2406063079834 and batch: 400, loss is 4.21859920501709 and perplexity is 67.93825004335679
At time: 91.5669457912445 and batch: 450, loss is 4.275848608016968 and perplexity is 71.94116326118314
At time: 91.89086771011353 and batch: 500, loss is 4.216118264198303 and perplexity is 67.76990817511978
At time: 92.21624159812927 and batch: 550, loss is 4.2507143259048465 and perplexity is 70.155508349103
At time: 92.54282736778259 and batch: 600, loss is 4.295823554992676 and perplexity is 73.3926324317067
At time: 92.86741876602173 and batch: 650, loss is 4.256386413574218 and perplexity is 70.55456722150772
At time: 93.19403886795044 and batch: 700, loss is 4.235422983169555 and perplexity is 69.09089684023175
At time: 93.51933526992798 and batch: 750, loss is 4.196074271202088 and perplexity is 66.42505178204146
At time: 93.8461434841156 and batch: 800, loss is 4.207355909347534 and perplexity is 67.17867825747894
At time: 94.17187476158142 and batch: 850, loss is 4.177958955764771 and perplexity is 65.23257467609069
At time: 94.4965546131134 and batch: 900, loss is 4.303495435714722 and perplexity is 73.9578573503585
At time: 94.81974649429321 and batch: 950, loss is 4.242822065353393 and perplexity is 69.6040019767579
At time: 95.14519000053406 and batch: 1000, loss is 4.214524989128113 and perplexity is 67.66201804206841
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.746337518459413 and perplexity of 115.16173348375658
Finished 13 epochs...
Completing Train Step...
At time: 96.24960780143738 and batch: 50, loss is 4.375854959487915 and perplexity is 79.50778643858372
At time: 96.58996915817261 and batch: 100, loss is 4.269450120925903 and perplexity is 71.48231817935643
At time: 96.91488695144653 and batch: 150, loss is 4.300241270065308 and perplexity is 73.71757739888574
At time: 97.25544452667236 and batch: 200, loss is 4.321686067581177 and perplexity is 75.31550833476467
At time: 97.57975196838379 and batch: 250, loss is 4.30152398109436 and perplexity is 73.81219641990778
At time: 97.90443849563599 and batch: 300, loss is 4.21018805027008 and perplexity is 67.36920741665563
At time: 98.22990679740906 and batch: 350, loss is 4.2783635711669925 and perplexity is 72.12232034196069
At time: 98.55558395385742 and batch: 400, loss is 4.2066635370254515 and perplexity is 67.13218169834214
At time: 98.88224458694458 and batch: 450, loss is 4.264661474227905 and perplexity is 71.140832891194
At time: 99.21141123771667 and batch: 500, loss is 4.204932465553283 and perplexity is 67.01607162015642
At time: 99.5380425453186 and batch: 550, loss is 4.240094175338745 and perplexity is 69.41438865451228
At time: 99.86508417129517 and batch: 600, loss is 4.285511360168457 and perplexity is 72.63968226428538
At time: 100.19070529937744 and batch: 650, loss is 4.247329807281494 and perplexity is 69.91846708612414
At time: 100.51533079147339 and batch: 700, loss is 4.228330841064453 and perplexity is 68.60262786474891
At time: 100.84142780303955 and batch: 750, loss is 4.189360737800598 and perplexity is 65.98059857328586
At time: 101.16771149635315 and batch: 800, loss is 4.19973289012909 and perplexity is 66.66852084239511
At time: 101.49224662780762 and batch: 850, loss is 4.171680727005005 and perplexity is 64.82431257213634
At time: 101.8168728351593 and batch: 900, loss is 4.296905269622803 and perplexity is 73.47206527003785
At time: 102.14428758621216 and batch: 950, loss is 4.236780910491944 and perplexity is 69.18478098627634
At time: 102.47130346298218 and batch: 1000, loss is 4.207587404251099 and perplexity is 67.19423157931166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.74668735411109 and perplexity of 115.20202821169345
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 103.5898208618164 and batch: 50, loss is 4.363455533981323 and perplexity is 78.52802237207104
At time: 103.91437005996704 and batch: 100, loss is 4.252313003540039 and perplexity is 70.26775408975772
At time: 104.24000120162964 and batch: 150, loss is 4.2796681785583495 and perplexity is 72.21647305697059
At time: 104.5652539730072 and batch: 200, loss is 4.295722713470459 and perplexity is 73.38523178008553
At time: 104.88989806175232 and batch: 250, loss is 4.266402416229248 and perplexity is 71.26479282740553
At time: 105.21571969985962 and batch: 300, loss is 4.173125195503235 and perplexity is 64.91801690975535
At time: 105.55553388595581 and batch: 350, loss is 4.239891901016235 and perplexity is 69.40034932602042
At time: 105.88013696670532 and batch: 400, loss is 4.160990200042725 and perplexity is 64.1349976456626
At time: 106.20464730262756 and batch: 450, loss is 4.219441385269165 and perplexity is 67.99549039586786
At time: 106.53064846992493 and batch: 500, loss is 4.154152975082398 and perplexity is 63.69798790830128
At time: 106.85721445083618 and batch: 550, loss is 4.187684941291809 and perplexity is 65.87012111127592
At time: 107.18209958076477 and batch: 600, loss is 4.229586629867554 and perplexity is 68.68883239269648
At time: 107.50785684585571 and batch: 650, loss is 4.186095113754273 and perplexity is 65.76548217979978
At time: 107.83389067649841 and batch: 700, loss is 4.164069757461548 and perplexity is 64.33280948359315
At time: 108.16097092628479 and batch: 750, loss is 4.118813939094544 and perplexity is 61.48627253563459
At time: 108.48638606071472 and batch: 800, loss is 4.123976373672486 and perplexity is 61.80451213381801
At time: 108.8105137348175 and batch: 850, loss is 4.085215487480164 and perplexity is 59.454748124813925
At time: 109.13825106620789 and batch: 900, loss is 4.207367563247681 and perplexity is 67.17946115564926
At time: 109.46389126777649 and batch: 950, loss is 4.140482831001282 and perplexity is 62.83315192002186
At time: 109.79026103019714 and batch: 1000, loss is 4.114872393608093 and perplexity is 61.24439858736925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.704947867044589 and perplexity of 110.49252449546574
Finished 15 epochs...
Completing Train Step...
At time: 110.91015839576721 and batch: 50, loss is 4.330986275672912 and perplexity is 76.01922552068493
At time: 111.25001573562622 and batch: 100, loss is 4.223135161399841 and perplexity is 68.24711495157796
At time: 111.57533049583435 and batch: 150, loss is 4.256286420822144 and perplexity is 70.54751262886974
At time: 111.90050005912781 and batch: 200, loss is 4.273627934455871 and perplexity is 71.78158267574744
At time: 112.22727870941162 and batch: 250, loss is 4.248232474327088 and perplexity is 69.98160867587
At time: 112.55265378952026 and batch: 300, loss is 4.157736392021179 and perplexity is 63.926653815090845
At time: 112.87862777709961 and batch: 350, loss is 4.224116067886353 and perplexity is 68.3140918329781
At time: 113.20371747016907 and batch: 400, loss is 4.1476424169540405 and perplexity is 63.284625525386794
At time: 113.52977514266968 and batch: 450, loss is 4.207272763252258 and perplexity is 67.17309284490189
At time: 113.85572242736816 and batch: 500, loss is 4.142130374908447 and perplexity is 62.93675762067052
At time: 114.19629836082458 and batch: 550, loss is 4.177449960708618 and perplexity is 65.19938006674312
At time: 114.52102375030518 and batch: 600, loss is 4.2212208652496335 and perplexity is 68.11659472920093
At time: 114.84640192985535 and batch: 650, loss is 4.178950214385987 and perplexity is 65.29726908727154
At time: 115.17408847808838 and batch: 700, loss is 4.159124121665955 and perplexity is 64.01542831093438
At time: 115.49942207336426 and batch: 750, loss is 4.116025733947754 and perplexity is 61.315074971966595
At time: 115.82472681999207 and batch: 800, loss is 4.122507357597351 and perplexity is 61.71378696665756
At time: 116.14979076385498 and batch: 850, loss is 4.086171355247497 and perplexity is 59.51160617221961
At time: 116.47595429420471 and batch: 900, loss is 4.211755485534668 and perplexity is 67.47488708949416
At time: 116.80249953269958 and batch: 950, loss is 4.1475347232818605 and perplexity is 63.27781053864349
At time: 117.12801027297974 and batch: 1000, loss is 4.121475872993469 and perplexity is 61.650162964784464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.7020349269959985 and perplexity of 110.1711347176258
Finished 16 epochs...
Completing Train Step...
At time: 118.2361011505127 and batch: 50, loss is 4.322603230476379 and perplexity is 75.38461661137174
At time: 118.57551908493042 and batch: 100, loss is 4.21515043258667 and perplexity is 67.70435004540295
At time: 118.90080952644348 and batch: 150, loss is 4.248240537643433 and perplexity is 69.98217296199411
At time: 119.22636818885803 and batch: 200, loss is 4.265829935073852 and perplexity is 71.2240067522123
At time: 119.5531096458435 and batch: 250, loss is 4.241094274520874 and perplexity is 69.48384465348835
At time: 119.87685060501099 and batch: 300, loss is 4.151204977035523 and perplexity is 63.51048288237852
At time: 120.20067977905273 and batch: 350, loss is 4.217446985244751 and perplexity is 67.8600153288067
At time: 120.52685928344727 and batch: 400, loss is 4.141912121772766 and perplexity is 62.923022974839455
At time: 120.85402727127075 and batch: 450, loss is 4.202204289436341 and perplexity is 66.8334891459297
At time: 121.18003940582275 and batch: 500, loss is 4.136839923858642 and perplexity is 62.60467299851967
At time: 121.50631928443909 and batch: 550, loss is 4.172988696098328 and perplexity is 64.90915624383099
At time: 121.83066701889038 and batch: 600, loss is 4.2175710582733155 and perplexity is 67.86843544877104
At time: 122.15754723548889 and batch: 650, loss is 4.175967812538147 and perplexity is 65.1028165032931
At time: 122.49889755249023 and batch: 700, loss is 4.1573839855194095 and perplexity is 63.904129615720294
At time: 122.82483649253845 and batch: 750, loss is 4.115353355407715 and perplexity is 61.273861888323744
At time: 123.15049862861633 and batch: 800, loss is 4.1223029613494875 and perplexity is 61.7011741892062
At time: 123.47666215896606 and batch: 850, loss is 4.087323446273803 and perplexity is 59.5802084701095
At time: 123.8033390045166 and batch: 900, loss is 4.21462760925293 and perplexity is 67.6689618830881
At time: 124.12792992591858 and batch: 950, loss is 4.151225056648254 and perplexity is 63.5117581610827
At time: 124.4533576965332 and batch: 1000, loss is 4.124286379814148 and perplexity is 61.82367488229331
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.700687687571456 and perplexity of 110.02280775988537
Finished 17 epochs...
Completing Train Step...
At time: 125.57403683662415 and batch: 50, loss is 4.316932554244995 and perplexity is 74.958344625238
At time: 125.89944553375244 and batch: 100, loss is 4.209682164192199 and perplexity is 67.33513489168034
At time: 126.22447323799133 and batch: 150, loss is 4.242825899124146 and perplexity is 69.60426882305644
At time: 126.55118441581726 and batch: 200, loss is 4.260581188201904 and perplexity is 70.85114934264278
At time: 126.87661290168762 and batch: 250, loss is 4.236220397949219 and perplexity is 69.14601291477803
At time: 127.20159149169922 and batch: 300, loss is 4.146715211868286 and perplexity is 63.22597489353353
At time: 127.52779269218445 and batch: 350, loss is 4.212909793853759 and perplexity is 67.55281888299692
At time: 127.8532063961029 and batch: 400, loss is 4.137831225395202 and perplexity is 62.66676387736614
At time: 128.1774082183838 and batch: 450, loss is 4.198730220794678 and perplexity is 66.60170786223655
At time: 128.50315189361572 and batch: 500, loss is 4.133067460060119 and perplexity is 62.36894405503149
At time: 128.82887649536133 and batch: 550, loss is 4.16991400718689 and perplexity is 64.70988728289551
At time: 129.15454244613647 and batch: 600, loss is 4.215106630325318 and perplexity is 67.7013845067167
At time: 129.48091387748718 and batch: 650, loss is 4.17394464969635 and perplexity is 64.97123605326784
At time: 129.8055899143219 and batch: 700, loss is 4.156297144889831 and perplexity is 63.834713740086045
At time: 130.13117671012878 and batch: 750, loss is 4.115040698051453 and perplexity is 61.254707159247296
At time: 130.45655274391174 and batch: 800, loss is 4.122084550857544 and perplexity is 61.68769947696085
At time: 130.78214168548584 and batch: 850, loss is 4.08802444934845 and perplexity is 59.621989021871826
At time: 131.12224435806274 and batch: 900, loss is 4.216319770812988 and perplexity is 67.78356563588179
At time: 131.44821071624756 and batch: 950, loss is 4.15328911781311 and perplexity is 63.64298569885739
At time: 131.77545022964478 and batch: 1000, loss is 4.125555500984192 and perplexity is 61.90218642668991
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.699825007741044 and perplexity of 109.92793423136922
Finished 18 epochs...
Completing Train Step...
At time: 132.8871295452118 and batch: 50, loss is 4.312479224205017 and perplexity is 74.625272567732
At time: 133.22933888435364 and batch: 100, loss is 4.205336494445801 and perplexity is 67.04315351992057
At time: 133.55788445472717 and batch: 150, loss is 4.238447508811951 and perplexity is 69.30018036152995
At time: 133.88561940193176 and batch: 200, loss is 4.256359977722168 and perplexity is 70.55270207606063
At time: 134.21193289756775 and batch: 250, loss is 4.23230767250061 and perplexity is 68.87599215341723
At time: 134.537780046463 and batch: 300, loss is 4.143177618980408 and perplexity is 63.002702291052856
At time: 134.86578750610352 and batch: 350, loss is 4.209301891326905 and perplexity is 67.30953403496416
At time: 135.19155764579773 and batch: 400, loss is 4.134661273956299 and perplexity is 62.468427802987
At time: 135.51914286613464 and batch: 450, loss is 4.196007170677185 and perplexity is 66.42059477573561
At time: 135.84551692008972 and batch: 500, loss is 4.130285301208496 and perplexity is 62.19566490209788
At time: 136.17385292053223 and batch: 550, loss is 4.167706890106201 and perplexity is 64.5672224822575
At time: 136.5013678073883 and batch: 600, loss is 4.213168020248413 and perplexity is 67.57026505630004
At time: 136.82697319984436 and batch: 650, loss is 4.172201614379883 and perplexity is 64.85808753385224
At time: 137.15386366844177 and batch: 700, loss is 4.1554486322402955 and perplexity is 63.78057215115801
At time: 137.48072600364685 and batch: 750, loss is 4.114852857589722 and perplexity is 61.2432021273604
At time: 137.80775785446167 and batch: 800, loss is 4.121879858970642 and perplexity is 61.675073797587444
At time: 138.13444256782532 and batch: 850, loss is 4.088511943817139 and perplexity is 59.65106149749198
At time: 138.4601595401764 and batch: 900, loss is 4.217325611114502 and perplexity is 67.85177937829322
At time: 138.7857449054718 and batch: 950, loss is 4.1544840097427365 and perplexity is 63.719077640616355
At time: 139.11470699310303 and batch: 1000, loss is 4.126053805351257 and perplexity is 61.933040243174524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.699228054139672 and perplexity of 109.86233193784925
Finished 19 epochs...
Completing Train Step...
At time: 140.2355318069458 and batch: 50, loss is 4.308662796020508 and perplexity is 74.34101334670176
At time: 140.57529520988464 and batch: 100, loss is 4.201617369651794 and perplexity is 66.79427475785548
At time: 140.90036702156067 and batch: 150, loss is 4.234716019630432 and perplexity is 69.04206935693924
At time: 141.22538542747498 and batch: 200, loss is 4.252784757614136 and perplexity is 70.3009110093682
At time: 141.55107426643372 and batch: 250, loss is 4.228912243843078 and perplexity is 68.64252522030328
At time: 141.87611484527588 and batch: 300, loss is 4.139899849891663 and perplexity is 62.79653205477666
At time: 142.20243787765503 and batch: 350, loss is 4.206331396102906 and perplexity is 67.10988805610073
At time: 142.52947473526 and batch: 400, loss is 4.131855525970459 and perplexity is 62.293402790358144
At time: 142.85481929779053 and batch: 450, loss is 4.1935312461853025 and perplexity is 66.25634581628229
At time: 143.18034315109253 and batch: 500, loss is 4.127612495422364 and perplexity is 62.02964993079586
At time: 143.5081558227539 and batch: 550, loss is 4.165348334312439 and perplexity is 64.41511653128188
At time: 143.83445048332214 and batch: 600, loss is 4.211311359405517 and perplexity is 67.44492638273506
At time: 144.15914821624756 and batch: 650, loss is 4.170582208633423 and perplexity is 64.7531409726626
At time: 144.48372602462769 and batch: 700, loss is 4.154540858268738 and perplexity is 63.72270007922256
At time: 144.81023979187012 and batch: 750, loss is 4.114254341125489 and perplexity is 61.20655802970658
At time: 145.13624262809753 and batch: 800, loss is 4.121375470161438 and perplexity is 61.643973424556535
At time: 145.46131253242493 and batch: 850, loss is 4.08853178024292 and perplexity is 59.652244773082074
At time: 145.78661274909973 and batch: 900, loss is 4.217927951812744 and perplexity is 67.89266157773112
At time: 146.112651348114 and batch: 950, loss is 4.15514799118042 and perplexity is 63.76139997446318
At time: 146.4412591457367 and batch: 1000, loss is 4.126048607826233 and perplexity is 61.93271834548455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6987974585556405 and perplexity of 109.81503588633164
Finished 20 epochs...
Completing Train Step...
At time: 147.57770705223083 and batch: 50, loss is 4.305414276123047 and perplexity is 74.09990691719499
At time: 147.90369319915771 and batch: 100, loss is 4.198470630645752 and perplexity is 66.58442095882586
At time: 148.24446868896484 and batch: 150, loss is 4.231548523902893 and perplexity is 68.82372488238047
At time: 148.56973147392273 and batch: 200, loss is 4.249809188842773 and perplexity is 70.09203672797936
At time: 148.89518022537231 and batch: 250, loss is 4.226130166053772 and perplexity is 68.45182177439959
At time: 149.22099947929382 and batch: 300, loss is 4.137449488639832 and perplexity is 62.642846235665935
At time: 149.5457320213318 and batch: 350, loss is 4.20364538192749 and perplexity is 66.92987181678664
At time: 149.87098288536072 and batch: 400, loss is 4.129384131431579 and perplexity is 62.13964129581469
At time: 150.19610619544983 and batch: 450, loss is 4.191449885368347 and perplexity is 66.11858586803692
At time: 150.5213086605072 and batch: 500, loss is 4.125484747886658 and perplexity is 61.8978068101936
At time: 150.8468415737152 and batch: 550, loss is 4.1637868356704715 and perplexity is 64.31461090441776
At time: 151.1721272468567 and batch: 600, loss is 4.2098957586288455 and perplexity is 67.3495188379948
At time: 151.49660277366638 and batch: 650, loss is 4.169189958572388 and perplexity is 64.66305113653334
At time: 151.82394409179688 and batch: 700, loss is 4.153843340873718 and perplexity is 63.67826788537519
At time: 152.15007090568542 and batch: 750, loss is 4.114073262214661 and perplexity is 61.195475816250905
At time: 152.4756224155426 and batch: 800, loss is 4.121004600524902 and perplexity is 61.62111578540207
At time: 152.80028462409973 and batch: 850, loss is 4.088496127128601 and perplexity is 59.65011802269268
At time: 153.124657869339 and batch: 900, loss is 4.218150329589844 and perplexity is 67.90776107572836
At time: 153.45011639595032 and batch: 950, loss is 4.155482339859009 and perplexity is 63.78272207859965
At time: 153.77437901496887 and batch: 1000, loss is 4.125750985145569 and perplexity is 61.91428850653483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.69840259086795 and perplexity of 109.77168203711796
Finished 21 epochs...
Completing Train Step...
At time: 154.89748525619507 and batch: 50, loss is 4.302489242553711 and perplexity is 73.88347888591588
At time: 155.237961769104 and batch: 100, loss is 4.195589489936829 and perplexity is 66.39285796550374
At time: 155.56240153312683 and batch: 150, loss is 4.22858241558075 and perplexity is 68.61988870877386
At time: 155.88787031173706 and batch: 200, loss is 4.24699709892273 and perplexity is 69.89520849707068
At time: 156.21368050575256 and batch: 250, loss is 4.223445672988891 and perplexity is 68.26830976213651
At time: 156.55403566360474 and batch: 300, loss is 4.134979639053345 and perplexity is 62.48831873618825
At time: 156.87877798080444 and batch: 350, loss is 4.201140613555908 and perplexity is 66.76243777003641
At time: 157.20304155349731 and batch: 400, loss is 4.127009172439575 and perplexity is 61.99223730446177
At time: 157.52920174598694 and batch: 450, loss is 4.189380917549133 and perplexity is 65.98193005860776
At time: 157.85428309440613 and batch: 500, loss is 4.123003463745118 and perplexity is 61.74441115156874
At time: 158.17848563194275 and batch: 550, loss is 4.1618285608291625 and perplexity is 64.18878845763015
At time: 158.50264310836792 and batch: 600, loss is 4.208227453231811 and perplexity is 67.23725294516575
At time: 158.82665038108826 and batch: 650, loss is 4.167613391876221 and perplexity is 64.56118584345258
At time: 159.1543266773224 and batch: 700, loss is 4.152893919944763 and perplexity is 63.61783909584316
At time: 159.48029112815857 and batch: 750, loss is 4.113218407630921 and perplexity is 61.14318493697855
At time: 159.80633640289307 and batch: 800, loss is 4.120256385803223 and perplexity is 61.57502720363295
At time: 160.13070511817932 and batch: 850, loss is 4.088072080612182 and perplexity is 59.62482896018956
At time: 160.45533847808838 and batch: 900, loss is 4.218119659423828 and perplexity is 67.90567836536111
At time: 160.78075408935547 and batch: 950, loss is 4.155533537864685 and perplexity is 63.78598771036291
At time: 161.1059651374817 and batch: 1000, loss is 4.125216665267945 and perplexity is 61.881215308097374
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.69805424387862 and perplexity of 109.73345006155009
Finished 22 epochs...
Completing Train Step...
At time: 162.2153947353363 and batch: 50, loss is 4.299732074737549 and perplexity is 73.68005030802105
At time: 162.5568723678589 and batch: 100, loss is 4.192899694442749 and perplexity is 66.21451471626925
At time: 162.8816273212433 and batch: 150, loss is 4.225829362869263 and perplexity is 68.43123434496178
At time: 163.20819211006165 and batch: 200, loss is 4.244372682571411 and perplexity is 69.71201486234379
At time: 163.53424835205078 and batch: 250, loss is 4.220966234207153 and perplexity is 68.09925233772412
At time: 163.8601038455963 and batch: 300, loss is 4.132760787010193 and perplexity is 62.349820113286114
At time: 164.1872181892395 and batch: 350, loss is 4.198824510574341 and perplexity is 66.6079880186687
At time: 164.51247096061707 and batch: 400, loss is 4.124819355010986 and perplexity is 61.856634150038886
At time: 164.8404290676117 and batch: 450, loss is 4.187614774703979 and perplexity is 65.86549939178465
At time: 165.18087911605835 and batch: 500, loss is 4.12128520488739 and perplexity is 61.63840936552638
At time: 165.5055787563324 and batch: 550, loss is 4.160382237434387 and perplexity is 64.09601781554845
At time: 165.8314573764801 and batch: 600, loss is 4.206956186294556 and perplexity is 67.15183075725066
At time: 166.15663743019104 and batch: 650, loss is 4.1662109375 and perplexity is 64.47070518815829
At time: 166.48338150978088 and batch: 700, loss is 4.152228827476502 and perplexity is 63.57554141770623
At time: 166.80852723121643 and batch: 750, loss is 4.112895708084107 and perplexity is 61.12345724213891
At time: 167.13441014289856 and batch: 800, loss is 4.11969494342804 and perplexity is 61.54046607705614
At time: 167.4606602191925 and batch: 850, loss is 4.0878038549423215 and perplexity is 59.60883819516408
At time: 167.78574228286743 and batch: 900, loss is 4.217969741821289 and perplexity is 67.89549887192346
At time: 168.11046600341797 and batch: 950, loss is 4.155383062362671 and perplexity is 63.776390203953625
At time: 168.4347529411316 and batch: 1000, loss is 4.12463041305542 and perplexity is 61.844947940660205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.697733437142721 and perplexity of 109.69825247772968
Finished 23 epochs...
Completing Train Step...
At time: 169.55337810516357 and batch: 50, loss is 4.2970868110656735 and perplexity is 73.4854047055712
At time: 169.87765526771545 and batch: 100, loss is 4.190300054550171 and perplexity is 66.04260437165604
At time: 170.20270657539368 and batch: 150, loss is 4.223205370903015 and perplexity is 68.2519067158235
At time: 170.52804279327393 and batch: 200, loss is 4.241990032196045 and perplexity is 69.54611322524907
At time: 170.85463166236877 and batch: 250, loss is 4.218396863937378 and perplexity is 67.92450473515643
At time: 171.17905282974243 and batch: 300, loss is 4.130324649810791 and perplexity is 62.19811226273037
At time: 171.50388479232788 and batch: 350, loss is 4.196826505661011 and perplexity is 66.4750377931706
At time: 171.83178067207336 and batch: 400, loss is 4.122621803283692 and perplexity is 61.720850247536916
At time: 172.15674376487732 and batch: 450, loss is 4.1855996799468995 and perplexity is 65.73290780645891
At time: 172.48255705833435 and batch: 500, loss is 4.1189526128768925 and perplexity is 61.49479966084032
At time: 172.80809807777405 and batch: 550, loss is 4.158357601165772 and perplexity is 63.966377974252005
At time: 173.13355660438538 and batch: 600, loss is 4.205302243232727 and perplexity is 67.04085724990945
At time: 173.47630047798157 and batch: 650, loss is 4.164559464454651 and perplexity is 64.36432142546263
At time: 173.80148005485535 and batch: 700, loss is 4.151255798339844 and perplexity is 63.513710649975664
At time: 174.1270136833191 and batch: 750, loss is 4.111757473945618 and perplexity is 61.05392401649616
At time: 174.45217418670654 and batch: 800, loss is 4.11867121219635 and perplexity is 61.47749741691173
At time: 174.77841305732727 and batch: 850, loss is 4.087299442291259 and perplexity is 59.57877832499008
At time: 175.1029155254364 and batch: 900, loss is 4.217495040893555 and perplexity is 67.86327646422849
At time: 175.4286403656006 and batch: 950, loss is 4.154897665977478 and perplexity is 63.74544088663898
At time: 175.7573161125183 and batch: 1000, loss is 4.124054660797119 and perplexity is 61.80935082076332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.697300608565167 and perplexity of 109.6507822131343
Finished 24 epochs...
Completing Train Step...
At time: 176.86633729934692 and batch: 50, loss is 4.294653267860412 and perplexity is 73.30679221701527
At time: 177.20684838294983 and batch: 100, loss is 4.187812628746033 and perplexity is 65.87853243634898
At time: 177.53447890281677 and batch: 150, loss is 4.2206996822357175 and perplexity is 68.08110276676963
At time: 177.8619954586029 and batch: 200, loss is 4.239647769927979 and perplexity is 69.38340861117581
At time: 178.18722128868103 and batch: 250, loss is 4.216216187477112 and perplexity is 67.77654475166526
At time: 178.51345896720886 and batch: 300, loss is 4.1286405658721925 and perplexity is 62.093453572602655
At time: 178.84073996543884 and batch: 350, loss is 4.194640707969666 and perplexity is 66.32989549267023
At time: 179.16631746292114 and batch: 400, loss is 4.120432085990906 and perplexity is 61.585846897952564
At time: 179.49263763427734 and batch: 450, loss is 4.183841242790222 and perplexity is 65.61742218581652
At time: 179.81797647476196 and batch: 500, loss is 4.117220788002014 and perplexity is 61.388393602036345
At time: 180.14602065086365 and batch: 550, loss is 4.157173986434937 and perplexity is 63.8907112159821
At time: 180.47312259674072 and batch: 600, loss is 4.204001502990723 and perplexity is 66.95371119850108
At time: 180.79906177520752 and batch: 650, loss is 4.163122973442078 and perplexity is 64.2719290325133
At time: 181.1252121925354 and batch: 700, loss is 4.150593228340149 and perplexity is 63.47164230887907
At time: 181.45127272605896 and batch: 750, loss is 4.111333608627319 and perplexity is 61.028050859313915
At time: 181.77920150756836 and batch: 800, loss is 4.118069186210632 and perplexity is 61.44049750450171
At time: 182.11958956718445 and batch: 850, loss is 4.0868434143066406 and perplexity is 59.55161492888962
At time: 182.44474983215332 and batch: 900, loss is 4.217294321060181 and perplexity is 67.8496563256461
At time: 182.76905512809753 and batch: 950, loss is 4.154559078216553 and perplexity is 63.72386111406962
At time: 183.0954189300537 and batch: 1000, loss is 4.123289408683777 and perplexity is 61.76206917791937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.697070238066883 and perplexity of 109.62552481718971
Finished 25 epochs...
Completing Train Step...
At time: 184.2038938999176 and batch: 50, loss is 4.292442650794983 and perplexity is 73.14491795804905
At time: 184.54569029808044 and batch: 100, loss is 4.185603728294373 and perplexity is 65.73317391664878
At time: 184.87239241600037 and batch: 150, loss is 4.218375654220581 and perplexity is 67.9230640909253
At time: 185.19957089424133 and batch: 200, loss is 4.237473335266113 and perplexity is 69.23270283184858
At time: 185.5256724357605 and batch: 250, loss is 4.214089374542237 and perplexity is 67.63254989894196
At time: 185.85260581970215 and batch: 300, loss is 4.1266672229766845 and perplexity is 61.97104271615771
At time: 186.18045806884766 and batch: 350, loss is 4.19262405872345 and perplexity is 66.19626614597179
At time: 186.50898146629333 and batch: 400, loss is 4.118369183540344 and perplexity is 61.458932254738905
At time: 186.83447933197021 and batch: 450, loss is 4.181945171356201 and perplexity is 65.49312474169082
At time: 187.16172242164612 and batch: 500, loss is 4.114971189498902 and perplexity is 61.25044958118647
At time: 187.4889895915985 and batch: 550, loss is 4.155349788665771 and perplexity is 63.774268162980874
At time: 187.8150405883789 and batch: 600, loss is 4.2024303150177005 and perplexity is 66.8485969314769
At time: 188.14150643348694 and batch: 650, loss is 4.161574358940125 and perplexity is 64.17247362006829
At time: 188.46835017204285 and batch: 700, loss is 4.149630150794983 and perplexity is 63.41054362152599
At time: 188.79453945159912 and batch: 750, loss is 4.110412888526916 and perplexity is 60.971886965779795
At time: 189.12502002716064 and batch: 800, loss is 4.117135543823242 and perplexity is 61.38316082187276
At time: 189.45664143562317 and batch: 850, loss is 4.0861081743240355 and perplexity is 59.50784629276253
At time: 189.78475046157837 and batch: 900, loss is 4.216889657974243 and perplexity is 67.82220562885396
At time: 190.11223578453064 and batch: 950, loss is 4.154146704673767 and perplexity is 63.69758849714039
At time: 190.45336294174194 and batch: 1000, loss is 4.122486290931701 and perplexity is 61.71248687663585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.696862197503811 and perplexity of 109.60272063345957
Finished 26 epochs...
Completing Train Step...
At time: 191.57533502578735 and batch: 50, loss is 4.290255947113037 and perplexity is 72.98514644677515
At time: 191.9008593559265 and batch: 100, loss is 4.183430004119873 and perplexity is 65.5904433121234
At time: 192.22658944129944 and batch: 150, loss is 4.216180438995361 and perplexity is 67.7741218863992
At time: 192.55118012428284 and batch: 200, loss is 4.235298619270325 and perplexity is 69.08230496117025
At time: 192.8788342475891 and batch: 250, loss is 4.212029104232788 and perplexity is 67.49335200631346
At time: 193.20348620414734 and batch: 300, loss is 4.124894585609436 and perplexity is 61.86128783669169
At time: 193.5292613506317 and batch: 350, loss is 4.190705327987671 and perplexity is 66.06937510931921
At time: 193.85400390625 and batch: 400, loss is 4.116477379798889 and perplexity is 61.34277392577767
At time: 194.17804884910583 and batch: 450, loss is 4.180348634719849 and perplexity is 65.38864599287902
At time: 194.50625324249268 and batch: 500, loss is 4.113300676345825 and perplexity is 61.14821531514702
At time: 194.83177757263184 and batch: 550, loss is 4.154381022453308 and perplexity is 63.712515723428126
At time: 195.15648579597473 and batch: 600, loss is 4.201168875694275 and perplexity is 66.76432464595386
At time: 195.48072481155396 and batch: 650, loss is 4.160182409286499 and perplexity is 64.0832109066544
At time: 195.80621242523193 and batch: 700, loss is 4.148717279434204 and perplexity is 63.352684365329466
At time: 196.13169884681702 and batch: 750, loss is 4.11011278629303 and perplexity is 60.95359191162771
At time: 196.45661115646362 and batch: 800, loss is 4.116295185089111 and perplexity is 61.33159861495747
At time: 196.78128862380981 and batch: 850, loss is 4.085565657615661 and perplexity is 59.47557104759085
At time: 197.10720586776733 and batch: 900, loss is 4.216302013397216 and perplexity is 67.7823619856112
At time: 197.43430042266846 and batch: 950, loss is 4.15357421875 and perplexity is 63.66113296048432
At time: 197.7585756778717 and batch: 1000, loss is 4.121732316017151 and perplexity is 61.665974746309516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.69664448063548 and perplexity of 109.57886086979344
Finished 27 epochs...
Completing Train Step...
At time: 198.86130213737488 and batch: 50, loss is 4.288097791671753 and perplexity is 72.82780300271376
At time: 199.20169854164124 and batch: 100, loss is 4.181217794418335 and perplexity is 65.44550387442482
At time: 199.5282325744629 and batch: 150, loss is 4.213981766700744 and perplexity is 67.62527249779244
At time: 199.85195589065552 and batch: 200, loss is 4.233116531372071 and perplexity is 68.93172564792825
At time: 200.17878222465515 and batch: 250, loss is 4.209867076873779 and perplexity is 67.3475871632937
At time: 200.50381755828857 and batch: 300, loss is 4.1229043674469 and perplexity is 61.73829281214538
At time: 200.82894706726074 and batch: 350, loss is 4.188797221183777 and perplexity is 65.94342788373626
At time: 201.1537435054779 and batch: 400, loss is 4.114504499435425 and perplexity is 61.22187127410811
At time: 201.48145389556885 and batch: 450, loss is 4.1784660482406615 and perplexity is 65.26566201235254
At time: 201.8071517944336 and batch: 500, loss is 4.111174473762512 and perplexity is 61.01833994138426
At time: 202.13269901275635 and batch: 550, loss is 4.152720947265625 and perplexity is 63.60683589942739
At time: 202.45717906951904 and batch: 600, loss is 4.199467811584473 and perplexity is 66.65085078999782
At time: 202.7839059829712 and batch: 650, loss is 4.15855119228363 and perplexity is 63.978762495597366
At time: 203.11299443244934 and batch: 700, loss is 4.147402048110962 and perplexity is 63.26941570122175
At time: 203.43925714492798 and batch: 750, loss is 4.108831853866577 and perplexity is 60.87556446385221
At time: 203.76529502868652 and batch: 800, loss is 4.115150084495545 and perplexity is 61.26140796032906
At time: 204.09765672683716 and batch: 850, loss is 4.084606809616089 and perplexity is 59.41857034713938
At time: 204.42691683769226 and batch: 900, loss is 4.215624628067016 and perplexity is 67.7364627554435
At time: 204.75308322906494 and batch: 950, loss is 4.152968153953553 and perplexity is 63.62256187836316
At time: 205.078284740448 and batch: 1000, loss is 4.120785450935363 and perplexity is 61.60761302278728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.696346376000381 and perplexity of 109.54619977191354
Finished 28 epochs...
Completing Train Step...
At time: 206.20059633255005 and batch: 50, loss is 4.285868196487427 and perplexity is 72.66560736634959
At time: 206.5395393371582 and batch: 100, loss is 4.179063696861267 and perplexity is 65.30467960347049
At time: 206.86445879936218 and batch: 150, loss is 4.211764965057373 and perplexity is 67.47552672225008
At time: 207.18939232826233 and batch: 200, loss is 4.230956478118896 and perplexity is 68.78299014582932
At time: 207.51584577560425 and batch: 250, loss is 4.207808213233948 and perplexity is 67.20907030744196
At time: 207.85537147521973 and batch: 300, loss is 4.121484847068786 and perplexity is 61.650716220472724
At time: 208.1800401210785 and batch: 350, loss is 4.186792998313904 and perplexity is 65.81139491340025
At time: 208.50616073608398 and batch: 400, loss is 4.112354507446289 and perplexity is 61.0903861379524
At time: 208.831152677536 and batch: 450, loss is 4.176462440490723 and perplexity is 65.13502614136402
At time: 209.15711951255798 and batch: 500, loss is 4.10959677696228 and perplexity is 60.92214740298625
At time: 209.4831714630127 and batch: 550, loss is 4.151660351753235 and perplexity is 63.53941053655416
At time: 209.80838060379028 and batch: 600, loss is 4.197951464653015 and perplexity is 66.54986156362558
At time: 210.13541340827942 and batch: 650, loss is 4.156908922195434 and perplexity is 63.87377831745445
At time: 210.46150016784668 and batch: 700, loss is 4.14662006855011 and perplexity is 63.21995965064687
At time: 210.78687143325806 and batch: 750, loss is 4.108125329017639 and perplexity is 60.832569555139585
At time: 211.11343240737915 and batch: 800, loss is 4.114023642539978 and perplexity is 61.19243939198265
At time: 211.43899393081665 and batch: 850, loss is 4.083637704849243 and perplexity is 59.36101542025828
At time: 211.7662079334259 and batch: 900, loss is 4.214488964080811 and perplexity is 67.65958055857341
At time: 212.09103798866272 and batch: 950, loss is 4.152022366523743 and perplexity is 63.5624169057492
At time: 212.41690707206726 and batch: 1000, loss is 4.119533863067627 and perplexity is 61.53055391494922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.695894939143483 and perplexity of 109.49675774062062
Finished 29 epochs...
Completing Train Step...
At time: 213.53889536857605 and batch: 50, loss is 4.2836079454422 and perplexity is 72.5015503262396
At time: 213.8646638393402 and batch: 100, loss is 4.17678475856781 and perplexity is 65.15602372150896
At time: 214.19112944602966 and batch: 150, loss is 4.209272603988648 and perplexity is 67.30756274674
At time: 214.51664566993713 and batch: 200, loss is 4.22873363494873 and perplexity is 68.63026614959053
At time: 214.8416395187378 and batch: 250, loss is 4.205538187026978 and perplexity is 67.05667699035023
At time: 215.16686367988586 and batch: 300, loss is 4.1192351341247555 and perplexity is 61.51217570281254
At time: 215.49370646476746 and batch: 350, loss is 4.184796075820923 and perplexity is 65.68010578933416
At time: 215.82072257995605 and batch: 400, loss is 4.110216469764709 and perplexity is 60.959912119293314
At time: 216.1625542640686 and batch: 450, loss is 4.174455389976502 and perplexity is 65.00442795607086
At time: 216.48949122428894 and batch: 500, loss is 4.107643837928772 and perplexity is 60.803286265373174
At time: 216.8166060447693 and batch: 550, loss is 4.149781255722046 and perplexity is 63.42012599104835
At time: 217.14304184913635 and batch: 600, loss is 4.196099681854248 and perplexity is 66.42673970737258
At time: 217.47077465057373 and batch: 650, loss is 4.15514491558075 and perplexity is 63.76120387022402
At time: 217.79572939872742 and batch: 700, loss is 4.145573196411132 and perplexity is 63.15381106685727
At time: 218.12152814865112 and batch: 750, loss is 4.106890215873718 and perplexity is 60.757480829984914
At time: 218.44762301445007 and batch: 800, loss is 4.11279990196228 and perplexity is 61.11760152125932
At time: 218.77425599098206 and batch: 850, loss is 4.0826043081283565 and perplexity is 59.29970362673395
At time: 219.10026383399963 and batch: 900, loss is 4.2140561294555665 and perplexity is 67.63030148633338
At time: 219.42589902877808 and batch: 950, loss is 4.151283540725708 and perplexity is 63.51547269628575
At time: 219.75247716903687 and batch: 1000, loss is 4.118627510070801 and perplexity is 61.474810778307486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.695612465463033 and perplexity of 109.46583215650172
Finished 30 epochs...
Completing Train Step...
At time: 220.8642292022705 and batch: 50, loss is 4.281680197715759 and perplexity is 72.36192025645076
At time: 221.20560431480408 and batch: 100, loss is 4.174902167320251 and perplexity is 65.0334769504581
At time: 221.531352519989 and batch: 150, loss is 4.207116556167603 and perplexity is 67.16260075139229
At time: 221.8575370311737 and batch: 200, loss is 4.22681604385376 and perplexity is 68.4987874638441
At time: 222.18291568756104 and batch: 250, loss is 4.20381233215332 and perplexity is 66.94104670680046
At time: 222.50912857055664 and batch: 300, loss is 4.117832736968994 and perplexity is 61.425971662840716
At time: 222.83559131622314 and batch: 350, loss is 4.18295117855072 and perplexity is 65.55904444866273
At time: 223.1623020172119 and batch: 400, loss is 4.108131780624389 and perplexity is 60.832962024221985
At time: 223.48834371566772 and batch: 450, loss is 4.172707867622376 and perplexity is 64.89093046368653
At time: 223.81393671035767 and batch: 500, loss is 4.106039538383484 and perplexity is 60.70581778608926
At time: 224.13986372947693 and batch: 550, loss is 4.148541011810303 and perplexity is 63.34151832232346
At time: 224.46544885635376 and batch: 600, loss is 4.1947081089019775 and perplexity is 66.33436634013454
At time: 224.8053662776947 and batch: 650, loss is 4.153772649765014 and perplexity is 63.673766557121866
At time: 225.13110876083374 and batch: 700, loss is 4.1448079872131345 and perplexity is 63.10550367472786
At time: 225.45757842063904 and batch: 750, loss is 4.106332244873047 and perplexity is 60.72358937371215
At time: 225.7834403514862 and batch: 800, loss is 4.111772518157959 and perplexity is 61.05484253160251
At time: 226.11005330085754 and batch: 850, loss is 4.081755504608155 and perplexity is 59.24939118525807
At time: 226.43812227249146 and batch: 900, loss is 4.213044481277466 and perplexity is 67.56191801089166
At time: 226.76508116722107 and batch: 950, loss is 4.150328221321106 and perplexity is 63.45482410672627
At time: 227.09358930587769 and batch: 1000, loss is 4.117350511550903 and perplexity is 61.39635763886536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.695281982421875 and perplexity of 109.42966153260585
Finished 31 epochs...
Completing Train Step...
At time: 228.21286582946777 and batch: 50, loss is 4.279536504745483 and perplexity is 72.20696466462827
At time: 228.55514788627625 and batch: 100, loss is 4.172912092208862 and perplexity is 64.90418414044444
At time: 228.881737947464 and batch: 150, loss is 4.20486659526825 and perplexity is 67.01165739780161
At time: 229.2070050239563 and batch: 200, loss is 4.224733743667603 and perplexity is 68.35630082741622
At time: 229.53320574760437 and batch: 250, loss is 4.201659574508667 and perplexity is 66.79709386015102
At time: 229.86000609397888 and batch: 300, loss is 4.1155748462677 and perplexity is 61.2874349917917
At time: 230.18608212471008 and batch: 350, loss is 4.181125063896179 and perplexity is 65.43943536005038
At time: 230.51233768463135 and batch: 400, loss is 4.10607831954956 and perplexity is 60.708172074141366
At time: 230.83765029907227 and batch: 450, loss is 4.1708161163330075 and perplexity is 64.76828900245972
At time: 231.16427540779114 and batch: 500, loss is 4.1042314577102665 and perplexity is 60.59615593880687
At time: 231.4913387298584 and batch: 550, loss is 4.146562023162842 and perplexity is 63.216290130106266
At time: 231.81802988052368 and batch: 600, loss is 4.193012452125549 and perplexity is 66.22198133246764
At time: 232.14387822151184 and batch: 650, loss is 4.152182688713074 and perplexity is 63.57260818850928
At time: 232.46964955329895 and batch: 700, loss is 4.1437865829467775 and perplexity is 63.04108035077375
At time: 232.79711508750916 and batch: 750, loss is 4.105059695243836 and perplexity is 60.64636473908167
At time: 233.13672852516174 and batch: 800, loss is 4.110600047111511 and perplexity is 60.98329944578287
At time: 233.4630630016327 and batch: 850, loss is 4.080768685340882 and perplexity is 59.19095158386702
At time: 233.79062247276306 and batch: 900, loss is 4.212541561126709 and perplexity is 67.52794830364137
At time: 234.11684322357178 and batch: 950, loss is 4.149611825942993 and perplexity is 63.409381643346116
At time: 234.44366025924683 and batch: 1000, loss is 4.116612901687622 and perplexity is 61.35108777770197
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6953887939453125 and perplexity of 109.44135050571079
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 235.56318759918213 and batch: 50, loss is 4.2785901927948 and perplexity is 72.1386666717438
At time: 235.89080166816711 and batch: 100, loss is 4.169710192680359 and perplexity is 64.69669981309576
At time: 236.21707487106323 and batch: 150, loss is 4.202327585220337 and perplexity is 66.84172994138835
At time: 236.54335188865662 and batch: 200, loss is 4.221733779907226 and perplexity is 68.15154169070108
At time: 236.87065720558167 and batch: 250, loss is 4.198025784492493 and perplexity is 66.55480772245082
At time: 237.19963335990906 and batch: 300, loss is 4.111424779891967 and perplexity is 61.03361511753614
At time: 237.52898907661438 and batch: 350, loss is 4.1748539113998415 and perplexity is 65.03033877588877
At time: 237.8553764820099 and batch: 400, loss is 4.0982064485549925 and perplexity is 60.232161181479604
At time: 238.1828408241272 and batch: 450, loss is 4.162762823104859 and perplexity is 64.24878564339839
At time: 238.51074028015137 and batch: 500, loss is 4.094742608070374 and perplexity is 60.02388750474203
At time: 238.83751368522644 and batch: 550, loss is 4.1369760131835935 and perplexity is 62.61319340596194
At time: 239.16316604614258 and batch: 600, loss is 4.182319898605346 and perplexity is 65.51767139902665
At time: 239.4898726940155 and batch: 650, loss is 4.140368752479553 and perplexity is 62.82598441577219
At time: 239.81908535957336 and batch: 700, loss is 4.131335654258728 and perplexity is 62.261026628874404
At time: 240.14771270751953 and batch: 750, loss is 4.092245440483094 and perplexity is 59.8741847925765
At time: 240.47487902641296 and batch: 800, loss is 4.094546480178833 and perplexity is 60.0121163006121
At time: 240.80088710784912 and batch: 850, loss is 4.062199206352234 and perplexity is 58.101948851607276
At time: 241.1273238658905 and batch: 900, loss is 4.193388018608093 and perplexity is 66.2468567599598
At time: 241.45494079589844 and batch: 950, loss is 4.130501265525818 and perplexity is 62.2090983969344
At time: 241.79703950881958 and batch: 1000, loss is 4.097133231163025 and perplexity is 60.16755365370631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.690944020341083 and perplexity of 108.95598794219593
Finished 33 epochs...
Completing Train Step...
At time: 242.90571427345276 and batch: 50, loss is 4.274391498565674 and perplexity is 71.8364134467639
At time: 243.24588537216187 and batch: 100, loss is 4.166141119003296 and perplexity is 64.46620409757212
At time: 243.57164859771729 and batch: 150, loss is 4.199232773780823 and perplexity is 66.63518716126129
At time: 243.89839148521423 and batch: 200, loss is 4.218758721351623 and perplexity is 67.94908816838432
At time: 244.22528219223022 and batch: 250, loss is 4.195310502052307 and perplexity is 66.37433774609114
At time: 244.55169010162354 and batch: 300, loss is 4.109246311187744 and perplexity is 60.900800016392935
At time: 244.87626028060913 and batch: 350, loss is 4.172901563644409 and perplexity is 64.90350079615577
At time: 245.20267629623413 and batch: 400, loss is 4.096283984184265 and perplexity is 60.116478231440276
At time: 245.52908301353455 and batch: 450, loss is 4.1610216045379635 and perplexity is 64.13701180451746
At time: 245.85621333122253 and batch: 500, loss is 4.093140892982483 and perplexity is 59.92782329277832
At time: 246.1818733215332 and batch: 550, loss is 4.1358645629882815 and perplexity is 62.54364061931093
At time: 246.5073606967926 and batch: 600, loss is 4.181395273208619 and perplexity is 65.45712009406778
At time: 246.833922624588 and batch: 650, loss is 4.139569497108459 and perplexity is 62.7757904718453
At time: 247.16006731987 and batch: 700, loss is 4.130443396568299 and perplexity is 62.20549852542337
At time: 247.48746514320374 and batch: 750, loss is 4.09150839805603 and perplexity is 59.83007123687659
At time: 247.81306409835815 and batch: 800, loss is 4.094343085289001 and perplexity is 59.99991138407946
At time: 248.13910603523254 and batch: 850, loss is 4.062490577697754 and perplexity is 58.11888056120924
At time: 248.46787524223328 and batch: 900, loss is 4.194380211830139 and perplexity is 66.31261906127871
At time: 248.795556306839 and batch: 950, loss is 4.132008681297302 and perplexity is 62.30294408745425
At time: 249.12160921096802 and batch: 1000, loss is 4.0986541700363155 and perplexity is 60.25913445171274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.690528311380526 and perplexity of 108.91070337495444
Finished 34 epochs...
Completing Train Step...
At time: 250.22387409210205 and batch: 50, loss is 4.272867488861084 and perplexity is 71.72701743699201
At time: 250.56041836738586 and batch: 100, loss is 4.164658317565918 and perplexity is 64.37068435338253
At time: 250.8868591785431 and batch: 150, loss is 4.197771353721619 and perplexity is 66.53787628544724
At time: 251.2123830318451 and batch: 200, loss is 4.21765516281128 and perplexity is 67.87414373221972
At time: 251.5386085510254 and batch: 250, loss is 4.194269886016846 and perplexity is 66.30530347120582
At time: 251.8664095401764 and batch: 300, loss is 4.10813316822052 and perplexity is 60.83304643586326
At time: 252.19331192970276 and batch: 350, loss is 4.172028474807739 and perplexity is 64.84685900440597
At time: 252.52061319351196 and batch: 400, loss is 4.095480895042419 and perplexity is 60.068218721464554
At time: 252.8464376926422 and batch: 450, loss is 4.160223302841186 and perplexity is 64.08583155052752
At time: 253.1723666191101 and batch: 500, loss is 4.092438106536865 and perplexity is 59.88572162682574
At time: 253.49823665618896 and batch: 550, loss is 4.13541054725647 and perplexity is 62.515251267638774
At time: 253.82564210891724 and batch: 600, loss is 4.181079549789429 and perplexity is 65.43645701038271
At time: 254.15054559707642 and batch: 650, loss is 4.139265356063842 and perplexity is 62.756700680496245
At time: 254.47618746757507 and batch: 700, loss is 4.13009669303894 and perplexity is 62.18393539776132
At time: 254.8014681339264 and batch: 750, loss is 4.091195549964905 and perplexity is 59.811356440894976
At time: 255.1271915435791 and batch: 800, loss is 4.0943189191818234 and perplexity is 59.99846143731016
At time: 255.4532437324524 and batch: 850, loss is 4.062712521553039 and perplexity is 58.131781121172914
At time: 255.77995443344116 and batch: 900, loss is 4.195035662651062 and perplexity is 66.3560979694664
At time: 256.10447001457214 and batch: 950, loss is 4.132909841537476 and perplexity is 62.359114328904504
At time: 256.43128299713135 and batch: 1000, loss is 4.09947555065155 and perplexity is 60.30865046960692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.690288636742569 and perplexity of 108.88460336943297
Finished 35 epochs...
Completing Train Step...
At time: 257.54850459098816 and batch: 50, loss is 4.271825752258301 and perplexity is 71.65233568363128
At time: 257.87710762023926 and batch: 100, loss is 4.163557753562928 and perplexity is 64.29987926524781
At time: 258.20259284973145 and batch: 150, loss is 4.196685085296631 and perplexity is 66.46563753381344
At time: 258.5282235145569 and batch: 200, loss is 4.216941723823547 and perplexity is 67.8257369415213
At time: 258.870219707489 and batch: 250, loss is 4.193572487831116 and perplexity is 66.25907839337692
At time: 259.19787883758545 and batch: 300, loss is 4.107358345985412 and perplexity is 60.7859298946845
At time: 259.52539110183716 and batch: 350, loss is 4.171439638137818 and perplexity is 64.80868603581979
At time: 259.8511543273926 and batch: 400, loss is 4.0949428224563595 and perplexity is 60.03590635365635
At time: 260.1764693260193 and batch: 450, loss is 4.15969419002533 and perplexity is 64.05193188490396
At time: 260.5021741390228 and batch: 500, loss is 4.091976890563965 and perplexity is 59.85810774393421
At time: 260.82929134368896 and batch: 550, loss is 4.135136942863465 and perplexity is 62.49814915997472
At time: 261.1547427177429 and batch: 600, loss is 4.180896120071411 and perplexity is 65.42445512031054
At time: 261.48202252388 and batch: 650, loss is 4.139061465263366 and perplexity is 62.7439064709144
At time: 261.80651807785034 and batch: 700, loss is 4.129863519668579 and perplexity is 62.16943745029561
At time: 262.1327986717224 and batch: 750, loss is 4.0909654235839845 and perplexity is 59.79759385352726
At time: 262.45890069007874 and batch: 800, loss is 4.094275469779968 and perplexity is 59.99585459668173
At time: 262.78380584716797 and batch: 850, loss is 4.0628267097473145 and perplexity is 58.13841946329208
At time: 263.1105377674103 and batch: 900, loss is 4.1954640293121335 and perplexity is 66.38452879857024
At time: 263.43843173980713 and batch: 950, loss is 4.133504700660706 and perplexity is 62.396220252283925
At time: 263.76478600502014 and batch: 1000, loss is 4.0999681091308595 and perplexity is 60.338363323829846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.690117068407012 and perplexity of 108.86592382172044
Finished 36 epochs...
Completing Train Step...
At time: 264.87652564048767 and batch: 50, loss is 4.270990762710571 and perplexity is 71.59253170358882
At time: 265.2183287143707 and batch: 100, loss is 4.1626358270645145 and perplexity is 64.24062682010484
At time: 265.5447678565979 and batch: 150, loss is 4.195785465240479 and perplexity is 66.40587060103968
At time: 265.8710563182831 and batch: 200, loss is 4.2163925457000735 and perplexity is 67.78849875671867
At time: 266.1967577934265 and batch: 250, loss is 4.1930113744735715 and perplexity is 66.22190996825695
At time: 266.5227792263031 and batch: 300, loss is 4.106741600036621 and perplexity is 60.74845197704334
At time: 266.8493411540985 and batch: 350, loss is 4.17097644329071 and perplexity is 64.77867393766141
At time: 267.175217628479 and batch: 400, loss is 4.094514393806458 and perplexity is 60.01019076039343
At time: 267.5161793231964 and batch: 450, loss is 4.159274320602417 and perplexity is 64.02504408230512
At time: 267.84329867362976 and batch: 500, loss is 4.091602129936218 and perplexity is 59.83567948477712
At time: 268.16936206817627 and batch: 550, loss is 4.134915652275086 and perplexity is 62.48432043791397
At time: 268.49688816070557 and batch: 600, loss is 4.180735306739807 and perplexity is 65.41393484163747
At time: 268.82332968711853 and batch: 650, loss is 4.138872222900391 and perplexity is 62.73203378923412
At time: 269.1497552394867 and batch: 700, loss is 4.129654760360718 and perplexity is 62.15646035615512
At time: 269.4761118888855 and batch: 750, loss is 4.090725951194763 and perplexity is 59.78327569532765
At time: 269.80185079574585 and batch: 800, loss is 4.094187459945679 and perplexity is 59.99057460380966
At time: 270.1288843154907 and batch: 850, loss is 4.062842493057251 and perplexity is 58.139337087227226
At time: 270.4554717540741 and batch: 900, loss is 4.1957026290893555 and perplexity is 66.40037002213363
At time: 270.78266048431396 and batch: 950, loss is 4.133874731063843 and perplexity is 62.41931302306826
At time: 271.11042284965515 and batch: 1000, loss is 4.100206303596496 and perplexity is 60.352737299871976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.689896374213986 and perplexity of 108.84190039552738
Finished 37 epochs...
Completing Train Step...
At time: 272.2307689189911 and batch: 50, loss is 4.270130472183228 and perplexity is 71.53096781194901
At time: 272.57165908813477 and batch: 100, loss is 4.161712946891785 and perplexity is 64.1813677680369
At time: 272.8966507911682 and batch: 150, loss is 4.194913911819458 and perplexity is 66.34801955114368
At time: 273.2228512763977 and batch: 200, loss is 4.215776991844177 and perplexity is 67.74678412504191
At time: 273.5484824180603 and batch: 250, loss is 4.192343945503235 and perplexity is 66.17772629344559
At time: 273.87394976615906 and batch: 300, loss is 4.105911326408386 and perplexity is 60.698035072220236
At time: 274.19955801963806 and batch: 350, loss is 4.170600409507752 and perplexity is 64.75431954716935
At time: 274.5260899066925 and batch: 400, loss is 4.094065847396851 and perplexity is 59.98327944072714
At time: 274.8525393009186 and batch: 450, loss is 4.158818321228027 and perplexity is 63.99585535778414
At time: 275.1791796684265 and batch: 500, loss is 4.091075572967529 and perplexity is 59.80418088439973
At time: 275.5051100254059 and batch: 550, loss is 4.134323315620422 and perplexity is 62.44731964412192
At time: 275.84566926956177 and batch: 600, loss is 4.180443868637085 and perplexity is 65.39487350631028
At time: 276.1701591014862 and batch: 650, loss is 4.138575077056885 and perplexity is 62.71339599534761
At time: 276.4958462715149 and batch: 700, loss is 4.129368314743042 and perplexity is 62.1386584602351
At time: 276.8212661743164 and batch: 750, loss is 4.090265688896179 and perplexity is 59.75576603875376
At time: 277.14631390571594 and batch: 800, loss is 4.094127492904663 and perplexity is 59.98697725442413
At time: 277.4716708660126 and batch: 850, loss is 4.062665281295776 and perplexity is 58.12903502574121
At time: 277.79847049713135 and batch: 900, loss is 4.195644268989563 and perplexity is 66.39649500298721
At time: 278.12501311302185 and batch: 950, loss is 4.134025139808655 and perplexity is 62.428702139677
At time: 278.45106863975525 and batch: 1000, loss is 4.1002521800994876 and perplexity is 60.35550613591701
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6897735595703125 and perplexity of 108.8285338371349
Finished 38 epochs...
Completing Train Step...
At time: 279.57830715179443 and batch: 50, loss is 4.269400472640991 and perplexity is 71.4787692929561
At time: 279.9047477245331 and batch: 100, loss is 4.160932130813599 and perplexity is 64.13127348391998
At time: 280.2318823337555 and batch: 150, loss is 4.194153895378113 and perplexity is 66.29761312271765
At time: 280.5579619407654 and batch: 200, loss is 4.215380983352661 and perplexity is 67.719961134672
At time: 280.88547229766846 and batch: 250, loss is 4.1918696737289425 and perplexity is 66.1463475074022
At time: 281.2129898071289 and batch: 300, loss is 4.105431909561157 and perplexity is 60.668942385932105
At time: 281.53998589515686 and batch: 350, loss is 4.170259547233582 and perplexity is 64.73225100392676
At time: 281.86806631088257 and batch: 400, loss is 4.093702869415283 and perplexity is 59.961510782038836
At time: 282.1952226161957 and batch: 450, loss is 4.158457183837891 and perplexity is 63.97274823427495
At time: 282.5212285518646 and batch: 500, loss is 4.090835270881652 and perplexity is 59.78981154154972
At time: 282.8474793434143 and batch: 550, loss is 4.134180951118469 and perplexity is 62.43842999536268
At time: 283.1732659339905 and batch: 600, loss is 4.18036301612854 and perplexity is 65.38958638048287
At time: 283.50092792510986 and batch: 650, loss is 4.138391900062561 and perplexity is 62.70190939603783
At time: 283.82762145996094 and batch: 700, loss is 4.129183821678161 and perplexity is 62.127195366151405
At time: 284.16797637939453 and batch: 750, loss is 4.09006251335144 and perplexity is 59.743626361722065
At time: 284.493705034256 and batch: 800, loss is 4.094001626968383 and perplexity is 59.97942741251134
At time: 284.81958770751953 and batch: 850, loss is 4.062575197219848 and perplexity is 58.123798761191914
At time: 285.14578580856323 and batch: 900, loss is 4.195780725479126 and perplexity is 66.40555585380649
At time: 285.47160935401917 and batch: 950, loss is 4.134254455566406 and perplexity is 62.44301966636811
At time: 285.7981696128845 and batch: 1000, loss is 4.100361442565918 and perplexity is 60.362101087663845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.68966377072218 and perplexity of 108.81658633362427
Finished 39 epochs...
Completing Train Step...
At time: 286.91256833076477 and batch: 50, loss is 4.268771429061889 and perplexity is 71.4338201710507
At time: 287.253338098526 and batch: 100, loss is 4.160215821266174 and perplexity is 64.08535208936513
At time: 287.5808045864105 and batch: 150, loss is 4.193465189933777 and perplexity is 66.25196931498712
At time: 287.9092242717743 and batch: 200, loss is 4.215014157295227 and perplexity is 67.69512424400719
At time: 288.23571038246155 and batch: 250, loss is 4.191451616287232 and perplexity is 66.11870031404486
At time: 288.56220388412476 and batch: 300, loss is 4.105018720626831 and perplexity is 60.643879828423266
At time: 288.88814067840576 and batch: 350, loss is 4.169958133697509 and perplexity is 64.71274276742471
At time: 289.2150263786316 and batch: 400, loss is 4.093367028236389 and perplexity is 59.94137661869933
At time: 289.54391741752625 and batch: 450, loss is 4.158136739730835 and perplexity is 63.95225182824274
At time: 289.8716561794281 and batch: 500, loss is 4.090616316795349 and perplexity is 59.776721751077226
At time: 290.19837260246277 and batch: 550, loss is 4.134057979583741 and perplexity is 62.43075231787768
At time: 290.52523159980774 and batch: 600, loss is 4.180287837982178 and perplexity is 65.38467069736566
At time: 290.8532953262329 and batch: 650, loss is 4.138198661804199 and perplexity is 62.689794158871536
At time: 291.18052864074707 and batch: 700, loss is 4.128997783660889 and perplexity is 62.11563842095572
At time: 291.50777530670166 and batch: 750, loss is 4.089872527122497 and perplexity is 59.732276973594104
At time: 291.83557510375977 and batch: 800, loss is 4.0938585758209225 and perplexity is 59.9708479002651
At time: 292.1632761955261 and batch: 850, loss is 4.062478413581848 and perplexity is 58.11817360070957
At time: 292.49077916145325 and batch: 900, loss is 4.195879735946655 and perplexity is 66.41213102443812
At time: 292.8319113254547 and batch: 950, loss is 4.134431862831116 and perplexity is 62.45409849439057
At time: 293.15810465812683 and batch: 1000, loss is 4.100431022644043 and perplexity is 60.36630123349485
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.689569984994283 and perplexity of 108.80638136941522
Finished 40 epochs...
Completing Train Step...
At time: 294.27881956100464 and batch: 50, loss is 4.268192796707154 and perplexity is 71.39249820774276
At time: 294.6194636821747 and batch: 100, loss is 4.159538898468018 and perplexity is 64.04198593293388
At time: 294.9460983276367 and batch: 150, loss is 4.192823100090027 and perplexity is 66.20944325259894
At time: 295.27248215675354 and batch: 200, loss is 4.214669127464294 and perplexity is 67.67177143567375
At time: 295.5986862182617 and batch: 250, loss is 4.191066155433655 and perplexity is 66.09321905471388
At time: 295.92461133003235 and batch: 300, loss is 4.104638953208923 and perplexity is 60.62085363134532
At time: 296.25006103515625 and batch: 350, loss is 4.169676470756531 and perplexity is 64.69451815269781
At time: 296.57912492752075 and batch: 400, loss is 4.0930488538742065 and perplexity is 59.922307843183944
At time: 296.90336537361145 and batch: 450, loss is 4.157838401794433 and perplexity is 63.933175291172915
At time: 297.228905916214 and batch: 500, loss is 4.090406475067138 and perplexity is 59.76417941647683
At time: 297.5549466609955 and batch: 550, loss is 4.133940453529358 and perplexity is 62.423415509027166
At time: 297.8827488422394 and batch: 600, loss is 4.1802089357376095 and perplexity is 65.37951190361017
At time: 298.2120006084442 and batch: 650, loss is 4.138000721931458 and perplexity is 62.677386577011646
At time: 298.5412299633026 and batch: 700, loss is 4.128812837600708 and perplexity is 62.104151440622736
At time: 298.86904644966125 and batch: 750, loss is 4.089688186645508 and perplexity is 59.72126691199629
At time: 299.20099997520447 and batch: 800, loss is 4.093703756332397 and perplexity is 59.96156396295251
At time: 299.5298984050751 and batch: 850, loss is 4.062371077537537 and perplexity is 58.1119357606312
At time: 299.8556122779846 and batch: 900, loss is 4.195946445465088 and perplexity is 66.41656149349245
At time: 300.18000650405884 and batch: 950, loss is 4.134571824073792 and perplexity is 62.4628402593659
At time: 300.5062916278839 and batch: 1000, loss is 4.1004700851440425 and perplexity is 60.36865933819322
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.689488480730755 and perplexity of 108.79751354682209
Finished 41 epochs...
Completing Train Step...
At time: 301.62619805336 and batch: 50, loss is 4.267650413513183 and perplexity is 71.3537866157468
At time: 301.95102071762085 and batch: 100, loss is 4.158891468048096 and perplexity is 64.00053662230884
At time: 302.2779381275177 and batch: 150, loss is 4.1922186088562015 and perplexity is 66.169432318904
At time: 302.60584688186646 and batch: 200, loss is 4.214338521957398 and perplexity is 67.64940247323052
At time: 302.9324758052826 and batch: 250, loss is 4.190696482658386 and perplexity is 66.06879070652533
At time: 303.25850105285645 and batch: 300, loss is 4.104281029701233 and perplexity is 60.59915988535685
At time: 303.5855495929718 and batch: 350, loss is 4.169406580924988 and perplexity is 64.67706011607109
At time: 303.9126703739166 and batch: 400, loss is 4.092744908332825 and perplexity is 59.90409749250335
At time: 304.2388129234314 and batch: 450, loss is 4.157556791305542 and perplexity is 63.91517357327803
At time: 304.5641460418701 and batch: 500, loss is 4.090203313827515 and perplexity is 59.75203888498597
At time: 304.89017963409424 and batch: 550, loss is 4.133824996948242 and perplexity is 62.41620873093389
At time: 305.21699690818787 and batch: 600, loss is 4.180126051902771 and perplexity is 65.37409322350734
At time: 305.54516530036926 and batch: 650, loss is 4.1378013801574705 and perplexity is 62.66489360080989
At time: 305.8708953857422 and batch: 700, loss is 4.1286296558380124 and perplexity is 62.09277613459726
At time: 306.1968593597412 and batch: 750, loss is 4.089506621360779 and perplexity is 59.710424587489705
At time: 306.5238597393036 and batch: 800, loss is 4.093539853096008 and perplexity is 59.95173687392786
At time: 306.8482768535614 and batch: 850, loss is 4.062254199981689 and perplexity is 58.10514417651352
At time: 307.17312264442444 and batch: 900, loss is 4.195986866950989 and perplexity is 66.41924620385605
At time: 307.4993691444397 and batch: 950, loss is 4.1346833658218385 and perplexity is 62.469807862338484
At time: 307.8251049518585 and batch: 1000, loss is 4.1004870414733885 and perplexity is 60.36968297774169
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.689415908441311 and perplexity of 108.78961814867525
Finished 42 epochs...
Completing Train Step...
At time: 308.9307198524475 and batch: 50, loss is 4.267134490013123 and perplexity is 71.31698301515101
At time: 309.2726044654846 and batch: 100, loss is 4.158274750709534 and perplexity is 63.96107855018584
At time: 309.5987386703491 and batch: 150, loss is 4.191644563674926 and perplexity is 66.13145897538384
At time: 309.93857622146606 and batch: 200, loss is 4.21401771068573 and perplexity is 67.62770326325719
At time: 310.2640974521637 and batch: 250, loss is 4.190334148406983 and perplexity is 66.04485605713705
At time: 310.59027314186096 and batch: 300, loss is 4.103938932418823 and perplexity is 60.57843262301597
At time: 310.91661405563354 and batch: 350, loss is 4.169144802093506 and perplexity is 64.66013124675669
At time: 311.24240159988403 and batch: 400, loss is 4.09245349407196 and perplexity is 59.88664312755876
At time: 311.56778383255005 and batch: 450, loss is 4.1572887992858885 and perplexity is 63.89804711180565
At time: 311.89388060569763 and batch: 500, loss is 4.090004820823669 and perplexity is 59.7401797003233
At time: 312.21989583969116 and batch: 550, loss is 4.133709301948548 and perplexity is 62.408987905398604
At time: 312.5453476905823 and batch: 600, loss is 4.180039939880371 and perplexity is 65.36846397050392
At time: 312.8703668117523 and batch: 650, loss is 4.13760199546814 and perplexity is 62.65240042598265
At time: 313.197434425354 and batch: 700, loss is 4.128448243141174 and perplexity is 62.08151273831696
At time: 313.5239839553833 and batch: 750, loss is 4.089326872825622 and perplexity is 59.69969269068678
At time: 313.84960412979126 and batch: 800, loss is 4.093368458747864 and perplexity is 59.94146236558772
At time: 314.1756181716919 and batch: 850, loss is 4.062129907608032 and perplexity is 58.097922599024926
At time: 314.50104427337646 and batch: 900, loss is 4.19600670337677 and perplexity is 66.42056373737134
At time: 314.8280379772186 and batch: 950, loss is 4.134773001670838 and perplexity is 62.475407647570044
At time: 315.153284072876 and batch: 1000, loss is 4.100487632751465 and perplexity is 60.369718673022255
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.689348174304497 and perplexity of 108.78224962734865
Finished 43 epochs...
Completing Train Step...
At time: 316.2551853656769 and batch: 50, loss is 4.2666382980346675 and perplexity is 71.28160487815101
At time: 316.5976068973541 and batch: 100, loss is 4.15768681049347 and perplexity is 63.923484312508386
At time: 316.9233272075653 and batch: 150, loss is 4.1910943508148195 and perplexity is 66.09508260448918
At time: 317.25027418136597 and batch: 200, loss is 4.213710045814514 and perplexity is 67.60689979505435
At time: 317.5758230686188 and batch: 250, loss is 4.189984731674194 and perplexity is 66.02178291062256
At time: 317.9028468132019 and batch: 300, loss is 4.103611574172974 and perplexity is 60.558605019118076
At time: 318.22834515571594 and batch: 350, loss is 4.16889109134674 and perplexity is 64.64372835745478
At time: 318.56964468955994 and batch: 400, loss is 4.092171063423157 and perplexity is 59.86973169235182
At time: 318.89457869529724 and batch: 450, loss is 4.1570298385620115 and perplexity is 63.881502169608815
At time: 319.2208478450775 and batch: 500, loss is 4.0898061895370486 and perplexity is 59.728314609994584
At time: 319.5469002723694 and batch: 550, loss is 4.1335901927948 and perplexity is 62.401554866343204
At time: 319.87341046333313 and batch: 600, loss is 4.179948868751526 and perplexity is 65.36251106177247
At time: 320.198805809021 and batch: 650, loss is 4.137401399612426 and perplexity is 62.63983387454789
At time: 320.5244905948639 and batch: 700, loss is 4.128268122673035 and perplexity is 62.0703315941864
At time: 320.85143995285034 and batch: 750, loss is 4.089149041175842 and perplexity is 59.689077139762674
At time: 321.17556524276733 and batch: 800, loss is 4.093191623687744 and perplexity is 59.93086355063531
At time: 321.50110936164856 and batch: 850, loss is 4.062000875473022 and perplexity is 58.09042658365618
At time: 321.82827377319336 and batch: 900, loss is 4.196010165214538 and perplexity is 66.42079367498549
At time: 322.1559200286865 and batch: 950, loss is 4.134844527244568 and perplexity is 62.4798763967591
At time: 322.4813041687012 and batch: 1000, loss is 4.100475635528564 and perplexity is 60.36899440839549
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.689287511313834 and perplexity of 108.77565077091043
Finished 44 epochs...
Completing Train Step...
At time: 323.6011643409729 and batch: 50, loss is 4.266159381866455 and perplexity is 71.24747513837315
At time: 323.9281497001648 and batch: 100, loss is 4.1571220016479495 and perplexity is 63.8873899572973
At time: 324.25456261634827 and batch: 150, loss is 4.190564522743225 and perplexity is 66.06007284972537
At time: 324.58103680610657 and batch: 200, loss is 4.213415055274964 and perplexity is 67.58695934047381
At time: 324.9085943698883 and batch: 250, loss is 4.1896485376358035 and perplexity is 65.99959051148926
At time: 325.2359473705292 and batch: 300, loss is 4.103296294212341 and perplexity is 60.53951511400206
At time: 325.56080317497253 and batch: 350, loss is 4.168644127845764 and perplexity is 64.62776568716303
At time: 325.886545419693 and batch: 400, loss is 4.0918958568573 and perplexity is 59.853257416112456
At time: 326.2130126953125 and batch: 450, loss is 4.156777753829956 and perplexity is 63.86540064781024
At time: 326.53934049606323 and batch: 500, loss is 4.089605655670166 and perplexity is 59.71633826097538
At time: 326.8809857368469 and batch: 550, loss is 4.133467059135437 and perplexity is 62.39387160758622
At time: 327.20603346824646 and batch: 600, loss is 4.17985209941864 and perplexity is 65.35618628120858
At time: 327.533314704895 and batch: 650, loss is 4.137199907302857 and perplexity is 62.62721370122608
At time: 327.8609228134155 and batch: 700, loss is 4.128089032173157 and perplexity is 62.059216382817695
At time: 328.1860468387604 and batch: 750, loss is 4.088973727226257 and perplexity is 59.678613729119945
At time: 328.51231241226196 and batch: 800, loss is 4.093010563850402 and perplexity is 59.9200134605165
At time: 328.8381378650665 and batch: 850, loss is 4.061868023872376 and perplexity is 58.08270969011448
At time: 329.16505575180054 and batch: 900, loss is 4.195999989509582 and perplexity is 66.42011780002487
At time: 329.4969127178192 and batch: 950, loss is 4.134900298118591 and perplexity is 62.483361051244806
At time: 329.82571625709534 and batch: 1000, loss is 4.100452799797058 and perplexity is 60.36761585398806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.689229825647866 and perplexity of 108.76937615603397
Finished 45 epochs...
Completing Train Step...
At time: 330.93920946121216 and batch: 50, loss is 4.26569631099701 and perplexity is 71.21449014589905
At time: 331.280189037323 and batch: 100, loss is 4.156575994491577 and perplexity is 63.85251650662187
At time: 331.6057279109955 and batch: 150, loss is 4.19005214214325 and perplexity is 66.02623361998697
At time: 331.93219232559204 and batch: 200, loss is 4.2131302404403685 and perplexity is 67.56771231287807
At time: 332.2607424259186 and batch: 250, loss is 4.18932276725769 and perplexity is 65.9780933017002
At time: 332.5868704319 and batch: 300, loss is 4.102990880012512 and perplexity is 60.52102830964546
At time: 332.91261076927185 and batch: 350, loss is 4.168402218818665 and perplexity is 64.61213353809731
At time: 333.238849401474 and batch: 400, loss is 4.091626858711242 and perplexity is 59.83715916612879
At time: 333.5650610923767 and batch: 450, loss is 4.156531438827515 and perplexity is 63.849671578726166
At time: 333.8920006752014 and batch: 500, loss is 4.089403529167175 and perplexity is 59.704269226128254
At time: 334.2182688713074 and batch: 550, loss is 4.1333401489257815 and perplexity is 62.385953690702124
At time: 334.54371094703674 and batch: 600, loss is 4.179750366210937 and perplexity is 65.3495377249302
At time: 334.8708219528198 and batch: 650, loss is 4.136997556686401 and perplexity is 62.61454232800006
At time: 335.1952772140503 and batch: 700, loss is 4.127910542488098 and perplexity is 62.04814044133059
At time: 335.53694915771484 and batch: 750, loss is 4.088800158500671 and perplexity is 59.668256287080254
At time: 335.86391973495483 and batch: 800, loss is 4.092826275825501 and perplexity is 59.908971937025306
At time: 336.19019746780396 and batch: 850, loss is 4.0617317819595335 and perplexity is 58.07479692968029
At time: 336.51614594459534 and batch: 900, loss is 4.195978326797485 and perplexity is 66.41867897572
At time: 336.84190583229065 and batch: 950, loss is 4.134942288398743 and perplexity is 62.48598480016575
At time: 337.1676127910614 and batch: 1000, loss is 4.10042115688324 and perplexity is 60.36570567694414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.689176233803353 and perplexity of 108.76354716073402
Finished 46 epochs...
Completing Train Step...
At time: 338.2797152996063 and batch: 50, loss is 4.265247163772583 and perplexity is 71.18251153740025
At time: 338.6217269897461 and batch: 100, loss is 4.156045813560485 and perplexity is 63.81867209259209
At time: 338.9489965438843 and batch: 150, loss is 4.189555068016052 and perplexity is 65.99342184315641
At time: 339.2769606113434 and batch: 200, loss is 4.212853593826294 and perplexity is 67.54902251939919
At time: 339.60427951812744 and batch: 250, loss is 4.189005303382873 and perplexity is 65.95715096494119
At time: 339.93090295791626 and batch: 300, loss is 4.102693638801575 and perplexity is 60.50304163922581
At time: 340.2568700313568 and batch: 350, loss is 4.16816433429718 and perplexity is 64.59676513965302
At time: 340.58810663223267 and batch: 400, loss is 4.091362943649292 and perplexity is 59.821369322241004
At time: 340.91536688804626 and batch: 450, loss is 4.156289415359497 and perplexity is 63.83422032963675
At time: 341.2422912120819 and batch: 500, loss is 4.089199719429016 and perplexity is 59.69210215457625
At time: 341.5691740512848 and batch: 550, loss is 4.133209638595581 and perplexity is 62.37781221057125
At time: 341.8967134952545 and batch: 600, loss is 4.179644351005554 and perplexity is 65.34261004749266
At time: 342.22376894950867 and batch: 650, loss is 4.1367941045761105 and perplexity is 62.601804563035074
At time: 342.5498468875885 and batch: 700, loss is 4.1277320861816404 and perplexity is 62.03706854731891
At time: 342.8750903606415 and batch: 750, loss is 4.08862804889679 and perplexity is 59.65798769081375
At time: 343.20234727859497 and batch: 800, loss is 4.092639021873474 and perplexity is 59.897754795528044
At time: 343.5301945209503 and batch: 850, loss is 4.061592874526977 and perplexity is 58.06673046900119
At time: 343.8844680786133 and batch: 900, loss is 4.195946407318115 and perplexity is 66.41655895990174
At time: 344.21154522895813 and batch: 950, loss is 4.134972548484802 and perplexity is 62.48787566005194
At time: 344.53919196128845 and batch: 1000, loss is 4.100381798744202 and perplexity is 60.36332984186153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.689124874952363 and perplexity of 108.75796133336424
Finished 47 epochs...
Completing Train Step...
At time: 345.66400122642517 and batch: 50, loss is 4.264809737205505 and perplexity is 71.15138122485175
At time: 345.98986554145813 and batch: 100, loss is 4.155529661178589 and perplexity is 63.78574043259051
At time: 346.317994594574 and batch: 150, loss is 4.189071311950683 and perplexity is 65.96150484570835
At time: 346.64356565475464 and batch: 200, loss is 4.212583732604981 and perplexity is 67.53079611709379
At time: 346.97083735466003 and batch: 250, loss is 4.188695182800293 and perplexity is 65.93669946623788
At time: 347.2944903373718 and batch: 300, loss is 4.102404294013977 and perplexity is 60.485537931919026
At time: 347.6217758655548 and batch: 350, loss is 4.167929830551148 and perplexity is 64.58161873226024
At time: 347.94820523262024 and batch: 400, loss is 4.091103253364563 and perplexity is 59.805836310782176
At time: 348.2740783691406 and batch: 450, loss is 4.156050701141357 and perplexity is 63.81898401227532
At time: 348.599885225296 and batch: 500, loss is 4.088994617462158 and perplexity is 59.67986044246102
At time: 348.92618060112 and batch: 550, loss is 4.1330757904052735 and perplexity is 62.36946361202641
At time: 349.2533392906189 and batch: 600, loss is 4.179534497261048 and perplexity is 65.33543231136093
At time: 349.5788428783417 and batch: 650, loss is 4.136590347290039 and perplexity is 62.58905028866645
At time: 349.90641808509827 and batch: 700, loss is 4.127553772926331 and perplexity is 62.026007501871185
At time: 350.23321175575256 and batch: 750, loss is 4.088457431793213 and perplexity is 59.64780988602719
At time: 350.5596420764923 and batch: 800, loss is 4.092449321746826 and perplexity is 59.88639326153369
At time: 350.88533997535706 and batch: 850, loss is 4.061452302932739 and perplexity is 58.05856850981024
At time: 351.21164417266846 and batch: 900, loss is 4.195905756950379 and perplexity is 66.41385915723056
At time: 351.5374422073364 and batch: 950, loss is 4.134992399215698 and perplexity is 62.48911610236776
At time: 351.8638641834259 and batch: 1000, loss is 4.1003355169296265 and perplexity is 60.36053618207095
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.689077237757241 and perplexity of 108.75278053253952
Finished 48 epochs...
Completing Train Step...
At time: 352.9725742340088 and batch: 50, loss is 4.264382538795471 and perplexity is 71.12099195950583
At time: 353.3156695365906 and batch: 100, loss is 4.155027074813843 and perplexity is 63.753690643751916
At time: 353.64188623428345 and batch: 150, loss is 4.1885996913909915 and perplexity is 65.93040337849708
At time: 353.9667685031891 and batch: 200, loss is 4.212319655418396 and perplexity is 67.51296512892958
At time: 354.2917387485504 and batch: 250, loss is 4.188391647338867 and perplexity is 65.91668837693173
At time: 354.6178958415985 and batch: 300, loss is 4.102122292518616 and perplexity is 60.468483324599944
At time: 354.942969083786 and batch: 350, loss is 4.167697896957398 and perplexity is 64.56664182222889
At time: 355.26855659484863 and batch: 400, loss is 4.090846362113953 and perplexity is 59.79047468791628
At time: 355.5931279659271 and batch: 450, loss is 4.155814652442932 and perplexity is 63.803921401989136
At time: 355.9185907840729 and batch: 500, loss is 4.0887887573242185 and perplexity is 59.66757600263973
At time: 356.2437469959259 and batch: 550, loss is 4.132938942909241 and perplexity is 62.36092909107942
At time: 356.57109546661377 and batch: 600, loss is 4.179421467781067 and perplexity is 65.32804789875847
At time: 356.89895391464233 and batch: 650, loss is 4.136386032104492 and perplexity is 62.57626370153495
At time: 357.2256944179535 and batch: 700, loss is 4.12737551689148 and perplexity is 62.014951977102534
At time: 357.55193042755127 and batch: 750, loss is 4.08828863620758 and perplexity is 59.63774244871917
At time: 357.87715697288513 and batch: 800, loss is 4.092258009910584 and perplexity is 59.874937381530565
At time: 358.203027009964 and batch: 850, loss is 4.061311135292053 and perplexity is 58.050373097149226
At time: 358.5278055667877 and batch: 900, loss is 4.195857491493225 and perplexity is 66.4106537393131
At time: 358.85492277145386 and batch: 950, loss is 4.135003132820129 and perplexity is 62.48978683942096
At time: 359.1801209449768 and batch: 1000, loss is 4.100282759666443 and perplexity is 60.3573518093778
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.689028856230945 and perplexity of 108.74751903430916
Finished 49 epochs...
Completing Train Step...
At time: 360.2875325679779 and batch: 50, loss is 4.263964548110962 and perplexity is 71.0912702595233
At time: 360.62899231910706 and batch: 100, loss is 4.154537043571472 and perplexity is 63.72245699687645
At time: 360.9536728858948 and batch: 150, loss is 4.188139152526856 and perplexity is 65.90004685612882
At time: 361.2952356338501 and batch: 200, loss is 4.212060976028442 and perplexity is 67.49550317491723
At time: 361.6222515106201 and batch: 250, loss is 4.188094315528869 and perplexity is 65.89709216210093
At time: 361.9507074356079 and batch: 300, loss is 4.101847381591797 and perplexity is 60.45186216258002
At time: 362.2758994102478 and batch: 350, loss is 4.167468485832214 and perplexity is 64.55183121520308
At time: 362.6014494895935 and batch: 400, loss is 4.09059193611145 and perplexity is 59.77526437148576
At time: 362.9272952079773 and batch: 450, loss is 4.155580296516418 and perplexity is 63.78897032687871
At time: 363.2536053657532 and batch: 500, loss is 4.088582415580749 and perplexity is 59.65526536112204
At time: 363.5802528858185 and batch: 550, loss is 4.132799429893494 and perplexity is 62.35222953666178
At time: 363.9079349040985 and batch: 600, loss is 4.179305438995361 and perplexity is 65.32046840441653
At time: 364.2331266403198 and batch: 650, loss is 4.136181650161743 and perplexity is 62.56347555006776
At time: 364.55981254577637 and batch: 700, loss is 4.127197418212891 and perplexity is 62.00390817957473
At time: 364.8867483139038 and batch: 750, loss is 4.08812171459198 and perplexity is 59.62778845119069
At time: 365.21265506744385 and batch: 800, loss is 4.092065420150757 and perplexity is 59.86340719205445
At time: 365.53757524490356 and batch: 850, loss is 4.061169986724853 and perplexity is 58.04217994840037
At time: 365.863511800766 and batch: 900, loss is 4.195802326202393 and perplexity is 66.40699027733412
At time: 366.1893343925476 and batch: 950, loss is 4.135005564689636 and perplexity is 62.48993880661283
At time: 366.51566767692566 and batch: 1000, loss is 4.1002242517471315 and perplexity is 60.35382052961323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6889841963605186 and perplexity of 108.74266249264693
Finished Training.
Improved accuracyfrom -10000000 to -108.74266249264693
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f4997a568d0>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'num_layers': 1, 'wordvec_source': 'glove', 'data': 'ptb', 'dropout': 0.7063518846167919, 'seq_len': 20, 'batch_size': 50, 'lr': 16.819278223872786, 'anneal': 5.500730385800407, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.61301589012146 and batch: 50, loss is 6.6487353324890135 and perplexity is 771.8076280346205
At time: 0.9610133171081543 and batch: 100, loss is 6.098383359909057 and perplexity is 445.1375608529575
At time: 1.2908053398132324 and batch: 150, loss is 6.029451808929443 and perplexity is 415.4872005675489
At time: 1.6226205825805664 and batch: 200, loss is 5.986567068099975 and perplexity is 398.04579763533593
At time: 1.9553637504577637 and batch: 250, loss is 5.9895174694061275 and perplexity is 399.22192664970714
At time: 2.2853200435638428 and batch: 300, loss is 5.894308586120605 and perplexity is 362.9657895358005
At time: 2.616328716278076 and batch: 350, loss is 5.949643125534058 and perplexity is 383.6164117275686
At time: 2.9467031955718994 and batch: 400, loss is 5.923196907043457 and perplexity is 373.6041846007595
At time: 3.2777185440063477 and batch: 450, loss is 5.913163414001465 and perplexity is 369.87437242662605
At time: 3.6219747066497803 and batch: 500, loss is 5.913197355270386 and perplexity is 369.8869266452193
At time: 3.9520998001098633 and batch: 550, loss is 5.999984378814697 and perplexity is 403.42249150601793
At time: 4.282492399215698 and batch: 600, loss is 6.033263130187988 and perplexity is 417.0737773240122
At time: 4.616022825241089 and batch: 650, loss is 6.024826602935791 and perplexity is 413.56992399469476
At time: 4.949114084243774 and batch: 700, loss is 6.04502275466919 and perplexity is 422.0073600113825
At time: 5.281016826629639 and batch: 750, loss is 5.943381137847901 and perplexity is 381.22171608359986
At time: 5.611422538757324 and batch: 800, loss is 6.031570386886597 and perplexity is 416.36837568165043
At time: 5.941481113433838 and batch: 850, loss is 6.000526676177978 and perplexity is 403.64132579071463
At time: 6.273315191268921 and batch: 900, loss is 6.055871076583863 and perplexity is 426.6103539408322
At time: 6.605729341506958 and batch: 950, loss is 6.090017290115356 and perplexity is 441.4290434390715
At time: 6.936418294906616 and batch: 1000, loss is 6.021051435470581 and perplexity is 412.0115716443752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.0594355885575455 and perplexity of 428.13372507992074
Finished 1 epochs...
Completing Train Step...
At time: 8.066457509994507 and batch: 50, loss is 5.990471305847168 and perplexity is 399.60290073600714
At time: 8.388559341430664 and batch: 100, loss is 6.109456100463867 and perplexity is 450.09384274837277
At time: 8.710367679595947 and batch: 150, loss is 6.1570024585723875 and perplexity is 472.0110793798173
At time: 9.03326678276062 and batch: 200, loss is 6.137934856414795 and perplexity is 463.0962224754045
At time: 9.357161283493042 and batch: 250, loss is 6.1180950546264645 and perplexity is 453.9990268906858
At time: 9.680715322494507 and batch: 300, loss is 6.024076910018921 and perplexity is 413.25998974431764
At time: 10.002233028411865 and batch: 350, loss is 6.045575246810913 and perplexity is 422.2405801817616
At time: 10.323749542236328 and batch: 400, loss is 6.008716068267822 and perplexity is 406.9604752291656
At time: 10.64700174331665 and batch: 450, loss is 6.0103584671020505 and perplexity is 407.62941582251347
At time: 10.97062373161316 and batch: 500, loss is 6.0412225341796875 and perplexity is 420.40668238547386
At time: 11.293235778808594 and batch: 550, loss is 6.118189754486084 and perplexity is 454.0420225706102
At time: 11.614742755889893 and batch: 600, loss is 6.110010204315185 and perplexity is 450.343310589257
At time: 11.936589241027832 and batch: 650, loss is 6.103028192520141 and perplexity is 447.2099595586641
At time: 12.274482727050781 and batch: 700, loss is 6.102578239440918 and perplexity is 447.0087813240786
At time: 12.595379114151001 and batch: 750, loss is 5.991677885055542 and perplexity is 400.0853442822873
At time: 12.917640447616577 and batch: 800, loss is 6.054290628433227 and perplexity is 425.93665091248937
At time: 13.240098476409912 and batch: 850, loss is 6.003292760848999 and perplexity is 404.75937747451485
At time: 13.564324855804443 and batch: 900, loss is 6.075719413757324 and perplexity is 435.16245178731185
At time: 13.886672258377075 and batch: 950, loss is 6.066233263015747 and perplexity is 431.0539529017223
At time: 14.209485054016113 and batch: 1000, loss is 6.053770561218261 and perplexity is 425.71519281623654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.1415319675352515 and perplexity of 464.7650306919882
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 15.318680047988892 and batch: 50, loss is 5.975229434967041 and perplexity is 393.5583867888533
At time: 15.659828424453735 and batch: 100, loss is 5.850092935562134 and perplexity is 347.26665240065785
At time: 15.984327554702759 and batch: 150, loss is 5.78480432510376 and perplexity is 325.3183772445103
At time: 16.312479257583618 and batch: 200, loss is 5.764918127059937 and perplexity is 318.9129326279345
At time: 16.636607885360718 and batch: 250, loss is 5.740126333236694 and perplexity is 311.1037112376646
At time: 16.960968494415283 and batch: 300, loss is 5.624009847640991 and perplexity is 276.99787859078526
At time: 17.283806562423706 and batch: 350, loss is 5.678947038650513 and perplexity is 292.641127839062
At time: 17.606204748153687 and batch: 400, loss is 5.613172063827514 and perplexity is 274.01204460313073
At time: 17.930928468704224 and batch: 450, loss is 5.605346012115478 and perplexity is 271.87598154461585
At time: 18.255945682525635 and batch: 500, loss is 5.59936975479126 and perplexity is 270.2560261751788
At time: 18.5808424949646 and batch: 550, loss is 5.665026712417602 and perplexity is 288.5956901022442
At time: 18.90627408027649 and batch: 600, loss is 5.686535387039185 and perplexity is 294.87023760414115
At time: 19.23153805732727 and batch: 650, loss is 5.624358015060425 and perplexity is 277.0943370182293
At time: 19.556227684020996 and batch: 700, loss is 5.623678073883057 and perplexity is 276.90599320708776
At time: 19.88109827041626 and batch: 750, loss is 5.535562038421631 and perplexity is 253.55025258171764
At time: 20.204246997833252 and batch: 800, loss is 5.600479412078857 and perplexity is 270.55608419413545
At time: 20.54219126701355 and batch: 850, loss is 5.563673000335694 and perplexity is 260.7789204617955
At time: 20.86681342124939 and batch: 900, loss is 5.627655954360962 and perplexity is 278.00968587508044
At time: 21.19076633453369 and batch: 950, loss is 5.58220139503479 and perplexity is 265.6557758617062
At time: 21.514655590057373 and batch: 1000, loss is 5.528416347503662 and perplexity is 251.74491870536224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.639019105492569 and perplexity of 281.18676860166426
Finished 3 epochs...
Completing Train Step...
At time: 22.61788535118103 and batch: 50, loss is 5.6395382308959965 and perplexity is 281.332777691588
At time: 22.95711588859558 and batch: 100, loss is 5.620454540252686 and perplexity is 276.0148145693869
At time: 23.282612562179565 and batch: 150, loss is 5.620416460037231 and perplexity is 276.00430406590175
At time: 23.609676837921143 and batch: 200, loss is 5.606348085403442 and perplexity is 272.14855775121947
At time: 23.93374538421631 and batch: 250, loss is 5.588261337280273 and perplexity is 267.2705222139138
At time: 24.25874423980713 and batch: 300, loss is 5.471313686370849 and perplexity is 237.77234597083287
At time: 24.58311629295349 and batch: 350, loss is 5.5256789016723635 and perplexity is 251.05672300608143
At time: 24.908072233200073 and batch: 400, loss is 5.472267093658448 and perplexity is 237.99914795842471
At time: 25.233647346496582 and batch: 450, loss is 5.462366647720337 and perplexity is 235.65447606122808
At time: 25.557990550994873 and batch: 500, loss is 5.454067163467407 and perplexity is 233.70675915105372
At time: 25.881574392318726 and batch: 550, loss is 5.5108542728424075 and perplexity is 247.3623517723668
At time: 26.207257747650146 and batch: 600, loss is 5.534691104888916 and perplexity is 253.32952329873535
At time: 26.532586097717285 and batch: 650, loss is 5.485496892929077 and perplexity is 241.1687492765943
At time: 26.864521980285645 and batch: 700, loss is 5.471912956237793 and perplexity is 237.91487847641784
At time: 27.19010829925537 and batch: 750, loss is 5.397370252609253 and perplexity is 220.82493816416314
At time: 27.516273260116577 and batch: 800, loss is 5.460913810729981 and perplexity is 235.31235710317878
At time: 27.84152603149414 and batch: 850, loss is 5.409627828598023 and perplexity is 223.54837388489054
At time: 28.16652512550354 and batch: 900, loss is 5.485356969833374 and perplexity is 241.1350065593565
At time: 28.491636514663696 and batch: 950, loss is 5.451910924911499 and perplexity is 233.20337452989514
At time: 28.831597805023193 and batch: 1000, loss is 5.393187808990478 and perplexity is 219.90327904785244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.514301951338605 and perplexity of 248.21664945900662
Finished 4 epochs...
Completing Train Step...
At time: 29.954210996627808 and batch: 50, loss is 5.484139623641968 and perplexity is 240.8416403778378
At time: 30.27809762954712 and batch: 100, loss is 5.462524490356445 and perplexity is 235.69167532067763
At time: 30.602542877197266 and batch: 150, loss is 5.467192726135254 and perplexity is 236.79451177969565
At time: 30.92825436592102 and batch: 200, loss is 5.44982834815979 and perplexity is 232.71821596899844
At time: 31.253262758255005 and batch: 250, loss is 5.4387516689300535 and perplexity is 230.1546948010484
At time: 31.57840061187744 and batch: 300, loss is 5.318166275024414 and perplexity is 204.00944157924226
At time: 31.903223991394043 and batch: 350, loss is 5.37556170463562 and perplexity is 216.0612008101065
At time: 32.22787427902222 and batch: 400, loss is 5.329029407501221 and perplexity is 206.23770421310323
At time: 32.553539514541626 and batch: 450, loss is 5.320933656692505 and perplexity is 204.57479548219084
At time: 32.87946581840515 and batch: 500, loss is 5.31515326499939 and perplexity is 203.39568417948288
At time: 33.20521426200867 and batch: 550, loss is 5.367009334564209 and perplexity is 214.2212446705992
At time: 33.53128790855408 and batch: 600, loss is 5.390181980133057 and perplexity is 219.24327984509821
At time: 33.85660791397095 and batch: 650, loss is 5.357029972076416 and perplexity is 212.09408472076316
At time: 34.18213200569153 and batch: 700, loss is 5.34466064453125 and perplexity is 209.4867820499141
At time: 34.50504446029663 and batch: 750, loss is 5.276359882354736 and perplexity is 195.65636532161844
At time: 34.82927918434143 and batch: 800, loss is 5.333171033859253 and perplexity is 207.09363497410632
At time: 35.15479373931885 and batch: 850, loss is 5.289945230484009 and perplexity is 198.33256253345147
At time: 35.48002338409424 and batch: 900, loss is 5.375225086212158 and perplexity is 215.98848286910436
At time: 35.80487275123596 and batch: 950, loss is 5.346484394073486 and perplexity is 209.86918206772035
At time: 36.12833499908447 and batch: 1000, loss is 5.300355138778687 and perplexity is 200.4079699799029
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.428835240806022 and perplexity of 227.8836611936599
Finished 5 epochs...
Completing Train Step...
At time: 37.23936414718628 and batch: 50, loss is 5.388589735031128 and perplexity is 218.89446857684428
At time: 37.57884979248047 and batch: 100, loss is 5.374024391174316 and perplexity is 215.72930219910245
At time: 37.90282130241394 and batch: 150, loss is 5.387427082061768 and perplexity is 218.64011816226102
At time: 38.23002648353577 and batch: 200, loss is 5.382591218948364 and perplexity is 217.58535687584464
At time: 38.55246877670288 and batch: 250, loss is 5.371399507522583 and perplexity is 215.16378041970134
At time: 38.87528681755066 and batch: 300, loss is 5.255175199508667 and perplexity is 191.55504326413293
At time: 39.19810891151428 and batch: 350, loss is 5.316689529418945 and perplexity is 203.7083938730645
At time: 39.523215770721436 and batch: 400, loss is 5.272022132873535 and perplexity is 194.80949510776335
At time: 39.84735107421875 and batch: 450, loss is 5.269963531494141 and perplexity is 194.40887251500763
At time: 40.170602321624756 and batch: 500, loss is 5.2609230899810795 and perplexity is 192.65925106491332
At time: 40.494657039642334 and batch: 550, loss is 5.315822925567627 and perplexity is 203.5319358650262
At time: 40.819217681884766 and batch: 600, loss is 5.341046447753906 and perplexity is 208.73102215249688
At time: 41.143821239471436 and batch: 650, loss is 5.3149738788604735 and perplexity is 203.35920108540415
At time: 41.46882200241089 and batch: 700, loss is 5.305233421325684 and perplexity is 201.3880051829051
At time: 41.792269229888916 and batch: 750, loss is 5.236978387832641 and perplexity is 188.10087495401092
At time: 42.11718797683716 and batch: 800, loss is 5.287566747665405 and perplexity is 197.86139249813726
At time: 42.44200897216797 and batch: 850, loss is 5.251252603530884 and perplexity is 190.8051220025345
At time: 42.76552867889404 and batch: 900, loss is 5.335367364883423 and perplexity is 207.54898101177847
At time: 43.08945083618164 and batch: 950, loss is 5.306239080429077 and perplexity is 201.59063473468072
At time: 43.414084911346436 and batch: 1000, loss is 5.263066082000733 and perplexity is 193.0725610042344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.402181113638529 and perplexity of 221.88985578490556
Finished 6 epochs...
Completing Train Step...
At time: 44.529645919799805 and batch: 50, loss is 5.35035590171814 and perplexity is 210.683267061781
At time: 44.86935257911682 and batch: 100, loss is 5.3340581607818605 and perplexity is 207.2774348280377
At time: 45.19582533836365 and batch: 150, loss is 5.353704099655151 and perplexity is 211.3898585859625
At time: 45.52028751373291 and batch: 200, loss is 5.351784629821777 and perplexity is 210.98449129892506
At time: 45.84373092651367 and batch: 250, loss is 5.3370340633392335 and perplexity is 207.89519101162767
At time: 46.18225383758545 and batch: 300, loss is 5.225357418060303 and perplexity is 185.92761253808925
At time: 46.50634026527405 and batch: 350, loss is 5.282708234786988 and perplexity is 196.90241187081327
At time: 46.83252716064453 and batch: 400, loss is 5.240070514678955 and perplexity is 188.68340688644054
At time: 47.15727138519287 and batch: 450, loss is 5.239455089569092 and perplexity is 188.56732210443695
At time: 47.48240566253662 and batch: 500, loss is 5.2301052284240725 and perplexity is 186.81246046672723
At time: 47.807594537734985 and batch: 550, loss is 5.286033878326416 and perplexity is 197.558329173788
At time: 48.13416337966919 and batch: 600, loss is 5.3131856441497805 and perplexity is 202.99587205889335
At time: 48.45893168449402 and batch: 650, loss is 5.284788465499878 and perplexity is 197.31244064479162
At time: 48.78515434265137 and batch: 700, loss is 5.272924251556397 and perplexity is 194.98531568648954
At time: 49.109357833862305 and batch: 750, loss is 5.209854707717896 and perplexity is 183.0674579726137
At time: 49.434003591537476 and batch: 800, loss is 5.25981556892395 and perplexity is 192.44599500209662
At time: 49.75860905647278 and batch: 850, loss is 5.226849031448364 and perplexity is 186.20515159318828
At time: 50.0822389125824 and batch: 900, loss is 5.311694059371948 and perplexity is 202.69331220909598
At time: 50.40802454948425 and batch: 950, loss is 5.284340705871582 and perplexity is 197.22411187621444
At time: 50.73270535469055 and batch: 1000, loss is 5.238603887557983 and perplexity is 188.406881513996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.370025634765625 and perplexity of 214.86837573418876
Finished 7 epochs...
Completing Train Step...
At time: 51.85161375999451 and batch: 50, loss is 5.311931648254395 and perplexity is 202.74147560794054
At time: 52.17711544036865 and batch: 100, loss is 5.296683950424194 and perplexity is 199.67358343484642
At time: 52.50195121765137 and batch: 150, loss is 5.310398654937744 and perplexity is 202.43091238731694
At time: 52.82617521286011 and batch: 200, loss is 5.312415008544922 and perplexity is 202.83949647428108
At time: 53.14863896369934 and batch: 250, loss is 5.300673093795776 and perplexity is 200.47170083065765
At time: 53.473244190216064 and batch: 300, loss is 5.184315462112426 and perplexity is 178.45125132724033
At time: 53.79794788360596 and batch: 350, loss is 5.240975799560547 and perplexity is 188.854296462306
At time: 54.12085318565369 and batch: 400, loss is 5.201842260360718 and perplexity is 181.6065003418807
At time: 54.459413290023804 and batch: 450, loss is 5.201658325195313 and perplexity is 181.573099592082
At time: 54.78529334068298 and batch: 500, loss is 5.187155771255493 and perplexity is 178.95882854484896
At time: 55.11206388473511 and batch: 550, loss is 5.24606164932251 and perplexity is 189.81722762704527
At time: 55.43755626678467 and batch: 600, loss is 5.274615221023559 and perplexity is 195.31530882739202
At time: 55.76383352279663 and batch: 650, loss is 5.245551519393921 and perplexity is 189.72042087238168
At time: 56.09019637107849 and batch: 700, loss is 5.236418714523316 and perplexity is 187.99562936915797
At time: 56.414982318878174 and batch: 750, loss is 5.168635330200195 and perplexity is 175.67493554870757
At time: 56.740216970443726 and batch: 800, loss is 5.222267456054688 and perplexity is 185.35398997192522
At time: 57.06714081764221 and batch: 850, loss is 5.186615610122681 and perplexity is 178.86218804436635
At time: 57.39719319343567 and batch: 900, loss is 5.271735963821411 and perplexity is 194.75375463518304
At time: 57.72223687171936 and batch: 950, loss is 5.243874654769898 and perplexity is 189.40255199618682
At time: 58.04674530029297 and batch: 1000, loss is 5.194527368545533 and perplexity is 180.28291528152883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.345274018078316 and perplexity of 209.6153151158449
Finished 8 epochs...
Completing Train Step...
At time: 59.152952432632446 and batch: 50, loss is 5.28261435508728 and perplexity is 196.8839275991777
At time: 59.49234342575073 and batch: 100, loss is 5.2599050903320315 and perplexity is 192.46322380971085
At time: 59.81883120536804 and batch: 150, loss is 5.281876792907715 and perplexity is 196.73876699948104
At time: 60.143590450286865 and batch: 200, loss is 5.286299848556519 and perplexity is 197.61088079633092
At time: 60.46780061721802 and batch: 250, loss is 5.271282100677491 and perplexity is 194.66538313961345
At time: 60.79266095161438 and batch: 300, loss is 5.156925859451294 and perplexity is 173.6298717040153
At time: 61.115553855895996 and batch: 350, loss is 5.21617733001709 and perplexity is 184.22859119999006
At time: 61.43936324119568 and batch: 400, loss is 5.171560621261596 and perplexity is 176.18958825485663
At time: 61.765347719192505 and batch: 450, loss is 5.172615346908569 and perplexity is 176.3755179675004
At time: 62.08926296234131 and batch: 500, loss is 5.161325807571411 and perplexity is 174.39551729684692
At time: 62.414761543273926 and batch: 550, loss is 5.220013933181763 and perplexity is 184.93676081023528
At time: 62.73822093009949 and batch: 600, loss is 5.245872344970703 and perplexity is 189.78129780075113
At time: 63.07735323905945 and batch: 650, loss is 5.218407163619995 and perplexity is 184.63984865070208
At time: 63.40200972557068 and batch: 700, loss is 5.212000045776367 and perplexity is 183.4606211406301
At time: 63.727447271347046 and batch: 750, loss is 5.1414511775970455 and perplexity is 170.9636870633273
At time: 64.05220222473145 and batch: 800, loss is 5.196838083267212 and perplexity is 180.6999793403948
At time: 64.37650632858276 and batch: 850, loss is 5.162808780670166 and perplexity is 174.6543330184854
At time: 64.70161533355713 and batch: 900, loss is 5.251161842346192 and perplexity is 190.78780508947983
At time: 65.02660465240479 and batch: 950, loss is 5.223067312240601 and perplexity is 185.50230581515294
At time: 65.35083770751953 and batch: 1000, loss is 5.179806928634644 and perplexity is 177.64850884193382
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.333644215653583 and perplexity of 207.19165109979184
Finished 9 epochs...
Completing Train Step...
At time: 66.46149563789368 and batch: 50, loss is 5.265628528594971 and perplexity is 193.5679335424004
At time: 66.80268263816833 and batch: 100, loss is 5.24342493057251 and perplexity is 189.31739223614477
At time: 67.12730765342712 and batch: 150, loss is 5.268503217697144 and perplexity is 194.1251817454024
At time: 67.45274758338928 and batch: 200, loss is 5.273402051925659 and perplexity is 195.07850200278065
At time: 67.77634835243225 and batch: 250, loss is 5.256642770767212 and perplexity is 191.83637032331723
At time: 68.10021638870239 and batch: 300, loss is 5.142513475418091 and perplexity is 171.1453979140049
At time: 68.42315721511841 and batch: 350, loss is 5.202762937545776 and perplexity is 181.77377829608565
At time: 68.74738836288452 and batch: 400, loss is 5.156454219818115 and perplexity is 173.54800028343686
At time: 69.0724925994873 and batch: 450, loss is 5.160713768005371 and perplexity is 174.28881299707868
At time: 69.39660310745239 and batch: 500, loss is 5.148302774429322 and perplexity is 172.13908339274434
At time: 69.7217652797699 and batch: 550, loss is 5.204758768081665 and perplexity is 182.13693022774362
At time: 70.04696798324585 and batch: 600, loss is 5.233181304931641 and perplexity is 187.38799462742503
At time: 70.37377142906189 and batch: 650, loss is 5.206591186523437 and perplexity is 182.47098727028987
At time: 70.69995141029358 and batch: 700, loss is 5.200614795684815 and perplexity is 181.38372153232746
At time: 71.02470517158508 and batch: 750, loss is 5.127991571426391 and perplexity is 168.67799989686014
At time: 71.36466407775879 and batch: 800, loss is 5.182765684127808 and perplexity is 178.17490569910186
At time: 71.69027066230774 and batch: 850, loss is 5.148747568130493 and perplexity is 172.21566680340956
At time: 72.01379752159119 and batch: 900, loss is 5.237872772216797 and perplexity is 188.2691846947981
At time: 72.3385956287384 and batch: 950, loss is 5.212442636489868 and perplexity is 183.5418370792244
At time: 72.6640977859497 and batch: 1000, loss is 5.167548809051514 and perplexity is 175.48416467300368
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.326079950100038 and perplexity of 205.63031107043346
Finished 10 epochs...
Completing Train Step...
At time: 73.79004502296448 and batch: 50, loss is 5.2541616249084475 and perplexity is 191.36098629995487
At time: 74.11543679237366 and batch: 100, loss is 5.230035762786866 and perplexity is 186.7994838708417
At time: 74.4424295425415 and batch: 150, loss is 5.250734024047851 and perplexity is 190.7062000326753
At time: 74.76869177818298 and batch: 200, loss is 5.258936491012573 and perplexity is 192.27689431598603
At time: 75.09402585029602 and batch: 250, loss is 5.244218664169312 and perplexity is 189.46771946281433
At time: 75.41828799247742 and batch: 300, loss is 5.129527235031128 and perplexity is 168.9372315575902
At time: 75.74278593063354 and batch: 350, loss is 5.186789178848267 and perplexity is 178.89323562076692
At time: 76.0681836605072 and batch: 400, loss is 5.14151593208313 and perplexity is 170.97475808746756
At time: 76.39338898658752 and batch: 450, loss is 5.144874610900879 and perplexity is 171.54997282586805
At time: 76.71898436546326 and batch: 500, loss is 5.1329323673248295 and perplexity is 169.51346570033695
At time: 77.0444712638855 and batch: 550, loss is 5.186261997222901 and perplexity is 178.7989512487232
At time: 77.3707025051117 and batch: 600, loss is 5.218653964996338 and perplexity is 184.6854236432309
At time: 77.69689059257507 and batch: 650, loss is 5.188940238952637 and perplexity is 179.27845989459777
At time: 78.02169823646545 and batch: 700, loss is 5.184106006622314 and perplexity is 178.4138776471303
At time: 78.34735035896301 and batch: 750, loss is 5.1077027511596675 and perplexity is 165.29020565897167
At time: 78.67405939102173 and batch: 800, loss is 5.163483180999756 and perplexity is 174.77215968494295
At time: 78.99969577789307 and batch: 850, loss is 5.128667640686035 and perplexity is 168.79207646485395
At time: 79.32450318336487 and batch: 900, loss is 5.222797117233276 and perplexity is 185.4521907889943
At time: 79.6503164768219 and batch: 950, loss is 5.192913017272949 and perplexity is 179.9921101218152
At time: 79.990797996521 and batch: 1000, loss is 5.144958982467651 and perplexity is 171.56444737646675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.307202222870617 and perplexity of 201.78488876294367
Finished 11 epochs...
Completing Train Step...
At time: 81.10362434387207 and batch: 50, loss is 5.2389413928985595 and perplexity is 188.47048057461595
At time: 81.44560408592224 and batch: 100, loss is 5.207782917022705 and perplexity is 182.68857313715657
At time: 81.7707006931305 and batch: 150, loss is 5.231904487609864 and perplexity is 187.14888707081067
At time: 82.09739208221436 and batch: 200, loss is 5.240131168365479 and perplexity is 188.69485157773195
At time: 82.42038130760193 and batch: 250, loss is 5.2257304191589355 and perplexity is 185.99697667747802
At time: 82.74485802650452 and batch: 300, loss is 5.110435104370117 and perplexity is 165.74245445341768
At time: 83.06913352012634 and batch: 350, loss is 5.16721471786499 and perplexity is 175.42554675262247
At time: 83.39446711540222 and batch: 400, loss is 5.116646556854248 and perplexity is 166.77515981366614
At time: 83.71846222877502 and batch: 450, loss is 5.120130853652954 and perplexity is 167.3572674981648
At time: 84.04230833053589 and batch: 500, loss is 5.108168249130249 and perplexity is 165.36716582527382
At time: 84.36753010749817 and batch: 550, loss is 5.161456699371338 and perplexity is 174.41834573400007
At time: 84.69187521934509 and batch: 600, loss is 5.1950085163116455 and perplexity is 180.36967887486557
At time: 85.01655054092407 and batch: 650, loss is 5.165867948532105 and perplexity is 175.1894480269991
At time: 85.34089350700378 and batch: 700, loss is 5.157160005569458 and perplexity is 173.6705312244207
At time: 85.66479349136353 and batch: 750, loss is 5.080755195617676 and perplexity is 160.89551764898732
At time: 85.99038171768188 and batch: 800, loss is 5.135939645767212 and perplexity is 170.02400717685293
At time: 86.31567120552063 and batch: 850, loss is 5.104719038009644 and perplexity is 164.79776211924272
At time: 86.63963437080383 and batch: 900, loss is 5.199230642318725 and perplexity is 181.13283231824587
At time: 86.96481800079346 and batch: 950, loss is 5.164768695831299 and perplexity is 174.99697635998376
At time: 87.29162836074829 and batch: 1000, loss is 5.119163370132446 and perplexity is 167.19543039980948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.283742765100991 and perplexity of 197.10621878883433
Finished 12 epochs...
Completing Train Step...
At time: 88.3984706401825 and batch: 50, loss is 5.211449995040893 and perplexity is 183.3597362394918
At time: 88.73900103569031 and batch: 100, loss is 5.180323314666748 and perplexity is 177.7402677399916
At time: 89.06379842758179 and batch: 150, loss is 5.2065284633636475 and perplexity is 182.45954247232905
At time: 89.38930583000183 and batch: 200, loss is 5.21435212135315 and perplexity is 183.89264226090808
At time: 89.71200203895569 and batch: 250, loss is 5.201246862411499 and perplexity is 181.49840438726048
At time: 90.03707003593445 and batch: 300, loss is 5.087368392944336 and perplexity is 161.96307755758912
At time: 90.36066007614136 and batch: 350, loss is 5.1439688110351565 and perplexity is 171.39465323836805
At time: 90.68425273895264 and batch: 400, loss is 5.095156307220459 and perplexity is 163.22935655750675
At time: 91.00828838348389 and batch: 450, loss is 5.104523286819458 and perplexity is 164.76550591836588
At time: 91.33455753326416 and batch: 500, loss is 5.095685338973999 and perplexity is 163.3157329161392
At time: 91.66047811508179 and batch: 550, loss is 5.14702898979187 and perplexity is 171.91995486371917
At time: 91.98605751991272 and batch: 600, loss is 5.176553325653076 and perplexity is 177.07145039249883
At time: 92.31148362159729 and batch: 650, loss is 5.146711854934693 and perplexity is 171.86544169785688
At time: 92.63624858856201 and batch: 700, loss is 5.139849100112915 and perplexity is 170.69000927472706
At time: 92.9618272781372 and batch: 750, loss is 5.06445345878601 and perplexity is 158.2939042849866
At time: 93.28693890571594 and batch: 800, loss is 5.121477127075195 and perplexity is 167.58272787101734
At time: 93.61109018325806 and batch: 850, loss is 5.085516529083252 and perplexity is 161.6634215341472
At time: 93.93443036079407 and batch: 900, loss is 5.181485681533814 and perplexity is 177.94698725683963
At time: 94.26054883003235 and batch: 950, loss is 5.146209688186645 and perplexity is 171.77915825404912
At time: 94.58580422401428 and batch: 1000, loss is 5.101432523727417 and perplexity is 164.2570409509819
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.279738635551639 and perplexity of 196.3185579542161
Finished 13 epochs...
Completing Train Step...
At time: 95.70259380340576 and batch: 50, loss is 5.194883480072021 and perplexity is 180.34712753837334
At time: 96.02774715423584 and batch: 100, loss is 5.164747314453125 and perplexity is 174.99323472345378
At time: 96.35216689109802 and batch: 150, loss is 5.188893785476685 and perplexity is 179.2701319804041
At time: 96.67598843574524 and batch: 200, loss is 5.199355297088623 and perplexity is 181.15541279713236
At time: 97.0149929523468 and batch: 250, loss is 5.183026943206787 and perplexity is 178.22146159217215
At time: 97.34100389480591 and batch: 300, loss is 5.07164381980896 and perplexity is 159.43619642949022
At time: 97.66493916511536 and batch: 350, loss is 5.127957553863525 and perplexity is 168.67226197999005
At time: 97.9888391494751 and batch: 400, loss is 5.074560556411743 and perplexity is 159.90190866922785
At time: 98.31371665000916 and batch: 450, loss is 5.0862389659881595 and perplexity is 161.78025535348706
At time: 98.63736844062805 and batch: 500, loss is 5.0751408767700195 and perplexity is 159.99472993258524
At time: 98.96181106567383 and batch: 550, loss is 5.124464292526245 and perplexity is 168.08407363447918
At time: 99.28614449501038 and batch: 600, loss is 5.157063446044922 and perplexity is 173.65376249010328
At time: 99.6104805469513 and batch: 650, loss is 5.129796113967895 and perplexity is 168.98266132807225
At time: 99.93603277206421 and batch: 700, loss is 5.121713762283325 and perplexity is 167.62238853707595
At time: 100.26278686523438 and batch: 750, loss is 5.049665575027466 and perplexity is 155.97029540180296
At time: 100.5883400440216 and batch: 800, loss is 5.101200208663941 and perplexity is 164.21888599824368
At time: 100.91260266304016 and batch: 850, loss is 5.067720651626587 and perplexity is 158.81192677465356
At time: 101.23630666732788 and batch: 900, loss is 5.165799980163574 and perplexity is 175.17754109068497
At time: 101.55929660797119 and batch: 950, loss is 5.1234739303588865 and perplexity is 167.9176919296882
At time: 101.88443756103516 and batch: 1000, loss is 5.080885219573974 and perplexity is 160.91643928086998
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.258530035251525 and perplexity of 192.19875814509894
Finished 14 epochs...
Completing Train Step...
At time: 102.99504899978638 and batch: 50, loss is 5.173828477859497 and perplexity is 176.58961440458302
At time: 103.33495020866394 and batch: 100, loss is 5.143758602142334 and perplexity is 171.35862834458436
At time: 103.66006731987 and batch: 150, loss is 5.16790880203247 and perplexity is 175.54734911285004
At time: 103.98479342460632 and batch: 200, loss is 5.181149415969848 and perplexity is 177.88715987232212
At time: 104.30999636650085 and batch: 250, loss is 5.160095643997193 and perplexity is 174.18111418644767
At time: 104.63949203491211 and batch: 300, loss is 5.049455041885376 and perplexity is 155.9374619418264
At time: 104.96424007415771 and batch: 350, loss is 5.107946834564209 and perplexity is 165.3305551792301
At time: 105.28935146331787 and batch: 400, loss is 5.052521915435791 and perplexity is 156.4164365211842
At time: 105.62965679168701 and batch: 450, loss is 5.0638453483581545 and perplexity is 158.19767337360074
At time: 105.95647835731506 and batch: 500, loss is 5.049370632171631 and perplexity is 155.92429986081333
At time: 106.28244876861572 and batch: 550, loss is 5.101239013671875 and perplexity is 164.22525863706213
At time: 106.607754945755 and batch: 600, loss is 5.131692342758178 and perplexity is 169.30339511175376
At time: 106.93364596366882 and batch: 650, loss is 5.105664348602295 and perplexity is 164.9536208452651
At time: 107.25926089286804 and batch: 700, loss is 5.0942424774169925 and perplexity is 163.08026084111407
At time: 107.58489465713501 and batch: 750, loss is 5.026705379486084 and perplexity is 152.42998555151675
At time: 107.91035056114197 and batch: 800, loss is 5.075446138381958 and perplexity is 160.0435776370313
At time: 108.23668074607849 and batch: 850, loss is 5.044752759933472 and perplexity is 155.20592133196678
At time: 108.56096363067627 and batch: 900, loss is 5.146072731018067 and perplexity is 171.75563347789398
At time: 108.88562297821045 and batch: 950, loss is 5.098256216049195 and perplexity is 163.73613776295136
At time: 109.21056580543518 and batch: 1000, loss is 5.057301330566406 and perplexity is 157.16580494950006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.240102256216654 and perplexity of 188.68939608296617
Finished 15 epochs...
Completing Train Step...
At time: 110.32111310958862 and batch: 50, loss is 5.149643907546997 and perplexity is 172.37009969550954
At time: 110.66044569015503 and batch: 100, loss is 5.120507688522339 and perplexity is 167.42034543643987
At time: 110.9856116771698 and batch: 150, loss is 5.145239276885986 and perplexity is 171.6125426735579
At time: 111.31040835380554 and batch: 200, loss is 5.159543724060058 and perplexity is 174.08500668111972
At time: 111.63539052009583 and batch: 250, loss is 5.141854486465454 and perplexity is 171.03265214067463
At time: 111.95877170562744 and batch: 300, loss is 5.028720531463623 and perplexity is 152.73746484303638
At time: 112.28281569480896 and batch: 350, loss is 5.086232233047485 and perplexity is 161.77916610029243
At time: 112.60727572441101 and batch: 400, loss is 5.033885526657104 and perplexity is 153.5283939289667
At time: 112.93436813354492 and batch: 450, loss is 5.047434110641479 and perplexity is 155.6226412755028
At time: 113.26179361343384 and batch: 500, loss is 5.034858627319336 and perplexity is 153.67786522428372
At time: 113.58758187294006 and batch: 550, loss is 5.08771782875061 and perplexity is 162.0196831456157
At time: 113.92656826972961 and batch: 600, loss is 5.117213468551636 and perplexity is 166.86973340750362
At time: 114.25062870979309 and batch: 650, loss is 5.09316330909729 and perplexity is 162.9043647178781
At time: 114.57802844047546 and batch: 700, loss is 5.080804748535156 and perplexity is 160.90349068883842
At time: 114.90322399139404 and batch: 750, loss is 5.014658975601196 and perplexity is 150.60476810693098
At time: 115.22817897796631 and batch: 800, loss is 5.062828998565674 and perplexity is 158.03697087990304
At time: 115.55257439613342 and batch: 850, loss is 5.029549512863159 and perplexity is 152.86413385627748
At time: 115.87829446792603 and batch: 900, loss is 5.133348407745362 and perplexity is 169.58400482643916
At time: 116.20391345024109 and batch: 950, loss is 5.088837194442749 and perplexity is 162.201143961939
At time: 116.53094553947449 and batch: 1000, loss is 5.045772619247437 and perplexity is 155.36429027971613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.236187446408156 and perplexity of 187.95215700137553
Finished 16 epochs...
Completing Train Step...
At time: 117.66920495033264 and batch: 50, loss is 5.1383457946777344 and perplexity is 170.43360283294044
At time: 117.99506568908691 and batch: 100, loss is 5.107190656661987 and perplexity is 165.20558312334998
At time: 118.32108092308044 and batch: 150, loss is 5.1325590038299564 and perplexity is 169.45018737399863
At time: 118.64660954475403 and batch: 200, loss is 5.148013153076172 and perplexity is 172.08923545734385
At time: 118.97208786010742 and batch: 250, loss is 5.128986091613769 and perplexity is 168.84583701779303
At time: 119.29564428329468 and batch: 300, loss is 5.013251581192017 and perplexity is 150.39295688422908
At time: 119.62087821960449 and batch: 350, loss is 5.074921770095825 and perplexity is 159.95967785963288
At time: 119.9469997882843 and batch: 400, loss is 5.020371837615967 and perplexity is 151.4676146808901
At time: 120.27342247962952 and batch: 450, loss is 5.031821060180664 and perplexity is 153.21176665217388
At time: 120.5970151424408 and batch: 500, loss is 5.0225629806518555 and perplexity is 151.79986576185084
At time: 120.92362785339355 and batch: 550, loss is 5.0717409896850585 and perplexity is 159.45168957766433
At time: 121.24987244606018 and batch: 600, loss is 5.104333448410034 and perplexity is 164.73423006556732
At time: 121.57630133628845 and batch: 650, loss is 5.07896162033081 and perplexity is 160.6071980634979
At time: 121.90277767181396 and batch: 700, loss is 5.067574453353882 and perplexity is 158.78871044240935
At time: 122.24216675758362 and batch: 750, loss is 5.002208051681518 and perplexity is 148.74122508812775
At time: 122.56727981567383 and batch: 800, loss is 5.047967052459716 and perplexity is 155.70560119334422
At time: 122.89197278022766 and batch: 850, loss is 5.014595441818237 and perplexity is 150.59519992023624
At time: 123.21911644935608 and batch: 900, loss is 5.123454532623291 and perplexity is 167.91443473828951
At time: 123.54376316070557 and batch: 950, loss is 5.075031127929687 and perplexity is 159.97717166003355
At time: 123.86855340003967 and batch: 1000, loss is 5.034987621307373 and perplexity is 153.69769002360448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.230284900200076 and perplexity of 186.84602840879452
Finished 17 epochs...
Completing Train Step...
At time: 124.98017311096191 and batch: 50, loss is 5.1258558082580565 and perplexity is 168.31812807465946
At time: 125.32140779495239 and batch: 100, loss is 5.096737461090088 and perplexity is 163.48765143442432
At time: 125.64809203147888 and batch: 150, loss is 5.1212460231781005 and perplexity is 167.54400332438973
At time: 125.97636914253235 and batch: 200, loss is 5.133788452148438 and perplexity is 169.65864574006795
At time: 126.3023910522461 and batch: 250, loss is 5.115476903915405 and perplexity is 166.5802047949456
At time: 126.62851405143738 and batch: 300, loss is 5.003036823272705 and perplexity is 148.86454868640763
At time: 126.95459485054016 and batch: 350, loss is 5.062750148773193 and perplexity is 158.02451018881274
At time: 127.28078293800354 and batch: 400, loss is 5.009922323226928 and perplexity is 149.89309248510148
At time: 127.60634589195251 and batch: 450, loss is 5.023476467132569 and perplexity is 151.93859624156013
At time: 127.93203949928284 and batch: 500, loss is 5.011571712493897 and perplexity is 150.14052854613527
At time: 128.2580258846283 and batch: 550, loss is 5.061705827713013 and perplexity is 157.859568006086
At time: 128.58618259429932 and batch: 600, loss is 5.092658386230469 and perplexity is 162.82213134153037
At time: 128.91355299949646 and batch: 650, loss is 5.067535524368286 and perplexity is 158.78252907930562
At time: 129.2390842437744 and batch: 700, loss is 5.057976531982422 and perplexity is 157.2719593573215
At time: 129.56518816947937 and batch: 750, loss is 4.989928503036499 and perplexity is 146.92591836313412
At time: 129.89048862457275 and batch: 800, loss is 5.037902317047119 and perplexity is 154.14632552630596
At time: 130.21780586242676 and batch: 850, loss is 5.004178171157837 and perplexity is 149.03455192218385
At time: 130.54367637634277 and batch: 900, loss is 5.1129039478302 and perplexity is 166.152152159806
At time: 130.88365960121155 and batch: 950, loss is 5.063830127716065 and perplexity is 158.19526552175944
At time: 131.2101595401764 and batch: 1000, loss is 5.024048337936401 and perplexity is 152.02551033817446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.227788413443217 and perplexity of 186.38015154297577
Finished 18 epochs...
Completing Train Step...
At time: 132.3180603981018 and batch: 50, loss is 5.120489587783814 and perplexity is 167.41731503196974
At time: 132.65934133529663 and batch: 100, loss is 5.089111480712891 and perplexity is 162.2456396107229
At time: 132.9861204624176 and batch: 150, loss is 5.1139778137207035 and perplexity is 166.3306731252762
At time: 133.3128867149353 and batch: 200, loss is 5.125809640884399 and perplexity is 168.3103574481234
At time: 133.63772678375244 and batch: 250, loss is 5.105481777191162 and perplexity is 164.92350777891215
At time: 133.96355986595154 and batch: 300, loss is 4.989120645523071 and perplexity is 146.8072710875665
At time: 134.28928804397583 and batch: 350, loss is 5.043191804885864 and perplexity is 154.96384085317175
At time: 134.6152377128601 and batch: 400, loss is 4.98812388420105 and perplexity is 146.66101218267406
At time: 134.94140315055847 and batch: 450, loss is 5.006411151885986 and perplexity is 149.36771503942967
At time: 135.2690553665161 and batch: 500, loss is 4.993595752716065 and perplexity is 147.46572158178984
At time: 135.5945565700531 and batch: 550, loss is 5.043267879486084 and perplexity is 154.9756301138392
At time: 135.92335844039917 and batch: 600, loss is 5.075128726959228 and perplexity is 159.99278603869797
At time: 136.24691462516785 and batch: 650, loss is 5.048186130523682 and perplexity is 155.73971661183572
At time: 136.57245802879333 and batch: 700, loss is 5.036622819900512 and perplexity is 153.94922186631047
At time: 136.89707803726196 and batch: 750, loss is 4.968821887969971 and perplexity is 143.85730747393256
At time: 137.22382974624634 and batch: 800, loss is 5.018583459854126 and perplexity is 151.196975441886
At time: 137.54942083358765 and batch: 850, loss is 4.985333509445191 and perplexity is 146.25234343125888
At time: 137.87462902069092 and batch: 900, loss is 5.096549196243286 and perplexity is 163.45687535389095
At time: 138.19920539855957 and batch: 950, loss is 5.041250324249267 and perplexity is 154.6632734241163
At time: 138.5257806777954 and batch: 1000, loss is 5.00003511428833 and perplexity is 148.41837061653612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.206080739091083 and perplexity of 182.3778691913226
Finished 19 epochs...
Completing Train Step...
At time: 139.6427927017212 and batch: 50, loss is 5.09836974143982 and perplexity is 163.7547270271071
At time: 139.96819925308228 and batch: 100, loss is 5.064108219146728 and perplexity is 158.23926438704277
At time: 140.2938506603241 and batch: 150, loss is 5.086777992248535 and perplexity is 161.86748266632705
At time: 140.6182222366333 and batch: 200, loss is 5.101773071289062 and perplexity is 164.3129878115036
At time: 140.94269108772278 and batch: 250, loss is 5.080964403152466 and perplexity is 160.92918172485983
At time: 141.2673032283783 and batch: 300, loss is 4.968548412322998 and perplexity is 143.8179713826761
At time: 141.59242725372314 and batch: 350, loss is 5.026984844207764 and perplexity is 152.47259030799046
At time: 141.91792511940002 and batch: 400, loss is 4.972619180679321 and perplexity is 144.40461426317617
At time: 142.24402737617493 and batch: 450, loss is 4.99070785522461 and perplexity is 147.04047003134917
At time: 142.56765866279602 and batch: 500, loss is 4.973202753067016 and perplexity is 144.4889094024815
At time: 142.89322090148926 and batch: 550, loss is 5.02521409034729 and perplexity is 152.20283778323403
At time: 143.21954941749573 and batch: 600, loss is 5.056510963439941 and perplexity is 157.04163534011616
At time: 143.54531407356262 and batch: 650, loss is 5.032121639251709 and perplexity is 153.25782582453226
At time: 143.8709089756012 and batch: 700, loss is 5.022980241775513 and perplexity is 151.86321916094573
At time: 144.19688177108765 and batch: 750, loss is 4.953229236602783 and perplexity is 141.63158815941486
At time: 144.5225419998169 and batch: 800, loss is 4.996984930038452 and perplexity is 147.96635695301458
At time: 144.8457624912262 and batch: 850, loss is 4.967932462692261 and perplexity is 143.7294140325429
At time: 145.16979908943176 and batch: 900, loss is 5.080920400619507 and perplexity is 160.92210058903208
At time: 145.49567890167236 and batch: 950, loss is 5.024302291870117 and perplexity is 152.06412271722525
At time: 145.82197046279907 and batch: 1000, loss is 4.983219833374023 and perplexity is 145.9435398229631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.191698399985709 and perplexity of 179.7736213102995
Finished 20 epochs...
Completing Train Step...
At time: 146.92793583869934 and batch: 50, loss is 5.078953113555908 and perplexity is 160.60583182002745
At time: 147.2691831588745 and batch: 100, loss is 5.044618844985962 and perplexity is 155.18513833076673
At time: 147.593834400177 and batch: 150, loss is 5.06929084777832 and perplexity is 159.0614885301036
At time: 147.93304681777954 and batch: 200, loss is 5.084044618606567 and perplexity is 161.42564248795844
At time: 148.25716757774353 and batch: 250, loss is 5.0604374217987065 and perplexity is 157.6594649291884
At time: 148.58193588256836 and batch: 300, loss is 4.950725212097168 and perplexity is 141.27738284636277
At time: 148.90710067749023 and batch: 350, loss is 5.011619701385498 and perplexity is 150.14773379656918
At time: 149.23352575302124 and batch: 400, loss is 4.955824031829834 and perplexity is 141.9995703411074
At time: 149.5586805343628 and batch: 450, loss is 4.976084518432617 and perplexity is 144.90589307328875
At time: 149.88473272323608 and batch: 500, loss is 4.959305238723755 and perplexity is 142.49476165592853
At time: 150.21122097969055 and batch: 550, loss is 5.0083949089050295 and perplexity is 149.66431838977826
At time: 150.53684854507446 and batch: 600, loss is 5.040343837738037 and perplexity is 154.5231367785602
At time: 150.8616189956665 and batch: 650, loss is 5.016381778717041 and perplexity is 150.86445410037592
At time: 151.18575882911682 and batch: 700, loss is 5.004326267242432 and perplexity is 149.05662499022
At time: 151.5113124847412 and batch: 750, loss is 4.938562316894531 and perplexity is 139.5694486122153
At time: 151.83543229103088 and batch: 800, loss is 4.981723470687866 and perplexity is 145.72531866540334
At time: 152.15954494476318 and batch: 850, loss is 4.9541293811798095 and perplexity is 141.7591344618763
At time: 152.4843509197235 and batch: 900, loss is 5.065289573669434 and perplexity is 158.4263115204309
At time: 152.8115677833557 and batch: 950, loss is 5.007283687591553 and perplexity is 149.49810057880765
At time: 153.13699865341187 and batch: 1000, loss is 4.966925325393677 and perplexity is 143.58473164850946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.175789437642911 and perplexity of 176.93623928424861
Finished 21 epochs...
Completing Train Step...
At time: 154.24688148498535 and batch: 50, loss is 5.062125577926635 and perplexity is 157.92584350208267
At time: 154.5877504348755 and batch: 100, loss is 5.028744754791259 and perplexity is 152.74116469750078
At time: 154.91082000732422 and batch: 150, loss is 5.053789052963257 and perplexity is 156.61476328491852
At time: 155.23481678962708 and batch: 200, loss is 5.068099327087403 and perplexity is 158.87207634207178
At time: 155.5600085258484 and batch: 250, loss is 5.045527420043945 and perplexity is 155.3261997495632
At time: 155.88516974449158 and batch: 300, loss is 4.93331223487854 and perplexity is 138.83861769986822
At time: 156.2109775543213 and batch: 350, loss is 4.995410442352295 and perplexity is 147.73356905495635
At time: 156.55034279823303 and batch: 400, loss is 4.939817056655884 and perplexity is 139.74468186193192
At time: 156.87616324424744 and batch: 450, loss is 4.96137077331543 and perplexity is 142.78939369684517
At time: 157.20377731323242 and batch: 500, loss is 4.942583713531494 and perplexity is 140.13184277057343
At time: 157.52849626541138 and batch: 550, loss is 4.990389175415039 and perplexity is 146.9936186680487
At time: 157.8546278476715 and batch: 600, loss is 5.021701374053955 and perplexity is 151.66913032529519
At time: 158.17976427078247 and batch: 650, loss is 4.999002227783203 and perplexity is 148.26515042756802
At time: 158.50548434257507 and batch: 700, loss is 4.986147871017456 and perplexity is 146.3714942289336
At time: 158.83004641532898 and batch: 750, loss is 4.9197444725036625 and perplexity is 136.96760972409376
At time: 159.15519428253174 and batch: 800, loss is 4.961482038497925 and perplexity is 142.80528206868811
At time: 159.4779212474823 and batch: 850, loss is 4.928298530578613 and perplexity is 138.14426402183122
At time: 159.80387902259827 and batch: 900, loss is 5.04617377281189 and perplexity is 155.42662772115798
At time: 160.12910509109497 and batch: 950, loss is 4.985754308700561 and perplexity is 146.3138992588858
At time: 160.45455503463745 and batch: 1000, loss is 4.944246492385864 and perplexity is 140.36504486358035
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.15792065131955 and perplexity of 173.80268322997176
Finished 22 epochs...
Completing Train Step...
At time: 161.5797312259674 and batch: 50, loss is 5.040918006896972 and perplexity is 154.61188467375442
At time: 161.90577912330627 and batch: 100, loss is 5.004472742080688 and perplexity is 149.07845963433047
At time: 162.23060250282288 and batch: 150, loss is 5.033330163955688 and perplexity is 153.44315365719496
At time: 162.5559184551239 and batch: 200, loss is 5.044043941497803 and perplexity is 155.09594749394807
At time: 162.8799180984497 and batch: 250, loss is 5.024980621337891 and perplexity is 152.16730728526554
At time: 163.20485925674438 and batch: 300, loss is 4.910686063766479 and perplexity is 135.73250362491527
At time: 163.52971482276917 and batch: 350, loss is 4.975602540969849 and perplexity is 144.83606852688567
At time: 163.85392379760742 and batch: 400, loss is 4.918581466674805 and perplexity is 136.80840818972064
At time: 164.18077111244202 and batch: 450, loss is 4.940547704696655 and perplexity is 139.84682335013767
At time: 164.5067503452301 and batch: 500, loss is 4.923629684448242 and perplexity is 137.50079301175901
At time: 164.84652829170227 and batch: 550, loss is 4.970858364105225 and perplexity is 144.15056795511916
At time: 165.17252731323242 and batch: 600, loss is 5.00053370475769 and perplexity is 148.49238905251346
At time: 165.49988460540771 and batch: 650, loss is 4.9829328250885006 and perplexity is 145.90165882819912
At time: 165.8251953125 and batch: 700, loss is 4.969532690048218 and perplexity is 143.9595978968719
At time: 166.15066719055176 and batch: 750, loss is 4.905959062576294 and perplexity is 135.0924099723449
At time: 166.4746527671814 and batch: 800, loss is 4.946087236404419 and perplexity is 140.62365892845102
At time: 166.79890298843384 and batch: 850, loss is 4.916273860931397 and perplexity is 136.49307229662594
At time: 167.11916422843933 and batch: 900, loss is 5.03473313331604 and perplexity is 153.6585807838249
At time: 167.45420956611633 and batch: 950, loss is 4.9707612133026124 and perplexity is 144.13656429198986
At time: 167.78105759620667 and batch: 1000, loss is 4.931797885894776 and perplexity is 138.62852669594568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.148833112018864 and perplexity of 172.2303994313205
Finished 23 epochs...
Completing Train Step...
At time: 168.89499020576477 and batch: 50, loss is 5.028688898086548 and perplexity is 152.7326333176366
At time: 169.2353253364563 and batch: 100, loss is 4.99432822227478 and perplexity is 147.5737753020019
At time: 169.5604796409607 and batch: 150, loss is 5.017801494598388 and perplexity is 151.07879087445278
At time: 169.88623929023743 and batch: 200, loss is 5.030294895172119 and perplexity is 152.9781185531392
At time: 170.2113254070282 and batch: 250, loss is 5.011807928085327 and perplexity is 150.1759982689693
At time: 170.53778314590454 and batch: 300, loss is 4.9007258796691895 and perplexity is 134.38729329307986
At time: 170.86312818527222 and batch: 350, loss is 4.9643580436706545 and perplexity is 143.2165819654237
At time: 171.19036149978638 and batch: 400, loss is 4.90598578453064 and perplexity is 135.09601995378938
At time: 171.51701831817627 and batch: 450, loss is 4.929516811370849 and perplexity is 138.31266508430804
At time: 171.84123706817627 and batch: 500, loss is 4.914291114807129 and perplexity is 136.22270930569934
At time: 172.16655325889587 and batch: 550, loss is 4.958906002044678 and perplexity is 142.43788387510278
At time: 172.49085307121277 and batch: 600, loss is 4.989850673675537 and perplexity is 146.91448365778294
At time: 172.8179168701172 and batch: 650, loss is 4.975542011260987 and perplexity is 144.82730190714815
At time: 173.1448667049408 and batch: 700, loss is 4.957013320922852 and perplexity is 142.16854934385265
At time: 173.48440384864807 and batch: 750, loss is 4.895909566879272 and perplexity is 133.74159823644464
At time: 173.80994582176208 and batch: 800, loss is 4.936555948257446 and perplexity is 139.2897015795047
At time: 174.135826587677 and batch: 850, loss is 4.906030492782593 and perplexity is 135.10205999570618
At time: 174.46246814727783 and batch: 900, loss is 5.025971145629883 and perplexity is 152.3181073728239
At time: 174.78646159172058 and batch: 950, loss is 4.960950927734375 and perplexity is 142.7294567838845
At time: 175.11183738708496 and batch: 1000, loss is 4.920243549346924 and perplexity is 137.03598414701025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.148563291968369 and perplexity of 172.18393448511807
Finished 24 epochs...
Completing Train Step...
At time: 176.22254252433777 and batch: 50, loss is 5.0181611633300784 and perplexity is 151.13313896462031
At time: 176.5632448196411 and batch: 100, loss is 4.983825721740723 and perplexity is 146.0319921093365
At time: 176.88987517356873 and batch: 150, loss is 5.0071107864379885 and perplexity is 149.47225441924172
At time: 177.21702075004578 and batch: 200, loss is 5.023281192779541 and perplexity is 151.90892942715735
At time: 177.5425238609314 and batch: 250, loss is 5.003955459594726 and perplexity is 149.00136389998312
At time: 177.8704628944397 and batch: 300, loss is 4.890683565139771 and perplexity is 133.04448754882094
At time: 178.19735479354858 and batch: 350, loss is 4.9590449714660645 and perplexity is 142.45767976088797
At time: 178.52583646774292 and batch: 400, loss is 4.897250576019287 and perplexity is 133.92106724997868
At time: 178.85310125350952 and batch: 450, loss is 4.9225576210021975 and perplexity is 137.35346242574767
At time: 179.1798095703125 and batch: 500, loss is 4.903522129058838 and perplexity is 134.7635995578135
At time: 179.50763201713562 and batch: 550, loss is 4.952024269104004 and perplexity is 141.46102947835277
At time: 179.83428859710693 and batch: 600, loss is 4.983713865280151 and perplexity is 146.0156584011014
At time: 180.16231775283813 and batch: 650, loss is 4.966788558959961 and perplexity is 143.56509541964493
At time: 180.48887729644775 and batch: 700, loss is 4.949953165054321 and perplexity is 141.16835215454245
At time: 180.81431412696838 and batch: 750, loss is 4.889670705795288 and perplexity is 132.90980041744942
At time: 181.140527009964 and batch: 800, loss is 4.926567935943604 and perplexity is 137.90539904849433
At time: 181.46724891662598 and batch: 850, loss is 4.896458578109741 and perplexity is 133.81504403529678
At time: 181.8072075843811 and batch: 900, loss is 5.018741693496704 and perplexity is 151.22090178298308
At time: 182.13382172584534 and batch: 950, loss is 4.952718210220337 and perplexity is 141.5592291715283
At time: 182.46013045310974 and batch: 1000, loss is 4.9108711624145505 and perplexity is 135.75762985317925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.146271868449886 and perplexity of 171.78983985941767
Finished 25 epochs...
Completing Train Step...
At time: 183.58658862113953 and batch: 50, loss is 5.010679378509521 and perplexity is 150.0066128077496
At time: 183.91089153289795 and batch: 100, loss is 4.977382459640503 and perplexity is 145.09409451395555
At time: 184.2386555671692 and batch: 150, loss is 4.999035263061524 and perplexity is 148.27004848898164
At time: 184.5637583732605 and batch: 200, loss is 5.016311693191528 and perplexity is 150.8538810563421
At time: 184.88728857040405 and batch: 250, loss is 4.994326705932617 and perplexity is 147.57355152983394
At time: 185.21215748786926 and batch: 300, loss is 4.8837682723999025 and perplexity is 132.12761982815346
At time: 185.53882360458374 and batch: 350, loss is 4.9517265319824215 and perplexity is 141.41891754807338
At time: 185.86556911468506 and batch: 400, loss is 4.888519515991211 and perplexity is 132.7568840451189
At time: 186.19201683998108 and batch: 450, loss is 4.915055475234985 and perplexity is 136.32687235805423
At time: 186.5188729763031 and batch: 500, loss is 4.898677415847779 and perplexity is 134.1122875505338
At time: 186.84472584724426 and batch: 550, loss is 4.944524641036987 and perplexity is 140.4040926417657
At time: 187.17122054100037 and batch: 600, loss is 4.978178758621215 and perplexity is 145.20967880724427
At time: 187.4965534210205 and batch: 650, loss is 4.959333553314209 and perplexity is 142.4987963938673
At time: 187.82047629356384 and batch: 700, loss is 4.941226482391357 and perplexity is 139.94178047824707
At time: 188.14608573913574 and batch: 750, loss is 4.884039344787598 and perplexity is 132.16344083236402
At time: 188.4740493297577 and batch: 800, loss is 4.920043811798096 and perplexity is 137.0086156487849
At time: 188.80016565322876 and batch: 850, loss is 4.8897662925720216 and perplexity is 132.92250544407426
At time: 189.12661361694336 and batch: 900, loss is 5.011407661437988 and perplexity is 150.11589985412928
At time: 189.4527394771576 and batch: 950, loss is 4.945693750381469 and perplexity is 140.56833636920734
At time: 189.7795329093933 and batch: 1000, loss is 4.906975212097168 and perplexity is 135.2297538291413
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.14255058474657 and perplexity of 171.15174912263763
Finished 26 epochs...
Completing Train Step...
At time: 190.88536095619202 and batch: 50, loss is 5.001265754699707 and perplexity is 148.60113269534173
At time: 191.22661352157593 and batch: 100, loss is 4.9691196537017825 and perplexity is 143.90014962851524
At time: 191.5547971725464 and batch: 150, loss is 4.991164236068726 and perplexity is 147.10759180056112
At time: 191.8803369998932 and batch: 200, loss is 5.0091362476348875 and perplexity is 149.77531148213413
At time: 192.20327138900757 and batch: 250, loss is 4.989287614822388 and perplexity is 146.83178544126983
At time: 192.52802324295044 and batch: 300, loss is 4.879174394607544 and perplexity is 131.52203374889734
At time: 192.85538506507874 and batch: 350, loss is 4.944769010543824 and perplexity is 140.43840731320122
At time: 193.18108558654785 and batch: 400, loss is 4.8843120765686034 and perplexity is 132.19949091874471
At time: 193.50684928894043 and batch: 450, loss is 4.9113500213623045 and perplexity is 135.8226541764573
At time: 193.83081984519958 and batch: 500, loss is 4.893621778488159 and perplexity is 133.43597549397336
At time: 194.156405210495 and batch: 550, loss is 4.937766647338867 and perplexity is 139.4584416193525
At time: 194.4812572002411 and batch: 600, loss is 4.973206272125244 and perplexity is 144.48941786826165
At time: 194.80609893798828 and batch: 650, loss is 4.954462766647339 and perplexity is 141.80640277604215
At time: 195.13087606430054 and batch: 700, loss is 4.936903200149536 and perplexity is 139.3380785909451
At time: 195.45721244812012 and batch: 750, loss is 4.878466110229493 and perplexity is 131.4289117293523
At time: 195.7832853794098 and batch: 800, loss is 4.914441986083984 and perplexity is 136.24326295022732
At time: 196.10857129096985 and batch: 850, loss is 4.882158422470093 and perplexity is 131.91508530888837
At time: 196.43278312683105 and batch: 900, loss is 5.005593004226685 and perplexity is 149.245560170169
At time: 196.7594313621521 and batch: 950, loss is 4.9411283874511716 and perplexity is 139.92805357094264
At time: 197.0835952758789 and batch: 1000, loss is 4.900261726379394 and perplexity is 134.3249314626352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.139440024771342 and perplexity of 170.62019848079058
Finished 27 epochs...
Completing Train Step...
At time: 198.21140122413635 and batch: 50, loss is 4.999399337768555 and perplexity is 148.32403969126594
At time: 198.55360293388367 and batch: 100, loss is 4.964791021347046 and perplexity is 143.27860497462407
At time: 198.87744617462158 and batch: 150, loss is 4.984834890365601 and perplexity is 146.17943740006893
At time: 199.2153537273407 and batch: 200, loss is 5.00392593383789 and perplexity is 148.99696458689124
At time: 199.53939175605774 and batch: 250, loss is 4.982791633605957 and perplexity is 145.88106021089246
At time: 199.86420512199402 and batch: 300, loss is 4.872576923370361 and perplexity is 130.6571769850177
At time: 200.18985962867737 and batch: 350, loss is 4.940417518615723 and perplexity is 139.8286184253145
At time: 200.51431465148926 and batch: 400, loss is 4.879030151367187 and perplexity is 131.50306395273645
At time: 200.84144115447998 and batch: 450, loss is 4.903262042999268 and perplexity is 134.72855398186147
At time: 201.16783142089844 and batch: 500, loss is 4.884844827651977 and perplexity is 132.26993910476062
At time: 201.49434065818787 and batch: 550, loss is 4.932020568847657 and perplexity is 138.65940034301374
At time: 201.8199064731598 and batch: 600, loss is 4.966712551116943 and perplexity is 143.55418376110052
At time: 202.14527368545532 and batch: 650, loss is 4.946136531829834 and perplexity is 140.63059120240453
At time: 202.4708216190338 and batch: 700, loss is 4.929730863571167 and perplexity is 138.34227438345587
At time: 202.79587030410767 and batch: 750, loss is 4.871884860992432 and perplexity is 130.5667853503525
At time: 203.1231050491333 and batch: 800, loss is 4.907085857391357 and perplexity is 135.24471719283446
At time: 203.44759559631348 and batch: 850, loss is 4.876691837310791 and perplexity is 131.1959277202657
At time: 203.7713646888733 and batch: 900, loss is 4.999064350128174 and perplexity is 148.27436129248738
At time: 204.095299243927 and batch: 950, loss is 4.936345520019532 and perplexity is 139.2603941767024
At time: 204.42012643814087 and batch: 1000, loss is 4.894810256958007 and perplexity is 133.59465555319045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.137256157107469 and perplexity of 170.247993118082
Finished 28 epochs...
Completing Train Step...
At time: 205.5406973361969 and batch: 50, loss is 4.990721235275268 and perplexity is 147.04243745344914
At time: 205.86779403686523 and batch: 100, loss is 4.956687755584717 and perplexity is 142.1222717256164
At time: 206.19316005706787 and batch: 150, loss is 4.980757970809936 and perplexity is 145.5846887880098
At time: 206.518803358078 and batch: 200, loss is 4.9989080810546875 and perplexity is 148.25119240576376
At time: 206.84307384490967 and batch: 250, loss is 4.979469594955444 and perplexity is 145.39724176722814
At time: 207.16835165023804 and batch: 300, loss is 4.865776500701904 and perplexity is 129.77166728954094
At time: 207.50812911987305 and batch: 350, loss is 4.933726615905762 and perplexity is 138.89616171063855
At time: 207.83299899101257 and batch: 400, loss is 4.87292221069336 and perplexity is 130.70229904147814
At time: 208.1577854156494 and batch: 450, loss is 4.897153768539429 and perplexity is 133.90810331647128
At time: 208.48300004005432 and batch: 500, loss is 4.876682691574096 and perplexity is 131.1947278423423
At time: 208.80829405784607 and batch: 550, loss is 4.922591495513916 and perplexity is 137.3581152860264
At time: 209.13432669639587 and batch: 600, loss is 4.959349288940429 and perplexity is 142.50103871930642
At time: 209.4584767818451 and batch: 650, loss is 4.942086973190308 and perplexity is 140.06225091715598
At time: 209.78345441818237 and batch: 700, loss is 4.925138292312622 and perplexity is 137.70838433703653
At time: 210.10796189308167 and batch: 750, loss is 4.864525470733643 and perplexity is 129.60942055365558
At time: 210.43340635299683 and batch: 800, loss is 4.901024131774903 and perplexity is 134.42738056404494
At time: 210.7572717666626 and batch: 850, loss is 4.872392139434814 and perplexity is 130.633035868161
At time: 211.08488726615906 and batch: 900, loss is 4.99410454750061 and perplexity is 147.54077046245044
At time: 211.41093277931213 and batch: 950, loss is 4.931601467132569 and perplexity is 138.60130012631757
At time: 211.7384295463562 and batch: 1000, loss is 4.887521715164184 and perplexity is 132.62448518125788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.1331303294112045 and perplexity of 169.54702626142537
Finished 29 epochs...
Completing Train Step...
At time: 212.84949803352356 and batch: 50, loss is 4.988062648773194 and perplexity is 146.65203160781067
At time: 213.1898148059845 and batch: 100, loss is 4.9492832469940184 and perplexity is 141.0738125963054
At time: 213.51360321044922 and batch: 150, loss is 4.976625328063965 and perplexity is 144.98428077040208
At time: 213.83848690986633 and batch: 200, loss is 4.99568395614624 and perplexity is 147.7739817503845
At time: 214.16333293914795 and batch: 250, loss is 4.975320777893066 and perplexity is 144.79526481934752
At time: 214.48854851722717 and batch: 300, loss is 4.861918096542358 and perplexity is 129.27192048125085
At time: 214.81525659561157 and batch: 350, loss is 4.928326940536499 and perplexity is 138.14818875030468
At time: 215.1406762599945 and batch: 400, loss is 4.867043800354004 and perplexity is 129.93623113217487
At time: 215.4665584564209 and batch: 450, loss is 4.891477928161621 and perplexity is 133.15021515748256
At time: 215.79356002807617 and batch: 500, loss is 4.8726119041442875 and perplexity is 130.66174755412823
At time: 216.13275456428528 and batch: 550, loss is 4.918387098312378 and perplexity is 136.78181954753364
At time: 216.4571192264557 and batch: 600, loss is 4.954715032577514 and perplexity is 141.8421802126677
At time: 216.78252720832825 and batch: 650, loss is 4.936541099548339 and perplexity is 139.2876333225999
At time: 217.10787057876587 and batch: 700, loss is 4.9167978477478025 and perplexity is 136.56461160823133
At time: 217.433039188385 and batch: 750, loss is 4.8613237953186035 and perplexity is 129.1951168452197
At time: 217.7568860054016 and batch: 800, loss is 4.896806650161743 and perplexity is 133.86162941932758
At time: 218.08182764053345 and batch: 850, loss is 4.868387861251831 and perplexity is 130.11099075713122
At time: 218.40782737731934 and batch: 900, loss is 4.988691368103027 and perplexity is 146.74426356581114
At time: 218.73321557044983 and batch: 950, loss is 4.925049028396606 and perplexity is 137.69609249599955
At time: 219.05707144737244 and batch: 1000, loss is 4.881434726715088 and perplexity is 131.81965345760705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.13343848251715 and perplexity of 169.59928075495037
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 220.16365551948547 and batch: 50, loss is 4.973733711242676 and perplexity is 144.5656473407359
At time: 220.50348019599915 and batch: 100, loss is 4.910291299819947 and perplexity is 135.67893190089984
At time: 220.82788610458374 and batch: 150, loss is 4.933445672988892 and perplexity is 138.8571452987744
At time: 221.1541087627411 and batch: 200, loss is 4.948058958053589 and perplexity is 140.9012031712247
At time: 221.4798936843872 and batch: 250, loss is 4.916580533981323 and perplexity is 136.53493746253187
At time: 221.80593752861023 and batch: 300, loss is 4.805695371627808 and perplexity is 122.2044389822185
At time: 222.13182306289673 and batch: 350, loss is 4.870877542495728 and perplexity is 130.43532923261893
At time: 222.45811104774475 and batch: 400, loss is 4.800012245178222 and perplexity is 121.51190544456321
At time: 222.7845664024353 and batch: 450, loss is 4.826826572418213 and perplexity is 124.81424254970351
At time: 223.11170721054077 and batch: 500, loss is 4.799160718917847 and perplexity is 121.40847890758452
At time: 223.43642210960388 and batch: 550, loss is 4.841320495605469 and perplexity is 126.63646426470174
At time: 223.76287293434143 and batch: 600, loss is 4.873458824157715 and perplexity is 130.77245447645802
At time: 224.0882453918457 and batch: 650, loss is 4.8537869548797605 and perplexity is 128.22505406023922
At time: 224.4287600517273 and batch: 700, loss is 4.828235816955567 and perplexity is 124.9902603361221
At time: 224.7549397945404 and batch: 750, loss is 4.770629053115845 and perplexity is 117.99344276292017
At time: 225.08156538009644 and batch: 800, loss is 4.799404306411743 and perplexity is 121.43805609686986
At time: 225.40714645385742 and batch: 850, loss is 4.765547885894775 and perplexity is 117.39541896537146
At time: 225.73391270637512 and batch: 900, loss is 4.883864879608154 and perplexity is 132.1403849252283
At time: 226.06019282341003 and batch: 950, loss is 4.808243713378906 and perplexity is 122.51625479412704
At time: 226.38574647903442 and batch: 1000, loss is 4.770191116333008 and perplexity is 117.94178040744929
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0651941066834985 and perplexity of 158.4111877598986
Finished 31 epochs...
Completing Train Step...
At time: 227.5080816745758 and batch: 50, loss is 4.925341815948486 and perplexity is 137.73641410036873
At time: 227.83456540107727 and batch: 100, loss is 4.873621330261231 and perplexity is 130.7937075253105
At time: 228.16049218177795 and batch: 150, loss is 4.904010343551636 and perplexity is 134.82940916351365
At time: 228.4866578578949 and batch: 200, loss is 4.921689291000366 and perplexity is 137.23424606053194
At time: 228.81213402748108 and batch: 250, loss is 4.895086688995361 and perplexity is 133.6315905007708
At time: 229.13763403892517 and batch: 300, loss is 4.78707911491394 and perplexity is 119.95049487012778
At time: 229.46417689323425 and batch: 350, loss is 4.852125091552734 and perplexity is 128.0121385125631
At time: 229.79198265075684 and batch: 400, loss is 4.784148616790771 and perplexity is 119.59949472386644
At time: 230.12003350257874 and batch: 450, loss is 4.81280369758606 and perplexity is 123.07620268761276
At time: 230.44591999053955 and batch: 500, loss is 4.786099071502686 and perplexity is 119.83299576447045
At time: 230.77120757102966 and batch: 550, loss is 4.8294203662872315 and perplexity is 125.13840519058951
At time: 231.09625720977783 and batch: 600, loss is 4.863048324584961 and perplexity is 129.41810982898795
At time: 231.42328906059265 and batch: 650, loss is 4.844603614807129 and perplexity is 127.05291011878151
At time: 231.74993562698364 and batch: 700, loss is 4.821456203460693 and perplexity is 124.1457406736001
At time: 232.07688903808594 and batch: 750, loss is 4.765679397583008 and perplexity is 117.4108588503508
At time: 232.40318894386292 and batch: 800, loss is 4.7949494361877445 and perplexity is 120.89826855114845
At time: 232.73421812057495 and batch: 850, loss is 4.763532276153565 and perplexity is 117.1590339253558
At time: 233.0759153366089 and batch: 900, loss is 4.88475679397583 and perplexity is 132.25829540830372
At time: 233.40473675727844 and batch: 950, loss is 4.813292188644409 and perplexity is 123.13633899895062
At time: 233.73132348060608 and batch: 1000, loss is 4.775468530654908 and perplexity is 118.56585334582748
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.060648848370808 and perplexity of 157.69280185344937
Finished 32 epochs...
Completing Train Step...
At time: 234.84156155586243 and batch: 50, loss is 4.915272350311279 and perplexity is 136.3564414651831
At time: 235.1807050704956 and batch: 100, loss is 4.86323748588562 and perplexity is 129.44259304253737
At time: 235.5066545009613 and batch: 150, loss is 4.894461326599121 and perplexity is 133.54804845387176
At time: 235.83388543128967 and batch: 200, loss is 4.9123804473876955 and perplexity is 135.96268150567073
At time: 236.16051745414734 and batch: 250, loss is 4.88598219871521 and perplexity is 132.42046469153044
At time: 236.48667669296265 and batch: 300, loss is 4.77938292503357 and perplexity is 119.03087640371201
At time: 236.8122034072876 and batch: 350, loss is 4.8446910953521725 and perplexity is 127.0640252627804
At time: 237.13958621025085 and batch: 400, loss is 4.777572860717774 and perplexity is 118.81561773664326
At time: 237.46967387199402 and batch: 450, loss is 4.8072677326202395 and perplexity is 122.39673961856683
At time: 237.81709623336792 and batch: 500, loss is 4.780509614944458 and perplexity is 119.16506287031432
At time: 238.1432340145111 and batch: 550, loss is 4.82421576499939 and perplexity is 124.48880161594106
At time: 238.46901059150696 and batch: 600, loss is 4.858980684280396 and perplexity is 128.89275271555786
At time: 238.79437613487244 and batch: 650, loss is 4.841187181472779 and perplexity is 126.61958295958459
At time: 239.11927032470703 and batch: 700, loss is 4.819089708328247 and perplexity is 123.85229773517213
At time: 239.44536137580872 and batch: 750, loss is 4.763855247497559 and perplexity is 117.1968790471197
At time: 239.77110266685486 and batch: 800, loss is 4.793183879852295 and perplexity is 120.68500416770353
At time: 240.09681272506714 and batch: 850, loss is 4.763008880615234 and perplexity is 117.09772945436697
At time: 240.42184686660767 and batch: 900, loss is 4.885104036331176 and perplexity is 132.30422906490935
At time: 240.74670124053955 and batch: 950, loss is 4.814687042236328 and perplexity is 123.3082160074632
At time: 241.0735411643982 and batch: 1000, loss is 4.777339305877685 and perplexity is 118.78787101435955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0582733154296875 and perplexity of 157.31864199855275
Finished 33 epochs...
Completing Train Step...
At time: 242.1920735836029 and batch: 50, loss is 4.909369115829468 and perplexity is 135.55386863658867
At time: 242.53687047958374 and batch: 100, loss is 4.856950473785401 and perplexity is 128.63133874874572
At time: 242.86277055740356 and batch: 150, loss is 4.888312091827393 and perplexity is 132.7293499151754
At time: 243.18793725967407 and batch: 200, loss is 4.9066009426116945 and perplexity is 135.17915092890632
At time: 243.51247668266296 and batch: 250, loss is 4.880931749343872 and perplexity is 131.75336782632007
At time: 243.83888936042786 and batch: 300, loss is 4.775217847824097 and perplexity is 118.53613464721262
At time: 244.16546177864075 and batch: 350, loss is 4.840060930252076 and perplexity is 126.47705777435877
At time: 244.49148106575012 and batch: 400, loss is 4.773444871902466 and perplexity is 118.32615913041151
At time: 244.8151502609253 and batch: 450, loss is 4.80383186340332 and perplexity is 121.9769220607742
At time: 245.14179944992065 and batch: 500, loss is 4.777030429840088 and perplexity is 118.75118595330581
At time: 245.4671812057495 and batch: 550, loss is 4.820880498886108 and perplexity is 124.07428997200041
At time: 245.79163432121277 and batch: 600, loss is 4.856148557662964 and perplexity is 128.52822855274238
At time: 246.11614513397217 and batch: 650, loss is 4.838791570663452 and perplexity is 126.3166147598146
At time: 246.44155168533325 and batch: 700, loss is 4.817429256439209 and perplexity is 123.64681759558987
At time: 246.76810598373413 and batch: 750, loss is 4.762526626586914 and perplexity is 117.04127221708971
At time: 247.0942873954773 and batch: 800, loss is 4.791412744522095 and perplexity is 120.47144387091694
At time: 247.4201557636261 and batch: 850, loss is 4.762089138031006 and perplexity is 116.99007919892207
At time: 247.7446949481964 and batch: 900, loss is 4.8847033882141115 and perplexity is 132.2512322419024
At time: 248.07206463813782 and batch: 950, loss is 4.815017480850219 and perplexity is 123.34896853617582
At time: 248.3976812362671 and batch: 1000, loss is 4.777433109283447 and perplexity is 118.7990142438522
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.056274786228087 and perplexity of 157.00455006405744
Finished 34 epochs...
Completing Train Step...
At time: 249.5186905860901 and batch: 50, loss is 4.90491925239563 and perplexity is 134.95201251512788
At time: 249.84431982040405 and batch: 100, loss is 4.8520914077758786 and perplexity is 128.00782665287488
At time: 250.1854910850525 and batch: 150, loss is 4.8836126804351805 and perplexity is 132.1070634314369
At time: 250.51032090187073 and batch: 200, loss is 4.90197569847107 and perplexity is 134.55535806234852
At time: 250.83681321144104 and batch: 250, loss is 4.8772673988342286 and perplexity is 131.27146078317045
At time: 251.16260814666748 and batch: 300, loss is 4.772012662887573 and perplexity is 118.15681263734159
At time: 251.48799753189087 and batch: 350, loss is 4.836521234512329 and perplexity is 126.03015888144904
At time: 251.81275534629822 and batch: 400, loss is 4.770146827697754 and perplexity is 117.93655704262439
At time: 252.1370370388031 and batch: 450, loss is 4.801050453186035 and perplexity is 121.63812558788467
At time: 252.46323132514954 and batch: 500, loss is 4.773969478607178 and perplexity is 118.38825011208274
At time: 252.78865098953247 and batch: 550, loss is 4.817925930023193 and perplexity is 123.70824495704443
At time: 253.11534309387207 and batch: 600, loss is 4.8537618350982665 and perplexity is 128.22183311535406
At time: 253.44085454940796 and batch: 650, loss is 4.836846084594726 and perplexity is 126.07110643949494
At time: 253.76715850830078 and batch: 700, loss is 4.8159992885589595 and perplexity is 123.47013297470667
At time: 254.09291124343872 and batch: 750, loss is 4.761160345077514 and perplexity is 116.88147008323209
At time: 254.41855239868164 and batch: 800, loss is 4.789311513900757 and perplexity is 120.21857134884154
At time: 254.74529600143433 and batch: 850, loss is 4.760791082382202 and perplexity is 116.8383180842591
At time: 255.07307147979736 and batch: 900, loss is 4.883420295715332 and perplexity is 132.08165049565733
At time: 255.40079545974731 and batch: 950, loss is 4.814457693099976 and perplexity is 123.27993861743813
At time: 255.72574019432068 and batch: 1000, loss is 4.776061611175537 and perplexity is 118.63619330042535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0547120629287345 and perplexity of 156.75938700647188
Finished 35 epochs...
Completing Train Step...
At time: 256.83242654800415 and batch: 50, loss is 4.901267995834351 and perplexity is 134.46016656827183
At time: 257.1715281009674 and batch: 100, loss is 4.847130908966064 and perplexity is 127.37441629561438
At time: 257.4972698688507 and batch: 150, loss is 4.879428691864014 and perplexity is 131.55548369417943
At time: 257.8232047557831 and batch: 200, loss is 4.898180074691773 and perplexity is 134.04560457387768
At time: 258.1490411758423 and batch: 250, loss is 4.874175043106079 and perplexity is 130.86614973543283
At time: 258.4887328147888 and batch: 300, loss is 4.769277839660645 and perplexity is 117.83411610183893
At time: 258.81398248672485 and batch: 350, loss is 4.833295583724976 and perplexity is 125.62428455656145
At time: 259.1402516365051 and batch: 400, loss is 4.767122974395752 and perplexity is 117.58047283970757
At time: 259.4657962322235 and batch: 450, loss is 4.798163976669311 and perplexity is 121.28752623665935
At time: 259.7934832572937 and batch: 500, loss is 4.7713074874877925 and perplexity is 118.07352073090648
At time: 260.11852407455444 and batch: 550, loss is 4.81521782875061 and perplexity is 123.3736837187722
At time: 260.4429335594177 and batch: 600, loss is 4.851440896987915 and perplexity is 127.9245832589918
At time: 260.76778745651245 and batch: 650, loss is 4.834656372070312 and perplexity is 125.79534898371935
At time: 261.0935401916504 and batch: 700, loss is 4.814536600112915 and perplexity is 123.28966665294989
At time: 261.41820764541626 and batch: 750, loss is 4.759420785903931 and perplexity is 116.67832459275527
At time: 261.7425193786621 and batch: 800, loss is 4.787163019180298 and perplexity is 119.96055965063223
At time: 262.0671708583832 and batch: 850, loss is 4.758837490081787 and perplexity is 116.61028645859439
At time: 262.3915367126465 and batch: 900, loss is 4.881832685470581 and perplexity is 131.87212268244784
At time: 262.7186839580536 and batch: 950, loss is 4.812880439758301 and perplexity is 123.08564818518755
At time: 263.0445499420166 and batch: 1000, loss is 4.774774150848389 and perplexity is 118.48355218895902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.053228238733803 and perplexity of 156.5269561211806
Finished 36 epochs...
Completing Train Step...
At time: 264.1559557914734 and batch: 50, loss is 4.897633972167969 and perplexity is 133.9724219153511
At time: 264.496915102005 and batch: 100, loss is 4.842874774932861 and perplexity is 126.83344574566885
At time: 264.8221220970154 and batch: 150, loss is 4.875438280105591 and perplexity is 131.0315691577321
At time: 265.1469373703003 and batch: 200, loss is 4.894583072662353 and perplexity is 133.5643083927951
At time: 265.4731993675232 and batch: 250, loss is 4.871120271682739 and perplexity is 130.46699353681106
At time: 265.79779529571533 and batch: 300, loss is 4.766356496810913 and perplexity is 117.49038457259036
At time: 266.1247854232788 and batch: 350, loss is 4.829953918457031 and perplexity is 125.20519087344283
At time: 266.4511637687683 and batch: 400, loss is 4.763817977905274 and perplexity is 117.19251124861405
At time: 266.7795169353485 and batch: 450, loss is 4.795299835205078 and perplexity is 120.94063860842584
At time: 267.1305921077728 and batch: 500, loss is 4.768364353179932 and perplexity is 117.72652537863031
At time: 267.4555892944336 and batch: 550, loss is 4.812804927825928 and perplexity is 123.07635410095727
At time: 267.7800626754761 and batch: 600, loss is 4.848721218109131 and perplexity is 127.5771421501062
At time: 268.1064667701721 and batch: 650, loss is 4.832656888961792 and perplexity is 125.54407460145234
At time: 268.4323081970215 and batch: 700, loss is 4.812542533874511 and perplexity is 123.04406384664776
At time: 268.7579433917999 and batch: 750, loss is 4.75742015838623 and perplexity is 116.44512807332364
At time: 269.08321619033813 and batch: 800, loss is 4.784534463882446 and perplexity is 119.64565074508204
At time: 269.4086608886719 and batch: 850, loss is 4.756717586517334 and perplexity is 116.36334573441874
At time: 269.7361283302307 and batch: 900, loss is 4.879908790588379 and perplexity is 131.61865847793734
At time: 270.06063985824585 and batch: 950, loss is 4.810870122909546 and perplexity is 122.83845558377654
At time: 270.3870840072632 and batch: 1000, loss is 4.773328065872192 and perplexity is 118.31233872865478
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.051721712438072 and perplexity of 156.29132168509707
Finished 37 epochs...
Completing Train Step...
At time: 271.5215744972229 and batch: 50, loss is 4.894366607666016 and perplexity is 133.53539952426
At time: 271.8477621078491 and batch: 100, loss is 4.839014091491699 and perplexity is 126.34472596510068
At time: 272.1735351085663 and batch: 150, loss is 4.872047739028931 and perplexity is 130.58805354399612
At time: 272.5000329017639 and batch: 200, loss is 4.8914120388031 and perplexity is 133.1414422642422
At time: 272.82625460624695 and batch: 250, loss is 4.868059701919556 and perplexity is 130.06830062624823
At time: 273.1517617702484 and batch: 300, loss is 4.763615493774414 and perplexity is 117.16878402710459
At time: 273.4775404930115 and batch: 350, loss is 4.827185010910034 and perplexity is 124.8589887974705
At time: 273.8038098812103 and batch: 400, loss is 4.760879278182983 and perplexity is 116.84862318771127
At time: 274.13132405281067 and batch: 450, loss is 4.7926647663116455 and perplexity is 120.62237120610249
At time: 274.45631194114685 and batch: 500, loss is 4.765839309692383 and perplexity is 117.42963576974144
At time: 274.78075194358826 and batch: 550, loss is 4.81038969039917 and perplexity is 122.7794541704231
At time: 275.1081714630127 and batch: 600, loss is 4.846427383422852 and perplexity is 127.28483665467623
At time: 275.4499192237854 and batch: 650, loss is 4.830579948425293 and perplexity is 125.28359761503584
At time: 275.7751076221466 and batch: 700, loss is 4.81076117515564 and perplexity is 122.82507333894334
At time: 276.10095286369324 and batch: 750, loss is 4.755545454025269 and perplexity is 116.22703238026038
At time: 276.4267818927765 and batch: 800, loss is 4.782074184417724 and perplexity is 119.35165081680158
At time: 276.75131607055664 and batch: 850, loss is 4.754690017700195 and perplexity is 116.12765006848703
At time: 277.0765452384949 and batch: 900, loss is 4.877938289642334 and perplexity is 131.35955914846375
At time: 277.4011080265045 and batch: 950, loss is 4.808858470916748 and perplexity is 122.59159574110464
At time: 277.7274901866913 and batch: 1000, loss is 4.771678781509399 and perplexity is 118.11736886303483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.050206626333842 and perplexity of 156.05470616710423
Finished 38 epochs...
Completing Train Step...
At time: 278.83613419532776 and batch: 50, loss is 4.891473922729492 and perplexity is 133.14968183440087
At time: 279.17590403556824 and batch: 100, loss is 4.835652141571045 and perplexity is 125.92067454290053
At time: 279.5021469593048 and batch: 150, loss is 4.8690446090698245 and perplexity is 130.19646893213803
At time: 279.8282389640808 and batch: 200, loss is 4.88832950592041 and perplexity is 132.73166129654618
At time: 280.1547865867615 and batch: 250, loss is 4.865050497055054 and perplexity is 129.67748677766517
At time: 280.4820749759674 and batch: 300, loss is 4.760923013687134 and perplexity is 116.85373373291102
At time: 280.80868101119995 and batch: 350, loss is 4.824599990844726 and perplexity is 124.53664262126368
At time: 281.13549304008484 and batch: 400, loss is 4.758210506439209 and perplexity is 116.5371966318857
At time: 281.4636960029602 and batch: 450, loss is 4.790187387466431 and perplexity is 120.32391374416427
At time: 281.7894654273987 and batch: 500, loss is 4.763357734680175 and perplexity is 117.13858659945873
At time: 282.1166136264801 and batch: 550, loss is 4.807942571640015 and perplexity is 122.47936559083409
At time: 282.44421339035034 and batch: 600, loss is 4.8440664196014405 and perplexity is 126.9846762336958
At time: 282.7724335193634 and batch: 650, loss is 4.828363056182861 and perplexity is 125.00616501209454
At time: 283.0982472896576 and batch: 700, loss is 4.808890352249145 and perplexity is 122.59550418682046
At time: 283.423476934433 and batch: 750, loss is 4.753573188781738 and perplexity is 115.9980277471098
At time: 283.7505326271057 and batch: 800, loss is 4.779632816314697 and perplexity is 119.0606248987002
At time: 284.09240078926086 and batch: 850, loss is 4.752448577880859 and perplexity is 115.86764842736848
At time: 284.4186952114105 and batch: 900, loss is 4.875896682739258 and perplexity is 131.09164814325806
At time: 284.7457420825958 and batch: 950, loss is 4.806703062057495 and perplexity is 122.32764529233025
At time: 285.0726788043976 and batch: 1000, loss is 4.769912128448486 and perplexity is 117.9088806691643
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.048543418326029 and perplexity of 155.7953704545497
Finished 39 epochs...
Completing Train Step...
At time: 286.18203353881836 and batch: 50, loss is 4.888291444778442 and perplexity is 132.7266094740816
At time: 286.52284598350525 and batch: 100, loss is 4.832505121231079 and perplexity is 125.52502250792863
At time: 286.84726643562317 and batch: 150, loss is 4.8659978771209715 and perplexity is 129.80039885667026
At time: 287.17264676094055 and batch: 200, loss is 4.885367403030395 and perplexity is 132.3390781818579
At time: 287.4985845088959 and batch: 250, loss is 4.862131576538086 and perplexity is 129.29952039619297
At time: 287.82476115226746 and batch: 300, loss is 4.758187532424927 and perplexity is 116.53451933542013
At time: 288.1515293121338 and batch: 350, loss is 4.822114915847778 and perplexity is 124.22754395024357
At time: 288.47883653640747 and batch: 400, loss is 4.755470094680786 and perplexity is 116.21827391730918
At time: 288.803822517395 and batch: 450, loss is 4.787624950408936 and perplexity is 120.01598597993035
At time: 289.12952971458435 and batch: 500, loss is 4.760986680984497 and perplexity is 116.86117373116436
At time: 289.45559549331665 and batch: 550, loss is 4.805639028549194 and perplexity is 122.19755380187398
At time: 289.7814266681671 and batch: 600, loss is 4.841791276931763 and perplexity is 126.69609638304789
At time: 290.10639238357544 and batch: 650, loss is 4.826225681304932 and perplexity is 124.73926530937916
At time: 290.43221735954285 and batch: 700, loss is 4.80692421913147 and perplexity is 122.35470190820006
At time: 290.7568805217743 and batch: 750, loss is 4.751581420898438 and perplexity is 115.76721653839824
At time: 291.0836372375488 and batch: 800, loss is 4.777323875427246 and perplexity is 118.78603807814466
At time: 291.4083776473999 and batch: 850, loss is 4.750446758270264 and perplexity is 115.63593429883493
At time: 291.7343850135803 and batch: 900, loss is 4.87385705947876 and perplexity is 130.8245430579184
At time: 292.05873823165894 and batch: 950, loss is 4.804606742858887 and perplexity is 122.07147610099283
At time: 292.4015510082245 and batch: 1000, loss is 4.768257293701172 and perplexity is 117.71392231283797
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.047026471393864 and perplexity of 155.55921630723284
Finished 40 epochs...
Completing Train Step...
At time: 293.70548582077026 and batch: 50, loss is 4.885467567443848 and perplexity is 132.352334511896
At time: 294.0314049720764 and batch: 100, loss is 4.829471731185913 and perplexity is 125.1448330771758
At time: 294.3585479259491 and batch: 150, loss is 4.86320839881897 and perplexity is 129.43882799196356
At time: 294.68348717689514 and batch: 200, loss is 4.882564992904663 and perplexity is 131.96872898667644
At time: 295.01163148880005 and batch: 250, loss is 4.859462308883667 and perplexity is 128.95484558797625
At time: 295.3383686542511 and batch: 300, loss is 4.755667486190796 and perplexity is 116.2412166821675
At time: 295.6627926826477 and batch: 350, loss is 4.819483489990234 and perplexity is 123.901078102592
At time: 295.98878479003906 and batch: 400, loss is 4.752835464477539 and perplexity is 115.91248474025888
At time: 296.31433272361755 and batch: 450, loss is 4.78530556678772 and perplexity is 119.73794543375305
At time: 296.64117527008057 and batch: 500, loss is 4.758629207611084 and perplexity is 116.58600110921543
At time: 296.9668781757355 and batch: 550, loss is 4.803492755889892 and perplexity is 121.93556578254724
At time: 297.2920925617218 and batch: 600, loss is 4.839547290802002 and perplexity is 126.41211084903595
At time: 297.61715388298035 and batch: 650, loss is 4.824042844772339 and perplexity is 124.46727684518808
At time: 297.9426255226135 and batch: 700, loss is 4.8048351383209225 and perplexity is 122.09935985631864
At time: 298.26983666419983 and batch: 750, loss is 4.749564189910888 and perplexity is 115.53392270474022
At time: 298.59487104415894 and batch: 800, loss is 4.774948558807373 and perplexity is 118.50421846559868
At time: 298.9221279621124 and batch: 850, loss is 4.748417348861694 and perplexity is 115.40149960824351
At time: 299.2473192214966 and batch: 900, loss is 4.87193380355835 and perplexity is 130.57317578023196
At time: 299.5738899707794 and batch: 950, loss is 4.8024475955963135 and perplexity is 121.8081901463899
At time: 299.8994872570038 and batch: 1000, loss is 4.766505193710327 and perplexity is 117.50785632745296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.045376289181593 and perplexity of 155.3027269408268
Finished 41 epochs...
Completing Train Step...
At time: 301.0249922275543 and batch: 50, loss is 4.8826667594909665 and perplexity is 131.98215967711042
At time: 301.36535716056824 and batch: 100, loss is 4.826573314666748 and perplexity is 124.78263637770759
At time: 301.6916661262512 and batch: 150, loss is 4.860631847381592 and perplexity is 129.10575147230503
At time: 302.0177755355835 and batch: 200, loss is 4.879719514846801 and perplexity is 131.5937486162391
At time: 302.3449022769928 and batch: 250, loss is 4.856796846389771 and perplexity is 128.611578969038
At time: 302.6713180541992 and batch: 300, loss is 4.753115892410278 and perplexity is 115.94499439682808
At time: 302.9959433078766 and batch: 350, loss is 4.816710329055786 and perplexity is 123.55795645872692
At time: 303.32306027412415 and batch: 400, loss is 4.750117673873901 and perplexity is 115.59788657799736
At time: 303.65007996559143 and batch: 450, loss is 4.783161554336548 and perplexity is 119.4815007963483
At time: 303.97786355018616 and batch: 500, loss is 4.756110248565673 and perplexity is 116.2926953148842
At time: 304.30409479141235 and batch: 550, loss is 4.801110401153564 and perplexity is 121.6454177648611
At time: 304.63041734695435 and batch: 600, loss is 4.837212610244751 and perplexity is 126.11732320302923
At time: 304.9565646648407 and batch: 650, loss is 4.821529893875122 and perplexity is 124.15488936176114
At time: 305.28411865234375 and batch: 700, loss is 4.802600889205933 and perplexity is 121.82686399479289
At time: 305.6095564365387 and batch: 750, loss is 4.747332754135132 and perplexity is 115.27640360182853
At time: 305.93652153015137 and batch: 800, loss is 4.772566289901733 and perplexity is 118.22224555176787
At time: 306.2620255947113 and batch: 850, loss is 4.746308908462525 and perplexity is 115.15843875405132
At time: 306.5896592140198 and batch: 900, loss is 4.870031900405884 and perplexity is 130.32507425291334
At time: 306.91581630706787 and batch: 950, loss is 4.800199575424195 and perplexity is 121.53467043191746
At time: 307.241108417511 and batch: 1000, loss is 4.76476942062378 and perplexity is 117.3040662707964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.043645347036967 and perplexity of 155.0341394273737
Finished 42 epochs...
Completing Train Step...
At time: 308.35386657714844 and batch: 50, loss is 4.87994532585144 and perplexity is 131.62346728809362
At time: 308.69408106803894 and batch: 100, loss is 4.823828601837159 and perplexity is 124.4406134667851
At time: 309.021080493927 and batch: 150, loss is 4.857976989746094 and perplexity is 128.76344866588516
At time: 309.349529504776 and batch: 200, loss is 4.876963300704956 and perplexity is 131.23154744661247
At time: 309.67757272720337 and batch: 250, loss is 4.854233169555664 and perplexity is 128.28228272836571
At time: 310.00455379486084 and batch: 300, loss is 4.750644855499267 and perplexity is 115.65884372606489
At time: 310.32983684539795 and batch: 350, loss is 4.814118909835815 and perplexity is 123.23818051130098
At time: 310.65517950057983 and batch: 400, loss is 4.747549819946289 and perplexity is 115.3014288838519
At time: 310.9814968109131 and batch: 450, loss is 4.7812242603302 and perplexity is 119.25025406975927
At time: 311.3082642555237 and batch: 500, loss is 4.7538578319549565 and perplexity is 116.03105049344151
At time: 311.63406896591187 and batch: 550, loss is 4.79878155708313 and perplexity is 121.36245417193474
At time: 311.9597752094269 and batch: 600, loss is 4.8350731563568115 and perplexity is 125.84778943590958
At time: 312.2861485481262 and batch: 650, loss is 4.819282083511353 and perplexity is 123.87612613555001
At time: 312.6135127544403 and batch: 700, loss is 4.800627164840698 and perplexity is 121.58664848257261
At time: 312.93982887268066 and batch: 750, loss is 4.7452597904205325 and perplexity is 115.03768731060116
At time: 313.2669234275818 and batch: 800, loss is 4.770438776016236 and perplexity is 117.9709934487213
At time: 313.5933668613434 and batch: 850, loss is 4.74424633026123 and perplexity is 114.92116025543132
At time: 313.9207019805908 and batch: 900, loss is 4.868214244842529 and perplexity is 130.08840331493732
At time: 314.2456772327423 and batch: 950, loss is 4.7980960083007815 and perplexity is 121.27928280152761
At time: 314.5720489025116 and batch: 1000, loss is 4.763103723526001 and perplexity is 117.1088358705467
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.042419805759337 and perplexity of 154.84425506938945
Finished 43 epochs...
Completing Train Step...
At time: 315.69606471061707 and batch: 50, loss is 4.877396287918091 and perplexity is 131.2883813319017
At time: 316.02267813682556 and batch: 100, loss is 4.821193113327026 and perplexity is 124.11308345017709
At time: 316.36428141593933 and batch: 150, loss is 4.855712566375733 and perplexity is 128.47220357901776
At time: 316.69061970710754 and batch: 200, loss is 4.874416065216065 and perplexity is 130.89769517238895
At time: 317.0158660411835 and batch: 250, loss is 4.851873025894165 and perplexity is 127.97987511498222
At time: 317.3407025337219 and batch: 300, loss is 4.748421325683593 and perplexity is 115.40195854036692
At time: 317.6665663719177 and batch: 350, loss is 4.811792402267456 and perplexity is 122.95179921493053
At time: 317.9932436943054 and batch: 400, loss is 4.745225715637207 and perplexity is 115.03376749311562
At time: 318.3199996948242 and batch: 450, loss is 4.779133634567261 and perplexity is 119.00120683935158
At time: 318.6464378833771 and batch: 500, loss is 4.7518421459198 and perplexity is 115.79740388353034
At time: 318.97306513786316 and batch: 550, loss is 4.796623163223266 and perplexity is 121.10078868620523
At time: 319.30056047439575 and batch: 600, loss is 4.832996101379394 and perplexity is 125.58666793419879
At time: 319.6270775794983 and batch: 650, loss is 4.8171436309814455 and perplexity is 123.61150595990424
At time: 319.95298075675964 and batch: 700, loss is 4.798764171600342 and perplexity is 121.36034424541776
At time: 320.27993297576904 and batch: 750, loss is 4.743161392211914 and perplexity is 114.79654552785193
At time: 320.60719203948975 and batch: 800, loss is 4.768094120025634 and perplexity is 117.69471606649175
At time: 320.93328523635864 and batch: 850, loss is 4.741691417694092 and perplexity is 114.62792149806934
At time: 321.259316444397 and batch: 900, loss is 4.865692672729492 and perplexity is 129.76078924974385
At time: 321.5841372013092 and batch: 950, loss is 4.794884929656982 and perplexity is 120.89047007479807
At time: 321.9106352329254 and batch: 1000, loss is 4.760367422103882 and perplexity is 116.78882881396036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.040089677019817 and perplexity of 154.48386805762948
Finished 44 epochs...
Completing Train Step...
At time: 323.0245096683502 and batch: 50, loss is 4.873375778198242 and perplexity is 130.76159480343566
At time: 323.3661620616913 and batch: 100, loss is 4.8173446941375735 and perplexity is 123.63636217817738
At time: 323.69403982162476 and batch: 150, loss is 4.851847410202026 and perplexity is 127.9765968638888
At time: 324.0193157196045 and batch: 200, loss is 4.871136417388916 and perplexity is 130.46910003555996
At time: 324.34534072875977 and batch: 250, loss is 4.848272714614868 and perplexity is 127.51993618556025
At time: 324.68665742874146 and batch: 300, loss is 4.7454430198669435 and perplexity is 115.0587675335633
At time: 325.01347160339355 and batch: 350, loss is 4.807826929092407 and perplexity is 122.46520258390663
At time: 325.3409049510956 and batch: 400, loss is 4.740632123947144 and perplexity is 114.50656114707947
At time: 325.6669132709503 and batch: 450, loss is 4.775355167388916 and perplexity is 118.55241309528688
At time: 325.99168944358826 and batch: 500, loss is 4.748638114929199 and perplexity is 115.4269791559025
At time: 326.3190715312958 and batch: 550, loss is 4.7929837608337404 and perplexity is 120.66085521952773
At time: 326.64541578292847 and batch: 600, loss is 4.829755277633667 and perplexity is 125.18032248125897
At time: 326.9719705581665 and batch: 650, loss is 4.813970375061035 and perplexity is 123.21987671532317
At time: 327.29754066467285 and batch: 700, loss is 4.7953205490112305 and perplexity is 120.94314377531556
At time: 327.6235296726227 and batch: 750, loss is 4.740067071914673 and perplexity is 114.44187725857445
At time: 327.9507384300232 and batch: 800, loss is 4.764872217178345 and perplexity is 117.31612534445054
At time: 328.2774467468262 and batch: 850, loss is 4.7391321563720705 and perplexity is 114.33493376815612
At time: 328.60529685020447 and batch: 900, loss is 4.863423871994018 and perplexity is 129.46672159225892
At time: 328.93194341659546 and batch: 950, loss is 4.792513608932495 and perplexity is 120.60413962255797
At time: 329.2586317062378 and batch: 1000, loss is 4.758125038146972 and perplexity is 116.52723682233757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.03808109934737 and perplexity of 154.17388662460368
Finished 45 epochs...
Completing Train Step...
At time: 330.3696618080139 and batch: 50, loss is 4.87053840637207 and perplexity is 130.39110140072663
At time: 330.7133197784424 and batch: 100, loss is 4.814265213012695 and perplexity is 123.2562119676202
At time: 331.0420677661896 and batch: 150, loss is 4.849530181884766 and perplexity is 127.68038919263269
At time: 331.37049889564514 and batch: 200, loss is 4.86874472618103 and perplexity is 130.1574310926065
At time: 331.6977572441101 and batch: 250, loss is 4.846044197082519 and perplexity is 127.2360721874742
At time: 332.02616596221924 and batch: 300, loss is 4.743292779922485 and perplexity is 114.8116293740446
At time: 332.3550589084625 and batch: 350, loss is 4.805531272888183 and perplexity is 122.18438703309981
At time: 332.68326902389526 and batch: 400, loss is 4.738237323760987 and perplexity is 114.23266890262643
At time: 333.00909757614136 and batch: 450, loss is 4.773311128616333 and perplexity is 118.31033485927252
At time: 333.3511846065521 and batch: 500, loss is 4.746433820724487 and perplexity is 115.17282435357033
At time: 333.6791515350342 and batch: 550, loss is 4.790481910705567 and perplexity is 120.35935715218315
At time: 334.0055720806122 and batch: 600, loss is 4.827606325149536 and perplexity is 124.91160475052754
At time: 334.3321568965912 and batch: 650, loss is 4.81209885597229 and perplexity is 122.9894840233456
At time: 334.6592683792114 and batch: 700, loss is 4.793387565612793 and perplexity is 120.70958848821115
At time: 334.98732471466064 and batch: 750, loss is 4.7380966949462895 and perplexity is 114.21660562730511
At time: 335.3145158290863 and batch: 800, loss is 4.7630332088470455 and perplexity is 117.1005782697269
At time: 335.6418421268463 and batch: 850, loss is 4.73729606628418 and perplexity is 114.12519713616277
At time: 335.96863555908203 and batch: 900, loss is 4.8615679264068605 and perplexity is 129.22666124001967
At time: 336.29531264305115 and batch: 950, loss is 4.790357065200806 and perplexity is 120.3443317654323
At time: 336.622677564621 and batch: 1000, loss is 4.756275043487549 and perplexity is 116.31186133971174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.03661625559737 and perplexity of 153.9482113002665
Finished 46 epochs...
Completing Train Step...
At time: 337.74916768074036 and batch: 50, loss is 4.8679836559295655 and perplexity is 130.0584098296432
At time: 338.0743613243103 and batch: 100, loss is 4.811829166412354 and perplexity is 122.95631951578424
At time: 338.4004147052765 and batch: 150, loss is 4.847450647354126 and perplexity is 127.41514929776628
At time: 338.72712326049805 and batch: 200, loss is 4.866423320770264 and perplexity is 129.85563336079082
At time: 339.05360078811646 and batch: 250, loss is 4.843974027633667 and perplexity is 126.97294441155292
At time: 339.3811123371124 and batch: 300, loss is 4.741286554336548 and perplexity is 114.58152224621556
At time: 339.7074820995331 and batch: 350, loss is 4.803371343612671 and perplexity is 121.92076220651744
At time: 340.0327744483948 and batch: 400, loss is 4.736158599853516 and perplexity is 113.99545735683778
At time: 340.3591706752777 and batch: 450, loss is 4.771416540145874 and perplexity is 118.08639766431038
At time: 340.6862919330597 and batch: 500, loss is 4.744266557693481 and perplexity is 114.92348483892468
At time: 341.0123448371887 and batch: 550, loss is 4.788067741394043 and perplexity is 120.06913974372627
At time: 341.33867359161377 and batch: 600, loss is 4.825335664749145 and perplexity is 124.62829468826747
At time: 341.6796998977661 and batch: 650, loss is 4.8100854110717775 and perplexity is 122.74210060393779
At time: 342.008558511734 and batch: 700, loss is 4.791574087142944 and perplexity is 120.49088261751557
At time: 342.3366951942444 and batch: 750, loss is 4.736242122650147 and perplexity is 114.00497897386995
At time: 342.6632559299469 and batch: 800, loss is 4.76116153717041 and perplexity is 116.88160941688524
At time: 342.98928332328796 and batch: 850, loss is 4.73548843383789 and perplexity is 113.9190870686257
At time: 343.3170552253723 and batch: 900, loss is 4.8598039436340335 and perplexity is 128.99890857075175
At time: 343.643798828125 and batch: 950, loss is 4.788444185256958 and perplexity is 120.11434754307378
At time: 343.96869230270386 and batch: 1000, loss is 4.7545687770843506 and perplexity is 116.11357153413847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.03516704280202 and perplexity of 153.72526916694392
Finished 47 epochs...
Completing Train Step...
At time: 345.0948305130005 and batch: 50, loss is 4.865498332977295 and perplexity is 129.7355740203563
At time: 345.4358284473419 and batch: 100, loss is 4.809489021301269 and perplexity is 122.66892029488984
At time: 345.76183700561523 and batch: 150, loss is 4.845068044662476 and perplexity is 127.11193098791536
At time: 346.0877821445465 and batch: 200, loss is 4.8640156173706055 and perplexity is 129.54335559785608
At time: 346.41467332839966 and batch: 250, loss is 4.841832847595215 and perplexity is 126.70136333330586
At time: 346.7412443161011 and batch: 300, loss is 4.739400186538696 and perplexity is 114.36558308679265
At time: 347.0670692920685 and batch: 350, loss is 4.8011058330535885 and perplexity is 121.64486207770038
At time: 347.3924376964569 and batch: 400, loss is 4.734046392440796 and perplexity is 113.75492941866389
At time: 347.72185587882996 and batch: 450, loss is 4.769419269561768 and perplexity is 117.85078254776722
At time: 348.04849433898926 and batch: 500, loss is 4.742042541503906 and perplexity is 114.66817715752349
At time: 348.37392830848694 and batch: 550, loss is 4.78559681892395 and perplexity is 119.77282444519516
At time: 348.6984329223633 and batch: 600, loss is 4.823181581497193 and perplexity is 124.36012390075541
At time: 349.02525210380554 and batch: 650, loss is 4.808055906295777 and perplexity is 122.493247534209
At time: 349.35076570510864 and batch: 700, loss is 4.789665422439575 and perplexity is 120.26112525741803
At time: 349.67650389671326 and batch: 750, loss is 4.734489135742187 and perplexity is 113.80530480252327
At time: 350.0025143623352 and batch: 800, loss is 4.759162645339966 and perplexity is 116.64820907142852
At time: 350.3419313430786 and batch: 850, loss is 4.7336796855926515 and perplexity is 113.7132223546363
At time: 350.66828751564026 and batch: 900, loss is 4.858046979904175 and perplexity is 128.77246115540143
At time: 350.9942436218262 and batch: 950, loss is 4.786574678421021 and perplexity is 119.89000272167787
At time: 351.31927132606506 and batch: 1000, loss is 4.753044567108154 and perplexity is 115.93672487998936
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0341923411299545 and perplexity of 153.5755058891636
Finished 48 epochs...
Completing Train Step...
At time: 352.43018436431885 and batch: 50, loss is 4.863547859191894 and perplexity is 129.48277480346306
At time: 352.7696831226349 and batch: 100, loss is 4.807547330856323 and perplexity is 122.43096631570567
At time: 353.0944068431854 and batch: 150, loss is 4.842602252960205 and perplexity is 126.79888555425515
At time: 353.422287940979 and batch: 200, loss is 4.861762571334839 and perplexity is 129.25181700233094
At time: 353.7488613128662 and batch: 250, loss is 4.839692640304565 and perplexity is 126.43048612185191
At time: 354.074898481369 and batch: 300, loss is 4.737669525146484 and perplexity is 114.16782616204378
At time: 354.3999207019806 and batch: 350, loss is 4.799110708236694 and perplexity is 121.40240733867955
At time: 354.7276165485382 and batch: 400, loss is 4.732018251419067 and perplexity is 113.52445217906107
At time: 355.05506801605225 and batch: 450, loss is 4.767535572052002 and perplexity is 117.6289962768618
At time: 355.38195276260376 and batch: 500, loss is 4.739822769165039 and perplexity is 114.41392220817382
At time: 355.7087278366089 and batch: 550, loss is 4.783287286758423 and perplexity is 119.49652443927239
At time: 356.0361135005951 and batch: 600, loss is 4.821001892089844 and perplexity is 124.0893526617969
At time: 356.363632440567 and batch: 650, loss is 4.80608172416687 and perplexity is 122.25166209930218
At time: 356.6941044330597 and batch: 700, loss is 4.787837991714477 and perplexity is 120.04155706602145
At time: 357.01953506469727 and batch: 750, loss is 4.732778577804566 and perplexity is 113.61080063779805
At time: 357.3455650806427 and batch: 800, loss is 4.7575047492980955 and perplexity is 116.45497868952009
At time: 357.67268323898315 and batch: 850, loss is 4.731884756088257 and perplexity is 113.5092982062772
At time: 357.998886346817 and batch: 900, loss is 4.8562964248657225 and perplexity is 128.54723506755946
At time: 358.3246886730194 and batch: 950, loss is 4.784694128036499 and perplexity is 119.66475539181738
At time: 358.66621232032776 and batch: 1000, loss is 4.750896396636963 and perplexity is 115.68794034254493
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.032591284775153 and perplexity of 153.32981958081282
Finished 49 epochs...
Completing Train Step...
At time: 359.79029726982117 and batch: 50, loss is 4.860907506942749 and perplexity is 129.14134561280594
At time: 360.1157581806183 and batch: 100, loss is 4.804878187179566 and perplexity is 122.10461620754066
At time: 360.44177889823914 and batch: 150, loss is 4.840564088821411 and perplexity is 126.54071180249422
At time: 360.7703721523285 and batch: 200, loss is 4.859886407852173 and perplexity is 129.00954680351862
At time: 361.09664845466614 and batch: 250, loss is 4.837268676757812 and perplexity is 126.12439435980357
At time: 361.4218316078186 and batch: 300, loss is 4.735463447570801 and perplexity is 113.91624069144997
At time: 361.74940729141235 and batch: 350, loss is 4.797029361724854 and perplexity is 121.14998963711041
At time: 362.07568621635437 and batch: 400, loss is 4.7300027561187745 and perplexity is 113.29587460511664
At time: 362.40124344825745 and batch: 450, loss is 4.765681943893433 and perplexity is 117.4111578152253
At time: 362.72729110717773 and batch: 500, loss is 4.737878446578979 and perplexity is 114.19168075961203
At time: 363.05424070358276 and batch: 550, loss is 4.7811298656463626 and perplexity is 119.23899801099323
At time: 363.38088035583496 and batch: 600, loss is 4.818861227035523 and perplexity is 123.82400303460676
At time: 363.70823669433594 and batch: 650, loss is 4.804144496917725 and perplexity is 122.01506209621033
At time: 364.034455537796 and batch: 700, loss is 4.786059484481812 and perplexity is 119.8282520270616
At time: 364.36050295829773 and batch: 750, loss is 4.730875539779663 and perplexity is 113.39480055751574
At time: 364.6882562637329 and batch: 800, loss is 4.755063161849976 and perplexity is 116.1709905073506
At time: 365.01552295684814 and batch: 850, loss is 4.729697036743164 and perplexity is 113.26124315509547
At time: 365.34351778030396 and batch: 900, loss is 4.854398279190064 and perplexity is 128.30346511782707
At time: 365.6704566478729 and batch: 950, loss is 4.782897024154663 and perplexity is 119.44989851327392
At time: 365.9962115287781 and batch: 1000, loss is 4.749384880065918 and perplexity is 115.51320819218473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.031368720822218 and perplexity of 153.14247861197538
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f4997a568d0>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'num_layers': 1, 'wordvec_source': 'glove', 'data': 'ptb', 'dropout': 0.06800583424510376, 'seq_len': 20, 'batch_size': 50, 'lr': 23.27018546471844, 'anneal': 6.218656461582015, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.6147031784057617 and batch: 50, loss is 6.826916103363037 and perplexity is 922.3420146007746
At time: 0.9657034873962402 and batch: 100, loss is 5.925924673080444 and perplexity is 374.6246806112011
At time: 1.2949674129486084 and batch: 150, loss is 5.829854164123535 and perplexity is 340.3090461846897
At time: 1.6257975101470947 and batch: 200, loss is 5.818160543441772 and perplexity is 336.3527779160838
At time: 1.9569032192230225 and batch: 250, loss is 5.842307119369507 and perplexity is 344.5733962789783
At time: 2.302496910095215 and batch: 300, loss is 5.801929769515991 and perplexity is 330.9375773468093
At time: 2.6316375732421875 and batch: 350, loss is 5.909807729721069 and perplexity is 368.6352709887149
At time: 2.9616031646728516 and batch: 400, loss is 5.896065464019776 and perplexity is 363.6040366065085
At time: 3.2911739349365234 and batch: 450, loss is 5.890244016647339 and perplexity is 361.49348403876485
At time: 3.622643232345581 and batch: 500, loss is 5.907119522094726 and perplexity is 367.64563361323866
At time: 3.955108165740967 and batch: 550, loss is 5.950977077484131 and perplexity is 384.1284790487413
At time: 4.283861398696899 and batch: 600, loss is 6.011183023452759 and perplexity is 407.96566785638913
At time: 4.611373424530029 and batch: 650, loss is 6.012473020553589 and perplexity is 408.492281977479
At time: 4.941870927810669 and batch: 700, loss is 6.049564418792724 and perplexity is 423.92833460696704
At time: 5.271533966064453 and batch: 750, loss is 5.967355728149414 and perplexity is 390.4717908575499
At time: 5.601628541946411 and batch: 800, loss is 6.0345995903015135 and perplexity is 417.63155243087306
At time: 5.932317733764648 and batch: 850, loss is 6.057417469024658 and perplexity is 427.27057131334215
At time: 6.262273550033569 and batch: 900, loss is 6.079968070983886 and perplexity is 437.0152410289561
At time: 6.590748310089111 and batch: 950, loss is 6.15290452003479 and perplexity is 470.0807648439465
At time: 6.923419952392578 and batch: 1000, loss is 6.119177150726318 and perplexity is 454.4905633639089
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.2283809010575455 and perplexity of 506.9340423061703
Finished 1 epochs...
Completing Train Step...
At time: 8.080620527267456 and batch: 50, loss is 6.225557909011841 and perplexity is 505.5049895883147
At time: 8.406626224517822 and batch: 100, loss is 6.302191534042358 and perplexity is 545.7666667021532
At time: 8.733325719833374 and batch: 150, loss is 6.286486043930053 and perplexity is 537.2620927779877
At time: 9.056803703308105 and batch: 200, loss is 6.277658605575562 and perplexity is 532.5403160318339
At time: 9.38032078742981 and batch: 250, loss is 6.2505433464050295 and perplexity is 518.2943615534856
At time: 9.702955484390259 and batch: 300, loss is 6.120048246383667 and perplexity is 454.8866406054982
At time: 10.02684998512268 and batch: 350, loss is 6.226273603439331 and perplexity is 505.86690618782995
At time: 10.35062313079834 and batch: 400, loss is 6.125457801818848 and perplexity is 457.35404286219836
At time: 10.688737869262695 and batch: 450, loss is 6.132246751785278 and perplexity is 460.46956015551393
At time: 11.012293100357056 and batch: 500, loss is 6.117295293807984 and perplexity is 453.63608141148245
At time: 11.337262392044067 and batch: 550, loss is 6.165528755187989 and perplexity is 476.0527917826022
At time: 11.6605224609375 and batch: 600, loss is 6.239169921875 and perplexity is 512.4329749501685
At time: 11.983866691589355 and batch: 650, loss is 6.22339879989624 and perplexity is 504.4147265792581
At time: 12.309947967529297 and batch: 700, loss is 6.244218263626099 and perplexity is 515.026452604953
At time: 12.635094404220581 and batch: 750, loss is 6.183735189437866 and perplexity is 484.7993962705785
At time: 12.95813775062561 and batch: 800, loss is 6.227123422622681 and perplexity is 506.29698430736477
At time: 13.281327724456787 and batch: 850, loss is 6.179301481246949 and perplexity is 482.65469521902685
At time: 13.603994131088257 and batch: 900, loss is 6.24461820602417 and perplexity is 515.2324747152203
At time: 13.928521633148193 and batch: 950, loss is 6.280362377166748 and perplexity is 533.9821317012536
At time: 14.251836061477661 and batch: 1000, loss is 6.168299922943115 and perplexity is 477.37384351150126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.218626441025153 and perplexity of 502.01321348327616
Finished 2 epochs...
Completing Train Step...
At time: 15.36296033859253 and batch: 50, loss is 6.283469352722168 and perplexity is 535.6437811468859
At time: 15.700500011444092 and batch: 100, loss is 6.3867017364501955 and perplexity is 593.8945251155508
At time: 16.022528886795044 and batch: 150, loss is 6.227789573669433 and perplexity is 506.6343669348478
At time: 16.346255779266357 and batch: 200, loss is 6.1723806762695315 and perplexity is 479.3258685688796
At time: 16.670501708984375 and batch: 250, loss is 6.192552366256714 and perplexity is 489.09285856188416
At time: 16.994798183441162 and batch: 300, loss is 6.1498722743988035 and perplexity is 468.6575233955237
At time: 17.319706916809082 and batch: 350, loss is 6.2414766120910645 and perplexity is 513.6163634104503
At time: 17.643243312835693 and batch: 400, loss is 6.246378803253174 and perplexity is 516.1403905850567
At time: 17.967153072357178 and batch: 450, loss is 6.207781867980957 and perplexity is 496.5985076730043
At time: 18.29095768928528 and batch: 500, loss is 6.177430353164673 and perplexity is 481.7524308541003
At time: 18.61297059059143 and batch: 550, loss is 6.1837420082092285 and perplexity is 484.802702018089
At time: 18.936925649642944 and batch: 600, loss is 6.243256378173828 and perplexity is 514.5312943336043
At time: 19.276861429214478 and batch: 650, loss is 6.18905556678772 and perplexity is 487.3855856489385
At time: 19.601963758468628 and batch: 700, loss is 6.233050279617309 and perplexity is 509.3066442319733
At time: 19.92675280570984 and batch: 750, loss is 6.148061046600342 and perplexity is 467.80944612360287
At time: 20.251355171203613 and batch: 800, loss is 6.242917337417603 and perplexity is 514.3568768234647
At time: 20.57598328590393 and batch: 850, loss is 6.193303270339966 and perplexity is 489.4602583101898
At time: 20.899043083190918 and batch: 900, loss is 6.204784593582153 and perplexity is 495.1122940870735
At time: 21.224629640579224 and batch: 950, loss is 6.2748620986938475 and perplexity is 531.0531437861665
At time: 21.54779624938965 and batch: 1000, loss is 6.181086463928223 and perplexity is 483.516994857036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.483852479516006 and perplexity of 654.487495351137
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 22.663731575012207 and batch: 50, loss is 6.0138517761230466 and perplexity is 409.0558814299959
At time: 23.003111362457275 and batch: 100, loss is 5.825321388244629 and perplexity is 338.7699922771077
At time: 23.328641176223755 and batch: 150, loss is 5.7456889152526855 and perplexity is 312.83907321890274
At time: 23.65422248840332 and batch: 200, loss is 5.731348009109497 and perplexity is 308.38469369227636
At time: 23.978355169296265 and batch: 250, loss is 5.7028375911712645 and perplexity is 299.7166688330692
At time: 24.301271438598633 and batch: 300, loss is 5.569429512023926 and perplexity is 262.2844264394765
At time: 24.6247980594635 and batch: 350, loss is 5.624402732849121 and perplexity is 277.10672834129514
At time: 24.95089054107666 and batch: 400, loss is 5.579418258666992 and perplexity is 264.9174475213991
At time: 25.275447845458984 and batch: 450, loss is 5.565280609130859 and perplexity is 261.19848810788153
At time: 25.60126495361328 and batch: 500, loss is 5.561235113143921 and perplexity is 260.14394518525125
At time: 25.9254093170166 and batch: 550, loss is 5.636083869934082 and perplexity is 280.36262931306624
At time: 26.248476266860962 and batch: 600, loss is 5.646098651885986 and perplexity is 283.1845065706367
At time: 26.573283672332764 and batch: 650, loss is 5.605816984176636 and perplexity is 272.0040576937998
At time: 26.89861536026001 and batch: 700, loss is 5.603025636672974 and perplexity is 271.2458585373314
At time: 27.22271156311035 and batch: 750, loss is 5.524419460296631 and perplexity is 250.7407308100635
At time: 27.560896635055542 and batch: 800, loss is 5.581472873687744 and perplexity is 265.462310438427
At time: 27.88465142250061 and batch: 850, loss is 5.54629298210144 and perplexity is 256.285736976743
At time: 28.210487365722656 and batch: 900, loss is 5.57299108505249 and perplexity is 263.2202370682292
At time: 28.53494429588318 and batch: 950, loss is 5.537372856140137 and perplexity is 254.00980182600722
At time: 28.858402013778687 and batch: 1000, loss is 5.47696662902832 and perplexity is 239.1202656750543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.610624732040778 and perplexity of 273.314933275843
Finished 4 epochs...
Completing Train Step...
At time: 29.98583459854126 and batch: 50, loss is 5.576837511062622 and perplexity is 264.2346439042226
At time: 30.31092095375061 and batch: 100, loss is 5.569366788864135 and perplexity is 262.2679756474146
At time: 30.635154724121094 and batch: 150, loss is 5.575468320846557 and perplexity is 263.8731039799882
At time: 30.959940195083618 and batch: 200, loss is 5.571708536148071 and perplexity is 262.8828606386881
At time: 31.28497314453125 and batch: 250, loss is 5.560239324569702 and perplexity is 259.885025752885
At time: 31.610077142715454 and batch: 300, loss is 5.455066499710083 and perplexity is 233.94042752282147
At time: 31.934674501419067 and batch: 350, loss is 5.504576015472412 and perplexity is 245.81421215894713
At time: 32.25752305984497 and batch: 400, loss is 5.444457025527954 and perplexity is 231.4715624351702
At time: 32.58221507072449 and batch: 450, loss is 5.4526606369018555 and perplexity is 233.37827545043822
At time: 32.90619397163391 and batch: 500, loss is 5.434294996261596 and perplexity is 229.1312529298794
At time: 33.229994773864746 and batch: 550, loss is 5.490210800170899 and perplexity is 242.30828010149628
At time: 33.55499505996704 and batch: 600, loss is 5.525463562011719 and perplexity is 251.00266635702593
At time: 33.8801007270813 and batch: 650, loss is 5.49004861831665 and perplexity is 242.26898528186913
At time: 34.20637655258179 and batch: 700, loss is 5.47908839225769 and perplexity is 239.62816088831457
At time: 34.53265142440796 and batch: 750, loss is 5.411203107833862 and perplexity is 223.90080251034559
At time: 34.86241126060486 and batch: 800, loss is 5.460771331787109 and perplexity is 235.27883243563042
At time: 35.18766736984253 and batch: 850, loss is 5.438529195785523 and perplexity is 230.10349725761793
At time: 35.511189460754395 and batch: 900, loss is 5.48525595664978 and perplexity is 241.11064997485684
At time: 35.83459544181824 and batch: 950, loss is 5.467806806564331 and perplexity is 236.93996731121922
At time: 36.173760175704956 and batch: 1000, loss is 5.420698394775391 and perplexity is 226.03693040075026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.570348600061928 and perplexity of 262.52559973113796
Finished 5 epochs...
Completing Train Step...
At time: 37.30172514915466 and batch: 50, loss is 5.525505456924439 and perplexity is 251.01318231210627
At time: 37.64014220237732 and batch: 100, loss is 5.523983125686645 and perplexity is 250.6313478166063
At time: 37.96446490287781 and batch: 150, loss is 5.520177268981934 and perplexity is 249.6792936611254
At time: 38.28994083404541 and batch: 200, loss is 5.525244026184082 and perplexity is 250.94756832714526
At time: 38.61519455909729 and batch: 250, loss is 5.5088334655761715 and perplexity is 246.86298486658495
At time: 38.93866205215454 and batch: 300, loss is 5.404941558837891 and perplexity is 222.5032167572827
At time: 39.26167702674866 and batch: 350, loss is 5.462969951629638 and perplexity is 235.79669022274638
At time: 39.58770680427551 and batch: 400, loss is 5.409250316619873 and perplexity is 223.46399762357976
At time: 39.91463828086853 and batch: 450, loss is 5.42079231262207 and perplexity is 226.05816029944106
At time: 40.23792338371277 and batch: 500, loss is 5.402491035461426 and perplexity is 221.9586349510245
At time: 40.56521534919739 and batch: 550, loss is 5.448266716003418 and perplexity is 232.35507933628733
At time: 40.887996435165405 and batch: 600, loss is 5.490230226516724 and perplexity is 242.31298731166368
At time: 41.214516162872314 and batch: 650, loss is 5.451324596405029 and perplexity is 233.06668082122832
At time: 41.54005789756775 and batch: 700, loss is 5.440073251724243 and perplexity is 230.45906436618912
At time: 41.86533999443054 and batch: 750, loss is 5.373729944229126 and perplexity is 215.66579071592182
At time: 42.189537525177 and batch: 800, loss is 5.417373609542847 and perplexity is 225.2866540974727
At time: 42.51423931121826 and batch: 850, loss is 5.404864311218262 and perplexity is 222.48602957727132
At time: 42.838449001312256 and batch: 900, loss is 5.459506597518921 and perplexity is 234.98145532482025
At time: 43.16321611404419 and batch: 950, loss is 5.444796552658081 and perplexity is 231.55016665384719
At time: 43.48819613456726 and batch: 1000, loss is 5.391265335083008 and perplexity is 219.4809268423444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.540281714462653 and perplexity of 254.74975604128062
Finished 6 epochs...
Completing Train Step...
At time: 44.61619973182678 and batch: 50, loss is 5.496220331192017 and perplexity is 243.7688234220776
At time: 44.95642018318176 and batch: 100, loss is 5.494555358886719 and perplexity is 243.3632927745153
At time: 45.28137278556824 and batch: 150, loss is 5.491491270065308 and perplexity is 242.61874728885536
At time: 45.605544567108154 and batch: 200, loss is 5.49522403717041 and perplexity is 243.52607894316364
At time: 45.93069934844971 and batch: 250, loss is 5.484802980422973 and perplexity is 241.00145731509483
At time: 46.25430989265442 and batch: 300, loss is 5.378275651931762 and perplexity is 216.64837594262426
At time: 46.580079793930054 and batch: 350, loss is 5.436766939163208 and perplexity is 229.69835293474048
At time: 46.90511894226074 and batch: 400, loss is 5.38277850151062 and perplexity is 217.6261106351048
At time: 47.2289400100708 and batch: 450, loss is 5.397165975570679 and perplexity is 220.77983330685177
At time: 47.55258774757385 and batch: 500, loss is 5.378735847473145 and perplexity is 216.74809950368996
At time: 47.87732434272766 and batch: 550, loss is 5.424496994018555 and perplexity is 226.89718696405072
At time: 48.20258021354675 and batch: 600, loss is 5.463391675949096 and perplexity is 235.89615239279487
At time: 48.52533268928528 and batch: 650, loss is 5.427559776306152 and perplexity is 227.593188956599
At time: 48.85006523132324 and batch: 700, loss is 5.421892099380493 and perplexity is 226.3069128330703
At time: 49.17503237724304 and batch: 750, loss is 5.354342584609985 and perplexity is 211.52487092736183
At time: 49.49997854232788 and batch: 800, loss is 5.3911026573181156 and perplexity is 219.44522507974932
At time: 49.823280811309814 and batch: 850, loss is 5.380836048126221 and perplexity is 217.20379235953817
At time: 50.148083448410034 and batch: 900, loss is 5.438640527725219 and perplexity is 230.129116552395
At time: 50.471771001815796 and batch: 950, loss is 5.4296025466918945 and perplexity is 228.0585847695504
At time: 50.796740770339966 and batch: 1000, loss is 5.374521894454956 and perplexity is 215.8366549366319
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.537183435951791 and perplexity of 253.96169179815269
Finished 7 epochs...
Completing Train Step...
At time: 51.92079710960388 and batch: 50, loss is 5.481698322296142 and perplexity is 240.2543904757058
At time: 52.24804139137268 and batch: 100, loss is 5.478037395477295 and perplexity is 239.37644476231824
At time: 52.57448697090149 and batch: 150, loss is 5.473406343460083 and perplexity is 238.2704429475645
At time: 52.90005564689636 and batch: 200, loss is 5.479932842254638 and perplexity is 239.8306003510173
At time: 53.23776841163635 and batch: 250, loss is 5.466845588684082 and perplexity is 236.71232580216565
At time: 53.5624577999115 and batch: 300, loss is 5.365385179519653 and perplexity is 213.8735985474395
At time: 53.88698220252991 and batch: 350, loss is 5.421857824325562 and perplexity is 226.29915628413048
At time: 54.21042561531067 and batch: 400, loss is 5.368995056152344 and perplexity is 214.64705104732005
At time: 54.53577756881714 and batch: 450, loss is 5.383181600570679 and perplexity is 217.71385319903058
At time: 54.86070370674133 and batch: 500, loss is 5.361954069137573 and perplexity is 213.14103209986163
At time: 55.185901165008545 and batch: 550, loss is 5.408669500350952 and perplexity is 223.33424378346035
At time: 55.5112829208374 and batch: 600, loss is 5.450445833206177 and perplexity is 232.8619603628207
At time: 55.83776569366455 and batch: 650, loss is 5.418336715698242 and perplexity is 225.50373357927518
At time: 56.161259174346924 and batch: 700, loss is 5.410651702880859 and perplexity is 223.77737653083545
At time: 56.48695707321167 and batch: 750, loss is 5.3403623676300045 and perplexity is 208.5882822373365
At time: 56.811514377593994 and batch: 800, loss is 5.375206575393677 and perplexity is 215.98448478250808
At time: 57.133994579315186 and batch: 850, loss is 5.367409267425537 and perplexity is 214.30693592016877
At time: 57.45845580101013 and batch: 900, loss is 5.427779083251953 and perplexity is 227.6431071972625
At time: 57.78357172012329 and batch: 950, loss is 5.417129964828491 and perplexity is 225.2317708812612
At time: 58.108198165893555 and batch: 1000, loss is 5.3626252460479735 and perplexity is 213.28413545772872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.530065396936928 and perplexity of 252.16040100214994
Finished 8 epochs...
Completing Train Step...
At time: 59.2173957824707 and batch: 50, loss is 5.46918399810791 and perplexity is 237.26650383059203
At time: 59.55966258049011 and batch: 100, loss is 5.465236520767212 and perplexity is 236.3317458647864
At time: 59.884300231933594 and batch: 150, loss is 5.462862138748169 and perplexity is 235.7712696724885
At time: 60.211482524871826 and batch: 200, loss is 5.466877746582031 and perplexity is 236.7199380953791
At time: 60.536038637161255 and batch: 250, loss is 5.45423828125 and perplexity is 233.74675395527026
At time: 60.86294770240784 and batch: 300, loss is 5.350149955749512 and perplexity is 210.6398821598984
At time: 61.18762969970703 and batch: 350, loss is 5.4055936241149904 and perplexity is 222.64835069223045
At time: 61.51085615158081 and batch: 400, loss is 5.35654109954834 and perplexity is 211.99042309010215
At time: 61.84990668296814 and batch: 450, loss is 5.3756804847717286 and perplexity is 216.08686611318035
At time: 62.17545747756958 and batch: 500, loss is 5.352038068771362 and perplexity is 211.03796976325523
At time: 62.50041103363037 and batch: 550, loss is 5.396963787078858 and perplexity is 220.73519867778717
At time: 62.82451629638672 and batch: 600, loss is 5.442718353271484 and perplexity is 231.0694589158151
At time: 63.14951729774475 and batch: 650, loss is 5.410920658111572 and perplexity is 223.83757072117706
At time: 63.47821664810181 and batch: 700, loss is 5.402309007644654 and perplexity is 221.91823598227012
At time: 63.80572152137756 and batch: 750, loss is 5.330393981933594 and perplexity is 206.51932301247112
At time: 64.13094663619995 and batch: 800, loss is 5.366755504608154 and perplexity is 214.1668758019941
At time: 64.45436692237854 and batch: 850, loss is 5.358151454925537 and perplexity is 212.33207802693892
At time: 64.77895545959473 and batch: 900, loss is 5.418182964324951 and perplexity is 225.4690647358146
At time: 65.10410046577454 and batch: 950, loss is 5.40855598449707 and perplexity is 223.3088932449474
At time: 65.42770028114319 and batch: 1000, loss is 5.354059677124024 and perplexity is 211.46503742198183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.528110038943407 and perplexity of 251.66781889053135
Finished 9 epochs...
Completing Train Step...
At time: 66.5563645362854 and batch: 50, loss is 5.463345022201538 and perplexity is 235.88514720996963
At time: 66.89642524719238 and batch: 100, loss is 5.455926628112793 and perplexity is 234.14173289089752
At time: 67.22192406654358 and batch: 150, loss is 5.457571516036987 and perplexity is 234.5271867271512
At time: 67.54749417304993 and batch: 200, loss is 5.462216215133667 and perplexity is 235.61902861512613
At time: 67.87227368354797 and batch: 250, loss is 5.447176132202149 and perplexity is 232.10181477883464
At time: 68.1974663734436 and batch: 300, loss is 5.344663944244385 and perplexity is 209.48747329734084
At time: 68.52268409729004 and batch: 350, loss is 5.400772552490235 and perplexity is 221.5775303712069
At time: 68.84826707839966 and batch: 400, loss is 5.351892185211182 and perplexity is 211.00718503844016
At time: 69.17544221878052 and batch: 450, loss is 5.365310440063476 and perplexity is 213.8576143483261
At time: 69.50273418426514 and batch: 500, loss is 5.3447425746917725 and perplexity is 209.5039460387094
At time: 69.8281192779541 and batch: 550, loss is 5.391499757766724 and perplexity is 219.53238418138724
At time: 70.16752123832703 and batch: 600, loss is 5.434714155197144 and perplexity is 229.22731547328837
At time: 70.49435997009277 and batch: 650, loss is 5.403151245117187 and perplexity is 222.10522256895456
At time: 70.82278490066528 and batch: 700, loss is 5.394859666824341 and perplexity is 220.2712335657362
At time: 71.14779686927795 and batch: 750, loss is 5.3230523586273195 and perplexity is 205.0086879793216
At time: 71.47356867790222 and batch: 800, loss is 5.359349813461304 and perplexity is 212.5866805071185
At time: 71.7979588508606 and batch: 850, loss is 5.349055643081665 and perplexity is 210.4095023452623
At time: 72.12428283691406 and batch: 900, loss is 5.412557430267334 and perplexity is 224.2042418210101
At time: 72.44840383529663 and batch: 950, loss is 5.401245594024658 and perplexity is 221.68237054108988
At time: 72.7729663848877 and batch: 1000, loss is 5.346260499954224 and perplexity is 209.82219885187013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.522721081245236 and perplexity of 250.3152394306241
Finished 10 epochs...
Completing Train Step...
At time: 73.90220642089844 and batch: 50, loss is 5.454608592987061 and perplexity is 233.8333291506758
At time: 74.22621440887451 and batch: 100, loss is 5.450001745223999 and perplexity is 232.7585721231537
At time: 74.55225086212158 and batch: 150, loss is 5.451982622146606 and perplexity is 233.22009516647114
At time: 74.88053369522095 and batch: 200, loss is 5.454153528213501 and perplexity is 233.7269440475875
At time: 75.20624899864197 and batch: 250, loss is 5.440837259292603 and perplexity is 230.63520411304393
At time: 75.53039789199829 and batch: 300, loss is 5.338970422744751 and perplexity is 208.29814082202796
At time: 75.8559672832489 and batch: 350, loss is 5.394356460571289 and perplexity is 220.16041958712023
At time: 76.18118810653687 and batch: 400, loss is 5.3434195137023925 and perplexity is 209.22694282706365
At time: 76.50634717941284 and batch: 450, loss is 5.358905410766601 and perplexity is 212.4922274026049
At time: 76.83036971092224 and batch: 500, loss is 5.337411394119263 and perplexity is 207.97365106798267
At time: 77.15448665618896 and batch: 550, loss is 5.383076238632202 and perplexity is 217.69091565381757
At time: 77.4782977104187 and batch: 600, loss is 5.427809963226318 and perplexity is 227.6501369191154
At time: 77.80504846572876 and batch: 650, loss is 5.398414869308471 and perplexity is 221.05573610887916
At time: 78.13313555717468 and batch: 700, loss is 5.388173351287842 and perplexity is 218.8033434514605
At time: 78.4722740650177 and batch: 750, loss is 5.318307752609253 and perplexity is 204.03830638413464
At time: 78.79606032371521 and batch: 800, loss is 5.351228265762329 and perplexity is 210.8671397589821
At time: 79.11922144889832 and batch: 850, loss is 5.343481912612915 and perplexity is 209.23999876768195
At time: 79.44575452804565 and batch: 900, loss is 5.406211462020874 and perplexity is 222.78595378679722
At time: 79.77004051208496 and batch: 950, loss is 5.3952494144439695 and perplexity is 220.3571004868182
At time: 80.0935549736023 and batch: 1000, loss is 5.340909805297851 and perplexity is 208.7025025815116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.525104429663681 and perplexity of 250.9125393648212
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 81.20686459541321 and batch: 50, loss is 5.43893651008606 and perplexity is 230.1972407928997
At time: 81.54637002944946 and batch: 100, loss is 5.399907817840576 and perplexity is 221.38600742330834
At time: 81.87010049819946 and batch: 150, loss is 5.3936804676055905 and perplexity is 220.0116429837883
At time: 82.19867968559265 and batch: 200, loss is 5.38979567527771 and perplexity is 219.15860145848086
At time: 82.52343106269836 and batch: 250, loss is 5.366337003707886 and perplexity is 214.07726552396235
At time: 82.84753823280334 and batch: 300, loss is 5.260569858551025 and perplexity is 192.5912097800141
At time: 83.17092442512512 and batch: 350, loss is 5.320773630142212 and perplexity is 204.54206070267975
At time: 83.49675607681274 and batch: 400, loss is 5.261279411315918 and perplexity is 192.72791189835525
At time: 83.82264447212219 and batch: 450, loss is 5.275332307815551 and perplexity is 195.4554170845125
At time: 84.14817214012146 and batch: 500, loss is 5.24626425743103 and perplexity is 189.8556900327655
At time: 84.47296929359436 and batch: 550, loss is 5.296474151611328 and perplexity is 199.6316965481442
At time: 84.79673051834106 and batch: 600, loss is 5.341022672653199 and perplexity is 208.72605961041708
At time: 85.12190127372742 and batch: 650, loss is 5.3034085941314695 and perplexity is 201.02084198104404
At time: 85.4459867477417 and batch: 700, loss is 5.286214942932129 and perplexity is 197.59410323337548
At time: 85.77038240432739 and batch: 750, loss is 5.2122280979156494 and perplexity is 183.5024644988076
At time: 86.09455633163452 and batch: 800, loss is 5.23720796585083 and perplexity is 188.14406373751044
At time: 86.41920304298401 and batch: 850, loss is 5.221818075180054 and perplexity is 185.27071414647781
At time: 86.74545216560364 and batch: 900, loss is 5.286266593933106 and perplexity is 197.60430943017244
At time: 87.08555436134338 and batch: 950, loss is 5.257473669052124 and perplexity is 191.99583307389125
At time: 87.40807104110718 and batch: 1000, loss is 5.224478788375855 and perplexity is 185.76432276468063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.423326073623285 and perplexity of 226.63166390372564
Finished 12 epochs...
Completing Train Step...
At time: 88.5310468673706 and batch: 50, loss is 5.369680404663086 and perplexity is 214.79420950574558
At time: 88.87017846107483 and batch: 100, loss is 5.352222766876221 and perplexity is 211.07695167615557
At time: 89.19431734085083 and batch: 150, loss is 5.357123098373413 and perplexity is 212.11383717721054
At time: 89.51953077316284 and batch: 200, loss is 5.358381452560425 and perplexity is 212.3809195191946
At time: 89.84572577476501 and batch: 250, loss is 5.340748977661133 and perplexity is 208.66894015019977
At time: 90.17065763473511 and batch: 300, loss is 5.240096683502197 and perplexity is 188.68834457377062
At time: 90.49542665481567 and batch: 350, loss is 5.29892593383789 and perplexity is 200.12175050089934
At time: 90.8216483592987 and batch: 400, loss is 5.243372716903687 and perplexity is 189.30750753858447
At time: 91.14629578590393 and batch: 450, loss is 5.258856143951416 and perplexity is 192.26144605321886
At time: 91.46994614601135 and batch: 500, loss is 5.230540456771851 and perplexity is 186.89378424116285
At time: 91.79429054260254 and batch: 550, loss is 5.282918138504028 and perplexity is 196.9437467569806
At time: 92.11937117576599 and batch: 600, loss is 5.32766300201416 and perplexity is 205.9560923242583
At time: 92.44496035575867 and batch: 650, loss is 5.291463356018067 and perplexity is 198.63388492560577
At time: 92.76904106140137 and batch: 700, loss is 5.276793699264527 and perplexity is 195.74126277504712
At time: 93.09295749664307 and batch: 750, loss is 5.203957233428955 and perplexity is 181.9909996586448
At time: 93.41736650466919 and batch: 800, loss is 5.229296855926513 and perplexity is 186.6615074328453
At time: 93.74251437187195 and batch: 850, loss is 5.217472534179688 and perplexity is 184.4673594316067
At time: 94.06692123413086 and batch: 900, loss is 5.285575447082519 and perplexity is 197.46778301938156
At time: 94.39206862449646 and batch: 950, loss is 5.260149211883545 and perplexity is 192.51021396593805
At time: 94.71639037132263 and batch: 1000, loss is 5.225268049240112 and perplexity is 185.9109971491758
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.413603619831364 and perplexity of 224.4389246989981
Finished 13 epochs...
Completing Train Step...
At time: 95.84144067764282 and batch: 50, loss is 5.353248109817505 and perplexity is 211.29348893212705
At time: 96.16658926010132 and batch: 100, loss is 5.33515627861023 and perplexity is 207.50517489446932
At time: 96.49479103088379 and batch: 150, loss is 5.340380983352661 and perplexity is 208.59216529509092
At time: 96.82086968421936 and batch: 200, loss is 5.34015811920166 and perplexity is 208.54568275910475
At time: 97.1458945274353 and batch: 250, loss is 5.323974828720093 and perplexity is 205.19788961575006
At time: 97.47249007225037 and batch: 300, loss is 5.223250150680542 and perplexity is 185.53622586820396
At time: 97.79954242706299 and batch: 350, loss is 5.281002016067505 and perplexity is 196.56673973624132
At time: 98.12491536140442 and batch: 400, loss is 5.224616413116455 and perplexity is 185.78989029073563
At time: 98.44933319091797 and batch: 450, loss is 5.238626613616943 and perplexity is 188.41116330854777
At time: 98.77428245544434 and batch: 500, loss is 5.2122525787353515 and perplexity is 183.5069568445438
At time: 99.09986805915833 and batch: 550, loss is 5.264935827255249 and perplexity is 193.4338952051359
At time: 99.42648935317993 and batch: 600, loss is 5.308410387039185 and perplexity is 202.02882536336344
At time: 99.75244855880737 and batch: 650, loss is 5.273567943572998 and perplexity is 195.11086658127073
At time: 100.07629871368408 and batch: 700, loss is 5.25879921913147 and perplexity is 192.25050191651923
At time: 100.40107655525208 and batch: 750, loss is 5.188232583999634 and perplexity is 179.15163748303289
At time: 100.72659635543823 and batch: 800, loss is 5.214279985427856 and perplexity is 183.87937747344353
At time: 101.05101180076599 and batch: 850, loss is 5.200258874893189 and perplexity is 181.31917478201822
At time: 101.37680387496948 and batch: 900, loss is 5.271042022705078 and perplexity is 194.61865387868798
At time: 101.70173168182373 and batch: 950, loss is 5.24384316444397 and perplexity is 189.39658774200146
At time: 102.0276288986206 and batch: 1000, loss is 5.208937902450561 and perplexity is 182.8996976763791
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.3958241532488564 and perplexity of 220.4837846650697
Finished 14 epochs...
Completing Train Step...
At time: 103.13656115531921 and batch: 50, loss is 5.332573671340942 and perplexity is 206.96996194128675
At time: 103.47660088539124 and batch: 100, loss is 5.315268602371216 and perplexity is 203.41914465604566
At time: 103.80234789848328 and batch: 150, loss is 5.31905463218689 and perplexity is 204.1907553516521
At time: 104.14182043075562 and batch: 200, loss is 5.318057012557984 and perplexity is 203.98715222219755
At time: 104.466872215271 and batch: 250, loss is 5.304370393753052 and perplexity is 201.21427675862802
At time: 104.79500031471252 and batch: 300, loss is 5.204761190414429 and perplexity is 182.13737142453155
At time: 105.12102913856506 and batch: 350, loss is 5.263446550369263 and perplexity is 193.14603298252322
At time: 105.4463357925415 and batch: 400, loss is 5.2071820163726805 and perplexity is 182.57882843094131
At time: 105.77024102210999 and batch: 450, loss is 5.222125616073608 and perplexity is 185.327701229936
At time: 106.0960259437561 and batch: 500, loss is 5.196669359207153 and perplexity is 180.6694934781494
At time: 106.42324995994568 and batch: 550, loss is 5.251587495803833 and perplexity is 190.86903186439443
At time: 106.75021171569824 and batch: 600, loss is 5.294064598083496 and perplexity is 199.1512523499188
At time: 107.07524609565735 and batch: 650, loss is 5.25946234703064 and perplexity is 192.37803086729878
At time: 107.40063047409058 and batch: 700, loss is 5.245170345306397 and perplexity is 189.64811814490952
At time: 107.72598481178284 and batch: 750, loss is 5.177709760665894 and perplexity is 177.27634046569634
At time: 108.05202889442444 and batch: 800, loss is 5.2038612556457515 and perplexity is 181.97353340413412
At time: 108.37713980674744 and batch: 850, loss is 5.1912461948394775 and perplexity is 179.69234513172438
At time: 108.70149898529053 and batch: 900, loss is 5.26200086593628 and perplexity is 192.86700651005887
At time: 109.02763509750366 and batch: 950, loss is 5.2350115394592285 and perplexity is 187.73127264906657
At time: 109.3535168170929 and batch: 1000, loss is 5.202020483016968 and perplexity is 181.63886961914747
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.387360735637386 and perplexity of 218.62561265339414
Finished 15 epochs...
Completing Train Step...
At time: 110.46212267875671 and batch: 50, loss is 5.321638355255127 and perplexity is 204.7190098543781
At time: 110.80396342277527 and batch: 100, loss is 5.303416595458985 and perplexity is 201.02245042107293
At time: 111.12811946868896 and batch: 150, loss is 5.307285661697388 and perplexity is 201.8017261597586
At time: 111.45335412025452 and batch: 200, loss is 5.305888833999634 and perplexity is 201.52004069803203
At time: 111.77865934371948 and batch: 250, loss is 5.293110647201538 and perplexity is 198.96136242432786
At time: 112.10438919067383 and batch: 300, loss is 5.1946978092193605 and perplexity is 180.3136454418493
At time: 112.4305830001831 and batch: 350, loss is 5.252812728881836 and perplexity is 191.10303424025307
At time: 112.76851081848145 and batch: 400, loss is 5.196530179977417 and perplexity is 180.64434978699072
At time: 113.09347987174988 and batch: 450, loss is 5.211849431991578 and perplexity is 183.43299152287312
At time: 113.41926455497742 and batch: 500, loss is 5.187991571426392 and perplexity is 179.1084648886606
At time: 113.74621081352234 and batch: 550, loss is 5.243286800384522 and perplexity is 189.2912435951657
At time: 114.0709023475647 and batch: 600, loss is 5.2865215682983395 and perplexity is 197.65469988740128
At time: 114.39662194252014 and batch: 650, loss is 5.252630586624146 and perplexity is 191.0682294719508
At time: 114.72278356552124 and batch: 700, loss is 5.238487377166748 and perplexity is 188.3849314332505
At time: 115.04796409606934 and batch: 750, loss is 5.1697650909423825 and perplexity is 175.87351834868056
At time: 115.37245154380798 and batch: 800, loss is 5.196519565582276 and perplexity is 180.64243236665814
At time: 115.69716310501099 and batch: 850, loss is 5.183974847793579 and perplexity is 178.3904786264357
At time: 116.02451920509338 and batch: 900, loss is 5.255789670944214 and perplexity is 191.6727845371736
At time: 116.34937644004822 and batch: 950, loss is 5.227970552444458 and perplexity is 186.41410172936168
At time: 116.67404913902283 and batch: 1000, loss is 5.194783639907837 and perplexity is 180.32912255037522
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.3793223311261436 and perplexity of 216.8752560048273
Finished 16 epochs...
Completing Train Step...
At time: 117.81330561637878 and batch: 50, loss is 5.309022903442383 and perplexity is 202.1526092387743
At time: 118.13790464401245 and batch: 100, loss is 5.291139707565308 and perplexity is 198.56960777824483
At time: 118.46276378631592 and batch: 150, loss is 5.295068702697754 and perplexity is 199.35132146969212
At time: 118.78910303115845 and batch: 200, loss is 5.294977293014527 and perplexity is 199.33309966138322
At time: 119.11687207221985 and batch: 250, loss is 5.279412412643433 and perplexity is 196.25452478841612
At time: 119.44377279281616 and batch: 300, loss is 5.182248640060425 and perplexity is 178.08280523320516
At time: 119.7692642211914 and batch: 350, loss is 5.2394529151916505 and perplexity is 188.5669120883513
At time: 120.09492373466492 and batch: 400, loss is 5.183732013702393 and perplexity is 178.34716459595663
At time: 120.42044305801392 and batch: 450, loss is 5.200787210464478 and perplexity is 181.4149974628488
At time: 120.74541330337524 and batch: 500, loss is 5.175808000564575 and perplexity is 176.93952376828264
At time: 121.08633422851562 and batch: 550, loss is 5.231582975387573 and perplexity is 187.08872608799317
At time: 121.41214609146118 and batch: 600, loss is 5.27066554069519 and perplexity is 194.54539724748116
At time: 121.73908829689026 and batch: 650, loss is 5.2388170337677 and perplexity is 188.4470440067648
At time: 122.06633710861206 and batch: 700, loss is 5.218584890365601 and perplexity is 184.6726670063753
At time: 122.39351725578308 and batch: 750, loss is 5.15159122467041 and perplexity is 172.70608597195286
At time: 122.71964073181152 and batch: 800, loss is 5.176502380371094 and perplexity is 177.06242966731122
At time: 123.04483270645142 and batch: 850, loss is 5.163927116394043 and perplexity is 174.84976445704112
At time: 123.37059664726257 and batch: 900, loss is 5.238345184326172 and perplexity is 188.35814634909704
At time: 123.69651365280151 and batch: 950, loss is 5.208390436172485 and perplexity is 182.79959366391685
At time: 124.02235722541809 and batch: 1000, loss is 5.176621608734131 and perplexity is 177.08354178951262
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.357640336199505 and perplexity of 212.22357885606309
Finished 17 epochs...
Completing Train Step...
At time: 125.13426971435547 and batch: 50, loss is 5.289629669189453 and perplexity is 198.2699863270994
At time: 125.47392773628235 and batch: 100, loss is 5.27074462890625 and perplexity is 194.56078410337088
At time: 125.79889154434204 and batch: 150, loss is 5.274447660446167 and perplexity is 195.28258442320802
At time: 126.1247808933258 and batch: 200, loss is 5.274381217956543 and perplexity is 195.2696097931567
At time: 126.45232844352722 and batch: 250, loss is 5.260039129257202 and perplexity is 192.48902310238128
At time: 126.77726793289185 and batch: 300, loss is 5.16470932006836 and perplexity is 174.98658608946852
At time: 127.10298204421997 and batch: 350, loss is 5.220991010665894 and perplexity is 185.1175466617208
At time: 127.4272096157074 and batch: 400, loss is 5.165096855163574 and perplexity is 175.0544126745121
At time: 127.75303769111633 and batch: 450, loss is 5.183303651809692 and perplexity is 178.27078382744565
At time: 128.0790820121765 and batch: 500, loss is 5.160105905532837 and perplexity is 174.18290156133006
At time: 128.40507292747498 and batch: 550, loss is 5.212302103042602 and perplexity is 183.51604512450072
At time: 128.7299771308899 and batch: 600, loss is 5.255741968154907 and perplexity is 191.66364142879453
At time: 129.0561385154724 and batch: 650, loss is 5.225176744461059 and perplexity is 185.89402336156334
At time: 129.38149571418762 and batch: 700, loss is 5.20833004951477 and perplexity is 182.78855534071087
At time: 129.723712682724 and batch: 750, loss is 5.141843357086182 and perplexity is 171.03074866401326
At time: 130.0491783618927 and batch: 800, loss is 5.168651237487793 and perplexity is 175.67773008265763
At time: 130.37229180335999 and batch: 850, loss is 5.156123743057251 and perplexity is 173.49065617841777
At time: 130.69552731513977 and batch: 900, loss is 5.230252237319946 and perplexity is 186.83992557903431
At time: 131.0219383239746 and batch: 950, loss is 5.2008786869049075 and perplexity is 181.4315934201152
At time: 131.34877800941467 and batch: 1000, loss is 5.167908821105957 and perplexity is 175.5473524611501
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.348739996189025 and perplexity of 210.3430977204231
Finished 18 epochs...
Completing Train Step...
At time: 132.4599413871765 and batch: 50, loss is 5.280339469909668 and perplexity is 196.43654833174054
At time: 132.80164885520935 and batch: 100, loss is 5.2612074279785155 and perplexity is 192.71403919935372
At time: 133.12662601470947 and batch: 150, loss is 5.263601312637329 and perplexity is 193.17592701382986
At time: 133.451180934906 and batch: 200, loss is 5.2642458724975585 and perplexity is 193.30048059915418
At time: 133.77576541900635 and batch: 250, loss is 5.2513315296173095 and perplexity is 190.82018209839347
At time: 134.10187125205994 and batch: 300, loss is 5.155476875305176 and perplexity is 173.3784669573562
At time: 134.4265341758728 and batch: 350, loss is 5.213987169265747 and perplexity is 183.82554250210018
At time: 134.7537853717804 and batch: 400, loss is 5.156850414276123 and perplexity is 173.61677266206559
At time: 135.07862901687622 and batch: 450, loss is 5.177255239486694 and perplexity is 177.19578292332508
At time: 135.4056041240692 and batch: 500, loss is 5.15332859992981 and perplexity is 173.0064020581961
At time: 135.7338833808899 and batch: 550, loss is 5.204526262283325 and perplexity is 182.09458725805763
At time: 136.05805945396423 and batch: 600, loss is 5.248473415374756 and perplexity is 190.27557486375133
At time: 136.38303899765015 and batch: 650, loss is 5.219421377182007 and perplexity is 184.8272078843691
At time: 136.71249055862427 and batch: 700, loss is 5.203099927902222 and perplexity is 181.835044628928
At time: 137.041512966156 and batch: 750, loss is 5.1368035793304445 and perplexity is 170.17096009284626
At time: 137.3669674396515 and batch: 800, loss is 5.164288330078125 and perplexity is 174.912933992785
At time: 137.6909511089325 and batch: 850, loss is 5.150119018554688 and perplexity is 172.45201408494484
At time: 138.03033804893494 and batch: 900, loss is 5.2236004829406735 and perplexity is 185.6012365805586
At time: 138.3545627593994 and batch: 950, loss is 5.193464031219483 and perplexity is 180.09131561405317
At time: 138.6800274848938 and batch: 1000, loss is 5.160603666305542 and perplexity is 174.26962455886584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.3416357273008765 and perplexity of 208.8540593227531
Finished 19 epochs...
Completing Train Step...
At time: 139.80431938171387 and batch: 50, loss is 5.273018140792846 and perplexity is 195.00362356834128
At time: 140.12820553779602 and batch: 100, loss is 5.253673658370972 and perplexity is 191.26763132096752
At time: 140.4537501335144 and batch: 150, loss is 5.2548930454254155 and perplexity is 191.50100285072875
At time: 140.7795717716217 and batch: 200, loss is 5.2558604335784915 and perplexity is 191.68634828822448
At time: 141.10588455200195 and batch: 250, loss is 5.244495449066162 and perplexity is 189.52016852422145
At time: 141.43105554580688 and batch: 300, loss is 5.147493600845337 and perplexity is 171.99984933355
At time: 141.75474977493286 and batch: 350, loss is 5.205484094619751 and perplexity is 182.26908689938003
At time: 142.0800015926361 and batch: 400, loss is 5.147178211212158 and perplexity is 171.94561091772854
At time: 142.4049391746521 and batch: 450, loss is 5.167220649719238 and perplexity is 175.42658735448347
At time: 142.7322599887848 and batch: 500, loss is 5.145155715942383 and perplexity is 171.5982031666776
At time: 143.05793523788452 and batch: 550, loss is 5.196545543670655 and perplexity is 180.64712517268597
At time: 143.3817126750946 and batch: 600, loss is 5.239565773010254 and perplexity is 188.5881945396333
At time: 143.70658421516418 and batch: 650, loss is 5.2121243572235105 and perplexity is 183.48342881353594
At time: 144.03368496894836 and batch: 700, loss is 5.195658483505249 and perplexity is 180.48695135635933
At time: 144.35893487930298 and batch: 750, loss is 5.128046016693116 and perplexity is 168.6871838655646
At time: 144.6828944683075 and batch: 800, loss is 5.1560741138458255 and perplexity is 173.4820461876172
At time: 145.0087113380432 and batch: 850, loss is 5.143016023635864 and perplexity is 171.23142834411573
At time: 145.3353877067566 and batch: 900, loss is 5.218462991714477 and perplexity is 184.65015702936367
At time: 145.66229367256165 and batch: 950, loss is 5.187756433486938 and perplexity is 179.06635464434004
At time: 145.98783493041992 and batch: 1000, loss is 5.155690841674804 and perplexity is 173.41556808755834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.337133268030678 and perplexity of 207.91581621294628
Finished 20 epochs...
Completing Train Step...
At time: 147.11662197113037 and batch: 50, loss is 5.2679398822784425 and perplexity is 194.01585495158216
At time: 147.45684361457825 and batch: 100, loss is 5.2475746154785154 and perplexity is 190.10463203001524
At time: 147.78255343437195 and batch: 150, loss is 5.248554201126098 and perplexity is 190.2909470399469
At time: 148.1084921360016 and batch: 200, loss is 5.250276546478272 and perplexity is 190.61897617678065
At time: 148.43450689315796 and batch: 250, loss is 5.239796743392945 and perplexity is 188.63175785782155
At time: 148.75820636749268 and batch: 300, loss is 5.141275959014893 and perplexity is 170.9337336727524
At time: 149.08367681503296 and batch: 350, loss is 5.199519577026368 and perplexity is 181.18517544170456
At time: 149.40809559822083 and batch: 400, loss is 5.141575717926026 and perplexity is 170.98498026306137
At time: 149.73418521881104 and batch: 450, loss is 5.161388540267945 and perplexity is 174.40645794107476
At time: 150.05804347991943 and batch: 500, loss is 5.138439159393311 and perplexity is 170.44951606064905
At time: 150.38303327560425 and batch: 550, loss is 5.191004037857056 and perplexity is 179.64883664381767
At time: 150.70829153060913 and batch: 600, loss is 5.234204988479615 and perplexity is 187.57991885268862
At time: 151.034814119339 and batch: 650, loss is 5.206263389587402 and perplexity is 182.41118364200597
At time: 151.36189889907837 and batch: 700, loss is 5.189265756607056 and perplexity is 179.3368276977103
At time: 151.6891586780548 and batch: 750, loss is 5.122092390060425 and perplexity is 167.685867046045
At time: 152.01320672035217 and batch: 800, loss is 5.150205469131469 and perplexity is 172.4669233054757
At time: 152.33914065361023 and batch: 850, loss is 5.137676448822021 and perplexity is 170.31956197785453
At time: 152.665030002594 and batch: 900, loss is 5.2144302558898925 and perplexity is 183.907011188669
At time: 152.99212169647217 and batch: 950, loss is 5.181125583648682 and perplexity is 177.8829204589144
At time: 153.31878328323364 and batch: 1000, loss is 5.15072940826416 and perplexity is 172.5573091519636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.333306661466273 and perplexity of 207.12172449306098
Finished 21 epochs...
Completing Train Step...
At time: 154.44774961471558 and batch: 50, loss is 5.263988342285156 and perplexity is 193.2507062947967
At time: 154.78732538223267 and batch: 100, loss is 5.2413756370544435 and perplexity is 188.92982258899747
At time: 155.11228013038635 and batch: 150, loss is 5.243217658996582 and perplexity is 189.27815618830468
At time: 155.45419359207153 and batch: 200, loss is 5.245333070755005 and perplexity is 189.67898123104902
At time: 155.7795672416687 and batch: 250, loss is 5.23538987159729 and perplexity is 187.80231086000066
At time: 156.10425209999084 and batch: 300, loss is 5.136295318603516 and perplexity is 170.08449085328803
At time: 156.42838883399963 and batch: 350, loss is 5.1947057342529295 and perplexity is 180.31507443920478
At time: 156.75431299209595 and batch: 400, loss is 5.137357149124146 and perplexity is 170.265187674484
At time: 157.08179998397827 and batch: 450, loss is 5.157927103042603 and perplexity is 173.80380456036704
At time: 157.40557050704956 and batch: 500, loss is 5.134385986328125 and perplexity is 169.76005287387406
At time: 157.73007655143738 and batch: 550, loss is 5.187008600234986 and perplexity is 178.93249292939046
At time: 158.05553722381592 and batch: 600, loss is 5.230684223175049 and perplexity is 186.920655219829
At time: 158.38221049308777 and batch: 650, loss is 5.2030259704589845 and perplexity is 181.82159707121596
At time: 158.70729184150696 and batch: 700, loss is 5.184597263336181 and perplexity is 178.5015461944897
At time: 159.0315911769867 and batch: 750, loss is 5.118526411056519 and perplexity is 167.08896766276555
At time: 159.3574047088623 and batch: 800, loss is 5.146365728378296 and perplexity is 171.80596479821938
At time: 159.68421506881714 and batch: 850, loss is 5.134141693115234 and perplexity is 169.7185867102944
At time: 160.00973796844482 and batch: 900, loss is 5.210798826217651 and perplexity is 183.24037696155264
At time: 160.3357982635498 and batch: 950, loss is 5.177792568206787 and perplexity is 177.29102089132536
At time: 160.66127252578735 and batch: 1000, loss is 5.147650556564331 and perplexity is 172.02684781229647
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.330895400628811 and perplexity of 206.62290162791007
Finished 22 epochs...
Completing Train Step...
At time: 161.81383728981018 and batch: 50, loss is 5.260577030181885 and perplexity is 192.59259097803024
At time: 162.1421835422516 and batch: 100, loss is 5.237103242874145 and perplexity is 188.12436176275295
At time: 162.46818017959595 and batch: 150, loss is 5.239253873825073 and perplexity is 188.52938320750178
At time: 162.79697394371033 and batch: 200, loss is 5.241646223068237 and perplexity is 188.98095127362004
At time: 163.12377619743347 and batch: 250, loss is 5.2324028491973875 and perplexity is 187.24217813168437
At time: 163.44663667678833 and batch: 300, loss is 5.133018636703492 and perplexity is 169.5280901525099
At time: 163.78723335266113 and batch: 350, loss is 5.1910476398468015 and perplexity is 179.65666986132157
At time: 164.1126298904419 and batch: 400, loss is 5.1341792011260985 and perplexity is 169.72495263627445
At time: 164.43853878974915 and batch: 450, loss is 5.155156593322754 and perplexity is 173.32294584992965
At time: 164.76520252227783 and batch: 500, loss is 5.130842180252075 and perplexity is 169.15952087997817
At time: 165.09100580215454 and batch: 550, loss is 5.184137554168701 and perplexity is 178.41950625599537
At time: 165.41688227653503 and batch: 600, loss is 5.227945079803467 and perplexity is 186.40935333035
At time: 165.74262475967407 and batch: 650, loss is 5.199927930831909 and perplexity is 181.25917820623079
At time: 166.06765222549438 and batch: 700, loss is 5.181069240570069 and perplexity is 177.87289826988618
At time: 166.39249920845032 and batch: 750, loss is 5.115171785354614 and perplexity is 166.52938583590046
At time: 166.71741318702698 and batch: 800, loss is 5.142806844711304 and perplexity is 171.19561408400855
At time: 167.04478192329407 and batch: 850, loss is 5.130612163543701 and perplexity is 169.12061583837166
At time: 167.37162971496582 and batch: 900, loss is 5.208134746551513 and perplexity is 182.75285968005102
At time: 167.69661164283752 and batch: 950, loss is 5.1752291011810305 and perplexity is 176.8371232297126
At time: 168.02122235298157 and batch: 1000, loss is 5.145233564376831 and perplexity is 171.6115623381368
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.327746507598133 and perplexity of 205.97329152612534
Finished 23 epochs...
Completing Train Step...
At time: 169.13443994522095 and batch: 50, loss is 5.2574599170684815 and perplexity is 191.99319276849016
At time: 169.4779667854309 and batch: 100, loss is 5.233012771606445 and perplexity is 187.35641616667493
At time: 169.80578112602234 and batch: 150, loss is 5.235466575622558 and perplexity is 187.8167166056806
At time: 170.13421535491943 and batch: 200, loss is 5.238604965209961 and perplexity is 188.407084551154
At time: 170.4616084098816 and batch: 250, loss is 5.229762659072876 and perplexity is 186.74847520367052
At time: 170.78751373291016 and batch: 300, loss is 5.12985071182251 and perplexity is 168.99188767071476
At time: 171.11579942703247 and batch: 350, loss is 5.187644634246826 and perplexity is 179.04633628100098
At time: 171.44218516349792 and batch: 400, loss is 5.130898275375366 and perplexity is 169.16901017030682
At time: 171.7689175605774 and batch: 450, loss is 5.152199077606201 and perplexity is 172.81109778597622
At time: 172.09316873550415 and batch: 500, loss is 5.127840814590454 and perplexity is 168.65257245203216
At time: 172.43518948554993 and batch: 550, loss is 5.181213731765747 and perplexity is 177.8986011945144
At time: 172.76217818260193 and batch: 600, loss is 5.225257492065429 and perplexity is 185.90903446466365
At time: 173.09022688865662 and batch: 650, loss is 5.197033290863037 and perplexity is 180.73525679202865
At time: 173.4169795513153 and batch: 700, loss is 5.1772197723388675 and perplexity is 177.1894984057455
At time: 173.74281406402588 and batch: 750, loss is 5.111244440078735 and perplexity is 165.8766500375549
At time: 174.0687472820282 and batch: 800, loss is 5.138634252548218 and perplexity is 170.48277283846897
At time: 174.39711356163025 and batch: 850, loss is 5.1263315296173095 and perplexity is 168.3982196524607
At time: 174.72415924072266 and batch: 900, loss is 5.203686456680298 and perplexity is 181.94172739866463
At time: 175.04947304725647 and batch: 950, loss is 5.170152969360352 and perplexity is 175.94174912250722
At time: 175.3760154247284 and batch: 1000, loss is 5.1400179672241215 and perplexity is 170.71883563734886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.320204944145389 and perplexity of 204.425773565564
Finished 24 epochs...
Completing Train Step...
At time: 176.489910364151 and batch: 50, loss is 5.247950067520142 and perplexity is 190.17602060288766
At time: 176.83155012130737 and batch: 100, loss is 5.220465526580811 and perplexity is 185.02029589118445
At time: 177.1574203968048 and batch: 150, loss is 5.221828870773315 and perplexity is 185.27271426454715
At time: 177.4830174446106 and batch: 200, loss is 5.222406606674195 and perplexity is 185.37978388903605
At time: 177.80783867835999 and batch: 250, loss is 5.209583606719971 and perplexity is 183.01783492880196
At time: 178.13171935081482 and batch: 300, loss is 5.107114543914795 and perplexity is 165.19300935108527
At time: 178.45840454101562 and batch: 350, loss is 5.16502311706543 and perplexity is 175.04150497095037
At time: 178.78500866889954 and batch: 400, loss is 5.106150894165039 and perplexity is 165.03389782516658
At time: 179.11023211479187 and batch: 450, loss is 5.1270937252044675 and perplexity is 168.52662095960378
At time: 179.43495202064514 and batch: 500, loss is 5.102384643554688 and perplexity is 164.4135078122204
At time: 179.76011443138123 and batch: 550, loss is 5.155661354064941 and perplexity is 173.4104545523357
At time: 180.08518815040588 and batch: 600, loss is 5.199861459732055 and perplexity is 181.24713010972638
At time: 180.40884804725647 and batch: 650, loss is 5.170761432647705 and perplexity is 176.048835793411
At time: 180.74867939949036 and batch: 700, loss is 5.150154819488526 and perplexity is 172.45818813860907
At time: 181.07354164123535 and batch: 750, loss is 5.0846345996856686 and perplexity is 161.520908662565
At time: 181.40062808990479 and batch: 800, loss is 5.108893957138061 and perplexity is 165.4872176577618
At time: 181.72535634040833 and batch: 850, loss is 5.0967523384094235 and perplexity is 163.49008371051497
At time: 182.05161380767822 and batch: 900, loss is 5.1781697082519536 and perplexity is 177.35789704499712
At time: 182.37649726867676 and batch: 950, loss is 5.140011339187622 and perplexity is 170.71770411042507
At time: 182.70314049720764 and batch: 1000, loss is 5.112625913619995 and perplexity is 166.10596259884298
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.293611852134147 and perplexity of 199.0611078349313
Finished 25 epochs...
Completing Train Step...
At time: 183.8422191143036 and batch: 50, loss is 5.222721767425537 and perplexity is 185.43821752852142
At time: 184.16977953910828 and batch: 100, loss is 5.193727598190308 and perplexity is 180.13878799238216
At time: 184.49584126472473 and batch: 150, loss is 5.197017955780029 and perplexity is 180.73248522311448
At time: 184.8244833946228 and batch: 200, loss is 5.202354888916016 and perplexity is 181.69962088586942
At time: 185.15194153785706 and batch: 250, loss is 5.189501361846924 and perplexity is 179.37908537188642
At time: 185.47763967514038 and batch: 300, loss is 5.088346920013428 and perplexity is 162.1216403795271
At time: 185.8042049407959 and batch: 350, loss is 5.148797416687012 and perplexity is 172.22425171978074
At time: 186.12897872924805 and batch: 400, loss is 5.089480504989624 and perplexity is 162.30552323911016
At time: 186.45347023010254 and batch: 450, loss is 5.1117321968078615 and perplexity is 165.9575772246012
At time: 186.7784252166748 and batch: 500, loss is 5.089420356750488 and perplexity is 162.29576114127474
At time: 187.10567712783813 and batch: 550, loss is 5.140405149459839 and perplexity is 170.78494773569906
At time: 187.43452715873718 and batch: 600, loss is 5.185147476196289 and perplexity is 178.5997870649718
At time: 187.75652599334717 and batch: 650, loss is 5.155910577774048 and perplexity is 173.4536779349392
At time: 188.0809600353241 and batch: 700, loss is 5.136308755874634 and perplexity is 170.0867763400599
At time: 188.40831971168518 and batch: 750, loss is 5.07079852104187 and perplexity is 159.3014821541457
At time: 188.73303413391113 and batch: 800, loss is 5.097347850799561 and perplexity is 163.58747307643802
At time: 189.05758929252625 and batch: 850, loss is 5.0854540538787845 and perplexity is 161.65332189432377
At time: 189.39735317230225 and batch: 900, loss is 5.168083057403565 and perplexity is 175.577941846711
At time: 189.72212290763855 and batch: 950, loss is 5.129073400497436 and perplexity is 168.8605794028961
At time: 190.04894304275513 and batch: 1000, loss is 5.102107400894165 and perplexity is 164.36793169200305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.2837703053544205 and perplexity of 197.11164721880218
Finished 26 epochs...
Completing Train Step...
At time: 191.16225290298462 and batch: 50, loss is 5.212282905578613 and perplexity is 183.51252211564955
At time: 191.507554769516 and batch: 100, loss is 5.182195835113525 and perplexity is 178.07340182840645
At time: 191.83747172355652 and batch: 150, loss is 5.182866010665894 and perplexity is 178.19278226729656
At time: 192.16292667388916 and batch: 200, loss is 5.189867067337036 and perplexity is 179.44469728480536
At time: 192.4884283542633 and batch: 250, loss is 5.178774309158325 and perplexity is 177.4651602127395
At time: 192.81444764137268 and batch: 300, loss is 5.076649398803711 and perplexity is 160.23626764463896
At time: 193.13828039169312 and batch: 350, loss is 5.138653860092163 and perplexity is 170.48611561970117
At time: 193.4639208316803 and batch: 400, loss is 5.078510637283325 and perplexity is 160.53478327001665
At time: 193.78896641731262 and batch: 450, loss is 5.100627508163452 and perplexity is 164.12486468563898
At time: 194.11477899551392 and batch: 500, loss is 5.078725252151489 and perplexity is 160.56924011870765
At time: 194.4420919418335 and batch: 550, loss is 5.129409637451172 and perplexity is 168.91736611608127
At time: 194.7665662765503 and batch: 600, loss is 5.1735697364807125 and perplexity is 176.54392927484454
At time: 195.09296417236328 and batch: 650, loss is 5.143324241638184 and perplexity is 171.28421308708045
At time: 195.42006826400757 and batch: 700, loss is 5.126179857254028 and perplexity is 168.37268023337464
At time: 195.74587178230286 and batch: 750, loss is 5.059722232818603 and perplexity is 157.5467489286873
At time: 196.0712389945984 and batch: 800, loss is 5.088289470672607 and perplexity is 162.11232686568474
At time: 196.39454221725464 and batch: 850, loss is 5.076878509521484 and perplexity is 160.2729836967923
At time: 196.72111678123474 and batch: 900, loss is 5.159922132492065 and perplexity is 174.1508943809782
At time: 197.04721927642822 and batch: 950, loss is 5.1190762424469 and perplexity is 167.18086368351618
At time: 197.3709840774536 and batch: 1000, loss is 5.092081880569458 and perplexity is 162.72829051355737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.27302104670827 and perplexity of 195.00419023320205
Finished 27 epochs...
Completing Train Step...
At time: 198.4979224205017 and batch: 50, loss is 5.20151535987854 and perplexity is 181.54714279188673
At time: 198.8380389213562 and batch: 100, loss is 5.1703627395629885 and perplexity is 175.9786603301733
At time: 199.16281461715698 and batch: 150, loss is 5.1733121585845945 and perplexity is 176.49846131699135
At time: 199.489572763443 and batch: 200, loss is 5.18188307762146 and perplexity is 178.0177167462644
At time: 199.815495967865 and batch: 250, loss is 5.168009634017944 and perplexity is 175.56505079303858
At time: 200.14134168624878 and batch: 300, loss is 5.067624959945679 and perplexity is 158.79673052152134
At time: 200.46805334091187 and batch: 350, loss is 5.1280027103424075 and perplexity is 168.67987879739923
At time: 200.79549407958984 and batch: 400, loss is 5.0672966003417965 and perplexity is 158.74459664978664
At time: 201.12046122550964 and batch: 450, loss is 5.0888517951965335 and perplexity is 162.20351223819486
At time: 201.44602632522583 and batch: 500, loss is 5.067265396118164 and perplexity is 158.7396432251766
At time: 201.7700891494751 and batch: 550, loss is 5.1184406757354735 and perplexity is 167.07464285055983
At time: 202.0949788093567 and batch: 600, loss is 5.162593050003052 and perplexity is 174.61665878659863
At time: 202.41920709609985 and batch: 650, loss is 5.132270822525024 and perplexity is 169.40136203349326
At time: 202.74644255638123 and batch: 700, loss is 5.1164334106445315 and perplexity is 166.7396161086145
At time: 203.07147884368896 and batch: 750, loss is 5.048019876480103 and perplexity is 155.71382640643841
At time: 203.39735674858093 and batch: 800, loss is 5.074803190231323 and perplexity is 159.94071098727335
At time: 203.72365856170654 and batch: 850, loss is 5.064021329879761 and perplexity is 158.22551569067048
At time: 204.0511612892151 and batch: 900, loss is 5.151113586425781 and perplexity is 172.6236146375121
At time: 204.37646961212158 and batch: 950, loss is 5.110052013397217 and perplexity is 165.67897217581046
At time: 204.70140027999878 and batch: 1000, loss is 5.083992424011231 and perplexity is 161.417217161752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.265956227372333 and perplexity of 193.6313759119844
Finished 28 epochs...
Completing Train Step...
At time: 205.84479451179504 and batch: 50, loss is 5.191216087341308 and perplexity is 179.68693512621357
At time: 206.17170214653015 and batch: 100, loss is 5.160359811782837 and perplexity is 174.22713330379995
At time: 206.5121603012085 and batch: 150, loss is 5.164667587280274 and perplexity is 174.97928356373183
At time: 206.83790612220764 and batch: 200, loss is 5.174533700942993 and perplexity is 176.71419339979514
At time: 207.16392636299133 and batch: 250, loss is 5.160875558853149 and perplexity is 174.3170136131311
At time: 207.48985958099365 and batch: 300, loss is 5.0609901332855225 and perplexity is 157.7466292125914
At time: 207.8151466846466 and batch: 350, loss is 5.120230226516724 and perplexity is 167.3738990954598
At time: 208.14069247245789 and batch: 400, loss is 5.059371099472046 and perplexity is 157.4914387226939
At time: 208.46740126609802 and batch: 450, loss is 5.082129259109497 and perplexity is 161.11675026480853
At time: 208.79178953170776 and batch: 500, loss is 5.060538072586059 and perplexity is 157.67533427708491
At time: 209.11738848686218 and batch: 550, loss is 5.112789402008056 and perplexity is 166.13312121491433
At time: 209.44143962860107 and batch: 600, loss is 5.1554008483886715 and perplexity is 173.36528602818422
At time: 209.76761627197266 and batch: 650, loss is 5.126284637451172 and perplexity is 168.39032328030794
At time: 210.09381222724915 and batch: 700, loss is 5.108647241592407 and perplexity is 165.4463944246287
At time: 210.42004680633545 and batch: 750, loss is 5.042177457809448 and perplexity is 154.80673342845125
At time: 210.74506521224976 and batch: 800, loss is 5.06914439201355 and perplexity is 159.03819476395026
At time: 211.0707459449768 and batch: 850, loss is 5.058391380310058 and perplexity is 157.33721690170069
At time: 211.3972246646881 and batch: 900, loss is 5.146458959579467 and perplexity is 171.8219832213828
At time: 211.72330260276794 and batch: 950, loss is 5.106113796234131 and perplexity is 165.0277755225906
At time: 212.04900789260864 and batch: 1000, loss is 5.078741092681884 and perplexity is 160.57178364078155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.260913383669969 and perplexity of 192.6573810633596
Finished 29 epochs...
Completing Train Step...
At time: 213.1606183052063 and batch: 50, loss is 5.185551557540894 and perplexity is 178.67197049008064
At time: 213.50051093101501 and batch: 100, loss is 5.154360666275024 and perplexity is 173.18504831480428
At time: 213.82645773887634 and batch: 150, loss is 5.159675006866455 and perplexity is 174.10786254960684
At time: 214.15276980400085 and batch: 200, loss is 5.168610639572144 and perplexity is 175.67059807776363
At time: 214.47737860679626 and batch: 250, loss is 5.155663719177246 and perplexity is 173.4108646880206
At time: 214.81710720062256 and batch: 300, loss is 5.05571141242981 and perplexity is 156.91612272550267
At time: 215.1446726322174 and batch: 350, loss is 5.114888772964478 and perplexity is 166.48226262492787
At time: 215.47179126739502 and batch: 400, loss is 5.054357166290283 and perplexity is 156.70376349785954
At time: 215.79963898658752 and batch: 450, loss is 5.077202396392822 and perplexity is 160.32490241948432
At time: 216.12467527389526 and batch: 500, loss is 5.056229228973389 and perplexity is 156.99739753070747
At time: 216.45093274116516 and batch: 550, loss is 5.109040813446045 and perplexity is 165.51152228416564
At time: 216.77925562858582 and batch: 600, loss is 5.151482172012329 and perplexity is 172.6872529411256
At time: 217.10622191429138 and batch: 650, loss is 5.122243318557739 and perplexity is 167.71117753196782
At time: 217.43108654022217 and batch: 700, loss is 5.104050893783569 and perplexity is 164.68769022205882
At time: 217.75638556480408 and batch: 750, loss is 5.037586765289307 and perplexity is 154.0976920559179
At time: 218.0828595161438 and batch: 800, loss is 5.064649457931519 and perplexity is 158.3249327956471
At time: 218.4107322692871 and batch: 850, loss is 5.054441804885864 and perplexity is 156.71702724562883
At time: 218.73549938201904 and batch: 900, loss is 5.142994995117188 and perplexity is 171.22782763868565
At time: 219.0611608028412 and batch: 950, loss is 5.102518548965454 and perplexity is 164.43552514460666
At time: 219.38749361038208 and batch: 1000, loss is 5.075408592224121 and perplexity is 160.03756872841095
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.25947719667016 and perplexity of 192.3808876329196
Finished 30 epochs...
Completing Train Step...
At time: 220.53005027770996 and batch: 50, loss is 5.182051706314087 and perplexity is 178.04773817226942
At time: 220.87096905708313 and batch: 100, loss is 5.1511890316009525 and perplexity is 172.63663874765408
At time: 221.19774556159973 and batch: 150, loss is 5.156934270858764 and perplexity is 173.6313321817575
At time: 221.52407884597778 and batch: 200, loss is 5.165230865478516 and perplexity is 175.07787334343726
At time: 221.85048294067383 and batch: 250, loss is 5.152763223648071 and perplexity is 172.9086159874507
At time: 222.1771171092987 and batch: 300, loss is 5.05184718132019 and perplexity is 156.3109326127686
At time: 222.50474786758423 and batch: 350, loss is 5.110671672821045 and perplexity is 165.78166852734424
At time: 222.83049964904785 and batch: 400, loss is 5.050455923080444 and perplexity is 156.09361494735836
At time: 223.15747547149658 and batch: 450, loss is 5.0734961891174315 and perplexity is 159.73180484987216
At time: 223.49817204475403 and batch: 500, loss is 5.0527247714996335 and perplexity is 156.44816976234858
At time: 223.82473611831665 and batch: 550, loss is 5.105006017684937 and perplexity is 164.84506251430608
At time: 224.152578830719 and batch: 600, loss is 5.147825374603271 and perplexity is 172.0569238373153
At time: 224.4801802635193 and batch: 650, loss is 5.1183263874053955 and perplexity is 167.05554925973814
At time: 224.8078215122223 and batch: 700, loss is 5.099394493103027 and perplexity is 163.92262096609048
At time: 225.13417196273804 and batch: 750, loss is 5.032258787155151 and perplexity is 153.27884625545124
At time: 225.45687532424927 and batch: 800, loss is 5.060176162719727 and perplexity is 157.6182803427464
At time: 225.77920722961426 and batch: 850, loss is 5.050604467391968 and perplexity is 156.11680348814417
At time: 226.10206699371338 and batch: 900, loss is 5.1385041618347165 and perplexity is 170.46059605543883
At time: 226.427499294281 and batch: 950, loss is 5.098837308883667 and perplexity is 163.83131130900426
At time: 226.75335431098938 and batch: 1000, loss is 5.07105525970459 and perplexity is 159.34238625425053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.255250139934261 and perplexity of 191.5693990185063
Finished 31 epochs...
Completing Train Step...
At time: 227.87981390953064 and batch: 50, loss is 5.176895685195923 and perplexity is 177.13208287176815
At time: 228.20611572265625 and batch: 100, loss is 5.146136445999145 and perplexity is 171.76657723346807
At time: 228.53128004074097 and batch: 150, loss is 5.151891775131226 and perplexity is 172.75800066677934
At time: 228.8565547466278 and batch: 200, loss is 5.1598153400421145 and perplexity is 174.13229737333384
At time: 229.18357348442078 and batch: 250, loss is 5.14740966796875 and perplexity is 171.98541349725207
At time: 229.50868391990662 and batch: 300, loss is 5.048167934417725 and perplexity is 155.73688278123294
At time: 229.83456015586853 and batch: 350, loss is 5.105446605682373 and perplexity is 164.91770727231568
At time: 230.16043329238892 and batch: 400, loss is 5.0461798191070555 and perplexity is 155.42756747926683
At time: 230.4861545562744 and batch: 450, loss is 5.068658761978149 and perplexity is 158.96097979026965
At time: 230.81222987174988 and batch: 500, loss is 5.047776050567627 and perplexity is 155.67586396893088
At time: 231.13788866996765 and batch: 550, loss is 5.1001199436187745 and perplexity is 164.04158186091703
At time: 231.4632387161255 and batch: 600, loss is 5.142771663665772 and perplexity is 171.18959134925828
At time: 231.80356168746948 and batch: 650, loss is 5.113224077224731 and perplexity is 166.20535086245684
At time: 232.12865829467773 and batch: 700, loss is 5.094078006744385 and perplexity is 163.05344112651326
At time: 232.45448517799377 and batch: 750, loss is 5.028086624145508 and perplexity is 152.64067412773454
At time: 232.77936434745789 and batch: 800, loss is 5.056635046005249 and perplexity is 157.06112267808376
At time: 233.1050727367401 and batch: 850, loss is 5.046617641448974 and perplexity is 155.49563203986474
At time: 233.43107533454895 and batch: 900, loss is 5.134233484268188 and perplexity is 169.73416609005994
At time: 233.75758981704712 and batch: 950, loss is 5.094593954086304 and perplexity is 163.13758982233443
At time: 234.08287835121155 and batch: 1000, loss is 5.067433433532715 and perplexity is 158.76631966566816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.252677731397675 and perplexity of 191.07723755269822
Finished 32 epochs...
Completing Train Step...
At time: 235.1987338066101 and batch: 50, loss is 5.172943496704102 and perplexity is 176.43340505495698
At time: 235.53732132911682 and batch: 100, loss is 5.141857595443725 and perplexity is 171.03318387830038
At time: 235.86176776885986 and batch: 150, loss is 5.147854518890381 and perplexity is 172.06193838677524
At time: 236.1876106262207 and batch: 200, loss is 5.155673236846924 and perplexity is 173.41251516320347
At time: 236.51326036453247 and batch: 250, loss is 5.143494691848755 and perplexity is 171.3134110055936
At time: 236.83751726150513 and batch: 300, loss is 5.044240417480469 and perplexity is 155.12642311640542
At time: 237.1625063419342 and batch: 350, loss is 5.10075686454773 and perplexity is 164.1460966579208
At time: 237.48811149597168 and batch: 400, loss is 5.042128438949585 and perplexity is 154.79914516486505
At time: 237.8153612613678 and batch: 450, loss is 5.065183830261231 and perplexity is 158.40955986800572
At time: 238.14116859436035 and batch: 500, loss is 5.045162343978882 and perplexity is 155.26950422147726
At time: 238.4647445678711 and batch: 550, loss is 5.0965025806427 and perplexity is 163.44925589107075
At time: 238.79152464866638 and batch: 600, loss is 5.139374122619629 and perplexity is 170.60895461308667
At time: 239.11826419830322 and batch: 650, loss is 5.109745845794678 and perplexity is 165.62825440657335
At time: 239.44502425193787 and batch: 700, loss is 5.0908139705657955 and perplexity is 162.5220964315291
At time: 239.77008366584778 and batch: 750, loss is 5.025023565292359 and perplexity is 152.17384209149532
At time: 240.0943570137024 and batch: 800, loss is 5.053304805755615 and perplexity is 156.5389413828761
At time: 240.44606494903564 and batch: 850, loss is 5.0434143924713135 and perplexity is 154.9983377194839
At time: 240.77213191986084 and batch: 900, loss is 5.1312415599823 and perplexity is 169.22709325641276
At time: 241.09792137145996 and batch: 950, loss is 5.091597976684571 and perplexity is 162.6495647109939
At time: 241.42360520362854 and batch: 1000, loss is 5.0647572326660155 and perplexity is 158.3419971427796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.251421300376334 and perplexity of 190.8373129398905
Finished 33 epochs...
Completing Train Step...
At time: 242.53982758522034 and batch: 50, loss is 5.170130872726441 and perplexity is 175.93786144503966
At time: 242.88001942634583 and batch: 100, loss is 5.138864679336548 and perplexity is 170.5220611626471
At time: 243.2053039073944 and batch: 150, loss is 5.144535884857178 and perplexity is 171.49187422258626
At time: 243.53329277038574 and batch: 200, loss is 5.153260955810547 and perplexity is 172.99469958830807
At time: 243.85987949371338 and batch: 250, loss is 5.14032099723816 and perplexity is 170.77057640761586
At time: 244.18526601791382 and batch: 300, loss is 5.041393547058106 and perplexity is 154.6854263189223
At time: 244.5106840133667 and batch: 350, loss is 5.097234182357788 and perplexity is 163.568879400057
At time: 244.83697152137756 and batch: 400, loss is 5.03943452835083 and perplexity is 154.3826913036183
At time: 245.16403698921204 and batch: 450, loss is 5.062602491378784 and perplexity is 158.00117842398166
At time: 245.49021458625793 and batch: 500, loss is 5.042594356536865 and perplexity is 154.87128561354015
At time: 245.81470823287964 and batch: 550, loss is 5.093333587646485 and perplexity is 162.93210619858235
At time: 246.14042973518372 and batch: 600, loss is 5.136761732101441 and perplexity is 170.16383905870828
At time: 246.46612739562988 and batch: 650, loss is 5.106883583068847 and perplexity is 165.1548606394998
At time: 246.7921848297119 and batch: 700, loss is 5.087966337203979 and perplexity is 162.05995140978436
At time: 247.11822867393494 and batch: 750, loss is 5.022364492416382 and perplexity is 151.76973826442645
At time: 247.44421648979187 and batch: 800, loss is 5.0505972003936765 and perplexity is 156.1156689917222
At time: 247.77068281173706 and batch: 850, loss is 5.040633172988891 and perplexity is 154.56785223767014
At time: 248.09654784202576 and batch: 900, loss is 5.1283683013916015 and perplexity is 168.74155792524823
At time: 248.42051935195923 and batch: 950, loss is 5.088561277389527 and perplexity is 162.15639607390438
At time: 248.76017355918884 and batch: 1000, loss is 5.062109279632568 and perplexity is 157.9232696012196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.249784516125191 and perplexity of 190.52520892468718
Finished 34 epochs...
Completing Train Step...
At time: 249.88875460624695 and batch: 50, loss is 5.166936073303223 and perplexity is 175.37667218765688
At time: 250.21249794960022 and batch: 100, loss is 5.135858860015869 and perplexity is 170.01027221448894
At time: 250.5402500629425 and batch: 150, loss is 5.141354207992554 and perplexity is 170.947109585979
At time: 250.86638188362122 and batch: 200, loss is 5.150453987121582 and perplexity is 172.50978976493911
At time: 251.19222903251648 and batch: 250, loss is 5.136902141571045 and perplexity is 170.18773335054945
At time: 251.51858162879944 and batch: 300, loss is 5.0374854469299315 and perplexity is 154.08207992148655
At time: 251.84439635276794 and batch: 350, loss is 5.093829908370972 and perplexity is 163.0129928507735
At time: 252.1715178489685 and batch: 400, loss is 5.0365194606781 and perplexity is 153.9333106167488
At time: 252.49697589874268 and batch: 450, loss is 5.059728984832764 and perplexity is 157.54781269015822
At time: 252.82191061973572 and batch: 500, loss is 5.038855400085449 and perplexity is 154.29330980757234
At time: 253.1478033065796 and batch: 550, loss is 5.0899903774261475 and perplexity is 162.3882994525668
At time: 253.47337746620178 and batch: 600, loss is 5.133302335739136 and perplexity is 169.5761919310922
At time: 253.7989239692688 and batch: 650, loss is 5.103164176940918 and perplexity is 164.54172359845757
At time: 254.1243040561676 and batch: 700, loss is 5.084608039855957 and perplexity is 161.51661875170598
At time: 254.4481348991394 and batch: 750, loss is 5.018725109100342 and perplexity is 151.21839389640564
At time: 254.77327942848206 and batch: 800, loss is 5.047191619873047 and perplexity is 155.58490879670822
At time: 255.09972977638245 and batch: 850, loss is 5.038257493972778 and perplexity is 154.20108446828868
At time: 255.42409133911133 and batch: 900, loss is 5.125248756408691 and perplexity is 168.21598125107596
At time: 255.749906539917 and batch: 950, loss is 5.085294370651245 and perplexity is 161.62751063100896
At time: 256.0759689807892 and batch: 1000, loss is 5.058430089950561 and perplexity is 157.3433074866861
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.246887579196837 and perplexity of 189.97406810670745
Finished 35 epochs...
Completing Train Step...
At time: 257.1867620944977 and batch: 50, loss is 5.162841682434082 and perplexity is 174.66007954865228
At time: 257.5394678115845 and batch: 100, loss is 5.13308892250061 and perplexity is 169.54000598821233
At time: 257.8655219078064 and batch: 150, loss is 5.1376662445068355 and perplexity is 170.3178239922293
At time: 258.1924090385437 and batch: 200, loss is 5.1465517520904545 and perplexity is 171.83792775440386
At time: 258.5172233581543 and batch: 250, loss is 5.1330481624603275 and perplexity is 169.53309567157208
At time: 258.84388041496277 and batch: 300, loss is 5.033165197372437 and perplexity is 153.41784275219695
At time: 259.169584274292 and batch: 350, loss is 5.089649515151978 and perplexity is 162.332956840157
At time: 259.49532985687256 and batch: 400, loss is 5.033284778594971 and perplexity is 153.43618974234766
At time: 259.81981015205383 and batch: 450, loss is 5.056028614044189 and perplexity is 156.9659046679922
At time: 260.14573311805725 and batch: 500, loss is 5.035247793197632 and perplexity is 153.7376830444507
At time: 260.47318291664124 and batch: 550, loss is 5.087024478912354 and perplexity is 161.9073857596983
At time: 260.79892778396606 and batch: 600, loss is 5.1292691421508785 and perplexity is 168.89363568704997
At time: 261.12452125549316 and batch: 650, loss is 5.099765996932984 and perplexity is 163.98353016089496
At time: 261.4505350589752 and batch: 700, loss is 5.0818070125579835 and perplexity is 161.06483931215664
At time: 261.77636098861694 and batch: 750, loss is 5.015786304473877 and perplexity is 150.77464494592303
At time: 262.10240268707275 and batch: 800, loss is 5.043937168121338 and perplexity is 155.07938826001651
At time: 262.4268000125885 and batch: 850, loss is 5.034748239517212 and perplexity is 153.6609019987903
At time: 262.75320744514465 and batch: 900, loss is 5.122072649002075 and perplexity is 167.68255678223335
At time: 263.0767431259155 and batch: 950, loss is 5.082201089859009 and perplexity is 161.1283238174025
At time: 263.4041085243225 and batch: 1000, loss is 5.054961957931519 and perplexity is 156.7985652889489
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.241781095179116 and perplexity of 189.00644125233916
Finished 36 epochs...
Completing Train Step...
At time: 264.5169689655304 and batch: 50, loss is 5.15879979133606 and perplexity is 173.95554730839467
At time: 264.85902643203735 and batch: 100, loss is 5.12932861328125 and perplexity is 168.90368028115552
At time: 265.18535137176514 and batch: 150, loss is 5.135049619674683 and perplexity is 169.87274869609138
At time: 265.51132249832153 and batch: 200, loss is 5.1435504913330075 and perplexity is 171.32297047227746
At time: 265.83730602264404 and batch: 250, loss is 5.129214582443237 and perplexity is 168.88442115103783
At time: 266.1783196926117 and batch: 300, loss is 5.029748096466064 and perplexity is 152.89449318106585
At time: 266.50447487831116 and batch: 350, loss is 5.087243547439575 and perplexity is 161.94285845757696
At time: 266.8289806842804 and batch: 400, loss is 5.029745121002197 and perplexity is 152.8940382497027
At time: 267.1538224220276 and batch: 450, loss is 5.052224817276001 and perplexity is 156.3699723882789
At time: 267.4787755012512 and batch: 500, loss is 5.03187087059021 and perplexity is 153.21939838308637
At time: 267.80451369285583 and batch: 550, loss is 5.0841073894500735 and perplexity is 161.43577562973053
At time: 268.1300766468048 and batch: 600, loss is 5.126326293945312 and perplexity is 168.39733797692577
At time: 268.45658135414124 and batch: 650, loss is 5.096765575408935 and perplexity is 163.49224784299648
At time: 268.78329205513 and batch: 700, loss is 5.079163246154785 and perplexity is 160.63958388694311
At time: 269.1103036403656 and batch: 750, loss is 5.012576637268066 and perplexity is 150.29148431977265
At time: 269.4356486797333 and batch: 800, loss is 5.040671901702881 and perplexity is 154.5738385677321
At time: 269.7620210647583 and batch: 850, loss is 5.031249094009399 and perplexity is 153.1241597610865
At time: 270.08756017684937 and batch: 900, loss is 5.118984661102295 and perplexity is 167.16555373629134
At time: 270.4140577316284 and batch: 950, loss is 5.078837919235229 and perplexity is 160.5873320058911
At time: 270.7400133609772 and batch: 1000, loss is 5.051862688064575 and perplexity is 156.31335650523843
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.239383604468369 and perplexity of 188.5538428322125
Finished 37 epochs...
Completing Train Step...
At time: 271.87119579315186 and batch: 50, loss is 5.154006567001343 and perplexity is 173.12373447121792
At time: 272.1982328891754 and batch: 100, loss is 5.126540327072144 and perplexity is 168.43338444315444
At time: 272.52466344833374 and batch: 150, loss is 5.131650819778442 and perplexity is 169.2963652762601
At time: 272.85113644599915 and batch: 200, loss is 5.139804954528809 and perplexity is 170.68247423088718
At time: 273.1779270172119 and batch: 250, loss is 5.125317716598511 and perplexity is 168.22758185705916
At time: 273.50401973724365 and batch: 300, loss is 5.0259484195709225 and perplexity is 152.31464582186894
At time: 273.82985043525696 and batch: 350, loss is 5.0837062644958495 and perplexity is 161.37103269749193
At time: 274.15655303001404 and batch: 400, loss is 5.025846176147461 and perplexity is 152.29907344713672
At time: 274.4992527961731 and batch: 450, loss is 5.048338298797607 and perplexity is 155.76341705887648
At time: 274.8262243270874 and batch: 500, loss is 5.028128309249878 and perplexity is 152.64703710278638
At time: 275.15216875076294 and batch: 550, loss is 5.080997772216797 and perplexity is 160.93455187067548
At time: 275.4782099723816 and batch: 600, loss is 5.12227346420288 and perplexity is 167.71623336981725
At time: 275.8043124675751 and batch: 650, loss is 5.092755765914917 and perplexity is 162.83798768133164
At time: 276.1315267086029 and batch: 700, loss is 5.075044841766357 and perplexity is 159.97936557588005
At time: 276.4579563140869 and batch: 750, loss is 5.008604383468628 and perplexity is 149.69567254139312
At time: 276.7837302684784 and batch: 800, loss is 5.037749004364014 and perplexity is 154.12269475104378
At time: 277.10930347442627 and batch: 850, loss is 5.028198108673096 and perplexity is 152.65769214978587
At time: 277.4368624687195 and batch: 900, loss is 5.116242294311523 and perplexity is 166.70775248954263
At time: 277.7655510902405 and batch: 950, loss is 5.0760837745666505 and perplexity is 160.14565975542786
At time: 278.09118700027466 and batch: 1000, loss is 5.048996438980103 and perplexity is 155.8659649643528
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.237912806068978 and perplexity of 188.27672198638098
Finished 38 epochs...
Completing Train Step...
At time: 279.2072401046753 and batch: 50, loss is 5.151277256011963 and perplexity is 172.65187018530887
At time: 279.55118560791016 and batch: 100, loss is 5.124372320175171 and perplexity is 168.0686152579315
At time: 279.87599539756775 and batch: 150, loss is 5.129213151931762 and perplexity is 168.8841795601083
At time: 280.2019383907318 and batch: 200, loss is 5.13638011932373 and perplexity is 170.0989147521796
At time: 280.5285861492157 and batch: 250, loss is 5.12284556388855 and perplexity is 167.8122112260427
At time: 280.8541991710663 and batch: 300, loss is 5.0230597877502445 and perplexity is 151.8752997492145
At time: 281.17970848083496 and batch: 350, loss is 5.081151752471924 and perplexity is 160.95933452200177
At time: 281.5072076320648 and batch: 400, loss is 5.022678174972534 and perplexity is 151.8173532514768
At time: 281.83256936073303 and batch: 450, loss is 5.045343894958496 and perplexity is 155.29769611112795
At time: 282.1592552661896 and batch: 500, loss is 5.0256006717681885 and perplexity is 152.26168794698614
At time: 282.4852406978607 and batch: 550, loss is 5.0780747032165525 and perplexity is 160.4648159407665
At time: 282.81093192100525 and batch: 600, loss is 5.118389844894409 and perplexity is 167.06615052178114
At time: 283.16310358047485 and batch: 650, loss is 5.088970975875855 and perplexity is 162.22284491498957
At time: 283.48887276649475 and batch: 700, loss is 5.0716181564331055 and perplexity is 159.432104810959
At time: 283.81353187561035 and batch: 750, loss is 5.004694633483886 and perplexity is 149.11154253319506
At time: 284.13979625701904 and batch: 800, loss is 5.034956035614013 and perplexity is 153.6928354521652
At time: 284.4661741256714 and batch: 850, loss is 5.024939785003662 and perplexity is 152.16109345712238
At time: 284.7943546772003 and batch: 900, loss is 5.113850860595703 and perplexity is 166.3095582668666
At time: 285.1204135417938 and batch: 950, loss is 5.072241687774659 and perplexity is 159.53154672451146
At time: 285.44655990600586 and batch: 1000, loss is 5.045436944961548 and perplexity is 155.31214723455315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.234910918445122 and perplexity of 187.71238388835218
Finished 39 epochs...
Completing Train Step...
At time: 286.5756826400757 and batch: 50, loss is 5.148752851486206 and perplexity is 172.21657668244023
At time: 286.9148790836334 and batch: 100, loss is 5.120770902633667 and perplexity is 167.4644186339715
At time: 287.2405581474304 and batch: 150, loss is 5.126134576797486 and perplexity is 168.3650564141507
At time: 287.5671555995941 and batch: 200, loss is 5.132127923965454 and perplexity is 169.37715655237483
At time: 287.8913571834564 and batch: 250, loss is 5.1194133281707765 and perplexity is 167.23722746514866
At time: 288.21623635292053 and batch: 300, loss is 5.019834308624268 and perplexity is 151.38621832510606
At time: 288.54154109954834 and batch: 350, loss is 5.077760705947876 and perplexity is 160.41443833647136
At time: 288.8695020675659 and batch: 400, loss is 5.019974727630615 and perplexity is 151.40747732000642
At time: 289.1948444843292 and batch: 450, loss is 5.043153944015503 and perplexity is 154.95797389834732
At time: 289.51970624923706 and batch: 500, loss is 5.023231258392334 and perplexity is 151.9013441372402
At time: 289.8443503379822 and batch: 550, loss is 5.0756548976898195 and perplexity is 160.07699171115465
At time: 290.169029712677 and batch: 600, loss is 5.116150178909302 and perplexity is 166.69239684512516
At time: 290.49684262275696 and batch: 650, loss is 5.085881700515747 and perplexity is 161.72246717760586
At time: 290.82131934165955 and batch: 700, loss is 5.068852806091309 and perplexity is 158.9918282255019
At time: 291.14710450172424 and batch: 750, loss is 5.00217568397522 and perplexity is 148.73641075375454
At time: 291.48818588256836 and batch: 800, loss is 5.033017778396607 and perplexity is 153.3952277179279
At time: 291.8137938976288 and batch: 850, loss is 5.022246789932251 and perplexity is 151.75187564047508
At time: 292.138103723526 and batch: 900, loss is 5.1117258644104 and perplexity is 165.95652631858786
At time: 292.46349239349365 and batch: 950, loss is 5.070080051422119 and perplexity is 159.18706998459223
At time: 292.7879557609558 and batch: 1000, loss is 5.043355207443238 and perplexity is 154.98916440997826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.23423729873285 and perplexity of 187.58597970528197
Finished 40 epochs...
Completing Train Step...
At time: 293.9042217731476 and batch: 50, loss is 5.146641397476197 and perplexity is 171.85333292221333
At time: 294.23029708862305 and batch: 100, loss is 5.118379402160644 and perplexity is 167.06440590355945
At time: 294.5558588504791 and batch: 150, loss is 5.123699617385864 and perplexity is 167.95559305109165
At time: 294.88371896743774 and batch: 200, loss is 5.129880418777466 and perplexity is 168.99690797967847
At time: 295.20803689956665 and batch: 250, loss is 5.1171252632141115 and perplexity is 166.8550152554651
At time: 295.53276801109314 and batch: 300, loss is 5.017680215835571 and perplexity is 151.06046933663677
At time: 295.85654878616333 and batch: 350, loss is 5.076046943664551 and perplexity is 160.1397615549304
At time: 296.1819405555725 and batch: 400, loss is 5.017751226425171 and perplexity is 151.07119661050007
At time: 296.51049304008484 and batch: 450, loss is 5.041708478927612 and perplexity is 154.734149361235
At time: 296.83728528022766 and batch: 500, loss is 5.021131324768066 and perplexity is 151.5826960840912
At time: 297.1652777194977 and batch: 550, loss is 5.072991905212402 and perplexity is 159.6512749782307
At time: 297.48988819122314 and batch: 600, loss is 5.113797855377197 and perplexity is 166.30074322601467
At time: 297.8166756629944 and batch: 650, loss is 5.084080419540405 and perplexity is 161.4314217801563
At time: 298.141827583313 and batch: 700, loss is 5.06674108505249 and perplexity is 158.6564360887898
At time: 298.4672038555145 and batch: 750, loss is 4.999970741271973 and perplexity is 148.40881678584444
At time: 298.7920174598694 and batch: 800, loss is 5.030646829605103 and perplexity is 153.03196629543265
At time: 299.1170325279236 and batch: 850, loss is 5.019782600402832 and perplexity is 151.37839061538673
At time: 299.44394087791443 and batch: 900, loss is 5.1096993255615235 and perplexity is 165.62054952077932
At time: 299.7687759399414 and batch: 950, loss is 5.067686128616333 and perplexity is 158.806444203515
At time: 300.10842657089233 and batch: 1000, loss is 5.040681600570679 and perplexity is 154.57533776622765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.232051198075458 and perplexity of 187.17634578535936
Finished 41 epochs...
Completing Train Step...
At time: 301.2224473953247 and batch: 50, loss is 5.14344741821289 and perplexity is 171.3053125892053
At time: 301.56403398513794 and batch: 100, loss is 5.114141206741333 and perplexity is 166.3578526167907
At time: 301.8915569782257 and batch: 150, loss is 5.120906286239624 and perplexity is 167.48709210560918
At time: 302.2176330089569 and batch: 200, loss is 5.1266832447052 and perplexity is 168.45745826403245
At time: 302.54593443870544 and batch: 250, loss is 5.114538383483887 and perplexity is 166.42393920995136
At time: 302.87517523765564 and batch: 300, loss is 5.015302133560181 and perplexity is 150.7016619178728
At time: 303.20238733291626 and batch: 350, loss is 5.073131008148193 and perplexity is 159.67348448395092
At time: 303.52924060821533 and batch: 400, loss is 5.015131034851074 and perplexity is 150.67587926381077
At time: 303.85722279548645 and batch: 450, loss is 5.03942232131958 and perplexity is 154.38080676078349
At time: 304.1839199066162 and batch: 500, loss is 5.018722095489502 and perplexity is 151.21793818370125
At time: 304.51174783706665 and batch: 550, loss is 5.070222311019897 and perplexity is 159.20971748401362
At time: 304.8418469429016 and batch: 600, loss is 5.111515655517578 and perplexity is 165.92164444730201
At time: 305.1685721874237 and batch: 650, loss is 5.081669216156006 and perplexity is 161.04264668588146
At time: 305.4933853149414 and batch: 700, loss is 5.063927040100098 and perplexity is 158.21059734499372
At time: 305.8198313713074 and batch: 750, loss is 4.997069911956787 and perplexity is 147.97893195219373
At time: 306.1469147205353 and batch: 800, loss is 5.028243217468262 and perplexity is 152.66457850966808
At time: 306.4736201763153 and batch: 850, loss is 5.017067098617554 and perplexity is 150.9678799488991
At time: 306.7985997200012 and batch: 900, loss is 5.106438302993775 and perplexity is 165.08133684131164
At time: 307.12536811828613 and batch: 950, loss is 5.06539945602417 and perplexity is 158.44372073305803
At time: 307.4510509967804 and batch: 1000, loss is 5.03790885925293 and perplexity is 154.14733398659132
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.2291174167540015 and perplexity of 186.6280160513345
Finished 42 epochs...
Completing Train Step...
At time: 308.5916745662689 and batch: 50, loss is 5.139304037094116 and perplexity is 170.59699781384955
At time: 308.93671321868896 and batch: 100, loss is 5.109321002960205 and perplexity is 165.55790337462648
At time: 309.26275420188904 and batch: 150, loss is 5.115663537979126 and perplexity is 166.61129723686605
At time: 309.5902099609375 and batch: 200, loss is 5.121731376647949 and perplexity is 167.62534112495067
At time: 309.91594791412354 and batch: 250, loss is 5.110486679077148 and perplexity is 165.75100279238592
At time: 310.24266743659973 and batch: 300, loss is 5.008948078155518 and perplexity is 149.7471309911874
At time: 310.570428609848 and batch: 350, loss is 5.068978185653687 and perplexity is 159.0117638010772
At time: 310.8975341320038 and batch: 400, loss is 5.012245836257935 and perplexity is 150.2417759671855
At time: 311.2231137752533 and batch: 450, loss is 5.0345793724060055 and perplexity is 153.634955916946
At time: 311.548513174057 and batch: 500, loss is 5.011929941177368 and perplexity is 150.19432282476362
At time: 311.87384557724 and batch: 550, loss is 5.0662081432342525 and perplexity is 158.5719039666178
At time: 312.20037961006165 and batch: 600, loss is 5.1067214012146 and perplexity is 165.12807768987616
At time: 312.52639293670654 and batch: 650, loss is 5.077162361145019 and perplexity is 160.3184839007713
At time: 312.8512554168701 and batch: 700, loss is 5.059947214126587 and perplexity is 157.58219798986838
At time: 313.17720460891724 and batch: 750, loss is 4.992430257797241 and perplexity is 147.29395115081198
At time: 313.503333568573 and batch: 800, loss is 5.024714374542237 and perplexity is 152.12679862018797
At time: 313.8307399749756 and batch: 850, loss is 5.012463884353638 and perplexity is 150.27453947221028
At time: 314.1572012901306 and batch: 900, loss is 5.104145154953003 and perplexity is 164.7032146079925
At time: 314.4842154979706 and batch: 950, loss is 5.060971813201904 and perplexity is 157.74373930762548
At time: 314.81052231788635 and batch: 1000, loss is 5.035126104354858 and perplexity is 153.71897602195133
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.2274348561356705 and perplexity of 186.31426712606097
Finished 43 epochs...
Completing Train Step...
At time: 315.9539315700531 and batch: 50, loss is 5.13655758857727 and perplexity is 170.12910475842833
At time: 316.28119468688965 and batch: 100, loss is 5.107294988632202 and perplexity is 165.22282024650366
At time: 316.6071126461029 and batch: 150, loss is 5.11275393486023 and perplexity is 166.12722905143502
At time: 316.9319818019867 and batch: 200, loss is 5.11863564491272 and perplexity is 167.10722043192408
At time: 317.2727880477905 and batch: 250, loss is 5.107804546356201 and perplexity is 165.30703226436106
At time: 317.5994293689728 and batch: 300, loss is 5.006296911239624 and perplexity is 149.35065214977425
At time: 317.9253845214844 and batch: 350, loss is 5.066668453216553 and perplexity is 158.64491299903014
At time: 318.2502851486206 and batch: 400, loss is 5.009903497695923 and perplexity is 149.89027069460238
At time: 318.5771954059601 and batch: 450, loss is 5.031227931976319 and perplexity is 153.12091937683888
At time: 318.9035620689392 and batch: 500, loss is 5.007625379562378 and perplexity is 149.54919160762915
At time: 319.22940611839294 and batch: 550, loss is 5.061907224655151 and perplexity is 157.89136364203418
At time: 319.55420422554016 and batch: 600, loss is 5.102465543746948 and perplexity is 164.42680943465732
At time: 319.8794906139374 and batch: 650, loss is 5.073473834991455 and perplexity is 159.7282342248934
At time: 320.20602107048035 and batch: 700, loss is 5.05675778388977 and perplexity is 157.0804012111007
At time: 320.5325803756714 and batch: 750, loss is 4.9886378479003906 and perplexity is 146.73640999325357
At time: 320.8585081100464 and batch: 800, loss is 5.021198949813843 and perplexity is 151.5929472174656
At time: 321.18455719947815 and batch: 850, loss is 5.007851657867431 and perplexity is 149.58303517411622
At time: 321.5099675655365 and batch: 900, loss is 5.101546792984009 and perplexity is 164.27581155337234
At time: 321.8349401950836 and batch: 950, loss is 5.057674732208252 and perplexity is 157.22450187719278
At time: 322.16074872016907 and batch: 1000, loss is 5.031308279037476 and perplexity is 153.1332226869733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.224480233541349 and perplexity of 185.7645912250639
Finished 44 epochs...
Completing Train Step...
At time: 323.28726720809937 and batch: 50, loss is 5.133247356414795 and perplexity is 169.56686900292215
At time: 323.62866973876953 and batch: 100, loss is 5.103124828338623 and perplexity is 164.535249238994
At time: 323.9551811218262 and batch: 150, loss is 5.108455696105957 and perplexity is 165.41470694942058
At time: 324.2818579673767 and batch: 200, loss is 5.114283084869385 and perplexity is 166.38145683192795
At time: 324.6091568470001 and batch: 250, loss is 5.103954439163208 and perplexity is 164.67180609948113
At time: 324.93613624572754 and batch: 300, loss is 5.001553153991699 and perplexity is 148.6438466933599
At time: 325.2625241279602 and batch: 350, loss is 5.062499265670777 and perplexity is 157.9848694822384
At time: 325.5890679359436 and batch: 400, loss is 5.0065583324432374 and perplexity is 149.3897006808543
At time: 325.9299330711365 and batch: 450, loss is 5.027975559234619 and perplexity is 152.6237220462727
At time: 326.2587721347809 and batch: 500, loss is 5.005060882568359 and perplexity is 149.16616450114444
At time: 326.583327293396 and batch: 550, loss is 5.059259185791015 and perplexity is 157.47381426228313
At time: 326.910973072052 and batch: 600, loss is 5.099823846817016 and perplexity is 163.99301686349767
At time: 327.23699402809143 and batch: 650, loss is 5.071580963134766 and perplexity is 159.426175115393
At time: 327.56418442726135 and batch: 700, loss is 5.054390306472778 and perplexity is 156.70895677523214
At time: 327.89167165756226 and batch: 750, loss is 4.986093578338623 and perplexity is 146.36354754413242
At time: 328.2170190811157 and batch: 800, loss is 5.01958215713501 and perplexity is 151.3480508768939
At time: 328.5424201488495 and batch: 850, loss is 5.00571192741394 and perplexity is 149.26330998327938
At time: 328.87001156806946 and batch: 900, loss is 5.099587869644165 and perplexity is 163.9543228206459
At time: 329.1981043815613 and batch: 950, loss is 5.055894498825073 and perplexity is 156.94485456289516
At time: 329.52500462532043 and batch: 1000, loss is 5.028722944259644 and perplexity is 152.73783336782836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.222043293278392 and perplexity of 185.31244516350938
Finished 45 epochs...
Completing Train Step...
At time: 330.641939163208 and batch: 50, loss is 5.130779609680176 and perplexity is 169.14893680314393
At time: 330.9829318523407 and batch: 100, loss is 5.100742626190185 and perplexity is 164.1437595037457
At time: 331.3090350627899 and batch: 150, loss is 5.105949239730835 and perplexity is 165.00062136316194
At time: 331.6362452507019 and batch: 200, loss is 5.111967535018921 and perplexity is 165.99663798000083
At time: 331.9638879299164 and batch: 250, loss is 5.101352386474609 and perplexity is 164.2438783703793
At time: 332.2913453578949 and batch: 300, loss is 4.999412164688111 and perplexity is 148.32594224399318
At time: 332.61713099479675 and batch: 350, loss is 5.060007982254028 and perplexity is 157.59177425592122
At time: 332.94281935691833 and batch: 400, loss is 5.003848371505737 and perplexity is 148.98540848299908
At time: 333.2695486545563 and batch: 450, loss is 5.025558271408081 and perplexity is 152.255232133452
At time: 333.5958037376404 and batch: 500, loss is 5.002390766143799 and perplexity is 148.76840474407123
At time: 333.9217987060547 and batch: 550, loss is 5.056609945297241 and perplexity is 157.0571803821814
At time: 334.26215982437134 and batch: 600, loss is 5.097497501373291 and perplexity is 163.61195586752518
At time: 334.5866675376892 and batch: 650, loss is 5.069446907043457 and perplexity is 159.08631348614705
At time: 334.9154591560364 and batch: 700, loss is 5.0522761726379395 and perplexity is 156.3780030310137
At time: 335.2404839992523 and batch: 750, loss is 4.983655061721802 and perplexity is 146.00707241325844
At time: 335.5673179626465 and batch: 800, loss is 5.017260818481446 and perplexity is 150.99712825895267
At time: 335.89372062683105 and batch: 850, loss is 5.0032133197784425 and perplexity is 148.8908250778564
At time: 336.2190718650818 and batch: 900, loss is 5.0974910449981685 and perplexity is 163.61089953077362
At time: 336.54585313796997 and batch: 950, loss is 5.053243532180786 and perplexity is 156.5293499761905
At time: 336.87256956100464 and batch: 1000, loss is 5.026245889663696 and perplexity is 152.3599616134045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.219770478039253 and perplexity of 184.8917424849707
Finished 46 epochs...
Completing Train Step...
At time: 338.03076124191284 and batch: 50, loss is 5.128872327804565 and perplexity is 168.82662956478288
At time: 338.35643219947815 and batch: 100, loss is 5.098101587295532 and perplexity is 163.71082140540838
At time: 338.68118119239807 and batch: 150, loss is 5.1037929916381835 and perplexity is 164.64522238993987
At time: 339.0080580711365 and batch: 200, loss is 5.109623689651489 and perplexity is 165.60802313352417
At time: 339.3334882259369 and batch: 250, loss is 5.0991353511810305 and perplexity is 163.88014724663293
At time: 339.6593313217163 and batch: 300, loss is 4.996709327697754 and perplexity is 147.92558269769202
At time: 339.985315322876 and batch: 350, loss is 5.0576377201080325 and perplexity is 157.21868277586162
At time: 340.31306529045105 and batch: 400, loss is 5.001988315582276 and perplexity is 148.7085448621745
At time: 340.6404695510864 and batch: 450, loss is 5.023344306945801 and perplexity is 151.91851733514886
At time: 340.9645164012909 and batch: 500, loss is 5.000681829452515 and perplexity is 148.51438607143626
At time: 341.2907407283783 and batch: 550, loss is 5.054422760009766 and perplexity is 156.71404261768345
At time: 341.6152274608612 and batch: 600, loss is 5.095736742019653 and perplexity is 163.3241280579805
At time: 341.9402651786804 and batch: 650, loss is 5.067759160995483 and perplexity is 158.81804263948516
At time: 342.2643630504608 and batch: 700, loss is 5.050161724090576 and perplexity is 156.04769911804243
At time: 342.604056596756 and batch: 750, loss is 4.981322584152221 and perplexity is 145.66691105544226
At time: 342.92874240875244 and batch: 800, loss is 5.015539274215699 and perplexity is 150.7374036465086
At time: 343.25545597076416 and batch: 850, loss is 5.001036853790283 and perplexity is 148.56712165365397
At time: 343.58322191238403 and batch: 900, loss is 5.095649003982544 and perplexity is 163.30979894818526
At time: 343.9069564342499 and batch: 950, loss is 5.0510940265655515 and perplexity is 156.19325061261108
At time: 344.23201155662537 and batch: 1000, loss is 5.023629865646362 and perplexity is 151.96190518414434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.2183610869617 and perplexity of 184.63134125948574
Finished 47 epochs...
Completing Train Step...
At time: 345.36023902893066 and batch: 50, loss is 5.126879558563233 and perplexity is 168.4905320438931
At time: 345.70040225982666 and batch: 100, loss is 5.095902433395386 and perplexity is 163.35119169949283
At time: 346.026908159256 and batch: 150, loss is 5.102020196914673 and perplexity is 164.35359877921113
At time: 346.35274481773376 and batch: 200, loss is 5.107590570449829 and perplexity is 165.27166432638083
At time: 346.67744517326355 and batch: 250, loss is 5.096765851974487 and perplexity is 163.49229305932644
At time: 347.0030446052551 and batch: 300, loss is 4.99433084487915 and perplexity is 147.57416233013745
At time: 347.3279273509979 and batch: 350, loss is 5.0557441139221195 and perplexity is 156.92125420078622
At time: 347.6564426422119 and batch: 400, loss is 4.99962516784668 and perplexity is 148.3575395032274
At time: 347.9847185611725 and batch: 450, loss is 5.020857553482056 and perplexity is 151.5412027745449
At time: 348.3098039627075 and batch: 500, loss is 4.998364706039428 and perplexity is 148.17065829392172
At time: 348.6342167854309 and batch: 550, loss is 5.052301139831543 and perplexity is 156.38190739963102
At time: 348.95999360084534 and batch: 600, loss is 5.093524751663208 and perplexity is 162.96325593171557
At time: 349.2855293750763 and batch: 650, loss is 5.06555775642395 and perplexity is 158.46880442271944
At time: 349.6101016998291 and batch: 700, loss is 5.047626981735229 and perplexity is 155.65265927924744
At time: 349.93553495407104 and batch: 750, loss is 4.97891131401062 and perplexity is 145.3160919120362
At time: 350.2611436843872 and batch: 800, loss is 5.013519582748413 and perplexity is 150.43326783220218
At time: 350.58656573295593 and batch: 850, loss is 4.998401327133179 and perplexity is 148.17608456484754
At time: 350.91401958465576 and batch: 900, loss is 5.093849258422852 and perplexity is 163.01614719116048
At time: 351.2531359195709 and batch: 950, loss is 5.048424205780029 and perplexity is 155.77679879879196
At time: 351.5784978866577 and batch: 1000, loss is 5.021061563491822 and perplexity is 151.5721218505961
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.216221134837081 and perplexity of 184.23666147702198
Finished 48 epochs...
Completing Train Step...
At time: 352.68860912323 and batch: 50, loss is 5.124406967163086 and perplexity is 168.07443843009034
At time: 353.0279700756073 and batch: 100, loss is 5.093881025314331 and perplexity is 163.0213257896712
At time: 353.35518503189087 and batch: 150, loss is 5.09969241142273 and perplexity is 163.97146379311516
At time: 353.67996883392334 and batch: 200, loss is 5.1048533153533935 and perplexity is 164.81989221074772
At time: 354.00615072250366 and batch: 250, loss is 5.094713478088379 and perplexity is 163.1570898452961
At time: 354.3328528404236 and batch: 300, loss is 4.992651596069336 and perplexity is 147.32655654772768
At time: 354.6592981815338 and batch: 350, loss is 5.05324764251709 and perplexity is 156.52999336578264
At time: 354.9858865737915 and batch: 400, loss is 4.996210241317749 and perplexity is 147.85177347423448
At time: 355.3122441768646 and batch: 450, loss is 5.0186171722412105 and perplexity is 151.20207273876866
At time: 355.6374659538269 and batch: 500, loss is 4.995772094726562 and perplexity is 147.7870069133458
At time: 355.96258068084717 and batch: 550, loss is 5.049362268447876 and perplexity is 155.92299575849614
At time: 356.28997254371643 and batch: 600, loss is 5.090995092391967 and perplexity is 162.55153539636711
At time: 356.61554050445557 and batch: 650, loss is 5.063811912536621 and perplexity is 158.19238399285464
At time: 356.9395797252655 and batch: 700, loss is 5.04522783279419 and perplexity is 155.27967297032814
At time: 357.2638223171234 and batch: 750, loss is 4.9763616180419925 and perplexity is 144.94605200341053
At time: 357.590603351593 and batch: 800, loss is 5.011339588165283 and perplexity is 150.1056813213496
At time: 357.91703820228577 and batch: 850, loss is 4.996053237915039 and perplexity is 147.828562064885
At time: 358.2418487071991 and batch: 900, loss is 5.091484823226929 and perplexity is 162.63116139158214
At time: 358.56662464141846 and batch: 950, loss is 5.046496200561523 and perplexity is 155.4767496588848
At time: 358.89426016807556 and batch: 1000, loss is 5.018286514282226 and perplexity is 151.15208483490727
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.21454936702077 and perplexity of 183.9289178653866
Finished 49 epochs...
Completing Train Step...
At time: 360.02395033836365 and batch: 50, loss is 5.12202561378479 and perplexity is 167.67466998222025
At time: 360.35053992271423 and batch: 100, loss is 5.0922230529785155 and perplexity is 162.7512648799862
At time: 360.6765727996826 and batch: 150, loss is 5.09677529335022 and perplexity is 163.49383665878148
At time: 361.00383710861206 and batch: 200, loss is 5.101761293411255 and perplexity is 164.3110525646076
At time: 361.3289635181427 and batch: 250, loss is 5.091656007766724 and perplexity is 162.65900371512097
At time: 361.65373826026917 and batch: 300, loss is 4.9903016281127925 and perplexity is 146.9807503365889
At time: 361.9794268608093 and batch: 350, loss is 5.050883760452271 and perplexity is 156.16041191743747
At time: 362.3044991493225 and batch: 400, loss is 4.993586759567261 and perplexity is 147.4643954065755
At time: 362.631046295166 and batch: 450, loss is 5.015743923187256 and perplexity is 150.76825505788725
At time: 362.95643615722656 and batch: 500, loss is 4.992048826217651 and perplexity is 147.23777929990044
At time: 363.28299283981323 and batch: 550, loss is 5.046515970230103 and perplexity is 155.47982341308085
At time: 363.60772728919983 and batch: 600, loss is 5.087855348587036 and perplexity is 162.04196559804416
At time: 363.9333028793335 and batch: 650, loss is 5.061117706298828 and perplexity is 157.76675470912662
At time: 364.25852966308594 and batch: 700, loss is 5.0418635272979735 and perplexity is 154.75814249893313
At time: 364.5849051475525 and batch: 750, loss is 4.973085260391235 and perplexity is 144.4719340111567
At time: 364.9122955799103 and batch: 800, loss is 5.008814477920533 and perplexity is 149.72712607565907
At time: 365.23717617988586 and batch: 850, loss is 4.993087387084961 and perplexity is 147.39077412914034
At time: 365.5635898113251 and batch: 900, loss is 5.088831310272217 and perplexity is 162.2001895455554
At time: 365.88855624198914 and batch: 950, loss is 5.043263664245606 and perplexity is 154.9749768556668
At time: 366.2154984474182 and batch: 1000, loss is 5.015093746185303 and perplexity is 150.67026086606106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.211167498332698 and perplexity of 183.30794503335574
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f4997a568d0>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'num_layers': 1, 'wordvec_source': 'glove', 'data': 'ptb', 'dropout': 0.2460807837958705, 'seq_len': 20, 'batch_size': 50, 'lr': 18.419787295130018, 'anneal': 5.075412414399972, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.611194372177124 and batch: 50, loss is 6.696577167510986 and perplexity is 809.6298496746748
At time: 0.9585750102996826 and batch: 100, loss is 5.847197313308715 and perplexity is 346.2625538008043
At time: 1.2901580333709717 and batch: 150, loss is 5.7529753398895265 and perplexity is 315.1268763800384
At time: 1.6223433017730713 and batch: 200, loss is 5.768536987304688 and perplexity is 320.0691247484037
At time: 1.955143690109253 and batch: 250, loss is 5.780052375793457 and perplexity is 323.7761480058096
At time: 2.285510778427124 and batch: 300, loss is 5.660823669433594 and perplexity is 287.38525554789294
At time: 2.6177432537078857 and batch: 350, loss is 5.721722497940063 and perplexity is 305.4305736447634
At time: 2.9630115032196045 and batch: 400, loss is 5.720952138900757 and perplexity is 305.1953730475605
At time: 3.294218063354492 and batch: 450, loss is 5.7290280818939205 and perplexity is 307.670092879951
At time: 3.625516414642334 and batch: 500, loss is 5.777558841705322 and perplexity is 322.9698068811723
At time: 3.955260992050171 and batch: 550, loss is 5.795578784942627 and perplexity is 328.8424579759514
At time: 4.286625862121582 and batch: 600, loss is 5.845767059326172 and perplexity is 345.7676643973584
At time: 4.617257356643677 and batch: 650, loss is 5.8422974109649655 and perplexity is 344.5700510372914
At time: 4.948304653167725 and batch: 700, loss is 5.851241397857666 and perplexity is 347.66570416152507
At time: 5.2782793045043945 and batch: 750, loss is 5.7701962184906 and perplexity is 320.6006342484637
At time: 5.608052492141724 and batch: 800, loss is 5.811252660751343 and perplexity is 334.0372991192467
At time: 5.939307451248169 and batch: 850, loss is 5.781929655075073 and perplexity is 324.38453713982796
At time: 6.271849632263184 and batch: 900, loss is 5.873911876678466 and perplexity is 355.63747262683535
At time: 6.603423118591309 and batch: 950, loss is 5.902341766357422 and perplexity is 365.89330201747987
At time: 6.934314489364624 and batch: 1000, loss is 5.826697311401367 and perplexity is 339.2364345750816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.792782667206555 and perplexity of 327.92426003982524
Finished 1 epochs...
Completing Train Step...
At time: 8.071197509765625 and batch: 50, loss is 5.801213846206665 and perplexity is 330.70073621141574
At time: 8.396515369415283 and batch: 100, loss is 5.808270120620728 and perplexity is 333.04250371554514
At time: 8.721152782440186 and batch: 150, loss is 5.827345380783081 and perplexity is 339.45635457550367
At time: 9.047253131866455 and batch: 200, loss is 5.795798463821411 and perplexity is 328.91470565376994
At time: 9.372323751449585 and batch: 250, loss is 5.827878332138061 and perplexity is 339.6373165173041
At time: 9.696842193603516 and batch: 300, loss is 5.73756046295166 and perplexity is 310.30648269933664
At time: 10.019948244094849 and batch: 350, loss is 5.7521994686126705 and perplexity is 314.88247331301005
At time: 10.344707727432251 and batch: 400, loss is 5.671479825973511 and perplexity is 290.464052756776
At time: 10.668719291687012 and batch: 450, loss is 5.657658414840698 and perplexity is 286.4770461630696
At time: 10.994462251663208 and batch: 500, loss is 5.682496519088745 and perplexity is 293.6816974459863
At time: 11.340724468231201 and batch: 550, loss is 5.748693256378174 and perplexity is 313.7803617801909
At time: 11.667359352111816 and batch: 600, loss is 5.761596889495849 and perplexity is 317.8555039743078
At time: 11.995122194290161 and batch: 650, loss is 5.7489473915100096 and perplexity is 313.8601145273575
At time: 12.320186376571655 and batch: 700, loss is 5.7317283153533936 and perplexity is 308.501996620914
At time: 12.64435338973999 and batch: 750, loss is 5.646376857757568 and perplexity is 283.26330112315105
At time: 12.969699144363403 and batch: 800, loss is 5.720581216812134 and perplexity is 305.0821903346333
At time: 13.294885635375977 and batch: 850, loss is 5.676543474197388 and perplexity is 291.93859066157916
At time: 13.6195809841156 and batch: 900, loss is 5.75203577041626 and perplexity is 314.8309318387857
At time: 13.941899299621582 and batch: 950, loss is 5.758565626144409 and perplexity is 316.8934590764279
At time: 14.26569676399231 and batch: 1000, loss is 5.746431465148926 and perplexity is 313.0714581082714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.862373165967988 and perplexity of 351.55745902896643
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 15.37913203239441 and batch: 50, loss is 5.653483018875122 and perplexity is 285.283384798896
At time: 15.71810507774353 and batch: 100, loss is 5.500548219680786 and perplexity is 244.82611397438365
At time: 16.0441472530365 and batch: 150, loss is 5.40849796295166 and perplexity is 223.29593689373493
At time: 16.36990523338318 and batch: 200, loss is 5.404377336502075 and perplexity is 222.37771088253592
At time: 16.693819999694824 and batch: 250, loss is 5.388412456512452 and perplexity is 218.85566672917716
At time: 17.017664194107056 and batch: 300, loss is 5.272382726669312 and perplexity is 194.87975486991346
At time: 17.34323477745056 and batch: 350, loss is 5.331987438201904 and perplexity is 206.8486648484729
At time: 17.671794176101685 and batch: 400, loss is 5.256064701080322 and perplexity is 191.72550757907345
At time: 17.996824026107788 and batch: 450, loss is 5.241773862838745 and perplexity is 189.00507429836685
At time: 18.321462869644165 and batch: 500, loss is 5.238016557693482 and perplexity is 188.29625701552118
At time: 18.64616870880127 and batch: 550, loss is 5.304644050598145 and perplexity is 201.26934795775506
At time: 18.970935821533203 and batch: 600, loss is 5.313743476867676 and perplexity is 203.10914138765702
At time: 19.29673480987549 and batch: 650, loss is 5.278033323287964 and perplexity is 195.9840588035645
At time: 19.62192940711975 and batch: 700, loss is 5.260277433395386 and perplexity is 192.534899499192
At time: 19.961756706237793 and batch: 750, loss is 5.188007154464722 and perplexity is 179.11125596448085
At time: 20.28591251373291 and batch: 800, loss is 5.243270673751831 and perplexity is 189.28819098942287
At time: 20.61061692237854 and batch: 850, loss is 5.209067459106445 and perplexity is 182.92339508461765
At time: 20.935102701187134 and batch: 900, loss is 5.278470544815064 and perplexity is 196.06976598818952
At time: 21.258950233459473 and batch: 950, loss is 5.227257957458496 and perplexity is 186.28131129367523
At time: 21.584498643875122 and batch: 1000, loss is 5.198175210952758 and perplexity is 180.94175989526184
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.353593779773247 and perplexity of 211.36653936803785
Finished 3 epochs...
Completing Train Step...
At time: 22.708669185638428 and batch: 50, loss is 5.327705783843994 and perplexity is 205.96490369123532
At time: 23.048496961593628 and batch: 100, loss is 5.29633113861084 and perplexity is 199.6031486616364
At time: 23.373455286026 and batch: 150, loss is 5.291680631637573 and perplexity is 198.67704791497067
At time: 23.698119401931763 and batch: 200, loss is 5.300311107635498 and perplexity is 200.39914598214722
At time: 24.020352125167847 and batch: 250, loss is 5.300151033401489 and perplexity is 200.36706980971104
At time: 24.34488010406494 and batch: 300, loss is 5.186991357803345 and perplexity is 178.92940772471113
At time: 24.66999840736389 and batch: 350, loss is 5.24346619606018 and perplexity is 189.32520467185077
At time: 24.995153188705444 and batch: 400, loss is 5.179902753829956 and perplexity is 177.66553286064243
At time: 25.3193461894989 and batch: 450, loss is 5.1764742946624756 and perplexity is 177.05745681333772
At time: 25.643974781036377 and batch: 500, loss is 5.170870895385742 and perplexity is 176.0681076357603
At time: 25.96961998939514 and batch: 550, loss is 5.2365037536621095 and perplexity is 188.01161703535558
At time: 26.29383945465088 and batch: 600, loss is 5.254284782409668 and perplexity is 191.38455529217796
At time: 26.620195388793945 and batch: 650, loss is 5.2205948066711425 and perplexity is 185.0442168779708
At time: 26.94459366798401 and batch: 700, loss is 5.197101974487305 and perplexity is 180.7476707708118
At time: 27.268776178359985 and batch: 750, loss is 5.137595310211181 and perplexity is 170.30574304582885
At time: 27.592865467071533 and batch: 800, loss is 5.191911907196045 and perplexity is 179.81200837248883
At time: 27.915783166885376 and batch: 850, loss is 5.158910131454467 and perplexity is 173.9747426430706
At time: 28.253591775894165 and batch: 900, loss is 5.237040023803711 and perplexity is 188.11246909140215
At time: 28.577333211898804 and batch: 950, loss is 5.203269548416138 and perplexity is 181.8658901985927
At time: 28.901890754699707 and batch: 1000, loss is 5.173945627212524 and perplexity is 176.61030297546452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.33146518614234 and perplexity of 206.74066591102627
Finished 4 epochs...
Completing Train Step...
At time: 30.026296854019165 and batch: 50, loss is 5.28039810180664 and perplexity is 196.4480661168554
At time: 30.352317810058594 and batch: 100, loss is 5.251251153945923 and perplexity is 190.80484541449954
At time: 30.677393436431885 and batch: 150, loss is 5.255615177154541 and perplexity is 191.6393417444873
At time: 31.003291130065918 and batch: 200, loss is 5.2646993637084964 and perplexity is 193.38816054771635
At time: 31.328323364257812 and batch: 250, loss is 5.264301166534424 and perplexity is 193.31116925856142
At time: 31.650536060333252 and batch: 300, loss is 5.1492425727844235 and perplexity is 172.3009354624074
At time: 31.97136425971985 and batch: 350, loss is 5.200466842651367 and perplexity is 181.3568872456647
At time: 32.298989057540894 and batch: 400, loss is 5.143187208175659 and perplexity is 171.26074302641385
At time: 32.62558674812317 and batch: 450, loss is 5.144974422454834 and perplexity is 171.56709634978532
At time: 32.951828956604004 and batch: 500, loss is 5.13343189239502 and perplexity is 169.59816307865987
At time: 33.27722120285034 and batch: 550, loss is 5.2007621479034425 and perplexity is 181.41045079537793
At time: 33.60110545158386 and batch: 600, loss is 5.216001958847046 and perplexity is 184.19628564920959
At time: 33.925190925598145 and batch: 650, loss is 5.1889317512512205 and perplexity is 179.27693823901748
At time: 34.253392457962036 and batch: 700, loss is 5.1691272163391115 and perplexity is 175.7613688703927
At time: 34.578325271606445 and batch: 750, loss is 5.105129718780518 and perplexity is 164.8654552904186
At time: 34.902747631073 and batch: 800, loss is 5.1538181972503665 and perplexity is 173.0911262677635
At time: 35.22746396064758 and batch: 850, loss is 5.126340637207031 and perplexity is 168.39975336133932
At time: 35.55086159706116 and batch: 900, loss is 5.212339963912964 and perplexity is 183.52299333322622
At time: 35.87550711631775 and batch: 950, loss is 5.177524690628052 and perplexity is 177.2435349624093
At time: 36.2018461227417 and batch: 1000, loss is 5.147418584823608 and perplexity is 171.98694707305927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.309760116949314 and perplexity of 202.30169381959774
Finished 5 epochs...
Completing Train Step...
At time: 37.312997579574585 and batch: 50, loss is 5.244821538925171 and perplexity is 189.5819792066256
At time: 37.652278423309326 and batch: 100, loss is 5.213911275863648 and perplexity is 183.8115918856736
At time: 37.97664523124695 and batch: 150, loss is 5.223778467178345 and perplexity is 185.6342736151108
At time: 38.302111864089966 and batch: 200, loss is 5.231588068008423 and perplexity is 187.08967886236655
At time: 38.62707996368408 and batch: 250, loss is 5.230322017669677 and perplexity is 186.85296378928513
At time: 38.9504177570343 and batch: 300, loss is 5.116652317047119 and perplexity is 166.77612047351954
At time: 39.27690386772156 and batch: 350, loss is 5.166121654510498 and perplexity is 175.2339002759771
At time: 39.60322189331055 and batch: 400, loss is 5.10719708442688 and perplexity is 165.2066450294101
At time: 39.928696155548096 and batch: 450, loss is 5.111125297546387 and perplexity is 165.85688825067
At time: 40.253390073776245 and batch: 500, loss is 5.104926137924195 and perplexity is 164.83189525605866
At time: 40.577165365219116 and batch: 550, loss is 5.16592173576355 and perplexity is 175.19887123580867
At time: 40.902225971221924 and batch: 600, loss is 5.185843801498413 and perplexity is 178.72419392445426
At time: 41.22813320159912 and batch: 650, loss is 5.162299671173096 and perplexity is 174.56543746954543
At time: 41.55286955833435 and batch: 700, loss is 5.140015239715576 and perplexity is 170.71837000090085
At time: 41.87990856170654 and batch: 750, loss is 5.077755107879638 and perplexity is 160.41354032801283
At time: 42.20407772064209 and batch: 800, loss is 5.124798851013184 and perplexity is 168.14031699565518
At time: 42.52981352806091 and batch: 850, loss is 5.0939179801940915 and perplexity is 163.02735033448167
At time: 42.854125022888184 and batch: 900, loss is 5.183853578567505 and perplexity is 178.36884666282592
At time: 43.17827391624451 and batch: 950, loss is 5.149725980758667 and perplexity is 172.38424724374693
At time: 43.5026490688324 and batch: 1000, loss is 5.110777187347412 and perplexity is 165.79916182446146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.281468461199505 and perplexity of 196.65844872204664
Finished 6 epochs...
Completing Train Step...
At time: 44.61462068557739 and batch: 50, loss is 5.2048594093322755 and perplexity is 182.1552616386167
At time: 44.95433282852173 and batch: 100, loss is 5.174589195251465 and perplexity is 176.7240003038662
At time: 45.279613733291626 and batch: 150, loss is 5.189159994125366 and perplexity is 179.31786159272346
At time: 45.620123863220215 and batch: 200, loss is 5.200699443817139 and perplexity is 181.39907597544243
At time: 45.94468593597412 and batch: 250, loss is 5.196813745498657 and perplexity is 180.695581559636
At time: 46.26766633987427 and batch: 300, loss is 5.0796615028381344 and perplexity is 160.71964357670674
At time: 46.59165358543396 and batch: 350, loss is 5.129934139251709 and perplexity is 169.00598681757833
At time: 46.91638469696045 and batch: 400, loss is 5.066757793426514 and perplexity is 158.65908700201143
At time: 47.2427716255188 and batch: 450, loss is 5.076012496948242 and perplexity is 160.13424536100231
At time: 47.567824840545654 and batch: 500, loss is 5.065909442901611 and perplexity is 158.52454555949348
At time: 47.8926477432251 and batch: 550, loss is 5.125325889587402 and perplexity is 168.22895678483547
At time: 48.217872858047485 and batch: 600, loss is 5.149738941192627 and perplexity is 172.38648143287705
At time: 48.54253387451172 and batch: 650, loss is 5.1242336082458495 and perplexity is 168.04530375288
At time: 48.867390632629395 and batch: 700, loss is 5.100447769165039 and perplexity is 164.09536769780468
At time: 49.19134306907654 and batch: 750, loss is 5.04318787574768 and perplexity is 154.96323198002372
At time: 49.51701021194458 and batch: 800, loss is 5.085007028579712 and perplexity is 161.58107491907384
At time: 49.841588735580444 and batch: 850, loss is 5.0537694072723385 and perplexity is 156.61168650990854
At time: 50.16665005683899 and batch: 900, loss is 5.145447931289673 and perplexity is 171.64835412229291
At time: 50.49194002151489 and batch: 950, loss is 5.113733606338501 and perplexity is 166.2900589063602
At time: 50.81675052642822 and batch: 1000, loss is 5.0715979480743405 and perplexity is 159.42888298234038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.252175680020961 and perplexity of 190.98133103953907
Finished 7 epochs...
Completing Train Step...
At time: 51.93891716003418 and batch: 50, loss is 5.1645760917663575 and perplexity is 174.96327447664845
At time: 52.26396584510803 and batch: 100, loss is 5.1393714427948 and perplexity is 170.60849741158654
At time: 52.590147972106934 and batch: 150, loss is 5.151219348907471 and perplexity is 172.64187270488668
At time: 52.91612720489502 and batch: 200, loss is 5.16357611656189 and perplexity is 174.78840298862724
At time: 53.2413604259491 and batch: 250, loss is 5.154993209838867 and perplexity is 173.29463005642862
At time: 53.564345836639404 and batch: 300, loss is 5.03760440826416 and perplexity is 154.1004108216073
At time: 53.9040961265564 and batch: 350, loss is 5.098002090454101 and perplexity is 163.69453350608123
At time: 54.22923827171326 and batch: 400, loss is 5.03765489578247 and perplexity is 154.10819116532372
At time: 54.55451154708862 and batch: 450, loss is 5.045722370147705 and perplexity is 155.35648356014107
At time: 54.87981033325195 and batch: 500, loss is 5.03628475189209 and perplexity is 153.89718535591132
At time: 55.20472693443298 and batch: 550, loss is 5.094851608276367 and perplexity is 163.17962832137417
At time: 55.52995181083679 and batch: 600, loss is 5.117845506668091 and perplexity is 166.97523477643094
At time: 55.855048418045044 and batch: 650, loss is 5.093725290298462 and perplexity is 162.99593963772006
At time: 56.17938232421875 and batch: 700, loss is 5.071968269348145 and perplexity is 159.48793382258393
At time: 56.50321412086487 and batch: 750, loss is 5.013291101455689 and perplexity is 150.39890057098685
At time: 56.82829713821411 and batch: 800, loss is 5.055828971862793 and perplexity is 156.93457078026634
At time: 57.1531445980072 and batch: 850, loss is 5.027492933273315 and perplexity is 152.5500796479984
At time: 57.47754454612732 and batch: 900, loss is 5.121838006973267 and perplexity is 167.64321602259247
At time: 57.801445722579956 and batch: 950, loss is 5.086727066040039 and perplexity is 161.85923957905237
At time: 58.12706160545349 and batch: 1000, loss is 5.042983093261719 and perplexity is 154.93150147318318
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.235497079244474 and perplexity of 187.8224457831732
Finished 8 epochs...
Completing Train Step...
At time: 59.23543620109558 and batch: 50, loss is 5.136776885986328 and perplexity is 170.1664177214757
At time: 59.57623815536499 and batch: 100, loss is 5.109275369644165 and perplexity is 165.55034859087516
At time: 59.90416765213013 and batch: 150, loss is 5.123994188308716 and perplexity is 168.0050751727741
At time: 60.22972893714905 and batch: 200, loss is 5.137361307144165 and perplexity is 170.26589564201475
At time: 60.554166316986084 and batch: 250, loss is 5.133173656463623 and perplexity is 169.55437239346162
At time: 60.879313468933105 and batch: 300, loss is 5.0180870723724365 and perplexity is 151.12194178043316
At time: 61.20586609840393 and batch: 350, loss is 5.078481502532959 and perplexity is 160.530106197314
At time: 61.53263211250305 and batch: 400, loss is 5.015423564910889 and perplexity is 150.7199629353696
At time: 61.860992670059204 and batch: 450, loss is 5.0268424606323245 and perplexity is 152.45088226089726
At time: 62.18606734275818 and batch: 500, loss is 5.0186444187164305 and perplexity is 151.20619251842123
At time: 62.52593803405762 and batch: 550, loss is 5.073971748352051 and perplexity is 159.80778484979427
At time: 62.855003118515015 and batch: 600, loss is 5.0955625247955325 and perplexity is 163.29567666019202
At time: 63.182857275009155 and batch: 650, loss is 5.0765555953979495 and perplexity is 160.22123764195118
At time: 63.509605407714844 and batch: 700, loss is 5.05441593170166 and perplexity is 156.71297252956944
At time: 63.83556413650513 and batch: 750, loss is 4.99495909690857 and perplexity is 147.66690522701734
At time: 64.16289043426514 and batch: 800, loss is 5.037434892654419 and perplexity is 154.07429061045983
At time: 64.48794269561768 and batch: 850, loss is 5.009225063323974 and perplexity is 149.78861447037988
At time: 64.81234741210938 and batch: 900, loss is 5.10566933631897 and perplexity is 164.95444358924223
At time: 65.13772201538086 and batch: 950, loss is 5.069391117095948 and perplexity is 159.07743831664305
At time: 65.46497988700867 and batch: 1000, loss is 5.02422064781189 and perplexity is 152.051708091933
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.227466490210556 and perplexity of 186.32016109876423
Finished 9 epochs...
Completing Train Step...
At time: 66.59201192855835 and batch: 50, loss is 5.114784774780273 and perplexity is 166.464949672186
At time: 66.93579006195068 and batch: 100, loss is 5.090496997833252 and perplexity is 162.47058952209872
At time: 67.26102304458618 and batch: 150, loss is 5.101685247421265 and perplexity is 164.2985578430417
At time: 67.58451867103577 and batch: 200, loss is 5.113477087020874 and perplexity is 166.2474077645761
At time: 67.9079418182373 and batch: 250, loss is 5.10981333732605 and perplexity is 165.63943328833736
At time: 68.23323559761047 and batch: 300, loss is 4.994247379302979 and perplexity is 147.5618454816741
At time: 68.55768203735352 and batch: 350, loss is 5.055578746795654 and perplexity is 156.89530672938625
At time: 68.88408470153809 and batch: 400, loss is 4.993551225662231 and perplexity is 147.4591555138513
At time: 69.20864748954773 and batch: 450, loss is 5.0050392341613765 and perplexity is 149.16293532626065
At time: 69.53343033790588 and batch: 500, loss is 4.993503227233886 and perplexity is 147.4520778760007
At time: 69.85829591751099 and batch: 550, loss is 5.048326187133789 and perplexity is 155.76153051615844
At time: 70.18366312980652 and batch: 600, loss is 5.070126647949219 and perplexity is 159.19448772203177
At time: 70.50770139694214 and batch: 650, loss is 5.05031849861145 and perplexity is 156.07216533909502
At time: 70.84756517410278 and batch: 700, loss is 5.027828559875489 and perplexity is 152.6012881058716
At time: 71.17353367805481 and batch: 750, loss is 4.967866487503052 and perplexity is 143.71993177005774
At time: 71.49820017814636 and batch: 800, loss is 5.01031044960022 and perplexity is 149.9512812390337
At time: 71.82374334335327 and batch: 850, loss is 4.978085155487061 and perplexity is 145.19608736230916
At time: 72.14934778213501 and batch: 900, loss is 5.078665161132813 and perplexity is 160.55959163939718
At time: 72.4761712551117 and batch: 950, loss is 5.038316287994385 and perplexity is 154.2101508367022
At time: 72.80390572547913 and batch: 1000, loss is 4.991398038864136 and perplexity is 147.14198998778963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.205701502358041 and perplexity of 182.30871791721177
Finished 10 epochs...
Completing Train Step...
At time: 73.92500424385071 and batch: 50, loss is 5.086348266601562 and perplexity is 161.7979390010328
At time: 74.2490382194519 and batch: 100, loss is 5.055090417861939 and perplexity is 156.81870891553262
At time: 74.57277631759644 and batch: 150, loss is 5.072593421936035 and perplexity is 159.5876692888446
At time: 74.89805483818054 and batch: 200, loss is 5.084704008102417 and perplexity is 161.5321199621818
At time: 75.2227430343628 and batch: 250, loss is 5.08215594291687 and perplexity is 161.12104953049737
At time: 75.54725861549377 and batch: 300, loss is 4.963204364776612 and perplexity is 143.05145128973632
At time: 75.87454986572266 and batch: 350, loss is 5.025181941986084 and perplexity is 152.19794479007967
At time: 76.19898962974548 and batch: 400, loss is 4.969252862930298 and perplexity is 143.9193197332196
At time: 76.52263593673706 and batch: 450, loss is 4.977947063446045 and perplexity is 145.1760383225961
At time: 76.84759211540222 and batch: 500, loss is 4.96467378616333 and perplexity is 143.26180866562578
At time: 77.17319560050964 and batch: 550, loss is 5.022027082443238 and perplexity is 151.71853827929309
At time: 77.49764585494995 and batch: 600, loss is 5.044887104034424 and perplexity is 155.22677373259774
At time: 77.82144117355347 and batch: 650, loss is 5.024332180023193 and perplexity is 152.06866770092265
At time: 78.14674115180969 and batch: 700, loss is 5.00217758178711 and perplexity is 148.73669302775116
At time: 78.47080516815186 and batch: 750, loss is 4.94456371307373 and perplexity is 140.40957862280575
At time: 78.79427361488342 and batch: 800, loss is 4.986403970718384 and perplexity is 146.40898472527755
At time: 79.11731243133545 and batch: 850, loss is 4.955371408462525 and perplexity is 141.93531256080664
At time: 79.4676022529602 and batch: 900, loss is 5.055874671936035 and perplexity is 156.94174286552638
At time: 79.79326343536377 and batch: 950, loss is 5.014950199127197 and perplexity is 150.64863414563285
At time: 80.11727786064148 and batch: 1000, loss is 4.971308879852295 and perplexity is 144.2155246868486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.189350035132431 and perplexity of 179.35194257801513
Finished 11 epochs...
Completing Train Step...
At time: 81.23096823692322 and batch: 50, loss is 5.061275272369385 and perplexity is 157.79161535527643
At time: 81.57055878639221 and batch: 100, loss is 5.032126054763794 and perplexity is 153.2585025378083
At time: 81.89558029174805 and batch: 150, loss is 5.045640993118286 and perplexity is 155.34384162539646
At time: 82.22123980522156 and batch: 200, loss is 5.057965459823609 and perplexity is 157.2702180268508
At time: 82.5445306301117 and batch: 250, loss is 5.05536657333374 and perplexity is 156.86202124027292
At time: 82.86975193023682 and batch: 300, loss is 4.934766588211059 and perplexity is 139.040685009218
At time: 83.19372200965881 and batch: 350, loss is 5.000446557998657 and perplexity is 148.47944898591706
At time: 83.51760268211365 and batch: 400, loss is 4.942667732238769 and perplexity is 140.14361696147031
At time: 83.84331512451172 and batch: 450, loss is 4.956328496932984 and perplexity is 142.07122224042863
At time: 84.16794037818909 and batch: 500, loss is 4.941064109802246 and perplexity is 139.91905961369872
At time: 84.49247360229492 and batch: 550, loss is 4.99470739364624 and perplexity is 147.629741662525
At time: 84.81618523597717 and batch: 600, loss is 5.019566955566407 and perplexity is 151.34575016660278
At time: 85.1418719291687 and batch: 650, loss is 4.99922474861145 and perplexity is 148.29814618262657
At time: 85.46856713294983 and batch: 700, loss is 4.976912145614624 and perplexity is 145.02587077078263
At time: 85.79401683807373 and batch: 750, loss is 4.924551115036011 and perplexity is 137.62754883766178
At time: 86.1180591583252 and batch: 800, loss is 4.9609567832946775 and perplexity is 142.73029254727254
At time: 86.44228649139404 and batch: 850, loss is 4.930165214538574 and perplexity is 138.4023765359368
At time: 86.76792550086975 and batch: 900, loss is 5.030811901092529 and perplexity is 153.0572295948007
At time: 87.09314608573914 and batch: 950, loss is 4.990268898010254 and perplexity is 146.97593972028562
At time: 87.41750073432922 and batch: 1000, loss is 4.943243160247802 and perplexity is 140.22428273040273
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.173985923208842 and perplexity of 176.6174198069718
Finished 12 epochs...
Completing Train Step...
At time: 88.54540681838989 and batch: 50, loss is 5.0320000743865965 and perplexity is 153.23919618998585
At time: 88.8842511177063 and batch: 100, loss is 5.002973260879517 and perplexity is 148.85508680014553
At time: 89.20930767059326 and batch: 150, loss is 5.0147401809692385 and perplexity is 150.61699851913488
At time: 89.53558564186096 and batch: 200, loss is 5.032826271057129 and perplexity is 153.36585421868912
At time: 89.86010265350342 and batch: 250, loss is 5.027988367080688 and perplexity is 152.62567683992947
At time: 90.18433451652527 and batch: 300, loss is 4.907795009613037 and perplexity is 135.34066029961502
At time: 90.51039719581604 and batch: 350, loss is 4.974509077072144 and perplexity is 144.67778207089998
At time: 90.83676362037659 and batch: 400, loss is 4.915374059677124 and perplexity is 136.37031089768695
At time: 91.16318583488464 and batch: 450, loss is 4.931579723358154 and perplexity is 138.59828644367857
At time: 91.48707151412964 and batch: 500, loss is 4.914392967224121 and perplexity is 136.2365846244967
At time: 91.81136894226074 and batch: 550, loss is 4.967901268005371 and perplexity is 143.72493050840677
At time: 92.13667511940002 and batch: 600, loss is 4.991495876312256 and perplexity is 147.1563866888581
At time: 92.4632580280304 and batch: 650, loss is 4.976009044647217 and perplexity is 144.8949568897145
At time: 92.78847098350525 and batch: 700, loss is 4.951357135772705 and perplexity is 141.3666875833183
At time: 93.11190748214722 and batch: 750, loss is 4.900218296051025 and perplexity is 134.31909781343316
At time: 93.4369306564331 and batch: 800, loss is 4.939652013778686 and perplexity is 139.7216199007229
At time: 93.76222729682922 and batch: 850, loss is 4.9068178367614745 and perplexity is 135.2084736757662
At time: 94.08950686454773 and batch: 900, loss is 5.010719738006592 and perplexity is 150.0126671213735
At time: 94.4158718585968 and batch: 950, loss is 4.964933061599732 and perplexity is 143.29895774930156
At time: 94.74245929718018 and batch: 1000, loss is 4.921128349304199 and perplexity is 137.15728723649923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.16118900950362 and perplexity of 174.37166195876858
Finished 13 epochs...
Completing Train Step...
At time: 95.8825466632843 and batch: 50, loss is 5.0079190635681154 and perplexity is 149.59311826323764
At time: 96.20803380012512 and batch: 100, loss is 4.977354745864869 and perplexity is 145.0900734644938
At time: 96.54854679107666 and batch: 150, loss is 4.99706093788147 and perplexity is 147.97760398407164
At time: 96.87465167045593 and batch: 200, loss is 5.01089373588562 and perplexity is 150.03877127824813
At time: 97.19974827766418 and batch: 250, loss is 5.002227849960327 and perplexity is 148.74416993752385
At time: 97.52405643463135 and batch: 300, loss is 4.88531497001648 and perplexity is 132.3321394270415
At time: 97.85096335411072 and batch: 350, loss is 4.952103090286255 and perplexity is 141.47218004338333
At time: 98.17737483978271 and batch: 400, loss is 4.892461862564087 and perplexity is 133.28129070916916
At time: 98.50307321548462 and batch: 450, loss is 4.912423048019409 and perplexity is 135.9684737251676
At time: 98.82799363136292 and batch: 500, loss is 4.89488522529602 and perplexity is 133.60467129791186
At time: 99.15465521812439 and batch: 550, loss is 4.948743000030517 and perplexity is 140.9976184810936
At time: 99.48116087913513 and batch: 600, loss is 4.97485761642456 and perplexity is 144.72821676009846
At time: 99.8074312210083 and batch: 650, loss is 4.956462535858154 and perplexity is 142.09026659066905
At time: 100.13198447227478 and batch: 700, loss is 4.929234647750855 and perplexity is 138.27364378749087
At time: 100.45630478858948 and batch: 750, loss is 4.879613132476806 and perplexity is 131.57975010599534
At time: 100.78264784812927 and batch: 800, loss is 4.917525930404663 and perplexity is 136.66407813901822
At time: 101.10861158370972 and batch: 850, loss is 4.886393880844116 and perplexity is 132.4749910533396
At time: 101.43280267715454 and batch: 900, loss is 4.993728866577149 and perplexity is 147.48535261991978
At time: 101.75703620910645 and batch: 950, loss is 4.9462317276000975 and perplexity is 140.64397927709066
At time: 102.08211469650269 and batch: 1000, loss is 4.905841989517212 and perplexity is 135.07659521640971
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.149022544302592 and perplexity of 172.2630285196159
Finished 14 epochs...
Completing Train Step...
At time: 103.19163393974304 and batch: 50, loss is 4.989478349685669 and perplexity is 146.85979405281577
At time: 103.53372287750244 and batch: 100, loss is 4.959089670181275 and perplexity is 142.46404757846065
At time: 103.86071753501892 and batch: 150, loss is 4.98014087677002 and perplexity is 145.49487705824725
At time: 104.1867094039917 and batch: 200, loss is 4.995508890151978 and perplexity is 147.74811381570723
At time: 104.51187038421631 and batch: 250, loss is 4.98638388633728 and perplexity is 146.40604422096047
At time: 104.8526918888092 and batch: 300, loss is 4.872440328598023 and perplexity is 130.639331116527
At time: 105.18097114562988 and batch: 350, loss is 4.939373292922974 and perplexity is 139.68268199791748
At time: 105.50718903541565 and batch: 400, loss is 4.874880151748657 and perplexity is 130.95845712817803
At time: 105.83220100402832 and batch: 450, loss is 4.896956844329834 and perplexity is 133.88173616531773
At time: 106.15818309783936 and batch: 500, loss is 4.879680595397949 and perplexity is 131.58862715973333
At time: 106.48398280143738 and batch: 550, loss is 4.933217630386353 and perplexity is 138.8254835642287
At time: 106.81253218650818 and batch: 600, loss is 4.9605427074432376 and perplexity is 142.67120361435414
At time: 107.13850283622742 and batch: 650, loss is 4.942549457550049 and perplexity is 140.1270424989871
At time: 107.46399402618408 and batch: 700, loss is 4.913986530303955 and perplexity is 136.18122429763048
At time: 107.79055857658386 and batch: 750, loss is 4.864099063873291 and perplexity is 129.55416598886526
At time: 108.11666655540466 and batch: 800, loss is 4.90436240196228 and perplexity is 134.87688534771652
At time: 108.44200253486633 and batch: 850, loss is 4.872190494537353 and perplexity is 130.60669703867282
At time: 108.76934719085693 and batch: 900, loss is 4.977829504013061 and perplexity is 145.15897251299117
At time: 109.0943386554718 and batch: 950, loss is 4.932603464126587 and perplexity is 138.7402478133683
At time: 109.42156291007996 and batch: 1000, loss is 4.8911051750183105 and perplexity is 133.1005922453729
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.141275545445884 and perplexity of 170.93366297987228
Finished 15 epochs...
Completing Train Step...
At time: 110.54484128952026 and batch: 50, loss is 4.975927114486694 and perplexity is 144.88308610893174
At time: 110.8883273601532 and batch: 100, loss is 4.943804779052734 and perplexity is 140.30305744310166
At time: 111.21498250961304 and batch: 150, loss is 4.968442287445068 and perplexity is 143.80270952786452
At time: 111.54233360290527 and batch: 200, loss is 4.981699876785278 and perplexity is 145.72188047699044
At time: 111.86788487434387 and batch: 250, loss is 4.9730937290191655 and perplexity is 144.47315749539283
At time: 112.19552040100098 and batch: 300, loss is 4.854094161987304 and perplexity is 128.2644517595285
At time: 112.52255702018738 and batch: 350, loss is 4.921621189117432 and perplexity is 137.22490046820207
At time: 112.84904646873474 and batch: 400, loss is 4.85882061958313 and perplexity is 128.87212318718838
At time: 113.17557382583618 and batch: 450, loss is 4.884371881484985 and perplexity is 132.20739733466374
At time: 113.51884245872498 and batch: 500, loss is 4.867269058227539 and perplexity is 129.9655035880966
At time: 113.84623599052429 and batch: 550, loss is 4.919805240631104 and perplexity is 136.97593324215654
At time: 114.17235541343689 and batch: 600, loss is 4.947274198532105 and perplexity is 140.79067298592764
At time: 114.49921917915344 and batch: 650, loss is 4.925317249298096 and perplexity is 137.7330304196005
At time: 114.82647848129272 and batch: 700, loss is 4.89967001914978 and perplexity is 134.24547393968496
At time: 115.15256714820862 and batch: 750, loss is 4.853410816192627 and perplexity is 128.17683272627048
At time: 115.47888851165771 and batch: 800, loss is 4.891890363693237 and perplexity is 133.2051423634429
At time: 115.80421328544617 and batch: 850, loss is 4.858769407272339 and perplexity is 128.865523516957
At time: 116.13242316246033 and batch: 900, loss is 4.966118278503418 and perplexity is 143.46889878490714
At time: 116.46074438095093 and batch: 950, loss is 4.919207372665405 and perplexity is 136.89406419547825
At time: 116.78872919082642 and batch: 1000, loss is 4.8763746070861815 and perplexity is 131.1543150074013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.134398669731326 and perplexity of 169.7622060227268
Finished 16 epochs...
Completing Train Step...
At time: 117.91672277450562 and batch: 50, loss is 4.961663017272949 and perplexity is 142.8311291325183
At time: 118.24300289154053 and batch: 100, loss is 4.924766759872437 and perplexity is 137.65723070817458
At time: 118.56949996948242 and batch: 150, loss is 4.94805739402771 and perplexity is 140.90098279826896
At time: 118.89553618431091 and batch: 200, loss is 4.962023248672486 and perplexity is 142.88259065853535
At time: 119.22040319442749 and batch: 250, loss is 4.9535785484313966 and perplexity is 141.6810703903254
At time: 119.54618644714355 and batch: 300, loss is 4.834885187149048 and perplexity is 125.82413614974492
At time: 119.87232518196106 and batch: 350, loss is 4.9007430839538575 and perplexity is 134.38960535021803
At time: 120.19716143608093 and batch: 400, loss is 4.837121858596801 and perplexity is 126.10587836744028
At time: 120.52412343025208 and batch: 450, loss is 4.860525350570679 and perplexity is 129.09200285360774
At time: 120.85288667678833 and batch: 500, loss is 4.840286540985107 and perplexity is 126.50559557517096
At time: 121.17846417427063 and batch: 550, loss is 4.894432964324952 and perplexity is 133.54426078122052
At time: 121.5031099319458 and batch: 600, loss is 4.919617834091187 and perplexity is 136.95026546168583
At time: 121.84293746948242 and batch: 650, loss is 4.900814199447632 and perplexity is 134.39916287320065
At time: 122.17006874084473 and batch: 700, loss is 4.872545404434204 and perplexity is 130.65305887469742
At time: 122.4965648651123 and batch: 750, loss is 4.827070074081421 and perplexity is 124.84463872596596
At time: 122.8212218284607 and batch: 800, loss is 4.868597745895386 and perplexity is 130.13830192204563
At time: 123.14711904525757 and batch: 850, loss is 4.832070569992066 and perplexity is 125.4704873038991
At time: 123.47269773483276 and batch: 900, loss is 4.943269624710083 and perplexity is 140.22799373974854
At time: 123.80036997795105 and batch: 950, loss is 4.896344442367553 and perplexity is 133.79977182749886
At time: 124.1259994506836 and batch: 1000, loss is 4.852068939208984 and perplexity is 128.00495053277004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.1183054854230186 and perplexity of 167.052057504084
Finished 17 epochs...
Completing Train Step...
At time: 125.2392590045929 and batch: 50, loss is 4.939225606918335 and perplexity is 139.66205434394135
At time: 125.57861351966858 and batch: 100, loss is 4.900039749145508 and perplexity is 134.29511769501838
At time: 125.90391063690186 and batch: 150, loss is 4.928198547363281 and perplexity is 138.13045260460112
At time: 126.22972917556763 and batch: 200, loss is 4.942518901824951 and perplexity is 140.12276088101203
At time: 126.55625033378601 and batch: 250, loss is 4.933213634490967 and perplexity is 138.82492883322777
At time: 126.88201642036438 and batch: 300, loss is 4.815753469467163 and perplexity is 123.43978538891562
At time: 127.20757102966309 and batch: 350, loss is 4.883001651763916 and perplexity is 132.02636688446637
At time: 127.53229689598083 and batch: 400, loss is 4.817518873214722 and perplexity is 123.65789892121408
At time: 127.85845065116882 and batch: 450, loss is 4.846854791641236 and perplexity is 127.33925086765062
At time: 128.18432450294495 and batch: 500, loss is 4.826584358215332 and perplexity is 124.78401442842403
At time: 128.50898671150208 and batch: 550, loss is 4.87913034439087 and perplexity is 131.51624030241547
At time: 128.83341336250305 and batch: 600, loss is 4.906110858917236 and perplexity is 135.11291806235494
At time: 129.15923428535461 and batch: 650, loss is 4.88782961845398 and perplexity is 132.6653269838926
At time: 129.48697209358215 and batch: 700, loss is 4.859610481262207 and perplexity is 128.9739545498465
At time: 129.81192922592163 and batch: 750, loss is 4.814555025100708 and perplexity is 123.29193828448022
At time: 130.13702487945557 and batch: 800, loss is 4.853534832000732 and perplexity is 128.19272966547695
At time: 130.47570824623108 and batch: 850, loss is 4.819019136428833 and perplexity is 123.84355755168346
At time: 130.80099248886108 and batch: 900, loss is 4.928138971328735 and perplexity is 138.12222358511355
At time: 131.12715244293213 and batch: 950, loss is 4.880512161254883 and perplexity is 131.69809727873857
At time: 131.45122146606445 and batch: 1000, loss is 4.837106447219849 and perplexity is 126.10393491718852
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.109420032036014 and perplexity of 165.57429923261049
Finished 18 epochs...
Completing Train Step...
At time: 132.56448602676392 and batch: 50, loss is 4.924067983627319 and perplexity is 137.56107270576368
At time: 132.90727162361145 and batch: 100, loss is 4.886302309036255 and perplexity is 132.46286063432316
At time: 133.2328164577484 and batch: 150, loss is 4.913060674667358 and perplexity is 136.05519849336903
At time: 133.56025099754333 and batch: 200, loss is 4.924335231781006 and perplexity is 137.59784056131542
At time: 133.88397121429443 and batch: 250, loss is 4.9152125549316406 and perplexity is 136.34828822376502
At time: 134.20882415771484 and batch: 300, loss is 4.7979019832611085 and perplexity is 121.25575386654562
At time: 134.53355860710144 and batch: 350, loss is 4.8650319957733155 and perplexity is 129.67508760014115
At time: 134.85941910743713 and batch: 400, loss is 4.802108211517334 and perplexity is 121.76685740019963
At time: 135.1859645843506 and batch: 450, loss is 4.831205272674561 and perplexity is 125.36196498661526
At time: 135.51089429855347 and batch: 500, loss is 4.807197961807251 and perplexity is 122.38820019644118
At time: 135.83613920211792 and batch: 550, loss is 4.86159158706665 and perplexity is 129.22971886425958
At time: 136.16201615333557 and batch: 600, loss is 4.8887451648712155 and perplexity is 132.7868438673896
At time: 136.48828053474426 and batch: 650, loss is 4.866427001953125 and perplexity is 129.85611138400256
At time: 136.8138086795807 and batch: 700, loss is 4.841463279724121 and perplexity is 126.65454723159051
At time: 137.13894844055176 and batch: 750, loss is 4.8002636528015135 and perplexity is 121.54245830436261
At time: 137.46417379379272 and batch: 800, loss is 4.838693246841431 and perplexity is 126.30419543803424
At time: 137.7913408279419 and batch: 850, loss is 4.801871862411499 and perplexity is 121.73808131306794
At time: 138.1188268661499 and batch: 900, loss is 4.915171403884887 and perplexity is 136.34267746442669
At time: 138.44389295578003 and batch: 950, loss is 4.866267795562744 and perplexity is 129.83543910686325
At time: 138.7837631702423 and batch: 1000, loss is 4.819472818374634 and perplexity is 123.89975588496915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.10029750917016 and perplexity of 164.0707125810799
Finished 19 epochs...
Completing Train Step...
At time: 139.92147135734558 and batch: 50, loss is 4.903533544540405 and perplexity is 134.76513795798093
At time: 140.2483332157135 and batch: 100, loss is 4.868033027648925 and perplexity is 130.06483119546937
At time: 140.5740623474121 and batch: 150, loss is 4.897748746871948 and perplexity is 133.9877994428765
At time: 140.89986896514893 and batch: 200, loss is 4.906838436126709 and perplexity is 135.21125891318513
At time: 141.22417783737183 and batch: 250, loss is 4.899151220321655 and perplexity is 134.17584560823406
At time: 141.54822039604187 and batch: 300, loss is 4.78270676612854 and perplexity is 119.42717437315576
At time: 141.87538766860962 and batch: 350, loss is 4.849787664413452 and perplexity is 127.7132688948985
At time: 142.20088243484497 and batch: 400, loss is 4.786794519424438 and perplexity is 119.91636235753417
At time: 142.5267460346222 and batch: 450, loss is 4.8184741020202635 and perplexity is 123.77607694283556
At time: 142.8517153263092 and batch: 500, loss is 4.79121768951416 and perplexity is 120.44794760408354
At time: 143.177592754364 and batch: 550, loss is 4.845998802185059 and perplexity is 127.23029645011947
At time: 143.50295305252075 and batch: 600, loss is 4.871427021026611 and perplexity is 130.50702034025946
At time: 143.82866978645325 and batch: 650, loss is 4.852026681900025 and perplexity is 127.99954150231336
At time: 144.15393471717834 and batch: 700, loss is 4.8283764839172365 and perplexity is 125.00784357294317
At time: 144.4796862602234 and batch: 750, loss is 4.784810934066773 and perplexity is 119.67873377322212
At time: 144.80485033988953 and batch: 800, loss is 4.824327354431152 and perplexity is 124.50269402568281
At time: 145.13127875328064 and batch: 850, loss is 4.790685386657715 and perplexity is 120.38384987873552
At time: 145.4571418762207 and batch: 900, loss is 4.900253353118896 and perplexity is 134.3238067297015
At time: 145.78242802619934 and batch: 950, loss is 4.851474466323853 and perplexity is 127.92887767438184
At time: 146.1075587272644 and batch: 1000, loss is 4.810027647018432 and perplexity is 122.7350107274628
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.098052234184451 and perplexity of 163.70274196642885
Finished 20 epochs...
Completing Train Step...
At time: 147.23470544815063 and batch: 50, loss is 4.893754587173462 and perplexity is 133.45369812728464
At time: 147.5747299194336 and batch: 100, loss is 4.8551170539855955 and perplexity is 128.39571956584732
At time: 147.90231108665466 and batch: 150, loss is 4.883384704589844 and perplexity is 132.07694964471457
At time: 148.22772455215454 and batch: 200, loss is 4.8943124294281 and perplexity is 133.5281650075929
At time: 148.55378770828247 and batch: 250, loss is 4.8874661731719975 and perplexity is 132.61711915770186
At time: 148.8801064491272 and batch: 300, loss is 4.773437585830688 and perplexity is 118.32529700066365
At time: 149.20718264579773 and batch: 350, loss is 4.837494974136352 and perplexity is 126.15293920930765
At time: 149.5328402519226 and batch: 400, loss is 4.77338737487793 and perplexity is 118.31935592392067
At time: 149.85923981666565 and batch: 450, loss is 4.803890171051026 and perplexity is 121.98403445552543
At time: 150.18420505523682 and batch: 500, loss is 4.781169986724853 and perplexity is 119.24378210416242
At time: 150.5101020336151 and batch: 550, loss is 4.836497449874878 and perplexity is 126.02716133546011
At time: 150.8359649181366 and batch: 600, loss is 4.864413003921509 and perplexity is 129.59484461497348
At time: 151.16219997406006 and batch: 650, loss is 4.842919216156006 and perplexity is 126.83908250438479
At time: 151.48778986930847 and batch: 700, loss is 4.816452074050903 and perplexity is 123.52605111811266
At time: 151.81262254714966 and batch: 750, loss is 4.774037170410156 and perplexity is 118.39626429742852
At time: 152.1386489868164 and batch: 800, loss is 4.815752601623535 and perplexity is 123.4396782625309
At time: 152.46447801589966 and batch: 850, loss is 4.778594245910645 and perplexity is 118.93703624625964
At time: 152.78931140899658 and batch: 900, loss is 4.892259654998779 and perplexity is 133.25434294848614
At time: 153.11511278152466 and batch: 950, loss is 4.840772857666016 and perplexity is 126.56713231848701
At time: 153.44227147102356 and batch: 1000, loss is 4.798499212265015 and perplexity is 121.3281929488526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.091152284203506 and perplexity of 162.57708917505425
Finished 21 epochs...
Completing Train Step...
At time: 154.55222964286804 and batch: 50, loss is 4.883087787628174 and perplexity is 132.03773957947425
At time: 154.8934760093689 and batch: 100, loss is 4.843771295547485 and perplexity is 126.9472055307804
At time: 155.2197070121765 and batch: 150, loss is 4.86993034362793 and perplexity is 130.31183953033482
At time: 155.5456578731537 and batch: 200, loss is 4.8866486167907714 and perplexity is 132.5087414941365
At time: 155.86997413635254 and batch: 250, loss is 4.878075838088989 and perplexity is 131.3776286944685
At time: 156.20879101753235 and batch: 300, loss is 4.7637270736694335 and perplexity is 117.18185843713299
At time: 156.53558683395386 and batch: 350, loss is 4.828730316162109 and perplexity is 125.0520832051039
At time: 156.8610496520996 and batch: 400, loss is 4.763266324996948 and perplexity is 117.1278794877383
At time: 157.18692803382874 and batch: 450, loss is 4.79643853187561 and perplexity is 121.0784317483472
At time: 157.5129988193512 and batch: 500, loss is 4.771004257202148 and perplexity is 118.03772269128739
At time: 157.83932518959045 and batch: 550, loss is 4.82755033493042 and perplexity is 124.90461111819674
At time: 158.16786551475525 and batch: 600, loss is 4.852994413375854 and perplexity is 128.12347064291026
At time: 158.49335074424744 and batch: 650, loss is 4.8335105895996096 and perplexity is 125.65129741959586
At time: 158.81959629058838 and batch: 700, loss is 4.806716976165771 and perplexity is 122.32934738427353
At time: 159.1454451084137 and batch: 750, loss is 4.765689935684204 and perplexity is 117.41209614438225
At time: 159.47250509262085 and batch: 800, loss is 4.803187704086303 and perplexity is 121.89837479116154
At time: 159.79970026016235 and batch: 850, loss is 4.767331523895264 and perplexity is 117.60499674560586
At time: 160.12463116645813 and batch: 900, loss is 4.88069522857666 and perplexity is 131.7222091036668
At time: 160.45150208473206 and batch: 950, loss is 4.830560026168823 and perplexity is 125.28110170793477
At time: 160.78020310401917 and batch: 1000, loss is 4.788347024917602 and perplexity is 120.1026777592342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.086801668492759 and perplexity of 161.87131512574751
Finished 22 epochs...
Completing Train Step...
At time: 161.9057400226593 and batch: 50, loss is 4.870180568695068 and perplexity is 130.34445089905105
At time: 162.23317885398865 and batch: 100, loss is 4.834155035018921 and perplexity is 125.73229892036814
At time: 162.5606551170349 and batch: 150, loss is 4.860281820297241 and perplexity is 129.06056887057275
At time: 162.8860981464386 and batch: 200, loss is 4.873584251403809 and perplexity is 130.78885793398675
At time: 163.21218752861023 and batch: 250, loss is 4.868453369140625 and perplexity is 130.11951433262232
At time: 163.53893518447876 and batch: 300, loss is 4.754931259155273 and perplexity is 116.15566825120206
At time: 163.86612176895142 and batch: 350, loss is 4.818194971084595 and perplexity is 123.74153203216605
At time: 164.1922471523285 and batch: 400, loss is 4.755097579956055 and perplexity is 116.1749889616344
At time: 164.53384613990784 and batch: 450, loss is 4.787606058120727 and perplexity is 120.01371862475142
At time: 164.86072778701782 and batch: 500, loss is 4.761530704498291 and perplexity is 116.92476625386911
At time: 165.1896047592163 and batch: 550, loss is 4.817814540863037 and perplexity is 123.6944659669636
At time: 165.51650643348694 and batch: 600, loss is 4.842915344238281 and perplexity is 126.83859139484377
At time: 165.84299898147583 and batch: 650, loss is 4.824845504760742 and perplexity is 124.56722185374107
At time: 166.16917324066162 and batch: 700, loss is 4.795594100952148 and perplexity is 120.97623253257656
At time: 166.49738812446594 and batch: 750, loss is 4.755916624069214 and perplexity is 116.27018038010375
At time: 166.8242793083191 and batch: 800, loss is 4.793778047561646 and perplexity is 120.75673260743399
At time: 167.15063762664795 and batch: 850, loss is 4.759765396118164 and perplexity is 116.71854006412265
At time: 167.4757800102234 and batch: 900, loss is 4.869893465042114 and perplexity is 130.30703390259086
At time: 167.80287098884583 and batch: 950, loss is 4.822032585144043 and perplexity is 124.21731663014383
At time: 168.1294777393341 and batch: 1000, loss is 4.7804554748535155 and perplexity is 119.15861143761546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0846338039491235 and perplexity of 161.52078013452632
Finished 23 epochs...
Completing Train Step...
At time: 169.23929142951965 and batch: 50, loss is 4.8610123443603515 and perplexity is 129.15488516769923
At time: 169.579354763031 and batch: 100, loss is 4.8267624473571775 and perplexity is 124.80623908539599
At time: 169.90510272979736 and batch: 150, loss is 4.853622055053711 and perplexity is 128.20391151437843
At time: 170.23160004615784 and batch: 200, loss is 4.866302833557129 and perplexity is 129.83998835994754
At time: 170.55756759643555 and batch: 250, loss is 4.862497215270996 and perplexity is 129.34680595318522
At time: 170.88409686088562 and batch: 300, loss is 4.747383899688721 and perplexity is 115.28229962808295
At time: 171.21022582054138 and batch: 350, loss is 4.8079008197784425 and perplexity is 122.47425195606908
At time: 171.53721523284912 and batch: 400, loss is 4.746371431350708 and perplexity is 115.16563901732917
At time: 171.8625829219818 and batch: 450, loss is 4.777689018249512 and perplexity is 118.82941986712667
At time: 172.1898844242096 and batch: 500, loss is 4.754512910842895 and perplexity is 116.10708488650893
At time: 172.51705598831177 and batch: 550, loss is 4.808754215240478 and perplexity is 122.57881553760161
At time: 172.8441298007965 and batch: 600, loss is 4.832221841812133 and perplexity is 125.48946888853156
At time: 173.18415880203247 and batch: 650, loss is 4.813166656494141 and perplexity is 123.12088239971025
At time: 173.5117690563202 and batch: 700, loss is 4.784155921936035 and perplexity is 119.6003684187401
At time: 173.83774733543396 and batch: 750, loss is 4.745214223861694 and perplexity is 115.03244555847893
At time: 174.16370797157288 and batch: 800, loss is 4.7844071960449215 and perplexity is 119.63042467075782
At time: 174.4884786605835 and batch: 850, loss is 4.749168968200683 and perplexity is 115.48827021224447
At time: 174.81437993049622 and batch: 900, loss is 4.8611362934112545 and perplexity is 129.17089478530323
At time: 175.13986563682556 and batch: 950, loss is 4.811195774078369 and perplexity is 122.87846458454916
At time: 175.4655840396881 and batch: 1000, loss is 4.77037356376648 and perplexity is 117.96330054567106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.082896162823933 and perplexity of 161.24035869086677
Finished 24 epochs...
Completing Train Step...
At time: 176.57818365097046 and batch: 50, loss is 4.848254299163818 and perplexity is 127.5175878700402
At time: 176.91792035102844 and batch: 100, loss is 4.813235225677491 and perplexity is 123.1293249875171
At time: 177.24367594718933 and batch: 150, loss is 4.840056982040405 and perplexity is 126.476558417149
At time: 177.56842303276062 and batch: 200, loss is 4.854567136764526 and perplexity is 128.32513195899566
At time: 177.8938271999359 and batch: 250, loss is 4.851770448684692 and perplexity is 127.96674796980868
At time: 178.21925401687622 and batch: 300, loss is 4.739059715270996 and perplexity is 114.32665151965394
At time: 178.54412961006165 and batch: 350, loss is 4.798272943496704 and perplexity is 121.30074327369185
At time: 178.86980366706848 and batch: 400, loss is 4.73728874206543 and perplexity is 114.12436126131516
At time: 179.19516158103943 and batch: 450, loss is 4.768297786712647 and perplexity is 117.71868900055301
At time: 179.52233862876892 and batch: 500, loss is 4.743388671875 and perplexity is 114.82263941323528
At time: 179.8479142189026 and batch: 550, loss is 4.798186435699463 and perplexity is 121.2902502674575
At time: 180.17342495918274 and batch: 600, loss is 4.824975318908692 and perplexity is 124.58339349113938
At time: 180.4987542629242 and batch: 650, loss is 4.805178928375244 and perplexity is 122.14134361826265
At time: 180.82584810256958 and batch: 700, loss is 4.776973266601562 and perplexity is 118.74439794495206
At time: 181.15149903297424 and batch: 750, loss is 4.736603918075562 and perplexity is 114.04623291603102
At time: 181.48956036567688 and batch: 800, loss is 4.776060781478882 and perplexity is 118.6360948684134
At time: 181.81260085105896 and batch: 850, loss is 4.741512069702148 and perplexity is 114.60736505396154
At time: 182.13907194137573 and batch: 900, loss is 4.853917074203491 and perplexity is 128.24173970309764
At time: 182.46342706680298 and batch: 950, loss is 4.802066078186035 and perplexity is 121.76172706493541
At time: 182.78797936439514 and batch: 1000, loss is 4.759177093505859 and perplexity is 116.6498944362795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0775325123856705 and perplexity of 160.37783697061505
Finished 25 epochs...
Completing Train Step...
At time: 183.92445921897888 and batch: 50, loss is 4.8414780712127685 and perplexity is 126.65642065474334
At time: 184.24990105628967 and batch: 100, loss is 4.802488470077515 and perplexity is 121.8131690947235
At time: 184.57349586486816 and batch: 150, loss is 4.832788619995117 and perplexity is 125.56061374146252
At time: 184.89921712875366 and batch: 200, loss is 4.847626657485962 and perplexity is 127.43757762874087
At time: 185.223486661911 and batch: 250, loss is 4.842039375305176 and perplexity is 126.72753337805077
At time: 185.548810005188 and batch: 300, loss is 4.729516334533692 and perplexity is 113.24077844727442
At time: 185.87401747703552 and batch: 350, loss is 4.790858459472656 and perplexity is 120.4046868536125
At time: 186.1997947692871 and batch: 400, loss is 4.725888710021973 and perplexity is 112.8307276285309
At time: 186.5252079963684 and batch: 450, loss is 4.760741949081421 and perplexity is 116.83257757306023
At time: 186.84928274154663 and batch: 500, loss is 4.735858449935913 and perplexity is 113.96124676411513
At time: 187.17960596084595 and batch: 550, loss is 4.7921097946166995 and perplexity is 120.55544777632812
At time: 187.50566744804382 and batch: 600, loss is 4.815189399719238 and perplexity is 123.37017637430274
At time: 187.83193397521973 and batch: 650, loss is 4.796781482696534 and perplexity is 121.11996281706138
At time: 188.16199469566345 and batch: 700, loss is 4.764827260971069 and perplexity is 117.31085137495246
At time: 188.49280977249146 and batch: 750, loss is 4.727244968414307 and perplexity is 112.98385906921801
At time: 188.8216061592102 and batch: 800, loss is 4.765664577484131 and perplexity is 117.40911882270713
At time: 189.15056157112122 and batch: 850, loss is 4.730149917602539 and perplexity is 113.31254862098426
At time: 189.47603225708008 and batch: 900, loss is 4.8450164794921875 and perplexity is 127.10537660853858
At time: 189.80204677581787 and batch: 950, loss is 4.790220804214478 and perplexity is 120.32793464525622
At time: 190.15147924423218 and batch: 1000, loss is 4.751233873367309 and perplexity is 115.72698891901453
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0741026343368905 and perplexity of 159.82870281800396
Finished 26 epochs...
Completing Train Step...
At time: 191.26612091064453 and batch: 50, loss is 4.831270360946656 and perplexity is 125.37012484585533
At time: 191.60442662239075 and batch: 100, loss is 4.79020203590393 and perplexity is 120.32567631440385
At time: 191.9294216632843 and batch: 150, loss is 4.82121392250061 and perplexity is 124.1156661677467
At time: 192.25531458854675 and batch: 200, loss is 4.838468227386475 and perplexity is 126.27577773420612
At time: 192.57906579971313 and batch: 250, loss is 4.830319375991821 and perplexity is 125.25095641601418
At time: 192.9035677909851 and batch: 300, loss is 4.715593976974487 and perplexity is 111.67512392987061
At time: 193.2302827835083 and batch: 350, loss is 4.780694808959961 and perplexity is 119.18713357043258
At time: 193.55718398094177 and batch: 400, loss is 4.714919376373291 and perplexity is 111.59981322930685
At time: 193.88328862190247 and batch: 450, loss is 4.749862461090088 and perplexity is 115.56838828389189
At time: 194.20792818069458 and batch: 500, loss is 4.72472373008728 and perplexity is 112.69935863080767
At time: 194.53284740447998 and batch: 550, loss is 4.780467100143433 and perplexity is 119.1599966990716
At time: 194.8582489490509 and batch: 600, loss is 4.807880620956421 and perplexity is 122.47177814543575
At time: 195.18386030197144 and batch: 650, loss is 4.78538423538208 and perplexity is 119.74736542013552
At time: 195.5092339515686 and batch: 700, loss is 4.754327754974366 and perplexity is 116.0855889684734
At time: 195.83368110656738 and batch: 750, loss is 4.715427055358886 and perplexity is 111.65648449346752
At time: 196.15888857841492 and batch: 800, loss is 4.751035165786743 and perplexity is 115.7039953736118
At time: 196.4843556880951 and batch: 850, loss is 4.7192825603485105 and perplexity is 112.08780757640675
At time: 196.80944085121155 and batch: 900, loss is 4.835049867630005 and perplexity is 125.84485863524962
At time: 197.1339282989502 and batch: 950, loss is 4.776497316360474 and perplexity is 118.68789496749177
At time: 197.4592728614807 and batch: 1000, loss is 4.736294183731079 and perplexity is 114.01091435080674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.072807870260099 and perplexity of 159.62189626689272
Finished 27 epochs...
Completing Train Step...
At time: 198.5503067970276 and batch: 50, loss is 4.82098819732666 and perplexity is 124.08765329913476
At time: 198.89030766487122 and batch: 100, loss is 4.7792985725402835 and perplexity is 119.02083627597015
At time: 199.21544647216797 and batch: 150, loss is 4.812526378631592 and perplexity is 123.04207605596318
At time: 199.5421109199524 and batch: 200, loss is 4.8279550266265865 and perplexity is 124.95516920665847
At time: 199.86719250679016 and batch: 250, loss is 4.821630544662476 and perplexity is 124.16738627803086
At time: 200.1929042339325 and batch: 300, loss is 4.706199331283569 and perplexity is 110.6308884992895
At time: 200.5177938938141 and batch: 350, loss is 4.769235610961914 and perplexity is 117.82914022551299
At time: 200.84509372711182 and batch: 400, loss is 4.70248969078064 and perplexity is 110.22124795378703
At time: 201.1714208126068 and batch: 450, loss is 4.738785190582275 and perplexity is 114.2952703388845
At time: 201.49905681610107 and batch: 500, loss is 4.71406286239624 and perplexity is 111.5042673534699
At time: 201.824524641037 and batch: 550, loss is 4.766123628616333 and perplexity is 117.46302798421765
At time: 202.1499388217926 and batch: 600, loss is 4.792823934555054 and perplexity is 120.64157198507974
At time: 202.47817707061768 and batch: 650, loss is 4.77474892616272 and perplexity is 118.48056351629242
At time: 202.80362820625305 and batch: 700, loss is 4.74251314163208 and perplexity is 114.72215271584747
At time: 203.12887835502625 and batch: 750, loss is 4.707571830749512 and perplexity is 110.78283358309241
At time: 203.45386362075806 and batch: 800, loss is 4.74184208869934 and perplexity is 114.64519390342365
At time: 203.77999806404114 and batch: 850, loss is 4.707747192382812 and perplexity is 110.80226234521116
At time: 204.1054825782776 and batch: 900, loss is 4.824852495193482 and perplexity is 124.56809263557052
At time: 204.4298415184021 and batch: 950, loss is 4.764945774078369 and perplexity is 117.32475507233787
At time: 204.75546431541443 and batch: 1000, loss is 4.722361068725586 and perplexity is 112.43340251651817
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.064840921541539 and perplexity of 158.3552491609842
Finished 28 epochs...
Completing Train Step...
At time: 205.8966886997223 and batch: 50, loss is 4.806513814926148 and perplexity is 122.30449732678635
At time: 206.22187900543213 and batch: 100, loss is 4.7650196075439455 and perplexity is 117.33341788540154
At time: 206.5483536720276 and batch: 150, loss is 4.8000171184539795 and perplexity is 121.5124976070291
At time: 206.87457919120789 and batch: 200, loss is 4.818224868774414 and perplexity is 123.74523167341356
At time: 207.21418046951294 and batch: 250, loss is 4.8098672676086425 and perplexity is 122.71532813726503
At time: 207.53924012184143 and batch: 300, loss is 4.694605264663696 and perplexity is 109.35563358821261
At time: 207.86522459983826 and batch: 350, loss is 4.759375829696655 and perplexity is 116.67307929571548
At time: 208.1907012462616 and batch: 400, loss is 4.693159132003784 and perplexity is 109.19760512753483
At time: 208.5165445804596 and batch: 450, loss is 4.728912410736084 and perplexity is 113.17241029296609
At time: 208.84106969833374 and batch: 500, loss is 4.70417857170105 and perplexity is 110.40755579807762
At time: 209.16709566116333 and batch: 550, loss is 4.7572817134857175 and perplexity is 116.42900795505219
At time: 209.49447774887085 and batch: 600, loss is 4.781024827957153 and perplexity is 119.2264740799324
At time: 209.81982254981995 and batch: 650, loss is 4.764455795288086 and perplexity is 117.26728251209482
At time: 210.14527416229248 and batch: 700, loss is 4.735306615829468 and perplexity is 113.898376409936
At time: 210.47055292129517 and batch: 750, loss is 4.694838771820068 and perplexity is 109.38117189281743
At time: 210.7969913482666 and batch: 800, loss is 4.731123552322388 and perplexity is 113.42292737809179
At time: 211.12388610839844 and batch: 850, loss is 4.697687873840332 and perplexity is 109.69325437694637
At time: 211.45225882530212 and batch: 900, loss is 4.81518627166748 and perplexity is 123.36979046660922
At time: 211.77769303321838 and batch: 950, loss is 4.757125320434571 and perplexity is 116.410800691038
At time: 212.10832238197327 and batch: 1000, loss is 4.713927593231201 and perplexity is 111.48918528441915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.060529755383003 and perplexity of 157.67402286476644
Finished 29 epochs...
Completing Train Step...
At time: 213.26187086105347 and batch: 50, loss is 4.796165904998779 and perplexity is 121.04542701284144
At time: 213.60656213760376 and batch: 100, loss is 4.757336101531982 and perplexity is 116.43534047352851
At time: 213.93269610404968 and batch: 150, loss is 4.793355855941773 and perplexity is 120.70576088755747
At time: 214.26069378852844 and batch: 200, loss is 4.808248863220215 and perplexity is 122.51688573502162
At time: 214.58560848236084 and batch: 250, loss is 4.801566848754883 and perplexity is 121.70095519801107
At time: 214.91276907920837 and batch: 300, loss is 4.686665840148926 and perplexity is 108.49085027358927
At time: 215.2394437789917 and batch: 350, loss is 4.749449558258057 and perplexity is 115.52067961927537
At time: 215.5653965473175 and batch: 400, loss is 4.685171613693237 and perplexity is 108.32886142905835
At time: 215.90609121322632 and batch: 450, loss is 4.720776052474975 and perplexity is 112.25533490372757
At time: 216.23293709754944 and batch: 500, loss is 4.696395902633667 and perplexity is 109.55162536073198
At time: 216.56022381782532 and batch: 550, loss is 4.747554712295532 and perplexity is 115.30199298009009
At time: 216.88698410987854 and batch: 600, loss is 4.773736228942871 and perplexity is 118.36063931271627
At time: 217.21337509155273 and batch: 650, loss is 4.7546985149383545 and perplexity is 116.12863683697894
At time: 217.5395541191101 and batch: 700, loss is 4.724850549697876 and perplexity is 112.7136520259058
At time: 217.86725163459778 and batch: 750, loss is 4.686272068023682 and perplexity is 108.4481380109086
At time: 218.19326782226562 and batch: 800, loss is 4.719365968704223 and perplexity is 112.09715702603785
At time: 218.51872158050537 and batch: 850, loss is 4.689066915512085 and perplexity is 108.75165796547124
At time: 218.84374523162842 and batch: 900, loss is 4.808449172973633 and perplexity is 122.54142952029046
At time: 219.17008566856384 and batch: 950, loss is 4.750626144409179 and perplexity is 115.65667964326668
At time: 219.49650168418884 and batch: 1000, loss is 4.708682432174682 and perplexity is 110.90593750299529
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.059857996498666 and perplexity of 157.5681395071027
Finished 30 epochs...
Completing Train Step...
At time: 220.6111240386963 and batch: 50, loss is 4.78858193397522 and perplexity is 120.13089428011237
At time: 220.95149326324463 and batch: 100, loss is 4.749696121215821 and perplexity is 115.549166251455
At time: 221.2771053314209 and batch: 150, loss is 4.787474956512451 and perplexity is 119.99798566455524
At time: 221.60397124290466 and batch: 200, loss is 4.801618509292602 and perplexity is 121.70724249719882
At time: 221.9304554462433 and batch: 250, loss is 4.791634244918823 and perplexity is 120.49813129905763
At time: 222.25789403915405 and batch: 300, loss is 4.67812668800354 and perplexity is 107.5683745822094
At time: 222.58486771583557 and batch: 350, loss is 4.741026830673218 and perplexity is 114.55176657780072
At time: 222.91070008277893 and batch: 400, loss is 4.67696174621582 and perplexity is 107.44313664923548
At time: 223.2367823123932 and batch: 450, loss is 4.714235038757324 and perplexity is 111.52346740531826
At time: 223.564462184906 and batch: 500, loss is 4.687848415374756 and perplexity is 108.61922475663285
At time: 223.8891441822052 and batch: 550, loss is 4.73960578918457 and perplexity is 114.38909937069447
At time: 224.2294430732727 and batch: 600, loss is 4.765251560211182 and perplexity is 117.36063684126866
At time: 224.55536699295044 and batch: 650, loss is 4.74619119644165 and perplexity is 115.14488401929808
At time: 224.88085913658142 and batch: 700, loss is 4.719118185043335 and perplexity is 112.06938462301073
At time: 225.20669794082642 and batch: 750, loss is 4.679037036895752 and perplexity is 107.6663439192202
At time: 225.53262734413147 and batch: 800, loss is 4.7139065551757815 and perplexity is 111.48683979343284
At time: 225.85828280448914 and batch: 850, loss is 4.679045515060425 and perplexity is 107.66725673608313
At time: 226.18411421775818 and batch: 900, loss is 4.7983651447296145 and perplexity is 121.31192786738342
At time: 226.51000475883484 and batch: 950, loss is 4.741884307861328 and perplexity is 114.65003422961271
At time: 226.83685874938965 and batch: 1000, loss is 4.699488286972046 and perplexity is 109.8909254439838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.057477625404916 and perplexity of 157.19351491219282
Finished 31 epochs...
Completing Train Step...
At time: 227.9631519317627 and batch: 50, loss is 4.783294763565063 and perplexity is 119.49741789501992
At time: 228.28800916671753 and batch: 100, loss is 4.741737051010132 and perplexity is 114.63315246959041
At time: 228.61293077468872 and batch: 150, loss is 4.779912023544312 and perplexity is 119.09387212715164
At time: 228.93999409675598 and batch: 200, loss is 4.795003385543823 and perplexity is 120.90479111083015
At time: 229.26538062095642 and batch: 250, loss is 4.7860098075866695 and perplexity is 119.82229947940382
At time: 229.58959555625916 and batch: 300, loss is 4.6721796131134035 and perplexity is 106.93055586045833
At time: 229.91645121574402 and batch: 350, loss is 4.736890525817871 and perplexity is 114.07892413392022
At time: 230.24086117744446 and batch: 400, loss is 4.668298645019531 and perplexity is 106.51636603352956
At time: 230.56982040405273 and batch: 450, loss is 4.706626443862915 and perplexity is 110.6781504357971
At time: 230.89508056640625 and batch: 500, loss is 4.681623001098632 and perplexity is 107.9451255347334
At time: 231.22072076797485 and batch: 550, loss is 4.735479040145874 and perplexity is 113.91801695283318
At time: 231.5459258556366 and batch: 600, loss is 4.7619233894348145 and perplexity is 116.97068986444843
At time: 231.8725028038025 and batch: 650, loss is 4.738215847015381 and perplexity is 114.23021558300128
At time: 232.1981303691864 and batch: 700, loss is 4.711448249816894 and perplexity is 111.2131076941195
At time: 232.53751134872437 and batch: 750, loss is 4.673342704772949 and perplexity is 107.05499825305606
At time: 232.8632128238678 and batch: 800, loss is 4.706502447128296 and perplexity is 110.66442755736296
At time: 233.18807005882263 and batch: 850, loss is 4.674087429046631 and perplexity is 107.13475440335849
At time: 233.76895594596863 and batch: 900, loss is 4.793893175125122 and perplexity is 120.7706358361395
At time: 234.09462356567383 and batch: 950, loss is 4.735434846878052 and perplexity is 113.9129826546421
At time: 234.4209816455841 and batch: 1000, loss is 4.695988655090332 and perplexity is 109.50701981380163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.055777200838414 and perplexity of 156.92644632708408
Finished 32 epochs...
Completing Train Step...
At time: 235.55311179161072 and batch: 50, loss is 4.777310972213745 and perplexity is 118.78450536642283
At time: 235.87906169891357 and batch: 100, loss is 4.733709173202515 and perplexity is 113.71657553521177
At time: 236.2070779800415 and batch: 150, loss is 4.773007640838623 and perplexity is 118.27443456659147
At time: 236.53456592559814 and batch: 200, loss is 4.788131799697876 and perplexity is 120.07683141551603
At time: 236.87539267539978 and batch: 250, loss is 4.780369501113892 and perplexity is 119.14836736654868
At time: 237.20144152641296 and batch: 300, loss is 4.66755786895752 and perplexity is 106.43749047752826
At time: 237.5297076702118 and batch: 350, loss is 4.728311424255371 and perplexity is 113.10441563837101
At time: 237.85763096809387 and batch: 400, loss is 4.662167892456055 and perplexity is 105.86533823372679
At time: 238.1847574710846 and batch: 450, loss is 4.698422565460205 and perplexity is 109.77387470360651
At time: 238.5103542804718 and batch: 500, loss is 4.675281076431275 and perplexity is 107.26271187561477
At time: 238.83559560775757 and batch: 550, loss is 4.729471015930176 and perplexity is 113.23564664961795
At time: 239.16252064704895 and batch: 600, loss is 4.753080615997314 and perplexity is 115.94090434546622
At time: 239.48716497421265 and batch: 650, loss is 4.731034135818481 and perplexity is 113.41278594987503
At time: 239.81316781044006 and batch: 700, loss is 4.7066183757781985 and perplexity is 110.67725747870531
At time: 240.13876795768738 and batch: 750, loss is 4.667743701934814 and perplexity is 106.45727191124386
At time: 240.46686100959778 and batch: 800, loss is 4.699347763061524 and perplexity is 109.87548422636507
At time: 240.79279017448425 and batch: 850, loss is 4.668616056442261 and perplexity is 106.55018091114655
At time: 241.11834454536438 and batch: 900, loss is 4.789593114852905 and perplexity is 120.2524297800587
At time: 241.4430124759674 and batch: 950, loss is 4.729763765335083 and perplexity is 113.26880117053523
At time: 241.76857376098633 and batch: 1000, loss is 4.688880939483642 and perplexity is 108.73143464462096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.055154567811547 and perplexity of 156.82876915048553
Finished 33 epochs...
Completing Train Step...
At time: 242.89639735221863 and batch: 50, loss is 4.770722255706787 and perplexity is 118.00444057000404
At time: 243.22408413887024 and batch: 100, loss is 4.7298069667816165 and perplexity is 113.27369465229495
At time: 243.55098009109497 and batch: 150, loss is 4.766857595443725 and perplexity is 117.54927359704081
At time: 243.87812423706055 and batch: 200, loss is 4.782004528045654 and perplexity is 119.34333750334606
At time: 244.20295405387878 and batch: 250, loss is 4.772565793991089 and perplexity is 118.22218692411246
At time: 244.52981853485107 and batch: 300, loss is 4.659291477203369 and perplexity is 105.5612630929408
At time: 244.85646295547485 and batch: 350, loss is 4.7206160259246825 and perplexity is 112.23737250699949
At time: 245.18174147605896 and batch: 400, loss is 4.653403434753418 and perplexity is 104.94154016267188
At time: 245.50804448127747 and batch: 450, loss is 4.692741079330444 and perplexity is 109.15196431758494
At time: 245.8328857421875 and batch: 500, loss is 4.670019798278808 and perplexity is 106.69985488501511
At time: 246.16110157966614 and batch: 550, loss is 4.724147434234619 and perplexity is 112.63442916891798
At time: 246.4885551929474 and batch: 600, loss is 4.747303276062012 and perplexity is 115.27300552565873
At time: 246.81478905677795 and batch: 650, loss is 4.725634536743164 and perplexity is 112.80205271689167
At time: 247.1415092945099 and batch: 700, loss is 4.698297996520996 and perplexity is 109.76020114015014
At time: 247.46987175941467 and batch: 750, loss is 4.657756814956665 and perplexity is 105.39938645251284
At time: 247.8100779056549 and batch: 800, loss is 4.692702379226684 and perplexity is 109.14774020697755
At time: 248.13602113723755 and batch: 850, loss is 4.662731370925903 and perplexity is 105.92500788222702
At time: 248.46320915222168 and batch: 900, loss is 4.781388368606567 and perplexity is 119.26982562929454
At time: 248.78948378562927 and batch: 950, loss is 4.72519193649292 and perplexity is 112.7521375471793
At time: 249.11547303199768 and batch: 1000, loss is 4.684876461029052 and perplexity is 108.2968925950748
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.056195142792492 and perplexity of 156.99204618021903
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 250.24228763580322 and batch: 50, loss is 4.759015474319458 and perplexity is 116.63104309865689
At time: 250.56944346427917 and batch: 100, loss is 4.689420480728149 and perplexity is 108.79011556715159
At time: 250.8945450782776 and batch: 150, loss is 4.720882606506348 and perplexity is 112.26729679948743
At time: 251.22073245048523 and batch: 200, loss is 4.726214094161987 and perplexity is 112.86744693142289
At time: 251.54658818244934 and batch: 250, loss is 4.704365873336792 and perplexity is 110.42823725065143
At time: 251.87521028518677 and batch: 300, loss is 4.590213747024536 and perplexity is 98.51548530348356
At time: 252.2014558315277 and batch: 350, loss is 4.656114006042481 and perplexity is 105.22637755009269
At time: 252.5259199142456 and batch: 400, loss is 4.581923294067383 and perplexity is 97.70212353439912
At time: 252.85189580917358 and batch: 450, loss is 4.619169225692749 and perplexity is 101.40974851049333
At time: 253.17970085144043 and batch: 500, loss is 4.583262491226196 and perplexity is 97.83305359165874
At time: 253.50647068023682 and batch: 550, loss is 4.63458023071289 and perplexity is 102.98467911535184
At time: 253.83207631111145 and batch: 600, loss is 4.657311191558838 and perplexity is 105.35242848335498
At time: 254.15883493423462 and batch: 650, loss is 4.63146481513977 and perplexity is 102.66433829872484
At time: 254.48646354675293 and batch: 700, loss is 4.598503217697144 and perplexity is 99.33552066349394
At time: 254.8155860900879 and batch: 750, loss is 4.555227642059326 and perplexity is 95.1284079976418
At time: 255.1422474384308 and batch: 800, loss is 4.57746997833252 and perplexity is 97.26799250907129
At time: 255.47033643722534 and batch: 850, loss is 4.541494941711425 and perplexity is 93.8309671492147
At time: 255.798109292984 and batch: 900, loss is 4.6584806156158445 and perplexity is 105.47570221327153
At time: 256.1397566795349 and batch: 950, loss is 4.586258888244629 and perplexity is 98.1266398926423
At time: 256.46526193618774 and batch: 1000, loss is 4.557327337265015 and perplexity is 95.32835850393852
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.97068637754859 and perplexity of 144.12577812711933
Finished 35 epochs...
Completing Train Step...
At time: 257.5866401195526 and batch: 50, loss is 4.699075803756714 and perplexity is 109.84560662898872
At time: 257.9287760257721 and batch: 100, loss is 4.644841556549072 and perplexity is 104.04687893277651
At time: 258.25479197502136 and batch: 150, loss is 4.686746101379395 and perplexity is 108.4995582321779
At time: 258.5825536251068 and batch: 200, loss is 4.693882293701172 and perplexity is 109.2766012130385
At time: 258.9113075733185 and batch: 250, loss is 4.678471069335938 and perplexity is 107.60542550182824
At time: 259.2379894256592 and batch: 300, loss is 4.569388027191162 and perplexity is 96.48504547752492
At time: 259.56496715545654 and batch: 350, loss is 4.634823598861694 and perplexity is 103.00974535610192
At time: 259.8914680480957 and batch: 400, loss is 4.563810415267945 and perplexity is 95.94838736407789
At time: 260.2193338871002 and batch: 450, loss is 4.6016099166870115 and perplexity is 99.6446060942982
At time: 260.54752349853516 and batch: 500, loss is 4.56736120223999 and perplexity is 96.28968522732951
At time: 260.8739130496979 and batch: 550, loss is 4.618995056152344 and perplexity is 101.39208755924719
At time: 261.20058608055115 and batch: 600, loss is 4.644444818496704 and perplexity is 104.00560776413664
At time: 261.5271141529083 and batch: 650, loss is 4.6206928157806395 and perplexity is 101.56437316052023
At time: 261.854444026947 and batch: 700, loss is 4.589920377731323 and perplexity is 98.48658812416933
At time: 262.18070244789124 and batch: 750, loss is 4.54846794128418 and perplexity is 94.48753691305001
At time: 262.5074985027313 and batch: 800, loss is 4.572516384124756 and perplexity is 96.78735776234767
At time: 262.83384227752686 and batch: 850, loss is 4.540177068710327 and perplexity is 93.70739129746654
At time: 263.1608040332794 and batch: 900, loss is 4.660993280410767 and perplexity is 105.74106053558853
At time: 263.486323595047 and batch: 950, loss is 4.593648443222046 and perplexity is 98.85443783251014
At time: 263.8125488758087 and batch: 1000, loss is 4.564410924911499 and perplexity is 96.00602259949885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.967166435427782 and perplexity of 143.61935554197044
Finished 36 epochs...
Completing Train Step...
At time: 264.958119392395 and batch: 50, loss is 4.686707649230957 and perplexity is 108.49538627127028
At time: 265.2877321243286 and batch: 100, loss is 4.633371343612671 and perplexity is 102.86025748625285
At time: 265.6159756183624 and batch: 150, loss is 4.675802192687988 and perplexity is 107.31862278528905
At time: 265.94644236564636 and batch: 200, loss is 4.6829809474945066 and perplexity is 108.09180880037088
At time: 266.27519488334656 and batch: 250, loss is 4.668390769958496 and perplexity is 106.52617929926544
At time: 266.60162568092346 and batch: 300, loss is 4.561330690383911 and perplexity is 95.71075651168391
At time: 266.9303901195526 and batch: 350, loss is 4.626970500946045 and perplexity is 102.20396780565314
At time: 267.25837540626526 and batch: 400, loss is 4.556586103439331 and perplexity is 95.25772408161407
At time: 267.58654165267944 and batch: 450, loss is 4.594492197036743 and perplexity is 98.93788183968033
At time: 267.9137718677521 and batch: 500, loss is 4.560873403549194 and perplexity is 95.66699924836307
At time: 268.2413983345032 and batch: 550, loss is 4.612733039855957 and perplexity is 100.7591524478818
At time: 268.56978058815 and batch: 600, loss is 4.639710721969604 and perplexity is 103.5143988103234
At time: 268.8969569206238 and batch: 650, loss is 4.616483736038208 and perplexity is 101.13777902906347
At time: 269.223375082016 and batch: 700, loss is 4.586513814926147 and perplexity is 98.15165818009767
At time: 269.55009961128235 and batch: 750, loss is 4.545844917297363 and perplexity is 94.24001860242056
At time: 269.8768994808197 and batch: 800, loss is 4.57053840637207 and perplexity is 96.59610373241576
At time: 270.2072126865387 and batch: 850, loss is 4.539323902130127 and perplexity is 93.62747737768399
At time: 270.5351617336273 and batch: 900, loss is 4.661837997436524 and perplexity is 105.83041954596978
At time: 270.8620762825012 and batch: 950, loss is 4.596199941635132 and perplexity is 99.10698682593346
At time: 271.1894040107727 and batch: 1000, loss is 4.566792716979981 and perplexity is 96.234961516872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.965144831959794 and perplexity of 143.32930743464914
Finished 37 epochs...
Completing Train Step...
At time: 272.3098351955414 and batch: 50, loss is 4.678840579986573 and perplexity is 107.64519419964344
At time: 272.65157985687256 and batch: 100, loss is 4.626827421188355 and perplexity is 102.18934553280515
At time: 272.979159116745 and batch: 150, loss is 4.669200372695923 and perplexity is 106.61245810670047
At time: 273.3205029964447 and batch: 200, loss is 4.675911073684692 and perplexity is 107.33030838006087
At time: 273.6466431617737 and batch: 250, loss is 4.661614570617676 and perplexity is 105.80677683329966
At time: 273.97343468666077 and batch: 300, loss is 4.555830755233765 and perplexity is 95.18579849851251
At time: 274.3008921146393 and batch: 350, loss is 4.622080450057983 and perplexity is 101.70540519388646
At time: 274.6273412704468 and batch: 400, loss is 4.551665153503418 and perplexity is 94.79011706936863
At time: 274.9540801048279 and batch: 450, loss is 4.589933280944824 and perplexity is 98.4878589258416
At time: 275.2814598083496 and batch: 500, loss is 4.556178245544434 and perplexity is 95.2188803886893
At time: 275.6084554195404 and batch: 550, loss is 4.608075828552246 and perplexity is 100.2909868035088
At time: 275.9366297721863 and batch: 600, loss is 4.635646276473999 and perplexity is 103.09452403543479
At time: 276.262677192688 and batch: 650, loss is 4.612384109497071 and perplexity is 100.72400065377776
At time: 276.5898857116699 and batch: 700, loss is 4.582634162902832 and perplexity is 97.77160162115447
At time: 276.91925835609436 and batch: 750, loss is 4.542511768341065 and perplexity is 93.92642549936824
At time: 277.2470178604126 and batch: 800, loss is 4.567746591567993 and perplexity is 96.32680139604119
At time: 277.57302236557007 and batch: 850, loss is 4.537703409194946 and perplexity is 93.47587757843156
At time: 277.89941120147705 and batch: 900, loss is 4.661624641418457 and perplexity is 105.80784239763595
At time: 278.22523260116577 and batch: 950, loss is 4.5976340770721436 and perplexity is 99.2492216354079
At time: 278.5523283481598 and batch: 1000, loss is 4.567103881835937 and perplexity is 96.26491111419979
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.963582108660442 and perplexity of 143.10549830785666
Finished 38 epochs...
Completing Train Step...
At time: 279.66555070877075 and batch: 50, loss is 4.67313886642456 and perplexity is 107.03317856294574
At time: 280.0071527957916 and batch: 100, loss is 4.621780433654785 and perplexity is 101.67489648082041
At time: 280.33342814445496 and batch: 150, loss is 4.664210195541382 and perplexity is 106.08176827327196
At time: 280.6595904827118 and batch: 200, loss is 4.67047345161438 and perplexity is 106.74827061123624
At time: 280.9862084388733 and batch: 250, loss is 4.656266784667968 and perplexity is 105.24245511954305
At time: 281.31271529197693 and batch: 300, loss is 4.551510648727417 and perplexity is 94.77547267490732
At time: 281.6390941143036 and batch: 350, loss is 4.618282709121704 and perplexity is 101.31988692575054
At time: 281.98012495040894 and batch: 400, loss is 4.547999076843261 and perplexity is 94.44324545103868
At time: 282.3045902252197 and batch: 450, loss is 4.58612998008728 and perplexity is 98.11399138357245
At time: 282.6308271884918 and batch: 500, loss is 4.552398014068603 and perplexity is 94.85961046951402
At time: 282.9581928253174 and batch: 550, loss is 4.60533899307251 and perplexity is 100.01688213331364
At time: 283.2851107120514 and batch: 600, loss is 4.633266220092773 and perplexity is 102.84944502226038
At time: 283.6109986305237 and batch: 650, loss is 4.610267086029053 and perplexity is 100.51099113323332
At time: 283.9376244544983 and batch: 700, loss is 4.580866737365723 and perplexity is 97.59895021484134
At time: 284.2643303871155 and batch: 750, loss is 4.5405687236785885 and perplexity is 93.7440994508271
At time: 284.59028840065 and batch: 800, loss is 4.565983982086181 and perplexity is 96.15716440834831
At time: 284.9154872894287 and batch: 850, loss is 4.536231298446655 and perplexity is 93.33837197091627
At time: 285.2413260936737 and batch: 900, loss is 4.661002979278565 and perplexity is 105.7420861091289
At time: 285.56935572624207 and batch: 950, loss is 4.5977169799804685 and perplexity is 99.2574500256045
At time: 285.89637637138367 and batch: 1000, loss is 4.566599388122558 and perplexity is 96.21635832004318
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.962527391387195 and perplexity of 142.95464203623723
Finished 39 epochs...
Completing Train Step...
At time: 287.0218343734741 and batch: 50, loss is 4.668619184494019 and perplexity is 106.55051420614855
At time: 287.35041999816895 and batch: 100, loss is 4.61788390159607 and perplexity is 101.27948784861155
At time: 287.67517590522766 and batch: 150, loss is 4.660109815597534 and perplexity is 105.64768328312331
At time: 288.0014271736145 and batch: 200, loss is 4.665939598083496 and perplexity is 106.26538508091878
At time: 288.32832384109497 and batch: 250, loss is 4.651883611679077 and perplexity is 104.78216872734532
At time: 288.65525913238525 and batch: 300, loss is 4.5479245948791505 and perplexity is 94.4362113945789
At time: 288.98152256011963 and batch: 350, loss is 4.615266618728637 and perplexity is 101.01475736863861
At time: 289.3079409599304 and batch: 400, loss is 4.5448810005187985 and perplexity is 94.14922283408983
At time: 289.63432717323303 and batch: 450, loss is 4.583195533752441 and perplexity is 97.82650315684326
At time: 289.96222257614136 and batch: 500, loss is 4.5496300601959225 and perplexity is 94.59740649501506
At time: 290.3031268119812 and batch: 550, loss is 4.603195734024048 and perplexity is 99.80274959839818
At time: 290.6296052932739 and batch: 600, loss is 4.6313161945343015 and perplexity is 102.64908139637997
At time: 290.9554841518402 and batch: 650, loss is 4.6082862186431885 and perplexity is 100.31208925313844
At time: 291.2834417819977 and batch: 700, loss is 4.57908332824707 and perplexity is 97.4250464739084
At time: 291.61001324653625 and batch: 750, loss is 4.538775157928467 and perplexity is 93.57611393634116
At time: 291.9361035823822 and batch: 800, loss is 4.564241533279419 and perplexity is 95.98976135993904
At time: 292.2613642215729 and batch: 850, loss is 4.534859628677368 and perplexity is 93.21043031468031
At time: 292.58748507499695 and batch: 900, loss is 4.660093307495117 and perplexity is 105.64593925474294
At time: 292.91472244262695 and batch: 950, loss is 4.59720850944519 and perplexity is 99.20699336580913
At time: 293.2409954071045 and batch: 1000, loss is 4.565793151855469 and perplexity is 96.13881646520639
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.962013058546113 and perplexity of 142.8811346742984
Finished 40 epochs...
Completing Train Step...
At time: 294.3626010417938 and batch: 50, loss is 4.664999485015869 and perplexity is 106.1655305483976
At time: 294.70288467407227 and batch: 100, loss is 4.614613876342774 and perplexity is 100.94884227003614
At time: 295.0295317173004 and batch: 150, loss is 4.656636152267456 and perplexity is 105.28133545268037
At time: 295.35636377334595 and batch: 200, loss is 4.662162246704102 and perplexity is 105.86474054597389
At time: 295.6839246749878 and batch: 250, loss is 4.648218116760254 and perplexity is 104.39879327991717
At time: 296.0112977027893 and batch: 300, loss is 4.544861507415772 and perplexity is 94.14738759147657
At time: 296.33572220802307 and batch: 350, loss is 4.612728996276855 and perplexity is 100.75874502110238
At time: 296.6606776714325 and batch: 400, loss is 4.542405605316162 and perplexity is 93.91645451520321
At time: 296.98944783210754 and batch: 450, loss is 4.580725917816162 and perplexity is 97.58520734228976
At time: 297.3171362876892 and batch: 500, loss is 4.547204179763794 and perplexity is 94.36820262066895
At time: 297.64363288879395 and batch: 550, loss is 4.601305179595947 and perplexity is 99.61424531315977
At time: 297.97183537483215 and batch: 600, loss is 4.62950400352478 and perplexity is 102.46323010382044
At time: 298.30261158943176 and batch: 650, loss is 4.606510620117188 and perplexity is 100.13413329124656
At time: 298.6293771266937 and batch: 700, loss is 4.577363300323486 and perplexity is 97.25761670673255
At time: 298.971777677536 and batch: 750, loss is 4.536940793991089 and perplexity is 93.40461862802947
At time: 299.2977616786957 and batch: 800, loss is 4.562434406280517 and perplexity is 95.81645231346063
At time: 299.6246235370636 and batch: 850, loss is 4.5335078620910645 and perplexity is 93.0845166915734
At time: 299.95232939720154 and batch: 900, loss is 4.658962659835815 and perplexity is 105.52655842235683
At time: 300.2795989513397 and batch: 950, loss is 4.59644380569458 and perplexity is 99.13115840523045
At time: 300.60505843162537 and batch: 1000, loss is 4.564805059432984 and perplexity is 96.04386934514031
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.961301105778392 and perplexity of 142.779446257982
Finished 41 epochs...
Completing Train Step...
At time: 301.72250270843506 and batch: 50, loss is 4.661681928634644 and perplexity is 105.81390400800238
At time: 302.0631322860718 and batch: 100, loss is 4.611608781814575 and perplexity is 100.64593681421769
At time: 302.38862109184265 and batch: 150, loss is 4.653417596817016 and perplexity is 104.94302636196156
At time: 302.7168197631836 and batch: 200, loss is 4.658646965026856 and perplexity is 105.49324949366013
At time: 303.042902469635 and batch: 250, loss is 4.644906301498413 and perplexity is 104.05361566076425
At time: 303.36929416656494 and batch: 300, loss is 4.542193183898926 and perplexity is 93.89650676757269
At time: 303.69538140296936 and batch: 350, loss is 4.610595102310181 and perplexity is 100.5439657825726
At time: 304.0225670337677 and batch: 400, loss is 4.540154867172241 and perplexity is 93.7053108723441
At time: 304.3492341041565 and batch: 450, loss is 4.578364009857178 and perplexity is 97.35499204508369
At time: 304.6763756275177 and batch: 500, loss is 4.544995355606079 and perplexity is 94.15998989230661
At time: 305.0018389225006 and batch: 550, loss is 4.59955228805542 and perplexity is 99.43978529465052
At time: 305.3288161754608 and batch: 600, loss is 4.627789278030395 and perplexity is 102.28768434033525
At time: 305.65659046173096 and batch: 650, loss is 4.604615936279297 and perplexity is 99.94459038592022
At time: 305.9823625087738 and batch: 700, loss is 4.575542545318603 and perplexity is 97.080695528288
At time: 306.3074493408203 and batch: 750, loss is 4.535088043212891 and perplexity is 93.2317233635549
At time: 306.6336612701416 and batch: 800, loss is 4.560539102554321 and perplexity is 95.63502302047802
At time: 306.9617645740509 and batch: 850, loss is 4.5319865036010745 and perplexity is 92.94300944074037
At time: 307.3014509677887 and batch: 900, loss is 4.657783899307251 and perplexity is 105.40224116510595
At time: 307.62637281417847 and batch: 950, loss is 4.595527811050415 and perplexity is 99.0403963701765
At time: 307.952449798584 and batch: 1000, loss is 4.563745756149292 and perplexity is 95.94218362648101
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.960384834103468 and perplexity of 142.64868141282503
Finished 42 epochs...
Completing Train Step...
At time: 309.08041524887085 and batch: 50, loss is 4.658544073104858 and perplexity is 105.482395648858
At time: 309.408323764801 and batch: 100, loss is 4.608704280853272 and perplexity is 100.35403471416481
At time: 309.73657631874084 and batch: 150, loss is 4.650361661911011 and perplexity is 104.62281682350911
At time: 310.0622913837433 and batch: 200, loss is 4.655631446838379 and perplexity is 105.1756118428064
At time: 310.3888087272644 and batch: 250, loss is 4.641663055419922 and perplexity is 103.71669084020392
At time: 310.71617126464844 and batch: 300, loss is 4.539728622436524 and perplexity is 93.66537798807148
At time: 311.0437934398651 and batch: 350, loss is 4.6080908203125 and perplexity is 100.29249035320905
At time: 311.3696804046631 and batch: 400, loss is 4.537672700881958 and perplexity is 93.47300713599954
At time: 311.69593334198 and batch: 450, loss is 4.576125583648682 and perplexity is 97.13731379859868
At time: 312.02189803123474 and batch: 500, loss is 4.542808151245117 and perplexity is 93.95426781191388
At time: 312.3490471839905 and batch: 550, loss is 4.597770767211914 and perplexity is 99.2627889526235
At time: 312.6767876148224 and batch: 600, loss is 4.625932111740112 and perplexity is 102.09789539043683
At time: 313.0031912326813 and batch: 650, loss is 4.602708330154419 and perplexity is 99.75411720481549
At time: 313.3310537338257 and batch: 700, loss is 4.573757257461548 and perplexity is 96.90753315973647
At time: 313.6583740711212 and batch: 750, loss is 4.533377332687378 and perplexity is 93.0723672180662
At time: 313.98567056655884 and batch: 800, loss is 4.558260669708252 and perplexity is 95.4173730873283
At time: 314.3117685317993 and batch: 850, loss is 4.530117826461792 and perplexity is 92.76949113907729
At time: 314.6384961605072 and batch: 900, loss is 4.65627119064331 and perplexity is 105.24291881622673
At time: 314.9655373096466 and batch: 950, loss is 4.5942704963684085 and perplexity is 98.9159496764303
At time: 315.29328203201294 and batch: 1000, loss is 4.561896381378173 and perplexity is 95.7649145416073
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.958841463414634 and perplexity of 142.4286914258484
Finished 43 epochs...
Completing Train Step...
At time: 316.41148495674133 and batch: 50, loss is 4.65546365737915 and perplexity is 105.15796596420884
At time: 316.7546901702881 and batch: 100, loss is 4.605849800109863 and perplexity is 100.06798451117739
At time: 317.08250069618225 and batch: 150, loss is 4.6473261165618895 and perplexity is 104.30571105645522
At time: 317.4109880924225 and batch: 200, loss is 4.652733182907104 and perplexity is 104.87122646819688
At time: 317.7424600124359 and batch: 250, loss is 4.638305835723877 and perplexity is 103.36907496084089
At time: 318.07019090652466 and batch: 300, loss is 4.536844081878662 and perplexity is 93.39558570685502
At time: 318.4208734035492 and batch: 350, loss is 4.605815591812134 and perplexity is 100.0645614143196
At time: 318.7490599155426 and batch: 400, loss is 4.535341472625732 and perplexity is 93.25535401869014
At time: 319.07770800590515 and batch: 450, loss is 4.5738739395141605 and perplexity is 96.91884118932842
At time: 319.4050078392029 and batch: 500, loss is 4.540559511184693 and perplexity is 93.74323583786116
At time: 319.734041929245 and batch: 550, loss is 4.595964632034302 and perplexity is 99.08366874401582
At time: 320.0628547668457 and batch: 600, loss is 4.624213819503784 and perplexity is 101.92261200660825
At time: 320.3934440612793 and batch: 650, loss is 4.600715312957764 and perplexity is 99.55550351977708
At time: 320.7213008403778 and batch: 700, loss is 4.572304277420044 and perplexity is 96.7668306918765
At time: 321.0497610569 and batch: 750, loss is 4.531794786453247 and perplexity is 92.9251923800325
At time: 321.3788242340088 and batch: 800, loss is 4.556117448806763 and perplexity is 95.2130915673695
At time: 321.7065284252167 and batch: 850, loss is 4.528258543014527 and perplexity is 92.59716660950677
At time: 322.032429933548 and batch: 900, loss is 4.654892320632935 and perplexity is 105.09790251395818
At time: 322.36010670661926 and batch: 950, loss is 4.59287841796875 and perplexity is 98.77834671877812
At time: 322.6898274421692 and batch: 1000, loss is 4.5602027034759525 and perplexity is 95.60285689750458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.958008556831174 and perplexity of 142.31011102114127
Finished 44 epochs...
Completing Train Step...
At time: 323.8170349597931 and batch: 50, loss is 4.652787427902222 and perplexity is 104.87691536166027
At time: 324.1591076850891 and batch: 100, loss is 4.603361701965332 and perplexity is 99.81931502991071
At time: 324.48487877845764 and batch: 150, loss is 4.6446474742889405 and perplexity is 104.02668723884177
At time: 324.82390666007996 and batch: 200, loss is 4.649825859069824 and perplexity is 104.56677463612448
At time: 325.1496458053589 and batch: 250, loss is 4.635078115463257 and perplexity is 103.03596638311956
At time: 325.47738695144653 and batch: 300, loss is 4.534293584823608 and perplexity is 93.15768405325127
At time: 325.803115606308 and batch: 350, loss is 4.6038144588470455 and perplexity is 99.86451914418319
At time: 326.12892150878906 and batch: 400, loss is 4.533023929595947 and perplexity is 93.0394809671582
At time: 326.4548535346985 and batch: 450, loss is 4.5718069267272945 and perplexity is 96.71871560762375
At time: 326.78238105773926 and batch: 500, loss is 4.538410243988037 and perplexity is 93.54197293751656
At time: 327.10785818099976 and batch: 550, loss is 4.594180536270142 and perplexity is 98.90705158811973
At time: 327.43348574638367 and batch: 600, loss is 4.622421321868896 and perplexity is 101.7400796089641
At time: 327.7585096359253 and batch: 650, loss is 4.598703470230102 and perplexity is 99.3554148449832
At time: 328.085990190506 and batch: 700, loss is 4.570734481811524 and perplexity is 96.61504571287249
At time: 328.4134101867676 and batch: 750, loss is 4.53020562171936 and perplexity is 92.77763621799072
At time: 328.739146232605 and batch: 800, loss is 4.553977603912354 and perplexity is 95.00956815142945
At time: 329.066486120224 and batch: 850, loss is 4.5263112354278565 and perplexity is 92.41702689510441
At time: 329.3931713104248 and batch: 900, loss is 4.653461866378784 and perplexity is 104.94767224658405
At time: 329.72215580940247 and batch: 950, loss is 4.591819334030151 and perplexity is 98.6737875365355
At time: 330.04946208000183 and batch: 1000, loss is 4.558751468658447 and perplexity is 95.46421532799091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.957372153677592 and perplexity of 142.21957323002445
Finished 45 epochs...
Completing Train Step...
At time: 331.22116017341614 and batch: 50, loss is 4.650380430221557 and perplexity is 104.6247804354523
At time: 331.54746651649475 and batch: 100, loss is 4.601198635101318 and perplexity is 99.60363252911196
At time: 331.8733162879944 and batch: 150, loss is 4.642096738815308 and perplexity is 103.76168080184019
At time: 332.1995987892151 and batch: 200, loss is 4.647081508636474 and perplexity is 104.28020017307408
At time: 332.5268087387085 and batch: 250, loss is 4.632230701446534 and perplexity is 102.74299762782728
At time: 332.85218834877014 and batch: 300, loss is 4.5321260356903075 and perplexity is 92.95597887783252
At time: 333.1925277709961 and batch: 350, loss is 4.601855220794678 and perplexity is 99.66905232373762
At time: 333.5187256336212 and batch: 400, loss is 4.5309286785125735 and perplexity is 92.84474397655305
At time: 333.84630012512207 and batch: 450, loss is 4.569967422485352 and perplexity is 96.54096465692463
At time: 334.17140650749207 and batch: 500, loss is 4.536473417282105 and perplexity is 93.36097368488059
At time: 334.49638319015503 and batch: 550, loss is 4.592315263748169 and perplexity is 98.72273493639617
At time: 334.82235860824585 and batch: 600, loss is 4.620705051422119 and perplexity is 101.56561587338
At time: 335.15205907821655 and batch: 650, loss is 4.596742296218872 and perplexity is 99.16075253324018
At time: 335.47710061073303 and batch: 700, loss is 4.569293403625489 and perplexity is 96.47591615041922
At time: 335.80398058891296 and batch: 750, loss is 4.528607511520386 and perplexity is 92.62948574323721
At time: 336.128986120224 and batch: 800, loss is 4.552056808471679 and perplexity is 94.82724936070947
At time: 336.45537185668945 and batch: 850, loss is 4.524570837020874 and perplexity is 92.25632433252386
At time: 336.78432869911194 and batch: 900, loss is 4.652091779708862 and perplexity is 104.80398329543499
At time: 337.1093900203705 and batch: 950, loss is 4.590899362564087 and perplexity is 98.5830522109081
At time: 337.43484234809875 and batch: 1000, loss is 4.557266817092896 and perplexity is 95.32258938984968
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.956944907583842 and perplexity of 142.15882345138405
Finished 46 epochs...
Completing Train Step...
At time: 338.5395562648773 and batch: 50, loss is 4.6481498432159425 and perplexity is 104.39166584758841
At time: 338.8923833370209 and batch: 100, loss is 4.599274206161499 and perplexity is 99.41213673528476
At time: 339.2174942493439 and batch: 150, loss is 4.6397965621948245 and perplexity is 103.52328489101696
At time: 339.5449528694153 and batch: 200, loss is 4.644549493789673 and perplexity is 104.01649515141
At time: 339.87187123298645 and batch: 250, loss is 4.629692153930664 and perplexity is 102.48251041589504
At time: 340.19844818115234 and batch: 300, loss is 4.530080366134643 and perplexity is 92.7660160286796
At time: 340.5231432914734 and batch: 350, loss is 4.600007390975952 and perplexity is 99.48505093083554
At time: 340.84935879707336 and batch: 400, loss is 4.529057168960572 and perplexity is 92.67114664654783
At time: 341.17817640304565 and batch: 450, loss is 4.568312273025513 and perplexity is 96.38130709643167
At time: 341.5050358772278 and batch: 500, loss is 4.5347035694122315 and perplexity is 93.19588509840975
At time: 341.8473381996155 and batch: 550, loss is 4.59060073852539 and perplexity is 98.5536173369091
At time: 342.1753327846527 and batch: 600, loss is 4.6190805339813235 and perplexity is 101.40075470518657
At time: 342.50449657440186 and batch: 650, loss is 4.594893131256104 and perplexity is 98.97755737520936
At time: 342.8324089050293 and batch: 700, loss is 4.567931251525879 and perplexity is 96.34459074156959
At time: 343.1591935157776 and batch: 750, loss is 4.527090702056885 and perplexity is 92.48909096564358
At time: 343.48518323898315 and batch: 800, loss is 4.550336828231812 and perplexity is 94.66428855046915
At time: 343.8132722377777 and batch: 850, loss is 4.5229947471618654 and perplexity is 92.11103460022653
At time: 344.1406292915344 and batch: 900, loss is 4.65076153755188 and perplexity is 104.6646613051728
At time: 344.4656352996826 and batch: 950, loss is 4.589874773025513 and perplexity is 98.48209677470582
At time: 344.7899012565613 and batch: 1000, loss is 4.555813388824463 and perplexity is 95.18414547732964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.956699278296494 and perplexity of 142.12390936902557
Finished 47 epochs...
Completing Train Step...
At time: 345.9145128726959 and batch: 50, loss is 4.646107902526856 and perplexity is 104.17872174110002
At time: 346.25549817085266 and batch: 100, loss is 4.597379455566406 and perplexity is 99.22395386614707
At time: 346.582777261734 and batch: 150, loss is 4.637626714706421 and perplexity is 103.29889868127039
At time: 346.9090416431427 and batch: 200, loss is 4.642120981216431 and perplexity is 103.76419626461768
At time: 347.23548412323 and batch: 250, loss is 4.62731861114502 and perplexity is 102.23955224251536
At time: 347.5610601902008 and batch: 300, loss is 4.528061580657959 and perplexity is 92.57893024935706
At time: 347.8887572288513 and batch: 350, loss is 4.598212852478027 and perplexity is 99.30668127045136
At time: 348.2154288291931 and batch: 400, loss is 4.527419204711914 and perplexity is 92.51947886856716
At time: 348.54291558265686 and batch: 450, loss is 4.566632833480835 and perplexity is 96.2195763644333
At time: 348.86806297302246 and batch: 500, loss is 4.533010330200195 and perplexity is 93.03821569503947
At time: 349.19398164749146 and batch: 550, loss is 4.588955678939819 and perplexity is 98.39162404485774
At time: 349.5222535133362 and batch: 600, loss is 4.617362670898437 and perplexity is 101.22671162599288
At time: 349.8475341796875 and batch: 650, loss is 4.593179302215576 and perplexity is 98.80807203894936
At time: 350.1880838871002 and batch: 700, loss is 4.566422643661499 and perplexity is 96.19935411439072
At time: 350.5135643482208 and batch: 750, loss is 4.525605268478394 and perplexity is 92.35180655296061
At time: 350.841299533844 and batch: 800, loss is 4.548512659072876 and perplexity is 94.49176228123395
At time: 351.1820113658905 and batch: 850, loss is 4.521127586364746 and perplexity is 91.939208950417
At time: 351.5110170841217 and batch: 900, loss is 4.649258728027344 and perplexity is 104.50748838534196
At time: 351.83927035331726 and batch: 950, loss is 4.588747510910034 and perplexity is 98.37114418603281
At time: 352.16840624809265 and batch: 1000, loss is 4.554368085861206 and perplexity is 95.04667491705047
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.956460720155297 and perplexity of 142.0900085972002
Finished 48 epochs...
Completing Train Step...
At time: 353.30445289611816 and batch: 50, loss is 4.6441198825836185 and perplexity is 103.97181809704723
At time: 353.62945771217346 and batch: 100, loss is 4.5955788516998295 and perplexity is 99.04545158533513
At time: 353.97164392471313 and batch: 150, loss is 4.635609865188599 and perplexity is 103.09077029963653
At time: 354.30775260925293 and batch: 200, loss is 4.639812021255493 and perplexity is 103.52488527612893
At time: 354.6416895389557 and batch: 250, loss is 4.62495644569397 and perplexity is 101.998330519452
At time: 354.97331500053406 and batch: 300, loss is 4.526220245361328 and perplexity is 92.40861824623641
At time: 355.30110120773315 and batch: 350, loss is 4.59653790473938 and perplexity is 99.14048699144492
At time: 355.6335310935974 and batch: 400, loss is 4.525803689956665 and perplexity is 92.37013295305343
At time: 355.97104573249817 and batch: 450, loss is 4.565185956954956 and perplexity is 96.08045918502178
At time: 356.30783796310425 and batch: 500, loss is 4.531340627670288 and perplexity is 92.88299916968874
At time: 356.65113711357117 and batch: 550, loss is 4.587395544052124 and perplexity is 98.23823952091364
At time: 356.99047923088074 and batch: 600, loss is 4.6157717037200925 and perplexity is 101.06579129365085
At time: 357.3207857608795 and batch: 650, loss is 4.591598787307739 and perplexity is 98.65202775571872
At time: 357.65742683410645 and batch: 700, loss is 4.5649965953826905 and perplexity is 96.062266960715
At time: 357.9884512424469 and batch: 750, loss is 4.524054040908814 and perplexity is 92.20865894050137
At time: 358.33709502220154 and batch: 800, loss is 4.5467757225036625 and perplexity is 94.32777853974446
At time: 358.67343497276306 and batch: 850, loss is 4.51920443534851 and perplexity is 91.76256587738781
At time: 359.0629825592041 and batch: 900, loss is 4.647755107879639 and perplexity is 104.35046690013789
At time: 359.4030110836029 and batch: 950, loss is 4.5875073909759525 and perplexity is 98.24922778029641
At time: 359.7452027797699 and batch: 1000, loss is 4.552962522506714 and perplexity is 94.91317463734991
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.956148473227897 and perplexity of 142.04564835463657
Finished 49 epochs...
Completing Train Step...
At time: 360.8905119895935 and batch: 50, loss is 4.6422152328491215 and perplexity is 103.77397667043282
At time: 361.24126410484314 and batch: 100, loss is 4.593631038665771 and perplexity is 98.85271732985625
At time: 361.5736379623413 and batch: 150, loss is 4.6332752132415775 and perplexity is 102.85036996678299
At time: 361.9080114364624 and batch: 200, loss is 4.637391004562378 and perplexity is 103.27455295236308
At time: 362.2502546310425 and batch: 250, loss is 4.62250247001648 and perplexity is 101.74833596294877
At time: 362.5854868888855 and batch: 300, loss is 4.524416093826294 and perplexity is 92.24204939867937
At time: 362.9145829677582 and batch: 350, loss is 4.594717826843262 and perplexity is 98.96020769341148
At time: 363.248085975647 and batch: 400, loss is 4.524256725311279 and perplexity is 92.22735009157925
At time: 363.59213066101074 and batch: 450, loss is 4.563741788864136 and perplexity is 95.94180299723516
At time: 363.93076729774475 and batch: 500, loss is 4.529540634155273 and perplexity is 92.71596075266189
At time: 364.26299357414246 and batch: 550, loss is 4.585751237869263 and perplexity is 98.07683850898249
At time: 364.60695481300354 and batch: 600, loss is 4.6142026233673095 and perplexity is 100.90733529380165
At time: 364.9764304161072 and batch: 650, loss is 4.590016164779663 and perplexity is 98.49602231557631
At time: 365.31988167762756 and batch: 700, loss is 4.563745498657227 and perplexity is 95.9421589221332
At time: 365.6529531478882 and batch: 750, loss is 4.522560625076294 and perplexity is 92.07105584424009
At time: 365.98459672927856 and batch: 800, loss is 4.544828805923462 and perplexity is 94.14430888174475
At time: 366.3198230266571 and batch: 850, loss is 4.517276964187622 and perplexity is 91.58586652416908
At time: 366.6480779647827 and batch: 900, loss is 4.646223287582398 and perplexity is 104.19074310222382
At time: 366.9801061153412 and batch: 950, loss is 4.586238775253296 and perplexity is 98.12466629223219
At time: 367.30851674079895 and batch: 1000, loss is 4.551554346084595 and perplexity is 94.77961420307477
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.9557859839462655 and perplexity of 141.99416766076925
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f4997a568d0>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'num_layers': 1, 'wordvec_source': 'glove', 'data': 'ptb', 'dropout': 0.2685505938429682, 'seq_len': 20, 'batch_size': 50, 'lr': 19.00591146735055, 'anneal': 4.740145085532791, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.6050550937652588 and batch: 50, loss is 6.682969722747803 and perplexity is 798.6874739459203
At time: 0.9698286056518555 and batch: 100, loss is 5.897808990478516 and perplexity is 364.2385428433072
At time: 1.3001222610473633 and batch: 150, loss is 5.790453147888184 and perplexity is 327.16124321736675
At time: 1.6873667240142822 and batch: 200, loss is 5.811008749008178 and perplexity is 333.9558334349589
At time: 2.027414083480835 and batch: 250, loss is 5.8212794589996335 and perplexity is 337.403471488591
At time: 2.376132011413574 and batch: 300, loss is 5.735139608383179 and perplexity is 309.55618438104943
At time: 2.7159650325775146 and batch: 350, loss is 5.792059907913208 and perplexity is 327.6873353634371
At time: 3.046740770339966 and batch: 400, loss is 5.786858539581299 and perplexity is 325.9873378238692
At time: 3.384363889694214 and batch: 450, loss is 5.786274223327637 and perplexity is 325.79691376324547
At time: 3.7386298179626465 and batch: 500, loss is 5.827366857528687 and perplexity is 339.463645071563
At time: 4.069678544998169 and batch: 550, loss is 5.885320262908936 and perplexity is 359.7179538690413
At time: 4.4069695472717285 and batch: 600, loss is 5.926129550933838 and perplexity is 374.7014407745532
At time: 4.741515636444092 and batch: 650, loss is 5.919733190536499 and perplexity is 372.3123641609131
At time: 5.0758092403411865 and batch: 700, loss is 5.939226951599121 and perplexity is 379.641334944461
At time: 5.413447141647339 and batch: 750, loss is 5.870487289428711 and perplexity is 354.42164411619893
At time: 5.755083322525024 and batch: 800, loss is 5.932179441452027 and perplexity is 376.9752145742941
At time: 6.095364093780518 and batch: 850, loss is 5.923250217437744 and perplexity is 373.6241021180483
At time: 6.430175065994263 and batch: 900, loss is 5.966346130371094 and perplexity is 390.0777703395916
At time: 6.771237850189209 and batch: 950, loss is 5.981204090118408 and perplexity is 395.91680077137954
At time: 7.113650798797607 and batch: 1000, loss is 5.942790822982788 and perplexity is 380.99674164710217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.866707406392911 and perplexity of 353.08450047141156
Finished 1 epochs...
Completing Train Step...
At time: 8.289577960968018 and batch: 50, loss is 5.844702091217041 and perplexity is 345.3996288694408
At time: 8.622941255569458 and batch: 100, loss is 5.8626650524139405 and perplexity is 351.66008886363085
At time: 8.947137355804443 and batch: 150, loss is 5.88587718963623 and perplexity is 359.918346208584
At time: 9.270985841751099 and batch: 200, loss is 5.843084630966186 and perplexity is 344.84141026898317
At time: 9.610241174697876 and batch: 250, loss is 5.884049911499023 and perplexity is 359.26127579140814
At time: 9.937941789627075 and batch: 300, loss is 5.77393367767334 and perplexity is 321.8011079963329
At time: 10.294678926467896 and batch: 350, loss is 5.832466011047363 and perplexity is 341.19904308200296
At time: 10.639697074890137 and batch: 400, loss is 5.768680992126465 and perplexity is 320.11521956452776
At time: 10.994961023330688 and batch: 450, loss is 5.781697816848755 and perplexity is 324.3093411210851
At time: 11.322425842285156 and batch: 500, loss is 5.778748693466187 and perplexity is 323.3543217871003
At time: 11.65686321258545 and batch: 550, loss is 5.837938175201416 and perplexity is 343.0712581170665
At time: 11.983519554138184 and batch: 600, loss is 5.863848133087158 and perplexity is 352.0763773211658
At time: 12.310660123825073 and batch: 650, loss is 5.845039415359497 and perplexity is 345.51616015635705
At time: 12.637866735458374 and batch: 700, loss is 5.847994136810303 and perplexity is 346.5385738964376
At time: 12.976942777633667 and batch: 750, loss is 5.763186111450195 and perplexity is 318.3610485243809
At time: 13.304831743240356 and batch: 800, loss is 5.8205237865448 and perplexity is 337.14860129034435
At time: 13.639074802398682 and batch: 850, loss is 5.783033313751221 and perplexity is 324.74274458166616
At time: 13.978456258773804 and batch: 900, loss is 5.8473099422454835 and perplexity is 346.30155518038225
At time: 14.306464910507202 and batch: 950, loss is 5.860420913696289 and perplexity is 350.87179968871726
At time: 14.63455057144165 and batch: 1000, loss is 5.83506010055542 and perplexity is 342.0852929485746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.902123055806974 and perplexity of 365.8132860424783
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 15.760205507278442 and batch: 50, loss is 5.692880792617798 and perplexity is 296.7472577839103
At time: 16.0999813079834 and batch: 100, loss is 5.549608469009399 and perplexity is 257.13685914492896
At time: 16.42593288421631 and batch: 150, loss is 5.48538908958435 and perplexity is 241.14275188010708
At time: 16.750255823135376 and batch: 200, loss is 5.476830520629883 and perplexity is 239.08772161346968
At time: 17.076794862747192 and batch: 250, loss is 5.449308376312256 and perplexity is 232.59724050293028
At time: 17.402405261993408 and batch: 300, loss is 5.343287334442139 and perplexity is 209.1992891921944
At time: 17.73137140274048 and batch: 350, loss is 5.410133056640625 and perplexity is 223.66134532802278
At time: 18.071576833724976 and batch: 400, loss is 5.339558753967285 and perplexity is 208.42072517843567
At time: 18.414469957351685 and batch: 450, loss is 5.316698017120362 and perplexity is 203.7101228964254
At time: 18.74933123588562 and batch: 500, loss is 5.314625091552735 and perplexity is 203.28828434530203
At time: 19.092182159423828 and batch: 550, loss is 5.377227401733398 and perplexity is 216.4213932276897
At time: 19.419668674468994 and batch: 600, loss is 5.389514074325562 and perplexity is 219.09689487636268
At time: 19.750044107437134 and batch: 650, loss is 5.351838293075562 and perplexity is 210.99581371702243
At time: 20.07877469062805 and batch: 700, loss is 5.3359699535369876 and perplexity is 207.67408536224056
At time: 20.40614891052246 and batch: 750, loss is 5.258656454086304 and perplexity is 192.2230572240479
At time: 20.73958468437195 and batch: 800, loss is 5.329175415039063 and perplexity is 206.26781867092106
At time: 21.066683292388916 and batch: 850, loss is 5.296013603210449 and perplexity is 199.53977765770793
At time: 21.404735565185547 and batch: 900, loss is 5.3496051597595216 and perplexity is 210.52515765032965
At time: 21.734344482421875 and batch: 950, loss is 5.308825569152832 and perplexity is 202.1127215329848
At time: 22.05894160270691 and batch: 1000, loss is 5.269467687606811 and perplexity is 194.3124999587744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.440833114996189 and perplexity of 230.63424829437528
Finished 3 epochs...
Completing Train Step...
At time: 23.187334775924683 and batch: 50, loss is 5.400394535064697 and perplexity is 221.49378603302574
At time: 23.5262713432312 and batch: 100, loss is 5.380741748809815 and perplexity is 217.18331115609476
At time: 23.867223024368286 and batch: 150, loss is 5.367747812271118 and perplexity is 214.3795007112218
At time: 24.204546213150024 and batch: 200, loss is 5.381519908905029 and perplexity is 217.35238031508808
At time: 24.555936336517334 and batch: 250, loss is 5.370972290039062 and perplexity is 215.0718783233745
At time: 24.88677716255188 and batch: 300, loss is 5.270632543563843 and perplexity is 194.53897791336556
At time: 25.226182460784912 and batch: 350, loss is 5.3322044849395756 and perplexity is 206.893565548969
At time: 25.5684175491333 and batch: 400, loss is 5.263841257095337 and perplexity is 193.2222840682742
At time: 25.904698610305786 and batch: 450, loss is 5.244702072143554 and perplexity is 189.55933181054993
At time: 26.23803734779358 and batch: 500, loss is 5.237609519958496 and perplexity is 188.21962892987847
At time: 26.567381381988525 and batch: 550, loss is 5.300028152465821 and perplexity is 200.34245002937746
At time: 26.896317720413208 and batch: 600, loss is 5.315360736846924 and perplexity is 203.43788743570244
At time: 27.228551387786865 and batch: 650, loss is 5.2784712028503415 and perplexity is 196.0698950090549
At time: 27.59744930267334 and batch: 700, loss is 5.25932050704956 and perplexity is 192.3507459061354
At time: 27.9331533908844 and batch: 750, loss is 5.190843524932862 and perplexity is 179.62000299786826
At time: 28.25875687599182 and batch: 800, loss is 5.2462348365783695 and perplexity is 189.85010439864968
At time: 28.58159065246582 and batch: 850, loss is 5.21678635597229 and perplexity is 184.34082536701203
At time: 28.911237001419067 and batch: 900, loss is 5.281827173233032 and perplexity is 196.72900512805745
At time: 29.23770523071289 and batch: 950, loss is 5.252846622467041 and perplexity is 191.1095115169955
At time: 29.568052768707275 and batch: 1000, loss is 5.210341367721558 and perplexity is 183.15657126455974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.382195821622523 and perplexity of 217.4993412138958
Finished 4 epochs...
Completing Train Step...
At time: 30.749984741210938 and batch: 50, loss is 5.308536319732666 and perplexity is 202.0542689995615
At time: 31.086447954177856 and batch: 100, loss is 5.281264457702637 and perplexity is 196.61833380274976
At time: 31.41874122619629 and batch: 150, loss is 5.286786756515503 and perplexity is 197.70712253550545
At time: 31.74750256538391 and batch: 200, loss is 5.293323841094971 and perplexity is 199.00378429370681
At time: 32.08059573173523 and batch: 250, loss is 5.287269821166992 and perplexity is 197.8026509290875
At time: 32.41877841949463 and batch: 300, loss is 5.179861431121826 and perplexity is 177.658191391369
At time: 32.74458408355713 and batch: 350, loss is 5.2425150680542 and perplexity is 189.14521777630657
At time: 33.0729124546051 and batch: 400, loss is 5.177702207565307 and perplexity is 177.27500148472188
At time: 33.4085853099823 and batch: 450, loss is 5.175632219314576 and perplexity is 176.9084238510916
At time: 33.7521858215332 and batch: 500, loss is 5.167337799072266 and perplexity is 175.44713966951767
At time: 34.08212971687317 and batch: 550, loss is 5.230537061691284 and perplexity is 186.893149722785
At time: 34.417335510253906 and batch: 600, loss is 5.25325647354126 and perplexity is 191.1878540088647
At time: 34.74311947822571 and batch: 650, loss is 5.223205528259277 and perplexity is 185.52794697728686
At time: 35.074867248535156 and batch: 700, loss is 5.204214315414429 and perplexity is 182.0377922806878
At time: 35.40548300743103 and batch: 750, loss is 5.142621479034424 and perplexity is 171.1638832341211
At time: 35.73659920692444 and batch: 800, loss is 5.197553434371948 and perplexity is 180.82928951581584
At time: 36.05990481376648 and batch: 850, loss is 5.16736870765686 and perplexity is 175.45256257608267
At time: 36.417969703674316 and batch: 900, loss is 5.242592658996582 and perplexity is 189.15989430137628
At time: 36.74726366996765 and batch: 950, loss is 5.212534112930298 and perplexity is 183.5586276011072
At time: 37.077544927597046 and batch: 1000, loss is 5.170021057128906 and perplexity is 175.91854178447556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.349058569931403 and perplexity of 210.41011818316028
Finished 5 epochs...
Completing Train Step...
At time: 38.20600891113281 and batch: 50, loss is 5.266340017318726 and perplexity is 193.7057039496301
At time: 38.5459246635437 and batch: 100, loss is 5.244247274398804 and perplexity is 189.4731402552942
At time: 38.875028133392334 and batch: 150, loss is 5.252263536453247 and perplexity is 190.99811071500795
At time: 39.202049016952515 and batch: 200, loss is 5.255974159240723 and perplexity is 191.70814918476148
At time: 39.54147696495056 and batch: 250, loss is 5.24832522392273 and perplexity is 190.247379739217
At time: 39.86842632293701 and batch: 300, loss is 5.149094705581665 and perplexity is 172.27545968861
At time: 40.19819450378418 and batch: 350, loss is 5.207811126708984 and perplexity is 182.69372679718282
At time: 40.54451131820679 and batch: 400, loss is 5.148047552108765 and perplexity is 172.09515526238047
At time: 40.87229371070862 and batch: 450, loss is 5.147522401809693 and perplexity is 172.00480316641708
At time: 41.22592830657959 and batch: 500, loss is 5.134738931655884 and perplexity is 169.81997946615925
At time: 41.57539129257202 and batch: 550, loss is 5.194864110946655 and perplexity is 180.34363440608024
At time: 41.935214042663574 and batch: 600, loss is 5.225308818817139 and perplexity is 185.918576816403
At time: 42.26383948326111 and batch: 650, loss is 5.196378355026245 and perplexity is 180.61692554929925
At time: 42.592817544937134 and batch: 700, loss is 5.176900720596313 and perplexity is 177.13297480497295
At time: 42.92169523239136 and batch: 750, loss is 5.119969673156739 and perplexity is 167.33029494452734
At time: 43.24949264526367 and batch: 800, loss is 5.17252667427063 and perplexity is 176.35987897843984
At time: 43.577701568603516 and batch: 850, loss is 5.144267358779907 and perplexity is 171.4458303645824
At time: 43.90842247009277 and batch: 900, loss is 5.22584454536438 and perplexity is 186.01820501798227
At time: 44.23873329162598 and batch: 950, loss is 5.191762208938599 and perplexity is 179.7850928428218
At time: 44.580588579177856 and batch: 1000, loss is 5.148561754226685 and perplexity is 172.18366971090825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.335670285108613 and perplexity of 207.61186131922207
Finished 6 epochs...
Completing Train Step...
At time: 45.743823289871216 and batch: 50, loss is 5.244305725097656 and perplexity is 189.4842154164282
At time: 46.107136249542236 and batch: 100, loss is 5.220422286987304 and perplexity is 185.0122958617599
At time: 46.43529009819031 and batch: 150, loss is 5.229977750778199 and perplexity is 186.78864757188515
At time: 46.77977395057678 and batch: 200, loss is 5.236971807479859 and perplexity is 188.0996371879675
At time: 47.107030630111694 and batch: 250, loss is 5.227899293899537 and perplexity is 186.40081860499325
At time: 47.45067548751831 and batch: 300, loss is 5.124621286392212 and perplexity is 168.11046387450523
At time: 47.78160381317139 and batch: 350, loss is 5.1825498104095455 and perplexity is 178.13644657101446
At time: 48.10898017883301 and batch: 400, loss is 5.126364135742188 and perplexity is 168.40371055535803
At time: 48.449408292770386 and batch: 450, loss is 5.125376148223877 and perplexity is 168.23741195528996
At time: 48.77761888504028 and batch: 500, loss is 5.111818151473999 and perplexity is 165.9718426658264
At time: 49.10547232627869 and batch: 550, loss is 5.172063550949097 and perplexity is 176.27822151569933
At time: 49.44217038154602 and batch: 600, loss is 5.202007236480713 and perplexity is 181.63646354921192
At time: 49.78171706199646 and batch: 650, loss is 5.1687912750244145 and perplexity is 175.70233328186325
At time: 50.10795879364014 and batch: 700, loss is 5.150294256210327 and perplexity is 172.48223681960664
At time: 50.44440293312073 and batch: 750, loss is 5.0948543453216555 and perplexity is 163.1800749520182
At time: 50.7764847278595 and batch: 800, loss is 5.14795672416687 and perplexity is 172.07952492346442
At time: 51.1230628490448 and batch: 850, loss is 5.114492864608764 and perplexity is 166.416363951855
At time: 51.448538064956665 and batch: 900, loss is 5.196669187545776 and perplexity is 180.66946246417808
At time: 51.77531099319458 and batch: 950, loss is 5.1667923259735105 and perplexity is 175.35146407117918
At time: 52.108015060424805 and batch: 1000, loss is 5.123433055877686 and perplexity is 167.91082852141628
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.314239874118712 and perplexity of 203.20998923532858
Finished 7 epochs...
Completing Train Step...
At time: 53.29371619224548 and batch: 50, loss is 5.214099779129028 and perplexity is 183.84624423689684
At time: 53.62482404708862 and batch: 100, loss is 5.189394855499268 and perplexity is 179.35998137802397
At time: 53.978564500808716 and batch: 150, loss is 5.204612913131714 and perplexity is 182.11036659216705
At time: 54.30810880661011 and batch: 200, loss is 5.205355434417725 and perplexity is 182.24563763036255
At time: 54.64735460281372 and batch: 250, loss is 5.197630624771119 and perplexity is 180.84324833959192
At time: 54.98776030540466 and batch: 300, loss is 5.098306303024292 and perplexity is 163.74433901619227
At time: 55.316508293151855 and batch: 350, loss is 5.156205263137817 and perplexity is 173.50479972717065
At time: 55.65094876289368 and batch: 400, loss is 5.09617015838623 and perplexity is 163.39493075055225
At time: 55.97648525238037 and batch: 450, loss is 5.096885061264038 and perplexity is 163.51178402115787
At time: 56.30372714996338 and batch: 500, loss is 5.084662284851074 and perplexity is 161.5253804575387
At time: 56.65183234214783 and batch: 550, loss is 5.143409023284912 and perplexity is 171.29873546033113
At time: 56.98429775238037 and batch: 600, loss is 5.176174764633179 and perplexity is 177.00443072995978
At time: 57.309866189956665 and batch: 650, loss is 5.142603816986084 and perplexity is 171.1608601560384
At time: 57.64118766784668 and batch: 700, loss is 5.127668581008911 and perplexity is 168.623527316788
At time: 57.98228049278259 and batch: 750, loss is 5.066700267791748 and perplexity is 158.64996029983246
At time: 58.31614303588867 and batch: 800, loss is 5.121539115905762 and perplexity is 167.5931164503258
At time: 58.640782594680786 and batch: 850, loss is 5.091330614089966 and perplexity is 162.606084114163
At time: 58.98254132270813 and batch: 900, loss is 5.179730596542359 and perplexity is 177.63494907709176
At time: 59.32221031188965 and batch: 950, loss is 5.1452165222167965 and perplexity is 171.60863773134855
At time: 59.64761710166931 and batch: 1000, loss is 5.103954668045044 and perplexity is 164.6718437898708
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.3003428389386436 and perplexity of 200.40550500908813
Finished 8 epochs...
Completing Train Step...
At time: 60.78065371513367 and batch: 50, loss is 5.1939151096344 and perplexity is 180.17256924374223
At time: 61.12241888046265 and batch: 100, loss is 5.172326192855835 and perplexity is 176.32452564435084
At time: 61.45529866218567 and batch: 150, loss is 5.187842969894409 and perplexity is 179.08185107386288
At time: 61.78619718551636 and batch: 200, loss is 5.188581027984619 and perplexity is 179.21407267046519
At time: 62.11132216453552 and batch: 250, loss is 5.178634920120239 and perplexity is 177.44042523869584
At time: 62.47514319419861 and batch: 300, loss is 5.080445184707641 and perplexity is 160.8456460139154
At time: 62.80639290809631 and batch: 350, loss is 5.139702186584473 and perplexity is 170.66493444515564
At time: 63.136452436447144 and batch: 400, loss is 5.077162494659424 and perplexity is 160.3185053055997
At time: 63.46572017669678 and batch: 450, loss is 5.080942249298095 and perplexity is 160.92561656269513
At time: 63.79383206367493 and batch: 500, loss is 5.072483129501343 and perplexity is 159.57006894686208
At time: 64.14433217048645 and batch: 550, loss is 5.126013898849488 and perplexity is 168.34473969054298
At time: 64.47379350662231 and batch: 600, loss is 5.160607013702393 and perplexity is 174.27020790943456
At time: 64.79809975624084 and batch: 650, loss is 5.130522108078003 and perplexity is 169.10538628831546
At time: 65.13849925994873 and batch: 700, loss is 5.115390396118164 and perplexity is 166.56579493165654
At time: 65.46848821640015 and batch: 750, loss is 5.056108913421631 and perplexity is 156.9785094384874
At time: 65.80499505996704 and batch: 800, loss is 5.107758646011352 and perplexity is 165.29944478870945
At time: 66.14925241470337 and batch: 850, loss is 5.079766035079956 and perplexity is 160.73644483947612
At time: 66.48246431350708 and batch: 900, loss is 5.167831192016601 and perplexity is 175.5337254089744
At time: 66.81550073623657 and batch: 950, loss is 5.132850017547607 and perplexity is 169.49950687896117
At time: 67.14364218711853 and batch: 1000, loss is 5.091778745651245 and perplexity is 162.67896936238094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.294026072432355 and perplexity of 199.14358006603723
Finished 9 epochs...
Completing Train Step...
At time: 68.28539896011353 and batch: 50, loss is 5.1820237350463865 and perplexity is 178.04275802097246
At time: 68.62868022918701 and batch: 100, loss is 5.159005031585694 and perplexity is 173.99125365241358
At time: 68.96080303192139 and batch: 150, loss is 5.174842872619629 and perplexity is 176.7688368699253
At time: 69.28585410118103 and batch: 200, loss is 5.173652849197388 and perplexity is 176.5586029301943
At time: 69.62135100364685 and batch: 250, loss is 5.16268985748291 and perplexity is 174.63356380352988
At time: 69.97292923927307 and batch: 300, loss is 5.061665763854981 and perplexity is 157.85324366945392
At time: 70.30052137374878 and batch: 350, loss is 5.1207346343994145 and perplexity is 167.4583451053463
At time: 70.62865090370178 and batch: 400, loss is 5.055662603378296 and perplexity is 156.90846398529456
At time: 70.96538543701172 and batch: 450, loss is 5.05499493598938 and perplexity is 156.80373628637184
At time: 71.33517909049988 and batch: 500, loss is 5.050251636505127 and perplexity is 156.06173037423787
At time: 71.66271948814392 and batch: 550, loss is 5.100135545730591 and perplexity is 164.0441412759859
At time: 71.99135398864746 and batch: 600, loss is 5.131340675354004 and perplexity is 169.24386709392206
At time: 72.32499241828918 and batch: 650, loss is 5.102431354522705 and perplexity is 164.42118790569629
At time: 72.65513610839844 and batch: 700, loss is 5.088217401504517 and perplexity is 162.1006439861431
At time: 73.0070526599884 and batch: 750, loss is 5.027629413604736 and perplexity is 152.5709011542578
At time: 73.33270049095154 and batch: 800, loss is 5.079551954269409 and perplexity is 160.7020379341409
At time: 73.66347169876099 and batch: 850, loss is 5.050978317260742 and perplexity is 156.17517864573426
At time: 74.00358390808105 and batch: 900, loss is 5.139328832626343 and perplexity is 170.60122790965013
At time: 74.34420585632324 and batch: 950, loss is 5.105623455047607 and perplexity is 164.94687544327286
At time: 74.67373251914978 and batch: 1000, loss is 5.065108709335327 and perplexity is 158.39766044214994
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.264151968607089 and perplexity of 193.28232978422565
Finished 10 epochs...
Completing Train Step...
At time: 75.86071801185608 and batch: 50, loss is 5.147831907272339 and perplexity is 172.0580478319309
At time: 76.20852065086365 and batch: 100, loss is 5.12655276298523 and perplexity is 168.43547907910846
At time: 76.53466057777405 and batch: 150, loss is 5.14146650314331 and perplexity is 170.96630719530046
At time: 76.87925267219543 and batch: 200, loss is 5.143440790176392 and perplexity is 171.3041771751039
At time: 77.20971512794495 and batch: 250, loss is 5.132883472442627 and perplexity is 169.50517756202518
At time: 77.53547978401184 and batch: 300, loss is 5.036558275222778 and perplexity is 153.93928558406822
At time: 77.86673665046692 and batch: 350, loss is 5.095657882690429 and perplexity is 163.31124893462194
At time: 78.20843243598938 and batch: 400, loss is 5.030364780426026 and perplexity is 152.9888098413738
At time: 78.53643655776978 and batch: 450, loss is 5.036517019271851 and perplexity is 153.932934803461
At time: 78.86454582214355 and batch: 500, loss is 5.027113389968872 and perplexity is 152.49219127294324
At time: 79.19524431228638 and batch: 550, loss is 5.081463460922241 and perplexity is 161.00951472712038
At time: 79.52516961097717 and batch: 600, loss is 5.114560079574585 and perplexity is 166.42754999800067
At time: 79.87962007522583 and batch: 650, loss is 5.084560661315918 and perplexity is 161.50896651139493
At time: 80.21367049217224 and batch: 700, loss is 5.065935955047608 and perplexity is 158.5287484411029
At time: 80.55525183677673 and batch: 750, loss is 5.004597644805909 and perplexity is 149.09708110312235
At time: 80.88656330108643 and batch: 800, loss is 5.052696723937988 and perplexity is 156.44378183419846
At time: 81.21165132522583 and batch: 850, loss is 5.025806665420532 and perplexity is 152.2930561189096
At time: 81.54651188850403 and batch: 900, loss is 5.115552139282227 and perplexity is 166.59273798922243
At time: 81.87199807167053 and batch: 950, loss is 5.082279310226441 and perplexity is 161.14092782703392
At time: 82.20147562026978 and batch: 1000, loss is 5.050585107803345 and perplexity is 156.11378116030713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.2602040360613564 and perplexity of 192.52076846945747
Finished 11 epochs...
Completing Train Step...
At time: 83.3326530456543 and batch: 50, loss is 5.13308497428894 and perplexity is 169.53933660970364
At time: 83.70272660255432 and batch: 100, loss is 5.112346878051758 and perplexity is 166.0596195931558
At time: 84.03520011901855 and batch: 150, loss is 5.128286571502685 and perplexity is 168.7277672600259
At time: 84.36329126358032 and batch: 200, loss is 5.128749361038208 and perplexity is 168.80587077641704
At time: 84.68861389160156 and batch: 250, loss is 5.117525854110718 and perplexity is 166.92186924528536
At time: 85.01759672164917 and batch: 300, loss is 5.019990243911743 and perplexity is 151.4098266192156
At time: 85.36236667633057 and batch: 350, loss is 5.0792942237854 and perplexity is 160.66062545698105
At time: 85.69255781173706 and batch: 400, loss is 5.016554136276245 and perplexity is 150.89045897045884
At time: 86.02477550506592 and batch: 450, loss is 5.0210560131073 and perplexity is 151.57128056937177
At time: 86.3563084602356 and batch: 500, loss is 5.012662601470947 and perplexity is 150.30440456275332
At time: 86.68373608589172 and batch: 550, loss is 5.069239749908447 and perplexity is 159.05336103451145
At time: 87.04357242584229 and batch: 600, loss is 5.102321643829345 and perplexity is 164.40315013265456
At time: 87.3790020942688 and batch: 650, loss is 5.071969127655029 and perplexity is 159.48807071223422
At time: 87.71014165878296 and batch: 700, loss is 5.055853176116943 and perplexity is 156.93836931047258
At time: 88.05066108703613 and batch: 750, loss is 4.999533786773681 and perplexity is 148.3439830514912
At time: 88.39217138290405 and batch: 800, loss is 5.048094358444214 and perplexity is 155.72542470999525
At time: 88.74293446540833 and batch: 850, loss is 5.018774061203003 and perplexity is 151.22579653593382
At time: 89.07116365432739 and batch: 900, loss is 5.112682447433472 and perplexity is 166.11535346779823
At time: 89.40250945091248 and batch: 950, loss is 5.07529733657837 and perplexity is 160.0197646357789
At time: 89.72530961036682 and batch: 1000, loss is 5.037452983856201 and perplexity is 154.07707802475443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.251300346560594 and perplexity of 190.81423183460686
Finished 12 epochs...
Completing Train Step...
At time: 90.86557960510254 and batch: 50, loss is 5.117487516403198 and perplexity is 166.91546996615122
At time: 91.2157735824585 and batch: 100, loss is 5.097575874328613 and perplexity is 163.62477912252416
At time: 91.54312658309937 and batch: 150, loss is 5.112871465682983 and perplexity is 166.14675526879086
At time: 91.8745539188385 and batch: 200, loss is 5.11541446685791 and perplexity is 166.56980434181153
At time: 92.20102620124817 and batch: 250, loss is 5.099631328582763 and perplexity is 163.96144825632504
At time: 92.53197526931763 and batch: 300, loss is 5.003221168518066 and perplexity is 148.89199368776082
At time: 92.85841417312622 and batch: 350, loss is 5.064346380233765 and perplexity is 158.27695531033854
At time: 93.19935393333435 and batch: 400, loss is 5.001744365692138 and perplexity is 148.672271853571
At time: 93.53714656829834 and batch: 450, loss is 5.005256843566895 and perplexity is 149.19539811591844
At time: 93.86504101753235 and batch: 500, loss is 4.992740201950073 and perplexity is 147.33961112537423
At time: 94.20269393920898 and batch: 550, loss is 5.050687646865844 and perplexity is 156.129789741809
At time: 94.53858947753906 and batch: 600, loss is 5.086344594955444 and perplexity is 161.7973449373488
At time: 94.86348557472229 and batch: 650, loss is 5.058167457580566 and perplexity is 157.30198946690027
At time: 95.19940042495728 and batch: 700, loss is 5.039789056777954 and perplexity is 154.43743405969957
At time: 95.52660369873047 and batch: 750, loss is 4.978186454772949 and perplexity is 145.21079636726603
At time: 95.85464286804199 and batch: 800, loss is 5.022546033859253 and perplexity is 151.7972932628066
At time: 96.18217420578003 and batch: 850, loss is 4.9856093215942385 and perplexity is 146.2926871677945
At time: 96.52328372001648 and batch: 900, loss is 5.094214038848877 and perplexity is 163.075623137953
At time: 96.84922575950623 and batch: 950, loss is 5.056871385574341 and perplexity is 157.09824682291787
At time: 97.20404481887817 and batch: 1000, loss is 5.0227084064483645 and perplexity is 151.82194298349907
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.233126012290397 and perplexity of 187.37763373670776
Finished 13 epochs...
Completing Train Step...
At time: 98.34876441955566 and batch: 50, loss is 5.098171977996826 and perplexity is 163.72234553052746
At time: 98.68738174438477 and batch: 100, loss is 5.0724685001373295 and perplexity is 159.56773455531317
At time: 99.02326393127441 and batch: 150, loss is 5.084435663223267 and perplexity is 161.4887794603326
At time: 99.35166144371033 and batch: 200, loss is 5.082683153152466 and perplexity is 161.20601659276423
At time: 99.67676377296448 and batch: 250, loss is 5.064853897094727 and perplexity is 158.35730392127297
At time: 100.0066568851471 and batch: 300, loss is 4.971143064498901 and perplexity is 144.19161352113179
At time: 100.34162402153015 and batch: 350, loss is 5.033672332763672 and perplexity is 153.49566610172297
At time: 100.67017579078674 and batch: 400, loss is 4.960114297866821 and perplexity is 142.61009499514034
At time: 100.99883532524109 and batch: 450, loss is 4.966916599273682 and perplexity is 143.5834787163782
At time: 101.32933902740479 and batch: 500, loss is 4.954955129623413 and perplexity is 141.87624018980696
At time: 101.6555609703064 and batch: 550, loss is 5.012381362915039 and perplexity is 150.26213911268303
At time: 102.02017784118652 and batch: 600, loss is 5.0452024269104 and perplexity is 155.27572800311466
At time: 102.34824585914612 and batch: 650, loss is 5.018981161117554 and perplexity is 151.25711862876375
At time: 102.67516016960144 and batch: 700, loss is 5.000726919174195 and perplexity is 148.521082694743
At time: 103.01399064064026 and batch: 750, loss is 4.937886171340942 and perplexity is 139.47511124660846
At time: 103.36631155014038 and batch: 800, loss is 4.976267223358154 and perplexity is 144.93237051239822
At time: 103.69314002990723 and batch: 850, loss is 4.946627473831176 and perplexity is 140.6996496167505
At time: 104.01812553405762 and batch: 900, loss is 5.049086990356446 and perplexity is 155.88007948104016
At time: 104.34409427642822 and batch: 950, loss is 5.00871657371521 and perplexity is 149.71246787792674
At time: 104.70465922355652 and batch: 1000, loss is 4.980256118774414 and perplexity is 145.511645145684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.183354075362042 and perplexity of 178.27977310028015
Finished 14 epochs...
Completing Train Step...
At time: 105.8481695652008 and batch: 50, loss is 5.051588163375855 and perplexity is 156.2704505193467
At time: 106.19167566299438 and batch: 100, loss is 5.02766227722168 and perplexity is 152.57591526830055
At time: 106.52022552490234 and batch: 150, loss is 5.044079303741455 and perplexity is 155.10143213160683
At time: 106.85870957374573 and batch: 200, loss is 5.046489973068237 and perplexity is 155.47578143148496
At time: 107.19548916816711 and batch: 250, loss is 5.035541200637818 and perplexity is 153.78279744261926
At time: 107.52537631988525 and batch: 300, loss is 4.938021240234375 and perplexity is 139.4939512678677
At time: 107.87063407897949 and batch: 350, loss is 5.00378113746643 and perplexity is 148.9753919289195
At time: 108.20192193984985 and batch: 400, loss is 4.929956836700439 and perplexity is 138.37353955252001
At time: 108.5298228263855 and batch: 450, loss is 4.935582399368286 and perplexity is 139.15416223305596
At time: 108.85909581184387 and batch: 500, loss is 4.9229515933990475 and perplexity is 137.40758655956247
At time: 109.1904456615448 and batch: 550, loss is 4.981577606201172 and perplexity is 145.70406406678106
At time: 109.52153611183167 and batch: 600, loss is 5.016277837753296 and perplexity is 150.84877391854255
At time: 109.85048937797546 and batch: 650, loss is 4.987711849212647 and perplexity is 146.60059516203813
At time: 110.18646883964539 and batch: 700, loss is 4.9748849773406985 and perplexity is 144.73217671087372
At time: 110.51482129096985 and batch: 750, loss is 4.90740777015686 and perplexity is 135.28826120207583
At time: 110.84439373016357 and batch: 800, loss is 4.953281898498535 and perplexity is 141.6390469437408
At time: 111.18537592887878 and batch: 850, loss is 4.922345685958862 and perplexity is 137.3243554982341
At time: 111.51654529571533 and batch: 900, loss is 5.027770566940307 and perplexity is 152.5924385658698
At time: 111.84383869171143 and batch: 950, loss is 4.978450984954834 and perplexity is 145.2492140867398
At time: 112.17059659957886 and batch: 1000, loss is 4.945997552871704 and perplexity is 140.61104786744573
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.155482966725419 and perplexity of 173.37952308167615
Finished 15 epochs...
Completing Train Step...
At time: 113.32476592063904 and batch: 50, loss is 5.0166583919525145 and perplexity is 150.90619097736274
At time: 113.68009567260742 and batch: 100, loss is 4.993190565109253 and perplexity is 147.4059824025802
At time: 114.0320029258728 and batch: 150, loss is 5.01665997505188 and perplexity is 150.906429877047
At time: 114.37039256095886 and batch: 200, loss is 5.014276657104492 and perplexity is 150.5472001237455
At time: 114.69701671600342 and batch: 250, loss is 5.004668340682984 and perplexity is 149.10762202463573
At time: 115.03674125671387 and batch: 300, loss is 4.908849658966065 and perplexity is 135.4834725346167
At time: 115.37671995162964 and batch: 350, loss is 4.970081033706665 and perplexity is 144.03855887634901
At time: 115.70685124397278 and batch: 400, loss is 4.895847749710083 and perplexity is 133.73333096497117
At time: 116.04759669303894 and batch: 450, loss is 4.9089293289184575 and perplexity is 135.49426692641214
At time: 116.38059210777283 and batch: 500, loss is 4.894576711654663 and perplexity is 133.56345879190442
At time: 116.70730471611023 and batch: 550, loss is 4.950689897537232 and perplexity is 141.27239378585242
At time: 117.04217481613159 and batch: 600, loss is 4.993613319396973 and perplexity is 147.468312087819
At time: 117.37618708610535 and batch: 650, loss is 4.965572414398193 and perplexity is 143.39060563349884
At time: 117.703289270401 and batch: 700, loss is 4.953752212524414 and perplexity is 141.70567744153124
At time: 118.02995157241821 and batch: 750, loss is 4.880745725631714 and perplexity is 131.72886085525727
At time: 118.36313438415527 and batch: 800, loss is 4.928873338699341 and perplexity is 138.22369329273096
At time: 118.6974024772644 and batch: 850, loss is 4.895173292160035 and perplexity is 133.64316392054866
At time: 119.02375745773315 and batch: 900, loss is 4.994157133102417 and perplexity is 147.54852918665298
At time: 119.35872220993042 and batch: 950, loss is 4.944254283905029 and perplexity is 140.36613852477814
At time: 119.69333791732788 and batch: 1000, loss is 4.928160572052002 and perplexity is 138.12520715726578
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.14564663026391 and perplexity of 171.68246386285946
Finished 16 epochs...
Completing Train Step...
At time: 120.8197009563446 and batch: 50, loss is 4.99882511138916 and perplexity is 148.23889256417976
At time: 121.15064525604248 and batch: 100, loss is 4.972986965179444 and perplexity is 144.4577338097225
At time: 121.48067426681519 and batch: 150, loss is 4.99509861946106 and perplexity is 147.68750952790182
At time: 121.80832839012146 and batch: 200, loss is 4.995528736114502 and perplexity is 147.75104604833342
At time: 122.15002417564392 and batch: 250, loss is 4.986917095184326 and perplexity is 146.48413003517356
At time: 122.47594213485718 and batch: 300, loss is 4.891594696044922 and perplexity is 133.165763734037
At time: 122.80705976486206 and batch: 350, loss is 4.952967567443848 and perplexity is 141.59453238925005
At time: 123.13479351997375 and batch: 400, loss is 4.880245113372803 and perplexity is 131.66293227636504
At time: 123.47575879096985 and batch: 450, loss is 4.894583539962769 and perplexity is 133.56437080746647
At time: 123.80675220489502 and batch: 500, loss is 4.88075966835022 and perplexity is 131.73069752648735
At time: 124.14191961288452 and batch: 550, loss is 4.936367197036743 and perplexity is 139.26341295938283
At time: 124.47720074653625 and batch: 600, loss is 4.981309404373169 and perplexity is 145.66499121039092
At time: 124.80540657043457 and batch: 650, loss is 4.948879375457763 and perplexity is 141.01684840276923
At time: 125.14062333106995 and batch: 700, loss is 4.936260957717895 and perplexity is 139.2486184951404
At time: 125.47212934494019 and batch: 750, loss is 4.869184226989746 and perplexity is 130.21464796134106
At time: 125.79776740074158 and batch: 800, loss is 4.910134906768799 and perplexity is 135.6577143179479
At time: 126.14105367660522 and batch: 850, loss is 4.879599390029907 and perplexity is 131.57794189069114
At time: 126.48002910614014 and batch: 900, loss is 4.986688270568847 and perplexity is 146.45061469515812
At time: 126.80998420715332 and batch: 950, loss is 4.939747447967529 and perplexity is 139.73495475647243
At time: 127.14294004440308 and batch: 1000, loss is 4.915417051315307 and perplexity is 136.37617380677918
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.136100582960175 and perplexity of 170.0513725652997
Finished 17 epochs...
Completing Train Step...
At time: 128.28432631492615 and batch: 50, loss is 4.988159437179565 and perplexity is 146.6662265111811
At time: 128.62491035461426 and batch: 100, loss is 4.967186164855957 and perplexity is 143.62218909768237
At time: 128.95660305023193 and batch: 150, loss is 4.9832953929901125 and perplexity is 145.9545676774278
At time: 129.2874972820282 and batch: 200, loss is 4.9818196201324465 and perplexity is 145.73933074747046
At time: 129.6138265132904 and batch: 250, loss is 4.97463680267334 and perplexity is 144.69626230776925
At time: 129.96215748786926 and batch: 300, loss is 4.8820719718933105 and perplexity is 131.9036816666099
At time: 130.3022665977478 and batch: 350, loss is 4.944237833023071 and perplexity is 140.363829396996
At time: 130.63229203224182 and batch: 400, loss is 4.869951581954956 and perplexity is 130.314607165188
At time: 130.9682855606079 and batch: 450, loss is 4.883168916702271 and perplexity is 132.04845211357528
At time: 131.29835391044617 and batch: 500, loss is 4.870713415145874 and perplexity is 130.4139229844282
At time: 131.62618851661682 and batch: 550, loss is 4.925200204849244 and perplexity is 137.7169104763597
At time: 131.95662212371826 and batch: 600, loss is 4.970376100540161 and perplexity is 144.08106614875265
At time: 132.30381870269775 and batch: 650, loss is 4.941009569168091 and perplexity is 139.91142854756043
At time: 132.6286494731903 and batch: 700, loss is 4.928045053482055 and perplexity is 138.10925205243453
At time: 132.95468473434448 and batch: 750, loss is 4.857972030639648 and perplexity is 128.7628101158203
At time: 133.2800533771515 and batch: 800, loss is 4.898253679275513 and perplexity is 134.05547130791848
At time: 133.61122965812683 and batch: 850, loss is 4.868306303024292 and perplexity is 130.10037956806255
At time: 133.94286251068115 and batch: 900, loss is 4.975265979766846 and perplexity is 144.78733052754393
At time: 134.2828710079193 and batch: 950, loss is 4.927721109390259 and perplexity is 138.06451962200416
At time: 134.61110615730286 and batch: 1000, loss is 4.9044944190979 and perplexity is 134.8946925831857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.130599231254764 and perplexity of 169.1184287358445
Finished 18 epochs...
Completing Train Step...
At time: 135.73397159576416 and batch: 50, loss is 4.977264060974121 and perplexity is 145.07691658360724
At time: 136.0730857849121 and batch: 100, loss is 4.954245080947876 and perplexity is 141.7755369097163
At time: 136.3983178138733 and batch: 150, loss is 4.97279146194458 and perplexity is 144.42949461597848
At time: 136.74651956558228 and batch: 200, loss is 4.971023483276367 and perplexity is 144.17437194261285
At time: 137.0741832256317 and batch: 250, loss is 4.963032236099243 and perplexity is 143.02683015169293
At time: 137.40227150917053 and batch: 300, loss is 4.8691327762603756 and perplexity is 130.20794849507644
At time: 137.73610186576843 and batch: 350, loss is 4.932736139297486 and perplexity is 138.75865642061302
At time: 138.06910800933838 and batch: 400, loss is 4.8566098976135255 and perplexity is 128.58753743907607
At time: 138.4022183418274 and batch: 450, loss is 4.872248859405517 and perplexity is 130.61432010378428
At time: 138.73607087135315 and batch: 500, loss is 4.860447244644165 and perplexity is 129.08192039687424
At time: 139.07546377182007 and batch: 550, loss is 4.913764410018921 and perplexity is 136.15097904443965
At time: 139.40351915359497 and batch: 600, loss is 4.959366655349731 and perplexity is 142.50351347215945
At time: 139.7317578792572 and batch: 650, loss is 4.934567260742187 and perplexity is 139.01297314336472
At time: 140.06118512153625 and batch: 700, loss is 4.923785066604614 and perplexity is 137.52215984144905
At time: 140.40166306495667 and batch: 750, loss is 4.85049485206604 and perplexity is 127.803618084883
At time: 140.75261735916138 and batch: 800, loss is 4.891125793457031 and perplexity is 133.10333660006987
At time: 141.09186148643494 and batch: 850, loss is 4.862982234954834 and perplexity is 129.40955691661017
At time: 141.4182653427124 and batch: 900, loss is 4.962850294113159 and perplexity is 143.0008099333087
At time: 141.74408054351807 and batch: 950, loss is 4.913055028915405 and perplexity is 136.05443036163476
At time: 142.07293438911438 and batch: 1000, loss is 4.890770206451416 and perplexity is 133.0560151971074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.1324522437118905 and perplexity of 169.43209781762707
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 143.23417735099792 and batch: 50, loss is 4.956376686096191 and perplexity is 142.07806869870547
At time: 143.56677746772766 and batch: 100, loss is 4.900135688781738 and perplexity is 134.3080025378312
At time: 143.89557456970215 and batch: 150, loss is 4.907744569778442 and perplexity is 135.333833911258
At time: 144.24521160125732 and batch: 200, loss is 4.905201263427735 and perplexity is 134.9900758383481
At time: 144.5738606452942 and batch: 250, loss is 4.884283504486084 and perplexity is 132.1957137579421
At time: 144.9013319015503 and batch: 300, loss is 4.7859579277038575 and perplexity is 119.81608327379797
At time: 145.24754428863525 and batch: 350, loss is 4.851823148727417 and perplexity is 127.97349200059779
At time: 145.5763463973999 and batch: 400, loss is 4.768997211456298 and perplexity is 117.80105316483983
At time: 145.91596364974976 and batch: 450, loss is 4.78505955696106 and perplexity is 119.70849234557535
At time: 146.25024104118347 and batch: 500, loss is 4.766336603164673 and perplexity is 117.48804728369169
At time: 146.5781557559967 and batch: 550, loss is 4.822481460571289 and perplexity is 124.273087247311
At time: 146.90532970428467 and batch: 600, loss is 4.85460976600647 and perplexity is 128.3306024786946
At time: 147.2423756122589 and batch: 650, loss is 4.82206841468811 and perplexity is 124.22176735969732
At time: 147.57397627830505 and batch: 700, loss is 4.802816352844238 and perplexity is 121.85311608223611
At time: 147.9036729335785 and batch: 750, loss is 4.737545690536499 and perplexity is 114.15368910916446
At time: 148.24584817886353 and batch: 800, loss is 4.766331691741943 and perplexity is 117.48747025164279
At time: 148.57222747802734 and batch: 850, loss is 4.733834247589112 and perplexity is 113.73079945564828
At time: 148.89902639389038 and batch: 900, loss is 4.834336795806885 and perplexity is 125.75515419912621
At time: 149.24159717559814 and batch: 950, loss is 4.776381893157959 and perplexity is 118.67419642113512
At time: 149.60704040527344 and batch: 1000, loss is 4.757162437438965 and perplexity is 116.41512159142776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.052940554735137 and perplexity of 156.48193229717205
Finished 20 epochs...
Completing Train Step...
At time: 150.72686505317688 and batch: 50, loss is 4.892478628158569 and perplexity is 133.2835252679731
At time: 151.07013773918152 and batch: 100, loss is 4.853710241317749 and perplexity is 128.21521783689388
At time: 151.3985152244568 and batch: 150, loss is 4.871155443191529 and perplexity is 130.4715823385181
At time: 151.72619819641113 and batch: 200, loss is 4.872339744567871 and perplexity is 130.62619154693343
At time: 152.06837606430054 and batch: 250, loss is 4.85692476272583 and perplexity is 128.62803154324862
At time: 152.39474654197693 and batch: 300, loss is 4.765794715881348 and perplexity is 117.42439925151304
At time: 152.7194788455963 and batch: 350, loss is 4.830878267288208 and perplexity is 125.32097765072588
At time: 153.05566143989563 and batch: 400, loss is 4.749254312515259 and perplexity is 115.4981269001074
At time: 153.38787388801575 and batch: 450, loss is 4.768891220092773 and perplexity is 117.7885679322653
At time: 153.72869277000427 and batch: 500, loss is 4.751717739105224 and perplexity is 115.78299879344135
At time: 154.08274269104004 and batch: 550, loss is 4.806802711486816 and perplexity is 122.33983577975134
At time: 154.40885257720947 and batch: 600, loss is 4.840403985977173 and perplexity is 126.52045389634019
At time: 154.73659777641296 and batch: 650, loss is 4.810567893981934 and perplexity is 122.80133585868579
At time: 155.06916451454163 and batch: 700, loss is 4.79501410484314 and perplexity is 120.90608713242109
At time: 155.40788102149963 and batch: 750, loss is 4.732611360549927 and perplexity is 113.59180453990004
At time: 155.73272585868835 and batch: 800, loss is 4.761688385009766 and perplexity is 116.94320446445109
At time: 156.05924582481384 and batch: 850, loss is 4.732784776687622 and perplexity is 113.61150490004799
At time: 156.40239119529724 and batch: 900, loss is 4.836728191375732 and perplexity is 126.05624438701943
At time: 156.73133873939514 and batch: 950, loss is 4.7826111793518065 and perplexity is 119.41575926007863
At time: 157.0591459274292 and batch: 1000, loss is 4.762224760055542 and perplexity is 117.0059467062811
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0488061672303735 and perplexity of 155.836310895731
Finished 21 epochs...
Completing Train Step...
At time: 158.19019031524658 and batch: 50, loss is 4.880388259887695 and perplexity is 131.68178071527288
At time: 158.54986810684204 and batch: 100, loss is 4.841473379135132 and perplexity is 126.65582637437866
At time: 158.88273096084595 and batch: 150, loss is 4.8595226287841795 and perplexity is 128.96262436603794
At time: 159.21530270576477 and batch: 200, loss is 4.860996723175049 and perplexity is 129.1528676310635
At time: 159.55191898345947 and batch: 250, loss is 4.847084398269653 and perplexity is 127.36849216057645
At time: 159.89224410057068 and batch: 300, loss is 4.757075824737549 and perplexity is 116.40503899990756
At time: 160.22007942199707 and batch: 350, loss is 4.8221937084198 and perplexity is 124.2373325435765
At time: 160.54722261428833 and batch: 400, loss is 4.740949573516846 and perplexity is 114.54291697590949
At time: 160.88730335235596 and batch: 450, loss is 4.7631042957305905 and perplexity is 117.10890288077923
At time: 161.22859692573547 and batch: 500, loss is 4.746480102539063 and perplexity is 115.17815488422356
At time: 161.55669951438904 and batch: 550, loss is 4.801102952957153 and perplexity is 121.64451172927127
At time: 161.89120888710022 and batch: 600, loss is 4.835615968704223 and perplexity is 125.91611971349128
At time: 162.24804377555847 and batch: 650, loss is 4.805661420822144 and perplexity is 122.20029011348849
At time: 162.57574939727783 and batch: 700, loss is 4.790440864562989 and perplexity is 120.35441696623735
At time: 162.90632128715515 and batch: 750, loss is 4.730032691955566 and perplexity is 113.29926626269378
At time: 163.24729871749878 and batch: 800, loss is 4.759628601074219 and perplexity is 116.70257463832586
At time: 163.57821226119995 and batch: 850, loss is 4.732332344055176 and perplexity is 113.56011497392625
At time: 163.9210250377655 and batch: 900, loss is 4.836747436523438 and perplexity is 126.05867038140612
At time: 164.25320267677307 and batch: 950, loss is 4.783558664321899 and perplexity is 119.52895751552414
At time: 164.58904433250427 and batch: 1000, loss is 4.763032646179199 and perplexity is 117.10051238101522
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.046716829625572 and perplexity of 155.51105613300686
Finished 22 epochs...
Completing Train Step...
At time: 165.7563920021057 and batch: 50, loss is 4.872980728149414 and perplexity is 130.70994763130432
At time: 166.09041142463684 and batch: 100, loss is 4.834495172500611 and perplexity is 125.77507246191993
At time: 166.4194815158844 and batch: 150, loss is 4.85259524345398 and perplexity is 128.0723378131662
At time: 166.74774599075317 and batch: 200, loss is 4.853945550918579 and perplexity is 128.24539165857905
At time: 167.1120457649231 and batch: 250, loss is 4.8409185886383055 and perplexity is 126.58557841379076
At time: 167.4415316581726 and batch: 300, loss is 4.751485805511475 and perplexity is 115.75614794036392
At time: 167.78182768821716 and batch: 350, loss is 4.816956481933594 and perplexity is 123.58837434885731
At time: 168.12588262557983 and batch: 400, loss is 4.736000432968139 and perplexity is 113.9774284762241
At time: 168.4608519077301 and batch: 450, loss is 4.758926820755005 and perplexity is 116.62070379927495
At time: 168.79601287841797 and batch: 500, loss is 4.7422434425354005 and perplexity is 114.691216426823
At time: 169.1294276714325 and batch: 550, loss is 4.79726809501648 and perplexity is 121.17891562557777
At time: 169.45680856704712 and batch: 600, loss is 4.831830091476441 and perplexity is 125.4403179750406
At time: 169.78874015808105 and batch: 650, loss is 4.801760845184326 and perplexity is 121.72456703901169
At time: 170.11753845214844 and batch: 700, loss is 4.78743688583374 and perplexity is 119.99341734675708
At time: 170.44735407829285 and batch: 750, loss is 4.727566070556641 and perplexity is 113.02014425372779
At time: 170.7790825366974 and batch: 800, loss is 4.756808471679688 and perplexity is 116.37392191658952
At time: 171.10688257217407 and batch: 850, loss is 4.729983892440796 and perplexity is 113.29373744837915
At time: 171.4320101737976 and batch: 900, loss is 4.835329790115356 and perplexity is 125.88009037169503
At time: 171.75871682167053 and batch: 950, loss is 4.781637554168701 and perplexity is 119.29954965108432
At time: 172.0864133834839 and batch: 1000, loss is 4.761546640396118 and perplexity is 116.92662956984435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.045201371355755 and perplexity of 155.27556410118515
Finished 23 epochs...
Completing Train Step...
At time: 173.22627449035645 and batch: 50, loss is 4.8683477306365965 and perplexity is 130.10576942779164
At time: 173.57766485214233 and batch: 100, loss is 4.829048738479615 and perplexity is 125.0919089196086
At time: 173.91658902168274 and batch: 150, loss is 4.847299633026123 and perplexity is 127.39590923741657
At time: 174.24784922599792 and batch: 200, loss is 4.849013032913208 and perplexity is 127.61437648134779
At time: 174.59118390083313 and batch: 250, loss is 4.836258220672607 and perplexity is 125.99701556420752
At time: 174.94652247428894 and batch: 300, loss is 4.747235851287842 and perplexity is 115.26523353130868
At time: 175.2794234752655 and batch: 350, loss is 4.812470178604126 and perplexity is 123.03516128221648
At time: 175.6122443675995 and batch: 400, loss is 4.732189044952393 and perplexity is 113.5438430772405
At time: 175.99169158935547 and batch: 450, loss is 4.756163167953491 and perplexity is 116.29884961596741
At time: 176.33262014389038 and batch: 500, loss is 4.738998584747314 and perplexity is 114.31966288518733
At time: 176.66145873069763 and batch: 550, loss is 4.794198713302612 and perplexity is 120.80754151386687
At time: 176.99898266792297 and batch: 600, loss is 4.828483982086182 and perplexity is 125.02128240954326
At time: 177.33392548561096 and batch: 650, loss is 4.799144639968872 and perplexity is 121.406526802541
At time: 177.66211915016174 and batch: 700, loss is 4.78461992263794 and perplexity is 119.65587595040536
At time: 178.0023078918457 and batch: 750, loss is 4.725347843170166 and perplexity is 112.76971772869486
At time: 178.33687734603882 and batch: 800, loss is 4.754267225265503 and perplexity is 116.07856255422551
At time: 178.67492628097534 and batch: 850, loss is 4.7281484794616695 and perplexity is 113.08598736413452
At time: 179.00418782234192 and batch: 900, loss is 4.834173469543457 and perplexity is 125.73461675688162
At time: 179.33891582489014 and batch: 950, loss is 4.780457696914673 and perplexity is 119.15887621563166
At time: 179.6803436279297 and batch: 1000, loss is 4.759615421295166 and perplexity is 116.70103653431316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.043362129025343 and perplexity of 154.99023718391786
Finished 24 epochs...
Completing Train Step...
At time: 180.80929923057556 and batch: 50, loss is 4.8640907192230225 and perplexity is 129.55308490916985
At time: 181.16287350654602 and batch: 100, loss is 4.824692249298096 and perplexity is 124.54813270932046
At time: 181.49085307121277 and batch: 150, loss is 4.842804079055786 and perplexity is 126.82447946092283
At time: 181.8230745792389 and batch: 200, loss is 4.8447576427459715 and perplexity is 127.07248132386856
At time: 182.18330430984497 and batch: 250, loss is 4.833089838027954 and perplexity is 125.5984405593093
At time: 182.52310585975647 and batch: 300, loss is 4.743502626419067 and perplexity is 114.8357247203092
At time: 182.85046291351318 and batch: 350, loss is 4.8093420600891115 and perplexity is 122.65089404628132
At time: 183.19091820716858 and batch: 400, loss is 4.728367757797241 and perplexity is 113.11078739017509
At time: 183.5276243686676 and batch: 450, loss is 4.753740730285645 and perplexity is 116.01746385926106
At time: 183.8588809967041 and batch: 500, loss is 4.73579628944397 and perplexity is 113.95416309711823
At time: 184.20838141441345 and batch: 550, loss is 4.79100302696228 and perplexity is 120.42209471520499
At time: 184.5618646144867 and batch: 600, loss is 4.825855979919433 and perplexity is 124.69315755374399
At time: 184.89918565750122 and batch: 650, loss is 4.796312379837036 and perplexity is 121.06315842075571
At time: 185.23286509513855 and batch: 700, loss is 4.781270141601563 and perplexity is 119.25572554854455
At time: 185.55953001976013 and batch: 750, loss is 4.722128496170044 and perplexity is 112.40725663329191
At time: 185.8984797000885 and batch: 800, loss is 4.750884742736816 and perplexity is 115.6865921346959
At time: 186.22325992584229 and batch: 850, loss is 4.725867557525635 and perplexity is 112.8283410022195
At time: 186.56324529647827 and batch: 900, loss is 4.832070550918579 and perplexity is 125.47048491073944
At time: 186.8895778656006 and batch: 950, loss is 4.778132181167603 and perplexity is 118.8820923299692
At time: 187.21779465675354 and batch: 1000, loss is 4.7566631889343265 and perplexity is 116.35701602182218
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.039440248070694 and perplexity of 154.38357433188972
Finished 25 epochs...
Completing Train Step...
At time: 188.374853849411 and batch: 50, loss is 4.859841651916504 and perplexity is 129.00377298974843
At time: 188.70270800590515 and batch: 100, loss is 4.819490756988525 and perplexity is 123.9019784947864
At time: 189.03017044067383 and batch: 150, loss is 4.838787040710449 and perplexity is 126.31604255278228
At time: 189.37038612365723 and batch: 200, loss is 4.840665416717529 and perplexity is 126.55353455623482
At time: 189.69876337051392 and batch: 250, loss is 4.829591817855835 and perplexity is 125.15986220581891
At time: 190.02958011627197 and batch: 300, loss is 4.739928560256958 and perplexity is 114.4260268222038
At time: 190.36943459510803 and batch: 350, loss is 4.805540761947632 and perplexity is 122.18554645351293
At time: 190.70552372932434 and batch: 400, loss is 4.724097051620483 and perplexity is 112.62875449488841
At time: 191.03211092948914 and batch: 450, loss is 4.749282464981079 and perplexity is 115.50137850294742
At time: 191.37508630752563 and batch: 500, loss is 4.732888259887695 and perplexity is 113.62326239048103
At time: 191.73233151435852 and batch: 550, loss is 4.786787452697754 and perplexity is 119.9155149443707
At time: 192.06144976615906 and batch: 600, loss is 4.820921697616577 and perplexity is 124.07940178053038
At time: 192.40332317352295 and batch: 650, loss is 4.789659595489502 and perplexity is 120.260424503887
At time: 192.73418402671814 and batch: 700, loss is 4.777368211746216 and perplexity is 118.79130473056918
At time: 193.1196494102478 and batch: 750, loss is 4.717345733642578 and perplexity is 111.87092301898464
At time: 193.44683575630188 and batch: 800, loss is 4.74482569694519 and perplexity is 114.98776103823968
At time: 193.77913904190063 and batch: 850, loss is 4.7204211235046385 and perplexity is 112.21549930311792
At time: 194.11362886428833 and batch: 900, loss is 4.828848962783813 and perplexity is 125.06692109252846
At time: 194.4400897026062 and batch: 950, loss is 4.776312952041626 and perplexity is 118.66601517156941
At time: 194.77389001846313 and batch: 1000, loss is 4.753844966888428 and perplexity is 116.02955775585964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.037500893197408 and perplexity of 154.08445993288748
Finished 26 epochs...
Completing Train Step...
At time: 195.89449191093445 and batch: 50, loss is 4.85440731048584 and perplexity is 128.30462386960215
At time: 196.24669742584229 and batch: 100, loss is 4.8143735599517825 and perplexity is 123.2695671243925
At time: 196.59699034690857 and batch: 150, loss is 4.832665643692017 and perplexity is 125.54517371076794
At time: 196.92764592170715 and batch: 200, loss is 4.834486122131348 and perplexity is 125.77393415622113
At time: 197.26234006881714 and batch: 250, loss is 4.823657579421997 and perplexity is 124.41933315228313
At time: 197.60255789756775 and batch: 300, loss is 4.730675392150879 and perplexity is 113.37210712816147
At time: 197.93512988090515 and batch: 350, loss is 4.796931285858154 and perplexity is 121.13810832952035
At time: 198.27137994766235 and batch: 400, loss is 4.716616821289063 and perplexity is 111.7894086332275
At time: 198.60185885429382 and batch: 450, loss is 4.741948041915894 and perplexity is 114.65734157401111
At time: 198.935049533844 and batch: 500, loss is 4.72600832939148 and perplexity is 112.84422517629876
At time: 199.26770997047424 and batch: 550, loss is 4.7797893524169925 and perplexity is 119.07926364363888
At time: 199.64349246025085 and batch: 600, loss is 4.812940158843994 and perplexity is 123.09299896704795
At time: 199.98320603370667 and batch: 650, loss is 4.782806425094605 and perplexity is 119.43907695496344
At time: 200.32471585273743 and batch: 700, loss is 4.7732046031951905 and perplexity is 118.2977324722753
At time: 200.66684794425964 and batch: 750, loss is 4.713208703994751 and perplexity is 111.40906571113756
At time: 201.02034187316895 and batch: 800, loss is 4.739585981369019 and perplexity is 114.38683359495316
At time: 201.36687517166138 and batch: 850, loss is 4.713398399353028 and perplexity is 111.43020149838986
At time: 201.69696283340454 and batch: 900, loss is 4.821622314453125 and perplexity is 124.1663643586526
At time: 202.09073686599731 and batch: 950, loss is 4.768017997741699 and perplexity is 117.68575721688607
At time: 202.42801427841187 and batch: 1000, loss is 4.746605100631714 and perplexity is 115.19255283373859
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.029939604968559 and perplexity of 152.9237765804152
Finished 27 epochs...
Completing Train Step...
At time: 203.5879771709442 and batch: 50, loss is 4.8475354957580565 and perplexity is 127.42596072848059
At time: 203.95414805412292 and batch: 100, loss is 4.807234010696411 and perplexity is 122.39261223462861
At time: 204.30545449256897 and batch: 150, loss is 4.825498304367065 and perplexity is 124.64856583489868
At time: 204.6494927406311 and batch: 200, loss is 4.828626575469971 and perplexity is 125.03911088832909
At time: 204.98928236961365 and batch: 250, loss is 4.818194351196289 and perplexity is 123.74145532626115
At time: 205.3464925289154 and batch: 300, loss is 4.726724624633789 and perplexity is 112.9250839138219
At time: 205.6865680217743 and batch: 350, loss is 4.79281268119812 and perplexity is 120.64021437004794
At time: 206.0025670528412 and batch: 400, loss is 4.712129726409912 and perplexity is 111.28892265397596
At time: 206.34738111495972 and batch: 450, loss is 4.737917470932007 and perplexity is 114.19613710302716
At time: 206.69296836853027 and batch: 500, loss is 4.721858005523682 and perplexity is 112.37685563356776
At time: 207.02846598625183 and batch: 550, loss is 4.776367225646973 and perplexity is 118.67245577882082
At time: 207.35459685325623 and batch: 600, loss is 4.810994596481323 and perplexity is 122.85374667674334
At time: 207.69551157951355 and batch: 650, loss is 4.781463422775269 and perplexity is 119.27877766284782
At time: 208.04225611686707 and batch: 700, loss is 4.770409278869629 and perplexity is 117.96751369235385
At time: 208.3806529045105 and batch: 750, loss is 4.710591230392456 and perplexity is 111.11783673093778
At time: 208.72217798233032 and batch: 800, loss is 4.73512749671936 and perplexity is 113.8779768611398
At time: 209.06403970718384 and batch: 850, loss is 4.7093698596954345 and perplexity is 110.98220350732147
At time: 209.4083263874054 and batch: 900, loss is 4.817009582519531 and perplexity is 123.5949371381927
At time: 209.7428798675537 and batch: 950, loss is 4.76390905380249 and perplexity is 117.2031851477832
At time: 210.0738434791565 and batch: 1000, loss is 4.7421427249908445 and perplexity is 114.67966559081803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.029897922422828 and perplexity of 152.91740246095014
Finished 28 epochs...
Completing Train Step...
At time: 211.24574160575867 and batch: 50, loss is 4.843802213668823 and perplexity is 126.95113056056145
At time: 211.57205533981323 and batch: 100, loss is 4.8027108287811275 and perplexity is 121.84025832473776
At time: 211.9200258255005 and batch: 150, loss is 4.821585369110108 and perplexity is 124.16177707447002
At time: 212.2554371356964 and batch: 200, loss is 4.825030508041382 and perplexity is 124.59026933030785
At time: 212.59076738357544 and batch: 250, loss is 4.814147891998291 and perplexity is 123.24175227203025
At time: 212.94430565834045 and batch: 300, loss is 4.721880264282227 and perplexity is 112.37935703070222
At time: 213.28644394874573 and batch: 350, loss is 4.7907083702087405 and perplexity is 120.38661675888815
At time: 213.62449431419373 and batch: 400, loss is 4.710158224105835 and perplexity is 111.06973242456269
At time: 213.97565507888794 and batch: 450, loss is 4.7356099605560305 and perplexity is 113.93293212266563
At time: 214.30997562408447 and batch: 500, loss is 4.719397048950196 and perplexity is 112.10064108739348
At time: 214.6569185256958 and batch: 550, loss is 4.7719051551818845 and perplexity is 118.14411055230167
At time: 215.0001938343048 and batch: 600, loss is 4.805178384780884 and perplexity is 122.14127722293522
At time: 215.34326887130737 and batch: 650, loss is 4.77473973274231 and perplexity is 118.47947427966854
At time: 215.69235277175903 and batch: 700, loss is 4.765058269500733 and perplexity is 117.33795431262654
At time: 216.0280921459198 and batch: 750, loss is 4.706412487030029 and perplexity is 110.65447262236545
At time: 216.3736126422882 and batch: 800, loss is 4.731329469680786 and perplexity is 113.44628553252149
At time: 216.7236361503601 and batch: 850, loss is 4.7062266826629635 and perplexity is 110.63391444807532
At time: 217.04388189315796 and batch: 900, loss is 4.814197254180908 and perplexity is 123.24783590406138
At time: 217.37493634223938 and batch: 950, loss is 4.760053720474243 and perplexity is 116.75219771395608
At time: 217.7252733707428 and batch: 1000, loss is 4.738873920440674 and perplexity is 114.30541219197323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.028228759765625 and perplexity of 152.6623713465441
Finished 29 epochs...
Completing Train Step...
At time: 218.86497378349304 and batch: 50, loss is 4.837916707992553 and perplexity is 126.20615339515464
At time: 219.247496843338 and batch: 100, loss is 4.797373666763305 and perplexity is 121.1917093706959
At time: 219.6059172153473 and batch: 150, loss is 4.816085739135742 and perplexity is 123.48080750032055
At time: 219.96543049812317 and batch: 200, loss is 4.818813037872315 and perplexity is 123.81803620328489
At time: 220.31113934516907 and batch: 250, loss is 4.808947381973266 and perplexity is 122.60249597397065
At time: 220.65755414962769 and batch: 300, loss is 4.718097114562989 and perplexity is 111.95501228366821
At time: 221.0037236213684 and batch: 350, loss is 4.786507415771484 and perplexity is 119.88193887364282
At time: 221.33840250968933 and batch: 400, loss is 4.7085134887695315 and perplexity is 110.88720225890462
At time: 221.68773460388184 and batch: 450, loss is 4.735296401977539 and perplexity is 113.89721307472551
At time: 222.04613280296326 and batch: 500, loss is 4.718944034576416 and perplexity is 112.04986938669435
At time: 222.37734127044678 and batch: 550, loss is 4.769761943817139 and perplexity is 117.89117389706425
At time: 222.75105023384094 and batch: 600, loss is 4.803269233703613 and perplexity is 121.908313524154
At time: 223.07564401626587 and batch: 650, loss is 4.771798686981201 and perplexity is 118.13153263101604
At time: 223.41159844398499 and batch: 700, loss is 4.762729740142822 and perplexity is 117.06504730051682
At time: 223.77255773544312 and batch: 750, loss is 4.704216766357422 and perplexity is 110.41177285726624
At time: 224.11929845809937 and batch: 800, loss is 4.728327522277832 and perplexity is 113.10623641044974
At time: 224.45146083831787 and batch: 850, loss is 4.703323945999146 and perplexity is 110.31323897174077
At time: 224.7943046092987 and batch: 900, loss is 4.810533142089843 and perplexity is 122.79706835406584
At time: 225.12111401557922 and batch: 950, loss is 4.758452205657959 and perplexity is 116.56536698550609
At time: 225.461416721344 and batch: 1000, loss is 4.735517616271973 and perplexity is 113.92241155338287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.030208308522294 and perplexity of 152.96487326379804
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 226.6636517047882 and batch: 50, loss is 4.838126497268677 and perplexity is 126.23263287017558
At time: 227.0308063030243 and batch: 100, loss is 4.790820875167847 and perplexity is 120.40016161220085
At time: 227.36136841773987 and batch: 150, loss is 4.808474035263061 and perplexity is 122.54447621865192
At time: 227.69415402412415 and batch: 200, loss is 4.8101068782806395 and perplexity is 122.74473556253012
At time: 228.03703427314758 and batch: 250, loss is 4.79523922920227 and perplexity is 120.93330910185098
At time: 228.37233090400696 and batch: 300, loss is 4.702854156494141 and perplexity is 110.26142714108877
At time: 228.71589970588684 and batch: 350, loss is 4.768224658966065 and perplexity is 117.71008081284837
At time: 229.0896954536438 and batch: 400, loss is 4.688441972732544 and perplexity is 108.68371563430853
At time: 229.43474316596985 and batch: 450, loss is 4.716544551849365 and perplexity is 111.7813299672253
At time: 229.78561449050903 and batch: 500, loss is 4.693966293334961 and perplexity is 109.2857807930574
At time: 230.12954020500183 and batch: 550, loss is 4.746130590438843 and perplexity is 115.1379057595983
At time: 230.4713056087494 and batch: 600, loss is 4.778238458633423 and perplexity is 118.89472748887782
At time: 230.80858278274536 and batch: 650, loss is 4.746721811294556 and perplexity is 115.20599781750546
At time: 231.14604306221008 and batch: 700, loss is 4.737120332717896 and perplexity is 114.10514327038747
At time: 231.48338103294373 and batch: 750, loss is 4.6763090133667 and perplexity is 107.3730278681734
At time: 231.83293223381042 and batch: 800, loss is 4.69807315826416 and perplexity is 109.73552562196005
At time: 232.18703532218933 and batch: 850, loss is 4.669705352783203 and perplexity is 106.66630887073774
At time: 232.52145409584045 and batch: 900, loss is 4.77443941116333 and perplexity is 118.44389767935398
At time: 232.85333704948425 and batch: 950, loss is 4.720656881332397 and perplexity is 112.24195810428668
At time: 233.1818344593048 and batch: 1000, loss is 4.6996224498748775 and perplexity is 109.90566971858146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.009188396174733 and perplexity of 149.78312224959112
Finished 31 epochs...
Completing Train Step...
At time: 234.3830635547638 and batch: 50, loss is 4.823577928543091 and perplexity is 124.40942343770854
At time: 234.74417853355408 and batch: 100, loss is 4.781304454803466 and perplexity is 119.25981766453988
At time: 235.0866551399231 and batch: 150, loss is 4.800509977340698 and perplexity is 121.57240088204021
At time: 235.42301440238953 and batch: 200, loss is 4.801896934509277 and perplexity is 121.74113358040918
At time: 235.79800724983215 and batch: 250, loss is 4.789235973358155 and perplexity is 120.20949031572835
At time: 236.13991785049438 and batch: 300, loss is 4.6966799449920655 and perplexity is 109.58274708250019
At time: 236.51576733589172 and batch: 350, loss is 4.7616434669494625 and perplexity is 116.93795172051331
At time: 236.85426664352417 and batch: 400, loss is 4.683831491470337 and perplexity is 108.18378474643127
At time: 237.19967937469482 and batch: 450, loss is 4.711937770843506 and perplexity is 111.26756217599016
At time: 237.53853464126587 and batch: 500, loss is 4.689595642089844 and perplexity is 108.80917306095286
At time: 237.89064812660217 and batch: 550, loss is 4.742997579574585 and perplexity is 114.77774194314715
At time: 238.2243902683258 and batch: 600, loss is 4.775711593627929 and perplexity is 118.59467581733647
At time: 238.56448221206665 and batch: 650, loss is 4.744662408828735 and perplexity is 114.96898643620081
At time: 238.9018907546997 and batch: 700, loss is 4.7353472328186035 and perplexity is 113.90300271300583
At time: 239.2534818649292 and batch: 750, loss is 4.675551309585571 and perplexity is 107.29170173341724
At time: 239.612562417984 and batch: 800, loss is 4.698413314819336 and perplexity is 109.7728592296118
At time: 239.95800852775574 and batch: 850, loss is 4.670487470626831 and perplexity is 106.7497671270609
At time: 240.29697942733765 and batch: 900, loss is 4.776318712234497 and perplexity is 118.66669871267271
At time: 240.62127256393433 and batch: 950, loss is 4.723542137145996 and perplexity is 112.56627250645893
At time: 240.9672920703888 and batch: 1000, loss is 4.701680965423584 and perplexity is 110.13214527033661
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0082006687071265 and perplexity of 149.635250386134
Finished 32 epochs...
Completing Train Step...
At time: 242.11609506607056 and batch: 50, loss is 4.820202217102051 and perplexity is 123.9901611759829
At time: 242.46534609794617 and batch: 100, loss is 4.77868649482727 and perplexity is 118.94800856508448
At time: 242.80107617378235 and batch: 150, loss is 4.797604026794434 and perplexity is 121.21963031245011
At time: 243.1520495414734 and batch: 200, loss is 4.799188985824585 and perplexity is 121.41191079823925
At time: 243.49072122573853 and batch: 250, loss is 4.787053766250611 and perplexity is 119.94745432395419
At time: 243.83147287368774 and batch: 300, loss is 4.694202575683594 and perplexity is 109.31160614493258
At time: 244.18296217918396 and batch: 350, loss is 4.7588153743743895 and perplexity is 116.60770756813659
At time: 244.5160481929779 and batch: 400, loss is 4.681588220596313 and perplexity is 107.94137121433337
At time: 244.8620948791504 and batch: 450, loss is 4.709716186523438 and perplexity is 111.02064627832421
At time: 245.1946678161621 and batch: 500, loss is 4.687410049438476 and perplexity is 108.57162022333969
At time: 245.53597831726074 and batch: 550, loss is 4.741167058944702 and perplexity is 114.5678311003472
At time: 245.87196707725525 and batch: 600, loss is 4.7747047996521 and perplexity is 118.47533549779615
At time: 246.2036156654358 and batch: 650, loss is 4.743847770690918 and perplexity is 114.87536645356529
At time: 246.52904725074768 and batch: 700, loss is 4.73457426071167 and perplexity is 113.81499288796473
At time: 246.88905024528503 and batch: 750, loss is 4.6755469703674315 and perplexity is 107.29123617232895
At time: 247.26406407356262 and batch: 800, loss is 4.698582353591919 and perplexity is 109.79141666741796
At time: 247.59459352493286 and batch: 850, loss is 4.670685243606568 and perplexity is 106.77088143444269
At time: 247.93508076667786 and batch: 900, loss is 4.777149314880371 and perplexity is 118.76530453205883
At time: 248.3044364452362 and batch: 950, loss is 4.7244843482971195 and perplexity is 112.67238368537281
At time: 248.63478779792786 and batch: 1000, loss is 4.702289876937866 and perplexity is 110.19922642285468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.00766288943407 and perplexity of 149.5548012838265
Finished 33 epochs...
Completing Train Step...
At time: 249.78570699691772 and batch: 50, loss is 4.818131475448609 and perplexity is 123.73367523433056
At time: 250.14558720588684 and batch: 100, loss is 4.776876649856567 and perplexity is 118.73292580194574
At time: 250.48991799354553 and batch: 150, loss is 4.795682334899903 and perplexity is 120.98690721408508
At time: 250.8300986289978 and batch: 200, loss is 4.797278661727905 and perplexity is 121.18019609497513
At time: 251.17634081840515 and batch: 250, loss is 4.78546630859375 and perplexity is 119.75719387432551
At time: 251.52387690544128 and batch: 300, loss is 4.692549200057983 and perplexity is 109.13102232731516
At time: 251.86839294433594 and batch: 350, loss is 4.7569260406494145 and perplexity is 116.38760468301093
At time: 252.20680284500122 and batch: 400, loss is 4.680077838897705 and perplexity is 107.77846160154722
At time: 252.54350519180298 and batch: 450, loss is 4.708235845565796 and perplexity is 110.8564194543322
At time: 252.87004709243774 and batch: 500, loss is 4.68572548866272 and perplexity is 108.38887869337235
At time: 253.1890504360199 and batch: 550, loss is 4.739932746887207 and perplexity is 114.42650588267175
At time: 253.51664924621582 and batch: 600, loss is 4.774082679748535 and perplexity is 118.4016525556903
At time: 253.8453333377838 and batch: 650, loss is 4.743313293457032 and perplexity is 114.81398459052693
At time: 254.19749641418457 and batch: 700, loss is 4.734000215530395 and perplexity is 113.74967668875888
At time: 254.53740787506104 and batch: 750, loss is 4.6755029296875 and perplexity is 107.28651109738577
At time: 254.8811378479004 and batch: 800, loss is 4.6985294628143315 and perplexity is 109.78560986758238
At time: 255.228262424469 and batch: 850, loss is 4.670517892837524 and perplexity is 106.75301474036739
At time: 255.58897376060486 and batch: 900, loss is 4.777581710815429 and perplexity is 118.81666927111633
At time: 255.92015314102173 and batch: 950, loss is 4.724976749420166 and perplexity is 112.72787735508719
At time: 256.26721143722534 and batch: 1000, loss is 4.702407655715942 and perplexity is 110.21220631745075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.007288863019245 and perplexity of 149.49887429741955
Finished 34 epochs...
Completing Train Step...
At time: 257.44643998146057 and batch: 50, loss is 4.816608695983887 and perplexity is 123.54539952216531
At time: 257.78149819374084 and batch: 100, loss is 4.775490741729737 and perplexity is 118.5684868501147
At time: 258.13742780685425 and batch: 150, loss is 4.794336309432984 and perplexity is 120.82416530775747
At time: 258.46931648254395 and batch: 200, loss is 4.79572024345398 and perplexity is 120.99149373973356
At time: 258.8094024658203 and batch: 250, loss is 4.784271411895752 and perplexity is 119.61418185811227
At time: 259.14992594718933 and batch: 300, loss is 4.691228437423706 and perplexity is 108.98698129373118
At time: 259.5000548362732 and batch: 350, loss is 4.755462656021118 and perplexity is 116.21740941233772
At time: 259.8362946510315 and batch: 400, loss is 4.678843765258789 and perplexity is 107.64553707943587
At time: 260.17075657844543 and batch: 450, loss is 4.707064008712768 and perplexity is 110.72658990100533
At time: 260.48874139785767 and batch: 500, loss is 4.684426517486572 and perplexity is 108.24817606824617
At time: 260.82025384902954 and batch: 550, loss is 4.738964920043945 and perplexity is 114.31581441242628
At time: 261.1559386253357 and batch: 600, loss is 4.773275451660156 and perplexity is 118.30611398193494
At time: 261.4893407821655 and batch: 650, loss is 4.74273268699646 and perplexity is 114.74734219768372
At time: 261.82978343963623 and batch: 700, loss is 4.7334067821502686 and perplexity is 113.68219385889547
At time: 262.1646919250488 and batch: 750, loss is 4.675473957061768 and perplexity is 107.28340277048203
At time: 262.51334166526794 and batch: 800, loss is 4.698398246765136 and perplexity is 109.77120517868093
At time: 262.8443319797516 and batch: 850, loss is 4.67030460357666 and perplexity is 106.73024789680638
At time: 263.1859996318817 and batch: 900, loss is 4.777890253067016 and perplexity is 118.85333488993474
At time: 263.5305025577545 and batch: 950, loss is 4.725226125717163 and perplexity is 112.75599252119267
At time: 263.87138509750366 and batch: 1000, loss is 4.702249774932861 and perplexity is 110.19480730153356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.006764853872904 and perplexity of 149.42055604145335
Finished 35 epochs...
Completing Train Step...
At time: 264.9995663166046 and batch: 50, loss is 4.815272674560547 and perplexity is 123.38045043394197
At time: 265.33534145355225 and batch: 100, loss is 4.774342355728149 and perplexity is 118.4324026131585
At time: 265.65263867378235 and batch: 150, loss is 4.79334137916565 and perplexity is 120.7040134699289
At time: 265.9927270412445 and batch: 200, loss is 4.794407949447632 and perplexity is 120.83282146278985
At time: 266.3589611053467 and batch: 250, loss is 4.78324215888977 and perplexity is 119.49113193749001
At time: 266.70177483558655 and batch: 300, loss is 4.690089960098266 and perplexity is 108.8629726906458
At time: 267.0374734401703 and batch: 350, loss is 4.754040393829346 and perplexity is 116.05223527321468
At time: 267.3787944316864 and batch: 400, loss is 4.67783052444458 and perplexity is 107.53652146666047
At time: 267.7177040576935 and batch: 450, loss is 4.706024789810181 and perplexity is 110.61158050608039
At time: 268.04961252212524 and batch: 500, loss is 4.68322714805603 and perplexity is 108.11842434063968
At time: 268.3822913169861 and batch: 550, loss is 4.738215198516846 and perplexity is 114.23014150489786
At time: 268.71174693107605 and batch: 600, loss is 4.772521314620971 and perplexity is 118.21692859264864
At time: 269.0449597835541 and batch: 650, loss is 4.742196025848389 and perplexity is 114.68577827824124
At time: 269.37279987335205 and batch: 700, loss is 4.7330155849456785 and perplexity is 113.63773040000368
At time: 269.70309710502625 and batch: 750, loss is 4.675384407043457 and perplexity is 107.27379596995047
At time: 270.0253052711487 and batch: 800, loss is 4.698124132156372 and perplexity is 109.74111941138234
At time: 270.37790536880493 and batch: 850, loss is 4.669894828796386 and perplexity is 106.68652149252658
At time: 270.7217597961426 and batch: 900, loss is 4.7780398178100585 and perplexity is 118.87111248784504
At time: 271.05449891090393 and batch: 950, loss is 4.725193128585816 and perplexity is 112.75227195828154
At time: 271.3987600803375 and batch: 1000, loss is 4.701996355056763 and perplexity is 110.16688528526691
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.006263174661776 and perplexity of 149.34561365486263
Finished 36 epochs...
Completing Train Step...
At time: 272.5725910663605 and batch: 50, loss is 4.814150743484497 and perplexity is 123.24210369468781
At time: 272.9548363685608 and batch: 100, loss is 4.773378934860229 and perplexity is 118.31835731067649
At time: 273.2784764766693 and batch: 150, loss is 4.792357168197632 and perplexity is 120.58527369806588
At time: 273.6304395198822 and batch: 200, loss is 4.793194065093994 and perplexity is 120.68623337990032
At time: 273.9556701183319 and batch: 250, loss is 4.782324247360229 and perplexity is 119.3814999737261
At time: 274.2815752029419 and batch: 300, loss is 4.688988561630249 and perplexity is 108.74313718473601
At time: 274.6301393508911 and batch: 350, loss is 4.752787046432495 and perplexity is 115.90687262021655
At time: 274.9834270477295 and batch: 400, loss is 4.676886625289917 and perplexity is 107.43506572447995
At time: 275.3233480453491 and batch: 450, loss is 4.705013036727905 and perplexity is 110.49972549293678
At time: 275.6665794849396 and batch: 500, loss is 4.682000770568847 and perplexity is 107.98591161100111
At time: 276.00138235092163 and batch: 550, loss is 4.737343912124634 and perplexity is 114.1306576827679
At time: 276.35987305641174 and batch: 600, loss is 4.771605987548828 and perplexity is 118.10877094488507
At time: 276.69319105148315 and batch: 650, loss is 4.741543884277344 and perplexity is 114.61101129659656
At time: 277.03040289878845 and batch: 700, loss is 4.732480583190918 and perplexity is 113.57695027501912
At time: 277.35725235939026 and batch: 750, loss is 4.675196485519409 and perplexity is 107.25363880876297
At time: 277.6763129234314 and batch: 800, loss is 4.697751111984253 and perplexity is 109.7001913940936
At time: 278.0061733722687 and batch: 850, loss is 4.669374217987061 and perplexity is 106.63099379164302
At time: 278.35707569122314 and batch: 900, loss is 4.777998266220092 and perplexity is 118.86617330673626
At time: 278.7063591480255 and batch: 950, loss is 4.724902486801147 and perplexity is 112.71950619851424
At time: 279.0541994571686 and batch: 1000, loss is 4.701633768081665 and perplexity is 110.12694744848265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.005741026343369 and perplexity of 149.26765344899766
Finished 37 epochs...
Completing Train Step...
At time: 280.32973766326904 and batch: 50, loss is 4.813078699111938 and perplexity is 123.11005348544836
At time: 280.6793270111084 and batch: 100, loss is 4.772380237579346 and perplexity is 118.2002520744573
At time: 281.0083198547363 and batch: 150, loss is 4.791269607543946 and perplexity is 120.45420118655029
At time: 281.3424742221832 and batch: 200, loss is 4.791763458251953 and perplexity is 120.51370227020561
At time: 281.6955692768097 and batch: 250, loss is 4.7812660121917725 and perplexity is 119.25523309380071
At time: 282.05979585647583 and batch: 300, loss is 4.687753076553345 and perplexity is 108.60886962139303
At time: 282.4254274368286 and batch: 350, loss is 4.751166095733643 and perplexity is 115.71914548336203
At time: 282.790114402771 and batch: 400, loss is 4.67486762046814 and perplexity is 107.21837263456123
At time: 283.1326746940613 and batch: 450, loss is 4.703388013839722 and perplexity is 110.3203067291542
At time: 283.47552847862244 and batch: 500, loss is 4.6798093891143795 and perplexity is 107.74953238007849
At time: 283.83324670791626 and batch: 550, loss is 4.735172090530395 and perplexity is 113.883055227352
At time: 284.1648783683777 and batch: 600, loss is 4.770317811965942 and perplexity is 117.95672406259533
At time: 284.5203323364258 and batch: 650, loss is 4.740181789398194 and perplexity is 114.45500649580505
At time: 284.8608329296112 and batch: 700, loss is 4.731004047393799 and perplexity is 113.40937358914348
At time: 285.1914653778076 and batch: 750, loss is 4.673083438873291 and perplexity is 107.02724614036472
At time: 285.5323624610901 and batch: 800, loss is 4.695702772140503 and perplexity is 109.47571809847686
At time: 285.8671808242798 and batch: 850, loss is 4.667780647277832 and perplexity is 106.46120508432713
At time: 286.1990029811859 and batch: 900, loss is 4.776410913467407 and perplexity is 118.67764043301166
At time: 286.51228165626526 and batch: 950, loss is 4.722999973297119 and perplexity is 112.50525968387097
At time: 286.866863489151 and batch: 1000, loss is 4.700133981704712 and perplexity is 109.9619043485972
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.004265761956936 and perplexity of 149.04760654940438
Finished 38 epochs...
Completing Train Step...
At time: 288.07872200012207 and batch: 50, loss is 4.810775527954101 and perplexity is 122.82683623511824
At time: 288.4361410140991 and batch: 100, loss is 4.771086559295655 and perplexity is 118.0474378427608
At time: 288.7932653427124 and batch: 150, loss is 4.789299192428589 and perplexity is 120.21709008818627
At time: 289.1488251686096 and batch: 200, loss is 4.78963680267334 and perplexity is 120.25768346137792
At time: 289.4904248714447 and batch: 250, loss is 4.779684915542602 and perplexity is 119.06682802691901
At time: 289.83241987228394 and batch: 300, loss is 4.686377201080322 and perplexity is 108.45954009450213
At time: 290.18766236305237 and batch: 350, loss is 4.749933090209961 and perplexity is 115.57655106570314
At time: 290.54185461997986 and batch: 400, loss is 4.673779287338257 and perplexity is 107.10174680288075
At time: 290.8896598815918 and batch: 450, loss is 4.702499046325683 and perplexity is 110.22227913846065
At time: 291.2298378944397 and batch: 500, loss is 4.678822078704834 and perplexity is 107.64320264400097
At time: 291.59676599502563 and batch: 550, loss is 4.734440507888794 and perplexity is 113.7997708293997
At time: 291.9431540966034 and batch: 600, loss is 4.76964919090271 and perplexity is 117.8778820729819
At time: 292.2959394454956 and batch: 650, loss is 4.739606761932373 and perplexity is 114.38921064249371
At time: 292.63670468330383 and batch: 700, loss is 4.730246877670288 and perplexity is 113.32353594603264
At time: 292.98393177986145 and batch: 750, loss is 4.6727388477325436 and perplexity is 106.99037185316742
At time: 293.30055832862854 and batch: 800, loss is 4.69528600692749 and perplexity is 109.43010193377933
At time: 293.64653301239014 and batch: 850, loss is 4.667316951751709 and perplexity is 106.41185094335539
At time: 293.9750826358795 and batch: 900, loss is 4.776294555664062 and perplexity is 118.6638321668301
At time: 294.30247807502747 and batch: 950, loss is 4.722694654464721 and perplexity is 112.4709149526591
At time: 294.63970923423767 and batch: 1000, loss is 4.69981915473938 and perplexity is 109.9272908248702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.003798321979802 and perplexity of 148.977952020531
Finished 39 epochs...
Completing Train Step...
At time: 295.79590702056885 and batch: 50, loss is 4.810019321441651 and perplexity is 122.73398889196086
At time: 296.13792538642883 and batch: 100, loss is 4.770434885025025 and perplexity is 117.97053442551567
At time: 296.4572579860687 and batch: 150, loss is 4.788566789627075 and perplexity is 120.1290749898025
At time: 296.77268242836 and batch: 200, loss is 4.788690986633301 and perplexity is 120.14399558780457
At time: 297.10741448402405 and batch: 250, loss is 4.778866720199585 and perplexity is 118.96944794611642
At time: 297.4514129161835 and batch: 300, loss is 4.685533199310303 and perplexity is 108.36803866980074
At time: 297.7841446399689 and batch: 350, loss is 4.7489988803863525 and perplexity is 115.46862873521854
At time: 298.1151440143585 and batch: 400, loss is 4.672951364517212 and perplexity is 107.01311151917893
At time: 298.4462594985962 and batch: 450, loss is 4.7018057346344 and perplexity is 110.14588722845622
At time: 298.76609444618225 and batch: 500, loss is 4.677891960144043 and perplexity is 107.5431282510187
At time: 299.0827419757843 and batch: 550, loss is 4.733803100585938 and perplexity is 113.72725713724324
At time: 299.4258246421814 and batch: 600, loss is 4.769178314208984 and perplexity is 117.82238919178558
At time: 299.76084446907043 and batch: 650, loss is 4.739070558547974 and perplexity is 114.32789120192339
At time: 300.12431049346924 and batch: 700, loss is 4.729662809371948 and perplexity is 113.25736658682442
At time: 300.4678735733032 and batch: 750, loss is 4.672463502883911 and perplexity is 106.96091666077821
At time: 300.817684173584 and batch: 800, loss is 4.694882192611694 and perplexity is 109.38592141300323
At time: 301.1491024494171 and batch: 850, loss is 4.666894693374633 and perplexity is 106.3669271332695
At time: 301.4805483818054 and batch: 900, loss is 4.776092376708984 and perplexity is 118.63984326234458
At time: 301.8258430957794 and batch: 950, loss is 4.722513599395752 and perplexity is 112.4505533667351
At time: 302.1602110862732 and batch: 1000, loss is 4.699483184814453 and perplexity is 109.89036476459455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.003502822503811 and perplexity of 148.93393561750707
Finished 40 epochs...
Completing Train Step...
At time: 303.3606276512146 and batch: 50, loss is 4.80921142578125 and perplexity is 122.63487267812192
At time: 303.69590044021606 and batch: 100, loss is 4.7697725677490235 and perplexity is 117.8924263715186
At time: 304.0411477088928 and batch: 150, loss is 4.787792835235596 and perplexity is 120.03613653437188
At time: 304.39028334617615 and batch: 200, loss is 4.7878137302398684 and perplexity is 120.03864471616177
At time: 304.7012736797333 and batch: 250, loss is 4.778116588592529 and perplexity is 118.88023866647134
At time: 305.0212154388428 and batch: 300, loss is 4.684653215408325 and perplexity is 108.27271848654689
At time: 305.3583188056946 and batch: 350, loss is 4.748139066696167 and perplexity is 115.36938989701466
At time: 305.6735713481903 and batch: 400, loss is 4.672318429946899 and perplexity is 106.94540065195645
At time: 306.01287937164307 and batch: 450, loss is 4.7011807346343994 and perplexity is 110.07706755732585
At time: 306.3519344329834 and batch: 500, loss is 4.677039985656738 and perplexity is 107.45154326903773
At time: 306.67613554000854 and batch: 550, loss is 4.733137264251709 and perplexity is 113.6515586014635
At time: 307.02379727363586 and batch: 600, loss is 4.76860933303833 and perplexity is 117.75536953912221
At time: 307.35905718803406 and batch: 650, loss is 4.738509979248047 and perplexity is 114.26381931307643
At time: 307.7023415565491 and batch: 700, loss is 4.729106464385986 and perplexity is 113.19437394324606
At time: 308.04769945144653 and batch: 750, loss is 4.67220986366272 and perplexity is 106.93379061743813
At time: 308.3758270740509 and batch: 800, loss is 4.694470825195313 and perplexity is 109.34093286316924
At time: 308.6999886035919 and batch: 850, loss is 4.666479396820068 and perplexity is 106.322762486259
At time: 309.0545780658722 and batch: 900, loss is 4.775892829895019 and perplexity is 118.61617142151103
At time: 309.416855096817 and batch: 950, loss is 4.722407007217408 and perplexity is 112.43856765609856
At time: 309.7654757499695 and batch: 1000, loss is 4.699125862121582 and perplexity is 109.85110545807449
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0031306569169205 and perplexity of 148.87851784488365
Finished 41 epochs...
Completing Train Step...
At time: 310.87638211250305 and batch: 50, loss is 4.808340587615967 and perplexity is 122.52812403774043
At time: 311.22838616371155 and batch: 100, loss is 4.769184226989746 and perplexity is 117.8230858518013
At time: 311.5810925960541 and batch: 150, loss is 4.787066297531128 and perplexity is 119.94895742856956
At time: 311.9296524524689 and batch: 200, loss is 4.786953172683716 and perplexity is 119.93538898854152
At time: 312.2703695297241 and batch: 250, loss is 4.777409858703614 and perplexity is 118.79625212999784
At time: 312.60104179382324 and batch: 300, loss is 4.683871774673462 and perplexity is 108.18814282358512
At time: 312.92251682281494 and batch: 350, loss is 4.747323837280273 and perplexity is 115.27537570345177
At time: 313.25245928764343 and batch: 400, loss is 4.6717197132110595 and perplexity is 106.88138981485689
At time: 313.5809123516083 and batch: 450, loss is 4.700588312149048 and perplexity is 110.0118747401354
At time: 313.9308578968048 and batch: 500, loss is 4.676065464019775 and perplexity is 107.3468804215848
At time: 314.259565114975 and batch: 550, loss is 4.732488508224487 and perplexity is 113.57785037972948
At time: 314.59575366973877 and batch: 600, loss is 4.768235378265381 and perplexity is 117.71134258919975
At time: 314.94087266921997 and batch: 650, loss is 4.737908096313476 and perplexity is 114.19506656282215
At time: 315.2808368206024 and batch: 700, loss is 4.728601875305176 and perplexity is 113.13727170593944
At time: 315.6167821884155 and batch: 750, loss is 4.672016897201538 and perplexity is 106.91315797305016
At time: 315.95118379592896 and batch: 800, loss is 4.694098625183106 and perplexity is 109.30024373933874
At time: 316.2844479084015 and batch: 850, loss is 4.666116132736206 and perplexity is 106.28414625971944
At time: 316.6044261455536 and batch: 900, loss is 4.775593252182007 and perplexity is 118.58064198233005
At time: 316.9380476474762 and batch: 950, loss is 4.722151842117309 and perplexity is 112.40988091781057
At time: 317.2868435382843 and batch: 1000, loss is 4.698824262619018 and perplexity is 109.81797941496245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.002887260623094 and perplexity of 148.84228577496447
Finished 42 epochs...
Completing Train Step...
At time: 318.3745422363281 and batch: 50, loss is 4.807621307373047 and perplexity is 122.44002366714437
At time: 318.7541174888611 and batch: 100, loss is 4.768618936538696 and perplexity is 117.75650040828688
At time: 319.09182381629944 and batch: 150, loss is 4.78632905960083 and perplexity is 119.86055909675899
At time: 319.41592288017273 and batch: 200, loss is 4.786117248535156 and perplexity is 119.83517399252229
At time: 319.74170422554016 and batch: 250, loss is 4.776690311431885 and perplexity is 118.71080335678869
At time: 320.0788309574127 and batch: 300, loss is 4.683119897842407 and perplexity is 108.10682923833227
At time: 320.41818833351135 and batch: 350, loss is 4.746550540924073 and perplexity is 115.18626813318077
At time: 320.75645995140076 and batch: 400, loss is 4.671091480255127 and perplexity is 106.81426449077743
At time: 321.10627269744873 and batch: 450, loss is 4.699955282211303 and perplexity is 109.94225596762597
At time: 321.45002460479736 and batch: 500, loss is 4.675174388885498 and perplexity is 107.25126889055429
At time: 321.7890110015869 and batch: 550, loss is 4.731804113388062 and perplexity is 113.50014487905209
At time: 322.1302707195282 and batch: 600, loss is 4.7675543212890625 and perplexity is 117.63120175147363
At time: 322.4608418941498 and batch: 650, loss is 4.7374162673950195 and perplexity is 114.13891593612433
At time: 322.7830066680908 and batch: 700, loss is 4.728112316131591 and perplexity is 113.08189787219155
At time: 323.1266531944275 and batch: 750, loss is 4.671764507293701 and perplexity is 106.88617757589596
At time: 323.4645245075226 and batch: 800, loss is 4.693675947189331 and perplexity is 109.25405469383016
At time: 323.8163902759552 and batch: 850, loss is 4.665650072097779 and perplexity is 106.23462294398945
At time: 324.16503834724426 and batch: 900, loss is 4.775285425186158 and perplexity is 118.54414527716654
At time: 324.51803040504456 and batch: 950, loss is 4.721821823120117 and perplexity is 112.37278964238499
At time: 324.866028547287 and batch: 1000, loss is 4.698409490585327 and perplexity is 109.77243943331291
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.002530725990853 and perplexity of 148.7892278044079
Finished 43 epochs...
Completing Train Step...
At time: 326.00546503067017 and batch: 50, loss is 4.806861238479614 and perplexity is 122.346996171975
At time: 326.3679323196411 and batch: 100, loss is 4.76794394493103 and perplexity is 117.67704257846412
At time: 326.71261978149414 and batch: 150, loss is 4.785534658432007 and perplexity is 119.76537953889815
At time: 327.0472037792206 and batch: 200, loss is 4.785358943939209 and perplexity is 119.74433687478239
At time: 327.3960566520691 and batch: 250, loss is 4.77585732460022 and perplexity is 118.61195999414117
At time: 327.73251700401306 and batch: 300, loss is 4.682279224395752 and perplexity is 108.01598488815624
At time: 328.06648087501526 and batch: 350, loss is 4.745801305770874 and perplexity is 115.09999885395354
At time: 328.3745231628418 and batch: 400, loss is 4.67043568611145 and perplexity is 106.74423928523267
At time: 328.6921434402466 and batch: 450, loss is 4.6992652702331545 and perplexity is 109.86642066074754
At time: 329.0204277038574 and batch: 500, loss is 4.674248552322387 and perplexity is 107.15201769665713
At time: 329.34962916374207 and batch: 550, loss is 4.731210889816285 and perplexity is 113.43283388491784
At time: 329.68771409988403 and batch: 600, loss is 4.766959133148194 and perplexity is 117.56120988642523
At time: 330.0300543308258 and batch: 650, loss is 4.736872673034668 and perplexity is 114.07688752579925
At time: 330.3814513683319 and batch: 700, loss is 4.727639226913452 and perplexity is 113.02841269816865
At time: 330.71400904655457 and batch: 750, loss is 4.67155969619751 and perplexity is 106.86428834235443
At time: 331.03282403945923 and batch: 800, loss is 4.69328803062439 and perplexity is 109.21168145539933
At time: 331.35738134384155 and batch: 850, loss is 4.665271682739258 and perplexity is 106.19443249745913
At time: 331.70084404945374 and batch: 900, loss is 4.775021190643311 and perplexity is 118.51282595713738
At time: 332.0570778846741 and batch: 950, loss is 4.7215481376647945 and perplexity is 112.34203905247237
At time: 332.39381527900696 and batch: 1000, loss is 4.698013219833374 and perplexity is 109.72894844386765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0022363430116235 and perplexity of 148.74543323475373
Finished 44 epochs...
Completing Train Step...
At time: 333.527485370636 and batch: 50, loss is 4.806266756057739 and perplexity is 122.27428464838424
At time: 333.90025305747986 and batch: 100, loss is 4.767417697906494 and perplexity is 117.61513167659378
At time: 334.25263261795044 and batch: 150, loss is 4.7848351860046385 and perplexity is 119.68163624963266
At time: 334.5811982154846 and batch: 200, loss is 4.784683561325073 and perplexity is 119.66349093555989
At time: 334.91594076156616 and batch: 250, loss is 4.775155897140503 and perplexity is 118.52879148010012
At time: 335.28815484046936 and batch: 300, loss is 4.681529226303101 and perplexity is 107.93500347726213
At time: 335.63877964019775 and batch: 350, loss is 4.745131015777588 and perplexity is 115.02287432728136
At time: 335.9426500797272 and batch: 400, loss is 4.669799098968506 and perplexity is 106.6763088990196
At time: 336.25840163230896 and batch: 450, loss is 4.698507766723633 and perplexity is 109.7832279748723
At time: 336.5931406021118 and batch: 500, loss is 4.673435974121094 and perplexity is 107.06498366861808
At time: 336.9283061027527 and batch: 550, loss is 4.730630378723145 and perplexity is 113.3670039758663
At time: 337.27701115608215 and batch: 600, loss is 4.766398820877075 and perplexity is 117.49535734863372
At time: 337.61177706718445 and batch: 650, loss is 4.736357803344727 and perplexity is 114.01816791186125
At time: 337.9422085285187 and batch: 700, loss is 4.727201900482178 and perplexity is 112.97899319282631
At time: 338.28730630874634 and batch: 750, loss is 4.6712167453765865 and perplexity is 106.82764543065751
At time: 338.62997698783875 and batch: 800, loss is 4.692904691696167 and perplexity is 109.16982438971475
At time: 338.96945118904114 and batch: 850, loss is 4.66483772277832 and perplexity is 106.14835836356849
At time: 339.30920219421387 and batch: 900, loss is 4.774684047698974 and perplexity is 118.47287692869749
At time: 339.6449933052063 and batch: 950, loss is 4.721178617477417 and perplexity is 112.30053407008802
At time: 339.97700572013855 and batch: 1000, loss is 4.697563724517822 and perplexity is 109.67963687905066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.00196391780202 and perplexity of 148.70491674803398
Finished 45 epochs...
Completing Train Step...
At time: 341.0644865036011 and batch: 50, loss is 4.805577373504638 and perplexity is 122.19001993850242
At time: 341.4119076728821 and batch: 100, loss is 4.766780347824096 and perplexity is 117.540193546177
At time: 341.75537157058716 and batch: 150, loss is 4.78400876045227 and perplexity is 119.58276914606405
At time: 342.089501619339 and batch: 200, loss is 4.783816690444946 and perplexity is 119.55980308833442
At time: 342.42353677749634 and batch: 250, loss is 4.774434118270874 and perplexity is 118.44327077020058
At time: 342.76771426200867 and batch: 300, loss is 4.680672864913941 and perplexity is 107.84261167377826
At time: 343.08734369277954 and batch: 350, loss is 4.744310007095337 and perplexity is 114.92847830408088
At time: 343.4168150424957 and batch: 400, loss is 4.668866682052612 and perplexity is 106.57688846192876
At time: 343.7475907802582 and batch: 450, loss is 4.69751745223999 and perplexity is 109.67456186983746
At time: 344.10445642471313 and batch: 500, loss is 4.672422885894775 and perplexity is 106.95657231861591
At time: 344.43852066993713 and batch: 550, loss is 4.7298449039459225 and perplexity is 113.27799201657491
At time: 344.7812879085541 and batch: 600, loss is 4.765915021896363 and perplexity is 117.43852696285981
At time: 345.13239216804504 and batch: 650, loss is 4.735824689865113 and perplexity is 113.9573994892984
At time: 345.4626920223236 and batch: 700, loss is 4.726753301620484 and perplexity is 112.92832231138429
At time: 345.7980055809021 and batch: 750, loss is 4.670868806838989 and perplexity is 106.79048244152435
At time: 346.1308751106262 and batch: 800, loss is 4.692521257400513 and perplexity is 109.12797295914278
At time: 346.4755506515503 and batch: 850, loss is 4.664379711151123 and perplexity is 106.09975231314792
At time: 346.8243200778961 and batch: 900, loss is 4.774365673065185 and perplexity is 118.43516417360225
At time: 347.1643702983856 and batch: 950, loss is 4.720839557647705 and perplexity is 112.26246392452289
At time: 347.5021460056305 and batch: 1000, loss is 4.697191333770752 and perplexity is 109.63880080109553
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0017201493426064 and perplexity of 148.6686715974628
Finished 46 epochs...
Completing Train Step...
At time: 348.6027400493622 and batch: 50, loss is 4.80491810798645 and perplexity is 122.10949081964253
At time: 348.9432325363159 and batch: 100, loss is 4.7662631797790525 and perplexity is 117.47942123017108
At time: 349.2914865016937 and batch: 150, loss is 4.783306856155395 and perplexity is 119.49886293707736
At time: 349.6305944919586 and batch: 200, loss is 4.783095874786377 and perplexity is 119.47365356282644
At time: 349.9694194793701 and batch: 250, loss is 4.773837146759033 and perplexity is 118.37258461269184
At time: 350.29439902305603 and batch: 300, loss is 4.679895668029785 and perplexity is 107.75882929392579
At time: 350.6082715988159 and batch: 350, loss is 4.74361065864563 and perplexity is 114.8481313494855
At time: 350.93820118904114 and batch: 400, loss is 4.668128929138184 and perplexity is 106.49829004852448
At time: 351.28977513313293 and batch: 450, loss is 4.696735744476318 and perplexity is 109.58886191387103
At time: 351.6415297985077 and batch: 500, loss is 4.671483592987061 and perplexity is 106.85615593638424
At time: 351.9874837398529 and batch: 550, loss is 4.729277591705323 and perplexity is 113.21374625054116
At time: 352.33343482017517 and batch: 600, loss is 4.765424928665161 and perplexity is 117.3809852372699
At time: 352.69303703308105 and batch: 650, loss is 4.7352392578125 and perplexity is 113.8907046995437
At time: 353.02306842803955 and batch: 700, loss is 4.7261834239959715 and perplexity is 112.86398532117208
At time: 353.3580424785614 and batch: 750, loss is 4.670481147766114 and perplexity is 106.74909216528556
At time: 353.69740986824036 and batch: 800, loss is 4.692065258026123 and perplexity is 109.07822201580656
At time: 354.0183026790619 and batch: 850, loss is 4.663885126113891 and perplexity is 106.04728993782219
At time: 354.3476378917694 and batch: 900, loss is 4.773956880569458 and perplexity is 118.38675866183696
At time: 354.68923687934875 and batch: 950, loss is 4.720460977554321 and perplexity is 112.21997163432198
At time: 355.0249693393707 and batch: 1000, loss is 4.696755132675171 and perplexity is 109.59098666511532
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.00130927853468 and perplexity of 148.6076005272693
Finished 47 epochs...
Completing Train Step...
At time: 356.19930934906006 and batch: 50, loss is 4.804147834777832 and perplexity is 122.0154693660983
At time: 356.5310709476471 and batch: 100, loss is 4.765685787200928 and perplexity is 117.41160906327531
At time: 356.8599331378937 and batch: 150, loss is 4.7825311756134035 and perplexity is 119.40620593506954
At time: 357.2104182243347 and batch: 200, loss is 4.782322425842285 and perplexity is 119.3812825183797
At time: 357.5386598110199 and batch: 250, loss is 4.773070392608642 and perplexity is 118.28185672958305
At time: 357.8447515964508 and batch: 300, loss is 4.679060077667236 and perplexity is 107.66882466342594
At time: 358.1775496006012 and batch: 350, loss is 4.742886657714844 and perplexity is 114.76501128862095
At time: 358.5240228176117 and batch: 400, loss is 4.667299251556397 and perplexity is 106.40996744947932
At time: 358.85903573036194 and batch: 450, loss is 4.695940217971802 and perplexity is 109.50171573776113
At time: 359.20121574401855 and batch: 500, loss is 4.670487108230591 and perplexity is 106.74972844135367
At time: 359.53869462013245 and batch: 550, loss is 4.728534460067749 and perplexity is 113.1296447869937
At time: 359.87855529785156 and batch: 600, loss is 4.764890308380127 and perplexity is 117.31824775334483
At time: 360.2340302467346 and batch: 650, loss is 4.7345913124084475 and perplexity is 113.81693364325871
At time: 360.56205677986145 and batch: 700, loss is 4.72560299873352 and perplexity is 112.7984952207637
At time: 360.89542865753174 and batch: 750, loss is 4.670024099349976 and perplexity is 106.70031380967148
At time: 361.22668170928955 and batch: 800, loss is 4.691488628387451 and perplexity is 109.01534241092287
At time: 361.56975841522217 and batch: 850, loss is 4.66337031364441 and perplexity is 105.99270952115245
At time: 361.900826215744 and batch: 900, loss is 4.773588953018188 and perplexity is 118.34320892368522
At time: 362.2318022251129 and batch: 950, loss is 4.720095100402832 and perplexity is 112.1789204210687
At time: 362.559538602829 and batch: 1000, loss is 4.696253080368042 and perplexity is 109.53598006666975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.000941578934833 and perplexity of 148.55296761688635
Finished 48 epochs...
Completing Train Step...
At time: 363.707599401474 and batch: 50, loss is 4.803406133651733 and perplexity is 121.92500390838114
At time: 364.0667724609375 and batch: 100, loss is 4.76508053779602 and perplexity is 117.34056725793444
At time: 364.4027955532074 and batch: 150, loss is 4.7818185997009275 and perplexity is 119.32115025683285
At time: 364.7331027984619 and batch: 200, loss is 4.781539335250854 and perplexity is 119.28783275383786
At time: 365.0665473937988 and batch: 250, loss is 4.772378530502319 and perplexity is 118.20005029769469
At time: 365.42425203323364 and batch: 300, loss is 4.678173179626465 and perplexity is 107.57337572677388
At time: 365.7773275375366 and batch: 350, loss is 4.74223916053772 and perplexity is 114.6907253203517
At time: 366.1156368255615 and batch: 400, loss is 4.666514081954956 and perplexity is 106.32645036957447
At time: 366.45528292655945 and batch: 450, loss is 4.6951726531982425 and perplexity is 109.41769832664404
At time: 366.7867498397827 and batch: 500, loss is 4.669634008407593 and perplexity is 106.6586991009931
At time: 367.1305818557739 and batch: 550, loss is 4.72790020942688 and perplexity is 113.05791498702668
At time: 367.46370816230774 and batch: 600, loss is 4.764330005645752 and perplexity is 117.25253243029331
At time: 367.79470014572144 and batch: 650, loss is 4.734060297012329 and perplexity is 113.75651114321383
At time: 368.137934923172 and batch: 700, loss is 4.725085697174072 and perplexity is 112.74015947317011
At time: 368.48138546943665 and batch: 750, loss is 4.669594011306763 and perplexity is 106.65443314756416
At time: 368.81767320632935 and batch: 800, loss is 4.690887565612793 and perplexity is 108.94983703512666
At time: 369.1557638645172 and batch: 850, loss is 4.6628286743164065 and perplexity is 105.93531524609563
At time: 369.4983801841736 and batch: 900, loss is 4.77323697090149 and perplexity is 118.3015615605049
At time: 369.8255331516266 and batch: 950, loss is 4.71974139213562 and perplexity is 112.13924882600797
At time: 370.192631483078 and batch: 1000, loss is 4.6957902908325195 and perplexity is 109.48529968941041
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.000636403153583 and perplexity of 148.50763976576985
Finished 49 epochs...
Completing Train Step...
At time: 371.32541251182556 and batch: 50, loss is 4.802742509841919 and perplexity is 121.84411841451416
At time: 371.6693027019501 and batch: 100, loss is 4.764504899978638 and perplexity is 117.27304102709834
At time: 371.99964356422424 and batch: 150, loss is 4.781124639511108 and perplexity is 119.23837485349038
At time: 372.3242862224579 and batch: 200, loss is 4.780782451629639 and perplexity is 119.19757990677569
At time: 372.6729848384857 and batch: 250, loss is 4.771743364334107 and perplexity is 118.12499746269864
At time: 373.0257408618927 and batch: 300, loss is 4.677364025115967 and perplexity is 107.48636745091174
At time: 373.3633396625519 and batch: 350, loss is 4.741504573822022 and perplexity is 114.60650597411129
At time: 373.70816445350647 and batch: 400, loss is 4.665723285675049 and perplexity is 106.24240104549297
At time: 374.0429196357727 and batch: 450, loss is 4.694447088241577 and perplexity is 109.33833747330787
At time: 374.3797414302826 and batch: 500, loss is 4.668809909820556 and perplexity is 106.57083802583522
At time: 374.72909021377563 and batch: 550, loss is 4.72725739479065 and perplexity is 112.98526305789473
At time: 375.0733530521393 and batch: 600, loss is 4.763768148422241 and perplexity is 117.18667175185405
At time: 375.3922429084778 and batch: 650, loss is 4.733524789810181 and perplexity is 113.6956100201558
At time: 375.7200493812561 and batch: 700, loss is 4.724525957107544 and perplexity is 112.67707194676149
At time: 376.0616445541382 and batch: 750, loss is 4.669174385070801 and perplexity is 106.60968753810626
At time: 376.41393518447876 and batch: 800, loss is 4.6902874565124515 and perplexity is 108.8844748606206
At time: 376.7599067687988 and batch: 850, loss is 4.662357606887817 and perplexity is 105.88542432146325
At time: 377.12065291404724 and batch: 900, loss is 4.772879161834717 and perplexity is 118.25923976117845
At time: 377.4535903930664 and batch: 950, loss is 4.719388360977173 and perplexity is 112.0996671642786
At time: 377.80572986602783 and batch: 1000, loss is 4.695315771102905 and perplexity is 109.43335907900185
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0003111304306405 and perplexity of 148.459342136782
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f4997a568d0>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'num_layers': 1, 'wordvec_source': 'glove', 'data': 'ptb', 'dropout': 0.17614170834692716, 'seq_len': 20, 'batch_size': 50, 'lr': 11.571182403989823, 'anneal': 6.897572663966025, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.5709168910980225 and batch: 50, loss is 6.702553997039795 and perplexity is 814.483359118618
At time: 0.9419121742248535 and batch: 100, loss is 5.822110137939453 and perplexity is 337.68386188765515
At time: 1.2958316802978516 and batch: 150, loss is 5.5731301021575925 and perplexity is 263.25683172717356
At time: 1.6475582122802734 and batch: 200, loss is 5.452623510360718 and perplexity is 233.36961108313412
At time: 1.9772648811340332 and batch: 250, loss is 5.391967763900757 and perplexity is 219.63515072961485
At time: 2.343277931213379 and batch: 300, loss is 5.244609813690186 and perplexity is 189.54184416647945
At time: 2.6881613731384277 and batch: 350, loss is 5.277452535629273 and perplexity is 195.87026672862618
At time: 3.0240447521209717 and batch: 400, loss is 5.197711877822876 and perplexity is 180.8579430023942
At time: 3.3531301021575928 and batch: 450, loss is 5.185534458160401 and perplexity is 178.6689153361945
At time: 3.6961493492126465 and batch: 500, loss is 5.181858205795288 and perplexity is 178.01328917561906
At time: 4.038674592971802 and batch: 550, loss is 5.2098949432373045 and perplexity is 183.07482393505774
At time: 4.380772590637207 and batch: 600, loss is 5.22679030418396 and perplexity is 186.1942165951116
At time: 4.720274448394775 and batch: 650, loss is 5.193781185150146 and perplexity is 180.14844134102415
At time: 5.05536675453186 and batch: 700, loss is 5.1741928863525395 and perplexity is 176.65397688626035
At time: 5.370731353759766 and batch: 750, loss is 5.088987693786621 and perplexity is 162.22555696470502
At time: 5.72232985496521 and batch: 800, loss is 5.1259908103942875 and perplexity is 168.34085291543244
At time: 6.084956169128418 and batch: 850, loss is 5.09772198677063 and perplexity is 163.64868848526135
At time: 6.43047022819519 and batch: 900, loss is 5.20780312538147 and perplexity is 182.692265010688
At time: 6.791071653366089 and batch: 950, loss is 5.133928451538086 and perplexity is 169.68239950963795
At time: 7.130154371261597 and batch: 1000, loss is 5.109375467300415 and perplexity is 165.56692062215737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.1024281571551064 and perplexity of 164.42066219155808
Finished 1 epochs...
Completing Train Step...
At time: 8.231666088104248 and batch: 50, loss is 5.05226879119873 and perplexity is 156.37684874055083
At time: 8.548356294631958 and batch: 100, loss is 4.9527152633666995 and perplexity is 141.55881201781352
At time: 8.87881326675415 and batch: 150, loss is 4.966137981414795 and perplexity is 143.47172556775297
At time: 9.20242714881897 and batch: 200, loss is 4.969707670211792 and perplexity is 143.98479017487023
At time: 9.533669710159302 and batch: 250, loss is 4.938929452896118 and perplexity is 139.62069898887597
At time: 9.884533643722534 and batch: 300, loss is 4.828620910644531 and perplexity is 125.03840256559901
At time: 10.238250494003296 and batch: 350, loss is 4.873252067565918 and perplexity is 130.74541920442624
At time: 10.58713436126709 and batch: 400, loss is 4.7955458450317385 and perplexity is 120.9703948539804
At time: 10.910852670669556 and batch: 450, loss is 4.836533651351929 and perplexity is 126.03172378743221
At time: 11.23993968963623 and batch: 500, loss is 4.814522895812988 and perplexity is 123.28797706595752
At time: 11.581052780151367 and batch: 550, loss is 4.848203763961792 and perplexity is 127.51114390580025
At time: 11.907829761505127 and batch: 600, loss is 4.8635138988494875 and perplexity is 129.4783775987607
At time: 12.239036083221436 and batch: 650, loss is 4.843227891921997 and perplexity is 126.87824069861632
At time: 12.575831890106201 and batch: 700, loss is 4.815280141830445 and perplexity is 123.3813717525053
At time: 12.929702520370483 and batch: 750, loss is 4.742515878677368 and perplexity is 114.72246671600473
At time: 13.250818252563477 and batch: 800, loss is 4.769419498443604 and perplexity is 117.85080952167382
At time: 13.586501121520996 and batch: 850, loss is 4.767618408203125 and perplexity is 117.63874061375999
At time: 13.925236225128174 and batch: 900, loss is 4.894154767990113 and perplexity is 133.50711442456213
At time: 14.254402875900269 and batch: 950, loss is 4.822661695480346 and perplexity is 124.29548761449267
At time: 14.599543571472168 and batch: 1000, loss is 4.791955900192261 and perplexity is 120.53689639259352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.004250875333461 and perplexity of 149.04538775032105
Finished 2 epochs...
Completing Train Step...
At time: 15.760544300079346 and batch: 50, loss is 4.863492126464844 and perplexity is 129.4755585764091
At time: 16.1123526096344 and batch: 100, loss is 4.7613576221466065 and perplexity is 116.90453039164153
At time: 16.446038961410522 and batch: 150, loss is 4.787501087188721 and perplexity is 120.00112133404008
At time: 16.793310403823853 and batch: 200, loss is 4.805782222747803 and perplexity is 122.21505303552705
At time: 17.128711462020874 and batch: 250, loss is 4.764453296661377 and perplexity is 117.26698950529669
At time: 17.46342945098877 and batch: 300, loss is 4.6770946979522705 and perplexity is 107.45742235045603
At time: 17.79521131515503 and batch: 350, loss is 4.743010597229004 and perplexity is 114.7792360898519
At time: 18.125733613967896 and batch: 400, loss is 4.662304515838623 and perplexity is 105.87980290241649
At time: 18.501589059829712 and batch: 450, loss is 4.697937526702881 and perplexity is 109.72064303058953
At time: 18.843487977981567 and batch: 500, loss is 4.669695196151733 and perplexity is 106.66522550584995
At time: 19.191637754440308 and batch: 550, loss is 4.708575162887573 and perplexity is 110.89404134020106
At time: 19.53103470802307 and batch: 600, loss is 4.733395700454712 and perplexity is 113.6809340744132
At time: 19.877690076828003 and batch: 650, loss is 4.710902614593506 and perplexity is 111.15244245730922
At time: 20.2090744972229 and batch: 700, loss is 4.684636487960815 and perplexity is 108.27090737547934
At time: 20.521790742874146 and batch: 750, loss is 4.6145040321350095 and perplexity is 100.93775423342171
At time: 20.858455896377563 and batch: 800, loss is 4.640586843490601 and perplexity is 103.60512974271172
At time: 21.171589374542236 and batch: 850, loss is 4.639445142745972 and perplexity is 103.486911186885
At time: 21.481728076934814 and batch: 900, loss is 4.783063373565674 and perplexity is 119.46977058634486
At time: 21.78675127029419 and batch: 950, loss is 4.699801549911499 and perplexity is 109.9253555908706
At time: 22.109931468963623 and batch: 1000, loss is 4.674909181594849 and perplexity is 107.22282884353373
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.945143536823552 and perplexity of 140.491015038296
Finished 3 epochs...
Completing Train Step...
At time: 23.312968254089355 and batch: 50, loss is 4.754806175231933 and perplexity is 116.14113995314622
At time: 23.661654233932495 and batch: 100, loss is 4.648080949783325 and perplexity is 104.38447419512319
At time: 23.991688013076782 and batch: 150, loss is 4.674186296463013 and perplexity is 107.14534706335688
At time: 24.32923150062561 and batch: 200, loss is 4.699314851760864 and perplexity is 109.87186814077391
At time: 24.659544706344604 and batch: 250, loss is 4.667207059860229 and perplexity is 106.40015778628269
At time: 25.000766277313232 and batch: 300, loss is 4.584750423431396 and perplexity is 97.97873089492964
At time: 25.343963146209717 and batch: 350, loss is 4.640711507797241 and perplexity is 103.61804640948242
At time: 25.65728759765625 and batch: 400, loss is 4.559646282196045 and perplexity is 95.5496762303064
At time: 25.9956476688385 and batch: 450, loss is 4.63245192527771 and perplexity is 102.76572934169478
At time: 26.341044425964355 and batch: 500, loss is 4.580701131820678 and perplexity is 97.58278862575658
At time: 26.698886156082153 and batch: 550, loss is 4.622478647232056 and perplexity is 101.74591206314774
At time: 27.059109449386597 and batch: 600, loss is 4.644156045913697 and perplexity is 103.97557813221117
At time: 27.395445346832275 and batch: 650, loss is 4.629817762374878 and perplexity is 102.49538389307929
At time: 27.717647790908813 and batch: 700, loss is 4.594763078689575 and perplexity is 98.96468592684458
At time: 28.06188130378723 and batch: 750, loss is 4.528650770187378 and perplexity is 92.63349285798517
At time: 28.400571584701538 and batch: 800, loss is 4.575225772857666 and perplexity is 97.04994790771289
At time: 28.746767282485962 and batch: 850, loss is 4.556719789505005 and perplexity is 95.2704595632308
At time: 29.09191393852234 and batch: 900, loss is 4.693642644882202 and perplexity is 109.25041634232876
At time: 29.42104482650757 and batch: 950, loss is 4.620280656814575 and perplexity is 101.522521118929
At time: 29.745185136795044 and batch: 1000, loss is 4.6000900554656985 and perplexity is 99.493275151729
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.922103137504764 and perplexity of 137.29105172715035
Finished 4 epochs...
Completing Train Step...
At time: 30.94692873954773 and batch: 50, loss is 4.6771449375152585 and perplexity is 107.462821100009
At time: 31.27598476409912 and batch: 100, loss is 4.569443740844727 and perplexity is 96.49042116167092
At time: 31.613736867904663 and batch: 150, loss is 4.607406501770019 and perplexity is 100.22388182011184
At time: 31.945494174957275 and batch: 200, loss is 4.630293760299683 and perplexity is 102.54418309635282
At time: 32.28550982475281 and batch: 250, loss is 4.594948215484619 and perplexity is 98.9830096277629
At time: 32.61380362510681 and batch: 300, loss is 4.5091188621521 and perplexity is 90.84173914070297
At time: 32.94253134727478 and batch: 350, loss is 4.574757976531982 and perplexity is 97.00455891590335
At time: 33.25442171096802 and batch: 400, loss is 4.493363018035889 and perplexity is 89.42166745085807
At time: 33.605661153793335 and batch: 450, loss is 4.560800352096558 and perplexity is 95.66001089035657
At time: 33.93869996070862 and batch: 500, loss is 4.518436441421509 and perplexity is 91.69211983859537
At time: 34.26765990257263 and batch: 550, loss is 4.559463272094726 and perplexity is 95.5321912743891
At time: 34.61099600791931 and batch: 600, loss is 4.584595193862915 and perplexity is 97.96352287920989
At time: 34.93809247016907 and batch: 650, loss is 4.5616318798065185 and perplexity is 95.7395879208148
At time: 35.2662672996521 and batch: 700, loss is 4.519515380859375 and perplexity is 91.79110347190135
At time: 35.61532497406006 and batch: 750, loss is 4.465096921920776 and perplexity is 86.92945450116633
At time: 35.9265353679657 and batch: 800, loss is 4.499847545623779 and perplexity is 90.00340884097128
At time: 36.25839853286743 and batch: 850, loss is 4.4981160163879395 and perplexity is 89.8477001532142
At time: 36.60282063484192 and batch: 900, loss is 4.637345542907715 and perplexity is 103.26985802702163
At time: 36.93300437927246 and batch: 950, loss is 4.56217679977417 and perplexity is 95.79177255089971
At time: 37.284358501434326 and batch: 1000, loss is 4.534724311828613 and perplexity is 93.19781822631234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.9159936672303735 and perplexity of 136.4548331549686
Finished 5 epochs...
Completing Train Step...
At time: 38.42512392997742 and batch: 50, loss is 4.623767852783203 and perplexity is 101.87716804757848
At time: 38.785935163497925 and batch: 100, loss is 4.510912809371948 and perplexity is 91.00485068909059
At time: 39.13026523590088 and batch: 150, loss is 4.5470115756988525 and perplexity is 94.35002867148742
At time: 39.47402763366699 and batch: 200, loss is 4.578902730941772 and perplexity is 97.40745336172883
At time: 39.8044216632843 and batch: 250, loss is 4.541413536071778 and perplexity is 93.8233290902099
At time: 40.138054847717285 and batch: 300, loss is 4.468109283447266 and perplexity is 87.19171225469017
At time: 40.48277735710144 and batch: 350, loss is 4.532688436508178 and perplexity is 93.00827209987649
At time: 40.81352257728577 and batch: 400, loss is 4.445404062271118 and perplexity is 85.23431084953258
At time: 41.14744162559509 and batch: 450, loss is 4.50295018196106 and perplexity is 90.28309033834938
At time: 41.50095224380493 and batch: 500, loss is 4.465505685806274 and perplexity is 86.96499538617273
At time: 41.83711504936218 and batch: 550, loss is 4.503760166168213 and perplexity is 90.35624783990716
At time: 42.14501762390137 and batch: 600, loss is 4.526719551086426 and perplexity is 92.45476991931365
At time: 42.48008418083191 and batch: 650, loss is 4.50797028541565 and perplexity is 90.73746032999928
At time: 42.81850337982178 and batch: 700, loss is 4.475489912033081 and perplexity is 87.83762257861366
At time: 43.16273641586304 and batch: 750, loss is 4.423755331039429 and perplexity is 83.40892608530606
At time: 43.505412340164185 and batch: 800, loss is 4.446923427581787 and perplexity is 85.36391133502549
At time: 43.85021185874939 and batch: 850, loss is 4.451385040283203 and perplexity is 85.7456229375521
At time: 44.19044041633606 and batch: 900, loss is 4.583767595291138 and perplexity is 97.88248194689426
At time: 44.562472343444824 and batch: 950, loss is 4.510938482284546 and perplexity is 91.00718707865911
At time: 44.92203903198242 and batch: 1000, loss is 4.498817071914673 and perplexity is 89.91071046425364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8991267506669205 and perplexity of 134.17256241178782
Finished 6 epochs...
Completing Train Step...
At time: 46.12088751792908 and batch: 50, loss is 4.579204244613647 and perplexity is 97.43682746878483
At time: 46.47295546531677 and batch: 100, loss is 4.462511310577392 and perplexity is 86.70497904573465
At time: 46.81708216667175 and batch: 150, loss is 4.49800404548645 and perplexity is 89.83764038844227
At time: 47.15732407569885 and batch: 200, loss is 4.526859359741211 and perplexity is 92.46769679994834
At time: 47.49494123458862 and batch: 250, loss is 4.49277081489563 and perplexity is 89.36872733578491
At time: 47.83445930480957 and batch: 300, loss is 4.418934478759765 and perplexity is 83.00779165564228
At time: 48.165650606155396 and batch: 350, loss is 4.481278390884399 and perplexity is 88.34754320784585
At time: 48.48098182678223 and batch: 400, loss is 4.396403608322143 and perplexity is 81.15846555197086
At time: 48.81062650680542 and batch: 450, loss is 4.463592958450318 and perplexity is 86.79881404095435
At time: 49.16499400138855 and batch: 500, loss is 4.423909749984741 and perplexity is 83.42180699820469
At time: 49.5052330493927 and batch: 550, loss is 4.459319105148316 and perplexity is 86.42864024067545
At time: 49.83492422103882 and batch: 600, loss is 4.486313858032227 and perplexity is 88.79353630868353
At time: 50.17832708358765 and batch: 650, loss is 4.47894778251648 and perplexity is 88.14187943830424
At time: 50.52342677116394 and batch: 700, loss is 4.43919584274292 and perplexity is 84.70679669257531
At time: 50.86811971664429 and batch: 750, loss is 4.38318904876709 and perplexity is 80.09304719666585
At time: 51.22056317329407 and batch: 800, loss is 4.411250495910645 and perplexity is 82.37240548273168
At time: 51.553938150405884 and batch: 850, loss is 4.403109903335571 and perplexity is 81.70456727772506
At time: 51.89074373245239 and batch: 900, loss is 4.543876476287842 and perplexity is 94.05469514405358
At time: 52.22262668609619 and batch: 950, loss is 4.473087253570557 and perplexity is 87.62683210156264
At time: 52.55847978591919 and batch: 1000, loss is 4.457909603118896 and perplexity is 86.30690471024913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.906533962342797 and perplexity of 135.17009689625192
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 53.737117767333984 and batch: 50, loss is 4.507341661453247 and perplexity is 90.68043851266644
At time: 54.05986022949219 and batch: 100, loss is 4.33393087387085 and perplexity is 76.24340148725594
At time: 54.39773368835449 and batch: 150, loss is 4.326826033592224 and perplexity is 75.70362408411319
At time: 54.73447346687317 and batch: 200, loss is 4.332266054153442 and perplexity is 76.11657556957476
At time: 55.08397102355957 and batch: 250, loss is 4.289485130310059 and perplexity is 72.92890994633034
At time: 55.42867994308472 and batch: 300, loss is 4.186264147758484 and perplexity is 65.7765997221864
At time: 55.77480983734131 and batch: 350, loss is 4.252714576721192 and perplexity is 70.2959774017833
At time: 56.10683631896973 and batch: 400, loss is 4.145938754081726 and perplexity is 63.17690164713221
At time: 56.45528030395508 and batch: 450, loss is 4.199405474662781 and perplexity is 66.64669611062772
At time: 56.800544023513794 and batch: 500, loss is 4.135712156295776 and perplexity is 62.53410927624558
At time: 57.13466143608093 and batch: 550, loss is 4.168089723587036 and perplexity is 64.59194570892572
At time: 57.468727350234985 and batch: 600, loss is 4.189699678421021 and perplexity is 66.0029658686805
At time: 57.801735639572144 and batch: 650, loss is 4.158200988769531 and perplexity is 63.95636083094349
At time: 58.14406371116638 and batch: 700, loss is 4.1056744003295895 and perplexity is 60.68365582825611
At time: 58.495914697647095 and batch: 750, loss is 4.052988295555115 and perplexity is 57.569234143750165
At time: 58.82480216026306 and batch: 800, loss is 4.046040058135986 and perplexity is 57.17061588772972
At time: 59.18162941932678 and batch: 850, loss is 4.028660259246826 and perplexity is 56.18558668048189
At time: 59.52961325645447 and batch: 900, loss is 4.152274556159973 and perplexity is 63.57844870998909
At time: 59.88346195220947 and batch: 950, loss is 4.050463600158691 and perplexity is 57.42407268467153
At time: 60.213568687438965 and batch: 1000, loss is 4.0190133333206175 and perplexity is 55.64617450558006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.639931283346036 and perplexity of 103.53723260663975
Finished 8 epochs...
Completing Train Step...
At time: 61.29491329193115 and batch: 50, loss is 4.2952365922927855 and perplexity is 73.34956633435533
At time: 61.63010835647583 and batch: 100, loss is 4.17382703781128 and perplexity is 64.96359511306177
At time: 61.95233678817749 and batch: 150, loss is 4.206464071273803 and perplexity is 67.11879246265127
At time: 62.29890823364258 and batch: 200, loss is 4.226021919250488 and perplexity is 68.44441248453677
At time: 62.62910485267639 and batch: 250, loss is 4.194470362663269 and perplexity is 66.31859746860928
At time: 62.98793315887451 and batch: 300, loss is 4.104466104507447 and perplexity is 60.610376301035316
At time: 63.33106517791748 and batch: 350, loss is 4.176630163192749 and perplexity is 65.14595168014984
At time: 63.66777992248535 and batch: 400, loss is 4.073750567436218 and perplexity is 58.77699679758618
At time: 63.988579750061035 and batch: 450, loss is 4.133788146972656 and perplexity is 62.41390873754362
At time: 64.32370662689209 and batch: 500, loss is 4.071429872512818 and perplexity is 58.64075147256951
At time: 64.6650903224945 and batch: 550, loss is 4.10768039226532 and perplexity is 60.8055089297863
At time: 65.00437593460083 and batch: 600, loss is 4.135833969116211 and perplexity is 62.54172719644066
At time: 65.32466006278992 and batch: 650, loss is 4.108115296363831 and perplexity is 60.83195924609049
At time: 65.65112400054932 and batch: 700, loss is 4.0629128408432 and perplexity is 58.14342720473155
At time: 65.98579478263855 and batch: 750, loss is 4.015381770133972 and perplexity is 55.44445840084818
At time: 66.31911706924438 and batch: 800, loss is 4.013421850204468 and perplexity is 55.335898121326395
At time: 66.63812446594238 and batch: 850, loss is 4.0048446321487425 and perplexity is 54.86329974386445
At time: 66.95627927780151 and batch: 900, loss is 4.135077595710754 and perplexity is 62.494440182832115
At time: 67.27887988090515 and batch: 950, loss is 4.0495336055755615 and perplexity is 57.370693433188165
At time: 67.62570309638977 and batch: 1000, loss is 4.0238852787017825 and perplexity is 55.917941106910305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6278559987137955 and perplexity of 102.29450927221741
Finished 9 epochs...
Completing Train Step...
At time: 68.84323930740356 and batch: 50, loss is 4.250297017097473 and perplexity is 70.12623794540659
At time: 69.19310450553894 and batch: 100, loss is 4.129788408279419 and perplexity is 62.16476799285234
At time: 69.55477380752563 and batch: 150, loss is 4.16572856426239 and perplexity is 64.43961374480867
At time: 69.89673399925232 and batch: 200, loss is 4.18707995891571 and perplexity is 65.83028290081481
At time: 70.2461051940918 and batch: 250, loss is 4.15696072101593 and perplexity is 63.87708698952383
At time: 70.57315683364868 and batch: 300, loss is 4.06934775352478 and perplexity is 58.518781472546074
At time: 70.9240951538086 and batch: 350, loss is 4.142285218238831 and perplexity is 62.94650371236322
At time: 71.28949236869812 and batch: 400, loss is 4.039787802696228 and perplexity is 56.81428568803164
At time: 71.62566685676575 and batch: 450, loss is 4.102657685279846 and perplexity is 60.500866380907915
At time: 71.97150659561157 and batch: 500, loss is 4.041614785194397 and perplexity is 56.91817927060957
At time: 72.3079628944397 and batch: 550, loss is 4.080228524208069 and perplexity is 59.15898756604444
At time: 72.67178893089294 and batch: 600, loss is 4.1102381563186645 and perplexity is 60.96123414405163
At time: 73.0013473033905 and batch: 650, loss is 4.084606242179871 and perplexity is 59.41853663090011
At time: 73.33835124969482 and batch: 700, loss is 4.043269777297974 and perplexity is 57.01245640028627
At time: 73.6778039932251 and batch: 750, loss is 3.9981685209274294 and perplexity is 54.49824617779339
At time: 74.0285849571228 and batch: 800, loss is 3.9980687427520754 and perplexity is 54.49280871350442
At time: 74.36617112159729 and batch: 850, loss is 3.9915849351882935 and perplexity is 54.14063078893042
At time: 74.70735359191895 and batch: 900, loss is 4.125425758361817 and perplexity is 61.89415559567394
At time: 75.04383707046509 and batch: 950, loss is 4.045280523300171 and perplexity is 57.12720929987251
At time: 75.37607312202454 and batch: 1000, loss is 4.019004435539245 and perplexity is 55.64567938028787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.623162618497523 and perplexity of 101.81552714802265
Finished 10 epochs...
Completing Train Step...
At time: 76.59250593185425 and batch: 50, loss is 4.220925116539002 and perplexity is 68.09645231283079
At time: 76.94176530838013 and batch: 100, loss is 4.101885633468628 and perplexity is 60.45417460399302
At time: 77.2627055644989 and batch: 150, loss is 4.139383912086487 and perplexity is 62.764141306380004
At time: 77.59375286102295 and batch: 200, loss is 4.1609137964248655 and perplexity is 64.13009768700074
At time: 77.93238854408264 and batch: 250, loss is 4.132083773612976 and perplexity is 62.30762273546215
At time: 78.27498030662537 and batch: 300, loss is 4.045129780769348 and perplexity is 57.118598448790834
At time: 78.60880208015442 and batch: 350, loss is 4.119210438728333 and perplexity is 61.51065665400556
At time: 78.94504356384277 and batch: 400, loss is 4.017180724143982 and perplexity is 55.544290201106634
At time: 79.2804753780365 and batch: 450, loss is 4.081480531692505 and perplexity is 59.23310144713444
At time: 79.6099169254303 and batch: 500, loss is 4.020585536956787 and perplexity is 55.7337304333154
At time: 79.98755836486816 and batch: 550, loss is 4.061061391830444 and perplexity is 58.035877206226594
At time: 80.3480224609375 and batch: 600, loss is 4.091749205589294 and perplexity is 59.84448050360651
At time: 80.70384931564331 and batch: 650, loss is 4.067468671798706 and perplexity is 58.408923148358966
At time: 81.0474362373352 and batch: 700, loss is 4.028558516502381 and perplexity is 56.17987049548985
At time: 81.40697288513184 and batch: 750, loss is 3.9852793264389037 and perplexity is 53.80031522971218
At time: 81.75008845329285 and batch: 800, loss is 3.986114354133606 and perplexity is 53.845258744841146
At time: 82.08887243270874 and batch: 850, loss is 3.9795688724517824 and perplexity is 53.493966532653445
At time: 82.41482543945312 and batch: 900, loss is 4.115256023406983 and perplexity is 61.267898270983686
At time: 82.74257779121399 and batch: 950, loss is 4.037969813346863 and perplexity is 56.71109175291855
At time: 83.08510541915894 and batch: 1000, loss is 4.012334089279175 and perplexity is 55.275738619093794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6205943037823936 and perplexity of 101.55436834397491
Finished 11 epochs...
Completing Train Step...
At time: 84.22374963760376 and batch: 50, loss is 4.198779330253601 and perplexity is 66.60497871638732
At time: 84.59746408462524 and batch: 100, loss is 4.079788436889649 and perplexity is 59.13295817386884
At time: 84.95124053955078 and batch: 150, loss is 4.117609000205993 and perplexity is 61.41222995209695
At time: 85.29511046409607 and batch: 200, loss is 4.140182852745056 and perplexity is 62.81430616747506
At time: 85.65499567985535 and batch: 250, loss is 4.112267956733704 and perplexity is 61.08509895030827
At time: 86.0011351108551 and batch: 300, loss is 4.026343159675598 and perplexity is 56.05554979406235
At time: 86.33224749565125 and batch: 350, loss is 4.100668153762817 and perplexity is 60.3806176594106
At time: 86.67285990715027 and batch: 400, loss is 3.9988198471069336 and perplexity is 54.53375387455702
At time: 87.01975560188293 and batch: 450, loss is 4.063752107620239 and perplexity is 58.19224553442325
At time: 87.36158990859985 and batch: 500, loss is 4.0036687088012695 and perplexity is 54.79882262628009
At time: 87.69915294647217 and batch: 550, loss is 4.045126705169678 and perplexity is 57.11842277511845
At time: 88.03963232040405 and batch: 600, loss is 4.076541647911072 and perplexity is 58.94127727909344
At time: 88.39108872413635 and batch: 650, loss is 4.052362737655639 and perplexity is 57.53323251629004
At time: 88.742196559906 and batch: 700, loss is 4.01591757774353 and perplexity is 55.47417392375791
At time: 89.09895825386047 and batch: 750, loss is 3.9734188985824583 and perplexity is 53.16598959484264
At time: 89.44248914718628 and batch: 800, loss is 3.975143647193909 and perplexity is 53.25776668502595
At time: 89.78929328918457 and batch: 850, loss is 3.9691619539260863 and perplexity is 52.94014596251154
At time: 90.13042545318604 and batch: 900, loss is 4.105844793319702 and perplexity is 60.69399677881135
At time: 90.45992660522461 and batch: 950, loss is 4.03021237373352 and perplexity is 56.272860855747226
At time: 90.78231048583984 and batch: 1000, loss is 4.003609557151794 and perplexity is 54.795581281398825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.619287630406822 and perplexity of 101.42175661366387
Finished 12 epochs...
Completing Train Step...
At time: 91.9546172618866 and batch: 50, loss is 4.179909744262695 and perplexity is 65.35995383696385
At time: 92.30117154121399 and batch: 100, loss is 4.061365604400635 and perplexity is 58.05353513534059
At time: 92.63317966461182 and batch: 150, loss is 4.099036049842835 and perplexity is 60.282150592738205
At time: 92.94863963127136 and batch: 200, loss is 4.123087902069091 and perplexity is 61.74962496628092
At time: 93.26184129714966 and batch: 250, loss is 4.095510005950928 and perplexity is 60.06996738733653
At time: 93.59073734283447 and batch: 300, loss is 4.010495772361756 and perplexity is 55.17421763612776
At time: 93.93773365020752 and batch: 350, loss is 4.084822998046875 and perplexity is 59.43141734325824
At time: 94.26904559135437 and batch: 400, loss is 3.9827159881591796 and perplexity is 53.66258342424351
At time: 94.61672329902649 and batch: 450, loss is 4.047820200920105 and perplexity is 57.27247838506128
At time: 94.95513391494751 and batch: 500, loss is 3.9882375383377076 and perplexity is 53.95970359843437
At time: 95.28347659111023 and batch: 550, loss is 4.0313304948806765 and perplexity is 56.33581592061685
At time: 95.61416339874268 and batch: 600, loss is 4.062948083877563 and perplexity is 58.14547639164407
At time: 95.95221662521362 and batch: 650, loss is 4.04007342338562 and perplexity is 56.8305153411292
At time: 96.28590965270996 and batch: 700, loss is 4.004298930168152 and perplexity is 54.83336889993565
At time: 96.61893486976624 and batch: 750, loss is 3.9622228574752807 and perplexity is 52.57406080262263
At time: 96.95810627937317 and batch: 800, loss is 3.965078287124634 and perplexity is 52.72439686950991
At time: 97.31042623519897 and batch: 850, loss is 3.9589369964599608 and perplexity is 52.40159325318874
At time: 97.67290997505188 and batch: 900, loss is 4.096850328445434 and perplexity is 60.15053449676327
At time: 98.00289392471313 and batch: 950, loss is 4.022685389518738 and perplexity is 55.850886011624894
At time: 98.35086679458618 and batch: 1000, loss is 3.9941293811798095 and perplexity is 54.27856410744178
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.618167784155869 and perplexity of 101.30824341028527
Finished 13 epochs...
Completing Train Step...
At time: 99.51027345657349 and batch: 50, loss is 4.163336834907532 and perplexity is 64.28567579134236
At time: 99.82759594917297 and batch: 100, loss is 4.045145998001098 and perplexity is 57.119524761850236
At time: 100.17007398605347 and batch: 150, loss is 4.0831687641143795 and perplexity is 59.33318514794902
At time: 100.48587489128113 and batch: 200, loss is 4.107899122238159 and perplexity is 60.818810371761955
At time: 100.84561133384705 and batch: 250, loss is 4.080403094291687 and perplexity is 59.169315856929664
At time: 101.18072485923767 and batch: 300, loss is 3.9965736722946166 and perplexity is 54.41139899684823
At time: 101.52236914634705 and batch: 350, loss is 4.069919738769531 and perplexity is 58.55226292662522
At time: 101.86093711853027 and batch: 400, loss is 3.9680202627182006 and perplexity is 52.87973915284518
At time: 102.19976353645325 and batch: 450, loss is 4.033975691795349 and perplexity is 56.485032513704915
At time: 102.53351593017578 and batch: 500, loss is 3.974535665512085 and perplexity is 53.22539677961848
At time: 102.88246607780457 and batch: 550, loss is 4.01886923789978 and perplexity is 55.63815672432336
At time: 103.22390151023865 and batch: 600, loss is 4.050066132545471 and perplexity is 57.40125301090121
At time: 103.55698442459106 and batch: 650, loss is 4.028056483268738 and perplexity is 56.15167341196812
At time: 103.90486073493958 and batch: 700, loss is 3.9935402154922484 and perplexity is 54.246594458534446
At time: 104.24923801422119 and batch: 750, loss is 3.951945357322693 and perplexity is 52.03649801725994
At time: 104.58977007865906 and batch: 800, loss is 3.954947090148926 and perplexity is 52.19293235109675
At time: 104.94440698623657 and batch: 850, loss is 3.9488563108444215 and perplexity is 51.876002872422255
At time: 105.28306317329407 and batch: 900, loss is 4.0876908874511715 and perplexity is 59.60210471460217
At time: 105.61584663391113 and batch: 950, loss is 4.014066662788391 and perplexity is 55.37159091112102
At time: 105.97287893295288 and batch: 1000, loss is 3.984563250541687 and perplexity is 53.76180391087273
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.617598370807927 and perplexity of 101.25057356477899
Finished 14 epochs...
Completing Train Step...
At time: 107.14186692237854 and batch: 50, loss is 4.148772058486938 and perplexity is 63.3561548604215
At time: 107.48510098457336 and batch: 100, loss is 4.029568982124329 and perplexity is 56.23666701390306
At time: 107.82358241081238 and batch: 150, loss is 4.06799473285675 and perplexity is 58.4396578917366
At time: 108.17257308959961 and batch: 200, loss is 4.093495945930481 and perplexity is 59.949104620881144
At time: 108.50908946990967 and batch: 250, loss is 4.066061687469483 and perplexity is 58.32680049499311
At time: 108.84617805480957 and batch: 300, loss is 3.983358368873596 and perplexity is 53.697066307304745
At time: 109.20021367073059 and batch: 350, loss is 4.056033630371093 and perplexity is 57.74481895839324
At time: 109.55260157585144 and batch: 400, loss is 3.954692873954773 and perplexity is 52.1796657488368
At time: 109.88247036933899 and batch: 450, loss is 4.02151469707489 and perplexity is 55.785540058858594
At time: 110.21920084953308 and batch: 500, loss is 3.961079688072205 and perplexity is 52.51399408467612
At time: 110.56246018409729 and batch: 550, loss is 4.006100978851318 and perplexity is 54.93227038603753
At time: 110.89873790740967 and batch: 600, loss is 4.0369352436065675 and perplexity is 56.65245051290856
At time: 111.23165464401245 and batch: 650, loss is 4.015831370353698 and perplexity is 55.46939184614903
At time: 111.56497073173523 and batch: 700, loss is 3.982615213394165 and perplexity is 53.65717586248632
At time: 111.90635895729065 and batch: 750, loss is 3.942150745391846 and perplexity is 51.52930862929282
At time: 112.22601199150085 and batch: 800, loss is 3.9448095083236696 and perplexity is 51.66649513736559
At time: 112.55334496498108 and batch: 850, loss is 3.938751139640808 and perplexity is 51.35442672851796
At time: 112.91308355331421 and batch: 900, loss is 4.0776625967025755 and perplexity is 59.007384477098846
At time: 113.2583258152008 and batch: 950, loss is 4.004327244758606 and perplexity is 54.83492150629989
At time: 113.61743664741516 and batch: 1000, loss is 3.9741135025024414 and perplexity is 53.20293172821503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.615866312166539 and perplexity of 101.07535342347606
Finished 15 epochs...
Completing Train Step...
At time: 114.71933174133301 and batch: 50, loss is 4.134249014854431 and perplexity is 62.44267993280744
At time: 115.0866539478302 and batch: 100, loss is 4.014737529754639 and perplexity is 55.40875034545668
At time: 115.43047213554382 and batch: 150, loss is 4.054013795852661 and perplexity is 57.62830169221185
At time: 115.81483483314514 and batch: 200, loss is 4.079359841346741 and perplexity is 59.107619481969664
At time: 116.16529536247253 and batch: 250, loss is 4.052446646690369 and perplexity is 57.53806027683887
At time: 116.49798655509949 and batch: 300, loss is 3.9703810739517214 and perplexity is 53.00472571184882
At time: 116.83897352218628 and batch: 350, loss is 4.042422366142273 and perplexity is 56.96416387342168
At time: 117.18409156799316 and batch: 400, loss is 3.941825575828552 and perplexity is 51.512555590445785
At time: 117.52158451080322 and batch: 450, loss is 4.008410663604736 and perplexity is 55.05929324833451
At time: 117.85282349586487 and batch: 500, loss is 3.948496961593628 and perplexity is 51.857364618677714
At time: 118.1812424659729 and batch: 550, loss is 3.994415044784546 and perplexity is 54.294071732601225
At time: 118.51503801345825 and batch: 600, loss is 4.025070552825928 and perplexity is 55.98425848995488
At time: 118.8445394039154 and batch: 650, loss is 4.004577569961548 and perplexity is 54.848649787349906
At time: 119.18280839920044 and batch: 700, loss is 3.9723078346252443 and perplexity is 53.10695158363073
At time: 119.51824498176575 and batch: 750, loss is 3.9334112215042114 and perplexity is 51.08092917101165
At time: 119.85812520980835 and batch: 800, loss is 3.936427283287048 and perplexity is 51.23522497519096
At time: 120.20521283149719 and batch: 850, loss is 3.9292365741729736 and perplexity is 50.86812879867528
At time: 120.5373182296753 and batch: 900, loss is 4.068275065422058 and perplexity is 58.45604272744332
At time: 120.84639310836792 and batch: 950, loss is 3.9948992919921875 and perplexity is 54.3203698521108
At time: 121.16967344284058 and batch: 1000, loss is 3.963916416168213 and perplexity is 52.66317349780847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.615557042563834 and perplexity of 101.0440987223931
Finished 16 epochs...
Completing Train Step...
At time: 122.32396483421326 and batch: 50, loss is 4.120884275436401 and perplexity is 61.61370166525295
At time: 122.66913866996765 and batch: 100, loss is 4.001513237953186 and perplexity is 54.68083256934625
At time: 123.00087666511536 and batch: 150, loss is 4.040960731506348 and perplexity is 56.88096389729339
At time: 123.31254649162292 and batch: 200, loss is 4.066038999557495 and perplexity is 58.32547719668847
At time: 123.61868667602539 and batch: 250, loss is 4.04042121887207 and perplexity is 56.85028417541385
At time: 123.96494030952454 and batch: 300, loss is 3.9582449197769165 and perplexity is 52.365339878848395
At time: 124.3346655368805 and batch: 350, loss is 4.030292110443115 and perplexity is 56.27734804740594
At time: 124.67708277702332 and batch: 400, loss is 3.9290201091766357 and perplexity is 50.85711882104154
At time: 125.01618075370789 and batch: 450, loss is 3.996238079071045 and perplexity is 54.39314196369947
At time: 125.35308122634888 and batch: 500, loss is 3.937112922668457 and perplexity is 51.27036590877663
At time: 125.69381022453308 and batch: 550, loss is 3.9835048151016235 and perplexity is 53.704930615956705
At time: 126.024099111557 and batch: 600, loss is 4.0137294483184816 and perplexity is 55.352921957341465
At time: 126.36049103736877 and batch: 650, loss is 3.994076352119446 and perplexity is 54.27568584250579
At time: 126.67475485801697 and batch: 700, loss is 3.9625327301025393 and perplexity is 52.59035458933806
At time: 126.98698234558105 and batch: 750, loss is 3.9241092300415037 and perplexity is 50.60797790851011
At time: 127.32134294509888 and batch: 800, loss is 3.927351059913635 and perplexity is 50.77230658194616
At time: 127.65810871124268 and batch: 850, loss is 3.9196518516540526 and perplexity is 50.38290100067673
At time: 127.99599599838257 and batch: 900, loss is 4.0594282579422 and perplexity is 57.94117420081957
At time: 128.32557320594788 and batch: 950, loss is 3.9863645315170286 and perplexity is 53.85873129597716
At time: 128.67476797103882 and batch: 1000, loss is 3.9544535970687864 and perplexity is 52.16718185451783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.615945955602134 and perplexity of 101.08340373244955
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 129.8288471698761 and batch: 50, loss is 4.111221733093262 and perplexity is 61.021223695434976
At time: 130.20970582962036 and batch: 100, loss is 3.9933884525299073 and perplexity is 54.238362459334596
At time: 130.54825592041016 and batch: 150, loss is 4.029764304161072 and perplexity is 56.24765234704774
At time: 130.88011479377747 and batch: 200, loss is 4.051609225273133 and perplexity is 57.48989684221732
At time: 131.2100899219513 and batch: 250, loss is 4.017718243598938 and perplexity is 55.57415436326648
At time: 131.5511999130249 and batch: 300, loss is 3.9309005308151246 and perplexity is 50.95284161914805
At time: 131.86938381195068 and batch: 350, loss is 4.000911636352539 and perplexity is 54.6479463861299
At time: 132.21419548988342 and batch: 400, loss is 3.893913459777832 and perplexity is 49.10267233910858
At time: 132.55226945877075 and batch: 450, loss is 3.959545707702637 and perplexity is 52.43350040227107
At time: 132.89726376533508 and batch: 500, loss is 3.8925135374069213 and perplexity is 49.03398050246739
At time: 133.2759621143341 and batch: 550, loss is 3.936230044364929 and perplexity is 51.225120391183715
At time: 133.61023020744324 and batch: 600, loss is 3.9666993522644045 and perplexity is 52.809935864700115
At time: 133.95505571365356 and batch: 650, loss is 3.936932258605957 and perplexity is 51.26110403285495
At time: 134.29318690299988 and batch: 700, loss is 3.9044786930084228 and perplexity is 49.62420372369156
At time: 134.6378357410431 and batch: 750, loss is 3.8640741109848022 and perplexity is 47.659124937748466
At time: 134.97307443618774 and batch: 800, loss is 3.860856490135193 and perplexity is 47.506022388741805
At time: 135.30392861366272 and batch: 850, loss is 3.843931851387024 and perplexity is 46.70876579204406
At time: 135.63593077659607 and batch: 900, loss is 3.978589015007019 and perplexity is 53.44157574324082
At time: 135.99227118492126 and batch: 950, loss is 3.8980290365219115 and perplexity is 49.30517457637737
At time: 136.33081245422363 and batch: 1000, loss is 3.8619270515441895 and perplexity is 47.556907736090714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5910451005144814 and perplexity of 98.5974205498617
Finished 18 epochs...
Completing Train Step...
At time: 137.5446789264679 and batch: 50, loss is 4.088775243759155 and perplexity is 59.666769686417354
At time: 137.89316201210022 and batch: 100, loss is 3.966917624473572 and perplexity is 52.8214640641643
At time: 138.21318340301514 and batch: 150, loss is 4.006750135421753 and perplexity is 54.96794160713865
At time: 138.52441000938416 and batch: 200, loss is 4.030152759552002 and perplexity is 56.2695062951963
At time: 138.85764980316162 and batch: 250, loss is 3.9994488191604614 and perplexity is 54.568064870915855
At time: 139.19045543670654 and batch: 300, loss is 3.9154591751098633 and perplexity is 50.17210400402651
At time: 139.5271315574646 and batch: 350, loss is 3.9869315576553346 and perplexity is 53.88927926432776
At time: 139.85767531394958 and batch: 400, loss is 3.8815987730026245 and perplexity is 48.50169632003484
At time: 140.18809914588928 and batch: 450, loss is 3.948647503852844 and perplexity is 51.8651719311549
At time: 140.5288097858429 and batch: 500, loss is 3.8826369428634644 and perplexity is 48.55207546588446
At time: 140.8742880821228 and batch: 550, loss is 3.92699818611145 and perplexity is 50.754393525786874
At time: 141.2199900150299 and batch: 600, loss is 3.9592837381362913 and perplexity is 52.419766219956145
At time: 141.54495668411255 and batch: 650, loss is 3.9298804235458373 and perplexity is 50.90089075725253
At time: 141.91922450065613 and batch: 700, loss is 3.8989255094528197 and perplexity is 49.34939514905045
At time: 142.2655370235443 and batch: 750, loss is 3.859858980178833 and perplexity is 47.45865828543168
At time: 142.59614753723145 and batch: 800, loss is 3.8581781578063965 and perplexity is 47.37895571251794
At time: 142.91428565979004 and batch: 850, loss is 3.8438573455810547 and perplexity is 46.705285847442546
At time: 143.2420048713684 and batch: 900, loss is 3.9808506059646604 and perplexity is 53.56257550211004
At time: 143.58875489234924 and batch: 950, loss is 3.903542203903198 and perplexity is 49.57775295126485
At time: 143.9013295173645 and batch: 1000, loss is 3.868660912513733 and perplexity is 47.878229996448326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.589299643911967 and perplexity of 98.42547313819937
Finished 19 epochs...
Completing Train Step...
At time: 145.0402009487152 and batch: 50, loss is 4.081231060028077 and perplexity is 59.218326309792836
At time: 145.3712511062622 and batch: 100, loss is 3.9591540575027464 and perplexity is 52.41296883221677
At time: 145.73196816444397 and batch: 150, loss is 3.9986092901229857 and perplexity is 54.522272620589355
At time: 146.08738803863525 and batch: 200, loss is 4.022653856277466 and perplexity is 55.849124879928254
At time: 146.46138310432434 and batch: 250, loss is 3.992161817550659 and perplexity is 54.171872574471
At time: 146.787362575531 and batch: 300, loss is 3.908806838989258 and perplexity is 49.839449994216196
At time: 147.1253809928894 and batch: 350, loss is 3.9806996297836306 and perplexity is 53.5544894394314
At time: 147.47450399398804 and batch: 400, loss is 3.876408200263977 and perplexity is 48.250596975803916
At time: 147.81702518463135 and batch: 450, loss is 3.943661580085754 and perplexity is 51.6072197371053
At time: 148.16684007644653 and batch: 500, loss is 3.877736349105835 and perplexity is 48.31472352567126
At time: 148.51135635375977 and batch: 550, loss is 3.92276478767395 and perplexity is 50.539984115983096
At time: 148.85336828231812 and batch: 600, loss is 3.956052508354187 and perplexity is 52.25065926901833
At time: 149.18786215782166 and batch: 650, loss is 3.9266811847686767 and perplexity is 50.73830686476884
At time: 149.54560589790344 and batch: 700, loss is 3.8966235589981078 and perplexity is 49.23592593680558
At time: 149.8890895843506 and batch: 750, loss is 3.8584609842300415 and perplexity is 47.39235762823676
At time: 150.22303676605225 and batch: 800, loss is 3.857263946533203 and perplexity is 47.33566113030639
At time: 150.55475115776062 and batch: 850, loss is 3.844225821495056 and perplexity is 46.722498791416754
At time: 150.90970277786255 and batch: 900, loss is 3.982339286804199 and perplexity is 53.64237246334302
At time: 151.2293872833252 and batch: 950, loss is 3.9064603519439696 and perplexity is 49.722639471246495
At time: 151.5601499080658 and batch: 1000, loss is 3.8715827083587646 and perplexity is 48.01832497471752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.588468598156441 and perplexity of 98.34371104523801
Finished 20 epochs...
Completing Train Step...
At time: 152.75201559066772 and batch: 50, loss is 4.075781741142273 and perplexity is 58.89650441728231
At time: 153.10547828674316 and batch: 100, loss is 3.9537139177322387 and perplexity is 52.12860913553439
At time: 153.44038033485413 and batch: 150, loss is 3.9929985284805296 and perplexity is 54.217217740098015
At time: 153.79343581199646 and batch: 200, loss is 4.017506647109985 and perplexity is 55.562396311351826
At time: 154.1373794078827 and batch: 250, loss is 3.9870253372192384 and perplexity is 53.894333214411176
At time: 154.45594310760498 and batch: 300, loss is 3.904074954986572 and perplexity is 49.60417258978153
At time: 154.79137682914734 and batch: 350, loss is 3.97642391204834 and perplexity is 53.3259943973902
At time: 155.12608814239502 and batch: 400, loss is 3.872908310890198 and perplexity is 48.08202039593775
At time: 155.46420907974243 and batch: 450, loss is 3.940188889503479 and perplexity is 51.42831465194234
At time: 155.79676008224487 and batch: 500, loss is 3.874201731681824 and perplexity is 48.14425091727067
At time: 156.14593863487244 and batch: 550, loss is 3.9198261165618895 and perplexity is 50.39168173734108
At time: 156.47813940048218 and batch: 600, loss is 3.9538015842437746 and perplexity is 52.133179269169425
At time: 156.80079197883606 and batch: 650, loss is 3.9243785524368286 and perplexity is 50.62160960592128
At time: 157.1378002166748 and batch: 700, loss is 3.895007038116455 and perplexity is 49.1563993299294
At time: 157.48399424552917 and batch: 750, loss is 3.8575351858139038 and perplexity is 47.3485021624003
At time: 157.8198413848877 and batch: 800, loss is 3.8564896535873414 and perplexity is 47.299023647708104
At time: 158.1510772705078 and batch: 850, loss is 3.8443087196350096 and perplexity is 46.726372160205855
At time: 158.49929070472717 and batch: 900, loss is 3.983077745437622 and perplexity is 53.68199976616701
At time: 158.8569746017456 and batch: 950, loss is 3.908014326095581 and perplexity is 49.79996723484646
At time: 159.21248602867126 and batch: 1000, loss is 3.872975344657898 and perplexity is 48.08524362295484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.58797436225705 and perplexity of 98.29511806194772
Finished 21 epochs...
Completing Train Step...
At time: 160.42646765708923 and batch: 50, loss is 4.071352820396424 and perplexity is 58.63623325263306
At time: 160.81103515625 and batch: 100, loss is 3.9493096923828124 and perplexity is 51.899527826897184
At time: 161.13802552223206 and batch: 150, loss is 3.9885003423690795 and perplexity is 53.973886289624254
At time: 161.47414994239807 and batch: 200, loss is 4.013391609191895 and perplexity is 55.33422473303816
At time: 161.82319903373718 and batch: 250, loss is 3.982864270210266 and perplexity is 53.670541212164224
At time: 162.1620659828186 and batch: 300, loss is 3.9002205467224123 and perplexity is 49.413345855336594
At time: 162.50235605239868 and batch: 350, loss is 3.9730242681503296 and perplexity is 53.145012816704025
At time: 162.8438549041748 and batch: 400, loss is 3.8701125526428224 and perplexity is 47.94778242675666
At time: 163.18418049812317 and batch: 450, loss is 3.9373613214492797 and perplexity is 51.283102987032564
At time: 163.53877305984497 and batch: 500, loss is 3.8712639665603636 and perplexity is 48.003021966442624
At time: 163.88703751564026 and batch: 550, loss is 3.9174632453918456 and perplexity is 50.27275324703747
At time: 164.2353537082672 and batch: 600, loss is 3.95193651676178 and perplexity is 52.036037987462976
At time: 164.57095909118652 and batch: 650, loss is 3.922402777671814 and perplexity is 50.521691447489566
At time: 164.9221706390381 and batch: 700, loss is 3.893558554649353 and perplexity is 49.085248640936214
At time: 165.26916575431824 and batch: 750, loss is 3.856704955101013 and perplexity is 47.30920829544034
At time: 165.6092779636383 and batch: 800, loss is 3.855605430603027 and perplexity is 47.25721924879209
At time: 165.9575457572937 and batch: 850, loss is 3.8440599060058593 and perplexity is 46.71474744822501
At time: 166.28421545028687 and batch: 900, loss is 3.9833347320556642 and perplexity is 53.695797094525105
At time: 166.61925864219666 and batch: 950, loss is 3.90881142616272 and perplexity is 49.83967861694293
At time: 166.94911980628967 and batch: 1000, loss is 3.8735930824279787 and perplexity is 48.114956870677084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.587644251381478 and perplexity of 98.26267512963688
Finished 22 epochs...
Completing Train Step...
At time: 168.02227759361267 and batch: 50, loss is 4.0675498437881465 and perplexity is 58.41366450928223
At time: 168.3592495918274 and batch: 100, loss is 3.945530161857605 and perplexity is 51.703742199173746
At time: 168.7138957977295 and batch: 150, loss is 3.9846474361419677 and perplexity is 53.76633007112322
At time: 169.05936551094055 and batch: 200, loss is 4.009874839782714 and perplexity is 55.13996880108352
At time: 169.38694143295288 and batch: 250, loss is 3.979278244972229 and perplexity is 53.478421974935976
At time: 169.71519327163696 and batch: 300, loss is 3.8968993330001833 and perplexity is 49.24950579754728
At time: 170.04576802253723 and batch: 350, loss is 3.9701060485839843 and perplexity is 52.99015007209533
At time: 170.38655185699463 and batch: 400, loss is 3.8676926708221435 and perplexity is 47.83189473353285
At time: 170.7172667980194 and batch: 450, loss is 3.934924440383911 and perplexity is 51.15828431029358
At time: 171.03322196006775 and batch: 500, loss is 3.8686586236953735 and perplexity is 47.87812041200191
At time: 171.37454867362976 and batch: 550, loss is 3.9153779315948487 and perplexity is 50.16802801151776
At time: 171.7112421989441 and batch: 600, loss is 3.9502772426605226 and perplexity is 51.94976753025926
At time: 172.05886316299438 and batch: 650, loss is 3.920574469566345 and perplexity is 50.429406617772
At time: 172.39755988121033 and batch: 700, loss is 3.892171411514282 and perplexity is 49.01720757750789
At time: 172.7594859600067 and batch: 750, loss is 3.8558608055114747 and perplexity is 47.26928909793395
At time: 173.11299300193787 and batch: 800, loss is 3.854663152694702 and perplexity is 47.21271078804794
At time: 173.45697593688965 and batch: 850, loss is 3.8435989809036255 and perplexity is 46.69322041003887
At time: 173.78764081001282 and batch: 900, loss is 3.983290867805481 and perplexity is 53.69344182030411
At time: 174.12921595573425 and batch: 950, loss is 3.909197988510132 and perplexity is 49.85894848436599
At time: 174.46609616279602 and batch: 1000, loss is 3.873741364479065 and perplexity is 48.12209198416136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.587414625214367 and perplexity of 98.24011403858434
Finished 23 epochs...
Completing Train Step...
At time: 175.55884909629822 and batch: 50, loss is 4.0641657161712645 and perplexity is 58.21631932299888
At time: 175.92498779296875 and batch: 100, loss is 3.94214590549469 and perplexity is 51.52905923334207
At time: 176.23421382904053 and batch: 150, loss is 3.98125066280365 and perplexity is 53.58400786354909
At time: 176.57378673553467 and batch: 200, loss is 4.006772961616516 and perplexity is 54.969196330399704
At time: 176.90900206565857 and batch: 250, loss is 3.9760610818862916 and perplexity is 53.30664962784675
At time: 177.2527391910553 and batch: 300, loss is 3.8939812850952147 and perplexity is 49.10600285638973
At time: 177.58274221420288 and batch: 350, loss is 3.9674760580062864 and perplexity is 52.85096957861237
At time: 177.9032747745514 and batch: 400, loss is 3.86545991897583 and perplexity is 47.72521711889167
At time: 178.2192223072052 and batch: 450, loss is 3.932734241485596 and perplexity is 51.046360105234214
At time: 178.56185173988342 and batch: 500, loss is 3.8662559270858763 and perplexity is 47.763221902823396
At time: 178.9104709625244 and batch: 550, loss is 3.9134564924240114 and perplexity is 50.07172574647334
At time: 179.2605962753296 and batch: 600, loss is 3.9487340354919436 and perplexity is 51.86966010367593
At time: 179.6031105518341 and batch: 650, loss is 3.9188136529922484 and perplexity is 50.340687814469746
At time: 179.93573141098022 and batch: 700, loss is 3.8908169221878053 and perplexity is 48.95085923723361
At time: 180.27258825302124 and batch: 750, loss is 3.854981951713562 and perplexity is 47.2277645533602
At time: 180.60662364959717 and batch: 800, loss is 3.8536523294448854 and perplexity is 47.165011194287175
At time: 180.93539667129517 and batch: 850, loss is 3.8429771900177 and perplexity is 46.66419601564048
At time: 181.2742416858673 and batch: 900, loss is 3.983010058403015 and perplexity is 53.67836631376126
At time: 181.61554288864136 and batch: 950, loss is 3.909281358718872 and perplexity is 49.86310540858811
At time: 181.93675303459167 and batch: 1000, loss is 3.873563084602356 and perplexity is 48.11351354823941
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.587238963057355 and perplexity of 98.22285848386554
Finished 24 epochs...
Completing Train Step...
At time: 182.99778699874878 and batch: 50, loss is 4.061077618598938 and perplexity is 58.03681894861107
At time: 183.31966280937195 and batch: 100, loss is 3.9390366458892823 and perplexity is 51.36909083148266
At time: 183.6383352279663 and batch: 150, loss is 3.978141574859619 and perplexity is 53.41766918548868
At time: 183.95096111297607 and batch: 200, loss is 4.003952827453613 and perplexity is 54.81439420589805
At time: 184.2570698261261 and batch: 250, loss is 3.973105483055115 and perplexity is 53.14932915913304
At time: 184.59625554084778 and batch: 300, loss is 3.8913167238235475 and perplexity is 48.97533107177341
At time: 184.92794561386108 and batch: 350, loss is 3.965041766166687 and perplexity is 52.72247135919
At time: 185.27622938156128 and batch: 400, loss is 3.8633737564086914 and perplexity is 47.62575833709445
At time: 185.62192344665527 and batch: 450, loss is 3.930721435546875 and perplexity is 50.94371702342055
At time: 185.99998450279236 and batch: 500, loss is 3.8639978170394897 and perplexity is 47.655488973779605
At time: 186.3453333377838 and batch: 550, loss is 3.9116380405426026 and perplexity is 49.98075546019653
At time: 186.92596912384033 and batch: 600, loss is 3.947261400222778 and perplexity is 51.79333122889841
At time: 187.26545071601868 and batch: 650, loss is 3.9171026039123533 and perplexity is 50.254626075829705
At time: 187.58536410331726 and batch: 700, loss is 3.8894694519042967 and perplexity is 48.88494382855985
At time: 187.92901706695557 and batch: 750, loss is 3.8540545082092286 and perplexity is 47.18398377513835
At time: 188.27214312553406 and batch: 800, loss is 3.852590217590332 and perplexity is 47.11494327035208
At time: 188.6109790802002 and batch: 850, loss is 3.842255783081055 and perplexity is 46.63054428069877
At time: 188.92081475257874 and batch: 900, loss is 3.982522339820862 and perplexity is 53.652192760233454
At time: 189.2580602169037 and batch: 950, loss is 3.9091335487365724 and perplexity is 49.855735688532775
At time: 189.60823893547058 and batch: 1000, loss is 3.873158583641052 and perplexity is 48.09405552141802
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.58709232981612 and perplexity of 98.20845680367106
Finished 25 epochs...
Completing Train Step...
At time: 190.76381182670593 and batch: 50, loss is 4.058215045928955 and perplexity is 57.870921896304495
At time: 191.10529780387878 and batch: 100, loss is 3.936168918609619 and perplexity is 51.22198931270469
At time: 191.44300651550293 and batch: 150, loss is 3.9752637720108033 and perplexity is 53.26416464876662
At time: 191.78618049621582 and batch: 200, loss is 4.001332368850708 and perplexity is 54.67094339058707
At time: 192.10409283638 and batch: 250, loss is 3.9703970098495485 and perplexity is 53.00557039647251
At time: 192.43673872947693 and batch: 300, loss is 3.8888232564926146 and perplexity is 48.85336480636599
At time: 192.76752591133118 and batch: 350, loss is 3.962733602523804 and perplexity is 52.600919602273926
At time: 193.12179112434387 and batch: 400, loss is 3.8613889837265014 and perplexity is 47.53132577756067
At time: 193.4806351661682 and batch: 450, loss is 3.9288313961029053 and perplexity is 50.847522323348514
At time: 193.845600605011 and batch: 500, loss is 3.8618355274200438 and perplexity is 47.55255533094116
At time: 194.17598509788513 and batch: 550, loss is 3.9098880386352537 and perplexity is 49.89336553138418
At time: 194.5163106918335 and batch: 600, loss is 3.9458126258850097 and perplexity is 51.718348709236935
At time: 194.90272855758667 and batch: 650, loss is 3.915426769256592 and perplexity is 50.1704781605294
At time: 195.25191116333008 and batch: 700, loss is 3.8881168603897094 and perplexity is 48.81886716578691
At time: 195.58549332618713 and batch: 750, loss is 3.853086986541748 and perplexity is 47.1383543257778
At time: 195.9269983768463 and batch: 800, loss is 3.8514949178695677 and perplexity is 47.063366537291195
At time: 196.261225938797 and batch: 850, loss is 3.841461853981018 and perplexity is 46.59353762691729
At time: 196.5751314163208 and batch: 900, loss is 3.9818803548812864 and perplexity is 53.617759914373025
At time: 196.92870235443115 and batch: 950, loss is 3.9088128852844237 and perplexity is 49.83975133915278
At time: 197.28138661384583 and batch: 1000, loss is 3.872584147453308 and perplexity is 48.066436488956484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.586974353325076 and perplexity of 98.1968711979745
Finished 26 epochs...
Completing Train Step...
At time: 198.4691469669342 and batch: 50, loss is 4.055524826049805 and perplexity is 57.71544561825006
At time: 198.80416870117188 and batch: 100, loss is 3.9334794378280638 and perplexity is 51.084413843073065
At time: 199.12348628044128 and batch: 150, loss is 3.9725796222686767 and perplexity is 53.12138735849528
At time: 199.447021484375 and batch: 200, loss is 3.9988637828826903 and perplexity is 54.53614990997387
At time: 199.79274320602417 and batch: 250, loss is 3.9678501319885253 and perplexity is 52.87074344948255
At time: 200.13446235656738 and batch: 300, loss is 3.886473088264465 and perplexity is 48.7386859905982
At time: 200.46847105026245 and batch: 350, loss is 3.9605517053604125 and perplexity is 52.48627492193553
At time: 200.7983272075653 and batch: 400, loss is 3.85948899269104 and perplexity is 47.44110242360348
At time: 201.12993717193604 and batch: 450, loss is 3.9270182752609255 and perplexity is 50.755413148626594
At time: 201.44941401481628 and batch: 500, loss is 3.859733986854553 and perplexity is 47.45272664068243
At time: 201.80752420425415 and batch: 550, loss is 3.9081889963150025 and perplexity is 49.80866656578547
At time: 202.13887739181519 and batch: 600, loss is 3.9443794441223146 and perplexity is 51.644280004706566
At time: 202.46913480758667 and batch: 650, loss is 3.913768801689148 and perplexity is 50.087366052524466
At time: 202.78543710708618 and batch: 700, loss is 3.8867585945129393 and perplexity is 48.75260317661818
At time: 203.13278126716614 and batch: 750, loss is 3.8520844554901124 and perplexity is 47.09112034256674
At time: 203.45696258544922 and batch: 800, loss is 3.85036554813385 and perplexity is 47.010244598264414
At time: 203.77664589881897 and batch: 850, loss is 3.8406172084808348 and perplexity is 46.55419922086567
At time: 204.09393000602722 and batch: 900, loss is 3.981118097305298 and perplexity is 53.57690494365443
At time: 204.41435050964355 and batch: 950, loss is 3.9083625936508177 and perplexity is 49.81731396816309
At time: 204.73257565498352 and batch: 1000, loss is 3.871883225440979 and perplexity is 48.03275747012884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.586879078934833 and perplexity of 98.1875159966099
Finished 27 epochs...
Completing Train Step...
At time: 205.8541350364685 and batch: 50, loss is 4.052965431213379 and perplexity is 57.56791787615511
At time: 206.22337198257446 and batch: 100, loss is 3.9309279251098634 and perplexity is 50.954237455428036
At time: 206.550452709198 and batch: 150, loss is 3.9700447797775267 and perplexity is 52.98690352830335
At time: 206.88684940338135 and batch: 200, loss is 3.996524953842163 and perplexity is 54.408748222264606
At time: 207.23157691955566 and batch: 250, loss is 3.9654333114624025 and perplexity is 52.74311863673762
At time: 207.56454253196716 and batch: 300, loss is 3.8842341232299806 and perplexity is 48.62968384836642
At time: 207.90276098251343 and batch: 350, loss is 3.9584629917144776 and perplexity is 52.37676053519389
At time: 208.23945212364197 and batch: 400, loss is 3.857642955780029 and perplexity is 47.353605183845715
At time: 208.58225011825562 and batch: 450, loss is 3.9252713918685913 and perplexity is 50.6668267579011
At time: 208.93886971473694 and batch: 500, loss is 3.8577089071273805 and perplexity is 47.35672832089595
At time: 209.26210284233093 and batch: 550, loss is 3.9065407657623292 and perplexity is 49.726638019312404
At time: 209.59635376930237 and batch: 600, loss is 3.9429610109329225 and perplexity is 51.57107797227505
At time: 209.94513154029846 and batch: 650, loss is 3.912118320465088 and perplexity is 50.004765978978206
At time: 210.30294823646545 and batch: 700, loss is 3.885391659736633 and perplexity is 48.68600707453419
At time: 210.64164423942566 and batch: 750, loss is 3.851051845550537 and perplexity is 47.042518681233155
At time: 210.97328424453735 and batch: 800, loss is 3.8492128086090087 and perplexity is 46.956085253049764
At time: 211.32576751708984 and batch: 850, loss is 3.839741244316101 and perplexity is 46.513437266242526
At time: 211.6611886024475 and batch: 900, loss is 3.9802783346176147 and perplexity is 53.53193194392767
At time: 212.00698471069336 and batch: 950, loss is 3.9078122854232786 and perplexity is 49.78990663234538
At time: 212.35766506195068 and batch: 1000, loss is 3.871092925071716 and perplexity is 47.99481216023466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.586805762314215 and perplexity of 98.18031748363877
Finished 28 epochs...
Completing Train Step...
At time: 213.55943322181702 and batch: 50, loss is 4.0505075359344485 and perplexity is 57.42659571127723
At time: 213.8908817768097 and batch: 100, loss is 3.9284913301467896 and perplexity is 50.83023375184742
At time: 214.2271068096161 and batch: 150, loss is 3.9676249980926515 and perplexity is 52.85884179281546
At time: 214.57004261016846 and batch: 200, loss is 3.994293661117554 and perplexity is 54.28748171904649
At time: 214.91944241523743 and batch: 250, loss is 3.9631151390075683 and perplexity is 52.62099260122897
At time: 215.2521526813507 and batch: 300, loss is 3.882082862854004 and perplexity is 48.52518118293132
At time: 215.59417271614075 and batch: 350, loss is 3.9564555168151854 and perplexity is 52.27172097053339
At time: 215.93077611923218 and batch: 400, loss is 3.85584810256958 and perplexity is 47.26868864271491
At time: 216.2572340965271 and batch: 450, loss is 3.9235678339004516 and perplexity is 50.58058636007236
At time: 216.59429550170898 and batch: 500, loss is 3.855742793083191 and perplexity is 47.263711063489424
At time: 216.94598960876465 and batch: 550, loss is 3.9049240446090696 and perplexity is 49.64630886416521
At time: 217.2858808040619 and batch: 600, loss is 3.9415493059158324 and perplexity is 51.49832618687739
At time: 217.64543676376343 and batch: 650, loss is 3.910472011566162 and perplexity is 49.922510415369835
At time: 217.993754863739 and batch: 700, loss is 3.8840213203430176 and perplexity is 48.619336412272574
At time: 218.32383155822754 and batch: 750, loss is 3.8499996709823607 and perplexity is 46.9930477700351
At time: 218.660573720932 and batch: 800, loss is 3.848034462928772 and perplexity is 46.90078733925834
At time: 219.00626373291016 and batch: 850, loss is 3.838842902183533 and perplexity is 46.47167104880095
At time: 219.37752437591553 and batch: 900, loss is 3.9793909311294557 and perplexity is 53.48444859235468
At time: 219.71147227287292 and batch: 950, loss is 3.907182149887085 and perplexity is 49.758542125815474
At time: 220.05606961250305 and batch: 1000, loss is 3.8702349853515625 and perplexity is 47.953653163014984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.586749193145008 and perplexity of 98.17476366173521
Finished 29 epochs...
Completing Train Step...
At time: 221.21298742294312 and batch: 50, loss is 4.0481375789642335 and perplexity is 57.29065829703336
At time: 221.5701389312744 and batch: 100, loss is 3.9261502695083617 and perplexity is 50.71137627293545
At time: 221.91405200958252 and batch: 150, loss is 3.965298976898193 and perplexity is 52.736033888754434
At time: 222.25803399085999 and batch: 200, loss is 3.992155737876892 and perplexity is 54.171543228159564
At time: 222.60277247428894 and batch: 250, loss is 3.9608842945098877 and perplexity is 52.503734190691674
At time: 222.94320797920227 and batch: 300, loss is 3.8800341796875 and perplexity is 48.4258702241699
At time: 223.2868492603302 and batch: 350, loss is 3.9544939470291136 and perplexity is 52.16928684070381
At time: 223.6171932220459 and batch: 400, loss is 3.854061484336853 and perplexity is 47.18431293777914
At time: 223.94854140281677 and batch: 450, loss is 3.9218774223327637 and perplexity is 50.495156577877786
At time: 224.29940915107727 and batch: 500, loss is 3.8538101863861085 and perplexity is 47.172457106366046
At time: 224.6485755443573 and batch: 550, loss is 3.903368411064148 and perplexity is 49.56913744150435
At time: 224.98738384246826 and batch: 600, loss is 3.9401520490646362 and perplexity is 51.42642004516091
At time: 225.33312320709229 and batch: 650, loss is 3.9088176679611206 and perplexity is 49.8399897071401
At time: 225.67923617362976 and batch: 700, loss is 3.8826415300369264 and perplexity is 48.55229818318738
At time: 226.01096558570862 and batch: 750, loss is 3.848939452171326 and perplexity is 46.94325125906481
At time: 226.34211015701294 and batch: 800, loss is 3.846836919784546 and perplexity is 46.84465523993507
At time: 226.66092014312744 and batch: 850, loss is 3.8379123258590697 and perplexity is 46.42844572731442
At time: 226.9792881011963 and batch: 900, loss is 3.9784802198410034 and perplexity is 53.43576187440184
At time: 227.2980146408081 and batch: 950, loss is 3.9064914083480833 and perplexity is 49.724183701610514
At time: 227.6428244113922 and batch: 1000, loss is 3.8693252420425415 and perplexity is 47.91004748590077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.58670825493045 and perplexity of 98.17074464446252
Finished 30 epochs...
Completing Train Step...
At time: 228.79398488998413 and batch: 50, loss is 4.045839509963989 and perplexity is 57.1591515748334
At time: 229.16267704963684 and batch: 100, loss is 3.92389123916626 and perplexity is 50.59694703347025
At time: 229.52409434318542 and batch: 150, loss is 3.9630537128448484 and perplexity is 52.617760394847004
At time: 229.85749697685242 and batch: 200, loss is 3.9900862073898313 and perplexity is 54.059549495112236
At time: 230.2062349319458 and batch: 250, loss is 3.958664779663086 and perplexity is 52.38733060067708
At time: 230.54712986946106 and batch: 300, loss is 3.8780565881729125 and perplexity is 48.33019826533508
At time: 230.88284993171692 and batch: 350, loss is 3.952581467628479 and perplexity is 52.06960950008695
At time: 231.21791672706604 and batch: 400, loss is 3.852320222854614 and perplexity is 47.1022242008136
At time: 231.55597448349 and batch: 450, loss is 3.9202209186553953 and perplexity is 50.41158040654593
At time: 231.89410400390625 and batch: 500, loss is 3.8519131755828857 and perplexity is 47.08305527055538
At time: 232.22013449668884 and batch: 550, loss is 3.9018179845809935 and perplexity is 49.49234368498806
At time: 232.5634126663208 and batch: 600, loss is 3.9387715673446655 and perplexity is 51.35547579225388
At time: 232.91964411735535 and batch: 650, loss is 3.907186427116394 and perplexity is 49.75875495496539
At time: 233.26588416099548 and batch: 700, loss is 3.881285157203674 and perplexity is 48.48648780673267
At time: 233.59541010856628 and batch: 750, loss is 3.847888789176941 and perplexity is 46.89395562321574
At time: 233.92898631095886 and batch: 800, loss is 3.8456468200683593 and perplexity is 46.78893858979006
At time: 234.26545691490173 and batch: 850, loss is 3.836921744346619 and perplexity is 46.3824773388019
At time: 234.61666011810303 and batch: 900, loss is 3.977521104812622 and perplexity is 53.38453540210175
At time: 234.9440701007843 and batch: 950, loss is 3.9057766914367678 and perplexity is 49.688657683652806
At time: 235.281259059906 and batch: 1000, loss is 3.8683708763122557 and perplexity is 47.86434559007429
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.586679226014672 and perplexity of 98.16789489554714
Finished 31 epochs...
Completing Train Step...
At time: 236.3647289276123 and batch: 50, loss is 4.043628530502319 and perplexity is 57.0329134710079
At time: 236.68554949760437 and batch: 100, loss is 3.9217253875732423 and perplexity is 50.487480142447765
At time: 237.01092076301575 and batch: 150, loss is 3.9608864116668703 and perplexity is 52.503845349456796
At time: 237.34138441085815 and batch: 200, loss is 3.9880875730514527 and perplexity is 53.951612122773405
At time: 237.67752122879028 and batch: 250, loss is 3.9565279483795166 and perplexity is 52.27550723017427
At time: 238.01659560203552 and batch: 300, loss is 3.876116313934326 and perplexity is 48.23651534136746
At time: 238.35111141204834 and batch: 350, loss is 3.9507211112976073 and perplexity is 51.97283152108219
At time: 238.68423104286194 and batch: 400, loss is 3.850614056587219 and perplexity is 47.02192849315524
At time: 239.02616596221924 and batch: 450, loss is 3.918590536117554 and perplexity is 50.32945721044984
At time: 239.37016820907593 and batch: 500, loss is 3.8500517654418944 and perplexity is 46.99549591122726
At time: 239.7009904384613 and batch: 550, loss is 3.9003065824508667 and perplexity is 49.41759735143029
At time: 240.0357391834259 and batch: 600, loss is 3.9373789644241333 and perplexity is 51.28400778151059
At time: 240.37717413902283 and batch: 650, loss is 3.9055634117126465 and perplexity is 49.678061230494556
At time: 240.72049808502197 and batch: 700, loss is 3.8799315118789672 and perplexity is 48.420898701409726
At time: 241.0654456615448 and batch: 750, loss is 3.8468354415893553 and perplexity is 46.84458599444217
At time: 241.40990114212036 and batch: 800, loss is 3.844458932876587 and perplexity is 46.73339160722894
At time: 241.74574041366577 and batch: 850, loss is 3.8359483098983764 and perplexity is 46.33734900586597
At time: 242.07594323158264 and batch: 900, loss is 3.9765591478347777 and perplexity is 53.33320646783412
At time: 242.43250274658203 and batch: 950, loss is 3.904970531463623 and perplexity is 49.648616818548845
At time: 242.7893772125244 and batch: 1000, loss is 3.8673979806900025 and perplexity is 47.817801222864524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.586658384741806 and perplexity of 98.16584897298286
Finished 32 epochs...
Completing Train Step...
At time: 243.99949526786804 and batch: 50, loss is 4.041483688354492 and perplexity is 56.910717966261466
At time: 244.36135578155518 and batch: 100, loss is 3.9196254444122314 and perplexity is 50.38157054479326
At time: 244.69847512245178 and batch: 150, loss is 3.958789291381836 and perplexity is 52.39385384335255
At time: 245.04555439949036 and batch: 200, loss is 3.9861423444747923 and perplexity is 53.84676591309765
At time: 245.3718066215515 and batch: 250, loss is 3.954442067146301 and perplexity is 52.166580374422296
At time: 245.74207830429077 and batch: 300, loss is 3.874219856262207 and perplexity is 48.14512351952416
At time: 246.0868434906006 and batch: 350, loss is 3.9488983821868895 and perplexity is 51.87818541141583
At time: 246.43072652816772 and batch: 400, loss is 3.848935670852661 and perplexity is 46.94307375200824
At time: 246.77491688728333 and batch: 450, loss is 3.916986246109009 and perplexity is 50.24877889812059
At time: 247.11052656173706 and batch: 500, loss is 3.8482351875305176 and perplexity is 46.910202426006734
At time: 247.45138716697693 and batch: 550, loss is 3.8988141345977785 and perplexity is 49.343899173381786
At time: 247.83680629730225 and batch: 600, loss is 3.9360059881210328 and perplexity is 51.21364436880097
At time: 248.1783995628357 and batch: 650, loss is 3.9039595460891725 and perplexity is 49.59844815724809
At time: 248.5137710571289 and batch: 700, loss is 3.8785937643051147 and perplexity is 48.356167068594004
At time: 248.86011743545532 and batch: 750, loss is 3.8457721948623655 and perplexity is 46.79480511107678
At time: 249.1983573436737 and batch: 800, loss is 3.8432631492614746 and perplexity is 46.67754198195473
At time: 249.53957056999207 and batch: 850, loss is 3.834949154853821 and perplexity is 46.29107393169206
At time: 249.887305021286 and batch: 900, loss is 3.9755574655532837 and perplexity is 53.279810287368626
At time: 250.22066473960876 and batch: 950, loss is 3.9040611219406127 and perplexity is 49.60348641772825
At time: 250.54120230674744 and batch: 1000, loss is 3.866392369270325 and perplexity is 47.769739265767626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5866472197742 and perplexity of 98.1647529605775
Finished 33 epochs...
Completing Train Step...
At time: 251.73751306533813 and batch: 50, loss is 4.039400391578674 and perplexity is 56.79227946512314
At time: 252.05826115608215 and batch: 100, loss is 3.917581753730774 and perplexity is 50.278711340553045
At time: 252.41180086135864 and batch: 150, loss is 3.9567467260360716 and perplexity is 52.28694519428118
At time: 252.77719902992249 and batch: 200, loss is 3.9842457485198977 and perplexity is 53.74473713894665
At time: 253.12796068191528 and batch: 250, loss is 3.9524037551879885 and perplexity is 52.06035690488229
At time: 253.46515607833862 and batch: 300, loss is 3.8723567008972166 and perplexity is 48.05550518670413
At time: 253.82223844528198 and batch: 350, loss is 3.9471031522750852 and perplexity is 51.785135689008044
At time: 254.16152238845825 and batch: 400, loss is 3.847278037071228 and perplexity is 46.86532378544128
At time: 254.49408721923828 and batch: 450, loss is 3.915416803359985 and perplexity is 50.16997816922278
At time: 254.82880091667175 and batch: 500, loss is 3.846452627182007 and perplexity is 46.826656644043005
At time: 255.1502068042755 and batch: 550, loss is 3.8973468065261843 and perplexity is 49.27154857897307
At time: 255.46970391273499 and batch: 600, loss is 3.9346429395675657 and perplexity is 51.14388523826775
At time: 255.7992444038391 and batch: 650, loss is 3.902375421524048 and perplexity is 49.51994023671052
At time: 256.1463153362274 and batch: 700, loss is 3.877267475128174 and perplexity is 48.292075319064466
At time: 256.48263573646545 and batch: 750, loss is 3.8447121143341065 and perplexity is 46.74522513338238
At time: 256.81688618659973 and batch: 800, loss is 3.842061171531677 and perplexity is 46.62147032120527
At time: 257.17039799690247 and batch: 850, loss is 3.833927903175354 and perplexity is 46.2438232262791
At time: 257.5173707008362 and batch: 900, loss is 3.974531168937683 and perplexity is 53.22515744819988
At time: 257.8451199531555 and batch: 950, loss is 3.903130440711975 and perplexity is 49.55734285984652
At time: 258.1742162704468 and batch: 1000, loss is 3.865358428955078 and perplexity is 47.72037373139779
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.586644986780678 and perplexity of 98.16453375956482
Finished 34 epochs...
Completing Train Step...
At time: 259.3371021747589 and batch: 50, loss is 4.0373627996444705 and perplexity is 56.67667778907739
At time: 259.6683111190796 and batch: 100, loss is 3.915583543777466 and perplexity is 50.17834422978855
At time: 259.9994125366211 and batch: 150, loss is 3.954750323295593 and perplexity is 52.18266352234751
At time: 260.33414340019226 and batch: 200, loss is 3.98238929271698 and perplexity is 53.645054966211724
At time: 260.6808488368988 and batch: 250, loss is 3.950402817726135 and perplexity is 51.95629153534304
At time: 261.01243591308594 and batch: 300, loss is 3.870525221824646 and perplexity is 47.967573082116886
At time: 261.3519399166107 and batch: 350, loss is 3.9453296136856078 and perplexity is 51.69337414787191
At time: 261.6952760219574 and batch: 400, loss is 3.845642924308777 and perplexity is 46.78875631168925
At time: 262.0406289100647 and batch: 450, loss is 3.913874044418335 and perplexity is 50.092637661019985
At time: 262.3787350654602 and batch: 500, loss is 3.84470814704895 and perplexity is 46.74503968211244
At time: 262.7289412021637 and batch: 550, loss is 3.8958816051483156 and perplexity is 49.199408700735844
At time: 263.06119775772095 and batch: 600, loss is 3.933293523788452 and perplexity is 51.07491741612121
At time: 263.41582465171814 and batch: 650, loss is 3.900801410675049 and perplexity is 49.44205662444129
At time: 263.7602753639221 and batch: 700, loss is 3.8759524250030517 and perplexity is 48.22861055819068
At time: 264.0945191383362 and batch: 750, loss is 3.843649535179138 and perplexity is 46.69558101163679
At time: 264.44609093666077 and batch: 800, loss is 3.840854454040527 and perplexity is 46.56524530818163
At time: 264.78913164138794 and batch: 850, loss is 3.8328899574279784 and perplexity is 46.19584954796365
At time: 265.13346123695374 and batch: 900, loss is 3.973485345840454 and perplexity is 53.16952244644271
At time: 265.4713397026062 and batch: 950, loss is 3.9022093772888184 and perplexity is 49.511718418716946
At time: 265.80337381362915 and batch: 1000, loss is 3.8643016958236696 and perplexity is 47.66997266636044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.58665429092035 and perplexity of 98.16544710034668
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 266.9280152320862 and batch: 50, loss is 4.037150983810425 and perplexity is 56.66467404263721
At time: 267.2771751880646 and batch: 100, loss is 3.914750475883484 and perplexity is 50.13655966934196
At time: 267.6114037036896 and batch: 150, loss is 3.9544693183898927 and perplexity is 52.168001997981825
At time: 267.95368003845215 and batch: 200, loss is 3.981981048583984 and perplexity is 53.62315915697946
At time: 268.3048527240753 and batch: 250, loss is 3.94924795627594 and perplexity is 51.89632385100215
At time: 268.6356871128082 and batch: 300, loss is 3.8689293336868285 and perplexity is 47.89108325207552
At time: 268.98040652275085 and batch: 350, loss is 3.9423418998718263 and perplexity is 51.53915962898897
At time: 269.32832884788513 and batch: 400, loss is 3.842111225128174 and perplexity is 46.623803951871636
At time: 269.67789340019226 and batch: 450, loss is 3.909532985687256 and perplexity is 49.87565388933815
At time: 270.00838971138 and batch: 500, loss is 3.8385276699066164 and perplexity is 46.45702398685876
At time: 270.3410367965698 and batch: 550, loss is 3.8891407251358032 and perplexity is 48.86887667994271
At time: 270.67083525657654 and batch: 600, loss is 3.924571499824524 and perplexity is 50.63137785560449
At time: 271.0031077861786 and batch: 650, loss is 3.889588837623596 and perplexity is 48.890780341132924
At time: 271.3408169746399 and batch: 700, loss is 3.865843563079834 and perplexity is 47.743530129666325
At time: 271.679167509079 and batch: 750, loss is 3.831651978492737 and perplexity is 46.138695444415305
At time: 272.0111241340637 and batch: 800, loss is 3.8261883449554444 and perplexity is 45.887297917974685
At time: 272.38827896118164 and batch: 850, loss is 3.816569838523865 and perplexity is 45.448046505555084
At time: 272.7431204319 and batch: 900, loss is 3.955840239524841 and perplexity is 52.23956925981603
At time: 273.0941619873047 and batch: 950, loss is 3.8831003093719483 and perplexity is 48.57457808465005
At time: 273.43632793426514 and batch: 1000, loss is 3.846102948188782 and perplexity is 46.810285208432
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5834171946455795 and perplexity of 97.84818987036493
Finished 36 epochs...
Completing Train Step...
At time: 274.5383994579315 and batch: 50, loss is 4.034133443832397 and perplexity is 56.493943845518594
At time: 274.8820996284485 and batch: 100, loss is 3.9110643243789673 and perplexity is 49.95208891693422
At time: 275.21329259872437 and batch: 150, loss is 3.951058602333069 and perplexity is 51.99037484599905
At time: 275.5462737083435 and batch: 200, loss is 3.9785284757614137 and perplexity is 53.438340528491075
At time: 275.886127948761 and batch: 250, loss is 3.9460228061676026 and perplexity is 51.72922002881237
At time: 276.2055311203003 and batch: 300, loss is 3.8659886503219605 and perplexity is 47.75045760931454
At time: 276.5457317829132 and batch: 350, loss is 3.9398978996276854 and perplexity is 51.413351710187364
At time: 276.8950870037079 and batch: 400, loss is 3.8398951625823976 and perplexity is 46.52059708486533
At time: 277.207279920578 and batch: 450, loss is 3.9074009895324706 and perplexity is 49.76943245910407
At time: 277.52646017074585 and batch: 500, loss is 3.8367954444885255 and perplexity is 46.37661960841904
At time: 277.8591048717499 and batch: 550, loss is 3.88760546207428 and perplexity is 48.79390766202521
At time: 278.1982333660126 and batch: 600, loss is 3.9232674932479856 and perplexity is 50.565397234832936
At time: 278.53184151649475 and batch: 650, loss is 3.8886101341247556 and perplexity is 48.842954170989614
At time: 278.89371967315674 and batch: 700, loss is 3.86474892616272 and perplexity is 47.69129689246354
At time: 279.2415111064911 and batch: 750, loss is 3.831322193145752 and perplexity is 46.123482087437715
At time: 279.57285594940186 and batch: 800, loss is 3.8262130308151243 and perplexity is 45.88843069935396
At time: 279.90251088142395 and batch: 850, loss is 3.816842374801636 and perplexity is 45.460434434985075
At time: 280.2541038990021 and batch: 900, loss is 3.9568178939819334 and perplexity is 52.29066648118265
At time: 280.5956447124481 and batch: 950, loss is 3.884743194580078 and perplexity is 48.6544461295271
At time: 280.97405910491943 and batch: 1000, loss is 3.8479109096527098 and perplexity is 46.894992951297866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.583078896127096 and perplexity of 97.81509357122559
Finished 37 epochs...
Completing Train Step...
At time: 282.0735182762146 and batch: 50, loss is 4.03310920715332 and perplexity is 56.43611029876212
At time: 282.41988706588745 and batch: 100, loss is 3.909658932685852 and perplexity is 49.881935973845074
At time: 282.75084686279297 and batch: 150, loss is 3.9496415853500366 and perplexity is 51.91675577394428
At time: 283.0981285572052 and batch: 200, loss is 3.977210454940796 and perplexity is 53.36795407864795
At time: 283.44103169441223 and batch: 250, loss is 3.9447018623352053 and perplexity is 51.66093374576216
At time: 283.7704174518585 and batch: 300, loss is 3.864677629470825 and perplexity is 47.6878967819727
At time: 284.1161653995514 and batch: 350, loss is 3.9388276052474978 and perplexity is 51.358353726052165
At time: 284.4545729160309 and batch: 400, loss is 3.8390751600265505 and perplexity is 46.482465712404284
At time: 284.79273748397827 and batch: 450, loss is 3.906469912528992 and perplexity is 49.723114851041146
At time: 285.1221172809601 and batch: 500, loss is 3.8360104370117187 and perplexity is 46.34022790102743
At time: 285.46538519859314 and batch: 550, loss is 3.886991624832153 and perplexity is 48.76396533511124
At time: 285.80687046051025 and batch: 600, loss is 3.9227149200439455 and perplexity is 50.537463869594646
At time: 286.14148807525635 and batch: 650, loss is 3.8882033824920654 and perplexity is 48.82309125954484
At time: 286.47781014442444 and batch: 700, loss is 3.8643873262405397 and perplexity is 47.674054840768704
At time: 286.8215036392212 and batch: 750, loss is 3.8312678146362305 and perplexity is 46.12097402942071
At time: 287.16502046585083 and batch: 800, loss is 3.8263001155853273 and perplexity is 45.892427056804806
At time: 287.4964904785156 and batch: 850, loss is 3.8171382474899294 and perplexity is 45.473886925946545
At time: 287.85224318504333 and batch: 900, loss is 3.9574614334106446 and perplexity is 52.32432841704663
At time: 288.185485124588 and batch: 950, loss is 3.885698299407959 and perplexity is 48.700938424896876
At time: 288.5177466869354 and batch: 1000, loss is 3.848855996131897 and perplexity is 46.93933372471004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.582880159703697 and perplexity of 97.79565608090745
Finished 38 epochs...
Completing Train Step...
At time: 289.7246024608612 and batch: 50, loss is 4.032300839424133 and perplexity is 56.39050760279295
At time: 290.09978723526 and batch: 100, loss is 3.908680868148804 and perplexity is 49.83317207223852
At time: 290.44087958335876 and batch: 150, loss is 3.948604116439819 and perplexity is 51.862921684335255
At time: 290.7739851474762 and batch: 200, loss is 3.97637996673584 and perplexity is 53.32365102139259
At time: 291.10575103759766 and batch: 250, loss is 3.943825263977051 and perplexity is 51.61566769902964
At time: 291.42990922927856 and batch: 300, loss is 3.8638185119628905 and perplexity is 47.646944868702384
At time: 291.7692549228668 and batch: 350, loss is 3.9381446170806886 and perplexity is 51.323288554102305
At time: 292.1029691696167 and batch: 400, loss is 3.838586645126343 and perplexity is 46.45976388084835
At time: 292.44007110595703 and batch: 450, loss is 3.9058790731430055 and perplexity is 49.69374515363461
At time: 292.79256939888 and batch: 500, loss is 3.8355046272277833 and perplexity is 46.31679448728987
At time: 293.13863730430603 and batch: 550, loss is 3.8866196775436403 and perplexity is 48.745831083129694
At time: 293.481338262558 and batch: 600, loss is 3.9223613023757933 and perplexity is 50.519596088834426
At time: 293.8211054801941 and batch: 650, loss is 3.887930736541748 and perplexity is 48.80978165591815
At time: 294.15093421936035 and batch: 700, loss is 3.864191427230835 and perplexity is 47.66471645535688
At time: 294.49913930892944 and batch: 750, loss is 3.8312173795700075 and perplexity is 46.11864797369915
At time: 294.8386244773865 and batch: 800, loss is 3.8263194704055787 and perplexity is 45.8933153050773
At time: 295.17480635643005 and batch: 850, loss is 3.817311487197876 and perplexity is 45.48176549125726
At time: 295.5141592025757 and batch: 900, loss is 3.9578321981430054 and perplexity is 52.34373202953344
At time: 295.8475720882416 and batch: 950, loss is 3.886261329650879 and perplexity is 48.7283662467107
At time: 296.1914486885071 and batch: 1000, loss is 3.849394941329956 and perplexity is 46.96463827149055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.582753251238567 and perplexity of 97.78324577180118
Finished 39 epochs...
Completing Train Step...
At time: 297.3644917011261 and batch: 50, loss is 4.031598978042602 and perplexity is 56.35094316921979
At time: 297.7248532772064 and batch: 100, loss is 3.9078712701797484 and perplexity is 49.792843564479
At time: 298.032856464386 and batch: 150, loss is 3.947751245498657 and perplexity is 51.818708162400995
At time: 298.37038350105286 and batch: 200, loss is 3.9757486820220946 and perplexity is 53.28999923866729
At time: 298.71436953544617 and batch: 250, loss is 3.943127083778381 and perplexity is 51.57964323914671
At time: 299.0704643726349 and batch: 300, loss is 3.8631626033782958 and perplexity is 47.61570307553352
At time: 299.4078845977783 and batch: 350, loss is 3.937622504234314 and perplexity is 51.29649900002376
At time: 299.7604310512543 and batch: 400, loss is 3.8382333612442014 and perplexity is 46.44335329406974
At time: 300.09899616241455 and batch: 450, loss is 3.9054246616363524 and perplexity is 49.67116887387724
At time: 300.4424924850464 and batch: 500, loss is 3.835122241973877 and perplexity is 46.29908701382386
At time: 300.7504985332489 and batch: 550, loss is 3.8863458395004273 and perplexity is 48.73248444762278
At time: 301.06998324394226 and batch: 600, loss is 3.9220851373672487 and perplexity is 50.5056462704634
At time: 301.3845148086548 and batch: 650, loss is 3.8877038240432737 and perplexity is 48.7987073629079
At time: 301.72439312934875 and batch: 700, loss is 3.8640410804748537 and perplexity is 47.65755075854618
At time: 302.06442737579346 and batch: 750, loss is 3.831142544746399 and perplexity is 46.11519682194775
At time: 302.42041206359863 and batch: 800, loss is 3.8262867307662964 and perplexity is 45.89181279908463
At time: 302.7630865573883 and batch: 850, loss is 3.8173877811431884 and perplexity is 45.485235606959094
At time: 303.10067343711853 and batch: 900, loss is 3.958037467002869 and perplexity is 52.354477670563156
At time: 303.444500207901 and batch: 950, loss is 3.8866085052490233 and perplexity is 48.745286483385705
At time: 303.78337001800537 and batch: 1000, loss is 3.849727373123169 and perplexity is 46.98025340574848
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.582670630478278 and perplexity of 97.77516717942626
Finished 40 epochs...
Completing Train Step...
At time: 304.901887178421 and batch: 50, loss is 4.030970058441162 and perplexity is 56.31551409868728
At time: 305.2666540145874 and batch: 100, loss is 3.907154049873352 and perplexity is 49.75714392974315
At time: 305.61795377731323 and batch: 150, loss is 3.9470078468322756 and perplexity is 51.78020051889827
At time: 305.96223545074463 and batch: 200, loss is 3.975222282409668 and perplexity is 53.26195478566403
At time: 306.3061189651489 and batch: 250, loss is 3.94252480506897 and perplexity is 51.54858727129763
At time: 306.63407278060913 and batch: 300, loss is 3.8626177978515623 and perplexity is 47.58976884253687
At time: 306.944815158844 and batch: 350, loss is 3.937185025215149 and perplexity is 51.27406276600329
At time: 307.28007912635803 and batch: 400, loss is 3.8379475355148314 and perplexity is 46.4300804856855
At time: 307.6665737628937 and batch: 450, loss is 3.905046591758728 and perplexity is 49.652393250612064
At time: 308.02778697013855 and batch: 500, loss is 3.8348034715652464 and perplexity is 46.28433058701829
At time: 308.3737859725952 and batch: 550, loss is 3.8861211156845092 and perplexity is 48.72153432818094
At time: 308.7107021808624 and batch: 600, loss is 3.921845564842224 and perplexity is 50.49354795452829
At time: 309.0532696247101 and batch: 650, loss is 3.887494502067566 and perplexity is 48.78849379007062
At time: 309.4038624763489 and batch: 700, loss is 3.8639016437530516 and perplexity is 47.650906009171095
At time: 309.7481827735901 and batch: 750, loss is 3.8310463428497314 and perplexity is 46.11076066593501
At time: 310.1055269241333 and batch: 800, loss is 3.8262161779403687 and perplexity is 45.88857511621989
At time: 310.45563220977783 and batch: 850, loss is 3.8173991346359255 and perplexity is 45.48575202618278
At time: 310.79740595817566 and batch: 900, loss is 3.95814218044281 and perplexity is 52.35996017505731
At time: 311.12999749183655 and batch: 950, loss is 3.8868315696716307 and perplexity is 48.75616103538769
At time: 311.4939830303192 and batch: 1000, loss is 3.849941248893738 and perplexity is 46.99030241822899
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.582613316977897 and perplexity of 97.76956350292956
Finished 41 epochs...
Completing Train Step...
At time: 312.7026653289795 and batch: 50, loss is 4.03039475440979 and perplexity is 56.28312487411659
At time: 313.0486102104187 and batch: 100, loss is 3.9064972591400147 and perplexity is 49.72447462831438
At time: 313.3916320800781 and batch: 150, loss is 3.9463368606567384 and perplexity is 51.745468373881224
At time: 313.7343604564667 and batch: 200, loss is 3.9747603845596315 and perplexity is 53.23735888409411
At time: 314.0642809867859 and batch: 250, loss is 3.9419828701019286 and perplexity is 51.52065885772921
At time: 314.4105808734894 and batch: 300, loss is 3.8621404457092283 and perplexity is 47.567057185587025
At time: 314.76012563705444 and batch: 350, loss is 3.9367998361587526 and perplexity is 51.25431636144246
At time: 315.1018314361572 and batch: 400, loss is 3.837700157165527 and perplexity is 46.418596109568455
At time: 315.4321186542511 and batch: 450, loss is 3.90471390247345 and perplexity is 49.63587717890156
At time: 315.7811403274536 and batch: 500, loss is 3.834520683288574 and perplexity is 46.271243771420885
At time: 316.1093850135803 and batch: 550, loss is 3.8859230136871337 and perplexity is 48.711883450875916
At time: 316.44054222106934 and batch: 600, loss is 3.9216254234313963 and perplexity is 50.48243345706935
At time: 316.79982709884644 and batch: 650, loss is 3.887292408943176 and perplexity is 48.77863496716003
At time: 317.14399933815 and batch: 700, loss is 3.8637625217437743 and perplexity is 47.64427718050189
At time: 317.48753809928894 and batch: 750, loss is 3.8309338426589967 and perplexity is 46.105573488349876
At time: 317.8334701061249 and batch: 800, loss is 3.826117739677429 and perplexity is 45.88405814692163
At time: 318.191121339798 and batch: 850, loss is 3.8173672103881837 and perplexity is 45.4842999509447
At time: 318.54061102867126 and batch: 900, loss is 3.9581827878952027 and perplexity is 52.36208642281786
At time: 318.88858294487 and batch: 950, loss is 3.88697744846344 and perplexity is 48.76327404405874
At time: 319.22867488861084 and batch: 1000, loss is 3.850080494880676 and perplexity is 46.99684608484483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.582572378763339 and perplexity of 97.76556107348836
Finished 42 epochs...
Completing Train Step...
At time: 320.5504903793335 and batch: 50, loss is 4.02986014842987 and perplexity is 56.2530436205158
At time: 320.9223310947418 and batch: 100, loss is 3.9058848237991333 and perplexity is 49.694030926096374
At time: 321.26011657714844 and batch: 150, loss is 3.9457176399230955 and perplexity is 51.71343642543887
At time: 321.6056795120239 and batch: 200, loss is 3.974342021942139 and perplexity is 53.21509102162836
At time: 321.9469270706177 and batch: 250, loss is 3.9414825963974 and perplexity is 51.49489087292273
At time: 322.29729652404785 and batch: 300, loss is 3.8617080211639405 and perplexity is 47.54649246917739
At time: 322.6489233970642 and batch: 350, loss is 3.936450090408325 and perplexity is 51.2363935165061
At time: 322.99414014816284 and batch: 400, loss is 3.837476716041565 and perplexity is 46.40822544494061
At time: 323.3390598297119 and batch: 450, loss is 3.9044103717803953 and perplexity is 49.620813452968314
At time: 323.68794441223145 and batch: 500, loss is 3.834259424209595 and perplexity is 46.25915656790452
At time: 324.0404922962189 and batch: 550, loss is 3.8857405853271483 and perplexity is 48.70299783238522
At time: 324.3776466846466 and batch: 600, loss is 3.9214163208007813 and perplexity is 50.471878551001396
At time: 324.7437856197357 and batch: 650, loss is 3.8870926761627196 and perplexity is 48.76889324767391
At time: 325.09041690826416 and batch: 700, loss is 3.8636202001571656 and perplexity is 47.6374968538856
At time: 325.42231488227844 and batch: 750, loss is 3.8308089447021483 and perplexity is 46.09981535601882
At time: 325.86094427108765 and batch: 800, loss is 3.8259986639022827 and perplexity is 45.87859479241396
At time: 326.220828294754 and batch: 850, loss is 3.8173057079315185 and perplexity is 45.481502640779624
At time: 326.56324648857117 and batch: 900, loss is 3.958180718421936 and perplexity is 52.36197806099196
At time: 326.88261365890503 and batch: 950, loss is 3.8870713996887205 and perplexity is 48.76785562862324
At time: 327.218585729599 and batch: 1000, loss is 3.8501687908172606 and perplexity is 47.00099589858956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.582540744688453 and perplexity of 97.7624683993253
Finished 43 epochs...
Completing Train Step...
At time: 328.38578748703003 and batch: 50, loss is 4.029357295036316 and perplexity is 56.224763697556995
At time: 328.72702956199646 and batch: 100, loss is 3.9053060674667357 and perplexity is 49.66527851213893
At time: 329.0824224948883 and batch: 150, loss is 3.9451378202438354 and perplexity is 51.68346064842974
At time: 329.4379069805145 and batch: 200, loss is 3.9739552688598634 and perplexity is 53.19451390054117
At time: 329.7861804962158 and batch: 250, loss is 3.9410125732421877 and perplexity is 51.47069276911736
At time: 330.12960267066956 and batch: 300, loss is 3.861306962966919 and perplexity is 47.5274273819933
At time: 330.4859297275543 and batch: 350, loss is 3.9361257934570313 and perplexity is 51.21978040422984
At time: 330.8271577358246 and batch: 400, loss is 3.8372693347930906 and perplexity is 46.398602247078244
At time: 331.18081736564636 and batch: 450, loss is 3.904126148223877 and perplexity is 49.60671205296134
At time: 331.52640533447266 and batch: 500, loss is 3.8340117645263674 and perplexity is 46.24770145838549
At time: 331.87529969215393 and batch: 550, loss is 3.8855671501159668 and perplexity is 48.69455175011619
At time: 332.2101082801819 and batch: 600, loss is 3.9212138414382935 and perplexity is 50.461660071759326
At time: 332.51652431488037 and batch: 650, loss is 3.886893563270569 and perplexity is 48.75918369897262
At time: 332.86717200279236 and batch: 700, loss is 3.863473892211914 and perplexity is 47.6305276194436
At time: 333.2159912586212 and batch: 750, loss is 3.8306748914718627 and perplexity is 46.0936359410493
At time: 333.5469946861267 and batch: 800, loss is 3.8258646059036256 and perplexity is 45.872444812051135
At time: 333.87645053863525 and batch: 850, loss is 3.817222318649292 and perplexity is 45.47771012904944
At time: 334.21338415145874 and batch: 900, loss is 3.9581485557556153 and perplexity is 52.360293987245974
At time: 334.5537042617798 and batch: 950, loss is 3.8871287631988527 and perplexity is 48.77065320424232
At time: 334.8974061012268 and batch: 1000, loss is 3.8502205324172976 and perplexity is 47.00342786823714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.582518042587653 and perplexity of 97.76024901110571
Finished 44 epochs...
Completing Train Step...
At time: 336.0437412261963 and batch: 50, loss is 4.0288794803619385 and perplexity is 56.197905097626474
At time: 336.4036626815796 and batch: 100, loss is 3.904754400253296 and perplexity is 49.63788736243173
At time: 336.7360460758209 and batch: 150, loss is 3.9445888805389404 and perplexity is 51.655097330381395
At time: 337.0873992443085 and batch: 200, loss is 3.9735920667648315 and perplexity is 53.175197049820966
At time: 337.44457602500916 and batch: 250, loss is 3.9405657052993774 and perplexity is 51.44769730487518
At time: 337.79639768600464 and batch: 300, loss is 3.8609287786483764 and perplexity is 47.50945665259496
At time: 338.13603687286377 and batch: 350, loss is 3.9358204412460327 and perplexity is 51.20414271865898
At time: 338.47644662857056 and batch: 400, loss is 3.8370730972290037 and perplexity is 46.38949799172391
At time: 338.8109221458435 and batch: 450, loss is 3.903856039047241 and perplexity is 49.59331463427743
At time: 339.16121435165405 and batch: 500, loss is 3.83377320766449 and perplexity is 46.23667006771582
At time: 339.4842505455017 and batch: 550, loss is 3.8853992557525636 and perplexity is 48.6863768956241
At time: 339.8278257846832 and batch: 600, loss is 3.921015772819519 and perplexity is 50.45166619021768
At time: 340.1845796108246 and batch: 650, loss is 3.8866939544677734 and perplexity is 48.74945190799706
At time: 340.523060798645 and batch: 700, loss is 3.863323426246643 and perplexity is 47.623361385279665
At time: 340.8779935836792 and batch: 750, loss is 3.830533604621887 and perplexity is 46.08712397646159
At time: 341.2067551612854 and batch: 800, loss is 3.8257190942764283 and perplexity is 45.86577032358263
At time: 341.54437494277954 and batch: 850, loss is 3.817123074531555 and perplexity is 45.47319695778755
At time: 341.88448429107666 and batch: 900, loss is 3.958094611167908 and perplexity is 52.357469508957934
At time: 342.22578835487366 and batch: 950, loss is 3.887158169746399 and perplexity is 48.77208740186194
At time: 342.5629127025604 and batch: 1000, loss is 3.850244736671448 and perplexity is 47.004565564919695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.582500922970656 and perplexity of 97.75857540741089
Finished 45 epochs...
Completing Train Step...
At time: 343.75284457206726 and batch: 50, loss is 4.0284220361709595 and perplexity is 56.1722035713495
At time: 344.11193013191223 and batch: 100, loss is 3.904224519729614 and perplexity is 49.61159217994945
At time: 344.4437823295593 and batch: 150, loss is 3.9440651082992555 and perplexity is 51.628048908586116
At time: 344.751225233078 and batch: 200, loss is 3.9732474088668823 and perplexity is 53.15687295613594
At time: 345.08740615844727 and batch: 250, loss is 3.940137119293213 and perplexity is 51.42565226619592
At time: 345.43412947654724 and batch: 300, loss is 3.860567817687988 and perplexity is 47.49231068819218
At time: 345.768652677536 and batch: 350, loss is 3.9355295515060424 and perplexity is 51.18925012505348
At time: 346.10743498802185 and batch: 400, loss is 3.8368851375579833 and perplexity is 46.38077945633477
At time: 346.4268288612366 and batch: 450, loss is 3.9035959577560426 and perplexity is 49.580418018129514
At time: 346.73286962509155 and batch: 500, loss is 3.833541040420532 and perplexity is 46.2259366734759
At time: 347.0775520801544 and batch: 550, loss is 3.8852348041534426 and perplexity is 48.6783710013973
At time: 347.4110562801361 and batch: 600, loss is 3.9208205842971804 and perplexity is 50.441819565049855
At time: 347.73361444473267 and batch: 650, loss is 3.8864934396743775 and perplexity is 48.73967790166875
At time: 348.08046221733093 and batch: 700, loss is 3.8631695747375487 and perplexity is 47.6160350228628
At time: 348.4265537261963 and batch: 750, loss is 3.830386757850647 and perplexity is 46.08035672799635
At time: 348.74724292755127 and batch: 800, loss is 3.825564923286438 and perplexity is 45.8586996974219
At time: 349.07633662223816 and batch: 850, loss is 3.8170115423202513 and perplexity is 45.468125514395695
At time: 349.4441921710968 and batch: 900, loss is 3.9580236721038817 and perplexity is 52.35375545081366
At time: 349.78766107559204 and batch: 950, loss is 3.8871659517288206 and perplexity is 48.77246694686557
At time: 350.14074897766113 and batch: 1000, loss is 3.850247373580933 and perplexity is 47.00468951186789
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.582486408512767 and perplexity of 97.75715650498219
Finished 46 epochs...
Completing Train Step...
At time: 351.34821128845215 and batch: 50, loss is 4.027981061935424 and perplexity is 56.14743853759353
At time: 351.68705582618713 and batch: 100, loss is 3.9037129878997803 and perplexity is 49.58622076111805
At time: 352.026083946228 and batch: 150, loss is 3.943562235832214 and perplexity is 51.60209311103788
At time: 352.3680763244629 and batch: 200, loss is 3.972917103767395 and perplexity is 53.13931786935267
At time: 352.73423981666565 and batch: 250, loss is 3.9397231578826903 and perplexity is 51.40436843629281
At time: 353.0975983142853 and batch: 300, loss is 3.8602205419540407 and perplexity is 47.47582062460627
At time: 353.46130108833313 and batch: 350, loss is 3.935250415802002 and perplexity is 51.17496337174477
At time: 353.7990930080414 and batch: 400, loss is 3.836703186035156 and perplexity is 46.37234117058553
At time: 354.1422452926636 and batch: 450, loss is 3.90334379196167 and perplexity is 49.567917108851745
At time: 354.4833972454071 and batch: 500, loss is 3.8333134937286375 and perplexity is 46.215419311147
At time: 354.83399057388306 and batch: 550, loss is 3.8850725317001342 and perplexity is 48.670472483585115
At time: 355.16667079925537 and batch: 600, loss is 3.9206275081634523 and perplexity is 50.43208139368451
At time: 355.51987504959106 and batch: 650, loss is 3.8862918186187745 and perplexity is 48.729851946953424
At time: 355.8804864883423 and batch: 700, loss is 3.863012504577637 and perplexity is 47.60855655196508
At time: 356.220662355423 and batch: 750, loss is 3.830235505104065 and perplexity is 46.07338747455041
At time: 356.5556757450104 and batch: 800, loss is 3.825404100418091 and perplexity is 45.851325162810696
At time: 356.8925426006317 and batch: 850, loss is 3.8168907499313356 and perplexity is 45.46263364259013
At time: 357.2306590080261 and batch: 900, loss is 3.9579391717910766 and perplexity is 52.349331729007105
At time: 357.562349319458 and batch: 950, loss is 3.8871564054489136 and perplexity is 48.772001353466685
At time: 357.9045286178589 and batch: 1000, loss is 3.8502330684661867 and perplexity is 47.00401710920023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.58247524354516 and perplexity of 97.75606505558943
Finished 47 epochs...
Completing Train Step...
At time: 359.0022587776184 and batch: 50, loss is 4.02755419254303 and perplexity is 56.123476029414995
At time: 359.36195945739746 and batch: 100, loss is 3.9032172107696534 and perplexity is 49.56164313990994
At time: 359.70984506607056 and batch: 150, loss is 3.9430769157409666 and perplexity is 51.57705565458242
At time: 360.0532805919647 and batch: 200, loss is 3.97259868144989 and perplexity is 53.12239981829154
At time: 360.39130783081055 and batch: 250, loss is 3.939321422576904 and perplexity is 51.38372163417269
At time: 360.7364568710327 and batch: 300, loss is 3.859884271621704 and perplexity is 47.459858598555186
At time: 361.0851902961731 and batch: 350, loss is 3.9349804162979125 and perplexity is 51.161148022165285
At time: 361.4348168373108 and batch: 400, loss is 3.836525726318359 and perplexity is 46.36411267818875
At time: 361.79685497283936 and batch: 450, loss is 3.9030977058410645 and perplexity is 49.555720633177174
At time: 362.14496207237244 and batch: 500, loss is 3.8330893611907957 and perplexity is 46.20506209266756
At time: 362.49522066116333 and batch: 550, loss is 3.884911379814148 and perplexity is 48.662629777102985
At time: 362.8546259403229 and batch: 600, loss is 3.9204360818862916 and perplexity is 50.42242829205198
At time: 363.1999363899231 and batch: 650, loss is 3.886089344024658 and perplexity is 48.719986388755274
At time: 363.53770685195923 and batch: 700, loss is 3.862852940559387 and perplexity is 47.60096054541935
At time: 363.87686133384705 and batch: 750, loss is 3.8300805854797364 and perplexity is 46.06625035552548
At time: 364.2186357975006 and batch: 800, loss is 3.8252380514144897 and perplexity is 45.843712228031215
At time: 364.5806038379669 and batch: 850, loss is 3.816762523651123 and perplexity is 45.456804511921334
At time: 364.91985726356506 and batch: 900, loss is 3.9578436517715456 and perplexity is 52.34433155862991
At time: 365.2506592273712 and batch: 950, loss is 3.8871326351165774 and perplexity is 48.77084204056449
At time: 365.5863881111145 and batch: 1000, loss is 3.850205044746399 and perplexity is 47.00269990025249
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.582468544564596 and perplexity of 97.75541019180307
Finished 48 epochs...
Completing Train Step...
At time: 366.8028199672699 and batch: 50, loss is 4.027139387130737 and perplexity is 56.10020053553548
At time: 367.1688632965088 and batch: 100, loss is 3.9027350521087647 and perplexity is 49.53775232446679
At time: 367.5155282020569 and batch: 150, loss is 3.9426063394546507 and perplexity is 51.55279042504196
At time: 367.85620975494385 and batch: 200, loss is 3.972290062904358 and perplexity is 53.1060077900965
At time: 368.1904239654541 and batch: 250, loss is 3.938930106163025 and perplexity is 51.36361827413495
At time: 368.53195881843567 and batch: 300, loss is 3.8595569276809694 and perplexity is 47.444325443895565
At time: 368.87323784828186 and batch: 350, loss is 3.9347180986404418 and perplexity is 51.14772930972215
At time: 369.22021222114563 and batch: 400, loss is 3.836351890563965 and perplexity is 46.35605363817945
At time: 369.5624985694885 and batch: 450, loss is 3.9028564643859864 and perplexity is 49.54376718091627
At time: 369.908251285553 and batch: 500, loss is 3.832867674827576 and perplexity is 46.19482019577627
At time: 370.245906829834 and batch: 550, loss is 3.8847508907318113 and perplexity is 48.654820582967936
At time: 370.6187672615051 and batch: 600, loss is 3.9202458381652834 and perplexity is 50.412836654074816
At time: 370.9564619064331 and batch: 650, loss is 3.8858856773376464 and perplexity is 48.710064760923046
At time: 371.2929108142853 and batch: 700, loss is 3.8626907014846803 and perplexity is 47.59323843605619
At time: 371.6357901096344 and batch: 750, loss is 3.829922637939453 and perplexity is 46.05897487917868
At time: 371.9943063259125 and batch: 800, loss is 3.8250678062438963 and perplexity is 45.835908221738364
At time: 372.3394331932068 and batch: 850, loss is 3.8166286325454712 and perplexity is 45.45071865753581
At time: 372.6840555667877 and batch: 900, loss is 3.9577389574050903 and perplexity is 52.3388516888606
At time: 373.01516246795654 and batch: 950, loss is 3.88709707736969 and perplexity is 48.76910789013915
At time: 373.3682470321655 and batch: 1000, loss is 3.8501655530929564 and perplexity is 47.000843722569165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.582461845584032 and perplexity of 97.7547553324036
Finished 49 epochs...
Completing Train Step...
At time: 374.4872534275055 and batch: 50, loss is 4.0267347240447995 and perplexity is 56.077503447911525
At time: 374.82344698905945 and batch: 100, loss is 3.9022647190093993 and perplexity is 49.51445855822447
At time: 375.1554946899414 and batch: 150, loss is 3.9421486282348632 and perplexity is 51.52919953377273
At time: 375.49110412597656 and batch: 200, loss is 3.971989674568176 and perplexity is 53.09005776049655
At time: 375.82590222358704 and batch: 250, loss is 3.938547496795654 and perplexity is 51.34396983171996
At time: 376.1799168586731 and batch: 300, loss is 3.8592370843887327 and perplexity is 47.42915312115981
At time: 376.5165386199951 and batch: 350, loss is 3.9344620084762574 and perplexity is 51.134632556372
At time: 376.85931491851807 and batch: 400, loss is 3.836180830001831 and perplexity is 46.348124623775966
At time: 377.1947684288025 and batch: 450, loss is 3.902619366645813 and perplexity is 49.532021858127756
At time: 377.5274760723114 and batch: 500, loss is 3.8326482915878297 and perplexity is 46.184686938036336
At time: 377.8668141365051 and batch: 550, loss is 3.8845908212661744 and perplexity is 48.647033055125895
At time: 378.22575521469116 and batch: 600, loss is 3.920056395530701 and perplexity is 50.40328721804601
At time: 378.5754008293152 and batch: 650, loss is 3.885681300163269 and perplexity is 48.70011055276458
At time: 378.90884280204773 and batch: 700, loss is 3.8625263738632203 and perplexity is 47.58541819494482
At time: 379.2912435531616 and batch: 750, loss is 3.8297622203826904 and perplexity is 46.051586803565925
At time: 379.62314200401306 and batch: 800, loss is 3.824894108772278 and perplexity is 45.82794733178416
At time: 379.95820808410645 and batch: 850, loss is 3.8164902544021606 and perplexity is 45.44442970661256
At time: 380.3080904483795 and batch: 900, loss is 3.957626724243164 and perplexity is 52.33297786366912
At time: 380.66280245780945 and batch: 950, loss is 3.8870515871047973 and perplexity is 48.7668894209624
At time: 380.998140335083 and batch: 1000, loss is 3.850116500854492 and perplexity is 46.99853828251882
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.58245849609375 and perplexity of 97.754427904349
Finished Training.
Improved accuracyfrom -108.74266249264693 to -97.754427904349
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f498fb76b70>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -108.74266249264693, 'params': {'wordvec_dim': 200, 'num_layers': 1, 'wordvec_source': 'glove', 'data': 'ptb', 'dropout': 0.3681258570530319, 'seq_len': 20, 'batch_size': 50, 'lr': 12.725560315816402, 'anneal': 6.578833205369682, 'tune_wordvecs': True}}, {'best_accuracy': -153.14247861197538, 'params': {'wordvec_dim': 200, 'num_layers': 1, 'wordvec_source': 'glove', 'data': 'ptb', 'dropout': 0.7063518846167919, 'seq_len': 20, 'batch_size': 50, 'lr': 16.819278223872786, 'anneal': 5.500730385800407, 'tune_wordvecs': True}}, {'best_accuracy': -183.30794503335574, 'params': {'wordvec_dim': 200, 'num_layers': 1, 'wordvec_source': 'glove', 'data': 'ptb', 'dropout': 0.06800583424510376, 'seq_len': 20, 'batch_size': 50, 'lr': 23.27018546471844, 'anneal': 6.218656461582015, 'tune_wordvecs': True}}, {'best_accuracy': -141.99416766076925, 'params': {'wordvec_dim': 200, 'num_layers': 1, 'wordvec_source': 'glove', 'data': 'ptb', 'dropout': 0.2460807837958705, 'seq_len': 20, 'batch_size': 50, 'lr': 18.419787295130018, 'anneal': 5.075412414399972, 'tune_wordvecs': True}}, {'best_accuracy': -148.459342136782, 'params': {'wordvec_dim': 200, 'num_layers': 1, 'wordvec_source': 'glove', 'data': 'ptb', 'dropout': 0.2685505938429682, 'seq_len': 20, 'batch_size': 50, 'lr': 19.00591146735055, 'anneal': 4.740145085532791, 'tune_wordvecs': True}}, {'best_accuracy': -97.754427904349, 'params': {'wordvec_dim': 200, 'num_layers': 1, 'wordvec_source': 'glove', 'data': 'ptb', 'dropout': 0.17614170834692716, 'seq_len': 20, 'batch_size': 50, 'lr': 11.571182403989823, 'anneal': 6.897572663966025, 'tune_wordvecs': True}}]
