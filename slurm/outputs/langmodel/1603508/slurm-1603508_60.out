Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'domain': [0, 30], 'name': 'lr', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [2, 8], 'name': 'anneal', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'lr': 10.023112699705816, 'anneal': 2.2869837742887213, 'wordvec_source': 'glove', 'seq_len': 50, 'tune_wordvecs': True, 'num_layers': 1, 'dropout': 0.4064131726922323, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.327049970626831 and batch: 50, loss is 7.4086723899841305 and perplexity is 1650.234024692342
At time: 3.881600856781006 and batch: 100, loss is 6.488627490997314 and perplexity is 657.6201539439442
At time: 5.444767951965332 and batch: 150, loss is 6.089512414932251 and perplexity is 441.2062331203704
At time: 7.003372430801392 and batch: 200, loss is 5.94756594657898 and perplexity is 382.82039880715985
At time: 8.563032388687134 and batch: 250, loss is 5.919461317062378 and perplexity is 372.2111560635323
At time: 10.10980749130249 and batch: 300, loss is 5.818239440917969 and perplexity is 336.379316348267
At time: 11.663532972335815 and batch: 350, loss is 5.766605482101441 and perplexity is 319.4515062272601
At time: 13.211609125137329 and batch: 400, loss is 5.757279891967773 and perplexity is 316.4862801437685
At time: 14.753822088241577 and batch: 450, loss is 5.744086589813232 and perplexity is 312.3382045978586
At time: 16.294536590576172 and batch: 500, loss is 5.732040843963623 and perplexity is 308.598427389073
At time: 17.83546018600464 and batch: 550, loss is 5.6985751914978025 and perplexity is 298.44187537136355
At time: 19.374993085861206 and batch: 600, loss is 5.718582067489624 and perplexity is 304.47289471989956
At time: 20.914889812469482 and batch: 650, loss is 5.717950191497803 and perplexity is 304.2805663777958
At time: 22.45694589614868 and batch: 700, loss is 5.663620700836182 and perplexity is 288.1902063438416
At time: 24.000006437301636 and batch: 750, loss is 5.657695350646972 and perplexity is 286.4876276191647
At time: 25.543823719024658 and batch: 800, loss is 5.658643808364868 and perplexity is 286.7594779195404
At time: 27.088610887527466 and batch: 850, loss is 5.645606327056885 and perplexity is 283.04512212086314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.081322352091472 and perplexity of 160.9867964656658
Finished 1 epochs...
Completing Train Step...
At time: 31.256030559539795 and batch: 50, loss is 5.359743270874024 and perplexity is 212.67034076970555
At time: 32.80134081840515 and batch: 100, loss is 5.225204181671143 and perplexity is 185.89912384490677
At time: 34.34031891822815 and batch: 150, loss is 5.1976722145080565 and perplexity is 180.85076971912238
At time: 35.883914947509766 and batch: 200, loss is 5.189521799087524 and perplexity is 179.3827514228747
At time: 37.42853617668152 and batch: 250, loss is 5.1971065711975095 and perplexity is 180.74850161738405
At time: 38.9702730178833 and batch: 300, loss is 5.145038251876831 and perplexity is 171.57804772788535
At time: 40.50859069824219 and batch: 350, loss is 5.102087326049805 and perplexity is 164.36463206447638
At time: 42.094221115112305 and batch: 400, loss is 5.097956256866455 and perplexity is 163.68703097026795
At time: 43.6334125995636 and batch: 450, loss is 5.088525247573853 and perplexity is 162.15055371409366
At time: 45.17593789100647 and batch: 500, loss is 5.084087886810303 and perplexity is 161.43262723665342
At time: 46.722702980041504 and batch: 550, loss is 5.077800731658936 and perplexity is 160.42085916692838
At time: 48.265517473220825 and batch: 600, loss is 5.102893867492676 and perplexity is 164.49725242669854
At time: 49.81010603904724 and batch: 650, loss is 5.10042296409607 and perplexity is 164.09129735137418
At time: 51.34725213050842 and batch: 700, loss is 5.047113857269287 and perplexity is 155.572810579495
At time: 52.88674211502075 and batch: 750, loss is 5.03276273727417 and perplexity is 153.3561106153211
At time: 54.42361617088318 and batch: 800, loss is 5.0163046073913575 and perplexity is 150.85281213967303
At time: 55.95576572418213 and batch: 850, loss is 5.002774181365967 and perplexity is 148.82545575144118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.818826039632161 and perplexity of 123.81964606612178
Finished 2 epochs...
Completing Train Step...
At time: 59.99495887756348 and batch: 50, loss is 4.965394821166992 and perplexity is 143.36514269361032
At time: 61.54495930671692 and batch: 100, loss is 4.868136529922485 and perplexity is 130.0782938979064
At time: 63.088892698287964 and batch: 150, loss is 4.88379132270813 and perplexity is 132.13066544561693
At time: 64.631671667099 and batch: 200, loss is 4.891248035430908 and perplexity is 133.1196084091942
At time: 66.17415881156921 and batch: 250, loss is 4.888260803222656 and perplexity is 132.7225425865813
At time: 67.71645593643188 and batch: 300, loss is 4.868512363433838 and perplexity is 130.12719086783716
At time: 69.26538610458374 and batch: 350, loss is 4.814519300460815 and perplexity is 123.28753380305808
At time: 70.81410241127014 and batch: 400, loss is 4.827665224075317 and perplexity is 124.91896212653357
At time: 72.3650803565979 and batch: 450, loss is 4.844150590896606 and perplexity is 126.99536514820419
At time: 73.94527626037598 and batch: 500, loss is 4.838042001724244 and perplexity is 126.22196722574203
At time: 75.48203778266907 and batch: 550, loss is 4.844112997055054 and perplexity is 126.99059099430883
At time: 77.01996922492981 and batch: 600, loss is 4.884256734848022 and perplexity is 132.19217497389766
At time: 78.55879139900208 and batch: 650, loss is 4.868938026428222 and perplexity is 130.1825929880325
At time: 80.12283086776733 and batch: 700, loss is 4.828823537826538 and perplexity is 125.06374131182616
At time: 81.66556596755981 and batch: 750, loss is 4.816450567245483 and perplexity is 123.52586498852956
At time: 83.20366191864014 and batch: 800, loss is 4.796876659393311 and perplexity is 121.13149116364235
At time: 84.74172639846802 and batch: 850, loss is 4.797748994827271 and perplexity is 121.2372045576188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.745039304097493 and perplexity of 115.01232586994324
Finished 3 epochs...
Completing Train Step...
At time: 88.73108434677124 and batch: 50, loss is 4.767524003982544 and perplexity is 117.62763554433315
At time: 90.31153845787048 and batch: 100, loss is 4.700131645202637 and perplexity is 109.96164742267963
At time: 91.85374641418457 and batch: 150, loss is 4.728875722885132 and perplexity is 113.16825831660941
At time: 93.3931815624237 and batch: 200, loss is 4.734782648086548 and perplexity is 113.83871296695105
At time: 94.92709016799927 and batch: 250, loss is 4.7264950466156 and perplexity is 112.8991617725439
At time: 96.46138048171997 and batch: 300, loss is 4.7228950500488285 and perplexity is 112.49345588583242
At time: 98.00410413742065 and batch: 350, loss is 4.659595413208008 and perplexity is 105.59335183770531
At time: 99.54735732078552 and batch: 400, loss is 4.674998979568482 and perplexity is 107.23245766860909
At time: 101.08199644088745 and batch: 450, loss is 4.709596853256226 and perplexity is 111.00739861233521
At time: 102.61482095718384 and batch: 500, loss is 4.6909499168396 and perplexity is 108.95663040291139
At time: 104.15442633628845 and batch: 550, loss is 4.698542070388794 and perplexity is 109.78699400655901
At time: 105.69048166275024 and batch: 600, loss is 4.752223901748657 and perplexity is 115.84161865651609
At time: 107.22372722625732 and batch: 650, loss is 4.726630973815918 and perplexity is 112.91450888254307
At time: 108.75687980651855 and batch: 700, loss is 4.697438259124755 and perplexity is 109.66587674352665
At time: 110.29006791114807 and batch: 750, loss is 4.685235471725464 and perplexity is 108.33577931786158
At time: 111.82399201393127 and batch: 800, loss is 4.665445184707641 and perplexity is 106.2128590390032
At time: 113.40226864814758 and batch: 850, loss is 4.6741788291931154 and perplexity is 107.1445469831193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.712730725606282 and perplexity of 111.35582730986432
Finished 4 epochs...
Completing Train Step...
At time: 117.38638114929199 and batch: 50, loss is 4.649411334991455 and perplexity is 104.52343817286467
At time: 118.94839763641357 and batch: 100, loss is 4.587168340682983 and perplexity is 98.2159219973203
At time: 120.48329377174377 and batch: 150, loss is 4.61109489440918 and perplexity is 100.59422942191141
At time: 122.02158164978027 and batch: 200, loss is 4.62681209564209 and perplexity is 102.18777943726306
At time: 123.55684208869934 and batch: 250, loss is 4.619047288894653 and perplexity is 101.39738368434321
At time: 125.09261560440063 and batch: 300, loss is 4.6172580146789555 and perplexity is 101.21611817538846
At time: 126.62881922721863 and batch: 350, loss is 4.55893367767334 and perplexity is 95.48161135343156
At time: 128.16470050811768 and batch: 400, loss is 4.579038066864014 and perplexity is 97.42063698135125
At time: 129.70375156402588 and batch: 450, loss is 4.601447725296021 and perplexity is 99.62844590758806
At time: 131.23886442184448 and batch: 500, loss is 4.594876108169555 and perplexity is 98.97587248602485
At time: 132.7755675315857 and batch: 550, loss is 4.60016058921814 and perplexity is 99.50029303326399
At time: 134.31381249427795 and batch: 600, loss is 4.652053728103637 and perplexity is 104.7999954115098
At time: 135.8524341583252 and batch: 650, loss is 4.629537773132324 and perplexity is 102.46669030531324
At time: 137.39948654174805 and batch: 700, loss is 4.6092869091033934 and perplexity is 100.41252084598484
At time: 138.95053553581238 and batch: 750, loss is 4.607463464736939 and perplexity is 100.22959103238179
At time: 140.50459003448486 and batch: 800, loss is 4.577250003814697 and perplexity is 97.24659838248716
At time: 142.0528552532196 and batch: 850, loss is 4.588929252624512 and perplexity is 98.3890239511327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.696433067321777 and perplexity of 109.55569688837852
Finished 5 epochs...
Completing Train Step...
At time: 146.0805377960205 and batch: 50, loss is 4.555835647583008 and perplexity is 95.18626418182089
At time: 147.621022939682 and batch: 100, loss is 4.50501971244812 and perplexity is 90.47012741898492
At time: 149.17108273506165 and batch: 150, loss is 4.524013910293579 and perplexity is 92.20495862453662
At time: 150.71393036842346 and batch: 200, loss is 4.550001754760742 and perplexity is 94.63257437230479
At time: 152.2849371433258 and batch: 250, loss is 4.534942712783813 and perplexity is 93.21817494171765
At time: 153.8324146270752 and batch: 300, loss is 4.539202165603638 and perplexity is 93.61608018754538
At time: 155.3727686405182 and batch: 350, loss is 4.488422384262085 and perplexity is 88.98095733066508
At time: 156.91415739059448 and batch: 400, loss is 4.502751979827881 and perplexity is 90.26519781048151
At time: 158.45987796783447 and batch: 450, loss is 4.532346525192261 and perplexity is 92.97647695504125
At time: 160.01128768920898 and batch: 500, loss is 4.515301389694214 and perplexity is 91.40511042966149
At time: 161.55366253852844 and batch: 550, loss is 4.524670362472534 and perplexity is 92.26550664180076
At time: 163.1088948249817 and batch: 600, loss is 4.581462574005127 and perplexity is 97.65712057364134
At time: 164.65386962890625 and batch: 650, loss is 4.555766544342041 and perplexity is 95.17968672973466
At time: 166.19419932365417 and batch: 700, loss is 4.5406435680389405 and perplexity is 93.75111593055595
At time: 167.73343110084534 and batch: 750, loss is 4.536498756408691 and perplexity is 93.36333940038347
At time: 169.27281403541565 and batch: 800, loss is 4.505106372833252 and perplexity is 90.47796793479614
At time: 170.81257438659668 and batch: 850, loss is 4.524016447067261 and perplexity is 92.20519252794567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.696515401204427 and perplexity of 109.56471740561172
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 174.85277843475342 and batch: 50, loss is 4.4698031044006346 and perplexity is 87.33952455231089
At time: 176.39727091789246 and batch: 100, loss is 4.387593860626221 and perplexity is 80.44662014026001
At time: 177.93693494796753 and batch: 150, loss is 4.380424537658691 and perplexity is 79.87193485257224
At time: 179.47506070137024 and batch: 200, loss is 4.4049634265899655 and perplexity is 81.85614902991864
At time: 181.01434206962585 and batch: 250, loss is 4.375659475326538 and perplexity is 79.49224544468737
At time: 182.55336666107178 and batch: 300, loss is 4.37430477142334 and perplexity is 79.3846298995672
At time: 184.09639191627502 and batch: 350, loss is 4.310510587692261 and perplexity is 74.47850704276593
At time: 185.63758301734924 and batch: 400, loss is 4.307737007141113 and perplexity is 74.27222111171888
At time: 187.1794891357422 and batch: 450, loss is 4.32828127861023 and perplexity is 75.81387160502808
At time: 188.72105240821838 and batch: 500, loss is 4.300226545333862 and perplexity is 73.7164919353473
At time: 190.32183504104614 and batch: 550, loss is 4.308521947860718 and perplexity is 74.3305432891364
At time: 191.88200855255127 and batch: 600, loss is 4.3455831241607665 and perplexity is 77.13700481845578
At time: 193.44518661499023 and batch: 650, loss is 4.305959138870239 and perplexity is 74.14029219726281
At time: 195.0062997341156 and batch: 700, loss is 4.294550542831421 and perplexity is 73.29926216142871
At time: 196.56911897659302 and batch: 750, loss is 4.281316776275634 and perplexity is 72.33562716120764
At time: 198.12735414505005 and batch: 800, loss is 4.240054006576538 and perplexity is 69.41160042044099
At time: 199.6934473514557 and batch: 850, loss is 4.260384292602539 and perplexity is 70.83720043641144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.563451766967773 and perplexity of 95.91398180816147
Finished 7 epochs...
Completing Train Step...
At time: 203.78634762763977 and batch: 50, loss is 4.322904653549195 and perplexity is 75.40734269906527
At time: 205.3305504322052 and batch: 100, loss is 4.274413976669312 and perplexity is 71.83802821125869
At time: 206.86700582504272 and batch: 150, loss is 4.275113706588745 and perplexity is 71.88831301979201
At time: 208.40273904800415 and batch: 200, loss is 4.307875785827637 and perplexity is 74.28252922826687
At time: 209.94194293022156 and batch: 250, loss is 4.291094827651977 and perplexity is 73.04639795351756
At time: 211.48055696487427 and batch: 300, loss is 4.295797176361084 and perplexity is 73.39069646002847
At time: 213.01881647109985 and batch: 350, loss is 4.238432874679566 and perplexity is 69.2991662209368
At time: 214.5596992969513 and batch: 400, loss is 4.238050899505615 and perplexity is 69.27270071476988
At time: 216.09788179397583 and batch: 450, loss is 4.266599464416504 and perplexity is 71.27883680927248
At time: 217.63558840751648 and batch: 500, loss is 4.242686882019043 and perplexity is 69.59459331164823
At time: 219.17245316505432 and batch: 550, loss is 4.256470584869385 and perplexity is 70.56050614075043
At time: 220.70955991744995 and batch: 600, loss is 4.294926643371582 and perplexity is 73.32683523832058
At time: 222.24841117858887 and batch: 650, loss is 4.26393196105957 and perplexity is 71.08895364239193
At time: 223.78930139541626 and batch: 700, loss is 4.256451148986816 and perplexity is 70.55913474836622
At time: 225.32950448989868 and batch: 750, loss is 4.246989612579346 and perplexity is 69.89468523949759
At time: 226.86856532096863 and batch: 800, loss is 4.212033314704895 and perplexity is 67.49363618578775
At time: 228.40962767601013 and batch: 850, loss is 4.2327532386779785 and perplexity is 68.90668780391746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.558238347371419 and perplexity of 95.41524317235894
Finished 8 epochs...
Completing Train Step...
At time: 232.4588212966919 and batch: 50, loss is 4.264042663574219 and perplexity is 71.09682380393917
At time: 234.02454710006714 and batch: 100, loss is 4.2197157001495365 and perplexity is 68.01414512920012
At time: 235.56144261360168 and batch: 150, loss is 4.227289338111877 and perplexity is 68.53121521996876
At time: 237.1058485507965 and batch: 200, loss is 4.259510679244995 and perplexity is 70.77534313551547
At time: 238.64438009262085 and batch: 250, loss is 4.2453046321868895 and perplexity is 69.77701323068673
At time: 240.1820993423462 and batch: 300, loss is 4.251061334609985 and perplexity is 70.1798571455914
At time: 241.7193684577942 and batch: 350, loss is 4.194693398475647 and perplexity is 66.33339054050255
At time: 243.26004576683044 and batch: 400, loss is 4.195000710487366 and perplexity is 66.35377872079958
At time: 244.79847931861877 and batch: 450, loss is 4.225399932861328 and perplexity is 68.40185422825742
At time: 246.34480929374695 and batch: 500, loss is 4.204506778717041 and perplexity is 66.98754983175671
At time: 247.88414359092712 and batch: 550, loss is 4.219406499862671 and perplexity is 67.99311838692012
At time: 249.4318082332611 and batch: 600, loss is 4.2558832931518555 and perplexity is 70.51907870609465
At time: 250.9730076789856 and batch: 650, loss is 4.231259336471558 and perplexity is 68.80382480373184
At time: 252.51438689231873 and batch: 700, loss is 4.227356786727905 and perplexity is 68.53583771147866
At time: 254.05809831619263 and batch: 750, loss is 4.217957391738891 and perplexity is 67.8946603620958
At time: 255.60579085350037 and batch: 800, loss is 4.1850712823867795 and perplexity is 65.69818387318311
At time: 257.1517333984375 and batch: 850, loss is 4.204376268386841 and perplexity is 66.97880783498168
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.559624989827474 and perplexity of 95.54764177304251
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 261.2479782104492 and batch: 50, loss is 4.2259407806396485 and perplexity is 68.43885922528294
At time: 262.8178985118866 and batch: 100, loss is 4.179072179794312 and perplexity is 65.30523358104476
At time: 264.361656665802 and batch: 150, loss is 4.176104025840759 and perplexity is 65.1116849769267
At time: 265.9077799320221 and batch: 200, loss is 4.206814966201782 and perplexity is 67.14234823905802
At time: 267.45115876197815 and batch: 250, loss is 4.187525229454041 and perplexity is 65.85960171323778
At time: 269.02395391464233 and batch: 300, loss is 4.186130475997925 and perplexity is 65.7678078359245
At time: 270.56662487983704 and batch: 350, loss is 4.123963007926941 and perplexity is 61.803686075955746
At time: 272.11198830604553 and batch: 400, loss is 4.117673678398132 and perplexity is 61.416202112560164
At time: 273.6568169593811 and batch: 450, loss is 4.152370958328247 and perplexity is 63.584578105739
At time: 275.200576543808 and batch: 500, loss is 4.12174684047699 and perplexity is 61.66687041778773
At time: 276.7496099472046 and batch: 550, loss is 4.1325719356536865 and perplexity is 62.3380463769578
At time: 278.30497908592224 and batch: 600, loss is 4.15808027267456 and perplexity is 63.94864073479474
At time: 279.8555727005005 and batch: 650, loss is 4.1324489974975585 and perplexity is 62.33038312354236
At time: 281.41065549850464 and batch: 700, loss is 4.118232154846192 and perplexity is 61.45051119448466
At time: 282.9586615562439 and batch: 750, loss is 4.10109808921814 and perplexity is 60.40658300906891
At time: 284.51399397850037 and batch: 800, loss is 4.062543058395386 and perplexity is 58.12193076064428
At time: 286.0677239894867 and batch: 850, loss is 4.082976684570313 and perplexity is 59.32178955126332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.500630060831706 and perplexity of 90.0738654402396
Finished 10 epochs...
Completing Train Step...
At time: 290.12632417678833 and batch: 50, loss is 4.16973075389862 and perplexity is 64.69803006973721
At time: 291.6980628967285 and batch: 100, loss is 4.124694671630859 and perplexity is 61.84892213659971
At time: 293.2386300563812 and batch: 150, loss is 4.125182461738587 and perplexity is 61.879098788329294
At time: 294.77731227874756 and batch: 200, loss is 4.161180491447449 and perplexity is 64.14720314572061
At time: 296.3196177482605 and batch: 250, loss is 4.142764368057251 and perplexity is 62.97667174511372
At time: 297.8580689430237 and batch: 300, loss is 4.147226443290711 and perplexity is 63.25830626231419
At time: 299.3989040851593 and batch: 350, loss is 4.08717200756073 and perplexity is 59.57118640317647
At time: 300.9384503364563 and batch: 400, loss is 4.085748491287231 and perplexity is 59.486446178753965
At time: 302.47753834724426 and batch: 450, loss is 4.125762538909912 and perplexity is 61.915003853766194
At time: 304.0158486366272 and batch: 500, loss is 4.098057780265808 and perplexity is 60.22320723472335
At time: 305.5572621822357 and batch: 550, loss is 4.110219979286194 and perplexity is 60.960126059790035
At time: 307.0940809249878 and batch: 600, loss is 4.141308898925781 and perplexity is 62.885077815623816
At time: 308.65693759918213 and batch: 650, loss is 4.116549053192139 and perplexity is 61.347170728101275
At time: 310.1958930492401 and batch: 700, loss is 4.106073522567749 and perplexity is 60.707880858842636
At time: 311.7331347465515 and batch: 750, loss is 4.0941916847229 and perplexity is 59.990828051158125
At time: 313.2730746269226 and batch: 800, loss is 4.056663022041321 and perplexity is 57.781174506183795
At time: 314.81049609184265 and batch: 850, loss is 4.076682066917419 and perplexity is 58.949554335796236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.499003728230794 and perplexity of 89.92749443261683
Finished 11 epochs...
Completing Train Step...
At time: 318.84109115600586 and batch: 50, loss is 4.1443402957916256 and perplexity is 63.07599667263292
At time: 320.3797152042389 and batch: 100, loss is 4.09789402961731 and perplexity is 60.21334645286074
At time: 321.92286944389343 and batch: 150, loss is 4.10179491519928 and perplexity is 60.44869055465357
At time: 323.4671542644501 and batch: 200, loss is 4.137307362556458 and perplexity is 62.633943685936295
At time: 325.0140872001648 and batch: 250, loss is 4.120195398330688 and perplexity is 61.571272012863524
At time: 326.57111978530884 and batch: 300, loss is 4.125633883476257 and perplexity is 61.907038664489065
At time: 328.11515522003174 and batch: 350, loss is 4.065730590820312 and perplexity is 58.307491883477056
At time: 329.65649485588074 and batch: 400, loss is 4.066548385620117 and perplexity is 58.35519495013036
At time: 331.19549226760864 and batch: 450, loss is 4.109456362724305 and perplexity is 60.913593666630064
At time: 332.73882269859314 and batch: 500, loss is 4.0818088388442995 and perplexity is 59.252551290549135
At time: 334.2786853313446 and batch: 550, loss is 4.094759907722473 and perplexity is 60.024925906095476
At time: 335.823362827301 and batch: 600, loss is 4.128401951789856 and perplexity is 62.07863896771629
At time: 337.36689162254333 and batch: 650, loss is 4.103629450798035 and perplexity is 60.55968761227076
At time: 338.9181492328644 and batch: 700, loss is 4.095773372650147 and perplexity is 60.08578989983951
At time: 340.46481823921204 and batch: 750, loss is 4.0857933282852175 and perplexity is 59.489113432216875
At time: 342.00800943374634 and batch: 800, loss is 4.04779462814331 and perplexity is 57.27101378748204
At time: 343.5491359233856 and batch: 850, loss is 4.066234345436096 and perplexity is 58.336871951199214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.499538421630859 and perplexity of 89.97559092766771
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 347.58134174346924 and batch: 50, loss is 4.1355771589279176 and perplexity is 62.52566790588612
At time: 349.12888860702515 and batch: 100, loss is 4.087196760177612 and perplexity is 59.57266096418029
At time: 350.68265295028687 and batch: 150, loss is 4.0898932838439945 and perplexity is 59.733516832699294
At time: 352.23339891433716 and batch: 200, loss is 4.127325406074524 and perplexity is 62.011844435056894
At time: 353.7830455303192 and batch: 250, loss is 4.103123302459717 and perplexity is 60.529043182984445
At time: 355.33143877983093 and batch: 300, loss is 4.104580626487732 and perplexity is 60.61731791883148
At time: 356.88968324661255 and batch: 350, loss is 4.036956448554992 and perplexity is 56.65365183793679
At time: 358.4348497390747 and batch: 400, loss is 4.034941182136536 and perplexity is 56.5395946023649
At time: 359.9822826385498 and batch: 450, loss is 4.0818035364151 and perplexity is 59.25223710892398
At time: 361.53000688552856 and batch: 500, loss is 4.04835196018219 and perplexity is 57.30294165475016
At time: 363.07524061203003 and batch: 550, loss is 4.059065904617309 and perplexity is 57.92018282707657
At time: 364.6221580505371 and batch: 600, loss is 4.087297449111938 and perplexity is 59.57865957391949
At time: 366.16874861717224 and batch: 650, loss is 4.0577902460098265 and perplexity is 57.846343554171504
At time: 367.71972823143005 and batch: 700, loss is 4.043347320556641 and perplexity is 57.01687750335129
At time: 369.26324486732483 and batch: 750, loss is 4.032213411331177 and perplexity is 56.38557770376138
At time: 370.80928683280945 and batch: 800, loss is 3.9917953252792358 and perplexity is 54.15202263949
At time: 372.34946942329407 and batch: 850, loss is 4.010889019966125 and perplexity is 55.195919031767396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.480075200398763 and perplexity of 88.24130820771559
Finished 13 epochs...
Completing Train Step...
At time: 376.4040310382843 and batch: 50, loss is 4.1121343326568605 and perplexity is 61.07693705567711
At time: 377.94599771499634 and batch: 100, loss is 4.061120281219482 and perplexity is 58.03929500421258
At time: 379.4924304485321 and batch: 150, loss is 4.064778380393982 and perplexity is 58.25199730722259
At time: 381.0426695346832 and batch: 200, loss is 4.105336346626282 and perplexity is 60.66314496075581
At time: 382.5932967662811 and batch: 250, loss is 4.082377824783325 and perplexity is 59.28627475226282
At time: 384.1442708969116 and batch: 300, loss is 4.085763158798218 and perplexity is 59.48731870325574
At time: 385.69489192962646 and batch: 350, loss is 4.019638996124268 and perplexity is 55.6810011408627
At time: 387.2814314365387 and batch: 400, loss is 4.0199255275726316 and perplexity is 55.696957784696885
At time: 388.8317563533783 and batch: 450, loss is 4.0695035982131955 and perplexity is 58.527902024487446
At time: 390.3808135986328 and batch: 500, loss is 4.037779493331909 and perplexity is 56.70029952410923
At time: 391.92429661750793 and batch: 550, loss is 4.0500271701812744 and perplexity is 57.39901656594495
At time: 393.4690203666687 and batch: 600, loss is 4.081460151672363 and perplexity is 59.2318942876349
At time: 395.020325422287 and batch: 650, loss is 4.05203839302063 and perplexity is 57.51457494689347
At time: 396.5795826911926 and batch: 700, loss is 4.040600409507752 and perplexity is 56.860472126739396
At time: 398.12450909614563 and batch: 750, loss is 4.031329174041748 and perplexity is 56.335741510127235
At time: 399.6698627471924 and batch: 800, loss is 3.9921315193176268 and perplexity is 54.17023128731613
At time: 401.2153379917145 and batch: 850, loss is 4.010705580711365 and perplexity is 55.18579486212785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.4791412353515625 and perplexity of 88.15893238418829
Finished 14 epochs...
Completing Train Step...
At time: 405.28912019729614 and batch: 50, loss is 4.097887063026429 and perplexity is 60.2129269725716
At time: 406.870805978775 and batch: 100, loss is 4.0476353931427 and perplexity is 57.261894963603886
At time: 408.42438220977783 and batch: 150, loss is 4.052256026268005 and perplexity is 57.52709339277619
At time: 409.9722921848297 and batch: 200, loss is 4.094621109962463 and perplexity is 60.01659515899285
At time: 411.5206527709961 and batch: 250, loss is 4.0708850431442265 and perplexity is 58.608810970822795
At time: 413.06856060028076 and batch: 300, loss is 4.074670748710632 and perplexity is 58.83110718126843
At time: 414.6151180267334 and batch: 350, loss is 4.009012393951416 and perplexity is 55.09243406586661
At time: 416.16565799713135 and batch: 400, loss is 4.010835452079773 and perplexity is 55.19296238224102
At time: 417.70849990844727 and batch: 450, loss is 4.061737794876098 and perplexity is 58.07514612963859
At time: 419.25031208992004 and batch: 500, loss is 4.030785751342774 and perplexity is 56.30513570612459
At time: 420.7877676486969 and batch: 550, loss is 4.043331522941589 and perplexity is 57.0159767797837
At time: 422.3356113433838 and batch: 600, loss is 4.07670371055603 and perplexity is 58.95083023245404
At time: 423.8763356208801 and batch: 650, loss is 4.0480373573303225 and perplexity is 57.28491682136584
At time: 425.4430091381073 and batch: 700, loss is 4.0362453317642215 and perplexity is 56.61337879597166
At time: 426.98904752731323 and batch: 750, loss is 4.02844277381897 and perplexity is 56.17336846281364
At time: 428.53011655807495 and batch: 800, loss is 3.989325132369995 and perplexity is 54.0184217750333
At time: 430.070853471756 and batch: 850, loss is 4.007684292793274 and perplexity is 55.019314306345265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.479152997334798 and perplexity of 88.1599693141712
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 434.0702865123749 and batch: 50, loss is 4.095506272315979 and perplexity is 60.06974310842561
At time: 435.64321517944336 and batch: 100, loss is 4.049911999702454 and perplexity is 57.39240627438575
At time: 437.1899118423462 and batch: 150, loss is 4.049563174247742 and perplexity is 57.37238983349506
At time: 438.74006175994873 and batch: 200, loss is 4.098507614135742 and perplexity is 60.250303767105656
At time: 440.2838203907013 and batch: 250, loss is 4.067945327758789 and perplexity is 58.43677074603409
At time: 441.827584028244 and batch: 300, loss is 4.067657318115234 and perplexity is 58.41994281594044
At time: 443.3695487976074 and batch: 350, loss is 3.998054485321045 and perplexity is 54.492031791581
At time: 444.9137909412384 and batch: 400, loss is 3.999195246696472 and perplexity is 54.554229666438985
At time: 446.4570264816284 and batch: 450, loss is 4.051360230445862 and perplexity is 57.4755839372769
At time: 448.0047936439514 and batch: 500, loss is 4.016280045509339 and perplexity is 55.494285168258855
At time: 449.5536205768585 and batch: 550, loss is 4.0258393907547 and perplexity is 56.0273178620143
At time: 451.1019561290741 and batch: 600, loss is 4.058801860809326 and perplexity is 57.90489138033857
At time: 452.6466989517212 and batch: 650, loss is 4.025642929077148 and perplexity is 56.01631172233612
At time: 454.1972670555115 and batch: 700, loss is 4.012459354400635 and perplexity is 55.2826631748993
At time: 455.7463676929474 and batch: 750, loss is 4.003015275001526 and perplexity is 54.76302691972587
At time: 457.29665994644165 and batch: 800, loss is 3.9640092992782594 and perplexity is 52.668065244324666
At time: 458.8454022407532 and batch: 850, loss is 3.9847687292099 and perplexity is 53.77285194977034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.470693270365397 and perplexity of 87.4173058384366
Finished 16 epochs...
Completing Train Step...
At time: 462.92848539352417 and batch: 50, loss is 4.0811504459381105 and perplexity is 59.21355267072193
At time: 464.5040500164032 and batch: 100, loss is 4.035559244155884 and perplexity is 56.57455037968436
At time: 466.0527083873749 and batch: 150, loss is 4.035992770195008 and perplexity is 56.59908223764104
At time: 467.6001374721527 and batch: 200, loss is 4.086991209983825 and perplexity is 59.56041705058722
At time: 469.16046929359436 and batch: 250, loss is 4.057396755218506 and perplexity is 57.82358602840344
At time: 470.7224233150482 and batch: 300, loss is 4.058389320373535 and perplexity is 57.881008197942045
At time: 472.2823848724365 and batch: 350, loss is 3.989739389419556 and perplexity is 54.04080392272091
At time: 473.83760595321655 and batch: 400, loss is 3.991920075416565 and perplexity is 54.15877853314149
At time: 475.3854122161865 and batch: 450, loss is 4.045962662696838 and perplexity is 57.16619131402983
At time: 476.9292986392975 and batch: 500, loss is 4.011257162094116 and perplexity is 55.21624271562875
At time: 478.475590467453 and batch: 550, loss is 4.0219894123077395 and perplexity is 55.81202859125474
At time: 480.02063035964966 and batch: 600, loss is 4.056256937980652 and perplexity is 57.75771525574629
At time: 481.5656771659851 and batch: 650, loss is 4.023854532241821 and perplexity is 55.91622185460355
At time: 483.11269760131836 and batch: 700, loss is 4.012172327041626 and perplexity is 55.26679781509433
At time: 484.6572468280792 and batch: 750, loss is 4.004132499694824 and perplexity is 54.82424371577092
At time: 486.1992325782776 and batch: 800, loss is 3.9655727624893187 and perplexity is 52.750474231654984
At time: 487.7416133880615 and batch: 850, loss is 3.9859255743026734 and perplexity is 53.83509480540233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.469924290974935 and perplexity of 87.35010957146054
Finished 17 epochs...
Completing Train Step...
At time: 491.7275948524475 and batch: 50, loss is 4.074192786216736 and perplexity is 58.80299483741956
At time: 493.2659800052643 and batch: 100, loss is 4.028682994842529 and perplexity is 56.18686410778648
At time: 494.80531215667725 and batch: 150, loss is 4.029064645767212 and perplexity is 56.20831196896594
At time: 496.3438010215759 and batch: 200, loss is 4.081757850646973 and perplexity is 59.2495301867928
At time: 497.88485741615295 and batch: 250, loss is 4.051635475158691 and perplexity is 57.49140596523727
At time: 499.42437720298767 and batch: 300, loss is 4.052866072654724 and perplexity is 57.56219829495952
At time: 500.96306681632996 and batch: 350, loss is 3.98450786113739 and perplexity is 53.75882615904889
At time: 502.51055669784546 and batch: 400, loss is 3.9876589012145995 and perplexity is 53.928489542457115
At time: 504.0898666381836 and batch: 450, loss is 4.042550740242004 and perplexity is 56.97147706607807
At time: 505.6347532272339 and batch: 500, loss is 4.007734146118164 and perplexity is 55.022057270468956
At time: 507.1788694858551 and batch: 550, loss is 4.018983912467957 and perplexity is 55.64453737176185
At time: 508.72069454193115 and batch: 600, loss is 4.054097213745117 and perplexity is 57.63310912419495
At time: 510.2709367275238 and batch: 650, loss is 4.021609101295471 and perplexity is 55.79080669788012
At time: 511.8248920440674 and batch: 700, loss is 4.0108511352539065 and perplexity is 55.19382798986873
At time: 513.3761301040649 and batch: 750, loss is 4.003698077201843 and perplexity is 54.80043200368625
At time: 514.9250702857971 and batch: 800, loss is 3.9654051065444946 and perplexity is 52.741631042385094
At time: 516.4686646461487 and batch: 850, loss is 3.9853746032714845 and perplexity is 53.805441397537756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.469789822896321 and perplexity of 87.33836455974208
Finished 18 epochs...
Completing Train Step...
At time: 520.5456993579865 and batch: 50, loss is 4.068410978317261 and perplexity is 58.46398819738621
At time: 522.0976283550262 and batch: 100, loss is 4.023269124031067 and perplexity is 55.88349761866827
At time: 523.6465930938721 and batch: 150, loss is 4.023595881462097 and perplexity is 55.90176095046432
At time: 525.1940171718597 and batch: 200, loss is 4.077542386054993 and perplexity is 59.000291587527656
At time: 526.7423174381256 and batch: 250, loss is 4.047055859565734 and perplexity is 57.22871938690063
At time: 528.287045955658 and batch: 300, loss is 4.048417572975159 and perplexity is 57.30670158414583
At time: 529.831442117691 and batch: 350, loss is 3.980124955177307 and perplexity is 53.52372187581144
At time: 531.3761546611786 and batch: 400, loss is 3.984061532020569 and perplexity is 53.734837383490714
At time: 532.9192955493927 and batch: 450, loss is 4.039547476768494 and perplexity is 56.80063338268228
At time: 534.4682822227478 and batch: 500, loss is 4.004584512710571 and perplexity is 54.84903058908157
At time: 536.0156164169312 and batch: 550, loss is 4.015994000434875 and perplexity is 55.47841357142988
At time: 537.5592796802521 and batch: 600, loss is 4.051877579689026 and perplexity is 57.505326580130735
At time: 539.108423948288 and batch: 650, loss is 4.01920150756836 and perplexity is 55.6566466678721
At time: 540.6501312255859 and batch: 700, loss is 4.009211559295654 and perplexity is 55.10340766220603
At time: 542.1951777935028 and batch: 750, loss is 4.002649421691895 and perplexity is 54.742995349613395
At time: 543.7867727279663 and batch: 800, loss is 3.9645985078811647 and perplexity is 52.69910686556209
At time: 545.3402233123779 and batch: 850, loss is 3.9842367696762087 and perplexity is 53.74425457551921
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.469612121582031 and perplexity of 87.32284579646502
Finished 19 epochs...
Completing Train Step...
At time: 549.4174437522888 and batch: 50, loss is 4.063139872550964 and perplexity is 58.15662910487336
At time: 550.9672865867615 and batch: 100, loss is 4.018439292907715 and perplexity is 55.614240519166714
At time: 552.5174856185913 and batch: 150, loss is 4.018915643692017 and perplexity is 55.64073871697397
At time: 554.0744948387146 and batch: 200, loss is 4.073995785713196 and perplexity is 58.791411758800535
At time: 555.6313955783844 and batch: 250, loss is 4.042952055931091 and perplexity is 56.994345202019126
At time: 557.1868844032288 and batch: 300, loss is 4.044426846504211 and perplexity is 57.07846193708195
At time: 558.7385249137878 and batch: 350, loss is 3.9760883617401124 and perplexity is 53.308103845291605
At time: 560.2860450744629 and batch: 400, loss is 3.98078152179718 and perplexity is 53.5588753039874
At time: 561.8339040279388 and batch: 450, loss is 4.03682318687439 and perplexity is 56.646102580105165
At time: 563.3852651119232 and batch: 500, loss is 4.001754970550537 and perplexity is 54.6940523067849
At time: 564.9355401992798 and batch: 550, loss is 4.013020095825195 and perplexity is 55.31367114711623
At time: 566.4915030002594 and batch: 600, loss is 4.049817266464234 and perplexity is 57.38696956341281
At time: 568.0416469573975 and batch: 650, loss is 4.016718769073487 and perplexity is 55.5186371603435
At time: 569.5864763259888 and batch: 700, loss is 4.0074498748779295 and perplexity is 55.0064183049678
At time: 571.1362097263336 and batch: 750, loss is 4.001373476982117 and perplexity is 54.67319085710648
At time: 572.687474489212 and batch: 800, loss is 3.9634607362747194 and perplexity is 52.639181415287915
At time: 574.2373902797699 and batch: 850, loss is 3.982817702293396 and perplexity is 53.66804194505598
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.469535827636719 and perplexity of 87.31618384617975
Finished 20 epochs...
Completing Train Step...
At time: 578.2228491306305 and batch: 50, loss is 4.0582885694503785 and perplexity is 57.8751769266908
At time: 579.7933306694031 and batch: 100, loss is 4.014010653495789 and perplexity is 55.36848967433368
At time: 581.338544845581 and batch: 150, loss is 4.014680509567261 and perplexity is 55.405591018203175
At time: 582.9065399169922 and batch: 200, loss is 4.070840268135071 and perplexity is 58.60618681952362
At time: 584.4510419368744 and batch: 250, loss is 4.039288806915283 and perplexity is 56.7859426712869
At time: 585.9936091899872 and batch: 300, loss is 4.040752000808716 and perplexity is 56.86909233304018
At time: 587.5378804206848 and batch: 350, loss is 3.9723958015441894 and perplexity is 53.11162344401768
At time: 589.078209400177 and batch: 400, loss is 3.977736248970032 and perplexity is 53.39602200858338
At time: 590.6239216327667 and batch: 450, loss is 4.034140934944153 and perplexity is 56.4943670495506
At time: 592.1660807132721 and batch: 500, loss is 3.998927044868469 and perplexity is 54.539600084245194
At time: 593.7109024524689 and batch: 550, loss is 4.0103297996521 and perplexity is 55.16506098162194
At time: 595.2516186237335 and batch: 600, loss is 4.047691278457641 and perplexity is 57.26509515205898
At time: 596.791161775589 and batch: 650, loss is 4.014270782470703 and perplexity is 55.38289449626977
At time: 598.3325481414795 and batch: 700, loss is 4.005440077781677 and perplexity is 54.89597758407844
At time: 599.8736627101898 and batch: 750, loss is 3.9999018335342407 and perplexity is 54.59279058878165
At time: 601.4162082672119 and batch: 800, loss is 3.962105803489685 and perplexity is 52.567907159428
At time: 602.9598546028137 and batch: 850, loss is 3.9810106801986693 and perplexity is 53.571150176629004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.4693603515625 and perplexity of 87.30086328925756
Finished 21 epochs...
Completing Train Step...
At time: 607.0902230739594 and batch: 50, loss is 4.053699340820312 and perplexity is 57.6101830316403
At time: 608.6573674678802 and batch: 100, loss is 4.009958047866821 and perplexity is 55.14455708313358
At time: 610.202499628067 and batch: 150, loss is 4.010759778022766 and perplexity is 55.18878586488835
At time: 611.751119852066 and batch: 200, loss is 4.06788896560669 and perplexity is 58.43347721668919
At time: 613.3086941242218 and batch: 250, loss is 4.0358503055572506 and perplexity is 56.59101944423779
At time: 614.8653376102448 and batch: 300, loss is 4.037197914123535 and perplexity is 56.66733339593397
At time: 616.4102551937103 and batch: 350, loss is 3.9688744592666625 and perplexity is 52.924928140904235
At time: 617.9535624980927 and batch: 400, loss is 3.9746992349624635 and perplexity is 53.23410354057654
At time: 619.5033378601074 and batch: 450, loss is 4.031588172912597 and perplexity is 56.35033429324244
At time: 621.0786416530609 and batch: 500, loss is 3.996245837211609 and perplexity is 54.39356395497749
At time: 622.6279933452606 and batch: 550, loss is 4.0073797178268435 and perplexity is 55.00255935223668
At time: 624.1804876327515 and batch: 600, loss is 4.045596871376038 and perplexity is 57.14528424144991
At time: 625.734100818634 and batch: 650, loss is 4.011832566261291 and perplexity is 55.2480235143082
At time: 627.2826709747314 and batch: 700, loss is 4.003428688049317 and perplexity is 54.785671350019946
At time: 628.8312473297119 and batch: 750, loss is 3.9983437204360963 and perplexity is 54.507795080204026
At time: 630.3858189582825 and batch: 800, loss is 3.9604948806762694 and perplexity is 52.48329249067991
At time: 631.9284820556641 and batch: 850, loss is 3.979304161071777 and perplexity is 53.47980794500295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.46936575571696 and perplexity of 87.30133507788207
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 635.9702260494232 and batch: 50, loss is 4.053685140609741 and perplexity is 57.60936496071862
At time: 637.5497751235962 and batch: 100, loss is 4.015372915267944 and perplexity is 55.4439674497707
At time: 639.0939848423004 and batch: 150, loss is 4.01269727230072 and perplexity is 55.295817474792635
At time: 640.6454663276672 and batch: 200, loss is 4.073052163124085 and perplexity is 58.73596102101812
At time: 642.1910715103149 and batch: 250, loss is 4.0403445529937745 and perplexity is 56.84592586551515
At time: 643.7450060844421 and batch: 300, loss is 4.03618658542633 and perplexity is 56.610053064979965
At time: 645.3059146404266 and batch: 350, loss is 3.964727592468262 and perplexity is 52.70590994708925
At time: 646.8546357154846 and batch: 400, loss is 3.9722074937820433 and perplexity is 53.10162305466801
At time: 648.4027953147888 and batch: 450, loss is 4.028379664421082 and perplexity is 56.1698235072138
At time: 649.9492645263672 and batch: 500, loss is 3.989908757209778 and perplexity is 54.049957469399224
At time: 651.4965782165527 and batch: 550, loss is 3.9999139881134034 and perplexity is 54.593454145209186
At time: 653.0503413677216 and batch: 600, loss is 4.037192616462708 and perplexity is 56.66703319241684
At time: 654.6005489826202 and batch: 650, loss is 4.000869903564453 and perplexity is 54.64566582255152
At time: 656.155606508255 and batch: 700, loss is 3.9920370292663576 and perplexity is 54.16511298120284
At time: 657.7204744815826 and batch: 750, loss is 3.9855374908447265 and perplexity is 53.814206349145685
At time: 659.2692592144012 and batch: 800, loss is 3.9467850875854493 and perplexity is 51.768667285045105
At time: 660.8637759685516 and batch: 850, loss is 3.968779511451721 and perplexity is 52.9199032731752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.46581490834554 and perplexity of 86.9918910810365
Finished 23 epochs...
Completing Train Step...
At time: 664.9446210861206 and batch: 50, loss is 4.046705231666565 and perplexity is 57.20865691868594
At time: 666.5003612041473 and batch: 100, loss is 4.00896342754364 and perplexity is 55.08973645332153
At time: 668.0532641410828 and batch: 150, loss is 4.00597580909729 and perplexity is 54.92539495757192
At time: 669.6043810844421 and batch: 200, loss is 4.066421866416931 and perplexity is 58.34781236439279
At time: 671.1505076885223 and batch: 250, loss is 4.034501552581787 and perplexity is 56.5147435885845
At time: 672.7061288356781 and batch: 300, loss is 4.03162202835083 and perplexity is 56.35224209079896
At time: 674.252806186676 and batch: 350, loss is 3.960939497947693 and perplexity is 52.50663265731855
At time: 675.8003525733948 and batch: 400, loss is 3.9693897104263307 and perplexity is 52.95220479806535
At time: 677.3659207820892 and batch: 450, loss is 4.025986309051514 and perplexity is 56.03554990483367
At time: 678.9204199314117 and batch: 500, loss is 3.987734236717224 and perplexity is 53.93255242536033
At time: 680.4777190685272 and batch: 550, loss is 3.998584117889404 and perplexity is 54.52090019048119
At time: 682.0231685638428 and batch: 600, loss is 4.036445050239563 and perplexity is 56.624686662826115
At time: 683.5758574008942 and batch: 650, loss is 4.000607943534851 and perplexity is 54.631352717127605
At time: 685.1268112659454 and batch: 700, loss is 3.9923435735702513 and perplexity is 54.18171953324891
At time: 686.6732754707336 and batch: 750, loss is 3.9865073013305663 and perplexity is 53.866421245934966
At time: 688.227861404419 and batch: 800, loss is 3.947766923904419 and perplexity is 51.81952060350248
At time: 689.7840311527252 and batch: 850, loss is 3.9698544216156004 and perplexity is 52.97681799870344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.465631167093913 and perplexity of 86.97590855045792
Finished 24 epochs...
Completing Train Step...
At time: 693.8247017860413 and batch: 50, loss is 4.043626852035523 and perplexity is 57.03281774323666
At time: 695.3646161556244 and batch: 100, loss is 4.006134529113769 and perplexity is 54.93411340904279
At time: 696.9070818424225 and batch: 150, loss is 4.0027862167358395 and perplexity is 54.75048443229094
At time: 698.4489054679871 and batch: 200, loss is 4.0637924766540525 and perplexity is 58.194594746568306
At time: 700.0124726295471 and batch: 250, loss is 4.031920704841614 and perplexity is 56.36907569449016
At time: 701.552960395813 and batch: 300, loss is 4.029122152328491 and perplexity is 56.21154440864493
At time: 703.0935809612274 and batch: 350, loss is 3.958811297416687 and perplexity is 52.39500683701258
At time: 704.6383559703827 and batch: 400, loss is 3.9677592611312864 and perplexity is 52.86593925798625
At time: 706.1795847415924 and batch: 450, loss is 4.024546966552735 and perplexity is 55.95495357319021
At time: 707.7178978919983 and batch: 500, loss is 3.986185688972473 and perplexity is 53.8490999247008
At time: 709.2555568218231 and batch: 550, loss is 3.9975107526779174 and perplexity is 54.46241074879929
At time: 710.7941272258759 and batch: 600, loss is 4.035697340965271 and perplexity is 56.582363684068085
At time: 712.3348426818848 and batch: 650, loss is 3.9999613332748414 and perplexity is 54.59603894229748
At time: 713.8759663105011 and batch: 700, loss is 3.992120666503906 and perplexity is 54.16964339107693
At time: 715.4190802574158 and batch: 750, loss is 3.9866496229171755 and perplexity is 53.874088146041395
At time: 716.960839509964 and batch: 800, loss is 3.947849864959717 and perplexity is 51.82381874747021
At time: 718.5008888244629 and batch: 850, loss is 3.9699863052368163 and perplexity is 52.98380523404242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.465537389119466 and perplexity of 86.96775250836288
Finished 25 epochs...
Completing Train Step...
At time: 722.5804860591888 and batch: 50, loss is 4.041056060791016 and perplexity is 56.88638657735894
At time: 724.1229453086853 and batch: 100, loss is 4.003891887664795 and perplexity is 54.811053930069846
At time: 725.6626968383789 and batch: 150, loss is 4.000209240913391 and perplexity is 54.609575395211564
At time: 727.2175815105438 and batch: 200, loss is 4.06175883769989 and perplexity is 58.07636820756323
At time: 728.7624599933624 and batch: 250, loss is 4.029914498329163 and perplexity is 56.256101050856216
At time: 730.3054659366608 and batch: 300, loss is 4.026992893218994 and perplexity is 56.091982799648264
At time: 731.8485383987427 and batch: 350, loss is 3.956928687095642 and perplexity is 52.2964602478888
At time: 733.3928279876709 and batch: 400, loss is 3.9663532066345213 and perplexity is 52.79165909957981
At time: 734.9365911483765 and batch: 450, loss is 4.023262100219727 and perplexity is 55.88310510490243
At time: 736.4832503795624 and batch: 500, loss is 3.98475341796875 and perplexity is 53.77202862696987
At time: 738.0246822834015 and batch: 550, loss is 3.9964278316497803 and perplexity is 54.40346418195541
At time: 739.614495754242 and batch: 600, loss is 4.03490261554718 and perplexity is 56.53741410508497
At time: 741.1584763526917 and batch: 650, loss is 3.9991597986221312 and perplexity is 54.55229585832526
At time: 742.6991512775421 and batch: 700, loss is 3.9916438913345336 and perplexity is 54.14382280597034
At time: 744.2400939464569 and batch: 750, loss is 3.9864457416534425 and perplexity is 53.86310534849908
At time: 745.7883563041687 and batch: 800, loss is 3.947661266326904 and perplexity is 51.81404576772158
At time: 747.3349301815033 and batch: 850, loss is 3.9698219871520997 and perplexity is 52.97509975189903
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.465504328409831 and perplexity of 86.96487734027741
Finished 26 epochs...
Completing Train Step...
At time: 751.4128127098083 and batch: 50, loss is 4.038674216270447 and perplexity is 56.75105328460647
At time: 752.9996106624603 and batch: 100, loss is 4.001889233589172 and perplexity is 54.70139618943786
At time: 754.5468645095825 and batch: 150, loss is 3.997977237701416 and perplexity is 54.48782257441445
At time: 756.0940940380096 and batch: 200, loss is 4.06004762172699 and perplexity is 57.97707198151287
At time: 757.6417195796967 and batch: 250, loss is 4.028135938644409 and perplexity is 56.15613514152547
At time: 759.1975591182709 and batch: 300, loss is 4.02522828578949 and perplexity is 55.993089749431405
At time: 760.7473666667938 and batch: 350, loss is 3.9551984596252443 and perplexity is 52.20605371025487
At time: 762.2944874763489 and batch: 400, loss is 3.965038747787476 and perplexity is 52.72231222301867
At time: 763.8429353237152 and batch: 450, loss is 4.02203709602356 and perplexity is 55.81468997961738
At time: 765.3891503810883 and batch: 500, loss is 3.983425941467285 and perplexity is 53.70069487994327
At time: 766.9407696723938 and batch: 550, loss is 3.9953738164901735 and perplexity is 54.34615231506664
At time: 768.4882278442383 and batch: 600, loss is 4.034079194068909 and perplexity is 56.49087914555681
At time: 770.0308430194855 and batch: 650, loss is 3.9982531213760377 and perplexity is 54.5028569489023
At time: 771.5733687877655 and batch: 700, loss is 3.9910619926452635 and perplexity is 54.112325751383615
At time: 773.1324076652527 and batch: 750, loss is 3.986085958480835 and perplexity is 53.84372979527831
At time: 774.6954865455627 and batch: 800, loss is 3.947219533920288 and perplexity is 51.79116287901692
At time: 776.2619450092316 and batch: 850, loss is 3.9694430780410768 and perplexity is 52.95503080633896
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.465415000915527 and perplexity of 86.95710933264567
Finished 27 epochs...
Completing Train Step...
At time: 780.3992474079132 and batch: 50, loss is 4.036464953422547 and perplexity is 56.62581368554178
At time: 781.9912087917328 and batch: 100, loss is 4.000031485557556 and perplexity is 54.599869113402505
At time: 783.5433723926544 and batch: 150, loss is 3.9959327173233032 and perplexity is 54.37653491451211
At time: 785.0954806804657 and batch: 200, loss is 4.058447604179382 and perplexity is 57.884381821698966
At time: 786.6497993469238 and batch: 250, loss is 4.026644134521485 and perplexity is 56.07242364369749
At time: 788.2046644687653 and batch: 300, loss is 4.023416895866394 and perplexity is 55.89175623585633
At time: 789.7653951644897 and batch: 350, loss is 3.953606743812561 and perplexity is 52.123022607536384
At time: 791.3264377117157 and batch: 400, loss is 3.963869194984436 and perplexity is 52.660686739128764
At time: 792.8876445293427 and batch: 450, loss is 4.020891189575195 and perplexity is 55.750768197641996
At time: 794.4481010437012 and batch: 500, loss is 3.982105255126953 and perplexity is 53.629819917847904
At time: 796.0060727596283 and batch: 550, loss is 3.9942559432983398 and perplexity is 54.28543415224054
At time: 797.567950963974 and batch: 600, loss is 4.033237943649292 and perplexity is 56.44337615352857
At time: 799.127259016037 and batch: 650, loss is 3.9973362922668456 and perplexity is 54.452910043004714
At time: 800.686806678772 and batch: 700, loss is 3.990407724380493 and perplexity is 54.07693335323556
At time: 802.2555170059204 and batch: 750, loss is 3.985616612434387 and perplexity is 53.81846438314734
At time: 803.816299200058 and batch: 800, loss is 3.946719374656677 and perplexity is 51.765265526070166
At time: 805.3781750202179 and batch: 850, loss is 3.968966717720032 and perplexity is 52.929811138164624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.465373675028483 and perplexity of 86.95351582722064
Finished 28 epochs...
Completing Train Step...
At time: 809.4785304069519 and batch: 50, loss is 4.034392457008362 and perplexity is 56.50857841652835
At time: 811.0314998626709 and batch: 100, loss is 3.9982811975479127 and perplexity is 54.5043872019634
At time: 812.5744683742523 and batch: 150, loss is 3.993971199989319 and perplexity is 54.26997893857717
At time: 814.1189775466919 and batch: 200, loss is 4.056996507644653 and perplexity is 57.80044690938112
At time: 815.6639447212219 and batch: 250, loss is 4.024993343353271 and perplexity is 55.979936141745085
At time: 817.2100703716278 and batch: 300, loss is 4.021771121025085 and perplexity is 55.7998466416011
At time: 818.8005764484406 and batch: 350, loss is 3.9520701503753664 and perplexity is 52.04299221590495
At time: 820.3410503864288 and batch: 400, loss is 3.9627025938034057 and perplexity is 52.599288540353996
At time: 821.883960723877 and batch: 450, loss is 4.019744029045105 and perplexity is 55.68684978619243
At time: 823.4257819652557 and batch: 500, loss is 3.9808284997940064 and perplexity is 53.56139145176278
At time: 824.9706108570099 and batch: 550, loss is 3.9931632614135744 and perplexity is 54.226149837082936
At time: 826.5245091915131 and batch: 600, loss is 4.032392220497131 and perplexity is 56.39566086333627
At time: 828.083352804184 and batch: 650, loss is 3.996364550590515 and perplexity is 54.40002158204111
At time: 829.6349856853485 and batch: 700, loss is 3.9897245168685913 and perplexity is 54.0400002040871
At time: 831.1797606945038 and batch: 750, loss is 3.9851107501983645 and perplexity is 53.79124653923528
At time: 832.7188880443573 and batch: 800, loss is 3.9461869192123413 and perplexity is 51.73771016526528
At time: 834.2592680454254 and batch: 850, loss is 3.968429651260376 and perplexity is 52.90139194406747
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.465339342753093 and perplexity of 86.95053056641477
Finished 29 epochs...
Completing Train Step...
At time: 838.2945964336395 and batch: 50, loss is 4.03237181186676 and perplexity is 56.39450991688382
At time: 839.843090057373 and batch: 100, loss is 3.9965950632095337 and perplexity is 54.412562918903326
At time: 841.3926825523376 and batch: 150, loss is 3.992114534378052 and perplexity is 54.16931121702465
At time: 842.942622423172 and batch: 200, loss is 4.055612473487854 and perplexity is 57.72050445088801
At time: 844.4846549034119 and batch: 250, loss is 4.023529863357544 and perplexity is 55.898070543983316
At time: 846.0277268886566 and batch: 300, loss is 4.020153422355651 and perplexity is 55.709652277255465
At time: 847.571037530899 and batch: 350, loss is 3.950540885925293 and perplexity is 51.96346554218994
At time: 849.1143925189972 and batch: 400, loss is 3.9616027641296387 and perplexity is 52.54147008305402
At time: 850.6668102741241 and batch: 450, loss is 4.018650379180908 and perplexity is 55.62598116103495
At time: 852.2184529304504 and batch: 500, loss is 3.9795760202407835 and perplexity is 53.49434889760558
At time: 853.7737462520599 and batch: 550, loss is 3.99206787109375 and perplexity is 54.166783558029785
At time: 855.3272905349731 and batch: 600, loss is 4.0315086460113525 and perplexity is 56.3458531039619
At time: 856.9327666759491 and batch: 650, loss is 3.995375576019287 and perplexity is 54.346247938787975
At time: 858.4778234958649 and batch: 700, loss is 3.989042477607727 and perplexity is 54.00315536853129
At time: 860.0227103233337 and batch: 750, loss is 3.9845362758636473 and perplexity is 53.76035372308066
At time: 861.5690357685089 and batch: 800, loss is 3.9457152462005616 and perplexity is 51.71331263796895
At time: 863.114572763443 and batch: 850, loss is 3.9678739309310913 and perplexity is 52.872001732242175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.465273539225261 and perplexity of 86.94480910300493
Finished 30 epochs...
Completing Train Step...
At time: 867.1918563842773 and batch: 50, loss is 4.030429539680481 and perplexity is 56.285082731902754
At time: 868.7436559200287 and batch: 100, loss is 3.9949890422821044 and perplexity is 54.32524533983837
At time: 870.2925453186035 and batch: 150, loss is 3.9903276443481444 and perplexity is 54.07260304405127
At time: 871.8392984867096 and batch: 200, loss is 4.0542799949646 and perplexity is 57.64364433695343
At time: 873.3851528167725 and batch: 250, loss is 4.022145042419433 and perplexity is 55.8207152994374
At time: 874.9418215751648 and batch: 300, loss is 4.018650770187378 and perplexity is 55.62600291115773
At time: 876.4969892501831 and batch: 350, loss is 3.94908474445343 and perplexity is 51.88785444857697
At time: 878.0528407096863 and batch: 400, loss is 3.9604922246932985 and perplexity is 52.48315309613391
At time: 879.6054718494415 and batch: 450, loss is 4.0175418329238894 and perplexity is 55.56435135388319
At time: 881.156711101532 and batch: 500, loss is 3.9783471632003784 and perplexity is 53.42865236443184
At time: 882.7096314430237 and batch: 550, loss is 3.9909032154083253 and perplexity is 54.10373462787199
At time: 884.2593235969543 and batch: 600, loss is 4.030605278015137 and perplexity is 56.294975047812144
At time: 885.8083779811859 and batch: 650, loss is 3.994390721321106 and perplexity is 54.29275112879333
At time: 887.36443567276 and batch: 700, loss is 3.9883380365371703 and perplexity is 53.96512672399217
At time: 888.9195926189423 and batch: 750, loss is 3.983898539543152 and perplexity is 53.726079722957756
At time: 890.4766325950623 and batch: 800, loss is 3.944933762550354 and perplexity is 51.67291531662333
At time: 892.0237317085266 and batch: 850, loss is 3.967245559692383 and perplexity is 52.83878892310054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.465245882670085 and perplexity of 86.94240454234581
Finished 31 epochs...
Completing Train Step...
At time: 896.0710575580597 and batch: 50, loss is 4.028606286048889 and perplexity is 56.18255424652661
At time: 897.6385378837585 and batch: 100, loss is 3.993384304046631 and perplexity is 54.23813745286171
At time: 899.1865220069885 and batch: 150, loss is 3.9885665225982665 and perplexity is 53.97745841198965
At time: 900.7343821525574 and batch: 200, loss is 4.052972807884216 and perplexity is 57.56834253730233
At time: 902.2837476730347 and batch: 250, loss is 4.0206720495224 and perplexity is 55.7385523098998
At time: 903.8281533718109 and batch: 300, loss is 4.017125906944275 and perplexity is 55.54124550210982
At time: 905.3714213371277 and batch: 350, loss is 3.94761314868927 and perplexity is 51.81155265822471
At time: 906.9154653549194 and batch: 400, loss is 3.959396243095398 and perplexity is 52.425664035371625
At time: 908.4583370685577 and batch: 450, loss is 4.01644199848175 and perplexity is 55.503273360506284
At time: 910.0017201900482 and batch: 500, loss is 3.977125587463379 and perplexity is 53.36342506719598
At time: 911.5489075183868 and batch: 550, loss is 3.9898497009277345 and perplexity is 54.04676557411815
At time: 913.100579738617 and batch: 600, loss is 4.029681396484375 and perplexity is 56.242989178180935
At time: 914.6522898674011 and batch: 650, loss is 3.993383626937866 and perplexity is 54.23810072775589
At time: 916.1975038051605 and batch: 700, loss is 3.987570447921753 and perplexity is 53.923719600940466
At time: 917.7419867515564 and batch: 750, loss is 3.9832723140716553 and perplexity is 53.69244561571807
At time: 919.2884755134583 and batch: 800, loss is 3.944322304725647 and perplexity is 51.64132916601114
At time: 920.832263469696 and batch: 850, loss is 3.9666578722000123 and perplexity is 52.807745350591546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.465227762858073 and perplexity of 86.94082917659237
Finished 32 epochs...
Completing Train Step...
At time: 924.9124765396118 and batch: 50, loss is 4.026785068511963 and perplexity is 56.08032671100969
At time: 926.4810793399811 and batch: 100, loss is 3.9918764066696166 and perplexity is 54.156413538785245
At time: 928.023764371872 and batch: 150, loss is 3.986957321166992 and perplexity is 53.89066765928718
At time: 929.5766587257385 and batch: 200, loss is 4.051754055023193 and perplexity is 57.49822369258125
At time: 931.1255729198456 and batch: 250, loss is 4.019301075935363 and perplexity is 55.66218858518898
At time: 932.6780979633331 and batch: 300, loss is 4.015672245025635 and perplexity is 55.460565963203415
At time: 934.2219302654266 and batch: 350, loss is 3.9462219047546387 and perplexity is 51.739520268776175
At time: 935.7958114147186 and batch: 400, loss is 3.9583343982696535 and perplexity is 52.37002566016761
At time: 937.3387899398804 and batch: 450, loss is 4.015356955528259 and perplexity is 55.4430825855442
At time: 938.8827860355377 and batch: 500, loss is 3.9759436368942263 and perplexity is 53.30038939642779
At time: 940.4281747341156 and batch: 550, loss is 3.9887347841262817 and perplexity is 53.98654150576662
At time: 941.9813215732574 and batch: 600, loss is 4.0287951612472535 and perplexity is 56.19316673979136
At time: 943.5319149494171 and batch: 650, loss is 3.9923861265182494 and perplexity is 54.18402517419822
At time: 945.078287601471 and batch: 700, loss is 3.986784772872925 and perplexity is 53.8813697187114
At time: 946.620377779007 and batch: 750, loss is 3.982611212730408 and perplexity is 53.65696119859693
At time: 948.1634006500244 and batch: 800, loss is 3.9436460399627684 and perplexity is 51.606417760795054
At time: 949.7035977840424 and batch: 850, loss is 3.9659927034378053 and perplexity is 52.77263096777172
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.465212821960449 and perplexity of 86.93953021226817
Finished 33 epochs...
Completing Train Step...
At time: 953.7055838108063 and batch: 50, loss is 4.02499969959259 and perplexity is 55.9802919647471
At time: 955.2739953994751 and batch: 100, loss is 3.9903806972503664 and perplexity is 54.07547182867146
At time: 956.8147885799408 and batch: 150, loss is 3.9853423166275026 and perplexity is 53.8037042284509
At time: 958.3562679290771 and batch: 200, loss is 4.050526337623596 and perplexity is 57.42767543842892
At time: 959.9037957191467 and batch: 250, loss is 4.017956652641296 and perplexity is 55.58740532369975
At time: 961.4532146453857 and batch: 300, loss is 4.014279894828796 and perplexity is 55.383399167336016
At time: 963.0014305114746 and batch: 350, loss is 3.944861845970154 and perplexity is 51.66919931088759
At time: 964.5490617752075 and batch: 400, loss is 3.9572813224792482 and perplexity is 52.31490508216903
At time: 966.1028916835785 and batch: 450, loss is 4.014307498931885 and perplexity is 55.38492799749692
At time: 967.658941745758 and batch: 500, loss is 3.9747651338577272 and perplexity is 53.23761172478168
At time: 969.2151935100555 and batch: 550, loss is 3.987654461860657 and perplexity is 53.92825013533585
At time: 970.7688736915588 and batch: 600, loss is 4.027938075065613 and perplexity is 56.14502498683874
At time: 972.3159465789795 and batch: 650, loss is 3.991406435966492 and perplexity is 54.13096759092907
At time: 973.8617656230927 and batch: 700, loss is 3.986005187034607 and perplexity is 53.83938093498667
At time: 975.4402358531952 and batch: 750, loss is 3.9819510316848756 and perplexity is 53.62154958017854
At time: 976.9870307445526 and batch: 800, loss is 3.9429484367370606 and perplexity is 51.570429511516764
At time: 978.536479473114 and batch: 850, loss is 3.9652987051010133 and perplexity is 52.73601955525109
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.465220133463542 and perplexity of 86.94016587323601
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 982.6020693778992 and batch: 50, loss is 4.025045442581177 and perplexity is 55.98285272917157
At time: 984.1461496353149 and batch: 100, loss is 3.99411856174469 and perplexity is 54.27797684721597
At time: 985.6995587348938 and batch: 150, loss is 3.9875760746002196 and perplexity is 53.92402301322599
At time: 987.2495231628418 and batch: 200, loss is 4.053602776527405 and perplexity is 57.604620213640686
At time: 988.8016698360443 and batch: 250, loss is 4.020684623718262 and perplexity is 55.739253181780064
At time: 990.3479943275452 and batch: 300, loss is 4.014787111282349 and perplexity is 55.411497664054885
At time: 991.8951025009155 and batch: 350, loss is 3.9451717376708983 and perplexity is 51.685213648165096
At time: 993.4411804676056 and batch: 400, loss is 3.957514524459839 and perplexity is 52.32710644428421
At time: 994.9856383800507 and batch: 450, loss is 4.013031659126281 and perplexity is 55.314310759447906
At time: 996.5354351997375 and batch: 500, loss is 3.9712037372589113 and perplexity is 53.04834869585353
At time: 998.0826323032379 and batch: 550, loss is 3.984189338684082 and perplexity is 53.74170549265681
At time: 999.6204741001129 and batch: 600, loss is 4.023410739898682 and perplexity is 55.89141216906863
At time: 1001.1575570106506 and batch: 650, loss is 3.985705599784851 and perplexity is 53.823253758792596
At time: 1002.6949322223663 and batch: 700, loss is 3.980121693611145 and perplexity is 53.523547304935995
At time: 1004.2412621974945 and batch: 750, loss is 3.975778131484985 and perplexity is 53.291568623630965
At time: 1005.7855291366577 and batch: 800, loss is 3.935696258544922 and perplexity is 51.19778444471011
At time: 1007.3396687507629 and batch: 850, loss is 3.9596351099014284 and perplexity is 52.43818828204763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.464220364888509 and perplexity of 86.85328926297656
Finished 35 epochs...
Completing Train Step...
At time: 1011.4331729412079 and batch: 50, loss is 4.022219381332397 and perplexity is 55.82486510497775
At time: 1012.9782812595367 and batch: 100, loss is 3.9908987426757814 and perplexity is 54.10349263687855
At time: 1014.5530931949615 and batch: 150, loss is 3.984194917678833 and perplexity is 53.74200531818603
At time: 1016.1020884513855 and batch: 200, loss is 4.0495195484161375 and perplexity is 57.36988696987261
At time: 1017.6505992412567 and batch: 250, loss is 4.016674036979675 and perplexity is 55.516153751002236
At time: 1019.201550245285 and batch: 300, loss is 4.012221913337708 and perplexity is 55.26953835884042
At time: 1020.7530159950256 and batch: 350, loss is 3.9430872344970704 and perplexity is 51.57758786838616
At time: 1022.2986934185028 and batch: 400, loss is 3.955931100845337 and perplexity is 52.24431603170683
At time: 1023.8416187763214 and batch: 450, loss is 4.01153461933136 and perplexity is 55.23156498732288
At time: 1025.3828966617584 and batch: 500, loss is 3.970167512893677 and perplexity is 52.993407175186796
At time: 1026.9282438755035 and batch: 550, loss is 3.9838114881515505 and perplexity is 53.72140299651331
At time: 1028.4724099636078 and batch: 600, loss is 4.0232874870300295 and perplexity is 55.884523816699065
At time: 1030.014401435852 and batch: 650, loss is 3.985791711807251 and perplexity is 53.82788878758889
At time: 1031.5597295761108 and batch: 700, loss is 3.980461387634277 and perplexity is 53.541732022497385
At time: 1033.1131060123444 and batch: 750, loss is 3.9765619802474976 and perplexity is 53.33335752970044
At time: 1034.6648044586182 and batch: 800, loss is 3.9364694690704347 and perplexity is 51.237386418884306
At time: 1036.215891122818 and batch: 850, loss is 3.960413317680359 and perplexity is 52.479011970677504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.4641984303792315 and perplexity of 86.85138419959085
Finished 36 epochs...
Completing Train Step...
At time: 1040.2789752483368 and batch: 50, loss is 4.0209857368469235 and perplexity is 55.756039529862946
At time: 1041.8248143196106 and batch: 100, loss is 3.989768967628479 and perplexity is 54.04240237654929
At time: 1043.3732385635376 and batch: 150, loss is 3.9827784061431886 and perplexity is 53.66593303905458
At time: 1044.922268629074 and batch: 200, loss is 4.048202147483826 and perplexity is 57.2943575894537
At time: 1046.4731891155243 and batch: 250, loss is 4.015318984985352 and perplexity is 55.44097742156534
At time: 1048.0178711414337 and batch: 300, loss is 4.011158471107483 and perplexity is 55.21079363904874
At time: 1049.56481051445 and batch: 350, loss is 3.942330422401428 and perplexity is 51.53856809320466
At time: 1051.1072480678558 and batch: 400, loss is 3.955263223648071 and perplexity is 52.2094348937974
At time: 1052.6501469612122 and batch: 450, loss is 4.010754566192627 and perplexity is 55.18849823106039
At time: 1054.2250101566315 and batch: 500, loss is 3.9694636821746827 and perplexity is 52.95612191010939
At time: 1055.774593114853 and batch: 550, loss is 3.9833823108673094 and perplexity is 53.698351937518915
At time: 1057.3287978172302 and batch: 600, loss is 4.023024935722351 and perplexity is 55.86985318787264
At time: 1058.8750169277191 and batch: 650, loss is 3.9855839014053345 and perplexity is 53.816703954588206
At time: 1060.4190723896027 and batch: 700, loss is 3.980441870689392 and perplexity is 53.5406870616617
At time: 1061.9637773036957 and batch: 750, loss is 3.9767758226394654 and perplexity is 53.34476368196224
At time: 1063.5109279155731 and batch: 800, loss is 3.9365714502334597 and perplexity is 51.24261193358917
At time: 1065.056675195694 and batch: 850, loss is 3.960529704093933 and perplexity is 52.48512017011751
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.464200973510742 and perplexity of 86.85160507436362
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 1069.1198844909668 and batch: 50, loss is 4.0208333253860475 and perplexity is 55.74754231797823
At time: 1070.6960213184357 and batch: 100, loss is 3.9916448640823363 and perplexity is 54.14387547428062
At time: 1072.2443573474884 and batch: 150, loss is 3.98413245677948 and perplexity is 53.7386486490322
At time: 1073.795566558838 and batch: 200, loss is 4.049217801094056 and perplexity is 57.352578371654225
At time: 1075.3437249660492 and batch: 250, loss is 4.015379905700684 and perplexity is 55.44435502845063
At time: 1076.8985266685486 and batch: 300, loss is 4.011135621070862 and perplexity is 55.209532084805545
At time: 1078.4516775608063 and batch: 350, loss is 3.9433157444000244 and perplexity is 51.58937520469482
At time: 1080.0046920776367 and batch: 400, loss is 3.9564931201934814 and perplexity is 52.27368660078817
At time: 1081.553132534027 and batch: 450, loss is 4.0103130578994755 and perplexity is 55.16413742954845
At time: 1083.1030852794647 and batch: 500, loss is 3.967581238746643 and perplexity is 52.856528775076285
At time: 1084.6514160633087 and batch: 550, loss is 3.9814672756195066 and perplexity is 53.59561610357869
At time: 1086.1973922252655 and batch: 600, loss is 4.0203854370117185 and perplexity is 55.72257923263175
At time: 1087.744369506836 and batch: 650, loss is 3.982521595954895 and perplexity is 53.65215285020805
At time: 1089.2868695259094 and batch: 700, loss is 3.9768884658813475 and perplexity is 53.35077294752603
At time: 1090.8287119865417 and batch: 750, loss is 3.9731830835342405 and perplexity is 53.15345373257337
At time: 1092.4010646343231 and batch: 800, loss is 3.9321674633026125 and perplexity is 51.01743633945984
At time: 1093.9551219940186 and batch: 850, loss is 3.9571014404296876 and perplexity is 52.30549541616065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.464392979939778 and perplexity of 86.86828274196847
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 1097.9839754104614 and batch: 50, loss is 4.020236644744873 and perplexity is 55.714288760547625
At time: 1099.5530405044556 and batch: 100, loss is 3.9913452434539796 and perplexity is 54.12765528236274
At time: 1101.0959310531616 and batch: 150, loss is 3.983803434371948 and perplexity is 53.72097033791592
At time: 1102.6353871822357 and batch: 200, loss is 4.047876925468445 and perplexity is 57.27572723267378
At time: 1104.1758122444153 and batch: 250, loss is 4.013609004020691 and perplexity is 55.34625541500723
At time: 1105.7207283973694 and batch: 300, loss is 4.010167593955994 and perplexity is 55.156113620180804
At time: 1107.2719802856445 and batch: 350, loss is 3.9427940797805787 and perplexity is 51.56246987130162
At time: 1108.8238642215729 and batch: 400, loss is 3.9563001108169558 and perplexity is 52.26359826273189
At time: 1110.3732779026031 and batch: 450, loss is 4.00988477230072 and perplexity is 55.14051648253638
At time: 1111.9165172576904 and batch: 500, loss is 3.966850242614746 and perplexity is 52.817904975640104
At time: 1113.4598288536072 and batch: 550, loss is 3.980501036643982 and perplexity is 53.54385494123549
At time: 1115.004554271698 and batch: 600, loss is 4.018890280723571 and perplexity is 55.63932752056975
At time: 1116.5508725643158 and batch: 650, loss is 3.980913600921631 and perplexity is 53.565949780529166
At time: 1118.0929837226868 and batch: 700, loss is 3.975133876800537 and perplexity is 53.25724633823734
At time: 1119.6355431079865 and batch: 750, loss is 3.971347522735596 and perplexity is 53.055976826352136
At time: 1121.1857225894928 and batch: 800, loss is 3.9301717185974123 and perplexity is 50.91572009460232
At time: 1122.7352693080902 and batch: 850, loss is 3.9555759239196777 and perplexity is 52.22576335109258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.464369138081868 and perplexity of 86.86621166540368
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 1126.7653892040253 and batch: 50, loss is 4.019984459877014 and perplexity is 55.70024023148635
At time: 1128.3337016105652 and batch: 100, loss is 3.9910484981536865 and perplexity is 54.11159553798649
At time: 1129.8746840953827 and batch: 150, loss is 3.9835331296920775 and perplexity is 53.706451270600716
At time: 1131.4461596012115 and batch: 200, loss is 4.046970915794373 and perplexity is 57.22385837010518
At time: 1132.9967987537384 and batch: 250, loss is 4.0126624059677125 and perplexity is 55.293889546016736
At time: 1134.5388894081116 and batch: 300, loss is 4.009621667861938 and perplexity is 55.12601067624781
At time: 1136.089275598526 and batch: 350, loss is 3.942343897819519 and perplexity is 51.5392626016369
At time: 1137.6369128227234 and batch: 400, loss is 3.955893898010254 and perplexity is 52.24237243118741
At time: 1139.1859285831451 and batch: 450, loss is 4.009607043266296 and perplexity is 55.12520448652742
At time: 1140.7367463111877 and batch: 500, loss is 3.966543130874634 and perplexity is 52.80168646750856
At time: 1142.2899656295776 and batch: 550, loss is 3.9800323343276975 and perplexity is 53.51876469278984
At time: 1143.8352801799774 and batch: 600, loss is 4.018198895454407 and perplexity is 55.600872604248956
At time: 1145.3808107376099 and batch: 650, loss is 3.9800810766220094 and perplexity is 53.521373383745974
At time: 1146.9259996414185 and batch: 700, loss is 3.9743037366867067 and perplexity is 53.21305370727555
At time: 1148.4694488048553 and batch: 750, loss is 3.9704595518112185 and perplexity is 53.00888557249226
At time: 1150.0172028541565 and batch: 800, loss is 3.9293399953842165 and perplexity is 50.873389914220084
At time: 1151.5646228790283 and batch: 850, loss is 3.954901723861694 and perplexity is 52.190564605244575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.464288711547852 and perplexity of 86.85922559801264
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 1155.6260840892792 and batch: 50, loss is 4.019853401184082 and perplexity is 55.692940709148985
At time: 1157.1745357513428 and batch: 100, loss is 3.990870532989502 and perplexity is 54.101966415851855
At time: 1158.7214903831482 and batch: 150, loss is 3.983377928733826 and perplexity is 53.69811662468846
At time: 1160.268929719925 and batch: 200, loss is 4.0465259885787965 and perplexity is 57.19840358132011
At time: 1161.822856426239 and batch: 250, loss is 4.012258424758911 and perplexity is 55.27155636507504
At time: 1163.3813815116882 and batch: 300, loss is 4.009374356269836 and perplexity is 55.112379060479
At time: 1164.9412784576416 and batch: 350, loss is 3.9421459197998048 and perplexity is 51.52905997047118
At time: 1166.4876780509949 and batch: 400, loss is 3.9556426906585695 and perplexity is 52.2292504114062
At time: 1168.0377645492554 and batch: 450, loss is 4.0094378423690795 and perplexity is 55.11587804151273
At time: 1169.5853221416473 and batch: 500, loss is 3.9664533138275146 and perplexity is 52.79694418891904
At time: 1171.1595616340637 and batch: 550, loss is 3.979822187423706 and perplexity is 53.5075190717418
At time: 1172.7133333683014 and batch: 600, loss is 4.017888813018799 and perplexity is 55.58363442301666
At time: 1174.269767522812 and batch: 650, loss is 3.9797005462646484 and perplexity is 53.501010750951494
At time: 1175.8222830295563 and batch: 700, loss is 3.973915333747864 and perplexity is 53.19238961408789
At time: 1177.3688561916351 and batch: 750, loss is 3.9700824069976806 and perplexity is 52.98889731569779
At time: 1178.9233357906342 and batch: 800, loss is 3.928969397544861 and perplexity is 50.85453983895151
At time: 1180.4701578617096 and batch: 850, loss is 3.9546052503585813 and perplexity is 52.17509377918494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.464260737101237 and perplexity of 86.85679579322958
Annealing...
Model not improving. Stopping early with 86.85138419959085loss at 40 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -86.85138419959085
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fc6e480c898>
SETTINGS FOR THIS RUN
{'lr': 20.945653848541383, 'anneal': 7.9288002959792685, 'wordvec_source': 'glove', 'seq_len': 50, 'tune_wordvecs': True, 'num_layers': 1, 'dropout': 0.6085402528955687, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.313814401626587 and batch: 50, loss is 7.405180501937866 and perplexity is 1644.4816414122215
At time: 3.8834643363952637 and batch: 100, loss is 6.416346750259399 and perplexity is 611.7640997377106
At time: 5.451365947723389 and batch: 150, loss is 6.351140003204346 and perplexity is 573.1457246856687
At time: 7.020685911178589 and batch: 200, loss is 6.43647952079773 and perplexity is 624.2054249214616
At time: 8.594256162643433 and batch: 250, loss is 6.524871301651001 and perplexity is 681.8920096274394
At time: 10.151094675064087 and batch: 300, loss is 6.530780096054077 and perplexity is 685.9330965357958
At time: 11.703917026519775 and batch: 350, loss is 6.583290243148804 and perplexity is 722.9139832929912
At time: 13.260932683944702 and batch: 400, loss is 6.620036363601685 and perplexity is 749.9723683125156
At time: 14.811259031295776 and batch: 450, loss is 6.689850177764892 and perplexity is 804.201755813159
At time: 16.363617181777954 and batch: 500, loss is 6.727587747573852 and perplexity is 835.1302891374711
At time: 17.98688316345215 and batch: 550, loss is 6.714215383529663 and perplexity is 824.0369602119314
At time: 19.54601526260376 and batch: 600, loss is 6.765666236877442 and perplexity is 867.544005102522
At time: 21.10961389541626 and batch: 650, loss is 6.802191486358643 and perplexity is 899.8170693243251
At time: 22.66122078895569 and batch: 700, loss is 6.757716932296753 and perplexity is 860.6749717598921
At time: 24.213279962539673 and batch: 750, loss is 6.808753480911255 and perplexity is 905.7410794354228
At time: 25.768632173538208 and batch: 800, loss is 7.014814329147339 and perplexity is 1112.9999753713073
At time: 27.316275119781494 and batch: 850, loss is 6.993163375854492 and perplexity is 1089.1614593996665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.314289728800456 and perplexity of 552.409560616096
Finished 1 epochs...
Completing Train Step...
At time: 31.49843192100525 and batch: 50, loss is 6.816289854049683 and perplexity is 912.5928685666171
At time: 33.038957357406616 and batch: 100, loss is 6.822138652801514 and perplexity is 917.946080265759
At time: 34.577974796295166 and batch: 150, loss is 6.837859725952148 and perplexity is 932.4912107012926
At time: 36.11843705177307 and batch: 200, loss is 7.04775954246521 and perplexity is 1150.278703040552
At time: 37.659050941467285 and batch: 250, loss is 7.076815233230591 and perplexity is 1184.1911340851875
At time: 39.19797372817993 and batch: 300, loss is 6.945913915634155 and perplexity is 1038.89602679446
At time: 40.790674924850464 and batch: 350, loss is 6.8664897918701175 and perplexity is 959.5743415070771
At time: 42.330398082733154 and batch: 400, loss is 6.887209405899048 and perplexity is 979.6637552468857
At time: 43.87952136993408 and batch: 450, loss is 6.873358316421509 and perplexity is 966.1878880837379
At time: 45.42832827568054 and batch: 500, loss is 6.838307285308838 and perplexity is 932.908649274975
At time: 46.97540807723999 and batch: 550, loss is 6.890574464797973 and perplexity is 982.9659343820246
At time: 48.52690267562866 and batch: 600, loss is 6.9917682266235355 and perplexity is 1087.642976128884
At time: 50.06941771507263 and batch: 650, loss is 6.784757795333863 and perplexity is 884.2658876820136
At time: 51.60752272605896 and batch: 700, loss is 6.705626640319824 and perplexity is 816.9898247040386
At time: 53.148926973342896 and batch: 750, loss is 6.680169477462768 and perplexity is 796.4540815953917
At time: 54.68584370613098 and batch: 800, loss is 6.701310300827027 and perplexity is 813.4710189018555
At time: 56.230711221694946 and batch: 850, loss is 6.8722625255584715 and perplexity is 965.129728090907
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.560000101725261 and perplexity of 706.2717664410419
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 60.93533706665039 and batch: 50, loss is 6.62091814994812 and perplexity is 750.63397536226
At time: 62.477434396743774 and batch: 100, loss is 6.378462438583374 and perplexity is 589.0213545419241
At time: 64.02205777168274 and batch: 150, loss is 6.303226652145386 and perplexity is 546.3318921459618
At time: 65.5645637512207 and batch: 200, loss is 6.319036655426025 and perplexity is 555.038041935887
At time: 67.1188178062439 and batch: 250, loss is 6.356538143157959 and perplexity is 576.2480112765734
At time: 68.68419003486633 and batch: 300, loss is 6.302039976119995 and perplexity is 545.6839577078127
At time: 70.23281264305115 and batch: 350, loss is 6.259759378433228 and perplexity is 523.0930574925104
At time: 71.83852338790894 and batch: 400, loss is 6.2684589290618895 and perplexity is 527.6635839673203
At time: 73.38562035560608 and batch: 450, loss is 6.260104074478149 and perplexity is 523.2733966798706
At time: 74.93283748626709 and batch: 500, loss is 6.261004238128662 and perplexity is 523.7446404372724
At time: 76.47997188568115 and batch: 550, loss is 6.230164041519165 and perplexity is 507.838783308798
At time: 78.03187704086304 and batch: 600, loss is 6.235727872848511 and perplexity is 510.6721876241908
At time: 79.58457922935486 and batch: 650, loss is 6.25199291229248 and perplexity is 519.0462081736072
At time: 81.13591194152832 and batch: 700, loss is 6.210864782333374 and perplexity is 498.13184069243886
At time: 82.68051838874817 and batch: 750, loss is 6.194651460647583 and perplexity is 490.12058891204686
At time: 84.23033809661865 and batch: 800, loss is 6.171526136398316 and perplexity is 478.91644046411744
At time: 85.78839921951294 and batch: 850, loss is 6.172311954498291 and perplexity is 479.29292957801596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.7733720143636065 and perplexity of 321.6204148700396
Finished 3 epochs...
Completing Train Step...
At time: 89.89972996711731 and batch: 50, loss is 6.226792440414429 and perplexity is 506.1294367426287
At time: 91.48723864555359 and batch: 100, loss is 6.147230434417724 and perplexity is 467.42103922858934
At time: 93.0393795967102 and batch: 150, loss is 6.122232360839844 and perplexity is 455.8812508691068
At time: 94.58828234672546 and batch: 200, loss is 6.161125392913818 and perplexity is 473.9611673484612
At time: 96.14265489578247 and batch: 250, loss is 6.215190887451172 and perplexity is 500.29147944175054
At time: 97.69413876533508 and batch: 300, loss is 6.1636433410644536 and perplexity is 475.15608072690617
At time: 99.24347424507141 and batch: 350, loss is 6.1369442653656 and perplexity is 462.637710638859
At time: 100.79674482345581 and batch: 400, loss is 6.161462678909301 and perplexity is 474.1210547749899
At time: 102.3517792224884 and batch: 450, loss is 6.17127613067627 and perplexity is 478.79672357919605
At time: 103.90799498558044 and batch: 500, loss is 6.177477893829345 and perplexity is 481.77533422928724
At time: 105.4542248249054 and batch: 550, loss is 6.143521070480347 and perplexity is 465.6904162217465
At time: 107.00340008735657 and batch: 600, loss is 6.14923882484436 and perplexity is 468.36074650266136
At time: 108.56474876403809 and batch: 650, loss is 6.1609266567230225 and perplexity is 473.8669834706598
At time: 110.11632442474365 and batch: 700, loss is 6.118349895477295 and perplexity is 454.1147391324518
At time: 111.72145342826843 and batch: 750, loss is 6.118957901000977 and perplexity is 454.390927355704
At time: 113.2743730545044 and batch: 800, loss is 6.133419485092163 and perplexity is 461.0098849120083
At time: 114.82554197311401 and batch: 850, loss is 6.137882900238037 and perplexity is 463.0721623912543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.738585789998372 and perplexity of 310.62481149649284
Finished 4 epochs...
Completing Train Step...
At time: 118.95214176177979 and batch: 50, loss is 6.163720541000366 and perplexity is 475.1927641618479
At time: 120.50011897087097 and batch: 100, loss is 6.097652597427368 and perplexity is 444.81238985015045
At time: 122.05986475944519 and batch: 150, loss is 6.083375816345215 and perplexity is 438.5070180438705
At time: 123.61073327064514 and batch: 200, loss is 6.118793239593506 and perplexity is 454.31611286576225
At time: 125.1600067615509 and batch: 250, loss is 6.168401899337769 and perplexity is 477.4225268571985
At time: 126.7054328918457 and batch: 300, loss is 6.119905872344971 and perplexity is 454.821881167467
At time: 128.2492380142212 and batch: 350, loss is 6.098616733551025 and perplexity is 445.24145634947405
At time: 129.79312086105347 and batch: 400, loss is 6.125619106292724 and perplexity is 457.4278220657554
At time: 131.3361587524414 and batch: 450, loss is 6.133232135772705 and perplexity is 460.9235231139712
At time: 132.88839101791382 and batch: 500, loss is 6.140003108978272 and perplexity is 464.0550135939878
At time: 134.4417688846588 and batch: 550, loss is 6.105996541976928 and perplexity is 448.53940715555825
At time: 135.9969847202301 and batch: 600, loss is 6.113532409667969 and perplexity is 451.9323089549295
At time: 137.54309129714966 and batch: 650, loss is 6.135155372619629 and perplexity is 461.81084120515806
At time: 139.0904347896576 and batch: 700, loss is 6.088929977416992 and perplexity is 440.9493328796663
At time: 140.65458393096924 and batch: 750, loss is 6.08170636177063 and perplexity is 437.7755612334544
At time: 142.20669054985046 and batch: 800, loss is 6.104764595031738 and perplexity is 447.9871706360058
At time: 143.7586212158203 and batch: 850, loss is 6.111216011047364 and perplexity is 450.8866651093912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.710409800211589 and perplexity of 301.99480046145
Finished 5 epochs...
Completing Train Step...
At time: 148.55955290794373 and batch: 50, loss is 6.124126386642456 and perplexity is 456.745519936769
At time: 150.10437083244324 and batch: 100, loss is 6.059877147674561 and perplexity is 428.322813173245
At time: 151.6769425868988 and batch: 150, loss is 6.049642782211304 and perplexity is 423.9615563821687
At time: 153.2203369140625 and batch: 200, loss is 6.086025829315186 and perplexity is 439.67060841294557
At time: 154.7729172706604 and batch: 250, loss is 6.130450162887573 and perplexity is 459.6430283472614
At time: 156.32092356681824 and batch: 300, loss is 6.077030944824219 and perplexity is 435.7335552901066
At time: 157.86654353141785 and batch: 350, loss is 6.043568391799926 and perplexity is 421.39405426907956
At time: 159.41542863845825 and batch: 400, loss is 6.054474372863769 and perplexity is 426.01492159053856
At time: 160.96635746955872 and batch: 450, loss is 6.057416944503784 and perplexity is 427.27034720106747
At time: 162.52907848358154 and batch: 500, loss is 6.061638841629028 and perplexity is 429.0780519382282
At time: 164.06647539138794 and batch: 550, loss is 6.021676416397095 and perplexity is 412.26915150102025
At time: 165.60102128982544 and batch: 600, loss is 6.02222710609436 and perplexity is 412.49624639892335
At time: 167.13645696640015 and batch: 650, loss is 6.037742500305176 and perplexity is 418.9461956353348
At time: 168.6718578338623 and batch: 700, loss is 5.98896969795227 and perplexity is 399.0033041575812
At time: 170.21011877059937 and batch: 750, loss is 5.982323341369629 and perplexity is 396.36017922568124
At time: 171.75181412696838 and batch: 800, loss is 6.004989509582519 and perplexity is 405.4467354075704
At time: 173.2868573665619 and batch: 850, loss is 6.00990782737732 and perplexity is 407.445763198467
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.617003122965495 and perplexity of 275.06381436019075
Finished 6 epochs...
Completing Train Step...
At time: 177.33316254615784 and batch: 50, loss is 6.0167356204986575 and perplexity is 410.23723753781735
At time: 178.88347697257996 and batch: 100, loss is 5.952883787155152 and perplexity is 384.8615992362263
At time: 180.42394399642944 and batch: 150, loss is 5.943527584075928 and perplexity is 381.2775486540972
At time: 181.96492290496826 and batch: 200, loss is 5.973551073074341 and perplexity is 392.8984073869781
At time: 183.5063350200653 and batch: 250, loss is 6.024766893386841 and perplexity is 413.5452306582949
At time: 185.0516755580902 and batch: 300, loss is 5.975083990097046 and perplexity is 393.501149902958
At time: 186.60268330574036 and batch: 350, loss is 5.953321676254273 and perplexity is 385.03016283859074
At time: 188.15578937530518 and batch: 400, loss is 5.978669881820679 and perplexity is 394.91473538714973
At time: 189.70200562477112 and batch: 450, loss is 5.98715386390686 and perplexity is 398.27943778316177
At time: 191.29503297805786 and batch: 500, loss is 5.995195064544678 and perplexity is 401.49499379109227
At time: 192.83905744552612 and batch: 550, loss is 5.9612458801269534 and perplexity is 388.09334094111625
At time: 194.38272738456726 and batch: 600, loss is 5.966614313125611 and perplexity is 390.1823964993558
At time: 195.93372249603271 and batch: 650, loss is 5.9889940166473385 and perplexity is 399.01300751525235
At time: 197.48441410064697 and batch: 700, loss is 5.940302858352661 and perplexity is 380.0500134311996
At time: 199.0360221862793 and batch: 750, loss is 5.936779661178589 and perplexity is 378.7133782950682
At time: 200.5798065662384 and batch: 800, loss is 5.959331636428833 and perplexity is 387.35114630627555
At time: 202.12974882125854 and batch: 850, loss is 5.962672777175904 and perplexity is 388.64750545785427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.594516118367513 and perplexity of 268.9474798557072
Finished 7 epochs...
Completing Train Step...
At time: 206.19030737876892 and batch: 50, loss is 5.977431335449219 and perplexity is 394.4259179486194
At time: 207.7625367641449 and batch: 100, loss is 5.916980609893799 and perplexity is 371.2889535106314
At time: 209.30992674827576 and batch: 150, loss is 5.905570974349976 and perplexity is 367.0767573761269
At time: 210.85791182518005 and batch: 200, loss is 5.937789506912232 and perplexity is 379.0960135531872
At time: 212.3991208076477 and batch: 250, loss is 5.987310285568237 and perplexity is 398.34174218726446
At time: 213.93927216529846 and batch: 300, loss is 5.94216835975647 and perplexity is 380.7596589813552
At time: 215.4784209728241 and batch: 350, loss is 5.920593433380127 and perplexity is 372.632781006166
At time: 217.0179204940796 and batch: 400, loss is 5.946569881439209 and perplexity is 382.4392745968957
At time: 218.56438183784485 and batch: 450, loss is 5.956959657669067 and perplexity is 386.4334464272465
At time: 220.1130712032318 and batch: 500, loss is 5.965499601364136 and perplexity is 389.7476979196796
At time: 221.66888999938965 and batch: 550, loss is 5.933246622085571 and perplexity is 377.3777299627265
At time: 223.21360206604004 and batch: 600, loss is 5.940775833129883 and perplexity is 380.2298100179089
At time: 224.76179003715515 and batch: 650, loss is 5.961132116317749 and perplexity is 388.0491924756205
At time: 226.3081293106079 and batch: 700, loss is 5.9128368377685545 and perplexity is 369.7535999692093
At time: 227.85442090034485 and batch: 750, loss is 5.910272970199585 and perplexity is 368.8068149400882
At time: 229.4539225101471 and batch: 800, loss is 5.935554752349853 and perplexity is 378.24977292955316
At time: 231.00770783424377 and batch: 850, loss is 5.937437219619751 and perplexity is 378.96248636632885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.576999664306641 and perplexity of 264.2774938829515
Finished 8 epochs...
Completing Train Step...
At time: 235.08848881721497 and batch: 50, loss is 5.951632490158081 and perplexity is 384.38032424458635
At time: 236.67169833183289 and batch: 100, loss is 5.893245506286621 and perplexity is 362.58013295270746
At time: 238.22313141822815 and batch: 150, loss is 5.884489116668701 and perplexity is 359.4190998570365
At time: 239.7731795310974 and batch: 200, loss is 5.9120838069915775 and perplexity is 369.4752689376034
At time: 241.32759475708008 and batch: 250, loss is 5.960480451583862 and perplexity is 387.79639687980426
At time: 242.88648319244385 and batch: 300, loss is 5.91225233078003 and perplexity is 369.537539556557
At time: 244.44235515594482 and batch: 350, loss is 5.890759725570678 and perplexity is 361.6799575331024
At time: 245.9878900051117 and batch: 400, loss is 5.9150582122802735 and perplexity is 370.5758741431167
At time: 247.534814119339 and batch: 450, loss is 5.922976789474487 and perplexity is 373.5219568061112
At time: 249.08688735961914 and batch: 500, loss is 5.931381406784058 and perplexity is 376.6744952922648
At time: 250.64501905441284 and batch: 550, loss is 5.902827234268188 and perplexity is 366.0709745980525
At time: 252.20834946632385 and batch: 600, loss is 5.91120831489563 and perplexity is 369.15193781757273
At time: 253.7609498500824 and batch: 650, loss is 5.931281299591064 and perplexity is 376.6367893532181
At time: 255.31252789497375 and batch: 700, loss is 5.879996194839477 and perplexity is 357.80788018984174
At time: 256.86513590812683 and batch: 750, loss is 5.878411827087402 and perplexity is 357.2414297744092
At time: 258.4131922721863 and batch: 800, loss is 5.8986923885345455 and perplexity is 364.5604526302915
At time: 259.96162700653076 and batch: 850, loss is 5.901959419250488 and perplexity is 365.75343051344356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.54893430074056 and perplexity of 256.96356405533317
Finished 9 epochs...
Completing Train Step...
At time: 264.08086228370667 and batch: 50, loss is 5.921028308868408 and perplexity is 372.794865109403
At time: 265.67978405952454 and batch: 100, loss is 5.854662437438964 and perplexity is 348.8571190733292
At time: 267.231657743454 and batch: 150, loss is 5.846470470428467 and perplexity is 346.01096677207397
At time: 268.8149576187134 and batch: 200, loss is 5.881524286270142 and perplexity is 358.3550613102994
At time: 270.3620400428772 and batch: 250, loss is 5.926099157333374 and perplexity is 374.6900524217364
At time: 271.907345533371 and batch: 300, loss is 5.8798693180084225 and perplexity is 357.7624855397024
At time: 273.45539355278015 and batch: 350, loss is 5.858735570907593 and perplexity is 350.2809584562114
At time: 275.0116000175476 and batch: 400, loss is 5.884018592834472 and perplexity is 359.25002438421603
At time: 276.5711236000061 and batch: 450, loss is 5.893202505111694 and perplexity is 362.5645419162043
At time: 278.11784291267395 and batch: 500, loss is 5.904958181381225 and perplexity is 366.8518842275942
At time: 279.6636321544647 and batch: 550, loss is 5.875867023468017 and perplexity is 356.33347626261076
At time: 281.2112476825714 and batch: 600, loss is 5.886149263381958 and perplexity is 360.01628386372124
At time: 282.7602708339691 and batch: 650, loss is 5.90627459526062 and perplexity is 367.33513114636236
At time: 284.3085734844208 and batch: 700, loss is 5.857109670639038 and perplexity is 349.71189929387174
At time: 285.860720872879 and batch: 750, loss is 5.855298376083374 and perplexity is 349.0790413537232
At time: 287.4161310195923 and batch: 800, loss is 5.881968030929565 and perplexity is 358.51411474187876
At time: 288.98191022872925 and batch: 850, loss is 5.8807814502716065 and perplexity is 358.0889611171385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.534486770629883 and perplexity of 253.27776468650984
Finished 10 epochs...
Completing Train Step...
At time: 293.1364071369171 and batch: 50, loss is 5.898135499954224 and perplexity is 364.3574895965279
At time: 294.6897692680359 and batch: 100, loss is 5.837080993652344 and perplexity is 342.7773097661413
At time: 296.2539098262787 and batch: 150, loss is 5.831035318374634 and perplexity is 340.71124114166037
At time: 297.8113567829132 and batch: 200, loss is 5.865168771743774 and perplexity is 352.5416501561604
At time: 299.361487865448 and batch: 250, loss is 5.9059015560150145 and perplexity is 367.1981262818395
At time: 300.91277050971985 and batch: 300, loss is 5.858336057662964 and perplexity is 350.14104452456223
At time: 302.4603395462036 and batch: 350, loss is 5.840446357727051 and perplexity is 343.9328234816451
At time: 304.0093891620636 and batch: 400, loss is 5.864188203811645 and perplexity is 352.19612855066583
At time: 305.56966733932495 and batch: 450, loss is 5.871910476684571 and perplexity is 354.92641158742316
At time: 307.12474179267883 and batch: 500, loss is 5.883361301422119 and perplexity is 359.01397001505404
At time: 308.71117997169495 and batch: 550, loss is 5.856458549499512 and perplexity is 349.4842685991184
At time: 310.2650291919708 and batch: 600, loss is 5.867644863128662 and perplexity is 353.4156571129275
At time: 311.81379294395447 and batch: 650, loss is 5.889393615722656 and perplexity is 361.18620032136874
At time: 313.36332511901855 and batch: 700, loss is 5.837959995269776 and perplexity is 343.07874403704204
At time: 314.91248655319214 and batch: 750, loss is 5.838751611709594 and perplexity is 343.3504383356226
At time: 316.4655508995056 and batch: 800, loss is 5.861125879287719 and perplexity is 351.1192394425097
At time: 318.02437686920166 and batch: 850, loss is 5.862826128005981 and perplexity is 351.71673728285793
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.523342132568359 and perplexity of 250.47074632515339
Finished 11 epochs...
Completing Train Step...
At time: 322.134920835495 and batch: 50, loss is 5.882946043014527 and perplexity is 358.86491739538076
At time: 323.6915805339813 and batch: 100, loss is 5.818788623809814 and perplexity is 336.5641008495752
At time: 325.2401638031006 and batch: 150, loss is 5.814368515014649 and perplexity is 335.07973385579237
At time: 326.7887098789215 and batch: 200, loss is 5.850082864761353 and perplexity is 347.2631551649937
At time: 328.3425714969635 and batch: 250, loss is 5.889414587020874 and perplexity is 361.19377494431245
At time: 329.89687275886536 and batch: 300, loss is 5.845919847488403 and perplexity is 345.8204976394049
At time: 331.4522783756256 and batch: 350, loss is 5.826720361709595 and perplexity is 339.24425416958223
At time: 332.99810886383057 and batch: 400, loss is 5.848751020431519 and perplexity is 346.80096255345154
At time: 334.54816198349 and batch: 450, loss is 5.857790050506591 and perplexity is 349.9499171917062
At time: 336.10044527053833 and batch: 500, loss is 5.872122278213501 and perplexity is 355.0015935055967
At time: 337.6474814414978 and batch: 550, loss is 5.844123754501343 and perplexity is 345.1999293348298
At time: 339.221533536911 and batch: 600, loss is 5.854085912704468 and perplexity is 348.65605228093705
At time: 340.775648355484 and batch: 650, loss is 5.875659494400025 and perplexity is 356.2595343811977
At time: 342.3370509147644 and batch: 700, loss is 5.826187677383423 and perplexity is 339.06359219481544
At time: 343.887490272522 and batch: 750, loss is 5.826786031723023 and perplexity is 339.266533075829
At time: 345.43324279785156 and batch: 800, loss is 5.847447557449341 and perplexity is 346.3492148187647
At time: 346.98417019844055 and batch: 850, loss is 5.852664766311645 and perplexity is 348.1609129060048
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.517244338989258 and perplexity of 248.948074603516
Finished 12 epochs...
Completing Train Step...
At time: 351.16677927970886 and batch: 50, loss is 5.868167114257813 and perplexity is 353.6002770437016
At time: 352.70424723625183 and batch: 100, loss is 5.805584344863892 and perplexity is 332.14922634139094
At time: 354.2431104183197 and batch: 150, loss is 5.800917749404907 and perplexity is 330.6028312764688
At time: 355.78315711021423 and batch: 200, loss is 5.838486919403076 and perplexity is 343.2595681430064
At time: 357.3233036994934 and batch: 250, loss is 5.879456272125244 and perplexity is 357.6147437320679
At time: 358.8660526275635 and batch: 300, loss is 5.834880599975586 and perplexity is 342.02389395088016
At time: 360.407185792923 and batch: 350, loss is 5.81420111656189 and perplexity is 335.02364672137475
At time: 361.9515836238861 and batch: 400, loss is 5.837961006164551 and perplexity is 343.0790908537271
At time: 363.4905984401703 and batch: 450, loss is 5.846948614120484 and perplexity is 346.1764492922635
At time: 365.02879786491394 and batch: 500, loss is 5.863926115036011 and perplexity is 352.1038339937629
At time: 366.5681207180023 and batch: 550, loss is 5.836712541580201 and perplexity is 342.6510360204145
At time: 368.1089651584625 and batch: 600, loss is 5.84565616607666 and perplexity is 345.7293232234052
At time: 369.6511948108673 and batch: 650, loss is 5.862673301696777 and perplexity is 351.66298981913155
At time: 371.2041416168213 and batch: 700, loss is 5.815174236297607 and perplexity is 335.349823522733
At time: 372.7614963054657 and batch: 750, loss is 5.812582550048828 and perplexity is 334.4818272694045
At time: 374.31363344192505 and batch: 800, loss is 5.8313935852050784 and perplexity is 340.8333285467489
At time: 375.8760735988617 and batch: 850, loss is 5.835897760391235 and perplexity is 342.3719641085966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.507771174112956 and perplexity of 246.60088366376004
Finished 13 epochs...
Completing Train Step...
At time: 379.9792642593384 and batch: 50, loss is 5.8514101028442385 and perplexity is 347.72436204727717
At time: 381.5681221485138 and batch: 100, loss is 5.7890078830719 and perplexity is 326.688750104374
At time: 383.11528158187866 and batch: 150, loss is 5.78448070526123 and perplexity is 325.2131147959215
At time: 384.6707007884979 and batch: 200, loss is 5.820589265823364 and perplexity is 337.17067826031
At time: 386.2284598350525 and batch: 250, loss is 5.860220756530762 and perplexity is 350.80157721182854
At time: 387.82299876213074 and batch: 300, loss is 5.8166124057769775 and perplexity is 335.8324603774449
At time: 389.37314772605896 and batch: 350, loss is 5.79690318107605 and perplexity is 329.2782641821258
At time: 390.92134046554565 and batch: 400, loss is 5.820276432037353 and perplexity is 337.0652163773781
At time: 392.46833205223083 and batch: 450, loss is 5.826982669830322 and perplexity is 339.3332523643253
At time: 394.0131154060364 and batch: 500, loss is 5.841346035003662 and perplexity is 344.2423912622803
At time: 395.5637013912201 and batch: 550, loss is 5.815414657592774 and perplexity is 335.43045845442714
At time: 397.12056016921997 and batch: 600, loss is 5.826983938217163 and perplexity is 339.33368277043024
At time: 398.6750464439392 and batch: 650, loss is 5.846817588806152 and perplexity is 346.13109438556614
At time: 400.2331175804138 and batch: 700, loss is 5.800412120819092 and perplexity is 330.4357112883062
At time: 401.78392601013184 and batch: 750, loss is 5.797274799346924 and perplexity is 329.4006527408013
At time: 403.33411622047424 and batch: 800, loss is 5.816149702072144 and perplexity is 335.6771053982588
At time: 404.8887093067169 and batch: 850, loss is 5.821861248016358 and perplexity is 337.5998262354868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.499198913574219 and perplexity of 244.4959913720584
Finished 14 epochs...
Completing Train Step...
At time: 409.01302695274353 and batch: 50, loss is 5.838849401473999 and perplexity is 343.38401613585137
At time: 410.604877948761 and batch: 100, loss is 5.77386248588562 and perplexity is 321.7781992156325
At time: 412.15917778015137 and batch: 150, loss is 5.774167108535766 and perplexity is 321.8762350746504
At time: 413.7146294116974 and batch: 200, loss is 5.809899711608887 and perplexity is 333.5856692268841
At time: 415.2748439311981 and batch: 250, loss is 5.849313831329345 and perplexity is 346.99620085052044
At time: 416.8187303543091 and batch: 300, loss is 5.802876014709472 and perplexity is 331.25087364289664
At time: 418.37177562713623 and batch: 350, loss is 5.780893745422364 and perplexity is 324.04867805648274
At time: 419.9188666343689 and batch: 400, loss is 5.805473546981812 and perplexity is 332.11242694926284
At time: 421.4728162288666 and batch: 450, loss is 5.814211235046387 and perplexity is 335.0270366701009
At time: 423.025559425354 and batch: 500, loss is 5.830306577682495 and perplexity is 340.4630414435652
At time: 424.5735695362091 and batch: 550, loss is 5.803173894882202 and perplexity is 331.34956140818895
At time: 426.16891837120056 and batch: 600, loss is 5.813197755813599 and perplexity is 334.68766572776286
At time: 427.7239372730255 and batch: 650, loss is 5.836156787872315 and perplexity is 342.4606593428075
At time: 429.2812340259552 and batch: 700, loss is 5.7883596611022945 and perplexity is 326.47705190045957
At time: 430.8343963623047 and batch: 750, loss is 5.785979976654053 and perplexity is 325.7010632076518
At time: 432.3968963623047 and batch: 800, loss is 5.806344203948974 and perplexity is 332.4017088620498
At time: 433.9550311565399 and batch: 850, loss is 5.812306652069092 and perplexity is 334.3895571381591
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.489938735961914 and perplexity of 242.24236565782712
Finished 15 epochs...
Completing Train Step...
At time: 438.66399931907654 and batch: 50, loss is 5.830025329589843 and perplexity is 340.3673003266752
At time: 440.24341678619385 and batch: 100, loss is 5.767329177856445 and perplexity is 319.6827756004847
At time: 441.80086636543274 and batch: 150, loss is 5.764235506057739 and perplexity is 318.6953102475045
At time: 443.35424757003784 and batch: 200, loss is 5.797631731033325 and perplexity is 329.5182472566419
At time: 444.8994388580322 and batch: 250, loss is 5.83838321685791 and perplexity is 343.22397309781763
At time: 446.4477937221527 and batch: 300, loss is 5.792119188308716 and perplexity is 327.70676137406554
At time: 447.9944701194763 and batch: 350, loss is 5.771215581893921 and perplexity is 320.92760942705013
At time: 449.5480468273163 and batch: 400, loss is 5.795795536041259 and perplexity is 328.91374266523286
At time: 451.10562682151794 and batch: 450, loss is 5.805844650268555 and perplexity is 332.23569783417247
At time: 452.66702795028687 and batch: 500, loss is 5.822957859039307 and perplexity is 337.9702449918072
At time: 454.2325367927551 and batch: 550, loss is 5.796617250442505 and perplexity is 329.18412689844376
At time: 455.7840507030487 and batch: 600, loss is 5.806792182922363 and perplexity is 332.5506511973649
At time: 457.3315677642822 and batch: 650, loss is 5.825548505783081 and perplexity is 338.84694162179704
At time: 458.8816478252411 and batch: 700, loss is 5.779059867858887 and perplexity is 323.45495702855237
At time: 460.42898082733154 and batch: 750, loss is 5.776347045898437 and perplexity is 322.57867046004446
At time: 461.9784071445465 and batch: 800, loss is 5.797674903869629 and perplexity is 329.5324738010876
At time: 463.54247856140137 and batch: 850, loss is 5.803262252807617 and perplexity is 331.37884006150347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.48719851175944 and perplexity of 241.57947591264247
Finished 16 epochs...
Completing Train Step...
At time: 467.6898047924042 and batch: 50, loss is 5.820597562789917 and perplexity is 337.17347576575554
At time: 469.2484085559845 and batch: 100, loss is 5.757842588424682 and perplexity is 316.66441596576357
At time: 470.795028924942 and batch: 150, loss is 5.755333261489868 and perplexity is 315.8707975579396
At time: 472.34264183044434 and batch: 200, loss is 5.790850801467895 and perplexity is 327.29136592704106
At time: 473.88950848579407 and batch: 250, loss is 5.831469392776489 and perplexity is 340.8591672730149
At time: 475.42925810813904 and batch: 300, loss is 5.78433217048645 and perplexity is 325.1648129265022
At time: 476.9664349555969 and batch: 350, loss is 5.763339986801148 and perplexity is 318.41004021166214
At time: 478.50412154197693 and batch: 400, loss is 5.78740668296814 and perplexity is 326.1660746095093
At time: 480.043399810791 and batch: 450, loss is 5.797160596847534 and perplexity is 329.3630365109323
At time: 481.58825492858887 and batch: 500, loss is 5.814268016815186 and perplexity is 335.0460606379406
At time: 483.1279799938202 and batch: 550, loss is 5.786029825210571 and perplexity is 325.7172993401797
At time: 484.6671566963196 and batch: 600, loss is 5.798385238647461 and perplexity is 329.7666353343478
At time: 486.2078969478607 and batch: 650, loss is 5.816952047348022 and perplexity is 335.9465424142997
At time: 487.74820733070374 and batch: 700, loss is 5.770729894638062 and perplexity is 320.7717768231161
At time: 489.2922394275665 and batch: 750, loss is 5.772143716812134 and perplexity is 321.22561181939847
At time: 490.8470447063446 and batch: 800, loss is 5.7893016910552975 and perplexity is 326.7847479690198
At time: 492.40099835395813 and batch: 850, loss is 5.796475400924683 and perplexity is 329.13743560041576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.480496088663737 and perplexity of 239.9657221253119
Finished 17 epochs...
Completing Train Step...
At time: 496.5192425251007 and batch: 50, loss is 5.812782907485962 and perplexity is 334.5488499051018
At time: 498.0741386413574 and batch: 100, loss is 5.748168144226074 and perplexity is 313.61563515289197
At time: 499.62947845458984 and batch: 150, loss is 5.749082193374634 and perplexity is 313.9024263078165
At time: 501.190447807312 and batch: 200, loss is 5.785117797851562 and perplexity is 325.42037167553775
At time: 502.74978041648865 and batch: 250, loss is 5.825950269699097 and perplexity is 338.9831054470173
At time: 504.3065218925476 and batch: 300, loss is 5.7811943054199215 and perplexity is 324.1460887645158
At time: 505.8926830291748 and batch: 350, loss is 5.754873895645142 and perplexity is 315.7257306241433
At time: 507.44976139068604 and batch: 400, loss is 5.779955148696899 and perplexity is 323.7446697213002
At time: 508.996235370636 and batch: 450, loss is 5.78950192451477 and perplexity is 326.8501877610078
At time: 510.5480251312256 and batch: 500, loss is 5.805601758956909 and perplexity is 332.1550104692766
At time: 512.1081485748291 and batch: 550, loss is 5.778189868927002 and perplexity is 323.1736739371609
At time: 513.6563038825989 and batch: 600, loss is 5.791750841140747 and perplexity is 327.5860737454268
At time: 515.2103102207184 and batch: 650, loss is 5.809029054641724 and perplexity is 333.2953569393729
At time: 516.7601351737976 and batch: 700, loss is 5.763791551589966 and perplexity is 318.5538554427314
At time: 518.3196094036102 and batch: 750, loss is 5.764333381652832 and perplexity is 318.7265042671899
At time: 519.8874588012695 and batch: 800, loss is 5.782460699081421 and perplexity is 324.5568453516001
At time: 521.4520018100739 and batch: 850, loss is 5.79217604637146 and perplexity is 327.7253946753871
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.478542963663737 and perplexity of 239.49749647474272
Finished 18 epochs...
Completing Train Step...
At time: 525.6107804775238 and batch: 50, loss is 5.809666776657105 and perplexity is 333.50797451436586
At time: 527.1686384677887 and batch: 100, loss is 5.746707210540771 and perplexity is 313.1577980235329
At time: 528.7174386978149 and batch: 150, loss is 5.74260479927063 and perplexity is 311.8757275318853
At time: 530.2721552848816 and batch: 200, loss is 5.779401330947876 and perplexity is 323.56542381642015
At time: 531.8235695362091 and batch: 250, loss is 5.817346410751343 and perplexity is 336.0790535631565
At time: 533.3709745407104 and batch: 300, loss is 5.772986526489258 and perplexity is 321.49645799343574
At time: 534.9173474311829 and batch: 350, loss is 5.74708158493042 and perplexity is 313.27505823127154
At time: 536.4642870426178 and batch: 400, loss is 5.774056339263916 and perplexity is 321.84058305307605
At time: 538.00843334198 and batch: 450, loss is 5.783461046218872 and perplexity is 324.88167730805867
At time: 539.5578935146332 and batch: 500, loss is 5.798016281127929 and perplexity is 329.644987897299
At time: 541.1171138286591 and batch: 550, loss is 5.773273849487305 and perplexity is 321.5888445913695
At time: 542.6700983047485 and batch: 600, loss is 5.788092584609985 and perplexity is 326.389869197362
At time: 544.2216739654541 and batch: 650, loss is 5.802401113510132 and perplexity is 331.09359955350374
At time: 545.8171429634094 and batch: 700, loss is 5.760903091430664 and perplexity is 317.635052923741
At time: 547.365786075592 and batch: 750, loss is 5.761249685287476 and perplexity is 317.7451623623178
At time: 548.9136207103729 and batch: 800, loss is 5.781061725616455 and perplexity is 324.10311638847054
At time: 550.459320306778 and batch: 850, loss is 5.785659446716308 and perplexity is 325.5966829955236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.476704279581706 and perplexity of 239.05754083395368
Finished 19 epochs...
Completing Train Step...
At time: 554.5286953449249 and batch: 50, loss is 5.803547286987305 and perplexity is 331.47330781997687
At time: 556.0952429771423 and batch: 100, loss is 5.740327663421631 and perplexity is 311.1663521109203
At time: 557.6428537368774 and batch: 150, loss is 5.740662574768066 and perplexity is 311.27058270590135
At time: 559.188882112503 and batch: 200, loss is 5.775538587570191 and perplexity is 322.3179844384877
At time: 560.73712849617 and batch: 250, loss is 5.8139479446411135 and perplexity is 334.9388388771643
At time: 562.2839803695679 and batch: 300, loss is 5.768176097869873 and perplexity is 319.9536360234552
At time: 563.8285965919495 and batch: 350, loss is 5.746134166717529 and perplexity is 312.9783962891126
At time: 565.3789584636688 and batch: 400, loss is 5.772499446868896 and perplexity is 321.3399017515013
At time: 566.9358701705933 and batch: 450, loss is 5.785845565795898 and perplexity is 325.6572883902155
At time: 568.4912254810333 and batch: 500, loss is 5.799038619995117 and perplexity is 329.98216910816075
At time: 570.0432569980621 and batch: 550, loss is 5.771545324325562 and perplexity is 321.0334503265219
At time: 571.5917646884918 and batch: 600, loss is 5.786751594543457 and perplexity is 325.95247695982096
At time: 573.1470096111298 and batch: 650, loss is 5.799935216903687 and perplexity is 330.2781627745406
At time: 574.6993329524994 and batch: 700, loss is 5.757017889022827 and perplexity is 316.40337066807353
At time: 576.249596118927 and batch: 750, loss is 5.755815715789795 and perplexity is 316.02322754973846
At time: 577.8064596652985 and batch: 800, loss is 5.776155405044555 and perplexity is 322.5168571313621
At time: 579.3588874340057 and batch: 850, loss is 5.782534112930298 and perplexity is 324.580673193433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.478033701578776 and perplexity of 239.37556053165667
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 583.4646580219269 and batch: 50, loss is 5.803576440811157 and perplexity is 331.4829716752732
At time: 585.0601410865784 and batch: 100, loss is 5.742195501327514 and perplexity is 311.74810355799536
At time: 586.6142680644989 and batch: 150, loss is 5.736790027618408 and perplexity is 310.0675036916844
At time: 588.1762683391571 and batch: 200, loss is 5.761063776016235 and perplexity is 317.6860960813975
At time: 589.746949672699 and batch: 250, loss is 5.794742631912231 and perplexity is 328.56761028157155
At time: 591.3050148487091 and batch: 300, loss is 5.737619657516479 and perplexity is 310.3248517002076
At time: 592.8666114807129 and batch: 350, loss is 5.709412956237793 and perplexity is 301.69390876044815
At time: 594.4164962768555 and batch: 400, loss is 5.73295295715332 and perplexity is 308.8800324933559
At time: 595.9741523265839 and batch: 450, loss is 5.749333429336548 and perplexity is 313.98129979335005
At time: 597.5247092247009 and batch: 500, loss is 5.7543791389465335 and perplexity is 315.56956183995567
At time: 599.0792996883392 and batch: 550, loss is 5.713771200180053 and perplexity is 303.011633808554
At time: 600.6389966011047 and batch: 600, loss is 5.71226354598999 and perplexity is 302.55514125213045
At time: 602.1944608688354 and batch: 650, loss is 5.7178956985473635 and perplexity is 304.2639856837422
At time: 603.7511720657349 and batch: 700, loss is 5.670664119720459 and perplexity is 290.2272160203911
At time: 605.3053798675537 and batch: 750, loss is 5.6571276569366455 and perplexity is 286.32503655020906
At time: 606.8751635551453 and batch: 800, loss is 5.659154462814331 and perplexity is 286.90595031807374
At time: 608.4254076480865 and batch: 850, loss is 5.690068950653076 and perplexity is 295.91402340225204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.410423278808594 and perplexity of 223.72626622884135
Finished 21 epochs...
Completing Train Step...
At time: 612.5408687591553 and batch: 50, loss is 5.759541473388672 and perplexity is 317.2028496195949
At time: 614.125006198883 and batch: 100, loss is 5.703230257034302 and perplexity is 299.8343804466569
At time: 615.6815605163574 and batch: 150, loss is 5.698471508026123 and perplexity is 298.4109334857393
At time: 617.2358210086823 and batch: 200, loss is 5.729195508956909 and perplexity is 307.7216094924886
At time: 618.7873721122742 and batch: 250, loss is 5.764166440963745 and perplexity is 318.67330028601555
At time: 620.3373539447784 and batch: 300, loss is 5.712914237976074 and perplexity is 302.752075522719
At time: 621.8852033615112 and batch: 350, loss is 5.685235195159912 and perplexity is 294.48709884669415
At time: 623.4389040470123 and batch: 400, loss is 5.710472526550293 and perplexity is 302.0137440837166
At time: 625.0234167575836 and batch: 450, loss is 5.729101228713989 and perplexity is 307.6925987919833
At time: 626.5812530517578 and batch: 500, loss is 5.735219850540161 and perplexity is 309.58102483360375
At time: 628.1308434009552 and batch: 550, loss is 5.69931113243103 and perplexity is 298.6615918028798
At time: 629.6774272918701 and batch: 600, loss is 5.702176971435547 and perplexity is 299.51873547305496
At time: 631.2226865291595 and batch: 650, loss is 5.70945068359375 and perplexity is 301.70529108864514
At time: 632.772891998291 and batch: 700, loss is 5.6672155380249025 and perplexity is 289.2280675684177
At time: 634.324081659317 and batch: 750, loss is 5.659608612060547 and perplexity is 287.0362780310218
At time: 635.8789637088776 and batch: 800, loss is 5.669050312042236 and perplexity is 289.759222837813
At time: 637.4332177639008 and batch: 850, loss is 5.698805704116821 and perplexity is 298.5106779193033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.406718571980794 and perplexity of 222.89895941361053
Finished 22 epochs...
Completing Train Step...
At time: 641.546178817749 and batch: 50, loss is 5.750363817214966 and perplexity is 314.3049890528134
At time: 643.0935785770416 and batch: 100, loss is 5.692827138900757 and perplexity is 296.7313366176272
At time: 644.6447806358337 and batch: 150, loss is 5.686224126815796 and perplexity is 294.7784705105778
At time: 646.1965773105621 and batch: 200, loss is 5.718133974075317 and perplexity is 304.3364929835828
At time: 647.7425649166107 and batch: 250, loss is 5.752975130081177 and perplexity is 315.1268102637956
At time: 649.2942934036255 and batch: 300, loss is 5.7023255443573 and perplexity is 299.56323915264295
At time: 650.8388314247131 and batch: 350, loss is 5.67499083518982 and perplexity is 291.485667122269
At time: 652.3869271278381 and batch: 400, loss is 5.701800193786621 and perplexity is 299.40590476547334
At time: 653.9389369487762 and batch: 450, loss is 5.721385946273804 and perplexity is 305.32779777194247
At time: 655.5018348693848 and batch: 500, loss is 5.728409633636475 and perplexity is 307.4798736735725
At time: 657.0506393909454 and batch: 550, loss is 5.694697370529175 and perplexity is 297.2868122206227
At time: 658.5959711074829 and batch: 600, loss is 5.700110578536988 and perplexity is 298.90045111429725
At time: 660.1427657604218 and batch: 650, loss is 5.708676280975342 and perplexity is 301.471740164134
At time: 661.6972923278809 and batch: 700, loss is 5.669158353805542 and perplexity is 289.79053062642635
At time: 663.3066802024841 and batch: 750, loss is 5.664181184768677 and perplexity is 288.3517775988162
At time: 664.8531959056854 and batch: 800, loss is 5.674702100753784 and perplexity is 291.4015173216171
At time: 666.4085645675659 and batch: 850, loss is 5.703184299468994 and perplexity is 299.82060110517085
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.405964533487956 and perplexity of 222.73094836955693
Finished 23 epochs...
Completing Train Step...
At time: 671.1023771762848 and batch: 50, loss is 5.745795698165893 and perplexity is 312.8724808701577
At time: 672.6484792232513 and batch: 100, loss is 5.687214622497558 and perplexity is 295.0705919613298
At time: 674.2001340389252 and batch: 150, loss is 5.679427719116211 and perplexity is 292.7818285260007
At time: 675.7551112174988 and batch: 200, loss is 5.711965808868408 and perplexity is 302.4650727642857
At time: 677.3118493556976 and batch: 250, loss is 5.7469356250762935 and perplexity is 313.22933598635785
At time: 678.8530843257904 and batch: 300, loss is 5.696280870437622 and perplexity is 297.757938776541
At time: 680.3981139659882 and batch: 350, loss is 5.669562072753906 and perplexity is 289.9075481742081
At time: 681.953711271286 and batch: 400, loss is 5.697332191467285 and perplexity is 298.0711425693447
At time: 683.5000903606415 and batch: 450, loss is 5.717491912841797 and perplexity is 304.14115303630683
At time: 685.0536947250366 and batch: 500, loss is 5.725230865478515 and perplexity is 306.50401827267933
At time: 686.6208868026733 and batch: 550, loss is 5.693088855743408 and perplexity is 296.8090063694602
At time: 688.174402475357 and batch: 600, loss is 5.6998633289337155 and perplexity is 298.8265572318344
At time: 689.7257249355316 and batch: 650, loss is 5.709197998046875 and perplexity is 301.62906415329934
At time: 691.2845969200134 and batch: 700, loss is 5.670963439941406 and perplexity is 290.31409989721726
At time: 692.8321635723114 and batch: 750, loss is 5.667186899185181 and perplexity is 289.2197845307564
At time: 694.3793661594391 and batch: 800, loss is 5.677794055938721 and perplexity is 292.3039121172099
At time: 695.9254295825958 and batch: 850, loss is 5.705309848785401 and perplexity is 300.45856234774595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.4056854248046875 and perplexity of 222.66879090258226
Finished 24 epochs...
Completing Train Step...
At time: 700.0253772735596 and batch: 50, loss is 5.742648029327393 and perplexity is 311.88921022871614
At time: 701.5827965736389 and batch: 100, loss is 5.683342456817627 and perplexity is 293.9302389846488
At time: 703.1392278671265 and batch: 150, loss is 5.674894771575928 and perplexity is 291.457667300591
At time: 704.7208304405212 and batch: 200, loss is 5.707796440124512 and perplexity is 301.2066096651922
At time: 706.2715587615967 and batch: 250, loss is 5.7428369140625 and perplexity is 311.94812690362807
At time: 707.8235054016113 and batch: 300, loss is 5.692100133895874 and perplexity is 296.5156898484389
At time: 709.3836691379547 and batch: 350, loss is 5.6659235000610355 and perplexity is 288.8546152341525
At time: 710.9395809173584 and batch: 400, loss is 5.694852876663208 and perplexity is 297.3330457381947
At time: 712.491480588913 and batch: 450, loss is 5.715371837615967 and perplexity is 303.49703394432515
At time: 714.0465760231018 and batch: 500, loss is 5.723545207977295 and perplexity is 305.9877926871244
At time: 715.5901718139648 and batch: 550, loss is 5.6922422218322755 and perplexity is 296.5578241442368
At time: 717.1432764530182 and batch: 600, loss is 5.699959573745727 and perplexity is 298.85531912172837
At time: 718.6840674877167 and batch: 650, loss is 5.709402809143066 and perplexity is 301.69084745930905
At time: 720.2255427837372 and batch: 700, loss is 5.6720124530792235 and perplexity is 290.61880299293625
At time: 721.7673015594482 and batch: 750, loss is 5.66896125793457 and perplexity is 289.7334197377383
At time: 723.309642791748 and batch: 800, loss is 5.679461822509766 and perplexity is 292.7918135501854
At time: 724.8596260547638 and batch: 850, loss is 5.706572370529175 and perplexity is 300.8381373762575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.4056345621744795 and perplexity of 222.6574656702297
Finished 25 epochs...
Completing Train Step...
At time: 728.9249539375305 and batch: 50, loss is 5.740469923019409 and perplexity is 311.2106216598232
At time: 730.5064356327057 and batch: 100, loss is 5.680708780288696 and perplexity is 293.15714030588146
At time: 732.0465157032013 and batch: 150, loss is 5.671775131225586 and perplexity is 290.5498409833277
At time: 733.5856244564056 and batch: 200, loss is 5.705026550292969 and perplexity is 300.37345494596303
At time: 735.124041557312 and batch: 250, loss is 5.740358657836914 and perplexity is 311.1759966795229
At time: 736.6640884876251 and batch: 300, loss is 5.689537496566772 and perplexity is 295.7568004674063
At time: 738.2066793441772 and batch: 350, loss is 5.663678913116455 and perplexity is 288.20698304120555
At time: 739.7494990825653 and batch: 400, loss is 5.693177614212036 and perplexity is 296.8353518515156
At time: 741.2885587215424 and batch: 450, loss is 5.714056243896485 and perplexity is 303.0980176817819
At time: 742.8563919067383 and batch: 500, loss is 5.722362928390503 and perplexity is 305.62624333434655
At time: 744.3966131210327 and batch: 550, loss is 5.691589450836181 and perplexity is 296.3643029673481
At time: 745.9441738128662 and batch: 600, loss is 5.700020208358764 and perplexity is 298.87344064774777
At time: 747.4904096126556 and batch: 650, loss is 5.709981079101563 and perplexity is 301.8653566650007
At time: 749.0316579341888 and batch: 700, loss is 5.672858734130859 and perplexity is 290.86485227789706
At time: 750.5744957923889 and batch: 750, loss is 5.670187788009644 and perplexity is 290.0890045139339
At time: 752.1183581352234 and batch: 800, loss is 5.680691652297973 and perplexity is 293.15211915610325
At time: 753.6602110862732 and batch: 850, loss is 5.7074143600463865 and perplexity is 301.091546603291
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.40542475382487 and perplexity of 222.6107551751265
Finished 26 epochs...
Completing Train Step...
At time: 757.6784179210663 and batch: 50, loss is 5.738727588653564 and perplexity is 310.6688608000245
At time: 759.2502024173737 and batch: 100, loss is 5.678776531219483 and perplexity is 292.59123460584476
At time: 760.8012311458588 and batch: 150, loss is 5.6695741844177245 and perplexity is 289.91105945823375
At time: 762.3473651409149 and batch: 200, loss is 5.702978963851929 and perplexity is 299.7590435772269
At time: 763.8999016284943 and batch: 250, loss is 5.738467741012573 and perplexity is 310.58814471681234
At time: 765.4485790729523 and batch: 300, loss is 5.687601776123047 and perplexity is 295.18485172739673
At time: 766.996529340744 and batch: 350, loss is 5.6620521736145015 and perplexity is 287.73852648884844
At time: 768.5497095584869 and batch: 400, loss is 5.691909646987915 and perplexity is 296.45921287075146
At time: 770.0945312976837 and batch: 450, loss is 5.713145294189453 and perplexity is 302.822036353013
At time: 771.6480762958527 and batch: 500, loss is 5.721614580154419 and perplexity is 305.39761403208394
At time: 773.2152283191681 and batch: 550, loss is 5.69134036064148 and perplexity is 296.2904907187547
At time: 774.7689874172211 and batch: 600, loss is 5.700107555389405 and perplexity is 298.89954749548684
At time: 776.3236248493195 and batch: 650, loss is 5.710173892974853 and perplexity is 301.9235661052451
At time: 777.874981880188 and batch: 700, loss is 5.673365201950073 and perplexity is 291.012203276436
At time: 779.4230234622955 and batch: 750, loss is 5.671101093292236 and perplexity is 290.35406535648787
At time: 780.9797556400299 and batch: 800, loss is 5.681434631347656 and perplexity is 293.37000597165286
At time: 782.5833039283752 and batch: 850, loss is 5.707768898010254 and perplexity is 301.19831391257543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.405251185099284 and perplexity of 222.57212026305226
Finished 27 epochs...
Completing Train Step...
At time: 786.7766180038452 and batch: 50, loss is 5.737271003723144 and perplexity is 310.2166746227663
At time: 788.3540241718292 and batch: 100, loss is 5.677254114151001 and perplexity is 292.14612762139865
At time: 789.9018559455872 and batch: 150, loss is 5.667894430160523 and perplexity is 289.4244888957936
At time: 791.4508798122406 and batch: 200, loss is 5.70128059387207 and perplexity is 299.25037389335444
At time: 793.0036520957947 and batch: 250, loss is 5.736886968612671 and perplexity is 310.09756340076643
At time: 794.5555777549744 and batch: 300, loss is 5.686095304489136 and perplexity is 294.74049890800484
At time: 796.1102306842804 and batch: 350, loss is 5.6607341480255124 and perplexity is 287.3595295666848
At time: 797.6739234924316 and batch: 400, loss is 5.690845041275025 and perplexity is 296.1437686407484
At time: 799.2307221889496 and batch: 450, loss is 5.712414255142212 and perplexity is 302.60074251714667
At time: 800.7890775203705 and batch: 500, loss is 5.721017732620239 and perplexity is 305.2153926038187
At time: 802.339695930481 and batch: 550, loss is 5.691062717437744 and perplexity is 296.2082390965056
At time: 803.889347076416 and batch: 600, loss is 5.700156593322754 and perplexity is 298.9142052709657
At time: 805.4410254955292 and batch: 650, loss is 5.710299434661866 and perplexity is 301.9614724784479
At time: 806.9975123405457 and batch: 700, loss is 5.673696737289429 and perplexity is 291.1087001011759
At time: 808.547420501709 and batch: 750, loss is 5.67164475440979 and perplexity is 290.5119624895227
At time: 810.0958964824677 and batch: 800, loss is 5.68183388710022 and perplexity is 293.48715901962464
At time: 811.643105506897 and batch: 850, loss is 5.707995376586914 and perplexity is 301.2665366032
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.405157089233398 and perplexity of 222.55117813197353
Finished 28 epochs...
Completing Train Step...
At time: 815.732504606247 and batch: 50, loss is 5.736095066070557 and perplexity is 309.8520935589298
At time: 817.2887766361237 and batch: 100, loss is 5.675987844467163 and perplexity is 291.7764259571452
At time: 818.8377895355225 and batch: 150, loss is 5.666479740142822 and perplexity is 289.0153324434878
At time: 820.3882291316986 and batch: 200, loss is 5.699924516677856 and perplexity is 298.8448423141665
At time: 821.9670240879059 and batch: 250, loss is 5.735585355758667 and perplexity is 309.69419899534444
At time: 823.5163924694061 and batch: 300, loss is 5.68481855392456 and perplexity is 294.36442893448196
At time: 825.0666797161102 and batch: 350, loss is 5.659603824615479 and perplexity is 287.0349038638976
At time: 826.6159133911133 and batch: 400, loss is 5.689983749389649 and perplexity is 295.88881222761944
At time: 828.1726634502411 and batch: 450, loss is 5.711742534637451 and perplexity is 302.39754764636274
At time: 829.7210507392883 and batch: 500, loss is 5.720449180603027 and perplexity is 305.04191109797296
At time: 831.2723519802094 and batch: 550, loss is 5.6907815170288085 and perplexity is 296.1249569285801
At time: 832.8343663215637 and batch: 600, loss is 5.700127944946289 and perplexity is 298.90564198694506
At time: 834.3794009685516 and batch: 650, loss is 5.710329637527466 and perplexity is 301.9705927179457
At time: 835.9291462898254 and batch: 700, loss is 5.673839864730835 and perplexity is 291.15036872648295
At time: 837.4737741947174 and batch: 750, loss is 5.671935300827027 and perplexity is 290.59638196268224
At time: 839.0174608230591 and batch: 800, loss is 5.681950979232788 and perplexity is 293.52152606897175
At time: 840.5747861862183 and batch: 850, loss is 5.708073902130127 and perplexity is 301.2901946505069
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.40516726175944 and perplexity of 222.5534420511436
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 844.6822257041931 and batch: 50, loss is 5.737208976745605 and perplexity is 310.19743341680027
At time: 846.2322998046875 and batch: 100, loss is 5.678378000259399 and perplexity is 292.47465117280206
At time: 847.7793207168579 and batch: 150, loss is 5.667959947586059 and perplexity is 289.4434518643889
At time: 849.3344280719757 and batch: 200, loss is 5.701766433715821 and perplexity is 299.3957969715513
At time: 850.8772912025452 and batch: 250, loss is 5.733632545471192 and perplexity is 309.0900150978431
At time: 852.4208698272705 and batch: 300, loss is 5.679999074935913 and perplexity is 292.94915892567894
At time: 853.9639060497284 and batch: 350, loss is 5.655499858856201 and perplexity is 285.8593363411142
At time: 855.5069932937622 and batch: 400, loss is 5.683022499084473 and perplexity is 293.8362087753262
At time: 857.0487525463104 and batch: 450, loss is 5.703935422897339 and perplexity is 300.0458879814949
At time: 858.595950126648 and batch: 500, loss is 5.712159118652344 and perplexity is 302.52354787387316
At time: 860.1670701503754 and batch: 550, loss is 5.6811536598205565 and perplexity is 293.2875889320327
At time: 861.7094991207123 and batch: 600, loss is 5.68455415725708 and perplexity is 294.2866102484008
At time: 863.2520444393158 and batch: 650, loss is 5.693212947845459 and perplexity is 296.8458403083214
At time: 864.7960088253021 and batch: 700, loss is 5.656444206237793 and perplexity is 286.12941436055553
At time: 866.3472430706024 and batch: 750, loss is 5.650884981155396 and perplexity is 284.5431697740134
At time: 867.8977420330048 and batch: 800, loss is 5.660238962173462 and perplexity is 287.21726841895236
At time: 869.4481589794159 and batch: 850, loss is 5.692797250747681 and perplexity is 296.7224679985499
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.401742299397786 and perplexity of 221.79250871651274
Finished 30 epochs...
Completing Train Step...
At time: 873.5174770355225 and batch: 50, loss is 5.731905317306518 and perplexity is 308.55660690978004
At time: 875.0618188381195 and batch: 100, loss is 5.673102912902832 and perplexity is 290.93588397219935
At time: 876.6045370101929 and batch: 150, loss is 5.662906656265259 and perplexity is 287.9844991424039
At time: 878.1460499763489 and batch: 200, loss is 5.697662229537964 and perplexity is 298.1695336296682
At time: 879.6874687671661 and batch: 250, loss is 5.730290355682373 and perplexity is 308.0587019876308
At time: 881.2296626567841 and batch: 300, loss is 5.677273607254028 and perplexity is 292.1518225114686
At time: 882.7748188972473 and batch: 350, loss is 5.652937688827515 and perplexity is 285.127853608852
At time: 884.332117319107 and batch: 400, loss is 5.680618591308594 and perplexity is 293.13070195462956
At time: 885.8805389404297 and batch: 450, loss is 5.702406997680664 and perplexity is 299.58764056780444
At time: 887.427122592926 and batch: 500, loss is 5.710565528869629 and perplexity is 302.04183336855164
At time: 888.966894865036 and batch: 550, loss is 5.679854383468628 and perplexity is 292.90677474842244
At time: 890.513210773468 and batch: 600, loss is 5.684175691604614 and perplexity is 294.17525394803795
At time: 892.0770065784454 and batch: 650, loss is 5.692920551300049 and perplexity is 296.7590562983866
At time: 893.6411874294281 and batch: 700, loss is 5.656797714233399 and perplexity is 286.2305812769134
At time: 895.20436668396 and batch: 750, loss is 5.652228832244873 and perplexity is 284.9258104711911
At time: 896.7576117515564 and batch: 800, loss is 5.662042417526245 and perplexity is 287.7357193000829
At time: 898.307678937912 and batch: 850, loss is 5.694648895263672 and perplexity is 297.2724015127541
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.401480356852214 and perplexity of 221.7344194305505
Finished 31 epochs...
Completing Train Step...
At time: 902.6201574802399 and batch: 50, loss is 5.73017993927002 and perplexity is 308.02468912878646
At time: 904.2044429779053 and batch: 100, loss is 5.671162300109863 and perplexity is 290.3718375486974
At time: 905.7654056549072 and batch: 150, loss is 5.66098069190979 and perplexity is 287.4303850354207
At time: 907.3182475566864 and batch: 200, loss is 5.696068906784058 and perplexity is 297.69483160440984
At time: 908.8794720172882 and batch: 250, loss is 5.728725709915161 and perplexity is 307.57707612870007
At time: 910.4439721107483 and batch: 300, loss is 5.675947408676148 and perplexity is 291.76462798509385
At time: 912.0086445808411 and batch: 350, loss is 5.65176872253418 and perplexity is 284.7947434938729
At time: 913.5746743679047 and batch: 400, loss is 5.679500226974487 and perplexity is 292.80305827898206
At time: 915.1419372558594 and batch: 450, loss is 5.701605005264282 and perplexity is 299.3474698724384
At time: 916.709465265274 and batch: 500, loss is 5.70991732597351 and perplexity is 301.8461124177097
At time: 918.2744445800781 and batch: 550, loss is 5.679517116546631 and perplexity is 292.8080036391211
At time: 919.8427133560181 and batch: 600, loss is 5.684352798461914 and perplexity is 294.2273590167063
At time: 921.4103186130524 and batch: 650, loss is 5.693346204757691 and perplexity is 296.8853997041327
At time: 922.9741990566254 and batch: 700, loss is 5.6575846862792964 and perplexity is 286.4559254011926
At time: 924.5362937450409 and batch: 750, loss is 5.6534950637817385 and perplexity is 285.28682103131956
At time: 926.1052436828613 and batch: 800, loss is 5.663314723968506 and perplexity is 288.1020402962831
At time: 927.6705884933472 and batch: 850, loss is 5.6957285118103025 and perplexity is 297.5935150248242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.4014542897542315 and perplexity of 221.72863953304605
Finished 32 epochs...
Completing Train Step...
At time: 932.7051191329956 and batch: 50, loss is 5.728944540023804 and perplexity is 307.6443906186352
At time: 934.2974514961243 and batch: 100, loss is 5.669819297790528 and perplexity is 289.9821292455523
At time: 935.8601567745209 and batch: 150, loss is 5.659676389694214 and perplexity is 287.05573333003304
At time: 937.4237005710602 and batch: 200, loss is 5.6950159740448 and perplexity is 297.3815439342724
At time: 938.9785854816437 and batch: 250, loss is 5.727646102905274 and perplexity is 307.2451929451721
At time: 940.5628271102905 and batch: 300, loss is 5.675047073364258 and perplexity is 291.502060205017
At time: 942.1182024478912 and batch: 350, loss is 5.651028776168824 and perplexity is 284.5840886048233
At time: 943.673731803894 and batch: 400, loss is 5.678795738220215 and perplexity is 292.59685445987213
At time: 945.2227852344513 and batch: 450, loss is 5.701087350845337 and perplexity is 299.19255143242583
At time: 946.7914426326752 and batch: 500, loss is 5.709576482772827 and perplexity is 301.74324775399595
At time: 948.3397998809814 and batch: 550, loss is 5.679427442550659 and perplexity is 292.78174755264394
At time: 949.8885130882263 and batch: 600, loss is 5.684603948593139 and perplexity is 294.3012635367099
At time: 951.4440760612488 and batch: 650, loss is 5.693836221694946 and perplexity is 297.0309142277904
At time: 952.9984951019287 and batch: 700, loss is 5.658325414657593 and perplexity is 286.66819003978384
At time: 954.5551767349243 and batch: 750, loss is 5.654518547058106 and perplexity is 285.5789567941455
At time: 956.1015014648438 and batch: 800, loss is 5.664276142120361 and perplexity is 288.3791600200309
At time: 957.6531565189362 and batch: 850, loss is 5.696484928131103 and perplexity is 297.8187047744173
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.401465733846028 and perplexity of 221.73117703047052
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 965.2686166763306 and batch: 50, loss is 5.728439922332764 and perplexity is 307.4891869791244
At time: 966.8056962490082 and batch: 100, loss is 5.669323902130127 and perplexity is 289.83850893450705
At time: 968.3443384170532 and batch: 150, loss is 5.660002927780152 and perplexity is 287.1494832653813
At time: 969.881974697113 and batch: 200, loss is 5.695232982635498 and perplexity is 297.44608528678185
At time: 971.4222123622894 and batch: 250, loss is 5.726559705734253 and perplexity is 306.9115838855282
At time: 972.9630372524261 and batch: 300, loss is 5.6740092849731445 and perplexity is 291.1996996712135
At time: 974.5040671825409 and batch: 350, loss is 5.650666637420654 and perplexity is 284.4810483377898
At time: 976.0456602573395 and batch: 400, loss is 5.676730089187622 and perplexity is 291.99307586254395
At time: 977.5874767303467 and batch: 450, loss is 5.699618215560913 and perplexity is 298.7533198226093
At time: 979.1413757801056 and batch: 500, loss is 5.708638772964478 and perplexity is 301.46043277088893
At time: 980.6988224983215 and batch: 550, loss is 5.678391704559326 and perplexity is 292.47865936060737
At time: 982.250039100647 and batch: 600, loss is 5.682357187271118 and perplexity is 293.64078109181867
At time: 983.853954076767 and batch: 650, loss is 5.6916474437713624 and perplexity is 296.3814905015331
At time: 985.4008378982544 and batch: 700, loss is 5.655626373291016 and perplexity is 285.8955039613026
At time: 986.9544444084167 and batch: 750, loss is 5.6521499061584475 and perplexity is 284.9033232794736
At time: 988.5001535415649 and batch: 800, loss is 5.661576490402222 and perplexity is 287.6016866510588
At time: 990.0492401123047 and batch: 850, loss is 5.694159822463989 and perplexity is 297.1270492139029
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.400327682495117 and perplexity of 221.47897909922978
Finished 34 epochs...
Completing Train Step...
At time: 994.6649580001831 and batch: 50, loss is 5.728241357803345 and perplexity is 307.42813659483147
At time: 996.233564376831 and batch: 100, loss is 5.668531665802002 and perplexity is 289.6089792713133
At time: 997.7836039066315 and batch: 150, loss is 5.659346933364868 and perplexity is 286.96117657882786
At time: 999.3342833518982 and batch: 200, loss is 5.694466171264648 and perplexity is 297.2180876731223
At time: 1000.8891890048981 and batch: 250, loss is 5.726103963851929 and perplexity is 306.77174329060955
At time: 1002.4493947029114 and batch: 300, loss is 5.673691501617432 and perplexity is 291.1071759554966
At time: 1004.0167241096497 and batch: 350, loss is 5.650261154174805 and perplexity is 284.3657194224788
At time: 1005.5821142196655 and batch: 400, loss is 5.6766763687133786 and perplexity is 291.97739027735514
At time: 1007.1421613693237 and batch: 450, loss is 5.699562311172485 and perplexity is 298.7366186678121
At time: 1008.6944551467896 and batch: 500, loss is 5.708550672531128 and perplexity is 301.4338751460104
At time: 1010.2443687915802 and batch: 550, loss is 5.6783073616027835 and perplexity is 292.45399188602977
At time: 1011.8014862537384 and batch: 600, loss is 5.682383241653443 and perplexity is 293.6484318206624
At time: 1013.356600522995 and batch: 650, loss is 5.691550464630127 and perplexity is 296.3527490727882
At time: 1014.9156303405762 and batch: 700, loss is 5.655645923614502 and perplexity is 285.9010933655255
At time: 1016.4912557601929 and batch: 750, loss is 5.6523243427276615 and perplexity is 284.953025172532
At time: 1018.0524156093597 and batch: 800, loss is 5.661782188415527 and perplexity is 287.66085183149755
At time: 1019.6063694953918 and batch: 850, loss is 5.694408006668091 and perplexity is 297.20080060571587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.400220235188802 and perplexity of 221.45518305795326
Finished 35 epochs...
Completing Train Step...
At time: 1024.5169246196747 and batch: 50, loss is 5.7280624485015865 and perplexity is 307.3731397614417
At time: 1026.0989985466003 and batch: 100, loss is 5.66830846786499 and perplexity is 289.5443463578346
At time: 1027.652872800827 and batch: 150, loss is 5.659072771072387 and perplexity is 286.8825134285315
At time: 1029.2059693336487 and batch: 200, loss is 5.694160394668579 and perplexity is 297.1272192314128
At time: 1030.7516605854034 and batch: 250, loss is 5.7258445835113525 and perplexity is 306.69218304998276
At time: 1032.301415681839 and batch: 300, loss is 5.673501968383789 and perplexity is 291.05200669947004
At time: 1033.8504748344421 and batch: 350, loss is 5.6500527286529545 and perplexity is 284.3064565251765
At time: 1035.3988137245178 and batch: 400, loss is 5.6766256332397464 and perplexity is 291.962577041951
At time: 1036.9523556232452 and batch: 450, loss is 5.699580602645874 and perplexity is 298.7420830506982
At time: 1038.5096197128296 and batch: 500, loss is 5.7086021423339846 and perplexity is 301.4493902874155
At time: 1040.0713574886322 and batch: 550, loss is 5.678379888534546 and perplexity is 292.47520344593835
At time: 1041.6220362186432 and batch: 600, loss is 5.682456922531128 and perplexity is 293.67006889195926
At time: 1043.173738002777 and batch: 650, loss is 5.69159366607666 and perplexity is 296.3655522167873
At time: 1044.726373910904 and batch: 700, loss is 5.655710830688476 and perplexity is 285.9196509711953
At time: 1046.288845539093 and batch: 750, loss is 5.65251088142395 and perplexity is 285.00618489636986
At time: 1047.8415234088898 and batch: 800, loss is 5.661975326538086 and perplexity is 287.71641547391005
At time: 1049.4033434391022 and batch: 850, loss is 5.694539031982422 and perplexity is 297.23974398526315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.400187810262044 and perplexity of 221.44800250627753
Finished 36 epochs...
Completing Train Step...
At time: 1053.488374710083 and batch: 50, loss is 5.727883415222168 and perplexity is 307.31811466603364
At time: 1055.0631175041199 and batch: 100, loss is 5.6681350898742675 and perplexity is 289.4941500924276
At time: 1056.6166882514954 and batch: 150, loss is 5.65884467124939 and perplexity is 286.81708304061095
At time: 1058.1636588573456 and batch: 200, loss is 5.693921356201172 and perplexity is 297.05620288446045
At time: 1059.7107586860657 and batch: 250, loss is 5.72561882019043 and perplexity is 306.6229510195571
At time: 1061.273116827011 and batch: 300, loss is 5.673336544036865 and perplexity is 291.00386359347164
At time: 1062.8306810855865 and batch: 350, loss is 5.649883527755737 and perplexity is 284.2583556871149
At time: 1064.3990471363068 and batch: 400, loss is 5.676580095291138 and perplexity is 291.9492819678396
At time: 1065.98934841156 and batch: 450, loss is 5.699602317810059 and perplexity is 298.7485703545168
At time: 1067.5373558998108 and batch: 500, loss is 5.708663892745972 and perplexity is 301.46800548620126
At time: 1069.0819730758667 and batch: 550, loss is 5.6784598255157475 and perplexity is 292.49858396524974
At time: 1070.628161430359 and batch: 600, loss is 5.682534980773926 and perplexity is 293.69299315620145
At time: 1072.178321838379 and batch: 650, loss is 5.6916538524627684 and perplexity is 296.3833899251306
At time: 1073.7282524108887 and batch: 700, loss is 5.655780925750732 and perplexity is 285.9396932293538
At time: 1075.2736599445343 and batch: 750, loss is 5.652674512863159 and perplexity is 285.05282468435166
At time: 1076.825973033905 and batch: 800, loss is 5.66214337348938 and perplexity is 287.7647694031191
At time: 1078.3869364261627 and batch: 850, loss is 5.694644241333008 and perplexity is 297.27101803082843
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.400175094604492 and perplexity of 221.44518666721478
Finished 37 epochs...
Completing Train Step...
At time: 1083.646904706955 and batch: 50, loss is 5.727722902297973 and perplexity is 307.268790095512
At time: 1085.1976480484009 and batch: 100, loss is 5.667981777191162 and perplexity is 289.4497703696078
At time: 1086.7496225833893 and batch: 150, loss is 5.658641185760498 and perplexity is 286.75872586386663
At time: 1088.2953624725342 and batch: 200, loss is 5.6937112045288085 and perplexity is 296.9937825857331
At time: 1089.845567703247 and batch: 250, loss is 5.725410251617432 and perplexity is 306.55900577693023
At time: 1091.3930449485779 and batch: 300, loss is 5.673183336257934 and perplexity is 290.9592829530078
At time: 1092.938488960266 and batch: 350, loss is 5.649736595153809 and perplexity is 284.2165919355974
At time: 1094.4903070926666 and batch: 400, loss is 5.676537799835205 and perplexity is 291.9369341009806
At time: 1096.046259880066 and batch: 450, loss is 5.699625539779663 and perplexity is 298.75550796528904
At time: 1097.6003994941711 and batch: 500, loss is 5.708724756240844 and perplexity is 301.4863544409925
At time: 1099.1505501270294 and batch: 550, loss is 5.6785363101959225 and perplexity is 292.52095648146246
At time: 1100.6981751918793 and batch: 600, loss is 5.68260947227478 and perplexity is 293.7148716029212
At time: 1102.2462730407715 and batch: 650, loss is 5.691716318130493 and perplexity is 296.4019042897348
At time: 1103.7939057350159 and batch: 700, loss is 5.655851488113403 and perplexity is 285.9598705215598
At time: 1105.4022357463837 and batch: 750, loss is 5.652823209762573 and perplexity is 285.0952143070761
At time: 1106.962191104889 and batch: 800, loss is 5.662294616699219 and perplexity is 287.8082951619269
At time: 1108.5162143707275 and batch: 850, loss is 5.694738826751709 and perplexity is 297.2991368643314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.400171279907227 and perplexity of 221.44434192247795
Finished 38 epochs...
Completing Train Step...
At time: 1112.629721403122 and batch: 50, loss is 5.727577085494995 and perplexity is 307.22398840938405
At time: 1114.1794962882996 and batch: 100, loss is 5.667840147018433 and perplexity is 289.40877845154864
At time: 1115.7503736019135 and batch: 150, loss is 5.6584538555145265 and perplexity is 286.7050123124596
At time: 1117.3001482486725 and batch: 200, loss is 5.69351824760437 and perplexity is 296.9364811074043
At time: 1118.8628618717194 and batch: 250, loss is 5.7252130126953125 and perplexity is 306.4985463717348
At time: 1120.4188032150269 and batch: 300, loss is 5.673038215637207 and perplexity is 290.91706182491146
At time: 1121.9757254123688 and batch: 350, loss is 5.649604339599609 and perplexity is 284.1790051983004
At time: 1123.5242931842804 and batch: 400, loss is 5.6764977836608885 and perplexity is 291.92525213547157
At time: 1125.0770778656006 and batch: 450, loss is 5.699650039672852 and perplexity is 298.7628275329877
At time: 1126.6294164657593 and batch: 500, loss is 5.708784036636352 and perplexity is 301.50422720107093
At time: 1128.1787147521973 and batch: 550, loss is 5.678608283996582 and perplexity is 292.54201108415384
At time: 1129.7352664470673 and batch: 600, loss is 5.682680130004883 and perplexity is 293.7356255622512
At time: 1131.2919578552246 and batch: 650, loss is 5.691777887344361 and perplexity is 296.4201540837778
At time: 1132.8478982448578 and batch: 700, loss is 5.6559215259552005 and perplexity is 285.97989923510755
At time: 1134.4000225067139 and batch: 750, loss is 5.6529615688323975 and perplexity is 285.13466254468676
At time: 1135.949315071106 and batch: 800, loss is 5.662434015274048 and perplexity is 287.8484180245664
At time: 1137.4985709190369 and batch: 850, loss is 5.6948264217376705 and perplexity is 297.32517991865524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.400169372558594 and perplexity of 221.44391955131795
Finished 39 epochs...
Completing Train Step...
At time: 1142.4572575092316 and batch: 50, loss is 5.727442245483399 and perplexity is 307.1825651160457
At time: 1144.0421512126923 and batch: 100, loss is 5.667707004547119 and perplexity is 289.37024841661463
At time: 1145.5995757579803 and batch: 150, loss is 5.658278951644897 and perplexity is 286.654870881457
At time: 1147.1734037399292 and batch: 200, loss is 5.693337745666504 and perplexity is 296.8828883340866
At time: 1148.722204208374 and batch: 250, loss is 5.7250245094299315 and perplexity is 306.44077584004714
At time: 1150.2721428871155 and batch: 300, loss is 5.672898960113526 and perplexity is 290.87655283773483
At time: 1151.8255145549774 and batch: 350, loss is 5.64948260307312 and perplexity is 284.1444123389565
At time: 1153.3755314350128 and batch: 400, loss is 5.676459636688232 and perplexity is 291.91411628326154
At time: 1154.9281492233276 and batch: 450, loss is 5.69967547416687 and perplexity is 298.77042651097526
At time: 1156.4836509227753 and batch: 500, loss is 5.708841886520386 and perplexity is 301.52166969016815
At time: 1158.0363817214966 and batch: 550, loss is 5.678676643371582 and perplexity is 292.56200975673335
At time: 1159.5891933441162 and batch: 600, loss is 5.682746715545655 and perplexity is 293.75518475889584
At time: 1161.1469440460205 and batch: 650, loss is 5.691837539672852 and perplexity is 296.4378367635818
At time: 1162.6923115253448 and batch: 700, loss is 5.655990066528321 and perplexity is 285.999501133057
At time: 1164.240805387497 and batch: 750, loss is 5.653092279434204 and perplexity is 285.1719351039301
At time: 1165.7882928848267 and batch: 800, loss is 5.662564678192139 and perplexity is 287.8860315961292
At time: 1167.3355784416199 and batch: 850, loss is 5.694909563064575 and perplexity is 297.3499009562916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.400170644124349 and perplexity of 221.4442011320017
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 1171.462123632431 and batch: 50, loss is 5.727438621520996 and perplexity is 307.1814518999959
At time: 1173.016820192337 and batch: 100, loss is 5.6677048301696775 and perplexity is 289.3696192171583
At time: 1174.562350988388 and batch: 150, loss is 5.658544111251831 and perplexity is 286.7308902525334
At time: 1176.1085081100464 and batch: 200, loss is 5.693122510910034 and perplexity is 296.81899569412093
At time: 1177.658587694168 and batch: 250, loss is 5.724906311035157 and perplexity is 306.40455717278564
At time: 1179.2076482772827 and batch: 300, loss is 5.6727429962158205 and perplexity is 290.8311901343672
At time: 1180.760406255722 and batch: 350, loss is 5.649509925842285 and perplexity is 284.1521760572069
At time: 1182.3122627735138 and batch: 400, loss is 5.676289863586426 and perplexity is 291.86456132494766
At time: 1183.8700551986694 and batch: 450, loss is 5.699570951461792 and perplexity is 298.73919984977476
At time: 1185.417896747589 and batch: 500, loss is 5.708931465148925 and perplexity is 301.54868079760496
At time: 1186.9922995567322 and batch: 550, loss is 5.678678255081177 and perplexity is 292.5624812821116
At time: 1188.5442552566528 and batch: 600, loss is 5.682495594024658 and perplexity is 293.68142577172074
At time: 1190.0920815467834 and batch: 650, loss is 5.691340646743774 and perplexity is 296.29057548815615
At time: 1191.6411335468292 and batch: 700, loss is 5.655539083480835 and perplexity is 285.87054928619057
At time: 1193.1954391002655 and batch: 750, loss is 5.652748155593872 and perplexity is 285.0738175257344
At time: 1194.7480552196503 and batch: 800, loss is 5.662101984024048 and perplexity is 287.7528592196517
At time: 1196.3011260032654 and batch: 850, loss is 5.694514036178589 and perplexity is 297.2323143317866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.40006955464681 and perplexity of 221.42181658484535
Finished 41 epochs...
Completing Train Step...
At time: 1200.3514659404755 and batch: 50, loss is 5.727340135574341 and perplexity is 307.1512003336123
At time: 1201.894935131073 and batch: 100, loss is 5.667560625076294 and perplexity is 289.3278936527884
At time: 1203.4383294582367 and batch: 150, loss is 5.658341932296753 and perplexity is 286.67292516061184
At time: 1204.9806880950928 and batch: 200, loss is 5.693083343505859 and perplexity is 296.8073702922196
At time: 1206.5271170139313 and batch: 250, loss is 5.724829235076904 and perplexity is 306.3809416580346
At time: 1208.0690400600433 and batch: 300, loss is 5.672699708938598 and perplexity is 290.8186011164891
At time: 1209.6121413707733 and batch: 350, loss is 5.649489765167236 and perplexity is 284.1464474152679
At time: 1211.1548020839691 and batch: 400, loss is 5.676218547821045 and perplexity is 291.84374752255405
At time: 1212.6980154514313 and batch: 450, loss is 5.699517250061035 and perplexity is 298.7231575670324
At time: 1214.2419092655182 and batch: 500, loss is 5.70883279800415 and perplexity is 301.5189293180308
At time: 1215.7865929603577 and batch: 550, loss is 5.678615636825562 and perplexity is 292.5441621034387
At time: 1217.3499341011047 and batch: 600, loss is 5.682467746734619 and perplexity is 293.67324765374786
At time: 1218.9114797115326 and batch: 650, loss is 5.691352672576905 and perplexity is 296.29413865059996
At time: 1220.4628868103027 and batch: 700, loss is 5.655547065734863 and perplexity is 285.8728311866415
At time: 1222.0053493976593 and batch: 750, loss is 5.652776346206665 and perplexity is 285.08185404461847
At time: 1223.5466227531433 and batch: 800, loss is 5.662212381362915 and perplexity is 287.7846281231299
At time: 1225.092123746872 and batch: 850, loss is 5.69456901550293 and perplexity is 297.2486564128351
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.399995803833008 and perplexity of 221.4054871478408
Finished 42 epochs...
Completing Train Step...
At time: 1229.1753015518188 and batch: 50, loss is 5.727286491394043 and perplexity is 307.1347239011792
At time: 1230.7502510547638 and batch: 100, loss is 5.667490768432617 and perplexity is 289.3076828831522
At time: 1232.2936980724335 and batch: 150, loss is 5.658243532180786 and perplexity is 286.6447178993532
At time: 1233.8387310504913 and batch: 200, loss is 5.693050508499145 and perplexity is 296.79762478022127
At time: 1235.3835422992706 and batch: 250, loss is 5.7247907829284665 and perplexity is 306.369160879087
At time: 1236.9301137924194 and batch: 300, loss is 5.672679710388183 and perplexity is 290.812785224188
At time: 1238.4837851524353 and batch: 350, loss is 5.649480428695679 and perplexity is 284.14379450242785
At time: 1240.0293703079224 and batch: 400, loss is 5.676188497543335 and perplexity is 291.8349776686621
At time: 1241.5707931518555 and batch: 450, loss is 5.699507398605347 and perplexity is 298.7202147235781
At time: 1243.11346077919 and batch: 500, loss is 5.708815135955811 and perplexity is 301.51360392315473
At time: 1244.6610760688782 and batch: 550, loss is 5.678602714538574 and perplexity is 292.54038178824453
At time: 1246.215443611145 and batch: 600, loss is 5.682455987930298 and perplexity is 293.6697944277972
At time: 1247.7635686397552 and batch: 650, loss is 5.691371173858642 and perplexity is 296.2996205226471
At time: 1249.3295631408691 and batch: 700, loss is 5.655559167861939 and perplexity is 285.87629087690675
At time: 1250.8883574008942 and batch: 750, loss is 5.652815971374512 and perplexity is 285.09315068474916
At time: 1252.4411113262177 and batch: 800, loss is 5.662280435562134 and perplexity is 287.80421374197863
At time: 1253.9912950992584 and batch: 850, loss is 5.69460241317749 and perplexity is 297.25858399250353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.399946848551433 and perplexity of 221.3946484451832
Finished 43 epochs...
Completing Train Step...
At time: 1258.072101354599 and batch: 50, loss is 5.727245044708252 and perplexity is 307.1219944485808
At time: 1259.6481142044067 and batch: 100, loss is 5.667444219589234 and perplexity is 289.2942162585624
At time: 1261.2136270999908 and batch: 150, loss is 5.658181753158569 and perplexity is 286.6270098159574
At time: 1262.7740619182587 and batch: 200, loss is 5.693018455505371 and perplexity is 296.7881116802643
At time: 1264.3265509605408 and batch: 250, loss is 5.7247621536254885 and perplexity is 306.3603898691116
At time: 1265.9049978256226 and batch: 300, loss is 5.672663812637329 and perplexity is 290.80816199173285
At time: 1267.4658410549164 and batch: 350, loss is 5.64946943283081 and perplexity is 284.14067011283805
At time: 1269.0133092403412 and batch: 400, loss is 5.676171436309814 and perplexity is 291.8299986464327
At time: 1270.5594515800476 and batch: 450, loss is 5.699510202407837 and perplexity is 298.7210522772343
At time: 1272.1114382743835 and batch: 500, loss is 5.708820762634278 and perplexity is 301.5153004480304
At time: 1273.6661562919617 and batch: 550, loss is 5.678606224060059 and perplexity is 292.5414084668012
At time: 1275.2210531234741 and batch: 600, loss is 5.682450618743896 and perplexity is 293.6682176641635
At time: 1276.7793667316437 and batch: 650, loss is 5.691386795043945 and perplexity is 296.3042491100763
At time: 1278.3396844863892 and batch: 700, loss is 5.655570316314697 and perplexity is 285.8794779729959
At time: 1279.8909254074097 and batch: 750, loss is 5.652853651046753 and perplexity is 285.1038931036092
At time: 1281.4498071670532 and batch: 800, loss is 5.662326869964599 and perplexity is 287.8175780689506
At time: 1283.0074803829193 and batch: 850, loss is 5.694624834060669 and perplexity is 297.26524886720495
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.39991569519043 and perplexity of 221.38775136521025
Finished 44 epochs...
Completing Train Step...
At time: 1287.5411295890808 and batch: 50, loss is 5.72720965385437 and perplexity is 307.1111253312861
At time: 1289.093188047409 and batch: 100, loss is 5.6674074459075925 and perplexity is 289.2835780407575
At time: 1290.6475324630737 and batch: 150, loss is 5.658135757446289 and perplexity is 286.6138265056725
At time: 1292.1950466632843 and batch: 200, loss is 5.692988319396973 and perplexity is 296.77916777632737
At time: 1293.743060350418 and batch: 250, loss is 5.724736824035644 and perplexity is 306.35262998436946
At time: 1295.2910232543945 and batch: 300, loss is 5.672649364471436 and perplexity is 290.8039603775182
At time: 1296.8377239704132 and batch: 350, loss is 5.649456338882446 and perplexity is 284.13694961393355
At time: 1298.3900587558746 and batch: 400, loss is 5.676159658432007 and perplexity is 291.8265615286092
At time: 1299.947093486786 and batch: 450, loss is 5.699516878128052 and perplexity is 298.72304646205794
At time: 1301.5089631080627 and batch: 500, loss is 5.708833675384522 and perplexity is 301.51919386493717
At time: 1303.0640201568604 and batch: 550, loss is 5.678615732192993 and perplexity is 292.5441900026253
At time: 1304.6124436855316 and batch: 600, loss is 5.682449188232422 and perplexity is 293.66779756870886
At time: 1306.2026526927948 and batch: 650, loss is 5.691399116516113 and perplexity is 296.3079000371273
At time: 1307.7571094036102 and batch: 700, loss is 5.655580291748047 and perplexity is 285.8823297588983
At time: 1309.3023946285248 and batch: 750, loss is 5.6528874778747555 and perplexity is 285.11353742708167
At time: 1310.852905511856 and batch: 800, loss is 5.6623618698120115 and perplexity is 287.82765181655435
At time: 1312.400695323944 and batch: 850, loss is 5.69464220046997 and perplexity is 297.27041134201465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.399895350138347 and perplexity of 221.3832472656964
Finished 45 epochs...
Completing Train Step...
At time: 1316.5037004947662 and batch: 50, loss is 5.727177829742431 and perplexity is 307.1013519479714
At time: 1318.0550644397736 and batch: 100, loss is 5.667375631332398 and perplexity is 289.27437475301144
At time: 1319.6087563037872 and batch: 150, loss is 5.658097629547119 and perplexity is 286.6028987309225
At time: 1321.1771759986877 and batch: 200, loss is 5.692960205078125 and perplexity is 296.7708241494655
At time: 1322.7387614250183 and batch: 250, loss is 5.724713096618652 and perplexity is 306.34536111400735
At time: 1324.3078837394714 and batch: 300, loss is 5.6726353740692135 and perplexity is 290.79989194160424
At time: 1325.8673577308655 and batch: 350, loss is 5.649442720413208 and perplexity is 284.133080129974
At time: 1327.4306619167328 and batch: 400, loss is 5.6761507892608645 and perplexity is 291.82397328036893
At time: 1328.983071565628 and batch: 450, loss is 5.699525003433227 and perplexity is 298.7254736878343
At time: 1330.5334136486053 and batch: 500, loss is 5.708848714828491 and perplexity is 301.5237285800587
At time: 1332.083684682846 and batch: 550, loss is 5.678627252578735 and perplexity is 292.547560243954
At time: 1333.632183790207 and batch: 600, loss is 5.682449741363525 and perplexity is 293.6679600055468
At time: 1335.17942404747 and batch: 650, loss is 5.69140884399414 and perplexity is 296.3107823797332
At time: 1336.7422721385956 and batch: 700, loss is 5.655589303970337 and perplexity is 285.88490620561265
At time: 1338.3082246780396 and batch: 750, loss is 5.652917938232422 and perplexity is 285.1222222196775
At time: 1339.8664333820343 and batch: 800, loss is 5.662390604019165 and perplexity is 287.83592243475033
At time: 1341.4292793273926 and batch: 850, loss is 5.694657192230225 and perplexity is 297.2748679821586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.399883270263672 and perplexity of 221.38057299996677
Finished 46 epochs...
Completing Train Step...
At time: 1346.1596639156342 and batch: 50, loss is 5.727148818969726 and perplexity is 307.0924428296835
At time: 1347.7410168647766 and batch: 100, loss is 5.6673471641540525 and perplexity is 289.26614004500465
At time: 1349.2994713783264 and batch: 150, loss is 5.658064098358154 and perplexity is 286.5932887560851
At time: 1350.8567261695862 and batch: 200, loss is 5.6929334449768065 and perplexity is 296.7628826384011
At time: 1352.4057796001434 and batch: 250, loss is 5.724689931869507 and perplexity is 306.3382647827579
At time: 1353.9548473358154 and batch: 300, loss is 5.672621784210205 and perplexity is 290.79594003892595
At time: 1355.5020861625671 and batch: 350, loss is 5.649429244995117 and perplexity is 284.1292513437232
At time: 1357.0486385822296 and batch: 400, loss is 5.6761432456970216 and perplexity is 291.82177189589873
At time: 1358.6018993854523 and batch: 450, loss is 5.69953330039978 and perplexity is 298.7279522133801
At time: 1360.1571865081787 and batch: 500, loss is 5.70886384010315 and perplexity is 301.52828924376007
At time: 1361.7104187011719 and batch: 550, loss is 5.678639230728149 and perplexity is 292.5510644433281
At time: 1363.2701218128204 and batch: 600, loss is 5.682451515197754 and perplexity is 293.66848092428813
At time: 1364.824383020401 and batch: 650, loss is 5.69141655921936 and perplexity is 296.3130684929732
At time: 1366.3809561729431 and batch: 700, loss is 5.65559757232666 and perplexity is 285.887270013657
At time: 1367.936454296112 and batch: 750, loss is 5.6529457378387455 and perplexity is 285.13014861538414
At time: 1369.4851388931274 and batch: 800, loss is 5.662415580749512 and perplexity is 287.8431117247513
At time: 1371.0331017971039 and batch: 850, loss is 5.694670991897583 and perplexity is 297.27897030475594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.399874369303386 and perplexity of 221.37860250904802
Finished 47 epochs...
Completing Train Step...
At time: 1375.1189622879028 and batch: 50, loss is 5.7271215534210205 and perplexity is 307.08406989987304
At time: 1376.656759262085 and batch: 100, loss is 5.667320375442505 and perplexity is 289.2583910816113
At time: 1378.194679260254 and batch: 150, loss is 5.658033199310303 and perplexity is 286.58443343315327
At time: 1379.7349524497986 and batch: 200, loss is 5.692907743453979 and perplexity is 296.7552554784139
At time: 1381.2751035690308 and batch: 250, loss is 5.724667530059815 and perplexity is 306.33140232811496
At time: 1382.8174154758453 and batch: 300, loss is 5.6726084804534915 and perplexity is 290.79207138622036
At time: 1384.3626763820648 and batch: 350, loss is 5.649415607452393 and perplexity is 284.12537654534015
At time: 1385.9050710201263 and batch: 400, loss is 5.676136655807495 and perplexity is 291.8198488289969
At time: 1387.4792773723602 and batch: 450, loss is 5.699541616439819 and perplexity is 298.73043645732093
At time: 1389.019079208374 and batch: 500, loss is 5.708878965377807 and perplexity is 301.532849976443
At time: 1390.557921409607 and batch: 550, loss is 5.6786511039733885 and perplexity is 292.5545379944824
At time: 1392.0985479354858 and batch: 600, loss is 5.6824543666839595 and perplexity is 293.66931831710434
At time: 1393.6394979953766 and batch: 650, loss is 5.6914230155944825 and perplexity is 296.314981607473
At time: 1395.1819379329681 and batch: 700, loss is 5.655605487823486 and perplexity is 285.88953296239157
At time: 1396.7243721485138 and batch: 750, loss is 5.652971391677856 and perplexity is 285.137463392168
At time: 1398.2693502902985 and batch: 800, loss is 5.662438631057739 and perplexity is 287.84974667366623
At time: 1399.8141832351685 and batch: 850, loss is 5.694684352874756 and perplexity is 297.28294226882673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.399869283040364 and perplexity of 221.37747652211186
Finished 48 epochs...
Completing Train Step...
At time: 1404.6269235610962 and batch: 50, loss is 5.7270958042144775 and perplexity is 307.076162830532
At time: 1406.1816673278809 and batch: 100, loss is 5.667295217514038 and perplexity is 289.2511140312382
At time: 1407.7432670593262 and batch: 150, loss is 5.658004198074341 and perplexity is 286.57612225089406
At time: 1409.2980263233185 and batch: 200, loss is 5.692882719039917 and perplexity is 296.7478294449422
At time: 1410.8530428409576 and batch: 250, loss is 5.724645557403565 and perplexity is 306.3246714874605
At time: 1412.4049880504608 and batch: 300, loss is 5.672595481872559 and perplexity is 290.78829152651235
At time: 1413.969004392624 and batch: 350, loss is 5.649402151107788 and perplexity is 284.121553282086
At time: 1415.5268528461456 and batch: 400, loss is 5.676130647659302 and perplexity is 291.81809553736645
At time: 1417.0816979408264 and batch: 450, loss is 5.6995494556427 and perplexity is 298.73277827499794
At time: 1418.641887664795 and batch: 500, loss is 5.708893566131592 and perplexity is 301.53725261548436
At time: 1420.1989681720734 and batch: 550, loss is 5.678662710189819 and perplexity is 292.55793346547233
At time: 1421.7535424232483 and batch: 600, loss is 5.6824575042724605 and perplexity is 293.6702397320261
At time: 1423.3059272766113 and batch: 650, loss is 5.691428394317627 and perplexity is 296.3165754080088
At time: 1424.8569042682648 and batch: 700, loss is 5.65561318397522 and perplexity is 285.89173322008304
At time: 1426.4128124713898 and batch: 750, loss is 5.652995557785034 and perplexity is 285.1443541379296
At time: 1428.024082183838 and batch: 800, loss is 5.662460050582886 and perplexity is 287.85591234458616
At time: 1429.5939075946808 and batch: 850, loss is 5.694697217941284 and perplexity is 297.28676685825855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.399866104125977 and perplexity of 221.3767727831852
Finished 49 epochs...
Completing Train Step...
At time: 1433.6695504188538 and batch: 50, loss is 5.727071037292481 and perplexity is 307.0685575933396
At time: 1435.2544140815735 and batch: 100, loss is 5.667270793914795 and perplexity is 289.24404956421864
At time: 1436.801831960678 and batch: 150, loss is 5.657976522445678 and perplexity is 286.5681911863002
At time: 1438.3482358455658 and batch: 200, loss is 5.692858428955078 and perplexity is 296.7406215025302
At time: 1439.8929190635681 and batch: 250, loss is 5.7246237564086915 and perplexity is 306.3179933776628
At time: 1441.4394555091858 and batch: 300, loss is 5.672582664489746 and perplexity is 290.78456440554834
At time: 1442.994960308075 and batch: 350, loss is 5.649389305114746 and perplexity is 284.1179034820322
At time: 1444.5519711971283 and batch: 400, loss is 5.6761250400543215 and perplexity is 291.81645914134873
At time: 1446.114691734314 and batch: 450, loss is 5.699557304382324 and perplexity is 298.7351229599933
At time: 1447.6688270568848 and batch: 500, loss is 5.708907966613769 and perplexity is 301.54159492858213
At time: 1449.230295419693 and batch: 550, loss is 5.67867416381836 and perplexity is 292.56128433455865
At time: 1450.7777316570282 and batch: 600, loss is 5.682461061477661 and perplexity is 293.6712843791882
At time: 1452.3238642215729 and batch: 650, loss is 5.691433048248291 and perplexity is 296.31795444801435
At time: 1453.8674564361572 and batch: 700, loss is 5.655620508193969 and perplexity is 285.8938271613441
At time: 1455.4152932167053 and batch: 750, loss is 5.653018264770508 and perplexity is 285.15082898014884
At time: 1456.9583992958069 and batch: 800, loss is 5.662480840682983 and perplexity is 287.8618969600276
At time: 1458.5049233436584 and batch: 850, loss is 5.694709901809692 and perplexity is 297.2905376284028
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.399861653645833 and perplexity of 221.37578755244607
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fc6e480c898>
SETTINGS FOR THIS RUN
{'lr': 20.231477962974886, 'anneal': 2.001912088084294, 'wordvec_source': 'glove', 'seq_len': 50, 'tune_wordvecs': True, 'num_layers': 1, 'dropout': 0.5188204368403374, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.3140530586242676 and batch: 50, loss is 7.585858125686645 and perplexity is 1970.1365335216854
At time: 3.8766767978668213 and batch: 100, loss is 6.4906258869171145 and perplexity is 658.9356533829232
At time: 5.449699401855469 and batch: 150, loss is 6.2839225482940675 and perplexity is 535.8865875518551
At time: 7.026192665100098 and batch: 200, loss is 6.333660726547241 and perplexity is 563.2145992925781
At time: 8.647620677947998 and batch: 250, loss is 6.431803512573242 and perplexity is 621.2934487389227
At time: 10.223361492156982 and batch: 300, loss is 6.444213905334473 and perplexity is 629.051988139151
At time: 11.794661283493042 and batch: 350, loss is 6.479070291519165 and perplexity is 651.3650850339714
At time: 13.341686487197876 and batch: 400, loss is 6.5373450374603275 and perplexity is 690.4510208510802
At time: 14.88825798034668 and batch: 450, loss is 6.596706857681275 and perplexity is 732.6783977692578
At time: 16.437763929367065 and batch: 500, loss is 6.6641287517547605 and perplexity is 783.7803002099264
At time: 17.99703097343445 and batch: 550, loss is 6.627107009887696 and perplexity is 755.2939489898706
At time: 19.55665135383606 and batch: 600, loss is 6.682825031280518 and perplexity is 798.5719190435187
At time: 21.111595153808594 and batch: 650, loss is 6.733223285675049 and perplexity is 839.8499842209292
At time: 22.6641526222229 and batch: 700, loss is 6.73116774559021 and perplexity is 838.1254119840214
At time: 24.21642780303955 and batch: 750, loss is 6.787637872695923 and perplexity is 886.8163127950361
At time: 25.77490234375 and batch: 800, loss is 6.86264856338501 and perplexity is 955.8954674343345
At time: 27.332229137420654 and batch: 850, loss is 6.852533683776856 and perplexity is 946.2754346194203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.474240620930989 and perplexity of 648.2267908045312
Finished 1 epochs...
Completing Train Step...
At time: 32.150668144226074 and batch: 50, loss is 6.783777799606323 and perplexity is 883.3997353721783
At time: 33.693910121917725 and batch: 100, loss is 6.737446985244751 and perplexity is 843.4047601059275
At time: 35.24048376083374 and batch: 150, loss is 6.795559368133545 and perplexity is 893.8691216872545
At time: 36.78172564506531 and batch: 200, loss is 6.827827653884888 and perplexity is 923.1831592601915
At time: 38.325557470321655 and batch: 250, loss is 6.874786157608032 and perplexity is 967.5684363115259
At time: 39.87131118774414 and batch: 300, loss is 6.843235597610474 and perplexity is 937.5176624469165
At time: 41.406697511672974 and batch: 350, loss is 6.801792573928833 and perplexity is 899.4581926957497
At time: 42.940247774124146 and batch: 400, loss is 6.781417112350464 and perplexity is 881.3167644638343
At time: 44.474278688430786 and batch: 450, loss is 6.8042542839050295 and perplexity is 901.6751255051332
At time: 46.01077342033386 and batch: 500, loss is 6.85194052696228 and perplexity is 945.7143113304132
At time: 47.547857999801636 and batch: 550, loss is 6.761587686538697 and perplexity is 864.012889018867
At time: 49.085580825805664 and batch: 600, loss is 6.796134748458862 and perplexity is 894.383584384946
At time: 50.62849426269531 and batch: 650, loss is 6.8084758853912355 and perplexity is 905.4896846641193
At time: 52.17536759376526 and batch: 700, loss is 6.768509006500244 and perplexity is 870.0137416294277
At time: 53.714426040649414 and batch: 750, loss is 6.8063342571258545 and perplexity is 903.5525374264762
At time: 55.301732301712036 and batch: 800, loss is 6.784042921066284 and perplexity is 883.6339746493068
At time: 56.847718715667725 and batch: 850, loss is 6.8109715461730955 and perplexity is 907.7523019467636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.32375971476237 and perplexity of 557.6657199968822
Finished 2 epochs...
Completing Train Step...
At time: 60.95611047744751 and batch: 50, loss is 6.80508939743042 and perplexity is 902.4284411061931
At time: 62.50216794013977 and batch: 100, loss is 6.76316879272461 and perplexity is 865.3800656833275
At time: 64.04784345626831 and batch: 150, loss is 6.747949810028076 and perplexity is 852.3095735133671
At time: 65.59664463996887 and batch: 200, loss is 6.754197654724121 and perplexity is 857.6513412526848
At time: 67.14566421508789 and batch: 250, loss is 6.8372878074646 and perplexity is 931.958054213975
At time: 68.69697451591492 and batch: 300, loss is 6.827698373794556 and perplexity is 923.063817772375
At time: 70.24609375 and batch: 350, loss is 6.739709796905518 and perplexity is 845.3153871111238
At time: 71.79907560348511 and batch: 400, loss is 6.69948748588562 and perplexity is 811.9895623949251
At time: 73.34280371665955 and batch: 450, loss is 6.733471918106079 and perplexity is 840.0588241253174
At time: 74.88357424736023 and batch: 500, loss is 6.7926532173156735 and perplexity is 891.2751742350922
At time: 76.43105292320251 and batch: 550, loss is 6.717447786331177 and perplexity is 826.7048891782322
At time: 77.97904253005981 and batch: 600, loss is 6.718730821609497 and perplexity is 827.7662614592464
At time: 79.52192902565002 and batch: 650, loss is 6.708856191635132 and perplexity is 819.6326004599509
At time: 81.07385516166687 and batch: 700, loss is 6.686590719223022 and perplexity is 801.5847608410226
At time: 82.63509154319763 and batch: 750, loss is 6.751337261199951 and perplexity is 855.2016261550376
At time: 84.18667221069336 and batch: 800, loss is 6.780491638183594 and perplexity is 880.5015058740642
At time: 85.73259353637695 and batch: 850, loss is 6.731346454620361 and perplexity is 838.2752059479104
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.2426503499348955 and perplexity of 514.2195683063519
Finished 3 epochs...
Completing Train Step...
At time: 89.86326479911804 and batch: 50, loss is 6.673888158798218 and perplexity is 791.4669788949988
At time: 91.41761803627014 and batch: 100, loss is 6.661796274185181 and perplexity is 781.9542806429135
At time: 92.99950647354126 and batch: 150, loss is 6.657675867080688 and perplexity is 778.7389394759628
At time: 94.54902458190918 and batch: 200, loss is 6.679292430877686 and perplexity is 795.7558604939264
At time: 96.09560036659241 and batch: 250, loss is 6.73052149772644 and perplexity is 837.5839502050588
At time: 97.64713978767395 and batch: 300, loss is 6.73036485671997 and perplexity is 837.4527604872088
At time: 99.18867111206055 and batch: 350, loss is 6.695000791549683 and perplexity is 808.3545740630875
At time: 100.73587679862976 and batch: 400, loss is 6.665800333023071 and perplexity is 785.0915483017802
At time: 102.28435969352722 and batch: 450, loss is 6.6789599132537845 and perplexity is 795.491301633669
At time: 103.83182430267334 and batch: 500, loss is 6.736645383834839 and perplexity is 842.728956559825
At time: 105.37912368774414 and batch: 550, loss is 6.67293565750122 and perplexity is 790.7134644897641
At time: 106.92332863807678 and batch: 600, loss is 6.694018850326538 and perplexity is 807.5612069675727
At time: 108.48152112960815 and batch: 650, loss is 6.716849536895752 and perplexity is 826.2104613553473
At time: 110.0369119644165 and batch: 700, loss is 6.6839221096038814 and perplexity is 799.448495734272
At time: 111.58721494674683 and batch: 750, loss is 6.667479200363159 and perplexity is 786.4107199084153
At time: 113.14147782325745 and batch: 800, loss is 6.721456518173218 and perplexity is 830.0255788216709
At time: 114.69881987571716 and batch: 850, loss is 6.691143302917481 and perplexity is 805.2423620033697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.286084493001302 and perplexity of 537.0463979947683
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 118.84056258201599 and batch: 50, loss is 6.635806360244751 and perplexity is 761.8931785613902
At time: 120.39933061599731 and batch: 100, loss is 6.461334657669068 and perplexity is 639.9145537462866
At time: 121.95185041427612 and batch: 150, loss is 6.401784801483155 and perplexity is 602.9201709524591
At time: 123.50051951408386 and batch: 200, loss is 6.4042156791687015 and perplexity is 604.387578964293
At time: 125.05609273910522 and batch: 250, loss is 6.462709188461304 and perplexity is 640.7947407883264
At time: 126.6035487651825 and batch: 300, loss is 6.476390151977539 and perplexity is 649.6216730508326
At time: 128.14691591262817 and batch: 350, loss is 6.397298831939697 and perplexity is 600.2215469265983
At time: 129.6903018951416 and batch: 400, loss is 6.351405630111694 and perplexity is 573.297987833676
At time: 131.27188777923584 and batch: 450, loss is 6.320187501907348 and perplexity is 555.6771732138993
At time: 132.83406043052673 and batch: 500, loss is 6.348617086410522 and perplexity is 571.7015482460755
At time: 134.38959646224976 and batch: 550, loss is 6.334321546554565 and perplexity is 563.5869057684434
At time: 135.93665194511414 and batch: 600, loss is 6.321269197463989 and perplexity is 556.278571949622
At time: 137.4909098148346 and batch: 650, loss is 6.33556321144104 and perplexity is 564.2871264695792
At time: 139.036461353302 and batch: 700, loss is 6.302146701812744 and perplexity is 545.7421993141038
At time: 140.58764672279358 and batch: 750, loss is 6.302496089935302 and perplexity is 545.9329084703398
At time: 142.1330807209015 and batch: 800, loss is 6.302234144210815 and perplexity is 545.7899224072207
At time: 143.67793583869934 and batch: 850, loss is 6.25603102684021 and perplexity is 521.1464137993291
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.882362365722656 and perplexity of 358.65551720925976
Finished 5 epochs...
Completing Train Step...
At time: 147.8169138431549 and batch: 50, loss is 6.275475339889526 and perplexity is 531.3789073266335
At time: 149.36574125289917 and batch: 100, loss is 6.210200653076172 and perplexity is 497.8011265937012
At time: 150.91476202011108 and batch: 150, loss is 6.197303924560547 and perplexity is 491.4223417501364
At time: 152.46156287193298 and batch: 200, loss is 6.220699272155762 and perplexity is 503.05488132795773
At time: 154.01028275489807 and batch: 250, loss is 6.2470743942260745 and perplexity is 516.4995380768925
At time: 155.56007981300354 and batch: 300, loss is 6.226121950149536 and perplexity is 505.79019562415965
At time: 157.1081566810608 and batch: 350, loss is 6.214317293167114 and perplexity is 499.854618512336
At time: 158.65357208251953 and batch: 400, loss is 6.235518131256104 and perplexity is 510.56508965820217
At time: 160.19988346099854 and batch: 450, loss is 6.207647609710693 and perplexity is 496.53183969181254
At time: 161.74959540367126 and batch: 500, loss is 6.243532552719116 and perplexity is 514.6734144039228
At time: 163.3022804260254 and batch: 550, loss is 6.220990943908691 and perplexity is 503.2016296271409
At time: 164.8529496192932 and batch: 600, loss is 6.226950597763062 and perplexity is 506.20949116283936
At time: 166.40279722213745 and batch: 650, loss is 6.252224025726318 and perplexity is 519.166180588183
At time: 167.94840812683105 and batch: 700, loss is 6.209881353378296 and perplexity is 497.64220421766083
At time: 169.494300365448 and batch: 750, loss is 6.225770807266235 and perplexity is 505.61262217517384
At time: 171.06882858276367 and batch: 800, loss is 6.2688596725463865 and perplexity is 527.8750840864184
At time: 172.615136384964 and batch: 850, loss is 6.267908697128296 and perplexity is 527.3733264750207
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.871028900146484 and perplexity of 354.61365467005635
Finished 6 epochs...
Completing Train Step...
At time: 176.66141772270203 and batch: 50, loss is 6.27154390335083 and perplexity is 529.2939260457608
At time: 178.2336847782135 and batch: 100, loss is 6.217273502349854 and perplexity is 501.3344796374106
At time: 179.7736976146698 and batch: 150, loss is 6.218834447860718 and perplexity is 502.11764652424114
At time: 181.32080912590027 and batch: 200, loss is 6.235779905319214 and perplexity is 510.69875985113583
At time: 182.8701000213623 and batch: 250, loss is 6.271721878051758 and perplexity is 529.3881353571401
At time: 184.41876983642578 and batch: 300, loss is 6.268552446365357 and perplexity is 527.71293195024
At time: 185.97190380096436 and batch: 350, loss is 6.233192691802978 and perplexity is 509.37918086928204
At time: 187.52061009407043 and batch: 400, loss is 6.2440174865722655 and perplexity is 514.9230574911833
At time: 189.0705862045288 and batch: 450, loss is 6.224215497970581 and perplexity is 504.8268493821674
At time: 190.62376737594604 and batch: 500, loss is 6.235690174102783 and perplexity is 510.65293628611727
At time: 192.17576599121094 and batch: 550, loss is 6.208276996612549 and perplexity is 496.84444869375494
At time: 193.721036195755 and batch: 600, loss is 6.220298080444336 and perplexity is 502.8531003583096
At time: 195.2664611339569 and batch: 650, loss is 6.258658857345581 and perplexity is 522.5176992069717
At time: 196.8104751110077 and batch: 700, loss is 6.217886047363281 and perplexity is 501.641663645383
At time: 198.35567665100098 and batch: 750, loss is 6.233994588851929 and perplexity is 509.78781435031203
At time: 199.9048924446106 and batch: 800, loss is 6.248620977401734 and perplexity is 517.2989656039525
At time: 201.4583580493927 and batch: 850, loss is 6.240205974578857 and perplexity is 512.964157638527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.858535130818685 and perplexity of 350.2107551457696
Finished 7 epochs...
Completing Train Step...
At time: 205.468576669693 and batch: 50, loss is 6.235343418121338 and perplexity is 510.47589502285007
At time: 207.03496646881104 and batch: 100, loss is 6.178918323516846 and perplexity is 482.4697977662894
At time: 208.57419443130493 and batch: 150, loss is 6.164282941818238 and perplexity is 475.46008812562104
At time: 210.1423876285553 and batch: 200, loss is 6.193678379058838 and perplexity is 489.64389356006177
At time: 211.6818563938141 and batch: 250, loss is 6.225681171417237 and perplexity is 505.56730318965407
At time: 213.21972918510437 and batch: 300, loss is 6.2036301708221435 and perplexity is 494.54105497517526
At time: 214.7565734386444 and batch: 350, loss is 6.184357490539551 and perplexity is 485.1011813598315
At time: 216.29468703269958 and batch: 400, loss is 6.204775609970093 and perplexity is 495.1078462102761
At time: 217.83550667762756 and batch: 450, loss is 6.182537097930908 and perplexity is 484.2189100385775
At time: 219.37719130516052 and batch: 500, loss is 6.201983709335327 and perplexity is 493.72748211648667
At time: 220.92359328269958 and batch: 550, loss is 6.172198457717895 and perplexity is 479.23853446053636
At time: 222.49505996704102 and batch: 600, loss is 6.180247917175293 and perplexity is 483.11171319856976
At time: 224.0632848739624 and batch: 650, loss is 6.21493257522583 and perplexity is 500.16226472596514
At time: 225.6130621433258 and batch: 700, loss is 6.164873790740967 and perplexity is 475.7410962149792
At time: 227.15085649490356 and batch: 750, loss is 6.188029298782348 and perplexity is 486.88565399191856
At time: 228.68755912780762 and batch: 800, loss is 6.207435264587402 and perplexity is 496.4264147707253
At time: 230.22662568092346 and batch: 850, loss is 6.213247909545898 and perplexity is 499.32036788066057
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.829936345418294 and perplexity of 340.33701437193844
Finished 8 epochs...
Completing Train Step...
At time: 234.25972318649292 and batch: 50, loss is 6.207044515609741 and perplexity is 496.232474550109
At time: 235.84715747833252 and batch: 100, loss is 6.15724931716919 and perplexity is 472.1276137557126
At time: 237.38814282417297 and batch: 150, loss is 6.1439408111572265 and perplexity is 465.8859264611975
At time: 238.92761301994324 and batch: 200, loss is 6.1652015113830565 and perplexity is 475.89703194278195
At time: 240.46471667289734 and batch: 250, loss is 6.199110870361328 and perplexity is 492.3111180304304
At time: 242.00193858146667 and batch: 300, loss is 6.192823505401611 and perplexity is 489.2254887611367
At time: 243.53726148605347 and batch: 350, loss is 6.1629225730896 and perplexity is 474.8137268345433
At time: 245.078307390213 and batch: 400, loss is 6.169639167785644 and perplexity is 478.01359226387103
At time: 246.6256401538849 and batch: 450, loss is 6.152667875289917 and perplexity is 469.9695358626765
At time: 248.17164611816406 and batch: 500, loss is 6.161654376983643 and perplexity is 474.2119515802879
At time: 249.76529169082642 and batch: 550, loss is 6.128249101638794 and perplexity is 458.63243848238733
At time: 251.31324648857117 and batch: 600, loss is 6.147091255187989 and perplexity is 467.3559884553504
At time: 252.8681607246399 and batch: 650, loss is 6.18576473236084 and perplexity is 485.78431658529144
At time: 254.4173128604889 and batch: 700, loss is 6.140313320159912 and perplexity is 464.19899097864345
At time: 255.97041773796082 and batch: 750, loss is 6.158700141906738 and perplexity is 472.8130853063335
At time: 257.5203938484192 and batch: 800, loss is 6.167811222076416 and perplexity is 477.14060749640123
At time: 259.07159328460693 and batch: 850, loss is 6.163209133148193 and perplexity is 474.94980898084856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.80046272277832 and perplexity of 330.4524324057534
Finished 9 epochs...
Completing Train Step...
At time: 263.1923973560333 and batch: 50, loss is 6.163104162216187 and perplexity is 474.89995567336354
At time: 264.74348735809326 and batch: 100, loss is 6.0995810604095455 and perplexity is 445.67102173178773
At time: 266.288204908371 and batch: 150, loss is 6.08674988746643 and perplexity is 439.9890707795528
At time: 267.829509973526 and batch: 200, loss is 6.126655807495117 and perplexity is 457.9022839339997
At time: 269.3929283618927 and batch: 250, loss is 6.158361206054687 and perplexity is 472.6528591551317
At time: 270.9536032676697 and batch: 300, loss is 6.142714672088623 and perplexity is 465.3150355925737
At time: 272.50354743003845 and batch: 350, loss is 6.126195831298828 and perplexity is 457.691708216785
At time: 274.05038380622864 and batch: 400, loss is 6.141960535049439 and perplexity is 464.96425657369406
At time: 275.59936690330505 and batch: 450, loss is 6.120373296737671 and perplexity is 455.03452570280814
At time: 277.15146684646606 and batch: 500, loss is 6.142728500366211 and perplexity is 465.321470142541
At time: 278.69824147224426 and batch: 550, loss is 6.116423969268799 and perplexity is 453.2409893136054
At time: 280.24168705940247 and batch: 600, loss is 6.13179235458374 and perplexity is 460.26037160693085
At time: 281.79191517829895 and batch: 650, loss is 6.159003124237061 and perplexity is 472.9563610206354
At time: 283.3432922363281 and batch: 700, loss is 6.11719054222107 and perplexity is 453.5885648008368
At time: 284.89704990386963 and batch: 750, loss is 6.144445028305054 and perplexity is 466.1208933664439
At time: 286.4417152404785 and batch: 800, loss is 6.162232627868653 and perplexity is 474.48624435834915
At time: 288.03448033332825 and batch: 850, loss is 6.166203784942627 and perplexity is 476.3742500665808
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.796150207519531 and perplexity of 329.0304196783491
Finished 10 epochs...
Completing Train Step...
At time: 292.130010843277 and batch: 50, loss is 6.167426795959472 and perplexity is 476.9572174376477
At time: 293.67672085762024 and batch: 100, loss is 6.1074691581726075 and perplexity is 449.20042014069355
At time: 295.23632740974426 and batch: 150, loss is 6.101052389144898 and perplexity is 446.3272329454021
At time: 296.78750586509705 and batch: 200, loss is 6.128414716720581 and perplexity is 458.70840122131085
At time: 298.34219765663147 and batch: 250, loss is 6.160040111541748 and perplexity is 473.4470651456858
At time: 299.8849766254425 and batch: 300, loss is 6.1506709861755375 and perplexity is 469.03199520632984
At time: 301.4290509223938 and batch: 350, loss is 6.122522716522217 and perplexity is 456.01363779950265
At time: 302.97290110588074 and batch: 400, loss is 6.1303440284729005 and perplexity is 459.59424699222535
At time: 304.5119540691376 and batch: 450, loss is 6.117304649353027 and perplexity is 453.64032544412805
At time: 306.05153703689575 and batch: 500, loss is 6.130950498580932 and perplexity is 459.87306170269756
At time: 307.5891652107239 and batch: 550, loss is 6.093128261566162 and perplexity is 442.8044549143977
At time: 309.12803983688354 and batch: 600, loss is 6.111997909545899 and perplexity is 451.2393505800013
At time: 310.6648678779602 and batch: 650, loss is 6.137713441848755 and perplexity is 462.99369757694217
At time: 312.2021632194519 and batch: 700, loss is 6.061545934677124 and perplexity is 429.03818945607316
At time: 313.74309945106506 and batch: 750, loss is 6.066279602050781 and perplexity is 431.0739279887569
At time: 315.2822949886322 and batch: 800, loss is 6.077317333221435 and perplexity is 435.8583621953914
At time: 316.82046365737915 and batch: 850, loss is 6.085285224914551 and perplexity is 439.3451069743021
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.711536407470703 and perplexity of 302.3352217203889
Finished 11 epochs...
Completing Train Step...
At time: 320.8718774318695 and batch: 50, loss is 6.076068782806397 and perplexity is 435.3145106400668
At time: 322.41968846321106 and batch: 100, loss is 6.007178421020508 and perplexity is 406.3351944285137
At time: 323.9739089012146 and batch: 150, loss is 5.994123840332032 and perplexity is 401.0651329122839
At time: 325.5197603702545 and batch: 200, loss is 6.018923358917236 and perplexity is 411.13571175803895
At time: 327.06023716926575 and batch: 250, loss is 6.064625396728515 and perplexity is 430.36143267203266
At time: 328.64229917526245 and batch: 300, loss is 6.041272230148316 and perplexity is 420.42757542191833
At time: 330.190456867218 and batch: 350, loss is 6.017798280715942 and perplexity is 410.67341204139814
At time: 331.7256443500519 and batch: 400, loss is 6.034698286056519 and perplexity is 417.6727729263647
At time: 333.2625274658203 and batch: 450, loss is 6.029901924133301 and perplexity is 415.6742597694698
At time: 334.7972979545593 and batch: 500, loss is 6.050468311309815 and perplexity is 424.31169348791184
At time: 336.33215260505676 and batch: 550, loss is 6.009661741256714 and perplexity is 407.3455087873598
At time: 337.8750422000885 and batch: 600, loss is 6.023855819702148 and perplexity is 413.1686320617973
At time: 339.41478085517883 and batch: 650, loss is 6.056944274902344 and perplexity is 427.0684372184634
At time: 340.9523675441742 and batch: 700, loss is 6.005221538543701 and perplexity is 405.54082170735245
At time: 342.4956338405609 and batch: 750, loss is 6.02217583656311 and perplexity is 412.47509845185544
At time: 344.052631855011 and batch: 800, loss is 6.040674600601196 and perplexity is 420.1763905456505
At time: 345.60192680358887 and batch: 850, loss is 6.049044542312622 and perplexity is 423.70800151451465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.689148585001628 and perplexity of 295.6417995912694
Finished 12 epochs...
Completing Train Step...
At time: 349.5923252105713 and batch: 50, loss is 6.057689218521118 and perplexity is 427.3866976538714
At time: 351.15318870544434 and batch: 100, loss is 5.998389625549317 and perplexity is 402.77964489746745
At time: 352.69300079345703 and batch: 150, loss is 5.9876961994171145 and perplexity is 398.4954974483944
At time: 354.2486307621002 and batch: 200, loss is 6.007159757614136 and perplexity is 406.3276109004242
At time: 355.7878649234772 and batch: 250, loss is 6.03846755027771 and perplexity is 419.2500627090686
At time: 357.3271424770355 and batch: 300, loss is 6.023395376205444 and perplexity is 412.97843504297293
At time: 358.8671770095825 and batch: 350, loss is 6.001767053604126 and perplexity is 404.14230401629555
At time: 360.4062304496765 and batch: 400, loss is 6.018121700286866 and perplexity is 410.8062533406904
At time: 361.94548416137695 and batch: 450, loss is 6.005489320755005 and perplexity is 405.6494328667824
At time: 363.4843924045563 and batch: 500, loss is 6.037050724029541 and perplexity is 418.6564788175912
At time: 365.02614569664 and batch: 550, loss is 6.006153993606567 and perplexity is 405.9191466578482
At time: 366.59538984298706 and batch: 600, loss is 6.0262978076934814 and perplexity is 414.1788178284396
At time: 368.1370692253113 and batch: 650, loss is 6.05468505859375 and perplexity is 426.1046863110176
At time: 369.6774809360504 and batch: 700, loss is 6.010935659408569 and perplexity is 407.8647642993683
At time: 371.2190885543823 and batch: 750, loss is 6.030525074005127 and perplexity is 415.93336785436685
At time: 372.7659978866577 and batch: 800, loss is 6.031055040359497 and perplexity is 416.1538569657342
At time: 374.3048167228699 and batch: 850, loss is 6.039975643157959 and perplexity is 419.88280774290087
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.69515864054362 and perplexity of 297.4239733445149
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 378.3095088005066 and batch: 50, loss is 6.003384332656861 and perplexity is 404.79644371954487
At time: 379.87795305252075 and batch: 100, loss is 5.899899539947509 and perplexity is 365.0007980241039
At time: 381.41543412208557 and batch: 150, loss is 5.8837534523010255 and perplexity is 359.15478526754305
At time: 382.954532623291 and batch: 200, loss is 5.894403448104859 and perplexity is 363.00022282399124
At time: 384.4922785758972 and batch: 250, loss is 5.920326356887817 and perplexity is 372.533272838833
At time: 386.03238701820374 and batch: 300, loss is 5.896082496643066 and perplexity is 363.6102297898338
At time: 387.57090163230896 and batch: 350, loss is 5.8661642742156985 and perplexity is 352.8927809871539
At time: 389.110258102417 and batch: 400, loss is 5.884269685745239 and perplexity is 359.34024084441444
At time: 390.6490545272827 and batch: 450, loss is 5.8877492523193355 and perplexity is 360.59276699551447
At time: 392.189551115036 and batch: 500, loss is 5.888882141113282 and perplexity is 361.00150998688264
At time: 393.72872853279114 and batch: 550, loss is 5.854920148849487 and perplexity is 348.94703511925445
At time: 395.2685341835022 and batch: 600, loss is 5.853582038879394 and perplexity is 348.48041787474517
At time: 396.80788588523865 and batch: 650, loss is 5.8698100566864015 and perplexity is 354.18169943258914
At time: 398.34921383857727 and batch: 700, loss is 5.8335026836395265 and perplexity is 341.55293818342057
At time: 399.8872151374817 and batch: 750, loss is 5.8320159912109375 and perplexity is 341.0455312887134
At time: 401.42487382888794 and batch: 800, loss is 5.841654090881348 and perplexity is 344.34845348996487
At time: 402.96413803100586 and batch: 850, loss is 5.855636491775512 and perplexity is 349.1970904113964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.56205685933431 and perplexity of 260.3578053385086
Finished 14 epochs...
Completing Train Step...
At time: 406.9430251121521 and batch: 50, loss is 5.851922359466553 and perplexity is 347.90253178489377
At time: 408.50699758529663 and batch: 100, loss is 5.789071302413941 and perplexity is 326.70946914694485
At time: 410.04405212402344 and batch: 150, loss is 5.784731388092041 and perplexity is 325.29465035951137
At time: 411.5815887451172 and batch: 200, loss is 5.825880336761474 and perplexity is 338.95940019154784
At time: 413.119193315506 and batch: 250, loss is 5.839252815246582 and perplexity is 343.52256992261954
At time: 414.659068107605 and batch: 300, loss is 5.811519384384155 and perplexity is 334.1264066441757
At time: 416.19584107398987 and batch: 350, loss is 5.797465400695801 and perplexity is 329.46344293329236
At time: 417.73513317108154 and batch: 400, loss is 5.82168981552124 and perplexity is 337.54195561551796
At time: 419.2747573852539 and batch: 450, loss is 5.82839991569519 and perplexity is 339.81451196408545
At time: 420.81378507614136 and batch: 500, loss is 5.851665964126587 and perplexity is 347.81334263131225
At time: 422.3516356945038 and batch: 550, loss is 5.824074459075928 and perplexity is 338.34783334809674
At time: 423.88920545578003 and batch: 600, loss is 5.833606185913086 and perplexity is 341.5882915186073
At time: 425.43290162086487 and batch: 650, loss is 5.845800485610962 and perplexity is 345.7792223189478
At time: 426.96970796585083 and batch: 700, loss is 5.815886468887329 and perplexity is 335.5887556735352
At time: 428.505886554718 and batch: 750, loss is 5.826248350143433 and perplexity is 339.0841647428631
At time: 430.04260635375977 and batch: 800, loss is 5.842566347122192 and perplexity is 344.6627308446298
At time: 431.58030676841736 and batch: 850, loss is 5.856766920089722 and perplexity is 349.5920558876564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.552204132080078 and perplexity of 257.8051667698076
Finished 15 epochs...
Completing Train Step...
At time: 435.6190094947815 and batch: 50, loss is 5.855389003753662 and perplexity is 349.11067900758934
At time: 437.1572151184082 and batch: 100, loss is 5.79413290977478 and perplexity is 328.36733639784825
At time: 438.6937801837921 and batch: 150, loss is 5.793453187942505 and perplexity is 328.1442137895371
At time: 440.2298381328583 and batch: 200, loss is 5.826886959075928 and perplexity is 339.30077607694034
At time: 441.76527214050293 and batch: 250, loss is 5.847073822021485 and perplexity is 346.2197960324759
At time: 443.3017382621765 and batch: 300, loss is 5.818740158081055 and perplexity is 336.5477894204292
At time: 444.8640949726105 and batch: 350, loss is 5.801266784667969 and perplexity is 330.7182434629421
At time: 446.4001772403717 and batch: 400, loss is 5.829181299209595 and perplexity is 340.08014118732257
At time: 447.93764567375183 and batch: 450, loss is 5.834333915710449 and perplexity is 341.83696596970486
At time: 449.4800841808319 and batch: 500, loss is 5.8586078548431395 and perplexity is 350.2362248074081
At time: 451.04963731765747 and batch: 550, loss is 5.823437433242798 and perplexity is 338.13236567419426
At time: 452.60459542274475 and batch: 600, loss is 5.831522226333618 and perplexity is 340.877176551045
At time: 454.1646809577942 and batch: 650, loss is 5.849876852035522 and perplexity is 347.19162190445167
At time: 455.72744369506836 and batch: 700, loss is 5.818435850143433 and perplexity is 336.4453908378604
At time: 457.27086448669434 and batch: 750, loss is 5.821122961044312 and perplexity is 337.35067266674605
At time: 458.8170223236084 and batch: 800, loss is 5.833968887329101 and perplexity is 341.7122085467176
At time: 460.3781158924103 and batch: 850, loss is 5.845500841140747 and perplexity is 345.6756270087207
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.551554361979167 and perplexity of 257.637707091618
Finished 16 epochs...
Completing Train Step...
At time: 464.43533539772034 and batch: 50, loss is 5.843145055770874 and perplexity is 344.86224787339467
At time: 465.97763991355896 and batch: 100, loss is 5.780765228271484 and perplexity is 324.0070349196186
At time: 467.52235293388367 and batch: 150, loss is 5.780010461807251 and perplexity is 323.7625775412063
At time: 469.06897377967834 and batch: 200, loss is 5.816496829986573 and perplexity is 335.79364851829206
At time: 470.61709475517273 and batch: 250, loss is 5.835354270935059 and perplexity is 342.1859391119043
At time: 472.1613266468048 and batch: 300, loss is 5.8063743114471436 and perplexity is 332.411716796547
At time: 473.7060971260071 and batch: 350, loss is 5.789966487884522 and perplexity is 327.00206566136114
At time: 475.261602640152 and batch: 400, loss is 5.817589998245239 and perplexity is 336.16092818897766
At time: 476.80315232276917 and batch: 450, loss is 5.823951597213745 and perplexity is 338.3062658568081
At time: 478.34307074546814 and batch: 500, loss is 5.840616970062256 and perplexity is 343.9915076697939
At time: 479.8837306499481 and batch: 550, loss is 5.8101135349273685 and perplexity is 333.65700524805663
At time: 481.4248194694519 and batch: 600, loss is 5.818518924713135 and perplexity is 336.4733420549339
At time: 483.01158475875854 and batch: 650, loss is 5.830315055847168 and perplexity is 340.46592795753196
At time: 484.55693316459656 and batch: 700, loss is 5.793610506057739 and perplexity is 328.1958408796073
At time: 486.1078646183014 and batch: 750, loss is 5.797891597747803 and perplexity is 329.6038892081813
At time: 487.6527998447418 and batch: 800, loss is 5.806747016906738 and perplexity is 332.5356315486481
At time: 489.1967375278473 and batch: 850, loss is 5.81921311378479 and perplexity is 336.706999263649
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.5242970784505205 and perplexity of 250.71004657416688
Finished 17 epochs...
Completing Train Step...
At time: 493.24005913734436 and batch: 50, loss is 5.814531297683716 and perplexity is 335.134283468965
At time: 494.78150367736816 and batch: 100, loss is 5.754096403121948 and perplexity is 315.4803516317299
At time: 496.3325254917145 and batch: 150, loss is 5.753735523223877 and perplexity is 315.3665216553003
At time: 497.8820731639862 and batch: 200, loss is 5.786034145355225 and perplexity is 325.71870648906855
At time: 499.4317488670349 and batch: 250, loss is 5.807363109588623 and perplexity is 332.7405674409808
At time: 500.973806142807 and batch: 300, loss is 5.777736682891845 and perplexity is 323.02724932250374
At time: 502.51480054855347 and batch: 350, loss is 5.757867918014527 and perplexity is 316.67243704712314
At time: 504.06757140159607 and batch: 400, loss is 5.783458166122436 and perplexity is 324.88074161884515
At time: 505.6154818534851 and batch: 450, loss is 5.789993076324463 and perplexity is 327.01076025173194
At time: 507.1591281890869 and batch: 500, loss is 5.813497829437256 and perplexity is 334.7881117382564
At time: 508.7033061981201 and batch: 550, loss is 5.781304712295532 and perplexity is 324.18187869710874
At time: 510.25589966773987 and batch: 600, loss is 5.790355663299561 and perplexity is 327.1293515926546
At time: 511.79586148262024 and batch: 650, loss is 5.8058669471740725 and perplexity is 332.2431057447231
At time: 513.3362092971802 and batch: 700, loss is 5.777459268569946 and perplexity is 322.9376493659082
At time: 514.8768982887268 and batch: 750, loss is 5.783730344772339 and perplexity is 324.96917925535064
At time: 516.4250364303589 and batch: 800, loss is 5.794349956512451 and perplexity is 328.43861519210606
At time: 517.9635450839996 and batch: 850, loss is 5.806064586639405 and perplexity is 332.3087765838638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.520811716715495 and perplexity of 249.83775238467956
Finished 18 epochs...
Completing Train Step...
At time: 521.9605252742767 and batch: 50, loss is 5.804970216751099 and perplexity is 331.945306786627
At time: 523.5385329723358 and batch: 100, loss is 5.742125606536865 and perplexity is 311.72631475103276
At time: 525.0776193141937 and batch: 150, loss is 5.744084396362305 and perplexity is 312.33751950008536
At time: 526.6146137714386 and batch: 200, loss is 5.779903182983398 and perplexity is 323.7278465356642
At time: 528.1492974758148 and batch: 250, loss is 5.799314479827881 and perplexity is 330.0732104908991
At time: 529.686297416687 and batch: 300, loss is 5.769129877090454 and perplexity is 320.25894672940245
At time: 531.2231876850128 and batch: 350, loss is 5.751059417724609 and perplexity is 314.523695820833
At time: 532.7608399391174 and batch: 400, loss is 5.780235242843628 and perplexity is 323.8353614088363
At time: 534.3042314052582 and batch: 450, loss is 5.7876998233795165 and perplexity is 326.2617010821045
At time: 535.8515455722809 and batch: 500, loss is 5.804486904144287 and perplexity is 331.78491219857057
At time: 537.4002225399017 and batch: 550, loss is 5.777120838165283 and perplexity is 322.82837593832045
At time: 538.9414105415344 and batch: 600, loss is 5.785881614685058 and perplexity is 325.6690281853108
At time: 540.4812014102936 and batch: 650, loss is 5.8016633129119874 and perplexity is 330.8494085909077
At time: 542.0211071968079 and batch: 700, loss is 5.7705810546875 and perplexity is 320.72403672061756
At time: 543.5598773956299 and batch: 750, loss is 5.778960266113281 and perplexity is 323.4227419545729
At time: 545.0990476608276 and batch: 800, loss is 5.791060581207275 and perplexity is 327.3600322266338
At time: 546.6370997428894 and batch: 850, loss is 5.802094945907593 and perplexity is 330.9922449364542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.518758773803711 and perplexity of 249.32537586165498
Finished 19 epochs...
Completing Train Step...
At time: 550.6508650779724 and batch: 50, loss is 5.797415313720703 and perplexity is 329.44694151928684
At time: 552.2216084003448 and batch: 100, loss is 5.737985258102417 and perplexity is 310.4383273899456
At time: 553.7625994682312 and batch: 150, loss is 5.73864351272583 and perplexity is 310.64274212532604
At time: 555.3058054447174 and batch: 200, loss is 5.774538183212281 and perplexity is 321.99569735780165
At time: 556.8510813713074 and batch: 250, loss is 5.794716863632202 and perplexity is 328.5591437684656
At time: 558.3924050331116 and batch: 300, loss is 5.766267585754394 and perplexity is 319.34358296471237
At time: 559.9396266937256 and batch: 350, loss is 5.745436906814575 and perplexity is 312.760245065778
At time: 561.5076012611389 and batch: 400, loss is 5.774195613861084 and perplexity is 321.8854103922154
At time: 563.0476727485657 and batch: 450, loss is 5.781876535415649 and perplexity is 324.3673064013422
At time: 564.5903677940369 and batch: 500, loss is 5.803092374801635 and perplexity is 331.3225508662129
At time: 566.1337745189667 and batch: 550, loss is 5.7710532951354985 and perplexity is 320.8755313515332
At time: 567.6761600971222 and batch: 600, loss is 5.78007251739502 and perplexity is 323.7826694416541
At time: 569.2227811813354 and batch: 650, loss is 5.798142538070679 and perplexity is 329.6866104931792
At time: 570.7823905944824 and batch: 700, loss is 5.769815502166748 and perplexity is 320.47859958533604
At time: 572.3319089412689 and batch: 750, loss is 5.775359649658203 and perplexity is 322.260314691162
At time: 573.8830487728119 and batch: 800, loss is 5.785735788345337 and perplexity is 325.62154052553007
At time: 575.4235026836395 and batch: 850, loss is 5.797205371856689 and perplexity is 329.37778407406626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.51803716023763 and perplexity of 249.14552418757336
Finished 20 epochs...
Completing Train Step...
At time: 579.3890295028687 and batch: 50, loss is 5.7947500133514405 and perplexity is 328.5700355923641
At time: 580.9526054859161 and batch: 100, loss is 5.733178586959839 and perplexity is 308.9497328982735
At time: 582.4881165027618 and batch: 150, loss is 5.735031337738037 and perplexity is 309.5226703475846
At time: 584.0241169929504 and batch: 200, loss is 5.770750741958619 and perplexity is 320.7784641248791
At time: 585.5587959289551 and batch: 250, loss is 5.789197082519531 and perplexity is 326.75056528295556
At time: 587.0964980125427 and batch: 300, loss is 5.761796703338623 and perplexity is 317.9190222497078
At time: 588.6336462497711 and batch: 350, loss is 5.743463373184204 and perplexity is 312.14361087819157
At time: 590.171085357666 and batch: 400, loss is 5.77141713142395 and perplexity is 320.99229875473634
At time: 591.7081034183502 and batch: 450, loss is 5.778761377334595 and perplexity is 323.3584231967778
At time: 593.24706864357 and batch: 500, loss is 5.796826524734497 and perplexity is 329.25302388246763
At time: 594.7932965755463 and batch: 550, loss is 5.767571287155151 and perplexity is 319.76018314325864
At time: 596.3317074775696 and batch: 600, loss is 5.776102790832519 and perplexity is 322.4998886074525
At time: 597.8732569217682 and batch: 650, loss is 5.792954435348511 and perplexity is 327.98059181853864
At time: 599.4166035652161 and batch: 700, loss is 5.763230409622192 and perplexity is 318.37515164923474
At time: 600.9974203109741 and batch: 750, loss is 5.769573459625244 and perplexity is 320.40103951739144
At time: 602.551460981369 and batch: 800, loss is 5.781777257919312 and perplexity is 324.3351056257016
At time: 604.0913844108582 and batch: 850, loss is 5.793611707687378 and perplexity is 328.19623524969415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.51974614461263 and perplexity of 249.5716740335064
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 608.093377828598 and batch: 50, loss is 5.779413900375366 and perplexity is 323.56949087411334
At time: 609.6312036514282 and batch: 100, loss is 5.703281688690185 and perplexity is 299.8498018219041
At time: 611.1696655750275 and batch: 150, loss is 5.690391302108765 and perplexity is 296.0094270943877
At time: 612.7212660312653 and batch: 200, loss is 5.720918159484864 and perplexity is 305.18500286323837
At time: 614.2633728981018 and batch: 250, loss is 5.742655830383301 and perplexity is 311.8916433033725
At time: 615.813102722168 and batch: 300, loss is 5.716999168395996 and perplexity is 303.99132608863846
At time: 617.3528816699982 and batch: 350, loss is 5.679896945953369 and perplexity is 292.9192418538669
At time: 618.8935613632202 and batch: 400, loss is 5.706316823959351 and perplexity is 300.76126904431635
At time: 620.4345903396606 and batch: 450, loss is 5.723520669937134 and perplexity is 305.98028443849796
At time: 621.9736921787262 and batch: 500, loss is 5.734820184707641 and perplexity is 309.45732059740624
At time: 623.5133049488068 and batch: 550, loss is 5.698613195419312 and perplexity is 298.4532175484925
At time: 625.0523273944855 and batch: 600, loss is 5.700234489440918 and perplexity is 298.93749043412043
At time: 626.5922696590424 and batch: 650, loss is 5.715915861129761 and perplexity is 303.66218838703446
At time: 628.1297521591187 and batch: 700, loss is 5.678766870498658 and perplexity is 292.588407977265
At time: 629.6705098152161 and batch: 750, loss is 5.677024936676025 and perplexity is 292.07918198105574
At time: 631.2097342014313 and batch: 800, loss is 5.688621320724487 and perplexity is 295.4859593196298
At time: 632.7473347187042 and batch: 850, loss is 5.710719833374023 and perplexity is 302.08844337993077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.4403839111328125 and perplexity of 230.53066976469404
Finished 22 epochs...
Completing Train Step...
At time: 636.7525854110718 and batch: 50, loss is 5.711448440551758 and perplexity is 302.30862739217315
At time: 638.2983083724976 and batch: 100, loss is 5.652017259597779 and perplexity is 284.8655343398594
At time: 639.8634111881256 and batch: 150, loss is 5.643099708557129 and perplexity is 282.3365244445295
At time: 641.3999929428101 and batch: 200, loss is 5.681303300857544 and perplexity is 293.33148007485215
At time: 642.9353921413422 and batch: 250, loss is 5.703059387207031 and perplexity is 299.78315217466786
At time: 644.4739887714386 and batch: 300, loss is 5.672734928131104 and perplexity is 290.8288436931526
At time: 646.0118129253387 and batch: 350, loss is 5.647194747924805 and perplexity is 283.49507416140284
At time: 647.5499317646027 and batch: 400, loss is 5.6750987529754635 and perplexity is 291.51712530743004
At time: 649.0898025035858 and batch: 450, loss is 5.69254430770874 and perplexity is 296.64742360715064
At time: 650.6274282932281 and batch: 500, loss is 5.7061263275146485 and perplexity is 300.7039805486599
At time: 652.1654996871948 and batch: 550, loss is 5.684905395507813 and perplexity is 294.38999311754486
At time: 653.7032570838928 and batch: 600, loss is 5.6920513343811034 and perplexity is 296.50122037970687
At time: 655.2479693889618 and batch: 650, loss is 5.707194795608521 and perplexity is 301.0254448641761
At time: 656.7835092544556 and batch: 700, loss is 5.669538879394532 and perplexity is 289.9008243222326
At time: 658.320396900177 and batch: 750, loss is 5.673983268737793 and perplexity is 291.19212384984013
At time: 659.8581800460815 and batch: 800, loss is 5.687182807922364 and perplexity is 295.06120456512286
At time: 661.3949859142303 and batch: 850, loss is 5.7053032493591305 and perplexity is 300.4565795001593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.432877858479817 and perplexity of 228.80677234551789
Finished 23 epochs...
Completing Train Step...
At time: 665.4089503288269 and batch: 50, loss is 5.703270502090454 and perplexity is 299.84644754095325
At time: 666.9488301277161 and batch: 100, loss is 5.645285730361938 and perplexity is 282.95439333464213
At time: 668.4893264770508 and batch: 150, loss is 5.6374311351776125 and perplexity is 280.74060669984954
At time: 670.0298907756805 and batch: 200, loss is 5.675956573486328 and perplexity is 291.76730196477985
At time: 671.5758624076843 and batch: 250, loss is 5.695863618850708 and perplexity is 297.63372472012827
At time: 673.1325960159302 and batch: 300, loss is 5.664882726669312 and perplexity is 288.5541394272757
At time: 674.6828937530518 and batch: 350, loss is 5.6345570945739745 and perplexity is 279.934905161171
At time: 676.2306203842163 and batch: 400, loss is 5.657863416671753 and perplexity is 286.53578050220597
At time: 677.7708737850189 and batch: 450, loss is 5.675760936737061 and perplexity is 291.71022714142947
At time: 679.3376412391663 and batch: 500, loss is 5.6845433712005615 and perplexity is 294.2834360735086
At time: 680.8775894641876 and batch: 550, loss is 5.66388319015503 and perplexity is 288.26586312391737
At time: 682.4175379276276 and batch: 600, loss is 5.669840984344482 and perplexity is 289.9884180268347
At time: 683.9585847854614 and batch: 650, loss is 5.686161651611328 and perplexity is 294.7600547406302
At time: 685.4982483386993 and batch: 700, loss is 5.647559976577758 and perplexity is 283.598633595743
At time: 687.0421023368835 and batch: 750, loss is 5.649956817626953 and perplexity is 284.2791897088474
At time: 688.5962688922882 and batch: 800, loss is 5.659756240844726 and perplexity is 287.0786559757882
At time: 690.1499481201172 and batch: 850, loss is 5.676243362426757 and perplexity is 291.85098959993275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.4055124918619795 and perplexity of 222.63028746267477
Finished 24 epochs...
Completing Train Step...
At time: 694.1314423084259 and batch: 50, loss is 5.672055616378784 and perplexity is 290.6313473301132
At time: 695.6966876983643 and batch: 100, loss is 5.612572069168091 and perplexity is 273.84768815117843
At time: 697.2348306179047 and batch: 150, loss is 5.6066053676605225 and perplexity is 272.218585754514
At time: 698.7739946842194 and batch: 200, loss is 5.644557914733887 and perplexity is 282.74852962964565
At time: 700.3133335113525 and batch: 250, loss is 5.6666634654998775 and perplexity is 289.0684367667914
At time: 701.851057767868 and batch: 300, loss is 5.635935096740723 and perplexity is 280.3209219719488
At time: 703.3896977901459 and batch: 350, loss is 5.611280326843262 and perplexity is 273.4941758743779
At time: 704.9281694889069 and batch: 400, loss is 5.6371255874633786 and perplexity is 280.65484015273324
At time: 706.4673347473145 and batch: 450, loss is 5.655452795028687 and perplexity is 285.8458830232001
At time: 708.0061323642731 and batch: 500, loss is 5.667769784927368 and perplexity is 289.38841576111344
At time: 709.5448174476624 and batch: 550, loss is 5.648881244659424 and perplexity is 283.9735910734421
At time: 711.0829749107361 and batch: 600, loss is 5.653726482391358 and perplexity is 285.35284935058155
At time: 712.6313097476959 and batch: 650, loss is 5.669099273681641 and perplexity is 289.77341027171303
At time: 714.1708543300629 and batch: 700, loss is 5.633773164749146 and perplexity is 279.7155418339659
At time: 715.7230536937714 and batch: 750, loss is 5.63684609413147 and perplexity is 280.5764099572116
At time: 717.3052535057068 and batch: 800, loss is 5.6468284225463865 and perplexity is 283.3912417404676
At time: 718.856730222702 and batch: 850, loss is 5.6641607761383055 and perplexity is 288.34589279402087
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.398298263549805 and perplexity of 221.02996123984084
Finished 25 epochs...
Completing Train Step...
At time: 722.8577666282654 and batch: 50, loss is 5.659701080322265 and perplexity is 287.0628210038739
At time: 724.4346339702606 and batch: 100, loss is 5.599761056900024 and perplexity is 270.36179862126835
At time: 725.9783067703247 and batch: 150, loss is 5.595433778762818 and perplexity is 269.1943955814518
At time: 727.5181860923767 and batch: 200, loss is 5.63227840423584 and perplexity is 279.2977464169109
At time: 729.058445930481 and batch: 250, loss is 5.6560422134399415 and perplexity is 286.0144155126322
At time: 730.599728345871 and batch: 300, loss is 5.624349565505981 and perplexity is 277.09199570443417
At time: 732.139760017395 and batch: 350, loss is 5.601952743530274 and perplexity is 270.95499677541676
At time: 733.680757522583 and batch: 400, loss is 5.6263533782958985 and perplexity is 277.6477928601623
At time: 735.2216432094574 and batch: 450, loss is 5.643136196136474 and perplexity is 282.34682640881306
At time: 736.7600646018982 and batch: 500, loss is 5.657096757888794 and perplexity is 286.31618951588666
At time: 738.2996277809143 and batch: 550, loss is 5.6374690723419185 and perplexity is 280.75125740440075
At time: 739.838057756424 and batch: 600, loss is 5.643239917755127 and perplexity is 282.37611339749037
At time: 741.3804094791412 and batch: 650, loss is 5.658545312881469 and perplexity is 286.7312347970764
At time: 742.9206190109253 and batch: 700, loss is 5.623810863494873 and perplexity is 276.94276588789614
At time: 744.4592514038086 and batch: 750, loss is 5.626650218963623 and perplexity is 277.7302222499794
At time: 746.0037989616394 and batch: 800, loss is 5.635704030990601 and perplexity is 280.25615689062266
At time: 747.5460348129272 and batch: 850, loss is 5.651055002212525 and perplexity is 284.59155221743777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.388924916585286 and perplexity of 218.96785026242517
Finished 26 epochs...
Completing Train Step...
At time: 751.5503709316254 and batch: 50, loss is 5.649302759170532 and perplexity is 284.09331529382666
At time: 753.1180610656738 and batch: 100, loss is 5.587848300933838 and perplexity is 267.1601525688153
At time: 754.6598691940308 and batch: 150, loss is 5.583931732177734 and perplexity is 266.1158478429456
At time: 756.2298567295074 and batch: 200, loss is 5.622064762115478 and perplexity is 276.4596176779985
At time: 757.7747027873993 and batch: 250, loss is 5.645891914367676 and perplexity is 283.125967759817
At time: 759.3176693916321 and batch: 300, loss is 5.613079280853271 and perplexity is 273.98662213005855
At time: 760.8590483665466 and batch: 350, loss is 5.590640392303467 and perplexity is 267.9071304550937
At time: 762.401816368103 and batch: 400, loss is 5.614952659606933 and perplexity is 274.5003839317105
At time: 763.9441978931427 and batch: 450, loss is 5.629865217208862 and perplexity is 278.62456130607114
At time: 765.4868569374084 and batch: 500, loss is 5.642203779220581 and perplexity is 282.083684149932
At time: 767.0298717021942 and batch: 550, loss is 5.623470621109009 and perplexity is 276.84855424878805
At time: 768.5714762210846 and batch: 600, loss is 5.625156154632569 and perplexity is 277.31558525556215
At time: 770.1143565177917 and batch: 650, loss is 5.64003770828247 and perplexity is 281.47333215108375
At time: 771.6581020355225 and batch: 700, loss is 5.602586545944214 and perplexity is 271.1267831399979
At time: 773.2017924785614 and batch: 750, loss is 5.608419141769409 and perplexity is 272.712776817309
At time: 774.7457978725433 and batch: 800, loss is 5.616079730987549 and perplexity is 274.8099398714593
At time: 776.2953681945801 and batch: 850, loss is 5.629688425064087 and perplexity is 278.57530702630396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.369919459025065 and perplexity of 214.845563136366
Finished 27 epochs...
Completing Train Step...
At time: 780.325101852417 and batch: 50, loss is 5.6273539352416995 and perplexity is 277.9257343127327
At time: 781.8644120693207 and batch: 100, loss is 5.566771001815796 and perplexity is 261.5880666642761
At time: 783.4046084880829 and batch: 150, loss is 5.56180772781372 and perplexity is 260.29295008164723
At time: 784.9454576969147 and batch: 200, loss is 5.60071946144104 and perplexity is 270.6210388054255
At time: 786.4865353107452 and batch: 250, loss is 5.623146486282349 and perplexity is 276.75883253239755
At time: 788.0261342525482 and batch: 300, loss is 5.591423864364624 and perplexity is 268.1171104527799
At time: 789.565835237503 and batch: 350, loss is 5.570239362716674 and perplexity is 262.49692369783753
At time: 791.1074783802032 and batch: 400, loss is 5.5919046974182125 and perplexity is 268.2460610210815
At time: 792.6544530391693 and batch: 450, loss is 5.611162271499634 and perplexity is 273.4618903312427
At time: 794.1927897930145 and batch: 500, loss is 5.625424022674561 and perplexity is 277.38987918843935
At time: 795.7574048042297 and batch: 550, loss is 5.605737590789795 and perplexity is 271.9824632276653
At time: 797.2972185611725 and batch: 600, loss is 5.61193956375122 and perplexity is 273.6745327716559
At time: 798.8376393318176 and batch: 650, loss is 5.629298830032349 and perplexity is 278.4667966096538
At time: 800.3762123584747 and batch: 700, loss is 5.592589826583862 and perplexity is 268.4299071930534
At time: 801.9158186912537 and batch: 750, loss is 5.5979968929290775 and perplexity is 269.8852565497911
At time: 803.4538815021515 and batch: 800, loss is 5.605512838363648 and perplexity is 271.9213413780846
At time: 804.9927182197571 and batch: 850, loss is 5.620325298309326 and perplexity is 275.97914418346045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.36341667175293 and perplexity of 213.45300081836874
Finished 28 epochs...
Completing Train Step...
At time: 808.9807541370392 and batch: 50, loss is 5.61734130859375 and perplexity is 275.1568527202918
At time: 810.51837849617 and batch: 100, loss is 5.556775035858155 and perplexity is 258.98626667119566
At time: 812.0558612346649 and batch: 150, loss is 5.553600769042969 and perplexity is 258.1654785487484
At time: 813.5940845012665 and batch: 200, loss is 5.5912954616546635 and perplexity is 268.0826856993741
At time: 815.1327040195465 and batch: 250, loss is 5.615790195465088 and perplexity is 274.73038414960075
At time: 816.669855594635 and batch: 300, loss is 5.58429838180542 and perplexity is 266.2134370089009
At time: 818.2082591056824 and batch: 350, loss is 5.56181004524231 and perplexity is 260.2935532926703
At time: 819.7461986541748 and batch: 400, loss is 5.583754692077637 and perplexity is 266.0687388368206
At time: 821.2884719371796 and batch: 450, loss is 5.602330799102783 and perplexity is 271.0574521875465
At time: 822.8262796401978 and batch: 500, loss is 5.617331924438477 and perplexity is 275.15427061777683
At time: 824.3652181625366 and batch: 550, loss is 5.598423862457276 and perplexity is 270.0005139343924
At time: 825.9055314064026 and batch: 600, loss is 5.605875768661499 and perplexity is 272.0200477821919
At time: 827.4480812549591 and batch: 650, loss is 5.623934669494629 and perplexity is 276.97705518646876
At time: 828.9886178970337 and batch: 700, loss is 5.5880705261230466 and perplexity is 267.2195288814807
At time: 830.528326511383 and batch: 750, loss is 5.590678100585937 and perplexity is 267.9172329633177
At time: 832.0663077831268 and batch: 800, loss is 5.598761987686157 and perplexity is 270.0918233561042
At time: 833.606050491333 and batch: 850, loss is 5.612042827606201 and perplexity is 273.7027949181239
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.357204437255859 and perplexity of 212.131090981371
Finished 29 epochs...
Completing Train Step...
At time: 837.6670660972595 and batch: 50, loss is 5.607090272903442 and perplexity is 272.35061798293935
At time: 839.2073147296906 and batch: 100, loss is 5.548462162017822 and perplexity is 256.84227024241613
At time: 840.7473888397217 and batch: 150, loss is 5.546321868896484 and perplexity is 256.2931403572292
At time: 842.2873950004578 and batch: 200, loss is 5.583459453582764 and perplexity is 265.99019669773566
At time: 843.8277842998505 and batch: 250, loss is 5.606046981811524 and perplexity is 272.06662517866016
At time: 845.3679518699646 and batch: 300, loss is 5.575455856323242 and perplexity is 263.86981494802956
At time: 846.9057986736298 and batch: 350, loss is 5.55293475151062 and perplexity is 257.9935930595162
At time: 848.4472055435181 and batch: 400, loss is 5.574134511947632 and perplexity is 263.5213823026083
At time: 850.0020203590393 and batch: 450, loss is 5.592466592788696 and perplexity is 268.39682959503483
At time: 851.5557527542114 and batch: 500, loss is 5.606636571884155 and perplexity is 272.22708025667225
At time: 853.1071758270264 and batch: 550, loss is 5.588209600448608 and perplexity is 267.2566948415923
At time: 854.6592447757721 and batch: 600, loss is 5.5960500621795655 and perplexity is 269.3603467545556
At time: 856.2117028236389 and batch: 650, loss is 5.615068340301514 and perplexity is 274.53214016354246
At time: 857.7626729011536 and batch: 700, loss is 5.580034894943237 and perplexity is 265.08085560624966
At time: 859.3155198097229 and batch: 750, loss is 5.583081874847412 and perplexity is 265.88978341380397
At time: 860.8688931465149 and batch: 800, loss is 5.590392265319824 and perplexity is 267.8406637133547
At time: 862.4210886955261 and batch: 850, loss is 5.603823585510254 and perplexity is 271.4623852318915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.351545333862305 and perplexity of 210.93400960291967
Finished 30 epochs...
Completing Train Step...
At time: 866.4098510742188 and batch: 50, loss is 5.599404458999634 and perplexity is 270.26540535937306
At time: 867.9780042171478 and batch: 100, loss is 5.541391429901123 and perplexity is 255.0326126946762
At time: 869.5194082260132 and batch: 150, loss is 5.53871868133545 and perplexity is 254.35188475744204
At time: 871.0616397857666 and batch: 200, loss is 5.576147222518921 and perplexity is 264.0523086958797
At time: 872.6036992073059 and batch: 250, loss is 5.5987270736694335 and perplexity is 270.0823935302841
At time: 874.1718726158142 and batch: 300, loss is 5.567889223098755 and perplexity is 261.88074361608903
At time: 875.7125363349915 and batch: 350, loss is 5.546292896270752 and perplexity is 256.2857149795627
At time: 877.2545475959778 and batch: 400, loss is 5.566798315048218 and perplexity is 261.5952115775147
At time: 878.7988994121552 and batch: 450, loss is 5.585144052505493 and perplexity is 266.43866113184333
At time: 880.3440825939178 and batch: 500, loss is 5.602812204360962 and perplexity is 271.18797208425974
At time: 881.8832139968872 and batch: 550, loss is 5.583326845169068 and perplexity is 265.95492649830715
At time: 883.4236574172974 and batch: 600, loss is 5.592453594207764 and perplexity is 268.3933408397978
At time: 884.9641597270966 and batch: 650, loss is 5.609822225570679 and perplexity is 273.0956842596582
At time: 886.503520488739 and batch: 700, loss is 5.575663166046143 and perplexity is 263.92452339684894
At time: 888.0516951084137 and batch: 750, loss is 5.576558256149292 and perplexity is 264.16086538362714
At time: 889.599515914917 and batch: 800, loss is 5.582783107757568 and perplexity is 265.8103561627093
At time: 891.1390748023987 and batch: 850, loss is 5.598381032943726 and perplexity is 269.98895019135887
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.348934173583984 and perplexity of 210.38394556092146
Finished 31 epochs...
Completing Train Step...
At time: 895.1293771266937 and batch: 50, loss is 5.5919505214691165 and perplexity is 268.25835342387825
At time: 896.6951911449432 and batch: 100, loss is 5.534568433761597 and perplexity is 253.29844898652848
At time: 898.234046459198 and batch: 150, loss is 5.533651809692383 and perplexity is 253.06637590962603
At time: 899.7731535434723 and batch: 200, loss is 5.570452032089233 and perplexity is 262.5527546904596
At time: 901.3108358383179 and batch: 250, loss is 5.59293646812439 and perplexity is 268.522972278787
At time: 902.8492567539215 and batch: 300, loss is 5.562311296463013 and perplexity is 260.4240584592039
At time: 904.3864331245422 and batch: 350, loss is 5.539145793914795 and perplexity is 254.4605448503965
At time: 905.9261832237244 and batch: 400, loss is 5.560175266265869 and perplexity is 259.86837849214703
At time: 907.4652881622314 and batch: 450, loss is 5.579675769805908 and perplexity is 264.98567549938406
At time: 909.0047345161438 and batch: 500, loss is 5.596521291732788 and perplexity is 269.48730722182546
At time: 910.5428574085236 and batch: 550, loss is 5.578408946990967 and perplexity is 264.6501981400809
At time: 912.1103684902191 and batch: 600, loss is 5.587740631103515 and perplexity is 267.1313890290298
At time: 913.6643607616425 and batch: 650, loss is 5.605589914321899 and perplexity is 271.9423007837631
At time: 915.2102663516998 and batch: 700, loss is 5.570927600860596 and perplexity is 262.6776462763409
At time: 916.7509155273438 and batch: 750, loss is 5.572211132049561 and perplexity is 263.01501769503255
At time: 918.2928113937378 and batch: 800, loss is 5.578652219772339 and perplexity is 264.7145881617253
At time: 919.8315942287445 and batch: 850, loss is 5.593591833114624 and perplexity is 268.69901051223906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.346925735473633 and perplexity of 209.96182646878282
Finished 32 epochs...
Completing Train Step...
At time: 923.8146741390228 and batch: 50, loss is 5.586423873901367 and perplexity is 266.7798733298099
At time: 925.3814215660095 and batch: 100, loss is 5.529283809661865 and perplexity is 251.96339264109488
At time: 926.9202210903168 and batch: 150, loss is 5.529629030227661 and perplexity is 252.050390601941
At time: 928.4600656032562 and batch: 200, loss is 5.566805019378662 and perplexity is 261.5969654041347
At time: 930.0066001415253 and batch: 250, loss is 5.587849712371826 and perplexity is 267.1605296490696
At time: 931.5464432239532 and batch: 300, loss is 5.557905492782592 and perplexity is 259.2792050355933
At time: 933.0858969688416 and batch: 350, loss is 5.535382308959961 and perplexity is 253.5046862262458
At time: 934.6256120204926 and batch: 400, loss is 5.556337041854858 and perplexity is 258.8728570776315
At time: 936.166384935379 and batch: 450, loss is 5.5759196281433105 and perplexity is 263.9922187138838
At time: 937.704416513443 and batch: 500, loss is 5.590260410308838 and perplexity is 267.8053499079005
At time: 939.244012594223 and batch: 550, loss is 5.572474699020386 and perplexity is 263.0843489028349
At time: 940.7838532924652 and batch: 600, loss is 5.584901256561279 and perplexity is 266.37397875816555
At time: 942.32470870018 and batch: 650, loss is 5.602746000289917 and perplexity is 271.17001893078185
At time: 943.8636445999146 and batch: 700, loss is 5.567953424453735 and perplexity is 261.89755725439596
At time: 945.4023158550262 and batch: 750, loss is 5.569844541549682 and perplexity is 262.3933048129019
At time: 946.9417507648468 and batch: 800, loss is 5.573317613601684 and perplexity is 263.3062000242735
At time: 948.4806270599365 and batch: 850, loss is 5.589167680740356 and perplexity is 267.5128709127846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.344671885172526 and perplexity of 209.48913682891785
Finished 33 epochs...
Completing Train Step...
At time: 952.5756893157959 and batch: 50, loss is 5.582291679382324 and perplexity is 265.67976150284625
At time: 954.1242702007294 and batch: 100, loss is 5.526376686096191 and perplexity is 251.23196761125965
At time: 955.6710834503174 and batch: 150, loss is 5.5256947326660155 and perplexity is 251.06069751492976
At time: 957.22012591362 and batch: 200, loss is 5.561478996276856 and perplexity is 260.20739764279466
At time: 958.7614605426788 and batch: 250, loss is 5.584770975112915 and perplexity is 266.339277430925
At time: 960.3022146224976 and batch: 300, loss is 5.555711622238159 and perplexity is 258.7110035330509
At time: 961.8421447277069 and batch: 350, loss is 5.532881593704223 and perplexity is 252.87153518518747
At time: 963.3827495574951 and batch: 400, loss is 5.551456117630005 and perplexity is 257.612396886078
At time: 964.9250774383545 and batch: 450, loss is 5.572200059890747 and perplexity is 263.01210556710805
At time: 966.4671759605408 and batch: 500, loss is 5.587037773132324 and perplexity is 266.94369957013816
At time: 968.0091128349304 and batch: 550, loss is 5.570284404754639 and perplexity is 262.5087473605192
At time: 969.5518596172333 and batch: 600, loss is 5.582035055160523 and perplexity is 265.61159038835586
At time: 971.0959339141846 and batch: 650, loss is 5.600301961898804 and perplexity is 270.5080782277404
At time: 972.638589143753 and batch: 700, loss is 5.564560737609863 and perplexity is 261.01052641721657
At time: 974.1814029216766 and batch: 750, loss is 5.567088928222656 and perplexity is 261.671245640084
At time: 975.7248775959015 and batch: 800, loss is 5.569820966720581 and perplexity is 262.38711900849853
At time: 977.2669007778168 and batch: 850, loss is 5.586070346832275 and perplexity is 266.68557609239394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.342507044474284 and perplexity of 209.03611675474238
Finished 34 epochs...
Completing Train Step...
At time: 981.326929807663 and batch: 50, loss is 5.578451948165894 and perplexity is 264.66157865423145
At time: 982.8704061508179 and batch: 100, loss is 5.5224715423583985 and perplexity is 250.25278383729182
At time: 984.4147458076477 and batch: 150, loss is 5.523200521469116 and perplexity is 250.43527939875247
At time: 985.9587013721466 and batch: 200, loss is 5.559939184188843 and perplexity is 259.8070354668785
At time: 987.5017085075378 and batch: 250, loss is 5.58164116859436 and perplexity is 265.506990152751
At time: 989.0452597141266 and batch: 300, loss is 5.551048412322998 and perplexity is 257.50738835243936
At time: 990.6254434585571 and batch: 350, loss is 5.52696662902832 and perplexity is 251.38022386194044
At time: 992.1651446819305 and batch: 400, loss is 5.548091030120849 and perplexity is 256.7469655698342
At time: 993.7070350646973 and batch: 450, loss is 5.568742618560791 and perplexity is 262.10432684317215
At time: 995.2479383945465 and batch: 500, loss is 5.583938055038452 and perplexity is 266.11753046170577
At time: 996.7878959178925 and batch: 550, loss is 5.566793355941773 and perplexity is 261.5939143022315
At time: 998.329488992691 and batch: 600, loss is 5.578910598754883 and perplexity is 264.78299368457317
At time: 999.878045797348 and batch: 650, loss is 5.597710800170899 and perplexity is 269.80805537622786
At time: 1001.4255676269531 and batch: 700, loss is 5.5623985958099365 and perplexity is 260.4467943018283
At time: 1002.9671325683594 and batch: 750, loss is 5.563355197906494 and perplexity is 260.6960574551186
At time: 1004.5086896419525 and batch: 800, loss is 5.567691812515259 and perplexity is 261.8290506882188
At time: 1006.0493137836456 and batch: 850, loss is 5.583775520324707 and perplexity is 266.0742806399636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.341541290283203 and perplexity of 208.8343366995492
Finished 35 epochs...
Completing Train Step...
At time: 1010.0938744544983 and batch: 50, loss is 5.576533508300781 and perplexity is 264.1543280514409
At time: 1011.6344697475433 and batch: 100, loss is 5.520581665039063 and perplexity is 249.78028340157883
At time: 1013.176557302475 and batch: 150, loss is 5.521534337997436 and perplexity is 250.01835570762847
At time: 1014.7200500965118 and batch: 200, loss is 5.556544675827026 and perplexity is 258.92661345786524
At time: 1016.2613592147827 and batch: 250, loss is 5.577492065429688 and perplexity is 264.4076564612055
At time: 1017.8021445274353 and batch: 300, loss is 5.548106660842896 and perplexity is 256.75097874165357
At time: 1019.3413999080658 and batch: 350, loss is 5.524630813598633 and perplexity is 250.79373129218277
At time: 1020.8837728500366 and batch: 400, loss is 5.544518785476685 and perplexity is 255.8314388135287
At time: 1022.4264996051788 and batch: 450, loss is 5.564944896697998 and perplexity is 261.1108152452377
At time: 1023.9679141044617 and batch: 500, loss is 5.579982481002808 and perplexity is 265.0669620381864
At time: 1025.508737564087 and batch: 550, loss is 5.562728414535522 and perplexity is 260.5327086989175
At time: 1027.0491335391998 and batch: 600, loss is 5.57558391571045 and perplexity is 263.9036081185545
At time: 1028.5898082256317 and batch: 650, loss is 5.595647029876709 and perplexity is 269.2518077075451
At time: 1030.1567976474762 and batch: 700, loss is 5.560489435195922 and perplexity is 259.95003388869526
At time: 1031.6979005336761 and batch: 750, loss is 5.563012571334839 and perplexity is 260.606751358903
At time: 1033.2381563186646 and batch: 800, loss is 5.5662337589263915 and perplexity is 261.4475680798198
At time: 1034.7885110378265 and batch: 850, loss is 5.580074625015259 and perplexity is 265.09138749694944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.341280619303386 and perplexity of 208.7799067428455
Finished 36 epochs...
Completing Train Step...
At time: 1039.0484068393707 and batch: 50, loss is 5.574179239273072 and perplexity is 263.53316917283047
At time: 1040.6350004673004 and batch: 100, loss is 5.5179185962677 and perplexity is 249.11598625623722
At time: 1042.1758563518524 and batch: 150, loss is 5.5179823207855225 and perplexity is 249.1318615581609
At time: 1043.7179036140442 and batch: 200, loss is 5.554406385421753 and perplexity is 258.37354468619486
At time: 1045.2617654800415 and batch: 250, loss is 5.575700149536133 and perplexity is 263.9342844273156
At time: 1046.8064198493958 and batch: 300, loss is 5.544253787994385 and perplexity is 255.76365310826725
At time: 1048.345647573471 and batch: 350, loss is 5.521984539031982 and perplexity is 250.13093957080787
At time: 1049.8855156898499 and batch: 400, loss is 5.543439426422119 and perplexity is 255.5554538038563
At time: 1051.4257254600525 and batch: 450, loss is 5.56360671043396 and perplexity is 260.7616340257482
At time: 1052.966549873352 and batch: 500, loss is 5.577886905670166 and perplexity is 264.51207585699996
At time: 1054.5064368247986 and batch: 550, loss is 5.560909576416016 and perplexity is 260.0592725593239
At time: 1056.0536077022552 and batch: 600, loss is 5.57357195854187 and perplexity is 263.3731791415079
At time: 1057.5967860221863 and batch: 650, loss is 5.593433485031128 and perplexity is 268.65646590740437
At time: 1059.1368060112 and batch: 700, loss is 5.559020404815674 and perplexity is 259.5684397468847
At time: 1060.6836409568787 and batch: 750, loss is 5.559961709976196 and perplexity is 259.81288789082737
At time: 1062.2263803482056 and batch: 800, loss is 5.563487224578857 and perplexity is 260.73047856028444
At time: 1063.7692618370056 and batch: 850, loss is 5.577222137451172 and perplexity is 264.33629506864435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.340834935506185 and perplexity of 208.6868776535478
Finished 37 epochs...
Completing Train Step...
At time: 1067.7804431915283 and batch: 50, loss is 5.571310892105102 and perplexity is 262.7783476160242
At time: 1069.3487486839294 and batch: 100, loss is 5.51528000831604 and perplexity is 248.4595382450994
At time: 1070.890604019165 and batch: 150, loss is 5.5153076171875 and perplexity is 248.46639802724883
At time: 1072.4370334148407 and batch: 200, loss is 5.551597929000854 and perplexity is 257.64893184370305
At time: 1073.9833567142487 and batch: 250, loss is 5.573486795425415 and perplexity is 263.3507504158452
At time: 1075.531753540039 and batch: 300, loss is 5.5420573043823245 and perplexity is 255.20248895520047
At time: 1077.0734779834747 and batch: 350, loss is 5.520065031051636 and perplexity is 249.6512717465568
At time: 1078.6165976524353 and batch: 400, loss is 5.540089168548584 and perplexity is 254.70070973862894
At time: 1080.1601192951202 and batch: 450, loss is 5.559588050842285 and perplexity is 259.715824567587
At time: 1081.7041716575623 and batch: 500, loss is 5.574211835861206 and perplexity is 263.54175959501424
At time: 1083.2464435100555 and batch: 550, loss is 5.559694633483887 and perplexity is 259.74350724145535
At time: 1084.7884767055511 and batch: 600, loss is 5.572310419082641 and perplexity is 263.0411329722279
At time: 1086.3311445713043 and batch: 650, loss is 5.5908481407165525 and perplexity is 267.9627935180564
At time: 1087.8736469745636 and batch: 700, loss is 5.556490077972412 and perplexity is 258.91247700618135
At time: 1089.4156396389008 and batch: 750, loss is 5.556338920593261 and perplexity is 258.8733434324665
At time: 1090.957869052887 and batch: 800, loss is 5.560631589889526 and perplexity is 259.9869896327691
At time: 1092.5002689361572 and batch: 850, loss is 5.575539808273316 and perplexity is 263.89196826345403
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.339151382446289 and perplexity of 208.33583780213633
Finished 38 epochs...
Completing Train Step...
At time: 1096.532457113266 and batch: 50, loss is 5.568183145523071 and perplexity is 261.9577275522329
At time: 1098.0730612277985 and batch: 100, loss is 5.512492399215699 and perplexity is 247.7678946391091
At time: 1099.6143724918365 and batch: 150, loss is 5.5113003540039065 and perplexity is 247.4727200723361
At time: 1101.158637046814 and batch: 200, loss is 5.548552837371826 and perplexity is 256.86556056210657
At time: 1102.699316740036 and batch: 250, loss is 5.571187582015991 and perplexity is 262.7459463923031
At time: 1104.2387609481812 and batch: 300, loss is 5.5410436916351316 and perplexity is 254.94394351389366
At time: 1105.7770030498505 and batch: 350, loss is 5.515967311859131 and perplexity is 248.63036406392067
At time: 1107.3155937194824 and batch: 400, loss is 5.5363232517242436 and perplexity is 253.7433318848074
At time: 1108.8974604606628 and batch: 450, loss is 5.55613676071167 and perplexity is 258.8210149175525
At time: 1110.435468196869 and batch: 500, loss is 5.571283645629883 and perplexity is 262.7711879298263
At time: 1111.9737462997437 and batch: 550, loss is 5.556545171737671 and perplexity is 258.9267418623609
At time: 1113.5224413871765 and batch: 600, loss is 5.568297119140625 and perplexity is 261.9875855235663
At time: 1115.0758047103882 and batch: 650, loss is 5.587574691772461 and perplexity is 267.08706510267365
At time: 1116.6278455257416 and batch: 700, loss is 5.553827333450317 and perplexity is 258.2239762838957
At time: 1118.1865139007568 and batch: 750, loss is 5.554231586456299 and perplexity is 258.3283852049133
At time: 1119.741842508316 and batch: 800, loss is 5.558797369003296 and perplexity is 259.5105531446907
At time: 1121.295122385025 and batch: 850, loss is 5.572116184234619 and perplexity is 262.99004617931973
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.33803113301595 and perplexity of 208.10258037616262
Finished 39 epochs...
Completing Train Step...
At time: 1125.3375432491302 and batch: 50, loss is 5.566466493606567 and perplexity is 261.50842307721905
At time: 1126.8904776573181 and batch: 100, loss is 5.509012851715088 and perplexity is 246.90727263646886
At time: 1128.4440231323242 and batch: 150, loss is 5.508238286972046 and perplexity is 246.71610101524053
At time: 1129.9961585998535 and batch: 200, loss is 5.5466565418243405 and perplexity is 256.37892908768407
At time: 1131.5480470657349 and batch: 250, loss is 5.567122735977173 and perplexity is 261.6800923068629
At time: 1133.101050376892 and batch: 300, loss is 5.535552558898925 and perplexity is 253.54784905773346
At time: 1134.653835773468 and batch: 350, loss is 5.511533203125 and perplexity is 247.53035058704663
At time: 1136.2229919433594 and batch: 400, loss is 5.531247358322144 and perplexity is 252.45862106659663
At time: 1137.7773354053497 and batch: 450, loss is 5.550823478698731 and perplexity is 257.4494727961239
At time: 1139.3299882411957 and batch: 500, loss is 5.566749601364136 and perplexity is 261.58246862140123
At time: 1140.8828303813934 and batch: 550, loss is 5.553758592605591 and perplexity is 258.20622635971665
At time: 1142.437968492508 and batch: 600, loss is 5.567059841156006 and perplexity is 261.663634501815
At time: 1143.9989156723022 and batch: 650, loss is 5.5852015018463135 and perplexity is 266.45396829698353
At time: 1145.5528664588928 and batch: 700, loss is 5.551425428390503 and perplexity is 257.60449107884335
At time: 1147.1059873104095 and batch: 750, loss is 5.552059364318848 and perplexity is 257.7678475943527
At time: 1148.6861324310303 and batch: 800, loss is 5.556114320755005 and perplexity is 258.815207050358
At time: 1150.239102602005 and batch: 850, loss is 5.570071077346801 and perplexity is 262.452753022686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.33851432800293 and perplexity of 208.20315879731544
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 1154.3004984855652 and batch: 50, loss is 5.558311309814453 and perplexity is 259.3844463059097
At time: 1155.8576922416687 and batch: 100, loss is 5.497249345779419 and perplexity is 244.01979420148385
At time: 1157.4144649505615 and batch: 150, loss is 5.489796123504639 and perplexity is 242.20782134208787
At time: 1158.971736907959 and batch: 200, loss is 5.525622997283936 and perplexity is 251.04268822582762
At time: 1160.5285584926605 and batch: 250, loss is 5.543560094833374 and perplexity is 255.5862931350835
At time: 1162.0861551761627 and batch: 300, loss is 5.503061304092407 and perplexity is 245.44215642408633
At time: 1163.6432485580444 and batch: 350, loss is 5.476008787155151 and perplexity is 238.89133592861884
At time: 1165.200870513916 and batch: 400, loss is 5.49871792793274 and perplexity is 244.37842058800535
At time: 1166.7583675384521 and batch: 450, loss is 5.519677038192749 and perplexity is 249.55442762453598
At time: 1168.3152678012848 and batch: 500, loss is 5.530701360702515 and perplexity is 252.32081688424057
At time: 1169.8725745677948 and batch: 550, loss is 5.507019691467285 and perplexity is 246.41563699282656
At time: 1171.4297754764557 and batch: 600, loss is 5.523454799652099 and perplexity is 250.4989677234855
At time: 1172.9870436191559 and batch: 650, loss is 5.538578033447266 and perplexity is 254.31611321764998
At time: 1174.5438351631165 and batch: 700, loss is 5.504122896194458 and perplexity is 245.70285423173993
At time: 1176.1004333496094 and batch: 750, loss is 5.504957809448242 and perplexity is 245.90808046235895
At time: 1177.6571912765503 and batch: 800, loss is 5.507094507217407 and perplexity is 246.43407345321023
At time: 1179.2146048545837 and batch: 850, loss is 5.527415657043457 and perplexity is 251.49312597111398
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.317857106526692 and perplexity of 203.9463780358011
Finished 41 epochs...
Completing Train Step...
At time: 1183.236899137497 and batch: 50, loss is 5.532912826538086 and perplexity is 252.8794332031725
At time: 1184.8174681663513 and batch: 100, loss is 5.4753208827972415 and perplexity is 238.72705804778
At time: 1186.372332572937 and batch: 150, loss is 5.471223134994506 and perplexity is 237.7508163324328
At time: 1187.9513819217682 and batch: 200, loss is 5.513001956939697 and perplexity is 247.89417885539777
At time: 1189.5047099590302 and batch: 250, loss is 5.530521621704102 and perplexity is 252.27546906884265
At time: 1191.0590860843658 and batch: 300, loss is 5.490605945587158 and perplexity is 242.40404602718735
At time: 1192.6176280975342 and batch: 350, loss is 5.466555547714234 and perplexity is 236.6436794852183
At time: 1194.176552772522 and batch: 400, loss is 5.48890100479126 and perplexity is 241.99111359272834
At time: 1195.7345888614655 and batch: 450, loss is 5.509670057296753 and perplexity is 247.06959480787523
At time: 1197.2923662662506 and batch: 500, loss is 5.522815847396851 and perplexity is 250.33896196658446
At time: 1198.8471162319183 and batch: 550, loss is 5.504022750854492 and perplexity is 245.67824946791916
At time: 1200.4030039310455 and batch: 600, loss is 5.521331129074096 and perplexity is 249.96755490851257
At time: 1201.9675908088684 and batch: 650, loss is 5.536798906326294 and perplexity is 253.86405477728164
At time: 1203.5236792564392 and batch: 700, loss is 5.504196548461914 and perplexity is 245.72095147051778
At time: 1205.079255104065 and batch: 750, loss is 5.504579353332519 and perplexity is 245.81503265376907
At time: 1206.635159254074 and batch: 800, loss is 5.505683794021606 and perplexity is 246.0866707542948
At time: 1208.19007730484 and batch: 850, loss is 5.525546808242797 and perplexity is 251.02356225273095
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.316203753153483 and perplexity of 203.60946120176288
Finished 42 epochs...
Completing Train Step...
At time: 1212.2033696174622 and batch: 50, loss is 5.5251023387908935 and perplexity is 250.912014739169
At time: 1213.785840511322 and batch: 100, loss is 5.466281585693359 and perplexity is 236.57885698441646
At time: 1215.3413336277008 and batch: 150, loss is 5.466297283172607 and perplexity is 236.58257070526244
At time: 1216.89511013031 and batch: 200, loss is 5.507649412155152 and perplexity is 246.57085888534385
At time: 1218.4494168758392 and batch: 250, loss is 5.52545428276062 and perplexity is 251.00033725106445
At time: 1220.0042912960052 and batch: 300, loss is 5.485028676986694 and perplexity is 241.05585665450363
At time: 1221.5588541030884 and batch: 350, loss is 5.461841888427735 and perplexity is 235.5308466257561
At time: 1223.113251209259 and batch: 400, loss is 5.483111066818237 and perplexity is 240.59404841821632
At time: 1224.6673347949982 and batch: 450, loss is 5.504734907150269 and perplexity is 245.85327309470475
At time: 1226.265429019928 and batch: 500, loss is 5.51715573310852 and perplexity is 248.92601731732495
At time: 1227.8183574676514 and batch: 550, loss is 5.501329345703125 and perplexity is 245.017428733698
At time: 1229.3719866275787 and batch: 600, loss is 5.519653396606445 and perplexity is 249.5485278317383
At time: 1230.9268157482147 and batch: 650, loss is 5.534595670700074 and perplexity is 253.30534815475554
At time: 1232.4813215732574 and batch: 700, loss is 5.503324785232544 and perplexity is 245.50683432362783
At time: 1234.0359020233154 and batch: 750, loss is 5.50362045288086 and perplexity is 245.57943348406036
At time: 1235.5895447731018 and batch: 800, loss is 5.50363392829895 and perplexity is 245.58274279189814
At time: 1237.1448457241058 and batch: 850, loss is 5.5242533779144285 and perplexity is 250.69909065011956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.313620249430339 and perplexity of 203.08411431070706
Finished 43 epochs...
Completing Train Step...
At time: 1241.170940876007 and batch: 50, loss is 5.520983610153198 and perplexity is 249.88070154603878
At time: 1242.7517750263214 and batch: 100, loss is 5.460334529876709 and perplexity is 235.17608463399756
At time: 1244.3054094314575 and batch: 150, loss is 5.4628159713745115 and perplexity is 235.76038498344454
At time: 1245.8594920635223 and batch: 200, loss is 5.504472303390503 and perplexity is 245.78871957720847
At time: 1247.420622587204 and batch: 250, loss is 5.520711660385132 and perplexity is 249.81275578654453
At time: 1248.9750034809113 and batch: 300, loss is 5.481045074462891 and perplexity is 240.09749606674407
At time: 1250.5288846492767 and batch: 350, loss is 5.458225622177124 and perplexity is 234.68064258275567
At time: 1252.0851697921753 and batch: 400, loss is 5.478119430541992 and perplexity is 239.3960828299453
At time: 1253.6478734016418 and batch: 450, loss is 5.50161771774292 and perplexity is 245.08809509801932
At time: 1255.2080364227295 and batch: 500, loss is 5.514328584671021 and perplexity is 248.22326038357792
At time: 1256.7624428272247 and batch: 550, loss is 5.498906383514404 and perplexity is 244.4244794052878
At time: 1258.3163414001465 and batch: 600, loss is 5.51788890838623 and perplexity is 249.10859064014568
At time: 1259.87646484375 and batch: 650, loss is 5.532141675949097 and perplexity is 252.68450025030558
At time: 1261.4292075634003 and batch: 700, loss is 5.502246999740601 and perplexity is 245.24237316122327
At time: 1262.9838902950287 and batch: 750, loss is 5.5024572849273685 and perplexity is 245.29394942213858
At time: 1264.537079334259 and batch: 800, loss is 5.5016192531585695 and perplexity is 245.08847141040502
At time: 1266.1173677444458 and batch: 850, loss is 5.522521142959595 and perplexity is 250.26519683366462
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.31256898244222 and perplexity of 202.87073086665805
Finished 44 epochs...
Completing Train Step...
At time: 1270.1609642505646 and batch: 50, loss is 5.515933256149292 and perplexity is 248.62189692456303
At time: 1271.7184145450592 and batch: 100, loss is 5.45505277633667 and perplexity is 233.93721709300723
At time: 1273.2753071784973 and batch: 150, loss is 5.4598767375946045 and perplexity is 235.06844747712617
At time: 1274.8350899219513 and batch: 200, loss is 5.50174111366272 and perplexity is 245.11833983494625
At time: 1276.391351222992 and batch: 250, loss is 5.515146503448486 and perplexity is 248.4263699014704
At time: 1277.9477503299713 and batch: 300, loss is 5.476361408233642 and perplexity is 238.97558890295053
At time: 1279.5044691562653 and batch: 350, loss is 5.455147180557251 and perplexity is 233.95930279612725
At time: 1281.0610539913177 and batch: 400, loss is 5.472558422088623 and perplexity is 238.06849397732546
At time: 1282.6177849769592 and batch: 450, loss is 5.497071876525879 and perplexity is 243.9764920332731
At time: 1284.1743783950806 and batch: 500, loss is 5.510295000076294 and perplexity is 247.22404742419133
At time: 1285.7306406497955 and batch: 550, loss is 5.495138273239136 and perplexity is 243.50519408486227
At time: 1287.2874705791473 and batch: 600, loss is 5.515773906707763 and perplexity is 248.58228232050254
At time: 1288.8444697856903 and batch: 650, loss is 5.529330930709839 and perplexity is 251.97526569994045
At time: 1290.4019706249237 and batch: 700, loss is 5.499801425933838 and perplexity is 244.64334761625886
At time: 1291.960132598877 and batch: 750, loss is 5.500198135375976 and perplexity is 244.7404191955475
At time: 1293.517523765564 and batch: 800, loss is 5.497654314041138 and perplexity is 244.11863448560473
At time: 1295.073745727539 and batch: 850, loss is 5.520193691253662 and perplexity is 249.68339399599913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.311001141866048 and perplexity of 202.55291111354342
Finished 45 epochs...
Completing Train Step...
At time: 1299.137645483017 and batch: 50, loss is 5.511985597610473 and perplexity is 247.64235728633562
At time: 1300.6976356506348 and batch: 100, loss is 5.450168895721435 and perplexity is 232.79748108600225
At time: 1302.2581040859222 and batch: 150, loss is 5.456158332824707 and perplexity is 234.19599091934174
At time: 1303.8150544166565 and batch: 200, loss is 5.4983580207824705 and perplexity is 244.29048287271976
At time: 1305.4196174144745 and batch: 250, loss is 5.511092739105225 and perplexity is 247.42134638178808
At time: 1306.9745585918427 and batch: 300, loss is 5.4723742389678955 and perplexity is 238.02464981695894
At time: 1308.5290522575378 and batch: 350, loss is 5.451035232543945 and perplexity is 232.99924950313965
At time: 1310.0849249362946 and batch: 400, loss is 5.468356628417968 and perplexity is 237.07027790376995
At time: 1311.6394352912903 and batch: 450, loss is 5.493794498443603 and perplexity is 243.17819769642782
At time: 1313.1944370269775 and batch: 500, loss is 5.506311225891113 and perplexity is 246.24112182288135
At time: 1314.7531492710114 and batch: 550, loss is 5.492544221878052 and perplexity is 242.87434768245217
At time: 1316.3078753948212 and batch: 600, loss is 5.513485212326049 and perplexity is 248.0140040033171
At time: 1317.8631579875946 and batch: 650, loss is 5.526832275390625 and perplexity is 251.346452283138
At time: 1319.4242424964905 and batch: 700, loss is 5.498002376556396 and perplexity is 244.20361782044392
At time: 1320.9898591041565 and batch: 750, loss is 5.4979072952270505 and perplexity is 244.18039971965194
At time: 1322.5523138046265 and batch: 800, loss is 5.4952242469787596 and perplexity is 243.52613003697365
At time: 1324.1083426475525 and batch: 850, loss is 5.517768316268921 and perplexity is 249.07855191901322
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.308504422505696 and perplexity of 202.0478241314702
Finished 46 epochs...
Completing Train Step...
At time: 1328.44721865654 and batch: 50, loss is 5.507591733932495 and perplexity is 246.5566375265797
At time: 1329.9935910701752 and batch: 100, loss is 5.445488996505738 and perplexity is 231.7105576666468
At time: 1331.5376300811768 and batch: 150, loss is 5.452698335647583 and perplexity is 233.38707368454297
At time: 1333.0891416072845 and batch: 200, loss is 5.495025129318237 and perplexity is 243.47764451100704
At time: 1334.6288282871246 and batch: 250, loss is 5.506517486572266 and perplexity is 246.29191692273218
At time: 1336.1751289367676 and batch: 300, loss is 5.467119941711426 and perplexity is 236.77727745479285
At time: 1337.7245788574219 and batch: 350, loss is 5.44671215057373 and perplexity is 231.99414878030888
At time: 1339.2719786167145 and batch: 400, loss is 5.463084192276001 and perplexity is 235.82362932778864
At time: 1340.8217573165894 and batch: 450, loss is 5.49090090751648 and perplexity is 242.47555653819953
At time: 1342.3619301319122 and batch: 500, loss is 5.502333250045776 and perplexity is 245.2635263029697
At time: 1343.901638507843 and batch: 550, loss is 5.489279584884644 and perplexity is 242.08274395473137
At time: 1345.5760915279388 and batch: 600, loss is 5.510233926773071 and perplexity is 247.20894909603612
At time: 1347.1145629882812 and batch: 650, loss is 5.523111324310303 and perplexity is 250.41294227958218
At time: 1348.6525857448578 and batch: 700, loss is 5.49404055595398 and perplexity is 243.23804088046137
At time: 1350.1911759376526 and batch: 750, loss is 5.495057306289673 and perplexity is 243.48547901026427
At time: 1351.7368705272675 and batch: 800, loss is 5.490661010742188 and perplexity is 242.41739441107367
At time: 1353.2751133441925 and batch: 850, loss is 5.514461898803711 and perplexity is 248.25635425814113
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.306826909383138 and perplexity of 201.70917038257488
Finished 47 epochs...
Completing Train Step...
At time: 1357.2969152927399 and batch: 50, loss is 5.502320127487183 and perplexity is 245.2603078390922
At time: 1358.8641114234924 and batch: 100, loss is 5.439916954040528 and perplexity is 230.4230469630291
At time: 1360.4038214683533 and batch: 150, loss is 5.448148488998413 and perplexity is 232.32761031498185
At time: 1361.9470255374908 and batch: 200, loss is 5.492155961990356 and perplexity is 242.78006761926463
At time: 1363.4912765026093 and batch: 250, loss is 5.503598022460937 and perplexity is 245.573925096021
At time: 1365.0352766513824 and batch: 300, loss is 5.463289480209351 and perplexity is 235.87204604280214
At time: 1366.5794341564178 and batch: 350, loss is 5.442711973190308 and perplexity is 231.06798467861273
At time: 1368.1233427524567 and batch: 400, loss is 5.459364633560181 and perplexity is 234.94809879496046
At time: 1369.6673254966736 and batch: 450, loss is 5.487997131347656 and perplexity is 241.77248307360873
At time: 1371.2105884552002 and batch: 500, loss is 5.49884804725647 and perplexity is 244.41022101170643
At time: 1372.7542827129364 and batch: 550, loss is 5.487227048873901 and perplexity is 241.58636999216608
At time: 1374.2960085868835 and batch: 600, loss is 5.508001232147217 and perplexity is 246.65762270468866
At time: 1375.8400225639343 and batch: 650, loss is 5.520740175247193 and perplexity is 249.81987926437887
At time: 1377.3816764354706 and batch: 700, loss is 5.493210020065308 and perplexity is 243.03610682636716
At time: 1378.9231097698212 and batch: 750, loss is 5.4928128623962404 and perplexity is 242.93960233769812
At time: 1380.4665246009827 and batch: 800, loss is 5.487216377258301 and perplexity is 241.58379188904757
At time: 1382.009391784668 and batch: 850, loss is 5.5126485252380375 and perplexity is 247.80658067483222
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.305901209513347 and perplexity of 201.5225346274908
Finished 48 epochs...
Completing Train Step...
At time: 1385.9908683300018 and batch: 50, loss is 5.5022602462768555 and perplexity is 245.24562179472704
At time: 1387.5570137500763 and batch: 100, loss is 5.436117649078369 and perplexity is 229.54926047904058
At time: 1389.0993566513062 and batch: 150, loss is 5.4458906364440915 and perplexity is 231.80364057239893
At time: 1390.6379935741425 and batch: 200, loss is 5.490509386062622 and perplexity is 242.38064073777733
At time: 1392.1851320266724 and batch: 250, loss is 5.500273475646972 and perplexity is 244.75885869966328
At time: 1393.7261846065521 and batch: 300, loss is 5.459928569793701 and perplexity is 235.08063190746725
At time: 1395.2675623893738 and batch: 350, loss is 5.439586973190307 and perplexity is 230.3470243137809
At time: 1396.8163950443268 and batch: 400, loss is 5.456485471725464 and perplexity is 234.27261807155057
At time: 1398.3593928813934 and batch: 450, loss is 5.485146751403809 and perplexity is 241.08432086468466
At time: 1399.9020137786865 and batch: 500, loss is 5.495628309249878 and perplexity is 243.62454964063835
At time: 1401.444634437561 and batch: 550, loss is 5.484444036483764 and perplexity is 240.91496682619808
At time: 1402.9863567352295 and batch: 600, loss is 5.506002454757691 and perplexity is 246.16510140965946
At time: 1404.5287215709686 and batch: 650, loss is 5.517854537963867 and perplexity is 249.10002881980859
At time: 1406.069906949997 and batch: 700, loss is 5.490439167022705 and perplexity is 242.36362159943104
At time: 1407.625197649002 and batch: 750, loss is 5.489688119888306 and perplexity is 242.18166343407853
At time: 1409.1861724853516 and batch: 800, loss is 5.484068098068238 and perplexity is 240.8244146573787
At time: 1410.742131948471 and batch: 850, loss is 5.510586185455322 and perplexity is 247.29604593409346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.303677558898926 and perplexity of 201.0749167768428
Finished 49 epochs...
Completing Train Step...
At time: 1414.7943482398987 and batch: 50, loss is 5.497466630935669 and perplexity is 244.07282184144586
At time: 1416.3558938503265 and batch: 100, loss is 5.431994524002075 and perplexity is 228.60474867513707
At time: 1417.9106221199036 and batch: 150, loss is 5.442186441421509 and perplexity is 230.94658301492132
At time: 1419.4625623226166 and batch: 200, loss is 5.486571893692017 and perplexity is 241.42814526652708
At time: 1421.0153269767761 and batch: 250, loss is 5.495657415390014 and perplexity is 243.63164071411722
At time: 1422.5689063072205 and batch: 300, loss is 5.455567359924316 and perplexity is 234.05762832362748
At time: 1424.1485261917114 and batch: 350, loss is 5.435666522979736 and perplexity is 229.44572817153545
At time: 1425.70241689682 and batch: 400, loss is 5.451280498504639 and perplexity is 233.05640329656353
At time: 1427.2561905384064 and batch: 450, loss is 5.481489276885986 and perplexity is 240.20417164729758
At time: 1428.809909105301 and batch: 500, loss is 5.49214126586914 and perplexity is 242.77649972017926
At time: 1430.3634028434753 and batch: 550, loss is 5.481235475540161 and perplexity is 240.14321524099688
At time: 1431.9166400432587 and batch: 600, loss is 5.502137660980225 and perplexity is 245.2155601300286
At time: 1433.469887495041 and batch: 650, loss is 5.51405821800232 and perplexity is 248.15615815908507
At time: 1435.0241932868958 and batch: 700, loss is 5.487171144485473 and perplexity is 241.57286463140713
At time: 1436.578429698944 and batch: 750, loss is 5.48696834564209 and perplexity is 241.52387890115972
At time: 1438.1301710605621 and batch: 800, loss is 5.4795565700531 and perplexity is 239.74037573859835
At time: 1439.6837253570557 and batch: 850, loss is 5.5068127632141115 and perplexity is 246.36465191081794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.300510406494141 and perplexity of 200.43908928340946
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fc6e480c898>
SETTINGS FOR THIS RUN
{'lr': 26.87229322417958, 'anneal': 3.0907908598910194, 'wordvec_source': 'glove', 'seq_len': 50, 'tune_wordvecs': True, 'num_layers': 1, 'dropout': 0.7470724338480177, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.0328309535980225 and batch: 50, loss is 7.450044393539429 and perplexity is 1719.939497883045
At time: 3.573289394378662 and batch: 100, loss is 6.560032978057861 and perplexity is 706.2949864482339
At time: 5.113749027252197 and batch: 150, loss is 6.531923551559448 and perplexity is 686.7178791078142
At time: 6.6549072265625 and batch: 200, loss is 6.59839599609375 and perplexity is 733.917038818502
At time: 8.196494579315186 and batch: 250, loss is 6.70715443611145 and perplexity is 818.2389722982655
At time: 9.738139390945435 and batch: 300, loss is 6.674197607040405 and perplexity is 791.7119348590085
At time: 11.279767274856567 and batch: 350, loss is 6.686112222671508 and perplexity is 801.2012970475819
At time: 12.821703433990479 and batch: 400, loss is 6.733129386901855 and perplexity is 839.7711270400986
At time: 14.369845867156982 and batch: 450, loss is 6.731623620986938 and perplexity is 838.507579842561
At time: 15.962896823883057 and batch: 500, loss is 6.723879203796387 and perplexity is 832.0389077050614
At time: 17.51365852355957 and batch: 550, loss is 6.680934896469116 and perplexity is 797.063936054451
At time: 19.05544662475586 and batch: 600, loss is 6.772890977859497 and perplexity is 873.8344820011291
At time: 20.597482204437256 and batch: 650, loss is 6.815793762207031 and perplexity is 912.140250968091
At time: 22.1389901638031 and batch: 700, loss is 6.78695764541626 and perplexity is 886.2132812695226
At time: 23.67769694328308 and batch: 750, loss is 7.075142736434937 and perplexity is 1182.2122335217023
At time: 25.215399742126465 and batch: 800, loss is 7.01845832824707 and perplexity is 1117.0631448764275
At time: 26.753389358520508 and batch: 850, loss is 7.192840871810913 and perplexity is 1329.8758510862044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.755747477213542 and perplexity of 858.9815791394734
Finished 1 epochs...
Completing Train Step...
At time: 30.755024194717407 and batch: 50, loss is 7.424659042358399 and perplexity is 1676.8277483044658
At time: 32.28706693649292 and batch: 100, loss is 7.318247699737549 and perplexity is 1507.5599561800893
At time: 33.821067810058594 and batch: 150, loss is 7.180032529830933 and perplexity is 1312.950967623993
At time: 35.36262249946594 and batch: 200, loss is 7.30581883430481 and perplexity is 1488.9386568769994
At time: 36.89709138870239 and batch: 250, loss is 7.364566679000855 and perplexity is 1579.031050937994
At time: 38.45717787742615 and batch: 300, loss is 7.178530111312866 and perplexity is 1310.9798468718586
At time: 39.99181532859802 and batch: 350, loss is 7.2603309631347654 and perplexity is 1422.727329586337
At time: 41.525452613830566 and batch: 400, loss is 7.575114469528199 and perplexity is 1949.0833605528767
At time: 43.05853819847107 and batch: 450, loss is 7.288033514022827 and perplexity is 1462.6915048529056
At time: 44.592759132385254 and batch: 500, loss is 7.469775905609131 and perplexity is 1754.2135321121168
At time: 46.12709593772888 and batch: 550, loss is 7.196615629196167 and perplexity is 1334.905296272284
At time: 47.662818908691406 and batch: 600, loss is 7.361602487564087 and perplexity is 1574.3574307928566
At time: 49.197967767715454 and batch: 650, loss is 7.447132177352906 and perplexity is 1714.937948568751
At time: 50.73224234580994 and batch: 700, loss is 7.261029148101807 and perplexity is 1423.7210032637004
At time: 52.26656723022461 and batch: 750, loss is 7.123011884689331 and perplexity is 1240.1800934287235
At time: 53.79956579208374 and batch: 800, loss is 7.0741756153106685 and perplexity is 1181.0694437944114
At time: 55.33324122428894 and batch: 850, loss is 7.006800518035889 and perplexity is 1104.116247603346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.799322764078776 and perplexity of 897.2394430644196
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 59.322200536727905 and batch: 50, loss is 6.647382936477661 and perplexity is 770.7645439672824
At time: 60.861586570739746 and batch: 100, loss is 6.537903108596802 and perplexity is 690.8364491751746
At time: 62.402730226516724 and batch: 150, loss is 6.563453788757324 and perplexity is 708.7152251239071
At time: 63.94438934326172 and batch: 200, loss is 6.625414714813233 and perplexity is 754.0168496794462
At time: 65.48932099342346 and batch: 250, loss is 6.659863796234131 and perplexity is 780.4446303895679
At time: 67.07491040229797 and batch: 300, loss is 6.649653177261353 and perplexity is 772.5163528311838
At time: 68.61266326904297 and batch: 350, loss is 6.644733352661133 and perplexity is 768.7250418143406
At time: 70.14487624168396 and batch: 400, loss is 6.653178234100341 and perplexity is 775.2443221826536
At time: 71.67952489852905 and batch: 450, loss is 6.650746688842774 and perplexity is 773.3615704532473
At time: 73.2291612625122 and batch: 500, loss is 6.677601184844971 and perplexity is 794.4111789660234
At time: 74.77380156517029 and batch: 550, loss is 6.633530979156494 and perplexity is 760.1615520338502
At time: 76.31712794303894 and batch: 600, loss is 6.655134220123291 and perplexity is 776.7621732049795
At time: 77.85040831565857 and batch: 650, loss is 6.698716630935669 and perplexity is 811.3638774086116
At time: 79.39101529121399 and batch: 700, loss is 6.642265529632568 and perplexity is 766.8303033547221
At time: 80.9369547367096 and batch: 750, loss is 6.653524570465088 and perplexity is 775.5128639831997
At time: 82.48385620117188 and batch: 800, loss is 6.69529634475708 and perplexity is 808.5935211592166
At time: 84.03578186035156 and batch: 850, loss is 6.699215478897095 and perplexity is 811.7687255952823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.232889175415039 and perplexity of 509.2245994004044
Finished 3 epochs...
Completing Train Step...
At time: 88.07336044311523 and batch: 50, loss is 6.719183101654052 and perplexity is 828.1407282964165
At time: 89.60706949234009 and batch: 100, loss is 6.671043825149536 and perplexity is 789.2189812783076
At time: 91.13885140419006 and batch: 150, loss is 6.664532566070557 and perplexity is 784.096865828153
At time: 92.67677283287048 and batch: 200, loss is 6.689371337890625 and perplexity is 803.8167641275612
At time: 94.21006679534912 and batch: 250, loss is 6.738846435546875 and perplexity is 844.5858894253955
At time: 95.74314737319946 and batch: 300, loss is 6.728600053787232 and perplexity is 835.9761247682467
At time: 97.27469253540039 and batch: 350, loss is 6.6951799392700195 and perplexity is 808.4994019146569
At time: 98.806631565094 and batch: 400, loss is 6.6889857006073 and perplexity is 803.5068421769222
At time: 100.33775091171265 and batch: 450, loss is 6.68601131439209 and perplexity is 801.1204532822001
At time: 101.86863613128662 and batch: 500, loss is 6.707641716003418 and perplexity is 818.6377808540948
At time: 103.40541100502014 and batch: 550, loss is 6.653993682861328 and perplexity is 775.876752026661
At time: 104.9375638961792 and batch: 600, loss is 6.686009769439697 and perplexity is 801.119215590195
At time: 106.4961485862732 and batch: 650, loss is 6.724715366363525 and perplexity is 832.734918442597
At time: 108.02970099449158 and batch: 700, loss is 6.6929956150054934 and perplexity is 806.7353044350805
At time: 109.56177496910095 and batch: 750, loss is 6.699555234909058 and perplexity is 812.0445757583486
At time: 111.09417581558228 and batch: 800, loss is 6.704294195175171 and perplexity is 815.9019555028493
At time: 112.62799882888794 and batch: 850, loss is 6.705334072113037 and perplexity is 816.7508344182459
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.200072606404622 and perplexity of 492.7848191283525
Finished 4 epochs...
Completing Train Step...
At time: 116.60809850692749 and batch: 50, loss is 6.68936972618103 and perplexity is 803.8154686094138
At time: 118.14364957809448 and batch: 100, loss is 6.625810613632202 and perplexity is 754.3154231582615
At time: 119.67948031425476 and batch: 150, loss is 6.6267969608306885 and perplexity is 755.0598071128329
At time: 121.21569061279297 and batch: 200, loss is 6.665669240951538 and perplexity is 784.988635770026
At time: 122.75445461273193 and batch: 250, loss is 6.701657629013061 and perplexity is 813.7536093882218
At time: 124.29387640953064 and batch: 300, loss is 6.677289381027221 and perplexity is 794.163517140517
At time: 125.83270192146301 and batch: 350, loss is 6.672361707687378 and perplexity is 790.2597648568818
At time: 127.37195014953613 and batch: 400, loss is 6.677006521224976 and perplexity is 793.9389119724921
At time: 128.91021394729614 and batch: 450, loss is 6.678478860855103 and perplexity is 795.1087206630124
At time: 130.44747853279114 and batch: 500, loss is 6.689690809249878 and perplexity is 804.0736015857091
At time: 131.99213528633118 and batch: 550, loss is 6.6340367794036865 and perplexity is 760.546139188715
At time: 133.53048634529114 and batch: 600, loss is 6.664147872924804 and perplexity is 783.7952871496071
At time: 135.06816601753235 and batch: 650, loss is 6.716465749740601 and perplexity is 825.8934332323797
At time: 136.60664439201355 and batch: 700, loss is 6.679041614532471 and perplexity is 795.5562969452602
At time: 138.14386367797852 and batch: 750, loss is 6.673251428604126 and perplexity is 790.9631883782472
At time: 139.68042540550232 and batch: 800, loss is 6.708312873840332 and perplexity is 819.1874004364223
At time: 141.21909046173096 and batch: 850, loss is 6.706079769134521 and perplexity is 817.3601102221094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.210849761962891 and perplexity of 498.124358623834
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 145.2443242073059 and batch: 50, loss is 6.676291780471802 and perplexity is 793.3716542218398
At time: 146.7834849357605 and batch: 100, loss is 6.607445755004883 and perplexity is 740.5889551630481
At time: 148.32946634292603 and batch: 150, loss is 6.5758052158355715 and perplexity is 717.5231528083359
At time: 149.87449407577515 and batch: 200, loss is 6.591305427551269 and perplexity is 728.7315554882531
At time: 151.41287112236023 and batch: 250, loss is 6.622284002304077 and perplexity is 751.6599310384609
At time: 152.95450925827026 and batch: 300, loss is 6.594477710723877 and perplexity is 731.0469689709506
At time: 154.49278593063354 and batch: 350, loss is 6.551620140075683 and perplexity is 700.3779655272639
At time: 156.03025555610657 and batch: 400, loss is 6.568792428970337 and perplexity is 712.5089182954302
At time: 157.56887531280518 and batch: 450, loss is 6.5620940399169925 and perplexity is 707.7522052993428
At time: 159.10592460632324 and batch: 500, loss is 6.554153814315796 and perplexity is 702.1547450766462
At time: 160.64389967918396 and batch: 550, loss is 6.481615571975708 and perplexity is 653.0251035651493
At time: 162.18030333518982 and batch: 600, loss is 6.483889989852905 and perplexity is 654.5120458580296
At time: 163.71576642990112 and batch: 650, loss is 6.517147665023804 and perplexity is 676.6456102482365
At time: 165.2546751499176 and batch: 700, loss is 6.476509342193603 and perplexity is 649.6991062129493
At time: 166.80601716041565 and batch: 750, loss is 6.449206123352051 and perplexity is 632.2002045613242
At time: 168.36304998397827 and batch: 800, loss is 6.479331188201904 and perplexity is 651.5350461940959
At time: 169.91271090507507 and batch: 850, loss is 6.493697834014893 and perplexity is 660.9629811785242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.03866449991862 and perplexity of 419.33264199008346
Finished 6 epochs...
Completing Train Step...
At time: 173.89057779312134 and batch: 50, loss is 6.5106782054901124 and perplexity is 672.2822084983842
At time: 175.4482297897339 and batch: 100, loss is 6.4694302368164065 and perplexity is 645.1160588536516
At time: 176.98579716682434 and batch: 150, loss is 6.454156608581543 and perplexity is 635.3376618966362
At time: 178.52148723602295 and batch: 200, loss is 6.496895208358764 and perplexity is 663.0797094397996
At time: 180.05873775482178 and batch: 250, loss is 6.5268869972229 and perplexity is 683.2678825363728
At time: 181.59443712234497 and batch: 300, loss is 6.494465684890747 and perplexity is 661.4706970826344
At time: 183.12723684310913 and batch: 350, loss is 6.472171487808228 and perplexity is 646.8869099545772
At time: 184.6880133152008 and batch: 400, loss is 6.487062320709229 and perplexity is 656.5916715031851
At time: 186.22496819496155 and batch: 450, loss is 6.487970905303955 and perplexity is 657.1885116797746
At time: 187.75945711135864 and batch: 500, loss is 6.486784896850586 and perplexity is 656.4095425727473
At time: 189.29667687416077 and batch: 550, loss is 6.43980001449585 and perplexity is 626.2815400573105
At time: 190.832861661911 and batch: 600, loss is 6.456449165344238 and perplexity is 636.7958804357215
At time: 192.36837816238403 and batch: 650, loss is 6.486499176025391 and perplexity is 656.2220194874786
At time: 193.90350604057312 and batch: 700, loss is 6.456175022125244 and perplexity is 636.6213310900696
At time: 195.44093203544617 and batch: 750, loss is 6.441935262680054 and perplexity is 627.6202352931798
At time: 196.9771761894226 and batch: 800, loss is 6.48558443069458 and perplexity is 655.6220179253288
At time: 198.51812744140625 and batch: 850, loss is 6.493039903640747 and perplexity is 660.5282565819606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.032341003417969 and perplexity of 416.6893596970008
Finished 7 epochs...
Completing Train Step...
At time: 202.49417686462402 and batch: 50, loss is 6.476667213439941 and perplexity is 649.8016831173506
At time: 204.05756497383118 and batch: 100, loss is 6.438181018829345 and perplexity is 625.2684133031545
At time: 205.59292030334473 and batch: 150, loss is 6.420190668106079 and perplexity is 614.1201960979928
At time: 207.12907767295837 and batch: 200, loss is 6.4652617645263675 and perplexity is 642.4325074790502
At time: 208.6651849746704 and batch: 250, loss is 6.492574396133423 and perplexity is 660.2208472759436
At time: 210.2088086605072 and batch: 300, loss is 6.453084669113159 and perplexity is 634.6569832694196
At time: 211.74318838119507 and batch: 350, loss is 6.437043733596802 and perplexity is 624.5577089836958
At time: 213.27831435203552 and batch: 400, loss is 6.465445070266724 and perplexity is 642.5502798393077
At time: 214.81412625312805 and batch: 450, loss is 6.462755908966065 and perplexity is 640.8246797414403
At time: 216.3477680683136 and batch: 500, loss is 6.462853784561157 and perplexity is 640.8874039078526
At time: 217.88323402404785 and batch: 550, loss is 6.419253559112549 and perplexity is 613.5449681068785
At time: 219.4187514781952 and batch: 600, loss is 6.4369509887695315 and perplexity is 624.4997871728713
At time: 220.95334601402283 and batch: 650, loss is 6.466528024673462 and perplexity is 643.2465094208128
At time: 222.54546213150024 and batch: 700, loss is 6.434993925094605 and perplexity is 623.2787964923594
At time: 224.0796217918396 and batch: 750, loss is 6.421595115661621 and perplexity is 614.9833016575872
At time: 225.62197089195251 and batch: 800, loss is 6.461370286941528 and perplexity is 639.9373538424464
At time: 227.17477464675903 and batch: 850, loss is 6.4701808834075925 and perplexity is 645.6004948215062
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.010978698730469 and perplexity of 407.8823189000167
Finished 8 epochs...
Completing Train Step...
At time: 231.1368293762207 and batch: 50, loss is 6.456631965637207 and perplexity is 636.9122975494439
At time: 232.7033302783966 and batch: 100, loss is 6.411901655197144 and perplexity is 609.0507850971247
At time: 234.24209022521973 and batch: 150, loss is 6.396773977279663 and perplexity is 599.9066005084187
At time: 235.7814118862152 and batch: 200, loss is 6.44748046875 and perplexity is 631.1101861370565
At time: 237.32078409194946 and batch: 250, loss is 6.480459566116333 and perplexity is 652.270638885923
At time: 238.86052799224854 and batch: 300, loss is 6.446307439804077 and perplexity is 630.3703096537553
At time: 240.396986246109 and batch: 350, loss is 6.4279263401031494 and perplexity is 618.8892506407144
At time: 241.93337297439575 and batch: 400, loss is 6.453422145843506 and perplexity is 634.8712013778054
At time: 243.4738085269928 and batch: 450, loss is 6.453609867095947 and perplexity is 634.9903913817653
At time: 245.01179480552673 and batch: 500, loss is 6.452610168457031 and perplexity is 634.3559095499298
At time: 246.55101704597473 and batch: 550, loss is 6.408388538360596 and perplexity is 606.9148725815808
At time: 248.08792781829834 and batch: 600, loss is 6.428752126693726 and perplexity is 619.4005321606024
At time: 249.62574362754822 and batch: 650, loss is 6.45911584854126 and perplexity is 638.4962795151828
At time: 251.1637477874756 and batch: 700, loss is 6.427989654541015 and perplexity is 618.9284365062224
At time: 252.70339250564575 and batch: 750, loss is 6.416678047180175 and perplexity is 611.9668088767977
At time: 254.24278855323792 and batch: 800, loss is 6.455842323303223 and perplexity is 636.4095631525676
At time: 255.7852783203125 and batch: 850, loss is 6.463831844329834 and perplexity is 641.5145367305654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.005982081095378 and perplexity of 405.8493700759121
Finished 9 epochs...
Completing Train Step...
At time: 259.77019238471985 and batch: 50, loss is 6.449919576644898 and perplexity is 632.6514108171623
At time: 261.3134801387787 and batch: 100, loss is 6.406948976516723 and perplexity is 606.0418096533466
At time: 262.877290725708 and batch: 150, loss is 6.391897268295288 and perplexity is 596.9881525944605
At time: 264.4132559299469 and batch: 200, loss is 6.440880842208863 and perplexity is 626.9588084412406
At time: 265.9494185447693 and batch: 250, loss is 6.475857334136963 and perplexity is 649.275635229555
At time: 267.48578000068665 and batch: 300, loss is 6.4414701175689695 and perplexity is 627.3283686945387
At time: 269.02117133140564 and batch: 350, loss is 6.4219496631622315 and perplexity is 615.2013811075848
At time: 270.5616955757141 and batch: 400, loss is 6.449340238571167 and perplexity is 632.2849979161967
At time: 272.0999927520752 and batch: 450, loss is 6.449851865768433 and perplexity is 632.6085748858847
At time: 273.6361846923828 and batch: 500, loss is 6.448229875564575 and perplexity is 631.5833216746502
At time: 275.1747989654541 and batch: 550, loss is 6.405255765914917 and perplexity is 605.016521495448
At time: 276.7123818397522 and batch: 600, loss is 6.4241263294219975 and perplexity is 616.5419276287156
At time: 278.2493019104004 and batch: 650, loss is 6.45540337562561 and perplexity is 636.1302739539681
At time: 279.7859501838684 and batch: 700, loss is 6.423951444625854 and perplexity is 616.4341132472113
At time: 281.3244948387146 and batch: 750, loss is 6.412345724105835 and perplexity is 609.3213056750437
At time: 282.86140608787537 and batch: 800, loss is 6.4500155544281 and perplexity is 632.7121342111144
At time: 284.4031672477722 and batch: 850, loss is 6.457417783737182 and perplexity is 637.4129914623593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.001554489135742 and perplexity of 404.05640685195704
Finished 10 epochs...
Completing Train Step...
At time: 288.421560049057 and batch: 50, loss is 6.43911036491394 and perplexity is 625.849774155752
At time: 289.9591040611267 and batch: 100, loss is 6.398323421478271 and perplexity is 600.8368428034382
At time: 291.49577045440674 and batch: 150, loss is 6.383180065155029 and perplexity is 591.806702284755
At time: 293.0313608646393 and batch: 200, loss is 6.434186925888062 and perplexity is 622.7760138979986
At time: 294.56747794151306 and batch: 250, loss is 6.468715991973877 and perplexity is 644.6554525482027
At time: 296.10333824157715 and batch: 300, loss is 6.432709503173828 and perplexity is 621.8565898254317
At time: 297.63724970817566 and batch: 350, loss is 6.4127565574646 and perplexity is 609.5716866225222
At time: 299.17215871810913 and batch: 400, loss is 6.43760419845581 and perplexity is 624.9078497436342
At time: 300.7347483634949 and batch: 450, loss is 6.439155359268188 and perplexity is 625.8779344957197
At time: 302.2711067199707 and batch: 500, loss is 6.433250389099121 and perplexity is 622.1930342832526
At time: 303.8122959136963 and batch: 550, loss is 6.386531534194947 and perplexity is 593.7934515297163
At time: 305.34841871261597 and batch: 600, loss is 6.411701726913452 and perplexity is 608.9290307904494
At time: 306.8837454319 and batch: 650, loss is 6.438926486968994 and perplexity is 625.7347047651211
At time: 308.4198977947235 and batch: 700, loss is 6.3963205909729 and perplexity is 599.6346727192376
At time: 309.96790957450867 and batch: 750, loss is 6.386018753051758 and perplexity is 593.4890434988421
At time: 311.50409603118896 and batch: 800, loss is 6.421907320022583 and perplexity is 615.1753321010947
At time: 313.039302110672 and batch: 850, loss is 6.428574295043945 and perplexity is 619.2903929355034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.958924611409505 and perplexity of 387.1935167802202
Finished 11 epochs...
Completing Train Step...
At time: 317.0286912918091 and batch: 50, loss is 6.400282611846924 and perplexity is 602.0151504465794
At time: 318.5701713562012 and batch: 100, loss is 6.360970182418823 and perplexity is 578.807633068804
At time: 320.11147689819336 and batch: 150, loss is 6.340689725875855 and perplexity is 567.1873803135722
At time: 321.65293979644775 and batch: 200, loss is 6.390033235549927 and perplexity is 595.8763836382159
At time: 323.194580078125 and batch: 250, loss is 6.4270169639587404 and perplexity is 618.3267033424644
At time: 324.73563051223755 and batch: 300, loss is 6.395440769195557 and perplexity is 599.1073330921903
At time: 326.2735981941223 and batch: 350, loss is 6.36809775352478 and perplexity is 582.9478629940244
At time: 327.81223487854004 and batch: 400, loss is 6.40429690361023 and perplexity is 604.4366720016112
At time: 329.3539569377899 and batch: 450, loss is 6.408739814758301 and perplexity is 607.1281049012827
At time: 330.8958320617676 and batch: 500, loss is 6.403645715713501 and perplexity is 604.0431982830305
At time: 332.4520814418793 and batch: 550, loss is 6.362291860580444 and perplexity is 579.5731362401889
At time: 333.99450755119324 and batch: 600, loss is 6.3856510829925535 and perplexity is 593.2708754564679
At time: 335.5335922241211 and batch: 650, loss is 6.410234699249267 and perplexity is 608.0363699958914
At time: 337.073105096817 and batch: 700, loss is 6.375590677261353 and perplexity is 587.332252308175
At time: 338.61345052719116 and batch: 750, loss is 6.373607501983643 and perplexity is 586.1686237266913
At time: 340.1975028514862 and batch: 800, loss is 6.404901809692383 and perplexity is 604.8024100282075
At time: 341.7369682788849 and batch: 850, loss is 6.4123172855377195 and perplexity is 609.3039776959806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.950440088907878 and perplexity of 383.92226181678285
Finished 12 epochs...
Completing Train Step...
At time: 345.7188274860382 and batch: 50, loss is 6.3879352474212645 and perplexity is 594.627552533675
At time: 347.2934935092926 and batch: 100, loss is 6.351425943374633 and perplexity is 573.3096335047258
At time: 348.83203625679016 and batch: 150, loss is 6.32961989402771 and perplexity is 560.9433354132481
At time: 350.3715434074402 and batch: 200, loss is 6.3784028434753415 and perplexity is 588.9862527966233
At time: 351.9120981693268 and batch: 250, loss is 6.4177358436584475 and perplexity is 612.6144877078478
At time: 353.4524862766266 and batch: 300, loss is 6.388542757034302 and perplexity is 594.98890423922
At time: 354.9895317554474 and batch: 350, loss is 6.359972372055053 and perplexity is 578.230380855913
At time: 356.5270233154297 and batch: 400, loss is 6.397128963470459 and perplexity is 600.1195968705136
At time: 358.0664701461792 and batch: 450, loss is 6.403363265991211 and perplexity is 603.8726105417885
At time: 359.60418152809143 and batch: 500, loss is 6.397697124481201 and perplexity is 600.4606583069681
At time: 361.1411278247833 and batch: 550, loss is 6.3557344341278075 and perplexity is 575.7850616096376
At time: 362.67809677124023 and batch: 600, loss is 6.377710266113281 and perplexity is 588.5784754763416
At time: 364.217502117157 and batch: 650, loss is 6.40267315864563 and perplexity is 603.4560173809922
At time: 365.7556140422821 and batch: 700, loss is 6.36781042098999 and perplexity is 582.7803871686788
At time: 367.2921862602234 and batch: 750, loss is 6.366412239074707 and perplexity is 581.9661235477437
At time: 368.8299503326416 and batch: 800, loss is 6.39938401222229 and perplexity is 601.4744228435603
At time: 370.36794543266296 and batch: 850, loss is 6.406584806442261 and perplexity is 605.8211475441049
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.944965362548828 and perplexity of 381.82613558448827
Finished 13 epochs...
Completing Train Step...
At time: 374.35548305511475 and batch: 50, loss is 6.381511516571045 and perplexity is 590.8200674026871
At time: 375.91874408721924 and batch: 100, loss is 6.346228351593018 and perplexity is 570.3375346345562
At time: 377.45639419555664 and batch: 150, loss is 6.324527988433838 and perplexity is 558.094324508735
At time: 379.0196306705475 and batch: 200, loss is 6.373799638748169 and perplexity is 586.2812590898715
At time: 380.5568964481354 and batch: 250, loss is 6.414301443099975 and perplexity is 610.5141329633816
At time: 382.0948693752289 and batch: 300, loss is 6.383863182067871 and perplexity is 592.2111135665855
At time: 383.63094687461853 and batch: 350, loss is 6.354737701416016 and perplexity is 575.211443723116
At time: 385.1669030189514 and batch: 400, loss is 6.392459192276001 and perplexity is 597.3237088233211
At time: 386.70542907714844 and batch: 450, loss is 6.4012262153625485 and perplexity is 602.5834821567302
At time: 388.25053429603577 and batch: 500, loss is 6.39458535194397 and perplexity is 598.5950654764263
At time: 389.787220954895 and batch: 550, loss is 6.354351406097412 and perplexity is 574.9892851473697
At time: 391.3230164051056 and batch: 600, loss is 6.374931669235229 and perplexity is 586.945323148617
At time: 392.8585772514343 and batch: 650, loss is 6.3991075611114505 and perplexity is 601.3081675530123
At time: 394.39513063430786 and batch: 700, loss is 6.366242275238037 and perplexity is 581.8672187579306
At time: 395.93044352531433 and batch: 750, loss is 6.364737339019776 and perplexity is 580.9922042918301
At time: 397.4732828140259 and batch: 800, loss is 6.395943107604981 and perplexity is 599.4083633201627
At time: 399.0100157260895 and batch: 850, loss is 6.402763872146607 and perplexity is 603.5107614719908
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.943391799926758 and perplexity of 381.22578072126754
Finished 14 epochs...
Completing Train Step...
At time: 403.1005859375 and batch: 50, loss is 6.379239149093628 and perplexity is 589.4790313369406
At time: 404.66562008857727 and batch: 100, loss is 6.342431106567383 and perplexity is 568.1759299369351
At time: 406.20812225341797 and batch: 150, loss is 6.321337661743164 and perplexity is 556.316658464839
At time: 407.7555363178253 and batch: 200, loss is 6.368881788253784 and perplexity is 583.4050935827187
At time: 409.29162073135376 and batch: 250, loss is 6.408963766098022 and perplexity is 607.2640872799076
At time: 410.8289942741394 and batch: 300, loss is 6.379773044586182 and perplexity is 589.7938355635274
At time: 412.36451983451843 and batch: 350, loss is 6.352923383712769 and perplexity is 574.1687735711337
At time: 413.8995440006256 and batch: 400, loss is 6.389556341171264 and perplexity is 595.592281289272
At time: 415.43816804885864 and batch: 450, loss is 6.397515525817871 and perplexity is 600.3516253544561
At time: 416.97515058517456 and batch: 500, loss is 6.391433143615723 and perplexity is 596.7111399486088
At time: 418.55456018447876 and batch: 550, loss is 6.3517303466796875 and perplexity is 573.484177416508
At time: 420.08769822120667 and batch: 600, loss is 6.371599330902099 and perplexity is 584.9926779929144
At time: 421.6219074726105 and batch: 650, loss is 6.396774396896363 and perplexity is 599.9068522392993
At time: 423.15716671943665 and batch: 700, loss is 6.363701810836792 and perplexity is 580.3908818870749
At time: 424.69266653060913 and batch: 750, loss is 6.361935949325561 and perplexity is 579.3668963417008
At time: 426.2278940677643 and batch: 800, loss is 6.394139499664306 and perplexity is 598.3282399886842
At time: 427.76258754730225 and batch: 850, loss is 6.400417594909668 and perplexity is 602.0964177801384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.942274729410808 and perplexity of 380.8001624087965
Finished 15 epochs...
Completing Train Step...
At time: 431.75282049179077 and batch: 50, loss is 6.377873783111572 and perplexity is 588.6747259309884
At time: 433.29305028915405 and batch: 100, loss is 6.340213108062744 and perplexity is 566.9171131169395
At time: 434.8337461948395 and batch: 150, loss is 6.319151105880738 and perplexity is 555.1015699275038
At time: 436.37387585639954 and batch: 200, loss is 6.367455253601074 and perplexity is 582.5734393329933
At time: 437.9143285751343 and batch: 250, loss is 6.407348375320435 and perplexity is 606.2839103712715
At time: 439.45404624938965 and batch: 300, loss is 6.378872594833374 and perplexity is 589.2629948836336
At time: 440.9928410053253 and batch: 350, loss is 6.350426855087281 and perplexity is 572.7371326019594
At time: 442.531325340271 and batch: 400, loss is 6.386592779159546 and perplexity is 593.8298195023009
At time: 444.07325768470764 and batch: 450, loss is 6.394184799194336 and perplexity is 598.3553445906672
At time: 445.61222290992737 and batch: 500, loss is 6.388486776351929 and perplexity is 594.955597286638
At time: 447.15199637413025 and batch: 550, loss is 6.348503370285034 and perplexity is 571.6365402573716
At time: 448.6904082298279 and batch: 600, loss is 6.367992515563965 and perplexity is 582.8865179776302
At time: 450.2306754589081 and batch: 650, loss is 6.393273458480835 and perplexity is 597.8102874081263
At time: 451.7679536342621 and batch: 700, loss is 6.360509605407715 and perplexity is 578.5411089613083
At time: 453.3069498538971 and batch: 750, loss is 6.357894477844238 and perplexity is 577.0301267274233
At time: 454.8458387851715 and batch: 800, loss is 6.390987911224365 and perplexity is 596.4455239556293
At time: 456.4315845966339 and batch: 850, loss is 6.39633960723877 and perplexity is 599.6460756400188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.938287734985352 and perplexity of 379.284936889094
Finished 16 epochs...
Completing Train Step...
At time: 460.4280159473419 and batch: 50, loss is 6.373705959320068 and perplexity is 586.2263391692871
At time: 461.97021555900574 and batch: 100, loss is 6.337547731399536 and perplexity is 565.4080774412798
At time: 463.5105736255646 and batch: 150, loss is 6.316144561767578 and perplexity is 553.4351389246748
At time: 465.04932975769043 and batch: 200, loss is 6.36459903717041 and perplexity is 580.911857551689
At time: 466.58832025527954 and batch: 250, loss is 6.404405298233033 and perplexity is 604.5021932376917
At time: 468.13127422332764 and batch: 300, loss is 6.3759605598449705 and perplexity is 587.5495364614357
At time: 469.6694846153259 and batch: 350, loss is 6.348160762786865 and perplexity is 571.4407268379402
At time: 471.20743918418884 and batch: 400, loss is 6.385044355392456 and perplexity is 592.9110308168704
At time: 472.74704599380493 and batch: 450, loss is 6.392768039703369 and perplexity is 597.5082192054087
At time: 474.28550696372986 and batch: 500, loss is 6.387017822265625 and perplexity is 594.0822764220001
At time: 475.82226371765137 and batch: 550, loss is 6.3470273685455325 and perplexity is 570.7934261016355
At time: 477.3585641384125 and batch: 600, loss is 6.367281665802002 and perplexity is 582.4723204686162
At time: 478.89519214630127 and batch: 650, loss is 6.393892307281494 and perplexity is 598.1803560839777
At time: 480.43054246902466 and batch: 700, loss is 6.359751968383789 and perplexity is 578.1029508006778
At time: 481.96658396720886 and batch: 750, loss is 6.357705907821655 and perplexity is 576.9213264019578
At time: 483.50326681137085 and batch: 800, loss is 6.388528833389282 and perplexity is 594.9806198826009
At time: 485.0415997505188 and batch: 850, loss is 6.39467866897583 and perplexity is 598.6509271976071
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.937878926595052 and perplexity of 379.12991371412573
Finished 17 epochs...
Completing Train Step...
At time: 489.04766726493835 and batch: 50, loss is 6.372199296951294 and perplexity is 585.3437590465628
At time: 490.58523511886597 and batch: 100, loss is 6.335933084487915 and perplexity is 564.4958796720704
At time: 492.1242516040802 and batch: 150, loss is 6.31421989440918 and perplexity is 552.3709847776712
At time: 493.6606204509735 and batch: 200, loss is 6.362289705276489 and perplexity is 579.5718870852623
At time: 495.19869804382324 and batch: 250, loss is 6.4032492923736575 and perplexity is 603.8037889178229
At time: 496.76250290870667 and batch: 300, loss is 6.3744545173645015 and perplexity is 586.6653278951036
At time: 498.29805421829224 and batch: 350, loss is 6.347312784194946 and perplexity is 570.9563627292528
At time: 499.83274102211 and batch: 400, loss is 6.383295278549195 and perplexity is 591.8748902716245
At time: 501.36987042427063 and batch: 450, loss is 6.391168584823609 and perplexity is 596.5532956506518
At time: 502.9038670063019 and batch: 500, loss is 6.3856798362731935 and perplexity is 593.2879341856916
At time: 504.44038128852844 and batch: 550, loss is 6.344966859817505 and perplexity is 569.6185121411812
At time: 505.97482323646545 and batch: 600, loss is 6.366274337768555 and perplexity is 581.8858751924739
At time: 507.51069259643555 and batch: 650, loss is 6.392625961303711 and perplexity is 597.4233322242877
At time: 509.046391248703 and batch: 700, loss is 6.35780550956726 and perplexity is 576.9787916349154
At time: 510.5839841365814 and batch: 750, loss is 6.355703058242798 and perplexity is 575.7669961271663
At time: 512.1200242042542 and batch: 800, loss is 6.38871916770935 and perplexity is 595.093875892251
At time: 513.6576051712036 and batch: 850, loss is 6.39421688079834 and perplexity is 598.374541097812
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.93818473815918 and perplexity of 379.2458737561035
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 517.6417183876038 and batch: 50, loss is 6.367569341659546 and perplexity is 582.6399077971608
At time: 519.2100434303284 and batch: 100, loss is 6.325441932678222 and perplexity is 558.6046247619123
At time: 520.751683473587 and batch: 150, loss is 6.297983570098877 and perplexity is 543.4749254111234
At time: 522.2932226657867 and batch: 200, loss is 6.344353075027466 and perplexity is 569.2689962370501
At time: 523.8326332569122 and batch: 250, loss is 6.37503872871399 and perplexity is 587.0081645727992
At time: 525.3739478588104 and batch: 300, loss is 6.338162488937378 and perplexity is 565.755773182182
At time: 526.9210474491119 and batch: 350, loss is 6.310496025085449 and perplexity is 550.317852582737
At time: 528.4616854190826 and batch: 400, loss is 6.340466375350952 and perplexity is 567.0607128606088
At time: 530.0034906864166 and batch: 450, loss is 6.350860319137573 and perplexity is 572.9854473730852
At time: 531.5476746559143 and batch: 500, loss is 6.342992324829101 and perplexity is 568.4948901394579
At time: 533.0998303890228 and batch: 550, loss is 6.2891990661621096 and perplexity is 538.7216758254938
At time: 534.6826515197754 and batch: 600, loss is 6.302821607589721 and perplexity is 546.1106481973153
At time: 536.2245042324066 and batch: 650, loss is 6.32779260635376 and perplexity is 559.919266489869
At time: 537.7644331455231 and batch: 700, loss is 6.291511869430542 and perplexity is 539.969075016547
At time: 539.3030021190643 and batch: 750, loss is 6.282086877822876 and perplexity is 534.9037786997527
At time: 540.843686580658 and batch: 800, loss is 6.3167031955718995 and perplexity is 553.7443928735902
At time: 542.384875535965 and batch: 850, loss is 6.330412921905517 and perplexity is 561.3883555494656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.899218877156575 and perplexity of 364.75244009581667
Finished 19 epochs...
Completing Train Step...
At time: 546.3451957702637 and batch: 50, loss is 6.328080911636352 and perplexity is 560.0807174446865
At time: 547.9102876186371 and batch: 100, loss is 6.28984447479248 and perplexity is 539.0694836714588
At time: 549.4496850967407 and batch: 150, loss is 6.265255136489868 and perplexity is 525.975764451981
At time: 550.987279176712 and batch: 200, loss is 6.319503145217896 and perplexity is 555.2970219176123
At time: 552.5325355529785 and batch: 250, loss is 6.353654289245606 and perplexity is 574.5885901089446
At time: 554.0724897384644 and batch: 300, loss is 6.317791767120362 and perplexity is 554.3475114741099
At time: 555.6107423305511 and batch: 350, loss is 6.291184997558593 and perplexity is 539.7926031576169
At time: 557.1489408016205 and batch: 400, loss is 6.321513051986694 and perplexity is 556.414239536181
At time: 558.6893556118011 and batch: 450, loss is 6.333722686767578 and perplexity is 563.2494972743792
At time: 560.2292945384979 and batch: 500, loss is 6.328125009536743 and perplexity is 560.1054163729566
At time: 561.767884016037 and batch: 550, loss is 6.28276294708252 and perplexity is 535.2655329730919
At time: 563.3065898418427 and batch: 600, loss is 6.302337436676026 and perplexity is 545.8463013055008
At time: 564.8459904193878 and batch: 650, loss is 6.328370685577393 and perplexity is 560.243037758444
At time: 566.384019613266 and batch: 700, loss is 6.294928197860718 and perplexity is 541.8169413808898
At time: 567.9208416938782 and batch: 750, loss is 6.288072919845581 and perplexity is 538.1153378713356
At time: 569.4609189033508 and batch: 800, loss is 6.320844287872315 and perplexity is 556.0422540591823
At time: 571.0007524490356 and batch: 850, loss is 6.331473293304444 and perplexity is 561.9839514259903
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.898338953653972 and perplexity of 364.43162701732774
Finished 20 epochs...
Completing Train Step...
At time: 574.9790937900543 and batch: 50, loss is 6.321411771774292 and perplexity is 556.3578886374805
At time: 576.5443575382233 and batch: 100, loss is 6.282891912460327 and perplexity is 535.3345681462569
At time: 578.0813221931458 and batch: 150, loss is 6.259180965423584 and perplexity is 522.7905811493617
At time: 579.6278369426727 and batch: 200, loss is 6.3152108573913575 and perplexity is 552.9186352818867
At time: 581.164647102356 and batch: 250, loss is 6.350071811676026 and perplexity is 572.5338221508018
At time: 582.7099494934082 and batch: 300, loss is 6.314960126876831 and perplexity is 552.7800190863524
At time: 584.2484230995178 and batch: 350, loss is 6.288562488555908 and perplexity is 538.3788468009051
At time: 585.7873241901398 and batch: 400, loss is 6.319546775817871 and perplexity is 555.3212503883907
At time: 587.3334038257599 and batch: 450, loss is 6.3323995304107665 and perplexity is 562.5047229566853
At time: 588.871856212616 and batch: 500, loss is 6.327238330841064 and perplexity is 559.6090029450531
At time: 590.4105951786041 and batch: 550, loss is 6.283536758422851 and perplexity is 535.6798878081544
At time: 591.9615831375122 and batch: 600, loss is 6.303002414703369 and perplexity is 546.2093978143955
At time: 593.5094373226166 and batch: 650, loss is 6.328640441894532 and perplexity is 560.39418724286
At time: 595.0515477657318 and batch: 700, loss is 6.294949131011963 and perplexity is 541.8282834355831
At time: 596.5903289318085 and batch: 750, loss is 6.286911516189575 and perplexity is 537.4907315308318
At time: 598.129435300827 and batch: 800, loss is 6.319753608703613 and perplexity is 555.4361209642193
At time: 599.6691546440125 and batch: 850, loss is 6.330147743225098 and perplexity is 561.239507062733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.897274017333984 and perplexity of 364.0437371172448
Finished 21 epochs...
Completing Train Step...
At time: 603.670202255249 and batch: 50, loss is 6.318567581176758 and perplexity is 554.7777489361957
At time: 605.2098689079285 and batch: 100, loss is 6.2796297454833985 and perplexity is 533.5910627454728
At time: 606.7487459182739 and batch: 150, loss is 6.257152671813965 and perplexity is 521.7312830014398
At time: 608.2859663963318 and batch: 200, loss is 6.312966709136963 and perplexity is 551.679195155567
At time: 609.8210661411285 and batch: 250, loss is 6.348129758834839 and perplexity is 571.4230101917041
At time: 611.3593189716339 and batch: 300, loss is 6.312129907608032 and perplexity is 551.2177422607526
At time: 612.9224495887756 and batch: 350, loss is 6.28612078666687 and perplexity is 537.0658897308427
At time: 614.459321975708 and batch: 400, loss is 6.318358421325684 and perplexity is 554.6617238391677
At time: 615.9990634918213 and batch: 450, loss is 6.331170806884765 and perplexity is 561.8139846202281
At time: 617.5408782958984 and batch: 500, loss is 6.326605892181396 and perplexity is 559.2551964695319
At time: 619.0762145519257 and batch: 550, loss is 6.281421022415161 and perplexity is 534.5477286781694
At time: 620.6122462749481 and batch: 600, loss is 6.302926378250122 and perplexity is 546.1678675679821
At time: 622.1497900485992 and batch: 650, loss is 6.328652610778809 and perplexity is 560.4010066563663
At time: 623.687707901001 and batch: 700, loss is 6.294987621307373 and perplexity is 541.8491389676395
At time: 625.2248160839081 and batch: 750, loss is 6.2868881511688235 and perplexity is 537.478173195449
At time: 626.7621791362762 and batch: 800, loss is 6.31984510421753 and perplexity is 555.4869432025226
At time: 628.2989592552185 and batch: 850, loss is 6.329578037261963 and perplexity is 560.9198566308368
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.897096633911133 and perplexity of 363.97916752004465
Finished 22 epochs...
Completing Train Step...
At time: 632.3264982700348 and batch: 50, loss is 6.317388219833374 and perplexity is 554.1238511715956
At time: 633.8670253753662 and batch: 100, loss is 6.278545808792114 and perplexity is 533.0129971642667
At time: 635.407651424408 and batch: 150, loss is 6.256065950393677 and perplexity is 521.1646144017885
At time: 636.9471323490143 and batch: 200, loss is 6.311818866729737 and perplexity is 551.0463176714774
At time: 638.4865064620972 and batch: 250, loss is 6.347455358505249 and perplexity is 571.0377722421965
At time: 640.0259387493134 and batch: 300, loss is 6.3116671752929685 and perplexity is 550.9627350033718
At time: 641.5644211769104 and batch: 350, loss is 6.285550546646118 and perplexity is 536.7597205699885
At time: 643.1036534309387 and batch: 400, loss is 6.317532978057861 and perplexity is 554.2040709625344
At time: 644.6447286605835 and batch: 450, loss is 6.3308146286010745 and perplexity is 561.6139143118971
At time: 646.1855635643005 and batch: 500, loss is 6.325876760482788 and perplexity is 558.8475744013189
At time: 647.7321443557739 and batch: 550, loss is 6.28206693649292 and perplexity is 534.8931121133602
At time: 649.2722585201263 and batch: 600, loss is 6.303543863296508 and perplexity is 546.5052222040244
At time: 650.8111088275909 and batch: 650, loss is 6.330142669677734 and perplexity is 561.236659594735
At time: 652.3767411708832 and batch: 700, loss is 6.296330871582032 and perplexity is 542.577467026302
At time: 653.9149234294891 and batch: 750, loss is 6.288701715469361 and perplexity is 538.4538088442591
At time: 655.4558839797974 and batch: 800, loss is 6.32062328338623 and perplexity is 555.9193798049637
At time: 656.9977378845215 and batch: 850, loss is 6.330181531906128 and perplexity is 561.2584709257987
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.89778200785319 and perplexity of 364.2287148637696
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 660.991758108139 and batch: 50, loss is 6.318706798553467 and perplexity is 554.8549890155147
At time: 662.5305423736572 and batch: 100, loss is 6.27960958480835 and perplexity is 533.580305297887
At time: 664.0705142021179 and batch: 150, loss is 6.253216257095337 and perplexity is 519.6815692083902
At time: 665.6091837882996 and batch: 200, loss is 6.307227649688721 and perplexity is 548.5221433773903
At time: 667.1454703807831 and batch: 250, loss is 6.337679357528686 and perplexity is 565.4825048160897
At time: 668.6814134120941 and batch: 300, loss is 6.29753924369812 and perplexity is 543.2334987936957
At time: 670.218866109848 and batch: 350, loss is 6.270704460144043 and perplexity is 528.8498002904023
At time: 671.7579770088196 and batch: 400, loss is 6.302681255340576 and perplexity is 546.0340057181565
At time: 673.2965769767761 and batch: 450, loss is 6.318677320480346 and perplexity is 554.8386332006475
At time: 674.8346474170685 and batch: 500, loss is 6.3118588161468505 and perplexity is 551.0683320903997
At time: 676.3719351291656 and batch: 550, loss is 6.261339960098266 and perplexity is 523.9205025382632
At time: 677.9076905250549 and batch: 600, loss is 6.275603666305542 and perplexity is 531.4471016528299
At time: 679.4449348449707 and batch: 650, loss is 6.299053430557251 and perplexity is 544.0566788859685
At time: 680.9832129478455 and batch: 700, loss is 6.267011928558349 and perplexity is 526.9006066430335
At time: 682.5203204154968 and batch: 750, loss is 6.257282447814942 and perplexity is 521.7989955945719
At time: 684.0596921443939 and batch: 800, loss is 6.290697841644287 and perplexity is 539.5297040401192
At time: 685.598739862442 and batch: 850, loss is 6.309659700393677 and perplexity is 549.8578005775399
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.884278615315755 and perplexity of 359.3434496127607
Finished 24 epochs...
Completing Train Step...
At time: 689.5764074325562 and batch: 50, loss is 6.30278510093689 and perplexity is 546.0907118893803
At time: 691.1444268226624 and batch: 100, loss is 6.264108276367187 and perplexity is 525.3728895948802
At time: 692.6815714836121 and batch: 150, loss is 6.239983282089233 and perplexity is 512.8499370916758
At time: 694.2207973003387 and batch: 200, loss is 6.296581172943116 and perplexity is 542.7132919026587
At time: 695.7635674476624 and batch: 250, loss is 6.329035491943359 and perplexity is 560.6156147284827
At time: 697.3169202804565 and batch: 300, loss is 6.290833826065064 and perplexity is 539.6030766630685
At time: 698.861398935318 and batch: 350, loss is 6.266096487045288 and perplexity is 526.4184806671914
At time: 700.4020035266876 and batch: 400, loss is 6.298148050308227 and perplexity is 543.5643236325492
At time: 701.9391942024231 and batch: 450, loss is 6.315350294113159 and perplexity is 552.9957378191511
At time: 703.4773368835449 and batch: 500, loss is 6.30992733001709 and perplexity is 550.0049785073476
At time: 705.0158755779266 and batch: 550, loss is 6.261898698806763 and perplexity is 524.2133189995343
At time: 706.5529363155365 and batch: 600, loss is 6.278598089218139 and perplexity is 533.0408640392749
At time: 708.0887081623077 and batch: 650, loss is 6.303028259277344 and perplexity is 546.223514546003
At time: 709.6254527568817 and batch: 700, loss is 6.271415519714355 and perplexity is 529.2259777285905
At time: 711.1628148555756 and batch: 750, loss is 6.2621302318573 and perplexity is 524.3347057603954
At time: 712.7007582187653 and batch: 800, loss is 6.29423867225647 and perplexity is 541.4434734996228
At time: 714.2385199069977 and batch: 850, loss is 6.311055631637573 and perplexity is 550.6259002434989
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.8838348388671875 and perplexity of 359.1840168317488
Finished 25 epochs...
Completing Train Step...
At time: 718.2045693397522 and batch: 50, loss is 6.298658056259155 and perplexity is 543.8416153765234
At time: 719.7730002403259 and batch: 100, loss is 6.259638662338257 and perplexity is 523.0299155525046
At time: 721.3149461746216 and batch: 150, loss is 6.235570011138916 and perplexity is 510.59157840233206
At time: 722.8569850921631 and batch: 200, loss is 6.292986307144165 and perplexity is 540.765813010875
At time: 724.3984310626984 and batch: 250, loss is 6.325913286209106 and perplexity is 558.8679870876668
At time: 725.9374346733093 and batch: 300, loss is 6.288602905273438 and perplexity is 538.4006067464097
At time: 727.4778437614441 and batch: 350, loss is 6.264938316345215 and perplexity is 525.8091511288255
At time: 729.0440948009491 and batch: 400, loss is 6.297730083465576 and perplexity is 543.3371792411416
At time: 730.5851466655731 and batch: 450, loss is 6.315409727096558 and perplexity is 553.0286049823436
At time: 732.1283941268921 and batch: 500, loss is 6.310327959060669 and perplexity is 550.2253706206436
At time: 733.669864654541 and batch: 550, loss is 6.263001289367676 and perplexity is 524.7916304187358
At time: 735.2108657360077 and batch: 600, loss is 6.280459098815918 and perplexity is 534.0337818314616
At time: 736.751314163208 and batch: 650, loss is 6.30493540763855 and perplexity is 547.2662378245668
At time: 738.2936627864838 and batch: 700, loss is 6.273407201766968 and perplexity is 530.281077973539
At time: 739.8353447914124 and batch: 750, loss is 6.264406690597534 and perplexity is 525.5296917362053
At time: 741.37682056427 and batch: 800, loss is 6.295754613876343 and perplexity is 542.2648926503414
At time: 742.9246573448181 and batch: 850, loss is 6.311501445770264 and perplexity is 550.8714317783007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.883759180704753 and perplexity of 359.15684265704647
Finished 26 epochs...
Completing Train Step...
At time: 746.8910465240479 and batch: 50, loss is 6.296432580947876 and perplexity is 542.6326550429161
At time: 748.4775886535645 and batch: 100, loss is 6.257338771820068 and perplexity is 521.8283862315662
At time: 750.0254905223846 and batch: 150, loss is 6.233233261108398 and perplexity is 509.39984644803656
At time: 751.5658802986145 and batch: 200, loss is 6.291053876876831 and perplexity is 539.721829823506
At time: 753.1071040630341 and batch: 250, loss is 6.324210805892944 and perplexity is 557.9173348033353
At time: 754.6550476551056 and batch: 300, loss is 6.2873883628845215 and perplexity is 537.7470933275065
At time: 756.209951877594 and batch: 350, loss is 6.264457321166992 and perplexity is 525.5563002773621
At time: 757.7625226974487 and batch: 400, loss is 6.297835140228272 and perplexity is 543.3942634847352
At time: 759.3135852813721 and batch: 450, loss is 6.315757045745849 and perplexity is 553.2207154902954
At time: 760.8692216873169 and batch: 500, loss is 6.310842800140381 and perplexity is 550.508722178791
At time: 762.4092683792114 and batch: 550, loss is 6.263811683654785 and perplexity is 525.217090930049
At time: 763.949422121048 and batch: 600, loss is 6.281603412628174 and perplexity is 534.6452338439922
At time: 765.4882230758667 and batch: 650, loss is 6.305989322662353 and perplexity is 547.8433139758282
At time: 767.0275990962982 and batch: 700, loss is 6.274442348480225 and perplexity is 530.8302808922631
At time: 768.5929236412048 and batch: 750, loss is 6.265638999938965 and perplexity is 526.1777060795899
At time: 770.1325867176056 and batch: 800, loss is 6.296535816192627 and perplexity is 542.6886767495268
At time: 771.6730387210846 and batch: 850, loss is 6.311693830490112 and perplexity is 550.9774212194233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.88372802734375 and perplexity of 359.1456538885553
Finished 27 epochs...
Completing Train Step...
At time: 775.6604363918304 and batch: 50, loss is 6.2949935245513915 and perplexity is 541.852337644769
At time: 777.1973977088928 and batch: 100, loss is 6.255897388458252 and perplexity is 521.076773289252
At time: 778.7395920753479 and batch: 150, loss is 6.231758289337158 and perplexity is 508.64904989223015
At time: 780.2812249660492 and batch: 200, loss is 6.289784965515136 and perplexity is 539.0374049905463
At time: 781.8210520744324 and batch: 250, loss is 6.323068418502808 and perplexity is 557.2803409914421
At time: 783.3606464862823 and batch: 300, loss is 6.286597061157226 and perplexity is 537.3217414367463
At time: 784.9019682407379 and batch: 350, loss is 6.264204359054565 and perplexity is 525.4233712591562
At time: 786.4424364566803 and batch: 400, loss is 6.298011140823364 and perplexity is 543.4899096151195
At time: 787.9830775260925 and batch: 450, loss is 6.316070699691773 and perplexity is 553.3942625661153
At time: 789.5387258529663 and batch: 500, loss is 6.311273775100708 and perplexity is 550.7460287864216
At time: 791.0994925498962 and batch: 550, loss is 6.264338274002075 and perplexity is 525.4937380138138
At time: 792.6409759521484 and batch: 600, loss is 6.282336854934693 and perplexity is 535.037509115527
At time: 794.1795470714569 and batch: 650, loss is 6.306623325347901 and perplexity is 548.1907582368073
At time: 795.7190337181091 and batch: 700, loss is 6.275046482086181 and perplexity is 531.1510701940515
At time: 797.2646114826202 and batch: 750, loss is 6.2663829803466795 and perplexity is 526.569317641491
At time: 798.8048579692841 and batch: 800, loss is 6.2969824314117435 and perplexity is 542.9311039036041
At time: 800.3448076248169 and batch: 850, loss is 6.311779928207398 and perplexity is 551.024861159872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.8837025960286455 and perplexity of 359.13652045840087
Finished 28 epochs...
Completing Train Step...
At time: 804.37833070755 and batch: 50, loss is 6.293955812454223 and perplexity is 541.290342564095
At time: 805.9166679382324 and batch: 100, loss is 6.254879026412964 and perplexity is 520.5463985832241
At time: 807.5009973049164 and batch: 150, loss is 6.230814256668091 and perplexity is 508.16909515416273
At time: 809.0398452281952 and batch: 200, loss is 6.288933429718018 and perplexity is 538.578590720323
At time: 810.5807085037231 and batch: 250, loss is 6.322307186126709 and perplexity is 556.8562825772908
At time: 812.119781255722 and batch: 300, loss is 6.286069555282593 and perplexity is 537.038375806658
At time: 813.6592819690704 and batch: 350, loss is 6.264047899246216 and perplexity is 525.3411700499472
At time: 815.1990127563477 and batch: 400, loss is 6.298152561187744 and perplexity is 543.5667755912531
At time: 816.7387640476227 and batch: 450, loss is 6.31630160331726 and perplexity is 553.5220580613196
At time: 818.2776045799255 and batch: 500, loss is 6.311530542373657 and perplexity is 550.8874604990614
At time: 819.8169901371002 and batch: 550, loss is 6.264752120971679 and perplexity is 525.7112570115214
At time: 821.3560655117035 and batch: 600, loss is 6.28290867805481 and perplexity is 535.3435434237765
At time: 822.8955552577972 and batch: 650, loss is 6.307078790664673 and perplexity is 548.4404969835106
At time: 824.4351680278778 and batch: 700, loss is 6.275371112823486 and perplexity is 531.3235261483263
At time: 825.9747207164764 and batch: 750, loss is 6.2667965316772465 and perplexity is 526.7871261178258
At time: 827.5137054920197 and batch: 800, loss is 6.297264223098755 and perplexity is 543.0841189334732
At time: 829.0513961315155 and batch: 850, loss is 6.311797561645508 and perplexity is 551.0345777083265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.883678436279297 and perplexity of 359.12784391489663
Finished 29 epochs...
Completing Train Step...
At time: 833.0749406814575 and batch: 50, loss is 6.29319501876831 and perplexity is 540.8786889008361
At time: 834.6170122623444 and batch: 100, loss is 6.254126262664795 and perplexity is 520.1546975727877
At time: 836.1581003665924 and batch: 150, loss is 6.230048904418945 and perplexity is 507.7803155898749
At time: 837.7006275653839 and batch: 200, loss is 6.288245973587036 and perplexity is 538.2084688019842
At time: 839.2427515983582 and batch: 250, loss is 6.3216661357879635 and perplexity is 556.4994240630687
At time: 840.7831430435181 and batch: 300, loss is 6.285648193359375 and perplexity is 536.812135951564
At time: 842.3245997428894 and batch: 350, loss is 6.2639267539978025 and perplexity is 525.2775313182426
At time: 843.8657565116882 and batch: 400, loss is 6.298302059173584 and perplexity is 543.648043803939
At time: 845.4085347652435 and batch: 450, loss is 6.316506185531616 and perplexity is 553.6353104139662
At time: 846.9942417144775 and batch: 500, loss is 6.3117807006835935 and perplexity is 551.025286813625
At time: 848.5356202125549 and batch: 550, loss is 6.265057563781738 and perplexity is 525.871856260837
At time: 850.0764091014862 and batch: 600, loss is 6.283310317993164 and perplexity is 535.5586019567129
At time: 851.6172320842743 and batch: 650, loss is 6.307400665283203 and perplexity is 548.6170544724265
At time: 853.1596875190735 and batch: 700, loss is 6.275575218200683 and perplexity is 531.431983205002
At time: 854.7027933597565 and batch: 750, loss is 6.2670684432983395 and perplexity is 526.9303851352724
At time: 856.2448399066925 and batch: 800, loss is 6.297446994781494 and perplexity is 543.1833884033075
At time: 857.7870016098022 and batch: 850, loss is 6.311791887283325 and perplexity is 551.0314509474285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.883647282918294 and perplexity of 359.1166560497997
Finished 30 epochs...
Completing Train Step...
At time: 861.7593204975128 and batch: 50, loss is 6.2925796222686765 and perplexity is 540.5459364468092
At time: 863.3242678642273 and batch: 100, loss is 6.253505039215088 and perplexity is 519.8316656250792
At time: 864.8623127937317 and batch: 150, loss is 6.229481086730957 and perplexity is 507.492070788064
At time: 866.4001984596252 and batch: 200, loss is 6.2877383518219 and perplexity is 537.9353318000484
At time: 867.9378523826599 and batch: 250, loss is 6.321182193756104 and perplexity is 556.2301757565999
At time: 869.4777748584747 and batch: 300, loss is 6.285325269699097 and perplexity is 536.63881459803
At time: 871.0149078369141 and batch: 350, loss is 6.26384295463562 and perplexity is 525.2335152404346
At time: 872.5536434650421 and batch: 400, loss is 6.298425569534301 and perplexity is 543.7151941167269
At time: 874.1019396781921 and batch: 450, loss is 6.316650514602661 and perplexity is 553.7152218506488
At time: 875.6551949977875 and batch: 500, loss is 6.311948709487915 and perplexity is 551.1178716905328
At time: 877.2069809436798 and batch: 550, loss is 6.265297956466675 and perplexity is 525.9982872042244
At time: 878.7618362903595 and batch: 600, loss is 6.283631505966187 and perplexity is 535.7306445660396
At time: 880.3084833621979 and batch: 650, loss is 6.307642126083374 and perplexity is 548.7495399797716
At time: 881.8516855239868 and batch: 700, loss is 6.275696487426758 and perplexity is 531.4964334581537
At time: 883.3895351886749 and batch: 750, loss is 6.267258672714234 and perplexity is 527.0306323293339
At time: 884.9742467403412 and batch: 800, loss is 6.297566509246826 and perplexity is 543.2483105550403
At time: 886.514820098877 and batch: 850, loss is 6.311765918731689 and perplexity is 551.0171416445381
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.8836212158203125 and perplexity of 359.1072950427473
Finished 31 epochs...
Completing Train Step...
At time: 890.4908094406128 and batch: 50, loss is 6.292086668014527 and perplexity is 540.2795376944681
At time: 892.055906534195 and batch: 100, loss is 6.252991743087769 and perplexity is 519.5649065133225
At time: 893.5942466259003 and batch: 150, loss is 6.229001121520996 and perplexity is 507.24855069501575
At time: 895.1311919689178 and batch: 200, loss is 6.2873047351837155 and perplexity is 537.7021246548159
At time: 896.6694884300232 and batch: 250, loss is 6.320768260955811 and perplexity is 555.9999814881073
At time: 898.2066760063171 and batch: 300, loss is 6.28506293296814 and perplexity is 536.498052989981
At time: 899.7442245483398 and batch: 350, loss is 6.263781414031983 and perplexity is 525.2011930474301
At time: 901.2814249992371 and batch: 400, loss is 6.298526725769043 and perplexity is 543.770197080435
At time: 902.8176822662354 and batch: 450, loss is 6.316765403747558 and perplexity is 553.7788413735301
At time: 904.3557567596436 and batch: 500, loss is 6.3120969581604 and perplexity is 551.1995802398363
At time: 905.8931632041931 and batch: 550, loss is 6.265498533248901 and perplexity is 526.1038008295661
At time: 907.4307813644409 and batch: 600, loss is 6.283893089294434 and perplexity is 535.8708011015965
At time: 908.9682426452637 and batch: 650, loss is 6.307825345993042 and perplexity is 548.8500910321127
At time: 910.5070905685425 and batch: 700, loss is 6.275783109664917 and perplexity is 531.5424748628694
At time: 912.0452437400818 and batch: 750, loss is 6.267403230667115 and perplexity is 527.1068243055944
At time: 913.5839447975159 and batch: 800, loss is 6.297655096054077 and perplexity is 543.2964373200839
At time: 915.122927904129 and batch: 850, loss is 6.311734447479248 and perplexity is 550.9998007178457
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.883597056070964 and perplexity of 359.09861920531324
Finished 32 epochs...
Completing Train Step...
At time: 919.1041004657745 and batch: 50, loss is 6.291655530929566 and perplexity is 540.0466533556673
At time: 920.6898064613342 and batch: 100, loss is 6.25254340171814 and perplexity is 519.3320162825888
At time: 922.2304973602295 and batch: 150, loss is 6.228615131378174 and perplexity is 507.0527955366977
At time: 923.7979266643524 and batch: 200, loss is 6.2869540977478025 and perplexity is 537.513619211006
At time: 925.3378443717957 and batch: 250, loss is 6.32041805267334 and perplexity is 555.8052997810992
At time: 926.8792035579681 and batch: 300, loss is 6.284855842590332 and perplexity is 536.3869609089415
At time: 928.4210391044617 and batch: 350, loss is 6.2637230396270756 and perplexity is 525.1705356351422
At time: 929.9630954265594 and batch: 400, loss is 6.298599967956543 and perplexity is 543.8100254577073
At time: 931.505704164505 and batch: 450, loss is 6.316853942871094 and perplexity is 553.8278746374272
At time: 933.0473704338074 and batch: 500, loss is 6.312206707000732 and perplexity is 551.2600770742272
At time: 934.5879735946655 and batch: 550, loss is 6.265653352737427 and perplexity is 526.1852582563581
At time: 936.1291360855103 and batch: 600, loss is 6.2841027736663815 and perplexity is 535.9831766152532
At time: 937.6708734035492 and batch: 650, loss is 6.307962245941162 and perplexity is 548.9252337244991
At time: 939.2126915454865 and batch: 700, loss is 6.275834941864014 and perplexity is 531.5700265922824
At time: 940.7541522979736 and batch: 750, loss is 6.267508687973023 and perplexity is 527.1624145023562
At time: 942.2961111068726 and batch: 800, loss is 6.297716512680053 and perplexity is 543.3298057788475
At time: 943.8384976387024 and batch: 850, loss is 6.311696176528931 and perplexity is 550.9787138353577
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.883588790893555 and perplexity of 359.0956512037837
Finished 33 epochs...
Completing Train Step...
At time: 947.8605332374573 and batch: 50, loss is 6.291285552978516 and perplexity is 539.8468849586172
At time: 949.4020445346832 and batch: 100, loss is 6.252164325714111 and perplexity is 519.135187286024
At time: 950.9416878223419 and batch: 150, loss is 6.228291244506836 and perplexity is 506.8885943858821
At time: 952.4828453063965 and batch: 200, loss is 6.286651601791382 and perplexity is 537.3510481044647
At time: 954.0231249332428 and batch: 250, loss is 6.320109672546387 and perplexity is 555.6339268975457
At time: 955.5624899864197 and batch: 300, loss is 6.28467963218689 and perplexity is 536.2924522731058
At time: 957.1057889461517 and batch: 350, loss is 6.263661155700683 and perplexity is 525.1380370259527
At time: 958.6458451747894 and batch: 400, loss is 6.298661890029908 and perplexity is 543.8437003445989
At time: 960.1867184638977 and batch: 450, loss is 6.316932926177978 and perplexity is 553.8716195219457
At time: 961.727085351944 and batch: 500, loss is 6.312288131713867 and perplexity is 551.3049650953378
At time: 963.2939007282257 and batch: 550, loss is 6.265771512985229 and perplexity is 526.2474361102666
At time: 964.8323979377747 and batch: 600, loss is 6.284281082153321 and perplexity is 536.0787554855089
At time: 966.3712725639343 and batch: 650, loss is 6.308060474395752 and perplexity is 548.9791564502228
At time: 967.9114017486572 and batch: 700, loss is 6.275857915878296 and perplexity is 531.582239029949
At time: 969.4514272212982 and batch: 750, loss is 6.267580232620239 and perplexity is 527.2001315005361
At time: 970.9921448230743 and batch: 800, loss is 6.297744274139404 and perplexity is 543.3448896165385
At time: 972.5324485301971 and batch: 850, loss is 6.31164080619812 and perplexity is 550.9482068063032
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.883591969807942 and perplexity of 359.0967927399303
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 976.5460515022278 and batch: 50, loss is 6.291572532653809 and perplexity is 540.0018322746723
At time: 978.0885989665985 and batch: 100, loss is 6.252352705001831 and perplexity is 519.2329908146256
At time: 979.6274712085724 and batch: 150, loss is 6.225882349014282 and perplexity is 505.669022236308
At time: 981.168692111969 and batch: 200, loss is 6.285242252349853 and perplexity is 536.5942661153148
At time: 982.7084350585938 and batch: 250, loss is 6.315998058319092 and perplexity is 553.3540647072805
At time: 984.2465960979462 and batch: 300, loss is 6.279839649200439 and perplexity is 533.7030772486416
At time: 985.7840824127197 and batch: 350, loss is 6.258296890258789 and perplexity is 522.3285992236422
At time: 987.3219840526581 and batch: 400, loss is 6.2917939567565915 and perplexity is 540.1214149346379
At time: 988.8608286380768 and batch: 450, loss is 6.308187971115112 and perplexity is 549.0491539537984
At time: 990.4014115333557 and batch: 500, loss is 6.302856225967407 and perplexity is 546.1295539892346
At time: 991.9467024803162 and batch: 550, loss is 6.2568784427642825 and perplexity is 521.5882287432258
At time: 993.4846856594086 and batch: 600, loss is 6.272025957107544 and perplexity is 529.549135678656
At time: 995.0332646369934 and batch: 650, loss is 6.295053033828736 and perplexity is 541.884583845274
At time: 996.5725889205933 and batch: 700, loss is 6.264732341766358 and perplexity is 525.700858963462
At time: 998.1109409332275 and batch: 750, loss is 6.254433212280273 and perplexity is 520.314383363689
At time: 999.6492972373962 and batch: 800, loss is 6.286999683380127 and perplexity is 537.5381226677196
At time: 1001.1899871826172 and batch: 850, loss is 6.303396816253662 and perplexity is 546.4248661354053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.881086985270183 and perplexity of 358.198386543161
Finished 35 epochs...
Completing Train Step...
At time: 1005.2515740394592 and batch: 50, loss is 6.288684577941894 and perplexity is 538.4445811563907
At time: 1006.7903101444244 and batch: 100, loss is 6.249725437164306 and perplexity is 517.8706171216223
At time: 1008.3291311264038 and batch: 150, loss is 6.223149061203003 and perplexity is 504.28877043330885
At time: 1009.8686106204987 and batch: 200, loss is 6.282881174087525 and perplexity is 535.3288195549553
At time: 1011.4165596961975 and batch: 250, loss is 6.314223432540894 and perplexity is 552.3729391424276
At time: 1012.954836845398 and batch: 300, loss is 6.278345956802368 and perplexity is 532.9064841000011
At time: 1014.4929943084717 and batch: 350, loss is 6.25706316947937 and perplexity is 521.6845889232254
At time: 1016.030689239502 and batch: 400, loss is 6.290495948791504 and perplexity is 539.4207878440799
At time: 1017.5685184001923 and batch: 450, loss is 6.3071144008636475 and perplexity is 548.4600274064729
At time: 1019.1071076393127 and batch: 500, loss is 6.3021015930175786 and perplexity is 545.7175820962523
At time: 1020.6446628570557 and batch: 550, loss is 6.256969890594482 and perplexity is 521.6359290360131
At time: 1022.1821887493134 and batch: 600, loss is 6.27277907371521 and perplexity is 529.9480981410819
At time: 1023.720242023468 and batch: 650, loss is 6.296319494247436 and perplexity is 542.5712939760323
At time: 1025.2659738063812 and batch: 700, loss is 6.266307983398438 and perplexity is 526.5298280304487
At time: 1026.803885936737 and batch: 750, loss is 6.255951900482177 and perplexity is 521.105179013004
At time: 1028.340565443039 and batch: 800, loss is 6.288259906768799 and perplexity is 538.2159678106487
At time: 1029.8800873756409 and batch: 850, loss is 6.3042182350158695 and perplexity is 546.8738941673629
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.881242752075195 and perplexity of 358.2541863071562
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 1033.8802807331085 and batch: 50, loss is 6.288338146209717 and perplexity is 538.2580791744267
At time: 1035.4487965106964 and batch: 100, loss is 6.2488578414916995 and perplexity is 517.4215096652514
At time: 1036.9902439117432 and batch: 150, loss is 6.222521724700928 and perplexity is 503.97251089097915
At time: 1038.531890153885 and batch: 200, loss is 6.282521772384643 and perplexity is 535.1364560355647
At time: 1040.0728731155396 and batch: 250, loss is 6.312287006378174 and perplexity is 551.3043446925319
At time: 1041.658769607544 and batch: 300, loss is 6.27637713432312 and perplexity is 531.8583179998385
At time: 1043.2006583213806 and batch: 350, loss is 6.255169935226441 and perplexity is 520.6978521469156
At time: 1044.7429957389832 and batch: 400, loss is 6.2868843841552735 and perplexity is 537.4761485117014
At time: 1046.2848839759827 and batch: 450, loss is 6.303455057144165 and perplexity is 546.456691332957
At time: 1047.8265755176544 and batch: 500, loss is 6.299634799957276 and perplexity is 544.3730687517337
At time: 1049.3691747188568 and batch: 550, loss is 6.253988456726074 and perplexity is 520.0830221051746
At time: 1050.9100031852722 and batch: 600, loss is 6.268749523162842 and perplexity is 527.8169421735242
At time: 1052.4586727619171 and batch: 650, loss is 6.29126088142395 and perplexity is 539.8335662610351
At time: 1053.9998457431793 and batch: 700, loss is 6.261774559020996 and perplexity is 524.1482473094937
At time: 1055.5407798290253 and batch: 750, loss is 6.251523475646973 and perplexity is 518.8026060451356
At time: 1057.0825765132904 and batch: 800, loss is 6.284118165969849 and perplexity is 535.9914266944547
At time: 1058.62326836586 and batch: 850, loss is 6.3017441940307615 and perplexity is 545.5225780345256
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.882192611694336 and perplexity of 358.5946391577276
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 1062.6121649742126 and batch: 50, loss is 6.287776651382447 and perplexity is 537.9559348814009
At time: 1064.1777606010437 and batch: 100, loss is 6.247695560455322 and perplexity is 516.820469813027
At time: 1065.7222754955292 and batch: 150, loss is 6.222043895721436 and perplexity is 503.73175574488164
At time: 1067.2622961997986 and batch: 200, loss is 6.281842365264892 and perplexity is 534.7730039972276
At time: 1068.8015785217285 and batch: 250, loss is 6.311628246307373 and perplexity is 550.9412870004745
At time: 1070.3407752513885 and batch: 300, loss is 6.275342845916748 and perplexity is 531.3085074880315
At time: 1071.8810217380524 and batch: 350, loss is 6.254588642120361 and perplexity is 520.3952620304071
At time: 1073.4317047595978 and batch: 400, loss is 6.285561513900757 and perplexity is 536.7656073828049
At time: 1074.9812467098236 and batch: 450, loss is 6.302753705978393 and perplexity is 546.0735676632675
At time: 1076.5205109119415 and batch: 500, loss is 6.299030599594116 and perplexity is 544.0442576897839
At time: 1078.069576740265 and batch: 550, loss is 6.253488569259644 and perplexity is 519.8231040912121
At time: 1079.6596455574036 and batch: 600, loss is 6.2675243091583255 and perplexity is 527.1706494684375
At time: 1081.2148053646088 and batch: 650, loss is 6.289547967910766 and perplexity is 538.9096695539919
At time: 1082.7627792358398 and batch: 700, loss is 6.259889783859253 and perplexity is 523.1612761134663
At time: 1084.3040475845337 and batch: 750, loss is 6.250611371994019 and perplexity is 518.3296200319259
At time: 1085.8481419086456 and batch: 800, loss is 6.2832286930084225 and perplexity is 535.5148887780678
At time: 1087.3890738487244 and batch: 850, loss is 6.301132793426514 and perplexity is 545.189147140996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.881412506103516 and perplexity of 358.3150065605399
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 1091.3758420944214 and batch: 50, loss is 6.287836647033691 and perplexity is 537.9882108662547
At time: 1092.9680528640747 and batch: 100, loss is 6.247344493865967 and perplexity is 516.639063258134
At time: 1094.50705575943 and batch: 150, loss is 6.222061195373535 and perplexity is 503.7404702043857
At time: 1096.0440623760223 and batch: 200, loss is 6.2811681842803955 and perplexity is 534.4125917121163
At time: 1097.5830161571503 and batch: 250, loss is 6.311188039779663 and perplexity is 550.6988124229132
At time: 1099.1206617355347 and batch: 300, loss is 6.274950456619263 and perplexity is 531.1000686132965
At time: 1100.6592211723328 and batch: 350, loss is 6.254384031295777 and perplexity is 520.2887944193174
At time: 1102.1966395378113 and batch: 400, loss is 6.2852551555633545 and perplexity is 536.6011899503637
At time: 1103.7358355522156 and batch: 450, loss is 6.302577152252197 and perplexity is 545.9771648505075
At time: 1105.2730658054352 and batch: 500, loss is 6.298996534347534 and perplexity is 544.0257250036564
At time: 1106.8116328716278 and batch: 550, loss is 6.253304824829102 and perplexity is 519.7275982655689
At time: 1108.3515040874481 and batch: 600, loss is 6.26705075263977 and perplexity is 526.9210634721928
At time: 1109.8933160305023 and batch: 650, loss is 6.289068546295166 and perplexity is 538.651366532525
At time: 1111.4305548667908 and batch: 700, loss is 6.2594191932678225 and perplexity is 522.9151392585128
At time: 1112.9691891670227 and batch: 750, loss is 6.250294198989868 and perplexity is 518.1652459380907
At time: 1114.5074548721313 and batch: 800, loss is 6.282951002120972 and perplexity is 535.3662018188221
At time: 1116.0475251674652 and batch: 850, loss is 6.300666103363037 and perplexity is 544.9347721450683
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.880546569824219 and perplexity of 358.00486289864693
Finished 39 epochs...
Completing Train Step...
At time: 1120.0681011676788 and batch: 50, loss is 6.2875094318389895 and perplexity is 537.8122017470885
At time: 1121.6083869934082 and batch: 100, loss is 6.247083644866944 and perplexity is 516.504316050728
At time: 1123.1499876976013 and batch: 150, loss is 6.2216717147827145 and perplexity is 503.5443112709586
At time: 1124.7123069763184 and batch: 200, loss is 6.28096908569336 and perplexity is 534.306201511633
At time: 1126.2669672966003 and batch: 250, loss is 6.311050930023193 and perplexity is 550.6233114189343
At time: 1127.814091205597 and batch: 300, loss is 6.274880838394165 and perplexity is 531.0630956561811
At time: 1129.359093427658 and batch: 350, loss is 6.254266185760498 and perplexity is 520.2274843204711
At time: 1130.9016227722168 and batch: 400, loss is 6.285240268707275 and perplexity is 536.593201705137
At time: 1132.443858385086 and batch: 450, loss is 6.302656421661377 and perplexity is 546.020445853198
At time: 1133.985533952713 and batch: 500, loss is 6.29908224105835 and perplexity is 544.0723536573112
At time: 1135.5271413326263 and batch: 550, loss is 6.253344669342041 and perplexity is 519.7483069711441
At time: 1137.0683057308197 and batch: 600, loss is 6.267091979980469 and perplexity is 526.9427874742062
At time: 1138.6114864349365 and batch: 650, loss is 6.2891138076782225 and perplexity is 538.6757471901066
At time: 1140.1527571678162 and batch: 700, loss is 6.259478464126587 and perplexity is 522.946133806405
At time: 1141.6944818496704 and batch: 750, loss is 6.250456943511963 and perplexity is 518.2495813557842
At time: 1143.2362558841705 and batch: 800, loss is 6.283067398071289 and perplexity is 535.4285199033665
At time: 1144.785347700119 and batch: 850, loss is 6.300717334747315 and perplexity is 544.9626906229315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.880535125732422 and perplexity of 358.00076588157555
Finished 40 epochs...
Completing Train Step...
At time: 1148.8109920024872 and batch: 50, loss is 6.287357158660889 and perplexity is 537.7303136087489
At time: 1150.351175069809 and batch: 100, loss is 6.246965799331665 and perplexity is 516.4434519094834
At time: 1151.8927886486053 and batch: 150, loss is 6.2215006637573245 and perplexity is 503.4581868662302
At time: 1153.431473493576 and batch: 200, loss is 6.280796136856079 and perplexity is 534.2138018657657
At time: 1154.972951889038 and batch: 250, loss is 6.310933265686035 and perplexity is 550.5585265034842
At time: 1156.5123226642609 and batch: 300, loss is 6.27481406211853 and perplexity is 531.0276344245235
At time: 1158.0975496768951 and batch: 350, loss is 6.254176235198974 and perplexity is 520.1806916706806
At time: 1159.636734008789 and batch: 400, loss is 6.285246591567994 and perplexity is 536.59659451994
At time: 1161.1767551898956 and batch: 450, loss is 6.302716274261474 and perplexity is 546.053127574622
At time: 1162.7175199985504 and batch: 500, loss is 6.299150667190552 and perplexity is 544.1095836978494
At time: 1164.2567636966705 and batch: 550, loss is 6.25339054107666 and perplexity is 519.7721492743901
At time: 1165.7977192401886 and batch: 600, loss is 6.267138595581055 and perplexity is 526.9673518012546
At time: 1167.3389387130737 and batch: 650, loss is 6.289141635894776 and perplexity is 538.6907377840312
At time: 1168.8808648586273 and batch: 700, loss is 6.259529361724853 and perplexity is 522.9727511860133
At time: 1170.4277095794678 and batch: 750, loss is 6.250583219528198 and perplexity is 518.3150279804161
At time: 1171.9689116477966 and batch: 800, loss is 6.283161153793335 and perplexity is 535.4787217441724
At time: 1173.5098204612732 and batch: 850, loss is 6.300762691497803 and perplexity is 544.987408920282
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.880540211995442 and perplexity of 358.00258677226316
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 1177.5318820476532 and batch: 50, loss is 6.287302551269531 and perplexity is 537.700950360801
At time: 1179.0704832077026 and batch: 100, loss is 6.24691104888916 and perplexity is 516.4151771759966
At time: 1180.6092100143433 and batch: 150, loss is 6.221502122879028 and perplexity is 503.4589214735334
At time: 1182.146716594696 and batch: 200, loss is 6.280586547851563 and perplexity is 534.1018482593661
At time: 1183.6853301525116 and batch: 250, loss is 6.310820074081421 and perplexity is 550.4962114272732
At time: 1185.2226922512054 and batch: 300, loss is 6.274719009399414 and perplexity is 530.9771612027921
At time: 1186.7623147964478 and batch: 350, loss is 6.25417610168457 and perplexity is 520.1806222190702
At time: 1188.3012280464172 and batch: 400, loss is 6.285186748504639 and perplexity is 536.564483896747
At time: 1189.8397874832153 and batch: 450, loss is 6.302619876861573 and perplexity is 546.0004920099221
At time: 1191.3809294700623 and batch: 500, loss is 6.299117946624756 and perplexity is 544.0917804156844
At time: 1192.9232444763184 and batch: 550, loss is 6.253248472213745 and perplexity is 519.6983110813458
At time: 1194.46284532547 and batch: 600, loss is 6.266964511871338 and perplexity is 526.8756233541993
At time: 1195.9996473789215 and batch: 650, loss is 6.288896322250366 and perplexity is 538.5586058034871
At time: 1197.5835194587708 and batch: 700, loss is 6.259325847625733 and perplexity is 522.8663296871961
At time: 1199.1213142871857 and batch: 750, loss is 6.250487117767334 and perplexity is 518.2652193869301
At time: 1200.660392999649 and batch: 800, loss is 6.2830592632293705 and perplexity is 535.4241642947146
At time: 1202.1985869407654 and batch: 850, loss is 6.30063835144043 and perplexity is 544.9196493672899
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.880491256713867 and perplexity of 357.98506108381406
Finished 42 epochs...
Completing Train Step...
At time: 1206.1765022277832 and batch: 50, loss is 6.287218341827392 and perplexity is 537.6556727701609
At time: 1207.7415380477905 and batch: 100, loss is 6.246820840835571 and perplexity is 516.3685944691192
At time: 1209.2794346809387 and batch: 150, loss is 6.221353168487549 and perplexity is 503.38393464119827
At time: 1210.8188228607178 and batch: 200, loss is 6.280564041137695 and perplexity is 534.0898275171653
At time: 1212.3568711280823 and batch: 250, loss is 6.310772638320923 and perplexity is 550.4700988401735
At time: 1213.8943836688995 and batch: 300, loss is 6.274703445434571 and perplexity is 530.9688971572335
At time: 1215.431305885315 and batch: 350, loss is 6.254163074493408 and perplexity is 520.1738457708049
At time: 1216.9696271419525 and batch: 400, loss is 6.285175352096558 and perplexity is 536.5583690237704
At time: 1218.5067751407623 and batch: 450, loss is 6.302631530761719 and perplexity is 546.0068550822131
At time: 1220.0460321903229 and batch: 500, loss is 6.299116334915161 and perplexity is 544.0909034984479
At time: 1221.5834946632385 and batch: 550, loss is 6.253246583938599 and perplexity is 519.697329748868
At time: 1223.1211216449738 and batch: 600, loss is 6.266967649459839 and perplexity is 526.87727647569
At time: 1224.6585841178894 and batch: 650, loss is 6.288919706344604 and perplexity is 538.5711996559252
At time: 1226.1971576213837 and batch: 700, loss is 6.259348049163818 and perplexity is 522.8779382527916
At time: 1227.736585855484 and batch: 750, loss is 6.250548620223999 and perplexity is 518.2970949513289
At time: 1229.274751663208 and batch: 800, loss is 6.283121347427368 and perplexity is 535.457406706447
At time: 1230.8139743804932 and batch: 850, loss is 6.300655918121338 and perplexity is 544.9292218809693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.880449930826823 and perplexity of 357.9702673393007
Finished 43 epochs...
Completing Train Step...
At time: 1234.8204913139343 and batch: 50, loss is 6.287153463363648 and perplexity is 537.6207916276178
At time: 1236.387330532074 and batch: 100, loss is 6.246771879196167 and perplexity is 516.3433128351173
At time: 1237.9297125339508 and batch: 150, loss is 6.221281251907349 and perplexity is 503.34773429180933
At time: 1239.4712407588959 and batch: 200, loss is 6.280514945983887 and perplexity is 534.0636069385929
At time: 1241.012012720108 and batch: 250, loss is 6.310739116668701 and perplexity is 550.45164648224
At time: 1242.5543138980865 and batch: 300, loss is 6.274688453674316 and perplexity is 530.9609370584927
At time: 1244.0947873592377 and batch: 350, loss is 6.254145107269287 and perplexity is 520.1644997746971
At time: 1245.6370885372162 and batch: 400, loss is 6.285173435211181 and perplexity is 536.5573405038649
At time: 1247.1771171092987 and batch: 450, loss is 6.302653789520264 and perplexity is 546.0190086522254
At time: 1248.7194488048553 and batch: 500, loss is 6.299139595031738 and perplexity is 544.1035592634784
At time: 1250.2606689929962 and batch: 550, loss is 6.253258600234985 and perplexity is 519.7035746235335
At time: 1251.8019425868988 and batch: 600, loss is 6.266973705291748 and perplexity is 526.880467165574
At time: 1253.3418982028961 and batch: 650, loss is 6.2889354801177975 and perplexity is 538.5796950228788
At time: 1254.8827197551727 and batch: 700, loss is 6.25936731338501 and perplexity is 522.8880111860736
At time: 1256.4242935180664 and batch: 750, loss is 6.250603494644165 and perplexity is 518.3255369842509
At time: 1257.9665491580963 and batch: 800, loss is 6.28316330909729 and perplexity is 535.4798758648229
At time: 1259.5074589252472 and batch: 850, loss is 6.300668439865112 and perplexity is 544.9360453877819
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.880434036254883 and perplexity of 357.9645776003521
Finished 44 epochs...
Completing Train Step...
At time: 1263.512743473053 and batch: 50, loss is 6.287100496292115 and perplexity is 537.5923161828271
At time: 1265.0512256622314 and batch: 100, loss is 6.24673056602478 and perplexity is 516.3219814959754
At time: 1266.5907790660858 and batch: 150, loss is 6.221223783493042 and perplexity is 503.31880852684174
At time: 1268.129690170288 and batch: 200, loss is 6.280465478897095 and perplexity is 534.0371890212101
At time: 1269.6701567173004 and batch: 250, loss is 6.310706596374512 and perplexity is 550.4337459238268
At time: 1271.2095308303833 and batch: 300, loss is 6.274673795700073 and perplexity is 530.953154303793
At time: 1272.749653339386 and batch: 350, loss is 6.254127588272095 and perplexity is 520.1553870941086
At time: 1274.2880070209503 and batch: 400, loss is 6.285174055099487 and perplexity is 536.5576731095889
At time: 1275.85395860672 and batch: 450, loss is 6.302674474716187 and perplexity is 546.0303032792125
At time: 1277.3922832012177 and batch: 500, loss is 6.29916277885437 and perplexity is 544.1161738101162
At time: 1278.932015657425 and batch: 550, loss is 6.25327166557312 and perplexity is 519.7103647708236
At time: 1280.468960762024 and batch: 600, loss is 6.266982116699219 and perplexity is 526.8848989905108
At time: 1282.007977962494 and batch: 650, loss is 6.288946552276611 and perplexity is 538.5856582958091
At time: 1283.545486688614 and batch: 700, loss is 6.2593849086761475 and perplexity is 522.897211633805
At time: 1285.0860407352448 and batch: 750, loss is 6.250651960372925 and perplexity is 518.3506586179
At time: 1286.6251504421234 and batch: 800, loss is 6.283198947906494 and perplexity is 535.4989600700187
At time: 1288.1717276573181 and batch: 850, loss is 6.300681705474854 and perplexity is 544.9432743446421
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.880431493123372 and perplexity of 357.96366725051263
Finished 45 epochs...
Completing Train Step...
At time: 1292.1951677799225 and batch: 50, loss is 6.287055292129517 and perplexity is 537.568015321609
At time: 1293.733947277069 and batch: 100, loss is 6.246692571640015 and perplexity is 516.302364532617
At time: 1295.271907567978 and batch: 150, loss is 6.221171703338623 and perplexity is 503.29259628814617
At time: 1296.8096849918365 and batch: 200, loss is 6.280416822433471 and perplexity is 534.0112052922922
At time: 1298.348242521286 and batch: 250, loss is 6.310675125122071 and perplexity is 550.4164233570394
At time: 1299.8867926597595 and batch: 300, loss is 6.2746594047546385 and perplexity is 530.945513440901
At time: 1301.4262886047363 and batch: 350, loss is 6.2541107559204105 and perplexity is 520.1466317293896
At time: 1302.9645714759827 and batch: 400, loss is 6.2851762866973875 and perplexity is 536.5588704919016
At time: 1304.502738237381 and batch: 450, loss is 6.302693328857422 and perplexity is 546.0405983087207
At time: 1306.0418736934662 and batch: 500, loss is 6.299184913635254 and perplexity is 544.1282178356942
At time: 1307.5819494724274 and batch: 550, loss is 6.253284702301025 and perplexity is 519.7171401376029
At time: 1309.1294786930084 and batch: 600, loss is 6.266991157531738 and perplexity is 526.8896624901723
At time: 1310.6747262477875 and batch: 650, loss is 6.288954782485962 and perplexity is 538.5900909867711
At time: 1312.2125766277313 and batch: 700, loss is 6.259402046203613 and perplexity is 522.9061728759176
At time: 1313.7498626708984 and batch: 750, loss is 6.25069595336914 and perplexity is 518.3734629180741
At time: 1315.3148300647736 and batch: 800, loss is 6.283231391906738 and perplexity is 535.5163340802494
At time: 1316.852233171463 and batch: 850, loss is 6.30069504737854 and perplexity is 544.9505449738251
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.880433400472005 and perplexity of 357.9643500126751
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 1320.8544194698334 and batch: 50, loss is 6.2870323181152346 and perplexity is 537.555665368212
At time: 1322.3967134952545 and batch: 100, loss is 6.246665048599243 and perplexity is 516.2881545171398
At time: 1323.938482284546 and batch: 150, loss is 6.221152925491333 and perplexity is 503.283145625363
At time: 1325.4796543121338 and batch: 200, loss is 6.280360174179077 and perplexity is 533.980955346497
At time: 1327.0199146270752 and batch: 250, loss is 6.310631246566772 and perplexity is 550.3922724094282
At time: 1328.5623936653137 and batch: 300, loss is 6.274632968902588 and perplexity is 530.9314776293859
At time: 1330.1116247177124 and batch: 350, loss is 6.2541233158111575 and perplexity is 520.1531647552835
At time: 1331.6605682373047 and batch: 400, loss is 6.285152339935303 and perplexity is 536.5460217981285
At time: 1333.2007179260254 and batch: 450, loss is 6.302642288208008 and perplexity is 546.0127287532231
At time: 1334.7429721355438 and batch: 500, loss is 6.299147262573242 and perplexity is 544.1077312160958
At time: 1336.2852230072021 and batch: 550, loss is 6.253218650817871 and perplexity is 519.6828131833618
At time: 1337.8271007537842 and batch: 600, loss is 6.266929807662964 and perplexity is 526.8573388700555
At time: 1339.3675985336304 and batch: 650, loss is 6.288868112564087 and perplexity is 538.5434134484611
At time: 1340.910301208496 and batch: 700, loss is 6.259334268569947 and perplexity is 522.8707327339285
At time: 1342.4503717422485 and batch: 750, loss is 6.250659999847412 and perplexity is 518.3548259015466
At time: 1343.9911360740662 and batch: 800, loss is 6.283152256011963 and perplexity is 535.4739571927739
At time: 1345.5336499214172 and batch: 850, loss is 6.300636281967163 and perplexity is 544.91852167181
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.880421956380208 and perplexity of 357.9602534592342
Finished 47 epochs...
Completing Train Step...
At time: 1349.5359482765198 and batch: 50, loss is 6.287016839981079 and perplexity is 537.5473450738988
At time: 1351.1010882854462 and batch: 100, loss is 6.246647825241089 and perplexity is 516.2792623779204
At time: 1352.642091035843 and batch: 150, loss is 6.221126680374145 and perplexity is 503.2699370735581
At time: 1354.2073240280151 and batch: 200, loss is 6.280349903106689 and perplexity is 533.9754708176167
At time: 1355.7476150989532 and batch: 250, loss is 6.310620365142822 and perplexity is 550.3862833903577
At time: 1357.2869808673859 and batch: 300, loss is 6.274629364013672 and perplexity is 530.9295636838367
At time: 1358.8277707099915 and batch: 350, loss is 6.254118461608886 and perplexity is 520.150639832738
At time: 1360.367246389389 and batch: 400, loss is 6.285150775909424 and perplexity is 536.5451826269212
At time: 1361.9070179462433 and batch: 450, loss is 6.302646350860596 and perplexity is 546.0149470177545
At time: 1363.448659658432 and batch: 500, loss is 6.29914701461792 and perplexity is 544.1075963017048
At time: 1364.9887807369232 and batch: 550, loss is 6.253219289779663 and perplexity is 519.6831452409294
At time: 1366.5291504859924 and batch: 600, loss is 6.2669328022003175 and perplexity is 526.8589165663988
At time: 1368.0769369602203 and batch: 650, loss is 6.28887393951416 and perplexity is 538.5465515231864
At time: 1369.6170001029968 and batch: 700, loss is 6.259340467453003 and perplexity is 522.8739739585005
At time: 1371.156768321991 and batch: 750, loss is 6.25067590713501 and perplexity is 518.3630715864233
At time: 1372.695015668869 and batch: 800, loss is 6.283169975280762 and perplexity is 535.4834454838187
At time: 1374.235090970993 and batch: 850, loss is 6.300642633438111 and perplexity is 544.9219827169604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.8804136912028 and perplexity of 357.9572948664609
Finished 48 epochs...
Completing Train Step...
At time: 1378.2385807037354 and batch: 50, loss is 6.28700101852417 and perplexity is 537.5388403590207
At time: 1379.8042073249817 and batch: 100, loss is 6.246632680892945 and perplexity is 516.2714437242355
At time: 1381.343991279602 and batch: 150, loss is 6.221104927062989 and perplexity is 503.25898940509546
At time: 1382.8830873966217 and batch: 200, loss is 6.280337247848511 and perplexity is 533.968713262932
At time: 1384.420727968216 and batch: 250, loss is 6.310610084533692 and perplexity is 550.3806251131928
At time: 1385.9596705436707 and batch: 300, loss is 6.27462550163269 and perplexity is 530.9275130355477
At time: 1387.4988465309143 and batch: 350, loss is 6.254113969802856 and perplexity is 520.1483034222048
At time: 1389.0370960235596 and batch: 400, loss is 6.285150127410889 and perplexity is 536.5448346782692
At time: 1390.575406551361 and batch: 450, loss is 6.302651557922363 and perplexity is 546.0177901587117
At time: 1392.1503269672394 and batch: 500, loss is 6.299151134490967 and perplexity is 544.1098379605432
At time: 1393.6897690296173 and batch: 550, loss is 6.253221683502197 and perplexity is 519.6843892196736
At time: 1395.2292432785034 and batch: 600, loss is 6.2669355583190915 and perplexity is 526.8603686541511
At time: 1396.7674646377563 and batch: 650, loss is 6.288879222869873 and perplexity is 538.5493968637026
At time: 1398.306925535202 and batch: 700, loss is 6.259346408843994 and perplexity is 522.8770805664475
At time: 1399.8440291881561 and batch: 750, loss is 6.250691757202149 and perplexity is 518.3712877410234
At time: 1401.3807308673859 and batch: 800, loss is 6.283184146881103 and perplexity is 535.4910341949694
At time: 1402.9167184829712 and batch: 850, loss is 6.300647602081299 and perplexity is 544.9246902465845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.880407333374023 and perplexity of 357.95501904250557
Finished 49 epochs...
Completing Train Step...
At time: 1406.8875181674957 and batch: 50, loss is 6.28698561668396 and perplexity is 537.5305613354515
At time: 1408.4720158576965 and batch: 100, loss is 6.246618843078613 and perplexity is 516.2642997052815
At time: 1410.0123150348663 and batch: 150, loss is 6.221085510253906 and perplexity is 503.2492178162461
At time: 1411.5505676269531 and batch: 200, loss is 6.280323534011841 and perplexity is 533.9613905534229
At time: 1413.089028596878 and batch: 250, loss is 6.3106004333496095 and perplexity is 550.3753133140972
At time: 1414.6279666423798 and batch: 300, loss is 6.274621629714966 and perplexity is 530.9254573318794
At time: 1416.1665184497833 and batch: 350, loss is 6.254109554290771 and perplexity is 520.1460067061556
At time: 1417.7055068016052 and batch: 400, loss is 6.285150098800659 and perplexity is 536.5448193275985
At time: 1419.2437074184418 and batch: 450, loss is 6.302657222747802 and perplexity is 546.0208832629405
At time: 1420.7822518348694 and batch: 500, loss is 6.299156608581543 and perplexity is 544.1128164752321
At time: 1422.3207216262817 and batch: 550, loss is 6.25322491645813 and perplexity is 519.686069339119
At time: 1423.8591125011444 and batch: 600, loss is 6.2669381999969485 and perplexity is 526.8617604513589
At time: 1425.397079706192 and batch: 650, loss is 6.2888839817047115 and perplexity is 538.5519597374326
At time: 1426.9353096485138 and batch: 700, loss is 6.259352407455444 and perplexity is 522.8802171122975
At time: 1428.4727456569672 and batch: 750, loss is 6.250707349777222 and perplexity is 518.3793705472588
At time: 1430.0094532966614 and batch: 800, loss is 6.283196687698364 and perplexity is 535.4977497322833
At time: 1431.5741908550262 and batch: 850, loss is 6.300651874542236 and perplexity is 544.9270184210108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.880400975545247 and perplexity of 357.9527432330195
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fc6e480c898>
SETTINGS FOR THIS RUN
{'lr': 0.874506659112172, 'anneal': 6.62117689633781, 'wordvec_source': 'glove', 'seq_len': 50, 'tune_wordvecs': True, 'num_layers': 1, 'dropout': 0.6348270522530168, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.1368846893310547 and batch: 50, loss is 8.330434045791627 and perplexity is 4148.217677993715
At time: 3.7480309009552 and batch: 100, loss is 6.91590069770813 and perplexity is 1008.1786829046529
At time: 5.355668544769287 and batch: 150, loss is 6.628481731414795 and perplexity is 756.3329818676318
At time: 6.912939548492432 and batch: 200, loss is 6.576645812988281 and perplexity is 718.1265543008477
At time: 8.468095541000366 and batch: 250, loss is 6.544927930831909 and perplexity is 695.7065381513617
At time: 10.020299196243286 and batch: 300, loss is 6.447284374237061 and perplexity is 630.98644102576
At time: 11.572206497192383 and batch: 350, loss is 6.394717741012573 and perplexity is 598.6743181655941
At time: 13.124008178710938 and batch: 400, loss is 6.3948694896698 and perplexity is 598.7651730828733
At time: 14.6762855052948 and batch: 450, loss is 6.364368591308594 and perplexity is 580.7780042415993
At time: 16.22871732711792 and batch: 500, loss is 6.342463855743408 and perplexity is 568.1945375351679
At time: 17.779811143875122 and batch: 550, loss is 6.268422565460205 and perplexity is 527.6443965677931
At time: 19.333569526672363 and batch: 600, loss is 6.273375215530396 and perplexity is 530.2641165487967
At time: 20.886199474334717 and batch: 650, loss is 6.278348655700683 and perplexity is 532.9079223623542
At time: 22.438905715942383 and batch: 700, loss is 6.221898736953736 and perplexity is 503.6586399707925
At time: 23.99110245704651 and batch: 750, loss is 6.191437654495239 and perplexity is 488.5479647561817
At time: 25.54330348968506 and batch: 800, loss is 6.209287748336792 and perplexity is 497.3468889553582
At time: 27.09701681137085 and batch: 850, loss is 6.199028062820434 and perplexity is 492.2703526452557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.5670928955078125 and perplexity of 261.672283766592
Finished 1 epochs...
Completing Train Step...
At time: 31.322520971298218 and batch: 50, loss is 5.911542015075684 and perplexity is 369.27514444158265
At time: 32.86226224899292 and batch: 100, loss is 5.795662097930908 and perplexity is 328.86985596508725
At time: 34.40362310409546 and batch: 150, loss is 5.733500776290893 and perplexity is 309.04928924318034
At time: 35.945995569229126 and batch: 200, loss is 5.7362895202636714 and perplexity is 309.91235145627826
At time: 37.48776960372925 and batch: 250, loss is 5.762380208969116 and perplexity is 318.1045839221066
At time: 39.034292697906494 and batch: 300, loss is 5.693835525512696 and perplexity is 297.03070744021204
At time: 40.576982498168945 and batch: 350, loss is 5.644146423339844 and perplexity is 282.6322049779628
At time: 42.11926865577698 and batch: 400, loss is 5.645994796752929 and perplexity is 283.15509793317256
At time: 43.660747051239014 and batch: 450, loss is 5.6401533508300785 and perplexity is 281.5058843264694
At time: 45.20624613761902 and batch: 500, loss is 5.622624206542969 and perplexity is 276.6143247415019
At time: 46.747891902923584 and batch: 550, loss is 5.580661020278931 and perplexity is 265.24688141701125
At time: 48.3373007774353 and batch: 600, loss is 5.58379280090332 and perplexity is 266.0788785972148
At time: 49.8790123462677 and batch: 650, loss is 5.5856449413299565 and perplexity is 266.572150708537
At time: 51.420663595199585 and batch: 700, loss is 5.524213342666626 and perplexity is 250.68905405081176
At time: 52.9620475769043 and batch: 750, loss is 5.505926284790039 and perplexity is 246.14635173593825
At time: 54.504457235336304 and batch: 800, loss is 5.510212659835815 and perplexity is 247.20369177473043
At time: 56.04553151130676 and batch: 850, loss is 5.498232278823853 and perplexity is 244.25976724009402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.1415589650472 and perplexity of 170.98211579640014
Finished 2 epochs...
Completing Train Step...
At time: 60.11696910858154 and batch: 50, loss is 5.480864105224609 and perplexity is 240.05404973711063
At time: 61.66100335121155 and batch: 100, loss is 5.409646911621094 and perplexity is 223.552639904371
At time: 63.206796169281006 and batch: 150, loss is 5.385969924926758 and perplexity is 218.32175716141487
At time: 64.75072813034058 and batch: 200, loss is 5.410702323913574 and perplexity is 223.78870465945204
At time: 66.29577231407166 and batch: 250, loss is 5.4500284671783445 and perplexity is 232.7647919701943
At time: 67.83929944038391 and batch: 300, loss is 5.400841255187988 and perplexity is 221.59275386824655
At time: 69.3886296749115 and batch: 350, loss is 5.362656946182251 and perplexity is 213.2908967006276
At time: 70.93225264549255 and batch: 400, loss is 5.374829568862915 and perplexity is 215.90307256863426
At time: 72.47622561454773 and batch: 450, loss is 5.38289734840393 and perplexity is 217.6519763592571
At time: 74.02360391616821 and batch: 500, loss is 5.377599315643311 and perplexity is 216.50189832378828
At time: 75.56984162330627 and batch: 550, loss is 5.354079866409302 and perplexity is 211.46930679304634
At time: 77.1134901046753 and batch: 600, loss is 5.366249599456787 and perplexity is 214.05855507859005
At time: 78.66882061958313 and batch: 650, loss is 5.376928205490112 and perplexity is 216.35665044575086
At time: 80.22593140602112 and batch: 700, loss is 5.320418815612793 and perplexity is 204.46949908138518
At time: 81.82602047920227 and batch: 750, loss is 5.306292953491211 and perplexity is 201.60149533201562
At time: 83.38132190704346 and batch: 800, loss is 5.30872820854187 and perplexity is 202.0930446728243
At time: 84.93429446220398 and batch: 850, loss is 5.30394211769104 and perplexity is 201.12811995132324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.996165593465169 and perplexity of 147.84517235741257
Finished 3 epochs...
Completing Train Step...
At time: 88.97884631156921 and batch: 50, loss is 5.300565185546875 and perplexity is 200.45006944759018
At time: 90.56691098213196 and batch: 100, loss is 5.2359810733795165 and perplexity is 187.91337274764544
At time: 92.10864400863647 and batch: 150, loss is 5.220969982147217 and perplexity is 185.11365395486254
At time: 93.65026211738586 and batch: 200, loss is 5.2519799423217775 and perplexity is 190.9439524515332
At time: 95.1898021697998 and batch: 250, loss is 5.288509931564331 and perplexity is 198.04810021377492
At time: 96.7298891544342 and batch: 300, loss is 5.243893623352051 and perplexity is 189.40614472812874
At time: 98.27262043952942 and batch: 350, loss is 5.207899703979492 and perplexity is 182.7099100255639
At time: 99.8132848739624 and batch: 400, loss is 5.220884466171265 and perplexity is 185.09782445692966
At time: 101.35528564453125 and batch: 450, loss is 5.233046436309815 and perplexity is 187.36272357101745
At time: 102.89622616767883 and batch: 500, loss is 5.231474876403809 and perplexity is 187.0685030798923
At time: 104.43870544433594 and batch: 550, loss is 5.216613225936889 and perplexity is 184.30891319594787
At time: 105.97924590110779 and batch: 600, loss is 5.233127031326294 and perplexity is 187.3778246813402
At time: 107.52342867851257 and batch: 650, loss is 5.243665523529053 and perplexity is 189.36294614702322
At time: 109.06560707092285 and batch: 700, loss is 5.189648809432984 and perplexity is 179.40553633503154
At time: 110.61200761795044 and batch: 750, loss is 5.17684266090393 and perplexity is 177.1226908174904
At time: 112.15289235115051 and batch: 800, loss is 5.175500688552856 and perplexity is 176.88515648156883
At time: 113.70450234413147 and batch: 850, loss is 5.176502799987793 and perplexity is 177.062503965679
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.896572430928548 and perplexity of 133.83028012257822
Finished 4 epochs...
Completing Train Step...
At time: 117.85879969596863 and batch: 50, loss is 5.1766061687469485 and perplexity is 177.08080764300487
At time: 119.40447306632996 and batch: 100, loss is 5.115952081680298 and perplexity is 166.6593788137124
At time: 120.99746632575989 and batch: 150, loss is 5.105242919921875 and perplexity is 164.88411930450184
At time: 122.54307174682617 and batch: 200, loss is 5.138973579406739 and perplexity is 170.5406320382433
At time: 124.08677124977112 and batch: 250, loss is 5.172038908004761 and perplexity is 176.27387755482312
At time: 125.63218784332275 and batch: 300, loss is 5.129977512359619 and perplexity is 169.01331729145352
At time: 127.17820858955383 and batch: 350, loss is 5.094594202041626 and perplexity is 163.13763027317307
At time: 128.72348046302795 and batch: 400, loss is 5.108116159439087 and perplexity is 165.35855212502204
At time: 130.2684907913208 and batch: 450, loss is 5.124429931640625 and perplexity is 168.0782982160754
At time: 131.8123962879181 and batch: 500, loss is 5.122334403991699 and perplexity is 167.72645427308697
At time: 133.35870099067688 and batch: 550, loss is 5.116127557754517 and perplexity is 166.68862611326398
At time: 134.90405750274658 and batch: 600, loss is 5.132408533096314 and perplexity is 169.4246919981906
At time: 136.44936513900757 and batch: 650, loss is 5.142080516815185 and perplexity is 171.0713150801877
At time: 137.99461126327515 and batch: 700, loss is 5.090100383758545 and perplexity is 162.40616417641215
At time: 139.5398564338684 and batch: 750, loss is 5.077690706253052 and perplexity is 160.40320976774527
At time: 141.09811520576477 and batch: 800, loss is 5.073341407775879 and perplexity is 159.70708326009867
At time: 142.65675711631775 and batch: 850, loss is 5.078473920822144 and perplexity is 160.52888910908544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.833775520324707 and perplexity of 125.68459071894492
Finished 5 epochs...
Completing Train Step...
At time: 146.7870156764984 and batch: 50, loss is 5.080842370986939 and perplexity is 160.90954438653526
At time: 148.3306691646576 and batch: 100, loss is 5.022752866744995 and perplexity is 151.828693182176
At time: 149.87394618988037 and batch: 150, loss is 5.013939723968506 and perplexity is 150.49648432788976
At time: 151.41645574569702 and batch: 200, loss is 5.050408735275268 and perplexity is 156.0862494060502
At time: 152.96610951423645 and batch: 250, loss is 5.0795685577392575 and perplexity is 160.70470616773326
At time: 154.5025634765625 and batch: 300, loss is 5.0409739971160885 and perplexity is 154.62054166940655
At time: 156.0470314025879 and batch: 350, loss is 5.005950469970703 and perplexity is 149.2989198819429
At time: 157.59707927703857 and batch: 400, loss is 5.019474906921387 and perplexity is 151.33181963652348
At time: 159.1695818901062 and batch: 450, loss is 5.039049139022827 and perplexity is 154.32320532532844
At time: 160.71378755569458 and batch: 500, loss is 5.0358430290222165 and perplexity is 153.82922046145964
At time: 162.2570514678955 and batch: 550, loss is 5.035139245986938 and perplexity is 153.72099615345186
At time: 163.80449080467224 and batch: 600, loss is 5.051184148788452 and perplexity is 156.20732772987935
At time: 165.3480293750763 and batch: 650, loss is 5.0594940280914305 and perplexity is 157.51080011783134
At time: 166.89257621765137 and batch: 700, loss is 5.010446949005127 and perplexity is 149.97175089670415
At time: 168.43718814849854 and batch: 750, loss is 4.998792352676392 and perplexity is 148.23403652841634
At time: 169.98250079154968 and batch: 800, loss is 4.992198066711426 and perplexity is 147.2597547785603
At time: 171.52724504470825 and batch: 850, loss is 4.999514150619507 and perplexity is 148.34107017476805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.776808420817058 and perplexity of 118.72482504481667
Finished 6 epochs...
Completing Train Step...
At time: 175.62814021110535 and batch: 50, loss is 5.001871891021729 and perplexity is 148.6912325429985
At time: 177.1712715625763 and batch: 100, loss is 4.9463113498687745 and perplexity is 140.655178115629
At time: 178.71378326416016 and batch: 150, loss is 4.9391927146911625 and perplexity is 139.65746062347182
At time: 180.25678396224976 and batch: 200, loss is 4.977941284179687 and perplexity is 145.1751993140263
At time: 181.8004653453827 and batch: 250, loss is 5.002938442230224 and perplexity is 148.84990395731313
At time: 183.34287214279175 and batch: 300, loss is 4.967764167785645 and perplexity is 143.70522713955287
At time: 184.88605523109436 and batch: 350, loss is 4.932783088684082 and perplexity is 138.7651712073483
At time: 186.43213987350464 and batch: 400, loss is 4.946824388504028 and perplexity is 140.727358170246
At time: 187.97493171691895 and batch: 450, loss is 4.9677461242675784 and perplexity is 143.70263421508358
At time: 189.51896023750305 and batch: 500, loss is 4.964379968643189 and perplexity is 143.21972201947244
At time: 191.06336617469788 and batch: 550, loss is 4.966718635559082 and perplexity is 143.5550572108826
At time: 192.6072804927826 and batch: 600, loss is 4.9828094863891605 and perplexity is 145.88366461708182
At time: 194.15197324752808 and batch: 650, loss is 4.990993928909302 and perplexity is 147.08254045774393
At time: 195.69600558280945 and batch: 700, loss is 4.943360042572022 and perplexity is 140.24067342835303
At time: 197.24020147323608 and batch: 750, loss is 4.932413825988769 and perplexity is 138.71393986570652
At time: 198.8315658569336 and batch: 800, loss is 4.923773355484009 and perplexity is 137.52054931227988
At time: 200.37540650367737 and batch: 850, loss is 4.933187570571899 and perplexity is 138.82131055867154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.732474644978841 and perplexity of 113.57627583300385
Finished 7 epochs...
Completing Train Step...
At time: 204.4532082080841 and batch: 50, loss is 4.93511323928833 and perplexity is 139.08889196747518
At time: 206.02348828315735 and batch: 100, loss is 4.881174182891845 and perplexity is 131.78531313489046
At time: 207.57308530807495 and batch: 150, loss is 4.874357500076294 and perplexity is 130.89002935505255
At time: 209.11991238594055 and batch: 200, loss is 4.916459274291992 and perplexity is 136.51838228219071
At time: 210.6679174900055 and batch: 250, loss is 4.937863349914551 and perplexity is 139.471928261944
At time: 212.215026140213 and batch: 300, loss is 4.904960327148437 and perplexity is 134.95755574952545
At time: 213.76001739501953 and batch: 350, loss is 4.869931745529175 and perplexity is 130.31202221479293
At time: 215.31218719482422 and batch: 400, loss is 4.884546937942505 and perplexity is 132.2305431191495
At time: 216.8622317314148 and batch: 450, loss is 4.906879549026489 and perplexity is 135.21681795439542
At time: 218.41316986083984 and batch: 500, loss is 4.902365665435791 and perplexity is 134.6078404394507
At time: 219.96142482757568 and batch: 550, loss is 4.907534103393555 and perplexity is 135.30535368565197
At time: 221.51030230522156 and batch: 600, loss is 4.924402322769165 and perplexity is 137.60707244609188
At time: 223.05751299858093 and batch: 650, loss is 4.931788101196289 and perplexity is 138.62717026424642
At time: 224.60622262954712 and batch: 700, loss is 4.884845609664917 and perplexity is 132.270042541605
At time: 226.15635085105896 and batch: 750, loss is 4.87483853340149 and perplexity is 130.95300696705885
At time: 227.70574164390564 and batch: 800, loss is 4.864114437103272 and perplexity is 129.5561576701632
At time: 229.2546627521515 and batch: 850, loss is 4.875946931838989 and perplexity is 131.09823554606365
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.695878028869629 and perplexity of 109.49490613611671
Finished 8 epochs...
Completing Train Step...
At time: 233.31500434875488 and batch: 50, loss is 4.876576690673828 and perplexity is 131.18082182011906
At time: 234.89132118225098 and batch: 100, loss is 4.823320512771606 and perplexity is 124.37740261150405
At time: 236.43453073501587 and batch: 150, loss is 4.817944602966309 and perplexity is 123.71055497563276
At time: 238.02719950675964 and batch: 200, loss is 4.862977418899536 and perplexity is 129.4089336745288
At time: 239.57079553604126 and batch: 250, loss is 4.880331420898438 and perplexity is 131.67429626865962
At time: 241.11698126792908 and batch: 300, loss is 4.849645633697509 and perplexity is 127.69513097598323
At time: 242.66932010650635 and batch: 350, loss is 4.814349927902222 and perplexity is 123.26665404629401
At time: 244.22022771835327 and batch: 400, loss is 4.829395265579223 and perplexity is 125.13526416744133
At time: 245.76918601989746 and batch: 450, loss is 4.853453569412231 and perplexity is 128.18231281569314
At time: 247.31108260154724 and batch: 500, loss is 4.84776517868042 and perplexity is 127.45523165691314
At time: 248.84937167167664 and batch: 550, loss is 4.855440273284912 and perplexity is 128.4372262478796
At time: 250.3885622024536 and batch: 600, loss is 4.872835998535156 and perplexity is 130.69103139990645
At time: 251.93033146858215 and batch: 650, loss is 4.878882989883423 and perplexity is 131.48371319060396
At time: 253.4762897491455 and batch: 700, loss is 4.832600736618042 and perplexity is 125.53702520534146
At time: 255.01690888404846 and batch: 750, loss is 4.823849058151245 and perplexity is 124.44315908909618
At time: 256.5643482208252 and batch: 800, loss is 4.811423435211181 and perplexity is 122.90644241960668
At time: 258.1099693775177 and batch: 850, loss is 4.825400218963623 and perplexity is 124.6363402296168
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.665707270304362 and perplexity of 106.2406995476827
Finished 9 epochs...
Completing Train Step...
At time: 262.1402895450592 and batch: 50, loss is 4.8250323677062985 and perplexity is 124.59050102667608
At time: 263.7270176410675 and batch: 100, loss is 4.772586069107056 and perplexity is 118.22458391696175
At time: 265.269816160202 and batch: 150, loss is 4.768326787948609 and perplexity is 117.72210303753528
At time: 266.81577157974243 and batch: 200, loss is 4.815054140090942 and perplexity is 123.35349049859167
At time: 268.36070370674133 and batch: 250, loss is 4.829401044845581 and perplexity is 125.1359873595535
At time: 269.9012906551361 and batch: 300, loss is 4.800120897293091 and perplexity is 121.52510868733863
At time: 271.4412353038788 and batch: 350, loss is 4.7650528907775875 and perplexity is 117.3373231859532
At time: 272.9827048778534 and batch: 400, loss is 4.780430431365967 and perplexity is 119.15562732778002
At time: 274.52438974380493 and batch: 450, loss is 4.805347356796265 and perplexity is 122.16191742446784
At time: 276.0719256401062 and batch: 500, loss is 4.798902969360352 and perplexity is 121.37718995839963
At time: 277.6603090763092 and batch: 550, loss is 4.80865252494812 and perplexity is 122.56635109577977
At time: 279.200377702713 and batch: 600, loss is 4.827017602920532 and perplexity is 124.83808815470077
At time: 280.74102210998535 and batch: 650, loss is 4.830902843475342 and perplexity is 125.32405760037102
At time: 282.2833390235901 and batch: 700, loss is 4.785778493881225 and perplexity is 119.79458614467704
At time: 283.8264870643616 and batch: 750, loss is 4.778496160507202 and perplexity is 118.92537083118793
At time: 285.37038135528564 and batch: 800, loss is 4.764664306640625 and perplexity is 117.29173662117104
At time: 286.91433453559875 and batch: 850, loss is 4.779313297271728 and perplexity is 119.02258883872364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.6386416753133135 and perplexity of 103.40379621859634
Finished 10 epochs...
Completing Train Step...
At time: 291.00293374061584 and batch: 50, loss is 4.778108396530151 and perplexity is 118.87926479612972
At time: 292.55027413368225 and batch: 100, loss is 4.727201156616211 and perplexity is 112.97890915162954
At time: 294.09790086746216 and batch: 150, loss is 4.723267297744751 and perplexity is 112.53533911057563
At time: 295.64534759521484 and batch: 200, loss is 4.772018604278564 and perplexity is 118.15751465524916
At time: 297.1932518482208 and batch: 250, loss is 4.7834076309204105 and perplexity is 119.51090601371838
At time: 298.74031949043274 and batch: 300, loss is 4.755397319793701 and perplexity is 116.20981645329691
At time: 300.29011631011963 and batch: 350, loss is 4.720074262619018 and perplexity is 112.17658288533109
At time: 301.83894777297974 and batch: 400, loss is 4.735762195587158 and perplexity is 113.95027802642741
At time: 303.39047360420227 and batch: 450, loss is 4.7620433330535885 and perplexity is 116.98472059371267
At time: 304.9402003288269 and batch: 500, loss is 4.754145135879517 and perplexity is 116.0643914588849
At time: 306.4900612831116 and batch: 550, loss is 4.765839328765869 and perplexity is 117.42963800953407
At time: 308.04070687294006 and batch: 600, loss is 4.785321731567382 and perplexity is 119.73988098690207
At time: 309.59244680404663 and batch: 650, loss is 4.7871424579620365 and perplexity is 119.95809314073982
At time: 311.1434597969055 and batch: 700, loss is 4.742804803848267 and perplexity is 114.75561771315468
At time: 312.69030928611755 and batch: 750, loss is 4.7371461296081545 and perplexity is 114.10808686621401
At time: 314.2377939224243 and batch: 800, loss is 4.721942539215088 and perplexity is 112.38635566553378
At time: 315.83253359794617 and batch: 850, loss is 4.736752443313598 and perplexity is 114.06317291789979
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.615082422892253 and perplexity of 100.99615258443242
Finished 11 epochs...
Completing Train Step...
At time: 319.89312052726746 and batch: 50, loss is 4.734831981658935 and perplexity is 113.84432917587036
At time: 321.43960189819336 and batch: 100, loss is 4.68580994606018 and perplexity is 108.3980333225631
At time: 322.9846968650818 and batch: 150, loss is 4.682088813781738 and perplexity is 107.99541945615078
At time: 324.52984976768494 and batch: 200, loss is 4.7327073955535885 and perplexity is 113.60271385309451
At time: 326.0735878944397 and batch: 250, loss is 4.742011260986328 and perplexity is 114.66459033369144
At time: 327.6172716617584 and batch: 300, loss is 4.714104051589966 and perplexity is 111.50886021892676
At time: 329.1632311344147 and batch: 350, loss is 4.678943872451782 and perplexity is 107.65631371139129
At time: 330.7079999446869 and batch: 400, loss is 4.69483325958252 and perplexity is 109.38056895947642
At time: 332.2528028488159 and batch: 450, loss is 4.722263832092285 and perplexity is 112.42247040249642
At time: 333.7995755672455 and batch: 500, loss is 4.71299750328064 and perplexity is 111.38553852146798
At time: 335.3436989784241 and batch: 550, loss is 4.726486120223999 and perplexity is 112.89815399491242
At time: 336.8873324394226 and batch: 600, loss is 4.746873741149902 and perplexity is 115.2235023777879
At time: 338.4308478832245 and batch: 650, loss is 4.747023706436157 and perplexity is 115.24078319903454
At time: 339.976437330246 and batch: 700, loss is 4.703370161056519 and perplexity is 110.31833722221593
At time: 341.524649143219 and batch: 750, loss is 4.699114227294922 and perplexity is 109.84982736693934
At time: 343.0706305503845 and batch: 800, loss is 4.682411756515503 and perplexity is 108.03030142427991
At time: 344.61428022384644 and batch: 850, loss is 4.697707185745239 and perplexity is 109.69537278309902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.594200770060222 and perplexity of 98.9090528728857
Finished 12 epochs...
Completing Train Step...
At time: 348.67162895202637 and batch: 50, loss is 4.694876699447632 and perplexity is 109.38532053984116
At time: 350.2164890766144 and batch: 100, loss is 4.64747332572937 and perplexity is 104.3210669435793
At time: 351.759224653244 and batch: 150, loss is 4.6444174098968505 and perplexity is 104.00275715511667
At time: 353.3071150779724 and batch: 200, loss is 4.696380977630615 and perplexity is 109.54999031459067
At time: 354.85000824928284 and batch: 250, loss is 4.704086828231811 and perplexity is 110.39742709050707
At time: 356.41935443878174 and batch: 300, loss is 4.676096706390381 and perplexity is 107.35023424499677
At time: 357.95978689193726 and batch: 350, loss is 4.641197280883789 and perplexity is 103.66839349534841
At time: 359.502970457077 and batch: 400, loss is 4.657217321395874 and perplexity is 105.34253949787215
At time: 361.04588985443115 and batch: 450, loss is 4.685872325897217 and perplexity is 108.4047953851228
At time: 362.5916631221771 and batch: 500, loss is 4.675007543563843 and perplexity is 107.23337601081143
At time: 364.1336987018585 and batch: 550, loss is 4.690240211486817 and perplexity is 108.87933073233278
At time: 365.68349170684814 and batch: 600, loss is 4.71152780532837 and perplexity is 111.22195566173251
At time: 367.2393579483032 and batch: 650, loss is 4.710292634963989 and perplexity is 111.08466240596611
At time: 368.79481744766235 and batch: 700, loss is 4.667297954559326 and perplexity is 106.40982943615276
At time: 370.3473889827728 and batch: 750, loss is 4.663869619369507 and perplexity is 106.04564550235439
At time: 371.8991997241974 and batch: 800, loss is 4.645687847137451 and perplexity is 104.13497009727165
At time: 373.4517879486084 and batch: 850, loss is 4.661779279708862 and perplexity is 105.82420560665257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5755971272786455 and perplexity of 97.08599452754578
Finished 13 epochs...
Completing Train Step...
At time: 377.47406792640686 and batch: 50, loss is 4.658374156951904 and perplexity is 105.46447400861618
At time: 379.0508716106415 and batch: 100, loss is 4.612067174911499 and perplexity is 100.69208279257771
At time: 380.601943731308 and batch: 150, loss is 4.609715518951416 and perplexity is 100.45556786581348
At time: 382.1531138420105 and batch: 200, loss is 4.6626464366912845 and perplexity is 105.91601160480693
At time: 383.70472717285156 and batch: 250, loss is 4.669173812866211 and perplexity is 106.60962653557112
At time: 385.25651025772095 and batch: 300, loss is 4.641084804534912 and perplexity is 103.65673390868024
At time: 386.808185338974 and batch: 350, loss is 4.606174459457398 and perplexity is 100.10047779207618
At time: 388.36168098449707 and batch: 400, loss is 4.622214879989624 and perplexity is 101.71907836357519
At time: 389.9185903072357 and batch: 450, loss is 4.652118921279907 and perplexity is 104.80682787879644
At time: 391.4795153141022 and batch: 500, loss is 4.639959297180176 and perplexity is 103.54013312212835
At time: 393.0305037498474 and batch: 550, loss is 4.657179164886474 and perplexity is 105.3385200709578
At time: 394.6079897880554 and batch: 600, loss is 4.678684787750244 and perplexity is 107.62842522038139
At time: 396.1595137119293 and batch: 650, loss is 4.676416521072388 and perplexity is 107.38457191657824
At time: 397.7117121219635 and batch: 700, loss is 4.633800096511841 and perplexity is 102.90436857556243
At time: 399.26266264915466 and batch: 750, loss is 4.631285486221313 and perplexity is 102.64592926465903
At time: 400.81374645233154 and batch: 800, loss is 4.611712532043457 and perplexity is 100.65637939489889
At time: 402.36474347114563 and batch: 850, loss is 4.628421792984009 and perplexity is 102.35240329594691
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.559483528137207 and perplexity of 95.53412639811273
Finished 14 epochs...
Completing Train Step...
At time: 406.3989465236664 and batch: 50, loss is 4.624471206665039 and perplexity is 101.94884895476223
At time: 407.9694769382477 and batch: 100, loss is 4.579352884292603 and perplexity is 97.45131152396421
At time: 409.5114974975586 and batch: 150, loss is 4.577458915710449 and perplexity is 97.26691647598246
At time: 411.060177564621 and batch: 200, loss is 4.631106519699097 and perplexity is 102.62756072340491
At time: 412.60249876976013 and batch: 250, loss is 4.636793813705444 and perplexity is 103.21289674567987
At time: 414.14561796188354 and batch: 300, loss is 4.6085721874237064 and perplexity is 100.34077948103258
At time: 415.68722796440125 and batch: 350, loss is 4.5733372020721434 and perplexity is 96.86683517646088
At time: 417.2358617782593 and batch: 400, loss is 4.5900283050537105 and perplexity is 98.49721809153831
At time: 418.7812716960907 and batch: 450, loss is 4.62074164390564 and perplexity is 101.56933247950462
At time: 420.3323640823364 and batch: 500, loss is 4.607291402816773 and perplexity is 100.21234682007
At time: 421.8804988861084 and batch: 550, loss is 4.626172351837158 and perplexity is 102.12242634527513
At time: 423.424033164978 and batch: 600, loss is 4.648096036911011 and perplexity is 104.38604906889391
At time: 424.97207403182983 and batch: 650, loss is 4.644904708862304 and perplexity is 104.05344994135072
At time: 426.51785016059875 and batch: 700, loss is 4.602807941436768 and perplexity is 99.76405433526676
At time: 428.0621075630188 and batch: 750, loss is 4.601074514389038 and perplexity is 99.59127042250933
At time: 429.6062788963318 and batch: 800, loss is 4.580202236175537 and perplexity is 97.53411713947621
At time: 431.15106678009033 and batch: 850, loss is 4.597385997772217 and perplexity is 99.22460301179804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.544449806213379 and perplexity of 94.10863497659946
Finished 15 epochs...
Completing Train Step...
At time: 435.1668293476105 and batch: 50, loss is 4.592867546081543 and perplexity is 98.77727281757174
At time: 436.748441696167 and batch: 100, loss is 4.54872296333313 and perplexity is 94.51163639113237
At time: 438.2912702560425 and batch: 150, loss is 4.547340278625488 and perplexity is 94.3810468996518
At time: 439.83643436431885 and batch: 200, loss is 4.601646299362183 and perplexity is 99.64823149758483
At time: 441.37897849082947 and batch: 250, loss is 4.606377964019775 and perplexity is 100.1208507689296
At time: 442.9244270324707 and batch: 300, loss is 4.5782132339477535 and perplexity is 97.34031436416925
At time: 444.4754886627197 and batch: 350, loss is 4.543032178878784 and perplexity is 93.97531852208706
At time: 446.0249500274658 and batch: 400, loss is 4.559927158355713 and perplexity is 95.57651762580103
At time: 447.56674313545227 and batch: 450, loss is 4.591812992095948 and perplexity is 98.67316175585165
At time: 449.1067695617676 and batch: 500, loss is 4.576841354370117 and perplexity is 97.20686673278207
At time: 450.65246963500977 and batch: 550, loss is 4.5976600837707515 and perplexity is 99.25180281356587
At time: 452.195175409317 and batch: 600, loss is 4.619573230743408 and perplexity is 101.45072683824596
At time: 453.73933148384094 and batch: 650, loss is 4.615444755554199 and perplexity is 101.03275341968325
At time: 455.2796845436096 and batch: 700, loss is 4.573836641311646 and perplexity is 96.915226358176
At time: 456.82476019859314 and batch: 750, loss is 4.57275990486145 and perplexity is 96.81093036110444
At time: 458.3707332611084 and batch: 800, loss is 4.550660734176636 and perplexity is 94.69495584268418
At time: 459.91327571868896 and batch: 850, loss is 4.568467025756836 and perplexity is 96.39622352110227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.53044859568278 and perplexity of 92.80018150682754
Finished 16 epochs...
Completing Train Step...
At time: 463.9631655216217 and batch: 50, loss is 4.563528089523316 and perplexity is 95.92130248772823
At time: 465.50921726226807 and batch: 100, loss is 4.52017725944519 and perplexity is 91.85187814816742
At time: 467.0564603805542 and batch: 150, loss is 4.519218215942383 and perplexity is 91.76383042875396
At time: 468.60045409202576 and batch: 200, loss is 4.574167633056641 and perplexity is 96.94730980745044
At time: 470.14376878738403 and batch: 250, loss is 4.577708625793457 and perplexity is 97.29120803856749
At time: 471.6883120536804 and batch: 300, loss is 4.549600582122803 and perplexity is 94.59461798684963
At time: 473.26288652420044 and batch: 350, loss is 4.514419012069702 and perplexity is 91.32449217855348
At time: 474.80783247947693 and batch: 400, loss is 4.531958379745483 and perplexity is 92.94039556172102
At time: 476.3529453277588 and batch: 450, loss is 4.564931983947754 and perplexity is 96.05606044031167
At time: 477.89720344543457 and batch: 500, loss is 4.548568067550659 and perplexity is 94.4969980709972
At time: 479.4398181438446 and batch: 550, loss is 4.571087007522583 and perplexity is 96.64911100465424
At time: 480.9822316169739 and batch: 600, loss is 4.592958402633667 and perplexity is 98.78624778771945
At time: 482.5244333744049 and batch: 650, loss is 4.587920732498169 and perplexity is 98.28984665982017
At time: 484.06852293014526 and batch: 700, loss is 4.546793575286865 and perplexity is 94.32946256815698
At time: 485.6138184070587 and batch: 750, loss is 4.546256895065308 and perplexity is 94.27885139350948
At time: 487.16167068481445 and batch: 800, loss is 4.522987756729126 and perplexity is 92.11039070648516
At time: 488.7098217010498 and batch: 850, loss is 4.541428699493408 and perplexity is 93.82475178369407
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.518420855204265 and perplexity of 91.69069071643335
Finished 17 epochs...
Completing Train Step...
At time: 492.746239900589 and batch: 50, loss is 4.536065826416015 and perplexity is 93.32292835874783
At time: 494.29593801498413 and batch: 100, loss is 4.49344573020935 and perplexity is 89.4290640172163
At time: 495.8435184955597 and batch: 150, loss is 4.492857608795166 and perplexity is 89.37648433275209
At time: 497.3909230232239 and batch: 200, loss is 4.548608055114746 and perplexity is 94.50077685131525
At time: 498.9430923461914 and batch: 250, loss is 4.550720462799072 and perplexity is 94.70061201086423
At time: 500.4949448108673 and batch: 300, loss is 4.522770938873291 and perplexity is 92.09042169396947
At time: 502.0487439632416 and batch: 350, loss is 4.487765579223633 and perplexity is 88.92253337823746
At time: 503.60186314582825 and batch: 400, loss is 4.505986852645874 and perplexity is 90.55766704062782
At time: 505.15464639663696 and batch: 450, loss is 4.539662885665893 and perplexity is 93.6592209309774
At time: 506.7132008075714 and batch: 500, loss is 4.522309036254883 and perplexity is 92.04789470947759
At time: 508.26470851898193 and batch: 550, loss is 4.5457420158386235 and perplexity is 94.23032166595772
At time: 509.8191034793854 and batch: 600, loss is 4.567891826629639 and perplexity is 96.34079244095061
At time: 511.4215052127838 and batch: 650, loss is 4.5620223331451415 and perplexity is 95.77697706143925
At time: 512.9768443107605 and batch: 700, loss is 4.521193761825561 and perplexity is 91.94529327125042
At time: 514.5331237316132 and batch: 750, loss is 4.521267566680908 and perplexity is 91.95207953074653
At time: 516.0899460315704 and batch: 800, loss is 4.497059860229492 and perplexity is 89.75285704475779
At time: 517.645308971405 and batch: 850, loss is 4.51591383934021 and perplexity is 91.46110860346687
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.507881164550781 and perplexity of 90.72937408939045
Finished 18 epochs...
Completing Train Step...
At time: 521.7023024559021 and batch: 50, loss is 4.510209741592408 and perplexity is 90.94089059756358
At time: 523.2563669681549 and batch: 100, loss is 4.468202295303345 and perplexity is 87.19982249485012
At time: 524.8134064674377 and batch: 150, loss is 4.4680307102203365 and perplexity is 87.1848615896398
At time: 526.3710103034973 and batch: 200, loss is 4.524359540939331 and perplexity is 92.23683299198933
At time: 527.9265143871307 and batch: 250, loss is 4.525497760772705 and perplexity is 92.34187855579954
At time: 529.4827857017517 and batch: 300, loss is 4.4975849151611325 and perplexity is 89.79999459879764
At time: 531.0389544963837 and batch: 350, loss is 4.462576198577881 and perplexity is 86.71060534099485
At time: 532.5948073863983 and batch: 400, loss is 4.481074810028076 and perplexity is 88.32955917001058
At time: 534.1505043506622 and batch: 450, loss is 4.51594331741333 and perplexity is 91.4638047404522
At time: 535.7082898616791 and batch: 500, loss is 4.497380533218384 and perplexity is 89.78164297687667
At time: 537.265481710434 and batch: 550, loss is 4.521783246994018 and perplexity is 91.99950963624481
At time: 538.8224198818207 and batch: 600, loss is 4.544338235855102 and perplexity is 94.09813582818597
At time: 540.379243850708 and batch: 650, loss is 4.537732276916504 and perplexity is 93.47857605298711
At time: 541.9347598552704 and batch: 700, loss is 4.497220506668091 and perplexity is 89.7672766797966
At time: 543.4887704849243 and batch: 750, loss is 4.497603540420532 and perplexity is 89.80166716256704
At time: 545.0441753864288 and batch: 800, loss is 4.472626066207885 and perplexity is 87.58642903137527
At time: 546.5999794006348 and batch: 850, loss is 4.4918703365325925 and perplexity is 89.28828895243406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.498634656270345 and perplexity of 89.89431083989177
Finished 19 epochs...
Completing Train Step...
At time: 550.6217272281647 and batch: 50, loss is 4.48576078414917 and perplexity is 88.74444050082296
At time: 552.2042820453644 and batch: 100, loss is 4.444720859527588 and perplexity is 85.1760984222266
At time: 553.7583680152893 and batch: 150, loss is 4.444452781677246 and perplexity is 85.15326765720896
At time: 555.3127915859222 and batch: 200, loss is 4.501361217498779 and perplexity is 90.1397476296513
At time: 556.8713343143463 and batch: 250, loss is 4.501315393447876 and perplexity is 90.13561715590576
At time: 558.4261479377747 and batch: 300, loss is 4.47394323348999 and perplexity is 87.70187102156771
At time: 559.9802458286285 and batch: 350, loss is 4.4388156986236575 and perplexity is 84.67460202164621
At time: 561.5352139472961 and batch: 400, loss is 4.457631177902222 and perplexity is 86.28287803657584
At time: 563.0900194644928 and batch: 450, loss is 4.493320684432984 and perplexity is 89.41788198962378
At time: 564.6453869342804 and batch: 500, loss is 4.473887157440186 and perplexity is 87.69695318496808
At time: 566.199782371521 and batch: 550, loss is 4.499213094711304 and perplexity is 89.94632420672023
At time: 567.7549159526825 and batch: 600, loss is 4.521976976394654 and perplexity is 92.01733437263738
At time: 569.3081748485565 and batch: 650, loss is 4.51482006072998 and perplexity is 91.36112508909116
At time: 570.862681388855 and batch: 700, loss is 4.474655542373657 and perplexity is 87.76436409795502
At time: 572.4191038608551 and batch: 750, loss is 4.475237789154053 and perplexity is 87.8154794958284
At time: 573.9716880321503 and batch: 800, loss is 4.449517660140991 and perplexity is 85.58565267313064
At time: 575.5256705284119 and batch: 850, loss is 4.4693325424194335 and perplexity is 87.29843556081721
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.490900993347168 and perplexity of 89.20177989325714
Finished 20 epochs...
Completing Train Step...
At time: 579.5710067749023 and batch: 50, loss is 4.462779626846314 and perplexity is 86.72824652359094
At time: 581.1519954204559 and batch: 100, loss is 4.422490701675415 and perplexity is 83.30351137747427
At time: 582.7059943675995 and batch: 150, loss is 4.422126760482788 and perplexity is 83.27319931443229
At time: 584.2596259117126 and batch: 200, loss is 4.479657363891602 and perplexity is 88.20444546954195
At time: 585.813916683197 and batch: 250, loss is 4.4786254501342775 and perplexity is 88.11347303473092
At time: 587.3687756061554 and batch: 300, loss is 4.4515508270263675 and perplexity is 85.75983960355437
At time: 588.9255330562592 and batch: 350, loss is 4.416305141448975 and perplexity is 82.78982285428248
At time: 590.5104591846466 and batch: 400, loss is 4.435284185409546 and perplexity is 84.37609993796637
At time: 592.0680212974548 and batch: 450, loss is 4.471584386825562 and perplexity is 87.49523955742517
At time: 593.6254322528839 and batch: 500, loss is 4.452042913436889 and perplexity is 85.8020512402264
At time: 595.1824977397919 and batch: 550, loss is 4.477669763565063 and perplexity is 88.02930439780893
At time: 596.7393081188202 and batch: 600, loss is 4.5012727546691895 and perplexity is 90.13177396520913
At time: 598.2947280406952 and batch: 650, loss is 4.4932048130035405 and perplexity is 89.40752161206731
At time: 599.8494668006897 and batch: 700, loss is 4.453381032943725 and perplexity is 85.9169414900165
At time: 601.4033679962158 and batch: 750, loss is 4.454137020111084 and perplexity is 85.98191815290869
At time: 602.955194234848 and batch: 800, loss is 4.427758703231811 and perplexity is 83.74351235039988
At time: 604.5094995498657 and batch: 850, loss is 4.447965002059936 and perplexity is 85.45287052719665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.483315785725911 and perplexity of 88.52772552557238
Finished 21 epochs...
Completing Train Step...
At time: 608.5409379005432 and batch: 50, loss is 4.4406452465057376 and perplexity is 84.82966006024249
At time: 610.1268148422241 and batch: 100, loss is 4.401195487976074 and perplexity is 81.54830042674368
At time: 611.686850309372 and batch: 150, loss is 4.400650510787964 and perplexity is 81.50387057101048
At time: 613.2480704784393 and batch: 200, loss is 4.459078941345215 and perplexity is 86.40788570208265
At time: 614.8073859214783 and batch: 250, loss is 4.456970806121826 and perplexity is 86.225918068237
At time: 616.3663744926453 and batch: 300, loss is 4.43056658744812 and perplexity is 83.97898487202563
At time: 617.9257066249847 and batch: 350, loss is 4.395300683975219 and perplexity is 81.06900324849974
At time: 619.4840610027313 and batch: 400, loss is 4.414219341278076 and perplexity is 82.6173197936215
At time: 621.0441591739655 and batch: 450, loss is 4.451056079864502 and perplexity is 85.71742066049957
At time: 622.6060872077942 and batch: 500, loss is 4.430981187820435 and perplexity is 84.01380980913729
At time: 624.1675896644592 and batch: 550, loss is 4.457417058944702 and perplexity is 86.2644052144401
At time: 625.7298338413239 and batch: 600, loss is 4.481754302978516 and perplexity is 88.38959887875137
At time: 627.292352437973 and batch: 650, loss is 4.472632169723511 and perplexity is 87.58696361814486
At time: 628.8547763824463 and batch: 700, loss is 4.433194103240967 and perplexity is 84.19993112381235
At time: 630.4926471710205 and batch: 750, loss is 4.433990049362182 and perplexity is 84.2669764110755
At time: 632.0498583316803 and batch: 800, loss is 4.40706353187561 and perplexity is 82.02823619849669
At time: 633.6077907085419 and batch: 850, loss is 4.4276871013641355 and perplexity is 83.737516373174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.476151784261067 and perplexity of 87.89577910554277
Finished 22 epochs...
Completing Train Step...
At time: 637.6593794822693 and batch: 50, loss is 4.41964469909668 and perplexity is 83.06676641745713
At time: 639.212185382843 and batch: 100, loss is 4.380981178283691 and perplexity is 79.91640719271581
At time: 640.7664744853973 and batch: 150, loss is 4.380321912765503 and perplexity is 79.86373842437519
At time: 642.3229982852936 and batch: 200, loss is 4.439452648162842 and perplexity is 84.72855265047458
At time: 643.8796014785767 and batch: 250, loss is 4.436344261169434 and perplexity is 84.46559242225047
At time: 645.432003736496 and batch: 300, loss is 4.410443267822266 and perplexity is 82.3059389937363
At time: 646.9850080013275 and batch: 350, loss is 4.375200109481812 and perplexity is 79.45573780803214
At time: 648.537755727768 and batch: 400, loss is 4.394142894744873 and perplexity is 80.97519674418272
At time: 650.0958533287048 and batch: 450, loss is 4.431745624542236 and perplexity is 84.07805760405199
At time: 651.6489233970642 and batch: 500, loss is 4.411087303161621 and perplexity is 82.35896400024335
At time: 653.2021825313568 and batch: 550, loss is 4.438027563095093 and perplexity is 84.6078932506552
At time: 654.7551326751709 and batch: 600, loss is 4.463240852355957 and perplexity is 86.76825702951797
At time: 656.307902097702 and batch: 650, loss is 4.4530876350402835 and perplexity is 85.8917373371168
At time: 657.8603870868683 and batch: 700, loss is 4.413963899612427 and perplexity is 82.59621858302089
At time: 659.4136326313019 and batch: 750, loss is 4.414998760223389 and perplexity is 82.68173839915924
At time: 660.9687399864197 and batch: 800, loss is 4.387402858734131 and perplexity is 80.43125615092319
At time: 662.5208978652954 and batch: 850, loss is 4.408422994613647 and perplexity is 82.13982636322471
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.4696807861328125 and perplexity of 87.32884198630505
Finished 23 epochs...
Completing Train Step...
At time: 666.5764780044556 and batch: 50, loss is 4.3995059204101565 and perplexity is 81.4106353932234
At time: 668.1303248405457 and batch: 100, loss is 4.361911220550537 and perplexity is 78.40684408514
At time: 669.7100584506989 and batch: 150, loss is 4.3610340595245365 and perplexity is 78.33809881208171
At time: 671.2620549201965 and batch: 200, loss is 4.4206102848052975 and perplexity is 83.14701323632674
At time: 672.8185276985168 and batch: 250, loss is 4.416592054367065 and perplexity is 82.81357973186036
At time: 674.3698995113373 and batch: 300, loss is 4.391239786148072 and perplexity is 80.74045785549556
At time: 675.9197285175323 and batch: 350, loss is 4.355959453582764 and perplexity is 77.94157079303989
At time: 677.4694497585297 and batch: 400, loss is 4.374958133697509 and perplexity is 79.43651376952843
At time: 679.021516084671 and batch: 450, loss is 4.4129855251312256 and perplexity is 82.51544806886336
At time: 680.5748152732849 and batch: 500, loss is 4.392017583847046 and perplexity is 80.80328202691048
At time: 682.1284549236298 and batch: 550, loss is 4.420184879302979 and perplexity is 83.1116495618788
At time: 683.6814961433411 and batch: 600, loss is 4.445826387405395 and perplexity is 85.27031504351001
At time: 685.2347102165222 and batch: 650, loss is 4.434612092971801 and perplexity is 84.31941045169314
At time: 686.7885775566101 and batch: 700, loss is 4.396060657501221 and perplexity is 81.1306369617767
At time: 688.3419926166534 and batch: 750, loss is 4.396595630645752 and perplexity is 81.17405128546318
At time: 689.8968167304993 and batch: 800, loss is 4.3686542797088626 and perplexity is 78.93733262095664
At time: 691.4509181976318 and batch: 850, loss is 4.38988263130188 and perplexity is 80.63095487489117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.464170455932617 and perplexity of 86.84895461416349
Finished 24 epochs...
Completing Train Step...
At time: 695.492128610611 and batch: 50, loss is 4.380572662353516 and perplexity is 79.88376673482266
At time: 697.0480678081512 and batch: 100, loss is 4.343895454406738 and perplexity is 77.00693281867015
At time: 698.6044454574585 and batch: 150, loss is 4.342327642440796 and perplexity is 76.88629502133863
At time: 700.1606705188751 and batch: 200, loss is 4.40274151802063 and perplexity is 81.67447405826667
At time: 701.7166774272919 and batch: 250, loss is 4.397844285964966 and perplexity is 81.27547300358923
At time: 703.2733039855957 and batch: 300, loss is 4.372657680511475 and perplexity is 79.25398381963923
At time: 704.8296971321106 and batch: 350, loss is 4.337798519134521 and perplexity is 76.53885490410914
At time: 706.3867175579071 and batch: 400, loss is 4.35654658317566 and perplexity is 77.98734603245313
At time: 707.943220615387 and batch: 450, loss is 4.3951686668395995 and perplexity is 81.05830145732877
At time: 709.5269944667816 and batch: 500, loss is 4.373703536987304 and perplexity is 79.33691547159556
At time: 711.0828404426575 and batch: 550, loss is 4.403275737762451 and perplexity is 81.7181178313561
At time: 712.6396312713623 and batch: 600, loss is 4.429087333679199 and perplexity is 83.8548504779145
At time: 714.1975939273834 and batch: 650, loss is 4.416521406173706 and perplexity is 82.80772930873002
At time: 715.7589559555054 and batch: 700, loss is 4.3786549949646 and perplexity is 79.73072303081247
At time: 717.3150949478149 and batch: 750, loss is 4.379066514968872 and perplexity is 79.76354057036862
At time: 718.8728337287903 and batch: 800, loss is 4.350690212249756 and perplexity is 77.53195796880757
At time: 720.4302914142609 and batch: 850, loss is 4.3722828674316405 and perplexity is 79.22428395617099
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.459322293599446 and perplexity of 86.42891581461048
Finished 25 epochs...
Completing Train Step...
At time: 724.4614851474762 and batch: 50, loss is 4.362436571121216 and perplexity is 78.44804598719901
At time: 726.0437679290771 and batch: 100, loss is 4.326769819259644 and perplexity is 75.69936857502285
At time: 727.5984423160553 and batch: 150, loss is 4.324593982696533 and perplexity is 75.53483818160666
At time: 729.1536667346954 and batch: 200, loss is 4.385414505004883 and perplexity is 80.2714892519993
At time: 730.7077815532684 and batch: 250, loss is 4.379987468719483 and perplexity is 79.83703293856054
At time: 732.262369632721 and batch: 300, loss is 4.354919862747193 and perplexity is 77.86058555338363
At time: 733.8197028636932 and batch: 350, loss is 4.320462474822998 and perplexity is 75.22340918160685
At time: 735.3737261295319 and batch: 400, loss is 4.338956136703491 and perplexity is 76.62750893108426
At time: 736.9278569221497 and batch: 450, loss is 4.378237628936768 and perplexity is 79.69745307900172
At time: 738.4822554588318 and batch: 500, loss is 4.356550855636597 and perplexity is 77.98767923105443
At time: 740.0372214317322 and batch: 550, loss is 4.385899810791016 and perplexity is 80.31045492456319
At time: 741.5927469730377 and batch: 600, loss is 4.412758798599243 and perplexity is 82.49674174817746
At time: 743.1473958492279 and batch: 650, loss is 4.399509344100952 and perplexity is 81.4109141185436
At time: 744.7027537822723 and batch: 700, loss is 4.362095804214477 and perplexity is 78.42131804348625
At time: 746.2570378780365 and batch: 750, loss is 4.362478132247925 and perplexity is 78.45130644413202
At time: 747.8392233848572 and batch: 800, loss is 4.333723249435425 and perplexity is 76.22757313730025
At time: 749.395025730133 and batch: 850, loss is 4.3553596782684325 and perplexity is 77.89483737909299
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.454984029134114 and perplexity of 86.05477646486963
Finished 26 epochs...
Completing Train Step...
At time: 753.409998178482 and batch: 50, loss is 4.345116710662841 and perplexity is 77.10103546716648
At time: 754.9911904335022 and batch: 100, loss is 4.310662488937378 and perplexity is 74.48982128002181
At time: 756.5448267459869 and batch: 150, loss is 4.3076677417755125 and perplexity is 74.26707679733308
At time: 758.0991234779358 and batch: 200, loss is 4.3693068313598635 and perplexity is 78.98886011803222
At time: 759.6530504226685 and batch: 250, loss is 4.363562574386597 and perplexity is 78.53642849330036
At time: 761.2061638832092 and batch: 300, loss is 4.337828912734985 and perplexity is 76.54118123083762
At time: 762.7599241733551 and batch: 350, loss is 4.303866157531738 and perplexity is 73.98528022443283
At time: 764.3170981407166 and batch: 400, loss is 4.322418546676635 and perplexity is 75.37069557946782
At time: 765.8696355819702 and batch: 450, loss is 4.362580251693726 and perplexity is 78.45931825714244
At time: 767.4219698905945 and batch: 500, loss is 4.339597091674805 and perplexity is 76.67663945741862
At time: 768.975647687912 and batch: 550, loss is 4.369576406478882 and perplexity is 79.01015641974725
At time: 770.5290536880493 and batch: 600, loss is 4.397239656448364 and perplexity is 81.2263463068576
At time: 772.0820760726929 and batch: 650, loss is 4.383087358474731 and perplexity is 80.08490292538424
At time: 773.6358757019043 and batch: 700, loss is 4.34593433380127 and perplexity is 77.16410083610037
At time: 775.189325094223 and batch: 750, loss is 4.346473293304443 and perplexity is 77.20570037077512
At time: 776.7420268058777 and batch: 800, loss is 4.317398014068604 and perplexity is 74.99324284433465
At time: 778.294554233551 and batch: 850, loss is 4.339197587966919 and perplexity is 76.64601297375138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.451318422953288 and perplexity of 85.73991098335982
Finished 27 epochs...
Completing Train Step...
At time: 782.3175411224365 and batch: 50, loss is 4.328434267044067 and perplexity is 75.82547113778267
At time: 783.894095659256 and batch: 100, loss is 4.2951108169555665 and perplexity is 73.3403413480648
At time: 785.4470810890198 and batch: 150, loss is 4.291388502120972 and perplexity is 73.067852965889
At time: 787.0264778137207 and batch: 200, loss is 4.353789463043213 and perplexity is 77.77262169704767
At time: 788.5798442363739 and batch: 250, loss is 4.346744146347046 and perplexity is 77.22661460184048
At time: 790.1321864128113 and batch: 300, loss is 4.321385736465454 and perplexity is 75.2928921404587
At time: 791.6847450733185 and batch: 350, loss is 4.287775001525879 and perplexity is 72.80429869924171
At time: 793.2420327663422 and batch: 400, loss is 4.30628454208374 and perplexity is 74.16442161227536
At time: 794.795524597168 and batch: 450, loss is 4.3470453262329105 and perplexity is 77.24987720775064
At time: 796.3475329875946 and batch: 500, loss is 4.323769598007202 and perplexity is 75.47259407756698
At time: 797.8994832038879 and batch: 550, loss is 4.35426097869873 and perplexity is 77.8093013525848
At time: 799.4505186080933 and batch: 600, loss is 4.38225751876831 and perplexity is 80.018472860017
At time: 801.00226354599 and batch: 650, loss is 4.367227563858032 and perplexity is 78.824791778276
At time: 802.5529115200043 and batch: 700, loss is 4.330592107772827 and perplexity is 75.9892670869097
At time: 804.1054685115814 and batch: 750, loss is 4.331278562545776 and perplexity is 76.04144818992896
At time: 805.6584804058075 and batch: 800, loss is 4.301728811264038 and perplexity is 73.82731693314057
At time: 807.2116122245789 and batch: 850, loss is 4.323681650161743 and perplexity is 75.46595671740165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.447759628295898 and perplexity of 85.43532255153724
Finished 28 epochs...
Completing Train Step...
At time: 811.2604720592499 and batch: 50, loss is 4.312586297988892 and perplexity is 74.6332634058355
At time: 812.8183465003967 and batch: 100, loss is 4.280046911239624 and perplexity is 72.2438289754068
At time: 814.3762109279633 and batch: 150, loss is 4.275742969512939 and perplexity is 71.93356390571746
At time: 815.9335377216339 and batch: 200, loss is 4.338487396240234 and perplexity is 76.59159893394155
At time: 817.4921901226044 and batch: 250, loss is 4.331117382049561 and perplexity is 76.02919277926986
At time: 819.0497448444366 and batch: 300, loss is 4.305731420516968 and perplexity is 74.12341101416311
At time: 820.6062705516815 and batch: 350, loss is 4.272734479904175 and perplexity is 71.71747773566746
At time: 822.1609375476837 and batch: 400, loss is 4.29091007232666 and perplexity is 73.03290348912589
At time: 823.7163190841675 and batch: 450, loss is 4.332334899902344 and perplexity is 76.12181605261411
At time: 825.2716476917267 and batch: 500, loss is 4.308563346862793 and perplexity is 74.33362056314988
At time: 826.8540465831757 and batch: 550, loss is 4.339571676254272 and perplexity is 76.67469071314596
At time: 828.4090964794159 and batch: 600, loss is 4.3680947971343995 and perplexity is 78.89318091108846
At time: 829.9638485908508 and batch: 650, loss is 4.351902313232422 and perplexity is 77.62599150880999
At time: 831.5197863578796 and batch: 700, loss is 4.315946311950683 and perplexity is 74.88445397848676
At time: 833.075656414032 and batch: 750, loss is 4.3166037368774415 and perplexity is 74.93370107151841
At time: 834.6344227790833 and batch: 800, loss is 4.286760988235474 and perplexity is 72.73051158963945
At time: 836.1932151317596 and batch: 850, loss is 4.308824911117553 and perplexity is 74.3530661242371
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.444144248962402 and perplexity of 85.1269991409176
Finished 29 epochs...
Completing Train Step...
At time: 840.2455332279205 and batch: 50, loss is 4.29734658241272 and perplexity is 73.50449658779443
At time: 841.799375295639 and batch: 100, loss is 4.265553293228149 and perplexity is 71.20430593668662
At time: 843.352311372757 and batch: 150, loss is 4.260896592140198 and perplexity is 70.87349959867221
At time: 844.9053711891174 and batch: 200, loss is 4.323656759262085 and perplexity is 75.46407832522296
At time: 846.4582145214081 and batch: 250, loss is 4.315795078277588 and perplexity is 74.87312978377518
At time: 848.0117828845978 and batch: 300, loss is 4.290710210800171 and perplexity is 73.01830848008957
At time: 849.5658049583435 and batch: 350, loss is 4.257907991409302 and perplexity is 70.6620032025316
At time: 851.1184403896332 and batch: 400, loss is 4.27655179977417 and perplexity is 71.99176948504633
At time: 852.6754884719849 and batch: 450, loss is 4.317897052764892 and perplexity is 75.03067671417205
At time: 854.2285289764404 and batch: 500, loss is 4.293849277496338 and perplexity is 73.24787794887703
At time: 855.781002998352 and batch: 550, loss is 4.325317249298096 and perplexity is 75.5894897687693
At time: 857.3338768482208 and batch: 600, loss is 4.355679693222046 and perplexity is 77.91976888087754
At time: 858.8870997428894 and batch: 650, loss is 4.337256879806518 and perplexity is 76.49740967536893
At time: 860.4409682750702 and batch: 700, loss is 4.302073841094971 and perplexity is 73.8527939547337
At time: 861.9944677352905 and batch: 750, loss is 4.30247841835022 and perplexity is 73.88267916043401
At time: 863.5477879047394 and batch: 800, loss is 4.272188301086426 and perplexity is 71.67831786358565
At time: 865.1007490158081 and batch: 850, loss is 4.294515905380249 and perplexity is 73.29672330578464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.440698941548665 and perplexity of 84.83421511477178
Finished 30 epochs...
Completing Train Step...
At time: 869.1850514411926 and batch: 50, loss is 4.282811470031739 and perplexity is 72.44382761459612
At time: 870.7392473220825 and batch: 100, loss is 4.2516223812103275 and perplexity is 70.21924236329359
At time: 872.2920708656311 and batch: 150, loss is 4.246516571044922 and perplexity is 69.86162996921814
At time: 873.8458228111267 and batch: 200, loss is 4.309437227249146 and perplexity is 74.3986076475401
At time: 875.3985135555267 and batch: 250, loss is 4.301252241134644 and perplexity is 73.7921414216217
At time: 876.9527306556702 and batch: 300, loss is 4.276069316864014 and perplexity is 71.95704306472373
At time: 878.5066273212433 and batch: 350, loss is 4.24326608657837 and perplexity is 69.63491449337774
At time: 880.060629606247 and batch: 400, loss is 4.262017116546631 and perplexity is 70.95295959486039
At time: 881.6146972179413 and batch: 450, loss is 4.30390664100647 and perplexity is 73.9882754662838
At time: 883.1679437160492 and batch: 500, loss is 4.279292688369751 and perplexity is 72.1893615702497
At time: 884.7215142250061 and batch: 550, loss is 4.311851892471314 and perplexity is 74.57847244724918
At time: 886.2756445407867 and batch: 600, loss is 4.341745643615723 and perplexity is 76.84156030701033
At time: 887.8290045261383 and batch: 650, loss is 4.3230257320404055 and perplexity is 75.41647345910367
At time: 889.3835079669952 and batch: 700, loss is 4.288254261016846 and perplexity is 72.83919921290969
At time: 890.9365725517273 and batch: 750, loss is 4.289072647094726 and perplexity is 72.89883419836202
At time: 892.4902679920197 and batch: 800, loss is 4.258178796768188 and perplexity is 70.68114144292058
At time: 894.0436632633209 and batch: 850, loss is 4.2808887481689455 and perplexity is 72.30467210495733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.437624931335449 and perplexity of 84.57383428278712
Finished 31 epochs...
Completing Train Step...
At time: 898.0739696025848 and batch: 50, loss is 4.269023799896241 and perplexity is 71.45185025888662
At time: 899.6583602428436 and batch: 100, loss is 4.237974157333374 and perplexity is 69.26738478122078
At time: 901.216005563736 and batch: 150, loss is 4.2326211261749265 and perplexity is 68.89758497022697
At time: 902.7736852169037 and batch: 200, loss is 4.295918245315551 and perplexity is 73.3995823328073
At time: 904.3314161300659 and batch: 250, loss is 4.287646560668946 and perplexity is 72.79494825323086
At time: 905.9161684513092 and batch: 300, loss is 4.26182186126709 and perplexity is 70.93910700733986
At time: 907.4735698699951 and batch: 350, loss is 4.229443569183349 and perplexity is 68.67900642420881
At time: 909.0305411815643 and batch: 400, loss is 4.248633365631104 and perplexity is 70.00966931848723
At time: 910.5887525081635 and batch: 450, loss is 4.2904150104522705 and perplexity is 72.9967566312363
At time: 912.1458945274353 and batch: 500, loss is 4.2656940078735355 and perplexity is 71.21432613032395
At time: 913.7073452472687 and batch: 550, loss is 4.2988536930084225 and perplexity is 73.61535951379457
At time: 915.2653748989105 and batch: 600, loss is 4.328692655563355 and perplexity is 75.84506610044191
At time: 916.8225049972534 and batch: 650, loss is 4.309238338470459 and perplexity is 74.38381207071498
At time: 918.3802418708801 and batch: 700, loss is 4.275169944763183 and perplexity is 71.89235600096356
At time: 919.9369201660156 and batch: 750, loss is 4.276331729888916 and perplexity is 71.97592800777653
At time: 921.4952344894409 and batch: 800, loss is 4.244787397384644 and perplexity is 69.74093146322443
At time: 923.0531389713287 and batch: 850, loss is 4.267793807983399 and perplexity is 71.36401908779884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.434624036153157 and perplexity of 84.32041749971765
Finished 32 epochs...
Completing Train Step...
At time: 927.0656435489655 and batch: 50, loss is 4.255561590194702 and perplexity is 70.49639615865607
At time: 928.6470942497253 and batch: 100, loss is 4.224752254486084 and perplexity is 68.35756617020414
At time: 930.2014255523682 and batch: 150, loss is 4.2194150447845455 and perplexity is 67.99369938528706
At time: 931.7560927867889 and batch: 200, loss is 4.282681331634522 and perplexity is 72.43440050440991
At time: 933.311005115509 and batch: 250, loss is 4.273793468475342 and perplexity is 71.79346595316791
At time: 934.8658347129822 and batch: 300, loss is 4.248192310333252 and perplexity is 69.97879799141506
At time: 936.4209184646606 and batch: 350, loss is 4.215669031143189 and perplexity is 67.7394705295356
At time: 937.9762845039368 and batch: 400, loss is 4.235372991561889 and perplexity is 69.08744296155682
At time: 939.5361104011536 and batch: 450, loss is 4.27750412940979 and perplexity is 72.06036203682352
At time: 941.0953397750854 and batch: 500, loss is 4.252421960830689 and perplexity is 70.27541069097704
At time: 942.6486539840698 and batch: 550, loss is 4.285928754806519 and perplexity is 72.67000800663384
At time: 944.2460899353027 and batch: 600, loss is 4.315637636184692 and perplexity is 74.86134252944974
At time: 945.8051178455353 and batch: 650, loss is 4.29601806640625 and perplexity is 73.40690952487068
At time: 947.3580429553986 and batch: 700, loss is 4.2627709865570065 and perplexity is 71.00646907030652
At time: 948.9135806560516 and batch: 750, loss is 4.263737192153931 and perplexity is 71.07510907297754
At time: 950.4704864025116 and batch: 800, loss is 4.2319666767120365 and perplexity is 68.85250973408967
At time: 952.022748708725 and batch: 850, loss is 4.255009984970092 and perplexity is 70.45752070115725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.432040214538574 and perplexity of 84.1028298073762
Finished 33 epochs...
Completing Train Step...
At time: 956.0325131416321 and batch: 50, loss is 4.242597026824951 and perplexity is 69.58834015690191
At time: 957.6097431182861 and batch: 100, loss is 4.212027444839477 and perplexity is 67.49324000838953
At time: 959.1599678993225 and batch: 150, loss is 4.206743721961975 and perplexity is 67.13756490389342
At time: 960.7096526622772 and batch: 200, loss is 4.26998646736145 and perplexity is 71.52066774932483
At time: 962.2606673240662 and batch: 250, loss is 4.260884714126587 and perplexity is 70.87265776727898
At time: 963.811009645462 and batch: 300, loss is 4.235369424819947 and perplexity is 69.08719654491576
At time: 965.3645732402802 and batch: 350, loss is 4.202822847366333 and perplexity is 66.874842318978
At time: 966.9176745414734 and batch: 400, loss is 4.2227361869812015 and perplexity is 68.2198915296519
At time: 968.4699621200562 and batch: 450, loss is 4.264934072494507 and perplexity is 71.16022840239597
At time: 970.022264957428 and batch: 500, loss is 4.239350452423095 and perplexity is 69.36278277561014
At time: 971.5756540298462 and batch: 550, loss is 4.273660554885864 and perplexity is 71.78392426003143
At time: 973.132283449173 and batch: 600, loss is 4.303542051315308 and perplexity is 73.96130502065391
At time: 974.6873257160187 and batch: 650, loss is 4.283477287292481 and perplexity is 72.49207802665225
At time: 976.2408511638641 and batch: 700, loss is 4.250483150482178 and perplexity is 70.13929199429141
At time: 977.7938458919525 and batch: 750, loss is 4.251302251815796 and perplexity is 70.19676671750072
At time: 979.3473496437073 and batch: 800, loss is 4.219425773620605 and perplexity is 67.99442888245417
At time: 980.9012048244476 and batch: 850, loss is 4.242606925964355 and perplexity is 69.58902902499162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.429424285888672 and perplexity of 83.88311031589811
Finished 34 epochs...
Completing Train Step...
At time: 984.9558145999908 and batch: 50, loss is 4.230090398788452 and perplexity is 68.72344440916578
At time: 986.5066318511963 and batch: 100, loss is 4.19948486328125 and perplexity is 66.6519873097852
At time: 988.0576460361481 and batch: 150, loss is 4.194532465934754 and perplexity is 66.32271619836432
At time: 989.608883857727 and batch: 200, loss is 4.25766526222229 and perplexity is 70.64485355338606
At time: 991.1596782207489 and batch: 250, loss is 4.247838020324707 and perplexity is 69.9540095938768
At time: 992.7097434997559 and batch: 300, loss is 4.222688522338867 and perplexity is 68.21663993041582
At time: 994.2610347270966 and batch: 350, loss is 4.190109901428222 and perplexity is 66.03004735816863
At time: 995.8116810321808 and batch: 400, loss is 4.210389232635498 and perplexity is 67.38276227661369
At time: 997.3627922534943 and batch: 450, loss is 4.252681617736816 and perplexity is 70.2936605559431
At time: 998.9126045703888 and batch: 500, loss is 4.22727596282959 and perplexity is 68.53029860174973
At time: 1000.4628071784973 and batch: 550, loss is 4.262086305618286 and perplexity is 70.95786893410029
At time: 1002.0132768154144 and batch: 600, loss is 4.291783390045166 and perplexity is 73.09671227639359
At time: 1003.5643253326416 and batch: 650, loss is 4.270978832244873 and perplexity is 71.59167757644018
At time: 1005.1156251430511 and batch: 700, loss is 4.238402214050293 and perplexity is 69.29704149746523
At time: 1006.6666121482849 and batch: 750, loss is 4.239676656723023 and perplexity is 69.38541290442848
At time: 1008.217077255249 and batch: 800, loss is 4.207390718460083 and perplexity is 67.18101672835108
At time: 1009.7686896324158 and batch: 850, loss is 4.2307776260375975 and perplexity is 68.77068926493564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.427198092142741 and perplexity of 83.69657796593599
Finished 35 epochs...
Completing Train Step...
At time: 1013.8235981464386 and batch: 50, loss is 4.217824144363403 and perplexity is 67.88561417949634
At time: 1015.3786635398865 and batch: 100, loss is 4.187428832054138 and perplexity is 65.853253324863
At time: 1016.9341599941254 and batch: 150, loss is 4.18247905254364 and perplexity is 65.52809962428222
At time: 1018.4893918037415 and batch: 200, loss is 4.245786962509155 and perplexity is 69.81067691782091
At time: 1020.0455424785614 and batch: 250, loss is 4.235725355148316 and perplexity is 69.11179115019138
At time: 1021.6001346111298 and batch: 300, loss is 4.210471868515015 and perplexity is 67.38833074051298
At time: 1023.1823346614838 and batch: 350, loss is 4.177836956977845 and perplexity is 65.22461686654357
At time: 1024.7373208999634 and batch: 400, loss is 4.1983751726150516 and perplexity is 66.57806524448262
At time: 1026.2982199192047 and batch: 450, loss is 4.240844955444336 and perplexity is 69.46652316488354
At time: 1027.853301525116 and batch: 500, loss is 4.215207319259644 and perplexity is 67.70820163017379
At time: 1029.4136028289795 and batch: 550, loss is 4.25029504776001 and perplexity is 70.12609984331499
At time: 1030.9682471752167 and batch: 600, loss is 4.280420541763306 and perplexity is 72.27082651829863
At time: 1032.5289180278778 and batch: 650, loss is 4.259403314590454 and perplexity is 70.76774477315477
At time: 1034.0849347114563 and batch: 700, loss is 4.227000465393067 and perplexity is 68.51142128060697
At time: 1035.6413147449493 and batch: 750, loss is 4.228746728897095 and perplexity is 68.63116479663515
At time: 1037.1956765651703 and batch: 800, loss is 4.195783472061157 and perplexity is 66.40573824236344
At time: 1038.7496016025543 and batch: 850, loss is 4.219303913116455 and perplexity is 67.98614355190871
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.425859769185384 and perplexity of 83.58463983558453
Finished 36 epochs...
Completing Train Step...
At time: 1042.7857284545898 and batch: 50, loss is 4.205981321334839 and perplexity is 67.08639868936173
At time: 1044.335572719574 and batch: 100, loss is 4.175707259178162 and perplexity is 65.08585595536847
At time: 1045.8873100280762 and batch: 150, loss is 4.1708896398544315 and perplexity is 64.77305117020732
At time: 1047.4415197372437 and batch: 200, loss is 4.234288091659546 and perplexity is 69.01253064496485
At time: 1048.9918496608734 and batch: 250, loss is 4.223497791290283 and perplexity is 68.27186788320103
At time: 1050.5449893474579 and batch: 300, loss is 4.198301219940186 and perplexity is 66.57314180052357
At time: 1052.0983629226685 and batch: 350, loss is 4.1664652967453 and perplexity is 64.48710599383377
At time: 1053.6480057239532 and batch: 400, loss is 4.187286586761474 and perplexity is 65.8438866757676
At time: 1055.2032124996185 and batch: 450, loss is 4.229564189910889 and perplexity is 68.68729103556826
At time: 1056.7534427642822 and batch: 500, loss is 4.2036677169799805 and perplexity is 66.9313667156811
At time: 1058.3038551807404 and batch: 550, loss is 4.238834810256958 and perplexity is 69.32702561979134
At time: 1059.8543043136597 and batch: 600, loss is 4.269224472045899 and perplexity is 71.46619009402978
At time: 1061.4045414924622 and batch: 650, loss is 4.247640724182129 and perplexity is 69.9402092990433
At time: 1062.999664068222 and batch: 700, loss is 4.215888643264771 and perplexity is 67.75434857201289
At time: 1064.550078868866 and batch: 750, loss is 4.217518796920777 and perplexity is 67.86488864522097
At time: 1066.1003482341766 and batch: 800, loss is 4.184329996109009 and perplexity is 65.64950075729564
At time: 1067.6573770046234 and batch: 850, loss is 4.208047714233398 and perplexity is 67.22516887468726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.423292477925618 and perplexity of 83.37032893724204
Finished 37 epochs...
Completing Train Step...
At time: 1071.6698377132416 and batch: 50, loss is 4.194390907287597 and perplexity is 66.3133283088677
At time: 1073.246859073639 and batch: 100, loss is 4.164701571464539 and perplexity is 64.37346869665413
At time: 1074.7975788116455 and batch: 150, loss is 4.159816765785218 and perplexity is 64.05978358033099
At time: 1076.34894657135 and batch: 200, loss is 4.223131241798401 and perplexity is 68.24684745061218
At time: 1077.9000487327576 and batch: 250, loss is 4.212090983390808 and perplexity is 67.49752856732724
At time: 1079.457585811615 and batch: 300, loss is 4.187006411552429 and perplexity is 65.82544143512409
At time: 1081.0122032165527 and batch: 350, loss is 4.15526526927948 and perplexity is 63.76887822875472
At time: 1082.5626270771027 and batch: 400, loss is 4.176545867919922 and perplexity is 65.14046041582641
At time: 1084.1132230758667 and batch: 450, loss is 4.2183091259002685 and perplexity is 67.91854543387186
At time: 1085.6687452793121 and batch: 500, loss is 4.1928315925598145 and perplexity is 66.21000553668303
At time: 1087.2189390659332 and batch: 550, loss is 4.227728939056396 and perplexity is 68.561348229673
At time: 1088.7695517539978 and batch: 600, loss is 4.259182682037354 and perplexity is 70.7521328272634
At time: 1090.3190188407898 and batch: 650, loss is 4.236818809509277 and perplexity is 69.18740307117704
At time: 1091.8692529201508 and batch: 700, loss is 4.204867610931396 and perplexity is 67.01172545910698
At time: 1093.4224736690521 and batch: 750, loss is 4.206873226165771 and perplexity is 67.14626006379885
At time: 1094.9729588031769 and batch: 800, loss is 4.173145298957825 and perplexity is 64.91932199927872
At time: 1096.5231711864471 and batch: 850, loss is 4.197194843292237 and perplexity is 66.49952756110166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.422323226928711 and perplexity of 83.28956131118125
Finished 38 epochs...
Completing Train Step...
At time: 1100.6928279399872 and batch: 50, loss is 4.183222856521606 and perplexity is 65.57685781647203
At time: 1102.2746210098267 and batch: 100, loss is 4.1532022619247435 and perplexity is 63.637458170848845
At time: 1103.8303263187408 and batch: 150, loss is 4.148792290687561 and perplexity is 63.357436707824604
At time: 1105.3852546215057 and batch: 200, loss is 4.212548356056214 and perplexity is 67.52840715284859
At time: 1106.939961194992 and batch: 250, loss is 4.2011605072021485 and perplexity is 66.76376593156651
At time: 1108.4949464797974 and batch: 300, loss is 4.176132197380066 and perplexity is 65.11351929915708
At time: 1110.0502481460571 and batch: 350, loss is 4.144388632774353 and perplexity is 63.07904564968319
At time: 1111.6060235500336 and batch: 400, loss is 4.165576977729797 and perplexity is 64.42984630752414
At time: 1113.1615018844604 and batch: 450, loss is 4.207460975646972 and perplexity is 67.18573684340787
At time: 1114.7166681289673 and batch: 500, loss is 4.181762170791626 and perplexity is 65.48114055947683
At time: 1116.272587776184 and batch: 550, loss is 4.2169632148742675 and perplexity is 67.82719460353735
At time: 1117.827737569809 and batch: 600, loss is 4.248502035140991 and perplexity is 70.00047551802935
At time: 1119.3831760883331 and batch: 650, loss is 4.2256121158599855 and perplexity is 68.41636947869154
At time: 1120.9382197856903 and batch: 700, loss is 4.19422520160675 and perplexity is 66.3023407240288
At time: 1122.4934940338135 and batch: 750, loss is 4.196427249908448 and perplexity is 66.44850254945666
At time: 1124.0486381053925 and batch: 800, loss is 4.162379159927368 and perplexity is 64.22414047817202
At time: 1125.6047322750092 and batch: 850, loss is 4.186611223220825 and perplexity is 65.79943312817446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.4204559326171875 and perplexity of 83.13418030332002
Finished 39 epochs...
Completing Train Step...
At time: 1129.6291816234589 and batch: 50, loss is 4.172528667449951 and perplexity is 64.87930303960796
At time: 1131.1814386844635 and batch: 100, loss is 4.142677116394043 and perplexity is 62.97117716546973
At time: 1132.7378430366516 and batch: 150, loss is 4.13800639629364 and perplexity is 62.67774223221281
At time: 1134.290023803711 and batch: 200, loss is 4.201796269416809 and perplexity is 66.8062253068573
At time: 1135.841719865799 and batch: 250, loss is 4.190236644744873 and perplexity is 66.03841675574081
At time: 1137.393048286438 and batch: 300, loss is 4.16570212841034 and perplexity is 64.43791025123028
At time: 1138.9520452022552 and batch: 350, loss is 4.133290414810181 and perplexity is 62.38285105762339
At time: 1140.5040011405945 and batch: 400, loss is 4.154719681739807 and perplexity is 63.73409621255431
At time: 1142.0826194286346 and batch: 450, loss is 4.196878137588501 and perplexity is 66.47847011610985
At time: 1143.6349663734436 and batch: 500, loss is 4.171348190307617 and perplexity is 64.80275969308337
At time: 1145.1921994686127 and batch: 550, loss is 4.206588063240051 and perplexity is 67.12711516966465
At time: 1146.7451033592224 and batch: 600, loss is 4.237838039398193 and perplexity is 69.2579568894961
At time: 1148.3030097484589 and batch: 650, loss is 4.214941444396973 and perplexity is 67.6902021142791
At time: 1149.8557109832764 and batch: 700, loss is 4.184045143127442 and perplexity is 65.63080296445399
At time: 1151.408936738968 and batch: 750, loss is 4.18630446434021 and perplexity is 65.77925166330297
At time: 1152.963849067688 and batch: 800, loss is 4.15195140838623 and perplexity is 63.5579067950462
At time: 1154.5157496929169 and batch: 850, loss is 4.176423530578614 and perplexity is 65.13249179252759
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.419322649637858 and perplexity of 83.04001911749424
Finished 40 epochs...
Completing Train Step...
At time: 1158.5673513412476 and batch: 50, loss is 4.162181649208069 and perplexity is 64.21145677461476
At time: 1160.1183941364288 and batch: 100, loss is 4.132296233177185 and perplexity is 62.320861992188995
At time: 1161.6704874038696 and batch: 150, loss is 4.128341774940491 and perplexity is 62.07490338320933
At time: 1163.2218420505524 and batch: 200, loss is 4.191761155128479 and perplexity is 66.13916978780998
At time: 1164.7726221084595 and batch: 250, loss is 4.179833779335022 and perplexity is 65.35498896137848
At time: 1166.3227405548096 and batch: 300, loss is 4.155567774772644 and perplexity is 63.7881715827375
At time: 1167.8747644424438 and batch: 350, loss is 4.122824211120605 and perplexity is 61.73334429573195
At time: 1169.427399635315 and batch: 400, loss is 4.144291553497315 and perplexity is 63.07292227876626
At time: 1170.9794418811798 and batch: 450, loss is 4.186578893661499 and perplexity is 65.79730589588397
At time: 1172.529616355896 and batch: 500, loss is 4.161212825775147 and perplexity is 64.14927733594163
At time: 1174.0813512802124 and batch: 550, loss is 4.196265993118286 and perplexity is 66.43778814113324
At time: 1175.632747411728 and batch: 600, loss is 4.227735748291016 and perplexity is 68.56181508156838
At time: 1177.1845264434814 and batch: 650, loss is 4.204749169349671 and perplexity is 67.00378895436481
At time: 1178.7353355884552 and batch: 700, loss is 4.174274759292603 and perplexity is 64.992687222193
At time: 1180.2854957580566 and batch: 750, loss is 4.176927118301392 and perplexity is 65.1652999759541
At time: 1181.8625319004059 and batch: 800, loss is 4.1417539596557615 and perplexity is 62.913071723293704
At time: 1183.4132022857666 and batch: 850, loss is 4.166573438644409 and perplexity is 64.49408012903532
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.418488184611003 and perplexity of 82.97075402937737
Finished 41 epochs...
Completing Train Step...
At time: 1187.4527535438538 and batch: 50, loss is 4.15176143169403 and perplexity is 63.54583342101628
At time: 1189.0036466121674 and batch: 100, loss is 4.122197160720825 and perplexity is 61.69464651151006
At time: 1190.5531780719757 and batch: 150, loss is 4.118496556282043 and perplexity is 61.46676094601243
At time: 1192.1058084964752 and batch: 200, loss is 4.1818680620193485 and perplexity is 65.48807480497473
At time: 1193.6636385917664 and batch: 250, loss is 4.169548592567444 and perplexity is 64.68624566381934
At time: 1195.222547531128 and batch: 300, loss is 4.1452904272079465 and perplexity is 63.13595563862532
At time: 1196.776652097702 and batch: 350, loss is 4.112302899360657 and perplexity is 61.08723346142576
At time: 1198.3278365135193 and batch: 400, loss is 4.134047880172729 and perplexity is 62.43012180723417
At time: 1199.8786022663116 and batch: 450, loss is 4.17660349369049 and perplexity is 65.14421429321206
At time: 1201.4299612045288 and batch: 500, loss is 4.151684217453003 and perplexity is 63.5409269671447
At time: 1202.9809868335724 and batch: 550, loss is 4.186602535247803 and perplexity is 65.79886146695787
At time: 1204.532598733902 and batch: 600, loss is 4.2181307888031006 and perplexity is 67.9064341176159
At time: 1206.084240913391 and batch: 650, loss is 4.194579458236694 and perplexity is 66.32583292870001
At time: 1207.635675907135 and batch: 700, loss is 4.165057826042175 and perplexity is 64.39640612509498
At time: 1209.1869721412659 and batch: 750, loss is 4.167017593383789 and perplexity is 64.52273184281276
At time: 1210.7388274669647 and batch: 800, loss is 4.132026042938232 and perplexity is 62.30402577818833
At time: 1212.290816783905 and batch: 850, loss is 4.1566064739227295 and perplexity is 63.85446272466229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.417203903198242 and perplexity of 82.86426462799092
Finished 42 epochs...
Completing Train Step...
At time: 1216.3223249912262 and batch: 50, loss is 4.141856150627136 and perplexity is 62.919501199716905
At time: 1217.9036993980408 and batch: 100, loss is 4.112605266571045 and perplexity is 61.105707030557845
At time: 1219.4583699703217 and batch: 150, loss is 4.108830122947693 and perplexity is 60.87545909327931
At time: 1221.0402085781097 and batch: 200, loss is 4.172157731056213 and perplexity is 64.85524140785357
At time: 1222.60200881958 and batch: 250, loss is 4.159549469947815 and perplexity is 64.04266295507291
At time: 1224.1579027175903 and batch: 300, loss is 4.135474052429199 and perplexity is 62.51922143553051
At time: 1225.7136738300323 and batch: 350, loss is 4.102519812583924 and perplexity is 60.49252553835478
At time: 1227.2686159610748 and batch: 400, loss is 4.124716005325317 and perplexity is 61.85024161668178
At time: 1228.8239159584045 and batch: 450, loss is 4.166833353042603 and perplexity is 64.51084524771258
At time: 1230.3792617321014 and batch: 500, loss is 4.141791858673096 and perplexity is 62.91545611207221
At time: 1231.939962387085 and batch: 550, loss is 4.176717300415039 and perplexity is 65.15162856475303
At time: 1233.495617866516 and batch: 600, loss is 4.209190731048584 and perplexity is 67.30205230427052
At time: 1235.0503633022308 and batch: 650, loss is 4.185152077674866 and perplexity is 65.70349219131657
At time: 1236.606281042099 and batch: 700, loss is 4.155428442955017 and perplexity is 63.77928447999027
At time: 1238.1650981903076 and batch: 750, loss is 4.157732791900635 and perplexity is 63.92642367184541
At time: 1239.7208914756775 and batch: 800, loss is 4.12253812789917 and perplexity is 61.71568594772484
At time: 1241.276074886322 and batch: 850, loss is 4.147008914947509 and perplexity is 63.24454728429733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.41581376393636 and perplexity of 82.74915179030003
Finished 43 epochs...
Completing Train Step...
At time: 1245.286777496338 and batch: 50, loss is 4.132466773986817 and perplexity is 62.3314911487768
At time: 1246.863612651825 and batch: 100, loss is 4.102961735725403 and perplexity is 60.51926449312298
At time: 1248.4136672019958 and batch: 150, loss is 4.099468445777893 and perplexity is 60.30822198578704
At time: 1249.9696435928345 and batch: 200, loss is 4.162822713851929 and perplexity is 64.25263366639825
At time: 1251.51975107193 and batch: 250, loss is 4.149884543418884 and perplexity is 63.42667684810079
At time: 1253.0683813095093 and batch: 300, loss is 4.1264471912384035 and perplexity is 61.95740861992774
At time: 1254.6206212043762 and batch: 350, loss is 4.092931237220764 and perplexity is 59.915260396325564
At time: 1256.1775529384613 and batch: 400, loss is 4.115023713111878 and perplexity is 61.2536667605831
At time: 1257.7275929450989 and batch: 450, loss is 4.157356114387512 and perplexity is 63.902348560115115
At time: 1259.3222930431366 and batch: 500, loss is 4.132358856201172 and perplexity is 62.32476483522705
At time: 1260.8717231750488 and batch: 550, loss is 4.1673715209960935 and perplexity is 64.54557226092217
At time: 1262.4264566898346 and batch: 600, loss is 4.199695053100586 and perplexity is 66.66599835139382
At time: 1263.978670835495 and batch: 650, loss is 4.175469365119934 and perplexity is 65.07037425853689
At time: 1265.5288407802582 and batch: 700, loss is 4.146392531394959 and perplexity is 63.20557639730352
At time: 1267.0786669254303 and batch: 750, loss is 4.148450913429261 and perplexity is 63.335811611156444
At time: 1268.6293363571167 and batch: 800, loss is 4.113299827575684 and perplexity is 61.14816341438969
At time: 1270.1795620918274 and batch: 850, loss is 4.137842230796814 and perplexity is 62.66745355406537
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.416171073913574 and perplexity of 82.77872417077936
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 1274.2537014484406 and batch: 50, loss is 4.143532810211181 and perplexity is 63.02508427312823
At time: 1275.831800699234 and batch: 100, loss is 4.117351984977722 and perplexity is 61.39644810197193
At time: 1277.3886590003967 and batch: 150, loss is 4.117893137931824 and perplexity is 61.429681962723606
At time: 1278.9399950504303 and batch: 200, loss is 4.180547409057617 and perplexity is 65.40164486956257
At time: 1280.4903683662415 and batch: 250, loss is 4.166781578063965 and perplexity is 64.50750528654193
At time: 1282.0411121845245 and batch: 300, loss is 4.143024888038635 and perplexity is 62.99308056378415
At time: 1283.591099023819 and batch: 350, loss is 4.1022351408004765 and perplexity is 60.47530747408936
At time: 1285.1424520015717 and batch: 400, loss is 4.111145663261413 and perplexity is 61.01658199775808
At time: 1286.693618297577 and batch: 450, loss is 4.150446152687072 and perplexity is 63.46230786208639
At time: 1288.2437949180603 and batch: 500, loss is 4.1197677326202395 and perplexity is 61.54494572090234
At time: 1289.7934951782227 and batch: 550, loss is 4.152099957466126 and perplexity is 63.56734896491616
At time: 1291.3499512672424 and batch: 600, loss is 4.185045957565308 and perplexity is 65.69652009947298
At time: 1292.9006116390228 and batch: 650, loss is 4.158694987297058 and perplexity is 63.987962984085854
At time: 1294.451762676239 and batch: 700, loss is 4.121839280128479 and perplexity is 61.67257114528014
At time: 1296.003719329834 and batch: 750, loss is 4.116291170120239 and perplexity is 61.33135237099249
At time: 1297.5621857643127 and batch: 800, loss is 4.072530550956726 and perplexity is 58.70533161811714
At time: 1299.1451241970062 and batch: 850, loss is 4.093019847869873 and perplexity is 59.92056976167049
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.384540875752767 and perplexity of 80.2013923546861
Finished 45 epochs...
Completing Train Step...
At time: 1303.2294998168945 and batch: 50, loss is 4.131388311386108 and perplexity is 62.26430520200379
At time: 1304.7911493778229 and batch: 100, loss is 4.100455150604248 and perplexity is 60.3677577667803
At time: 1306.354402065277 and batch: 150, loss is 4.098307566642761 and perplexity is 60.23825205038378
At time: 1307.9159343242645 and batch: 200, loss is 4.162823271751404 and perplexity is 64.25266951291886
At time: 1309.4778356552124 and batch: 250, loss is 4.150098204612732 and perplexity is 63.44023011545003
At time: 1311.038694858551 and batch: 300, loss is 4.12806122303009 and perplexity is 62.05749059318718
At time: 1312.6037604808807 and batch: 350, loss is 4.088305287361145 and perplexity is 59.63873549419458
At time: 1314.1627564430237 and batch: 400, loss is 4.099201574325561 and perplexity is 60.29212959039414
At time: 1315.7235369682312 and batch: 450, loss is 4.1395803594589236 and perplexity is 62.77647236818558
At time: 1317.2834219932556 and batch: 500, loss is 4.111120138168335 and perplexity is 61.01502456370018
At time: 1318.8422017097473 and batch: 550, loss is 4.1450239229202275 and perplexity is 63.119131877640186
At time: 1320.4008305072784 and batch: 600, loss is 4.179617156982422 and perplexity is 65.34083314320421
At time: 1321.9585399627686 and batch: 650, loss is 4.154636569023133 and perplexity is 63.72879931879597
At time: 1323.5160942077637 and batch: 700, loss is 4.1194441652297975 and perplexity is 61.525035004823465
At time: 1325.0747027397156 and batch: 750, loss is 4.115343441963196 and perplexity is 61.27325445630435
At time: 1326.6421341896057 and batch: 800, loss is 4.0746444845199585 and perplexity is 58.82956205014276
At time: 1328.2015228271484 and batch: 850, loss is 4.096818351745606 and perplexity is 60.14861111192906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.383095741271973 and perplexity of 80.08557426370139
Finished 46 epochs...
Completing Train Step...
At time: 1332.3022043704987 and batch: 50, loss is 4.12628212928772 and perplexity is 61.94718265318386
At time: 1333.864640712738 and batch: 100, loss is 4.094436821937561 and perplexity is 60.0055358382911
At time: 1335.4167420864105 and batch: 150, loss is 4.091728057861328 and perplexity is 59.84321494219447
At time: 1336.9693989753723 and batch: 200, loss is 4.157052392959595 and perplexity is 63.88294299465538
At time: 1338.552232503891 and batch: 250, loss is 4.144257669448852 and perplexity is 63.070785149018626
At time: 1340.1058297157288 and batch: 300, loss is 4.122519631385803 and perplexity is 61.7145444332718
At time: 1341.659009695053 and batch: 350, loss is 4.08312352180481 and perplexity is 59.33050083834143
At time: 1343.2114284038544 and batch: 400, loss is 4.094339623451233 and perplexity is 59.999703674479655
At time: 1344.7637717723846 and batch: 450, loss is 4.135207262039184 and perplexity is 62.502544132831275
At time: 1346.3174817562103 and batch: 500, loss is 4.107508759498597 and perplexity is 60.79507360760388
At time: 1347.8713023662567 and batch: 550, loss is 4.141839275360107 and perplexity is 62.91843942529171
At time: 1349.4242568016052 and batch: 600, loss is 4.177023057937622 and perplexity is 65.1715522110424
At time: 1350.982827425003 and batch: 650, loss is 4.152406620979309 and perplexity is 63.58684574079558
At time: 1352.5395593643188 and batch: 700, loss is 4.1179825258255 and perplexity is 61.43517327802836
At time: 1354.095012664795 and batch: 750, loss is 4.11445568561554 and perplexity is 61.21888287364894
At time: 1355.6565434932709 and batch: 800, loss is 4.075153284072876 and perplexity is 58.85950212111365
At time: 1357.213012456894 and batch: 850, loss is 4.09791072845459 and perplexity is 60.21435195413058
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.38249937693278 and perplexity of 80.03782832153047
Finished 47 epochs...
Completing Train Step...
At time: 1361.3117604255676 and batch: 50, loss is 4.12222825050354 and perplexity is 61.69656461448135
At time: 1362.8650858402252 and batch: 100, loss is 4.0899019622802735 and perplexity is 59.73403522846827
At time: 1364.4189984798431 and batch: 150, loss is 4.087243638038635 and perplexity is 59.57545366855924
At time: 1365.9740121364594 and batch: 200, loss is 4.1531356954574585 and perplexity is 63.633222191060085
At time: 1367.5284633636475 and batch: 250, loss is 4.140445609092712 and perplexity is 62.83081319371215
At time: 1369.0826153755188 and batch: 300, loss is 4.118888621330261 and perplexity is 61.490864639405636
At time: 1370.6377415657043 and batch: 350, loss is 4.079662094116211 and perplexity is 59.12548762386651
At time: 1372.1925420761108 and batch: 400, loss is 4.090881929397583 and perplexity is 59.79260131050677
At time: 1373.7523348331451 and batch: 450, loss is 4.132018570899963 and perplexity is 62.303560241862655
At time: 1375.3068306446075 and batch: 500, loss is 4.104797573089599 and perplexity is 60.630470066573565
At time: 1376.8611235618591 and batch: 550, loss is 4.139584875106811 and perplexity is 62.77675584527046
At time: 1378.45849943161 and batch: 600, loss is 4.175033664703369 and perplexity is 65.04202924478194
At time: 1380.0158243179321 and batch: 650, loss is 4.150609545707702 and perplexity is 63.47267800744574
At time: 1381.56831240654 and batch: 700, loss is 4.116557760238647 and perplexity is 61.34770488309543
At time: 1383.1282217502594 and batch: 750, loss is 4.113402023315429 and perplexity is 61.154412815509545
At time: 1384.6816692352295 and batch: 800, loss is 4.075056467056275 and perplexity is 58.853803795571544
At time: 1386.2360258102417 and batch: 850, loss is 4.098088970184326 and perplexity is 60.225085620943105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.382101058959961 and perplexity of 80.00595416445081
Finished 48 epochs...
Completing Train Step...
At time: 1390.2817680835724 and batch: 50, loss is 4.118818874359131 and perplexity is 61.486575987406844
At time: 1391.8629732131958 and batch: 100, loss is 4.086173114776611 and perplexity is 59.511710884715384
At time: 1393.4186942577362 and batch: 150, loss is 4.08372845172882 and perplexity is 59.36640249160246
At time: 1394.9727210998535 and batch: 200, loss is 4.150048904418945 and perplexity is 63.43710257690621
At time: 1396.526180267334 and batch: 250, loss is 4.137434115409851 and perplexity is 62.641883220186465
At time: 1398.0798449516296 and batch: 300, loss is 4.115961952209473 and perplexity is 61.311164314617635
At time: 1399.6352660655975 and batch: 350, loss is 4.076909861564636 and perplexity is 58.96298425830392
At time: 1401.195336818695 and batch: 400, loss is 4.0881633996963505 and perplexity is 59.63027409358241
At time: 1402.748610496521 and batch: 450, loss is 4.129579858779907 and perplexity is 62.15180491336904
At time: 1404.3032820224762 and batch: 500, loss is 4.102658672332764 and perplexity is 60.50092609849407
At time: 1405.8568406105042 and batch: 550, loss is 4.137659220695496 and perplexity is 62.655985826428115
At time: 1407.412053346634 and batch: 600, loss is 4.173320322036743 and perplexity is 64.9306853732937
At time: 1408.966133594513 and batch: 650, loss is 4.148949694633484 and perplexity is 63.36741020327914
At time: 1410.5232191085815 and batch: 700, loss is 4.115212802886963 and perplexity is 61.265250297783695
At time: 1412.0800869464874 and batch: 750, loss is 4.112260546684265 and perplexity is 61.08464630838212
At time: 1413.6322643756866 and batch: 800, loss is 4.074613757133484 and perplexity is 58.82775439922587
At time: 1415.1960835456848 and batch: 850, loss is 4.097738666534424 and perplexity is 60.20399224839279
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.381910959879558 and perplexity of 79.99074655165977
Finished 49 epochs...
Completing Train Step...
At time: 1419.2710282802582 and batch: 50, loss is 4.115798015594482 and perplexity is 61.301113993709066
At time: 1420.8654551506042 and batch: 100, loss is 4.083081955909729 and perplexity is 59.32803476422116
At time: 1422.4233326911926 and batch: 150, loss is 4.08066915512085 and perplexity is 59.185060588602866
At time: 1423.9825382232666 and batch: 200, loss is 4.147361273765564 and perplexity is 63.26683598480631
At time: 1425.5456833839417 and batch: 250, loss is 4.13484146118164 and perplexity is 62.479684829820016
At time: 1427.1061351299286 and batch: 300, loss is 4.113421177864074 and perplexity is 61.155584211903424
At time: 1428.6681642532349 and batch: 350, loss is 4.074508934020996 and perplexity is 58.82158821409407
At time: 1430.231789112091 and batch: 400, loss is 4.08578022480011 and perplexity is 59.48833392261211
At time: 1431.7899053096771 and batch: 450, loss is 4.127386636734009 and perplexity is 62.01564157743711
At time: 1433.3516266345978 and batch: 500, loss is 4.100637264251709 and perplexity is 60.37875256045685
At time: 1434.9112763404846 and batch: 550, loss is 4.135790023803711 and perplexity is 62.53897884108383
At time: 1436.4748032093048 and batch: 600, loss is 4.1716493797302245 and perplexity is 64.82228053844722
At time: 1438.0332493782043 and batch: 650, loss is 4.147405858039856 and perplexity is 63.269656753655944
At time: 1439.5967602729797 and batch: 700, loss is 4.113832507133484 and perplexity is 61.18074446789774
At time: 1441.1545395851135 and batch: 750, loss is 4.1110458135604855 and perplexity is 61.01048981445037
At time: 1442.7180378437042 and batch: 800, loss is 4.073929114341736 and perplexity is 58.787492185411615
At time: 1444.2827360630035 and batch: 850, loss is 4.0971364402771 and perplexity is 60.16774673855941
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.381725629170735 and perplexity of 79.97592318355721
Finished Training.
Improved accuracyfrom -86.85138419959085 to -79.97592318355721
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fc6d1107b70>
SETTINGS FOR THIS RUN
{'lr': 7.10344333344402, 'anneal': 8.0, 'wordvec_source': 'glove', 'seq_len': 50, 'tune_wordvecs': True, 'num_layers': 1, 'dropout': 0.0, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.1826908588409424 and batch: 50, loss is 7.448379878997803 and perplexity is 1717.0790148963054
At time: 3.716365337371826 and batch: 100, loss is 6.541864614486695 and perplexity is 693.5786298337431
At time: 5.25041127204895 and batch: 150, loss is 6.115929126739502 and perplexity is 453.01676187928337
At time: 6.78980278968811 and batch: 200, loss is 5.895507602691651 and perplexity is 363.4012525436879
At time: 8.351416826248169 and batch: 250, loss is 5.719338493347168 and perplexity is 304.70329301901666
At time: 9.894964694976807 and batch: 300, loss is 5.531902961730957 and perplexity is 252.62418806636651
At time: 11.430992841720581 and batch: 350, loss is 5.408467607498169 and perplexity is 223.28915874718513
At time: 12.967509031295776 and batch: 400, loss is 5.3509736919403075 and perplexity is 210.813465337616
At time: 14.503946304321289 and batch: 450, loss is 5.290193128585815 and perplexity is 198.38173489384545
At time: 16.041334867477417 and batch: 500, loss is 5.243961610794067 and perplexity is 189.41902240516626
At time: 17.57801866531372 and batch: 550, loss is 5.198361864089966 and perplexity is 180.97553639454486
At time: 19.11559247970581 and batch: 600, loss is 5.2147297763824465 and perplexity is 183.96210335744925
At time: 20.654691457748413 and batch: 650, loss is 5.190922136306763 and perplexity is 179.63412372810177
At time: 22.19151210784912 and batch: 700, loss is 5.115499906539917 and perplexity is 166.58403662091845
At time: 23.728938102722168 and batch: 750, loss is 5.08527398109436 and perplexity is 161.62421515128364
At time: 25.265360593795776 and batch: 800, loss is 5.05765661239624 and perplexity is 157.22165302458555
At time: 26.802539110183716 and batch: 850, loss is 5.034672193527221 and perplexity is 153.64921714767365
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.78179136912028 and perplexity of 119.31790111686604
Finished 1 epochs...
Completing Train Step...
At time: 30.845062017440796 and batch: 50, loss is 5.023541822433471 and perplexity is 151.94852655873214
At time: 32.3818724155426 and batch: 100, loss is 4.92695273399353 and perplexity is 137.9584749882266
At time: 33.919204235076904 and batch: 150, loss is 4.920569944381714 and perplexity is 137.08071931209415
At time: 35.45796275138855 and batch: 200, loss is 4.916071662902832 and perplexity is 136.46547645650105
At time: 36.996577739715576 and batch: 250, loss is 4.901854982376099 and perplexity is 134.539116045322
At time: 38.536627531051636 and batch: 300, loss is 4.850288066864014 and perplexity is 127.77719292015829
At time: 40.07533144950867 and batch: 350, loss is 4.781639947891235 and perplexity is 119.2998352214464
At time: 41.61618447303772 and batch: 400, loss is 4.779272708892822 and perplexity is 119.01775800282806
At time: 43.15309762954712 and batch: 450, loss is 4.7755044651031495 and perplexity is 118.57011402089984
At time: 44.69135093688965 and batch: 500, loss is 4.755649127960205 and perplexity is 116.23908271869544
At time: 46.2286171913147 and batch: 550, loss is 4.760213708877563 and perplexity is 116.77087820594365
At time: 47.767040967941284 and batch: 600, loss is 4.790253982543946 and perplexity is 120.33192699134538
At time: 49.306884765625 and batch: 650, loss is 4.777884559631348 and perplexity is 118.85265820804486
At time: 50.84714150428772 and batch: 700, loss is 4.7281011295318605 and perplexity is 113.08063287733874
At time: 52.387542724609375 and batch: 750, loss is 4.708518304824829 and perplexity is 110.88773629908849
At time: 53.97631096839905 and batch: 800, loss is 4.675358448028565 and perplexity is 107.27101128402734
At time: 55.51482677459717 and batch: 850, loss is 4.685156087875367 and perplexity is 108.32717954794201
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.602184295654297 and perplexity of 99.70185630032536
Finished 2 epochs...
Completing Train Step...
At time: 59.559226274490356 and batch: 50, loss is 4.673366928100586 and perplexity is 107.0575915127625
At time: 61.106642961502075 and batch: 100, loss is 4.606190500259399 and perplexity is 100.10208349689904
At time: 62.65492296218872 and batch: 150, loss is 4.609640264511109 and perplexity is 100.44800842272242
At time: 64.19747853279114 and batch: 200, loss is 4.6266937351226805 and perplexity is 102.1756851543685
At time: 65.74265050888062 and batch: 250, loss is 4.6210496044158935 and perplexity is 101.6006166398569
At time: 67.30051708221436 and batch: 300, loss is 4.589102296829224 and perplexity is 98.40605107471475
At time: 68.85314631462097 and batch: 350, loss is 4.5302674770355225 and perplexity is 92.78337518550278
At time: 70.39707779884338 and batch: 400, loss is 4.541824550628662 and perplexity is 93.86189977025381
At time: 71.94520878791809 and batch: 450, loss is 4.553890657424927 and perplexity is 95.00130776231788
At time: 73.50297355651855 and batch: 500, loss is 4.53854772567749 and perplexity is 93.55483413005979
At time: 75.06131744384766 and batch: 550, loss is 4.551842041015625 and perplexity is 94.80688574039942
At time: 76.6186842918396 and batch: 600, loss is 4.5838729095458985 and perplexity is 97.89279091036548
At time: 78.17799758911133 and batch: 650, loss is 4.572986173629761 and perplexity is 96.83283812950464
At time: 79.73644375801086 and batch: 700, loss is 4.532684783935547 and perplexity is 93.00793238102779
At time: 81.29588317871094 and batch: 750, loss is 4.5130527973175045 and perplexity is 91.1998085018354
At time: 82.85351586341858 and batch: 800, loss is 4.481334571838379 and perplexity is 88.35250679653329
At time: 84.41288781166077 and batch: 850, loss is 4.500746593475342 and perplexity is 90.08436259752074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.552802721659343 and perplexity of 94.89800864341775
Finished 3 epochs...
Completing Train Step...
At time: 88.5025999546051 and batch: 50, loss is 4.495189628601074 and perplexity is 89.58515528222382
At time: 90.09142994880676 and batch: 100, loss is 4.431200551986694 and perplexity is 84.03224145002719
At time: 91.63548040390015 and batch: 150, loss is 4.437762479782105 and perplexity is 84.58546808240762
At time: 93.18190598487854 and batch: 200, loss is 4.4670023727417 and perplexity is 87.09525221113523
At time: 94.74001836776733 and batch: 250, loss is 4.458193855285645 and perplexity is 86.33144112201727
At time: 96.28784966468811 and batch: 300, loss is 4.430829439163208 and perplexity is 84.00106179358183
At time: 97.8316695690155 and batch: 350, loss is 4.378958950042724 and perplexity is 79.75496127244195
At time: 99.38285398483276 and batch: 400, loss is 4.398937921524048 and perplexity is 81.36440737297754
At time: 100.92965412139893 and batch: 450, loss is 4.419076957702637 and perplexity is 83.01961936062692
At time: 102.48114109039307 and batch: 500, loss is 4.40398157119751 and perplexity is 81.77581757197909
At time: 104.02688884735107 and batch: 550, loss is 4.418029565811157 and perplexity is 82.93271080607761
At time: 105.5738685131073 and batch: 600, loss is 4.4524337196350094 and perplexity is 85.83558976676767
At time: 107.12180209159851 and batch: 650, loss is 4.440517988204956 and perplexity is 84.8188654687126
At time: 108.67261457443237 and batch: 700, loss is 4.403293514251709 and perplexity is 81.71957050551163
At time: 110.22718358039856 and batch: 750, loss is 4.388971815109253 and perplexity is 80.55754833057347
At time: 111.77583765983582 and batch: 800, loss is 4.360986375808716 and perplexity is 78.33436344949874
At time: 113.32153987884521 and batch: 850, loss is 4.382053327560425 and perplexity is 80.00213545942412
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.524984995524089 and perplexity of 92.2945409870515
Finished 4 epochs...
Completing Train Step...
At time: 117.444100856781 and batch: 50, loss is 4.375196142196655 and perplexity is 79.45542258508823
At time: 118.99359917640686 and batch: 100, loss is 4.319273138046265 and perplexity is 75.1339963960927
At time: 120.5413384437561 and batch: 150, loss is 4.321823654174804 and perplexity is 75.32587145190041
At time: 122.08713269233704 and batch: 200, loss is 4.355936546325683 and perplexity is 77.93978538588993
At time: 123.63279747962952 and batch: 250, loss is 4.345707817077637 and perplexity is 77.1466238562857
At time: 125.17944121360779 and batch: 300, loss is 4.325268354415893 and perplexity is 75.58579391992609
At time: 126.72797727584839 and batch: 350, loss is 4.277395544052124 and perplexity is 72.05253776144674
At time: 128.27531385421753 and batch: 400, loss is 4.299456338882447 and perplexity is 73.65973687704471
At time: 129.85278725624084 and batch: 450, loss is 4.320007619857788 and perplexity is 75.18920122086074
At time: 131.39818477630615 and batch: 500, loss is 4.309872951507568 and perplexity is 74.43103198920811
At time: 132.94729495048523 and batch: 550, loss is 4.320650835037231 and perplexity is 75.23757961360656
At time: 134.4917597770691 and batch: 600, loss is 4.360769176483155 and perplexity is 78.31735112618868
At time: 136.04020881652832 and batch: 650, loss is 4.343443307876587 and perplexity is 76.97212227152573
At time: 137.58521676063538 and batch: 700, loss is 4.313527002334594 and perplexity is 74.7035042738204
At time: 139.13394331932068 and batch: 750, loss is 4.299940919876098 and perplexity is 73.69543963527309
At time: 140.6790680885315 and batch: 800, loss is 4.267773199081421 and perplexity is 71.36254836887971
At time: 142.2265923023224 and batch: 850, loss is 4.288601503372193 and perplexity is 72.8644964598896
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.510851860046387 and perplexity of 90.99930417384815
Finished 5 epochs...
Completing Train Step...
At time: 146.2727448940277 and batch: 50, loss is 4.2861844921112064 and perplexity is 72.68859481518194
At time: 147.81847310066223 and batch: 100, loss is 4.23053542137146 and perplexity is 68.75403470009026
At time: 149.36636638641357 and batch: 150, loss is 4.2316060256958 and perplexity is 68.8276824837393
At time: 150.91526794433594 and batch: 200, loss is 4.272412853240967 and perplexity is 71.694415191573
At time: 152.46103882789612 and batch: 250, loss is 4.2597317790985105 and perplexity is 70.79099328357421
At time: 154.0094997882843 and batch: 300, loss is 4.247301425933838 and perplexity is 69.91648273396162
At time: 155.55927085876465 and batch: 350, loss is 4.196347761154175 and perplexity is 66.44322085068632
At time: 157.11174941062927 and batch: 400, loss is 4.220366353988648 and perplexity is 68.05841319387366
At time: 158.66618466377258 and batch: 450, loss is 4.245181121826172 and perplexity is 69.76839557880842
At time: 160.2214274406433 and batch: 500, loss is 4.2370720481872555 and perplexity is 69.20492621633898
At time: 161.77550101280212 and batch: 550, loss is 4.2439293384552 and perplexity is 69.68111530080088
At time: 163.32833170890808 and batch: 600, loss is 4.289379253387451 and perplexity is 72.92118886652537
At time: 164.88148307800293 and batch: 650, loss is 4.2696271324157715 and perplexity is 71.49497249094284
At time: 166.43485832214355 and batch: 700, loss is 4.242164182662964 and perplexity is 69.55822576801148
At time: 167.98778557777405 and batch: 750, loss is 4.230107479095459 and perplexity is 68.7246182367195
At time: 169.5776243209839 and batch: 800, loss is 4.195935964584351 and perplexity is 66.41586539308031
At time: 171.13100790977478 and batch: 850, loss is 4.220276756286621 and perplexity is 68.05231558961766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.510923067728679 and perplexity of 91.00578425410156
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 175.2056369781494 and batch: 50, loss is 4.243930130004883 and perplexity is 69.68117045688741
At time: 176.76307702064514 and batch: 100, loss is 4.193140754699707 and perplexity is 66.23047832821997
At time: 178.32161903381348 and batch: 150, loss is 4.18070511341095 and perplexity is 65.41195980700752
At time: 179.87935876846313 and batch: 200, loss is 4.212265329360962 and perplexity is 67.50929751533285
At time: 181.4354727268219 and batch: 250, loss is 4.175649013519287 and perplexity is 65.08206509720652
At time: 183.000324010849 and batch: 300, loss is 4.16055613040924 and perplexity is 64.10716463191109
At time: 184.557541847229 and batch: 350, loss is 4.088739199638367 and perplexity is 59.66461908892215
At time: 186.11487483978271 and batch: 400, loss is 4.087722582817078 and perplexity is 59.603993855058214
At time: 187.67264080047607 and batch: 450, loss is 4.109353003501892 and perplexity is 60.907298010316985
At time: 189.23222851753235 and batch: 500, loss is 4.076676154136658 and perplexity is 58.94920578103596
At time: 190.78982615470886 and batch: 550, loss is 4.060668330192566 and perplexity is 58.013070011886455
At time: 192.34801602363586 and batch: 600, loss is 4.075835161209106 and perplexity is 58.8996507565226
At time: 193.90527033805847 and batch: 650, loss is 4.026568088531494 and perplexity is 56.06815972285944
At time: 195.4621286392212 and batch: 700, loss is 3.97914457321167 and perplexity is 53.47127389787733
At time: 197.02511978149414 and batch: 750, loss is 3.939937491416931 and perplexity is 51.41538729706869
At time: 198.58482003211975 and batch: 800, loss is 3.872558012008667 and perplexity is 48.06518026768256
At time: 200.14272260665894 and batch: 850, loss is 3.87488329410553 and perplexity is 48.17707541431458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.390884081522624 and perplexity of 80.71174320844474
Finished 7 epochs...
Completing Train Step...
At time: 204.16391944885254 and batch: 50, loss is 4.116899375915527 and perplexity is 61.36866580090931
At time: 205.7403872013092 and batch: 100, loss is 4.076455473899841 and perplexity is 58.93619829164474
At time: 207.29520726203918 and batch: 150, loss is 4.06673442363739 and perplexity is 58.366052244799945
At time: 208.87421584129333 and batch: 200, loss is 4.108072814941406 and perplexity is 60.82937507282288
At time: 210.42954874038696 and batch: 250, loss is 4.078985056877136 and perplexity is 59.08547101486251
At time: 211.98068261146545 and batch: 300, loss is 4.072037014961243 and perplexity is 58.676365572317906
At time: 213.53100156784058 and batch: 350, loss is 4.008146948814392 and perplexity is 55.04477521275355
At time: 215.0820004940033 and batch: 400, loss is 4.011100540161133 and perplexity is 55.207595318166256
At time: 216.63373494148254 and batch: 450, loss is 4.040243482589721 and perplexity is 56.84018071515637
At time: 218.18400287628174 and batch: 500, loss is 4.013617930412292 and perplexity is 55.34674945956173
At time: 219.7354633808136 and batch: 550, loss is 4.00541118144989 and perplexity is 54.8943913146152
At time: 221.287180185318 and batch: 600, loss is 4.030514006614685 and perplexity is 56.28983716107576
At time: 222.8445315361023 and batch: 650, loss is 3.9893904399871825 and perplexity is 54.02194970464274
At time: 224.40974640846252 and batch: 700, loss is 3.9500819635391236 and perplexity is 51.93962381575913
At time: 225.9622094631195 and batch: 750, loss is 3.923291983604431 and perplexity is 50.56663561459914
At time: 227.51463222503662 and batch: 800, loss is 3.868810729980469 and perplexity is 47.88540352892501
At time: 229.06835341453552 and batch: 850, loss is 3.8841073751449584 and perplexity is 48.62352051966671
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.381589571634929 and perplexity of 79.96504259673469
Finished 8 epochs...
Completing Train Step...
At time: 233.11173939704895 and batch: 50, loss is 4.074731492996216 and perplexity is 58.83468094338597
At time: 234.69075083732605 and batch: 100, loss is 4.032389359474182 and perplexity is 56.39549951428712
At time: 236.24222946166992 and batch: 150, loss is 4.021110506057739 and perplexity is 55.76299660091839
At time: 237.7942497730255 and batch: 200, loss is 4.064838256835937 and perplexity is 58.255485333982435
At time: 239.34701132774353 and batch: 250, loss is 4.036936740875245 and perplexity is 56.65253533691169
At time: 240.8993055820465 and batch: 300, loss is 4.031767873764038 and perplexity is 56.36046140619208
At time: 242.4512701034546 and batch: 350, loss is 3.9707821941375734 and perplexity is 53.02599124200897
At time: 244.0029640197754 and batch: 400, loss is 3.976420269012451 and perplexity is 53.325800129232654
At time: 245.555988073349 and batch: 450, loss is 4.008251342773438 and perplexity is 55.0505218547149
At time: 247.13402366638184 and batch: 500, loss is 3.9838675165176394 and perplexity is 53.7244130032693
At time: 248.6843454837799 and batch: 550, loss is 3.9785168886184694 and perplexity is 53.43772133438802
At time: 250.23710370063782 and batch: 600, loss is 4.007755722999573 and perplexity is 55.023244487681715
At time: 251.788889169693 and batch: 650, loss is 3.9700370740890505 and perplexity is 52.98649522930456
At time: 253.34053587913513 and batch: 700, loss is 3.9349421453475952 and perplexity is 51.15919007387767
At time: 254.89283752441406 and batch: 750, loss is 3.9138038396835326 and perplexity is 50.08912104412046
At time: 256.44513511657715 and batch: 800, loss is 3.864964413642883 and perplexity is 47.70157487720536
At time: 257.99972200393677 and batch: 850, loss is 3.8847091722488405 and perplexity is 48.6527908200036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.377729415893555 and perplexity of 79.65696008424169
Finished 9 epochs...
Completing Train Step...
At time: 262.01620841026306 and batch: 50, loss is 4.044341745376587 and perplexity is 57.073604702289074
At time: 263.5994658470154 and batch: 100, loss is 4.001646990776062 and perplexity is 54.688146774196476
At time: 265.1630656719208 and batch: 150, loss is 3.989796552658081 and perplexity is 54.043893158380165
At time: 266.72093200683594 and batch: 200, loss is 4.035528478622436 and perplexity is 56.5728098602365
At time: 268.27939796447754 and batch: 250, loss is 4.0087112617492675 and perplexity is 55.07584645753151
At time: 269.8374893665314 and batch: 300, loss is 4.00463939666748 and perplexity is 54.8520410035239
At time: 271.3961236476898 and batch: 350, loss is 3.945002999305725 and perplexity is 51.67649310547622
At time: 272.95512771606445 and batch: 400, loss is 3.9526101636886595 and perplexity is 52.07110371417367
At time: 274.51449704170227 and batch: 450, loss is 3.985926570892334 and perplexity is 53.835148456927925
At time: 276.07662200927734 and batch: 500, loss is 3.9636264038085938 and perplexity is 52.64790274105847
At time: 277.6368479728699 and batch: 550, loss is 3.9597661685943604 and perplexity is 52.44506121283242
At time: 279.19978523254395 and batch: 600, loss is 3.9908298444747925 and perplexity is 54.09976513197932
At time: 280.7709746360779 and batch: 650, loss is 3.9555820846557617 and perplexity is 52.22608510122848
At time: 282.332722902298 and batch: 700, loss is 3.9233965635299684 and perplexity is 50.57192414611868
At time: 283.88951802253723 and batch: 750, loss is 3.9052967834472656 and perplexity is 49.66481742086663
At time: 285.4459240436554 and batch: 800, loss is 3.859537034034729 and perplexity is 47.44338161265721
At time: 287.03478384017944 and batch: 850, loss is 3.8813034296035767 and perplexity is 48.487373779322134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.37568473815918 and perplexity of 79.49425366934699
Finished 10 epochs...
Completing Train Step...
At time: 291.0722990036011 and batch: 50, loss is 4.0200357913970945 and perplexity is 55.70309948287052
At time: 292.6251537799835 and batch: 100, loss is 3.9776242399215698 and perplexity is 53.390041505908
At time: 294.18445324897766 and batch: 150, loss is 3.9654163408279417 and perplexity is 52.74222356014594
At time: 295.73779296875 and batch: 200, loss is 4.012684459686279 and perplexity is 55.2951089953419
At time: 297.29106855392456 and batch: 250, loss is 3.986596999168396 and perplexity is 53.87125316415542
At time: 298.8446888923645 and batch: 300, loss is 3.983610806465149 and perplexity is 53.71062317645653
At time: 300.39764046669006 and batch: 350, loss is 3.9246036052703857 and perplexity is 50.63300342465982
At time: 301.9518013000488 and batch: 400, loss is 3.933834962844849 and perplexity is 51.10257885902954
At time: 303.50601983070374 and batch: 450, loss is 3.968647584915161 and perplexity is 52.912922194126196
At time: 305.0615305900574 and batch: 500, loss is 3.947391266822815 and perplexity is 51.80005788950451
At time: 306.61545300483704 and batch: 550, loss is 3.9444198989868164 and perplexity is 51.646369309317436
At time: 308.16915345191956 and batch: 600, loss is 3.97687602519989 and perplexity is 53.350109231682836
At time: 309.7232666015625 and batch: 650, loss is 3.9432786989212034 and perplexity is 51.58746408698764
At time: 311.277706861496 and batch: 700, loss is 3.9131086969375612 and perplexity is 50.054314054289435
At time: 312.8316869735718 and batch: 750, loss is 3.897015633583069 and perplexity is 49.25523387686138
At time: 314.3856289386749 and batch: 800, loss is 3.8532487678527834 and perplexity is 47.14598104745455
At time: 315.9397883415222 and batch: 850, loss is 3.8763244676589967 and perplexity is 48.24655699676862
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.374519983927409 and perplexity of 79.4017163030904
Finished 11 epochs...
Completing Train Step...
At time: 319.98328161239624 and batch: 50, loss is 3.9994904470443724 and perplexity is 54.57033647126619
At time: 321.53605222702026 and batch: 100, loss is 3.9575218248367308 and perplexity is 52.32748845327732
At time: 323.0882067680359 and batch: 150, loss is 3.9453025388717653 and perplexity is 51.69197457833659
At time: 324.6416254043579 and batch: 200, loss is 3.9936295843124388 and perplexity is 54.25144262931497
At time: 326.22068452835083 and batch: 250, loss is 3.968007593154907 and perplexity is 52.87906919388709
At time: 327.7736248970032 and batch: 300, loss is 3.966073250770569 and perplexity is 52.77688183363475
At time: 329.326345205307 and batch: 350, loss is 3.907306833267212 and perplexity is 49.76474657582576
At time: 330.87851452827454 and batch: 400, loss is 3.9180330419540406 and perplexity is 50.30140665154016
At time: 332.43040442466736 and batch: 450, loss is 3.9535054302215578 and perplexity is 52.117742104439955
At time: 333.9826331138611 and batch: 500, loss is 3.9336823797225953 and perplexity is 51.094782062836906
At time: 335.53487753868103 and batch: 550, loss is 3.9310802698135374 and perplexity is 50.96200065496027
At time: 337.08624243736267 and batch: 600, loss is 3.964681239128113 and perplexity is 52.70346690873955
At time: 338.6385772228241 and batch: 650, loss is 3.932233119010925 and perplexity is 51.02078603534113
At time: 340.1915473937988 and batch: 700, loss is 3.9034423828125 and perplexity is 49.57280429288524
At time: 341.751473903656 and batch: 750, loss is 3.8889800834655763 and perplexity is 48.861026932485885
At time: 343.3042721748352 and batch: 800, loss is 3.8464697980880738 and perplexity is 46.827460707068894
At time: 344.8601746559143 and batch: 850, loss is 3.870413908958435 and perplexity is 47.9622339712333
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.374223073323567 and perplexity of 79.37814459107557
Finished 12 epochs...
Completing Train Step...
At time: 348.8980689048767 and batch: 50, loss is 3.9813820123672485 and perplexity is 53.591046561852686
At time: 350.4505798816681 and batch: 100, loss is 3.9399789333343507 and perplexity is 51.41751809345499
At time: 352.0038204193115 and batch: 150, loss is 3.9277816820144653 and perplexity is 50.79417496743759
At time: 353.5561714172363 and batch: 200, loss is 3.9768920087814332 and perplexity is 53.35096196431892
At time: 355.1226668357849 and batch: 250, loss is 3.9518158626556397 and perplexity is 52.029760004552344
At time: 356.6815068721771 and batch: 300, loss is 3.9507462644577025 and perplexity is 51.9741388184753
At time: 358.2497444152832 and batch: 350, loss is 3.8921967267990114 and perplexity is 49.018448477781156
At time: 359.8071949481964 and batch: 400, loss is 3.9040810680389404 and perplexity is 49.60447582361309
At time: 361.36392188072205 and batch: 450, loss is 3.9405794954299926 and perplexity is 51.44840678023273
At time: 362.92025446891785 and batch: 500, loss is 3.9213758897781372 and perplexity is 50.469837962588635
At time: 364.47357201576233 and batch: 550, loss is 3.9191288471221926 and perplexity is 50.35655740463567
At time: 366.0841190814972 and batch: 600, loss is 3.9536391162872313 and perplexity is 52.12470998607765
At time: 367.63715648651123 and batch: 650, loss is 3.9219309282302857 and perplexity is 50.4978584388331
At time: 369.1956641674042 and batch: 700, loss is 3.894152674674988 and perplexity is 49.114419834854225
At time: 370.7490584850311 and batch: 750, loss is 3.8810604095458983 and perplexity is 48.475591806635435
At time: 372.30315351486206 and batch: 800, loss is 3.8395474100112916 and perplexity is 46.5044222402046
At time: 373.85638451576233 and batch: 850, loss is 3.8641027927398683 and perplexity is 47.6604919047
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.374401728312175 and perplexity of 79.39232715944873
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 377.9680678844452 and batch: 50, loss is 3.9806669425964354 and perplexity is 53.55273892241984
At time: 379.56124925613403 and batch: 100, loss is 3.9514603662490844 and perplexity is 52.011266899147806
At time: 381.12293767929077 and batch: 150, loss is 3.938802752494812 and perplexity is 51.35707734544952
At time: 382.68714118003845 and batch: 200, loss is 3.9895971727371218 and perplexity is 54.03311896534848
At time: 384.24483919143677 and batch: 250, loss is 3.9656787061691285 and perplexity is 52.75606310705459
At time: 385.80227422714233 and batch: 300, loss is 3.9603158807754517 and perplexity is 52.47389882728698
At time: 387.3590638637543 and batch: 350, loss is 3.8979995679855346 and perplexity is 49.30372164645476
At time: 388.9156856536865 and batch: 400, loss is 3.8989395475387574 and perplexity is 49.35008792496314
At time: 390.47202706336975 and batch: 450, loss is 3.93819281578064 and perplexity is 51.32576232950373
At time: 392.028156042099 and batch: 500, loss is 3.9121471786499025 and perplexity is 50.006209046578505
At time: 393.5840873718262 and batch: 550, loss is 3.9034249544143678 and perplexity is 49.57194032584429
At time: 395.1403226852417 and batch: 600, loss is 3.929822521209717 and perplexity is 50.89794356209264
At time: 396.6971700191498 and batch: 650, loss is 3.8960995292663574 and perplexity is 49.21013160683262
At time: 398.2543239593506 and batch: 700, loss is 3.8532920503616332 and perplexity is 47.14802168795817
At time: 399.8110001087189 and batch: 750, loss is 3.8313522863388063 and perplexity is 46.12487011117344
At time: 401.36962699890137 and batch: 800, loss is 3.782722373008728 and perplexity is 43.9354878596252
At time: 402.9271733760834 and batch: 850, loss is 3.8074791288375853 and perplexity is 45.03676376734814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.357189814249675 and perplexity of 78.03752605374552
Finished 14 epochs...
Completing Train Step...
At time: 406.9516499042511 and batch: 50, loss is 3.9708808135986327 and perplexity is 53.03122089455591
At time: 408.5300028324127 and batch: 100, loss is 3.9375785255432127 and perplexity is 51.29424309674586
At time: 410.0804717540741 and batch: 150, loss is 3.9202525138854982 and perplexity is 50.413173197190886
At time: 411.6317594051361 and batch: 200, loss is 3.9718887424468994 and perplexity is 53.084699538760965
At time: 413.18212246894836 and batch: 250, loss is 3.9493918943405153 and perplexity is 51.90379424503997
At time: 414.7334280014038 and batch: 300, loss is 3.946521272659302 and perplexity is 51.755011739256155
At time: 416.28403663635254 and batch: 350, loss is 3.8854371309280396 and perplexity is 48.68822093561376
At time: 417.83407521247864 and batch: 400, loss is 3.887053356170654 and perplexity is 48.766975692877736
At time: 419.3860881328583 and batch: 450, loss is 3.9276531457901003 and perplexity is 50.78764649554909
At time: 420.93759322166443 and batch: 500, loss is 3.9021576595306398 and perplexity is 49.509157849850766
At time: 422.4890468120575 and batch: 550, loss is 3.895003943443298 and perplexity is 49.15624720717529
At time: 424.0405397415161 and batch: 600, loss is 3.923472261428833 and perplexity is 50.57575247941466
At time: 425.5958595275879 and batch: 650, loss is 3.8914335489273073 and perplexity is 48.98105295413078
At time: 427.1463587284088 and batch: 700, loss is 3.8509498262405395 and perplexity is 47.03771968073627
At time: 428.697265625 and batch: 750, loss is 3.831993808746338 and perplexity is 46.15446974229241
At time: 430.2484931945801 and batch: 800, loss is 3.786365008354187 and perplexity is 44.09582066030086
At time: 431.7993984222412 and batch: 850, loss is 3.813696084022522 and perplexity is 45.31762746324254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.355805397033691 and perplexity of 77.92956430847677
Finished 15 epochs...
Completing Train Step...
At time: 435.8247220516205 and batch: 50, loss is 3.9663829898834226 and perplexity is 52.793231430117125
At time: 437.4044768810272 and batch: 100, loss is 3.9319408082962037 and perplexity is 51.00587429244678
At time: 438.95686173439026 and batch: 150, loss is 3.9133524894714355 and perplexity is 50.06651840994902
At time: 440.50885581970215 and batch: 200, loss is 3.965436544418335 and perplexity is 52.74328915319154
At time: 442.0612564086914 and batch: 250, loss is 3.9432507276535036 and perplexity is 51.58602114040032
At time: 443.64166831970215 and batch: 300, loss is 3.9409140491485597 and perplexity is 51.46562191556861
At time: 445.1937003135681 and batch: 350, loss is 3.880132040977478 and perplexity is 48.43060947418911
At time: 446.7451128959656 and batch: 400, loss is 3.8816500186920164 and perplexity is 48.50418188658618
At time: 448.29703998565674 and batch: 450, loss is 3.922747573852539 and perplexity is 50.53911413721026
At time: 449.84906244277954 and batch: 500, loss is 3.897532916069031 and perplexity is 49.28071933771083
At time: 451.40067958831787 and batch: 550, loss is 3.8915679025650025 and perplexity is 48.98763417886915
At time: 452.9521713256836 and batch: 600, loss is 3.9209903860092163 and perplexity is 50.450385399596314
At time: 454.5037486553192 and batch: 650, loss is 3.8897985506057737 and perplexity is 48.901034447651305
At time: 456.0546281337738 and batch: 700, loss is 3.8503233194351196 and perplexity is 47.008259458723266
At time: 457.60719561576843 and batch: 750, loss is 3.8328119277954102 and perplexity is 46.19224504342815
At time: 459.15912675857544 and batch: 800, loss is 3.788442430496216 and perplexity is 44.1875215122223
At time: 460.71032643318176 and batch: 850, loss is 3.8166138887405396 and perplexity is 45.45004854594593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.355309804280599 and perplexity of 77.890952549798
Finished 16 epochs...
Completing Train Step...
At time: 464.74752831459045 and batch: 50, loss is 3.962270894050598 and perplexity is 52.576586341112744
At time: 466.3057780265808 and batch: 100, loss is 3.927380084991455 and perplexity is 50.7737802734827
At time: 467.8634238243103 and batch: 150, loss is 3.908334941864014 and perplexity is 49.815936449463614
At time: 469.4210088253021 and batch: 200, loss is 3.9607652044296264 and perplexity is 52.49748188907383
At time: 470.97770261764526 and batch: 250, loss is 3.938895182609558 and perplexity is 51.36182450538847
At time: 472.5344891548157 and batch: 300, loss is 3.9368717288970947 and perplexity is 51.25800130705637
At time: 474.0934040546417 and batch: 350, loss is 3.8764171266555785 and perplexity is 48.25102768144984
At time: 475.65032267570496 and batch: 400, loss is 3.87788001537323 and perplexity is 48.321665220292154
At time: 477.20768642425537 and batch: 450, loss is 3.91932692527771 and perplexity is 50.36653292657846
At time: 478.7650856971741 and batch: 500, loss is 3.894378590583801 and perplexity is 49.12551681709227
At time: 480.32230949401855 and batch: 550, loss is 3.8892355394363403 and perplexity is 48.87351036796962
At time: 481.8788650035858 and batch: 600, loss is 3.9192631816864014 and perplexity is 50.36332248521158
At time: 483.48286628723145 and batch: 650, loss is 3.888625798225403 and perplexity is 48.843719257931845
At time: 485.0397992134094 and batch: 700, loss is 3.8497753858566286 and perplexity is 46.98250911028395
At time: 486.5973289012909 and batch: 750, loss is 3.8332096767425536 and perplexity is 46.21062161464902
At time: 488.1544289588928 and batch: 800, loss is 3.789576153755188 and perplexity is 44.23764634159058
At time: 489.71175718307495 and batch: 850, loss is 3.818243622779846 and perplexity is 45.52418042836742
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3550364176432295 and perplexity of 77.86966111472846
Finished 17 epochs...
Completing Train Step...
At time: 493.75405287742615 and batch: 50, loss is 3.9585752868652344 and perplexity is 52.38264252166744
At time: 495.3077232837677 and batch: 100, loss is 3.9234730911254885 and perplexity is 50.57579444196474
At time: 496.8634788990021 and batch: 150, loss is 3.90414005279541 and perplexity is 49.60740181783304
At time: 498.41670846939087 and batch: 200, loss is 3.9569388008117676 and perplexity is 52.29698916211676
At time: 499.9708411693573 and batch: 250, loss is 3.935332374572754 and perplexity is 51.17915778071794
At time: 501.52459692955017 and batch: 300, loss is 3.93353120803833 and perplexity is 51.087058562377294
At time: 503.07874298095703 and batch: 350, loss is 3.8734188079833984 and perplexity is 48.10657239391368
At time: 504.6319456100464 and batch: 400, loss is 3.8748265981674193 and perplexity is 48.17434404725798
At time: 506.1854774951935 and batch: 450, loss is 3.9165619134902956 and perplexity is 50.22746122539543
At time: 507.7384088039398 and batch: 500, loss is 3.8918403959274293 and perplexity is 49.00098480291949
At time: 509.2927014827728 and batch: 550, loss is 3.8873116827011107 and perplexity is 48.77957512382291
At time: 510.846396446228 and batch: 600, loss is 3.917788381576538 and perplexity is 50.28910139576554
At time: 512.4001693725586 and batch: 650, loss is 3.887546625137329 and perplexity is 48.791036862411865
At time: 513.9544494152069 and batch: 700, loss is 3.849131898880005 and perplexity is 46.952286202609
At time: 515.5087077617645 and batch: 750, loss is 3.8332740831375123 and perplexity is 46.21359797004315
At time: 517.0628838539124 and batch: 800, loss is 3.7901540040969848 and perplexity is 44.263216467790926
At time: 518.6170825958252 and batch: 850, loss is 3.8191778326034544 and perplexity is 45.56672943669155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.354881604512532 and perplexity of 77.85760680181198
Finished 18 epochs...
Completing Train Step...
At time: 522.651570558548 and batch: 50, loss is 3.955190439224243 and perplexity is 52.205634998448545
At time: 524.2021713256836 and batch: 100, loss is 3.920000147819519 and perplexity is 50.40045222823555
At time: 525.752513885498 and batch: 150, loss is 3.9004700422286986 and perplexity is 49.42567580114715
At time: 527.3040814399719 and batch: 200, loss is 3.953614311218262 and perplexity is 52.12341704508724
At time: 528.8543055057526 and batch: 250, loss is 3.9322357892990114 and perplexity is 51.02092227572013
At time: 530.40544962883 and batch: 300, loss is 3.9306042623519897 and perplexity is 50.93774813504126
At time: 531.9555809497833 and batch: 350, loss is 3.8708264684677123 and perplexity is 47.98202532922008
At time: 533.5061209201813 and batch: 400, loss is 3.8721845626831053 and perplexity is 48.04723370980213
At time: 535.0575420856476 and batch: 450, loss is 3.9141682052612303 and perplexity is 50.107375121022756
At time: 536.6082744598389 and batch: 500, loss is 3.889632797241211 and perplexity is 48.892929608381756
At time: 538.1588890552521 and batch: 550, loss is 3.8855871963500976 and perplexity is 48.69552790228553
At time: 539.7099344730377 and batch: 600, loss is 3.916423397064209 and perplexity is 50.220504378804975
At time: 541.2614476680756 and batch: 650, loss is 3.886501293182373 and perplexity is 48.74006068062192
At time: 542.8122091293335 and batch: 700, loss is 3.8484123802185057 and perplexity is 46.918515307337806
At time: 544.3633661270142 and batch: 750, loss is 3.833112874031067 and perplexity is 46.20614851768469
At time: 545.9225103855133 and batch: 800, loss is 3.790376133918762 and perplexity is 44.273049740267304
At time: 547.4733502864838 and batch: 850, loss is 3.8196908903121947 and perplexity is 45.59011379674127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.354789098103841 and perplexity of 77.8504048073379
Finished 19 epochs...
Completing Train Step...
At time: 551.5057163238525 and batch: 50, loss is 3.952034578323364 and perplexity is 52.04114097280595
At time: 553.0820634365082 and batch: 100, loss is 3.916832513809204 and perplexity is 50.241054631528044
At time: 554.6329319477081 and batch: 150, loss is 3.897145137786865 and perplexity is 49.26161304976334
At time: 556.1996366977692 and batch: 200, loss is 3.9506481790542605 and perplexity is 51.96904116410757
At time: 557.7555689811707 and batch: 250, loss is 3.9294598722457885 and perplexity is 50.87948882209219
At time: 559.306031703949 and batch: 300, loss is 3.9279809331893922 and perplexity is 50.804296774836814
At time: 560.8574440479279 and batch: 350, loss is 3.868509683609009 and perplexity is 47.87098997163016
At time: 562.435173034668 and batch: 400, loss is 3.869819231033325 and perplexity is 47.93372036849701
At time: 563.986257314682 and batch: 450, loss is 3.9120223331451416 and perplexity is 49.99996638586108
At time: 565.5369982719421 and batch: 500, loss is 3.8876055097579956 and perplexity is 48.793909988700086
At time: 567.0891613960266 and batch: 550, loss is 3.8839587926864625 and perplexity is 48.61629645414517
At time: 568.6407539844513 and batch: 600, loss is 3.9151080226898194 and perplexity is 50.15448904123621
At time: 570.1966989040375 and batch: 650, loss is 3.885449934005737 and perplexity is 48.68884429867983
At time: 571.7474870681763 and batch: 700, loss is 3.847613821029663 and perplexity is 46.88106305172397
At time: 573.2983329296112 and batch: 750, loss is 3.8327853298187256 and perplexity is 46.19101643951073
At time: 574.8485450744629 and batch: 800, loss is 3.7903509378433227 and perplexity is 44.2719342472192
At time: 576.399083852768 and batch: 850, loss is 3.8199082708358763 and perplexity is 45.60002527679544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.354726155598958 and perplexity of 77.84550486206221
Finished 20 epochs...
Completing Train Step...
At time: 580.4036319255829 and batch: 50, loss is 3.949065327644348 and perplexity is 51.88684696179459
At time: 581.9896819591522 and batch: 100, loss is 3.913900065422058 and perplexity is 50.09394113868988
At time: 583.5452075004578 and batch: 150, loss is 3.8940819549560546 and perplexity is 49.11094659970249
At time: 585.1010656356812 and batch: 200, loss is 3.947914605140686 and perplexity is 51.82717393948112
At time: 586.6555054187775 and batch: 250, loss is 3.9269134759902955 and perplexity is 50.750094297058844
At time: 588.2102992534637 and batch: 300, loss is 3.9255523824691774 and perplexity is 50.681065660386786
At time: 589.7657413482666 and batch: 350, loss is 3.866372604370117 and perplexity is 47.76879511096869
At time: 591.3216013908386 and batch: 400, loss is 3.8676263427734376 and perplexity is 47.828722242503005
At time: 592.8766720294952 and batch: 450, loss is 3.910036745071411 and perplexity is 49.90078554764867
At time: 594.4331674575806 and batch: 500, loss is 3.885709886550903 and perplexity is 48.70150273290128
At time: 595.9895262718201 and batch: 550, loss is 3.882395510673523 and perplexity is 48.54035484690281
At time: 597.5531146526337 and batch: 600, loss is 3.9138243198394775 and perplexity is 50.09014688763527
At time: 599.1120834350586 and batch: 650, loss is 3.8843937587738036 and perplexity is 48.6374474940539
At time: 600.7189269065857 and batch: 700, loss is 3.846759042739868 and perplexity is 46.84100725867488
At time: 602.2803592681885 and batch: 750, loss is 3.8323317909240724 and perplexity is 46.1700717669427
At time: 603.8394491672516 and batch: 800, loss is 3.7901513385772705 and perplexity is 44.26309848347206
At time: 605.3941922187805 and batch: 850, loss is 3.8199225425720216 and perplexity is 45.6006760729684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.354688962300618 and perplexity of 77.84260958481826
Finished 21 epochs...
Completing Train Step...
At time: 609.4607937335968 and batch: 50, loss is 3.9462359380722045 and perplexity is 51.740246350989466
At time: 611.0556256771088 and batch: 100, loss is 3.9111417388916014 and perplexity is 49.95595608323825
At time: 612.6105306148529 and batch: 150, loss is 3.8912236166000365 and perplexity is 48.970771326952665
At time: 614.1651866436005 and batch: 200, loss is 3.945354061126709 and perplexity is 51.69463793403982
At time: 615.7181634902954 and batch: 250, loss is 3.924538688659668 and perplexity is 50.629716608372675
At time: 617.2695679664612 and batch: 300, loss is 3.9232773637771605 and perplexity is 50.565896344524816
At time: 618.8203320503235 and batch: 350, loss is 3.8643676805496217 and perplexity is 47.67311826022241
At time: 620.370911359787 and batch: 400, loss is 3.8655824184417726 and perplexity is 47.7310637906005
At time: 621.9222536087036 and batch: 450, loss is 3.9081740283966067 and perplexity is 49.80792103930841
At time: 623.4728903770447 and batch: 500, loss is 3.8839452505111693 and perplexity is 48.615638088194345
At time: 625.0245599746704 and batch: 550, loss is 3.8809053802490237 and perplexity is 48.46807725222517
At time: 626.575204372406 and batch: 600, loss is 3.912589569091797 and perplexity is 50.028336209557914
At time: 628.127254486084 and batch: 650, loss is 3.8833319902420045 and perplexity is 48.58583318890914
At time: 629.6788997650146 and batch: 700, loss is 3.8458892726898193 and perplexity is 46.80028406592112
At time: 631.2311491966248 and batch: 750, loss is 3.8317993974685667 and perplexity is 46.14549766501972
At time: 632.7825360298157 and batch: 800, loss is 3.789824752807617 and perplexity is 44.24864514564228
At time: 634.3343412876129 and batch: 850, loss is 3.819803476333618 and perplexity is 45.59524689522203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3546648025512695 and perplexity of 77.84072894959998
Finished 22 epochs...
Completing Train Step...
At time: 638.3707730770111 and batch: 50, loss is 3.9435321378707884 and perplexity is 51.60054001660254
At time: 639.9223725795746 and batch: 100, loss is 3.9085328435897826 and perplexity is 49.825796084845
At time: 641.5011761188507 and batch: 150, loss is 3.8885228538513186 and perplexity is 48.83869133062774
At time: 643.0537619590759 and batch: 200, loss is 3.942952036857605 and perplexity is 51.57061517161373
At time: 644.6069948673248 and batch: 250, loss is 3.9222931432724 and perplexity is 50.51615283580707
At time: 646.1604630947113 and batch: 300, loss is 3.921119112968445 and perplexity is 50.45688014231598
At time: 647.712239742279 and batch: 350, loss is 3.8624656915664675 and perplexity is 47.58253069008791
At time: 649.2640137672424 and batch: 400, loss is 3.86362699508667 and perplexity is 47.637820548418226
At time: 650.8156890869141 and batch: 450, loss is 3.9063702917098997 and perplexity is 49.71816164033742
At time: 652.367654800415 and batch: 500, loss is 3.8822341060638426 and perplexity is 48.53252084211425
At time: 653.9200758934021 and batch: 550, loss is 3.8794397354125976 and perplexity is 48.39709229714237
At time: 655.4721789360046 and batch: 600, loss is 3.911360569000244 and perplexity is 49.96688914673339
At time: 657.0240170955658 and batch: 650, loss is 3.882270264625549 and perplexity is 48.534275739990996
At time: 658.5764248371124 and batch: 700, loss is 3.84497998714447 and perplexity is 46.7577485854812
At time: 660.1278042793274 and batch: 750, loss is 3.8311971282958983 and perplexity is 46.117714021774376
At time: 661.6797871589661 and batch: 800, loss is 3.7893994092941283 and perplexity is 44.229828273549856
At time: 663.2323701381683 and batch: 850, loss is 3.819572696685791 and perplexity is 45.58472565429179
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.354653676350911 and perplexity of 77.83986288287166
Finished 23 epochs...
Completing Train Step...
At time: 667.2928557395935 and batch: 50, loss is 3.940930471420288 and perplexity is 51.46646710493632
At time: 668.8494913578033 and batch: 100, loss is 3.906044430732727 and perplexity is 49.70196307098622
At time: 670.4055423736572 and batch: 150, loss is 3.8859514951705934 and perplexity is 48.71327085733632
At time: 671.9620299339294 and batch: 200, loss is 3.940663847923279 and perplexity is 51.45274676466193
At time: 673.5192332267761 and batch: 250, loss is 3.920155100822449 and perplexity is 50.40826253475693
At time: 675.0779795646667 and batch: 300, loss is 3.9190571784973143 and perplexity is 50.35294854873529
At time: 676.6354231834412 and batch: 350, loss is 3.8606482315063477 and perplexity is 47.496129879793166
At time: 678.1912381649017 and batch: 400, loss is 3.8617558002471926 and perplexity is 47.54876425127082
At time: 679.7738215923309 and batch: 450, loss is 3.90463671207428 and perplexity is 49.63204591359933
At time: 681.3283040523529 and batch: 500, loss is 3.880578999519348 and perplexity is 48.45226078704074
At time: 682.883971452713 and batch: 550, loss is 3.87800057888031 and perplexity is 48.32749140092439
At time: 684.4404103755951 and batch: 600, loss is 3.9101476192474367 and perplexity is 49.90631856285788
At time: 685.9969656467438 and batch: 650, loss is 3.881200222969055 and perplexity is 48.48236981888298
At time: 687.554435968399 and batch: 700, loss is 3.844045763015747 and perplexity is 46.71408676668576
At time: 689.1123704910278 and batch: 750, loss is 3.830541338920593 and perplexity is 46.08748042942337
At time: 690.669326543808 and batch: 800, loss is 3.7888970041275023 and perplexity is 44.20761256041806
At time: 692.2262334823608 and batch: 850, loss is 3.819250726699829 and perplexity is 45.57005110332206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.354652722676595 and perplexity of 77.83978864902903
Finished 24 epochs...
Completing Train Step...
At time: 696.2504060268402 and batch: 50, loss is 3.9384169244766234 and perplexity is 51.337266168176754
At time: 697.8033649921417 and batch: 100, loss is 3.903655858039856 and perplexity is 49.58338798819053
At time: 699.3575475215912 and batch: 150, loss is 3.8834885501861574 and perplexity is 48.59344037971496
At time: 700.9108035564423 and batch: 200, loss is 3.9384732007980348 and perplexity is 51.340155321962726
At time: 702.465898513794 and batch: 250, loss is 3.918105673789978 and perplexity is 50.30506026773833
At time: 704.0203177928925 and batch: 300, loss is 3.9170751762390137 and perplexity is 50.25324772726442
At time: 705.5743672847748 and batch: 350, loss is 3.858896908760071 and perplexity is 47.41302162310673
At time: 707.1290822029114 and batch: 400, loss is 3.8599525785446165 and perplexity is 47.46310054618039
At time: 708.6849908828735 and batch: 450, loss is 3.9029589462280274 and perplexity is 49.5488447776149
At time: 710.2392947673798 and batch: 500, loss is 3.878967967033386 and perplexity is 48.374265464263146
At time: 711.7926833629608 and batch: 550, loss is 3.876584143638611 and perplexity is 48.2590870955322
At time: 713.3453042507172 and batch: 600, loss is 3.9089465856552126 and perplexity is 49.84641537786914
At time: 714.8984467983246 and batch: 650, loss is 3.880132961273193 and perplexity is 48.430654044692005
At time: 716.4512410163879 and batch: 700, loss is 3.843096022605896 and perplexity is 46.669741572319
At time: 718.0049858093262 and batch: 750, loss is 3.8298440170288086 and perplexity is 46.05535382297756
At time: 719.606714963913 and batch: 800, loss is 3.7883335065841677 and perplexity is 44.182708696635586
At time: 721.159640789032 and batch: 850, loss is 3.818859133720398 and perplexity is 45.55220968475281
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.354658126831055 and perplexity of 77.84020930840667
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 725.1711976528168 and batch: 50, loss is 3.940278172492981 and perplexity is 51.43290653060479
At time: 726.7500524520874 and batch: 100, loss is 3.9116231203079224 and perplexity is 49.98000974115873
At time: 728.3024778366089 and batch: 150, loss is 3.8930596017837527 and perplexity is 49.0607635244356
At time: 729.8544752597809 and batch: 200, loss is 3.9479858255386353 and perplexity is 51.83086522287946
At time: 731.4065074920654 and batch: 250, loss is 3.924565801620483 and perplexity is 50.6310893485046
At time: 732.9594759941101 and batch: 300, loss is 3.9227608394622804 and perplexity is 50.53978457382194
At time: 734.5118737220764 and batch: 350, loss is 3.8685480308532716 and perplexity is 47.87282572737356
At time: 736.0634922981262 and batch: 400, loss is 3.8716101932525633 and perplexity is 48.01964477141699
At time: 737.6139738559723 and batch: 450, loss is 3.9085601139068604 and perplexity is 49.82715486863004
At time: 739.1630661487579 and batch: 500, loss is 3.878546690940857 and perplexity is 48.35389083470289
At time: 740.7127366065979 and batch: 550, loss is 3.8750106048583985 and perplexity is 48.183209264500846
At time: 742.2642047405243 and batch: 600, loss is 3.90544029712677 and perplexity is 49.67194551303374
At time: 743.8150715827942 and batch: 650, loss is 3.8764883375167845 and perplexity is 48.25446380102818
At time: 745.3650059700012 and batch: 700, loss is 3.838855686187744 and perplexity is 46.47226514663681
At time: 746.9151709079742 and batch: 750, loss is 3.820593862533569 and perplexity is 45.63129899481553
At time: 748.4655334949493 and batch: 800, loss is 3.776008973121643 and perplexity is 43.64151922896523
At time: 750.016853094101 and batch: 850, loss is 3.8055887842178344 and perplexity is 44.951709179838346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.35252030690511 and perplexity of 77.67397870676368
Finished 26 epochs...
Completing Train Step...
At time: 754.0194792747498 and batch: 50, loss is 3.938341178894043 and perplexity is 51.333377744310084
At time: 755.5962288379669 and batch: 100, loss is 3.908641381263733 and perplexity is 49.83120435434996
At time: 757.165581703186 and batch: 150, loss is 3.8898493337631224 and perplexity is 48.903517859635386
At time: 758.7504942417145 and batch: 200, loss is 3.9448187923431397 and perplexity is 51.66697481233905
At time: 760.3036913871765 and batch: 250, loss is 3.9213879346847533 and perplexity is 50.47044587073491
At time: 761.8678052425385 and batch: 300, loss is 3.9195236778259277 and perplexity is 50.37644364522426
At time: 763.4272074699402 and batch: 350, loss is 3.8649109840393066 and perplexity is 47.699026269055864
At time: 764.9809188842773 and batch: 400, loss is 3.8674311733245847 and perplexity is 47.819388448008986
At time: 766.532886505127 and batch: 450, loss is 3.9053482484817503 and perplexity is 49.66737348818136
At time: 768.084630727768 and batch: 500, loss is 3.8758863162994386 and perplexity is 48.225422332655505
At time: 769.636750459671 and batch: 550, loss is 3.873038034439087 and perplexity is 48.08825817084566
At time: 771.1894159317017 and batch: 600, loss is 3.904113006591797 and perplexity is 49.60606014408643
At time: 772.7423233985901 and batch: 650, loss is 3.8759496879577635 and perplexity is 48.22847855448005
At time: 774.2942357063293 and batch: 700, loss is 3.839120011329651 and perplexity is 46.484550558316485
At time: 775.8450934886932 and batch: 750, loss is 3.8216107273101807 and perplexity is 45.67772345517335
At time: 777.3979773521423 and batch: 800, loss is 3.7778094005584717 and perplexity is 43.720163392881176
At time: 778.9503374099731 and batch: 850, loss is 3.8077179384231568 and perplexity is 45.047520262564866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351907730102539 and perplexity of 77.62641199986257
Finished 27 epochs...
Completing Train Step...
At time: 782.9972949028015 and batch: 50, loss is 3.937558174133301 and perplexity is 51.29319919720092
At time: 784.5447652339935 and batch: 100, loss is 3.9075268411636355 and perplexity is 49.775696417517615
At time: 786.0905578136444 and batch: 150, loss is 3.88850022315979 and perplexity is 48.8375860897758
At time: 787.6398591995239 and batch: 200, loss is 3.9434122943878176 and perplexity is 51.59435639870433
At time: 789.1917700767517 and batch: 250, loss is 3.9198793458938597 and perplexity is 50.394364124287016
At time: 790.7398974895477 and batch: 300, loss is 3.9179342222213744 and perplexity is 50.2964361255792
At time: 792.291383266449 and batch: 350, loss is 3.863131561279297 and perplexity is 47.614225007106036
At time: 793.8413860797882 and batch: 400, loss is 3.8655675888061523 and perplexity is 47.730355961565145
At time: 795.3900637626648 and batch: 450, loss is 3.903941259384155 and perplexity is 49.59754117335022
At time: 796.9410581588745 and batch: 500, loss is 3.8747723579406737 and perplexity is 48.17173113077677
At time: 798.5137958526611 and batch: 550, loss is 3.872221007347107 and perplexity is 48.048984806999776
At time: 800.0628237724304 and batch: 600, loss is 3.903687763214111 and perplexity is 49.584969980061175
At time: 801.6134259700775 and batch: 650, loss is 3.875953531265259 and perplexity is 48.22866391170936
At time: 803.1615426540375 and batch: 700, loss is 3.8395417881011964 and perplexity is 46.504160797258635
At time: 804.7116174697876 and batch: 750, loss is 3.8223514938354493 and perplexity is 45.71157251923835
At time: 806.2631967067719 and batch: 800, loss is 3.7788830280303953 and perplexity is 43.76712776798719
At time: 807.8130834102631 and batch: 850, loss is 3.8088791751861573 and perplexity is 45.09986148356907
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351663907368978 and perplexity of 77.60748722313134
Finished 28 epochs...
Completing Train Step...
At time: 811.8276855945587 and batch: 50, loss is 3.9369641065597536 and perplexity is 51.26273662012489
At time: 813.3725442886353 and batch: 100, loss is 3.906764941215515 and perplexity is 49.737786760516165
At time: 814.9172501564026 and batch: 150, loss is 3.8876016426086424 and perplexity is 48.79372129572749
At time: 816.4618942737579 and batch: 200, loss is 3.9424890279769897 and perplexity is 51.54674304574009
At time: 818.0089361667633 and batch: 250, loss is 3.9189066886901855 and perplexity is 50.345371513367425
At time: 819.5529689788818 and batch: 300, loss is 3.916925449371338 and perplexity is 50.24572402915562
At time: 821.0975818634033 and batch: 350, loss is 3.8620602893829346 and perplexity is 47.56324453783638
At time: 822.6478726863861 and batch: 400, loss is 3.8645417404174807 and perplexity is 47.68141695910147
At time: 824.1940472126007 and batch: 450, loss is 3.903167824745178 and perplexity is 49.55919554782772
At time: 825.7407238483429 and batch: 500, loss is 3.8741588687896726 and perplexity is 48.14218735966124
At time: 827.2880175113678 and batch: 550, loss is 3.871770977973938 and perplexity is 48.02736621735071
At time: 828.8338825702667 and batch: 600, loss is 3.9034863567352294 and perplexity is 49.574984251481006
At time: 830.3796088695526 and batch: 650, loss is 3.8760207414627077 and perplexity is 48.23190547866552
At time: 831.9260830879211 and batch: 700, loss is 3.83986177444458 and perplexity is 46.519043874688045
At time: 833.472095489502 and batch: 750, loss is 3.8228410387039187 and perplexity is 45.733955863373424
At time: 835.0163695812225 and batch: 800, loss is 3.7795235204696653 and perplexity is 43.79516926163486
At time: 836.5610091686249 and batch: 850, loss is 3.8095385360717775 and perplexity is 45.12960837406854
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3515472412109375 and perplexity of 77.5984335838988
Finished 29 epochs...
Completing Train Step...
At time: 840.6231157779694 and batch: 50, loss is 3.9364329099655153 and perplexity is 51.23551326013913
At time: 842.1682267189026 and batch: 100, loss is 3.90613422870636 and perplexity is 49.706426406951834
At time: 843.7133057117462 and batch: 150, loss is 3.8868927574157714 and perplexity is 48.75914440616634
At time: 845.2590630054474 and batch: 200, loss is 3.941779899597168 and perplexity is 51.51020274477241
At time: 846.8068392276764 and batch: 250, loss is 3.918177857398987 and perplexity is 50.3086915995996
At time: 848.3523530960083 and batch: 300, loss is 3.916189923286438 and perplexity is 50.20878057657954
At time: 849.9006252288818 and batch: 350, loss is 3.861324305534363 and perplexity is 47.52825163675543
At time: 851.4466068744659 and batch: 400, loss is 3.863882484436035 and perplexity is 47.64999305910252
At time: 852.9946625232697 and batch: 450, loss is 3.90266592502594 and perplexity is 49.53432804251558
At time: 854.5395681858063 and batch: 500, loss is 3.873743133544922 and perplexity is 48.12217711538655
At time: 856.0884099006653 and batch: 550, loss is 3.871463460922241 and perplexity is 48.01259925395368
At time: 857.6333661079407 and batch: 600, loss is 3.9033455801010133 and perplexity is 49.56800574327374
At time: 859.1808471679688 and batch: 650, loss is 3.8760636758804323 and perplexity is 48.23397633189811
At time: 860.7254922389984 and batch: 700, loss is 3.8400748586654663 and perplexity is 46.528957405079396
At time: 862.2729196548462 and batch: 750, loss is 3.8231608200073244 and perplexity is 45.74858306601763
At time: 863.8195722103119 and batch: 800, loss is 3.7799209117889405 and perplexity is 43.81257654024725
At time: 865.364137172699 and batch: 850, loss is 3.8099329710006713 and perplexity is 45.1474125790079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351487159729004 and perplexity of 77.59377149506736
Finished 30 epochs...
Completing Train Step...
At time: 869.3581585884094 and batch: 50, loss is 3.9359388160705566 and perplexity is 51.21020435883392
At time: 870.9318015575409 and batch: 100, loss is 3.905577926635742 and perplexity is 49.678782308966035
At time: 872.4814813137054 and batch: 150, loss is 3.8862905645370485 and perplexity is 48.72979083577491
At time: 874.0316107273102 and batch: 200, loss is 3.9411920881271363 and perplexity is 51.47993335399449
At time: 875.5810134410858 and batch: 250, loss is 3.917584571838379 and perplexity is 50.278853031571494
At time: 877.154867887497 and batch: 300, loss is 3.9156029844284057 and perplexity is 50.17931973894569
At time: 878.7070305347443 and batch: 350, loss is 3.8607664585113524 and perplexity is 47.50174553693279
At time: 880.2566878795624 and batch: 400, loss is 3.863403196334839 and perplexity is 47.62716045654166
At time: 881.8078970909119 and batch: 450, loss is 3.902293338775635 and perplexity is 49.515875670732306
At time: 883.3576240539551 and batch: 500, loss is 3.8734162139892576 and perplexity is 48.106447605908606
At time: 884.9068202972412 and batch: 550, loss is 3.871217451095581 and perplexity is 48.00078913549587
At time: 886.4558198451996 and batch: 600, loss is 3.9032181024551393 and perplexity is 49.56168733332749
At time: 888.0064616203308 and batch: 650, loss is 3.8760722160339354 and perplexity is 48.234388259219
At time: 889.5565211772919 and batch: 700, loss is 3.840206570625305 and perplexity is 46.535086228859306
At time: 891.1072709560394 and batch: 750, loss is 3.823369650840759 and perplexity is 45.75813777837231
At time: 892.6575238704681 and batch: 800, loss is 3.7801763916015627 and perplexity is 43.823771199036024
At time: 894.2099466323853 and batch: 850, loss is 3.810179214477539 and perplexity is 45.158531203741155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351450284322103 and perplexity of 77.59091024592566
Finished 31 epochs...
Completing Train Step...
At time: 898.2118372917175 and batch: 50, loss is 3.9354724216461183 and perplexity is 51.1863257738989
At time: 899.79998087883 and batch: 100, loss is 3.905072793960571 and perplexity is 49.65369426968673
At time: 901.3477482795715 and batch: 150, loss is 3.8857563591003417 and perplexity is 48.70376606848584
At time: 902.8943617343903 and batch: 200, loss is 3.940681233406067 and perplexity is 51.45364130328118
At time: 904.4413461685181 and batch: 250, loss is 3.9170748090744016 and perplexity is 50.2532292760536
At time: 905.9882817268372 and batch: 300, loss is 3.9151045989990236 and perplexity is 50.15431732806766
At time: 907.5357677936554 and batch: 350, loss is 3.8603104019165038 and perplexity is 47.480086991750426
At time: 909.0849664211273 and batch: 400, loss is 3.8630198431015015 and perplexity is 47.60890592977545
At time: 910.6331079006195 and batch: 450, loss is 3.9019871854782107 and perplexity is 49.50071854244165
At time: 912.1816182136536 and batch: 500, loss is 3.8731322574615477 and perplexity is 48.09278940534534
At time: 913.7292001247406 and batch: 550, loss is 3.8709997224807737 and perplexity is 47.99033912784187
At time: 915.3025057315826 and batch: 600, loss is 3.9030897092819212 and perplexity is 49.555324359510664
At time: 916.8484303951263 and batch: 650, loss is 3.876050896644592 and perplexity is 48.233359942477556
At time: 918.3988332748413 and batch: 700, loss is 3.840280547142029 and perplexity is 46.53852885977932
At time: 919.9498693943024 and batch: 750, loss is 3.8235046815872193 and perplexity is 45.76431695105292
At time: 921.4979755878448 and batch: 800, loss is 3.780345091819763 and perplexity is 43.83116490244181
At time: 923.0457873344421 and batch: 850, loss is 3.8103372049331665 and perplexity is 45.165666384291875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351426124572754 and perplexity of 77.58903569162685
Finished 32 epochs...
Completing Train Step...
At time: 927.0472507476807 and batch: 50, loss is 3.9350280570983887 and perplexity is 51.163585438270104
At time: 928.6177308559418 and batch: 100, loss is 3.904605197906494 and perplexity is 49.63048182562243
At time: 930.1651480197906 and batch: 150, loss is 3.8852691078186035 and perplexity is 48.68004087657797
At time: 931.7119791507721 and batch: 200, loss is 3.9402231550216675 and perplexity is 51.43007689998545
At time: 933.2590363025665 and batch: 250, loss is 3.916620383262634 and perplexity is 50.23039809947676
At time: 934.806393623352 and batch: 300, loss is 3.9146623945236207 and perplexity is 50.13214376746961
At time: 936.3511683940887 and batch: 350, loss is 3.8599161291122437 and perplexity is 47.46137057463526
At time: 937.9010882377625 and batch: 400, loss is 3.862690863609314 and perplexity is 47.59324615209316
At time: 939.4486362934113 and batch: 450, loss is 3.901717710494995 and perplexity is 49.487381134272894
At time: 940.997837305069 and batch: 500, loss is 3.8728713846206664 and perplexity is 48.08024493907369
At time: 942.5465543270111 and batch: 550, loss is 3.870795955657959 and perplexity is 47.980561285145804
At time: 944.0912063121796 and batch: 600, loss is 3.902956748008728 and perplexity is 49.54873585850776
At time: 945.6388704776764 and batch: 650, loss is 3.876005783081055 and perplexity is 48.23118401281151
At time: 947.184376001358 and batch: 700, loss is 3.840313363075256 and perplexity is 46.54005609009348
At time: 948.7307376861572 and batch: 750, loss is 3.82358925819397 and perplexity is 45.76818770537622
At time: 950.2774956226349 and batch: 800, loss is 3.7804578256607058 and perplexity is 43.83610643654809
At time: 951.8221533298492 and batch: 850, loss is 3.8104402112960813 and perplexity is 45.17031897493385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351411819458008 and perplexity of 77.58792577950697
Finished 33 epochs...
Completing Train Step...
At time: 955.8498394489288 and batch: 50, loss is 3.934601731300354 and perplexity is 51.14177773080226
At time: 957.3984849452972 and batch: 100, loss is 3.9041665410995483 and perplexity is 49.60871585118307
At time: 958.946414232254 and batch: 150, loss is 3.884816346168518 and perplexity is 48.65800540972806
At time: 960.4942533969879 and batch: 200, loss is 3.939802966117859 and perplexity is 51.40847109192834
At time: 962.0415768623352 and batch: 250, loss is 3.9162043380737304 and perplexity is 50.20950433068813
At time: 963.587141752243 and batch: 300, loss is 3.914258074760437 and perplexity is 50.1118784480843
At time: 965.132030248642 and batch: 350, loss is 3.8595613861083984 and perplexity is 47.44453697144901
At time: 966.6812295913696 and batch: 400, loss is 3.8623951196670534 and perplexity is 47.579172819005244
At time: 968.2291676998138 and batch: 450, loss is 3.9014701652526855 and perplexity is 49.475132284653526
At time: 969.7760589122772 and batch: 500, loss is 3.8726237344741823 and perplexity is 48.06833933364485
At time: 971.3236918449402 and batch: 550, loss is 3.870599179267883 and perplexity is 47.97112077236776
At time: 972.8692922592163 and batch: 600, loss is 3.902818913459778 and perplexity is 49.54190680150047
At time: 974.4160604476929 and batch: 650, loss is 3.8759431266784667 and perplexity is 48.228162115000316
At time: 975.9632296562195 and batch: 700, loss is 3.84031672000885 and perplexity is 46.54021232223346
At time: 977.5113053321838 and batch: 750, loss is 3.8236384439468383 and perplexity is 45.77043890350891
At time: 979.0596849918365 and batch: 800, loss is 3.7805324506759646 and perplexity is 43.83937782872213
At time: 980.6070485115051 and batch: 850, loss is 3.8105065298080443 and perplexity is 45.17331470260816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351400057474772 and perplexity of 77.58701319699158
Finished 34 epochs...
Completing Train Step...
At time: 984.6474096775055 and batch: 50, loss is 3.9341904878616334 and perplexity is 51.12075033425175
At time: 986.19624376297 and batch: 100, loss is 3.903750443458557 and perplexity is 49.58807807550744
At time: 987.7459168434143 and batch: 150, loss is 3.8843894147872926 and perplexity is 48.63723621409696
At time: 989.2966139316559 and batch: 200, loss is 3.9394107103347777 and perplexity is 51.38830977629801
At time: 990.8476126194 and batch: 250, loss is 3.9158159112930297 and perplexity is 50.19000540175864
At time: 992.3969161510468 and batch: 300, loss is 3.9138805437088013 and perplexity is 50.09296322868035
At time: 993.971738576889 and batch: 350, loss is 3.859233536720276 and perplexity is 47.42898485854782
At time: 995.5214395523071 and batch: 400, loss is 3.862120714187622 and perplexity is 47.56611862443006
At time: 997.0723218917847 and batch: 450, loss is 3.9012366199493407 and perplexity is 49.46357894904223
At time: 998.6228702068329 and batch: 500, loss is 3.872384777069092 and perplexity is 48.05685442026788
At time: 1000.1752035617828 and batch: 550, loss is 3.870406122207642 and perplexity is 47.96186050272395
At time: 1001.7273066043854 and batch: 600, loss is 3.9026767587661744 and perplexity is 49.5348646874652
At time: 1003.2760400772095 and batch: 650, loss is 3.8758669900894165 and perplexity is 48.2244903270212
At time: 1004.8270883560181 and batch: 700, loss is 3.840298743247986 and perplexity is 46.539375687486
At time: 1006.3780291080475 and batch: 750, loss is 3.8236618852615356 and perplexity is 45.7715118353465
At time: 1007.9271903038025 and batch: 800, loss is 3.78058012008667 and perplexity is 43.84146767583941
At time: 1009.4767200946808 and batch: 850, loss is 3.8105475330352783 and perplexity is 45.17516699227049
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351392110188802 and perplexity of 77.58639659326026
Finished 35 epochs...
Completing Train Step...
At time: 1013.4997475147247 and batch: 50, loss is 3.9337912082672117 and perplexity is 51.10034293619169
At time: 1015.0429427623749 and batch: 100, loss is 3.903352289199829 and perplexity is 49.56833830103795
At time: 1016.5882503986359 and batch: 150, loss is 3.8839826726913453 and perplexity is 48.61745742540382
At time: 1018.1379971504211 and batch: 200, loss is 3.939039831161499 and perplexity is 51.36925445628107
At time: 1019.6860282421112 and batch: 250, loss is 3.9154480648040773 and perplexity is 50.17154657970568
At time: 1021.2325406074524 and batch: 300, loss is 3.913522777557373 and perplexity is 50.07504486749507
At time: 1022.7793202400208 and batch: 350, loss is 3.858924708366394 and perplexity is 47.41433970476343
At time: 1024.32679438591 and batch: 400, loss is 3.86186092376709 and perplexity is 47.55376300747451
At time: 1025.8725109100342 and batch: 450, loss is 3.9010126113891603 and perplexity is 49.45249992488494
At time: 1027.4203062057495 and batch: 500, loss is 3.872151746749878 and perplexity is 48.04565702085936
At time: 1028.9682836532593 and batch: 550, loss is 3.8702153730392457 and perplexity is 47.952712690214874
At time: 1030.5138359069824 and batch: 600, loss is 3.902531051635742 and perplexity is 49.527647630276356
At time: 1032.0604810714722 and batch: 650, loss is 3.875780668258667 and perplexity is 48.22032768039543
At time: 1033.6472103595734 and batch: 700, loss is 3.840264835357666 and perplexity is 46.53779766219353
At time: 1035.1951885223389 and batch: 750, loss is 3.8236663913726807 and perplexity is 45.77171808733081
At time: 1036.7410008907318 and batch: 800, loss is 3.7806076574325562 and perplexity is 43.84267497012173
At time: 1038.2860324382782 and batch: 850, loss is 3.8105699396133423 and perplexity is 45.17617922451655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.35138479868571 and perplexity of 77.58582932215549
Finished 36 epochs...
Completing Train Step...
At time: 1042.281536579132 and batch: 50, loss is 3.933402671813965 and perplexity is 51.080492446756665
At time: 1043.8559215068817 and batch: 100, loss is 3.9029687881469726 and perplexity is 49.54933243572877
At time: 1045.4035766124725 and batch: 150, loss is 3.8835919761657713 and perplexity is 48.59846646379841
At time: 1046.9513833522797 and batch: 200, loss is 3.938685903549194 and perplexity is 51.351076675704476
At time: 1048.4973154067993 and batch: 250, loss is 3.9150960779190065 and perplexity is 50.15388996093732
At time: 1050.04296541214 and batch: 300, loss is 3.9131797695159913 and perplexity is 50.057871669873784
At time: 1051.5889177322388 and batch: 350, loss is 3.858629961013794 and perplexity is 47.400366513041966
At time: 1053.1374411582947 and batch: 400, loss is 3.8616119527816775 and perplexity is 47.54192497396304
At time: 1054.6832344532013 and batch: 450, loss is 3.9007954692840574 and perplexity is 49.441762870724105
At time: 1056.2301614284515 and batch: 500, loss is 3.8719230318069457 and perplexity is 48.03466951770666
At time: 1057.7794225215912 and batch: 550, loss is 3.8700258350372314 and perplexity is 47.9436246901483
At time: 1059.32724070549 and batch: 600, loss is 3.9023826789855955 and perplexity is 49.52029962707677
At time: 1060.872389793396 and batch: 650, loss is 3.87568660736084 and perplexity is 48.21579224638714
At time: 1062.42027592659 and batch: 700, loss is 3.8402187824249268 and perplexity is 46.535654509477574
At time: 1063.9671952724457 and batch: 750, loss is 3.823656129837036 and perplexity is 45.771248401623986
At time: 1065.5140931606293 and batch: 800, loss is 3.780620050430298 and perplexity is 43.84321831566046
At time: 1067.0619533061981 and batch: 850, loss is 3.810578536987305 and perplexity is 45.17656762269314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351377169291179 and perplexity of 77.5852373915116
Finished 37 epochs...
Completing Train Step...
At time: 1071.065835237503 and batch: 50, loss is 3.9330229425430296 and perplexity is 51.06109937089315
At time: 1072.6433486938477 and batch: 100, loss is 3.902597198486328 and perplexity is 49.53092383653819
At time: 1074.1929066181183 and batch: 150, loss is 3.883214373588562 and perplexity is 48.58011902185194
At time: 1075.7436163425446 and batch: 200, loss is 3.9383453512191773 and perplexity is 51.33359192429909
At time: 1077.2936770915985 and batch: 250, loss is 3.9147564935684205 and perplexity is 50.13686137626965
At time: 1078.8467648029327 and batch: 300, loss is 3.912848334312439 and perplexity is 50.04128347809469
At time: 1080.3989629745483 and batch: 350, loss is 3.8583456754684446 and perplexity is 47.386893189224395
At time: 1081.9493403434753 and batch: 400, loss is 3.861370849609375 and perplexity is 47.53046384674703
At time: 1083.4982471466064 and batch: 450, loss is 3.900583472251892 and perplexity is 49.431282474676166
At time: 1085.0489358901978 and batch: 500, loss is 3.8716978549957277 and perplexity is 48.02385444169462
At time: 1086.5989072322845 and batch: 550, loss is 3.8698372268676757 and perplexity is 47.9345829835503
At time: 1088.1489703655243 and batch: 600, loss is 3.90223210811615 and perplexity is 49.51284387383042
At time: 1089.6982238292694 and batch: 650, loss is 3.875586471557617 and perplexity is 48.21096436102865
At time: 1091.2462570667267 and batch: 700, loss is 3.84016348361969 and perplexity is 46.53308121453301
At time: 1092.7984533309937 and batch: 750, loss is 3.823634614944458 and perplexity is 45.77026364872494
At time: 1094.3490235805511 and batch: 800, loss is 3.780620450973511 and perplexity is 43.8432358767675
At time: 1095.8984320163727 and batch: 850, loss is 3.8105762004852295 and perplexity is 45.176462067672446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351372718811035 and perplexity of 77.5848921007215
Finished 38 epochs...
Completing Train Step...
At time: 1099.9447996616364 and batch: 50, loss is 3.932650775909424 and perplexity is 51.0420996691789
At time: 1101.4905729293823 and batch: 100, loss is 3.902235760688782 and perplexity is 49.513024723419164
At time: 1103.0372874736786 and batch: 150, loss is 3.8828475761413572 and perplexity is 48.562303225803824
At time: 1104.5826165676117 and batch: 200, loss is 3.9380159854888914 and perplexity is 51.3166871823809
At time: 1106.1269204616547 and batch: 250, loss is 3.914427285194397 and perplexity is 50.12035861822943
At time: 1107.6750135421753 and batch: 300, loss is 3.912526216506958 and perplexity is 50.02516688553734
At time: 1109.222294330597 and batch: 350, loss is 3.858069829940796 and perplexity is 47.37382352935547
At time: 1110.7714006900787 and batch: 400, loss is 3.8611358118057253 and perplexity is 47.51929370367242
At time: 1112.3450751304626 and batch: 450, loss is 3.9003755760192873 and perplexity is 49.421006965433705
At time: 1113.8918752670288 and batch: 500, loss is 3.8714754581451416 and perplexity is 48.01317527526428
At time: 1115.4391195774078 and batch: 550, loss is 3.8696494674682618 and perplexity is 47.92558365991839
At time: 1116.9881646633148 and batch: 600, loss is 3.9020798206329346 and perplexity is 49.5053042615589
At time: 1118.5366897583008 and batch: 650, loss is 3.875481758117676 and perplexity is 48.205916289412606
At time: 1120.0842862129211 and batch: 700, loss is 3.8401005697250366 and perplexity is 46.530153729254295
At time: 1121.6340503692627 and batch: 750, loss is 3.8236040592193605 and perplexity is 45.76886512649778
At time: 1123.1798522472382 and batch: 800, loss is 3.7806110906600954 and perplexity is 43.84282549225921
At time: 1124.728705406189 and batch: 850, loss is 3.8105654764175414 and perplexity is 45.17597759483309
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351366996765137 and perplexity of 77.58444815767801
Finished 39 epochs...
Completing Train Step...
At time: 1128.7452187538147 and batch: 50, loss is 3.9322855043411256 and perplexity is 51.023458846072096
At time: 1130.293140888214 and batch: 100, loss is 3.90188316822052 and perplexity is 49.495569881224604
At time: 1131.8406586647034 and batch: 150, loss is 3.8824899435043334 and perplexity is 48.544938866457116
At time: 1133.3877201080322 and batch: 200, loss is 3.937695803642273 and perplexity is 51.300259140837184
At time: 1134.9347448349 and batch: 250, loss is 3.914106388092041 and perplexity is 50.10427772067452
At time: 1136.481716632843 and batch: 300, loss is 3.912211847305298 and perplexity is 50.009442985445084
At time: 1138.0271353721619 and batch: 350, loss is 3.8578006410598755 and perplexity is 47.36107273907716
At time: 1139.5716948509216 and batch: 400, loss is 3.860906023979187 and perplexity is 47.50837560292498
At time: 1141.1197383403778 and batch: 450, loss is 3.9001708936691286 and perplexity is 49.410892392753375
At time: 1142.6687533855438 and batch: 500, loss is 3.871255717277527 and perplexity is 48.002625977570716
At time: 1144.216527223587 and batch: 550, loss is 3.8694620370864867 and perplexity is 47.91660179124005
At time: 1145.7667577266693 and batch: 600, loss is 3.9019263696670534 and perplexity is 49.497708207629536
At time: 1147.316338300705 and batch: 650, loss is 3.875373067855835 and perplexity is 48.200677060480565
At time: 1148.8632807731628 and batch: 700, loss is 3.840031929016113 and perplexity is 46.526959976128
At time: 1150.4098813533783 and batch: 750, loss is 3.8235659265518187 and perplexity is 45.767119870855986
At time: 1151.999371767044 and batch: 800, loss is 3.780593767166138 and perplexity is 43.84206598791537
At time: 1153.5444250106812 and batch: 850, loss is 3.8105474853515626 and perplexity is 45.17516483815073
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351362228393555 and perplexity of 77.58407820708224
Finished 40 epochs...
Completing Train Step...
At time: 1157.5672752857208 and batch: 50, loss is 3.931925902366638 and perplexity is 51.00511400814352
At time: 1159.1121141910553 and batch: 100, loss is 3.9015380048751833 and perplexity is 49.4784887728009
At time: 1160.6594941616058 and batch: 150, loss is 3.882140154838562 and perplexity is 48.5279613665027
At time: 1162.2048969268799 and batch: 200, loss is 3.9373834848403932 and perplexity is 51.28423960709721
At time: 1163.7515330314636 and batch: 250, loss is 3.9137926483154297 and perplexity is 50.08856048146565
At time: 1165.2983055114746 and batch: 300, loss is 3.9119040393829345 and perplexity is 49.99405205154839
At time: 1166.845965385437 and batch: 350, loss is 3.8575368881225587 and perplexity is 47.34858276423393
At time: 1168.3923723697662 and batch: 400, loss is 3.860680136680603 and perplexity is 47.497645276267804
At time: 1169.941475391388 and batch: 450, loss is 3.8999688959121706 and perplexity is 49.40091251131151
At time: 1171.4863104820251 and batch: 500, loss is 3.8710380601882934 and perplexity is 47.992179002695195
At time: 1173.033414363861 and batch: 550, loss is 3.8692752599716185 and perplexity is 47.90765290235303
At time: 1174.5795547962189 and batch: 600, loss is 3.9017717838287354 and perplexity is 49.49005715429892
At time: 1176.129280090332 and batch: 650, loss is 3.8752613353729246 and perplexity is 48.19529178001562
At time: 1177.6767144203186 and batch: 700, loss is 3.8399581956863402 and perplexity is 46.523529514915964
At time: 1179.2238700389862 and batch: 750, loss is 3.8235217046737673 and perplexity is 45.76509600761214
At time: 1180.7731869220734 and batch: 800, loss is 3.7805698776245116 and perplexity is 43.841018633565426
At time: 1182.3249862194061 and batch: 850, loss is 3.81052396774292 and perplexity is 45.17410243879629
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351357777913411 and perplexity of 77.58373292145107
Finished 41 epochs...
Completing Train Step...
At time: 1186.3183770179749 and batch: 50, loss is 3.931571750640869 and perplexity is 50.98705365723548
At time: 1187.8941588401794 and batch: 100, loss is 3.9011994314193728 and perplexity is 49.46173950545747
At time: 1189.4463562965393 and batch: 150, loss is 3.8817973041534426 and perplexity is 48.51132637352306
At time: 1191.0227653980255 and batch: 200, loss is 3.937078080177307 and perplexity is 51.268579552626925
At time: 1192.5738716125488 and batch: 250, loss is 3.9134848928451538 and perplexity is 50.073147824765584
At time: 1194.1272213459015 and batch: 300, loss is 3.9116014766693117 and perplexity is 49.97892800359654
At time: 1195.6789937019348 and batch: 350, loss is 3.8572778701782227 and perplexity is 47.33632021983719
At time: 1197.2314186096191 and batch: 400, loss is 3.8604577207565307 and perplexity is 47.48708221834211
At time: 1198.7815062999725 and batch: 450, loss is 3.8997693634033204 and perplexity is 49.391056406637986
At time: 1200.331367969513 and batch: 500, loss is 3.870822205543518 and perplexity is 47.981820785919396
At time: 1201.8842241764069 and batch: 550, loss is 3.869088921546936 and perplexity is 47.898726697454286
At time: 1203.4364223480225 and batch: 600, loss is 3.901616530418396 and perplexity is 49.48237425056176
At time: 1204.990361213684 and batch: 650, loss is 3.875147166252136 and perplexity is 48.18978968001789
At time: 1206.5436120033264 and batch: 700, loss is 3.839880223274231 and perplexity is 46.51990210452066
At time: 1208.094919681549 and batch: 750, loss is 3.823472237586975 and perplexity is 45.762832197628384
At time: 1209.652146577835 and batch: 800, loss is 3.780540080070496 and perplexity is 43.83971229790749
At time: 1211.2097578048706 and batch: 850, loss is 3.8104953241348265 and perplexity is 45.17280850804157
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3513539632161455 and perplexity of 77.58343696356174
Finished 42 epochs...
Completing Train Step...
At time: 1215.2348401546478 and batch: 50, loss is 3.931222529411316 and perplexity is 50.96925100437911
At time: 1216.8126969337463 and batch: 100, loss is 3.900866689682007 and perplexity is 49.4452842581469
At time: 1218.364667892456 and batch: 150, loss is 3.881460485458374 and perplexity is 48.49498960329715
At time: 1219.9173963069916 and batch: 200, loss is 3.936778507232666 and perplexity is 51.25322317357547
At time: 1221.468828201294 and batch: 250, loss is 3.9131826877593996 and perplexity is 50.058017751140966
At time: 1223.0213460922241 and batch: 300, loss is 3.9113038778305054 and perplexity is 49.964056545631955
At time: 1224.5745918750763 and batch: 350, loss is 3.8570228815078735 and perplexity is 47.32425153323968
At time: 1226.1272370815277 and batch: 400, loss is 3.860238552093506 and perplexity is 47.476675678456736
At time: 1227.6793248653412 and batch: 450, loss is 3.8995717000961303 and perplexity is 49.38129457189309
At time: 1229.2593994140625 and batch: 500, loss is 3.870608382225037 and perplexity is 47.9715622505683
At time: 1230.812447309494 and batch: 550, loss is 3.868903021812439 and perplexity is 47.88982316448632
At time: 1232.3643207550049 and batch: 600, loss is 3.9014605855941773 and perplexity is 49.474658332051746
At time: 1233.915845632553 and batch: 650, loss is 3.8750305557250977 and perplexity is 48.18417057087544
At time: 1235.4706859588623 and batch: 700, loss is 3.8397988367080687 and perplexity is 46.51611616349462
At time: 1237.023280620575 and batch: 750, loss is 3.823418140411377 and perplexity is 45.76035662462047
At time: 1238.5759990215302 and batch: 800, loss is 3.780505409240723 and perplexity is 43.83819236505393
At time: 1240.1287250518799 and batch: 850, loss is 3.8104621744155884 and perplexity is 45.17131106694235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.3513491948445635 and perplexity of 77.58306701778771
Finished 43 epochs...
Completing Train Step...
At time: 1244.1324722766876 and batch: 50, loss is 3.930877375602722 and perplexity is 50.951661808937274
At time: 1245.720501422882 and batch: 100, loss is 3.900539231300354 and perplexity is 49.42909563607802
At time: 1247.274006843567 and batch: 150, loss is 3.881129035949707 and perplexity is 48.47891862632615
At time: 1248.8339545726776 and batch: 200, loss is 3.9364842987060547 and perplexity is 51.238146256289056
At time: 1250.3855607509613 and batch: 250, loss is 3.912885055541992 and perplexity is 50.04312108929189
At time: 1251.9383337497711 and batch: 300, loss is 3.911010489463806 and perplexity is 49.94939982284947
At time: 1253.4931409358978 and batch: 350, loss is 3.8567712450027467 and perplexity is 47.31234452215828
At time: 1255.048040151596 and batch: 400, loss is 3.8600220394134523 and perplexity is 47.4663974888849
At time: 1256.6023263931274 and batch: 450, loss is 3.8993760251998903 and perplexity is 49.37163283751181
At time: 1258.1563169956207 and batch: 500, loss is 3.870396065711975 and perplexity is 47.961378176906884
At time: 1259.7112293243408 and batch: 550, loss is 3.868717565536499 and perplexity is 47.88094251973792
At time: 1261.2660672664642 and batch: 600, loss is 3.901304078102112 and perplexity is 49.466915783254656
At time: 1262.8190441131592 and batch: 650, loss is 3.874911684989929 and perplexity is 48.17844322350994
At time: 1264.3785169124603 and batch: 700, loss is 3.839714093208313 and perplexity is 46.512174392038
At time: 1265.9329311847687 and batch: 750, loss is 3.823360090255737 and perplexity is 45.75770030589689
At time: 1267.4875392913818 and batch: 800, loss is 3.780466847419739 and perplexity is 43.836501917121275
At time: 1269.0885074138641 and batch: 850, loss is 3.8104249095916747 and perplexity is 45.16962779735305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351345380147298 and perplexity of 77.58277106243858
Finished 44 epochs...
Completing Train Step...
At time: 1273.1490166187286 and batch: 50, loss is 3.93053635597229 and perplexity is 50.93428925441671
At time: 1274.7056138515472 and batch: 100, loss is 3.9002164936065675 and perplexity is 49.41314557772744
At time: 1276.2624025344849 and batch: 150, loss is 3.880802755355835 and perplexity is 48.46310347619507
At time: 1277.8195633888245 and batch: 200, loss is 3.9361948919296266 and perplexity is 51.22331973510221
At time: 1279.377737045288 and batch: 250, loss is 3.9125915718078614 and perplexity is 50.02843640221085
At time: 1280.9360904693604 and batch: 300, loss is 3.9107208490371703 and perplexity is 49.93493455233927
At time: 1282.493042230606 and batch: 350, loss is 3.856522727012634 and perplexity is 47.30058801430247
At time: 1284.0507636070251 and batch: 400, loss is 3.8598079681396484 and perplexity is 47.45623738424364
At time: 1285.6080253124237 and batch: 450, loss is 3.899182438850403 and perplexity is 49.362076088400485
At time: 1287.1650364398956 and batch: 500, loss is 3.870185284614563 and perplexity is 47.951269890336654
At time: 1288.729620218277 and batch: 550, loss is 3.868532419204712 and perplexity is 47.87207835947658
At time: 1290.287436246872 and batch: 600, loss is 3.901146745681763 and perplexity is 49.45913364587457
At time: 1291.8448884487152 and batch: 650, loss is 3.8747910547256468 and perplexity is 48.17263179569525
At time: 1293.40225481987 and batch: 700, loss is 3.8396266508102417 and perplexity is 46.50810743378453
At time: 1294.9614865779877 and batch: 750, loss is 3.8232984495162965 and perplexity is 45.75487985434318
At time: 1296.5188975334167 and batch: 800, loss is 3.7804243469238283 and perplexity is 43.83463888364101
At time: 1298.0787150859833 and batch: 850, loss is 3.8103841304779054 and perplexity is 45.16778585751878
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351343154907227 and perplexity of 77.58259842233966
Finished 45 epochs...
Completing Train Step...
At time: 1302.156839132309 and batch: 50, loss is 3.9301991653442383 and perplexity is 50.91711758465941
At time: 1303.7104587554932 and batch: 100, loss is 3.8998982238769533 and perplexity is 49.39742137164717
At time: 1305.2639286518097 and batch: 150, loss is 3.880480933189392 and perplexity is 48.44750948462248
At time: 1306.8171916007996 and batch: 200, loss is 3.9359099149703978 and perplexity is 51.20872434897565
At time: 1308.3980178833008 and batch: 250, loss is 3.912302031517029 and perplexity is 50.013953251014144
At time: 1309.951474905014 and batch: 300, loss is 3.910434799194336 and perplexity is 49.920652714914695
At time: 1311.505527973175 and batch: 350, loss is 3.8562770318984985 and perplexity is 47.28896791849045
At time: 1313.0608484745026 and batch: 400, loss is 3.8595961856842043 and perplexity is 47.44618804993817
At time: 1314.6152629852295 and batch: 450, loss is 3.898990478515625 and perplexity is 49.35260143715693
At time: 1316.1696231365204 and batch: 500, loss is 3.86997576713562 and perplexity is 47.941224313555836
At time: 1317.723789691925 and batch: 550, loss is 3.8683477449417114 and perplexity is 47.86323843496564
At time: 1319.2782349586487 and batch: 600, loss is 3.9009892177581786 and perplexity is 49.4513430648822
At time: 1320.8326432704926 and batch: 650, loss is 3.874669075012207 and perplexity is 48.16675607024011
At time: 1322.3872063159943 and batch: 700, loss is 3.8395369052886963 and perplexity is 46.50393372671536
At time: 1323.9418323040009 and batch: 750, loss is 3.823233985900879 and perplexity is 45.751930424431244
At time: 1325.496618270874 and batch: 800, loss is 3.7803784942626955 and perplexity is 43.83262899487813
At time: 1327.0507228374481 and batch: 850, loss is 3.810340394973755 and perplexity is 45.16581046483066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351340611775716 and perplexity of 77.5824011198398
Finished 46 epochs...
Completing Train Step...
At time: 1331.1056673526764 and batch: 50, loss is 3.9298653411865234 and perplexity is 50.900123057518115
At time: 1332.6577219963074 and batch: 100, loss is 3.8995838499069215 and perplexity is 49.38189454892356
At time: 1334.209909915924 and batch: 150, loss is 3.880163297653198 and perplexity is 48.43212327770249
At time: 1335.7630088329315 and batch: 200, loss is 3.935629062652588 and perplexity is 51.19434427948261
At time: 1337.3151741027832 and batch: 250, loss is 3.9120159101486207 and perplexity is 49.9996452372823
At time: 1338.867175102234 and batch: 300, loss is 3.9101517248153685 and perplexity is 49.90652345705956
At time: 1340.4195799827576 and batch: 350, loss is 3.8560337591171265 and perplexity is 47.27746519894273
At time: 1341.9720282554626 and batch: 400, loss is 3.8593863248825073 and perplexity is 47.43623199960544
At time: 1343.5239889621735 and batch: 450, loss is 3.8987999773025512 and perplexity is 49.34320060217848
At time: 1345.0777280330658 and batch: 500, loss is 3.8697675800323488 and perplexity is 47.93124460779778
At time: 1346.6305494308472 and batch: 550, loss is 3.8681633281707763 and perplexity is 47.854412464940424
At time: 1348.2295985221863 and batch: 600, loss is 3.900831255912781 and perplexity is 49.44353225639538
At time: 1349.7820358276367 and batch: 650, loss is 3.8745458984375 and perplexity is 48.16082341960192
At time: 1351.3344643115997 and batch: 700, loss is 3.8394453525543213 and perplexity is 46.49967635931327
At time: 1352.8865368366241 and batch: 750, loss is 3.823166875839233 and perplexity is 45.74886011258562
At time: 1354.4420788288116 and batch: 800, loss is 3.7803297519683836 and perplexity is 43.83049254404338
At time: 1355.994414806366 and batch: 850, loss is 3.8102939081192018 and perplexity is 45.163710897170326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.35133679707845 and perplexity of 77.58210516703087
Finished 47 epochs...
Completing Train Step...
At time: 1360.0063679218292 and batch: 50, loss is 3.9295347118377686 and perplexity is 50.883296764766456
At time: 1361.590178012848 and batch: 100, loss is 3.8992732858657835 and perplexity is 49.36656068938933
At time: 1363.1432161331177 and batch: 150, loss is 3.879849524497986 and perplexity is 48.416928961476955
At time: 1364.696231842041 and batch: 200, loss is 3.9353517723083495 and perplexity is 51.18015055011728
At time: 1366.2481956481934 and batch: 250, loss is 3.9117330503463745 and perplexity is 49.985504347557026
At time: 1367.8014092445374 and batch: 300, loss is 3.9098716878890993 and perplexity is 49.89254974429897
At time: 1369.35418343544 and batch: 350, loss is 3.855792737007141 and perplexity is 47.266071657628565
At time: 1370.9065673351288 and batch: 400, loss is 3.8591784334182737 and perplexity is 47.426371436876245
At time: 1372.459261417389 and batch: 450, loss is 3.8986107301712036 and perplexity is 49.33386342655772
At time: 1374.0122015476227 and batch: 500, loss is 3.8695607376098633 and perplexity is 47.92133141831977
At time: 1375.5641376972198 and batch: 550, loss is 3.867979440689087 and perplexity is 47.84561344658398
At time: 1377.1163668632507 and batch: 600, loss is 3.9006731796264646 and perplexity is 49.43571702415165
At time: 1378.668308019638 and batch: 650, loss is 3.8744218254089358 and perplexity is 48.154848331063434
At time: 1380.221939086914 and batch: 700, loss is 3.8393516874313356 and perplexity is 46.49532116537635
At time: 1381.7738199234009 and batch: 750, loss is 3.823097438812256 and perplexity is 45.74568355803833
At time: 1383.3275802135468 and batch: 800, loss is 3.7802781867980957 and perplexity is 43.82823247550245
At time: 1384.8815879821777 and batch: 850, loss is 3.8102450227737426 and perplexity is 45.16150310752562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351334571838379 and perplexity of 77.58193252841373
Finished 48 epochs...
Completing Train Step...
At time: 1388.9184005260468 and batch: 50, loss is 3.9292068910598754 and perplexity is 50.86661889666447
At time: 1390.5037758350372 and batch: 100, loss is 3.8989659547805786 and perplexity is 49.35139114187598
At time: 1392.0610404014587 and batch: 150, loss is 3.879539294242859 and perplexity is 48.40191089490252
At time: 1393.6193327903748 and batch: 200, loss is 3.9350778341293333 and perplexity is 51.16613227303212
At time: 1395.1764826774597 and batch: 250, loss is 3.9114530658721924 and perplexity is 49.97151114143719
At time: 1396.7335290908813 and batch: 300, loss is 3.909594144821167 and perplexity is 49.87870433441361
At time: 1398.2915089130402 and batch: 350, loss is 3.8555538845062256 and perplexity is 47.254783386374264
At time: 1399.8493468761444 and batch: 400, loss is 3.858971996307373 and perplexity is 47.41658188427456
At time: 1401.4061942100525 and batch: 450, loss is 3.8984226274490354 and perplexity is 49.32458446527841
At time: 1402.9638178348541 and batch: 500, loss is 3.869355115890503 and perplexity is 47.911478764753994
At time: 1404.52148604393 and batch: 550, loss is 3.8677959299087523 and perplexity is 47.83683406630491
At time: 1406.0795469284058 and batch: 600, loss is 3.9005149888992308 and perplexity is 49.42789737063892
At time: 1407.6367299556732 and batch: 650, loss is 3.8742967081069946 and perplexity is 48.148823703265315
At time: 1409.194479227066 and batch: 700, loss is 3.839256510734558 and perplexity is 46.49089610487688
At time: 1410.7522139549255 and batch: 750, loss is 3.8230259370803834 and perplexity is 45.7424127793728
At time: 1412.309807062149 and batch: 800, loss is 3.780224223136902 and perplexity is 43.82586740742886
At time: 1413.8692350387573 and batch: 850, loss is 3.8101940679550172 and perplexity is 45.15920196994893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351332664489746 and perplexity of 77.5817845527619
Finished 49 epochs...
Completing Train Step...
At time: 1417.9072291851044 and batch: 50, loss is 3.928881912231445 and perplexity is 50.850091008201595
At time: 1419.4602653980255 and batch: 100, loss is 3.898661985397339 and perplexity is 49.33639210968734
At time: 1421.0125949382782 and batch: 150, loss is 3.879232392311096 and perplexity is 48.38705853417354
At time: 1422.571742773056 and batch: 200, loss is 3.9348066806793214 and perplexity is 51.15226028054682
At time: 1424.1243343353271 and batch: 250, loss is 3.911175808906555 and perplexity is 49.957658112402946
At time: 1425.6765251159668 and batch: 300, loss is 3.909319109916687 and perplexity is 49.86498783607578
At time: 1427.2552313804626 and batch: 350, loss is 3.855316848754883 and perplexity is 47.243583640712245
At time: 1428.806839466095 and batch: 400, loss is 3.8587671518325806 and perplexity is 47.40686985422383
At time: 1430.3592281341553 and batch: 450, loss is 3.898235626220703 and perplexity is 49.315361569769685
At time: 1431.9110550880432 and batch: 500, loss is 3.869150457382202 and perplexity is 47.90167427629972
At time: 1433.4628887176514 and batch: 550, loss is 3.8676126480102537 and perplexity is 47.82806724396354
At time: 1435.015259027481 and batch: 600, loss is 3.900356469154358 and perplexity is 49.42006269395001
At time: 1436.5696029663086 and batch: 650, loss is 3.87417094707489 and perplexity is 48.14276883824279
At time: 1438.1217246055603 and batch: 700, loss is 3.8391597032547 and perplexity is 46.48639565623068
At time: 1439.6794328689575 and batch: 750, loss is 3.822952404022217 and perplexity is 45.73904932353733
At time: 1441.2313454151154 and batch: 800, loss is 3.7801679468154905 and perplexity is 43.823401118226
At time: 1442.7847995758057 and batch: 850, loss is 3.8101411151885984 and perplexity is 45.15681072858734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.351333300272624 and perplexity of 77.58183387794784
Annealing...
Finished Training.
Improved accuracyfrom -79.97592318355721 to -77.5817845527619
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fc6e480ccf8>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -86.85138419959085, 'params': {'lr': 10.023112699705816, 'anneal': 2.2869837742887213, 'wordvec_source': 'glove', 'seq_len': 50, 'tune_wordvecs': True, 'num_layers': 1, 'dropout': 0.4064131726922323, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200}}, {'best_accuracy': -221.37578755244607, 'params': {'lr': 20.945653848541383, 'anneal': 7.9288002959792685, 'wordvec_source': 'glove', 'seq_len': 50, 'tune_wordvecs': True, 'num_layers': 1, 'dropout': 0.6085402528955687, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200}}, {'best_accuracy': -200.43908928340946, 'params': {'lr': 20.231477962974886, 'anneal': 2.001912088084294, 'wordvec_source': 'glove', 'seq_len': 50, 'tune_wordvecs': True, 'num_layers': 1, 'dropout': 0.5188204368403374, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200}}, {'best_accuracy': -357.9527432330195, 'params': {'lr': 26.87229322417958, 'anneal': 3.0907908598910194, 'wordvec_source': 'glove', 'seq_len': 50, 'tune_wordvecs': True, 'num_layers': 1, 'dropout': 0.7470724338480177, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200}}, {'best_accuracy': -79.97592318355721, 'params': {'lr': 0.874506659112172, 'anneal': 6.62117689633781, 'wordvec_source': 'glove', 'seq_len': 50, 'tune_wordvecs': True, 'num_layers': 1, 'dropout': 0.6348270522530168, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200}}, {'best_accuracy': -77.5817845527619, 'params': {'lr': 7.10344333344402, 'anneal': 8.0, 'wordvec_source': 'glove', 'seq_len': 50, 'tune_wordvecs': True, 'num_layers': 1, 'dropout': 0.0, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200}}]
