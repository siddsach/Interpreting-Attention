Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'type': 'continuous', 'name': 'lr', 'domain': [0, 30]}, {'type': 'continuous', 'name': 'dropout', 'domain': [0, 1]}, {'type': 'continuous', 'name': 'anneal', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'seq_len': 35, 'num_layers': 1, 'data': 'wikitext', 'dropout': 0.5024071221904451, 'wordvec_source': '', 'lr': 23.21866549715745, 'tune_wordvecs': True, 'batch_size': 50, 'anneal': 3.532086664725023}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.560234785079956 and batch: 50, loss is 7.019289646148682 and perplexity is 1117.9921655682567
At time: 2.403365135192871 and batch: 100, loss is 6.368474082946777 and perplexity is 583.1672847111939
At time: 3.2466492652893066 and batch: 150, loss is 6.203979644775391 and perplexity is 494.71391439587376
At time: 4.091036319732666 and batch: 200, loss is 6.16004861831665 and perplexity is 473.4510926704278
At time: 4.936860084533691 and batch: 250, loss is 6.169788637161255 and perplexity is 478.0850459969796
At time: 5.780107021331787 and batch: 300, loss is 6.16693531036377 and perplexity is 476.7228574325117
At time: 6.625053882598877 and batch: 350, loss is 6.164562616348267 and perplexity is 475.59308079877997
At time: 7.470530271530151 and batch: 400, loss is 6.107132539749146 and perplexity is 449.0492364504828
At time: 8.316384077072144 and batch: 450, loss is 6.084323978424072 and perplexity is 438.92299094339336
At time: 9.162638664245605 and batch: 500, loss is 6.082761259078979 and perplexity is 438.23761316047984
At time: 10.007516860961914 and batch: 550, loss is 6.08303822517395 and perplexity is 438.3590069310694
At time: 10.853232145309448 and batch: 600, loss is 6.1046703433990475 and perplexity is 447.9449491035044
At time: 11.698225259780884 and batch: 650, loss is 6.074655418395996 and perplexity is 434.69968719053577
At time: 12.543686389923096 and batch: 700, loss is 6.092157964706421 and perplexity is 442.3750115197961
At time: 13.389256715774536 and batch: 750, loss is 6.041998558044433 and perplexity is 420.7330546238477
At time: 14.23538613319397 and batch: 800, loss is 6.051376256942749 and perplexity is 424.69712038387297
At time: 15.081934690475464 and batch: 850, loss is 6.0932375431060795 and perplexity is 442.8528479112956
At time: 15.927781820297241 and batch: 900, loss is 6.083844518661499 and perplexity is 438.71259547247945
At time: 16.773824214935303 and batch: 950, loss is 6.0576768970489505 and perplexity is 427.38143165301386
At time: 17.619455337524414 and batch: 1000, loss is 6.048218822479248 and perplexity is 423.35828181921937
At time: 18.45999574661255 and batch: 1050, loss is 6.048998956680298 and perplexity is 423.68868695758135
At time: 19.303405284881592 and batch: 1100, loss is 6.04173677444458 and perplexity is 420.6229280255285
At time: 20.148385286331177 and batch: 1150, loss is 6.076999111175537 and perplexity is 435.7196845219685
At time: 20.993939638137817 and batch: 1200, loss is 6.078098106384277 and perplexity is 436.19880159307195
At time: 21.839993238449097 and batch: 1250, loss is 6.042294883728028 and perplexity is 420.8577471077502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.502355422416743 and perplexity of 245.26896443714745
Finished 1 epochs...
Completing Train Step...
At time: 24.119389057159424 and batch: 50, loss is 5.820907030105591 and perplexity is 337.277836083438
At time: 24.98678731918335 and batch: 100, loss is 5.853438501358032 and perplexity is 348.43040144902295
At time: 25.826858520507812 and batch: 150, loss is 5.775904598236084 and perplexity is 322.43597785070796
At time: 26.660517692565918 and batch: 200, loss is 5.835974922180176 and perplexity is 342.3983831610879
At time: 27.505107641220093 and batch: 250, loss is 5.809149188995361 and perplexity is 333.3353995668486
At time: 28.354365587234497 and batch: 300, loss is 5.783953990936279 and perplexity is 325.04186549338607
At time: 29.198896169662476 and batch: 350, loss is 5.809739646911621 and perplexity is 333.5322782108493
At time: 30.044159412384033 and batch: 400, loss is 5.789464673995972 and perplexity is 326.8380126487104
At time: 30.889581203460693 and batch: 450, loss is 5.771478395462037 and perplexity is 321.0119646415523
At time: 31.73549461364746 and batch: 500, loss is 5.7699997520446775 and perplexity is 320.5376531683294
At time: 32.580864667892456 and batch: 550, loss is 5.764933042526245 and perplexity is 318.917689398511
At time: 33.425944328308105 and batch: 600, loss is 5.7773762130737305 and perplexity is 322.9108287330093
At time: 34.2761127948761 and batch: 650, loss is 5.80924711227417 and perplexity is 333.3680424603405
At time: 35.12177634239197 and batch: 700, loss is 5.807414293289185 and perplexity is 332.75759877040883
At time: 35.96787476539612 and batch: 750, loss is 5.739449825286865 and perplexity is 310.8933182780699
At time: 36.813347816467285 and batch: 800, loss is 5.763692712783813 and perplexity is 318.52237151590657
At time: 37.658642530441284 and batch: 850, loss is 5.830362310409546 and perplexity is 340.4820169060972
At time: 38.50396180152893 and batch: 900, loss is 5.8013395404815675 and perplexity is 330.7423060131518
At time: 39.349058866500854 and batch: 950, loss is 5.79333724975586 and perplexity is 328.1061715497491
At time: 40.209033727645874 and batch: 1000, loss is 5.767630701065063 and perplexity is 319.77918191036326
At time: 41.07395052909851 and batch: 1050, loss is 5.771889905929566 and perplexity is 321.144091609155
At time: 41.91826272010803 and batch: 1100, loss is 5.787285346984863 and perplexity is 326.126501329015
At time: 42.75938034057617 and batch: 1150, loss is 5.833209133148193 and perplexity is 341.4526898652918
At time: 43.60174345970154 and batch: 1200, loss is 5.767021102905273 and perplexity is 319.5843045140175
At time: 44.44209837913513 and batch: 1250, loss is 5.756032695770264 and perplexity is 316.0918057032542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.592289611370894 and perplexity of 268.34933254677105
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 46.73745942115784 and batch: 50, loss is 5.715268754959107 and perplexity is 303.4657502761465
At time: 47.62401342391968 and batch: 100, loss is 5.6792443752288815 and perplexity is 292.7281536880465
At time: 48.46413254737854 and batch: 150, loss is 5.574426259994507 and perplexity is 263.59827536736015
At time: 49.30258131027222 and batch: 200, loss is 5.614139175415039 and perplexity is 274.2771730103004
At time: 50.1413140296936 and batch: 250, loss is 5.636903162002564 and perplexity is 280.59242231249806
At time: 50.97979402542114 and batch: 300, loss is 5.6056272315979 and perplexity is 271.9524491190106
At time: 51.81886625289917 and batch: 350, loss is 5.632246866226196 and perplexity is 279.28893806079066
At time: 52.65756678581238 and batch: 400, loss is 5.600942630767822 and perplexity is 270.6814398600494
At time: 53.49592852592468 and batch: 450, loss is 5.575930395126343 and perplexity is 263.99506112892544
At time: 54.33322334289551 and batch: 500, loss is 5.572326927185059 and perplexity is 263.04547531803286
At time: 55.174025774002075 and batch: 550, loss is 5.556664371490479 and perplexity is 258.9576077055502
At time: 56.01482176780701 and batch: 600, loss is 5.581392517089844 and perplexity is 265.4409796473358
At time: 56.8543860912323 and batch: 650, loss is 5.579437665939331 and perplexity is 264.92258889634024
At time: 57.693663597106934 and batch: 700, loss is 5.59948763847351 and perplexity is 270.2878868285834
At time: 58.532997608184814 and batch: 750, loss is 5.552923059463501 and perplexity is 257.99057660390383
At time: 59.38549590110779 and batch: 800, loss is 5.553486738204956 and perplexity is 258.1360414012867
At time: 60.23780918121338 and batch: 850, loss is 5.582566118240356 and perplexity is 265.75268435917025
At time: 61.07875418663025 and batch: 900, loss is 5.567767381668091 and perplexity is 261.8488376353982
At time: 61.91788625717163 and batch: 950, loss is 5.5669558811187745 and perplexity is 261.6364333545708
At time: 62.756190061569214 and batch: 1000, loss is 5.543533697128296 and perplexity is 255.57954633254562
At time: 63.640477657318115 and batch: 1050, loss is 5.532586736679077 and perplexity is 252.79698522790804
At time: 64.48405647277832 and batch: 1100, loss is 5.496694393157959 and perplexity is 243.88441234573594
At time: 65.32531690597534 and batch: 1150, loss is 5.52892165184021 and perplexity is 251.87215864922817
At time: 66.15893626213074 and batch: 1200, loss is 5.525909423828125 and perplexity is 251.1146038142352
At time: 66.99835753440857 and batch: 1250, loss is 5.4973032188415525 and perplexity is 244.03294064913507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.298979599110401 and perplexity of 200.13249037735198
Finished 3 epochs...
Completing Train Step...
At time: 69.27946138381958 and batch: 50, loss is 5.527278051376343 and perplexity is 251.4585214726822
At time: 70.11968350410461 and batch: 100, loss is 5.537567548751831 and perplexity is 254.0592604721808
At time: 70.95941948890686 and batch: 150, loss is 5.452483749389648 and perplexity is 233.3369973987841
At time: 71.79937195777893 and batch: 200, loss is 5.490478687286377 and perplexity is 242.37320006293146
At time: 72.64348888397217 and batch: 250, loss is 5.518077840805054 and perplexity is 249.1556597750278
At time: 73.48621225357056 and batch: 300, loss is 5.494882669448852 and perplexity is 243.4429611881214
At time: 74.32939219474792 and batch: 350, loss is 5.537001304626465 and perplexity is 253.91544163057338
At time: 75.17175841331482 and batch: 400, loss is 5.518884315490722 and perplexity is 249.35667855481074
At time: 76.01340675354004 and batch: 450, loss is 5.484172277450561 and perplexity is 240.8495049030666
At time: 76.85477089881897 and batch: 500, loss is 5.49250244140625 and perplexity is 242.86420048959627
At time: 77.6974024772644 and batch: 550, loss is 5.462957677841186 and perplexity is 235.79379612181367
At time: 78.53867721557617 and batch: 600, loss is 5.471617765426636 and perplexity is 237.84465855510183
At time: 79.37914609909058 and batch: 650, loss is 5.4781898021697994 and perplexity is 239.41293011476327
At time: 80.22031545639038 and batch: 700, loss is 5.497861661911011 and perplexity is 244.16925721253753
At time: 81.06122326850891 and batch: 750, loss is 5.4585481548309325 and perplexity is 234.7563469611162
At time: 81.90304803848267 and batch: 800, loss is 5.46620852470398 and perplexity is 236.5615729304617
At time: 82.74530696868896 and batch: 850, loss is 5.4914554977417 and perplexity is 242.61006840774672
At time: 83.58722472190857 and batch: 900, loss is 5.480956230163574 and perplexity is 240.0761657204922
At time: 84.42728042602539 and batch: 950, loss is 5.483244123458863 and perplexity is 240.62606318389467
At time: 85.31267595291138 and batch: 1000, loss is 5.454703273773194 and perplexity is 233.85546972223074
At time: 86.15524053573608 and batch: 1050, loss is 5.451770401000976 and perplexity is 233.17060618218116
At time: 86.99668455123901 and batch: 1100, loss is 5.428097972869873 and perplexity is 227.71571179654993
At time: 87.83916234970093 and batch: 1150, loss is 5.454308433532715 and perplexity is 233.7631523988247
At time: 88.68109035491943 and batch: 1200, loss is 5.462959337234497 and perplexity is 235.79418739678633
At time: 89.52021145820618 and batch: 1250, loss is 5.451038084030151 and perplexity is 232.99991389823293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.274315604328239 and perplexity of 195.25679786588378
Finished 4 epochs...
Completing Train Step...
At time: 91.78125023841858 and batch: 50, loss is 5.451237468719483 and perplexity is 233.0463751453565
At time: 92.6471996307373 and batch: 100, loss is 5.460941114425659 and perplexity is 235.31878208787904
At time: 93.49368929862976 and batch: 150, loss is 5.379171485900879 and perplexity is 216.84254387527724
At time: 94.33700728416443 and batch: 200, loss is 5.4257927417755125 and perplexity is 227.19137904332132
At time: 95.17833089828491 and batch: 250, loss is 5.452995471954345 and perplexity is 233.4564317615547
At time: 96.0174925327301 and batch: 300, loss is 5.431343240737915 and perplexity is 228.4559107013192
At time: 96.85796809196472 and batch: 350, loss is 5.473159513473511 and perplexity is 238.21163791505236
At time: 97.69707822799683 and batch: 400, loss is 5.443698625564576 and perplexity is 231.29608096167877
At time: 98.5382628440857 and batch: 450, loss is 5.412412023544311 and perplexity is 224.1716433869922
At time: 99.37896060943604 and batch: 500, loss is 5.41645432472229 and perplexity is 225.07964666006254
At time: 100.2199273109436 and batch: 550, loss is 5.397112560272217 and perplexity is 220.76804060111962
At time: 101.05992197990417 and batch: 600, loss is 5.419207086563111 and perplexity is 225.70009089835065
At time: 101.91301846504211 and batch: 650, loss is 5.421620492935181 and perplexity is 226.2454547635078
At time: 102.76573061943054 and batch: 700, loss is 5.443301591873169 and perplexity is 231.2042668527001
At time: 103.62052607536316 and batch: 750, loss is 5.396358366012573 and perplexity is 220.60160138382594
At time: 104.47523975372314 and batch: 800, loss is 5.416488943099975 and perplexity is 225.0874386871528
At time: 105.31536674499512 and batch: 850, loss is 5.441151638031005 and perplexity is 230.70772231603758
At time: 106.1541018486023 and batch: 900, loss is 5.433258304595947 and perplexity is 228.893837554161
At time: 107.0389473438263 and batch: 950, loss is 5.4339705848693844 and perplexity is 229.0569321970152
At time: 107.88099479675293 and batch: 1000, loss is 5.422040872573852 and perplexity is 226.34058373977382
At time: 108.7210385799408 and batch: 1050, loss is 5.417946271896362 and perplexity is 225.41570423057345
At time: 109.56251215934753 and batch: 1100, loss is 5.384363613128662 and perplexity is 217.97134585735685
At time: 110.42659306526184 and batch: 1150, loss is 5.407613983154297 and perplexity is 223.09863501497296
At time: 111.26749324798584 and batch: 1200, loss is 5.41659197807312 and perplexity is 225.1106317601814
At time: 112.13124513626099 and batch: 1250, loss is 5.3988581562042235 and perplexity is 221.1537489422244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.262671254847172 and perplexity of 192.99634576145615
Finished 5 epochs...
Completing Train Step...
At time: 114.44146227836609 and batch: 50, loss is 5.397363920211792 and perplexity is 220.8235398173127
At time: 115.28329539299011 and batch: 100, loss is 5.407584390640259 and perplexity is 223.092033063169
At time: 116.12532067298889 and batch: 150, loss is 5.329653940200806 and perplexity is 206.36654663223575
At time: 116.967693567276 and batch: 200, loss is 5.375033569335938 and perplexity is 215.94712139040277
At time: 117.80988049507141 and batch: 250, loss is 5.405819034576416 and perplexity is 222.6985436164881
At time: 118.65233731269836 and batch: 300, loss is 5.383359270095825 and perplexity is 217.75253775237758
At time: 119.4959967136383 and batch: 350, loss is 5.408518342971802 and perplexity is 223.30048771579916
At time: 120.33878231048584 and batch: 400, loss is 5.382374448776245 and perplexity is 217.53819597230736
At time: 121.18115568161011 and batch: 450, loss is 5.34949499130249 and perplexity is 210.5019656960793
At time: 122.02426624298096 and batch: 500, loss is 5.36047477722168 and perplexity is 212.82596738792668
At time: 122.86625528335571 and batch: 550, loss is 5.347593717575073 and perplexity is 210.10212406377408
At time: 123.72817611694336 and batch: 600, loss is 5.37378228187561 and perplexity is 215.67707845121924
At time: 124.58054327964783 and batch: 650, loss is 5.372690849304199 and perplexity is 215.4418098761385
At time: 125.42347407341003 and batch: 700, loss is 5.398927307128906 and perplexity is 221.169042457235
At time: 126.26759624481201 and batch: 750, loss is 5.353458366394043 and perplexity is 211.33791944849511
At time: 127.11009454727173 and batch: 800, loss is 5.376266202926636 and perplexity is 216.2134691869398
At time: 127.97837781906128 and batch: 850, loss is 5.3989731884002685 and perplexity is 221.17919020688305
At time: 128.8213312625885 and batch: 900, loss is 5.398378610610962 and perplexity is 221.04772106111935
At time: 129.6648919582367 and batch: 950, loss is 5.391990270614624 and perplexity is 219.64009405073617
At time: 130.50706219673157 and batch: 1000, loss is 5.383117799758911 and perplexity is 217.6999633215608
At time: 131.34892773628235 and batch: 1050, loss is 5.378059463500977 and perplexity is 216.6015441326271
At time: 132.19222617149353 and batch: 1100, loss is 5.3500444698333744 and perplexity is 210.61766379083684
At time: 133.0354871749878 and batch: 1150, loss is 5.380827960968017 and perplexity is 217.20203580520973
At time: 133.882572889328 and batch: 1200, loss is 5.385304899215698 and perplexity is 218.1766158463211
At time: 134.7425034046173 and batch: 1250, loss is 5.377643623352051 and perplexity is 216.5114912393594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.260064563611998 and perplexity of 192.49391899875087
Finished 6 epochs...
Completing Train Step...
At time: 137.01316118240356 and batch: 50, loss is 5.36537314414978 and perplexity is 213.87102451506465
At time: 137.87522196769714 and batch: 100, loss is 5.3694889354705815 and perplexity is 214.75308696887197
At time: 138.71867561340332 and batch: 150, loss is 5.290572032928467 and perplexity is 198.45691683718246
At time: 139.56272387504578 and batch: 200, loss is 5.338931951522827 and perplexity is 208.29012749216838
At time: 140.43204402923584 and batch: 250, loss is 5.369961614608765 and perplexity is 214.8546202673882
At time: 141.2905879020691 and batch: 300, loss is 5.357585783004761 and perplexity is 212.21200169762488
At time: 142.13427257537842 and batch: 350, loss is 5.401592874526978 and perplexity is 221.7593698754967
At time: 142.9771068096161 and batch: 400, loss is 5.3560912132263185 and perplexity is 211.89507294832484
At time: 143.8201858997345 and batch: 450, loss is 5.322626085281372 and perplexity is 204.9213168632603
At time: 144.68070912361145 and batch: 500, loss is 5.326604423522949 and perplexity is 205.73818699008845
At time: 145.53124475479126 and batch: 550, loss is 5.316305389404297 and perplexity is 203.63015635570073
At time: 146.37573647499084 and batch: 600, loss is 5.342308626174927 and perplexity is 208.99464427853684
At time: 147.21836805343628 and batch: 650, loss is 5.349265346527099 and perplexity is 210.45363056961483
At time: 148.06037163734436 and batch: 700, loss is 5.379700355529785 and perplexity is 216.95725564209857
At time: 148.92923045158386 and batch: 750, loss is 5.327857513427734 and perplexity is 205.9961570313056
At time: 149.7965588569641 and batch: 800, loss is 5.355898761749268 and perplexity is 211.85429735234388
At time: 150.65413665771484 and batch: 850, loss is 5.372211084365845 and perplexity is 215.33847324013394
At time: 151.49992108345032 and batch: 900, loss is 5.366330461502075 and perplexity is 214.07586499101316
At time: 152.36564421653748 and batch: 950, loss is 5.369044170379639 and perplexity is 214.65759353026422
At time: 153.2075572013855 and batch: 1000, loss is 5.349750585556031 and perplexity is 210.55577566533677
At time: 154.04992198944092 and batch: 1050, loss is 5.350416107177734 and perplexity is 210.69595172654255
At time: 154.89307403564453 and batch: 1100, loss is 5.323775539398193 and perplexity is 205.1569999420462
At time: 155.73667311668396 and batch: 1150, loss is 5.345178918838501 and perplexity is 209.5953818065602
At time: 156.57948303222656 and batch: 1200, loss is 5.3502382469177245 and perplexity is 210.65848062219442
At time: 157.42167472839355 and batch: 1250, loss is 5.345800447463989 and perplexity is 209.72569182763235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.237099306426779 and perplexity of 188.1236212225621
Finished 7 epochs...
Completing Train Step...
At time: 159.73606395721436 and batch: 50, loss is 5.320028314590454 and perplexity is 204.3896691208117
At time: 160.604989528656 and batch: 100, loss is 5.32518159866333 and perplexity is 205.44566573567153
At time: 161.4410581588745 and batch: 150, loss is 5.252532720565796 and perplexity is 191.04953129243
At time: 162.2819926738739 and batch: 200, loss is 5.306886472702026 and perplexity is 201.72118520803105
At time: 163.12558507919312 and batch: 250, loss is 5.338414707183838 and perplexity is 208.18241846120034
At time: 163.96914625167847 and batch: 300, loss is 5.324775943756103 and perplexity is 205.36234259456052
At time: 164.81123399734497 and batch: 350, loss is 5.344961624145508 and perplexity is 209.5498427902903
At time: 165.65462803840637 and batch: 400, loss is 5.31346565246582 and perplexity is 203.05272054984522
At time: 166.49668335914612 and batch: 450, loss is 5.284849157333374 and perplexity is 197.32441626199346
At time: 167.36334013938904 and batch: 500, loss is 5.293905153274536 and perplexity is 199.11950124787512
At time: 168.20514345169067 and batch: 550, loss is 5.28498833656311 and perplexity is 197.35188163351717
At time: 169.0464346408844 and batch: 600, loss is 5.313665351867676 and perplexity is 203.09327410580988
At time: 169.88845086097717 and batch: 650, loss is 5.317503318786621 and perplexity is 203.8742370696673
At time: 170.73022508621216 and batch: 700, loss is 5.346132984161377 and perplexity is 209.7954449136378
At time: 171.59735321998596 and batch: 750, loss is 5.3011006736755375 and perplexity is 200.55743682458115
At time: 172.4436719417572 and batch: 800, loss is 5.330271902084351 and perplexity is 206.49411270351865
At time: 173.28637194633484 and batch: 850, loss is 5.338798370361328 and perplexity is 208.2623057132833
At time: 174.12802290916443 and batch: 900, loss is 5.3424450969696045 and perplexity is 209.02316789000054
At time: 174.97077894210815 and batch: 950, loss is 5.341723384857178 and perplexity is 208.87236776164178
At time: 175.81225538253784 and batch: 1000, loss is 5.334138841629028 and perplexity is 207.29415882172228
At time: 176.65281701087952 and batch: 1050, loss is 5.350001955032349 and perplexity is 210.60870961311224
At time: 177.49390172958374 and batch: 1100, loss is 5.318477725982666 and perplexity is 204.07299041097315
At time: 178.33529901504517 and batch: 1150, loss is 5.330176601409912 and perplexity is 206.47443461299287
At time: 179.17658472061157 and batch: 1200, loss is 5.337038469314575 and perplexity is 207.89610699473087
At time: 180.01923871040344 and batch: 1250, loss is 5.340377597808838 and perplexity is 208.5914590983696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.255686516309306 and perplexity of 191.6530136208113
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 182.29638838768005 and batch: 50, loss is 5.3159094905853275 and perplexity is 203.54955537326254
At time: 183.13924145698547 and batch: 100, loss is 5.300391836166382 and perplexity is 200.41532456382058
At time: 183.9782063961029 and batch: 150, loss is 5.206410608291626 and perplexity is 182.43803995692508
At time: 184.8360311985016 and batch: 200, loss is 5.251405858993531 and perplexity is 190.83436617063936
At time: 185.68148803710938 and batch: 250, loss is 5.2713957786560055 and perplexity is 194.68751356470275
At time: 186.52473664283752 and batch: 300, loss is 5.27196551322937 and perplexity is 194.79846537572305
At time: 187.36770915985107 and batch: 350, loss is 5.276836776733399 and perplexity is 195.74969499481932
At time: 188.23212671279907 and batch: 400, loss is 5.241179275512695 and perplexity is 188.8927276798776
At time: 189.09100532531738 and batch: 450, loss is 5.199435615539551 and perplexity is 181.1699635036028
At time: 189.9400851726532 and batch: 500, loss is 5.185565738677979 and perplexity is 178.67450427975336
At time: 190.78319215774536 and batch: 550, loss is 5.182807865142823 and perplexity is 178.18242145598447
At time: 191.627272605896 and batch: 600, loss is 5.213338174819946 and perplexity is 183.7062794507351
At time: 192.4690227508545 and batch: 650, loss is 5.210304145812988 and perplexity is 183.14975395428766
At time: 193.33849358558655 and batch: 700, loss is 5.237559843063354 and perplexity is 188.21027899534832
At time: 194.1797752380371 and batch: 750, loss is 5.200235767364502 and perplexity is 181.31498499239348
At time: 195.02348446846008 and batch: 800, loss is 5.211720485687255 and perplexity is 183.40934004144384
At time: 195.8660945892334 and batch: 850, loss is 5.2244240665435795 and perplexity is 185.75415767869617
At time: 196.71025896072388 and batch: 900, loss is 5.21593747138977 and perplexity is 184.18440768210255
At time: 197.5530092716217 and batch: 950, loss is 5.198106079101563 and perplexity is 180.9292514888113
At time: 198.39466333389282 and batch: 1000, loss is 5.180031213760376 and perplexity is 177.6883572286087
At time: 199.23813033103943 and batch: 1050, loss is 5.180530986785889 and perplexity is 177.77718327108033
At time: 200.0810205936432 and batch: 1100, loss is 5.146016969680786 and perplexity is 171.74605642110294
At time: 200.937655210495 and batch: 1150, loss is 5.168246936798096 and perplexity is 175.6067178113433
At time: 201.78899025917053 and batch: 1200, loss is 5.181938972473144 and perplexity is 178.02766729822878
At time: 202.6316819190979 and batch: 1250, loss is 5.2108559322357175 and perplexity is 183.2508413886179
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.147300998659899 and perplexity of 171.96672497668757
Finished 9 epochs...
Completing Train Step...
At time: 204.90395522117615 and batch: 50, loss is 5.236007585525512 and perplexity is 187.9183548004605
At time: 205.7778079509735 and batch: 100, loss is 5.230281295776368 and perplexity is 186.84535493775357
At time: 206.63439440727234 and batch: 150, loss is 5.149088382720947 and perplexity is 172.27437041831698
At time: 207.47673177719116 and batch: 200, loss is 5.193431720733643 and perplexity is 180.0854968701539
At time: 208.32703733444214 and batch: 250, loss is 5.220028266906739 and perplexity is 184.93941166190092
At time: 209.1820330619812 and batch: 300, loss is 5.222309503555298 and perplexity is 185.36178380778617
At time: 210.0234501361847 and batch: 350, loss is 5.230577564239502 and perplexity is 186.9007195248903
At time: 210.8657968044281 and batch: 400, loss is 5.201135845184326 and perplexity is 181.4782560560957
At time: 211.707622051239 and batch: 450, loss is 5.157508955001831 and perplexity is 173.7311440324989
At time: 212.54968786239624 and batch: 500, loss is 5.151173028945923 and perplexity is 172.63387612518346
At time: 213.39224767684937 and batch: 550, loss is 5.15178069114685 and perplexity is 172.73881108557984
At time: 214.2615532875061 and batch: 600, loss is 5.185550718307495 and perplexity is 178.67182054265857
At time: 215.13555073738098 and batch: 650, loss is 5.186309976577759 and perplexity is 178.80753011285594
At time: 215.99248790740967 and batch: 700, loss is 5.210267400741577 and perplexity is 183.14302422714255
At time: 216.8343381881714 and batch: 750, loss is 5.172107563018799 and perplexity is 176.28598005580514
At time: 217.6789093017578 and batch: 800, loss is 5.190508260726928 and perplexity is 179.55979293389706
At time: 218.52530217170715 and batch: 850, loss is 5.207876300811767 and perplexity is 182.7056340849301
At time: 219.37238097190857 and batch: 900, loss is 5.200131139755249 and perplexity is 181.29601543137923
At time: 220.21932411193848 and batch: 950, loss is 5.184202308654785 and perplexity is 178.43106009350777
At time: 221.05988907814026 and batch: 1000, loss is 5.168369102478027 and perplexity is 175.62817223589573
At time: 221.90093731880188 and batch: 1050, loss is 5.172485570907593 and perplexity is 176.35263014328584
At time: 222.7441930770874 and batch: 1100, loss is 5.142426509857177 and perplexity is 171.13051480564584
At time: 223.58638191223145 and batch: 1150, loss is 5.169527606964111 and perplexity is 175.83175616499165
At time: 224.42870163917542 and batch: 1200, loss is 5.183895139694214 and perplexity is 178.37626002711582
At time: 225.27080845832825 and batch: 1250, loss is 5.2044250106811525 and perplexity is 182.07615082272582
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.144643519046533 and perplexity of 171.5103336048617
Finished 10 epochs...
Completing Train Step...
At time: 227.5672972202301 and batch: 50, loss is 5.211478128433227 and perplexity is 183.36489484345387
At time: 228.4092411994934 and batch: 100, loss is 5.20628101348877 and perplexity is 182.4143984670436
At time: 229.2521002292633 and batch: 150, loss is 5.128042125701905 and perplexity is 168.68652750649173
At time: 230.09424924850464 and batch: 200, loss is 5.171807928085327 and perplexity is 176.23316653068275
At time: 230.93581342697144 and batch: 250, loss is 5.198569631576538 and perplexity is 181.01314113325827
At time: 231.77876138687134 and batch: 300, loss is 5.199244346618652 and perplexity is 181.13531463391573
At time: 232.62051153182983 and batch: 350, loss is 5.208993940353394 and perplexity is 182.90994727904607
At time: 233.45865654945374 and batch: 400, loss is 5.183871898651123 and perplexity is 178.37211442494439
At time: 234.3009810447693 and batch: 450, loss is 5.1387837123870845 and perplexity is 170.5082550704647
At time: 235.1432399749756 and batch: 500, loss is 5.135697593688965 and perplexity is 169.9828574929481
At time: 236.0110423564911 and batch: 550, loss is 5.138038282394409 and perplexity is 170.38120046417689
At time: 236.8532292842865 and batch: 600, loss is 5.17184455871582 and perplexity is 176.23962218092316
At time: 237.6975109577179 and batch: 650, loss is 5.173823499679566 and perplexity is 176.58873531189664
At time: 238.5473129749298 and batch: 700, loss is 5.1965102863311765 and perplexity is 180.64075614794612
At time: 239.41150045394897 and batch: 750, loss is 5.159098815917969 and perplexity is 174.00757207115313
At time: 240.27460074424744 and batch: 800, loss is 5.179727468490601 and perplexity is 177.63439342664606
At time: 241.12344574928284 and batch: 850, loss is 5.198144826889038 and perplexity is 180.9362622328206
At time: 241.96968817710876 and batch: 900, loss is 5.18930347442627 and perplexity is 179.34359201932267
At time: 242.81646490097046 and batch: 950, loss is 5.172843551635742 and perplexity is 176.4157722873965
At time: 243.67543387413025 and batch: 1000, loss is 5.158499555587769 and perplexity is 173.9033274740019
At time: 244.5290412902832 and batch: 1050, loss is 5.16359375 and perplexity is 174.79148513628812
At time: 245.37727165222168 and batch: 1100, loss is 5.135920114517212 and perplexity is 170.02068642789212
At time: 246.2188687324524 and batch: 1150, loss is 5.163493394851685 and perplexity is 174.77394479101963
At time: 247.0626904964447 and batch: 1200, loss is 5.177235946655274 and perplexity is 177.19236434793385
At time: 247.90462470054626 and batch: 1250, loss is 5.195973072052002 and perplexity is 180.54373941606266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.142832957915146 and perplexity of 171.20008460834558
Finished 11 epochs...
Completing Train Step...
At time: 250.16357159614563 and batch: 50, loss is 5.193476629257202 and perplexity is 180.09358442553153
At time: 251.03289222717285 and batch: 100, loss is 5.188434953689575 and perplexity is 179.18789601305284
At time: 251.8743793964386 and batch: 150, loss is 5.113166990280152 and perplexity is 166.1958629776231
At time: 252.7156970500946 and batch: 200, loss is 5.15642861366272 and perplexity is 173.54355644326816
At time: 253.5615677833557 and batch: 250, loss is 5.183287143707275 and perplexity is 178.26784093937917
At time: 254.40837144851685 and batch: 300, loss is 5.1821657657623295 and perplexity is 178.0680473572514
At time: 255.25717997550964 and batch: 350, loss is 5.1925795459747315 and perplexity is 179.9320979259125
At time: 256.1282625198364 and batch: 400, loss is 5.169032077789307 and perplexity is 175.74464798405975
At time: 256.966792345047 and batch: 450, loss is 5.124592332839966 and perplexity is 168.10559654986994
At time: 257.8482491970062 and batch: 500, loss is 5.122608833312988 and perplexity is 167.77248964653455
At time: 258.68939900398254 and batch: 550, loss is 5.126995096206665 and perplexity is 168.51000016753497
At time: 259.53177523612976 and batch: 600, loss is 5.161507043838501 and perplexity is 174.42712695372057
At time: 260.38653326034546 and batch: 650, loss is 5.163753767013549 and perplexity is 174.81945698565963
At time: 261.2372486591339 and batch: 700, loss is 5.185774488449097 and perplexity is 178.71180643489907
At time: 262.0802857875824 and batch: 750, loss is 5.149587774276734 and perplexity is 172.36042426967725
At time: 262.92241048812866 and batch: 800, loss is 5.169451398849487 and perplexity is 175.8183568689373
At time: 263.76427698135376 and batch: 850, loss is 5.189868936538696 and perplexity is 179.4450327034449
At time: 264.60661268234253 and batch: 900, loss is 5.180323657989502 and perplexity is 177.74032876228017
At time: 265.44789481163025 and batch: 950, loss is 5.163508501052856 and perplexity is 174.77658498133079
At time: 266.3147654533386 and batch: 1000, loss is 5.150007591247559 and perplexity is 172.43279929195262
At time: 267.15803480148315 and batch: 1050, loss is 5.155459451675415 and perplexity is 173.3754461014566
At time: 268.0016655921936 and batch: 1100, loss is 5.127946176528931 and perplexity is 168.67034295014545
At time: 268.86553931236267 and batch: 1150, loss is 5.155191135406494 and perplexity is 173.32893288904108
At time: 269.72777342796326 and batch: 1200, loss is 5.168467197418213 and perplexity is 175.64540131597514
At time: 270.57488775253296 and batch: 1250, loss is 5.186549425125122 and perplexity is 178.8503504426274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.142357151003649 and perplexity of 171.11864580095028
Finished 12 epochs...
Completing Train Step...
At time: 272.8297371864319 and batch: 50, loss is 5.1775623798370365 and perplexity is 177.25021525692642
At time: 273.6999189853668 and batch: 100, loss is 5.17384108543396 and perplexity is 176.59184078533056
At time: 274.5458550453186 and batch: 150, loss is 5.100741996765136 and perplexity is 164.14365618758436
At time: 275.3907961845398 and batch: 200, loss is 5.144062919616699 and perplexity is 171.4107837050641
At time: 276.23472261428833 and batch: 250, loss is 5.170098476409912 and perplexity is 175.93216179871513
At time: 277.0783770084381 and batch: 300, loss is 5.167851104736328 and perplexity is 175.53722079765222
At time: 277.92522501945496 and batch: 350, loss is 5.178170299530029 and perplexity is 177.35800191286415
At time: 278.79502177238464 and batch: 400, loss is 5.157142944335938 and perplexity is 173.66756821620828
At time: 279.6379201412201 and batch: 450, loss is 5.112494497299195 and perplexity is 166.08413499865807
At time: 280.47946310043335 and batch: 500, loss is 5.111717777252197 and perplexity is 165.95518420733168
At time: 281.3170816898346 and batch: 550, loss is 5.116863079071045 and perplexity is 166.8112742506235
At time: 282.1586129665375 and batch: 600, loss is 5.152000875473022 and perplexity is 172.7768496518968
At time: 283.02207708358765 and batch: 650, loss is 5.154194498062134 and perplexity is 173.15627285567606
At time: 283.868679523468 and batch: 700, loss is 5.17581953048706 and perplexity is 176.9415638790374
At time: 284.71302676200867 and batch: 750, loss is 5.13941104888916 and perplexity is 170.6152546816474
At time: 285.5554881095886 and batch: 800, loss is 5.159836282730103 and perplexity is 174.13594420989355
At time: 286.3984007835388 and batch: 850, loss is 5.181547079086304 and perplexity is 177.95791310173684
At time: 287.26833271980286 and batch: 900, loss is 5.171215257644653 and perplexity is 176.1287492877768
At time: 288.1113119125366 and batch: 950, loss is 5.154238719940185 and perplexity is 173.16393032057053
At time: 288.9549882411957 and batch: 1000, loss is 5.141102952957153 and perplexity is 170.9041636593245
At time: 289.7985212802887 and batch: 1050, loss is 5.147130908966065 and perplexity is 171.93747769648758
At time: 290.6415853500366 and batch: 1100, loss is 5.120477247238159 and perplexity is 167.41524902369807
At time: 291.485967874527 and batch: 1150, loss is 5.148090467453003 and perplexity is 172.1025409436887
At time: 292.32966899871826 and batch: 1200, loss is 5.161049423217773 and perplexity is 174.34732376479732
At time: 293.17294430732727 and batch: 1250, loss is 5.177913188934326 and perplexity is 177.31240715303878
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.141979802263914 and perplexity of 171.05408657704905
Finished 13 epochs...
Completing Train Step...
At time: 295.52071237564087 and batch: 50, loss is 5.1640748691558835 and perplexity is 174.87560090130282
At time: 296.37781739234924 and batch: 100, loss is 5.161057434082031 and perplexity is 174.34872044313605
At time: 297.22765254974365 and batch: 150, loss is 5.088615036010742 and perplexity is 162.16511361249783
At time: 298.07497572898865 and batch: 200, loss is 5.132714881896972 and perplexity is 169.47660300041687
At time: 298.9388847351074 and batch: 250, loss is 5.158733243942261 and perplexity is 173.94397140526178
At time: 299.79889941215515 and batch: 300, loss is 5.155394649505615 and perplexity is 173.36421136038092
At time: 300.66440081596375 and batch: 350, loss is 5.166194181442261 and perplexity is 175.246609913995
At time: 301.5070583820343 and batch: 400, loss is 5.145696468353272 and perplexity is 171.69102040205436
At time: 302.36474227905273 and batch: 450, loss is 5.1016624164581295 and perplexity is 164.29480679154466
At time: 303.211128950119 and batch: 500, loss is 5.101074819564819 and perplexity is 164.19829603097534
At time: 304.05245637893677 and batch: 550, loss is 5.107987565994263 and perplexity is 165.33728946632175
At time: 304.89012241363525 and batch: 600, loss is 5.142752733230591 and perplexity is 171.18635068646924
At time: 305.747022151947 and batch: 650, loss is 5.145304088592529 and perplexity is 171.6236655357547
At time: 306.58805799484253 and batch: 700, loss is 5.167082662582398 and perplexity is 175.40238241199253
At time: 307.4294738769531 and batch: 750, loss is 5.131014461517334 and perplexity is 169.18866640680804
At time: 308.27290630340576 and batch: 800, loss is 5.151301012039185 and perplexity is 172.6559717565552
At time: 309.1166760921478 and batch: 850, loss is 5.174189434051514 and perplexity is 176.65336702460752
At time: 309.95968294143677 and batch: 900, loss is 5.162624006271362 and perplexity is 174.62206435040713
At time: 310.8022847175598 and batch: 950, loss is 5.145162477493286 and perplexity is 171.59936344058582
At time: 311.64492535591125 and batch: 1000, loss is 5.133104820251464 and perplexity is 169.54270131441214
At time: 312.4888045787811 and batch: 1050, loss is 5.138959264755249 and perplexity is 170.5381908260034
At time: 313.33286595344543 and batch: 1100, loss is 5.113307056427002 and perplexity is 166.21914302210777
At time: 314.17701745033264 and batch: 1150, loss is 5.141283674240112 and perplexity is 170.93505247009273
At time: 315.0219213962555 and batch: 1200, loss is 5.154075241088867 and perplexity is 173.1356239939579
At time: 315.8659579753876 and batch: 1250, loss is 5.169462652206421 and perplexity is 175.82033542679525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.142274731267108 and perplexity of 171.10454282843568
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 318.11800956726074 and batch: 50, loss is 5.15699369430542 and perplexity is 173.64165026052805
At time: 318.986234664917 and batch: 100, loss is 5.160513439178467 and perplexity is 174.25390142064285
At time: 319.82922172546387 and batch: 150, loss is 5.080445594787598 and perplexity is 160.84571197350448
At time: 320.67230319976807 and batch: 200, loss is 5.125906457901001 and perplexity is 168.3266535436517
At time: 321.51492977142334 and batch: 250, loss is 5.148868055343628 and perplexity is 172.23641783925297
At time: 322.38732528686523 and batch: 300, loss is 5.140391178131104 and perplexity is 170.78256165971968
At time: 323.229731798172 and batch: 350, loss is 5.148260564804077 and perplexity is 172.1318176198882
At time: 324.0722417831421 and batch: 400, loss is 5.119569206237793 and perplexity is 167.26329811276747
At time: 324.91392254829407 and batch: 450, loss is 5.074219007492065 and perplexity is 159.84730367073732
At time: 325.7543408870697 and batch: 500, loss is 5.069760284423828 and perplexity is 159.13617535070236
At time: 326.5953562259674 and batch: 550, loss is 5.0770971298217775 and perplexity is 160.30802645500657
At time: 327.43729162216187 and batch: 600, loss is 5.110824279785156 and perplexity is 165.8069698950168
At time: 328.27933406829834 and batch: 650, loss is 5.113178663253784 and perplexity is 166.19780298887224
At time: 329.1175844669342 and batch: 700, loss is 5.1309714603424075 and perplexity is 169.1813912517895
At time: 329.95867133140564 and batch: 750, loss is 5.090658388137817 and perplexity is 162.49681281606033
At time: 330.80048537254333 and batch: 800, loss is 5.100989723205567 and perplexity is 164.18432394828275
At time: 331.64386343955994 and batch: 850, loss is 5.121999053955078 and perplexity is 167.67021663067896
At time: 332.4873867034912 and batch: 900, loss is 5.108785266876221 and perplexity is 165.46923178620554
At time: 333.33009362220764 and batch: 950, loss is 5.085441455841065 and perplexity is 161.651285392505
At time: 334.1790289878845 and batch: 1000, loss is 5.073993082046509 and perplexity is 159.81119417661492
At time: 335.0407910346985 and batch: 1050, loss is 5.073543548583984 and perplexity is 159.73936984207697
At time: 335.9060344696045 and batch: 1100, loss is 5.043050622940063 and perplexity is 154.94196430091478
At time: 336.7489490509033 and batch: 1150, loss is 5.068025045394897 and perplexity is 158.86027549364817
At time: 337.5916407108307 and batch: 1200, loss is 5.092094421386719 and perplexity is 162.73033127210826
At time: 338.4572699069977 and batch: 1250, loss is 5.122463884353638 and perplexity is 167.74817296113443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.110079354613367 and perplexity of 165.68350210232677
Finished 15 epochs...
Completing Train Step...
At time: 340.8033106327057 and batch: 50, loss is 5.128525743484497 and perplexity is 168.7681270407801
At time: 341.64595222473145 and batch: 100, loss is 5.134018468856811 and perplexity is 169.69767455177552
At time: 342.4887297153473 and batch: 150, loss is 5.056882486343384 and perplexity is 157.09999074395242
At time: 343.33547949790955 and batch: 200, loss is 5.100422220230103 and perplexity is 164.091175289488
At time: 344.2089409828186 and batch: 250, loss is 5.127341651916504 and perplexity is 168.5684083905251
At time: 345.05629205703735 and batch: 300, loss is 5.121194143295288 and perplexity is 167.5353113866016
At time: 345.90298771858215 and batch: 350, loss is 5.130186605453491 and perplexity is 169.0486605037536
At time: 346.75269770622253 and batch: 400, loss is 5.104445686340332 and perplexity is 164.7527205322456
At time: 347.62330770492554 and batch: 450, loss is 5.059802942276001 and perplexity is 157.55946495445292
At time: 348.4700644016266 and batch: 500, loss is 5.056700887680054 and perplexity is 157.07146418589494
At time: 349.31694316864014 and batch: 550, loss is 5.063461456298828 and perplexity is 158.136954198539
At time: 350.16336464881897 and batch: 600, loss is 5.0985605335235595 and perplexity is 163.78597311335702
At time: 351.00999426841736 and batch: 650, loss is 5.103156032562256 and perplexity is 164.54038351381195
At time: 351.8592677116394 and batch: 700, loss is 5.121021575927735 and perplexity is 167.50640275335854
At time: 352.70281982421875 and batch: 750, loss is 5.082404108047485 and perplexity is 161.16103911860438
At time: 353.5456976890564 and batch: 800, loss is 5.096188688278199 and perplexity is 163.3979584690188
At time: 354.40495443344116 and batch: 850, loss is 5.118731365203858 and perplexity is 167.1232167492889
At time: 355.2517695426941 and batch: 900, loss is 5.105642557144165 and perplexity is 164.95002630450844
At time: 356.0992488861084 and batch: 950, loss is 5.08402310371399 and perplexity is 161.42216946996197
At time: 356.95543336868286 and batch: 1000, loss is 5.075287628173828 and perplexity is 160.0182111067103
At time: 357.8032236099243 and batch: 1050, loss is 5.077697792053223 and perplexity is 160.40434635686321
At time: 358.65424394607544 and batch: 1100, loss is 5.048715686798095 and perplexity is 155.82221139683054
At time: 359.50846576690674 and batch: 1150, loss is 5.075195598602295 and perplexity is 160.00348537691633
At time: 360.3515799045563 and batch: 1200, loss is 5.098547334671021 and perplexity is 163.78381134071645
At time: 361.19453263282776 and batch: 1250, loss is 5.123656530380249 and perplexity is 167.94835650341236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.109090763286953 and perplexity of 165.51978976485012
Finished 16 epochs...
Completing Train Step...
At time: 363.46616291999817 and batch: 50, loss is 5.120495176315307 and perplexity is 167.41825065152173
At time: 364.34272289276123 and batch: 100, loss is 5.125617647171021 and perplexity is 168.27804601949174
At time: 365.1988136768341 and batch: 150, loss is 5.048937397003174 and perplexity is 155.85676260131092
At time: 366.06662487983704 and batch: 200, loss is 5.091641855239868 and perplexity is 162.65670169549236
At time: 366.90764474868774 and batch: 250, loss is 5.119108238220215 and perplexity is 167.18621285010815
At time: 367.7614414691925 and batch: 300, loss is 5.113615398406982 and perplexity is 166.27040326421405
At time: 368.61650824546814 and batch: 350, loss is 5.123300542831421 and perplexity is 167.88857962019543
At time: 369.45774030685425 and batch: 400, loss is 5.097973947525024 and perplexity is 163.6899267272589
At time: 370.30873703956604 and batch: 450, loss is 5.0537081146240235 and perplexity is 156.60208765905776
At time: 371.1683659553528 and batch: 500, loss is 5.051539688110352 and perplexity is 156.2628754513707
At time: 372.01094698905945 and batch: 550, loss is 5.058174343109131 and perplexity is 157.30307257797082
At time: 372.8546140193939 and batch: 600, loss is 5.093928003311158 and perplexity is 163.0289843848882
At time: 373.6967341899872 and batch: 650, loss is 5.09918779373169 and perplexity is 163.88874176491424
At time: 374.53985500335693 and batch: 700, loss is 5.117340316772461 and perplexity is 166.89090187885898
At time: 375.3814923763275 and batch: 750, loss is 5.078909502029419 and perplexity is 160.5988277072699
At time: 376.223596572876 and batch: 800, loss is 5.093789901733398 and perplexity is 163.00647137950068
At time: 377.0595042705536 and batch: 850, loss is 5.117020740509033 and perplexity is 166.83757602932806
At time: 377.9012176990509 and batch: 900, loss is 5.104102821350097 and perplexity is 164.69624227509098
At time: 378.7427158355713 and batch: 950, loss is 5.08345666885376 and perplexity is 161.330760217098
At time: 379.5858995914459 and batch: 1000, loss is 5.075453805923462 and perplexity is 160.04480478250983
At time: 380.4283437728882 and batch: 1050, loss is 5.078970832824707 and perplexity is 160.6086776631452
At time: 381.27327156066895 and batch: 1100, loss is 5.0503499221801755 and perplexity is 156.07706976056554
At time: 382.1172511577606 and batch: 1150, loss is 5.077201824188233 and perplexity is 160.3248106808656
At time: 382.959064245224 and batch: 1200, loss is 5.0997547435760495 and perplexity is 163.98168480608194
At time: 383.80170154571533 and batch: 1250, loss is 5.1224024295806885 and perplexity is 167.73786435201225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1087610843407845 and perplexity of 165.46523036902664
Finished 17 epochs...
Completing Train Step...
At time: 386.0564138889313 and batch: 50, loss is 5.114672660827637 and perplexity is 166.44628767485753
At time: 386.92477679252625 and batch: 100, loss is 5.120031080245972 and perplexity is 167.34057052637394
At time: 387.7673647403717 and batch: 150, loss is 5.043808708190918 and perplexity is 155.0594680520927
At time: 388.60963010787964 and batch: 200, loss is 5.086340303421021 and perplexity is 161.79665057996326
At time: 389.4522578716278 and batch: 250, loss is 5.113896961212158 and perplexity is 166.31722541675384
At time: 390.29329323768616 and batch: 300, loss is 5.108492097854614 and perplexity is 165.42072844360294
At time: 391.1346881389618 and batch: 350, loss is 5.1186181163787845 and perplexity is 167.10429131301152
At time: 391.9767770767212 and batch: 400, loss is 5.093407192230225 and perplexity is 162.9440991898011
At time: 392.8192582130432 and batch: 450, loss is 5.049560136795044 and perplexity is 155.95385103649318
At time: 393.66085863113403 and batch: 500, loss is 5.047948141098022 and perplexity is 155.70265661624526
At time: 394.50338768959045 and batch: 550, loss is 5.054699516296386 and perplexity is 156.7574202164143
At time: 395.34545850753784 and batch: 600, loss is 5.090607013702392 and perplexity is 162.48846484848144
At time: 396.19220328330994 and batch: 650, loss is 5.096329774856567 and perplexity is 163.4210133542208
At time: 397.05024766921997 and batch: 700, loss is 5.114575519561767 and perplexity is 166.4301196570771
At time: 397.892032623291 and batch: 750, loss is 5.076215362548828 and perplexity is 160.1667343862032
At time: 398.7339985370636 and batch: 800, loss is 5.091680841445923 and perplexity is 162.66304318679536
At time: 399.59760189056396 and batch: 850, loss is 5.115448455810547 and perplexity is 166.57546597121797
At time: 400.44170212745667 and batch: 900, loss is 5.102118921279907 and perplexity is 164.36982528488724
At time: 401.2791631221771 and batch: 950, loss is 5.082254285812378 and perplexity is 161.13689542018804
At time: 402.12159299850464 and batch: 1000, loss is 5.074501962661743 and perplexity is 159.89253969125116
At time: 402.9796848297119 and batch: 1050, loss is 5.078887310028076 and perplexity is 160.59526373741565
At time: 403.8244512081146 and batch: 1100, loss is 5.05062014579773 and perplexity is 156.11925116992336
At time: 404.6682713031769 and batch: 1150, loss is 5.077636499404907 and perplexity is 160.39451505097009
At time: 405.5150496959686 and batch: 1200, loss is 5.099853029251099 and perplexity is 163.99780264873237
At time: 406.36831402778625 and batch: 1250, loss is 5.120896406173706 and perplexity is 167.48543733027353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.108282604356752 and perplexity of 165.38607750628216
Finished 18 epochs...
Completing Train Step...
At time: 408.7500820159912 and batch: 50, loss is 5.109972381591797 and perplexity is 165.66577938542582
At time: 409.5929365158081 and batch: 100, loss is 5.115613212585449 and perplexity is 166.6029126687218
At time: 410.43526244163513 and batch: 150, loss is 5.040004901885986 and perplexity is 154.4707722221486
At time: 411.2762348651886 and batch: 200, loss is 5.082492771148682 and perplexity is 161.17532878959867
At time: 412.1217768192291 and batch: 250, loss is 5.109826745986939 and perplexity is 165.64165430621858
At time: 412.98403549194336 and batch: 300, loss is 5.104346075057983 and perplexity is 164.7363101198276
At time: 413.8256757259369 and batch: 350, loss is 5.114699611663818 and perplexity is 166.45077360193915
At time: 414.6664946079254 and batch: 400, loss is 5.089789876937866 and perplexity is 162.35574378305824
At time: 415.5097997188568 and batch: 450, loss is 5.045989360809326 and perplexity is 155.39796782818357
At time: 416.35931730270386 and batch: 500, loss is 5.044542217254639 and perplexity is 155.17324730127854
At time: 417.21727418899536 and batch: 550, loss is 5.05168740272522 and perplexity is 156.28595946671737
At time: 418.07436537742615 and batch: 600, loss is 5.087882490158081 and perplexity is 162.04636373125155
At time: 418.9335505962372 and batch: 650, loss is 5.093980274200439 and perplexity is 163.0375062776022
At time: 419.78507566452026 and batch: 700, loss is 5.112264957427978 and perplexity is 166.046016442731
At time: 420.6383202075958 and batch: 750, loss is 5.073755617141724 and perplexity is 159.77324913209267
At time: 421.49029445648193 and batch: 800, loss is 5.089402952194214 and perplexity is 162.2929364801479
At time: 422.3312199115753 and batch: 850, loss is 5.113654451370239 and perplexity is 166.2768967429574
At time: 423.17524552345276 and batch: 900, loss is 5.100152912139893 and perplexity is 164.04699015842422
At time: 424.01910376548767 and batch: 950, loss is 5.080899419784546 and perplexity is 160.9187243444164
At time: 424.8795356750488 and batch: 1000, loss is 5.073195075988769 and perplexity is 159.68371474701095
At time: 425.7290997505188 and batch: 1050, loss is 5.078105812072754 and perplexity is 160.46980789529755
At time: 426.57440853118896 and batch: 1100, loss is 5.049749717712403 and perplexity is 155.98341971337814
At time: 427.4186682701111 and batch: 1150, loss is 5.077124738693238 and perplexity is 160.31245243980095
At time: 428.2632420063019 and batch: 1200, loss is 5.099097452163696 and perplexity is 163.87393646778364
At time: 429.10897612571716 and batch: 1250, loss is 5.118759508132935 and perplexity is 167.12792015230843
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.108205530765283 and perplexity of 165.3733310985218
Finished 19 epochs...
Completing Train Step...
At time: 431.42315101623535 and batch: 50, loss is 5.105846338272094 and perplexity is 164.9836434320725
At time: 432.3183557987213 and batch: 100, loss is 5.111802177429199 and perplexity is 165.96919144535156
At time: 433.16334867477417 and batch: 150, loss is 5.03639142036438 and perplexity is 153.91360220912767
At time: 434.006667137146 and batch: 200, loss is 5.079006500244141 and perplexity is 160.61440626237768
At time: 434.8482463359833 and batch: 250, loss is 5.106240739822388 and perplexity is 165.04872607031822
At time: 435.69231629371643 and batch: 300, loss is 5.100891027450562 and perplexity is 164.16812045209198
At time: 436.5354218482971 and batch: 350, loss is 5.111129055023193 and perplexity is 165.85751145525157
At time: 437.378577709198 and batch: 400, loss is 5.086470108032227 and perplexity is 161.81765389442032
At time: 438.2219183444977 and batch: 450, loss is 5.043085947036743 and perplexity is 154.94743758251028
At time: 439.0647163391113 and batch: 500, loss is 5.041875495910644 and perplexity is 154.75999475028274
At time: 439.9076838493347 and batch: 550, loss is 5.048952665328979 and perplexity is 155.8591422913082
At time: 440.7501428127289 and batch: 600, loss is 5.085221796035767 and perplexity is 161.615781002216
At time: 441.5928866863251 and batch: 650, loss is 5.0916197395324705 and perplexity is 162.65310446724914
At time: 442.4365406036377 and batch: 700, loss is 5.110064344406128 and perplexity is 165.68101517728883
At time: 443.2830560207367 and batch: 750, loss is 5.0714536380767825 and perplexity is 159.40587746062982
At time: 444.1287248134613 and batch: 800, loss is 5.087396469116211 and perplexity is 161.96762492462798
At time: 444.9725971221924 and batch: 850, loss is 5.111845283508301 and perplexity is 165.97634588064517
At time: 445.81516432762146 and batch: 900, loss is 5.097916889190674 and perplexity is 163.68058711914364
At time: 446.6570146083832 and batch: 950, loss is 5.078992700576782 and perplexity is 160.6121898522911
At time: 447.4996027946472 and batch: 1000, loss is 5.071879911422729 and perplexity is 159.47384242217518
At time: 448.34072732925415 and batch: 1050, loss is 5.0770011901855465 and perplexity is 160.2926472990107
At time: 449.18051862716675 and batch: 1100, loss is 5.048494520187378 and perplexity is 155.78775253717654
At time: 450.0251455307007 and batch: 1150, loss is 5.076065969467163 and perplexity is 160.14280837140814
At time: 450.8692276477814 and batch: 1200, loss is 5.097848978042602 and perplexity is 163.66947175998808
At time: 451.75697112083435 and batch: 1250, loss is 5.11658390045166 and perplexity is 166.76471060946935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.107797441691377 and perplexity of 165.30585781747587
Finished 20 epochs...
Completing Train Step...
At time: 454.09909105300903 and batch: 50, loss is 5.1020240974426265 and perplexity is 164.35423984626775
At time: 454.96356654167175 and batch: 100, loss is 5.10846697807312 and perplexity is 165.41657316323992
At time: 455.80706548690796 and batch: 150, loss is 5.033563861846924 and perplexity is 153.47901718908176
At time: 456.65091252326965 and batch: 200, loss is 5.076149110794067 and perplexity is 160.1561234104976
At time: 457.4948105812073 and batch: 250, loss is 5.10310622215271 and perplexity is 164.5321878940375
At time: 458.33797812461853 and batch: 300, loss is 5.097729511260987 and perplexity is 163.6499198628725
At time: 459.17992544174194 and batch: 350, loss is 5.108185710906983 and perplexity is 165.37005345501407
At time: 460.0464041233063 and batch: 400, loss is 5.083648014068603 and perplexity is 161.36163303966183
At time: 460.89153814315796 and batch: 450, loss is 5.040359106063843 and perplexity is 154.52549610616845
At time: 461.7354521751404 and batch: 500, loss is 5.039379816055298 and perplexity is 154.37424490324983
At time: 462.5925612449646 and batch: 550, loss is 5.046707973480225 and perplexity is 155.50967891058914
At time: 463.44341802597046 and batch: 600, loss is 5.082861108779907 and perplexity is 161.2347066632983
At time: 464.287230014801 and batch: 650, loss is 5.089193983078003 and perplexity is 162.25902581190894
At time: 465.1300456523895 and batch: 700, loss is 5.107927503585816 and perplexity is 165.3273592087309
At time: 465.9736042022705 and batch: 750, loss is 5.069289197921753 and perplexity is 159.06122610167867
At time: 466.81634759902954 and batch: 800, loss is 5.0853528881073 and perplexity is 161.63696893849493
At time: 467.6604835987091 and batch: 850, loss is 5.110120649337769 and perplexity is 165.6903440981522
At time: 468.50320291519165 and batch: 900, loss is 5.09590781211853 and perplexity is 163.3520703226912
At time: 469.34699296951294 and batch: 950, loss is 5.077129716873169 and perplexity is 160.3132505060209
At time: 470.1902163028717 and batch: 1000, loss is 5.070305204391479 and perplexity is 159.22291546128935
At time: 471.03396344184875 and batch: 1050, loss is 5.075983972549438 and perplexity is 160.1296776930708
At time: 471.87854266166687 and batch: 1100, loss is 5.047285814285278 and perplexity is 155.59956471598636
At time: 472.71710991859436 and batch: 1150, loss is 5.0750200557708744 and perplexity is 159.97540037718846
At time: 473.60902190208435 and batch: 1200, loss is 5.096533279418946 and perplexity is 163.45427366022412
At time: 474.4542510509491 and batch: 1250, loss is 5.114614591598511 and perplexity is 166.4366225478674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.107809470517792 and perplexity of 165.30784626490424
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 476.71993041038513 and batch: 50, loss is 5.101782760620117 and perplexity is 164.31457990215225
At time: 477.58738946914673 and batch: 100, loss is 5.111936893463135 and perplexity is 165.9915516626846
At time: 478.42818570137024 and batch: 150, loss is 5.034370574951172 and perplexity is 153.60288067790776
At time: 479.27166986465454 and batch: 200, loss is 5.0776892757415775 and perplexity is 160.40298030927724
At time: 480.1150028705597 and batch: 250, loss is 5.1026136016845705 and perplexity is 164.45115593125357
At time: 480.95720911026 and batch: 300, loss is 5.096630973815918 and perplexity is 163.47024300696702
At time: 481.7996015548706 and batch: 350, loss is 5.1049915790557865 and perplexity is 164.8426823947641
At time: 482.64352893829346 and batch: 400, loss is 5.0770334625244145 and perplexity is 160.29782040111596
At time: 483.48723793029785 and batch: 450, loss is 5.033613872528076 and perplexity is 153.486692971208
At time: 484.3300552368164 and batch: 500, loss is 5.029823207855225 and perplexity is 152.90597773014855
At time: 485.19700169563293 and batch: 550, loss is 5.03698205947876 and perplexity is 154.0045364548451
At time: 486.03999972343445 and batch: 600, loss is 5.072617044448853 and perplexity is 159.5914391951351
At time: 486.8819682598114 and batch: 650, loss is 5.078986883163452 and perplexity is 160.61125550751467
At time: 487.7253031730652 and batch: 700, loss is 5.097560071945191 and perplexity is 163.6221934814568
At time: 488.5671229362488 and batch: 750, loss is 5.0544802188873295 and perplexity is 156.72304748937313
At time: 489.40764141082764 and batch: 800, loss is 5.068567390441895 and perplexity is 158.94645594488483
At time: 490.2509677410126 and batch: 850, loss is 5.087584953308106 and perplexity is 161.9981561387603
At time: 491.0919842720032 and batch: 900, loss is 5.073722066879273 and perplexity is 159.76788878757264
At time: 491.93421936035156 and batch: 950, loss is 5.055490484237671 and perplexity is 156.88145935938988
At time: 492.77621030807495 and batch: 1000, loss is 5.046788082122803 and perplexity is 155.52213707887125
At time: 493.61700439453125 and batch: 1050, loss is 5.050294485092163 and perplexity is 156.0684175421416
At time: 494.5028131008148 and batch: 1100, loss is 5.018003568649292 and perplexity is 151.109323062508
At time: 495.34640645980835 and batch: 1150, loss is 5.043891286849975 and perplexity is 155.07227318374663
At time: 496.1883912086487 and batch: 1200, loss is 5.072976131439209 and perplexity is 159.6487566950906
At time: 497.02729296684265 and batch: 1250, loss is 5.0975277137756345 and perplexity is 163.61689905243642
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.100178293938185 and perplexity of 164.05115401888168
Finished 22 epochs...
Completing Train Step...
At time: 499.32746052742004 and batch: 50, loss is 5.093730955123902 and perplexity is 162.99686298388082
At time: 500.2015926837921 and batch: 100, loss is 5.102815456390381 and perplexity is 164.4843545214874
At time: 501.04452443122864 and batch: 150, loss is 5.026863613128662 and perplexity is 152.4541070117316
At time: 501.9015190601349 and batch: 200, loss is 5.069565677642823 and perplexity is 159.105209385067
At time: 502.75358867645264 and batch: 250, loss is 5.095444583892823 and perplexity is 163.2764185563595
At time: 503.59637236595154 and batch: 300, loss is 5.090132799148559 and perplexity is 162.4114287208904
At time: 504.43936681747437 and batch: 350, loss is 5.098924694061279 and perplexity is 163.84562836278147
At time: 505.28331565856934 and batch: 400, loss is 5.071765871047973 and perplexity is 159.45565700237734
At time: 506.1273629665375 and batch: 450, loss is 5.028628406524658 and perplexity is 152.72339456153185
At time: 506.97139024734497 and batch: 500, loss is 5.025553607940674 and perplexity is 152.25452209779505
At time: 507.81420731544495 and batch: 550, loss is 5.033022022247314 and perplexity is 153.39587870575488
At time: 508.6676914691925 and batch: 600, loss is 5.069295749664307 and perplexity is 159.06226823329627
At time: 509.5118217468262 and batch: 650, loss is 5.0759696197509765 and perplexity is 160.1273794005726
At time: 510.3664789199829 and batch: 700, loss is 5.094729423522949 and perplexity is 163.15969147673889
At time: 511.2193953990936 and batch: 750, loss is 5.052551221847534 and perplexity is 156.42102059284738
At time: 512.0654449462891 and batch: 800, loss is 5.067562704086304 and perplexity is 158.7868448023219
At time: 512.9117434024811 and batch: 850, loss is 5.088059902191162 and perplexity is 162.0751152564524
At time: 513.7504816055298 and batch: 900, loss is 5.074546022415161 and perplexity is 159.89958467232228
At time: 514.5952391624451 and batch: 950, loss is 5.0565102100372314 and perplexity is 157.04151702456707
At time: 515.44176197052 and batch: 1000, loss is 5.048280839920044 and perplexity is 155.75446732489013
At time: 516.3328166007996 and batch: 1050, loss is 5.052159185409546 and perplexity is 156.35970987191124
At time: 517.1792376041412 and batch: 1100, loss is 5.020632057189942 and perplexity is 151.5070346477544
At time: 518.0242993831635 and batch: 1150, loss is 5.0469631004333495 and perplexity is 155.5493586826254
At time: 518.8671276569366 and batch: 1200, loss is 5.076014785766602 and perplexity is 160.13461187962255
At time: 519.7118492126465 and batch: 1250, loss is 5.098099308013916 and perplexity is 163.71044826276807
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.099787134323677 and perplexity of 163.98699638147247
Finished 23 epochs...
Completing Train Step...
At time: 521.9763894081116 and batch: 50, loss is 5.090414390563965 and perplexity is 162.45716882469003
At time: 522.8196606636047 and batch: 100, loss is 5.099347982406616 and perplexity is 163.9149969881315
At time: 523.663092136383 and batch: 150, loss is 5.023526926040649 and perplexity is 151.94626309065055
At time: 524.5052874088287 and batch: 200, loss is 5.066069860458374 and perplexity is 158.549977719607
At time: 525.3475093841553 and batch: 250, loss is 5.092257862091064 and perplexity is 162.75693020568477
At time: 526.1903910636902 and batch: 300, loss is 5.086945009231568 and perplexity is 161.8945195426789
At time: 527.0312142372131 and batch: 350, loss is 5.095815296173096 and perplexity is 163.3369583505266
At time: 527.8727660179138 and batch: 400, loss is 5.069155340194702 and perplexity is 159.03993595244802
At time: 528.7189209461212 and batch: 450, loss is 5.026052942276001 and perplexity is 152.33056699272075
At time: 529.56556224823 and batch: 500, loss is 5.023213205337524 and perplexity is 151.8986018787019
At time: 530.412097454071 and batch: 550, loss is 5.031017570495606 and perplexity is 153.08871202122293
At time: 531.2574875354767 and batch: 600, loss is 5.067578859329224 and perplexity is 158.78941006309338
At time: 532.103821516037 and batch: 650, loss is 5.074399671554565 and perplexity is 159.8761849428236
At time: 532.9496667385101 and batch: 700, loss is 5.093351926803589 and perplexity is 162.9350942634744
At time: 533.7959604263306 and batch: 750, loss is 5.051559476852417 and perplexity is 156.26596772770344
At time: 534.6395461559296 and batch: 800, loss is 5.067072439193725 and perplexity is 158.70901626677028
At time: 535.4821281433105 and batch: 850, loss is 5.08833493232727 and perplexity is 162.11969692783165
At time: 536.3271753787994 and batch: 900, loss is 5.075133466720581 and perplexity is 159.99354436811916
At time: 537.1799421310425 and batch: 950, loss is 5.05702989578247 and perplexity is 157.1231504724078
At time: 538.0663027763367 and batch: 1000, loss is 5.049004859924317 and perplexity is 155.86727750847504
At time: 538.931925535202 and batch: 1050, loss is 5.052874336242676 and perplexity is 156.4715706425882
At time: 539.7768249511719 and batch: 1100, loss is 5.021837863922119 and perplexity is 151.68983303772097
At time: 540.619756937027 and batch: 1150, loss is 5.048434810638428 and perplexity is 155.7784507984446
At time: 541.4616785049438 and batch: 1200, loss is 5.0773228168487545 and perplexity is 160.34420997982025
At time: 542.3041388988495 and batch: 1250, loss is 5.0978925418853756 and perplexity is 163.6766019864315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.099614721145073 and perplexity of 163.9587252994003
Finished 24 epochs...
Completing Train Step...
At time: 544.5527653694153 and batch: 50, loss is 5.087944679260254 and perplexity is 162.05644156248474
At time: 545.4223453998566 and batch: 100, loss is 5.096842813491821 and perplexity is 163.50487615847373
At time: 546.2687184810638 and batch: 150, loss is 5.021138391494751 and perplexity is 151.58376728135948
At time: 547.1143839359283 and batch: 200, loss is 5.063643760681153 and perplexity is 158.1657858862885
At time: 547.959059715271 and batch: 250, loss is 5.090007638931274 and perplexity is 162.391102543223
At time: 548.8024270534515 and batch: 300, loss is 5.084714994430542 and perplexity is 161.53389461680297
At time: 549.6454005241394 and batch: 350, loss is 5.093574476242066 and perplexity is 162.9713594124555
At time: 550.4885125160217 and batch: 400, loss is 5.067144432067871 and perplexity is 158.72044259630638
At time: 551.3309071063995 and batch: 450, loss is 5.024042835235596 and perplexity is 152.02467378957795
At time: 552.1729919910431 and batch: 500, loss is 5.021563634872437 and perplexity is 151.64824098209613
At time: 553.0144934654236 and batch: 550, loss is 5.0297579574584965 and perplexity is 152.89600087993975
At time: 553.8563821315765 and batch: 600, loss is 5.066491470336914 and perplexity is 158.6168380499535
At time: 554.6993799209595 and batch: 650, loss is 5.0732872200012205 and perplexity is 159.6984293231305
At time: 555.5630824565887 and batch: 700, loss is 5.092430639266968 and perplexity is 162.7850533178904
At time: 556.4054906368256 and batch: 750, loss is 5.0508954715728756 and perplexity is 156.1622407415639
At time: 557.2489793300629 and batch: 800, loss is 5.066709308624268 and perplexity is 158.65139463403662
At time: 558.106593132019 and batch: 850, loss is 5.088492212295532 and perplexity is 162.1451971139007
At time: 558.956458568573 and batch: 900, loss is 5.075510530471802 and perplexity is 160.05388350926623
At time: 559.8462173938751 and batch: 950, loss is 5.057269191741943 and perplexity is 157.1607539064509
At time: 560.6879706382751 and batch: 1000, loss is 5.049502649307251 and perplexity is 155.9448858990795
At time: 561.530782699585 and batch: 1050, loss is 5.053257694244385 and perplexity is 156.53156677049714
At time: 562.3774700164795 and batch: 1100, loss is 5.022463188171387 and perplexity is 151.784718032539
At time: 563.2246017456055 and batch: 1150, loss is 5.049226150512696 and perplexity is 155.90177328667866
At time: 564.0881736278534 and batch: 1200, loss is 5.077979850769043 and perplexity is 160.4495961820624
At time: 564.9614396095276 and batch: 1250, loss is 5.097592430114746 and perplexity is 163.62748808179782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.099476166885265 and perplexity of 163.93600969328565
Finished 25 epochs...
Completing Train Step...
At time: 567.2773838043213 and batch: 50, loss is 5.086157636642456 and perplexity is 161.76709840620165
At time: 568.1189985275269 and batch: 100, loss is 5.094991044998169 and perplexity is 163.20238314020392
At time: 568.9565913677216 and batch: 150, loss is 5.0193625831604 and perplexity is 151.31482243199778
At time: 569.7973785400391 and batch: 200, loss is 5.061803979873657 and perplexity is 157.87506302418652
At time: 570.6408219337463 and batch: 250, loss is 5.088317604064941 and perplexity is 162.11688769953417
At time: 571.4838981628418 and batch: 300, loss is 5.082995262145996 and perplexity is 161.25633829287304
At time: 572.3272438049316 and batch: 350, loss is 5.091886377334594 and perplexity is 162.69647971601643
At time: 573.1743297576904 and batch: 400, loss is 5.065729608535767 and perplexity is 158.49603996159195
At time: 574.0262968540192 and batch: 450, loss is 5.022492504119873 and perplexity is 151.78916781073815
At time: 574.8696246147156 and batch: 500, loss is 5.020211563110352 and perplexity is 151.4433402291703
At time: 575.7114169597626 and batch: 550, loss is 5.0288416767120365 and perplexity is 152.75596938200266
At time: 576.5535140037537 and batch: 600, loss is 5.065799589157105 and perplexity is 158.5071320010574
At time: 577.3966417312622 and batch: 650, loss is 5.072407112121582 and perplexity is 159.55793930936997
At time: 578.2386691570282 and batch: 700, loss is 5.091739978790283 and perplexity is 162.67266293163524
At time: 579.0817296504974 and batch: 750, loss is 5.05041913986206 and perplexity is 156.08787342742784
At time: 579.924506187439 and batch: 800, loss is 5.066298990249634 and perplexity is 158.58631040519762
At time: 580.8120441436768 and batch: 850, loss is 5.088479747772217 and perplexity is 162.14317606390665
At time: 581.656275510788 and batch: 900, loss is 5.075763063430786 and perplexity is 160.09430749404515
At time: 582.5023875236511 and batch: 950, loss is 5.057308464050293 and perplexity is 157.1669260932361
At time: 583.3547961711884 and batch: 1000, loss is 5.049643983840943 and perplexity is 155.9669278544177
At time: 584.2139494419098 and batch: 1050, loss is 5.053391351699829 and perplexity is 156.55248977963527
At time: 585.0582144260406 and batch: 1100, loss is 5.022798099517822 and perplexity is 151.8355609702867
At time: 585.9013040065765 and batch: 1150, loss is 5.049692735671997 and perplexity is 155.97453171308408
At time: 586.7448856830597 and batch: 1200, loss is 5.078346681594849 and perplexity is 160.50846483668857
At time: 587.5856695175171 and batch: 1250, loss is 5.09716064453125 and perplexity is 163.55685134244075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.09931979214188 and perplexity of 163.91037624610235
Finished 26 epochs...
Completing Train Step...
At time: 589.8403890132904 and batch: 50, loss is 5.084531021118164 and perplexity is 161.50417942463395
At time: 590.7100207805634 and batch: 100, loss is 5.093457384109497 and perplexity is 162.95227786560562
At time: 591.5523266792297 and batch: 150, loss is 5.017892723083496 and perplexity is 151.0925741923822
At time: 592.3926589488983 and batch: 200, loss is 5.060366983413696 and perplexity is 157.64836004220697
At time: 593.2518882751465 and batch: 250, loss is 5.086935081481934 and perplexity is 161.89291230239994
At time: 594.1172449588776 and batch: 300, loss is 5.081639451980591 and perplexity is 161.03785345562983
At time: 594.978104352951 and batch: 350, loss is 5.090540285110474 and perplexity is 162.4776225837676
At time: 595.8299918174744 and batch: 400, loss is 5.064380617141723 and perplexity is 158.28237431665391
At time: 596.6745147705078 and batch: 450, loss is 5.02105034828186 and perplexity is 151.57042194695762
At time: 597.5190165042877 and batch: 500, loss is 5.019118432998657 and perplexity is 151.27788340314535
At time: 598.3706102371216 and batch: 550, loss is 5.028053522109985 and perplexity is 152.63562149434406
At time: 599.229257106781 and batch: 600, loss is 5.0651160335540775 and perplexity is 158.39882058551316
At time: 600.0949032306671 and batch: 650, loss is 5.071587467193604 and perplexity is 159.42721203598836
At time: 600.9391620159149 and batch: 700, loss is 5.091079063415528 and perplexity is 162.5651855882779
At time: 601.7803888320923 and batch: 750, loss is 5.049886398315429 and perplexity is 156.00474107832204
At time: 602.6689994335175 and batch: 800, loss is 5.0658448123931885 and perplexity is 158.51430036859605
At time: 603.5234282016754 and batch: 850, loss is 5.088383302688599 and perplexity is 162.12753890580868
At time: 604.367171049118 and batch: 900, loss is 5.075858564376831 and perplexity is 160.10959738195496
At time: 605.2155816555023 and batch: 950, loss is 5.057289562225342 and perplexity is 157.16395537958698
At time: 606.0586862564087 and batch: 1000, loss is 5.0496543025970455 and perplexity is 155.96853724740978
At time: 606.9036796092987 and batch: 1050, loss is 5.0533043479919435 and perplexity is 156.53886972505197
At time: 607.7518317699432 and batch: 1100, loss is 5.022858028411865 and perplexity is 151.84466058019407
At time: 608.6083261966705 and batch: 1150, loss is 5.049923515319824 and perplexity is 156.01053161444523
At time: 609.474969625473 and batch: 1200, loss is 5.07850004196167 and perplexity is 160.53308236136183
At time: 610.321578502655 and batch: 1250, loss is 5.096669797897339 and perplexity is 163.47658971219315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.099288160783531 and perplexity of 163.90519162025308
Finished 27 epochs...
Completing Train Step...
At time: 612.6419568061829 and batch: 50, loss is 5.08312762260437 and perplexity is 161.2776836683359
At time: 613.5162608623505 and batch: 100, loss is 5.092036905288697 and perplexity is 162.72097192758272
At time: 614.3625078201294 and batch: 150, loss is 5.01658579826355 and perplexity is 150.89523653788845
At time: 615.2050549983978 and batch: 200, loss is 5.059027528762817 and perplexity is 157.43733857154027
At time: 616.0464262962341 and batch: 250, loss is 5.085762672424316 and perplexity is 161.70321880656735
At time: 616.885739326477 and batch: 300, loss is 5.080417642593384 and perplexity is 160.84121604576066
At time: 617.7262196540833 and batch: 350, loss is 5.08931791305542 and perplexity is 162.27913581540395
At time: 618.5713098049164 and batch: 400, loss is 5.063214406967163 and perplexity is 158.09789139511116
At time: 619.4169366359711 and batch: 450, loss is 5.019775400161743 and perplexity is 151.37730065840265
At time: 620.2602572441101 and batch: 500, loss is 5.018019847869873 and perplexity is 151.111783024533
At time: 621.1015419960022 and batch: 550, loss is 5.027331228256226 and perplexity is 152.52541352913326
At time: 621.9442985057831 and batch: 600, loss is 5.064488401412964 and perplexity is 158.29943558647162
At time: 622.788369178772 and batch: 650, loss is 5.070932054519654 and perplexity is 159.32275565540453
At time: 623.6338150501251 and batch: 700, loss is 5.090541105270386 and perplexity is 162.47775584145498
At time: 624.5071551799774 and batch: 750, loss is 5.049372797012329 and perplexity is 155.92463741244882
At time: 625.3539752960205 and batch: 800, loss is 5.065339822769165 and perplexity is 158.43427249997285
At time: 626.200690984726 and batch: 850, loss is 5.08825270652771 and perplexity is 162.10636705416456
At time: 627.0467274188995 and batch: 900, loss is 5.075883855819702 and perplexity is 160.11364683589838
At time: 627.8938145637512 and batch: 950, loss is 5.057172946929931 and perplexity is 157.1456287271073
At time: 628.7415368556976 and batch: 1000, loss is 5.049509677886963 and perplexity is 155.94598197399264
At time: 629.5986731052399 and batch: 1050, loss is 5.053073682785034 and perplexity is 156.50276581839512
At time: 630.4538104534149 and batch: 1100, loss is 5.022847509384155 and perplexity is 151.84306333040254
At time: 631.302218914032 and batch: 1150, loss is 5.050130062103271 and perplexity is 156.04275841599062
At time: 632.1507439613342 and batch: 1200, loss is 5.078620309829712 and perplexity is 160.5523904939784
At time: 632.9996385574341 and batch: 1250, loss is 5.096200103759766 and perplexity is 163.3998237460483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.099256529425182 and perplexity of 163.90000715839795
Finished 28 epochs...
Completing Train Step...
At time: 635.2869746685028 and batch: 50, loss is 5.081772089004517 and perplexity is 161.05921445384988
At time: 636.1288251876831 and batch: 100, loss is 5.0909131813049315 and perplexity is 162.53822116870234
At time: 636.97185754776 and batch: 150, loss is 5.015473375320434 and perplexity is 150.72747054542708
At time: 637.8150532245636 and batch: 200, loss is 5.0578665447235105 and perplexity is 157.25466239684823
At time: 638.6576292514801 and batch: 250, loss is 5.0846961307525635 and perplexity is 161.53084752217205
At time: 639.4998970031738 and batch: 300, loss is 5.079291572570801 and perplexity is 160.66019951175
At time: 640.3406732082367 and batch: 350, loss is 5.088167066574097 and perplexity is 162.09248486685308
At time: 641.1788609027863 and batch: 400, loss is 5.062133159637451 and perplexity is 157.9270408546974
At time: 642.0243139266968 and batch: 450, loss is 5.018625211715698 and perplexity is 151.2032883288613
At time: 642.8832459449768 and batch: 500, loss is 5.017101936340332 and perplexity is 150.97313941766254
At time: 643.7505602836609 and batch: 550, loss is 5.0266822338104244 and perplexity is 152.42645749734012
At time: 644.5929834842682 and batch: 600, loss is 5.06382339477539 and perplexity is 158.1942004060074
At time: 645.4352080821991 and batch: 650, loss is 5.070231370925903 and perplexity is 159.21115991562337
At time: 646.3050570487976 and batch: 700, loss is 5.089845342636108 and perplexity is 162.36474920749458
At time: 647.1502192020416 and batch: 750, loss is 5.048772764205933 and perplexity is 155.83110557856656
At time: 647.9951949119568 and batch: 800, loss is 5.06485948562622 and perplexity is 158.35818890852607
At time: 648.8485074043274 and batch: 850, loss is 5.088108530044556 and perplexity is 162.0829968130259
At time: 649.738053560257 and batch: 900, loss is 5.075846281051636 and perplexity is 160.10763071578208
At time: 650.5897777080536 and batch: 950, loss is 5.056973304748535 and perplexity is 157.11425896245962
At time: 651.4325051307678 and batch: 1000, loss is 5.049314231872558 and perplexity is 155.9155059316609
At time: 652.2751133441925 and batch: 1050, loss is 5.052840662002564 and perplexity is 156.4663016700625
At time: 653.1179840564728 and batch: 1100, loss is 5.022673873901367 and perplexity is 151.81670027564036
At time: 653.9617893695831 and batch: 1150, loss is 5.050075044631958 and perplexity is 156.0341735741662
At time: 654.8046820163727 and batch: 1200, loss is 5.078513145446777 and perplexity is 160.53518591799778
At time: 655.651665687561 and batch: 1250, loss is 5.095646381378174 and perplexity is 163.30937065175448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.099301080634124 and perplexity of 163.90730926452
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 657.9346504211426 and batch: 50, loss is 5.081839456558227 and perplexity is 161.07006498461294
At time: 658.8029539585114 and batch: 100, loss is 5.091691722869873 and perplexity is 162.66481320195945
At time: 659.6464819908142 and batch: 150, loss is 5.0166504764556885 and perplexity is 150.90499648461454
At time: 660.4912509918213 and batch: 200, loss is 5.057852783203125 and perplexity is 157.2524983484963
At time: 661.3381524085999 and batch: 250, loss is 5.08545654296875 and perplexity is 161.6537242644859
At time: 662.1849436759949 and batch: 300, loss is 5.079108934402466 and perplexity is 160.63085950657083
At time: 663.0274848937988 and batch: 350, loss is 5.086267433166504 and perplexity is 161.7848608464212
At time: 663.870630979538 and batch: 400, loss is 5.059921541213989 and perplexity is 157.5781524478031
At time: 664.724958896637 and batch: 450, loss is 5.015554933547974 and perplexity is 150.73976411208022
At time: 665.5683991909027 and batch: 500, loss is 5.014639768600464 and perplexity is 150.60187546881923
At time: 666.4110980033875 and batch: 550, loss is 5.02332293510437 and perplexity is 151.91527059138056
At time: 667.2809071540833 and batch: 600, loss is 5.059183282852173 and perplexity is 157.46186199060188
At time: 668.1253197193146 and batch: 650, loss is 5.065416030883789 and perplexity is 158.44634693725112
At time: 668.9692604541779 and batch: 700, loss is 5.085529689788818 and perplexity is 161.6655491528393
At time: 669.8121726512909 and batch: 750, loss is 5.043540811538696 and perplexity is 155.01793370341113
At time: 670.6552565097809 and batch: 800, loss is 5.058402261734009 and perplexity is 157.33892896397577
At time: 671.4996263980865 and batch: 850, loss is 5.080518321990967 and perplexity is 160.85741025769678
At time: 672.3444628715515 and batch: 900, loss is 5.067592439651489 and perplexity is 158.79156648909674
At time: 673.1953313350677 and batch: 950, loss is 5.049085187911987 and perplexity is 155.8797985161079
At time: 674.0570662021637 and batch: 1000, loss is 5.04115852355957 and perplexity is 154.6490758806273
At time: 674.9035131931305 and batch: 1050, loss is 5.043248462677002 and perplexity is 154.97262101083055
At time: 675.751519203186 and batch: 1100, loss is 5.012430028915405 and perplexity is 150.26945194794195
At time: 676.6022801399231 and batch: 1150, loss is 5.039029855728149 and perplexity is 154.32022949417646
At time: 677.4524252414703 and batch: 1200, loss is 5.069874353408814 and perplexity is 159.15432888805722
At time: 678.2992448806763 and batch: 1250, loss is 5.09035756111145 and perplexity is 162.4479367350583
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098296005360401 and perplexity of 163.74265284070697
Finished 30 epochs...
Completing Train Step...
At time: 680.6294085979462 and batch: 50, loss is 5.080213527679444 and perplexity is 160.80838930511715
At time: 681.4760746955872 and batch: 100, loss is 5.089509153366089 and perplexity is 162.31017309544734
At time: 682.3220057487488 and batch: 150, loss is 5.0145771694183345 and perplexity is 150.59244820966015
At time: 683.1832475662231 and batch: 200, loss is 5.0557059383392335 and perplexity is 156.915263754785
At time: 684.0497524738312 and batch: 250, loss is 5.083595399856567 and perplexity is 161.35314334782777
At time: 684.8953022956848 and batch: 300, loss is 5.07746223449707 and perplexity is 160.36656635089682
At time: 685.7424607276917 and batch: 350, loss is 5.084825210571289 and perplexity is 161.5516992404278
At time: 686.5916650295258 and batch: 400, loss is 5.058510246276856 and perplexity is 157.35592005366303
At time: 687.4386742115021 and batch: 450, loss is 5.014295444488526 and perplexity is 150.55002853837857
At time: 688.2844972610474 and batch: 500, loss is 5.013692445755005 and perplexity is 150.4592744268955
At time: 689.1533892154694 and batch: 550, loss is 5.022311973571777 and perplexity is 151.76176770243103
At time: 690.0152785778046 and batch: 600, loss is 5.058230276107788 and perplexity is 157.3118712565841
At time: 690.8817398548126 and batch: 650, loss is 5.064944295883179 and perplexity is 158.3716198767528
At time: 691.7560257911682 and batch: 700, loss is 5.085057544708252 and perplexity is 161.58923757559523
At time: 692.6039452552795 and batch: 750, loss is 5.043346700668335 and perplexity is 154.98784595765207
At time: 693.4509973526001 and batch: 800, loss is 5.058290042877197 and perplexity is 157.32127355988868
At time: 694.2973024845123 and batch: 850, loss is 5.080651006698608 and perplexity is 160.8787549921774
At time: 695.144205570221 and batch: 900, loss is 5.067627019882202 and perplexity is 158.79705763304327
At time: 695.9926340579987 and batch: 950, loss is 5.049385738372803 and perplexity is 155.92665530244545
At time: 696.8649151325226 and batch: 1000, loss is 5.041611633300781 and perplexity is 154.7191647611562
At time: 697.7314760684967 and batch: 1050, loss is 5.043961915969849 and perplexity is 155.08322618871438
At time: 698.5838832855225 and batch: 1100, loss is 5.013298282623291 and perplexity is 150.39998061457698
At time: 699.4249992370605 and batch: 1150, loss is 5.039940958023071 and perplexity is 154.46089508005997
At time: 700.2943966388702 and batch: 1200, loss is 5.071001214981079 and perplexity is 159.33377487174388
At time: 701.1436550617218 and batch: 1250, loss is 5.090747966766357 and perplexity is 162.511369709678
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098316498916515 and perplexity of 163.74600854433615
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 703.6713757514954 and batch: 50, loss is 5.07976767539978 and perplexity is 160.73670849886926
At time: 704.5745685100555 and batch: 100, loss is 5.089085664749145 and perplexity is 162.24145113723782
At time: 705.4184257984161 and batch: 150, loss is 5.014291133880615 and perplexity is 150.54937957763332
At time: 706.2640450000763 and batch: 200, loss is 5.055219125747681 and perplexity is 156.83889401896508
At time: 707.10719871521 and batch: 250, loss is 5.083363761901856 and perplexity is 161.31577216417577
At time: 707.9524245262146 and batch: 300, loss is 5.0773172855377195 and perplexity is 160.34332306857507
At time: 708.797910451889 and batch: 350, loss is 5.083911762237549 and perplexity is 161.40419748781503
At time: 709.6432802677155 and batch: 400, loss is 5.057501668930054 and perplexity is 157.19729444385658
At time: 710.4874362945557 and batch: 450, loss is 5.013175973892212 and perplexity is 150.3815865086965
At time: 711.381954908371 and batch: 500, loss is 5.012853507995605 and perplexity is 150.33310139338812
At time: 712.2235684394836 and batch: 550, loss is 5.020681238174438 and perplexity is 151.5144860961098
At time: 713.0626623630524 and batch: 600, loss is 5.056243476867675 and perplexity is 156.9996344289663
At time: 713.9063851833344 and batch: 650, loss is 5.0634321689605715 and perplexity is 158.13232285589066
At time: 714.7508721351624 and batch: 700, loss is 5.083792476654053 and perplexity is 161.38494544220754
At time: 715.5936806201935 and batch: 750, loss is 5.042000102996826 and perplexity is 154.77928014380947
At time: 716.437251329422 and batch: 800, loss is 5.056906547546387 and perplexity is 157.1037708041976
At time: 717.2814977169037 and batch: 850, loss is 5.078219785690307 and perplexity is 160.48809826210132
At time: 718.124104976654 and batch: 900, loss is 5.064823093414307 and perplexity is 158.35242600862009
At time: 718.9693334102631 and batch: 950, loss is 5.04708122253418 and perplexity is 155.5677335848755
At time: 719.8126890659332 and batch: 1000, loss is 5.038773889541626 and perplexity is 154.28073378852773
At time: 720.655394077301 and batch: 1050, loss is 5.041488494873047 and perplexity is 154.7001140594279
At time: 721.4987173080444 and batch: 1100, loss is 5.010620746612549 and perplexity is 149.9978178933161
At time: 722.341735124588 and batch: 1150, loss is 5.036796751022339 and perplexity is 153.97600075594772
At time: 723.1830344200134 and batch: 1200, loss is 5.068628053665162 and perplexity is 158.9560984416988
At time: 724.0257339477539 and batch: 1250, loss is 5.089830951690674 and perplexity is 162.362412642061
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098777603929061 and perplexity of 163.82153005999692
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 726.3140392303467 and batch: 50, loss is 5.079883136749268 and perplexity is 160.75526844760248
At time: 727.2019956111908 and batch: 100, loss is 5.088519515991211 and perplexity is 162.1496243374581
At time: 728.0663955211639 and batch: 150, loss is 5.0138849258422855 and perplexity is 150.4882376284997
At time: 728.9200534820557 and batch: 200, loss is 5.054931421279907 and perplexity is 156.79377725889302
At time: 729.7816705703735 and batch: 250, loss is 5.082807569503784 and perplexity is 161.22607450489951
At time: 730.6270184516907 and batch: 300, loss is 5.076761722564697 and perplexity is 160.25426699572571
At time: 731.4696192741394 and batch: 350, loss is 5.083335762023926 and perplexity is 161.31125540548163
At time: 732.3593881130219 and batch: 400, loss is 5.0568451404571535 and perplexity is 157.09412381512476
At time: 733.2035052776337 and batch: 450, loss is 5.012752351760864 and perplexity is 150.31789503201622
At time: 734.047040939331 and batch: 500, loss is 5.0125942802429195 and perplexity is 150.2941359320423
At time: 734.8900096416473 and batch: 550, loss is 5.020159788131714 and perplexity is 151.43549945644477
At time: 735.7342302799225 and batch: 600, loss is 5.0556884860992435 and perplexity is 156.9125252558404
At time: 736.5746700763702 and batch: 650, loss is 5.063212718963623 and perplexity is 158.09762452553596
At time: 737.4123182296753 and batch: 700, loss is 5.083698091506958 and perplexity is 161.3697138192238
At time: 738.2566981315613 and batch: 750, loss is 5.041831464767456 and perplexity is 154.75318064081168
At time: 739.1018958091736 and batch: 800, loss is 5.056509160995484 and perplexity is 157.041352281546
At time: 739.9671857357025 and batch: 850, loss is 5.077441596984864 and perplexity is 160.3632568180766
At time: 740.8397483825684 and batch: 900, loss is 5.0641289329528805 and perplexity is 158.24254215843837
At time: 741.6874399185181 and batch: 950, loss is 5.046440515518189 and perplexity is 155.4680921703911
At time: 742.5363273620605 and batch: 1000, loss is 5.037950639724731 and perplexity is 154.15377446947446
At time: 743.3797459602356 and batch: 1050, loss is 5.040804462432861 and perplexity is 154.59433034678113
At time: 744.2347214221954 and batch: 1100, loss is 5.01002423286438 and perplexity is 149.90836881420248
At time: 745.0937566757202 and batch: 1150, loss is 5.036051521301269 and perplexity is 153.86129600985447
At time: 745.9362633228302 and batch: 1200, loss is 5.0677978038787845 and perplexity is 158.82417994515382
At time: 746.7786140441895 and batch: 1250, loss is 5.089232320785523 and perplexity is 162.265246570227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098367287294708 and perplexity of 163.75432514973733
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 749.1041464805603 and batch: 50, loss is 5.079762697219849 and perplexity is 160.73590832460448
At time: 749.9491291046143 and batch: 100, loss is 5.088361473083496 and perplexity is 162.12399976428722
At time: 750.7952613830566 and batch: 150, loss is 5.013777484893799 and perplexity is 150.4720698980665
At time: 751.6402051448822 and batch: 200, loss is 5.054815750122071 and perplexity is 156.77564179003156
At time: 752.4991374015808 and batch: 250, loss is 5.082621812820435 and perplexity is 161.19612846545454
At time: 753.3561758995056 and batch: 300, loss is 5.076526947021485 and perplexity is 160.21664762936604
At time: 754.2485284805298 and batch: 350, loss is 5.083169069290161 and perplexity is 161.28436823234154
At time: 755.1020038127899 and batch: 400, loss is 5.0566737174987795 and perplexity is 157.06719658371597
At time: 755.9506406784058 and batch: 450, loss is 5.012688522338867 and perplexity is 150.30830063386628
At time: 756.809460401535 and batch: 500, loss is 5.01254150390625 and perplexity is 150.28620416743144
At time: 757.6671984195709 and batch: 550, loss is 5.020044603347778 and perplexity is 151.41805739570896
At time: 758.5253763198853 and batch: 600, loss is 5.055542068481445 and perplexity is 156.88955217956234
At time: 759.3900284767151 and batch: 650, loss is 5.063170804977417 and perplexity is 158.090998162752
At time: 760.2440996170044 and batch: 700, loss is 5.083799285888672 and perplexity is 161.38604435390639
At time: 761.0857141017914 and batch: 750, loss is 5.041877994537353 and perplexity is 154.76038143822223
At time: 761.9293072223663 and batch: 800, loss is 5.056354160308838 and perplexity is 157.01701265048908
At time: 762.782791852951 and batch: 850, loss is 5.077198934555054 and perplexity is 160.3243474016426
At time: 763.6408989429474 and batch: 900, loss is 5.063802680969238 and perplexity is 158.19092363594314
At time: 764.4873132705688 and batch: 950, loss is 5.046181297302246 and perplexity is 155.42779723171932
At time: 765.3313829898834 and batch: 1000, loss is 5.037689199447632 and perplexity is 154.11347773178593
At time: 766.2029831409454 and batch: 1050, loss is 5.0406398010253906 and perplexity is 154.56887672243153
At time: 767.0736744403839 and batch: 1100, loss is 5.009837646484375 and perplexity is 149.8804005636614
At time: 767.928674697876 and batch: 1150, loss is 5.035840158462524 and perplexity is 153.8287788861337
At time: 768.7836427688599 and batch: 1200, loss is 5.067558097839355 and perplexity is 158.78611339258714
At time: 769.6318821907043 and batch: 1250, loss is 5.08904052734375 and perplexity is 162.23412814435756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098310707259352 and perplexity of 163.74506018633917
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 771.938672542572 and batch: 50, loss is 5.079705038070679 and perplexity is 160.7266406960738
At time: 772.8085968494415 and batch: 100, loss is 5.0883141613006595 and perplexity is 162.1163295702645
At time: 773.6528844833374 and batch: 150, loss is 5.013673601150512 and perplexity is 150.45643910809196
At time: 774.4977931976318 and batch: 200, loss is 5.0547295093536375 and perplexity is 156.7621219212023
At time: 775.340897321701 and batch: 250, loss is 5.082569093704223 and perplexity is 161.18763057202787
At time: 776.2103970050812 and batch: 300, loss is 5.076505508422851 and perplexity is 160.21321284578175
At time: 777.053512096405 and batch: 350, loss is 5.083096494674683 and perplexity is 161.27266350607258
At time: 777.8975656032562 and batch: 400, loss is 5.056629648208618 and perplexity is 157.060274896373
At time: 778.7419805526733 and batch: 450, loss is 5.012668981552124 and perplexity is 150.30536352011467
At time: 779.5870358943939 and batch: 500, loss is 5.012547826766967 and perplexity is 150.2871544091723
At time: 780.4305362701416 and batch: 550, loss is 5.020006265640259 and perplexity is 151.4122524857855
At time: 781.2746572494507 and batch: 600, loss is 5.055478830337524 and perplexity is 156.87963108918083
At time: 782.1196320056915 and batch: 650, loss is 5.0631166934967045 and perplexity is 158.08244385619932
At time: 782.9633905887604 and batch: 700, loss is 5.083761472702026 and perplexity is 161.37994194866562
At time: 783.8076639175415 and batch: 750, loss is 5.041840324401855 and perplexity is 154.75455170348786
At time: 784.6607570648193 and batch: 800, loss is 5.056300001144409 and perplexity is 157.00850897056102
At time: 785.5172305107117 and batch: 850, loss is 5.077140054702759 and perplexity is 160.314907805652
At time: 786.3612303733826 and batch: 900, loss is 5.063710641860962 and perplexity is 158.176364554407
At time: 787.2045121192932 and batch: 950, loss is 5.046106939315796 and perplexity is 155.41624036335688
At time: 788.047604560852 and batch: 1000, loss is 5.037614126205444 and perplexity is 154.10190836762797
At time: 788.8990049362183 and batch: 1050, loss is 5.040600681304932 and perplexity is 154.56283014945345
At time: 789.7453985214233 and batch: 1100, loss is 5.009765453338623 and perplexity is 149.86958061662594
At time: 790.5893950462341 and batch: 1150, loss is 5.0357576179504395 and perplexity is 153.81608230394946
At time: 791.4323194026947 and batch: 1200, loss is 5.067480726242065 and perplexity is 158.77382833262996
At time: 792.2768077850342 and batch: 1250, loss is 5.088988304138184 and perplexity is 162.22565597935736
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098288431654882 and perplexity of 163.74141270676955
Finished 35 epochs...
Completing Train Step...
At time: 794.5933887958527 and batch: 50, loss is 5.0796875 and perplexity is 160.7238218856077
At time: 795.4383797645569 and batch: 100, loss is 5.088295412063599 and perplexity is 162.1132900412645
At time: 796.2859528064728 and batch: 150, loss is 5.013642492294312 and perplexity is 150.45175865316548
At time: 797.1575033664703 and batch: 200, loss is 5.054698705673218 and perplexity is 156.75729314526913
At time: 798.0603427886963 and batch: 250, loss is 5.082551345825196 and perplexity is 161.18476985884558
At time: 798.9051225185394 and batch: 300, loss is 5.0764967918396 and perplexity is 160.21181634006038
At time: 799.7471528053284 and batch: 350, loss is 5.083078012466431 and perplexity is 161.26968285866488
At time: 800.5906352996826 and batch: 400, loss is 5.05662109375 and perplexity is 157.05893133649744
At time: 801.4342534542084 and batch: 450, loss is 5.012657499313354 and perplexity is 150.3036376879506
At time: 802.2756059169769 and batch: 500, loss is 5.01253945350647 and perplexity is 150.28589602094743
At time: 803.1197154521942 and batch: 550, loss is 5.019996166229248 and perplexity is 151.41072331893747
At time: 803.9663994312286 and batch: 600, loss is 5.055473079681397 and perplexity is 156.87872893096306
At time: 804.811817407608 and batch: 650, loss is 5.06311107635498 and perplexity is 158.08155588720197
At time: 805.6583819389343 and batch: 700, loss is 5.083747825622559 and perplexity is 161.37773959880124
At time: 806.5023884773254 and batch: 750, loss is 5.041827430725098 and perplexity is 154.75255636118513
At time: 807.3461015224457 and batch: 800, loss is 5.056299104690551 and perplexity is 157.00836821974053
At time: 808.1914074420929 and batch: 850, loss is 5.07714017868042 and perplexity is 160.31492768112054
At time: 809.0639200210571 and batch: 900, loss is 5.063711175918579 and perplexity is 158.17644902972185
At time: 809.9043924808502 and batch: 950, loss is 5.046107015609741 and perplexity is 155.4162522206755
At time: 810.7467143535614 and batch: 1000, loss is 5.037617025375366 and perplexity is 154.10235513589325
At time: 811.589928150177 and batch: 1050, loss is 5.040610237121582 and perplexity is 154.56430713057614
At time: 812.4320032596588 and batch: 1100, loss is 5.00979208946228 and perplexity is 149.87357261447312
At time: 813.2737340927124 and batch: 1150, loss is 5.035781354904175 and perplexity is 153.81973347251244
At time: 814.1178514957428 and batch: 1200, loss is 5.067496089935303 and perplexity is 158.77626770376145
At time: 814.9876742362976 and batch: 1250, loss is 5.088992595672607 and perplexity is 162.2263521778383
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098269274635037 and perplexity of 163.7382759393225
Finished 36 epochs...
Completing Train Step...
At time: 817.2831037044525 and batch: 50, loss is 5.07967095375061 and perplexity is 160.72116253116906
At time: 818.1552889347076 and batch: 100, loss is 5.0882771205902095 and perplexity is 162.11032477745326
At time: 818.9994766712189 and batch: 150, loss is 5.013617134094238 and perplexity is 150.4479435157408
At time: 819.869745016098 and batch: 200, loss is 5.0546722507476805 and perplexity is 156.7531461976054
At time: 820.7293064594269 and batch: 250, loss is 5.082533826828003 and perplexity is 161.18194608804984
At time: 821.5793340206146 and batch: 300, loss is 5.076485328674316 and perplexity is 160.20997981605547
At time: 822.4262461662292 and batch: 350, loss is 5.083061971664429 and perplexity is 161.26709598436102
At time: 823.2699899673462 and batch: 400, loss is 5.056612577438354 and perplexity is 157.05759377938705
At time: 824.1136763095856 and batch: 450, loss is 5.012646942138672 and perplexity is 150.30205091456816
At time: 824.9575600624084 and batch: 500, loss is 5.012530755996704 and perplexity is 150.28458891358343
At time: 825.8016629219055 and batch: 550, loss is 5.019987287521363 and perplexity is 151.40937899332235
At time: 826.6442968845367 and batch: 600, loss is 5.055469102859497 and perplexity is 156.87810505343884
At time: 827.4875540733337 and batch: 650, loss is 5.063109121322632 and perplexity is 158.08124683294864
At time: 828.3300755023956 and batch: 700, loss is 5.083740835189819 and perplexity is 161.37661150250992
At time: 829.1730270385742 and batch: 750, loss is 5.041821022033691 and perplexity is 154.75156460298493
At time: 830.0171091556549 and batch: 800, loss is 5.056299552917481 and perplexity is 157.00843859513506
At time: 830.8611760139465 and batch: 850, loss is 5.077140579223633 and perplexity is 160.3149918941896
At time: 831.7044386863708 and batch: 900, loss is 5.063711004257202 and perplexity is 158.17642187693718
At time: 832.546642780304 and batch: 950, loss is 5.046108074188233 and perplexity is 155.4164167410644
At time: 833.3991854190826 and batch: 1000, loss is 5.037619752883911 and perplexity is 154.10277545195694
At time: 834.2566130161285 and batch: 1050, loss is 5.040621376037597 and perplexity is 154.56602881900108
At time: 835.0997607707977 and batch: 1100, loss is 5.009815196990967 and perplexity is 149.87703586236512
At time: 835.9426836967468 and batch: 1150, loss is 5.035801267623901 and perplexity is 153.8227964722497
At time: 836.7856879234314 and batch: 1200, loss is 5.067509555816651 and perplexity is 158.77840578053872
At time: 837.629492521286 and batch: 1250, loss is 5.0889966011047365 and perplexity is 162.22700196578285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098253681711907 and perplexity of 163.73572280087794
Finished 37 epochs...
Completing Train Step...
At time: 839.9015157222748 and batch: 50, loss is 5.079654874801636 and perplexity is 160.71857832457337
At time: 840.7730860710144 and batch: 100, loss is 5.088259191513061 and perplexity is 162.10741831498896
At time: 841.617532491684 and batch: 150, loss is 5.013594636917114 and perplexity is 150.44455889977988
At time: 842.4697442054749 and batch: 200, loss is 5.054648494720459 and perplexity is 156.74942240982858
At time: 843.3242337703705 and batch: 250, loss is 5.082516775131226 and perplexity is 161.17919768581172
At time: 844.1692402362823 and batch: 300, loss is 5.076472244262695 and perplexity is 160.20788357644787
At time: 845.0259013175964 and batch: 350, loss is 5.083047533035279 and perplexity is 161.2647675253779
At time: 845.8840000629425 and batch: 400, loss is 5.056603698730469 and perplexity is 157.05619931708122
At time: 846.7341721057892 and batch: 450, loss is 5.012637062072754 and perplexity is 150.30056592773335
At time: 847.5777447223663 and batch: 500, loss is 5.012522039413452 and perplexity is 150.2832789511618
At time: 848.421281337738 and batch: 550, loss is 5.019979095458984 and perplexity is 151.40813864332543
At time: 849.2649002075195 and batch: 600, loss is 5.055466051101685 and perplexity is 156.87762630018662
At time: 850.1086740493774 and batch: 650, loss is 5.063109426498413 and perplexity is 158.08129507552405
At time: 850.9516839981079 and batch: 700, loss is 5.083738470077515 and perplexity is 161.37622982915178
At time: 851.800240278244 and batch: 750, loss is 5.0418190574646 and perplexity is 154.7512605831429
At time: 852.6416914463043 and batch: 800, loss is 5.0563010978698735 and perplexity is 157.00868116588532
At time: 853.4842984676361 and batch: 850, loss is 5.077140684127808 and perplexity is 160.31500871190246
At time: 854.3272652626038 and batch: 900, loss is 5.063710260391235 and perplexity is 158.17630421492393
At time: 855.1705725193024 and batch: 950, loss is 5.046109590530396 and perplexity is 155.4166524057086
At time: 856.0153651237488 and batch: 1000, loss is 5.037622108459472 and perplexity is 154.10313845311626
At time: 856.8575694561005 and batch: 1050, loss is 5.0406331443786625 and perplexity is 154.56784781544857
At time: 857.7101345062256 and batch: 1100, loss is 5.009835186004639 and perplexity is 149.88003178642674
At time: 858.5702168941498 and batch: 1150, loss is 5.035818243026734 and perplexity is 153.82540769834793
At time: 859.4336647987366 and batch: 1200, loss is 5.067521457672119 and perplexity is 158.78029554942165
At time: 860.2833437919617 and batch: 1250, loss is 5.0890001201629635 and perplexity is 162.22757285305326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098240316349225 and perplexity of 163.73353442818285
Finished 38 epochs...
Completing Train Step...
At time: 862.6387052536011 and batch: 50, loss is 5.079639301300049 and perplexity is 160.71607539302855
At time: 863.5027134418488 and batch: 100, loss is 5.0882414531707765 and perplexity is 162.10454282361928
At time: 864.5244312286377 and batch: 150, loss is 5.013574571609497 and perplexity is 150.44154021371176
At time: 865.3703594207764 and batch: 200, loss is 5.054626960754394 and perplexity is 156.74604700942876
At time: 866.2132382392883 and batch: 250, loss is 5.082499942779541 and perplexity is 161.17648468370513
At time: 867.0565078258514 and batch: 300, loss is 5.076458444595337 and perplexity is 160.20567277620054
At time: 867.919102191925 and batch: 350, loss is 5.0830338764190675 and perplexity is 161.26256520937756
At time: 868.7713706493378 and batch: 400, loss is 5.056594762802124 and perplexity is 157.05479588040853
At time: 869.6164228916168 and batch: 450, loss is 5.012627763748169 and perplexity is 150.29916839078345
At time: 870.4614028930664 and batch: 500, loss is 5.012513389587403 and perplexity is 150.28197903256282
At time: 871.3069236278534 and batch: 550, loss is 5.019971256256103 and perplexity is 151.4069517288611
At time: 872.1490197181702 and batch: 600, loss is 5.0554631805419925 and perplexity is 156.8771759742423
At time: 873.0068702697754 and batch: 650, loss is 5.063110675811767 and perplexity is 158.0814925687204
At time: 873.865909576416 and batch: 700, loss is 5.08373854637146 and perplexity is 161.3762421411815
At time: 874.7109735012054 and batch: 750, loss is 5.041819725036621 and perplexity is 154.75136389078926
At time: 875.5753719806671 and batch: 800, loss is 5.0563028526306155 and perplexity is 157.00895667879692
At time: 876.422426700592 and batch: 850, loss is 5.0771411037445064 and perplexity is 160.3150759827713
At time: 877.2668132781982 and batch: 900, loss is 5.063709144592285 and perplexity is 158.17612772206823
At time: 878.1182172298431 and batch: 950, loss is 5.046111326217652 and perplexity is 155.41692216064567
At time: 878.9765291213989 and batch: 1000, loss is 5.037624130249023 and perplexity is 154.1034500175463
At time: 879.8362970352173 and batch: 1050, loss is 5.040645246505737 and perplexity is 154.56971842650367
At time: 880.6892087459564 and batch: 1100, loss is 5.0098533439636235 and perplexity is 149.88275332660524
At time: 881.5322721004486 and batch: 1150, loss is 5.0358335971832275 and perplexity is 153.82776957586273
At time: 882.3896553516388 and batch: 1200, loss is 5.067532968521118 and perplexity is 158.782123255947
At time: 883.2636597156525 and batch: 1250, loss is 5.0890035152435305 and perplexity is 162.22812362966823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098229624059078 and perplexity of 163.7317837510854
Finished 39 epochs...
Completing Train Step...
At time: 885.564281463623 and batch: 50, loss is 5.079624366760254 and perplexity is 160.71367519032788
At time: 886.4071867465973 and batch: 100, loss is 5.0882242393493655 and perplexity is 162.10175240898616
At time: 887.2556080818176 and batch: 150, loss is 5.013556070327759 and perplexity is 150.43875687813892
At time: 888.1011047363281 and batch: 200, loss is 5.054606447219848 and perplexity is 156.7428316269581
At time: 888.9497580528259 and batch: 250, loss is 5.082483253479004 and perplexity is 161.17379478335906
At time: 889.8114840984344 and batch: 300, loss is 5.076444501876831 and perplexity is 160.20343908917383
At time: 890.6546444892883 and batch: 350, loss is 5.083020725250244 and perplexity is 161.260444432103
At time: 891.4977309703827 and batch: 400, loss is 5.056585359573364 and perplexity is 157.05331906517844
At time: 892.3422770500183 and batch: 450, loss is 5.012618789672851 and perplexity is 150.2978196007782
At time: 893.1865816116333 and batch: 500, loss is 5.012504892349243 and perplexity is 150.28070205622132
At time: 894.0303778648376 and batch: 550, loss is 5.019963998794555 and perplexity is 151.40585290271812
At time: 894.8776438236237 and batch: 600, loss is 5.055460863113403 and perplexity is 156.8768124230109
At time: 895.7449989318848 and batch: 650, loss is 5.063112535476685 and perplexity is 158.0817865475995
At time: 896.6023855209351 and batch: 700, loss is 5.083740282058716 and perplexity is 161.37652224011137
At time: 897.4516506195068 and batch: 750, loss is 5.041821985244751 and perplexity is 154.75171366147532
At time: 898.330934047699 and batch: 800, loss is 5.056304903030395 and perplexity is 157.00927861025718
At time: 899.186506986618 and batch: 850, loss is 5.077141227722168 and perplexity is 160.3150958582608
At time: 900.043289899826 and batch: 900, loss is 5.063707761764526 and perplexity is 158.17590899187923
At time: 900.956787109375 and batch: 950, loss is 5.046113052368164 and perplexity is 155.417190433877
At time: 901.8136200904846 and batch: 1000, loss is 5.037626037597656 and perplexity is 154.10374394683132
At time: 902.6595773696899 and batch: 1050, loss is 5.040657072067261 and perplexity is 154.57154631102648
At time: 903.5086860656738 and batch: 1100, loss is 5.009870147705078 and perplexity is 149.8852719388016
At time: 904.3590533733368 and batch: 1150, loss is 5.035847702026367 and perplexity is 153.82993930772494
At time: 905.2114307880402 and batch: 1200, loss is 5.067543706893921 and perplexity is 158.78382832673572
At time: 906.0620477199554 and batch: 1250, loss is 5.0890067672729495 and perplexity is 162.2286512011567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098222941377737 and perplexity of 163.73068958740524
Finished 40 epochs...
Completing Train Step...
At time: 908.4710855484009 and batch: 50, loss is 5.079609727859497 and perplexity is 160.7113225360066
At time: 909.3233847618103 and batch: 100, loss is 5.088207406997681 and perplexity is 162.0990238782447
At time: 910.1720740795135 and batch: 150, loss is 5.013538780212403 and perplexity is 150.436155797165
At time: 911.0204396247864 and batch: 200, loss is 5.054587326049805 and perplexity is 156.73983454927532
At time: 911.8682081699371 and batch: 250, loss is 5.082467041015625 and perplexity is 161.17118178029517
At time: 912.7183737754822 and batch: 300, loss is 5.076430425643921 and perplexity is 160.20118404412355
At time: 913.5740172863007 and batch: 350, loss is 5.083007802963257 and perplexity is 161.25836059182438
At time: 914.4305663108826 and batch: 400, loss is 5.056576051712036 and perplexity is 157.0518572414667
At time: 915.280387878418 and batch: 450, loss is 5.012610120773315 and perplexity is 150.29651668972704
At time: 916.1299564838409 and batch: 500, loss is 5.012496662139893 and perplexity is 150.27946521967178
At time: 916.9824914932251 and batch: 550, loss is 5.019956912994385 and perplexity is 151.4047800749007
At time: 917.8310170173645 and batch: 600, loss is 5.055458612442017 and perplexity is 156.87645934525537
At time: 918.6795411109924 and batch: 650, loss is 5.063114891052246 and perplexity is 158.08215892163116
At time: 919.5270092487335 and batch: 700, loss is 5.083742771148682 and perplexity is 161.37692392129355
At time: 920.3774888515472 and batch: 750, loss is 5.0418252277374265 and perplexity is 154.75221544358686
At time: 921.2278249263763 and batch: 800, loss is 5.056307029724121 and perplexity is 157.00961252125998
At time: 922.1364119052887 and batch: 850, loss is 5.077141361236572 and perplexity is 160.3151172626367
At time: 922.9849808216095 and batch: 900, loss is 5.063705825805664 and perplexity is 158.17560277012294
At time: 923.8343825340271 and batch: 950, loss is 5.04611496925354 and perplexity is 155.41748835110204
At time: 924.6846914291382 and batch: 1000, loss is 5.037627763748169 and perplexity is 154.1040099533176
At time: 925.534918308258 and batch: 1050, loss is 5.040668640136719 and perplexity is 154.57333441575287
At time: 926.3989934921265 and batch: 1100, loss is 5.009885702133179 and perplexity is 149.8876033366191
At time: 927.2679355144501 and batch: 1150, loss is 5.035861043930054 and perplexity is 153.83199170565075
At time: 928.1271281242371 and batch: 1200, loss is 5.067554035186768 and perplexity is 158.78546830108306
At time: 928.9820036888123 and batch: 1250, loss is 5.089009799957275 and perplexity is 162.22914319019046
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098213585623859 and perplexity of 163.7291577705368
Finished 41 epochs...
Completing Train Step...
At time: 931.4286198616028 and batch: 50, loss is 5.079595708847046 and perplexity is 160.70906953776748
At time: 932.3089368343353 and batch: 100, loss is 5.088191041946411 and perplexity is 162.09637114111433
At time: 933.1626996994019 and batch: 150, loss is 5.013522367477417 and perplexity is 150.43368674866966
At time: 934.0349450111389 and batch: 200, loss is 5.054568796157837 and perplexity is 156.7369302039828
At time: 934.9001748561859 and batch: 250, loss is 5.08245078086853 and perplexity is 161.16856113447813
At time: 935.7521004676819 and batch: 300, loss is 5.076416549682617 and perplexity is 160.19896111411552
At time: 936.6033043861389 and batch: 350, loss is 5.082995138168335 and perplexity is 161.25631830069062
At time: 937.4554023742676 and batch: 400, loss is 5.056566476821899 and perplexity is 157.05035349438697
At time: 938.3059673309326 and batch: 450, loss is 5.012601633071899 and perplexity is 150.2952410231832
At time: 939.1576344966888 and batch: 500, loss is 5.01248872756958 and perplexity is 150.27827282141902
At time: 940.0113015174866 and batch: 550, loss is 5.019950351715088 and perplexity is 151.4037866691108
At time: 940.8620374202728 and batch: 600, loss is 5.0554563331604 and perplexity is 156.87610178003297
At time: 941.7115778923035 and batch: 650, loss is 5.063117437362671 and perplexity is 158.08256144839288
At time: 942.563277721405 and batch: 700, loss is 5.083745956420898 and perplexity is 161.3774379515444
At time: 943.4157745838165 and batch: 750, loss is 5.0418289947509765 and perplexity is 154.75279839837734
At time: 944.3409435749054 and batch: 800, loss is 5.056309108734131 and perplexity is 157.0099389461553
At time: 945.2187585830688 and batch: 850, loss is 5.077141485214233 and perplexity is 160.3151371381312
At time: 946.0797431468964 and batch: 900, loss is 5.06370418548584 and perplexity is 158.1753433117587
At time: 946.9477331638336 and batch: 950, loss is 5.0461166477203365 and perplexity is 155.41774921441478
At time: 947.7983825206757 and batch: 1000, loss is 5.0376293659210205 and perplexity is 154.1042568547764
At time: 948.6492178440094 and batch: 1050, loss is 5.040679998397827 and perplexity is 154.5750901100164
At time: 949.500030040741 and batch: 1100, loss is 5.009900455474853 and perplexity is 149.88981469595635
At time: 950.3525047302246 and batch: 1150, loss is 5.035873746871948 and perplexity is 153.83394583691438
At time: 951.2024545669556 and batch: 1200, loss is 5.067564363479614 and perplexity is 158.78710829236863
At time: 952.0523099899292 and batch: 1250, loss is 5.089013090133667 and perplexity is 162.2296769535655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098207793966697 and perplexity of 163.72820951013358
Finished 42 epochs...
Completing Train Step...
At time: 954.5436372756958 and batch: 50, loss is 5.079581956863404 and perplexity is 160.70685948446834
At time: 955.4367372989655 and batch: 100, loss is 5.0881748867034915 and perplexity is 162.09375245601498
At time: 956.288765668869 and batch: 150, loss is 5.01350658416748 and perplexity is 150.43131242590408
At time: 957.1416494846344 and batch: 200, loss is 5.054551095962524 and perplexity is 156.7341559542579
At time: 957.9930839538574 and batch: 250, loss is 5.082434968948364 and perplexity is 161.1660127702035
At time: 958.8440301418304 and batch: 300, loss is 5.07640266418457 and perplexity is 160.19673668719759
At time: 959.6951332092285 and batch: 350, loss is 5.082982711791992 and perplexity is 161.25431448144187
At time: 960.5469920635223 and batch: 400, loss is 5.056557006835938 and perplexity is 157.0488662367863
At time: 961.3980896472931 and batch: 450, loss is 5.012593259811402 and perplexity is 150.29398256724727
At time: 962.2511522769928 and batch: 500, loss is 5.012481060028076 and perplexity is 150.2771205609425
At time: 963.101309299469 and batch: 550, loss is 5.019944019317627 and perplexity is 151.40282792319215
At time: 963.9542977809906 and batch: 600, loss is 5.055454111099243 and perplexity is 156.875753192128
At time: 964.8153450489044 and batch: 650, loss is 5.063120098114013 and perplexity is 158.0829820673401
At time: 965.6702754497528 and batch: 700, loss is 5.08374942779541 and perplexity is 161.37799815404156
At time: 966.5722668170929 and batch: 750, loss is 5.041833267211914 and perplexity is 154.75345957507596
At time: 967.4236991405487 and batch: 800, loss is 5.056311120986939 and perplexity is 157.0102548901637
At time: 968.274729013443 and batch: 850, loss is 5.077141742706299 and perplexity is 160.31517841801238
At time: 969.1251029968262 and batch: 900, loss is 5.063701992034912 and perplexity is 158.1749963622857
At time: 969.97705078125 and batch: 950, loss is 5.046118478775025 and perplexity is 155.41803379307368
At time: 970.8269786834717 and batch: 1000, loss is 5.037630891799926 and perplexity is 154.10449199939066
At time: 971.6786587238312 and batch: 1050, loss is 5.040690975189209 and perplexity is 154.57678685784572
At time: 972.5300447940826 and batch: 1100, loss is 5.009914684295654 and perplexity is 149.89194746644287
At time: 973.3813376426697 and batch: 1150, loss is 5.035886001586914 and perplexity is 153.83583103962405
At time: 974.2306125164032 and batch: 1200, loss is 5.067574319839477 and perplexity is 158.78868924183058
At time: 975.0810832977295 and batch: 1250, loss is 5.08901608467102 and perplexity is 162.23016275712033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098202893333713 and perplexity of 163.72740714023573
Finished 43 epochs...
Completing Train Step...
At time: 977.3935797214508 and batch: 50, loss is 5.079568538665772 and perplexity is 160.70470310253435
At time: 978.2435293197632 and batch: 100, loss is 5.088159036636353 and perplexity is 162.09118327951666
At time: 979.0926954746246 and batch: 150, loss is 5.013491516113281 and perplexity is 150.42904573581256
At time: 979.9413642883301 and batch: 200, loss is 5.0545337200164795 and perplexity is 156.7314325736814
At time: 980.7928893566132 and batch: 250, loss is 5.082418956756592 and perplexity is 161.1634321697604
At time: 981.6442081928253 and batch: 300, loss is 5.076389169692993 and perplexity is 160.19457492826956
At time: 982.4944319725037 and batch: 350, loss is 5.082970542907715 and perplexity is 161.25235220828912
At time: 983.3461346626282 and batch: 400, loss is 5.056547393798828 and perplexity is 157.04735652746362
At time: 984.1986947059631 and batch: 450, loss is 5.012585191726685 and perplexity is 150.29276998755512
At time: 985.0482261180878 and batch: 500, loss is 5.012473382949829 and perplexity is 150.27596687615772
At time: 985.9017431735992 and batch: 550, loss is 5.0199378395080565 and perplexity is 151.40189228543815
At time: 986.7536187171936 and batch: 600, loss is 5.055452117919922 and perplexity is 156.87544051093235
At time: 987.6054263114929 and batch: 650, loss is 5.063122959136963 and perplexity is 158.08343434702667
At time: 988.5008826255798 and batch: 700, loss is 5.083753414154053 and perplexity is 161.37864146590155
At time: 989.3499717712402 and batch: 750, loss is 5.0418376541137695 and perplexity is 154.75413846480396
At time: 990.1986811161041 and batch: 800, loss is 5.056313495635987 and perplexity is 157.0106277348587
At time: 991.0504469871521 and batch: 850, loss is 5.077142105102539 and perplexity is 160.3152365156408
At time: 991.9016025066376 and batch: 900, loss is 5.063699741363525 and perplexity is 158.17464036274785
At time: 992.7524092197418 and batch: 950, loss is 5.046120128631592 and perplexity is 155.41829021074886
At time: 993.6035304069519 and batch: 1000, loss is 5.037632455825806 and perplexity is 154.10473302299278
At time: 994.4561531543732 and batch: 1050, loss is 5.04070182800293 and perplexity is 154.57846446002236
At time: 995.3089716434479 and batch: 1100, loss is 5.00992826461792 and perplexity is 149.8939830612165
At time: 996.1587312221527 and batch: 1150, loss is 5.035897846221924 and perplexity is 153.83765317968542
At time: 997.0104990005493 and batch: 1200, loss is 5.067584056854248 and perplexity is 158.7902353771705
At time: 997.8619611263275 and batch: 1250, loss is 5.089019174575806 and perplexity is 162.230664033651
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098200665773266 and perplexity of 163.72704242794566
Finished 44 epochs...
Completing Train Step...
At time: 1000.1642653942108 and batch: 50, loss is 5.079555568695068 and perplexity is 160.70261878076
At time: 1001.059722661972 and batch: 100, loss is 5.088143539428711 and perplexity is 162.08867133825657
At time: 1001.9159910678864 and batch: 150, loss is 5.013476867675781 and perplexity is 150.4268422014771
At time: 1002.7669930458069 and batch: 200, loss is 5.054516839981079 and perplexity is 156.72878696388022
At time: 1003.6172533035278 and batch: 250, loss is 5.082403526306153 and perplexity is 161.16094536459403
At time: 1004.4675154685974 and batch: 300, loss is 5.0763757610321045 and perplexity is 160.19242694793903
At time: 1005.3191947937012 and batch: 350, loss is 5.082958307266235 and perplexity is 161.25037919439032
At time: 1006.171094417572 and batch: 400, loss is 5.056537675857544 and perplexity is 157.0458303578896
At time: 1007.0196495056152 and batch: 450, loss is 5.012577085494995 and perplexity is 150.29155168447826
At time: 1007.871205329895 and batch: 500, loss is 5.012465991973877 and perplexity is 150.2748561942049
At time: 1008.7230014801025 and batch: 550, loss is 5.019931659698487 and perplexity is 151.40095665346632
At time: 1009.598030090332 and batch: 600, loss is 5.055450029373169 and perplexity is 156.8751128695827
At time: 1010.4466984272003 and batch: 650, loss is 5.063125915527344 and perplexity is 158.08390170406227
At time: 1011.2933895587921 and batch: 700, loss is 5.083757467269898 and perplexity is 161.37929555355583
At time: 1012.1406691074371 and batch: 750, loss is 5.041842756271362 and perplexity is 154.75492804682077
At time: 1012.9891822338104 and batch: 800, loss is 5.056315622329712 and perplexity is 157.01096164873056
At time: 1013.8466413021088 and batch: 850, loss is 5.077142248153686 and perplexity is 160.3152594489209
At time: 1014.6944921016693 and batch: 900, loss is 5.0636975383758545 and perplexity is 158.17429190634917
At time: 1015.5429406166077 and batch: 950, loss is 5.046121959686279 and perplexity is 155.41857479039822
At time: 1016.3905103206635 and batch: 1000, loss is 5.037633905410766 and perplexity is 154.104956411058
At time: 1017.2387733459473 and batch: 1050, loss is 5.04071228981018 and perplexity is 154.58008163858193
At time: 1018.0834224224091 and batch: 1100, loss is 5.009941453933716 and perplexity is 149.8959600733327
At time: 1018.9280817508698 and batch: 1150, loss is 5.035909414291382 and perplexity is 153.83943279463597
At time: 1019.7741875648499 and batch: 1200, loss is 5.0675936126708985 and perplexity is 158.79175275479557
At time: 1020.6219737529755 and batch: 1250, loss is 5.089022178649902 and perplexity is 162.23115138731845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0981971016765515 and perplexity of 163.72645888997155
Finished 45 epochs...
Completing Train Step...
At time: 1022.9175183773041 and batch: 50, loss is 5.079542846679687 and perplexity is 160.70057433257693
At time: 1023.7640578746796 and batch: 100, loss is 5.088128261566162 and perplexity is 162.0861949887318
At time: 1024.6146795749664 and batch: 150, loss is 5.013462562561035 and perplexity is 150.42469034362983
At time: 1025.464067697525 and batch: 200, loss is 5.0545001888275145 and perplexity is 156.72617727050778
At time: 1026.3165934085846 and batch: 250, loss is 5.082388191223145 and perplexity is 161.1584739670688
At time: 1027.1742119789124 and batch: 300, loss is 5.076362342834472 and perplexity is 160.19027746871615
At time: 1028.028389930725 and batch: 350, loss is 5.082946233749389 and perplexity is 161.2484323469734
At time: 1028.8825056552887 and batch: 400, loss is 5.056528053283691 and perplexity is 157.0443191800595
At time: 1029.7369356155396 and batch: 450, loss is 5.012569093704224 and perplexity is 150.29035059064196
At time: 1030.5919210910797 and batch: 500, loss is 5.012458629608155 and perplexity is 150.27374981982746
At time: 1031.4944739341736 and batch: 550, loss is 5.019925746917725 and perplexity is 151.40006145544905
At time: 1032.3496530056 and batch: 600, loss is 5.055448026657104 and perplexity is 156.87479869358856
At time: 1033.2020947933197 and batch: 650, loss is 5.0631288719177245 and perplexity is 158.08436906247942
At time: 1034.053415775299 and batch: 700, loss is 5.083761758804322 and perplexity is 161.37998811984409
At time: 1034.9052064418793 and batch: 750, loss is 5.041847743988037 and perplexity is 154.75569992248091
At time: 1035.7575633525848 and batch: 800, loss is 5.056317920684815 and perplexity is 157.01132251609013
At time: 1036.609144449234 and batch: 850, loss is 5.077142553329468 and perplexity is 160.31530837326295
At time: 1037.4602506160736 and batch: 900, loss is 5.063695344924927 and perplexity is 158.1739449591824
At time: 1038.3154969215393 and batch: 950, loss is 5.046123466491699 and perplexity is 155.41880897612552
At time: 1039.1663963794708 and batch: 1000, loss is 5.037635250091553 and perplexity is 154.1051636331713
At time: 1040.014743566513 and batch: 1050, loss is 5.040722360610962 and perplexity is 154.58163839162782
At time: 1040.866750240326 and batch: 1100, loss is 5.009954252243042 and perplexity is 149.89787850047273
At time: 1041.7223796844482 and batch: 1150, loss is 5.035920658111572 and perplexity is 153.84116254728102
At time: 1042.5779790878296 and batch: 1200, loss is 5.067602977752686 and perplexity is 158.7932398595107
At time: 1043.4266583919525 and batch: 1250, loss is 5.089025068283081 and perplexity is 162.23162017651347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098196656164462 and perplexity of 163.726385947871
Finished 46 epochs...
Completing Train Step...
At time: 1045.746913909912 and batch: 50, loss is 5.079530572891235 and perplexity is 160.6986019398278
At time: 1046.6415207386017 and batch: 100, loss is 5.088113050460816 and perplexity is 162.08372949729616
At time: 1047.4896216392517 and batch: 150, loss is 5.013448724746704 and perplexity is 150.42260880909606
At time: 1048.3362956047058 and batch: 200, loss is 5.054484062194824 and perplexity is 156.7236498253936
At time: 1049.1852655410767 and batch: 250, loss is 5.082372999191284 and perplexity is 161.15602566099506
At time: 1050.0470316410065 and batch: 300, loss is 5.0763492774963375 and perplexity is 160.1881845422475
At time: 1050.9135484695435 and batch: 350, loss is 5.08293420791626 and perplexity is 161.24649321189352
At time: 1051.7617344856262 and batch: 400, loss is 5.0565183734893795 and perplexity is 157.04279903070938
At time: 1052.634046792984 and batch: 450, loss is 5.012561273574829 and perplexity is 150.28917530524902
At time: 1053.5366911888123 and batch: 500, loss is 5.0124514770507815 and perplexity is 150.27267498205413
At time: 1054.3871095180511 and batch: 550, loss is 5.0199200057983395 and perplexity is 151.39919225211642
At time: 1055.2463314533234 and batch: 600, loss is 5.055446014404297 and perplexity is 156.87448302215213
At time: 1056.1043140888214 and batch: 650, loss is 5.063131923675537 and perplexity is 158.08485149842392
At time: 1056.9605596065521 and batch: 700, loss is 5.083765964508057 and perplexity is 161.3806668376902
At time: 1057.816926240921 and batch: 750, loss is 5.041852827072144 and perplexity is 154.75648656071886
At time: 1058.6764063835144 and batch: 800, loss is 5.0563199424743654 and perplexity is 157.01163996026227
At time: 1059.5253434181213 and batch: 850, loss is 5.077142896652222 and perplexity is 160.3153634131656
At time: 1060.3726329803467 and batch: 900, loss is 5.063692941665649 and perplexity is 158.17356482663843
At time: 1061.2184200286865 and batch: 950, loss is 5.046124992370605 and perplexity is 155.41904612658863
At time: 1062.0666835308075 and batch: 1000, loss is 5.037636728286743 and perplexity is 154.10539143085134
At time: 1062.9114558696747 and batch: 1050, loss is 5.040732288360596 and perplexity is 154.5831730470496
At time: 1063.760312795639 and batch: 1100, loss is 5.0099665069580075 and perplexity is 149.89971546750343
At time: 1064.6086077690125 and batch: 1150, loss is 5.035931901931763 and perplexity is 153.84289231937518
At time: 1065.4579803943634 and batch: 1200, loss is 5.067612600326538 and perplexity is 158.79476786654018
At time: 1066.306833744049 and batch: 1250, loss is 5.089028034210205 and perplexity is 162.23210134438975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098193537579836 and perplexity of 163.72587535407706
Finished 47 epochs...
Completing Train Step...
At time: 1068.6861007213593 and batch: 50, loss is 5.079518346786499 and perplexity is 160.69663723389996
At time: 1069.5826156139374 and batch: 100, loss is 5.088098440170288 and perplexity is 162.08136142421756
At time: 1070.4409909248352 and batch: 150, loss is 5.013435182571411 and perplexity is 150.4205717735525
At time: 1071.3107306957245 and batch: 200, loss is 5.0544681739807125 and perplexity is 156.72115978627002
At time: 1072.1584298610687 and batch: 250, loss is 5.082358112335205 and perplexity is 161.15362657229235
At time: 1073.0048532485962 and batch: 300, loss is 5.07633641242981 and perplexity is 160.18612372385275
At time: 1073.8539772033691 and batch: 350, loss is 5.082922134399414 and perplexity is 161.24454641139377
At time: 1074.756275653839 and batch: 400, loss is 5.056508836746215 and perplexity is 157.0413013610107
At time: 1075.604253768921 and batch: 450, loss is 5.012553634643555 and perplexity is 150.28802726095253
At time: 1076.453182220459 and batch: 500, loss is 5.0124445724487305 and perplexity is 150.27163741261626
At time: 1077.30291390419 and batch: 550, loss is 5.019914321899414 and perplexity is 151.3983317168558
At time: 1078.1485376358032 and batch: 600, loss is 5.055444030761719 and perplexity is 156.87417183955682
At time: 1078.994732618332 and batch: 650, loss is 5.063134832382202 and perplexity is 158.08531132155383
At time: 1079.8417732715607 and batch: 700, loss is 5.083770523071289 and perplexity is 161.38140250334115
At time: 1080.6893968582153 and batch: 750, loss is 5.041858158111572 and perplexity is 154.75731157584963
At time: 1081.5359771251678 and batch: 800, loss is 5.056322221755981 and perplexity is 157.01199783441456
At time: 1082.3819708824158 and batch: 850, loss is 5.077143201828003 and perplexity is 160.31541233753927
At time: 1083.2283596992493 and batch: 900, loss is 5.063690605163575 and perplexity is 158.17319525420785
At time: 1084.0742316246033 and batch: 950, loss is 5.046126518249512 and perplexity is 155.41928327741377
At time: 1084.9234883785248 and batch: 1000, loss is 5.037638187408447 and perplexity is 154.10561628953678
At time: 1085.8012838363647 and batch: 1050, loss is 5.0407420253753665 and perplexity is 154.5846782330168
At time: 1086.6587765216827 and batch: 1100, loss is 5.009978590011596 and perplexity is 149.9015267247411
At time: 1087.5093700885773 and batch: 1150, loss is 5.035942878723144 and perplexity is 153.844581029978
At time: 1088.374564409256 and batch: 1200, loss is 5.067621841430664 and perplexity is 158.79623531230501
At time: 1089.2317316532135 and batch: 1250, loss is 5.089030904769897 and perplexity is 162.23256704198903
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098192201043568 and perplexity of 163.72565652865282
Finished 48 epochs...
Completing Train Step...
At time: 1091.5906913280487 and batch: 50, loss is 5.079506464004517 and perplexity is 160.6947277221396
At time: 1092.4391293525696 and batch: 100, loss is 5.088083896636963 and perplexity is 162.07900420567748
At time: 1093.2866659164429 and batch: 150, loss is 5.0134218883514405 and perplexity is 150.41857206267562
At time: 1094.133540391922 and batch: 200, loss is 5.054452409744263 and perplexity is 156.7186892163239
At time: 1094.9796590805054 and batch: 250, loss is 5.08234317779541 and perplexity is 161.15121983501498
At time: 1095.8239431381226 and batch: 300, loss is 5.0763235282897945 and perplexity is 160.18405987670167
At time: 1096.7065162658691 and batch: 350, loss is 5.08291036605835 and perplexity is 161.2426488417425
At time: 1097.5469868183136 and batch: 400, loss is 5.056499338150024 and perplexity is 157.0398096961881
At time: 1098.389175415039 and batch: 450, loss is 5.012545862197876 and perplexity is 150.28685915996394
At time: 1099.2401978969574 and batch: 500, loss is 5.012437629699707 and perplexity is 150.27059411797399
At time: 1100.081006526947 and batch: 550, loss is 5.01990891456604 and perplexity is 151.39751305781735
At time: 1100.9237291812897 and batch: 600, loss is 5.0554420948028564 and perplexity is 156.87386813790752
At time: 1101.7660884857178 and batch: 650, loss is 5.063137807846069 and perplexity is 158.08578169938542
At time: 1102.627008676529 and batch: 700, loss is 5.0837750816345215 and perplexity is 161.38213817234583
At time: 1103.4829399585724 and batch: 750, loss is 5.041863584518433 and perplexity is 154.7581513542654
At time: 1104.3269708156586 and batch: 800, loss is 5.056324529647827 and perplexity is 157.01236020154218
At time: 1105.1694736480713 and batch: 850, loss is 5.077143507003784 and perplexity is 160.315461261928
At time: 1106.011688709259 and batch: 900, loss is 5.063688278198242 and perplexity is 158.1728271910942
At time: 1106.8576321601868 and batch: 950, loss is 5.0461280632019045 and perplexity is 155.41952339299283
At time: 1107.715532541275 and batch: 1000, loss is 5.037639446258545 and perplexity is 154.10581028552903
At time: 1108.5746655464172 and batch: 1050, loss is 5.040751667022705 and perplexity is 154.58616869115346
At time: 1109.416021823883 and batch: 1100, loss is 5.00999038696289 and perplexity is 149.90329511618157
At time: 1110.260339975357 and batch: 1150, loss is 5.03595350265503 and perplexity is 153.8462154730099
At time: 1111.1033825874329 and batch: 1200, loss is 5.067630872726441 and perplexity is 158.79766945455046
At time: 1111.9464991092682 and batch: 1250, loss is 5.089033908843994 and perplexity is 162.2330544013734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098190864507299 and perplexity of 163.72543770352107
Finished 49 epochs...
Completing Train Step...
At time: 1114.2207725048065 and batch: 50, loss is 5.079494876861572 and perplexity is 160.69286574014657
At time: 1115.091293811798 and batch: 100, loss is 5.088069353103638 and perplexity is 162.07664702141943
At time: 1115.9381759166718 and batch: 150, loss is 5.013408794403076 and perplexity is 150.4166025025546
At time: 1116.780469417572 and batch: 200, loss is 5.054436960220337 and perplexity is 156.7162680058885
At time: 1117.6240558624268 and batch: 250, loss is 5.082328519821167 and perplexity is 161.14885770189744
At time: 1118.5106818675995 and batch: 300, loss is 5.076310882568359 and perplexity is 160.1820342465099
At time: 1119.3525927066803 and batch: 350, loss is 5.082898654937744 and perplexity is 161.2407605206923
At time: 1120.2025187015533 and batch: 400, loss is 5.056489696502686 and perplexity is 157.03829558102422
At time: 1121.0457208156586 and batch: 450, loss is 5.012538242340088 and perplexity is 150.28571399983267
At time: 1121.906766653061 and batch: 500, loss is 5.012430858612061 and perplexity is 150.26957662605534
At time: 1122.7524800300598 and batch: 550, loss is 5.019903326034546 and perplexity is 151.39666697041176
At time: 1123.5976440906525 and batch: 600, loss is 5.055440034866333 and perplexity is 156.87354498802978
At time: 1124.4435250759125 and batch: 650, loss is 5.063140811920166 and perplexity is 158.08625660150065
At time: 1125.2890725135803 and batch: 700, loss is 5.0837797260284425 and perplexity is 161.38288769630785
At time: 1126.1347374916077 and batch: 750, loss is 5.041868906021119 and perplexity is 154.75897490237472
At time: 1126.980872631073 and batch: 800, loss is 5.056326684951782 and perplexity is 157.01269861126778
At time: 1127.8324887752533 and batch: 850, loss is 5.077143745422363 and perplexity is 160.31549948411703
At time: 1128.6983819007874 and batch: 900, loss is 5.0636858463287355 and perplexity is 158.1724425358867
At time: 1129.5446465015411 and batch: 950, loss is 5.046129484176635 and perplexity is 155.41974424036513
At time: 1130.390219926834 and batch: 1000, loss is 5.037640810012817 and perplexity is 154.1060204481295
At time: 1131.2370352745056 and batch: 1050, loss is 5.040761041641235 and perplexity is 154.5876178843078
At time: 1132.0836822986603 and batch: 1100, loss is 5.010002155303955 and perplexity is 149.90505923966555
At time: 1132.929387331009 and batch: 1150, loss is 5.035964403152466 and perplexity is 153.84789248242734
At time: 1133.7766163349152 and batch: 1200, loss is 5.067640132904053 and perplexity is 158.79913995598258
At time: 1134.6244840621948 and batch: 1250, loss is 5.089036750793457 and perplexity is 162.2335154601703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.098190864507299 and perplexity of 163.72543770352107
Finished Training.
Improved accuracyfrom -10000000 to -163.72543770352107
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f1083e08860>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'seq_len': 35, 'num_layers': 1, 'data': 'wikitext', 'dropout': 0.41373980563436863, 'wordvec_source': '', 'lr': 20.68176370898624, 'tune_wordvecs': True, 'batch_size': 50, 'anneal': 7.89598993802388}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.426947832107544 and batch: 50, loss is 7.043381395339966 and perplexity is 1145.253621942587
At time: 2.2529196739196777 and batch: 100, loss is 6.284187393188477 and perplexity is 536.0285331745075
At time: 3.0820140838623047 and batch: 150, loss is 6.077054300308228 and perplexity is 435.743732177032
At time: 3.904589891433716 and batch: 200, loss is 6.04954348564148 and perplexity is 423.9194605439033
At time: 4.779172658920288 and batch: 250, loss is 6.053063526153564 and perplexity is 425.41430362947847
At time: 5.605732679367065 and batch: 300, loss is 6.041516246795655 and perplexity is 420.5301792673357
At time: 6.431885480880737 and batch: 350, loss is 6.062479753494262 and perplexity is 429.4390205133225
At time: 7.258319616317749 and batch: 400, loss is 5.99336353302002 and perplexity is 400.7603160510656
At time: 8.083701372146606 and batch: 450, loss is 5.969982509613037 and perplexity is 391.4988232240917
At time: 8.912418842315674 and batch: 500, loss is 5.956185665130615 and perplexity is 386.1344655425162
At time: 9.737448692321777 and batch: 550, loss is 5.957683534622192 and perplexity is 386.7132779626122
At time: 10.562418460845947 and batch: 600, loss is 5.974982194900512 and perplexity is 393.4610954147793
At time: 11.385476350784302 and batch: 650, loss is 5.9512956428527835 and perplexity is 384.25086857277626
At time: 12.209236860275269 and batch: 700, loss is 5.977565040588379 and perplexity is 394.478658246613
At time: 13.032794713973999 and batch: 750, loss is 5.9236653041839595 and perplexity is 373.7792207225155
At time: 13.857288360595703 and batch: 800, loss is 5.930108728408814 and perplexity is 376.19541473000044
At time: 14.685291767120361 and batch: 850, loss is 5.968716821670532 and perplexity is 391.0036213356699
At time: 15.51080870628357 and batch: 900, loss is 5.962221822738647 and perplexity is 388.47228265245974
At time: 16.335415840148926 and batch: 950, loss is 5.93590913772583 and perplexity is 378.38384287235647
At time: 17.159721851348877 and batch: 1000, loss is 5.9280225372314455 and perplexity is 375.41141724348637
At time: 17.984079599380493 and batch: 1050, loss is 5.920107727050781 and perplexity is 372.4518348528129
At time: 18.809534549713135 and batch: 1100, loss is 5.918875093460083 and perplexity is 371.9930210430061
At time: 19.635103940963745 and batch: 1150, loss is 5.953517360687256 and perplexity is 385.105514620032
At time: 20.46097230911255 and batch: 1200, loss is 5.934455308914185 and perplexity is 377.8341372255034
At time: 21.287248134613037 and batch: 1250, loss is 5.9149672794342045 and perplexity is 370.5421781562556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.390341208799042 and perplexity of 219.27819243954343
Finished 1 epochs...
Completing Train Step...
At time: 23.57804846763611 and batch: 50, loss is 5.708390741348267 and perplexity is 301.38567032463357
At time: 24.397087574005127 and batch: 100, loss is 5.759737281799317 and perplexity is 317.2649666867545
At time: 25.22667694091797 and batch: 150, loss is 5.646046924591064 and perplexity is 283.1698585810017
At time: 26.066449880599976 and batch: 200, loss is 5.694869146347046 and perplexity is 297.3378832921962
At time: 26.886483669281006 and batch: 250, loss is 5.682072257995605 and perplexity is 293.55712615523726
At time: 27.70781898498535 and batch: 300, loss is 5.730768508911133 and perplexity is 308.2060364720563
At time: 28.552942514419556 and batch: 350, loss is 5.7684727382659915 and perplexity is 320.048561275421
At time: 29.4008526802063 and batch: 400, loss is 5.681216459274292 and perplexity is 293.30600781074685
At time: 30.221825122833252 and batch: 450, loss is 5.660065784454345 and perplexity is 287.16753309416595
At time: 31.07585906982422 and batch: 500, loss is 5.673212652206421 and perplexity is 290.96781282538853
At time: 31.919669151306152 and batch: 550, loss is 5.6646098804473874 and perplexity is 288.47541926025076
At time: 32.74920439720154 and batch: 600, loss is 5.671995334625244 and perplexity is 290.61382809091305
At time: 33.585832357406616 and batch: 650, loss is 5.668947992324829 and perplexity is 289.729576272756
At time: 34.420088052749634 and batch: 700, loss is 5.687133378982544 and perplexity is 295.0466203630432
At time: 35.24291110038757 and batch: 750, loss is 5.650934190750122 and perplexity is 284.55717237260836
At time: 36.06560015678406 and batch: 800, loss is 5.709736461639404 and perplexity is 301.7915241582682
At time: 36.88719725608826 and batch: 850, loss is 5.713180561065673 and perplexity is 302.83271612860585
At time: 37.706801414489746 and batch: 900, loss is 5.701931390762329 and perplexity is 299.4451884915845
At time: 38.52993726730347 and batch: 950, loss is 5.69272364616394 and perplexity is 296.7006286685536
At time: 39.35320210456848 and batch: 1000, loss is 5.674705171585083 and perplexity is 291.4024121678912
At time: 40.17520189285278 and batch: 1050, loss is 5.646739645004272 and perplexity is 283.3660840793034
At time: 41.021650552749634 and batch: 1100, loss is 5.676368732452392 and perplexity is 291.88758125968116
At time: 41.849103689193726 and batch: 1150, loss is 5.712978048324585 and perplexity is 302.77139485455064
At time: 42.68970084190369 and batch: 1200, loss is 5.690925283432007 and perplexity is 296.1675328089522
At time: 43.51155185699463 and batch: 1250, loss is 5.710395011901856 and perplexity is 301.99033450182503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.426641088332573 and perplexity of 227.38419784454553
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 45.78720474243164 and batch: 50, loss is 5.614699649810791 and perplexity is 274.4309414307179
At time: 46.6049542427063 and batch: 100, loss is 5.619849166870117 and perplexity is 275.8477731137789
At time: 47.42918848991394 and batch: 150, loss is 5.50961612701416 and perplexity is 247.05627063409867
At time: 48.252116441726685 and batch: 200, loss is 5.543827476501465 and perplexity is 255.65464136160736
At time: 49.10497331619263 and batch: 250, loss is 5.542429361343384 and perplexity is 255.2974564832654
At time: 49.948699712753296 and batch: 300, loss is 5.5219481086730955 and perplexity is 250.12182737689184
At time: 50.77273344993591 and batch: 350, loss is 5.555013751983642 and perplexity is 258.53051980360704
At time: 51.63756775856018 and batch: 400, loss is 5.524576797485351 and perplexity is 250.78018475545184
At time: 52.47000026702881 and batch: 450, loss is 5.483129110336304 and perplexity is 240.59838962044077
At time: 53.330012798309326 and batch: 500, loss is 5.460126113891602 and perplexity is 235.127075285988
At time: 54.1535804271698 and batch: 550, loss is 5.466362180709839 and perplexity is 236.59792482967032
At time: 54.97552967071533 and batch: 600, loss is 5.474167366027832 and perplexity is 238.451841147182
At time: 55.79750871658325 and batch: 650, loss is 5.460902948379516 and perplexity is 235.30980107176958
At time: 56.619832277297974 and batch: 700, loss is 5.470284900665283 and perplexity is 237.52785496610863
At time: 57.43987035751343 and batch: 750, loss is 5.425424690246582 and perplexity is 227.10777629490806
At time: 58.28363490104675 and batch: 800, loss is 5.4290541076660155 and perplexity is 227.93354283354907
At time: 59.11773467063904 and batch: 850, loss is 5.4399967384338375 and perplexity is 230.4414318594398
At time: 59.95824933052063 and batch: 900, loss is 5.40521071434021 and perplexity is 222.56311278266756
At time: 60.778897285461426 and batch: 950, loss is 5.388372058868408 and perplexity is 218.84682565443626
At time: 61.60163426399231 and batch: 1000, loss is 5.377549304962158 and perplexity is 216.49107118712075
At time: 62.423075437545776 and batch: 1050, loss is 5.353961191177368 and perplexity is 211.4442121131037
At time: 63.24505090713501 and batch: 1100, loss is 5.30156252861023 and perplexity is 200.6500866602128
At time: 64.06798386573792 and batch: 1150, loss is 5.3077463722229 and perplexity is 201.89471975898715
At time: 64.8916118144989 and batch: 1200, loss is 5.288068609237671 and perplexity is 197.96071644901946
At time: 65.71430516242981 and batch: 1250, loss is 5.3340048122406 and perplexity is 207.26637717421096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.174708902400775 and perplexity of 176.74515669641778
Finished 3 epochs...
Completing Train Step...
At time: 67.98465967178345 and batch: 50, loss is 5.417891902923584 and perplexity is 225.4034489434431
At time: 68.835031747818 and batch: 100, loss is 5.439450712203979 and perplexity is 230.31563913938615
At time: 69.6586081981659 and batch: 150, loss is 5.345051097869873 and perplexity is 209.56859283397037
At time: 70.51019215583801 and batch: 200, loss is 5.385715227127076 and perplexity is 218.26615817101398
At time: 71.33640766143799 and batch: 250, loss is 5.3983299255371096 and perplexity is 221.03695959845803
At time: 72.1628897190094 and batch: 300, loss is 5.38863395690918 and perplexity is 218.90414871537527
At time: 72.98809385299683 and batch: 350, loss is 5.425405368804932 and perplexity is 227.10338828765154
At time: 73.82054018974304 and batch: 400, loss is 5.399281597137451 and perplexity is 221.2474143215415
At time: 74.65652322769165 and batch: 450, loss is 5.3603722095489506 and perplexity is 212.80413944319523
At time: 75.50901222229004 and batch: 500, loss is 5.345444307327271 and perplexity is 209.65101338985536
At time: 76.34077596664429 and batch: 550, loss is 5.355188207626343 and perplexity is 211.70381687649336
At time: 77.19441819190979 and batch: 600, loss is 5.37312047958374 and perplexity is 215.53439008734736
At time: 78.01762986183167 and batch: 650, loss is 5.364590387344361 and perplexity is 213.70368101831832
At time: 78.85117077827454 and batch: 700, loss is 5.381301031112671 and perplexity is 217.30481191194434
At time: 79.6850073337555 and batch: 750, loss is 5.338009386062622 and perplexity is 208.0980548282684
At time: 80.50571608543396 and batch: 800, loss is 5.344836215972901 and perplexity is 209.52356517518407
At time: 81.33001232147217 and batch: 850, loss is 5.361678018569946 and perplexity is 213.0822025173093
At time: 82.15817141532898 and batch: 900, loss is 5.334099550247192 and perplexity is 207.2860141077852
At time: 82.99651145935059 and batch: 950, loss is 5.323406744003296 and perplexity is 205.0813529352279
At time: 83.8272168636322 and batch: 1000, loss is 5.312963542938232 and perplexity is 202.95079143618554
At time: 84.68389296531677 and batch: 1050, loss is 5.298807077407837 and perplexity is 200.09796615754777
At time: 85.51625156402588 and batch: 1100, loss is 5.258050575256347 and perplexity is 192.10662861742193
At time: 86.35050058364868 and batch: 1150, loss is 5.273796472549439 and perplexity is 195.15546016317165
At time: 87.17938256263733 and batch: 1200, loss is 5.2652005767822265 and perplexity is 193.4851135171057
At time: 88.00345373153687 and batch: 1250, loss is 5.304901828765869 and perplexity is 201.32123748919753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1531657198049725 and perplexity of 172.9782250486309
Finished 4 epochs...
Completing Train Step...
At time: 90.27915954589844 and batch: 50, loss is 5.346874847412109 and perplexity is 209.95114219029324
At time: 91.1000235080719 and batch: 100, loss is 5.36689564704895 and perplexity is 214.19689177391334
At time: 91.95114994049072 and batch: 150, loss is 5.275834589004517 and perplexity is 195.55361532329502
At time: 92.77674388885498 and batch: 200, loss is 5.320362243652344 and perplexity is 204.4579321681546
At time: 93.6025083065033 and batch: 250, loss is 5.333304271697998 and perplexity is 207.1212295207262
At time: 94.42727088928223 and batch: 300, loss is 5.326756734848022 and perplexity is 205.76952563252155
At time: 95.25252437591553 and batch: 350, loss is 5.360247812271118 and perplexity is 212.77766883400702
At time: 96.07859396934509 and batch: 400, loss is 5.335396184921264 and perplexity is 207.55496266746056
At time: 96.90353202819824 and batch: 450, loss is 5.298305444717407 and perplexity is 199.997615648087
At time: 97.72666883468628 and batch: 500, loss is 5.288441429138183 and perplexity is 198.03453390308428
At time: 98.56786394119263 and batch: 550, loss is 5.298737277984619 and perplexity is 200.0839999223463
At time: 99.40320229530334 and batch: 600, loss is 5.3203802490234375 and perplexity is 204.46161354223847
At time: 100.22774600982666 and batch: 650, loss is 5.310905504226684 and perplexity is 202.53354035757977
At time: 101.04804158210754 and batch: 700, loss is 5.330175447463989 and perplexity is 206.47419635279834
At time: 101.8755795955658 and batch: 750, loss is 5.2871832847595215 and perplexity is 197.785534538894
At time: 102.69644141197205 and batch: 800, loss is 5.294939174652099 and perplexity is 199.32550155486115
At time: 103.51773810386658 and batch: 850, loss is 5.317642011642456 and perplexity is 203.90251493076093
At time: 104.34242153167725 and batch: 900, loss is 5.2918050003051755 and perplexity is 198.70175865129193
At time: 105.17062878608704 and batch: 950, loss is 5.283343677520752 and perplexity is 197.02757183948205
At time: 106.01378893852234 and batch: 1000, loss is 5.271714954376221 and perplexity is 194.74966300983107
At time: 106.8589768409729 and batch: 1050, loss is 5.262459707260132 and perplexity is 192.95552216842134
At time: 107.68192100524902 and batch: 1100, loss is 5.2289728260040285 and perplexity is 186.60103331730153
At time: 108.50578808784485 and batch: 1150, loss is 5.2435975265502925 and perplexity is 189.35007047655435
At time: 109.3293387889862 and batch: 1200, loss is 5.240067739486694 and perplexity is 188.68288325443658
At time: 110.17868280410767 and batch: 1250, loss is 5.2771038150787355 and perplexity is 195.80197464949646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.142960374372719 and perplexity of 171.22189970643439
Finished 5 epochs...
Completing Train Step...
At time: 112.58892583847046 and batch: 50, loss is 5.297527408599853 and perplexity is 199.84207079729052
At time: 113.43625593185425 and batch: 100, loss is 5.317298021316528 and perplexity is 203.83238650063467
At time: 114.26084113121033 and batch: 150, loss is 5.228098468780518 and perplexity is 186.4379486634184
At time: 115.10422730445862 and batch: 200, loss is 5.2750837516784665 and perplexity is 195.4068414782023
At time: 115.93957662582397 and batch: 250, loss is 5.286430149078369 and perplexity is 197.63663127483596
At time: 116.76349544525146 and batch: 300, loss is 5.28172212600708 and perplexity is 196.70834037721073
At time: 117.58856678009033 and batch: 350, loss is 5.314080629348755 and perplexity is 203.1776316837936
At time: 118.41333532333374 and batch: 400, loss is 5.291695165634155 and perplexity is 198.67993550749011
At time: 119.2383680343628 and batch: 450, loss is 5.25370530128479 and perplexity is 191.27368368189826
At time: 120.06371545791626 and batch: 500, loss is 5.245380029678345 and perplexity is 189.68788856092556
At time: 120.88657760620117 and batch: 550, loss is 5.253857049942017 and perplexity is 191.3027114089634
At time: 121.71223092079163 and batch: 600, loss is 5.276608848571778 and perplexity is 195.70508321103532
At time: 122.53603053092957 and batch: 650, loss is 5.269045810699463 and perplexity is 194.23054129168284
At time: 123.39774799346924 and batch: 700, loss is 5.289884977340698 and perplexity is 198.320612733148
At time: 124.25836277008057 and batch: 750, loss is 5.248276348114014 and perplexity is 190.2380814719082
At time: 125.10897159576416 and batch: 800, loss is 5.258074893951416 and perplexity is 192.11130045675017
At time: 125.93545484542847 and batch: 850, loss is 5.283994665145874 and perplexity is 197.15587610827197
At time: 126.78346228599548 and batch: 900, loss is 5.258977031707763 and perplexity is 192.2846895129609
At time: 127.60925912857056 and batch: 950, loss is 5.251063747406006 and perplexity is 190.7690906890604
At time: 128.43376064300537 and batch: 1000, loss is 5.237847528457642 and perplexity is 188.26443213282988
At time: 129.25814509391785 and batch: 1050, loss is 5.231747426986694 and perplexity is 187.1194956581591
At time: 130.0835816860199 and batch: 1100, loss is 5.198496112823486 and perplexity is 180.99983376201286
At time: 130.91876983642578 and batch: 1150, loss is 5.215566711425781 and perplexity is 184.11613213544476
At time: 131.75011610984802 and batch: 1200, loss is 5.215786628723144 and perplexity is 184.1566269102124
At time: 132.5733516216278 and batch: 1250, loss is 5.251465120315552 and perplexity is 190.84567560256832
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.133983751283075 and perplexity of 169.69178316251427
Finished 6 epochs...
Completing Train Step...
At time: 134.87536597251892 and batch: 50, loss is 5.256869440078735 and perplexity is 191.87985866985412
At time: 135.73275876045227 and batch: 100, loss is 5.276886005401611 and perplexity is 195.75933172880667
At time: 136.55743861198425 and batch: 150, loss is 5.187787923812866 and perplexity is 179.07199359099624
At time: 137.38290810585022 and batch: 200, loss is 5.237618455886841 and perplexity is 188.22131085451042
At time: 138.2060432434082 and batch: 250, loss is 5.249410495758057 and perplexity is 190.45396194083904
At time: 139.03031277656555 and batch: 300, loss is 5.246122303009034 and perplexity is 189.82874109083008
At time: 139.8661286830902 and batch: 350, loss is 5.276139335632324 and perplexity is 195.61321870962456
At time: 140.68987321853638 and batch: 400, loss is 5.254174156188965 and perplexity is 191.36338431317932
At time: 141.5144612789154 and batch: 450, loss is 5.2161948871612545 and perplexity is 184.23182575631967
At time: 142.3404667377472 and batch: 500, loss is 5.209808683395385 and perplexity is 183.05903261077407
At time: 143.16785430908203 and batch: 550, loss is 5.2168912506103515 and perplexity is 184.36016274534484
At time: 143.99047684669495 and batch: 600, loss is 5.240981702804565 and perplexity is 188.85541131859253
At time: 144.814368724823 and batch: 650, loss is 5.2361218452453615 and perplexity is 187.93982752574493
At time: 145.63644123077393 and batch: 700, loss is 5.256839618682862 and perplexity is 191.87413662994857
At time: 146.45932149887085 and batch: 750, loss is 5.215797796249389 and perplexity is 184.15868349566009
At time: 147.28563022613525 and batch: 800, loss is 5.228789863586425 and perplexity is 186.56689546418585
At time: 148.11032009124756 and batch: 850, loss is 5.256476306915284 and perplexity is 191.8044391599392
At time: 148.93437504768372 and batch: 900, loss is 5.229648933410645 and perplexity is 186.72723831726967
At time: 149.75724029541016 and batch: 950, loss is 5.220702047348023 and perplexity is 185.06406220913587
At time: 150.58085536956787 and batch: 1000, loss is 5.210023307800293 and perplexity is 183.09832576319508
At time: 151.4039487838745 and batch: 1050, loss is 5.205090103149414 and perplexity is 182.19728857872715
At time: 152.229172706604 and batch: 1100, loss is 5.172332239151001 and perplexity is 176.3255917577009
At time: 153.05283737182617 and batch: 1150, loss is 5.191899185180664 and perplexity is 179.80972081590386
At time: 153.8771266937256 and batch: 1200, loss is 5.193225870132446 and perplexity is 180.0484299776083
At time: 154.74432468414307 and batch: 1250, loss is 5.229240159988404 and perplexity is 186.6509247835719
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.129449774749087 and perplexity of 168.92414613879316
Finished 7 epochs...
Completing Train Step...
At time: 157.00947976112366 and batch: 50, loss is 5.225635747909546 and perplexity is 185.97936894479952
At time: 157.8410849571228 and batch: 100, loss is 5.246024332046509 and perplexity is 189.8101442973383
At time: 158.67901229858398 and batch: 150, loss is 5.156940135955811 and perplexity is 173.63235054935743
At time: 159.5075912475586 and batch: 200, loss is 5.208291969299316 and perplexity is 182.78159484567044
At time: 160.3356430530548 and batch: 250, loss is 5.220112571716308 and perplexity is 184.9550036010115
At time: 161.1575334072113 and batch: 300, loss is 5.216958847045898 and perplexity is 184.37262525640915
At time: 161.97687935829163 and batch: 350, loss is 5.2446331119537355 and perplexity is 189.54626021376143
At time: 162.8048233985901 and batch: 400, loss is 5.222959127426147 and perplexity is 185.4822383681337
At time: 163.63487720489502 and batch: 450, loss is 5.1849831771850585 and perplexity is 178.57044570699506
At time: 164.46490287780762 and batch: 500, loss is 5.17942361831665 and perplexity is 177.58042738451462
At time: 165.29360723495483 and batch: 550, loss is 5.186233234405518 and perplexity is 178.7938085610996
At time: 166.11931920051575 and batch: 600, loss is 5.211394128799438 and perplexity is 183.34949290632483
At time: 166.9612410068512 and batch: 650, loss is 5.206876993179321 and perplexity is 182.52314614627937
At time: 167.80191040039062 and batch: 700, loss is 5.2302965259552 and perplexity is 186.84820064759361
At time: 168.63258481025696 and batch: 750, loss is 5.191309843063355 and perplexity is 179.70378259431956
At time: 169.4589138031006 and batch: 800, loss is 5.205368738174439 and perplexity is 182.24806219811566
At time: 170.28381180763245 and batch: 850, loss is 5.231018695831299 and perplexity is 186.98318552464394
At time: 171.11110019683838 and batch: 900, loss is 5.201216526031494 and perplexity is 181.49289846621005
At time: 171.9371840953827 and batch: 950, loss is 5.192327795028686 and perplexity is 179.886805551474
At time: 172.76281785964966 and batch: 1000, loss is 5.183250646591187 and perplexity is 178.26133479602203
At time: 173.58404636383057 and batch: 1050, loss is 5.181014204025269 and perplexity is 177.86310902953733
At time: 174.4068946838379 and batch: 1100, loss is 5.1508253383636475 and perplexity is 172.57386338580974
At time: 175.23328232765198 and batch: 1150, loss is 5.1705733680725094 and perplexity is 176.01573035697135
At time: 176.17321372032166 and batch: 1200, loss is 5.1724124526977535 and perplexity is 176.33973602607236
At time: 177.02118921279907 and batch: 1250, loss is 5.207800264358521 and perplexity is 182.69174232467287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.124865009836907 and perplexity of 168.15144132923047
Finished 8 epochs...
Completing Train Step...
At time: 179.27532815933228 and batch: 50, loss is 5.199378862380981 and perplexity is 181.15968182769748
At time: 180.12504744529724 and batch: 100, loss is 5.2177182388305665 and perplexity is 184.51268948842917
At time: 180.94419312477112 and batch: 150, loss is 5.130100498199463 and perplexity is 169.03410481448466
At time: 181.76529812812805 and batch: 200, loss is 5.183311977386475 and perplexity is 178.272268040723
At time: 182.59402918815613 and batch: 250, loss is 5.192201128005982 and perplexity is 179.8640212684301
At time: 183.43405961990356 and batch: 300, loss is 5.18940728187561 and perplexity is 179.36221018650127
At time: 184.26420760154724 and batch: 350, loss is 5.2173882007598875 and perplexity is 184.45180332430334
At time: 185.08717560768127 and batch: 400, loss is 5.1966845703125 and perplexity is 180.6722416817492
At time: 185.91160368919373 and batch: 450, loss is 5.15855767250061 and perplexity is 173.91343449221904
At time: 186.73475551605225 and batch: 500, loss is 5.153136720657349 and perplexity is 172.9732089002794
At time: 187.5567181110382 and batch: 550, loss is 5.159472217559815 and perplexity is 174.0725589166004
At time: 188.38014841079712 and batch: 600, loss is 5.184467344284058 and perplexity is 178.47835694920053
At time: 189.22860407829285 and batch: 650, loss is 5.182297973632813 and perplexity is 178.09159091088088
At time: 190.0533208847046 and batch: 700, loss is 5.206370763778686 and perplexity is 182.43077094689738
At time: 190.87641501426697 and batch: 750, loss is 5.168870334625244 and perplexity is 175.7162247873185
At time: 191.7311680316925 and batch: 800, loss is 5.182123250961304 and perplexity is 178.06047699057646
At time: 192.56779599189758 and batch: 850, loss is 5.208883571624756 and perplexity is 182.8897608547051
At time: 193.41759657859802 and batch: 900, loss is 5.1785076713562015 and perplexity is 177.4178476004126
At time: 194.24305200576782 and batch: 950, loss is 5.170121335983277 and perplexity is 175.93618357884293
At time: 195.07655596733093 and batch: 1000, loss is 5.159816122055053 and perplexity is 174.1324335470967
At time: 195.93225169181824 and batch: 1050, loss is 5.160127849578857 and perplexity is 174.1867238808763
At time: 196.75717759132385 and batch: 1100, loss is 5.1308728790283205 and perplexity is 169.16471394996952
At time: 197.62868189811707 and batch: 1150, loss is 5.150364742279053 and perplexity is 172.49439484288567
At time: 198.45281195640564 and batch: 1200, loss is 5.153790292739868 and perplexity is 173.08629631200242
At time: 199.2769856452942 and batch: 1250, loss is 5.188084506988526 and perplexity is 179.12511120803353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.122714077469206 and perplexity of 167.79014764982858
Finished 9 epochs...
Completing Train Step...
At time: 201.63245940208435 and batch: 50, loss is 5.174852418899536 and perplexity is 176.77052436277555
At time: 202.45544505119324 and batch: 100, loss is 5.192350940704346 and perplexity is 179.8909692013158
At time: 203.3069784641266 and batch: 150, loss is 5.107219457626343 and perplexity is 165.21034127198018
At time: 204.13561820983887 and batch: 200, loss is 5.1612734031677245 and perplexity is 174.38637844321752
At time: 204.9790072441101 and batch: 250, loss is 5.167005281448365 and perplexity is 175.38881010185656
At time: 205.80284976959229 and batch: 300, loss is 5.164596176147461 and perplexity is 174.96678854102086
At time: 206.62646293640137 and batch: 350, loss is 5.1924568462371825 and perplexity is 179.9100216591241
At time: 207.44971990585327 and batch: 400, loss is 5.17219539642334 and perplexity is 176.3014645336235
At time: 208.3064787387848 and batch: 450, loss is 5.1332418155670165 and perplexity is 169.5659294613156
At time: 209.13570475578308 and batch: 500, loss is 5.129837646484375 and perplexity is 168.98967974898392
At time: 209.96712517738342 and batch: 550, loss is 5.135573863983154 and perplexity is 169.96182686508115
At time: 210.7992844581604 and batch: 600, loss is 5.160217428207398 and perplexity is 174.20232798759832
At time: 211.61869883537292 and batch: 650, loss is 5.159835367202759 and perplexity is 174.13578478374808
At time: 212.44332456588745 and batch: 700, loss is 5.186444158554077 and perplexity is 178.8315244703963
At time: 213.27175331115723 and batch: 750, loss is 5.1472982406616214 and perplexity is 171.9662506934103
At time: 214.094651222229 and batch: 800, loss is 5.161822633743286 and perplexity is 174.48218308122992
At time: 214.91703510284424 and batch: 850, loss is 5.188326406478882 and perplexity is 179.1684467223528
At time: 215.7398693561554 and batch: 900, loss is 5.157165250778198 and perplexity is 173.67144216499804
At time: 216.56491374969482 and batch: 950, loss is 5.149485759735107 and perplexity is 172.3428418968449
At time: 217.38936567306519 and batch: 1000, loss is 5.13884331703186 and perplexity is 170.51841845732898
At time: 218.21627187728882 and batch: 1050, loss is 5.1396920776367185 and perplexity is 170.66320921096997
At time: 219.0899965763092 and batch: 1100, loss is 5.110913181304932 and perplexity is 165.8217110418754
At time: 219.91331005096436 and batch: 1150, loss is 5.130463047027588 and perplexity is 169.09539904150205
At time: 220.73942875862122 and batch: 1200, loss is 5.13659330368042 and perplexity is 170.13518104546048
At time: 221.56376767158508 and batch: 1250, loss is 5.169945020675659 and perplexity is 175.90516607102515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.120552452811359 and perplexity of 167.42784005691533
Finished 10 epochs...
Completing Train Step...
At time: 223.81075716018677 and batch: 50, loss is 5.151863803863526 and perplexity is 172.75316847407692
At time: 224.6732211112976 and batch: 100, loss is 5.16791543006897 and perplexity is 175.54851265094328
At time: 225.5018389225006 and batch: 150, loss is 5.085551738739014 and perplexity is 161.66911374777854
At time: 226.3268542289734 and batch: 200, loss is 5.140646133422852 and perplexity is 170.826109128646
At time: 227.15130925178528 and batch: 250, loss is 5.145543184280395 and perplexity is 171.6647049200945
At time: 227.9758894443512 and batch: 300, loss is 5.144148750305176 and perplexity is 171.42549664204353
At time: 228.84018659591675 and batch: 350, loss is 5.170947418212891 and perplexity is 176.08158138064525
At time: 229.6744019985199 and batch: 400, loss is 5.1501460456848145 and perplexity is 172.45667503095584
At time: 230.50000524520874 and batch: 450, loss is 5.112354383468628 and perplexity is 166.06086594450332
At time: 231.3246204853058 and batch: 500, loss is 5.108546667098999 and perplexity is 165.42975557405924
At time: 232.14851093292236 and batch: 550, loss is 5.1148575401306156 and perplexity is 166.47706299327834
At time: 232.97471117973328 and batch: 600, loss is 5.139710893630982 and perplexity is 170.66642043914655
At time: 233.79871249198914 and batch: 650, loss is 5.1407785320281985 and perplexity is 170.84872776455634
At time: 234.64443373680115 and batch: 700, loss is 5.166628789901734 and perplexity is 175.32279012624073
At time: 235.4702923297882 and batch: 750, loss is 5.127715978622437 and perplexity is 168.6315198589793
At time: 236.29554104804993 and batch: 800, loss is 5.142826900482178 and perplexity is 171.1990475784499
At time: 237.1321611404419 and batch: 850, loss is 5.170144939422608 and perplexity is 175.94033632688755
At time: 237.96814465522766 and batch: 900, loss is 5.138656702041626 and perplexity is 170.48660013331428
At time: 238.7915506362915 and batch: 950, loss is 5.131947679519653 and perplexity is 169.34663001186308
At time: 239.6648461818695 and batch: 1000, loss is 5.1196434211730955 and perplexity is 167.27571200825787
At time: 240.48761868476868 and batch: 1050, loss is 5.122527589797974 and perplexity is 167.7588597734299
At time: 241.30917716026306 and batch: 1100, loss is 5.094548845291138 and perplexity is 163.13023104818527
At time: 242.13455057144165 and batch: 1150, loss is 5.1144367027282716 and perplexity is 166.40701795835741
At time: 242.95990562438965 and batch: 1200, loss is 5.121064805984497 and perplexity is 167.5136442211811
At time: 243.78414130210876 and batch: 1250, loss is 5.153669586181641 and perplexity is 173.06540492178763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1193602624600825 and perplexity of 167.22835313828205
Finished 11 epochs...
Completing Train Step...
At time: 246.01221346855164 and batch: 50, loss is 5.132418212890625 and perplexity is 169.4263320022978
At time: 246.88396501541138 and batch: 100, loss is 5.148024616241455 and perplexity is 172.09120815599996
At time: 247.7106647491455 and batch: 150, loss is 5.066223850250244 and perplexity is 158.57439467761006
At time: 248.5356297492981 and batch: 200, loss is 5.123099069595337 and perplexity is 167.85475797195073
At time: 249.39049124717712 and batch: 250, loss is 5.127075853347779 and perplexity is 168.52360910289954
At time: 250.22646522521973 and batch: 300, loss is 5.12417498588562 and perplexity is 168.03545282929358
At time: 251.0745530128479 and batch: 350, loss is 5.151654815673828 and perplexity is 172.71706887445933
At time: 251.90221571922302 and batch: 400, loss is 5.1313575172424315 and perplexity is 169.24671750425185
At time: 252.72799348831177 and batch: 450, loss is 5.092392539978027 and perplexity is 162.77885144125312
At time: 253.55370497703552 and batch: 500, loss is 5.089661998748779 and perplexity is 162.3349833519869
At time: 254.3816363811493 and batch: 550, loss is 5.095644054412841 and perplexity is 163.30899063695267
At time: 255.2375087738037 and batch: 600, loss is 5.120502185821533 and perplexity is 167.41942417490495
At time: 256.0870497226715 and batch: 650, loss is 5.122526197433472 and perplexity is 167.75862619211128
At time: 256.9125552177429 and batch: 700, loss is 5.147917022705078 and perplexity is 172.0726932503951
At time: 257.73926424980164 and batch: 750, loss is 5.1110426044464115 and perplexity is 165.84317359748965
At time: 258.56582021713257 and batch: 800, loss is 5.1267243766784665 and perplexity is 168.46438739420572
At time: 259.39205169677734 and batch: 850, loss is 5.153837671279907 and perplexity is 173.0944970822913
At time: 260.21896862983704 and batch: 900, loss is 5.120861206054688 and perplexity is 167.47954192670565
At time: 261.0731439590454 and batch: 950, loss is 5.114686708450318 and perplexity is 166.44862586592643
At time: 261.9011468887329 and batch: 1000, loss is 5.103992204666138 and perplexity is 164.6780251304886
At time: 262.725955247879 and batch: 1050, loss is 5.106769886016846 and perplexity is 165.13608408616093
At time: 263.5498459339142 and batch: 1100, loss is 5.08055609703064 and perplexity is 160.86348676752047
At time: 264.37810587882996 and batch: 1150, loss is 5.099130420684815 and perplexity is 163.87933923817903
At time: 265.20451641082764 and batch: 1200, loss is 5.10757698059082 and perplexity is 165.26941832302595
At time: 266.02974820137024 and batch: 1250, loss is 5.137969188690185 and perplexity is 170.36942860259217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.118519135635265 and perplexity of 167.08775202456107
Finished 12 epochs...
Completing Train Step...
At time: 268.29838347435 and batch: 50, loss is 5.11355149269104 and perplexity is 166.25977797456542
At time: 269.1242039203644 and batch: 100, loss is 5.12888129234314 and perplexity is 168.82814302439974
At time: 269.9502477645874 and batch: 150, loss is 5.048968257904053 and perplexity is 155.86157255563228
At time: 270.77672958374023 and batch: 200, loss is 5.106334686279297 and perplexity is 165.0642325417275
At time: 271.6010248661041 and batch: 250, loss is 5.110434494018555 and perplexity is 165.74235329228256
At time: 272.42570757865906 and batch: 300, loss is 5.1071075916290285 and perplexity is 165.19186088606983
At time: 273.2519021034241 and batch: 350, loss is 5.133287210464477 and perplexity is 169.57362706401094
At time: 274.08041763305664 and batch: 400, loss is 5.112680339813233 and perplexity is 166.11500336008618
At time: 274.9057092666626 and batch: 450, loss is 5.075104160308838 and perplexity is 159.9888556001374
At time: 275.73322677612305 and batch: 500, loss is 5.072999153137207 and perplexity is 159.65243212286023
At time: 276.5590081214905 and batch: 550, loss is 5.078425493240356 and perplexity is 160.52111527141446
At time: 277.40333890914917 and batch: 600, loss is 5.102905893325806 and perplexity is 164.49923065510148
At time: 278.23090744018555 and batch: 650, loss is 5.104403018951416 and perplexity is 164.7456911138082
At time: 279.057719707489 and batch: 700, loss is 5.131644220352173 and perplexity is 169.2952480210664
At time: 279.8830988407135 and batch: 750, loss is 5.094513854980469 and perplexity is 163.1245231705821
At time: 280.7256324291229 and batch: 800, loss is 5.111425495147705 and perplexity is 165.90668556482802
At time: 281.55214500427246 and batch: 850, loss is 5.137112255096436 and perplexity is 170.22349585214312
At time: 282.40595388412476 and batch: 900, loss is 5.105306882858276 and perplexity is 164.89466611423725
At time: 283.23248958587646 and batch: 950, loss is 5.098370962142944 and perplexity is 163.75492692313603
At time: 284.0609710216522 and batch: 1000, loss is 5.087079172134399 and perplexity is 161.91624123846358
At time: 284.8974771499634 and batch: 1050, loss is 5.090403718948364 and perplexity is 162.45543515348325
At time: 285.7241950035095 and batch: 1100, loss is 5.062898092269897 and perplexity is 158.0478906168635
At time: 286.55146837234497 and batch: 1150, loss is 5.083694620132446 and perplexity is 161.36915364548463
At time: 287.3817255496979 and batch: 1200, loss is 5.09345121383667 and perplexity is 162.9512724086954
At time: 288.2104766368866 and batch: 1250, loss is 5.123581628799439 and perplexity is 167.93577737711996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.118616257270757 and perplexity of 167.10398064837088
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 290.48645281791687 and batch: 50, loss is 5.109629278182983 and perplexity is 165.6089486417633
At time: 291.36026334762573 and batch: 100, loss is 5.133736906051635 and perplexity is 169.64990072447893
At time: 292.1886143684387 and batch: 150, loss is 5.04712739944458 and perplexity is 155.57491738803208
At time: 293.0174722671509 and batch: 200, loss is 5.101860809326172 and perplexity is 164.32740494298193
At time: 293.872526884079 and batch: 250, loss is 5.102564067840576 and perplexity is 164.4430102350964
At time: 294.721524477005 and batch: 300, loss is 5.0913739681243895 and perplexity is 162.61313389674837
At time: 295.54624485969543 and batch: 350, loss is 5.1136620998382565 and perplexity is 166.27816851134767
At time: 296.37296295166016 and batch: 400, loss is 5.081301679611206 and perplexity is 160.98346850369163
At time: 297.19802498817444 and batch: 450, loss is 5.042041387557983 and perplexity is 154.78567027037232
At time: 298.0261960029602 and batch: 500, loss is 5.032182455062866 and perplexity is 153.2671466069485
At time: 298.8564875125885 and batch: 550, loss is 5.037510213851928 and perplexity is 154.08589610759847
At time: 299.6815493106842 and batch: 600, loss is 5.056867055892944 and perplexity is 157.09756663903366
At time: 300.508202791214 and batch: 650, loss is 5.064368705749512 and perplexity is 158.2804889644419
At time: 301.33509612083435 and batch: 700, loss is 5.080024166107178 and perplexity is 160.7779412586207
At time: 302.15943574905396 and batch: 750, loss is 5.032913856506347 and perplexity is 153.37928742419373
At time: 303.03243589401245 and batch: 800, loss is 5.038431119918823 and perplexity is 154.22786010186582
At time: 303.8561866283417 and batch: 850, loss is 5.058097200393677 and perplexity is 157.2909382598461
At time: 304.68186044692993 and batch: 900, loss is 5.0231685447692875 and perplexity is 151.89181815231132
At time: 305.50520157814026 and batch: 950, loss is 5.006021223068237 and perplexity is 149.3094836166872
At time: 306.329128742218 and batch: 1000, loss is 4.99081202507019 and perplexity is 147.05578801222845
At time: 307.15563344955444 and batch: 1050, loss is 4.9827987098693844 and perplexity is 145.88209250735602
At time: 307.98025941848755 and batch: 1100, loss is 4.937671041488647 and perplexity is 139.4451092138093
At time: 308.8046028614044 and batch: 1150, loss is 4.949887037277222 and perplexity is 141.15901731386722
At time: 309.6308856010437 and batch: 1200, loss is 4.970574045181275 and perplexity is 144.109589046567
At time: 310.4558641910553 and batch: 1250, loss is 5.030161666870117 and perplexity is 152.95773889576463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.053096576328695 and perplexity of 156.5063487623104
Finished 14 epochs...
Completing Train Step...
At time: 312.7683050632477 and batch: 50, loss is 5.065291652679443 and perplexity is 158.42664089066068
At time: 313.59858298301697 and batch: 100, loss is 5.091537389755249 and perplexity is 162.6397105718316
At time: 314.44910311698914 and batch: 150, loss is 5.0077136039733885 and perplexity is 149.5623860790045
At time: 315.279079914093 and batch: 200, loss is 5.062135410308838 and perplexity is 157.92739629696948
At time: 316.1046025753021 and batch: 250, loss is 5.0669463348388675 and perplexity is 158.68900363053064
At time: 316.941358089447 and batch: 300, loss is 5.058778133392334 and perplexity is 157.3980793239001
At time: 317.7963910102844 and batch: 350, loss is 5.0827490425109865 and perplexity is 161.21663870372535
At time: 318.627897977829 and batch: 400, loss is 5.053880872726441 and perplexity is 156.6291442756139
At time: 319.4544906616211 and batch: 450, loss is 5.015803031921386 and perplexity is 150.7771670419762
At time: 320.3021574020386 and batch: 500, loss is 5.007229948043824 and perplexity is 149.49006683436303
At time: 321.13496804237366 and batch: 550, loss is 5.01364649772644 and perplexity is 150.45236127868023
At time: 321.9626934528351 and batch: 600, loss is 5.0350979137420655 and perplexity is 153.7146426508999
At time: 322.82424330711365 and batch: 650, loss is 5.045194215774536 and perplexity is 155.27445301825026
At time: 323.6772587299347 and batch: 700, loss is 5.061505241394043 and perplexity is 157.8279067119429
At time: 324.564697265625 and batch: 750, loss is 5.016498556137085 and perplexity is 150.88207269080888
At time: 325.3919279575348 and batch: 800, loss is 5.025648488998413 and perplexity is 152.26896885324805
At time: 326.2212598323822 and batch: 850, loss is 5.0481006526947025 and perplexity is 155.7264048879106
At time: 327.04719257354736 and batch: 900, loss is 5.0143852710723875 and perplexity is 150.56355254054083
At time: 327.87338495254517 and batch: 950, loss is 4.999011878967285 and perplexity is 148.2665813687329
At time: 328.69960737228394 and batch: 1000, loss is 4.986505784988403 and perplexity is 146.4238920080558
At time: 329.52868390083313 and batch: 1050, loss is 4.9832367515563964 and perplexity is 145.94600894327246
At time: 330.3584179878235 and batch: 1100, loss is 4.943859195709228 and perplexity is 140.31069247411813
At time: 331.1837885379791 and batch: 1150, loss is 4.961502180099488 and perplexity is 142.8081584247477
At time: 332.01310753822327 and batch: 1200, loss is 4.984472026824951 and perplexity is 146.12640383439938
At time: 332.8857035636902 and batch: 1250, loss is 5.037187871932983 and perplexity is 154.03623576839925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.049707565864507 and perplexity of 155.97684486256816
Finished 15 epochs...
Completing Train Step...
At time: 335.17564725875854 and batch: 50, loss is 5.054572582244873 and perplexity is 156.73752362476566
At time: 336.0328013896942 and batch: 100, loss is 5.079757051467896 and perplexity is 160.7350008520978
At time: 336.85990357398987 and batch: 150, loss is 4.9953693008422855 and perplexity is 147.72749119787332
At time: 337.6843948364258 and batch: 200, loss is 5.047718420028686 and perplexity is 155.66689254350675
At time: 338.50889110565186 and batch: 250, loss is 5.0537034702301025 and perplexity is 156.6013603389628
At time: 339.33481669425964 and batch: 300, loss is 5.045701522827148 and perplexity is 155.35324482748726
At time: 340.16058897972107 and batch: 350, loss is 5.07065242767334 and perplexity is 159.2782109639307
At time: 340.98425674438477 and batch: 400, loss is 5.042267513275147 and perplexity is 154.8206752486826
At time: 341.81093192100525 and batch: 450, loss is 5.004220418930053 and perplexity is 149.0408484329917
At time: 342.6471724510193 and batch: 500, loss is 4.996739091873169 and perplexity is 147.92998564620856
At time: 343.48273158073425 and batch: 550, loss is 5.00375732421875 and perplexity is 148.97184438325266
At time: 344.3086450099945 and batch: 600, loss is 5.0266588687896725 and perplexity is 152.42289609160386
At time: 345.14394187927246 and batch: 650, loss is 5.037218685150147 and perplexity is 154.040982193509
At time: 346.0250897407532 and batch: 700, loss is 5.053846616744995 and perplexity is 156.6237788824526
At time: 346.85136103630066 and batch: 750, loss is 5.0095689010620115 and perplexity is 149.8401263041118
At time: 347.6778917312622 and batch: 800, loss is 5.020855588912964 and perplexity is 151.54090506167415
At time: 348.5062391757965 and batch: 850, loss is 5.044524803161621 and perplexity is 155.17054512344416
At time: 349.3285529613495 and batch: 900, loss is 5.011448669433594 and perplexity is 150.12205593251417
At time: 350.171103477478 and batch: 950, loss is 4.997284488677979 and perplexity is 148.01068819317064
At time: 351.0061242580414 and batch: 1000, loss is 4.986324033737183 and perplexity is 146.3972817007763
At time: 351.83254766464233 and batch: 1050, loss is 4.985013628005982 and perplexity is 146.20556750292891
At time: 352.66048312187195 and batch: 1100, loss is 4.94772084236145 and perplexity is 140.8535703165529
At time: 353.5173270702362 and batch: 1150, loss is 4.967123527526855 and perplexity is 143.61319326909776
At time: 354.34785294532776 and batch: 1200, loss is 4.990865030288696 and perplexity is 147.06358294298877
At time: 355.17468094825745 and batch: 1250, loss is 5.039574174880982 and perplexity is 154.4042518161647
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.048878467866104 and perplexity of 155.8475783673965
Finished 16 epochs...
Completing Train Step...
At time: 357.42913007736206 and batch: 50, loss is 5.047359895706177 and perplexity is 155.61109217981198
At time: 358.30554270744324 and batch: 100, loss is 5.0720725345611575 and perplexity is 159.5045637329224
At time: 359.15412974357605 and batch: 150, loss is 4.98748854637146 and perplexity is 146.56786248740443
At time: 359.9815709590912 and batch: 200, loss is 5.038703660964966 and perplexity is 154.2698992526392
At time: 360.80715107917786 and batch: 250, loss is 5.045234041213989 and perplexity is 155.2806370147169
At time: 361.6343619823456 and batch: 300, loss is 5.037687931060791 and perplexity is 154.11328225640273
At time: 362.45960330963135 and batch: 350, loss is 5.06297679901123 and perplexity is 158.06033054085506
At time: 363.285902261734 and batch: 400, loss is 5.034936180114746 and perplexity is 153.68978383447936
At time: 364.11667227745056 and batch: 450, loss is 4.99693021774292 and perplexity is 147.95826159542395
At time: 364.9440143108368 and batch: 500, loss is 4.990090837478638 and perplexity is 146.94977143616268
At time: 365.76935601234436 and batch: 550, loss is 4.997686824798584 and perplexity is 148.07025022044044
At time: 366.59656047821045 and batch: 600, loss is 5.021478576660156 and perplexity is 151.63534260238703
At time: 367.4849998950958 and batch: 650, loss is 5.03262583732605 and perplexity is 153.33511760873756
At time: 368.31047654151917 and batch: 700, loss is 5.049287548065186 and perplexity is 155.91134556784166
At time: 369.1362884044647 and batch: 750, loss is 5.005673208236694 and perplexity is 149.25753074260828
At time: 369.9621367454529 and batch: 800, loss is 5.017921524047852 and perplexity is 151.0969258668918
At time: 370.7879991531372 and batch: 850, loss is 5.042202291488647 and perplexity is 154.81057789694304
At time: 371.61660528182983 and batch: 900, loss is 5.009820079803466 and perplexity is 149.87776768561585
At time: 372.4449954032898 and batch: 950, loss is 4.9961097621917725 and perplexity is 147.83691820359644
At time: 373.27000308036804 and batch: 1000, loss is 4.986002273559571 and perplexity is 146.3501844628285
At time: 374.0968656539917 and batch: 1050, loss is 4.985842580795288 and perplexity is 146.32681526331407
At time: 374.92387294769287 and batch: 1100, loss is 4.949478244781494 and perplexity is 141.10132435991173
At time: 375.7542338371277 and batch: 1150, loss is 4.970053052902221 and perplexity is 144.0345286179859
At time: 376.58173513412476 and batch: 1200, loss is 4.993652896881104 and perplexity is 147.47414862809748
At time: 377.41144943237305 and batch: 1250, loss is 5.039620819091797 and perplexity is 154.41145404860717
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.04827880859375 and perplexity of 155.7541509370666
Finished 17 epochs...
Completing Train Step...
At time: 379.70297718048096 and batch: 50, loss is 5.041690559387207 and perplexity is 154.73137662123665
At time: 380.52846217155457 and batch: 100, loss is 5.066030197143554 and perplexity is 158.5436892266382
At time: 381.357581615448 and batch: 150, loss is 4.981610689163208 and perplexity is 145.7088844685373
At time: 382.1830861568451 and batch: 200, loss is 5.032085676193237 and perplexity is 153.25231430348694
At time: 383.01078176498413 and batch: 250, loss is 5.039157981872559 and perplexity is 154.34000321692434
At time: 383.83554434776306 and batch: 300, loss is 5.031707801818848 and perplexity is 153.1944151210934
At time: 384.65926790237427 and batch: 350, loss is 5.057341070175171 and perplexity is 157.17205078120256
At time: 385.4838614463806 and batch: 400, loss is 5.029253644943237 and perplexity is 152.81891295299988
At time: 386.3089234828949 and batch: 450, loss is 4.991200046539307 and perplexity is 147.1128598869753
At time: 387.1349136829376 and batch: 500, loss is 4.985125055313111 and perplexity is 146.22185970328414
At time: 388.00674629211426 and batch: 550, loss is 4.993121719360351 and perplexity is 147.3958344766539
At time: 388.8328022956848 and batch: 600, loss is 5.0178924083709715 and perplexity is 151.09252664166422
At time: 389.65658140182495 and batch: 650, loss is 5.029169931411743 and perplexity is 152.80612047757657
At time: 390.4915060997009 and batch: 700, loss is 5.045734176635742 and perplexity is 155.35831778543354
At time: 391.32958936691284 and batch: 750, loss is 5.002661828994751 and perplexity is 148.80873579787095
At time: 392.1540689468384 and batch: 800, loss is 5.01570463180542 and perplexity is 150.76233128118645
At time: 392.9795997142792 and batch: 850, loss is 5.040032958984375 and perplexity is 154.47510628460333
At time: 393.80596709251404 and batch: 900, loss is 5.0081257724761965 and perplexity is 149.6240436895407
At time: 394.63203406333923 and batch: 950, loss is 4.994878425598144 and perplexity is 147.65499322475105
At time: 395.45861315727234 and batch: 1000, loss is 4.985547609329224 and perplexity is 146.28365939327938
At time: 396.2962398529053 and batch: 1050, loss is 4.985829248428344 and perplexity is 146.32486439352417
At time: 397.14682173728943 and batch: 1100, loss is 4.950037393569946 and perplexity is 141.18024305606716
At time: 397.9737482070923 and batch: 1150, loss is 4.971500749588013 and perplexity is 144.24319793620134
At time: 398.8008167743683 and batch: 1200, loss is 4.995186548233033 and perplexity is 147.70049608018778
At time: 399.6301703453064 and batch: 1250, loss is 5.038852348327636 and perplexity is 154.29283894247718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.047985661638914 and perplexity of 155.70849877373962
Finished 18 epochs...
Completing Train Step...
At time: 401.87633633613586 and batch: 50, loss is 5.036932039260864 and perplexity is 153.9968333070329
At time: 402.73100233078003 and batch: 100, loss is 5.0611404609680175 and perplexity is 157.7703446802833
At time: 403.5571258068085 and batch: 150, loss is 4.976618223190307 and perplexity is 144.98325067906424
At time: 404.3823823928833 and batch: 200, loss is 5.02664960861206 and perplexity is 152.42148463504907
At time: 405.2221837043762 and batch: 250, loss is 5.034177694320679 and perplexity is 153.57325651449256
At time: 406.06198740005493 and batch: 300, loss is 5.02681155204773 and perplexity is 152.44617029272686
At time: 406.9102449417114 and batch: 350, loss is 5.052555541992188 and perplexity is 156.4216963557429
At time: 407.7521884441376 and batch: 400, loss is 5.024483261108398 and perplexity is 152.09164413587322
At time: 408.6079754829407 and batch: 450, loss is 4.98648154258728 and perplexity is 146.42034238435764
At time: 409.4913287162781 and batch: 500, loss is 4.981042184829712 and perplexity is 145.62607187817667
At time: 410.3195149898529 and batch: 550, loss is 4.989726076126098 and perplexity is 146.89617961348532
At time: 411.14670062065125 and batch: 600, loss is 5.015046873092651 and perplexity is 150.66319865047885
At time: 411.97430872917175 and batch: 650, loss is 5.026559457778931 and perplexity is 152.40774433058166
At time: 412.8024859428406 and batch: 700, loss is 5.042780017852783 and perplexity is 154.90004188960646
At time: 413.626549243927 and batch: 750, loss is 5.000014810562134 and perplexity is 148.4153572011685
At time: 414.45467948913574 and batch: 800, loss is 5.0135142993927 and perplexity is 150.43247304183674
At time: 415.2808892726898 and batch: 850, loss is 5.0383719539642335 and perplexity is 154.218735333239
At time: 416.12469005584717 and batch: 900, loss is 5.006708517074585 and perplexity is 149.41213840283643
At time: 416.9498767852783 and batch: 950, loss is 4.993617219924927 and perplexity is 147.4688872932144
At time: 417.7746112346649 and batch: 1000, loss is 4.984614372253418 and perplexity is 146.14720574045646
At time: 418.6059341430664 and batch: 1050, loss is 4.985367383956909 and perplexity is 146.25729774189222
At time: 419.4323136806488 and batch: 1100, loss is 4.9500370121002195 and perplexity is 141.18018920008868
At time: 420.2611770629883 and batch: 1150, loss is 4.971899995803833 and perplexity is 144.30079798467074
At time: 421.10264921188354 and batch: 1200, loss is 4.995667400360108 and perplexity is 147.77153525619863
At time: 421.93247079849243 and batch: 1250, loss is 5.037813148498535 and perplexity is 154.13258113498205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.047815476020757 and perplexity of 155.68200168139947
Finished 19 epochs...
Completing Train Step...
At time: 424.2939999103546 and batch: 50, loss is 5.032707786560058 and perplexity is 153.34768381906082
At time: 425.1456174850464 and batch: 100, loss is 5.056922178268433 and perplexity is 157.10622646876334
At time: 425.9760160446167 and batch: 150, loss is 4.972599859237671 and perplexity is 144.4018241848019
At time: 426.80332684516907 and batch: 200, loss is 5.022334356307983 and perplexity is 151.76516458405925
At time: 427.63492012023926 and batch: 250, loss is 5.030211153030396 and perplexity is 152.9653083742382
At time: 428.4715449810028 and batch: 300, loss is 5.022701454162598 and perplexity is 151.8208874776349
At time: 429.29952239990234 and batch: 350, loss is 5.048758926391602 and perplexity is 155.8289492315802
At time: 430.1342008113861 and batch: 400, loss is 5.0204516124725345 and perplexity is 151.4796984701108
At time: 431.0156452655792 and batch: 450, loss is 4.982407255172729 and perplexity is 145.82499745287885
At time: 431.84060311317444 and batch: 500, loss is 4.9776579284667966 and perplexity is 145.13406891947193
At time: 432.67026138305664 and batch: 550, loss is 4.986353893280029 and perplexity is 146.40165312194583
At time: 433.49616956710815 and batch: 600, loss is 5.012248048782348 and perplexity is 150.24210838115047
At time: 434.3215515613556 and batch: 650, loss is 5.024148998260498 and perplexity is 152.04081404554117
At time: 435.14586305618286 and batch: 700, loss is 5.040111827850342 and perplexity is 154.48729004150937
At time: 435.97057151794434 and batch: 750, loss is 4.99746545791626 and perplexity is 148.03747599848165
At time: 436.7961287498474 and batch: 800, loss is 5.01163405418396 and perplexity is 150.14988885219748
At time: 437.62512588500977 and batch: 850, loss is 5.036921949386596 and perplexity is 153.99527950618605
At time: 438.44945883750916 and batch: 900, loss is 5.004854564666748 and perplexity is 149.13539202565863
At time: 439.2754361629486 and batch: 950, loss is 4.992181940078735 and perplexity is 147.25737999373357
At time: 440.1017379760742 and batch: 1000, loss is 4.983664503097534 and perplexity is 146.00845092739624
At time: 440.9294624328613 and batch: 1050, loss is 4.984464330673218 and perplexity is 146.12527922775075
At time: 441.7549252510071 and batch: 1100, loss is 4.9494295406341555 and perplexity is 141.09445230757055
At time: 442.57989287376404 and batch: 1150, loss is 4.971869564056396 and perplexity is 144.29640672604873
At time: 443.4158356189728 and batch: 1200, loss is 4.995446729660034 and perplexity is 147.7389300057067
At time: 444.26916003227234 and batch: 1250, loss is 5.036529378890991 and perplexity is 153.9348373676658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.047408277971031 and perplexity of 155.6186211790216
Finished 20 epochs...
Completing Train Step...
At time: 446.52392315864563 and batch: 50, loss is 5.029044237136841 and perplexity is 152.78691483010778
At time: 447.3760769367218 and batch: 100, loss is 5.0530898475646975 and perplexity is 156.5052956715685
At time: 448.20116353034973 and batch: 150, loss is 4.96890887260437 and perplexity is 143.86982139348032
At time: 449.0276679992676 and batch: 200, loss is 5.018639078140259 and perplexity is 151.20538499238882
At time: 449.85272884368896 and batch: 250, loss is 5.02653489112854 and perplexity is 152.4040002288
At time: 450.6786997318268 and batch: 300, loss is 5.019093589782715 and perplexity is 151.27412522070355
At time: 451.504314661026 and batch: 350, loss is 5.044963912963867 and perplexity is 155.23869699280965
At time: 452.3743302822113 and batch: 400, loss is 5.016933135986328 and perplexity is 150.9476572490438
At time: 453.2000105381012 and batch: 450, loss is 4.978612928390503 and perplexity is 145.27273814822956
At time: 454.0253462791443 and batch: 500, loss is 4.974341678619385 and perplexity is 144.6535652610175
At time: 454.85100984573364 and batch: 550, loss is 4.983376588821411 and perplexity is 145.96641906101712
At time: 455.67595911026 and batch: 600, loss is 5.00968581199646 and perplexity is 149.85764527735566
At time: 456.49999737739563 and batch: 650, loss is 5.021826448440552 and perplexity is 151.68810143511155
At time: 457.3252065181732 and batch: 700, loss is 5.037824764251709 and perplexity is 154.13437151139877
At time: 458.1734471321106 and batch: 750, loss is 4.995026273727417 and perplexity is 147.67682535315703
At time: 458.9995357990265 and batch: 800, loss is 5.009784364700318 and perplexity is 149.8724148812718
At time: 459.82611775398254 and batch: 850, loss is 5.035195856094361 and perplexity is 153.72969856187697
At time: 460.65553402900696 and batch: 900, loss is 5.003226947784424 and perplexity is 148.89285417673742
At time: 461.4891974925995 and batch: 950, loss is 4.990592908859253 and perplexity is 147.02356923511988
At time: 462.32205533981323 and batch: 1000, loss is 4.982175426483154 and perplexity is 145.79119495315805
At time: 463.1746778488159 and batch: 1050, loss is 4.983398351669312 and perplexity is 145.96959574056044
At time: 464.00207471847534 and batch: 1100, loss is 4.948652067184448 and perplexity is 140.98479774927782
At time: 464.83788204193115 and batch: 1150, loss is 4.9713827419281005 and perplexity is 144.22617713826665
At time: 465.66345500946045 and batch: 1200, loss is 4.994906024932861 and perplexity is 147.6590684605684
At time: 466.5265486240387 and batch: 1250, loss is 5.034992818832397 and perplexity is 153.6984888732706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.047152108519617 and perplexity of 155.57876154783227
Finished 21 epochs...
Completing Train Step...
At time: 468.9643054008484 and batch: 50, loss is 5.025349960327149 and perplexity is 152.22351898468352
At time: 469.8368594646454 and batch: 100, loss is 5.049606037139893 and perplexity is 155.96100953632376
At time: 470.66301560401917 and batch: 150, loss is 4.965608720779419 and perplexity is 143.39581172199772
At time: 471.4876356124878 and batch: 200, loss is 5.015254859924316 and perplexity is 150.69453787077424
At time: 472.3122160434723 and batch: 250, loss is 5.023217096328735 and perplexity is 151.8991929159766
At time: 473.18750286102295 and batch: 300, loss is 5.015615510940552 and perplexity is 150.74889581053236
At time: 474.0128264427185 and batch: 350, loss is 5.041620206832886 and perplexity is 154.72049125656886
At time: 474.8389165401459 and batch: 400, loss is 5.0137160110473635 and perplexity is 150.46282008546245
At time: 475.6650650501251 and batch: 450, loss is 4.97520827293396 and perplexity is 144.7789755503292
At time: 476.49129819869995 and batch: 500, loss is 4.971368865966797 and perplexity is 144.22417587529847
At time: 477.33112382888794 and batch: 550, loss is 4.980791301727295 and perplexity is 145.58954134011
At time: 478.1519374847412 and batch: 600, loss is 5.0074231243133545 and perplexity is 149.51894755725135
At time: 478.97598576545715 and batch: 650, loss is 5.019785642623901 and perplexity is 151.37885114261667
At time: 479.8025412559509 and batch: 700, loss is 5.035573959350586 and perplexity is 153.78783525162498
At time: 480.6291480064392 and batch: 750, loss is 4.993004741668702 and perplexity is 147.37859346060483
At time: 481.45592403411865 and batch: 800, loss is 5.007883825302124 and perplexity is 149.58784695402224
At time: 482.282267332077 and batch: 850, loss is 5.033624057769775 and perplexity is 153.4882562782348
At time: 483.10690689086914 and batch: 900, loss is 5.001872272491455 and perplexity is 148.69128926421305
At time: 483.9322626590729 and batch: 950, loss is 4.98895601272583 and perplexity is 146.7831037852919
At time: 484.75845313072205 and batch: 1000, loss is 4.980674629211426 and perplexity is 145.57255603291617
At time: 485.58370327949524 and batch: 1050, loss is 4.9821397399902345 and perplexity is 145.7859922695449
At time: 486.41015124320984 and batch: 1100, loss is 4.947448692321777 and perplexity is 140.8152422275352
At time: 487.23710227012634 and batch: 1150, loss is 4.9707863235473635 and perplexity is 144.140183641838
At time: 488.0628659725189 and batch: 1200, loss is 4.994151306152344 and perplexity is 147.547669431245
At time: 488.89151430130005 and batch: 1250, loss is 5.033431596755982 and perplexity is 153.45871861534152
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.047023355525775 and perplexity of 155.55873160598998
Finished 22 epochs...
Completing Train Step...
At time: 491.1658992767334 and batch: 50, loss is 5.022156047821045 and perplexity is 151.73810597965343
At time: 491.99061584472656 and batch: 100, loss is 5.046492834091186 and perplexity is 155.4762262519
At time: 492.81574416160583 and batch: 150, loss is 4.9623610782623295 and perplexity is 142.9308687799632
At time: 493.6415162086487 and batch: 200, loss is 5.012107934951782 and perplexity is 150.2210588585319
At time: 494.5132029056549 and batch: 250, loss is 5.020126428604126 and perplexity is 151.430447723985
At time: 495.3625056743622 and batch: 300, loss is 5.012561283111572 and perplexity is 150.28917673851825
At time: 496.2078914642334 and batch: 350, loss is 5.038455171585083 and perplexity is 154.23156958349432
At time: 497.04347038269043 and batch: 400, loss is 5.010811719894409 and perplexity is 150.026466204315
At time: 497.8839330673218 and batch: 450, loss is 4.972111492156983 and perplexity is 144.33132030476702
At time: 498.7427668571472 and batch: 500, loss is 4.96868408203125 and perplexity is 143.83748444853097
At time: 499.5802788734436 and batch: 550, loss is 4.978068370819091 and perplexity is 145.1936503146449
At time: 500.40664744377136 and batch: 600, loss is 5.005088863372802 and perplexity is 149.17033834881644
At time: 501.2294969558716 and batch: 650, loss is 5.017681579589844 and perplexity is 151.0606753461378
At time: 502.0533092021942 and batch: 700, loss is 5.033364486694336 and perplexity is 153.44842033683813
At time: 502.87689328193665 and batch: 750, loss is 4.990790824890137 and perplexity is 147.0526704360914
At time: 503.7022726535797 and batch: 800, loss is 5.006163835525513 and perplexity is 149.330778527464
At time: 504.5338361263275 and batch: 850, loss is 5.03181936264038 and perplexity is 153.21150656924883
At time: 505.36556458473206 and batch: 900, loss is 5.000219640731811 and perplexity is 148.445760257592
At time: 506.2070014476776 and batch: 950, loss is 4.987427234649658 and perplexity is 146.5588764348725
At time: 507.03719687461853 and batch: 1000, loss is 4.979131307601929 and perplexity is 145.34806403767325
At time: 507.8616523742676 and batch: 1050, loss is 4.980789623260498 and perplexity is 145.589296973104
At time: 508.6887321472168 and batch: 1100, loss is 4.946353006362915 and perplexity is 140.6610374392706
At time: 509.515371799469 and batch: 1150, loss is 4.969947385787964 and perplexity is 144.01930970907662
At time: 510.3420579433441 and batch: 1200, loss is 4.993245239257813 and perplexity is 147.41404191948226
At time: 511.16620111465454 and batch: 1250, loss is 5.031844635009765 and perplexity is 153.21537863596478
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.046733772667655 and perplexity of 155.51369098569646
Finished 23 epochs...
Completing Train Step...
At time: 513.4302046298981 and batch: 50, loss is 5.018874473571778 and perplexity is 151.24098223878724
At time: 514.2842473983765 and batch: 100, loss is 5.043427486419677 and perplexity is 155.00036727300187
At time: 515.1105983257294 and batch: 150, loss is 4.95940691947937 and perplexity is 142.50925136761487
At time: 515.9877161979675 and batch: 200, loss is 5.009082260131836 and perplexity is 149.7672257053159
At time: 516.8121631145477 and batch: 250, loss is 5.017099342346191 and perplexity is 150.97274779473148
At time: 517.6374671459198 and batch: 300, loss is 5.009599056243896 and perplexity is 149.84464482850228
At time: 518.4612846374512 and batch: 350, loss is 5.035777807235718 and perplexity is 153.81918777206397
At time: 519.2854659557343 and batch: 400, loss is 5.007885074615478 and perplexity is 149.5880338362338
At time: 520.1111850738525 and batch: 450, loss is 4.969047546386719 and perplexity is 143.88977374918116
At time: 520.9373834133148 and batch: 500, loss is 4.9660068702697755 and perplexity is 143.45291605863102
At time: 521.7609014511108 and batch: 550, loss is 4.975494136810303 and perplexity is 144.82036854559212
At time: 522.5880086421967 and batch: 600, loss is 5.00284218788147 and perplexity is 148.8355771962631
At time: 523.4145069122314 and batch: 650, loss is 5.015588207244873 and perplexity is 150.74477986474778
At time: 524.2413594722748 and batch: 700, loss is 5.03108380317688 and perplexity is 153.09885183288273
At time: 525.0755989551544 and batch: 750, loss is 4.988589563369751 and perplexity is 146.7293250656169
At time: 525.8985362052917 and batch: 800, loss is 5.004292907714844 and perplexity is 149.05165261456506
At time: 526.7363862991333 and batch: 850, loss is 5.030028219223023 and perplexity is 152.9373284073002
At time: 527.565948009491 and batch: 900, loss is 4.998652000427246 and perplexity is 148.21323300793415
At time: 528.4176189899445 and batch: 950, loss is 4.985768432617188 and perplexity is 146.31596579879402
At time: 529.2743508815765 and batch: 1000, loss is 4.977677001953125 and perplexity is 145.13683715855112
At time: 530.1293041706085 and batch: 1050, loss is 4.979305801391601 and perplexity is 145.3734285851024
At time: 530.9677712917328 and batch: 1100, loss is 4.945163249969482 and perplexity is 140.4937845854756
At time: 531.8098890781403 and batch: 1150, loss is 4.968882293701172 and perplexity is 143.86599754224147
At time: 532.6376357078552 and batch: 1200, loss is 4.9920538139343265 and perplexity is 147.23851368205894
At time: 533.4784736633301 and batch: 1250, loss is 5.030088672637939 and perplexity is 152.94657427053986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.046879900632984 and perplexity of 155.5364175453935
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 535.8339567184448 and batch: 50, loss is 5.018820390701294 and perplexity is 151.23280291351574
At time: 536.6606197357178 and batch: 100, loss is 5.046515693664551 and perplexity is 155.47978041272364
At time: 537.5322825908661 and batch: 150, loss is 4.962585229873657 and perplexity is 142.96291055548227
At time: 538.3554353713989 and batch: 200, loss is 5.012011117935181 and perplexity is 150.20651560781099
At time: 539.1911346912384 and batch: 250, loss is 5.019274253845214 and perplexity is 151.30145748762183
At time: 540.0156166553497 and batch: 300, loss is 5.011060953140259 and perplexity is 150.06386244745036
At time: 540.8602225780487 and batch: 350, loss is 5.0350054836273195 and perplexity is 153.70043544543847
At time: 541.7034606933594 and batch: 400, loss is 5.004109621047974 and perplexity is 149.02433593743362
At time: 542.5417625904083 and batch: 450, loss is 4.964891328811645 and perplexity is 143.29297760904961
At time: 543.3696773052216 and batch: 500, loss is 4.961050386428833 and perplexity is 142.7436531752825
At time: 544.196375131607 and batch: 550, loss is 4.968936634063721 and perplexity is 143.87381548511942
At time: 545.0219135284424 and batch: 600, loss is 4.995411043167114 and perplexity is 147.7336578155006
At time: 545.8459002971649 and batch: 650, loss is 5.007609901428222 and perplexity is 149.5468768830924
At time: 546.6726751327515 and batch: 700, loss is 5.023442173004151 and perplexity is 151.9333857291745
At time: 547.497688293457 and batch: 750, loss is 4.977264289855957 and perplexity is 145.0769497890821
At time: 548.3232660293579 and batch: 800, loss is 4.990551586151123 and perplexity is 147.01749394860468
At time: 549.1510014533997 and batch: 850, loss is 5.011544094085694 and perplexity is 150.13638196099134
At time: 550.0060043334961 and batch: 900, loss is 4.979056625366211 and perplexity is 145.33720952461843
At time: 550.8315205574036 and batch: 950, loss is 4.966137752532959 and perplexity is 143.4716927296849
At time: 551.6585166454315 and batch: 1000, loss is 4.956666336059571 and perplexity is 142.11922756664563
At time: 552.4849078655243 and batch: 1050, loss is 4.955132789611817 and perplexity is 141.90144816015138
At time: 553.3106033802032 and batch: 1100, loss is 4.917679824829102 and perplexity is 136.68511159708927
At time: 554.1366307735443 and batch: 1150, loss is 4.93927529335022 and perplexity is 139.66899382548908
At time: 554.9659600257874 and batch: 1200, loss is 4.967040672302246 and perplexity is 143.60129465865043
At time: 555.8003096580505 and batch: 1250, loss is 5.01346607208252 and perplexity is 150.42521826323883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.041843386462135 and perplexity of 154.75502557197927
Finished 25 epochs...
Completing Train Step...
At time: 558.0805423259735 and batch: 50, loss is 5.015248508453369 and perplexity is 150.69358074183464
At time: 558.9785215854645 and batch: 100, loss is 5.0406889152526855 and perplexity is 154.57646843980473
At time: 559.8090753555298 and batch: 150, loss is 4.957308511734009 and perplexity is 142.21052238797824
At time: 560.9675090312958 and batch: 200, loss is 5.006644926071167 and perplexity is 149.4026374371238
At time: 561.7934336662292 and batch: 250, loss is 5.014722366333007 and perplexity is 150.6143153559959
At time: 562.6183669567108 and batch: 300, loss is 5.006414880752564 and perplexity is 149.36827201274846
At time: 563.4463903903961 and batch: 350, loss is 5.030745239257812 and perplexity is 153.04702685912952
At time: 564.268040895462 and batch: 400, loss is 5.000155639648438 and perplexity is 148.43625987213406
At time: 565.0930368900299 and batch: 450, loss is 4.961562337875367 and perplexity is 142.81674970434943
At time: 565.9208581447601 and batch: 500, loss is 4.958275966644287 and perplexity is 142.3481712299434
At time: 566.7479465007782 and batch: 550, loss is 4.965940265655518 and perplexity is 143.44336175067681
At time: 567.5774266719818 and batch: 600, loss is 4.9925761318206785 and perplexity is 147.31543907932152
At time: 568.399151802063 and batch: 650, loss is 5.005536451339721 and perplexity is 149.2371201415311
At time: 569.2236309051514 and batch: 700, loss is 5.021253414154053 and perplexity is 151.60120385216038
At time: 570.0497620105743 and batch: 750, loss is 4.975537872314453 and perplexity is 144.8267024759294
At time: 570.8759996891022 and batch: 800, loss is 4.989237241744995 and perplexity is 146.82438925866387
At time: 571.7086007595062 and batch: 850, loss is 5.010804462432861 and perplexity is 150.02537739695632
At time: 572.5405964851379 and batch: 900, loss is 4.978583154678344 and perplexity is 145.268412903929
At time: 573.3708982467651 and batch: 950, loss is 4.965837669372559 and perplexity is 143.42864574986245
At time: 574.2000198364258 and batch: 1000, loss is 4.957042980194092 and perplexity is 142.1727660219509
At time: 575.0231812000275 and batch: 1050, loss is 4.95610746383667 and perplexity is 142.03982326851354
At time: 575.847573518753 and batch: 1100, loss is 4.919122924804688 and perplexity is 136.88250427272425
At time: 576.6731960773468 and batch: 1150, loss is 4.9411763572692875 and perplexity is 139.9347660552188
At time: 577.4970288276672 and batch: 1200, loss is 4.969425611495971 and perplexity is 143.94418373682484
At time: 578.3206391334534 and batch: 1250, loss is 5.014675331115723 and perplexity is 150.60723134554726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.041576079208485 and perplexity of 154.7136639594809
Finished 26 epochs...
Completing Train Step...
At time: 580.586452960968 and batch: 50, loss is 5.013290948867798 and perplexity is 150.3988776219376
At time: 581.4408724308014 and batch: 100, loss is 5.038223438262939 and perplexity is 154.19583313031873
At time: 582.2666156291962 and batch: 150, loss is 4.955071134567261 and perplexity is 141.8926994897451
At time: 583.0913178920746 and batch: 200, loss is 5.004070606231689 and perplexity is 149.0185218937628
At time: 583.916597366333 and batch: 250, loss is 5.012605781555176 and perplexity is 150.29586452177045
At time: 584.7455151081085 and batch: 300, loss is 5.004339675903321 and perplexity is 149.05862365335744
At time: 585.572820186615 and batch: 350, loss is 5.028618040084839 and perplexity is 152.72181137185927
At time: 586.3966500759125 and batch: 400, loss is 4.998108425140381 and perplexity is 148.13268984990302
At time: 587.2210986614227 and batch: 450, loss is 4.959700670242309 and perplexity is 142.55111971805886
At time: 588.0492203235626 and batch: 500, loss is 4.956731567382812 and perplexity is 142.12849849429202
At time: 588.8817739486694 and batch: 550, loss is 4.964419116973877 and perplexity is 143.22532894226126
At time: 589.735681772232 and batch: 600, loss is 4.991198205947876 and perplexity is 147.11258911255527
At time: 590.5710179805756 and batch: 650, loss is 5.004385185241699 and perplexity is 149.06540736705955
At time: 591.4016978740692 and batch: 700, loss is 5.020185098648072 and perplexity is 151.43933241563775
At time: 592.2259047031403 and batch: 750, loss is 4.974893274307251 and perplexity is 144.7333775538847
At time: 593.0488860607147 and batch: 800, loss is 4.988603572845459 and perplexity is 146.73138068093112
At time: 593.8738915920258 and batch: 850, loss is 5.010650844573974 and perplexity is 150.00233258979418
At time: 594.6969261169434 and batch: 900, loss is 4.978551273345947 and perplexity is 145.26378162719644
At time: 595.5607826709747 and batch: 950, loss is 4.965974807739258 and perplexity is 143.44831666886645
At time: 596.3838701248169 and batch: 1000, loss is 4.957629642486572 and perplexity is 142.25619789356728
At time: 597.2082047462463 and batch: 1050, loss is 4.956996479034424 and perplexity is 142.16615497716955
At time: 598.033518075943 and batch: 1100, loss is 4.920266065597534 and perplexity is 137.0390697183096
At time: 598.8605403900146 and batch: 1150, loss is 4.942500495910645 and perplexity is 140.12018181721797
At time: 599.6866130828857 and batch: 1200, loss is 4.970947360992431 and perplexity is 144.16339747784153
At time: 600.5161786079407 and batch: 1250, loss is 5.01529335975647 and perplexity is 150.70033969687267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0414673742586675 and perplexity of 154.69684673247855
Finished 27 epochs...
Completing Train Step...
At time: 602.7901802062988 and batch: 50, loss is 5.011869735717774 and perplexity is 150.18528057872845
At time: 603.6161930561066 and batch: 100, loss is 5.0364851379394535 and perplexity is 153.92802729462903
At time: 604.4425318241119 and batch: 150, loss is 4.95344313621521 and perplexity is 141.66188634149876
At time: 605.2686278820038 and batch: 200, loss is 5.002277908325195 and perplexity is 148.75161601382214
At time: 606.0937457084656 and batch: 250, loss is 5.011068992614746 and perplexity is 150.0650688868935
At time: 606.9258897304535 and batch: 300, loss is 5.002851247787476 and perplexity is 148.83692563871122
At time: 607.7476692199707 and batch: 350, loss is 5.027041034698486 and perplexity is 152.4811580583724
At time: 608.5728859901428 and batch: 400, loss is 4.996624069213867 and perplexity is 147.91297132440403
At time: 609.4046764373779 and batch: 450, loss is 4.958390979766846 and perplexity is 142.36454407913536
At time: 610.2308478355408 and batch: 500, loss is 4.955603847503662 and perplexity is 141.96830770329288
At time: 611.0562875270844 and batch: 550, loss is 4.963333644866943 and perplexity is 143.06994618976654
At time: 611.8814074993134 and batch: 600, loss is 4.990178184509277 and perplexity is 146.962607622944
At time: 612.705751657486 and batch: 650, loss is 5.003557357788086 and perplexity is 148.9420579935002
At time: 613.5319113731384 and batch: 700, loss is 5.01946325302124 and perplexity is 151.33005604088487
At time: 614.3590843677521 and batch: 750, loss is 4.974488124847412 and perplexity is 144.67475078125267
At time: 615.1870245933533 and batch: 800, loss is 4.988197317123413 and perplexity is 146.67178232483207
At time: 616.0116300582886 and batch: 850, loss is 5.0104969787597655 and perplexity is 149.97925413429525
At time: 616.9072999954224 and batch: 900, loss is 4.978520021438599 and perplexity is 145.2592419278895
At time: 617.7310557365417 and batch: 950, loss is 4.966092863082886 and perplexity is 143.4652525088472
At time: 618.5577352046967 and batch: 1000, loss is 4.958119916915893 and perplexity is 142.32595956959253
At time: 619.3889701366425 and batch: 1050, loss is 4.957767391204834 and perplexity is 142.27579485218894
At time: 620.2156987190247 and batch: 1100, loss is 4.921055593490601 and perplexity is 137.1473086094813
At time: 621.0412521362305 and batch: 1150, loss is 4.943430032730102 and perplexity is 140.25048923875667
At time: 621.8657531738281 and batch: 1200, loss is 4.97195611000061 and perplexity is 144.3088955352356
At time: 622.6917572021484 and batch: 1250, loss is 5.015614709854126 and perplexity is 150.74877504768662
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.041332829607664 and perplexity of 154.6760344993423
Finished 28 epochs...
Completing Train Step...
At time: 624.9672734737396 and batch: 50, loss is 5.010594482421875 and perplexity is 149.99387837376065
At time: 625.8504803180695 and batch: 100, loss is 5.035051412582398 and perplexity is 153.70749490794918
At time: 626.6767275333405 and batch: 150, loss is 4.952014799118042 and perplexity is 141.45968985073264
At time: 627.5034999847412 and batch: 200, loss is 5.000851860046387 and perplexity is 148.53964020762558
At time: 628.3291866779327 and batch: 250, loss is 5.009822845458984 and perplexity is 149.87818219646425
At time: 629.1561210155487 and batch: 300, loss is 5.001658067703247 and perplexity is 148.65944228909697
At time: 629.9815320968628 and batch: 350, loss is 5.025761737823486 and perplexity is 152.28621411154998
At time: 630.8091652393341 and batch: 400, loss is 4.995451374053955 and perplexity is 147.7396161650885
At time: 631.6368632316589 and batch: 450, loss is 4.957315979003906 and perplexity is 142.2115843162959
At time: 632.4630451202393 and batch: 500, loss is 4.954712800979614 and perplexity is 141.84186367830935
At time: 633.3071312904358 and batch: 550, loss is 4.96245509147644 and perplexity is 142.94430680199872
At time: 634.1422185897827 and batch: 600, loss is 4.989347610473633 and perplexity is 146.84059497412508
At time: 634.9679334163666 and batch: 650, loss is 5.002924385070801 and perplexity is 148.84781156518966
At time: 635.7982542514801 and batch: 700, loss is 5.018987493515015 and perplexity is 151.2580764519903
At time: 636.6346142292023 and batch: 750, loss is 4.974180574417114 and perplexity is 144.63026284088988
At time: 637.4640367031097 and batch: 800, loss is 4.987917642593384 and perplexity is 146.63076769868232
At time: 638.3498106002808 and batch: 850, loss is 5.010411434173584 and perplexity is 149.9664247698136
At time: 639.2035841941833 and batch: 900, loss is 4.978539848327637 and perplexity is 145.26212199531224
At time: 640.0305962562561 and batch: 950, loss is 4.966237668991089 and perplexity is 143.4860286292485
At time: 640.8580360412598 and batch: 1000, loss is 4.958587026596069 and perplexity is 142.39245693261967
At time: 641.6836202144623 and batch: 1050, loss is 4.958385105133057 and perplexity is 142.3637077420309
At time: 642.5096426010132 and batch: 1100, loss is 4.921700611114502 and perplexity is 137.23579957665368
At time: 643.336489200592 and batch: 1150, loss is 4.944133729934692 and perplexity is 140.34921784942588
At time: 644.1630268096924 and batch: 1200, loss is 4.972750415802002 and perplexity is 144.4235664640147
At time: 645.0022897720337 and batch: 1250, loss is 5.015821952819824 and perplexity is 150.7800199084298
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.041270903427235 and perplexity of 154.6664562998956
Finished 29 epochs...
Completing Train Step...
At time: 647.3557260036469 and batch: 50, loss is 5.009587316513062 and perplexity is 149.84288570303084
At time: 648.1881358623505 and batch: 100, loss is 5.033834743499756 and perplexity is 153.5205974703464
At time: 649.0182440280914 and batch: 150, loss is 4.950875663757325 and perplexity is 141.2986398621912
At time: 649.8499276638031 and batch: 200, loss is 4.999610691070557 and perplexity is 148.35539177988798
At time: 650.6782023906708 and batch: 250, loss is 5.008821477890015 and perplexity is 149.72817416464056
At time: 651.5070195198059 and batch: 300, loss is 5.000665245056152 and perplexity is 148.5119230704158
At time: 652.3383677005768 and batch: 350, loss is 5.0246821308135985 and perplexity is 152.12189356405372
At time: 653.168755531311 and batch: 400, loss is 4.9944593334198 and perplexity is 147.59312513711848
At time: 653.9987487792969 and batch: 450, loss is 4.95642201423645 and perplexity is 142.08450897929305
At time: 654.8300142288208 and batch: 500, loss is 4.953997564315796 and perplexity is 141.74044944885327
At time: 655.6635205745697 and batch: 550, loss is 4.9617900562286374 and perplexity is 142.8492754026203
At time: 656.48792552948 and batch: 600, loss is 4.988670358657837 and perplexity is 146.7411805826347
At time: 657.313346862793 and batch: 650, loss is 5.002390470504761 and perplexity is 148.7683607623296
At time: 658.1402535438538 and batch: 700, loss is 5.018570213317871 and perplexity is 151.1949726189345
At time: 658.964364528656 and batch: 750, loss is 4.973920183181763 and perplexity is 144.59260729087987
At time: 659.8365161418915 and batch: 800, loss is 4.987666492462158 and perplexity is 146.59394598621552
At time: 660.6622083187103 and batch: 850, loss is 5.010368070602417 and perplexity is 149.95992183107683
At time: 661.4881415367126 and batch: 900, loss is 4.978522510528564 and perplexity is 145.2596034916609
At time: 662.314154624939 and batch: 950, loss is 4.966352643966675 and perplexity is 143.50252688031196
At time: 663.1391496658325 and batch: 1000, loss is 4.958914709091187 and perplexity is 142.4391240937816
At time: 663.9829630851746 and batch: 1050, loss is 4.958827562332154 and perplexity is 142.42671152662268
At time: 664.8241286277771 and batch: 1100, loss is 4.9222118949890135 and perplexity is 137.30598396852795
At time: 665.6708548069 and batch: 1150, loss is 4.944676923751831 and perplexity is 140.42547538623816
At time: 666.5131597518921 and batch: 1200, loss is 4.973352146148682 and perplexity is 144.5104966583768
At time: 667.3422152996063 and batch: 1250, loss is 5.015964469909668 and perplexity is 150.80151016940215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.041243727189781 and perplexity of 154.66225310466675
Finished 30 epochs...
Completing Train Step...
At time: 669.614143371582 and batch: 50, loss is 5.008686599731445 and perplexity is 149.70798046609826
At time: 670.4678602218628 and batch: 100, loss is 5.032749223709106 and perplexity is 153.35403824154506
At time: 671.2935135364532 and batch: 150, loss is 4.949840755462646 and perplexity is 141.15248436958166
At time: 672.1202070713043 and batch: 200, loss is 4.99853663444519 and perplexity is 148.19613522902458
At time: 672.9435999393463 and batch: 250, loss is 5.007964191436767 and perplexity is 149.59986923415715
At time: 673.7729284763336 and batch: 300, loss is 4.999800443649292 and perplexity is 148.38354526906159
At time: 674.5972270965576 and batch: 350, loss is 5.0237384700775145 and perplexity is 151.97840981663558
At time: 675.4286513328552 and batch: 400, loss is 4.993574514389038 and perplexity is 147.46258968982784
At time: 676.257241487503 and batch: 450, loss is 4.955620574951172 and perplexity is 141.97068249057003
At time: 677.0847275257111 and batch: 500, loss is 4.953366422653199 and perplexity is 141.65101937042363
At time: 677.9093329906464 and batch: 550, loss is 4.961214752197265 and perplexity is 142.76711727381996
At time: 678.737485408783 and batch: 600, loss is 4.988066902160645 and perplexity is 146.65265537704818
At time: 679.5638558864594 and batch: 650, loss is 5.001904439926148 and perplexity is 148.69607235847937
At time: 680.4676101207733 and batch: 700, loss is 5.018163814544677 and perplexity is 151.13353965153587
At time: 681.3194737434387 and batch: 750, loss is 4.973642864227295 and perplexity is 144.5525145796935
At time: 682.1492435932159 and batch: 800, loss is 4.987445344924927 and perplexity is 146.5615306805023
At time: 682.9763078689575 and batch: 850, loss is 5.010294713973999 and perplexity is 149.94892168028542
At time: 683.8012328147888 and batch: 900, loss is 4.978465223312378 and perplexity is 145.25128221170635
At time: 684.6428520679474 and batch: 950, loss is 4.966435298919678 and perplexity is 143.5143885651338
At time: 685.471700668335 and batch: 1000, loss is 4.959152059555054 and perplexity is 142.47293609844684
At time: 686.299218416214 and batch: 1050, loss is 4.959090194702148 and perplexity is 142.46412230384698
At time: 687.1282558441162 and batch: 1100, loss is 4.922551460266114 and perplexity is 137.35261622992203
At time: 687.9766693115234 and batch: 1150, loss is 4.9449794960021975 and perplexity is 140.46797066695675
At time: 688.8064830303192 and batch: 1200, loss is 4.973729743957519 and perplexity is 144.56507380872677
At time: 689.6299378871918 and batch: 1250, loss is 5.0160086059570315 and perplexity is 150.8081660988796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.041246845774407 and perplexity of 154.66273543274363
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 691.9131791591644 and batch: 50, loss is 5.008467149734497 and perplexity is 149.67513065481924
At time: 692.7406899929047 and batch: 100, loss is 5.032675733566284 and perplexity is 153.34276864547957
At time: 693.565572977066 and batch: 150, loss is 4.9498198699951175 and perplexity is 141.14953636473808
At time: 694.3888108730316 and batch: 200, loss is 4.998279714584351 and perplexity is 148.1580655892173
At time: 695.2162523269653 and batch: 250, loss is 5.008025646209717 and perplexity is 149.6090631426561
At time: 696.0437431335449 and batch: 300, loss is 5.000006113052368 and perplexity is 148.4140663627634
At time: 696.8686938285828 and batch: 350, loss is 5.023035907745362 and perplexity is 151.87167300961835
At time: 697.6940062046051 and batch: 400, loss is 4.9925073146820065 and perplexity is 147.305301601142
At time: 698.5184359550476 and batch: 450, loss is 4.954696846008301 and perplexity is 141.839600613497
At time: 699.3573505878448 and batch: 500, loss is 4.95265414237976 and perplexity is 141.55016006792363
At time: 700.21058177948 and batch: 550, loss is 4.959409484863281 and perplexity is 142.50961695902438
At time: 701.0660042762756 and batch: 600, loss is 4.986134243011475 and perplexity is 146.36949949092693
At time: 701.9394750595093 and batch: 650, loss is 5.000443296432495 and perplexity is 148.47896471116024
At time: 702.7670500278473 and batch: 700, loss is 5.016737546920776 and perplexity is 150.91813642488458
At time: 703.5945286750793 and batch: 750, loss is 4.971679859161377 and perplexity is 144.26903558766062
At time: 704.4199140071869 and batch: 800, loss is 4.985693912506104 and perplexity is 146.30506272302364
At time: 705.2811253070831 and batch: 850, loss is 5.007385931015015 and perplexity is 149.51338655784397
At time: 706.1175854206085 and batch: 900, loss is 4.975411319732666 and perplexity is 144.80837544251065
At time: 706.9444601535797 and batch: 950, loss is 4.963526802062988 and perplexity is 143.09758384853126
At time: 707.7694470882416 and batch: 1000, loss is 4.955648422241211 and perplexity is 141.97463604439005
At time: 708.5966448783875 and batch: 1050, loss is 4.955443992614746 and perplexity is 141.94561518903834
At time: 709.4248058795929 and batch: 1100, loss is 4.918861703872681 and perplexity is 136.84675236715904
At time: 710.2511293888092 and batch: 1150, loss is 4.940567235946656 and perplexity is 139.84955476008014
At time: 711.0762794017792 and batch: 1200, loss is 4.969826488494873 and perplexity is 144.00189921684193
At time: 711.900600194931 and batch: 1250, loss is 5.013682842254639 and perplexity is 150.4578294981366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.041482967181797 and perplexity of 154.69925892732456
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 714.168463230133 and batch: 50, loss is 5.00842432975769 and perplexity is 149.66872170641253
At time: 714.994419336319 and batch: 100, loss is 5.032287063598633 and perplexity is 153.28318049736254
At time: 715.8201858997345 and batch: 150, loss is 4.949455881118775 and perplexity is 141.09816885276902
At time: 716.6477346420288 and batch: 200, loss is 4.99813401222229 and perplexity is 148.13648018166322
At time: 717.4769089221954 and batch: 250, loss is 5.007634286880493 and perplexity is 149.5505236957853
At time: 718.3060004711151 and batch: 300, loss is 4.999430046081543 and perplexity is 148.32859454223603
At time: 719.159029006958 and batch: 350, loss is 5.0226836776733395 and perplexity is 151.81818865924728
At time: 719.9931366443634 and batch: 400, loss is 4.992125577926636 and perplexity is 147.2490804847759
At time: 720.8568515777588 and batch: 450, loss is 4.954525566101074 and perplexity is 141.8153084203045
At time: 721.7051777839661 and batch: 500, loss is 4.95252628326416 and perplexity is 141.5320627466227
At time: 722.5343174934387 and batch: 550, loss is 4.959120368957519 and perplexity is 142.46842111751099
At time: 723.4109163284302 and batch: 600, loss is 4.985764389038086 and perplexity is 146.31537415980864
At time: 724.2373130321503 and batch: 650, loss is 5.000313568115234 and perplexity is 148.45970403427424
At time: 725.0631830692291 and batch: 700, loss is 5.0168256664276125 and perplexity is 150.93143584259946
At time: 725.8895266056061 and batch: 750, loss is 4.971749238967895 and perplexity is 144.279045292668
At time: 726.716924905777 and batch: 800, loss is 4.9854232025146485 and perplexity is 146.26546184116305
At time: 727.5444915294647 and batch: 850, loss is 5.006940393447876 and perplexity is 149.44678756462076
At time: 728.3975484371185 and batch: 900, loss is 4.97481858253479 and perplexity is 144.72256756509455
At time: 729.2217781543732 and batch: 950, loss is 4.963018856048584 and perplexity is 143.02491645826254
At time: 730.0526006221771 and batch: 1000, loss is 4.955172357559204 and perplexity is 141.90706302026982
At time: 730.8851835727692 and batch: 1050, loss is 4.954996471405029 and perplexity is 141.88210572759044
At time: 731.7099800109863 and batch: 1100, loss is 4.918343677520752 and perplexity is 136.7758805015888
At time: 732.5583598613739 and batch: 1150, loss is 4.940034551620483 and perplexity is 139.77507893205143
At time: 733.3864035606384 and batch: 1200, loss is 4.969309415817261 and perplexity is 143.927459016388
At time: 734.2123866081238 and batch: 1250, loss is 5.013300819396973 and perplexity is 150.40036214577347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.041404557054061 and perplexity of 154.6871294142159
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 736.5059707164764 and batch: 50, loss is 5.00836763381958 and perplexity is 149.6602363383748
At time: 737.3762776851654 and batch: 100, loss is 5.032239322662353 and perplexity is 153.27586278948814
At time: 738.2026028633118 and batch: 150, loss is 4.949321460723877 and perplexity is 141.0792036558757
At time: 739.0321052074432 and batch: 200, loss is 4.998030548095703 and perplexity is 148.12115416298576
At time: 739.8591763973236 and batch: 250, loss is 5.007580137252807 and perplexity is 149.5424258098577
At time: 740.6873068809509 and batch: 300, loss is 4.999421806335449 and perplexity is 148.32737235731372
At time: 741.5145945549011 and batch: 350, loss is 5.022605085372925 and perplexity is 151.806257387415
At time: 742.3430843353271 and batch: 400, loss is 4.99208345413208 and perplexity is 147.24287792539965
At time: 743.1714305877686 and batch: 450, loss is 4.954502563476563 and perplexity is 141.8120463335334
At time: 744.0469131469727 and batch: 500, loss is 4.9525300407409665 and perplexity is 141.5325945510649
At time: 744.8746495246887 and batch: 550, loss is 4.959071807861328 and perplexity is 142.46150286278925
At time: 745.70215010643 and batch: 600, loss is 4.985690279006958 and perplexity is 146.30453112466896
At time: 746.5318424701691 and batch: 650, loss is 5.00023624420166 and perplexity is 148.44822499275816
At time: 747.3621182441711 and batch: 700, loss is 5.0167544651031495 and perplexity is 150.92068970703843
At time: 748.1932337284088 and batch: 750, loss is 4.97169135093689 and perplexity is 144.27069350455733
At time: 749.0232682228088 and batch: 800, loss is 4.985374870300293 and perplexity is 146.2583926783441
At time: 749.8503413200378 and batch: 850, loss is 5.006890830993652 and perplexity is 149.4393807986034
At time: 750.6772568225861 and batch: 900, loss is 4.9747453212738035 and perplexity is 144.7119653956704
At time: 751.5047726631165 and batch: 950, loss is 4.962953310012818 and perplexity is 143.01554204920393
At time: 752.3306963443756 and batch: 1000, loss is 4.9551113414764405 and perplexity is 141.89840467131984
At time: 753.1581659317017 and batch: 1050, loss is 4.95494668006897 and perplexity is 141.87504140385585
At time: 753.9846284389496 and batch: 1100, loss is 4.918266611099243 and perplexity is 136.76534008009136
At time: 754.8129532337189 and batch: 1150, loss is 4.939954299926757 and perplexity is 139.76386219531355
At time: 755.6423997879028 and batch: 1200, loss is 4.969239950180054 and perplexity is 143.917461350987
At time: 756.4713571071625 and batch: 1250, loss is 5.013254041671753 and perplexity is 150.39332692350723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.041393864763914 and perplexity of 154.6854754633885
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 758.7403745651245 and batch: 50, loss is 5.008363647460937 and perplexity is 149.65963974018737
At time: 759.567085981369 and batch: 100, loss is 5.0322355365753175 and perplexity is 153.27528247482968
At time: 760.3929312229156 and batch: 150, loss is 4.949309501647949 and perplexity is 141.07751648905582
At time: 761.2375366687775 and batch: 200, loss is 4.998021221160888 and perplexity is 148.11977265307883
At time: 762.0789420604706 and batch: 250, loss is 5.007575349807739 and perplexity is 149.54170988542253
At time: 762.9088201522827 and batch: 300, loss is 4.999420719146729 and perplexity is 148.32721109755525
At time: 763.7401683330536 and batch: 350, loss is 5.022597589492798 and perplexity is 151.80511947017192
At time: 764.5854530334473 and batch: 400, loss is 4.992080087661743 and perplexity is 147.2423822374531
At time: 765.487223148346 and batch: 450, loss is 4.954501152038574 and perplexity is 141.8118461747652
At time: 766.3172059059143 and batch: 500, loss is 4.952532014846802 and perplexity is 141.5328739516615
At time: 767.1438426971436 and batch: 550, loss is 4.959066925048828 and perplexity is 142.46080725168054
At time: 767.9745492935181 and batch: 600, loss is 4.985681943893432 and perplexity is 146.3033116648749
At time: 768.8029654026031 and batch: 650, loss is 5.0002267551422115 and perplexity is 148.44681636540946
At time: 769.6301784515381 and batch: 700, loss is 5.0167450332641605 and perplexity is 150.9192662541059
At time: 770.4563543796539 and batch: 750, loss is 4.971683359146118 and perplexity is 144.26954052796745
At time: 771.284138917923 and batch: 800, loss is 4.9853692150115965 and perplexity is 146.257565547248
At time: 772.1110424995422 and batch: 850, loss is 5.006885452270508 and perplexity is 149.43857700770891
At time: 772.9379131793976 and batch: 900, loss is 4.97473690032959 and perplexity is 144.71074678941363
At time: 773.765882730484 and batch: 950, loss is 4.962945985794067 and perplexity is 143.0144945759252
At time: 774.5960788726807 and batch: 1000, loss is 4.9551045608520505 and perplexity is 141.89744251479826
At time: 775.4209389686584 and batch: 1050, loss is 4.9549408054351805 and perplexity is 141.8742079423919
At time: 776.2481343746185 and batch: 1100, loss is 4.91825701713562 and perplexity is 136.7640279646879
At time: 777.07550573349 and batch: 1150, loss is 4.939944400787353 and perplexity is 139.76247866020591
At time: 777.9015529155731 and batch: 1200, loss is 4.969231615066528 and perplexity is 143.91626178760757
At time: 778.728199005127 and batch: 1250, loss is 5.013248262405395 and perplexity is 150.39245776292404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.041392973739735 and perplexity of 154.68533763495117
Annealing...
Model not improving. Stopping early with 154.66225310466675loss at 34 epochs.
Finished Training.
Improved accuracyfrom -163.72543770352107 to -154.66225310466675
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f106faa1128>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'seq_len': 35, 'num_layers': 1, 'data': 'wikitext', 'dropout': 0.569224735752856, 'wordvec_source': '', 'lr': 19.825360077865398, 'tune_wordvecs': True, 'batch_size': 50, 'anneal': 3.400308428814557}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.3761963844299316 and batch: 50, loss is 7.107709836959839 and perplexity is 1221.347256409864
At time: 2.2015528678894043 and batch: 100, loss is 6.456477632522583 and perplexity is 636.8140084756453
At time: 3.0277271270751953 and batch: 150, loss is 6.253439321517944 and perplexity is 519.7975046076158
At time: 3.8540444374084473 and batch: 200, loss is 6.223302497863769 and perplexity is 504.36615275479676
At time: 4.679486989974976 and batch: 250, loss is 6.226182374954224 and perplexity is 505.82075882132153
At time: 5.548037052154541 and batch: 300, loss is 6.211749448776245 and perplexity is 498.57271620115523
At time: 6.375188112258911 and batch: 350, loss is 6.2215775108337406 and perplexity is 503.4968776426061
At time: 7.21582555770874 and batch: 400, loss is 6.1545945739746095 and perplexity is 470.87589841265424
At time: 8.059451580047607 and batch: 450, loss is 6.128529443740844 and perplexity is 458.7610304882946
At time: 8.885383605957031 and batch: 500, loss is 6.120900859832764 and perplexity is 455.27464845994535
At time: 9.713217973709106 and batch: 550, loss is 6.122696704864502 and perplexity is 456.0929857590128
At time: 10.538950204849243 and batch: 600, loss is 6.137579774856567 and perplexity is 462.93181473792384
At time: 11.365531206130981 and batch: 650, loss is 6.108443555831909 and perplexity is 449.63833329476387
At time: 12.193120241165161 and batch: 700, loss is 6.129454708099365 and perplexity is 459.18570215536505
At time: 13.01963186264038 and batch: 750, loss is 6.070581521987915 and perplexity is 432.9323680780831
At time: 13.84580135345459 and batch: 800, loss is 6.070455265045166 and perplexity is 432.87771081137487
At time: 14.685981273651123 and batch: 850, loss is 6.116563415527343 and perplexity is 453.3041964806833
At time: 15.51276683807373 and batch: 900, loss is 6.113186922073364 and perplexity is 451.77619891717796
At time: 16.338695287704468 and batch: 950, loss is 6.083326234817505 and perplexity is 438.4852767349924
At time: 17.164944887161255 and batch: 1000, loss is 6.085605220794678 and perplexity is 439.4857180947776
At time: 17.99091649055481 and batch: 1050, loss is 6.075932340621948 and perplexity is 435.25511942913835
At time: 18.817969799041748 and batch: 1100, loss is 6.068181762695312 and perplexity is 431.8946802032697
At time: 19.6458523273468 and batch: 1150, loss is 6.095582618713379 and perplexity is 443.89258998294576
At time: 20.48444890975952 and batch: 1200, loss is 6.09274806022644 and perplexity is 442.63613206771805
At time: 21.340303897857666 and batch: 1250, loss is 6.063148670196533 and perplexity is 429.7263755443257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.462583221658303 and perplexity of 235.7055182061075
Finished 1 epochs...
Completing Train Step...
At time: 23.645052433013916 and batch: 50, loss is 5.793011341094971 and perplexity is 327.99925632999435
At time: 24.469905853271484 and batch: 100, loss is 5.838663873672485 and perplexity is 343.32031476363323
At time: 25.29788613319397 and batch: 150, loss is 5.7409687519073485 and perplexity is 311.36590123388936
At time: 26.11943793296814 and batch: 200, loss is 5.779839715957642 and perplexity is 323.7073011440743
At time: 26.941553115844727 and batch: 250, loss is 5.761402368545532 and perplexity is 317.79368043279317
At time: 27.76473379135132 and batch: 300, loss is 5.702781028747559 and perplexity is 299.6997166112889
At time: 28.5868821144104 and batch: 350, loss is 5.794281854629516 and perplexity is 328.41624866559295
At time: 29.40989112854004 and batch: 400, loss is 5.7262013721466065 and perplexity is 306.8016268584088
At time: 30.233488082885742 and batch: 450, loss is 5.701874179840088 and perplexity is 299.4280574462362
At time: 31.087329864501953 and batch: 500, loss is 5.694483680725098 and perplexity is 297.2232918470343
At time: 31.909066200256348 and batch: 550, loss is 5.706416530609131 and perplexity is 300.7912584378825
At time: 32.73831820487976 and batch: 600, loss is 5.696666622161866 and perplexity is 297.87282157152475
At time: 33.56010150909424 and batch: 650, loss is 5.728951377868652 and perplexity is 307.64649425043893
At time: 34.382291316986084 and batch: 700, loss is 5.77959508895874 and perplexity is 323.62812328338913
At time: 35.20514941215515 and batch: 750, loss is 5.744144906997681 and perplexity is 312.3564198136713
At time: 36.02797985076904 and batch: 800, loss is 5.720766935348511 and perplexity is 305.1388550141748
At time: 36.852055311203 and batch: 850, loss is 5.7842000961303714 and perplexity is 325.12186982911896
At time: 37.67503213882446 and batch: 900, loss is 5.767936553955078 and perplexity is 319.8770022558722
At time: 38.502402782440186 and batch: 950, loss is 5.7084555912017825 and perplexity is 301.4052157749587
At time: 39.32987976074219 and batch: 1000, loss is 5.7483533668518065 and perplexity is 313.67372924430964
At time: 40.15476059913635 and batch: 1050, loss is 5.721514701843262 and perplexity is 305.3671129573861
At time: 40.977627992630005 and batch: 1100, loss is 5.718731279373169 and perplexity is 304.5183290835983
At time: 41.80519223213196 and batch: 1150, loss is 5.729039745330811 and perplexity is 307.6736813915894
At time: 42.63433265686035 and batch: 1200, loss is 5.75721960067749 and perplexity is 316.4671993527903
At time: 43.46308779716492 and batch: 1250, loss is 5.744368095397949 and perplexity is 312.4261419236166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.5136643012944795 and perplexity of 248.05842455294675
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 45.82871460914612 and batch: 50, loss is 5.665361309051514 and perplexity is 288.69226940536765
At time: 46.67628264427185 and batch: 100, loss is 5.645907773971557 and perplexity is 283.1304580611214
At time: 47.50042271614075 and batch: 150, loss is 5.539808340072632 and perplexity is 254.6291925689881
At time: 48.32526636123657 and batch: 200, loss is 5.578232278823853 and perplexity is 264.6034470044894
At time: 49.14901781082153 and batch: 250, loss is 5.612266263961792 and perplexity is 273.7639569057868
At time: 49.97262692451477 and batch: 300, loss is 5.57470287322998 and perplexity is 263.67120022470016
At time: 50.83291840553284 and batch: 350, loss is 5.615557155609131 and perplexity is 274.6663684798424
At time: 51.6550087928772 and batch: 400, loss is 5.565593366622925 and perplexity is 261.28019266814414
At time: 52.481311559677124 and batch: 450, loss is 5.533577527999878 and perplexity is 253.04757840907098
At time: 53.30739378929138 and batch: 500, loss is 5.531506338119507 and perplexity is 252.52401121612135
At time: 54.13128304481506 and batch: 550, loss is 5.543796625137329 and perplexity is 255.64675418883922
At time: 54.955156326293945 and batch: 600, loss is 5.531325206756592 and perplexity is 252.4782753400269
At time: 55.78121018409729 and batch: 650, loss is 5.509179315567017 and perplexity is 246.94837719325758
At time: 56.60407280921936 and batch: 700, loss is 5.537342691421509 and perplexity is 254.0021398073683
At time: 57.42677402496338 and batch: 750, loss is 5.517455244064331 and perplexity is 249.00058455299248
At time: 58.250943183898926 and batch: 800, loss is 5.5006379222869874 and perplexity is 244.84807649990654
At time: 59.07611560821533 and batch: 850, loss is 5.521740875244141 and perplexity is 250.0699991434048
At time: 59.90095090866089 and batch: 900, loss is 5.513896903991699 and perplexity is 248.1161303225645
At time: 60.72416567802429 and batch: 950, loss is 5.479724006652832 and perplexity is 239.78052041268052
At time: 61.5509467124939 and batch: 1000, loss is 5.46862343788147 and perplexity is 237.1335389363754
At time: 62.37212538719177 and batch: 1050, loss is 5.46483380317688 and perplexity is 236.23659007531407
At time: 63.1968457698822 and batch: 1100, loss is 5.45101565361023 and perplexity is 232.994687670936
At time: 64.02148628234863 and batch: 1150, loss is 5.434635038375855 and perplexity is 229.2091804541375
At time: 64.84837746620178 and batch: 1200, loss is 5.416782350540161 and perplexity is 225.15349070595897
At time: 65.67161917686462 and batch: 1250, loss is 5.421953821182251 and perplexity is 226.32088133455684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.214946663292655 and perplexity of 184.0020066567371
Finished 3 epochs...
Completing Train Step...
At time: 67.9374577999115 and batch: 50, loss is 5.449910650253296 and perplexity is 232.7373699535653
At time: 68.78870105743408 and batch: 100, loss is 5.473319873809815 and perplexity is 238.24984067644192
At time: 69.6082923412323 and batch: 150, loss is 5.374746799468994 and perplexity is 215.8852031417031
At time: 70.43507528305054 and batch: 200, loss is 5.416009826660156 and perplexity is 224.97962142545478
At time: 71.26617574691772 and batch: 250, loss is 5.44938817024231 and perplexity is 232.61580109137094
At time: 72.13527297973633 and batch: 300, loss is 5.4242391109466555 and perplexity is 226.83868156457137
At time: 72.96144342422485 and batch: 350, loss is 5.464368381500244 and perplexity is 236.12666602797995
At time: 73.7918016910553 and batch: 400, loss is 5.427238855361939 and perplexity is 227.52016125418882
At time: 74.62329578399658 and batch: 450, loss is 5.3957525157928465 and perplexity is 220.46799033338448
At time: 75.452463388443 and batch: 500, loss is 5.398123502731323 and perplexity is 220.99133723798505
At time: 76.28138899803162 and batch: 550, loss is 5.4064106750488286 and perplexity is 222.83034007225322
At time: 77.11682462692261 and batch: 600, loss is 5.406114521026612 and perplexity is 222.76435774171745
At time: 77.96167707443237 and batch: 650, loss is 5.398187885284424 and perplexity is 221.0055656825164
At time: 78.82326316833496 and batch: 700, loss is 5.4283938980102535 and perplexity is 227.78310857223852
At time: 79.6477837562561 and batch: 750, loss is 5.396513757705688 and perplexity is 220.63588370369368
At time: 80.48230695724487 and batch: 800, loss is 5.386430072784424 and perplexity is 218.4222405670935
At time: 81.32741284370422 and batch: 850, loss is 5.42728705406189 and perplexity is 227.53112769445593
At time: 82.14749765396118 and batch: 900, loss is 5.412812461853028 and perplexity is 224.2614282761871
At time: 82.97775936126709 and batch: 950, loss is 5.392573595046997 and perplexity is 219.76825285937284
At time: 83.81665849685669 and batch: 1000, loss is 5.390538339614868 and perplexity is 219.32142318942834
At time: 84.66169714927673 and batch: 1050, loss is 5.385686664581299 and perplexity is 218.25992402291175
At time: 85.48753786087036 and batch: 1100, loss is 5.368771238327026 and perplexity is 214.59901458705232
At time: 86.31140899658203 and batch: 1150, loss is 5.375202522277832 and perplexity is 215.98360937414466
At time: 87.13576126098633 and batch: 1200, loss is 5.3808099555969235 and perplexity is 217.19812503716028
At time: 87.96074366569519 and batch: 1250, loss is 5.396920757293701 and perplexity is 220.7257006939612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.223585142706432 and perplexity of 185.59838943595201
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 90.25872945785522 and batch: 50, loss is 5.397849330902099 and perplexity is 220.93075594403348
At time: 91.08310675621033 and batch: 100, loss is 5.40478214263916 and perplexity is 222.46774896740715
At time: 91.90975975990295 and batch: 150, loss is 5.2929607772827145 and perplexity is 198.93154633541553
At time: 92.73639988899231 and batch: 200, loss is 5.333761415481567 and perplexity is 207.2159353486877
At time: 93.59189367294312 and batch: 250, loss is 5.354873580932617 and perplexity is 211.63721968171737
At time: 94.44086456298828 and batch: 300, loss is 5.328831377029419 and perplexity is 206.19686690688366
At time: 95.26977968215942 and batch: 350, loss is 5.350601291656494 and perplexity is 210.73497295949159
At time: 96.09602451324463 and batch: 400, loss is 5.313759241104126 and perplexity is 203.11234327342467
At time: 96.9217414855957 and batch: 450, loss is 5.286682605743408 and perplexity is 197.68653225831002
At time: 97.74890732765198 and batch: 500, loss is 5.282848196029663 and perplexity is 196.9299725057299
At time: 98.5762255191803 and batch: 550, loss is 5.290597190856934 and perplexity is 198.4619096649043
At time: 99.40416550636292 and batch: 600, loss is 5.290449514389038 and perplexity is 198.4326036750289
At time: 100.23379707336426 and batch: 650, loss is 5.2841094207763675 and perplexity is 197.17850215334843
At time: 101.05901288986206 and batch: 700, loss is 5.310475749969482 and perplexity is 202.4465194065363
At time: 101.8857352733612 and batch: 750, loss is 5.299381704330444 and perplexity is 200.2129808781736
At time: 102.72001433372498 and batch: 800, loss is 5.2852530097961425 and perplexity is 197.4041223071232
At time: 103.5623836517334 and batch: 850, loss is 5.2941507148742675 and perplexity is 199.16840335513342
At time: 104.38941955566406 and batch: 900, loss is 5.284305725097656 and perplexity is 197.21721294480992
At time: 105.21610713005066 and batch: 950, loss is 5.250032682418823 and perplexity is 190.57249672700615
At time: 106.04089760780334 and batch: 1000, loss is 5.246139249801636 and perplexity is 189.83195810639424
At time: 106.86651635169983 and batch: 1050, loss is 5.233818244934082 and perplexity is 187.50738755621026
At time: 107.69489884376526 and batch: 1100, loss is 5.20102442741394 and perplexity is 181.45803727981883
At time: 108.53271722793579 and batch: 1150, loss is 5.217432727813721 and perplexity is 184.46001660253484
At time: 109.36544752120972 and batch: 1200, loss is 5.2195493030548095 and perplexity is 184.85085357867112
At time: 110.19283175468445 and batch: 1250, loss is 5.26139645576477 and perplexity is 192.75047095076198
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.115545787950502 and perplexity of 166.59167990684065
Finished 5 epochs...
Completing Train Step...
At time: 112.51891231536865 and batch: 50, loss is 5.308414573669434 and perplexity is 202.02967118512538
At time: 113.3456301689148 and batch: 100, loss is 5.324867782592773 and perplexity is 205.38120369927788
At time: 114.22245001792908 and batch: 150, loss is 5.224677648544311 and perplexity is 185.80126756250147
At time: 115.04915571212769 and batch: 200, loss is 5.266420049667358 and perplexity is 193.72120729243696
At time: 115.87590003013611 and batch: 250, loss is 5.293488025665283 and perplexity is 199.0364603268985
At time: 116.70320463180542 and batch: 300, loss is 5.272707481384277 and perplexity is 194.94305326682883
At time: 117.53283643722534 and batch: 350, loss is 5.2962393474578855 and perplexity is 199.58482769935145
At time: 118.36184000968933 and batch: 400, loss is 5.2656646347045895 and perplexity is 193.57492265360156
At time: 119.19089221954346 and batch: 450, loss is 5.239091520309448 and perplexity is 188.49877728389626
At time: 120.01605987548828 and batch: 500, loss is 5.240147790908813 and perplexity is 188.69798819214853
At time: 120.8421778678894 and batch: 550, loss is 5.247992992401123 and perplexity is 190.184184061144
At time: 121.66746973991394 and batch: 600, loss is 5.252349472045898 and perplexity is 191.0145249561211
At time: 122.49288439750671 and batch: 650, loss is 5.251694927215576 and perplexity is 190.88953829544326
At time: 123.32036852836609 and batch: 700, loss is 5.275140161514282 and perplexity is 195.41786465695228
At time: 124.14618277549744 and batch: 750, loss is 5.26209493637085 and perplexity is 192.8851504465661
At time: 124.97228050231934 and batch: 800, loss is 5.254862632751465 and perplexity is 191.4951788817295
At time: 125.80482935905457 and batch: 850, loss is 5.268001489639282 and perplexity is 194.0278081245647
At time: 126.64432787895203 and batch: 900, loss is 5.259204473495483 and perplexity is 192.32842806029305
At time: 127.47002482414246 and batch: 950, loss is 5.227671604156495 and perplexity is 186.35838188188887
At time: 128.29874467849731 and batch: 1000, loss is 5.224254865646362 and perplexity is 185.72273056737774
At time: 129.1259889602661 and batch: 1050, loss is 5.215526790618896 and perplexity is 184.10878221759765
At time: 129.96068477630615 and batch: 1100, loss is 5.188789539337158 and perplexity is 179.25144473526618
At time: 130.79728746414185 and batch: 1150, loss is 5.2092531967163085 and perplexity is 182.95737399429174
At time: 131.62322115898132 and batch: 1200, loss is 5.213859901428223 and perplexity is 183.80214891148174
At time: 132.45405793190002 and batch: 1250, loss is 5.247781839370727 and perplexity is 190.14403033378596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.109148234346487 and perplexity of 165.52930263589658
Finished 6 epochs...
Completing Train Step...
At time: 134.75214052200317 and batch: 50, loss is 5.274902229309082 and perplexity is 195.37137398451233
At time: 135.606454372406 and batch: 100, loss is 5.2922585010528564 and perplexity is 198.79189048328766
At time: 136.43256640434265 and batch: 150, loss is 5.194014139175415 and perplexity is 180.19041253406985
At time: 137.25984644889832 and batch: 200, loss is 5.2362160396575925 and perplexity is 187.95753124111596
At time: 138.0867953300476 and batch: 250, loss is 5.264164886474609 and perplexity is 193.28482659588258
At time: 138.9134612083435 and batch: 300, loss is 5.243224582672119 and perplexity is 189.2794666933811
At time: 139.7420060634613 and batch: 350, loss is 5.267158756256103 and perplexity is 193.86436329327415
At time: 140.5728621482849 and batch: 400, loss is 5.2392454433441165 and perplexity is 188.5277938208265
At time: 141.39915418624878 and batch: 450, loss is 5.213569803237915 and perplexity is 183.748835974075
At time: 142.22478580474854 and batch: 500, loss is 5.214076633453369 and perplexity is 183.84198904060153
At time: 143.050359249115 and batch: 550, loss is 5.222972345352173 and perplexity is 185.48469007484275
At time: 143.8762331008911 and batch: 600, loss is 5.229767217636108 and perplexity is 186.74932651034356
At time: 144.70577430725098 and batch: 650, loss is 5.23061429977417 and perplexity is 186.90758554886472
At time: 145.53545928001404 and batch: 700, loss is 5.2530778121948245 and perplexity is 191.15369918061003
At time: 146.36239433288574 and batch: 750, loss is 5.237145643234253 and perplexity is 188.13233847254332
At time: 147.19262838363647 and batch: 800, loss is 5.234154462814331 and perplexity is 187.57044149192174
At time: 148.02267408370972 and batch: 850, loss is 5.250499811172485 and perplexity is 190.66153941546546
At time: 148.8482096195221 and batch: 900, loss is 5.241833667755127 and perplexity is 189.01637806903832
At time: 149.6739444732666 and batch: 950, loss is 5.209821395874023 and perplexity is 183.06135975960748
At time: 150.5011909008026 and batch: 1000, loss is 5.206691770553589 and perplexity is 182.48934186064918
At time: 151.32869696617126 and batch: 1050, loss is 5.198813352584839 and perplexity is 181.0572632150518
At time: 152.15582084655762 and batch: 1100, loss is 5.175372323989868 and perplexity is 176.8624521530048
At time: 152.98388814926147 and batch: 1150, loss is 5.197376546859741 and perplexity is 180.79730590149353
At time: 153.8083794116974 and batch: 1200, loss is 5.201854801177978 and perplexity is 181.60877785009563
At time: 154.63510060310364 and batch: 1250, loss is 5.233142423629761 and perplexity is 187.38070887987777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.105042394930429 and perplexity of 164.85105923268475
Finished 7 epochs...
Completing Train Step...
At time: 156.9351842403412 and batch: 50, loss is 5.248816890716553 and perplexity is 190.34094105705844
At time: 157.76156330108643 and batch: 100, loss is 5.266437187194824 and perplexity is 193.72452722339523
At time: 158.59003829956055 and batch: 150, loss is 5.171565437316895 and perplexity is 176.1904367957
At time: 159.41806864738464 and batch: 200, loss is 5.214905824661255 and perplexity is 183.99449242004698
At time: 160.24629735946655 and batch: 250, loss is 5.241783494949341 and perplexity is 189.00689482491347
At time: 161.08510279655457 and batch: 300, loss is 5.221110134124756 and perplexity is 185.13959981767331
At time: 161.93874835968018 and batch: 350, loss is 5.244733285903931 and perplexity is 189.56524876245487
At time: 162.77007579803467 and batch: 400, loss is 5.217937488555908 and perplexity is 184.55314828004887
At time: 163.59642505645752 and batch: 450, loss is 5.193516063690185 and perplexity is 180.10068645394853
At time: 164.42548656463623 and batch: 500, loss is 5.193695831298828 and perplexity is 180.13306563394391
At time: 165.25313758850098 and batch: 550, loss is 5.203057842254639 and perplexity is 181.82739214435253
At time: 166.0831220149994 and batch: 600, loss is 5.210062131881714 and perplexity is 183.10543452549723
At time: 166.9075574874878 and batch: 650, loss is 5.2137931442260745 and perplexity is 183.78987920382176
At time: 167.73214983940125 and batch: 700, loss is 5.234785947799683 and perplexity is 187.68892681633395
At time: 168.58300471305847 and batch: 750, loss is 5.217635669708252 and perplexity is 184.49745506655728
At time: 169.41301012039185 and batch: 800, loss is 5.215977792739868 and perplexity is 184.19183439581383
At time: 170.23889064788818 and batch: 850, loss is 5.235515851974487 and perplexity is 187.8259717563343
At time: 171.08811378479004 and batch: 900, loss is 5.2263898754119875 and perplexity is 186.11967399910816
At time: 171.91526865959167 and batch: 950, loss is 5.192082996368408 and perplexity is 179.84277489201645
At time: 172.7600929737091 and batch: 1000, loss is 5.190434694290161 and perplexity is 179.54658384562296
At time: 173.58284282684326 and batch: 1050, loss is 5.184360160827636 and perplexity is 178.45922804717551
At time: 174.4068214893341 and batch: 1100, loss is 5.161646118164063 and perplexity is 174.4513869756953
At time: 175.22943115234375 and batch: 1150, loss is 5.184226207733154 and perplexity is 178.43532448235362
At time: 176.0550787448883 and batch: 1200, loss is 5.189110431671143 and perplexity is 179.3089743796547
At time: 176.87980151176453 and batch: 1250, loss is 5.219041957855224 and perplexity is 184.75709417167243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.101536214786725 and perplexity of 164.27407382061975
Finished 8 epochs...
Completing Train Step...
At time: 179.25608563423157 and batch: 50, loss is 5.226688632965088 and perplexity is 186.17528696447903
At time: 180.11376857757568 and batch: 100, loss is 5.244251365661621 and perplexity is 189.47391544129357
At time: 180.94019627571106 and batch: 150, loss is 5.151826362609864 and perplexity is 172.74670049996047
At time: 181.76618432998657 and batch: 200, loss is 5.195573472976685 and perplexity is 180.47160871737776
At time: 182.60202288627625 and batch: 250, loss is 5.221682939529419 and perplexity is 185.24567915957576
At time: 183.44006252288818 and batch: 300, loss is 5.201823740005493 and perplexity is 181.60313695612894
At time: 184.2939760684967 and batch: 350, loss is 5.225674390792847 and perplexity is 185.98655586271065
At time: 185.1295359134674 and batch: 400, loss is 5.199619913101197 and perplexity is 181.2033557630814
At time: 185.96574139595032 and batch: 450, loss is 5.175729465484619 and perplexity is 176.92562835428205
At time: 186.81395077705383 and batch: 500, loss is 5.176009168624878 and perplexity is 176.97512192955654
At time: 187.64091396331787 and batch: 550, loss is 5.185338258743286 and perplexity is 178.6338640377836
At time: 188.46708154678345 and batch: 600, loss is 5.1931474494934085 and perplexity is 180.03431101828787
At time: 189.29235243797302 and batch: 650, loss is 5.197758846282959 and perplexity is 180.8664378209635
At time: 190.12978649139404 and batch: 700, loss is 5.217541046142578 and perplexity is 184.4799980854351
At time: 190.9797568321228 and batch: 750, loss is 5.200193195343018 and perplexity is 181.30726621126024
At time: 191.80464148521423 and batch: 800, loss is 5.199503774642944 and perplexity is 181.18231230671387
At time: 192.63101148605347 and batch: 850, loss is 5.220628929138184 and perplexity is 185.05053115089106
At time: 193.45909667015076 and batch: 900, loss is 5.210554790496826 and perplexity is 183.19566521992954
At time: 194.2860450744629 and batch: 950, loss is 5.177234525680542 and perplexity is 177.19211256224034
At time: 195.11257553100586 and batch: 1000, loss is 5.174860877990723 and perplexity is 176.77201968708476
At time: 195.9368031024933 and batch: 1050, loss is 5.169617347717285 and perplexity is 175.84753614726486
At time: 196.76370859146118 and batch: 1100, loss is 5.147144784927368 and perplexity is 171.93986351082742
At time: 197.58877658843994 and batch: 1150, loss is 5.172114353179932 and perplexity is 176.28717707007917
At time: 198.41574239730835 and batch: 1200, loss is 5.176547327041626 and perplexity is 177.0703882128548
At time: 199.2927815914154 and batch: 1250, loss is 5.205448179244995 and perplexity is 182.2625407543719
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.099000805485858 and perplexity of 163.8580993615539
Finished 9 epochs...
Completing Train Step...
At time: 201.59769320487976 and batch: 50, loss is 5.206712551116944 and perplexity is 182.49313413138188
At time: 202.42473721504211 and batch: 100, loss is 5.225310869216919 and perplexity is 185.91895802420294
At time: 203.26724004745483 and batch: 150, loss is 5.13484372138977 and perplexity is 169.83777578903582
At time: 204.1178047657013 and batch: 200, loss is 5.178746938705444 and perplexity is 177.46030297740654
At time: 204.97587275505066 and batch: 250, loss is 5.2034762001037596 and perplexity is 181.90347697527469
At time: 205.80303406715393 and batch: 300, loss is 5.183809385299683 and perplexity is 178.36096413479282
At time: 206.63057899475098 and batch: 350, loss is 5.206831750869751 and perplexity is 182.51488856439505
At time: 207.49633717536926 and batch: 400, loss is 5.183151006698608 and perplexity is 178.24357374064155
At time: 208.331552028656 and batch: 450, loss is 5.159112672805787 and perplexity is 174.00998329126472
At time: 209.16733980178833 and batch: 500, loss is 5.15984881401062 and perplexity is 174.13812636993117
At time: 210.00512433052063 and batch: 550, loss is 5.169568204879761 and perplexity is 175.83889471270106
At time: 210.84066462516785 and batch: 600, loss is 5.177121772766113 and perplexity is 177.1721347614339
At time: 211.66302704811096 and batch: 650, loss is 5.183306541442871 and perplexity is 178.27129896536172
At time: 212.48888492584229 and batch: 700, loss is 5.203193588256836 and perplexity is 181.85207616126647
At time: 213.31865787506104 and batch: 750, loss is 5.184161262512207 and perplexity is 178.4237363370818
At time: 214.14793515205383 and batch: 800, loss is 5.18526065826416 and perplexity is 178.620002502184
At time: 214.983163356781 and batch: 850, loss is 5.207239303588867 and perplexity is 182.5892881633584
At time: 215.8132722377777 and batch: 900, loss is 5.197362613677979 and perplexity is 180.79478683731764
At time: 216.64161705970764 and batch: 950, loss is 5.162218065261841 and perplexity is 174.55119247919268
At time: 217.46821880340576 and batch: 1000, loss is 5.161576299667359 and perplexity is 174.43920746729117
At time: 218.3167474269867 and batch: 1050, loss is 5.15719069480896 and perplexity is 173.67586112273267
At time: 219.1444320678711 and batch: 1100, loss is 5.135319118499756 and perplexity is 169.91853537172744
At time: 219.97189092636108 and batch: 1150, loss is 5.160204029083252 and perplexity is 174.19999384461684
At time: 220.85157084465027 and batch: 1200, loss is 5.163719644546509 and perplexity is 174.81349181627436
At time: 221.67845392227173 and batch: 1250, loss is 5.191679248809814 and perplexity is 179.77017846702387
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.097261080776688 and perplexity of 163.57327920346776
Finished 10 epochs...
Completing Train Step...
At time: 223.93941593170166 and batch: 50, loss is 5.188532924652099 and perplexity is 179.20545208367642
At time: 224.81640553474426 and batch: 100, loss is 5.206496524810791 and perplexity is 182.45371507164745
At time: 225.6453676223755 and batch: 150, loss is 5.118817987442017 and perplexity is 167.1376939633874
At time: 226.47327661514282 and batch: 200, loss is 5.163611516952515 and perplexity is 174.7945906758924
At time: 227.29915976524353 and batch: 250, loss is 5.188054695129394 and perplexity is 179.11977123504892
At time: 228.12574815750122 and batch: 300, loss is 5.167627000808716 and perplexity is 175.4978866246654
At time: 228.95277786254883 and batch: 350, loss is 5.18933632850647 and perplexity is 179.34948428487024
At time: 229.782329082489 and batch: 400, loss is 5.167313423156738 and perplexity is 175.44286303698513
At time: 230.61161470413208 and batch: 450, loss is 5.14347578048706 and perplexity is 171.310171266349
At time: 231.43860411643982 and batch: 500, loss is 5.145016298294068 and perplexity is 171.57428101636074
At time: 232.26706957817078 and batch: 550, loss is 5.1536839485168455 and perplexity is 173.06789056299522
At time: 233.09302592277527 and batch: 600, loss is 5.163558149337769 and perplexity is 174.78526255442952
At time: 233.91988158226013 and batch: 650, loss is 5.169531211853028 and perplexity is 175.83239002008304
At time: 234.74757194519043 and batch: 700, loss is 5.189132328033447 and perplexity is 179.31290063690741
At time: 235.57071614265442 and batch: 750, loss is 5.168975915908813 and perplexity is 175.73477811130016
At time: 236.3964147567749 and batch: 800, loss is 5.171844062805175 and perplexity is 176.23953478184012
At time: 237.21840286254883 and batch: 850, loss is 5.195052261352539 and perplexity is 180.37756932642674
At time: 238.04144549369812 and batch: 900, loss is 5.184350891113281 and perplexity is 178.45757378877474
At time: 238.88497233390808 and batch: 950, loss is 5.149228096008301 and perplexity is 172.29844111839404
At time: 239.70748805999756 and batch: 1000, loss is 5.14853681564331 and perplexity is 172.17937574764835
At time: 240.53057837486267 and batch: 1050, loss is 5.14522536277771 and perplexity is 171.61015485466982
At time: 241.40194272994995 and batch: 1100, loss is 5.123067512512207 and perplexity is 167.84946104897767
At time: 242.22583317756653 and batch: 1150, loss is 5.149207305908203 and perplexity is 172.29485905379244
At time: 243.04963898658752 and batch: 1200, loss is 5.151167039871216 and perplexity is 172.63284221109842
At time: 243.87457847595215 and batch: 1250, loss is 5.180078420639038 and perplexity is 177.6967455393195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.09629832915146 and perplexity of 163.41587454602367
Finished 11 epochs...
Completing Train Step...
At time: 246.16666650772095 and batch: 50, loss is 5.172635841369629 and perplexity is 176.37913272572644
At time: 247.02260398864746 and batch: 100, loss is 5.190434036254882 and perplexity is 179.5464656976755
At time: 247.84485793113708 and batch: 150, loss is 5.10396824836731 and perplexity is 164.67408010176246
At time: 248.66629695892334 and batch: 200, loss is 5.149820079803467 and perplexity is 172.4004691999744
At time: 249.49200701713562 and batch: 250, loss is 5.172643690109253 and perplexity is 176.3805170850471
At time: 250.31702494621277 and batch: 300, loss is 5.153083381652832 and perplexity is 172.9639829275628
At time: 251.13992977142334 and batch: 350, loss is 5.174008150100708 and perplexity is 176.62134550689134
At time: 251.9641194343567 and batch: 400, loss is 5.153190450668335 and perplexity is 172.9825030023778
At time: 252.78951263427734 and batch: 450, loss is 5.128023090362549 and perplexity is 168.68331653175704
At time: 253.61399841308594 and batch: 500, loss is 5.130633087158203 and perplexity is 169.12415448996228
At time: 254.44038581848145 and batch: 550, loss is 5.138335428237915 and perplexity is 170.4318360524111
At time: 255.29277539253235 and batch: 600, loss is 5.1485338878631595 and perplexity is 172.1788716450276
At time: 256.12116503715515 and batch: 650, loss is 5.156320495605469 and perplexity is 173.52479426538045
At time: 256.94748544692993 and batch: 700, loss is 5.175132646560669 and perplexity is 176.8200672947007
At time: 257.78205943107605 and batch: 750, loss is 5.153663368225097 and perplexity is 173.06432881196625
At time: 258.6070466041565 and batch: 800, loss is 5.158115301132202 and perplexity is 173.83651718248038
At time: 259.43044424057007 and batch: 850, loss is 5.18326473236084 and perplexity is 178.26384576180638
At time: 260.2703504562378 and batch: 900, loss is 5.171057453155518 and perplexity is 176.1009575733597
At time: 261.0971965789795 and batch: 950, loss is 5.136818618774414 and perplexity is 170.17351938871093
At time: 261.9191551208496 and batch: 1000, loss is 5.1360264587402344 and perplexity is 170.03876810711202
At time: 262.7928922176361 and batch: 1050, loss is 5.1337471771240235 and perplexity is 169.65164321983852
At time: 263.63158416748047 and batch: 1100, loss is 5.11024676322937 and perplexity is 165.71124126993138
At time: 264.46985602378845 and batch: 1150, loss is 5.137610502243042 and perplexity is 170.3083303557565
At time: 265.2971317768097 and batch: 1200, loss is 5.139495210647583 and perplexity is 170.62961456576326
At time: 266.12661480903625 and batch: 1250, loss is 5.1684068775177 and perplexity is 175.63480672237787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.094684238851506 and perplexity of 163.15231932624204
Finished 12 epochs...
Completing Train Step...
At time: 268.44344997406006 and batch: 50, loss is 5.155413589477539 and perplexity is 173.36749490477175
At time: 269.2711732387543 and batch: 100, loss is 5.174628496170044 and perplexity is 176.73094585589553
At time: 270.09670639038086 and batch: 150, loss is 5.091348199844361 and perplexity is 162.60894368996517
At time: 270.9248592853546 and batch: 200, loss is 5.1359696960449215 and perplexity is 170.02911652225416
At time: 271.7681267261505 and batch: 250, loss is 5.158568601608277 and perplexity is 173.91533522125593
At time: 272.59347128868103 and batch: 300, loss is 5.139678554534912 and perplexity is 170.6609013306221
At time: 273.4174873828888 and batch: 350, loss is 5.159357175827027 and perplexity is 174.052534459638
At time: 274.2397618293762 and batch: 400, loss is 5.139190807342529 and perplexity is 170.57768225173436
At time: 275.0632040500641 and batch: 450, loss is 5.11407546043396 and perplexity is 166.346915561818
At time: 275.8881447315216 and batch: 500, loss is 5.117361822128296 and perplexity is 166.89449096568163
At time: 276.71199011802673 and batch: 550, loss is 5.12468542098999 and perplexity is 168.12124591723546
At time: 277.53437089920044 and batch: 600, loss is 5.13548490524292 and perplexity is 169.94670794756516
At time: 278.3564805984497 and batch: 650, loss is 5.14463755607605 and perplexity is 171.50931089685398
At time: 279.1793842315674 and batch: 700, loss is 5.162807712554931 and perplexity is 174.65414646763116
At time: 280.0023891925812 and batch: 750, loss is 5.1406181526184085 and perplexity is 170.82132934356412
At time: 280.83751702308655 and batch: 800, loss is 5.146157293319702 and perplexity is 171.77015814369068
At time: 281.6883535385132 and batch: 850, loss is 5.172589817047119 and perplexity is 176.37101518244154
At time: 282.5174810886383 and batch: 900, loss is 5.159301719665527 and perplexity is 174.04288244181203
At time: 283.34246253967285 and batch: 950, loss is 5.12504054069519 and perplexity is 168.18095968666404
At time: 284.2192642688751 and batch: 1000, loss is 5.124392681121826 and perplexity is 168.0720373288793
At time: 285.0461058616638 and batch: 1050, loss is 5.123012924194336 and perplexity is 167.840298679325
At time: 285.8917019367218 and batch: 1100, loss is 5.099884119033813 and perplexity is 164.00290138404128
At time: 286.7424237728119 and batch: 1150, loss is 5.1265387439727785 and perplexity is 168.43311779658148
At time: 287.6076157093048 and batch: 1200, loss is 5.129424562454224 and perplexity is 168.91988722709985
At time: 288.45235204696655 and batch: 1250, loss is 5.157874422073364 and perplexity is 173.79464864865872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.09391751254562 and perplexity of 163.02727409500838
Finished 13 epochs...
Completing Train Step...
At time: 290.7693815231323 and batch: 50, loss is 5.142035846710205 and perplexity is 171.0636734772608
At time: 291.62344670295715 and batch: 100, loss is 5.160718231201172 and perplexity is 174.289590883913
At time: 292.449378490448 and batch: 150, loss is 5.078061494827271 and perplexity is 160.4626964730089
At time: 293.27566742897034 and batch: 200, loss is 5.124856061935425 and perplexity is 168.14993673343088
At time: 294.1000270843506 and batch: 250, loss is 5.146599082946778 and perplexity is 171.84606118315284
At time: 294.9298679828644 and batch: 300, loss is 5.128230724334717 and perplexity is 168.71834455518493
At time: 295.7542119026184 and batch: 350, loss is 5.147224197387695 and perplexity is 171.95351822058691
At time: 296.606791973114 and batch: 400, loss is 5.127817869186401 and perplexity is 168.64870269500952
At time: 297.4298942089081 and batch: 450, loss is 5.1021991539001466 and perplexity is 164.38301363571898
At time: 298.26252698898315 and batch: 500, loss is 5.105069379806519 and perplexity is 164.85550777811292
At time: 299.100617647171 and batch: 550, loss is 5.112155017852783 and perplexity is 166.02776241765864
At time: 299.9293336868286 and batch: 600, loss is 5.124018964767456 and perplexity is 168.0092377951545
At time: 300.7560157775879 and batch: 650, loss is 5.133415775299072 and perplexity is 169.59542967082035
At time: 301.5838623046875 and batch: 700, loss is 5.150374269485473 and perplexity is 172.49603824042015
At time: 302.41118454933167 and batch: 750, loss is 5.129329242706299 and perplexity is 168.9037865933962
At time: 303.2387447357178 and batch: 800, loss is 5.1358591651916505 and perplexity is 170.01032409751454
At time: 304.0798325538635 and batch: 850, loss is 5.161080865859986 and perplexity is 174.3528057915034
At time: 304.91788840293884 and batch: 900, loss is 5.148014488220215 and perplexity is 172.08946522141488
At time: 305.7947130203247 and batch: 950, loss is 5.113667449951172 and perplexity is 166.27905812070432
At time: 306.6222257614136 and batch: 1000, loss is 5.1129684734344485 and perplexity is 166.1628735737204
At time: 307.47137999534607 and batch: 1050, loss is 5.112088508605957 and perplexity is 166.0167204034302
At time: 308.3252682685852 and batch: 1100, loss is 5.089350748062134 and perplexity is 162.28446433939848
At time: 309.1653916835785 and batch: 1150, loss is 5.1164674472808835 and perplexity is 166.74529146087787
At time: 309.9921820163727 and batch: 1200, loss is 5.118489589691162 and perplexity is 167.08281533210945
At time: 310.8201720714569 and batch: 1250, loss is 5.146984968185425 and perplexity is 171.91238683770558
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.093243007242244 and perplexity of 162.91734841093242
Finished 14 epochs...
Completing Train Step...
At time: 313.16796588897705 and batch: 50, loss is 5.1290000057220455 and perplexity is 168.84818637339586
At time: 313.99521255493164 and batch: 100, loss is 5.1476689720153805 and perplexity is 172.03001579346133
At time: 314.8243725299835 and batch: 150, loss is 5.066172561645508 and perplexity is 158.56626182672315
At time: 315.6520481109619 and batch: 200, loss is 5.112367467880249 and perplexity is 166.0630387674425
At time: 316.4888846874237 and batch: 250, loss is 5.13448616027832 and perplexity is 169.77705926074006
At time: 317.3176300525665 and batch: 300, loss is 5.11498688697815 and perplexity is 166.49859766925445
At time: 318.1445252895355 and batch: 350, loss is 5.133861322402954 and perplexity is 169.67100925922477
At time: 318.9727759361267 and batch: 400, loss is 5.116309270858765 and perplexity is 166.71891837312478
At time: 319.8002824783325 and batch: 450, loss is 5.09033989906311 and perplexity is 162.44506759708455
At time: 320.6486053466797 and batch: 500, loss is 5.0930497932434085 and perplexity is 162.88587353935702
At time: 321.47509002685547 and batch: 550, loss is 5.0999329566955565 and perplexity is 164.0109110978502
At time: 322.302348613739 and batch: 600, loss is 5.112406330108643 and perplexity is 166.06949247258467
At time: 323.13016295433044 and batch: 650, loss is 5.121797227859497 and perplexity is 167.6363798202017
At time: 323.9568569660187 and batch: 700, loss is 5.138393392562866 and perplexity is 170.44171530505741
At time: 324.78584456443787 and batch: 750, loss is 5.116575145721436 and perplexity is 166.76325063580785
At time: 325.61360025405884 and batch: 800, loss is 5.125106706619262 and perplexity is 168.19208790342336
At time: 326.4886407852173 and batch: 850, loss is 5.150918560028076 and perplexity is 172.58995175847224
At time: 327.3170440196991 and batch: 900, loss is 5.1367430496215825 and perplexity is 170.16066000590885
At time: 328.1460497379303 and batch: 950, loss is 5.103843116760254 and perplexity is 164.65347545865185
At time: 328.97442841529846 and batch: 1000, loss is 5.103447093963623 and perplexity is 164.5882818387575
At time: 329.8163330554962 and batch: 1050, loss is 5.102050724029541 and perplexity is 164.3586160969819
At time: 330.66866517066956 and batch: 1100, loss is 5.079423875808716 and perplexity is 160.68145678252128
At time: 331.50163102149963 and batch: 1150, loss is 5.105916061401367 and perplexity is 164.99514700899942
At time: 332.3291473388672 and batch: 1200, loss is 5.1080290222167966 and perplexity is 165.34414386786483
At time: 333.1564836502075 and batch: 1250, loss is 5.137455358505249 and perplexity is 170.2819101343267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.092718639513002 and perplexity of 162.8319422049781
Finished 15 epochs...
Completing Train Step...
At time: 335.4146478176117 and batch: 50, loss is 5.116320114135743 and perplexity is 166.7207261623353
At time: 336.27203845977783 and batch: 100, loss is 5.134705686569214 and perplexity is 169.81433388006823
At time: 337.09879636764526 and batch: 150, loss is 5.0550986003875735 and perplexity is 156.81999209388817
At time: 337.9276111125946 and batch: 200, loss is 5.1014462757110595 and perplexity is 164.2592998266542
At time: 338.75442600250244 and batch: 250, loss is 5.1230787086486815 and perplexity is 167.85134032497112
At time: 339.58084630966187 and batch: 300, loss is 5.104368352890015 and perplexity is 164.73998012855373
At time: 340.4111044406891 and batch: 350, loss is 5.122260036468506 and perplexity is 167.71398133590523
At time: 341.2380118370056 and batch: 400, loss is 5.106269903182984 and perplexity is 165.0535395160203
At time: 342.06477904319763 and batch: 450, loss is 5.079239530563354 and perplexity is 160.65183865001055
At time: 342.89184284210205 and batch: 500, loss is 5.082401838302612 and perplexity is 161.16067332457726
At time: 343.7181782722473 and batch: 550, loss is 5.088751440048218 and perplexity is 162.18723509742793
At time: 344.54394698143005 and batch: 600, loss is 5.10197193145752 and perplexity is 164.3456663690636
At time: 345.3724355697632 and batch: 650, loss is 5.1123020362854 and perplexity is 166.05217335344503
At time: 346.2322041988373 and batch: 700, loss is 5.127815942764283 and perplexity is 168.64837780673128
At time: 347.08811378479004 and batch: 750, loss is 5.106305608749389 and perplexity is 165.05943295134958
At time: 347.963671207428 and batch: 800, loss is 5.114977264404297 and perplexity is 166.49699553191041
At time: 348.8248517513275 and batch: 850, loss is 5.141520338058472 and perplexity is 170.9755113996953
At time: 349.67875027656555 and batch: 900, loss is 5.126556816101075 and perplexity is 168.43616176900105
At time: 350.50643253326416 and batch: 950, loss is 5.09328893661499 and perplexity is 162.92483127439465
At time: 351.3347930908203 and batch: 1000, loss is 5.093397092819214 and perplexity is 162.9424535586816
At time: 352.1639003753662 and batch: 1050, loss is 5.092827644348144 and perplexity is 162.8496926414182
At time: 353.0118546485901 and batch: 1100, loss is 5.069805326461792 and perplexity is 159.143343329783
At time: 353.83762431144714 and batch: 1150, loss is 5.0967473220825195 and perplexity is 163.48926359286648
At time: 354.664674282074 and batch: 1200, loss is 5.099120750427246 and perplexity is 163.87775449042084
At time: 355.4903995990753 and batch: 1250, loss is 5.128021450042724 and perplexity is 168.68303983739574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.093203356666287 and perplexity of 162.91088877229947
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 357.74309611320496 and batch: 50, loss is 5.110238809585571 and perplexity is 165.7099232669862
At time: 358.5966303348541 and batch: 100, loss is 5.130310039520264 and perplexity is 169.06952815526517
At time: 359.42474484443665 and batch: 150, loss is 5.047056560516357 and perplexity is 155.56389701796616
At time: 360.25895643234253 and batch: 200, loss is 5.093435134887695 and perplexity is 162.94865234456495
At time: 361.0956561565399 and batch: 250, loss is 5.109947462081909 and perplexity is 165.66165112683555
At time: 361.9395503997803 and batch: 300, loss is 5.08925573348999 and perplexity is 162.26904568296388
At time: 362.78931069374084 and batch: 350, loss is 5.1036608695983885 and perplexity is 164.62347056429465
At time: 363.6171543598175 and batch: 400, loss is 5.0808225440979005 and perplexity is 160.90635408248053
At time: 364.4450035095215 and batch: 450, loss is 5.052518529891968 and perplexity is 156.41590696738012
At time: 365.26794624328613 and batch: 500, loss is 5.050303764343262 and perplexity is 156.06986574689572
At time: 366.0928032398224 and batch: 550, loss is 5.057281341552734 and perplexity is 157.1626633914746
At time: 366.91723585128784 and batch: 600, loss is 5.069730577468872 and perplexity is 159.13144796972767
At time: 367.7755353450775 and batch: 650, loss is 5.080576124191285 and perplexity is 160.86670843867216
At time: 368.60517263412476 and batch: 700, loss is 5.091684846878052 and perplexity is 162.6636947238796
At time: 369.45883989334106 and batch: 750, loss is 5.066791601181031 and perplexity is 158.66445100014852
At time: 370.2837791442871 and batch: 800, loss is 5.069643859863281 and perplexity is 159.11764906989802
At time: 371.13538002967834 and batch: 850, loss is 5.091544532775879 and perplexity is 162.64087231478868
At time: 371.978675365448 and batch: 900, loss is 5.073340539932251 and perplexity is 159.70694465938422
At time: 372.8318588733673 and batch: 950, loss is 5.036145963668823 and perplexity is 153.87582772111838
At time: 373.65732502937317 and batch: 1000, loss is 5.035343313217163 and perplexity is 153.75236877231717
At time: 374.4841184616089 and batch: 1050, loss is 5.0311995697021485 and perplexity is 153.11657658092844
At time: 375.3099567890167 and batch: 1100, loss is 5.00209095954895 and perplexity is 148.72380968050496
At time: 376.1358690261841 and batch: 1150, loss is 5.024301319122315 and perplexity is 152.063974797256
At time: 376.9604117870331 and batch: 1200, loss is 5.037640409469605 and perplexity is 154.1059587220213
At time: 377.79748797416687 and batch: 1250, loss is 5.080836143493652 and perplexity is 160.90854232654803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.062258532447537 and perplexity of 157.94684185282532
Finished 17 epochs...
Completing Train Step...
At time: 380.1633756160736 and batch: 50, loss is 5.083476247787476 and perplexity is 161.33391893228068
At time: 381.01550555229187 and batch: 100, loss is 5.10600866317749 and perplexity is 165.01042656010998
At time: 381.86288690567017 and batch: 150, loss is 5.024117259979248 and perplexity is 152.03598860799957
At time: 382.70189094543457 and batch: 200, loss is 5.068928251266479 and perplexity is 159.0038238443331
At time: 383.5386950969696 and batch: 250, loss is 5.088793125152588 and perplexity is 162.19399603016458
At time: 384.38712882995605 and batch: 300, loss is 5.070841398239136 and perplexity is 159.30831270165706
At time: 385.21405696868896 and batch: 350, loss is 5.086206645965576 and perplexity is 161.77502669647606
At time: 386.04037523269653 and batch: 400, loss is 5.064716262817383 and perplexity is 158.3355100280134
At time: 386.8682312965393 and batch: 450, loss is 5.036434001922608 and perplexity is 153.92015622968142
At time: 387.6958656311035 and batch: 500, loss is 5.036161766052246 and perplexity is 153.87825934516025
At time: 388.5223512649536 and batch: 550, loss is 5.042800312042236 and perplexity is 154.90318549230122
At time: 389.3527536392212 and batch: 600, loss is 5.0574424457550045 and perplexity is 157.1879849966407
At time: 390.1863570213318 and batch: 650, loss is 5.070194749832154 and perplexity is 159.20532953556824
At time: 391.0604817867279 and batch: 700, loss is 5.081724500656128 and perplexity is 161.05155009420974
At time: 391.9237279891968 and batch: 750, loss is 5.0581223678588865 and perplexity is 157.2948969238771
At time: 392.775377035141 and batch: 800, loss is 5.06344780921936 and perplexity is 158.13479610568405
At time: 393.60319566726685 and batch: 850, loss is 5.086689682006836 and perplexity is 161.85318874096876
At time: 394.4290328025818 and batch: 900, loss is 5.069273118972778 and perplexity is 159.05866858490143
At time: 395.2557337284088 and batch: 950, loss is 5.033222723007202 and perplexity is 153.42666846483547
At time: 396.0827646255493 and batch: 1000, loss is 5.035009527206421 and perplexity is 153.7010569465637
At time: 396.9146316051483 and batch: 1050, loss is 5.033464984893799 and perplexity is 153.46384240172426
At time: 397.7426517009735 and batch: 1100, loss is 5.005867929458618 and perplexity is 149.28659718121017
At time: 398.56953144073486 and batch: 1150, loss is 5.02966724395752 and perplexity is 152.88213177748153
At time: 399.3965938091278 and batch: 1200, loss is 5.042711200714112 and perplexity is 154.88938247872287
At time: 400.2248809337616 and batch: 1250, loss is 5.081168060302734 and perplexity is 160.96195944099978
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.060891255845118 and perplexity of 157.73103240076156
Finished 18 epochs...
Completing Train Step...
At time: 402.4965546131134 and batch: 50, loss is 5.0748179912567135 and perplexity is 159.94307829131682
At time: 403.35118985176086 and batch: 100, loss is 5.096805658340454 and perplexity is 163.49880122290907
At time: 404.176598072052 and batch: 150, loss is 5.01548020362854 and perplexity is 150.72849976254986
At time: 405.0055079460144 and batch: 200, loss is 5.059591512680054 and perplexity is 157.5261557418408
At time: 405.8525116443634 and batch: 250, loss is 5.080444440841675 and perplexity is 160.84552636635803
At time: 406.6843378543854 and batch: 300, loss is 5.063039646148682 and perplexity is 158.07026449233115
At time: 407.5163621902466 and batch: 350, loss is 5.07867992401123 and perplexity is 160.5619619786238
At time: 408.3688015937805 and batch: 400, loss is 5.056981763839722 and perplexity is 157.11558801192416
At time: 409.19951343536377 and batch: 450, loss is 5.028608503341675 and perplexity is 152.72035491011363
At time: 410.0251498222351 and batch: 500, loss is 5.029362487792969 and perplexity is 152.83554710421632
At time: 410.8570182323456 and batch: 550, loss is 5.036210784912109 and perplexity is 153.88580246686712
At time: 411.7319176197052 and batch: 600, loss is 5.051864166259765 and perplexity is 156.31358756706095
At time: 412.55886459350586 and batch: 650, loss is 5.065260429382324 and perplexity is 158.42169436580434
At time: 413.3874909877777 and batch: 700, loss is 5.077171325683594 and perplexity is 160.3199210884463
At time: 414.2152144908905 and batch: 750, loss is 5.0541528224945065 and perplexity is 156.6717453274825
At time: 415.0430612564087 and batch: 800, loss is 5.059979763031006 and perplexity is 157.5873272012435
At time: 415.8699645996094 and batch: 850, loss is 5.083854446411133 and perplexity is 161.39494673795863
At time: 416.69630336761475 and batch: 900, loss is 5.0664215183258055 and perplexity is 158.6057428712057
At time: 417.5230538845062 and batch: 950, loss is 5.031656408309937 and perplexity is 153.18654212486697
At time: 418.36303305625916 and batch: 1000, loss is 5.034499673843384 and perplexity is 153.622711919712
At time: 419.2110130786896 and batch: 1050, loss is 5.03371376991272 and perplexity is 153.50202665629817
At time: 420.0624134540558 and batch: 1100, loss is 5.006303615570069 and perplexity is 149.35165344925488
At time: 420.90623927116394 and batch: 1150, loss is 5.030226497650147 and perplexity is 152.96765558673874
At time: 421.7451994419098 and batch: 1200, loss is 5.0435030555725096 and perplexity is 155.0120809620366
At time: 422.56711411476135 and batch: 1250, loss is 5.07921275138855 and perplexity is 160.64753658394378
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.060222542198905 and perplexity of 157.62559076602315
Finished 19 epochs...
Completing Train Step...
At time: 424.8809766769409 and batch: 50, loss is 5.068295288085937 and perplexity is 158.90321212339083
At time: 425.7078068256378 and batch: 100, loss is 5.090417461395264 and perplexity is 162.4576677040147
At time: 426.535475730896 and batch: 150, loss is 5.009781446456909 and perplexity is 149.87197751772308
At time: 427.360714673996 and batch: 200, loss is 5.053688554763794 and perplexity is 156.59902457406838
At time: 428.2008972167969 and batch: 250, loss is 5.07448751449585 and perplexity is 159.89022955400128
At time: 429.0343806743622 and batch: 300, loss is 5.057413263320923 and perplexity is 157.18339793556123
At time: 429.8612713813782 and batch: 350, loss is 5.073150463104248 and perplexity is 159.67659095479274
At time: 430.6890366077423 and batch: 400, loss is 5.051235799789429 and perplexity is 156.21539620308707
At time: 431.5156309604645 and batch: 450, loss is 5.023209390640258 and perplexity is 151.89802243262585
At time: 432.3425085544586 and batch: 500, loss is 5.02522533416748 and perplexity is 152.2045491341956
At time: 433.21493554115295 and batch: 550, loss is 5.031959314346313 and perplexity is 153.23295028146876
At time: 434.0422921180725 and batch: 600, loss is 5.047487859725952 and perplexity is 155.63100607479277
At time: 434.8689913749695 and batch: 650, loss is 5.061332902908325 and perplexity is 157.8007092331496
At time: 435.6967029571533 and batch: 700, loss is 5.07380072593689 and perplexity is 159.7804564734168
At time: 436.52189087867737 and batch: 750, loss is 5.050765209197998 and perplexity is 156.14190000206395
At time: 437.3508026599884 and batch: 800, loss is 5.0569841670989994 and perplexity is 157.11596560187246
At time: 438.17786502838135 and batch: 850, loss is 5.0811442947387695 and perplexity is 160.95813413471225
At time: 439.0067493915558 and batch: 900, loss is 5.063652400970459 and perplexity is 158.1671524903408
At time: 439.8340241909027 and batch: 950, loss is 5.029410514831543 and perplexity is 152.84288751920045
At time: 440.67796063423157 and batch: 1000, loss is 5.032509832382202 and perplexity is 153.31733100871625
At time: 441.5055034160614 and batch: 1050, loss is 5.032844743728638 and perplexity is 153.3686873219022
At time: 442.3344757556915 and batch: 1100, loss is 5.0053773117065425 and perplexity is 149.21337249062148
At time: 443.1602690219879 and batch: 1150, loss is 5.029645471572876 and perplexity is 152.87880320513895
At time: 443.9867784976959 and batch: 1200, loss is 5.042489500045776 and perplexity is 154.8550472053264
At time: 444.8143632411957 and batch: 1250, loss is 5.07687819480896 and perplexity is 160.27293325688493
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.059871478672445 and perplexity of 157.57026388246217
Finished 20 epochs...
Completing Train Step...
At time: 447.0628159046173 and batch: 50, loss is 5.0629721450805665 and perplexity is 158.05959494074773
At time: 447.91946029663086 and batch: 100, loss is 5.084933338165283 and perplexity is 161.5691683814034
At time: 448.74327659606934 and batch: 150, loss is 5.005132474899292 and perplexity is 149.17684403683927
At time: 449.56854605674744 and batch: 200, loss is 5.048732252120971 and perplexity is 155.82479266345328
At time: 450.39216470718384 and batch: 250, loss is 5.069630470275879 and perplexity is 159.11551856449185
At time: 451.2207193374634 and batch: 300, loss is 5.052969713211059 and perplexity is 156.48649513834957
At time: 452.0447678565979 and batch: 350, loss is 5.068385229110718 and perplexity is 158.91750468386428
At time: 452.8705005645752 and batch: 400, loss is 5.046566820144653 and perplexity is 155.48772974983225
At time: 453.6967182159424 and batch: 450, loss is 5.018465738296509 and perplexity is 151.17917734606038
At time: 454.5660665035248 and batch: 500, loss is 5.020720891952514 and perplexity is 151.52049433704073
At time: 455.3913023471832 and batch: 550, loss is 5.027917938232422 and perplexity is 152.61492796781357
At time: 456.21859192848206 and batch: 600, loss is 5.0436540222167965 and perplexity is 155.03548438224684
At time: 457.04291439056396 and batch: 650, loss is 5.058086624145508 and perplexity is 157.28927472064535
At time: 457.8688244819641 and batch: 700, loss is 5.070425434112549 and perplexity is 159.2420599388477
At time: 458.69390869140625 and batch: 750, loss is 5.047625102996826 and perplexity is 155.65236684889362
At time: 459.53114223480225 and batch: 800, loss is 5.054258470535278 and perplexity is 156.68829826479796
At time: 460.3568911552429 and batch: 850, loss is 5.0786103248596195 and perplexity is 160.5507873911645
At time: 461.1832859516144 and batch: 900, loss is 5.0607981300354 and perplexity is 157.71634425458498
At time: 462.00921654701233 and batch: 950, loss is 5.027090950012207 and perplexity is 152.48876939317284
At time: 462.83398485183716 and batch: 1000, loss is 5.030519609451294 and perplexity is 153.01249878349907
At time: 463.6606721878052 and batch: 1050, loss is 5.031695251464844 and perplexity is 153.1924924890171
At time: 464.48745369911194 and batch: 1100, loss is 5.003961362838745 and perplexity is 149.0022434939896
At time: 465.32880687713623 and batch: 1150, loss is 5.028126230239868 and perplexity is 152.64671974839823
At time: 466.1673185825348 and batch: 1200, loss is 5.041144571304321 and perplexity is 154.64691819229893
At time: 466.9905846118927 and batch: 1250, loss is 5.0743297100067135 and perplexity is 159.86500014871768
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.059425521070939 and perplexity of 157.50000989182743
Finished 21 epochs...
Completing Train Step...
At time: 469.2351927757263 and batch: 50, loss is 5.057928562164307 and perplexity is 157.26441523098313
At time: 470.1013286113739 and batch: 100, loss is 5.0805441188812255 and perplexity is 160.8615599321806
At time: 470.9479627609253 and batch: 150, loss is 5.001008176803589 and perplexity is 148.56286125737114
At time: 471.7909688949585 and batch: 200, loss is 5.044641933441162 and perplexity is 155.18872135724394
At time: 472.62172770500183 and batch: 250, loss is 5.065526371002197 and perplexity is 158.46383089050656
At time: 473.44708275794983 and batch: 300, loss is 5.048991966247558 and perplexity is 155.8652678191378
At time: 474.2730531692505 and batch: 350, loss is 5.064636516571045 and perplexity is 158.32288386887777
At time: 475.14969277381897 and batch: 400, loss is 5.04209487915039 and perplexity is 154.79395022380888
At time: 475.97752380371094 and batch: 450, loss is 5.014006023406982 and perplexity is 150.50646249106296
At time: 476.8034420013428 and batch: 500, loss is 5.016251802444458 and perplexity is 150.8448465752508
At time: 477.630699634552 and batch: 550, loss is 5.023883419036865 and perplexity is 152.00044052560787
At time: 478.4894735813141 and batch: 600, loss is 5.039633388519287 and perplexity is 154.41339492438027
At time: 479.34289288520813 and batch: 650, loss is 5.055047416687012 and perplexity is 156.81196567178353
At time: 480.17003893852234 and batch: 700, loss is 5.06739504814148 and perplexity is 158.7602254753375
At time: 480.99768471717834 and batch: 750, loss is 5.044263162612915 and perplexity is 155.12995152757202
At time: 481.82686972618103 and batch: 800, loss is 5.0514812088012695 and perplexity is 156.25373757357008
At time: 482.65220189094543 and batch: 850, loss is 5.076085348129272 and perplexity is 160.14591175485032
At time: 483.47828245162964 and batch: 900, loss is 5.057969808578491 and perplexity is 157.27090195796646
At time: 484.30352663993835 and batch: 950, loss is 5.0241147232055665 and perplexity is 152.03560292759417
At time: 485.12874150276184 and batch: 1000, loss is 5.028100337982178 and perplexity is 152.64276743136227
At time: 485.95499658584595 and batch: 1050, loss is 5.0297314643859865 and perplexity is 152.89195024875903
At time: 486.78078389167786 and batch: 1100, loss is 5.0015441417694095 and perplexity is 148.642507088008
At time: 487.607182264328 and batch: 1150, loss is 5.026412372589111 and perplexity is 152.3853290570946
At time: 488.43299293518066 and batch: 1200, loss is 5.0389520454406735 and perplexity is 154.30822225990548
At time: 489.2622904777527 and batch: 1250, loss is 5.07137360572815 and perplexity is 159.39312034436858
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.058976890396898 and perplexity of 157.4293664038308
Finished 22 epochs...
Completing Train Step...
At time: 491.5825242996216 and batch: 50, loss is 5.053451013565064 and perplexity is 156.5618302718155
At time: 492.41977071762085 and batch: 100, loss is 5.075931997299194 and perplexity is 160.1213551292867
At time: 493.26461720466614 and batch: 150, loss is 4.997332382202148 and perplexity is 148.01777711639838
At time: 494.09859013557434 and batch: 200, loss is 5.040956392288208 and perplexity is 154.61781962534423
At time: 494.93430352211 and batch: 250, loss is 5.061674757003784 and perplexity is 157.8546632735467
At time: 495.7668435573578 and batch: 300, loss is 5.045255861282349 and perplexity is 155.28402528579744
At time: 496.62659072875977 and batch: 350, loss is 5.0608840179443355 and perplexity is 157.72989076333025
At time: 497.45055103302 and batch: 400, loss is 5.0379423236846925 and perplexity is 154.1524925258442
At time: 498.27618408203125 and batch: 450, loss is 5.009561405181885 and perplexity is 149.83900312469652
At time: 499.1024913787842 and batch: 500, loss is 5.012595157623291 and perplexity is 150.29426779722502
At time: 499.9290111064911 and batch: 550, loss is 5.0200198364257815 and perplexity is 151.4143072829321
At time: 500.758239030838 and batch: 600, loss is 5.036124620437622 and perplexity is 153.8725435487986
At time: 501.58258605003357 and batch: 650, loss is 5.052078142166137 and perplexity is 156.34703848735677
At time: 502.40922236442566 and batch: 700, loss is 5.063957204818726 and perplexity is 158.215369795126
At time: 503.2387502193451 and batch: 750, loss is 5.041016731262207 and perplexity is 154.62714938741377
At time: 504.0650556087494 and batch: 800, loss is 5.048742208480835 and perplexity is 155.82634411888813
At time: 504.89137268066406 and batch: 850, loss is 5.073434257507325 and perplexity is 159.7219127083338
At time: 505.7181794643402 and batch: 900, loss is 5.054932355880737 and perplexity is 156.79392379855594
At time: 506.54235434532166 and batch: 950, loss is 5.021154565811157 and perplexity is 151.58621906500144
At time: 507.36804008483887 and batch: 1000, loss is 5.025403175354004 and perplexity is 152.23161977887438
At time: 508.1941182613373 and batch: 1050, loss is 5.027513437271118 and perplexity is 152.5532075665636
At time: 509.0196158885956 and batch: 1100, loss is 4.999426536560058 and perplexity is 148.3280739807601
At time: 509.8474624156952 and batch: 1150, loss is 5.024372825622558 and perplexity is 152.07484874868138
At time: 510.6731321811676 and batch: 1200, loss is 5.036808881759644 and perplexity is 153.97786860969327
At time: 511.49741792678833 and batch: 1250, loss is 5.0686563205718995 and perplexity is 158.9605917024139
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.058754579864279 and perplexity of 157.39437208748237
Finished 23 epochs...
Completing Train Step...
At time: 513.7578048706055 and batch: 50, loss is 5.049516429901123 and perplexity is 155.94703492702592
At time: 514.6114978790283 and batch: 100, loss is 5.072317419052124 and perplexity is 159.5436287098272
At time: 515.4377853870392 and batch: 150, loss is 4.993632097244262 and perplexity is 147.4710812512628
At time: 516.2630488872528 and batch: 200, loss is 5.037162733078003 and perplexity is 154.03236352247873
At time: 517.0884113311768 and batch: 250, loss is 5.058114967346191 and perplexity is 157.29373286530284
At time: 517.9435684680939 and batch: 300, loss is 5.04159107208252 and perplexity is 154.71598357934494
At time: 518.7671637535095 and batch: 350, loss is 5.0576458835601805 and perplexity is 157.21996622829394
At time: 519.6084032058716 and batch: 400, loss is 5.034147872924804 and perplexity is 153.56867681390116
At time: 520.4390318393707 and batch: 450, loss is 5.006110095977784 and perplexity is 149.3227537745891
At time: 521.2652175426483 and batch: 500, loss is 5.008992938995362 and perplexity is 149.7538489239328
At time: 522.0910847187042 and batch: 550, loss is 5.01617317199707 and perplexity is 150.832986043784
At time: 522.918479681015 and batch: 600, loss is 5.032812538146973 and perplexity is 153.36374807365397
At time: 523.7448301315308 and batch: 650, loss is 5.049193964004517 and perplexity is 155.89675543373173
At time: 524.5707986354828 and batch: 700, loss is 5.061039476394654 and perplexity is 157.75441311377105
At time: 525.3972885608673 and batch: 750, loss is 5.037817058563232 and perplexity is 154.1331838045244
At time: 526.2237727642059 and batch: 800, loss is 5.0457947731018065 and perplexity is 155.3677322357034
At time: 527.0475327968597 and batch: 850, loss is 5.070635786056519 and perplexity is 159.27556033903122
At time: 527.8720469474792 and batch: 900, loss is 5.052059345245361 and perplexity is 156.34409967208114
At time: 528.6984839439392 and batch: 950, loss is 5.018261890411377 and perplexity is 151.14836293131575
At time: 529.5214955806732 and batch: 1000, loss is 5.02260726928711 and perplexity is 151.80658891961585
At time: 530.3505141735077 and batch: 1050, loss is 5.024884395599365 and perplexity is 152.15266557820837
At time: 531.187358379364 and batch: 1100, loss is 4.996766414642334 and perplexity is 147.93402755827674
At time: 532.0087766647339 and batch: 1150, loss is 5.022001390457153 and perplexity is 151.71464037879147
At time: 532.8357763290405 and batch: 1200, loss is 5.034529314041138 and perplexity is 153.62726539475543
At time: 533.6623959541321 and batch: 1250, loss is 5.066149673461914 and perplexity is 158.56263257454435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.058607560874772 and perplexity of 157.3712338268675
Finished 24 epochs...
Completing Train Step...
At time: 535.956255197525 and batch: 50, loss is 5.045853204727173 and perplexity is 155.37681089006514
At time: 536.7810733318329 and batch: 100, loss is 5.068783254623413 and perplexity is 158.9807704950105
At time: 537.6163492202759 and batch: 150, loss is 4.990203628540039 and perplexity is 146.96634699162524
At time: 538.4485704898834 and batch: 200, loss is 5.034038581848145 and perplexity is 153.55189404499112
At time: 539.3236300945282 and batch: 250, loss is 5.055034017562866 and perplexity is 156.80986454286455
At time: 540.1501495838165 and batch: 300, loss is 5.03834945678711 and perplexity is 154.21526588606093
At time: 540.9737300872803 and batch: 350, loss is 5.053926773071289 and perplexity is 156.63633377234842
At time: 541.7994701862335 and batch: 400, loss is 5.030707426071167 and perplexity is 153.04123977275222
At time: 542.6295788288116 and batch: 450, loss is 5.001884107589722 and perplexity is 148.6930490506465
At time: 543.4656302928925 and batch: 500, loss is 5.00549241065979 and perplexity is 149.23054778201575
At time: 544.290442943573 and batch: 550, loss is 5.01285611152649 and perplexity is 150.33349279077
At time: 545.1171960830688 and batch: 600, loss is 5.029322834014892 and perplexity is 152.8294867175087
At time: 545.946218252182 and batch: 650, loss is 5.0461382389068605 and perplexity is 155.42110490425375
At time: 546.774176120758 and batch: 700, loss is 5.0580011653900145 and perplexity is 157.27583354931633
At time: 547.6042892932892 and batch: 750, loss is 5.034736452102661 and perplexity is 153.65909074471318
At time: 548.4295842647552 and batch: 800, loss is 5.043204431533813 and perplexity is 154.96579753938826
At time: 549.2551329135895 and batch: 850, loss is 5.067746591567993 and perplexity is 158.8160464001599
At time: 550.080733537674 and batch: 900, loss is 5.048809270858765 and perplexity is 155.8367945544805
At time: 550.9071843624115 and batch: 950, loss is 5.015333518981934 and perplexity is 150.7063918273157
At time: 551.7330350875854 and batch: 1000, loss is 5.019857606887817 and perplexity is 151.38974540220556
At time: 552.5813715457916 and batch: 1050, loss is 5.022554121017456 and perplexity is 151.7985208764959
At time: 553.4192147254944 and batch: 1100, loss is 4.994374094009399 and perplexity is 147.58054492232517
At time: 554.2458863258362 and batch: 1150, loss is 5.019591579437256 and perplexity is 151.34947693069194
At time: 555.0737137794495 and batch: 1200, loss is 5.0320449066162105 and perplexity is 153.24606639881756
At time: 555.9035875797272 and batch: 1250, loss is 5.063392190933228 and perplexity is 158.12600116392886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.058284119097856 and perplexity of 157.32034162613863
Finished 25 epochs...
Completing Train Step...
At time: 558.1630911827087 and batch: 50, loss is 5.042007675170899 and perplexity is 154.78045216389893
At time: 559.0392191410065 and batch: 100, loss is 5.064977722167969 and perplexity is 158.37691374009802
At time: 559.8874044418335 and batch: 150, loss is 4.986699352264404 and perplexity is 146.45223762527667
At time: 560.7732744216919 and batch: 200, loss is 5.030748863220214 and perplexity is 153.0475814968056
At time: 561.6026248931885 and batch: 250, loss is 5.051617107391357 and perplexity is 156.27497367914802
At time: 562.4464962482452 and batch: 300, loss is 5.034908952713013 and perplexity is 153.68559931795963
At time: 563.2881174087524 and batch: 350, loss is 5.051013832092285 and perplexity is 156.18072527938838
At time: 564.1150257587433 and batch: 400, loss is 5.026973237991333 and perplexity is 152.47082068837693
At time: 564.9417998790741 and batch: 450, loss is 4.998429908752441 and perplexity is 148.18031973780415
At time: 565.7695596218109 and batch: 500, loss is 5.002040777206421 and perplexity is 148.71634655860532
At time: 566.5953307151794 and batch: 550, loss is 5.009608068466187 and perplexity is 149.8459952678357
At time: 567.423220872879 and batch: 600, loss is 5.025935659408569 and perplexity is 152.31270227465944
At time: 568.2500646114349 and batch: 650, loss is 5.0429896068573 and perplexity is 154.93251063761318
At time: 569.0759649276733 and batch: 700, loss is 5.055132312774658 and perplexity is 156.82527895928015
At time: 569.903116941452 and batch: 750, loss is 5.031960973739624 and perplexity is 153.2332045554124
At time: 570.7296717166901 and batch: 800, loss is 5.040310163497924 and perplexity is 154.51793341695944
At time: 571.5641837120056 and batch: 850, loss is 5.064690837860107 and perplexity is 158.3314844056116
At time: 572.3863451480865 and batch: 900, loss is 5.04592547416687 and perplexity is 155.38804029089638
At time: 573.2151167392731 and batch: 950, loss is 5.012500123977661 and perplexity is 150.2799854637008
At time: 574.0443501472473 and batch: 1000, loss is 5.01695034980774 and perplexity is 150.95025565742242
At time: 574.869523525238 and batch: 1050, loss is 5.020245790481567 and perplexity is 151.44852382530436
At time: 575.6981256008148 and batch: 1100, loss is 4.991678123474121 and perplexity is 147.18320796665907
At time: 576.5231821537018 and batch: 1150, loss is 5.017344532012939 and perplexity is 151.00976929090993
At time: 577.3515646457672 and batch: 1200, loss is 5.029775438308715 and perplexity is 152.89867365539132
At time: 578.1771838665009 and batch: 1250, loss is 5.060867366790771 and perplexity is 157.72726440056348
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0578167769160585 and perplexity of 157.2468363718328
Finished 26 epochs...
Completing Train Step...
At time: 580.4315280914307 and batch: 50, loss is 5.0383944320678715 and perplexity is 154.2222019169157
At time: 581.3043859004974 and batch: 100, loss is 5.061197395324707 and perplexity is 157.77932748907517
At time: 582.1287899017334 and batch: 150, loss is 4.9833770179748536 and perplexity is 145.9664817030218
At time: 582.9569857120514 and batch: 200, loss is 5.02774450302124 and perplexity is 152.5884614607304
At time: 583.7814223766327 and batch: 250, loss is 5.048367071151733 and perplexity is 155.76789880354684
At time: 584.6090552806854 and batch: 300, loss is 5.031888341903686 and perplexity is 153.2220753506111
At time: 585.4350373744965 and batch: 350, loss is 5.047582626342773 and perplexity is 155.64575539757163
At time: 586.2712354660034 and batch: 400, loss is 5.023687019348144 and perplexity is 151.97059061775545
At time: 587.1273641586304 and batch: 450, loss is 4.994781742095947 and perplexity is 147.64071811298422
At time: 587.9549977779388 and batch: 500, loss is 4.998776369094848 and perplexity is 148.2316672365409
At time: 588.782975435257 and batch: 550, loss is 5.006470279693604 and perplexity is 149.37654708605595
At time: 589.60733294487 and batch: 600, loss is 5.022592887878418 and perplexity is 151.80440574271717
At time: 590.4432063102722 and batch: 650, loss is 5.04033709526062 and perplexity is 154.5220949133124
At time: 591.2802662849426 and batch: 700, loss is 5.052198572158813 and perplexity is 156.36586849388274
At time: 592.1032230854034 and batch: 750, loss is 5.0288955593109135 and perplexity is 152.76420049238175
At time: 592.9284117221832 and batch: 800, loss is 5.0374239063262936 and perplexity is 154.07259790904578
At time: 593.7551765441895 and batch: 850, loss is 5.061887245178223 and perplexity is 157.88820908669044
At time: 594.5862536430359 and batch: 900, loss is 5.043157806396485 and perplexity is 154.95857240623445
At time: 595.4239885807037 and batch: 950, loss is 5.0098027324676515 and perplexity is 149.87516772819984
At time: 596.2515196800232 and batch: 1000, loss is 5.014025497436523 and perplexity is 150.5093934868987
At time: 597.0781900882721 and batch: 1050, loss is 5.017730875015259 and perplexity is 151.06812212993708
At time: 597.9230506420135 and batch: 1100, loss is 4.989673109054565 and perplexity is 146.88839915908753
At time: 598.7521874904633 and batch: 1150, loss is 5.015263500213623 and perplexity is 150.6958399208035
At time: 599.5790393352509 and batch: 1200, loss is 5.027451515197754 and perplexity is 152.5437614481174
At time: 600.4049668312073 and batch: 1250, loss is 5.058330392837524 and perplexity is 157.327621595106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.057872911439325 and perplexity of 157.25566359578133
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 602.6750710010529 and batch: 50, loss is 5.037144937515259 and perplexity is 154.02962245427852
At time: 603.5005898475647 and batch: 100, loss is 5.062646646499633 and perplexity is 158.00815513913705
At time: 604.3253488540649 and batch: 150, loss is 4.983481674194336 and perplexity is 145.98175880257583
At time: 605.147483587265 and batch: 200, loss is 5.028017997741699 and perplexity is 152.63019930662298
At time: 605.9742388725281 and batch: 250, loss is 5.046544780731201 and perplexity is 155.48430292923234
At time: 606.8014433383942 and batch: 300, loss is 5.029474439620972 and perplexity is 152.85265828089445
At time: 607.6254141330719 and batch: 350, loss is 5.043162879943847 and perplexity is 154.9593585978852
At time: 608.4507126808167 and batch: 400, loss is 5.016308975219727 and perplexity is 150.85347104030444
At time: 609.2773241996765 and batch: 450, loss is 4.987528009414673 and perplexity is 146.57364661542425
At time: 610.1030194759369 and batch: 500, loss is 4.988886289596557 and perplexity is 146.77286996474277
At time: 610.9288351535797 and batch: 550, loss is 4.996279821395874 and perplexity is 147.86206137009586
At time: 611.7542095184326 and batch: 600, loss is 5.012040519714356 and perplexity is 150.21093201153826
At time: 612.5801782608032 and batch: 650, loss is 5.029028978347778 and perplexity is 152.78458350458953
At time: 613.4095034599304 and batch: 700, loss is 5.040667734146118 and perplexity is 154.57319437382822
At time: 614.2328872680664 and batch: 750, loss is 5.01381856918335 and perplexity is 150.4782520631496
At time: 615.0564186573029 and batch: 800, loss is 5.020083360671997 and perplexity is 151.42392606817828
At time: 615.8805863857269 and batch: 850, loss is 5.039098424911499 and perplexity is 154.3308114690818
At time: 616.7081334590912 and batch: 900, loss is 5.020481481552124 and perplexity is 151.48422309685344
At time: 617.533623456955 and batch: 950, loss is 4.987262010574341 and perplexity is 146.53466338037884
At time: 618.3824656009674 and batch: 1000, loss is 4.990551977157593 and perplexity is 147.01755143340722
At time: 619.2200834751129 and batch: 1050, loss is 4.991808204650879 and perplexity is 147.20235497685624
At time: 620.049144744873 and batch: 1100, loss is 4.960744934082031 and perplexity is 142.70005844982362
At time: 620.8857402801514 and batch: 1150, loss is 4.983420534133911 and perplexity is 145.97283374186384
At time: 621.7247943878174 and batch: 1200, loss is 5.0021336555480955 and perplexity is 148.73015972771574
At time: 622.5501453876495 and batch: 1250, loss is 5.039543237686157 and perplexity is 154.3994750556347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.051940026944571 and perplexity of 156.32544607264975
Finished 28 epochs...
Completing Train Step...
At time: 624.7951633930206 and batch: 50, loss is 5.029743127822876 and perplexity is 152.89373350477103
At time: 625.6485502719879 and batch: 100, loss is 5.054180603027344 and perplexity is 156.67609781250522
At time: 626.4835712909698 and batch: 150, loss is 4.9759828567504885 and perplexity is 144.8911624452315
At time: 627.3078548908234 and batch: 200, loss is 5.01970211982727 and perplexity is 151.36620808561756
At time: 628.1327939033508 and batch: 250, loss is 5.03919548034668 and perplexity is 154.34579084005372
At time: 628.9595177173615 and batch: 300, loss is 5.02280348777771 and perplexity is 151.8363791019536
At time: 629.7840433120728 and batch: 350, loss is 5.037245416641236 and perplexity is 154.04509999368906
At time: 630.6090497970581 and batch: 400, loss is 5.0110253238677975 and perplexity is 150.05851587645643
At time: 631.4355773925781 and batch: 450, loss is 4.982188415527344 and perplexity is 145.79308865373042
At time: 632.2622208595276 and batch: 500, loss is 4.984205007553101 and perplexity is 146.08739047734025
At time: 633.0874645709991 and batch: 550, loss is 4.9919748878479 and perplexity is 147.22689318098912
At time: 633.9122970104218 and batch: 600, loss is 5.008141698837281 and perplexity is 149.62642667506347
At time: 634.738391160965 and batch: 650, loss is 5.025833702087402 and perplexity is 152.29717367119665
At time: 635.5633976459503 and batch: 700, loss is 5.037497987747193 and perplexity is 154.08401224881052
At time: 636.3881731033325 and batch: 750, loss is 5.011374855041504 and perplexity is 150.11097517318112
At time: 637.2149612903595 and batch: 800, loss is 5.018254642486572 and perplexity is 151.1472674233169
At time: 638.0400333404541 and batch: 850, loss is 5.038781070709229 and perplexity is 154.281841708313
At time: 638.8669407367706 and batch: 900, loss is 5.020628461837768 and perplexity is 151.50648992758735
At time: 639.6918902397156 and batch: 950, loss is 4.987422351837158 and perplexity is 146.55816081710574
At time: 640.5181047916412 and batch: 1000, loss is 4.991229305267334 and perplexity is 147.1171642851023
At time: 641.3428318500519 and batch: 1050, loss is 4.99295036315918 and perplexity is 147.37057945025396
At time: 642.168164730072 and batch: 1100, loss is 4.962410678863526 and perplexity is 142.93795841280772
At time: 642.9975271224976 and batch: 1150, loss is 4.985698442459107 and perplexity is 146.305725479583
At time: 643.8460462093353 and batch: 1200, loss is 5.004653453826904 and perplexity is 149.10540229744873
At time: 644.7274181842804 and batch: 1250, loss is 5.040290231704712 and perplexity is 154.51485362815595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.051592527514827 and perplexity of 156.27113250679812
Finished 29 epochs...
Completing Train Step...
At time: 647.0241043567657 and batch: 50, loss is 5.026622400283814 and perplexity is 152.41733755768107
At time: 647.8534255027771 and batch: 100, loss is 5.050738153457641 and perplexity is 156.13767552450716
At time: 648.6765224933624 and batch: 150, loss is 4.972463073730469 and perplexity is 144.38207345887724
At time: 649.5196170806885 and batch: 200, loss is 5.015948429107666 and perplexity is 150.79909121163695
At time: 650.3565936088562 and batch: 250, loss is 5.035453853607177 and perplexity is 153.7693655585252
At time: 651.1814956665039 and batch: 300, loss is 5.019443693161011 and perplexity is 151.32709607508858
At time: 652.0084698200226 and batch: 350, loss is 5.034078369140625 and perplexity is 153.5580035806505
At time: 652.8351826667786 and batch: 400, loss is 5.008244495391846 and perplexity is 149.64180854678577
At time: 653.6599068641663 and batch: 450, loss is 4.979313688278198 and perplexity is 145.37457513336923
At time: 654.4853954315186 and batch: 500, loss is 4.981696748733521 and perplexity is 145.72142465211905
At time: 655.3106286525726 and batch: 550, loss is 4.989823675155639 and perplexity is 146.91051723771656
At time: 656.1364743709564 and batch: 600, loss is 5.006223964691162 and perplexity is 149.33975793254194
At time: 656.9606919288635 and batch: 650, loss is 5.023982524871826 and perplexity is 152.01550540267687
At time: 657.7932448387146 and batch: 700, loss is 5.035842237472534 and perplexity is 153.8290986980372
At time: 658.6200551986694 and batch: 750, loss is 5.010157279968261 and perplexity is 149.92831501538353
At time: 659.445737361908 and batch: 800, loss is 5.017271938323975 and perplexity is 150.9988073325768
At time: 660.2710301876068 and batch: 850, loss is 5.038596172332763 and perplexity is 154.25331788334876
At time: 661.0952713489532 and batch: 900, loss is 5.020820932388306 and perplexity is 151.5356532715661
At time: 661.921544790268 and batch: 950, loss is 4.9875404644012455 and perplexity is 146.57547219959355
At time: 662.7467358112335 and batch: 1000, loss is 4.99151915550232 and perplexity is 147.15981241022675
At time: 663.572384595871 and batch: 1050, loss is 4.9934518527984615 and perplexity is 147.4445028033318
At time: 664.3987693786621 and batch: 1100, loss is 4.962985076904297 and perplexity is 143.02008528056982
At time: 665.2244873046875 and batch: 1150, loss is 4.9866992855072025 and perplexity is 146.45222784853547
At time: 666.1020634174347 and batch: 1200, loss is 5.005730924606323 and perplexity is 149.26614559402904
At time: 666.9283983707428 and batch: 1250, loss is 5.040354881286621 and perplexity is 154.52484327175142
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.051368880445939 and perplexity of 156.23618683395557
Finished 30 epochs...
Completing Train Step...
At time: 669.2209498882294 and batch: 50, loss is 5.024249181747437 and perplexity is 152.0560467874712
At time: 670.071165561676 and batch: 100, loss is 5.048192920684815 and perplexity is 155.7407741131966
At time: 670.8981263637543 and batch: 150, loss is 4.969913005828857 and perplexity is 144.01435841621142
At time: 671.724328994751 and batch: 200, loss is 5.013338422775268 and perplexity is 150.40601781382247
At time: 672.5492012500763 and batch: 250, loss is 5.032992677688599 and perplexity is 153.3913774374297
At time: 673.375513792038 and batch: 300, loss is 5.017187929153442 and perplexity is 150.9861225808465
At time: 674.2014172077179 and batch: 350, loss is 5.032137327194214 and perplexity is 153.26023014335146
At time: 675.0264401435852 and batch: 400, loss is 5.006266651153564 and perplexity is 149.34613285456456
At time: 675.851231098175 and batch: 450, loss is 4.977208700180054 and perplexity is 145.0688852326174
At time: 676.6765630245209 and batch: 500, loss is 4.979831504821777 and perplexity is 145.44987198668474
At time: 677.5016806125641 and batch: 550, loss is 4.9882918548583985 and perplexity is 146.68564899837094
At time: 678.3276979923248 and batch: 600, loss is 5.0047320365905765 and perplexity is 149.1171198724334
At time: 679.1575539112091 and batch: 650, loss is 5.02249737739563 and perplexity is 151.78990752301226
At time: 679.988596200943 and batch: 700, loss is 5.034467935562134 and perplexity is 153.61783627624735
At time: 680.834490776062 and batch: 750, loss is 5.008913383483887 and perplexity is 149.74193565377578
At time: 681.6896848678589 and batch: 800, loss is 5.0165836620330815 and perplexity is 150.89491419123095
At time: 682.514223575592 and batch: 850, loss is 5.038287343978882 and perplexity is 154.20568744030038
At time: 683.3384602069855 and batch: 900, loss is 5.020834493637085 and perplexity is 151.53770829819337
At time: 684.1750178337097 and batch: 950, loss is 4.987347040176392 and perplexity is 146.547123694233
At time: 685.0124800205231 and batch: 1000, loss is 4.991322116851807 and perplexity is 147.13081909587527
At time: 685.8394651412964 and batch: 1050, loss is 4.993437442779541 and perplexity is 147.44237814056495
At time: 686.7177150249481 and batch: 1100, loss is 4.963019752502442 and perplexity is 143.0250446735581
At time: 687.5431787967682 and batch: 1150, loss is 4.9870975303649905 and perplexity is 146.51056331031558
At time: 688.368182182312 and batch: 1200, loss is 5.006123600006103 and perplexity is 149.32477024690004
At time: 689.1936764717102 and batch: 1250, loss is 5.039980831146241 and perplexity is 154.46705404114203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0512000313640515 and perplexity of 156.20980872427347
Finished 31 epochs...
Completing Train Step...
At time: 691.4439313411713 and batch: 50, loss is 5.022192821502686 and perplexity is 151.7436860510548
At time: 692.2986147403717 and batch: 100, loss is 5.046024169921875 and perplexity is 155.4033771876825
At time: 693.1267378330231 and batch: 150, loss is 4.967850761413574 and perplexity is 143.71767163532257
At time: 693.9526131153107 and batch: 200, loss is 5.011264963150024 and perplexity is 150.0944801005414
At time: 694.7767767906189 and batch: 250, loss is 5.03093466758728 and perplexity is 153.0760210478311
At time: 695.6037464141846 and batch: 300, loss is 5.015321044921875 and perplexity is 150.7045119184579
At time: 696.4293260574341 and batch: 350, loss is 5.030450086593628 and perplexity is 153.00186128710396
At time: 697.2527878284454 and batch: 400, loss is 5.0045522117614745 and perplexity is 149.09030732268948
At time: 698.0778586864471 and batch: 450, loss is 4.975463523864746 and perplexity is 144.81593523539294
At time: 698.9034142494202 and batch: 500, loss is 4.978257961273194 and perplexity is 145.22118025436527
At time: 699.7293014526367 and batch: 550, loss is 4.987099514007569 and perplexity is 146.51085393519534
At time: 700.5554332733154 and batch: 600, loss is 5.003502969741821 and perplexity is 148.933957546245
At time: 701.3830955028534 and batch: 650, loss is 5.021378107070923 and perplexity is 151.62010862709099
At time: 702.2093191146851 and batch: 700, loss is 5.033260374069214 and perplexity is 153.43244525059453
At time: 703.0360035896301 and batch: 750, loss is 5.008050556182861 and perplexity is 149.61278994681828
At time: 703.874627828598 and batch: 800, loss is 5.015865154266358 and perplexity is 150.78653396410556
At time: 704.7090742588043 and batch: 850, loss is 5.037859783172608 and perplexity is 154.13976922527306
At time: 705.5459835529327 and batch: 900, loss is 5.020675849914551 and perplexity is 151.51366969888153
At time: 706.3862862586975 and batch: 950, loss is 4.987066307067871 and perplexity is 146.50598883888162
At time: 707.2135515213013 and batch: 1000, loss is 4.991094446182251 and perplexity is 147.0973255366746
At time: 708.089066028595 and batch: 1050, loss is 4.993252811431884 and perplexity is 147.41515816849451
At time: 708.91521692276 and batch: 1100, loss is 4.962872991561889 and perplexity is 143.00405572369598
At time: 709.7423303127289 and batch: 1150, loss is 4.987117738723755 and perplexity is 146.51352407825772
At time: 710.5680456161499 and batch: 1200, loss is 5.006287145614624 and perplexity is 149.34919365443338
At time: 711.3939497470856 and batch: 1250, loss is 5.039511117935181 and perplexity is 154.3945158625895
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.051022272040374 and perplexity of 156.18204344216483
Finished 32 epochs...
Completing Train Step...
At time: 713.649560213089 and batch: 50, loss is 5.020381860733032 and perplexity is 151.4691328661321
At time: 714.4751296043396 and batch: 100, loss is 5.044181156158447 and perplexity is 155.11723039188018
At time: 715.299307346344 and batch: 150, loss is 4.965950336456299 and perplexity is 143.44480634747046
At time: 716.1291863918304 and batch: 200, loss is 5.009507694244385 and perplexity is 149.83095534749336
At time: 716.9627993106842 and batch: 250, loss is 5.029215631484985 and perplexity is 152.81310388804422
At time: 717.7905807495117 and batch: 300, loss is 5.013733806610108 and perplexity is 150.46549767984254
At time: 718.6164965629578 and batch: 350, loss is 5.029003753662109 and perplexity is 152.78072961010238
At time: 719.4392991065979 and batch: 400, loss is 5.003310127258301 and perplexity is 148.90523952110792
At time: 720.263918876648 and batch: 450, loss is 4.974081172943115 and perplexity is 144.61588709407562
At time: 721.089174747467 and batch: 500, loss is 4.976932611465454 and perplexity is 145.02883887899264
At time: 721.918080329895 and batch: 550, loss is 4.98600905418396 and perplexity is 146.35117681182297
At time: 722.7431886196136 and batch: 600, loss is 5.002484827041626 and perplexity is 148.7823986919211
At time: 723.56995844841 and batch: 650, loss is 5.020305862426758 and perplexity is 151.45762190599376
At time: 724.4106004238129 and batch: 700, loss is 5.0322062778472905 and perplexity is 153.27079790063326
At time: 725.2518332004547 and batch: 750, loss is 5.007251110076904 and perplexity is 149.493230381576
At time: 726.0800156593323 and batch: 800, loss is 5.015161924362182 and perplexity is 150.68053363994233
At time: 726.9043250083923 and batch: 850, loss is 5.0373584651947025 and perplexity is 154.0625155537954
At time: 727.7329070568085 and batch: 900, loss is 5.020404720306397 and perplexity is 151.47259542546354
At time: 728.5554826259613 and batch: 950, loss is 4.986697196960449 and perplexity is 146.4519219765299
At time: 729.4558141231537 and batch: 1000, loss is 4.990747852325439 and perplexity is 147.04635134147193
At time: 730.2824940681458 and batch: 1050, loss is 4.993055429458618 and perplexity is 147.3860639451179
At time: 731.3661856651306 and batch: 1100, loss is 4.962628154754639 and perplexity is 142.96904735311244
At time: 732.1987400054932 and batch: 1150, loss is 4.987090702056885 and perplexity is 146.50956289446412
At time: 733.0251860618591 and batch: 1200, loss is 5.0061765480041505 and perplexity is 149.33267690386248
At time: 733.8533270359039 and batch: 1250, loss is 5.038938779830932 and perplexity is 154.30617528082635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.050955445226962 and perplexity of 156.17160664262235
Finished 33 epochs...
Completing Train Step...
At time: 736.1439971923828 and batch: 50, loss is 5.018815183639527 and perplexity is 151.2320154370199
At time: 736.9698359966278 and batch: 100, loss is 5.042592525482178 and perplexity is 154.8710020360063
At time: 737.796088218689 and batch: 150, loss is 4.9643918228149415 and perplexity is 143.22141978071838
At time: 738.6495008468628 and batch: 200, loss is 5.008119249343872 and perplexity is 149.62306767528818
At time: 739.4754014015198 and batch: 250, loss is 5.027772645950318 and perplexity is 152.59275580740686
At time: 740.3011841773987 and batch: 300, loss is 5.012312107086181 and perplexity is 150.25173294404095
At time: 741.132973909378 and batch: 350, loss is 5.027647771835327 and perplexity is 152.57370211175288
At time: 741.9578378200531 and batch: 400, loss is 5.00200888633728 and perplexity is 148.71160394068158
At time: 742.786670923233 and batch: 450, loss is 4.972761020660401 and perplexity is 144.42509806360763
At time: 743.6130352020264 and batch: 500, loss is 4.9757216262817385 and perplexity is 144.85331740230257
At time: 744.4393372535706 and batch: 550, loss is 4.984908752441406 and perplexity is 146.19023491551383
At time: 745.2647597789764 and batch: 600, loss is 5.001529731750488 and perplexity is 148.64036516210098
At time: 746.0894694328308 and batch: 650, loss is 5.01923436164856 and perplexity is 151.29542186051364
At time: 746.9151048660278 and batch: 700, loss is 5.031198282241821 and perplexity is 153.11637944953748
At time: 747.7432496547699 and batch: 750, loss is 5.006360769271851 and perplexity is 149.3601896930534
At time: 748.5682833194733 and batch: 800, loss is 5.014413967132568 and perplexity is 150.56787318329793
At time: 749.3942823410034 and batch: 850, loss is 5.036812181472778 and perplexity is 153.97837669332705
At time: 750.21897149086 and batch: 900, loss is 5.020057363510132 and perplexity is 151.4199895270317
At time: 751.0450336933136 and batch: 950, loss is 4.986249361038208 and perplexity is 146.3863502287755
At time: 751.8685388565063 and batch: 1000, loss is 4.990327606201172 and perplexity is 146.9845686651074
At time: 752.6941282749176 and batch: 1050, loss is 4.992766714096069 and perplexity is 147.34351746643787
At time: 753.5176327228546 and batch: 1100, loss is 4.962206735610962 and perplexity is 142.90881015305064
At time: 754.3435044288635 and batch: 1150, loss is 4.986825208663941 and perplexity is 146.47067073654648
At time: 755.1686065196991 and batch: 1200, loss is 5.005899724960327 and perplexity is 149.2913438989277
At time: 756.0020244121552 and batch: 1250, loss is 5.0383013725280765 and perplexity is 154.20785073754658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.050843621692518 and perplexity of 156.15414395797558
Finished 34 epochs...
Completing Train Step...
At time: 758.309472322464 and batch: 50, loss is 5.017381553649902 and perplexity is 151.01536002325489
At time: 759.1347172260284 and batch: 100, loss is 5.0410227203369145 and perplexity is 154.6280754637364
At time: 759.9623084068298 and batch: 150, loss is 4.962862110137939 and perplexity is 143.00249964440522
At time: 760.8092877864838 and batch: 200, loss is 5.0067494773864745 and perplexity is 149.41825849596515
At time: 761.6436922550201 and batch: 250, loss is 5.0264817237854 and perplexity is 152.3958975284254
At time: 762.4884264469147 and batch: 300, loss is 5.010923767089844 and perplexity is 150.04327719088855
At time: 763.3149044513702 and batch: 350, loss is 5.026386137008667 and perplexity is 152.381331191979
At time: 764.147477388382 and batch: 400, loss is 5.000876703262329 and perplexity is 148.54333045582186
At time: 764.9734485149384 and batch: 450, loss is 4.9714907455444335 and perplexity is 144.24175492818114
At time: 765.8148710727692 and batch: 500, loss is 4.974599866867066 and perplexity is 144.69091793335608
At time: 766.6961667537689 and batch: 550, loss is 4.983918628692627 and perplexity is 146.0455601268757
At time: 767.5561647415161 and batch: 600, loss is 5.000642738342285 and perplexity is 148.50858059267168
At time: 768.398241519928 and batch: 650, loss is 5.018275136947632 and perplexity is 151.15036513684626
At time: 769.2266185283661 and batch: 700, loss is 5.030255460739136 and perplexity is 152.9720860667199
At time: 770.0509674549103 and batch: 750, loss is 5.0054997634887695 and perplexity is 149.2316450527461
At time: 770.8791179656982 and batch: 800, loss is 5.013576278686523 and perplexity is 150.44179702922807
At time: 771.703729391098 and batch: 850, loss is 5.036228742599487 and perplexity is 153.88856592481233
At time: 772.5295555591583 and batch: 900, loss is 5.0196208572387695 and perplexity is 151.35390817550498
At time: 773.3545341491699 and batch: 950, loss is 4.985783052444458 and perplexity is 146.31810492857775
At time: 774.1804842948914 and batch: 1000, loss is 4.9898363208770755 and perplexity is 146.9123750389402
At time: 775.0070989131927 and batch: 1050, loss is 4.9923180103302 and perplexity is 147.2774187057647
At time: 775.833021402359 and batch: 1100, loss is 4.961738328933716 and perplexity is 142.84188638713078
At time: 776.6587002277374 and batch: 1150, loss is 4.986581945419312 and perplexity is 146.4350441394366
At time: 777.4791243076324 and batch: 1200, loss is 5.005576457977295 and perplexity is 149.24309073634004
At time: 778.306015253067 and batch: 1250, loss is 5.037739038467407 and perplexity is 154.12115878785679
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.050832038378194 and perplexity of 156.15233518591882
Finished 35 epochs...
Completing Train Step...
At time: 780.619889497757 and batch: 50, loss is 5.016073675155639 and perplexity is 150.81797938465905
At time: 781.4486587047577 and batch: 100, loss is 5.0396622371673585 and perplexity is 154.4178496063234
At time: 782.2774770259857 and batch: 150, loss is 4.961609468460083 and perplexity is 142.8234808998917
At time: 783.1068921089172 and batch: 200, loss is 5.005575008392334 and perplexity is 149.24287439595705
At time: 783.9356553554535 and batch: 250, loss is 5.025366725921631 and perplexity is 152.2260711238675
At time: 784.7653186321259 and batch: 300, loss is 5.009813394546509 and perplexity is 149.8767657175758
At time: 785.5954170227051 and batch: 350, loss is 5.0253442287445065 and perplexity is 152.2226465055047
At time: 786.441995382309 and batch: 400, loss is 4.999880180358887 and perplexity is 148.39537735643898
At time: 787.2686913013458 and batch: 450, loss is 4.970267267227173 and perplexity is 144.06538618225161
At time: 788.1444928646088 and batch: 500, loss is 4.973566427230835 and perplexity is 144.5414658419198
At time: 788.9734144210815 and batch: 550, loss is 4.983095560073853 and perplexity is 145.92540406454933
At time: 789.8002200126648 and batch: 600, loss is 4.999821615219116 and perplexity is 148.38668681490665
At time: 790.6333999633789 and batch: 650, loss is 5.0173876667022705 and perplexity is 151.0162831908808
At time: 791.4594006538391 and batch: 700, loss is 5.029345989227295 and perplexity is 152.83302555770618
At time: 792.285080909729 and batch: 750, loss is 5.0047194766998295 and perplexity is 149.11524698946093
At time: 793.1126842498779 and batch: 800, loss is 5.0127841758728025 and perplexity is 150.32267884165395
At time: 793.9354746341705 and batch: 850, loss is 5.035649957656861 and perplexity is 153.7995233107605
At time: 794.7613332271576 and batch: 900, loss is 5.0192013931274415 and perplexity is 151.29043395642526
At time: 795.5846569538116 and batch: 950, loss is 4.985262479782104 and perplexity is 146.24195554550775
At time: 796.4066169261932 and batch: 1000, loss is 4.989394903182983 and perplexity is 146.8475396279185
At time: 797.2344870567322 and batch: 1050, loss is 4.991864032745362 and perplexity is 147.21057323324058
At time: 798.0736613273621 and batch: 1100, loss is 4.961219959259033 and perplexity is 142.76786067295342
At time: 798.9047899246216 and batch: 1150, loss is 4.986299905776978 and perplexity is 146.3937494756022
At time: 799.7324800491333 and batch: 1200, loss is 5.0052077579498295 and perplexity is 149.18807494747122
At time: 800.5604314804077 and batch: 1250, loss is 5.03716555595398 and perplexity is 154.03279833735104
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.050811099309991 and perplexity of 156.14906553575415
Finished 36 epochs...
Completing Train Step...
At time: 802.881165266037 and batch: 50, loss is 5.014827728271484 and perplexity is 150.6301852082487
At time: 803.7387170791626 and batch: 100, loss is 5.0383665657043455 and perplexity is 154.21790436485216
At time: 804.5655767917633 and batch: 150, loss is 4.960371646881104 and perplexity is 142.64680028534664
At time: 805.4107799530029 and batch: 200, loss is 5.004495172500611 and perplexity is 149.0818035642842
At time: 806.2315571308136 and batch: 250, loss is 5.024275865554809 and perplexity is 152.0601042758679
At time: 807.0523324012756 and batch: 300, loss is 5.008662996292114 and perplexity is 149.70444688456644
At time: 807.8887865543365 and batch: 350, loss is 5.024268636703491 and perplexity is 152.05900505995567
At time: 808.7975835800171 and batch: 400, loss is 4.998904590606689 and perplexity is 148.2506749435891
At time: 809.6246206760406 and batch: 450, loss is 4.9691253280639645 and perplexity is 143.90096617239894
At time: 810.449933052063 and batch: 500, loss is 4.972605504989624 and perplexity is 144.4026394439842
At time: 811.276957988739 and batch: 550, loss is 4.982333393096924 and perplexity is 145.81422691363468
At time: 812.1027052402496 and batch: 600, loss is 4.999071435928345 and perplexity is 148.27541193870428
At time: 812.9290907382965 and batch: 650, loss is 5.016475744247437 and perplexity is 150.87863082487462
At time: 813.7527825832367 and batch: 700, loss is 5.0284450721740725 and perplexity is 152.69539768364385
At time: 814.5777292251587 and batch: 750, loss is 5.003923397064209 and perplexity is 148.99658661579235
At time: 815.4059607982635 and batch: 800, loss is 5.011893815994263 and perplexity is 150.18889712535295
At time: 816.2313249111176 and batch: 850, loss is 5.035085887908935 and perplexity is 153.71279411537282
At time: 817.0564863681793 and batch: 900, loss is 5.018764390945434 and perplexity is 151.2243341506011
At time: 817.8827750682831 and batch: 950, loss is 4.984768581390381 and perplexity is 146.16974471273633
At time: 818.7077567577362 and batch: 1000, loss is 4.98887451171875 and perplexity is 146.7711413019949
At time: 819.5427234172821 and batch: 1050, loss is 4.991351079940796 and perplexity is 147.13508052059353
At time: 820.3664419651031 and batch: 1100, loss is 4.960692510604859 and perplexity is 142.69257781264915
At time: 821.1949851512909 and batch: 1150, loss is 4.985926694869995 and perplexity is 146.33912392564343
At time: 822.0424540042877 and batch: 1200, loss is 5.004847679138184 and perplexity is 149.1343651531922
At time: 822.87646651268 and batch: 1250, loss is 5.03659104347229 and perplexity is 153.94432998763563
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.050782586536268 and perplexity of 156.14461335625384
Finished 37 epochs...
Completing Train Step...
At time: 825.3366167545319 and batch: 50, loss is 5.013625926971436 and perplexity is 150.44926639184874
At time: 826.1722273826599 and batch: 100, loss is 5.037157592773437 and perplexity is 154.0315717512522
At time: 827.0029606819153 and batch: 150, loss is 4.959224958419799 and perplexity is 142.4833225923226
At time: 827.8380494117737 and batch: 200, loss is 5.003391075134277 and perplexity is 148.91729357183723
At time: 828.67458319664 and batch: 250, loss is 5.023215293884277 and perplexity is 151.89891912636497
At time: 829.5094928741455 and batch: 300, loss is 5.007484769821167 and perplexity is 149.5281650128055
At time: 830.4258871078491 and batch: 350, loss is 5.023296709060669 and perplexity is 151.91128650709877
At time: 831.2741711139679 and batch: 400, loss is 4.9979683303833005 and perplexity is 148.1119386903011
At time: 832.1083505153656 and batch: 450, loss is 4.968019914627075 and perplexity is 143.74198399751572
At time: 832.9599883556366 and batch: 500, loss is 4.971648206710816 and perplexity is 144.2644691914134
At time: 833.7952160835266 and batch: 550, loss is 4.981541662216187 and perplexity is 145.69882697621154
At time: 834.6334207057953 and batch: 600, loss is 4.998226127624512 and perplexity is 148.15012646162546
At time: 835.4574544429779 and batch: 650, loss is 5.015606918334961 and perplexity is 150.74760049029254
At time: 836.2836332321167 and batch: 700, loss is 5.02755934715271 and perplexity is 152.5602114270315
At time: 837.1148869991302 and batch: 750, loss is 5.0031752777099605 and perplexity is 148.8851610706284
At time: 837.9439988136292 and batch: 800, loss is 5.011068134307862 and perplexity is 150.06494008506706
At time: 838.7701253890991 and batch: 850, loss is 5.034360017776489 and perplexity is 153.60125907402445
At time: 839.5955457687378 and batch: 900, loss is 5.018325138092041 and perplexity is 151.15792301703067
At time: 840.4198863506317 and batch: 950, loss is 4.984186716079712 and perplexity is 146.08471834816362
At time: 841.2495172023773 and batch: 1000, loss is 4.9882848262786865 and perplexity is 146.68461801021752
At time: 842.0954520702362 and batch: 1050, loss is 4.990808725357056 and perplexity is 147.0553027711138
At time: 842.9211747646332 and batch: 1100, loss is 4.960126895904541 and perplexity is 142.6118916138132
At time: 843.7489376068115 and batch: 1150, loss is 4.985520401000977 and perplexity is 146.27967931360328
At time: 844.5756249427795 and batch: 1200, loss is 5.004413270950318 and perplexity is 149.0695940334484
At time: 845.4168386459351 and batch: 1250, loss is 5.035954732894897 and perplexity is 153.84640474087402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.050752291714188 and perplexity of 156.1398830546256
Finished 38 epochs...
Completing Train Step...
At time: 847.6756641864777 and batch: 50, loss is 5.012468748092651 and perplexity is 150.27527037012806
At time: 848.5270578861237 and batch: 100, loss is 5.035992622375488 and perplexity is 153.85223401167383
At time: 849.3502485752106 and batch: 150, loss is 4.958149423599243 and perplexity is 142.3301591985723
At time: 850.174788236618 and batch: 200, loss is 5.002318181991577 and perplexity is 148.75760690741612
At time: 851.0126144886017 and batch: 250, loss is 5.022168188095093 and perplexity is 151.73994813302568
At time: 851.8870549201965 and batch: 300, loss is 5.006352939605713 and perplexity is 149.359020257212
At time: 852.7171530723572 and batch: 350, loss is 5.022325534820556 and perplexity is 151.7638257954731
At time: 853.5442144870758 and batch: 400, loss is 4.997041444778443 and perplexity is 147.9747194695057
At time: 854.3682751655579 and batch: 450, loss is 4.966962404251099 and perplexity is 143.59005570500656
At time: 855.195506811142 and batch: 500, loss is 4.970655326843262 and perplexity is 144.1213029895309
At time: 856.0210003852844 and batch: 550, loss is 4.980713148117065 and perplexity is 145.57816343646053
At time: 856.8457577228546 and batch: 600, loss is 4.997328577041626 and perplexity is 148.01721388606785
At time: 857.6705284118652 and batch: 650, loss is 5.014683227539063 and perplexity is 150.6084206086995
At time: 858.4963278770447 and batch: 700, loss is 5.026655693054199 and perplexity is 152.42241203757436
At time: 859.328905582428 and batch: 750, loss is 5.002327346801758 and perplexity is 148.75897024889377
At time: 860.1636724472046 and batch: 800, loss is 5.0102682876586915 and perplexity is 149.94495913515902
At time: 860.9987018108368 and batch: 850, loss is 5.03364013671875 and perplexity is 153.4907242279167
At time: 861.8399722576141 and batch: 900, loss is 5.0178392791748045 and perplexity is 151.08449943041845
At time: 862.6808655261993 and batch: 950, loss is 4.983614683151245 and perplexity is 146.00117697540873
At time: 863.5031788349152 and batch: 1000, loss is 4.987736577987671 and perplexity is 146.60422045999871
At time: 864.3331758975983 and batch: 1050, loss is 4.990257120132446 and perplexity is 146.97420866582095
At time: 865.15877866745 and batch: 1100, loss is 4.95947377204895 and perplexity is 142.51877879572066
At time: 865.9866828918457 and batch: 1150, loss is 4.9851078414917 and perplexity is 146.2193426879686
At time: 866.8176958560944 and batch: 1200, loss is 5.0039472484588625 and perplexity is 149.0001404345633
At time: 867.6560921669006 and batch: 1250, loss is 5.035360450744629 and perplexity is 153.7550037303382
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.050712195626141 and perplexity of 156.13362258163806
Finished 39 epochs...
Completing Train Step...
At time: 870.1921842098236 and batch: 50, loss is 5.011398544311524 and perplexity is 150.11453123472495
At time: 871.0439441204071 and batch: 100, loss is 5.0348935794830325 and perplexity is 153.68323669205725
At time: 871.8692946434021 and batch: 150, loss is 4.957125701904297 and perplexity is 142.18452728274997
At time: 872.6952705383301 and batch: 200, loss is 5.001370525360107 and perplexity is 148.61670254976812
At time: 873.598393201828 and batch: 250, loss is 5.021144123077392 and perplexity is 151.5846360987386
At time: 874.4280564785004 and batch: 300, loss is 5.005293207168579 and perplexity is 149.20082349659128
At time: 875.2547731399536 and batch: 350, loss is 5.021383752822876 and perplexity is 151.6209646390318
At time: 876.0815320014954 and batch: 400, loss is 4.996133422851562 and perplexity is 147.84041616400427
At time: 876.9067678451538 and batch: 450, loss is 4.965931005477906 and perplexity is 143.44203344581987
At time: 877.7319419384003 and batch: 500, loss is 4.96976469039917 and perplexity is 143.99300044865905
At time: 878.5575287342072 and batch: 550, loss is 4.979964799880982 and perplexity is 145.4692610281855
At time: 879.3828873634338 and batch: 600, loss is 4.996473741531372 and perplexity is 147.89073758142928
At time: 880.2089519500732 and batch: 650, loss is 5.013843278884888 and perplexity is 150.48197038178515
At time: 881.0369584560394 and batch: 700, loss is 5.025839786529541 and perplexity is 152.29810031735693
At time: 881.8636832237244 and batch: 750, loss is 5.00146674156189 and perplexity is 148.63100257234484
At time: 882.6897854804993 and batch: 800, loss is 5.0094645309448245 and perplexity is 149.8244882886551
At time: 883.5155577659607 and batch: 850, loss is 5.032910432815552 and perplexity is 153.37876230183807
At time: 884.3435587882996 and batch: 900, loss is 5.017329330444336 and perplexity is 151.00747372299048
At time: 885.1669244766235 and batch: 950, loss is 4.983031616210938 and perplexity is 145.91607332884084
At time: 885.9932837486267 and batch: 1000, loss is 4.9871149921417235 and perplexity is 146.5131216673978
At time: 886.8180823326111 and batch: 1050, loss is 4.989711618423462 and perplexity is 146.8940558475544
At time: 887.6455533504486 and batch: 1100, loss is 4.958770513534546 and perplexity is 142.41858648574697
At time: 888.4727010726929 and batch: 1150, loss is 4.9846971988677975 and perplexity is 146.15931112002585
At time: 889.2998297214508 and batch: 1200, loss is 5.003486204147339 and perplexity is 148.93146060083956
At time: 890.124673128128 and batch: 1250, loss is 5.0346975421905515 and perplexity is 153.6531119993145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.050619529111542 and perplexity of 156.11915489336724
Finished 40 epochs...
Completing Train Step...
At time: 892.572190284729 and batch: 50, loss is 5.010310001373291 and perplexity is 149.95121402684654
At time: 893.4046723842621 and batch: 100, loss is 5.03383770942688 and perplexity is 153.5210528019258
At time: 894.2887046337128 and batch: 150, loss is 4.9560745906829835 and perplexity is 142.0351540483201
At time: 895.1331114768982 and batch: 200, loss is 5.00037917137146 and perplexity is 148.46944379375364
At time: 895.9572937488556 and batch: 250, loss is 5.020089359283447 and perplexity is 151.4248344041994
At time: 896.7818109989166 and batch: 300, loss is 5.0042203998565675 and perplexity is 149.04084559026322
At time: 897.6054832935333 and batch: 350, loss is 5.020460968017578 and perplexity is 151.48111565188225
At time: 898.4555954933167 and batch: 400, loss is 4.995194339752198 and perplexity is 147.70164689591695
At time: 899.2945790290833 and batch: 450, loss is 4.964847497940063 and perplexity is 143.28669709059074
At time: 900.1211709976196 and batch: 500, loss is 4.968811235427856 and perplexity is 143.85577503606845
At time: 900.9476556777954 and batch: 550, loss is 4.979160270690918 and perplexity is 145.35227382755036
At time: 901.7722806930542 and batch: 600, loss is 4.995555229187012 and perplexity is 147.75496047936224
At time: 902.5969450473785 and batch: 650, loss is 5.012949247360229 and perplexity is 150.34749487799797
At time: 903.4240639209747 and batch: 700, loss is 5.0249285316467285 and perplexity is 152.15938114366097
At time: 904.2474422454834 and batch: 750, loss is 5.000645771026611 and perplexity is 148.50903097299923
At time: 905.0732266902924 and batch: 800, loss is 5.008600835800171 and perplexity is 149.69514147171955
At time: 905.9016563892365 and batch: 850, loss is 5.03222734451294 and perplexity is 153.2740268392978
At time: 906.7257010936737 and batch: 900, loss is 5.016779403686524 and perplexity is 150.92445350217326
At time: 907.5530095100403 and batch: 950, loss is 4.98238320350647 and perplexity is 145.8214901608861
At time: 908.3832318782806 and batch: 1000, loss is 4.986504917144775 and perplexity is 146.42376493506927
At time: 909.2114901542664 and batch: 1050, loss is 4.989138517379761 and perplexity is 146.80989482952188
At time: 910.0416328907013 and batch: 1100, loss is 4.957982454299927 and perplexity is 142.30639641550212
At time: 910.8717932701111 and batch: 1150, loss is 4.984218893051147 and perplexity is 146.08941898759875
At time: 911.7028434276581 and batch: 1200, loss is 5.002997999191284 and perplexity is 148.85876926923976
At time: 912.5426034927368 and batch: 1250, loss is 5.03405198097229 and perplexity is 153.5539515196663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.050568295221259 and perplexity of 156.1111565066103
Finished 41 epochs...
Completing Train Step...
At time: 914.8534994125366 and batch: 50, loss is 5.009272804260254 and perplexity is 149.79576568978035
At time: 915.7052755355835 and batch: 100, loss is 5.032794799804687 and perplexity is 153.36102767912413
At time: 916.5285243988037 and batch: 150, loss is 4.955098190307617 and perplexity is 141.8965385537151
At time: 917.3713212013245 and batch: 200, loss is 4.999412488937378 and perplexity is 148.3259903385791
At time: 918.2037210464478 and batch: 250, loss is 5.019045267105103 and perplexity is 151.26681542653526
At time: 919.0302917957306 and batch: 300, loss is 5.00315601348877 and perplexity is 148.88229294157975
At time: 919.8777997493744 and batch: 350, loss is 5.01954740524292 and perplexity is 151.34279133715188
At time: 920.726628780365 and batch: 400, loss is 4.9942711353302 and perplexity is 147.56535100653065
At time: 921.5716891288757 and batch: 450, loss is 4.963806295394898 and perplexity is 143.13758425869284
At time: 922.396595954895 and batch: 500, loss is 4.967904844284058 and perplexity is 143.72544450973157
At time: 923.2208094596863 and batch: 550, loss is 4.978363990783691 and perplexity is 145.23657880135733
At time: 924.0488781929016 and batch: 600, loss is 4.994708404541016 and perplexity is 147.62989090073503
At time: 924.9102509021759 and batch: 650, loss is 5.012128686904907 and perplexity is 150.22417627124977
At time: 925.7458198070526 and batch: 700, loss is 5.024110298156739 and perplexity is 152.03493016411616
At time: 926.5897307395935 and batch: 750, loss is 4.999933786392212 and perplexity is 148.40333245720166
At time: 927.4171841144562 and batch: 800, loss is 5.007926330566407 and perplexity is 149.59420536012237
At time: 928.2431247234344 and batch: 850, loss is 5.031527919769287 and perplexity is 153.16686067406968
At time: 929.0690608024597 and batch: 900, loss is 5.016321249008179 and perplexity is 150.85532259525806
At time: 929.8948395252228 and batch: 950, loss is 4.981784763336182 and perplexity is 145.73425082984622
At time: 930.7230713367462 and batch: 1000, loss is 4.985939731597901 and perplexity is 146.3410317214196
At time: 931.5579171180725 and batch: 1050, loss is 4.9885768222808835 and perplexity is 146.7274555861564
At time: 932.3918933868408 and batch: 1100, loss is 4.9573702716827395 and perplexity is 142.21930557377158
At time: 933.2180426120758 and batch: 1150, loss is 4.983737545013428 and perplexity is 146.01911605388452
At time: 934.0616683959961 and batch: 1200, loss is 5.002502822875977 and perplexity is 148.78507617941398
At time: 934.8940711021423 and batch: 1250, loss is 5.0334409236907955 and perplexity is 153.4601499214815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.050523744012318 and perplexity of 156.1042017207819
Finished 42 epochs...
Completing Train Step...
At time: 937.2146003246307 and batch: 50, loss is 5.008223791122436 and perplexity is 149.63871035453963
At time: 938.0465312004089 and batch: 100, loss is 5.031826295852661 and perplexity is 153.2125688208301
At time: 938.8768961429596 and batch: 150, loss is 4.954141654968262 and perplexity is 141.76087439418163
At time: 939.7053322792053 and batch: 200, loss is 4.99842264175415 and perplexity is 148.17924291558646
At time: 940.5550925731659 and batch: 250, loss is 5.01818510055542 and perplexity is 151.13675671592355
At time: 941.3807101249695 and batch: 300, loss is 5.002166538238526 and perplexity is 148.7350504559254
At time: 942.2080745697021 and batch: 350, loss is 5.018583421707153 and perplexity is 151.1969696741792
At time: 943.0330533981323 and batch: 400, loss is 4.993351411819458 and perplexity is 147.42969407683552
At time: 943.8602764606476 and batch: 450, loss is 4.962754421234131 and perplexity is 142.98710069113972
At time: 944.6849989891052 and batch: 500, loss is 4.967025775909423 and perplexity is 143.59915553328804
At time: 945.5087249279022 and batch: 550, loss is 4.977558679580689 and perplexity is 145.1196652395818
At time: 946.33376121521 and batch: 600, loss is 4.99384838104248 and perplexity is 147.50298030634315
At time: 947.1584477424622 and batch: 650, loss is 5.011320610046386 and perplexity is 150.10283262491393
At time: 947.9774720668793 and batch: 700, loss is 5.02327787399292 and perplexity is 151.9084252746714
At time: 948.8013315200806 and batch: 750, loss is 4.99900598526001 and perplexity is 148.26570753147868
At time: 949.6270761489868 and batch: 800, loss is 5.007101449966431 and perplexity is 149.47085888230438
At time: 950.4580488204956 and batch: 850, loss is 5.030865821838379 and perplexity is 153.065482777285
At time: 951.3041076660156 and batch: 900, loss is 5.015777626037598 and perplexity is 150.77333646345218
At time: 952.1427528858185 and batch: 950, loss is 4.981120824813843 and perplexity is 145.63752436046383
At time: 952.9667959213257 and batch: 1000, loss is 4.985295715332032 and perplexity is 146.2468160580932
At time: 953.7904517650604 and batch: 1050, loss is 4.987983589172363 and perplexity is 146.64043781503716
At time: 954.6170325279236 and batch: 1100, loss is 4.9567098045349125 and perplexity is 142.1254054070544
At time: 955.4426782131195 and batch: 1150, loss is 4.983158845901489 and perplexity is 145.9346393667475
At time: 956.2681877613068 and batch: 1200, loss is 5.001983213424682 and perplexity is 148.70778612967874
At time: 957.0942854881287 and batch: 1250, loss is 5.032783632278442 and perplexity is 153.35931502538563
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.050484984460538 and perplexity of 156.09815130914865
Finished 43 epochs...
Completing Train Step...
At time: 959.351065158844 and batch: 50, loss is 5.007214441299438 and perplexity is 149.4877487480815
At time: 960.2014501094818 and batch: 100, loss is 5.030879020690918 and perplexity is 153.0675030793538
At time: 961.0263192653656 and batch: 150, loss is 4.953246793746948 and perplexity is 141.63407482745586
At time: 961.8502469062805 and batch: 200, loss is 4.997564449310302 and perplexity is 148.0521311599568
At time: 962.6761064529419 and batch: 250, loss is 5.017240257263183 and perplexity is 150.9940236059594
At time: 963.517763376236 and batch: 300, loss is 5.001211309432984 and perplexity is 148.59304228728092
At time: 964.3488581180573 and batch: 350, loss is 5.017716131210327 and perplexity is 151.0658948274325
At time: 965.1720805168152 and batch: 400, loss is 4.992477884292603 and perplexity is 147.3009664125481
At time: 965.9963417053223 and batch: 450, loss is 4.961807165145874 and perplexity is 142.85171941995765
At time: 966.82182097435 and batch: 500, loss is 4.96611403465271 and perplexity is 143.4682899256114
At time: 967.6474380493164 and batch: 550, loss is 4.9767786693573 and perplexity is 145.0065145521635
At time: 968.4731552600861 and batch: 600, loss is 4.993000965118409 and perplexity is 147.3780368789855
At time: 969.304892539978 and batch: 650, loss is 5.01050479888916 and perplexity is 149.98042699605506
At time: 970.1528141498566 and batch: 700, loss is 5.022406215667725 and perplexity is 151.77607072346677
At time: 970.9757153987885 and batch: 750, loss is 4.998133249282837 and perplexity is 148.13636716254118
At time: 971.8119888305664 and batch: 800, loss is 5.006237983703613 and perplexity is 149.34185154314304
At time: 972.6446895599365 and batch: 850, loss is 5.030207891464233 and perplexity is 152.9648094685779
At time: 973.4887208938599 and batch: 900, loss is 5.015115413665772 and perplexity is 150.67352554636412
At time: 974.3196227550507 and batch: 950, loss is 4.980552892684937 and perplexity is 145.55483561422562
At time: 975.1428835391998 and batch: 1000, loss is 4.984654903411865 and perplexity is 146.15312937605404
At time: 975.9677090644836 and batch: 1050, loss is 4.987361183166504 and perplexity is 146.54919632341094
At time: 976.7901465892792 and batch: 1100, loss is 4.95600061416626 and perplexity is 142.02464717100713
At time: 977.6510074138641 and batch: 1150, loss is 4.982603740692139 and perplexity is 145.85365276833133
At time: 978.4858863353729 and batch: 1200, loss is 5.001488609313965 and perplexity is 148.63425283379755
At time: 979.38410115242 and batch: 1250, loss is 5.03213583946228 and perplexity is 153.2600021333826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0504725101220345 and perplexity of 156.09620410011456
Finished 44 epochs...
Completing Train Step...
At time: 981.809130191803 and batch: 50, loss is 5.006295204162598 and perplexity is 149.3503971969247
At time: 982.6356070041656 and batch: 100, loss is 5.029937362670898 and perplexity is 152.92343368017305
At time: 983.468193769455 and batch: 150, loss is 4.952311868667603 and perplexity is 141.50171945962697
At time: 984.3157866001129 and batch: 200, loss is 4.996713123321533 and perplexity is 147.9261441686168
At time: 985.1412677764893 and batch: 250, loss is 5.016368055343628 and perplexity is 150.86238374534375
At time: 985.96794962883 and batch: 300, loss is 5.000253190994263 and perplexity is 148.4507407353562
At time: 986.8223805427551 and batch: 350, loss is 5.016855974197387 and perplexity is 150.9360103071293
At time: 987.669221162796 and batch: 400, loss is 4.991666994094849 and perplexity is 147.18156991803033
At time: 988.502224445343 and batch: 450, loss is 4.960864782333374 and perplexity is 142.71716182717978
At time: 989.3308110237122 and batch: 500, loss is 4.965280990600586 and perplexity is 143.34882428699905
At time: 990.1563596725464 and batch: 550, loss is 4.976062507629394 and perplexity is 144.90270361329192
At time: 990.9817781448364 and batch: 600, loss is 4.992195291519165 and perplexity is 147.25934610499556
At time: 991.8087658882141 and batch: 650, loss is 5.009693288803101 and perplexity is 149.85876573818186
At time: 992.6317851543427 and batch: 700, loss is 5.02159815788269 and perplexity is 151.65347642624556
At time: 993.4587352275848 and batch: 750, loss is 4.9972217082977295 and perplexity is 148.00139631756173
At time: 994.2847583293915 and batch: 800, loss is 5.005464658737183 and perplexity is 149.22640640486895
At time: 995.1113493442535 and batch: 850, loss is 5.029506587982178 and perplexity is 152.85757232235292
At time: 995.9380438327789 and batch: 900, loss is 5.0145163822174075 and perplexity is 150.58329439447294
At time: 996.7657806873322 and batch: 950, loss is 4.979979448318481 and perplexity is 145.471391941171
At time: 997.5918893814087 and batch: 1000, loss is 4.98396035194397 and perplexity is 146.05165374961038
At time: 998.4180645942688 and batch: 1050, loss is 4.9867061996459965 and perplexity is 146.45324044306608
At time: 999.2448360919952 and batch: 1100, loss is 4.9554124927520755 and perplexity is 141.94114399207467
At time: 1000.0713052749634 and batch: 1150, loss is 4.9820594978332515 and perplexity is 145.77429455639844
At time: 1000.9504585266113 and batch: 1200, loss is 5.000959949493408 and perplexity is 148.55569664294634
At time: 1001.7754600048065 and batch: 1250, loss is 5.031465644836426 and perplexity is 153.15732251510184
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.050638240619297 and perplexity of 156.12207614547515
Annealing...
Finished 45 epochs...
Completing Train Step...
At time: 1004.0576276779175 and batch: 50, loss is 5.006590423583984 and perplexity is 149.39449484368959
At time: 1004.8824417591095 and batch: 100, loss is 5.030529737472534 and perplexity is 153.01404850518458
At time: 1005.7093856334686 and batch: 150, loss is 4.953053960800171 and perplexity is 141.60676574457287
At time: 1006.5346779823303 and batch: 200, loss is 4.996523876190185 and perplexity is 147.89815221896313
At time: 1007.3600299358368 and batch: 250, loss is 5.016750087738037 and perplexity is 150.92002907352253
At time: 1008.1851830482483 and batch: 300, loss is 4.999833364486694 and perplexity is 148.38843026003707
At time: 1009.0097949504852 and batch: 350, loss is 5.015386762619019 and perplexity is 150.71441619736999
At time: 1009.8359372615814 and batch: 400, loss is 4.989400510787964 and perplexity is 146.8483630932219
At time: 1010.6621205806732 and batch: 450, loss is 4.95801438331604 and perplexity is 142.310940191265
At time: 1011.48557472229 and batch: 500, loss is 4.962678232192993 and perplexity is 142.97620705603612
At time: 1012.3123562335968 and batch: 550, loss is 4.97215669631958 and perplexity is 144.33784482870465
At time: 1013.138897895813 and batch: 600, loss is 4.987572689056396 and perplexity is 146.5801956197436
At time: 1013.9628579616547 and batch: 650, loss is 5.00489185333252 and perplexity is 149.1409531891302
At time: 1014.7880790233612 and batch: 700, loss is 5.017202587127685 and perplexity is 150.98833574776262
At time: 1015.613329410553 and batch: 750, loss is 4.991833391189576 and perplexity is 147.20606254135623
At time: 1016.441666841507 and batch: 800, loss is 4.999276704788208 and perplexity is 148.3058513874873
At time: 1017.2654433250427 and batch: 850, loss is 5.021881113052368 and perplexity is 151.6963936329364
At time: 1018.0917644500732 and batch: 900, loss is 5.0060861206054685 and perplexity is 149.31917374888866
At time: 1018.9161624908447 and batch: 950, loss is 4.972195463180542 and perplexity is 144.3434404623286
At time: 1019.7411034107208 and batch: 1000, loss is 4.975727701187134 and perplexity is 144.8541973751749
At time: 1020.5674276351929 and batch: 1050, loss is 4.977352933883667 and perplexity is 145.08981056424633
At time: 1021.4440441131592 and batch: 1100, loss is 4.945175399780274 and perplexity is 140.4954915687454
At time: 1022.2698152065277 and batch: 1150, loss is 4.970898132324219 and perplexity is 144.15630068046315
At time: 1023.0955350399017 and batch: 1200, loss is 4.991545028686524 and perplexity is 147.1636199524171
At time: 1023.9479339122772 and batch: 1250, loss is 5.025340366363525 and perplexity is 152.22205856478536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.049454514997719 and perplexity of 155.93737978032408
Finished 46 epochs...
Completing Train Step...
At time: 1026.3270981311798 and batch: 50, loss is 5.0051178550720214 and perplexity is 149.17466311308908
At time: 1027.1815991401672 and batch: 100, loss is 5.028454484939576 and perplexity is 152.69683497638007
At time: 1028.0053095817566 and batch: 150, loss is 4.95120020866394 and perplexity is 141.34450505832552
At time: 1028.8279082775116 and batch: 200, loss is 4.994480800628662 and perplexity is 147.59629358357114
At time: 1029.6506652832031 and batch: 250, loss is 5.015090093612671 and perplexity is 150.66971053299483
At time: 1030.4787211418152 and batch: 300, loss is 4.998269872665405 and perplexity is 148.15660743672015
At time: 1031.3157978057861 and batch: 350, loss is 5.013882598876953 and perplexity is 150.4878874479952
At time: 1032.1470956802368 and batch: 400, loss is 4.988051586151123 and perplexity is 146.6504092607828
At time: 1032.990938425064 and batch: 450, loss is 4.956872034072876 and perplexity is 142.14846421626584
At time: 1033.8378341197968 and batch: 500, loss is 4.9616978740692135 and perplexity is 142.83610785485735
At time: 1034.6664505004883 and batch: 550, loss is 4.971273260116577 and perplexity is 144.21038785945967
At time: 1035.4976086616516 and batch: 600, loss is 4.986584024429321 and perplexity is 146.43534857967558
At time: 1036.3344731330872 and batch: 650, loss is 5.004330444335937 and perplexity is 149.0572476149806
At time: 1037.1601085662842 and batch: 700, loss is 5.016641111373901 and perplexity is 150.90358325359787
At time: 1037.9879698753357 and batch: 750, loss is 4.991468095779419 and perplexity is 147.15229866280959
At time: 1038.821050643921 and batch: 800, loss is 4.999010915756226 and perplexity is 148.26643855679075
At time: 1039.6490457057953 and batch: 850, loss is 5.02182279586792 and perplexity is 151.68754738431554
At time: 1040.4804587364197 and batch: 900, loss is 5.0060100269317624 and perplexity is 149.30781193668983
At time: 1041.3451175689697 and batch: 950, loss is 4.972223272323609 and perplexity is 144.34745458552953
At time: 1042.184382200241 and batch: 1000, loss is 4.9758321475982665 and perplexity is 144.86932766636673
At time: 1043.0622839927673 and batch: 1050, loss is 4.977833213806153 and perplexity is 145.15951102374342
At time: 1043.9136941432953 and batch: 1100, loss is 4.945612859725952 and perplexity is 140.55696616421136
At time: 1044.7411873340607 and batch: 1150, loss is 4.971526365280152 and perplexity is 144.2468928728768
At time: 1045.5670342445374 and batch: 1200, loss is 4.992407751083374 and perplexity is 147.29063608530487
At time: 1046.3960673809052 and batch: 1250, loss is 5.0257931900024415 and perplexity is 152.29100392013322
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.049438922074589 and perplexity of 155.93494827970528
Finished 47 epochs...
Completing Train Step...
At time: 1048.671770811081 and batch: 50, loss is 5.0042244243621825 and perplexity is 149.04144540719014
At time: 1049.4968194961548 and batch: 100, loss is 5.027376298904419 and perplexity is 152.5322881033065
At time: 1050.3251140117645 and batch: 150, loss is 4.950136194229126 and perplexity is 141.1941924462373
At time: 1051.1495821475983 and batch: 200, loss is 4.993301782608032 and perplexity is 147.42237743893872
At time: 1051.9745211601257 and batch: 250, loss is 5.014227962493896 and perplexity is 150.53986946494237
At time: 1052.8131721019745 and batch: 300, loss is 4.9974188709259035 and perplexity is 148.03057953865888
At time: 1053.6554925441742 and batch: 350, loss is 5.012946443557739 and perplexity is 150.34707333390847
At time: 1054.4913125038147 and batch: 400, loss is 4.98728687286377 and perplexity is 146.53830661288055
At time: 1055.3266417980194 and batch: 450, loss is 4.956205902099609 and perplexity is 142.05380611019632
At time: 1056.1621265411377 and batch: 500, loss is 4.961091642379761 and perplexity is 142.74954232191345
At time: 1056.9955863952637 and batch: 550, loss is 4.970738382339477 and perplexity is 144.1332735529696
At time: 1057.8305823802948 and batch: 600, loss is 4.985989189147949 and perplexity is 146.34826956930172
At time: 1058.6786184310913 and batch: 650, loss is 5.00385648727417 and perplexity is 148.98661761898077
At time: 1059.5067615509033 and batch: 700, loss is 5.016276693344116 and perplexity is 150.84860128591967
At time: 1060.3335568904877 and batch: 750, loss is 4.991238021850586 and perplexity is 147.11844664970158
At time: 1061.1714282035828 and batch: 800, loss is 4.998859310150147 and perplexity is 148.24396223732327
At time: 1062.0014643669128 and batch: 850, loss is 5.021783323287964 and perplexity is 151.68156000364243
At time: 1062.8274869918823 and batch: 900, loss is 5.005947170257568 and perplexity is 149.29842723914877
At time: 1063.65358710289 and batch: 950, loss is 4.972306547164917 and perplexity is 144.35947559742033
At time: 1064.5476019382477 and batch: 1000, loss is 4.975967750549317 and perplexity is 144.88897370671538
At time: 1065.381765127182 and batch: 1050, loss is 4.978135080337524 and perplexity is 145.20333643621157
At time: 1066.2080228328705 and batch: 1100, loss is 4.94592695236206 and perplexity is 140.6011210062296
At time: 1067.0344359874725 and batch: 1150, loss is 4.971901502609253 and perplexity is 144.30101541805905
At time: 1067.8630373477936 and batch: 1200, loss is 4.992894401550293 and perplexity is 147.3623325862814
At time: 1068.6885659694672 and batch: 1250, loss is 5.026036787033081 and perplexity is 152.3281060752852
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.049466543824133 and perplexity of 155.93925553527845
Annealing...
Finished 48 epochs...
Completing Train Step...
At time: 1070.9694283008575 and batch: 50, loss is 5.0040941905975345 and perplexity is 149.02203644254487
At time: 1071.8238248825073 and batch: 100, loss is 5.027251043319702 and perplexity is 152.51318377885863
At time: 1072.678780555725 and batch: 150, loss is 4.950044469833374 and perplexity is 141.1812420881925
At time: 1073.5308148860931 and batch: 200, loss is 4.992927083969116 and perplexity is 147.36714882245641
At time: 1074.3568024635315 and batch: 250, loss is 5.014123086929321 and perplexity is 150.52408233899754
At time: 1075.1936151981354 and batch: 300, loss is 4.997388696670532 and perplexity is 148.02611289353825
At time: 1076.023666381836 and batch: 350, loss is 5.012254028320313 and perplexity is 150.24300676222734
At time: 1076.854902267456 and batch: 400, loss is 4.986382169723511 and perplexity is 146.40579289854477
At time: 1077.6946074962616 and batch: 450, loss is 4.955079917907715 and perplexity is 141.89394578710596
At time: 1078.541481256485 and batch: 500, loss is 4.960302267074585 and perplexity is 142.63690382125372
At time: 1079.3780126571655 and batch: 550, loss is 4.969131002426147 and perplexity is 143.90178272091617
At time: 1080.2029325962067 and batch: 600, loss is 4.9840804290771485 and perplexity is 146.069192266455
At time: 1081.0268619060516 and batch: 650, loss is 5.002351236343384 and perplexity is 148.76252407495522
At time: 1081.8504374027252 and batch: 700, loss is 5.015013418197632 and perplexity is 150.65815831329712
At time: 1082.6754171848297 and batch: 750, loss is 4.989676055908203 and perplexity is 146.8888320183387
At time: 1083.5003955364227 and batch: 800, loss is 4.997471199035645 and perplexity is 148.03832590174454
At time: 1084.3271350860596 and batch: 850, loss is 5.019392356872559 and perplexity is 151.31932770303524
At time: 1085.151848077774 and batch: 900, loss is 5.003256130218506 and perplexity is 148.89719929603993
At time: 1086.0286841392517 and batch: 950, loss is 4.96989351272583 and perplexity is 144.01155115684654
At time: 1086.8538925647736 and batch: 1000, loss is 4.973001222610474 and perplexity is 144.45979342058143
At time: 1087.6790490150452 and batch: 1050, loss is 4.975371923446655 and perplexity is 144.8026706427096
At time: 1088.504186630249 and batch: 1100, loss is 4.943085527420044 and perplexity is 140.20218052225803
At time: 1089.3261258602142 and batch: 1150, loss is 4.968478689193725 and perplexity is 143.8079442932297
At time: 1090.1517672538757 and batch: 1200, loss is 4.990005016326904 and perplexity is 146.93716057867792
At time: 1090.9812021255493 and batch: 1250, loss is 5.024570426940918 and perplexity is 152.10490190845766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.049659450558851 and perplexity of 155.96934016954907
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 1093.2622256278992 and batch: 50, loss is 5.004137849807739 and perplexity is 149.02854276898853
At time: 1094.0864415168762 and batch: 100, loss is 5.026807680130005 and perplexity is 152.44558003484067
At time: 1094.9132063388824 and batch: 150, loss is 4.949714088439942 and perplexity is 141.13460613695437
At time: 1095.7386651039124 and batch: 200, loss is 4.992666091918945 and perplexity is 147.32869218681378
At time: 1096.5656230449677 and batch: 250, loss is 5.013603401184082 and perplexity is 150.44587744183602
At time: 1097.3919920921326 and batch: 300, loss is 4.996870718002319 and perplexity is 147.94945837913647
At time: 1098.220205783844 and batch: 350, loss is 5.0116919326782225 and perplexity is 150.15857955317782
At time: 1099.0494830608368 and batch: 400, loss is 4.985755958557129 and perplexity is 146.3141406560326
At time: 1099.8717613220215 and batch: 450, loss is 4.954640607833863 and perplexity is 141.83162403759158
At time: 1100.7032327651978 and batch: 500, loss is 4.96003981590271 and perplexity is 142.59947351072248
At time: 1101.531545162201 and batch: 550, loss is 4.968549203872681 and perplexity is 143.81808522179074
At time: 1102.3580367565155 and batch: 600, loss is 4.983478336334229 and perplexity is 145.98127153669992
At time: 1103.1831858158112 and batch: 650, loss is 5.002077684402466 and perplexity is 148.72183536325045
At time: 1104.014811038971 and batch: 700, loss is 5.014883785247803 and perplexity is 150.6386293176472
At time: 1104.8381023406982 and batch: 750, loss is 4.989409532546997 and perplexity is 146.8496879297443
At time: 1105.664033651352 and batch: 800, loss is 4.99706262588501 and perplexity is 147.9778537710019
At time: 1106.5452654361725 and batch: 850, loss is 5.018575315475464 and perplexity is 151.1957440414799
At time: 1107.3744230270386 and batch: 900, loss is 5.002484893798828 and perplexity is 148.782408624218
At time: 1108.2000951766968 and batch: 950, loss is 4.969208984375 and perplexity is 143.91300489993418
At time: 1109.036123752594 and batch: 1000, loss is 4.972109270095825 and perplexity is 144.33099959210273
At time: 1109.8709080219269 and batch: 1050, loss is 4.974553699493408 and perplexity is 144.68423808787966
At time: 1110.7023389339447 and batch: 1100, loss is 4.942387323379517 and perplexity is 140.10432495887693
At time: 1111.529048204422 and batch: 1150, loss is 4.967562913894653 and perplexity is 143.67630881349567
At time: 1112.3570370674133 and batch: 1200, loss is 4.989034786224365 and perplexity is 146.7946668593288
At time: 1113.183277606964 and batch: 1250, loss is 5.023890352249145 and perplexity is 152.00149438058196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.049342245951186 and perplexity of 155.91987382208427
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f106faa1128>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'seq_len': 35, 'num_layers': 1, 'data': 'wikitext', 'dropout': 0.8897950912075626, 'wordvec_source': '', 'lr': 22.34800878682042, 'tune_wordvecs': True, 'batch_size': 50, 'anneal': 3.2464305562406413}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.4221928119659424 and batch: 50, loss is 7.957769622802735 and perplexity is 2857.692107992107
At time: 2.2465903759002686 and batch: 100, loss is 7.287634325027466 and perplexity is 1462.1077310263554
At time: 3.0667126178741455 and batch: 150, loss is 7.096095952987671 and perplexity is 1207.244722197057
At time: 3.891381025314331 and batch: 200, loss is 7.007720966339111 and perplexity is 1105.132997391036
At time: 4.715159177780151 and batch: 250, loss is 7.027907066345215 and perplexity is 1127.6680043424399
At time: 5.53917670249939 and batch: 300, loss is 6.9829918575286865 and perplexity is 1078.1391853170446
At time: 6.36529278755188 and batch: 350, loss is 6.963119163513183 and perplexity is 1056.9251434193845
At time: 7.189014673233032 and batch: 400, loss is 6.897172346115112 and perplexity is 989.4728693431789
At time: 8.01695990562439 and batch: 450, loss is 6.869180555343628 and perplexity is 962.1598059720528
At time: 8.896964311599731 and batch: 500, loss is 6.842646236419678 and perplexity is 936.9652887107945
At time: 9.745991706848145 and batch: 550, loss is 6.848099908828735 and perplexity is 942.0891496839749
At time: 10.576856136322021 and batch: 600, loss is 6.849953527450562 and perplexity is 943.8370431387121
At time: 11.436463117599487 and batch: 650, loss is 6.8182229232788085 and perplexity is 914.3586799274987
At time: 12.282873153686523 and batch: 700, loss is 6.808755283355713 and perplexity is 905.7427119848828
At time: 13.114804744720459 and batch: 750, loss is 6.729137525558472 and perplexity is 836.4255591050558
At time: 13.937238216400146 and batch: 800, loss is 6.728728141784668 and perplexity is 836.0832101340013
At time: 14.762793779373169 and batch: 850, loss is 6.759842329025268 and perplexity is 862.5061928755201
At time: 15.587928771972656 and batch: 900, loss is 6.754563970565796 and perplexity is 857.9655700756036
At time: 16.438941717147827 and batch: 950, loss is 6.721112670898438 and perplexity is 829.7402258501276
At time: 17.28584623336792 and batch: 1000, loss is 6.713949823379517 and perplexity is 823.8181578869263
At time: 18.10873293876648 and batch: 1050, loss is 6.685566415786743 and perplexity is 800.7641151828597
At time: 18.943698167800903 and batch: 1100, loss is 6.670329694747925 and perplexity is 788.6555772061798
At time: 19.767313957214355 and batch: 1150, loss is 6.7166831588745115 and perplexity is 826.0730095284575
At time: 20.590601921081543 and batch: 1200, loss is 6.70930516242981 and perplexity is 820.0006741809161
At time: 21.42077398300171 and batch: 1250, loss is 6.684195041656494 and perplexity is 799.6667206321009
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.952335998089644 and perplexity of 384.65083399314926
Finished 1 epochs...
Completing Train Step...
At time: 23.824577808380127 and batch: 50, loss is 6.242547540664673 and perplexity is 514.1667044852923
At time: 24.647034168243408 and batch: 100, loss is 6.153894634246826 and perplexity is 470.54642898232214
At time: 25.47066354751587 and batch: 150, loss is 6.0085604381561275 and perplexity is 406.89714485313493
At time: 26.30889105796814 and batch: 200, loss is 6.01368483543396 and perplexity is 408.98759905898663
At time: 27.157185792922974 and batch: 250, loss is 5.983284330368042 and perplexity is 396.74126007525075
At time: 27.979702711105347 and batch: 300, loss is 5.9725486946105955 and perplexity is 392.5047718038482
At time: 28.79973793029785 and batch: 350, loss is 5.955777168273926 and perplexity is 385.976763039764
At time: 29.635173320770264 and batch: 400, loss is 5.925619516372681 and perplexity is 374.51037881788534
At time: 30.456952333450317 and batch: 450, loss is 5.88788480758667 and perplexity is 360.64165055758076
At time: 31.276694536209106 and batch: 500, loss is 5.905996904373169 and perplexity is 367.2331396895069
At time: 32.099252462387085 and batch: 550, loss is 5.897851190567017 and perplexity is 364.2539140663818
At time: 32.91987729072571 and batch: 600, loss is 5.921727409362793 and perplexity is 373.0555773052994
At time: 33.741973876953125 and batch: 650, loss is 5.904533081054687 and perplexity is 366.69596851407624
At time: 34.56146311759949 and batch: 700, loss is 5.912097568511963 and perplexity is 369.48035351403445
At time: 35.38113880157471 and batch: 750, loss is 5.844338893890381 and perplexity is 345.2742034260777
At time: 36.2022750377655 and batch: 800, loss is 5.877673616409302 and perplexity is 356.97780765259455
At time: 37.08098745346069 and batch: 850, loss is 5.885007610321045 and perplexity is 359.6055046995314
At time: 37.9008903503418 and batch: 900, loss is 5.889027633666992 and perplexity is 361.0540368395043
At time: 38.722278356552124 and batch: 950, loss is 5.861563997268677 and perplexity is 351.2731047978978
At time: 39.544183015823364 and batch: 1000, loss is 5.8468200206756595 and perplexity is 346.1319361322437
At time: 40.36365365982056 and batch: 1050, loss is 5.860377330780029 and perplexity is 350.85650800568396
At time: 41.192978382110596 and batch: 1100, loss is 5.842593326568603 and perplexity is 344.672029779746
At time: 42.03042125701904 and batch: 1150, loss is 5.905996589660645 and perplexity is 367.2330241166566
At time: 42.857338666915894 and batch: 1200, loss is 5.910824012756348 and perplexity is 369.0100991943382
At time: 43.676658153533936 and batch: 1250, loss is 5.8779263687133785 and perplexity is 357.0680460194801
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.645273473140967 and perplexity of 282.9509251213736
Finished 2 epochs...
Completing Train Step...
At time: 46.00879406929016 and batch: 50, loss is 5.8467690372467045 and perplexity is 346.1142895891133
At time: 46.85234618186951 and batch: 100, loss is 5.892702178955078 and perplexity is 362.3831867645683
At time: 47.68420886993408 and batch: 150, loss is 5.831616907119751 and perplexity is 340.9094525980323
At time: 48.50748538970947 and batch: 200, loss is 5.8391317653656 and perplexity is 343.480989073145
At time: 49.33941102027893 and batch: 250, loss is 5.877223644256592 and perplexity is 356.8172137141252
At time: 50.16169619560242 and batch: 300, loss is 5.880040168762207 and perplexity is 357.82361475187
At time: 50.99814534187317 and batch: 350, loss is 5.9326260375976565 and perplexity is 377.1436078512164
At time: 51.827086448669434 and batch: 400, loss is 5.868362922668457 and perplexity is 353.6695217310776
At time: 52.67927432060242 and batch: 450, loss is 5.865544872283936 and perplexity is 352.6742661981328
At time: 53.54477047920227 and batch: 500, loss is 5.836655921936035 and perplexity is 342.63163578990446
At time: 54.380533933639526 and batch: 550, loss is 5.810922651290894 and perplexity is 333.9270818377675
At time: 55.205634355545044 and batch: 600, loss is 5.808745670318603 and perplexity is 333.20091964190425
At time: 56.02738165855408 and batch: 650, loss is 5.814334897994995 and perplexity is 335.0684696631293
At time: 56.86562418937683 and batch: 700, loss is 5.8775563621521 and perplexity is 356.9359529387902
At time: 57.70294952392578 and batch: 750, loss is 5.780792436599731 and perplexity is 324.01585072931033
At time: 58.595407485961914 and batch: 800, loss is 5.777152681350708 and perplexity is 322.8386559858299
At time: 59.416093826293945 and batch: 850, loss is 5.825160322189331 and perplexity is 338.7154323247942
At time: 60.23914980888367 and batch: 900, loss is 5.833872423171997 and perplexity is 341.6792471563699
At time: 61.06147241592407 and batch: 950, loss is 5.82677791595459 and perplexity is 339.26377967838266
At time: 61.88348388671875 and batch: 1000, loss is 5.817518405914306 and perplexity is 336.1368625060295
At time: 62.70516562461853 and batch: 1050, loss is 5.819456043243409 and perplexity is 336.7888052488327
At time: 63.55535936355591 and batch: 1100, loss is 5.8148290634155275 and perplexity is 335.2340898328575
At time: 64.38608431816101 and batch: 1150, loss is 5.837941656112671 and perplexity is 343.0724523197484
At time: 65.20712614059448 and batch: 1200, loss is 5.804876155853272 and perplexity is 331.91408518143027
At time: 66.0283591747284 and batch: 1250, loss is 5.822235174179077 and perplexity is 337.72608724772255
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.673275690009124 and perplexity of 290.9861553750973
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 68.23805832862854 and batch: 50, loss is 5.76247350692749 and perplexity is 318.1342638148511
At time: 69.08796286582947 and batch: 100, loss is 5.737114448547363 and perplexity is 310.1681123981734
At time: 69.90843224525452 and batch: 150, loss is 5.642185335159302 and perplexity is 282.0784814291555
At time: 70.72971606254578 and batch: 200, loss is 5.676788454055786 and perplexity is 292.0101184972622
At time: 71.55251693725586 and batch: 250, loss is 5.691076755523682 and perplexity is 296.2123973224081
At time: 72.37716102600098 and batch: 300, loss is 5.655443525314331 and perplexity is 285.84323332579555
At time: 73.20055365562439 and batch: 350, loss is 5.682261409759522 and perplexity is 293.6126582552916
At time: 74.02249121665955 and batch: 400, loss is 5.6340508460998535 and perplexity is 279.7932244084383
At time: 74.8450255393982 and batch: 450, loss is 5.615954084396362 and perplexity is 274.7754131084207
At time: 75.66924381256104 and batch: 500, loss is 5.607558183670044 and perplexity is 272.47808358832305
At time: 76.4912462234497 and batch: 550, loss is 5.615163125991821 and perplexity is 274.55816311524165
At time: 77.31524538993835 and batch: 600, loss is 5.620434417724609 and perplexity is 276.0092605094122
At time: 78.13977384567261 and batch: 650, loss is 5.6023502540588375 and perplexity is 271.0627256496643
At time: 79.01277136802673 and batch: 700, loss is 5.626142749786377 and perplexity is 277.58931847778234
At time: 79.83496689796448 and batch: 750, loss is 5.583992071151734 and perplexity is 266.1319054846154
At time: 80.67161822319031 and batch: 800, loss is 5.597026700973511 and perplexity is 269.6235430216689
At time: 81.50028395652771 and batch: 850, loss is 5.597965497970581 and perplexity is 269.87678364636673
At time: 82.32185196876526 and batch: 900, loss is 5.581106147766113 and perplexity is 265.3649763765258
At time: 83.14516067504883 and batch: 950, loss is 5.595666847229004 and perplexity is 269.25714361834616
At time: 83.9674563407898 and batch: 1000, loss is 5.575274200439453 and perplexity is 263.8218857970278
At time: 84.7916624546051 and batch: 1050, loss is 5.56625111579895 and perplexity is 261.45210603132193
At time: 85.61503958702087 and batch: 1100, loss is 5.530033121109009 and perplexity is 252.15226244777904
At time: 86.43795204162598 and batch: 1150, loss is 5.559379091262818 and perplexity is 259.6615601278391
At time: 87.26123547554016 and batch: 1200, loss is 5.549316892623901 and perplexity is 257.0618950383738
At time: 88.08495378494263 and batch: 1250, loss is 5.5244377326965335 and perplexity is 250.7453124868277
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.376080366816834 and perplexity of 216.17329265018057
Finished 4 epochs...
Completing Train Step...
At time: 90.35064268112183 and batch: 50, loss is 5.574798307418823 and perplexity is 263.6963646725701
At time: 91.17582678794861 and batch: 100, loss is 5.574622497558594 and perplexity is 263.6500083266248
At time: 92.01336717605591 and batch: 150, loss is 5.498476800918579 and perplexity is 244.31950145290665
At time: 92.83946394920349 and batch: 200, loss is 5.539346084594727 and perplexity is 254.5115160302962
At time: 93.69693350791931 and batch: 250, loss is 5.549748201370239 and perplexity is 257.1727919957487
At time: 94.52838897705078 and batch: 300, loss is 5.534660539627075 and perplexity is 253.32178033385725
At time: 95.37388515472412 and batch: 350, loss is 5.561232929229736 and perplexity is 260.1433770538197
At time: 96.20495510101318 and batch: 400, loss is 5.518900480270386 and perplexity is 249.36070938315578
At time: 97.04017210006714 and batch: 450, loss is 5.495623378753662 and perplexity is 243.6233484536795
At time: 97.86379718780518 and batch: 500, loss is 5.504038619995117 and perplexity is 245.68214820154307
At time: 98.68808603286743 and batch: 550, loss is 5.493724966049195 and perplexity is 243.1612895219138
At time: 99.51095294952393 and batch: 600, loss is 5.499469680786133 and perplexity is 244.56220183337285
At time: 100.3854706287384 and batch: 650, loss is 5.500024509429932 and perplexity is 244.6979295974844
At time: 101.20877432823181 and batch: 700, loss is 5.5245340538024905 and perplexity is 250.76946571585412
At time: 102.03556323051453 and batch: 750, loss is 5.48625807762146 and perplexity is 241.3523931213996
At time: 102.86078953742981 and batch: 800, loss is 5.512521095275879 and perplexity is 247.7750047035394
At time: 103.68387365341187 and batch: 850, loss is 5.512951726913452 and perplexity is 247.88172743700804
At time: 104.50814938545227 and batch: 900, loss is 5.498725433349609 and perplexity is 244.38025475680885
At time: 105.3351240158081 and batch: 950, loss is 5.50178430557251 and perplexity is 245.12892719281078
At time: 106.15746521949768 and batch: 1000, loss is 5.4908059883117675 and perplexity is 242.45254204348683
At time: 106.97889804840088 and batch: 1050, loss is 5.4857948112487795 and perplexity is 241.24060856871188
At time: 107.80142641067505 and batch: 1100, loss is 5.450266418457031 and perplexity is 232.82018524026512
At time: 108.62567973136902 and batch: 1150, loss is 5.489751100540161 and perplexity is 242.1969166734335
At time: 109.45212864875793 and batch: 1200, loss is 5.481545515060425 and perplexity is 240.21768067126155
At time: 110.27479863166809 and batch: 1250, loss is 5.478389940261841 and perplexity is 239.46085055699947
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.343030943487682 and perplexity of 209.14565926217506
Finished 5 epochs...
Completing Train Step...
At time: 112.55221247673035 and batch: 50, loss is 5.486194753646851 and perplexity is 241.33711021247817
At time: 113.37643551826477 and batch: 100, loss is 5.493743257522583 and perplexity is 243.1657373408486
At time: 114.20087051391602 and batch: 150, loss is 5.427813844680786 and perplexity is 227.65102053447137
At time: 115.02718663215637 and batch: 200, loss is 5.467947635650635 and perplexity is 236.97333770003
At time: 115.86363434791565 and batch: 250, loss is 5.476654844284058 and perplexity is 239.0457232453729
At time: 116.70428991317749 and batch: 300, loss is 5.4587886524200435 and perplexity is 234.81281208617992
At time: 117.56028342247009 and batch: 350, loss is 5.488665580749512 and perplexity is 241.93414977228866
At time: 118.39786624908447 and batch: 400, loss is 5.455388555526733 and perplexity is 234.01578153170203
At time: 119.2228364944458 and batch: 450, loss is 5.429255409240723 and perplexity is 227.97943083315837
At time: 120.04542756080627 and batch: 500, loss is 5.4384251403808594 and perplexity is 230.07955499077906
At time: 120.87197852134705 and batch: 550, loss is 5.433437442779541 and perplexity is 228.9348448533331
At time: 121.74700093269348 and batch: 600, loss is 5.449986410140991 and perplexity is 232.75500277849756
At time: 122.57207942008972 and batch: 650, loss is 5.451344337463379 and perplexity is 233.07128184958822
At time: 123.39621949195862 and batch: 700, loss is 5.475503044128418 and perplexity is 238.77054884751254
At time: 124.24140310287476 and batch: 750, loss is 5.431313924789428 and perplexity is 228.449213397779
At time: 125.06492948532104 and batch: 800, loss is 5.453421354293823 and perplexity is 233.55587790755504
At time: 125.88879370689392 and batch: 850, loss is 5.45706997871399 and perplexity is 234.40959208126685
At time: 126.71377110481262 and batch: 900, loss is 5.451584405899048 and perplexity is 233.12724162444036
At time: 127.53837418556213 and batch: 950, loss is 5.464809312820434 and perplexity is 236.2308046278617
At time: 128.36112356185913 and batch: 1000, loss is 5.435704727172851 and perplexity is 229.45449412789097
At time: 129.18377947807312 and batch: 1050, loss is 5.434331731796265 and perplexity is 229.1396703435732
At time: 130.0056266784668 and batch: 1100, loss is 5.389394397735596 and perplexity is 219.07067567605526
At time: 130.84480237960815 and batch: 1150, loss is 5.43048921585083 and perplexity is 228.26088695739762
At time: 131.69842791557312 and batch: 1200, loss is 5.4346920776367185 and perplexity is 229.22225474924434
At time: 132.53037667274475 and batch: 1250, loss is 5.444323854446411 and perplexity is 231.44073916928372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.360293200416287 and perplexity of 212.7873266369019
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 134.790301322937 and batch: 50, loss is 5.431787471771241 and perplexity is 228.55742045182075
At time: 135.64161610603333 and batch: 100, loss is 5.414699459075928 and perplexity is 224.68500849020285
At time: 136.46526074409485 and batch: 150, loss is 5.333765449523926 and perplexity is 207.2167712682344
At time: 137.29125118255615 and batch: 200, loss is 5.374759283065796 and perplexity is 215.88789818235642
At time: 138.14774084091187 and batch: 250, loss is 5.374672250747681 and perplexity is 215.86910977573578
At time: 138.9957468509674 and batch: 300, loss is 5.352869739532471 and perplexity is 211.21355687740473
At time: 139.8235800266266 and batch: 350, loss is 5.37615421295166 and perplexity is 216.18925680173373
At time: 140.65947103500366 and batch: 400, loss is 5.341279945373535 and perplexity is 208.77976603988154
At time: 141.4836905002594 and batch: 450, loss is 5.311528444290161 and perplexity is 202.65974591923538
At time: 142.30569458007812 and batch: 500, loss is 5.312000713348389 and perplexity is 202.75547845055814
At time: 143.18605589866638 and batch: 550, loss is 5.299211254119873 and perplexity is 200.17885744167998
At time: 144.02662897109985 and batch: 600, loss is 5.320953235626221 and perplexity is 204.57880087776212
At time: 144.87455105781555 and batch: 650, loss is 5.321071062088013 and perplexity is 204.60290709417436
At time: 145.70083284378052 and batch: 700, loss is 5.334775609970093 and perplexity is 207.42619921445691
At time: 146.55204343795776 and batch: 750, loss is 5.296885004043579 and perplexity is 199.71373256742055
At time: 147.37460374832153 and batch: 800, loss is 5.304048662185669 and perplexity is 201.14955018683563
At time: 148.19942808151245 and batch: 850, loss is 5.311914052963257 and perplexity is 202.7379083440351
At time: 149.02270817756653 and batch: 900, loss is 5.310980787277222 and perplexity is 202.5487882742817
At time: 149.84671640396118 and batch: 950, loss is 5.323572864532471 and perplexity is 205.11542398797377
At time: 150.6714973449707 and batch: 1000, loss is 5.296701316833496 and perplexity is 199.67705107813325
At time: 151.5123951435089 and batch: 1050, loss is 5.283323545455932 and perplexity is 197.0236053075619
At time: 152.33600783348083 and batch: 1100, loss is 5.240384845733643 and perplexity is 188.74272526304483
At time: 153.18199610710144 and batch: 1150, loss is 5.26628436088562 and perplexity is 193.6949232810856
At time: 154.00447535514832 and batch: 1200, loss is 5.265230655670166 and perplexity is 193.49093342168084
At time: 154.8314049243927 and batch: 1250, loss is 5.305047721862793 and perplexity is 201.35061101065617
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.223209576015055 and perplexity of 185.52869795062603
Finished 7 epochs...
Completing Train Step...
At time: 157.10627961158752 and batch: 50, loss is 5.3408786487579345 and perplexity is 208.696000234955
At time: 157.93101477622986 and batch: 100, loss is 5.341690092086792 and perplexity is 208.86541393761863
At time: 158.75426292419434 and batch: 150, loss is 5.273095149993896 and perplexity is 195.01864121983533
At time: 159.5792272090912 and batch: 200, loss is 5.311812009811401 and perplexity is 202.71722138436633
At time: 160.40545773506165 and batch: 250, loss is 5.318262157440185 and perplexity is 204.02900343514509
At time: 161.24539995193481 and batch: 300, loss is 5.304241895675659 and perplexity is 201.1884227720499
At time: 162.07341051101685 and batch: 350, loss is 5.328330974578858 and perplexity is 206.09371130119683
At time: 162.89763379096985 and batch: 400, loss is 5.295668449401855 and perplexity is 199.47091762781784
At time: 163.75115871429443 and batch: 450, loss is 5.265112638473511 and perplexity is 193.46809951156374
At time: 164.57398343086243 and batch: 500, loss is 5.270467414855957 and perplexity is 194.5068565954581
At time: 165.3982195854187 and batch: 550, loss is 5.261159067153931 and perplexity is 192.70471961486217
At time: 166.25520038604736 and batch: 600, loss is 5.28429762840271 and perplexity is 197.2156161436629
At time: 167.10421442985535 and batch: 650, loss is 5.289292411804199 and perplexity is 198.20312958453525
At time: 167.93074774742126 and batch: 700, loss is 5.302031421661377 and perplexity is 200.74419215254144
At time: 168.75665092468262 and batch: 750, loss is 5.266822462081909 and perplexity is 193.799178798512
At time: 169.58143711090088 and batch: 800, loss is 5.2825752544403075 and perplexity is 196.87622946073213
At time: 170.40587949752808 and batch: 850, loss is 5.291424312591553 and perplexity is 198.62612972951217
At time: 171.25121808052063 and batch: 900, loss is 5.287290439605713 and perplexity is 197.8067293529697
At time: 172.0810420513153 and batch: 950, loss is 5.297906913757324 and perplexity is 199.9179262867021
At time: 172.90669918060303 and batch: 1000, loss is 5.276003293991089 and perplexity is 195.58660897636082
At time: 173.73634815216064 and batch: 1050, loss is 5.269425745010376 and perplexity is 194.3043501589194
At time: 174.55996227264404 and batch: 1100, loss is 5.227092151641846 and perplexity is 186.25042732917063
At time: 175.3784739971161 and batch: 1150, loss is 5.256533479690551 and perplexity is 191.81540546551872
At time: 176.20163297653198 and batch: 1200, loss is 5.261191129684448 and perplexity is 192.71089831486756
At time: 177.02486634254456 and batch: 1250, loss is 5.296804132461548 and perplexity is 199.69758205498204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.2176999280052465 and perplexity of 184.50931093973472
Finished 8 epochs...
Completing Train Step...
At time: 179.3148467540741 and batch: 50, loss is 5.311378479003906 and perplexity is 202.62935627117935
At time: 180.17329168319702 and batch: 100, loss is 5.311634521484375 and perplexity is 202.68124463670432
At time: 180.99888491630554 and batch: 150, loss is 5.246869192123413 and perplexity is 189.970575071682
At time: 181.84378790855408 and batch: 200, loss is 5.287083492279053 and perplexity is 197.76579801459633
At time: 182.6705403327942 and batch: 250, loss is 5.2919982814788815 and perplexity is 198.74016767217225
At time: 183.51685166358948 and batch: 300, loss is 5.279568300247193 and perplexity is 196.28512082072152
At time: 184.34259128570557 and batch: 350, loss is 5.303945026397705 and perplexity is 201.12870497487708
At time: 185.21597456932068 and batch: 400, loss is 5.27265115737915 and perplexity is 194.93207360250946
At time: 186.03845524787903 and batch: 450, loss is 5.2405733871459965 and perplexity is 188.77831443794864
At time: 186.86165690422058 and batch: 500, loss is 5.246626195907592 and perplexity is 189.92441854897953
At time: 187.6858777999878 and batch: 550, loss is 5.238070783615112 and perplexity is 188.3064678304403
At time: 188.50972151756287 and batch: 600, loss is 5.262309923171997 and perplexity is 192.92662266587953
At time: 189.3320279121399 and batch: 650, loss is 5.267840957641601 and perplexity is 193.99666295288603
At time: 190.15627574920654 and batch: 700, loss is 5.2814601707458495 and perplexity is 196.65681834104993
At time: 190.97830510139465 and batch: 750, loss is 5.246420068740845 and perplexity is 189.88527400120378
At time: 191.80502676963806 and batch: 800, loss is 5.2648614978790285 and perplexity is 193.41951791869946
At time: 192.62934398651123 and batch: 850, loss is 5.275634031295777 and perplexity is 195.51439947092214
At time: 193.45419549942017 and batch: 900, loss is 5.268978872299194 and perplexity is 194.21754024510489
At time: 194.27724051475525 and batch: 950, loss is 5.278347930908203 and perplexity is 196.04572658197733
At time: 195.10185360908508 and batch: 1000, loss is 5.260620756149292 and perplexity is 192.60101245950352
At time: 195.92638325691223 and batch: 1050, loss is 5.254876880645752 and perplexity is 191.49790730423186
At time: 196.74922275543213 and batch: 1100, loss is 5.211845397949219 and perplexity is 183.43225154790787
At time: 197.57075953483582 and batch: 1150, loss is 5.243340425491333 and perplexity is 189.30139463049463
At time: 198.39157438278198 and batch: 1200, loss is 5.251262569427491 and perplexity is 190.8070235561277
At time: 199.21196031570435 and batch: 1250, loss is 5.284666633605957 and perplexity is 197.28840316074957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.2138671875 and perplexity of 183.80348811201023
Finished 9 epochs...
Completing Train Step...
At time: 201.47202515602112 and batch: 50, loss is 5.288332929611206 and perplexity is 198.01304841543435
At time: 202.2914581298828 and batch: 100, loss is 5.288102502822876 and perplexity is 197.96742616113715
At time: 203.11144304275513 and batch: 150, loss is 5.226384801864624 and perplexity is 186.1187297145223
At time: 203.946594953537 and batch: 200, loss is 5.268142814636231 and perplexity is 194.05523104168205
At time: 204.80598616600037 and batch: 250, loss is 5.270435810089111 and perplexity is 194.50070934874722
At time: 205.6252646446228 and batch: 300, loss is 5.259140319824219 and perplexity is 192.31608988131848
At time: 206.4962854385376 and batch: 350, loss is 5.282246360778808 and perplexity is 196.81148876374866
At time: 207.31432557106018 and batch: 400, loss is 5.252594661712647 and perplexity is 191.06136548601154
At time: 208.13480019569397 and batch: 450, loss is 5.219655628204346 and perplexity is 184.87050891822975
At time: 208.95261788368225 and batch: 500, loss is 5.226802711486816 and perplexity is 186.19652677747854
At time: 209.77406287193298 and batch: 550, loss is 5.21944296836853 and perplexity is 184.83119856617068
At time: 210.59406900405884 and batch: 600, loss is 5.245215129852295 and perplexity is 189.65661163994838
At time: 211.41447401046753 and batch: 650, loss is 5.250888576507569 and perplexity is 190.7356764227393
At time: 212.23558282852173 and batch: 700, loss is 5.267329616546631 and perplexity is 193.8974898446349
At time: 213.05413961410522 and batch: 750, loss is 5.23279764175415 and perplexity is 187.31611454373498
At time: 213.87298822402954 and batch: 800, loss is 5.250333366394043 and perplexity is 190.62980743866407
At time: 214.69154262542725 and batch: 850, loss is 5.264340143203736 and perplexity is 193.31870403091906
At time: 215.50767636299133 and batch: 900, loss is 5.255741844177246 and perplexity is 191.66361766678605
At time: 216.32560753822327 and batch: 950, loss is 5.26247612953186 and perplexity is 192.9586909624572
At time: 217.15951824188232 and batch: 1000, loss is 5.24762041091919 and perplexity is 190.11333815476218
At time: 217.9867649078369 and batch: 1050, loss is 5.243956060409546 and perplexity is 189.41797105967385
At time: 218.8236804008484 and batch: 1100, loss is 5.198715858459472 and perplexity is 181.0396120559895
At time: 219.6668040752411 and batch: 1150, loss is 5.231067142486572 and perplexity is 186.9922444540106
At time: 220.49625205993652 and batch: 1200, loss is 5.240881481170654 and perplexity is 188.83648486913293
At time: 221.34747338294983 and batch: 1250, loss is 5.272780437469482 and perplexity is 194.95727606764672
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.211946139370438 and perplexity of 183.45073170446923
Finished 10 epochs...
Completing Train Step...
At time: 223.7568497657776 and batch: 50, loss is 5.269745988845825 and perplexity is 194.36658489387196
At time: 224.64105367660522 and batch: 100, loss is 5.26949291229248 and perplexity is 194.31740149232698
At time: 225.46868300437927 and batch: 150, loss is 5.209066286087036 and perplexity is 182.92318051205058
At time: 226.30346822738647 and batch: 200, loss is 5.2520904541015625 and perplexity is 190.965055173586
At time: 227.1422290802002 and batch: 250, loss is 5.25307336807251 and perplexity is 191.15284967207765
At time: 228.01772284507751 and batch: 300, loss is 5.242534246444702 and perplexity is 189.14884531193977
At time: 228.84284377098083 and batch: 350, loss is 5.266040897369384 and perplexity is 193.64777137410408
At time: 229.66704964637756 and batch: 400, loss is 5.238289890289306 and perplexity is 188.34773155474954
At time: 230.50700044631958 and batch: 450, loss is 5.204238224029541 and perplexity is 182.042144604228
At time: 231.35394668579102 and batch: 500, loss is 5.210895767211914 and perplexity is 183.25814132691812
At time: 232.19347953796387 and batch: 550, loss is 5.202906475067139 and perplexity is 181.79987152630505
At time: 233.0199728012085 and batch: 600, loss is 5.229939193725586 and perplexity is 186.78144569101576
At time: 233.84193992614746 and batch: 650, loss is 5.236006669998169 and perplexity is 187.91818275614705
At time: 234.66847276687622 and batch: 700, loss is 5.253313646316529 and perplexity is 191.1987850615524
At time: 235.49271869659424 and batch: 750, loss is 5.219964056015015 and perplexity is 184.92753691861125
At time: 236.3169777393341 and batch: 800, loss is 5.236732158660889 and perplexity is 188.05456473306276
At time: 237.1388123035431 and batch: 850, loss is 5.251849851608276 and perplexity is 190.91911403217892
At time: 237.96416926383972 and batch: 900, loss is 5.240173606872559 and perplexity is 188.70285967545126
At time: 238.79013466835022 and batch: 950, loss is 5.246523113250732 and perplexity is 189.9048416443499
At time: 239.61684656143188 and batch: 1000, loss is 5.23534595489502 and perplexity is 187.79406338293126
At time: 240.44111108779907 and batch: 1050, loss is 5.23119857788086 and perplexity is 187.01682346862995
At time: 241.26716470718384 and batch: 1100, loss is 5.186151599884033 and perplexity is 178.77921340983548
At time: 242.10985803604126 and batch: 1150, loss is 5.218983554840088 and perplexity is 184.746304115394
At time: 242.93399715423584 and batch: 1200, loss is 5.228969202041626 and perplexity is 186.6003570833979
At time: 243.78169465065002 and batch: 1250, loss is 5.260591716766357 and perplexity is 192.59541952615714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.2103307125342155 and perplexity of 183.1546197073771
Finished 11 epochs...
Completing Train Step...
At time: 246.09695625305176 and batch: 50, loss is 5.2539105796813965 and perplexity is 191.3129520673352
At time: 246.9634885787964 and batch: 100, loss is 5.252733411788941 and perplexity is 191.08787710425156
At time: 247.7878909111023 and batch: 150, loss is 5.193314180374146 and perplexity is 180.06433080006968
At time: 248.64178848266602 and batch: 200, loss is 5.238928928375244 and perplexity is 188.46813139455725
At time: 249.4667673110962 and batch: 250, loss is 5.237424087524414 and perplexity is 188.18473014172767
At time: 250.290132522583 and batch: 300, loss is 5.2261451053619385 and perplexity is 186.0741230521695
At time: 251.11479949951172 and batch: 350, loss is 5.249594602584839 and perplexity is 190.48902904336748
At time: 251.940851688385 and batch: 400, loss is 5.223536586761474 and perplexity is 185.58937774955749
At time: 252.763658285141 and batch: 450, loss is 5.189494800567627 and perplexity is 179.37790841946847
At time: 253.59010767936707 and batch: 500, loss is 5.195483846664429 and perplexity is 180.45543443745282
At time: 254.41411018371582 and batch: 550, loss is 5.187969045639038 and perplexity is 179.10443037490776
At time: 255.23789024353027 and batch: 600, loss is 5.2158510112762455 and perplexity is 184.1684837657066
At time: 256.06195855140686 and batch: 650, loss is 5.222192583084106 and perplexity is 185.34011248761755
At time: 256.8855576515198 and batch: 700, loss is 5.239338407516479 and perplexity is 188.54532096583208
At time: 257.7098915576935 and batch: 750, loss is 5.207772855758667 and perplexity is 182.68673506843217
At time: 258.5341794490814 and batch: 800, loss is 5.224033679962158 and perplexity is 185.68165590087656
At time: 259.3684525489807 and batch: 850, loss is 5.2411066341400145 and perplexity is 188.87900675120912
At time: 260.1911826133728 and batch: 900, loss is 5.2266827487945555 and perplexity is 186.17419148056464
At time: 261.0258173942566 and batch: 950, loss is 5.232729148864746 and perplexity is 187.30328516118377
At time: 261.86072278022766 and batch: 1000, loss is 5.224609413146973 and perplexity is 185.78858977172527
At time: 262.6859600543976 and batch: 1050, loss is 5.220630016326904 and perplexity is 185.0507323358506
At time: 263.50972080230713 and batch: 1100, loss is 5.173956184387207 and perplexity is 176.61216749112583
At time: 264.33545303344727 and batch: 1150, loss is 5.208065624237061 and perplexity is 182.74022781599396
At time: 265.1591091156006 and batch: 1200, loss is 5.218472871780396 and perplexity is 184.65198139409932
At time: 265.98358941078186 and batch: 1250, loss is 5.248703050613403 and perplexity is 190.3192738580164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.209880299811815 and perplexity of 183.07214311214278
Finished 12 epochs...
Completing Train Step...
At time: 268.2725794315338 and batch: 50, loss is 5.238609390258789 and perplexity is 188.40791826354726
At time: 269.1198933124542 and batch: 100, loss is 5.237317972183227 and perplexity is 188.16476191436888
At time: 269.99123096466064 and batch: 150, loss is 5.179922866821289 and perplexity is 177.66910628190098
At time: 270.81609320640564 and batch: 200, loss is 5.226648921966553 and perplexity is 186.16789390472502
At time: 271.6423463821411 and batch: 250, loss is 5.223923568725586 and perplexity is 185.6612113897424
At time: 272.46607542037964 and batch: 300, loss is 5.2128511428833 and perplexity is 183.61683040971815
At time: 273.28894329071045 and batch: 350, loss is 5.235266819000244 and perplexity is 187.77920271970558
At time: 274.11291885375977 and batch: 400, loss is 5.211383047103882 and perplexity is 183.3474610943221
At time: 274.93488025665283 and batch: 450, loss is 5.176295795440674 and perplexity is 177.0258550156144
At time: 275.75719594955444 and batch: 500, loss is 5.181921014785766 and perplexity is 178.02447036173967
At time: 276.57925724983215 and batch: 550, loss is 5.175134115219116 and perplexity is 176.82032698317687
At time: 277.4016559123993 and batch: 600, loss is 5.203872585296631 and perplexity is 181.97559511241613
At time: 278.224853515625 and batch: 650, loss is 5.209616222381592 and perplexity is 183.0238042739232
At time: 279.04941725730896 and batch: 700, loss is 5.228649597167969 and perplexity is 186.54072822919167
At time: 279.8713023662567 and batch: 750, loss is 5.195359678268432 and perplexity is 180.43302896666438
At time: 280.69651103019714 and batch: 800, loss is 5.212803812026977 and perplexity is 183.60813987356642
At time: 281.51808524131775 and batch: 850, loss is 5.2297029876708985 and perplexity is 186.73733199280673
At time: 282.3422317504883 and batch: 900, loss is 5.214452447891236 and perplexity is 183.91109249859437
At time: 283.164231300354 and batch: 950, loss is 5.219259777069092 and perplexity is 184.79734219991917
At time: 283.9874906539917 and batch: 1000, loss is 5.213531847000122 and perplexity is 183.74186169192222
At time: 284.8067903518677 and batch: 1050, loss is 5.2102984428405765 and perplexity is 183.14870945927203
At time: 285.62948346138 and batch: 1100, loss is 5.163658447265625 and perplexity is 174.80279403325437
At time: 286.45508074760437 and batch: 1150, loss is 5.1975637054443355 and perplexity is 180.83114683607653
At time: 287.2790710926056 and batch: 1200, loss is 5.208519783020019 and perplexity is 182.8232397443278
At time: 288.1000850200653 and batch: 1250, loss is 5.238708629608154 and perplexity is 188.42661667056444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.209545274720575 and perplexity of 183.0108196237202
Finished 13 epochs...
Completing Train Step...
At time: 290.34697937965393 and batch: 50, loss is 5.22569577217102 and perplexity is 185.99053255411022
At time: 291.2001347541809 and batch: 100, loss is 5.222786731719971 and perplexity is 185.45026478280067
At time: 292.0272288322449 and batch: 150, loss is 5.167582120895386 and perplexity is 175.49001047146584
At time: 292.8800382614136 and batch: 200, loss is 5.215172538757324 and perplexity is 184.0435728897101
At time: 293.71370363235474 and batch: 250, loss is 5.212264165878296 and perplexity is 183.50908317820313
At time: 294.5519344806671 and batch: 300, loss is 5.201087989807129 and perplexity is 181.46957155350083
At time: 295.3803527355194 and batch: 350, loss is 5.222246408462524 and perplexity is 185.35008875779417
At time: 296.2067184448242 and batch: 400, loss is 5.199096584320069 and perplexity is 181.10855164080016
At time: 297.0290265083313 and batch: 450, loss is 5.164427070617676 and perplexity is 174.93720319114357
At time: 297.85254311561584 and batch: 500, loss is 5.169787788391114 and perplexity is 175.87751027414956
At time: 298.67410135269165 and batch: 550, loss is 5.163071155548096 and perplexity is 174.70016393996934
At time: 299.5167193412781 and batch: 600, loss is 5.191424942016601 and perplexity is 179.7244675019734
At time: 300.34742975234985 and batch: 650, loss is 5.198013162612915 and perplexity is 180.91244095906904
At time: 301.17271900177 and batch: 700, loss is 5.216974201202393 and perplexity is 184.37545616428372
At time: 301.9971127510071 and batch: 750, loss is 5.185430374145508 and perplexity is 178.6503197259194
At time: 302.8228530883789 and batch: 800, loss is 5.202882118225098 and perplexity is 181.79544350947768
At time: 303.6449613571167 and batch: 850, loss is 5.220568742752075 and perplexity is 185.0393939633305
At time: 304.4692232608795 and batch: 900, loss is 5.2034008598327635 and perplexity is 181.88977283426757
At time: 305.29433608055115 and batch: 950, loss is 5.208273401260376 and perplexity is 182.7782009814086
At time: 306.118515253067 and batch: 1000, loss is 5.204201793670654 and perplexity is 182.0355128643666
At time: 306.9437928199768 and batch: 1050, loss is 5.201976318359375 and perplexity is 181.63084777780736
At time: 307.7668571472168 and batch: 1100, loss is 5.1543027210235595 and perplexity is 173.1750133543717
At time: 308.59255743026733 and batch: 1150, loss is 5.188908910751342 and perplexity is 179.2728435108945
At time: 309.4156186580658 and batch: 1200, loss is 5.1999979496002195 and perplexity is 181.27187019496728
At time: 310.2390558719635 and batch: 1250, loss is 5.23002145767212 and perplexity is 186.7968117019033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.209866043424954 and perplexity of 183.06953318345117
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 312.4854612350464 and batch: 50, loss is 5.2176336002349855 and perplexity is 184.49707325440136
At time: 313.3075530529022 and batch: 100, loss is 5.216929235458374 and perplexity is 184.3671657711117
At time: 314.12883162498474 and batch: 150, loss is 5.157558479309082 and perplexity is 173.73974816010994
At time: 314.94849705696106 and batch: 200, loss is 5.200176239013672 and perplexity is 181.30419193160586
At time: 315.77034091949463 and batch: 250, loss is 5.197725887298584 and perplexity is 180.86047674510152
At time: 316.5930006504059 and batch: 300, loss is 5.184497308731079 and perplexity is 178.4837050345976
At time: 317.4163587093353 and batch: 350, loss is 5.19785252571106 and perplexity is 180.88338207907307
At time: 318.23985981941223 and batch: 400, loss is 5.169635581970215 and perplexity is 175.85074262494982
At time: 319.0640377998352 and batch: 450, loss is 5.135200834274292 and perplexity is 169.89843787801067
At time: 319.8887960910797 and batch: 500, loss is 5.134625511169434 and perplexity is 169.80071949373766
At time: 320.714590549469 and batch: 550, loss is 5.127807464599609 and perplexity is 168.64694798407348
At time: 321.5473430156708 and batch: 600, loss is 5.154564304351807 and perplexity is 173.22031897607351
At time: 322.38197135925293 and batch: 650, loss is 5.163327274322509 and perplexity is 174.7449136622249
At time: 323.23498368263245 and batch: 700, loss is 5.176640157699585 and perplexity is 177.08682653647602
At time: 324.0607113838196 and batch: 750, loss is 5.142434225082398 and perplexity is 171.13183512120293
At time: 324.8858366012573 and batch: 800, loss is 5.149872722625733 and perplexity is 172.40954508612108
At time: 325.7110528945923 and batch: 850, loss is 5.168554563522338 and perplexity is 175.66074744075146
At time: 326.5352671146393 and batch: 900, loss is 5.148762950897217 and perplexity is 172.2183159772139
At time: 327.3589689731598 and batch: 950, loss is 5.147289733886719 and perplexity is 171.96478782144703
At time: 328.1821460723877 and batch: 1000, loss is 5.1430748462677 and perplexity is 171.24150092362882
At time: 329.00527238845825 and batch: 1050, loss is 5.131757678985596 and perplexity is 169.31445711825032
At time: 329.8295714855194 and batch: 1100, loss is 5.079555931091309 and perplexity is 160.70267701879553
At time: 330.6555678844452 and batch: 1150, loss is 5.116822519302368 and perplexity is 166.80450856113552
At time: 331.47815775871277 and batch: 1200, loss is 5.133254060745239 and perplexity is 169.5680058390551
At time: 332.3023271560669 and batch: 1250, loss is 5.173325710296631 and perplexity is 176.50085318952088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.159583766965101 and perplexity of 174.09197769008037
Finished 15 epochs...
Completing Train Step...
At time: 334.54530215263367 and batch: 50, loss is 5.187047290802002 and perplexity is 178.93941606292861
At time: 335.3998017311096 and batch: 100, loss is 5.189086198806763 and perplexity is 179.30462926224396
At time: 336.2223861217499 and batch: 150, loss is 5.131799612045288 and perplexity is 169.32155714034917
At time: 337.04771995544434 and batch: 200, loss is 5.171997709274292 and perplexity is 176.26661544444892
At time: 337.87171626091003 and batch: 250, loss is 5.173506259918213 and perplexity is 176.5327232287484
At time: 338.69541692733765 and batch: 300, loss is 5.1628883934021 and perplexity is 174.66823828059174
At time: 339.51991534233093 and batch: 350, loss is 5.1776206588745115 and perplexity is 177.26054552988012
At time: 340.34876823425293 and batch: 400, loss is 5.152256126403809 and perplexity is 172.82095673253602
At time: 341.18872022628784 and batch: 450, loss is 5.117252473831177 and perplexity is 166.87624233504246
At time: 342.0379755496979 and batch: 500, loss is 5.118864736557007 and perplexity is 167.14550768530248
At time: 342.88476943969727 and batch: 550, loss is 5.112776908874512 and perplexity is 166.13104570460965
At time: 343.7177233695984 and batch: 600, loss is 5.1413686656951905 and perplexity is 170.94958110632226
At time: 344.5392940044403 and batch: 650, loss is 5.152253484725952 and perplexity is 172.82050019584443
At time: 345.3613829612732 and batch: 700, loss is 5.165457677841187 and perplexity is 175.11758767322195
At time: 346.1848931312561 and batch: 750, loss is 5.13273473739624 and perplexity is 169.4799680763912
At time: 347.01138138771057 and batch: 800, loss is 5.1442014026641845 and perplexity is 171.4345228364591
At time: 347.8380320072174 and batch: 850, loss is 5.164365663528442 and perplexity is 174.9264611365194
At time: 348.66298604011536 and batch: 900, loss is 5.144584903717041 and perplexity is 171.50028076477423
At time: 349.48789286613464 and batch: 950, loss is 5.143676862716675 and perplexity is 171.3446221611582
At time: 350.3112270832062 and batch: 1000, loss is 5.1421808338165285 and perplexity is 171.0884773023493
At time: 351.13559317588806 and batch: 1050, loss is 5.133699436187744 and perplexity is 169.64354408488148
At time: 351.9845004081726 and batch: 1100, loss is 5.084155282974243 and perplexity is 161.4435075431053
At time: 352.8159992694855 and batch: 1150, loss is 5.12306583404541 and perplexity is 167.8491793194669
At time: 353.6428713798523 and batch: 1200, loss is 5.139488344192505 and perplexity is 170.62844294920222
At time: 354.5369436740875 and batch: 1250, loss is 5.174378623962403 and perplexity is 176.68679122103373
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.158457066890967 and perplexity of 173.89593870521873
Finished 16 epochs...
Completing Train Step...
At time: 356.8040974140167 and batch: 50, loss is 5.177327327728271 and perplexity is 177.2085571161599
At time: 357.65870904922485 and batch: 100, loss is 5.179169578552246 and perplexity is 177.53532062427703
At time: 358.4798278808594 and batch: 150, loss is 5.122711772918701 and perplexity is 167.78976096940463
At time: 359.3026089668274 and batch: 200, loss is 5.1624797821044925 and perplexity is 174.5968814446946
At time: 360.12561106681824 and batch: 250, loss is 5.164567813873291 and perplexity is 174.96182615536637
At time: 360.9504086971283 and batch: 300, loss is 5.154424257278443 and perplexity is 173.19606167597533
At time: 361.7767813205719 and batch: 350, loss is 5.169119911193848 and perplexity is 175.76008491275107
At time: 362.6002154350281 and batch: 400, loss is 5.144356966018677 and perplexity is 171.46119384036896
At time: 363.42886090278625 and batch: 450, loss is 5.1096931743621825 and perplexity is 165.61953075889755
At time: 364.2536942958832 and batch: 500, loss is 5.112564725875854 and perplexity is 166.09579926064225
At time: 365.0774097442627 and batch: 550, loss is 5.1070847320556645 and perplexity is 165.18808471376778
At time: 365.9048864841461 and batch: 600, loss is 5.136517133712768 and perplexity is 170.12222234776192
At time: 366.7322540283203 and batch: 650, loss is 5.14801682472229 and perplexity is 172.0898673092771
At time: 367.580011844635 and batch: 700, loss is 5.161415796279908 and perplexity is 174.41121163036183
At time: 368.4090111255646 and batch: 750, loss is 5.1286483478546145 and perplexity is 168.78882001919075
At time: 369.2394633293152 and batch: 800, loss is 5.141368703842163 and perplexity is 170.94958762753137
At time: 370.06469774246216 and batch: 850, loss is 5.162095079421997 and perplexity is 174.5297264742243
At time: 370.8979444503784 and batch: 900, loss is 5.142163734436036 and perplexity is 171.08555182039
At time: 371.7318637371063 and batch: 950, loss is 5.141359539031982 and perplexity is 170.94802091418958
At time: 372.5569851398468 and batch: 1000, loss is 5.1406730270385745 and perplexity is 170.8307033221573
At time: 373.3805775642395 and batch: 1050, loss is 5.133306360244751 and perplexity is 169.5768743928021
At time: 374.20502638816833 and batch: 1100, loss is 5.085221633911133 and perplexity is 161.61575480031885
At time: 375.07458567619324 and batch: 1150, loss is 5.124500026702881 and perplexity is 168.0900800877734
At time: 375.899227142334 and batch: 1200, loss is 5.140592164993286 and perplexity is 170.81689016057643
At time: 376.72416520118713 and batch: 1250, loss is 5.172915201187134 and perplexity is 176.4284128511794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.157973240761861 and perplexity of 173.81182365649786
Finished 17 epochs...
Completing Train Step...
At time: 378.98172903060913 and batch: 50, loss is 5.170286502838135 and perplexity is 175.96524480484993
At time: 379.8214898109436 and batch: 100, loss is 5.17247462272644 and perplexity is 176.35069941331327
At time: 380.6909372806549 and batch: 150, loss is 5.116499576568604 and perplexity is 166.75064895438896
At time: 381.54248785972595 and batch: 200, loss is 5.156330699920654 and perplexity is 173.5265649761081
At time: 382.37599635124207 and batch: 250, loss is 5.158672332763672 and perplexity is 173.93337659562948
At time: 383.2257146835327 and batch: 300, loss is 5.148726024627686 and perplexity is 172.2119567146726
At time: 384.0529818534851 and batch: 350, loss is 5.1629581546783445 and perplexity is 174.6804237848468
At time: 384.89065885543823 and batch: 400, loss is 5.138677339553833 and perplexity is 170.49011858891174
At time: 385.75856351852417 and batch: 450, loss is 5.104264078140258 and perplexity is 164.7228028039471
At time: 386.6012318134308 and batch: 500, loss is 5.107760200500488 and perplexity is 165.2997017451002
At time: 387.4258236885071 and batch: 550, loss is 5.102440776824952 and perplexity is 164.4227371391232
At time: 388.24887132644653 and batch: 600, loss is 5.132379369735718 and perplexity is 169.41975107685133
At time: 389.07206106185913 and batch: 650, loss is 5.144407033920288 and perplexity is 171.46977875746492
At time: 389.8974680900574 and batch: 700, loss is 5.15799952507019 and perplexity is 173.81639224010286
At time: 390.7221693992615 and batch: 750, loss is 5.12548906326294 and perplexity is 168.25640956177025
At time: 391.5457851886749 and batch: 800, loss is 5.138485708236694 and perplexity is 170.45745047314435
At time: 392.3710596561432 and batch: 850, loss is 5.159720010757447 and perplexity is 174.11569825719116
At time: 393.20044708251953 and batch: 900, loss is 5.139467964172363 and perplexity is 170.6249655735328
At time: 394.02594900131226 and batch: 950, loss is 5.138695335388183 and perplexity is 170.49318672845092
At time: 394.8513789176941 and batch: 1000, loss is 5.138306827545166 and perplexity is 170.42696165353937
At time: 395.6747217178345 and batch: 1050, loss is 5.131846485137939 and perplexity is 169.3294939513948
At time: 396.5390338897705 and batch: 1100, loss is 5.084544353485107 and perplexity is 161.50633267197077
At time: 397.3662736415863 and batch: 1150, loss is 5.12390254020691 and perplexity is 167.9896785320879
At time: 398.19428420066833 and batch: 1200, loss is 5.139899196624756 and perplexity is 170.6985604629884
At time: 399.0202841758728 and batch: 1250, loss is 5.170492153167725 and perplexity is 176.00143583666176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.157910869069343 and perplexity of 173.8009830569535
Finished 18 epochs...
Completing Train Step...
At time: 401.2599723339081 and batch: 50, loss is 5.164408092498779 and perplexity is 174.9338832436052
At time: 402.11335349082947 and batch: 100, loss is 5.166843709945678 and perplexity is 175.3604745574239
At time: 402.93870520591736 and batch: 150, loss is 5.111515769958496 and perplexity is 165.92166343552844
At time: 403.7632517814636 and batch: 200, loss is 5.151557588577271 and perplexity is 172.70027691165697
At time: 404.58915758132935 and batch: 250, loss is 5.153925113677978 and perplexity is 173.10963354198356
At time: 405.4148485660553 and batch: 300, loss is 5.144296417236328 and perplexity is 171.45081238815717
At time: 406.2372033596039 and batch: 350, loss is 5.158517761230469 and perplexity is 173.90649352466616
At time: 407.0596590042114 and batch: 400, loss is 5.134065160751343 and perplexity is 169.70559824268253
At time: 407.8838884830475 and batch: 450, loss is 5.099666042327881 and perplexity is 163.96714007104046
At time: 408.7077238559723 and batch: 500, loss is 5.103427486419678 and perplexity is 164.58505469842677
At time: 409.5318887233734 and batch: 550, loss is 5.098563175201416 and perplexity is 163.78640578370695
At time: 410.3613896369934 and batch: 600, loss is 5.129103937149048 and perplexity is 168.86573591831123
At time: 411.1927766799927 and batch: 650, loss is 5.141377992630005 and perplexity is 170.95117554935743
At time: 412.0237579345703 and batch: 700, loss is 5.155109243392944 and perplexity is 173.31473921490254
At time: 412.84988045692444 and batch: 750, loss is 5.122542085647583 and perplexity is 167.76129159825746
At time: 413.6709415912628 and batch: 800, loss is 5.13598970413208 and perplexity is 170.0325185136706
At time: 414.49310278892517 and batch: 850, loss is 5.157618055343628 and perplexity is 173.750099193682
At time: 415.3172302246094 and batch: 900, loss is 5.136931943893432 and perplexity is 170.1928054158245
At time: 416.13978910446167 and batch: 950, loss is 5.135977115631103 and perplexity is 170.0303780726177
At time: 416.967449426651 and batch: 1000, loss is 5.136137008666992 and perplexity is 170.05756691955676
At time: 417.82901668548584 and batch: 1050, loss is 5.130185804367065 and perplexity is 169.04852508122065
At time: 418.65389800071716 and batch: 1100, loss is 5.083696231842041 and perplexity is 161.3694137259075
At time: 419.4990129470825 and batch: 1150, loss is 5.1225728416442875 and perplexity is 167.7664513433351
At time: 420.3392972946167 and batch: 1200, loss is 5.138526458740234 and perplexity is 170.46439684161624
At time: 421.18939661979675 and batch: 1250, loss is 5.16789587020874 and perplexity is 175.54507898015353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.157976359346487 and perplexity of 173.81236570422416
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 423.4278733730316 and batch: 50, loss is 5.162895259857177 and perplexity is 174.669437636321
At time: 424.24808955192566 and batch: 100, loss is 5.16907506942749 and perplexity is 175.75220369679352
At time: 425.0700693130493 and batch: 150, loss is 5.111153144836425 and perplexity is 165.8615069798512
At time: 425.89134669303894 and batch: 200, loss is 5.152296895980835 and perplexity is 172.82800271347315
At time: 426.7138228416443 and batch: 250, loss is 5.1525745677948 and perplexity is 172.87599884176817
At time: 427.53561305999756 and batch: 300, loss is 5.142683191299438 and perplexity is 171.17444647098145
At time: 428.3590474128723 and batch: 350, loss is 5.153527393341064 and perplexity is 173.04079800976106
At time: 429.1817226409912 and batch: 400, loss is 5.125272636413574 and perplexity is 168.21999829749296
At time: 430.00322103500366 and batch: 450, loss is 5.092054615020752 and perplexity is 162.72385369791292
At time: 430.8267066478729 and batch: 500, loss is 5.092526016235351 and perplexity is 162.80058000320633
At time: 431.6559405326843 and batch: 550, loss is 5.086874008178711 and perplexity is 161.88302526939722
At time: 432.4852750301361 and batch: 600, loss is 5.11796127319336 and perplexity is 166.9945660381019
At time: 433.3076972961426 and batch: 650, loss is 5.131245927810669 and perplexity is 169.22783241292578
At time: 434.1300404071808 and batch: 700, loss is 5.14424464225769 and perplexity is 171.44193575580405
At time: 434.97233033180237 and batch: 750, loss is 5.106524667739868 and perplexity is 165.0955946647305
At time: 435.7936918735504 and batch: 800, loss is 5.1165790939331055 and perplexity is 166.76390905371986
At time: 436.6227276325226 and batch: 850, loss is 5.133029460906982 and perplexity is 169.5299251689864
At time: 437.4482910633087 and batch: 900, loss is 5.111949424743653 and perplexity is 165.99363176241528
At time: 438.3032133579254 and batch: 950, loss is 5.110576848983765 and perplexity is 165.76594921867817
At time: 439.1615891456604 and batch: 1000, loss is 5.111020832061768 and perplexity is 165.83956283543117
At time: 439.9995174407959 and batch: 1050, loss is 5.102672319412232 and perplexity is 164.4608124129417
At time: 440.8322448730469 and batch: 1100, loss is 5.052084112167359 and perplexity is 156.34797188215367
At time: 441.6577227115631 and batch: 1150, loss is 5.087895936965943 and perplexity is 162.04854275221976
At time: 442.48229789733887 and batch: 1200, loss is 5.11117148399353 and perplexity is 165.86454876797723
At time: 443.30387592315674 and batch: 1250, loss is 5.146762161254883 and perplexity is 171.8740878332724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.148172865818887 and perplexity of 172.11672249608716
Finished 20 epochs...
Completing Train Step...
At time: 445.5587363243103 and batch: 50, loss is 5.153229131698608 and perplexity is 172.9891942732249
At time: 446.41610884666443 and batch: 100, loss is 5.158962421417236 and perplexity is 173.9838400137346
At time: 447.23904943466187 and batch: 150, loss is 5.101932697296142 and perplexity is 164.33921853115615
At time: 448.0617027282715 and batch: 200, loss is 5.141520404815674 and perplexity is 170.9755228135425
At time: 448.88368797302246 and batch: 250, loss is 5.143141298294068 and perplexity is 171.2528806464621
At time: 449.70644664764404 and batch: 300, loss is 5.133825321197509 and perplexity is 169.66490100831535
At time: 450.5298171043396 and batch: 350, loss is 5.1456013107299805 and perplexity is 171.6746834699166
At time: 451.35455679893494 and batch: 400, loss is 5.118486957550049 and perplexity is 167.0823755471407
At time: 452.17802715301514 and batch: 450, loss is 5.085235872268677 and perplexity is 161.6180559596027
At time: 453.0028851032257 and batch: 500, loss is 5.086525754928589 and perplexity is 161.82665879518117
At time: 453.8251347541809 and batch: 550, loss is 5.08119309425354 and perplexity is 160.96598900521175
At time: 454.64938139915466 and batch: 600, loss is 5.1132231616973876 and perplexity is 166.20519869698313
At time: 455.47299909591675 and batch: 650, loss is 5.1266755199432374 and perplexity is 168.45615697529263
At time: 456.2970230579376 and batch: 700, loss is 5.140305461883545 and perplexity is 170.76792344676107
At time: 457.12111234664917 and batch: 750, loss is 5.103930168151855 and perplexity is 164.66780939670804
At time: 457.95993185043335 and batch: 800, loss is 5.114718084335327 and perplexity is 166.45384842080227
At time: 458.78971767425537 and batch: 850, loss is 5.132557210922241 and perplexity is 169.44988356572264
At time: 459.64190673828125 and batch: 900, loss is 5.112230310440063 and perplexity is 166.04026354806675
At time: 460.4643905162811 and batch: 950, loss is 5.111599235534668 and perplexity is 165.93551276072947
At time: 461.28669023513794 and batch: 1000, loss is 5.11244234085083 and perplexity is 166.07547286594183
At time: 462.1150572299957 and batch: 1050, loss is 5.104803018569946 and perplexity is 164.81160250879577
At time: 462.94468331336975 and batch: 1100, loss is 5.055282936096192 and perplexity is 156.8489022827645
At time: 463.7693431377411 and batch: 1150, loss is 5.09193151473999 and perplexity is 162.70382357871802
At time: 464.5940029621124 and batch: 1200, loss is 5.1152011585235595 and perplexity is 166.534277403525
At time: 465.4218912124634 and batch: 1250, loss is 5.14809404373169 and perplexity is 172.10315643143835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.147730917826186 and perplexity of 172.0406726623433
Finished 21 epochs...
Completing Train Step...
At time: 467.64579796791077 and batch: 50, loss is 5.149361028671264 and perplexity is 172.3213467314147
At time: 468.499178647995 and batch: 100, loss is 5.155090675354004 and perplexity is 173.31152112995275
At time: 469.3227484226227 and batch: 150, loss is 5.0979495620727535 and perplexity is 163.6859351230322
At time: 470.1455545425415 and batch: 200, loss is 5.136927747726441 and perplexity is 170.19209125989056
At time: 470.97035098075867 and batch: 250, loss is 5.138627128601074 and perplexity is 170.48155833253247
At time: 471.7964401245117 and batch: 300, loss is 5.129545469284057 and perplexity is 168.9403120298845
At time: 472.63485050201416 and batch: 350, loss is 5.1419289112091064 and perplexity is 171.0453816756567
At time: 473.4574246406555 and batch: 400, loss is 5.115096683502197 and perplexity is 166.51687964016742
At time: 474.2818338871002 and batch: 450, loss is 5.081819248199463 and perplexity is 161.06681005584207
At time: 475.1062388420105 and batch: 500, loss is 5.083675689697266 and perplexity is 161.3660988860955
At time: 475.93045377731323 and batch: 550, loss is 5.078643922805786 and perplexity is 160.55618165849393
At time: 476.7760832309723 and batch: 600, loss is 5.111123046875 and perplexity is 165.85651496173733
At time: 477.6051285266876 and batch: 650, loss is 5.124728775024414 and perplexity is 168.1285348095186
At time: 478.4299039840698 and batch: 700, loss is 5.138692464828491 and perplexity is 170.49269731828375
At time: 479.2678060531616 and batch: 750, loss is 5.102963342666626 and perplexity is 164.50868129893198
At time: 480.1018671989441 and batch: 800, loss is 5.113994474411011 and perplexity is 166.3334443321948
At time: 480.9747564792633 and batch: 850, loss is 5.1322519397735595 and perplexity is 169.39816329987676
At time: 481.8079643249512 and batch: 900, loss is 5.112177467346191 and perplexity is 166.03148969865424
At time: 482.64328384399414 and batch: 950, loss is 5.111899070739746 and perplexity is 165.98527352887
At time: 483.4856798648834 and batch: 1000, loss is 5.113179244995117 and perplexity is 166.19789967303186
At time: 484.31229424476624 and batch: 1050, loss is 5.105766153335571 and perplexity is 164.9704147594737
At time: 485.1344735622406 and batch: 1100, loss is 5.056621561050415 and perplexity is 157.05900473021842
At time: 485.9585311412811 and batch: 1150, loss is 5.093687181472778 and perplexity is 162.98972817222585
At time: 486.77863907814026 and batch: 1200, loss is 5.116908283233642 and perplexity is 166.81881498502278
At time: 487.6015372276306 and batch: 1250, loss is 5.148181600570679 and perplexity is 172.11822589950336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.147599046247719 and perplexity of 172.01798688311646
Finished 22 epochs...
Completing Train Step...
At time: 489.8414077758789 and batch: 50, loss is 5.1465957069396975 and perplexity is 171.8454810306129
At time: 490.6611132621765 and batch: 100, loss is 5.152396860122681 and perplexity is 172.84528018000054
At time: 491.48195481300354 and batch: 150, loss is 5.095216093063354 and perplexity is 163.23911565389875
At time: 492.3059630393982 and batch: 200, loss is 5.133736743927002 and perplexity is 169.64987322005314
At time: 493.1312599182129 and batch: 250, loss is 5.13573016166687 and perplexity is 169.98839358104422
At time: 493.95511960983276 and batch: 300, loss is 5.126642713546753 and perplexity is 168.45063062646688
At time: 494.7729640007019 and batch: 350, loss is 5.13930230140686 and perplexity is 170.5967017110715
At time: 495.59889936447144 and batch: 400, loss is 5.1127950477600095 and perplexity is 166.13405916395558
At time: 496.42205238342285 and batch: 450, loss is 5.079318933486938 and perplexity is 160.66459538213266
At time: 497.2448830604553 and batch: 500, loss is 5.081609649658203 and perplexity is 161.03305422511897
At time: 498.0686821937561 and batch: 550, loss is 5.0768705749511716 and perplexity is 160.2717120045791
At time: 498.89369559288025 and batch: 600, loss is 5.10970097541809 and perplexity is 165.62082277115596
At time: 499.71898221969604 and batch: 650, loss is 5.123392705917358 and perplexity is 167.90405346283444
At time: 500.5430791378021 and batch: 700, loss is 5.137350692749023 and perplexity is 170.2640883821108
At time: 501.37874031066895 and batch: 750, loss is 5.102042169570923 and perplexity is 164.35721010401562
At time: 502.2312295436859 and batch: 800, loss is 5.113137817382812 and perplexity is 166.19101463349466
At time: 503.06326937675476 and batch: 850, loss is 5.131716222763061 and perplexity is 169.3074381259291
At time: 503.91340804100037 and batch: 900, loss is 5.111934242248535 and perplexity is 165.99111158404276
At time: 504.75178956985474 and batch: 950, loss is 5.111757774353027 and perplexity is 165.9618220663143
At time: 505.58608531951904 and batch: 1000, loss is 5.113373603820801 and perplexity is 166.2302048409389
At time: 506.4068601131439 and batch: 1050, loss is 5.106069955825806 and perplexity is 165.02054079611455
At time: 507.22939920425415 and batch: 1100, loss is 5.05705376625061 and perplexity is 157.12690112032988
At time: 508.0536768436432 and batch: 1150, loss is 5.094449443817139 and perplexity is 163.1140164686551
At time: 508.87723541259766 and batch: 1200, loss is 5.117769975662231 and perplexity is 166.9626234452739
At time: 509.70029735565186 and batch: 1250, loss is 5.147962818145752 and perplexity is 172.08057357565048
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.147465392620894 and perplexity of 171.99499759162578
Finished 23 epochs...
Completing Train Step...
At time: 511.91788363456726 and batch: 50, loss is 5.144433288574219 and perplexity is 171.47428069626397
At time: 512.7694520950317 and batch: 100, loss is 5.150103139877319 and perplexity is 172.44927579679188
At time: 513.5927810668945 and batch: 150, loss is 5.093042707443237 and perplexity is 162.8847193666955
At time: 514.4192290306091 and batch: 200, loss is 5.131240253448486 and perplexity is 169.2268721556377
At time: 515.2413711547852 and batch: 250, loss is 5.133096780776977 and perplexity is 169.5413382856695
At time: 516.064902305603 and batch: 300, loss is 5.1242364978790285 and perplexity is 168.04578934286687
At time: 516.8917169570923 and batch: 350, loss is 5.137195596694946 and perplexity is 170.23768314157894
At time: 517.7161478996277 and batch: 400, loss is 5.110852642059326 and perplexity is 165.811672624446
At time: 518.5402684211731 and batch: 450, loss is 5.077335357666016 and perplexity is 160.34622083986537
At time: 519.3652169704437 and batch: 500, loss is 5.0799378490448 and perplexity is 160.7640639779678
At time: 520.1864397525787 and batch: 550, loss is 5.075191078186035 and perplexity is 160.00276209619415
At time: 521.0044429302216 and batch: 600, loss is 5.108292217254639 and perplexity is 165.3876673533986
At time: 521.8658118247986 and batch: 650, loss is 5.122327432632447 and perplexity is 167.7252849957938
At time: 522.7222008705139 and batch: 700, loss is 5.136461925506592 and perplexity is 170.11283046429227
At time: 523.5407783985138 and batch: 750, loss is 5.1011544132232665 and perplexity is 164.2113656941915
At time: 524.3587284088135 and batch: 800, loss is 5.112196483612061 and perplexity is 166.03464702762528
At time: 525.2001631259918 and batch: 850, loss is 5.131167573928833 and perplexity is 169.21457327480078
At time: 526.0441906452179 and batch: 900, loss is 5.111608533859253 and perplexity is 165.9370556901606
At time: 526.869473695755 and batch: 950, loss is 5.111385107040405 and perplexity is 165.89998504312135
At time: 527.6888575553894 and batch: 1000, loss is 5.113220138549805 and perplexity is 166.2046962348979
At time: 528.5127239227295 and batch: 1050, loss is 5.106038246154785 and perplexity is 165.01530813201776
At time: 529.3456900119781 and batch: 1100, loss is 5.0570586395263675 and perplexity is 157.12766684491368
At time: 530.1697313785553 and batch: 1150, loss is 5.094653635025025 and perplexity is 163.1473263173744
At time: 531.0028021335602 and batch: 1200, loss is 5.118073921203614 and perplexity is 167.01337870327407
At time: 531.8491668701172 and batch: 1250, loss is 5.147526760101318 and perplexity is 172.00555281514386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.147378072251368 and perplexity of 171.9799795805781
Finished 24 epochs...
Completing Train Step...
At time: 534.1855990886688 and batch: 50, loss is 5.142443609237671 and perplexity is 171.13344105645106
At time: 535.0191028118134 and batch: 100, loss is 5.148204374313354 and perplexity is 172.1221457203241
At time: 535.8551070690155 and batch: 150, loss is 5.091207675933838 and perplexity is 162.58609485075357
At time: 536.6889791488647 and batch: 200, loss is 5.129308700561523 and perplexity is 168.90031698299563
At time: 537.513685464859 and batch: 250, loss is 5.131113786697387 and perplexity is 169.20547193615417
At time: 538.3374216556549 and batch: 300, loss is 5.122141571044922 and perplexity is 167.69411420487248
At time: 539.1825587749481 and batch: 350, loss is 5.135387058258057 and perplexity is 169.93007998811623
At time: 540.0086236000061 and batch: 400, loss is 5.109181509017945 and perplexity is 165.5348106606967
At time: 540.8469014167786 and batch: 450, loss is 5.075629501342774 and perplexity is 160.07292639194145
At time: 541.6836783885956 and batch: 500, loss is 5.078463220596314 and perplexity is 160.5271714229096
At time: 542.5076658725739 and batch: 550, loss is 5.0738232135772705 and perplexity is 159.78404959926218
At time: 543.338707447052 and batch: 600, loss is 5.107239360809326 and perplexity is 165.21362951635643
At time: 544.1912982463837 and batch: 650, loss is 5.121312866210937 and perplexity is 167.55520284800645
At time: 545.0172955989838 and batch: 700, loss is 5.135648603439331 and perplexity is 169.9745301943059
At time: 545.842209815979 and batch: 750, loss is 5.100341939926148 and perplexity is 164.07800252882313
At time: 546.6672585010529 and batch: 800, loss is 5.111303119659424 and perplexity is 165.8863838954117
At time: 547.4908504486084 and batch: 850, loss is 5.130533857345581 and perplexity is 169.10737316442007
At time: 548.318213224411 and batch: 900, loss is 5.111136951446533 and perplexity is 165.85882114154708
At time: 549.1468636989594 and batch: 950, loss is 5.110933303833008 and perplexity is 165.8250478274836
At time: 549.9726903438568 and batch: 1000, loss is 5.112902736663818 and perplexity is 166.15195092202683
At time: 550.7967298030853 and batch: 1050, loss is 5.105858221054077 and perplexity is 164.985603908386
At time: 551.6219754219055 and batch: 1100, loss is 5.056861352920532 and perplexity is 157.0966707184999
At time: 552.4461772441864 and batch: 1150, loss is 5.094665231704712 and perplexity is 163.14921829562988
At time: 553.2668511867523 and batch: 1200, loss is 5.118102474212646 and perplexity is 167.01814750586627
At time: 554.1054112911224 and batch: 1250, loss is 5.146988639831543 and perplexity is 171.9130180403121
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.147300998659899 and perplexity of 171.96672497668757
Finished 25 epochs...
Completing Train Step...
At time: 556.3765096664429 and batch: 50, loss is 5.140634212493897 and perplexity is 170.82407273487328
At time: 557.2332470417023 and batch: 100, loss is 5.146443729400635 and perplexity is 171.8193663617786
At time: 558.057722568512 and batch: 150, loss is 5.089635763168335 and perplexity is 162.3307244553398
At time: 558.8822996616364 and batch: 200, loss is 5.127591209411621 and perplexity is 168.61048114984672
At time: 559.706629037857 and batch: 250, loss is 5.129402904510498 and perplexity is 168.9162288093052
At time: 560.5380575656891 and batch: 300, loss is 5.120355224609375 and perplexity is 167.3948218212294
At time: 561.3763997554779 and batch: 350, loss is 5.1337002182006835 and perplexity is 169.64367674838
At time: 562.1976964473724 and batch: 400, loss is 5.1076281642913814 and perplexity is 165.27787763993308
At time: 563.022135257721 and batch: 450, loss is 5.074029026031494 and perplexity is 159.8169385310157
At time: 563.8490223884583 and batch: 500, loss is 5.076980476379394 and perplexity is 160.28932706257442
At time: 564.6743817329407 and batch: 550, loss is 5.072561836242675 and perplexity is 159.58262868126445
At time: 565.5333054065704 and batch: 600, loss is 5.106242084503174 and perplexity is 165.0489480083182
At time: 566.3530321121216 and batch: 650, loss is 5.120265417098999 and perplexity is 167.3797891840638
At time: 567.1761150360107 and batch: 700, loss is 5.13487208366394 and perplexity is 169.84259284290818
At time: 567.9982514381409 and batch: 750, loss is 5.09955979347229 and perplexity is 163.9497196755183
At time: 568.8216893672943 and batch: 800, loss is 5.110484790802002 and perplexity is 165.7506898091824
At time: 569.6441686153412 and batch: 850, loss is 5.129788866043091 and perplexity is 168.98143655888825
At time: 570.4699673652649 and batch: 900, loss is 5.1106280422210695 and perplexity is 165.77443553147256
At time: 571.2950568199158 and batch: 950, loss is 5.11045313835144 and perplexity is 165.74544347669772
At time: 572.1204583644867 and batch: 1000, loss is 5.112649726867676 and perplexity is 166.10991816836884
At time: 572.9440188407898 and batch: 1050, loss is 5.105530366897583 and perplexity is 164.9315215584291
At time: 573.7683565616608 and batch: 1100, loss is 5.05647614479065 and perplexity is 157.03616745768375
At time: 574.5929486751556 and batch: 1150, loss is 5.094444131851196 and perplexity is 163.11315001485616
At time: 575.4179637432098 and batch: 1200, loss is 5.118074150085449 and perplexity is 167.01341692960708
At time: 576.2412195205688 and batch: 1250, loss is 5.146463871002197 and perplexity is 171.82282711384903
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.147162889912181 and perplexity of 171.94297650762527
Finished 26 epochs...
Completing Train Step...
At time: 578.4709749221802 and batch: 50, loss is 5.138955230712891 and perplexity is 170.53750286910557
At time: 579.346449136734 and batch: 100, loss is 5.144902353286743 and perplexity is 171.5547320974256
At time: 580.1677412986755 and batch: 150, loss is 5.088150196075439 and perplexity is 162.08975030887146
At time: 580.9897062778473 and batch: 200, loss is 5.125963888168335 and perplexity is 168.3363208659599
At time: 581.8138053417206 and batch: 250, loss is 5.127788763046265 and perplexity is 168.6437940536712
At time: 582.6383955478668 and batch: 300, loss is 5.118593282699585 and perplexity is 167.10014155017538
At time: 583.4610788822174 and batch: 350, loss is 5.1320088100433345 and perplexity is 169.356982576465
At time: 584.2856202125549 and batch: 400, loss is 5.106088247299194 and perplexity is 165.02355929255137
At time: 585.1248207092285 and batch: 450, loss is 5.0725329399108885 and perplexity is 159.5780173953036
At time: 585.964328289032 and batch: 500, loss is 5.075721616744995 and perplexity is 160.08767225309086
At time: 586.8156054019928 and batch: 550, loss is 5.071499443054199 and perplexity is 159.4131792104739
At time: 587.6641280651093 and batch: 600, loss is 5.105221300125122 and perplexity is 164.88055458188913
At time: 588.5076768398285 and batch: 650, loss is 5.119360046386719 and perplexity is 167.2283170046932
At time: 589.3341846466064 and batch: 700, loss is 5.134046592712402 and perplexity is 169.70244717178068
At time: 590.1584303379059 and batch: 750, loss is 5.098796195983887 and perplexity is 163.82457586717936
At time: 590.9808449745178 and batch: 800, loss is 5.10948489189148 and perplexity is 165.58503870600418
At time: 591.8197708129883 and batch: 850, loss is 5.129071197509766 and perplexity is 168.86020740553138
At time: 592.6469650268555 and batch: 900, loss is 5.109964666366577 and perplexity is 165.66450124155713
At time: 593.4703936576843 and batch: 950, loss is 5.109873847961426 and perplexity is 165.64945653894253
At time: 594.3210022449493 and batch: 1000, loss is 5.1119797706604 and perplexity is 165.99866906777578
At time: 595.1803736686707 and batch: 1050, loss is 5.1050344085693355 and perplexity is 164.84974267785634
At time: 596.0355160236359 and batch: 1100, loss is 5.056085977554321 and perplexity is 156.9749090415138
At time: 596.8902945518494 and batch: 1150, loss is 5.094253435134887 and perplexity is 163.08204783839727
At time: 597.7653498649597 and batch: 1200, loss is 5.11784815788269 and perplexity is 166.97567746419801
At time: 598.5969784259796 and batch: 1250, loss is 5.145781307220459 and perplexity is 171.70558709163353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.147078688127281 and perplexity of 171.92849921161792
Finished 27 epochs...
Completing Train Step...
At time: 600.8576362133026 and batch: 50, loss is 5.137366256713867 and perplexity is 170.26673838701882
At time: 601.6817348003387 and batch: 100, loss is 5.14342432975769 and perplexity is 171.30135745982912
At time: 602.5272951126099 and batch: 150, loss is 5.086699438095093 and perplexity is 161.85476780266555
At time: 603.3518278598785 and batch: 200, loss is 5.124581785202026 and perplexity is 168.10382344225292
At time: 604.1768002510071 and batch: 250, loss is 5.126271438598633 and perplexity is 168.38810073592953
At time: 605.0029838085175 and batch: 300, loss is 5.116947040557862 and perplexity is 166.82528056121444
At time: 605.8265397548676 and batch: 350, loss is 5.130534057617187 and perplexity is 169.10740703182873
At time: 606.6500880718231 and batch: 400, loss is 5.104673614501953 and perplexity is 164.7902765968384
At time: 607.5024690628052 and batch: 450, loss is 5.071168670654297 and perplexity is 159.36045845037728
At time: 608.3274970054626 and batch: 500, loss is 5.0745945644378665 and perplexity is 159.907346709983
At time: 609.3466534614563 and batch: 550, loss is 5.070467929840088 and perplexity is 159.2488271898283
At time: 610.1705899238586 and batch: 600, loss is 5.104169874191284 and perplexity is 164.70728599632065
At time: 611.011892080307 and batch: 650, loss is 5.118469696044922 and perplexity is 167.0794914787504
At time: 611.8428168296814 and batch: 700, loss is 5.133267068862915 and perplexity is 169.57021161397566
At time: 612.6654500961304 and batch: 750, loss is 5.097829923629761 and perplexity is 163.66635316401525
At time: 613.4893033504486 and batch: 800, loss is 5.1084700775146485 and perplexity is 165.41708586303082
At time: 614.3154215812683 and batch: 850, loss is 5.1282037734985355 and perplexity is 168.71379751599355
At time: 615.1614668369293 and batch: 900, loss is 5.109300813674927 and perplexity is 165.55456091262627
At time: 615.9863412380219 and batch: 950, loss is 5.109439296722412 and perplexity is 165.57748900028562
At time: 616.8079860210419 and batch: 1000, loss is 5.111621570587158 and perplexity is 165.9392189805061
At time: 617.6544482707977 and batch: 1050, loss is 5.1044680118560795 and perplexity is 164.75639876276128
At time: 618.4761781692505 and batch: 1100, loss is 5.055718040466308 and perplexity is 156.917162774738
At time: 619.3002505302429 and batch: 1150, loss is 5.093949346542359 and perplexity is 163.03246398732722
At time: 620.1247727870941 and batch: 1200, loss is 5.117539377212524 and perplexity is 166.92412656197982
At time: 620.9897418022156 and batch: 1250, loss is 5.145220327377319 and perplexity is 171.6092907310046
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.147056412522811 and perplexity of 171.92466944302768
Finished 28 epochs...
Completing Train Step...
At time: 623.2605798244476 and batch: 50, loss is 5.136000318527222 and perplexity is 170.03432331558741
At time: 624.0847723484039 and batch: 100, loss is 5.1420873928070066 and perplexity is 171.07249136919512
At time: 624.9285700321198 and batch: 150, loss is 5.085452346801758 and perplexity is 161.65304593988725
At time: 625.7756221294403 and batch: 200, loss is 5.1232891941070555 and perplexity is 167.88667430979265
At time: 626.629133939743 and batch: 250, loss is 5.124882335662842 and perplexity is 168.15435471707195
At time: 627.4681701660156 and batch: 300, loss is 5.115540676116943 and perplexity is 166.59082832007724
At time: 628.2931413650513 and batch: 350, loss is 5.129231357574463 and perplexity is 168.88725423312732
At time: 629.1450369358063 and batch: 400, loss is 5.103449459075928 and perplexity is 164.58867110898848
At time: 629.9693639278412 and batch: 450, loss is 5.06989501953125 and perplexity is 159.15761802489104
At time: 630.7935543060303 and batch: 500, loss is 5.073315486907959 and perplexity is 159.70294356754007
At time: 631.6384644508362 and batch: 550, loss is 5.069424114227295 and perplexity is 159.08268750237315
At time: 632.4627356529236 and batch: 600, loss is 5.103333349227905 and perplexity is 164.56956185280768
At time: 633.2841708660126 and batch: 650, loss is 5.117564792633057 and perplexity is 166.92836906276565
At time: 634.1085641384125 and batch: 700, loss is 5.132384328842163 and perplexity is 169.42059124951413
At time: 634.9357063770294 and batch: 750, loss is 5.096997261047363 and perplexity is 163.53013103714258
At time: 635.7577219009399 and batch: 800, loss is 5.107636528015137 and perplexity is 165.27925998422532
At time: 636.5797488689423 and batch: 850, loss is 5.127484550476074 and perplexity is 168.59249829443843
At time: 637.4035937786102 and batch: 900, loss is 5.10871376991272 and perplexity is 165.45740166149406
At time: 638.249749660492 and batch: 950, loss is 5.108844680786133 and perplexity is 165.4790632522961
At time: 639.0938622951508 and batch: 1000, loss is 5.111064987182617 and perplexity is 165.84688566303876
At time: 639.9175415039062 and batch: 1050, loss is 5.10390851020813 and perplexity is 164.66424306917853
At time: 640.7577335834503 and batch: 1100, loss is 5.0552021026611325 and perplexity is 156.83622415962273
At time: 641.5928966999054 and batch: 1150, loss is 5.093517742156982 and perplexity is 162.96211364376208
At time: 642.4168388843536 and batch: 1200, loss is 5.1172425651550295 and perplexity is 166.8745888205926
At time: 643.2407233715057 and batch: 1250, loss is 5.14459345817566 and perplexity is 171.50174786310427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1470662137887775 and perplexity of 171.92635453069713
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 645.4831919670105 and batch: 50, loss is 5.136178922653198 and perplexity is 170.06469485945004
At time: 646.3073830604553 and batch: 100, loss is 5.143117475509643 and perplexity is 171.2488009745992
At time: 647.1291179656982 and batch: 150, loss is 5.086235113143921 and perplexity is 161.77963204056311
At time: 647.9524245262146 and batch: 200, loss is 5.1239158725738525 and perplexity is 167.99191824705503
At time: 648.7754073143005 and batch: 250, loss is 5.125361661911011 and perplexity is 168.23497483315714
At time: 649.5987219810486 and batch: 300, loss is 5.115205335617065 and perplexity is 166.53497303422648
At time: 650.4220490455627 and batch: 350, loss is 5.127851152420044 and perplexity is 168.65431596259822
At time: 651.2500267028809 and batch: 400, loss is 5.100677881240845 and perplexity is 164.13313236838246
At time: 652.0732309818268 and batch: 450, loss is 5.066544675827027 and perplexity is 158.62527756107417
At time: 652.8968603610992 and batch: 500, loss is 5.069775333404541 and perplexity is 159.138570205956
At time: 653.7180259227753 and batch: 550, loss is 5.065067987442017 and perplexity is 158.39121032085237
At time: 654.5414898395538 and batch: 600, loss is 5.098315668106079 and perplexity is 163.7458725025
At time: 655.3619434833527 and batch: 650, loss is 5.112805023193359 and perplexity is 166.13571643145585
At time: 656.1848466396332 and batch: 700, loss is 5.128049144744873 and perplexity is 168.6877115286318
At time: 657.0097661018372 and batch: 750, loss is 5.090945014953613 and perplexity is 162.5433954356898
At time: 657.8330030441284 and batch: 800, loss is 5.100055990219116 and perplexity is 164.03109117953258
At time: 658.6579673290253 and batch: 850, loss is 5.1175001525878905 and perplexity is 166.91757915418376
At time: 659.481537103653 and batch: 900, loss is 5.098450317382812 and perplexity is 163.7679222502584
At time: 660.3047497272491 and batch: 950, loss is 5.0987341022491455 and perplexity is 163.8144037032384
At time: 661.129508972168 and batch: 1000, loss is 5.1011560344696045 and perplexity is 164.2116319214826
At time: 661.9538640975952 and batch: 1050, loss is 5.092114515304566 and perplexity is 162.73360119486873
At time: 662.7784321308136 and batch: 1100, loss is 5.042149085998535 and perplexity is 154.80234134338835
At time: 663.6010346412659 and batch: 1150, loss is 5.079127454757691 and perplexity is 160.63383447469755
At time: 664.4257652759552 and batch: 1200, loss is 5.105512800216675 and perplexity is 164.928624284466
At time: 665.2510430812836 and batch: 1250, loss is 5.1367676734924315 and perplexity is 170.16485007161202
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.144926864735401 and perplexity of 171.55893720396995
Finished 30 epochs...
Completing Train Step...
At time: 667.4668619632721 and batch: 50, loss is 5.1337431526184085 and perplexity is 169.6509604572217
At time: 668.3166093826294 and batch: 100, loss is 5.139875030517578 and perplexity is 170.6944353931247
At time: 669.1412994861603 and batch: 150, loss is 5.0835364627838135 and perplexity is 161.34363394611015
At time: 669.9638500213623 and batch: 200, loss is 5.120867862701416 and perplexity is 167.48065678256103
At time: 670.7841279506683 and batch: 250, loss is 5.122898941040039 and perplexity is 167.82116880292588
At time: 671.6079123020172 and batch: 300, loss is 5.11298303604126 and perplexity is 166.16529335593398
At time: 672.4333655834198 and batch: 350, loss is 5.1257024765014645 and perplexity is 168.29232153894677
At time: 673.2573335170746 and batch: 400, loss is 5.098643341064453 and perplexity is 163.7995363885864
At time: 674.0802447795868 and batch: 450, loss is 5.064840230941773 and perplexity is 158.3551398009238
At time: 674.9032390117645 and batch: 500, loss is 5.068424472808838 and perplexity is 158.92374131681748
At time: 675.7281627655029 and batch: 550, loss is 5.063745441436768 and perplexity is 158.18186912057374
At time: 676.5542199611664 and batch: 600, loss is 5.097055778503418 and perplexity is 163.5397006843922
At time: 677.3813705444336 and batch: 650, loss is 5.111816539764404 and perplexity is 165.97157516763062
At time: 678.2045135498047 and batch: 700, loss is 5.127179069519043 and perplexity is 168.54100436231937
At time: 679.029388666153 and batch: 750, loss is 5.090376844406128 and perplexity is 162.4510692966952
At time: 679.8538746833801 and batch: 800, loss is 5.09962456703186 and perplexity is 163.96033962639453
At time: 680.6938877105713 and batch: 850, loss is 5.11760498046875 and perplexity is 166.93507768743572
At time: 681.5270669460297 and batch: 900, loss is 5.09876184463501 and perplexity is 163.81894836867568
At time: 682.3507235050201 and batch: 950, loss is 5.099211282730103 and perplexity is 163.89259139252124
At time: 683.1752138137817 and batch: 1000, loss is 5.101831388473511 and perplexity is 164.32257036173206
At time: 684.0372183322906 and batch: 1050, loss is 5.093093328475952 and perplexity is 162.89296496810198
At time: 684.9012837409973 and batch: 1100, loss is 5.043133563995362 and perplexity is 154.95481588389848
At time: 685.7304577827454 and batch: 1150, loss is 5.080269947052002 and perplexity is 160.81746226948687
At time: 686.5552423000336 and batch: 1200, loss is 5.106830959320068 and perplexity is 165.14616980027813
At time: 687.4039497375488 and batch: 1250, loss is 5.137269115447998 and perplexity is 170.2501992638451
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.144921518590328 and perplexity of 171.55802002745472
Finished 31 epochs...
Completing Train Step...
At time: 689.6481914520264 and batch: 50, loss is 5.132525043487549 and perplexity is 169.44443288532716
At time: 690.4744617938995 and batch: 100, loss is 5.138407316207886 and perplexity is 170.44408849151947
At time: 691.2990274429321 and batch: 150, loss is 5.082131567001343 and perplexity is 161.1171221052719
At time: 692.1289598941803 and batch: 200, loss is 5.11934799194336 and perplexity is 167.2263011725677
At time: 692.952957868576 and batch: 250, loss is 5.12161732673645 and perplexity is 167.6062245597724
At time: 693.7759473323822 and batch: 300, loss is 5.111821928024292 and perplexity is 165.972469468021
At time: 694.6000969409943 and batch: 350, loss is 5.12446626663208 and perplexity is 168.08440545055728
At time: 695.4251754283905 and batch: 400, loss is 5.097412796020508 and perplexity is 163.59809764602488
At time: 696.2476198673248 and batch: 450, loss is 5.06377914428711 and perplexity is 158.18720039027448
At time: 697.0717775821686 and batch: 500, loss is 5.067623853683472 and perplexity is 158.7965548507969
At time: 697.8952565193176 and batch: 550, loss is 5.063006534576416 and perplexity is 158.06503062399653
At time: 698.7188012599945 and batch: 600, loss is 5.096330318450928 and perplexity is 163.42110218898623
At time: 699.5461394786835 and batch: 650, loss is 5.111186075210571 and perplexity is 165.86696895126434
At time: 700.375883102417 and batch: 700, loss is 5.1267092895507815 and perplexity is 168.4618457696557
At time: 701.2056250572205 and batch: 750, loss is 5.090154294967651 and perplexity is 162.41491992510365
At time: 702.0557370185852 and batch: 800, loss is 5.099422359466553 and perplexity is 163.9271889570828
At time: 702.886137008667 and batch: 850, loss is 5.1176735496521 and perplexity is 166.94652468183625
At time: 703.7095155715942 and batch: 900, loss is 5.0990176486969 and perplexity is 163.86085928134526
At time: 704.5336377620697 and batch: 950, loss is 5.099559640884399 and perplexity is 163.9496946587783
At time: 705.3597800731659 and batch: 1000, loss is 5.102302837371826 and perplexity is 164.40005832086865
At time: 706.1821267604828 and batch: 1050, loss is 5.0936679267883305 and perplexity is 162.98658988665514
At time: 707.0077033042908 and batch: 1100, loss is 5.043660259246826 and perplexity is 155.03645134623403
At time: 707.8611369132996 and batch: 1150, loss is 5.080892152786255 and perplexity is 160.91755495257055
At time: 708.6833503246307 and batch: 1200, loss is 5.107572803497314 and perplexity is 165.2687279786538
At time: 709.5058317184448 and batch: 1250, loss is 5.137427759170532 and perplexity is 170.277210531746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.144933547416743 and perplexity of 171.56008368150935
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 711.8110690116882 and batch: 50, loss is 5.132343521118164 and perplexity is 169.4136777218503
At time: 712.6602742671967 and batch: 100, loss is 5.138412265777588 and perplexity is 170.44493211850363
At time: 713.4998781681061 and batch: 150, loss is 5.082162208557129 and perplexity is 161.12205906019457
At time: 714.3483512401581 and batch: 200, loss is 5.11897614479065 and perplexity is 167.16413010840142
At time: 715.196634054184 and batch: 250, loss is 5.121580066680909 and perplexity is 167.59997965887962
At time: 716.022540807724 and batch: 300, loss is 5.11174708366394 and perplexity is 165.96004782955822
At time: 716.8454186916351 and batch: 350, loss is 5.123594188690186 and perplexity is 167.93788664538238
At time: 717.669798374176 and batch: 400, loss is 5.0964004421234135 and perplexity is 163.43256227864046
At time: 718.5276947021484 and batch: 450, loss is 5.062265005111694 and perplexity is 157.94786419299598
At time: 719.3852849006653 and batch: 500, loss is 5.066433219909668 and perplexity is 158.6075988204658
At time: 720.2097718715668 and batch: 550, loss is 5.06112301826477 and perplexity is 157.76759276298037
At time: 721.0347280502319 and batch: 600, loss is 5.0939422130584715 and perplexity is 163.03130100202037
At time: 721.8594574928284 and batch: 650, loss is 5.109161615371704 and perplexity is 165.5315176024885
At time: 722.6832098960876 and batch: 700, loss is 5.124947786331177 and perplexity is 168.16536089214844
At time: 723.5068383216858 and batch: 750, loss is 5.088227062225342 and perplexity is 162.10221000277463
At time: 724.3307011127472 and batch: 800, loss is 5.097378253936768 and perplexity is 163.59244672443415
At time: 725.1558372974396 and batch: 850, loss is 5.114413681030274 and perplexity is 166.40318703034268
At time: 725.9787256717682 and batch: 900, loss is 5.095178699493408 and perplexity is 163.23301167473508
At time: 726.8191609382629 and batch: 950, loss is 5.096300954818726 and perplexity is 163.4163036222995
At time: 727.675703048706 and batch: 1000, loss is 5.098561878204346 and perplexity is 163.7861933533562
At time: 728.5137362480164 and batch: 1050, loss is 5.089599342346191 and perplexity is 162.32481234455855
At time: 729.3683552742004 and batch: 1100, loss is 5.039551401138306 and perplexity is 154.4007354935059
At time: 730.195853471756 and batch: 1150, loss is 5.076156425476074 and perplexity is 160.15729490589635
At time: 731.0174798965454 and batch: 1200, loss is 5.103631401062012 and perplexity is 164.61861942304228
At time: 731.8410997390747 and batch: 1250, loss is 5.13542368888855 and perplexity is 169.9363047480937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.145994311701642 and perplexity of 171.74216504662405
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 734.1208226680756 and batch: 50, loss is 5.132111864089966 and perplexity is 169.3744363981728
At time: 734.979462146759 and batch: 100, loss is 5.1376979637146 and perplexity is 170.3232264243545
At time: 735.8026909828186 and batch: 150, loss is 5.081573705673218 and perplexity is 161.02726615945954
At time: 736.6252446174622 and batch: 200, loss is 5.118560752868652 and perplexity is 167.09470589923276
At time: 737.4495625495911 and batch: 250, loss is 5.121132850646973 and perplexity is 167.52504301837138
At time: 738.2734062671661 and batch: 300, loss is 5.1113494110107425 and perplexity is 165.89406317802835
At time: 739.0958514213562 and batch: 350, loss is 5.122966957092285 and perplexity is 167.83258372450578
At time: 739.9386789798737 and batch: 400, loss is 5.095731744766235 and perplexity is 163.3233118879626
At time: 740.7731759548187 and batch: 450, loss is 5.061624202728272 and perplexity is 157.84668324712257
At time: 741.5941777229309 and batch: 500, loss is 5.066030769348145 and perplexity is 158.5437799460909
At time: 742.4193105697632 and batch: 550, loss is 5.060313062667847 and perplexity is 157.63985975422455
At time: 743.2628221511841 and batch: 600, loss is 5.0931321811676025 and perplexity is 162.89929392118955
At time: 744.0956566333771 and batch: 650, loss is 5.108715934753418 and perplexity is 165.45775985079865
At time: 744.9208931922913 and batch: 700, loss is 5.124533519744873 and perplexity is 168.0957100301657
At time: 745.7451913356781 and batch: 750, loss is 5.087772607803345 and perplexity is 162.02855867347708
At time: 746.5697569847107 and batch: 800, loss is 5.096921834945679 and perplexity is 163.51779706200819
At time: 747.4168574810028 and batch: 850, loss is 5.113356122970581 and perplexity is 166.22729902102427
At time: 748.2575705051422 and batch: 900, loss is 5.094082508087158 and perplexity is 163.05417508759402
At time: 749.0832009315491 and batch: 950, loss is 5.095372858047486 and perplexity is 163.2647078372006
At time: 749.9071047306061 and batch: 1000, loss is 5.097267694473267 and perplexity is 163.57436103108225
At time: 750.7741322517395 and batch: 1050, loss is 5.088500461578369 and perplexity is 162.14653470100947
At time: 751.6037933826447 and batch: 1100, loss is 5.038430023193359 and perplexity is 154.22769095633709
At time: 752.4382567405701 and batch: 1150, loss is 5.07479266166687 and perplexity is 159.93902705005377
At time: 753.2749984264374 and batch: 1200, loss is 5.102375068664551 and perplexity is 164.41193357848266
At time: 754.0998203754425 and batch: 1250, loss is 5.1348105144500735 and perplexity is 169.83213608989607
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.145917683622263 and perplexity of 171.72900527857837
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 756.3495666980743 and batch: 50, loss is 5.132141761779785 and perplexity is 169.37950037823578
At time: 757.1741261482239 and batch: 100, loss is 5.137432107925415 and perplexity is 170.2779510272068
At time: 757.9988610744476 and batch: 150, loss is 5.081412057876587 and perplexity is 161.00123856039608
At time: 758.830335855484 and batch: 200, loss is 5.1184313678741455 and perplexity is 167.0730877501901
At time: 759.6537051200867 and batch: 250, loss is 5.120858201980591 and perplexity is 167.47903880650767
At time: 760.4792604446411 and batch: 300, loss is 5.111008071899414 and perplexity is 165.83744670918585
At time: 761.3042798042297 and batch: 350, loss is 5.122623596191406 and perplexity is 167.77496646968362
At time: 762.1302213668823 and batch: 400, loss is 5.0954360675811765 and perplexity is 163.27502804941562
At time: 762.9497923851013 and batch: 450, loss is 5.061464824676514 and perplexity is 157.82152795492416
At time: 763.7733917236328 and batch: 500, loss is 5.065939226150513 and perplexity is 158.5292670058006
At time: 764.597161769867 and batch: 550, loss is 5.060094118118286 and perplexity is 157.60534914423067
At time: 765.4214539527893 and batch: 600, loss is 5.092862939834594 and perplexity is 162.8554406019761
At time: 766.2451627254486 and batch: 650, loss is 5.108641061782837 and perplexity is 165.44537200057633
At time: 767.0681474208832 and batch: 700, loss is 5.124631652832031 and perplexity is 168.11220659054487
At time: 767.9058630466461 and batch: 750, loss is 5.087827262878418 and perplexity is 162.03741459852375
At time: 768.7414939403534 and batch: 800, loss is 5.096654834747315 and perplexity is 163.47414360575664
At time: 769.5891997814178 and batch: 850, loss is 5.112962961196899 and perplexity is 166.16195764701382
At time: 770.4121761322021 and batch: 900, loss is 5.0936528587341305 and perplexity is 162.9841340143875
At time: 771.2586829662323 and batch: 950, loss is 5.095043754577636 and perplexity is 163.21098569590387
At time: 772.1121544837952 and batch: 1000, loss is 5.096878480911255 and perplexity is 163.51070805947498
At time: 772.9370565414429 and batch: 1050, loss is 5.088146924972534 and perplexity is 162.08922009748554
At time: 773.7612600326538 and batch: 1100, loss is 5.038138675689697 and perplexity is 154.18276364862868
At time: 774.5848524570465 and batch: 1150, loss is 5.074431591033935 and perplexity is 159.88128818885667
At time: 775.4095628261566 and batch: 1200, loss is 5.101929225921631 and perplexity is 164.3386480491719
At time: 776.239572763443 and batch: 1250, loss is 5.134461307525635 and perplexity is 169.77283988590642
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.145806751111998 and perplexity of 171.7099560055489
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 778.57191157341 and batch: 50, loss is 5.132046489715576 and perplexity is 169.3633640122848
At time: 779.4240083694458 and batch: 100, loss is 5.137348098754883 and perplexity is 170.26364671863604
At time: 780.2523806095123 and batch: 150, loss is 5.081286411285401 and perplexity is 160.9810105744095
At time: 781.1004581451416 and batch: 200, loss is 5.118322286605835 and perplexity is 167.05486419981983
At time: 781.9534313678741 and batch: 250, loss is 5.12076777458191 and perplexity is 167.4638947974218
At time: 782.8032355308533 and batch: 300, loss is 5.110916976928711 and perplexity is 165.82234043989936
At time: 783.626957654953 and batch: 350, loss is 5.122507028579712 and perplexity is 167.7554104823596
At time: 784.4482316970825 and batch: 400, loss is 5.0953592109680175 and perplexity is 163.26247976596176
At time: 785.2711367607117 and batch: 450, loss is 5.0614290523529055 and perplexity is 157.8158824131315
At time: 786.096212387085 and batch: 500, loss is 5.065919303894043 and perplexity is 158.52610877654502
At time: 786.9231488704681 and batch: 550, loss is 5.0600183486938475 and perplexity is 157.59340793003275
At time: 787.7462131977081 and batch: 600, loss is 5.092758445739746 and perplexity is 162.83842405919893
At time: 788.5682091712952 and batch: 650, loss is 5.1085881423950195 and perplexity is 165.4366169644304
At time: 789.3913631439209 and batch: 700, loss is 5.124667539596557 and perplexity is 168.11823970197057
At time: 790.2141606807709 and batch: 750, loss is 5.08788984298706 and perplexity is 162.04755523483124
At time: 791.0371870994568 and batch: 800, loss is 5.096592817306519 and perplexity is 163.46400567210148
At time: 791.8802959918976 and batch: 850, loss is 5.112842073440552 and perplexity is 166.1418719148476
At time: 792.724545955658 and batch: 900, loss is 5.0934657382965085 and perplexity is 162.9536392050953
At time: 793.5762317180634 and batch: 950, loss is 5.094898109436035 and perplexity is 163.18721653975436
At time: 794.4239387512207 and batch: 1000, loss is 5.096736469268799 and perplexity is 163.4874892839715
At time: 795.250293970108 and batch: 1050, loss is 5.088069353103638 and perplexity is 162.07664702141943
At time: 796.0749638080597 and batch: 1100, loss is 5.038008737564087 and perplexity is 154.16273073086683
At time: 796.9004402160645 and batch: 1150, loss is 5.0742902755737305 and perplexity is 159.85869608738236
At time: 797.7230243682861 and batch: 1200, loss is 5.10179033279419 and perplexity is 164.31582412546467
At time: 798.5475759506226 and batch: 1250, loss is 5.134353046417236 and perplexity is 169.75446108495697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.145778683850365 and perplexity of 171.70513664492213
Annealing...
Model not improving. Stopping early with 171.55802002745472loss at 35 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f106faa1128>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'seq_len': 35, 'num_layers': 1, 'data': 'wikitext', 'dropout': 0.7963113862382708, 'wordvec_source': '', 'lr': 0.08714782372523766, 'tune_wordvecs': True, 'batch_size': 50, 'anneal': 2.8334704573244425}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.3438844680786133 and batch: 50, loss is 9.969657535552978 and perplexity is 21368.166259994465
At time: 2.1673390865325928 and batch: 100, loss is 9.875712299346924 and perplexity is 19452.138650911955
At time: 2.9876723289489746 and batch: 150, loss is 9.770054855346679 and perplexity is 17501.72725534849
At time: 3.8117895126342773 and batch: 200, loss is 9.652113285064697 and perplexity is 15554.624751552503
At time: 4.636127710342407 and batch: 250, loss is 9.534010391235352 and perplexity is 13821.911354048481
At time: 5.4597578048706055 and batch: 300, loss is 9.364281482696533 and perplexity is 11664.221949930245
At time: 6.282185077667236 and batch: 350, loss is 9.178800754547119 and perplexity is 9689.525682836842
At time: 7.111965894699097 and batch: 400, loss is 8.93652738571167 and perplexity is 7604.742810992958
At time: 7.946172475814819 and batch: 450, loss is 8.68232587814331 and perplexity is 5897.748094533818
At time: 8.826736688613892 and batch: 500, loss is 8.455867156982421 and perplexity is 4702.5827450115075
At time: 9.654383659362793 and batch: 550, loss is 8.243052797317505 and perplexity is 3801.126683086758
At time: 10.482759237289429 and batch: 600, loss is 8.090445728302003 and perplexity is 3263.141720742994
At time: 11.308887720108032 and batch: 650, loss is 7.918332567214966 and perplexity is 2747.1864757648145
At time: 12.13174057006836 and batch: 700, loss is 7.802414960861206 and perplexity is 2446.5030584499186
At time: 12.953561544418335 and batch: 750, loss is 7.614936075210571 and perplexity is 2028.2650982181638
At time: 13.777650594711304 and batch: 800, loss is 7.5412211608886714 and perplexity is 1884.1294460842464
At time: 14.601279973983765 and batch: 850, loss is 7.521133527755738 and perplexity is 1846.6593477548865
At time: 15.423023462295532 and batch: 900, loss is 7.454586620330811 and perplexity is 1727.769622788974
At time: 16.247071743011475 and batch: 950, loss is 7.373298511505127 and perplexity is 1592.8792577170545
At time: 17.09291386604309 and batch: 1000, loss is 7.34541672706604 and perplexity is 1549.080374504731
At time: 17.916964530944824 and batch: 1050, loss is 7.282472629547119 and perplexity is 1454.580220231571
At time: 18.757790088653564 and batch: 1100, loss is 7.2468794822692875 and perplexity is 1403.717680736158
At time: 19.61201572418213 and batch: 1150, loss is 7.267242650985718 and perplexity is 1432.594838072669
At time: 20.450364351272583 and batch: 1200, loss is 7.244187545776367 and perplexity is 1399.9440433605205
At time: 21.27286410331726 and batch: 1250, loss is 7.190220355987549 and perplexity is 1326.3954525866811
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.457167576699361 and perplexity of 637.2535261963947
Finished 1 epochs...
Completing Train Step...
At time: 23.667409658432007 and batch: 50, loss is 6.790246381759643 and perplexity is 889.1326009006791
At time: 24.489610195159912 and batch: 100, loss is 6.73186333656311 and perplexity is 838.7086072639272
At time: 25.317338705062866 and batch: 150, loss is 6.613491640090943 and perplexity is 745.0800334735699
At time: 26.161651372909546 and batch: 200, loss is 6.559167680740356 and perplexity is 705.6840956303026
At time: 26.99084162712097 and batch: 250, loss is 6.584055500030518 and perplexity is 723.4674099233539
At time: 27.813615083694458 and batch: 300, loss is 6.535773677825928 and perplexity is 689.366925961927
At time: 28.638220071792603 and batch: 350, loss is 6.524556064605713 and perplexity is 681.677085882862
At time: 29.464841842651367 and batch: 400, loss is 6.452107906341553 and perplexity is 634.0373766090435
At time: 30.290782690048218 and batch: 450, loss is 6.423129482269287 and perplexity is 615.9276357920352
At time: 31.139514446258545 and batch: 500, loss is 6.399032735824585 and perplexity is 601.263176180148
At time: 31.974797010421753 and batch: 550, loss is 6.390610971450806 and perplexity is 596.2207422821268
At time: 32.79479694366455 and batch: 600, loss is 6.399842557907104 and perplexity is 601.7502895886003
At time: 33.62820291519165 and batch: 650, loss is 6.372571544647217 and perplexity is 585.561692472282
At time: 34.45869517326355 and batch: 700, loss is 6.354256172180175 and perplexity is 574.9345292727256
At time: 35.277827501297 and batch: 750, loss is 6.275659770965576 and perplexity is 531.4769191482368
At time: 36.097893476486206 and batch: 800, loss is 6.264820318222046 and perplexity is 525.7471102962704
At time: 36.98061513900757 and batch: 850, loss is 6.3038632202148435 and perplexity is 546.6797802993257
At time: 37.81748867034912 and batch: 900, loss is 6.284732818603516 and perplexity is 536.3209765054496
At time: 38.63874435424805 and batch: 950, loss is 6.246173925399781 and perplexity is 516.0346556815238
At time: 39.45870637893677 and batch: 1000, loss is 6.241788330078125 and perplexity is 513.776491825531
At time: 40.28181481361389 and batch: 1050, loss is 6.210665645599366 and perplexity is 498.0326542207406
At time: 41.13933300971985 and batch: 1100, loss is 6.185304002761841 and perplexity is 485.5605529231403
At time: 41.972445249557495 and batch: 1150, loss is 6.226366024017334 and perplexity is 505.91366086020605
At time: 42.80083990097046 and batch: 1200, loss is 6.224406433105469 and perplexity is 504.9232477673756
At time: 43.62473106384277 and batch: 1250, loss is 6.188105344772339 and perplexity is 486.9226811013523
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.754210089244982 and perplexity of 315.5162194085959
Finished 2 epochs...
Completing Train Step...
At time: 45.9070565700531 and batch: 50, loss is 6.164769039154053 and perplexity is 475.69126419022797
At time: 46.728968143463135 and batch: 100, loss is 6.183255414962769 and perplexity is 484.56685768220564
At time: 47.56608200073242 and batch: 150, loss is 6.093955430984497 and perplexity is 443.1708807450335
At time: 48.39031624794006 and batch: 200, loss is 6.099400320053101 and perplexity is 445.5904782714089
At time: 49.211774587631226 and batch: 250, loss is 6.14594033241272 and perplexity is 466.8184072206633
At time: 50.03630805015564 and batch: 300, loss is 6.1355673122406005 and perplexity is 462.0011185767482
At time: 50.859848499298096 and batch: 350, loss is 6.156257295608521 and perplexity is 471.6594852186714
At time: 51.68196702003479 and batch: 400, loss is 6.103058891296387 and perplexity is 447.2236885678782
At time: 52.506572008132935 and batch: 450, loss is 6.094698762893676 and perplexity is 443.50042626758756
At time: 53.32864809036255 and batch: 500, loss is 6.085180244445801 and perplexity is 439.2989867399331
At time: 54.150617599487305 and batch: 550, loss is 6.091621446609497 and perplexity is 442.13773297829636
At time: 54.97402548789978 and batch: 600, loss is 6.119761304855347 and perplexity is 454.7561334624854
At time: 55.794150829315186 and batch: 650, loss is 6.105516052246093 and perplexity is 448.32394034548537
At time: 56.61466717720032 and batch: 700, loss is 6.105911035537719 and perplexity is 448.5010557876752
At time: 57.437315464019775 and batch: 750, loss is 6.040963773727417 and perplexity is 420.29791183556745
At time: 58.30970788002014 and batch: 800, loss is 6.04000108718872 and perplexity is 419.8934913898942
At time: 59.14603090286255 and batch: 850, loss is 6.084530515670776 and perplexity is 439.0136542518108
At time: 59.98272728919983 and batch: 900, loss is 6.076648607254028 and perplexity is 435.56698982547306
At time: 60.80346059799194 and batch: 950, loss is 6.042195177078247 and perplexity is 420.81578688364317
At time: 61.62456750869751 and batch: 1000, loss is 6.040255975723267 and perplexity is 420.0005310675939
At time: 62.46515083312988 and batch: 1050, loss is 6.020719547271728 and perplexity is 411.8748525549366
At time: 63.306177854537964 and batch: 1100, loss is 5.999541091918945 and perplexity is 403.2436992331452
At time: 64.13065934181213 and batch: 1150, loss is 6.046284465789795 and perplexity is 422.5401474317135
At time: 64.95320963859558 and batch: 1200, loss is 6.051931209564209 and perplexity is 424.932872573754
At time: 65.77632355690002 and batch: 1250, loss is 6.021237306594848 and perplexity is 412.088159815952
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.598804334654425 and perplexity of 270.10326116815395
Finished 3 epochs...
Completing Train Step...
At time: 68.02026343345642 and batch: 50, loss is 6.00270489692688 and perplexity is 404.52150396483586
At time: 68.86996865272522 and batch: 100, loss is 6.0294978809356685 and perplexity is 415.5063433374094
At time: 69.6939709186554 and batch: 150, loss is 5.938631362915039 and perplexity is 379.415292182296
At time: 70.51451206207275 and batch: 200, loss is 5.951562042236328 and perplexity is 384.3532464033806
At time: 71.33756518363953 and batch: 250, loss is 6.001391773223877 and perplexity is 403.99066579397424
At time: 72.16100811958313 and batch: 300, loss is 5.996140384674073 and perplexity is 401.874714541131
At time: 72.98524951934814 and batch: 350, loss is 6.022362108230591 and perplexity is 412.5519380325346
At time: 73.80645275115967 and batch: 400, loss is 5.9741669940948485 and perplexity is 393.1404763150297
At time: 74.62976551055908 and batch: 450, loss is 5.966167154312134 and perplexity is 390.00796200476503
At time: 75.4548966884613 and batch: 500, loss is 5.958110551834107 and perplexity is 386.87844645076206
At time: 76.29093527793884 and batch: 550, loss is 5.968160696029663 and perplexity is 390.7862346489305
At time: 77.125643491745 and batch: 600, loss is 5.998930740356445 and perplexity is 402.99765390596315
At time: 77.94977974891663 and batch: 650, loss is 5.988640155792236 and perplexity is 398.8718374099762
At time: 78.81977415084839 and batch: 700, loss is 5.993470134735108 and perplexity is 400.80304006528206
At time: 79.64139175415039 and batch: 750, loss is 5.932865571975708 and perplexity is 377.2339575312545
At time: 80.46190071105957 and batch: 800, loss is 5.934099159240723 and perplexity is 377.69959568079344
At time: 81.28359460830688 and batch: 850, loss is 5.981045093536377 and perplexity is 395.8538563573942
At time: 82.10772609710693 and batch: 900, loss is 5.973492279052734 and perplexity is 392.8753079885848
At time: 82.92989110946655 and batch: 950, loss is 5.9403178501129155 and perplexity is 380.0557110925946
At time: 83.7529821395874 and batch: 1000, loss is 5.937461061477661 and perplexity is 378.97152164379054
At time: 84.57535815238953 and batch: 1050, loss is 5.922681760787964 and perplexity is 373.4117733682439
At time: 85.39594507217407 and batch: 1100, loss is 5.904025583267212 and perplexity is 366.50991833539956
At time: 86.22057676315308 and batch: 1150, loss is 5.951968441009521 and perplexity is 384.5094788353757
At time: 87.04310011863708 and batch: 1200, loss is 5.957875556945801 and perplexity is 386.7875426748301
At time: 87.86622285842896 and batch: 1250, loss is 5.9284427452087405 and perplexity is 375.5692012645102
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.515537234118385 and perplexity of 248.52345666955637
Finished 4 epochs...
Completing Train Step...
At time: 90.10833668708801 and batch: 50, loss is 5.91213210105896 and perplexity is 369.49311283201126
At time: 90.93077349662781 and batch: 100, loss is 5.941979169845581 and perplexity is 380.6876299092043
At time: 91.75402784347534 and batch: 150, loss is 5.849611492156982 and perplexity is 347.099503400651
At time: 92.57710599899292 and batch: 200, loss is 5.86436128616333 and perplexity is 352.2570927606119
At time: 93.40153241157532 and batch: 250, loss is 5.9156733417510985 and perplexity is 370.8038964088967
At time: 94.22138619422913 and batch: 300, loss is 5.912410926818848 and perplexity is 369.5961513942051
At time: 95.06373262405396 and batch: 350, loss is 5.941041383743286 and perplexity is 380.3307936847374
At time: 95.8936219215393 and batch: 400, loss is 5.895645961761475 and perplexity is 363.45153588146053
At time: 96.72325921058655 and batch: 450, loss is 5.887761287689209 and perplexity is 360.5971068889549
At time: 97.54843401908875 and batch: 500, loss is 5.8794999599456785 and perplexity is 357.630367482058
At time: 98.37072944641113 and batch: 550, loss is 5.890912857055664 and perplexity is 361.7353463628677
At time: 99.19261813163757 and batch: 600, loss is 5.923077478408813 and perplexity is 373.55956822738466
At time: 100.04913449287415 and batch: 650, loss is 5.9144192314147945 and perplexity is 370.339158886645
At time: 100.8717098236084 and batch: 700, loss is 5.920681676864624 and perplexity is 372.665664872072
At time: 101.69399905204773 and batch: 750, loss is 5.86292462348938 and perplexity is 351.7513814990374
At time: 102.51610779762268 and batch: 800, loss is 5.864949007034301 and perplexity is 352.4641824554819
At time: 103.34008240699768 and batch: 850, loss is 5.91272050857544 and perplexity is 369.7105893330184
At time: 104.1639347076416 and batch: 900, loss is 5.905690660476685 and perplexity is 367.12069400067196
At time: 104.98567724227905 and batch: 950, loss is 5.87208966255188 and perplexity is 354.99001508256794
At time: 105.81501889228821 and batch: 1000, loss is 5.8680640316009525 and perplexity is 353.5638288662972
At time: 106.64903807640076 and batch: 1050, loss is 5.855414962768554 and perplexity is 349.1197416945335
At time: 107.47455048561096 and batch: 1100, loss is 5.838923625946045 and perplexity is 343.4095045790795
At time: 108.2995069026947 and batch: 1150, loss is 5.8874563789367675 and perplexity is 360.4871744354948
At time: 109.12533283233643 and batch: 1200, loss is 5.892634553909302 and perplexity is 362.35868141357173
At time: 109.95103883743286 and batch: 1250, loss is 5.863952836990356 and perplexity is 352.1132430220529
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.457042833314325 and perplexity of 234.40322902556872
Finished 5 epochs...
Completing Train Step...
At time: 112.23045992851257 and batch: 50, loss is 5.848778448104858 and perplexity is 346.81047462741276
At time: 113.05746221542358 and batch: 100, loss is 5.879278602600098 and perplexity is 357.5512121343452
At time: 113.88160133361816 and batch: 150, loss is 5.786299505233765 and perplexity is 325.8051506343574
At time: 114.70558333396912 and batch: 200, loss is 5.801967630386352 and perplexity is 330.9501071687165
At time: 115.53380680084229 and batch: 250, loss is 5.85330509185791 and perplexity is 348.38392062389914
At time: 116.35955500602722 and batch: 300, loss is 5.851844778060913 and perplexity is 347.8755420644162
At time: 117.18565559387207 and batch: 350, loss is 5.881840829849243 and perplexity is 358.46851425944965
At time: 118.0092945098877 and batch: 400, loss is 5.838083076477051 and perplexity is 343.1209731818023
At time: 118.85947847366333 and batch: 450, loss is 5.830017604827881 and perplexity is 340.36467108045554
At time: 119.69085264205933 and batch: 500, loss is 5.821218585968017 and perplexity is 337.38293334151956
At time: 120.51332807540894 and batch: 550, loss is 5.833770093917846 and perplexity is 341.6442851626974
At time: 121.39085721969604 and batch: 600, loss is 5.866609802246094 and perplexity is 353.050039641776
At time: 122.21458125114441 and batch: 650, loss is 5.858737363815307 and perplexity is 350.28158647820715
At time: 123.04362320899963 and batch: 700, loss is 5.865209245681763 and perplexity is 352.55591919380737
At time: 123.87153768539429 and batch: 750, loss is 5.80976188659668 and perplexity is 333.5396959461574
At time: 124.70824003219604 and batch: 800, loss is 5.81225830078125 and perplexity is 334.3733893633001
At time: 125.5398428440094 and batch: 850, loss is 5.861289272308349 and perplexity is 351.176614562863
At time: 126.37073159217834 and batch: 900, loss is 5.854134540557862 and perplexity is 348.67300708856686
At time: 127.19670534133911 and batch: 950, loss is 5.820036268234253 and perplexity is 336.98427523307555
At time: 128.01945328712463 and batch: 1000, loss is 5.814896125793457 and perplexity is 335.25657218193606
At time: 128.84366297721863 and batch: 1050, loss is 5.803467903137207 and perplexity is 331.44699523700837
At time: 129.6684386730194 and batch: 1100, loss is 5.7885080909729005 and perplexity is 326.52551444358454
At time: 130.49443531036377 and batch: 1150, loss is 5.8372110176086425 and perplexity is 342.821881925748
At time: 131.31906819343567 and batch: 1200, loss is 5.842091264724732 and perplexity is 344.49902653774126
At time: 132.14462208747864 and batch: 1250, loss is 5.814125127792359 and perplexity is 334.9981896539345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.412503207687044 and perplexity of 224.19208521809063
Finished 6 epochs...
Completing Train Step...
At time: 134.42544531822205 and batch: 50, loss is 5.799092025756836 and perplexity is 329.99979252784743
At time: 135.27785730361938 and batch: 100, loss is 5.8297467613220215 and perplexity is 340.2724980024727
At time: 136.100741147995 and batch: 150, loss is 5.7359089851379395 and perplexity is 309.79444135654114
At time: 136.92540740966797 and batch: 200, loss is 5.752862882614136 and perplexity is 315.09144006269156
At time: 137.74913787841797 and batch: 250, loss is 5.803575639724731 and perplexity is 331.4827061288706
At time: 138.57398438453674 and batch: 300, loss is 5.80380407333374 and perplexity is 331.55843656911526
At time: 139.39826321601868 and batch: 350, loss is 5.834247484207153 and perplexity is 341.8074217636475
At time: 140.22185635566711 and batch: 400, loss is 5.791785001754761 and perplexity is 327.5972644779884
At time: 141.04658842086792 and batch: 450, loss is 5.78329628944397 and perplexity is 324.82815525986166
At time: 141.87099313735962 and batch: 500, loss is 5.774212579727173 and perplexity is 321.8908715033102
At time: 142.72433018684387 and batch: 550, loss is 5.787231073379517 and perplexity is 326.10880174830345
At time: 143.54977989196777 and batch: 600, loss is 5.820506906509399 and perplexity is 337.1429102580518
At time: 144.37442421913147 and batch: 650, loss is 5.813323268890381 and perplexity is 334.729676042816
At time: 145.19561457633972 and batch: 700, loss is 5.8198490238189695 and perplexity is 336.92118271654493
At time: 146.01565217971802 and batch: 750, loss is 5.766571130752563 and perplexity is 319.4405328255969
At time: 146.8388319015503 and batch: 800, loss is 5.76976897239685 and perplexity is 320.46368813675605
At time: 147.6630413532257 and batch: 850, loss is 5.818396816253662 and perplexity is 336.4322583218687
At time: 148.48743057250977 and batch: 900, loss is 5.811571741104126 and perplexity is 334.1439008648493
At time: 149.31265211105347 and batch: 950, loss is 5.777271127700805 and perplexity is 322.8768973110296
At time: 150.1362087726593 and batch: 1000, loss is 5.771280708312989 and perplexity is 320.9485109736461
At time: 150.96075439453125 and batch: 1050, loss is 5.760735607147216 and perplexity is 317.58185849924365
At time: 151.78357100486755 and batch: 1100, loss is 5.746934003829956 and perplexity is 313.2288281648558
At time: 152.60838794708252 and batch: 1150, loss is 5.795732631683349 and perplexity is 328.893053208178
At time: 153.434015750885 and batch: 1200, loss is 5.800525102615357 and perplexity is 330.47304661758403
At time: 154.2559690475464 and batch: 1250, loss is 5.772864112854004 and perplexity is 321.4571048520211
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.375807267906022 and perplexity of 216.11426402010534
Finished 7 epochs...
Completing Train Step...
At time: 156.5400698184967 and batch: 50, loss is 5.758065338134766 and perplexity is 316.73496072923473
At time: 157.36783027648926 and batch: 100, loss is 5.7887179565429685 and perplexity is 326.59404809799065
At time: 158.19274401664734 and batch: 150, loss is 5.694406623840332 and perplexity is 297.200389628483
At time: 159.01439023017883 and batch: 200, loss is 5.711864948272705 and perplexity is 302.43456749528303
At time: 159.84659600257874 and batch: 250, loss is 5.761847524642945 and perplexity is 317.9351797196554
At time: 160.6777319908142 and batch: 300, loss is 5.763590269088745 and perplexity is 318.4897425785495
At time: 161.50332188606262 and batch: 350, loss is 5.794144372940064 and perplexity is 328.37110054847363
At time: 162.32776308059692 and batch: 400, loss is 5.75265588760376 and perplexity is 315.02622445667186
At time: 163.20116472244263 and batch: 450, loss is 5.7437754821777345 and perplexity is 312.24104891127206
At time: 164.02467584609985 and batch: 500, loss is 5.734400215148926 and perplexity is 309.3273852293932
At time: 164.85044741630554 and batch: 550, loss is 5.747933864593506 and perplexity is 313.5421700031508
At time: 165.67446637153625 and batch: 600, loss is 5.781588678359985 and perplexity is 324.2739484210892
At time: 166.49957156181335 and batch: 650, loss is 5.774480133056641 and perplexity is 321.977005999978
At time: 167.32223773002625 and batch: 700, loss is 5.78116096496582 and perplexity is 324.13528176687726
At time: 168.14676976203918 and batch: 750, loss is 5.729866056442261 and perplexity is 307.92802064042615
At time: 168.9729781150818 and batch: 800, loss is 5.734396448135376 and perplexity is 309.3262199911364
At time: 169.79903173446655 and batch: 850, loss is 5.781940565109253 and perplexity is 324.3880762055213
At time: 170.66033172607422 and batch: 900, loss is 5.7752671051025395 and perplexity is 322.2304926334869
At time: 171.4957618713379 and batch: 950, loss is 5.740895290374755 and perplexity is 311.34302865772503
At time: 172.32124066352844 and batch: 1000, loss is 5.734257755279541 and perplexity is 309.28332162921794
At time: 173.14533734321594 and batch: 1050, loss is 5.724224195480347 and perplexity is 306.19562512427467
At time: 173.9693489074707 and batch: 1100, loss is 5.711384668350219 and perplexity is 302.2893491201758
At time: 174.7927918434143 and batch: 1150, loss is 5.759860715866089 and perplexity is 317.304130408856
At time: 175.6160750389099 and batch: 1200, loss is 5.765149574279786 and perplexity is 318.98675268195876
At time: 176.44435596466064 and batch: 1250, loss is 5.737520618438721 and perplexity is 310.2941189349871
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.3448931840214415 and perplexity of 209.5355016638114
Finished 8 epochs...
Completing Train Step...
At time: 178.69573831558228 and batch: 50, loss is 5.722704067230224 and perplexity is 305.73052210214377
At time: 179.54569745063782 and batch: 100, loss is 5.753590908050537 and perplexity is 315.3209181686629
At time: 180.3689033985138 and batch: 150, loss is 5.658787088394165 and perplexity is 286.8005677695451
At time: 181.19405388832092 and batch: 200, loss is 5.676732187271118 and perplexity is 291.9936884890401
At time: 182.01862382888794 and batch: 250, loss is 5.725769472122193 and perplexity is 306.66914783918355
At time: 182.85785746574402 and batch: 300, loss is 5.728883228302002 and perplexity is 307.62552898956915
At time: 183.7011308670044 and batch: 350, loss is 5.7593135643005375 and perplexity is 317.13056444492145
At time: 184.5868330001831 and batch: 400, loss is 5.7188733863830565 and perplexity is 304.56160634772874
At time: 185.4340536594391 and batch: 450, loss is 5.709442958831787 and perplexity is 301.70296049609027
At time: 186.2718448638916 and batch: 500, loss is 5.6997079753875735 and perplexity is 298.78013707234857
At time: 187.1267340183258 and batch: 550, loss is 5.713857364654541 and perplexity is 303.0377437716043
At time: 187.96905303001404 and batch: 600, loss is 5.747520275115967 and perplexity is 313.41251907385094
At time: 188.7969946861267 and batch: 650, loss is 5.740433864593506 and perplexity is 311.1994000969988
At time: 189.62667059898376 and batch: 700, loss is 5.747115650177002 and perplexity is 313.28573020514875
At time: 190.4580419063568 and batch: 750, loss is 5.697783241271972 and perplexity is 298.2056178252227
At time: 191.29209661483765 and batch: 800, loss is 5.702694244384766 and perplexity is 299.67370849091895
At time: 192.11804866790771 and batch: 850, loss is 5.749843626022339 and perplexity is 314.1415328836252
At time: 192.94265842437744 and batch: 900, loss is 5.743470830917358 and perplexity is 312.1459387706278
At time: 193.7655746936798 and batch: 950, loss is 5.7092467784881595 and perplexity is 301.64377811102673
At time: 194.59732747077942 and batch: 1000, loss is 5.701968250274658 and perplexity is 299.45622609862056
At time: 195.42163133621216 and batch: 1050, loss is 5.692245655059814 and perplexity is 296.5588422964733
At time: 196.2470943927765 and batch: 1100, loss is 5.680185823440552 and perplexity is 293.0038718516631
At time: 197.06978583335876 and batch: 1150, loss is 5.728355169296265 and perplexity is 307.46312744116534
At time: 197.89375495910645 and batch: 1200, loss is 5.733952789306641 and perplexity is 309.1890151210045
At time: 198.72015237808228 and batch: 1250, loss is 5.706300821304321 and perplexity is 300.7564561039916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.318337990419708 and perplexity of 204.04447614904817
Finished 9 epochs...
Completing Train Step...
At time: 201.00905513763428 and batch: 50, loss is 5.691738071441651 and perplexity is 296.4083520827167
At time: 201.83361792564392 and batch: 100, loss is 5.722946586608887 and perplexity is 305.8046766699986
At time: 202.65797686576843 and batch: 150, loss is 5.627543239593506 and perplexity is 277.97835184392386
At time: 203.48410892486572 and batch: 200, loss is 5.646041011810302 and perplexity is 283.16818426465943
At time: 204.3122158050537 and batch: 250, loss is 5.694145383834839 and perplexity is 297.1227591376004
At time: 205.13985586166382 and batch: 300, loss is 5.698392295837403 and perplexity is 298.3872966387354
At time: 206.01600980758667 and batch: 350, loss is 5.728516416549683 and perplexity is 307.5127090233321
At time: 206.8411602973938 and batch: 400, loss is 5.689032564163208 and perplexity is 295.60750097152453
At time: 207.66645574569702 and batch: 450, loss is 5.678917312622071 and perplexity is 292.63242890986544
At time: 208.4918303489685 and batch: 500, loss is 5.668863458633423 and perplexity is 289.70508539733083
At time: 209.31667017936707 and batch: 550, loss is 5.683636770248413 and perplexity is 294.01675933312197
At time: 210.14338636398315 and batch: 600, loss is 5.716899738311768 and perplexity is 303.96110170811204
At time: 210.97674536705017 and batch: 650, loss is 5.710080327987671 and perplexity is 301.8953179521921
At time: 211.80759406089783 and batch: 700, loss is 5.7168179130554195 and perplexity is 303.9362310305835
At time: 212.63244485855103 and batch: 750, loss is 5.669203510284424 and perplexity is 289.80361684186437
At time: 213.45965957641602 and batch: 800, loss is 5.67428225517273 and perplexity is 291.27919936138153
At time: 214.28365468978882 and batch: 850, loss is 5.721222877502441 and perplexity is 305.2780124024268
At time: 215.10871815681458 and batch: 900, loss is 5.715237817764282 and perplexity is 303.45636204233114
At time: 215.93600702285767 and batch: 950, loss is 5.681147623062134 and perplexity is 293.2858184310539
At time: 216.7634298801422 and batch: 1000, loss is 5.673269271850586 and perplexity is 290.984287785813
At time: 217.58809614181519 and batch: 1050, loss is 5.663858327865601 and perplexity is 288.25869626368853
At time: 218.4121117591858 and batch: 1100, loss is 5.652288436889648 and perplexity is 284.9427938790515
At time: 219.24139738082886 and batch: 1150, loss is 5.699617366790772 and perplexity is 298.7530662498194
At time: 220.06926918029785 and batch: 1200, loss is 5.705974035263061 and perplexity is 300.65818914932703
At time: 220.89887619018555 and batch: 1250, loss is 5.678241529464722 and perplexity is 292.4347396481742
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.295134384266651 and perplexity of 199.3644156072651
Finished 10 epochs...
Completing Train Step...
At time: 223.15436148643494 and batch: 50, loss is 5.6639824390411375 and perplexity is 288.2944746095409
At time: 224.02373886108398 and batch: 100, loss is 5.695767240524292 and perplexity is 297.60504066213787
At time: 224.8443796634674 and batch: 150, loss is 5.599716529846192 and perplexity is 270.3497604749205
At time: 225.66590070724487 and batch: 200, loss is 5.618584604263305 and perplexity is 275.49916679843756
At time: 226.48562932014465 and batch: 250, loss is 5.666034126281739 and perplexity is 288.8865718961583
At time: 227.353355884552 and batch: 300, loss is 5.671163311004639 and perplexity is 290.3721310842194
At time: 228.17561745643616 and batch: 350, loss is 5.70074535369873 and perplexity is 299.0902459286423
At time: 229.00034737586975 and batch: 400, loss is 5.662167911529541 and perplexity is 287.77183067322204
At time: 229.82327437400818 and batch: 450, loss is 5.6515185737609865 and perplexity is 284.7235113478644
At time: 230.66026663780212 and batch: 500, loss is 5.640994348526001 and perplexity is 281.74272970582615
At time: 231.49860453605652 and batch: 550, loss is 5.6562354850769045 and perplexity is 286.0696993291381
At time: 232.32331466674805 and batch: 600, loss is 5.6891061973571775 and perplexity is 295.6292682973715
At time: 233.14702939987183 and batch: 650, loss is 5.682791337966919 and perplexity is 293.76829311896
At time: 233.97548842430115 and batch: 700, loss is 5.689236211776733 and perplexity is 295.6677068638227
At time: 234.80043578147888 and batch: 750, loss is 5.64330530166626 and perplexity is 282.39457685579464
At time: 235.62795758247375 and batch: 800, loss is 5.648814859390259 and perplexity is 283.9547400358856
At time: 236.46283888816833 and batch: 850, loss is 5.695520715713501 and perplexity is 297.53168267845126
At time: 237.28722834587097 and batch: 900, loss is 5.68984543800354 and perplexity is 295.847890265918
At time: 238.11222553253174 and batch: 950, loss is 5.655788125991822 and perplexity is 285.9417520714942
At time: 238.9435272216797 and batch: 1000, loss is 5.647343368530273 and perplexity is 283.5372105020587
At time: 239.77058863639832 and batch: 1050, loss is 5.638337154388427 and perplexity is 280.9950783434669
At time: 240.60270404815674 and batch: 1100, loss is 5.626834239959717 and perplexity is 277.78133514490185
At time: 241.43703436851501 and batch: 1150, loss is 5.673432874679565 and perplexity is 291.0318975329218
At time: 242.2618691921234 and batch: 1200, loss is 5.680369215011597 and perplexity is 293.0576112195671
At time: 243.0864245891571 and batch: 1250, loss is 5.652720308303833 and perplexity is 285.0658791029874
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.274238976248859 and perplexity of 195.24183628572342
Finished 11 epochs...
Completing Train Step...
At time: 245.37789607048035 and batch: 50, loss is 5.638950777053833 and perplexity is 281.1675562051566
At time: 246.25832605361938 and batch: 100, loss is 5.671234760284424 and perplexity is 290.3928787050475
At time: 247.116694688797 and batch: 150, loss is 5.5742459392547605 and perplexity is 263.5507474166165
At time: 248.00629472732544 and batch: 200, loss is 5.593616676330567 and perplexity is 268.7056859426999
At time: 248.83514142036438 and batch: 250, loss is 5.640561628341675 and perplexity is 281.62084031379726
At time: 249.6603262424469 and batch: 300, loss is 5.646429786682129 and perplexity is 283.27829434183093
At time: 250.48414874076843 and batch: 350, loss is 5.675464744567871 and perplexity is 291.6238376510545
At time: 251.3071324825287 and batch: 400, loss is 5.637556858062744 and perplexity is 280.77590443771777
At time: 252.1323585510254 and batch: 450, loss is 5.62657904624939 and perplexity is 277.71045613964947
At time: 252.95984625816345 and batch: 500, loss is 5.615596981048584 and perplexity is 274.67730740649233
At time: 253.78545379638672 and batch: 550, loss is 5.631568145751953 and perplexity is 279.0994432545761
At time: 254.60925698280334 and batch: 600, loss is 5.663948926925659 and perplexity is 288.2848134137005
At time: 255.43950939178467 and batch: 650, loss is 5.658032331466675 and perplexity is 286.58418472278674
At time: 256.2780191898346 and batch: 700, loss is 5.664066305160523 and perplexity is 288.31865376225824
At time: 257.1265859603882 and batch: 750, loss is 5.619446697235108 and perplexity is 275.7367750993791
At time: 257.9558596611023 and batch: 800, loss is 5.625486555099488 and perplexity is 277.4072255925859
At time: 258.7814431190491 and batch: 850, loss is 5.672139616012573 and perplexity is 290.65576128222375
At time: 259.6083493232727 and batch: 900, loss is 5.6664920425415035 and perplexity is 289.0188880472037
At time: 260.43288493156433 and batch: 950, loss is 5.632589445114136 and perplexity is 279.3846329451931
At time: 261.2581157684326 and batch: 1000, loss is 5.623704919815063 and perplexity is 276.9134271063381
At time: 262.0791094303131 and batch: 1050, loss is 5.615014600753784 and perplexity is 274.51738732690166
At time: 262.9048490524292 and batch: 1100, loss is 5.603420848846436 and perplexity is 271.3530793887238
At time: 263.731703042984 and batch: 1150, loss is 5.64931806564331 and perplexity is 284.09766379370365
At time: 264.5563802719116 and batch: 1200, loss is 5.656880741119385 and perplexity is 286.2543470973382
At time: 265.38190722465515 and batch: 1250, loss is 5.629289007186889 and perplexity is 278.4640612867794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.254969241845346 and perplexity of 191.51559509749057
Finished 12 epochs...
Completing Train Step...
At time: 267.6580994129181 and batch: 50, loss is 5.616173753738403 and perplexity is 274.83577947270425
At time: 268.4812982082367 and batch: 100, loss is 5.648719329833984 and perplexity is 283.9276152611972
At time: 269.33265447616577 and batch: 150, loss is 5.550711183547974 and perplexity is 257.42056409194805
At time: 270.1693959236145 and batch: 200, loss is 5.570958032608032 and perplexity is 262.6856401377624
At time: 271.0071325302124 and batch: 250, loss is 5.617250585556031 and perplexity is 275.131890787092
At time: 271.8333647251129 and batch: 300, loss is 5.623725080490113 and perplexity is 276.9190099242353
At time: 272.6736605167389 and batch: 350, loss is 5.6523136806488035 and perplexity is 284.94998699710345
At time: 273.5278551578522 and batch: 400, loss is 5.614839153289795 and perplexity is 274.4692281722986
At time: 274.3796353340149 and batch: 450, loss is 5.603653993606567 and perplexity is 271.41635131280105
At time: 275.2054784297943 and batch: 500, loss is 5.592216739654541 and perplexity is 268.3297781828147
At time: 276.0295960903168 and batch: 550, loss is 5.608395776748657 and perplexity is 272.706404952059
At time: 276.8615574836731 and batch: 600, loss is 5.640463171005249 and perplexity is 281.59311404092784
At time: 277.68954849243164 and batch: 650, loss is 5.635153789520263 and perplexity is 280.1019907489759
At time: 278.5144326686859 and batch: 700, loss is 5.64084231376648 and perplexity is 281.6998982736842
At time: 279.33927488327026 and batch: 750, loss is 5.597231531143189 and perplexity is 269.67877571420297
At time: 280.1635546684265 and batch: 800, loss is 5.60411416053772 and perplexity is 271.5412768833531
At time: 280.9858386516571 and batch: 850, loss is 5.650920734405518 and perplexity is 284.5533432990001
At time: 281.80731415748596 and batch: 900, loss is 5.644890947341919 and perplexity is 282.84270979154746
At time: 282.63171553611755 and batch: 950, loss is 5.611215839385986 and perplexity is 273.4765394990647
At time: 283.4546694755554 and batch: 1000, loss is 5.601860723495483 and perplexity is 270.9300646343327
At time: 284.28032660484314 and batch: 1050, loss is 5.593662967681885 and perplexity is 268.7181249799168
At time: 285.1070592403412 and batch: 1100, loss is 5.5818177318573 and perplexity is 265.55387307204455
At time: 285.9351375102997 and batch: 1150, loss is 5.62705738067627 and perplexity is 277.8433263871991
At time: 286.7615990638733 and batch: 1200, loss is 5.635235977172852 and perplexity is 280.1250126201247
At time: 287.5878195762634 and batch: 1250, loss is 5.607446565628051 and perplexity is 272.44767181542994
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.236830662636861 and perplexity of 188.07308976776852
Finished 13 epochs...
Completing Train Step...
At time: 289.8650093078613 and batch: 50, loss is 5.595136604309082 and perplexity is 269.11440976945477
At time: 290.7229497432709 and batch: 100, loss is 5.627825326919556 and perplexity is 278.056777074757
At time: 291.5458149909973 and batch: 150, loss is 5.529008560180664 and perplexity is 251.89404939177322
At time: 292.36894369125366 and batch: 200, loss is 5.5499864673614505 and perplexity is 257.2340748264635
At time: 293.19221115112305 and batch: 250, loss is 5.5958247756958 and perplexity is 269.2996703442137
At time: 294.01545453071594 and batch: 300, loss is 5.602748947143555 and perplexity is 271.17081803031596
At time: 294.83723068237305 and batch: 350, loss is 5.630913639068604 and perplexity is 278.9168305707768
At time: 295.6601004600525 and batch: 400, loss is 5.593949718475342 and perplexity is 268.7951911643377
At time: 296.48342418670654 and batch: 450, loss is 5.5823717212677 and perplexity is 265.7010278629515
At time: 297.31101417541504 and batch: 500, loss is 5.570587301254273 and perplexity is 262.58827238453574
At time: 298.136830329895 and batch: 550, loss is 5.587067785263062 and perplexity is 266.9517112395724
At time: 298.95964527130127 and batch: 600, loss is 5.618850393295288 and perplexity is 275.57240118731045
At time: 299.78020429611206 and batch: 650, loss is 5.614090394973755 and perplexity is 274.26379397508714
At time: 300.60150170326233 and batch: 700, loss is 5.619343328475952 and perplexity is 275.7082740041703
At time: 301.43290090560913 and batch: 750, loss is 5.576528816223145 and perplexity is 264.1530886217334
At time: 302.28190565109253 and batch: 800, loss is 5.584483556747436 and perplexity is 266.26273763114204
At time: 303.1026818752289 and batch: 850, loss is 5.631353006362915 and perplexity is 279.0394044294631
At time: 303.94359397888184 and batch: 900, loss is 5.624842014312744 and perplexity is 277.22848293081
At time: 304.76427960395813 and batch: 950, loss is 5.591357021331787 and perplexity is 268.0991892909209
At time: 305.5868263244629 and batch: 1000, loss is 5.581610822677613 and perplexity is 265.49893322198005
At time: 306.4101424217224 and batch: 1050, loss is 5.5739097404479985 and perplexity is 263.4621568626675
At time: 307.2393879890442 and batch: 1100, loss is 5.5618151092529295 and perplexity is 260.29487142532594
At time: 308.0696520805359 and batch: 1150, loss is 5.606467151641846 and perplexity is 272.1809633854482
At time: 308.89211106300354 and batch: 1200, loss is 5.6153374290466305 and perplexity is 274.6060236127897
At time: 309.7197177410126 and batch: 1250, loss is 5.5873117351531985 and perplexity is 267.0168420242019
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.220200141851049 and perplexity of 184.9712008447849
Finished 14 epochs...
Completing Train Step...
At time: 311.99990916252136 and batch: 50, loss is 5.575390853881836 and perplexity is 263.8526633232993
At time: 312.8242943286896 and batch: 100, loss is 5.608442239761352 and perplexity is 272.71907600757976
At time: 313.650351524353 and batch: 150, loss is 5.508929519653321 and perplexity is 246.88669820164162
At time: 314.4768624305725 and batch: 200, loss is 5.530613479614257 and perplexity is 252.29864363058195
At time: 315.2989594936371 and batch: 250, loss is 5.575989046096802 and perplexity is 264.010545149529
At time: 316.12398886680603 and batch: 300, loss is 5.58322919845581 and perplexity is 265.9289581417429
At time: 316.94932770729065 and batch: 350, loss is 5.611167106628418 and perplexity is 273.46321255789667
At time: 317.7813973426819 and batch: 400, loss is 5.5746891307830815 and perplexity is 263.66757676213
At time: 318.6143455505371 and batch: 450, loss is 5.562577466964722 and perplexity is 260.493384887417
At time: 319.44349217414856 and batch: 500, loss is 5.5506690406799315 and perplexity is 257.4097158796731
At time: 320.2838361263275 and batch: 550, loss is 5.5672572803497316 and perplexity is 261.71530225929075
At time: 321.12780928611755 and batch: 600, loss is 5.598827447891235 and perplexity is 270.10950420094053
At time: 321.9990065097809 and batch: 650, loss is 5.594579420089722 and perplexity is 268.96450523322795
At time: 322.85152077674866 and batch: 700, loss is 5.599342498779297 and perplexity is 270.24866017408067
At time: 323.68507385253906 and batch: 750, loss is 5.557307262420654 and perplexity is 259.1241427290361
At time: 324.51017236709595 and batch: 800, loss is 5.566200876235962 and perplexity is 261.43897112172147
At time: 325.3365194797516 and batch: 850, loss is 5.613105182647705 and perplexity is 273.99371896713257
At time: 326.1607358455658 and batch: 900, loss is 5.606195068359375 and perplexity is 272.1069175692752
At time: 326.985652923584 and batch: 950, loss is 5.572803955078125 and perplexity is 263.1709852804009
At time: 327.8090019226074 and batch: 1000, loss is 5.56283055305481 and perplexity is 260.55932048303094
At time: 328.6499710083008 and batch: 1050, loss is 5.555555038452148 and perplexity is 258.67049675607
At time: 329.48463582992554 and batch: 1100, loss is 5.543202486038208 and perplexity is 255.49490956949168
At time: 330.3093454837799 and batch: 1150, loss is 5.587317571640015 and perplexity is 267.01840046902805
At time: 331.1548156738281 and batch: 1200, loss is 5.59678318977356 and perplexity is 269.55789466255237
At time: 332.0032088756561 and batch: 1250, loss is 5.568531503677368 and perplexity is 262.0489985592836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.205099955092382 and perplexity of 182.1990835848653
Finished 15 epochs...
Completing Train Step...
At time: 334.3307316303253 and batch: 50, loss is 5.5574655437469485 and perplexity is 259.1651604881092
At time: 335.1967623233795 and batch: 100, loss is 5.590403127670288 and perplexity is 267.8435731083137
At time: 336.02189588546753 and batch: 150, loss is 5.490285110473633 and perplexity is 242.32628677217795
At time: 336.8708083629608 and batch: 200, loss is 5.512534685134888 and perplexity is 247.7783719537994
At time: 337.7019760608673 and batch: 250, loss is 5.557551488876343 and perplexity is 259.1874354285596
At time: 338.5456886291504 and batch: 300, loss is 5.565057182312012 and perplexity is 261.14013587956504
At time: 339.3980977535248 and batch: 350, loss is 5.592720718383789 and perplexity is 268.46504476632003
At time: 340.2408254146576 and batch: 400, loss is 5.556810445785523 and perplexity is 258.99543751845636
At time: 341.06830525398254 and batch: 450, loss is 5.544113397598267 and perplexity is 255.727748868059
At time: 341.89784574508667 and batch: 500, loss is 5.5321455001831055 and perplexity is 252.68546657681262
At time: 342.7269678115845 and batch: 550, loss is 5.548877058029174 and perplexity is 256.9488551851913
At time: 343.5514986515045 and batch: 600, loss is 5.5801282787323 and perplexity is 265.1056110168131
At time: 344.3758912086487 and batch: 650, loss is 5.576323280334472 and perplexity is 264.09880126110954
At time: 345.1993999481201 and batch: 700, loss is 5.580568389892578 and perplexity is 265.22231263383236
At time: 346.02384757995605 and batch: 750, loss is 5.5393260478973385 and perplexity is 254.5064165111566
At time: 346.84915494918823 and batch: 800, loss is 5.549220666885376 and perplexity is 257.0371602577575
At time: 347.6719033718109 and batch: 850, loss is 5.596002197265625 and perplexity is 269.3474541532932
At time: 348.49379229545593 and batch: 900, loss is 5.588714275360108 and perplexity is 267.3916066308433
At time: 349.31733202934265 and batch: 950, loss is 5.555407600402832 and perplexity is 258.63236169396134
At time: 350.14302229881287 and batch: 1000, loss is 5.545217962265014 and perplexity is 256.01037276321495
At time: 350.96929931640625 and batch: 1050, loss is 5.538440399169922 and perplexity is 254.28111301185962
At time: 351.7940003871918 and batch: 1100, loss is 5.525756025314331 and perplexity is 251.0760861615691
At time: 352.61966490745544 and batch: 1150, loss is 5.569391202926636 and perplexity is 262.27437875232647
At time: 353.44456481933594 and batch: 1200, loss is 5.579368991851807 and perplexity is 264.90439620397365
At time: 354.32187056541443 and batch: 1250, loss is 5.5509052658081055 and perplexity is 257.4705297053941
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.191191067660812 and perplexity of 179.68243947274752
Finished 16 epochs...
Completing Train Step...
At time: 356.5830008983612 and batch: 50, loss is 5.540350093841552 and perplexity is 254.76717626695884
At time: 357.4690158367157 and batch: 100, loss is 5.57341609954834 and perplexity is 263.33213326165196
At time: 358.30790734291077 and batch: 150, loss is 5.472807970046997 and perplexity is 238.12791089732787
At time: 359.13544249534607 and batch: 200, loss is 5.495616340637207 and perplexity is 243.62163381021588
At time: 359.9613482952118 and batch: 250, loss is 5.540300941467285 and perplexity is 254.75465416310797
At time: 360.79867148399353 and batch: 300, loss is 5.547978076934815 and perplexity is 256.7179668198498
At time: 361.6350221633911 and batch: 350, loss is 5.575354042053223 and perplexity is 263.84295060305055
At time: 362.4590184688568 and batch: 400, loss is 5.540039443969727 and perplexity is 254.68804516797542
At time: 363.28657960891724 and batch: 450, loss is 5.526718435287475 and perplexity is 251.31784060569288
At time: 364.11093187332153 and batch: 500, loss is 5.514849977493286 and perplexity is 248.35271595553152
At time: 364.9593358039856 and batch: 550, loss is 5.531739711761475 and perplexity is 252.58295054147817
At time: 365.8000822067261 and batch: 600, loss is 5.562696294784546 and perplexity is 260.52434058758445
At time: 366.64047408103943 and batch: 650, loss is 5.5592359352111815 and perplexity is 259.6243906647098
At time: 367.4987452030182 and batch: 700, loss is 5.562973394393921 and perplexity is 260.59654178359347
At time: 368.3268139362335 and batch: 750, loss is 5.522528133392334 and perplexity is 250.26694630180486
At time: 369.17561411857605 and batch: 800, loss is 5.533197355270386 and perplexity is 252.95139490482703
At time: 370.03426480293274 and batch: 850, loss is 5.579883604049683 and perplexity is 265.04075432029697
At time: 370.87471103668213 and batch: 900, loss is 5.572320375442505 and perplexity is 263.04375191744424
At time: 371.69938945770264 and batch: 950, loss is 5.539091653823853 and perplexity is 254.44676870628138
At time: 372.53033208847046 and batch: 1000, loss is 5.5287469959259035 and perplexity is 251.8281715284882
At time: 373.37693881988525 and batch: 1050, loss is 5.5223496150970455 and perplexity is 250.22227306079805
At time: 374.2021882534027 and batch: 1100, loss is 5.509188985824585 and perplexity is 246.95076525921766
At time: 375.0859754085541 and batch: 1150, loss is 5.5524036312103275 and perplexity is 257.8566038070035
At time: 375.9178168773651 and batch: 1200, loss is 5.563089361190796 and perplexity is 260.6267640821786
At time: 376.748961687088 and batch: 1250, loss is 5.534412288665772 and perplexity is 253.2589007636501
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.178121970517792 and perplexity of 177.34943058294306
Finished 17 epochs...
Completing Train Step...
At time: 379.10431003570557 and batch: 50, loss is 5.5242257499694825 and perplexity is 250.6921644451239
At time: 379.95232248306274 and batch: 100, loss is 5.557433910369873 and perplexity is 259.1569623485309
At time: 380.7924563884735 and batch: 150, loss is 5.456476802825928 and perplexity is 234.27058719456323
At time: 381.6298041343689 and batch: 200, loss is 5.479747476577759 and perplexity is 239.7861481095341
At time: 382.45394945144653 and batch: 250, loss is 5.524117298126221 and perplexity is 250.66497789204115
At time: 383.27795672416687 and batch: 300, loss is 5.531942892074585 and perplexity is 252.63427563840352
At time: 384.10356187820435 and batch: 350, loss is 5.559000930786133 and perplexity is 259.5633849526399
At time: 384.92741417884827 and batch: 400, loss is 5.524188871383667 and perplexity is 250.6829194430965
At time: 385.7537302970886 and batch: 450, loss is 5.510271244049072 and perplexity is 247.21817443275057
At time: 386.57968068122864 and batch: 500, loss is 5.498607511520386 and perplexity is 244.351438689197
At time: 387.4046950340271 and batch: 550, loss is 5.515434198379516 and perplexity is 248.49785119073454
At time: 388.23230743408203 and batch: 600, loss is 5.546204900741577 and perplexity is 256.2631639746614
At time: 389.0578143596649 and batch: 650, loss is 5.54304874420166 and perplexity is 255.45563233222052
At time: 389.8832836151123 and batch: 700, loss is 5.5463474750518795 and perplexity is 256.29970312323115
At time: 390.7224931716919 and batch: 750, loss is 5.506660852432251 and perplexity is 246.3272293064439
At time: 391.5601763725281 and batch: 800, loss is 5.5180692386627195 and perplexity is 249.1535165117973
At time: 392.3860032558441 and batch: 850, loss is 5.564648885726928 and perplexity is 261.03353501772193
At time: 393.2080533504486 and batch: 900, loss is 5.557443780899048 and perplexity is 259.15952037751305
At time: 394.0341787338257 and batch: 950, loss is 5.523767309188843 and perplexity is 250.57726327325926
At time: 394.8597149848938 and batch: 1000, loss is 5.513265466690063 and perplexity is 247.95950999590335
At time: 395.6840159893036 and batch: 1050, loss is 5.50701268196106 and perplexity is 246.41390974693863
At time: 396.5605652332306 and batch: 1100, loss is 5.493525142669678 and perplexity is 243.11270506559063
At time: 397.39031076431274 and batch: 1150, loss is 5.536363868713379 and perplexity is 253.75363838426986
At time: 398.2158534526825 and batch: 1200, loss is 5.547733812332154 and perplexity is 256.65526736562896
At time: 399.0415782928467 and batch: 1250, loss is 5.518881721496582 and perplexity is 249.35603172588665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.165784404225593 and perplexity of 175.17481255741905
Finished 18 epochs...
Completing Train Step...
At time: 401.283584356308 and batch: 50, loss is 5.508968887329101 and perplexity is 246.89641774844767
At time: 402.1637969017029 and batch: 100, loss is 5.54231740951538 and perplexity is 255.2688770661163
At time: 403.01920533180237 and batch: 150, loss is 5.4410600757598875 and perplexity is 230.68659916007468
At time: 403.84625244140625 and batch: 200, loss is 5.464743585586548 and perplexity is 236.21527834077023
At time: 404.6707134246826 and batch: 250, loss is 5.508846817016601 and perplexity is 246.8662808650248
At time: 405.4954082965851 and batch: 300, loss is 5.516803932189942 and perplexity is 248.838460317994
At time: 406.3195106983185 and batch: 350, loss is 5.543532657623291 and perplexity is 255.57928065646618
At time: 407.14332580566406 and batch: 400, loss is 5.509137601852417 and perplexity is 246.93807627397675
At time: 407.9668974876404 and batch: 450, loss is 5.495012226104737 and perplexity is 243.4745028872459
At time: 408.7906594276428 and batch: 500, loss is 5.483297491073609 and perplexity is 240.63890516560244
At time: 409.61660075187683 and batch: 550, loss is 5.500130033493042 and perplexity is 244.72375247969387
At time: 410.4415786266327 and batch: 600, loss is 5.5306846809387205 and perplexity is 252.31660826771406
At time: 411.267347574234 and batch: 650, loss is 5.527726488113403 and perplexity is 251.57130999892667
At time: 412.09261107444763 and batch: 700, loss is 5.53061806678772 and perplexity is 252.29980097087906
At time: 412.9179925918579 and batch: 750, loss is 5.491676368713379 and perplexity is 242.66365984747395
At time: 413.74292731285095 and batch: 800, loss is 5.503726043701172 and perplexity is 245.60536578697622
At time: 414.56919836997986 and batch: 850, loss is 5.550337657928467 and perplexity is 257.32442887187335
At time: 415.39244055747986 and batch: 900, loss is 5.542249116897583 and perplexity is 255.25144468151763
At time: 416.21308875083923 and batch: 950, loss is 5.50926254272461 and perplexity is 246.9689308600634
At time: 417.0340328216553 and batch: 1000, loss is 5.498863143920898 and perplexity is 244.41391081864782
At time: 417.8863956928253 and batch: 1050, loss is 5.492602033615112 and perplexity is 242.88838907625393
At time: 418.70851826667786 and batch: 1100, loss is 5.478621215820312 and perplexity is 239.51623840362478
At time: 419.5468738079071 and batch: 1150, loss is 5.521107273101807 and perplexity is 249.91160444112447
At time: 420.372679233551 and batch: 1200, loss is 5.533073015213013 and perplexity is 252.91994486916212
At time: 421.1976594924927 and batch: 1250, loss is 5.5041702842712406 and perplexity is 245.71449789334517
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1542073270700275 and perplexity of 173.1584942931167
Finished 19 epochs...
Completing Train Step...
At time: 423.5646011829376 and batch: 50, loss is 5.494969863891601 and perplexity is 243.4641889869229
At time: 424.3905997276306 and batch: 100, loss is 5.528011198043823 and perplexity is 251.6429450462129
At time: 425.21998620033264 and batch: 150, loss is 5.4266085433959965 and perplexity is 227.3767977606664
At time: 426.04586243629456 and batch: 200, loss is 5.450614252090454 and perplexity is 232.90118201711647
At time: 426.8735568523407 and batch: 250, loss is 5.494444141387939 and perplexity is 243.33622802286308
At time: 427.6954605579376 and batch: 300, loss is 5.5024800205230715 and perplexity is 245.29952638959864
At time: 428.5220236778259 and batch: 350, loss is 5.528886795043945 and perplexity is 251.86337934571952
At time: 429.3460018634796 and batch: 400, loss is 5.4948450946807865 and perplexity is 243.43381404716942
At time: 430.17158818244934 and batch: 450, loss is 5.480300998687744 and perplexity is 239.91891178460583
At time: 430.9974629878998 and batch: 500, loss is 5.468835916519165 and perplexity is 237.18393010101386
At time: 431.82216596603394 and batch: 550, loss is 5.485430536270141 and perplexity is 241.15274665509875
At time: 432.6462986469269 and batch: 600, loss is 5.515983037948608 and perplexity is 248.63427407801714
At time: 433.47087001800537 and batch: 650, loss is 5.513103160858154 and perplexity is 247.91926798719848
At time: 434.2961177825928 and batch: 700, loss is 5.515649042129517 and perplexity is 248.55124513642374
At time: 435.1210517883301 and batch: 750, loss is 5.477409009933472 and perplexity is 239.2260713160958
At time: 435.95515179634094 and batch: 800, loss is 5.490115442276001 and perplexity is 242.28517519562465
At time: 436.8099675178528 and batch: 850, loss is 5.536586103439331 and perplexity is 253.81003752124664
At time: 437.6435332298279 and batch: 900, loss is 5.528269596099854 and perplexity is 251.70797749579404
At time: 438.52452874183655 and batch: 950, loss is 5.495489931106567 and perplexity is 243.59083966021012
At time: 439.34961223602295 and batch: 1000, loss is 5.484813203811646 and perplexity is 241.0039211792581
At time: 440.175411939621 and batch: 1050, loss is 5.478691215515137 and perplexity is 239.53300505404192
At time: 440.9990186691284 and batch: 1100, loss is 5.46435962677002 and perplexity is 236.12459881176906
At time: 441.8252971172333 and batch: 1150, loss is 5.506912517547607 and perplexity is 246.38922907828533
At time: 442.6495635509491 and batch: 1200, loss is 5.5193952941894535 and perplexity is 249.48412706490248
At time: 443.47409200668335 and batch: 1250, loss is 5.49032299041748 and perplexity is 242.3354662521716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.143298518048586 and perplexity of 171.27980709894754
Finished 20 epochs...
Completing Train Step...
At time: 445.6968688964844 and batch: 50, loss is 5.481248550415039 and perplexity is 240.14635510401556
At time: 446.57054901123047 and batch: 100, loss is 5.514413709640503 and perplexity is 248.24439128041357
At time: 447.39315390586853 and batch: 150, loss is 5.412760763168335 and perplexity is 224.24983455501064
At time: 448.2182433605194 and batch: 200, loss is 5.436920690536499 and perplexity is 229.73367208706134
At time: 449.0600914955139 and batch: 250, loss is 5.480735702514648 and perplexity is 240.02322812541894
At time: 449.9133093357086 and batch: 300, loss is 5.488857727050782 and perplexity is 241.9806409907331
At time: 450.7450177669525 and batch: 350, loss is 5.514917526245117 and perplexity is 248.36949243811705
At time: 451.56957602500916 and batch: 400, loss is 5.481233568191528 and perplexity is 240.14275720460043
At time: 452.3933255672455 and batch: 450, loss is 5.466306838989258 and perplexity is 236.58483145573254
At time: 453.2268896102905 and batch: 500, loss is 5.4549886512756345 and perplexity is 233.9222163356501
At time: 454.0742962360382 and batch: 550, loss is 5.471425895690918 and perplexity is 237.799027741049
At time: 454.8960061073303 and batch: 600, loss is 5.501974925994873 and perplexity is 245.17565822624886
At time: 455.7209732532501 and batch: 650, loss is 5.499154853820801 and perplexity is 244.48521917627872
At time: 456.5681481361389 and batch: 700, loss is 5.50141396522522 and perplexity is 245.03816286866638
At time: 457.43367409706116 and batch: 750, loss is 5.463837175369263 and perplexity is 236.00126740450528
At time: 458.2774679660797 and batch: 800, loss is 5.4770807266235355 and perplexity is 239.14755027885934
At time: 459.128874540329 and batch: 850, loss is 5.523532991409302 and perplexity is 250.5185554437383
At time: 460.0037536621094 and batch: 900, loss is 5.514910554885864 and perplexity is 248.367760971193
At time: 460.86915469169617 and batch: 950, loss is 5.482154741287231 and perplexity is 240.36407217071158
At time: 461.70967507362366 and batch: 1000, loss is 5.471441440582275 and perplexity is 237.80272432983153
At time: 462.5367121696472 and batch: 1050, loss is 5.465530986785889 and perplexity is 236.40134778027476
At time: 463.3606128692627 and batch: 1100, loss is 5.450881109237671 and perplexity is 232.96334165563607
At time: 464.1866955757141 and batch: 1150, loss is 5.4932050704956055 and perplexity is 243.03490390519315
At time: 465.01135063171387 and batch: 1200, loss is 5.50612174987793 and perplexity is 246.1944694567283
At time: 465.8356409072876 and batch: 1250, loss is 5.477002649307251 and perplexity is 239.12887900884843
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.132886455006843 and perplexity of 169.50568312285617
Finished 21 epochs...
Completing Train Step...
At time: 468.07452607154846 and batch: 50, loss is 5.468154582977295 and perplexity is 237.0223837735586
At time: 468.94751143455505 and batch: 100, loss is 5.501431064605713 and perplexity is 245.04235290527194
At time: 469.77776861190796 and batch: 150, loss is 5.399678297042847 and perplexity is 221.33520056111723
At time: 470.607976436615 and batch: 200, loss is 5.423847246170044 and perplexity is 226.74980888946592
At time: 471.43616461753845 and batch: 250, loss is 5.4676742839813235 and perplexity is 236.90856949523913
At time: 472.2666127681732 and batch: 300, loss is 5.475839128494263 and perplexity is 238.8508093824044
At time: 473.09609389305115 and batch: 350, loss is 5.501536121368408 and perplexity is 245.06809761389565
At time: 473.92780017852783 and batch: 400, loss is 5.468197784423828 and perplexity is 237.03262370458665
At time: 474.7590217590332 and batch: 450, loss is 5.452916898727417 and perplexity is 233.4380890569962
At time: 475.5879728794098 and batch: 500, loss is 5.441881093978882 and perplexity is 230.87607483170888
At time: 476.4183437824249 and batch: 550, loss is 5.458115663528442 and perplexity is 234.6548388351242
At time: 477.2487590312958 and batch: 600, loss is 5.488649320602417 and perplexity is 241.93021591940877
At time: 478.07701587677 and batch: 650, loss is 5.485822544097901 and perplexity is 241.24729895088248
At time: 478.9057478904724 and batch: 700, loss is 5.4878690147399904 and perplexity is 241.7415099873761
At time: 479.7350251674652 and batch: 750, loss is 5.4508567142486575 and perplexity is 232.95765858679525
At time: 480.56513023376465 and batch: 800, loss is 5.464697198867798 and perplexity is 236.20432134322098
At time: 481.43948769569397 and batch: 850, loss is 5.510910835266113 and perplexity is 247.37634358221817
At time: 482.26070261001587 and batch: 900, loss is 5.502136096954346 and perplexity is 245.21517660684663
At time: 483.08033990859985 and batch: 950, loss is 5.469467506408692 and perplexity is 237.33378039020633
At time: 483.9011616706848 and batch: 1000, loss is 5.458962659835816 and perplexity is 234.8536748119068
At time: 484.719256401062 and batch: 1050, loss is 5.453233737945556 and perplexity is 233.51206311693977
At time: 485.53826451301575 and batch: 1100, loss is 5.4378993034362795 and perplexity is 229.95860246401293
At time: 486.35893177986145 and batch: 1150, loss is 5.4799384593963625 and perplexity is 239.83194751727288
At time: 487.2306716442108 and batch: 1200, loss is 5.493321409225464 and perplexity is 243.06317992199084
At time: 488.07902240753174 and batch: 1250, loss is 5.464199848175049 and perplexity is 236.08687416900835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.123037073734033 and perplexity of 167.84435199422336
Finished 22 epochs...
Completing Train Step...
At time: 490.3917820453644 and batch: 50, loss is 5.455599937438965 and perplexity is 234.0652534636462
At time: 491.2111346721649 and batch: 100, loss is 5.488974876403809 and perplexity is 242.00899052680276
At time: 492.0305302143097 and batch: 150, loss is 5.387146167755127 and perplexity is 218.57870765101296
At time: 492.84789395332336 and batch: 200, loss is 5.411298809051513 and perplexity is 223.92223111513428
At time: 493.6655638217926 and batch: 250, loss is 5.455180444717407 and perplexity is 233.9670853852854
At time: 494.4842414855957 and batch: 300, loss is 5.4633605098724365 and perplexity is 235.8888005497903
At time: 495.3383903503418 and batch: 350, loss is 5.488724040985107 and perplexity is 241.94829371310797
At time: 496.18196630477905 and batch: 400, loss is 5.455734920501709 and perplexity is 234.09685044092112
At time: 497.0015835762024 and batch: 450, loss is 5.440127277374268 and perplexity is 230.47151540328025
At time: 497.8209810256958 and batch: 500, loss is 5.429227619171143 and perplexity is 227.97309535694484
At time: 498.6389420032501 and batch: 550, loss is 5.445243968963623 and perplexity is 231.65378915342833
At time: 499.457661151886 and batch: 600, loss is 5.475877828598023 and perplexity is 238.86005311237614
At time: 500.2764148712158 and batch: 650, loss is 5.472947292327881 and perplexity is 238.16108973223794
At time: 501.0942347049713 and batch: 700, loss is 5.474824209213256 and perplexity is 238.6085180645412
At time: 501.9129288196564 and batch: 750, loss is 5.438438510894775 and perplexity is 230.08263129323657
At time: 502.7775983810425 and batch: 800, loss is 5.452953796386719 and perplexity is 233.446702534982
At time: 503.5978739261627 and batch: 850, loss is 5.498892993927002 and perplexity is 244.42120668426776
At time: 504.41627645492554 and batch: 900, loss is 5.489892911911011 and perplexity is 242.23126538566396
At time: 505.2390899658203 and batch: 950, loss is 5.457351093292236 and perplexity is 234.47549729790632
At time: 506.0651490688324 and batch: 1000, loss is 5.446851463317871 and perplexity is 232.02647077318053
At time: 506.91560673713684 and batch: 1050, loss is 5.440997123718262 and perplexity is 230.6720774247731
At time: 507.74664282798767 and batch: 1100, loss is 5.425310249328613 and perplexity is 227.0817873596382
At time: 508.56669783592224 and batch: 1150, loss is 5.467095718383789 and perplexity is 236.77154199069057
At time: 509.3868741989136 and batch: 1200, loss is 5.480978488922119 and perplexity is 240.08150957737095
At time: 510.2082998752594 and batch: 1250, loss is 5.452024211883545 and perplexity is 233.22979493058187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1135788520757295 and perplexity of 166.2643268020157
Finished 23 epochs...
Completing Train Step...
At time: 512.4578893184662 and batch: 50, loss is 5.4436496925354 and perplexity is 231.28476322070887
At time: 513.3176355361938 and batch: 100, loss is 5.477105903625488 and perplexity is 239.1535713729959
At time: 514.1429121494293 and batch: 150, loss is 5.37505274772644 and perplexity is 215.95126294833872
At time: 514.9686262607574 and batch: 200, loss is 5.399286766052246 and perplexity is 221.24855793353032
At time: 515.7945475578308 and batch: 250, loss is 5.4432088565826415 and perplexity is 231.18282705197075
At time: 516.6228482723236 and batch: 300, loss is 5.4514162063598635 and perplexity is 233.08803302735416
At time: 517.4473588466644 and batch: 350, loss is 5.476450958251953 and perplexity is 238.99699012953846
At time: 518.2739973068237 and batch: 400, loss is 5.443732604980469 and perplexity is 231.3039404009371
At time: 519.1007311344147 and batch: 450, loss is 5.427876996994018 and perplexity is 227.6653976769979
At time: 519.9251651763916 and batch: 500, loss is 5.417105646133423 and perplexity is 225.2262936051058
At time: 520.750330209732 and batch: 550, loss is 5.433033046722412 and perplexity is 228.84228322177583
At time: 521.5761229991913 and batch: 600, loss is 5.463615303039551 and perplexity is 235.9489110619178
At time: 522.4016332626343 and batch: 650, loss is 5.460565242767334 and perplexity is 235.23034904779848
At time: 523.2557168006897 and batch: 700, loss is 5.462370538711548 and perplexity is 235.65539299250713
At time: 524.0792121887207 and batch: 750, loss is 5.426486873626709 and perplexity is 227.34913456106327
At time: 524.9047608375549 and batch: 800, loss is 5.441266269683838 and perplexity is 230.73417023942343
At time: 525.7298913002014 and batch: 850, loss is 5.48731369972229 and perplexity is 241.60730456307118
At time: 526.5548286437988 and batch: 900, loss is 5.478018436431885 and perplexity is 239.37190645645302
At time: 527.3847320079803 and batch: 950, loss is 5.44573865890503 and perplexity is 231.76841430242786
At time: 528.2108414173126 and batch: 1000, loss is 5.435156717300415 and perplexity is 229.32878524783368
At time: 529.0347628593445 and batch: 1050, loss is 5.429288778305054 and perplexity is 227.98703842038037
At time: 529.8589663505554 and batch: 1100, loss is 5.413262615203857 and perplexity is 224.36240303494537
At time: 530.6844210624695 and batch: 1150, loss is 5.454903612136841 and perplexity is 233.90232463762658
At time: 531.510561466217 and batch: 1200, loss is 5.4690677928924565 and perplexity is 237.23893382731455
At time: 532.3363244533539 and batch: 1250, loss is 5.440304832458496 and perplexity is 230.5124404257253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.104659254533531 and perplexity of 164.78791023066066
Finished 24 epochs...
Completing Train Step...
At time: 534.6056010723114 and batch: 50, loss is 5.4321225643157955 and perplexity is 228.63402117286535
At time: 535.4300410747528 and batch: 100, loss is 5.465642957687378 and perplexity is 236.42781933429325
At time: 536.2544267177582 and batch: 150, loss is 5.363454608917237 and perplexity is 213.46109877353834
At time: 537.1033616065979 and batch: 200, loss is 5.38766921043396 and perplexity is 218.69306354769887
At time: 537.956330537796 and batch: 250, loss is 5.431704463958741 and perplexity is 228.53844918771085
At time: 538.7884528636932 and batch: 300, loss is 5.439828729629516 and perplexity is 230.40271892215523
At time: 539.6101047992706 and batch: 350, loss is 5.46463532447815 and perplexity is 236.18970679714332
At time: 540.4325087070465 and batch: 400, loss is 5.432207107543945 and perplexity is 228.6533514481906
At time: 541.2543964385986 and batch: 450, loss is 5.415898170471191 and perplexity is 224.9545024607017
At time: 542.0782423019409 and batch: 500, loss is 5.405430841445923 and perplexity is 222.61211034917113
At time: 542.9074923992157 and batch: 550, loss is 5.421192388534546 and perplexity is 226.1486188181262
At time: 543.7436385154724 and batch: 600, loss is 5.451730127334595 and perplexity is 233.16121573607296
At time: 544.5988335609436 and batch: 650, loss is 5.448678951263428 and perplexity is 232.45088403851372
At time: 545.4550368785858 and batch: 700, loss is 5.450417137145996 and perplexity is 232.85527823786796
At time: 546.2900259494781 and batch: 750, loss is 5.415063085556031 and perplexity is 224.76672476517226
At time: 547.1450419425964 and batch: 800, loss is 5.430205955505371 and perplexity is 228.19623885625717
At time: 547.9795718193054 and batch: 850, loss is 5.476327095031738 and perplexity is 238.96738902600123
At time: 548.8129196166992 and batch: 900, loss is 5.466740560531616 and perplexity is 236.6874656494516
At time: 549.6492018699646 and batch: 950, loss is 5.434561672210694 and perplexity is 229.19236487240283
At time: 550.4849860668182 and batch: 1000, loss is 5.423840646743774 and perplexity is 226.7483124757583
At time: 551.3074541091919 and batch: 1050, loss is 5.417934684753418 and perplexity is 225.41309232171906
At time: 552.1317369937897 and batch: 1100, loss is 5.401684904098511 and perplexity is 221.7797792344077
At time: 552.9572193622589 and batch: 1150, loss is 5.443081865310669 and perplexity is 231.1534707147439
At time: 553.7821226119995 and batch: 1200, loss is 5.457593002319336 and perplexity is 234.53222589864004
At time: 554.6142029762268 and batch: 1250, loss is 5.429071760177612 and perplexity is 227.9375664685707
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.096033694970346 and perplexity of 163.3726348414865
Finished 25 epochs...
Completing Train Step...
At time: 556.9032549858093 and batch: 50, loss is 5.421013221740723 and perplexity is 226.10810412471722
At time: 557.7568638324738 and batch: 100, loss is 5.454516201019287 and perplexity is 233.81172582726674
At time: 558.583172082901 and batch: 150, loss is 5.352340927124024 and perplexity is 211.10189405464092
At time: 559.4087545871735 and batch: 200, loss is 5.376476526260376 and perplexity is 216.25894870711323
At time: 560.2345645427704 and batch: 250, loss is 5.420560302734375 and perplexity is 226.0057186547828
At time: 561.0591461658478 and batch: 300, loss is 5.428691816329956 and perplexity is 227.85097944268685
At time: 561.8833816051483 and batch: 350, loss is 5.453232908248902 and perplexity is 233.51186937284257
At time: 562.7077529430389 and batch: 400, loss is 5.421047430038453 and perplexity is 226.1158390303604
At time: 563.5320625305176 and batch: 450, loss is 5.404576244354248 and perplexity is 222.4219479547834
At time: 564.3562433719635 and batch: 500, loss is 5.394210929870606 and perplexity is 220.12838181829102
At time: 565.1819338798523 and batch: 550, loss is 5.4098817825317385 and perplexity is 223.6051520830326
At time: 566.047313451767 and batch: 600, loss is 5.440317907333374 and perplexity is 230.51545436674505
At time: 566.8748080730438 and batch: 650, loss is 5.437226905822754 and perplexity is 229.80403082113202
At time: 567.7014064788818 and batch: 700, loss is 5.438900709152222 and perplexity is 230.1889996642333
At time: 568.5263764858246 and batch: 750, loss is 5.404028444290161 and perplexity is 222.30013856408152
At time: 569.3490374088287 and batch: 800, loss is 5.419634828567505 and perplexity is 225.79665295797565
At time: 570.1746323108673 and batch: 850, loss is 5.465536880493164 and perplexity is 236.4027410647238
At time: 571.0023329257965 and batch: 900, loss is 5.4565225601196286 and perplexity is 234.28130702788033
At time: 571.8259947299957 and batch: 950, loss is 5.423562364578247 and perplexity is 226.68522124332245
At time: 572.6477844715118 and batch: 1000, loss is 5.412721862792969 and perplexity is 224.24111132194025
At time: 573.4809973239899 and batch: 1050, loss is 5.406975402832031 and perplexity is 222.95621409516616
At time: 574.3322353363037 and batch: 1100, loss is 5.3904137134552 and perplexity is 219.2940917058677
At time: 575.1598620414734 and batch: 1150, loss is 5.431727828979493 and perplexity is 228.54378905570155
At time: 575.9844214916229 and batch: 1200, loss is 5.446458911895752 and perplexity is 231.93540632701752
At time: 576.8272562026978 and batch: 1250, loss is 5.418209991455078 and perplexity is 225.47515859991626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.087763654054516 and perplexity of 162.02710791695452
Finished 26 epochs...
Completing Train Step...
At time: 579.0753917694092 and batch: 50, loss is 5.410387287139892 and perplexity is 223.71821409209323
At time: 579.951744556427 and batch: 100, loss is 5.443831110000611 and perplexity is 231.3267261224807
At time: 580.7767674922943 and batch: 150, loss is 5.341558275222778 and perplexity is 208.8378837682625
At time: 581.6401863098145 and batch: 200, loss is 5.365654563903808 and perplexity is 213.93122051595202
At time: 582.4808361530304 and batch: 250, loss is 5.409793519973755 and perplexity is 223.58541699127917
At time: 583.3063073158264 and batch: 300, loss is 5.417888174057007 and perplexity is 225.40260844562297
At time: 584.1297662258148 and batch: 350, loss is 5.442246017456054 and perplexity is 230.96034230638716
At time: 584.9547772407532 and batch: 400, loss is 5.410303220748902 and perplexity is 223.69940769973945
At time: 585.7794909477234 and batch: 450, loss is 5.393335933685303 and perplexity is 219.93585456651212
At time: 586.60502409935 and batch: 500, loss is 5.383318319320678 and perplexity is 217.7436207997457
At time: 587.4800865650177 and batch: 550, loss is 5.398893966674804 and perplexity is 221.1616687038491
At time: 588.3060173988342 and batch: 600, loss is 5.429185857772827 and perplexity is 227.96357508049584
At time: 589.1306130886078 and batch: 650, loss is 5.4261681079864506 and perplexity is 227.2766750180425
At time: 589.9545412063599 and batch: 700, loss is 5.427807807922363 and perplexity is 227.64964626440374
At time: 590.7774155139923 and batch: 750, loss is 5.393325433731079 and perplexity is 219.9335452622309
At time: 591.6042680740356 and batch: 800, loss is 5.4096793365478515 and perplexity is 223.55988869986663
At time: 592.4318268299103 and batch: 850, loss is 5.455240297317505 and perplexity is 233.9810893427654
At time: 593.2575402259827 and batch: 900, loss is 5.445084972381592 and perplexity is 231.61695992067703
At time: 594.082288980484 and batch: 950, loss is 5.412968416213989 and perplexity is 224.29640555128057
At time: 594.906415939331 and batch: 1000, loss is 5.402153882980347 and perplexity is 221.8838136603544
At time: 595.7306048870087 and batch: 1050, loss is 5.3964002895355225 and perplexity is 220.6108499739896
At time: 596.5555186271667 and batch: 1100, loss is 5.37954197883606 and perplexity is 216.9228973901153
At time: 597.3777105808258 and batch: 1150, loss is 5.420800380706787 and perplexity is 226.0599841631868
At time: 598.2064411640167 and batch: 1200, loss is 5.435711812973023 and perplexity is 229.45612000234513
At time: 599.0320188999176 and batch: 1250, loss is 5.407793054580688 and perplexity is 223.13858918298916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.079678500655794 and perplexity of 160.72237548312066
Finished 27 epochs...
Completing Train Step...
At time: 601.3076021671295 and batch: 50, loss is 5.3999937152862545 and perplexity is 221.40502473261233
At time: 602.1312465667725 and batch: 100, loss is 5.433493394851684 and perplexity is 228.9476545906508
At time: 602.9557025432587 and batch: 150, loss is 5.331149797439576 and perplexity is 206.6754725217655
At time: 603.7808899879456 and batch: 200, loss is 5.355143222808838 and perplexity is 211.69429363312838
At time: 604.6274962425232 and batch: 250, loss is 5.399330444335938 and perplexity is 221.25822190186145
At time: 605.4519844055176 and batch: 300, loss is 5.407447128295899 and perplexity is 223.0614130292396
At time: 606.2763736248016 and batch: 350, loss is 5.431619787216187 and perplexity is 228.5190981155898
At time: 607.1032574176788 and batch: 400, loss is 5.399882698059082 and perplexity is 221.38044632502312
At time: 607.955762386322 and batch: 450, loss is 5.382759981155395 and perplexity is 217.62208015955272
At time: 608.7808609008789 and batch: 500, loss is 5.372812261581421 and perplexity is 215.4679687448562
At time: 609.6230895519257 and batch: 550, loss is 5.38834137916565 and perplexity is 218.8401116018686
At time: 610.4530580043793 and batch: 600, loss is 5.418414373397827 and perplexity is 225.52124636046506
At time: 611.2780482769012 and batch: 650, loss is 5.415479650497437 and perplexity is 224.86037420688697
At time: 612.1012532711029 and batch: 700, loss is 5.417053594589233 and perplexity is 225.21457053383622
At time: 612.92689204216 and batch: 750, loss is 5.3829655361175535 and perplexity is 217.6668180558955
At time: 613.7528166770935 and batch: 800, loss is 5.399613132476807 and perplexity is 221.32077781875424
At time: 614.5759634971619 and batch: 850, loss is 5.4451392650604244 and perplexity is 231.6295353672685
At time: 615.3999004364014 and batch: 900, loss is 5.435309505462646 and perplexity is 229.3638266483659
At time: 616.2226803302765 and batch: 950, loss is 5.4027652931213375 and perplexity is 222.01951715516705
At time: 617.050265789032 and batch: 1000, loss is 5.391968765258789 and perplexity is 219.63537066314726
At time: 617.8755021095276 and batch: 1050, loss is 5.386322298049927 and perplexity is 218.39870143659275
At time: 618.7026369571686 and batch: 1100, loss is 5.369029369354248 and perplexity is 214.6544164012844
At time: 619.5266342163086 and batch: 1150, loss is 5.4101442527770995 and perplexity is 223.66384948498762
At time: 620.3510639667511 and batch: 1200, loss is 5.425388011932373 and perplexity is 227.09944651729185
At time: 621.1751594543457 and batch: 1250, loss is 5.397675933837891 and perplexity is 220.8924505206794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.071942183223084 and perplexity of 159.48377345466068
Finished 28 epochs...
Completing Train Step...
At time: 623.415762424469 and batch: 50, loss is 5.389818859100342 and perplexity is 219.1636824515236
At time: 624.2690472602844 and batch: 100, loss is 5.4234467792510985 and perplexity is 226.65902127206058
At time: 625.0931630134583 and batch: 150, loss is 5.321053800582885 and perplexity is 204.599375370526
At time: 625.9201157093048 and batch: 200, loss is 5.344961547851563 and perplexity is 209.54982680290664
At time: 626.7460572719574 and batch: 250, loss is 5.389213275909424 and perplexity is 219.03100078831866
At time: 627.571679353714 and batch: 300, loss is 5.397342472076416 and perplexity is 220.81880361492819
At time: 628.40185713768 and batch: 350, loss is 5.421335439682007 and perplexity is 226.1809719515661
At time: 629.2556021213531 and batch: 400, loss is 5.38980993270874 and perplexity is 219.16172611940064
At time: 630.0813708305359 and batch: 450, loss is 5.372354021072388 and perplexity is 215.3692552121753
At time: 630.9060778617859 and batch: 500, loss is 5.362606983184815 and perplexity is 213.2802403143173
At time: 631.7294456958771 and batch: 550, loss is 5.378081874847412 and perplexity is 216.60639851926763
At time: 632.5543532371521 and batch: 600, loss is 5.407939815521241 and perplexity is 223.17133961539454
At time: 633.3764872550964 and batch: 650, loss is 5.4051300811767575 and perplexity is 222.54516753831663
At time: 634.2000050544739 and batch: 700, loss is 5.4066446590423585 and perplexity is 222.88248490539323
At time: 635.0260713100433 and batch: 750, loss is 5.372882280349732 and perplexity is 215.48305607483005
At time: 635.8507566452026 and batch: 800, loss is 5.389903917312622 and perplexity is 219.18232491538566
At time: 636.6919391155243 and batch: 850, loss is 5.435407953262329 and perplexity is 229.38640812395636
At time: 637.5466043949127 and batch: 900, loss is 5.424927835464477 and perplexity is 226.9949647380043
At time: 638.386335849762 and batch: 950, loss is 5.392863740921021 and perplexity is 219.83202696263044
At time: 639.2247440814972 and batch: 1000, loss is 5.381898632049561 and perplexity is 217.43471228155892
At time: 640.0627019405365 and batch: 1050, loss is 5.376247968673706 and perplexity is 216.20952673179977
At time: 640.9033305644989 and batch: 1100, loss is 5.3588596439361575 and perplexity is 212.48250252940278
At time: 641.7375473976135 and batch: 1150, loss is 5.399877643585205 and perplexity is 221.379327366168
At time: 642.5602278709412 and batch: 1200, loss is 5.415229043960571 and perplexity is 224.8040297876635
At time: 643.3843564987183 and batch: 1250, loss is 5.387854471206665 and perplexity is 218.73358254681156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.064478964701186 and perplexity of 158.29794176737173
Finished 29 epochs...
Completing Train Step...
At time: 645.7814881801605 and batch: 50, loss is 5.379897632598877 and perplexity is 217.00006055568502
At time: 646.606107711792 and batch: 100, loss is 5.413742513656616 and perplexity is 224.47010004476635
At time: 647.4313344955444 and batch: 150, loss is 5.311259355545044 and perplexity is 202.60521979903183
At time: 648.256590127945 and batch: 200, loss is 5.335096588134766 and perplexity is 207.4927891815769
At time: 649.0802102088928 and batch: 250, loss is 5.379425430297852 and perplexity is 216.89761681675623
At time: 649.9053707122803 and batch: 300, loss is 5.387499027252197 and perplexity is 218.65584883306394
At time: 650.7803530693054 and batch: 350, loss is 5.411356115341187 and perplexity is 223.93506363506347
At time: 651.6069784164429 and batch: 400, loss is 5.380043992996216 and perplexity is 217.0318230951027
At time: 652.433776140213 and batch: 450, loss is 5.362220945358277 and perplexity is 213.19792196392368
At time: 653.2573184967041 and batch: 500, loss is 5.3527302742004395 and perplexity is 211.18410196258276
At time: 654.0828514099121 and batch: 550, loss is 5.368162508010864 and perplexity is 214.4684214130994
At time: 654.9046692848206 and batch: 600, loss is 5.397786407470703 and perplexity is 220.91685466013138
At time: 655.7339525222778 and batch: 650, loss is 5.395132236480713 and perplexity is 220.33128100336074
At time: 656.5570585727692 and batch: 700, loss is 5.396560010910034 and perplexity is 220.64608905632193
At time: 657.3783643245697 and batch: 750, loss is 5.3631026077270505 and perplexity is 213.38597343559152
At time: 658.2013337612152 and batch: 800, loss is 5.3804931640625 and perplexity is 217.12932940736775
At time: 659.0463936328888 and batch: 850, loss is 5.425953531265259 and perplexity is 227.22791196620884
At time: 659.8709487915039 and batch: 900, loss is 5.414767694473267 and perplexity is 224.7003404841196
At time: 660.6960556507111 and batch: 950, loss is 5.383184471130371 and perplexity is 217.714478160539
At time: 661.5374283790588 and batch: 1000, loss is 5.3720995044708255 and perplexity is 215.31444713633613
At time: 662.3583183288574 and batch: 1050, loss is 5.366545009613037 and perplexity is 214.12179949085333
At time: 663.1794009208679 and batch: 1100, loss is 5.34889859199524 and perplexity is 210.3764598990687
At time: 664.0013098716736 and batch: 1150, loss is 5.389807100296021 and perplexity is 219.16110536381908
At time: 664.8215601444244 and batch: 1200, loss is 5.405314722061157 and perplexity is 222.58626226863672
At time: 665.6400761604309 and batch: 1250, loss is 5.378258543014526 and perplexity is 216.64466935519891
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.057142271612682 and perplexity of 157.14080830902608
Finished 30 epochs...
Completing Train Step...
At time: 667.899739742279 and batch: 50, loss is 5.370263090133667 and perplexity is 214.9194034415911
At time: 668.7540597915649 and batch: 100, loss is 5.404296207427978 and perplexity is 222.35967031656932
At time: 669.5786395072937 and batch: 150, loss is 5.3017564964294435 and perplexity is 200.6890100947721
At time: 670.4122395515442 and batch: 200, loss is 5.325526084899902 and perplexity is 205.51645113147814
At time: 671.2536950111389 and batch: 250, loss is 5.36986270904541 and perplexity is 214.83337100098402
At time: 672.1082847118378 and batch: 300, loss is 5.378042449951172 and perplexity is 216.59785900281693
At time: 672.9361145496368 and batch: 350, loss is 5.401731348037719 and perplexity is 221.79007980018966
At time: 673.7617630958557 and batch: 400, loss is 5.370605325698852 and perplexity is 214.9929690927702
At time: 674.5888230800629 and batch: 450, loss is 5.352407712936401 and perplexity is 211.11599313693384
At time: 675.4142210483551 and batch: 500, loss is 5.343103637695313 and perplexity is 209.16086349277748
At time: 676.2394552230835 and batch: 550, loss is 5.3586548328399655 and perplexity is 212.4389882113978
At time: 677.0629932880402 and batch: 600, loss is 5.387945652008057 and perplexity is 218.75352775945592
At time: 677.8973245620728 and batch: 650, loss is 5.3854539108276365 and perplexity is 218.20912911790302
At time: 678.7241053581238 and batch: 700, loss is 5.386729326248169 and perplexity is 218.48761396026148
At time: 679.5730545520782 and batch: 750, loss is 5.353562602996826 and perplexity is 211.35994974341926
At time: 680.3971557617188 and batch: 800, loss is 5.371305446624756 and perplexity is 215.14354287313103
At time: 681.221108675003 and batch: 850, loss is 5.416675453186035 and perplexity is 225.1294236799039
At time: 682.0474410057068 and batch: 900, loss is 5.405096111297607 and perplexity is 222.53760783427165
At time: 682.884514093399 and batch: 950, loss is 5.373816194534302 and perplexity is 215.6843927583915
At time: 683.7281770706177 and batch: 1000, loss is 5.362618999481201 and perplexity is 213.28280316829625
At time: 684.578661441803 and batch: 1050, loss is 5.357405939102173 and perplexity is 212.17384009473224
At time: 685.4053392410278 and batch: 1100, loss is 5.3392784214019775 and perplexity is 208.36230625062962
At time: 686.2307527065277 and batch: 1150, loss is 5.380403156280518 and perplexity is 217.10978695752397
At time: 687.0685884952545 and batch: 1200, loss is 5.3957918262481686 and perplexity is 220.47665720081656
At time: 687.9182443618774 and batch: 1250, loss is 5.368889398574829 and perplexity is 214.62437315795225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.050108081232892 and perplexity of 156.03932849804073
Finished 31 epochs...
Completing Train Step...
At time: 690.1609394550323 and batch: 50, loss is 5.360879535675049 and perplexity is 212.9121279332516
At time: 691.015331029892 and batch: 100, loss is 5.395038948059082 and perplexity is 220.31072760463212
At time: 691.8421275615692 and batch: 150, loss is 5.292491483688354 and perplexity is 198.8382109375693
At time: 692.723254442215 and batch: 200, loss is 5.316170988082885 and perplexity is 203.60279003268343
At time: 693.5493271350861 and batch: 250, loss is 5.360564832687378 and perplexity is 212.84513439256622
At time: 694.4108979701996 and batch: 300, loss is 5.368751478195191 and perplexity is 214.5947741241285
At time: 695.2416262626648 and batch: 350, loss is 5.392434272766113 and perplexity is 219.73763637794718
At time: 696.0700025558472 and batch: 400, loss is 5.361426868438721 and perplexity is 213.02869361385063
At time: 696.8995454311371 and batch: 450, loss is 5.3428266239166256 and perplexity is 209.10293107603718
At time: 697.7308266162872 and batch: 500, loss is 5.333746585845947 and perplexity is 207.21286243465698
At time: 698.5549516677856 and batch: 550, loss is 5.3494332408905025 and perplexity is 210.4889675142991
At time: 699.3817229270935 and batch: 600, loss is 5.3783587455749515 and perplexity is 216.66637879342323
At time: 700.232085943222 and batch: 650, loss is 5.376045255661011 and perplexity is 216.16570268926455
At time: 701.0670962333679 and batch: 700, loss is 5.37720136642456 and perplexity is 216.41575870322652
At time: 701.8988370895386 and batch: 750, loss is 5.344301958084106 and perplexity is 209.41165545458824
At time: 702.7532997131348 and batch: 800, loss is 5.362348670959473 and perplexity is 213.22515453579152
At time: 703.5880336761475 and batch: 850, loss is 5.407644634246826 and perplexity is 223.10547333667836
At time: 704.4167516231537 and batch: 900, loss is 5.395709762573242 and perplexity is 220.45856481846496
At time: 705.2476623058319 and batch: 950, loss is 5.364689779281616 and perplexity is 213.72492249677177
At time: 706.0741338729858 and batch: 1000, loss is 5.353419828414917 and perplexity is 211.32977506910183
At time: 706.8980622291565 and batch: 1050, loss is 5.3481927013397215 and perplexity is 210.22800952291578
At time: 707.7236540317535 and batch: 1100, loss is 5.329830255508423 and perplexity is 206.4029354212431
At time: 708.5490152835846 and batch: 1150, loss is 5.370523538589477 and perplexity is 214.9753861583307
At time: 709.3730206489563 and batch: 1200, loss is 5.386241607666015 and perplexity is 218.38107947249912
At time: 710.198100566864 and batch: 1250, loss is 5.359812431335449 and perplexity is 212.6850496572355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.043297092409899 and perplexity of 154.98015747124325
Finished 32 epochs...
Completing Train Step...
At time: 712.4947745800018 and batch: 50, loss is 5.351748628616333 and perplexity is 210.9768957396337
At time: 713.3258121013641 and batch: 100, loss is 5.386093873977661 and perplexity is 218.34881961315446
At time: 714.1780498027802 and batch: 150, loss is 5.283523139953613 and perplexity is 197.06293405986523
At time: 715.0034599304199 and batch: 200, loss is 5.307113876342774 and perplexity is 201.76706255610347
At time: 715.8350148200989 and batch: 250, loss is 5.351397724151611 and perplexity is 210.90287599265488
At time: 716.6744599342346 and batch: 300, loss is 5.3596703243255615 and perplexity is 212.65482776820295
At time: 717.5004110336304 and batch: 350, loss is 5.383233880996704 and perplexity is 217.72523566956494
At time: 718.329431772232 and batch: 400, loss is 5.352473449707031 and perplexity is 211.1298716767113
At time: 719.159440279007 and batch: 450, loss is 5.333473405838013 and perplexity is 207.15626375441886
At time: 719.9907641410828 and batch: 500, loss is 5.324621810913086 and perplexity is 205.33069195211203
At time: 720.8253667354584 and batch: 550, loss is 5.34026349067688 and perplexity is 208.56765868314528
At time: 721.6499872207642 and batch: 600, loss is 5.368986444473267 and perplexity is 214.64520258376072
At time: 722.4795851707458 and batch: 650, loss is 5.366876916885376 and perplexity is 214.19287986866533
At time: 723.3180875778198 and batch: 700, loss is 5.367932472229004 and perplexity is 214.41909167611513
At time: 724.1424579620361 and batch: 750, loss is 5.33527177810669 and perplexity is 207.5291430218092
At time: 724.9680488109589 and batch: 800, loss is 5.353684778213501 and perplexity is 211.38577426860195
At time: 725.7927842140198 and batch: 850, loss is 5.398843660354614 and perplexity is 221.15054315397452
At time: 726.6195163726807 and batch: 900, loss is 5.386457509994507 and perplexity is 218.42823354620978
At time: 727.4439392089844 and batch: 950, loss is 5.355780935287475 and perplexity is 211.8293367806081
At time: 728.2715899944305 and batch: 1000, loss is 5.344337739944458 and perplexity is 209.41914872726062
At time: 729.0963141918182 and batch: 1050, loss is 5.3391798400878905 and perplexity is 208.3417666331011
At time: 729.9229753017426 and batch: 1100, loss is 5.320552673339844 and perplexity is 204.49687073569615
At time: 730.7480735778809 and batch: 1150, loss is 5.361222772598267 and perplexity is 212.98521978015194
At time: 731.5748467445374 and batch: 1200, loss is 5.377226257324219 and perplexity is 216.42114555320248
At time: 732.3995952606201 and batch: 1250, loss is 5.35094012260437 and perplexity is 210.80638858835945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.036606837363139 and perplexity of 153.94676138678076
Finished 33 epochs...
Completing Train Step...
At time: 734.6517877578735 and batch: 50, loss is 5.342848339080811 and perplexity is 209.1074718298185
At time: 735.5130920410156 and batch: 100, loss is 5.37727219581604 and perplexity is 216.43108784259255
At time: 736.3358609676361 and batch: 150, loss is 5.274711627960205 and perplexity is 195.33413948568446
At time: 737.1628549098969 and batch: 200, loss is 5.298226690292358 and perplexity is 199.98186557105777
At time: 737.9997451305389 and batch: 250, loss is 5.342344751358032 and perplexity is 209.0021943847031
At time: 738.8434457778931 and batch: 300, loss is 5.350786571502685 and perplexity is 210.7740215202127
At time: 739.6760907173157 and batch: 350, loss is 5.374283533096314 and perplexity is 215.78521394933182
At time: 740.5127289295197 and batch: 400, loss is 5.3437606239318844 and perplexity is 209.29832445135625
At time: 741.3593790531158 and batch: 450, loss is 5.324299182891846 and perplexity is 205.26445720243532
At time: 742.1873235702515 and batch: 500, loss is 5.3156600666046145 and perplexity is 203.49879156400786
At time: 743.0214471817017 and batch: 550, loss is 5.331412515640259 and perplexity is 206.7297770631151
At time: 743.8452732563019 and batch: 600, loss is 5.359834489822387 and perplexity is 212.68974121936955
At time: 744.6879198551178 and batch: 650, loss is 5.357953729629517 and perplexity is 212.29009875427536
At time: 745.5313665866852 and batch: 700, loss is 5.3588759803771975 and perplexity is 212.48597376563112
At time: 746.3643281459808 and batch: 750, loss is 5.326434860229492 and perplexity is 205.7033043030078
At time: 747.1885104179382 and batch: 800, loss is 5.345126695632935 and perplexity is 209.5844363496563
At time: 748.0193648338318 and batch: 850, loss is 5.390269622802735 and perplexity is 219.26249575350727
At time: 748.8468685150146 and batch: 900, loss is 5.377360925674439 and perplexity is 216.4502925943747
At time: 749.7022194862366 and batch: 950, loss is 5.346979665756225 and perplexity is 209.97315007475783
At time: 750.5395503044128 and batch: 1000, loss is 5.335477504730225 and perplexity is 207.57184168366334
At time: 751.3650209903717 and batch: 1050, loss is 5.330337448120117 and perplexity is 206.50764801760366
At time: 752.1887493133545 and batch: 1100, loss is 5.311455659866333 and perplexity is 202.64499598318474
At time: 753.0154004096985 and batch: 1150, loss is 5.352395477294922 and perplexity is 211.1134100131345
At time: 753.8451254367828 and batch: 1200, loss is 5.368048763275146 and perplexity is 214.444028146515
At time: 754.6773059368134 and batch: 1250, loss is 5.34214451789856 and perplexity is 208.96034934181276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.030075630132299 and perplexity of 152.94457947699092
Finished 34 epochs...
Completing Train Step...
At time: 757.0225048065186 and batch: 50, loss is 5.334321632385254 and perplexity is 207.3320537410894
At time: 757.8496119976044 and batch: 100, loss is 5.3686504077911374 and perplexity is 214.57308603963025
At time: 758.6743261814117 and batch: 150, loss is 5.26609899520874 and perplexity is 193.65902221803861
At time: 759.5002074241638 and batch: 200, loss is 5.289477281570434 and perplexity is 198.2397747379547
At time: 760.3243200778961 and batch: 250, loss is 5.3333415699005124 and perplexity is 207.12895491436058
At time: 761.1695122718811 and batch: 300, loss is 5.342063064575195 and perplexity is 208.9433295200774
At time: 761.9951047897339 and batch: 350, loss is 5.365512371063232 and perplexity is 213.9008031906332
At time: 762.8193020820618 and batch: 400, loss is 5.335261936187744 and perplexity is 207.52710054685568
At time: 763.6456506252289 and batch: 450, loss is 5.3153408336639405 and perplexity is 203.43383841449736
At time: 764.4706587791443 and batch: 500, loss is 5.306780052185059 and perplexity is 201.69971907745648
At time: 765.290668964386 and batch: 550, loss is 5.32260293006897 and perplexity is 204.91657192157797
At time: 766.1163575649261 and batch: 600, loss is 5.350824394226074 and perplexity is 210.78199371849038
At time: 766.944876909256 and batch: 650, loss is 5.3491884708404545 and perplexity is 210.43745242411887
At time: 767.7757611274719 and batch: 700, loss is 5.350018653869629 and perplexity is 210.61222656304824
At time: 768.6236581802368 and batch: 750, loss is 5.317795133590698 and perplexity is 203.93373927160346
At time: 769.4573926925659 and batch: 800, loss is 5.336973829269409 and perplexity is 207.8826690153053
At time: 770.2961597442627 and batch: 850, loss is 5.381831407546997 and perplexity is 217.42009583248299
At time: 771.1267535686493 and batch: 900, loss is 5.368432416915893 and perplexity is 214.52631616268903
At time: 771.9554877281189 and batch: 950, loss is 5.338197708129883 and perplexity is 208.13724797449785
At time: 772.797336101532 and batch: 1000, loss is 5.326676893234253 and perplexity is 205.75309731737084
At time: 773.6294932365417 and batch: 1050, loss is 5.321555280685425 and perplexity is 204.70200361712742
At time: 774.4642078876495 and batch: 1100, loss is 5.302352752685547 and perplexity is 200.80870785429502
At time: 775.3077516555786 and batch: 1150, loss is 5.343024349212646 and perplexity is 209.14428010272252
At time: 776.1512675285339 and batch: 1200, loss is 5.358996887207031 and perplexity is 212.51166632427478
At time: 776.9832072257996 and batch: 1250, loss is 5.333434944152832 and perplexity is 207.1482963286404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0236041215214415 and perplexity of 151.9579931082308
Finished 35 epochs...
Completing Train Step...
At time: 779.4294466972351 and batch: 50, loss is 5.3257048797607425 and perplexity is 205.55319970188845
At time: 780.4872479438782 and batch: 100, loss is 5.359912128448486 and perplexity is 212.70625479970067
At time: 781.3230652809143 and batch: 150, loss is 5.257561054229736 and perplexity is 192.01261139694293
At time: 782.1532039642334 and batch: 200, loss is 5.280903730392456 and perplexity is 196.54742099092778
At time: 783.0214405059814 and batch: 250, loss is 5.324663372039795 and perplexity is 205.33922590435662
At time: 783.8560454845428 and batch: 300, loss is 5.3335394859313965 and perplexity is 207.16995311196482
At time: 784.6952974796295 and batch: 350, loss is 5.3569792461395265 and perplexity is 212.08332632247428
At time: 785.5220203399658 and batch: 400, loss is 5.32703143119812 and perplexity is 205.8260575343722
At time: 786.3468387126923 and batch: 450, loss is 5.306531457901001 and perplexity is 201.64958391211377
At time: 787.2112531661987 and batch: 500, loss is 5.298175630569458 and perplexity is 199.97165481309804
At time: 788.0421512126923 and batch: 550, loss is 5.3141395854949955 and perplexity is 203.18961060707207
At time: 788.885080575943 and batch: 600, loss is 5.3420954513549805 and perplexity is 208.95009663126004
At time: 789.7097668647766 and batch: 650, loss is 5.340707206726075 and perplexity is 208.66022403549346
At time: 790.5321400165558 and batch: 700, loss is 5.341351413726807 and perplexity is 208.79468771916447
At time: 791.3565521240234 and batch: 750, loss is 5.309407815933228 and perplexity is 202.23043528027407
At time: 792.1786675453186 and batch: 800, loss is 5.328880043029785 and perplexity is 206.20690192786412
At time: 793.0024304389954 and batch: 850, loss is 5.373583860397339 and perplexity is 215.63428773192302
At time: 793.8251359462738 and batch: 900, loss is 5.3598342132568355 and perplexity is 212.68968239672205
At time: 794.6483244895935 and batch: 950, loss is 5.3297434997558595 and perplexity is 206.385029555979
At time: 795.4794120788574 and batch: 1000, loss is 5.318092164993286 and perplexity is 203.9943229934028
At time: 796.3005528450012 and batch: 1050, loss is 5.31311294555664 and perplexity is 202.9811150809937
At time: 797.1230504512787 and batch: 1100, loss is 5.293654451370239 and perplexity is 199.06958786668093
At time: 797.9455575942993 and batch: 1150, loss is 5.3346443939208985 and perplexity is 207.39898335371453
At time: 798.7697451114655 and batch: 1200, loss is 5.3506717777252195 and perplexity is 210.74982736278662
At time: 799.5921607017517 and batch: 1250, loss is 5.325078115463257 and perplexity is 205.4244066607376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.017421750256615 and perplexity of 151.02143045029376
Finished 36 epochs...
Completing Train Step...
At time: 801.8994646072388 and batch: 50, loss is 5.317357749938965 and perplexity is 203.8445614918824
At time: 802.7225258350372 and batch: 100, loss is 5.351538906097412 and perplexity is 210.93265377305553
At time: 803.5437753200531 and batch: 150, loss is 5.249428968429566 and perplexity is 190.45748016681097
At time: 804.3648171424866 and batch: 200, loss is 5.272615089416504 and perplexity is 194.92504292655195
At time: 805.1863253116608 and batch: 250, loss is 5.316222896575928 and perplexity is 203.61335902100106
At time: 806.0071229934692 and batch: 300, loss is 5.325282859802246 and perplexity is 205.46647045110603
At time: 806.8316071033478 and batch: 350, loss is 5.348622817993164 and perplexity is 210.31845153974606
At time: 807.6551704406738 and batch: 400, loss is 5.319017496109009 and perplexity is 204.1831726486555
At time: 808.4789910316467 and batch: 450, loss is 5.298183135986328 and perplexity is 199.973155689362
At time: 809.3320038318634 and batch: 500, loss is 5.289827976226807 and perplexity is 198.30930855949293
At time: 810.1814866065979 and batch: 550, loss is 5.305812339782715 and perplexity is 201.504626169894
At time: 811.0095851421356 and batch: 600, loss is 5.333499097824097 and perplexity is 207.16158607863477
At time: 811.839241027832 and batch: 650, loss is 5.332360429763794 and perplexity is 206.9258320455116
At time: 812.7148261070251 and batch: 700, loss is 5.332869424819946 and perplexity is 207.0311830803164
At time: 813.5621676445007 and batch: 750, loss is 5.301194019317627 and perplexity is 200.57615886109406
At time: 814.3897178173065 and batch: 800, loss is 5.321079206466675 and perplexity is 204.60457346451088
At time: 815.3854467868805 and batch: 850, loss is 5.365578470230102 and perplexity is 213.91494232280414
At time: 816.2120313644409 and batch: 900, loss is 5.351488656997681 and perplexity is 210.92205486339464
At time: 817.044917345047 and batch: 950, loss is 5.321608581542969 and perplexity is 204.71291470024357
At time: 817.8782961368561 and batch: 1000, loss is 5.309884929656983 and perplexity is 202.32694521758424
At time: 818.7278971672058 and batch: 1050, loss is 5.304959173202515 and perplexity is 201.33278247316323
At time: 819.5559771060944 and batch: 1100, loss is 5.285334615707398 and perplexity is 197.42023230773628
At time: 820.3803536891937 and batch: 1150, loss is 5.326305332183838 and perplexity is 205.6766616815375
At time: 821.2044367790222 and batch: 1200, loss is 5.342163543701172 and perplexity is 208.96432501799327
At time: 822.0288362503052 and batch: 1250, loss is 5.316937475204468 and perplexity is 203.75890877302038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.011599352760037 and perplexity of 150.14467852765569
Finished 37 epochs...
Completing Train Step...
At time: 824.2969200611115 and batch: 50, loss is 5.3091652202606205 and perplexity is 202.18138100222396
At time: 825.1209645271301 and batch: 100, loss is 5.343465213775635 and perplexity is 209.2365047321633
At time: 825.9457244873047 and batch: 150, loss is 5.241466341018676 and perplexity is 188.94696005007415
At time: 826.774080991745 and batch: 200, loss is 5.264559230804443 and perplexity is 193.3610624018846
At time: 827.6110417842865 and batch: 250, loss is 5.308036384582519 and perplexity is 201.95328021427648
At time: 828.4382085800171 and batch: 300, loss is 5.317220392227173 and perplexity is 203.81656379224873
At time: 829.2686233520508 and batch: 350, loss is 5.340434885025024 and perplexity is 208.6034090646684
At time: 830.1044592857361 and batch: 400, loss is 5.311232433319092 and perplexity is 202.5997652889495
At time: 830.9347231388092 and batch: 450, loss is 5.2900037097930905 and perplexity is 198.34416122381518
At time: 831.7674374580383 and batch: 500, loss is 5.281653270721436 and perplexity is 196.69479643453693
At time: 832.6096622943878 and batch: 550, loss is 5.297786750793457 and perplexity is 199.89390499941277
At time: 833.4345965385437 and batch: 600, loss is 5.325131378173828 and perplexity is 205.43534841284486
At time: 834.2602338790894 and batch: 650, loss is 5.3243172740936275 and perplexity is 205.26817071674012
At time: 835.0851128101349 and batch: 700, loss is 5.324665451049805 and perplexity is 205.33965280710657
At time: 835.9073393344879 and batch: 750, loss is 5.29334303855896 and perplexity is 199.0076046983604
At time: 836.7813258171082 and batch: 800, loss is 5.313291730880738 and perplexity is 203.0174083696961
At time: 837.6048362255096 and batch: 850, loss is 5.357752885818481 and perplexity is 212.2474658832139
At time: 838.4356374740601 and batch: 900, loss is 5.34339072227478 and perplexity is 209.22091897140274
At time: 839.2731714248657 and batch: 950, loss is 5.313755073547363 and perplexity is 203.11149679296872
At time: 840.1113584041595 and batch: 1000, loss is 5.301834135055542 and perplexity is 200.70459191865686
At time: 840.9646661281586 and batch: 1050, loss is 5.297283887863159 and perplexity is 199.7934110340587
At time: 841.7896463871002 and batch: 1100, loss is 5.2771689987182615 and perplexity is 195.8147381508117
At time: 842.6137406826019 and batch: 1150, loss is 5.318418998718261 and perplexity is 204.06100611441343
At time: 843.4395377635956 and batch: 1200, loss is 5.334244289398193 and perplexity is 207.31601868084746
At time: 844.2642202377319 and batch: 1250, loss is 5.309004945755005 and perplexity is 202.14897907800966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.005989464530109 and perplexity of 149.30474184105518
Finished 38 epochs...
Completing Train Step...
At time: 846.5536787509918 and batch: 50, loss is 5.301223697662354 and perplexity is 200.58211171781574
At time: 847.4050130844116 and batch: 100, loss is 5.335561361312866 and perplexity is 207.58924867879486
At time: 848.2296471595764 and batch: 150, loss is 5.233641920089721 and perplexity is 187.4743282599562
At time: 849.0523874759674 and batch: 200, loss is 5.256707134246827 and perplexity is 191.84871797699256
At time: 849.8767879009247 and batch: 250, loss is 5.300044422149658 and perplexity is 200.3457095642144
At time: 850.7015645503998 and batch: 300, loss is 5.309331045150757 and perplexity is 202.21491048745108
At time: 851.5261449813843 and batch: 350, loss is 5.332364730834961 and perplexity is 206.9267220501555
At time: 852.3499984741211 and batch: 400, loss is 5.303559627532959 and perplexity is 201.05120513544622
At time: 853.1751363277435 and batch: 450, loss is 5.281825761795044 and perplexity is 196.72872745746227
At time: 854.000301361084 and batch: 500, loss is 5.273646259307862 and perplexity is 195.12614743052453
At time: 854.8273704051971 and batch: 550, loss is 5.289970045089722 and perplexity is 198.33748413885428
At time: 855.6572968959808 and batch: 600, loss is 5.316947803497315 and perplexity is 203.7610132655682
At time: 856.5038511753082 and batch: 650, loss is 5.316364183425903 and perplexity is 203.64212894346798
At time: 857.3300380706787 and batch: 700, loss is 5.316520738601684 and perplexity is 203.6740126684772
At time: 858.2663118839264 and batch: 750, loss is 5.28555591583252 and perplexity is 197.46392626440831
At time: 859.0920443534851 and batch: 800, loss is 5.3057695007324215 and perplexity is 201.4959940879755
At time: 859.9186210632324 and batch: 850, loss is 5.350102014541626 and perplexity is 210.62978407157803
At time: 860.7453536987305 and batch: 900, loss is 5.335465726852417 and perplexity is 207.56939694227253
At time: 861.5804271697998 and batch: 950, loss is 5.3060989093780515 and perplexity is 201.56237954385915
At time: 862.4179756641388 and batch: 1000, loss is 5.294049882888794 and perplexity is 199.14832182202704
At time: 863.2431547641754 and batch: 1050, loss is 5.289564046859741 and perplexity is 198.25697581557944
At time: 864.0699865818024 and batch: 1100, loss is 5.269178209304809 and perplexity is 194.2562588469121
At time: 864.895348072052 and batch: 1150, loss is 5.31032977104187 and perplexity is 202.4169686376817
At time: 865.7203223705292 and batch: 1200, loss is 5.3260836410522465 and perplexity is 205.63107004348495
At time: 866.5461113452911 and batch: 1250, loss is 5.301355781555176 and perplexity is 200.60860713373225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.00047313383896 and perplexity of 148.48339500447568
Finished 39 epochs...
Completing Train Step...
At time: 868.8097448348999 and batch: 50, loss is 5.293493003845215 and perplexity is 199.0374511686772
At time: 869.6359791755676 and batch: 100, loss is 5.327921714782715 and perplexity is 206.00938268825584
At time: 870.4584457874298 and batch: 150, loss is 5.226048631668091 and perplexity is 186.0561726600734
At time: 871.2818231582642 and batch: 200, loss is 5.249012422561646 and perplexity is 190.3781624113215
At time: 872.1055972576141 and batch: 250, loss is 5.292178897857666 and perplexity is 198.77606664344975
At time: 872.931726694107 and batch: 300, loss is 5.301576919555664 and perplexity is 200.6529742254387
At time: 873.7568504810333 and batch: 350, loss is 5.324434099197387 and perplexity is 205.29215259289796
At time: 874.5826179981232 and batch: 400, loss is 5.296083698272705 and perplexity is 199.55376490105797
At time: 875.4065496921539 and batch: 450, loss is 5.273908843994141 and perplexity is 195.17739129636522
At time: 876.2287726402283 and batch: 500, loss is 5.265869741439819 and perplexity is 193.61463024601753
At time: 877.0559759140015 and batch: 550, loss is 5.282240171432495 and perplexity is 196.81027063305592
At time: 877.8778254985809 and batch: 600, loss is 5.308881998062134 and perplexity is 202.12412685520883
At time: 878.7018918991089 and batch: 650, loss is 5.308652954101563 and perplexity is 202.0778368460928
At time: 879.5746393203735 and batch: 700, loss is 5.308607711791992 and perplexity is 202.0686945848509
At time: 880.3999810218811 and batch: 750, loss is 5.277893362045288 and perplexity is 195.95663055064054
At time: 881.2230458259583 and batch: 800, loss is 5.298405904769897 and perplexity is 200.01770842829688
At time: 882.0465610027313 and batch: 850, loss is 5.34264310836792 and perplexity is 209.06456095776912
At time: 882.8722188472748 and batch: 900, loss is 5.327696790695191 and perplexity is 205.963051426537
At time: 883.695595741272 and batch: 950, loss is 5.298542490005493 and perplexity is 200.04502975992878
At time: 884.5228939056396 and batch: 1000, loss is 5.286291904449463 and perplexity is 197.60931096057396
At time: 885.349148273468 and batch: 1050, loss is 5.281921319961548 and perplexity is 196.74752739218596
At time: 886.1732907295227 and batch: 1100, loss is 5.261235857009888 and perplexity is 192.71951795069748
At time: 887.0016677379608 and batch: 1150, loss is 5.30240195274353 and perplexity is 200.81858789741233
At time: 887.8282573223114 and batch: 1200, loss is 5.318348035812378 and perplexity is 204.04652586622845
At time: 888.6555891036987 and batch: 1250, loss is 5.293800239562988 and perplexity is 199.09861197776226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.995139463104471 and perplexity of 147.69354174706535
Finished 40 epochs...
Completing Train Step...
At time: 890.9260995388031 and batch: 50, loss is 5.2858976554870605 and perplexity is 197.531419050178
At time: 891.7778654098511 and batch: 100, loss is 5.320396566390992 and perplexity is 204.4649498447571
At time: 892.6272075176239 and batch: 150, loss is 5.218516645431518 and perplexity is 184.66006446242338
At time: 893.4745173454285 and batch: 200, loss is 5.241473817825318 and perplexity is 188.94837277524127
At time: 894.3110880851746 and batch: 250, loss is 5.284505815505981 and perplexity is 197.25667816565112
At time: 895.1458678245544 and batch: 300, loss is 5.29395830154419 and perplexity is 199.130084386056
At time: 895.9768440723419 and batch: 350, loss is 5.316658668518066 and perplexity is 203.70210734551736
At time: 896.8100035190582 and batch: 400, loss is 5.288738222122192 and perplexity is 198.09331788624354
At time: 897.6452128887177 and batch: 450, loss is 5.2662269878387455 and perplexity is 193.68381073195633
At time: 898.486478805542 and batch: 500, loss is 5.258190584182739 and perplexity is 192.1335271432204
At time: 899.3199353218079 and batch: 550, loss is 5.274634103775025 and perplexity is 195.31899695264687
At time: 900.1950619220734 and batch: 600, loss is 5.301030311584473 and perplexity is 200.54332568039823
At time: 901.0203382968903 and batch: 650, loss is 5.301027450561524 and perplexity is 200.5427519221619
At time: 901.8469612598419 and batch: 700, loss is 5.300855808258056 and perplexity is 200.50833325621238
At time: 902.6698381900787 and batch: 750, loss is 5.270387525558472 and perplexity is 194.4913182000127
At time: 903.5030183792114 and batch: 800, loss is 5.291257295608521 and perplexity is 198.59295856272462
At time: 904.337418794632 and batch: 850, loss is 5.335238485336304 and perplexity is 207.52223391671447
At time: 905.198141336441 and batch: 900, loss is 5.3202379322052 and perplexity is 204.43251728643978
At time: 906.022243976593 and batch: 950, loss is 5.291221008300782 and perplexity is 198.58575228967135
At time: 906.8456342220306 and batch: 1000, loss is 5.278815155029297 and perplexity is 196.13734527583892
At time: 907.673992395401 and batch: 1050, loss is 5.274478492736816 and perplexity is 195.28860552543128
At time: 908.4976215362549 and batch: 1100, loss is 5.253443794250488 and perplexity is 191.22367080778287
At time: 909.3232100009918 and batch: 1150, loss is 5.294618272781372 and perplexity is 199.26154789050273
At time: 910.1483566761017 and batch: 1200, loss is 5.310503129959106 and perplexity is 202.45206246602118
At time: 910.9723477363586 and batch: 1250, loss is 5.286411094665527 and perplexity is 197.63286546074872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.989934990875913 and perplexity of 146.92687159799044
Finished 41 epochs...
Completing Train Step...
At time: 913.2526626586914 and batch: 50, loss is 5.278438272476197 and perplexity is 196.06343846036256
At time: 914.0781857967377 and batch: 100, loss is 5.313005981445312 and perplexity is 202.9594045475473
At time: 914.9042706489563 and batch: 150, loss is 5.211173448562622 and perplexity is 183.3090357610206
At time: 915.7317311763763 and batch: 200, loss is 5.234077091217041 and perplexity is 187.55592942867713
At time: 916.5565073490143 and batch: 250, loss is 5.27701286315918 and perplexity is 195.78416689388635
At time: 917.3790671825409 and batch: 300, loss is 5.286514663696289 and perplexity is 197.65333516506666
At time: 918.202228307724 and batch: 350, loss is 5.309031429290772 and perplexity is 202.15433276861938
At time: 919.0279672145844 and batch: 400, loss is 5.2815799903869625 and perplexity is 196.68038310217838
At time: 919.8550002574921 and batch: 450, loss is 5.258687610626221 and perplexity is 192.22904632270254
At time: 920.6873757839203 and batch: 500, loss is 5.250566358566284 and perplexity is 190.67422786619824
At time: 921.566840171814 and batch: 550, loss is 5.267134141921997 and perplexity is 193.85959150979218
At time: 922.3937294483185 and batch: 600, loss is 5.293261156082154 and perplexity is 198.9913101299134
At time: 923.221170425415 and batch: 650, loss is 5.293562622070312 and perplexity is 199.05130828510386
At time: 924.0490806102753 and batch: 700, loss is 5.293260555267334 and perplexity is 198.99119057302127
At time: 924.8961133956909 and batch: 750, loss is 5.2631133270263675 and perplexity is 193.0816829378097
At time: 925.7414033412933 and batch: 800, loss is 5.284080266952515 and perplexity is 197.17275372982377
At time: 926.5686323642731 and batch: 850, loss is 5.328073759078979 and perplexity is 206.04070762119886
At time: 927.3945758342743 and batch: 900, loss is 5.312869300842285 and perplexity is 202.93166582945935
At time: 928.2491617202759 and batch: 950, loss is 5.284127950668335 and perplexity is 197.18215588354315
At time: 929.0982899665833 and batch: 1000, loss is 5.271481132507324 and perplexity is 194.7041316029858
At time: 929.9366278648376 and batch: 1050, loss is 5.267405290603637 and perplexity is 193.9121634095319
At time: 930.763251543045 and batch: 1100, loss is 5.245909461975097 and perplexity is 189.7883420447454
At time: 931.5927186012268 and batch: 1150, loss is 5.2870896053314205 and perplexity is 197.7670069709713
At time: 932.4161250591278 and batch: 1200, loss is 5.302981433868408 and perplexity is 200.93499220241495
At time: 933.2423663139343 and batch: 1250, loss is 5.279156188964844 and perplexity is 196.20424617369602
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.984868181882984 and perplexity of 146.18430401635823
Finished 42 epochs...
Completing Train Step...
At time: 935.486341714859 and batch: 50, loss is 5.27110613822937 and perplexity is 194.6311323557461
At time: 936.3390164375305 and batch: 100, loss is 5.3057342052459715 and perplexity is 201.48888231435393
At time: 937.1698453426361 and batch: 150, loss is 5.203905553817749 and perplexity is 181.98159467756435
At time: 937.9980037212372 and batch: 200, loss is 5.226800241470337 and perplexity is 186.19606686955697
At time: 938.8325803279877 and batch: 250, loss is 5.269667367935181 and perplexity is 194.35130421666707
At time: 939.6590337753296 and batch: 300, loss is 5.279167413711548 and perplexity is 196.2064485290219
At time: 940.4840884208679 and batch: 350, loss is 5.301511869430542 and perplexity is 200.63992214888341
At time: 941.3095183372498 and batch: 400, loss is 5.274499406814575 and perplexity is 195.29268984922254
At time: 942.1330058574677 and batch: 450, loss is 5.251119365692139 and perplexity is 190.7797012339991
At time: 943.0077509880066 and batch: 500, loss is 5.243143329620361 and perplexity is 189.26408778387727
At time: 943.8315796852112 and batch: 550, loss is 5.26003885269165 and perplexity is 192.48896986655578
At time: 944.6570830345154 and batch: 600, loss is 5.285632343292236 and perplexity is 197.4790185074019
At time: 945.4818232059479 and batch: 650, loss is 5.286190824508667 and perplexity is 197.58933763258972
At time: 946.3065161705017 and batch: 700, loss is 5.285762367248535 and perplexity is 197.5046971800593
At time: 947.128805398941 and batch: 750, loss is 5.255920143127441 and perplexity is 191.69779413533035
At time: 947.9500203132629 and batch: 800, loss is 5.2771132373809815 and perplexity is 195.80381956357363
At time: 948.774320602417 and batch: 850, loss is 5.32102252960205 and perplexity is 204.5929774474151
At time: 949.5975978374481 and batch: 900, loss is 5.305563268661499 and perplexity is 201.45444343651778
At time: 950.4223942756653 and batch: 950, loss is 5.277087154388428 and perplexity is 195.79871248061033
At time: 951.2459630966187 and batch: 1000, loss is 5.264250373840332 and perplexity is 193.3013507128339
At time: 952.0702106952667 and batch: 1050, loss is 5.2603162670135495 and perplexity is 192.54237647114033
At time: 952.8950479030609 and batch: 1100, loss is 5.2384609031677245 and perplexity is 188.37994419677608
At time: 953.7184557914734 and batch: 1150, loss is 5.279741821289062 and perplexity is 196.31918337458924
At time: 954.5421259403229 and batch: 1200, loss is 5.295943565368653 and perplexity is 199.525802811718
At time: 955.367014169693 and batch: 1250, loss is 5.2722015285491945 and perplexity is 194.84444622370887
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.979899831061815 and perplexity of 145.45981036907392
Finished 43 epochs...
Completing Train Step...
At time: 957.5771751403809 and batch: 50, loss is 5.263900947570801 and perplexity is 193.2338179425082
At time: 958.4517226219177 and batch: 100, loss is 5.298665084838867 and perplexity is 200.0695557503686
At time: 959.2763116359711 and batch: 150, loss is 5.196735010147095 and perplexity is 180.68135498957054
At time: 960.0996999740601 and batch: 200, loss is 5.21959231376648 and perplexity is 184.85880431641866
At time: 960.9222722053528 and batch: 250, loss is 5.262366991043091 and perplexity is 192.9376328916746
At time: 961.7455649375916 and batch: 300, loss is 5.271902351379395 and perplexity is 194.7861619328358
At time: 962.569149017334 and batch: 350, loss is 5.294097537994385 and perplexity is 199.15781248246907
At time: 963.4289703369141 and batch: 400, loss is 5.267549467086792 and perplexity is 193.94012299880242
At time: 964.2534804344177 and batch: 450, loss is 5.2436740016937256 and perplexity is 189.3645516040692
At time: 965.0781095027924 and batch: 500, loss is 5.23581316947937 and perplexity is 187.88182400812383
At time: 965.9038052558899 and batch: 550, loss is 5.252604637145996 and perplexity is 191.06327141543483
At time: 966.7347643375397 and batch: 600, loss is 5.278059778213501 and perplexity is 195.9892436158281
At time: 967.5610032081604 and batch: 650, loss is 5.278977890014648 and perplexity is 196.16926628111094
At time: 968.392825126648 and batch: 700, loss is 5.278437080383301 and perplexity is 196.06320473466982
At time: 969.2192916870117 and batch: 750, loss is 5.249005212783813 and perplexity is 190.37678983201437
At time: 970.0477313995361 and batch: 800, loss is 5.270362663269043 and perplexity is 194.48648276067846
At time: 970.8724417686462 and batch: 850, loss is 5.314134788513184 and perplexity is 203.18863591254345
At time: 971.6980457305908 and batch: 900, loss is 5.298467235565186 and perplexity is 200.02997604961422
At time: 972.5205142498016 and batch: 950, loss is 5.270197896957398 and perplexity is 194.45444058005762
At time: 973.3414180278778 and batch: 1000, loss is 5.2571385383605955 and perplexity is 191.931500158152
At time: 974.1643190383911 and batch: 1050, loss is 5.253142290115356 and perplexity is 191.16602477099508
At time: 975.0068073272705 and batch: 1100, loss is 5.231112089157104 and perplexity is 187.00064932169815
At time: 975.8428394794464 and batch: 1150, loss is 5.272355346679688 and perplexity is 194.87441913729367
At time: 976.6706702709198 and batch: 1200, loss is 5.288546075820923 and perplexity is 198.05525864449393
At time: 977.4948444366455 and batch: 1250, loss is 5.265137767791748 and perplexity is 193.47296129409145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.975096319713732 and perplexity of 144.76276798505646
Finished 44 epochs...
Completing Train Step...
At time: 979.7709550857544 and batch: 50, loss is 5.256885185241699 and perplexity is 191.88287987328292
At time: 980.5993885993958 and batch: 100, loss is 5.29175235748291 and perplexity is 198.6912987052503
At time: 981.4291226863861 and batch: 150, loss is 5.1898217868804934 and perplexity is 179.43657213094488
At time: 982.2565941810608 and batch: 200, loss is 5.212597723007202 and perplexity is 183.57030415089457
At time: 983.0807597637177 and batch: 250, loss is 5.255328111648559 and perplexity is 191.58433659530155
At time: 983.9062552452087 and batch: 300, loss is 5.264827470779419 and perplexity is 193.41293652547037
At time: 984.7622802257538 and batch: 350, loss is 5.286854019165039 and perplexity is 197.72042128764906
At time: 985.5863847732544 and batch: 400, loss is 5.260731000900268 and perplexity is 192.62224688063012
At time: 986.4106664657593 and batch: 450, loss is 5.236434965133667 and perplexity is 187.99868443770194
At time: 987.2366414070129 and batch: 500, loss is 5.228650426864624 and perplexity is 186.54088300147401
At time: 988.061363697052 and batch: 550, loss is 5.245613880157471 and perplexity is 189.73225235159458
At time: 988.8860745429993 and batch: 600, loss is 5.270755643844605 and perplexity is 194.5629271902169
At time: 989.7101218700409 and batch: 650, loss is 5.271850357055664 and perplexity is 194.7760344213628
At time: 990.5335392951965 and batch: 700, loss is 5.271227216720581 and perplexity is 194.6546994262983
At time: 991.3657476902008 and batch: 750, loss is 5.242108774185181 and perplexity is 189.06838484340156
At time: 992.1896953582764 and batch: 800, loss is 5.263571252822876 and perplexity is 193.1701202685824
At time: 993.0158519744873 and batch: 850, loss is 5.307340068817139 and perplexity is 201.81270590912527
At time: 993.840416431427 and batch: 900, loss is 5.291430320739746 and perplexity is 198.62732310831962
At time: 994.6670467853546 and batch: 950, loss is 5.263259296417236 and perplexity is 193.10986901055793
At time: 995.4924454689026 and batch: 1000, loss is 5.250267124176025 and perplexity is 190.61718011563465
At time: 996.3193337917328 and batch: 1050, loss is 5.24617341041565 and perplexity is 189.83844299340586
At time: 997.1529223918915 and batch: 1100, loss is 5.223881235122681 and perplexity is 185.6533518481071
At time: 997.9813768863678 and batch: 1150, loss is 5.26511622428894 and perplexity is 193.4687932537039
At time: 998.8095715045929 and batch: 1200, loss is 5.281362609863281 and perplexity is 196.63763326416125
At time: 999.6321375370026 and batch: 1250, loss is 5.258204927444458 and perplexity is 192.13628298444897
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.970346269816377 and perplexity of 144.07676817037216
Finished 45 epochs...
Completing Train Step...
At time: 1001.8691253662109 and batch: 50, loss is 5.250080661773682 and perplexity is 190.58164049180718
At time: 1002.7422788143158 and batch: 100, loss is 5.284944982528686 and perplexity is 197.3433258187131
At time: 1003.5657424926758 and batch: 150, loss is 5.182872095108032 and perplexity is 178.1938664742682
At time: 1004.3889837265015 and batch: 200, loss is 5.20568510055542 and perplexity is 182.30572775012655
At time: 1005.2125959396362 and batch: 250, loss is 5.2483595275878905 and perplexity is 190.25390603356658
At time: 1006.0660181045532 and batch: 300, loss is 5.257833709716797 and perplexity is 192.06497182688008
At time: 1006.8897306919098 and batch: 350, loss is 5.279676198959351 and perplexity is 196.30630087510357
At time: 1007.716964006424 and batch: 400, loss is 5.2540194702148435 and perplexity is 191.33378537099532
At time: 1008.5481612682343 and batch: 450, loss is 5.229460926055908 and perplexity is 186.69213552303145
At time: 1009.3705472946167 and batch: 500, loss is 5.221676435470581 and perplexity is 185.2444743146972
At time: 1010.1931414604187 and batch: 550, loss is 5.238863382339478 and perplexity is 188.45577846052345
At time: 1011.0173299312592 and batch: 600, loss is 5.2634100914001465 and perplexity is 193.13899120564045
At time: 1011.8398213386536 and batch: 650, loss is 5.264838399887085 and perplexity is 193.41505036782885
At time: 1012.6637659072876 and batch: 700, loss is 5.264119691848755 and perplexity is 193.27609135785568
At time: 1013.4878120422363 and batch: 750, loss is 5.235349264144897 and perplexity is 187.79468484144084
At time: 1014.3204462528229 and batch: 800, loss is 5.2567656707763675 and perplexity is 191.85994846383346
At time: 1015.1449935436249 and batch: 850, loss is 5.300599670410156 and perplexity is 200.45698206001921
At time: 1015.9686048030853 and batch: 900, loss is 5.284542541503907 and perplexity is 197.26392274703565
At time: 1016.792564868927 and batch: 950, loss is 5.256620597839356 and perplexity is 191.83211679647457
At time: 1017.6193497180939 and batch: 1000, loss is 5.24330659866333 and perplexity is 189.29499127308102
At time: 1018.4516832828522 and batch: 1050, loss is 5.239550466537476 and perplexity is 188.58530794165915
At time: 1019.2759189605713 and batch: 1100, loss is 5.216761093139649 and perplexity is 184.33616845441554
At time: 1020.1018176078796 and batch: 1150, loss is 5.257953290939331 and perplexity is 192.08794056430455
At time: 1020.9232761859894 and batch: 1200, loss is 5.27432806968689 and perplexity is 195.2592318270683
At time: 1021.7483186721802 and batch: 1250, loss is 5.251427850723267 and perplexity is 190.83856299459205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.965709825501825 and perplexity of 143.41031044866853
Finished 46 epochs...
Completing Train Step...
At time: 1024.0536437034607 and batch: 50, loss is 5.243496074676513 and perplexity is 189.33086153151248
At time: 1024.8808357715607 and batch: 100, loss is 5.27817626953125 and perplexity is 196.01207599094252
At time: 1025.7108800411224 and batch: 150, loss is 5.176172399520874 and perplexity is 177.0040120950978
At time: 1026.539716720581 and batch: 200, loss is 5.19887861251831 and perplexity is 181.06907938556077
At time: 1027.396775484085 and batch: 250, loss is 5.241510028839111 and perplexity is 188.9552149112537
At time: 1028.2204160690308 and batch: 300, loss is 5.250998077392578 and perplexity is 190.75656329165486
At time: 1029.0453162193298 and batch: 350, loss is 5.272683744430542 and perplexity is 194.93842596751168
At time: 1029.8688399791718 and batch: 400, loss is 5.247494411468506 and perplexity is 190.08938548763015
At time: 1030.6928186416626 and batch: 450, loss is 5.222494859695434 and perplexity is 185.3961449369863
At time: 1031.5180282592773 and batch: 500, loss is 5.2147405815124515 and perplexity is 183.9640911026309
At time: 1032.3413560390472 and batch: 550, loss is 5.232162637710571 and perplexity is 187.19720581133083
At time: 1033.1623513698578 and batch: 600, loss is 5.256319446563721 and perplexity is 191.77435500774826
At time: 1033.986876487732 and batch: 650, loss is 5.258021497726441 and perplexity is 192.1010427123956
At time: 1034.8111097812653 and batch: 700, loss is 5.257172756195068 and perplexity is 191.93806775081825
At time: 1035.6342825889587 and batch: 750, loss is 5.228713788986206 and perplexity is 186.55270300204904
At time: 1036.4572575092316 and batch: 800, loss is 5.250195293426514 and perplexity is 190.60348843246507
At time: 1037.280505180359 and batch: 850, loss is 5.294038248062134 and perplexity is 199.14600477930216
At time: 1038.102928161621 and batch: 900, loss is 5.277694168090821 and perplexity is 195.9176010618453
At time: 1038.9271221160889 and batch: 950, loss is 5.249955406188965 and perplexity is 190.5577705719427
At time: 1039.7522768974304 and batch: 1000, loss is 5.236712894439697 and perplexity is 188.05094204322592
At time: 1040.5764999389648 and batch: 1050, loss is 5.232747917175293 and perplexity is 187.30680056039503
At time: 1041.4012744426727 and batch: 1100, loss is 5.209838819503784 and perplexity is 183.0645493807508
At time: 1042.2215881347656 and batch: 1150, loss is 5.250864486694336 and perplexity is 190.7310816912609
At time: 1043.0452783107758 and batch: 1200, loss is 5.267433366775513 and perplexity is 193.91760779718894
At time: 1043.8709206581116 and batch: 1250, loss is 5.244780464172363 and perplexity is 189.57419233361588
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.961184759209626 and perplexity of 142.7628353256508
Finished 47 epochs...
Completing Train Step...
At time: 1046.1124238967896 and batch: 50, loss is 5.236880826950073 and perplexity is 188.08252456179335
At time: 1046.965751171112 and batch: 100, loss is 5.271497583389282 and perplexity is 194.7073346840182
At time: 1047.7893121242523 and batch: 150, loss is 5.169476585388184 and perplexity is 175.82278518055278
At time: 1048.6433453559875 and batch: 200, loss is 5.192239904403687 and perplexity is 179.870995882476
At time: 1049.4694674015045 and batch: 250, loss is 5.234759159088135 and perplexity is 187.68389893915824
At time: 1050.2931423187256 and batch: 300, loss is 5.244246139526367 and perplexity is 189.4729252275718
At time: 1051.1323776245117 and batch: 350, loss is 5.265751447677612 and perplexity is 193.59172819759883
At time: 1051.9778654575348 and batch: 400, loss is 5.240989875793457 and perplexity is 188.85695483807896
At time: 1052.8298654556274 and batch: 450, loss is 5.215565919876099 and perplexity is 184.11598639843646
At time: 1053.6630494594574 and batch: 500, loss is 5.207951183319092 and perplexity is 182.71931605317636
At time: 1054.5088231563568 and batch: 550, loss is 5.225444364547729 and perplexity is 185.94377899371315
At time: 1055.3505358695984 and batch: 600, loss is 5.24928334236145 and perplexity is 190.42974661224437
At time: 1056.1858994960785 and batch: 650, loss is 5.251159200668335 and perplexity is 190.78730109022544
At time: 1057.0239901542664 and batch: 700, loss is 5.250333337783814 and perplexity is 190.62980198470174
At time: 1057.8484835624695 and batch: 750, loss is 5.222162475585938 and perplexity is 185.3345324445213
At time: 1058.673142194748 and batch: 800, loss is 5.243589696884155 and perplexity is 189.3485879345233
At time: 1059.4995107650757 and batch: 850, loss is 5.287555789947509 and perplexity is 197.85922440069453
At time: 1060.3244149684906 and batch: 900, loss is 5.270972204208374 and perplexity is 194.6050663711788
At time: 1061.1492264270782 and batch: 950, loss is 5.243466882705689 and perplexity is 189.32533467119683
At time: 1061.9789905548096 and batch: 1000, loss is 5.229955682754516 and perplexity is 186.78452556106944
At time: 1062.806144475937 and batch: 1050, loss is 5.226225070953369 and perplexity is 186.0890031744102
At time: 1063.6291348934174 and batch: 1100, loss is 5.203110542297363 and perplexity is 181.83697470818555
At time: 1064.4542820453644 and batch: 1150, loss is 5.2442331123352055 and perplexity is 189.4704569436324
At time: 1065.28595328331 and batch: 1200, loss is 5.260955753326416 and perplexity is 192.66554406333782
At time: 1066.1106531620026 and batch: 1250, loss is 5.238358116149902 and perplexity is 188.36058217919359
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.956714936416515 and perplexity of 142.12613477967915
Finished 48 epochs...
Completing Train Step...
At time: 1068.351143360138 and batch: 50, loss is 5.230790901184082 and perplexity is 186.94059660680975
At time: 1069.2055265903473 and batch: 100, loss is 5.264956407546997 and perplexity is 193.4378761720978
At time: 1070.0296621322632 and batch: 150, loss is 5.162949190139771 and perplexity is 174.67885786246862
At time: 1070.8901674747467 and batch: 200, loss is 5.185685615539551 and perplexity is 178.69592450243832
At time: 1071.72274518013 and batch: 250, loss is 5.2281873989105225 and perplexity is 186.45452935168134
At time: 1072.5648219585419 and batch: 300, loss is 5.237653465270996 and perplexity is 188.22790048203706
At time: 1073.401317358017 and batch: 350, loss is 5.25899642944336 and perplexity is 192.28841943670324
At time: 1074.2267889976501 and batch: 400, loss is 5.234640216827392 and perplexity is 187.66157671946735
At time: 1075.0617368221283 and batch: 450, loss is 5.20873456954956 and perplexity is 182.86251193092923
At time: 1075.8979947566986 and batch: 500, loss is 5.201254215240478 and perplexity is 181.49973891889425
At time: 1076.7212798595428 and batch: 550, loss is 5.218748369216919 and perplexity is 184.7028595497013
At time: 1077.5441632270813 and batch: 600, loss is 5.242313060760498 and perplexity is 189.10701292170657
At time: 1078.3670864105225 and batch: 650, loss is 5.24444688796997 and perplexity is 189.51096544054528
At time: 1079.1924893856049 and batch: 700, loss is 5.243577833175659 and perplexity is 189.34634157139712
At time: 1080.0183248519897 and batch: 750, loss is 5.215732402801514 and perplexity is 184.1466411181407
At time: 1080.8429534435272 and batch: 800, loss is 5.237204465866089 and perplexity is 188.1434052373106
At time: 1081.674756526947 and batch: 850, loss is 5.281139726638794 and perplexity is 196.59381091821808
At time: 1082.5161442756653 and batch: 900, loss is 5.264449100494385 and perplexity is 193.33976866069378
At time: 1083.3550708293915 and batch: 950, loss is 5.237084016799927 and perplexity is 188.1207449045806
At time: 1084.1785836219788 and batch: 1000, loss is 5.223482999801636 and perplexity is 185.57943284548656
At time: 1085.0011403560638 and batch: 1050, loss is 5.219700908660888 and perplexity is 184.8788801287995
At time: 1085.8275682926178 and batch: 1100, loss is 5.196295166015625 and perplexity is 180.60190083091604
At time: 1086.663754940033 and batch: 1150, loss is 5.237502908706665 and perplexity is 188.19956366922935
At time: 1087.510624408722 and batch: 1200, loss is 5.254197273254395 and perplexity is 191.36780812418795
At time: 1088.3590009212494 and batch: 1250, loss is 5.231896448135376 and perplexity is 187.14738249815562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.952386786467837 and perplexity of 141.51232085428992
Finished 49 epochs...
Completing Train Step...
At time: 1090.711753129959 and batch: 50, loss is 5.2237194156646725 and perplexity is 185.62331195391883
At time: 1091.5400123596191 and batch: 100, loss is 5.258500089645386 and perplexity is 192.19300272296238
At time: 1092.3606307506561 and batch: 150, loss is 5.15648458480835 and perplexity is 173.55327014678005
At time: 1093.1915464401245 and batch: 200, loss is 5.179277038574218 and perplexity is 177.55439959882733
At time: 1094.047390460968 and batch: 250, loss is 5.22177264213562 and perplexity is 185.2622969251011
At time: 1094.8791031837463 and batch: 300, loss is 5.231197214126587 and perplexity is 187.01656842381172
At time: 1095.703854560852 and batch: 350, loss is 5.2523909950256344 and perplexity is 191.0224566130421
At time: 1096.5282747745514 and batch: 400, loss is 5.228418922424316 and perplexity is 186.49770295713913
At time: 1097.3537874221802 and batch: 450, loss is 5.202011337280274 and perplexity is 181.63720840546904
At time: 1098.1769516468048 and batch: 500, loss is 5.194702053070069 and perplexity is 180.31441066766493
At time: 1099.0137948989868 and batch: 550, loss is 5.212227058410645 and perplexity is 183.5022737471765
At time: 1099.8459413051605 and batch: 600, loss is 5.235510082244873 and perplexity is 187.82488805438908
At time: 1100.6697437763214 and batch: 650, loss is 5.237865324020386 and perplexity is 188.26778243415458
At time: 1101.4928376674652 and batch: 700, loss is 5.236983604431153 and perplexity is 188.10185620331458
At time: 1102.318707227707 and batch: 750, loss is 5.2094251251220705 and perplexity is 182.98883226813294
At time: 1103.1414737701416 and batch: 800, loss is 5.230881443023682 and perplexity is 186.9575233185987
At time: 1103.9670023918152 and batch: 850, loss is 5.27488184928894 and perplexity is 195.36739235254842
At time: 1104.7922067642212 and batch: 900, loss is 5.2579682922363284 and perplexity is 192.09082215416433
At time: 1105.6143374443054 and batch: 950, loss is 5.230795984268188 and perplexity is 186.9415468440003
At time: 1106.4361040592194 and batch: 1000, loss is 5.217268075942993 and perplexity is 184.42964741596703
At time: 1107.2591524124146 and batch: 1050, loss is 5.21363995552063 and perplexity is 183.76172682652094
At time: 1108.0852437019348 and batch: 1100, loss is 5.1896787452697755 and perplexity is 179.41090707027513
At time: 1108.9108772277832 and batch: 1150, loss is 5.230756759643555 and perplexity is 186.93421427580645
At time: 1109.7362203598022 and batch: 1200, loss is 5.247505016326905 and perplexity is 190.0914013693354
At time: 1110.5608723163605 and batch: 1250, loss is 5.225547027587891 and perplexity is 185.9628695272931
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.948111206945712 and perplexity of 140.90856529531894
Finished Training.
Improved accuracyfrom -154.66225310466675 to -140.90856529531894
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f10b4e20a90>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'seq_len': 35, 'num_layers': 1, 'data': 'wikitext', 'dropout': 0.7870759554568665, 'wordvec_source': '', 'lr': 0.701859308968518, 'tune_wordvecs': True, 'batch_size': 50, 'anneal': 2.685593450441546}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.4033839702606201 and batch: 50, loss is 9.473598289489747 and perplexity is 13011.622682148098
At time: 2.2233757972717285 and batch: 100, loss is 7.9808749580383305 and perplexity is 2924.488749924167
At time: 3.045105457305908 and batch: 150, loss is 7.251496267318726 and perplexity is 1410.2133265006123
At time: 3.893157958984375 and batch: 200, loss is 7.021269388198853 and perplexity is 1120.207694034003
At time: 4.714877128601074 and batch: 250, loss is 6.971973476409912 and perplexity is 1066.3250427609232
At time: 5.5389182567596436 and batch: 300, loss is 6.878718481063843 and perplexity is 971.380719020599
At time: 6.359687328338623 and batch: 350, loss is 6.837587251663208 and perplexity is 932.2371654336872
At time: 7.184494733810425 and batch: 400, loss is 6.747051000595093 and perplexity is 851.5438537984722
At time: 8.007073402404785 and batch: 450, loss is 6.7074049282073975 and perplexity is 818.4439603662959
At time: 8.830646753311157 and batch: 500, loss is 6.673917255401611 and perplexity is 791.4900082308187
At time: 9.65475869178772 and batch: 550, loss is 6.6620003128051755 and perplexity is 782.1138457934076
At time: 10.533033847808838 and batch: 600, loss is 6.667528810501099 and perplexity is 786.449734820467
At time: 11.355968952178955 and batch: 650, loss is 6.632670984268189 and perplexity is 759.5080980086498
At time: 12.17815089225769 and batch: 700, loss is 6.61694296836853 and perplexity is 747.6559919443229
At time: 13.002300262451172 and batch: 750, loss is 6.5380282211303715 and perplexity is 690.9228868807206
At time: 13.862222671508789 and batch: 800, loss is 6.521513605117798 and perplexity is 679.606262760343
At time: 14.709551811218262 and batch: 850, loss is 6.559220294952393 and perplexity is 705.7212256197151
At time: 15.532967805862427 and batch: 900, loss is 6.543948163986206 and perplexity is 695.0252417612505
At time: 16.357106924057007 and batch: 950, loss is 6.507003870010376 and perplexity is 669.8165507282324
At time: 17.20815348625183 and batch: 1000, loss is 6.501158094406128 and perplexity is 665.9123760593179
At time: 18.04749846458435 and batch: 1050, loss is 6.46668960571289 and perplexity is 643.350454257945
At time: 18.8978111743927 and batch: 1100, loss is 6.448435487747193 and perplexity is 631.7131962513646
At time: 19.72525191307068 and batch: 1150, loss is 6.488198194503784 and perplexity is 657.3379005073216
At time: 20.54961919784546 and batch: 1200, loss is 6.482687330245971 and perplexity is 653.7253638084078
At time: 21.38373303413391 and batch: 1250, loss is 6.447519445419312 and perplexity is 631.1347851894723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.790813808023494 and perplexity of 327.27925851604147
Finished 1 epochs...
Completing Train Step...
At time: 23.67652940750122 and batch: 50, loss is 6.120189399719238 and perplexity is 454.95085390398043
At time: 24.495655298233032 and batch: 100, loss is 6.05677716255188 and perplexity is 426.9970747710499
At time: 25.32154417037964 and batch: 150, loss is 5.916744651794434 and perplexity is 371.20135521001646
At time: 26.14164924621582 and batch: 200, loss is 5.890305061340332 and perplexity is 361.5155519710752
At time: 26.963759422302246 and batch: 250, loss is 5.900261421203613 and perplexity is 365.1329088741501
At time: 27.78546643257141 and batch: 300, loss is 5.872946872711181 and perplexity is 355.2944465921971
At time: 28.60687518119812 and batch: 350, loss is 5.883740921020507 and perplexity is 359.1502846263789
At time: 29.427064180374146 and batch: 400, loss is 5.8181274795532225 and perplexity is 336.3416569691733
At time: 30.247950315475464 and batch: 450, loss is 5.785787372589112 and perplexity is 325.63833789969084
At time: 31.071280479431152 and batch: 500, loss is 5.763703031539917 and perplexity is 318.5256582875295
At time: 31.89400577545166 and batch: 550, loss is 5.762869720458984 and perplexity is 318.26033788948433
At time: 32.715497970581055 and batch: 600, loss is 5.776966543197632 and perplexity is 322.7785689870779
At time: 33.539451122283936 and batch: 650, loss is 5.749517164230347 and perplexity is 314.03899441421765
At time: 34.38366508483887 and batch: 700, loss is 5.7455230331420895 and perplexity is 312.78718311709906
At time: 35.202266454696655 and batch: 750, loss is 5.6890499305725095 and perplexity is 295.6126346569558
At time: 36.02673292160034 and batch: 800, loss is 5.680127010345459 and perplexity is 292.9866398938227
At time: 36.85107231140137 and batch: 850, loss is 5.721310939788818 and perplexity is 305.30489706592465
At time: 37.673258543014526 and batch: 900, loss is 5.704603643417358 and perplexity is 300.2464518037594
At time: 38.49403262138367 and batch: 950, loss is 5.665558414459229 and perplexity is 288.7491778211268
At time: 39.3150999546051 and batch: 1000, loss is 5.643095254898071 and perplexity is 282.3352670167102
At time: 40.13708710670471 and batch: 1050, loss is 5.625554752349854 and perplexity is 277.42614464770946
At time: 40.97148013114929 and batch: 1100, loss is 5.602929124832153 and perplexity is 271.219681363433
At time: 41.7932562828064 and batch: 1150, loss is 5.64654990196228 and perplexity is 283.3123224371228
At time: 42.61568355560303 and batch: 1200, loss is 5.642175321578979 and perplexity is 282.07565682776675
At time: 43.43810176849365 and batch: 1250, loss is 5.604831562042237 and perplexity is 271.73615089702497
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.23774752651688 and perplexity of 188.24560626556746
Finished 2 epochs...
Completing Train Step...
At time: 45.82378530502319 and batch: 50, loss is 5.566120519638061 and perplexity is 261.41796361950037
At time: 46.66250514984131 and batch: 100, loss is 5.58562816619873 and perplexity is 266.5676789632347
At time: 47.481457233428955 and batch: 150, loss is 5.484729156494141 and perplexity is 240.98366629737103
At time: 48.30475425720215 and batch: 200, loss is 5.499377355575562 and perplexity is 244.53962361887613
At time: 49.12760663032532 and batch: 250, loss is 5.532220277786255 and perplexity is 252.70436249684096
At time: 49.97648024559021 and batch: 300, loss is 5.535998239517212 and perplexity is 253.66087560486648
At time: 50.80005669593811 and batch: 350, loss is 5.558525972366333 and perplexity is 259.4401324097184
At time: 51.622687339782715 and batch: 400, loss is 5.518485107421875 and perplexity is 249.25715322369007
At time: 52.446215867996216 and batch: 450, loss is 5.491661405563354 and perplexity is 242.66002886189165
At time: 53.27302384376526 and batch: 500, loss is 5.483007326126098 and perplexity is 240.56909031971648
At time: 54.09847664833069 and batch: 550, loss is 5.492674140930176 and perplexity is 242.9059037373104
At time: 54.922664642333984 and batch: 600, loss is 5.51348198890686 and perplexity is 248.01320455150585
At time: 55.74509859085083 and batch: 650, loss is 5.495351781845093 and perplexity is 243.55719008999438
At time: 56.565021276474 and batch: 700, loss is 5.496356477737427 and perplexity is 243.80201396460274
At time: 57.391005754470825 and batch: 750, loss is 5.455978631973267 and perplexity is 234.15390948151799
At time: 58.221242904663086 and batch: 800, loss is 5.456889743804932 and perplexity is 234.36734709688062
At time: 59.03933525085449 and batch: 850, loss is 5.503051509857178 and perplexity is 245.43975251764343
At time: 59.85861659049988 and batch: 900, loss is 5.488355979919434 and perplexity is 241.85925835251606
At time: 60.678043842315674 and batch: 950, loss is 5.457399883270264 and perplexity is 234.486937631352
At time: 61.5451762676239 and batch: 1000, loss is 5.440359334945679 and perplexity is 230.52500426943234
At time: 62.36498832702637 and batch: 1050, loss is 5.427003602981568 and perplexity is 227.46664289007953
At time: 63.191460847854614 and batch: 1100, loss is 5.40804744720459 and perplexity is 223.195361215072
At time: 64.01287460327148 and batch: 1150, loss is 5.447953577041626 and perplexity is 232.2823312986939
At time: 64.83695435523987 and batch: 1200, loss is 5.451781244277954 and perplexity is 233.1731345293548
At time: 65.6608316898346 and batch: 1250, loss is 5.422894487380981 and perplexity is 226.53387389944263
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.096259569599681 and perplexity of 163.40954074272122
Finished 3 epochs...
Completing Train Step...
At time: 67.93864393234253 and batch: 50, loss is 5.394814834594727 and perplexity is 220.26135853657706
At time: 68.78599858283997 and batch: 100, loss is 5.423685579299927 and perplexity is 226.7131539205901
At time: 69.64298105239868 and batch: 150, loss is 5.323829717636109 and perplexity is 205.16811528790132
At time: 70.47689533233643 and batch: 200, loss is 5.341270055770874 and perplexity is 208.77770130116144
At time: 71.299551486969 and batch: 250, loss is 5.375289077758789 and perplexity is 216.00230474841473
At time: 72.1227035522461 and batch: 300, loss is 5.379581241607666 and perplexity is 216.93141455149433
At time: 72.95258641242981 and batch: 350, loss is 5.401089286804199 and perplexity is 221.64772269385065
At time: 73.78675580024719 and batch: 400, loss is 5.371061506271363 and perplexity is 215.09106708199394
At time: 74.62007665634155 and batch: 450, loss is 5.3424383068084715 and perplexity is 209.0217485938287
At time: 75.44534111022949 and batch: 500, loss is 5.336930112838745 and perplexity is 207.8735813256612
At time: 76.26492285728455 and batch: 550, loss is 5.349466629028321 and perplexity is 210.4959954662801
At time: 77.08313584327698 and batch: 600, loss is 5.370328340530396 and perplexity is 214.9334274754622
At time: 77.90149283409119 and batch: 650, loss is 5.354244432449341 and perplexity is 211.504110323116
At time: 78.74117064476013 and batch: 700, loss is 5.354074096679687 and perplexity is 211.46808667584418
At time: 79.57585787773132 and batch: 750, loss is 5.319949846267701 and perplexity is 204.37363163551322
At time: 80.39789962768555 and batch: 800, loss is 5.327382431030274 and perplexity is 205.8983151264792
At time: 81.22025513648987 and batch: 850, loss is 5.374044246673584 and perplexity is 215.73358565462925
At time: 82.09015321731567 and batch: 900, loss is 5.359396371841431 and perplexity is 212.59657842901336
At time: 82.91286993026733 and batch: 950, loss is 5.329972867965698 and perplexity is 206.4323731500959
At time: 83.73678660392761 and batch: 1000, loss is 5.316413145065308 and perplexity is 203.6520998400467
At time: 84.55915021896362 and batch: 1050, loss is 5.302945852279663 and perplexity is 200.9278427433533
At time: 85.38157725334167 and batch: 1100, loss is 5.2848805809021 and perplexity is 197.33061699677324
At time: 86.2025854587555 and batch: 1150, loss is 5.322522201538086 and perplexity is 204.9000299754854
At time: 87.02361226081848 and batch: 1200, loss is 5.329237432479858 and perplexity is 206.28061126983147
At time: 87.84201741218567 and batch: 1250, loss is 5.305177345275879 and perplexity is 201.37671245572497
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.010825498260721 and perplexity of 150.02853333816367
Finished 4 epochs...
Completing Train Step...
At time: 90.11203980445862 and batch: 50, loss is 5.2793010807037355 and perplexity is 196.23267660771975
At time: 90.93821430206299 and batch: 100, loss is 5.314147891998291 and perplexity is 203.1912984092521
At time: 91.76374411582947 and batch: 150, loss is 5.214421653747559 and perplexity is 183.90542920118685
At time: 92.58667135238647 and batch: 200, loss is 5.234390802383423 and perplexity is 187.6147770481551
At time: 93.4264645576477 and batch: 250, loss is 5.267586889266968 and perplexity is 193.9473807968291
At time: 94.27298331260681 and batch: 300, loss is 5.272686281204224 and perplexity is 194.93892048280748
At time: 95.10122847557068 and batch: 350, loss is 5.293218564987183 and perplexity is 198.98283505260795
At time: 95.92650508880615 and batch: 400, loss is 5.269122953414917 and perplexity is 194.24552534100982
At time: 96.75318717956543 and batch: 450, loss is 5.236258764266967 and perplexity is 187.96556182476783
At time: 97.57678866386414 and batch: 500, loss is 5.234537630081177 and perplexity is 187.64232611636734
At time: 98.39899897575378 and batch: 550, loss is 5.246770009994507 and perplexity is 189.95173431996523
At time: 99.22292017936707 and batch: 600, loss is 5.267874536514282 and perplexity is 194.0031772515026
At time: 100.04202175140381 and batch: 650, loss is 5.254613418579101 and perplexity is 191.4474615153818
At time: 100.88676857948303 and batch: 700, loss is 5.254198474884033 and perplexity is 191.3680380775562
At time: 101.71451473236084 and batch: 750, loss is 5.224252510070801 and perplexity is 185.7222930839677
At time: 102.53881001472473 and batch: 800, loss is 5.2341259098052975 and perplexity is 187.56508586787135
At time: 103.4096622467041 and batch: 850, loss is 5.281151132583618 and perplexity is 196.59605326916622
At time: 104.25311088562012 and batch: 900, loss is 5.2646107006072995 and perplexity is 193.3710149137712
At time: 105.07788467407227 and batch: 950, loss is 5.23681037902832 and perplexity is 188.06927500552723
At time: 105.90644216537476 and batch: 1000, loss is 5.223911466598511 and perplexity is 185.65896450776538
At time: 106.72879719734192 and batch: 1050, loss is 5.211087074279785 and perplexity is 183.29320325828954
At time: 107.56509351730347 and batch: 1100, loss is 5.19119384765625 and perplexity is 179.68293898980392
At time: 108.39553475379944 and batch: 1150, loss is 5.229059867858886 and perplexity is 186.61727612425165
At time: 109.21970748901367 and batch: 1200, loss is 5.236019849777222 and perplexity is 187.9206594925972
At time: 110.04281163215637 and batch: 1250, loss is 5.2151776313781735 and perplexity is 184.0445101562332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.948985301665146 and perplexity of 141.0317865738555
Finished 5 epochs...
Completing Train Step...
At time: 112.3231692314148 and batch: 50, loss is 5.1913346004486085 and perplexity is 179.70823164517003
At time: 113.14673590660095 and batch: 100, loss is 5.229581212997436 and perplexity is 186.71459349969447
At time: 113.96933054924011 and batch: 150, loss is 5.1293979930877684 and perplexity is 168.9153991923369
At time: 114.79270386695862 and batch: 200, loss is 5.150762920379639 and perplexity is 172.56309200933185
At time: 115.6180636882782 and batch: 250, loss is 5.18248254776001 and perplexity is 178.12446504459837
At time: 116.4401957988739 and batch: 300, loss is 5.189758977890015 and perplexity is 179.42530225492283
At time: 117.26752996444702 and batch: 350, loss is 5.208457174301148 and perplexity is 182.81179377381997
At time: 118.12501239776611 and batch: 400, loss is 5.189119396209716 and perplexity is 179.31058180907704
At time: 118.98354911804199 and batch: 450, loss is 5.152703485488892 and perplexity is 172.89828705354418
At time: 119.82829141616821 and batch: 500, loss is 5.152265892028809 and perplexity is 172.82264444543242
At time: 120.65415930747986 and batch: 550, loss is 5.165030946731568 and perplexity is 175.04287549285988
At time: 121.47983121871948 and batch: 600, loss is 5.185234375 and perplexity is 178.61530784716973
At time: 122.30766153335571 and batch: 650, loss is 5.175252780914307 and perplexity is 176.84131073520336
At time: 123.13340210914612 and batch: 700, loss is 5.175053634643555 and perplexity is 176.8060969541185
At time: 123.95774149894714 and batch: 750, loss is 5.148338260650635 and perplexity is 172.14519206674055
At time: 124.83294868469238 and batch: 800, loss is 5.159291076660156 and perplexity is 174.04103011233656
At time: 125.65726375579834 and batch: 850, loss is 5.20649935722351 and perplexity is 182.4542318566026
At time: 126.4812695980072 and batch: 900, loss is 5.187294988632202 and perplexity is 178.98374445782173
At time: 127.30668044090271 and batch: 950, loss is 5.161061210632324 and perplexity is 174.34937888109064
At time: 128.13227033615112 and batch: 1000, loss is 5.148649578094482 and perplexity is 172.19879221080237
At time: 128.95752787590027 and batch: 1050, loss is 5.137048997879028 and perplexity is 170.21272832802356
At time: 129.7832567691803 and batch: 1100, loss is 5.115229148864746 and perplexity is 166.53893882000574
At time: 130.61037611961365 and batch: 1150, loss is 5.153956327438355 and perplexity is 173.1150370289349
At time: 131.43494319915771 and batch: 1200, loss is 5.160224609375 and perplexity is 174.203578968204
At time: 132.26624155044556 and batch: 1250, loss is 5.1416254901885985 and perplexity is 170.99349078418723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.900515368385037 and perplexity of 134.35900622887246
Finished 6 epochs...
Completing Train Step...
At time: 134.52625703811646 and batch: 50, loss is 5.119058027267456 and perplexity is 167.17781848181914
At time: 135.37607264518738 and batch: 100, loss is 5.158901138305664 and perplexity is 173.9731780693572
At time: 136.20253038406372 and batch: 150, loss is 5.0589672946929936 and perplexity is 157.42785576549278
At time: 137.02851819992065 and batch: 200, loss is 5.081720514297485 and perplexity is 161.05090808625081
At time: 137.85177159309387 and batch: 250, loss is 5.112699184417725 and perplexity is 166.11813376112016
At time: 138.6770932674408 and batch: 300, loss is 5.121501245498657 and perplexity is 167.58676975095472
At time: 139.5015869140625 and batch: 350, loss is 5.137528495788574 and perplexity is 170.29436454606878
At time: 140.32170724868774 and batch: 400, loss is 5.122453107833862 and perplexity is 167.74636522937166
At time: 141.14350962638855 and batch: 450, loss is 5.083697195053101 and perplexity is 161.3695691587863
At time: 141.96593141555786 and batch: 500, loss is 5.085537977218628 and perplexity is 161.66688895028233
At time: 142.7879934310913 and batch: 550, loss is 5.098041076660156 and perplexity is 163.70091545929776
At time: 143.61016249656677 and batch: 600, loss is 5.116714649200439 and perplexity is 166.78651631222513
At time: 144.43608593940735 and batch: 650, loss is 5.109106273651123 and perplexity is 165.52235705697646
At time: 145.261296749115 and batch: 700, loss is 5.108875522613525 and perplexity is 165.48416700770616
At time: 146.13661527633667 and batch: 750, loss is 5.086569452285767 and perplexity is 161.83373034699443
At time: 146.96324825286865 and batch: 800, loss is 5.097013120651245 and perplexity is 163.53272458080977
At time: 147.78598499298096 and batch: 850, loss is 5.14428032875061 and perplexity is 171.44805402639972
At time: 148.60978865623474 and batch: 900, loss is 5.12324857711792 and perplexity is 167.87985539704897
At time: 149.43384408950806 and batch: 950, loss is 5.098436422348023 and perplexity is 163.76564670509077
At time: 150.2593972682953 and batch: 1000, loss is 5.085887336730957 and perplexity is 161.7233786828039
At time: 151.08404994010925 and batch: 1050, loss is 5.074725074768066 and perplexity is 159.92821763250936
At time: 151.9075973033905 and batch: 1100, loss is 5.050137996673584 and perplexity is 156.0439965531411
At time: 152.73150277137756 and batch: 1150, loss is 5.089898500442505 and perplexity is 162.37338039080396
At time: 153.55695104599 and batch: 1200, loss is 5.095540761947632 and perplexity is 163.29212291988793
At time: 154.38125658035278 and batch: 1250, loss is 5.078933162689209 and perplexity is 160.60262762644902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.862908801893248 and perplexity of 129.40005432555373
Finished 7 epochs...
Completing Train Step...
At time: 156.662579536438 and batch: 50, loss is 5.056978378295899 and perplexity is 157.1150560911161
At time: 157.48729848861694 and batch: 100, loss is 5.0977896785736085 and perplexity is 163.6597665349824
At time: 158.31465411186218 and batch: 150, loss is 4.99765398979187 and perplexity is 148.06538841259953
At time: 159.13475942611694 and batch: 200, loss is 5.02218861579895 and perplexity is 151.74304786340954
At time: 159.96539282798767 and batch: 250, loss is 5.053218288421631 and perplexity is 156.52539863685254
At time: 160.8052487373352 and batch: 300, loss is 5.062859258651733 and perplexity is 158.0417531645982
At time: 161.64398097991943 and batch: 350, loss is 5.076342372894287 and perplexity is 160.1870785103985
At time: 162.48992371559143 and batch: 400, loss is 5.064859991073608 and perplexity is 158.35826895027924
At time: 163.32850742340088 and batch: 450, loss is 5.02372254371643 and perplexity is 151.9759893728783
At time: 164.15228462219238 and batch: 500, loss is 5.028306798934937 and perplexity is 152.67428545606666
At time: 164.97694206237793 and batch: 550, loss is 5.040666570663452 and perplexity is 154.5730145307006
At time: 165.81598019599915 and batch: 600, loss is 5.0571722984313965 and perplexity is 157.14552681843034
At time: 166.64106702804565 and batch: 650, loss is 5.051526517868042 and perplexity is 156.2608174449892
At time: 167.5151937007904 and batch: 700, loss is 5.05133810043335 and perplexity is 156.23137795616694
At time: 168.33864331245422 and batch: 750, loss is 5.031912546157837 and perplexity is 153.2257840215471
At time: 169.16586780548096 and batch: 800, loss is 5.043051929473877 and perplexity is 154.9421667379625
At time: 169.99209880828857 and batch: 850, loss is 5.089875192642212 and perplexity is 162.36959586858546
At time: 170.81830096244812 and batch: 900, loss is 5.068024950027466 and perplexity is 158.86026034355248
At time: 171.6445755958557 and batch: 950, loss is 5.0445842456817624 and perplexity is 155.1797691258443
At time: 172.4690706729889 and batch: 1000, loss is 5.031490154266358 and perplexity is 153.16107635976869
At time: 173.30678033828735 and batch: 1050, loss is 5.021110286712647 and perplexity is 151.57950711247534
At time: 174.12965488433838 and batch: 1100, loss is 4.994135160446167 and perplexity is 147.54528718915876
At time: 174.95432472229004 and batch: 1150, loss is 5.034376859664917 and perplexity is 153.60384603107673
At time: 175.7979166507721 and batch: 1200, loss is 5.040243635177612 and perplexity is 154.50765394033303
At time: 176.62095427513123 and batch: 1250, loss is 5.025413427352905 and perplexity is 152.23318046527316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.829705231381158 and perplexity of 125.17405783199341
Finished 8 epochs...
Completing Train Step...
At time: 178.88654804229736 and batch: 50, loss is 5.002648525238037 and perplexity is 148.80675609582175
At time: 179.75661063194275 and batch: 100, loss is 5.043200969696045 and perplexity is 154.9652610738661
At time: 180.5805425643921 and batch: 150, loss is 4.943925828933716 and perplexity is 140.32004213948346
At time: 181.4179539680481 and batch: 200, loss is 4.969911375045776 and perplexity is 144.0141235602238
At time: 182.2430956363678 and batch: 250, loss is 5.001391897201538 and perplexity is 148.61987879630996
At time: 183.06904315948486 and batch: 300, loss is 5.011428050994873 and perplexity is 150.118960682013
At time: 183.89366555213928 and batch: 350, loss is 5.023015756607055 and perplexity is 151.86861265336563
At time: 184.71927118301392 and batch: 400, loss is 5.013993272781372 and perplexity is 150.5045434517423
At time: 185.54690170288086 and batch: 450, loss is 4.971273937225342 and perplexity is 144.2104855056103
At time: 186.37059092521667 and batch: 500, loss is 4.977971429824829 and perplexity is 145.1795757800336
At time: 187.19447684288025 and batch: 550, loss is 4.989843454360962 and perplexity is 146.91342303973818
At time: 188.06694388389587 and batch: 600, loss is 5.004732961654663 and perplexity is 149.1172578153895
At time: 188.89294505119324 and batch: 650, loss is 5.000938129425049 and perplexity is 148.552455182855
At time: 189.7197036743164 and batch: 700, loss is 5.001213512420654 and perplexity is 148.59336963628158
At time: 190.54627108573914 and batch: 750, loss is 4.984634075164795 and perplexity is 146.1500852942669
At time: 191.37295794487 and batch: 800, loss is 4.995996723175049 and perplexity is 147.82020780820835
At time: 192.19850826263428 and batch: 850, loss is 5.042266511917115 and perplexity is 154.82052021783352
At time: 193.0253701210022 and batch: 900, loss is 5.019717578887939 and perplexity is 151.3685480830986
At time: 193.86732387542725 and batch: 950, loss is 4.997491025924683 and perplexity is 148.04126107030314
At time: 194.6970715522766 and batch: 1000, loss is 4.983680763244629 and perplexity is 146.01082506558726
At time: 195.53492641448975 and batch: 1050, loss is 4.974296522140503 and perplexity is 144.64703336283253
At time: 196.37620544433594 and batch: 1100, loss is 4.944873352050781 and perplexity is 140.45306163276857
At time: 197.2075710296631 and batch: 1150, loss is 4.985545635223389 and perplexity is 146.28337061413882
At time: 198.0319573879242 and batch: 1200, loss is 4.991536083221436 and perplexity is 147.1623035112807
At time: 198.86012148857117 and batch: 1250, loss is 4.978272504806519 and perplexity is 145.22329229879807
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.802512509979471 and perplexity of 121.81609750656474
Finished 9 epochs...
Completing Train Step...
At time: 201.1786971092224 and batch: 50, loss is 4.953834438323975 and perplexity is 141.7173297832162
At time: 202.00266861915588 and batch: 100, loss is 4.9949173164367675 and perplexity is 147.66073576292982
At time: 202.8405864238739 and batch: 150, loss is 4.8962568664550785 and perplexity is 133.78805470346853
At time: 203.6732532978058 and batch: 200, loss is 4.924063863754273 and perplexity is 137.56050597277547
At time: 204.51770162582397 and batch: 250, loss is 4.955375394821167 and perplexity is 141.93587836699427
At time: 205.34972739219666 and batch: 300, loss is 4.966300640106201 and perplexity is 143.4950643889673
At time: 206.17495107650757 and batch: 350, loss is 4.97614893913269 and perplexity is 144.91522831305335
At time: 207.0004050731659 and batch: 400, loss is 4.9682271194458005 and perplexity is 143.7717711151637
At time: 207.82421207427979 and batch: 450, loss is 4.9237916469573975 and perplexity is 137.5230647887538
At time: 208.64961433410645 and batch: 500, loss is 4.932589073181152 and perplexity is 138.7382512243988
At time: 209.5100257396698 and batch: 550, loss is 4.943960380554199 and perplexity is 140.32489050808474
At time: 210.3326690196991 and batch: 600, loss is 4.958314733505249 and perplexity is 142.35368972867246
At time: 211.15769839286804 and batch: 650, loss is 4.955888805389404 and perplexity is 142.00876845662995
At time: 212.02672171592712 and batch: 700, loss is 4.956513633728028 and perplexity is 142.09752728612298
At time: 212.86260056495667 and batch: 750, loss is 4.942940826416016 and perplexity is 140.18189459370208
At time: 213.69278407096863 and batch: 800, loss is 4.954055948257446 and perplexity is 141.7487250565623
At time: 214.51991057395935 and batch: 850, loss is 4.999345750808716 and perplexity is 148.31609166986487
At time: 215.351496219635 and batch: 900, loss is 4.9767424011230466 and perplexity is 145.00125551729403
At time: 216.1897451877594 and batch: 950, loss is 4.954755601882934 and perplexity is 141.84793476812655
At time: 217.03602576255798 and batch: 1000, loss is 4.941264228820801 and perplexity is 139.9470628804854
At time: 217.86232018470764 and batch: 1050, loss is 4.932405891418457 and perplexity is 138.71283923456394
At time: 218.69047713279724 and batch: 1100, loss is 4.901203556060791 and perplexity is 134.4515022647512
At time: 219.51722860336304 and batch: 1150, loss is 4.941879549026489 and perplexity is 140.03320163474268
At time: 220.34334206581116 and batch: 1200, loss is 4.948057289123535 and perplexity is 140.90096801716837
At time: 221.18446278572083 and batch: 1250, loss is 4.936124515533447 and perplexity is 139.2296204055515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.779484435589644 and perplexity of 119.04295990745702
Finished 10 epochs...
Completing Train Step...
At time: 223.43328166007996 and batch: 50, loss is 4.910110626220703 and perplexity is 135.6544205142787
At time: 224.32387614250183 and batch: 100, loss is 4.951437778472901 and perplexity is 141.37808823440625
At time: 225.15120697021484 and batch: 150, loss is 4.853361196517945 and perplexity is 128.1704727913191
At time: 225.98077392578125 and batch: 200, loss is 4.883056211471557 and perplexity is 132.03357040095364
At time: 226.8033847808838 and batch: 250, loss is 4.9139172458648686 and perplexity is 136.171789384741
At time: 227.6278030872345 and batch: 300, loss is 4.92560299873352 and perplexity is 137.77239317895754
At time: 228.45274233818054 and batch: 350, loss is 4.933686170578003 and perplexity is 138.89054412345703
At time: 229.2750952243805 and batch: 400, loss is 4.927287998199463 and perplexity is 138.00473528107216
At time: 230.10026359558105 and batch: 450, loss is 4.880795974731445 and perplexity is 131.73548027823247
At time: 230.9598948955536 and batch: 500, loss is 4.891359529495239 and perplexity is 133.1344512828087
At time: 231.7860198020935 and batch: 550, loss is 4.902490615844727 and perplexity is 134.62466079499276
At time: 232.61059737205505 and batch: 600, loss is 4.916485834121704 and perplexity is 136.52200823532877
At time: 233.43369340896606 and batch: 650, loss is 4.915054092407226 and perplexity is 136.3266838416012
At time: 234.25633454322815 and batch: 700, loss is 4.916195659637451 and perplexity is 136.48239877910464
At time: 235.0805311203003 and batch: 750, loss is 4.905354099273682 and perplexity is 135.01070873746644
At time: 235.90560030937195 and batch: 800, loss is 4.916484937667847 and perplexity is 136.52188584970278
At time: 236.73090648651123 and batch: 850, loss is 4.9606537342071535 and perplexity is 142.68704481577785
At time: 237.55565452575684 and batch: 900, loss is 4.937597465515137 and perplexity is 139.43484978157616
At time: 238.38107132911682 and batch: 950, loss is 4.916286544799805 and perplexity is 136.49480356777315
At time: 239.2065989971161 and batch: 1000, loss is 4.903166208267212 and perplexity is 134.715642925664
At time: 240.0308084487915 and batch: 1050, loss is 4.894683332443237 and perplexity is 133.5777001924067
At time: 240.85734963417053 and batch: 1100, loss is 4.861728610992432 and perplexity is 129.24742764090138
At time: 241.68246054649353 and batch: 1150, loss is 4.902240724563598 and perplexity is 134.5910234690475
At time: 242.50831294059753 and batch: 1200, loss is 4.908656406402588 and perplexity is 135.457292535998
At time: 243.33177995681763 and batch: 1250, loss is 4.897910737991333 and perplexity is 134.0095060345823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.759316548813868 and perplexity of 116.66616301758096
Finished 11 epochs...
Completing Train Step...
At time: 245.58810567855835 and batch: 50, loss is 4.870302600860596 and perplexity is 130.36035808523317
At time: 246.44953298568726 and batch: 100, loss is 4.912019329071045 and perplexity is 135.9135917551356
At time: 247.27416348457336 and batch: 150, loss is 4.814430885314941 and perplexity is 123.27663379964224
At time: 248.09830713272095 and batch: 200, loss is 4.845819616317749 and perplexity is 127.2075006215082
At time: 248.92222571372986 and batch: 250, loss is 4.876008443832397 and perplexity is 131.10629990788934
At time: 249.746844291687 and batch: 300, loss is 4.888654918670654 and perplexity is 132.7748608999628
At time: 250.56438851356506 and batch: 350, loss is 4.8952209091186525 and perplexity is 133.64952775306662
At time: 251.4119176864624 and batch: 400, loss is 4.890009098052978 and perplexity is 132.95478367544575
At time: 252.23664164543152 and batch: 450, loss is 4.8413081741333 and perplexity is 126.6349039266446
At time: 253.07715582847595 and batch: 500, loss is 4.85330825805664 and perplexity is 128.16368782329934
At time: 253.90431451797485 and batch: 550, loss is 4.864276857376098 and perplexity is 129.57720192559896
At time: 254.75005340576172 and batch: 600, loss is 4.878666477203369 and perplexity is 131.4552483810849
At time: 255.57183265686035 and batch: 650, loss is 4.878054304122925 and perplexity is 131.37479964353102
At time: 256.39531421661377 and batch: 700, loss is 4.879593601226807 and perplexity is 131.57718021409784
At time: 257.21706438064575 and batch: 750, loss is 4.871001329421997 and perplexity is 130.45147642048633
At time: 258.0380358695984 and batch: 800, loss is 4.882208280563354 and perplexity is 131.92166250747616
At time: 258.87411522865295 and batch: 850, loss is 4.925665235519409 and perplexity is 137.7809679567237
At time: 259.69757986068726 and batch: 900, loss is 4.902236347198486 and perplexity is 134.59043431628646
At time: 260.52092838287354 and batch: 950, loss is 4.881410827636719 and perplexity is 131.81650312702368
At time: 261.34848403930664 and batch: 1000, loss is 4.868218545913696 and perplexity is 130.08896283562123
At time: 262.17234539985657 and batch: 1050, loss is 4.860220689773559 and perplexity is 129.05267957155192
At time: 262.9960021972656 and batch: 1100, loss is 4.826103467941284 and perplexity is 124.72402143570825
At time: 263.82090163230896 and batch: 1150, loss is 4.866012945175171 and perplexity is 129.80235471085072
At time: 264.6452684402466 and batch: 1200, loss is 4.872514209747314 and perplexity is 130.6489832570038
At time: 265.4693982601166 and batch: 1250, loss is 4.863272724151611 and perplexity is 129.44715445542917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.742201756386861 and perplexity of 114.68643549138896
Finished 12 epochs...
Completing Train Step...
At time: 267.7323157787323 and batch: 50, loss is 4.834332609176636 and perplexity is 125.75462770989583
At time: 268.5533616542816 and batch: 100, loss is 4.876048812866211 and perplexity is 131.1115926493742
At time: 269.3732554912567 and batch: 150, loss is 4.779045286178589 and perplexity is 118.99069373889193
At time: 270.20026993751526 and batch: 200, loss is 4.811718902587891 and perplexity is 122.94276262918336
At time: 271.0283854007721 and batch: 250, loss is 4.8414768409729 and perplexity is 126.65626483706093
At time: 271.848105430603 and batch: 300, loss is 4.854846134185791 and perplexity is 128.36093933474612
At time: 272.73666858673096 and batch: 350, loss is 4.859679946899414 and perplexity is 128.9829141189699
At time: 273.5657317638397 and batch: 400, loss is 4.855926189422608 and perplexity is 128.49965113419586
At time: 274.3970699310303 and batch: 450, loss is 4.804769268035889 and perplexity is 122.0913174015648
At time: 275.2438118457794 and batch: 500, loss is 4.818064203262329 and perplexity is 123.72535167945617
At time: 276.0716688632965 and batch: 550, loss is 4.82913330078125 and perplexity is 125.102487426608
At time: 276.89451336860657 and batch: 600, loss is 4.8439116954803465 and perplexity is 126.96503016117299
At time: 277.7191243171692 and batch: 650, loss is 4.843859710693359 and perplexity is 126.95843008267857
At time: 278.5442707538605 and batch: 700, loss is 4.845510835647583 and perplexity is 127.1682274679235
At time: 279.36866545677185 and batch: 750, loss is 4.8389732360839846 and perplexity is 126.33956420525239
At time: 280.2038640975952 and batch: 800, loss is 4.850892372131348 and perplexity is 127.85443268673343
At time: 281.045702457428 and batch: 850, loss is 4.89339955329895 and perplexity is 133.40632595363135
At time: 281.86997747421265 and batch: 900, loss is 4.869586257934571 and perplexity is 130.26700880392522
At time: 282.69773721694946 and batch: 950, loss is 4.8495821857452395 and perplexity is 127.68702923843003
At time: 283.5255970954895 and batch: 1000, loss is 4.836412630081177 and perplexity is 126.01647219096677
At time: 284.3540141582489 and batch: 1050, loss is 4.828360376358032 and perplexity is 125.00583001791854
At time: 285.1797778606415 and batch: 1100, loss is 4.793391189575195 and perplexity is 120.71002593601406
At time: 286.0050299167633 and batch: 1150, loss is 4.832448654174804 and perplexity is 125.51793467953512
At time: 286.83153915405273 and batch: 1200, loss is 4.8391133975982665 and perplexity is 126.35727339092927
At time: 287.6557848453522 and batch: 1250, loss is 4.830986700057983 and perplexity is 125.33456728821122
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.727034742814781 and perplexity of 112.96010946618318
Finished 13 epochs...
Completing Train Step...
At time: 290.0525813102722 and batch: 50, loss is 4.800787048339844 and perplexity is 121.60608973550778
At time: 290.91286277770996 and batch: 100, loss is 4.842802276611328 and perplexity is 126.82425086704873
At time: 291.73656392097473 and batch: 150, loss is 4.746392574310303 and perplexity is 115.16807398552274
At time: 292.56118965148926 and batch: 200, loss is 4.780084352493287 and perplexity is 119.11439721744748
At time: 293.38469791412354 and batch: 250, loss is 4.8096324634552 and perplexity is 122.68651745109412
At time: 294.2441999912262 and batch: 300, loss is 4.8233427238464355 and perplexity is 124.3801651979804
At time: 295.0722658634186 and batch: 350, loss is 4.826827354431153 and perplexity is 124.8143401560944
At time: 295.9097509384155 and batch: 400, loss is 4.824394865036011 and perplexity is 124.51109956159121
At time: 296.74566435813904 and batch: 450, loss is 4.770393867492675 and perplexity is 117.96569566454129
At time: 297.5702950954437 and batch: 500, loss is 4.785501775741577 and perplexity is 119.76144139574724
At time: 298.4127764701843 and batch: 550, loss is 4.796448259353638 and perplexity is 121.07960954186011
At time: 299.25193452835083 and batch: 600, loss is 4.811624460220337 and perplexity is 122.9311521718752
At time: 300.0905816555023 and batch: 650, loss is 4.8122311782836915 and perplexity is 123.00575935292049
At time: 300.93839168548584 and batch: 700, loss is 4.814335947036743 and perplexity is 123.26493068383287
At time: 301.77291321754456 and batch: 750, loss is 4.809583740234375 and perplexity is 122.6805399144357
At time: 302.59725999832153 and batch: 800, loss is 4.821506757736206 and perplexity is 124.15201693022229
At time: 303.4213273525238 and batch: 850, loss is 4.86273419380188 and perplexity is 129.37746200150374
At time: 304.24768710136414 and batch: 900, loss is 4.839365234375 and perplexity is 126.38909880661375
At time: 305.0779495239258 and batch: 950, loss is 4.820153408050537 and perplexity is 123.98410948150874
At time: 305.89874625205994 and batch: 1000, loss is 4.806442966461182 and perplexity is 122.29583254783908
At time: 306.7195522785187 and batch: 1050, loss is 4.798892774581909 and perplexity is 121.37595255114755
At time: 307.54491996765137 and batch: 1100, loss is 4.763056230545044 and perplexity is 117.1032741549071
At time: 308.3697428703308 and batch: 1150, loss is 4.801501836776733 and perplexity is 121.69304343535345
At time: 309.1951217651367 and batch: 1200, loss is 4.8080652236938475 and perplexity is 122.49438885787434
At time: 310.02066373825073 and batch: 1250, loss is 4.801486463546753 and perplexity is 121.69117263458983
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713216294337363 and perplexity of 111.40991134732573
Finished 14 epochs...
Completing Train Step...
At time: 312.37931966781616 and batch: 50, loss is 4.769792470932007 and perplexity is 117.89477282940395
At time: 313.20429015159607 and batch: 100, loss is 4.812023143768311 and perplexity is 122.98017257094367
At time: 314.0478799343109 and batch: 150, loss is 4.716100339889526 and perplexity is 111.73168639051875
At time: 314.9087760448456 and batch: 200, loss is 4.751260509490967 and perplexity is 115.73007147845546
At time: 315.7752573490143 and batch: 250, loss is 4.780073375701904 and perplexity is 119.11308973073459
At time: 316.59987211227417 and batch: 300, loss is 4.794123640060425 and perplexity is 120.79847244047137
At time: 317.4276177883148 and batch: 350, loss is 4.796567144393921 and perplexity is 121.09400495180047
At time: 318.258282661438 and batch: 400, loss is 4.7950731468200685 and perplexity is 120.91322587756886
At time: 319.08358573913574 and batch: 450, loss is 4.738776798248291 and perplexity is 114.29431113882798
At time: 319.92487955093384 and batch: 500, loss is 4.75529637336731 and perplexity is 116.19808607969398
At time: 320.7619671821594 and batch: 550, loss is 4.766141738891601 and perplexity is 117.46515529125134
At time: 321.59649777412415 and batch: 600, loss is 4.781989221572876 and perplexity is 119.3415107917796
At time: 322.4247760772705 and batch: 650, loss is 4.783278913497925 and perplexity is 119.49552386793364
At time: 323.25277161598206 and batch: 700, loss is 4.785568161010742 and perplexity is 119.76939205517034
At time: 324.0819823741913 and batch: 750, loss is 4.781754760742188 and perplexity is 119.31353316198111
At time: 324.92264437675476 and batch: 800, loss is 4.793986234664917 and perplexity is 120.78187521888896
At time: 325.7483186721802 and batch: 850, loss is 4.834733543395996 and perplexity is 125.80505715215655
At time: 326.5749337673187 and batch: 900, loss is 4.811209335327148 and perplexity is 122.88013098127617
At time: 327.39987230300903 and batch: 950, loss is 4.792983589172363 and perplexity is 120.66083450672089
At time: 328.2258598804474 and batch: 1000, loss is 4.778816347122192 and perplexity is 118.96345523984444
At time: 329.04991698265076 and batch: 1050, loss is 4.771380348205566 and perplexity is 118.08212396579215
At time: 329.87560749053955 and batch: 1100, loss is 4.734902391433716 and perplexity is 113.85234521164797
At time: 330.7007210254669 and batch: 1150, loss is 4.77253755569458 and perplexity is 118.21884857807896
At time: 331.5299742221832 and batch: 1200, loss is 4.779189872741699 and perplexity is 119.00789943816818
At time: 332.35758781433105 and batch: 1250, loss is 4.773876819610596 and perplexity is 118.37728088382693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.701057823905109 and perplexity of 110.0635387361099
Finished 15 epochs...
Completing Train Step...
At time: 334.65708112716675 and batch: 50, loss is 4.74086853981018 and perplexity is 114.53363551483531
At time: 335.5320153236389 and batch: 100, loss is 4.783364543914795 and perplexity is 119.50575675757362
At time: 336.37422227859497 and batch: 150, loss is 4.687556390762329 and perplexity is 108.58750990060585
At time: 337.22805166244507 and batch: 200, loss is 4.724318227767944 and perplexity is 112.65366804394074
At time: 338.0537121295929 and batch: 250, loss is 4.751944255828858 and perplexity is 115.80922854960754
At time: 338.880854845047 and batch: 300, loss is 4.766701936721802 and perplexity is 117.53097745136391
At time: 339.70626854896545 and batch: 350, loss is 4.768253126144409 and perplexity is 117.71343173440722
At time: 340.5382297039032 and batch: 400, loss is 4.767762851715088 and perplexity is 117.65573399385934
At time: 341.3681991100311 and batch: 450, loss is 4.7094291305541995 and perplexity is 110.98878171277701
At time: 342.2025029659271 and batch: 500, loss is 4.7271909999847415 and perplexity is 112.97776167231277
At time: 343.0295388698578 and batch: 550, loss is 4.737828111648559 and perplexity is 114.18593307396232
At time: 343.85651087760925 and batch: 600, loss is 4.754039478302002 and perplexity is 116.05212902426862
At time: 344.6823949813843 and batch: 650, loss is 4.756184539794922 and perplexity is 116.30133516310025
At time: 345.50779366493225 and batch: 700, loss is 4.7585036659240725 and perplexity is 116.5713656246552
At time: 346.33273124694824 and batch: 750, loss is 4.75635630607605 and perplexity is 116.32131352668695
At time: 347.1529414653778 and batch: 800, loss is 4.768868494033813 and perplexity is 117.78589109274277
At time: 347.9752390384674 and batch: 850, loss is 4.808575296401978 and perplexity is 122.55688584017736
At time: 348.7955844402313 and batch: 900, loss is 4.784664669036865 and perplexity is 119.66123023975614
At time: 349.6170494556427 and batch: 950, loss is 4.767278852462769 and perplexity is 117.59880248509046
At time: 350.4388837814331 and batch: 1000, loss is 4.752772817611694 and perplexity is 115.90522341382963
At time: 351.2670052051544 and batch: 1050, loss is 4.745641784667969 and perplexity is 115.08163943958795
At time: 352.10891819000244 and batch: 1100, loss is 4.708401403427124 and perplexity is 110.87477412538964
At time: 352.9434266090393 and batch: 1150, loss is 4.745419845581055 and perplexity is 115.05610115968626
At time: 353.7768466472626 and batch: 1200, loss is 4.752296304702758 and perplexity is 115.8500062355538
At time: 354.6160976886749 and batch: 1250, loss is 4.7482181167602535 and perplexity is 115.37851021515577
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.690171736000228 and perplexity of 108.87187542243734
Finished 16 epochs...
Completing Train Step...
At time: 356.8548629283905 and batch: 50, loss is 4.713735294342041 and perplexity is 111.46774809917514
At time: 357.70648860931396 and batch: 100, loss is 4.756253204345703 and perplexity is 116.30932121621072
At time: 358.5287563800812 and batch: 150, loss is 4.660480604171753 and perplexity is 105.686863500317
At time: 359.3548390865326 and batch: 200, loss is 4.698939733505249 and perplexity is 109.83066092652814
At time: 360.1802635192871 and batch: 250, loss is 4.725758628845215 and perplexity is 112.81605142927582
At time: 361.00508213043213 and batch: 300, loss is 4.740984840393066 and perplexity is 114.54695661801674
At time: 361.83207845687866 and batch: 350, loss is 4.741457061767578 and perplexity is 114.6010609129139
At time: 362.6575894355774 and batch: 400, loss is 4.742203321456909 and perplexity is 114.6866149838345
At time: 363.4849750995636 and batch: 450, loss is 4.682019195556641 and perplexity is 107.98790126843404
At time: 364.32401490211487 and batch: 500, loss is 4.700717182159424 and perplexity is 110.02605288512385
At time: 365.1562695503235 and batch: 550, loss is 4.7113599109649655 and perplexity is 111.20328368979341
At time: 365.9819247722626 and batch: 600, loss is 4.727344064712525 and perplexity is 112.99505590618354
At time: 366.80999398231506 and batch: 650, loss is 4.730625610351563 and perplexity is 113.36646340115503
At time: 367.6346695423126 and batch: 700, loss is 4.733302850723266 and perplexity is 113.67037932022453
At time: 368.45929884910583 and batch: 750, loss is 4.731402225494385 and perplexity is 113.45453970959119
At time: 369.2849340438843 and batch: 800, loss is 4.74475435256958 and perplexity is 114.97955760086366
At time: 370.11189913749695 and batch: 850, loss is 4.783981971740722 and perplexity is 119.57956572066333
At time: 370.93767261505127 and batch: 900, loss is 4.7596218872070315 and perplexity is 116.70179111536953
At time: 371.76275420188904 and batch: 950, loss is 4.742990217208862 and perplexity is 114.77689691054489
At time: 372.58845710754395 and batch: 1000, loss is 4.728255271911621 and perplexity is 113.09806473865507
At time: 373.417284488678 and batch: 1050, loss is 4.7213176727294925 and perplexity is 112.31615113495366
At time: 374.2469630241394 and batch: 1100, loss is 4.683583326339722 and perplexity is 108.1569406343833
At time: 375.0731089115143 and batch: 1150, loss is 4.719730920791626 and perplexity is 112.13807458350628
At time: 375.897997379303 and batch: 1200, loss is 4.7273282718658445 and perplexity is 112.99327140668119
At time: 376.7232620716095 and batch: 1250, loss is 4.724214754104614 and perplexity is 112.64201195927996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.680408338560675 and perplexity of 107.81408823375268
Finished 17 epochs...
Completing Train Step...
At time: 379.0218470096588 and batch: 50, loss is 4.688020448684693 and perplexity is 108.63791248880065
At time: 379.8458032608032 and batch: 100, loss is 4.730780763626099 and perplexity is 113.3840539437541
At time: 380.6702628135681 and batch: 150, loss is 4.634960603713989 and perplexity is 103.02385915785696
At time: 381.49607968330383 and batch: 200, loss is 4.675202589035035 and perplexity is 107.25429343502107
At time: 382.3246057033539 and batch: 250, loss is 4.7010744762420655 and perplexity is 110.06537156650397
At time: 383.1500988006592 and batch: 300, loss is 4.716534280776978 and perplexity is 111.78018185898983
At time: 383.97877264022827 and batch: 350, loss is 4.716383209228516 and perplexity is 111.7632963293227
At time: 384.80502557754517 and batch: 400, loss is 4.718116931915283 and perplexity is 111.9572309575718
At time: 385.62977409362793 and batch: 450, loss is 4.656432905197144 and perplexity is 105.2599395040966
At time: 386.455605506897 and batch: 500, loss is 4.675687828063965 and perplexity is 107.30635003314123
At time: 387.28018450737 and batch: 550, loss is 4.686333093643189 and perplexity is 108.45475632765647
At time: 388.1062548160553 and batch: 600, loss is 4.702210092544556 and perplexity is 110.19043459516129
At time: 388.93168020248413 and batch: 650, loss is 4.706565532684326 and perplexity is 110.6714091045233
At time: 389.75712180137634 and batch: 700, loss is 4.70944842338562 and perplexity is 110.99092302128804
At time: 390.58217310905457 and batch: 750, loss is 4.707932662963867 and perplexity is 110.82281481107051
At time: 391.40884041786194 and batch: 800, loss is 4.721869964599609 and perplexity is 112.37819956495277
At time: 392.23433208465576 and batch: 850, loss is 4.760833835601806 and perplexity is 116.84331340531266
At time: 393.0595359802246 and batch: 900, loss is 4.736241750717163 and perplexity is 114.00493657166581
At time: 393.8846445083618 and batch: 950, loss is 4.7197491073608395 and perplexity is 112.14011400890617
At time: 394.7094280719757 and batch: 1000, loss is 4.704786109924316 and perplexity is 110.47465298835245
At time: 395.53393292427063 and batch: 1050, loss is 4.698201742172241 and perplexity is 109.74963675191253
At time: 396.3590853214264 and batch: 1100, loss is 4.659821176528931 and perplexity is 105.61719363468399
At time: 397.18462800979614 and batch: 1150, loss is 4.695304441452026 and perplexity is 109.43211924427244
At time: 398.01709842681885 and batch: 1200, loss is 4.703717288970947 and perplexity is 110.35663844386603
At time: 398.85490679740906 and batch: 1250, loss is 4.701460266113282 and perplexity is 110.10784186380448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.671443298785356 and perplexity of 106.8518503396294
Finished 18 epochs...
Completing Train Step...
At time: 401.1308550834656 and batch: 50, loss is 4.663662481307983 and perplexity is 106.02368168776154
At time: 402.0002233982086 and batch: 100, loss is 4.70653377532959 and perplexity is 110.66789452913233
At time: 402.8257751464844 and batch: 150, loss is 4.611205472946167 and perplexity is 100.60535359966649
At time: 403.6507878303528 and batch: 200, loss is 4.652202739715576 and perplexity is 104.81561299132869
At time: 404.4778757095337 and batch: 250, loss is 4.677804555892944 and perplexity is 107.53372893520914
At time: 405.30308198928833 and batch: 300, loss is 4.693419246673584 and perplexity is 109.22601272099124
At time: 406.1284146308899 and batch: 350, loss is 4.692247447967529 and perplexity is 109.09809678112052
At time: 406.95468831062317 and batch: 400, loss is 4.695136432647705 and perplexity is 109.41373522914532
At time: 407.78027868270874 and batch: 450, loss is 4.632270946502685 and perplexity is 102.74713260874171
At time: 408.60506415367126 and batch: 500, loss is 4.651814270019531 and perplexity is 104.77490320977938
At time: 409.432222366333 and batch: 550, loss is 4.662843561172485 and perplexity is 105.936892301626
At time: 410.25690817832947 and batch: 600, loss is 4.67865611076355 and perplexity is 107.62533880571812
At time: 411.0819687843323 and batch: 650, loss is 4.683855781555176 and perplexity is 108.18641257165584
At time: 411.9087951183319 and batch: 700, loss is 4.686695108413696 and perplexity is 108.49402565898899
At time: 412.73496532440186 and batch: 750, loss is 4.68572395324707 and perplexity is 108.38871227151951
At time: 413.55934047698975 and batch: 800, loss is 4.699542932510376 and perplexity is 109.89693065683981
At time: 414.3853905200958 and batch: 850, loss is 4.738363265991211 and perplexity is 114.24705652569814
At time: 415.2110288143158 and batch: 900, loss is 4.713087959289551 and perplexity is 111.39561446844111
At time: 416.0373706817627 and batch: 950, loss is 4.697306079864502 and perplexity is 109.65138214702726
At time: 416.8635609149933 and batch: 1000, loss is 4.682717018127441 and perplexity is 108.06328396212608
At time: 417.68911480903625 and batch: 1050, loss is 4.676287508010864 and perplexity is 107.37071879783072
At time: 418.51519322395325 and batch: 1100, loss is 4.637481641769409 and perplexity is 103.28391389361846
At time: 419.3473105430603 and batch: 1150, loss is 4.6723701286315915 and perplexity is 106.95092973142589
At time: 420.2139837741852 and batch: 1200, loss is 4.681546316146851 and perplexity is 107.9368480853687
At time: 421.08957529067993 and batch: 1250, loss is 4.679857692718506 and perplexity is 107.75473719654005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.663260132726962 and perplexity of 105.981031790518
Finished 19 epochs...
Completing Train Step...
At time: 423.3320281505585 and batch: 50, loss is 4.640631742477417 and perplexity is 103.6097816124975
At time: 424.1569163799286 and batch: 100, loss is 4.683369836807251 and perplexity is 108.13385272429502
At time: 424.9822142124176 and batch: 150, loss is 4.588652124404907 and perplexity is 98.36176135388851
At time: 425.8130466938019 and batch: 200, loss is 4.630410213470459 and perplexity is 102.55612538696433
At time: 426.63901567459106 and batch: 250, loss is 4.6555768013000485 and perplexity is 105.16986462190941
At time: 427.4649016857147 and batch: 300, loss is 4.671379833221436 and perplexity is 106.84506914188026
At time: 428.2916283607483 and batch: 350, loss is 4.669065456390381 and perplexity is 106.59807531798616
At time: 429.1326596736908 and batch: 400, loss is 4.673219985961914 and perplexity is 107.04186139704126
At time: 429.973290681839 and batch: 450, loss is 4.609430856704712 and perplexity is 100.42697602787241
At time: 430.8092975616455 and batch: 500, loss is 4.628905010223389 and perplexity is 102.40187369322331
At time: 431.6412470340729 and batch: 550, loss is 4.640387392044067 and perplexity is 103.58446761033082
At time: 432.47398495674133 and batch: 600, loss is 4.655977010726929 and perplexity is 105.21196301668014
At time: 433.30080008506775 and batch: 650, loss is 4.662389259338379 and perplexity is 105.8887759076645
At time: 434.12482047080994 and batch: 700, loss is 4.664911241531372 and perplexity is 106.1561625453872
At time: 434.95082116127014 and batch: 750, loss is 4.66470329284668 and perplexity is 106.134089806092
At time: 435.7784459590912 and batch: 800, loss is 4.678661527633667 and perplexity is 107.62592179977882
At time: 436.6029613018036 and batch: 850, loss is 4.717333507537842 and perplexity is 111.869555281724
At time: 437.42906069755554 and batch: 900, loss is 4.691606922149658 and perplexity is 109.02823900869345
At time: 438.253879070282 and batch: 950, loss is 4.676144580841065 and perplexity is 107.35537370151538
At time: 439.10304021835327 and batch: 1000, loss is 4.661956739425659 and perplexity is 105.84298680661371
At time: 439.9281516075134 and batch: 1050, loss is 4.655503797531128 and perplexity is 105.16218710566235
At time: 440.7535312175751 and batch: 1100, loss is 4.61617070198059 and perplexity is 101.1061244144603
At time: 441.57857942581177 and batch: 1150, loss is 4.64956413269043 and perplexity is 104.53941033393012
At time: 442.4829502105713 and batch: 1200, loss is 4.659941911697388 and perplexity is 105.62994611417028
At time: 443.30696845054626 and batch: 1250, loss is 4.659197645187378 and perplexity is 105.551358531504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.656306134523266 and perplexity of 105.24659647640354
Finished 20 epochs...
Completing Train Step...
At time: 445.5350708961487 and batch: 50, loss is 4.618749723434449 and perplexity is 101.36721581388437
At time: 446.39457845687866 and batch: 100, loss is 4.6614675712585445 and perplexity is 105.79122444803676
At time: 447.23758482933044 and batch: 150, loss is 4.567223119735718 and perplexity is 96.2763902243825
At time: 448.06624817848206 and batch: 200, loss is 4.60983024597168 and perplexity is 100.46709349492158
At time: 448.90863728523254 and batch: 250, loss is 4.634573354721069 and perplexity is 102.98397099597504
At time: 449.7286832332611 and batch: 300, loss is 4.650656471252441 and perplexity is 104.65366515420006
At time: 450.55129504203796 and batch: 350, loss is 4.647110004425048 and perplexity is 104.28317176194936
At time: 451.37148785591125 and batch: 400, loss is 4.652405242919922 and perplexity is 104.83684063808568
At time: 452.19092178344727 and batch: 450, loss is 4.587413463592529 and perplexity is 98.2399999207888
At time: 453.0163035392761 and batch: 500, loss is 4.6070911407470705 and perplexity is 100.19228009745484
At time: 453.8376131057739 and batch: 550, loss is 4.618793783187866 and perplexity is 101.37168212680932
At time: 454.659259557724 and batch: 600, loss is 4.634242916107178 and perplexity is 102.94994673712021
At time: 455.48142290115356 and batch: 650, loss is 4.641734209060669 and perplexity is 103.72407092292015
At time: 456.3010776042938 and batch: 700, loss is 4.644626398086547 and perplexity is 104.02449477443174
At time: 457.1206259727478 and batch: 750, loss is 4.644623394012451 and perplexity is 104.024182277611
At time: 457.93994212150574 and batch: 800, loss is 4.659748649597168 and perplexity is 105.60953382146293
At time: 458.7599174976349 and batch: 850, loss is 4.69732253074646 and perplexity is 109.65318602380914
At time: 459.57909750938416 and batch: 900, loss is 4.670722312927246 and perplexity is 106.77483943184542
At time: 460.39962673187256 and batch: 950, loss is 4.655781164169311 and perplexity is 105.19135963351982
At time: 461.2360372543335 and batch: 1000, loss is 4.64143590927124 and perplexity is 103.6931346687736
At time: 462.05666851997375 and batch: 1050, loss is 4.635144748687744 and perplexity is 103.04283223054207
At time: 462.922078371048 and batch: 1100, loss is 4.595213880538941 and perplexity is 99.00930944770943
At time: 463.74118852615356 and batch: 1150, loss is 4.628266229629516 and perplexity is 102.33648225114752
At time: 464.5648982524872 and batch: 1200, loss is 4.639433460235596 and perplexity is 103.48570220703326
At time: 465.3848946094513 and batch: 1250, loss is 4.639169340133667 and perplexity is 103.45837316205208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.65048240049042 and perplexity of 104.63544959640234
Finished 21 epochs...
Completing Train Step...
At time: 467.6192991733551 and batch: 50, loss is 4.597555456161499 and perplexity is 99.2414188779561
At time: 468.47329592704773 and batch: 100, loss is 4.640613126754761 and perplexity is 103.6078528594911
At time: 469.2946014404297 and batch: 150, loss is 4.547204303741455 and perplexity is 94.36821432021871
At time: 470.1189806461334 and batch: 200, loss is 4.589490938186645 and perplexity is 98.44430316867431
At time: 470.94833850860596 and batch: 250, loss is 4.614320478439331 and perplexity is 100.91922843588999
At time: 471.7755103111267 and batch: 300, loss is 4.6308025550842284 and perplexity is 102.5963703170648
At time: 472.5987751483917 and batch: 350, loss is 4.625924596786499 and perplexity is 102.09712813237192
At time: 473.4242684841156 and batch: 400, loss is 4.632312421798706 and perplexity is 102.75139416485598
At time: 474.2593283653259 and batch: 450, loss is 4.566613368988037 and perplexity is 96.21770351740919
At time: 475.105593919754 and batch: 500, loss is 4.586516914367675 and perplexity is 98.15196239589456
At time: 475.93142104148865 and batch: 550, loss is 4.598268337249756 and perplexity is 99.31219143185645
At time: 476.75497031211853 and batch: 600, loss is 4.613395919799805 and perplexity is 100.82596581137138
At time: 477.5797321796417 and batch: 650, loss is 4.622213983535767 and perplexity is 101.71898717715594
At time: 478.4045469760895 and batch: 700, loss is 4.625257940292358 and perplexity is 102.02908710138692
At time: 479.22963404655457 and batch: 750, loss is 4.625634384155274 and perplexity is 102.06750255524237
At time: 480.0555317401886 and batch: 800, loss is 4.640383024215698 and perplexity is 103.58401517214267
At time: 480.88101601600647 and batch: 850, loss is 4.678193378448486 and perplexity is 107.57554860418911
At time: 481.7081136703491 and batch: 900, loss is 4.650899229049682 and perplexity is 104.67907373136643
At time: 482.53232979774475 and batch: 950, loss is 4.6362080574035645 and perplexity is 103.15245684423051
At time: 483.3568158149719 and batch: 1000, loss is 4.622094163894653 and perplexity is 101.7067999747656
At time: 484.23185563087463 and batch: 1050, loss is 4.615637254714966 and perplexity is 101.05220401197782
At time: 485.07452058792114 and batch: 1100, loss is 4.575186443328858 and perplexity is 97.04613105404877
At time: 485.8986494541168 and batch: 1150, loss is 4.607520418167114 and perplexity is 100.23529961395629
At time: 486.7248418331146 and batch: 1200, loss is 4.619919996261597 and perplexity is 101.48591255235226
At time: 487.5717120170593 and batch: 1250, loss is 4.620229749679566 and perplexity is 101.5173530297873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.645017303689553 and perplexity of 104.06516648116998
Finished 22 epochs...
Completing Train Step...
At time: 489.9492783546448 and batch: 50, loss is 4.57741003036499 and perplexity is 97.26216166539005
At time: 490.81174540519714 and batch: 100, loss is 4.6207731914520265 and perplexity is 101.57253679327634
At time: 491.6383111476898 and batch: 150, loss is 4.527830018997192 and perplexity is 92.55749500040318
At time: 492.4636609554291 and batch: 200, loss is 4.57033143043518 and perplexity is 96.57611273224519
At time: 493.29836678504944 and batch: 250, loss is 4.595600185394287 and perplexity is 99.0475646132759
At time: 494.12417554855347 and batch: 300, loss is 4.611691064834595 and perplexity is 100.65421860657224
At time: 494.95155692100525 and batch: 350, loss is 4.606057643890381 and perplexity is 100.08878518095705
At time: 495.775230884552 and batch: 400, loss is 4.613144874572754 and perplexity is 100.80065711083878
At time: 496.6029736995697 and batch: 450, loss is 4.546822376251221 and perplexity is 94.33217938676941
At time: 497.42679476737976 and batch: 500, loss is 4.566581573486328 and perplexity is 96.2146442758879
At time: 498.2525055408478 and batch: 550, loss is 4.57861515045166 and perplexity is 97.37944490608622
At time: 499.07758712768555 and batch: 600, loss is 4.593261766433716 and perplexity is 98.81622050532975
At time: 499.9032473564148 and batch: 650, loss is 4.603668155670166 and perplexity is 99.84990971650379
At time: 500.7306065559387 and batch: 700, loss is 4.6063957786560055 and perplexity is 100.12263440135244
At time: 501.5554268360138 and batch: 750, loss is 4.607119617462158 and perplexity is 100.19513328509363
At time: 502.38000297546387 and batch: 800, loss is 4.622263116836548 and perplexity is 101.723985089529
At time: 503.2060737609863 and batch: 850, loss is 4.659857130050659 and perplexity is 105.6209910130143
At time: 504.0310697555542 and batch: 900, loss is 4.631630458831787 and perplexity is 102.68134540728036
At time: 504.860985994339 and batch: 950, loss is 4.61726071357727 and perplexity is 101.21639134776784
At time: 505.7155547142029 and batch: 1000, loss is 4.603308238983154 and perplexity is 99.81397853430407
At time: 506.5401771068573 and batch: 1050, loss is 4.596840524673462 and perplexity is 99.17049341912575
At time: 507.3650393486023 and batch: 1100, loss is 4.555762348175048 and perplexity is 95.17928734071279
At time: 508.1904203891754 and batch: 1150, loss is 4.587778940200805 and perplexity is 98.27591090466944
At time: 509.0507273674011 and batch: 1200, loss is 4.601224479675293 and perplexity is 99.60620677582598
At time: 509.885315656662 and batch: 1250, loss is 4.601513271331787 and perplexity is 99.63497637128751
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.639971433764827 and perplexity of 103.54138975334436
Finished 23 epochs...
Completing Train Step...
At time: 512.1319751739502 and batch: 50, loss is 4.558293170928955 and perplexity is 95.42047431882656
At time: 512.985303401947 and batch: 100, loss is 4.601647176742554 and perplexity is 99.64831892702551
At time: 513.8079335689545 and batch: 150, loss is 4.5093072891235355 and perplexity is 90.85885778724591
At time: 514.6395018100739 and batch: 200, loss is 4.551919345855713 and perplexity is 94.81421505483286
At time: 515.4803402423859 and batch: 250, loss is 4.577093935012817 and perplexity is 97.23142240666967
At time: 516.3202588558197 and batch: 300, loss is 4.593226594924927 and perplexity is 98.81274505088061
At time: 517.1758048534393 and batch: 350, loss is 4.587397966384888 and perplexity is 98.23847748690808
At time: 518.0011897087097 and batch: 400, loss is 4.594878854751587 and perplexity is 98.97614433175116
At time: 518.8342607021332 and batch: 450, loss is 4.527285175323486 and perplexity is 92.50707937036388
At time: 519.6937403678894 and batch: 500, loss is 4.54760100364685 and perplexity is 94.40565760829524
At time: 520.5267379283905 and batch: 550, loss is 4.559657039642334 and perplexity is 95.55070410634507
At time: 521.3602139949799 and batch: 600, loss is 4.574200077056885 and perplexity is 96.950455217018
At time: 522.1871271133423 and batch: 650, loss is 4.585893659591675 and perplexity is 98.09080777599159
At time: 523.0131211280823 and batch: 700, loss is 4.588395900726319 and perplexity is 98.33656197003921
At time: 523.8387746810913 and batch: 750, loss is 4.589514751434326 and perplexity is 98.44664747516106
At time: 524.6887755393982 and batch: 800, loss is 4.605025205612183 and perplexity is 99.98550301332382
At time: 525.511798620224 and batch: 850, loss is 4.642366695404053 and perplexity is 103.78969573246927
At time: 526.3373024463654 and batch: 900, loss is 4.613234434127808 and perplexity is 100.80968517710753
At time: 527.1926116943359 and batch: 950, loss is 4.599442014694214 and perplexity is 99.42882033987081
At time: 528.0238134860992 and batch: 1000, loss is 4.585323581695556 and perplexity is 98.03490431084496
At time: 528.875 and batch: 1050, loss is 4.578752145767212 and perplexity is 97.39278634770605
At time: 529.7089278697968 and batch: 1100, loss is 4.53721920967102 and perplexity is 93.43062755891259
At time: 530.5490396022797 and batch: 1150, loss is 4.569287128448487 and perplexity is 96.47531074886845
At time: 531.3905599117279 and batch: 1200, loss is 4.583066577911377 and perplexity is 97.81388867122358
At time: 532.2178032398224 and batch: 1250, loss is 4.583690547943116 and perplexity is 97.87494065176452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.634521929887089 and perplexity of 102.97867519853294
Finished 24 epochs...
Completing Train Step...
At time: 534.483594417572 and batch: 50, loss is 4.539889688491821 and perplexity is 93.68046551603366
At time: 535.3107526302338 and batch: 100, loss is 4.583250312805176 and perplexity is 97.83186214679739
At time: 536.1345391273499 and batch: 150, loss is 4.491308622360229 and perplexity is 89.23814853871228
At time: 536.9597878456116 and batch: 200, loss is 4.533951072692871 and perplexity is 93.12578188014422
At time: 537.7860209941864 and batch: 250, loss is 4.559299211502076 and perplexity is 95.5165194920675
At time: 538.6099891662598 and batch: 300, loss is 4.5754059886932374 and perplexity is 97.06743942124372
At time: 539.4339573383331 and batch: 350, loss is 4.569078569412231 and perplexity is 96.45519204907748
At time: 540.2599165439606 and batch: 400, loss is 4.577184391021729 and perplexity is 97.2402179708812
At time: 541.0866396427155 and batch: 450, loss is 4.508357286453247 and perplexity is 90.77258261703837
At time: 541.9118278026581 and batch: 500, loss is 4.529403505325317 and perplexity is 92.70324759313588
At time: 542.736314535141 and batch: 550, loss is 4.541574697494507 and perplexity is 93.83845100991363
At time: 543.5615630149841 and batch: 600, loss is 4.555834140777588 and perplexity is 95.18612075475016
At time: 544.3870849609375 and batch: 650, loss is 4.5685031032562256 and perplexity is 96.39970131853227
At time: 545.2151546478271 and batch: 700, loss is 4.5710567855834965 and perplexity is 96.64619012524625
At time: 546.0442130565643 and batch: 750, loss is 4.572369527816773 and perplexity is 96.7731449719717
At time: 546.874261379242 and batch: 800, loss is 4.588231582641601 and perplexity is 98.32040482201076
At time: 547.7389607429504 and batch: 850, loss is 4.6256061458587645 and perplexity is 102.06462038353527
At time: 548.5838346481323 and batch: 900, loss is 4.595332174301148 and perplexity is 99.02102232418397
At time: 549.4226086139679 and batch: 950, loss is 4.581986999511718 and perplexity is 97.70834788985198
At time: 550.2485554218292 and batch: 1000, loss is 4.567817115783692 and perplexity is 96.33359500771464
At time: 551.0759892463684 and batch: 1050, loss is 4.561154088973999 and perplexity is 95.69385534956868
At time: 551.9022219181061 and batch: 1100, loss is 4.519635734558105 and perplexity is 91.80215153553901
At time: 552.7286117076874 and batch: 1150, loss is 4.551057920455933 and perplexity is 94.73257485024583
At time: 553.5531656742096 and batch: 1200, loss is 4.565606908798218 and perplexity is 96.1209129453623
At time: 554.3789691925049 and batch: 1250, loss is 4.566442060470581 and perplexity is 96.20122201701761
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.629546005360401 and perplexity of 102.4675338379502
Finished 25 epochs...
Completing Train Step...
At time: 556.6036925315857 and batch: 50, loss is 4.5223440742492675 and perplexity is 92.05111993959802
At time: 557.4562587738037 and batch: 100, loss is 4.5655885887146 and perplexity is 96.11915201832991
At time: 558.2827169895172 and batch: 150, loss is 4.474106216430664 and perplexity is 87.71616609530437
At time: 559.1079533100128 and batch: 200, loss is 4.516894016265869 and perplexity is 91.55080062155753
At time: 559.9350020885468 and batch: 250, loss is 4.542209548950195 and perplexity is 93.89804340129349
At time: 560.7598044872284 and batch: 300, loss is 4.5581388092041015 and perplexity is 95.40574618658354
At time: 561.5842008590698 and batch: 350, loss is 4.552267827987671 and perplexity is 94.8472618724134
At time: 562.408858537674 and batch: 400, loss is 4.561038875579834 and perplexity is 95.68283077079492
At time: 563.2372646331787 and batch: 450, loss is 4.490197534561157 and perplexity is 89.13905218322958
At time: 564.064661026001 and batch: 500, loss is 4.51255618095398 and perplexity is 91.15452842892626
At time: 564.8947794437408 and batch: 550, loss is 4.524563484191894 and perplexity is 92.25564599004264
At time: 565.7190613746643 and batch: 600, loss is 4.538372812271118 and perplexity is 93.5384715663971
At time: 566.5446655750275 and batch: 650, loss is 4.5521212005615235 and perplexity is 94.83335568206728
At time: 567.402494430542 and batch: 700, loss is 4.554347696304322 and perplexity is 95.04473697722248
At time: 568.2396607398987 and batch: 750, loss is 4.556094074249268 and perplexity is 95.21086602949698
At time: 569.1103668212891 and batch: 800, loss is 4.572543067932129 and perplexity is 96.78994045201614
At time: 569.9438107013702 and batch: 850, loss is 4.609188928604126 and perplexity is 100.40268285903292
At time: 570.7725443840027 and batch: 900, loss is 4.578045644760132 and perplexity is 97.32400254684241
At time: 571.5987560749054 and batch: 950, loss is 4.56537275314331 and perplexity is 96.09840832493617
At time: 572.4238920211792 and batch: 1000, loss is 4.55092267036438 and perplexity is 94.71976312723717
At time: 573.2466449737549 and batch: 1050, loss is 4.54418251991272 and perplexity is 94.08348438905001
At time: 574.0713016986847 and batch: 1100, loss is 4.502190475463867 and perplexity is 90.21452773505789
At time: 574.8974688053131 and batch: 1150, loss is 4.533523197174072 and perplexity is 93.08594416130877
At time: 575.7260236740112 and batch: 1200, loss is 4.548841104507447 and perplexity is 94.52280276643353
At time: 576.5560076236725 and batch: 1250, loss is 4.549962024688721 and perplexity is 94.62881468799623
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.62513766323563 and perplexity of 102.01681608106556
Finished 26 epochs...
Completing Train Step...
At time: 578.825923204422 and batch: 50, loss is 4.505266361236572 and perplexity is 90.49244451843354
At time: 579.6936373710632 and batch: 100, loss is 4.54826714515686 and perplexity is 94.46856608625588
At time: 580.5247905254364 and batch: 150, loss is 4.4575619125366215 and perplexity is 86.27690182845808
At time: 581.3489637374878 and batch: 200, loss is 4.500481843948364 and perplexity is 90.06051596196697
At time: 582.1746525764465 and batch: 250, loss is 4.526113834381103 and perplexity is 92.39878547775417
At time: 583.0011751651764 and batch: 300, loss is 4.54148491859436 and perplexity is 93.83002667515986
At time: 583.8264906406403 and batch: 350, loss is 4.5359792804718015 and perplexity is 93.31485198728987
At time: 584.6512632369995 and batch: 400, loss is 4.543920392990112 and perplexity is 94.05882580679923
At time: 585.4739003181458 and batch: 450, loss is 4.4728632831573485 and perplexity is 87.60720848140589
At time: 586.2989485263824 and batch: 500, loss is 4.495884943008423 and perplexity is 89.64746679191087
At time: 587.1250071525574 and batch: 550, loss is 4.507318038940429 and perplexity is 90.67829643814605
At time: 587.9500546455383 and batch: 600, loss is 4.5216498374938965 and perplexity is 91.98723684632438
At time: 588.7745137214661 and batch: 650, loss is 4.536130495071411 and perplexity is 93.32896362218649
At time: 589.6005170345306 and batch: 700, loss is 4.538397607803344 and perplexity is 93.54079093133808
At time: 590.460428237915 and batch: 750, loss is 4.5405095672607425 and perplexity is 93.73855404973403
At time: 591.2866487503052 and batch: 800, loss is 4.557248592376709 and perplexity is 95.320852178542
At time: 592.1105432510376 and batch: 850, loss is 4.593616142272949 and perplexity is 98.85124479191514
At time: 592.971381187439 and batch: 900, loss is 4.561521129608154 and perplexity is 95.72898532959165
At time: 593.8065185546875 and batch: 950, loss is 4.548991842269897 and perplexity is 94.53705199614464
At time: 594.6598782539368 and batch: 1000, loss is 4.534603843688965 and perplexity is 93.1865915347742
At time: 595.4911823272705 and batch: 1050, loss is 4.527534446716309 and perplexity is 92.53014161314373
At time: 596.3156700134277 and batch: 1100, loss is 4.485475301742554 and perplexity is 88.71910914037592
At time: 597.1427810192108 and batch: 1150, loss is 4.516838254928589 and perplexity is 91.54569576881384
At time: 597.9681797027588 and batch: 1200, loss is 4.532948522567749 and perplexity is 93.03246540090824
At time: 598.7933971881866 and batch: 1250, loss is 4.534058752059937 and perplexity is 93.13581014530348
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.620958314324818 and perplexity of 101.5913419336613
Finished 27 epochs...
Completing Train Step...
At time: 601.031576871872 and batch: 50, loss is 4.489305486679077 and perplexity is 89.05957133615013
At time: 601.8605334758759 and batch: 100, loss is 4.531875839233399 and perplexity is 92.93272453046777
At time: 602.7056381702423 and batch: 150, loss is 4.441293468475342 and perplexity is 84.88466633577966
At time: 603.5550627708435 and batch: 200, loss is 4.484468536376953 and perplexity is 88.62983476074372
At time: 604.3897969722748 and batch: 250, loss is 4.509899158477783 and perplexity is 90.91265027822521
At time: 605.2182726860046 and batch: 300, loss is 4.525284547805786 and perplexity is 92.32219216867284
At time: 606.047265291214 and batch: 350, loss is 4.519068355560303 and perplexity is 91.75007969643553
At time: 606.876327753067 and batch: 400, loss is 4.52809965133667 and perplexity is 92.58245485915769
At time: 607.7040636539459 and batch: 450, loss is 4.456037874221802 and perplexity is 86.14551267087413
At time: 608.5320131778717 and batch: 500, loss is 4.479426107406616 and perplexity is 88.18404997790832
At time: 609.3587718009949 and batch: 550, loss is 4.491167526245118 and perplexity is 89.22555827087353
At time: 610.1879351139069 and batch: 600, loss is 4.505414552688599 and perplexity is 90.5058557188723
At time: 611.0139043331146 and batch: 650, loss is 4.520736598968506 and perplexity is 91.90326890500737
At time: 611.8672397136688 and batch: 700, loss is 4.522735185623169 and perplexity is 92.08712922094752
At time: 612.6928329467773 and batch: 750, loss is 4.525391159057617 and perplexity is 92.33203527783552
At time: 613.5180034637451 and batch: 800, loss is 4.542260608673096 and perplexity is 93.9028379317731
At time: 614.3416013717651 and batch: 850, loss is 4.578187713623047 and perplexity is 97.33783023943758
At time: 615.1695997714996 and batch: 900, loss is 4.545701560974121 and perplexity is 94.22650966817012
At time: 616.0028960704803 and batch: 950, loss is 4.533417043685913 and perplexity is 93.07606328809128
At time: 616.8368084430695 and batch: 1000, loss is 4.5187334537506105 and perplexity is 91.71935757344023
At time: 617.6665985584259 and batch: 1050, loss is 4.511852607727051 and perplexity is 91.09041709936254
At time: 618.5097041130066 and batch: 1100, loss is 4.469340763092041 and perplexity is 87.2991532156249
At time: 619.3452353477478 and batch: 1150, loss is 4.501012077331543 and perplexity is 90.10828171641602
At time: 620.1687760353088 and batch: 1200, loss is 4.517559986114502 and perplexity is 91.6117910010124
At time: 620.9950015544891 and batch: 1250, loss is 4.5184760093688965 and perplexity is 91.6957479793476
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.617596034585994 and perplexity of 101.25033702124466
Finished 28 epochs...
Completing Train Step...
At time: 623.2275900840759 and batch: 50, loss is 4.473764677047729 and perplexity is 87.68621268548846
At time: 624.0800230503082 and batch: 100, loss is 4.516383666992187 and perplexity is 91.50408965742244
At time: 624.9066987037659 and batch: 150, loss is 4.425687093734741 and perplexity is 83.57020806627608
At time: 625.7329502105713 and batch: 200, loss is 4.468885927200318 and perplexity is 87.25945545609268
At time: 626.5585842132568 and batch: 250, loss is 4.493764982223511 and perplexity is 89.4576189839011
At time: 627.3843863010406 and batch: 300, loss is 4.509900884628296 and perplexity is 90.91280720727859
At time: 628.209522485733 and batch: 350, loss is 4.503236265182495 and perplexity is 90.30892251057382
At time: 629.034304857254 and batch: 400, loss is 4.51284460067749 and perplexity is 91.18082298456441
At time: 629.8593835830688 and batch: 450, loss is 4.440166482925415 and perplexity is 84.78905642902
At time: 630.6856029033661 and batch: 500, loss is 4.463774929046631 and perplexity is 86.81461031008934
At time: 631.5096209049225 and batch: 550, loss is 4.475293025970459 and perplexity is 87.82033027731656
At time: 632.3829724788666 and batch: 600, loss is 4.49083722114563 and perplexity is 89.19609148075544
At time: 633.2086980342865 and batch: 650, loss is 4.5058389377594 and perplexity is 90.54427320418645
At time: 634.0369975566864 and batch: 700, loss is 4.5075634765625 and perplexity is 90.70055503503404
At time: 634.8666172027588 and batch: 750, loss is 4.510520696640015 and perplexity is 90.96917352366019
At time: 635.6932950019836 and batch: 800, loss is 4.528031253814698 and perplexity is 92.57612266522287
At time: 636.5240564346313 and batch: 850, loss is 4.5639361381530765 and perplexity is 95.96045103048463
At time: 637.3532817363739 and batch: 900, loss is 4.530063228607178 and perplexity is 92.7644262621544
At time: 638.1805183887482 and batch: 950, loss is 4.518194646835327 and perplexity is 91.66995186057942
At time: 639.0066401958466 and batch: 1000, loss is 4.50340235710144 and perplexity is 90.32392333853517
At time: 639.8342781066895 and batch: 1050, loss is 4.49669264793396 and perplexity is 89.71990474270736
At time: 640.6829586029053 and batch: 1100, loss is 4.453554735183716 and perplexity is 85.93186675144462
At time: 641.516361951828 and batch: 1150, loss is 4.485331745147705 and perplexity is 88.70637384130967
At time: 642.3423192501068 and batch: 1200, loss is 4.502430057525634 and perplexity is 90.2361441069581
At time: 643.1664552688599 and batch: 1250, loss is 4.50405179977417 and perplexity is 90.38260260106567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.614191431198677 and perplexity of 100.90620592820879
Finished 29 epochs...
Completing Train Step...
At time: 645.4447863101959 and batch: 50, loss is 4.459085292816162 and perplexity is 86.40843452100121
At time: 646.2709443569183 and batch: 100, loss is 4.500305824279785 and perplexity is 90.04466493488245
At time: 647.0971655845642 and batch: 150, loss is 4.410313882827759 and perplexity is 82.29529052916234
At time: 647.9227685928345 and batch: 200, loss is 4.4540348243713375 and perplexity is 85.97313161615921
At time: 648.7479658126831 and batch: 250, loss is 4.478484764099121 and perplexity is 88.1010775715205
At time: 649.5736172199249 and batch: 300, loss is 4.495074224472046 and perplexity is 89.57481738193428
At time: 650.4018340110779 and batch: 350, loss is 4.488020114898681 and perplexity is 88.94517021611709
At time: 651.2263329029083 and batch: 400, loss is 4.497895050048828 and perplexity is 89.82784902912964
At time: 652.0524933338165 and batch: 450, loss is 4.424204378128052 and perplexity is 83.4463890314138
At time: 652.8789403438568 and batch: 500, loss is 4.448762283325196 and perplexity is 85.52102766652538
At time: 653.7414922714233 and batch: 550, loss is 4.45998441696167 and perplexity is 86.48616136866096
At time: 654.5676381587982 and batch: 600, loss is 4.475420370101928 and perplexity is 87.83151439310168
At time: 655.3913676738739 and batch: 650, loss is 4.49058048248291 and perplexity is 89.17319433492592
At time: 656.2141556739807 and batch: 700, loss is 4.492992038726807 and perplexity is 89.38850001504679
At time: 657.0393452644348 and batch: 750, loss is 4.496362800598145 and perplexity is 89.6903157513516
At time: 657.8646509647369 and batch: 800, loss is 4.514710512161255 and perplexity is 91.35111715678781
At time: 658.6918795108795 and batch: 850, loss is 4.549652557373047 and perplexity is 94.59953469356365
At time: 659.5177049636841 and batch: 900, loss is 4.515230655670166 and perplexity is 91.39864520704057
At time: 660.3432033061981 and batch: 950, loss is 4.503453044891358 and perplexity is 90.32850177462029
At time: 661.1691474914551 and batch: 1000, loss is 4.488692178726196 and perplexity is 89.00496713907522
At time: 661.9947309494019 and batch: 1050, loss is 4.481877670288086 and perplexity is 88.40050393840917
At time: 662.8204371929169 and batch: 1100, loss is 4.438135814666748 and perplexity is 84.61705268382657
At time: 663.6480038166046 and batch: 1150, loss is 4.470415163040161 and perplexity is 87.39299782560539
At time: 664.4746992588043 and batch: 1200, loss is 4.488039360046387 and perplexity is 88.94688199552728
At time: 665.3009579181671 and batch: 1250, loss is 4.490153980255127 and perplexity is 89.13516987821771
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.612407600792655 and perplexity of 100.7263668188416
Finished 30 epochs...
Completing Train Step...
At time: 667.5447692871094 and batch: 50, loss is 4.44394476890564 and perplexity is 85.11001969588358
At time: 668.4138815402985 and batch: 100, loss is 4.484936323165893 and perplexity is 88.67130432525175
At time: 669.2417767047882 and batch: 150, loss is 4.395564031600952 and perplexity is 81.09035538942017
At time: 670.069611787796 and batch: 200, loss is 4.439818992614746 and perplexity is 84.75959817198601
At time: 670.8959712982178 and batch: 250, loss is 4.464105215072632 and perplexity is 86.84328869850223
At time: 671.7269539833069 and batch: 300, loss is 4.480421495437622 and perplexity is 88.27187102654123
At time: 672.5584530830383 and batch: 350, loss is 4.473845024108886 and perplexity is 87.69325829802509
At time: 673.3869333267212 and batch: 400, loss is 4.4832729148864745 and perplexity is 88.52393034901746
At time: 674.2201039791107 and batch: 450, loss is 4.409236812591553 and perplexity is 82.20670043859266
At time: 675.1006762981415 and batch: 500, loss is 4.43424147605896 and perplexity is 84.28816604230987
At time: 675.9324855804443 and batch: 550, loss is 4.44487377166748 and perplexity is 85.18912387755378
At time: 676.7637760639191 and batch: 600, loss is 4.460876121520996 and perplexity is 86.56331586746879
At time: 677.5936503410339 and batch: 650, loss is 4.476677198410034 and perplexity is 87.94197292589996
At time: 678.425311088562 and batch: 700, loss is 4.478738803863525 and perplexity is 88.12346159160575
At time: 679.2554903030396 and batch: 750, loss is 4.483150396347046 and perplexity is 88.51308519074666
At time: 680.0846722126007 and batch: 800, loss is 4.5014472007751465 and perplexity is 90.14749847370013
At time: 680.9157385826111 and batch: 850, loss is 4.535577411651611 and perplexity is 93.27735919191282
At time: 681.7444641590118 and batch: 900, loss is 4.5005468654632566 and perplexity is 90.06637202352972
At time: 682.5724244117737 and batch: 950, loss is 4.489129934310913 and perplexity is 89.04393808976126
At time: 683.3980104923248 and batch: 1000, loss is 4.474215517044067 and perplexity is 87.72575405003906
At time: 684.2223010063171 and batch: 1050, loss is 4.467670755386353 and perplexity is 87.15348462474427
At time: 685.0475149154663 and batch: 1100, loss is 4.423521232604981 and perplexity is 83.38940247160336
At time: 685.8735489845276 and batch: 1150, loss is 4.455938386917114 and perplexity is 86.13694271231566
At time: 686.6970438957214 and batch: 1200, loss is 4.47420226097107 and perplexity is 87.7245911587473
At time: 687.524685382843 and batch: 1250, loss is 4.475881319046021 and perplexity is 87.87200956930658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.609410640967154 and perplexity of 100.42494584300213
Finished 31 epochs...
Completing Train Step...
At time: 689.7819802761078 and batch: 50, loss is 4.429270639419555 and perplexity is 83.8702229622531
At time: 690.6534843444824 and batch: 100, loss is 4.470275001525879 and perplexity is 87.38074954908106
At time: 691.4798996448517 and batch: 150, loss is 4.3816305446624755 and perplexity is 79.96831907375092
At time: 692.3051800727844 and batch: 200, loss is 4.426260442733764 and perplexity is 83.6181367000232
At time: 693.131992816925 and batch: 250, loss is 4.449708862304687 and perplexity is 85.60201839963442
At time: 693.9587075710297 and batch: 300, loss is 4.466324062347412 and perplexity is 87.03619462821787
At time: 694.7842490673065 and batch: 350, loss is 4.458667974472046 and perplexity is 86.37238221935588
At time: 695.6585516929626 and batch: 400, loss is 4.469287929534912 and perplexity is 87.29454101266677
At time: 696.4837532043457 and batch: 450, loss is 4.394588031768799 and perplexity is 81.01124982595867
At time: 697.3102552890778 and batch: 500, loss is 4.420565929412842 and perplexity is 83.14332529971365
At time: 698.3705778121948 and batch: 550, loss is 4.430502433776855 and perplexity is 83.97359748464916
At time: 699.1949973106384 and batch: 600, loss is 4.446782493591309 and perplexity is 85.35188150608406
At time: 700.018413066864 and batch: 650, loss is 4.462971019744873 and perplexity is 86.74484728266306
At time: 700.8496811389923 and batch: 700, loss is 4.464849834442139 and perplexity is 86.90797797483394
At time: 701.6820523738861 and batch: 750, loss is 4.4698309803009035 and perplexity is 87.34195925412145
At time: 702.5061938762665 and batch: 800, loss is 4.488245611190796 and perplexity is 88.96522928374
At time: 703.3294360637665 and batch: 850, loss is 4.5223630428314205 and perplexity is 92.05286603538931
At time: 704.1522278785706 and batch: 900, loss is 4.48660493850708 and perplexity is 88.81938613540356
At time: 704.9778432846069 and batch: 950, loss is 4.474960861206054 and perplexity is 87.79116430222281
At time: 705.8001725673676 and batch: 1000, loss is 4.460354280471802 and perplexity is 86.51815536022288
At time: 706.6242587566376 and batch: 1050, loss is 4.453921518325806 and perplexity is 85.96339089244566
At time: 707.448383808136 and batch: 1100, loss is 4.4093585777282716 and perplexity is 82.21671095816453
At time: 708.2724597454071 and batch: 1150, loss is 4.441936016082764 and perplexity is 84.93922630185116
At time: 709.0977902412415 and batch: 1200, loss is 4.460865135192871 and perplexity is 86.56236485970116
At time: 709.9237921237946 and batch: 1250, loss is 4.462206010818481 and perplexity is 86.67851207692077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.607170606181569 and perplexity of 100.20024223687942
Finished 32 epochs...
Completing Train Step...
At time: 712.1955614089966 and batch: 50, loss is 4.415157957077026 and perplexity is 82.6949021195493
At time: 713.0212848186493 and batch: 100, loss is 4.455841827392578 and perplexity is 86.12862577162873
At time: 713.8439056873322 and batch: 150, loss is 4.3677763175964355 and perplexity is 78.86805904789543
At time: 714.6942126750946 and batch: 200, loss is 4.4129371833801265 and perplexity is 82.51145922402564
At time: 715.5355558395386 and batch: 250, loss is 4.435819473266601 and perplexity is 84.42127753011839
At time: 716.3812358379364 and batch: 300, loss is 4.452695741653442 and perplexity is 85.85808352805338
At time: 717.2632596492767 and batch: 350, loss is 4.44457703590393 and perplexity is 85.16384896800406
At time: 718.0985622406006 and batch: 400, loss is 4.455731391906738 and perplexity is 86.11911464018982
At time: 718.9300262928009 and batch: 450, loss is 4.380390720367432 and perplexity is 79.86923384575847
At time: 719.7561151981354 and batch: 500, loss is 4.4069195652008055 and perplexity is 82.01642771612546
At time: 720.5796432495117 and batch: 550, loss is 4.4163812732696535 and perplexity is 82.79612603416328
At time: 721.4047751426697 and batch: 600, loss is 4.433677911758423 and perplexity is 84.24067762361626
At time: 722.2285141944885 and batch: 650, loss is 4.450188655853271 and perplexity is 85.64309955025574
At time: 723.051887512207 and batch: 700, loss is 4.451302642822266 and perplexity is 85.73855800700575
At time: 723.875809431076 and batch: 750, loss is 4.457050971984863 and perplexity is 86.23283072045072
At time: 724.7017040252686 and batch: 800, loss is 4.475746088027954 and perplexity is 87.86012735143323
At time: 725.5276696681976 and batch: 850, loss is 4.509697227478028 and perplexity is 90.89429404927235
At time: 726.3519656658173 and batch: 900, loss is 4.472909851074219 and perplexity is 87.61128826160038
At time: 727.1743314266205 and batch: 950, loss is 4.461634035110474 and perplexity is 86.62894824961039
At time: 727.9988741874695 and batch: 1000, loss is 4.446694602966309 and perplexity is 85.34438020552538
At time: 728.8232069015503 and batch: 1050, loss is 4.440409164428711 and perplexity is 84.80963566169585
At time: 729.6445498466492 and batch: 1100, loss is 4.3953031635284425 and perplexity is 81.06920426365726
At time: 730.4723391532898 and batch: 1150, loss is 4.428371086120605 and perplexity is 83.79481115006118
At time: 731.2984907627106 and batch: 1200, loss is 4.447855701446533 and perplexity is 85.44353098644908
At time: 732.1233625411987 and batch: 1250, loss is 4.448908796310425 and perplexity is 85.53355852553307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.6063232421875 and perplexity of 100.11537212242872
Finished 33 epochs...
Completing Train Step...
At time: 734.3883306980133 and batch: 50, loss is 4.401967134475708 and perplexity is 81.61125117205658
At time: 735.2384886741638 and batch: 100, loss is 4.442148160934448 and perplexity is 84.9572476329159
At time: 736.0633771419525 and batch: 150, loss is 4.354449348449707 and perplexity is 77.82395965185103
At time: 736.8880536556244 and batch: 200, loss is 4.399945821762085 and perplexity is 81.44645591996564
At time: 737.712630033493 and batch: 250, loss is 4.422626180648804 and perplexity is 83.31479801620917
At time: 738.5382835865021 and batch: 300, loss is 4.439416418075561 and perplexity is 84.7254829832244
At time: 739.3654804229736 and batch: 350, loss is 4.430705909729004 and perplexity is 83.9906858308275
At time: 740.1879434585571 and batch: 400, loss is 4.442256107330322 and perplexity is 84.96641895659796
At time: 741.0116891860962 and batch: 450, loss is 4.366988859176636 and perplexity is 78.80597817700512
At time: 741.8369750976562 and batch: 500, loss is 4.393856372833252 and perplexity is 80.95199889952177
At time: 742.6629722118378 and batch: 550, loss is 4.402693433761597 and perplexity is 81.67054689611776
At time: 743.4877614974976 and batch: 600, loss is 4.420088119506836 and perplexity is 83.10360808466224
At time: 744.3107697963715 and batch: 650, loss is 4.436794490814209 and perplexity is 84.50362989807952
At time: 745.1360459327698 and batch: 700, loss is 4.43817180633545 and perplexity is 84.62009824756039
At time: 745.962560415268 and batch: 750, loss is 4.4447494316101075 and perplexity is 85.1785321155071
At time: 746.7869799137115 and batch: 800, loss is 4.4635913372039795 and perplexity is 86.79867331880902
At time: 747.6125612258911 and batch: 850, loss is 4.497197771072388 and perplexity is 89.76523579048715
At time: 748.4374661445618 and batch: 900, loss is 4.459505262374878 and perplexity is 86.4447310543024
At time: 749.261531829834 and batch: 950, loss is 4.4496265888214115 and perplexity is 85.59497591311415
At time: 750.0837142467499 and batch: 1000, loss is 4.433782453536987 and perplexity is 84.24948475423105
At time: 750.9047141075134 and batch: 1050, loss is 4.427202672958374 and perplexity is 83.69696136540686
At time: 751.7308571338654 and batch: 1100, loss is 4.3819269275665285 and perplexity is 79.99202382905882
At time: 752.556708574295 and batch: 1150, loss is 4.41479528427124 and perplexity is 82.66491636521089
At time: 753.3822362422943 and batch: 1200, loss is 4.435459547042846 and perplexity is 84.39089756609323
At time: 754.2577826976776 and batch: 1250, loss is 4.43619888305664 and perplexity is 84.4533138663664
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.604578616845346 and perplexity of 99.94086057997818
Finished 34 epochs...
Completing Train Step...
At time: 756.5326223373413 and batch: 50, loss is 4.388862390518188 and perplexity is 80.54873383606025
At time: 757.3629152774811 and batch: 100, loss is 4.42834132194519 and perplexity is 83.79231710372004
At time: 758.1904988288879 and batch: 150, loss is 4.3413944339752195 and perplexity is 76.81457754881893
At time: 759.0166981220245 and batch: 200, loss is 4.387066745758057 and perplexity is 80.40422670477722
At time: 759.8416669368744 and batch: 250, loss is 4.409511613845825 and perplexity is 82.22929404721657
At time: 760.6761517524719 and batch: 300, loss is 4.426176300048828 and perplexity is 83.61110114149146
At time: 761.531126499176 and batch: 350, loss is 4.417682809829712 and perplexity is 82.90395837787449
At time: 762.3569295406342 and batch: 400, loss is 4.429753179550171 and perplexity is 83.91070347654693
At time: 763.1820209026337 and batch: 450, loss is 4.352969493865967 and perplexity is 77.70887668246763
At time: 764.0064418315887 and batch: 500, loss is 4.381041946411133 and perplexity is 79.92126371069206
At time: 764.8602914810181 and batch: 550, loss is 4.389117231369019 and perplexity is 80.56926355971937
At time: 765.6968019008636 and batch: 600, loss is 4.406974258422852 and perplexity is 82.02091358149009
At time: 766.5263576507568 and batch: 650, loss is 4.424723558425903 and perplexity is 83.48972400088348
At time: 767.3652393817902 and batch: 700, loss is 4.425473718643189 and perplexity is 83.55237816777468
At time: 768.1991302967072 and batch: 750, loss is 4.432900791168213 and perplexity is 84.17523788907761
At time: 769.0310962200165 and batch: 800, loss is 4.451775875091553 and perplexity is 85.77914186141416
At time: 769.866302728653 and batch: 850, loss is 4.4852251529693605 and perplexity is 88.69691893960713
At time: 770.7166147232056 and batch: 900, loss is 4.447926712036133 and perplexity is 85.44959859739166
At time: 771.5451884269714 and batch: 950, loss is 4.436234807968139 and perplexity is 84.45634789869119
At time: 772.367782831192 and batch: 1000, loss is 4.420812120437622 and perplexity is 83.16379696003898
At time: 773.1923532485962 and batch: 1050, loss is 4.414157333374024 and perplexity is 82.61219702561037
At time: 774.0160944461823 and batch: 1100, loss is 4.368860692977905 and perplexity is 78.95362801556739
At time: 774.8410935401917 and batch: 1150, loss is 4.401768808364868 and perplexity is 81.59506713492247
At time: 775.7155601978302 and batch: 1200, loss is 4.423131580352783 and perplexity is 83.35691593275845
At time: 776.5657870769501 and batch: 1250, loss is 4.423847818374634 and perplexity is 83.41664071135915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.603517407048358 and perplexity of 99.83485861472123
Finished 35 epochs...
Completing Train Step...
At time: 778.7926664352417 and batch: 50, loss is 4.376384840011597 and perplexity is 79.54992722990487
At time: 779.6472125053406 and batch: 100, loss is 4.415251731872559 and perplexity is 82.70265718069608
At time: 780.4711191654205 and batch: 150, loss is 4.328478679656983 and perplexity is 75.82883881986466
At time: 781.2949664592743 and batch: 200, loss is 4.374857177734375 and perplexity is 79.4284945845719
At time: 782.119580745697 and batch: 250, loss is 4.397470111846924 and perplexity is 81.24506751398806
At time: 782.943062543869 and batch: 300, loss is 4.41362998008728 and perplexity is 82.56864269725227
At time: 783.7692606449127 and batch: 350, loss is 4.404891185760498 and perplexity is 81.85023588740341
At time: 784.5923976898193 and batch: 400, loss is 4.417124843597412 and perplexity is 82.85771367126775
At time: 785.4187660217285 and batch: 450, loss is 4.339784336090088 and perplexity is 76.69099807418314
At time: 786.2588567733765 and batch: 500, loss is 4.368458776473999 and perplexity is 78.92190162553167
At time: 787.0987770557404 and batch: 550, loss is 4.376182518005371 and perplexity is 79.53383415707887
At time: 787.923369884491 and batch: 600, loss is 4.3943659114837645 and perplexity is 80.99325758235145
At time: 788.7485542297363 and batch: 650, loss is 4.412505359649658 and perplexity is 82.47583650981734
At time: 789.571977853775 and batch: 700, loss is 4.413568449020386 and perplexity is 82.56356231687732
At time: 790.3975846767426 and batch: 750, loss is 4.421902551651001 and perplexity is 83.25453082059359
At time: 791.2227935791016 and batch: 800, loss is 4.440988845825196 and perplexity is 84.85881248179834
At time: 792.0486347675323 and batch: 850, loss is 4.4732875156402585 and perplexity is 87.6443821895705
At time: 792.9060051441193 and batch: 900, loss is 4.434764680862426 and perplexity is 84.33227755432976
At time: 793.7455010414124 and batch: 950, loss is 4.423390245437622 and perplexity is 83.37848024533722
At time: 794.5760667324066 and batch: 1000, loss is 4.408183040618897 and perplexity is 82.12011894829178
At time: 795.4022843837738 and batch: 1050, loss is 4.401878747940064 and perplexity is 81.60403815506737
At time: 796.2769560813904 and batch: 1100, loss is 4.356096086502075 and perplexity is 77.9522209049544
At time: 797.0980150699615 and batch: 1150, loss is 4.389569301605224 and perplexity is 80.60569475983789
At time: 797.9225912094116 and batch: 1200, loss is 4.411998901367188 and perplexity is 82.4340765150535
At time: 798.7492382526398 and batch: 1250, loss is 4.411757917404175 and perplexity is 82.41421361802354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.601632445398038 and perplexity of 99.64685098412139
Finished 36 epochs...
Completing Train Step...
At time: 801.1439282894135 and batch: 50, loss is 4.363741073608399 and perplexity is 78.5504484359067
At time: 801.9686822891235 and batch: 100, loss is 4.4021850681304935 and perplexity is 81.6290389485033
At time: 802.7929489612579 and batch: 150, loss is 4.3158410453796385 and perplexity is 74.87657156367655
At time: 803.6174957752228 and batch: 200, loss is 4.362891063690186 and perplexity is 78.4837081446293
At time: 804.441878080368 and batch: 250, loss is 4.385023336410523 and perplexity is 80.24009570686614
At time: 805.2683820724487 and batch: 300, loss is 4.4012528419494625 and perplexity is 81.55297767992447
At time: 806.0983724594116 and batch: 350, loss is 4.391832628250122 and perplexity is 80.78833838964704
At time: 806.9243702888489 and batch: 400, loss is 4.405013608932495 and perplexity is 81.86025686629696
At time: 807.751980304718 and batch: 450, loss is 4.326744966506958 and perplexity is 75.69748726071516
At time: 808.5774388313293 and batch: 500, loss is 4.356161937713623 and perplexity is 77.95735432216291
At time: 809.4047377109528 and batch: 550, loss is 4.363839178085327 and perplexity is 78.55815496457933
At time: 810.2590811252594 and batch: 600, loss is 4.3825718212127684 and perplexity is 80.04362681440632
At time: 811.0840146541595 and batch: 650, loss is 4.401505098342896 and perplexity is 81.573552534908
At time: 811.9100768566132 and batch: 700, loss is 4.401578063964844 and perplexity is 81.57950481705656
At time: 812.7365007400513 and batch: 750, loss is 4.409734544754028 and perplexity is 82.24762754189477
At time: 813.5599255561829 and batch: 800, loss is 4.429110317230225 and perplexity is 83.8567777822973
At time: 814.3871214389801 and batch: 850, loss is 4.461630449295044 and perplexity is 86.62863761474806
At time: 815.2116706371307 and batch: 900, loss is 4.422405118942261 and perplexity is 83.2963823403548
At time: 816.0371036529541 and batch: 950, loss is 4.410779237747192 and perplexity is 82.33359595957528
At time: 816.8610117435455 and batch: 1000, loss is 4.396158065795898 and perplexity is 81.13854014368073
At time: 817.7423310279846 and batch: 1050, loss is 4.3896776390075685 and perplexity is 80.61442784447364
At time: 818.5679664611816 and batch: 1100, loss is 4.34397334098816 and perplexity is 77.01293085899357
At time: 819.3935310840607 and batch: 1150, loss is 4.377948350906372 and perplexity is 79.67440169103828
At time: 820.2188546657562 and batch: 1200, loss is 4.40043046951294 and perplexity is 81.48593832839977
At time: 821.0554602146149 and batch: 1250, loss is 4.400125398635864 and perplexity is 81.46108313321544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.6000330569970345 and perplexity of 99.48760434901806
Finished 37 epochs...
Completing Train Step...
At time: 823.4300899505615 and batch: 50, loss is 4.351635723114014 and perplexity is 77.6052999447479
At time: 824.2556984424591 and batch: 100, loss is 4.3900251293182375 and perplexity is 80.6424454446901
At time: 825.0811462402344 and batch: 150, loss is 4.303660402297973 and perplexity is 73.97005893178911
At time: 825.9084134101868 and batch: 200, loss is 4.351723585128784 and perplexity is 77.6121188023128
At time: 826.7360944747925 and batch: 250, loss is 4.373705282211303 and perplexity is 79.33705393240528
At time: 827.5637762546539 and batch: 300, loss is 4.3893686866760255 and perplexity is 80.58952567602466
At time: 828.3946604728699 and batch: 350, loss is 4.379636211395264 and perplexity is 79.80899452063507
At time: 829.2244534492493 and batch: 400, loss is 4.393280124664306 and perplexity is 80.9053638963415
At time: 830.0552768707275 and batch: 450, loss is 4.314368600845337 and perplexity is 74.7664010949876
At time: 830.8775224685669 and batch: 500, loss is 4.345004014968872 and perplexity is 77.09234700205424
At time: 831.7000081539154 and batch: 550, loss is 4.352210855484008 and perplexity is 77.64994610232381
At time: 832.5229341983795 and batch: 600, loss is 4.370493688583374 and perplexity is 79.08266427229793
At time: 833.3467783927917 and batch: 650, loss is 4.389281215667725 and perplexity is 80.58247673724871
At time: 834.1696016788483 and batch: 700, loss is 4.389201364517212 and perplexity is 80.57604239066845
At time: 834.9919214248657 and batch: 750, loss is 4.39793360710144 and perplexity is 81.28273294543374
At time: 835.815761089325 and batch: 800, loss is 4.418002367019653 and perplexity is 82.93045516724304
At time: 836.640664100647 and batch: 850, loss is 4.4500267219543455 and perplexity is 85.62923215206072
At time: 837.4695496559143 and batch: 900, loss is 4.4102694702148435 and perplexity is 82.29163566144103
At time: 838.3186836242676 and batch: 950, loss is 4.3989935302734375 and perplexity is 81.3689320717216
At time: 839.1916399002075 and batch: 1000, loss is 4.384452543258667 and perplexity is 80.1943082785508
At time: 840.0156006813049 and batch: 1050, loss is 4.377700252532959 and perplexity is 79.65463705346465
At time: 840.8417508602142 and batch: 1100, loss is 4.331990690231323 and perplexity is 76.09561869630346
At time: 841.6679611206055 and batch: 1150, loss is 4.366012134552002 and perplexity is 78.72904401541255
At time: 842.4929893016815 and batch: 1200, loss is 4.389073276519776 and perplexity is 80.56572222771592
At time: 843.3164944648743 and batch: 1250, loss is 4.388951835632324 and perplexity is 80.5559388489735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.599163417398494 and perplexity of 99.40112359770544
Finished 38 epochs...
Completing Train Step...
At time: 845.5896713733673 and batch: 50, loss is 4.339140539169311 and perplexity is 76.64164053559216
At time: 846.4413485527039 and batch: 100, loss is 4.377414789199829 and perplexity is 79.6319018204642
At time: 847.2658045291901 and batch: 150, loss is 4.291803097724914 and perplexity is 73.09815285718501
At time: 848.0902860164642 and batch: 200, loss is 4.340454359054565 and perplexity is 76.74240002238136
At time: 848.9142553806305 and batch: 250, loss is 4.3618222427368165 and perplexity is 78.39986792593886
At time: 849.738153219223 and batch: 300, loss is 4.377581968307495 and perplexity is 79.64521572362449
At time: 850.5627596378326 and batch: 350, loss is 4.367930822372436 and perplexity is 78.88024548109905
At time: 851.3844957351685 and batch: 400, loss is 4.3818439102172855 and perplexity is 79.98538337892005
At time: 852.209874868393 and batch: 450, loss is 4.302140436172485 and perplexity is 73.85771235104046
At time: 853.0346848964691 and batch: 500, loss is 4.33418794631958 and perplexity is 76.26300408471168
At time: 853.8603899478912 and batch: 550, loss is 4.340094900131225 and perplexity is 76.71481923927108
At time: 854.6857883930206 and batch: 600, loss is 4.358652229309082 and perplexity is 78.15173279552086
At time: 855.5101120471954 and batch: 650, loss is 4.377691469192505 and perplexity is 79.65393742274121
At time: 856.3340966701508 and batch: 700, loss is 4.377174625396728 and perplexity is 79.6127794164214
At time: 857.1584892272949 and batch: 750, loss is 4.387329654693604 and perplexity is 80.4253684734918
At time: 857.9829754829407 and batch: 800, loss is 4.4068030166625975 and perplexity is 82.00686937838208
At time: 858.8088583946228 and batch: 850, loss is 4.438739175796509 and perplexity is 84.66812272962186
At time: 859.6329965591431 and batch: 900, loss is 4.398648595809936 and perplexity is 81.34086996286423
At time: 860.5108962059021 and batch: 950, loss is 4.387297344207764 and perplexity is 80.42276993274285
At time: 861.3401906490326 and batch: 1000, loss is 4.372914400100708 and perplexity is 79.2743324816488
At time: 862.1703059673309 and batch: 1050, loss is 4.366523284912109 and perplexity is 78.76929668131783
At time: 862.9942660331726 and batch: 1100, loss is 4.320537900924682 and perplexity is 75.22908320409884
At time: 863.8195824623108 and batch: 1150, loss is 4.355051803588867 and perplexity is 77.87085922231816
At time: 864.6439986228943 and batch: 1200, loss is 4.378646144866943 and perplexity is 79.73001740924984
At time: 865.4671130180359 and batch: 1250, loss is 4.378188447952271 and perplexity is 79.69353357618066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.598015332744069 and perplexity of 99.28706817823671
Finished 39 epochs...
Completing Train Step...
At time: 867.7249717712402 and batch: 50, loss is 4.327913637161255 and perplexity is 75.78600440630137
At time: 868.549382686615 and batch: 100, loss is 4.365748891830444 and perplexity is 78.7083218951936
At time: 869.3753674030304 and batch: 150, loss is 4.280108237266541 and perplexity is 72.24825953826016
At time: 870.1998112201691 and batch: 200, loss is 4.330041837692261 and perplexity is 75.94746396934487
At time: 871.0227930545807 and batch: 250, loss is 4.350271472930908 and perplexity is 77.49949908591833
At time: 871.8486204147339 and batch: 300, loss is 4.366384267807007 and perplexity is 78.75834716282507
At time: 872.6729807853699 and batch: 350, loss is 4.355923643112183 and perplexity is 77.93877971868712
At time: 873.5004005432129 and batch: 400, loss is 4.370810651779175 and perplexity is 79.10773453926423
At time: 874.3251714706421 and batch: 450, loss is 4.29003586769104 and perplexity is 72.96908568531718
At time: 875.1506242752075 and batch: 500, loss is 4.322967376708984 and perplexity is 75.41207263420738
At time: 875.9760999679565 and batch: 550, loss is 4.328289022445679 and perplexity is 75.81445869744773
At time: 876.8012874126434 and batch: 600, loss is 4.347074098587036 and perplexity is 77.25209990054975
At time: 877.628660440445 and batch: 650, loss is 4.366858301162719 and perplexity is 78.79569009661988
At time: 878.4528663158417 and batch: 700, loss is 4.365808458328247 and perplexity is 78.71301041391477
At time: 879.2753393650055 and batch: 750, loss is 4.375932836532593 and perplexity is 79.51397851112746
At time: 880.1122958660126 and batch: 800, loss is 4.397024345397949 and perplexity is 81.20885925955773
At time: 880.9487595558167 and batch: 850, loss is 4.427829723358155 and perplexity is 83.74946003642765
At time: 881.8047959804535 and batch: 900, loss is 4.387327718734741 and perplexity is 80.42521277343764
At time: 882.6285495758057 and batch: 950, loss is 4.376701431274414 and perplexity is 79.57511602889004
At time: 883.4528300762177 and batch: 1000, loss is 4.362051725387573 and perplexity is 78.41786139996559
At time: 884.278059720993 and batch: 1050, loss is 4.354955224990845 and perplexity is 77.86333892706334
At time: 885.1034417152405 and batch: 1100, loss is 4.3096769905090335 and perplexity is 74.41644783886653
At time: 885.9295678138733 and batch: 1150, loss is 4.34303988456726 and perplexity is 76.94107618601141
At time: 886.7544000148773 and batch: 1200, loss is 4.368038339614868 and perplexity is 78.888726923518
At time: 887.5801112651825 and batch: 1250, loss is 4.3677307224273685 and perplexity is 78.86446312738808
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.596826706489507 and perplexity of 99.16912307247262
Finished 40 epochs...
Completing Train Step...
At time: 889.8437926769257 and batch: 50, loss is 4.3165781211853025 and perplexity is 74.9317816174851
At time: 890.7010817527771 and batch: 100, loss is 4.353869934082031 and perplexity is 77.77888039252574
At time: 891.5254986286163 and batch: 150, loss is 4.268592395782471 and perplexity is 71.42103228472685
At time: 892.3588709831238 and batch: 200, loss is 4.319410371780395 and perplexity is 75.14430802251358
At time: 893.1862781047821 and batch: 250, loss is 4.338618793487549 and perplexity is 76.6016635204239
At time: 894.014928817749 and batch: 300, loss is 4.3551936435699465 and perplexity is 77.88190520687951
At time: 894.8423931598663 and batch: 350, loss is 4.344110689163208 and perplexity is 77.02350917094148
At time: 895.6693816184998 and batch: 400, loss is 4.360106964111328 and perplexity is 78.26550557562133
At time: 896.4945073127747 and batch: 450, loss is 4.278654613494873 and perplexity is 72.14331404484504
At time: 897.3202624320984 and batch: 500, loss is 4.311507692337036 and perplexity is 74.55280694430681
At time: 898.1439695358276 and batch: 550, loss is 4.317208204269409 and perplexity is 74.97900974280321
At time: 898.9692423343658 and batch: 600, loss is 4.335685482025147 and perplexity is 76.37729621325457
At time: 899.7945585250854 and batch: 650, loss is 4.355731239318848 and perplexity is 77.92378544434523
At time: 900.6206436157227 and batch: 700, loss is 4.3549465084075925 and perplexity is 77.86266022774522
At time: 901.4430956840515 and batch: 750, loss is 4.36522967338562 and perplexity is 78.66746569026995
At time: 902.322535276413 and batch: 800, loss is 4.386922268867493 and perplexity is 80.392610991236
At time: 903.1476769447327 and batch: 850, loss is 4.416893482208252 and perplexity is 82.83854581296681
At time: 903.9755067825317 and batch: 900, loss is 4.375941352844238 and perplexity is 79.5146556798321
At time: 904.8013074398041 and batch: 950, loss is 4.366005620956421 and perplexity is 78.72853120792945
At time: 905.6299011707306 and batch: 1000, loss is 4.351030158996582 and perplexity is 77.55831918614531
At time: 906.4573638439178 and batch: 1050, loss is 4.343803224563598 and perplexity is 76.9998308088493
At time: 907.2853789329529 and batch: 1100, loss is 4.2982105541229245 and perplexity is 73.5680298349312
At time: 908.1140983104706 and batch: 1150, loss is 4.332042684555054 and perplexity is 76.09957533939706
At time: 908.963297367096 and batch: 1200, loss is 4.35789942741394 and perplexity is 78.0929221621193
At time: 909.812260389328 and batch: 1250, loss is 4.3570370388031 and perplexity is 78.0256047465178
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.597550218122719 and perplexity of 99.24089904891692
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 912.1009681224823 and batch: 50, loss is 4.315197019577027 and perplexity is 74.82836464449457
At time: 912.9276468753815 and batch: 100, loss is 4.355528326034546 and perplexity is 77.90797527721617
At time: 913.7677760124207 and batch: 150, loss is 4.269441928863525 and perplexity is 71.48173259414558
At time: 914.6200423240662 and batch: 200, loss is 4.323861494064331 and perplexity is 75.47953003007243
At time: 915.4460573196411 and batch: 250, loss is 4.341735582351685 and perplexity is 76.84078718767228
At time: 916.2710444927216 and batch: 300, loss is 4.3525033378601075 and perplexity is 77.67266066470644
At time: 917.0983417034149 and batch: 350, loss is 4.340019121170044 and perplexity is 76.70900609022188
At time: 917.92693400383 and batch: 400, loss is 4.353079986572266 and perplexity is 77.717463420933
At time: 918.7736530303955 and batch: 450, loss is 4.270297584533691 and perplexity is 71.54292251897276
At time: 919.6029465198517 and batch: 500, loss is 4.299200706481933 and perplexity is 73.64090946823603
At time: 920.4302682876587 and batch: 550, loss is 4.300587320327759 and perplexity is 73.74309180026994
At time: 921.257566690445 and batch: 600, loss is 4.317092599868775 and perplexity is 74.97034234032631
At time: 922.0861649513245 and batch: 650, loss is 4.337267446517944 and perplexity is 76.49821800569248
At time: 922.9116816520691 and batch: 700, loss is 4.333785667419433 and perplexity is 76.23233125723588
At time: 923.7907683849335 and batch: 750, loss is 4.344271411895752 and perplexity is 77.0358895946867
At time: 924.6190643310547 and batch: 800, loss is 4.364544377326966 and perplexity is 78.61357365419713
At time: 925.4468038082123 and batch: 850, loss is 4.397851400375366 and perplexity is 81.27605123271654
At time: 926.2720370292664 and batch: 900, loss is 4.3533501625061035 and perplexity is 77.7384636459382
At time: 927.0993661880493 and batch: 950, loss is 4.3385541629791256 and perplexity is 76.59671287594756
At time: 927.927262544632 and batch: 1000, loss is 4.319149742126465 and perplexity is 75.12472573949148
At time: 928.7545487880707 and batch: 1050, loss is 4.31083251953125 and perplexity is 74.50248790539781
At time: 929.5805287361145 and batch: 1100, loss is 4.2612330436706545 and perplexity is 70.89734910795796
At time: 930.4080514907837 and batch: 1150, loss is 4.292716197967529 and perplexity is 73.16492928043816
At time: 931.2369439601898 and batch: 1200, loss is 4.322179870605469 and perplexity is 75.35270854458861
At time: 932.0656452178955 and batch: 1250, loss is 4.322060174942017 and perplexity is 75.34368969191601
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.582753425096944 and perplexity of 97.78326277223901
Finished 42 epochs...
Completing Train Step...
At time: 934.3435926437378 and batch: 50, loss is 4.30475848197937 and perplexity is 74.05132856263288
At time: 935.2146730422974 and batch: 100, loss is 4.341883363723755 and perplexity is 76.85214366375097
At time: 936.0418839454651 and batch: 150, loss is 4.254505953788757 and perplexity is 70.42201686203693
At time: 936.8650197982788 and batch: 200, loss is 4.30750470161438 and perplexity is 74.25496926819875
At time: 937.6899824142456 and batch: 250, loss is 4.325574903488159 and perplexity is 75.60896822677839
At time: 938.5156435966492 and batch: 300, loss is 4.339181690216065 and perplexity is 76.64479448421878
At time: 939.3402771949768 and batch: 350, loss is 4.3268629169464115 and perplexity is 75.70641633918713
At time: 940.1728851795197 and batch: 400, loss is 4.341795301437378 and perplexity is 76.84537618625106
At time: 941.0152962207794 and batch: 450, loss is 4.2600225067138675 and perplexity is 70.81157717224775
At time: 941.8414433002472 and batch: 500, loss is 4.289387617111206 and perplexity is 72.92179876175545
At time: 942.6668243408203 and batch: 550, loss is 4.291060743331909 and perplexity is 73.04390825913987
At time: 943.5008428096771 and batch: 600, loss is 4.308787803649903 and perplexity is 74.35030712143129
At time: 944.3415706157684 and batch: 650, loss is 4.3297982597351075 and perplexity is 75.92896709402723
At time: 945.2159929275513 and batch: 700, loss is 4.326551332473755 and perplexity is 75.68283106996799
At time: 946.0421521663666 and batch: 750, loss is 4.3383392810821535 and perplexity is 76.58025539725341
At time: 946.8679285049438 and batch: 800, loss is 4.358764672279358 and perplexity is 78.16052090255978
At time: 947.6923558712006 and batch: 850, loss is 4.393121433258057 and perplexity is 80.89252592903615
At time: 948.518660068512 and batch: 900, loss is 4.348801527023316 and perplexity is 77.38566270155776
At time: 949.3438115119934 and batch: 950, loss is 4.334819011688232 and perplexity is 76.31114621431661
At time: 950.167530298233 and batch: 1000, loss is 4.316250848770141 and perplexity is 74.90726252477302
At time: 951.0005886554718 and batch: 1050, loss is 4.308987417221069 and perplexity is 74.36514993311717
At time: 951.8294396400452 and batch: 1100, loss is 4.259866075515747 and perplexity is 70.80050089874986
At time: 952.6542174816132 and batch: 1150, loss is 4.292380599975586 and perplexity is 73.14037939676724
At time: 953.4800267219543 and batch: 1200, loss is 4.323194980621338 and perplexity is 75.42923867043203
At time: 954.3049025535583 and batch: 1250, loss is 4.322566385269165 and perplexity is 75.38183910072122
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.582494582572993 and perplexity of 97.75795558113282
Finished 43 epochs...
Completing Train Step...
At time: 956.5514588356018 and batch: 50, loss is 4.2978722095489506 and perplexity is 73.54314270166925
At time: 957.4256763458252 and batch: 100, loss is 4.334206161499023 and perplexity is 76.26439324166778
At time: 958.2737777233124 and batch: 150, loss is 4.246638135910034 and perplexity is 69.87012320507094
At time: 959.1009340286255 and batch: 200, loss is 4.29956356048584 and perplexity is 73.66763521556689
At time: 959.9288339614868 and batch: 250, loss is 4.317907648086548 and perplexity is 75.0314716925374
At time: 960.7576298713684 and batch: 300, loss is 4.332031450271606 and perplexity is 76.09872041999968
At time: 961.5970513820648 and batch: 350, loss is 4.319601211547852 and perplexity is 75.15864991324028
At time: 962.4322507381439 and batch: 400, loss is 4.335240879058838 and perplexity is 76.34334618849962
At time: 963.258998632431 and batch: 450, loss is 4.25391263961792 and perplexity is 70.38024687408196
At time: 964.0876350402832 and batch: 500, loss is 4.283267393112182 and perplexity is 72.47686395808478
At time: 964.9161601066589 and batch: 550, loss is 4.285302782058716 and perplexity is 72.62453279664508
At time: 965.7416450977325 and batch: 600, loss is 4.303443946838379 and perplexity is 73.95404944142011
At time: 966.6206691265106 and batch: 650, loss is 4.324787273406982 and perplexity is 75.54943977527317
At time: 967.4452841281891 and batch: 700, loss is 4.321462106704712 and perplexity is 75.29864249622112
At time: 968.2738742828369 and batch: 750, loss is 4.3339125919342045 and perplexity is 76.24200762296162
At time: 969.1031532287598 and batch: 800, loss is 4.354633855819702 and perplexity is 77.83832007072981
At time: 969.9290854930878 and batch: 850, loss is 4.389462261199951 and perplexity is 80.59706715536288
At time: 970.7581045627594 and batch: 900, loss is 4.345057153701783 and perplexity is 77.09644370053677
At time: 971.5835657119751 and batch: 950, loss is 4.331333818435669 and perplexity is 76.04565004390496
At time: 972.4099962711334 and batch: 1000, loss is 4.313109636306763 and perplexity is 74.67233207453754
At time: 973.2350134849548 and batch: 1050, loss is 4.306273736953735 and perplexity is 74.16362026038748
At time: 974.0618031024933 and batch: 1100, loss is 4.257264461517334 and perplexity is 70.6165447197595
At time: 974.8868591785431 and batch: 1150, loss is 4.290132789611817 and perplexity is 72.97615833200081
At time: 975.713849067688 and batch: 1200, loss is 4.32148775100708 and perplexity is 75.30057350213673
At time: 976.5374875068665 and batch: 1250, loss is 4.320779418945312 and perplexity is 75.24725457763685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.582244204778741 and perplexity of 97.73348222376428
Finished 44 epochs...
Completing Train Step...
At time: 978.8275630474091 and batch: 50, loss is 4.291616849899292 and perplexity is 73.08453975290323
At time: 979.6549003124237 and batch: 100, loss is 4.327947635650634 and perplexity is 75.7885810597682
At time: 980.4795293807983 and batch: 150, loss is 4.240482697486877 and perplexity is 69.44136292161437
At time: 981.3047552108765 and batch: 200, loss is 4.293543205261231 and perplexity is 73.22546223774478
At time: 982.1304602622986 and batch: 250, loss is 4.311964149475098 and perplexity is 74.58684487303583
At time: 982.9796624183655 and batch: 300, loss is 4.326419734954834 and perplexity is 75.67287205247969
At time: 983.8293056488037 and batch: 350, loss is 4.313590984344483 and perplexity is 74.70828410707959
At time: 984.6828172206879 and batch: 400, loss is 4.329806470870972 and perplexity is 75.92959055965177
At time: 985.5232377052307 and batch: 450, loss is 4.248014001846314 and perplexity is 69.96632129021138
At time: 986.3629829883575 and batch: 500, loss is 4.2781917762756345 and perplexity is 72.1099311600026
At time: 987.2404172420502 and batch: 550, loss is 4.280336589813232 and perplexity is 72.26475949615174
At time: 988.0747971534729 and batch: 600, loss is 4.298725547790528 and perplexity is 73.6059266619096
At time: 988.902594089508 and batch: 650, loss is 4.320401821136475 and perplexity is 75.21884674289291
At time: 989.735523223877 and batch: 700, loss is 4.3169248580932615 and perplexity is 74.95776773666397
At time: 990.564840555191 and batch: 750, loss is 4.329833688735962 and perplexity is 75.93165722912143
At time: 991.391161441803 and batch: 800, loss is 4.35039813041687 and perplexity is 77.50931559929023
At time: 992.2170996665955 and batch: 850, loss is 4.385949420928955 and perplexity is 80.31443923614025
At time: 993.0433690547943 and batch: 900, loss is 4.341446304321289 and perplexity is 76.81856205087746
At time: 993.868260383606 and batch: 950, loss is 4.327788724899292 and perplexity is 75.77653839628849
At time: 994.6944923400879 and batch: 1000, loss is 4.309648952484131 and perplexity is 74.41436137789913
At time: 995.5202567577362 and batch: 1050, loss is 4.303068375587463 and perplexity is 73.92627964165673
At time: 996.3442988395691 and batch: 1100, loss is 4.2540386772155765 and perplexity is 70.38911799035577
At time: 997.168853521347 and batch: 1150, loss is 4.287310390472412 and perplexity is 72.7704808740022
At time: 997.9946765899658 and batch: 1200, loss is 4.31894627571106 and perplexity is 75.1094419357601
At time: 998.8187854290009 and batch: 1250, loss is 4.318156147003174 and perplexity is 75.05011924881866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.582174704892792 and perplexity of 97.72668999392872
Finished 45 epochs...
Completing Train Step...
At time: 1001.0816686153412 and batch: 50, loss is 4.286274805068969 and perplexity is 72.69515983362399
At time: 1001.9335725307465 and batch: 100, loss is 4.322275428771973 and perplexity is 75.35990945530568
At time: 1002.7590065002441 and batch: 150, loss is 4.235085725784302 and perplexity is 69.06759935386408
At time: 1003.584662437439 and batch: 200, loss is 4.288254709243774 and perplexity is 72.83923186140754
At time: 1004.4176280498505 and batch: 250, loss is 4.306695003509521 and perplexity is 74.19486949492573
At time: 1005.2618594169617 and batch: 300, loss is 4.321176900863647 and perplexity is 75.2771699457509
At time: 1006.087319612503 and batch: 350, loss is 4.308351612091064 and perplexity is 74.31788321709864
At time: 1006.913078546524 and batch: 400, loss is 4.325008897781372 and perplexity is 75.5661852281306
At time: 1007.7397706508636 and batch: 450, loss is 4.242985782623291 and perplexity is 69.61539828679639
At time: 1008.6119675636292 and batch: 500, loss is 4.273450107574463 and perplexity is 71.76881911564332
At time: 1009.435821056366 and batch: 550, loss is 4.2763134765625 and perplexity is 71.974614219659
At time: 1010.2605249881744 and batch: 600, loss is 4.294254426956177 and perplexity is 73.27756029955052
At time: 1011.0870563983917 and batch: 650, loss is 4.316334142684936 and perplexity is 74.91350210377114
At time: 1011.9126451015472 and batch: 700, loss is 4.312388486862183 and perplexity is 74.61850157598647
At time: 1012.7368750572205 and batch: 750, loss is 4.325824127197266 and perplexity is 75.62781412260605
At time: 1013.5626530647278 and batch: 800, loss is 4.3463372802734375 and perplexity is 77.19520010355953
At time: 1014.3875873088837 and batch: 850, loss is 4.382506914138794 and perplexity is 80.0384315854049
At time: 1015.2126948833466 and batch: 900, loss is 4.3378504943847656 and perplexity is 76.54283313363003
At time: 1016.0350961685181 and batch: 950, loss is 4.324152116775513 and perplexity is 75.50146928359675
At time: 1016.8603579998016 and batch: 1000, loss is 4.306176662445068 and perplexity is 74.15642121281738
At time: 1017.6849796772003 and batch: 1050, loss is 4.299693946838379 and perplexity is 73.67724109604706
At time: 1018.5265281200409 and batch: 1100, loss is 4.2506907081604 and perplexity is 70.15385145380145
At time: 1019.359537601471 and batch: 1150, loss is 4.283878488540649 and perplexity is 72.52116777387012
At time: 1020.1856417655945 and batch: 1200, loss is 4.316020364761353 and perplexity is 74.88999958811077
At time: 1021.0112748146057 and batch: 1250, loss is 4.315156955718994 and perplexity is 74.82536679156979
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.582219256101734 and perplexity of 97.73104393309971
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 1023.2928991317749 and batch: 50, loss is 4.288948240280152 and perplexity is 72.88976565071947
At time: 1024.1220705509186 and batch: 100, loss is 4.328552293777466 and perplexity is 75.83442109860637
At time: 1024.9444243907928 and batch: 150, loss is 4.241855921745301 and perplexity is 69.53678699004367
At time: 1025.7708427906036 and batch: 200, loss is 4.296777248382568 and perplexity is 73.46265988717661
At time: 1026.5967774391174 and batch: 250, loss is 4.318590478897095 and perplexity is 75.08272298916057
At time: 1027.444009065628 and batch: 300, loss is 4.331098423004151 and perplexity is 76.02775135201556
At time: 1028.2706718444824 and batch: 350, loss is 4.313681421279907 and perplexity is 74.71504080086775
At time: 1029.104379415512 and batch: 400, loss is 4.328680009841919 and perplexity is 75.84410699092803
At time: 1029.9794795513153 and batch: 450, loss is 4.247237339019775 and perplexity is 69.91200214591615
At time: 1030.803655385971 and batch: 500, loss is 4.275762825012207 and perplexity is 71.9349921967226
At time: 1031.6304059028625 and batch: 550, loss is 4.274285063743592 and perplexity is 71.82876795776002
At time: 1032.4690854549408 and batch: 600, loss is 4.2912814235687256 and perplexity is 73.060029384854
At time: 1033.301770210266 and batch: 650, loss is 4.312467756271363 and perplexity is 74.62441677496435
At time: 1034.1312515735626 and batch: 700, loss is 4.3064932346344 and perplexity is 74.17990078972814
At time: 1034.9542281627655 and batch: 750, loss is 4.318263874053955 and perplexity is 75.05820461232449
At time: 1035.7789521217346 and batch: 800, loss is 4.338955664634705 and perplexity is 76.62747275763769
At time: 1036.6032915115356 and batch: 850, loss is 4.381247215270996 and perplexity is 79.93767074124135
At time: 1037.4264941215515 and batch: 900, loss is 4.341936130523681 and perplexity is 76.8561990124325
At time: 1038.2508628368378 and batch: 950, loss is 4.3226296043396 and perplexity is 75.38660482115745
At time: 1039.077642440796 and batch: 1000, loss is 4.300942115783691 and perplexity is 73.76926015607941
At time: 1039.9032475948334 and batch: 1050, loss is 4.290734643936157 and perplexity is 73.0200925681455
At time: 1040.72794175148 and batch: 1100, loss is 4.238610897064209 and perplexity is 69.31150412193995
At time: 1041.5541925430298 and batch: 1150, loss is 4.2707036590576175 and perplexity is 71.57198017656292
At time: 1042.3799631595612 and batch: 1200, loss is 4.302615966796875 and perplexity is 73.89284230713372
At time: 1043.2059590816498 and batch: 1250, loss is 4.302617321014404 and perplexity is 73.89294237418382
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.57420360258896 and perplexity of 96.9507970195601
Finished 47 epochs...
Completing Train Step...
At time: 1045.4226236343384 and batch: 50, loss is 4.286946191787719 and perplexity is 72.74398278616069
At time: 1046.2759809494019 and batch: 100, loss is 4.322988986968994 and perplexity is 75.41370232631387
At time: 1047.0974643230438 and batch: 150, loss is 4.235289082527161 and perplexity is 69.08164614411214
At time: 1047.9174315929413 and batch: 200, loss is 4.289145278930664 and perplexity is 72.90412916681687
At time: 1048.73716878891 and batch: 250, loss is 4.310009975433349 and perplexity is 74.44123152018513
At time: 1049.5724263191223 and batch: 300, loss is 4.322819080352783 and perplexity is 75.40089012780511
At time: 1050.4075934886932 and batch: 350, loss is 4.3064798164367675 and perplexity is 74.17890543583695
At time: 1051.2830588817596 and batch: 400, loss is 4.3227576160430905 and perplexity is 75.39625580656734
At time: 1052.1052129268646 and batch: 450, loss is 4.2414369297027585 and perplexity is 69.5076577325206
At time: 1052.9479167461395 and batch: 500, loss is 4.270657367706299 and perplexity is 71.56866708956824
At time: 1053.7755012512207 and batch: 550, loss is 4.2700297117233275 and perplexity is 71.52376068183816
At time: 1054.6099064350128 and batch: 600, loss is 4.287616548538208 and perplexity is 72.79276355451071
At time: 1055.4699132442474 and batch: 650, loss is 4.309226713180542 and perplexity is 74.38294734236088
At time: 1056.3024520874023 and batch: 700, loss is 4.303685865402222 and perplexity is 73.97194246309112
At time: 1057.1498329639435 and batch: 750, loss is 4.316387948989868 and perplexity is 74.91753303095251
At time: 1057.978554725647 and batch: 800, loss is 4.337169976234436 and perplexity is 76.49076206606776
At time: 1058.8039376735687 and batch: 850, loss is 4.379652767181397 and perplexity is 79.8103158322175
At time: 1059.6308722496033 and batch: 900, loss is 4.340484294891358 and perplexity is 76.74469740473036
At time: 1060.4568161964417 and batch: 950, loss is 4.321708097457885 and perplexity is 75.31716754440356
At time: 1061.2864439487457 and batch: 1000, loss is 4.300420560836792 and perplexity is 73.73079546511278
At time: 1062.1178255081177 and batch: 1050, loss is 4.290544085502624 and perplexity is 73.00617929937673
At time: 1062.9415419101715 and batch: 1100, loss is 4.238593320846558 and perplexity is 69.31028589856369
At time: 1063.766597032547 and batch: 1150, loss is 4.271236925125122 and perplexity is 71.61015726334296
At time: 1064.5925781726837 and batch: 1200, loss is 4.303290777206421 and perplexity is 73.94272279435667
At time: 1065.4168584346771 and batch: 1250, loss is 4.303110857009887 and perplexity is 73.9294202018777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.573832045506387 and perplexity of 96.9147809556925
Finished 48 epochs...
Completing Train Step...
At time: 1067.6598844528198 and batch: 50, loss is 4.283700704574585 and perplexity is 72.50827581906503
At time: 1068.5326585769653 and batch: 100, loss is 4.3195658302307125 and perplexity is 75.15599074825461
At time: 1069.357370853424 and batch: 150, loss is 4.231781053543091 and perplexity is 68.83973029915924
At time: 1070.1961097717285 and batch: 200, loss is 4.285520935058594 and perplexity is 72.64037778459239
At time: 1071.0277445316315 and batch: 250, loss is 4.306330480575562 and perplexity is 74.16782869220849
At time: 1071.918167591095 and batch: 300, loss is 4.3193574714660645 and perplexity is 75.1403329701407
At time: 1072.7657570838928 and batch: 350, loss is 4.303246421813965 and perplexity is 73.93944310860428
At time: 1073.5886776447296 and batch: 400, loss is 4.319663639068604 and perplexity is 75.16334202787445
At time: 1074.4153690338135 and batch: 450, loss is 4.237883667945862 and perplexity is 69.2611171015809
At time: 1075.2385947704315 and batch: 500, loss is 4.267984609603882 and perplexity is 71.37763675738042
At time: 1076.0641868114471 and batch: 550, loss is 4.26758270740509 and perplexity is 71.34895569210069
At time: 1076.8861796855927 and batch: 600, loss is 4.285381679534912 and perplexity is 72.6302629150356
At time: 1077.7190415859222 and batch: 650, loss is 4.307112684249878 and perplexity is 74.22586573576284
At time: 1078.5536081790924 and batch: 700, loss is 4.301812763214111 and perplexity is 73.83351514053795
At time: 1079.3889963626862 and batch: 750, loss is 4.314704294204712 and perplexity is 74.79150389252503
At time: 1080.2246437072754 and batch: 800, loss is 4.335562930107117 and perplexity is 76.36793660264055
At time: 1081.048733472824 and batch: 850, loss is 4.378280820846558 and perplexity is 79.70089543854611
At time: 1081.8726825714111 and batch: 900, loss is 4.339300384521485 and perplexity is 76.6538923247844
At time: 1082.7097725868225 and batch: 950, loss is 4.320811834335327 and perplexity is 75.24969378627526
At time: 1083.5367949008942 and batch: 1000, loss is 4.299446020126343 and perplexity is 73.65897680410671
At time: 1084.3644742965698 and batch: 1050, loss is 4.289730319976806 and perplexity is 72.94679355380464
At time: 1085.188658952713 and batch: 1100, loss is 4.237822532653809 and perplexity is 69.25688293238883
At time: 1086.0122783184052 and batch: 1150, loss is 4.270752429962158 and perplexity is 71.57547089189787
At time: 1086.8379092216492 and batch: 1200, loss is 4.303019828796387 and perplexity is 73.92269084511693
At time: 1087.6857903003693 and batch: 1250, loss is 4.302776374816895 and perplexity is 73.90469626237116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.573799523123859 and perplexity of 96.91162910736679
Finished 49 epochs...
Completing Train Step...
At time: 1089.9511771202087 and batch: 50, loss is 4.281001224517822 and perplexity is 72.3128051278602
At time: 1090.7772188186646 and batch: 100, loss is 4.316719913482666 and perplexity is 74.94240712023564
At time: 1091.6017725467682 and batch: 150, loss is 4.228958282470703 and perplexity is 68.64568550070805
At time: 1092.4267966747284 and batch: 200, loss is 4.282596693038941 and perplexity is 72.42827001792101
At time: 1093.2782740592957 and batch: 250, loss is 4.303439493179321 and perplexity is 73.95372007603142
At time: 1094.1038084030151 and batch: 300, loss is 4.316745986938477 and perplexity is 74.9443611532501
At time: 1094.925665140152 and batch: 350, loss is 4.300617036819458 and perplexity is 73.74528321880572
At time: 1095.7522196769714 and batch: 400, loss is 4.31726113319397 and perplexity is 74.98297840618113
At time: 1096.5800395011902 and batch: 450, loss is 4.23546335697174 and perplexity is 69.09368635874492
At time: 1097.4040505886078 and batch: 500, loss is 4.265791397094727 and perplexity is 71.22126197581629
At time: 1098.2262654304504 and batch: 550, loss is 4.2655050086975095 and perplexity is 71.20086795319634
At time: 1099.0515162944794 and batch: 600, loss is 4.283450937271118 and perplexity is 72.49016788401386
At time: 1099.8770232200623 and batch: 650, loss is 4.305212459564209 and perplexity is 74.084953837911
At time: 1100.7009100914001 and batch: 700, loss is 4.300064249038696 and perplexity is 73.70452899261049
At time: 1101.5253338813782 and batch: 750, loss is 4.313150711059571 and perplexity is 74.67539928511111
At time: 1102.3502683639526 and batch: 800, loss is 4.334041385650635 and perplexity is 76.25182774684305
At time: 1103.1716859340668 and batch: 850, loss is 4.376940746307373 and perplexity is 79.59416182928744
At time: 1103.9955039024353 and batch: 900, loss is 4.338143825531006 and perplexity is 76.56528882392527
At time: 1104.8203859329224 and batch: 950, loss is 4.3197636318206785 and perplexity is 75.17085819307373
At time: 1105.661093711853 and batch: 1000, loss is 4.2986281299591065 and perplexity is 73.59875648141184
At time: 1106.5063910484314 and batch: 1050, loss is 4.288731074333191 and perplexity is 72.87393819439262
At time: 1107.3437695503235 and batch: 1100, loss is 4.236813917160034 and perplexity is 69.187064583066
At time: 1108.1745698451996 and batch: 1150, loss is 4.269937171936035 and perplexity is 71.51714219447972
At time: 1108.9992678165436 and batch: 1200, loss is 4.302362718582153 and perplexity is 73.87413144608482
At time: 1109.8249192237854 and batch: 1250, loss is 4.302039413452149 and perplexity is 73.850251420889
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.573810660926095 and perplexity of 96.91270849593715
Annealing...
Finished Training.
Improved accuracyfrom -140.90856529531894 to -96.91162910736679
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f106f4f9390>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'wordvec_dim': 200, 'seq_len': 35, 'num_layers': 1, 'data': 'wikitext', 'dropout': 0.5024071221904451, 'wordvec_source': '', 'lr': 23.21866549715745, 'tune_wordvecs': True, 'batch_size': 50, 'anneal': 3.532086664725023}, 'best_accuracy': -163.72543770352107}, {'params': {'wordvec_dim': 200, 'seq_len': 35, 'num_layers': 1, 'data': 'wikitext', 'dropout': 0.41373980563436863, 'wordvec_source': '', 'lr': 20.68176370898624, 'tune_wordvecs': True, 'batch_size': 50, 'anneal': 7.89598993802388}, 'best_accuracy': -154.66225310466675}, {'params': {'wordvec_dim': 200, 'seq_len': 35, 'num_layers': 1, 'data': 'wikitext', 'dropout': 0.569224735752856, 'wordvec_source': '', 'lr': 19.825360077865398, 'tune_wordvecs': True, 'batch_size': 50, 'anneal': 3.400308428814557}, 'best_accuracy': -155.91987382208427}, {'params': {'wordvec_dim': 200, 'seq_len': 35, 'num_layers': 1, 'data': 'wikitext', 'dropout': 0.8897950912075626, 'wordvec_source': '', 'lr': 22.34800878682042, 'tune_wordvecs': True, 'batch_size': 50, 'anneal': 3.2464305562406413}, 'best_accuracy': -171.55802002745472}, {'params': {'wordvec_dim': 200, 'seq_len': 35, 'num_layers': 1, 'data': 'wikitext', 'dropout': 0.7963113862382708, 'wordvec_source': '', 'lr': 0.08714782372523766, 'tune_wordvecs': True, 'batch_size': 50, 'anneal': 2.8334704573244425}, 'best_accuracy': -140.90856529531894}, {'params': {'wordvec_dim': 200, 'seq_len': 35, 'num_layers': 1, 'data': 'wikitext', 'dropout': 0.7870759554568665, 'wordvec_source': '', 'lr': 0.701859308968518, 'tune_wordvecs': True, 'batch_size': 50, 'anneal': 2.685593450441546}, 'best_accuracy': -96.91162910736679}]
